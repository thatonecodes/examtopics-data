{"pageProps":{"questions":[{"id":"aCFex84XVrlszLgpYOly","topic":"1","exam_id":32,"answer_images":[],"choices":{"B":"Use AWS SMS to migrate the virtual machines.","A":"Use new Amazon EC2 instances and reinstall all application code.","D":"Use AWS Snowball to migrate the data.","E":"Use AWS SMS to copy the infrequently accessed data from the NAS.","C":"Use AWS Storage Gateway to migrate the data to cloud-native storage."},"question_text":"A company's lease of a colocated storage facility will expire in 90 days. The company wants to move to AWS to avoid signing a contract extension. The company's environment consists of 200 virtual machines and a NAS with 40 TB of data. Most of the data is archival, yet instant access is required when data is requested.\nLeadership wants to ensure minimal downtime during the migration. Each virtual machine has a number of customized configurations. The company's existing 1\nGbps network connection is mostly idle, especially after business hours.\nWhich combination of steps should the company take to migrate to AWS while minimizing downtime and operational impact? (Choose two.)","answer":"BC","timestamp":"2022-04-22 12:19:00","answers_community":["BC (82%)","BD (18%)"],"discussion":[{"poster":"Bigbearcn","comment_id":"590730","timestamp":"1650735000.0","comments":[{"timestamp":"1651850640.0","comment_id":"597778","content":"\"after hours\" meaning 16h instead of 24.\nWhich means roughly 6 days for the data transfer to AWS, better than snowball, takes 7 days for data to available on AWS.\nAC is good.","upvote_count":"1","comments":[{"comment_id":"597779","content":"I mean BC","poster":"Ryannn","upvote_count":"2","timestamp":"1651850640.0"}],"poster":"Ryannn"}],"content":"Selected Answer: BC\n1/8*3600*24=10,800. \nWith 1 Gb netowrk to transfer 40TB data, it takes about 4 days. So snowball is not needed.","upvote_count":"11"},{"timestamp":"1672772340.0","upvote_count":"1","poster":"evargasbrz","comment_id":"764943","content":"Selected Answer: BC\nI'll go with B and C"},{"upvote_count":"1","comment_id":"706817","timestamp":"1666999860.0","poster":"sindra","content":"Selected Answer: BC\nvote for BC"},{"poster":"Ell89","content":"Selected Answer: BC\nBC\n40TB is easy work for a 1Gbps line so more feasible than snowball","upvote_count":"1","timestamp":"1664976240.0","comment_id":"686920"},{"poster":"Enigmaaaaaa","comments":[{"poster":"joancarles","upvote_count":"1","comment_id":"672093","content":"But you need the BW for the SMS migration too, so it takes a lot of time more sharing the same connection for both. Maybe the snowball have more sense in this scenario.\nBD in my opinion.","timestamp":"1663487220.0"}],"comment_id":"633006","upvote_count":"2","timestamp":"1658146440.0","content":"Selected Answer: BC\nBC\nwith 1Gbps of inactive speed means about 128MBs which will take about 4 days lets say only off hours so it will take about 8 days in total.\nBetter to use Storage File GW no need to order and send a snowball for this"},{"content":"Selected Answer: BC\nvote for bc","poster":"aandc","comment_id":"627841","upvote_count":"2","timestamp":"1657105200.0"},{"poster":"future77","content":"C. 1024Mbps is nearly 100MB per second, which will be enough to transfer the 40TB in 90days which data is unchanging(archived). \nB. SMS is good for mission critical VMs which also guarantees little downtime.\nA. is wrong, since it will create downtime, .\nD. Will not be needed. Already explained it can be done online via Storage gateway.\n E. AWS SMS is for VM, not for data.","upvote_count":"2","timestamp":"1655049840.0","comment_id":"615412"},{"content":"BC \nC is better than D because it's less disruption to the system, which is the question requires.","poster":"SeanQi","timestamp":"1654992540.0","upvote_count":"1","comment_id":"615114"},{"content":"Should be BD.","comment_id":"590740","poster":"shailurtm2001","timestamp":"1650736440.0","upvote_count":"3"},{"content":"Selected Answer: BD\nIsn't the SMS designed for VMs migration?","upvote_count":"4","poster":"mirnuj_atom","comment_id":"589891","timestamp":"1650622740.0"}],"answer_description":"","question_id":866,"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/74112-exam-aws-certified-solutions-architect-professional-topic-1/","question_images":[],"unix_timestamp":1650622740,"answer_ET":"BC"},{"id":"qTgZgxDgRgwcVVrN0WV6","answer_description":"","topic":"1","discussion":[{"upvote_count":"12","timestamp":"1650731100.0","content":"Selected Answer: BD\nB & D\nYou can write S3 streams in mutliple languages using its SDK, there is no need to download the files.\nYou should use Fargate and not Lambda becuase the proccesing time is bigger than 15 minutes. In Fargate you only pay for the resources you consume while your container is running instead of running the EC2 instance the all month.","comment_id":"590708","poster":"snakecharmer2"},{"upvote_count":"1","poster":"mrgreatness","comment_id":"715582","content":"B and D for me","timestamp":"1668116100.0"},{"comment_id":"650291","poster":"asfsdfsdf","timestamp":"1661173620.0","content":"Selected Answer: BD\nLooks like B and D.\nLambda cant be used as the process is 2 hours - So A and E are out\nRegarding C - It will do a full download of the files - need to split it\nB - use stream (passthrough) so only the size of the stream is needed.\nD - Not need to pay for an instance just create a task once a month","upvote_count":"2"},{"timestamp":"1651687080.0","poster":"Yamchi","comment_id":"596978","upvote_count":"1","content":"BD is for me"},{"poster":"hfeng95","upvote_count":"2","comment_id":"596033","timestamp":"1651494540.0","content":"It is B & C. The key is to avoid the network traffic during the compression. Streaming compression and local disk compression fit into this requirement."},{"upvote_count":"1","content":"Selected Answer: CD\nIt's C and D. use Fargate instead of EC2 to optimize cost.\nLambda has 15 min limits, It may not be enough to download the file, compress it and upload to S3. So A and E is rule out.\n\nFor C, I am not sure Fargate local storage can store such large files because Fargate local storage max size is 200GB.","comment_id":"590723","timestamp":"1650734040.0","poster":"Bigbearcn"}],"unix_timestamp":1650731100,"timestamp":"2022-04-23 18:25:00","answers_community":["BD (93%)","7%"],"answer_ET":"BD","choices":{"C":"Configure the application to download the source files from Amazon S3 and save the files to local storage. Compress the files and upload them to Amazon S3.","E":"Provision an Amazon Elastic File System (Amazon EFS) file system. Attach the file system to the AWS Lambda function.","A":"Migrate the application to run an AWS Lambda function. Use Amazon EventBridge (Amazon CloudWatch Events) to schedule the Lambda function to run once each month.","D":"Configure the application to run as a container in AWS Fargate. Use Amazon EventBridge (Amazon CloudWatch Events) to schedule the task to run once each month.","B":"Configure the application to download the source files by using streams. Direct the streams into a compression library. Direct the output of the compression library into a target object in Amazon S3."},"question_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/74255-exam-aws-certified-solutions-architect-professional-topic-1/","question_text":"A company has an application. Once a month, the application creates a compressed file that contains every object within an Amazon S3 bucket. The total size of the objects before compression is 1 TB.\nThe application runs by using a scheduled cron job on an Amazon EC2 instance that has a 5 TB Amazon Elastic Block Store (Amazon EBS) volume attached. The application downloads all the files from the source S3 bucket to the EBS volume, compresses the file, and uploads the file to a target S3 bucket. Every invocation of the application takes 2 hours from start to finish.\nWhich combination of actions should a solutions architect take to OPTIMIZE costs for this application? (Choose two.)","question_id":867,"answer_images":[],"answer":"BD","exam_id":32},{"id":"1heM2WnlE7yd1cHA1Tba","answers_community":["ACF (100%)"],"answer_ET":"ACF","choices":{"A":"Create a bucket policy that includes read permissions for the S3 bucket. Set the principal of the bucket policy to the account ID of the Strategy account.","D":"Create a bucket policy that includes read permissions for the S3 bucket. Set the principal of the bucket policy to an anonymous user.","B":"Update the strategy_reviewer IAM role to grant full permissions for the S3 bucket and to grant decrypt permissions for the custom KMS key.","C":"Update the custom KMS key policy in the Creative account to grant decrypt permissions to the strategy_reviewer IAM role.","E":"Update the custom KMS key policy in the Creative account to grant encrypt permissions to the strategy_reviewer IAM role.","F":"Update the strategy_reviewer IAM role to grant read permissions for the S3 bucket and to grant decrypt permissions for the custom KMS key."},"question_images":[],"answer":"ACF","discussion":[{"upvote_count":"4","content":"Selected Answer: ACF\nACF nothing else make sense - users need to decrypt and read","timestamp":"1658153160.0","comment_id":"633055","poster":"Enigmaaaaaa"},{"upvote_count":"2","timestamp":"1656749880.0","poster":"aandc","content":"ACF for sure","comment_id":"626023"},{"timestamp":"1656493860.0","content":"Selected Answer: ACF\nShould be ACF","upvote_count":"1","comment_id":"624529","poster":"TechX"},{"upvote_count":"2","poster":"Yamchi","content":"A C F for me","comment_id":"596984","timestamp":"1651687500.0"},{"comment_id":"590672","upvote_count":"1","content":"Selected Answer: ACF\nIt's ACF.","timestamp":"1650722400.0","poster":"Bigbearcn"},{"timestamp":"1650647160.0","poster":"snakecharmer2","comment_id":"590146","content":"Selected Answer: ACF\nhttps://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-denied-error-s3/\nACF\nIn addition to the url above, you can eliminate the 3 of the answers easily\nB- wrong becuase of the \"full access\"\nD- wrong because of the \"anonymous user\"\nE- wrong because of the \"encrypt\" - u need decrypt permission","upvote_count":"4"},{"timestamp":"1650608580.0","upvote_count":"2","poster":"mirnuj_atom","comment_id":"589778","content":"Selected Answer: ACF\nshouldn't it be ACF?"}],"topic":"1","isMC":true,"question_id":868,"exam_id":32,"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/74095-exam-aws-certified-solutions-architect-professional-topic-1/","unix_timestamp":1650608580,"timestamp":"2022-04-22 08:23:00","answer_description":"","question_text":"A digital marketing company has multiple AWS accounts that belong to various teams. The creative team uses an Amazon S3 bucket in its AWS account to securely store images and media files that are used as content for the company's marketing campaigns. The creative team wants to share the S3 bucket with the strategy team so that the strategy team can view the objects.\nA solutions architect has created an IAM role that is named strategy_reviewer in the Strategy account. The solutions architect also has set up a custom AWS Key\nManagement Service (AWS KMS) key in the Creative account and has associated the key with the S3 bucket. However, when users from the Strategy account assume the IAM role and try to access objects in the S3 bucket, they receive an Access Denied error.\nThe solutions architect must ensure that users in the Strategy account can access the S3 bucket. The solution must provide these users with only the minimum permissions that they need.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose three.)"},{"id":"adAKiOcPi8jkr4C4yLRr","answers_community":["D (100%)"],"question_id":869,"answer_ET":"D","question_text":"A company is running several large workloads on Amazon EC2 instances. Each EC2 instance has multiple Amazon Elastic Block Store (Amazon EBS) volumes attached to it. Once each day, an AWS Lambda function invokes the creation of EBS volume snapshots. These snapshots accumulate until an administrator manually purges them.\nThe company must maintain backups for a minimum of 30 days. A solutions architect needs to reduce the costs of this process.\nWhich solution meets these requirements MOST cost-effectively?","url":"https://www.examtopics.com/discussions/amazon/view/74247-exam-aws-certified-solutions-architect-professional-topic-1/","exam_id":32,"timestamp":"2022-04-23 15:51:00","question_images":[],"answer_images":[],"answer_description":"","isMC":true,"topic":"1","answer":"D","unix_timestamp":1650721860,"choices":{"D":"Migrate the backup functionality to Amazon Data Lifecycle Manager (Amazon DLM). Create a lifecycle policy for the daily backup of the EBS volumes. Set the retention period for the EBS snapshots to 30 days.","B":"Create a second Lambda function to move the EBS snapshots that are older than 30 days to Amazon S3 Glacier Deep Archive.","A":"Search AWS Marketplace. Find a third-party solution to deploy to automatically manage the EBS volume backups.","C":"Set an Amazon S3 Lifecycle policy on the $3 bucket that contains the snapshots. Create a rule with an expiration action to delete EBS snapshots that are older than 30 days."},"discussion":[{"content":"Selected Answer: D\nD. \nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/automating-snapshots.html","upvote_count":"5","timestamp":"1650721860.0","comment_id":"590663","poster":"Bigbearcn"},{"content":"Selected Answer: D\nDLM no doubts","poster":"AwsBRFan","timestamp":"1665935640.0","comment_id":"696355","upvote_count":"2"},{"comment_id":"633056","poster":"Enigmaaaaaa","content":"D - DLM is exactly for this on EBS no need for a lambda function","upvote_count":"3","timestamp":"1658153220.0"}]},{"id":"zYdrQDrPHA3xbobv1yCk","question_images":[],"question_id":870,"answer_description":"","isMC":true,"answer_ET":"AC","exam_id":32,"question_text":"A company runs a serverless application in a single AWS Region. The application accesses external URLs and extracts metadata from those sites. The company uses an Amazon Simple Notification Service (Amazon SNS) topic to publish URLs to an Amazon Simple Queue Service (Amazon SQS) queue. An AWS Lambda function uses the queue as an event source and processes the URLs from the queue. Results are saved to an Amazon S3 bucket.\nThe company wants to process each URL in other Regions to compare possible differences in site localization. URLs must be published from the existing Region.\nResults must be written to the existing S3 bucket in the current Region.\nWhich combination of changes will produce multi-Region deployment that meets these requirements? (Choose two.)","unix_timestamp":1650538440,"topic":"1","answer_images":[],"answer":"AC","choices":{"A":"Deploy the SQS queue with the Lambda function to other Regions.","C":"Subscribe the SQS queue in each Region to the SNS topic.","B":"Subscribe the SNS topic in each Region to the SQS queue.","E":"Deploy the SNS topic and the Lambda function to other Regions.","D":"Configure the SQS queue to publish URLs to SNS topics in each Region."},"answers_community":["AC (94%)","6%"],"timestamp":"2022-04-21 12:54:00","url":"https://www.examtopics.com/discussions/amazon/view/74009-exam-aws-certified-solutions-architect-professional-topic-1/","discussion":[{"poster":"snakecharmer2","comment_id":"590169","timestamp":"1650649320.0","content":"Selected Answer: AC\nA & C\nFirst you create the SQS and Lambda in each region and then you subscribe the SQS queue to the SNS topic (there is no need to create antoher SNS topic in each region)\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-configure-subscribe-queue-sns-topic.html","upvote_count":"5"},{"upvote_count":"1","content":"A - Process URLs in other regions = Lambda + SQS as source >>\nB - After the above is complete send \n\n D - Wrong answer - URLs must be published from the existing region.\nE: Wrong answer - \"Process in other regions\" = lambda, but \"urls must be published from existing region\"","timestamp":"1668061260.0","comment_id":"714964","poster":"janvandermerwer"},{"poster":"sb333","comment_id":"685783","upvote_count":"2","timestamp":"1664830200.0","content":"Selected Answer: AC\nhttps://docs.aws.amazon.com/sns/latest/dg/sns-cross-region-delivery.html"},{"upvote_count":"1","poster":"MarkChoi","comment_id":"638978","content":"Selected Answer: CE\nIn other region. it is needed that SQS & Lambda.\nso, C&E is correct.\nA is not correct answerr.\nbecause, they say that \"deploy SQS to other region with using Lambda\"","timestamp":"1659067380.0"},{"poster":"Enigmaaaaaa","comment_id":"633059","upvote_count":"3","content":"Selected Answer: AC\nConfusing question... A and C since multiple SQS queues can subscribe to one SNS","timestamp":"1658153580.0"},{"poster":"bobsmith2000","content":"Selected Answer: AC\nIt's a fan out.","upvote_count":"2","timestamp":"1652373780.0","comment_id":"600733"},{"content":"It's A C","upvote_count":"1","timestamp":"1650721440.0","comment_id":"590659","poster":"Bigbearcn"},{"comments":[{"content":"true that is the key he company wants to process the URLs published through the same endpoint","comment_id":"598734","poster":"user0001","timestamp":"1652047500.0","upvote_count":"1"}],"content":"Selected Answer: AC\nAC - the company wants to process the URLs published through the same endpoint, so they are looking to a single SNS with cross-regional delivery but separate SQS + Lambda per region.","poster":"mirnuj_atom","comment_id":"589268","upvote_count":"3","timestamp":"1650538440.0"}]}],"exam":{"isImplemented":true,"id":32,"isBeta":false,"isMCOnly":false,"numberOfQuestions":1019,"name":"AWS Certified Solutions Architect - Professional","lastUpdated":"11 Apr 2025","provider":"Amazon"},"currentPage":174},"__N_SSP":true}