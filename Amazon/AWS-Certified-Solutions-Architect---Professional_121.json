{"pageProps":{"questions":[{"id":"IY47aXt0bDx3NH2vBBoq","question_images":[],"timestamp":"2020-11-02 09:30:00","question_text":"A fitness tracking company serves users around the world, with its primary markets in North America and Asia. The company needs to design an infrastructure for its read-heavy user authorization application with the following requirements:\n✑ Be resilient to problems with the application in any Region.\n✑ Write to a database in a single Region.\n✑ Read from multiple Regions.\n✑ Support resiliency across application tiers in each Region.\n✑ Support the relational database semantics reflected in the application.\nWhich combination of steps should a solutions architect take? (Choose two.)","unix_timestamp":1604305800,"answers_community":["CE (50%)","AE (50%)"],"topic":"1","isMC":true,"answer_description":"","exam_id":32,"answer_images":[],"choices":{"D":"Set up web, application, and Amazon RDS for MySQL instances in each Region. Set up the application so that reads are local and writes are partitioned based on the user. Set up a Multi-AZ failover for the web, application, and database servers. Set up cross-Region replication for the database layer.","B":"Deploy web, application, and MySQL database servers to Amazon EC2 instance in each Region. Set up the application so that reads and writes are local to the Region. Create snapshots of the web, application, and database servers and store the snapshots in an Amazon S3 bucket in both Regions. Set up cross- Region replication for the database layer.","A":"Use an Amazon Route 53 geoproximity routing policy combined with a multivalue answer routing policy.","E":"Set up active-active web and application servers in each Region. Deploy an Amazon Aurora global database with clusters in each Region. Set up the application to use the in-Region Aurora database endpoints. Create snapshots of the web application servers and store them in an Amazon S3 bucket in both Regions.","C":"Use an Amazon Route 53 geolocation routing policy combined with a failover routing policy."},"discussion":[{"poster":"bbnbnuyh","comment_id":"211035","comments":[{"comment_id":"214598","content":"E \"...snapshots of the web application servers...\" is for what?","upvote_count":"3","poster":"keos","timestamp":"1632157200.0","comments":[{"content":"Be robust to application-related issues in any Region.","timestamp":"1646829300.0","poster":"Sonujunko","upvote_count":"1","comment_id":"564030"}]},{"upvote_count":"7","comments":[{"comment_id":"582123","content":"Selected Anser : C, E. \nThe question says, most of the revenue comes from North America & Asia. So we can deploy our infrastructure by prioritising that. Then we can serve all North American Users from the North American region and Asia users from Asia deployments ( geolocation routing).\n\nWhy not A? A is a valid answer. But, if we set up geoproximity base routing, it will route traffic based on the closeness of AWS resources and users. In other terms, we can't give higher priority to our higher revenue regions.","timestamp":"1649298960.0","poster":"Hasitha99","upvote_count":"1"}],"content":"A,E\nFrom https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-multivalue\n\"Multivalue answer routing lets you configure Amazon Route 53 to return multiple values, such as IP addresses for your web servers, in response to DNS queries. You can specify multiple values for almost any record, but multivalue answer routing also lets you check the health of each resource, so Route 53 returns only values for healthy resources. It's not a substitute for a load balancer, but the ability to return multiple health-checkable IP addresses is a way to use DNS to improve availability and load balancing.\"","timestamp":"1634547540.0","poster":"pablobairat","comment_id":"427572"}],"upvote_count":"35","content":"C,E \nC because \"failover routing\" gives resiliency\nE because rest of the options dont make sense for read- heavy and write to central requirement","timestamp":"1632118440.0"},{"timestamp":"1634801160.0","content":"AAA EEE\n---\nThe first important thing to note is that users are from all over the world and not only from North America and Asia and that you have to be resilient to problem with the application in ANY REGION.\n\nWhat I don't like about Failover is that it works by creating 2 records (primary + secondary)\nSince you have to be resilient to problem with the application in ANY Region, how are you configuring the failover policy/ies?","comment_id":"436238","upvote_count":"7","poster":"tgv"},{"upvote_count":"1","timestamp":"1681948980.0","content":"Selected Answer: CE\nA is not correct.\nWith multi value answers, if one region is down, it can provide answer to that DNS query and route user to that region","comment_id":"875168","poster":"romiao106"},{"poster":"dmscountera","timestamp":"1666160340.0","upvote_count":"3","content":"Selected Answer: AE\nAs per the Q, you need to read/be resilient in ANY region not from just 2.\nSo multi-value supports up to 8 IPs > failover ~2\n8 > 2 =>\nAE","comment_id":"698723"},{"upvote_count":"3","poster":"kadev","comment_id":"652468","timestamp":"1661578080.0","content":"many people confuse A and C, this is explain:\n\"multivalue answer routing policy may cause the users to be randomly sent to other healthy regions\" => not good for performance\nand the point of this Q is \"resiliency\" => if request failed, it can route to another enpoint => failover"},{"comments":[{"comment_id":"891492","poster":"MikelH93","content":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html#dns-failover-types-active-active","upvote_count":"1","timestamp":"1683471540.0"}],"upvote_count":"2","comment_id":"635515","timestamp":"1658569500.0","poster":"Student1950","content":"I would vote for A and E.\nMulti value routing >> checks health of one or more DNS records and send traffic only to healthy record.\nFail-Over: used in active - passive traffic flow\nIf E is selected, network is active-active, and we need multivalve DNS routing and not failover DNS routing."},{"timestamp":"1651755120.0","upvote_count":"3","comment_id":"597312","content":"E for sure.\nBetween A and C.\nThat a tough one.\nWe have global users and have to proved a failover.\nOn one hand with geoproximity policy we can serve the content for global users from only two regions. In case of geolocation we must set up default region front users outside Asia and North America, but it's not mentioned in C.\nOn the other hand, multi-answer is not about failover, because it's random.\nSo with A we cover global users, but get random distribution b/w two regions.\nWith C we cover only two region but provide failover.","poster":"bobsmith2000"},{"poster":"Hasitha99","comment_id":"582132","upvote_count":"2","content":"Selected Answer: CE\nThe question says, most of the revenue comes from North America & Asia. So we can deploy our infrastructure by prioritising that. Then we can serve all North American Users from the North American region and Asia users from Asia deployments ( geolocation routing).\n\nWhy not A? A is a valid answer. But, if we set up geoproximity base routing, it will route traffic based on the closeness of AWS resources and users. In other terms, we can't give higher priority to our higher revenue regions.","timestamp":"1649299920.0"},{"content":"A. Use an Amazon Route 53 geoproximity routing policy combined with a multivalue answer routing policy.\nE. Set up active-active web and application servers in each Region. Deploy an Amazon Aurora global database with clusters in each Region. Set up the application to use the in-Region Aurora database endpoints. Create snapshots of the web application servers and store them in an Amazon S3 bucket in both Regions.","poster":"cldy","timestamp":"1638712140.0","upvote_count":"2","comment_id":"494385"},{"upvote_count":"3","poster":"backfringe","comment_id":"483991","timestamp":"1637571300.0","content":"I go for CE"},{"comment_id":"449473","content":"A is correct, as the question says there are users all around the world but the primary markets are in North America and Asia. To have better resilience use Geoproximity routing policy – Use when you want to route traffic based on the location of your resources and, optionally, shift traffic from resources in one location to resources in another.","timestamp":"1635734280.0","upvote_count":"1","poster":"CloudMan01"},{"comment_id":"447279","timestamp":"1635586080.0","content":"If you choose E, you have to choose A because the application is in active-active mode. If you choose C, it will become active-standby mode.","comments":[{"upvote_count":"1","poster":"RVivek","timestamp":"1640609640.0","comment_id":"510301","content":"under normal condition DNS will rresolve to the region user is closese t (goelocation policy), only when the region fails , failover policy is applied. So it is Active Active"}],"poster":"johnnsmith","upvote_count":"3"},{"content":"Answer E is either incorrect or badly written.\n\nEBS volume snapshots are stored in S3, however you cannot choose what bucket they are stored in nor can they be accessed through the S3 api.","timestamp":"1635559380.0","upvote_count":"2","poster":"Viper57","comment_id":"447249"},{"comment_id":"443236","upvote_count":"1","poster":"andylogan","timestamp":"1635105600.0","content":"It's C, E"},{"comments":[{"timestamp":"1635054420.0","comment_id":"442219","content":"So, we don't need geoproximity.","poster":"student22","upvote_count":"1"}],"content":"C,E\n\nWhy not A? Failover routing is better than multivalue answer for this case, and geolocation can be used here with no issues. \n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html","timestamp":"1634900400.0","poster":"student22","upvote_count":"1","comment_id":"442217"},{"timestamp":"1634898900.0","comments":[{"comment_id":"458476","timestamp":"1636234680.0","content":"Changing to C,E","poster":"student22","upvote_count":"1"}],"poster":"student22","upvote_count":"1","comment_id":"438196","content":"A,E\nA vs C - This applications is for 'users around the world'. So, Geoproximity is more suitable. It was for users in the given two regions, I'd have selected C."},{"comment_id":"430583","comments":[{"upvote_count":"1","comment_id":"433710","content":"You are completely wrong.\nIn this case: You can back up the data on your Amazon EBS volumes to Amazon S3 by taking point-in-time snapshots.","poster":"DerekKey","timestamp":"1634680200.0","comments":[{"content":"The underlying snapshot is stored in S3, however you cannot access the snapshots in any buckets.","comment_id":"447248","upvote_count":"1","poster":"Viper57","timestamp":"1635282420.0"}]}],"timestamp":"1634614740.0","content":"C,D\nB,E make no sense, You cannot save any aws snapshot to s3 bucket.","poster":"near22","upvote_count":"1"},{"poster":"WhyIronMan","upvote_count":"4","comment_id":"413277","timestamp":"1634483040.0","content":"I'll go with C,E"},{"upvote_count":"1","content":"Will go with C and E. Why c, because want to limit the user to region only. Geolocation policy allows to send the traffic to resources in the same region from where the request was originated i.e. it allows to have site affinity based on the location of the users.","comment_id":"391115","timestamp":"1634451840.0","poster":"tushar321"},{"timestamp":"1634430540.0","comment_id":"354282","poster":"Waiweng","upvote_count":"3","content":"it's C,E"},{"poster":"Amitv2706","content":"For those who are supporting A:\nhow will you support failover/resiliency with multi value routing?\nC with E seems the answer","timestamp":"1634333700.0","comments":[{"poster":"bobsmith2000","content":"Healthchecks","upvote_count":"1","timestamp":"1651754340.0","comment_id":"597304"}],"comment_id":"342655","upvote_count":"2"},{"upvote_count":"4","content":"C,E are correct answer\nHere is C VS A \nGeoproximity routing policy is good to control the user traffic to specific regions. However, a multivalue answer routing policy may cause the users to be randomly sent to other healthy regions that may be far away from the user’s location.\n\nYou can use geolocation routing policy to direct the North American users to your servers on the North America region and configure failover routing to the Asia region in case the North America region fails. You can configure the same for the Asian users pointed to the Asia region servers and have the North America region as its backup.","comment_id":"327216","poster":"ExtHo","timestamp":"1633966200.0"},{"upvote_count":"5","comment_id":"297940","content":"AE\n===\nIf it is country-based, should go for geolocation.\nBut if it is region-based like North America, Asia, Geoproximity would be a better solution.","timestamp":"1633883220.0","poster":"gpark"},{"comment_id":"293102","poster":"Kian1","timestamp":"1633807560.0","content":"going with AE","upvote_count":"3"},{"comment_id":"265327","poster":"Ebi","content":"I will go with AE","timestamp":"1633335120.0","upvote_count":"5"},{"timestamp":"1633267980.0","comments":[{"timestamp":"1634204400.0","poster":"certainly","comment_id":"334044","upvote_count":"1","content":"that make perfect sense to me. I would agree A over C."}],"comment_id":"258944","poster":"Bulti","content":"This link shows how geolocation routing policy works with failover routing policy. That doesn't mean it works only for active-passive scenarios. It can work for active-active scenarios as well. https://aws.amazon.com/premiumsupport/knowledge-center/route-53-active-passive-failover/ However the problem is that the application has users all over the world so you would need to create many geolocation policies and associate the failover policy to them. Instead option A which is based on Geoproximity makes more sense and therefore A &E are the right answers.","upvote_count":"4"},{"timestamp":"1633205340.0","upvote_count":"4","content":"A, E- Given that there are users all over the world, Geoproximity routing policy makes more sense that Geolocation routing policy. I would also go with E as it meets the requirements around allowing writes to a single Region and reads from multiple regions. The Aurora Global DB enables that. Option B and D doesn't seem to meet the requirement around being able to write to a single region.","comment_id":"255339","poster":"Bulti"},{"comments":[{"timestamp":"1633369260.0","poster":"shammous","content":"C: failover routing policy\nA: not really appropriate for resilience","upvote_count":"3","comment_id":"277809"}],"timestamp":"1633194480.0","comment_id":"251739","poster":"darthvoodoo","content":"A and E\nA. Multivalue for active-active settings and health checks for handling failures\nB. Wrong because database on EC2 cannot provide resiliency\nC. Is for Active-Passive\nD. The two databases will go out of sync. \nE. Fulfills the resiliency and read/write objectives","upvote_count":"4"},{"content":"May be AD?","upvote_count":"1","poster":"T14102020","comment_id":"244867","timestamp":"1633142280.0"},{"timestamp":"1632906840.0","poster":"joos","upvote_count":"2","content":"CD !!!","comment_id":"239019"},{"timestamp":"1632646320.0","poster":"jackdryan","upvote_count":"3","content":"I'll go with C,E","comment_id":"232563"},{"timestamp":"1632577980.0","upvote_count":"2","content":"C: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\nE: active-active make more sense and aurora global databases https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html#aurora-global-database-overview","comment_id":"217944","poster":"pinox1"},{"content":"AE !!!","upvote_count":"4","timestamp":"1632484380.0","comment_id":"217438","poster":"smartassX"},{"poster":"MarkDillon1075","content":"A and D","upvote_count":"1","comment_id":"216108","timestamp":"1632227460.0"}],"answer":"CE","url":"https://www.examtopics.com/discussions/amazon/view/35742-exam-aws-certified-solutions-architect-professional-topic-1/","answer_ET":"CE","question_id":601},{"id":"abf2F3AYX3i3UBKvgXgs","choices":{"D":"Configure CloudTrail and VPC Flow Logs to send data to a log group in Amazon CloudWatch Logs in each AWS account. Create AWS Lambda functions in each AWS accounts to subscribe to the log groups and stream the data to an Amazon S3 bucket in the logging account. Create another Lambda function to load data from the S3 bucket to Amazon ES in the logging account.","C":"Configure CloudTrail and VPC Flow Logs to send data to a separate Amazon S3 bucket in each AWS account. Create an AWS Lambda function triggered by S3 events to copy the data to a centralized logging bucket. Create another Lambda function to load data from the S3 bucket to Amazon ES in the logging account.","B":"Configure CloudTrail and VPC Flow Logs to send data to a log group in Amazon CloudWatch account. Configure a CloudWatch subscription filter in each AWS account to send data to Amazon Kinesis Data Firehouse in the logging account. Load data from Kinesis Data Firehouse into Amazon ES in the logging account.","A":"Configure CloudTrail and VPC Flow Logs in each AWS account to send data to a centralized Amazon S3 bucket in the logging account. Create and AWS Lambda function to load data from the S3 bucket to Amazon ES in the logging account."},"answer_description":"","answer":"B","url":"https://www.examtopics.com/discussions/amazon/view/35842-exam-aws-certified-solutions-architect-professional-topic-1/","answer_images":[],"exam_id":32,"answers_community":["B (92%)","8%"],"unix_timestamp":1604366460,"question_text":"A company needs to create a centralized logging architecture for all of its AWS accounts. The architecture should provide near-real-time data analysis for all AWS\nCloudTrail logs and VPC Flow Logs across all AWS accounts. The company plans to use Amazon Elasticsearch Service (Amazon ES) to perform log analysis in the logging account.\nWhich strategy a solutions architect use to meet these requirements?","topic":"1","isMC":true,"timestamp":"2020-11-03 02:21:00","answer_ET":"B","question_images":[],"question_id":602,"discussion":[{"content":"B. It is well defined here - https://www.cloudjourney.io/articles/publiccloud/central_logging_part_2-su/","comments":[{"content":"https://aws.amazon.com/solutions/implementations/centralized-logging/","timestamp":"1634605320.0","comment_id":"302437","upvote_count":"3","comments":[{"comment_id":"327982","timestamp":"1635004800.0","content":"Thanks certainly. This is what I need.","poster":"sayakan","upvote_count":"1"}],"poster":"certainly"},{"comment_id":"406852","poster":"Kopa","timestamp":"1635441900.0","upvote_count":"2","comments":[{"content":"CloudWatch subscription filter support sending to Kinesis data streams and Firehose so B looks correct.","comment_id":"457329","upvote_count":"1","poster":"Viper57","timestamp":"1636270800.0"},{"poster":"student22","comment_id":"442221","timestamp":"1636121400.0","upvote_count":"1","content":"Thanks. I also read the question as \"... send data to a log group in each account\" So, my answer is B. If it's really taking about a central cloudwatch account, the answer is A.\n\nSite admins, verify please?"}],"content":"The B answer is saying: \"Configure a CloudWatch subscription filter in each AWS account to send data to Amazon Kinesis Data Firehouse \" on the link it is described to send data to Amazon Kinesis DataStream then Lambda and after that to Kinesis FireHose, it looks that Kinesis DataStream not mention on the answer. Im again for B but it looks suspicious."}],"timestamp":"1632124980.0","upvote_count":"19","comment_id":"211610","poster":"bbnbnuyh"},{"content":"I think A still a correct option..\nBased on AWS documentation: A trail enables CloudTrail to deliver log files to an Amazon S3 bucket\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/cloudtrail-logging.html","upvote_count":"5","timestamp":"1633097580.0","poster":"Kelvin1477","comment_id":"236569","comments":[{"upvote_count":"1","poster":"DerekKey","content":"CloudTrail delivers to both S3 and CloudWatch","timestamp":"1635558360.0","comment_id":"433718"}]},{"content":"B for sure, CW loggrpup supcription supports to kinesis firehose now\nA. you cant trigger event when S3 object update for log file updating, it's so expensive. if you run lambda as scheduled => it not near-realtime","comment_id":"656211","upvote_count":"1","timestamp":"1662033120.0","poster":"kadev"},{"upvote_count":"2","poster":"gnic","timestamp":"1661766960.0","content":"Selected Answer: B\nthe keyword is \"near real time\"\nI was for A, but B is better","comment_id":"653428"},{"comment_id":"642728","poster":"fdoxxx","content":"The answer is A - why not B? the service Amazon Kinesis Data Firehouse does not exists - there is Amazon Kinesis Data Firehose - this typo is on purpose imho.","timestamp":"1659674940.0","upvote_count":"1"},{"comment_id":"640781","poster":"shucht","comments":[{"comment_id":"653427","content":"it can","poster":"gnic","timestamp":"1661766900.0","upvote_count":"1"}],"content":"Selected Answer: A\nIt cannot be B because firehose cannot output to ElasticSearch","upvote_count":"1","timestamp":"1659375000.0"},{"comment_id":"637679","content":"the keyword here is real time. B","poster":"hilft","timestamp":"1658878980.0","upvote_count":"1"},{"timestamp":"1654651260.0","content":"Selected Answer: B\nShould be B. Due to \"near-real-time data analysis\" -> Use Kinesis Data Firehouse to send data log to ES is best practice","comment_id":"613006","poster":"Anhdd","upvote_count":"2"},{"poster":"alexph169","comment_id":"600514","timestamp":"1652341440.0","upvote_count":"2","content":"Selected Answer: B\nNear real time is the keyword. Can not be Lambda here that is an async call mechanism."},{"content":"The requirement says near real time, based on that Kinesis will satisfy this, so the only answer likely is B","upvote_count":"3","comment_id":"549195","poster":"jyrajan69","timestamp":"1645078620.0"},{"timestamp":"1643832660.0","content":"Selected Answer: B\nFirehose for near-real time.","comment_id":"539158","upvote_count":"2","poster":"Jonfernz"},{"poster":"cldy","comment_id":"499218","upvote_count":"1","content":"B. Configure CloudTrail and VPC Flow Logs to send data to a log group in Amazon CloudWatch account. Configure a CloudWatch subscription filter in each AWS account to send data to Amazon Kinesis Data Firehouse in the logging account. Load data from Kinesis Data Firehouse into Amazon ES in the logging account.","timestamp":"1639213140.0"},{"timestamp":"1638951660.0","comment_id":"496633","poster":"bill_smoke","upvote_count":"2","content":"Could someone please confirm whether these question sets are still on the SAA-C02 exam for December? I'm taking my test in a week and want to make sure this is all legit."},{"content":"B is right","upvote_count":"1","poster":"AzureDP900","timestamp":"1638916440.0","comment_id":"496364"},{"timestamp":"1638492660.0","poster":"Rho_Ohm","content":">>> Ans: B","comment_id":"492873","upvote_count":"1"},{"timestamp":"1638234480.0","poster":"acloudguru","comment_id":"490310","content":"Selected Answer: B\nB, near-real-time","upvote_count":"3"},{"content":"It's B","comment_id":"443238","poster":"andylogan","timestamp":"1636198140.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1635939780.0","content":"BBB\n---","comment_id":"436246","poster":"tgv"},{"poster":"DerekKey","content":"B correct - \nCross-account log data sharing using Kinesis Data Firehose with a destionation set to Amazon ES\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions-Firehose.html","comment_id":"433720","timestamp":"1635684060.0","upvote_count":"1"},{"timestamp":"1635557760.0","content":"B is the answer. Near real-time.","poster":"blackgamer","comment_id":"433281","upvote_count":"1"},{"comment_id":"415350","timestamp":"1635497640.0","content":"The answer is B. You can create a cloudwatch log subscription filter to send data to Kinesis data firehouse (https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html#FirehoseExample). Lambda could work but needs a heavy administration to sync the \"near-real-time\" processing","poster":"zolthar_z","upvote_count":"2"},{"timestamp":"1635474720.0","upvote_count":"2","content":"I'll go with B","poster":"WhyIronMan","comment_id":"413279"},{"content":"it's B","poster":"Waiweng","comment_id":"355269","upvote_count":"2","timestamp":"1635368280.0"},{"timestamp":"1635097800.0","poster":"Amitv2706","upvote_count":"1","comment_id":"342685","content":"B is possible and possibly a better option than A, as in A will have to use lambda functions. https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions-Firehose.html"},{"comment_id":"329260","content":"B. check these links\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html\n\" In this example, you'll create a CloudWatch Logs subscription that sends any incoming log events that match your defined filters to your Amazon Kinesis Data Firehose delivery stream. Data sent from CloudWatch Logs to Amazon Kinesis Data Firehose is already compressed with gzip level 6 compression, so you do not need to use compression within your Kinesis Data Firehose delivery stream.\"","upvote_count":"1","poster":"AJBA","timestamp":"1635094020.0"},{"timestamp":"1634920440.0","comment_id":"322493","upvote_count":"1","poster":"awsnoob","content":"B, the requirement is near real time"},{"comment_id":"293321","upvote_count":"4","content":"going with B, Kinesis Firehose","timestamp":"1634063340.0","poster":"Kian1"},{"poster":"01037","upvote_count":"2","comment_id":"285487","content":"A.\nI think all options works, and I've tried A and B. but A is the best here, because it's easy and cheaper.\nThought\n-------------------------------------------------\nKinesis streams are currently the only resource supported as a destination for cross-account subscriptions.\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html\n-------------------------------------------------\nI tried firehose, it works. Below is the cloudformation template.\nhttps://gist.github.com/xinwo/ed0f37e3e3a133a8114c0d420360ccdf\nCreate a ES domain and change ExtendedS3DestinationConfiguration to ElasticsearchDestinationConfiguration","timestamp":"1633880580.0","comments":[{"poster":"01037","comments":[{"poster":"sotaku","comment_id":"350720","content":"Well explained, but would it B more \"real-time\" compared with A? as in Option A, Lambda will be scheduled to run instead of instantly triggered.","timestamp":"1635138900.0","upvote_count":"1"}],"upvote_count":"1","comment_id":"285489","content":"Then, create a subscription filter on log groups against destination \"TestDestination\" in sending accounts.","timestamp":"1634050980.0"}]},{"content":"My answer is A\nB: not possible, CW log in account A, Kinesis firehose in account B not possible, only Kinesis DS is supported for cross-account subscription\nC: No need to have sperate bucket which needs two Lambda functions \nD: Technically possible, but again two Lambda functions","comments":[{"timestamp":"1635769920.0","upvote_count":"1","comment_id":"436243","poster":"tgv","content":"B is possible --> https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions-Firehose.html"}],"upvote_count":"3","comment_id":"285137","timestamp":"1633798020.0","poster":"Ebi"},{"poster":"cox1960","content":"Outdated. A and B would work.\nA better solution would be to use CloudWatch Logs subscription with ES as a target. \nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_ES_Stream.html","upvote_count":"1","comment_id":"280081","timestamp":"1633667520.0"},{"timestamp":"1633610700.0","content":"A:\nhttps://aws.amazon.com/blogs/mt/visualizing-aws-cloudtrail-events-using-kibana/\nhttps://docs.aws.amazon.com/elasticsearch-service/latest/developerguide/es-aws-integrations.html","poster":"rasti","comment_id":"263428","upvote_count":"1"},{"timestamp":"1633594860.0","content":"The correct answer is A. It is the fast way to get the logs to the ES. \nhttps://docs.aws.amazon.com/vpc/latest/userguide/flow-logs-s3.html\nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-receive-logs-from-multiple-accounts.html\nBoth flowLogs and Cloudtrail allow writing to S3 bucket in a centralized account. Once there , a lambda function can be triggered using the putObject event to write to ES.","upvote_count":"3","poster":"Bulti","comment_id":"255353","comments":[{"timestamp":"1633606080.0","comment_id":"258967","poster":"Bulti","content":"Changing answer to B and here is the reason. This link-> https://docs.aws.amazon.com/elasticsearch-service/latest/developerguide/es-aws-integrations.html shows different ways in which you can stream data into ES. The option that is available out-of-the-box with no coding involved is using Kinesis Firehose. So as long as its possible to stream data from KFH cross-account into ES, I think that would be the best option. Not sure if its going to be a fastest but KFH is considered a near real-time service.","upvote_count":"3"}]},{"comment_id":"251969","upvote_count":"1","poster":"ting_66","content":"D is correct\n\"near real-time\", either lambda or kinesis data stream\nhttps://aws.amazon.com/about-aws/whats-new/2015/09/near-real-time-processing-of-amazon-cloudwatch-logs-with-aws-lambda/\nWith firehose, the data will be process \"within 60s\" which is NOT near real-time.","timestamp":"1633586880.0"},{"poster":"T14102020","upvote_count":"2","comment_id":"244869","content":"Correct is B. Logs to send data to a log group CloudWatch + send data to Amazon Kinesis Data Firehouse + send into Amazon ES","timestamp":"1633502340.0"},{"timestamp":"1633455780.0","upvote_count":"1","poster":"rscloud","comment_id":"244766","content":"A\nCentralized bucket for all logs as company wants to analyze logs in logging account."},{"content":"I'll go with D","upvote_count":"1","comments":[{"timestamp":"1632763140.0","poster":"jackdryan","upvote_count":"1","content":"Changing to B","comment_id":"232568","comments":[{"timestamp":"1632852120.0","poster":"ali98","comment_id":"233530","comments":[{"content":"ali98 good point that I overlooked. Which answer do you support?","poster":"jackdryan","timestamp":"1632854760.0","upvote_count":"2","comment_id":"234649","comments":[{"content":"How do I set up cross-account streaming from Kinesis Data Firehose to Amazon Elasticsearch Service\nhttps://aws.amazon.com/premiumsupport/knowledge-center/kinesis-firehose-cross-account-streaming/","upvote_count":"2","timestamp":"1632860280.0","poster":"jackdryan","comment_id":"235073"}]},{"poster":"ali98","content":"Kinesis streams are currently the only resource supported as a destination for cross-account subscriptions. \nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html","comment_id":"235262","upvote_count":"1","timestamp":"1633024920.0","comments":[{"poster":"jackdryan","content":"Thank you for the link. I am convinced","comment_id":"235933","upvote_count":"1","timestamp":"1633051620.0"},{"timestamp":"1633173840.0","comments":[{"content":"B is correct","poster":"darthvoodoo","upvote_count":"1","timestamp":"1633453260.0","comment_id":"243046"}],"comment_id":"243043","poster":"darthvoodoo","content":"Wrong! You can create subscription filters and add a Kinesis FH endpoint as a destination and ship the logs cross account to the endpoint. https://aws.amazon.com/blogs/architecture/stream-amazon-cloudwatch-logs-to-a-centralized-account-for-audit-and-analysis/","upvote_count":"2"}]}],"upvote_count":"3","content":"I agree with your most of answer. but this wrong. as \"subscription filter in each AWS account to send data to Amazon Kinesis Data Firehouse in the logging account\". in same account yes possible but cross account you need Kinesis stream . correct me if I am wrong."}]}],"comment_id":"232566","timestamp":"1632662580.0","poster":"jackdryan"},{"content":"Answer is D\nAWS published a Solution Reference in the link below. Review the Appendix A: Sample Logs section that describes answer D.\n\nhttps://s3.amazonaws.com/solutions-reference/centralized-logging/latest/centralized-logging.pdf","comment_id":"220963","comments":[{"timestamp":"1632359700.0","poster":"XRiddlerX","comment_id":"229603","content":"For added clarity, on page 5 has a solution architecture that describes answer D.","upvote_count":"2"}],"timestamp":"1632279480.0","poster":"XRiddlerX","upvote_count":"4"}]},{"id":"SUZBP4CWFPTD7SWebfTw","choices":{"C":"Update the CloudWatch Events rule to trigger on Amazon EC2 ג€Instance Launch Successfulג€ and ג€Instance Terminate Successfulג€ events for the Auto Scaling group used by the cluster.","A":"Configure an Amazon SOS FIFO queue and configure a CloudWatch Events rule to use this queue as a target. Remove the Lambda target from the CloudWatch Events rule.","E":"Configure an Amazon SQS standard queue and configure the existing CloudWatch Events rule to use this queue as a target. Remove the Lambda target from the CloudWatch Events rule.","B":"Configure an Amazon Kinesis data stream and configure a CloudWatch Events rule to use this queue as a target. Remove the Lambda target from the CloudWatch Events rule.","F":"Configure a Lambda function to read data from the Amazon Kinesis data stream and configure the batch window to 5 minutes. Modify the function to make a single API call to Amazon Route 53 with all records read from the kinesis data stream.","D":"Configure a Lambda function to retrieve messages from an Amazon SQS queue. Modify the Lambda function to retrieve a maximum of 10 messages then batch the messages by Amazon Route 53 API call type and submit. Delete the messages from the SQS queue after successful API calls."},"answer":"ACD","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/35864-exam-aws-certified-solutions-architect-professional-topic-1/","answer_images":[],"exam_id":32,"answers_community":["ACD (50%)","CDE (50%)"],"unix_timestamp":1604372880,"question_text":"A financial company is using a high-performance compute cluster running on Amazon EC2 instances to perform market simulations. A DNS record must be created in an Amazon Route 53 private hosted zone when instances start. The DNS record must be removed after instances are terminated.\nCurrently the company uses a combination of Amazon CloudWatch Events and AWS Lambda to create the DNS record. The solution worked well in testing with small clusters, but in production with clusters containing thousands of instances the company sees the following error in the Lambda logs:\nHTTP 400 error (Bad request).\nThe response header also includes a status code element with a value of `Throttling` and a status message element with a value of `Rate exceeded`.\nWhich combination of steps should the Solutions Architect take to resolve these issues? (Choose three.)","topic":"1","isMC":true,"answer_ET":"ACD","timestamp":"2020-11-03 04:08:00","question_images":[],"question_id":603,"discussion":[{"timestamp":"1632096960.0","content":"C, D, E \nYou have to introduce a SQS: FIFO has limited throughput so may be a normal SQS queue with batching that can overcome the rate limits","comments":[{"timestamp":"1632151500.0","content":"CloudWatch--> SQS--> Lambda (batch) --> R53","poster":"beso","comment_id":"218331","upvote_count":"3"},{"comment_id":"339270","timestamp":"1633721580.0","content":"ACD is better as you need FIFO mode to ensure processing DNS records exactly once.","poster":"Kelvin","upvote_count":"6"},{"timestamp":"1634944920.0","upvote_count":"6","content":"ACD\nIf you use UPSERT to introduce the DNS records, if it does not exists, it creates it, if it exists, it update the values (in case of duplicates). For deleting, you use a delete, if it exists, it deletes it, if it was already deleted(duplicated message in the queue), it does nothing.\n\nThe goal here is to support thousands of instances launching and terminating, with a SQS FIFO queue this requirement is not fullfilled. And it was the original problem with Lambda and the concurrency.","poster":"pablobairat","comment_id":"427582"}],"comment_id":"211679","upvote_count":"28","poster":"bbnbnuyh"},{"upvote_count":"13","content":"I will go with ACD","comment_id":"267536","comments":[{"comment_id":"285143","timestamp":"1633328880.0","poster":"Ebi","content":"We need FIFO queue here for exactly-once-processing feature as well as order","upvote_count":"4"},{"comment_id":"367783","poster":"LCC92","content":"FIFO SQS is limited 300 message/second. CDE is correct.","upvote_count":"5","timestamp":"1634577480.0"}],"timestamp":"1632869940.0","poster":"Ebi"},{"comment_id":"902711","upvote_count":"1","content":"Selected Answer: ACD\nA,C,D. Here the problem is not the speed that SQS FIFO is less than SQS, it's the order we need to maintain. Each instance should have a record in Route 53 when it's started and shoud have this record deleted from Route 53 when the instance is terminated. You don't want to record is deleted before its creation in your environment.","poster":"Jesuisleon","timestamp":"1684605480.0"},{"upvote_count":"1","content":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/RelayEventsKinesisStream.html\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/events/CloudWatch-Events-tutorial-CloudWatch-Logs.html","comment_id":"768283","timestamp":"1673074680.0","poster":"maxh8086"},{"comment_id":"760334","content":"Selected Answer: CDE\nCDE is the answer","poster":"evargasbrz","upvote_count":"1","timestamp":"1672261260.0"},{"timestamp":"1664345580.0","content":"Selected Answer: CDE\nCDE is the answer","comment_id":"681442","poster":"JohnPi","upvote_count":"2"},{"content":"Selected Answer: ACD\nEven the default message groups support 300 requests , you can enable high performance option to improve high throughput:https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/high-throughput-fifo.html","upvote_count":"1","timestamp":"1662370020.0","poster":"aqiao","comment_id":"659990"},{"content":"Selected Answer: ACD\nACD looks right","comment_id":"577925","poster":"jj22222","upvote_count":"1","timestamp":"1648596180.0"},{"content":"If you are choosing E, then you are okay with duplicate DNS records, which means that when you delete the records you have to figure out the timestamp so that you do not delete the latest entry. Yes you do have limited throughput but thats where D comes in with batching, now you can get 3000 TPS for FIFO. So my answer will have to be A,C,D","comment_id":"555091","timestamp":"1645682520.0","poster":"jyrajan69","upvote_count":"2"},{"poster":"tkanmani76","upvote_count":"1","comment_id":"506967","timestamp":"1640168340.0","content":"Answer C, D, E - \nIf we would have gone with A, C, D - which makes sense from FIFO perspective, the option D does not mention 'SQS FIFO' instead just mentions SQS. Hence C, D, E"},{"comment_id":"491949","upvote_count":"6","poster":"AzureDP900","timestamp":"1638393660.0","content":"CDE is perfect answer, This question in Neal Davis practice test. \n\nThe errors in the Lambda logs indicate that throttling is occurring. Throttling is intended to protect your resources and downstream applications. Though Lambda automatically scales to accommodate incoming traffic, functions can still be throttled for various reasons.\n\nIn this case it is most likely that the throttling is not occurring in Lambda itself but in API calls made to Amazon Route 53. In Route 53 you are limited (by default) to five requests per second per AWS account. If you submit more than five requests per second, Amazon Route 53 returns an HTTP 400 error (Bad request). The response header also includes a Code element with a value of Throttling and a Message element with a value of Rate exceeded.\n\nThe resolution here is to place the data for the DNS records into an SQS queue where they can buffer. AWS Lambda can then poll the queue and process the messages, making sure to batch the messages to reduce the likelihood of receiving more errors."},{"content":"CDE better than ADE FIFO SQS is limited 300 message/second as commented below.","timestamp":"1636001940.0","poster":"Cotter","comment_id":"443737","upvote_count":"1"},{"timestamp":"1635922200.0","content":"It's C, D, E","poster":"andylogan","upvote_count":"1","comment_id":"443240"},{"content":"CCC DDD EEE\n---","upvote_count":"2","comment_id":"436280","poster":"tgv","timestamp":"1635091860.0"},{"poster":"WhyIronMan","upvote_count":"2","comment_id":"413280","timestamp":"1634781660.0","content":"I'll go with C,D,E"},{"content":"The Correct Answer is C D E.","upvote_count":"1","comment_id":"365386","poster":"Chibuzo1","timestamp":"1634461680.0"},{"timestamp":"1634394240.0","content":"C, D , E","poster":"vkbajoria","comment_id":"363148","upvote_count":"1"},{"content":"BDE- SQS for decoupling , no FIFO as it has limit ...kinesis is not decoupling solution","comment_id":"357303","timestamp":"1634209500.0","poster":"Santoshhhhh","upvote_count":"1"},{"content":"it's C,D,E","poster":"Waiweng","comment_id":"355311","upvote_count":"2","timestamp":"1633928940.0"},{"comment_id":"333333","content":"BCF is corrrect answer","upvote_count":"2","timestamp":"1633712580.0","poster":"anandbabu"},{"comments":[{"comment_id":"435888","content":"There is a need that you may process terminate event before instance startevent","upvote_count":"1","poster":"blackgamer","timestamp":"1635042660.0"}],"content":"I ll go with CDE. I don't see a need for order when updating DNS records.","upvote_count":"1","timestamp":"1633515900.0","comment_id":"293789","poster":"bnagaraja9099"},{"poster":"gookseang","comment_id":"278901","timestamp":"1632961320.0","content":"ACD seems","upvote_count":"3"},{"content":"was a typo..going for ACD","upvote_count":"1","comment_id":"265979","timestamp":"1632691080.0","poster":"njthomas"},{"upvote_count":"1","content":"A,D,E- Will stick to FIFO over standard where DNS records are being created and deleted.","comment_id":"256660","poster":"njthomas","timestamp":"1632407220.0"},{"upvote_count":"1","content":"C,D, E is the correct answer.","poster":"Bulti","comment_id":"255357","timestamp":"1632394320.0","comments":[{"content":"It is important to maintain the order in which the messages are received. FIFO support batching messages and by doing so the throughput increases from 300 transactions/sec to 3000 transactions/sec if you batch 10 messages per API call and process them. That should be enough to meet the requirements of this use case. And therefore I am changing my answer to A, C,E.","timestamp":"1632483540.0","poster":"Bulti","upvote_count":"1","comments":[{"comment_id":"259023","content":"sorry I meant A,C,D.","poster":"Bulti","upvote_count":"7","timestamp":"1632509100.0"},{"timestamp":"1633652820.0","upvote_count":"2","content":"Batching would work for ReceiveMessage but how CloudWatch Events would do the batching for SendMessage operations? Each event (start or stop) would be an individual API call to SQS and if that exceeds 300 per second the events would be lost.","comment_id":"330767","poster":"RedKane"}],"comment_id":"259021"}]},{"content":"Correct CED. CloudWatch--> SQS--> Lambda (batch) --> R53","timestamp":"1632320820.0","poster":"T14102020","upvote_count":"3","comment_id":"244889"},{"timestamp":"1632293040.0","poster":"jackdryan","upvote_count":"3","comment_id":"232569","content":"I'll go with C,D,E"}]},{"id":"0xBWVEtoyGRIBOanhv92","question_text":"A North American company with headquarters on the East Coast is deploying a new web application running on Amazon EC2 in the us-east-1 Region. The application should dynamically scale to meet user demand and maintain resiliency. Additionally, the application must have disaster recover capabilities in an active-passive configuration with the us-west-1 Region.\nWhich steps should a solutions architect take after creating a VPC in the us-east-1 Region?","isMC":true,"answer":"B","answer_images":[],"answer_description":"","timestamp":"2020-11-02 13:49:00","choices":{"C":"Create a VPC in the us-west-1 Region. Use inter-Region VPC peering to connect both VPCs. Deploy an Application Load Balancer (ALB) that spans both VPCs. Deploy EC2 instances across multiple Availability Zones as part of an Auto Scaling group in each VPC served by the ALB. Create an Amazon Route 53 record that points to the ALB.","A":"Create a VPC in the us-west-1 Region. Use inter-Region VPC peering to connect both VPCs. Deploy an Application Load Balancer (ALB) spanning multiple Availability Zones (AZs) to the VPC in the us-east-1 Region. Deploy EC2 instances across multiple AZs in each Region as part of an Auto Scaling group spanning both VPCs and served by the ALB.","D":"Deploy an Application Load Balancer (ALB) spanning multiple Availability Zones (AZs) to the VPC in the us-east-1 Region. Deploy EC2 instances across multiple AZs as part of an Auto Scaling group served by the ALB. Deploy the same solution to the us-west-1 Region. Create separate Amazon Route 53 records in each Region that point to the ALB in the Region. Use Route 53 health checks to provide high availability across both Regions.","B":"Deploy an Application Load Balancer (ALB) spanning multiple Availability Zones (AZs) to the VPC in the us-east-1 Region. Deploy EC2 instances across multiple AZs as part of an Auto Scaling group served by the ALB. Deploy the same solution to the us-west-1 Region. Create an Amazon Route 53 record set with a failover routing policy and health checks enabled to provide high availability across both Regions."},"answers_community":["B (100%)"],"answer_ET":"B","url":"https://www.examtopics.com/discussions/amazon/view/35775-exam-aws-certified-solutions-architect-professional-topic-1/","question_id":604,"exam_id":32,"discussion":[{"content":"I support B.\nA new web application in a active-passive DR mode.\na Route 53 record set with a failover routing policy.","comment_id":"211220","poster":"porlarowl","upvote_count":"28","timestamp":"1632423300.0"},{"timestamp":"1662372060.0","poster":"aqiao","content":"Selected Answer: B\nALB can not cross region, so A and C rule out. Route 53 is a global service but not region, D rules out","upvote_count":"2","comment_id":"660020"},{"content":"Selected Answer: B\nb looks right","upvote_count":"1","timestamp":"1648562940.0","poster":"jj22222","comment_id":"577656"},{"upvote_count":"1","timestamp":"1646470920.0","comment_id":"561320","poster":"pal40sg","content":"Selected Answer: B\nA new web application in a active-passive DR mode.\na Route 53 record set with a failover routing policy."},{"upvote_count":"1","content":"Selected Answer: B\nIt is definitely B","poster":"shotty1","timestamp":"1643132880.0","comment_id":"532308"},{"timestamp":"1641901020.0","upvote_count":"1","comment_id":"521493","poster":"pititcu667","content":"Selected Answer: B\ncomes down to the route53 being a global service. i initially voted d."},{"comment_id":"491950","poster":"AzureDP900","timestamp":"1638393900.0","content":"Selected Answer: B\nB Correct answer","upvote_count":"1"},{"upvote_count":"2","poster":"kirrim","content":"There is no reason for the two regions to intercommunicate with each other, so I see no need for the inter-Region peering. That rules out A and C.\n\nBetween B vs D... Route53 doesn't have per-region records. It's a global service. So D is wrong. B should work great.","comment_id":"462263","timestamp":"1636290420.0"},{"upvote_count":"2","content":"It's B with failover routing policy","timestamp":"1636176120.0","comment_id":"443242","poster":"andylogan"},{"poster":"tgv","content":"BBB\n---","timestamp":"1636126920.0","upvote_count":"1","comment_id":"436279"},{"poster":"blackgamer","timestamp":"1636045320.0","comment_id":"433282","content":"Definitely B.","upvote_count":"1"},{"poster":"WhyIronMan","upvote_count":"2","timestamp":"1636006260.0","comment_id":"413282","content":"I'll go with B"},{"timestamp":"1635970020.0","upvote_count":"3","comment_id":"354641","content":"ir's B","poster":"Waiweng"},{"comment_id":"327240","upvote_count":"2","content":"D Did not mention the routing policy to be used on Amazon Route 53. The question requires that the second region acts as a passive backup, which means only the main region receives all the traffic so you need to specifically use failover routing policy in Amazon Route 53. So B is correct as per requirement","timestamp":"1635915540.0","poster":"ExtHo"},{"timestamp":"1635743280.0","content":"i choose B","upvote_count":"2","poster":"alisyech","comment_id":"321739"},{"content":"going with B","comment_id":"293328","timestamp":"1635336600.0","poster":"Kian1","upvote_count":"2"},{"poster":"Ebi","upvote_count":"4","comment_id":"267546","timestamp":"1635136800.0","content":"I will go with B"},{"comment_id":"266059","upvote_count":"2","timestamp":"1635059340.0","content":"B. R53 failover routing poilcy for DR","poster":"rkbala"},{"content":"Correct answer is B. Failover routing policy is the key.","comment_id":"255358","poster":"Bulti","upvote_count":"1","timestamp":"1635026640.0"},{"upvote_count":"1","content":"Correct is B. a failover routing policy","poster":"T14102020","comment_id":"244901","timestamp":"1634529420.0"},{"comment_id":"236620","poster":"Kelvin1477","upvote_count":"1","timestamp":"1634077980.0","content":"Should be B"},{"comment_id":"232571","upvote_count":"3","timestamp":"1634012280.0","poster":"jackdryan","content":"I'll go with B"},{"upvote_count":"2","timestamp":"1633191360.0","content":"it should be B.","comment_id":"216057","poster":"Rajarshi"},{"upvote_count":"1","timestamp":"1633078560.0","comment_id":"215031","comments":[{"comments":[{"upvote_count":"3","comment_id":"243082","timestamp":"1634267760.0","content":"No! Route53 is a global service. You can't create separate R53 records in each region.","poster":"darthvoodoo"}],"timestamp":"1633255560.0","poster":"cloudgc","content":"as there is no clarity on the R53 routing policy being used. creating the record sets alone will not work.","upvote_count":"1","comment_id":"232433"}],"content":"Why not D","poster":"Ash1235"},{"poster":"bbnbnuyh","timestamp":"1632672060.0","upvote_count":"3","comment_id":"211695","content":"B. Keywords Active-Passive failover for DR. We dont need ALBs to span across VPCs using peering"}],"question_images":[],"unix_timestamp":1604321340,"topic":"1"},{"id":"HpmvdHW9kHJmSClaCKnM","topic":"1","exam_id":32,"timestamp":"2021-12-30 11:23:00","choices":{"A":"Create Cloud Formation templates and re-use parts of the Python scripts as Instance user data. Use the AWS Cloud Development Kit (AWS CDK) to deploy the application using these templates. Incorporate the AWS CDK into CodePipeline and deploy the application to AWS using these templates.","C":"Standardize on AWS OpsWorks. Integrate OpsWorks with CodePipeline. Have the developers create Chef recipes to deploy their applications on AWS.","B":"Use a third-party resource provisioning engine inside AWS CodeBuild to standardize the deployment processes of the existing and acquired company. Orchestrate the CodeBuild job using CodePipeline.","D":"Define the AWS resources using TypeScript or Python. Use the AWS Cloud Development Kit (AWS CDK) to create CloudFormation templates from the developers' code, and use the AWS CDK to create CloudFormation stacks. Incorporate the AWS CDK as a CodeBuild job in CodePipeline."},"question_id":605,"discussion":[{"comment_id":"540698","content":"D as it lets developers use their skills","poster":"AMKazi","upvote_count":"6","timestamp":"1644017700.0"},{"upvote_count":"5","comment_id":"554863","content":"Selected Answer: D\nanswer should be d","timestamp":"1645648620.0","poster":"pititcu667"},{"upvote_count":"3","poster":"Rocky2222","comment_id":"644721","content":"Selected Answer: D\nWith this solution, the developers no longer need to learn the AWS CloudFormation specific language as they can continue writing TypeScript or Python scripts. The AWS CDK stacks can be converted to AWS CloudFormation templates which can be integrated into the company deployment process.","timestamp":"1660090800.0"},{"comments":[{"upvote_count":"1","timestamp":"1659120300.0","content":"when you have a hammer everything looks like a nail ;-) It's D","comment_id":"639368","poster":"fdoxxx"},{"upvote_count":"1","timestamp":"1670239080.0","comment_id":"735873","content":"In real world but not in AWS exam :)\nHere D is the answer.","poster":"SureNot"}],"poster":"adsdadasdad","upvote_count":"3","content":"Selected Answer: B\nPEOPLE TERRAFORM. Its B","timestamp":"1657692000.0","comment_id":"630790"},{"timestamp":"1654715880.0","upvote_count":"2","content":"It is D.","poster":"hilft","comment_id":"613483"},{"upvote_count":"3","comment_id":"521756","content":"Agree its D based on this link","poster":"Ni_yot","timestamp":"1641932640.0"},{"comment_id":"513239","timestamp":"1640859780.0","upvote_count":"3","poster":"krisvija12","content":"Answer should be : D \nRef : https://docs.aws.amazon.com/cdk/v2/guide/home.html"}],"question_text":"A company standardized its method of deploying applications to AWS using AWS CodePipeline and AWS CloudFormation. The applications are in TypeScript and\nPython. The company has recently acquired another business that deploys applications to AWS using Python scripts.\nDevelopers from the newly acquired company are hesitant to move their applications under CloudFormation because it would require that they learn a new domain-specific language and eliminate their access to language features, such as looping.\nHow can the acquired applications quickly be brought up to deployment standards while addressing the developers' concerns?","answer":"D","answer_ET":"D","answers_community":["D (73%)","B (27%)"],"url":"https://www.examtopics.com/discussions/amazon/view/69042-exam-aws-certified-solutions-architect-professional-topic-1/","answer_description":"","unix_timestamp":1640859780,"isMC":true,"question_images":[],"answer_images":[]}],"exam":{"isImplemented":true,"isBeta":false,"id":32,"name":"AWS Certified Solutions Architect - Professional","lastUpdated":"11 Apr 2025","isMCOnly":false,"numberOfQuestions":1019,"provider":"Amazon"},"currentPage":121},"__N_SSP":true}