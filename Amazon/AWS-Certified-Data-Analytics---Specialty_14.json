{"pageProps":{"questions":[{"id":"UXZpn5GQ8EDhXyf5Pmpq","question_id":66,"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/74259-exam-aws-certified-data-analytics-specialty-topic-1-question/","unix_timestamp":1650735900,"answer_ET":"AD","question_images":[],"answers_community":["AD (100%)"],"question_text":"A manufacturing company is storing data from its operational systems in Amazon S3. The company's business analysts need to perform one-time queries of the data in Amazon S3 with Amazon Athena. The company needs to access the Athena network from the on-premises network by using a JDBC connection. The company has created a VPC Security policies mandate that requests to AWS services cannot traverse the Internet.\nWhich combination of steps should a data analytics specialist take to meet these requirements? (Choose two.)","isMC":true,"exam_id":20,"timestamp":"2022-04-23 19:45:00","choices":{"A":"Establish an AWS Direct Connect connection between the on-premises network and the VPC.","E":"Deploy Athena within a private subnet.","D":"Configure the JDBC connection to use an interface VPC endpoint for Athena.","B":"Configure the JDBC connection to connect to Athena through Amazon API Gateway.","C":"Configure the JDBC connection to use a gateway VPC endpoint for Amazon S3."},"answer_images":[],"answer":"AD","topic":"1","discussion":[{"comment_id":"590735","timestamp":"1650735900.0","content":"AD looks right\n\nhttps://docs.aws.amazon.com/athena/latest/ug/interface-vpc-endpoint.html","upvote_count":"9","poster":"CHRIS12722222"},{"poster":"Ali_Hussein","timestamp":"1692541920.0","content":"Selected Answer: AD\nThe correct answers are A and D.\n\n Option B is incorrect because Amazon API Gateway is a public facing service and requests to it will traverse the internet.\n Option C is incorrect because a gateway VPC endpoint for Amazon S3 allows you to connect to Amazon S3 from within your VPC, but it does not allow you to connect to Athena.\n\nThe following are the explanations for the correct answers:\n\n Option A: An AWS Direct Connect connection is a dedicated network connection between your on-premises network and AWS. This will allow you to connect to Athena from your on-premises network without traversing the internet.\n Option D: An interface VPC endpoint for Athena is a private connection to Athena that is created within your VPC. This will allow you to connect to Athena from your VPC without traversing the internet.","upvote_count":"3","comment_id":"985862"},{"comment_id":"886686","timestamp":"1682970600.0","poster":"pk349","upvote_count":"1","content":"AD: I passed the test"},{"comment_id":"846083","timestamp":"1679410200.0","poster":"AwsNewPeople","upvote_count":"2","content":"Selected Answer: AD\nA. Establish an AWS Direct Connect connection between the on-premises network and the VPC, which provides a dedicated network connection between the on-premises network and the VPC, avoiding the need to traverse the public internet.\n\nD. Configure the JDBC connection to use an interface VPC endpoint for Athena, which enables private connectivity from the VPC to Athena without traversing the internet. The VPC endpoint can be configured to use a security group to control access to the endpoint."},{"timestamp":"1674918540.0","upvote_count":"1","content":"https://docs.aws.amazon.com/athena/latest/ug/interface-vpc-endpoint.html","poster":"austinoy","comment_id":"790698"},{"comment_id":"637120","timestamp":"1658807040.0","upvote_count":"3","poster":"rocky48","content":"Selected Answer: AD\nSelected Answer: AD"},{"comment_id":"617419","timestamp":"1655418240.0","upvote_count":"2","content":"Selected Answer: AD\nEstablish an AWS Direct Connect connection between the on-premises network and the VPC.\n Configure the JDBC connection to use an interface VPC endpoint for Athena.","poster":"samsanta2012"}]},{"id":"8JNZ2PnsINdGnZGqqt9o","answers_community":["C (100%)"],"question_images":[],"exam_id":20,"answer_ET":"C","choices":{"C":"Create an Amazon QuickSight analysis by using the data in Amazon Redshift. Add a forecasting widget Publish the analysis as a dashboard.","D":"Create an Amazon SageMaker model for forecasting. Integrate the model with an Amazon QuickSight dataset. Create a widget for the dataset. Publish the analysis as a dashboard.","B":"Create a JavaScript dashboard by using D3.js charts and the data in Amazon Redshift. Export the data to Amazon SageMaker. Run a Python script to run a regression model to forecast revenue. Import the data back into Amazon Redshift. Add the new forecast information to the dashboard.","A":"Create an Amazon QuickSight analysis by using the data in Amazon Redshift. Add a custom field in QuickSight that applies a linear regression function to the data. Publish the analysis as a dashboard."},"discussion":[{"timestamp":"1650736320.0","comment_id":"590737","content":"C \nhttps://docs.aws.amazon.com/quicksight/latest/user/forecasts-and-whatifs.html","upvote_count":"9","poster":"CHRIS12722222"},{"poster":"pk349","upvote_count":"1","comment_id":"886690","timestamp":"1682970720.0","content":"C: I passed the test"},{"poster":"rocky48","comment_id":"633834","timestamp":"1658291580.0","content":"Selected Answer: C\nOption C","upvote_count":"2"},{"timestamp":"1653206160.0","content":"Selected Answer: C\nC should be Correct","upvote_count":"2","comment_id":"605271","poster":"Bik000"},{"upvote_count":"3","comment_id":"589229","comments":[{"timestamp":"1650793500.0","upvote_count":"1","poster":"astalavista1","comment_id":"590968","content":"Agree C is the most-cost effective."}],"timestamp":"1650532500.0","content":"Selected Answer: C\nC, per https://docs.aws.amazon.com/quicksight/latest/user/forecasts-and-whatifs.html","poster":"rb39"}],"timestamp":"2022-04-21 11:15:00","question_text":"A company stores revenue data in Amazon Redshift. A data analyst needs to create a dashboard so that the company's sales team can visualize historical revenue and accurately forecast revenue for the upcoming months.\nWhich solution will MOST cost-effectively meet these requirements?","answer":"C","topic":"1","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/73999-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer_description":"","isMC":true,"question_id":67,"unix_timestamp":1650532500},{"id":"kc34Y1yaND4BHSMVdGu3","answers_community":["B (100%)"],"question_images":[],"exam_id":20,"answer_ET":"B","choices":{"D":"Edit the permissions for the new S3 bucket from within the S3 console.","B":"Edit the permissions for the new S3 bucket from within the Amazon QuickSight console.","A":"Edit the permissions for the AWS Glue Data Catalog from within the Amazon QuickSight console.","C":"Edit the permissions for the AWS Glue Data Catalog from within the AWS Glue console."},"discussion":[{"timestamp":"1632172440.0","comments":[{"timestamp":"1640052000.0","comment_id":"505775","content":"For other issues please read - https://docs.aws.amazon.com/quicksight/latest/user/troubleshoot-athena.html","upvote_count":"1","poster":"lakediver"}],"poster":"cloud4gr8","comment_id":"159567","content":"B. \nhttps://docs.aws.amazon.com/quicksight/latest/user/troubleshoot-athena-insufficient-permissions.html","upvote_count":"29"},{"comment_id":"1043534","timestamp":"1697294640.0","poster":"gofavad926","content":"Selected Answer: B\nB. https://docs.aws.amazon.com/quicksight/latest/user/troubleshoot-connect-S3.html","upvote_count":"1"},{"poster":"nroopa","comment_id":"989740","upvote_count":"1","content":"Quick sight already has access to AWS Glue Data Catalog So A and C are not valid.\nThe issue is Quick sight does not have access to new S3 bucket, so we need to Edit the permissions for the new S3 bucket from within the S3 console to Give access to Quick sight\nSo Option D","comments":[{"content":"So its B . https://docs.aws.amazon.com/quicksight/latest/user/troubleshoot-connect-S3.html","poster":"nroopa","timestamp":"1692946560.0","comment_id":"989742","upvote_count":"2"}],"timestamp":"1692946320.0"},{"upvote_count":"1","timestamp":"1690906200.0","content":"Selected Answer: B\nits a b","poster":"NikkyDicky","comment_id":"969196"},{"comment_id":"886280","timestamp":"1682947020.0","poster":"pk349","upvote_count":"2","content":"B: I passed the test"},{"comment_id":"678418","upvote_count":"2","poster":"Arka_01","content":"Selected Answer: B\nAs new S3 bucket is added. The permission to this bucket needs to be added from QuickSight console.","timestamp":"1664080500.0"},{"upvote_count":"1","timestamp":"1658380920.0","content":"Selected Answer: B\nSelected Answer: B","poster":"rocky48","comment_id":"634373"},{"upvote_count":"1","comment_id":"631611","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/quicksight/latest/user/troubleshoot-connect-athena.html","poster":"ru4aws","timestamp":"1657859400.0"},{"poster":"ahmed_maher","timestamp":"1654369020.0","upvote_count":"1","comment_id":"611547","content":"B is right"},{"content":"Answer : B","timestamp":"1651349760.0","comment_id":"595259","poster":"jrheen","upvote_count":"1"},{"timestamp":"1637356740.0","upvote_count":"1","content":"B is right","comment_id":"482103","poster":"aws2019"},{"poster":"keitahigaki","timestamp":"1636146660.0","content":"The new Quicksight dashboard doesn't have S3 access. From within the S3 console, edit the new bucket permissions.\nhttps://docs.aws.amazon.com/ja_jp/quicksight/latest/user/create-a-data-set-athena.html","comment_id":"428169","upvote_count":"1"},{"poster":"Huy","content":"Agree with B. D is possible if the bucket policy has a Deny rule with Quicksight service role but it is rarely. https://aws.amazon.com/premiumsupport/knowledge-center/quicksight-deny-policy-allow-bucket/","upvote_count":"2","comment_id":"387718","timestamp":"1635810600.0"},{"timestamp":"1635111600.0","poster":"Shraddha","upvote_count":"2","content":"Answer B. A and C = wrong, Glue is updated without error so no problem there, also Glue permission is not set in QuickSight, QuickSight connects to Athena, not via Glue. D = wrong, QuickSight will create the rules for you.\n\nNote: Athena is a very strange service as it transparently uses user’s access to S3 buckets, instead of relying on service roles like most other AWS services. So, to be able to use Athena, the user itself will need to have S3 access, there is no service role creation for Athena.\n\nhttps://docs.aws.amazon.com/quicksight/latest/user/troubleshoot-athena-insufficient-permissions.html","comment_id":"383544"},{"poster":"sayed","timestamp":"1634985600.0","content":"B as per the below\nIf you need use Amazon QuickSight with Amazon Athena or Amazon Athena Federated Query, you first need to authorize connections to Athena and the associated buckets in Amazon Simple Storage Service (Amazon S3). \nhttps://docs.aws.amazon.com/quicksight/latest/user/athena.html","upvote_count":"1","comment_id":"297251"},{"timestamp":"1633957380.0","poster":"Pruthvi","content":"B - \nAthena is able to access the data source so its not S3 problem. On the quick sight end the new bucket should be added \nhttps://docs.aws.amazon.com/quicksight/latest/user/troubleshoot-connect-S3.html","comment_id":"277534","upvote_count":"2"},{"upvote_count":"1","content":"B is the right answer","poster":"lostsoul07","timestamp":"1633744260.0","comment_id":"274255"},{"poster":"lydia_young","upvote_count":"2","timestamp":"1633635120.0","comment_id":"252497","content":"B is correct!"},{"comment_id":"249085","timestamp":"1632741060.0","content":"Answer is D, because S3 needs to provide the access to Quicksight","upvote_count":"1","poster":"hoty"},{"timestamp":"1632733140.0","upvote_count":"1","poster":"SumaD2020","comment_id":"243121","content":"B is the correct answer as Quick sight has an option of selecting the S3 bucket that you need to access."},{"content":"B is correct!","poster":"BillyC","upvote_count":"1","timestamp":"1632699480.0","comment_id":"216818"},{"content":"B is the right answer.","poster":"sanjaym","upvote_count":"1","comment_id":"204683","timestamp":"1632641580.0"},{"upvote_count":"2","comment_id":"198481","content":"Answer D is correct. Please refer - https://docs.aws.amazon.com/quicksight/latest/user/create-a-data-set-athena.html","timestamp":"1632608760.0","poster":"sagdal"},{"poster":"Karan_Sharma","comment_id":"177483","content":"Option B, the key here is Athena source is updated to have the new S3 path but the problem is with Quicksight not having the new bucket reference","timestamp":"1632496620.0","upvote_count":"3"},{"comments":[{"upvote_count":"6","timestamp":"1632451260.0","poster":"Paitan","comments":[{"content":"You are right. It should be B","comment_id":"177753","timestamp":"1632592920.0","upvote_count":"2","poster":"GauravM17"}],"content":"The question says\"After updating the catalog to include the new application data source...\". So the S3 permission is not the issue over here. The issue is with Quicksight not including the new bucket, hence option B is the right option.","comment_id":"177087"}],"comment_id":"176254","content":"should D be the answer?","timestamp":"1632297780.0","upvote_count":"1","poster":"GauravM17"}],"timestamp":"2020-08-17 04:17:00","question_text":"A data analyst is using Amazon QuickSight for data visualization across multiple datasets generated by applications. Each application stores files within a separate Amazon S3 bucket. AWS Glue Data Catalog is used as a central catalog across all application data in Amazon S3. A new application stores its data within a separate S3 bucket. After updating the catalog to include the new application data source, the data analyst created a new Amazon QuickSight data source from an Amazon Athena table, but the import into SPICE failed.\nHow should the data analyst resolve the issue?","answer":"B","answer_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/28772-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer_description":"","isMC":true,"question_id":68,"unix_timestamp":1597630620},{"id":"v4TYFKJR4YN5bSIL3cey","url":"https://www.examtopics.com/discussions/amazon/view/73997-exam-aws-certified-data-analytics-specialty-topic-1-question/","choices":{"A":"Modify the Lambda function to upload the query response payload as an object into the S3 bucket. Include an S3 object presigned URL as the payload in the Lambda function response.","C":"Create a separate folder in the S3 bucket. Move the data files that need to be queried into that folder. Create an AWS Glue crawler that points to the folder instead of the S3 bucket.","D":"Check the schema of the queried table for any characters that Athena does not support. Replace any unsupported characters with characters that Athena supports.","B":"Run the MSCK REPAIR TABLE command on the queried table."},"exam_id":20,"question_text":"A company is using an AWS Lambda function to run Amazon Athena queries against a cross-account AWS Glue Data Catalog. A query returns the following error:\n\nHIVE_METASTORE_ERROR -\nThe error message states that the response payload size exceeds the maximum allowed size. The queried table is already partitioned, and the data is stored in an\nAmazon S3 bucket in the Apache Hive partition format.\nWhich solution will resolve this error?","answers_community":["A (100%)"],"topic":"1","answer_images":[],"question_images":[],"timestamp":"2022-04-21 11:04:00","answer_description":"","question_id":69,"answer":"A","discussion":[{"comment_id":"589224","timestamp":"1650531840.0","poster":"rb39","upvote_count":"12","content":"Selected Answer: A\nA from https://aws.amazon.com/premiumsupport/knowledge-center/athena-hive-metastore-error/"},{"comments":[{"upvote_count":"1","comment_id":"851196","timestamp":"1679843820.0","poster":"CC_DC","content":"Article says, \"If your table is partitioned, and your use case permits, query only the specific partition.\" This is not one of the 4 options. So the simple thing is maybe the use case does not permit querying the specific partition since it is not explicitly or implicitly mentioned in the question. The article also does not mention running MSCK."}],"upvote_count":"6","poster":"somenath","timestamp":"1653668640.0","comment_id":"608149","content":"I think it is B, per the link https://aws.amazon.com/premiumsupport/knowledge-center/athena-hive-metastore-error/\nIt says, If your table is already partitioned, and the data is loaded in Amazon Simple Storage Service (Amazon S3) Hive partition format, then load the partitions by running a command MSCK REPAIR TABLE table_name"},{"content":"Lambda is limitted 6MB response.\nso this error occuerd.\nCounterplan is using pre shared URL (S3).\nLambda dosent back data directly to frontend. \nFrontend can access to data by pre shared URL(S3).","comment_id":"1063474","poster":"LocalHero","timestamp":"1699238040.0","upvote_count":"1"},{"timestamp":"1682970780.0","comment_id":"886691","content":"A: I passed the test","upvote_count":"2","poster":"pk349"},{"poster":"Chelseajcole","comment_id":"773842","timestamp":"1673552580.0","content":"Selected Answer: A\nHIVE_METASTORE_ERROR: com.facebook.presto.spi.PrestoException: java.io.IOException: Response payload size (11112222 bytes) exceeded maximum allowed payload size (6291556 bytes)\nTo resolve this error, choose one or more of the following solutions:\n\nUpload the Lambda function's response payload as an object into an Amazon S3 bucket. Then, include this object as payload in the Lambda function response. For information on generating a presigned URL for your object, see Sharing an object with a presigned URL.\nIf your table is partitioned, and your use case permits, query only the specific partition.\nSpecify the spill location in Amazon S3 when you create the Lambda function. Responses that are larger than the threshold spill into the specified S3 location.","upvote_count":"2"},{"upvote_count":"1","content":"Selected Answer: A\nhttps://aws.amazon.com/premiumsupport/knowledge-center/athena-hive-metastore-error/","poster":"bp339","comment_id":"694364","timestamp":"1665705480.0"},{"content":"Selected Answer: A\nAnswer is A.","upvote_count":"4","poster":"rocky48","comment_id":"643516","timestamp":"1659817500.0"},{"comment_id":"608151","upvote_count":"5","poster":"somenath","timestamp":"1653669120.0","content":"If the full error is HIVE_METASTORE_ERROR: com.facebook.presto.spi.PrestoException: Required Table Storage Descriptor is not populated: Then it is B.\nIf the full error is \nHIVE_METASTORE_ERROR: com.facebook.presto.spi.PrestoException: java.io.IOException: Response payload size (11112222 bytes) exceeded maximum allowed payload size\nThen the answer is A."}],"unix_timestamp":1650531840,"answer_ET":"A","isMC":true},{"id":"v4Z1XK2tw78FjCpkSiEI","isMC":true,"choices":{"D":"Use Amazon Kinesis Data Firehose to receive the data from the sensors. Use an AWS Lambda function to aggregate the data during capture. Store the data in Amazon S3.","B":"Use Amazon Kinesis Data Firehose to receive the data from the sensors. Use Amazon Kinesis Data Analytics to aggregate the data. Use an AWS Lambda function to read the data from Kinesis Data Analytics and store the data in Amazon S3.","C":"Use Amazon Kinesis Data Firehose to receive the data from the sensors. Use an AWS Lambda function to aggregate the data during capture. Store the data from Kinesis Data Firehose in Amazon DynamoDB.","A":"Use Amazon Kinesis Data Streams to receive the data from the sensors. Use Amazon Kinesis Data Analytics to read the stream, aggregate the data, and send the data to an AWS Lambda function. Configure the Lambda function to store the data in Amazon DynamoDB."},"question_images":[],"answer_description":"","question_text":"A machinery company wants to collect data from sensors. A data analytics specialist needs to implement a solution that aggregates the data in near-real time and saves the data to a persistent data store. The data must be stored in nested JSON format and must be queried from the data store with a latency of single-digit milliseconds.\nWhich solution will meet these requirements?","answer":"A","answer_images":[],"topic":"1","question_id":70,"answers_community":["A (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/73996-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer_ET":"A","exam_id":20,"unix_timestamp":1650531720,"timestamp":"2022-04-21 11:02:00","discussion":[{"poster":"Teraxs","comment_id":"594211","content":"Selected Answer: A\nsingle-digit milliseconds --> DynamoDB\nFirehose cannot store directly to DynamoDB, Lambda can\nhttps://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html","timestamp":"1651206180.0","upvote_count":"11"},{"upvote_count":"2","timestamp":"1682970840.0","content":"A: I passed the test","poster":"pk349","comment_id":"886692"},{"comment_id":"856426","content":"Answer: B ; device --> PutRecord --> Firehose --> KDA --> Lambda -->S3. Kinesis data analytics can only write to lambda, data streams or firehose.","timestamp":"1680219360.0","poster":"thirstylion","upvote_count":"2"},{"poster":"Merrick","timestamp":"1676435460.0","comment_id":"809093","content":"A over C - Lambda function not to aggregate but to store the data in Amazon DynamoDB.","upvote_count":"1"},{"content":"The answer is C - KDS cant directly connect to source ,, answer C is firehose - near real time and DyanmoDB to save nested json strcture with single digit millisecond requirmeent.","poster":"Arjun777","timestamp":"1676140140.0","comment_id":"805519","comments":[{"poster":"VijayAmit","timestamp":"1677506940.0","upvote_count":"3","content":"Ans is A. Firehose can't write directly to DynamoDb","comment_id":"823793"}],"upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: A\nAnswer is A","timestamp":"1672533660.0","comment_id":"762996","poster":"Kapil_KK"},{"timestamp":"1671114780.0","poster":"nadavw","comment_id":"746188","upvote_count":"1","content":"Selected Answer: A\nA is The only answer which combines KDA (analytics) for aggregation and DynamoDB for single-digit latency. Lambda is the way to persist and not to compute. \nhttps://docs.aws.amazon.com/kinesisanalytics/latest/dev/how-it-works-output-lambda.html"},{"content":"Selected Answer: A\nAnswer - A","comment_id":"634916","timestamp":"1658451540.0","poster":"rocky48","upvote_count":"1"},{"content":"The answer is C, kinesis data streams is for realtime, it should be kinesis firehose","timestamp":"1657057500.0","upvote_count":"1","comment_id":"627622","poster":"jealbave","comments":[{"comment_id":"635732","content":"Firehose cannot store directly to DynamoDB","upvote_count":"4","timestamp":"1658606640.0","poster":"Richie1217"}]},{"content":"Answer - A","upvote_count":"1","comment_id":"595287","poster":"jrheen","timestamp":"1651352160.0"},{"upvote_count":"3","timestamp":"1650927660.0","comments":[{"comment_id":"593861","timestamp":"1651154940.0","poster":"CHRIS12722222","upvote_count":"1","content":"You need lambda"},{"poster":"dushmantha","comment_id":"641174","upvote_count":"4","timestamp":"1659429780.0","content":"Firehose cant send data to DynamoDB"}],"content":"I vote for C","poster":"AWSRanger","comment_id":"591992"},{"timestamp":"1650531720.0","comment_id":"589221","poster":"rb39","upvote_count":"3","content":"Selected Answer: A\nsingle-digit performance => DynamoDb. You need a lambda to store data to S3, Firehose doesn't store it natively"}]}],"exam":{"id":20,"isBeta":false,"numberOfQuestions":164,"provider":"Amazon","isMCOnly":true,"lastUpdated":"11 Apr 2025","name":"AWS Certified Data Analytics - Specialty","isImplemented":true},"currentPage":14},"__N_SSP":true}