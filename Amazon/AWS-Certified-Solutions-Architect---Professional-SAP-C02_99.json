{"pageProps":{"questions":[{"id":"XMyfbhyM5fQ1Y7jUXGFH","answer_images":[],"exam_id":33,"isMC":true,"answer_description":"","timestamp":"2023-01-14 15:19:00","answer":"B","discussion":[{"poster":"masetromain","content":"Selected Answer: B\nThe correct answer is B. AWS Control Tower provides a set of \"strongly recommended guardrails\" that can be enabled to implement governance and policy enforcement. One of these guardrails is \"Encrypt Amazon RDS instances\" which will detect RDS DB instances that are not encrypted at rest. By enabling this guardrail and applying it to the production OU, the company will be able to enforce encryption for RDS instances in the production environment.\n\nOption A is incorrect because mandatory guardrails are pre-defined by AWS and cannot be customized.\nOption C is incorrect because AWS Config does not provide mandatory guardrails for RDS instances.\nOption D is incorrect because AWS Control Tower does not provide a feature called custom SCP (Service Control Policy), it uses guardrails instead.","comment_id":"775495","timestamp":"1673705940.0","upvote_count":"19"},{"comments":[{"timestamp":"1675025460.0","comment_id":"792053","poster":"Musk","content":"The only thing is that this option talks about guardrails, while the article talks about controls, not mandatory.","upvote_count":"1"}],"upvote_count":"5","comment_id":"787745","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/controltower/latest/userguide/strongly-recommended-controls.html#disallow-rds-storage-unencrypted","poster":"pitakk","timestamp":"1674657480.0"},{"content":"Selected Answer: B\nGuardrails are now called Controls in Control Tower.","poster":"pk0619","upvote_count":"1","timestamp":"1734553620.0","comment_id":"1328700"},{"content":"B. Enable the appropriate guardrail from the list of strongly recommended guardrails in AWS Control Tower. Apply the guardrail to the production OU.","timestamp":"1725095340.0","poster":"amministrazione","comment_id":"1275519","upvote_count":"1"},{"timestamp":"1719830880.0","poster":"AloraCloud","upvote_count":"1","comment_id":"1240110","content":"The keyword in the question is detect which indicates Config.\n\n \"The company must implement a policy that will detect Amazon RDS DB instances that are not encrypted at rest in the company’s production OU.\""},{"upvote_count":"1","poster":"8608f25","content":"Selected Answer: B\nOption B is correct because AWS Control Tower’s strongly recommended guardrails include checks for best practices and additional security measures that are not enforced by default but are highly recommended. Among these, there is likely a guardrail that can detect unencrypted RDS DB instances, aligning with the company’s requirement. Applying this guardrail to the production OU will ensure that all RDS DB instances in that OU are checked for encryption at rest.","timestamp":"1707578880.0","comment_id":"1146405"},{"content":"Selected Answer: B\nA = Mandatory controls are owned by AWS Control Tower, and they apply by default to every OU on your landing zone and they can't be deactivated\nB = correct https://docs.aws.amazon.com/controltower/latest/userguide/strongly-recommended-controls.html#disallow-rds-storage-unencrypted\nC = You cannot create new mandatory controls as they are owned by AWS Control Tower\nD = You can create custom SCP in AWS Control Tower as part of the Customizations for AWS Control Tower https://docs.aws.amazon.com/controltower/latest/userguide/cfcn-set-up-custom-scps.html However this requires a lot of work","timestamp":"1704469740.0","poster":"ninomfr64","upvote_count":"3","comment_id":"1114620","comments":[{"upvote_count":"3","poster":"ninomfr64","comment_id":"1114623","timestamp":"1704469980.0","content":"Note on D, the question is asking to detect and not to mandate, thus D would not meet requirement"}]},{"content":"Selected Answer: B\ncheck masetromain's comment","poster":"severlight","comment_id":"1070019","timestamp":"1699940220.0","upvote_count":"1"},{"comment_id":"953316","content":"A. No, because mandatory controls are owned by AWS Control Tower, and they apply to every OU on your landing zone. These controls are applied by default when you set up your landing zone, and they can't be deactivated. Moreover, none of them address RDS encrypted at rest. \n\nB. Yes, because Strongly recommended controls are owned by AWS Control Tower. They are based on best practices for well-architected multi-account environments. These controls are not enabled by default, and they can be deactivated through the AWS Control Tower console or the control APIs. Moreover, three of them are RDS detective controls\n\nC. No, because AWS Config does not create mandatory guardrails; AWS Config has managed and custom rules\n\nD. No, because SCPs are created in AWS Orgs and are not designed to detect Amazon RDS DB instances that are not encrypted at rest.","poster":"dkx","upvote_count":"4","timestamp":"1689511500.0"},{"poster":"NikkyDicky","comment_id":"941128","content":"Selected Answer: B\nIt's. B","timestamp":"1688319660.0","upvote_count":"1"},{"content":"Selected Answer: B\nA seems but previous exist rule\nthen B is more apropiate in this case \nhttps://docs.aws.amazon.com/controltower/latest/userguide/strongly-recommended-controls.html#disallow-rds-storage-unencrypted","timestamp":"1687102140.0","poster":"SkyZeroZx","comment_id":"926768","upvote_count":"1"},{"upvote_count":"2","timestamp":"1685526780.0","poster":"EricZhang","comment_id":"911056","content":"C - using AWS Config for detective action"},{"comments":[{"content":"It's incorrect ideally you only apply to the OU and not to an individual account, therefore this needs to be discounted.","comment_id":"870602","upvote_count":"1","poster":"passthatexam1","timestamp":"1681531740.0"}],"comment_id":"864477","upvote_count":"3","timestamp":"1680936960.0","content":"Selected Answer: C\nOption B suggests enabling an appropriate guardrail from the list of strongly recommended guardrails in AWS Control Tower and applying it to the production OU. While AWS Control Tower provides a set of pre-packaged guardrails that enforce best practices for security, operations, and compliance, there is no guarantee that there is a pre-packaged guardrail specifically for detecting Amazon RDS DB instances that are not encrypted at rest.\n\nIn contrast, option C creates a custom rule in AWS Config that specifically checks for Amazon RDS DB instances that are not encrypted at rest. This provides more flexibility and control in ensuring that the company’s specific requirement is met.","poster":"OCHT"},{"upvote_count":"2","timestamp":"1679831100.0","comment_id":"850957","poster":"mfsec","content":"Selected Answer: B\nEnable the appropriate guardrail"},{"timestamp":"1677972900.0","comment_id":"829490","poster":"Ajani","content":"Selected Answer: B\nMandatory controls are owned by AWS Control Tower, and they apply to every OU on your landing zone. These controls are applied by default when you set up your landing zone, and they can't be deactivated.\nThe solution requirement falls under a proactive(Recommended Control). \nhttps://docs.aws.amazon.com/controltower/latest/userguide/rds-rules.html#ct-rds-pr-16-description\nOptional controls are OU specific.","upvote_count":"4"},{"poster":"God_Is_Love","timestamp":"1677642360.0","comment_id":"825505","content":"Selected Answer: B\nTip - As this detective guardrail is available, answer is B. But if the guardrail is not available in that predefined list, the answer would be --C https://aws.amazon.com/blogs/mt/aws-control-tower-detective-guardrails-as-an-aws-config-conformance-pack/","upvote_count":"3"},{"upvote_count":"2","comment_id":"809646","timestamp":"1676471340.0","content":"Selected Answer: B\nquestion is asking for detection, not mandate","poster":"klog"}],"answer_ET":"B","answers_community":["B (93%)","7%"],"url":"https://www.examtopics.com/discussions/amazon/view/95276-exam-aws-certified-solutions-architect-professional-sap-c02/","choices":{"B":"Enable the appropriate guardrail from the list of strongly recommended guardrails in AWS Control Tower. Apply the guardrail to the production OU.","C":"Use AWS Config to create a new mandatory guardrail. Apply the rule to all accounts in the production OU.","D":"Create a custom SCP in AWS Control Tower. Apply the SCP to the production OU.","A":"Turn on mandatory guardrails in AWS Control Tower. Apply the mandatory guardrails to the production OU."},"question_text":"A company has an organization in AWS Organizations. The company is using AWS Control Tower to deploy a landing zone for the organization. The company wants to implement governance and policy enforcement. The company must implement a policy that will detect Amazon RDS DB instances that are not encrypted at rest in the company’s production OU.\n\nWhich solution will meet this requirement?","question_images":[],"unix_timestamp":1673705940,"question_id":491,"topic":"1"},{"id":"ftrVbZ0c8G1QXFc1L0WT","answer_ET":"D","exam_id":33,"question_id":492,"topic":"1","answers_community":["D (91%)","9%"],"answer_images":[],"unix_timestamp":1673706240,"choices":{"B":"Update the EC2 security groups to only allow inbound TCP on port 22 to the IP addresses of the engineer’s devices. Install the Amazon CloudWatch agent on all EC2 instances and send operating system audit logs to CloudWatch Logs.","C":"Update the EC2 security groups to only allow inbound TCP on port 22 to the IP addresses of the engineer’s devices. Enable AWS Config for EC2 security group resource changes. Enable AWS Firewall Manager and apply a security group policy that automatically remediates changes to rules.","A":"Install and configure EC2 Instance Connect on the fleet of EC2 instances. Remove all security group rules attached to EC2 instances that allow inbound TCP on port 22. Advise the engineers to remotely access the instances by using the EC2 Instance Connect CLI.","D":"Create an IAM role with the AmazonSSMManagedInstanceCore managed policy attached. Attach the IAM role to all the EC2 instances. Remove all security group rules attached to the EC2 instances that allow inbound TCP on port 22. Have the engineers install the AWS Systems Manager Session Manager plugin for their devices and remotely access the instances by using the start-session API call from Systems Manager."},"question_images":[],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/95278-exam-aws-certified-solutions-architect-professional-sap-c02/","discussion":[{"upvote_count":"20","timestamp":"1673706240.0","poster":"masetromain","comment_id":"775506","content":"Selected Answer: D\nThe correct answer is D. This strategy uses IAM roles and AWS Systems Manager to provide secure and auditable SSH access to the instances. The IAM role is attached to all the EC2 instances and has the AmazonSSMManagedInstanceCore managed policy attached, which allows the instances to be managed by Systems Manager. The engineers then install the AWS Systems Manager Session Manager plugin for their devices and remotely access the instances by using the start-session API call from Systems Manager. This approach provides secure and auditable access to the instances without the need for IP-based security group rules or additional infrastructure.","comments":[{"comment_id":"775507","upvote_count":"4","content":"Option A uses EC2 Instance Connect to provide secure and auditable SSH access to the instances, but it requires additional infrastructure and configuration. \n\nOption B provides auditing of commands run by the engineers, but it relies on IP-based security group rules, which can be difficult to manage and may not be as secure as using IAM roles. \n\nOption C uses AWS Config and Firewall Manager to automatically remediate changes to security group rules, but it still relies on IP-based security group rules and does not provide an auditable method of access to the instances.","comments":[{"timestamp":"1673706300.0","comments":[{"poster":"adrian202","timestamp":"1703171760.0","content":"The key factor is that Option A explains to remove the port 22 inbound SSH access security group, they would need to keep that present: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-connect-prerequisites.html","upvote_count":"3","comment_id":"1102654"}],"content":"For option A to work, the following additional infrastructure and configuration would be required:\n\nThe EC2 Instance Connect service needs to be enabled in the AWS account and the appropriate IAM permissions would need to be granted to the engineers.\n\nThe EC2 instances would need to have the EC2 Instance Connect agent installed and configured.\n\nThe engineers would need to install the EC2 Instance Connect CLI on their devices and have the necessary credentials to authenticate with AWS.\n\nIn addition, the company would need to update their processes and procedures to ensure that engineers are only using EC2 Instance Connect to access the instances and that all access is being logged and audited.","upvote_count":"4","comment_id":"775508","poster":"masetromain"}],"poster":"masetromain","timestamp":"1673706240.0"}]},{"comments":[{"upvote_count":"1","content":"The explanation for A is wrong.\nAWS EC2 Instance Connect does support auditing.","comment_id":"1279293","poster":"kgpoj","timestamp":"1725589140.0"}],"timestamp":"1677784200.0","poster":"God_Is_Love","upvote_count":"11","comment_id":"827297","content":"Selected Answer: D\nA is wrong because Instance connect does not provided auditing\nB is wrong because it mentions OS audit logs. we need to audit SSH trafic\nC is wrong because we want to audit not remediate as asked in question. config service is to record \nusing predefined rules and remediate as well\n\nD is correct because,\nBy attaching the AmazonSSMManagedInstanceCore policy to an IAM role, EC2 instances can be controlled and monitored through the Systems Manager service, enabling capabilities such as remote instance management, patching, and compliance reporting. (ChatGPT response its answers are brief and helpful sometimes)"},{"content":"D. Create an IAM role with the AmazonSSMManagedInstanceCore managed policy attached. Attach the IAM role to all the EC2 instances. Remove all security group rules attached to the EC2 instances that allow inbound TCP on port 22. Have the engineers install the AWS Systems Manager Session Manager plugin for their devices and remotely access the instances by using the start-session API call from Systems Manager.","poster":"amministrazione","timestamp":"1725095400.0","upvote_count":"1","comment_id":"1275520"},{"comment_id":"1175665","timestamp":"1710662820.0","content":"Selected Answer: D\nD, use SSM","upvote_count":"1","poster":"gofavad926"},{"upvote_count":"2","content":"Selected Answer: D\nOption D is the best strategy because it leverages AWS Systems Manager Session Manager, which allows for secure instance management without the need for SSH access. By attaching an IAM role with the AmazonSSMManagedInstanceCore policy to EC2 instances, engineers can use Session Manager for shell access to instances without needing to open port 22, significantly enhancing security. Session Manager also automatically logs session activity to S3 or CloudWatch Logs, providing the required command auditing capability. This eliminates the need for direct SSH access and offers a centralized, secure, and audited method for engineers to access and run commands on instances.","poster":"8608f25","comment_id":"1146411","timestamp":"1707579600.0"},{"timestamp":"1703388780.0","upvote_count":"1","poster":"rioisverycute","comment_id":"1104436","content":"Selected Answer: B\nIt required to increase security around ssh access, why so many people voted on D?","comments":[{"poster":"djeong95","content":"Cloudwatch agent does not provide auditable logs for SSH sessions; it only provides metrics about CPU/Memory/Network Packets/etc; nothing about what user started session at what time and ran certain trackable API calls while in that session.","timestamp":"1709142540.0","upvote_count":"1","comment_id":"1161875"}]},{"upvote_count":"2","poster":"Chung234","content":"The answer is D. Option A is wrong because EC2 Instance Connect requires the host security group to permit SSH traffic inbound. https://repost.aws/questions/QUnV4R9EoeSdW0GT3cKBUR7w/what-is-the-difference-between-ec2-instance-connect-and-session-manager-ssh-connections","comment_id":"1045432","timestamp":"1697505840.0"},{"poster":"NikkyDicky","upvote_count":"1","comment_id":"941131","timestamp":"1688319840.0","content":"Selected Answer: D\nIt's D"},{"upvote_count":"1","timestamp":"1687102260.0","content":"Selected Answer: D\nkeyword = AWS Systems Manager Session Manager\nthen D","poster":"SkyZeroZx","comment_id":"926770"},{"upvote_count":"2","comment_id":"850959","timestamp":"1679831220.0","poster":"mfsec","content":"Selected Answer: D\nD for sure."},{"upvote_count":"2","comment_id":"829492","poster":"Ajani","content":"Why its NOT A\nTo connect using the Amazon EC2 console, the instance must have a public IPv4 address.\n\nIf the instance does not have a public IP address, you can connect to the instance over a private network using an SSH client or the EC2 Instance Connect CLI. For example, you can connect from within the same VPC or through a VPN connection, transit gateway, or AWS Direct Connect.\n\nEC2 Instance Connect does not support connecting using an IPv6 address.\ngoing with D:","timestamp":"1677973380.0"},{"upvote_count":"2","content":"Selected Answer: D\nNeed to be able to audit the commands ran on the machine.","comment_id":"814694","poster":"lygf","timestamp":"1676853360.0"},{"content":"I don't understand why it can't be A for this one. Why is AWS Systems Manager Session better than EC2 Instance Connect? They both require installing something on the instances.","poster":"DWsk","comment_id":"810003","upvote_count":"1","timestamp":"1676496840.0","comments":[{"poster":"lygf","comment_id":"814693","timestamp":"1676853300.0","upvote_count":"1","content":"Could option A audit the commands ran on the server, as required by the question? I knew D certainly can."},{"timestamp":"1677961380.0","comment_id":"829378","upvote_count":"4","content":"For EC2 instance connect there are a few requirements:\n- instance has public IP (the instances in question are private)\n- you have port 22 open (A says remove port 22 inbound)","poster":"anita_student"}]},{"timestamp":"1676084940.0","upvote_count":"2","content":"Selected Answer: D\nAccording to ChatGPT,\n\nYes, AWS Systems Manager Session Manager can track the commands that are executed during a session. The session is recorded in the form of a log, which can be accessed and reviewed later. The log contains information such as the start time, end time, and the user who initiated the session, as well as a record of all the commands executed during the session, including their output and exit codes. This information can be useful for auditing purposes, troubleshooting, and compliance reporting.","poster":"moota","comment_id":"804943"},{"comment_id":"802678","content":"Selected Answer: B\nprovide auditing of commands run by the engineers = B Only","poster":"tinyflame","upvote_count":"3","comments":[{"timestamp":"1693273740.0","content":"Read docs you can audit command using SSM https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-logging.html","upvote_count":"1","poster":"joefromnc","comments":[{"comment_id":"1028655","timestamp":"1696841340.0","content":"\"In addition to providing information about current and completed sessions in the Systems Manager console, Session Manager provides you with the ability to audit session activity in your AWS account using AWS CloudTrail\"\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-auditing.html\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-auditing.html","poster":"rlf","upvote_count":"1"}],"comment_id":"992718"}],"timestamp":"1675900320.0"}],"timestamp":"2023-01-14 15:24:00","answer":"D","question_text":"A startup company hosts a fleet of Amazon EC2 instances in private subnets using the latest Amazon Linux 2 AMI. The company’s engineers rely heavily on SSH access to the instances for troubleshooting.\n\nThe company’s existing architecture includes the following:\n\n• A VPC with private and public subnets, and a NAT gateway.\n• Site-to-Site VPN for connectivity with the on-premises environment.\n• EC2 security groups with direct SSH access from the on-premises environment.\n\nThe company needs to increase security controls around SSH access and provide auditing of commands run by the engineers.\n\nWhich strategy should a solutions architect use?","isMC":true},{"id":"XncxrQS6pKZeNLewWxzP","question_images":[],"choices":{"B":"Use AWS Budgets to create a fixed monthly budget for each developer’s account as part of the account creation process.","A":"Create an SCP to set a fixed monthly account usage limit. Apply the SCP to the developer accounts.","D":"Create an IAM policy to deny access to costly services and components. Apply the IAM policy to the developer accounts.","C":"Create an SCP to deny access to costly services and components. Apply the SCP to the developer accounts.","E":"Create an AWS Budgets alert action to terminate services when the budgeted amount is reached. Configure the action to terminate all services.","F":"Create an AWS Budgets alert action to send an Amazon Simple Notification Service (Amazon SNS) notification when the budgeted amount is reached. Invoke an AWS Lambda function to terminate all services."},"unix_timestamp":1673710500,"answer_images":[],"question_text":"A company that uses AWS Organizations allows developers to experiment on AWS. As part of the landing zone that the company has deployed, developers use their company email address to request an account. The company wants to ensure that developers are not launching costly services or running services unnecessarily. The company must give developers a fixed monthly budget to limit their AWS costs.\n\nWhich combination of steps will meet these requirements? (Choose three.)","question_id":493,"answer":"BCF","url":"https://www.examtopics.com/discussions/amazon/view/95284-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_ET":"BCF","answers_community":["BCF (81%)","Other"],"timestamp":"2023-01-14 16:35:00","answer_description":"","discussion":[{"comment_id":"811171","upvote_count":"20","poster":"spd","timestamp":"1676587560.0","content":"Selected Answer: BCF\nClear - BCF - SCP is preferable over IAM"},{"content":"Selected Answer: BCF\nI prefer D over C as IAM cant be applied to Account","timestamp":"1677259440.0","comment_id":"820771","upvote_count":"16","poster":"kiran15789","comments":[{"comment_id":"1282780","upvote_count":"1","timestamp":"1726161600.0","content":"Option D says apply it to the Developers accounts. Unnecessary operational overhead","poster":"AWSum1"}]},{"comment_id":"1307537","content":"Fixed monthly budget> implement AWS budget. hence B is correct.\nPrevent running unnecessary services> implement SCP. hence C is correct.\non F: I'm just not sure why do we want to terminate all resources and why not just don't let them run additional.","upvote_count":"2","poster":"vda2024","timestamp":"1730836020.0"},{"upvote_count":"1","content":"B. Use AWS Budgets to create a fixed monthly budget for each developer’s account as part of the account creation process.\nC. Create an SCP to deny access to costly services and components. Apply the SCP to the developer accounts.\nF. Create an AWS Budgets alert action to send an Amazon Simple Notification Service (Amazon SNS) notification when the budgeted amount is reached. Invoke an AWS Lambda function to terminate all services.","comment_id":"1275521","poster":"amministrazione","timestamp":"1725095400.0"},{"poster":"MAZIADI","content":"Selected Answer: BCF\nWhy Option C is Preferred to D :\nCentralized Control: SCPs provide a centralized way to manage permissions across all accounts in an organization, ensuring consistent enforcement of policies.\nScalability: SCPs are easier to manage and scale when dealing with multiple accounts, as changes to the SCP will automatically apply to all accounts under the organization.\nCompliance: SCPs help ensure compliance with organizational policies by preventing the use of restricted services across all accounts.","comment_id":"1264556","upvote_count":"2","timestamp":"1723452240.0"},{"poster":"gofavad926","upvote_count":"2","comment_id":"1175667","content":"Selected Answer: BCF\nBCF - SCP, budget and custom lambda to terminate services","timestamp":"1710663060.0"},{"comment_id":"1174815","poster":"wooin992","timestamp":"1710573540.0","content":"Selected Answer: BDF\nBDF \ncannot apply scp in account, need to apply it in OU","comments":[{"poster":"MAZIADI","upvote_count":"2","comment_id":"1264555","content":"wrong, you can apply scp to an account","timestamp":"1723452060.0"}],"upvote_count":"1"},{"timestamp":"1707580440.0","content":"Selected Answer: BCF\nB. Use AWS Budgets to create a fixed monthly budget for each developer’s account as part of the account creation process. AWS Budgets allows you to set custom cost and usage budgets that alert you when you exceed your thresholds.\nC. Create an SCP to deny access to costly services and components. Apply the SCP to the developer accounts. By creating an SCP that specifically denies access to costly AWS services, the company can prevent developers from launching such services, thereby helping to keep costs within the fixed monthly budget.\nF. Create an AWS Budgets alert action to send an Amazon Simple Notification Service (Amazon SNS) notification when the budgeted amount is reached. Invoke an AWS Lambda function to terminate all services. While AWS Budgets cannot directly terminate services when a budget is exceeded, you can configure an alert to trigger a notification. This notification can then invoke a Lambda function designed to assess and terminate services as necessary, based on the company’s policies.","upvote_count":"2","poster":"8608f25","comment_id":"1146421"},{"upvote_count":"1","comment_id":"1131827","content":"Setting a monthly cost budget with a variable target amount, with each subsequent month growing the budget target by 5 percent. Then, you can configure your notifications for 80 percent of your budgeted amount and apply an action. For example, you could automatically apply a custom IAM policy that denies you the ability to provision additional resources within an account.\n\nhttps://docs.aws.amazon.com/cost-management/latest/userguide/budgets-managing-costs.html\nans :bdf","poster":"duriselvan","timestamp":"1706198340.0"},{"content":"Selected Answer: BCF\nA = SCP is used to limit permission that administrator can grant IAM users/roles, SCP cannot set a fixed monthly account usage limit\nB = correct\nC = correct\nD = it could work, but it would required more work wrt SCP\nE = Budget actions cannot terminate all kind of services, actually supports 3 types of actions 1/ apply IAM policy to IAM identities, 2/ apply SCP to an OU and 3/ terminate EC2 and RDS instances\nF = correct","timestamp":"1704550080.0","upvote_count":"2","comment_id":"1115209","poster":"ninomfr64"},{"content":"Selected Answer: BDF\nAlthough, C is correct, some people here says that SCP cannot be attached to an account, but it is not true, you can, the most common option when we want to deny permissions to an account is to use an IAM policy.","timestamp":"1704215340.0","poster":"jpa8300","comment_id":"1112092","upvote_count":"1"},{"comment_id":"1028677","timestamp":"1696842780.0","content":"BCF. \nIn Option D, we can not apply IAM policy to an AWS Account.","upvote_count":"1","poster":"rlf"},{"comments":[{"upvote_count":"1","poster":"vn_thanhtung","timestamp":"1692616200.0","content":"IAM policies for user ? https://docs.aws.amazon.com/kms/latest/developerguide/iam-policies-overview.html","comment_id":"986429"},{"timestamp":"1692618300.0","comment_id":"986455","upvote_count":"1","content":"Sorry I mistake, IAM policies can applied on User.","poster":"vn_thanhtung"}],"comment_id":"984181","poster":"SK_Tyagi","timestamp":"1692334440.0","upvote_count":"4","content":"Selected Answer: BDF\nI'd go with BDF, since there's no mention of OU. As a rule of thumb, IAM policies to restrict are applied on Accounts, Users, Groups and SCP's on OU's."},{"upvote_count":"2","poster":"CuteRunRun","content":"Selected Answer: BCF\nBCF is right.\nI think SCP is more convenient than iam.\nYou need to config the IAM to all account manually","timestamp":"1691487540.0","comment_id":"975397"},{"poster":"[Removed]","content":"Selected Answer: BCF\nprefer SCP over IAm in org accounts","comment_id":"957010","upvote_count":"2","timestamp":"1689811380.0"},{"upvote_count":"2","timestamp":"1688320020.0","poster":"NikkyDicky","comment_id":"941133","content":"Selected Answer: BCF\nIt's a BCF"},{"poster":"PhuocT","content":"Selected Answer: BCF\nC - SCP would be prefer to control the services could be used in Organization's AWS accounts.","comment_id":"929062","timestamp":"1687323360.0","upvote_count":"2"},{"timestamp":"1687102380.0","poster":"SkyZeroZx","content":"Selected Answer: BCF\nClear - BCF - SCP is preferable over IAM","comment_id":"926771","upvote_count":"2"},{"timestamp":"1685813880.0","upvote_count":"6","poster":"Roontha","comment_id":"913845","content":"Answer : B,C,F\n\nUse case reference from AWS with architecture diagram.\nhttps://aws.amazon.com/blogs/mt/control-developer-account-costs-with-aws-cloudformation-and-aws-budgets/"},{"timestamp":"1683475680.0","content":"Selected Answer: BCF\nI agree with B C and F. C instead of D because with option D states that the IAM policy should be applied to the developer accounts, this seems like we would require to apply this for each user individually, since the company already makes use of Organizations why not create a SCP as guardrail for avoiding the use of all costly services. Something like the SCP below:\n{\n \"Version\": \"2012-10-17\",\n \"Statement\": [\n {\n \"Sid\": \"DenyCostlyServices\",\n \"Effect\": \"Deny\",\n \"Action\": [\n \"aws-portal:*\",\n \"cloudfront:*\",\n \"directconnect:*\",\n \"globalaccelerator:*\",\n \"shield:*\",\n \"waf:*\",\n \"waf-regional:*\"\n ],\n \"Resource\": \"*\"\n }\n ]\n}","upvote_count":"4","comment_id":"891540","poster":"rbm2023"},{"timestamp":"1681768920.0","content":"Selected Answer: BCF\nFrom https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_attach.html\n\nYou can attach an SCP to the organization root, to an organizational unit (OU), or directly to an account.\n\nWhy attach it to the IAM role when you can make an SCP?","upvote_count":"3","comment_id":"873129","poster":"Anonymous9999"},{"upvote_count":"3","timestamp":"1681627020.0","poster":"yama234","comments":[{"poster":"Roontha","upvote_count":"1","content":"you are correct. clear use case above mentioned URL","comment_id":"913842","timestamp":"1685813700.0"}],"content":"BCF \nhttps://aws.amazon.com/blogs/mt/control-developer-account-costs-with-aws-cloudformation-and-aws-budgets/\n\nThis solution utilizes integrations with AWS Organizations and AWS CloudFormation in order to deploy a budget to every account in a specific organizational unit in your organization. In turn, this budget will send notifications through Amazon Simple Notification Service (SNS) when forecasted thresholds are exceeded. Then, we will utilize these SNS notifications to execute an AWS Lambda function that will shut down every EC2 instance that is not tagged as critical in a single region.","comment_id":"871517"},{"comment_id":"869502","content":"Selected Answer: BDF\nAnswer: BDF: \nSPC it would be ideal but, the question doesn´t inform us if the developers accounts are in inside on developers´ s OU or if exist one OU for them. Because of this, we have to use an IAM policy to apply the limitation only on Developers account","upvote_count":"4","poster":"Cassa","timestamp":"1681397340.0"},{"comments":[{"timestamp":"1682016480.0","comment_id":"875910","content":"it is not possible to apply an SCP to individual user accounts in AWS. Instead, an SCP is applied to an entire account or an OU in AWS Organizations to restrict the permissions of all IAM users and roles within that account or OU. https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_attach.html","upvote_count":"4","poster":"[Removed]"}],"timestamp":"1679831580.0","poster":"mfsec","comment_id":"850961","content":"Selected Answer: BCF\nBCF - SCP is more efficient at restrictions than using IAM across accounts.","upvote_count":"2"},{"poster":"zejou1","content":"Selected Answer: BCF\nFirst sentence \"A company that uses AWS Organizations...\" -\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_ous.html\n\nIt is BCF - when they are granted access to the AWS Organization, you will want to set the SCP for the: OrganizationAccountAccessRole. Yes, you \"could create\" a new IAM role specific to developers, but you can create a SCP for only what is necessary for the developers to do their job.","timestamp":"1679018400.0","upvote_count":"3","comment_id":"841507"},{"upvote_count":"1","content":"Invoke an AWS Lambda function to terminate all services\nIs there a Lambda to terminate all services?","poster":"vherman","timestamp":"1678369260.0","comment_id":"834006"},{"timestamp":"1677509820.0","upvote_count":"3","poster":"lkyixoayffasdrlaqd","comments":[{"content":"good point. Again a question to dice about how to interpret the answers.","comment_id":"860873","upvote_count":"1","poster":"hobokabobo","timestamp":"1680603240.0"},{"upvote_count":"3","timestamp":"1677510060.0","poster":"lkyixoayffasdrlaqd","comment_id":"823852","content":"Answer should be A-B-F\nA. Create an SCP to set a fixed monthly account usage limit. Apply the SCP to the developer accounts.\nB. Use AWS Budgets to create a fixed monthly budget for each developer's account as part of the account creation process.\nF. Create an AWS Budgets alert action to send an Amazon Simple Notification Service (Amazon SNS) notification when the budgeted amount is reached. Invoke an AWS Lambda function to terminate all services."}],"content":"I ignore everyone here answer includes C and D.\n\"deny access to costly services and components.\" What does that mean? WHO is going to decide which services are costly one by one? Come on guys.","comment_id":"823841"},{"poster":"scuzzy2010","upvote_count":"3","comment_id":"819955","timestamp":"1677200100.0","content":"Selected Answer: BCF\nNot D as you can't apply IAM policy to an AWS Account."},{"poster":"rtgfdv3","timestamp":"1676668680.0","upvote_count":"2","content":"Selected Answer: BCF\nBCF\nI dont think C is a valid answer [ strictly speaking ], \nIn a Landing Zone you cant apply SCPs directly to accounts, \nYou apply guardrails or controls,,,, that can have scp inside as artifacts. \n\nEither way an account is not a principal you can apply either an IAM policy to an account.","comment_id":"812390"},{"content":"The questions is asking which steps will ensure that developers are not launching costly services or running services unnecessarily that will meet the requirements. Both E and F mention \"Terminate All Services\", which implies it will terminate all of the services once the budget is exceeded. B and C are correct and so is D by process of elimination.","timestamp":"1676524560.0","comment_id":"810272","upvote_count":"1","comments":[{"content":"The moderator remove the above comment and forgot to see the word running services so BCF seems logical in this case.","poster":"c73bf38","comment_id":"810277","upvote_count":"1","timestamp":"1676524860.0"}],"poster":"c73bf38"},{"content":"Selected Answer: BDF\nBDF\nI see some votes for C over D, but that can't be because you can't apply a policy to an account. Additionally, an SCP would make more sense for a situation where you don't want anyone in the account to use the services","upvote_count":"2","poster":"DWsk","comment_id":"810009","timestamp":"1676497020.0"},{"timestamp":"1676471820.0","upvote_count":"2","poster":"klog","comment_id":"809653","content":"Selected Answer: BCF\nIAM should apply to user/groups, not accounts"},{"upvote_count":"1","content":"Thinking A because perhaps you can do the below:\n{\n \"Version\": \"2012-10-17\",\n \"Statement\": [\n {\n \"Effect\": \"Allow\",\n \"Action\": \"*\",\n \"Resource\": \"*\"\n },\n {\n \"Sid\": \"DenyCloudTrail\",\n \"Effect\": \"Deny\",\n \"Action\": \"cloudtrail:*\",\n \"Resource\": \"*\"\n },\n {\n \"Sid\": \"AllowS3CreateBucket\",\n \"Effect\": \"Allow\",\n \"Action\": \"s3:CreateBucket\",\n \"Resource\": \"*\"\n }\n ]\n}\nThe first \"Allow\" statement in the SCP allows all actions on all resources, which would allow the creation of S3 buckets. However, the second \"Deny\" statement specifically denies all cloudtrail actions, which could potentially impact the ability to create S3 buckets if there is a dependency on cloudtrail for that action. To ensure that the developers are able to create S3 buckets, a new statement with \"Allow\" effect for the s3:CreateBucket action should be added to the SCP.","timestamp":"1676224560.0","poster":"CloudFloater","comment_id":"806628"},{"upvote_count":"2","comment_id":"804960","content":"Selected Answer: BCF\nI mean BCF","poster":"moota","timestamp":"1676086440.0"},{"comment_id":"804957","timestamp":"1676086380.0","upvote_count":"1","poster":"moota","content":"Selected Answer: ACF\nI go for C vs D because accounts usually mean AWS account in the context of AWS Organization."},{"content":"Selected Answer: BCF\nBCF is the correct","upvote_count":"3","timestamp":"1675283040.0","poster":"jojom19980","comment_id":"795498"},{"poster":"zozza2023","timestamp":"1675109160.0","comment_id":"793275","content":"Selected Answer: BDF\nBDF seems ok","upvote_count":"1"},{"upvote_count":"2","comment_id":"787088","content":"You don't attach a policy to an account, you attach a policy to a user, group, or role.","timestamp":"1674603720.0","poster":"harleydog"},{"poster":"bititan","timestamp":"1674588960.0","comment_id":"786860","upvote_count":"3","content":"Selected Answer: BDF\noption D is wrong because you cannot apply IAM policy to an account. So I choose option C because SCP will block them from using services"},{"content":"BDF\nB: set budget limit, F to check and use lambda to terminate, D: deny costly services\nThere is no organizations used here. All policy must be directly applied to accounts","poster":"zhangyu20000","comments":[{"timestamp":"1673995020.0","content":"I have doubts between C and D because at the beginning it says that the company uses AWS Organizations so I think that D is correct","upvote_count":"2","comment_id":"779377","poster":"Nicocacik","comments":[{"comments":[{"poster":"harleydog","comment_id":"787086","comments":[{"upvote_count":"3","poster":"ignorica","comment_id":"791644","timestamp":"1674997020.0","content":"I second this. C should be correct, not D.\nWe are talking about \"accounts\" and you do not apply IAM to accounts but to users, group, roles.\nSCP can be used to deny access from services as well as operations for starting a certain service.\nhttps://stackoverflow.com/questions/63573198/how-to-restrict-the-instance-types-allowed-to-launch-in-an-aws-account\n(for example how to restrict the RunInstances)\nD is for sure not OK."}],"upvote_count":"4","content":"You can attach an SCP to the root, to an OU, or to an account. https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_attach.html","timestamp":"1674603480.0"}],"timestamp":"1674058020.0","poster":"masetromain","upvote_count":"1","content":"While creating an SCP (Service Control Policy) to deny access to costly services and components can help prevent developers from accidentally launching or using these services, it may not be the most effective approach to controlling costs.\n\nAn SCP can only be applied to the root level of an organization, which means that it will apply to all accounts and OUs within that organization. This means that it would not be possible to apply different policies for different accounts or OUs, which would make it more difficult to manage costs for each developer's account specifically.\n\nAdditionally, an SCP only denies access to services and does not prevent developers from accidentally running services unnecessarily, which is one of the requirements for the company. While an SCP is a powerful tool for controlling access to AWS services, it may not be the best option for managing costs specifically.\n\nUsing AWS Budgets, in conjunction with an IAM policy to deny access to costly services and components, would be a more effective approach in this scenario.","comment_id":"780189"}]}],"upvote_count":"1","comment_id":"776839","timestamp":"1673802180.0"},{"poster":"masetromain","timestamp":"1673710500.0","upvote_count":"1","comment_id":"775616","comments":[{"comment_id":"775617","poster":"masetromain","upvote_count":"1","content":"A is not the best solution because it's only set the fixed monthly account usage limit but it will not prevent developers from using costly services and components.\n\nC is not the best solution because it does not set a fixed monthly budget for each developer’s account and does not provide a way to alert or terminate services when the budgeted amount is reached.\n\nE is not the best solution because it does not provide a way to alert or notify the company when the budgeted amount is reached and does not provide a way to automatically terminate services when the budgeted amount is reached.","timestamp":"1673710560.0"}],"content":"Selected Answer: BDF\nThe correct answer is B, D and F.\n\nB: Use AWS Budgets to create a fixed monthly budget for each developer’s account as part of the account creation process. This will ensure that each developer has a set budget limit and will not be able to incur additional costs beyond that limit.\n\nD: Create an IAM policy to deny access to costly services and components. Apply the IAM policy to the developer accounts. This will prevent developers from launching or using services that are deemed too costly for the company.\n\nF: Create an AWS Budgets alert action to send an Amazon Simple Notification Service (Amazon SNS) notification when the budgeted amount is reached. Invoke an AWS Lambda function to terminate all services. This will ensure that when the budget limit is reached, all services will be terminated to prevent additional costs."}],"isMC":true,"exam_id":33,"topic":"1"},{"id":"3LyQNiFairCRVSKEZOTD","answer":"B","url":"https://www.examtopics.com/discussions/amazon/view/95285-exam-aws-certified-solutions-architect-professional-sap-c02/","question_images":[],"topic":"1","answer_description":"","discussion":[{"timestamp":"1673711280.0","comments":[{"timestamp":"1673711280.0","comment_id":"775643","content":"Option A is not the best solution because it doesn't share the Aurora DB cluster with the Target account and this would cause data inconsistencies as the Source and Target accounts would not share the same data.\n\nOption C is not the best solution because, it does not specify how the data will be migrated and it would cause downtime as the Source and Target accounts are not sharing the same data.\n\nOption D is not the best solution because it does not specify how the Lambda function will be migrated and it would cause data inconsistencies as the Source and Target accounts are not sharing the same data.","comments":[{"content":"For option A, its also not possible because automated snapshots cannot be shared..\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-share-snapshot.html","upvote_count":"4","timestamp":"1688288760.0","comment_id":"940689","poster":"lxrdm"}],"poster":"masetromain","upvote_count":"2"}],"comment_id":"775641","poster":"masetromain","content":"Selected Answer: B\nThe correct answer is option B. This solution uses a combination of AWS Resource Access Manager (RAM) and automated backups to migrate the Lambda functions and the Aurora database to the Target account while minimizing downtime.\n\nIn this solution, the Lambda function deployment package is downloaded from the Source account and used to create new Lambda functions in the Target account. The Aurora DB cluster is shared with the Target account using AWS RAM and the Target account is granted permission to clone the Aurora DB cluster, allowing for a new copy of the Aurora database to be created in the Target account. This approach allows for the data to be migrated to the Target account while minimizing downtime, as the Target account can use the cloned Aurora database while the original Aurora database continues to be used in the Source account.","upvote_count":"21"},{"timestamp":"1692678540.0","comment_id":"987067","upvote_count":"18","poster":"Simon523","content":"Selected Answer: B\nAWS Resource Access Manager (RAM) can only share the follow services:\n Amazon Aurora – DB clusters\n Amazon EC2 – capacity reservations and dedicated hosts\n AWS License Manager – License configurations\n AWS Outposts – Local gateway route tables, outposts, and sites\n Amazon Route 53 – Forwarding rules\n Amazon VPC – Customer-owned IPv4 addresses, prefix lists, subnets, traffic mirror targets, transit gateways, transit gateway multicast domains\nhttps://docs.aws.amazon.com/ram/latest/userguide/shareable.html"},{"upvote_count":"1","poster":"JOJO9","timestamp":"1734380280.0","comment_id":"1327616","content":"Selected Answer: A\nLambda function and Aurora cluster can NOT be shared with RAM!"},{"comment_id":"1275523","timestamp":"1725095460.0","content":"B. Download the Lambda function deployment package from the Source account. Use the deployment package and create new Lambda functions in the Target account. Share the Aurora DB cluster with the Target account by using AWS Resource Access Manager {AWS RAM). Grant the Target account permission to clone the Aurora DB cluster.","poster":"amministrazione","upvote_count":"1"},{"poster":"Dgix","comments":[{"poster":"mnsait","content":"Option A is NOT viable. As @lxrdm has pointed out with documentation, it is not possible to share 'automated' db cluster snapshots.","comment_id":"1316599","timestamp":"1732349520.0","upvote_count":"1"}],"timestamp":"1709480880.0","comment_id":"1164898","upvote_count":"3","content":"Selected Answer: B\nA is viable, but as AWS RAM can share Aurora clusters, B is faster. However, AWS RAM can't share lambdas, so C and D are out."},{"timestamp":"1709480580.0","upvote_count":"1","poster":"Dgix","comment_id":"1164895","content":"B, C, and D are all out since AWS RAM cannot share either Lambdas or Aurora DB clusters. A is the only viable one - you must use a manual shapshot for the DB, share it, and redeploy any deployment package in the destination account. (The question tries to trip you up by its wording: lambda deployments can't be downloaded, but the same deployment packages used to deploy the lambdas can, for instance from S3 or from source)"},{"comment_id":"1146435","upvote_count":"2","content":"Selected Answer: B\nOption B is the most accurate and efficient solution based on this AWS article content (https://aws.amazon.com/about-aws/whats-new/2019/07/amazon_aurora_supportscloningacrossawsaccounts-/). It correctly outlines the steps for Lambda migration and utilizes the Aurora DB cluster cloning feature across accounts via AWS RAM, which aligns with the article’s description. This approach ensures minimal downtime and efficient migration by allowing direct cloning of the Aurora database.\n\nOption C incorrectly suggests using AWS RAM to share Lambda functions, which is not supported yet based on latest sharable AWS resources: https://docs.aws.amazon.com/ram/latest/userguide/shareable.html","timestamp":"1707583020.0","poster":"8608f25"},{"timestamp":"1705990680.0","poster":"master9","comment_id":"1129227","upvote_count":"1","content":"Selected Answer: C\nAWS Resource Access Manager (RAM) to share AWS Lambda functions and Aurora DB clusters with another AWS account. AWS RAM allows you to share resources that are created and managed by other AWS services with individual AWS accounts or with the accounts in an organization or organizational units (OUs) in AWS Organizations.\n\nTo share a Lambda function with another AWS account, you can delegate access to an IAM user (or all users) in the other AWS account so that they can assume a role in your account and invoke the Lambda function in your account.\n\nTo share an Aurora DB cluster with another AWS account, you can create a resource share in AWS RAM and specify the Amazon Resource Name (ARN) of the Aurora DB cluster as the resource to share. You can then specify the AWS account IDs of the accounts with which you want to share the resource."},{"content":"Selected Answer: B\nA = you can share snapshot to restore DB, but this will introduce some downtime\nB = correct (cloning a DB allows for very limited downtime)\nC = if you only share Lambda you are not migrating it, also it appears the Lambda is not a RAM sharable resource https://docs.aws.amazon.com/ram/latest/userguide/getting-started-sharing.html\nD = it appears the Lambda is not a RAM sharable resource and you cannot directly share an automated snapshot, you need first to create a manual snapshot by copying the automated snapshot, and then share that copy https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-share-snapshot.html","upvote_count":"1","poster":"ninomfr64","comment_id":"1115227","timestamp":"1704551940.0","comments":[{"content":"A is not correct as you cannot directly share an automated snapshot, you need first to create a manual snapshot by copying the automated snapshot, and then share that copy https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-share-snapshot.html","upvote_count":"1","timestamp":"1704552060.0","poster":"ninomfr64","comment_id":"1115236"}]},{"poster":"learnwithaniket","upvote_count":"2","comment_id":"1111595","timestamp":"1704171420.0","content":"Selected Answer: B\nThere is limit on the number of resources you can share with AWS RAM.\nAWS RAM does not support direct sharing of Lambda functions between accounts.\nhttps://docs.aws.amazon.com/ram/latest/userguide/shareable.html"},{"timestamp":"1688320320.0","content":"Selected Answer: B\nit's B. \nIn A - automated snapshots are not shareable","poster":"NikkyDicky","comment_id":"941136","upvote_count":"2"},{"timestamp":"1687282320.0","poster":"Maria2023","comment_id":"928645","upvote_count":"1","content":"Selected Answer: B\nOption B minimizes downtime, compared to A, where we only share a snapshot of the cluster. For C we do not migrate the lambdas, we just share them, which is not the idea of the exercise."},{"comment_id":"926773","timestamp":"1687102500.0","content":"Selected Answer: B\nThe correct answer is option B. This solution uses a combination of AWS Resource Access Manager (RAM) and automated backups to migrate the Lambda functions and the Aurora database to the Target account while minimizing downtime.","poster":"SkyZeroZx","comments":[{"upvote_count":"2","poster":"SkyZeroZx","timestamp":"1687102500.0","comment_id":"926774","content":"in case the letter A use only snapshot not sync the complete data and is posible lost data in the process"}],"upvote_count":"1"},{"content":"Selected Answer: C\nThey just want to migrate the Lambda and Aurora DB, they dont care about the app itself","timestamp":"1686824040.0","upvote_count":"1","comment_id":"924064","poster":"Perkuns"},{"timestamp":"1683477600.0","poster":"rbm2023","upvote_count":"3","content":"Selected Answer: B\nThe question is about migration and not sharing, so the answer is how to use a RAM feature to help you on the migration. In option D they are not migrating anything, both Lambda and Aurora are being shared with the Target account and not migrated. In option C is a similar situation, the Lambda is not being migrated. Option A seems a good option but might cause a larger downtime. Hence option D is more appropriate because you can use the cluster share with the Target account and clone the database cluster into it. In my view this answer should contemplate in which moment the cutoff from Source to Target would occur.","comment_id":"891560"},{"timestamp":"1680482940.0","content":"Selected Answer: B\nYou can share the following Amazon Aurora resources by using AWS RAM.","poster":"takecoffe","comment_id":"859448","upvote_count":"2"},{"comment_id":"850962","timestamp":"1679831700.0","poster":"mfsec","content":"Selected Answer: B\nB is the way forward","upvote_count":"2"},{"content":"Selected Answer: B\nAWS RAM can share ec2 instances, lambdas, DB clusters, RDS, event Redshift clusters.\nRefer AWS SA video here - https://www.youtube.com/watch?v=KL9SICG52zY\nIf company would not have had critical data, answer C is good. as existing app should not be down, we have to download lambda and then share. so answer is B. other wise you can stop app and share with RAM (Resource shares)","comments":[{"comment_id":"869625","upvote_count":"2","content":"However, if on migration the AWS RMS already will stop the Aurora I don´t see a problem use this window to migrate Lambda also?","timestamp":"1681405020.0","poster":"Cassa"}],"upvote_count":"4","comment_id":"827395","poster":"God_Is_Love","timestamp":"1677788400.0"},{"comment_id":"793281","timestamp":"1675109520.0","upvote_count":"4","content":"Selected Answer: B\nB is correct. Move Lambda and Aurora both to target account","poster":"zozza2023"},{"upvote_count":"3","timestamp":"1675027140.0","comment_id":"792074","poster":"Musk","content":"Selected Answer: B\nB can be done with this: https://aws.amazon.com/about-aws/whats-new/2019/07/amazon_aurora_supportscloningacrossawsaccounts-/"},{"comments":[{"comment_id":"824843","poster":"Satya80","comments":[{"poster":"Sarutobi","content":"You cannot share lambda, but creating a Lambda in a shared subnet is allowed.","timestamp":"1677624360.0","upvote_count":"4","comment_id":"825359"},{"upvote_count":"1","comment_id":"824917","timestamp":"1677595500.0","content":"scratch that","poster":"Satya80"}],"timestamp":"1677590940.0","upvote_count":"1","content":"As per the above link, Lambda can be shared. Please see the \"Subnets\" section ."}],"poster":"SK_Cert_master","timestamp":"1674259320.0","comment_id":"782866","upvote_count":"2","content":"B.\nIt seems that Lambda cannot be shared via RAM\nhttps://docs.aws.amazon.com/ram/latest/userguide/shareable.html"},{"timestamp":"1673809140.0","poster":"zhangyu20000","upvote_count":"3","content":"B is correct. Move Lambda and Aurora both to target account\nA: not move Aurora \nC: Lambda not move\nd: Lambda and Aurora both not moved","comment_id":"776955"}],"timestamp":"2023-01-14 16:48:00","answer_ET":"B","exam_id":33,"question_id":494,"answer_images":[],"unix_timestamp":1673711280,"question_text":"A company has applications in an AWS account that is named Source. The account is in an organization in AWS Organizations. One of the applications uses AWS Lambda functions and stores inventory data in an Amazon Aurora database. The application deploys the Lambda functions by using a deployment package. The company has configured automated backups for Aurora.\n\nThe company wants to migrate the Lambda functions and the Aurora database to a new AWS account that is named Target. The application processes critical data, so the company must minimize downtime.\n\nWhich solution will meet these requirements?","choices":{"C":"Use AWS Resource Access Manager (AWS RAM) to share the Lambda functions and the Aurora DB cluster with the Target account. Grant the Target account permission to clone the Aurora DB cluster.","B":"Download the Lambda function deployment package from the Source account. Use the deployment package and create new Lambda functions in the Target account. Share the Aurora DB cluster with the Target account by using AWS Resource Access Manager {AWS RAM). Grant the Target account permission to clone the Aurora DB cluster.","D":"Use AWS Resource Access Manager (AWS RAM) to share the Lambda functions with the Target account. Share the automated Aurora DB cluster snapshot with the Target account.","A":"Download the Lambda function deployment package from the Source account. Use the deployment package and create new Lambda functions in the Target account. Share the automated Aurora DB cluster snapshot with the Target account."},"isMC":true,"answers_community":["B (96%)","3%"]},{"id":"XOZWIvsGccX8hoyP6pua","question_id":495,"answer_description":"","discussion":[{"upvote_count":"21","timestamp":"1673711700.0","comment_id":"775652","poster":"masetromain","comments":[{"poster":"hamimelon","timestamp":"1693872900.0","upvote_count":"2","content":"Agree. Also, it says the company does not wanna manage long-term overhead, which points to serverless.","comment_id":"998903"},{"content":"SQS is out of the question because the script already has a built in logic that will prevent it to reprocess a message that's already been processed","comment_id":"1041060","upvote_count":"1","timestamp":"1697052900.0","poster":"dpatra"},{"comments":[{"comment_id":"1184007","poster":"red_panda","comments":[{"upvote_count":"1","comment_id":"1251383","content":"Use an S3 event notification to invoke the Lambda function to process the objects","poster":"NirvanaSNM","timestamp":"1721413020.0"}],"content":"ECS in Fargate mode you don't need to manage anything underling infra!\nYou're totally forgot about cost, for sure running an ECS Fargate has lower cost than running a Lambda for 5 minutes every 10 minutes!\nAlso the function to trigger the ECS workload (in option D), running for milliseconds (as need only to notify the doc upload in S3), so it's more correct the D answer.\nAsk to any Gen AI model, you will have mine answer with more details :)","upvote_count":"1","timestamp":"1711536540.0"}],"content":"Option C, migrating the data processing script to a container image and running it on an EC2 instance, would still require the company to manage the underlying EC2 instances and may not be as cost-effective as using Lambda.\n\nOption D, migrating the data processing script to a container image that runs on Amazon ECS on AWS Fargate, would still require the company to manage the underlying infrastructure and may not be as cost-effective as using Lambda. Additionally, it introduces additional complexity by adding a Lambda function that calls the Fargate RunTask API operation.","comment_id":"775653","timestamp":"1673711700.0","upvote_count":"5","poster":"masetromain"}],"content":"Selected Answer: A\nThe correct answer is A, migrating the data processing script to an AWS Lambda function and using an S3 event notification to invoke the Lambda function to process the objects when the company uploads the objects. This solution meets the company's requirements of high availability and scalability, as well as reducing long-term management overhead, and is likely to be the most cost-effective option.\n\nOption B involves creating an SQS queue and configuring S3 to send event notifications to it. The data processing script would then poll the SQS queue and process the S3 objects that the SQS message identifies. While this option also provides high availability and scalability, it is less cost-effective than using Lambda, as it requires additional resources such as an SQS queue and an EC2 Auto Scaling group."},{"poster":"zhangyu20000","content":"A is correct, it provide HA, scale, less management. Task only need 5 minutes\nB: enen more complex\nC: container still run on one EC2, not scale\nd: need container, Farget and Lambda. Complex than A","upvote_count":"7","comment_id":"776962","timestamp":"1673809560.0"},{"content":"Selected Answer: A\n+----------------------+\n| Amazon S3 |\n| (Stores Uploaded Files)|\n+----------------------+\n |\n | (S3 Event Notification)\n v\n+----------------------+\n| AWS Lambda |\n| (Processes Files) |\n+----------------------+\n |\n | (Processed Data)\n v\n+----------------------+\n| Amazon S3 |\n| (Stores Processed Data)|\n+----------------------+","comment_id":"1364205","timestamp":"1740997980.0","upvote_count":"1","poster":"albert_kuo"},{"upvote_count":"1","poster":"SIJUTHOMASP","comment_id":"1316829","content":"Option with Lambda would be more reasonable because the rational behind the cost on the solution would be triggering lambda on the S3 event. The current behaviour is 40% of EC2 being not utilised so that Option D to run in ECS with Fargate would be costlier than Lambda option here","timestamp":"1732401240.0"},{"timestamp":"1725095460.0","poster":"amministrazione","upvote_count":"1","comment_id":"1275524","content":"A. Migrate the data processing script to an AWS Lambda function. Use an S3 event notification to invoke the Lambda function to process the objects when the company uploads the objects."},{"comment_id":"1264602","timestamp":"1723459020.0","upvote_count":"1","content":"Selected Answer: A\ninstead of scheduled 10 min with multiple files processing (each takes 5 minutes) it will be event driven with lambda each time a file is uploaded --> Answer A","poster":"MAZIADI"},{"poster":"zolthar_z","timestamp":"1721307900.0","upvote_count":"2","comment_id":"1250415","content":"Selected Answer: A\nA, the company wants to reduce management overhead not costs, we should stay with the question requirement, it doesn't said anything about cost, probably D will be cheaper but the solution must resolve the question necessity and is reduce long-term management overhead"},{"timestamp":"1718655660.0","upvote_count":"1","poster":"Helpnosense","content":"Selected Answer: D\nI vote D because the service is from EC2 to lambda and work is processing data. Without given how big is the data we can't assume that the data is always below the lambda ephemeral storage limit 0.5GB. Nowadays, a file can easily break 0.5GB.\nWhile D is still EC2 based so whatever previous EC2 can do farget can do as well.","comment_id":"1232108"},{"poster":"Shenannigan","comment_id":"1229528","upvote_count":"2","timestamp":"1718232000.0","content":"Selected Answer: A\nThe answer is A:\n\nAWS Pricing Calculator \n(using:\n10,000 request per month,\n300,000 ms which = 5 minutes\n128 MB of Memory\n512 MB of Storage\n)\n\nAmount of memory allocated: 128 MB x 0.0009765625 GB in a MB = 0.125 GB\nAmount of ephemeral storage allocated: 512 MB x 0.0009765625 GB in a MB = 0.5 GB\nPricing calculations\n10,000 requests x 300,000 ms x 0.001 ms to sec conversion factor = 3,000,000.00 total compute (seconds)\n0.125 GB x 3,000,000.00 seconds = 375,000.00 total compute (GB-s)\n375,000.00 GB-s x 0.0000166667 USD = 6.25 USD (monthly compute charges)\n10,000 requests x 0.0000002 USD = 0.00 USD (monthly request charges)\n0.50 GB - 0.5 GB (no additional charge) = 0.00 GB billable ephemeral storage per function\nLambda costs - Without Free Tier (monthly): 6.25 USD\n\nFor those thinking D is the cheaper option, do you really believe ECS Fargate would be cheaper?"},{"timestamp":"1711536300.0","upvote_count":"3","content":"Selected Answer: D\nOk i was thinking between A and D.\nI'm pretty sure which is D our answer, see the details.\n\nThe requirements are:\n- COST as much as possible low\n- OPERATIONS as much as possible managed.\n\nSo at the first reading, the A option seems to be the correct option (because it's totally AWS managed), but here we're totally forgot the cost.\nRunning a Lambda function, for 5 minutes every 10 minutes, it's very very more expensive than a simple ECS task running continously.\n\nFinally, ECS in fargate mode is totally AWS managed, so we will have lower cost, and a serverless and HA environment, which auto-scale if we need more processing at time.\n\nFor me, option D is the correct answer.","comment_id":"1184004","poster":"red_panda"},{"comment_id":"1175669","timestamp":"1710663540.0","upvote_count":"1","content":"Selected Answer: A\nA, use lambda function is much cost-effective than use ECS Margate","poster":"gofavad926"},{"content":"Selected Answer: A\nOption A is the most cost-effective and efficient solution. AWS Lambda allows for running code in response to triggers such as S3 event notifications without the need to manage servers, thereby directly addressing the requirement to reduce long-term management overhead. Since the script is only needed when new files are uploaded and takes about 5 minutes to process each file, Lambda’s ability to scale automatically and its billing model based on actual compute time used make it an ideal solution. Lambda can process files immediately upon upload, maximizing efficiency and minimizing idle time.\nOption D proposes using Amazon ECS on AWS Fargate with Lambda to trigger tasks. This solution introduces container orchestration, which can improve scalability and reduce some management overhead. However, it is not as cost-effective as directly invoking a Lambda function to process files, considering the lightweight nature of the task and the added complexity of managing container orchestration and Lambda functions together.","comment_id":"1146441","poster":"8608f25","upvote_count":"1","timestamp":"1707583740.0"},{"comment_id":"1137793","content":"Selected Answer: D\n100% the answer is D. \n5 minutes to process EACH FILE? And the EC2 instance is processing files 60% of the time? \nLambda would be crazy expensive in this scenario. ECS/Fargate = cheaper for sure. See link in @covabix879 comment for proof of this.\n\nGreyeye said something rather ridiculous: \"If you get 1000 images, you will see 1000 tasks. That is not economical or cheap.\"\n\nHow can 1x EC2 instance running a script every 10 minutes process 1000 images with each one taking 5 minutes? Even if the script processed images in parallel, e.g. one image per vCPU at a time, that instance would need 500 vCPUs! For the EC2 instance to be idle 40% of the time, it would need 833 vCPUs. That's ridiculous. \n\nBut even if 1000 images suddenly appeared, the Lambda solution would still result in 1000 Lambdas all firing and running for 5 minutes each. Which is going to be more expensive than ECS/Fargate.","timestamp":"1706808660.0","poster":"LazyAutonomy","upvote_count":"2"},{"timestamp":"1705046100.0","comment_id":"1120535","poster":"ninomfr64","upvote_count":"1","content":"Selected Answer: A\nA = correct\nB = does not reduce long-term management overhead\nC = does not reduce long-term management overhead\nD = does not reduce long-term management overhead\n\nNote: D is a cheap options as mentioned by other here below could be cheaper than A. However, in addition to maintaining the script code it requires to maintain the container image and the lambda"},{"content":"Selected Answer: A\nin the real world it might be D, but with provided details and keeping in mind lambda retries in case of A, I would vote for A.","timestamp":"1699941420.0","upvote_count":"1","poster":"severlight","comment_id":"1070041"},{"content":"Selected Answer: A\nD is more complex and overload for administration. Hence Vote for A","comment_id":"1053876","poster":"Sandeep_B","upvote_count":"1","timestamp":"1698251880.0"},{"timestamp":"1696159620.0","content":"Selected Answer: D\nhttps://blogs.perficient.com/2021/06/17/aws-cost-analysis-comparing-lambda-ec2-fargate/ Even Fargate running continuosly is cheaper than Lambda running half of the time. So long running work load not cost effective with Lambda ( Every 10 minutes run for 5 minutes. So half of the time lambda is running) Therefore Fargate is the most cost-effective solution.","poster":"covabix879","comment_id":"1022206","upvote_count":"5"},{"timestamp":"1694372400.0","upvote_count":"2","comment_id":"1004252","content":"running lambda for 5 minutes is not cost effective, so answer is D","poster":"kjcncjek"},{"content":"Selected Answer: A\nI vote A\n\nD will invoke a new Fargate task per every PUT command. \nIf you get 1000 images, you will see 1000 tasks. That is not economical or cheap.\n\nif D was invoking a new task by other means like EventBridge, this would have been a lot cheaper.","upvote_count":"1","comment_id":"985095","poster":"Greyeye","timestamp":"1692438660.0"},{"content":"Selected Answer: A\nI prefer A","poster":"CuteRunRun","upvote_count":"1","comment_id":"975427","timestamp":"1691489100.0"},{"comment_id":"973293","content":"Selected Answer: A\nI would go with A as well. According to Olabiba:\n\"Yes, option A would generally be more cost-effective than option D. \n\nIn option A, you would migrate the data processing script to an AWS Lambda function, which has a pay-per-use pricing model. You would only pay for the actual number of requests and the duration of the function execution. This can be more cost-effective for short-duration tasks like processing files.\n\nOn the other hand, in option D, you would migrate the data processing script to a container image that runs on Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. Fargate has a different pricing model, where you pay for the vCPU and memory resources allocated to your containers. This can be more expensive compared to the pay-per-use model of AWS Lambda.\"","poster":"chico2023","timestamp":"1691260320.0","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: A\nit's A","poster":"NikkyDicky","comment_id":"941137","timestamp":"1688320500.0"},{"content":"Selected Answer: A\nThe question is about the most cost effective, the lambda choice (A) is appropriate because the task will run for around 5 minutes and lambdas have a time limit of 15 minutes. If the task took more than 15 minutes then the option D would be appropriate for the scalability, availability and cost effectiveness.","comment_id":"891581","timestamp":"1683479520.0","upvote_count":"2","poster":"rbm2023"},{"comment_id":"887796","timestamp":"1683052500.0","content":"Selected Answer: D\nI Actually Like Fargate Answer. AWS Lambda is expensive if you're using it for regularly occurring, long-running processes that do not to take advantage of the very short scaling time the service provides. SInce it is going to run for 5 min for every 10 min it roughly going to be active 50 % of the time. Anyway it could be cheaper Look into these analysis https://blogs.perficient.com/2021/06/17/aws-cost-analysis-comparing-lambda-ec2-fargate/ https://sixfeetup.com/blog/cost-to-run-aws-lambda-function-all-the-time","upvote_count":"4","poster":"sergza"},{"timestamp":"1681432140.0","content":"If the process takes less than 15 mins, the answer is usually lambda","comment_id":"869855","comments":[{"content":"Correct. Anytime you see a process or a batch script that needs to be moved off EC2 and execution time is less than 15 min, your best bet is that Lambda is the answer.","timestamp":"1686706740.0","comment_id":"922661","upvote_count":"2","poster":"SmileyCloud"}],"poster":"devopsy","upvote_count":"2"},{"comment_id":"850965","poster":"mfsec","timestamp":"1679831880.0","content":"Selected Answer: A\nMigrate the data processing script to an AWS Lambda function.","upvote_count":"1"},{"timestamp":"1679737800.0","poster":"Asagumo","upvote_count":"4","comment_id":"849977","content":"Selected Answer: D\nThere are two points of concern when choosing Lambda in the following two ways\nThe fact that the original EC2 specs are so fast that it may take only 5 minutes to complete.\nThe fact that the average time is only 5 minutes, so there may be cases where the time exceeds 15 minutes."},{"poster":"dev112233xx","upvote_count":"1","content":"Selected Answer: A\nA best practice to handle files in S3","timestamp":"1679101860.0","comment_id":"842398"},{"timestamp":"1678057380.0","comment_id":"830409","comments":[{"comment_id":"830413","poster":"hobokabobo","comments":[{"upvote_count":"1","content":"how would anyone say EC2 overhead is less than lambda?\nYou need to manage a server, patch them, nurture them and when service crashes for any reason, you need to restart for some means. (need monitoring.)\n\nI agree Lambda cannot be used for 100% of usecases but I would make stuff run on lambda over managing EC2 any day. (I hate shit running on ec2 and fail)","comment_id":"985090","poster":"Greyeye","timestamp":"1692438060.0"}],"upvote_count":"1","content":"Also \"long-term management overhead\" should be reduced. Ec2 the long term management overhead is way lower than maintaining Lambda.","timestamp":"1678057920.0"}],"upvote_count":"2","poster":"hobokabobo","content":"It asks for the most cost effective solution.\nWhile Lambda may be simple and cheap for if you have only a few invocations and low memory requirements. \nAs processing is called every 10 minutes. The EC2 is indeed idle for 40% of the time, 60% of the time its under load. But we are asked to look at how it scales - in regards to cost.\nWe have a 60% used EC2. Lambda costs explode when it scales.\nLambda is the by far most expensive solution.\nB) is more cost effective.\n(Who votes for Lambda when it comes to cost for processing big load, never had to pay the AWS bill for it.)"},{"comment_id":"827456","poster":"God_Is_Love","upvote_count":"1","comments":[{"comment_id":"827458","upvote_count":"1","timestamp":"1677792540.0","poster":"God_Is_Love","content":"D is not cost effective and not good.. (meant C in above comment)"}],"content":"Selected Answer: A\nA and D are good but A is most cost effective as asked in question. B has only one instance that means not highly available. C has container/ec2 combo with more work on ec2 which is cost ineffective and more operating effort.","timestamp":"1677792480.0"},{"poster":"zozza2023","upvote_count":"2","comment_id":"793296","timestamp":"1675110540.0","content":"Selected Answer: A\nthe script takes approximately 5 minutes==>Lamda is the simpliest soltion (compared to D)"}],"url":"https://www.examtopics.com/discussions/amazon/view/95287-exam-aws-certified-solutions-architect-professional-sap-c02/","timestamp":"2023-01-14 16:55:00","isMC":true,"answer_images":[],"answer":"A","exam_id":33,"question_text":"A company runs a Python script on an Amazon EC2 instance to process data. The script runs every 10 minutes. The script ingests files from an Amazon S3 bucket and processes the files. On average, the script takes approximately 5 minutes to process each file The script will not reprocess a file that the script has already processed.\n\nThe company reviewed Amazon CloudWatch metrics and noticed that the EC2 instance is idle for approximately 40% of the time because of the file processing speed. The company wants to make the workload highly available and scalable. The company also wants to reduce long-term management overhead.\n\nWhich solution will meet these requirements MOST cost-effectively?","choices":{"C":"Migrate the data processing script to a container image. Run the data processing container on an EC2 instance. Configure the container to poll the S3 bucket for new objects and to process the resulting objects.","B":"Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure Amazon S3 to send event notifications to the SQS queue. Create an EC2 Auto Scaling group with a minimum size of one instance. Update the data processing script to poll the SQS queue. Process the S3 objects that the SQS message identifies.","D":"Migrate the data processing script to a container image that runs on Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. Create an AWS Lambda function that calls the Fargate RunTaskAPI operation when the container processes the file. Use an S3 event notification to invoke the Lambda function.","A":"Migrate the data processing script to an AWS Lambda function. Use an S3 event notification to invoke the Lambda function to process the objects when the company uploads the objects."},"topic":"1","unix_timestamp":1673711700,"answers_community":["A (69%)","D (31%)"],"question_images":[],"answer_ET":"A"}],"exam":{"isImplemented":true,"isMCOnly":true,"provider":"Amazon","numberOfQuestions":529,"id":33,"lastUpdated":"11 Apr 2025","name":"AWS Certified Solutions Architect - Professional SAP-C02","isBeta":false},"currentPage":99},"__N_SSP":true}