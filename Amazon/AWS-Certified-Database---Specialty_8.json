{"pageProps":{"questions":[{"id":"UEZD6PEeL6UL7blsTN9A","timestamp":"2021-04-09 13:39:00","answer_description":"","choices":{"B":"Create an RDS for MySQL DB instance with an AWS managed CMK. Create a new key policy to include the Amazon Resource Name (ARN) of the other AWS accounts as a principal, and then allow the kms:CreateGrant action.","C":"Create an RDS for MySQL DB instance with an AWS owned CMK. Create a new key policy to include the administrator user name of the other AWS accounts as a principal, and then allow the kms:CreateGrant action.","D":"Create an RDS for MySQL DB instance with an AWS CloudHSM key. Update the key policy to include the Amazon Resource Name (ARN) of the other AWS accounts as a principal, and then allow the kms:CreateGrant action.","A":"Create an RDS for MySQL DB instance with an AWS Key Management Service (AWS KMS) customer managed CMK. Update the key policy to include the Amazon Resource Name (ARN) of the other AWS accounts as a principal, and then allow the kms:CreateGrant action."},"question_text":"A company wants to migrate its on-premises MySQL databases to Amazon RDS for MySQL. To comply with the company's security policy, all databases must be encrypted at rest. RDS DB instance snapshots must also be shared across various accounts to provision testing and staging environments.\nWhich solution meets these requirements?","isMC":true,"unix_timestamp":1617968340,"discussion":[{"content":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ShareSnapshot.html - A","timestamp":"1632409380.0","poster":"Zhongkai","upvote_count":"5","comment_id":"356943"},{"content":"Selected Answer: A\n- KMS Customer managed CMK: The key is stored in your account that you created, own, and manage.\n- KMS AWS managed CMK: The key is stored in your account and is managed by AWS Key Management Service.\n\nSo you should choose Customer managed CMK","poster":"Hisayuki","comment_id":"1112453","upvote_count":"1","timestamp":"1704251160.0"},{"timestamp":"1651275300.0","content":"Selected Answer: A\n-Need customer managed CMK for sharing (B and C are out), C is also out for administrator user\n- AWS CloudHSM key is also AWS generated (D is out)\n- policy update to include ARN of other AWS accounts as principal\n- CreateGrant action","poster":"novice_expert","comment_id":"594727","upvote_count":"4"},{"content":"Selected Answer: A\nA!\n\nYou can't share a snapshot that has been encrypted using the default KMS key of the AWS account that shared the snapshot, therefore it must be encrypted with a customer managed key. https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ShareSnapshot.html","upvote_count":"4","timestamp":"1647214860.0","comment_id":"567254","poster":"Dantas"},{"comment_id":"562564","content":"Got this question in my exam. (i cleared it). A is correct","upvote_count":"1","poster":"RotterDam","timestamp":"1646652240.0"},{"content":"A: To allow another AWS account access to a KMS key, update the key policy for the KMS key. You update it with the Amazon Resource Name (ARN) of the AWS account that you are sharing to as Principal in the KMS key policy. Then you allow the kms:CreateGrant action.","upvote_count":"2","poster":"kped21","timestamp":"1645753860.0","comment_id":"555686"},{"comment_id":"555603","timestamp":"1645740900.0","content":"Selected Answer: A\nKMS managed CMK","upvote_count":"1","poster":"tugboat"},{"content":"Selected Answer: A\nCMK is needed for data share and you just need to update the key policy.","poster":"jove","timestamp":"1638042780.0","comment_id":"488472","upvote_count":"2"},{"upvote_count":"1","timestamp":"1634382240.0","comment_id":"433896","content":"Agree with A","poster":"ChauPhan"},{"content":"A final answer","upvote_count":"2","timestamp":"1633885200.0","comment_id":"361578","poster":"Aesthet"},{"upvote_count":"3","timestamp":"1632076800.0","poster":"shantest1","content":"A. Answer\nKey to the answer CMK - Customer managed Key - If I am not wrong","comment_id":"331915"}],"question_images":[],"answer_ET":"A","answers_community":["A (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/49707-exam-aws-certified-database-specialty-topic-1-question-130/","answer_images":[],"question_id":36,"answer":"A","exam_id":22,"topic":"1"},{"id":"pnAH8mEPSqvZ4AoNThSA","exam_id":22,"isMC":true,"choices":{"A":"Use AWS Glue to crawl the data in the DynamoDB table. Create a job using an available blueprint to export the data to Amazon S3. Import the data from the S3 file to a DynamoDB table in the new account.","C":"Use AWS Data Pipeline in the current account to export the data from the DynamoDB table to a file in Amazon S3. Use Data Pipeline to import the data from the S3 file to a DynamoDB table in the new account.","B":"Create an AWS Lambda function to scan the items of the DynamoDB table in the current account and write to a file in Amazon S3. Create another Lambda function to read the S3 file and restore the items of a DynamoDB table in the new account.","D":"Configure Amazon DynamoDB Streams for the DynamoDB table in the current account. Create an AWS Lambda function to read from the stream and write to a file in Amazon S3. Create another Lambda function to read the S3 file and restore the items to a DynamoDB table in the new account."},"discussion":[{"timestamp":"1632305640.0","content":"I think the answer is C\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/dynamodb-cross-account-migration/","comments":[{"upvote_count":"4","poster":"DevoteamAnalytix","comment_id":"628382","content":"For me it is A because it seems to be easier with Glue than Data Pipeline (\"with the MINIMUM amount of administrative work\")\nGLUE: https://aws.amazon.com/de/premiumsupport/knowledge-center/dynamodb-cross-account-migration/\nDATA PIPELINE: https://aws.amazon.com/de/premiumsupport/knowledge-center/data-pipeline-account-access-dynamodb-s3/","timestamp":"1657203960.0"}],"comment_id":"327773","upvote_count":"11","poster":"novak18"},{"upvote_count":"2","content":"Selected Answer: C\nCreate a DynamoDB table in your source account.\n Create an Amazon Simple Storage Service (Amazon S3) bucket in the destination account.\n Attach an AWS Identity and Access Management (IAM) policy to the Data Pipeline default roles in the source account.\n Create an S3 bucket policy in the destination account.\n Create and activate a pipeline in the source account.\n Create a DynamoDB table in the destination account.\n Restore the DynamoDB export in the destination account.\n\nhttps://aws.amazon.com/de/blogs/database/how-to-migrate-amazon-dynamodb-tables-from-one-aws-account-to-another-with-aws-data-pipeline/","timestamp":"1696394460.0","poster":"roymunson","comment_id":"1024447"},{"upvote_count":"1","content":"Selected Answer: C\nThe correct answer is C, because:\nhttps://aws.amazon.com/blogs/database/how-to-migrate-amazon-dynamodb-tables-from-one-aws-account-to-another-with-aws-data-pipeline/","poster":"alexpl","timestamp":"1696182900.0","comment_id":"1022590"},{"content":"Selected Answer: A\nI think it is A","upvote_count":"1","poster":"thuyeinaung","comment_id":"1018788","timestamp":"1695817620.0"},{"content":"Selected Answer: A\nMigrate your DynamoDB table to a different AWS account with one of these methods that suit your use case:\nhttps://repost.aws/knowledge-center/dynamodb-cross-account-migration\n\n\n- AWS Backup\n- DynamoDB import and export to Amazon Simple Storage Service (Amazon S3)\n- Amazon S3 and AWS Glue\n- Amazon EMR","timestamp":"1695487980.0","upvote_count":"1","poster":"Germaneli","comment_id":"1015118"},{"poster":"Monknil","timestamp":"1689821940.0","upvote_count":"1","comment_id":"957068","content":"C looks like the best option\nhttps://aws.amazon.com/premiumsupport/knowledge-center/dynamodb-cross-account-migration/"},{"timestamp":"1686274920.0","comment_id":"918816","upvote_count":"2","poster":"milan9527","content":"Selected Answer: A\nA. Why not?"},{"timestamp":"1682765400.0","comment_id":"884241","comments":[{"timestamp":"1685550480.0","upvote_count":"2","content":"Amazon S3 and AWS Glue is on your list, listed before AWS Data Pipeline, and yet you select C?","poster":"Paulv82003","comment_id":"911418"}],"content":"Selected Answer: C\nYou can migrate your DynamoDB tables to a different AWS account by choosing one of the following methods depending on your use case:\n\nAWS Backup\nDynamoDB import and export to Amazon Simple Storage Service (Amazon S3)\nAmazon S3 and AWS Glue\nAWS Data Pipeline\nAmazon EMR","poster":"clarksu","upvote_count":"1"},{"timestamp":"1679685360.0","upvote_count":"1","poster":"redman50","comment_id":"849575","content":"Selected Answer: D\nConfigure Amazon DynamoDB Streams for the DynamoDB table in the current account. Create an AWS Lambda function to read from the stream and write to a file in Amazon S3. Create another Lambda function to read the S3 file and restore the items to a DynamoDB table in the new account.\nThis approach leverages DynamoDB Streams, which captures item-level modifications to a table, including creates, updates, and deletes. The DynamoDB Streams data can be used to replicate data in near real-time across different AWS accounts. The approach allows for minimal administrative work as it only requires the creation of two Lambda functions to read and write to S3 and DynamoDB tables, respectively. This approach also ensures that data consistency is maintained during the migration process."},{"timestamp":"1677416580.0","poster":"sk1974","upvote_count":"1","content":"https://aws.amazon.com/premiumsupport/knowledge-center/dynamodb-cross-account-migration/ . I initially thought answer was 'A' since answer had the word 'existing blueprint' . but , I went for C based on the link pasted above. Scroll to the 'Data Pipeline' section in there .","comment_id":"822384"},{"content":"DATAPIEPLINE is the correct one \"Note: The destination account can't access the DynamoDB data in S3 bucket. To work with the data, restore it to a DynamoDB table. Data Pipeline provides the easiest method to move the table with the least manual effort. However, there are fewer options for customization.\"","poster":"jjyy80","comment_id":"752545","upvote_count":"2","timestamp":"1671638400.0"},{"content":"Selected Answer: C\nhttps://aws.amazon.com/premiumsupport/knowledge-center/dynamodb-cross-account-migration/\n\nexport dynamoDB to S3 in other account -> use Glue job (or data pipeline or EMR) to import data\n\nC. Use AWS Data Pipeline in the current account to export the data from the DynamoDB table to a file in Amazon S3. Use Data Pipeline to import the data from the S3 file to a DynamoDB table in the new account.","comment_id":"595222","upvote_count":"2","timestamp":"1651345680.0","poster":"novice_expert"},{"content":"Why not A? being glue serverless wouldn't it be easier to do this way?","comment_id":"482050","poster":"johnconnor","upvote_count":"2","timestamp":"1637351340.0"},{"comment_id":"433904","content":"Agree with C, LEAST amount of work.","timestamp":"1635559800.0","poster":"ChauPhan","upvote_count":"2"},{"poster":"AM","content":"I agree with C and also that the question is a bit ambigupus","comment_id":"376318","timestamp":"1635310560.0","upvote_count":"1"},{"timestamp":"1634400540.0","comment_id":"361581","content":"C\nhttps://aws.amazon.com/premiumsupport/knowledge-center/data-pipeline-account-access-dynamodb-s3/","upvote_count":"4","poster":"Aesthet"},{"poster":"manan728","timestamp":"1634032020.0","comment_id":"342566","upvote_count":"2","content":"A seems to be the answer. \nhttps://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-dynamo-db-cross-account.html"},{"upvote_count":"1","poster":"Jaypdv","content":"I would go for C. but I feel the question is ambiguous.","comment_id":"328520","timestamp":"1633834320.0"}],"answer":"C","question_text":"A retail company manages a web application that stores data in an Amazon DynamoDB table. The company is undergoing account consolidation efforts. A database engineer needs to migrate the DynamoDB table from the current AWS account to a new AWS account.\nWhich strategy meets these requirements with the LEAST amount of administrative work?","unix_timestamp":1617514320,"answers_community":["C (55%)","A (36%)","9%"],"url":"https://www.examtopics.com/discussions/amazon/view/49008-exam-aws-certified-database-specialty-topic-1-question-131/","question_id":37,"answer_ET":"C","answer_images":[],"question_images":[],"timestamp":"2021-04-04 07:32:00","topic":"1","answer_description":""},{"id":"1hHvxPvTqdsErucaJhu4","url":"https://www.examtopics.com/discussions/amazon/view/49161-exam-aws-certified-database-specialty-topic-1-question-132/","answers_community":[],"timestamp":"2021-04-05 11:25:00","answer_images":[],"topic":"1","answer_ET":"A","question_id":38,"question_text":"A company uses the Amazon DynamoDB table contractDB in us-east-1 for its contract system with the following schema: orderID (primary key) timestamp (sort key) contract (map) createdBy (string) customerEmail (string)\nAfter a problem in production, the operations team has asked a database specialist to provide an IAM policy to read items from the database to debug the application. In addition, the developer is not allowed to access the value of the customerEmail field to stay compliant.\nWhich IAM policy should the database specialist use to achieve these requirements?\nA.\n//IMG//\n\nB.\n//IMG//\n\nC.\n//IMG//\n\nD.\n//IMG//","exam_id":22,"question_images":["https://www.examtopics.com/assets/media/exam-media/04237/0008100001.png","https://www.examtopics.com/assets/media/exam-media/04237/0008200001.png","https://www.examtopics.com/assets/media/exam-media/04237/0008300001.png","https://www.examtopics.com/assets/media/exam-media/04237/0008400001.png"],"unix_timestamp":1617614700,"answer":"A","isMC":false,"discussion":[{"comment_id":"328522","timestamp":"1633330680.0","poster":"Jaypdv","upvote_count":"9","content":"A. Answer"},{"timestamp":"1673608440.0","comment_id":"774367","content":"A is correct as per IAM guideline","upvote_count":"1","poster":"SachinGoel"},{"poster":"novice_expert","timestamp":"1651421820.0","comment_id":"595657","content":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/specifying-conditions.html\n\nYou use the IAM Condition element to implement a fine-grained access control policy.","upvote_count":"3"},{"timestamp":"1635972900.0","content":"The correct answer is A. you have access to all columns except CustomerEmail!","poster":"learnazureportal","upvote_count":"2","comment_id":"390908"},{"upvote_count":"2","poster":"Aesthet","content":"A final answer","timestamp":"1635571620.0","comment_id":"361583"},{"content":"A.\nC would be correct if using ForAnyValue instead of ForAllValues","poster":"Huy","upvote_count":"3","comment_id":"360977","timestamp":"1634437620.0"}],"answer_description":""},{"id":"C0oYObXF02wGzlblZgkc","discussion":[{"timestamp":"1634413620.0","poster":"Jaypdv","upvote_count":"13","comment_id":"328527","content":"B.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Scan.html#Scan.ParallelScan"},{"poster":"MultiAZ","upvote_count":"1","comment_id":"1121351","timestamp":"1705128600.0","content":"Selected Answer: B\nAnswer is B"},{"upvote_count":"1","content":"Selected Answer: B\nB. Use four threads and parallel DynamoDB API Scan operations.","poster":"Pranava_GCP","comment_id":"994454","timestamp":"1693428360.0"},{"upvote_count":"1","poster":"SachinGoel","comment_id":"774373","content":"Selected Answer: B\nMore thread is a key","timestamp":"1673608740.0"},{"content":"B make sense but this has to done programmatically by the application team to process read by multi threaded application calls","comment_id":"630554","upvote_count":"1","poster":"db2luwdba","timestamp":"1657632420.0"},{"poster":"novice_expert","comment_id":"594049","upvote_count":"2","timestamp":"1651179240.0","content":"Selected Answer: B\n15% means max 6 threads possible, 4 are good"},{"timestamp":"1635523620.0","upvote_count":"1","content":"Every morning, a single-threaded process calls the DynamoDB API Scan operation to scan the entire table ==> B","poster":"ChauPhan","comment_id":"433911"},{"upvote_count":"2","poster":"Aesthet","content":"B final answer","timestamp":"1634823480.0","comment_id":"361584"},{"timestamp":"1634280180.0","upvote_count":"1","comments":[{"timestamp":"1641902460.0","comment_id":"521514","content":"15% utilization of the allocated RCUs does not required to scale the Dynamo DB.","poster":"awsmonster","upvote_count":"2"},{"timestamp":"1640209380.0","content":"No, it is B","comment_id":"507404","upvote_count":"1","poster":"jove"}],"content":"https://aws.amazon.com/blogs/database/amazon-dynamodb-auto-scaling-performance-and-cost-optimization-at-any-scale/#:~:text=To%20configure%20auto%20scaling%20in,alarms%20that%20track%20consumed%20capacity.\n\nAnswer should be A?","comment_id":"327779","poster":"novak18"}],"answers_community":["B (100%)"],"isMC":true,"question_id":39,"question_images":[],"timestamp":"2021-04-04 07:38:00","answer_images":[],"question_text":"A company has an application that uses an Amazon DynamoDB table to store user data. Every morning, a single-threaded process calls the DynamoDB API Scan operation to scan the entire table and generate a critical start-of-day report for management. A successful marketing campaign recently doubled the number of items in the table, and now the process takes too long to run and the report is not generated in time.\nA database specialist needs to improve the performance of the process. The database specialist notes that, when the process is running, 15% of the table's provisioned read capacity units (RCUs) are being used.\nWhat should the database specialist do?","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/49011-exam-aws-certified-database-specialty-topic-1-question-133/","choices":{"C":"Double the table's provisioned RCUs.","A":"Enable auto scaling for the DynamoDB table.","B":"Use four threads and parallel DynamoDB API Scan operations.","D":"Set the Limit and Offset parameters before every call to the API."},"answer_ET":"B","answer_description":"","unix_timestamp":1617514680,"answer":"B","exam_id":22},{"id":"VWQrnnHqTO2AyUrxrgVh","question_text":"A company is building a software as a service application. As part of the new user sign-on workflow, a Python script invokes the CreateTable operation using the\nAmazon DynamoDB API. After the call returns, the script attempts to call PutItem.\nOccasionally, the PutItem request fails with a ResourceNotFoundException error, which causes the workflow to fail. The development team has confirmed that the same table name is used in the two API calls.\nHow should a database specialist fix this issue?","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/49017-exam-aws-certified-database-specialty-topic-1-question-134/","timestamp":"2021-04-04 08:30:00","unix_timestamp":1617517800,"question_id":40,"discussion":[{"upvote_count":"11","comment_id":"327813","content":"C. Answer","timestamp":"1633453200.0","poster":"Jaypdv"},{"comment_id":"1121352","poster":"MultiAZ","upvote_count":"1","timestamp":"1705128780.0","content":"Selected Answer: C\nAnswer C. The error happens OCCCASIONALLY. So you need to wait for the table creation to complete."},{"poster":"Pranava_GCP","content":"Selected Answer: C\nC. Change the application to call DescribeTable periodically until the TableStatus is ACTIVE, then call PutItem. \n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DescribeTable.html\n\n\"ResourceNotFoundException\nThe operation tried to access a nonexistent table or index. The resource might not be specified correctly, or its status might not be ACTIVE.\"","timestamp":"1693432980.0","comment_id":"994482","upvote_count":"1"},{"upvote_count":"1","timestamp":"1688602200.0","poster":"SamDDD","comment_id":"944193","content":"Selected Answer: C\nDynamoDB responds with this error when you're trying to run an operation against non-existent or not active table."},{"poster":"santosrk","content":"Selected Answer: D\nConditionExpression\nA condition that must be satisfied in order for a conditional PutItem operation to succeed.\n\nAn expression can contain any of the following:\n\nFunctions: attribute_exists | attribute_not_exists | attribute_type | contains | begins_with | size\n\nThese function names are case-sensitive.\n\nComparison operators: = | <> | < | > | <= | >= | BETWEEN | IN\n\nLogical operators: AND | OR | NOT","upvote_count":"1","comment_id":"939037","timestamp":"1688123580.0"},{"content":"D. answer. ConditionExpression\nA condition that must be satisfied in order for a conditional PutItem operation to succeed.\n\nAn expression can contain any of the following:\n\nFunctions: attribute_exists | attribute_not_exists | attribute_type | contains | begins_with | size\n\nThese function names are case-sensitive.\n\nComparison operators: = | <> | < | > | <= | >= | BETWEEN | IN\n\nLogical operators: AND | OR | NOT","comment_id":"939036","timestamp":"1688123520.0","poster":"santosrk","upvote_count":"1"},{"poster":"Kodoma","content":"Selected Answer: C\nThe answer is C.","timestamp":"1685641440.0","upvote_count":"1","comment_id":"912316"},{"comment_id":"700135","upvote_count":"1","timestamp":"1666284840.0","poster":"TL12345","content":"Answer is C.\n\nResourceNotFoundException\nThe operation tried to access a nonexistent table or index. The resource might not be specified correctly, or its status might not be ACTIVE.\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_PutItem.html"},{"upvote_count":"1","timestamp":"1656825480.0","poster":"sachin","content":"Condition Expression is used for PutItem DML conditions. you can specify a condition expression to determine which items should be modified. If the condition expression evaluates to true, the operation succeeds; otherwise, the operation fails.\nTo evalute if the update or new enrty should be made if the key attributes are same. \nexample : The PutItem operation overwrites an item with the same key (if it exists). If you want to avoid this, use a condition expression. This allows the write to proceed only if the item in question does not already have the same key.\nC is correct.","comment_id":"626418"},{"upvote_count":"1","content":"Selected Answer: C\nTableStatus should be ACTIVE","timestamp":"1651325640.0","comment_id":"595041","poster":"novice_expert"},{"timestamp":"1635777900.0","poster":"Scunningham99","content":"C https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DescribeTable.html","comment_id":"450032","upvote_count":"3"},{"comment_id":"361585","timestamp":"1635061080.0","upvote_count":"1","poster":"Aesthet","content":"C final answer"},{"timestamp":"1635034020.0","comment_id":"332728","content":"C is the right one","upvote_count":"1","poster":"Chhotu_DBA"}],"answer_images":[],"answer_ET":"C","isMC":true,"answer":"C","answers_community":["C (83%)","D (17%)"],"topic":"1","exam_id":22,"question_images":[],"choices":{"A":"Add an allow statement for the dynamodb:PutItem action in a policy attached to the role used by the application creating the table.","D":"Add a ConditionExpression parameter in the PutItem request.","C":"Change the application to call DescribeTable periodically until the TableStatus is ACTIVE, then call PutItem.","B":"Set the StreamEnabled property of the StreamSpecification parameter to true, then call PutItem."}}],"exam":{"provider":"Amazon","isImplemented":true,"name":"AWS Certified Database - Specialty","lastUpdated":"11 Apr 2025","isBeta":false,"id":22,"isMCOnly":false,"numberOfQuestions":359},"currentPage":8},"__N_SSP":true}