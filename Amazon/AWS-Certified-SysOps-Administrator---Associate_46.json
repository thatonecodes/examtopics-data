{"pageProps":{"questions":[{"id":"JNWTQOo8Tz99wxeXVCII","question_text":"A SysOps administrator needs to update an AWS account name.\n\nWhat should the SysOps administrator do to accomplish this goal?","answers_community":["D (100%)"],"answer_ET":"D","isMC":true,"unix_timestamp":1686348480,"exam_id":34,"answer":"D","answer_images":[],"choices":{"A":"Add the AdministratorAccess policy to the SysOps administrator’s IAM user.","C":"Change the AWS account name through the AWS Trusted Advisor interface.","B":"Add the AWS_ConfigureRole policy to the SysOps administrator’s IAM user.","D":"Sign in as the AWS account root user to make the change."},"answer_description":"","question_id":226,"discussion":[{"timestamp":"1702166880.0","poster":"Gomer","content":"Selected Answer: D\n\"To edit your AWS account name, root user password, or root user email address\"\n\"Minimum permissions\"\n\"To perform the following steps, you must have at least the following IAM permissions:\"\n\"You must sign in as the AWS account root user, which requires no additional IAM permissions. You can't perform these steps as an IAM user or role.\"\nhttps://docs.aws.amazon.com/accounts/latest/reference/manage-acct-update-root-user.html","comment_id":"919657","upvote_count":"11"},{"content":"Selected Answer: D\nThe SysOps admin would have to sign is as root for this task.","timestamp":"1729226400.0","upvote_count":"1","poster":"tgv","comment_id":"1197690"},{"upvote_count":"2","comment_id":"1140250","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/accounts/latest/reference/manage-acct-update-root-user.html","poster":"Learning4life","timestamp":"1722774960.0"}],"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/111776-exam-aws-certified-sysops-administrator-associate-topic-1/","timestamp":"2023-06-10 00:08:00","topic":"1"},{"id":"jYzBJ8KhDhbVWh2k2G5e","timestamp":"2023-06-10 01:07:00","answer_ET":"C","answer":"C","unix_timestamp":1686352020,"exam_id":34,"answers_community":["C (100%)"],"topic":"1","question_images":[],"question_text":"A team of developers is using several Amazon S3 buckets as centralized repositories. Users across the world upload large sets of files to these repositories. The development team's applications later process these files.\n\nA SysOps administrator sets up a new S3 bucket, DOC-EXAMPLE-BUCKET, to support a new workload, The rew S3 bucket also receives regular uploads cf large sets of files from users worldwide. When the new S3 bucket is put into production, the upload performance from certain geographic areas is lower than the upload performance that the existing $3 buckets provide\n\nWhat should the SysOps administrator do to remediate this issue?","discussion":[{"timestamp":"1717974420.0","content":"Selected Answer: C\nAfter researching this realize that the \"s3-accelerate\" domain in the special URL (e.g.: \"acloudguru.s3-accelerate.amazonaws.com\") will resolve to the nearest edge globally, and therefore there is only one url that can be used globally to make use of transfer acceleration, albeit at some additional cost for the data transfer. Setting must be enabled on bucket though. The URLs are worth reviewing (at least they were to me)\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/transfer-acceleration-examples.html\nhttps://www.javatpoint.com/aws-s3-transfer-acceleration\netc.","comment_id":"919667","poster":"Gomer","upvote_count":"12"},{"timestamp":"1730290140.0","poster":"walala97","content":"Selected Answer: C\nmultipart upload ,not a solution for global performance issues,so D is out","comment_id":"1057632","upvote_count":"4"}],"answer_images":[],"choices":{"D":"Use S3 multipart upload for the new S3 bucket. Verify that the developers are using Region-specific S3 endpoint names such as DOC-EXAMPLE-BUCKETS3, [Region] amazonaws.com in their API calls.","C":"Enable S3 Transfer Acceleration for the new S3 bucket. Verify that the developers are using the DOC-EXAMPLE-BUCKET.s3-accelerate.amazonaws.com endpoint name in their API calls.","A":"Provision an Amazon ElastiCache for Redis cluster for the new S3 bucket. Provide the developers with the configuration endpoint of the cluster for use in their API calls","B":"Add the new S3 bucket to a new Amazon CloudFront distribution. Provide the developers with the domain name of the new distribution for use in their API calls."},"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/111780-exam-aws-certified-sysops-administrator-associate-topic-1/","answer_description":"","question_id":227},{"id":"HMqBbC74h6AH5iH9jy0r","exam_id":34,"unix_timestamp":1686360540,"choices":{"A":"Use tags to identify development instances and production instances. In Patch Manager, create two patch groups and one patch baseline. Add an auto-approval delay to each patch group. Create a single maintenance window.","D":"Use tags to identify development instances. In Patch Manager, create one patch group and one patch baseline. Specify auto-approval delays in the patch baseline, Add development instances to the new patch group. Use predefined Patch Manager patch baselines for all remaining instances. Create a single maintenance window.","B":"Use tags to identify development instances and production instances. In Patch Manager, create two patch groups and two patch baselines. Specify an auto-approval delay in each of the patch baselines. Create a single maintenance window.","C":"Use tags to identity development instances and production instances. In Patch Manager, create two patch groups and one patch baseline, Create two separate maintenance windows, each with an auto-approval delay."},"timestamp":"2023-06-10 03:29:00","answers_community":["B (100%)"],"topic":"1","question_images":[],"answer":"B","answer_description":"","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/111792-exam-aws-certified-sysops-administrator-associate-topic-1/","isMC":true,"question_text":"A SysOps administrator wants to use AWS Systems Manager Patch Manager to automate the process of patching Amazon EC2 Windows instances. The SysOps administrator wants to ensure that patches are auto-approved 2 days after the release date for development instances. Patches also must be auto-approved 5 days after the release date for production instances. Maintenance must occur only during a 2-hour window for all instances.\n\nWhich solution will meet these requirements?","question_id":228,"discussion":[{"upvote_count":"17","timestamp":"1717982940.0","content":"Selected Answer: B\nBased requirements and my analyzis of link, I concluded \"B\" was only plausible solution:\nTwo patch groups using same maintenance window (the patch groups defines maintenance Windows)\n- Development 2-hour maintenance window (same as Production)\n- Production 2-hour maintenance window (same as Development) \nTwo patch baselines with individual Auto-Approval rules with unique Auto-Approval Delays (the patch baseline defines rules for auto-approving patches, which rules can contain auto-approval delays)\n- Development Auto-approval Rule to auto-approved 2 days after the release date development instances (unique Auto-Approval Delay)\n- Producton Production Auto-approval Rule to auto-approved 5 days after the release date development instances (unique Auto-Approval Delay)\nhttps://aws.amazon.com/blogs/mt/patching-your-windows-ec2-instances-using-aws-systems-manager-patch-manager/","poster":"Gomer","comment_id":"919727"},{"upvote_count":"2","comment_id":"1073252","content":"Selected Answer: B\nhttps://aws.amazon.com/blogs/mt/patching-your-windows-ec2-instances-using-aws-systems-manager-patch-manager/#:~:text=Multi%2Dpatching%20approaches,following%20CLI%20command%3A\n\nI originally thought one baseline could be used with multiple rules applied to it. AWS docs highlight this multi patching approach for designated environments.","poster":"DeaconStJohn","timestamp":"1731842640.0"},{"poster":"breadops","comment_id":"1040085","content":"Selected Answer: B\nB fulfils all requirements","timestamp":"1728612720.0","upvote_count":"2"},{"content":"Selected Answer: B\nThis solution correctly uses two patch baselines—one for development instances and one for production instances.","comment_id":"985078","poster":"jipark","timestamp":"1724058540.0","upvote_count":"2"}],"answer_ET":"B"},{"id":"OqjqrD4tYxtJZr3SQy1x","answer_description":"","choices":{"A":"Use S3 Select to write a query to search for errors. Run the query across all log groups of interest.","C":"Use Amazon CloudWatch Logs Insights to write a query to search for errors. Run the query across all log groups of interest.","D":"Use Amazon CloudWatch Contributor Insights to create a rule. Apply the rule across all log groups of interest.","B":"Create an AWS Glue processing job to index the logs of interest. Run a query in Amazon Athena to search for errors."},"question_images":[],"answer_images":[],"question_id":229,"discussion":[{"timestamp":"1708716240.0","comment_id":"988516","upvote_count":"8","content":"Selected Answer: B\nI vote for B.\nSeems to me this is the right answer.\nhttps://docs.aws.amazon.com/athena/latest/ug/glue-athena.html\n\nWhy not A\nLooks like S3 select has several limitations.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/selecting-content-from-objects.html\nAccording to the question \"Errors sometimes do not appear in the same field\". So I assume we need more advanced tool.\n\nWhy not C,D\nBoth Amazon CloudWatch Logs Insights and Amazon CloudWatch Contributor Insights process logs from AWS CloudWatch log groups. But in our case we have logs stored on S3.\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/ContributorInsights-CreateRule.html\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html","comments":[{"timestamp":"1711572900.0","comment_id":"1019166","content":"agree with you","poster":"TwinSpark","upvote_count":"1"}],"poster":"xSohox"},{"poster":"TwinSpark","content":"Selected Answer: B\n\"AWS Glue is a serverless data integration service that can discover, prepare, and combine data for analytics, machine learning, and application development. Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL.\" from Freshtimi down here\nTo query logs stored in Amazon S3, you may need to use Amazon Athena, which is a service designed for querying data in S3. You can use AWS Glue to catalog and prepare the logs in S3, and then query them using Athena.\nhttps://docs.aws.amazon.com/athena/latest/ug/glue-athena.html","upvote_count":"5","timestamp":"1711573260.0","comment_id":"1019172"},{"comment_id":"1204054","poster":"mestule","upvote_count":"3","timestamp":"1730217600.0","content":"Selected Answer: B\nCloudWatch Logs Insights cannot directly analyze Lambda logs stored in S3. CloudWatch Logs Insights works with logs stored within CloudWatch Logs service, not S3 buckets."},{"upvote_count":"2","content":"Selected Answer: B\nB makes the most sense.","timestamp":"1725448320.0","poster":"March2023","comment_id":"1165587"},{"poster":"TareDHakim","upvote_count":"1","timestamp":"1720334940.0","content":"Selected Answer: C\nC is the MOST operationally efficient way to analyze the log files","comment_id":"1115656"},{"poster":"r2c3po","content":"Selected Answer: C\nAmazon CloudWatch Logs Insights is specifically designed for analyzing log data efficiently. It allows you to run queries on your log data using a powerful and flexible query language. Given that the logs are in JSON format and the errors have a common string prefix, CloudWatch Logs Insights is well-suited for this task.\n\nOption C provides an operationally efficient solution as it directly addresses log analysis needs without requiring additional processing or indexing. With CloudWatch Logs Insights, you can write queries to filter and analyze log data, making it easier to identify errors across multiple log groups.","comment_id":"1108361","timestamp":"1719631560.0","upvote_count":"1"},{"timestamp":"1717664460.0","upvote_count":"2","comment_id":"1089227","content":"Selected Answer: B\nB is the correct one, if the s3 was not mentioned it might be C then.","poster":"Hatem08"},{"timestamp":"1715302860.0","poster":"8Wire","comment_id":"1066913","content":"Answer: C","upvote_count":"1"},{"timestamp":"1713237360.0","comment_id":"1044618","content":"Selected Answer: C\nCloudWatch Logs Insights automatically discovers fields in logs from AWS services such as Amazon Route 53, AWS Lambda, AWS CloudTrail, and Amazon VPC, and any application or custom log that emits log events as JSON.","comments":[{"timestamp":"1717664400.0","comment_id":"1089226","poster":"Hatem08","content":"but logs are stored on s3 as per the question, so Answer is B for me","upvote_count":"2"}],"poster":"callspace","upvote_count":"2"},{"timestamp":"1708635720.0","comment_id":"987701","content":"Selected Answer: C\njust overall from what i've seen if you don't have to don't \"mix\" services in aws. it makes things more complicated. Which is why I chose C. CW logs insights is made for quering CW log. If not C then B.","upvote_count":"2","poster":"Bhrino"},{"comment_id":"975970","poster":"JamesF92","content":"B is correct. Question is asking for MOST operationally efficient way. A and C, D require you to run the queries manually across 10 different logs. B is right because you can use Glue to create a AWS Glue Data Catalog ( a logical single data set to query ... made up from the 10 different logs. Then Athena will query the whole Data Catalog. A, C would work, but they are not the most efficient way. https://docs.aws.amazon.com/athena/latest/ug/glue-best-practices.html","upvote_count":"1","timestamp":"1707427920.0"},{"timestamp":"1704930300.0","comment_id":"948398","upvote_count":"1","content":"Selected Answer: C\n\"CloudWatch Logs Insights automatically discovers fields in logs from AWS services such as Amazon Route 53, AWS Lambda, AWS CloudTrail, and Amazon VPC, and any application or custom log that emits log events as JSON.\"\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html","poster":"alexpaw"},{"content":"Selected Answer: C\nC is correct","upvote_count":"1","comment_id":"944436","timestamp":"1704536820.0","poster":"ctd983"},{"upvote_count":"2","timestamp":"1703118120.0","comment_id":"928847","poster":"jas26says","content":"Ans is B\n\nhttps://docs.aws.amazon.com/athena/latest/ug/glue-athena.html"},{"comment_id":"927767","content":"Selected Answer: A\nSee my other comments","upvote_count":"1","timestamp":"1703014260.0","poster":"Gomer"},{"upvote_count":"3","comments":[{"timestamp":"1703013660.0","comments":[{"content":"https://docs.aws.amazon.com/athena/latest/ug/glue-athena.html","upvote_count":"1","comment_id":"975973","timestamp":"1707428040.0","poster":"JamesF92"}],"upvote_count":"2","comment_id":"927753","content":"I know that Athena can parse logs in S3 natively (with out Glue). I'd like to see a reference for using Glue working with Athena. I couldn't find anything showing how Glue works or integrates with Athena to parse logs stored in s3.","poster":"Gomer"}],"poster":"Freshtimi","timestamp":"1702566060.0","comment_id":"923217","content":"B. Create an AWS Glue processing job to index the logs of interest. Run a query in Amazon Athena to search for errors.\nSince the logs are stored in Amazon S3, using AWS Glue to create a processing job to index the logs and then querying them using Amazon Athena is the most operationally efficient way to analyze the log files. AWS Glue is a serverless data integration service that can discover, prepare, and combine data for analytics, machine learning, and application development. Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. This combination allows the SysOps administrator to efficiently search for errors across the logs from multiple AWS Lambda functions."},{"poster":"Gomer","comment_id":"921644","comments":[{"comment_id":"975976","upvote_count":"1","poster":"JamesF92","timestamp":"1707428160.0","content":"the question clearly says the logs are in S3. So it's irrelevant that Lambda logs are stored in Cloudwatch by default. Somehow they got in S3."},{"comment_id":"921645","timestamp":"1702412520.0","content":"https://docs.aws.amazon.com/AmazonS3/latest/userguide/selecting-content-from-objects.html","poster":"Gomer","upvote_count":"1"}],"content":"Problem I have is that Lambda by default stores logs in CloudWatch not S3. CloudWatch Logs Insights is a tool for parsing CloudWatch logs, and NOT S3 objects. Parsing S3 CSV/JSON requires Athena, or S3 Select. I agree CloudWatch Logs Insights seems like the tool to use to query a log for a Lambda error, but I also think that applies to Cloudwatch logs and not CSV/JSON logs stored in S3. \"S3 Select\" looks pretty limited compared to CloudWatch Logs insights. I agree A is not most efficient way, but it also seems CloudWatch Logs Insights (B) is not a tool that can be used to parse JSON/CSV objects stored in S3. We need a seasoned AWS person to weigh in here.","timestamp":"1702412460.0","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: C\ni go for C too, A is not the most efficent way","timestamp":"1702316340.0","comments":[{"poster":"Gomer","comment_id":"927756","upvote_count":"1","content":"I'd agree A is not the most efficient way. On the other hand, I can find NO reference that CloudWatch Logs Insights parses S3 files. A CloudWatch log group is an entity in CloudWatch logs, not S3. Since the logs are supposedly stored in S3 (presumably exported from CloudWatch, IMHO, I think the answer is S3 select or Athena (w/out Glue). I think \"A\" is inefficient, but the other answers are not efficient because they don't work with logs stored in S3.","timestamp":"1703014020.0"}],"poster":"CheccoBam","comment_id":"920817"},{"poster":"tts1234","timestamp":"1701897240.0","upvote_count":"1","content":"Leaning more towards C\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_QuerySyntax.html","comment_id":"916582"},{"content":"Selected Answer: C\nI go for C here","upvote_count":"1","comment_id":"916263","timestamp":"1701874920.0","poster":"pepecastr0"}],"topic":"1","exam_id":34,"unix_timestamp":1686056520,"answer":"B","question_text":"A SysOps administrator must analyze Amazon CloudWatch logs across 10 AWS Lambda functions for historical errors. The logs are in JSON format and are stored in Amazon S3. Errors sometimes do not appear in the same field, but all errors begin with the same string prefix.\n\nWhat is the MOST operationally efficient way for the SysOps administrator to analyze the log files?","answers_community":["B (63%)","C (34%)","3%"],"url":"https://www.examtopics.com/discussions/amazon/view/111269-exam-aws-certified-sysops-administrator-associate-topic-1/","isMC":true,"answer_ET":"B","timestamp":"2023-06-06 15:02:00"},{"id":"KP94DVeapuB1AXAVQjUs","answer_images":[],"isMC":true,"question_id":230,"answer":"D","exam_id":34,"topic":"1","question_images":[],"discussion":[{"poster":"joshnort","comment_id":"1188970","content":"Selected Answer: D\nIt's a permissions issue:\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/iam-identity-based-access-control-cwl.html","timestamp":"1727998200.0","upvote_count":"1"},{"comment_id":"1025212","poster":"xile1021","upvote_count":"3","content":"Selected Answer: D\nD\nAmazon EC2 instances need the appropriate permissions to interact with Amazon CloudWatch Logs. These permissions are granted through an IAM (Identity and Access Management) role attached to the EC2 instance.","timestamp":"1712274240.0"},{"upvote_count":"4","poster":"rdiaz","content":"Selected Answer: D\nD is ok","timestamp":"1702899300.0","comment_id":"926593"}],"choices":{"D":"Ensure that the IAM role that is attached to the EC2 instance has permissions in CloudWatch Logs for the CreateLogGroup, CreateLogStream, PutLogEvents, and DescribeLogStreams actions.","B":"Inspect the retention period of the CloudWatch Logs log group. Ensure that the retention period is set to a value that is greater than 1 day.","A":"Configure the AWS CLI on the EC2 instance. Create a cron job that calls the PutLogEvents API operation to push the log files to CloudWatch every 5 minutes.","C":"Set up an Amazon Kinesis data stream that is running in the same AWS Region as the EC2 instance. Configure the CloudWatch agent on the EC2 instance to send CloudWatch events to the data stream."},"unix_timestamp":1687080900,"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/112511-exam-aws-certified-sysops-administrator-associate-topic-1/","question_text":"A company has a policy that all Amazon EC2 instance logs must be published to Amazon CloudWatch Logs. A SysOps administrator is troubleshooting an EC2 instance that is running Amazon Linux 2. The EC2 instance is not publishing logs to CloudWatch Logs. The Amazon CloudWatch agent is running on the EC2 instance, and the agent configuration file is correct.\n\nWhat should the SysOps administrator do to resolve the issue?","answers_community":["D (100%)"],"answer_ET":"D","timestamp":"2023-06-18 11:35:00"}],"exam":{"id":34,"numberOfQuestions":477,"isImplemented":true,"lastUpdated":"11 Apr 2025","isBeta":false,"provider":"Amazon","isMCOnly":false,"name":"AWS Certified SysOps Administrator - Associate"},"currentPage":46},"__N_SSP":true}