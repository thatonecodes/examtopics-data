{"pageProps":{"questions":[{"id":"TGgfgkGlCcYyHfIG3v9I","isMC":true,"answer_description":"Materialized views:\nReference:\nhttps://aws.amazon.com/redshift/faqs/","question_images":[],"exam_id":20,"answer_ET":"D","timestamp":"2021-10-25 05:14:00","question_text":"A company is hosting an enterprise reporting solution with Amazon Redshift. The application provides reporting capabilities to three main groups: an executive group to access financial reports, a data analyst group to run long-running ad-hoc queries, and a data engineering group to run stored procedures and ETL processes. The executive team requires queries to run with optimal performance. The data engineering team expects queries to take minutes.\nWhich Amazon Redshift feature meets the requirements for this task?","topic":"1","discussion":[{"content":"Answer C\nWLM is main feature which supports Concurrency scaling and SQA.","comment_id":"505864","upvote_count":"10","timestamp":"1640068920.0","poster":"lakediver"},{"upvote_count":"6","content":"Answer: C\n\nIn some cases, you might have multiple sessions or users running queries at the same time. In these cases, some queries might consume cluster resources for long periods of time and affect the performance of other queries. For example, suppose that one group of users submits occasional complex, long-running queries that select and sort rows from several large tables. Another group frequently submits short queries that select only a few rows from one or two tables and run in a few seconds. In this situation, the short-running queries might have to wait in a queue for a long-running query to complete. WLM helps manage this situation.","timestamp":"1637049720.0","comment_id":"479232","poster":"polooor"},{"comment_id":"886618","upvote_count":"2","timestamp":"1682966640.0","comments":[{"content":"Are all the questions from here?","upvote_count":"1","timestamp":"1698918060.0","poster":"AidenRoy","comment_id":"1060443"}],"poster":"pk349","content":"C: I passed the test"},{"poster":"cloudlearnerhere","comment_id":"705890","timestamp":"1666900020.0","content":"Correct answer is C as Redshift Workload Management can help define multiple query queues and manage the scheduling between long and short running workloads. \n\nhttps://docs.aws.amazon.com/redshift/latest/dg/cm-c-implementing-workload-management.html\n\nOption A is wrong as Concurrency scaling helps support virtually unlimited concurrent users and concurrent queries, with consistently fast query performance.\n\nOption B is wrong as Short query acceleration (SQA) prioritizes selected short-running queries ahead of longer-running queries.\n\nOption D is wrong as materialized view contains a precomputed result set, based on an SQL query over one or more base tables.","upvote_count":"3"},{"timestamp":"1658805120.0","content":"Selected Answer: C\nAnswer : C","upvote_count":"1","comment_id":"637086","poster":"rocky48"},{"poster":"CloudTimes","comment_id":"613499","content":"Selected Answer: C\nWLM will manage query execution","upvote_count":"1","timestamp":"1654719840.0"},{"content":"Answer-C","comment_id":"595325","poster":"jrheen","timestamp":"1651357140.0","upvote_count":"1"},{"content":"Answer : C","upvote_count":"3","comment_id":"482685","timestamp":"1637424600.0","poster":"aws2019"},{"timestamp":"1636861260.0","content":"Answer : C","poster":"ali98","upvote_count":"2","comment_id":"477918"},{"poster":"srinivasa","comment_id":"467221","timestamp":"1633305360.0","upvote_count":"3","content":"Answer: C\nhttps://docs.aws.amazon.com/redshift/latest/dg/cm-c-implementing-workload-management.html"}],"choices":{"C":"Workload management (WLM)","B":"Short query acceleration (SQA)","A":"Concurrency scaling","D":"Materialized views"},"unix_timestamp":1635131640,"answers_community":["C (100%)"],"answer_images":[],"question_id":31,"answer":"C","url":"https://www.examtopics.com/discussions/amazon/view/64645-exam-aws-certified-data-analytics-specialty-topic-1-question/"},{"id":"xWIAXgL3wv5qv5MLBYL8","timestamp":"2021-10-25 16:08:00","answers_community":["B (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/64703-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer":"B","answer_description":"","answer_images":[],"discussion":[{"content":"Answer: B\nhttps://docs.aws.amazon.com/glue/latest/dg/grouping-input-files.html","poster":"srinivasa","comments":[{"content":"Agree \n\ndf = glueContext.create_dynamic_frame.from_options(\"s3\", {'paths': [\"s3://s3path/\"], 'recurse':True, 'groupFiles': 'inPartition', 'groupSize': '1048576'}, format=\"json\"","poster":"lakediver","timestamp":"1640063340.0","comment_id":"505828","upvote_count":"3"}],"upvote_count":"18","comment_id":"467489","timestamp":"1632590160.0"},{"content":"The data engineering team needs to process those files, convert them into Apache Parquet format---so answer is A?","timestamp":"1709397660.0","upvote_count":"1","comment_id":"1164217","poster":"rajeevramadurai"},{"comment_id":"886620","timestamp":"1682966640.0","content":"B: I passed the test","poster":"pk349","upvote_count":"1"},{"timestamp":"1666900380.0","upvote_count":"4","poster":"cloudlearnerhere","content":"Correct answer is B as the AWS Glue job can be updated to group files to create larger files which can help improve the processing time without any additional steps or changes. \n\nhttps://docs.aws.amazon.com/glue/latest/dg/grouping-input-files.html\n\nOptions A & D are wrong as using a staging space or EMR would add additional steps to the processing.\n\nOption C is wrong as this only performs the loading of data, not processing before the load.","comment_id":"705895"},{"upvote_count":"1","comment_id":"634936","poster":"rocky48","content":"Selected Answer: B\nAnswer: B","timestamp":"1658454420.0"},{"upvote_count":"1","comment_id":"628152","content":"Answer C","timestamp":"1657151700.0","poster":"jealbave"},{"timestamp":"1651355580.0","comment_id":"595309","poster":"jrheen","upvote_count":"1","content":"Answer-B"},{"content":"Selected Answer: B\nhttps://docs.aws.amazon.com/glue/latest/dg/grouping-input-files.html","poster":"Teraxs","upvote_count":"2","comment_id":"594268","timestamp":"1651213680.0"},{"content":"I'm confused between B and C. But I think I will vote for B, since the input files are in json so I think they need to be processed before loading to redshift (flatten the data ...). We need ETL instead of ELT.","comment_id":"557313","timestamp":"1645965000.0","poster":"simo40010","upvote_count":"2"},{"comments":[{"content":"B is right. The question says that input file format is json, output is parquet, so DynamicFrames with groupFiles can help input json format","timestamp":"1640221260.0","comment_id":"507463","upvote_count":"1","poster":"npt"}],"upvote_count":"2","poster":"cynthiacy","timestamp":"1639566720.0","comment_id":"502071","content":"groupFiles is supported for DynamicFrames created from the following data formats: csv, ion, grokLog, json, and xml. This option is not supported for avro, parquet, and orc.\nfrom https://docs.aws.amazon.com/glue/latest/dg/grouping-input-files.html. B is wrong.\nWhy not C?"},{"poster":"aws2019","comment_id":"482583","timestamp":"1637416800.0","upvote_count":"2","content":"This is a weird question as both A and B are equally efficient","comments":[{"timestamp":"1638582840.0","comment_id":"493453","content":"\"most effectively\" might be be A","upvote_count":"1","poster":"lakeswimmer"},{"timestamp":"1644368760.0","poster":"cnmc","content":"A is not as efficient since you're creating an additional step (a \"staging\" location in S3 to store those large files). Not to mention now you're paying double the storage cost","upvote_count":"3","comment_id":"543454"},{"content":"You are correct, but the question says they already have Glue job. So, I would choose B.","timestamp":"1637605680.0","comment_id":"484458","poster":"Olga2022","upvote_count":"5"}]},{"content":"Answer : B","poster":"ali98","timestamp":"1636861140.0","upvote_count":"1","comment_id":"477916"}],"question_images":[],"isMC":true,"answer_ET":"B","topic":"1","question_text":"A global pharmaceutical company receives test results for new drugs from various testing facilities worldwide. The results are sent in millions of 1 KB-sized JSON objects to an Amazon S3 bucket owned by the company. The data engineering team needs to process those files, convert them into Apache Parquet format, and load them into Amazon Redshift for data analysts to perform dashboard reporting. The engineering team uses AWS Glue to process the objects, AWS Step\nFunctions for process orchestration, and Amazon CloudWatch for job scheduling.\nMore testing facilities were recently added, and the time to process files is increasing.\nWhat will MOST efficiently decrease the data processing time?","unix_timestamp":1635170880,"exam_id":20,"choices":{"A":"Use AWS Lambda to group the small files into larger files. Write the files back to Amazon S3. Process the files using AWS Glue and load them into Amazon Redshift tables.","D":"Use Amazon EMR instead of AWS Glue to group the small input files. Process the files in Amazon EMR and load them into Amazon Redshift tables.","C":"Use the Amazon Redshift COPY command to move the files from Amazon S3 into Amazon Redshift tables directly. Process the files in Amazon Redshift.","B":"Use the AWS Glue dynamic frame file grouping option while ingesting the raw input files. Process the files and load them into Amazon Redshift tables."},"question_id":32},{"id":"4mqhryPCM1aIlggy8XAH","timestamp":"2021-10-25 16:32:00","discussion":[{"upvote_count":"19","comments":[{"upvote_count":"1","timestamp":"1670922660.0","content":"Load from KDF to KDA : https://aws.amazon.com/kinesis/data-firehose/features/#:~:text=Support%20for%20multiple%20data%20destinations,MongoDB%2C%20and%20Splunk%20as%20destinations.","poster":"nadavw","comment_id":"743797"}],"poster":"ali98","comment_id":"477915","timestamp":"1636861080.0","content":"Answer : A \nhttps://docs.aws.amazon.com/kinesisanalytics/latest/dev/app-add-reference-data.html"},{"comment_id":"488496","timestamp":"1638046620.0","content":"answer : A","poster":"awsmani","upvote_count":"8"},{"poster":"wally_1995","comment_id":"942240","content":"I'd go with A, but it's not word correctly, KDF can't deliver simultaneously. It's KDA who is set up to read data from KDF (not KDF delivering to KDA)\n\nEverything else doesn't seem to be as efficient.","upvote_count":"2","timestamp":"1688424540.0"},{"upvote_count":"3","content":"A: I passed the test","timestamp":"1682966820.0","comment_id":"886621","poster":"pk349"},{"comment_id":"781645","timestamp":"1674164880.0","poster":"rocky48","content":"Selected Answer: A\nAnswer : A\nhttps://docs.aws.amazon.com/kinesisanalytics/latest/dev/app-add-reference-data.html","upvote_count":"3"},{"poster":"Chelseajcole","comment_id":"771792","timestamp":"1673384100.0","upvote_count":"1","content":"It is Kinesis analytics that can deliver up to 3 destinations, not firehose and Firehose multiple destinations means it can dump to those locations but it cannot deliver to multiple destinations at same time"},{"comment_id":"741603","upvote_count":"2","poster":"siju13","content":"Selected Answer: C\nKDF can have only one destination","comments":[{"poster":"nadavw","timestamp":"1670922600.0","content":"Amazon Kinesis Data Firehose is the easiest way to load streaming data Kinesis Data Firehose is a fully managed service that makes it easy to capture, transform, and load massive volumes of streaming data ... into Amazon S3, Amazon Redshift, Amazon OpenSearch Service, Kinesis Data Analytics,","comment_id":"743795","upvote_count":"2"}],"timestamp":"1670755440.0"},{"upvote_count":"1","content":"Selected Answer: A\nAnswer: A","comment_id":"638467","timestamp":"1658985000.0","poster":"rocky48"},{"content":"Selected Answer: A\nA: You can use KDF with Redshift and KDA at the same stream, is the most efficient and works \nB: Not effective at all and KDF to KDS makes no sense\nC: Even less effective\nD: SQS? will not send a notification and the in-app data makes no sense too\n\nSo it must be A","upvote_count":"1","poster":"f4bi4n","timestamp":"1653206520.0","comment_id":"605276"},{"content":"Its going to have to be C,\nA: KDF, a single stream cannot have more than one destination\nB. KDS cannot cannot send to S3\nD: Same as before KDF cannot have more than one destination","comment_id":"603004","upvote_count":"1","poster":"chp2022","timestamp":"1652810940.0"},{"comment_id":"581061","comments":[{"comments":[{"upvote_count":"1","comment_id":"647891","content":"https://aws.amazon.com/kinesis/data-firehose/features/#:~:text=Support%20for%20multiple%20data%20destinations&text=You%20can%20specify%20the%20destination,the%20data%20should%20be%20loaded.\n\nSupport for multiple data destinations\nAmazon Kinesis Data Firehose currently supports Amazon S3, Amazon Redshift, Amazon OpenSearch Service, HTTP endpoints, Datadog, New Relic, MongoDB, and Splunk as destinations. You can specify the destination Amazon S3 bucket, the Amazon Redshift table, the Amazon OpenSearch Service domain, generic HTTP endpoints, or a service provider where the data should be loaded.","timestamp":"1660710540.0","poster":"WonderTan"}],"upvote_count":"2","poster":"DevoteamAnalytix","content":"ANSWER: B\nSorry for the confusion, but via process of elimination I now see B as most likely\nA: Firehose cannot deliver to 2 destinations at the same time\nB: S3 can be used for reference data from KDA (https://docs.aws.amazon.com/kinesisanalytics/latest/dev/app-add-reference-data.html)\nC: First store everything in Redshift and then near-real-time check? Unlikely!\nD: No SQS, but SNS makes sense","timestamp":"1649488140.0","comment_id":"583174"}],"timestamp":"1649139360.0","content":"Answer: A\nIt seems to me that KDA can output into 2 destinations, so it is A for me\nhttps://docs.aws.amazon.com/kinesisanalytics/latest/dev/how-it-works-output.html","upvote_count":"1","poster":"DevoteamAnalytix"},{"timestamp":"1648585380.0","comment_id":"577854","poster":"Kamalt","upvote_count":"2","content":"C:\nQ: What is a destination?\nA destination is the data store where your data will be delivered. Amazon Kinesis Data Firehose currently supports Amazon S3, Amazon Redshift, and Amazon OpenSearch Service as destinations.\nhttps://www.amazonaws.cn/en/kinesis/data-firehose/faqs/"},{"content":"How can KDF sends to two data streams? I am not aware of it. The answer is C","upvote_count":"4","comment_id":"543989","timestamp":"1644429960.0","poster":"vkbajoria"},{"timestamp":"1642051560.0","content":"I think C is correct. I get the reference data in KDA thing, however KDF can only have one consumer for a stream. i.e. Redshift and KDA both cant be consumers for the same delivery stream. Thoughts?","comment_id":"522644","upvote_count":"5","poster":"Shivanikats"},{"comments":[{"comment_id":"575453","upvote_count":"1","content":"Absolutely incorrect !\n1. When you create KDF, configure it so send data to redshift destination. \n2. Create legacy SQL KDA application Configure KDA to use KDF as the streaming source.","poster":"CHRIS12722222","timestamp":"1648288140.0"}],"poster":"vKaspar","timestamp":"1642000020.0","comment_id":"522247","upvote_count":"5","content":"Firehose cant delivery to 2 destinations and it doesnt support KDA delivery. So option C"},{"comment_id":"503837","upvote_count":"2","poster":"Ritzritz","content":"https://docs.aws.amazon.com/kinesisanalytics/latest/dev/how-it-works.html","comments":[{"timestamp":"1639764300.0","poster":"Ritzritz","upvote_count":"2","content":"Answer: A","comment_id":"503838"}],"timestamp":"1639764120.0"},{"comment_id":"475130","content":"Would vote for Option A - As its near real time ,Firehose can do the job . Also KDA can join the stream data with reference data in S3. \nhttps://docs.aws.amazon.com/kinesisanalytics/latest/dev/app-add-reference-data.html","poster":"Fazil_Cp","upvote_count":"4","timestamp":"1636502160.0"},{"comment_id":"467500","upvote_count":"3","poster":"srinivasa","timestamp":"1633347120.0","content":"Answer: B"}],"answer_ET":"A","question_id":33,"choices":{"B":"Use Amazon Kinesis Data Streams to collect all the data from toll stations. Create a stream in Kinesis Data Streams to temporarily store the threshold values from Amazon S3. Send both streams to Amazon Kinesis Data Analytics to compare the count of vehicles for a particular toll station against its corresponding threshold value. Use AWS Lambda to publish an Amazon Simple Notification Service (Amazon SNS) notification if the threshold is not met. Connect Amazon Kinesis Data Firehose to Kinesis Data Streams to deliver the data to Amazon Redshift.","D":"Use Amazon Kinesis Data Firehose to collect data and deliver it to Amazon Redshift and Amazon Kinesis Data Analytics simultaneously. Use Kinesis Data Analytics to compare the count of vehicles against the threshold value for the station stored in a table as an in-application stream based on information stored in Amazon S3. Configure an AWS Lambda function as an output for the application that will publish an Amazon Simple Queue Service (Amazon SQS) notification to alert operations personnel if the threshold is not met.","C":"Use Amazon Kinesis Data Firehose to collect data and deliver it to Amazon Redshift. Then, automatically trigger an AWS Lambda function that queries the data in Amazon Redshift, compares the count of vehicles for a particular toll station against its corresponding threshold values read from Amazon S3, and publishes an Amazon Simple Notification Service (Amazon SNS) notification if the threshold is not met.","A":"Use Amazon Kinesis Data Firehose to collect data and deliver it to Amazon Redshift and Amazon Kinesis Data Analytics simultaneously. Create a reference data source in Kinesis Data Analytics to temporarily store the threshold values from Amazon S3 and compare the count of vehicles for a particular toll station against its corresponding threshold value. Use AWS Lambda to publish an Amazon Simple Notification Service (Amazon SNS) notification if the threshold is not met."},"question_images":[],"answer_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/64706-exam-aws-certified-data-analytics-specialty-topic-1-question/","question_text":"A company operates toll services for highways across the country and collects data that is used to understand usage patterns. Analysts have requested the ability to run traffic reports in near-real time. The company is interested in building an ingestion pipeline that loads all the data into an Amazon Redshift cluster and alerts operations personnel when toll traffic for a particular toll station does not meet a specified threshold. Station data and the corresponding threshold values are stored in Amazon S3.\nWhich approach is the MOST efficient way to meet these requirements?","exam_id":20,"topic":"1","answers_community":["A (71%)","C (29%)"],"answer":"A","answer_description":"","unix_timestamp":1635172320},{"id":"BbJGOXGOO9Cov9bM4SEo","answers_community":["B (100%)"],"answer":"B","question_id":34,"isMC":true,"answer_ET":"B","question_text":"An online retail company uses Amazon Redshift to store historical sales transactions. The company is required to encrypt data at rest in the clusters to comply with the Payment Card Industry Data Security Standard (PCI DSS). A corporate governance policy mandates management of encryption keys using an on- premises hardware security module (HSM).\nWhich solution meets these requirements?","question_images":[],"unix_timestamp":1635174660,"answer_images":[],"discussion":[{"upvote_count":"18","poster":"lakediver","comment_id":"505241","timestamp":"1639983540.0","content":"I will go with B\nA- Wrong because on-premise HCM is required\nC- Encryption can not be enabled on the existing unencrypted cluster\nD - Redshift doesn't support AWS CloudHSM."},{"timestamp":"1636020540.0","poster":"srinivasa","content":"Answer: B","comments":[{"comment_id":"479815","comments":[{"content":"https://docs.aws.amazon.com/redshift/latest/mgmt/security-key-management.html\n\nAmazon Redshift supports management of encryption keys in external hardware security modules (HSMs). The HSM can be on-premises or can be AWS CloudHSM.","comment_id":"484803","poster":"ali98","timestamp":"1637652420.0","upvote_count":"2"},{"content":"From my understanding AWS CloudHSM Classic is an old service and AWS CloudHSM is newer. So, Redshift supports old AWS CloudHSM Classic or on-premise HSM:\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/security-key-management.html\n\"The HSM can be on-premises or can be AWS CloudHSM. When you use an HSM, you must use client and server certificates to configure a trusted connection between Amazon Redshift and your HSM. Amazon Redshift supports only AWS CloudHSM Classic for key management.\"\nAgree the answer should be B","timestamp":"1637590620.0","comment_id":"484280","upvote_count":"2","poster":"Olga2022"}],"timestamp":"1637126220.0","poster":"varun_5757","upvote_count":"1","content":"Could you please explain why B is the best option? I See this link - https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html but it's not clear as to whether or not an On premises HSM can be used? - \"Amazon Redshift supports only AWS CloudHSM Classic. We don't support the newer AWS CloudHSM service.\""}],"upvote_count":"8","comment_id":"467513"},{"poster":"pk349","comment_id":"886622","timestamp":"1682966820.0","upvote_count":"1","content":"B: I passed the test"},{"content":"B for me","comment_id":"850852","poster":"CleverMonkey092","upvote_count":"1","timestamp":"1679822220.0"},{"upvote_count":"1","content":"Correct answer is B as Redshift can be configured to use on-premises HSM using a VPN connection. \nOption A is wrong as AWS Classic CloudHSM does not meet the on-premises HSM requirement.\n\nOption C is wrong as enabling encryption on an existing cluster cannot be done with HSM.\n\nYou can enable encryption when you launch your cluster, or you can modify an unencrypted cluster to use AWS Key Management Service (AWS KMS) encryption. To do so, you can use either an AWS-managed key or a customer managed key. When you modify your cluster to enable AWS KMS encryption, Amazon Redshift automatically migrates your data to a new encrypted cluster. Snapshots created from the encrypted cluster are also encrypted\n\nOption D is wrong as Redshift only supports classic CloudHSM.","timestamp":"1666954500.0","comment_id":"706369","poster":"cloudlearnerhere"},{"upvote_count":"1","timestamp":"1658295300.0","content":"Selected Answer: B\nAnswer: B","poster":"rocky48","comment_id":"633862"},{"upvote_count":"1","poster":"Teraxs","timestamp":"1651060560.0","content":"Selected Answer: B\nB - https://docs.aws.amazon.com/redshift/latest/mgmt/security-key-management.html","comment_id":"593134"},{"poster":"rav009","upvote_count":"3","comment_id":"535307","timestamp":"1643443800.0","content":"C is wrong \nBecause “When you modify your cluster to enable AWS KMS encryption, Amazon Redshift automatically migrates your data to a new encrypted cluster.”"},{"comment_id":"499379","comments":[{"poster":"ali98","content":"You can't enable hardware security module (HSM) encryption by modifying the cluster. Instead, create a new, HSM-encrypted cluster and migrate your data to the new cluste","timestamp":"1639257180.0","comment_id":"499670","upvote_count":"1"}],"timestamp":"1639224780.0","upvote_count":"1","poster":"tobsam","content":"Answer is C."},{"content":"A : [Amazon Redshift supports only AWS CloudHSM Classic. We don't support the newer AWS CloudHSM service.] It looks like this is outdated question. see the article:\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#working-with-HSM","timestamp":"1637019660.0","poster":"DMK2021","comment_id":"479068","upvote_count":"1"}],"choices":{"C":"Create an HSM connection and client certificate for the on-premises HSM. Enable HSM encryption on the existing unencrypted cluster by modifying the cluster. Connect to the VPC where the Amazon Redshift cluster resides from the on-premises network using a VPN.","B":"Create a VPC and establish a VPN connection between the VPC and the on-premises network. Create an HSM connection and client certificate for the on- premises HSM. Launch a cluster in the VPC with the option to use the on-premises HSM to store keys.","D":"Create a replica of the on-premises HSM in AWS CloudHSM. Launch a cluster in a VPC with the option to use CloudHSM to store keys.","A":"Create and manage encryption keys using AWS CloudHSM Classic. Launch an Amazon Redshift cluster in a VPC with the option to use CloudHSM Classic for key management."},"topic":"1","exam_id":20,"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/64708-exam-aws-certified-data-analytics-specialty-topic-1-question/","timestamp":"2021-10-25 17:11:00"},{"id":"cMkyDmyuRtTtiRWSstVw","discussion":[{"timestamp":"1632722880.0","poster":"abhineet","content":"C is correct, s3 is a valid target for DMS https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html","comments":[{"upvote_count":"2","comment_id":"174077","content":"I guess it should be A. DMS can can not do the data preprocessing and Spark is the best option on the large datasets","timestamp":"1632902400.0","comments":[{"timestamp":"1635974160.0","upvote_count":"2","poster":"Brijeshkrishna","comment_id":"368550","content":"C is correct as AWS Glue uses Spark engine"}],"poster":"GauravM17"}],"upvote_count":"28","comment_id":"160155"},{"upvote_count":"6","timestamp":"1632351480.0","poster":"zeronine","content":"C. DMS supports S3 as a target.","comment_id":"159580"},{"upvote_count":"1","content":"C: Option A, using AWS DataSync and Apache Spark scripts, involves maintaining an on-premises EMR cluster, which adds complexity and management overhead. Option B, creating custom ETL jobs on-premises, requires significant development effort and may not be as efficient as using AWS Glue. Option D, using AWS Snowball for data transfer and AWS Batch for data curation, is less efficient and more time-consuming compared to the direct ingestion and curation approach.","timestamp":"1700796000.0","comment_id":"1079003","poster":"Frazy"},{"poster":"jerkane","content":"Selected Answer: C\nC is correct using glue would be faster than using EMR","comment_id":"1075747","upvote_count":"1","timestamp":"1700506080.0"},{"upvote_count":"2","timestamp":"1699433100.0","comment_id":"1065449","content":"This is the differentiator. DMS can read a database source. DataSync cannot. The question says \"hosted in the company's 3 TB data warehouse.\". DataSync can read NFS, SMB, HDFS, S3. \nhttps://docs.aws.amazon.com/datasync/latest/userguide/how-datasync-transfer-works.html#onprem-aws","poster":"monkeydba"},{"comment_id":"1065442","poster":"monkeydba","upvote_count":"1","timestamp":"1699432860.0","content":"DataSync can indeed pull a subset of data. https://docs.aws.amazon.com/datasync/latest/userguide/filtering.html"},{"comment_id":"1065435","content":"The question mentions \"subset\" of data. Can DataSync do that? DMS can.","timestamp":"1699432620.0","upvote_count":"1","poster":"monkeydba"},{"comment_id":"1043525","timestamp":"1697294160.0","content":"Selected Answer: A\nA. I don't understand that all people agree on C. DMS means database migration service and here they mention data warehouse and not database, so this is not a DMS compatible source: https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.html.\nA is the valid option because with DataSync you can migrate your DATA to the S3 and then we can process it with EMR (more efficient than Glue)","poster":"gofavad926","upvote_count":"1"},{"poster":"debasishg","timestamp":"1695687060.0","comment_id":"1017281","content":"Selected Answer: C\nC. \nBecause, 1. Datasync is used for file migration, DMS for Data. 2. GLUE ETL required to transform data after migration.","upvote_count":"1"},{"content":"Selected Answer: C\nC for sure","timestamp":"1690905000.0","comment_id":"969179","poster":"NikkyDicky","upvote_count":"1"},{"comment_id":"886273","upvote_count":"1","poster":"pk349","timestamp":"1682946720.0","content":"C: I passed the test"},{"content":"Correct answer is C as DMS can be used for data migration to S3. AWS Glue can be used for preprocessing and data curation.\n\nOption A is wrong as DataSync is usually for storage migration and using Spark might be as operationally efficient as Glue.\n\nOption B is wrong as using on-premises custom ETL jobs might not be time-efficient.\n\nOption D is wrong as the data migration using Snowball will take time.","comment_id":"711130","poster":"cloudlearnerhere","timestamp":"1667565120.0","upvote_count":"4"},{"content":"Selected Answer: C\nGlue is the answer, as all the mentioned data operations are readily available with Glue.","timestamp":"1664080140.0","comment_id":"678409","poster":"Arka_01","upvote_count":"1"},{"upvote_count":"1","timestamp":"1659816180.0","poster":"rocky48","content":"Selected Answer: C\nC is correct","comment_id":"643501"},{"comment_id":"490785","upvote_count":"1","timestamp":"1638285660.0","content":"C is the correct answer, use DMS to ingest the data from on-prem data warehouse to S3 and use Glue DataBrew to data curation.","poster":"Thiya"},{"content":"Answer C. Ingest data into Amazon S3 using AWS DMS. Use AWS Glue to perform data curation and store the data in Amazon 3 for ML processing.","comment_id":"386234","poster":"Donell","upvote_count":"1","timestamp":"1636271340.0"},{"poster":"Shraddha","timestamp":"1636125900.0","upvote_count":"3","content":"A = wrong, DataSync is for storage migration not data warehouse. B = wrong, ETL job on-premise is not fast. D = wrong, too slow.","comment_id":"383533"},{"comment_id":"297242","upvote_count":"2","content":"C\nA is wrong DataSync can not read from Data Warehouse as per the link below\nhttps://docs.aws.amazon.com/datasync/latest/userguide/working-with-locations.html\n\nB can be but as the curate is simple we can use Glue\n\nC: correct from my point of view \n\nD: is wrong as we talk a full batch which doesn't make sense","comments":[{"content":"plus snowball take at least 2 days. generally using snowball in this situation is not ideal","timestamp":"1635852840.0","poster":"sayed","comment_id":"297243","upvote_count":"1"}],"poster":"sayed","timestamp":"1635801540.0"},{"timestamp":"1635375240.0","poster":"Exia","comment_id":"285157","upvote_count":"1","content":"C.\n\nA. DataSync is designed for online data trasfering.\nB. Although custom ETL jobs can reduce the total migrated data size. But time for writing custom ETL jobs is much longer than using Glue ETL jobs.\nD. Snowball is too slow for 3TB data warehouse migration."},{"timestamp":"1635231840.0","comment_id":"274251","upvote_count":"1","poster":"lostsoul07","content":"C is the right answer"},{"upvote_count":"2","poster":"BillyC","content":"C is correct for me","timestamp":"1635134280.0","comment_id":"216826"},{"poster":"sanjaym","content":"Answer should be C. A and C both looks promising but ask is fastest solution, it takes more time to develop spark script and setup EMR cluster than use Glue.","upvote_count":"2","timestamp":"1635039300.0","comment_id":"204655"},{"upvote_count":"1","content":"Answer is C. The reason is fastest mechanism multiple step, including mapping, dropping null fields, resolving choice, and splitting fields which Glue offers . DMS supports loading the subset of data in S3","comment_id":"199035","poster":"sparun","timestamp":"1634609280.0"},{"content":"I would say C as per link:\nhttps://docs.aws.amazon.com/glue/latest/dg/built-in-transforms.html","timestamp":"1634510160.0","poster":"syu31svc","comment_id":"191286","upvote_count":"1"},{"timestamp":"1634229120.0","upvote_count":"1","comment_id":"187741","poster":"noblate","content":"AWS DataSync makes it simple and fast to move large amounts of data online between on-premises storage and Amazon S3, Amazon Elastic File System (Amazon EFS), or Amazon FSx for Windows File Server."},{"upvote_count":"1","timestamp":"1633616700.0","poster":"tesla_elon","comment_id":"178790","content":"\"perform multiple step, including mapping, dropping null fields, resolving choice, and splitting fields.\" Al this is actually Glue's built in transformation functions. So, should be C"},{"comments":[{"poster":"Karan_Sharma","content":"Question talks about subset of that data and all the asks wrt are inbuilt glue function.","timestamp":"1633702200.0","upvote_count":"1","comment_id":"182267","comments":[{"content":"wrt transformation*","timestamp":"1634057640.0","comment_id":"182268","poster":"Karan_Sharma","upvote_count":"1"}]}],"comment_id":"177747","poster":"GauravM17","content":"Data processing on 3 TB data does not sound a good option, rather it should be EMR based spark processing. Also with Direct Connect, using Data Sync is a an accelerated way to move data from on-prem to S3. I would go with A","upvote_count":"4","timestamp":"1633555140.0"},{"timestamp":"1633541100.0","upvote_count":"4","poster":"Karan_Sharma","content":"Option C, the keyword to look for in the question is fastest solution. Glue being a fully managed serverless ETL service provides instant job creation with predefined py spark/scala code that can be tailored as needed. And DMS does support S3 as a target.","comment_id":"177476"},{"upvote_count":"2","comment_id":"176596","poster":"greenv","content":"The answer is C. DataSync doesn't support connections to DWH databases.","timestamp":"1633538100.0"},{"poster":"Paitan","comment_id":"175262","content":"I will go with C simply because Amazon will prefer using its own service Glue rather than use custom ETL or Spark programming :-)","timestamp":"1633389900.0","upvote_count":"4"},{"timestamp":"1632873120.0","upvote_count":"3","content":"The answer is A.\n\nDMS can ingest data to S3, but only from supported database sources, and the question didn't mention where the data was coming from.","poster":"[Removed]","comment_id":"171051","comments":[{"content":"the question did mention it : \"on-premises data hosted in the company's 3 TB data warehouse\"","timestamp":"1632980760.0","poster":"bigollo","comment_id":"174806","upvote_count":"2"}]},{"comments":[{"poster":"carol1522","timestamp":"1632582600.0","upvote_count":"3","comment_id":"160086","content":"Actually it can: https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html"},{"comment_id":"168165","poster":"ramozo","content":"\"The company needs the fastest solution to curate..\". If Spark is the fastest, the answer should be A.","timestamp":"1632735120.0","upvote_count":"2"}],"upvote_count":"2","comment_id":"159210","timestamp":"1632177720.0","poster":"zanhsieh","content":"A. BC dropped because DMS cannot ingest data to S3. D is slower (1 weeks) than A.\nhttps://aws.amazon.com/snowball/faqs/#:~:text=The%20end%2Dto%2Dend%20time,time%20in%20AWS%20data%20centers."}],"question_images":[],"isMC":true,"answers_community":["C (83%)","A (17%)"],"question_text":"A company is planning to do a proof of concept for a machine learning (ML) project using Amazon SageMaker with a subset of existing on-premises data hosted in the company's 3 TB data warehouse. For part of the project, AWS Direct Connect is established and tested. To prepare the data for ML, data analysts are performing data curation. The data analysts want to perform multiple step, including mapping, dropping null fields, resolving choice, and splitting fields. The company needs the fastest solution to curate the data for this project.\nWhich solution meets these requirements?","url":"https://www.examtopics.com/discussions/amazon/view/28717-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer":"C","answer_images":[],"question_id":35,"answer_description":"","choices":{"B":"Create custom ETL jobs on-premises to curate the data. Use AWS DMS to ingest data into Amazon S3 for ML processing.","A":"Ingest data into Amazon S3 using AWS DataSync and use Apache Spark scrips to curate the data in an Amazon EMR cluster. Store the curated data in Amazon S3 for ML processing.","C":"Ingest data into Amazon S3 using AWS DMS. Use AWS Glue to perform data curation and store the data in Amazon S3 for ML processing.","D":"Take a full backup of the data store and ship the backup files using AWS Snowball. Upload Snowball data into Amazon S3 and schedule data curation jobs using AWS Batch to prepare the data for ML."},"unix_timestamp":1597579140,"exam_id":20,"answer_ET":"C","topic":"1","timestamp":"2020-08-16 13:59:00"}],"exam":{"id":20,"name":"AWS Certified Data Analytics - Specialty","provider":"Amazon","isBeta":false,"isImplemented":true,"isMCOnly":true,"lastUpdated":"11 Apr 2025","numberOfQuestions":164},"currentPage":7},"__N_SSP":true}