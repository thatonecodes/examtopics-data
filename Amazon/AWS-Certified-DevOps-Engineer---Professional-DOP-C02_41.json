{"pageProps":{"questions":[{"id":"8MdiOTtvAkSJRZWZby9l","isMC":true,"topic":"1","answers_community":["D (86%)","14%"],"exam_id":23,"question_id":201,"answer_images":[],"question_images":[],"answer_description":"","unix_timestamp":1681507500,"answer_ET":"D","discussion":[{"upvote_count":"10","timestamp":"1686383940.0","poster":"madperro","content":"Selected Answer: D\nD is the easiest solution.","comment_id":"919900"},{"comment_id":"1319172","timestamp":"1732789860.0","content":"Selected Answer: C\nwhy C is not correctï¼Ÿ","upvote_count":"1","poster":"wikn","comments":[{"poster":"hayjaykay","comment_id":"1351180","upvote_count":"1","content":"Think fleet of EC2s, think SSM (systems manager).","timestamp":"1738638360.0"}]},{"upvote_count":"1","timestamp":"1731860640.0","content":"Option A is the most efficient and straightforward approach to automate log collection and prevent premature termination of EC2 instances by using Auto Scaling lifecycle hooks, CloudWatch alarms, Lambda functions, and SSM to gather and store logs in Amazon S3 before the instance is terminated.","comment_id":"1313653","poster":"Ravi_Bulusu"},{"timestamp":"1731429900.0","poster":"Saudis","comment_id":"1310739","content":"it is D not C because CloudWatch agent can not invokes a script","upvote_count":"1"},{"content":"Selected Answer: D\nD as the EC2 is Terminating and Cloudwatch Agent should be not running and cannot collect the logs","upvote_count":"2","comment_id":"1259239","poster":"jamesf","timestamp":"1722496200.0"},{"upvote_count":"1","content":"Selected Answer: C\nIt must be 'C' as CloudWatch Agent will push the logs to a particular CloudWatch log group.","comment_id":"1234191","timestamp":"1718944320.0","poster":"Rahul369"},{"content":"Selected Answer: D\nTerminating:Wait refers to a state in which an instance is determined to be terminated by the Auto Scaling group as part of the termination process and is temporarily put on hold before it is actually terminated. This state pauses the termination process and provides an opportunity to perform custom actions (logging, graceful shutdown, data backup, etc).","comment_id":"1153760","comments":[{"comment_id":"1193373","upvote_count":"1","content":"why not C bro","poster":"hoazgazh","timestamp":"1712799180.0"}],"poster":"dzn","upvote_count":"2","timestamp":"1708328640.0"},{"content":"D is correct: Using Eventbridge in combination with lambda is a common practice.\nA: Cloudwatch alarm only alert, no action so it cannot trigger lambda (when this question came out, it could not)\nB: AWS config rule cannot triger a script. \nC: cloudwatch agent itself does not have any direct action on the host but collecting logs","upvote_count":"4","timestamp":"1706536260.0","comment_id":"1135013","poster":"thanhnv142"},{"upvote_count":"1","content":"C is also a good choice in this question. Why? you need to have a CW agent installed on the hosts to be able to collect logs from the servers before termination.","poster":"Jaguaroooo","comment_id":"1112686","timestamp":"1704279240.0","comments":[{"comment_id":"1115094","upvote_count":"2","timestamp":"1704533760.0","content":"I think we can't select C because it says that it invokes the cloudwatch agent after the EC2 instance is terminated. It can't collect the logs from terminated EC2 Instance.","poster":"davdan99"}]},{"upvote_count":"3","comment_id":"910380","content":"Selected Answer: D\nD is the correct one IMHO.\n\nASG actions are not logged to cloudwatch logs to use a filter, and if so it would be complicated to extract the data. The canonical way is to rely in an EventBridge event.","timestamp":"1685460300.0","poster":"bcx"},{"content":"D\n\n\"When a scale-in event occurs, a lifecycle hook pauses the instance before it is terminated and sends you a notification using Amazon EventBridge. While the instance is in the wait state, you can invoke an AWS Lambda function or connect to the instance to download logs or other data before the instance is fully terminated. \"\n\nhttps://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/","timestamp":"1684825380.0","comment_id":"904663","upvote_count":"4","poster":"levster"},{"content":"Selected Answer: D\nD for sure 100%","poster":"vherman","upvote_count":"1","timestamp":"1683125160.0","comment_id":"888631"},{"comment_id":"886936","upvote_count":"4","poster":"haazybanj","content":"Selected Answer: D\nD. Use Auto Scaling lifecycle hooks to put instances in a Terminating:Wait state. Create an Amazon EventBridge rule for EC2 Instance-terminate Lifecycle Action and trigger an AWS Lambda function that invokes an SSM Run Command script to collect logs, push them to Amazon S3, and complete the lifecycle action once logs are collected.\n\nWith this solution, you can use an Auto Scaling lifecycle hook to put instances in a wait state before termination. This provides an opportunity to collect logs before the instance is terminated. The solution can use an Amazon EventBridge rule for EC2 Instance-terminate Lifecycle Action to trigger an AWS Lambda function that will execute an SSM Run Command script. The script can collect logs and push them to Amazon S3 before completing the lifecycle action and allowing the instance to terminate. This solution provides a way to collect logs before instances are terminated, allowing for root cause analysis of issues.","timestamp":"1682989860.0"},{"content":"Selected Answer: D\nD seems to be more relevant for this scenario","upvote_count":"1","comment_id":"884070","poster":"ParagSanyashiv","timestamp":"1682748660.0"},{"upvote_count":"1","comment_id":"878885","poster":"henryyvr","comments":[{"content":"read this link and you will understand that C is wrong option- https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/","comment_id":"896874","upvote_count":"1","timestamp":"1683994140.0","poster":"ipsingh"}],"timestamp":"1682295420.0","content":"Note that there is a similar question on Tutorial Dojo and the answer is to \"trigger cloudwatch agent\""},{"poster":"henryyvr","comment_id":"878873","content":"Selected Answer: C\nshould be C","upvote_count":"2","timestamp":"1682294700.0","comments":[{"comment_id":"998437","upvote_count":"2","timestamp":"1693820520.0","content":"No way. Cloudwatch subscription filter is normally used to send cloudwatch log to kinesis firehose stream so that it can be consumed by other tools such as Splunk. If you need to invoke a lambda, the easiest way is to use event rule.","poster":"beanxyz"}]},{"comment_id":"870805","upvote_count":"1","content":"Selected Answer: D\nD sure","poster":"ele","timestamp":"1681550880.0"},{"content":"Selected Answer: D\nI think is D","poster":"alce2020","comment_id":"870483","timestamp":"1681507500.0","upvote_count":"1"}],"timestamp":"2023-04-14 23:25:00","question_text":"A developer is maintaining a fleet of 50 Amazon EC2 Linux servers. The servers are part of an Amazon EC2 Auto Scaling group, and also use Elastic Load Balancing for load balancing.\nOccasionally, some application servers are being terminated after failing ELB HTTP health checks. The developer would like to perform a root cause analysis on the issue, but before being able to access application logs, the server is terminated.\nHow can log collection be automated?","url":"https://www.examtopics.com/discussions/amazon/view/106206-exam-aws-certified-devops-engineer-professional-dop-c02/","choices":{"A":"Use Auto Scaling lifecycle hooks to put instances in a Pending:Wait state. Create an Amazon CloudWatch alarm for EC2 Instance Terminate Successful and trigger an AWS Lambda function that invokes an SSM Run Command script to collect logs, push them to Amazon S3, and complete the lifecycle action once logs are collected.","D":"Use Auto Scaling lifecycle hooks to put instances in a Terminating:Wait state. Create an Amazon EventBridge rule for EC2 Instance-terminate Lifecycle Action and trigger an AWS Lambda function that invokes an SSM Run Command script to collect logs, push them to Amazon S3, and complete the lifecycle action once logs are collected.","C":"Use Auto Scaling lifecycle hooks to put instances in a Terminating:Wait state. Create an Amazon CloudWatch subscription filter for EC2 Instance Terminate Successful and trigger a CloudWatch agent that invokes a script to collect logs, push them to Amazon S3, and complete the lifecycle action once logs are collected.","B":"Use Auto Scaling lifecycle hooks to put instances in a Terminating:Wait state. Create an AWS Config rule for EC2 Instance-terminate Lifecycle Action and trigger a step function that invokes a script to collect logs, push them to Amazon S3, and complete the lifecycle action once logs are collected."},"answer":"D"},{"id":"t213AdxoUzDeDpRsDfz3","timestamp":"2024-08-21 16:03:00","unix_timestamp":1724248980,"answer_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/146260-exam-aws-certified-devops-engineer-professional-dop-c02/","answer_description":"","answers_community":["B (77%)","A (23%)"],"discussion":[{"poster":"teo2157","content":"Selected Answer: B\nIt's B based on this url\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_evaluation.html","comment_id":"1327225","timestamp":"1734338400.0","upvote_count":"2"},{"upvote_count":"2","timestamp":"1726294200.0","comment_id":"1283504","poster":"aws_god","content":"Selected Answer: A\nThe default FullAWSAccess policy is attached to the root, the Department OU, and the Engineering OU. So even if it is removed from the Department OU, it is still attached on the Engineering OU."},{"comment_id":"1274505","upvote_count":"2","timestamp":"1724937780.0","content":"Selected Answer: B\nI'ts B","poster":"ApacheKafkaAWS"},{"upvote_count":"2","timestamp":"1724646840.0","content":"Selected Answer: B\nvote B..","comment_id":"1272405","poster":"siheom"},{"upvote_count":"4","poster":"hzaki","content":"Selected Answer: B\nWhen the FullAWSAccess policy is replaced with a policy that allows only EC2 actions, this new SCP will act as a boundary. Even if an IAM role or user within the account has a broader permission set (like AdministratorAccess), the SCP limits what can be done.","timestamp":"1724631540.0","comment_id":"1272339"},{"content":"Selected Answer: A\nThe answer is A \nStill, the root has attached a full access policy.","comment_id":"1270177","upvote_count":"1","timestamp":"1724248980.0","poster":"hzaki","comments":[{"poster":"hzaki","timestamp":"1724631600.0","content":"Sorry the Answer: B\nWhen the FullAWSAccess policy is replaced with a policy that allows only EC2 actions, this new SCP will act as a boundary. Even if an IAM role or user within the account has a broader permission set (like AdministratorAccess), the SCP limits what can be done.","upvote_count":"1","comment_id":"1272340"}]}],"isMC":true,"answer_ET":"B","question_images":[],"exam_id":23,"answer":"B","choices":{"D":"All API actions on EC2 resources will be denied. All other API actions will be allowed.","A":"All API actions on all resources will be allowed.","B":"All API actions on EC2 resources will be allowed. All other API actions will be denied.","C":"All API actions on all resources will be denied."},"question_text":"A company uses AWS Organizations to manage its AWS accounts. The organization root has a child OU that is named Department. The Department OU has a child OU that is named Engineering. The default FullAWSAccess policy is attached to the root, the Department OU, and the Engineering OU.\n\nThe company has many AWS accounts in the Engineering OU. Each account has an administrative IAM role with the AdministratorAccess IAM policy attached. The default FullAWSAccessPolicy is also attached to each account.\n\nA DevOps engineer plans to remove the FullAWSAccess policy from the Department OU. The DevOps engineer will replace the policy with a policy that contains an Allow statement for all Amazon EC2 API operations.\n\nWhat will happen to the permissions of the administrative 1AM roles as a result of this change?","question_id":202},{"id":"UM5KdXOkcMEKcLO9Joa2","url":"https://www.examtopics.com/discussions/amazon/view/146217-exam-aws-certified-devops-engineer-professional-dop-c02/","answer_description":"","question_images":[],"topic":"1","answer":"D","question_text":"A company manages AWS accounts in AWS Organizations. The company needs a solution to send Amazon CloudWatch Logs data to an Amazon S3 bucket in a dedicated AWS account. The solution must support all existing and future CloudWatch Logs log groups.\n\nWhich solution will meet these requirements?","exam_id":23,"unix_timestamp":1724224980,"choices":{"D":"Create a CloudWatch Logs destination and an Amazon Kinesis Data Firehose delivery stream in the dedicated AWS account. Specify the S3 bucket as the destination of the delivery stream. Create subscription filters for all existing log groups in all accounts. Create an AWS Lambda function to call the CloudWatch Logs PutSubscriptionFilter API operation. Create an Amazon EventBridge rule to invoke the Lambda function when a CreateLogGroup event occurs.","A":"Enable Organizations backup policies to back up all log groups to a dedicated S3 bucket. Add an S3 bucket policy that allows access from all accounts that belong to the company.","C":"Create a backup plan in AWS Backup. Specify a dedicated S3 bucket as a backup vault. Assign all existing log groups to the backup plan. Create resource assignments in the backup plan for all accounts that belong to the company. Create an AWS Systems Manager Automation runbook to assign log groups to a backup plan. Create an AWS Config rule that has an automatic remediation action for all noncompliant log groups. Specify the runbook as the rule's target.","B":"Create a backup plan in AWS Backup. Specify a dedicated S3 bucket as a backup vault. Assign all CloudWatch Logs log group resources to the backup plan. Create resource assignments in the backup plan for all accounts that belong to the company."},"isMC":true,"timestamp":"2024-08-21 09:23:00","answer_ET":"D","discussion":[{"timestamp":"1726294860.0","upvote_count":"2","comment_id":"1283509","poster":"aws_god","content":"Selected Answer: D\nC seems like the most elegant solution, but I was not able to find any documentation that supports it.\n\nFor option D I found this - https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html#FirehoseExample"},{"content":"Selected Answer: D\nCorrect answer D","comment_id":"1270135","timestamp":"1724245680.0","upvote_count":"2","poster":"hzaki"},{"content":"Correct answer D","upvote_count":"2","poster":"hzaki","comment_id":"1269923","timestamp":"1724224980.0"}],"answer_images":[],"answers_community":["D (100%)"],"question_id":203},{"id":"FD0xmNZWZrEm9Wji24dP","exam_id":23,"answer_description":"","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/153026-exam-aws-certified-devops-engineer-professional-dop-c02/","answer_images":[],"answer":"AD","timestamp":"2024-12-16 09:59:00","unix_timestamp":1734339540,"choices":{"C":"Create an Amazon Managed Service for Prometheus workspace. Deploy AWS Distro for OpenTelemetry as a container sidecar to publish the JVM metrics from port 9404 to the Prometheus workspace. Configure rules for the workspace to use the JVM thread count metric to scale the application. Add a step scaling policy in Fargate. Select the Prometheus rules to scale up and scaling down.","B":"Deploy the Amazon CloudWatch agent as a container sidecar. Configure a metric filter for the JVM thread count metric on the CloudWatch log group for the CloudWatch agent. Add a target tracking policy in Fargate. Select the metric from the metric filter as a scale target.","A":"Deploy the Amazon CloudWatch agent as a container sidecar. Configure the CloudWatch agent to retrieve JVM metrics from port 9404. Create CloudWatch alarms on the JVM thread count metric to scale the application. Add a step scaling policy in Fargate to scale up and scale down based on the CloudWatch alarms.","D":"Create an Amazon Managed Service for Prometheus workspace. Deploy AWS Distro for OpenTelemetry as a container sidecar to retrieve JVM metrics from port 9404 to publish the JVM metrics from port 9404 to the Prometheus workspace. Add a target tracking policy in Fargate. Select the Prometheus metric as a scale target."},"discussion":[{"content":"Selected Answer: AD\nBoth A and D are feasible based on this links:\nhttps://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-step-scaling-policies.html\nhttps://aws.amazon.com/blogs/containers/autoscaling-amazon-ecs-services-based-on-custom-metrics-with-application-auto-scaling/","timestamp":"1734339540.0","comment_id":"1327239","upvote_count":"3","poster":"teo2157"}],"answers_community":["AD (100%)"],"question_text":"A DevOps engineer manages a Java-based application that runs in an Amazon Elastic Container Service (Amazon ECS) cluster on AWS Fargate. Auto scaling has not been configured for the application.\n\nThe DevOps engineer has determined that the Java Virtual Machine (JVM) thread count is a good indicator of when to scale the application. The application serves customer traffic on port 8080 and makes JVM metrics available on port 9404.\n\nApplication use has recently increased. The DevOps engineer needs to configure auto scaling for the application.\n\nWhich solution will meet these requirements with the LEAST operational overhead? (Choose two.)","question_id":204,"answer_ET":"AD","isMC":true,"topic":"1"},{"id":"fS4hTpnbTQcB2nIwe301","answers_community":["C (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/152993-exam-aws-certified-devops-engineer-professional-dop-c02/","answer_images":[],"timestamp":"2024-12-15 12:02:00","question_text":"A company has an application that runs in a single AWS Region. The application runs on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster and connects to an Amazon Aurora MySQL cluster. The application is built in an AWS CodeBuild project. The container images are published to Amazon Elastic Container Registry (Amazon ECR).\n\nThe company needs to replicate the state of the application for the container images and the database to a second Region.\n\nWhich solution will meet these requirements in the MOST operationally efficient way?","discussion":[{"timestamp":"1734343440.0","comment_id":"1327257","poster":"teo2157","content":"Selected Answer: C\nAgree with C based on this link:\nhttps://aws.amazon.com/blogs/containers/cross-region-replication-in-amazon-ecr-has-landed/","upvote_count":"2"},{"comment_id":"1326794","timestamp":"1734260520.0","poster":"Ky_24","upvote_count":"2","content":"Selected Answer: C\nWhy this is the most operationally efficient:\n\n â€¢ Minimal Management Overhead: AWS manages the replication for both the ECR images and the Aurora database. There is no need to configure or maintain additional Lambda functions or EventBridge rules.\n â€¢ Scalability: Aurora global databases and ECR replication are designed for production-grade systems and can scale to meet large workloads.\n â€¢ Reliability: The solution leverages AWS-native features, reducing the chance of errors or operational disruptions."}],"question_id":205,"answer":"C","exam_id":23,"topic":"1","answer_ET":"C","question_images":[],"answer_description":"","unix_timestamp":1734260520,"isMC":true,"choices":{"C":"Turn on Cross-Region Replication to replicate the ECR repository to the second Region. Deploy the application to an EKS cluster in the second Region by referencing the new ECR repository in a Kubernetes deployment file. Configure an Aurora global database with clusters in the initial Region and the second Region. Configure the new application deployment to use the endpoints for the second Region's cluster in the Aurora global database.","B":"Create an Amazon EventBridge rule that reacts to image pushes to the ECR repository. Configure the EventBridge rule to invoke an AWS Lambda function to replicate the image to a new ECR repository in the second Region. Deploy the application to an EKS cluster in the second Region by referencing the new ECR repository in a Kubernetes deployment file. Configure a cross-Region Aurora Replica in the second Region. Configure the new application deployment to use the endpoints for the cross-Region Aurora Replica.","D":"Configure the CodeBuild project to also push the container image to an ECR repository in the second Region. Deploy the application to an EKS cluster in the second Region by referencing the new ECR repository in a Kubernetes deployment file. Configure an Aurora MySQL cluster in the second Region as the target for binary log replication from the Aurora MySQL cluster in the initial Region. Configure the new application deployment to use the endpoints for the second Region's cluster.","A":"Turn on Amazon S3 Cross-Region Replication (CRR) on the bucket that holds the ECR container images. Deploy the application to an EKS cluster in the second Region by referencing the new S3 bucket object URL for the container image in a Kubernetes deployment file. Configure a cross-Region Aurora Replica in the second Region. Configure the new application deployment to use the endpoints for the cross-Region Aurora Replica."}}],"exam":{"isImplemented":true,"name":"AWS Certified DevOps Engineer - Professional DOP-C02","lastUpdated":"11 Apr 2025","isBeta":false,"provider":"Amazon","numberOfQuestions":355,"isMCOnly":true,"id":23},"currentPage":41},"__N_SSP":true}