{"pageProps":{"questions":[{"id":"CDVc5g6TdzBiVbj0MfuF","timestamp":"2024-02-02 10:23:00","question_text":"A data engineer is using Amazon Athena to analyze sales data that is in Amazon S3. The data engineer writes a query to retrieve sales amounts for 2023 for several products from a table named sales_data. However, the query does not return results for all of the products that are in the sales_data table. The data engineer needs to troubleshoot the query to resolve the issue.\nThe data engineer's original query is as follows:\nSELECT product_name, sum(sales_amount)\n\nFROM sales_data -\n\nWHERE year = 2023 -\n\nGROUP BY product_name -\nHow should the data engineer modify the Athena query to meet these requirements?","answers_community":["B (61%)","C (39%)"],"unix_timestamp":1706865780,"discussion":[{"timestamp":"1710837120.0","upvote_count":"12","content":"Selected Answer: B\n\"SELECT product_name, sum(sales_amount)\nFROM sales_data\nWHERE extract(year FROM sales_date) = 2023\nGROUP BY product_name;\"\nA. This would change the query to count the number of rows instead of summing sales.\nC. This would filter out products with zero sales amounts.\nD. Removing the GROUP BY clause would result in a single sum of all sales amounts without grouping by product_name.","poster":"GiorgioGss","comment_id":"1177119"},{"comment_id":"1305019","upvote_count":"7","poster":"pikuantne","timestamp":"1730296680.0","content":"None of these options make sense. I think the question is worded incorrectly. I understand that the problem is supposed to be: the products that did not have any sales in 2023 should also be visible in the report with sum of sales_amount = 0. So, the WHERE condition should be deleted and replaced with a CASE WHEN. That way all of the products in the table will be visible, but only sales for 2023 will be summed. Which is what I think this question is asking. None of the provided options do that."},{"timestamp":"1738047540.0","content":"Selected Answer: B\nhy Option (B) Works\nIf the underlying table field is a date or timestamp (rather than a numeric year column), using WHERE year = 2023 filters out all rows that do not literally match year = 2023.\nBy using extract(year FROM sales_data) = 2023, you are correctly filtering rows whose date (or timestamp) in the sales_data column corresponds to the year 2023.\nHence, (B) resolves the problem by filtering on the correct year value from the actual date/timestamp column, ensuring all qualifying products are included in the results.","comment_id":"1347765","poster":"YUICH","upvote_count":"2"},{"poster":"Udyan","upvote_count":"1","comment_id":"1339229","timestamp":"1736611800.0","content":"Selected Answer: C\nThe issue might be that some products have sales amounts of 0 or NULL, and those records are being excluded from the results because Athena may not include them in the final output when performing aggregation. By using the HAVING clause, you can filter the groups based on the aggregated sales amount (sum). This ensures that only products with a non-zero sum of sales are returned in the results. The HAVING clause is used to filter results after the aggregation."},{"upvote_count":"1","timestamp":"1735429200.0","content":"Selected Answer: C\nThe HAVING clause filters the results to include only products with an aggregated sales amount greater than zero.","comment_id":"1333208","poster":"MLOPS_eng"},{"content":"Selected Answer: C\nSELECT product_name, sum(sales_amount)\nFROM sales_data\nWHERE year = 2023\nGROUP BY product_name\nHAVING sum(sales_amount) > 0\n\nExplanation:\nThe HAVING clause ensures that only products with a non-zero aggregated sales amount are included in the results. This will address cases where products exist in the table but have no sales data for 2023.","upvote_count":"1","poster":"Assassin27","comment_id":"1332514","timestamp":"1735315620.0"},{"timestamp":"1734986520.0","comment_id":"1330940","poster":"kailu","upvote_count":"1","content":"Selected Answer: C\nThere is no issue with the WHERE clause from the original query, so B is not the right option IMO."},{"poster":"Shatheesh","upvote_count":"2","content":"C, query in the question is correct you just need to get amounts grater than Zero","timestamp":"1727864280.0","comment_id":"1292349"},{"timestamp":"1716556620.0","content":"Selected Answer: B\nyear should be the partition in s3 so its necessary to extract. its not a column","comment_id":"1217488","upvote_count":"5","poster":"valuedate"},{"timestamp":"1716125100.0","content":"Selected Answer: C\nNo need to extract the year again","upvote_count":"2","poster":"VerRi","comment_id":"1213800"},{"timestamp":"1715868240.0","comment_id":"1212444","upvote_count":"1","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/kinesisanalytics/latest/sqlref/sql-reference-having-clause.html","poster":"Just_Ninja"},{"poster":"Snape","upvote_count":"1","content":"Selected Answer: C\nWrong answers \n\nA. Replace sum(sales_amount) with count(*) for the aggregation. This option will return the count of records for each product, not the sum of sales amounts, which is the desired result.\n\nB. Change WHERE year = 2023 to WHERE extract(year FROM sales_data) = 2023. The year column likely stores the year value directly, so there's no need to extract it from a date or timestamp column.\n\nD. Remove the GROUP BY clause. Removing the GROUP BY clause will cause an error because the sum(sales_amount) aggregation function requires a GROUP BY clause to specify the grouping column (product_name in this case).","comment_id":"1203806","timestamp":"1714367040.0"},{"upvote_count":"3","content":"B\nB. Change `WHERE year = 2023` to `WHERE extract(year FROM sales_data) = 2023`.\n\nThe issue with the original query is that it assumes there is a column named `year` in the `sales_data` table. However, it's more likely that the date or timestamp information is stored in a single column, for example, a column named `sales_date`.\n\nTo extract the year from a date or timestamp column, you need to use the `extract()` function in Athena SQL.","poster":"khchan123","timestamp":"1714277820.0","comment_id":"1203361"},{"upvote_count":"2","comment_id":"1197696","content":"None of the answer makes senses. Option C will exclude any amount that is 0. This option would be correct if it is: Add HAVING sum(sales_amount) >= 0 after the GROUP BY clause.","timestamp":"1713415740.0","poster":"chris_spencer"},{"comment_id":"1194639","poster":"Christina666","upvote_count":"2","comments":[{"poster":"altonh","upvote_count":"1","comment_id":"1322196","content":"Why would the query miss out on products with zero sales when the condition is based on year?","timestamp":"1733366700.0"}],"content":"Selected Answer: C\nGemini: C. Add HAVING sum(sales_amount) > 0 after the GROUP BY clause.\n\nZero Sales Products: The original query is likely missing products that had zero sales amount in 2023. This modification filters the grouped results, ensuring only products with positive sales are displayed.\nWhy Other Options Don't Address the Core Issue:\n\nA. Replace sum(sales_amount) with count(*) for the aggregation. This would show how many sales transactions a product had, but not if it generated any revenue. It wouldn't solve the issue of missing products.\nB. Change WHERE year = 2023 to WHERE extract(year FROM sales_data) = 2023. This is functionally equivalent to the original WHERE clause if the year column is already an integer type. It wouldn't fix missing products.\nD. Remove the GROUP BY clause. This would aggregate all sales for 2023 with no product breakdown, losing the granularity needed.","timestamp":"1712981280.0"},{"comments":[{"comment_id":"1207239","upvote_count":"1","poster":"DevoteamAnalytix","timestamp":"1714981740.0","content":"But this is the table name..."},{"timestamp":"1710542760.0","comment_id":"1174554","poster":"Felix_G","content":"Add HAVING sum(sales_amount) > 0 this does NOT filter out the data with sales_mount zero. it can not be any negative value. \nThe original queryâ€™s WHERE year = 2023 condition is already appropriate for filtering data by the year 2023, so that B is unnecessary.","comments":[{"poster":"FuriouZ","comment_id":"1180974","upvote_count":"3","content":">0 filters out every product which is not sold. The question was about \"some products are not displayed\" so using the having argument can not be the right choice","timestamp":"1711207980.0"}],"upvote_count":"1"}],"poster":"kj07","content":"Not A because the engineer wants a sum not the total count.\nNot C because it will filter out the data with sales_amount zero.\nNot D because it will return just one result and the engineer wants the sales for multiple products.\n\nB should be the right answer if the sales_data is a date field.","timestamp":"1710398880.0","comment_id":"1173184","upvote_count":"4"},{"comments":[{"upvote_count":"1","poster":"nyaopoko","timestamp":"1712223900.0","comment_id":"1189220","content":"answer is C!"}],"upvote_count":"2","timestamp":"1706865780.0","comment_id":"1138366","poster":"rralucard_","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/athena/latest/ug/select.html"}],"answer_images":[],"isMC":true,"choices":{"A":"Replace sum(sales_amount) with count(*) for the aggregation.","C":"Add HAVING sum(sales_amount) > 0 after the GROUP BY clause.","B":"Change WHERE year = 2023 to WHERE extract(year FROM sales_data) = 2023.","D":"Remove the GROUP BY clause."},"answer":"B","exam_id":21,"answer_ET":"B","answer_description":"","topic":"1","question_images":[],"question_id":151,"url":"https://www.examtopics.com/discussions/amazon/view/132672-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/"},{"id":"iISCzyGUXj0nvAaziDoD","isMC":true,"question_id":152,"answer_ET":"B","topic":"1","answer_description":"","answers_community":["B (71%)","D (29%)"],"exam_id":21,"choices":{"D":"Run an AWS Glue crawler on the S3 objects. Use a SQL SELECT statement in Amazon Athena to query the required column.","B":"Use S3 Select to write a SQL SELECT statement to retrieve the required column from the S3 objects.","A":"Configure an AWS Lambda function to load data from the S3 bucket into a pandas dataframe. Write a SQL SELECT statement on the dataframe to query the required column.","C":"Prepare an AWS Glue DataBrew project to consume the S3 objects and to query the required column."},"answer":"B","unix_timestamp":1706866140,"question_images":[],"question_text":"A data engineer has a one-time task to read data from objects that are in Apache Parquet format in an Amazon S3 bucket. The data engineer needs to query only one column of the data.\nWhich solution will meet these requirements with the LEAST operational overhead?","answer_images":[],"timestamp":"2024-02-02 10:29:00","discussion":[{"poster":"imymoco","comment_id":"1331469","timestamp":"1735117680.0","upvote_count":"1","content":"Selected Answer: B\nonly one column -> S3 select"},{"comment_id":"1322817","content":"Selected Answer: D\nS3 select is for querying one object. Here the requirement is to query one column from multiple objects. Also S3 select is discontinued for new users. So answer could be D","upvote_count":"4","timestamp":"1733499600.0","poster":"JoeAWSOCM"},{"poster":"catoteja","comment_id":"1265337","upvote_count":"1","timestamp":"1723579740.0","content":"Amazon S3 Select is no longer available to new customers. Existing customers of Amazon S3 Select can continue to use the feature as usual\n\nBut with it you can only query one object xD. Glue + athena"},{"timestamp":"1718541480.0","comment_id":"1231367","content":"but s3 select can only select one object","upvote_count":"3","poster":"dungct"},{"comment_id":"1230687","content":"Selected Answer: B\nomly once","poster":"hogs","upvote_count":"2","timestamp":"1718402880.0"},{"content":"Selected Answer: B\nif is one-time task","timestamp":"1716512340.0","poster":"FunkyFresco","upvote_count":"2","comment_id":"1217157"},{"upvote_count":"2","poster":"GiorgioGss","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/using-select.html","comment_id":"1177127","timestamp":"1710837420.0"},{"poster":"rralucard_","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-inventory-athena-query.html\nS3 Select allows you to retrieve a subset of data from an object stored in S3 using simple SQL expressions. It is capable of working directly with objects in Parquet format.","comment_id":"1138370","timestamp":"1706866140.0","upvote_count":"3"}],"url":"https://www.examtopics.com/discussions/amazon/view/132673-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/"},{"id":"edtce3XeFZm5iaVJNh7S","timestamp":"2024-01-21 02:16:00","question_text":"A data engineer maintains custom Python scripts that perform a data formatting process that many AWS Lambda functions use. When the data engineer needs to modify the Python scripts, the data engineer must manually update all the Lambda functions.\nThe data engineer requires a less manual way to update the Lambda functions.\nWhich solution will meet this requirement?","question_id":153,"answer_ET":"B","topic":"1","exam_id":21,"url":"https://www.examtopics.com/discussions/amazon/view/131707-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_images":[],"choices":{"A":"Store a pointer to the custom Python scripts in the execution context object in a shared Amazon S3 bucket.","B":"Package the custom Python scripts into Lambda layers. Apply the Lambda layers to the Lambda functions.","C":"Store a pointer to the custom Python scripts in environment variables in a shared Amazon S3 bucket.","D":"Assign the same alias to each Lambda function. Call reach Lambda function by specifying the function's alias."},"answer":"B","answers_community":["B (100%)"],"discussion":[{"content":"B. Package the custom Python scripts into Lambda layers. Apply the Lambda layers to the Lambda functions.\nExplanation:\nLambda layers allow you to centrally manage shared code and dependencies across multiple Lambda functions. By packaging the custom Python scripts into a Lambda layer, you can simply update the layer whenever changes are made to the scripts, and all the Lambda functions that use the layer will automatically inherit the updates. This approach reduces manual effort and ensures consistency across the functions.","poster":"TonyStark0122","upvote_count":"20","timestamp":"1726863540.0","comment_id":"1137879"},{"timestamp":"1727080080.0","content":"Selected Answer: B\nCentralized Code Management: Lambda layers allow you to store and manage the custom Python scripts in a central location outside the individual Lambda function code. This eliminates the need to update the script in each Lambda function manually.\nReusable Code: Layers provide a way to share code across multiple Lambda functions. Any changes made to the layer code are automatically reflected in all the functions using that layer, streamlining updates.\nReduced Deployment Size: By separating core functionality into layers, you can keep the individual Lambda function code focused and smaller. This reduces deployment package size and potentially improves Lambda execution times.","upvote_count":"4","poster":"pypelyncar","comment_id":"1226775"},{"upvote_count":"2","content":"Selected Answer: B\nLambda Layers is a feature created with this literal objective in mind.","poster":"JavierEF","comment_id":"1277511","timestamp":"1725362160.0"},{"comment_id":"1235360","poster":"John2025","content":"B is right","timestamp":"1719057780.0","upvote_count":"2"},{"comment_id":"1219531","poster":"4c78df0","timestamp":"1716811380.0","content":"Selected Answer: B\nB is correct","upvote_count":"1"},{"poster":"4c78df0","comment_id":"1218784","content":"Selected Answer: B\nB is correct","timestamp":"1716707520.0","upvote_count":"1"},{"comment_id":"1212615","upvote_count":"1","timestamp":"1715901720.0","poster":"FunkyFresco","content":"Selected Answer: B\nLamba layers"},{"upvote_count":"2","timestamp":"1711649520.0","content":"Option B","comment_id":"1184939","poster":"ba72eb9"},{"upvote_count":"2","content":"Typical use case for Lambda Layers.\nOption B.","timestamp":"1710422220.0","poster":"kj07","comment_id":"1173428"}],"isMC":true,"answer_description":"","answer_images":[],"unix_timestamp":1705799760},{"id":"u1vZCTFgF12c7U5mpJF2","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/132674-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_text":"A company uses Amazon Redshift for its data warehouse. The company must automate refresh schedules for Amazon Redshift materialized views.\nWhich solution will meet this requirement with the LEAST effort?","unix_timestamp":1706866740,"isMC":true,"answers_community":["C (88%)","12%"],"exam_id":21,"discussion":[{"poster":"magnorm","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-schedule-query.html","timestamp":"1735475280.0","comment_id":"1333503","upvote_count":"2"},{"content":"Selected Answer: C\nthe company can automate the refresh schedules for materialized views with minimal effort. This approach leverages the built-in capabilities of Amazon Redshift, reducing the need for additional services, configurations, or custom code. It aligns with the principle of using the simplest and most straightforward solution that meets the requirements, minimizing operational overhead and complexity","poster":"pypelyncar","upvote_count":"2","timestamp":"1733799300.0","comment_id":"1227594"},{"upvote_count":"3","comment_id":"1208157","poster":"d8945a1","timestamp":"1731043920.0","content":"Selected Answer: C\nWe can schedule the refresh using query scheduler from Query Editor V2."},{"upvote_count":"2","content":"Selected Answer: C\nAmazon Redshift can automatically refresh materialized views with up-to-date data from its base tables when materialized views are created with or altered to have the autorefresh option. For more details, refer to the documentation here, https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-refresh.html.","comment_id":"1194640","timestamp":"1728792840.0","poster":"Christina666"},{"comment_id":"1186790","timestamp":"1727700360.0","poster":"[Removed]","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-schedule-query.html","upvote_count":"2"},{"poster":"FuriouZ","content":"Selected Answer: C\nYou can set autorefresh for materialized views using CREATE MATERIALIZED VIEW. You can also use the AUTO REFRESH clause to refresh materialized views automatically.","upvote_count":"2","comment_id":"1180978","timestamp":"1727098680.0"},{"upvote_count":"1","timestamp":"1726728000.0","poster":"GiorgioGss","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/redshift/latest/dg/materialized-view-refresh.html","comment_id":"1177133"},{"comment_id":"1173189","timestamp":"1726289700.0","poster":"kj07","content":"You can set AUTO REFRESH option on creation. So I will vote with C.","upvote_count":"1"},{"poster":"confusedyeti69","content":"Selected Answer: C\nLambda requires code and configuring permissions. A and D are additional overheads as well. Vote C","upvote_count":"1","timestamp":"1726226520.0","comment_id":"1172582"},{"comment_id":"1167432","timestamp":"1725643260.0","poster":"damaldon","upvote_count":"1","content":"B.\nhttps://docs.aws.amazon.com/redshift/latest/dg/materialized-view-UDFs.html"},{"comment_id":"1138385","timestamp":"1722584340.0","poster":"rralucard_","content":"Selected Answer: B\nAWS Lambda allows running code in response to triggers without needing to provision or manage servers. However, creating a UDF within Amazon Redshift to call a Lambda function for this purpose involves writing custom code and managing permissions between Lambda and Redshift.","upvote_count":"2"}],"question_id":154,"question_images":[],"timestamp":"2024-02-02 10:39:00","answer":"C","choices":{"B":"Use an AWS Lambda user-defined function (UDF) within Amazon Redshift to refresh the materialized views.","A":"Use Apache Airflow to refresh the materialized views.","D":"Use an AWS Glue workflow to refresh the materialized views.","C":"Use the query editor v2 in Amazon Redshift to refresh the materialized views."},"topic":"1","answer_description":"","answer_ET":"C"},{"id":"EzmG52BuwPapIdLcVqj7","answer":"A","isMC":true,"answer_ET":"A","answers_community":["A (86%)","14%"],"topic":"1","exam_id":21,"question_images":[],"unix_timestamp":1706867160,"answer_description":"","question_id":155,"discussion":[{"content":"Selected Answer: A\nStep Functions is a managed service for building serverless workflows. You define a state machine that orchestrates the execution sequence.\nThis eliminates the need to manage and maintain your own workflow orchestration server like Airflow.","poster":"pypelyncar","comment_id":"1227600","upvote_count":"6","timestamp":"1717982220.0"},{"upvote_count":"3","comment_id":"1269321","timestamp":"1724147880.0","content":"Selected Answer: C\nAWS Glue is a fully managed ETL (extract, transform, load) service that makes it easy to orchestrate data pipelines. Using the AWS Glue workflow to run Lambda functions and glue jobs is the easiest and least expensive option because it's a fully managed service that requires no additional workflow tools or infrastructure to configure and manage. Other options require additional tools or resources to configure and manage, and are therefore more expensive to manage.","poster":"hcong"},{"content":"Selected Answer: A\nStep Functions can handle both Lambda and Glue in this scenario, making it the best choice.","upvote_count":"2","comment_id":"1222507","timestamp":"1717217160.0","poster":"tgv"},{"upvote_count":"4","timestamp":"1715580600.0","content":"Selected Answer: A\nB and D require additional effort\nC Glue workflows do not have a direct integration with lambda\nhence the best choice is A","comment_id":"1210741","poster":"hnk"},{"content":"Selected Answer: A\nKey word orchestrating is most likely step functions","comment_id":"1181390","timestamp":"1711269840.0","poster":"FuriouZ","upvote_count":"3"},{"upvote_count":"4","poster":"rralucard_","content":"Selected Answer: A\nOption A, using AWS Step Functions, is the best solution to meet the requirement with the least management overhead. Step Functions is designed for easy integration with AWS services like Lambda and Glue, providing a managed, low-code approach to orchestrate workflows. This allows for a more straightforward setup and less ongoing management compared to the other options.","comment_id":"1138395","timestamp":"1706867160.0"}],"url":"https://www.examtopics.com/discussions/amazon/view/132676-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer_images":[],"choices":{"C":"Use an AWS Glue workflow to run the Lambda function and then the AWS Glue job.","A":"Use an AWS Step Functions workflow that includes a state machine. Configure the state machine to run the Lambda function and then the AWS Glue job.","B":"Use an Apache Airflow workflow that is deployed on an Amazon EC2 instance. Define a directed acyclic graph (DAG) in which the first task is to call the Lambda function and the second task is to call the AWS Glue job.","D":"Use an Apache Airflow workflow that is deployed on Amazon Elastic Kubernetes Service (Amazon EKS). Define a directed acyclic graph (DAG) in which the first task is to call the Lambda function and the second task is to call the AWS Glue job."},"timestamp":"2024-02-02 10:46:00","question_text":"A data engineer must orchestrate a data pipeline that consists of one AWS Lambda function and one AWS Glue job. The solution must integrate with AWS services.\nWhich solution will meet these requirements with the LEAST management overhead?"}],"exam":{"isMCOnly":true,"lastUpdated":"11 Apr 2025","isBeta":false,"numberOfQuestions":207,"isImplemented":true,"name":"AWS Certified Data Engineer - Associate DEA-C01","provider":"Amazon","id":21},"currentPage":31},"__N_SSP":true}