{"pageProps":{"questions":[{"id":"8yxgAI7aBmr1Ycz8alav","timestamp":"2022-08-31 00:22:00","answers_community":["B (100%)"],"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/78521-exam-aws-devops-engineer-professional-topic-1-question-7/","answer_ET":"B","isMC":true,"question_text":"A DevOps engineer is developing an application for a company. The application needs to persist files to Amazon S3. The application needs to upload files with different security classifications that the company defines. These classifications include confidential, private, and public. Files that have a confidential classification must not be viewable by anyone other than the user who uploaded them. The application uses the IAM role of the user to call the S3 API operations.\nThe DevOps engineer has modified the application to add a DataClassification tag with the value of confidential and an Owner tag with the uploading user's ID to each confidential object that is uploaded to Amazon S3.\nWhich set of additional steps must the DevOps engineer take to meet the company's requirements?","exam_id":35,"answer":"B","topic":"1","answer_images":[],"unix_timestamp":1661898120,"choices":{"A":"Modify the S3 bucket's ACL to grant bucket-owner-read access to the uploading user's IAM role. Create an IAM policy that grants s3:GetObject operations on the S3 bucket when aws:ResourceTag/DataClassification equals confidential, and s3:ExistingObjectTag/Owner equals ${aws:userid}. Attach the policy to the IAM roles for users who require access to the S3 bucket.","B":"Modify the S3 bucket policy to allow the s3:GetObject action when aws:ResourceTag/DataClassification equals confidential, and s3:ExistingObjectTag/Owner equals ${aws:userid}. Create an IAM policy that grants s3:GetObject operations on the S3 bucket. Attach the policy to the IAM roles for users who require access to the S3 bucket.","C":"Modify the S3 bucket policy to allow the s3:GetObject action when aws:ResourceTag/DataClassification equals confidential, and aws:RequesttTag/Owner equals ${aws:userid}. Create an IAM policy that grants s3:GetObject operations on the S3 bucket. Attach the policy to the IAM roles for users who require access to the S3 bucket.","D":"Modify the S3 bucket's ACL to grant authenticated-read access when aws:ResourceTag/DataClassification equals confidential, and s3:ExistingObjectTag/Owner equals ${aws:userid}. Create an IAM policy that grants s3:GetObject operations on the S3 bucket. Attach the policy to the IAM roles for users who require access to the S3 bucket."},"answer_description":"","discussion":[{"comment_id":"655010","timestamp":"1661943240.0","content":"Selected Answer: B\nB - https://docs.aws.amazon.com/AmazonS3/latest/userguide/tagging-and-policies.html","poster":"ohcn","upvote_count":"8"},{"poster":"xdkonorek2","upvote_count":"1","comment_id":"1133303","content":"None\n\nYou can rule out B and C momentarily because GetObject on IAM permission will give it permission to all IAM roles with attached policy regardless of bucket policy, having no deny statements.\n\nYou can rule out A and D because ACLs can be conditionally applied or be applied to specific principals","timestamp":"1706355960.0"},{"poster":"xhi158","timestamp":"1700399340.0","content":"B is the answer\n\nTo meet the companyâ€™s requirements, the DevOps engineer must modify the S3 bucket policy to allow the s3:GetObject action when aws:ResourceTag/DataClassification equals confidential, and s3:ExistingObjectTag/Owner equals ${aws:userid}. The engineer must also create an IAM policy that grants s3:GetObject operations on the S3 bucket. The policy should be attached to the IAM roles for users who require access to the S3 bucket","comment_id":"1074621","upvote_count":"1"},{"poster":"hp298","timestamp":"1679266380.0","content":"Selected Answer: B\nB is the answer","comment_id":"844310","upvote_count":"1"},{"content":"Selected Answer: B\nB sounds great.","timestamp":"1677681120.0","poster":"sasa33_p","comment_id":"825966","upvote_count":"1"},{"timestamp":"1672033740.0","upvote_count":"1","poster":"Bulti","comment_id":"757092","content":"B. Based on the link given by ohcn"},{"timestamp":"1665380220.0","comment_id":"690732","upvote_count":"1","poster":"nzin4x","content":"Selected Answer: B\ngood enough"},{"content":"B sound correct","poster":"dangdoan","timestamp":"1662434460.0","upvote_count":"1","comment_id":"660754"},{"comment_id":"654469","content":"B\nSounds most doable","poster":"colinquek","upvote_count":"1","timestamp":"1661898120.0"}],"question_id":176},{"id":"67fsvWPWXRYmHLFyXDkH","answer_description":"","answers_community":["A (75%)","C (25%)"],"choices":{"D":"Enable versioning and branching on each S3 bucket, use the main branch for production code, and create a testing branch for code deployed to testing. Have developers use each branch for developing in each environment.","A":"Create an AWS CodeCommit repository for each project, use the main branch for production code, and create a testing branch for code deployed to testing. Use feature branches to develop new features and pull requests to merge code to testing and main branches.","B":"Create another S3 bucket for each project for testing code, and use an AWS Lambda function to promote code changes between testing and production buckets. Enable versioning on all buckets to prevent code conflicts.","C":"Create an AWS CodeCommit repository for each project, and use the main branch for production and test code with different deployment pipelines for each environment. Use feature branches to develop new features."},"answer_ET":"A","question_id":177,"answer_images":[],"discussion":[{"poster":"dinhvu","timestamp":"1633053060.0","upvote_count":"15","content":"the correct answer is A","comment_id":"37073"},{"comments":[{"comment_id":"353851","poster":"JohnnieWalker","upvote_count":"2","timestamp":"1636055160.0","content":"I think it is (C) too because it has a pipeline for deployment and this is a requirement. \n(A) doesnt mention deployment, there is no pipelines.\nRegarding WhyIronMan comment, it doesnt mention it will deploy to production without test, in fact it will have 3 pipelines dev, test and prod, so the code will be tested on the test pipeline, and then deployed to prod as they are separate pipelines."},{"comments":[{"content":"I accidentally liked your answer, but I don't. Nobody is saying that you should not test the code deployed to test environment before proceeding to deploy it to production. How will you know that it is the same code that is deployed in test and production if you deploy from different branches? I'm a developer so I should know :) You're not so genius yourself.","timestamp":"1636161480.0","upvote_count":"2","poster":"is4_","comment_id":"439174"}],"content":"So, you want deploy do production without test? what warranties you that the code is functional and its not broken?\n\nGenius strategy","upvote_count":"4","timestamp":"1635203400.0","comment_id":"324232","poster":"WhyIronMan"}],"upvote_count":"6","content":"The answer should be C because of below line in the question: \"allow Developers to automatically deploy to both environments when code is changed in the repository\".\nOption A does not talk about deployment.","comment_id":"199827","timestamp":"1633948380.0","poster":"shankyup43"},{"upvote_count":"2","content":"using the main branch for both production and test code can lead to issues with tracking changes and version control. It can also make it difficult to deploy code automatically to the correct environment. That's why the answer is A and not C.","comment_id":"838367","timestamp":"1678746180.0","poster":"easytoo"},{"poster":"Piccaso","comment_id":"811991","content":"Selected Answer: A\nB and D are eliminated, will make chaos.\nC is an incomplete version of A. If we do not use CodeCommit, we should combine different env files to different branches, like test, staging, prod....","timestamp":"1676644740.0","upvote_count":"1"},{"comment_id":"787586","upvote_count":"2","poster":"sasivarenan","content":"Selected Answer: A\nA) Explains the branching strategy","timestamp":"1674648720.0"},{"upvote_count":"6","content":"Correct answer is A. There is nothing that suggests that the code needs to be deployed simultaneously in both environments. It just says automatically to both environments. That doesn't mean the pull request from a feature branch needs to be created against both test and prod at the same time. What A will enable us to do is have the feature engineer submit a pull request into the test branch and have the admin user of the tear branch review and approve the pull request. This should automatically start the deployment into the test environment. Once the code is full tested by the QA resource in the test environment, they will initiate a pull request into the Prod environment. The prod environment admin user will review and approve the pull request at which point the code will be automatically deployment to the Prod environment. With C on the other hand submitting a pull request from the feature branch to the master branch will result in triggering the deployment via both pipelines at the same time which means the code will be deployed into production before testing it in the test environment successfully.","poster":"Bulti","timestamp":"1674317220.0","comment_id":"783512"},{"timestamp":"1672785360.0","upvote_count":"2","poster":"saeidp","content":"Selected Answer: A\nI'll go with A","comment_id":"765102"},{"upvote_count":"2","comment_id":"758536","poster":"Chinta","timestamp":"1672147200.0","content":"Answer is A ,as per the question you have to eploy new version of code for testing"},{"comment_id":"721417","poster":"USalo","content":"Selected Answer: C\nC.\nIt cannot be A because you make the same PR to both branches at the same time ONLY when you need a hotfix to prod and test env. In real world you develop in one branch and when you need a release you merge all the dev branch to prod branch (or create a new version from dev branch if you need to support multiple versions). But you NEVER EVER make PRs to both dev and prod branches during development.\nIf you both branches are always the same - why do you need 2 branches ? If your master and testing branches differ then at some point you will have conflicts. \nOnly C makes sense.","upvote_count":"1","timestamp":"1668789420.0"},{"upvote_count":"1","poster":"RightAnswers","comment_id":"680186","timestamp":"1664227860.0","content":"Selected Answer: C\nWith a different test branch, there is no way to know if the same code is deployed to two different environments."},{"comment_id":"665749","timestamp":"1662859200.0","poster":"MichaelExam","content":"Selected Answer: A\nI choice A \nThe difference between A and C is that C uses master branch for the production and testing environment, A uses different branches for the production and testing environment. The better way is A. \nB,D S3 is not good choice for team working.","upvote_count":"3"},{"poster":"jexam211","comment_id":"648587","upvote_count":"4","content":"Selected Answer: A\nHonestly a prefer 3 different branches, A option\n\nEvent with the C option different pipelines, which event is triggering the pipeline, when i push the commit in the test/prod branch bad idea, manual trigger? not too devOps at least, another type of trigger maybe add more complexity.","timestamp":"1660849980.0"},{"content":"Selected Answer: C\ndifferent codepipelines for the same branch seems normal","poster":"[Removed]","timestamp":"1646459820.0","upvote_count":"2","comment_id":"561252"},{"comments":[{"comments":[{"timestamp":"1640502660.0","poster":"gofavad926","upvote_count":"3","content":"Agree with C, talking about branch strategy (only master for 2 environments) and mention deployments. \n\nA is not talking about deployment so you are not meeting the requirements... only mention branch strategy (in more detail and different from A)","comment_id":"509462"}],"content":"You often deploy from the same branch so that you know that it is the same code running in all environments. How will you know this otherwise? That can be really unstable if you dont have a pipeline set up that makes sure that no one can push to the master branch anywhere else than from this \"test branch\". \nBut you ALWAYS test the code in the test / staging environment before proceeding to deploy it to production. This is how its often done (ofc exceptions exists) and it is a very good strategy. I bet y'all answering here have no experience working as developers, I have and I know this.","poster":"is4_","timestamp":"1636289700.0","upvote_count":"4","comment_id":"439179"}],"poster":"WhyIronMan","timestamp":"1635907920.0","content":"I'll go with A.\n\nfor those choosing C:\nSo, do you want deploy do production without testing first? \nwhat warranties you that the code is functional and its not broken?\n\nYeah, keep deploying untested code to production...its a genius strategy","comment_id":"324236","upvote_count":"4"},{"content":"Correct Answer: C","poster":"aws_Tamilan","comment_id":"322037","upvote_count":"2","timestamp":"1634794080.0"},{"timestamp":"1634453100.0","upvote_count":"2","poster":"rscloud","comment_id":"305305","content":"A is correct"},{"content":"Agreed with A","timestamp":"1634411220.0","poster":"dnevado","comment_id":"235676","upvote_count":"2"},{"poster":"jackdryan","content":"I'll go with A","upvote_count":"4","timestamp":"1634280060.0","comment_id":"223253"},{"timestamp":"1634170320.0","content":"A.https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html","upvote_count":"1","comment_id":"219105","poster":"awsrook"},{"comments":[{"comment_id":"324234","upvote_count":"1","comments":[{"content":"You typically want to make sure that it is always the same code revision that is deployed to BOTH test and production environments. Therefore, you always deploy the MASTER branch to BOTH environments but in stages - first to test and after successfully tested that environment, you proceed to production. So there is nothing stupid about that answer.","comment_id":"439173","upvote_count":"3","poster":"is4_","timestamp":"1636149480.0"}],"poster":"WhyIronMan","timestamp":"1635538620.0","content":"So, you want deploy do production without test? what warranties you that the code is functional and its not broken?\n\nGenius strategy"}],"content":"Prefer to C. I don't see a problem using the master branch for both prod and test environments in an agile team. For A, don't think merging PR to 2 branches is not good practice.","poster":"cloudyland","comment_id":"218956","upvote_count":"3","timestamp":"1634060520.0"},{"poster":"ChauPhan","timestamp":"1634033340.0","upvote_count":"3","comment_id":"206854","content":"It's A"},{"timestamp":"1633892160.0","upvote_count":"2","comment_id":"107572","poster":"un","content":"A is correct"},{"upvote_count":"3","content":"Will go with A","timestamp":"1633772400.0","poster":"Raj9","comment_id":"107359"},{"upvote_count":"2","content":"I will go with C. Why we want to have two branches to manage code when PR is going to be merged in both the branch at same time.\n\nSo C is correct answer.","comments":[{"poster":"nawfal6809","comments":[{"content":"\"You can't use the master branch for testing..\". How do you know? Do you have developer experience? Because I have and you typically deploy the master branch to FIRST test and THEN production. This is how you know it is the SAME code running in both environments. How would you know that if you deploy from different branches? You typically have a pipeline that makes sure that there is test running or approvals made before deploying the same branch to production environment.","timestamp":"1636200360.0","upvote_count":"2","comment_id":"439175","poster":"is4_"}],"upvote_count":"4","content":"You can't use the master branch for testing, use it only on production. So the answer is A","timestamp":"1633617240.0","comment_id":"85849"},{"upvote_count":"3","comment_id":"324229","timestamp":"1634863020.0","poster":"WhyIronMan","content":"When \"testing\" is stable, tested and production ready, you open a pull request to merge it into master branch.\nThis is the basics of devops man..."}],"timestamp":"1633589040.0","poster":"secreatUser","comment_id":"82477"},{"timestamp":"1633236120.0","upvote_count":"3","content":"A is correct","comment_id":"62195","poster":"yassu"},{"poster":"Ebi","content":"A is the answer","upvote_count":"2","timestamp":"1633119900.0","comment_id":"61579"},{"timestamp":"1633063500.0","content":"It is A","upvote_count":"2","poster":"xaocho","comment_id":"56495"},{"poster":"jiedee","upvote_count":"1","comments":[{"timestamp":"1632627900.0","comment_id":"29569","poster":"Booby","content":"You shouldn't use master branch for production and test, so answer is A","comments":[{"comment_id":"439177","timestamp":"1636275480.0","content":"This is simply not true, the master is commonly used for ALL ENVIRONMENTS, this is so you know that it is the same code running in all environments. But the code is tested in the test / stage environments before deployed to productions. There is exceptions of course, but this is common practice. I have been working as a developer for several years, I now this for a fact.","upvote_count":"1","poster":"is4_"}],"upvote_count":"10"}],"comment_id":"28414","content":"Why not c?","timestamp":"1632537420.0"}],"question_images":[],"question_text":"A company has a single developer writing code for an automated deployment pipeline. The developer is storing source code in an Amazon S3 bucket for each project. The company wants to add more developers to the team but is concerned about code conflicts and lost work. The company also wants to build a test environment to deploy newer versions of code for testing and allow developers to automatically deploy to both environments when code is changed in the repository.\nWhat is the MOST efficient way to meet these requirements?","answer":"A","unix_timestamp":1575945540,"isMC":true,"topic":"1","timestamp":"2019-12-10 03:39:00","exam_id":35,"url":"https://www.examtopics.com/discussions/amazon/view/10086-exam-aws-devops-engineer-professional-topic-1-question-70/"},{"id":"AYBpCe0uZfdUgO2NG8Ho","unix_timestamp":1662400860,"question_images":[],"isMC":true,"discussion":[{"timestamp":"1675859100.0","content":"Selected Answer: B\nThe link and material offered by Paresh_Jadhav helps.","poster":"Piccaso","upvote_count":"3","comment_id":"802007"},{"comment_id":"783527","timestamp":"1674318000.0","content":"B us the correct answer. You don't want to lose messages and therefore SQS queue and the retrieve configuration is at the subscription level and not at the topic level.","upvote_count":"1","poster":"Bulti"},{"comment_id":"758542","timestamp":"1672147380.0","content":"B is correct","poster":"Chinta","upvote_count":"1"},{"comment_id":"728774","timestamp":"1669609440.0","poster":"Paresh_Jadhav","content":"B make sense\nhttps://docs.aws.amazon.com/sns/latest/dg/sns-message-delivery-retries.html\n\n\"To keep the message after the retries specified in the delivery policy are exhausted, configure your subscription to move undeliverables messages to a dead-letter queue (DLQ).\"\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-available-cloudwatch-metrics.html","upvote_count":"4"},{"upvote_count":"3","timestamp":"1669264680.0","content":"Selected Answer: B\nVoted B, because backoffFunction is just a definition algorithm of backoff retry, cannot use it to notify the team.","poster":"kyozanuro","comment_id":"725556"},{"poster":"hankun","content":"B is true. \nC, D is Https endpoint not http","timestamp":"1663484760.0","upvote_count":"1","comment_id":"672055"},{"comment_id":"668625","poster":"MikeyJ","upvote_count":"1","content":"Selected Answer: B\nYou can use Amazon CloudWatch metrics to monitor dead-letter queues associated with your Amazon SNS subscriptions. All Amazon SQS queues emit CloudWatch metrics at one-minute intervals.","timestamp":"1663132800.0"},{"upvote_count":"3","timestamp":"1663065720.0","poster":"syaldram","comment_id":"667884","content":"The answer is B:\nhttps://docs.aws.amazon.com/sns/latest/dg/sns-dead-letter-queues.html\n\nThe dead letter queue for SNS are SQS."},{"upvote_count":"1","content":"C makes sense.","timestamp":"1662405000.0","comment_id":"660482","poster":"ohcn","comments":[{"comments":[{"upvote_count":"1","poster":"network_zeal","comment_id":"672248","timestamp":"1663498560.0","content":"D is incorrect. the same link clearly mentions, When the delivery policy is exhausted, Amazon SNS stops retrying the delivery and discards the messageâ€”unless a dead-letter queue is attached to the subscription. So B is correct."}],"comment_id":"663650","poster":"ohcn","content":"In fact might be D because one of the HTTP endpoints is not always available. \n\n\"You should customize your delivery policy according to your HTTP/S server's capacity. You can set the policy as a topic attribute or a subscription attribute. If all HTTP/S subscriptions in your topic target the same HTTP/S server, we recommend that you set the delivery policy as a topic attribute, so that it remains valid for all HTTP/S subscriptions in the topic. Otherwise, you must compose a delivery policy for each HTTP/S subscription in your topic, according the capacity of the HTTP/S server that the policy targets.\"\"\n\nhttps://docs.aws.amazon.com/sns/latest/dg/sns-message-delivery-retries.html","upvote_count":"1","timestamp":"1662643320.0"}]},{"content":"I vote for B","poster":"costin","timestamp":"1662400860.0","comment_id":"660419","upvote_count":"1"}],"url":"https://www.examtopics.com/discussions/amazon/view/80403-exam-aws-devops-engineer-professional-topic-1-question-71/","timestamp":"2022-09-05 20:01:00","topic":"1","exam_id":35,"answer_ET":"B","question_text":"A development team is building an ecommerce application and is using Amazon Simple Notification Service (Amazon SNS) to send order messages to multiple endpoints. One of the endpoints is an external HTTP endpoint that is not always available. The development team needs to receive a notification if an order message is not delivered to the HTTP endpoint.\nWhat should a DevOps engineer do to meet these requirements?","answers_community":["B (100%)"],"answer":"B","choices":{"C":"On the SNS topic, configure an HTTPS delivery policy that will retry delivery until the order message is delivered successfully. Configure the backoffFunction parameter in the policy to notify the development team when a message cannot be delivered within the set constraints.","B":"Create an Amazon Simple Queue Service (Amazon SQS) queue. On the HTTP endpoint subscription of the SNS topic, configure a redrive policy that sends undelivered messages to the SQS queue. Create an Amazon CloudWatch alarm for the new SQS queue to notify the development team when messages are delivered to the queue.","A":"Create an Amazon Simple Queue Service (Amazon SQS) queue. On the SNS topic, configure a redrive policy that sends undelivered messages to the SQS queue. Create an Amazon CloudWatch alarm for the new SQS queue to notify the development team when messages are delivered to the queue.","D":"On the HTTP endpoint subscription of the SNS topic, configure an HTTPS delivery policy that will retry delivery until the order message is delivered successfully. Configure the backoffFunction parameter in the policy to notify the development team when a message cannot be delivered within the set constraints."},"question_id":178,"answer_images":[],"answer_description":""},{"id":"gpY3DqQaHpy19HdQRpQP","isMC":true,"question_id":179,"timestamp":"2020-08-11 09:57:00","answer":"C","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/28041-exam-aws-devops-engineer-professional-topic-1-question-72/","answer_description":"","unix_timestamp":1597132620,"exam_id":35,"answers_community":["C (100%)"],"discussion":[{"poster":"Augustoosouza","content":"Ans C\nhttps://docs.aws.amazon.com/pt_br/codebuild/latest/userguide/build-spec-ref.html","timestamp":"1633213140.0","comment_id":"155219","upvote_count":"12"},{"timestamp":"1676485140.0","comment_id":"809838","content":"Selected Answer: C\nA: \"Store the encrypted password in the buildspec.yml file ...\" , does it make any sense ?\nB: AWS CloudHSM lets you manage and access your keys on FIPS-validated hardware, protected with customer-owned, single-tenant HSM instances that run in your own Virtual Private Cloud (VPC), does not match this context. \nD: unreliable method","poster":"Piccaso","upvote_count":"2"},{"timestamp":"1674318120.0","comment_id":"783528","upvote_count":"1","poster":"Bulti","content":"C is the right answer"},{"poster":"saggy4","timestamp":"1671075660.0","upvote_count":"1","content":"Selected Answer: C\nIt is C","comment_id":"745671"},{"timestamp":"1635795000.0","poster":"gmandala","upvote_count":"1","content":"C it is","comment_id":"253098"},{"content":"Its completely C","comment_id":"238037","poster":"dnevado","timestamp":"1635376680.0","upvote_count":"1"},{"poster":"jackdryan","timestamp":"1634288580.0","content":"I'll go with C","comment_id":"224230","upvote_count":"3"},{"comment_id":"209781","content":"C is fine","upvote_count":"1","poster":"ChauPhan","timestamp":"1633797960.0"},{"timestamp":"1633721340.0","comment_id":"172213","poster":"incorrigble_maverick","upvote_count":"2","content":"C without a shadow of a doubt"},{"comment_id":"163407","timestamp":"1633672620.0","content":"C. \nStore password in System manager parameter store.","poster":"halfway","upvote_count":"1"},{"poster":"krrish96","timestamp":"1633548420.0","comment_id":"162258","content":"I will go with C","upvote_count":"1"}],"question_text":"A company is deploying a container-based application using AWS CodeBuild. The Security team mandates that all containers are scanned for vulnerabilities prior to deployment using a password-protected endpoint. All sensitive information must be stored securely.\nWhich solution should be used to meet these requirements?","answer_ET":"C","choices":{"C":"Store the password in the AWS Systems Manager Parameter Store as a secure string. Add the Parameter Store key to the buildspec.yml file as an environment variable under the parameter-store mapping. Reference the environment variable to initiate scanning.","B":"Import the password into an AWS CloudHSM key. Reference the CloudHSM key in the buildpec.yml file as an environment variable under the variables mapping. Reference the environment variable to initiate scanning.","D":"Use the AWS Encryption SDK to encrypt the password and embed in the buildspec.yml file as a variable under the secrets mapping. Attach a policy to CodeBuild to enable access to the required decryption key.","A":"Encrypt the password using AWS KMS. Store the encrypted password in the buildspec.yml file as an environment variable under the variables mapping. Reference the environment variable to initiate scanning."},"topic":"1","answer_images":[]},{"id":"UpRcFNDrYOrVb37GblRd","unix_timestamp":1563912840,"question_images":[],"isMC":true,"discussion":[{"poster":"agomes","timestamp":"1632466440.0","comment_id":"4805","upvote_count":"17","content":"In this case would choose D, because application is executed nodejs | I don't know how S3 would help in this case."},{"comments":[{"content":"Node.js is key because we want to get those Node.js configs into AMI.","poster":"a112883","timestamp":"1635024360.0","comment_id":"231415","upvote_count":"2"}],"poster":"YashBindlish","content":"Node.js is a key. Correct answer is D","comment_id":"12412","timestamp":"1632557160.0","upvote_count":"7"},{"content":"In terms of reducing configuration time, option A has the potential to be faster. This is because AWS OpsWorks can automatically configure each new EC2 instance as it is launched. OpsWorks provides a framework for defining and managing applications using Chef or Puppet, allowing for automated configuration and deployment.\n\nOn the other hand, option D involves using AWS Elastic Beanstalk with a custom AMI. While Elastic Beanstalk simplifies the deployment and management of applications, it may still require some initial configuration and setup time for the custom AMI. This could potentially result in longer configuration times compared to using OpsWorks for automated configuration.","upvote_count":"1","comment_id":"906191","timestamp":"1684969740.0","poster":"easytoo"},{"timestamp":"1675952880.0","comment_id":"803314","upvote_count":"1","content":"Selected Answer: D\nD is corretct","poster":"bihani"},{"content":"Pre-baked EC2 instances along with node.js points to Elastic BeanStalk.","comments":[{"upvote_count":"1","timestamp":"1674318480.0","content":"Answer is D","comment_id":"783533","poster":"Bulti"}],"upvote_count":"1","timestamp":"1674318420.0","poster":"Bulti","comment_id":"783532"},{"upvote_count":"1","timestamp":"1635998040.0","poster":"oopsy","comment_id":"447427","content":"Go D -1"},{"poster":"Dantehilary","content":"I think the answer is A, D is wrong because THERE IS NOTHING LIKE Elastic Beanstalk load balance","comment_id":"374347","upvote_count":"1","comments":[{"upvote_count":"4","content":"They refer to a LB created by Beanstalk","comment_id":"403972","poster":"siejas","timestamp":"1635887940.0"}],"timestamp":"1635886080.0"},{"timestamp":"1635844380.0","comment_id":"322347","poster":"poylan","upvote_count":"1","content":"i'll go with D"},{"upvote_count":"6","comment_id":"320264","timestamp":"1635830520.0","content":"I'll go with D)\nCreating AMI is always a good practice when instances took up to 20 minutes to become fully configured...","poster":"WhyIronMan"},{"comment_id":"318240","poster":"glam","timestamp":"1635730320.0","upvote_count":"1","content":"D. Use AWS Elastic Beanstalk with a custom AMI including all web components. Deploy the platform by using an Auto Scaling group behind an Application Load Balancer across multiple Availability Zones. Implement Amazon DynamoDB Auto Scaling. Use Amazon Route 53 to point the application DNS record to the Elastic Beanstalk load balancer."},{"content":"D is the answer.","poster":"fogunfunminiyi","timestamp":"1635704760.0","comment_id":"252334","upvote_count":"1"},{"upvote_count":"1","content":"When your instance is taking time to boot, it means some configurations are going on probably through user data. May be like fetching some application artifacts from internet, and installing them. This is good but not the best way to configure your instance during booting with user data. The best way is to create a custom IMAGE with preconfigured applications. Then when you lunch the custom image, it automatically come with existing applications, thus reducing time to boot. For instance, AMI from AWS are preconfigured with cloud watch log agent. You donâ€™t need to install it once you lunch AWS ima. But you have to manually when you lunched (using sudo yum install after ssh into the lunched ec2 or through user data ) when the instance is booting.","timestamp":"1635195840.0","comment_id":"252332","poster":"fogunfunminiyi"},{"poster":"[Removed]","timestamp":"1635169740.0","comment_id":"247595","upvote_count":"2","content":"Yup, fully baked EC2 is the only option here (D)\nA would not speed things up, if anything, slow things down as chef would have to push that config in to the stock AMI"},{"timestamp":"1634982960.0","comment_id":"229688","content":"Right answer: D - custom AMI is key here it will reduce the provisioning time dramatically - main issue - and also multi-az and ALB are mentioned for resiliency and high-availability. \nWrong:\nA - OpsWorks could do it but \"automatically configure each new EC2 as it is launched\" would keep the slow start issue. \nB - Could be right but does not mention multi-AZ as well is based on manual changes instead of auto scaling. \nC - you need Ec2 due to Node.js server-side","upvote_count":"4","poster":"Coffeinerd"},{"comment_id":"223190","timestamp":"1634933880.0","upvote_count":"3","content":"I'll go with D","poster":"jackdryan"},{"comment_id":"207891","timestamp":"1634780820.0","poster":"Dr_Wells","upvote_count":"1","content":"Its Option D"},{"content":"The answer should be D. Only with custom AMI we can reduce the configuration time. Because custom AMI that means all application/configuration was installed/built in-bulk/in ready and put into an AMI image and we don't need to configure anything after that.","upvote_count":"1","poster":"ChauPhan","comment_id":"205196","timestamp":"1634556540.0"},{"poster":"FrankSparrow","upvote_count":"2","timestamp":"1634515560.0","comment_id":"177078","content":"Certainly \"D\" - The custom AMI will reduce the config time of 20mins"},{"upvote_count":"1","poster":"nqobza","comment_id":"166297","content":"Has to be D","timestamp":"1634467080.0"},{"poster":"MouradMik","timestamp":"1634302680.0","comment_id":"114494","content":"My answer is D, using custom AMI is the key,","upvote_count":"1"},{"upvote_count":"1","poster":"Raj9","timestamp":"1634007540.0","content":"D seems correct. The key being \"custom AMI including all web components\" . This will reduce that 20 minutes load time which the question points to.","comment_id":"93793"},{"content":"D is correct","timestamp":"1633878180.0","poster":"yassu","upvote_count":"1","comment_id":"62127"},{"upvote_count":"1","content":"It is D","poster":"xaocho","comment_id":"54585","timestamp":"1633559220.0"},{"poster":"consultsk","comment_id":"40498","upvote_count":"1","timestamp":"1633495260.0","content":"The answer is 'D' IMO. There are a lot of distractors in the question. Answer 'C' is another one. There is no static page here to make use of S3 and Cloudfront/Caching. The issue here is \"the instances took up to 20 minutes to become fully configured. The team wants to reduce this configuration time\". It menas the instances are taking time to come up and available to serve. I will agree with 'neil001'. I would chose 'D' if I get this in exam."},{"upvote_count":"2","comment_id":"34993","poster":"un","timestamp":"1633422420.0","content":"I would go with D"},{"timestamp":"1633168980.0","content":"\"but the instances took up to 20 minutes to become fully configured\" means that you need a custom AMI, I will go with D","poster":"jiedee","comment_id":"28335","upvote_count":"4"},{"timestamp":"1633051620.0","poster":"kkkn","comment_id":"24933","upvote_count":"2","content":"The key here is customized AMI to bring boot time down"},{"comments":[{"comment_id":"12848","poster":"neil001","timestamp":"1632883860.0","upvote_count":"15","content":"Well, the main problem here is the 20 minutes it took for the configuration of the app before it became online, to decrease that time you make an ec2 instance with all web components included and configured, create an AMI from the EC2 instance and then use that AMI inside Elastic Beanstalk for faster deployment times.\nAs for `there is no such thing as \"Elastic Beanstalk load balancer\".` --> actually there is as below\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.elb.html\n\n\"By default, Elastic Beanstalk creates an Application Load Balancer for your environment when you enable load balancing with the Elastic Beanstalk console or the EB CLI\""}],"poster":"sorbhs","content":"Moreover, if we go by actual aws terminology for option D, there is no such thing as \"Elastic Beanstalk load balancer\".","timestamp":"1632793260.0","comment_id":"12743","upvote_count":"1"},{"content":"Why not Option A. Can OpsWorks do it?\nMoreover, if we go by actual aws terminology, there is no such thing as \"Elastic Beanstalk load balancer\".","comments":[{"comment_id":"133043","poster":"AKD","upvote_count":"2","content":"No there are two kinds of EB environments: single EC2 or ASG with ELB.\nOption D is the correct one.","timestamp":"1634389740.0"}],"comment_id":"12742","timestamp":"1632763740.0","upvote_count":"1","poster":"sorbhs"},{"poster":"toma","content":"i agree, answer is D from beginning , my bad typing","comment_id":"4846","timestamp":"1632545400.0","upvote_count":"4"},{"comment_id":"4698","upvote_count":"1","timestamp":"1632125700.0","poster":"toma","content":"i would say C"}],"url":"https://www.examtopics.com/discussions/amazon/view/2748-exam-aws-devops-engineer-professional-topic-1-question-73/","timestamp":"2019-07-23 22:14:00","topic":"1","exam_id":35,"answer_ET":"D","answers_community":["D (100%)"],"question_text":"An Engineering team manages a Node.js e-commerce application. The current environment consists of the following components:\nâœ‘ Amazon S3 buckets for storing content\nâœ‘ Amazon EC2 for the front-end web servers\nâœ‘ AWS Lambda for image processing\nâœ‘ Amazon DynamoDB for storing session-related data\nThe team expects a significant increase in traffic to the site. The application should handle the additional load without interruption. The team ran initial tests by adding new servers to the EC2 front-end to handle the larger load, but the instances took up to 20 minutes to become fully configured. The team wants to reduce this configuration time.\nWhat changes will the Engineering team need to implement to make the solution the MOST resilient and highly available while meeting the expected increase in demand?","answer":"D","question_id":180,"choices":{"D":"Use AWS Elastic Beanstalk with a custom AMI including all web components. Deploy the platform by using an Auto Scaling group behind an Application Load Balancer across multiple Availability Zones. Implement Amazon DynamoDB Auto Scaling. Use Amazon Route 53 to point the application DNS record to the Elastic Beanstalk load balancer.","A":"Use AWS OpsWorks to automatically configure each new EC2 instance as it is launched. Configure the EC2 instances by using an Auto Scaling group behind an Application Load Balancer across multiple Availability Zones. Implement Amazon DynamoDB Auto Scaling. Use Amazon Route 53 to point the application DNS record to the Application Load Balancer.","C":"Configure Amazon CloudFront and have its origin point to Amazon S3 to host the web application. Implement Amazon DynamoDB Auto Scaling. Use Amazon Route 53 to point the application DNS record to the CloudFront DNS name.","B":"Deploy a fleet of EC2 instances, doubling the current capacity, and place them behind an Application Load Balancer. Increase the Amazon DynamoDB read and write capacity units. Add an alias record that contains the Application Load Balancer endpoint to the existing Amazon Route 53 DNS record that points to the application."},"answer_images":[],"answer_description":""}],"exam":{"lastUpdated":"11 Apr 2025","numberOfQuestions":208,"isImplemented":true,"isBeta":false,"provider":"Amazon","id":35,"isMCOnly":false,"name":"AWS DevOps Engineer Professional"},"currentPage":36},"__N_SSP":true}