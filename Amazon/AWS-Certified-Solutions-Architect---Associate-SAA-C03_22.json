{"pageProps":{"questions":[{"id":"bWEf2xOGEHTc2zAldsFa","answer_description":"","timestamp":"2022-11-15 16:19:00","question_id":106,"question_images":[],"choices":{"C":"Use a NAT instance in a private subnet.","B":"Use a NAT gateway in a public subnet.","D":"Use the internet gateway attached to the VPC.","A":"Use a VPC endpoint for DynamoDB."},"answer_images":[],"question_text":"An application runs on Amazon EC2 instances in private subnets. The application needs to access an Amazon DynamoDB table.\n\nWhat is the MOST secure way to access the table while ensuring that the traffic does not leave the AWS network?","answer":"A","discussion":[{"poster":"mabotega","content":"Selected Answer: A\nVPC endpoints for service in private subnets\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html","timestamp":"1700083560.0","upvote_count":"18","comment_id":"719145"},{"comment_id":"934231","content":"Option B (using a NAT gateway in a public subnet) and option C (using a NAT instance in a private subnet) are not the most secure options because they involve routing traffic through a network address translation (NAT) device, which requires an internet gateway and traverses the public internet.\n\nOption D (using the internet gateway attached to the VPC) would require routing traffic through the internet gateway, which would result in the traffic leaving the AWS network.\n\nTherefore, the recommended and most secure approach is to use a VPC endpoint for DynamoDB to ensure private and secure access to the DynamoDB table from your EC2 instances in private subnets, without the need to traverse the internet or leave the AWS network.","poster":"cookieMr","timestamp":"1719393120.0","upvote_count":"7"},{"content":"Selected Answer: A\nA gateway VPC endpoint is the most suitable for accessing DynamoDB and S3.","poster":"satyaammm","timestamp":"1739035260.0","upvote_count":"1","comment_id":"1353535"},{"content":"Selected Answer: A\nUsing an internet gateway (Option D) is used for enabling outbound internet connectivity from resources in your VPC. It's not the appropriate choice for securely accessing DynamoDB within your VPC.","timestamp":"1727447700.0","poster":"vijaykamal","comment_id":"1018900","upvote_count":"3"},{"upvote_count":"2","poster":"Ramdi1","timestamp":"1726326780.0","content":"Selected Answer: A\nA gateway VPC Endpoint is designed for supported AWS service such as dynamo db or s3 in this case i assume the endpoint is still the valid option","comment_id":"1007769"},{"poster":"TariqKipkemei","upvote_count":"3","comment_id":"1005360","content":"Selected Answer: A\nUse a VPC endpoint for DynamoDB. A VPC endpoint enables customers to privately connect to supported AWS services: Amazon DynamoDB or Amazon Simple Storage Service (Amazon S3).","timestamp":"1726115760.0"},{"upvote_count":"2","poster":"Guru4Cloud","timestamp":"1723912260.0","content":"Selected Answer: A\nA VPC endpoint enables private connectivity between VPCs and AWS services without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect. Traffic remains within the AWS network.","comment_id":"983818"},{"upvote_count":"2","content":"Selected Answer: A\nVPC endpoints for service in private subnets","poster":"MikeDu","comment_id":"979412","timestamp":"1723473780.0"},{"content":"Selected Answer: A\nVPC endpoint for dynamodb and S3","comment_id":"947213","timestamp":"1720527840.0","upvote_count":"2","poster":"RashiJaiswal"},{"content":"VPC endpoints for DynamoDB can alleviate these challenges. A VPC endpoint for DynamoDB enables Amazon EC2 instances in your VPC to use their private IP addresses to access DynamoDB with no exposure to the public internet. Your EC2 instances do not require public IP addresses, and you don't need an internet gateway, a NAT device, or a virtual private gateway in your VPC. You use endpoint policies to control access to DynamoDB. Traffic between your VPC and the AWS service does not leave the Amazon network.","comment_id":"926094","timestamp":"1718638260.0","poster":"markw92","upvote_count":"4"},{"poster":"dmt6263","comment_id":"897006","content":"AAAAAAAAA","timestamp":"1715625360.0","upvote_count":"2"},{"timestamp":"1712351160.0","content":"Selected Answer: A\nOption A: Use a VPC endpoint for DynamoDB - This is the correct option. A VPC endpoint for DynamoDB allows communication between resources in your VPC and Amazon DynamoDB without traversing the internet or a NAT instance, which is more secure.","upvote_count":"3","poster":"gx2222","comment_id":"862471"},{"timestamp":"1710088800.0","upvote_count":"2","poster":"GalileoEC2","comment_id":"835234","content":"A\nThe most secure way to access an Amazon DynamoDB table from Amazon EC2 instances in private subnets while ensuring that the traffic does not leave the AWS network is to use Amazon VPC Endpoints for DynamoDB.\n\nAmazon VPC Endpoints enable private communication between Amazon EC2 instances in a VPC and Amazon services such as DynamoDB, without the need for an internet gateway, NAT device, or VPN connection. When you create a VPC endpoint for DynamoDB, traffic from the EC2 instances to the DynamoDB table remains within the AWS network and does not traverse the public internet."},{"poster":"AllGOD","content":"private...backend Answer A","upvote_count":"2","timestamp":"1707879780.0","comment_id":"808017"},{"poster":"bdp123","upvote_count":"3","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpointsdynamodb.\nhtml A VPC endpoint for DynamoDB enables Amazon EC2 instances in your VPC to use\ntheir private IP addresses to access DynamoDB with no exposure to the public internet. Your EC2\ninstances do not require public IP addresses, and you don't need an internet gateway, a NAT device,\nor a virtual private gateway in your VPC. You use endpoint policies to control access to DynamoDB.\nTraffic between your VPC and the AWS service does not leave the Amazon network.","timestamp":"1706800080.0","comment_id":"795241"},{"poster":"ProfXsamson","upvote_count":"4","comment_id":"791525","timestamp":"1706521560.0","content":"ExamTopics.com should be sued for this answer tagged as Correct answer."},{"upvote_count":"4","poster":"mp165","comment_id":"761979","content":"Selected Answer: A\nA is correct. VPC end point. D exposed to the internet","timestamp":"1703942340.0"},{"poster":"Buruguduystunstugudunstuy","timestamp":"1703280540.0","comments":[{"content":"Option B, using a NAT gateway in a public subnet, would allow the traffic to leave the AWS network and traverse the public Internet, which is less secure. \n\nOption C, using a NAT instance in a private subnet, would also allow the traffic to leave the AWS network but would require you to manage the NAT instance yourself. \n\nOption D, using the internet gateway attached to the VPC, would also expose the traffic to the public Internet.","comment_id":"753651","upvote_count":"3","poster":"Buruguduystunstugudunstuy","timestamp":"1703280600.0"}],"content":"Selected Answer: A\nThe most secure way to access the DynamoDB table while ensuring that the traffic does not leave the AWS network is Option A (Use a VPC endpoint for DynamoDB.)\n\nA VPC endpoint for DynamoDB allows you to privately connect your VPC to the DynamoDB service without requiring an Internet Gateway, VPN connection, or AWS Direct Connect connection. This ensures that the traffic between the application and the DynamoDB table stays within the AWS network and is not exposed to the public Internet.","comment_id":"753650","upvote_count":"3"},{"upvote_count":"2","content":"A ---- is correct answer","timestamp":"1703045160.0","comment_id":"750480","poster":"BENICE"},{"timestamp":"1702841760.0","poster":"career360guru","content":"Selected Answer: A\nOption A.","upvote_count":"2","comment_id":"748342"},{"upvote_count":"2","poster":"Wpcorgan","timestamp":"1700671440.0","content":"A is correct","comment_id":"724490"},{"comment_id":"721973","content":"Sure A","poster":"xua81376","timestamp":"1700395560.0","upvote_count":"1"},{"comment_id":"720383","upvote_count":"3","poster":"ds0321","content":"Selected Answer: A\nA - VPC endpoint","timestamp":"1700215740.0"},{"timestamp":"1700100900.0","poster":"goatbernard","comment_id":"719276","content":"Selected Answer: A\nA - VPC endpoint","upvote_count":"4"},{"upvote_count":"2","timestamp":"1700062080.0","poster":"sdasdawa","content":"Selected Answer: A\nhttps://www.examtopics.com/discussions/amazon/view/27700-exam-aws-certified-solutions-architect-associate-saa-c02/","comment_id":"718912"},{"comment_id":"718908","timestamp":"1700061900.0","upvote_count":"4","content":"A for sure. https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html","poster":"Nigma"},{"content":"Its A.","timestamp":"1700061540.0","comment_id":"718903","poster":"Ohnet","upvote_count":"2"}],"exam_id":31,"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/87532-exam-aws-certified-solutions-architect-associate-saa-c03/","unix_timestamp":1668525540,"answer_ET":"A","answers_community":["A (100%)"],"isMC":true},{"id":"dMajD1Z61GhBdrL2s8S8","answer_images":[],"timestamp":"2022-11-16 01:52:00","answer_ET":"B","answer":"B","exam_id":31,"url":"https://www.examtopics.com/discussions/amazon/view/87572-exam-aws-certified-solutions-architect-associate-saa-c03/","unix_timestamp":1668559920,"question_id":107,"isMC":true,"answers_community":["B (100%)"],"question_text":"An entertainment company is using Amazon DynamoDB to store media metadata. The application is read intensive and experiencing delays. The company does not have staff to handle additional operational overhead and needs to improve the performance efficiency of DynamoDB without reconfiguring the application.\n\nWhat should a solutions architect recommend to meet this requirement?","choices":{"D":"Use Amazon ElastiCache for Memcached with Auto Discovery enabled.","A":"Use Amazon ElastiCache for Redis.","C":"Replicate data by using DynamoDB global tables.","B":"Use Amazon DynamoDB Accelerator (DAX)."},"topic":"1","answer_description":"","discussion":[{"upvote_count":"25","content":"Selected Answer: B\nDAX stands for DynamoDB Accelerator, and it's like a turbo boost for your DynamoDB tables. It's a fully managed, in-memory cache that speeds up the read and write performance of your DynamoDB tables, so you can get your data faster than ever before.","timestamp":"1672014840.0","poster":"techhb","comment_id":"756107"},{"timestamp":"1687770960.0","content":"Selected Answer: B\nA. Using Amazon ElastiCache for Redis would require modifying the application code and is not specifically designed to enhance DynamoDB performance.\n\nC. Replicating data with DynamoDB global tables would require additional configuration and operational overhead.\n\nD. Using Amazon ElastiCache for Memcached with Auto Discovery enabled would also require application code modifications and is not specifically designed for improving DynamoDB performance.\n\nIn contrast, option B, using Amazon DynamoDB Accelerator (DAX), is the recommended solution as it is purpose-built for enhancing DynamoDB performance without the need for application reconfiguration. DAX provides a managed caching layer that significantly reduces read latency and offloads traffic from DynamoDB tables.","poster":"cookieMr","comment_id":"934236","upvote_count":"12"},{"upvote_count":"2","poster":"1e22522","comment_id":"1260060","timestamp":"1722635700.0","content":"Selected Answer: B\nUse the turbo"},{"content":"B: https://aws.amazon.com/dynamodb/dax/\nimprove 10x performance (marketing pitch on above link) with fully managed service so no reconfiguration or operational overhead involved.","poster":"awsgeek75","upvote_count":"2","comment_id":"1110850","timestamp":"1704062100.0"},{"content":"Selected Answer: B\nAmazon DynamoDB Accelerator (DAX) is a fully managed, highly available caching service built for Amazon DynamoDB. DAX delivers up to a 10 times performance improvement—from milliseconds to microseconds—even at millions of requests per second.\n\nhttps://aws.amazon.com/dynamodb/dax/#:~:text=Amazon%20DynamoDB%20Accelerator%20(-,DAX),-is%20a%20fully","poster":"TariqKipkemei","upvote_count":"2","comment_id":"1005365","timestamp":"1694493540.0"},{"comment_id":"910898","upvote_count":"2","timestamp":"1685513460.0","content":"Selected Answer: B\nimprove the performance efficiency of DynamoDB","poster":"Abrar2022"},{"content":"Selected Answer: B\nAmazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that helps improve the read performance of DynamoDB tables. DAX provides a caching layer between the application and DynamoDB, reducing the number of read requests made directly to DynamoDB. This can significantly reduce read latencies and improve overall application performance.","poster":"gx2222","comment_id":"862473","timestamp":"1680728880.0","upvote_count":"3"},{"comment_id":"850921","timestamp":"1679828820.0","poster":"osmk","upvote_count":"2","content":"B-->Applications that are read-intensive===>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.html#DAX.use-cases"},{"comment_id":"778437","poster":"LuckyAro","content":"Selected Answer: B\nDynamoDB Accelerator, less over head.","timestamp":"1673917980.0","upvote_count":"3"},{"timestamp":"1673863440.0","poster":"wmp7039","upvote_count":"3","content":"Option B is incorrect as the constraint in the question is not to recode the application. DAX requires application to be reconfigured and point to DAX instead of DynamoDB\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.client.modify-your-app.html\nAnswer should be A","comment_id":"777526","comments":[{"comment_id":"1124199","poster":"LoXoL","timestamp":"1705408860.0","content":"DAX does not require application logic modification (compatible with existing DynamoDB APIs). ElastiCache would work after changes on app's code.","upvote_count":"2"}]},{"timestamp":"1671744780.0","content":"Selected Answer: B\nTo improve the performance efficiency of DynamoDB without reconfiguring the application, a solutions architect should recommend using Amazon DynamoDB Accelerator (DAX) which is Option B as the correct answer.\n\nDAX is a fully managed, in-memory cache that can be used to improve the performance of read-intensive workloads on DynamoDB. DAX stores frequently accessed data in memory, allowing the application to retrieve data from the cache rather than making a request to DynamoDB. This can significantly reduce the number of read requests made to DynamoDB, improving the performance and reducing the latency of the application.","poster":"Buruguduystunstugudunstuy","upvote_count":"4","comments":[{"upvote_count":"2","comment_id":"753654","poster":"Buruguduystunstugudunstuy","content":"Option A, using Amazon ElastiCache for Redis, would not be a good fit because it is not specifically designed for use with DynamoDB and would require reconfiguring the application to use it. \n\nOption C, replicating data using DynamoDB global tables, would not directly improve the performance of reading requests and would require additional operational overhead to maintain the replication. \n\nOption D, using Amazon ElastiCache for Memcached with Auto Discovery enabled, would also not be a good fit because it is not specifically designed for use with DynamoDB and would require reconfiguring the application to use it.","timestamp":"1671744780.0"}],"comment_id":"753653"},{"upvote_count":"3","comment_id":"748344","content":"Selected Answer: B\nOption B","poster":"career360guru","timestamp":"1671305820.0"},{"content":"Selected Answer: B\nAgreed","poster":"k1kavi1","comment_id":"746082","upvote_count":"3","timestamp":"1671108780.0"},{"comment_id":"741820","content":"B\nDAX is a fully managed, highly available, in-memory cache for DynamoDB that delivers lightning-fast performance and consistent low-latency responses. It provides fast performance without requiring any application reconfiguration","upvote_count":"4","timestamp":"1670771520.0","poster":"Shasha1"},{"poster":"Wpcorgan","upvote_count":"2","timestamp":"1669135500.0","content":"B is correct","comment_id":"724491"},{"timestamp":"1668565020.0","poster":"goatbernard","comment_id":"719277","content":"Selected Answer: B\nDAX is the cache for this","upvote_count":"2"},{"poster":"nhlegend","content":"B is correct, DAX provides caching + no changes","timestamp":"1668559920.0","upvote_count":"3","comment_id":"719220"}],"question_images":[]},{"id":"wSQvumPBbjdPjiJZhWcP","answer_images":[],"timestamp":"2022-11-16 12:29:00","answer_ET":"A","answer":"A","exam_id":31,"url":"https://www.examtopics.com/discussions/amazon/view/87639-exam-aws-certified-solutions-architect-associate-saa-c03/","unix_timestamp":1668598140,"isMC":true,"question_id":108,"choices":{"A":"Use AWS Backup to copy EC2 backups and RDS backups to the separate Region.","D":"Create Amazon Elastic Block Store (Amazon EBS) snapshots. Copy the EBS snapshots to the separate Region. Create RDS snapshots. Export the RDS snapshots to Amazon S3. Configure S3 Cross-Region Replication (CRR) to the separate Region.","B":"Use Amazon Data Lifecycle Manager (Amazon DLM) to copy EC2 backups and RDS backups to the separate Region.","C":"Create Amazon Machine Images (AMIs) of the EC2 instances. Copy the AMIs to the separate Region. Create a read replica for the RDS DB instance in the separate Region."},"question_text":"A company’s infrastructure consists of Amazon EC2 instances and an Amazon RDS DB instance in a single AWS Region. The company wants to back up its data in a separate Region.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","answers_community":["A (98%)","2%"],"topic":"1","answer_description":"","discussion":[{"content":"Selected Answer: A\nUsing AWS Backup to copy EC2 and RDS backups to the separate Region is the solution that meets the requirements with the least operational overhead. AWS Backup simplifies the backup process and automates the copying of backups to another Region, reducing the manual effort and operational complexity involved in managing separate backup processes for EC2 instances and RDS databases.\n\nOption B is incorrect because Amazon Data Lifecycle Manager (Amazon DLM) is not designed for directly copying RDS backups to a separate region.\n\nOption C is incorrect because creating Amazon Machine Images (AMIs) and read replicas adds complexity and operational overhead compared to a dedicated backup solution.\n\nOption D is incorrect because using Amazon EBS snapshots, RDS snapshots, and S3 Cross-Region Replication (CRR) involves multiple manual steps and additional configuration, increasing complexity.","upvote_count":"12","timestamp":"1687771620.0","comment_id":"934243","poster":"cookieMr"},{"upvote_count":"5","timestamp":"1672684140.0","comment_id":"763906","content":"Selected Answer: A\nCross-Region backup\nUsing AWS Backup, you can copy backups to multiple different AWS Regions on demand or automatically as part of a scheduled backup plan. Cross-Region backup is particularly valuable if you have business continuity or compliance requirements to store backups a minimum distance away from your production data.\nhttps://docs.aws.amazon.com/aws-backup/latest/devguide/whatisbackup.html","poster":"vtbk"},{"poster":"satyaammm","upvote_count":"1","content":"Selected Answer: A\nAWS Backups are the most suitable here.","timestamp":"1739035800.0","comment_id":"1353547"},{"upvote_count":"2","comment_id":"1236859","timestamp":"1719313200.0","content":"Selected Answer: A\nA for sure","poster":"ChymKuBoy"},{"poster":"thewalker","content":"Selected Answer: A\nThe easiest way to backup an EC2 instance and RDS Database would be to use AWS Backup. With AWS Backup you can:\n\nCreate a backup plan and select both the EC2 volume and RDS database for backup.\n\nChoose a backup schedule that meets your requirements, such as daily or weekly backups.\n\nAWS Backup will automatically take snapshots of the EC2 volume and backups of the RDS database as per the configured schedule.\n\nThe backups will be stored in S3 for long term retention based on your backup plan configuration.","upvote_count":"4","comment_id":"1135769","timestamp":"1706617920.0","comments":[{"comment_id":"1135771","timestamp":"1706617920.0","poster":"thewalker","content":"You can easily restore the EC2 volume or RDS database from these backups in case of data loss or corruption.\n\nAWS manages the entire backup process so there is no operational overhead for you.\n\nSome other options include using cron jobs to trigger snapshots and backups. But AWS Backup provides a fully managed service to centrally backup both EC2 and RDS with minimal effort. \n\nThe above is the output from Amazon Q.","upvote_count":"3"}]},{"poster":"Guru4Cloud","timestamp":"1692290400.0","content":"Selected Answer: A\nAWS Backup provides a fully managed, centralized backup service across AWS services. It can be configured to automatically copy backups across Regions.\n\nThis requires minimal operational overhead compared to the other options:","upvote_count":"3","comment_id":"983825"},{"timestamp":"1691004120.0","poster":"oguzbeliren","comment_id":"970497","content":"D would have been a great option but the questions requires less mannual effort. So, A is better.","upvote_count":"2"},{"upvote_count":"3","content":"Selected Answer: A\nA is correct","poster":"cheese929","comment_id":"890043","timestamp":"1683291420.0"},{"upvote_count":"4","comment_id":"883000","comments":[{"timestamp":"1682625060.0","upvote_count":"4","poster":"kruasan","comment_id":"883001","content":"Option D, creating EBS snapshots and RDS snapshots, exporting them to Amazon S3, and configuring S3 Cross-Region Replication (CRR) to the separate Region, would require more configuration and management effort. Additionally, S3 CRR can have additional charges for data transfer and storage in the destination region.\n\nTherefore, option A is the best choice for meeting the company's requirements with the least operational overhead."}],"poster":"kruasan","content":"Selected Answer: A\nOption B, using Amazon Data Lifecycle Manager (Amazon DLM) to copy EC2 backups and RDS backups to the separate Region, would require more operational overhead because DLM is primarily designed for managing the lifecycle of Amazon EBS snapshots, and would require additional configuration to manage RDS backups.\n\nOption C, creating AMIs of the EC2 instances and read replicas of the RDS DB instance in the separate Region, would require more manual effort to manage the backup and disaster recovery process, as it requires manual creation and management of AMIs and read replicas.","timestamp":"1682625060.0"},{"poster":"gx2222","content":"Selected Answer: A\nOption A, using AWS Backup to copy EC2 backups and RDS backups to the separate region, is the correct answer for the given scenario.\n\nUsing AWS Backup is a simple and efficient way to backup EC2 instances and RDS databases to a separate region. It requires minimal operational overhead and can be easily managed through the AWS Backup console or API. AWS Backup can also provide automated scheduling and retention management for backups, which can help ensure that backups are always available and up to date.","comment_id":"862481","timestamp":"1680729540.0","upvote_count":"4"},{"upvote_count":"2","content":"A is correct - you need to find a backup solution for EC2 and RDS. DLM doent work with RDS , only with snapshots.","timestamp":"1672608600.0","comment_id":"763347","poster":"dan80"},{"poster":"techhb","timestamp":"1672015320.0","content":"Selected Answer: A\nusing Amazon DLM to copy EC2 backups and RDS backups to the separate region, is not a valid solution because Amazon DLM does not support backing up data across regions.","upvote_count":"2","comment_id":"756109"},{"timestamp":"1671745620.0","comment_id":"753663","poster":"Buruguduystunstugudunstuy","comments":[{"poster":"HayLLlHuK","upvote_count":"3","content":"Buruguduystunstugudunstuy, sorry, but I haven’t found any info about copying RDS backups by DLM. The DLM works only with EBS.\nSo the only answer is A - AWS Backup","timestamp":"1672857420.0","comment_id":"765992"},{"upvote_count":"1","poster":"Buruguduystunstugudunstuy","comment_id":"753666","timestamp":"1671745680.0","content":"Option A, using AWS Backup to copy EC2 backups and RDS backups to the separate Region, would also work, but it may require more manual configuration and management.\n\nOption C, creating AMIs of the EC2 instances and copying them to the separate Region, and creating a read replica for the RDS DB instance in the separate Region, would work, but it may require more manual effort to set up and maintain.\n\nOption D, creating EBS snapshots and copying them to the separate Region, creating RDS snapshots, and exporting them to Amazon S3, and configuring S3 CRR to the separate Region, would also work, but it would involve multiple steps and may require more manual effort to set up and maintain. Overall, using Amazon DLM is likely to be the easiest and most efficient option for meeting the requirements with the least operational overhead.","comments":[{"timestamp":"1672086900.0","upvote_count":"2","poster":"PassNow1234","comment_id":"757829","comments":[{"comment_id":"789714","poster":"jwu413","timestamp":"1674831780.0","upvote_count":"5","content":"All of their answers are from ChatGPT"}],"content":"Some of your answers are very detailed. Can you back them up with a reference?"},{"upvote_count":"2","content":"I choose A, but DLM support cross regions. DLM doesn't support RDS. Cross region copy rules it's a feature of DLM (\"For each schedule, you can define the frequency, fast snapshot restore settings (snapshot lifecycle policies only), cross-Region copy rules, and tags\")\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html","comment_id":"781486","poster":"egmiranda","timestamp":"1674153840.0"},{"timestamp":"1672269300.0","upvote_count":"2","comment_id":"760416","content":"Thanks techhb","poster":"PassNow1234"},{"upvote_count":"5","content":"This guy is giving wrong answers in detail...lol","timestamp":"1673728860.0","comment_id":"775908","poster":"Kruiz29"}]},{"comment_id":"905203","content":"AWS DLM does not support RDS backups, only works with EBS storage. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html","timestamp":"1684872540.0","poster":"YogK","upvote_count":"2"}],"content":"Selected Answer: B\nOption B. Use Amazon Data Lifecycle Manager (Amazon DLM) to copy EC2 backups and RDS backups to the separate Region.\n\nAmazon DLM is a fully managed service that helps automate the creation and retention of Amazon EBS snapshots and RDS DB snapshots. It can be used to create and manage backup policies that specify when and how often snapshots should be created, as well as how long they should be retained. With Amazon DLM, you can easily and automatically create and manage backups of your EC2 instances and RDS DB instances in a separate Region, with minimal operational overhead.","upvote_count":"1"},{"timestamp":"1671306060.0","upvote_count":"2","content":"Selected Answer: A\nOption A as it is fully managed service with least operational overhead","poster":"career360guru","comment_id":"748347"},{"comment_id":"741846","upvote_count":"2","content":"A\n AWS Backup is a fully managed service that handles the process of copying backups to a separate Region automatically","poster":"Shasha1","timestamp":"1670773260.0"},{"upvote_count":"2","comment_id":"719599","poster":"babaxoxo","content":"Selected Answer: A\nAns A with least operational overhead","timestamp":"1668600660.0"},{"comment_id":"719573","upvote_count":"4","content":"AWS Backup supports Supports cross-region backups","timestamp":"1668598260.0","poster":"rjam"},{"timestamp":"1668598140.0","content":"Selected Answer: A\nOption A\nAws back up supports , EC2, RDS","poster":"rjam","comments":[],"comment_id":"719572","upvote_count":"4"}],"question_images":[]},{"id":"PzEDRRIAqqeNn7Gwwmx8","unix_timestamp":1668565320,"answers_community":["A (95%)","5%"],"answer_description":"","discussion":[{"poster":"Buruguduystunstugudunstuy","timestamp":"1687463580.0","content":"Selected Answer: A\nCORRECT Option A\n\nTo securely store a database user name and password in AWS Systems Manager Parameter Store and allow an application running on an EC2 instance to access it, the solutions architect should create an IAM role that has read access to the Parameter Store parameter and allow Decrypt access to an AWS KMS key that is used to encrypt the parameter. The solutions architect should then assign this IAM role to the EC2 instance.\n\nThis approach allows the EC2 instance to access the parameter in the Parameter Store and decrypt it using the specified KMS key while enforcing the necessary security controls to ensure that the parameter is only accessible to authorized parties.","upvote_count":"25","comment_id":"753667","comments":[{"comment_id":"753672","timestamp":"1687463760.0","content":"Option B, would not be sufficient, as IAM policies cannot be directly attached to EC2 instances.\n\nOption C, would not be a valid solution, as the Parameter Store parameter and the EC2 instance are not entities that can be related through an IAM trust relationship.\n\nOption D, would not be a valid solution, as the trust policy would not allow the EC2 instance to access the parameter in the Parameter Store or decrypt it using the specified KMS key.","upvote_count":"9","poster":"Buruguduystunstugudunstuy"}]},{"content":"Selected Answer: A\nAgree with A, IAM role is for services (EC2 for example)\nIAM policy is more for users and groups","comment_id":"719875","timestamp":"1684251900.0","poster":"sdasdawa","upvote_count":"10"},{"upvote_count":"3","poster":"lofzee","comment_id":"1220162","content":"A all day. Don't even need to read the other answers.\nYou can't attach a policy to EC2. You have to attach a role.","timestamp":"1732798740.0"},{"content":"Selected Answer: A\npolicy needs to be assigned to something so B is inaccurate\nCD are just made up things","timestamp":"1721153640.0","comment_id":"1124485","poster":"awsgeek75","upvote_count":"3"},{"timestamp":"1710226080.0","comment_id":"1005372","upvote_count":"2","poster":"TariqKipkemei","content":"Selected Answer: A\nCreate an IAM role that has read access to the Parameter Store parameter. Allow Decrypt access to an AWS Key Management Service (AWS KMS) key that is used to encrypt the parameter. Assign this IAM role to the EC2 instance"},{"timestamp":"1708195380.0","upvote_count":"2","poster":"Guru4Cloud","content":"Selected Answer: A\nCORRECT Option A","comment_id":"983828"},{"comment_id":"934248","poster":"cookieMr","timestamp":"1703590260.0","content":"Selected Answer: A\nBy creating an IAM role with read access to the Parameter Store parameter and Decrypt access to the associated AWS KMS key, the EC2 will have the necessary permissions to securely retrieve and decrypt the database user name and password from the Parameter Store. This approach ensures that the sensitive information is protected and can be accessed only by authorized entities.\n\nAnswers B, C, and D are not correct because they do not provide a secure way to store and retrieve the database user name and password from the Parameter Store. IAM policies, trust relationships, and associations with the DB instance are not the appropriate mechanisms for securely managing sensitive credentials in this scenario. Answer A is the correct choice as it involves creating an IAM role with the necessary permissions and assigning it to the EC2 instance to access the Parameter Store securely.","upvote_count":"3"},{"content":"Selected Answer: A\nA is correct","comment_id":"895748","timestamp":"1699783200.0","upvote_count":"2","poster":"cheese929"},{"poster":"kruasan","upvote_count":"2","comment_id":"883004","timestamp":"1698436440.0","content":"Selected Answer: A\nBy creating an IAM role and assigning it to the EC2 instance, the application running on the EC2 instance can access the Parameter Store parameter securely without the need for hard-coding the database user name and password in the application code.\n\nThe IAM role should have read access to the Parameter Store parameter and Decrypt access to an AWS KMS key that is used to encrypt the parameter to ensure that the parameter is protected at rest."},{"content":"There should be the Decrypt access to KMS.\n\"If you choose the SecureString parameter type when you create your parameter, Systems Manager uses AWS KMS to encrypt the parameter value.\"\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html\n\nIAM role - for EC2","timestamp":"1688489280.0","upvote_count":"2","poster":"HayLLlHuK","comment_id":"766003"},{"timestamp":"1687226700.0","content":"A -- is correct option","poster":"BENICE","comment_id":"750477","upvote_count":"2"},{"upvote_count":"2","content":"Option A.","timestamp":"1687023840.0","poster":"career360guru","comment_id":"748349"},{"upvote_count":"2","timestamp":"1686826620.0","poster":"k1kavi1","content":"Selected Answer: A\nA is correct","comment_id":"746084"},{"comment_id":"741874","poster":"Shasha1","upvote_count":"2","content":"Answer A\nCreate an IAM role that has read access to the Parameter Store parameter. Allow Decrypt access to an AWS Key Management Service (AWS KMS) key that is used to encrypt the parameter. Assign this IAM role to the EC2 instance. This solution will allow the application to securely access the database user name and password stored in the parameter store.","timestamp":"1686492480.0"},{"poster":"[Removed]","upvote_count":"1","content":"Selected Answer: B\ni think policy","comments":[{"comment_id":"1106591","poster":"pentium75","content":"IAM Policies can be attached to IAM roles, and EC2 instances can be allowed to use IAM roles. You can't attach an IAM policy to an EC2 instance.","upvote_count":"2","timestamp":"1719459660.0"},{"poster":"[Removed]","upvote_count":"1","timestamp":"1685260080.0","comment_id":"728945","content":"Access to Parameter Store is enabled by IAM policies and supports resource level permissions for access. An IAM policy that grants permissions to specific parameters or a namespace can be used to limit access to these parameters. CloudTrail logs, if enabled for the service, record any attempt to access a parameter."},{"comment_id":"728943","timestamp":"1685260020.0","poster":"[Removed]","content":"https://aws.amazon.com/blogs/compute/managing-secrets-for-amazon-ecs-applications-using-parameter-store-and-iam-roles-for-tasks/","comments":[{"comment_id":"767708","timestamp":"1688641680.0","content":"This link gives the example \"Walkthrough: Securely access Parameter Store resources with IAM roles for tasks\" - essentially A above. It doe snot show how this can be done using a policy (B) alone.","poster":"JayBee65","upvote_count":"2"}],"upvote_count":"1"},{"poster":"turalmth","comment_id":"735890","timestamp":"1685958300.0","upvote_count":"2","content":"can you attach policy to ec2 directly ?"}],"timestamp":"1685260020.0","comment_id":"728942"},{"comment_id":"724054","content":"Selected Answer: A\nA. Attach IAM role to EC2 Instance\nhttps://aws.amazon.com/blogs/security/digital-signing-asymmetric-keys-aws-kms/","upvote_count":"2","poster":"EKA_CloudGod","timestamp":"1684716600.0"},{"upvote_count":"4","poster":"babaxoxo","timestamp":"1684232760.0","content":"Selected Answer: A\nAttach IAM role to EC2 Instance profile","comment_id":"719608"},{"content":"Selected Answer: B\nIAM policy","comment_id":"719280","timestamp":"1684196520.0","upvote_count":"2","poster":"goatbernard"}],"answer_ET":"A","question_text":"A solutions architect needs to securely store a database user name and password that an application uses to access an Amazon RDS DB instance. The application that accesses the database runs on an Amazon EC2 instance. The solutions architect wants to create a secure parameter in AWS Systems Manager Parameter Store.\n\nWhat should the solutions architect do to meet this requirement?","answer_images":[],"timestamp":"2022-11-16 03:22:00","url":"https://www.examtopics.com/discussions/amazon/view/87582-exam-aws-certified-solutions-architect-associate-saa-c03/","answer":"A","topic":"1","isMC":true,"question_images":[],"question_id":109,"exam_id":31,"choices":{"D":"Create an IAM trust relationship between the DB instance and the EC2 instance. Specify Systems Manager as a principal in the trust policy.","C":"Create an IAM trust relationship between the Parameter Store parameter and the EC2 instance. Specify Amazon RDS as a principal in the trust policy.","A":"Create an IAM role that has read access to the Parameter Store parameter. Allow Decrypt access to an AWS Key Management Service (AWS KMS) key that is used to encrypt the parameter. Assign this IAM role to the EC2 instance.","B":"Create an IAM policy that allows read access to the Parameter Store parameter. Allow Decrypt access to an AWS Key Management Service (AWS KMS) key that is used to encrypt the parameter. Assign this IAM policy to the EC2 instance."}},{"id":"BRTspUsUk5ivd1L37yX5","answer_images":[],"timestamp":"2022-10-10 16:34:00","answer_ET":"AB","exam_id":31,"answer":"AB","url":"https://www.examtopics.com/discussions/amazon/view/85033-exam-aws-certified-solutions-architect-associate-saa-c03/","unix_timestamp":1665412440,"isMC":true,"question_id":110,"answers_community":["AB (99%)","1%"],"choices":{"B":"Configure the Lambda function to use the Amazon Simple Queue Service (Amazon SQS) queue as the invocation source. When the SQS message is successfully processed, delete the message in the queue.","C":"Configure the Lambda function to monitor the S3 bucket for new uploads. When an uploaded image is detected, write the file name to a text file in memory and use the text file to keep track of the images that were processed.","D":"Launch an Amazon EC2 instance to monitor an Amazon Simple Queue Service (Amazon SQS) queue. When items are added to the queue, log the file name in a text file on the EC2 instance and invoke the Lambda function.","E":"Configure an Amazon EventBridge (Amazon CloudWatch Events) event to monitor the S3 bucket. When an image is uploaded, send an alert to an Amazon ample Notification Service (Amazon SNS) topic with the application owner's email address for further processing.","A":"Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure the S3 bucket to send a notification to the SQS queue when an image is uploaded to the S3 bucket."},"question_text":"An application development team is designing a microservice that will convert large images to smaller, compressed images. When a user uploads an image through the web interface, the microservice should store the image in an Amazon S3 bucket, process and compress the image with an AWS Lambda function, and store the image in its compressed form in a different S3 bucket.\nA solutions architect needs to design a solution that uses durable, stateless components to process the images automatically.\nWhich combination of actions will meet these requirements? (Choose two.)","topic":"1","answer_description":"","question_images":[],"discussion":[{"upvote_count":"40","comment_id":"759021","poster":"Buruguduystunstugudunstuy","content":"Selected Answer: AB\nTo design a solution that uses durable, stateless components to process images automatically, a solutions architect could consider the following actions:\n\nOption A involves creating an SQS queue and configuring the S3 bucket to send a notification to the queue when an image is uploaded. This allows the application to decouple the image upload process from the image processing process and ensures that the image processing process is triggered automatically when a new image is uploaded. \n\nOption B involves configuring the Lambda function to use the SQS queue as the invocation source. When the SQS message is successfully processed, the message is deleted from the queue. This ensures that the Lambda function is invoked only once per image and that the image is not processed multiple times.","timestamp":"1672172400.0","comments":[{"poster":"Buruguduystunstugudunstuy","comment_id":"759022","comments":[{"comment_id":"965185","upvote_count":"2","content":"So storing states invokes the stateless principle, nice understanding!","poster":"hsinchang","timestamp":"1690503360.0","comments":[{"poster":"op22233","content":"A stateless system sends a request to the server and relays the response (or the state) back without storing any information. On the other hand, stateful systems expect a response, track information, and resend the request if no response is received","timestamp":"1696822200.0","upvote_count":"3","comment_id":"1028457"}]}],"timestamp":"1672172400.0","content":"Option C is incorrect because it involves storing state (the file name) in memory, which is not a durable or scalable solution.\n\nOption D is incorrect because it involves launching an EC2 instance to monitor the SQS queue, which is not a stateless solution.\n\nOption E is incorrect because it involves using Amazon EventBridge (formerly Amazon CloudWatch Events) to send an alert to an Amazon Simple Notification Service (Amazon SNS) topic, which is not related to the image processing process.","upvote_count":"24"}]},{"poster":"sba21","upvote_count":"15","timestamp":"1665490200.0","content":"Selected Answer: AB\nIt looks like A-B","comment_id":"692072"},{"poster":"EzKkk","timestamp":"1731571260.0","comment_id":"1311788","content":"Selected Answer: AB\nHigh level flow: Web (1) -> Internet (2) -> Uploaded image bucket (3) -> Compressed image bucket (4)\nLet's break this down.\nImage uploading (1)(2)(3): If the image's size is big, multi upload is a MUST. If you want to accelerate the process, you can also use Acceleration Transfer with additional fee.\nImage compressing (3)(4): SQS Simple Queue can directly integrate with S3 events. For every image uploaded, S3 event will create a message in SQS queue. After that, Lambda can be invoked to compress the image and then upload the image to destination bucket.","upvote_count":"2"},{"upvote_count":"9","content":"Selected Answer: AB\nKeywords:\n- Store the image in an Amazon S3 bucket, process and compress the image with an AWS Lambda function.\n- Durable, stateless components to process the images automatically\n\nA,B: Correct - SQS has message retention function(store message) default 4 days(can increate update 14 days) so that you can re-run lambda if there are any errors when processing the images.\nC: Incorrect - Lambda function just run the request then stop, the max tmeout is 15 mins. So we cannot store data in the ram of Lambda function.\nD: Incorrect - we can trigger Lambda dirrectly from SQS no need EC2 instance in this case\nE: Incorrect - It kinds of manually step -> the owner has to read email then process it :))","comment_id":"863442","poster":"PhucVuu","timestamp":"1726905180.0"},{"upvote_count":"3","timestamp":"1726905120.0","comment_id":"957717","poster":"Guru4Cloud","content":"Selected Answer: AB\nExplanation:\n\n Option A: By creating an Amazon SQS queue and configuring the S3 bucket to send a notification to the SQS queue when an image is uploaded, the system establishes a durable and scalable way to handle incoming image processing tasks.\n\n Option B: Configuring the Lambda function to use the SQS queue as the invocation source allows it to retrieve messages from the queue and process them in a stateless manner. After successfully processing the image, the Lambda function can delete the message from the queue to avoid duplicate processing."},{"upvote_count":"3","timestamp":"1726905120.0","poster":"DigitalDanny","comment_id":"1091823","content":"Selected Answer: AB\nA. Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure the S3 bucket to send a notification to the SQS queue when an image is uploaded to the S3 bucket.\n\nB. Configure the Lambda function to use the Amazon Simple Queue Service (Amazon SQS) queue as the invocation source. When the SQS message is successfully processed, delete the message in the queue.\n\nExplanation:\n\nA (SQS Queue): Using SQS to decouple the S3 bucket from the processing components provides durability and scalability. When an image is uploaded, a notification is sent to the SQS queue.\nB (Lambda with SQS Trigger): Configuring the Lambda function to use the SQS queue as the invocation source allows for stateless and scalable image processing. Lambda can be triggered by messages in the SQS queue, and upon successful processing, the message can be deleted, ensuring that each message (image) is processed once.\nThis combination ensures a durable, stateless, and scalable architecture for processing images automatically in response to user uploads."},{"content":"Selected Answer: AB\nOption A is a correct because it allows for decoupling between the image upload process and image processing. By configuring S3 to send a notification to SQS, image upload event is recorded and can be processed independently by microservice.\n\nOption B is also a correct because it ensures that Lambda is triggered by messages in SQS. Lambda can retrieve image information from SQS, process and compress image, and store compressed image in a different S3. Once processing is successful, Lambda can delete processed message from SQS, indicating that image has been processed.\n\nOption C is not recommended because it introduces a stateful approach by using a text file to keep track of processed images.\n\nOption D is not optimal solution as it introduces unnecessary complexity by involving an EC2 to monitor SQS and maintain a text file.\n\nOption E is not directly related to requirement of processing images automatically. Although EventBridge and SNS can be useful for event notifications and further processing, they don't provide the same level of durability and scalability as SQS.","upvote_count":"6","timestamp":"1726905120.0","poster":"cookieMr","comment_id":"926546"},{"poster":"PaulGa","timestamp":"1723380180.0","content":"Selected Answer: AB\nAns A,B - as per Buruguduystunstugudunstuy's response: stateless, robust","upvote_count":"1","comment_id":"1264145"},{"timestamp":"1712442660.0","content":"Selected Answer: AB\nwhenever we talk about microservices we should mention SQS, so A and B are the right answers","upvote_count":"4","poster":"soufiyane","comment_id":"1190644"},{"comment_id":"1153693","content":"Selected Answer: AB\nA/B make the most sense and in practice this works, I've done it.","timestamp":"1708313940.0","upvote_count":"3","poster":"han_ds"},{"upvote_count":"1","content":"Selected Answer: AB\nAnswer- A,B","timestamp":"1705149300.0","comment_id":"1121627","poster":"A_jaa"},{"content":"I can not understand why it is not as simple like s3-1 event destination to notify the lambda function to process and upload to s3-2","upvote_count":"2","comment_id":"1114801","timestamp":"1704489720.0","poster":"mohamedsambo"},{"timestamp":"1689180180.0","upvote_count":"1","comment_id":"950016","content":"Option AB MET THE REQUIREMENT","poster":"miki111"},{"upvote_count":"2","comment_id":"948803","content":"Selected Answer: AB\nD and E are distractions. C seems a valid solution. However, as you have to select two, A and B are the only two that work in conjunction with each other.","poster":"RupeC","timestamp":"1689062640.0"},{"poster":"tester0071","comment_id":"948374","upvote_count":"1","timestamp":"1689020580.0","content":"Selected Answer: AB\nA and B are optimal solutions"},{"comment_id":"899819","poster":"beginnercloud","upvote_count":"1","timestamp":"1684309320.0","content":"Selected Answer: AB\nOption A nad B"},{"timestamp":"1684049460.0","poster":"cheese929","comment_id":"897358","content":"Selected Answer: AB\nA and B","upvote_count":"1"},{"comment_id":"856355","poster":"linux_admin","upvote_count":"2","timestamp":"1680214080.0","content":"Selected Answer: AB\nA. Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure the S3 bucket to send a notification to the SQS queue when an image is uploaded to the S3 bucket.\nB. Configure the Lambda function to use the Amazon Simple Queue Service (Amazon SQS) queue as the invocation source. When the SQS message is successfully processed, delete the message in the queue."},{"upvote_count":"1","timestamp":"1678523220.0","content":"Selected Answer: AB\nAgree with the general answer. its A+B.","poster":"cheese929","comment_id":"835738"},{"poster":"Nikhilcy","upvote_count":"1","timestamp":"1677782040.0","content":"Why B?\nMessage gets automatically deleted from queue once it goes out of it. FIFO","comments":[{"comment_id":"833703","content":"Not deleted but hidden while being processed","upvote_count":"1","timestamp":"1678349640.0","poster":"camelstrike"}],"comment_id":"827240"},{"content":"Selected Answer: AB\nAB definitely Okay","poster":"bilel500","comment_id":"823365","upvote_count":"1","timestamp":"1677484800.0"},{"comment_id":"807155","content":"Selected Answer: AB\nAB definitely Okay","poster":"buiducvu","timestamp":"1676274840.0","upvote_count":"1"},{"timestamp":"1673027880.0","upvote_count":"1","poster":"SilentMilli","comment_id":"767943","content":"Selected Answer: AB\nAB definitely Okay"},{"timestamp":"1672257120.0","comment_id":"760276","upvote_count":"1","content":"Selected Answer: AB\nObviously A & B","poster":"LuckyAro"},{"upvote_count":"1","comment_id":"750545","content":"1)SQS + Lambda 2) SQS FIFO + Lambda 3 ) SNS + Lambda","timestamp":"1671515640.0","poster":"duriselvan"},{"poster":"Vickysss","content":"Selected Answer: AB\nA and B looks reasonable","upvote_count":"1","timestamp":"1670882520.0","comment_id":"743366"},{"content":"ok, A and B are the \"correct\" options given the set that we were provided, but you can simply configure a trigger in the S3 to invoke the lambda that will process and upload the image... As an architect I would never go the way the solution is presented in this scenario.","timestamp":"1670679540.0","comment_id":"741010","upvote_count":"2","poster":"wh1t4k3r"},{"comment_id":"731187","upvote_count":"1","timestamp":"1669792320.0","poster":"miki111","content":"AAAAAAAAAABBBBBBBBBB"},{"content":"Selected Answer: AB\nA and B are most correct","timestamp":"1669320600.0","comment_id":"726158","poster":"sherbo","upvote_count":"1"},{"upvote_count":"1","comment_id":"723488","poster":"Wpcorgan","content":"A and B","timestamp":"1669034700.0"},{"timestamp":"1668387480.0","upvote_count":"2","content":"How about \"E\". Amazon EventBridge can monitor S3 bucket and send an alert to an SNS.","comments":[{"timestamp":"1669450680.0","content":"it required the owner's app process image which is not realistic in usage. It's like automation all process and manual the last steps using human effort.","upvote_count":"3","comment_id":"727388","poster":"TuLe"}],"poster":"crystally77","comment_id":"717596"},{"content":"Selected Answer: AB\nAB is OK. It can be done more straightforwardly. Just connect the S3 event to Lambda, and it is done. I don't think we need SQS or anything.","upvote_count":"9","comments":[{"poster":"hahahumble","comment_id":"771951","timestamp":"1673401680.0","upvote_count":"3","content":"Use SQS can make it more durable."}],"poster":"iis","comment_id":"709943","timestamp":"1667408940.0"},{"poster":"GameDad09","content":"Selected Answer: AB\nA+B seems to be correct","timestamp":"1666036080.0","comment_id":"697659","upvote_count":"3"},{"comment_id":"691252","upvote_count":"3","timestamp":"1665412500.0","content":"Selected Answer: AB\noops i ment A + B also","poster":"ogerber"},{"comment_id":"691251","content":"Selected Answer: AC\nwhy not?","poster":"ogerber","timestamp":"1665412440.0","comments":[{"upvote_count":"2","comments":[{"timestamp":"1671500640.0","upvote_count":"2","comment_id":"750386","poster":"Buruguduystunstugudunstuy","content":"Option B is incorrect because it involves using the SQS queue as the invocation source for the Lambda function, but the requirement specifies that the Lambda function should be invoked when an image is uploaded to the S3 bucket.\n\nOption D is incorrect because it involves launching an Amazon Elastic Compute Cloud (EC2) instance to monitor the SQS queue, but the requirement specifies that the microservice should use stateless and durable components. An EC2 instance is not a stateless component.\n\nOption E is incorrect because it involves using Amazon EventBridge (formerly known as Amazon CloudWatch Events) and Amazon Simple Notification Service (SNS) to send an alert to the application owner's email address when an image is uploaded, but this does not involve automatically processing the images using a stateless and durable component."}],"content":"To meet these requirements, the solutions architect should choose options A and C.\n\nOption A involves creating an Amazon Simple Queue Service (SQS) queue and configuring the S3 bucket to send a notification to the queue when an image is uploaded. This will allow the microservice to automatically process the images using a stateless and durable component, the SQS queue.\n\nOption C involves configuring the Lambda function to monitor the S3 bucket for new uploads. When an uploaded image is detected, the Lambda function can process and compress the image, storing the compressed image in a different S3 bucket. This will allow the microservice to automatically process the images using a stateless and durable component, the Lambda function.","timestamp":"1726905180.0","comment_id":"750385","poster":"Buruguduystunstugudunstuy"},{"poster":"GameDad09","content":"Lambda to monitor S3 put event is incorrect.","upvote_count":"2","timestamp":"1666036200.0","comment_id":"697661"},{"poster":"ocbn3wby","comment_id":"720577","upvote_count":"2","content":"Storing in-memory doesn't seem reliable for a possible huge workload. While SQS can handle bigger workloads.","timestamp":"1668698220.0"},{"poster":"123jhl0","content":"If you go for A-C... what's the purpose of SQS in A?","comment_id":"696236","upvote_count":"7","timestamp":"1665923820.0"}],"upvote_count":"1"}]}],"exam":{"isBeta":false,"provider":"Amazon","id":31,"name":"AWS Certified Solutions Architect - Associate SAA-C03","isImplemented":true,"isMCOnly":true,"lastUpdated":"11 Apr 2025","numberOfQuestions":1019},"currentPage":22},"__N_SSP":true}