{"pageProps":{"questions":[{"id":"EQqysaKdibBXU6oLnTe5","exam_id":32,"answer_images":[],"question_text":"A startup company recently migrated a large ecommerce website to AWS. The website has experienced a 70% increase in sales. Software engineers are using a private GitHub repository to manage code. The DevOps team is using Jenkins for builds and unit testing. The engineers need to receive notifications for bad builds and zero downtime during deployments. The engineers also need to ensure any changes to production are seamless for users and can be rolled back in the event of a major issue.\nThe software engineers have decided to use AWS CodePipeline to manage their build and deployment process.\nWhich solution will meet these requirements?","answers_community":["B (78%)","C (22%)"],"answer_ET":"B","timestamp":"2022-04-23 22:56:00","question_images":[],"answer_description":"","discussion":[{"timestamp":"1654332900.0","comment_id":"611367","poster":"Chuky64","upvote_count":"12","content":"B for sure.\n\nUse GitHub webhooks to trigger the CodePipeline pipeline. Use the Jenkins plugin for AWS CodeBuild to conduct unit testing. Send alerts to an Amazon SNS topic for any bad builds. Deploy in a blue/green deployment using AWS CodeDeploy."},{"poster":"Darkhorse_79","upvote_count":"4","comment_id":"913040","timestamp":"1685737680.0","content":"Selected Answer: B\nB is the answer, using Webhooks and blue/green\nA&C says Websockets, which are part of APIGateway\nD Says use Xray for unit test, which is also wrong"},{"upvote_count":"2","timestamp":"1684874880.0","poster":"Jesuisleon","comment_id":"905220","content":"Selected Answer: B\nB is the answer;\nC is wrong, \"AWS X-Ray for unit testing and static code analysis\" is wrong. X-ray is used to analyze and debug distributed applications and microservices architectures."},{"comment_id":"864933","timestamp":"1680978540.0","content":"Selected Answer: C\nBeg to differ. If CodePipeline is to be used in place of a jenkins pipeline it cant use a jenkins plugin - unless one wants to maintain two pipelines in jenkins and in CodePipeline.\nSo I guess when they talk about \"unit test\" its not about junit or alike itself that are used in a stage of the pipeline but examine if a step produces expected results. Xray might very well serve got this purpose. So C might very well be the solution.","upvote_count":"2","poster":"hobokabobo"},{"poster":"sindra","timestamp":"1666998840.0","content":"Selected Answer: B\nB for sure... blue green is the right one. \nrule out C and D since AWS X-Ray is for troubleshooting not for build purpose","upvote_count":"1","comment_id":"706802"},{"comment_id":"590799","timestamp":"1650747360.0","upvote_count":"4","content":"It's B. Blue/Green deployment for zero downtime.","poster":"Bigbearcn"}],"isMC":true,"unix_timestamp":1650747360,"topic":"1","answer":"B","url":"https://www.examtopics.com/discussions/amazon/view/74272-exam-aws-certified-solutions-architect-professional-topic-1/","choices":{"A":"Use GitHub websockets to trigger the CodePipeline pipeline. Use the Jenkins plugin for AWS CodeBuild to conduct unit testing. Send alerts to an Amazon SNS topic for any bad builds. Deploy in an in-place, all-at-once deployment configuration using AWS CodeDeploy.","C":"Use GitHub websockets to trigger the CodePipeline pipeline. Use AWS X-Ray for unit testing and static code analysis. Send alerts to an Amazon SNS topic for any bad builds. Deploy in a blue/green deployment using AWS CodeDeploy.","D":"Use GitHub webhooks to trigger the CodePipeline pipeline. Use AWS X-Ray for unit testing and static code analysis. Send alerts to an Amazon SNS topic for any bad builds. Deploy in an in-place, all-at-once deployment configuration using AWS CodeDeploy.","B":"Use GitHub webhooks to trigger the CodePipeline pipeline. Use the Jenkins plugin for AWS CodeBuild to conduct unit testing. Send alerts to an Amazon SNS topic for any bad builds. Deploy in a blue/green deployment using AWS CodeDeploy."},"question_id":861},{"id":"xESl8t5QAJ2t51N8KLiQ","unix_timestamp":1650624660,"topic":"1","exam_id":32,"question_text":"A solutions architect needs to deploy an application on a fleet of Amazon EC2 Instances. The EC2 instances run in private subnets in an Auto Scaling group. The application is expected to generate logs at a rate of 100 MB each second on each of the EC2 instances.\nThe logs must be stored in an Amazon S3 bucket so that an Amazon EMR cluster can consume them for further processing. The logs must be quickly accessible for the first 90 days and should be retrievable within 48 hours thereafter.\nWhat is the MOST cost-effective solution that meets these requirements?","answer":"B","url":"https://www.examtopics.com/discussions/amazon/view/74113-exam-aws-certified-solutions-architect-professional-topic-1/","choices":{"B":"Set up an S3 sync job to copy logs from each EC2 instance to the S3 bucket with S3 Standard storage. Use a gateway VPC endpoint for Amazon S3 to connect to Amazon S3. Create S3 Lifecycle policies to move logs that are older than 90 days to S3 Glacier Deep Archive.","A":"Set up an S3 copy job to write logs from each EC2 instance to the S3 bucket with S3 Standard storage. Use a NAT instance within the private subnets to connect to Amazon S3. Create S3 Lifecycle policies to move logs that are older than 90 days to S3 Glacier.","C":"Set up an S3 batch operation to copy logs from each EC2 instance to the S3 bucket with S3 Standard storage. Use a NAT gateway with the private subnets to connect to Amazon S3. Create S3 Lifecycle policies to move logs that are older than 90 days to S3 Glacier Deep Archive.","D":"Set up an S3 sync job to copy logs from each EC2 instance to the S3 bucket with S3 Standard storage. Use a gateway VPC endpoint for Amazon S3 to connect to Amazon S3. Create S3 Lifecycle policies to move logs that are older than 90 days to S3 Glacier."},"discussion":[{"timestamp":"1650736680.0","upvote_count":"11","poster":"Bigbearcn","content":"Selected Answer: B\nIt should be B. S3 Glacier Deep Archive bulk retrieval time is max 48 hours.","comment_id":"590741"},{"upvote_count":"1","timestamp":"1694763000.0","content":"Selected Answer: B\nWhy is the suggested answer A ? Where does it come? No one did vote for A","poster":"rodrod","comment_id":"1008236"},{"poster":"Santo99","upvote_count":"1","timestamp":"1668278520.0","content":"Selected Answer: B\nB\nS3 sync is recommended for copying from Ec2 https://aws.amazon.com/de/premiumsupport/knowledge-center/s3-transfer-data-bucket-instance/","comment_id":"716863"},{"comments":[{"timestamp":"1702957740.0","comment_id":"1100257","upvote_count":"1","content":"s3 sync works for EC2 to s3.\n\nhttps://www.youtube.com/watch?v=xjur5NU6xh0","poster":"sumaju"}],"poster":"Pigi_102","upvote_count":"1","comment_id":"710468","timestamp":"1667477040.0","content":"Selected Answer: A\nS3 \"sync\" operation only works from s3 to s3. S3 \"batch\" seems to works only from S3 to S3 so I think A is the only option available, also if is using NAT instead of gateway."},{"poster":"sindra","comment_id":"706805","content":"Selected Answer: B\nB is the complete set for the answer","timestamp":"1666998960.0","upvote_count":"1"},{"content":"definitely BBBB this came up","comment_id":"673692","upvote_count":"1","timestamp":"1663628940.0","poster":"JoMainAWS"},{"upvote_count":"1","comment_id":"623727","poster":"TechX","content":"Selected Answer: B\nNo doubt, it's B","timestamp":"1656392040.0"},{"upvote_count":"2","content":"B is correct","comment_id":"610822","poster":"Murtazaarif","timestamp":"1654213560.0"},{"poster":"user89","upvote_count":"3","timestamp":"1653662940.0","comment_id":"608125","content":"B. it satisfies both requirement\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html\nhttps://aws.amazon.com/s3/storage-classes/glacier/"},{"poster":"mirnuj_atom","timestamp":"1650624720.0","upvote_count":"1","comment_id":"589912","content":"Selected Answer: B\nWhoops, right answer is B )"},{"comment_id":"589911","poster":"mirnuj_atom","timestamp":"1650624660.0","upvote_count":"1","content":"Selected Answer: C\nThe right answer is C"}],"question_id":862,"timestamp":"2022-04-22 12:51:00","answers_community":["B (89%)","6%"],"question_images":[],"answer_ET":"B","answer_images":[],"answer_description":"","isMC":true},{"id":"003EL8GGPYoNP3h1wSeW","isMC":true,"answer":"B","question_text":"An online retail company hosts its stateful web-based application and MySQL database in an on-premises data center on a single server. The company wants to increase its customer base by conducting more marketing campaigns and promotions. In preparation, the company wants to migrate its application and database to AWS to increase the reliability of its architecture.\nWhich solution should provide the HIGHEST level of reliability?","discussion":[{"timestamp":"1654654620.0","comments":[{"timestamp":"1654654800.0","content":"Plus that Aurora have 3 AZ for HA and also stability with 15 read replica to ensure during promotion it will stay stability","poster":"Anhdd","upvote_count":"1","comment_id":"613029"}],"upvote_count":"8","comment_id":"613027","content":"Selected Answer: B\nB is right. The question not mention that need multi-thread, so Redis is better than Memcache. I will choose B over D.\nA) \"Store sessions in Amazon Neptune\" - wrong\nC) DocumentDB is NoSQL + FireHose for session = wrong","poster":"Anhdd"},{"comment_id":"1084165","poster":"HunkyBunky","timestamp":"1701336900.0","content":"Selected Answer: B\nElastiCache Redis - provides more HA than ElastiCache Memcache","upvote_count":"1"},{"poster":"mrgreatness","comment_id":"715576","content":"I'm gping B , better performance with B and it is reliability not availability question","upvote_count":"2","timestamp":"1668115740.0"},{"upvote_count":"2","comments":[],"timestamp":"1668115620.0","comment_id":"715574","poster":"mrgreatness","content":"the key word is \"reliability\" don't get it confused with \"availability\""},{"upvote_count":"2","poster":"alxjandroleiva","timestamp":"1667328900.0","comment_id":"709385","content":"Selected Answer: D\nD is more precise with multi AZ"},{"poster":"sindra","comment_id":"706806","upvote_count":"2","timestamp":"1666999140.0","content":"Selected Answer: D\nThe question ask about the highest evel of availability.. \nA is rule out because of neptune\nC is rule out because of document db \n\nits either B or D, but between of them because the question is availability.. i prefer D since it have Multi-AZ compared to the B"},{"upvote_count":"1","content":"Selected Answer: B\nFirst, keep the database using MySQL. No reason to convert it. So that leaves A or B. You would not store session state in Amazon Neptune. ElastiCache for Redis is a perfect fit for storing session state.","timestamp":"1664842380.0","poster":"sb333","comment_id":"685841"},{"content":"It's B. There is no clustering in Memcache, only sharding.","poster":"altonh","upvote_count":"4","comment_id":"626901","timestamp":"1656921120.0"},{"poster":"hilft","comment_id":"611641","upvote_count":"2","timestamp":"1654389600.0","content":"It's either B or D. \nAWS often puts Redis vs. Memcache in respect to multi-threading feature.\nSo, it's B"},{"content":"Selected Answer: D\nA) \"Store sessions in Amazon Neptune\" - wrong\nB) Even though Aurora stores data throughout 3 AZ, we still need HA on complete level.\nC) DocumentDB is NoSQL + FireHose for session = wrong\nD) Fits the bill. Sessions can be stored in Memcache https://aws.amazon.com/elasticache/memcached/","poster":"bobsmith2000","upvote_count":"3","comment_id":"606789","timestamp":"1653406620.0"},{"poster":"Bigbearcn","upvote_count":"4","content":"Selected Answer: B\nIt's B. use redis for session state.","comment_id":"590736","timestamp":"1650736140.0"}],"topic":"1","answer_description":"","answer_ET":"B","question_id":863,"question_images":[],"answers_community":["B (67%)","D (33%)"],"exam_id":32,"unix_timestamp":1650736140,"timestamp":"2022-04-23 19:49:00","url":"https://www.examtopics.com/discussions/amazon/view/74260-exam-aws-certified-solutions-architect-professional-topic-1/","choices":{"B":"Migrate the database to Amazon Aurora MySQL. Deploy the application in an Auto Scaling group on Amazon EC2 instances behind an Application Load Balancer. Store sessions in an Amazon ElastiCache for Redis replication group.","A":"Migrate the database to an Amazon RDS MySQL Multi-AZ DB instance. Deploy the application in an Auto Scaling group on Amazon EC2 instances behind an Application Load Balancer Store sessions in Amazon Neptune.","C":"Migrate the database to Amazon DocumentDB (with MongoDB compatibility). Deploy the application in an Auto Scaling group on Amazon EC2 instances behind a Network Load Balancer. Store sessions in Amazon Kinesis Data Firehose.","D":"Migrate the database to an Amazon RDS MariaDB Multi-AZ DB instance. Deploy the application in an Auto Scaling group on Amazon EC2 instances behind an Application Load Balancer. Store sessions in Amazon ElastiCache for Memcached."},"answer_images":[]},{"id":"PK9x66Y4DC5AMagcYL7E","topic":"1","exam_id":32,"question_images":[],"question_id":864,"answer":"C","answers_community":["C (100%)"],"answer_images":[],"timestamp":"2022-04-23 19:45:00","question_text":"A company has an organization that has many AWS accounts in AWS Organizations. A solutions architect must improve how the company manages common security group rules for the AWS accounts in the organization.\nThe company has a common set of IP CIDR ranges in an allow list in each AWS account to allow access to and from the company's on-premises network.\nDevelopers within each account are responsible for adding new IP CIDR ranges to their security groups. The security team has its own AWS account. Currently, the security team notifies the owners of the other AWS accounts when changes are made to the allow list.\nThe solutions architect must design a solution that distributes the common set of CIDR ranges across all accounts.\nWhich solution meets these requirements with the LEAST amount of operational overhead?","unix_timestamp":1650735900,"answer_description":"","answer_ET":"C","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/74258-exam-aws-certified-solutions-architect-professional-topic-1/","choices":{"C":"Create a new customer-managed prefix list in the security team's AWS account. Populate the customer-managed prefix list with all internal CIDR ranges. Share the customer-managed prefix list with the organization by using AWS Resource Access Manager. Notify the owner of each AWS account to allow the new customer-managed prefix list ID in their security groups.","B":"Create new customer-managed prefix lists in each AWS account within the organization. Populate the prefix lists in each account with all internal CIDR ranges. Notify the owner of each AWS account to allow the new customer-managed prefix list IDs in their accounts in their security groups. Instruct the security team to share updates with each AWS account owner.","D":"Create an IAM role in each account in the organization. Grant permissions to update security groups. Deploy an AWS Lambda function in the security team's AWS account. Configure the Lambda function to take a list of internal IP addresses as input, assume a role in each organization account, and add the list of IP addresses to the security groups in each account.","A":"Set up an Amazon Simple Notification Service (Amazon SNS) topic in the security team's AWS account. Deploy an AWS Lambda function in each AWS account. Configure the Lambda function to run every time an SNS topic receives a message. Configure the Lambda function to take an IP address as input and add it to a list of security groups in the account. Instruct the security team to distribute changes by publishing messages to its SNS topic."},"discussion":[{"comment_id":"696259","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/vpc/latest/userguide/managed-prefix-lists.html\nA managed prefix list is a set of one or more CIDR blocks. You can use prefix lists to make it easier to configure and maintain your security groups and route tables. \n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/sharing-managed-prefix-lists.html\n\nWith AWS Resource Access Manager (AWS RAM), the owner of a prefix list can share a prefix list with the following:\n\n Specific AWS accounts inside or outside of its organization in AWS Organizations\n\n An organizational unit inside its organization in AWS Organizations\n\n An entire organization in AWS Organizations","timestamp":"1665927060.0","upvote_count":"5","poster":"AwsBRFan"},{"comments":[{"poster":"nimbus_00","content":"C reduces operational overhead because changes to the CIDR ranges in the shared prefix list will automatically apply to all accounts that use the list in their security groups. There is no need to deploy Lambda functions, manage notifications, or implement complex automation in each individual account.","upvote_count":"1","comment_id":"1295070","timestamp":"1728466860.0"}],"content":"I would think the less overhead mean automation which equals to using Lambda functions?","comment_id":"768614","poster":"syaldram","upvote_count":"2","timestamp":"1673100840.0"},{"upvote_count":"2","content":"Selected Answer: C\nC is correct, RAM use case","comment_id":"626374","timestamp":"1656815820.0","poster":"aandc"},{"content":"It's C. Customer managed prefix list and RAM to share.","timestamp":"1650735900.0","upvote_count":"3","poster":"Bigbearcn","comment_id":"590734"}]},{"id":"XRsnUMlm6LOpY3CaiTmy","answer_images":[],"answer_description":"","question_images":[],"question_text":"Your system recently experienced down time during the troubleshooting process. You found that a new administrator mistakenly terminated several production\nEC2 instances.\nWhich of the following strategies will help prevent a similar situation in the future?\nThe administrator still must be able to:\n✑ launch, start stop, and terminate development resources.\n✑ launch and start production instances.","exam_id":32,"unix_timestamp":1572743460,"timestamp":"2019-11-03 02:11:00","url":"https://www.examtopics.com/discussions/amazon/view/7625-exam-aws-certified-solutions-architect-professional-topic-1/","answer":"C","choices":{"C":"Leverage EC2 termination protection and multi-factor authentication, which together require users to authenticate before terminating EC2 instances","D":"Create an IAM user and apply an IAM role which prevents users from terminating production EC2 instances.","A":"Create an IAM user, which is not allowed to terminate instances by leveraging production EC2 termination protection.","B":"Leverage resource based tagging, along with an IAM user which can prevent specific users from terminating production, EC2 resources."},"topic":"1","answer_ET":"C","discussion":[{"timestamp":"1633488600.0","content":"Answer is B\nKeyword is \"launch and start production instances\" => C does not stop him to do terminate","comments":[{"content":"\" still must be able to:\n✑ launch, start stop, and terminate development resources.\"\n\nIt is C","poster":"Ibranthovic","comments":[{"content":"Launch, start, stop and terminate **development** resources\" - notice this is development environment. \nThe production environment requirement is that they can only \"launch and start production instances\" - they should not be able to terminate, so B is correct.","upvote_count":"2","timestamp":"1663822860.0","poster":"kapara","comments":[{"timestamp":"1734531960.0","comment_id":"1328550","poster":"madmike123","upvote_count":"1","content":"If an administrator is prevented from terminating production instances 100% of the time, the implication is you'd need some shadow admin to perform the task when it's needed. In this case, 'c' makes the most sense as it prevents accidental termination which is the objective."}],"comment_id":"675740"}],"comment_id":"100511","upvote_count":"4","timestamp":"1635150000.0"},{"poster":"helpaws","comment_id":"642014","content":"it's C. Keyword here actually is \"mistake.\" This will prevent mistake only.","upvote_count":"3","timestamp":"1659545160.0"}],"comment_id":"38049","poster":"amog","upvote_count":"10"},{"content":"B\nhttp://jayendrapatil.com/tag/tags/","upvote_count":"5","comment_id":"40618","timestamp":"1635004260.0","poster":"CloudFloater"},{"timestamp":"1723803900.0","poster":"amministrazione","upvote_count":"1","content":"B. Leverage resource based tagging, along with an IAM user which can prevent specific users from terminating production, EC2 resources.","comment_id":"1266967"},{"timestamp":"1685362920.0","content":"B is right and C is wrong.\nC just complicates the stopping procedure and can NOT prevent admin from stopping them.","upvote_count":"1","comment_id":"909389","poster":"Jesuisleon"},{"upvote_count":"1","timestamp":"1672384080.0","poster":"hollie","content":"Selected Answer: B\nobviously terminate production instance should be denied.","comment_id":"761742"},{"poster":"hobokabobo","timestamp":"1672067340.0","content":"Selected Answer: C\nWell this is so poorly worded that one can only guess:\nA) \"not allowed ... by leveraging termination protection\". Termination protection is not user specific. \nB) So you have an IAM user that prevents \"specific users\": so an I am user is patrolling and as soon as one of the \"specific users\" tries to terminate an instance this hero steps in preventing the damage ... \nC) Multifactor requires to authenticate: yes, and with a second device. It does not prevent termination but at least its not completely ridiculous.\nD) It says apply an role to a user ... nonesense?\nYes C its bad as it gets. Who would do it that way? But C is only bad and not complete nonsense .","upvote_count":"1","comment_id":"757573"},{"comment_id":"755180","timestamp":"1671918240.0","content":"Selected Answer: C\nA. Wrong. EC2 termination protection prevents anyone to perform the termination unless the protection is disabled.\nB. Doable but too much work that C\nC. This is the answer. Instance termination protection is the easiest and simplest way to go. \nI use Terraform to bring up and manage AWS resources with the default setting to protect resources, not just EC2, from being terminated by mistake with a default variable file and another specific variable file for unsetting the protection during the development phase or using the command line variables (or even manually) disable the protection before performing any change after the development phase.\nD. Misunderstand IAM user, role, and policy","upvote_count":"1","poster":"TigerInTheCloud"},{"upvote_count":"2","timestamp":"1662259140.0","comment_id":"658922","poster":"welcomeYM","content":"Selected Answer: C\nCCCCCC"},{"poster":"hilft","upvote_count":"1","comment_id":"636801","timestamp":"1658764380.0","content":"thought it was D. not B."},{"timestamp":"1656826740.0","poster":"aandc","comment_id":"626430","upvote_count":"1","content":"Selected Answer: B\nkey word \"launch and start production instances.\" -> terminate production should be prohibited"},{"timestamp":"1655651700.0","poster":"Kb80","content":"Selected Answer: C\nC. https://aws.amazon.com/premiumsupport/knowledge-center/accidental-termination/","upvote_count":"2","comment_id":"618732"},{"poster":"tartarus23","timestamp":"1651355760.0","upvote_count":"2","content":"Selected Answer: B\nB. because it allows u separate dev and prod instances and utilize IAM to disable the prod termination access","comment_id":"595312"},{"upvote_count":"1","timestamp":"1651274940.0","content":"Selected Answer: B\nb lets you separate dev and prod","poster":"tartarus23","comment_id":"594725"},{"comment_id":"554189","content":"A B and D assume that you will login as this user that has been created, what if he is not logged in as that user? Therefore only possible answer is C","poster":"jyrajan69","upvote_count":"1","timestamp":"1645594560.0"},{"comments":[{"comment_id":"540208","upvote_count":"1","poster":"HellGate","timestamp":"1643952240.0","content":"Change to B"}],"upvote_count":"2","poster":"HellGate","content":"CCCCCC","comment_id":"540207","timestamp":"1643952120.0"},{"upvote_count":"2","poster":"CoryD","timestamp":"1641956460.0","comment_id":"521885","content":"Correct answer is C. Ignore everyone sayings it's B...IT'S NOT B. The requirement states that he still needs to be able to delete resources after the fix is implemented. This question was made for termination protection and MFA just adds onto it.","comments":[{"poster":"lulz111","content":"It states that he has to be able to terminate DEVELOPMENT resources after the fix, not all resources. The idea here is to allow them to continue to interact with dev ec2 instances but not kill prod instances.","upvote_count":"1","timestamp":"1643306640.0","comment_id":"534007"}]},{"poster":"bwestpha","timestamp":"1640200200.0","comment_id":"507314","content":"Pretty sure its C . He still has to be able to terminate them, just not y accident. Yes MFA won't help here, but termination protection does.","upvote_count":"2"},{"poster":"01037","upvote_count":"2","content":"B is correct","comment_id":"367728","timestamp":"1635921120.0"},{"upvote_count":"1","content":"B.\nFinally understand why D isn't the answer.\nD doesn't provide a solution to determine the difference between DEV and PROD.","poster":"newme","comment_id":"223311","timestamp":"1635881760.0"},{"upvote_count":"2","timestamp":"1635714420.0","poster":"manoj101","comment_id":"176165","content":"B is correct"},{"poster":"fullaws","timestamp":"1635506340.0","content":"B is correct","upvote_count":"3","comment_id":"144650"},{"poster":"Teri","timestamp":"1632424200.0","comment_id":"18911","upvote_count":"2","content":"why not C?","comments":[{"poster":"examacc","upvote_count":"3","content":"Because he can still terminate instance as he is IAM user so will have access to MFA as well. By using policy to deny termination will not let him to do that.","comment_id":"23246","comments":[{"timestamp":"1635252240.0","content":"yeah, but he still should be able to terminate instances as per the requirements. i reckon it is C","poster":"jimmy_sticks","comment_id":"126083","upvote_count":"3"}],"timestamp":"1632722760.0"}]}],"question_id":865,"isMC":true,"answers_community":["C (55%)","B (45%)"]}],"exam":{"lastUpdated":"11 Apr 2025","numberOfQuestions":1019,"isMCOnly":false,"isBeta":false,"id":32,"provider":"Amazon","name":"AWS Certified Solutions Architect - Professional","isImplemented":true},"currentPage":173},"__N_SSP":true}