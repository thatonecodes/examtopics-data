{"pageProps":{"questions":[{"id":"nTbwjmY0NNXlplxEYwhv","url":"https://www.examtopics.com/discussions/amazon/view/95563-exam-aws-certified-solutions-architect-professional-sap-c02/","topic":"1","discussion":[{"timestamp":"1689523620.0","comment_id":"778078","poster":"masetromain","upvote_count":"8","content":"Selected Answer: B\nB is correct. It can prevent the issue from happening again by monitoring the file system with the FreeStorageCapacity metric in Amazon CloudWatch and using Amazon EventBridge to invoke an AWS Lambda function to increase the capacity as required. This ensures that the file system always has enough free space to store user profiles and avoids reaching maximum capacity.\nA: Removing old user profiles may not be sufficient to create enough space and does not prevent the problem from happening again.\nC: AWS Step Functions cannot be used to increase capacity, it is a service for creating and running workflows that stitch together multiple AWS services.\nD: Creating an additional FSx for Windows File Server file system and updating user profile redirection for a portion of the users may not be sufficient to prevent the problem from happening again and does not address the current capacity issue."},{"timestamp":"1694314380.0","upvote_count":"8","poster":"God_Is_Love","comment_id":"834651","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/cli/latest/reference/fsx/update-file-system.html\nEventBridge invoking lambda to update settings will prevent too from occurring again"},{"content":"Selected Answer: B\nWouldn't you need a cloudwatch alarm that would trigger a Lambda based on the metric going above a certain treshold?\nMetric -> Lambda is a bit of a shortcut","comment_id":"1213872","upvote_count":"1","poster":"sse69","timestamp":"1732039920.0"},{"timestamp":"1730214000.0","content":"Selected Answer: D\nIt's D.\nOption B Simply do not prevent problem to happen again. It's not possible to resize the FSx Size after creation so option D is more suitable.","poster":"red_panda","upvote_count":"1","comment_id":"1204014"},{"comment_id":"1104394","upvote_count":"1","poster":"career360guru","content":"Selected Answer: B\nOption B","timestamp":"1719183300.0"},{"comment_id":"958597","poster":"rtguru","upvote_count":"1","content":"B is the correct answer","timestamp":"1705853640.0"},{"poster":"NikkyDicky","content":"Selected Answer: B\nit's B","comment_id":"942172","upvote_count":"1","timestamp":"1704317220.0"},{"timestamp":"1702935960.0","content":"Selected Answer: B\nkeyword == update-file-system","comment_id":"926879","upvote_count":"1","poster":"SkyZeroZx"},{"comment_id":"903791","timestamp":"1700644020.0","comments":[{"timestamp":"1703270520.0","poster":"Maria2023","comment_id":"930796","upvote_count":"1","content":"Perhaps the metric is used to trigger the step functions"}],"content":"Selected Answer: C\nIs it necessary to implement new cloudwatch metric? And using step functions seems to be able to increase storage capacity, according to the following reference. \nhttps://docs.aws.amazon.com/step-functions/latest/dg/supported-services-awssdk.html#supported-services-awssdk-list","upvote_count":"1","poster":"leehjworking"},{"upvote_count":"2","poster":"OCHT","comment_id":"884992","comments":[{"poster":"rbm2023","timestamp":"1700577960.0","content":"StorageCapacity\nUse this parameter to increase the storage capacity of an FSx for Windows File Server, FSx for Lustre, FSx for OpenZFS, or FSx for ONTAP file system. Specifies the storage capacity target value, in GiB, to increase the storage capacity for the file system that you're updating.\nhttps://docs.aws.amazon.com/fsx/latest/APIReference/API_UpdateFileSystem.html\nExample using the CLI\naws fsx update-file-system --file-system-id fs-0123456789abcdef0 --storage-capacity 10240","comment_id":"903221","upvote_count":"5"}],"timestamp":"1698660720.0","content":"Selected Answer: D\nB. Increasing capacity using the update-file-system command is not applicable to FSx for Windows File Server. The command is for Amazon EFS, not FSx for Windows File Server."},{"poster":"yama234","upvote_count":"3","comment_id":"884937","content":"B\nAs you need additional storage, you can increase the storage capacity that is configured on your FSx for Windows File Server file system. You can do so using the Amazon FSx console, the Amazon FSx API, or the AWS Command Line Interface (AWS CLI).","timestamp":"1698656100.0"},{"poster":"Cloud_noob","content":"Selected Answer: B\nhttps://chat.openai.com/chat","timestamp":"1697169360.0","comment_id":"869049","upvote_count":"2"},{"timestamp":"1695796740.0","upvote_count":"2","comment_id":"851808","content":"Selected Answer: B\nB is correct","poster":"mfsec"},{"poster":"zozza2023","content":"Selected Answer: B\nB seems to be the correct answer.\nthe unique possible solution is to add storage capacity using CLI","upvote_count":"4","comment_id":"792249","timestamp":"1690673100.0"},{"upvote_count":"3","content":"Selected Answer: B\nTo increase the storage capacity for an FSx for Windows File Server file system, use the AWS CLI command update-file-system. https://docs.aws.amazon.com/fsx/latest/WindowsGuide/managing-storage-capacity.html It's B.","timestamp":"1690274040.0","poster":"pitakk","comment_id":"787509"},{"content":"B is correct. It can prevent issue happen again with EventBridge and Lambda\nA: not make sense at all\nC: Cannot use Step Function to increase capacity\nD: not prevent happen again","upvote_count":"2","poster":"zhangyu20000","timestamp":"1689512580.0","comment_id":"777825"}],"answer":"B","answer_ET":"B","timestamp":"2023-01-16 16:03:00","question_text":"A solutions architect is investigating an issue in which a company cannot establish new sessions in Amazon Workspaces. An initial analysis indicates that the issue involves user profiles. The Amazon Workspaces environment is configured to use Amazon FSx for Windows File Server as the profile share storage. The FSx for Windows File Server file system is configured with 10 TB of storage.\n\nThe solutions architect discovers that the file system has reached Its maximum capacity. The solutions architect must ensure that users can regain access. The solution also must prevent the problem from occurring again.\n\nWhich solution will meet these requirements?","isMC":true,"answer_images":[],"unix_timestamp":1673881380,"answers_community":["B (89%)","9%"],"question_images":[],"answer_description":"","choices":{"D":"Remove old user profiles to create space. Create an additional FSx for Windows File Server file system. Update the user profile redirection for 50% of the users to use the new file system.","A":"Remove old user profiles to create space. Migrate the user profiles to an Amazon FSx for Lustre file system.","C":"Monitor the file system by using the FreeStorageCapacity metric in Amazon CloudWatch. Use AWS Step Functions to increase the capacity as required.","B":"Increase capacity by using the update-file-system command. Implement an Amazon CloudWatch metric that monitors free space. Use Amazon EventBridge to invoke an AWS Lambda function to increase capacity as required."},"exam_id":33,"question_id":51},{"id":"7NEVpuAfsDx7JvsuJJSo","question_text":"An international delivery company hosts a delivery management system on AWS. Drivers use the system to upload confirmation of delivery. Confirmation includes the recipient’s signature or a photo of the package with the recipient. The driver’s handheld device uploads signatures and photos through FTP to a single Amazon EC2 instance. Each handheld device saves a file in a directory based on the signed-in user, and the file name matches the delivery number. The EC2 instance then adds metadata to the file after querying a central database to pull delivery information. The file is then placed in Amazon S3 for archiving.\n\nAs the company expands, drivers report that the system is rejecting connections. The FTP server is having problems because of dropped connections and memory issues in response to these problems, a system engineer schedules a cron task to reboot the EC2 instance every 30 minutes. The billing team reports that files are not always in the archive and that the central system is not always updated.\n\nA solutions architect needs to design a solution that maximizes scalability to ensure that the archive always receives the files and that systems are always updated. The handheld devices cannot be modified, so the company cannot deploy a new application.\n\nWhich solution will meet these requirements?","answer_ET":"C","answer_description":"","question_id":52,"topic":"1","timestamp":"2023-01-16 16:04:00","url":"https://www.examtopics.com/discussions/amazon/view/95564-exam-aws-certified-solutions-architect-professional-sap-c02/","choices":{"B":"Use AWS Transfer Family to create an FTP server that places the files in Amazon Elastic File System (Amazon EFS). Mount the EFS volume to the existing EC2 instance. Point the EC2 instance to the new path for file processing.","D":"Update the handheld devices to place the files directly in Amazon S3. Use an S3 event notification through Amazon Simple Queue Service (Amazon SQS) to invoke an AWS Lambda function. Configure the Lambda function to add the metadata and update the delivery system.","A":"Create an AMI of the existing EC2 instance. Create an Auto Scaling group of EC2 instances behind an Application Load Balancer. Configure the Auto Scaling group to have a minimum of three instances.","C":"Use AWS Transfer Family to create an FTP server that places the files in Amazon S3. Use an S3 event notification through Amazon Simple Notification Service (Amazon SNS) to invoke an AWS Lambda function. Configure the Lambda function to add the metadata and update the delivery system."},"discussion":[{"content":"Selected Answer: C\nC is correct. Using AWS Transfer Family to create an FTP server that places the files in Amazon S3 and using S3 event notifications through Amazon Simple Notification Service (Amazon SNS) to invoke an AWS Lambda function will ensure that the archive always receives the files and that the central system is always updated. This solution maximizes scalability and eliminates the need for manual intervention, such as rebooting the EC2 instance. \n\nOption A and B still use EC2 instance, which is the source of the problem. Option D requires modification to the handheld devices which is not possible.","comment_id":"778082","upvote_count":"14","poster":"masetromain","timestamp":"1689523800.0"},{"timestamp":"1709429220.0","upvote_count":"7","poster":"venvig","comment_id":"997186","content":"Selected Answer: B\nI agree that \"C\" is the ideal design.\nBut here the question states that :\n Ec2 instance is running the SFTP server.\n File is uploaded from handheld devices to a file system in the Ec2 instance.\n The Ec2 instance then adds metadata to the file.\n The file is then placed in s3.\nThe condition states that:\n The company cannot deploy a new application.\n\nBased on the condition, if I use lambda to add meta data, then its like deploying a new application.\n(We don't know if the application can be seamlessly rewritten in lambda. Will it finish under 15 mins ? etc.,)\nIf we strictly interpret this as not being able to introduce any new logic or components (like a Lambda function for metadata processing), then Option (B) is the answer.\nOption B essentially replaces the FTP server with AWS Transfer Family and uses Amazon EFS as the file storage, which can scale and handle more connections. The existing EC2 instance, which already has the logic for metadata addition, would simply point to this new file path on EFS. This minimizes changes to the existing application logic.","comments":[{"content":"they had to reboot the ec2 because of memory, without scaling EC2 they will still have that problem and since B does nothing about adding more memory, it cannot be right choice.","poster":"pk0619","timestamp":"1734744300.0","comments":[{"comment_id":"1329775","timestamp":"1734744360.0","poster":"pk0619","upvote_count":"1","content":"Actually offloading FTP from EC2 might eliminate memory issue, so it could very well be B as well"}],"comment_id":"1329773","upvote_count":"1"},{"upvote_count":"2","comment_id":"1175945","content":"the text is: \"The handheld devices cannot be modified, so the company cannot deploy a new application\". Following your comment, you can't use neither the AWS Transfer Family. This is also new :D","timestamp":"1726583640.0","poster":"gofavad926"},{"poster":"kgcain","timestamp":"1713893880.0","upvote_count":"1","content":"From the app description, I am sure that it should work under 15min.","comment_id":"1052124"}]},{"content":"B is the best answer. The system is such that each handheld device saves a file in a directory based on the signed-in user, and the file name matches the delivery number. This means that we need a file storage that the data are stored hierarchically in a top-down network of folders. And a file system that has adaptive throughtput to resolve the dropped connections and memory issues. EFS will be the suitable solution component. S3 however has all the data stored on the same flat plane requiring more comprehensive metadata (labels) to make it manageable.","poster":"EApeer","timestamp":"1727169360.0","comment_id":"1181472","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: C\nIt says \"so the company cannot deploy a new application\".\n\nThis means that it's the handheld devices they can't deploy a new application into. While B works, It still relies on one EC2 instance, which is a part of the problem.","timestamp":"1726979220.0","poster":"kz407","comment_id":"1179882"},{"timestamp":"1726583760.0","content":"Selected Answer: C\nC, transfer family + S3","upvote_count":"1","comment_id":"1175946","poster":"gofavad926"},{"poster":"zanhsieh","content":"Selected Answer: C\nC.\nA: No. FTP is not HTTP / HTTPS. FTP -> NLB. HTTP / HTTPS -> ALB.\nB: No. This needs extra steps (DataSync?) to move to S3, and the billing team would still complain about not always updated since it will be certain lag-behind time.\nC: Correct.\nD: No. S3 event notification can directly trigger Lambda.","comment_id":"1139713","upvote_count":"2","timestamp":"1722728700.0"},{"poster":"JMAN1","upvote_count":"1","timestamp":"1719787320.0","content":"Selected Answer: C\nC. does not require handheld device to be changed. And it solves EC2 dropped Conection by uisng S3.","comment_id":"1110906"},{"upvote_count":"1","poster":"career360guru","content":"Selected Answer: C\nOption C","comment_id":"1104396","timestamp":"1719183600.0"},{"comment_id":"1047120","poster":"Chung234","comments":[{"timestamp":"1725257820.0","poster":"ele","content":"ALB is a load balancer that operates at Layer 7. Only HTTP and HTTPS can be used as ALB protocols.\nTherefore, it is not possible to set ALB at the front of the FTP server.","comment_id":"1163985","upvote_count":"1"}],"upvote_count":"3","timestamp":"1713462660.0","content":"Selected Answer: B\nThe answer is A.\n\n Q: Can I use FTP with an internet-facing endpoint?\n\nA: No, when you enable FTP, you will only be able to use VPC hosted endpoint‘s internal access option. If traffic needs to traverse the public network, secure protocols such as SFTP or FTPS should be used.\n\nSource: https://aws.amazon.com/aws-transfer-family/faqs/"},{"comment_id":"958617","content":"This one of those tricky questions. I'm not sure if to go with A or C","timestamp":"1705855320.0","upvote_count":"1","poster":"rtguru"},{"upvote_count":"1","comment_id":"949450","content":"IDK yall, it does say clearly \"cannot deploy a new application\" and the only instance of that is A.\n\nI Agree C is better but IDK the semantics here","timestamp":"1705036140.0","poster":"rrrrrrrrrr1"},{"upvote_count":"1","poster":"NikkyDicky","comment_id":"942175","content":"Selected Answer: C\nits a c","timestamp":"1704317460.0"},{"comment_id":"930803","content":"Selected Answer: C\nSince AWS Transfer Family supports Amazon S3 Access Point then it's a standard scenario - FTP->S3->Event->Lambda. Scalable and serverless","poster":"Maria2023","timestamp":"1703271060.0","upvote_count":"2"},{"comment_id":"928018","content":"Selected Answer: C\nolabiba.ai says C.\n\n\n1. Scalability: By using AWS Transfer Family to create an FTP server that places the files directly in Amazon S3, you can leverage the scalability and durability of S3. S3 is designed to handle high volumes of data and can scale seamlessly as your company expands.\n\n2. Reliability: With S3 as the destination for the files, you can ensure that the archive always receives the files. S3 provides high durability and availability, reducing the chances of data loss.\n\n3. System updates: By using an S3 event notification through Amazon SNS, you can trigger an AWS Lambda function whenever a new file is uploaded to S3. This Lambda function can then add the necessary metadata and update the delivery system, ensuring that the central system is always updated.\n\n4. No modification to handheld devices: Since the handheld devices cannot be modified, this solution allows the devices to continue uploading files through FTP. The only change is the destination, which is now the S3 bucket.","poster":"Jackhemo","timestamp":"1703035020.0","upvote_count":"1"},{"upvote_count":"3","poster":"mfsec","comment_id":"851809","timestamp":"1695796800.0","content":"Selected Answer: C\nC is the most efficient"},{"poster":"zozza2023","comment_id":"792253","content":"Selected Answer: C\nC is correct","timestamp":"1690673400.0","upvote_count":"3"},{"poster":"zhangyu20000","timestamp":"1689512640.0","comment_id":"777829","upvote_count":"2","content":"C is correct"}],"answers_community":["C (75%)","B (25%)"],"exam_id":33,"isMC":true,"answer":"C","unix_timestamp":1673881440,"question_images":[],"answer_images":[]},{"id":"SQRNAt7vverhRSJmhevM","discussion":[{"poster":"masetromain","comment_id":"778084","content":"Selected Answer: A\nA is correct. Provision an Aurora Replica in a different Region will meet the requirement of the application being able to recover to a separate AWS Region in the event of an application failure, and no data can be lost, with the least amount of operational overhead.\n\nB. AWS DataSync can replicate data, but it is not a fully managed service and requires more configuration and management.\n\nC. AWS DMS is a fully managed service for migrating data between databases, but it may require additional configuration and management to continuously replicate data in real-time.\n\nD. Amazon DLM can be used for scheduling snapshots, but it does not provide real-time replication and may not meet the requirement of no data loss in case of a failure.","upvote_count":"8","timestamp":"1689523860.0"},{"upvote_count":"1","content":"Selected Answer: A\nOption A","timestamp":"1719183840.0","comment_id":"1104397","poster":"career360guru"},{"comment_id":"942201","upvote_count":"2","poster":"NikkyDicky","content":"Selected Answer: A\nits an A","timestamp":"1704320640.0"},{"content":"When you provision an Aurora Replica in a different AWS Region, the replica is kept in sync with the primary database using Aurora's replication capabilities. In the event of a failure in the primary Region, you can promote the Aurora Replica to become the new primary database, which allows you to continue operations with no data loss.\n\nHowever, provisioning and maintaining an Aurora Replica in a different AWS Region requires ongoing management and monitoring to ensure that it stays in sync with the primary database","poster":"Goatin","upvote_count":"3","comment_id":"902673","timestamp":"1700504460.0"},{"comment_id":"851811","content":"Selected Answer: A\nReplica","poster":"mfsec","timestamp":"1695796860.0","upvote_count":"4"},{"comment_id":"835677","upvote_count":"4","content":"Selected Answer: A\nB,C are on premises usecase solutions. D is wrong because 5 minute worth of data could be lost against the requirement. So A is correct. In fact replica works as standby if primary DB fails.","timestamp":"1694402100.0","poster":"God_Is_Love"},{"comment_id":"792259","poster":"zozza2023","content":"Selected Answer: A\nA is correct","upvote_count":"4","timestamp":"1690673640.0"},{"upvote_count":"2","comment_id":"777839","poster":"zhangyu20000","timestamp":"1689512880.0","content":"A is correct\nB: cannot use DataSync for Aurora backup\nC: too complex\nD: DLM is for EBS backup. Here use managed Aurora server, no access to EBS"}],"answers_community":["A (100%)"],"choices":{"A":"Provision an Aurora Replica in a different Region.","D":"Use Amazon Data Lifecycle Manager (Amazon DLM) to schedule a snapshot every 5 minutes.","C":"Set up AWS Database Migration Service (AWS DMS) to perform a continuous replication of the data to a different Region.","B":"Set up AWS DataSync for continuous replication of the data to a different Region."},"answer_images":[],"topic":"1","exam_id":33,"answer_description":"","question_id":53,"answer":"A","answer_ET":"A","question_text":"A company is running an application in the AWS Cloud. The application runs on containers m an Amazon Elastic Container Service (Amazon ECS) cluster. The ECS tasks use the Fargate launch type. The application's data is relational and is stored in Amazon Aurora MySQL. To meet regulatory requirements, the application must be able to recover to a separate AWS Region in the event of an application failure. In case of a failure, no data can be lost.\n\nWhich solution will meet these requirements with the LEAST amount of operational overhead?","isMC":true,"unix_timestamp":1673881680,"url":"https://www.examtopics.com/discussions/amazon/view/95565-exam-aws-certified-solutions-architect-professional-sap-c02/","timestamp":"2023-01-16 16:08:00","question_images":[]},{"id":"0ImtwHnk9lPZ683JNHXN","exam_id":33,"answer":"C","unix_timestamp":1673881800,"question_text":"A financial services company receives a regular data feed from its credit card servicing partner. Approximately 5,000 records are sent every 15 minutes in plaintext, delivered over HTTPS directly into an Amazon S3 bucket with server-side encryption. This feed contains sensitive credit card primary account number (PAN) data. The company needs to automatically mask the PAN before sending the data to another S3 bucket for additional internal processing. The company also needs to remove and merge specific fields, and then transform the record into JSON format. Additionally, extra feeds are likely to be added in the future, so any design needs to be easily expandable.\n\nWhich solutions will meet these requirements?","topic":"1","answer_images":[],"question_id":54,"question_images":[],"answer_ET":"C","url":"https://www.examtopics.com/discussions/amazon/view/95566-exam-aws-certified-solutions-architect-professional-sap-c02/","choices":{"B":"Invoke an AWS Lambda function on file delivery that extracts each record and writes it to an Amazon SQS queue. Configure an AWS Fargate container application to automatically scale to a single instance when the SQS queue contains messages. Have the application process each record, and transform the record into JSON format. When the queue is empty, send the results to another S3 bucket for internal processing and scale down the AWS Fargate instance.","A":"Invoke an AWS Lambda function on file delivery that extracts each record and writes it to an Amazon SQS queue. Invoke another Lambda function when new messages arrive in the SQS queue to process the records, writing the results to a temporary location in Amazon S3. Invoke a final Lambda function once the SQS queue is empty to transform the records into JSON format and send the results to another S3 bucket for internal processing.","D":"Create an AWS Glue crawler and custom classifier based upon the data feed formats and build a table definition to match. Perform an Amazon Athena query on file delivery to start an Amazon EMR ETL job to transform the entire record according to the processing and transformation requirements. Define the output format as JSON. Once complete, send the results to another S3 bucket for internal processing and scale down the EMR cluster.","C":"Create an AWS Glue crawler and custom classifier based on the data feed formats and build a table definition to match. Invoke an AWS Lambda function on file delivery to start an AWS Glue ETL job to transform the entire record according to the processing and transformation requirements. Define the output format as JSON. Once complete, have the ETL job send the results to another S3 bucket for internal processing."},"discussion":[{"comments":[{"upvote_count":"2","content":"EMR and Glue are the same; Glue is managed cluster by AWS , EMR customer manages the clutster","poster":"tycho","comment_id":"867216","timestamp":"1697021640.0"}],"content":"Selected Answer: C\nExtract Data from S3 + mask + Send to another S3 + Transform/Process + Load into S3\nAll these are ETL, ELT tasks which should ring Glue\n\nEMR is more focused on big data processing frameworks such as Hadoop and Spark, \nwhile Glue is more focused on ETL, More over 5000 records every 15 minutes is not soo big data..So I choose C","poster":"God_Is_Love","comment_id":"835712","timestamp":"1694408400.0","upvote_count":"21"},{"poster":"masetromain","upvote_count":"7","comment_id":"778089","content":"Selected Answer: C\nC is correct. It will process the data in batch mode using Glue ETL job which can handle large amount of data and can be scheduled to run periodically. This solution is also easily expandable for future feeds.\n\nA: It uses multiple Lambda functions, SQS queue and S3 temporary location which will increase operational overhead.\nB: Using Fargate may not be the most cost-effective solution and also it may not handle large amount of data.\nD: Athena and EMR both are powerful tools but they are more complex and can be more costly than Glue.","timestamp":"1689524100.0"},{"comment_id":"1104400","poster":"career360guru","upvote_count":"1","content":"Selected Answer: C\nOption C","timestamp":"1719184080.0"},{"upvote_count":"3","comment_id":"1025763","content":"Selected Answer: C\nOption C is the most suitable solution for the described scenario:\n\n1) AWS Glue Crawler and Custom Classifier: Use AWS Glue to create a crawler and custom classifier to understand and catalogue the data feed formats. This step ensures that AWS Glue can work with the incoming data effectively.\n\n2) AWS Glue ETL Job: Create an AWS Lambda function that triggers an AWS Glue ETL job when a new data file is delivered. This ETL job can perform the required transformation, including masking, field removal, and converting records to JSON format. AWS Glue is a suitable service for data preparation and transformation.\n\n3) Output to S3 Bucket.\n\nThis approach is scalable, easily expandable to handle additional feeds in the future, and leverages AWS Glue's capabilities for data transformation and processing. It also maintains a clear separation of tasks, making it a robust and efficient solution for the given requirements.","timestamp":"1712327460.0","poster":"totten"},{"comment_id":"1003766","poster":"dkcloudguru","content":"C is the good option EMR(Big data, Spark, Hadoop) is for near real-time data processing and it isn't a good fit in this case","timestamp":"1710062220.0","upvote_count":"1"},{"content":"Selected Answer: C\nits a C","timestamp":"1704320940.0","comment_id":"942204","poster":"NikkyDicky","upvote_count":"1"},{"poster":"SkyZeroZx","comment_id":"926881","upvote_count":"1","timestamp":"1702936320.0","content":"Selected Answer: C\nEMR is big data but not is need in this case \nthen AWS Glue + Lambdas + S3 is good option \nC"},{"upvote_count":"2","comment_id":"847127","content":"Selected Answer: C\nC makes the most sense.","timestamp":"1695383760.0","poster":"mfsec"},{"comments":[{"comment_id":"828081","content":"That, I agree. Honestly, I will use it from day one, regardless.","timestamp":"1693747560.0","poster":"Sarutobi","upvote_count":"1"}],"timestamp":"1691341500.0","comment_id":"800159","upvote_count":"1","poster":"Musk","content":"The question is at what point Athena and EMR are a better choice because it is a lot of data to store and process"},{"content":"Selected Answer: C\nC is correct.","timestamp":"1690674000.0","comment_id":"792262","upvote_count":"4","poster":"zozza2023"},{"timestamp":"1689513000.0","upvote_count":"1","poster":"zhangyu20000","comment_id":"777844","content":"C is correct"}],"answer_description":"","timestamp":"2023-01-16 16:10:00","isMC":true,"answers_community":["C (100%)"]},{"id":"CFvUcjOTnRZeXcnpE99q","timestamp":"2023-01-16 16:18:00","answer_images":[],"unix_timestamp":1673882280,"question_id":55,"answer":"B","exam_id":33,"question_images":[],"discussion":[{"poster":"God_Is_Love","comments":[{"upvote_count":"4","comment_id":"835729","timestamp":"1694411880.0","poster":"God_Is_Love","content":"Moreover, B has least operational over head of just initiating DR solution with replicating agents. C has operational overhead with DMS , SCT ,CDC,migration etc"},{"timestamp":"1718003040.0","upvote_count":"3","content":"I also agreed with the answer but then see this \"The application runs on physical servers that also run other applications. The on-premises application that the company is planning to migrate uses a MySQL database as a data store\" just database and physical server has other applications which not mentioned. Also from DR the statement gets changed to Migrate","poster":"swadeey","comment_id":"1092361"}],"timestamp":"1694411640.0","upvote_count":"27","content":"Selected Answer: B\nTricky one. This is not an on premise migration use case which prompts for answer C. Its a current situation of on premise application which the company wants to continue its state in the requirement of using AWS as DR solution.\nhttps://docs.aws.amazon.com/images/drs/latest/userguide/images/drs-failback-arc.png\nhttps://docs.aws.amazon.com/drs/latest/userguide/what-is-drs.html","comment_id":"835727"},{"poster":"Untamables","comment_id":"790295","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/drs/latest/userguide/what-is-drs.html\nhttps://docs.aws.amazon.com/drs/latest/userguide/recovery-workflow-gs.html\nOption C is wrong. That just mentions the migration method. I think this question asks us the DR architecture between on-premises and AWS cloud.","upvote_count":"7","timestamp":"1690512420.0"},{"upvote_count":"1","content":"Selected Answer: C\nThe answer should be C.\n\nTake note of the statement, \"The application runs on physical servers that also run other applications\". If you use the Application Migration Service, you will migrate these other applications, which have nothing to do with the application you are trying to protect.","poster":"altonh","timestamp":"1737021480.0","comment_id":"1341614"},{"comment_id":"1129741","poster":"ninomfr64","timestamp":"1721742240.0","content":"Selected Answer: B\nA = to use AWS DRS you first need to set it up in each AWS Region in which you want to use it. installing AWS Replication agent is not enough\nB = correct (to me the sentence \"Frequently perform failover and fallback from the most recent point in time\" is ambiguous as this points to actual failover/failback and not to drills) \nC = SCT is not needed wwith same engine db migration. also, install the rest of the software is not enough for app DR\nD = Volume Gateway can be used in a Back and Restore DR scenario, but the option D is very confused. Anyway, Storage Gateway for DR requires more overhead with respect to AWS DRS","upvote_count":"4"},{"comment_id":"1104403","upvote_count":"1","poster":"career360guru","timestamp":"1719184500.0","content":"Selected Answer: B\nOption B is right option. \nOption C only addresses DB instance replication and DR, it does not meet requirements of replicating other applications running on on-premise."},{"upvote_count":"1","comment_id":"1092362","poster":"swadeey","content":"Selected answer C changed from B\n\nThe application runs on physical servers that also run other applications. The on-premises application that the company is planning to migrate uses a MySQL database as a data store.","timestamp":"1718003100.0"},{"poster":"severlight","comment_id":"1072269","content":"Selected Answer: B\nElastic Disaster Recovery does the job","upvote_count":"1","timestamp":"1715840760.0"},{"poster":"AMohanty","comment_id":"997658","comments":[{"timestamp":"1710162780.0","content":"but how is failover happening\nthe very own purpose of DR is its automatic failover which is supported by option B","comment_id":"1004664","upvote_count":"1","poster":"chikorita"}],"content":"C\nWe are looking for a Business Continuity Solution\nMeaning RTO should be low","upvote_count":"1","timestamp":"1709480340.0"},{"upvote_count":"1","content":"Selected Answer: B\nAnswer is B.\nQuestions mentions \"least operational overhead\" (efforts in the future), and B mentions \"Frequently performing...\".\nHowever, that is the best-practice for AWS DR (as misleading as it sounds):\nhttps://docs.aws.amazon.com/drs/latest/userguide/failback-overview.html","poster":"cmoreira","timestamp":"1709478060.0","comment_id":"997638"},{"upvote_count":"3","timestamp":"1709190060.0","comment_id":"992789","poster":"Gabehcoud","content":"Selected Answer: B\nthe question is a bit misleading, first part says \"company is planning for business continuity\" the later part of the sentence says \"applications are migrating\". \nnevertheless, we should focus on the word business continuity. Going by that \"no migration\" is required so choose B.\n\nthat is my analysis."},{"timestamp":"1704321960.0","upvote_count":"1","content":"Selected Answer: B\nB for BC","comment_id":"942206","poster":"NikkyDicky"},{"upvote_count":"1","timestamp":"1702936380.0","poster":"SkyZeroZx","content":"Selected Answer: B\nkeyword = AWS Elastic Disaster Recovery \nB","comment_id":"926882"},{"comment_id":"903310","timestamp":"1700582700.0","upvote_count":"2","poster":"rbm2023","content":"Selected Answer: B\nThe company is looking for a disaster recovery solution and not a full migration to cloud. In my view the answer should use Elastic Disaster Recovery and not DMS. \nReferences\nhttps://www.cloudthat.com/resources/blog/scalable-cost-effective-cloud-disaster-recovery-with-aws-drs-elastic-disaster-recovery\nhttps://catalog.us-east-1.prod.workshops.aws/workshops/080af3a5-623d-4147-934d-c8d17daba346/en-US/introduction\nhttps://docs.aws.amazon.com/pt_br/mgn/latest/ug/Network-Settings-Video.html"},{"comment_id":"884334","timestamp":"1698591660.0","poster":"OCHT","upvote_count":"3","content":"Selected Answer: C\nit appears that option C has the least operational overhead since it involves creating AWS DMS replication servers and a target Amazon Aurora MySQL DB cluster to host the database, creating a DMS replication task to copy existing data to the target DB cluster, creating a local AWS SCT CDC task to keep data synchronized, and installing the rest of the software on EC2 instances by starting with a compatible base AMI. The other options involve additional steps such as setting up replication for all servers (option A), initializing AWS Elastic Disaster Recovery and frequently performing failover and fallbacks (option B), or deploying an AWS Storage Gateway Volume Gateway and mounting volumes on all on-premises servers (option D)."},{"content":"Selected Answer: C\nC seems correct to me (DMS with SCT and CDC)","comment_id":"865721","timestamp":"1696873680.0","upvote_count":"1","poster":"dev112233xx"},{"poster":"mfsec","upvote_count":"3","timestamp":"1695383940.0","content":"Selected Answer: B\nB has less operational overhead.","comment_id":"847129"},{"poster":"taer","upvote_count":"2","comment_id":"843708","content":"Selected Answer: B\nB, tricky","timestamp":"1695116940.0"},{"timestamp":"1693068180.0","upvote_count":"3","poster":"kiran15789","comment_id":"822851","content":"Selected Answer: B\nhttps://aws.amazon.com/disaster-recovery/"},{"comment_id":"818461","upvote_count":"2","timestamp":"1692732720.0","content":"Selected Answer: B\nThe answer is definitely B. Database recovery is included as a feature with EDR.\nhttps://aws.amazon.com/blogs/storage/achieving-data-consistency-with-aws-elastic-disaster-recovery/","poster":"Yowie351"},{"upvote_count":"2","timestamp":"1692424560.0","comment_id":"813846","content":"Selected Answer: B\nDisaster recovery solution should be B , this option mentions AWS replication agent with reference to context of Elastic Disaster Recovery","poster":"Mahakali"},{"content":"Selected Answer: C\nSelecting C","poster":"spd","upvote_count":"1","comment_id":"807935","timestamp":"1691964300.0"},{"poster":"moota","comment_id":"805888","timestamp":"1691799900.0","content":"Selected Answer: B\nIt should be B. The frequent failover and failback should be mostly a drill like here https://docs.aws.amazon.com/drs/latest/userguide/failback-overview.html#drill-recover-instance-faq\n\nThe sentence does not make sense. CDC is not with SCT.\n\n> Create a local AWS Schema Conversion Tool (AWS SCT) change data capture (CDC) task to keep the data synchronized","upvote_count":"3"},{"timestamp":"1691516460.0","upvote_count":"2","comment_id":"802468","content":"If you Frequently perform failover and fallback... isn't that an operational overhead ?","poster":"MasterP007","comments":[{"comment_id":"805884","upvote_count":"2","timestamp":"1691799660.0","content":"Nope, it's a quick push button exercise.","poster":"moota"}]},{"comments":[{"content":"It's more of a drill like this https://docs.aws.amazon.com/drs/latest/userguide/failback-overview.html#drill-recover-instance-faq","comment_id":"805887","upvote_count":"1","poster":"moota","timestamp":"1691799780.0"}],"poster":"tatdatpham","timestamp":"1690922580.0","content":"Selected Answer: C\nC is correct.\nOption B, Initializing AWS Elastic Disaster Recovery in the target AWS Region, requires frequent failover and fallback from the most recent point in time which would increase operational overhead for the company. The goal is to have the solution with the LEAST operational overhead.\nOption C, using AWS DMS and SCT, provides a more efficient and less manual process for replicating and synchronizing the database, reducing the operational overhead.","upvote_count":"3","comment_id":"795563"},{"content":"Selected Answer: C\nI select C","timestamp":"1690674120.0","poster":"zozza2023","upvote_count":"3","comment_id":"792264"},{"timestamp":"1690669200.0","upvote_count":"4","poster":"bititan","content":"Selected Answer: B\nthis is asking about application and not data alone. So option B with DRS is good. C is concentrating only on DB migration.","comment_id":"792199"},{"timestamp":"1690111740.0","content":"Selected Answer: B\nC is intuitive and makes sense apart from the fact that there's no CDC in SCT... I guess they thought about B","comment_id":"785387","poster":"pitakk","comments":[{"comment_id":"801516","content":"CDC would be referenced as part of DMS\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Task.CDC.html","timestamp":"1691441280.0","upvote_count":"1","poster":"Signup_Nickname"}],"upvote_count":"2"},{"timestamp":"1689529860.0","content":"Selected Answer: C\nC. Create AWS Database Migration Service (AWS DMS) replication servers and a target Amazon Aurora MySQL DB cluster to host the database. Create a DMS replication task to copy the existing data to the target DB cluster. Create a local AWS Schema Conversion Tool (AWS SCT) change data capture (CDC) task to keep the data synchronized. Install the rest of the software on EC2 instances by starting with a compatible base AMI.\n\nThis option would allow the company to use the AWS DMS and SCT tools, which are specifically designed for migrating and replicating databases, to migrate their MySQL database to an Amazon Aurora MySQL DB cluster in the target AWS Region. This would reduce the operational overhead compared to option A and B, as it would automate the process of replicating and synchronizing the data. Option D is also a viable solution but it would have more operational overhead compared to option C as it involves more manual steps like taking regular snapshot and restoring them.","comment_id":"778180","poster":"masetromain","upvote_count":"4"},{"timestamp":"1689513480.0","comment_id":"777850","poster":"zhangyu20000","upvote_count":"2","content":"C is correct"}],"answer_ET":"B","isMC":true,"question_text":"A company wants to use AWS to create a business continuity solution in case the company's main on-premises application fails. The application runs on physical servers that also run other applications. The on-premises application that the company is planning to migrate uses a MySQL database as a data store. All the company's on-premises applications use operating systems that are compatible with Amazon EC2.\n\nWhich solution will achieve the company's goal with the LEAST operational overhead?","choices":{"C":"Create AWS Database Migration Service (AWS DMS) replication servers and a target Amazon Aurora MySQL DB cluster to host the database. Create a DMS replication task to copy the existing data to the target DB cluster. Create a local AWS Schema Conversion Tool (AWS SCT) change data capture (CDC) task to keep the data synchronized. Install the rest of the software on EC2 instances by starting with a compatible base AMI.","A":"Install the AWS Replication Agent on the source servers, including the MySQL servers. Set up replication for all servers. Launch test instances for regular drills. Cut over to the test instances to fail over the workload in the case of a failure event.","D":"Deploy an AWS Storage Gateway Volume Gateway on premises. Mount volumes on all on-premises servers. Install the application and the MySQL database on the new volumes. Take regular snapshots. Install all the software on EC2 Instances by starting with a compatible base AMI. Launch a Volume Gateway on an EC2 instance. Restore the volumes from the latest snapshot. Mount the new volumes on the EC2 instances in the case of a failure event.","B":"Install the AWS Replication Agent on the source servers, including the MySQL servers. Initialize AWS Elastic Disaster Recovery in the target AWS Region. Define the launch settings. Frequently perform failover and fallback from the most recent point in time."},"topic":"1","answers_community":["B (81%)","C (19%)"],"url":"https://www.examtopics.com/discussions/amazon/view/95568-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_description":""}],"exam":{"lastUpdated":"11 Apr 2025","isMCOnly":true,"provider":"Amazon","name":"AWS Certified Solutions Architect - Professional SAP-C02","numberOfQuestions":529,"id":33,"isImplemented":true,"isBeta":false},"currentPage":11},"__N_SSP":true}