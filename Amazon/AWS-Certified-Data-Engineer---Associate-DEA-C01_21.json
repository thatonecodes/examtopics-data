{"pageProps":{"questions":[{"id":"ufiWl4fuwq6KVdcyov4B","url":"https://www.examtopics.com/discussions/amazon/view/131683-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answers_community":["AB (66%)","BE (16%)","Other"],"answer_description":"","unix_timestamp":1705749240,"exam_id":21,"topic":"1","question_id":101,"question_text":"A data engineer must orchestrate a series of Amazon Athena queries that will run every day. Each query can run for more than 15 minutes.\nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.)","answer_ET":"AB","answer":"AB","question_images":[],"isMC":true,"choices":{"A":"Use an AWS Lambda function and the Athena Boto3 client start_query_execution API call to invoke the Athena queries programmatically.","E":"Use Amazon Managed Workflows for Apache Airflow (Amazon MWAA) to orchestrate the Athena queries in AWS Batch.","B":"Create an AWS Step Functions workflow and add two states. Add the first state before the Lambda function. Configure the second state as a Wait state to periodically check whether the Athena query has finished using the Athena Boto3 get_query_execution API call. Configure the workflow to invoke the next query when the current query has finished running.","C":"Use an AWS Glue Python shell job and the Athena Boto3 client start_query_execution API call to invoke the Athena queries programmatically.","D":"Use an AWS Glue Python shell script to run a sleep timer that checks every 5 minutes to determine whether the current Athena query has finished running successfully. Configure the Python shell script to invoke the next query when the current query has finished running."},"discussion":[{"upvote_count":"10","comments":[{"comment_id":"1270617","poster":"San_Juan","upvote_count":"3","timestamp":"1724321280.0","content":"Lambda max timeout is 15 minutes, and the query takes more than 15 minutes. So Lambda should be ended prior the Athena query."}],"comment_id":"1140646","content":"Selected Answer: AB\nAWS Lambda can be effectively used to trigger Athena queries. By using the start_query_execution API from the Athena Boto3 client, you can programmatically start Athena queries. Lambda functions are cost-effective as they charge based on the compute time used, and there's no charge when the code is not running. However, Lambda has a maximum execution timeout of 15 minutes, which means it's not suitable for long-running operations but can be used to trigger or start queries.\nAWS Step Functions can orchestrate multiple AWS services in workflows. By using a Wait state, the workflow can periodically check the status of the Athena query, and proceed to the next step once the query is complete. This approach is more scalable and reliable compared to continuously running a Lambda function, as Step Functions can handle long-running processes better and can maintain the state of each step in the workflow.","timestamp":"1707102780.0","poster":"rralucard_"},{"comment_id":"1167986","content":"Selected Answer: BE\nB - because\n https://docs.aws.amazon.com/step-functions/latest/dg/sample-athena-query.html\nE - because \nhttps://aws.amazon.com/blogs/big-data/orchestrate-amazon-emr-serverless-spark-jobs-with-amazon-mwaa-and-data-validation-using-amazon-athena/","comments":[{"upvote_count":"2","comment_id":"1270620","timestamp":"1724321340.0","poster":"San_Juan","content":"I discarted E because Airflow is more expensive than Glue/Step-Functions. So B (step-function) and D (glue python shell)."},{"comment_id":"1211212","timestamp":"1715660280.0","poster":"DevoteamAnalytix","content":"The question is about a \"combination of steps\" - MWAA and Step Functions are different options, so I would prefer AB","comments":[{"content":"BE is right. A is only giving option to envoke the athena query. how about the response. if the execution is beyong 15 mins","timestamp":"1723395240.0","upvote_count":"1","comment_id":"1264238","poster":"sachin"}],"upvote_count":"3"}],"timestamp":"1709815320.0","poster":"GiorgioGss","upvote_count":"8"},{"upvote_count":"2","content":"Selected Answer: AB\nAfter real-world testing, A is a valid answer. This is because the Lambda only sends the API request to Athena, which runs the query. Even if the Lambda times out, the query result is still stored in the designated S3 bucket.","timestamp":"1738743060.0","comment_id":"1351766","poster":"Evan_Lin"},{"poster":"Udyan","timestamp":"1736604660.0","upvote_count":"2","content":"Selected Answer: AB\nWhy?\nB (Step Functions): Step Functions are ideal for orchestrating long-running workflows, including polling the Athena query status and invoking the next query when ready.\nA (Lambda): Lambda is used to programmatically trigger Athena queries within Step Functions, despite its 15-minute limitation, because Step Functions can manage the long runtime using Wait states.\nWhy Not C, D, or E?\nC and D involve Glue, which is better suited for ETL jobs than orchestration, making them less efficient and cost-effective.\nE (Amazon MWAA) introduces unnecessary cost and complexity for a straightforward workflow.","comment_id":"1339175"},{"upvote_count":"4","comment_id":"1326504","timestamp":"1734189180.0","content":"Selected Answer: BC\nBC for me\nA - lambda function will stop at 900s, so it will stop before query finishes(more than 15mins)\nE - Airflow is way more complex and expensive than step function","poster":"haby"},{"comment_id":"1321633","timestamp":"1733280300.0","upvote_count":"1","content":"Selected Answer: CE\nAB - Because of the Lambda timeout\n\nCEâ€”is correct. The query will be executed by a glue job, which will be orchestrated by Airflow. The job will be scheduled using AWS Batch.","poster":"altonh"},{"upvote_count":"2","poster":"Eleftheriia","timestamp":"1733043900.0","content":"Selected Answer: AB\nNot E because \"You should use Step Functions if you prioritize cost and performance\"\nhttps://aws.amazon.com/managed-workflows-for-apache-airflow/faqs/\n\nAnd also the fact that the queries take longer than 15 min can be handled with step functions, therefore AB","comment_id":"1320504"},{"poster":"truongnguyen86","timestamp":"1731761100.0","upvote_count":"1","comment_id":"1313060","content":"A.Why it's correct: AWS Lambda is a cost-effective, serverless option for invoking Athena queries using the Boto3 API. Lambda charges are based on execution time and memory usage, making it an efficient solution for periodic query execution.\nB. Why it's correct: Step Functions provide a serverless orchestration option with a pay-per-use pricing model. Adding a Wait state prevents excessive API calls and ensures queries are executed in sequence, making it a cost-effective and scalable solution.\nWhy the other options are less optimal:\n\n--\nE. Use Amazon Managed Workflows for Apache Airflow (MWAA): MWAA is powerful for complex workflows, but its pricing includes environment uptime costs, which can be higher than Lambda and Step Functions for simple tasks like orchestrating Athena queries.\nBy choosing A and B, you balance cost-effectiveness and simplicity for orchestrating daily Athena queries."},{"poster":"San_Juan","upvote_count":"2","timestamp":"1724321220.0","comment_id":"1270616","content":"Selected Answer: BD\n\nLambda maximum timeout is 15 minutes. So the query takes more than Lambda could manage. So you cannot use lambda. Use Step-Function (answer B) or glue python (answer D)\nAirflow is more expensive than Glue/Step-Functions, so E is discarted also."},{"comment_id":"1260934","timestamp":"1722834840.0","content":"Selected Answer: AB\nIt should be AB","poster":"V0811","upvote_count":"2"},{"content":"Selected Answer: AB\nSince the Athena API supports async/await, users are able to separate the steps into trigger queries and get results after 15 minutes.","poster":"alex1991","upvote_count":"2","comment_id":"1239594","timestamp":"1719741780.0"},{"comments":[{"comments":[{"upvote_count":"1","poster":"JoeAWSOCM","comment_id":"1321125","content":"Lambda is just for triggering the query. Its not waiting for the query to finish. The status of the query will be checked using Step functions.","timestamp":"1733177820.0"}],"comment_id":"1270621","poster":"San_Juan","content":"A could be not valid, as queries takes more than 15 minutes, and Lambda maximum timeout is 15 minutes. Lambda would be ended prior than the query is finished.","timestamp":"1724321400.0","upvote_count":"1"}],"timestamp":"1717894980.0","content":"Selected Answer: BE\ntricky, A is valid. Still, cost effective:\nB no one doubt on it. then why E?\nMWAA offers a managed Apache Airflow environment for orchestrating complex workflows.\nIt can handle long-running tasks like Athena queries efficiently.\nBatch Processing: Leveraging AWS Batch within the Airflow workflow allows for distributed and scalable execution of the Athena queries, improving overall processing efficiency.","upvote_count":"1","poster":"pypelyncar","comment_id":"1227000"},{"upvote_count":"2","poster":"valuedate","comment_id":"1218436","timestamp":"1716653520.0","content":"Selected Answer: AB\nmy opinian"},{"content":"Selected Answer: AB\nI would prefer AB","poster":"valuedate","upvote_count":"2","timestamp":"1716371100.0","comment_id":"1215632"},{"upvote_count":"3","content":"Selected Answer: AB\nLambda for kick start Athena\nStep Functions for orchestration","comment_id":"1213595","poster":"VerRi","timestamp":"1716090720.0"},{"content":"Option C and D involve using an AWS Glue Python shell script to run a sleep timer and periodically check whether the current Athena query has finished running. While this approach might seem cost-effective in terms of using AWS Glue, it's not the most efficient way to manage the execution of Athena queries. AWS Glue is primarily designed for ETL (Extract, Transform, Load) tasks rather than orchestrating long-running query execution.\n\nTherefore, while both options B, C and D could technically work, they might not be the most cost-effective or efficient solutions for orchestrating long-running Athena queries. Instead, options A and E would likely be more cost-effective and suitable for this scenario.","comment_id":"1206425","timestamp":"1714817220.0","upvote_count":"1","poster":"sdas1"},{"poster":"Christina666","timestamp":"1713072180.0","upvote_count":"4","content":"Selected Answer: AB\nLambda call Athena query;\nStep function orchestrate query workflow","comment_id":"1195283"},{"comments":[{"timestamp":"1724321580.0","comment_id":"1270622","content":"Yes, lambda could asynchronically run, but in this case, you don't get the status of the query. The current query should be ended before launch the next one.","upvote_count":"1","poster":"San_Juan"}],"poster":"arvehisa","timestamp":"1711803720.0","comment_id":"1186100","upvote_count":"4","content":"Selected Answer: AB\nA: Lambda is a good option and it only trigger the athena not actually run it. No need 15 min for it.\nB. it mentioned a series of athena queries and it may means that one query should wait until the former one finished. B is the perfect way to do it.\nAnd lambda and step functions are very cost effective."},{"timestamp":"1711334280.0","comment_id":"1182108","upvote_count":"2","content":"Selected Answer: CD\nRemember, anything involves writing codes are gonna be cheaper than automated/UI guided workflows, so that left ACD, and aws Lambda can't run for more than 15 mins so CD.\n\nGuys, go take the associate architect certificate first... this is basic knowledge... stop spamming chatgpt (wrongly) generated answer","comments":[{"content":"C could be discarted because it not said nothing about waiting for the current query to start a new one. So BD are the most accurate.","upvote_count":"1","poster":"San_Juan","timestamp":"1724321640.0","comment_id":"1270624"},{"timestamp":"1711517940.0","poster":"halogi","comment_id":"1183893","upvote_count":"4","content":"The Lambda function for A is only used to start Athena's query and then stop. It would only take around one second to do it.\nThe StepFunction waits for the Athena query to complete; it performs no computing, so it would be cheaper than using AWS Glue Python shell scripts.","comments":[{"content":"But if you stop the lambda before the query ends, you don't get the status of the query, and it seems that \"orchestrate\" meaning is to run them secuencially.","poster":"San_Juan","timestamp":"1724321700.0","comment_id":"1270627","upvote_count":"1"}]}],"poster":"cd93"},{"timestamp":"1710952980.0","upvote_count":"1","comment_id":"1178514","poster":"certplan","content":"2. **AWS Glue Documentation**:\n - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy to prepare and load data for analytics. AWS Glue offers capabilities for running Python shell jobs, which can be used to execute custom scripts for various data processing tasks. The documentation provides details on how to create and manage Python shell jobs, including examples of using scripts to interact with AWS services like Athena.\n - Reference: [AWS Glue Documentation]https://docs.aws.amazon.com/glue/index.html"},{"upvote_count":"2","poster":"certplan","content":"To justify the selection of options B and D as the most cost-effective combination for orchestrating Amazon Athena queries, let's refer to the official AWS documentation:\n\n1. **AWS Step Functions Documentation**:\n - AWS Step Functions is a fully managed service provided by AWS for coordinating the components of distributed applications and microservices using visual workflows. With Step Functions, you can build workflows that execute a sequence of AWS Lambda functions, API calls, and other AWS services. The documentation provides information on how to create workflows, define states, and configure wait states for checking the status of tasks, which aligns with the requirements of orchestrating Amazon Athena queries.\n - Reference: [AWS Step Functions Documentation]https://docs.aws.amazon.com/step-functions/index.html","comment_id":"1178513","timestamp":"1710952920.0"},{"poster":"certplan","timestamp":"1710952800.0","upvote_count":"2","comment_id":"1178509","content":"Option E: Using Amazon Managed Workflows for Apache Airflow (Amazon MWAA) to orchestrate Athena queries in AWS Batch.\n\nWhile Amazon MWAA provides managed Apache Airflow environments, which can be used for orchestrating workflows, it might not be the most cost-effective option for orchestrating Athena queries due to:\n\n- **Complexity**: Setting up and managing an Amazon MWAA environment can introduce additional complexity and potentially higher costs compared to other options.\n\n- **Resource Allocation**: Amazon MWAA environments come with a minimum cost, regardless of usage, and managing resources in AWS Batch might not be as cost-efficient for this specific use case compared to simpler solutions like Step Functions or Glue Python shell scripts.\n\nReference: [Amazon Managed Workflows for Apache Airflow Documentation]https://docs.aws.amazon.com/mwaa/index.html"},{"poster":"milofficial","upvote_count":"2","comment_id":"1176607","timestamp":"1710776940.0","content":"Selected Answer: AB\nI changed my mind"},{"comment_id":"1167978","upvote_count":"3","content":"Guys... stop pasting from GPT... paste some official docs to prove your choice of options.","poster":"GiorgioGss","timestamp":"1709814720.0"},{"poster":"CalvinL4","comment_id":"1164388","timestamp":"1709417880.0","content":"Lambda is out because it cannot run over 15 min.","upvote_count":"1","comments":[{"content":"This means AB are OUT. To orchestrate, Apache should be the option. I will go with CE.","poster":"CalvinL4","timestamp":"1709418240.0","upvote_count":"1","comment_id":"1164391"}]},{"upvote_count":"2","poster":"BartoszGolebiowski24","comment_id":"1147076","content":"Selected Answer: AB\nWe do not need to wait till Athena completes the query, so we will not reach a 15-minute timeout hard limit. So \"A\" is valid, we will use the step function to orchestrate the process.","timestamp":"1707641820.0"},{"poster":"TonyStark0122","upvote_count":"1","timestamp":"1706823240.0","comment_id":"1137948","content":"A. Use an AWS Lambda function and the Athena Boto3 client start_query_execution API call to invoke the Athena queries programmatically.\nB. Create an AWS Step Functions workflow and add two states. Add the first state before the Lambda function. Configure the second state as a Wait state to periodically check whether the Athena query has finished using the Athena Boto3 get_query_execution API call. Configure the workflow to invoke the next query when the current query has finished running."},{"comment_id":"1127228","upvote_count":"3","content":"Selected Answer: BC\nhttps://docs.aws.amazon.com/step-functions/latest/dg/sample-athena-query.html","poster":"milofficial","timestamp":"1705749240.0"}],"timestamp":"2024-01-20 12:14:00","answer_images":[]},{"id":"mou0rOijjMVKHDEU7z5Z","exam_id":21,"question_id":102,"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/150746-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","discussion":[{"timestamp":"1730967660.0","upvote_count":"7","poster":"Eleftheriia","comment_id":"1308283","content":"Selected Answer: A\nEven though both MWAA and Step functions can be used for managing task failures, MWAA is more suitable since the engineer would like to use python to orchestrate jobs. Usually, Step functions is used for minimal infrastructure management"},{"poster":"kupo777","comment_id":"1307111","timestamp":"1730758860.0","content":"Correct Answer: B\n\nAWS Step Functions allows you to build serverless workflows that coordinate various AWS services. It supports integrating with EMR for running Spark jobs, making API calls (including to Salesforce), and loading data into Redshift.\nStep Functions provide built-in error handling and retry capabilities, making it easier to manage failures in your workflow.\nAdditionally, you can use AWS SDK for Python (Boto3) to interact with Step Functions, enabling you to write your orchestration logic in Python.","upvote_count":"1"}],"question_text":"A data engineer wants to orchestrate a set of extract, transform, and load (ETL) jobs that run on AWS. The ETL jobs contain tasks that must run Apache Spark jobs on Amazon EMR, make API calls to Salesforce, and load data into Amazon Redshift.\n\nThe ETL jobs need to handle failures and retries automatically. The data engineer needs to use Python to orchestrate the jobs.\n\nWhich service will meet these requirements?","timestamp":"2024-11-04 23:21:00","answer_images":[],"answers_community":["A (100%)"],"isMC":true,"answer_ET":"A","unix_timestamp":1730758860,"answer":"A","choices":{"C":"AWS Glue","A":"Amazon Managed Workflows for Apache Airflow (Amazon MWAA)","D":"Amazon EventBridge","B":"AWS Step Functions"},"answer_description":"","topic":"1"},{"id":"A2omQ4JSy3CrcGVNqyed","isMC":true,"unix_timestamp":1730758620,"answer_images":[],"question_images":[],"answer_ET":"B","answers_community":["B (100%)"],"topic":"1","timestamp":"2024-11-04 23:17:00","exam_id":21,"discussion":[{"upvote_count":"2","content":"Selected Answer: B\ntext book example of Lambda Layers ...","comment_id":"1330751","poster":"HagarTheHorrible","timestamp":"1734946320.0"},{"timestamp":"1730758620.0","content":"Correct Answer: B\n\nLambda layers allow you to package common code and dependencies that can be shared across multiple Lambda functions. By placing the custom Python scripts in a layer, you can update the layer once and then update the version used by each Lambda function without needing to modify the function code directly.\nThis approach reduces redundancy, streamlines updates, and ensures that all functions using the layer have access to the latest version of the scripts with minimal manual effort.","comment_id":"1307107","upvote_count":"3","poster":"kupo777"}],"answer":"B","question_id":103,"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/150745-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_text":"A data engineer maintains custom Python scripts that perform a data formatting process that many AWS Lambda functions use. When the data engineer needs to modify the Python scripts, the data engineer must manually update all the Lambda functions.\n\nThe data engineer requires a less manual way to update the Lambda functions.\n\nWhich solution will meet this requirement?","choices":{"B":"Package the custom Python scripts into Lambda layers. Apply the Lambda layers to the Lambda functions.","D":"Assign the same alias to each Lambda function. Call each Lambda function by specifying the function's alias.","A":"Store the custom Python scripts in a shared Amazon S3 bucket. Store a pointer to the custom scripts in the execution context object.","C":"Store the custom Python scripts in a shared Amazon S3 bucket. Store a pointer to the customer scripts in environment variables."}},{"id":"LDjobmz7wVUAn3cPJ2zW","exam_id":21,"unix_timestamp":1730758200,"url":"https://www.examtopics.com/discussions/amazon/view/150743-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer_ET":"B","isMC":true,"answers_community":["B (90%)","10%"],"discussion":[{"content":"Selected Answer: B\nit is not A, Macie can only detect the PII","timestamp":"1734946560.0","upvote_count":"3","comment_id":"1330752","poster":"HagarTheHorrible"},{"comment_id":"1328237","upvote_count":"2","poster":"WarPig666","timestamp":"1734488520.0","content":"Selected Answer: B\nA canâ€™t be correct. Macie can discover PII, but not automatically redact it."},{"poster":"paali","upvote_count":"2","content":"Selected Answer: B\nMacie will only detect sensitive data, it can't redact it. So, we can use option B\n\nWith S3 Object Lambda and a prebuilt AWS Lambda function powered by Amazon Comprehend, you can protect PII data retrieved from S3 before returning it to an application.","comment_id":"1327836","timestamp":"1734427440.0"},{"content":"Selected Answer: A\nAmazon Macie is a fully managed data security and privacy service that uses machine learning to automatically discover, classify, and protect sensitive data, including personally identifiable information (PII). By running a sensitive data discovery job in Macie, the company can automatically identify PII in the S3 bucket and provide actionable insights to help secure it. The operational overhead is minimized because Macie handles the discovery and classification of PII automatically.","poster":"7a1d491","comment_id":"1326768","timestamp":"1734255240.0","upvote_count":"1"},{"content":"Selected Answer: B\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/tutorial-s3-object-lambda-redact-pii.html","poster":"michele_scar","upvote_count":"2","comment_id":"1313981","timestamp":"1731933900.0"},{"content":"Correct Answer: A\n\nAmazon Macie is designed specifically for discovering and protecting sensitive data within AWS environments. It automates the process of identifying PII in your S3 buckets, allowing you to create jobs that can regularly scan for and manage sensitive information.\nThis approach minimizes manual effort and integrates well into existing workflows, providing ongoing protection without requiring additional infrastructure or complex setups.","upvote_count":"3","comment_id":"1307102","timestamp":"1730758200.0","poster":"kupo777"}],"question_text":"A company stores customer data in an Amazon S3 bucket. Multiple teams in the company want to use the customer data for downstream analysis. The company needs to ensure that the teams do not have access to personally identifiable information (PII) about the customers.\n\nWhich solution will meet this requirement with LEAST operational overhead?","question_id":104,"choices":{"B":"Use S3 Object Lambda to access the data, and use Amazon Comprehend to detect and remove PII.","A":"Use Amazon Macie to create and run a sensitive data discovery job to detect and remove PII.","C":"Use Amazon Data Firehose and Amazon Comprehend to detect and remove PII.","D":"Use an AWS Glue DataBrew job to store the PII data in a second S3 bucket. Perform analysis on the data that remains in the original S3 bucket."},"answer":"B","topic":"1","answer_images":[],"question_images":[],"answer_description":"","timestamp":"2024-11-04 23:10:00"},{"id":"CY0ZKEDRKup1HszKaefN","discussion":[{"content":"Selected Answer: C\nfor monitoring API calls use CloutTrial, it is that simple","poster":"HagarTheHorrible","upvote_count":"2","comment_id":"1330754","timestamp":"1734946620.0"},{"poster":"kupo777","content":"C is correct.","comment_id":"1307098","timestamp":"1730757240.0","upvote_count":"2"}],"answer_ET":"C","isMC":true,"topic":"1","choices":{"A":"Use AWS Config rules to detect violations of the data access policy. Set up compliance alarms.","C":"Use AWS CloudTrail to track object-level events for the S3 bucket. Forward events to Amazon CloudWatch to set up CloudWatch alarms.","B":"Use Amazon CloudWatch metrics to gather object-level metrics. Set up CloudWatch alarms.","D":"Use Amazon S3 server access logs to monitor access to the bucket. Forward the access logs to an Amazon CloudWatch log group. Use metric filters on the log group to set up CloudWatch alarms."},"exam_id":21,"url":"https://www.examtopics.com/discussions/amazon/view/150741-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer":"C","answers_community":["C (100%)"],"unix_timestamp":1730757240,"answer_images":[],"question_id":105,"question_images":[],"timestamp":"2024-11-04 22:54:00","answer_description":"","question_text":"A company stores its processed data in an S3 bucket. The company has a strict data access policy. The company uses IAM roles to grant teams within the company different levels of access to the S3 bucket.\n\nThe company wants to receive notifications when a user violates the data access policy. Each notification must include the username of the user who violated the policy.\n\nWhich solution will meet these requirements?"}],"exam":{"name":"AWS Certified Data Engineer - Associate DEA-C01","isBeta":false,"provider":"Amazon","id":21,"numberOfQuestions":207,"lastUpdated":"11 Apr 2025","isImplemented":true,"isMCOnly":true},"currentPage":21},"__N_SSP":true}