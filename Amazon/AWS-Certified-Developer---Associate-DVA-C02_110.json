{"pageProps":{"questions":[{"id":"HHjACXTJ9RALk9bkW1bI","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/109241-exam-aws-certified-developer-associate-dva-c02-topic-1/","exam_id":24,"answer_description":"","answer":"A","unix_timestamp":1684086960,"answer_images":[],"choices":{"C":"The AWS Key Management Service (AWS KMS) key policy that is attached to the EC2 instance profile role","D":"The Amazon VPC endpoint policy","B":"The session policy that is applied to the EC2 instance role session","A":"The IAM policy that is attached to the EC2 instance profile role"},"answer_ET":"A","question_text":"A developer has written an application that runs on Amazon EC2 instances. The developer is adding functionality for the application to write objects to an Amazon S3 bucket.\n\nWhich policy must the developer modify to allow the instances to write these objects?","question_id":546,"answers_community":["A (100%)"],"discussion":[{"content":"Selected Answer: A\na is correct","upvote_count":"6","poster":"Prem28","comment_id":"897799","timestamp":"1684086960.0"},{"poster":"Ja13","comment_id":"919477","content":"Selected Answer: A\nA: https://repost.aws/knowledge-center/ec2-instance-access-s3-bucket","upvote_count":"5","timestamp":"1686329580.0"},{"upvote_count":"1","poster":"sumanshu","timestamp":"1735233540.0","content":"Selected Answer: A\nEC2 instances assume an IAM role through an instance profile.","comment_id":"1332020"},{"comment_id":"1287090","upvote_count":"1","content":"Selected Answer: A\n{\n \"Version\": \"2012-10-17\",\n \"Statement\": [\n {\n \"Effect\": \"Allow\",\n \"Action\": [\n \"s3:PutObject\",\n \"s3:PutObjectAcl\"\n ],\n \"Resource\": \"arn:aws:s3:::your-bucket-name/*\"\n }\n ]\n}","timestamp":"1726879380.0","poster":"albert_kuo"},{"poster":"65703c1","content":"Selected Answer: A\nA is the correct answer.","upvote_count":"1","comment_id":"1215974","timestamp":"1716406980.0"},{"timestamp":"1685461620.0","comment_id":"910394","poster":"mgonblan","content":"B: I Think B is better, because we need to use it on the instance session","upvote_count":"1"}],"question_images":[],"topic":"1","timestamp":"2023-05-14 19:56:00"},{"id":"QIhXLZCEVVV22xc4eWPs","answer_description":"","answer_ET":"C","isMC":true,"answer":"C","choices":{"B":"BGP logs","C":"VPC Flow Logs","A":"VPN logs","D":"AWS CloudTrail logs"},"topic":"1","exam_id":24,"url":"https://www.examtopics.com/discussions/amazon/view/108742-exam-aws-certified-developer-associate-dva-c02-topic-1/","question_images":[],"question_text":"A developer is leveraging a Border Gateway Protocol (BGP)-based AWS VPN connection to connect from on-premises to Amazon EC2 instances in the developer's account. The developer is able to access an EC2 instance in subnet A, but is unable to access an EC2 instance in subnet B in the same VPC.\n\nWhich logs can the developer use to verify whether the traffic is reaching subnet B?","answer_images":[],"timestamp":"2023-05-08 14:07:00","unix_timestamp":1683547620,"question_id":547,"answers_community":["C (100%)"],"discussion":[{"timestamp":"1694286900.0","content":"Selected Answer: C\nVPC Flow Logs capture information about the IP traffic going to and from network interfaces in a VPC. This includes traffic that traverses a VPN connection. VPC Flow Logs can be used to monitor and troubleshoot connectivity issues, including verifying whether traffic is reaching a particular subnet within the VPC.","poster":"Dushank","comment_id":"1003448","upvote_count":"8"},{"poster":"sumanshu","timestamp":"1735290120.0","comment_id":"1332321","upvote_count":"2","content":"Selected Answer: C\nVPC Flow Logs capture detailed information about the traffic flowing to and from network interfaces in a VPC.\n\nD) Eliminated - AWS CloudTrail logs record API-level activity in an AWS account, hey do not provide network-level traffic details for verifying traffic reaching specific subnets."},{"upvote_count":"1","content":"Selected Answer: C\nversion account-id interface-id srcaddr dstaddr srcport dstport protocol packets bytes start end action log-status\n2 123456789012 eni-abc12345 192.168.1.10 10.0.0.20 443 80 6 10 2048 1620050730 1620050790 ACCEPT OK\n2 123456789012 eni-abc12345 192.168.1.10 10.0.0.30 22 443 6 5 1024 1620050730 1620050790 REJECT OK","poster":"albert_kuo","timestamp":"1726879500.0","comment_id":"1287091"},{"upvote_count":"1","timestamp":"1716407040.0","comment_id":"1215976","poster":"65703c1","content":"Selected Answer: C\nC is the correct answer."},{"poster":"KarBiswa","timestamp":"1702907880.0","upvote_count":"1","comment_id":"1099758","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html"},{"upvote_count":"3","comment_id":"897801","content":"Selected Answer: C\nhttps://www.examtopics.com/discussions/amazon/view/28802-exam-aws-certified-developer-associate-topic-1-question-219/","poster":"Prem28","timestamp":"1684086960.0"},{"content":"Selected Answer: C\nhttps://www.examtopics.com/discussions/amazon/view/28802-exam-aws-certified-developer-associate-topic-1-question-219/","poster":"zodraz","timestamp":"1683547620.0","upvote_count":"3","comment_id":"892132"}]},{"id":"9agPI7b08VD4J2sxPXPl","unix_timestamp":1682101980,"discussion":[{"content":"Selected Answer: A\nThis solution will allow the developer to receive notifications for each image uploaded to the S3 bucket, and also create a thumbnail using the Lambda function. The SNS topic will serve as a trigger for both the Lambda function and the email notification subscription. When an image is uploaded, S3 will send a notification to the SNS topic, which will trigger the Lambda function to create the thumbnail and also send an email notification to the specified email address.","comments":[{"upvote_count":"5","poster":"payireb682","content":"Thanks. As mentioned Multiple subscription can be added for SNS","timestamp":"1701963180.0","comment_id":"1090411"},{"timestamp":"1691042520.0","comment_id":"970781","upvote_count":"2","content":"greate !! send email do not need SQS.","poster":"jipark"}],"comment_id":"876759","upvote_count":"17","timestamp":"1682101980.0","poster":"MrTee"},{"comment_id":"1332322","content":"Selected Answer: A\nB) Eliminated - The SQS queue is not required because SNS can directly notify both the Lambda function and the email service\n\nC) Eliminated - SQS cannot send email notifications. Also, The Lambda function would need to poll the SQS queue, adding unnecessary complexity.\n\nD) Eliminated - Using EventBridge to forward S3 event notifications introduces unnecessary complexity.","timestamp":"1735290900.0","upvote_count":"1","poster":"sumanshu"},{"upvote_count":"1","comment_id":"1250316","poster":"Tluszczyk","timestamp":"1721300880.0","content":"None of these is really an optimal solution to the problem, which is a little annoying really"},{"poster":"65703c1","timestamp":"1716407160.0","content":"Selected Answer: A\nA is the correct answer.","upvote_count":"1","comment_id":"1215978"},{"timestamp":"1705219080.0","comment_id":"1122367","upvote_count":"2","poster":"SerialiDr","content":"Selected Answer: A\nSNS can be used to fan out notifications. When an image is uploaded to the S3 bucket, an event notification is sent to the SNS topic. The Lambda function is subscribed to this topic to create a thumbnail, and an email subscription can also be configured on the same SNS topic to send email notifications. This approach meets all requirements with minimal components."}],"question_text":"A developer is creating a service that uses an Amazon S3 bucket for image uploads. The service will use an AWS Lambda function to create a thumbnail of each image. Each time an image is uploaded, the service needs to send an email notification and create the thumbnail. The developer needs to configure the image processing and email notifications setup.\n\nWhich solution will meet these requirements?","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/106946-exam-aws-certified-developer-associate-dva-c02-topic-1/","topic":"1","choices":{"C":"Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure S3 event notifications with a destination of the SQS queue. Subscribe the Lambda function to the SQS queue. Create an email notification subscription to the SQS queue.","A":"Create an Amazon Simple Notification Service (Amazon SNS) topic. Configure S3 event notifications with a destination of the SNS topic. Subscribe the Lambda function to the SNS topic. Create an email notification subscription to the SNS topic.","B":"Create an Amazon Simple Notification Service (Amazon SNS) topic. Configure S3 event notifications with a destination of the SNS topic. Subscribe the Lambda function to the SNS topic. Create an Amazon Simple Queue Service (Amazon SQS) queue. Subscribe the SQS queue to the SNS topic. Create an email notification subscription to the SQS queue.","D":"Create an Amazon Simple Queue Service (Amazon SQS) queue. Send S3 event notifications to Amazon EventBridge. Create an EventBridge rule that runs the Lambda function when images are uploaded to the S3 bucket. Create an EventBridge rule that sends notifications to the SQS queue. Create an email notification subscription to the SQS queue."},"answer_images":[],"answer_description":"","exam_id":24,"question_images":[],"answer":"A","answer_ET":"A","timestamp":"2023-04-21 20:33:00","answers_community":["A (100%)"],"question_id":548},{"id":"6WmyfMhJvfJG2GAJf1C4","question_images":[],"choices":{"B":"Turn on auto scaling for the DynamoDB table. Use Amazon CloudWatch to monitor the table's read and write capacity metrics and to track consumed capacity.","C":"Create an alias for the Lambda function. Configure provisioned concurrency for the application to use.","D":"Refactor the Lambda function into two functions. Configure one function to store the data in the DynamoDB table. Configure the second function to process the data and update the items after the data is stored in DynamoDB. Create a DynamoDB stream to invoke the second function after the data is stored.","A":"Refactor the Lambda function into two functions. Configure one function to transform the data and one function to load the data into the DynamoDB table. Create an Amazon Simple Queue Service (Amazon SQS) queue in between the functions to hold the items as messages and to invoke the second function."},"question_text":"A developer has designed an application to store incoming data as JSON files in Amazon S3 objects. Custom business logic in an AWS Lambda function then transforms the objects, and the Lambda function loads the data into an Amazon DynamoDB table. Recently, the workload has experienced sudden and significant changes in traffic. The flow of data to the DynamoDB table is becoming throttled.\n\nThe developer needs to implement a solution to eliminate the throttling and load the data into the DynamoDB table more consistently.\n\nWhich solution will meet these requirements?","answer":"A","url":"https://www.examtopics.com/discussions/amazon/view/106947-exam-aws-certified-developer-associate-dva-c02-topic-1/","discussion":[{"upvote_count":"26","content":"Selected Answer: A\nA. Refactor the Lambda function into two functions. Configure one function to transform the data and one function to load the data into the DynamoDB table. Create an Amazon Simple Queue Service (Amazon SQS) queue in between the functions to hold the items as messages and to invoke the second function.\n\nBy breaking the Lambda function into two separate functions and using an SQS queue to hold the transformed data as messages, you can decouple the data transformation and loading processes. This allows for more controlled loading of data into the DynamoDB table and helps eliminate throttling issues.","poster":"ihebchorfi","timestamp":"1682759520.0","comment_id":"884186"},{"poster":"MrTee","comment_id":"876780","comments":[{"upvote_count":"2","content":"Read carefully: The flow of data to the DynamoDB table is becoming throttled.\nSo the bottleneck is the DynamoDB, not the lambda function transforming the data. Option D doesn't help because the first function storing data into the DynamoDB will still hit the throttling issue.","poster":"nbxyzd","timestamp":"1730533740.0","comment_id":"1306080"},{"comment_id":"1135584","upvote_count":"1","content":"The problem I have with option D is that it is adding more lad on the DynamoDB table. What is the need to insert the item and then update the item later. This is performing two operation on every item just to get it into the correct state. I would go with option A since it is not performing two operations on the DB and hence reducing the load which will help with throttling.","timestamp":"1706599980.0","poster":"Ashwinvdm22"},{"upvote_count":"5","comment_id":"912869","content":"Sorry but when you say \"the first function can store the data into the DynamoDB table and exit quickly, avoiding any throttling issues\" I dont understand your point","poster":"robotgeek","timestamp":"1685714340.0"},{"timestamp":"1702164060.0","content":"I disagree... the order of the function with this option makes NO sense. I go with A","comment_id":"1092146","upvote_count":"1","poster":"[Removed]"},{"comment_id":"1092148","content":"The issue is between S3 to DynamoDB this is where we need to fix the bottleneck. So configuring two functions to work on the data after it has been uploaded to DynamoDB makes no sense.","poster":"[Removed]","upvote_count":"1","timestamp":"1702164300.0"}],"timestamp":"1682103720.0","upvote_count":"9","content":"Selected Answer: D\nThis solution will allow the developer to store the incoming data into the DynamoDB table more consistently without being throttled. By splitting the Lambda function into two functions, the first function can store the data into the DynamoDB table and exit quickly, avoiding any throttling issues. The second function can then process the data and update the items after the data is stored in DynamoDB using a DynamoDB stream to invoke the second function.\nOption A is also a good option but not the best solution because it introduces additional complexity and cost by using an Amazon SQS queue."},{"timestamp":"1737735840.0","upvote_count":"1","comment_id":"1346184","poster":"mooncake1","content":"Selected Answer: B\nAdjusting the WCU is enough. no need to implement such complex solutions"},{"timestamp":"1735293120.0","poster":"sumanshu","comment_id":"1332337","content":"Selected Answer: A\nThe SQS queue acts as a buffer, which smooths out sudden traffic spikes by queuing up data. The second Lambda function processes data from the queue at a steady rate, reducing the likelihood of throttling in DynamoDB.\n\nB) Eliminated - Auto scaling takes time to adjust, during which throttling may still occur.\n\nC) Eliminated - Provisioned concurrency ensures that the Lambda function can handle a predictable number of concurrent requests, but it does not solve the throttling issue at the DynamoDB layer","upvote_count":"2"},{"content":"Selected Answer: A\nIt's hands-down A.","poster":"nbxyzd","comment_id":"1306082","timestamp":"1730533860.0","upvote_count":"1"},{"upvote_count":"1","comments":[{"upvote_count":"1","timestamp":"1721788500.0","content":"Yes, but it asks to \"load the data into the DynamoDB table more consistently.\" Therefore, option A will prevent unintentional data load into DynamoDB, it's the best option.","comment_id":"1254059","poster":"queekao"}],"comment_id":"1250326","content":"Selected Answer: B\n\"A\" would be optimal, but without backoff algorithm the lambda division and SQS won't affect the throttling. However Dynamo can autoscale\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html","poster":"Tluszczyk","timestamp":"1721301720.0"},{"content":"Selected Answer: A\nA is the correct answer.","comment_id":"1215984","timestamp":"1716407580.0","poster":"65703c1","upvote_count":"1"},{"timestamp":"1710505320.0","comment_id":"1174256","poster":"nder","content":"Selected Answer: B\nwe are trying to stop throttling...","upvote_count":"1"},{"timestamp":"1709066760.0","upvote_count":"2","comment_id":"1160976","content":"Selected Answer: A\nThis solution addresses the need to eliminate throttling and ensure consistent data loading into the Amazon DynamoDB table by separating the transformation and loading processes into two different functions. Using an Amazon SQS queue to hold items as messages between the two functions helps manage the flow of data and prevents overloading the DynamoDB table, thereby eliminating throttling issues.","poster":"SerialiDr"},{"comment_id":"1139797","timestamp":"1707022560.0","poster":"Brisun","content":"Selected Answer: A\nA is correct as it requires to write to DynamoDB \"more consistently\". Option B can solve the problem too but the writing won't be consistent as the traffic will go up and down instantly.\n\nIn reality, I will probably do Option B only.","upvote_count":"3"},{"timestamp":"1706960280.0","content":"Selected Answer: B\nI do not feel refactoring the data transformation and loading would help here as I do not think the number of concurrent calls to the DB would decrease because of this. Autoscaling DynamoDB would seem a more potent option to me.","comment_id":"1139190","upvote_count":"4","poster":"SD_CS"},{"poster":"peekingpicker","content":"Selected Answer: B\nWhy not B ? \nDynamoDB can autoscale the RCU and WCU","timestamp":"1705750980.0","comment_id":"1127242","upvote_count":"4"},{"poster":"SerialiDr","comment_id":"1122484","content":"Selected Answer: A\nA. Refactor the Lambda function into two functions, using an Amazon SQS queue to manage the data flow, and/or\n\nB. Turn on auto scaling for the DynamoDB table to automatically adjust its write capacity based on traffic patterns.\n\nBoth A and B address the core issue of managing write throughput to the DynamoDB table to prevent throttling. Option A provides a way to smooth out data flow and manage write requests more effectively, while option B allows the table to scale its capacity automatically in response to changing traffic, although with potential limitations in response speed to sudden traffic spikes. Combining these approaches could provide an even more robust solution.","timestamp":"1705231560.0","upvote_count":"3"},{"content":"Selected Answer: A\nOff course A & D are options but here after inserting the data further we cannot modify because one extra writing cost will incur rather using queue lambda can poll the transformed data","comment_id":"1099777","upvote_count":"3","timestamp":"1702909320.0","poster":"KarBiswa"},{"comment_id":"1046213","poster":"Nagasoracle","timestamp":"1697561400.0","content":"Selected Answer: A\nAnswer : A\nSQS can be configured to invoke Lambda.\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-configure-lambda-function-trigger.html","upvote_count":"4"},{"upvote_count":"1","content":"Selected Answer: B\nI think B","timestamp":"1697547900.0","comment_id":"1046023","poster":"dexdinh91"},{"upvote_count":"2","timestamp":"1696934940.0","content":"Lambda functions can be triggered by SQS: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-configure-lambda-function-trigger.html","comment_id":"1039394","poster":"jingle4944","comments":[{"timestamp":"1730533500.0","upvote_count":"1","poster":"nbxyzd","comment_id":"1306077","content":"Correct, and most importantly, it's triggered *synchronously* so that there won't be throttling issue. Quote:\nYou can use an AWS Lambda function to process messages in an Amazon SQS queue. Lambda polls the queue and invokes your Lambda function synchronously with an event that contains queue messages."}]},{"comment_id":"1015949","content":"Selected Answer: B\nI don't believe that option A is correct because an Amazon SQS queue wouldn't invoke a Lambda function; in any case, the Lambda function would be configured to retrieve messages from the SQS queue. For that reason, I believe option B would be the correct choice in this case.","timestamp":"1695569040.0","upvote_count":"1","poster":"Balliache520505"},{"timestamp":"1694287200.0","upvote_count":"3","poster":"Dushank","content":"Selected Answer: A\nRefactoring the Lambda function into two functions and introducing an Amazon Simple Queue Service (Amazon SQS) queue between them would provide a buffering mechanism. The first Lambda function would transform the data and push it to the SQS queue. The second Lambda function would be triggered by messages in the SQS queue to write the data into DynamoDB. This decouples the two operations and allows for more controlled and consistent data loading into DynamoDB, helping to avoid throttling.","comment_id":"1003451"},{"timestamp":"1691043240.0","comment_id":"970793","poster":"jipark","upvote_count":"3","content":"Selected Answer: A\nthe requirement is Lambda function load data to DynamoDB.\nD is incorrect : \"DynamoDB stream invoke Lambda\" - the order is reversed."},{"poster":"baboopan18","upvote_count":"3","content":"Selected Answer: B\nThe key point is \"eliminate the throttling\"\nI prefer B than A","timestamp":"1689946500.0","comment_id":"958556"},{"comment_id":"943140","comments":[{"comment_id":"952730","upvote_count":"4","content":"Lambda functions can be triggered by messages in a SQS queue.","timestamp":"1689459840.0","poster":"tttamtttam"}],"timestamp":"1688501160.0","poster":"qwan","upvote_count":"1","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-architecture.html\nThis is the lifecycle for a SQS message. \nFor my understanding, option A is wrong. SQS cannot invoke function, like is it stated there. \nSo D it's the right answer."},{"timestamp":"1688493960.0","comment_id":"943062","content":"Selected Answer: B\nThe developer needs to implement a solution to eliminate the throttling and load the data into the DynamoDB table more consistently. The problem is in DynamoDB does not associate with the lambada. Then the better solution is to auto scale the table of the DynamoDB.","upvote_count":"7","poster":"eberhe900"},{"upvote_count":"3","content":"Selected Answer: A\nSQS between Lambda function should deliver the traffic more consistently.","timestamp":"1688464260.0","poster":"Phongsanth","comment_id":"942585"},{"timestamp":"1687195680.0","poster":"gagol14","upvote_count":"3","content":"Selected Answer: A\nThis solution will not meet the requirements because it will not load the data into the DynamoDB table more consistently. By using a DynamoDB stream, you can trigger a Lambda function to process data changes in a DynamoDB table. However, this does not guarantee that all data changes will be processed in order, or that no duplicates will occur. Therefore, this solution may result in inconsistent or incorrect data in your DynamoDB table.\n\nThe best solution is A, because it will eliminate the throttling and load the data into the DynamoDB table more consistently.","comment_id":"927758"},{"comment_id":"912856","poster":"mgonblan","upvote_count":"1","timestamp":"1685712780.0","content":"I vote B, because refactoring the lambdas (A or D) could help, but it doesn't help the DynamoDB tables, C would give reserved concurrency to the lambda and improves performance, but it doesn't help with DynamoDB layer.\nSo the best option is B) Because you stablish autoscaling and configure cloudwatch to monitor which RCU and WCU must use for the table."},{"content":"Selected Answer: D\nOption D.","upvote_count":"1","poster":"FunkyFresco","timestamp":"1685203980.0","comment_id":"908096"},{"timestamp":"1684410540.0","upvote_count":"1","content":"Selected Answer: D\nD is true","poster":"loctong","comment_id":"901166"},{"upvote_count":"1","comment_id":"901158","content":"Selected Answer: D\nTo eliminate throttling and load the data into the DynamoDB table more consistently, you can refactor the Lambda function into two functions and utilize DynamoDB streams.","timestamp":"1684410300.0","poster":"loctong"},{"poster":"Prem28","timestamp":"1684087380.0","comment_id":"897807","upvote_count":"3","content":"Selected Answer: A\nOption A suggests refactoring the Lambda function into two functions, one to transform the data and the other to load the data into the DynamoDB table, and adding an Amazon SQS queue in between the functions to hold the items as messages and to invoke the second function. This approach separates the processing and loading stages, allowing the Lambda functions to scale independently and reducing the chance of throttling."},{"comment_id":"897538","poster":"rlnd2000","upvote_count":"3","content":"Selected Answer: B\nB => I think the problem here is the \"...sudden and significant changes in traffic...\" the current write capacity in DynamoDB is not enough to insert all incoming data, Decoupling is good practice but won't solve the problem in this case because the question doesn't mention that the increase in the traffic is temporary so any solution without increasing the write capacity will create a bottleneck.","timestamp":"1684069440.0","comments":[{"content":"Sorry I change to A, the problem is not in DynamoDB, => ...The flow of data to the DynamoDB table is becoming throttled..., is the flow of data, I go with A.","timestamp":"1684148940.0","upvote_count":"1","poster":"rlnd2000","comment_id":"898204"},{"poster":"mgonblan","comment_id":"910302","content":"I'm also with B. because autoscaling DynamoDB tables and monitoring the reads we can fix the RCU of the DyynamoDB Database. \nRefactoring the Lambdas would not help very much.","upvote_count":"1","timestamp":"1685455440.0"}]}],"answer_description":"","answers_community":["A (61%)","B (26%)","13%"],"topic":"1","answer_images":[],"answer_ET":"A","timestamp":"2023-04-21 21:02:00","exam_id":24,"question_id":549,"isMC":true,"unix_timestamp":1682103720},{"id":"EkktdSfB5bmltN40DWyS","question_images":[],"answer":"A","timestamp":"2023-05-18 13:50:00","answer_ET":"A","discussion":[{"timestamp":"1694287380.0","upvote_count":"9","content":"Selected Answer: A\nThe requirement is to have a shared file system that allows for appending to files and can be accessed by multiple Lambda functions, AWS services, and on-premises resources. Amazon Elastic File System (Amazon EFS) is a good fit for these requirements. EFS provides a scalable and elastic NFS file system which can be mounted to multiple EC2 instances and Lambda functions at the same time, making it easier for these resources to share files. You can also append to existing files on an EFS file system, which meets the requirement for a shared log file that can have new entries appended to it.","poster":"Dushank","comment_id":"1003455"},{"upvote_count":"1","comment_id":"1332351","timestamp":"1735294740.0","content":"Selected Answer: A\nB) Eliminated - Amazon Elastic Block Store (EBS) volumes are designed for single-instance use or Multi-Attach for specific EC2 instances, not for Lambda functions or serverless environments.\n\nC) Eliminated - The /tmp directory in Lambda is ephemeral storage that exists only for the duration of the Lambda invocation. It is not shared between functions or persistent across invocations\n\nD) Eliminated - The /opt directory in Lambda is used for deploying custom runtimes or external libraries. It is read-only during execution, so it cannot be used to store or update files.","poster":"sumanshu"},{"timestamp":"1726065240.0","upvote_count":"1","comment_id":"1282183","content":"Selected Answer: A\nElastic File System file shareing","poster":"Saudis"},{"upvote_count":"1","poster":"65703c1","content":"Selected Answer: A\nA is the correct answer.","comment_id":"1215989","timestamp":"1716407820.0"},{"comment_id":"912840","timestamp":"1685711520.0","upvote_count":"1","content":"A) There are several references for this:\nhttps://docs.aws.amazon.com/lambda/latest/operatorguide/networking-vpc.html and\nthis blog entry:\nhttps://aws.amazon.com/es/blogs/compute/choosing-between-aws-lambda-data-storage-options-in-web-apps/","poster":"mgonblan"},{"content":"Selected Answer: A\nshared files == EFS","poster":"delak","timestamp":"1684714680.0","comment_id":"903591","upvote_count":"3"},{"upvote_count":"2","content":"Selected Answer: A\nEFS is true","poster":"loctong","timestamp":"1684410600.0","comment_id":"901167"}],"answer_images":[],"answers_community":["A (100%)"],"isMC":true,"topic":"1","question_text":"A developer is creating an AWS Lambda function in VPC mode. An Amazon S3 event will invoke the Lambda function when an object is uploaded into an S3 bucket. The Lambda function will process the object and produce some analytic results that will be recorded into a file. Each processed object will also generate a log entry that will be recorded into a file.\n\nOther Lambda functions, AWS services, and on-premises resources must have access to the result files and log file. Each log entry must also be appended to the same shared log file. The developer needs a solution that can share files and append results into an existing file.\n\nWhich solution should the developer use to meet these requirements?","url":"https://www.examtopics.com/discussions/amazon/view/109646-exam-aws-certified-developer-associate-dva-c02-topic-1/","choices":{"D":"Create a reference to the /opt storage directory. Store the result files and log file by using the directory reference. Append the log entry to the log file.","A":"Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system in Lambda. Store the result files and log file in the mount point. Append the log entries to the log file.","B":"Create an Amazon Elastic Block Store (Amazon EBS) Multi-Attach enabled volume. Attach the EBS volume to all Lambda functions. Update the Lambda function code to download the log file, append the log entries, and upload the modified log file to Amazon EBS.","C":"Create a reference to the /tmp local directory. Store the result files and log file by using the directory reference. Append the log entry to the log file."},"question_id":550,"exam_id":24,"answer_description":"","unix_timestamp":1684410600}],"exam":{"id":24,"provider":"Amazon","isBeta":false,"isImplemented":true,"numberOfQuestions":551,"name":"AWS Certified Developer - Associate DVA-C02","isMCOnly":true,"lastUpdated":"11 Apr 2025"},"currentPage":110},"__N_SSP":true}