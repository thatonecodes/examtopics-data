{"pageProps":{"questions":[{"id":"YkpgjDdoXznkkQOIjOgF","discussion":[{"timestamp":"1707313860.0","poster":"kempes","content":"Selected Answer: B\nSNS is designed for precisely this kind of use case. It allows you to publish messages to a topic, which can then be delivered to multiple subscribers. Partners can subscribe to the SNS topic using an HTTP endpoint as the protocol, which meets the requirement to notify partners via an HTTP endpoint. This approach is highly scalable and requires the least implementation effort because it leverages managed services without the need for custom logic to manage subscriptions or deliver notifications.","comment_id":"1143365","upvote_count":"12"},{"upvote_count":"8","poster":"hajra313","comment_id":"1142807","timestamp":"1707259200.0","content":"Option A involves creating an Amazon Timestream database to store affiliated partners and implementing an AWS Lambda function to read the list and send user IDs to each partner. While this approach can work, it involves more implementation effort than the Amazon SNS solution. It requires setting up and managing a database, as well as configuring the Lambda function to send notifications to partners. The Amazon SNS solution provides a simpler and more scalable approach for rapidly adding partners and notifying them when users receive points. so answer is B"},{"upvote_count":"1","poster":"Scheldon","comment_id":"1230029","timestamp":"1718302620.0","content":"Selected Answer: B\nAnswerB\n\nSending Notification to multiple subscribers = SNS\n\nAmazon Simple Notification Service (Amazon SNS) is a managed service that provides message delivery from publishers to subscribers (also known as producers and consumers). Publishers communicate asynchronously with subscribers by sending messages to a topic, which is a logical access point and communication channel. Clients can subscribe to the SNS topic and receive published messages using a supported endpoint type, such as Amazon Data Firehose, Amazon SQS, AWS Lambda, HTTP, email, mobile push notifications, and mobile text messages (SMS).\nhttps://docs.aws.amazon.com/sns/latest/dg/welcome.html"},{"content":"Selected Answer: B\nThis is a perfect SNS use case","comment_id":"1153708","poster":"NayeraB","timestamp":"1708317600.0","upvote_count":"2"},{"comment_id":"1145921","poster":"jjcode","content":"The answer is B, create an SNS topic one subscriptions you can make is HTTP, This completely addresses the question objective.","upvote_count":"2","timestamp":"1707539760.0"},{"comment_id":"1141922","comments":[{"upvote_count":"1","timestamp":"1738386060.0","comment_id":"1349752","poster":"FlyingHawk","content":"Timestream is optimized for time-series data, not for managing partner subscriptions."}],"content":"Option A","timestamp":"1707207360.0","upvote_count":"1","poster":"Andy_09"}],"timestamp":"2024-02-06 09:16:00","answer_description":"","answer":"B","answer_images":[],"question_text":"A social media company is creating a rewards program website for its users. The company gives users points when users create and upload videos to the website. Users redeem their points for gifts or discounts from the company's affiliated partners. A unique ID identifies users. The partners refer to this ID to verify user eligibility for rewards.\n\nThe partners want to receive notification of user IDs through an HTTP endpoint when the company gives users points. Hundreds of vendors are interested in becoming affiliated partners every day. The company wants to design an architecture that gives the website the ability to add partners rapidly in a scalable way.\n\nWhich solution will meet these requirements with the LEAST implementation effort?","question_images":[],"unix_timestamp":1707207360,"topic":"1","exam_id":31,"answer_ET":"B","choices":{"B":"Create an Amazon Simple Notification Service (Amazon SNS) topic. Choose an endpoint protocol. Subscribe the partners to the topic. Publish user IDs to the topic when the company gives users points.","A":"Create an Amazon Timestream database to keep a list of affiliated partners. Implement an AWS Lambda function to read the list. Configure the Lambda function to send user IDs to each partner when the company gives users points.","C":"Create an AWS Step Functions state machine. Create a task for every affiliated partner. Invoke the state machine with user IDs as input when the company gives users points.","D":"Create a data stream in Amazon Kinesis Data Streams. Implement producer and consumer applications. Store a list of affiliated partners in the data stream. Send user IDs when the company gives users points."},"isMC":true,"answers_community":["B (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/133037-exam-aws-certified-solutions-architect-associate-saa-c03/","question_id":796},{"id":"oUOg6tUdwb6In3Xo4On3","question_id":797,"topic":"1","answer":"A","timestamp":"2024-03-05 22:24:00","question_images":[],"answer_ET":"A","isMC":true,"choices":{"B":"Use an Amazon EventBridge rule to invoke an AWS Lambda function when PutObject requests occur. Program the Lambda function to analyze the object by using Amazon Forecast to extract the ingredient names. Store the Forecast output in the DynamoDB table.","D":"Use an Amazon EventBridge rule to invoke an AWS Lambda function when a PutObject request occurs. Program the Lambda function to analyze the object and extract the ingredient names by using Amazon SageMaker. Store the inference output from the SageMaker endpoint in the DynamoDB table.","C":"Use S3 Event Notifications to invoke an AWS Lambda function when PutObject requests occur. Use Amazon Polly to create audio recordings of the recipe records. Save the audio files in the S3 bucket. Use Amazon Simple Notification Service (Amazon SNS) to send a URL as a message to employees. Instruct the employees to listen to the audio files and calculate the nutrition score. Store the ingredient names in the DynamoDB table.","A":"Use S3 Event Notifications to invoke an AWS Lambda function when PutObject requests occur. Program the Lambda function to analyze the object and extract the ingredient names by using Amazon Comprehend. Store the Amazon Comprehend output in the DynamoDB table."},"answer_images":[],"answers_community":["A (100%)"],"question_text":"A company needs to extract the names of ingredients from recipe records that are stored as text files in an Amazon S3 bucket. A web application will use the ingredient names to query an Amazon DynamoDB table and determine a nutrition score.\n\nThe application can handle non-food records and errors. The company does not have any employees who have machine learning knowledge to develop this solution.\n\nWhich solution will meet these requirements MOST cost-effectively?","discussion":[{"content":"Selected Answer: A\nA correct","poster":"seetpt","upvote_count":"5","comment_id":"1169054","timestamp":"1709926440.0"},{"timestamp":"1739197560.0","content":"Selected Answer: A\nI’m fascinated by how self-descriptive the names of AWS ML tools are.","poster":"Besisco","upvote_count":"1","comment_id":"1354495"},{"upvote_count":"2","timestamp":"1731528780.0","comment_id":"1311619","poster":"Danilus","content":"Selected Answer: A\nKey: Most cost-effective solution\nKey: Extract the names of ingredients from recipe records\nKey: The company does not have any employees with machine learning knowledge\n\n\n\nNot B because Forecast is used for making predictions in time series, not for extracting text or ingredients.\nNot C because it would be too expensive and involve too much operational overhead.\nNot D because SageMaker is designed for creating custom machine learning models, and the company lacks employees with machine learning knowledge.\n\nConclusion: The answer is A because Comprehend is used for natural language processing (NLP), which in this case can extract ingredient names from the recipes effectively.\n\nso the answer is A because comprehend is used for nlp (natular languages processing) which is in this case use for to extract the names of the recipes"},{"upvote_count":"3","content":"Selected Answer: A\nAnswerA\n\nAmazon Comprehend uses natural language processing (NLP) to extract insights about the content of documents. It develops insights by recognizing the entities, key phrases, language, sentiments, and other common elements in a document. Use Amazon Comprehend to create new products based on understanding the structure of documents. For example, using Amazon Comprehend you can search social networking feeds for mentions of products or scan an entire document repository for key phrases.\nhttps://docs.aws.amazon.com/comprehend/latest/dg/what-is.html","poster":"Scheldon","comment_id":"1230031","timestamp":"1718302980.0"},{"poster":"fae3297","comment_id":"1225110","content":"Selected Answer: A\nA seems right\n\nB. Forecast is time-series\nC. Using Polly to create audio recordings just to make your employees listen to them seems inefficient to say the least\nD. Question asks for no ML. SageMaker = ML","timestamp":"1717639980.0","upvote_count":"3"},{"upvote_count":"3","poster":"seetpt","timestamp":"1709888520.0","comment_id":"1168671","content":"Selected Answer: A\nA is correct"},{"timestamp":"1709673840.0","upvote_count":"3","poster":"asdfcdsxdfc","comment_id":"1166764","content":"shouldn't it be A?"}],"unix_timestamp":1709673840,"url":"https://www.examtopics.com/discussions/amazon/view/135257-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_description":"","exam_id":31},{"id":"lGNO4d9veRfi4KGJjf4J","question_text":"A company is migrating a distributed application to AWS. The application serves variable workloads. The legacy platform consists of a primary server that coordinates jobs across multiple compute nodes. The company wants to modernize the application with a solution that maximizes resiliency and scalability.\nHow should a solutions architect design the architecture to meet these requirements?","topic":"1","answer":"B","discussion":[{"upvote_count":"87","timestamp":"1665233520.0","poster":"rein_chau","comment_id":"689313","content":"Selected Answer: B\nA - incorrect: Schedule scaling policy doesn't make sense.\nC, D - incorrect: Primary server should not be in same Auto Scaling group with compute nodes.\nB is correct."},{"content":"Selected Answer: B\nThe answer seems to be B for me:\nA: doesn't make sense to schedule auto-scaling\nC: Not sure how CloudTrail would be helpful in this case, at all.\nD: EventBridge is not really used for this purpose, wouldn't be very reliable","timestamp":"1665216420.0","upvote_count":"23","poster":"Sinaneos","comment_id":"689123"},{"upvote_count":"1","poster":"alfteezy91","timestamp":"1742479260.0","content":"Selected Answer: B\nDecouple with Amazon SQS and EC2 instances can be managed in an auto scaling group for scalability\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/modernization-mainframe-decoupling-patterns/queue.html","comment_id":"1401063"},{"content":"Selected Answer: D\nB doesn't make sense to me because you're scaling based on the queue size and not the requirements of the varying workloads. Eventbridge can act as a trigger for workloads, so it might just cut, although it wouldn't be my first choice.","upvote_count":"1","poster":"VitMoreira","comment_id":"1395629","timestamp":"1741964220.0"},{"timestamp":"1735140420.0","poster":"MGKYAING","comment_id":"1331617","upvote_count":"2","content":"Selected Answer: B\n1.Amazon SQS for Decoupling:\nAmazon SQS provides a fully managed message queue to decouple the primary server from the compute nodes. Jobs are sent to the queue, and compute nodes process them independently.\nThis architecture eliminates a single point of failure (the primary server) and increases resilience.\n\n2.Auto Scaling Group for Compute Nodes:\nEC2 instances managed in an Auto Scaling group process the jobs in the SQS queue. Auto Scaling dynamically adjusts the number of instances based on the queue size, ensuring scalability to handle variable workloads.\n\n3.Scalability Based on Queue Size:\nScaling based on the size of the SQS queue ensures that the system adjusts to workload demands efficiently. When the queue grows, more instances are launched; when the queue shrinks, instances are terminated."},{"comment_id":"860748","poster":"PhucVuu","content":"Selected Answer: B\nkeywords:\n- Legacy platform consists of a primary server that coordinates jobs across multiple compute nodes.\n- Maximizes resiliency and scalability.\n\nA: Incorrect - the question don't mention about schedule for high workload. So we don't use scheduled scaling for this case.\nB: Correct - SQS can keep your message in the queue in case of high workload and if it too high we can increase the EC2 instance base on size of the queue.\nC: Incorrect - AWS CloudTrail is API logs it is use for audit log of AWS user activity.\nD: Incorrect - Event Bridge is use for filter event and trigger event.","timestamp":"1726903980.0","upvote_count":"14"},{"poster":"gx2222","comment_id":"859063","upvote_count":"5","content":"Selected Answer: B\nB. \nExplanation:\nTo maximize resiliency and scalability, the best solution is to use an Amazon SQS queue as a destination for the jobs. This decouples the primary server from the compute nodes, allowing them to scale independently. This also helps to prevent job loss in the event of a failure.\n\nUsing an Auto Scaling group of Amazon EC2 instances for the compute nodes allows for automatic scaling based on the workload. In this case, it's recommended to configure the Auto Scaling group based on the size of the Amazon SQS queue, which is a better indicator of the actual workload than the load on the primary server or compute nodes. This approach ensures that the application can handle variable workloads, while also minimizing costs by automatically scaling up or down the compute nodes as needed.","timestamp":"1726903980.0"},{"timestamp":"1726903980.0","upvote_count":"3","content":"Selected Answer: B\nSQS helps to process messages in case of variable workloads. The compute nodes must be implemented using EC2 instances (or alternatively, ECS tasks or managed Kubernetes nodes, but this option is not available). AutoScaling must be based on the workload, which is controlled by the queue. So, the correct option is B.\nA is not correct because the instances should not scale based on a schedule which is not deterministic. On the contrary, scales based on the workload (queue size) is more effective. \nAWS CloudTrail should not be used as a destination job and it is not related to the question. The same applies to EventBridge.","poster":"Andreshere","comment_id":"1116854"},{"comments":[{"content":"According to them , Option C is correct how?","comment_id":"1248894","timestamp":"1721133720.0","poster":"abdulghaffar","upvote_count":"1"}],"timestamp":"1726903920.0","comment_id":"1246741","content":"B:\n\nExplanation:\nAmazon SQS provides a reliable, highly scalable, and fully managed message queuing service that enables you to decouple and coordinate the components of a distributed application.\nEC2 Auto Scaling allows you to automatically adjust the number of EC2 instances based on demand, ensuring that your application can handle variable workloads efficiently.\nAuto Scaling based on the size of the queue ensures that your application scales out when there are many jobs to process and scales in when the job load decreases, providing cost efficiency and responsiveness to workload changes.","upvote_count":"2","poster":"WMF0187"},{"comment_id":"1263424","poster":"PaulGa","content":"A lot of answer B's... \nbut I'm not convinced its Ans B which states: “Configure EC2 Auto Scaling based on the size of the queue” – because basing scaling on the size of the queue ignores the specific workload each job requires. The problem states “The application serves variable workloads” – you can’t determine the processing required for a variable workload based solely on queue size; this can only be done when you scope the size of the specific variable load – and that to my mind points to answer D: “Configure EC2 Auto Scaling based on the load on the compute nodes” – but then I run into the (potential) problem that Eventbridge may not be up to the task…","timestamp":"1726903920.0","upvote_count":"2"},{"upvote_count":"2","content":"why are almost all of the \"correct answers\" I see on this site all wrong? how the fuck is this an educational resource? good thing the community voting system exists or else this side would be pure unadulterated putrid shit.","poster":"Zwein","timestamp":"1726030140.0","comment_id":"1281939"},{"upvote_count":"1","timestamp":"1724746860.0","comment_id":"1273291","poster":"LeonDong","content":"Selected Answer: B\nB for sure"},{"timestamp":"1717566960.0","content":"Selected Answer: B\nB for sure","poster":"ChymKuBoy","upvote_count":"1","comment_id":"1224542"},{"content":"Selected Answer: D\noption D leverages serverless services (EventBridge) and Auto Scaling for a modern, scalable, and resilient architecture suitable for the distributed application with varying workloads.","poster":"OBIOHAnze","upvote_count":"2","comment_id":"1216097","timestamp":"1716422940.0"},{"upvote_count":"4","timestamp":"1716313320.0","content":"Selected Answer: C\nIn option C, Ignore the line that's talking about cloud trail and then the answer would make much more sense.","comment_id":"1215140","poster":"lixep"},{"comment_id":"1198855","timestamp":"1713567900.0","poster":"lofzee","upvote_count":"1","content":"Selected Answer: B\nB for sure"},{"content":"B is correct because we need auto scaling, and a value to scaling","comment_id":"1182934","upvote_count":"1","poster":"TruthWS","timestamp":"1711420320.0"}],"answers_community":["B (92%)","4%"],"answer_ET":"B","choices":{"C":"Implement the primary server and the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure AWS CloudTrail as a destination for the jobs. Configure EC2 Auto Scaling based on the load on the primary server.","A":"Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the jobs. Implement the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure EC2 Auto Scaling to use scheduled scaling.","D":"Implement the primary server and the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure Amazon EventBridge (Amazon CloudWatch Events) as a destination for the jobs. Configure EC2 Auto Scaling based on the load on the compute nodes.","B":"Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the jobs. Implement the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure EC2 Auto Scaling based on the size of the queue."},"question_id":798,"answer_images":[],"answer_description":"","exam_id":31,"isMC":true,"question_images":[],"timestamp":"2022-10-08 10:07:00","unix_timestamp":1665216420,"url":"https://www.examtopics.com/discussions/amazon/view/84679-exam-aws-certified-solutions-architect-associate-saa-c03/"},{"id":"xTawmjjgOoKIihMYVtJW","answer_description":"","answer_ET":"B","question_text":"A company recently signed a contract with an AWS Managed Service Provider (MSP) Partner for help with an application migration initiative. A solutions architect needs ta share an Amazon Machine Image (AMI) from an existing AWS account with the MSP Partner's AWS account. The AMI is backed by Amazon Elastic Block Store (Amazon EBS) and uses an AWS Key Management Service (AWS KMS) customer managed key to encrypt EBS volume snapshots.\nWhat is the MOST secure way for the solutions architect to share the AMI with the MSP Partner's AWS account?","question_id":799,"exam_id":31,"isMC":true,"answers_community":["B (91%)","6%"],"question_images":[],"unix_timestamp":1665941160,"choices":{"C":"Modify the launchPermission property of the AMI. Share the AMI with the MSP Partner's AWS account only. Modify the key policy to trust a new KMS key that is owned by the MSP Partner for encryption.","A":"Make the encrypted AMI and snapshots publicly available. Modify the key policy to allow the MSP Partner's AWS account to use the key.","D":"Export the AMI from the source account to an Amazon S3 bucket in the MSP Partner's AWS account, Encrypt the S3 bucket with a new KMS key that is owned by the MSP Partner. Copy and launch the AMI in the MSP Partner's AWS account.","B":"Modify the launchPermission property of the AMI. Share the AMI with the MSP Partner's AWS account only. Modify the key policy to allow the MSP Partner's AWS account to use the key."},"answer":"B","url":"https://www.examtopics.com/discussions/amazon/view/85606-exam-aws-certified-solutions-architect-associate-saa-c03/","discussion":[{"timestamp":"1666106160.0","content":"Selected Answer: B\nShare the existing KMS key with the MSP external account because it has already been used to encrypt the AMI snapshot.\n\nhttps://docs.aws.amazon.com/kms/latest/developerguide/key-policy-modifying-external-accounts.html","comment_id":"698313","poster":"Sauran","upvote_count":"20"},{"timestamp":"1671592380.0","comments":[{"comments":[{"poster":"Gizmo2022","timestamp":"1732727100.0","content":"Thank u so much for explaining this Buruguduy","upvote_count":"3","comment_id":"1318813"}],"poster":"Buruguduystunstugudunstuy","upvote_count":"17","comment_id":"751826","timestamp":"1671592380.0","content":"Option A, making the AMI and snapshots publicly available, is not a secure option as it would allow anyone with access to the AMI to use it. \n\nOption C, modifying the key policy to trust a new KMS key owned by the MSP Partner, is also not a secure option as it would involve sharing the key with the MSP Partner, which could potentially compromise the security of the data encrypted with the key. \n\nOption D, exporting the AMI to an S3 bucket in the MSP Partner's AWS account and encrypting the S3 bucket with a new KMS key owned by the MSP Partner, is also not the most secure option as it involves sharing the AMI and a new key with the MSP Partner, which could potentially compromise the security of the data."}],"poster":"Buruguduystunstugudunstuy","upvote_count":"15","content":"Selected Answer: B\n***CORRECT***\nB. Modify the launchPermission property of the AMI. \n\nThe most secure way for the solutions architect to share the AMI with the MSP Partner's AWS account would be to modify the launchPermission property of the AMI and share it with the MSP Partner's AWS account only. The key policy should also be modified to allow the MSP Partner's AWS account to use the key. This ensures that the AMI is only shared with the MSP Partner and is encrypted with a key that they are authorized to use.","comment_id":"751825"},{"timestamp":"1742574780.0","content":"Selected Answer: C\nCan anyone explain to me? I am not familiar with the terminology\nI will go for C, because B mention sharing the key to partner, which might expose other resource which use the same key for encryption, or is this scenario not possible to happen (i.e. key will not be reused across resources)?\nThanks","poster":"jerryl","upvote_count":"1","comment_id":"1401635"},{"content":"Selected Answer: B\nI got this wrong! I thought C is right but it turns out that is not secure. \nIn this question, or any question with encryption, one must consider which of the available options is the most secure way, and it turns out that in this case it is B.","poster":"Dharmarajan","comment_id":"1348215","upvote_count":"1","timestamp":"1738102020.0"},{"comment_id":"1343291","timestamp":"1737338280.0","poster":"FlyingHawk","upvote_count":"1","content":"Selected Answer: B\nI got this wrong, select D initially, but after read the comments, I agree B is the right solution.\n 1. https://docs.aws.amazon.com/toolkit-for-visual-studio/latest/user-guide/tkv-set-ami-launch-perms.html\n2. https://docs.aws.amazon.com/toolkit-for-visual-studio/latest/user-guide/tkv-set-ami-launch-perms.html"},{"upvote_count":"1","timestamp":"1735918800.0","content":"Selected Answer: B\nUpdating the key policy is the most secure way to get access here.","poster":"satyaammm","comment_id":"1336069"},{"upvote_count":"2","comment_id":"1284076","poster":"PaulGa","timestamp":"1726399080.0","content":"Selected Answer: B\nAns B - keep the control simple by only allowing MSP Partner access to the key"},{"timestamp":"1705256520.0","comment_id":"1122753","content":"Selected Answer: B\nAD are unsecure.\nI was confused between B and C but read the article (link below). You have to allow the other account to use your key somehow otherwise they won't be able to use your AMI. C just allows a trust relationship with MSP's KMS, it won't give them access to your key.\nhttps://aws.amazon.com/blogs/security/how-to-share-encrypted-amis-across-accounts-to-launch-encrypted-ec2-instances/","poster":"awsgeek75","upvote_count":"4"},{"content":"Selected Answer: B\nwhen you export AMI to s3 bucket it remains encrypted, so partner couldn't launch ec2 instance","poster":"xdkonorek2","upvote_count":"3","timestamp":"1699018920.0","comment_id":"1061468"},{"timestamp":"1698464640.0","poster":"Ruffyit","comment_id":"1055963","upvote_count":"2","content":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sharingamis-explicit.html"},{"upvote_count":"2","content":"Selected Answer: B\nShare the AMI and Key with the MSP Partner's AWS account only","timestamp":"1692076380.0","poster":"TariqKipkemei","comment_id":"981304"},{"upvote_count":"2","poster":"tamefi5512","comment_id":"940054","content":"Selected Answer: B\nB - is the Answer\nhttps://docs.aws.amazon.com/kms/latest/developerguide/key-policy-modifying-external-accounts.html","timestamp":"1688221500.0"},{"comment_id":"929739","timestamp":"1687366020.0","upvote_count":"3","poster":"cookieMr","content":"Selected Answer: B\nBy modifying the launchPermission property of the AMI and sharing it with the MSP Partner's account only, the solutions architect restricts access to the AMI and ensures that it is not publicly available.\n\nAdditionally, modifying the key policy to allow the MSP Partner's account to use KMS customer managed key used for encrypting the EBS snapshots ensures that the MSP Partner has the necessary permissions to access and use the key for decryption."},{"timestamp":"1684565220.0","poster":"Abrar2022","upvote_count":"3","comment_id":"902395","content":"CORRECTION to my last comment Option B is correct not A.\n\nExplanation why..\nMaking the AMI and snapshots publicly available, is not a secure option as it would allow anyone with access to the AMI to use it. Best practice would be to share the AMI with the MSP Partner's AWS account then Modify launchPermission property of the AMI. This ensures that the AMI is shared only with the MSP Partner and is encrypted with a key that they are authorised to use."},{"comment_id":"902394","upvote_count":"1","content":"Selected Answer: A\nOption A, making the AMI and snapshots publicly available, is not a secure option as it would allow anyone with access to the AMI to use it. Best practice would be to share the AMI with the MSP Partner's AWS account then Modify launchPermission property of the AMI. This ensures that the AMI is shared only with the MSP Partner and is encrypted with a key that they are authorised to use.","poster":"Abrar2022","timestamp":"1684565100.0"},{"timestamp":"1680021720.0","content":"Selected Answer: D\nOption D","comment_id":"853446","poster":"draum010","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: B\nOption B","timestamp":"1671404400.0","comment_id":"749274","poster":"career360guru"},{"upvote_count":"3","timestamp":"1668172980.0","content":"Selected Answer: B\nMust use and share the existing KMS key to decrypt the same key","poster":"Jtic","comment_id":"716070"},{"timestamp":"1668041580.0","comment_id":"714828","upvote_count":"1","content":"Selected Answer: B\nhttps://aws.amazon.com/premiumsupport/knowledge-center/acm-certificate-expiration/","poster":"flbcobra"},{"content":"Selected Answer: B\nIf EBS snapshots are encrypted, then we need to share the same KMS key to partners to be able to access it. Read the note section in the link\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sharingamis-explicit.html","upvote_count":"5","comment_id":"705686","poster":"ManoAni","timestamp":"1666883820.0"},{"timestamp":"1666274220.0","poster":"tubtab","comment_id":"699997","content":"Selected Answer: C\nMOST secure way should be C","upvote_count":"3"},{"comments":[{"upvote_count":"1","content":"Must use and share the existing KMS key to decrypt the same key","poster":"Jtic","comment_id":"716068","timestamp":"1668172920.0"},{"content":"A seperate/new key is not possible because it won't be able to decrypt the AMI snapshot which was already encrypted with the existing/old key.","upvote_count":"11","poster":"Sauran","comment_id":"698314","comments":[{"content":"This is truth","timestamp":"1666815240.0","upvote_count":"2","poster":"UWSFish","comment_id":"704995"}],"timestamp":"1666106280.0"}],"timestamp":"1665941160.0","poster":"Chunsli","comment_id":"696416","upvote_count":"2","content":"MOST secure way should be C, with a separate key, not the one already used."}],"answer_images":[],"timestamp":"2022-10-16 19:26:00","topic":"1"},{"id":"VIs4bw7iCvacAipg9EEc","answer_description":"","timestamp":"2024-03-05 22:27:00","question_text":"A company needs to create an AWS Lambda function that will run in a VPC in the company's primary AWS account. The Lambda function needs to access files that the company stores in an Amazon Elastic File System (Amazon EFS) file system. The EFS file system is located in a secondary AWS account. As the company adds files to the file system, the solution must scale to meet the demand.\n\nWhich solution will meet these requirements MOST cost-effectively?","answers_community":["B (100%)"],"exam_id":31,"answer_ET":"B","isMC":true,"question_images":[],"discussion":[{"poster":"lenotc","content":"Selected Answer: B\nB -> VPC peering allows the Lambda access secondary account securely and efficiently\nA -> redundancy\nC -> additional complexity\nD -> sharing code libraries","upvote_count":"8","timestamp":"1710105900.0","comment_id":"1170632"},{"content":"Selected Answer: B\nA - Duplicating the EFS file system leads to unnecessary storage costs and additional complexity.\nB - VPC peering connection allows seamless access without additional manual scaling.\nC - Well... I don't know, but you could've directly enabled access to the EFS file system. Why make it even more complex?\nD - Lambda layers are intended for shared libraries or dependencies. You would be crazy to store large amounts of data there.","comment_id":"1331385","poster":"LeonSauveterre","upvote_count":"1","timestamp":"1735106820.0"},{"comment_id":"1230035","upvote_count":"1","content":"Selected Answer: B\nAnswerB\n\nYou can configure a function to mount an Amazon EFS file system in another AWS account. Before you mount the file system, you must ensure the following:\nVPC peering must be configured, and appropriate routes must be added to the route tables in each VPC.\n.\n.\n.\nhttps://docs.aws.amazon.com/lambda/latest/dg/configuration-filesystem-cross-account.html","poster":"Scheldon","timestamp":"1718303280.0"},{"comment_id":"1223626","upvote_count":"1","timestamp":"1717423620.0","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/lambda/latest/dg/configuration-filesystem.html#configuration-filesystem-cross-account","poster":"Nm55569"},{"comment_id":"1170549","upvote_count":"3","poster":"osmk","timestamp":"1710096540.0","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/efs/latest/ug/efs-different-vpc.html"},{"timestamp":"1709674020.0","upvote_count":"1","poster":"asdfcdsxdfc","content":"Shouldn't it be B?","comment_id":"1166767","comments":[{"timestamp":"1709945220.0","poster":"1dd","comments":[{"timestamp":"1710561060.0","poster":"[Removed]","content":"setting up a peering connection is free. same for data transfer in the same AZ. data sync at the end of the day cost $$$ to move data.","comment_id":"1174718","comments":[{"timestamp":"1718303160.0","comment_id":"1230033","upvote_count":"1","content":"When you will SyncData you need to have secondary storage for which you need to pay so it is not cheap solution.","poster":"Scheldon"}],"upvote_count":"3"}],"upvote_count":"1","comment_id":"1169184","content":"I thinks AWS DataSync less costly"}]}],"answer":"B","question_id":800,"url":"https://www.examtopics.com/discussions/amazon/view/135258-exam-aws-certified-solutions-architect-associate-saa-c03/","unix_timestamp":1709674020,"choices":{"D":"Move the contents of the file system to a Lambda layer. Configure the Lambda layer's permissions to allow the company's secondary account to use the Lambda layer.","B":"Create a VPC peering connection between the VPCs that are in the primary account and the secondary account.","C":"Create a second Lambda function in the secondary account that has a mount that is configured for the file system. Use the primary account's Lambda function to invoke the secondary account's Lambda function.","A":"Create a new EFS file system in the primary account. Use AWS DataSync to copy the contents of the original EFS file system to the new EFS file system."},"topic":"1","answer_images":[]}],"exam":{"lastUpdated":"11 Apr 2025","numberOfQuestions":1019,"isImplemented":true,"id":31,"name":"AWS Certified Solutions Architect - Associate SAA-C03","isMCOnly":true,"provider":"Amazon","isBeta":false},"currentPage":160},"__N_SSP":true}