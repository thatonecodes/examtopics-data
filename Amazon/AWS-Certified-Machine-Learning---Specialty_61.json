{"pageProps":{"questions":[{"id":"ceoDVMahDHiXYAbSmv2W","question_text":"A Machine Learning Specialist working for an online fashion company wants to build a data ingestion solution for the company's Amazon S3-based data lake.\nThe Specialist wants to create a set of ingestion mechanisms that will enable future capabilities comprised of:\n✑ Real-time analytics\n✑ Interactive analytics of historical data\n✑ Clickstream analytics\n✑ Product recommendations\nWhich services should the Specialist use?","timestamp":"2019-12-09 21:19:00","url":"https://www.examtopics.com/discussions/amazon/view/10058-exam-aws-certified-machine-learning-specialty-topic-1/","choices":{"C":"AWS Glue as the data catalog; Amazon Kinesis Data Streams and Amazon Kinesis Data Analytics for historical data insights; Amazon Kinesis Data Firehose for delivery to Amazon ES for clickstream analytics; Amazon EMR to generate personalized product recommendations","B":"Amazon Athena as the data catalog: Amazon Kinesis Data Streams and Amazon Kinesis Data Analytics for near-real-time data insights; Amazon Kinesis Data Firehose for clickstream analytics; AWS Glue to generate personalized product recommendations","A":"AWS Glue as the data catalog; Amazon Kinesis Data Streams and Amazon Kinesis Data Analytics for real-time data insights; Amazon Kinesis Data Firehose for delivery to Amazon ES for clickstream analytics; Amazon EMR to generate personalized product recommendations","D":"Amazon Athena as the data catalog; Amazon Kinesis Data Streams and Amazon Kinesis Data Analytics for historical data insights; Amazon DynamoDB streams for clickstream analytics; AWS Glue to generate personalized product recommendations"},"unix_timestamp":1575922740,"answer_images":[],"discussion":[{"upvote_count":"38","poster":"rsimham","comment_id":"28264","timestamp":"1663683120.0","content":"Ans: A seems to be reasonable"},{"timestamp":"1664394660.0","upvote_count":"13","comments":[{"poster":"ZSun","upvote_count":"1","comment_id":"875103","content":"AWS Glue as data catalog, then you can analyze historical data, such as running sql with Athena.","timestamp":"1713560700.0"},{"comment_id":"424576","content":"Once you insert real-time data to ES, you can see historical data from Kibana dashboard.","timestamp":"1667694480.0","poster":"planhanasan","upvote_count":"1"},{"comments":[{"upvote_count":"1","content":"and also C is saying historical data analytics for Kinesis Data analytics which is real-time analytics not historical, so the answer might not C but the answer is A","poster":"eji","comment_id":"130276","timestamp":"1666558200.0"}],"poster":"eji","upvote_count":"1","comment_id":"130275","content":"but C is missing for \"real-time analythics\"","timestamp":"1666420920.0"}],"content":"A looks correct but it is missing for \"Interactive analytics of historical data\"","poster":"cybe001","comment_id":"38280"},{"upvote_count":"3","poster":"loict","timestamp":"1726230840.0","content":"Selected Answer: A\nA. YES - Amazon Kinesis Data Analytics is for real-time data insights\nB. NO - Amazon Athena has no data catalog\nC. NO - Amazon Kinesis Data Streams and Amazon Kinesis Data Analytics is not for historical data insights\nD. NO - Amazon Athena has no data catalog","comment_id":"1006611"},{"content":"Selected Answer: A\nAthena can not be used for data catalog, so B and D are wrong. A and C are equals, but it's well known that Kinesis DS and Analytics are used together for real time solutions, which is mentioned in the question / answer, but lack on C.","upvote_count":"2","timestamp":"1722424260.0","comment_id":"968000","poster":"kaike_reis"},{"timestamp":"1709887680.0","upvote_count":"2","content":"Selected Answer: A\nAll are bad options, but A can do it.","poster":"Valcilio","comment_id":"832713"},{"upvote_count":"2","comment_id":"824294","content":"Selected Answer: A\nAWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy to move data between data stores. It can be used as a data catalog to store metadata information about the data in the data lake. Amazon Kinesis Data Streams and Amazon Kinesis Data Analytics can be used together to collect, process, and analyze real-time streaming data. Amazon Kinesis Data Firehose can be used to deliver streaming data to destinations such as Amazon ES for clickstream analytics. Finally, Amazon EMR can be used to run big data frameworks such as Apache Spark and Apache Hadoop to generate personalized product recommendations.","timestamp":"1709084760.0","poster":"hug_c0sm0s"},{"comment_id":"308737","content":"A or C\nhttps://aws.amazon.com/blogs/big-data/retaining-data-streams-up-to-one-year-with-amazon-kinesis-data-streams/","timestamp":"1667366040.0","upvote_count":"2","poster":"gamaX"},{"upvote_count":"5","comment_id":"262835","poster":"harmanbirstudy","content":"Athena can do Interactive analytics on Historical data, but here its only use is \"Athena as the data catalog\" and this is the work of Glue data catalog using its crawlers, so it cannot be B or D.\n--So its either A or C\n-- Now Kinesis data Streams/Analytics is know for real time data analytics but if it is reading from data already stored in S3 using DMS then we can say it is getting historical data.\n-- Here I am not very clear if Kinesis part will happen on incoming data before S3 or After data persists to S3 and Kinesis reads it through S3-->DMS--Kinesis data stream -- Kinesis analytics-->Firehose.\n But still insights are always on real-time/current data based on historical data trends , so the statement in C \"Analytics for historical data insights\" is in-correct in general .\nHence ANSWER is :A","timestamp":"1667362500.0"},{"content":"A is correct,\n\nfor those asking the difference between A and D, D talks about using kinesis stream and data analytics to create historical analysis.... waste of money no?","timestamp":"1666881120.0","upvote_count":"2","poster":"ybad","comment_id":"226053"},{"upvote_count":"4","timestamp":"1666778040.0","poster":"Th3Dud3","content":"Answer = A","comment_id":"179938"},{"content":"A it is","poster":"C10ud9","timestamp":"1666272060.0","comment_id":"102651","upvote_count":"2"},{"timestamp":"1666145580.0","upvote_count":"3","content":"it's A, ES can perform clickstream analytics and EMR can handle spark job recomendation at scale","comment_id":"84855","poster":"roytruong"},{"timestamp":"1666039260.0","content":"Only C and D mention interactive analytics of historical data.\nGlue won't provide personalised recommendation so it is C","poster":"BigPlums","upvote_count":"1","comment_id":"80936"},{"content":"What is the difference between the solution in A or C ????","comments":[{"comment_id":"51651","upvote_count":"6","poster":"JayK","content":"A is real time data analytics with Kinesis Data analytics and C is saying historical data which is wrong","timestamp":"1665260640.0"}],"comment_id":"45098","timestamp":"1664945460.0","poster":"BigEv","upvote_count":"2"},{"timestamp":"1663902060.0","upvote_count":"2","comment_id":"34782","content":"Looks like C Amazon ES has KIbana which supports click stream","poster":"ComPah","comments":[{"comment_id":"34787","poster":"ComPah","upvote_count":"4","content":"A is Correct","timestamp":"1664003700.0"}]}],"answer_description":"","isMC":true,"topic":"1","answer_ET":"A","exam_id":26,"question_images":[],"question_id":301,"answer":"A","answers_community":["A (100%)"]},{"id":"biLXWGqWKHd9xcopOCEZ","topic":"1","answer_images":[],"unix_timestamp":1573891020,"answer_description":"","exam_id":26,"timestamp":"2019-11-16 08:57:00","url":"https://www.examtopics.com/discussions/amazon/view/8317-exam-aws-certified-machine-learning-specialty-topic-1/","choices":{"E":"Download and apt-get install the inception network code into an Amazon EC2 instance and use this instance as a Jupyter notebook in Amazon SageMaker.","A":"Customize the built-in image classification algorithm to use Inception and use this for model training.","C":"Bundle a Docker container with TensorFlow Estimator loaded with an Inception network and use this for model training.","D":"Use custom code in Amazon SageMaker with TensorFlow Estimator to load the model with an Inception network, and use this for model training.","B":"Create a support case with the SageMaker team to change the default image classification algorithm to Inception."},"discussion":[{"upvote_count":"33","timestamp":"1679378880.0","comment_id":"21915","poster":"DonaldCMLIN","content":"You might be spent a lot of money for ask AWS A.CHANGE built-in image OR B.Create a support case.\n\nThe effectial way BOTH RELATIVE TO SageMaker Estimator \nC.DOCKER \nOR BRING YOUR CODE BY \nD.SageMaker with TensorFlow Estimator \n\nTHE BEAUTYFUL ANSWER ARE C AND D"},{"poster":"Phong","timestamp":"1680812580.0","upvote_count":"10","content":"I will go for C & D","comment_id":"51098"},{"upvote_count":"4","timestamp":"1724802600.0","poster":"hug_c0sm0s","comment_id":"824297","content":"Selected Answer: CD\nOption A is not possible because the built-in image classification algorithm cannot be customized. Option B is not feasible because it is not possible to change the default image classification algorithm through a support case. Option E is also not a recommended approach because it involves manually installing software on an EC2 instance rather than using the managed services provided by SageMaker."},{"content":"Selected Answer: CD\nThe effectial way BOTH RELATIVE TO SageMaker Estimator \nC.DOCKER \nOR BRING YOUR CODE BY \nD.SageMaker with TensorFlow Estimator","comment_id":"803072","timestamp":"1723191540.0","upvote_count":"2","poster":"sqavi"},{"timestamp":"1682520120.0","poster":"Huy","comment_id":"399269","content":"This question ask for 2 ways not a set of actions. So may be confused.","upvote_count":"1"},{"poster":"ahquiceno","upvote_count":"2","comment_id":"280545","timestamp":"1682272560.0","content":"Anwers AD go to: https://docs.aws.amazon.com/sagemaker/latest/dg/docker-containers.html"},{"poster":"ybad","content":"CD and also A says it but in a more general term....","comment_id":"226055","upvote_count":"1","timestamp":"1681532640.0"},{"content":"https://aws.amazon.com/blogs/machine-learning/transfer-learning-for-custom-labels-using-a-tensorflow-container-and-bring-your-own-algorithm-in-amazon-sagemaker/","upvote_count":"3","timestamp":"1681501140.0","comment_id":"134383","poster":"jaydec"},{"timestamp":"1681215360.0","comment_id":"108582","content":"C and D are correct","upvote_count":"5","poster":"Antriksh"},{"content":"https://docs.aws.amazon.com/zh_tw/sagemaker/latest/dg/your-algorithms.html\n\nhttps://aws.amazon.com/tw/blogs/machine-learning/transfer-learning-for-custom-labels-using-a-tensorflow-container-and-bring-your-own-algorithm-in-amazon-sagemaker/\nhttps://docs.aws.amazon.com/zh_tw/sagemaker/latest/dg/tf.html","upvote_count":"1","timestamp":"1680180540.0","poster":"DonaldCMLIN","comment_id":"21916"}],"isMC":true,"answer_ET":"CD","question_images":[],"question_id":302,"question_text":"A company is observing low accuracy while training on the default built-in image classification algorithm in Amazon SageMaker. The Data Science team wants to use an Inception neural network architecture instead of a ResNet architecture.\nWhich of the following will accomplish this? (Choose two.)","answer":"CD","answers_community":["CD (100%)"]},{"id":"DGGiKZgbqDcKPcgA4xi6","isMC":true,"answer_ET":"B","answer":"B","url":"https://www.examtopics.com/discussions/amazon/view/8318-exam-aws-certified-machine-learning-specialty-topic-1/","answer_images":[],"question_id":303,"question_images":[],"unix_timestamp":1573892580,"answers_community":["B (100%)"],"timestamp":"2019-11-16 09:23:00","topic":"1","question_text":"A Machine Learning Specialist built an image classification deep learning model. However, the Specialist ran into an overfitting problem in which the training and testing accuracies were 99% and 75%, respectively.\nHow should the Specialist address this issue and what is the reason behind it?","exam_id":26,"answer_description":"","discussion":[{"poster":"DonaldCMLIN","comment_id":"21917","content":"DROPOUT HELPS PREVENT OVERFITTING\nhttps://keras.io/layers/core/#dropout\n\nTHE BEAUTIFUL ANSER SHOULD BE B.","upvote_count":"55","comments":[{"poster":"rsimham","timestamp":"1648039740.0","comment_id":"28270","content":"agree. it should be B","upvote_count":"10"}],"timestamp":"1647848880.0"},{"comment_id":"171869","timestamp":"1650489540.0","upvote_count":"5","poster":"syu31svc","content":"https://kharshit.github.io/blog/2018/05/04/dropout-prevent-overfitting\n\nAnswer is B 100%"},{"poster":"fm99","content":"Selected Answer: B\nIncreasing dropout rate will reduce complexity of the model which inturn reduces overfitting","comment_id":"1194095","upvote_count":"1","timestamp":"1728700080.0"},{"upvote_count":"1","poster":"VR10","comment_id":"1154330","timestamp":"1724099880.0","content":"This is clearly B, dont get why the answer is marked as D."},{"timestamp":"1716793200.0","comment_id":"1081326","poster":"endeesa","content":"Selected Answer: B\nRegularization will seek to obtain similar accuracies in train and test sets. Anything else will make the overfitting worse","upvote_count":"1"},{"comment_id":"1067012","upvote_count":"1","poster":"elvin_ml_qayiran25091992razor","timestamp":"1715318520.0","content":"Selected Answer: B\nB is correct, D so stup*d answer"},{"comment_id":"1006622","poster":"loict","timestamp":"1710340920.0","upvote_count":"2","content":"Selected Answer: B\nA. NO - accuracy on training set is high\nB. YES - increased dropout rate => reduce model complexity => less overfitting\nC. NO - we want to reduce model complexity\nD. NO - the model converged"},{"comment_id":"1005467","upvote_count":"2","poster":"DavidRou","timestamp":"1710232620.0","content":"Selected Answer: B\nI don't understand why the highlighted \"right\" answer is D. To increase the number of epochs will make the situation even worse than it is; dropout is the right action to take in this case"},{"poster":"kaike_reis","timestamp":"1706707080.0","comment_id":"968007","content":"Selected Answer: B\nB is correct","upvote_count":"1"},{"timestamp":"1703161680.0","comment_id":"929335","upvote_count":"1","poster":"nilmans","content":"agree, B makes more sense here"},{"upvote_count":"1","poster":"soonmo","comments":[{"comment_id":"922688","poster":"soonmo","upvote_count":"1","content":"Correct my reasoning! D is worsening overfitting because it feeds more data after overfitting arises. D is used for underfitted models.","timestamp":"1702529340.0"}],"content":"Selected Answer: B\nDefinitely B because overfitting comes from complex model that captures patterns of training data well. But D is getting this model more complex, worsening overfitting.","timestamp":"1702529220.0","comment_id":"922685"},{"upvote_count":"1","content":"Selected Answer: B\nIncreasing Epoch only makes things worse on a overfitting model. You should perform regularization by introducing drop outs to generalize the model.","timestamp":"1700864040.0","comment_id":"906139","poster":"earthMover"},{"timestamp":"1695702360.0","content":"Option B is the correct answer because increasing the dropout rate at the flatten layer helps prevent overfitting by randomly dropping out units during training, effectively creating a more robust model that can generalize better to new data. Dropout is a regularization technique that helps prevent overfitting by forcing the model to learn redundant representations of the data. By increasing the dropout rate at the flatten layer, the model becomes more generalized, which should help to improve the testing accuracy.","upvote_count":"1","comment_id":"850717","poster":"user009"},{"timestamp":"1691659500.0","upvote_count":"1","content":"Selected Answer: B\nOverfitting occurs when a model is too complex and memorizes the training data instead of learning the underlying pattern. As a result, the model performs well on the training data but poorly on new, unseen data. \n\nIncreasing the dropout rate, a regularization technique, can help combat overfitting by randomly dropping out some neurons during training, which prevents the model from relying too heavily on any single feature.","poster":"AjoseO","comment_id":"804241"},{"timestamp":"1691569560.0","upvote_count":"2","comment_id":"803079","content":"Selected Answer: B\nModel is overfitting, I will go with option B, increasing epoch will cause more overfitting","poster":"sqavi"},{"poster":"desperatestudent","comment_id":"781254","upvote_count":"1","timestamp":"1689773880.0","content":"Selected Answer: B\nit should answer B."},{"content":"12-sep exam","poster":"Shailendraa","comment_id":"667390","timestamp":"1678655100.0","upvote_count":"2"},{"timestamp":"1677915300.0","upvote_count":"1","poster":"Moulichintakunta","comment_id":"658974","content":"The link which mentioned in answer section also talks about dropout only , so Option B is correct"},{"poster":"ovokpus","content":"Selected Answer: B\nI vote B for all the reasons already mentioned. Dropouts help prevent overfitting","timestamp":"1672092060.0","upvote_count":"3","comment_id":"622797"},{"comment_id":"601977","poster":"theprismdata","timestamp":"1668507240.0","upvote_count":"2","content":"Selected Answer: B\nDropout prevent overfitting, and Early stopping is useful, increasing epoch is not effect in this situation\nhttps://en.wikipedia.org/wiki/Early_stopping"},{"upvote_count":"2","timestamp":"1658334060.0","content":"B. All the other answers do not prevent overfitting.","comment_id":"528659","poster":"jonsnow777"},{"upvote_count":"4","poster":"eganilovic","timestamp":"1651515000.0","comment_id":"343306","content":"Correct answer is B! Number of epochs shoud be decresed."},{"upvote_count":"2","timestamp":"1651283220.0","content":"The reference article listed by the website also says dropout is the correct answer...go with B","comment_id":"333991","poster":"gcpwhiz"},{"upvote_count":"2","timestamp":"1650963960.0","comment_id":"324213","content":"B is true answer. More epochs will not stop overfitting.","poster":"Vita_Rasta84444"},{"upvote_count":"2","comment_id":"312283","content":"B!!!!!!!!!!!!!!","poster":"littlewat","timestamp":"1650942120.0"},{"timestamp":"1650670920.0","content":"I will go for b because the problem is the model is failing to generalize and hence dropout will solve the problem.","comment_id":"263879","poster":"DzR","upvote_count":"1"},{"content":"It cannot be because with 99% accuracy the testing does need epochs and nowhere it says about epoch getting Early Terminated in Question , so we cannot just assume it.\nAmong the rest of the list only \"Increasing Dropout\" helps preventing Overfitting.\nANSWER is B","poster":"harmanbirstudy","comment_id":"262847","upvote_count":"2","timestamp":"1650669000.0"},{"upvote_count":"1","timestamp":"1650629520.0","comment_id":"226755","poster":"ybad","content":"Agree with all it should be B, only solution to reduce overfitting,\n\nincreasing epochs would actually cause it to overfit more...."},{"poster":"hans1234","upvote_count":"4","comments":[{"poster":"mrsimoes","upvote_count":"1","comment_id":"171380","timestamp":"1650353940.0","content":"I agree with dropout, the problem is the second part of the answer. The model to have 99% accuracy in the training means it is generalizing well in the train set."}],"timestamp":"1650275100.0","comment_id":"147222","content":"It is not A, because the training error is at 99% versus 75% for test, which means it is not at a local minimum, but has overfitting problems. The answer is B."},{"comment_id":"134748","timestamp":"1650038460.0","upvote_count":"2","content":"it is ok to use dropout before flatten layer. Answer is B\nhttps://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py","poster":"algorithmish"},{"comments":[{"poster":"AMEJack","comment_id":"430620","content":"Flatten layer is the input layer for the dense layer, dropout can be used with input layer.","upvote_count":"2","timestamp":"1651642560.0"}],"poster":"andreylh","comment_id":"109650","content":"My answer is A, since dropout is used on fully connected (dense) layers not flatten.","upvote_count":"2","timestamp":"1649954820.0"},{"poster":"[Removed]","comment_id":"60737","timestamp":"1649667000.0","upvote_count":"2","content":"epoch number is utilized only in training phase, but there is a 99% of accuracy in this phase it wouldn't get better accuracy, I think increment number of epoch does not improve the result in test"},{"poster":"Phong","comment_id":"51099","upvote_count":"3","content":"go for B","timestamp":"1649165040.0"},{"comments":[{"timestamp":"1649922780.0","comment_id":"108587","poster":"Antriksh","content":"with dropout you are reducing over dependency on neurons thereby making model more generic and less complex. Over depending on few neuron makes the network learn the model. This can cause model to overfit. Dropping neuron prevents that. \nHence correct answer is dropout","upvote_count":"2"}],"timestamp":"1648962240.0","content":"Why not A? B is slightly confusing as flatten and dropout are separate layer types in TensorFlow.","upvote_count":"1","comment_id":"46856","poster":"tap123"},{"comment_id":"28521","timestamp":"1648618500.0","poster":"vetal","comments":[{"upvote_count":"2","comment_id":"194358","timestamp":"1650610080.0","poster":"SabinaMystique","content":"and even say this \"Dropout is one of the most effective and most commonly used regularization techniques for neural networks\""}],"upvote_count":"3","content":"What is interesting in the suggested link the best options are regularization and dropout :)\nhttps://www.tensorflow.org/tutorials/keras/overfit_and_underfit"},{"upvote_count":"4","timestamp":"1647968280.0","poster":"DonaldCMLIN","comment_id":"21918","content":"NOT D. \nAlthough increase epoch could improve overfitting, but not in status of early stopping\n\nB.INCREATE DROPOUT FOR not generalize well\n\nhttps://developer.ibm.com/articles/image-recognition-challenge-with-tensorflow-and-keras-pt2/","comments":[{"content":"https://keras.io/api/layers/regularization_layers/dropout/\nThe Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting","poster":"zzeng","upvote_count":"1","comment_id":"138650","timestamp":"1650200520.0"},{"content":"100% B. The model need to be less complex, it only memorized the info, needs a \"trepanning\" :-).","poster":"ahquiceno","timestamp":"1650926100.0","upvote_count":"1","comment_id":"280554"}]}],"choices":{"B":"The dropout rate at the flatten layer should be increased because the model is not generalized enough.","A":"The learning rate should be increased because the optimization process was trapped at a local minimum.","D":"The epoch number should be increased because the optimization process was terminated before it reached the global minimum.","C":"The dimensionality of dense layer next to the flatten layer should be increased because the model is not complex enough."}},{"id":"xl98WGkIBOxkxPE2CWHL","question_text":"A city wants to monitor its air quality to address the consequences of air pollution. A Machine Learning Specialist needs to forecast the air quality in parts per million of contaminates for the next 2 days in the city. As this is a prototype, only daily data from the last year is available.\nWhich model is MOST likely to provide the best results in Amazon SageMaker?","url":"https://www.examtopics.com/discussions/amazon/view/12382-exam-aws-certified-machine-learning-specialty-topic-1/","question_images":[],"choices":{"C":"Use the Amazon SageMaker Linear Learner algorithm on the single time series consisting of the full year of data with a predictor_type of regressor.","A":"Use the Amazon SageMaker k-Nearest-Neighbors (kNN) algorithm on the single time series consisting of the full year of data with a predictor_type of regressor.","B":"Use Amazon SageMaker Random Cut Forest (RCF) on the single time series consisting of the full year of data.","D":"Use the Amazon SageMaker Linear Learner algorithm on the single time series consisting of the full year of data with a predictor_type of classifier."},"answers_community":["C (78%)","A (17%)","6%"],"exam_id":26,"unix_timestamp":1579523760,"answer_images":[],"answer_ET":"C","isMC":true,"answer_description":"","timestamp":"2020-01-20 13:36:00","discussion":[{"upvote_count":"16","comment_id":"40988","poster":"ozan11","timestamp":"1632917640.0","content":"answer should be C"},{"content":"go for C","comment_id":"98661","upvote_count":"6","poster":"roytruong","timestamp":"1634181000.0"},{"upvote_count":"1","content":"Selected Answer: C\nAmazon SageMaker Linear Learner (Regressor) \nWhy?\nThe Linear Learner algorithm can be used for time series regression.\nUsing predictor_type=regressor, it learns trends and patterns in historical data and extrapolates future values.\nGiven limited historical data (only 1 year), a simple linear regression model might perform well as a baseline.\nWhile deep learning models (like Amazon Forecast) may be more advanced, Linear Learner is easier to implement and train for a prototype.","comment_id":"1357046","poster":"JonSno","timestamp":"1739653380.0"},{"upvote_count":"3","comment_id":"1006267","timestamp":"1727163840.0","content":"Selected Answer: C\nA. NO - kNN is not forecasting, it is similarities\nB. NO - RCF is for anomality detection\nC. YES - Linear Regression good for forecasting\nD. NO - we don't want to classify","poster":"loict"},{"content":"Selected Answer: C\nThe reason for this choice is that the Linear Learner algorithm is a versatile algorithm that can be used for both regression and classification tasks1. Regression is a type of supervised learning that predicts a continuous numeric value, such as the air quality in parts per million2. The predictor_type parameter specifies whether the algorithm should perform regression or classification3. Since the goal is to forecast a numeric value, the predictor_type should be set to regressor.","poster":"Mickey321","comment_id":"973020","upvote_count":"3","timestamp":"1727163840.0"},{"content":"Selected Answer: D\nA. Managing Kafka on EC2 is not compatible with least effort requirement\nB. Doable (in 2024) as Glue supports streaming ETL to consumes streams and supports CSV records -> https://docs.aws.amazon.com/glue/latest/dg/add-job-streaming.html\nC. Managing an EMR cluster imo is no compatible with least effort requirement\nD. Firehose supports kinesis data stream as source and it can use lambda to convert CSV records into parquet -> https://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html\n\nI guess this is a bit old question, pre Glue streaming ETL support (2023) -> https://aws.amazon.com/about-aws/whats-new/2023/03/aws-glue-4-0-streaming-etl/\n\nThus I'll go for D","upvote_count":"1","poster":"ninomfr64","comment_id":"1230810","timestamp":"1718437500.0"},{"content":"This blog wrote Japanese.\nbut its said using LinearLearner for air pollution prediction.\nhttps://aws.amazon.com/jp/blogs/news/build-a-model-to-predict-the-impact-of-weather-on-urban-air-quality-using-amazon-sagemaker/","comment_id":"1082585","timestamp":"1701176580.0","poster":"LocalHero","upvote_count":"2"},{"comment_id":"964573","timestamp":"1690452240.0","upvote_count":"1","poster":"jyrajan69","content":"The HyperParameter is . Either “binary_classifier” or “multiclass_classifier” or “regressor”., there is no classifier so the answer is C"},{"content":"Selected Answer: C\nAns should be c","timestamp":"1690207200.0","comment_id":"961667","poster":"Venkatesh_Babu","upvote_count":"1"},{"comment_id":"950508","timestamp":"1689240840.0","poster":"ortamina","upvote_count":"1","content":"a kNN will require a large value of k to avoid overfitting and we only have 1 year's worth of data - kNNs also face a difficult time extrapolating if the air quality series contains a trend\n\nIf we had assurances there is no trend in the air quality series (no extrapolation), and we had enough data, then kNN should beat a linear model ... I am inclined to go for C just going off of the cue that \"only daily data from last year is available\"","comments":[{"poster":"ninomfr64","timestamp":"1719729540.0","upvote_count":"1","comment_id":"1239541","content":"Agree with you analysis, to further expand it: we don't have info about dataset features based on \"only daily data from last year is available\" this let me think we could be in a situation where our dataset is made up by timestamp and pollution_value so KNN would be pretty useless in this situation."}]},{"upvote_count":"3","poster":"brunokiyoshi","comments":[{"poster":"brunokiyoshi","comment_id":"846477","timestamp":"1679443260.0","content":"I mean, you could use KNN's for regression, but for forecasting I don't think so","upvote_count":"1"}],"comment_id":"846475","content":"Selected Answer: C\nRandom cut forests in timeseries are used for anomaly detection, and not for forecasting. KNN's are classification algorithms. You would use the Linear Learner as a regressor, since forecasting falls into the domain of regression.","timestamp":"1679443200.0"},{"timestamp":"1678262640.0","comments":[{"comment_id":"832658","content":"Im sorry, I wanted to say go for C!","timestamp":"1678262640.0","upvote_count":"2","poster":"Valcilio"}],"content":"Selected Answer: C\nKNN isn't for time series predicting, go for A!","upvote_count":"2","comment_id":"832657","poster":"Valcilio"},{"timestamp":"1678088820.0","upvote_count":"1","content":"Creating a machine learning model to predict air quality\nTo start small, we will follow the second approach, where we will build a model that will predict the NO2 concentration of any given day based on wind speed, wind direction, maximum temperature, pressure values of that day, and the NO2 concentration of the previous day. For this we will use the Linear Learner algorithm provided in Amazon SageMaker, enabling us to quickly build a model with minimal work.\n\nOur model will consist of taking all of the variables in our dataset and using them as features of the Linear Learner algorithm available in Amazon SageMaker","poster":"rockyykrish","comment_id":"830615"},{"timestamp":"1675358640.0","poster":"AjoseO","content":"Selected Answer: A\nAnswer should be A.\n\n k-Nearest-Neighbors (kNN) algorithm will provide the best results for this use case as it is a good fit for time series data, especially for predicting continuous values. The predictor_type of regressor is also appropriate for this task, as the goal is to forecast a continuous value (air quality in parts per million of contaminants). The other options are also viable, but may not provide as good of results as the kNN algorithm, especially with limited data.\n\nusing the Amazon SageMaker Linear Learner algorithm with a predictor_type of regressor, may still provide reasonable results, but it assumes a linear relationship between the input features and the target variable (air quality), which may not always hold in practice, especially with complex time series data. In such cases, non-linear models like kNN may perform better. Furthermore, the kNN algorithm can handle irregular patterns in the data, which may be present in the air quality data, and provide more accurate predictions.","comment_id":"796271","upvote_count":"3"},{"timestamp":"1663474740.0","content":"Selected Answer: C\nAnswer is \"C\" !!!","upvote_count":"1","poster":"ryuhei","comment_id":"671974"},{"poster":"yemauricio","upvote_count":"1","timestamp":"1662640320.0","content":"answer C","comment_id":"663604"},{"poster":"Huy","timestamp":"1635991740.0","content":"I go with A. Linear regression is not suitable for time series data. there is a library that implements knn for time-series https://cran.r-project.org/web/packages/tsfknn/vignettes/tsfknn.html","comments":[{"upvote_count":"1","comment_id":"402362","poster":"Huy","content":"I mean the air quality have many feature correlations that are not linear.","timestamp":"1636153140.0"}],"upvote_count":"1","comment_id":"398474"},{"upvote_count":"4","poster":"syu31svc","timestamp":"1634332500.0","content":"If it's about forecasting then answer is C 100%; regression","comment_id":"164319"},{"content":"C should be the answer.. D is not because it's not a classifier problem (..forecast the air quality in parts per million..).. it should be regression.","comment_id":"56718","timestamp":"1633352520.0","poster":"VB","upvote_count":"3"},{"comment_id":"54110","upvote_count":"4","comments":[{"poster":"Nahe","content":"K-NN can also be used for regression. But it is not preferred here as we have very limited data of 2-3 days","comment_id":"249351","upvote_count":"4","timestamp":"1635184860.0"}],"content":"It's a Regression problem so A)KNN B)RCF D) can be eliminated leaving us with the answer C","poster":"rajs","timestamp":"1633000440.0"}],"topic":"1","question_id":304,"answer":"C"},{"id":"lpFjb7c9ytodZEsKOKtm","url":"https://www.examtopics.com/discussions/amazon/view/8319-exam-aws-certified-machine-learning-specialty-topic-1/","timestamp":"2019-11-16 10:02:00","topic":"1","answer_ET":"B","choices":{"B":"Use AWS CloudTrail to log Amazon SageMaker API calls to Amazon S3. Add code to push a custom metric to Amazon CloudWatch. Create an alarm in CloudWatch with Amazon SNS to receive a notification when the model is overfitting.","A":"Implement an AWS Lambda function to log Amazon SageMaker API calls to Amazon S3. Add code to push a custom metric to Amazon CloudWatch. Create an alarm in CloudWatch with Amazon SNS to receive a notification when the model is overfitting.","D":"Use AWS CloudTrail to log Amazon SageMaker API calls to Amazon S3. Set up Amazon SNS to receive a notification when the model is overfitting","C":"Implement an AWS Lambda function to log Amazon SageMaker API calls to AWS CloudTrail. Add code to push a custom metric to Amazon CloudWatch. Create an alarm in CloudWatch with Amazon SNS to receive a notification when the model is overfitting."},"answers_community":["B (88%)","13%"],"answer_images":[],"question_id":305,"answer":"B","discussion":[{"timestamp":"1632102660.0","upvote_count":"41","comment_id":"21920","content":"THE ANSWER SHOULD BE B.\nYOU DON'T NEED TO THROUGH LAMBDA TO INTERGE CLOUDTRAIL\n\nLog Amazon SageMaker API Calls with AWS CloudTrail\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/logging-using-cloudtrail.html","poster":"DonaldCMLIN"},{"poster":"rajs","upvote_count":"11","comment_id":"57351","timestamp":"1632584280.0","content":"Agreed B for the following reasons\n\n# CloudTrail logs captured in S3 without any code/lambda\n# The custom metrics can be published to Cloudwatch...in this case it would be a test for overfit on MXNET .... which will set off an alarm .... which can then be subscribed on SNS"},{"timestamp":"1739838840.0","upvote_count":"1","comment_id":"1358049","content":"Selected Answer: B\nBreakdown of the Chosen Solution (B)\nUse AWS CloudTrail to log SageMaker API calls to Amazon S3 ✅\n\nCloudTrail automatically logs all AWS API activity, including SageMaker API calls, for auditing.\nS3 stores these logs securely for auditor review.\nPush custom metrics to Amazon CloudWatch ✅\n\nModel overfitting can be detected using a custom CloudWatch metric (e.g., validation loss increasing while training loss decreases).\nThe SageMaker training script can push loss values to CloudWatch during training.\nCreate a CloudWatch alarm + SNS notification ✅\n\nSet a CloudWatch alarm on the overfitting metric (e.g., validation loss surpassing a threshold).\nUse Amazon SNS to send a notification (email, SMS, or Lambda trigger) when the alarm is triggered.","poster":"JonSno"},{"comment_id":"1306154","poster":"MultiCloudIronMan","upvote_count":"2","content":"Selected Answer: B\nOption D involves using AWS CloudTrail to log Amazon SageMaker API calls to Amazon S3 and setting up Amazon SNS to receive a notification when the model is overfitting. While this approach addresses the logging requirement, it does not provide a mechanism for pushing custom metrics to Amazon CloudWatch, which is necessary for monitoring model performance and detecting overfitting. So 'B ' is correct","timestamp":"1730546580.0"},{"comments":[{"comment_id":"1350466","content":"This would have been correct had the question not mentioned that the algorithm is \"hand-written\" which means it's not a built in algorithm. So, for SageMaker AI to understand your custom algorithm's metrics, it needs a regex definition to apply to the logs in order to generate those custom metrics and then alert on them using CW Alarms and SNS to deliver notifications. See https://docs.aws.amazon.com/sagemaker/latest/dg/define-train-metrics.html","poster":"Mobasher","upvote_count":"1","timestamp":"1738503960.0"},{"comment_id":"1204019","timestamp":"1714396200.0","poster":"Aja1","upvote_count":"1","content":"Custom metric Need to built and pushed."}],"poster":"Chiquitabandita","comment_id":"1203406","timestamp":"1714286880.0","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/training-metrics.html#define-train-metrics\n\nIt detects hardware resource usage issues (such as CPU, GPU, and I/O bottlenecks) and non-convergent model issues (such as overfitting, disappearing gradients, and tensor explosion).\n\nwhy couldn't the answer be D, as this covers all of the requirements, and B seems to add an extra step with adding push code, when it already has a builtin metric for overfitting.","upvote_count":"1"},{"content":"Selected Answer: B\nA. NO - CloudTrail has built-in SageMaker API calls tracking, no lambda needed\nB. YES - the chain works\nC. NO - CloudTrail has built-in SageMaker API calls tracking, no lambda needed\nD. NO - CloudTrail has not specific Amazon SageMaker integration to detect overfitting","poster":"loict","upvote_count":"1","comment_id":"1006626","timestamp":"1694609100.0"},{"timestamp":"1693246320.0","upvote_count":"1","poster":"Mickey321","comment_id":"992540","content":"Selected Answer: B\nOption B"},{"comments":[{"upvote_count":"1","content":"Agreed, with less code effort.","timestamp":"1689171540.0","comment_id":"949883","poster":"kukreti18"}],"timestamp":"1687990800.0","poster":"ADVIT","upvote_count":"2","comment_id":"937176","content":"\"least amount of code and fewest steps?\"\nI think it's D."},{"comments":[{"timestamp":"1699948260.0","poster":"khchan123","content":"The loss_not_decreasing, overfit, overtraining, and stalled_training_rule monitors if your model is optimizing the loss function without those training issues. If the rules detect training anomalies, the rule evaluation status changes to IssueFound. You can set up automated actions, such as notifying training issues and stopping training jobs using Amazon CloudWatch Events and AWS Lambda. For more information, see Action on Amazon SageMaker Debugger Rules.\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/use-debugger-built-in-rules.html","comment_id":"1070131","upvote_count":"1"}],"poster":"Paolo991","timestamp":"1679730960.0","upvote_count":"2","content":"I would consider D as well.\n\nYou can just setup a SNS that is triggered by a built-in action like here:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-actions.html \n\nYou can see that overfitting is a built-in rule for MXNet from here:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-rules.html\n \nNot that B is not working. Maybe the question was prior to this new solution.","comment_id":"849898"},{"comment_id":"832738","upvote_count":"1","content":"Selected Answer: B\nIt's B.","timestamp":"1678266180.0","poster":"Valcilio"},{"comment_id":"827885","timestamp":"1677840720.0","content":"Selected Answer: B\nAWS CloudTrail provides a history of AWS API calls made on the account. The Machine Learning team can use AWS CloudTrail to log Amazon SageMaker API calls to Amazon S3. They can then use CloudWatch to create alarms and receive notifications when the model is overfitting.\n\nTo ensure auditors can view the Amazon SageMaker log activity report, the team can add code to push a custom metric to Amazon CloudWatch. This provides a single place to view and analyze logs across all the services and resources in the environment.","upvote_count":"1","poster":"AjoseO"},{"content":"B. cloudwatch + metrics from sagemaker + sns https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/training-metrics.html#define-train-metrics","upvote_count":"4","poster":"sonalev419","comment_id":"324543","timestamp":"1635795000.0"},{"timestamp":"1635470220.0","content":"B requires the least amount of code and satisfies all conditions","poster":"ybad","upvote_count":"2","comment_id":"226760"},{"poster":"tochiebby","timestamp":"1634958000.0","comments":[{"poster":"Omar_Cascudo","upvote_count":"5","content":"It creates a metric for overfitting (accuracy of training data and accuracy of test data).","timestamp":"1635111120.0","comment_id":"211289"}],"content":"What does this line do?\n\n\"Add code to push a custom metric to Amazon CloudWatch\"","upvote_count":"1","comment_id":"175848"},{"poster":"jonclem","content":"Its not B. Why would you use CloudTrail?\n\nHaving used Lambda for API calls I'm inclined to agree with the original answer, C.","comments":[{"content":"https://docs.aws.amazon.com/sagemaker/latest/dg/logging-using-cloudtrail.html","upvote_count":"3","poster":"Pja1","comment_id":"160431","timestamp":"1634789340.0"},{"upvote_count":"3","timestamp":"1635110100.0","poster":"fhuadeen","content":"Because that is the only job of CloudTrail - to log actions taken on your AWS account. So why need a Lambda function to trigger it?","comment_id":"179916"}],"timestamp":"1634753880.0","upvote_count":"1","comment_id":"141066"},{"comment_id":"108588","timestamp":"1634216640.0","content":"B it is","upvote_count":"2","poster":"Antriksh"},{"comment_id":"102655","timestamp":"1633887720.0","upvote_count":"1","poster":"C10ud9","content":"B it is"},{"content":"Agree on B","timestamp":"1633375860.0","upvote_count":"2","poster":"lisiuyiu","comment_id":"74717"},{"comment_id":"67096","timestamp":"1632628260.0","upvote_count":"4","content":"ALL AWS Service's API calls are logged to CloudTrail automatically.","poster":"ac427"}],"answer_description":"","isMC":true,"question_images":[],"unix_timestamp":1573894920,"exam_id":26,"question_text":"A Machine Learning team uses Amazon SageMaker to train an Apache MXNet handwritten digit classifier model using a research dataset. The team wants to receive a notification when the model is overfitting. Auditors want to view the Amazon SageMaker log activity report to ensure there are no unauthorized API calls.\nWhat should the Machine Learning team do to address the requirements with the least amount of code and fewest steps?"}],"exam":{"name":"AWS Certified Machine Learning - Specialty","numberOfQuestions":369,"isBeta":false,"lastUpdated":"11 Apr 2025","provider":"Amazon","isMCOnly":false,"isImplemented":true,"id":26},"currentPage":61},"__N_SSP":true}