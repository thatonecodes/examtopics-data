{"pageProps":{"questions":[{"id":"jB6tV90wDKqj3HHlYzJC","answer":"B","question_images":[],"question_text":"A company has installed smart meters in all its customer locations. The smart meters measure power usage at 1-minute intervals and send the usage readings to a remote endpoint for collection. The company needs to create an endpoint that will receive the smart meter readings and store the readings in a database. The company wants to store the location ID and timestamp information.\n\nThe company wants to give its customers low-latency access to their current usage and historical usage on demand. The company expects demand to increase significantly. The solution must not impact performance or include downtime while scaling.\n\nWhich solution will meet these requirements MOST cost-effectively?","exam_id":24,"url":"https://www.examtopics.com/discussions/amazon/view/107439-exam-aws-certified-developer-associate-dva-c02-topic-1/","answers_community":["B (94%)","6%"],"answer_description":"","discussion":[{"timestamp":"1698237540.0","poster":"MrTee","upvote_count":"9","comment_id":"880379","content":"Selected Answer: B\nThe most cost-effective solution to meet these requirements would be to store the smart meter readings in an Amazon DynamoDB table and create a composite key using the location ID and timestamp columns"},{"timestamp":"1738601160.0","poster":"pinkynose","comment_id":"1351028","content":"Selected Answer: D\nI miss you Ruchi","upvote_count":"1"},{"content":"Selected Answer: B\nDynamoDB is purpose-built for low-latency, scalable storage of high-frequency, time-series data.\nComposite key design (location ID + timestamp) enables efficient querying.\nAutomatically scales without downtime or performance impact.","upvote_count":"1","timestamp":"1735047540.0","poster":"sumanshu","comment_id":"1331132"},{"comment_id":"1215332","timestamp":"1732246500.0","upvote_count":"1","poster":"65703c1","content":"Selected Answer: B\nB is the correct answer."},{"comment_id":"1119462","upvote_count":"2","content":"Selected Answer: B\nThis solution provides low-latency access to real-time and historical data, scales seamlessly to accommodate increased demand without downtime, and is likely to be more cost-effective than the alternatives for this specific use case. DynamoDB's managed service nature also reduces the administrative burden of managing the database.","poster":"SerialiDr","timestamp":"1720675680.0"},{"timestamp":"1713548940.0","upvote_count":"2","poster":"Gold07","content":"C is the right answer","comment_id":"1048166"},{"comment_id":"964336","timestamp":"1706337780.0","upvote_count":"3","poster":"Naj_64","content":"Selected Answer: B\nGoing with B. DynamoDB is the most cost-effective solution."},{"upvote_count":"2","poster":"jasper_pigeon","timestamp":"1706076720.0","content":"You need to use Athena as well to do partitoning","comment_id":"961111"},{"timestamp":"1701995160.0","comment_id":"917650","upvote_count":"1","poster":"HuiHsin","content":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-sort-keys.html"}],"answer_ET":"B","isMC":true,"answer_images":[],"question_id":516,"choices":{"C":"Store the smart meter readings in Amazon ElastiCache for Redis. Create a SortedSet key by using the location ID and timestamp columns. Use the columns to filter on the customers' data.","A":"Store the smart meter readings in an Amazon RDS database. Create an index on the location ID and timestamp columns. Use the columns to filter on the customers' data.","B":"Store the smart meter readings in an Amazon DynamoDB table. Create a composite key by using the location ID and timestamp columns. Use the columns to filter on the customers' data.","D":"Store the smart meter readings in Amazon S3. Partition the data by using the location ID and timestamp columns. Use Amazon Athena to filter on the customers' data."},"topic":"1","unix_timestamp":1682426340,"timestamp":"2023-04-25 14:39:00"},{"id":"NYLOPs5IAJ9zIohvBNlm","exam_id":24,"answer_ET":"B","unix_timestamp":1681738380,"url":"https://www.examtopics.com/discussions/amazon/view/106484-exam-aws-certified-developer-associate-dva-c02-topic-1/","question_text":"A company is building a serverless application that uses AWS Lambda functions. The company needs to create a set of test events to test Lambda functions in a development environment. The test events will be created once and then will be used by all the developers in an IAM developer group. The test events must be editable by any of the IAM users in the IAM developer group.\n\nWhich solution will meet these requirements?","question_id":517,"choices":{"C":"Create and store the test events in Amazon DynamoDB. Allow access to DynamoDB by using IAM roles.","D":"Create the test events. Configure the event sharing settings to make the test events private.","B":"Create the test events. Configure the event sharing settings to make the test events shareable.","A":"Create and store the test events in Amazon S3 as JSON objects. Allow S3 bucket access to all IAM users."},"timestamp":"2023-04-17 15:33:00","answer_images":[],"answer_description":"","isMC":true,"topic":"1","question_images":[],"answer":"B","discussion":[{"comment_id":"890394","timestamp":"1683334620.0","poster":"renekton","content":"Selected Answer: B\nUnder the \"Test\" tab there's an option. (Shareable)\nThis event is available to IAM users within the same account who have permissions to access and use shareable events.\n\nYou can check this by yourself on the Lambda\nAlso, here's a documentation \nhttps://docs.aws.amazon.com/lambda/latest/dg/testing-functions.html#creating-shareable-events","upvote_count":"30"},{"comment_id":"904373","poster":"delak","content":"Selected Answer: B\nSince March of this year, this is now possible to share test events within the same account with different users.","timestamp":"1684788840.0","upvote_count":"6"},{"comment_id":"1331134","content":"Selected Answer: B\n(Shareable events) directly leverages AWS Lambda's built-in functionality, reducing the need for custom storage and retrieval mechanisms.","timestamp":"1735047780.0","poster":"sumanshu","upvote_count":"2"},{"poster":"Anandesh","timestamp":"1721196600.0","upvote_count":"1","comment_id":"1249420","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/lambda/latest/dg/testing-functions.html"},{"poster":"65703c1","upvote_count":"1","comment_id":"1215334","timestamp":"1716341880.0","content":"Selected Answer: B\nB is the correct answer."},{"upvote_count":"1","content":"Selected Answer: A\nThis option is the most straightforward and aligns with AWS practices for managing shared resources like test events. IAM policies can be configured to grant the necessary permissions to the developer group, ensuring that all members can access and edit the test events stored in S3. This method leverages the scalability and security features of S3, along with the granular permission control provided by IAM, to meet the requirements.","comment_id":"1160022","timestamp":"1708975980.0","poster":"SerialiDr"},{"poster":"manngupta007","upvote_count":"1","content":"Answer: B\nhttps://aws.amazon.com/about-aws/whats-new/2022/03/aws-lambda-console-test-events/","timestamp":"1706536560.0","comment_id":"1135019"},{"timestamp":"1704958560.0","content":"Selected Answer: A\nThis option is viable. Amazon S3 can store JSON objects (test events), and access to these objects can be controlled through S3 bucket policies or IAM policies. By setting the correct permissions, all IAM users in the developer group can read and write to the S3 bucket, enabling them to edit and use the test events.","upvote_count":"1","poster":"SerialiDr","comment_id":"1119476"},{"timestamp":"1704422400.0","comment_id":"1114209","poster":"ez_24","content":"Selected Answer: A\nThe key Concept here is Sharing - test events in the Lambda console are for individual account can't be used by other developers","upvote_count":"1"},{"comment_id":"1105021","timestamp":"1703480640.0","content":"Selected Answer: A\nThis approach ensures that the test events are stored centrally in an S3 bucket where all IAM users within the developer group have access. By granting access to the S3 bucket to all IAM users, any user within the group can create, edit, and retrieve the test events, meeting the requirement for collaborative access and editing.\n\nOptions B and D don't directly address the need for IAM users to edit the test events; sharing settings might allow access, but they might not allow editing by all IAM users in the group. Option C, using DynamoDB, requires specific IAM role configurations for each user, which could become complex to manage and might not provide the same level of straightforward access and editing capability for all users within the IAM group.","upvote_count":"1","poster":"a_win"},{"upvote_count":"1","poster":"tqiu654","comment_id":"1085804","timestamp":"1701499860.0","content":"Selected Answer: A\nBased on ChatGPT:A"},{"upvote_count":"1","timestamp":"1698372480.0","comment_id":"1055055","content":"Selected Answer: B\nNo AWS Lambda, você pode criar eventos de teste no console da AWS para invocar sua função e ver a resposta. Esses eventos de teste podem ser salvos e compartilhados com outros usuários IAM. Ao definir as configurações de compartilhamento de eventos para tornar os eventos de teste compartilháveis, você permite que todos os desenvolvedores do grupo de desenvolvedores IAM os usem e editem.","poster":"Jonalb"},{"timestamp":"1690776300.0","content":"Would this not be C just because that's the only one that has the added security of the IAM roles?","poster":"DUBERS","comment_id":"967734","upvote_count":"1"},{"upvote_count":"1","comment_id":"878515","content":"Selected Answer: B\nthere is an option in lambda console to share the event with other users","poster":"Cloud_Cloud","timestamp":"1682262600.0"},{"poster":"MrTee","upvote_count":"3","timestamp":"1681999020.0","comment_id":"875674","content":"Selected Answer: A\nI meant to select A"},{"content":"Selected Answer: B\nTo create a set of test events that can be used by all developers in an IAM developer group and that are editable by any of the IAM users in the group, the company should create and store the test events in Amazon S3 as JSON objects and allow S3 bucket access to all IAM users (Option A). This will allow all developers in the IAM developer group to access and edit the test events as needed. The other options do not provide a way for multiple developers to access and edit the test events.","upvote_count":"1","comment_id":"875673","timestamp":"1681998960.0","poster":"MrTee"},{"comment_id":"874396","upvote_count":"1","poster":"Fyssy","timestamp":"1681892340.0","content":"Selected Answer: C\nUse roles. Not all IAM users"},{"content":"Selected Answer: A\nTo create test events that can be edited by any IAM user in a developer group, the company can create an Amazon S3 bucket and store the test event data as JSON files in the bucket.","poster":"Fyssy","comment_id":"872730","comments":[{"poster":"Naj_64","upvote_count":"3","content":"A is wrong. To edit a test you only need IAM permissions.\n\n\"To see, share, and edit shareable test events, you must have permissions for all of the following...\"\nhttps://docs.aws.amazon.com/lambda/latest/dg/testing-functions.html#creating-shareable-events\n\nI'll go with B.","comment_id":"947889","timestamp":"1688976420.0"}],"timestamp":"1681738380.0","upvote_count":"2"}],"answers_community":["B (81%)","Other"]},{"id":"g8brZ9gBcZmubZJHWJdH","question_text":"A developer is configuring an application's deployment environment in AWS CodePipeline. The application code is stored in a GitHub repository. The developer wants to ensure that the repository package's unit tests run in the new deployment environment. The developer has already set the pipeline's source provider to GitHub and has specified the repository and branch to use in the deployment.\n\nWhich combination of steps should the developer take next to meet these requirements with the LEAST overhead? (Choose two.)","url":"https://www.examtopics.com/discussions/amazon/view/107440-exam-aws-certified-developer-associate-dva-c02-topic-1/","unix_timestamp":1682426940,"question_id":518,"discussion":[{"content":"The correct answer is B and E\nThe buildspec file is a collection of build commands and related settings, in YAML format, that CodeBuild uses to run a build. By adding the build and test commands to the buildspec file, the developer can ensure that these commands are executed as part of the build process. Option E will ensure that the CodeBuild project is triggered as part of the pipeline and that the unit tests are run in the new deployment environment.","comment_id":"880388","poster":"MrTee","upvote_count":"21","timestamp":"1682426940.0"},{"comment_id":"906095","upvote_count":"14","content":"Selected Answer: BE\nFor those who just skim the question, keyword between D and E is \"unit tests run in the new deployment environment.\", which signifies a new stage should be created instead of just adding an action.","poster":"imvb88","timestamp":"1684952760.0"},{"timestamp":"1735049220.0","content":"Selected Answer: BE\nAWS CodeBuild is specifically designed for building and testing code in CI/CD pipelines. The buildspec file is where the developer can specify commands to build and run unit tests for the application package.\n\nA new stage after the source stage ensures that the unit tests run on the package retrieved from GitHub.","comment_id":"1331142","poster":"sumanshu","upvote_count":"3"},{"poster":"tsangckl","comment_id":"1231695","upvote_count":"3","timestamp":"1718595720.0","content":"This appear at 17 Jun exam"},{"upvote_count":"1","poster":"65703c1","content":"Selected Answer: BE\nBE is the correct answer.","timestamp":"1716342060.0","comment_id":"1215335"},{"comment_id":"1119485","poster":"SerialiDr","timestamp":"1704959100.0","upvote_count":"3","content":"Selected Answer: BE\nE. Add a new stage to the pipeline after the source stage: This is the correct step. The developer should add a new stage to the pipeline specifically for building and testing the code. Within this stage, an action should be added that specifies the AWS CodeBuild project (created in step B) as the action provider. The source artifact (code fetched from GitHub) should be specified as the action's input artifact.\n\nSo, the combination of steps that should be taken next to meet these requirements with the least overhead are:\n\nB. Create an AWS CodeBuild project. Add the repository package's build and test commands to the project's buildspec.\n\nE. Add a new stage to the pipeline after the source stage. Add an action to the new stage. Specify the newly created CodeBuild project as the action provider. Specify the source artifact as the action's input artifact."},{"poster":"LR2023","content":"Selected Answer: BD\nChoosing D as that is the least overhead. There is already a stage and you need to add an action test","comments":[{"timestamp":"1702094880.0","comment_id":"1091452","upvote_count":"2","poster":"LR2023","content":"Sorry will go with BE after ding more research as unit tests cannot be run in source stage as an action"}],"comment_id":"1091382","upvote_count":"1","timestamp":"1702081620.0"},{"poster":"marolisa","content":"B e D.\nhttps://docs.aws.amazon.com/pt_br/codebuild/latest/userguide/how-to-create-pipeline-add-test.html","comment_id":"983057","upvote_count":"3","timestamp":"1692222780.0"},{"comment_id":"892772","upvote_count":"3","poster":"aaok","content":"Selected Answer: BE\nAs MrTee says.","timestamp":"1683609540.0"}],"question_images":[],"answer_ET":"BE","answer_description":"","answer_images":[],"answer":"BE","topic":"1","answers_community":["BE (96%)","4%"],"exam_id":24,"isMC":true,"choices":{"D":"Add an action to the source stage. Specify the newly created project as the action provider. Specify the build artifact as the action's input artifact.","C":"Create an AWS CodeDeploy project. Add the repository package's build and test commands to the project's buildspec.","E":"Add a new stage to the pipeline after the source stage. Add an action to the new stage. Specify the newly created project as the action provider. Specify the source artifact as the action's input artifact.","B":"Create an AWS CodeBuild project. Add the repository package's build and test commands to the project's buildspec.","A":"Create an AWS CodeCommit project. Add the repository package's build and test commands to the project's buildspec."},"timestamp":"2023-04-25 14:49:00"},{"id":"DIeue9cZr3cz2UdnSz7X","url":"https://www.examtopics.com/discussions/amazon/view/102785-exam-aws-certified-developer-associate-dva-c02-topic-1/","exam_id":24,"answer_ET":"BD","answer":"BD","choices":{"C":"Update the application to use an AWS software development kit (AWS SDK) to make the requests.","B":"Retry the batch operation with exponential backoff and randomized delay.","A":"Retry the batch operation immediately.","E":"Increase the provisioned write capacity of the DynamoDB tables that the operation accesses.","D":"Increase the provisioned read capacity of the DynamoDB tables that the operation accesses."},"question_images":[],"topic":"1","timestamp":"2023-03-16 10:00:00","discussion":[{"upvote_count":"23","content":"Selected Answer: BD\n(B) If you delay the batch operation using exponential backoff, the individual requests in the batch are much more likely to succeed. \n(D) The most likely cause of a failed read or a failed write is throttling. For BatchGetItem, one or more of the tables in the batch request does not have enough provisioned read capacity to support the operation\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html#Programming.Errors.RetryAndBackoff","poster":"brandon87","comment_id":"858836","timestamp":"1680439200.0"},{"content":"Selected Answer: BC\nB & C\nhttps://docs.aws.amazon.com/general/latest/gr/api-retries.html","timestamp":"1679370960.0","comment_id":"845502","comments":[{"upvote_count":"1","content":"C already handles retries, why would want to to do that manually?","comment_id":"1135146","timestamp":"1706549160.0","poster":"konieczny69"}],"upvote_count":"19","poster":"Untamables"},{"content":"Selected Answer: BD\nBatchWriteItem\n• UnprocessedItems for failed write operations (exponential backoff or add WCU)\n BatchGetItem\n• UnprocessedKeys for failed read operations (exponential backoff or add RCU) \nSo answer is B & D","comment_id":"1340681","poster":"wtf3344","upvote_count":"2","timestamp":"1736918100.0"},{"comment_id":"1337706","poster":"Hasitha99","content":"Selected Answer: BC\nD also could be an answer.But is not the most suitable since there is a posibility of getting this result intermittently eventhouth we have necessary capacity.","upvote_count":"1","timestamp":"1736283000.0"},{"poster":"Arad","content":"Selected Answer: BC\nBC is correct answer.","upvote_count":"1","timestamp":"1735251480.0","comment_id":"1332129"},{"content":"Selected Answer: BD\nDynamoDB BatchGetItem API operation, the request fetches multiple items in one operation. However, DynamoDB has limits on the resources it allocates for processing requests. If it cannot process some items within a batch request due to these limits, it returns the processed items (the ones it successfully retrieved), and for the remaining items, it includes them in the UnprocessedKeys element in the response.","timestamp":"1734707640.0","comment_id":"1329525","upvote_count":"2","poster":"sumanshu"},{"poster":"trieudo","content":"Selected Answer: BC\nKeyword: increase, resiliency, application, response, UnprocessedKeys(Unprocessed due to be not enough infra)\n\n==> Discard A: horrible way, push so many traffic to busy system\n==> Discard D, E: Error due to huge traffic, it can be unlimited. Don't come from lacking resource ==> Your resource, your money is limit >< unpredicted huge traffic\n\nB: is good way to call x2 times after failed, call again after 1s -> 2s -> 4s -> 8s -> ... until success\n\nC: is intergrate method of 'B' option automaically inside SDK","upvote_count":"1","timestamp":"1733917380.0","comment_id":"1325003"},{"comment_id":"1299137","timestamp":"1729159020.0","upvote_count":"1","poster":"9d8dd9c","content":"Selected Answer: BD\nBD\nC handles retry but using SDK is not necessary here"},{"poster":"Phongsanth","comment_id":"938802","timestamp":"1727238120.0","content":"Selected Answer: BD\nI vote for B,D\nScroll down to the bottom of this page and you will see the reason. I paste some of words here.\n\nThe most likely cause of a failed read or a failed write is throttling. For BatchGetItem, one or more of the tables in the batch request does not have enough provisioned read capacity to support the operation. For BatchWriteItem, one or more of the tables does not have enough provisioned write capacity.\n\nIf DynamoDB returns any unprocessed items, you should retry the batch operation on those items. However, we strongly recommend that you use an exponential backoff algorithm. If you retry the batch operation immediately, the underlying read or write requests can still fail due to throttling on the individual tables. If you delay the batch operation using exponential backoff, the individual requests in the batch are much more likely to succeed.\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html","upvote_count":"3"},{"content":"Selected Answer: BD\nBackoff and Retry Strategy:\n\nImplement a backoff and retry strategy to prevent overwhelming the DynamoDB service with repeated requests.\nApply an exponential backoff algorithm, where you progressively increase the delay between each retry attempt.\nConsider implementing a maximum number of retries to avoid an infinite retry loop.\nFine-Tuning DynamoDB Provisioned Capacity:\n\nIf you consistently encounter unprocessed items during batch operations, it may indicate that your DynamoDB table's provisioned capacity is insufficient.\nMonitor the table's consumed capacity and adjust the provisioned capacity (read capacity units) accordingly to handle the load and reduce the occurrence of unprocessed items.\nBy implementing these steps, you can effectively handle unprocessed items returned by the BatchGetItem operation in DynamoDB and ensure that all items are processed successfully.","upvote_count":"2","comment_id":"944320","poster":"eberhe900","timestamp":"1727238120.0"},{"content":"Selected Answer: BD\nB & D.\nB is correct. Because in the question, it is mentioned that low-level API is being used.It means exponential backoff can be implemented manually.\nD is correct. Because there is a frequently keyword in the question. If UnprocessedKeys error occurs frequently, DynamoDB doesn't have enough capacity to process requests. So read capacity should be increased.","comment_id":"1096666","upvote_count":"3","timestamp":"1727238060.0","poster":"SherzodBek"},{"content":"Selected Answer: BD\nExponential backoff with randomized delay is a common technique used to handle transient failures and throttle errors in distributed systems like DynamoDB. This approach involves retrying the failed operation after waiting for an increasing amount of time, which helps reduce the load on the service and increases the likelihood of success during periods of high demand or throttling.\n\nIf the BatchGetItem operation frequently returns values in the UnprocessedKeys element, it indicates that the table's read capacity might be insufficient to handle the requested workload. By increasing the provisioned read capacity for the DynamoDB tables, the application can better handle the read throughput requirements and reduce the likelihood of encountering UnprocessedKeys in batch responses.\n\nAWS SDK might provide additional features and simplifications for making requests, it does not directly address the issue of UnprocessedKeys in batch responses. This option might be beneficial for improving code maintainability and leveraging SDK features however.","timestamp":"1727238060.0","poster":"TheFivePips","upvote_count":"1","comment_id":"1165599"},{"upvote_count":"1","timestamp":"1722756660.0","content":"BC is the correct answer.","comment_id":"1260553","poster":"phongnx8"},{"timestamp":"1721119320.0","content":"Selected Answer: BC\nhttps://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html#Programming.Errors.RetryAndBackoff","upvote_count":"1","comment_id":"1248772","poster":"Anandesh"},{"poster":"65703c1","timestamp":"1716296640.0","comment_id":"1214958","upvote_count":"1","content":"Selected Answer: BC\nBC is the correct answer."},{"poster":"drycleansing","content":"Selected Answer: BC\namong B C D, it is hard to say D copes with the problem directly I guess. Increasing RCU will affects the ratio of unprocessed items but that does not mean it handles the unprocessed items.","comment_id":"1191816","timestamp":"1712607420.0","upvote_count":"1"},{"content":"Selected Answer: BD\nB,D. La combinación de estrategias es ideal para este comportamiento","upvote_count":"1","timestamp":"1712440860.0","comment_id":"1190636","poster":"vinfo"},{"comment_id":"1183980","poster":"apa_1","upvote_count":"1","timestamp":"1711533420.0","content":"Selected Answer: BD\nB,D is correct"},{"upvote_count":"2","comment_id":"1183970","content":"Selected Answer: BC\nthe aws documentation for unprocessedkeys reads:\n\"If DynamoDB returns any unprocessed items, you should retry the batch operation on those items. However, we strongly recommend that you use an exponential backoff algorithm. If you retry the batch operation immediately, the underlying read or write requests can still fail due to throttling on the individual tables. If you delay the batch operation using exponential backoff, the individual requests in the batch are much more likely to succeed.\"\n\nTherefore I am taking the two options that provide this functionality.\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html","poster":"DeaconStJohn","timestamp":"1711532040.0"},{"comment_id":"1182556","poster":"ibratoev","timestamp":"1711376280.0","content":"Selected Answer: BD\nB & D is correct","upvote_count":"1"},{"upvote_count":"1","timestamp":"1710605100.0","content":"Selected Answer: BD\nAccording to AWS docs, the answer is B and D","comment_id":"1175081","poster":"HayLLlHuK"},{"comment_id":"1157987","timestamp":"1708784760.0","content":"BC ...No discussion","poster":"badsati","upvote_count":"1"},{"poster":"CrescentShared","comment_id":"1138115","content":"Why it's suggesting using SDK in the question from below link but not using C in this question?\nhttps://www.examtopics.com/discussions/amazon/view/96246-exam-aws-certified-developer-associate-topic-1-question-437/","timestamp":"1706850180.0","upvote_count":"2"},{"content":"Correct answer is B & D \nB- https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html#Programming.Errors.BatchOperations\nD - https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html","poster":"Sisanda_giiven","upvote_count":"2","timestamp":"1705923480.0","comment_id":"1128551"},{"upvote_count":"1","content":"Selected Answer: BC\nRetry with exponential backoff and randomized delay (Option B) helps prevent overwhelming the system with repeated immediate requests and increases the likelihood of successful retries during intermittent issues.\nUsing an AWS SDK (Option C) can provide built-in features for handling transient errors and retries, making the application more resilient to issues like UnprocessedKeys in batch responses.","comment_id":"1123644","poster":"Cambrian","timestamp":"1705350480.0"},{"poster":"Abdlhince","comment_id":"1060192","timestamp":"1698891120.0","upvote_count":"1","content":"Selected Answer: BC\nB. This is a good practice to handle throttling errors and avoid overwhelming the server with too many requests at the same time. Exponential backoff means increasing the waiting time between retries exponentially, such as 1 second, 2 seconds, 4 seconds, and so on. Randomized delay means adding some randomness to the waiting time, such as 1.2 seconds, 2.5 seconds, 3.8 seconds, and so on. This can help reduce the chance of collisions and spikes in the network traffic.\nC.This is a recommended way to interact with DynamoDB, as AWS SDKs provide high-level abstractions and convenience methods for working with DynamoDB. AWS SDKs also handle low-level details such as authentication, retry logic, error handling, and pagination for you."},{"poster":"ronn555","comment_id":"1060022","upvote_count":"2","timestamp":"1698867660.0","content":"BC\nThe question only states that there are UnprocessedKeys. \nThat means that the batch operation occurred correctly most of the time. It states that frequently\nthe batch contains more keys than can be returned with the present RCUs. \nThe does not state that any single key has violated the ProvisionedThroughputExceededException (in which case D would be necessary).\nSo D would only make it more performant because of less Retries. However B and C are examples of resilience"},{"poster":"Rameez1","content":"Selected Answer: BC\nOption B & C.","comment_id":"1049490","upvote_count":"1","timestamp":"1697892780.0"},{"content":"B&C\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html\nfirst thing first, this question ask for dealing with error. B&C\nin the doc, error handling has 2 part: 1. Error handling in your application(The AWS SDKs perform their own retries and error checking.) 2.Error retries and exponential backoff\n(If DynamoDB returns any unprocessed items, you should retry the batch operation on those items. However, we strongly recommend that you use an exponential backoff algorithm. If you retry the batch operation immediately, the underlying read or write requests can still fail due to throttling on the individual tables. If you delay the batch operation using exponential backoff, the individual requests in the batch are much more likely to succeed. which is b option) d is irrelevant","timestamp":"1697049360.0","comment_id":"1041027","poster":"ashley369534","upvote_count":"1"},{"poster":"cai123456","comment_id":"1018842","content":"between C and B I choose C because of the key work \"frequently\". using AWS SDK we update the code and do not need to retry frequently.","timestamp":"1695821880.0","upvote_count":"1"},{"timestamp":"1694424720.0","content":"Selected Answer: BD\nA single operation can retrieve up to 16 MB of data, which can contain as many as 100 items. BatchGetItem returns a partial result if the response size limit is exceeded, the table's provisioned throughput is exceeded, more than 1MB per partition is requested, or an internal processing failure occurs. If a partial result is returned, the operation returns a value for UnprocessedKeys. You can use this value to retry the operation starting with the next item to get.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html","comment_id":"1004587","upvote_count":"3","poster":"misa27"},{"poster":"chvtejaswi","upvote_count":"1","content":"Selected Answer: BD\nB and D","timestamp":"1694314080.0","comment_id":"1003637"},{"upvote_count":"9","poster":"mrsoa","timestamp":"1693080420.0","comment_id":"991024","content":"Selected Answer: BD\nB D \n\nFrom Stephan's maarek course \n\nBatchGetItem\n• Return items from one or more tables\n• Up to 100 items, up to 16 MB of data\n• Items are retrieved in parallel to minimize latency\n• UnprocessedKeys for failed read operations (exponential backoff or add RCU)"},{"timestamp":"1692972780.0","content":"Selected Answer: BC\nB. Retry with Exponential Backoff: When the batch response includes values in UnprocessedKeys, it indicates that some items could not be processed due to limitations like provisioned capacity or system overload. Retry the batch operation with an exponential backoff strategy, which means progressively increasing the time between retries. This helps prevent overwhelming the DynamoDB service and improves the chances of successfully processing the items in subsequent retries.\n\nC. Use AWS SDK: AWS SDKs provide built-in retry mechanisms that handle transient errors like UnprocessedKeys. When using an AWS SDK, you don't need to implement the retry logic yourself. The SDK will automatically handle retries with appropriate backoff strategies, making your application more resilient and reducing the burden of error handling.","upvote_count":"1","comment_id":"990145","poster":"love777"},{"poster":"aanataliya","content":"Selected Answer: BD\nB and D is correct answer. AWS SDK automatically takes care of both retry and exponential backoff. If we choose C, selecting only C will answer our question(no need of B) but We need to choose 2 answer. In addition, question doesnot specifically say to change core logic from low level api to SDK. by choosing B and D we can improve resiliency. \nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html#Programming.Errors.RetryAndBackoff","comment_id":"987261","timestamp":"1692695640.0","upvote_count":"6"},{"upvote_count":"1","comment_id":"983210","timestamp":"1692246240.0","content":"Selected Answer: BC\nIf DynamoDB returns any unprocessed items, you should retry the batch operation on those items. However, we strongly recommend that you use an exponential backoff algorithm. If you retry the batch operation immediately, the underlying read or write requests can still fail due to throttling on the individual tables. If you delay the batch operation using exponential backoff, the individual requests in the batch are much more likely to succeed.\n\nthus b) and c) as the \"AWS SDK implements an exponential backoff algorithm for better flow control\" https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html#Programming.Errors.RetryAndBackoff:~:text=each%20AWS%20SDK%20implements%20an%20exponential%20backoff%20algorithm%20for%20better%20flow%20control","poster":"ninomfr64"},{"content":"Selected Answer: BC\nB: for batch, exponential backoff looks answer\nC: direct to DynamoDB do not recommend","comment_id":"971616","upvote_count":"1","poster":"jipark","timestamp":"1691115000.0"},{"content":"Selected Answer: BD\nC. Using an AWS SDK can simplify making requests and handling responses, but on its own, it does not address the underlying issue of unprocessed keys.","poster":"KillThemWithKindness","comment_id":"951780","timestamp":"1689361440.0","upvote_count":"2"},{"comment_id":"951370","upvote_count":"3","content":"Selected Answer: BD\n(B) If you delay the batch operation using exponential backoff, the individual requests in the batch are much more likely to succeed.\n(D) The most likely cause of a failed read or a failed write is throttling. For BatchGetItem, one or more of the tables in the batch request does not have enough provisioned read capacity to support the operation\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html#Programming.Errors.RetryAndBackoff","timestamp":"1689323100.0","poster":"awsstark"},{"poster":"tttamtttam","comment_id":"948901","timestamp":"1689072780.0","upvote_count":"4","content":"Selected Answer: BC\nThe hint is it is using the low-level API operation currently. Using AWS SDK, retries and optimization will be done by the SDK."},{"comment_id":"944908","poster":"MrPie","upvote_count":"1","content":"Selected Answer: BC\nD don't solve the problem","timestamp":"1688666640.0"},{"timestamp":"1688149380.0","comment_id":"939318","poster":"Pupina","upvote_count":"3","content":"I completely agree on D. My question is why develop the exponential backoff algorithm (that is B) instead of using the automatic functionality of backoff in SDK (option C). Why BD instead of CD"},{"timestamp":"1688070060.0","comment_id":"938551","poster":"Pupina","upvote_count":"2","content":"Why B instead of C? Each AWS SDK implements retry logic automatically. Most AWS SDKs now support exponential backoff and jitter as part of their retry behavior\nThen D to increase capacity https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TroubleshootingThrottling.html\nC&D"},{"timestamp":"1687428420.0","content":"Selected Answer: BC\nRetry behavior includes settings regarding how the SDKs attempt to recover from failures resulting from requests made to AWS services.","upvote_count":"1","comment_id":"930368","poster":"MatthewHuiii"},{"timestamp":"1686842040.0","comment_id":"924311","poster":"Kaushik287","upvote_count":"3","comments":[{"poster":"Naj_64","upvote_count":"2","content":"\"The most likely cause of a failed read or a failed write is throttling. For BatchGetItem, one or more of the tables in the batch request does not have enough provisioned read capacity to support the operation.\"\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html#Programming.Errors.BatchOperations","timestamp":"1687089720.0","comment_id":"926665"}],"content":"BD is correct answer. https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html#Programming.Errors.BatchOperations"},{"content":"Selected Answer: BC\nCorrect answer: B & C\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html#Programming.Errors.RetryAndBackoff\n➜ \"The AWS SDKs implement automatic retry logic and exponential backoff.\"\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html#Programming.Errors.BatchOperations\n➜ \"If DynamoDB returns any unprocessed items, you should retry the batch operation on those items. However, we strongly recommend that you use an exponential backoff algorithm. If you retry the batch operation immediately, the underlying read or write requests can still fail due to throttling on the individual tables. If you delay the batch operation using exponential backoff, the individual requests in the batch are much more likely to succeed.\"","timestamp":"1686296220.0","poster":"tuongthuy","upvote_count":"1","comment_id":"919053"},{"upvote_count":"2","timestamp":"1685441820.0","comment_id":"910144","poster":"ezredame","content":"Selected Answer: BC\nIf DynamoDB returns any unprocessed items, you should retry the batch operation on those items. AWS SDKs typically include functionality for retrying requests with exponential backoff.\n\nIf questions indicate ProvisionedThroughputExceededException, then we should try to RCU, WCU."},{"timestamp":"1684363680.0","poster":"loctong","comment_id":"900540","upvote_count":"3","content":"Selected Answer: BD\nC could be beneficial but not necessary. AWS SDKs typically include functionality for retrying requests with exponential backoff and some of them do it automatically, but it is not a direct solution to the UnprocessedKeys problem."},{"timestamp":"1683646080.0","upvote_count":"1","poster":"Devon_Fazekas","content":"While increasing the provisioned read capacity of the DynamoDB table can help reduce the likelihood of encountering UnprocessedKeys errors due to throttling, it does not directly address the issue of how to handle the UnprocessedKeys responses. To handle UnprocessedKeys, the application should implement retry logic with exponential backoff and randomized delay, as well as consider other solutions like pagination, limiting the number of items per batch, and optimizing the request patterns.","comment_id":"893251"},{"poster":"Nagendhar","comment_id":"892580","timestamp":"1683591180.0","upvote_count":"1","content":"Ans: B & C\n\nThe two actions that the developer should take to increase the resiliency of the application when the batch response includes values in UnprocessedKeys are B, \"Retry the batch operation with exponential backoff and randomized delay,\" and C, \"Update the application to use an AWS software development kit (AWS SDK) to make the requests.\""},{"comment_id":"890866","content":"Got this question in exam.","timestamp":"1683390180.0","poster":"geekdamsel","upvote_count":"2"},{"poster":"rlnd2000","timestamp":"1682956260.0","upvote_count":"2","comment_id":"886451","content":"Selected Answer: BD\nI go with BD. \n\nFrom https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html\n\nIf DynamoDB returns any unprocessed items, you should retry the batch operation on those items. However, we strongly recommend that you use an exponential backoff algorithm. If you retry the batch operation immediately, the underlying read or write requests can still fail due to throttling on the individual tables. If you delay the batch operation using exponential backoff, the individual requests in the batch are much more likely to succeed."},{"timestamp":"1682705880.0","poster":"ihebchorfi","upvote_count":"2","comment_id":"883813","content":"Selected Answer: BD\nI would go with BD.\nNote that CD is also correct, but would require the developer to modify his application code."},{"upvote_count":"2","content":"B and C","comment_id":"879775","timestamp":"1682379120.0","poster":"MrTee"},{"upvote_count":"2","content":"C or AWS SDK usage does exponential back off by default so if B is right , C essentially does the same thing so should be correct . B and C","timestamp":"1681981980.0","poster":"Rpod","comment_id":"875438"},{"content":"the answers do not apply\none gets the unprocessedkeys only if the data exceeds 16mb\nyou just need to retry again","poster":"alecs_adam","timestamp":"1681081920.0","comment_id":"865878","upvote_count":"1"},{"poster":"Krok","comment_id":"861816","timestamp":"1680677340.0","upvote_count":"3","content":"Selected Answer: BC\nI think that B and C."},{"content":"Selected Answer: CD\nLook at the question \"Which actions should the developer take to\" -> BCD is right but B no need to do.","poster":"anhike","upvote_count":"1","timestamp":"1680060120.0","comment_id":"853960"},{"timestamp":"1679895480.0","comment_id":"851757","poster":"Kristijan92","upvote_count":"2","content":"Selected Answer: CD\nThe AWS SDKs implement automatic retry logic and exponential backoff.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html#Programming.Errors.RetryAndBackoff"},{"upvote_count":"3","content":"I think it should B and C since C offers what we need to do for B","timestamp":"1679308560.0","comment_id":"844760","poster":"prabhay786"},{"poster":"aragon_saa","comment_id":"840720","timestamp":"1678957200.0","content":"BD\nhttps://www.examtopics.com/discussions/amazon/view/88817-exam-aws-certified-developer-associate-topic-1-question-273/","upvote_count":"3"}],"answers_community":["BD (60%)","BC (38%)","3%"],"isMC":true,"question_text":"A developer has an application that makes batch requests directly to Amazon DynamoDB by using the BatchGetItem low-level API operation. The responses frequently return values in the UnprocessedKeys element.\nWhich actions should the developer take to increase the resiliency of the application when the batch response includes values in UnprocessedKeys? (Choose two.)","answer_images":[],"question_id":519,"unix_timestamp":1678957200,"answer_description":""},{"id":"RD2AkIcoTtOmbGrJ7vl2","isMC":true,"answer":"A","question_id":520,"question_text":"An engineer created an A/B test of a new feature on an Amazon CloudWatch Evidently project. The engineer configured two variations of the feature (Variation A and Variation B) for the test. The engineer wants to work exclusively with Variation A. The engineer needs to make updates so that Variation A is the only variation that appears when the engineer hits the application's endpoint.\n\nWhich solution will meet this requirement?","choices":{"C":"Add an experiment to the project. Set the identifier of the experiment to Variation B. Set the variation to 0%.","A":"Add an override to the feature. Set the identifier of the override to the engineer's user ID. Set the variation to Variation A.","D":"Add an experiment to the project. Set the identifier of the experiment to the AWS account's account ISet the variation to Variation A.","B":"Add an override to the feature. Set the identifier of the override to Variation A. Set the variation to 100%."},"answer_images":[],"discussion":[{"upvote_count":"15","comment_id":"872770","poster":"Fyssy","timestamp":"1681740840.0","content":"Selected Answer: A\nOverrides let you pre-define the variation for selected users. to always receive the editable variation. https://aws.amazon.com/blogs/aws/cloudwatch-evidently/","comments":[{"content":"the key looks \"override\" and allow only \"userID\"","comment_id":"969053","upvote_count":"1","poster":"jipark","timestamp":"1690892460.0"}]},{"poster":"Baba_Eni","content":"Selected Answer: A\nCheck Bullet point 9 in the link below\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Evidently-newfeature.html","timestamp":"1686567480.0","comment_id":"921359","upvote_count":"10"},{"poster":"sumanshu","timestamp":"1735049580.0","upvote_count":"2","comment_id":"1331144","content":"Selected Answer: A\nAn override allows you to force specific variations of a feature for a subset of users based on identifiers like user ID. This is useful for testing or debugging without affecting other users or the experiment as a whole."},{"timestamp":"1730299200.0","content":"Selected Answer: A\nQuote: To specify that certain users always see a certain variation, choose Overrides, Add override. Then, specify a user by entering their user ID, account ID, or some other identifier in Identifier, and specify which variation they should see.\n\nThis can be useful for members of your own testing team or other internal users when you want to make sure they see a specific variation. The sessions of users who are assigned overrides do not contribute to launch or experiment metrics.","upvote_count":"1","poster":"nbxyzd","comment_id":"1305035"},{"comment_id":"1232389","poster":"michele740","upvote_count":"1","content":"Therefore, option A is the best choice to meet the requirement of ensuring the engineer exclusively sees Variation A.","timestamp":"1718709180.0"},{"upvote_count":"1","content":"Selected Answer: A\nA is the correct answer.","comment_id":"1215336","timestamp":"1716342180.0","poster":"65703c1"},{"upvote_count":"1","comment_id":"1193950","poster":"badsati","timestamp":"1712858040.0","content":"Selected Answer: A\nBy adding an override to the feature and setting the identifier to the engineer's user ID, the engineer ensures that only their requests are directed to Variation A.\nSetting the variation to Variation A explicitly assigns the desired variation to the engineer's requests, effectively ensuring they only experience Variation A.\nTherefore, the correct solution is Option A."},{"upvote_count":"2","poster":"hsinchang","timestamp":"1694430660.0","comment_id":"1004663","content":"Set the variation to 0% or 100% makes no sense. Plus, the identifier should not be an account."},{"upvote_count":"1","timestamp":"1689576300.0","comment_id":"953899","poster":"ancomedian","content":"Selected Answer: A\nYou have to give identifier"}],"topic":"1","exam_id":24,"unix_timestamp":1681740840,"answer_ET":"A","answers_community":["A (100%)"],"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/106488-exam-aws-certified-developer-associate-dva-c02-topic-1/","answer_description":"","timestamp":"2023-04-17 16:14:00"}],"exam":{"lastUpdated":"11 Apr 2025","isMCOnly":true,"isBeta":false,"isImplemented":true,"name":"AWS Certified Developer - Associate DVA-C02","id":24,"numberOfQuestions":551,"provider":"Amazon"},"currentPage":104},"__N_SSP":true}