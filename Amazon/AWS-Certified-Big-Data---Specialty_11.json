{"pageProps":{"questions":[{"id":"Bqnx9q2yw0htJO4M4DL8","answers_community":[],"choices":{"A":"Use Amazon DynamoDB as the data store and use strongly consistent reads when necessary.","B":"Use an Amazon Relational Database Service (RDS) instance sized to meet the maximum anticipated transaction rate and with the High Availability option enabled.","D":"Use Amazon Redshift with synchronous replication to Amazon Simple Storage Service (S3) and row-level locking for strong consistency.","C":"Deploy a NoSQL data store on top of an Amazon Elastic MapReduce (EMR) cluster, and select the HDFS High Durability option."},"timestamp":"2019-09-18 07:29:00","url":"https://www.examtopics.com/discussions/amazon/view/5355-exam-aws-certified-big-data-specialty-topic-1-question-55/","question_text":"An organization is designing an application architecture. The application will have over 100 TB of data and will support transactions that arrive at rates from hundreds per second to tens of thousands per second, depending on the day of the week and time of the day. All transaction data, must be durably and reliably stored. Certain read operations must be performed with strong consistency.\nWhich solution meets these requirements?","answer":"A","answer_description":"","question_images":[],"answer_ET":"A","unix_timestamp":1568784540,"topic":"1","question_id":51,"exam_id":17,"isMC":true,"answer_images":[],"discussion":[{"poster":"jove","upvote_count":"1","content":"A it is","timestamp":"1635176100.0","comment_id":"142714"},{"comment_id":"52374","content":"my selection A","timestamp":"1634718840.0","poster":"san2020","upvote_count":"4","comments":[{"upvote_count":"4","poster":"Corram","content":"a discussion section without a san2020 selection would not be the same :D","comment_id":"101084","timestamp":"1634940420.0"}]},{"poster":"cybe001","comment_id":"19367","content":"Why not B?","upvote_count":"1","comments":[{"comment_id":"47143","poster":"AdamSmith","upvote_count":"2","timestamp":"1634424840.0","content":"Limit of RDS is 100TB, the data is over 100TB so DynamoDB"},{"content":"Proobably becouse is not a best practice \"sized to meet the maximum anticipated transaction rate\"...and also becouse if I read \"Strongly Consistent or Eventually Consistent\" my mind goes directlytoi DynamoDB :)","poster":"Erso","comment_id":"22925","timestamp":"1633755540.0","upvote_count":"3"},{"poster":"Kuntazulu","timestamp":"1633944840.0","comment_id":"36548","content":"100TB is too much for RDS. 64TB limit for Aurora. DynamoDB is limitless and is OLTP","upvote_count":"6"}],"timestamp":"1633739100.0"},{"timestamp":"1633732440.0","comment_id":"11556","poster":"exams","upvote_count":"4","content":"yes.. A"}]},{"id":"EKnbfR8bwemVlhuZgyb5","timestamp":"2019-09-10 23:37:00","answer":"B","answer_images":[],"isMC":true,"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/5023-exam-aws-certified-big-data-specialty-topic-1-question-56/","topic":"1","question_id":52,"discussion":[{"upvote_count":"9","poster":"jack870005","timestamp":"1634249100.0","content":"B is correct. Refer to https://aws.amazon.com/cn/blogs/aws/send-us-that-data/ - check the screenshots listed in the article. During manifest file creation we can specify \"log file prefix\"","comment_id":"24713"},{"comment_id":"52375","upvote_count":"7","poster":"san2020","timestamp":"1634619240.0","content":"my selection B"},{"poster":"exams","timestamp":"1633303440.0","upvote_count":"1","comment_id":"11563","content":"Thoughts b?"},{"content":"Any thoughts on C?","comments":[{"upvote_count":"1","content":"checksum is already included in the log file : https://aws.amazon.com/cn/blogs/aws/send-us-that-data/","comment_id":"11214","poster":"apertus","timestamp":"1632423060.0"},{"timestamp":"1635104880.0","upvote_count":"2","comments":[{"poster":"Corram","content":"(checksums = MD5 hashes)","timestamp":"1635372240.0","comment_id":"101097","upvote_count":"1"}],"comment_id":"101095","poster":"Corram","content":"Think C should actually work in practice. Howeer, two theoritcal issues come to my mind:\n1. MD5 hashes could coincide accidentally, but chances for that are really, reaaaaally low.\n2. MD5 hashes would also coincide if you sent the same log files twice (why would you tho).\nOverall, why provide cryptographic file names when you can just specify a log file prefix instead. B is better."}],"timestamp":"1632395160.0","upvote_count":"1","comment_id":"10504","poster":"Hitu"}],"question_text":"A company generates a large number of files each month and needs to use AWS import/export to move these files into Amazon S3 storage. To satisfy the auditors, the company needs to keep a record of which files were imported into Amazon S3.\nWhat is a low-cost way to create a unique log for each import job?","answer_description":"","choices":{"B":"Use the log file prefix in the import/export manifest files to create a unique log file in Amazon S3 for each import.","C":"Use the log file checksum in the import/export manifest files to create a unique log file in Amazon S3 for each import.","A":"Use the same log file prefix in the import/export manifest files to create a versioned log file in Amazon S3 for all imports.","D":"Use a script to iterate over files in Amazon S3 to generate a log after each import/export job."},"answer_ET":"B","unix_timestamp":1568151420,"exam_id":17,"answers_community":[]},{"id":"f1zAkuqFKM3OVwegUpVF","answers_community":[],"unix_timestamp":1566251640,"choices":{"A":"User profiles (age, gender, income, occupation)","D":"Quarterly results","C":"Each user time series events in the past 3 months","B":"Last user session"},"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/3834-exam-aws-certified-big-data-specialty-topic-1-question-57/","discussion":[{"upvote_count":"11","poster":"san2020","timestamp":"1635768240.0","comment_id":"52376","content":"my selection C"},{"poster":"Abhi09","comment_id":"151319","timestamp":"1636252020.0","content":"it's yearly subscription; and the time series data is only for 3 months. Is this not a mis-match ? We should at least be reviewing a years data to make prediction ?","upvote_count":"1"},{"comment_id":"38823","timestamp":"1635076440.0","poster":"jay1ram2","content":"The ask is \"On which basis should this binary classification model be built\". To build a model we need recent utilization data. I will go with C","upvote_count":"2"},{"upvote_count":"4","comment_id":"21959","comments":[{"timestamp":"1635197160.0","poster":"AdamSmith","upvote_count":"18","content":"listen to this guy and say bye bye to your $300","comments":[{"comment_id":"101099","content":"better listen to san2020, his selections are almost always spot-on","poster":"Corram","timestamp":"1635919920.0","upvote_count":"2"}],"comment_id":"47147"}],"content":"A. User profiles.\n\nhttps://aws.amazon.com/blogs/machine-learning/predicting-customer-churn-with-amazon-machine-learning/\nMobile operators have historical records on which customers ultimately ended up churning and which continued using the service. \nWe can use this historical information to construct an ML model of one mobile operatorâ€™s churn using a process called training. \n\nAfter training the model, we can pass the profile information of an arbitrary customer (the same profile information that we used to train the model) to the model, and have the model predict whether this customer is going to churn.","poster":"viduvivek","timestamp":"1635025680.0"},{"comment_id":"8744","comments":[{"content":"Would you agree C still?","comments":[{"comment_id":"9691","timestamp":"1634331420.0","upvote_count":"7","content":"C is correct","poster":"Jialu"}],"timestamp":"1634175180.0","comment_id":"8774","poster":"mattyb123","upvote_count":"2"}],"poster":"muhsin","timestamp":"1633281420.0","content":"to predict churn, we need to have user pattern for the last period. profile info can be added also","upvote_count":"2"},{"comment_id":"7556","comments":[{"comments":[{"comment_id":"7834","upvote_count":"1","content":"https://aws.amazon.com/blogs/machine-learning/predicting-customer-churn-with-amazon-machine-learning/","timestamp":"1632847860.0","poster":"mattyb123"}],"content":"Since it's a binary model responses can only be true or false. Due to that reason i thought time series is correct. For example sample data could be number of days offline, number of support call, number of support emails etc instead of user information","timestamp":"1632490200.0","poster":"mattyb123","comment_id":"7716","upvote_count":"2"}],"poster":"jlpl","content":"a? thoughts?","timestamp":"1632443700.0","upvote_count":"1"}],"answer_images":[],"question_images":[],"topic":"1","question_text":"A company needs a churn prevention model to predict which customers will NOT renew their yearly subscription to the companys service. The company plans to provide these customers with a promotional offer. A binary classification model that uses Amazon Machine Learning is required.\nOn which basis should this binary classification model be built?","answer_ET":"C","question_id":53,"answer":"C","exam_id":17,"timestamp":"2019-08-19 23:54:00","isMC":true},{"id":"0cLIAl3SUnm7BC3CQcDO","answer_description":"","answers_community":[],"url":"https://www.examtopics.com/discussions/amazon/view/5359-exam-aws-certified-big-data-specialty-topic-1-question-58/","exam_id":17,"unix_timestamp":1568787720,"question_images":[],"answer_images":[],"question_text":"A company with a support organization needs support engineers to be able to search historic cases to provide fast responses on new issues raised. The company has forwarded all support messages into an Amazon\nKinesis Stream. This meets a company objective of using only managed services to reduce operational overhead.\nThe company needs an appropriate architecture that allows support engineers to search on historic cases and find similar issues and their associated responses.\nWhich AWS Lambda action is most appropriate?","timestamp":"2019-09-18 08:22:00","answer":"A","isMC":true,"question_id":54,"choices":{"C":"Write data as JSON into Amazon DynamoDB with primary and secondary indexes.","B":"Stem and tokenize the input and store the results into Amazon ElastiCache.","D":"Aggregate feedback in Amazon S3 using a columnar format with partitioning.","A":"Ingest and index the content into an Amazon Elasticsearch domain."},"answer_ET":"A","topic":"1","discussion":[{"upvote_count":"7","timestamp":"1632519540.0","comment_id":"11564","poster":"exams","content":"Agree with A"},{"poster":"san2020","comment_id":"52377","content":"my selection A","upvote_count":"6","timestamp":"1633504860.0"},{"poster":"awane","comment_id":"104525","timestamp":"1633998240.0","upvote_count":"1","content":"Agree with A"},{"poster":"Yesven","content":"This question seems to be a straight forward. Will these kind of questions appear in real test ?","comment_id":"45998","comments":[{"content":"Nope. Mostly not possible. This is just for practice.","timestamp":"1634131980.0","poster":"Kitty0403","upvote_count":"1","comment_id":"118029"},{"upvote_count":"1","comment_id":"124355","poster":"yogesh88","content":"Just attempted exam - same question :D","timestamp":"1634557080.0","comments":[{"content":"I selected A","upvote_count":"1","timestamp":"1634741580.0","comment_id":"124357","poster":"yogesh88"}]}],"timestamp":"1633325880.0","upvote_count":"2"}]},{"id":"hlzwnHgkv6hmNMaTMvZ8","unix_timestamp":1565738220,"question_id":55,"answer":"D","timestamp":"2019-08-14 01:17:00","isMC":true,"discussion":[{"upvote_count":"18","content":"Correct Answer is B as S3 Server Side Encryption with S3 Managed Keys provide encryption. S3 ACLs allows fine grained control access and S3 to access logs would help provide traceability across all tools.\n\nUse Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3) â€“ Each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data.\n\nOption C is wrong as with Client-Side Encryption, the users must have the keys to decrypt the data.\n\nWhen downloading an objectâ€”The client downloads the encrypted object from Amazon S3. Using the material description from the object's metadata, the client determines which master key to use to decrypt the data key. The client uses that master key to decrypt the data key and then uses the data key to decrypt the object.\n\nOptions A & D are wrong as KMS Grants are mainly to provide access to the KMS keys. There is not mention of fine grained control over the S3 objects","poster":"aws4","comment_id":"26502","timestamp":"1634174940.0"},{"timestamp":"1634153340.0","comment_id":"23896","poster":"WWODIN","upvote_count":"10","content":"Why not B?\nAccess from AWS Console: SSE only, A or B\nFine grained access control: S3 ACL or Bucket List\nAccess Log: S3 Access Log\nanswer is B?"},{"timestamp":"1636195080.0","comment_id":"337510","content":"Wrong A&D: AWS KMS Grants are for accessing keys.\nNot C: There is no request for any additional protection\nOK B: Amazon S3 to access log should have enough information\nexample:\n79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be awsexamplebucket1 [06/Feb/2019:00:00:38 +0000] 192.0.2.3 79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be 3E57427F3EXAMPLE REST.GET.VERSIONING - \"GET /awsexamplebucket1?versioning HTTP/1.1\" 200 - 113 - 7 - \"-\" \"S3Console/0.4\" - s9lzHYrFp76ZVxRcpX9+5cjAnEH2ROuNkd2BHfIa6UkFVdtjf5mKR3/eTPFvsiP/XV/VLi31234= SigV2 ECDHE-RSA-AES128-GCM-SHA256 AuthHeader awsexamplebucket1.s3.us-west-1.amazonaws.com TLSV1.1","upvote_count":"1","poster":"DerekKey"},{"poster":"jkoffee","timestamp":"1636075740.0","comment_id":"109786","upvote_count":"1","content":"B my thinking\nhttps://docs.amazonaws.cn/en_us/sdk-for-java/v1/developer-guide/examples-s3-access-permissions.html"},{"comment_id":"103751","upvote_count":"2","timestamp":"1635500580.0","content":"b is correct","poster":"k115"},{"upvote_count":"2","content":"Answer is A. The user is not uploading, coming through SAML, that means from some other system with not necessarily aws. Server side encryption makes sense.","timestamp":"1635245400.0","poster":"srirampc","comment_id":"82573"},{"comment_id":"77478","poster":"Bulti","content":"Moreover if you encrypt the objects using SSE-C and Client side encryption, the data key will not be stored along with the object in S3 as its metadata. As a result you will not be able to access these object from AWS Console as AWS will not be able to decrypt these objects before returning to the console.","timestamp":"1635140940.0","upvote_count":"2"},{"timestamp":"1635102780.0","content":"A and D will record access log to the objects in S3 bucket using CloudTrail but it won't allow creation of object level permissions because it uses KMS Grant which only provide granular access to CMS key used for KMS encryption. \nOption C- Client side encryption is more expensive to setup than using Out of the box SSE-S3 encryption and therefore the correct answer is B.","upvote_count":"3","poster":"Bulti","comment_id":"75733"},{"timestamp":"1634898660.0","content":"D. It is recommended to use CloudTrail https://docs.aws.amazon.com/AmazonS3/latest/dev/using-s3-access-logs-to-identify-requests.html.\nClient-side encryption will ensure data is encrypted before upload, better than A","poster":"susan8840","upvote_count":"2","comment_id":"74064"},{"comment_id":"71927","timestamp":"1634655900.0","upvote_count":"1","poster":"jiedee","comments":[{"timestamp":"1634712780.0","upvote_count":"2","comment_id":"72495","poster":"jiedee","content":"Sorry for the earlier post. It seems that access logs do tell you \"console/cli\", so it should be B."}],"content":"It should be A or D.\nB and C are out because access logs won't tell you whether you are trying to access a object via cli or console. you have to use cloudtrail to do that.\nI am just not sure sse or cse, any thoughts?"},{"timestamp":"1634419680.0","poster":"san2020","comment_id":"52378","upvote_count":"4","content":"my selection B"},{"comment_id":"45531","content":"A is the answer \nhttps://docs.aws.amazon.com/whitepapers/latest/building-data-lakes/securing-protecting-managing-data.html","upvote_count":"2","poster":"kalpanareddy","timestamp":"1634263080.0"},{"poster":"viduvivek","comment_id":"21978","timestamp":"1633601160.0","content":"A is the Answer.\n\nServer side encryption is at object level, while client side encryption is at data level. \n\nRefer : https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\n\nServer-side encryption is the encryption of data at its destination by the application or service that receives it. Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it. As long as you authenticate your request and you have access permissions, there is no difference in the way you access encrypted or unencrypted objects.\n\nClient side encryption is the act of encrypting data before sending it to S3.","upvote_count":"2"},{"comment_id":"18264","upvote_count":"1","timestamp":"1633465620.0","content":"Should be C","poster":"asadao"},{"content":"Should be C.","poster":"exams","upvote_count":"1","timestamp":"1633405200.0","comment_id":"11565"},{"timestamp":"1633131960.0","comment_id":"11215","content":"Should be A. If client-side encryption then you cannot access the data in AWS Console?","poster":"apertus","upvote_count":"3"},{"content":"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html D is correct. Creating your own CMK gives you more flexibility, including the ability to create, rotate, disable, and define access controls, and to audit the encryption keys used to protect your data.","comments":[{"upvote_count":"6","content":"Your explanation is good but still allows for A. And Server Side Encryption is to be preferred, since it is simpler and there is no good reason not to use it.\n\nmy selection A","timestamp":"1635379380.0","poster":"Corram","comment_id":"101103"}],"comment_id":"6776","upvote_count":"3","poster":"mattyb123","timestamp":"1632189300.0"},{"timestamp":"1632166500.0","comment_id":"6768","poster":"Jialu","comments":[{"comment_id":"101102","content":"The solutions are completely unreliable, that's why. Always check the discussions for the actual correct answers.","poster":"Corram","upvote_count":"2","timestamp":"1635297000.0"},{"poster":"muhsin","comments":[{"content":"Server-Side Encryption â€“ Request Amazon S3 to encrypt your object before saving it on disks in its data centers and then decrypt it when you download the objects.\n\nClient-Side Encryption â€“ Encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.","timestamp":"1633093800.0","poster":"muhsin","upvote_count":"1","comment_id":"8748"}],"timestamp":"1632261000.0","upvote_count":"2","content":"maybe to encrypt the data before sending to S3. because the data is sensitive.","comment_id":"8747"}],"content":"Why it is client-side encryption , not server-side encryption?","upvote_count":"1"}],"choices":{"C":"Use Amazon S3 Client-Side Encryption with Client-Side Master Key. Set Amazon S3 ACLs to allow access to specific elements of the platform. Use Amazon S3 to access logs for auditing.","D":"Use Amazon S3 Client-Side Encryption with AWS KMS-Managed Keys for storing data. Use AWS KMS Grants to allow access to specific elements of the platform. Use AWS CloudTrail for auditing.","A":"Use Amazon S3 Server-Side Encryption with AWS KMS-Managed Keys for storing data. Use AWS KMS Grants to allow access to specific elements of the platform. Use AWS CloudTrail for auditing.","B":"Use Amazon S3 Server-Side Encryption with Amazon S3-Managed Keys. Set Amazon S3 ACLs to allow access to specific elements of the platform. Use Amazon S3 to access logs for auditing."},"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/3563-exam-aws-certified-big-data-specialty-topic-1-question-59/","exam_id":17,"question_images":[],"question_text":"A solutions architect works for a company that has a data lake based on a central Amazon S3 bucket. The data contains sensitive information. The architect must be able to specify exactly which files each user can access. Users access the platform through a SAML federation Single Sign On platform.\nThe architect needs to build a solution that allows fine grained access control, traceability of access to the objects, and usage of the standard tools (AWS Console, AWS CLI) to access the data.\nWhich solution should the architect build?","topic":"1","answer_description":"","answer_ET":"D","answers_community":[]}],"exam":{"isImplemented":true,"isBeta":false,"lastUpdated":"11 Apr 2025","numberOfQuestions":85,"isMCOnly":true,"name":"AWS Certified Big Data - Specialty","provider":"Amazon","id":17},"currentPage":11},"__N_SSP":true}