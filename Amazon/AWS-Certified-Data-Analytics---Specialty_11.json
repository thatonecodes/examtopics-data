{"pageProps":{"questions":[{"id":"SjHg0girGRWNFH0VEsML","answer_description":"","isMC":true,"answers_community":["D (66%)","C (26%)","5%"],"url":"https://www.examtopics.com/discussions/amazon/view/74013-exam-aws-certified-data-analytics-specialty-topic-1-question/","exam_id":20,"timestamp":"2022-04-21 13:25:00","topic":"1","answer":"D","choices":{"C":"Use AWS Glue for processing incoming data. Use AWS Step Functions for workflow orchestration. Use Amazon Redshift Spectrum for one-time queries and analytical reporting. Use OpenSearch Dashboards (Kibana) on Amazon OpenSearch Service (Amazon Elasticsearch Service) for the dashboard.","D":"Use AWS Glue for processing incoming data. Use AWS Lambda and S3 Event Notifications for workflow orchestration. Use Amazon Athena for one-time queries and analytical reporting. Use Amazon QuickSight for the dashboard.","A":"Use Amazon EMR for processing incoming data. Use AWS Step Functions for workflow orchestration. Use Apache Hive for one-time queries and analytical reporting. Bulk ingest the data in Amazon OpenSearch Service (Amazon Elasticsearch Service). Use OpenSearch Dashboards (Kibana) on Amazon OpenSearch Service (Amazon Elasticsearch Service) for the dashboard.","B":"Use Amazon EMR for processing incoming data. Use AWS Step Functions for workflow orchestration. Use Amazon Athena for one-time queries and analytical reporting. Use Amazon QuickSight for the dashboard."},"answer_images":[],"answer_ET":"D","question_images":[],"unix_timestamp":1650540300,"question_id":51,"question_text":"A real estate company maintains data about all properties listed in a market. The company receives data about new property listings from vendors who upload the data daily as compressed files into Amazon S3. The company's leadership team wants to see the most up-to-date listings as soon as the data is uploaded to\nAmazon S3. The data analytics team must automate and orchestrate the data processing workflow of the listings to feed a dashboard. The team also must provide the ability to perform one-time queries and analytical reporting in a scalable manner.\nWhich solution meets these requirements MOST cost-effectively?","discussion":[{"upvote_count":"13","comment_id":"605623","timestamp":"1653238260.0","poster":"fl0resi3nsis","content":"C seems correct\ncost-effective rules out emr [A&B]\nscalable reporting suggests redshift spectrum instead of athena\nimmediate access to updated dashboards suggests kibana instead of quicksight\nd may appear less expensive than c, but does not quite meet the criteria"},{"timestamp":"1650540300.0","upvote_count":"9","poster":"rb39","comment_id":"589290","content":"Selected Answer: D\nS3 Event Notifications to react to S3 updates"},{"poster":"zanhsieh","comment_id":"992546","timestamp":"1693246620.0","upvote_count":"2","content":"Selected Answer: D\nVote D. \nAWS Step Function vs Lambda+S3 Events do not have much price difference. Although Redshift Spectrum charge very less money, to use Redshift Spectrum the customer needs to have an existed Redshift cluster (AWS did have serverless around $260; around $180 per month for dense compute), which definitely beat Athena for \"one-time\" query. Quicksight cost $18 per user per month whereas Opensearch charge $43 per month (serverless one even more expensive; any combination indexing+searching-n-query would give $172.8 per month).\nhttps://aws.amazon.com/quicksight/pricing/\nhttps://aws.amazon.com/opensearch-service/pricing/\nhttps://aws.amazon.com/redshift/pricing/"},{"timestamp":"1689319260.0","content":"It is D\nCost effective\nWith Lambda + S3 event notification, Latest data will be picked up \nAthena can help with ad hoc queries perfect for that\nQuickSight with Direct Query can handle real time refresh\nhttps://docs.aws.amazon.com/quicksight/latest/user/refreshing-data.html","comment_id":"951321","poster":"rookiee1111","upvote_count":"1"},{"content":"Selected Answer: C\nI think its C due to the fact that Kibana provides real time dash board capabilities and glue is cheaper than emr.","timestamp":"1688969040.0","comment_id":"947793","poster":"Menyawy","upvote_count":"2"},{"upvote_count":"1","content":"D: I passed the test","comment_id":"886664","timestamp":"1682969760.0","poster":"pk349"},{"content":"Selected Answer: D\nThe question says \"cost effective\", so answer is D.","comment_id":"809719","poster":"murali12180","timestamp":"1676477520.0","upvote_count":"4"},{"content":"I would think it was C","upvote_count":"1","poster":"hughnguyen","timestamp":"1674634260.0","comment_id":"787388"},{"content":"Selected Answer: D\nD is a better option. Lambda and S3 notifications make so much sense if the data is already uploaded to S3 everyday and you want to trigger the process upon the upload event.","upvote_count":"5","poster":"rocky48","timestamp":"1671447360.0","comment_id":"749741"},{"comment_id":"742580","content":"I would go for D, only in doubt regarding Athena. But I think that if we compare creating a OpenSearch cluster and a Redshift cluster vs having Athena and Quicksight it is more cost efectively.","timestamp":"1670835780.0","upvote_count":"2","poster":"silvaa360"},{"content":"Selected Answer: C\nD is more cost-effective, however it can't meeting a must request \"must provide the ability to perform one-time queries and analytical reporting in a scalable manner.\" as athena is not scalable","timestamp":"1670170740.0","upvote_count":"1","poster":"pgf909","comment_id":"735232"},{"content":"Selected Answer: D\nD for sure. \nNot C because you need a redshift to use redshift spectrum and Opensearch+Kibana is used in near-realtime scenario but here the data is uploaded \"daily\"","poster":"rav009","upvote_count":"4","timestamp":"1669108800.0","comment_id":"724206"},{"content":"Selected Answer: C\nI vote for C because they want to see the up-to-date listings as soon as the data is uploaded to Amazon S3. OpenSearch Dashboards allows them to refresh and display the most up-to-date dashboard automatically right away. C may not be as cost-effective as D, but D does not meet the requirement of delivering the listings as soon as the data is uploaded.","timestamp":"1667955120.0","poster":"b33f","upvote_count":"4","comment_id":"714176"},{"upvote_count":"3","comment_id":"700034","poster":"MultiCloudIronMan","timestamp":"1666276680.0","content":"D because its most cost effective"},{"poster":"jazzok","timestamp":"1665276420.0","content":"Answer:D\nEMR is not cost-effective, AB is out; \nOpenSearch is also expensive compared to QuickSight. C is out.\nD is perfectly cost-effective.","upvote_count":"1","comment_id":"689791"},{"upvote_count":"2","comments":[{"timestamp":"1658984520.0","comment_id":"638460","upvote_count":"3","content":"Maybe D is better in this solution. Lambda and S3 notifications make so much sense if the data is already uploaded to S3 everyday and you want to trigger the process upon the upload event.","poster":"rocky48"}],"content":"Selected Answer: C\nC seems correct\ncost-effective rules out emr [A&B]","poster":"rocky48","timestamp":"1658984400.0","comment_id":"638458"},{"poster":"girish123456","comment_id":"636772","timestamp":"1658761020.0","content":"Selected Answer: D\nMost cost effective solution","upvote_count":"1"},{"comment_id":"620281","upvote_count":"2","content":"Selected Answer: B\nwow! The community is divided on this one. I guess we can go for B actually... EMR can be more cost-effective then Glue. Basically just spin-up EMR cluster when you need it (data scheduled daily). One-time queries on S3 data = Athena almost always. Dashboarding of data in S3? = QuickSight!","comments":[{"timestamp":"1656480840.0","comment_id":"624443","content":"I have changed my mind a little. Maybe D is better in this solution. Lambda and S3 notifications make so much sense if the data is already uploaded to S3 everyday and you want to trigger the process upon the upload event.","poster":"Ramshizzle","upvote_count":"1"},{"content":"Only immediate access to the most recent listing is missing in this solution. But I would still go for this solution even if it means the data is likely a few minutes slower in the dashboard.","poster":"Ramshizzle","timestamp":"1655889900.0","upvote_count":"2","comment_id":"620282"}],"timestamp":"1655889780.0","poster":"Ramshizzle"},{"poster":"Bik000","upvote_count":"1","comment_id":"604836","content":"Selected Answer: C\nAnswer is C","timestamp":"1653131220.0"},{"poster":"Shammy45","comment_id":"603958","timestamp":"1652969220.0","content":"Selected Answer: A\nAthena is not suitable for Analytics (complex) reporting. Option B ruled out as we are not leveraging Redshift, by process of Elimination its Option-A","upvote_count":"1"},{"timestamp":"1652809380.0","poster":"chp2022","upvote_count":"3","comments":[{"upvote_count":"1","comment_id":"852865","content":"I'm confused too. Is the combination of S3 event and Lambda considered workflow orchestration?","poster":"liar_p","timestamp":"1679985480.0"}],"content":"Can someone explain why is S3 event + Lambda a better option than Step Function? It seems to me both B and D are correct.","comment_id":"602999"},{"comments":[{"comment_id":"593993","poster":"CHRIS12722222","content":"Changed to C. Athena will not be the least latency due to AWS sharing of pool resources.","upvote_count":"1","timestamp":"1651171380.0"}],"comment_id":"590646","timestamp":"1650719880.0","poster":"CHRIS12722222","upvote_count":"4","content":"Looks like option D is right\nSounds okay to kickstart workflow using s3 event notification as soon as data lands in s3. I stand to be corrected"},{"comments":[{"timestamp":"1650745020.0","poster":"astalavista1","upvote_count":"2","content":"Inclined to agree, as Spectrum reduces cost and Opensearch and Kibana will provide up to date visualisation.","comment_id":"590793"}],"content":"I Think ans C, bcoz business users need to see the details ASAP.","comment_id":"590534","timestamp":"1650708660.0","upvote_count":"1","poster":"mouli15"}]},{"id":"MWdhS7VixcKJqluFOD3b","choices":{"C":"Use Amazon RDS for MySQL as the metastore","B":"Use an external Amazon EC2 instance running MySQL as the metastore","D":"Use Amazon S3 as the metastore","A":"Use AWS Glue Data Catalog as the metastore"},"answer_ET":"A","unix_timestamp":1650745080,"answer_description":"","answer":"A","question_id":52,"url":"https://www.examtopics.com/discussions/amazon/view/74270-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer_images":[],"isMC":true,"topic":"1","discussion":[{"comment_id":"591579","content":"answer: A\nhttps://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hive-metastore-glue.html","timestamp":"1650883560.0","poster":"wata9821","upvote_count":"11"},{"poster":"pk349","timestamp":"1682969820.0","content":"A: I passed the test","upvote_count":"2","comment_id":"886665"},{"comment_id":"640415","content":"Selected Answer: A\nA - Use AWS Glue Data Catalog","timestamp":"1659337860.0","poster":"rudramadhu","upvote_count":"2"},{"poster":"Raje14k","upvote_count":"3","timestamp":"1659276540.0","comment_id":"640136","content":"Use the AWS Glue Data Catalog as the metastore for Hive. \nUsing Amazon EMR version 5.8.0 or later, you can configure Hive to use the AWS Glue Data Catalog as its metastore. We recommend this configuration when you require a persistent metastore or a metastore shared by different clusters, services, applications, or AWS accounts.","comments":[{"poster":"Raje14k","upvote_count":"1","comment_id":"640137","content":"I mean, Ans A","timestamp":"1659276600.0"}]},{"content":"Selected Answer: A\nSelected Answer: A","comment_id":"638492","timestamp":"1658986620.0","poster":"rocky48","upvote_count":"1"},{"timestamp":"1656064740.0","comment_id":"621548","content":"Selected Answer: A\nYes A is the answer. Clear as day. All the other options are not Apache Hive metatores","poster":"Ramshizzle","upvote_count":"1"},{"content":"Answer is A.","poster":"GiveMeEz","upvote_count":"1","timestamp":"1655948400.0","comment_id":"620705"},{"comment_id":"590851","poster":"[Removed]","upvote_count":"2","content":"Selected Answer: A\nexternal data store","timestamp":"1650765900.0"},{"comment_id":"590794","timestamp":"1650745080.0","poster":"astalavista1","comments":[{"upvote_count":"1","comment_id":"842755","content":"I have seen this mentioned a few times in other answers - curious as to why people think this is a least effort solution. Is there a direct AWS import/service level function using an RDS as a metadata store?","timestamp":"1679142180.0","poster":"np2021"}],"content":"Selected Answer: C\nC - persistent and can be queried all time.","upvote_count":"1"}],"exam_id":20,"question_images":[],"answers_community":["A (86%)","14%"],"question_text":"A marketing company collects data from third-party providers and uses transient Amazon EMR clusters to process this data. The company wants to host an\nApache Hive metastore that is persistent, reliable, and can be accessed by EMR clusters and multiple AWS services and accounts simultaneously. The metastore must also be available at all times.\nWhich solution meets these requirements with the LEAST operational overhead?","timestamp":"2022-04-23 22:18:00"},{"id":"gs1CzPVK67Z6FqOYv2AR","choices":{"A":"Use the AWS Glue Data Catalog to manage the data catalog. Define an AWS Glue workflow for the ETL process. Define a trigger within the workflow that can start the crawler when an ETL job run is complete.","C":"Use an Apache Hive metastore to manage the data catalog. Update the AWS Glue ETL code to include the enableUpdateCatalog and partitionKeys arguments.","D":"Use the AWS Glue Data Catalog to manage the data catalog. Update the AWS Glue ETL code to include the enableUpdateCatalog and partitionKeys arguments.","B":"Use the AWS Glue Data Catalog to manage the data catalog. Use AWS Glue Studio to manage ETL jobs. Use the AWS Glue Studio feature that supports updates to the AWS Glue Data Catalog during job runs."},"answer_ET":"D","unix_timestamp":1650745440,"answer_description":"","answer":"D","question_id":53,"url":"https://www.examtopics.com/discussions/amazon/view/74271-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer_images":[],"topic":"1","isMC":true,"discussion":[{"timestamp":"1710226680.0","comment_id":"1171488","poster":"lovelazur","content":"Selected Answer: D\nD is the best for small effort","upvote_count":"2"},{"poster":"pk349","content":"D: I passed the test","comment_id":"886667","upvote_count":"4","timestamp":"1682969880.0"},{"timestamp":"1670836140.0","comment_id":"742585","upvote_count":"4","content":"Selected Answer: D\nAlthough A can be a very good solution, as it is possible and is even more visual on what is happening, of course creating a workflow and a trigger will be more costly than putting this options in the ETL code.\nSo, D for sure.","poster":"silvaa360"},{"comment_id":"714061","upvote_count":"2","content":"Selected Answer: B\n\"Business analysts use Amazon Athena to query the table and create monthly summary reports for the AWS accounts\" \nGiven the above, data should be partitioned by date first in order to calculate summary report for all accounts for a particular month. \nCorrect answer is B","timestamp":"1667934840.0","poster":"lkarwot"},{"content":"I choose D as well","upvote_count":"1","timestamp":"1666276980.0","poster":"MultiCloudIronMan","comment_id":"700040"},{"timestamp":"1658291340.0","content":"Selected Answer: D\nI agree with D","comment_id":"633831","upvote_count":"1","poster":"rocky48"},{"comment_id":"632510","timestamp":"1658049600.0","content":"Selected Answer: D\nD is cost effective","upvote_count":"2","poster":"arboles"},{"comment_id":"618597","content":"Selected Answer: D\nI agree with D","poster":"dushmantha","timestamp":"1655629200.0","upvote_count":"1"},{"timestamp":"1651055160.0","poster":"Teraxs","content":"Selected Answer: D\nD - most cost effective as not rerunning the crawler\nhttps://docs.aws.amazon.com/glue/latest/dg/update-from-job.html","upvote_count":"3","comment_id":"593053"},{"content":"answer: D\nhttps://docs.aws.amazon.com/glue/latest/dg/update-from-job.html","upvote_count":"2","timestamp":"1650883980.0","poster":"wata9821","comment_id":"591584"},{"upvote_count":"2","comment_id":"591414","content":"D seems correct","timestamp":"1650869280.0","poster":"siju13"},{"upvote_count":"1","timestamp":"1650765960.0","comment_id":"590852","content":"Selected Answer: D\nas per the document","poster":"[Removed]"},{"timestamp":"1650745440.0","comment_id":"590795","content":"D - According to doc- https://docs.aws.amazon.com/glue/latest/dg/update-from-job.html","upvote_count":"2","poster":"astalavista1"}],"exam_id":20,"answers_community":["D (88%)","13%"],"question_images":[],"question_text":"A data engineer is using AWS Glue ETL jobs to process data at frequent intervals. The processed data is then copied into Amazon S3. The ETL jobs run every 15 minutes. The AWS Glue Data Catalog partitions need to be updated automatically after the completion of each job.\nWhich solution will meet these requirements MOST cost-effectively?","timestamp":"2022-04-23 22:24:00"},{"id":"k813fcfvzmp712pZBgf8","isMC":true,"topic":"1","answer_images":[],"question_id":54,"choices":{"D":"Partition the data by account ID, year, and month","B":"Partition the data by date and account ID","A":"Change the file format to .csv.zip","C":"Partition the data by month and account ID"},"timestamp":"2022-04-21 13:02:00","answers_community":["D (83%)","B (17%)"],"url":"https://www.examtopics.com/discussions/amazon/view/74011-exam-aws-certified-data-analytics-specialty-topic-1-question/","question_images":[],"answer_ET":"D","question_text":"A reseller that has thousands of AWS accounts receives AWS Cost and Usage Reports in an Amazon S3 bucket. The reports are delivered to the S3 bucket in the following format:\n<example-report-prefix>/<example-report-name>/yyyymmdd-yyyymmdd/<example-report-name>.parquet\nAn AWS Glue crawler crawls the S3 bucket and populates an AWS Glue Data Catalog with a table. Business analysts use Amazon Athena to query the table and create monthly summary reports for the AWS accounts. The business analysts are experiencing slow queries because of the accumulation of reports from the last\n5 years. The business analysts want the operations team to make changes to improve query performance.\nWhich action should the operations team take to meet these requirements?","exam_id":20,"answer":"D","discussion":[{"comment_id":"623992","content":"Selected Answer: D\nShould be D. We want to create monthly reports for each account. So we want to query the data by account-id and month. \n\nOnly month is not enough, we have to add year, otherwise we query the previous year's months as well.","upvote_count":"8","poster":"Ramshizzle","timestamp":"1656423060.0"},{"content":"Selected Answer: D\nshould be D, by date is too precise and by account helps as well","comment_id":"605225","upvote_count":"6","timestamp":"1653203280.0","poster":"f4bi4n"},{"poster":"pk349","timestamp":"1682969940.0","upvote_count":"1","comment_id":"886668","content":"D: I passed the test"},{"comment_id":"846270","timestamp":"1679426100.0","upvote_count":"1","content":"Selected Answer: D\nDate/account id partitioning would create a partition for each day thus the analysts would need to ingest a different date range of partitions for each account int other analysis each month. Account ID/year/month would more accurately represent the query pattern and avoid the need for analysts to specify a data range. hence D","poster":"mawsman"},{"content":"Selected Answer: D\nSelected Answer: D","timestamp":"1658808000.0","poster":"rocky48","comment_id":"637136","upvote_count":"2"},{"upvote_count":"1","content":"Selected Answer: D\nPer account and monthly","timestamp":"1658665740.0","poster":"Richie1217","comment_id":"636045"},{"timestamp":"1653130800.0","upvote_count":"1","content":"Selected Answer: B\nAnswer is B","comment_id":"604831","poster":"Bik000"},{"comments":[{"content":"I concur","timestamp":"1651165140.0","comment_id":"593952","upvote_count":"1","poster":"CHRIS12722222"}],"content":"Selected Answer: D\nmonthly summary reports for the AWS accounts.","comment_id":"590853","poster":"[Removed]","timestamp":"1650766020.0","upvote_count":"2"},{"upvote_count":"3","poster":"CHRIS12722222","timestamp":"1650721260.0","comment_id":"590656","comments":[],"content":"B looks good to me"},{"timestamp":"1650538920.0","upvote_count":"3","poster":"rb39","content":"Selected Answer: B\nPartition by date is the good practice here","comment_id":"589279"}],"answer_description":"","unix_timestamp":1650538920},{"id":"QLkElmaDiO9ctGQvvXAz","answer_ET":"B","url":"https://www.examtopics.com/discussions/amazon/view/74405-exam-aws-certified-data-analytics-specialty-topic-1-question/","discussion":[{"timestamp":"1682970000.0","content":"B: I passed the test","comment_id":"886671","poster":"pk349","upvote_count":"3"},{"upvote_count":"1","poster":"IvanHuang","comment_id":"742795","timestamp":"1670849100.0","content":"A. Use QuickSight folders to organize dashboards, analytics, and datasets. Assign permissions to these folders for each user.\n\nUsing QuickSight folders to organize dashboards, analytics, and datasets, and assigning permissions to these folders for users is a way to simplify rights management. This allows the company to easily grant users access to QuickSight projects and make sharing easier.\n\nOptions B, C, and D all mention using AWS IAM and the QuickSight user management API, but do not mention using folders to organize projects. Option B mentions using folders to assign group permissions, but does not mention how to assign these permissions to users."},{"comment_id":"637106","timestamp":"1658806380.0","poster":"rocky48","content":"Selected Answer: B\nSelected Answer: B","upvote_count":"3"},{"content":"Selected Answer: B\nhttps://docs.aws.amazon.com/quicksight/latest/user/folders.html","timestamp":"1651749600.0","poster":"Teraxs","upvote_count":"2","comment_id":"597275"},{"poster":"[Removed]","comment_id":"591333","upvote_count":"2","timestamp":"1650850500.0","content":"Selected Answer: B\nas per the document"}],"exam_id":20,"topic":"1","question_text":"A company using Amazon QuickSight Enterprise edition has thousands of dashboards, analyses, and datasets. The company struggles to manage and assign permissions for granting users access to various items within QuickSight. The company wants to make it easier to implement sharing and permissions management.\nWhich solution should the company implement to simplify permissions management?","answer_description":"","answers_community":["B (100%)"],"unix_timestamp":1650850500,"timestamp":"2022-04-25 03:35:00","choices":{"D":"Use QuickSight user management APIs to provision group permissions based on dashboard naming conventions.","C":"Use AWS IAM resource-based policies to assign group permissions to QuickSight items.","B":"Use QuickSight folders to organize dashboards, analyses, and datasets. Assign group permissions by using these folders.","A":"Use QuickSight folders to organize dashboards, analyses, and datasets. Assign individual users permissions to these folders."},"question_images":[],"isMC":true,"answer_images":[],"answer":"B","question_id":55}],"exam":{"name":"AWS Certified Data Analytics - Specialty","id":20,"provider":"Amazon","lastUpdated":"11 Apr 2025","isMCOnly":true,"isBeta":false,"numberOfQuestions":164,"isImplemented":true},"currentPage":11},"__N_SSP":true}