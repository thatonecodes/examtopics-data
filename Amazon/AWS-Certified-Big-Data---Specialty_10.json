{"pageProps":{"questions":[{"id":"MxIlImQCmrFf63EPr1aH","isMC":true,"question_text":"An administrator tries to use the Amazon Machine Learning service to classify social media posts that mention the administrators company into posts that require a response and posts that do not. The training dataset of\n10,000 posts contains the details of each post including the timestamp, author, and full text of the post. The administrator is missing the target labels that are required for training.\nWhich Amazon Machine Learning model is the most appropriate for the task?","exam_id":17,"choices":{"D":"Regression model where the predicted value is the probability that the post requires a response","B":"Binary classification model, where the two classes are the require-response post and does-not-require- response","C":"Multi-class prediction model, with two classes: require-response post and does-not-require-response","A":"Binary classification model, where the target class is the require-response post"},"question_images":[],"timestamp":"2019-08-28 14:43:00","question_id":46,"unix_timestamp":1566996180,"url":"https://www.examtopics.com/discussions/amazon/view/4275-exam-aws-certified-big-data-specialty-topic-1-question-50/","answer_description":"","discussion":[{"comments":[{"upvote_count":"6","poster":"mattyb123","comments":[{"content":"Agree with B.Also, the administrator is missing the target labels that are required for training.","poster":"balajisush0312","upvote_count":"1","timestamp":"1633973520.0","comment_id":"62066"}],"content":"Thanks for the correction i have overlooked this one sadly.\nhttps://docs.aws.amazon.com/machine-learning/latest/dg/types-of-ml-models.html \nBinary Classification Model\nML models for binary classification problems predict a binary outcome (one of two possible classes). To train binary classification models, Amazon ML uses the industry-standard learning algorithm known as logistic regression.","timestamp":"1632690660.0","comment_id":"8778"}],"content":"B is correct. binary classification has two classess.","timestamp":"1632536640.0","comment_id":"8738","poster":"muhsin","upvote_count":"9"},{"content":"\"The administrator is missing the target labels that are required for training.\", so you can't train binary classification model.\nThe only option left is D, here is some Use Case:\nhttps://medium.com/analytics-vidhya/a-guide-to-machine-learning-in-r-for-beginners-part-5-4c00f2366b90","upvote_count":"9","comment_id":"30449","comments":[{"poster":"ME2000","upvote_count":"4","content":"Exactly, The answer is D.\nhttps://stats.stackexchange.com/questions/55357/binary-classifier-with-training-data-for-one-label-only","comment_id":"38144","timestamp":"1633522200.0","comments":[{"upvote_count":"2","timestamp":"1633738920.0","comment_id":"38147","content":"https://hbr.org/2015/11/a-refresher-on-regression-analysis","poster":"ME2000","comments":[{"content":"Binary Classification uses Logistic Regression under the hood. So the answer is B.","poster":"iamsajal","timestamp":"1634644980.0","comment_id":"98930","upvote_count":"1"}]}]}],"poster":"yuriy_ber","timestamp":"1633120140.0"},{"poster":"yogesh88","content":"Just attempted exam - Option A was\nUnary classification model, where the target class is the require-response post. \n\nI selected B .","timestamp":"1635417780.0","upvote_count":"1","comment_id":"124349"},{"upvote_count":"2","comment_id":"105681","timestamp":"1635245100.0","poster":"freedomeox","content":"find the exact sceniro in AWS official sample questions but with a different question:\n\nWhich two options will create valid target label data?\nA) Ask the social media handling team to review each post and provide the label.\nB) Use the sentiment analysis NLP library to determine whether a post requires a response.\nC) Use the Amazon Mechanical Turk web service to publish Human Intelligence Tasks that ask Turk workers to label the posts.\nD) Using the a priori probability distribution of the two classes, use Monte-Carlo simulation to generate the labels. \n\nwhich makes more sense then this one..."},{"poster":"freedomeox","upvote_count":"1","timestamp":"1635060960.0","content":"The key point here is missing the target labels. There are two kinds of machine learning: supervised/unsupervised. if the target labels are missing, then we can't perform supervised learning, because we don't know the right answer. Therefore, A, D are wrong: we don't know the target class or the prediction value of the training, so we can't train. For B, binary classification is one kind of supervised training, which you must provide the ground truth answer of whether the post is response-needed or not. Therefore we left with C. We use unsupervised learning to tell the model, here we have two classes in all these posts, classify them for me.","comment_id":"104328"},{"poster":"agm84","content":"Amazon ML only supports supervised models, meaning it must have a sample of data with the target value info otherwise the model can not be used for training. Not having the target value invalidates all options in this question. That is why i think its A.","upvote_count":"1","timestamp":"1634599560.0","comment_id":"98785"},{"timestamp":"1633916640.0","comment_id":"52369","poster":"san2020","content":"my selection D","comments":[{"timestamp":"1634793180.0","content":"Regression is not suited for classification, you'd at the minimum still transform the predicted probabilities to 0 or 1 - B is better.","upvote_count":"2","poster":"Corram","comment_id":"101023"}],"upvote_count":"2"}],"answers_community":[],"answer_images":[],"topic":"1","answer":"A","answer_ET":"A"},{"id":"sWvyvrfKveCW39Wj1OuX","timestamp":"2019-08-14 23:17:00","choices":{"D":"Use Amazon S3 event notification to populate an Amazon Redshift table with metadata about every file loaded to Amazon S3, and partition them based on the month and year of the file.","A":"Use Amazon Kinesis to get the data feeds directly from physicians, batch them using a Spark application on Amazon Elastic MapReduce (EMR), and then store them in Amazon S3 with folders separated per physician.","C":"Use Amazon S3 event notification to populate an Amazon DynamoDB table with metadata about every file loaded to Amazon S3, and partition them based on the month and year of the file.","B":"Use Amazon API Gateway to get the data feeds directly from physicians, batch them using a Spark application on Amazon Elastic MapReduce (EMR), and then store them in Amazon S3 with folders separated per physician."},"url":"https://www.examtopics.com/discussions/amazon/view/3597-exam-aws-certified-big-data-specialty-topic-1-question-51/","question_id":47,"question_text":"A medical record filing system for a government medical fund is using an Amazon S3 bucket to archive documents related to patients. Every patient visit to a physician creates a new file, which can add up millions of files each month. Collection of these files from each physician is handled via a batch process that runs ever night using AWS Data Pipeline. This is sensitive data, so the data and any associated metadata must be encrypted at rest.\nAuditors review some files on a quarterly basis to see whether the records are maintained according to regulations. Auditors must be able to locate any physical file in the S3 bucket for a given date, patient, or physician. Auditors spend a significant amount of time location such files.\nWhat is the most cost- and time-efficient collection methodology in this situation?","answer_images":[],"answers_community":[],"exam_id":17,"topic":"1","answer":"A","answer_ET":"A","question_images":[],"answer_description":"","unix_timestamp":1565817420,"isMC":true,"discussion":[{"poster":"jay1ram2","comment_id":"38811","timestamp":"1634671440.0","content":"C is the answer, it allows for easy search of metadata using DynamoDB queries. \n\nEMR methods do not provide options to search for data by patient or date. \n\nRedshift does not support partition","upvote_count":"6"},{"upvote_count":"1","timestamp":"1635738180.0","content":"Remember no of files and batch with AWS Data Pipeline\nA wrong: will be very expensive - shrads/24h + EMR why you need it?\nB wrong: API Gateway + EMR? you need to architecture everything from the beginning\nC OK: cheapest simplest and working\nD wrong: no partition in Redshift","comment_id":"337498","poster":"DerekKey"},{"upvote_count":"1","poster":"BobCai","content":"C is good answer.data stored in S3 and dynamoDB stored meta data to make it easy search","timestamp":"1635636360.0","comment_id":"87474"},{"timestamp":"1635297000.0","content":"I think the focus of this question is on the most cost-efficient and time-efficient way of collecting the data and not how to analyze or locate the data. Based on that- I would go with C instead of A because it's definitely much simpler to collect and store the data. As regards analyzing the data per the requirements ( to locate the file) , we could create GSI on the physician + patient to quickly access the metadata containing the link to the S3 file.","comment_id":"78128","poster":"Bulti","upvote_count":"2"},{"upvote_count":"1","content":"For Option C- It may work well if we create a GSI on physician + patient. However I think that would result in a significantly increased cost due to the creation of a separate table with its own RCU + WCU.","comment_id":"75530","poster":"Bulti","timestamp":"1635142140.0"},{"upvote_count":"1","comment_id":"75529","timestamp":"1635100620.0","content":"Option A- I would run a transient EMR cluster every night to produce the file structure in the S3 bucket like this-> bucket/physician/date/patient/ patient.txt. With this structure in place I can use S3 select to locate a file for a specific patient file very quickly. \nOption B- This is a big data exam and using API Gateway can be discarded. Integrating API Gateway with EMR using Lambda or EC2 doesn't seem cost efficient either.\nOption C: The only reason I wouldn't select this option is because of the partition + sort key. It will fetch millions of records to look through to find a specific patient's file for a specific physician. Ideal partition + sort key would have been physician + patient.\nOption D:- The data that needs to be located is too granular to consider RedShift which is a Data Warehouse solution.\nTherefore the choice is Option A","poster":"Bulti"},{"timestamp":"1634954580.0","comment_id":"52370","poster":"san2020","content":"my selection C","upvote_count":"1"},{"upvote_count":"2","content":"The record are already getting stored in S3, we only need to keep track of where to find them.\nS3 event push> Lambda >DynamoDB is the easiest and cost-efficient solution.","timestamp":"1634855700.0","comment_id":"47117","poster":"AdamSmith"},{"timestamp":"1634628660.0","comment_id":"36134","content":"All user data stored in Amazon DynamoDB is fully encrypted at rest. DynamoDB encryption at rest provides enhanced security by encrypting all your data at rest using encryption keys stored in AWS Key Management Service (AWS KMS). This functionality helps reduce the operational burden and complexity involved in protecting sensitive data. With encryption at rest, you can build security-sensitive applications that meet strict encryption compliance and regulatory requirements.\nNote - only metadata of each file has to be queried...\nSo going with C. Millions of data each month does not seem very high..so partition based on month and year will work.","poster":"PK1234","upvote_count":"3"},{"upvote_count":"2","content":"I choose B, it is most cost-effective compared to other solutions.","comments":[{"content":"Agreed. The answer is in the question: “Auditors must be able to locate any physical file in the S3 bucket for a given date, patient, or physician.” So it’s A or B. B make more sense for an enterprise solution.","poster":"Kuntazulu","comment_id":"36545","upvote_count":"1","timestamp":"1634629560.0"}],"timestamp":"1634433900.0","comment_id":"19351","poster":"cybe001"},{"poster":"asadao","content":"I think it is A","upvote_count":"2","timestamp":"1634419200.0","comment_id":"18263"},{"upvote_count":"4","content":"What is the most cost- and time-efficient collection methodology in this situation? C. Only lambda doing the job and both S3 for files and dynamo for metadata support encryption at rest. How do we hash/sort the dynamo table here is irrelevant to the answer--my opinion","poster":"Zire","timestamp":"1634347080.0","comment_id":"14179","comments":[{"comment_id":"19894","poster":"d00ku","upvote_count":"1","content":"Seems right.","timestamp":"1634556180.0"}]},{"upvote_count":"1","poster":"exams","content":"Any Thoughts on C?","comment_id":"11553","timestamp":"1634253960.0"},{"poster":"mattyb123","comments":[{"comment_id":"9910","upvote_count":"1","poster":"jlpl","content":"so D? final?","timestamp":"1634058060.0"}],"content":"Do we think D via redshift spectrum as per https://www.quora.com/Does-Amazon-Redshift-support-partitioning\nOr do we think dynamodb still in a timeseries format item2019Sept?","comment_id":"9746","timestamp":"1634016720.0","upvote_count":"2"},{"comment_id":"7557","timestamp":"1632530160.0","upvote_count":"3","content":"C? Possible answer?","comments":[{"poster":"mattyb123","timestamp":"1632935100.0","upvote_count":"5","comment_id":"8312","content":"I'm going to go with C.","comments":[{"upvote_count":"1","content":"As @muhsin mentioned if we use dynamodb, the partition key should be physican and sort key is patient. The partition/sort key on month and year is highly ineffective.","poster":"mattyb123","comment_id":"8924","timestamp":"1633978260.0"}]}],"poster":"jlpl"},{"content":"for option A, how is it possible to find easily patient information.","upvote_count":"1","timestamp":"1632447600.0","poster":"muhsin","comment_id":"6899","comments":[{"comment_id":"7037","comments":[{"comments":[{"upvote_count":"1","content":"Are you thinking either A or B due to the partitioning/sorting being done on physician","poster":"mattyb123","timestamp":"1633808400.0","comment_id":"8776","comments":[{"timestamp":"1633983120.0","comment_id":"8926","poster":"mattyb123","upvote_count":"3","content":"as the question says auditors must be able to locate any physical file in the S3 bucket for a given date, patient, or physician. Wouldnt A be correct?"}]}],"content":"in S3, the file has already timestamp information. if we use dynamodb, the partition key should be physican and sort key is patient.","timestamp":"1633022940.0","poster":"muhsin","upvote_count":"1","comment_id":"8740"}],"poster":"mattyb123","upvote_count":"3","timestamp":"1632482100.0","content":"Good point. what are your thoughts on C? Thinking since they refer to collection and dynamodb refers to collection items could be linked. Also this will also a look up or to find patient information quickly compared to the redshift answer."}]}]},{"id":"2S6FMS42zdicvbjPo2Wj","question_id":48,"timestamp":"2019-08-10 07:45:00","topic":"1","question_images":[],"isMC":true,"answer":"D","answer_description":"","unix_timestamp":1565415900,"url":"https://www.examtopics.com/discussions/amazon/view/3419-exam-aws-certified-big-data-specialty-topic-1-question-52/","answer_images":[],"discussion":[{"poster":"jay1ram2","content":"The asks are cost-effective visual reports.\n\nB is the right answer as transient clusters each night will run only during ETL and quicksight is capable of producing cost-effective daily reports for multiple individuals (physicians)\n\nA - Kinesis Aggregators Library does not exists\nC - Running spark streaming server is not cost effective\nD - Zeppelin on EMR does not have capability to provide reports/access for multiple physicians.","upvote_count":"9","comment_id":"38813","timestamp":"1634415720.0"},{"timestamp":"1633531740.0","content":"correct. B","poster":"mattyb123","upvote_count":"5","comment_id":"6473"},{"comment_id":"77454","timestamp":"1635296460.0","content":"Not A: No such library called Kinesis Aggregators Library.\nNot C: This is not cost-effective\nNot D: This is not cost-effective as the EMR cluster will be utilized not only to aggregate the data but also run Zeppelin command to provide reports on a daily basis.\nAnswer is B: Transient EMR cluster will reduce cost and QuickSight could use the S3 dataset to generate insight using Athena or directly.","comments":[{"content":"D: we can run EMR cluster at night and store data in S3. Shutdown the cluster once processing is completed. Use zeppelin for reporting. what about this solution? correct me if I am wrong","upvote_count":"1","comments":[{"comment_id":"123396","poster":"yogesh88","upvote_count":"1","timestamp":"1636211760.0","content":"Option B - correct.\nOption D states that we will provide Zeppelin notebooks that look at the new data residing on the cluster - in this case we will have to keep cluster in running mode.\nAlso Zeppelin will not run without EMR - so fails at cost-effective condition."}],"comment_id":"85755","timestamp":"1635544860.0","poster":"Balaji2020"}],"poster":"Bulti","upvote_count":"5"},{"content":"D because 24*7 patient care","poster":"susan8840","timestamp":"1635214200.0","comment_id":"74050","upvote_count":"1"},{"poster":"san2020","timestamp":"1635007320.0","comment_id":"52371","content":"my selection B","upvote_count":"2"},{"content":"D anyone? We may even use Zappelin for visualisation:\nhttps://scalegrid.io/blog/data-visualization-using-apache-zeppelin/","timestamp":"1634732580.0","upvote_count":"1","comment_id":"47275","poster":"sam3787"},{"comments":[{"comment_id":"101034","upvote_count":"1","timestamp":"1636101960.0","poster":"Corram","content":"It is asked for reports only, having (near-)realtime notebooks is overkill. Even patient care does not always require (near-)realtime data."}],"content":"transient EMR cluster is not suitable for patient care business, so I choose D","timestamp":"1634506860.0","poster":"bc5468521","upvote_count":"1","comment_id":"39864"},{"upvote_count":"1","poster":"am7","comment_id":"33804","timestamp":"1634039880.0","content":"Option B doesn't take into consideration the data generated from sensor at night. On the other hand Kinesis can aggregate this real time data and store it for 24 hours, which can be used for visualisation each morning"},{"timestamp":"1633860300.0","comment_id":"30661","upvote_count":"1","poster":"shwang","comments":[{"comment_id":"101029","upvote_count":"3","poster":"Corram","timestamp":"1635702600.0","content":"Zepplin Notebooks requires your cluster to run non-stop, which is not cost-effective, hence D is wrong."}],"content":"Zeppelin notebooks are cheaper than QuickSights?"},{"poster":"antoneti","comment_id":"26475","timestamp":"1633842180.0","upvote_count":"1","content":"agree with B"},{"poster":"cybe001","comment_id":"19359","content":"B is most cost effective","timestamp":"1633705020.0","upvote_count":"1"},{"upvote_count":"1","comments":[{"timestamp":"1632389760.0","comments":[{"poster":"Jialu","content":"Yes , B is correct .","timestamp":"1633566720.0","upvote_count":"4","comment_id":"6583"}],"comment_id":"6445","poster":"jlpl","upvote_count":"9","content":"B? Cost effecctive? transients emr cluster?"}],"poster":"mattyb123","timestamp":"1632170880.0","content":"is it C. Most cost effective solution?","comment_id":"6429"}],"answers_community":[],"choices":{"A":"Use Kinesis Aggregators Library to generate reports for reviewing the patient sensor data and generate a QuickSight visualization on the new data each morning for the physician to review.","D":"Use an EMR cluster to aggregate the patient sensor data each night and provide Zeppelin notebooks that look at the new data residing on the cluster each morning for the physician to review.","C":"Use Spark streaming on EMR to aggregate the patient sensor data in every 15 minutes and generate a QuickSight visualization on the new data each morning for the physician to review.","B":"Use a transient EMR cluster that shuts down after use to aggregate the patient sensor data each night and generate a QuickSight visualization on the new data each morning for the physician to review."},"question_text":"A clinical trial will rely on medical sensors to remotely assess patient health. Each physician who participates in the trial requires visual reports each morning. The reports are built from aggregations of all the sensor data taken each minute.\nWhat is the most cost-effective solution for creating this visualization each day?","answer_ET":"D","exam_id":17},{"id":"BfkX7presmNXE9gVM5HM","url":"https://www.examtopics.com/discussions/amazon/view/4231-exam-aws-certified-big-data-specialty-topic-1-question-53/","answer_images":[],"choices":{"A":"Create a DBLINK on the source DB to connect to Amazon Redshift. Use a PostgreSQL trigger on the source table to capture the new insert/update/delete event and execute the event on the Amazon Redshift staging table.","C":"Extract the incremental changes periodically using a SQL query. Upload the changes to multiple Amazon Simple Storage Service (S3) objects, and run the COPY command to load to the Amazon Redshift staging layer.","B":"Use a PostgreSQL trigger on the source table to capture the new insert/update/delete event and write it to Amazon Kinesis Streams. Use a KCL application to execute the event on the Amazon Redshift staging table.","D":"Extract the incremental changes periodically using a SQL query. Upload the changes to a single Amazon Simple Storage Service (S3) object, and run the COPY command to load to the Amazon Redshift staging layer."},"question_text":"A company uses Amazon Redshift for its enterprise data warehouse. A new on-premises PostgreSQL OLTP\nDB must be integrated into the data warehouse. Each table in the PostgreSQL DB has an indexed timestamp column. The data warehouse has a staging layer to load source data into the data warehouse environment for further processing.\nThe data lag between the source PostgreSQL DB and the Amazon Redshift staging layer should NOT exceed four hours.\nWhat is the most efficient technique to meet these requirements?","discussion":[{"comment_id":"75534","content":"A - Incorrect due to the DBLink constraint around using RDS PostgreSQL as opposed to on-prem PostgreSQL.\nB. Using Kinesis KCL to write to RedShift although can be done using JBDC/ODBC driver doesn't seem efficient. \nC. This makes the best sense. Using a cron job, we could extract new/changed records based on the timestamp from each table and create an S3 object for that table. Then use a Copy command to copy the data from the S3 object to a RedShift Table.\nD. It doesn't make any practical sense to extract changes across all tables into a single S3 object because it's not possible to then split the records into multiple RedShift tables.\nCorrect option is C.","poster":"Bulti","timestamp":"1635426420.0","upvote_count":"6"},{"content":"correct... C?","comment_id":"11554","comments":[{"poster":"Kuntazulu","timestamp":"1635007380.0","comment_id":"36547","content":"Agreed. COPY command is the fastest way to load in Redshit, if you have ++files that is...","upvote_count":"2"}],"upvote_count":"5","poster":"exams","timestamp":"1634196240.0"},{"timestamp":"1635512220.0","upvote_count":"1","poster":"Bulti","content":"https://stackoverflow.com/questions/55262639/automatically-load-data-into-redshift-with-the-copy-function","comment_id":"78137"},{"content":"I found this post which suggests that a cron job or a Lambda function triggered by an S3 event upon creating a new file in an S3 bucket could invoke the COPY command to load the data from each file into the corresponding RedShift table. My answer is C with more confidence this time.","poster":"Bulti","upvote_count":"3","timestamp":"1635428400.0","comment_id":"78136"},{"comment_id":"74745","timestamp":"1635290640.0","poster":"YashBindlish","content":"Correct Answer is A","upvote_count":"1"},{"poster":"susan8840","upvote_count":"1","comment_id":"74051","content":"C. per best practices, COPY is the best way to update data in Redshift not insert. D is incorrect as Redshift can process multiple S3 in parallel","timestamp":"1635190080.0"},{"comment_id":"52372","poster":"san2020","upvote_count":"2","content":"my selection C","timestamp":"1635070080.0"},{"content":"@shwang, the parallelisation increases.","timestamp":"1634353080.0","comment_id":"34601","upvote_count":"1","poster":"am7"},{"comment_id":"30102","content":"why not D? what is the difference to store the increment data to single or multiple S3?","comments":[{"timestamp":"1636158840.0","poster":"Corram","comment_id":"101866","upvote_count":"2","content":"Because of a Redshift best practice: Copy data split into a multiple of the number of slices in the Redshift cluster. It will be faster."},{"poster":"Corram","content":"Even worse, as Bulti correctly pointed out below, you might have multiple tables that you incrementally update, and their data must not be in a common file.","upvote_count":"1","timestamp":"1636218720.0","comment_id":"101868"}],"upvote_count":"3","poster":"shwang","timestamp":"1634291220.0"},{"comments":[{"content":"from the last post at this link https://forums.aws.amazon.com/message.jspa?messageID=807709. It doesn't seem to be time effective to do DBLINK. In addition as @muhsin said DBLINK to redshift ONLY works with RDS postgresql instances. Answer must be C.","timestamp":"1633815840.0","poster":"mattyb123","upvote_count":"4","comment_id":"8788","comments":[{"timestamp":"1635536520.0","content":"Additionally, it appears that DBLINK is used in RDS to query from Redshift, not the other way round. https://stackoverflow.com/questions/45516920/does-amazon-redshift-support-extension-dblink","comment_id":"101081","upvote_count":"1","poster":"Corram"}]}],"upvote_count":"2","content":"A! i think!","comment_id":"8754","poster":"jlpl","timestamp":"1632694140.0"},{"comment_id":"8652","poster":"mattyb123","timestamp":"1632068580.0","upvote_count":"1","comments":[{"comment_id":"8741","timestamp":"1632527100.0","content":"this is aws RDS. but in the question, db is on-prem. so we need to move the data to aws.","upvote_count":"1","poster":"muhsin"}],"content":"Is it A?\n1. https://aws.amazon.com/blogs/big-data/join-amazon-redshift-and-amazon-rds-postgresql-with-dblink/\n2. https://forums.aws.amazon.com/message.jspa?messageID=807709"}],"answers_community":[],"answer_ET":"C","exam_id":17,"answer_description":"","topic":"1","question_id":49,"question_images":[],"unix_timestamp":1566957360,"timestamp":"2019-08-28 03:56:00","answer":"C","isMC":true},{"id":"pMhfrJrA4IU0DRhlXZJ0","exam_id":17,"question_id":50,"isMC":true,"answer_ET":"A","choices":{"D":"Machine Learning on D instance types and ad-hoc queries on I instance types","A":"Machine Learning on C instance types and ad-hoc queries on R instance types","C":"Machine Learning on T instance types and ad-hoc queries on M instance types","B":"Machine Learning on R instance types and ad-hoc queries on G2 instance types"},"topic":"1","answer_images":[],"unix_timestamp":1568784480,"discussion":[{"content":"A is correct","poster":"exams","timestamp":"1632681660.0","comment_id":"11555","upvote_count":"6"},{"comments":[{"upvote_count":"2","poster":"MichRox","timestamp":"1635094740.0","comment_id":"133961","content":"T and M are actually general purpose, but yeah, it's A)"}],"timestamp":"1634057460.0","comment_id":"108743","content":"A) is correct\nDetails:\nC- Compute optimized\nR – Memory optimized\nG – GPU omptimized \nI, D – Storage optimized\nT – not exists\n\nC3, C4, and C5 are compute optimized instances featuring high performance processors and with a lowest price/compute performance in EC2 compared to R3, R4 or R5 although it's recommended use cases are distributed memory caches and in-memory analytics. But C5 will do the job for you for a lower price.\nhttps://stackoverflow.com/questions/30435610/spark-which-instance-type-is-preferred-for-aws-emr-cluster","upvote_count":"3","poster":"matthew95"},{"comment_id":"52373","timestamp":"1633664760.0","poster":"san2020","upvote_count":"4","content":"my selection A"}],"timestamp":"2019-09-18 07:28:00","answers_community":[],"question_images":[],"answer_description":"","answer":"A","url":"https://www.examtopics.com/discussions/amazon/view/5354-exam-aws-certified-big-data-specialty-topic-1-question-54/","question_text":"An administrator is deploying Spark on Amazon EMR for two distinct use cases: machine learning algorithms and ad-hoc querying. All data will be stored in Amazon S3. Two separate clusters for each use case will be deployed. The data volumes on Amazon S3 are less than 10 GB.\nHow should the administrator align instance types with the clusters purpose?"}],"exam":{"lastUpdated":"11 Apr 2025","isMCOnly":true,"name":"AWS Certified Big Data - Specialty","isBeta":false,"numberOfQuestions":85,"isImplemented":true,"id":17,"provider":"Amazon"},"currentPage":10},"__N_SSP":true}