{"pageProps":{"questions":[{"id":"0AnhXqAS7umpyJWMCCdS","answers_community":["B (100%)"],"answer_description":"","topic":"1","answer_images":[],"question_id":606,"unix_timestamp":1699083720,"url":"https://www.examtopics.com/discussions/amazon/view/125338-exam-aws-certified-solutions-architect-associate-saa-c03/","answer":"B","answer_ET":"B","question_images":[],"exam_id":31,"isMC":true,"timestamp":"2023-11-04 08:42:00","choices":{"C":"Create an Amazon S3 File Gateway on premises Configure the S3 File Gateway to perform the online data transfer to an S3 bucket","D":"Configure an accelerator in Amazon S3 Transfer Acceleration on premises. Configure the accelerator to perform the online data transfer to an S3 bucket.","A":"Order an AWS Snowball Edge device. Configure the Snowball Edge device to perform the online data transfer to an S3 bucket","B":"Deploy an AWS DataSync agent on premises. Configure the DataSync agent to perform the online data transfer to an S3 bucket."},"discussion":[{"comment_id":"1084251","poster":"TariqKipkemei","content":"Selected Answer: B\nDuring a transfer, AWS DataSync always checks the integrity of your data.\n\nhttps://docs.aws.amazon.com/datasync/latest/userguide/configure-data-verification-options.html","upvote_count":"12","timestamp":"1701340800.0"},{"timestamp":"1699315980.0","poster":"potomac","content":"Selected Answer: B\nDuring a transfer, AWS DataSync always checks the integrity of your data, but you can specify how and when this verification happens with the following options: Verify only the data transferred (recommended) – DataSync calculates the checksum of transferred files and metadata at the source location.\n\nhttps://docs.aws.amazon.com/datasync/latest/userguide/configure-data-verification-options.html","upvote_count":"6","comment_id":"1064358"},{"poster":"KennethNg923","comment_id":"1231598","content":"Selected Answer: B\n\"automatically validate the integrity of the data after the transfer\" -> so it should be datasync","timestamp":"1718585280.0","upvote_count":"2"},{"content":"Selected Answer: B\nhttps://aws.amazon.com/datasync/faqs/","comment_id":"1061949","upvote_count":"2","poster":"dilaaziz","timestamp":"1699083720.0"}],"question_text":"A company stores its data on premises. The amount of data is growing beyond the company's available capacity.\n\nThe company wants to migrate its data from the on-premises location to an Amazon S3 bucket. The company needs a solution that will automatically validate the integrity of the data after the transfer.\n\nWhich solution will meet these requirements?"},{"id":"PmNfEMDof32O7mfkjgx0","timestamp":"2023-11-07 01:29:00","discussion":[{"timestamp":"1720787100.0","comment_id":"1120832","content":"Selected Answer: A\nKey requirement it \"maximize availability while minimizing the operational overhead\" of 200 zones to process million requests\n\nR53 is designed exactly to do this and supports zone import functionality so literally does the job of their EC2 servers but much better so BCD become \"overhead\" by default. I doubt D will work.","poster":"awsgeek75","upvote_count":"5"},{"poster":"pentium75","comment_id":"1111930","timestamp":"1719919440.0","content":"Selected Answer: A\nB, C and D would not \"maximize availability\" (not HA) and also not minimize the operational overhead.","upvote_count":"4"},{"comment_id":"1084252","poster":"TariqKipkemei","content":"Selected Answer: A\n'maximize availability while minimizing the operational overhead' = severless = Amazon Route 53","timestamp":"1717058520.0","upvote_count":"4"},{"comment_id":"1073963","content":"Selected Answer: A\nOnly A makes sense","poster":"EdenWang","timestamp":"1716026040.0","upvote_count":"3"},{"poster":"NickGordon","upvote_count":"4","content":"Selected Answer: A\nShould be A\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/migrate-dns-domain-in-use.html","timestamp":"1715299320.0","comment_id":"1066883"},{"comment_id":"1064371","comments":[{"comment_id":"1111929","timestamp":"1719919380.0","upvote_count":"3","poster":"pentium75","content":"No, \"Desired capacity 1\" meaning that usually only 1 server would run, but they want to \"maximize availability\". And operating EC2 servers would not be \"minimizing the operational overhead that is related to the management of the two servers.\""},{"comment_id":"1127206","poster":"awsgeek75","content":"1 EC2 server for millions of requests?","timestamp":"1721465220.0","upvote_count":"2"}],"content":"Selected Answer: D\nD makes more sense to me","poster":"potomac","timestamp":"1715034540.0","upvote_count":"1"}],"answers_community":["A (95%)","5%"],"url":"https://www.examtopics.com/discussions/amazon/view/125541-exam-aws-certified-solutions-architect-associate-saa-c03/","isMC":true,"choices":{"B":"Launch a single large Amazon EC2 instance Import zone tiles. Configure Amazon CloudWatch alarms and notifications to alert the company about any downtime.","C":"Migrate the servers to AWS by using AWS Server Migration Service (AWS SMS). Configure Amazon CloudWatch alarms and notifications to alert the company about any downtime.","D":"Launch an Amazon EC2 instance in an Auto Scaling group across two Availability Zones. Import zone files. Set the desired capacity to 1 and the maximum capacity to 3 for the Auto Scaling group. Configure scaling alarms to scale based on CPU utilization.","A":"Create 200 new hosted zones in the Amazon Route 53 console Import zone files."},"answer_images":[],"unix_timestamp":1699316940,"answer_description":"","topic":"1","exam_id":31,"answer_ET":"A","question_id":607,"question_text":"A company wants to migrate two DNS servers to AWS. The servers host a total of approximately 200 zones and receive 1 million requests each day on average. The company wants to maximize availability while minimizing the operational overhead that is related to the management of the two servers.\n\nWhat should a solutions architect recommend to meet these requirements?","answer":"A","question_images":[]},{"id":"mrkH61Lk04UGv4DGjlAb","answer_ET":"C","answers_community":["C (100%)"],"question_id":608,"topic":"1","timestamp":"2023-11-05 21:00:00","answer":"C","answer_images":[],"discussion":[{"content":"Selected Answer: C\nS3 storage lenses can be used to find incomplete multipart uploads: https://aws.amazon.com/blogs/aws-cloud-financial-management/discovering-and-deleting-incomplete-multipart-uploads-to-lower-amazon-s3-costs/","timestamp":"1714932000.0","comment_id":"1063230","poster":"warp","upvote_count":"7"},{"upvote_count":"5","timestamp":"1719412980.0","comment_id":"1106223","content":"Selected Answer: C\nS3 Storage Lens provides four Cost Efficiency metrics for analyzing incomplete multipart uploads in your S3 buckets. These metrics are free of charge and automatically configured for all S3 Storage Lens dashboards.\n\nIncomplete Multipart Upload Storage Bytes – The total bytes in scope with incomplete multipart uploads\n% Incomplete MPU Bytes – The percentage of bytes in scope that are results of incomplete multipart uploads\nIncomplete Multipart Upload Object Count – The number of objects in scope that are incomplete multipart uploads\n% Incomplete MPU Objects – The percentage of objects in scope that are incomplete multipart uploads\nhttps://aws.amazon.com/blogs/aws-cloud-financial-management/discovering-and-deleting-incomplete-multipart-uploads-to-lower-amazon-s3-costs/","poster":"LocNV"},{"poster":"awsgeek75","comment_id":"1120837","timestamp":"1720787400.0","content":"Selected Answer: C\nABD cannot do any of this so C is the right product for this use case","upvote_count":"3"},{"comment_id":"1084253","upvote_count":"4","content":"Selected Answer: C\nAmazon S3 Storage Lens is a cloud storage analytics solution with support for AWS Organizations to give you organization-wide visibility into object storage, with point-in-time metrics and trend lines as well as actionable recommendations.","poster":"TariqKipkemei","timestamp":"1717058580.0"},{"upvote_count":"2","content":"Selected Answer: C\nC for sure","poster":"potomac","timestamp":"1715034720.0","comment_id":"1064374"}],"unix_timestamp":1699214400,"question_text":"A global company runs its applications in multiple AWS accounts in AWS Organizations. The company's applications use multipart uploads to upload data to multiple Amazon S3 buckets across AWS Regions. The company wants to report on incomplete multipart uploads for cost compliance purposes.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","choices":{"A":"Configure AWS Config with a rule to report the incomplete multipart upload object count.","C":"Configure S3 Storage Lens to report the incomplete multipart upload object count.","B":"Create a service control policy (SCP) to report the incomplete multipart upload object count.","D":"Create an S3 Multi-Region Access Point to report the incomplete multipart upload object count."},"question_images":[],"isMC":true,"exam_id":31,"url":"https://www.examtopics.com/discussions/amazon/view/125459-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_description":""},{"id":"zpmmJw9J8z77uVdGKWw4","choices":{"B":"Use native backup and restore. Restore the data to the upgraded new version of Amazon RDS for MySQL.","C":"Use AWS Database Migration Service (AWS DMS) to replicate the data to the upgraded new version of Amazon RDS for MySQL.","D":"Use Amazon RDS Blue/Green Deployments to deploy and test production changes.","A":"Create an RDS manual snapshot. Upgrade to the new version of Amazon RDS for MySQL."},"topic":"1","isMC":true,"answer":"D","answer_images":[],"answers_community":["D (100%)"],"answer_ET":"D","answer_description":"","unix_timestamp":1699214700,"exam_id":31,"timestamp":"2023-11-05 21:05:00","discussion":[{"comment_id":"1084257","content":"Selected Answer: D\nA blue/green deployment copies a production database environment to a separate, synchronized staging environment. You can make changes to the database in the staging environment without affecting the production environment. When you are ready, you can promote the staging environment to be the new production database environment, with downtime typically under one minute.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/blue-green-deployments.html","poster":"TariqKipkemei","upvote_count":"10","timestamp":"1701341460.0"},{"poster":"warp","comment_id":"1063231","timestamp":"1699214700.0","upvote_count":"6","content":"Selected Answer: D\nYou can make changes to the RDS DB instances in the green environment without affecting production workloads. For example, you can upgrade the major or minor DB engine version, upgrade the underlying file system configuration, or change database parameters in the staging environment. You can thoroughly test changes in the green environment.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/blue-green-deployments-overview.html"},{"upvote_count":"2","comment_id":"1229474","content":"Option A (Create an RDS manual snapshot and upgrade) is the most straightforward and least operationally intensive method to upgrade your Amazon RDS for MySQL instance while ensuring data safety and allowing thorough testing of application functionality post-upgrade. This approach leverages RDS's snapshot capabilities to provide a reliable rollback mechanism if needed, making it the recommended choice for your scenario.","timestamp":"1718223060.0","poster":"cjace"},{"upvote_count":"4","comment_id":"1120954","content":"Selected Answer: D\nLeast overhead, only CD qualify and D is actually a managed solution for what is being proposed (hopefully) in C so it's better.","poster":"awsgeek75","timestamp":"1705080000.0"},{"timestamp":"1704502380.0","upvote_count":"1","content":"C works for me","comment_id":"1114862","poster":"foha2012"},{"content":"Selected Answer: D\nD is the answer","comment_id":"1064378","poster":"potomac","upvote_count":"1","timestamp":"1699317420.0"}],"question_id":609,"url":"https://www.examtopics.com/discussions/amazon/view/125460-exam-aws-certified-solutions-architect-associate-saa-c03/","question_images":[],"question_text":"A company runs a production database on Amazon RDS for MySQL. The company wants to upgrade the database version for security compliance reasons. Because the database contains critical data, the company wants a quick solution to upgrade and test functionality without losing any data.\n\nWhich solution will meet these requirements with the LEAST operational overhead?"},{"id":"e5ZyBmFzrltGQ7OlUGbF","answers_community":["A (99%)","1%"],"unix_timestamp":1666093440,"answer":"A","isMC":true,"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/85795-exam-aws-certified-solutions-architect-associate-saa-c03/","question_images":[],"choices":{"A":"Save the .pdf files to Amazon S3. Configure an S3 PUT event to invoke an AWS Lambda function to convert the files to .jpg format and store them back in Amazon S3.","C":"Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2 instances, Amazon Elastic Block Store (Amazon EBS) storage, and an Auto Scaling group. Use a program in the EC2 instances to convert the files to .jpg format. Save the .pdf files and the .jpg files in the EBS store.","B":"Save the .pdf files to Amazon DynamoDUse the DynamoDB Streams feature to invoke an AWS Lambda function to convert the files to .jpg format and store them back in DynamoDB.","D":"Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2 instances, Amazon Elastic File System (Amazon EFS) storage, and an Auto Scaling group. Use a program in the EC2 instances to convert the file to .jpg format. Save the .pdf files and the .jpg files in the EBS store."},"question_id":610,"timestamp":"2022-10-18 13:44:00","answer_ET":"A","question_text":"A company runs its infrastructure on AWS and has a registered base of 700,000 users for its document management application. The company intends to create a product that converts large .pdf files to .jpg image files. The .pdf files average 5 MB in size. The company needs to store the original files and the converted files. A solutions architect must design a scalable solution to accommodate demand that will grow rapidly over time.\nWhich solution meets these requirements MOST cost-effectively?","answer_description":"","exam_id":31,"discussion":[{"comment_id":"698179","poster":"ArielSchivo","upvote_count":"51","timestamp":"1666093440.0","comments":[{"timestamp":"1666974420.0","comment_id":"706620","upvote_count":"5","comments":[{"upvote_count":"6","timestamp":"1701935280.0","comment_id":"1090035","content":"lambda has near inifinite scale","poster":"EtherealBagel"}],"content":"is lambda scalable as an EC2 ?","poster":"raffaello44"},{"poster":"rob74","content":"In addition to this Lambda is paid only when used....","timestamp":"1667329800.0","upvote_count":"7","comment_id":"709395"},{"timestamp":"1671560400.0","poster":"mrbottomwood","content":"I'm thinking when you wrote DocumentDB you meant it as DynamoDB...yes?","comments":[{"upvote_count":"8","timestamp":"1671851640.0","content":"Yes, DynamoDB has 400KB limit for the item.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ServiceQuotas.html","comment_id":"754676","poster":"benjl"}],"comment_id":"751329","upvote_count":"6"}],"content":"Selected Answer: A\nOption A. Elastic BeanStalk is expensive, and DocumentDB has a 400KB max to upload files. So Lambda and S3 should be the one."},{"upvote_count":"10","comment_id":"929467","content":"Selected Answer: A\nB. Using DynamoDB for storing and processing large .pdf files would not be cost-effective due to storage and throughput costs associated with DynamoDB.\n\nC. Using Elastic Beanstalk with EC2 and EBS storage can work, but it may not be most cost-effective solution. It involves managing the underlying infrastructure and scaling manually.\n\nD. Similar to C, using Elastic Beanstalk with EC2 and EFS storage can work, but it may not be most cost-effective solution. EFS is a shared file storage service and may not provide optimal performance for conversion process, especially as demand and file sizes increase.\n\nA. leverages Lambda and the scalable and cost-effective storage of S3. With Lambda, you only pay for actual compute time used during the file conversion, and S3 provides durable and scalable storage for both .pdf files and .jpg files. The S3 PUT event triggers Lambda to perform conversion, eliminating need to manage infrastructure and scaling, making it most cost-effective solution for this scenario.","poster":"cookieMr","timestamp":"1687349880.0"},{"upvote_count":"1","content":"Selected Answer: A\nAns A - a simple get and PUT back to S3 bucket. At \"...average 5 MB\" the returned .jpeg files should be smaller than standard S3","timestamp":"1726059360.0","comment_id":"1282113","poster":"PaulGa"},{"comment_id":"1234241","content":"BeanStack is expensive solution, and dynamoDB have a limitation of 400KB max to upload files. So Lambda an S3 should be the one.","upvote_count":"1","timestamp":"1718954880.0","poster":"shasi07"},{"timestamp":"1711906680.0","comment_id":"1186918","content":"Given the company's requirement for access to both AWS and on-premises file storage with minimum latency and no significant changes to existing file access patterns, the most suitable option is:\n\nA. Deploy and configure Amazon FSx for Windows File Server on AWS. Move the on-premises file data to FSx for Windows File Server. Reconfigure the workloads to use FSx for Windows File Server on AWS.","upvote_count":"1","poster":"Uzbekistan"},{"upvote_count":"1","content":"Selected Answer: A\nS3 is the only scalable option for such a large user base in cost effective way. \nBCD can work but will be extremely costly","poster":"awsgeek75","timestamp":"1705247340.0","comment_id":"1122644"},{"timestamp":"1698402960.0","poster":"Ruffyit","content":"B. Using DynamoDB for storing and processing large .pdf files would not be cost-effective due to storage and throughput costs associated with DynamoDB.\n\nC. Using Elastic Beanstalk with EC2 and EBS storage can work, but it may not be most cost-effective solution. It involves managing the underlying infrastructure and scaling manual","comment_id":"1055458","upvote_count":"1"},{"upvote_count":"4","poster":"Guru4Cloud","comment_id":"977932","timestamp":"1691687160.0","content":"Selected Answer: A\nOption A is the most cost-effective solution that meets the requirements. Here is why:\n\nStoring the PDFs in Amazon S3 is inexpensive and scalable storage.\nUsing S3 events to trigger Lambda functions to do the file conversion is a serverless approach that scales automatically. No need to manage EC2 instances.\nLambda usage is charged only for compute time used, which is cost-efficient for spiky workloads like this.\nStoring the converted JPGs back in S3 keeps the storage scalable and cost-effective."},{"upvote_count":"2","comment_id":"957365","poster":"RDX19","content":"Selected Answer: A\nOption A is right answer since Dynamo DB has size limitations.","timestamp":"1689846540.0"},{"poster":"miki111","timestamp":"1689782040.0","upvote_count":"1","content":"Option A is the right answer.","comment_id":"956790"},{"content":"Selected Answer: A\nThe solution meets these requirements most cost-effectively is option A.","poster":"Bmarodi","upvote_count":"1","timestamp":"1685983320.0","comment_id":"915586"},{"comment_id":"902659","timestamp":"1684597680.0","upvote_count":"1","poster":"Bmarodi","content":"Selected Answer: A\nI think the best solution is A.\nRef. https://s3.amazonaws.com/doc/s3-developer-guide/RESTObjectPUT.html"},{"poster":"Abrar2022","content":"Since this requires a cost-effect solution then you can use Lambda to convert pdf files to jpeg and store them on S3. Lambda is serverless, so only pay when you use it and automatically scales to cope with demand.","comment_id":"901610","timestamp":"1684463820.0","upvote_count":"1"},{"timestamp":"1682564580.0","poster":"srirajav","comments":[{"timestamp":"1682743140.0","comment_id":"884037","content":"In question, it is never mentioned that the jpg files will also be stored in same s3 bucket. We can have different s3 buckets right ?","poster":"bedwal2020","upvote_count":"2"}],"comment_id":"882182","content":"if Option A is correct, however storing the data back to the same S3, wont it cause infinite looping, it's not best practice right storing a object that is processed by Lambda function to the same S3 bucket, it has chances to cause infinite Loop and then if the option B cant we increase the limits of Dynamo DB requesting AWS?","upvote_count":"3"},{"poster":"cheese929","timestamp":"1681622040.0","comment_id":"871478","content":"Selected Answer: A\nAnswer A is the most cost effective solution that meets the requirement","upvote_count":"1"},{"poster":"channn","content":"Selected Answer: A\nKey words: MOST cost-effectively, so S3 + Lambda","comment_id":"858670","upvote_count":"1","timestamp":"1680425880.0"},{"poster":"SilentMilli","comment_id":"768203","content":"Selected Answer: A\nThis solution will meet the company's requirements in a cost-effective manner because it uses a serverless architecture with AWS Lambda to convert the files and store them in S3. The Lambda function will automatically scale to meet the demand for file conversions and S3 will automatically scale to store the original and converted files as needed.","timestamp":"1673057700.0","upvote_count":"2"},{"upvote_count":"1","comments":[{"timestamp":"1671557280.0","upvote_count":"2","content":"Option C is also a valid solution, but it may be more expensive due to the use of EC2 instances, EBS storage, and an Auto Scaling group. These resources can add additional cost, especially if the demand for the conversion service grows rapidly.\n\nOption D is not a valid solution because it uses Amazon EFS, which is a file storage service that is not suitable for storing large amounts of data. EFS is designed for storing and accessing files that are accessed frequently, such as application logs and media files. It is not designed for storing large files like .pdf or .jpg files.","comments":[{"comment_id":"768553","timestamp":"1673095140.0","upvote_count":"1","poster":"karbob","content":"EFS is optimized for a wide range of workloads and file sizes, and it can store files of any size up to the capacity of the file system. EFS scales automatically to meet your storage needs, and it can store petabyte-level capacity."}],"poster":"Buruguduystunstugudunstuy","comment_id":"751280"}],"comment_id":"751279","timestamp":"1671557100.0","poster":"Buruguduystunstugudunstuy","content":"Selected Answer: A\nOption A is the most cost-effective solution that meets the requirements.\n\nIn this solution, the .pdf files are saved to Amazon S3, which is an object storage service that is highly scalable, durable, and secure. S3 can store unlimited amounts of data at a very low cost.\n\nThe S3 PUT event triggers an AWS Lambda function to convert the .pdf files to .jpg format. Lambda is a serverless compute service that runs code in response to specific events and automatically scales to meet demand. This means that the conversion process can scale up or down as needed, without the need for manual intervention.\n\nThe converted .jpg files are then stored back in S3, which allows the company to store both the original .pdf files and the converted .jpg files in the same service. This reduces the complexity of the solution and helps to keep costs low."},{"content":"Selected Answer: A\nOption A","upvote_count":"1","comment_id":"749387","timestamp":"1671417180.0","poster":"career360guru"},{"upvote_count":"1","content":"This gives an example, using GET rather than PUT, but the idea is the same: https://docs.aws.amazon.com/AmazonS3/latest/userguide/tutorial-s3-object-lambda-uppercase.html","poster":"JayBee65","comment_id":"736822","timestamp":"1670331660.0"},{"comment_id":"723599","upvote_count":"1","content":"A is correct","timestamp":"1669040580.0","poster":"Wpcorgan"},{"comment_id":"718618","poster":"TonyghostR05","upvote_count":"1","timestamp":"1668502800.0","content":"S3 is cost effective"},{"comments":[{"poster":"ludovikush","timestamp":"1667387340.0","upvote_count":"11","content":"It is not correct because the maximum item size in DynamoDB is 400 KB.","comment_id":"709753"},{"poster":"pentium75","upvote_count":"1","comment_id":"1105117","timestamp":"1703494380.0","content":"DynamoDB is meant for storing JSON documents with key-value pairs, not PDF files."}],"poster":"goku58","upvote_count":"1","timestamp":"1666925940.0","content":"Selected Answer: B\nFor rapid scalability, B - DynamoDB looks to be a better solution.","comment_id":"706061"}],"topic":"1"}],"exam":{"isMCOnly":true,"isImplemented":true,"provider":"Amazon","numberOfQuestions":1019,"name":"AWS Certified Solutions Architect - Associate SAA-C03","lastUpdated":"11 Apr 2025","isBeta":false,"id":31},"currentPage":122},"__N_SSP":true}