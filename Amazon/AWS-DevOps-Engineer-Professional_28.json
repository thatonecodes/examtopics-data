{"pageProps":{"questions":[{"id":"7ZxVW42KhgntfJiJ31rs","isMC":true,"question_id":136,"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/2843-exam-aws-devops-engineer-professional-topic-1-question-33/","answer":"C","exam_id":35,"choices":{"A":"Use AWS Systems Manager Configuration Compliance. Use calls to the put-compliance-items API action to scan and build a database of noncompliant EC2 instances based on their host placement configuration. Use an Amazon DynamoDB table to store these instance IDs for fast access. Generate a report through Systems Manager by calling the list-compliance-summaries API action.","C":"Use AWS Config. Identify all EC2 instances to be audited by enabling Config Recording on all Amazon EC2 resources for the region. Create a custom AWS Config rule that triggers an AWS Lambda function by using the ג€config-rule-change-triggeredג€ blueprint. Modify the Lambda evaluateCompliance() function to verify host placement to return a NON_COMPLIANT result if the instance is not running on an EC2 Dedicated Host. Use the AWS Config report to address noncompliant instances.","D":"Use AWS CloudTrail. Identify all EC2 instances to be audited by analyzing all calls to the EC2 RunCommand API action. Invoke an AWS Lambda function that analyzes the host placement of the instance. Store the EC2 instance ID of noncompliant resources in an Amazon RDS MySQL DB instance. Generate a report by querying the RDS instance and exporting the query results to a CSV text file.","B":"Use custom Java code running on an EC2 instance. Set up EC2 Auto Scaling for the instance depending on the number of instances to be checked. Send the list of noncompliant EC2 instance IDs to an Amazon SQS queue. Set up another worker instance to process instance IDs from the SQS queue and write them to Amazon DynamoDB. Use an AWS Lambda function to terminate noncompliant instance IDs obtained from the queue, and send them to an Amazon SNS email topic for distribution."},"question_images":[],"topic":"1","answer_ET":"C","timestamp":"2019-07-24 19:20:00","answer_images":[],"unix_timestamp":1563988800,"discussion":[{"timestamp":"1632640920.0","upvote_count":"24","content":"Correct Answer is \"C\" A will only help in ompliance to scan your fleet of managed instances for patch compliance and configuration inconsistencies. but when it comes to software licensing compliance aws config rules nables you to assess compliance with your server-bound software licenses","comment_id":"12267","poster":"YashBindlish"},{"comments":[{"poster":"pleasespammelater","timestamp":"1634533500.0","content":"This is for tracking what is running on dedicated hosts. It won't help with detecting when your software ISN'T running on your dedicated hosts. For that you need a custom rule. https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_develop-rules_getting-started.html\n\nAlso, the Lambda blueprint function is actually called \"evaluateChangeNotificationCompliance\". I'm guessing this has just changed over time so it's still the correct answer. https://console.aws.amazon.com/lambda/home?region=us-east-1#/create/function/configure/blueprint?blueprint=config-rule-change-triggered","upvote_count":"2","comment_id":"72638"}],"comment_id":"30656","poster":"amzngenius","timestamp":"1633490160.0","content":"C：\nhttps://aws.amazon.com/about-aws/whats-new/2015/11/use-aws-config-to-track-ec2-instances-on-dedicated-hosts-and-assess-license-compliance/","upvote_count":"9"},{"upvote_count":"1","content":"Correct answer is C.","poster":"Bulti","comment_id":"778116","timestamp":"1673894640.0"},{"timestamp":"1662461340.0","upvote_count":"1","comment_id":"661108","content":"Selected Answer: C\nAns: C","poster":"SamHan"},{"poster":"nebojsaMa","content":"The answer is C why:\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-aws-config.html\n\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-compliance-about.html#sysman-compliance-custom\n\nhttps://aws.amazon.com/blogs/aws/now-available-ec2-dedicated-hosts/","comment_id":"323450","upvote_count":"2","timestamp":"1635817920.0"},{"content":"I'll go with C","poster":"WhyIronMan","timestamp":"1635816180.0","upvote_count":"2","comment_id":"321271"},{"content":"Correct Answer: C\n\nReference:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-aws-config.html\n\nhttps://aws.amazon.com/about-aws/whats-new/2015/11/use-aws-config-to-track-ec2-instances-on-dedicated-hosts-and-assess-license-compliance/","timestamp":"1635778080.0","comment_id":"318817","poster":"aws_Tamilan","upvote_count":"1"},{"upvote_count":"7","poster":"fogunfunminiyi","content":"C is the answer. Remember when compliance is mentioned, think config, config rule, etc. If the compliance has to do with EC2 instance or instance AMI inspection, think inspector","timestamp":"1635431220.0","comment_id":"252558"},{"poster":"jackdryan","comment_id":"223216","content":"I'll go with C","upvote_count":"2","timestamp":"1635142740.0"},{"upvote_count":"1","comment_id":"205837","content":"I'll go with C\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-aws-config.html","timestamp":"1634945640.0","poster":"ChauPhan"},{"poster":"Raj9","comment_id":"94948","content":"C - directly related to https://aws.amazon.com/blogs/aws/now-available-ec2-dedicated-hosts/","timestamp":"1634911800.0","upvote_count":"1"},{"content":"C is correct","upvote_count":"1","comment_id":"62153","poster":"yassu","timestamp":"1634394360.0"},{"comment_id":"54999","poster":"xaocho","upvote_count":"1","timestamp":"1634105400.0","content":"It is C"},{"comment_id":"50583","timestamp":"1634070540.0","content":"C -- AWS Conﬁg records the conﬁguration details of Dedicated hosts and the instances that you launch on them","upvote_count":"1","poster":"AdityaB"},{"poster":"dinhvu","upvote_count":"1","content":"answer is C\nthanks amzngenus for the document","comment_id":"36040","timestamp":"1633815720.0"},{"comment_id":"28366","content":"i will go with c","timestamp":"1633237080.0","upvote_count":"3","poster":"jiedee"},{"upvote_count":"6","poster":"neil001","comments":[{"content":"The page must have been changed - License Usage Reporting is not there.","comment_id":"72637","upvote_count":"1","timestamp":"1634472960.0","poster":"pleasespammelater"}],"content":"Answer is indeed C and mentioned in the aws dedicated hosts web page, check \"License Usage Reporting\" in the link below\nhttps://aws.amazon.com/ec2/dedicated-hosts/","comment_id":"12871","timestamp":"1632853020.0"},{"comments":[{"poster":"V007","timestamp":"1634921400.0","upvote_count":"2","content":"A is wrong because SSM checks from inside EC2 whereas Dedicated Host compliance should be outside.","comment_id":"139483"}],"poster":"agomes","timestamp":"1632299340.0","content":"Couldn't be option A?\nref: https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-compliance.html","upvote_count":"1","comment_id":"4842"}],"question_text":"A healthcare services company is concerned about the growing costs of software licensing for an application for monitoring patient wellness. The company wants to create an audit process to ensure that the application is running exclusively on Amazon EC2 Dedicated Hosts. A DevOps Engineer must create a workflow to audit the application to ensure compliance.\nWhat steps should the Engineer take to meet this requirement with the LEAST administrative overhead?","answers_community":["C (100%)"]},{"id":"QJb4MVK2rHeb2xJizdVs","answer":"BCE","topic":"1","isMC":true,"question_id":137,"answers_community":["BCE (75%)","BCF (25%)"],"answer_images":[],"question_text":"A company has 100 GB of log data in an Amazon S3 bucket stored in .csv format. SQL developers want to query this data and generate graphs to visualize it.\nThey also need an efficient, automated way to store metadata from the .csv file.\nWhich combination of steps should be taken to meet these requirements with the LEAST amount of effort? (Choose three.)","url":"https://www.examtopics.com/discussions/amazon/view/48004-exam-aws-devops-engineer-professional-topic-1-question-34/","discussion":[{"comments":[{"upvote_count":"9","poster":"shammous","comment_id":"450833","content":"Glue is an ETL. S3 would be the service to choose to store metadata: B, C , F","timestamp":"1635949800.0"}],"timestamp":"1632080760.0","comment_id":"317976","poster":"Jordanro","upvote_count":"21","content":"I will go with B,C,E"},{"poster":"WhyIronMan","timestamp":"1633002000.0","comment_id":"326921","content":"I'll go with B, C, F\n\nThat's not use case for Glue","upvote_count":"14"},{"content":"B, C and F. Glue is not a service to persist data.","upvote_count":"2","poster":"Stipy","timestamp":"1719076680.0","comment_id":"1235523"},{"poster":"Rahul369","upvote_count":"1","timestamp":"1715672280.0","comment_id":"1211279","content":"Selected Answer: BCE\nhttps://docs.aws.amazon.com/glue/latest/dg/components-overview.html#data-catalog-intro"},{"timestamp":"1707889320.0","content":"AWS Glue is not a persistent data store itself. AWS Glue is a fully managed extract, transform, and load (ETL) service that is designed to make it easy for you to prepare and load your data for analysis. It helps in the process of moving and transforming data from various sources to your desired target, such as a data warehouse or data lake.","poster":"primetimesosa","upvote_count":"2","comment_id":"1149852"},{"comment_id":"969437","timestamp":"1690926900.0","poster":"DaddyDee","upvote_count":"1","content":"I would go with BCF: https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingMetadata.html\n\nYou can set object metadata in Amazon S3 at the time you upload the object. Object metadata is a set of name-value pairs and the log files are already in S3"},{"poster":"DWsk","upvote_count":"1","comment_id":"892245","timestamp":"1683555300.0","content":"Selected Answer: BCE\nI was also convinced it was BCF at first, but I think the answer is just worded confusingly. Glue on its own is an ETL tool. But Glue Data Catalog is for storing metadata, so that's probably what the answer is referring to."},{"upvote_count":"1","poster":"kkkkkkkkkkkkkkk","comment_id":"846111","content":"B, C, F. Glue Data Catalog stores metadata but not Glue. Glue is the ETL tool.","timestamp":"1679411880.0"},{"poster":"yogi3100","upvote_count":"1","timestamp":"1677832800.0","content":"Selected Answer: BCE\nB, C, E.\nFor those who think F is correct over E, the question is not asking about the metadata storage... its asking about automated way to store metadata... that's what Glue can do easily.","comment_id":"827783"},{"timestamp":"1676968680.0","comment_id":"816369","upvote_count":"3","content":"Selected Answer: BCE\nTextbook question\nMetastore -> Always Glue\nAnalyze data from S3 -> Athena + Quicksight (if not real-time)","poster":"milofficial"},{"comments":[{"comment_id":"798112","upvote_count":"1","timestamp":"1675524960.0","content":"Sorry, F is better than E, because we are storing metadata.","poster":"Piccaso"}],"poster":"Piccaso","timestamp":"1675524780.0","upvote_count":"1","content":"Selected Answer: BCE\nX-Ray is for tracing apps. A is excluded\nAmazon Redshift is data warehouse, can be too heavy. D is excluded\nE is better than F.","comment_id":"798107"},{"upvote_count":"2","content":"B,C abd E are the right choices.","timestamp":"1673894760.0","poster":"Bulti","comment_id":"778121"},{"upvote_count":"2","comment_id":"760396","timestamp":"1672266900.0","content":"Selected Answer: BCE\nGo with BCE","poster":"Teonardo"},{"timestamp":"1672264380.0","upvote_count":"2","content":"I'll go with B, C and E","comment_id":"760369","poster":"saeidp"},{"poster":"apcloud","timestamp":"1672088400.0","content":"Selected Answer: BCE\nGlue data catalogue is persistent meta data store per links in other comments","upvote_count":"2","comment_id":"757843"},{"timestamp":"1671271560.0","comment_id":"747977","poster":"Arkarter","upvote_count":"3","content":"Selected Answer: BCE\nWS Glue uses the AWS Glue Data Catalog to store metadata about data sources, transforms, and targets.\nREF : https://docs.aws.amazon.com/glue/latest/dg/components-overview.html"},{"upvote_count":"3","poster":"neta1o","timestamp":"1671073200.0","comment_id":"745659","content":"Selected Answer: BCE\n\"The AWS Glue Data Catalog is a fully-managed persistent metadata store...\" Ref: https://docs.aws.amazon.com/whitepapers/latest/best-practices-building-data-lake-for-games/data-cataloging.html"},{"poster":"DonWang","timestamp":"1670294040.0","comment_id":"736482","upvote_count":"3","content":"Selected Answer: BCF\nAWS Glue is good for metadata, but the requirement is LEAST effort"},{"poster":"jlb","timestamp":"1668663420.0","comment_id":"720238","content":"BCF \nAWS Glue :https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-format-csv-home.html => Prerequisites: You will need the S3 paths (s3path) to the CSV files or folders that you want to read","upvote_count":"1"},{"comment_id":"715654","content":"Selected Answer: BCE\nKeyword mapping from the question to answer:\nB - Generate graphs to visualize data\nC - Query the data\nE - Efficient and automated way to store metadata","poster":"developer_404","upvote_count":"2","timestamp":"1668128100.0"},{"poster":"Chuky64","timestamp":"1663870320.0","comment_id":"676452","upvote_count":"2","content":"Selected Answer: BCE\nI think so."},{"timestamp":"1663789380.0","upvote_count":"3","content":"Selected Answer: BCE\nThe AWS Glue Data Catalog is your persistent technical metadata store in the AWS Cloud - The AWS Glue Data Catalog is your persistent technical metadata store in the AWS Cloud.","poster":"RightAnswers","comment_id":"675473"},{"upvote_count":"3","poster":"SamHan","content":"Selected Answer: BCF\nAns: BCF","comment_id":"661111","timestamp":"1662461460.0"},{"poster":"ohcn","comment_id":"657763","timestamp":"1662146760.0","upvote_count":"3","content":"Selected Answer: BCE\nBCE - https://docs.aws.amazon.com/glue/latest/dg/components-overview.html#data-catalog-intro"},{"poster":"guchao2000","upvote_count":"3","content":"The AWS Glue Data Catalog is your persistent technical metadata store in the AWS Cloud.\nhttps://docs.aws.amazon.com/glue/latest/dg/components-overview.html","timestamp":"1661920740.0","comment_id":"654762"},{"poster":"[Removed]","comments":[{"comment_id":"591286","content":"I thinks it's BCE after all: https://aws.amazon.com/blogs/big-data/harmonize-query-and-visualize-data-from-various-providers-using-aws-glue-amazon-athena-and-amazon-quicksight/","upvote_count":"2","poster":"[Removed]","timestamp":"1650842100.0"}],"comment_id":"590261","content":"Selected Answer: BCF\nBCF, F is used for persistent metadata storing: https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingMetadata.html","upvote_count":"3","timestamp":"1650665220.0"},{"timestamp":"1639867380.0","upvote_count":"3","content":"B,C,E as S3 is already available. If we think from metadata store perspective it is glue catalog https://docs.aws.amazon.com/glue/latest/dg/components-overview.html","poster":"Balki","comment_id":"504498"},{"comment_id":"427309","upvote_count":"2","content":"The question is not clear about the definition of \"metadata\". I prefer F over E since you can store any kind of data with S3. Glue can only store table/column information.","timestamp":"1635577320.0","poster":"PatrickLi"},{"timestamp":"1634504700.0","comment_id":"333168","upvote_count":"2","comments":[{"content":"BCE\n--> https://docs.aws.amazon.com/glue/latest/dg/components-overview.html","timestamp":"1634696280.0","comment_id":"333169","poster":"sashsz","upvote_count":"1"}],"content":"--> BCF","poster":"sashsz"},{"upvote_count":"1","poster":"devopp","comment_id":"331033","content":"... cannot see how F gives u \"efficient, automated way to store metadata\"","timestamp":"1634110260.0"},{"comment_id":"331031","poster":"devopp","content":"B C E\nGlue has a Metadata Catalog for persisting metadata.","timestamp":"1633689660.0","upvote_count":"2"},{"upvote_count":"4","comment_id":"323895","content":"Answer is BCF. E is not correct as AWS Glue is not a persistent data store. AWS Glue is used to run ETL jobs and then places the data into target data stores like Amazon Redshift and Amazon S3.","poster":"sb333","timestamp":"1632983880.0","comments":[{"poster":"sb333","comment_id":"333672","timestamp":"1634823420.0","upvote_count":"2","content":"Will change to BCE. https://docs.aws.amazon.com/athena/latest/ug/data-sources-glue.html"}]},{"timestamp":"1632175920.0","upvote_count":"1","comment_id":"318945","poster":"Spavanko","content":"Answer is: BCE"}],"question_images":[],"answer_ET":"BCE","exam_id":35,"answer_description":"","timestamp":"2021-03-23 11:25:00","unix_timestamp":1616495100,"choices":{"D":"Query the data with Amazon Redshift.","E":"Use AWS Glue as the persistent metadata store.","B":"Filter the data through Amazon QuickSight to visualize the data.","A":"Filter the data through AWS X-Ray to visualize the data.","F":"Use Amazon S3 as the persistent metadata store.","C":"Query the data with Amazon Athena."}},{"id":"ZP3DEWhLVdFCrLtGWSTT","answers_community":["D (100%)"],"answer_ET":"D","answer":"D","isMC":true,"choices":{"D":"Use Amazon Kinesis Agent on each server to upload the logs and have Amazon Kinesis Data Firehose use an AWS Lambda function to normalize the logs before writing them to Amazon S3","A":"Have the application send its logs to an Amazon EMR cluster and normalize the logs before sending them to Amazon S3","C":"Keep the logs in Amazon S3 and use Amazon Redshift Spectrum to normalize the logs in place","B":"Have the application send its logs to Amazon QuickSight, then use the Amazon QuickSight SPICE engine to normalize the logs. Do the analysis directly from Amazon QuickSight"},"answer_description":"","question_images":[],"question_text":"A DevOps Engineer has several legacy applications that all generate different log formats. The Engineer must standardize the formats before writing them to\nAmazon S3 for querying and analysis.\nHow can this requirement be met at the LOWEST cost?","exam_id":35,"topic":"1","discussion":[{"comment_id":"778122","content":"Correct answer is D","poster":"Bulti","upvote_count":"3","timestamp":"1673894880.0"},{"upvote_count":"2","timestamp":"1670776080.0","content":"Selected Answer: D\nThe most cost-effective option would be to use Amazon Kinesis Agent on each server to upload the logs and have Amazon Kinesis Data Firehose use an AWS Lambda function to normalize the logs before writing them to Amazon S3. This option allows the DevOps Engineer to use a serverless solution for log normalization, which will reduce costs compared to running an Amazon EMR cluster or using Amazon Redshift Spectrum. Additionally, using Amazon Kinesis Data Firehose and AWS Lambda allows for easy scalability as the volume of logs increases.","comment_id":"741887","poster":"SatenderRathee"},{"comments":[{"comment_id":"881566","timestamp":"1682509980.0","poster":"tycho","upvote_count":"1","content":"yes, compared with EMR and Redshift the cost will be lower ; the B option is not possible"}],"comment_id":"255227","upvote_count":"4","content":"I understand why the answer is D but is this solution LOWEST cost?","timestamp":"1636113000.0","poster":"cm3646"},{"content":"D it is","comment_id":"252670","poster":"gmandala","upvote_count":"3","timestamp":"1635650100.0"},{"timestamp":"1635495120.0","comment_id":"224198","poster":"jackdryan","upvote_count":"2","content":"I'll go with D"},{"upvote_count":"1","comment_id":"209379","poster":"ChauPhan","content":"Go with D","timestamp":"1635403560.0"},{"timestamp":"1634877540.0","upvote_count":"3","content":"Additional qs from devops exam:\nQ- Need to backup sensitive s3 objects that are stored within an S3 bucket with private bucket policy using the S3 CROSS region replication. The objects need to be copied to a target bucket in a diff AWS region and account. What should be done for replication? (choose 3)\n\nA. Create a replication IAM role in the source account\nB. Create a replication IAM role in the target account\nC. Add statements to the source bucket policy allowing the replication IAM role to replicate objects\nD. Add statements to the target bucket policy allowing the replication IAM role to replicate objects\nE. Set accesscontroltranslation.owneroverride to true in the replication config and add a statement to the target bucket policy allowing the replication IAM role to override object ownership\nF. Set accesscontroltranslation.owneroverride to destination in the replication config and add a statement to the target bucket policy allowing the replication IAM role to override object ownership","comments":[{"poster":"palomino","timestamp":"1634968560.0","upvote_count":"5","comment_id":"144445","content":"IMHO - ADF\nhttps://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-replication.html\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/replication-change-owner.html"},{"upvote_count":"1","timestamp":"1678666620.0","poster":"easytoo","content":"It's ADF Jeff.","comment_id":"837482"}],"poster":"cupcake","comment_id":"135244"},{"content":"D is correct","comment_id":"87339","poster":"Socrates","upvote_count":"1","timestamp":"1633075080.0"},{"content":"D is correct","timestamp":"1633049880.0","upvote_count":"1","comment_id":"85404","poster":"leotoras"},{"content":"Answer is D","timestamp":"1632605220.0","poster":"solotvun","upvote_count":"2","comment_id":"83662"}],"url":"https://www.examtopics.com/discussions/amazon/view/19604-exam-aws-devops-engineer-professional-topic-1-question-35/","answer_images":[],"question_id":138,"timestamp":"2020-05-04 16:38:00","unix_timestamp":1588603080},{"id":"ZHmORlplfeFigXlZOd36","answers_community":["ADF (100%)"],"question_text":"A company needs to implement a robust CI/CD pipeline to automate the deployment of an application in AWS. The pipeline must support continuous integration, continuous delivery, and automatic rollback upon deployment failure. The entire CI/CD pipeline must be capable of being re-provisioned in alternate AWS accounts or Regions within minutes. A DevOps engineer has already created an AWS CodeCommit repository to store the source code.\nWhich combination of actions should be taken when building this pipeline to meet these requirements? (Choose three.)","timestamp":"2022-09-01 05:00:00","unix_timestamp":1662001200,"answer_ET":"ADF","question_id":139,"answer_images":[],"isMC":true,"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/78881-exam-aws-devops-engineer-professional-topic-1-question-36/","discussion":[{"timestamp":"1663226040.0","comment_id":"669584","upvote_count":"14","poster":"MichaelExam","content":"Selected Answer: ADF\nA - For \"Integration\"\nD - For \"automatic rollback upon deployment failure\".\nF - For \"The entire CI/CD pipeline must be capable of being re-provisioned in alternate AWS accounts or Regions within minutes\""},{"content":"I will go ADF, storing artifacts in s3 but why is it required in this scenario","comment_id":"794493","poster":"Hamza5","timestamp":"1675188840.0","upvote_count":"2"},{"upvote_count":"3","comment_id":"778128","poster":"Bulti","content":"A- To build artifact\nD- To support automatic deployment rollback\nE- To provision the Codepipeline in minutes in other accounts or regions.","timestamp":"1673895300.0"},{"upvote_count":"2","timestamp":"1668390960.0","content":"Selected Answer: ADF\nA - for CI\nD - ASG and ALB are not in scope, hence I go with simple deployment. \nF - For provisioning into other AWS Account and region.","comment_id":"717616","poster":"developer_404"},{"poster":"animalrj","content":"Selected Answer: ADF\nADF, CodeCommit dont have artefacts, CloudFormation is mandatory, ELB(ASG) are not a requirement, Codebuild to bild the project.","upvote_count":"2","comment_id":"689277","timestamp":"1665231000.0"},{"poster":"youonebe","timestamp":"1663591260.0","content":"ADF.\n\nA - YES. CodeBuild is needed for building the source code\nB - Artifact is built under CodeBuild's container environment, not codecommit\nC - LB ?\nD - YES. EB can be used for rollback\nE - Codepipeline itself can handle different stages\nF - YES. CF is needed from re-provision the same stuff","comment_id":"673281","upvote_count":"2"},{"comment_id":"655714","timestamp":"1662001200.0","upvote_count":"3","comments":[{"poster":"alexderg","content":"ADF - to address \"continuous delivery, and automatic rollback upon deployment failure\"","comment_id":"657402","timestamp":"1662121080.0","upvote_count":"3"},{"content":"It might be ABF. F because question is not asking about application deployment, but how quickly you can re-provision the pipeline in another region. Cloudformation is an option to re-provision it quickly.","timestamp":"1662148080.0","upvote_count":"1","comment_id":"657769","poster":"ohcn"}],"content":"ABF - AB self explanatory.\n\nF - https://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-stackset-deployment.html\n\nu need a way to re-provision the pipelines in another region. CloudFormation is an option.","poster":"colinquek"}],"answer_description":"","exam_id":35,"choices":{"B":"Copy the build artifact from CodeCommit to Amazon S3.","E":"Implement an Amazon SQS queue to decouple the pipeline components.","F":"Provision all resources using AWS CloudFormation.","C":"Create an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer (ALB) and set the ALB as the deployment target in AWS CodePipeline.","A":"Configure an AWS CodePipeline pipeline with a build stage using AWS CodeBuild.","D":"Create an AWS Elastic Beanstalk environment as the deployment target in AWS CodePipeline."},"answer":"ADF","topic":"1"},{"id":"NkLzvERIfqLGpWvxZUok","exam_id":35,"choices":{"A":"Create primary and secondary Amazon S3 buckets in two separate Availability Zones that are at least 500 miles (805 kilometers) apart. Use a bucket policy to enforce access to the buckets only through HTTPS. Use a bucket policy to enforce Amazon S3 SSE-C on all objects uploaded to the bucket. Configure cross- region replication between the two buckets.","C":"Create primary and secondary Amazon S3 buckets in two separate AWS Regions that are at least 500 miles (805 kilometers) apart. Use an IAM role to enforce access to the buckets only through HTTPS. Use a bucket policy to enforce Amazon S3-Managed Keys (SSE-S3) on all objects uploaded to the bucket. Configure cross-region replication between the two buckets.","D":"Create primary and secondary Amazon S3 buckets in two separate Availability Zones that are at least 500 miles (805 kilometers) apart. Use a bucket policy to enforce access to the buckets only through HTTPS. Use a bucket policy to enforce AWS KMS encryption on all objects uploaded to the bucket. Configure cross-region replication between the two buckets. Create a KMS Customer Master Key (CMK) in the primary region for encrypting objects.","B":"Create primary and secondary Amazon S3 buckets in two separate AWS Regions that are at least 500 miles (805 kilometers) apart. Use a bucket policy to enforce access to the buckets only through HTTPS. Use a bucket policy to enforce S3-Managed Keys (SSE-S3) on all objects uploaded to the bucket. Configure cross-region replication between the two buckets."},"answer_description":"","discussion":[{"content":"Selected Answer: B\nA and D are excluded because we need two regions.\nThe difference between B and C is to use \"IAM role\" or \"bucket policy\" to enforce access only through HTTPS.\nBucket policy is responsible for this type of jobs. Reference: https://repost.aws/knowledge-center/s3-bucket-policy-for-config-rule","poster":"Piccaso","comment_id":"798762","timestamp":"1675595100.0","upvote_count":"3"},{"comment_id":"778137","timestamp":"1673896140.0","poster":"Bulti","upvote_count":"1","content":"B us the right answer."},{"timestamp":"1672265880.0","upvote_count":"1","content":"B is correct","poster":"saeidp","comment_id":"760384"},{"timestamp":"1670293740.0","content":"Selected Answer: B\nI choose B","comment_id":"736481","upvote_count":"1","poster":"DonWang"},{"timestamp":"1665231180.0","content":"Selected Answer: B\nAs its in another region and has encryption and replication.","comment_id":"689279","poster":"animalrj","upvote_count":"1"},{"upvote_count":"2","timestamp":"1663723380.0","poster":"Goozian","comment_id":"674663","content":"Selected Answer: B\nB\n\nhttps://www.examtopics.com/discussions/amazon/view/2753-exam-aws-devops-engineer-professional-topic-1-question-69/"},{"upvote_count":"2","content":"Selected Answer: B\nCross \"Region\" replication -","timestamp":"1663723320.0","poster":"Goozian","comment_id":"674661"},{"content":"B for sure","upvote_count":"2","timestamp":"1663641660.0","comment_id":"673746","poster":"lmimi"}],"question_text":"A company is building a solution for storing files containing Personally Identifiable Information (PII) on AWS.\nRequirements state:\n✑ All data must be encrypted at rest and in transit.\n✑ All data must be replicated in at least two locations that are at least 500 miles (805 kilometers) apart.\nWhich solution meets these requirements?","timestamp":"2022-09-20 04:41:00","url":"https://www.examtopics.com/discussions/amazon/view/82871-exam-aws-devops-engineer-professional-topic-1-question-37/","question_images":[],"answers_community":["B (100%)"],"answer":"B","isMC":true,"topic":"1","question_id":140,"unix_timestamp":1663641660,"answer_ET":"B","answer_images":[]}],"exam":{"isImplemented":true,"lastUpdated":"11 Apr 2025","isBeta":false,"isMCOnly":false,"numberOfQuestions":208,"name":"AWS DevOps Engineer Professional","id":35,"provider":"Amazon"},"currentPage":28},"__N_SSP":true}