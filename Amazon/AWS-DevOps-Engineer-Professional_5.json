{"pageProps":{"questions":[{"id":"RidCseTUeO6xqfXpgV05","answer_ET":"A","topic":"1","exam_id":35,"answer":"A","unix_timestamp":1672203960,"question_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/93046-exam-aws-devops-engineer-professional-topic-1-question-117/","answer_description":"","answers_community":["A (100%)"],"answer_images":[],"discussion":[{"content":"Selected Answer: A\nOPSWork's Configure Lifecycle event runs on all the nodes.","timestamp":"1672203960.0","poster":"saggy4","comment_id":"759372","upvote_count":"5"},{"comment_id":"853853","content":"It's B.","poster":"easytoo","timestamp":"1680050640.0","upvote_count":"1"},{"timestamp":"1676029680.0","upvote_count":"1","comment_id":"804259","content":"Selected Answer: A\nA looks more automatic.","poster":"Piccaso"},{"upvote_count":"4","timestamp":"1674529560.0","poster":"Bulti","comment_id":"786118","content":"Selected Answer: A\nA is correct. OpsWork is the right DevOps tool. Not B because using CodeDeploy here to reconfigure the cluster when new nodes are added doesn't make sense. It's not a deployment function. It is an Infrastructure provisioning function."},{"upvote_count":"4","poster":"strike3test","timestamp":"1672273380.0","content":"question discussed at below link with A as possible answer\nhttps://www.examtopics.com/discussions/amazon/view/3418-exam-aws-devops-engineer-professional-topic-1-question-4/","comment_id":"760448"}],"question_text":"A company wants to use a grid system for proprietary enterprise in-memory data store on top of AWS. The system can run in multiple server nodes in any Linux-based distribution. The system must be able to reconfigure the entire cluster every time a node is added or removed. When adding or removing nodes, an /etc/cluster/nodes.config file must be updated listing the IP addresses of the current node member of that cluster.\n\nThe company wants to automate the task of adding new nodes to a cluster.\n\nWhat can a DevOps engineer do to meet these requirements?","choices":{"B":"Put the file nodes.config in version control. Create an AWS CodeDeploy deployment configuration and deployment group based on an Amazon EC2 tag value for the cluster nodes. When adding a new node to the cluster, update the file with all tagged instances, and make a commit in version control. Deploy the new file and restart the services.","D":"Create a user data script that lists all members of the current security group of the cluster and automatically updates the /etc/cluster/nodes.config file whenever a new instance is added to the cluster.","A":"Use AWS OpsWorks Stacks to layer the server nodes of that cluster. Create a Chief recipe that populates the content of the /etc/cluster/nodes.config file and restarts the service by using the current members of the layers. Assign that recipe to the Configure lifecycle event.","C":"Create an Amazon S3 bucket and upload a version of the /etc/cluster/nodes.config file. Create a crontab script that will poll for that S3 file and download it frequently. Use a process manager, such as Monit or systemd, to restart the cluster services when it detects that the new file was modified. When adding a node to the cluster, edit the fileâ€™s most recent members. Upload the new file to the S3 bucket."},"timestamp":"2022-12-28 06:06:00","question_id":21},{"id":"yretTEGYR8sakvpSKNKd","answer_ET":"D","isMC":true,"answers_community":["D (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/92576-exam-aws-devops-engineer-professional-topic-1-question-118/","timestamp":"2022-12-23 14:15:00","choices":{"B":"Use GlusterFS on EC2 instances for checkpoint data. To run the batch job, configure EC2 instances manually. When the job completes, shut down the instances manually.","D":"Use Amazon EFS for checkpoint data. Use EC2 Fleet to launch EC2 Spot Instances. Create a custom AMI for the cluster and use the latest AMI when creating instances.","A":"Use Amazon EFS for checkpoint data. To complete the job, use an EC2 Auto Scaling group and an On-Demand pricing model to provision EC2 instances temporarily.","C":"Use Amazon EFS for checkpoint data. Use EC2 Fleet to launch EC2 Spot Instances, and utilize user data to configure the EC2 Linux instance on startup."},"discussion":[{"comment_id":"1291413","comments":[{"upvote_count":"1","poster":"4b18f59","comment_id":"1291414","timestamp":"1727674620.0","content":"My bad it has to be D for the 30 min load up time"}],"upvote_count":"1","poster":"4b18f59","content":"D. While this could work, creating and maintaining custom AMIs adds unnecessary complexity and potential costs when user data can achieve the same result more efficiently.\nThe chosen solution (C) provides the best balance of cost-effectiveness, scalability, and ease of management:\nIt uses the cheapest compute option (Spot Instances) that meets the requirements.\nIt leverages a fully managed, scalable file system (EFS) for checkpoint data.\nIt automates instance configuration using user data, reducing management overhead.\nIt can easily scale up or down based on demand using EC2 Fleet.","timestamp":"1727674500.0"},{"content":"C is cheaper than D","timestamp":"1680051180.0","poster":"easytoo","comment_id":"853864","upvote_count":"1"},{"poster":"Piccaso","upvote_count":"2","content":"Selected Answer: D\nA is almost un-doable.\nB is using GlusterFS. AWS-native products are better.\nBetween C and D. D is better because customised AMI brings convenience.","comment_id":"804321","timestamp":"1676034000.0"},{"timestamp":"1674529800.0","comment_id":"786125","poster":"Bulti","upvote_count":"1","content":"Selected Answer: D\nD is correct."},{"upvote_count":"1","content":"D sounds reasonable","timestamp":"1672273440.0","comment_id":"760449","poster":"strike3test"},{"timestamp":"1671948480.0","upvote_count":"2","poster":"Kapello10","content":"D is the correct ans","comment_id":"755433"},{"content":"D - correct","upvote_count":"1","timestamp":"1671801300.0","comment_id":"754237","poster":"Imstack"}],"topic":"1","question_images":[],"answer":"D","question_text":"A DevOps engineer is researching the least expensive way to implement an image batch processing cluster on AWS. The application cannot run in Docker containers and must run on Amazon EC2. The batch job stores checkpoint data on an NFS and can tolerate interruptions. Configuring the cluster software from a generic EC2 Linux image takes 30 minutes.\n\nWhat is the MOST cost-effective solution?","exam_id":35,"unix_timestamp":1671801300,"answer_images":[],"question_id":22,"answer_description":""},{"id":"7VlChbNPabpqLejCiMdg","answers_community":["C (63%)","A (38%)"],"question_id":23,"isMC":true,"answer_images":[],"unix_timestamp":1671758820,"url":"https://www.examtopics.com/discussions/amazon/view/92516-exam-aws-devops-engineer-professional-topic-1-question-119/","question_images":[],"answer":"C","discussion":[{"comment_id":"759380","upvote_count":"8","poster":"saggy4","timestamp":"1672204680.0","content":"Selected Answer: C\nC -- We need to use CloudWatch metric filter to generate count and Cloudwatch alarm for checking number of counts in a minute.\nA -- is wrong as you can only use metric filters to get count per occurrence in logs and not count in a particular time frame."},{"timestamp":"1683362700.0","comments":[{"content":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html\ncheck this link how to create metric filters to count the error messages every minute. \nC is correct answer","timestamp":"1714910820.0","comment_id":"1206877","poster":"vn_thanhtung","upvote_count":"1"}],"content":"Selected Answer: A\nA is more promising in this scenario.","comment_id":"890605","poster":"ParagSanyashiv","upvote_count":"1"},{"poster":"merki","content":"Selected Answer: A\nThe solution that meets the requirements with MINIMAL management overhead is option A. Here's why:\n\nOption A suggests installing the Amazon CloudWatch agent on all instances to push the application logs to CloudWatch Logs. Then, using metric filters to count the error messages every minute, and initiating a CloudWatch alarm if the count exceeds 10 errors. This approach is the most straightforward and requires the least amount of custom coding or scripts. It uses Amazon CloudWatch and its built-in capabilities for monitoring and alerting on logs.","upvote_count":"1","comment_id":"886237","timestamp":"1682945100.0"},{"poster":"easytoo","content":"It's C, and here's why...\n\nCloudWatch agent - easy to install and configure, and requires minimal management overhead.\nMetric filter to count the specific error messages - a lightweight and efficient way to monitor logs and generate metrics.\nGenerates a custom CloudWatch metric - can be used to track the specific error messages over time and trigger alarms if the count exceeds a certain threshold.\nAlso meets the requirement of issuing an alert if there are more than 10 errors within a 1-minute window.","upvote_count":"1","timestamp":"1680051540.0","comment_id":"853873"},{"comment_id":"834384","poster":"asfsdfsdf","upvote_count":"2","timestamp":"1678393740.0","content":"For me C is the correct one.\nWhy not A? how can a metric filter trigger a cloudwatch alarm? you must use the custom metric created by it in order to trigger the alarm - this cannot be done directly based on a filter.","comments":[{"timestamp":"1680200460.0","comment_id":"856070","content":"The metric filter is there to count the number of error messages per minute, and THEN we set up CloudWatch alarm with the metric. Maybe I'm wrong, but your rationale might be actually invalid.","poster":"AkaAka4","upvote_count":"1"}]},{"upvote_count":"3","poster":"SHoKMaSTeR","content":"Selected Answer: A\nA vs C --> A\n\"if there are more than 10 errors within a 1-minute\" \nC is checking 10 per minute","timestamp":"1677597660.0","comment_id":"824965","comments":[{"content":"Ohhhh you're right...!","comment_id":"856078","upvote_count":"1","timestamp":"1680200580.0","poster":"AkaAka4"}]},{"comment_id":"813314","comments":[{"upvote_count":"1","comment_id":"813319","content":"Most definitely not C, how can you generate a custom metric from a metric filter?!","timestamp":"1676739960.0","poster":"BelloMio"}],"upvote_count":"2","content":"Definitely A.\nYou can use metric filter to count the error, create an alarm for it within a 1 minute period that can check if threshold for the metric (count in this case) is above 10.\nJust go and test it","poster":"BelloMio","timestamp":"1676739780.0"},{"poster":"Piccaso","content":"Selected Answer: C\nD is eliminated in the first round because of \"cron job\".\nB is pushing only \"access logs\"\nA looks all errors, not a specific type of errors.","timestamp":"1676037240.0","upvote_count":"1","comment_id":"804375"},{"comment_id":"801910","poster":"DerekKey","content":"Selected Answer: C\nB & D - wrong\nA - wrong - metric filter is only providing custom metrics, it is not doing any calculations (\"Use metric filters to count the error messages every minute\")\nC - correct - metrics filter provides a new custom metric used by CW alarm to trigger an action","upvote_count":"1","timestamp":"1675854360.0"},{"content":"A is the right answer. You can generate a custom metric from a metric filter. You can generate only a standard metric from the CloudWatch metric filter. Custom metric can be generated only via CLI.","poster":"Bulti","upvote_count":"2","comment_id":"786141","timestamp":"1674531000.0"},{"comment_id":"781191","timestamp":"1674136680.0","content":"Selected Answer: C\nC is answer, you could test this solution from aws console. When you create metrics filter it will create new metric for you","poster":"Dimidrol","upvote_count":"2"},{"timestamp":"1674043320.0","poster":"bartekb3d","content":"Selected Answer: C\ncorrect answer C","comment_id":"779965","upvote_count":"1"},{"poster":"bartekb3d","comment_id":"779957","content":"Selected Answer: A\nthere is no need to create custom metric","timestamp":"1674042900.0","upvote_count":"1"},{"content":"Selected Answer: A\nAns:A\nC is wrong because to create a \"custom CloudWatch metric\", you need to write your own script or use an application monitoring script. See AWS docs below:\nhttps://docs.aws.amazon.com/managedservices/latest/userguide/custom-cloudwatch-events.html","upvote_count":"1","timestamp":"1672820700.0","poster":"obaf1","comment_id":"765380"},{"upvote_count":"1","content":"Selected Answer: C\nGoing with C. C states use of custom metric filter.","timestamp":"1672663620.0","poster":"ericzaj","comment_id":"763723"},{"poster":"obaf1","comment_id":"763370","timestamp":"1672613340.0","content":"Selected Answer: A\nAns: A\nFor those choosing C, please, read AWS documentation about \"custom CloudWatch metrics\". You don't use metric filter to generate \"custom CloudWatch metric\", instead you use the AWS CLI or an API (see excerpt below):\nFull link: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html \n\"Publishing custom metrics\n-------------------------------------------\nYou can publish your own metrics to CloudWatch using the AWS CLI or an API. You can view statistical graphs of your published metrics with the AWS Management Console.\"","upvote_count":"1"},{"comment_id":"761242","timestamp":"1672330440.0","upvote_count":"1","content":"Ans is A\nUser Saggy4 is wrong in stating that \"A is wrong as you can only use metric filters to get count per occurrence in logs and not count in a particular time frame.\"\nScroll to 2:08 of the tutorial below:\nhttps://www.youtube.com/watch?v=I_VjSvSSoF4&ab_channel=SREMasterClass\n(You can specify the time period within which the error count needs to occur by creating an alarm on top of your metric. Please, see the video above eg 10 errors within a minute)","poster":"obaf1"},{"poster":"strike3test","timestamp":"1672273500.0","comment_id":"760450","upvote_count":"1","content":"This is a confusing question and I am more inclined towards C"},{"content":"Ans: A;\n\"C\" is wrong because the question is asking for \"MINIMAL\" effort. You don't need to use \" a metric filter to generate a custom CloudWatch metric that records the number of failures\" when you have the option in \"A\" to simply count directly using metrics filter.\nHere's a link on using metric filter to count:\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CountOccurrencesExample.html","upvote_count":"1","timestamp":"1672224300.0","poster":"obaf1","comment_id":"759728"},{"content":"Ans: A; \nB is wrong because the question is asking for \"MINIMAL\" effort. You don't need to use \" a metric filter to generate a custom CloudWatch metric that records the number of failures\" when you have the option in \"A\" to simply count directly using metrics filter.\nHere's a link on using metric filter to count:\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CountOccurrencesExample.html","comment_id":"759695","poster":"obaf1","timestamp":"1672222200.0","upvote_count":"1"},{"content":"Selected Answer: C\nC quite obvious","poster":"tartealacreme","upvote_count":"1","timestamp":"1672070580.0","comment_id":"757632"},{"content":"C is better","comment_id":"754240","timestamp":"1671801480.0","poster":"Imstack","upvote_count":"2"},{"content":"Selected Answer: A\nAns: A\nIt's better to use the AWS CloudWatch Metric Filter than a custom script.","poster":"obaf1","comment_id":"753791","upvote_count":"1","timestamp":"1671758820.0"}],"topic":"1","choices":{"A":"Install the Amazon CloudWatch agent on all instances to push the application logs to CloudWatch Logs. Use metric filters to count the error messages every minute, and initiate a CloudWatch alarm if the count exceeds 10 errors.","C":"Install the Amazon CloudWatch agent on all instances to push the application logs to CloudWatch Logs. Use a metric filter to generate a custom CloudWatch metric that records the number of failures and initiates a CloudWatch alarm if the custom metric reaches 10 errors in a 1-minute period.","D":"Deploy a custom script on all instances to check application logs regularly in a cron job. Count the number of error messages every minute, and push a data point to a custom CloudWatch metric. Initiate a CloudWatch alarm if the custom metric reaches 10 errors in a 1-minute period.","B":"Install the Amazon CloudWatch agent on all instances to push the access logs to CloudWatch Logs. Create an Amazon EventBridge (Amazon CloudWatch Events) rule to count the error messages every minute, and initiate a CloudWatch alarm if the count exceeds 10 errors."},"question_text":"A mobile application running on eight Amazon EC2 instances is relying on a third-party API endpoint. The third-party service has a high failure rate because of limited capacity which is expected to be resolved in a few weeks.\n\nIn the meantime, the mobile application developers have added a retry mechanism and are logging failed API requests. A DevOps engineer must automate the monitoring of application logs and count the specific error messages, if there are more than 10 errors within a 1-minute window the system must issue an alert.\n\nHow can the requirements be met with MINIMAL management overhead?","timestamp":"2022-12-23 02:27:00","answer_ET":"C","exam_id":35,"answer_description":""},{"id":"Ee9crDD9WwvimlFGN11c","timestamp":"2021-05-03 00:42:00","isMC":true,"topic":"1","unix_timestamp":1619995320,"question_images":[],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/51619-exam-aws-devops-engineer-professional-topic-1-question-12/","answer_ET":"ADE","question_text":"A DevOps Engineer needs to back up sensitive Amazon S3 objects that are stored within an S3 bucket with a private bucket policy using the S3 cross-region replication functionality. The objects need to be copied to a target bucket in a different AWS Region and account.\nWhich actions should be performed to enable this replication? (Choose three.)","discussion":[{"content":"ADE - The replication rule is created in the source bucket","poster":"JohnnieWalker","comment_id":"348098","upvote_count":"12","timestamp":"1632355440.0"},{"comment_id":"1319715","upvote_count":"1","content":"Selected Answer: ADE\nADE is correct","timestamp":"1732885140.0","poster":"Simba84"},{"content":"ACE. The alternative D - \"Add statements to the target bucket policy allowing the replication IAM role to replicate objects\" might be chosen frequently due to a common misconception. \n When setting up cross-region replication in AWS S3, some people might assume that the target bucket (where the objects are being replicated to) also needs to explicitly grant permissions to the replication IAM ___role___. However, this is not the case in AWS S3 cross-region replication setup.","timestamp":"1698131880.0","poster":"Dgix","upvote_count":"1","comment_id":"1052628"},{"comment_id":"894539","upvote_count":"1","content":"Selected Answer: ADE\nS3 cross-Region replication (CRR) automatically replicates data between buckets across different AWS Regions. To enable CRR, you need to add a replication configuration to your source bucket that specifies the destination bucket, the IAM role, and the encryption type (optional). You also need to grant permissions to the IAM role to perform replication actions on both the source and destination buckets. Additionally, you can choose the destination storage class and enable additional replication options such as S3 Replication Time Control (S3 RTC) or S3 Batch Replication.\nhttps://medium.com/cloud-techies/s3-same-region-replication-srr-and-cross-region-replication-crr-34d446806bab\nhttps://aws.amazon.com/getting-started/hands-on/replicate-data-using-amazon-s3-replication/\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html","poster":"tschenhau","timestamp":"1683775560.0"},{"timestamp":"1683673800.0","comment_id":"893485","poster":"nicat","content":"Selected Answer: ACE\nACE\nCreate and attach the S3 bucket policy in the source account\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/copy-data-from-an-s3-bucket-to-another-account-and-region-by-using-the-aws-cli.html","upvote_count":"1"},{"content":"Selected Answer: ADE\nADE seems to be correct answer","comment_id":"885940","upvote_count":"1","poster":"ParagSanyashiv","timestamp":"1682924700.0"},{"comment_id":"881831","upvote_count":"1","poster":"mgonblan","content":"D, B, E: Refference: https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/copy-data-from-an-s3-bucket-to-another-account-and-region-by-using-the-aws-cli.html","timestamp":"1682526780.0"},{"timestamp":"1679030040.0","upvote_count":"1","poster":"Tika01","content":"ABCE are correct asnwers","comment_id":"841606"},{"upvote_count":"2","timestamp":"1677599580.0","poster":"ccienetrider","content":"ADE - Correct Answer , Tested and working","comment_id":"824989"},{"timestamp":"1677448140.0","comment_id":"822996","poster":"m00lecule","content":"Selected Answer: ADE\nADE - The replication rule is created in the source bucket","upvote_count":"1"},{"content":"The answer is ADE","comment_id":"800267","timestamp":"1675716840.0","poster":"Sabreen_Salama","upvote_count":"1"},{"timestamp":"1675430940.0","poster":"Piccaso","content":"Selected Answer: BCE\n1. B from (A, B) : the new IAM role should belong to the target account\n2. C from (C, D): the policy is used to allow the role to do something to the object what the policy is attached to.\n3. E from (E, F): the rule should be created to the bucket whose objects will be replicated","comment_id":"797070","comments":[{"poster":"Piccaso","content":"My voted answers are wrong. Sorry guys. \nThe permission to replicate should belong to the role on source account --> A from (A, B)\nThe target bucket policy should have statement to grant permission for replicating to the role in the source account. AWS official article: https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-walkthrough-2.html\nThe Replication Rule should be created on the target bucket --> F from(E, F). Please refer to this article which involves screenshots of hands-on experiment. https://aws.plainenglish.io/set-up-an-s3-bucket-with-cross-region-replication-97d43084ff36","timestamp":"1675434000.0","comment_id":"797092","upvote_count":"3"}],"upvote_count":"2"},{"content":"ADE is the correct answer. To enable cross account replication we need to \n1. create a replication rule in the source bucket\n2. Create an IAM role in the source account to perform replication\n3. Create a resource policy on the destination bucket that grant permission to the IAM role in the source account to replicate objects into the destination bucket.","timestamp":"1672550580.0","poster":"Bulti","upvote_count":"4","comment_id":"763064"},{"upvote_count":"1","timestamp":"1662140940.0","poster":"ohcn","comment_id":"657675","content":"ADE - https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-walkthrough-2.html"},{"comment_id":"655648","upvote_count":"3","poster":"colinquek","timestamp":"1661994840.0","content":"BDE - B becos the source acct should assume the target acct's IAM role to copy things into it."},{"poster":"SHAAHIBHUSHANAWS","comment_id":"617431","upvote_count":"3","timestamp":"1655420160.0","content":"ADE\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-walkthrough-2.html"},{"content":"Selected Answer: ADE\nADE should be the answer. F is incorrect.","poster":"blueorca","upvote_count":"2","comment_id":"546466","timestamp":"1644758760.0"},{"poster":"herohiro","content":"Selected Answer: ADE\nI would vote for ADE","upvote_count":"2","timestamp":"1642294560.0","comment_id":"524537"},{"comment_id":"377938","content":"https://aws.amazon.com/premiumsupport/knowledge-center/copy-s3-objects-account/\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-change-owner.html","poster":"peddyua","upvote_count":"1","timestamp":"1636152720.0"},{"timestamp":"1635436440.0","upvote_count":"1","content":"BCF\nAttach bucket policy in source account\nAttach am policy to user/role in B account\nUse an user/role in account B to perform cross-account copy","comments":[{"upvote_count":"4","content":"was it intentional to choose all incorrect options?","comment_id":"711815","poster":"XAvenger","timestamp":"1667658600.0"}],"poster":"peddyua","comment_id":"377937"},{"poster":"idforadf","upvote_count":"3","content":"Yes, looks like ADE\n\nhttps://www.stratoscale.com/blog/storage/replicate-s3-buckets-across-regions/","timestamp":"1635065520.0","comment_id":"357132"},{"timestamp":"1632584160.0","comment_id":"352982","content":"I would go with ADE","poster":"lgu","upvote_count":"2"}],"answer":"ADE","answer_images":[],"answers_community":["ADE (73%)","BCE (18%)","9%"],"choices":{"A":"Create a replication IAM role in the source account.","E":"Create a replication rule in the source bucket to enable the replication.","D":"Add statements to the target bucket policy allowing the replication IAM role to replicate objects.","C":"Add statements to the source bucket policy allowing the replication IAM role to replicate objects.","B":"Create a replication IAM role in the target account.","F":"Create a replication rule in the target bucket to enable the replication."},"question_id":24,"exam_id":35},{"id":"qDpaz3GGSUKpqUrW21oG","isMC":true,"answer":"B","question_images":[],"choices":{"A":"Create a new AWS account in AWS Organizations. Create a VPC in this account and use AWS Resource Access Manager to share the private subnets of this VPC with the organization. Instruct the service teams to launch a new Network Load Balancer (NLB) and EC2 instances that use the shared private subnets. Use the NLB DNS names for communication between microservices.","D":"Create a new AWS account in AWS Organizations. Create a transit gateway in this account. and use AWS Resource Access Manager to share the transit gateway with the organization. In each of the microservice VPCs, create a transit gateway attachment to the shared transit gateway. Update the route tables of each VPC to use the transit gateway. Create a Network Load Balancer (NLB) in each of the microservice VPCs. Use the NLB DNS names for communication between microservices.","B":"Create a Network Load Balancer (NLB) in each of the microservice VPCs. Use AWS PrivateLink to create VPC endpoints in each AWS account for the NLBs. Create subscriptions to each VPC endpoint in each of the other AWS accounts. Use the VPC endpoint DNS names for communication between microservices.","C":"Create a Network Load Balancer (NLB) in each of the microservice VPCs. Create VPC peering connections between each of the microservice VPCs. Update the route tables for each VPC to use the peering links. Use the NLB DNS names for communication between microservices."},"answer_description":"","unix_timestamp":1671801720,"timestamp":"2022-12-23 14:22:00","exam_id":35,"question_id":25,"answer_ET":"B","answers_community":["B (80%)","D (20%)"],"discussion":[{"poster":"Bulti","upvote_count":"4","content":"Selected Answer: B\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/connecting-networks-with-overlapping-ip-ranges/ Private link is the best option because Transit Gateway doesn't support overlapping CIDR ranges.","timestamp":"1674532320.0","comment_id":"786148"},{"comment_id":"773937","poster":"saeidp","upvote_count":"3","content":"Selected Answer: B\nI'll go with B. TGW cannot be used for vpc with overlapping ips. The same for vpc peering","timestamp":"1673563140.0"},{"poster":"USalo","content":"Selected Answer: B\nB. the link https://aws.amazon.com/blogs/networking-and-content-delivery/connecting-networks-with-overlapping-ip-ranges/ describes NAT + Private Links and Transit Gateways. In the article it is mentioned that TGW can be used when CIDR ranges don't overlap. So the possible solution is \"B\"","upvote_count":"1","timestamp":"1673005920.0","comment_id":"767588","comments":[{"comments":[{"comment_id":"1206865","poster":"vn_thanhtung","timestamp":"1714909500.0","upvote_count":"1","content":"no you can not use TGW with overlapping CIDRS. The link say subnets not overlapping"}],"timestamp":"1675855860.0","content":"You can use TG with overlapping CIDRS but you also have to implement private nat. Read carefully.","upvote_count":"1","poster":"DerekKey","comment_id":"801936"}]},{"poster":"Ace987","comment_id":"765372","upvote_count":"2","timestamp":"1672820040.0","content":"Selected Answer: D\nTransit Gateway fits better ; Connect Amazon VPCs, AWS accounts, and on-premises networks to a single gateway","comments":[{"comment_id":"767582","poster":"USalo","content":"If I am not mistaken Transit Gateway cannot have VPCs with overlapping IP addresses. So \"D\" is incorrect.\n\"B\" will definitely work","upvote_count":"4","timestamp":"1673005740.0"}]},{"timestamp":"1671801720.0","poster":"Imstack","comment_id":"754244","content":"B - private link fits well in this situation","upvote_count":"4"}],"question_text":"A company has 20 service teams. Each service team is responsible for its own microservice. Each service team uses a separate AWS account for its microservice and a VPC with the 192.168.0.0/22 CIDR block. The company manages the AWS accounts with AWS Organizations.\n\nEach service team hosts its microservice on multiple Amazon EC2 instances behind an Application Load Balancer. The microservices communicate with each other across the public Internet. The company's security team has issued a new guideline that all communication between microservices must use HTTPS over private network connections and cannot traverse the public Internet.\n\nA DevOps engineer must implement a solution that fulfills these obligations and minimizes the number of changes for each service team.\n\nWhich solution will meet these requirements?","url":"https://www.examtopics.com/discussions/amazon/view/92577-exam-aws-devops-engineer-professional-topic-1-question-120/","topic":"1","answer_images":[]}],"exam":{"id":35,"lastUpdated":"11 Apr 2025","provider":"Amazon","isImplemented":true,"name":"AWS DevOps Engineer Professional","isBeta":false,"numberOfQuestions":208,"isMCOnly":false},"currentPage":5},"__N_SSP":true}