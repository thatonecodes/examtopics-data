{"pageProps":{"questions":[{"id":"HTZk68MN0G7pLx9gTmWv","answer":"A","question_images":[],"question_id":21,"answer_images":[],"answer_ET":"A","answer_description":"","unix_timestamp":1612546320,"topic":"1","isMC":true,"answers_community":["A (67%)","C (33%)"],"choices":{"A":"Create an AWS Glue job to connect to the PostgreSQL DB instance. Ingest tables without sensitive data through an AWS Site-to-Site VPN connection directly into Amazon S3.","D":"Use PostgreSQL logical replication to replicate all data to PostgreSQL in Amazon EC2 through AWS Direct Connect with a VPN connection. Use AWS Glue to move data from Amazon EC2 to Amazon S3.","B":"Create an AWS Glue job to connect to the PostgreSQL DB instance. Ingest all data through an AWS Site-to-Site VPN connection into Amazon S3 while removing sensitive data using a PySpark job.","C":"Use AWS Database Migration Service (AWS DMS) with table mapping to select PostgreSQL tables with no sensitive data through an SSL connection. Replicate data directly into Amazon S3."},"discussion":[{"poster":"cnethers","upvote_count":"27","content":"ASK : Extract Data over IPsec\nSo we need an ETL + Site to site VPN\nGLUE is an ETL service but can it connect to PostgreSQL? yes\nhttps://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-connect.html#aws-glue-programming-etl-connect-jdbc\nHow to connect Glue to an on-site DB\nhttps://aws.amazon.com/blogs/big-data/how-to-access-and-analyze-on-premises-data-stores-using-aws-glue/\n\nMy Answer would be A \nAnser C only makes a 443 (SSL) connection so does not meet the IPsec requirement","comment_id":"286597","timestamp":"1633446900.0"},{"poster":"ksrivastavaSumit","upvote_count":"8","content":"A? IPSec needs to be covered as well","comment_id":"284641","timestamp":"1633053600.0","comments":[{"content":"Yes. https://aws.amazon.com/blogs/big-data/how-to-access-and-analyze-on-premises-data-stores-using-aws-glue/. 'A' is the correct answer.","comment_id":"355312","timestamp":"1633664400.0","upvote_count":"3","poster":"StelSen"}]},{"upvote_count":"1","poster":"MultiCloudIronMan","timestamp":"1729247640.0","comment_id":"1299620","content":"Selected Answer: A\nIt's 'A' because IPSec is required."},{"poster":"sachin80","upvote_count":"1","content":"A: https://medium.com/awsblackbelt/loading-on-prem-postgres-data-into-amazon-s3-with-server-side-filtering-c13bcee8b769","comment_id":"1205928","timestamp":"1714711560.0"},{"content":"Selected Answer: A\nB - Doesnt take care of only nonsensitive data being allowed to leave the on-premise.\nC - Uses SSL and not IPSec.\nD - like B transfers all data.\nHence the correct answer is A","upvote_count":"1","timestamp":"1711506720.0","poster":"VR10","comment_id":"1183808"},{"upvote_count":"1","comment_id":"1154238","content":"I will go with B\nSite to Site VPN -> IPsec requirement\nAWS Glue -> connect and catalog PostgressSQL \nPyspark -> remove sensitive information. AWS glue supports pyspark","timestamp":"1708372500.0","poster":"AIWave"},{"poster":"kyuhuck","comments":[{"content":"IPsec and SSL are two different things. Using SSL does not necessarily mean option C has IPsec implemented, which is required.","upvote_count":"1","timestamp":"1734774720.0","comment_id":"1329910","poster":"LeoD"}],"content":"Selected Answer: C\nThe best option is to use AWS Database Migration Service (AWS DMS) with table mapping to select PostgreSQL tables with no sensitive data through an SSL connection. Replicate data directly intoAmazon S3. This option meets the following requirements:It ensures that only nonsensitive data is transferred to the cloud by using table mapping to filter outthe tables that contain sensitive data1.It uses IPsec to secure the data transfer by enabling SSL encryption for the AWS DMS endpoint2.It uploads the data to Amazon S3 each day for model retraining by using the ongoing replicationfeature of AWS DMS3","upvote_count":"3","timestamp":"1707670200.0","comment_id":"1147514"},{"upvote_count":"1","timestamp":"1695976200.0","content":"but glue can not filter out the data during the ingestion and hence option A wouldn't be the right one! I would go for B","poster":"Rejju","comments":[{"comment_id":"1329913","upvote_count":"1","timestamp":"1734774900.0","poster":"LeoD","content":"I think A is saying only to ingest tables that don't contain sensitive data, meaning while configuring Glue, the specialist will only select the tables that don't contain sensitive data for ingestion."}],"comment_id":"1020634"},{"comments":[{"poster":"Hybrid_Cloud_boy","comment_id":"1099909","content":"I think the issue with this answer would be that the data actually leaves the DC and enters the glue service before sensitive data is redacted. - Which makes me lean A","upvote_count":"2","timestamp":"1702920360.0"}],"upvote_count":"3","poster":"jopaca1216","comment_id":"1008695","content":"B \n\nBoth A and C are not correct... due that the question is not talking about tables with no sensitive data... and that DMS tipically act on the data on AWS side, the right answer is B \n\nAWS Glue connects to the PostgreSQL database, allowing the removal of sensitive data using a PySpark job BEFORE securely ingesting the data into Amazon S3, thus aligning with the requirements.","timestamp":"1694811240.0"},{"content":"Selected Answer: C\nOption c","poster":"Mickey321","upvote_count":"1","timestamp":"1693058940.0","comment_id":"990840"},{"timestamp":"1688506560.0","poster":"ADVIT","content":"A:\nhttps://aws.amazon.com/blogs/big-data/doing-data-preparation-using-on-premises-postgresql-databases-with-aws-glue-databrew/","upvote_count":"2","comment_id":"943170"},{"content":"Selected Answer: A\nA. Create an AWS Glue job to connect to the PostgreSQL DB instance. Ingest tables without sensitive data through an AWS Site-to-Site VPN connection directly into Amazon S3.\n\nThis solution meets the requirements of data localization regulations and secure data transfer. By creating an AWS Glue job to connect to the PostgreSQL DB instance, the machine learning specialist can extract tables without sensitive data. By using a Site-to-Site VPN connection, the data can be securely transferred from the on-premises data center to Amazon S3, where it can be used for model retraining. This solution ensures that any sensitive data remains in the on-premises data center, and that only non-sensitive data is uploaded to the cloud.","timestamp":"1676319240.0","poster":"AjoseO","upvote_count":"2","comment_id":"807778"},{"timestamp":"1658991540.0","poster":"matteocal","content":"Selected Answer: A\nIPSec means VPN","comment_id":"638529","upvote_count":"4"},{"content":"Answer is A. IPsec is not the same as SSL. Site to site VPN is for IPsec: https://aws.amazon.com/vpn/site-to-site-vpn/ \nAlso Glue can directly connect to Postgres and upload to S3: https://aws.amazon.com/blogs/big-data/how-to-access-and-analyze-on-premises-data-stores-using-aws-glue/","poster":"geekgirl007","comment_id":"520366","upvote_count":"3","timestamp":"1641748980.0"},{"content":"A is the answer","poster":"Deepsachin","timestamp":"1636551000.0","upvote_count":"1","comment_id":"475449"},{"timestamp":"1634833680.0","content":"Between A and C, I pick A because IPSec requires VPN; otherwise DMS is a better option","comment_id":"434095","poster":"Dr_Kiko","upvote_count":"3"},{"content":"Between A and C, I pick A because IPSec requires VPN; otherwise DMS is a better option","timestamp":"1634707140.0","upvote_count":"1","poster":"Dr_Kiko","comment_id":"434093"},{"timestamp":"1634658960.0","content":"100% A, DMS is not supporting S3, but glue is. Also, Site to Site is supporting IPSec.","poster":"AMEJack","upvote_count":"1","comment_id":"431721"},{"timestamp":"1633931040.0","poster":"[Removed]","content":"This is a very tricky question since DMS over VPN started to be supported Nov 2020\nhttps://aws.amazon.com/about-aws/whats-new/2020/11/now-privately-connect-to-aws-database-migration-service-from-amazon-virtual-private-cloud/\nSo both A&C can work. If this question is older then A is the right answer, there have been a bit over 6 months since November 2020 so I would expect this question to become a bit clearer if it stays on the exam in its current form","upvote_count":"2","comment_id":"392331"},{"comment_id":"355313","upvote_count":"3","poster":"StelSen","content":"I was bit confused between C & A. But chose A. https://aws.amazon.com/blogs/big-data/how-to-access-and-analyze-on-premises-data-stores-using-aws-glue/\nBecause one of the requirement is to go through via IPSec. So, VPN required. DMS is not IPSec based although it's secured. AWS Glue supports on-premise","timestamp":"1633817520.0"},{"timestamp":"1632371220.0","poster":"[Removed]","comment_id":"284312","upvote_count":"2","content":"C it is, explanation is good."}],"url":"https://www.examtopics.com/discussions/amazon/view/44073-exam-aws-certified-machine-learning-specialty-topic-1/","question_text":"An e commerce company wants to launch a new cloud-based product recommendation feature for its web application. Due to data localization regulations, any sensitive data must not leave its on-premises data center, and the product recommendation model must be trained and tested using nonsensitive data only. Data transfer to the cloud must use IPsec. The web application is hosted on premises with a PostgreSQL database that contains all the data. The company wants the data to be uploaded securely to Amazon S3 each day for model retraining.\nHow should a machine learning specialist meet these requirements?","timestamp":"2021-02-05 18:32:00","exam_id":26},{"id":"YoM1s867T4MtPjS5uEa2","timestamp":"2021-02-25 20:01:00","topic":"1","answers_community":["AD (88%)","13%"],"url":"https://www.examtopics.com/discussions/amazon/view/45611-exam-aws-certified-machine-learning-specialty-topic-1/","answer_ET":"AD","question_images":[],"answer":"AD","discussion":[{"upvote_count":"17","content":"I would choose A and D, however both of them is not possible at the same time. The question is ambiguous, it could mean which two options, but no necessarily both. \nA - If you want Amazon Forecast to evaluate each algorithm and choose the one that minimizes the objective function, set PerformAutoML to true.\nD - The following algorithms support HPO: - > DeepAR+.","comment_id":"306768","timestamp":"1666467060.0","poster":"scuzzy2010","comments":[{"content":"If custom forecast types are specified, Forecast evaluates metrics at those specified forecast types, and takes the averages of those metrics to determine the optimal outcomes during HPO and AutoML.\n\nFor both AutoML and HPO, Forecast chooses the option that minimizes the average losses over the forecast types. During HPO, Forecast uses the first backtest window to find the optimal hyperparameter values. During AutoML, Forecast uses the averages across all backtest windows and the optimal hyperparameters values from HPO to find the optimal algorithm. https://docs.aws.amazon.com/forecast/latest/dg/metrics.html","comment_id":"309104","poster":"Oscaaaar","upvote_count":"3","timestamp":"1666481940.0"}]},{"content":"It is A and D, there are no weekly data, they have only monthly data and can not switch horizon to 4","upvote_count":"7","poster":"Vita_Rasta84444","comment_id":"325063","timestamp":"1666693380.0"},{"comment_id":"1010402","content":"Selected Answer: AC\nA. YES - DeepAR+ most likely to be chosen, but worth a try\nB. NO - increasing the forecast horizon is not likely to improve the 3 months we want\nC. NO - we want monthly, nto weekly\nD. YES\nE. NO - there are no missing values","upvote_count":"1","poster":"loict","timestamp":"1726649280.0"},{"poster":"Mickey321","comment_id":"990854","upvote_count":"2","content":"Selected Answer: AD\nThe changes to the CreatePredictor API call that could improve the MAPE are option A and option D. By setting PerformAutoML to true, you can enable Amazon Forecast to automatically explore different algorithms and choose the best one for your data and business problem. By setting PerformHPO to true, you can enable Amazon Forecast to perform hyperparameter optimization (HPO) and tune the algorithm parameters to improve the accuracy of the predictor. These options can help you find the optimal configuration for your forecast model without manually specifying the algorithm or the hyperparameters.","timestamp":"1724681640.0"},{"timestamp":"1707855180.0","content":"Selected Answer: AD\nA. Set PerformAutoML to true.\nD. Set PerformHPO to true.\n\nSetting PerformAutoML to true will enable Amazon Forecast to automatically select the best algorithm and hyperparameters for your data and problem. This can help improve the MAPE by finding the optimal combination of algorithm and hyperparameters that minimize prediction error.\n\nSetting PerformHPO to true will enable Amazon Forecast to perform a hyperparameter optimization search to find the best combination of hyperparameters that result in the best prediction performance. This can help improve the MAPE by finding the optimal combination of hyperparameters that minimize prediction error.","poster":"AjoseO","upvote_count":"4","comment_id":"807775"},{"timestamp":"1703620980.0","content":"Selected Answer: AD\nA. Looking for better algorithms performance\nD. Hyperparameters optimization","poster":"yemauricio","upvote_count":"1","comment_id":"757802"},{"content":"12-sep exam","timestamp":"1694543160.0","poster":"Shailendraa","upvote_count":"3","comment_id":"667339"},{"poster":"vanluigi","timestamp":"1684153980.0","upvote_count":"5","content":"Why are not B and C? The question asks about modifications that increase MAPE (thats bad):\nB - If FH is larger, error will increase\nC - Data is based on months, change that will make erros on forecasting values\nE - There is no data gap so is useless\nA - Selec best between all should DECREASE MAPE\nD - Tunning hyperparms will DECREASE MAPE","comment_id":"602079"},{"content":"A&D...>By default, Amazon Forecast uses the 0.1 (P10), 0.5 (P50), and 0.9 (P90) quantiles for hyperparameter tuning during hyperparameter optimization (HPO) and for model selection during AutoML. If you specify custom forecast types when creating a predictor, Forecast uses those forecast types during HPO and AutoML.\n\nIf custom forecast types are specified, Forecast evaluates metrics at those specified forecast types, and takes the averages of those metrics to determine the optimal outcomes during HPO and AutoML.\n\nFor both AutoML and HPO, Forecast chooses the option that minimizes the average losses over the forecast types. During HPO, Forecast uses the first backtest window to find the optimal hyperparameter values. During AutoML, Forecast uses the averages across all backtest windows and the optimal hyperparameters values from HPO to find the optimal algorithm.","comment_id":"353794","poster":"mona_mansour","upvote_count":"4","timestamp":"1666762860.0"},{"poster":"SophieSu","upvote_count":"2","comments":[{"timestamp":"1665744660.0","poster":"seanLu","content":"But for option C, according to the Developer Guide, The forecast frequency must be greater than or equal to the TARGET_TIME_SERIES dataset frequency. and the training data is monthly data, so ForecastFrequency can not be less than Monthly.","upvote_count":"6","comment_id":"302128"}],"timestamp":"1665397680.0","comment_id":"299328","content":"C. ForecastFrequency\nM- MONTHLY\nW- WEEKLY\n\nD. PerformHPO\nWhether to perform hyperparameter optimization (HPO). HPO finds optimal hyperparameter values for your training data. The process of performing HPO is known as running a hyperparameter tuning job.\nThe default value is false. In this case, Amazon Forecast uses default hyperparameter values from the chosen algorithm.\n\nE. FeaturizationMethodName\nThe name of the method. The \"filling\" method is the only supported method."},{"poster":"SophieSu","comment_id":"299327","timestamp":"1664603280.0","content":"ABE can be excluded. CD is my answer.\n\nA. PerformAutoML\n\nIf you want Amazon Forecast to evaluate each algorithm and choose the one that minimizes the objective function, set PerformAutoML to true. The objective function is defined as the mean of the weighted losses over the forecast types. By default, these are the p10, p50, and p90 quantile losses.\nWhen AutoML is enabled, the following properties are disallowed:\nAlgorithmArn\nHPOConfig\nPerformHPO\nTrainingParameters\n\nB. ForecastHorizon\nSpecifies the number of time-steps that the model is trained to predict. The forecast horizon is also called the prediction length.\n\nFor example, if you configure a dataset for daily data collection (using the DataFrequency parameter of the CreateDataset operation) and set the forecast horizon to 10, the model returns predictions for 10 days.\n\nThe maximum forecast horizon is the lesser of 500 time-steps or 1/3 of the TARGET_TIME_SERIES dataset length.","upvote_count":"3"}],"question_id":22,"exam_id":26,"choices":{"B":"Set ForecastHorizon to 4.","D":"Set PerformHPO to true.","C":"Set ForecastFrequency to W for weekly.","A":"Set PerformAutoML to true.","E":"Set FeaturizationMethodName to filling."},"isMC":true,"answer_images":[],"unix_timestamp":1614279660,"answer_description":"","question_text":"A logistics company needs a forecast model to predict next month's inventory requirements for a single item in 10 warehouses. A machine learning specialist uses\nAmazon Forecast to develop a forecast model from 3 years of monthly data. There is no missing data. The specialist selects the DeepAR+ algorithm to train a predictor. The predictor means absolute percentage error (MAPE) is much larger than the MAPE produced by the current human forecasters.\nWhich changes to the CreatePredictor API call could improve the MAPE? (Choose two.)"},{"id":"ZmstrSby2tMmRpwl7ln0","url":"https://www.examtopics.com/discussions/amazon/view/44077-exam-aws-certified-machine-learning-specialty-topic-1/","unix_timestamp":1612550700,"question_images":["https://www.examtopics.com/assets/media/exam-media/04145/0007200001.png"],"topic":"1","answer":"A","answer_description":"","answers_community":["A (100%)"],"discussion":[{"comment_id":"284339","upvote_count":"26","timestamp":"1648311600.0","content":"I would answer A. Target and metadata must be in two files and loaded from S3, based on documentation: https://docs.aws.amazon.com/forecast/latest/dg/dataset-import-guidelines-troubleshooting.html","comments":[{"timestamp":"1697728680.0","upvote_count":"1","comment_id":"874840","poster":"ZSun","content":"1. I cannot find any evidence support the seperate file defination.\n2. A,B,C all seperate datasets, this explanation is weak."}],"poster":"[Removed]"},{"upvote_count":"9","timestamp":"1695411900.0","comments":[{"upvote_count":"2","comment_id":"967905","timestamp":"1706698380.0","poster":"ccpmad","content":"thank you chatgpt"}],"poster":"AjoseO","comment_id":"847547","content":"Selected Answer: A\nAmazon Forecast requires the input data to be separated into a target time series dataset and an item metadata dataset. \n\nThe target time series dataset should include the time series data that you want to use for forecasting, such as inventory demand in this case. The item metadata dataset should include the metadata that describes the items in the time series, such as product IDs, categories, and attributes. \n\nTherefore, the data scientist should use ETL jobs in AWS Glue to separate the dataset into a target time series dataset and an item metadata dataset. Both datasets should be uploaded as .csv files to Amazon S3, which is a suitable storage option for input data to Amazon Forecast."},{"upvote_count":"1","timestamp":"1724091960.0","comment_id":"1154269","content":"I would go with A\nInput formats for forecast -> Json, CSV and paraquet (Selects A & eliminates B, C, D)\nData needs to be split in target time series dataset and an item metadata dataset","poster":"AIWave"},{"comment_id":"990845","poster":"Mickey321","content":"Selected Answer: A\nTarget and metadata must be in two files","timestamp":"1708963860.0","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: A\nLetter A is correct, as it uses a specific transformation service (AWS Glue) and saves it in a cloud database for AWS Forecast to access. By default in ML, our storage option will be AWS S3 (unless caveats or issue specifications). That said, we discard B and C. Letter D is discarded due to the format requested by AWS Forecast being csv.","timestamp":"1706995020.0","comment_id":"971407","poster":"kaike_reis"},{"poster":"ystotest","timestamp":"1684934040.0","upvote_count":"3","comment_id":"725991","content":"Selected Answer: A\nI would vote for A"},{"poster":"tgaos","timestamp":"1669082460.0","comments":[{"content":"You can find the same question with the picture at https://ccnav7.net/a-data-scientist-wants-to-use-amazon-forecast-to-build-a-forecasting-model-for-inventory-demand-for-a-retail-company/","comment_id":"610959","timestamp":"1670055600.0","poster":"tgaos","upvote_count":"1"}],"upvote_count":"3","comment_id":"605052","content":"The answer is A.\nAccording to the https://docs.aws.amazon.com/forecast/latest/dg/forecast.dg.pdf , page 51.\nTarget Time Series Dataset: \nRequired: timestamp, item_id, demand\nAdditional: lead_time\n\nItem Metadata Dataset:\nitem_id, category"},{"timestamp":"1650677160.0","poster":"DSJingguo","upvote_count":"1","content":"The correct answer is A \n''Forecast supports only the comma-separated values (CSV) file format. You can't separate values using tabs, spaces, colons, or any other characters.\n\nGuideline: Convert your dataset to CSV format (using only commas as your delimiter) and try importing the file again.''","comment_id":"344498"},{"upvote_count":"1","timestamp":"1650338580.0","comment_id":"318390","poster":"achiko","content":"lead time belongs to related time series, as its not a target variable"}],"question_text":"A data scientist wants to use Amazon Forecast to build a forecasting model for inventory demand for a retail company. The company has provided a dataset of historic inventory demand for its products as a .csv file stored in an Amazon S3 bucket. The table below shows a sample of the dataset.\n//IMG//\n\nHow should the data scientist transform the data?","question_id":23,"isMC":true,"choices":{"B":"Use a Jupyter notebook in Amazon SageMaker to separate the dataset into a related time series dataset and an item metadata dataset. Upload both datasets as tables in Amazon Aurora.","C":"Use AWS Batch jobs to separate the dataset into a target time series dataset, a related time series dataset, and an item metadata dataset. Upload them directly to Forecast from a local machine.","D":"Use a Jupyter notebook in Amazon SageMaker to transform the data into the optimized protobuf recordIO format. Upload the dataset in this format to Amazon S3.","A":"Use ETL jobs in AWS Glue to separate the dataset into a target time series dataset and an item metadata dataset. Upload both datasets as .csv files to Amazon S3."},"answer_ET":"A","timestamp":"2021-02-05 19:45:00","exam_id":26,"answer_images":[]},{"id":"vrS27ghCMM4O2a0vFVQw","answer_ET":"B","answer_images":[],"exam_id":26,"question_text":"A Machine Learning Specialist is working with a large company to leverage machine learning within its products. The company wants to group its customers into categories based on which customers will and will not churn within the next 6 months. The company has labeled the data available to the Specialist.\nWhich machine learning model type should the Specialist use to accomplish this task?","answer":"B","timestamp":"2019-12-09 02:13:00","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/10005-exam-aws-certified-machine-learning-specialty-topic-1/","choices":{"A":"Linear regression","B":"Classification","C":"Clustering","D":"Reinforcement learning"},"question_id":24,"isMC":true,"answers_community":["B (100%)"],"topic":"1","unix_timestamp":1575853980,"answer_description":"","discussion":[{"timestamp":"1632145020.0","comment_id":"28029","upvote_count":"14","poster":"rsimham","content":"B seems to be okay"},{"content":"Selected Answer: B\nCLASSIFICATION - Binary Classification - Supervised Learning to be precise\nThe company wants to predict customer churn (whether a customer will leave or stay).\nThe data is labeled, meaning we have historical outcomes (churn or no churn).\nThe task involves categorizing customers into two groups:\nCustomers who will churn (leave)\nCustomers who will not churn (stay)\nThis means the problem is a Supervised Learning problem, specifically a binary classification problem.\nThe company wants to predict customer churn (whether a customer will leave or stay).\nThe data is labeled, meaning we have historical outcomes (churn or no churn).\nThe task involves categorizing customers into two groups:\nCustomers who will churn (leave)\nCustomers who will not churn (stay)\nThis means the problem is a Supervised Learning problem, specifically a binary classification problem.","comment_id":"1357058","poster":"JonSno","upvote_count":"2","timestamp":"1739654820.0"},{"upvote_count":"1","content":"Selected Answer: B\nThe reason for this choice is that classification is a type of supervised learning that predicts a discrete categorical value, such as yes or no, spam or not spam, or churn or not churn1. Classification models are trained using labeled data, which means that the input data has a known target attribute that indicates the correct class for each instance2. For example, a classification model that predicts customer churn would use data that has a label indicating whether the customer churned or not in the past.\n\nClassification models can be used for various applications, such as sentiment analysis, image recognition, fraud detection, and customer segmentation2. Classification models can also handle both binary and multiclass problems, depending on the number of possible classes in the target attribute3.","timestamp":"1727164260.0","poster":"Mickey321","comment_id":"973053"},{"content":"Selected Answer: B\nOption B. This is a scenario for supervised learning model as data is labelled and only A, B are supervised learning algorithms from the options. Linear learning is to predict time series data and distribution is selecting which class the input belongs to. Hence most suitable is to use Binomial distribution model in this case.","upvote_count":"1","comment_id":"996570","poster":"Sharath1783","timestamp":"1727164260.0"},{"comment_id":"1006326","upvote_count":"1","poster":"loict","content":"Selected Answer: B\nA. NO - Linear regression is not best for classification\nB. YES - Classification\nC. NO - we want supervised classification\nD. NO - there is nothing to Reinforce from","timestamp":"1727164260.0"},{"upvote_count":"3","poster":"mirik","comment_id":"931569","timestamp":"1687522800.0","content":"The question is not clear. Actually we have 2 tasks here - group into categories (clustering) and predict if customers will churn/not churn (classification). If we had to simply do classification, why there was mentioned to group into categories?"},{"poster":"ovokpus","upvote_count":"4","comment_id":"622296","content":"Selected Answer: B\nThis is definitely a classification problem","timestamp":"1656203160.0"},{"timestamp":"1652503920.0","upvote_count":"2","comment_id":"601428","poster":"Sivadharan","content":"Selected Answer: B\nB is correct"},{"timestamp":"1636232160.0","poster":"FabG","content":"B - it's a Binary Classification problem. Will the customer churn: Yes or No","comment_id":"214806","upvote_count":"4"},{"upvote_count":"1","timestamp":"1636229460.0","comment_id":"165869","content":"100% is B since it is about labelled data","poster":"syu31svc"},{"timestamp":"1634849760.0","poster":"eji","upvote_count":"3","comment_id":"128590","content":"i think the key is \"the company has labeled the data\" so this is classification, so it's B"},{"content":"B is okey","upvote_count":"2","comment_id":"98677","poster":"roytruong","timestamp":"1633091700.0"},{"content":"B is correct","timestamp":"1632978420.0","upvote_count":"3","comment_id":"37760","poster":"cybe001"}]},{"id":"HfIdXoDz5TbsMCSGMM6b","answer_description":"","question_id":25,"answer":"B","choices":{"D":"Deploy the model onto an Amazon Elastic Container Service (Amazon ECS) cluster using a P3 instance.","A":"Redeploy the model as a batch transform job on an M5 instance.","C":"Redeploy the model on a P3dn instance.","B":"Redeploy the model on an M5 instance. Attach Amazon Elastic Inference to the instance."},"exam_id":26,"url":"https://www.examtopics.com/discussions/amazon/view/44078-exam-aws-certified-machine-learning-specialty-topic-1/","answer_ET":"B","unix_timestamp":1612550820,"timestamp":"2021-02-05 19:47:00","topic":"1","isMC":true,"answer_images":[],"question_text":"A machine learning specialist is running an Amazon SageMaker endpoint using the built-in object detection algorithm on a P3 instance for real-time predictions in a company's production application. When evaluating the model's resource utilization, the specialist notices that the model is using only a fraction of the GPU.\nWhich architecture changes would ensure that provisioned resources are being utilized effectively?","discussion":[{"comment_id":"284340","upvote_count":"25","timestamp":"1632734880.0","poster":"[Removed]","content":"B is correct. Redeploy with CPU and add elastic inference to reduce costs. See: https://aws.amazon.com/machine-learning/elastic-inference/"},{"poster":"Togy","content":"Selected Answer: D\nThe Amazon EC2 M5 instance family is designed for general-purpose workloads, and they are CPU-optimized. Therefore, M5 instances do not come with GPUs. The only option that talks about optmising the use of already provisoned resources is option D, So that must be the answer.","comment_id":"1411870","timestamp":"1743282780.0","upvote_count":"1"},{"timestamp":"1727176680.0","content":"Selected Answer: B\nThis solution allows you to use a more cost-effective instance type while leveraging Elastic Inference to provide the necessary GPU acceleration","comment_id":"1288560","upvote_count":"1","poster":"MultiCloudIronMan"},{"content":"Selected Answer: C\nredeploying the model on a P3dn instance is the best approach to ensure the provisioned GPU resources are being utilized effectively.","comment_id":"1278243","upvote_count":"2","poster":"GS_77","timestamp":"1725451860.0"},{"comment_id":"1154285","timestamp":"1708375320.0","content":"My vote is B\nElastic inference\n- provides cheper accelration that full GPU\n- works with M class machines\n- Works with Tensorflow, MXNet, pytorch, image classification and object detection algorithms","upvote_count":"1","poster":"AIWave"},{"poster":"sukye","content":"Elastic Inference has been depreciated since Apr 2023.","timestamp":"1699824000.0","comment_id":"1068878","upvote_count":"3"},{"upvote_count":"2","timestamp":"1693059360.0","content":"Selected Answer: B\ncan reduce the cost and improve the resource utilization of your model, as Amazon Elastic Inference allows you to attach low-cost GPU-powered acceleration to Amazon EC2 and Amazon SageMaker instances to run inference workloads with a fraction of the compute resources. You can also choose the right amount of inference acceleration that suits your needs, and scale it up or down as needed.","comment_id":"990855","poster":"Mickey321"},{"comment_id":"807765","content":"Selected Answer: B\nAmazon Elastic Inference allows you to attach low-cost GPU-powered acceleration to EC2 and Sagemaker instances, to reduce the cost of running deep learning inference. \n\nYou can choose any CPU instance that is best suited to the overall compute and memory needs of your application, and then separately configure the right amount of GPU-powered inference acceleration. This would allow you to efficiently utilize resources and reduce costs.","timestamp":"1676318100.0","upvote_count":"3","poster":"AjoseO"},{"poster":"Peeking","upvote_count":"1","content":"Selected Answer: B\nElastic inference enables GPU only when load increases. With 50% utilisation there is no need to deploy P3 as the base inference machine.","comment_id":"740578","timestamp":"1670633220.0"},{"timestamp":"1669303260.0","comment_id":"726002","content":"Selected Answer: B\nAgreed with B","upvote_count":"1","poster":"ystotest"},{"timestamp":"1663007280.0","comment_id":"667342","poster":"Shailendraa","content":"12-sep exam","upvote_count":"2"},{"poster":"SriAkula","upvote_count":"2","timestamp":"1646475060.0","comment_id":"561353","content":"Answer: B\nExplanation: https://aws.amazon.com/machine-learning/elastic-inference/"},{"content":"B: production mostly needs CPU with EI rather than GPU machines","comment_id":"435780","poster":"mahmoudai","upvote_count":"1","timestamp":"1634777040.0"},{"content":"B..>Amazon Elastic Inference (EI) is a resource you can attach to your Amazon EC2 CPU instances to accelerate your deep learning (DL) inference workloads. Amazon EI accelerators come in multiple sizes and are a cost-effective method to build intelligent capabilities into applications running on Amazon EC2 instances.","timestamp":"1634740980.0","comment_id":"353740","poster":"mona_mansour","upvote_count":"3"},{"content":"B is correct","poster":"Vita_Rasta84444","upvote_count":"1","comment_id":"325069","timestamp":"1634237400.0"}],"answers_community":["B (73%)","C (18%)","9%"],"question_images":[]}],"exam":{"provider":"Amazon","id":26,"name":"AWS Certified Machine Learning - Specialty","numberOfQuestions":369,"isBeta":false,"isImplemented":true,"lastUpdated":"11 Apr 2025","isMCOnly":false},"currentPage":5},"__N_SSP":true}