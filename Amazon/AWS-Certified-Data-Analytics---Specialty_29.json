{"pageProps":{"questions":[{"id":"s1JtOfDUm2UF374dsbgs","answers_community":["C (100%)"],"answer":"C","choices":{"D":"Store the contents of the mapping file in an Amazon DynamoDB table. Change the Kinesis Data Analytics application to send its output to an AWS Lambda function that fetches the mapping and supplements each record to include the territory code, if one exists. Forward the record from the Lambda function to the original application destination.","A":"Store the contents of the mapping file in an Amazon DynamoDB table. Preprocess the records as they arrive in the Kinesis Data Analytics application with an AWS Lambda function that fetches the mapping and supplements each record to include the territory code, if one exists. Change the SQL query in the application to include the new field in the SELECT statement.","B":"Store the mapping file in an Amazon S3 bucket and configure the reference data column headers for the .csv file in the Kinesis Data Analytics application. Change the SQL query in the application to include a join to the file's S3 Amazon Resource Name (ARN), and add the territory code field to the SELECT columns.","C":"Store the mapping file in an Amazon S3 bucket and configure it as a reference data source for the Kinesis Data Analytics application. Change the SQL query in the application to include a join to the reference table and add the territory code field to the SELECT columns."},"question_text":"An online gaming company is using an Amazon Kinesis Data Analytics SQL application with a Kinesis data stream as its source. The source sends three non-null fields to the application: player_id, score, and us_5_digit_zip_code.\nA data analyst has a .csv mapping file that maps a small number of us_5_digit_zip_code values to a territory code. The data analyst needs to include the territory code, if one exists, as an additional output of the Kinesis Data Analytics application.\nHow should the data analyst meet this requirement while minimizing costs?","topic":"1","discussion":[{"content":"ANSWER:C\nEXPLANATION:Since ,Kinesis data analytics only consume the data from KDS and KDF ,so S3 Data reference can be attached to it as a datasource .","timestamp":"1635556020.0","upvote_count":"23","poster":"Heer","comment_id":"357567"},{"poster":"DerekKey","comment_id":"353447","upvote_count":"7","content":"C - https://docs.aws.amazon.com/kinesisanalytics/latest/dev/app-add-reference-data.html","timestamp":"1633592820.0"},{"poster":"pk349","content":"C: I passed the test","upvote_count":"1","comment_id":"886480","timestamp":"1682958180.0"},{"upvote_count":"6","content":"Selected Answer: C\nCorrect answer is C as Kinesis Data Analytics allows adding S3 source for reference data which can be referred by Kinesis Data Analytics for data enrichment. Kinesis Data Analytics stores it as an in-application reference table.\n\nOptions A & D are wrong as they are not cost-effective.\n\nOption B is wrong as Kinesis Data Analytics stores the reference data as an in-application reference table.","comment_id":"713116","poster":"cloudlearnerhere","timestamp":"1667832360.0"},{"comment_id":"643518","timestamp":"1659817680.0","poster":"rocky48","content":"Selected Answer: C\nANSWER:C","upvote_count":"1"},{"upvote_count":"1","comment_id":"494739","timestamp":"1638748080.0","comments":[{"content":"My bad! C is correct","poster":"tobsam","upvote_count":"2","timestamp":"1639498080.0","comment_id":"501488"}],"poster":"tobsam","content":"I think B is more concise than C. \nhttps://docs.aws.amazon.com/kinesisanalytics/latest/dev/sch-mapping.html\nSince the mapping file is a .csv file, I think we need the columns headers to avoid schema discovery for kda joins. \nhttps://docs.aws.amazon.com/kinesisanalytics/latest/dev/app-add-reference-data.html\nAlso IAM role ARN is needed to Add the Reference Data Source to the Application Configuration"},{"comment_id":"353457","upvote_count":"1","content":"I think A is correct.","comments":[{"content":"sorry,change to C","comment_id":"356399","poster":"afantict","upvote_count":"1","timestamp":"1634221920.0"}],"poster":"afantict","timestamp":"1633622100.0"},{"timestamp":"1632205800.0","comment_id":"348752","upvote_count":"1","poster":"VikG12","content":"C should be on one."}],"url":"https://www.examtopics.com/discussions/amazon/view/51699-exam-aws-certified-data-analytics-specialty-topic-1-question/","isMC":true,"timestamp":"2021-05-03 17:35:00","answer_description":"","answer_images":[],"answer_ET":"C","exam_id":20,"unix_timestamp":1620056100,"question_id":141,"question_images":[]},{"id":"SzlsyHrXOpkOs818m1l9","timestamp":"2021-05-03 17:35:00","question_text":"A company has collected more than 100 TB of log files in the last 24 months. The files are stored as raw text in a dedicated Amazon S3 bucket. Each object has a key of the form year-month-day_log_HHmmss.txt where HHmmss represents the time the log file was initially created. A table was created in Amazon Athena that points to the S3 bucket. One-time queries are run against a subset of columns in the table several times an hour.\nA data analyst must make changes to reduce the cost of running these queries. Management wants a solution with minimal maintenance overhead.\nWhich combination of steps should the data analyst take to meet these requirements? (Choose three.)","topic":"1","choices":{"B":"Add a key prefix of the form date=year-month-day/ to the S3 objects to partition the data.","E":"Drop and recreate the table with the PARTITIONED BY clause. Run the ALTER TABLE ADD PARTITION statement.","C":"Convert the log files to Apache Parquet format.","D":"Add a key prefix of the form year-month-day/ to the S3 objects to partition the data.","A":"Convert the log files to Apace Avro format.","F":"Drop and recreate the table with the PARTITIONED BY clause. Run the MSCK REPAIR TABLE statement."},"unix_timestamp":1620056100,"question_images":[],"isMC":true,"answer_description":"","question_id":142,"answer_images":[],"answers_community":["BCF (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/51700-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer_ET":"BCF","exam_id":20,"discussion":[{"poster":"Heer","timestamp":"1632999960.0","comment_id":"354565","upvote_count":"30","comments":[{"content":"Agree - B C F\nif it was case of removing partitions - D would have been better\nMSCK REPAIR TABLE only adds partitions to metadata; it does not remove them. To remove partitions from metadata after the partitions have been manually deleted in Amazon S3, run the command ALTER TABLE table-name DROP PARTITION. For more information see ALTER TABLE DROP PARTITION.","upvote_count":"2","poster":"lakeswimmer","comment_id":"492956","timestamp":"1638506940.0"},{"upvote_count":"1","comment_id":"659441","content":"Agree - Answer is BCF","timestamp":"1662315000.0","poster":"riyamalin"}],"content":"ANSWER:B,C,F\nOPTION B: Add a key prefix of the form date=year-month-day/ to the S3 objects to partition the data.\nEXPLAINATION: Your Amazon S3 bucket can support 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per partitioned prefix.So with every partition prefix we get additional support and that is why it is wise to add prefix especially when we have large set of data .\nLINK:https://aws.amazon.com/premiumsupport/knowledge-center/s3-object-key-naming-pattern/\n\nOPTION C:Convert the log files to Apache Parquet format.\nEXPLAINATION:Parquet format is columnar based and which improves your query performance when done for Athena .\n\nOPTION:F:Drop and recreate the table with the PARTITIONED BY clause. Run the MSCK REPAIR TABLE statement.\nEXPLAINATION:MSCK REPAIR TABLE compares the partitions in the table metadata and the partitions in S3. If new partitions are present in the S3 location that you specified when you created the table, it adds those partitions to the metadata and to the Athena table."},{"comment_id":"423944","poster":"Dr_Kiko","timestamp":"1635826920.0","upvote_count":"7","content":"Details why B and not D\nhttps://docs.aws.amazon.com/athena/latest/ug/partitions.html"},{"upvote_count":"1","content":"BCF: I passed the test","timestamp":"1682958240.0","comment_id":"886481","poster":"pk349"},{"upvote_count":"1","poster":"rocky48","timestamp":"1658293440.0","comment_id":"633848","content":"Selected Answer: BCF\nANSWER: B,C,F"},{"comment_id":"631433","content":"Selected Answer: BCF\ninitially went with BCE\nbut E is wrong as Athena favors \"=\" hive style partition year=2021/month=01/day=26/ with MSCK repair \n\nfor non-hive partition style data/2021/01/26/ have to use ALTER TABLE","timestamp":"1657812060.0","upvote_count":"2","poster":"ru4aws"},{"comment_id":"480249","content":"ans is B,C,F","timestamp":"1637180160.0","upvote_count":"1","poster":"aws2019"},{"upvote_count":"1","content":"B, C, F is the answer. C, D, E is a valid solution, but would in this case be more work. MSCK REPAIR TABLE scans through the directories on S3 to find partitions, but requires Hive-style partitioning, i. e. date=Y-M-D. You should almost never rely on MSCK REPAIR TABLE, it’s extremely inefficient, but the docs are full of examples using it so an exam would be too. The real way to do this is to use a Y-M-D/ partitioning scheme and partition projection.","comment_id":"441779","timestamp":"1635986580.0","poster":"iconara"},{"comments":[{"upvote_count":"2","content":"If you use a prefix of the form 'date=2020-11-11/' , you can use Athena to filter by date field.\n\nhttps://www.mikulskibartosz.name/partitioning-s3-data-by-date/","comment_id":"415818","comments":[{"comment_id":"418442","timestamp":"1635466680.0","upvote_count":"1","poster":"mickies9","content":"Can you please explain why do you need \"date=\" as a prefix?","comments":[{"upvote_count":"1","poster":"Merrick","comment_id":"788385","timestamp":"1674707340.0","content":"Query possible like \"..date >= 2020-11-11\""},{"poster":"ay12","content":"https://docs.aws.amazon.com/athena/latest/ug/partitions.html","upvote_count":"1","comment_id":"590256","timestamp":"1650664740.0"}]}],"timestamp":"1635430560.0","poster":"Bmaster"}],"comment_id":"394320","content":"Can anyone please explain why B and not D?\nI understand C&F.","upvote_count":"1","poster":"gunjan4392","timestamp":"1633437780.0"},{"upvote_count":"5","content":"B, C, F","timestamp":"1632499500.0","poster":"VikG12","comment_id":"348753"}],"answer":"BCF"},{"id":"qpt7Mo2EhMsiFP6A1FlY","discussion":[{"content":"IThink its C.\nRefer the below link \nhttps://aws.amazon.com/premiumsupport/knowledge-center/high-jvm-memory-pressure-elasticsearch/","poster":"Priyanka_01","comment_id":"159020","timestamp":"1633744260.0","upvote_count":"29"},{"timestamp":"1633794000.0","poster":"singh100","comment_id":"159164","content":"I agree with Option C:\nUnbalanced shard allocations across nodes or too many shards in a cluster can cause JVMMemoryPressue.\n\nResolution - Reduce the number of shards by deleting old or unused indices.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/high-jvm-memory-pressure-elasticsearch/","upvote_count":"10"},{"timestamp":"1690840440.0","upvote_count":"1","poster":"NikkyDicky","comment_id":"968518","content":"Selected Answer: C\nits a C"},{"upvote_count":"1","timestamp":"1682946300.0","poster":"pk349","comment_id":"886262","content":"C: I passed the test"},{"content":"C. Decrease the number of Amazon ES shards for the index.\n\nTo improve the performance of Amazon ES in this scenario, the number of shards for the index should be decreased. Currently, the index has 1,000 shards, which is likely causing high overhead and slowing down query performance. In general, it's recommended to have 20-30 GB of data per shard for efficient indexing and query performance in Amazon ES. However, having too many shards can lead to inefficient resource utilization and slow query performance.\n\nAdditionally, since the cluster is configured with 3 dedicated master nodes, increasing the memory of the master nodes may not have a significant impact on performance. Decreasing the number of data nodes may also not be an effective solution, as this could reduce the capacity of the cluster to handle the 1 TB of daily data ingestion.\n\nIncreasing the number of shards for the index would further exacerbate the performance issues, as more shards would lead to more overhead and slower query performance.","comment_id":"835886","upvote_count":"1","timestamp":"1678533840.0","poster":"AwsNewPeople"},{"comment_id":"728132","poster":"MaxwellBlackmore","upvote_count":"2","timestamp":"1669545660.0","content":"Selected Answer: C\nAccording to this link \nhttps://aws.amazon.com/premiumsupport/knowledge-center/opensearch-high-jvm-memory-pressure/\nIt clearly states that this issue can be caused due to \"Unbalanced shard allocations across nodes or too many shards in a cluster.\""},{"upvote_count":"2","content":"Selected Answer: C\nCorrect answer is C as one of the causes of JVMMemoryPressure error can be Unbalanced shard allocations across nodes or too many shards in a cluster. and can be resolved by reducing the number of shards by deleting old or unused indices.\n\nOption A is wrong because dedicated master nodes are only used to increase cluster stability. Therefore, this option won't help you improve the performance of the cluster.\n\nOption B is incorrect because these nodes carry all the data in your indexes (storage) and do all the processing for your requests (CPU). If you decrease the number of data nodes, the performance of the cluster still won't improve.\n\nOption D is incorrect. The JVMMemoryPressure error signifies that there is an unbalanced shard allocations across nodes. This means that there are too many shards in the Amazon ES cluster and not the other way around. To improve the performance of the cluster, you must decrease the number of shards.","poster":"cloudlearnerhere","timestamp":"1667563560.0","comment_id":"711102"},{"content":"Selected Answer: C\nSelected Answer: C","comment_id":"637724","timestamp":"1658885280.0","upvote_count":"1","poster":"rocky48"},{"timestamp":"1647708180.0","poster":"moon2351","content":"Selected Answer: C\nAnswer is C.","comment_id":"571160","upvote_count":"2"},{"timestamp":"1639968840.0","comment_id":"505196","upvote_count":"1","poster":"Marvel_jarvis","content":"Ans - C\nI got this question for my certification I gave on Dec 9th 2021."},{"content":"C is the right answer","upvote_count":"1","timestamp":"1637504580.0","comment_id":"483331","poster":"aws2019"},{"comment_id":"386000","timestamp":"1636234800.0","content":"Answer: C. Decrease the number of Amazon ES shards for the index.\nMemory pressure in the JVM can result if:\n You have unbalanced shard allocations across nodes\n You have too many shards in a cluster\n\nFewer shards can yield better performance if JVMMemoryPressure errors\nare encountered\n Delete old or unused indices","poster":"Donell","upvote_count":"2"},{"poster":"yogen","content":"C is correct. from documentation --\nhttps://docs.aws.amazon.com/elasticsearch-service/latest/developerguide/sizing-domains.html\nHere the shard size is 1.5*1000 (GB)/1000 (number of shards)= 1.5 GB which is much less than recommended size of shards.\n\nThe overarching goal of choosing a number of shards is to distribute an index evenly across all data nodes in the cluster. However, these shards shouldn't be too large or too numerous. A good rule of thumb is to try to keep shard size between 10–50 GiB. Large shards can make it difficult for Elasticsearch to recover from failure, but because each shard uses some amount of CPU and memory, having too many small shards can cause performance issues and out of memory errors. In other words, shards should be small enough that the underlying Amazon ES instance can handle them, but not so small that they place needless strain on the hardware.","comment_id":"282208","upvote_count":"6","timestamp":"1636148520.0"},{"poster":"lostsoul07","upvote_count":"3","content":"C is the right answer","timestamp":"1635967080.0","comment_id":"274239"},{"timestamp":"1635761100.0","upvote_count":"1","poster":"Deep101","content":"The question says the domain is running one index, if so how can we assume there are old unused indices. shouldn't we reindex to adjust the number of shards?","comment_id":"254825"},{"content":"C is correct","comment_id":"216832","poster":"BillyC","upvote_count":"2","timestamp":"1635610440.0"},{"timestamp":"1635574260.0","content":"From link https://aws.amazon.com/premiumsupport/knowledge-center/high-jvm-memory-pressure-elasticsearch/\n\nYou can resolve high JVM memory pressure issues by reducing traffic to the cluster. To reduce traffic to the cluster, follow these best practices:\nReduce the number of shards by deleting old or unused indices.","poster":"syu31svc","comment_id":"191264","upvote_count":"3"},{"timestamp":"1635127440.0","comment_id":"175252","poster":"Paitan","upvote_count":"2","content":"https://aws.amazon.com/tw/premiumsupport/knowledge-center/high-jvm-memory-pressure-elasticsearch/\nSo option C is the right choice."},{"comment_id":"159519","timestamp":"1634170860.0","upvote_count":"3","poster":"zeronine","content":"my answer is C \nhttps://aws.amazon.com/premiumsupport/knowledge-center/high-jvm-memory-pressure-elasticsearch/"},{"timestamp":"1633979220.0","upvote_count":"2","content":"C. \n'Reduce the number of shards by deleting old or unused indices.'\nSee:\nhttps://aws.amazon.com/tw/premiumsupport/knowledge-center/high-jvm-memory-pressure-elasticsearch/","poster":"zanhsieh","comment_id":"159198"},{"content":"I wil go with option C\nhttps://docs.aws.amazon.com/elasticsearch-service/latest/developerguide/aes-handling-errors.html","poster":"Prodip","timestamp":"1633501140.0","comment_id":"155820","upvote_count":"1","comments":[{"timestamp":"1633724460.0","comment_id":"156525","poster":"testtaker3434","upvote_count":"1","comments":[{"comment_id":"161464","upvote_count":"1","poster":"Prodip","content":"Please take a look \"Recovering from a Continuous Heavy Processing Load\". 1st step is \"delete unnecessary indices\"","timestamp":"1634907660.0"},{"comment_id":"175269","upvote_count":"7","content":"In this question, each of the 1000 shards would contain ~1.5GB, which is not a good practice:\n\"A good rule of thumb is to try to keep shard size between 10–50 GiB. Large shards can make it difficult for Elasticsearch to recover from failure, but because each shard uses some amount of CPU and memory, having too many small shards can cause performance issues and out of memory errors.\"\nSource: same URL as Prodip has suggested already.","timestamp":"1635317820.0","poster":"greenv"}],"content":"Can you elaborate? The link is full of \"JVMMemoryPressure\" = out of memory."},{"upvote_count":"1","comment_id":"163924","content":"Agreed","timestamp":"1634988180.0","poster":"awssp12345"}]},{"poster":"testtaker3434","comment_id":"153571","timestamp":"1633094880.0","content":"Answer should be A. JVMMemoryPressure errors are out of memory errors. Thoughts?","comments":[{"poster":"testtaker3434","comments":[{"timestamp":"1634630100.0","poster":"testtaker3434","upvote_count":"2","comment_id":"160934","content":"Actually, C, NOT A"}],"comment_id":"160933","timestamp":"1634255460.0","upvote_count":"1","content":"Going through all the links, Ive changed my mind to A."}],"upvote_count":"1"}],"answers_community":["C (100%)"],"answer_ET":"C","answer_images":[],"choices":{"A":"Increase the memory of the Amazon ES master nodes.","B":"Decrease the number of Amazon ES data nodes.","D":"Increase the number of Amazon ES shards for the index.","C":"Decrease the number of Amazon ES shards for the index."},"question_id":143,"isMC":true,"answer_description":"","exam_id":20,"question_text":"A company uses Amazon OpenSearch Service (Amazon Elasticsearch Service) to store and analyze its website clickstream data. The company ingests 1 TB of data daily using Amazon Kinesis Data Firehose and stores one day's worth of data in an Amazon ES cluster.\nThe company has very slow query performance on the Amazon ES index and occasionally sees errors from Kinesis Data Firehose when attempting to write to the index. The Amazon ES cluster has 10 nodes running a single index and 3 dedicated master nodes. Each data node has 1.5 TB of Amazon EBS storage attached and the cluster is configured with 1,000 shards. Occasionally, JVMMemoryPressure errors are found in the cluster logs.\nWhich solution will improve the performance of Amazon ES?","question_images":[],"answer":"C","timestamp":"2020-08-09 13:52:00","unix_timestamp":1596973920,"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/27696-exam-aws-certified-data-analytics-specialty-topic-1-question/"},{"id":"8qNko2BJOmDGasxT8rS3","answer_ET":"B","answers_community":["B (100%)"],"topic":"1","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/51701-exam-aws-certified-data-analytics-specialty-topic-1-question/","choices":{"A":"Ingest the data stream with Amazon Kinesis Data Streams. Have an AWS Lambda consumer evaluate the stream, collect the number status codes, and evaluate the data against a previously trained RCF model. Persist the source and results as a time series to Amazon DynamoDB.","B":"Ingest the data stream with Amazon Kinesis Data Streams. Have a Kinesis Data Analytics application evaluate the stream over a 5-minute window using the RCF function and summarize the count of status codes. Persist the source and results to Amazon S3 through output delivery to Kinesis Data Firehouse.","D":"Ingest the data stream with Amazon Kinesis Data Firehose with a delivery frequency of 5 minutes or 1 MB into Amazon S3. Have a Kinesis Data Analytics application evaluate the stream over a 1-minute window using the RCF function and summarize the count of status codes. Persist the results to Amazon S3 through a Kinesis Data Analytics output to an AWS Lambda integration.","C":"Ingest the data stream with Amazon Kinesis Data Firehose with a delivery frequency of 1 minute or 1 MB in Amazon S3. Ensure Amazon S3 triggers an event to invoke an AWS Lambda consumer that evaluates the batch data, collects the number status codes, and evaluates the data against a previously trained RCF model. Persist the source and results as a time series to Amazon DynamoDB."},"isMC":true,"answer_description":"","discussion":[{"comment_id":"398065","timestamp":"1636175760.0","content":"Answer is B.\nFirst of all the question mentions about analyzing the stream over a 5-minute timeframe.\nOnly option B satisfies the above condition. Also KDA uses RCF.\n\nQ: How can I perform real-time anomaly detection in Kinesis Data Analytics?\nKinesis Data Analytics includes pre-built SQL functions for several advanced analytics including one for anomaly detection. You can simply make a call to this function from your SQL code for detecting anomalies in real-time. Kinesis Data Analytics uses the Random Cut Forest algorithm to implement anomaly detection. For more information on Random Cut Forests, see the Streaming Data Anomaly Detection whitepaper.","upvote_count":"20","poster":"Donell"},{"content":"Looks like 'B' it is.","comment_id":"348755","poster":"VikG12","timestamp":"1634131200.0","upvote_count":"6"},{"upvote_count":"1","comment_id":"1054977","poster":"weasd555","content":"Answer is . B\nIngest the data stream with Amazon Kinesis Data Streams. Have a Kinesis Data Analytics application evaluate the stream over a 5-minute window using the RCF function and summarize the count of status codes. Persist the source and results to Amazon S3 through output delivery to Kinesis Data Firehouse","timestamp":"1698363720.0"},{"timestamp":"1682958300.0","poster":"pk349","upvote_count":"1","content":"B: I passed the test","comment_id":"886484"},{"comments":[{"poster":"GoForTheWin","timestamp":"1676879340.0","content":"Because you don't pay for further pages.","upvote_count":"1","comment_id":"814968"}],"poster":"OLANGA","comment_id":"733890","upvote_count":"1","content":"Why further questions are not available?","timestamp":"1669996800.0"},{"poster":"cloudlearnerhere","content":"Selected Answer: B\nCorrect answer is B as Kinesis Data Streams can be used to capture the data with Kinesis Data Analytics to perform RCF over a 5 minutes window pushing the source and results data into Kinesis Data Firehose for S3 persistence.\n\nhttps://aws.amazon.com/blogs/big-data/perform-near-real-time-analytics-on-streaming-data-with-amazon-kinesis-and-amazon-elasticsearch-service/\n\nOption A is wrong as it would require more effort and the cost would be higher if storing the data in DynamoDB.\n\nOption C is wrong as Kinesis Data Firehose with a delivery frequency of 1 minute or 1 MB would not be able to meet the requirement accurately. Also, the cost would be higher if storing the data in DynamoDB.\n\nOption D is wrong as having the Kinesis Data Analytics application evaluate the stream over a 1-minute window would not meet the 5 minutes requirement.","timestamp":"1667832840.0","upvote_count":"5","comment_id":"713119"},{"content":"answer is B","upvote_count":"2","timestamp":"1667096820.0","comment_id":"707573","poster":"t47"},{"content":"Answer is B","timestamp":"1664760120.0","poster":"awsdatacert","upvote_count":"1","comment_id":"685145"},{"timestamp":"1659815700.0","upvote_count":"1","content":"Selected Answer: B\nAnswer is B","poster":"rocky48","comment_id":"643498"},{"upvote_count":"3","comments":[{"upvote_count":"6","poster":"Donell","content":"Answer is B. However no where it is mentioned about real-time in question. Streaming Data can also be near real-time.Ruling out KDF due to the above reason is incorrect.","timestamp":"1636057980.0","comment_id":"398064"}],"content":"ANSWER:OPTION B\nEXPLAINATION: Since it's a real time streaming data ,so filter out option C,D as KFH is near real time .Now out of option A and B ,S3 has a low data persistence cost .","comment_id":"354576","poster":"Heer","timestamp":"1636017780.0"}],"exam_id":20,"answer_images":[],"answer":"B","question_id":144,"timestamp":"2021-05-03 17:36:00","question_text":"A company has an application that ingests streaming data. The company needs to analyze this stream over a 5-minute timeframe to evaluate the stream for anomalies with Random Cut Forest (RCF) and summarize the current count of status codes. The source and summarized data should be persisted for future use.\nWhich approach would enable the desired outcome while keeping data persistence costs low?","unix_timestamp":1620056160},{"id":"RJYg3shho5YJk4YtVg5l","discussion":[{"poster":"bryankeb","content":"A,D for sure","comment_id":"349306","timestamp":"1632441120.0","upvote_count":"28"},{"timestamp":"1640618160.0","upvote_count":"14","poster":"npt","content":"B and D\nHave the daily roll-up data readily available for 1 year. After 1 year, archive the daily roll-up data for occasional but immediate access -> D: Standard and Standard IA after 1 year.\nQuery access will be needed ONLY for re-evaluation, which MAY occur within the first 90 days -> Glacier is enough, there is no require for immediate access, and we also don't know it's frequency, but \"ONLY\" and \"MAY\" keywords can indicate that it's rare to do re-evaluation.\nQuestion asks for minimizing the cost, so B, not A","comment_id":"510416"},{"upvote_count":"1","timestamp":"1693410480.0","comment_id":"994261","poster":"jerintom","content":"B and D, Objects cannot be directly transitioned to Amazon S3 Standard-Infrequent Access (S3 Standard-IA), objects must be stored for a minimum of 30 days before they can be transitioned to this storage class- So it has to be stored as Amazon S3 Standard storage class for the first year to satisfy immediate access requirement."},{"poster":"MLCL","comment_id":"971780","comments":[{"upvote_count":"1","content":"https://aws.amazon.com/s3/storage-classes/glacier/instant-retrieval/","timestamp":"1691133660.0","comment_id":"971782","poster":"MLCL"},{"upvote_count":"1","timestamp":"1691133720.0","content":"But the occasional keyword points to S3 IA, so probable BD","poster":"MLCL","comment_id":"971784"}],"timestamp":"1691133540.0","upvote_count":"2","content":"Selected Answer: BE\nThe right answer is BE. \nS3 Glacier has instant access."},{"comment_id":"886485","upvote_count":"1","timestamp":"1682958360.0","poster":"pk349","content":"BD: I passed the test"},{"content":"Selected Answer: BC\nThis seems more appropriate to me","upvote_count":"1","comment_id":"858862","poster":"rsn","comments":[{"upvote_count":"1","content":"I meant BD and not BC","poster":"rsn","timestamp":"1680469260.0","comment_id":"859353"}],"timestamp":"1680441360.0"},{"upvote_count":"4","poster":"Aina","content":"Selected Answer: BD\nIt is B instead of A because the source data is not really used for any immediate querying ever. It is processing to create the roll-up data and that's it. So it makes sense to store it in the cheapest storage, which is Glacier.","comment_id":"858431","timestamp":"1680399300.0"},{"timestamp":"1675517820.0","upvote_count":"2","content":"Selected Answer: BD\nTwo kinds of data. For the roll-up data everyone agrees on D instead of E.\nFor the source data it is B not A because the source data does not have to be accessed immediately.","poster":"milofficial","comment_id":"797998"},{"poster":"Ody__","comment_id":"768096","upvote_count":"3","timestamp":"1673041200.0","content":"Selected Answer: BD\nB & D https://aws.amazon.com/s3/pricing/\nB because S3 Glacier Instant Retrieval - For long-lived archive data accessed once a quarter with instant retrieval in milliseconds"},{"content":"As per Udemy preparation course, the Glacier that these questions mention must be the \"Glacier Flexible Retrieval\" that has 3 options: expedited (1 to 5 minutes), standard (3 to 5 hours) and bulk (5 to 12 hours). Only the bulk option is free.\nSo, the question here about A vs B is the cost of possible retrievals vs cost of IA. I think that possibly the glacier can become costlier as we MAY have to access the data a uncertain number of times.\nHaving this said I would pick A & D, but I'm still very doubtfull tbh...","poster":"silvaa360","upvote_count":"3","timestamp":"1670772540.0","comment_id":"741834"},{"timestamp":"1668789900.0","comment_id":"721424","upvote_count":"1","poster":"SivaFawkes16","content":"Option E is obvious for roll-up data and seems clear from the comments here. Between A and B, I would choose B as it has lesser cost, which is clearly stated in the question. Refer here for cost structure: https://aws.amazon.com/s3/pricing/"},{"poster":"cloudlearnerhere","upvote_count":"7","content":"Selected Answer: AD\nCorrect answers are A & D\n\nOption A as the source data exports can be stored in (S3 Standard-IA) storage class for 90 days if needed, and then moved to S3 Glacier Deep Archive for 5 years before it is expired.\n\nOption D as the roll-up data can be stored in Standard class for a year and then moved to S3 Standard-IA to save cost but still provide immediate access.\n\nOption B is wrong as although you can transition the objects directly to the S3 Glacier storage class, the data would not be readily available for roll-up calculation and would cost more to perform the roll-up.\n\nOptions C & E are wrong as S3 Glacier and Glacier Deep Archive would not be able to provide immediate access to the roll-up data.","timestamp":"1667841180.0","comment_id":"713187"},{"content":"Selected Answer: BD\nB and D\nB (S3 Glacier) instead of A (S3 Standard-IA) keeps storage costs to a minimum. Query access to the source data may or may not occur within the first 90 days. S3 Glacier will do.","poster":"b33f","timestamp":"1667722380.0","upvote_count":"3","comment_id":"712192"},{"timestamp":"1664281860.0","comment_id":"680750","content":"✑ Have the daily roll-up data readily available for 1 year.\n✑ After 1 year, archive the daily roll-up data for occasional but immediate access.\nWhich combination of actions will meet these requirements while keeping storage costs to a minimum?\n\n Answer E - Cost is low and readily available \nStore the daily roll-up data initially in the Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class. \nApply a lifecycle configuration that changes the storage class to Amazon S3 Glacier 1 year after data creation.","poster":"JHJHJHJHJ","upvote_count":"1"},{"comment_id":"680374","timestamp":"1664252220.0","poster":"lygf","upvote_count":"1","comments":[{"upvote_count":"1","timestamp":"1664252280.0","comment_id":"680375","content":"Sorry, should be B&D","poster":"lygf"}],"content":"Selected Answer: AD\nEven S3 Glacier can have instant retrieval\nhttps://aws.amazon.com/s3/storage-classes/glacier/\nS3 Glacier Instant Retrieval delivers the lowest cost storage, up to 68% lower cost (than S3 Standard-Infrequent Access), for long-lived data that is accessed once per quarter and requires millisecond retrieval.."},{"timestamp":"1663981200.0","comment_id":"677544","poster":"he11ow0rId","upvote_count":"3","content":"Selected Answer: BD\nBetween A and B, I think for the context of the exam, it should be B. 90 days is a keyword for Glacier as all Glacier tiers require 90 days min storage"},{"poster":"rocky48","comment_id":"638498","comments":[{"content":"Selected Answer : B & D \nLink : https://aws.amazon.com/s3/pricing/\nB because S3 Glacier Instant Retrieval - For long-lived archive data accessed once a quarter with instant retrieval in milliseconds.","poster":"rocky48","timestamp":"1674032820.0","comment_id":"779789","upvote_count":"1"}],"timestamp":"1658986920.0","content":"Selected Answer: AD","upvote_count":"1"},{"timestamp":"1653132120.0","poster":"Bik000","comment_id":"604849","content":"Selected Answer: AD\nMy Answer is A & D","upvote_count":"1"},{"timestamp":"1642095240.0","comment_id":"522980","content":"A and D. Glacier select will work only in uncompressed csv and we are not sure about the data format hence stay to rule out B.","upvote_count":"3","poster":"vKaspar"},{"content":"i think B and D\nB - Glacier select will be useful for Query access to do reevaluate within 90 days","poster":"arun004","timestamp":"1639435140.0","upvote_count":"5","comment_id":"500936","comments":[{"content":"You got the point!","poster":"Jerry84","upvote_count":"1","comment_id":"830448","timestamp":"1678061100.0"}]},{"poster":"awsmani","content":"The more I think on this, B& D might be better option from cost perspective.","timestamp":"1638226380.0","upvote_count":"4","comment_id":"490257"},{"comment_id":"490255","timestamp":"1638226020.0","upvote_count":"2","content":"Believe it is A& D. \n1. The data will be accessed occasionally which you read as in-frequent access but it will be readily available at the same time it is cost effective.\n2. The roll-up data should be available immediately for one year so it makes sense to put in the standard S3 storage for accessibility.","poster":"awsmani"},{"poster":"Balendu","timestamp":"1635763320.0","upvote_count":"2","content":"2 sets of data. \nSource data-> Chose between A & B - My choice would be B as \"lowest possible cost\" (mentioned in Question), it says query access may be needed so Glacier select can work here. S3 Standard IA is an overkill. \nRolled up data - Again the question mentions \"cost to a minimum\" only E gives that.","comment_id":"451026"},{"content":"The question clearly states that -- After 1 year, archive the daily roll-up data for occasional but immediate access.\nThis means all three options listing Glacier Deep Archive (i.e. A, B, C) are immediately out.\nYou are left with D & E.","comment_id":"439022","timestamp":"1635612300.0","upvote_count":"1","poster":"virendrapsingh"},{"comment_id":"413941","upvote_count":"1","content":"Supposed to rule out Glacier Deep Archive due to \"After 1 year, archive the daily roll-up data for occasional but immediate access.\"? If yes then D is the only one.\nIs the next close one Glacier as its expedited takes 1~5 mins so E?","timestamp":"1635082920.0","poster":"doobc"},{"timestamp":"1634467740.0","poster":"gunjan4392","upvote_count":"1","content":"A,D for sure","comment_id":"395204"},{"poster":"ksaws","upvote_count":"1","content":"its not A bcoz the data cant be moved from Standard to Glacier Deep it has to move from Glacier to Glacier Deep i think its B,E ( as it follows the data movement rules with in S3)","comments":[{"upvote_count":"3","poster":"julio_conca","timestamp":"1634311680.0","comment_id":"372016","content":"Data can be transition from Standard to Glacier Deep. It's A and D\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html"}],"timestamp":"1633519020.0","comment_id":"357883"},{"upvote_count":"2","timestamp":"1632537540.0","poster":"Monika14Sharma","comment_id":"351955","content":"why A? It should be B for cost effectiveness!","comments":[{"comment_id":"352283","poster":"VikG12","upvote_count":"1","comments":[{"upvote_count":"2","comment_id":"399134","content":"It says source data \"may\" be required for re-evaluation for 90 days. We can use glacier select incase of any requirement. So, B could also be an option here. cost-effective IMO.","timestamp":"1634572140.0","poster":"arvindn"},{"upvote_count":"1","content":"so D,E?","comment_id":"356110","poster":"afantict","comments":[{"poster":"afantict","content":"Only E?","upvote_count":"1","timestamp":"1633326060.0","comment_id":"356114"}],"timestamp":"1633138680.0"}],"content":"Data needs to readily available initially. So glacier is not a choice.","timestamp":"1633080420.0"}]},{"comment_id":"348757","timestamp":"1632318780.0","upvote_count":"1","poster":"VikG12","content":"Should be A, D"}],"answer_ET":"BD","question_text":"An online retailer needs to deploy a product sales reporting solution. The source data is exported from an external online transaction processing (OLTP) system for reporting. Roll-up data is calculated each day for the previous day's activities. The reporting system has the following requirements:\n✑ Have the daily roll-up data readily available for 1 year.\n✑ After 1 year, archive the daily roll-up data for occasional but immediate access.\n✑ The source data exports stored in the reporting system must be retained for 5 years. Query access will be needed only for re-evaluation, which may occur within the first 90 days.\nWhich combination of actions will meet these requirements while keeping storage costs to a minimum? (Choose two.)","answers_community":["BD (56%)","AD (33%)","7%"],"isMC":true,"answer":"BD","choices":{"B":"Store the source data initially in the Amazon S3 Glacier storage class. Apply a lifecycle configuration that changes the storage class from Amazon S3 Glacier to Amazon S3 Glacier Deep Archive 90 days after creation, and then deletes the data 5 years after creation.","A":"Store the source data initially in the Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class. Apply a lifecycle configuration that changes the storage class to Amazon S3 Glacier Deep Archive 90 days after creation, and then deletes the data 5 years after creation.","C":"Store the daily roll-up data initially in the Amazon S3 Standard storage class. Apply a lifecycle configuration that changes the storage class to Amazon S3 Glacier Deep Archive 1 year after data creation.","E":"Store the daily roll-up data initially in the Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class. Apply a lifecycle configuration that changes the storage class to Amazon S3 Glacier 1 year after data creation.","D":"Store the daily roll-up data initially in the Amazon S3 Standard storage class. Apply a lifecycle configuration that changes the storage class to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) 1 year after data creation."},"answer_description":"","question_images":[],"topic":"1","exam_id":20,"url":"https://www.examtopics.com/discussions/amazon/view/51702-exam-aws-certified-data-analytics-specialty-topic-1-question/","timestamp":"2021-05-03 17:37:00","answer_images":[],"unix_timestamp":1620056220,"question_id":145}],"exam":{"isBeta":false,"provider":"Amazon","isMCOnly":true,"name":"AWS Certified Data Analytics - Specialty","numberOfQuestions":164,"isImplemented":true,"id":20,"lastUpdated":"11 Apr 2025"},"currentPage":29},"__N_SSP":true}