{"pageProps":{"questions":[{"id":"0i5MmDdETioDZEIuu3wk","answer":"A","answer_description":"","question_images":[],"choices":{"C":"Use Apache Spark's DataFrame dropDuplicates() API to eliminate duplicates and then write the data to Amazon Redshift.","A":"Modify the AWS Glue job to copy the rows into a staging table. Add SQL commands to replace the existing rows in the main table as postactions in the DynamicFrameWriter class.","B":"Load the previously inserted data into a MySQL database in the AWS Glue job. Perform an upsert operation in MySQL, and copy the results to the Amazon Redshift table.","D":"Use the AWS Glue ResolveChoice built-in transform to select the most recent value of the column."},"exam_id":20,"discussion":[{"poster":"testtaker3434","upvote_count":"19","content":"Answer should be A according to the link provided. Thoughts?","timestamp":"1632428160.0","comments":[{"timestamp":"1640144640.0","comment_id":"506659","upvote_count":"6","content":"Indeed A\nhttps://aws.amazon.com/premiumsupport/knowledge-center/sql-commands-redshift-glue-job/","poster":"lakediver"}],"comment_id":"153569"},{"upvote_count":"15","comment_id":"387608","timestamp":"1636131300.0","poster":"Huy","content":"B is wrong. We don't need a staging DB here which is costly and moreover MySQL is not the right choice.\nC. dropDuplicates() is used to remove duplicate records in the Spark not destination DB\nD. ResolveChoice is to cast data with unidentified data type to a specified data type and also work on Spark not destination DB.\n\nA is the answer"},{"comment_id":"968516","poster":"NikkyDicky","timestamp":"1690840200.0","content":"Selected Answer: A\nIt's n A","upvote_count":"1"},{"poster":"Espa","upvote_count":"2","comment_id":"903190","timestamp":"1684671600.0","content":"Selected Answer: A\nTo me A looks correct answer, check this link\n\nhttps://stackoverflow.com/questions/52397646/aws-glue-to-redshift-duplicate-data"},{"comment_id":"886256","timestamp":"1682946180.0","poster":"pk349","content":"A: I passed the test","upvote_count":"1"},{"timestamp":"1678532340.0","poster":"AwsNewPeople","upvote_count":"4","content":"A. Modify the AWS Glue job to copy the rows into a staging table. Add SQL commands to replace the existing rows in the main table as postactions in the DynamicFrameWriter class.\n\nTo update the Redshift table without duplicates when AWS Glue jobs are rerun, the company should modify the AWS Glue job to copy the rows into a staging table. The job should then add SQL commands to replace the existing rows in the main table as postactions in the DynamicFrameWriter class. This approach ensures that the data written to the Redshift table does not contain any duplicates, and the table only contains the latest data.\n\nLoading the previously inserted data into a MySQL database and performing an upsert operation may be a feasible approach but adds complexity to the architecture. Using Spark's dropDuplicates() API to eliminate duplicates may not always work correctly when dealing with large datasets. Using the ResolveChoice built-in transform is used for handling schema changes in a column, not for removing duplicates.","comment_id":"835856"},{"upvote_count":"1","poster":"itsme1","content":"Selected Answer: A\nWith option B, it is to copy the RedShift data into SQL and back to RedShift.\n\nOption A is simpler","timestamp":"1677944760.0","comment_id":"829085"},{"comment_id":"796467","timestamp":"1675374120.0","poster":"tpompeu","content":"Selected Answer: A\nA, for sure","upvote_count":"1"},{"poster":"henom","upvote_count":"1","timestamp":"1669601880.0","content":"Correct Answer - A\n\nB is is incorrect because you can't use the COPY command to copy data directly from a MySQL database into Amazon Redshift. A workaround for this is to move the MySQL data into Amazon S3 and use AWS Glue as a staging table to perform the upsert operation. Since this method requires more effort, it is not the best approach to solve the problem.","comment_id":"728742"},{"upvote_count":"7","comment_id":"711098","timestamp":"1667562960.0","poster":"cloudlearnerhere","content":"Selected Answer: A\nCorrect answer is A as Redshift does not support merge or upsert on the single table. However, a staging table can be created and data merged with the main table.\n\nOption B is wrong as a staging DB as MySQL is not required.\n\nOption C is wrong as dropDuplicates() is used to remove duplicate records in the Spark and not destination DB.\n\nOption D is wrong as ResolveChoice is to cast data with an unidentified data type to a specified data type. It does not handle duplicates."},{"comment_id":"638490","comments":[{"timestamp":"1666383660.0","poster":"rocky48","content":"Got confused with C as dataframe.dropDuplicates() also will work, but as per the given question, we have to stick to AWS Glue job, thus Answer is A.","comment_id":"701145","upvote_count":"1"}],"upvote_count":"1","timestamp":"1658986560.0","content":"Selected Answer: A\nAnswer is A","poster":"rocky48"},{"poster":"Bik000","comment_id":"604844","upvote_count":"1","content":"Selected Answer: A\nAnswer is A","timestamp":"1653131760.0"},{"comment_id":"523397","upvote_count":"1","content":"Answer is A","poster":"Shivanikats","timestamp":"1642144140.0"},{"timestamp":"1635996420.0","upvote_count":"3","content":"Answer: A. Modify the AWS Glue job to copy the rows into a staging table. Add SQL commands to replace the existing rows in the main table as postactions in the DynamicFrameWriter class.","comment_id":"385993","poster":"Donell"},{"upvote_count":"2","comment_id":"359104","poster":"leliodesouza","content":"The answer is A.","timestamp":"1635990660.0"},{"comment_id":"357117","upvote_count":"1","timestamp":"1635727140.0","content":"A should be the right answer. I found a link that helps to explain why. https://aws.amazon.com/pt/premiumsupport/knowledge-center/sql-commands-redshift-glue-job/","poster":"ariane_tateishi"},{"poster":"lostsoul07","content":"A is the right answer","timestamp":"1635659940.0","comment_id":"274228","upvote_count":"4"},{"comment_id":"259392","poster":"Glendon","upvote_count":"6","timestamp":"1634858760.0","content":"Answer is A. https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-upsert.html"},{"content":"Can't we use Glue Bookmarks here?","comment_id":"254456","upvote_count":"4","timestamp":"1634823540.0","poster":"Deep101"},{"poster":"G000z","comment_id":"244178","timestamp":"1634686380.0","content":"B is the correct option creating a staging table the doing an upsert allows selection of only the most recent record eliminating duplicates","upvote_count":"2"},{"poster":"BillyC","upvote_count":"1","content":"A is correct!","timestamp":"1634680980.0","comment_id":"216834"},{"content":"A is the best option over here. B also works but is not at all cost effective and needs to setup an extra MYSQL database.","poster":"Paitan","timestamp":"1633912020.0","comment_id":"175250","upvote_count":"2"},{"timestamp":"1633818420.0","poster":"Jh2501","upvote_count":"1","comments":[{"comment_id":"175264","timestamp":"1634080080.0","comments":[{"content":"Thanks greenv for the detailed explanation. Appreciated.","timestamp":"1634216700.0","upvote_count":"1","comment_id":"176273","poster":"Jh2501"}],"content":"After the first job execution, the data will be already uploaded to Redshift. Therefore, since the dataFrame is reading S3 and not Redshift, it doesn't know which data exists on Redshift already. So, you could use dropDuplicates but only after loading back the recently-ingested data from Redshift, then load from S3, drop the dups, truncate table at Redshift and load again. As you can see, this is tremendously inefficient, so the answer is A","poster":"greenv","upvote_count":"7"}],"comment_id":"174962","content":"Does anyone know how come C doesn't work? Thanks."},{"content":"Answer is A - https://aws.amazon.com/premiumsupport/knowledge-center/sql-commands-redshift-glue-job/ See the section Merge an Amazon Redshift table in AWS Glue (upsert)","upvote_count":"2","timestamp":"1633118580.0","poster":"awssp12345","comment_id":"163861"},{"upvote_count":"1","timestamp":"1632602280.0","content":"my answer is A too","poster":"zeronine","comment_id":"159516"},{"upvote_count":"1","comment_id":"159157","poster":"singh100","content":"Agree with A. Option B is taking data to MySQL which is not the right approach as it takes more cost and effort.","timestamp":"1632512580.0"}],"answer_images":[],"answer_ET":"A","timestamp":"2020-08-09 13:51:00","isMC":true,"unix_timestamp":1596973860,"question_id":121,"question_text":"A company has a business unit uploading .csv files to an Amazon S3 bucket. The company's data platform team has set up an AWS Glue crawler to do discovery, and create tables and schemas. An AWS Glue job writes processed data from the created tables to an Amazon Redshift database. The AWS Glue job handles column mapping and creating the Amazon Redshift table appropriately. When the AWS Glue job is rerun for any reason in a day, duplicate records are introduced into the Amazon Redshift table.\nWhich solution will update the Redshift table without duplicates when jobs are rerun?","url":"https://www.examtopics.com/discussions/amazon/view/27694-exam-aws-certified-data-analytics-specialty-topic-1-question/","answers_community":["A (100%)"],"topic":"1"},{"id":"bWFx9dUUq3a4DVqBtWL2","answer":"D","answer_description":"","question_images":[],"choices":{"B":"The hash key generation process for the records is not working correctly. The data analyst should generate an explicit hash key on the producer side so the records are directed to the appropriate shard accurately.","A":"There are multiple shards in a stream and order needs to be maintained in the shard. The data analyst needs to make sure there is only a single shard in the stream and no stream resize runs.","C":"The records are not being received by Kinesis Data Streams in order. The producer should use the PutRecords API call instead of the PutRecord API call with the SequenceNumberForOrdering parameter.","D":"The consumer is not processing the parent shard completely before processing the child shards after a stream resize. The data analyst should process the parent shard completely first before processing the child shards."},"exam_id":20,"discussion":[{"content":"D:https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-after-resharding.html\n the parent shards that remain after the reshard could still contain data that you haven't read yet that was added to the stream before the reshard. If you read data from the child shards before having read all data from the parent shards, you could read data for a particular hash key out of the order given by the data records' sequence numbers. Therefore, assuming that the order of the data is important, you should, after a reshard, always continue to read data from the parent shards until it is exhausted. Only then should you begin reading data from the child shards.","comment_id":"158418","poster":"Priyanka_01","upvote_count":"80","timestamp":"1634149980.0"},{"comment_id":"886375","content":"D: I passed the test","timestamp":"1682950500.0","upvote_count":"1","poster":"pk349"},{"content":"Selected Answer: D\nCorrect answer is D as Kinesis Data Streams resharding causes the existing data to be in parent shard and new data is routed to child shards. The issue can occur if the consumers starts processing the child shard without completely processing the parent shard.","timestamp":"1667826120.0","upvote_count":"4","comment_id":"713049","poster":"cloudlearnerhere"},{"content":"Selected Answer: D\nAnswer is D for sure","poster":"Bik000","comment_id":"604860","timestamp":"1653132600.0","upvote_count":"2"},{"poster":"aws2019","content":"D is ans","timestamp":"1637787720.0","comment_id":"486255","upvote_count":"1"},{"timestamp":"1635948420.0","poster":"Kamalt","upvote_count":"1","content":"Answer D.","comment_id":"383565"},{"upvote_count":"2","timestamp":"1635386520.0","poster":"lostsoul07","content":"D is the right answer","comment_id":"274359"},{"upvote_count":"2","timestamp":"1635303960.0","comment_id":"205255","content":"Answer should be D.","poster":"sanjaym"},{"upvote_count":"3","poster":"Paitan","content":"Nicely explained by Priyanka_01.","comment_id":"175603","timestamp":"1634628480.0"}],"answer_images":[],"answer_ET":"D","timestamp":"2020-08-15 05:41:00","isMC":true,"unix_timestamp":1597462860,"question_id":122,"question_text":"A company is streaming its high-volume billing data (100 MBps) to Amazon Kinesis Data Streams. A data analyst partitioned the data on account_id to ensure that all records belonging to an account go to the same Kinesis shard and order is maintained. While building a custom consumer using the Kinesis Java SDK, the data analyst notices that, sometimes, the messages arrive out of order for account_id. Upon further investigation, the data analyst discovers the messages that are out of order seem to be arriving from different shards for the same account_id and are seen when a stream resize runs.\nWhat is an explanation for this behavior and what is the solution?","answers_community":["D (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/28622-exam-aws-certified-data-analytics-specialty-topic-1-question/","topic":"1"},{"id":"Aj7HZTRlrPP4EuJvA9ed","exam_id":20,"answer":"C","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/74030-exam-aws-certified-data-analytics-specialty-topic-1-question/","topic":"1","question_text":"A media analytics company consumes a stream of social media posts. The posts are sent to an Amazon Kinesis data stream partitioned on user_id. An AWS\nLambda function retrieves the records and validates the content before loading the posts into an Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster. The validation process needs to receive the posts for a given user in the order they were received by the Kinesis data stream.\nDuring peak hours, the social media posts take more than an hour to appear in the Amazon OpenSearch Service (Amazon ES) cluster. A data analytics specialist must implement a solution that reduces this latency with the least possible operational overhead.\nWhich solution meets these requirements?","answer_images":[],"question_images":[],"answers_community":["C (95%)","5%"],"discussion":[{"comment_id":"626104","upvote_count":"12","timestamp":"1656757320.0","poster":"Alekx42","content":"Selected Answer: C\nIncreasing the number of shards seems to be a good idea since Lambda can process 1 batch of data from each Kinesis shard with 1 lambda invocation. This means that if you have 100 shards you can have 100 concurrent lambda invocations. If you increase the number of shards you can increase the parallelism and you could be quicker to process the data. This is assuming that the Lambda ParallelizationFactor is set to 1.\nSwitching to AWS Glue could increase the speed of the data processing (since Glue can use Spark, which can be way faster than a Lambda function when processing a lot of data) but this would increase the operational overhead."},{"timestamp":"1708587060.0","content":"For those wondering why not go for 'A', and go for 'C' instead: in glue worker types such as G.1X, G.2X, etc must be selected which increases overhead. Hence, the one with least overhead is 'C' by using the concept of parallelism.","comment_id":"1156212","poster":"god_father","upvote_count":"1"},{"comment_id":"1120157","poster":"GCPereira","upvote_count":"2","content":"A: is wrong because glue has concurrency limit and spark is poor option to small files;\nB: if lambda is a unique consumer, dont have necessity to upgrade these for enhanced-fan-out;\nC: this is a tipically bottleneck problem... to solve this just insert more shards;\nD: is wrong because switch SaaS have A LOT OF operational overhead","timestamp":"1705004700.0"},{"content":"I want to contribute in this Question and telling you why B isn't correct and C yes.\nOption B is useless because the question never tell that there is another consumer, so lambda is leveraging the shard throughput (there is no need to set enhanced fan-out consumer).\nIncrementing shard will work, because in AWS there's 1 lambda function invocation per kinesis shard. At the same time, per shard you can increase lambda functions concurrency with the ParallelizationFactor set to a name between 1 (default value) to 10.","upvote_count":"2","comment_id":"977665","timestamp":"1691671560.0","poster":"juanife"},{"upvote_count":"2","content":"Could be C or B depending on multiple factors.\nTo increase the prformance of KDS you have 3 options : \n- More shards.\n- Parallelization factors (specific to Lambda)\n- HTTP/2\n- Enhanced Fan-out.","comment_id":"971597","timestamp":"1691112720.0","poster":"MLCL"},{"comment_id":"948281","upvote_count":"3","content":"B seems to be correct. As ordering has to be maintained, Lambda function will be handy only if the consumer is KCL because it has that inbuilt sorting logic for parent and child shard.","poster":"Debuggerrr","timestamp":"1689007680.0"},{"upvote_count":"1","content":"C can never be right - increasing shards cannot assure ordering and thats the catch here. B seems close.","comments":[{"comment_id":"971595","timestamp":"1691112480.0","poster":"MLCL","upvote_count":"3","content":"If you partition by user_id, it does guarantee order, since only the same user_id go to the same shard."},{"timestamp":"1691112600.0","content":"The stream is partitioned by user_id, increasing the number of shards won't impact the ordering of records for a specific user because all posts from a particular user would go to the same shard.","comment_id":"971596","upvote_count":"3","poster":"MLCL"}],"comment_id":"908153","poster":"Debi_mishra","timestamp":"1685211120.0"},{"upvote_count":"2","comment_id":"886377","content":"C: I passed the test","poster":"pk349","timestamp":"1682950500.0"},{"upvote_count":"2","poster":"rags1482","timestamp":"1679784300.0","comment_id":"850531","content":"Answer B\nbased on below link\nhttps://aws.amazon.com/about-aws/whats-new/2018/11/aws-lambda-supports-kinesis-data-streams-enhanced-fan-out-and-http2/"},{"comment_id":"811909","poster":"Arjun777","content":"option B- Migrating the Lambda consumers to an HTTP/2 stream consumer can significantly reduce processing latency and improve the overall performance of the system. This is because HTTP/2 stream consumers allow Lambda to retrieve records from the stream more efficiently, which can help to reduce processing latency and improve the overall performance of the system.\n\nMigrating the Lambda consumers to an HTTP/2 stream consumer can significantly reduce processing latency and improve the overall performance of the system. This is because HTTP/2 stream consumers allow Lambda to retrieve records from the stream more efficiently, which can help to reduce processing latency and improve the overall performance of the system.\n\nMigrating to an HTTP/2 stream consumer requires minimal operational overhead, as it only involves updating the Lambda function code to use the new consumer type. This can be done easily using the AWS SDK for Lambda, and does not require any major changes to the existing architecture.\n\nTherefore, option B is the best solution for reducing the latency with the least possible operational overhead.","comments":[{"content":"Increasing shards is easier than enhanced fan-out and cheaper too.","upvote_count":"2","timestamp":"1682117580.0","poster":"aws_kid","comment_id":"876896"}],"timestamp":"1676640300.0","upvote_count":"3"},{"upvote_count":"1","poster":"nadavw","timestamp":"1673865420.0","comment_id":"777558","content":"Selected Answer: B\nC is a temporary solution, as there is no idea of the expected number of shards you need to increase. There is no simple auto-scaling in Kinesis, so there will be an operational overhead to continuously monitor the system and increase the number of shards. In addition, the partitioning-sharding is according to user_id - how this can be solved?\n\nB - the enhanced fanout approach is good for it, as described here:\n\"The enhanced capacity enables you to achieve higher outbound throughput without provisioning more streams or shards in the same stream.\"\nhttps://aws.amazon.com/blogs/compute/increasing-real-time-stream-processing-performance-with-amazon-kinesis-data-streams-enhanced-fan-out-and-aws-lambda/"},{"upvote_count":"1","poster":"Arka_01","content":"Selected Answer: C\n\"least possible operational overhead\" - This is the key here. As the solution demands to reduce latency, this will be the easiest way to do so. Notice, that cost factor is not mentioned in the question.","comment_id":"678556","timestamp":"1664092500.0"},{"comment_id":"634360","content":"Selected Answer: C\nIncreasing the number of shards looks ok.","timestamp":"1658379780.0","poster":"rocky48","upvote_count":"1"},{"content":"I go with B for these two reasons.\n1. Messages should be received in same order for user. Scaling out the shards during peak hours and scaling in after peak hours may change the message order. C is not the correct one.\n2. HTTP/2 is enhanced fan out consumer which will reduce the latency from 200ms to 70ms. 65% latency reduction","upvote_count":"4","comment_id":"625454","poster":"Sen5476","timestamp":"1656628020.0"},{"poster":"f4bi4n","upvote_count":"2","content":"Selected Answer: C\nC, but you must ensure that you use Partition Keys (In this case the User) to ensure the requested ordering per User. HTTP/2 would also decrease latency but needs more effort","timestamp":"1653115380.0","comment_id":"604737"},{"upvote_count":"2","comment_id":"595240","content":"C - Increase Shards","timestamp":"1651348080.0","poster":"jrheen"},{"timestamp":"1651142520.0","poster":"CHRIS12722222","content":"I think B \nstandard consumer latency = 200ms\nHttp/2 latency = 70ms","comment_id":"593749","upvote_count":"3"},{"content":"Selected Answer: C\nIncrease shard as the Peak period will mean more data.","timestamp":"1650548280.0","upvote_count":"3","comment_id":"589434","poster":"astalavista1"}],"answer_ET":"C","choices":{"B":"Migrate the Lambda consumers from standard data stream iterators to an HTTP/2 stream consumer.","A":"Migrate the validation process from Lambda to AWS Glue.","D":"Send the posts stream to Amazon Managed Streaming for Apache Kafka instead of the Kinesis data stream.","C":"Increase the number of shards in the Kinesis data stream."},"question_id":123,"unix_timestamp":1650548280,"isMC":true,"timestamp":"2022-04-21 15:38:00"},{"id":"sbgkMOazyOamyLS6Zl8D","answer_description":"","answers_community":["CD (100%)"],"exam_id":20,"answer_images":[],"question_text":"A company launched a service that produces millions of messages every day and uses Amazon Kinesis Data Streams as the streaming service.\nThe company uses the Kinesis SDK to write data to Kinesis Data Streams. A few months after launch, a data analyst found that write performance is significantly reduced. The data analyst investigated the metrics and determined that Kinesis is throttling the write requests. The data analyst wants to address this issue without significant changes to the architecture.\nWhich actions should the data analyst take to resolve this issue? (Choose two.)","topic":"1","timestamp":"2020-08-14 10:17:00","choices":{"B":"Replace the Kinesis API-based data ingestion mechanism with Kinesis Agent.","A":"Increase the Kinesis Data Streams retention period to reduce throttling.","C":"Increase the number of shards in the stream using the UpdateShardCount API.","E":"Customize the application code to include retry logic to improve performance.","D":"Choose partition keys in a way that results in a uniform record distribution across shards."},"answer":"CD","url":"https://www.examtopics.com/discussions/amazon/view/28554-exam-aws-certified-data-analytics-specialty-topic-1-question/","isMC":true,"unix_timestamp":1597393020,"question_id":124,"answer_ET":"CD","question_images":[],"discussion":[{"content":"CD. If wrong partition keys are distributed well, then retrying would still hit the hot shards.\nhttps://aws.amazon.com/blogs/big-data/under-the-hood-scaling-your-kinesis-data-streams/","comment_id":"159900","timestamp":"1632089940.0","poster":"zanhsieh","upvote_count":"33","comments":[{"poster":"awssp12345","upvote_count":"2","timestamp":"1633153080.0","content":"agreed.","comment_id":"168892"},{"poster":"GeeBeeEl","upvote_count":"1","content":"D i agree with because of this other link https://aws.amazon.com/premiumsupport/knowledge-center/kinesis-data-stream-throttling-errors/ \"use a random partition key to ingest your records. If the operations already use a random partition key, then adjust the key to correct the distribution.\" Why would you want to increase the number of shards, is that in the link?","timestamp":"1633354500.0","comment_id":"185412"}]},{"timestamp":"1633211220.0","comment_id":"175610","poster":"Paitan","upvote_count":"7","content":"C and D for me."},{"content":"Selected Answer: CD\nMore shards and better partitioning is always the answer for write performance issues.","upvote_count":"4","poster":"MLCL","timestamp":"1691113320.0","comment_id":"971603"},{"comment_id":"886379","upvote_count":"1","content":"CD: I passed the test","poster":"pk349","timestamp":"1682950560.0"},{"upvote_count":"2","comment_id":"768903","poster":"Chelseajcole","timestamp":"1673123820.0","content":"Why no B? Kinesis should have better performance than KPL"},{"upvote_count":"3","comment_id":"713051","poster":"cloudlearnerhere","timestamp":"1667826300.0","content":"Selected Answer: CD\nCorrect answers are C & D as the common causes are hitting shard limits and. increasing shard count and choosing an appropriate partition key would help solve the issue."},{"poster":"thirukudil","content":"Selected Answer: CD\nthrottling writes mean there is no sufficient shards. so we need to increase the shard counts. Introducing partition key helps in uniform distribution of writes across the shards which in turn we can avoid hot shards","comment_id":"707691","timestamp":"1667117760.0","upvote_count":"3"},{"comment_id":"678557","timestamp":"1664092620.0","content":"Selected Answer: CD\nNone of the other options makes sense.","upvote_count":"1","poster":"Arka_01"},{"comment_id":"637116","upvote_count":"1","poster":"rocky48","timestamp":"1658806800.0","content":"Selected Answer: CD\nanswer is CD"},{"comment_id":"604828","timestamp":"1653130620.0","content":"Selected Answer: CD\nAnswer is C & D","poster":"Bik000","upvote_count":"1"},{"comment_id":"486264","upvote_count":"1","timestamp":"1637788080.0","poster":"aws2019","content":"C and D"},{"content":"C,D is the answer.","timestamp":"1636159920.0","poster":"Donell","comment_id":"387887","upvote_count":"1"},{"timestamp":"1636012200.0","upvote_count":"1","content":"Considering the link bellow the option E isn't a valid option. \"If there is a retry mechanic in the producer, failed records are tried again. This can also cause a delay in processing.\"\nhttps://aws.amazon.com/pt/premiumsupport/knowledge-center/kinesis-data-stream-throttling/","poster":"ariane_tateishi","comment_id":"362967"},{"content":"C,D is the right answer","poster":"lostsoul07","comment_id":"274371","upvote_count":"3","timestamp":"1635641760.0"},{"content":"When there are failed records that aren't able to enter the Kinesis data stream, the stream throttles. If there is a retry mechanic in the producer, failed records are tried again. This can also cause a delay in processing.\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/kinesis-data-stream-throttling/\n\nSo an existing retry logic can also result in throttling. I think C&D will make more sense as they are meant for performance increase. Retry logic is more for handling errors and not handling performance.","timestamp":"1635342360.0","comment_id":"253004","poster":"Par301","upvote_count":"5"},{"timestamp":"1635295560.0","upvote_count":"6","comment_id":"238998","poster":"Draco31","content":"C and D for me.\n\"A few months after launch, a data analyst found that write performance is significantly reduced\" means the load is increasing constantly and will not decrease, and it's not a temporary spike. So we are facing a lack of shard or bad partition key is burning some shards over time"},{"comments":[{"upvote_count":"2","comment_id":"597963","timestamp":"1651892220.0","content":"\"changing the partition key\" just change the code to use different key during putting item. No architecture changed.","poster":"MWL"}],"upvote_count":"1","content":"It's C & E for me. D is not an option because changing the partition key affects the architecture.","comment_id":"203206","timestamp":"1634431200.0","poster":"kamparia"},{"content":"C and D\nKey part of the question: \"without significant changes to the architecture\"","comment_id":"191920","upvote_count":"2","timestamp":"1634227740.0","poster":"syu31svc"},{"upvote_count":"1","timestamp":"1633595400.0","comments":[{"timestamp":"1634088540.0","poster":"GeeBeeEl","upvote_count":"1","content":"https://aws.amazon.com/premiumsupport/knowledge-center/kinesis-data-stream-throttling-errors/ clearly explains the what errors to look for in throttling \"Rate Exceeded\" or \"WriteProvisionedThroughputExceeded\" errors? \nAlso see https://aws.amazon.com/kinesis/data-streams/faqs/ which shows that “The capacity limits of an Amazon Kinesis data stream are defined by the number of shards within the data stream. ....rejected with a ProvisionedThroughputExceeded exception. If this is due to a temporary rise of the data stream’s input data rate, RETRY by the data producer will eventually lead to completion of the requests. If this is due to a sustained rise of the data stream’s input data rate, you should INCREASE the number of shards within your data stream to provide enough capacity for the put data calls to consistently succeed. I believe C and E is the answer --- tough eh?","comments":[{"upvote_count":"1","poster":"jove","content":"Retry logic doesn't improve the performance.","timestamp":"1634436300.0","comment_id":"209968"}],"comment_id":"185435"}],"content":"@zanhsieh gave a very good link which I believe supports D. The basis for saying it supports D is the choice of random partition keys. However, I have a problem rationalizing whether indeed a randomization of the partition key will result in a uniform record distribution. The link https://aws.amazon.com/premiumsupport/knowledge-center/kinesis-data-stream-throttling-errors/ only advised using a random partition key or correct the distribution, this is not necessarily the same as \"create a uniform distribution\" which is in D......So I wonder, is D really one of the options. In my next post, I will expand more on the thoughts around the answer","poster":"GeeBeeEl","comment_id":"185431"},{"content":"my answer is CD","poster":"zeronine","comment_id":"162240","timestamp":"1632753420.0","comments":[{"timestamp":"1634957760.0","poster":"LMax","content":"Agree with C & D","comment_id":"214582","upvote_count":"1"}],"upvote_count":"5"},{"upvote_count":"2","content":"C & E:\nhttps://aws.amazon.com/kinesis/data-streams/faqs/\ncapacity limits of an Amazon Kinesis data stream are defined by the number of shards within the data stream. The limits can be exceeded by either data throughput or the number of PUT records. While the capacity limits are exceeded, the put data call will be rejected with a ProvisionedThroughputExceeded exception. If this is due to a temporary rise of the data stream’s input data rate, retry by the data producer will eventually lead to completion of the requests. If this is due to a sustained rise of the data stream’s input data rate, you should increase the number of shards within your data stream to provide enough capacity for the put data calls to consistently succeed. In both cases, Amazon CloudWatch metrics allow you to learn about the change of the data stream’s input data rate and the occurrence of ProvisionedThroughputExceeded exceptions.","poster":"Priyanka_01","comment_id":"157927","timestamp":"1632085680.0"}]},{"id":"hWMyyqxlVpogIzGZv9BN","answers_community":["D (57%)","A (37%)","3%"],"answer_images":[],"topic":"1","answer_description":"","question_id":125,"unix_timestamp":1598085180,"discussion":[{"comments":[{"poster":"metin","comment_id":"196304","timestamp":"1634358000.0","content":"In question, it is not mentioned that they definitely need a real-time solution. And Glue can be used for batch processing of stream data.","upvote_count":"2"},{"timestamp":"1636185720.0","content":"they already use Firehouse that does data batching, so no problem with D","comment_id":"424541","poster":"Dr_Kiko","upvote_count":"1"},{"comments":[{"poster":"Ipc01","timestamp":"1643354100.0","upvote_count":"2","content":"Plus, AWS Kinesis Data Firehose has in-house data transformation so you don’t need to add operational overhead by utilizing AWS Lambda","comment_id":"534504"}],"poster":"Ipc01","timestamp":"1643354040.0","content":"Since question is searching for an answer that reflects operational efficiency, setting up individual glue jobs is definitely more time consuming so the answer is A","upvote_count":"2","comment_id":"534503"},{"comment_id":"930871","upvote_count":"2","poster":"juanife","content":"No, the question asks for a solution that replaces ETL with Pyspark in EMR cluster indeed, so AWS GLUE ETL jobs would be a good choice here.\nUndoubtedly, D is the correct option, but option A is not as bad as seems, but maybe it's not as cheap as D.","timestamp":"1687456500.0"},{"comment_id":"391668","timestamp":"1636128540.0","upvote_count":"4","content":"Question ask about concern on downstream processing so no problem with D","poster":"Huy"}],"content":"D seems the good choice because is the only answer dealing with small files but the doubt is... Glue is only batch! but there is a new article of 04/20 that says glue is now also supporting streaming process. So if we consider this article D is right. But we can?\nhttps://aws.amazon.com/it/about-aws/whats-new/2020/04/aws-glue-now-supports-serverless-streaming-etl/","timestamp":"1633144500.0","upvote_count":"28","comment_id":"176472","poster":"Phoenyx89"},{"comments":[{"poster":"GauravM17","comment_id":"174345","content":"Should this not be D? Where are we handling the small files in A?","comments":[{"poster":"GeeBeeEl","content":"You are changing them to parquet on the fly with Firehose.","timestamp":"1633695360.0","upvote_count":"1","comment_id":"185461"},{"content":"Firehose will batch in buffer time and reduce number of files.","upvote_count":"2","comment_id":"712605","timestamp":"1667763540.0","poster":"Ashish1101"},{"comment_id":"184760","comments":[{"comment_id":"186541","poster":"AjNapa","content":"This part of the question is what many ppl here have missed. You’re right. It’s A","timestamp":"1634043600.0","upvote_count":"2"},{"poster":"Haimett","timestamp":"1666625520.0","comment_id":"703164","upvote_count":"2","content":"There is no problem in using pyspark with Glue."},{"comment_id":"196302","poster":"metin","timestamp":"1634290140.0","upvote_count":"6","content":"Glue can utilize PySpark transformations.\nhttps://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-python-transforms.html"}],"content":"Remember \"They want to continue to use PySpark.\"\nD is migrating PySpark into Glue","timestamp":"1633195320.0","poster":"kzu19878","upvote_count":"8"}],"timestamp":"1632745320.0","upvote_count":"1"}],"upvote_count":"23","comment_id":"173176","content":"Anas: A\n\nhttps://aws.amazon.com/blogs/big-data/optimizing-downstream-data-processing-with-amazon-kinesis-data-firehose-and-amazon-emr-running-apache-spark/","timestamp":"1632630480.0","poster":"singh100"},{"upvote_count":"2","poster":"NarenKA","content":"Selected Answer: A\nConverting data into Apache Parquet format before storing it in S3 optimizes the data for analytical processing. KDF able to automatically batch, compress, and convert incoming streaming data into Parquet format. It reduces the overhead with processing a large number of small files without the need for additional processing or intermediate steps. And it allows the team to continue using PySpark on Amazon EMR for data processing.\nB - AWS Lambda to process individual messages could introduce operational overhead and not efficiently handle the conversion of a large number of small files.\nC - moving data processing to Redshift would require changes to the existing PySpark-based processing pipeline and not most cost-effective solution.\n D - merging small files into larger ones using Glue addresses the efficiency concern, it suggests migrating PySpark jobs from EMR to AWS Glue could involve refactoring of the existing jobs.","timestamp":"1708553340.0","comment_id":"1155910"},{"comment_id":"1120162","poster":"GCPereira","timestamp":"1705005420.0","upvote_count":"1","content":"emr is very expansive and spark doesn't work well with a large number of small files... the best option is to merge small files into large files and use job glue to decrease the cost of downstream processing... D is a perfect answer"},{"poster":"GCPereira","timestamp":"1703546940.0","content":"take a look at this sentence \"...the solution needs to be well-architected\"... that is, cost-efficient, secure, highly available and operational-efficient... aws emr are not highly available and need a lot of operational resources... then disagree emr... to continue pyspark job, aws glue are the best option","upvote_count":"1","comment_id":"1105585"},{"upvote_count":"2","timestamp":"1691113740.0","comment_id":"971607","content":"Selected Answer: D\nD solves the issue with small files and replaces the EMR batch job with a Glue one, which is cheaper.\nIf a Transient EMR cluster was in the A proposition, it would be acceptable.","poster":"MLCL"},{"timestamp":"1685212080.0","comment_id":"908157","content":"Both A and D correct technically and very difficult to figure out the cost effective solution without more context. Other answers assuming EMR is a long running and is expensive but thats not mentioned here. D has upper hand considering all will be serverless.","poster":"Debi_mishra","upvote_count":"1"},{"comment_id":"886380","poster":"pk349","timestamp":"1682950680.0","upvote_count":"3","content":"D: I passed the test"},{"content":"Selected Answer: D\n\"cost of downstream data processing\" so migrate it","comment_id":"849343","poster":"akashm99101001com","upvote_count":"1","timestamp":"1679666460.0"},{"comment_id":"768909","upvote_count":"4","timestamp":"1673124420.0","content":"Selected Answer: D\nThis question is testing if you know Glue can run PySpark job","poster":"Chelseajcole"},{"poster":"DeerSong","comment_id":"768687","upvote_count":"1","timestamp":"1673105280.0","content":"Selected Answer: D\nD for sue"},{"timestamp":"1672650240.0","upvote_count":"1","content":"Selected Answer: D\nD is correct","comment_id":"763608","poster":"Kinlive1991"},{"comment_id":"743000","timestamp":"1670860200.0","upvote_count":"1","content":"Selected Answer: D\nglue cheaper than emr","poster":"siju13"},{"upvote_count":"1","timestamp":"1669775340.0","content":"The correct answer is D :- the option that says: Replace the Amazon EMR with AWS Glue. Program an AWS Glue ETL script in Python to merge the small sensor data into larger files and convert them to Apache Parquet format.\n\nThe option that says: Deploy a Kinesis Data Firehose delivery stream to collect and convert sensor data to Apache Parquet format. Deliver the transformed data into an Amazon S3 bucket. Process the data from the bucket using a PySpark Job running on an Amazon EMR cluster is incorrect. Although this option is valid, there is no significant cost reduction since Amazon EMR is still running. AWS Glue can provide lower costs while providing the same function. In addition, it is better to merge the smaller files to a large file, than just compressing them using the Apache Parquet format to improve ingestion performance.","comment_id":"731023","poster":"henom"},{"comment_id":"728161","content":"Selected Answer: D\nThe question is about the data processing and not the full pipeline including ingestion so D is the most efficient from processing perspective","upvote_count":"1","poster":"nadavw","timestamp":"1669549560.0"},{"timestamp":"1669530000.0","poster":"thuyeinaung","comment_id":"727953","upvote_count":"1","content":"Selected Answer: D\nGlue can run PySpark"},{"comment_id":"717233","upvote_count":"1","timestamp":"1668334380.0","poster":"alinato","content":"Selected Answer: B\nLambda can run pyspark and is cost effective and serverless meaning well architectured."},{"content":"Selected Answer: D\nCorrect answer is D as the smaller files can be grouped into larger files and transformed into Parquet columnar data format. Also replacing EMR with Glue will provide a cost-effective solution for the downstream solution.\n\nOption A is wrong as using EMR would not provide a cost-effective solution for the downstream solution.\n\nOption B is wrong as Lambda is not good for ETL processing.\n\nOption C is wrong as Redshift would not meet the requirement of using PySpark.","upvote_count":"4","timestamp":"1667828940.0","poster":"cloudlearnerhere","comment_id":"713072"},{"comment_id":"708105","content":"Selected Answer: D\nAnswer D. PySpark Jobs are scheduled, not real time/near-real-time. So it can migrate to AWS Glue. And the question is about downstream processing","timestamp":"1667177340.0","poster":"aefuen1","upvote_count":"1"},{"poster":"cloudlearnerhere","timestamp":"1666618980.0","content":"Correct answer is D as the smaller files can be grouped into larger files and transformed into Parquet columnar data format. Also replacing EMR with Glue will provide a cost-effective solution for the downstream solution.\n\nOption A is wrong as using EMR would not provide a cost-effective solution for the downstream solution.\n\nOption B is wrong as Lambda is not good for ETL processing.\n\nOption C is wrong as Redshift would not meet the requirement of using PySpark.","comment_id":"703069","upvote_count":"2"},{"upvote_count":"1","timestamp":"1666596120.0","content":"Selected Answer: A\nBecause there is no need for real-time streams so KDS is not needed and KFH can push the data straight to S3.","comment_id":"702781","poster":"MultiCloudIronMan"},{"timestamp":"1665736680.0","poster":"rav009","content":"Selected Answer: A\nA over D. \nFirehouse can merge files and convert to parquet format on the fly.","upvote_count":"2","comment_id":"694604"},{"timestamp":"1664001420.0","upvote_count":"1","comment_id":"677652","comments":[{"timestamp":"1664001420.0","comment_id":"677653","content":"I meant the answer should be D","poster":"Arka_01","upvote_count":"1"}],"poster":"Arka_01","content":"Selected Answer: C\nThis question is talking about finding an alternative solution for downstream processing, that is EMR processing. And, what can be better than enhancing performance by merging files and using Glue to run PySpark code."},{"comment_id":"664316","content":"Selected Answer: D\nD as Glue can run Pyspark","timestamp":"1662706260.0","poster":"he11ow0rId","upvote_count":"1"},{"poster":"maitis","timestamp":"1661375160.0","content":"Selected Answer: A\nIt should be A as it's well architected.","comment_id":"651464","upvote_count":"2"},{"timestamp":"1661351460.0","content":"Selected Answer: A\nA is Def correct","comment_id":"651309","upvote_count":"2","poster":"redwan123"},{"timestamp":"1661330100.0","upvote_count":"2","poster":"dushmantha","comment_id":"651153","content":"Selected Answer: A\nI will remove answers B,C and D because it does not allow to use PySpark (eventhough some of them run spark under the hood, it clearly says they need to use PySpark). And ans A seemed to be a good option too, nothing wrong with it."},{"upvote_count":"2","poster":"alfredofmt","timestamp":"1659462720.0","content":"Selected Answer: A\nA - CORRECT, small files problem is solved by batching functionalities of Kinesis Data Firehose\nB - WRONG, trying to re-create real-time delivery with Lambda is inefficient\nC - WRONG, moving to Redshift does not satisfy the requirement of maintaining usage of PySpark\nD - WRONG, trying to re-create the batching functionalities of Kinesis Data Firehose through Glue is inefficient","comment_id":"641389"},{"poster":"rocky48","timestamp":"1658885580.0","upvote_count":"1","content":"Selected Answer: D\nSelected Answer: D","comment_id":"637729"},{"comment_id":"620423","comments":[{"content":"Not necessarily true. The Parquet file can be batched any way you like with FireHose + Lambda.","comment_id":"624013","timestamp":"1656425520.0","upvote_count":"1","poster":"Ramshizzle"}],"timestamp":"1655901240.0","upvote_count":"1","poster":"GiveMeEz","content":"It's D. Ans A converts small senor data into many small Parquets in S3, which is not efficient."},{"timestamp":"1653131160.0","comment_id":"604835","poster":"Bik000","content":"Selected Answer: D\nMy Answer is D","upvote_count":"1"},{"poster":"GoKhe","comment_id":"534366","upvote_count":"2","timestamp":"1643343900.0","content":"A over D because Glue is more expensive than EMR. The question says \"The data platform team controls data processing and is concerned with downstream data processing efficiency and cost. \""},{"poster":"lakediver","comment_id":"506107","content":"I will go with D\n\"optimizing data processing efficiency\" by combining small files to larger Parquet Files\nPySpark code can be reused with Glue. Its Serverless so \"Well Architected\" also.","timestamp":"1640090940.0","comments":[{"content":"Changing Answer to A\nKDF can convert small files to larger Parquet Files. Its Serverless and will improve data processing efficiency\nThey can continue using Pyspark on EMR","timestamp":"1640091480.0","comment_id":"506111","poster":"lakediver","upvote_count":"2"}],"upvote_count":"1"},{"comment_id":"486268","content":"D is the ans","upvote_count":"1","poster":"aws2019","timestamp":"1637788440.0"},{"content":"I think it’s A because that solution replaces a custom Kinesis consumer application with a managed service: Kinesis Firehose (in addition to solving the other issues)","timestamp":"1636194780.0","comment_id":"440947","upvote_count":"1","poster":"iconara"},{"comment_id":"388419","content":"D is correct","poster":"gunjan4392","upvote_count":"1","timestamp":"1636084860.0"},{"content":"D is correct answer.","upvote_count":"1","poster":"Donell","timestamp":"1635760080.0","comment_id":"387886"},{"timestamp":"1635652140.0","comment_id":"298974","poster":"jAWStest","upvote_count":"3","content":"It clearly mentions data processing (\"improves the efficiency of the data processing jobs\") and not data ingestion, that is why I would focus on the solution that improves the processing part. Kinesis in this case is for ingestion, and A does not mean a significant change in processing, so I would go with D."},{"comment_id":"274374","content":"D is the right answer","timestamp":"1635618000.0","poster":"lostsoul07","upvote_count":"2"},{"comment_id":"268223","upvote_count":"1","timestamp":"1635380700.0","content":"For every small file Spark job has an overhead. FH can deliver max 1MB per file after archive which is small for Spark. Glue can use PySpark with extension. So, option D is meeting all the criteria.","poster":"Subho_in"},{"comment_id":"267056","upvote_count":"2","timestamp":"1635296700.0","poster":"kempstonjoystick","content":"Key words \"improve efficiency\" and \"well architected\" point to A over D. They both work, but A is cleaner and simpler."},{"timestamp":"1635260160.0","comment_id":"256221","upvote_count":"5","poster":"Manue","content":"It cannot be D, as it requires the small files to be previously ingested in S3 via Kinesis Data Streams (\"These messages are ingested using Amazon Kinesis Data Streams and sent to Amazon S3 using a Kinesis data stream consumer application\"). So D not only is not improving the ingestion process, but adding additional overhead to the process.\n\nOn the other hand, A improves ingestion/storage efficiency by levaraging Firehose. This is clearly an option which \"improves the efficiency of the data processing jobs and is well architected\" while \"continue to use PySpark\". So A is the right one.","comments":[{"content":"Question asks for which option improves the data processing. Clearly its D which improves the data processing. Option A improves the Ingestion/Collection mechanism.","upvote_count":"2","poster":"KrishnaKM","comment_id":"324575","timestamp":"1635728160.0"},{"comment_id":"620239","upvote_count":"1","timestamp":"1655884860.0","content":"I agree with Manue. D was my initial thought as well. But in this case it is \"better-architected\" if we merge and transform the small files before storing them and this is mentioned in A. Option D implies the small files are first stored, and then we read them again and merge + transform them.","poster":"Ramshizzle"}]},{"comment_id":"239006","comments":[{"comment_id":"239007","poster":"Draco31","timestamp":"1635115080.0","content":"Hum, changes my mind. I guess the better option on the full stream is to use Firehose to write to S3. Keeping KDS + Apps to push in S3 is quite not well architected because it can suffer better issue at scale","upvote_count":"1"}],"content":"I will go for D. A is a good choice but AWS is pushing customers to use Glue with Spark more and more on their aws blog and posts","upvote_count":"3","poster":"Draco31","timestamp":"1635095580.0"},{"timestamp":"1635016020.0","upvote_count":"2","comment_id":"224498","content":"it's A. As one message contain multiple small files, firehorse merge them into one bigger parquet file","poster":"passteque"},{"content":"D. A is using the same processing architecture which seems to be the concern in the question. D replaces EMR with Glue and pyspark is suppoorted in Glue along with Scala.","comment_id":"217605","upvote_count":"2","poster":"Skdbc","timestamp":"1634796960.0"},{"upvote_count":"1","content":"D for sure","comment_id":"214584","poster":"LMax","timestamp":"1634654820.0"},{"comment_id":"209971","upvote_count":"1","poster":"jove","content":"D is the correct answer","timestamp":"1634521200.0"},{"timestamp":"1634391480.0","comment_id":"205310","poster":"sanjaym","upvote_count":"1","content":"A and D both yield result. D should be answer from cost perspective ."},{"upvote_count":"1","poster":"metin","timestamp":"1634381160.0","comment_id":"196305","content":"In A, it doesn't mention merging small files into large files, so you they will end up with many small parquet files. So it is D, I think."},{"comment_id":"188799","upvote_count":"2","poster":"YoJesse","content":"I ll go with D, Glue will use the pyspark jobs.","timestamp":"1634184360.0"},{"timestamp":"1633720020.0","comments":[{"timestamp":"1633943340.0","content":"Remember objectives to be achieved are cost and efficiency For cost saving comparisons please see https://dzone.com/articles/how-to-be-a-hero-with-powerful-parquet-google-and The total storage size for data in that example is 1 TB in small csv files but translates to 130GB in parquet. During processing, query run time for the parquet files are 34 times faster, the only concern I have with this answer is that this is not a querying scenario, its simply a processing scenario. See https://developer.hpe.com/blog/Ql2DXNL4rmhWB8AEDMQz/tips-and-best-practices-to-take-advantage-of-spark-2x#:~:text=Apache%20Parquet%20gives%20the%20fastest%20read%20performance%20with%20Spark.&text=Parquet%20also%20stores%20column%20metadata,providing%20~%2010x%20faster%20read%20performance. parquet gives the fastest read performance for spark","comment_id":"185514","upvote_count":"1","poster":"GeeBeeEl"}],"poster":"GeeBeeEl","comment_id":"185466","upvote_count":"3","content":"I agree with A, not D because even though Glue is good for small files, parquet is the best way to ingest data from S3 if you are dealing with Spark. Its lightning fast and good. SageMaker is an example of where this is used. Please check https://towardsdatascience.com/getting-started-with-pyspark-on-amazon-emr-c85154b6b921"},{"comment_id":"185436","upvote_count":"2","content":"Between A and D it’s D as the question says they’d like to continue to use Pyspark","poster":"AjNapa","timestamp":"1633318260.0"},{"content":"A and D both seems a good choice. However I will go with D since I think it is better architected as for small files and also more cost effective since we don't need to run EMR clusters.","upvote_count":"4","timestamp":"1632799260.0","poster":"Paitan","comment_id":"175613"},{"poster":"ramozo","timestamp":"1632233040.0","content":"A or D? Both imply change in cost but both seems more efficient.","comments":[{"upvote_count":"11","comments":[{"content":"And D is the only awnser dealing with small files","comment_id":"173553","upvote_count":"4","poster":"carol1522","timestamp":"1632636900.0","comments":[{"timestamp":"1633643280.0","comment_id":"185457","poster":"GeeBeeEl","content":"They did not say they will be shutting down the EMR in D so how does it save costs?","upvote_count":"1"}]}],"timestamp":"1632279900.0","content":"i will go for D, It's more efficient to use Glue in this scenario rather than having a permanent EMR cluster just for that.","poster":"ali_baba_acs","comment_id":"167131"}],"upvote_count":"1","comment_id":"163451"}],"choices":{"D":"Set up AWS Glue Python jobs to merge the small data files in Amazon S3 into larger files and transform them to Apache Parquet format. Migrate the downstream PySpark jobs from Amazon EMR to AWS Glue.","B":"Set up an AWS Lambda function with a Python runtime environment. Process individual Kinesis data stream messages from the connected devices and sensors using Lambda.","A":"Send the sensor and devices data directly to a Kinesis Data Firehose delivery stream to send the data to Amazon S3 with Apache Parquet record format conversion enabled. Use Amazon EMR running PySpark to process the data in Amazon S3.","C":"Launch an Amazon Redshift cluster. Copy the collected data from Amazon S3 to Amazon Redshift and move the data processing jobs from Amazon EMR to Amazon Redshift."},"answer":"D","url":"https://www.examtopics.com/discussions/amazon/view/29252-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer_ET":"D","timestamp":"2020-08-22 10:33:00","question_images":[],"exam_id":20,"question_text":"A smart home automation company must efficiently ingest and process messages from various connected devices and sensors. The majority of these messages are comprised of a large number of small files. These messages are ingested using Amazon Kinesis Data Streams and sent to Amazon S3 using a Kinesis data stream consumer application. The Amazon S3 message data is then passed through a processing pipeline built on Amazon EMR running scheduled PySpark jobs.\nThe data platform team manages data processing and is concerned about the efficiency and cost of downstream data processing. They want to continue to use\nPySpark.\nWhich solution improves the efficiency of the data processing jobs and is well architected?","isMC":true}],"exam":{"numberOfQuestions":164,"isMCOnly":true,"id":20,"isImplemented":true,"provider":"Amazon","isBeta":false,"name":"AWS Certified Data Analytics - Specialty","lastUpdated":"11 Apr 2025"},"currentPage":25},"__N_SSP":true}