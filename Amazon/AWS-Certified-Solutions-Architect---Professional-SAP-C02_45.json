{"pageProps":{"questions":[{"id":"5ss3ZgSd1LzcqP5j6x0i","question_id":221,"unix_timestamp":1687338780,"question_text":"A company runs an ecommerce application in a single AWS Region. The application uses a five-node Amazon Aurora MySQL DB cluster to store information about customers and their recent orders. The DB cluster experiences a large number of write transactions throughout the day.\n\nThe company needs to replicate the data in the Aurora database to another Region to meet disaster recovery requirements. The company has an RPO of 1 hour.\n\nWhich solution will meet these requirements with the LOWEST cost?","answer":"C","answer_description":"","exam_id":33,"isMC":true,"discussion":[{"content":"Selected Answer: C\nGood luck for the exams. I know I'm gonna fail coz it takes me 3 hours just to read the questions. >:(","comments":[{"comment_id":"1173461","timestamp":"1710424800.0","content":"The trick is not to read the question first. Here is what I do:\n1. Read all options. \n2. Eliminate the incorrect ones, and settle on 1 or 2 options.\n3. Scroll through the question last.","upvote_count":"9","poster":"yog927"},{"poster":"yorkicurke","content":"any news about your exam?","timestamp":"1698081000.0","upvote_count":"2","comment_id":"1052108"},{"comment_id":"1139116","content":"I failed just cuz of time mngmnt ,now I'm here again to give myself one more \nchance,...:-(,.....\nAnyone here's to help me with exam by sharing their experience??","poster":"AMYMY","comments":[{"comment_id":"1141792","poster":"07c2d2a","upvote_count":"2","timestamp":"1707197700.0","content":"did you actually see any of the same questions on the test?"}],"timestamp":"1706951340.0","upvote_count":"1"}],"poster":"YodaMaster","upvote_count":"33","timestamp":"1688645100.0","comment_id":"944618"},{"poster":"SkyZeroZx","content":"if you got far it means you are persistent, good luck on your exam","comment_id":"941399","upvote_count":"19","timestamp":"1688353860.0"},{"upvote_count":"1","comment_id":"1559244","poster":"CAIYasia","content":"Selected Answer: C\nD is wrong. Aurora’s built-in automated backups are continuous but not customizable in frequency","timestamp":"1744201080.0"},{"timestamp":"1734704100.0","upvote_count":"1","comment_id":"1329497","content":"Selected Answer: A\nI don't know but isn't the exam supposed to be about being a professional architect? What kind of professional architect would recommend C knowing that a 100 thing can go wrong in such a solution!","poster":"ahhatem"},{"upvote_count":"1","comment_id":"1278783","content":"For those who are voting for D, here are a few simple facts:\n\n- You can’t disabled Aurora automated backup;\n- You can’t change Aurora backup frequency - it’s automated and point in time, you just need to define retention period which is the period for which you can perform a point-in-time recovery(1 day for free, max 35)","poster":"kgpoj","timestamp":"1725524220.0"},{"content":"Between A and C, I choose A.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database-getting-started.html#aurora-global-database-attaching\nFor C, note there's high volume of write daily. Can CDC to S3 bucket in another region keep up with 1 hour RPO? Although C has considered lowest cost for backup, does it consider the time taken to restore to full aurora database in the second region up and running?","timestamp":"1723382580.0","upvote_count":"1","comment_id":"1264171","poster":"Daniel76"},{"timestamp":"1723346160.0","upvote_count":"4","comment_id":"1263722","poster":"kgpoj","content":"Selected Answer: C\nAsk for 1 hour RPO, cheapest\n\nAurora Global can do 1 second RPO, but expensive\n\nDMS with CDC can do 1-hour RPO and cheaper"},{"content":"Selected Answer: C\nOption A will replicate \"the whole database\", we only need \"data\"","upvote_count":"2","poster":"trungtd","timestamp":"1718085780.0","comment_id":"1228279"},{"timestamp":"1714436280.0","upvote_count":"4","poster":"BrijMohan08","content":"Selected Answer: D\noption D provides the most cost-effective solution by leveraging Aurora backups with a 1-hour frequency and cross-Region replication to meet the disaster recovery requirements with the desired RPO.","comment_id":"1204246"},{"poster":"YOUSSEFSWAID","content":"Selected Answer: C\nRPO is a requirement and not RTO. They are talking about replicating the data in the Aurora database to another Region to meet disaster recovery requirements.","comment_id":"1203680","timestamp":"1714328400.0","upvote_count":"1"},{"timestamp":"1710860460.0","upvote_count":"5","content":"Selected Answer: A\nMy head hurts after the reading the last 2 questions and 45 mins later, still confuse. I am looking at the key requirements, RPO <1h, meet DR requirements, and LOWEST Cost. After reading the link below and all of the comments, I think Option A fulfil all the requirements in the question. \nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html","comment_id":"1177469","poster":"TonytheTiger"},{"comments":[{"poster":"sashenka","content":"RTO is not mentioned in the requirements so all we care about is the RPO of the data. We do not care about the format either so long as all the data is there as of an hour or less prior to the disaster.","timestamp":"1729654680.0","upvote_count":"1","comment_id":"1301856"}],"comment_id":"1174242","content":"Selected Answer: A\nGithub copilot\nWhile AWS Database Migration Service (DMS) can be used to replicate ongoing changes from the Aurora database to an Amazon S3 bucket in another region using a change data capture (CDC) task, it's important to note that DMS does not create a standard SQL dump or backup file that can be directly restored to an Aurora database.\nThe data migrated to S3 by DMS is in Apache Parquet format, a columnar storage file format optimized for speed and for a small footprint. This format is not directly restorable to an Aurora database.\nIf you need to restore the data to an Aurora database, you would need to use a service like AWS Glue or Amazon Athena to read the data from S3 and then insert it into Aurora. This process could be complex and time-consuming, and might not meet your RPO of 1 hour.","poster":"teo2157","timestamp":"1710503460.0","upvote_count":"6"},{"timestamp":"1710503340.0","content":"Selected Answer: A\nUse AWS Database Migration Service (AWS DMS). Create a DMS change data capture (CDC) task that replicates the ongoing changes from the Aurora database to an Amazon S3 bucket in another Region. Is it possible to restore data from the S3 bucket to an Aurora Database?","comment_id":"1174241","upvote_count":"1","poster":"teo2157"},{"content":"D:\n\nthe auto back can be disabled, see the link below:\nhttps://repost.aws/questions/QU1FrG5tQkQwi-yHbhT_EdvA/easily-turn-off-the-auto-backups-snapshots-for-rds","upvote_count":"1","comment_id":"1156646","timestamp":"1708630260.0","poster":"adelynllllllllll","comments":[{"comment_id":"1168642","timestamp":"1709885160.0","content":"The link is for RDS not Aurora.","poster":"thotwielder","upvote_count":"1"}]},{"poster":"dankositzke","comment_id":"1152715","timestamp":"1708190220.0","upvote_count":"2","content":"Selected Answer: C\nC is the least worst answer"},{"content":"Selected Answer: D\nnot C because.. DMS is not cheap and moreover, DMS S3 Target support either .csv or parquet format.. good luck with restoring this data into a database from s3\nthis is not a Disaster Recovery this is purely \"playing around with data in a Disaster situation\"","upvote_count":"4","comment_id":"1147619","poster":"Wardove","timestamp":"1707681120.0"},{"poster":"career360guru","upvote_count":"2","content":"Selected Answer: C\nAs there is no RTO C is best and most cost-effective.","comment_id":"1076936","timestamp":"1700632260.0"},{"comment_id":"1074608","timestamp":"1700396040.0","content":"Selected Answer: C\nwe assume that dms instance is deployed in a different region and somehow accesses the aurora instance, through the public endpoint or with vpc connection or any other way, and then replicates changes to the bucket in the same region it resides(different region for aurora)","upvote_count":"1","poster":"severlight"},{"content":"I thought it has 304 questions, how come there is no more next page?","upvote_count":"4","comment_id":"1046429","comments":[{"poster":"yorkicurke","comment_id":"1052113","timestamp":"1698081480.0","content":"i wonder myself","upvote_count":"2"}],"timestamp":"1697586420.0","poster":"KCjoe"},{"timestamp":"1694107680.0","content":"its can't be D \nYou can't disable automated backups on Aurora. The backup retention period for Aurora is managed by the DB cluster.\n\nso answer is C","comments":[{"comment_id":"1278782","poster":"kgpoj","content":"Clearly, a lot of ppl still don't know what Aurora automated backup means.\n\nThe backup retention period determines the period for which you can perform a point-in-time recovery.\nAurora offers 1-day backup retention for free!","upvote_count":"1","timestamp":"1725524040.0"},{"timestamp":"1714436400.0","poster":"BrijMohan08","upvote_count":"1","comment_id":"1204247","content":"You can set the retention period to 0 to disable the automated backup.","comments":[{"comment_id":"1278780","upvote_count":"1","content":"BrijMohan08 is wrong.\n\nThe smallest value you can set for automated backup retention period is 1 day.","poster":"kgpoj","timestamp":"1725523860.0"}]}],"poster":"kjcncjek","upvote_count":"5","comment_id":"1001788"},{"comments":[{"comment_id":"1157556","content":"Good point","poster":"dankositzke","upvote_count":"1","timestamp":"1708730280.0"}],"upvote_count":"2","timestamp":"1693314840.0","poster":"aviathor","comment_id":"993118","content":"Selected Answer: C\nAlthough I also lean towards C, the problem is that I think the solution is not complete with only the CDC. We would also need a backup from which to recover the databases before applying the changes."},{"timestamp":"1692668100.0","comments":[{"content":"Who said DMS had to be configured in the source region? Actually it is recommended to configure DMS in the target region. So DMS to S3 it is! C","timestamp":"1693314360.0","comment_id":"993106","poster":"aviathor","upvote_count":"1"}],"upvote_count":"3","comment_id":"986996","poster":"longngo0924","content":"Before considering the cost, please consider the ability of solution.\n\nB. Backtrack feature is mainly use for solved incorrect data or configuration but don't clone to new DB, just roll-back to a PITR.\n\nC. How can create a S3 as a target for DMS in other regions? It must be the same region with DMS.\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html#CHAP_Target.S3.Prerequisites\n\nD. Cannot turn off automatic backup of Aurora, the automatic backup range is 1 to 35 days. Disable require 0 day but don't have any option which is 0 day.\n\nSo, the answer A is reasonable."},{"timestamp":"1691803320.0","upvote_count":"1","comment_id":"979061","content":"Selected Answer: C\nlean to C, but C doesn't backup the full data?","poster":"softarts"},{"timestamp":"1690797240.0","upvote_count":"1","comments":[{"timestamp":"1691146320.0","upvote_count":"1","poster":"khksoma","content":"Prerequisites for using Amazon S3 as a target\nBefore using Amazon S3 as a target, check that the following are true:\n\nThe S3 bucket that you're using as a target is in the same AWS Region as the DMS replication instance you are using to migrate your data.","comments":[{"comment_id":"993108","content":"Yep, so you configure DMS in the target region.","timestamp":"1693314420.0","poster":"aviathor","upvote_count":"1"}],"comment_id":"972025"}],"comment_id":"967947","poster":"breadops","content":"Selected Answer: C\nNo RTO = C\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html"},{"upvote_count":"2","content":"C looks more like a workaround than an architected solution. Also, I don't know how to confirm RPO < 1 hour if data amount wasn't provided.\nVote: A","timestamp":"1690395600.0","poster":"MegalodonBolado","comment_id":"964092"},{"content":"I dont think it can be C. Check the pre-req for S3 to be the target in this link.\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html","poster":"khksoma","timestamp":"1690282920.0","upvote_count":"2","comment_id":"962608"},{"comment_id":"957919","content":"Correct A.\nWe have an RPO of 1 hour, which means you can tolerate losing up to 1 hour of data in case of a disaster. However, you also need to consider the cost and the recovery time objectives (RTO) of your solution. Using AWS DMS will cost more than using Aurora global database, and it will take longer to recover your data from S3 to a new database. Aurora global database will replicate your data to another Region with low latency, and it will allow you to fail over to the secondary DB cluster in minutes if the primary Region is unavailable. Therefore, Aurora global database is a better solution for your requirements.","upvote_count":"4","poster":"ggrodskiy","timestamp":"1689893760.0"},{"poster":"dkx","comment_id":"949721","content":"A. No, because this will create more database resources and the question is asking about 'replication of data', not 'replication of databases'\nB. No, because a daily copy does not meet the RPO of 1 hour requirement\nC. Yes, because using DMS CDC to replicate ongoing changes addresses the large number of write transactions throughout the day and the RPO of 1 hour requirement\nD. No, because this just writes the active DB cluster's logs to a different region on an hourly basis.\n\nNote, this question and answer choices are poorly worded.","timestamp":"1689162300.0","upvote_count":"4"},{"comment_id":"946587","poster":"NikkyDicky","content":"Selected Answer: C\nbest guess is C, since can't disable aurora backups and global DB is expensive","timestamp":"1688829540.0","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: C\nC would be a better, less costly option, given that there are no RTO requirements","comment_id":"941044","timestamp":"1688314080.0","poster":"NikkyDicky"},{"content":"Selected Answer: C\nCorrect is C. Is cheaper and we do not need a RTO, just RPO","upvote_count":"3","poster":"javitech83","comment_id":"939133","timestamp":"1688133120.0"},{"content":"Selected Answer: C\nC - lowest cost. Otherwise A.","comments":[{"content":"you are right, it does not state RTO so the cheaper is C","comment_id":"939131","upvote_count":"1","timestamp":"1688133060.0","poster":"javitech83"}],"comment_id":"935784","poster":"SmileyCloud","upvote_count":"2","timestamp":"1687894620.0"},{"timestamp":"1687890660.0","comment_id":"935714","content":"Selected Answer: A\nAAAAAAAAAAAAAAA","poster":"Jonalb","upvote_count":"3"},{"upvote_count":"1","content":"Why A is not the answer:\nGlobal Database cost a lot more, and offer almost RPO in SECONDS, not 1 hour, so A would not meeting the 1 hour RPO and low cost.\n\nC is the better choice.","comment_id":"934905","timestamp":"1687825440.0","poster":"nexus2020"},{"content":"question is asking which one is cheaper, and GLOBAL Database cost a lot more","upvote_count":"1","comment_id":"934889","poster":"nexus2020","timestamp":"1687824300.0"},{"comment_id":"934736","timestamp":"1687807320.0","comments":[{"poster":"easytoo","upvote_count":"1","comment_id":"934737","content":"AWS DMS provides a cost-effective solution for replicating data between different database engines and AWS Regions. By creating a DMS CDC task, you can capture the ongoing changes from the Aurora database and replicate them to an Amazon S3 bucket in another Region. This approach allows for near real-time replication with an RPO of 1 hour.\n\nCompared to the other options, using AWS DMS eliminates the need for maintaining a separate Aurora database or modifying the database configuration. It leverages the scalability and cost-effectiveness of Amazon S3 for storing the replicated data.\n\nPlease note that the specific implementation details and costs may vary based on the data volume, network bandwidth, and other factors. It's recommended to evaluate the requirements and consult the AWS documentation for detailed guidance on setting up and configuring AWS DMS.","timestamp":"1687807380.0"}],"content":"c-c-c-c-c-c-c","upvote_count":"1","poster":"easytoo"},{"upvote_count":"1","poster":"NETeng01","timestamp":"1687797300.0","content":"You can't disable automated backups on Aurora. The backup retention period for Aurora is managed by the DB cluster.","comment_id":"934631"},{"content":"Selected Answer: A\nAmazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. B: Backtrack is a feature that lets you quickly move your Aurora database back in time without needing to restore data from a backup. C: AWS DMS (Database Migration Service) is generally used for database migrations, not for continuous replication for disaster recovery purposes.","upvote_count":"3","poster":"SkyZeroZx","timestamp":"1687725420.0","comment_id":"933883"},{"comment_id":"933711","timestamp":"1687705560.0","content":"Selected Answer: C\nI vote for C. A appears to be a costly option for DR with a RPO of 1 hour","upvote_count":"2","poster":"Maria2023"},{"upvote_count":"2","poster":"gd1","timestamp":"1687628640.0","content":"Selected Answer: A\nAmazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. B: Backtrack is a feature that lets you quickly move your Aurora database back in time without needing to restore data from a backup. C: AWS DMS (Database Migration Service) is generally used for database migrations, not for continuous replication for disaster recovery purposes.","comment_id":"932822"},{"timestamp":"1687618440.0","upvote_count":"4","content":"Selected Answer: A\nAmazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages.\n\nThis setup would allow your application to meet the RPO of 1 hour because data written to the primary AWS region is replicated to the secondary region typically in less than a second.\n\nOther options like using AWS Database Migration Service (DMS) or configuring backups to another region or enabling backtrack would not be as efficient or cost-effective. DMS and Backtrack may add complexity and additional cost. Turning off automated Aurora backups and configuring them manually could potentially risk data loss if not managed properly.","comment_id":"932681","poster":"i_am_robot"},{"comments":[{"content":"Remember we are talking RPO, not RTO.","comments":[{"timestamp":"1687673460.0","poster":"PhuocT","comment_id":"933305","upvote_count":"1","content":"Thanks, so, what is your option? I still select option A, though.","comments":[{"poster":"Jackhemo","comments":[],"timestamp":"1687703520.0","upvote_count":"2","content":"C is the answer.","comment_id":"933686"}]}],"poster":"Jackhemo","upvote_count":"1","timestamp":"1687625280.0","comment_id":"932780"}],"content":"I think the correct answer is A.\nWhy C is not a good options although it has lowest cost. CDC to S3, will the time to apply that change, will not meet the RPO of 1hour when the incident happen.","timestamp":"1687606320.0","poster":"PhuocT","upvote_count":"1","comment_id":"932500"},{"upvote_count":"1","poster":"psyx21","comment_id":"929260","timestamp":"1687338780.0","content":"Selected Answer: C\nCorrect Answer is C"}],"answer_images":[],"answers_community":["C (65%)","A (27%)","9%"],"url":"https://www.examtopics.com/discussions/amazon/view/112795-exam-aws-certified-solutions-architect-professional-sap-c02/","choices":{"A":"Modify the Aurora database to be an Aurora global database. Create a second Aurora database in another Region.","C":"Use AWS Database Migration Service (AWS DMS). Create a DMS change data capture (CDC) task that replicates the ongoing changes from the Aurora database to an Amazon S3 bucket in another Region.","B":"Enable the Backtrack feature for the Aurora database. Create an AWS Lambda function that runs daily to copy the snapshots of the database to a backup Region.","D":"Turn off automated Aurora backups. Configure Aurora backups with a backup frequency of 1 hour. Specify another Region as the destination Region. Select the Aurora database as the resource assignment."},"answer_ET":"C","question_images":[],"timestamp":"2023-06-21 11:13:00","topic":"1"},{"id":"ghIdWmbovke4sy8tSgjT","answer":"D","question_id":222,"answer_ET":"D","answer_description":"","discussion":[{"comment_id":"1061056","poster":"Ustad","timestamp":"1730604780.0","content":"Selected Answer: D\nNo development effort needed so no need to migrate nonSQL or to Neptune. and no need to rework it based on lambda.","upvote_count":"5"},{"upvote_count":"4","content":"Selected Answer: D\nA: No guarantee that the work can finish within 15 minutes limit of Lambda\nB, C: Migrate MySQL to DynamoDB or Neptune? Big no no to migrate to different type of database unless the requirement says so.\nD: Classic architecture: ALB + ASG + EC2, scale based on CPU Utilization for cost optimization. The use of SSM to create AMI for launch template of ASG is correct. Aurora MySQL is compatible with current MySQL database.","timestamp":"1730494080.0","poster":"joleneinthebackyard","comment_id":"1060055"},{"upvote_count":"1","comment_id":"1059178","poster":"Mikado211","content":"Selected Answer: D\nA - you will spend some time to adapt an old platform to a lamdba function + the application works with mysql not with mongodb\nB - You do not want a smaller instance when you have a performance problem\nC - you will have to readapt a whole application to containerization on ECS which is not even the most flexible virtualization platform even if it theorically requires less maintenance\n\nD - The most classic way of migrating such application : you create a new platform, you make the application more scalable by using an ASG + you migrate your MySQL server from an overloaded EC2 instance to a managed service.","timestamp":"1730405880.0"},{"poster":"AM_aws","upvote_count":"1","timestamp":"1730120460.0","content":"Selected Answer: D\nWith least development time, from MySQL to Amazon Aurora MySQL.","comment_id":"1056133"},{"upvote_count":"2","poster":"airgead","content":"Answer: D \n Because MySQL Database will be compatible with Aurora MySQL \n A. Lambda is not the correct solution as it will be more development effort \n B. Changing to DynamoDB from MySQL (Relational Database) will be more development effort.\n C. More development effort to convert to Docker.","timestamp":"1730114700.0","comment_id":"1056081"},{"poster":"patryk99999","timestamp":"1730056860.0","upvote_count":"1","comment_id":"1055759","content":"Selected Answer: D\nI think D"}],"choices":{"A":"Move the application tier to AWS Lambda functions in the existing VPC. Create an Application Load Balancer to distribute traffic across the Lambda functions. Use Amazon GuardDuty to scan the Lambda functions. Migrate the database to Amazon DocumentDB (with MongoDB compatibility.","B":"Change the EC2 instance type to a smaller Graviton powered instance type. Use the existing AMI to create a launch template for an Auto Scaling group. Create an Application Load Balancer to distribute traffic across the instances in the Auto Scaling group. Set the Auto Scaling group to scale based on CPU utilization. Migrate the database to Amazon DynamoDB.","D":"Create a now AMI that is configured with AWS Systems Manager Agent (SSM Agent). Use the new AMI to create a launch template for an Auto Scaling group. Use smaller instances in the Auto Scaling group. Create an Application Load Balancer to distribute traffic across the instances in the Auto Scaling group. Set the Auto Scaling group to scale based on CPU utilization. Migrate the database to Amazon Aurora MySQL.","C":"Move the application tier to containers by using Docker. Run the containers on Amazon Elastic Container Service (Amazon ECS) with EC2 instances. Create an Application Load Balancer to distribute traffic across the ECS cluster. Configure the ECS cluster to scale based on CPU utilization. Migrate the database to Amazon Neptune."},"answer_images":[],"timestamp":"2023-10-27 19:21:00","topic":"1","answers_community":["D (100%)"],"question_text":"A company's solutions architect is evaluating an AWS workload that was deployed several years ago. The application tier is stateless and runs on a single large Amazon EC2 instance that was launched from an AMI. The application stores data in a MySQL database that runs on a single EC2 instance.\n\nThe CPU utilization on the application server EC2 instance often reaches 100% and causes the application to stop responding. The company manually installs patches on the instances. Patching has caused downtime in the past. The company needs to make the application highly available.\n\nWhich solution will meet these requirements with the LEAST development me?","isMC":true,"unix_timestamp":1698427260,"url":"https://www.examtopics.com/discussions/amazon/view/124742-exam-aws-certified-solutions-architect-professional-sap-c02/","exam_id":33,"question_images":[]},{"id":"ycP3qrcsa2ZJoGZ8aUv8","answer_description":"","answers_community":["D (80%)","B (15%)","5%"],"answer_images":[],"answer_ET":"D","topic":"1","timestamp":"2022-12-09 16:57:00","isMC":true,"discussion":[{"timestamp":"1670601420.0","content":"Right answer is D.\nAn SCP at a lower level can't add a permission after it is blocked by an SCP at a higher level. SCPs can only filter; they never add permissions.\nSO you need to create a new OU for the new account assign an SCP, and move the root SCP to Production OU. Then move the new account to production OU when AWS config is done.","comment_id":"740240","upvote_count":"49","poster":"Snip"},{"comment_id":"741380","content":"Answer: D.\n\nNot A: too much overhead and maintenance.\nNot B: SCP at Root will still deny Config to the temporary OU.\nNot C: Too much overhead to create allow list.","poster":"robertohyena","timestamp":"1670726700.0","upvote_count":"19"},{"upvote_count":"1","poster":"Cpso","timestamp":"1734665040.0","content":"Selected Answer: C\nthe question test knowledge about allow /denied inherit. all ou under root with 'deny' can't allow. So B is fail. C , D is correct but D is less operation.","comment_id":"1329265"},{"poster":"hspc_","content":"Selected Answer: D\nFor a permission to be allowed for a specific account, there must be an explicit Allow statement at every level from the root through each OU in the direct path to the account (including the target account itself).\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_evaluation.html","upvote_count":"1","timestamp":"1733737920.0","comment_id":"1323964"},{"comments":[{"comment_id":"774675","timestamp":"1673628180.0","poster":"masetromain","upvote_count":"1","content":"This approach is the correct solution because it allows the new account to make necessary adjustments to AWS Config while still adhering to the company's policies, and it does not introduce additional long-term maintenance. The new account will be only in the new OU temporarily, and the SCP blocking AWS Config actions will only be in the root temporarily."}],"content":"Yes, in option D, the solution is to create a temporary OU named Onboarding for the new account. By creating a new OU for the new account, it allows for a new set of permissions and policies to be applied to this account, separate from the existing Production OU.\n\nOnce the new OU is created, an SCP is applied to it to allow AWS Config actions. This SCP allows the new account to make necessary adjustments to AWS Config without being blocked by the existing policies at the root level of the organization.\n\nThen, the root SCP that is blocking these actions is moved to the Production OU, where it will continue to block these actions for all other accounts that are members of the Production OU.\n\nFinally, once the necessary adjustments are made, the new account can be moved to the Production OU, where it will be subject to the existing policies and restrictions.","timestamp":"1727061180.0","poster":"masetromain","comment_id":"774674","upvote_count":"4"},{"timestamp":"1727061180.0","upvote_count":"1","content":"The best option to allow administrators to make changes and continue to enforce the current policies without introducing additional long-term maintenance would be option D:\n\nD. Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the organization’s root SCP to the Production OU. Move the new account to the Production OU when adjustments to AWS Config are complete.\n\nThis solution involves creating a temporary OU named Onboarding for the new account and applying an SCP to the Onboarding OU that allows AWS Config actions. The organization's root SCP should be moved to the Production OU, and the new account should be moved to the Production OU when the adjustments to AWS Config are complete. This approach allows the administrators of the new account to make changes to AWS Config rules while maintaining the current policies in the Production OU.","comment_id":"809117","poster":"c73bf38"},{"comment_id":"827086","timestamp":"1727061180.0","poster":"Ajani","content":"Please note Question Constraint: Which option will allow administrators to make changes and continue to enforce the current policies without introducing additional long-term maintenance?\nStrategies for using SCPs\nYou can configure the service control policies (SCPs) in your organization to work as either of the following:\nA deny list – actions are allowed by default, and you specify what services and actions are prohibited\nAn allow list – actions are prohibited by default, and you specify what services and actions are allowed.","upvote_count":"1"},{"content":"Selected Answer: B\nD: is not correct, because removing the root SCPs on the production OU means removing all the security rules on the services preventing changes, including changes to the AWS Config rules. and depending on the scenario this will be a security hole for production.\n\nDon't forget that the aim is to introduce the new AWS account into the Production OU with the same configurations and restrictions as the accounts that are already there.\n\nSo thanks to the temporary OU on which we have an SCP that authorises actions on AWS Config, we just need to modify the configuration of the new account so that it matches the production requirements. Once the configuration requirements have been met, we move the new account into the production OU.","comments":[{"timestamp":"1693476300.0","content":"\" All accounts are members of the Production OU\", therefore we don't need the SCP in root.","poster":"victorHugo","upvote_count":"5","comment_id":"994988"}],"comment_id":"966400","poster":"sebnzogang","upvote_count":"6","timestamp":"1727061180.0"},{"comment_id":"990321","poster":"dimitry_khan_arc","upvote_count":"3","timestamp":"1727061180.0","content":"Selected Answer: D\nChosen D.\nB is not correct because root having explicit deny will override any explicit allow in its child OU even if allowance is given. Unless I keep Onboarding account under a parent where there is not explicit deny for Config service, Onboarding account can not configure. So, need to move the explicit deny from root account to production account and then keep onboarding account under root."},{"content":"This question is ambiguous. If D was formulated like this:\n\n\"D. Create a temporary OU named Onboarding for the new account. Apply a Config non-blocking SCP to the Onboarding OU to allow AWS Config actions. Apply the organization’s root SCP to the Production OU instead of to the root OU. Move the new account to the Production OU when adjustments to AWS Config are complete.\"\n\nThen D would be a viable option. However, it isn't, and even if it were, it fails to mention the crucial fact that the Root OU always must have an SCP, which in this case must Allow everything. For someone with some experience this is a given, but as it isn't mentioned, I'd go for B.\n\nHowever, AWS should reformulate the question and the answers. They are really subpar.","poster":"Dgix","upvote_count":"2","timestamp":"1727061120.0","comments":[{"comment_id":"1167039","comments":[{"content":"This sentence:\n\n\"Apply the organization’s root SCP to the Production OU instead of to the root OU.\"\n\nsolves the issue you mentioned. You can safely move this SCP as the question states that all AWS accounts are in Production OU.","upvote_count":"1","timestamp":"1714043820.0","comment_id":"1201927","poster":"fartosh"}],"upvote_count":"3","content":"AWS Config will still be restricted despite the Allow SCP in Onboarding because of the Deny SCP in the root of the organization","timestamp":"1709717580.0","poster":"JOKERO"}],"comment_id":"1163464"},{"comment_id":"1158230","content":"I don't like any of the answers to be honest. Let's look at D since that's the one most people think is right. The problem with D is that you can't detach the last SCP associated with a root container, OU, or account. There has to be at least one. So, removing the SCP from the root and moving it down to the Production OU is a no-go unless you add a permissable SCP to the root. Check the section on detaching here: https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_attach.html\n\nThe only way B is correct is if the reason the new admins don't have access to Config is not because Config is in the Deny List, but because the management account doesn't have the appropriate IAM Policy giving PERMISSION to Config. You need both an IAM Policy and a permissable SCP to have permission and access to a service. But, why wasn't IAM Policy mentioned in choice B. Clearly, without that information, choice B also is not right.","comments":[{"comments":[{"comment_id":"1359602","timestamp":"1740105780.0","poster":"soulation","upvote_count":"1","content":"Before Onboarding OU, this organization only had 1 OU (Production). So moving the SCP from root to Production OU doesn't affect other existing accounts."}],"poster":"awsylum","timestamp":"1708819860.0","comment_id":"1158232","upvote_count":"1","content":"Also, even if you could remove a root SCP, you would never do that in production. You would never just flat remove an SCP with a Deny list just to give one account access to some service. Even if it's temporary, that's a fatal mistake as the other accounts will not be restricted from certain services they shouldn't have access to."},{"comment_id":"1158233","upvote_count":"1","timestamp":"1708820220.0","content":"The question mentioned a Deny List architecture, but it didn't specifically say Config was in the Deny List. We are assuming that, which could lead to the wrong answer. Unfortunately, I'm not satisfied with any of the answers. Hopefully, this is a question that would be thrown out from the exam. LOL.","poster":"awsylum"}],"upvote_count":"1","poster":"awsylum","timestamp":"1727061120.0"},{"upvote_count":"2","comment_id":"1091702","poster":"ninomfr64","timestamp":"1727061120.0","content":"Selected Answer: D\nThis was not easy for me due to wording, however here is my take:\n\nNot A. here we permanently remove SCPs that limit access to AWS Config, while we are requested to continue to enforce the current policies\nNot B. temporary OU and related SCP that allows AWS Config are nested under root where SCPs that limit access to AWS Config are applied. As SCP can only remove permission and not add, this will not work\nNot C. converting deny list into allow list here is not beneficial also temporarily apply SCP allowing AWS Config does not meet the request to avoid additional long-term maintenance.\n\nThus D does the job."},{"poster":"atirado","content":"Selected Answer: C\nOption A - This option actually rolls out AWS Config across the company which is exactly the opposite of what they are doing\n\nOption B - This option does not work because AWS Config will still be restricted despite the Allow SCP in Onboarding because of the Deny SCP in the root of the organization\n\nOption C - This option allows access to AWS Config in the new business unit and restricts access to everything else. However, the SCP will require regular updates to add new AWS services\n\nOption D - This option applies the correct level of access to each OU without needing updates: Onboarding gets access to AWS Config, Production does not and FullAWSAccess is established at the root after the company's Deny SCP is moved.","upvote_count":"1","timestamp":"1727061120.0","comment_id":"1100045"},{"comment_id":"1136887","upvote_count":"1","poster":"DmitriKonnovNN","timestamp":"1727061120.0","content":"The question itself is a bit confusing. It says \"Deny List in the root\", which should be understood as Deny List Architecture, but can be misinterpreted as \"Allow List Architecture with attached Deny List in the root that explicitly deny AWS Config\". Since AWS Config on Production OU is denied, an appropriate SCP is attached to it, which explicitly denies AWS Config. Thus, the root has FullAWSAccess SCP attached to it. That's why we just need to create Onboarding OU with no explicit deny of AWS Config and that's it. So the correct answer is truly correct, but the question is a bit tricky and easy to misunderstand."},{"comment_id":"1205673","upvote_count":"3","poster":"Mikep12357","timestamp":"1727061120.0","content":"Option B. \nIf a \"Deny\" list SCP is applied at the root of the organization to restrict access to a service, and then a new SCP is created at a lower level (e.g., an Organizational Unit or OU) to \"Allow\" access to that restricted service, the permissions are cumulative.\n\nSo, if an account is placed under the Test OU, it will inherit the permissions from both SCPs. Since the \"Allow\" SCP at the Test OU level overrides the \"Deny\" SCP at the root level, the account under the Test OU will effectively have access to the restricted service.\n\nThis is because SCPs are evaluated hierarchically, with SCPs at higher levels in the organizational structure being evaluated first, followed by SCPs at lower levels. When there are conflicting SCPs, the most permissive policy (i.e., the one that allows access) takes precedence."},{"timestamp":"1726634340.0","upvote_count":"1","comment_id":"1285578","content":"Selected Answer: D\nD is the best match !","poster":"pravinb"},{"content":"D. Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the organization’s root SCP to the Production OU. Move the new account to the Production OU when adjustments to AWS Config are complete.","comment_id":"1275430","timestamp":"1725090000.0","upvote_count":"1","poster":"amministrazione"},{"poster":"niroop893","content":"Answer: D","comment_id":"1259669","upvote_count":"1","timestamp":"1722574380.0"},{"upvote_count":"2","comment_id":"1187459","timestamp":"1711977720.0","content":"Selected Answer: D\nOption D: The link doesn't give you a full explanation on why \"D\" is correct however it does check all the boxes \n \nhttps://docs.aws.amazon.com/whitepapers/latest/organizing-your-aws-environment/transitional-ou.html","poster":"TonytheTiger"},{"timestamp":"1705997280.0","comment_id":"1129301","poster":"kobi44","content":"option D - how creating new OU will solve the problem? the root SCP will deny it , isnt? \nalso why do we need to Move the organization’s root SCP to the Production OU ?","upvote_count":"1"},{"timestamp":"1704259740.0","poster":"GabrielShiao","upvote_count":"1","content":"Answer D is the answer most accurate. it would be good that add another statement to say\" Add awsFullAccess SCP policy on the root and move the deny list scp policy from root to production OU\"","comment_id":"1112517"},{"poster":"cgsoft","upvote_count":"1","comment_id":"1094141","timestamp":"1702358460.0","content":"Selected Answer: D\nSCP at root must be moved to Production OU to prevent it from being applied to onboarded account."},{"comment_id":"1080901","content":"D is correct","upvote_count":"1","timestamp":"1701019680.0","poster":"abeb"},{"poster":"swadeey","timestamp":"1700665560.0","comments":[{"timestamp":"1740105840.0","upvote_count":"1","poster":"soulation","content":"The question said there's only 1 OU.","comment_id":"1359603"}],"upvote_count":"2","comment_id":"1077467","content":"The Root is not an OU. It is a container for the management account, and for all OUs and accounts in your organization. Conceptually, the Root contains all of the OUs. It cannot be deleted. You cannot govern enrolled accounts at the Root level within AWS Control Tower. Instead, govern enrolled accounts within your OUs. The SCP don't apply at root OU. This will impact production as when you move SCP from root to Production you are changing SCP for all OU which are part of it. Will customer allow to change existing production SCP to on board a new. I don't think D is correct"},{"upvote_count":"1","content":"B is horribly wrong. Correct answer must be D.","timestamp":"1700635140.0","poster":"jainparag1","comment_id":"1077015"},{"content":"Selected Answer: D\nneed to get rid of deny in root scp","poster":"severlight","timestamp":"1699691820.0","comment_id":"1067714","upvote_count":"2"},{"timestamp":"1698230820.0","comment_id":"1053640","poster":"Sandeep_B","upvote_count":"2","content":"Option D looks to be correct answer. Can anyone please confirm if you have got this question in the exam and cleared it.."},{"upvote_count":"2","timestamp":"1695811980.0","content":"Selected Answer: D\nD.\nAn SCP at a lower level can't add a permission after it is blocked by an SCP","comment_id":"1018721","poster":"ansgohar"},{"upvote_count":"3","content":"So which is the correct answer B or D? Why is the portal saying it as \"B\" though many of them think it is D?","poster":"autobahn","comment_id":"988861","timestamp":"1692855420.0"},{"comment_id":"983329","timestamp":"1692257460.0","poster":"technosavvy","upvote_count":"2","content":"Option D: This option would allow administrators to make changes to AWS Config rules for the new account, but it would also move the SCPs that limit access to other restricted services to the Production OU. This could create security risks for the other accounts in the organization."},{"poster":"Karamen","upvote_count":"2","content":"The right answer is D","timestamp":"1691323500.0","comment_id":"973797"},{"timestamp":"1691049840.0","upvote_count":"2","content":"I'm thinking it is B because in D, it says move the organization's SCP to Production OU.. First of all why is this extra step needed? After configuring the Onboarding Account, all that needs to happen is to move that account under Production OU. Production Account's SCP should stay as is. That's my opinion. SO, B seems to be more straightforward solution.","comment_id":"970887","poster":"autobahn"},{"comment_id":"964416","upvote_count":"2","content":"Selected Answer: D\nD is the only one that has: \"Move the organization’s root SCP to the Production OU\"\nAn SCP at a lower level can't add a permission after it is blocked by an SCP at a higher level.","poster":"chico2023","timestamp":"1690438080.0"},{"upvote_count":"2","comment_id":"934675","poster":"NikkyDicky","timestamp":"1687800600.0","content":"Selected Answer: D\nit's D"},{"comment_id":"927294","upvote_count":"2","poster":"Jonalb","timestamp":"1687162440.0","content":"Selected Answer: D\nExplanation:\n\nBy creating a temporary OU named Onboarding for the new account, the company can isolate the new account and make the necessary adjustments without affecting the existing accounts.\nApplying an SCP to the Onboarding OU that allows AWS Config actions will grant the administrators of the new business unit the required permissions to update existing AWS Config rules.\nMoving the organization's root SCP to the Production OU ensures that the existing policies and restrictions are still enforced for the rest of the accounts within the organization.\nOnce the adjustments to AWS Config are complete and the new account is aligned with the company's policies, the new account can be moved to the Production OU, integrating it into the existing account structure and applying the same policies."},{"comment_id":"922330","upvote_count":"1","timestamp":"1686669300.0","poster":"bhanus","content":"The question NOWHERE talks about shared services VPC. Not sure if its missing here. D is the answer. A is also correct but its time taking as association of R53 zone for all the VPCs is time consuming. Imagine in future VPCs grow in number and you need to make sure R53 zone is associated with all VPCs which is time consuming. D makes it easy by associating to shared services VPC's once"},{"content":"Selected Answer: D\nroot scp should to be move to production to let the onboarding OU the time to enforce the security","upvote_count":"1","poster":"RunkieMax","timestamp":"1683909960.0","comment_id":"896073"},{"poster":"Limlimwdwd","content":"Selected Answer: D\nRoot account deny control will supersede all the allow in the OU.. only way to workaround is move it to prod to keep the control measure","timestamp":"1683805800.0","upvote_count":"1","comment_id":"895017"},{"poster":"Anonymous9999","content":"Selected Answer: D\nFrom https://us-west-2.console.aws.amazon.com/vpc/home?region=us-west-2#subnets:\n\nAny account has only those permissions permitted by every parent above it. If a permission is blocked at any level above the account, either implicitly (by not being included in an Allow policy statement) or explicitly (by being included in a Deny policy statement), a user or role in the affected account can't use that permission\n\nThus it cannot be B","timestamp":"1681743600.0","comment_id":"872834","upvote_count":"1"},{"timestamp":"1679976960.0","comment_id":"852756","poster":"mfsec","upvote_count":"1","content":"Selected Answer: D\nD is correct"},{"poster":"dev112233xx","upvote_count":"2","comment_id":"852466","content":"Selected Answer: D\nD is the correct answer. Explicit Deny on root can’t be bypassed by just adding “allow” in the OU SCP","timestamp":"1679949720.0"},{"comment_id":"831697","upvote_count":"1","timestamp":"1678181040.0","content":"Selected Answer: D\nenforce the current policies without introducing additional long-term maintenance -> requires organisaiton SCP to move to Production OU to avoid such issues in future","poster":"kiran15789"},{"poster":"dev112233xx","upvote_count":"1","timestamp":"1677964680.0","content":"Selected Answer: D\nD is 100% the correct answer.\nexplicit deny in the root SCP can’t be bypassed even which explicit allow..","comment_id":"829420"},{"timestamp":"1677773520.0","comment_id":"827090","content":"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_strategies.html","poster":"Ajani","upvote_count":"1"},{"comment_id":"826510","timestamp":"1677733860.0","upvote_count":"1","content":"Selected Answer: D\nSCP at root level is the root cause of the new not working and Answer D is right fit for it","poster":"gameoflove"},{"poster":"promartyr","upvote_count":"2","comment_id":"811330","content":"When they say \"Move the organization’s root SCP to the Production OU\" - where is it moving from? Isn't there only one OU?","comments":[{"comment_id":"823052","content":"from Onboarding OU to Production OU?","timestamp":"1677454680.0","upvote_count":"1","poster":"kamonegi"},{"upvote_count":"2","comment_id":"870529","timestamp":"1681515540.0","poster":"Sarutobi","content":"From the root of the AWS Organization to the Production OU, that is one level below. So the Organization is the root, and Production and Onboarding OU are the branches."}],"timestamp":"1676602320.0"},{"poster":"Musk","comment_id":"806083","timestamp":"1676188680.0","content":"D makes sense but there is something that does not: \"Apply an SCP to the Onboarding OU to allow AWS Config actions.\" SCPs never allow. I think it mkes D incorrect.","upvote_count":"2"},{"comment_id":"804665","timestamp":"1676056320.0","poster":"Sarutobi","content":"Selected Answer: D\nD is correct.","upvote_count":"2"},{"comment_id":"780429","timestamp":"1674075900.0","poster":"skashanali","content":"Right answer is D\n\nAs permission are inherited from root, they have to remove the SCP from root and apply on Production OU..\nAlso allow SCP related to AWS config for onboarding temp OU and revert the changes.","upvote_count":"2"},{"content":"Selected Answer: D\nThe correct answer is D for me","comment_id":"741122","upvote_count":"2","timestamp":"1670691420.0","poster":"masetromain"}],"question_id":223,"choices":{"A":"Remove the organization’s root SCPs that limit access to AWS Config. Create AWS Service Catalog products for the company’s standard AWS Config rules and deploy them throughout the organization, including the new account.","B":"Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the new account to the Production OU when adjustments to AWS Config are complete.","D":"Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the organization’s root SCP to the Production OU. Move the new account to the Production OU when adjustments to AWS Config are complete.","C":"Convert the organization’s root SCPs from deny list SCPs to allow list SCPs to allow the required services only. Temporarily apply an SCP to the organization’s root that allows AWS Config actions for principals only in the new account."},"exam_id":33,"question_text":"A company uses AWS Organizations with a single OU named Production to manage multiple accounts. All accounts are members of the Production OU. Administrators use deny list SCPs in the root of the organization to manage access to restricted services.\nThe company recently acquired a new business unit and invited the new unit’s existing AWS account to the organization. Once onboarded, the administrators of the new business unit discovered that they are not able to update existing AWS Config rules to meet the company’s policies.\nWhich option will allow administrators to make changes and continue to enforce the current policies without introducing additional long-term maintenance?","unix_timestamp":1670601420,"url":"https://www.examtopics.com/discussions/amazon/view/90824-exam-aws-certified-solutions-architect-professional-sap-c02/","answer":"D","question_images":[]},{"id":"WdhCgudHJdUgpD7egn6G","url":"https://www.examtopics.com/discussions/amazon/view/91458-exam-aws-certified-solutions-architect-professional-sap-c02/","question_id":224,"question_text":"A company has 50 AWS accounts that are members of an organization in AWS Organizations. Each account contains multiple VPCs. The company wants to use AWS Transit Gateway to establish connectivity between the VPCs in each member account. Each time a new member account is created, the company wants to automate the process of creating a new VPC and a transit gateway attachment.\nWhich combination of steps will meet these requirements? (Choose two.)","answer_ET":"AC","unix_timestamp":1670946180,"topic":"1","answer":"AC","answers_community":["AC (100%)"],"answer_images":[],"discussion":[{"upvote_count":"23","comment_id":"774821","content":"Selected Answer: AC\nOption A is sharing the transit gateway with member accounts by using AWS Resource Access Manager, which allows the management account to share resources with member accounts. Option C is launching an AWS CloudFormation stack set from the management account that automatically creates a new VPC and a VPC transit gateway attachment in a member account, and associates the attachment with the transit gateway in the management account by using the transit gateway ID. This automation of creating a new VPC and transit gateway attachment in new member accounts can help to streamline the process and reduce operational effort.","poster":"masetromain","timestamp":"1673641020.0","comments":[{"content":"Precisely!","poster":"jainparag1","comment_id":"1079038","upvote_count":"1","timestamp":"1700801700.0"}]},{"timestamp":"1732898640.0","content":"Selected Answer: AC\nA. From the management account, share the transit gateway with member accounts by using AWS Resource Access Manager. Most Voted\nC. Launch an AWS CloudFormation stack set from the management account that automatically creates a new VPC and a VPC transit gateway attachment in a member account. Associate the attachment with the transit gateway in the management account by using the transit gateway ID. \nYou want to associate the gateway attachment with the transit gateway that you already shared using RAM","upvote_count":"1","comment_id":"1319808","poster":"Tiger4Code"},{"comment_id":"1275472","timestamp":"1725093120.0","poster":"amministrazione","upvote_count":"1","content":"A. From the management account, share the transit gateway with member accounts by using AWS Resource Access Manager.\nC. Launch an AWS CloudFormation stack set from the management account that automatically creates a new VPC and a VPC transit gateway attachment in a member account. Associate the attachment with the transit gateway in the management account by using the transit gateway ID."},{"content":"Selected Answer: AC\nAC are correct","comment_id":"1175359","upvote_count":"1","timestamp":"1710626940.0","poster":"gofavad926"},{"content":"Selected Answer: AC\nI am working on a project doing the exact same thing :D","comment_id":"1089554","upvote_count":"2","poster":"[Removed]","timestamp":"1701881580.0"},{"poster":"rlf","content":"AC. \nhttps://aws.amazon.com/ko/blogs/networking-and-content-delivery/automating-aws-transit-gateway-attachments-to-a-transit-gateway-in-a-central-account/\nhttps://cloudjourney.medium.com/aws-ram-and-transit-gateway-8ac230f298e8","comment_id":"1027749","timestamp":"1696744680.0","upvote_count":"1"},{"comment_id":"998285","upvote_count":"1","timestamp":"1693811820.0","content":"Selected Answer: AC\nYou can use AWS Resource Access Manager (RAM) to share a transit gateway for VPC attachments across accounts or across your organization in AWS Organizations.","poster":"Simon523"},{"comment_id":"937147","upvote_count":"1","timestamp":"1687987200.0","content":"AC of course","poster":"NikkyDicky"},{"timestamp":"1679981520.0","upvote_count":"2","content":"Selected Answer: AC\nAC are my choice.","comment_id":"852822","poster":"mfsec"},{"comment_id":"792941","poster":"zozza2023","upvote_count":"2","timestamp":"1675090440.0","content":"Selected Answer: AC\nA and C are the answer for me"},{"comment_id":"760904","content":"Selected Answer: AC\nA & C\nhttps://docs.aws.amazon.com/vpc/latest/tgw/tgw-transit-gateways.html\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ec2-transitgatewayattachment.html","poster":"Untamables","timestamp":"1672311060.0","upvote_count":"2"},{"content":"Selected Answer: AC\nhttps://www.examtopics.com/discussions/amazon/view/60090-exam-aws-certified-solutions-architect-professional-topic-1/","timestamp":"1670946180.0","comment_id":"744223","upvote_count":"3","poster":"masetromain"}],"timestamp":"2022-12-13 16:43:00","answer_description":"","exam_id":33,"isMC":true,"question_images":[],"choices":{"D":"Launch an AWS CloudFormation stack set from the management account that automatically creates a new VPC and a peering transit gateway attachment in a member account. Share the attachment with the transit gateway in the management account by using a transit gateway service-linked role.","C":"Launch an AWS CloudFormation stack set from the management account that automatically creates a new VPC and a VPC transit gateway attachment in a member account. Associate the attachment with the transit gateway in the management account by using the transit gateway ID.","A":"From the management account, share the transit gateway with member accounts by using AWS Resource Access Manager.","B":"From the management account, share the transit gateway with member accounts by using an AWS Organizations SCP.","E":"From the management account, share the transit gateway with member accounts by using AWS Service Catalog."}},{"id":"HfQXgx2ONkHUruHcW0an","answer_images":[],"question_images":[],"question_text":"A company is planning to migrate several applications to AWS. The company does not have a good understanding of its entire application estate. The estate consists of a mixture of physical machines and VMs.\n\nOne application that the company will migrate has many dependencies that are sensitive to latency. The company is unsure what all the dependencies are. However the company knows that the low-latency communications use a custom IP-based protocol that runs on port 1000. The company wants to migrate the application and these dependencies together to move all the low-latency interfaces to AWS at the same time.\n\nThe company has installed the AWS Application Discovery Agent and has been collecting data for several months.\n\nWhat should the company do to identify the dependencies that need to be migrated in the same phase as the application?","answer":"A","question_id":225,"topic":"1","answers_community":["A (93%)","7%"],"isMC":true,"exam_id":33,"answer_description":"","timestamp":"2023-10-28 11:31:00","discussion":[{"timestamp":"1698632700.0","upvote_count":"15","poster":"Sab","comment_id":"1057297","content":"Selected Answer: A\nAnswer A . Network access analyzer is to valid network usage OF aws services and not on-prem\n\nMigration hub has feature for network visualization and Athena can be used to query data\n\nhttps://aws.amazon.com/blogs/mt/using-aws-migration-hub-network-visualization-to-overcome-application-and-server-dependency-challenges/\n\nhttps://aws.amazon.com/about-aws/whats-new/2020/11/aws-migration-hub-includes-network-visualization/"},{"timestamp":"1698872520.0","comment_id":"1060064","content":"Selected Answer: A\nArchitecture pattern is Discovery Service + Migration Hub + Athena for data exploration:\nhttps://docs.aws.amazon.com/application-discovery/latest/userguide/explore-data.html\n\nA: looks fine \nB: AWS Application Migration Service is for lift and shift, not for dependency mapping\nC: Network Access Analyzer only for AWS resource, not for on prem\nD: not the use case of CloudWatch.","poster":"joleneinthebackyard","upvote_count":"7"},{"poster":"AzureDP900","timestamp":"1732491840.0","content":"A \n• AWS Migration Hub allows you to collect and analyze migration-related data, such as network traffic and server dependencies.\n• By selecting the servers that host the application and visualizing the network graph, you can identify other servers that interact with the application.\n• Turning on data exploration in Amazon Athena allows you to query the collected data and gain insights into the interactions between servers.\n• Querying the data for port 1000 will help you identify servers that communicate using this custom protocol.\n• Returning to Migration Hub and creating a move group based on the findings from the Athena queries ensures that you're moving all relevant dependencies together with the application.\nThis approach uses AWS Migration Hub's capabilities to collect, analyze, and visualize migration-related data, making it an efficient and effective way to identify dependencies for migration.\n`","upvote_count":"1","comment_id":"1317248"},{"upvote_count":"1","content":"Selected Answer: C\nC seems to be more suitable","poster":"Syre","comment_id":"1281187","timestamp":"1725906360.0"},{"comment_id":"1222882","timestamp":"1717272420.0","comments":[{"comment_id":"1269936","upvote_count":"2","content":"just A","poster":"helloworldabc","timestamp":"1724225700.0"}],"content":"Option C - dentifying Servers Communicating on Port 1000: In the Network Access Analyzer console, you can select the servers that host the application and specify a Network Access Scope of port 1000. This will allow you to identify the servers that communicate with the application using the custom IP-based protocol on port 1000, which are the dependencies that need to be migrated together.","upvote_count":"1","poster":"9f02c8d"},{"upvote_count":"2","content":"Selected Answer: A\nSee: https://docs.aws.amazon.com/migrationhub/latest/ug/network-diagram.html\n\nand https://aws.amazon.com/about-aws/whats-new/2020/11/aws-migration-hub-includes-network-visualization/","poster":"ayadmawla","comment_id":"1098930","timestamp":"1702818660.0"},{"timestamp":"1700353020.0","content":"Selected Answer: A\nA is right answer.","comment_id":"1074367","upvote_count":"1","poster":"career360guru"},{"upvote_count":"1","timestamp":"1698582780.0","comment_id":"1056806","poster":"KungLjao","content":"Selected Answer: C\nShould work with c"},{"comment_id":"1056087","poster":"airgead","content":"Answer: C\nTo identify the dependencies that need to be migrated in the same phase as the application, the company can use the AWS Application Discovery Agent data. In this case, the sensitive low-latency communications use a custom IP-based protocol that runs on port 1000. The goal is to find servers that communicate on port 1000. Option C would be the most appropriate approach","upvote_count":"3","timestamp":"1698485460.0"}],"answer_ET":"A","url":"https://www.examtopics.com/discussions/amazon/view/124796-exam-aws-certified-solutions-architect-professional-sap-c02/","choices":{"D":"Use AWS Migration Hub and select the servers that host the application. Push the Amazon CloudWalch agent to the identified servers by using the AWS Application Discovery Agent. Export the CloudWatch logs that the agents collect to Amazon S3. Use Amazon Athena to query the logs to find servers that communicate on port 1000. Return to Migration Hub Create a move group that is based on the findings from the Athena queries.","C":"Use AWS Migration Hub and select the servers that host the application. Turn on data exploration in Network Access Analyzer. Use the Network Access Analyzer console to select the servers that host the application. Select a Network Access Scope of port 1000 and note the matching servers. Return to Migration Hub. Create a move group that is based on the findings from Network Access Analyzer.","A":"Use AWS Migration Hub and select the servers that host the application. Visualize the network graph to find servers that interact with the application. Turn on data exploration in Amazon Athena. Query the data that is transferred between the servers to identify the servers that communicate on port 1000. Return to Migration Hub. Create a move group that is based on the findings from the Athena queries.","B":"Use AWS Application Migration Service and select the servers that host the application. Visualize the network graph to find servers that interact with the application. Configure Application Migration Service to launch test instances for all the servers that interact with the application. Perform acceptance tests on the test instances. If no issues are identified, create a move group that is based on the tested servers."},"unix_timestamp":1698485460}],"exam":{"name":"AWS Certified Solutions Architect - Professional SAP-C02","isImplemented":true,"provider":"Amazon","id":33,"isBeta":false,"lastUpdated":"11 Apr 2025","isMCOnly":true,"numberOfQuestions":529},"currentPage":45},"__N_SSP":true}