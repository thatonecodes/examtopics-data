{"pageProps":{"questions":[{"id":"aU6HMV1MwQq9dk6Sh17h","isMC":true,"question_text":"A mortgage company has a microservice for accepting payments. This microservice uses the Amazon DynamoDB encryption client with AWS KMS managed keys to encrypt the sensitive data before writing the data to DynamoDB. The finance team should be able to load this data into Amazon Redshift and aggregate the values within the sensitive fields. The Amazon Redshift cluster is shared with other data analysts from different business units.\nWhich steps should a data analyst take to accomplish this task efficiently and securely?","question_id":101,"answers_community":["B (55%)","A (45%)"],"timestamp":"2020-08-24 19:01:00","answer_images":[],"answer":"B","answer_description":"","choices":{"B":"Create an AWS Lambda function to process the DynamoDB stream. Save the output to a restricted S3 bucket for the finance team. Create a finance table in Amazon Redshift that is accessible to the finance team only. Use the COPY command with the IAM role that has access to the KMS key to load the data from S3 to the finance table.","D":"Create an Amazon EMR cluster. Create Apache Hive tables that reference the data stored in DynamoDB. Insert the output to the restricted Amazon S3 bucket for the finance team. Use the COPY command with the IAM role that has access to the KMS key to load the data from Amazon S3 to the finance table in Amazon Redshift.","C":"Create an Amazon EMR cluster with an EMR_EC2_DefaultRole role that has access to the KMS key. Create Apache Hive tables that reference the data stored in DynamoDB and the finance table in Amazon Redshift. In Hive, select the data from DynamoDB and then insert the output to the finance table in Amazon Redshift.","A":"Create an AWS Lambda function to process the DynamoDB stream. Decrypt the sensitive data using the same KMS key. Save the output to a restricted S3 bucket for the finance team. Create a finance table in Amazon Redshift that is accessible to the finance team only. Use the COPY command to load the data from Amazon S3 to the finance table."},"url":"https://www.examtopics.com/discussions/amazon/view/29479-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer_ET":"B","question_images":[],"exam_id":20,"discussion":[{"timestamp":"1632226020.0","comments":[{"comment_id":"249413","timestamp":"1633162020.0","poster":"freaky","upvote_count":"3","comments":[{"timestamp":"1703583420.0","poster":"blackgamer","content":"B is wrong because the data is encrypted before loading into DynamoDB which implies that it is client side encryption and Redshift doesn't support the client side encryption - \n\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_loading-encrypted-files.html","comment_id":"1105867","upvote_count":"1"}],"content":"But why do we need to create DynamoDB streams. Streams is mentioned only in answer. Also it will be only for new data. What about the data which is already present. One of the requirement is Finane team should be able to aggregate data on sensitive field. But if they do not hae all the data in Redshift then how will aggregation provide correct result?"}],"upvote_count":"53","content":"Answer is B – \nC and D are cancelled because – EMR is not needed to process DynamoDB streams. Lambda function would be good enough.\n\nOption A is wrong because it suggests decrypting the data and storing in S3 which is not good since it contains sensitive fields.\nOption B is correct because Redshift will only decrypt the data while reading it.","poster":"awssp12345","comment_id":"165984"},{"comment_id":"266566","poster":"JD78780","upvote_count":"13","timestamp":"1633812900.0","content":"Correct: A \nC and D can be eliminated because this is a shared Redshift cluster, so you need to create a table accessible only to the finance team. B is wrong as the application uses DynamoDB client-side encryption (not S3 client-side encryption), which means it will not automatically decrypt by AWS, and needs manual decryption before sending to S3 and then COPY’d into Redshift. Even if you want to use COPY ENCRYPTED to copy client-side encrypted S3 files, you need to specify credentials not IAM roles.\nREPORT THIS AD\nREPORT THIS AD\nHowever, DynamoDB stream only does new data, so existing data won’t be processed, this is not a perfect answer.\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_loading-encrypted-files.html","comments":[{"comment_id":"618560","timestamp":"1655622180.0","content":"I think this is the best explanation.","poster":"dushmantha","upvote_count":"1"},{"content":"In A the issue is\nthough S3 is restricted the data stored still is unencrypted before loading to redshift","upvote_count":"2","timestamp":"1657808160.0","poster":"ru4aws","comment_id":"631414"},{"comments":[{"content":"on the link you shared above, The COPY command doesn't support the following types of Amazon S3 encryption:\nClient-side encryption using an AWS KMS key\n\nif client side encruption does not support than decrypting will not work as well","poster":"siju13","comment_id":"737584","upvote_count":"1","timestamp":"1670401560.0"}],"comment_id":"698598","timestamp":"1666142880.0","poster":"JoellaLi","upvote_count":"3","content":"Actually it will automatically decrypt by AWS, and no need to do manual decryption.\n\"After you create and configure the required components, the DynamoDB Encryption Client transparently encrypts and signs your table items when you add them to a table, and verifies and decrypts them when you retrieve them.\"\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_loading-encrypted-files.html"}]},{"content":"<a href=“https://striveenterprise.com/digital-marketing-tampa-fl/”>Digital marketing</a> isn't just about selling a product; it's about crafting an immersive brand experience. It's the fusion of compelling storytelling, cutting-edge technology, and a deep understanding of consumer psychology.","upvote_count":"1","poster":"Cristian_T5","comment_id":"1194541","timestamp":"1712956380.0"},{"comment_id":"1161632","poster":"chinmayj213","upvote_count":"1","timestamp":"1709125080.0","content":"When you load an encrypted file from Amazon S3 to Redshift, the encryption involved is neither purely client-side nor server-side using AWS KMS. It's a hybrid approach \n\nDecryption during Load:\n\nWhen you use the COPY command in Redshift to load the data, Redshift retrieves the data key from KMS using its IAM role or credentials.\nThis retrieval can be considered a server-side operation from Redshift's perspective."},{"timestamp":"1708528800.0","upvote_count":"2","comments":[{"poster":"rag_mat_80","upvote_count":"1","comment_id":"1169089","timestamp":"1709929860.0","content":"COPY command does decrypt - https://docs.aws.amazon.com/redshift/latest/dg/c_loading-encrypted-files.html"}],"poster":"NarenKA","content":"Selected Answer: A\nLambda function processes the DynamoDB stream, the sensitive data encrypted with KMS keys can be decrypted securely using the same KMS key. Storing the decrypted data in a restricted S3 bucket accessible only to the finance team ensures that sensitive information is not exposed to unauthorised users. Creating a dedicated finance table in Redshift is accessible only to the finance team ensures that the aggregated sensitive data remains confidential and is not accessible by others. The COPY command to load data from the restricted S3 bucket into the finance table in Redshift is efficient. \nB- loading encrypted data directly into Redshift and decrypting it during the COPY process is not directly supported as part of the COPY command.\nC, D - involve using EMR and Apache Hive, which could add complexity and operational overhead to the data processing workflow and decryption needs to occur before the data can be processed by EMR.","comment_id":"1155633"},{"comment_id":"1152357","poster":"Adzz","timestamp":"1708141500.0","upvote_count":"1","content":"Selected Answer: B\nWill go for B"},{"content":"Selected Answer: A\nB is wrong because the data is encrypted before loading into DynamoDB which implies that it is client side encryption and Redshift doesn't support the client side encryption -\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_loading-encrypted-files.html","comment_id":"1105870","comments":[{"timestamp":"1709929980.0","upvote_count":"1","comment_id":"1169091","poster":"rag_mat_80","content":"the key is this i feel - \"AWS KMS managed keys to encrypt the sensitive data\""}],"upvote_count":"1","timestamp":"1703583660.0","poster":"blackgamer"},{"upvote_count":"1","comment_id":"1043850","poster":"gofavad926","timestamp":"1697342100.0","content":"Selected Answer: B\nB. A and B are similar but B is more secure option"},{"poster":"rlnd2000","content":"Selected Answer: A\nB is incorrect, this option omits the step of decrypting the data before saving, I think A is the correct option.","comment_id":"1015722","timestamp":"1695556260.0","upvote_count":"1"},{"content":"The COPY command doesn't support the following types of Amazon S3 encryption: Answer A\nServer-side encryption with customer-provided keys (SSE-C)\nClient-side encryption using an AWS KMS key\nClient-side encryption using a customer-provided asymmetric root key","timestamp":"1687617360.0","comment_id":"932659","poster":"SMALLAM","upvote_count":"3"},{"timestamp":"1686132540.0","poster":"Hisayuki","comment_id":"917088","upvote_count":"1","content":"Selected Answer: A\nA is the answer"},{"comment_id":"886328","timestamp":"1682948940.0","upvote_count":"3","content":"A: I passed the test","poster":"pk349"},{"timestamp":"1682077380.0","content":"Answer is B as lambda can analyze the data in dynamo db and is save to restricted bucket which cannot be accessed without desired permission. Also to copy from s3 bucket to redshift it requires IAM role.","poster":"anjuvinayan","comment_id":"876467","upvote_count":"2"},{"timestamp":"1679666400.0","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_loading-encrypted-files.html","poster":"akashm99101001com","comment_id":"849341","upvote_count":"2"},{"timestamp":"1676649360.0","comment_id":"812097","upvote_count":"3","content":"Option B is not the best solution because it does not address the need to decrypt the sensitive data before loading it into Amazon Redshift. The finance team needs to be able to aggregate the values within the sensitive fields, which would not be possible if the data is not decrypted before loading it into Redshift. Option A solves this problem by creating a Lambda function that processes the DynamoDB stream and decrypts the sensitive data using the same KMS key used for encryption before loading it into Redshift. The data is also saved to a restricted S3 bucket to ensure that only the finance team has access to it.","poster":"Arjun777"},{"timestamp":"1673127720.0","content":"Selected Answer: B\nthe decrypting is automatically done by AWS as the data is moved from Dynamo to S3. Before it's written to S3 it's reencyrpted using SSE. Writing to S3 and leaving it decrypting (as Option A suggests) would not be a secure move. Hence B should be the right answer.","poster":"ota123","comment_id":"768937","upvote_count":"4"},{"poster":"henom","upvote_count":"2","comment_id":"729891","timestamp":"1669693920.0","content":"Ans- B"},{"timestamp":"1669020720.0","comment_id":"723318","upvote_count":"3","content":"Selected Answer: B\nYou can use the COPY command to load data files uploaded to Amazon S3 using server-side encryption, client-side encryption, or both.\nA is wrong as data on S3 is decrypted","poster":"nadavw"},{"content":"Selected Answer: B\nB is correct as scenario tests the security of the transfer between the sensitive data from the S3 bucket to the Redshift Cluster. Amazon S3 supports both server-side encryption and client-side encryption. The COPY command supports the different types of Amazon S3 encryption, including Server-side encryption with AWS KMS-managed keys (SSE-KMS) and Client-side encryption using a client-side symmetric master key..\nA is wrong as DynamoDB streams will decrypt the data before it writes to the S3 bucket and will be stored in plaintext.However, the concern here is to secure the transfer of the same data from S3 to Amazon Redshift. \nC & D are both incorrect because using an Amazon EMR Cluster is unnecessary and expensive to implement in this scenario. These solutions do not comply with the AWS Well-Architected Framework. Enabling DynamoDB streams and an AWS Lambda function would suffice.","poster":"cloudlearnerhere","comment_id":"711911","upvote_count":"4","timestamp":"1667670180.0"},{"timestamp":"1667160000.0","comment_id":"708016","poster":"aefuen1","content":"Selected Answer: A\nAnswer is A. Redshift doesn't support client-side encryption with AWS KMS keys.\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_loading-encrypted-files.html","upvote_count":"2"},{"comment_id":"707609","content":"Sorry The Ans I believe is C. \nDynamo DB streams can not process the historical data rather only the changed that occurred in the dynamo DB itself. \nso Left with only C to call it as a right ans.","upvote_count":"1","timestamp":"1667105880.0","poster":"Rejju"},{"upvote_count":"1","content":"The Ans is A. we can not load the data with client encrypted using AWS KMS data into the redshift and hence the ans is not B.","comment_id":"707601","timestamp":"1667104740.0","poster":"Rejju"},{"timestamp":"1664083680.0","content":"Selected Answer: B\nA is not the correct option as data is being decrypted and then gets written into S3. Is this case S3 to Redshift in-transit data are not encrypted.","comment_id":"678465","upvote_count":"2","poster":"Arka_01"},{"poster":"Abep","timestamp":"1662507900.0","upvote_count":"4","comment_id":"661738","comments":[{"timestamp":"1666143060.0","content":"But it refers to 'Amazon S3 encryption' not dynamoDB encrytion.","upvote_count":"1","poster":"JoellaLi","comment_id":"698599"}],"content":"Selected Answer: A\nCorrect A: because redshift COPY command cannot load client-side encrypted data with AWS KMS keys. https://docs.aws.amazon.com/redshift/latest/dg/c_loading-encrypted-files.html"},{"poster":"WonderTan","timestamp":"1660188420.0","comment_id":"645244","upvote_count":"1","content":"B is appropriate"},{"content":"The correct option is B, cuzz it can be A by the explanation below:\n DynamoDB streams will decrypt the data before it writes to the S3 bucket and will be stored in plaintext. DynamoDB Encryption Client provides encryption-in-transit. However, the concern here is to secure the transfer of the same data from S3 to Amazon Redshift. You can use an IAM role with access to the same KMS key to transfer the data from the S3 bucket to the Redshift table.","comments":[{"content":"Agree. \n\n\"After you create and configure the required components, the DynamoDB Encryption Client transparently encrypts and signs your table items when you add them to a table, and verifies and decrypts them when you retrieve them.\"\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_loading-encrypted-files.html","upvote_count":"1","comment_id":"698600","poster":"JoellaLi","timestamp":"1666143120.0"}],"poster":"Kristb15_","timestamp":"1659961800.0","comment_id":"644109","upvote_count":"3"},{"timestamp":"1658290200.0","poster":"rocky48","content":"Selected Answer: B\nAnswer is B.\nOption B is correct because Redshift will only decrypt the data while reading it.","upvote_count":"1","comment_id":"633822"},{"content":"Selected Answer: A\nThis is definitely a tough one! I think you would like to do a big import with something like EMR to convert all your historical data. But for the daily updates you want to keep it up to date with Lambda + DynamoDB Table Streams. \n\nStoring the data encrypted in a secured S3 bucket is more safe then storing it decrypted. However, when copying the data Redshift COPY command can only decrypt the data if it was encrypted with SSE-KMS and I don't think this will be considered SSE-KMS if the data was not encrypted by the S3 service. \n\nSo answer is A. But I would like to add a step where you convert all historical data with EMR.","upvote_count":"3","comment_id":"611101","timestamp":"1654262640.0","poster":"Ramshizzle"},{"timestamp":"1652150220.0","content":"This is tough one.\nDynamoDB Stream can only process changed data. If we want to process all data, we need some ETL way. So A,B are wrong.\n\"The data is encryped before saved using the Amazon DynamoDB encryption client and AWS KMS controlled keys\". If there is no key, we cann't get original value. I didn't find document about whether the encrypted data by 'DynamoDB encryption client' can be decrypted in redshift. But, at least, we need to access the key to access it in Hive.\nSo I think C is correct.","comment_id":"599368","poster":"MWL","comments":[{"comment_id":"600000","poster":"MWL","upvote_count":"1","content":"Link about dynamodb-encryption-client:\nhttps://docs.aws.amazon.com/dynamodb-encryption-client/latest/devguide/what-is-ddb-encrypt.html","timestamp":"1652260560.0"}],"upvote_count":"1"},{"timestamp":"1651053660.0","upvote_count":"1","comment_id":"593025","content":"Selected Answer: A\nAs explained by others, COPY cannot by applied to CSE-KMS encrypted data which is used by the DynamoDB encryption client\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_loading-encrypted-files.html\nhttps://docs.aws.amazon.com/dynamodb-encryption-client/latest/devguide/what-is-ddb-encrypt.html","poster":"Teraxs"},{"timestamp":"1642070580.0","comment_id":"522783","poster":"rav009","content":"The answer should be A.\nB is not right. Because dynamodb encryption is column-level. While COPY command can only decrypt file-level data. How can COPY command know which column to use the KMS key to decrypt?","upvote_count":"2","comments":[{"timestamp":"1643656500.0","poster":"cnmc","upvote_count":"2","comments":[{"timestamp":"1669197240.0","upvote_count":"1","poster":"rav009","content":"I really suggest you study dynamodb encryption client first. It only encrypts certain attributes in a record. How can Copy command decrypt such encrypted record?","comment_id":"725039"}],"comment_id":"537343","content":"I don't know if you're simply mistaken or trolling... Study the Redshift COPY command again..."}]},{"comment_id":"478361","poster":"aws2019","content":"B is right","upvote_count":"1","timestamp":"1636927140.0"},{"poster":"goutes","upvote_count":"1","timestamp":"1636923540.0","comment_id":"478339","content":"B and C, could be the answers but in C EMR uses KMS keys, but HDFS encryption doesn't use a KMS key in AWS KMS.So the answer is B. Correct if I am wrong."},{"poster":"Billhardy","upvote_count":"2","timestamp":"1636231800.0","content":"Ans B","comment_id":"442338"},{"poster":"Shraddha","timestamp":"1636162260.0","comment_id":"392224","content":"Ans A\nThis is not a good question. C and D can be eliminated because this is a shared Redshift cluster, so you need to create a table accessible only to the finance team. B is wrong as the application uses DynamoDB client-side encryption (not S3 client-side encryption), which means it will not automatically decrypt by AWS, and needs manual decryption before sending to S3 and then COPY’d into Redshift. Even if you want to use COPY ENCRYPTED to copy client-side encrypted S3 files, you need to specify credentials not IAM roles.\n\nHowever, DynamoDB stream only does new data, so existing data won’t be processed, this is not a perfect answer.\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_loading-encrypted-files.html","comments":[{"content":"The COPY command doesn't support the following types of Amazon S3 encryption:\n\nServer-side encryption with customer-provided keys (SSE-C)\nClient-side encryption using an AWS KMS key\nClient-side encryption using a customer-provided asymmetric root key\n\nDynamodb encryption client means client side encryption\nRef: https://docs.aws.amazon.com/dynamodb-encryption-client/latest/devguide/what-is-ddb-encrypt.html\n\nSince COPY command dont support CSE-KMS then we may need to decrypt data before issuing copy command.\n\nLooks like it might be: Correct answer = A","timestamp":"1647795540.0","comments":[{"upvote_count":"1","comment_id":"571751","timestamp":"1647795720.0","poster":"CHRIS12722222","content":"UNLOAD command automatically encrypts your files using SSE-S3\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/t_unloading_encrypted_files.html"}],"upvote_count":"4","poster":"CHRIS12722222","comment_id":"571750"},{"content":"The link you posted show that you specify the role arn (not credentials) when using the copy command\n\ncopy customer from 's3://mybucket/encrypted/customer' \niam_role 'arn:aws:iam::0123456789012:role/MyRedshiftRole'\nmaster_symmetric_key '<root_key>' \nencrypted\ndelimiter '|';","timestamp":"1647795000.0","comment_id":"571747","upvote_count":"1","poster":"CHRIS12722222"}],"upvote_count":"5"},{"comment_id":"388902","poster":"Huy","upvote_count":"3","timestamp":"1635245340.0","content":"Agree with A. When you capture data change with DynamoDB Stream, the data output is JSON text file with the encrypted fields are shown as base64. S3 will treat it as normal unencrypted file."},{"poster":"Donell","comment_id":"387077","timestamp":"1635017220.0","upvote_count":"1","content":"Answer is B (Similar question is there in Jon Bonso's practice exam).\nCreate an AWS Lambda function to process the DynamoDB stream. Save the output to a restricted S3 bucket for the finance team. Create a finance table in Amazon Redshift that is accessible to the finance team only. Use the COPY command with the IAM role that has access to the KMS key to load the data from S3 to the finance table."},{"comment_id":"350704","poster":"DerekKey","timestamp":"1634735580.0","upvote_count":"1","content":"Correct B\nThe COPY command supports the following types of Amazon S3 encryption:\n- Server-side encryption with Amazon S3-managed keys (SSE-S3)\n- Server-side encryption with AWS KMS-managed keys (SSE-KMS)\n- Client-side encryption using a client-side symmetric master key\n\ncopy customer from 's3://mybucket/encrypted/customer' \niam_role 'arn:aws:iam::0123456789012:role/MyRedshiftRole'\nmaster_symmetric_key '<master_key>' \nencrypted\ndelimiter '|';"},{"content":"Answer A\n\nDynamoDB Encryption Client encrypts selected attributes and items. Additionally, it doesn't encrypt names of attributes. It is client side encryption https://docs.aws.amazon.com/dynamodb-encryption-client/latest/devguide/encrypted-and-signed.html\n\nRedshift COPY command support S3 encryption (server or client) which is different from DynamoDB Encryption Client. \nhttps://docs.aws.amazon.com/redshift/latest/dg/c_loading-encrypted-files.html","timestamp":"1634687760.0","comment_id":"341900","poster":"Kalan","upvote_count":"3"},{"comment_id":"340024","content":"Answer is B - DynamoDB supports trigger a Lambda function, and also the copy command support the KMS custom managed encryption\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_loading-encrypted-files.html\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html\nCD - Use an EMR in this case is not the best solution because its a shared cluster\nA - Decrypt the data and save in S3 is not secure at all","timestamp":"1634611680.0","poster":"mramirez1996","upvote_count":"3"},{"poster":"lostsoul07","content":"B is the right answer","upvote_count":"2","timestamp":"1633987920.0","comment_id":"274323"},{"comments":[{"content":"DynamoDB stream is good for the new data but what about the old data (already loaded to DynamoDB)?","upvote_count":"1","comment_id":"251971","poster":"adit","timestamp":"1633751580.0"}],"upvote_count":"1","poster":"saabji","comment_id":"251767","content":"Loading data into Redshift is most appropriate using COPY command. That takes out Option C from consideration. Option A does not deal with sensitive data properly. \nD seems right but it uses EMR Hive which might be an overkill here plus expensive. B. moving data from DynamoDb to S3 to Redshift via streams + Lambda works well for all the data being pushed to DynamoDb by microservice. There is no mention of moving historic data into Redshift","timestamp":"1633718040.0"},{"comment_id":"248753","poster":"heihei","content":"why we need a dynamodb stream","upvote_count":"1","timestamp":"1632885360.0"},{"timestamp":"1632625680.0","upvote_count":"4","comment_id":"216846","poster":"BillyC","content":"B is correct!"},{"comment_id":"191611","poster":"syu31svc","upvote_count":"1","timestamp":"1632431220.0","content":"From link: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html\n\"Amazon DynamoDB is integrated with AWS Lambda so that you can create triggers—pieces of code that automatically respond to events in DynamoDB Streams\"\nC and D are out\nFor security IAM role to match with the key is the way to go so answer is B"},{"content":"DynamoDB stream is good for the new data but what about the old data (already loaded to DynamoDB)?","comments":[{"timestamp":"1632493680.0","comment_id":"195217","upvote_count":"1","content":"thats right , streams will only need new data. Here finance team needs all data. This should be between C and D. Probably C as its avoids writing data to s3","poster":"abhineet"}],"comment_id":"177450","timestamp":"1632415080.0","poster":"KoMo","upvote_count":"1"},{"content":"Option B seems the correct choice.","timestamp":"1632367800.0","comment_id":"175557","poster":"Paitan","upvote_count":"2"},{"poster":"Saaho","content":"B and C seems right","upvote_count":"1","comment_id":"165412","timestamp":"1632178500.0"}],"topic":"1","unix_timestamp":1598288460},{"id":"xqL9nfDp9mYLkQJUDk8o","answer_ET":"A","discussion":[{"upvote_count":"27","timestamp":"1632195600.0","comment_id":"159327","content":"Seems answer is A","poster":"paul0099"},{"content":"my answer is A","upvote_count":"9","timestamp":"1632459000.0","comment_id":"160768","poster":"zeronine"},{"timestamp":"1701214680.0","comment_id":"1083020","upvote_count":"1","content":"It's B. Glue job bookmark option is for S3 not for database.","poster":"skb0071"},{"comment_id":"988673","upvote_count":"1","poster":"RollingGemini","content":"Selected Answer: A\nOf course A","timestamp":"1692824400.0"},{"poster":"juanife","timestamp":"1687198380.0","comment_id":"927796","content":"Undoubtedly it's A.","upvote_count":"1"},{"comment_id":"886330","poster":"pk349","timestamp":"1682949060.0","content":"A: I passed the test","upvote_count":"1"},{"comment_id":"876468","poster":"anjuvinayan","upvote_count":"1","timestamp":"1682077620.0","content":"Answer is A\nSince Glue has the Job bookmark option to save the info regarding the last load, this can be used when starting next load so that duplicates are not inserted."},{"timestamp":"1670797620.0","upvote_count":"2","poster":"renfdo","content":"Selected Answer: A\nI'm sure it's A. Follow documnet explain about JDBC source.\nhttps://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html","comment_id":"742164"},{"content":"Selected Answer: A\nCorrect answer is A as AWS Glue can be used to export the data from the relational database incrementally using job bookmarks in a cost-effective way.","upvote_count":"2","poster":"cloudlearnerhere","timestamp":"1667670360.0","comment_id":"711913"},{"comment_id":"678513","poster":"Arka_01","timestamp":"1664089740.0","content":"Selected Answer: A\nIncremental Data and Managed Service, these are the two key words here. So AWS Glue with Job Bookmark will do the trick.","upvote_count":"1"},{"poster":"rocky48","timestamp":"1658986920.0","content":"Selected Answer: A\nSelected Answer: A","upvote_count":"1","comment_id":"638499"},{"comment_id":"604847","timestamp":"1653132060.0","upvote_count":"1","content":"Selected Answer: A\nMy Answer is A","poster":"Bik000"},{"timestamp":"1647753000.0","content":"Selected Answer: A\nAnswer is A","upvote_count":"2","poster":"moon2351","comment_id":"571404"},{"upvote_count":"1","comment_id":"559612","content":"Answer is A. why to use other services when Glue itself can perform the desired functions as per the question","timestamp":"1646244480.0","poster":"KnightVictor"},{"poster":"Shraddha","timestamp":"1636056780.0","comment_id":"392228","content":"Ans A\nThis is a textbook question.","upvote_count":"2"},{"comment_id":"274324","timestamp":"1635281160.0","upvote_count":"2","poster":"lostsoul07","content":"A is the right answer"},{"comment_id":"251769","content":"A.\nJob bookmarks are implemented for JDBC data sources, the Relationalize transform, and some Amazon Simple Storage Service (Amazon S3) sources","poster":"saabji","timestamp":"1635164160.0","upvote_count":"2"},{"comment_id":"230450","content":"Guys, I go with B.\nA) bookmarks works only on S3 files as source - it would not work in a database as soruce (https://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html);\nC) I would work, but with more cost than B;\nD) Datasync also works only on files, not databases.","timestamp":"1633931700.0","poster":"viniassumps","upvote_count":"4","comments":[{"comments":[{"poster":"freaky","comment_id":"249423","upvote_count":"1","timestamp":"1634808000.0","content":"Below is the excerpt from same link : \n\"Job bookmarks are implemented for JDBC data sources, the Relationalize transform, and some Amazon Simple Storage Service (Amazon S3) sources.\" \nIt doesn't mention anywhere that it works only for RDS. Unless I am missing something. So in my view Answer is A"}],"timestamp":"1634084820.0","poster":"Katana19","comment_id":"235172","content":"no, it works with RDS using JDBC.... see the below link:\nhttps://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html","upvote_count":"1"},{"upvote_count":"5","content":"https://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html\nLink explains using columns and PK for glue boomarks \nFor JDBC sources, the following rules apply:\n\nFor each table, AWS Glue uses one or more columns as bookmark keys to determine new and processed data. The bookmark keys combine to form a single compound key.\n\nYou can specify the columns to use as bookmark keys. If you don't specify bookmark keys, AWS Glue by default uses the primary key as the bookmark key, provided that it is sequentially increasing or decreasing (with no gaps).\n\nIf user-defined bookmarks keys are used, they must be strictly monotonically increasing or decreasing. Gaps are permitted.","poster":"angadaws","timestamp":"1634056800.0","comment_id":"233592"},{"poster":"california","upvote_count":"3","comment_id":"707813","content":"Job bookmarks are implemented for JDBC data sources, the Relationalize transform, and some Amazon Simple Storage Service (Amazon S3) sources. (https://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html) - So database can be the source.","timestamp":"1667134320.0"}]},{"poster":"BillyC","timestamp":"1633872480.0","content":"A is corret!","upvote_count":"1","comment_id":"216848"},{"comment_id":"202454","poster":"kamparia","content":"The answer should be A.\nhttps://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html","timestamp":"1633566180.0","upvote_count":"1"},{"upvote_count":"1","poster":"syu31svc","content":"This is similar to question 40\nAnswer is A","comment_id":"191613","timestamp":"1632657300.0"},{"upvote_count":"3","timestamp":"1632540000.0","comment_id":"175558","poster":"Paitan","content":"Option A for me."},{"poster":"whereami","upvote_count":"1","content":"where is the amazon section?","comment_id":"159523","timestamp":"1632399780.0"}],"answer":"A","choices":{"D":"Use AWS Glue to connect to the data source using JDBC Drivers and ingest the full data. Use AWS DataSync to ensure the delta only is written into Amazon S3.","B":"Use AWS Glue to connect to the data source using JDBC Drivers. Store the last updated key in an Amazon DynamoDB table and ingest the data using the updated key as a filter.","C":"Use AWS Glue to connect to the data source using JDBC Drivers and ingest the entire dataset. Use appropriate Apache Spark libraries to compare the dataset, and find the delta.","A":"Use AWS Glue to connect to the data source using JDBC Drivers. Ingest incremental records only using job bookmarks."},"url":"https://www.examtopics.com/discussions/amazon/view/28736-exam-aws-certified-data-analytics-specialty-topic-1-question/","isMC":true,"unix_timestamp":1597594080,"timestamp":"2020-08-16 18:08:00","answer_description":"","answers_community":["A (100%)"],"question_images":[],"question_id":102,"answer_images":[],"exam_id":20,"question_text":"A company is building a data lake and needs to ingest data from a relational database that has time-series data. The company wants to use managed services to accomplish this. The process needs to be scheduled daily and bring incremental data only from the source into Amazon S3.\nWhat is the MOST cost-effective approach to meet these requirements?","topic":"1"},{"id":"6BkR4NyGqGIqlq1MuHYZ","topic":"1","choices":{"A":"Enable Amazon Redshift Enhanced VPC Routing. Enable VPC Flow Logs to monitor traffic.","B":"Allow access to the Amazon Redshift database using AWS IAM only. Log access using AWS CloudTrail.","C":"Enable audit logging for Amazon Redshift using the AWS Management Console or the AWS CLI.","D":"Enable and download audit reports from AWS Artifact."},"answer_ET":"C","isMC":true,"answers_community":["C (100%)"],"question_images":[],"answer":"C","question_text":"An Amazon Redshift database contains sensitive user data. Logging is necessary to meet compliance requirements. The logs must contain database authentication attempts, connections, and disconnections. The logs must also contain each query run against the database and record which database user ran each query.\nWhich steps will create the required logs?","answer_description":"","timestamp":"2020-08-20 18:27:00","discussion":[{"poster":"Prodip","comments":[{"upvote_count":"8","comment_id":"165997","content":"Agreed\n\nAmazon Redshift logs information in the following log files:\n• Connection log — logs authentication attempts, and connections and disconnections. \n• User log — logs information about changes to database user definitions. \n• User activity log — logs each query before it is run on the database. \nhttps://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html","timestamp":"1632813960.0","poster":"awssp12345","comments":[{"content":"I am inclined to C. But does anyone know how come B is not right? CloudTrail is supposed to provide the requested service.","comments":[{"comment_id":"298386","poster":"jAWStest","upvote_count":"5","content":"https://stackify.com/aws-redshift-monitoring-the-complete-guide/\nmay help with the difference between cloudtrail and db audit logging","timestamp":"1635508800.0"}],"poster":"Jh2501","upvote_count":"2","timestamp":"1634537340.0","comment_id":"176337"}]},{"comment_id":"505868","timestamp":"1640069400.0","content":"Agree\nFurther Audit logs can be analysed using Redshift Spectrum\nhttps://aws.amazon.com/blogs/big-data/analyze-database-audit-logs-for-security-and-compliance-using-amazon-redshift-spectrum/","poster":"lakediver","upvote_count":"2"}],"upvote_count":"20","comment_id":"164431","timestamp":"1632768600.0","content":"Its C; Enhanced VPC Routing enforce COPY/UNLOAD to use VPC"},{"content":"Ans C\nA = wrong, enhanced VPC routing means data in/out within VPC. B = wrong, CloudTrail do not log data events, only configuration events. D = wrong, nonsense. This is a textbook question.\n\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html","poster":"Shraddha","comment_id":"392233","upvote_count":"8","timestamp":"1636170060.0"},{"timestamp":"1682949120.0","poster":"pk349","comment_id":"886332","upvote_count":"1","content":"C: I passed the test"},{"poster":"anjuvinayan","comment_id":"876477","timestamp":"1682078160.0","content":"Answer is C\nA-User should connect to VPN first to access Redshift in VPC, in question there is no details regarding VPN\nB. Only users with AWS access will be able to connect to redshift\nD. Artifacts is not a solution.\n\nAlso cloudtrail will log only access to the service and not what happened inside the service","upvote_count":"2"},{"comments":[{"poster":"cloudlearnerhere","timestamp":"1667670540.0","comments":[{"upvote_count":"1","comment_id":"711918","content":"Option B is wrong as Amazon Redshift is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in Amazon Redshift. CloudTrail captures all API calls for Amazon Redshift as events. These include calls from the Amazon Redshift console and from code calls to the Amazon Redshift API operations. If you create a trail, you can enable continuous delivery of CloudTrail events to an Amazon S3 bucket, including events for Amazon Redshift. If you don't configure a trail, you can still view the most recent events in the CloudTrail console in Event history. Using the information collected by CloudTrail, you can determine certain details. These include the request that was made to Amazon Redshift, the IP address it was made from, who made it, when it was made, and other information.","timestamp":"1667670540.0","poster":"cloudlearnerhere"}],"upvote_count":"1","content":"Option A is wrong as Redshift Enhanced VPC Routing supports the use of standard VPC features such as VPC Endpoints, security groups, network ACLs, managed NAT and internet gateways, enabling you to tightly manage the flow of data between your Amazon Redshift cluster and all of your data sources.\nOption D is wrong as AWS Artifact is your go-to, central resource for compliance-related information that matters to you. It provides on-demand access to AWS’ security and compliance reports and select online agreements.","comment_id":"711917"}],"comment_id":"711915","content":"Selected Answer: C\nCorrect answer is C as Redshift Audit Logging can provide the required information.\n\nAudit logging is not enabled by default in Amazon Redshift. When you enable logging on your cluster, Amazon Redshift creates and uploads logs to Amazon S3 that capture data from the time audit logging is enabled to the present time. Each logging update is a continuation of the information that was already logged.The connection log, user log, and user activity log are enabled together by using the AWS Management Console, the Amazon Redshift API Reference, or the AWS Command Line Interface (AWS CLI).\n\nAmazon Redshift logs information in the following log files:\n\nConnection log — logs authentication attempts, and connections and disconnections.\nUser log — logs information about changes to database user definitions.\nUser activity log — logs each query before it is run on the database.","poster":"cloudlearnerhere","timestamp":"1667670480.0","upvote_count":"3"},{"timestamp":"1664089860.0","poster":"Arka_01","comment_id":"678514","upvote_count":"1","content":"Selected Answer: C\nIt can be done by enabling Audit Logging of Redshift."},{"comment_id":"637089","upvote_count":"1","poster":"rocky48","content":"Selected Answer: C\nSelected Answer: C","timestamp":"1658805300.0"},{"timestamp":"1651882260.0","poster":"MWL","comment_id":"597921","upvote_count":"1","content":"Selected Answer: C\nThis is what Redshift audit log do."},{"content":"Answer - A , Enhanced Routing","timestamp":"1651357860.0","upvote_count":"1","poster":"jrheen","comment_id":"595332"},{"poster":"Teraxs","timestamp":"1651237800.0","content":"Selected Answer: C\nas discussed by others","upvote_count":"1","comment_id":"594478"},{"poster":"aws2019","upvote_count":"1","timestamp":"1637424840.0","content":"Ans is C","comment_id":"482686"},{"content":"Correct C\nAmazon Redshift logs information in the following log files:\n- Connection log — logs authentication attempts, and connections and disconnections.\n- User log — logs information about changes to database user definitions.\n- User activity log — logs each query before it is run on the database.\nThe connection log, user log, and user activity log are enabled together by using the AWS Management Console, the Amazon Redshift API Reference, or the AWS Command Line Interface (AWS CLI).\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html","comment_id":"350709","upvote_count":"3","poster":"DerekKey","timestamp":"1635857400.0"},{"comment_id":"274325","timestamp":"1635332520.0","content":"C is the right answer","poster":"lostsoul07","upvote_count":"2"},{"comment_id":"223585","timestamp":"1635223680.0","upvote_count":"1","content":"C for sure: https://aws.amazon.com/premiumsupport/knowledge-center/logs-redshift-database-cluster/","poster":"mbaexam"},{"timestamp":"1635199620.0","comment_id":"216850","content":"C is correct","upvote_count":"1","poster":"BillyC"},{"content":"Link provided confirms C as the answer","timestamp":"1634833860.0","comment_id":"191617","upvote_count":"1","poster":"syu31svc"},{"content":"The connection log, user log, and user activity log are enabled together by using the AWS Management Console, the Amazon Redshift API Reference, or the AWS Command Line Interface (AWS CLI). Answer is C","timestamp":"1634548440.0","upvote_count":"2","comment_id":"177905","poster":"Woong"},{"timestamp":"1634517960.0","content":"Option C is correct. Enhanced VPC routing is used to monitor COPY and UNLOAD traffic.","upvote_count":"3","poster":"Paitan","comment_id":"175560"},{"upvote_count":"3","content":"The link provided confirms that its C, thoughts?","timestamp":"1632525660.0","poster":"jccccccccccccc","comment_id":"164369"},{"upvote_count":"1","content":"Isnt A the right answer","comment_id":"162349","poster":"Saaho","timestamp":"1632241020.0"}],"question_id":103,"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/29149-exam-aws-certified-data-analytics-specialty-topic-1-question/","exam_id":20,"unix_timestamp":1597940820},{"id":"uvSiLq9f0Uo1Abyn6ysM","topic":"1","question_images":[],"isMC":true,"question_id":104,"unix_timestamp":1597527060,"url":"https://www.examtopics.com/discussions/amazon/view/28680-exam-aws-certified-data-analytics-specialty-topic-1-question/","answers_community":["C (100%)"],"choices":{"B":"Create a separate Kinesis data stream for Station A with two shards, and stream Station A sensor data to the new stream.","A":"Increase the number of shards in Kinesis Data Streams to increase the level of parallelism.","C":"Modify the partition key to use the sensor ID instead of the station name.","D":"Reduce the number of sensors in Station A from 10 to 5 sensors."},"answer_images":[],"answer":"C","answer_ET":"C","timestamp":"2020-08-15 23:31:00","exam_id":20,"answer_description":"","discussion":[{"poster":"Priyanka_01","upvote_count":"34","content":"C?\nA and B increase the cost","comment_id":"158867","comments":[{"upvote_count":"2","poster":"awssp12345","content":"Agreed","timestamp":"1633730700.0","comment_id":"165999"},{"comment_id":"505833","poster":"lakediver","upvote_count":"2","timestamp":"1640064960.0","content":"Agreed\nFor further reading see - \nhttps://aws.amazon.com/blogs/big-data/under-the-hood-scaling-your-kinesis-data-streams/"}],"timestamp":"1632937800.0"},{"comment_id":"205048","timestamp":"1635757380.0","upvote_count":"11","poster":"sanjaym","content":"C is 100% correct answer."},{"content":"C: I passed the test","timestamp":"1682949180.0","upvote_count":"2","comment_id":"886334","poster":"pk349"},{"poster":"anjuvinayan","timestamp":"1682078580.0","comment_id":"876493","content":"Answer is C\n\nA. No need to Increase the number of shards as in question its mentioned the throughput is less\nB. More cost\nC. Modifying the partition key to use the sensor ID instead of the station name is the correct answer. As of now all data from Station A which has more sensors is going to one shard and all data from Station B to another shard which has less sensors. By changing partition key to sensor ID will help to divide the data base on sensors to shard.\nD. Change in infra which is not required","upvote_count":"4"},{"content":"Option B does involve creating a separate Kinesis data stream for Station A, which could be seen as increasing the complexity of the solution compared to modifying the partition key. However, in this scenario, the bottleneck is on data coming from Station A, and creating a separate stream with dedicated shards for that station can help to increase parallelism and improve throughput without increasing the overall cost of the solution.\n\nOn the other hand, modifying the partition key to use the sensor ID instead of the station name could result in uneven shard distribution and hot partitions if the distribution of sensors across stations is uneven. This could lead to degraded performance and require additional scaling in the future, which could increase complexity and cost over time.\n\nSo, while both options have their pros and cons, creating a separate Kinesis data stream for Station A with dedicated shards can be a more effective and scalable solution for improving throughput in this scenario.","timestamp":"1677807180.0","upvote_count":"1","poster":"rags140882","comment_id":"827584"},{"comment_id":"711919","timestamp":"1667670660.0","upvote_count":"5","comments":[{"comment_id":"711920","poster":"cloudlearnerhere","comments":[{"comment_id":"711922","upvote_count":"2","content":"If your use cases do not require data stored in a shard to have high affinity, you can achieve high overall throughput by using a random partition key to distribute data. Random partition keys help distribute the incoming data records evenly across all the shards in the stream and reduce the likelihood of one or more shards getting hit with a disproportionate number of records. You can use a universally unique identifier (UUID) as a partition key to achieve this uniform distribution of records across shards. This strategy can increase the latency of record processing if the consumer application has to aggregate data from multiple shards.","timestamp":"1667670720.0","poster":"cloudlearnerhere"}],"upvote_count":"2","content":"The partition key determines to which shard the record is written. The partition key is a Unicode string with a maximum length of 256 bytes. Kinesis runs the partition key value that you provide in the request through an MD5 hash function. The resulting value maps your record to a specific shard within the stream, and Kinesis writes the record to that shard. Partition keys dictate how to distribute data across the stream and use shards.\n\nCertain use cases require you to partition data based on specific criteria for efficient processing by the consuming applications. As an example, if you use player ID pk1234 as the hash key, all scores related to that player route to shard1. The consuming application can use the fact that data stored in shard1 has an affinity with the player ID and can efficiently calculate the leaderboard. An increase in traffic related to players mapped to shard1 can lead to a hot shard. Kinesis Data Streams allows you to handle such scenarios by splitting or merging shards without disrupting your streaming pipeline.","timestamp":"1667670660.0"}],"content":"Selected Answer: C\nCorrect answer is C as currently the partition keys are based on station names and with two shards, Station A shard is overloaded with 10 sensors, and Station B shard with 5 sensors. Changing the partition key from station names to sensor id would distribute the data equally across shards without increasing the overall cost and complexity of the solution.\n\nOption A is wrong as increasing shards would increase the cost.\n\nOption B is wrong as adding Kinesis Data Stream would increase the cost.\n\nOption D is wrong as reducing the number of sensors would reduce the data collection quality.","poster":"cloudlearnerhere"},{"comment_id":"705301","content":"Selected Answer: C\nAns is C.\nA and B will increase the overall cost. D - reducing the sensors is not the good option.\nC - by modifying the partition key to sensor id , input data will be evenly distributed across both the shards by avoiding the hot-sharding in the first shard","poster":"thirukudil","upvote_count":"1","timestamp":"1666853040.0"},{"comment_id":"678517","upvote_count":"2","poster":"Arka_01","content":"Selected Answer: C\nIt gives you the answer here - \n1. Station A is facing problem. This has 10 sensor ID and obviously more data.\n2. total stream throughput is still less than the allocated Kinesis Data Streams throughput.\n\nSo we are not utilizing full stream's capability. So workloads are not evenly distributed amongst Shards.","timestamp":"1664090040.0"},{"timestamp":"1658455200.0","content":"Selected Answer: C\nAnswer-C","upvote_count":"1","comment_id":"634945","poster":"rocky48"},{"content":"C is correct answer. Increasing shards won't help there as partitioning is based on station name and there are only two stations.","timestamp":"1653183300.0","upvote_count":"1","comment_id":"605072","poster":"certificationJunkie"},{"comment_id":"604811","upvote_count":"2","poster":"Bik000","content":"Selected Answer: C\nAnswer is C","timestamp":"1653129900.0"},{"poster":"jrheen","timestamp":"1651356600.0","upvote_count":"1","content":"Answer-C","comment_id":"595319"},{"content":"Selected Answer: C\nC- sensor id as partition key allows equal distribution of data between the two shards","comment_id":"594293","upvote_count":"1","poster":"Teraxs","timestamp":"1651217460.0"},{"comment_id":"482625","poster":"aws2019","content":"C is the right answer","upvote_count":"1","timestamp":"1637419140.0"},{"poster":"lostsoul07","upvote_count":"4","content":"C is the right answer","comment_id":"274326","timestamp":"1635881760.0"},{"poster":"BillyC","upvote_count":"4","comment_id":"216851","timestamp":"1635823620.0","content":"C is correct!"},{"poster":"syu31svc","content":"D is obviously wrong\nFrom link: https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding.html\n\"Splitting increases the number of shards in your stream and therefore increases the data capacity of the stream. Because you are charged on a per-shard basis, splitting increases the cost of your stream\"\nSo answer is C","timestamp":"1635753240.0","comment_id":"191620","upvote_count":"1"},{"comment_id":"179628","poster":"Karan_Sharma","timestamp":"1634054520.0","content":"Can someone please explain how does changing the partition key from name to id resolves the bottleneck here?","upvote_count":"1","comments":[{"content":"2 station names will create only 2 partition keys (one per shard), but when you use sensor_ids as the partition key, you have 15 partition keys distributed across 2 shards.","poster":"vicks316","comment_id":"180226","comments":[{"comment_id":"183409","upvote_count":"1","poster":"Karan_Sharma","timestamp":"1635702840.0","content":"Thanks I happen to mistake sensor id as station. This explains why C is correct, thanks."}],"timestamp":"1635421380.0","upvote_count":"6"},{"timestamp":"1634720100.0","content":"Station A has 10 sensors while station B has 5 sensors. For the data stream we have provisioned 2 shards. Now if we partition by station name the data from 10 sensors of station A will go to one shard while data from 5 sensors of station B will go to the other shard. So data is not evenly distributed and throughput from station A will be compromised. So the easiest solution without incurring cost is to go for partition by sensor id so that the distribution will be even across the two shards. We can always increase the number of shards but that will add up to the cost.","poster":"Paitan","upvote_count":"28","comment_id":"180083"}]},{"comment_id":"175561","content":"A,B and C all works. But C will have minimum effort and cost.","timestamp":"1634041620.0","upvote_count":"2","poster":"Paitan"},{"timestamp":"1633063440.0","upvote_count":"3","comment_id":"160773","content":"I think C as Priyanka pointed out that A and B increase the cost - charged on a per-shard basis.","poster":"zeronine"}],"question_text":"A company that monitors weather conditions from remote construction sites is setting up a solution to collect temperature data from the following two weather stations.\n✑ Station A, which has 10 sensors\n✑ Station B, which has five sensors\nThese weather stations were placed by onsite subject-matter experts.\nEach sensor has a unique ID. The data collected from each sensor will be collected using Amazon Kinesis Data Streams.\nBased on the total incoming and outgoing data throughput, a single Amazon Kinesis data stream with two shards is created. Two partition keys are created based on the station names. During testing, there is a bottleneck on data coming from Station A, but not from Station B. Upon review, it is confirmed that the total stream throughput is still less than the allocated Kinesis Data Streams throughput.\nHow can this bottleneck be resolved without increasing the overall cost and complexity of the solution, while retaining the data collection quality requirements?"},{"id":"TSexgbupNi9v8yPiJqpb","answer_ET":"A","unix_timestamp":1596975120,"discussion":[{"poster":"Paitan","timestamp":"1633611720.0","comment_id":"175564","comments":[{"timestamp":"1673580240.0","content":"https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/userguide/selecting-content-from-objects.html","poster":"Merrick","comment_id":"774067","upvote_count":"3"},{"comments":[{"timestamp":"1648035840.0","upvote_count":"2","poster":"iris22","comment_id":"573570","content":"https://docs.aws.amazon.com/amazonglacier/latest/dev/glacier-select.html"}],"upvote_count":"6","poster":"[Removed]","comment_id":"505895","content":"Archive objects that are queried by S3 Glacier Select must be formatted as uncompressed comma-separated values (CSV).","timestamp":"1640073120.0"}],"upvote_count":"39","content":"Since we are talking about compressed file, Amazon Glacier Select cannot be used. So we need to transfer data to S3 and then use S3 select. So option A is the right choice."},{"poster":"zeronine","upvote_count":"18","comments":[{"content":"Athena cant query Glacier data, so A cant be right.","poster":"Marvel_jarvis","comments":[{"timestamp":"1643639820.0","poster":"cnmc","content":"A doesn't mention Athena....","comment_id":"537167","comments":[{"comment_id":"557363","content":"please read \"The file is hosted in Amazon S3 Glacier...\"","poster":"strikeEagle","upvote_count":"1","timestamp":"1645971060.0"}],"upvote_count":"3"}],"comment_id":"499766","timestamp":"1639279920.0","upvote_count":"2"},{"poster":"awssp12345","upvote_count":"2","comment_id":"166387","timestamp":"1633493520.0","content":"Yes! I agree. Thank you."}],"content":"my answer is A. (You may need to use Athena if data is in multiple files but this question - data is in a single compressed file)","comment_id":"160789","timestamp":"1633457580.0"},{"upvote_count":"1","poster":"NarenKA","content":"Selected Answer: B\nGlacier Select allows to run queries directly on data stored in S3 Glacier without needing to restore and move the data to an active storage class in S3. This feature is designed for scenarios exactly like this, where you need to retrieve only a small subset of data from a large archive stored in Glacier and it is more cost-effective. You are charged for the queries you run and the data retrieved, which, for a small subset of a 100 MB file, could be minimal. This avoids the costs associated with moving the data to Amazon S3 and storing it there for querying. \nA - restoring the file to S3 and then querying it with S3 Select incurs extra costs and processing time.\nC and D - Athena or Redshift Spectrum are powerful for analyzing large datasets, they introduce unnecessary complexity and costs for the given task considering the relatively small size of the dataset.","comment_id":"1155673","timestamp":"1708531260.0"},{"comment_id":"1094313","content":"Selected Answer: B\nIt's B, you can use Amazon Glacier Select to query archived data in Amazon Glacier.\nhttps://aws.amazon.com/about-aws/whats-new/2017/11/amazon-glacier-select-makes-big-data-analytics-of-archive-data-possible/?nc1=h_ls","comments":[{"timestamp":"1704977520.0","content":"accepted, this is an old question that does not reflect current standards of the","upvote_count":"1","poster":"GCPereira","comment_id":"1119741"}],"poster":"teo2157","timestamp":"1702373100.0","upvote_count":"2"},{"comment_id":"1011985","timestamp":"1695192060.0","upvote_count":"1","poster":"chinmayj213","content":"It is tricky question as now a day \"Glacier-Select\" support compressed g-zip csv. but the problem is file loaded a month ago and deep archival has limitation of one month to retrieve else we need to pay expedited retrieval fees"},{"comment_id":"1011982","content":"https://github.com/awsdocs/amazon-glacier-developer-guide/blob/master/doc_source/glacier-select.md","poster":"chinmayj213","upvote_count":"1","timestamp":"1695191340.0"},{"comment_id":"972529","comments":[{"timestamp":"1691599800.0","content":"Amazon S3 objects that are stored in the S3 Glacier Flexible Retrieval or S3 Glacier Deep Archive storage classes are not immediately accessible. To access an object in these storage classes, you must restore a temporary copy of the object to its S3 bucket for a specified duration (number of days). https://docs.aws.amazon.com/AmazonS3/latest/userguide/restoring-objects.html","upvote_count":"2","comment_id":"976859","poster":"confuzz"}],"content":"Looks like not up-to-date question. Links to AWS Docs posted in this discussion before are redirected now and don't mention Glacier Select (I don't count Blog and unofficial resources). It's only S3 Select now and it has no limit for Amazon S3 Glacier Instant Retrieval storage class. \nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/selecting-content-from-objects.html\nThere is no just \"S3 Glacier\" now. \nThis limitation \"The archived objects that are being queried by the select request must be formatted as uncompressed comma-separated values (CSV) files\" is googled now only in Restore Object command in SDK and CLI which I guess is used to restore from Glacier.\nIf come back to the past, I would go with A, since I believe guys saw this limitation for Glacier Select to query from uncompressed files in the links before.","timestamp":"1691185980.0","upvote_count":"5","poster":"confuzz"},{"content":"Selected Answer: B\nThe Amazon S3 Glacier Select works on objects as it supports a subset of SQL with a format like CSV, JSON, or Apache Parquet format. Objects compressed with GZIP or BZIP2 (for CSV and JSON objects only), and server-side encrypted objects can also be retrieved.\nRef : https://www.scaler.com/topics/aws/s3-glacier-select/","comment_id":"955135","timestamp":"1689667860.0","poster":"Parthasarathi","upvote_count":"7"},{"timestamp":"1682949240.0","content":"A: I passed the test","poster":"pk349","upvote_count":"1","comment_id":"886338"},{"comment_id":"877166","content":"Option A:\nhttps://aws.amazon.com/blogs/aws/s3-glacier-select/","poster":"flanfranco","upvote_count":"1","timestamp":"1682154240.0"},{"content":"I have searched a lot and couldn't find document stating glacier will not support compressed file. For me Glacier select is first choice and then s3 select considering cost.","upvote_count":"3","comment_id":"877064","timestamp":"1682143680.0","poster":"anjuvinayan"},{"timestamp":"1679772240.0","content":"Answer: B.\nNowhere have I read that Glacier Select cannot query compressed CSV files.","upvote_count":"3","comment_id":"850397","poster":"thirstylion"},{"content":"Selected Answer: A\nCorrect answer is A as AWS S3 Select enables querying S3 data on selected fields. As S3 Glacier Select does not support uncompressed data, it needs to be restored to S3.\n\nWith Amazon S3 Select, you can use simple structured query language (SQL) statements to filter the contents of an Amazon S3 object and retrieve just the subset of data that you need. By using Amazon S3 Select to filter this data, you can reduce the amount of data that Amazon S3 transfers, which reduces the cost and latency to retrieve this data.\n\nAmazon S3 Select works on objects stored in CSV, JSON, or Apache Parquet format. It also works with objects that are compressed with GZIP or BZIP2 (for CSV and JSON objects only), and server-side encrypted objects. You can specify the format of the results as either CSV or JSON, and you can determine how the records in the result are delimited.\n\nOption B is wrong as Archive objects that are queried by S3 Glacier Select must be formatted as uncompressed comma-separated values (CSV).\n\nOptions C & D are wrong as Athena and Redshift would add additional cost.","comment_id":"711923","comments":[{"upvote_count":"1","timestamp":"1697783760.0","comment_id":"1048464","content":"https://www.scaler.com/topics/aws/s3-glacier-select/\nbut in this document they have mentioned s3 glacier select support compressed gzip format","poster":"Arumugam_S"}],"poster":"cloudlearnerhere","upvote_count":"4","timestamp":"1667670840.0"},{"content":"A and B seems to be wrong according to the below statement: Amazon S3 Select scan range requests support Parquet, CSV (without quoted delimiters), and JSON objects (in LINES mode only). CSV and JSON objects must be uncompressed. For line-based CSV and JSON objects, when a scan range is specified as part of the Amazon S3 Select request, all records that start within the scan range are processed. For Parquet objects, all of the row groups that start within the scan range requested are processed.\n\nD is costly and hence only feasible ans I see is C.","comment_id":"707656","poster":"Rejju","upvote_count":"1","timestamp":"1667112480.0"},{"upvote_count":"1","comment_id":"707653","content":"Amazon S3 Select scan range requests support Parquet, CSV (without quoted delimiters), and JSON objects (in LINES mode only). CSV and JSON objects must be uncompressed. For line-based CSV and JSON objects, when a scan range is specified as part of the Amazon S3 Select request, all records that start within the scan range are processed. For Parquet objects, all of the row groups that start within the scan range requested are processed.","poster":"Rejju","timestamp":"1667112300.0"},{"poster":"Haimett","timestamp":"1666616100.0","content":"Selected Answer: A\nGZIP or BZIP2 - CSV and JSON files can be compressed using GZIP or BZIP2. GZIP and BZIP2 are the only compression formats that Amazon S3 Select supports for CSV and JSON files. Amazon S3 Select supports columnar compression for Parquet using GZIP or Snappy. Amazon S3 Select does not support whole-object compression for Parquet objects.","comment_id":"703028","upvote_count":"1"},{"poster":"Arka_01","content":"Selected Answer: A\nBoth Athena and Redshift are viable but way more costly than the S3 select option. Glacier Select cannot query on zipped data.","comment_id":"678519","upvote_count":"1","timestamp":"1664090160.0"},{"comment_id":"633815","poster":"rocky48","upvote_count":"2","timestamp":"1658287080.0","content":"Selected Answer: A\nA: is most cost effective and works to query a portion of csv data.\nB: This would be perfect to save even more costs, but Glacier Select doesn't work for compressed files.\nC: This doesn't work unless you first add the S3 data to a Glue Data Catalog. This is not mentioned in the answer but could be implied? Even then I think this is more expensive than a single S3 Select query.\nD: this is probably the most expensive option."},{"comment_id":"611036","content":"Selected Answer: A\nA: is most cost effective and works to query a portion of csv data. \nB: This would be perfect to save even more costs, but Glacier Select doesn't work for compressed files.\nC: This doesn't work unless you first add the S3 data to a Glue Data Catalog. This is not mentioned in the answer but could be implied? Even then I think this is more expensive than a single S3 Select query. \nD: this is probably the most expensive option. \n\nC and D are good solutions when you need to analyze the data on a regular basis. Redshift is the more powerful of the two.","poster":"Ramshizzle","upvote_count":"3","comments":[{"content":"where can i find s3 glacier select doesn't support compressed file?","poster":"Arumugam_S","timestamp":"1697783820.0","upvote_count":"1","comment_id":"1048466"}],"timestamp":"1654252860.0"},{"upvote_count":"1","content":"Selected Answer: A\nThis is for sure A. The keywords to observe are partial fetch, compressed data read, CSV format, S3. All of team tied together in most efficient and cost effective manner with S3 Select","timestamp":"1653839280.0","comment_id":"608822","poster":"Ob1KN0B"},{"timestamp":"1653206280.0","upvote_count":"1","poster":"Bik000","comment_id":"605274","content":"Selected Answer: A\nAnswer should be A"},{"content":"option A is right. You don't need Athena as you don't want to run complex aggregations etc. but want to query only subset of information. Of course you cant use glacier select as it does not work with compressed file.","comment_id":"601818","upvote_count":"1","timestamp":"1652569020.0","poster":"certificationJunkie"},{"upvote_count":"1","comment_id":"599905","content":"Selected Answer: A\nChanged to A.\nFor C, loading data to S3 will add cost. And scanning data by Athena will also add cost.","timestamp":"1652246100.0","poster":"MWL"},{"poster":"rrshah83","content":"Selected Answer: A\nGlacier select cannot be used for compressed data. Hence, S3 select will be the most cost-effective.","comment_id":"598551","upvote_count":"1","timestamp":"1652014080.0"},{"upvote_count":"1","timestamp":"1652013600.0","content":"Selected Answer: B\nGlacier selects lets you query portions of data directly from glacier. This will be the most cost-effective way.\nRefer to https://aws.amazon.com/blogs/aws/s3-glacier-select/","comment_id":"598549","poster":"rrshah83"},{"content":"A - S3 Select","poster":"jrheen","upvote_count":"1","comment_id":"595143","timestamp":"1651334400.0"},{"poster":"MWL","comments":[{"poster":"MWL","content":"Change to A, since loading data to S3 to add cost. And scanning data by Athena will also add cost.","timestamp":"1652246040.0","upvote_count":"1","comment_id":"599904"}],"content":"Selected Answer: C\nI choose C between A and C, although we an query with \"S3 select\", but it can only select from one file. But the question didn't say to get data from one month. it just query by vendor, maybe multiple months. So I vote for C, query with athena.","comment_id":"592965","timestamp":"1651046820.0","upvote_count":"1"},{"timestamp":"1650377400.0","content":"Selected Answer: A\nArchive objects that are queried by S3 Glacier Select must be formatted as uncompressed comma-separated values (CSV)","poster":"MWL","comment_id":"588225","upvote_count":"1"},{"content":"C is correct , Question asked to query data not only select statement , so s3 select cannot be used , file is compressed so glacier select cannot work , best option is c we is required solution","upvote_count":"1","timestamp":"1649230680.0","comment_id":"581681","poster":"sbxme"},{"poster":"pidkiller","timestamp":"1648320780.0","upvote_count":"2","comment_id":"575727","content":"Selected Answer: A\nI think A is the answer\n\nB- is incorrect because the file cannot be compressed\n\nhttps://docs.aws.amazon.com/amazonglacier/latest/dev/glacier-select.html\nArchive objects that are queried by S3 Glacier Select must be formatted as uncompressed comma-separated values (CSV).\n\n\nC and D they work since you are moving the data to S3 Standard\nbut Athena and Redshift are better for large data sets. Here it is just 100 MBs per month.\n\nSo I would answer A as the data set is so small that it could have almost no cost and no set up required.\nFor Athena you would have to create a database and querying costs applies.\nRedshift Spectrum, you would need a redshift cluster which is very expensive."},{"timestamp":"1648320720.0","upvote_count":"2","poster":"pidkiller","comment_id":"575726","content":"I think A is the answer\n\nB- is incorrect because the file cannot be compressed\n\nhttps://docs.aws.amazon.com/amazonglacier/latest/dev/glacier-select.html\nArchive objects that are queried by S3 Glacier Select must be formatted as uncompressed comma-separated values (CSV).\n\n\nC and D they work since you are moving the data to S3 Standard\nbut Athena and Redshift are better for large data sets. Here it is just 100 MBs per month. 1.2 GB per year.\n\nSo I would answer A as the data set is so small that it could have almost no cost and no set up required using S3 Select. Really simple\n\nFor Athena you would have to create a database and querying costs applies.\nRedshift Spectrum, you would need a redshift cluster which is very expensive."},{"content":"Answer is A. \nhttps://docs.aws.amazon.com/amazonglacier/latest/dev/glacier-select.html\n\nArchive objects that are queried by S3 Glacier Select must be formatted as uncompressed comma-separated values (CSV).","upvote_count":"1","timestamp":"1647321900.0","poster":"youonebe","comment_id":"568135"},{"upvote_count":"1","poster":"vkumarmi","content":"Selected Answer: A\nAthena doesn't allow extraction from Glacier and Glacier select requires file in uncompressed format.","timestamp":"1646051940.0","comment_id":"558040"},{"comment_id":"554001","content":"Answer is A - because you need only portion of data from large G-zip file and S3 select primary use case is to select portion of data.","timestamp":"1645562880.0","upvote_count":"1","poster":"Agn3001"},{"content":"Correct Answer is C. As per requirement of S3 Glacier Select, Archive objects that are queried by S3 Glacier Select must be formatted as uncompressed comma-separated values (CSV)","timestamp":"1645553100.0","upvote_count":"1","poster":"AWSRanger","comment_id":"553888"},{"content":"Selected Answer: B\nCorrect answer is B. I tested it.","upvote_count":"2","comment_id":"551039","timestamp":"1645286340.0","poster":"RSSRAO"},{"content":"Option A","comment_id":"547775","upvote_count":"1","timestamp":"1644930960.0","poster":"Dodai"},{"upvote_count":"2","poster":"Marvel_jarvis","comment_id":"499037","timestamp":"1639186320.0","content":"S3 select, Athena and Redshift Spectrum DOES NOT support querying data in S3 Glacier/Glacier deep archive. Answer is B (glacier select)"},{"upvote_count":"2","content":"I would go with A\nGlacier select will not work with compressed file\nAthena is best when you have multiple objects.","poster":"lakeswimmer","timestamp":"1638454740.0","comment_id":"492579"},{"comment_id":"485486","poster":"aws2019","timestamp":"1637709360.0","upvote_count":"1","content":"Ans is A"},{"poster":"aws2019","upvote_count":"1","comment_id":"478350","content":"Answer is A","timestamp":"1636926180.0"},{"timestamp":"1636247100.0","content":"Answer is A.\nWith S3 Select, you can use a simple SQL expression to return only the data from the store you’re interested in, instead of retrieving the entire object. Amazon S3 Select works on objects stored in CSV, JSON, or Apache Parquet format. It also works with objects compressed with GZIP or BZIP2 (for CSV and JSON objects only) and server-side encrypted objects. You can specify the format of the results as either CSV or JSON, and you can determine how the records in the output are delimited.\nIn contrast, cold data stored in Glacier can now be easily queried within minutes. The following are requirements for using S3 Glacier Select:\n\n- Archive objects queried by S3 Glacier Select must be formatted as uncompressed comma-separated values (CSV).\n\n- You must have an S3 bucket to work with. The AWS account you use to initiate an S3 Glacier Select job must have write permissions for the S3 bucket. The Amazon S3 bucket must be in the same AWS Region as the vault that contains the queried archive object.\n- You must have permission to call Get Job Output (GET output).","upvote_count":"7","poster":"Donell","comment_id":"387096"},{"poster":"anandkl80","comments":[{"comment_id":"509622","content":"Only select uncompressed, so not B","timestamp":"1640526120.0","upvote_count":"1","poster":"wentjiang"}],"content":"B. Query the data from Amazon S3 Glacier directly with Amazon Glacier Select.\nAnswer: https://aws.amazon.com/blogs/aws/s3-glacier-select/","comment_id":"338677","timestamp":"1635994680.0","upvote_count":"2"},{"upvote_count":"2","content":"Explanation:\nA. Load the data into Amazon S3 and query it with Amazon S3 Select. ==> Because it's cost effective solution to query data and also Query operation is simple select with filter.\nB. Query the data from Amazon S3 Glacier directly with Amazon Glacier Select. ==> is incorrect S3 Glacier can query uncompressed JSON,CSV only. \nC. Load the data to Amazon S3 and query it with Amazon Athena. ==> This plausible solution but not cost effective in this scenario, because you will be charged for data scan compared to Option A. \nD. Load the data to Amazon S3 and query it with Amazon Redshift Spectrum. ==> Including Redshift is not cost effective solution.","timestamp":"1635841440.0","comment_id":"283766","poster":"Naresh_Dulam"},{"timestamp":"1635733140.0","poster":"satya1280","content":"S3 Glacier Select Requirements and Quotas\nThe following are requirements for using S3 Glacier Select:\n\nArchive objects that are queried by S3 Glacier Select must be formatted as uncompressed comma-separated values (CSV).\n\nYou must have an S3 bucket to work with. In addition, the AWS account that you use to initiate a S3 Glacier Select job must have write permissions for the S3 bucket. The Amazon S3 bucket must be in the same AWS Region as the vault that contains the archive object that is being queried.\n\nYou must have permission to call Get Job Output (GET output).","comment_id":"280298","upvote_count":"1"},{"content":"A is the right answer","poster":"lostsoul07","comment_id":"274329","upvote_count":"1","timestamp":"1635670800.0"},{"comment_id":"262867","upvote_count":"2","poster":"ARUK2020","timestamp":"1635530700.0","content":"C\ncheck the limitation of Glacier select https://docs.aws.amazon.com/amazonglacier/latest/dev/glacier-select.html"},{"timestamp":"1635337380.0","comments":[{"timestamp":"1635394380.0","upvote_count":"2","poster":"Subho_in","comment_id":"260654","content":"Correction...Option B is invalid. Among others A is most cost effective"}],"poster":"Subho_in","content":"Option A is invalid. Among B, C and D; B is most cost effective.\nSo B is the most appropriate answer.","comment_id":"260651","upvote_count":"1"},{"comment_id":"216852","upvote_count":"1","timestamp":"1635284400.0","content":"A is correct!","poster":"BillyC"},{"comment_id":"205062","content":"A must be correct answer and NOT B. https://docs.aws.amazon.com/amazonglacier/latest/dev/glacier-select.html \nArchive objects that are queried by S3 Glacier Select must be formatted as uncompressed comma-separated values (CSV).","upvote_count":"1","poster":"sanjaym","timestamp":"1635143220.0"},{"comment_id":"191624","content":"C and D are wrong\nFrom link: https://docs.aws.amazon.com/amazonglacier/latest/dev/glacier-select.html\n\"Archive objects that are queried by S3 Glacier Select must be formatted as uncompressed comma-separated values (CSV).\"\nSo A is the answer","poster":"syu31svc","timestamp":"1635094740.0","upvote_count":"1"},{"upvote_count":"1","poster":"oluakins","timestamp":"1634569500.0","comment_id":"182351","content":"ok, just seen the bit about compressed file.\nA seems most correct then."},{"content":"I believe the answer is B. Isnt that the whole reason for Glacier Select?\nhttps://aws.amazon.com/glacier/features/#amazon-glacier-select","comment_id":"182350","poster":"oluakins","upvote_count":"3","timestamp":"1633833900.0"},{"comment_id":"179629","poster":"Karan_Sharma","timestamp":"1633628940.0","content":"Option A. It's the advisable choice when you want to query a small subset of data with minimal effort.","upvote_count":"1"},{"upvote_count":"4","content":"Why not B?\nhttps://docs.aws.amazon.com/amazonglacier/latest/dev/glacier-select.html","comment_id":"159175","poster":"mohitmittal94","comments":[{"comment_id":"159315","content":"\"Archive objects that are queried by S3 Glacier Select must be formatted as uncompressed comma-separated values (CSV\"\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emrfs-metadata.html","timestamp":"1633166220.0","upvote_count":"9","poster":"testtaker3434"},{"timestamp":"1633123020.0","poster":"testtaker3434","content":"Glacier select cant read compressed files","comment_id":"159314","upvote_count":"6"}],"timestamp":"1632879000.0"},{"comment_id":"158503","poster":"Priyanka_01","upvote_count":"3","timestamp":"1632857820.0","content":"A\nhttps://aws.amazon.com/blogs/aws/s3-glacier-select/"},{"content":"option A will work if the selected result is less than 40 MB. I think, A is the answer","upvote_count":"1","poster":"Prodip","comment_id":"154780","timestamp":"1632823200.0"},{"upvote_count":"3","content":"compressed data cant be queried using S3/Glacier select. Will go with option C","comment_id":"154777","poster":"Prodip","timestamp":"1632090780.0"},{"comment_id":"153580","poster":"testtaker3434","content":"Tricky question... I would pick A: It returns the file with only the data that is required.","upvote_count":"5","timestamp":"1632080880.0"}],"question_images":[],"exam_id":20,"isMC":true,"question_text":"Once a month, a company receives a 100 MB .csv file compressed with gzip. The file contains 50,000 property listing records and is stored in Amazon S3 Glacier.\nThe company needs its data analyst to query a subset of the data for a specific vendor.\nWhat is the most cost-effective solution?","timestamp":"2020-08-09 14:12:00","topic":"1","answer":"A","answers_community":["A (58%)","B (39%)","3%"],"choices":{"A":"Load the data into Amazon S3 and query it with Amazon S3 Select.","D":"Load the data to Amazon S3 and query it with Amazon Redshift Spectrum.","B":"Query the data from Amazon S3 Glacier directly with Amazon Glacier Select.","C":"Load the data to Amazon S3 and query it with Amazon Athena."},"question_id":105,"answer_images":[],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/27703-exam-aws-certified-data-analytics-specialty-topic-1-question/"}],"exam":{"isMCOnly":true,"isBeta":false,"numberOfQuestions":164,"id":20,"lastUpdated":"11 Apr 2025","provider":"Amazon","name":"AWS Certified Data Analytics - Specialty","isImplemented":true},"currentPage":21},"__N_SSP":true}