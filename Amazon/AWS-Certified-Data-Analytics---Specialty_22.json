{"pageProps":{"questions":[{"id":"dQF81WZlceMG9tSGRYNs","question_text":"A retail company is building its data warehouse solution using Amazon Redshift. As a part of that effort, the company is loading hundreds of files into the fact table created in its Amazon Redshift cluster. The company wants the solution to achieve the highest throughput and optimally use cluster resources when loading data into the company's fact table.\nHow should the company meet these requirements?","question_id":106,"answer_ET":"D","answer":"D","choices":{"B":"Use S3DistCp to load multiple files into the Hadoop Distributed File System (HDFS) and use an HDFS connector to ingest the data into the Amazon Redshift cluster.","C":"Use LOAD commands equal to the number of Amazon Redshift cluster nodes and load the data in parallel into each node.","A":"Use multiple COPY commands to load the data into the Amazon Redshift cluster.","D":"Use a single COPY command to load the data into the Amazon Redshift cluster."},"unix_timestamp":1597140960,"answer_description":"","answers_community":["D (89%)","11%"],"timestamp":"2020-08-11 12:16:00","exam_id":20,"answer_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/28066-exam-aws-certified-data-analytics-specialty-topic-1-question/","discussion":[{"poster":"Priyanka_01","upvote_count":"35","comment_id":"155320","timestamp":"1632083400.0","content":"D.\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-single-copy-command.html"},{"content":"Selected Answer: D\nCorrect answer is D as using a single COPY command would load the data in parallel.\n\nAmazon Redshift can automatically load in parallel from multiple compressed data files.\n\n\nHowever, if you use multiple concurrent COPY commands to load one table from multiple files, Amazon Redshift is forced to perform a serialized load. This type of load is much slower and requires a VACUUM process at the end if the table has a sort column defined.\n\nOption A is wrong as multiple COPY commands would force Redshift to perform a serialized load.\n\nOption B is wrong as using EMR just makes the solution complicated.\n\nOption C is wrong as there is no LOAD command with Redshift.","comment_id":"711925","timestamp":"1667670900.0","poster":"cloudlearnerhere","upvote_count":"9"},{"content":"Selected Answer: D\nD. https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-single-copy-command.html","comment_id":"1043860","timestamp":"1697344260.0","poster":"gofavad926","upvote_count":"1"},{"poster":"pk349","upvote_count":"2","comment_id":"886339","content":"D: I passed the test","timestamp":"1682949300.0"},{"comment_id":"877067","upvote_count":"2","timestamp":"1682144100.0","content":"Answer is D\nCopy Command is used to load data to redshift. Already single copy command load data in parallel.","poster":"anjuvinayan"},{"content":"Selected Answer: D\nSingle copy command is the correct answer.","upvote_count":"1","poster":"Arka_01","timestamp":"1664090220.0","comment_id":"678521"},{"poster":"fqc","comment_id":"644464","timestamp":"1660042200.0","content":"Selected Answer: D\nThe copy command is by default parallelized and effcient. It uses to load data from sources other than RedShift. If it is RedShift then use INSERT INTO or CREATE TABLE AS commans like in SQL.","upvote_count":"2"},{"poster":"fqc","comment_id":"644463","timestamp":"1660042140.0","upvote_count":"2","content":"Selected Answer: B\nThe copy command is by default parallelized and effcient. It uses to load data from sources other than RedShift. If it is RedShift then use INSERT INTO or CREATE TABLE AS commans like in SQL."},{"timestamp":"1658290320.0","poster":"rocky48","comment_id":"633823","content":"Selected Answer: D\nThe copy command is by default parallelized and efficient. It uses to load data from sources other than RedShift. If it is RedShift then use INSERT INTO or CREATE TABLE AS commas like in SQL. No point of creating expensive solution as given in B. \nThe answer should be D","upvote_count":"1"},{"poster":"dushmantha","timestamp":"1655622720.0","upvote_count":"1","content":"The copy command is by default parallelized and effcient. It uses to load data from sources other than RedShift. If it is RedShift then use INSERT INTO or CREATE TABLE AS commans like in SQL. No point of creating expensive solution as given in B. The answer should be D","comment_id":"618565"},{"comment_id":"605273","content":"Selected Answer: D\nAnswer is D","poster":"Bik000","upvote_count":"1","timestamp":"1653206220.0"},{"poster":"certificationJunkie","content":"It's D. The only requirement is that all the files should lie under a common directory and in the redshift copy command you need to pass the path till directory so that it will consider all the files inside it.","upvote_count":"1","timestamp":"1652871000.0","comment_id":"603233"},{"upvote_count":"1","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-single-copy-command.html","timestamp":"1651053960.0","comment_id":"593028","poster":"Teraxs"},{"poster":"RSSRAO","content":"D is correct answer","timestamp":"1644748080.0","comment_id":"546390","upvote_count":"1"},{"timestamp":"1643037060.0","upvote_count":"1","poster":"shenshu","content":"Selected Answer: D\nits D https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-single-copy-command.html","comment_id":"531396"},{"timestamp":"1636927380.0","comment_id":"478365","poster":"aws2019","content":"Option D\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-single-copy-command.html","upvote_count":"1"},{"timestamp":"1636163640.0","poster":"Billhardy","content":"Ans D","comment_id":"442344","upvote_count":"1"},{"comment_id":"387098","poster":"Donell","timestamp":"1635444300.0","upvote_count":"1","content":"Answer D"},{"comments":[{"timestamp":"1635271620.0","upvote_count":"3","comment_id":"368851","content":"EMR is expensive and not required. Single Copy command loads data in Parallel.\nAnswer is D.","poster":"Brijeshkrishna"}],"upvote_count":"1","poster":"AjithkumarSL","content":"I think option D will be better than the current scenario, however using S3DistCp is the most Efficient Way? Hence option B ?","timestamp":"1634650920.0","comment_id":"350575"},{"timestamp":"1634368680.0","content":"Explanation:\nA. Use multiple COPY commands to load the data into the Amazon Redshift cluster. ==> This will make the copy process is serialized \nB. Use S3DistCp to load multiple files into the Hadoop Distributed File System (HDFS) and use an HDFS connector to ingest the data into the Amazon Redshift cluster. => It's over engineering of introducing HDFS/EMR\nC. Use LOAD commands equal to the number of Amazon Redshift cluster nodes and load the data in parallel into each node. ==> It's same as Option A\nD. Use a single COPY command to load the data into the Amazon Redshift cluster. ==> Single copy command starts multiple process to load by taking advantage of multiple slices of node.","poster":"Naresh_Dulam","comments":[{"comment_id":"388910","content":"Actually, there is no LOAD command.","poster":"Huy","timestamp":"1635982560.0","upvote_count":"4"}],"comment_id":"283772","upvote_count":"3"},{"comment_id":"274330","timestamp":"1633693620.0","poster":"lostsoul07","content":"D is the right answer","upvote_count":"3"},{"poster":"BillyC","content":"D is correct!","comment_id":"216854","upvote_count":"1","timestamp":"1633546740.0"},{"timestamp":"1633151940.0","content":"100% D","poster":"sanjaym","upvote_count":"1","comment_id":"205063"},{"content":"D for sure.","comment_id":"175566","poster":"Paitan","upvote_count":"2","timestamp":"1632780720.0"},{"upvote_count":"2","timestamp":"1632595020.0","poster":"Nicki1013","comment_id":"169354","content":"My answer is D"},{"timestamp":"1632382560.0","poster":"zeronine","comment_id":"160798","content":"I think D..","upvote_count":"2"},{"poster":"testtaker3434","content":"Agreed its D","upvote_count":"2","timestamp":"1632306060.0","comment_id":"156369"}],"isMC":true,"question_images":[]},{"id":"ImvEIHwuoJapgW4KX0YX","question_images":[],"question_id":107,"answer_description":"","isMC":true,"question_text":"A data analyst is designing a solution to interactively query datasets with SQL using a JDBC connection. Users will join data stored in Amazon S3 in Apache ORC format with data stored in Amazon OpenSearch Service (Amazon Elasticsearch Service) and Amazon Aurora MySQL.\nWhich solution will provide the MOST up-to-date results?","unix_timestamp":1650534780,"answers_community":["D (91%)","6%"],"topic":"1","answer_ET":"D","answer":"D","discussion":[{"comment_id":"711929","comments":[{"timestamp":"1667671140.0","upvote_count":"6","content":"Presto is an open-source distributed SQL query engine optimized for low-latency, ad-hoc analysis of data. It supports the ANSI SQL standard, including complex queries, aggregations, joins, and window functions. Presto can process data from multiple data sources including the Hadoop Distributed File System (HDFS) and Amazon S3.\n\nPresto uses a custom query execution engine with operators designed to support SQL semantics. Different from Hive/MapReduce, Presto executes queries in memory, pipelined across the network between stages, thus avoiding unnecessary I/O. The pipelined execution model runs multiple stages in parallel and streams data from one stage to the next as it becomes available.\n\nPresto supports the ANSI SQL standard, which makes it easy for data analysts and developers to query both structured and unstructured data at scale. Currently, Presto supports a wide variety of SQL functionality, including complex queries, aggregations, joins, and window functions.","poster":"cloudlearnerhere","comment_id":"711930"}],"upvote_count":"17","content":"Selected Answer: D\nCorrect answer is D as Presto is a fast SQL query engine designed for interactive analytic queries over large datasets from multiple sources.\n\nOption A is wrong as Glue is not ideal for interactive queries but more for batch ETL jobs.\n\nOption B is wrong as it would not provide the up-to-date results as the data needs to copied over to Redshift for querying. Also, it does not cover S3 which would need Redshift Spectrum.\n\nOption C is wrong as Spark SQL does not allow the capability to query multiple data sources. Also, Glue Developer Endpoints help test Glue ETL jobs.","poster":"cloudlearnerhere","timestamp":"1667671080.0"},{"upvote_count":"8","poster":"CHRIS12722222","comment_id":"589240","content":"Answer = D (use presto)","timestamp":"1650534780.0"},{"upvote_count":"1","timestamp":"1682949360.0","content":"D: I passed the test","comment_id":"886340","poster":"pk349"},{"timestamp":"1682144340.0","poster":"anjuvinayan","comment_id":"877071","upvote_count":"1","content":"Answer is D\nA-not upto date data as its glue job\nB- Up-to-date data as its DMS but DMS to ES integeration not possible\nC-Not interactive"},{"content":"Selected Answer: D\nDisparate data sources are present and OpenSearch data cannot be ingested via DMS.","comment_id":"678522","timestamp":"1664090280.0","poster":"Arka_01","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: D\nAnswer = D","timestamp":"1659818640.0","comment_id":"643526","poster":"rocky48"},{"poster":"Bik000","timestamp":"1653133500.0","comment_id":"604875","upvote_count":"2","content":"Selected Answer: D\nAnswer should be D"},{"content":"Selected Answer: D\nAnswer should be D","upvote_count":"2","comment_id":"604873","timestamp":"1653133380.0","poster":"Bik000"},{"upvote_count":"3","comment_id":"599395","poster":"MWL","content":"Selected Answer: D\nFor A, I didn't find document about exporting data from open search to S3.\nB: DMS doesn't support to export from ES.\nC: to use spark SQL to query from ES, we also need a third-party connector, so C is not complete. \nD: should work.","timestamp":"1652156520.0"},{"timestamp":"1650996300.0","upvote_count":"1","poster":"chp2022","comment_id":"592618","content":"Selected Answer: B\nI say it should be B"},{"timestamp":"1650754620.0","content":"Selected Answer: D\nD is correct","poster":"AWSRanger","upvote_count":"1","comment_id":"590824"},{"poster":"Tsyva","upvote_count":"1","comment_id":"590213","timestamp":"1650654720.0","content":"Selected Answer: B\nIMO the answer should be B since its highlighted that it must be most up to-date results and DMS supports change data capture."},{"timestamp":"1650628140.0","comment_id":"589952","content":"Selected Answer: D\nMost up-to-date -> in-place queries if possible, so Presto. Athena solution implies moving data from RDS to S3 so adds a potential delay (S3 is not real-time)","poster":"rb39","upvote_count":"2"},{"comment_id":"589444","timestamp":"1650549540.0","poster":"[Removed]","upvote_count":"1","comments":[{"timestamp":"1669224060.0","upvote_count":"1","poster":"Lazy_Lord","content":"You can use JDBC on the Presto running on EMR: \nhttps://docs.aws.amazon.com/emr/latest/ReleaseGuide/presto-adding-db-connectors.html","comment_id":"725292"}],"content":"Selected Answer: A\nJDBC connection is a key. so Athena is the answer"}],"choices":{"A":"Use AWS Glue jobs to ETL data from Amazon ES and Aurora MySQL to Amazon S3. Query the data with Amazon Athena.","D":"Query all the datasets in place with Apache Presto running on Amazon EMR.","C":"Query all the datasets in place with Apache Spark SQL running on an AWS Glue developer endpoint.","B":"Use Amazon DMS to stream data from Amazon ES and Aurora MySQL to Amazon Redshift. Query the data with Amazon Redshift."},"exam_id":20,"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/74000-exam-aws-certified-data-analytics-specialty-topic-1-question/","timestamp":"2022-04-21 11:53:00"},{"id":"9WgabvgttzNkvt30FME8","url":"https://www.examtopics.com/discussions/amazon/view/28633-exam-aws-certified-data-analytics-specialty-topic-1-question/","isMC":true,"timestamp":"2020-08-15 09:54:00","discussion":[{"upvote_count":"48","content":"A ? Any thoughts \nhttps://aws.amazon.com/blogs/big-data/analyzing-aws-waf-logs-with-amazon-es-amazon-athena-and-amazon-quicksight/","poster":"Priyanka_01","comments":[{"upvote_count":"1","comment_id":"480433","content":"https://docs.aws.amazon.com/athena/latest/ug/waf-logs.html","timestamp":"1637207760.0","poster":"attaraya"},{"upvote_count":"1","comment_id":"723328","poster":"nadavw","timestamp":"1669021920.0","content":"from the link: If your use case requires the analysis of data in real time, then Amazon OpenSearch Service is more suitable for your needs. If you prefer a serverless approach that doesn’t require capacity planning or cluster management, then the solution with AWS Glue, Athena, and Amazon QuickSight is more suitable."}],"comment_id":"158518","timestamp":"1632104340.0"},{"content":"\"infrequently\" is a typical keyword for athena. Same as \"ad-hoc\" .","comment_id":"227484","timestamp":"1634375100.0","poster":"hans1234","upvote_count":"16"},{"upvote_count":"1","poster":"apk123457890","comment_id":"1046809","content":"Why not B? the question doesn't restrict cost parameter and this will be with least development effort","timestamp":"1697624040.0"},{"content":"A: I passed the test","comment_id":"886342","poster":"pk349","timestamp":"1682949540.0","upvote_count":"2"},{"timestamp":"1682144880.0","upvote_count":"1","poster":"anjuvinayan","content":"Answer is A\nB. ES cost is high\nC. redshift cost is High\nD.EMR is less costly than Glue but nothing mentioned about queries, just job\n\nD.EMR is less costly than Glue. But development effort is more considering cluster creation","comment_id":"877077"},{"poster":"srirnag","content":"Why not B. Not a single line of code required. It just requires a streaming config in ES and configuration of Kibana. It is B for me.","timestamp":"1677087180.0","comments":[{"poster":"anjuvinayan","comment_id":"877073","timestamp":"1682144760.0","upvote_count":"1","content":"Cost of ES is high"}],"upvote_count":"2","comment_id":"818110"},{"upvote_count":"2","timestamp":"1667672160.0","comments":[{"upvote_count":"2","timestamp":"1682144820.0","comment_id":"877076","content":"EMR is less costly than Glue. But development effort is more considering cluster creation","poster":"anjuvinayan"}],"poster":"cloudlearnerhere","content":"Selected Answer: A\nCorrect answer is A. Using Glue crawler over S3 data can be used to create a data catalog, that can be queried using Athena and visualized using QuickSight. This solution does not require additional resources, data duplication and uses serverless managed services.\nOption B is wrong as Elasticsearch cluster would not provide a cost-effective solution.\n\nOption C is wrong as Redshift cluster with a Lambda job would not provide a cost-effective solution, and would also need development effort.\n\n\nOption D is wrong as EMR cluster with a Spark job would not provide a cost-effective solution, and would also need development effort.","comment_id":"711942"},{"upvote_count":"2","content":"Selected Answer: A\n\"low-cost option to perform this infrequent data analysis with visualizations of logs in a way that requires minimal development effort\" - This is the key. As the solution is required for infrequent analysis, so OpenSearch will be costlier solution than a combination of Athena and QuickSight.","timestamp":"1664090400.0","comment_id":"678523","poster":"Arka_01"},{"content":"Selected Answer: A\nA is the answer as its cost effective.","poster":"rocky48","comment_id":"634355","timestamp":"1658379060.0","upvote_count":"1"},{"poster":"jpratik1","upvote_count":"1","timestamp":"1656352920.0","comment_id":"623437","content":"Selected Answer: A\nLow cost and rare"},{"comment_id":"605265","content":"Either A or B","timestamp":"1653205920.0","upvote_count":"1","poster":"Bik000"},{"poster":"Jayproton","upvote_count":"1","comment_id":"597325","content":"Selected Answer: A\nPer this https://aws.amazon.com/blogs/big-data/analyzing-aws-waf-logs-with-amazon-es-amazon-athena-and-amazon-quicksight/ A should be answer","timestamp":"1651757700.0"},{"poster":"Crypt0zknight","comment_id":"501604","upvote_count":"1","content":"A\nhttps://docs.aws.amazon.com/athena/latest/ug/when-should-i-use-ate.html\nIntegrates easily with Glue and Quicksight","timestamp":"1639506900.0"},{"content":"A is prefered","comment_id":"480277","upvote_count":"2","timestamp":"1637183400.0","poster":"aws2019"},{"comment_id":"475237","timestamp":"1636523520.0","poster":"Chints01","upvote_count":"1","content":"Answer is A if the ask is low-cost (asked in this question)\nB - this should be the answer if the ask is to have the best solution for log analysis (this is not cost efficient)\nC and D can be ruled out for obvious reasons of including so many services when the requirement can be met without them"},{"content":"Answer A","comment_id":"387105","timestamp":"1635714300.0","upvote_count":"1","poster":"Donell"},{"content":"A is the answer as its cost effective. Athena and Quicksight Standard are very cost effective compared to Elasticsearch Cluster which is expensive.","timestamp":"1635516600.0","comment_id":"368860","upvote_count":"2","poster":"Brijeshkrishna"},{"content":"It is B. Architecture already has KDF, use the same KDF and send the logs to ES. We know KDF can send data to ES. On top of that Kibana is a free visualization tool available on ES. \"B\" is the most cost effective option available","timestamp":"1635242640.0","upvote_count":"2","comments":[{"content":"What about ES cluster cost?","timestamp":"1635276660.0","comments":[{"upvote_count":"2","comment_id":"316986","timestamp":"1635457440.0","content":"and what about \"infrequent data analysis with visualizations\"","poster":"yuliaqwerty"}],"comment_id":"316983","upvote_count":"3","poster":"yuliaqwerty"}],"comment_id":"316559","poster":"umatrilok"},{"timestamp":"1635081480.0","content":"As Athena is serverless and ES is not serverless, I will go with A.","poster":"gopikrishna7354","upvote_count":"1","comment_id":"311157"},{"content":"A is the right answer","upvote_count":"1","timestamp":"1634700060.0","poster":"lostsoul07","comment_id":"274334"},{"timestamp":"1634346240.0","poster":"BillyC","upvote_count":"1","comment_id":"216862","content":"A is correct!"},{"upvote_count":"1","content":"A is the correct answer","comment_id":"208255","poster":"jove","timestamp":"1634216520.0"},{"upvote_count":"1","content":"Answer is A for sure.","comment_id":"205077","timestamp":"1634114220.0","poster":"sanjaym"},{"comment_id":"175568","poster":"Paitan","timestamp":"1634063820.0","upvote_count":"3","content":"A is the right and most cost effective option here."},{"content":"It is B. ElasticSearch is the most popular option for log analytics. They are asking for a low-cost solution, then, Kibana is free. Take a look of this link.\nhttps://aws.amazon.com/blogs/security/how-to-analyze-aws-waf-logs-using-amazon-elasticsearch-service/","comment_id":"165989","comments":[{"comment_id":"166409","content":"Agreed : https://aws.amazon.com/blogs/security/deploy-dashboard-for-aws-waf-minimal-effort/ Thanks for pointing out.","comments":[{"upvote_count":"2","timestamp":"1633031040.0","comment_id":"166411","poster":"awssp12345","comments":[{"timestamp":"1633935900.0","upvote_count":"3","comment_id":"172953","poster":"awssp12345","content":"I would go with A."},{"upvote_count":"5","comment_id":"167344","content":"I change my answer to A as B seems to be over designed for infrequent analysis.\nhttps://aws.amazon.com/blogs/big-data/analyzing-aws-waf-logs-with-amazon-es-amazon-athena-and-amazon-quicksight/","poster":"ramozo","timestamp":"1633564560.0"}],"content":"I am skeptical if this is low cost or not."}],"timestamp":"1632941100.0","upvote_count":"1","poster":"awssp12345"},{"content":"this is not low cost, you will pay for the ES cluster while running queries or not, on Athena you pay only when you run queries","timestamp":"1633281720.0","upvote_count":"6","comment_id":"167089","poster":"ali_baba_acs"}],"timestamp":"1632678360.0","poster":"ramozo","upvote_count":"3"},{"content":"Can't be C or D as Lambda or Spark would require you to code and the question says \"minimum development efforts\"\nBetween A and B, ES is costlier than Quicksight, so I think B is right.","timestamp":"1632173580.0","poster":"mohitmittal94","upvote_count":"1","comments":[{"content":"Replying to my own answer, there was I typo in my last line, I think A is right.","comment_id":"159180","upvote_count":"1","poster":"mohitmittal94","timestamp":"1632379800.0"}],"comment_id":"159179"}],"topic":"1","exam_id":20,"answer_ET":"A","choices":{"A":"Use an AWS Glue crawler to create and update a table in the Glue data catalog from the logs. Use Athena to perform ad-hoc analyses and use Amazon QuickSight to develop data visualizations.","D":"Create an Amazon EMR cluster and use Amazon S3 as the data source. Create an Apache Spark job to perform ad-hoc analyses and use Amazon QuickSight to develop data visualizations.","B":"Create a second Kinesis Data Firehose delivery stream to deliver the log files to Amazon OpenSearch Service (Amazon Elasticsearch Service). Use Amazon ES to perform text-based searches of the logs for ad-hoc analyses and use OpenSearch Dashboards (Kibana) for data visualizations.","C":"Create an AWS Lambda function to convert the logs into .csv format. Then add the function to the Kinesis Data Firehose transformation configuration. Use Amazon Redshift to perform ad-hoc analyses of the logs using SQL queries and use Amazon QuickSight to develop data visualizations."},"answer_description":"","question_id":108,"question_images":[],"question_text":"A company developed a new elections reporting website that uses Amazon Kinesis Data Firehose to deliver full logs from AWS WAF to an Amazon S3 bucket.\nThe company is now seeking a low-cost option to perform this infrequent data analysis with visualizations of logs in a way that requires minimal development effort.\nWhich solution meets these requirements?","answer_images":[],"unix_timestamp":1597478040,"answers_community":["A (100%)"],"answer":"A"},{"id":"sRoFnQgSC7ewJY9T08U7","url":"https://www.examtopics.com/discussions/amazon/view/28627-exam-aws-certified-data-analytics-specialty-topic-1-question/","question_id":109,"unix_timestamp":1597467600,"answer_description":"","answer":"C","exam_id":20,"answer_ET":"C","choices":{"A":"Consolidate all AWS accounts into one account. Create different S3 buckets for each department and move all the data from every account to the central data lake account. Migrate the individual data catalogs into a central data catalog and apply fine-grained permissions to give to each user the required access to tables and databases in AWS Glue and Amazon S3.","C":"Set up an individual AWS account for the central data lake. Use AWS Lake Formation to catalog the cross-account locations. On each individual S3 bucket, modify the bucket policy to grant S3 permissions to the Lake Formation service-linked role. Use Lake Formation permissions to add fine-grained access controls to allow senior analysts to view specific tables and columns.","B":"Keep the account structure and the individual AWS Glue catalogs on each account. Add a central data lake account and use AWS Glue to catalog data from various accounts. Configure cross-account access for AWS Glue crawlers to scan the data in each departmental S3 bucket to identify the schema and populate the catalog. Add the senior data analysts into the central account and apply highly detailed access controls in the Data Catalog and Amazon S3.","D":"Set up an individual AWS account for the central data lake and configure a central S3 bucket. Use an AWS Lake Formation blueprint to move the data from the various buckets into the central S3 bucket. On each individual bucket, modify the bucket policy to grant S3 permissions to the Lake Formation service-linked role. Use Lake Formation permissions to add fine-grained access controls for both associate and senior analysts to view specific tables and columns."},"timestamp":"2020-08-15 07:00:00","topic":"1","isMC":true,"answer_images":[],"answers_community":["C (88%)","12%"],"discussion":[{"comment_id":"490357","timestamp":"1638240240.0","upvote_count":"15","poster":"Thiya","content":"Answer is C. I have implemented federated data lake."},{"upvote_count":"10","timestamp":"1667696460.0","comment_id":"712104","poster":"cloudlearnerhere","content":"Selected Answer: C\nCorrect answer is C as AWS Data Lake Formation can help provide a centralized place for maintaining data catalog to various locations, without moving the data. Also, AWS Lake Formation permissions can help provide a central access control location.\n\nOption A is wrong as consolidating accounts would increase administrative tasks.\n\nOption B is wrong as although it might work, it is more simpler to use AWS Lake Formation for access control.\n\nOption D is wrong as moving all the data to central S3 would duplicate the storage cost and increase administrative tasks."},{"content":"accounts consolidation has a big administrative effort... then A is discarded...\n\nB works but doesn't have permission requirements for an analyst role... then B is discarded...\n\nif we talk about fine-grained access control and the strong power of data catalog, lake formation always is the better option... not expansive and easy to use...\n\na central bucket is a big administrative effort and increases storage costs due to data storage duplication... then D is discarded...\n\nC any througs?","comment_id":"1120071","upvote_count":"1","timestamp":"1704998220.0","poster":"GCPereira"},{"timestamp":"1682949600.0","poster":"pk349","content":"C: I passed the test","comment_id":"886344","upvote_count":"1"},{"content":"Selected Answer: B\nSeems to be Data Lake Formation\nSimplified data lake setup: Streamlines creation and configuration of a centralized data lake.\nFine-grained access control: Enables table and column-level permissions for users and groups.\nData cataloging and discovery: Facilitates a searchable, centralized data catalog using AWS Glue.\nData transformation: Supports ETL jobs to clean, enrich, and prepare data for analysis.\nIntegration with AWS services: Seamlessly connects with various AWS analytics and processing tools.\nSecurity and compliance: Ensures data encryption, monitors access, and provides audit logs.","poster":"uk_dataguy","upvote_count":"1","timestamp":"1682624280.0","comment_id":"882990"},{"upvote_count":"2","poster":"anjuvinayan","content":"Answer is C\nA-Move all data means cost and lot of effort\nB-It works but Lakeformation is easy\nC-Answer\nD- Move all data means cost and lot of effort","timestamp":"1682146320.0","comment_id":"877090"},{"comment_id":"843760","upvote_count":"1","content":"Selected Answer: B\nOption C is incorrect because it requires setting up an individual AWS account for the central datalake. This would be an unnecessary expense. It also requires using AWS Lake Formation to catalog the cross-account locations. This would be a time-consuming and expensive process.\n\nAWS Lake Formation is a service that makes it easy to set up a secure data lake in days. It simplifies and automates many of the complex manual steps required to create a data lake, including collecting, cleaning, and cataloging data. You can use AWS Lake Formation to create a central data catalog that is accessible to all departments. You can use Lake Formation permissions to add fine-grained access controls to allow senior analysts to view specific tables and columns.\n\nHowever, setting up an individual AWS account for the central datalake would be an unnecessary expense. It would also require additional administrative overhead to manage the different accounts.","timestamp":"1679231160.0","poster":"akashm99101001com"},{"poster":"rags1482","upvote_count":"1","comment_id":"831445","content":"Option B, keeps the account structure and the individual AWS Glue catalogs on each account, but still allows for a centralized catalog using AWS Glue. It uses cross-account access for AWS Glue crawlers to scan the data in each departmental S3 bucket and identify the schema, which populates the central catalog. The senior data analysts can be added to the central account with highly detailed access controls in the Data Catalog and Amazon S3. This approach is more scalable and cost-effective in cases where there are many departments or AWS accounts involved.\n\nOption C is a valid solution to the problem described, but it may not be the most cost-effective and efficient one. Setting up an individual AWS account for the central data lake and using AWS Lake Formation to catalog the cross-account locations with fine-grained access controls for senior analysts is a good approach, but it may involve additional administrative tasks and costs. Additionally, modifying the bucket policy for each individual S3 bucket may be cumbersome and error-prone.","timestamp":"1678151340.0"},{"poster":"Arka_01","upvote_count":"1","content":"Selected Answer: C\nLake Formation for such fine grained access. Also, no need to use Lake Formation BluePrint as data source is S3.","comment_id":"678527","timestamp":"1664090880.0"},{"comment_id":"650487","upvote_count":"1","content":"should access to the subset of columns means fine-grained access control. It can be implemented by Lake Formation, not individual S3 buckets.\nSo the answer is C.","poster":"muhsin","timestamp":"1661214960.0"},{"timestamp":"1658806560.0","upvote_count":"1","poster":"rocky48","comment_id":"637109","content":"Selected Answer: C\nSelected Answer: C"},{"poster":"Bik000","comment_id":"604826","content":"Selected Answer: C\nAnswer is C","upvote_count":"2","timestamp":"1653130560.0"},{"poster":"MWL","upvote_count":"1","comment_id":"597948","content":"Selected Answer: C\nUse AWS Lake Formation for cross account catalog and permission.","timestamp":"1651890240.0"},{"upvote_count":"1","poster":"aws2019","timestamp":"1637710080.0","content":"Answer should be C.","comment_id":"485490"},{"content":"When did B say..to move the data.... isn't catalog of data and moving the data two different things?","timestamp":"1636261980.0","poster":"yogen","comment_id":"426310","upvote_count":"1"},{"content":"C is the correct answer","timestamp":"1636091880.0","comment_id":"316562","poster":"umatrilok","upvote_count":"2"},{"upvote_count":"2","poster":"lostsoul07","content":"C is the right answer","comment_id":"279049","timestamp":"1635632220.0"},{"timestamp":"1635345060.0","poster":"civ","upvote_count":"2","comment_id":"255968","content":"Something went wrong. My previous comment was meant for question 50 but ended up here. :("},{"comment_id":"255904","content":"Redshift is not for near real time - drop a and c. Glue is not for near real time - drop d. So the answer is b.","poster":"civ","timestamp":"1635240660.0","upvote_count":"1"},{"timestamp":"1635107820.0","poster":"mbaexam","content":"I will go with C. Moving data around is not a good idea, it will make security even more complex + data storage cost.","upvote_count":"3","comment_id":"223630"},{"poster":"Shreejay21","comment_id":"218308","timestamp":"1635000180.0","comments":[{"poster":"lydiaki","comments":[{"poster":"kempstonjoystick","timestamp":"1635607980.0","comments":[{"poster":"Japanese1","comment_id":"548557","timestamp":"1645015080.0","content":"Have you ever operated in a situation like the one described in this question?\nIf you have a lot of data and users, managing permissions with Glue and S3 can be very complicated.\nLake Formation is a must for optimizing permission management operations.\nThis satisfies the \"solution that minimizes administrative tasks\" in the question text.","upvote_count":"2"}],"content":"It's more administrative effort than C, though, and the question states:- \"Which solution achieves these required access patterns to minimize costs and administrative tasks?\"","upvote_count":"1","comment_id":"261802"}],"content":"Any insight you found on this? I also thought B was correct.","upvote_count":"1","timestamp":"1635395280.0","comment_id":"257605"}],"content":"B says\nAdd a central data lake account and use AWS Glue to catalog data from various accounts.\nC says \nUse AWS Lake Formation to catalog the cross-account locations.\n\nIsn't B more accurate? Any reason for discarding B as an answer.","upvote_count":"3"},{"content":"C is good choice so administrative task and cost can be minimized","upvote_count":"2","comment_id":"214469","poster":"PareshRane","timestamp":"1634899800.0"},{"content":"Answer should be C.","upvote_count":"1","timestamp":"1634272260.0","poster":"sanjaym","comment_id":"205079"},{"comment_id":"196673","poster":"jack42","upvote_count":"2","timestamp":"1633731360.0","content":"Answer should be D, because C only talks senior analysts and missing associates on fine grained access control","comments":[{"poster":"abhineet","comment_id":"199897","timestamp":"1633744140.0","upvote_count":"2","content":"No Dude, C also says \"On each individual S3 bucket, modify the bucket policy to grant S3 permissions to the Lake Formation service-linked role.\" which takes care of associates then further apply finer controls for senior analyst. think will you move 100s of TB or PBs of data?"}]},{"comments":[{"comment_id":"192827","timestamp":"1633643100.0","content":"Then again moving all the data to central S3 would duplicate the storage cost and increase administrative tasks so C is the answer","upvote_count":"1","poster":"syu31svc"},{"content":"blueprint doesnt give option to move S3 data","timestamp":"1636234020.0","upvote_count":"1","comment_id":"342684","poster":"sivajiboss"}],"poster":"syu31svc","timestamp":"1633213440.0","upvote_count":"2","comment_id":"191863","content":"Personally I would choose D as you would need a central S3 bucket to act as the Data Lake. Furthermore, a blueprint is needed to move data"},{"timestamp":"1633122120.0","content":"I am not sure. But between C and D will obviously chose C.","upvote_count":"2","poster":"Paitan","comment_id":"175571"},{"poster":"zeronine","timestamp":"1632912720.0","upvote_count":"3","comment_id":"162241","content":"My answer is C"},{"poster":"Priyanka_01","upvote_count":"1","content":"D? Any thoughts","comments":[{"content":"Could be the option C. Moving the data in the option D is not optimal. https://aws.amazon.com/blogs/big-data/access-and-manage-data-from-multiple-accounts-from-a-central-aws-lake-formation-account/","upvote_count":"10","comments":[{"timestamp":"1632450480.0","upvote_count":"2","comment_id":"161758","content":"Agree with C.","poster":"carol1522"},{"poster":"awssp12345","content":"Agree with this! \n\nLake Formation provides secure and granular access to data through a new grant/revoke permissions model that augments AWS Identity and Access Management (IAM) policies.\n\nAnalysts and data scientists can use the full portfolio of AWS analytics and machine learning services, such as Amazon Athena, to access the data. The configured Lake Formation security policies help ensure that users can access only the data that they are authorized to access. \n\nSource : https://docs.aws.amazon.com/lake-formation/latest/dg/how-it-works.html","comment_id":"166857","timestamp":"1633014780.0","upvote_count":"8"}],"timestamp":"1632404100.0","comment_id":"161062","poster":"skar21"}],"comment_id":"158453","timestamp":"1632115740.0"}],"question_text":"A large company has a central data lake to run analytics across different departments. Each department uses a separate AWS account and stores its data in an\nAmazon S3 bucket in that account. Each AWS account uses the AWS Glue Data Catalog as its data catalog. There are different data lake access requirements based on roles. Associate analysts should only have read access to their departmental data. Senior data analysts can have access in multiple departments including theirs, but for a subset of columns only.\nWhich solution achieves these required access patterns to minimize costs and administrative tasks?","question_images":[]},{"id":"DL97H2qD5WNAko2c0W1w","unix_timestamp":1596997380,"choices":{"D":"Enable job bookmarks in AWS Glue to estimate the number of data processing units (DPUs). Based on the profiled metrics, increase the value of the num- executors job parameter.","B":"Enable job metrics in AWS Glue to estimate the number of data processing units (DPUs). Based on the profiled metrics, increase the value of the maximum capacity job parameter.","C":"Enable job metrics in AWS Glue to estimate the number of data processing units (DPUs). Based on the profiled metrics, increase the value of the spark.yarn.executor.memoryOverhead job parameter.","A":"Enable job bookmarks in AWS Glue to estimate the number of data processing units (DPUs). Based on the profiled metrics, increase the value of the executor- cores job parameter."},"answer_ET":"B","discussion":[{"upvote_count":"15","poster":"Donell","timestamp":"1635863160.0","content":"Answer: B \nB. Enable job metrics in AWS Glue to estimate the number of data processing units (DPUs). Based on the profiled metrics, increase the value of the maximum capacity job parameter.\n\nSimilar question is there in Jon Bonso's practice exam.","comment_id":"385991"},{"timestamp":"1667562780.0","poster":"cloudlearnerhere","upvote_count":"6","comment_id":"711097","content":"Correct answer is B as job metrics can be used to estimate the number of DPUs needed.\n\nOptions A & D are wrong as Job bookmarks help AWS Glue maintain state information and prevent the reprocessing of old data.\n\n\nOptions A & D are wrong as Job bookmarks help AWS Glue maintain state information and prevent the reprocessing of old data."},{"upvote_count":"1","poster":"gofavad926","content":"Selected Answer: B\nB. Enable job metrics in AWS Glue to estimate the number of data processing units (DPUs). Based on the profiled metrics, increase the value of the maximum capacity job parameter.","timestamp":"1697292000.0","comment_id":"1043506"},{"upvote_count":"1","comment_id":"968515","poster":"NikkyDicky","content":"Selected Answer: B\nIt's a B","timestamp":"1690840020.0"},{"comment_id":"886255","upvote_count":"1","content":"B: I passed the test","poster":"pk349","timestamp":"1682946120.0"},{"comment_id":"835845","upvote_count":"5","timestamp":"1678531740.0","poster":"AwsNewPeople","content":"B. Enable job metrics in AWS Glue to estimate the number of data processing units (DPUs). Based on the profiled metrics, increase the value of the maximum capacity job parameter.\n\nThe data analyst should enable job metrics in AWS Glue to estimate the number of data processing units (DPUs) and profile the job to understand its resource requirements. Based on the profiled metrics, the data analyst should increase the value of the maximum capacity job parameter. This parameter controls the maximum number of DPUs that the job can use. By increasing the maximum capacity, the job can use more resources and complete faster without overprovisioning. Enabling job bookmarks can help with incremental processing but will not directly improve job execution time. Increasing the value of the executor-cores job parameter or the spark.yarn.executor.memoryOverhead job parameter may improve performance, but these parameters depend on the specific job requirements and are not directly related to the job's resource utilization. Similarly, increasing the num-executors job parameter will not directly improve job execution time."},{"upvote_count":"2","comment_id":"637126","content":"Selected Answer: B\nAnswer: B","poster":"rocky48","timestamp":"1658807280.0"},{"upvote_count":"2","comments":[{"content":"Clear like water","timestamp":"1704725160.0","poster":"teo2157","comment_id":"1116690","upvote_count":"1"}],"timestamp":"1636029900.0","poster":"killohotel","content":"정답:B\n북마크는 상태를 표시하는것이기때문에 A,D는 제외.\n메트릭을 통해서 dpu적정 개수를 추정하고, 최대용량을 늘리면 됩니다.\n오버헤드 파라미터는 에러가 발생했을때 수정해야하지만, 문제는 3시간동안 러닝상태이기때문에 에러가 난것은 아니므로 C 제외\nhttps://docs.aws.amazon.com/ko_kr/glue/latest/dg/monitor-debug-capacity.html#monitor-debug-capacity-fix","comment_id":"442511"},{"poster":"Donell","timestamp":"1635992160.0","content":"I suggest taking Jon Bonso's practice exams too.","comment_id":"392633","upvote_count":"1"},{"content":"This question is quite confused because for Glue version 2.0 jobs, you cannot instead specify a Maximum capacity. Instead, you should specify a Worker type and the Number of workers. However since A, D (no such parameters) and C (memoryOverhead not help in this case) are wrong, the best choice is B","comment_id":"387607","timestamp":"1635932280.0","poster":"Huy","upvote_count":"3"},{"poster":"Shraddha","timestamp":"1635725100.0","upvote_count":"1","content":"Ans B\nA and D = wrong, the name “bookmark” suggests persistency of a in-progress state, and so it is, used to track processed data, not for scaling. C = wrong, although you can set this parameter, the job metrics won’t help you, and this parameter won’t help long job running times because that was due to lack of computational power not memory.","comment_id":"385241"},{"content":"B is correct.","upvote_count":"1","comment_id":"377975","poster":"gunjan4392","timestamp":"1635613680.0"},{"timestamp":"1634597760.0","comment_id":"284606","poster":"Exia","content":"B. \n\nA, D. Bookmark is not used for monitoring ETL job status.","upvote_count":"1"},{"comment_id":"274217","content":"B is the right answer","poster":"lostsoul07","upvote_count":"1","timestamp":"1634505180.0"},{"poster":"Draco31","content":"B.\nB and C can make sense but C will be right only if the job returned the error spark.yarn.executor.memoryOverhead.\nIf no error, then the job is just taking too long so increase the max capacity\nFor AWS Glue version 2.0 jobs, you cannot instead specify a Maximum capacity. Instead, you should specify a Worker type and the Number of workers. \nhttps://docs.aws.amazon.com/glue/latest/dg/add-job.html","timestamp":"1634213580.0","comment_id":"239211","upvote_count":"4"},{"upvote_count":"1","content":"b is correct!","poster":"BillyC","timestamp":"1634145900.0","comment_id":"216836"},{"comment_id":"175249","timestamp":"1634081160.0","upvote_count":"1","content":"Option B for sure. We can eliminate the two options with Bookmarks and spark.yarn.executor.memoryOverhead has nothing to do with Glue.","poster":"Paitan"},{"comment_id":"159508","upvote_count":"1","content":"My answer is B","poster":"zeronine","timestamp":"1633194180.0"},{"timestamp":"1632941580.0","comment_id":"155781","content":"its B\nhttps://docs.aws.amazon.com/glue/latest/dg/monitor-debug-capacity.html","upvote_count":"1","poster":"Prodip"},{"upvote_count":"3","comments":[{"comment_id":"156396","timestamp":"1633049580.0","upvote_count":"16","content":"its actually an easy question, A and D are not valid coz bookmarks are not used for capacity. Also C is invalid because you cannot set spark.yarn.executor.memoryOverhead in glue, we can only play with DPUs, so B is by elimination even if you dont read through evry document.","poster":"abhineet","comments":[{"poster":"testtaker3434","upvote_count":"2","comment_id":"156520","content":"I did overlook the \"bookmark\" word...thanks abhineet.","timestamp":"1633104600.0"},{"timestamp":"1633740120.0","upvote_count":"1","content":"Agreed, answer is B","comment_id":"163748","poster":"awssp12345"}]}],"content":"I agree with B. BUT I found the link providede to be misleading: after reading the provided link i thought the answer had to be D. \n\nDoing a bit more search, I found the link below where it clearly says (search for \"maximum capacity\"):\n\nhttps://docs.aws.amazon.com/glue/latest/dg/add-job.html\n\"Standard – When you choose this type, you also provide a value for Maximum capacity. Maximum capacity is the number of AWS Glue data processing units (DPUs) that can be allocated when this job runs.\"\n\nWhat the hell of tricky question! How are we supposed to find this information buried in the documentation? I suppose this must happen very often to be on the exam!","poster":"testtaker3434","timestamp":"1632222540.0","comment_id":"153885"}],"timestamp":"2020-08-09 20:23:00","answer":"B","question_text":"A data analyst is using AWS Glue to organize, cleanse, validate, and format a 200 GB dataset. The data analyst triggered the job to run with the Standard worker type. After 3 hours, the AWS Glue job status is still RUNNING. Logs from the job run show no error codes. The data analyst wants to improve the job execution time without overprovisioning.\nWhich actions should the data analyst take?","question_id":110,"answer_description":"","exam_id":20,"url":"https://www.examtopics.com/discussions/amazon/view/27799-exam-aws-certified-data-analytics-specialty-topic-1-question/","answers_community":["B (100%)"],"topic":"1","isMC":true,"question_images":[],"answer_images":[]}],"exam":{"numberOfQuestions":164,"isImplemented":true,"isBeta":false,"lastUpdated":"11 Apr 2025","provider":"Amazon","name":"AWS Certified Data Analytics - Specialty","isMCOnly":true,"id":20},"currentPage":22},"__N_SSP":true}