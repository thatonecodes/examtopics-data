{"pageProps":{"questions":[{"id":"N3b60790phy9NGTf71YH","discussion":[{"comments":[{"poster":"gelsm","timestamp":"1635350820.0","upvote_count":"1","content":"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html","comment_id":"415072"}],"poster":"frankzeng","timestamp":"1634473080.0","content":"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html\nUsing Amazon S3 as a target for AWS Database Migration Service\nA","comment_id":"345564","upvote_count":"11"},{"poster":"novice_expert","timestamp":"1651344000.0","upvote_count":"5","comment_id":"595213","content":"Selected Answer: A\n-C and D out because of VPN Tunnel\n-B is out for multiple snowballs\n\nA. 80 TB = 1 Snowball -> S3 -> DMS -> CDC -> Target"},{"upvote_count":"2","comment_id":"1006233","poster":"Pranava_GCP","content":"Selected Answer: A\nA. 80 TB = 1 Snowball -> S3 -> DMS -> CDC -> Target","timestamp":"1694583420.0"},{"timestamp":"1669202640.0","content":"A for sure. 2nd snowball device not required for CDC.","poster":"Arun32","upvote_count":"1","comment_id":"725080"},{"comment_id":"585167","poster":"kret","upvote_count":"3","content":"Selected Answer: A\n80TB -> Snowball, ongoing changes rep with DMS","timestamp":"1649849880.0"},{"comment_id":"561732","upvote_count":"4","poster":"RotterDam","timestamp":"1646530200.0","content":"Selected Answer: A\nA is the correct Answer"},{"content":"Selected Answer: B\nI'm taking B here as the company network bandwidth is pretty much redundant for use in the migration.","poster":"thelad","timestamp":"1643812500.0","upvote_count":"3","comment_id":"538781"},{"poster":"Dip11","timestamp":"1635006120.0","upvote_count":"2","comment_id":"365699","content":"Ans: A"},{"timestamp":"1634534220.0","content":"A final answer","upvote_count":"3","comment_id":"358906","poster":"Aesthet"},{"comment_id":"342910","poster":"manan728","timestamp":"1633841640.0","upvote_count":"2","content":"You need snowball edge here. Of A and B only A makes more sense."},{"timestamp":"1633098540.0","content":"A is for me.","comment_id":"335240","poster":"wzlinux","upvote_count":"1"},{"content":"My answer is A , D is incorrect SCT is for schemas and doesn't support CDC","timestamp":"1632210060.0","poster":"Xgab","upvote_count":"2","comment_id":"328318"}],"isMC":true,"question_images":[],"question_text":"A company is due for renewing its database license. The company wants to migrate its 80 TB transactional database system from on-premises to the AWS Cloud.\nThe migration should incur the least possible downtime on the downstream database applications. The company's network infrastructure has limited network bandwidth that is shared with other applications.\nWhich solution should a database specialist use for a timely migration?","answer":"A","topic":"1","unix_timestamp":1617586020,"answer_ET":"A","answers_community":["A (82%)","B (18%)"],"url":"https://www.examtopics.com/discussions/amazon/view/49114-exam-aws-certified-database-specialty-topic-1-question-87/","exam_id":22,"timestamp":"2021-04-05 03:27:00","answer_images":[],"choices":{"B":"Perform a full backup of the source database to AWS Snowball Edge appliances and ship them to be loaded to Amazon S3. Periodically perform incremental backups of the source database to be shipped in another Snowball Edge appliance to handle syncing change data capture (CDC) data from the source to the target database.","D":"Use the AWS Schema Conversion Tool (AWS SCT) to migrate the full load of the source database over a VPN tunnel using the internet for its primary connection. Allow AWS SCT to handle syncing change data capture (CDC) data from the source to the target database.","A":"Perform a full backup of the source database to AWS Snowball Edge appliances and ship them to be loaded to Amazon S3. Use AWS DMS to migrate change data capture (CDC) data from the source database to Amazon S3. Use a second AWS DMS task to migrate all the S3 data to the target database.","C":"Use AWS DMS to migrate the full load of the source database over a VPN tunnel using the internet for its primary connection. Allow AWS DMS to handle syncing change data capture (CDC) data from the source to the target database."},"question_id":346,"answer_description":""},{"id":"dRTLCByLSeRVuptJctzQ","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/48781-exam-aws-certified-database-specialty-topic-1-question-88/","answer":"D","answer_ET":"D","answers_community":["D (100%)"],"isMC":true,"timestamp":"2021-04-02 12:23:00","question_images":[],"answer_description":"","answer_images":[],"choices":{"B":"Change the read_only parameter to false (read_only=0) in the default parameter group of the read replica. Perform a reboot without failover. Connect to the read replica and create the tables using the local_only MySQL option.","A":"Contact AWS Support to disable read-only mode on the read replica. Reboot the read replica. Connect to the read replica and create the tables.","C":"Change the read_only parameter to false (read_only=0) in the default parameter group. Reboot the read replica. Connect to the read replica and create the tables.","D":"Create a new DB parameter group. Change the read_only parameter to false (read_only=0). Associate the read replica with the new group. Reboot the read replica. Connect to the read replica and create the tables."},"exam_id":22,"question_id":347,"unix_timestamp":1617358980,"discussion":[{"poster":"jove","timestamp":"1640281740.0","upvote_count":"7","content":"Selected Answer: D\nhttps://aws.amazon.com/premiumsupport/knowledge-center/rds-read-replica/","comment_id":"508073"},{"content":"Selected Answer: D\nD. Create new DB parameter group --> change parameter value --> associate replica with new param group --> reboot replica -- > connect to read replica then create tables.","comment_id":"1007295","timestamp":"1694673240.0","poster":"Pranava_GCP","upvote_count":"2"},{"upvote_count":"1","content":"Selected Answer: D\nOption D, as default parameter group cannot be modified.","comment_id":"927553","poster":"navkumin","timestamp":"1687181940.0"},{"upvote_count":"1","comments":[{"upvote_count":"3","content":"wont be B becoz Default groups cannot be modified","comment_id":"883171","poster":"SeemaDataReader","timestamp":"1682647800.0"}],"comment_id":"818386","content":"Any reason , its not B . https://aws.amazon.com/premiumsupport/knowledge-center/rds-read-replica/ . The replica has to be rebooted without a failover option . However, I do not understand what it mseans 'local_only MySQL'","poster":"sk1974","timestamp":"1677098460.0"},{"comment_id":"610300","upvote_count":"2","poster":"Dantas","timestamp":"1654108260.0","content":"Selected Answer: D\nOption D"},{"poster":"KaranGandhi30","timestamp":"1651890660.0","comment_id":"597950","content":"Selected Answer: D\nOption D","upvote_count":"2"},{"timestamp":"1651193160.0","content":"Selected Answer: D\nread_only in default parameter group can not be changed (possible in custom parameter group)\nhttps://aws.amazon.com/premiumsupport/knowledge-center/rds-read-replica/","poster":"novice_expert","upvote_count":"4","comment_id":"594129"},{"poster":"GMartinelli","content":"Selected Answer: D\nOption D","upvote_count":"2","timestamp":"1639484880.0","comment_id":"501344"},{"content":"This Q was asked in my exam.","upvote_count":"1","poster":"guru_ji","timestamp":"1635429540.0","comment_id":"446073"},{"upvote_count":"1","content":"D is correct answer due to default parameter changes are not allowed","comment_id":"408519","poster":"Hits_23","timestamp":"1635208560.0"},{"poster":"Dip11","timestamp":"1634891820.0","content":"D is the correct ans.","comment_id":"365702","upvote_count":"1"},{"content":"D final answer","poster":"Aesthet","timestamp":"1634137380.0","upvote_count":"1","comment_id":"358915"},{"timestamp":"1634025840.0","poster":"shyamnsingh","upvote_count":"3","content":"D For me","comment_id":"343919"},{"poster":"Billhardy","upvote_count":"1","comment_id":"337578","timestamp":"1634017020.0","content":"D for me"},{"upvote_count":"1","content":"D is for me","comment_id":"335244","timestamp":"1633213620.0","poster":"wzlinux"},{"content":"I think D? \n\nDefault parameter group cannot be changed.","comments":[{"comments":[{"comment_id":"326629","comments":[{"upvote_count":"3","timestamp":"1633154760.0","poster":"shantest1","comment_id":"327790","content":"Sorry again, default parameter cannot be changed.\nD: answer. \n\n No way to delete the comments, otherwise it will make it easy to update."}],"upvote_count":"1","timestamp":"1632989880.0","poster":"shantest1","content":"Sorry C: - Do not think need to adjust anything like what is said in B: \n* We can edit default parameter group (no need of extra step of creating additional parameter group)"}],"content":"Sorry B.","comment_id":"326626","upvote_count":"1","timestamp":"1632391920.0","poster":"shantest1"}],"timestamp":"1632390660.0","poster":"shantest1","upvote_count":"3","comment_id":"326519"}],"question_text":"A database specialist is responsible for an Amazon RDS for MySQL DB instance with one read replica. The DB instance and the read replica are assigned to the default parameter group. The database team currently runs test queries against a read replica. The database team wants to create additional tables in the read replica that will only be accessible from the read replica to benefit the tests.\nWhich should the database specialist do to allow the database team to create the test tables?"},{"id":"lx73WCfN8jKbHT54JnK5","question_id":348,"topic":"1","answers_community":["D (100%)"],"answer_ET":"D","answer_images":[],"choices":{"D":"Use custom endpoints to satisfy the different workloads.","C":"Create additional readers to cater to the different scenarios.","B":"Use automatic scaling for the Aurora Replica to have the appropriate number of replicas for the desired workload.","A":"Use the writer endpoint for OLTP and the reader endpoint for the OLAP reporting workload."},"question_text":"A company has a heterogeneous six-node production Amazon Aurora DB cluster that handles online transaction processing (OLTP) for the core business and\nOLAP reports for the human resources department. To match compute resources to the use case, the company has decided to have the reporting workload for the human resources department be directed to two small nodes in the Aurora DB cluster, while every other workload goes to four large nodes in the same DB cluster.\nWhich option would ensure that the correct nodes are always available for the appropriate workload while meeting these requirements?","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/48680-exam-aws-certified-database-specialty-topic-1-question-89/","isMC":true,"exam_id":22,"answer":"D","discussion":[{"poster":"shantest1","timestamp":"1633048080.0","content":"D - Custom endpoints","comment_id":"326520","upvote_count":"8"},{"content":"Selected Answer: D\nD - custom endpoints can provide this flexibility","upvote_count":"1","comment_id":"1121662","poster":"MultiAZ","timestamp":"1705150500.0"},{"timestamp":"1669729200.0","content":"Selected Answer: D\nUse custom endpoints","comment_id":"730463","poster":"examineme","upvote_count":"1"},{"upvote_count":"3","timestamp":"1651424640.0","content":"Selected Answer: D\nD. Use custom endpoints to satisfy the different workloads.\n\nhttps://aws.amazon.com/about-aws/whats-new/2018/11/amazon-aurora-simplifies-workload-management-with-custom-endpoints/\n\nyou can now create custom endpoints for Amazon Aurora databases. This allows you to distribute and load balance workloads across different sets of database instances in your Aurora cluster.","comment_id":"595671","poster":"novice_expert"},{"comments":[{"timestamp":"1653034920.0","poster":"khchan123","comment_id":"604289","upvote_count":"3","content":"heterogeneous means the six nodes are of different size."}],"poster":"RotterDam","timestamp":"1646530620.0","upvote_count":"1","comment_id":"561733","content":"What does \" heterogeneous six-node production Amazon Aurora DB cluster \" even mean? I've never heard of any Aurora infrastructure described this way!!"},{"upvote_count":"1","poster":"guru_ji","content":"Correct Answer: D","comment_id":"439535","timestamp":"1635233700.0"},{"upvote_count":"1","comment_id":"419391","content":"D is correct as per link:\nhttps://aws.amazon.com/about-aws/whats-new/2018/11/amazon-aurora-simplifies-workload-management-with-custom-endpoints/\nou can now create custom endpoints for Amazon Aurora databases. This allows you to distribute and load balance workloads across different sets of database instances in your Aurora cluster.\n\nFor example, you may provision a set of Aurora Replicas to use an instance type with higher memory capacity in order to run an analytics workload. A custom endpoint can then help you route the analytics workload to these appropriately-configured instances, while keeping other instances in your cluster isolated from this workload. As you add or remove instances from the custom endpoint to match your workload, the endpoint helps spread the load around.","poster":"damaldon","timestamp":"1634726220.0"},{"timestamp":"1634504700.0","comment_id":"408525","poster":"Hits_23","upvote_count":"1","content":"D is right choice. Custom endpoints. OLTP can only work with master instance endpoint. OLAP reports could work with either master instance/any combination of reader endpoints"},{"poster":"Aesthet","content":"D final answer","comment_id":"358918","timestamp":"1634391300.0","upvote_count":"3"},{"timestamp":"1632365640.0","poster":"std2021","upvote_count":"3","content":"Answer D","comment_id":"326071"}],"unix_timestamp":1617301680,"timestamp":"2021-04-01 20:28:00","question_images":[]},{"id":"cQzIFmTWzKXx2S3z3mug","answer_description":"","isMC":true,"answer_images":[],"question_id":349,"discussion":[{"comment_id":"129617","upvote_count":"14","timestamp":"1632200880.0","poster":"[Removed]","content":"D. While C would work, the requirement is minimal downtime.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Migrating.html"},{"timestamp":"1694695320.0","poster":"Pranava_GCP","comment_id":"1007630","content":"Selected Answer: D\nD. Migrate data from the RDS for PostgreSQL DB instance to an Aurora PostgreSQL DB cluster using an Aurora Replica. Promote the replica during the cutover. \n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Migrating.html#AuroraPostgreSQL.Migrating.RDSPostgreSQL.Replica","upvote_count":"1"},{"upvote_count":"1","poster":"IhorK","comment_id":"968104","content":"D\nMigrating from RDS PostgreSQL to Aurora PostgreSQL.\nYou have two options when migrating data from RDS PostgreSQL to Aurora PostgreSQL:\n- Use an RDS PostgreSQL snapshot\n- Use an Aurora Read Replica for RDS PostgreSQL\nhttps://aws.amazon.com/blogs/database/migrate-to-an-amazon-aurora-postgresql-instance-from-another-postgresql-source/","timestamp":"1690807020.0"},{"upvote_count":"1","poster":"mraronsimon","comment_id":"942487","timestamp":"1688456700.0","content":"Selected Answer: D\nThe correct answer is D. The keyword is \"FASTEST\" ~ minimal downtime / minimal effort\n\n\"You can migrate your existing Amazon RDS MySQL databases to Amazon Aurora using Aurora Read Replica. This solution is beneficial since it's completely managed and does not involve manually configuring replication functionality to reduce downtime during migration.\"\n\"PostgreSQL follows the same process as the one described previously for MySQL for migration.\"\nhttps://docs.aws.amazon.com/whitepapers/latest/migrating-databases-to-amazon-aurora/migration-using-aurora-read-replica.html"},{"poster":"SteveMartin9","content":"Selected Answer: D\nAuthor from the Udemy.com practice test says D is the correct answer.","timestamp":"1673959020.0","comment_id":"778904","upvote_count":"1"},{"poster":"sirfans","upvote_count":"1","comment_id":"704365","content":"Selected Answer: D\nD is the right option","timestamp":"1666757160.0"},{"content":"Selected Answer: D\nC - snapshot will take time (and new transactions lost)\nD- Replica is fast","upvote_count":"2","timestamp":"1651178940.0","comment_id":"594045","poster":"novice_expert"},{"content":"Selected Answer: D\nD is absolutely correct","upvote_count":"1","timestamp":"1646525100.0","poster":"RotterDam","comment_id":"561705"},{"content":"We need the SPEEDIEST solution, however, we also need a reasonable solution; how would we deal with database changes when restoring the database when we pick B? AWS perform that job for us when choosing D.","timestamp":"1644932640.0","upvote_count":"2","poster":"pcpcpc888","comment_id":"547795"},{"content":"Keywords : seamless and speediest..\nAnswer : Option D","upvote_count":"2","timestamp":"1640208360.0","comment_id":"507396","poster":"jove"},{"content":"Selected Answer: D\nOption D","comment_id":"496244","poster":"GMartinelli","upvote_count":"1","timestamp":"1638902220.0"},{"content":"D: because in question talking about minimum downtime .","timestamp":"1636135860.0","upvote_count":"1","poster":"Anuragdba","comment_id":"439210"},{"upvote_count":"2","comment_id":"434493","timestamp":"1635915240.0","content":"D is the correct answer, Read Replica is faster than snapshots","poster":"aws4myself"},{"poster":"guru_ji","content":"D ==>> Correct Answer.","timestamp":"1635896700.0","upvote_count":"1","comment_id":"425827"},{"content":"if we say C is fastest then how to handle changes to source server while snapshot is being transferred and getting applied to target ?","upvote_count":"2","comment_id":"361035","poster":"jayshah7","timestamp":"1635581280.0"},{"content":"Answer D","comment_id":"314730","poster":"LMax","upvote_count":"2","timestamp":"1635313080.0"},{"upvote_count":"1","timestamp":"1635226020.0","content":"Answer D. When you take a snapshot there will be a temporary suspension of I/O Services, now this can be seconds or longer, but our size is 1TB which means that there will be downtime, so based on that the answer is D","poster":"jyrajan","comment_id":"301039"},{"comment_id":"296369","content":"Ans: D","timestamp":"1635202620.0","poster":"myutran","upvote_count":"2"},{"poster":"JobinAkaJoe","comments":[{"timestamp":"1635090660.0","comment_id":"290461","poster":"GeeBeeEl","upvote_count":"1","content":"I am confused, you said C is fastest, why choose D. They are asking for the FASTEST!"}],"upvote_count":"2","comment_id":"252800","content":"Option C ( Using snapshot) appears to be slightly faster option that D(creating aurora-replica)\nHowever D is the best option considering downtime required(minimal)\nAlso D is the recommended option for this requirement.\nBest answer is D","timestamp":"1634967780.0"},{"upvote_count":"2","content":"Answer: D \nMigrating data from an RDS PostgreSQL DB instance to an Aurora PostgreSQL DB cluster by using an Aurora read replica.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Migrating.html#AuroraPostgreSQL.Migrating.RDSPostgreSQ","comment_id":"244857","poster":"kilkar","timestamp":"1634892120.0"},{"poster":"Ashoks","comment_id":"211581","timestamp":"1634720100.0","upvote_count":"1","content":"D. Minimal down time"},{"poster":"Smart","content":"D is correct as pointed out by others with correct link. \n\n1. Replica migration is the recommend way by AWS\n2. Replica migration does create snapshot in the background so Option C/D is no different that manner.\n3. The difference with replica migration is that once the replica is running, it will synchronize with source RDS. In case of option C to maintain consistency, during migration, you will need to stop all activity during this migration process - hence there will be downtime.","upvote_count":"3","timestamp":"1634684160.0","comment_id":"174099"},{"timestamp":"1634661960.0","content":"I'll go with D too. https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Migrating.html#AuroraPostgreSQL.Migrating.RDSPostgreSQL.Replica","comment_id":"147167","upvote_count":"2","poster":"sonobab"},{"poster":"jnassp1","content":"C - Fastest. Snapshot will take minutes and it will have minimal downtime.\nD - Slowest. Will have downtime in secs.","comments":[{"timestamp":"1634299380.0","comment_id":"145721","upvote_count":"1","content":"D... cause: The migration needs to have minimal downtime.","poster":"BillyC"}],"timestamp":"1634262300.0","upvote_count":"1","comment_id":"144695"},{"poster":"halol","upvote_count":"1","timestamp":"1634010000.0","comment_id":"136918","content":"D gives minimal Downtime it will as it ask for minimal downtime not minimal time"},{"upvote_count":"1","timestamp":"1633993860.0","poster":"helpaws","comment_id":"136880","content":"D for me."},{"comments":[{"timestamp":"1632927120.0","content":"AWS user guide mentions \"Be prepared for migration to take a while, roughly several hours per tebibyte (TiB) of data \" for option C? See my full comment above. So i think C is not the FASTEST.","comment_id":"134957","upvote_count":"1","poster":"chicagomassageseeker","comments":[{"content":"i meant D is not the FASTEST but C is the FASTEST.","comments":[{"comment_id":"138304","poster":"BillyC","timestamp":"1634120700.0","upvote_count":"1","content":"The migration needs to have minimal downtime."}],"poster":"chicagomassageseeker","upvote_count":"1","comment_id":"134959","timestamp":"1633810140.0"}]}],"poster":"BillyC","upvote_count":"2","comment_id":"134158","timestamp":"1632704760.0","content":"D is for me: To migrate from an Amazon RDS for PostgreSQL DB instance to an Amazon Aurora PostgreSQL DB\ncluster, create an Aurora Replica of your source PostgreSQL DB instance. When the replica lag between the\nPostgreSQL DB instance and the Aurora PostgreSQL Replica is zero, you can promote the Aurora Replica to be\na standalone Aurora PostgreSQL DB cluster"},{"content":"Basically if the questtion says FASTEEST migratiton , choose C. C is FASTEST but there is a brief I/O suspension that can last from a few seconds to a few minutes when taking backup. if the settup is multiAZ, then there will not be any i/o suspension as backup will be taken from stand by.\n\nIf the question said MINIMAL Downtime, then Choose D. D is NOT fast and takes several hours to complete for even 1TB but the downtime is MINIMAL.","poster":"chicagomassageseeker","comment_id":"133687","timestamp":"1632604560.0","upvote_count":"4"},{"upvote_count":"4","content":"Answer is C. C is fastest.\n\nMigrating Data by Using an Aurora Read Replica Will take several hours to finish for 1TB as shown below: \nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Migrating.html#AuroraPostgreSQL.Migrating.RDSPostgreSQL.Replica\nOverview of Migrating Data by Using an Aurora Read Replica:\n\nTo migrate from an RDS PostgreSQL DB instance to an Aurora PostgreSQL DB cluster, we recommend creating an Aurora Read Replica of your source PostgreSQL DB instance. When the replica lag between the PostgreSQL DB instance and the Aurora PostgreSQL Read Replica is zero, you can stop replication. At this point, you can promote the Aurora Read Replica to be a standalone Aurora PostgreSQL DB cluster. This standalone DB cluster can then accept write loads.\n\nBe prepared for migration to take a while, roughly several hours per tebibyte (TiB) of data. While the migration is in progress, your Amazon RDS PostgreSQL instance accumulates write ahead log (WAL) segments. Make sure that your Amazon RDS instance has sufficient storage capacity for these segments.","comment_id":"133681","poster":"chicagomassageseeker","timestamp":"1632269700.0"}],"choices":{"D":"Migrate data from the RDS for PostgreSQL DB instance to an Aurora PostgreSQL DB cluster using an Aurora Replica. Promote the replica during the cutover.","C":"Create a database snapshot of the RDS for PostgreSQL DB instance and use this snapshot to create the Aurora PostgreSQL DB cluster.","B":"Use the pg_dump and pg_restore utilities to extract and restore the RDS for PostgreSQL DB instance to the Aurora PostgreSQL DB cluster.","A":"Create an Aurora PostgreSQL DB cluster. Set up replication from the source RDS for PostgreSQL DB instance using AWS DMS to the target DB cluster."},"answer_ET":"D","question_text":"A company is running an Amazon RDS for PostgreSQL DB instance and wants to migrate it to an Amazon Aurora PostgreSQL DB cluster. The current database is 1 TB in size. The migration needs to have minimal downtime.\nWhat is the FASTEST way to accomplish this?","answer":"D","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/25084-exam-aws-certified-database-specialty-topic-1-question-9/","question_images":[],"answers_community":["D (100%)"],"unix_timestamp":1594200840,"exam_id":22,"timestamp":"2020-07-08 11:34:00"},{"id":"WhUhiQN66TtPCXQNwhjc","question_text":"Developers have requested a new Amazon Redshift cluster so they can load new third-party marketing data. The new cluster is ready and the user credentials are given to the developers. The developers indicate that their copy jobs fail with the following error message:\n`Amazon Invalid operation: S3ServiceException:Access Denied,Status 403,Error AccessDenied.`\nThe developers need to load this data soon, so a database specialist must act quickly to solve this issue.\nWhat is the MOST secure solution?","isMC":true,"choices":{"A":"Create a new IAM role with the same user name as the Amazon Redshift developer user ID. Provide the IAM role with read-only access to Amazon S3 with the assume role action.","C":"Create a new IAM role with read-only access to the Amazon S3 bucket with the assume role action. Add this role to the developer IAM user ID used for the copy job that ended with an error message.","D":"Create a new IAM user with access keys and a new role with read-only access to the Amazon S3 bucket. Add this role to the Amazon Redshift cluster. Change the copy job to use the access keys created.","B":"Create a new IAM role with read-only access to the Amazon S3 bucket and include the assume role action. Modify the Amazon Redshift cluster to add the IAM role."},"answer":"B","topic":"1","question_id":350,"timestamp":"2021-04-01 20:31:00","answer_description":"","question_images":[],"unix_timestamp":1617301860,"discussion":[{"timestamp":"1632485280.0","content":"Answer B","upvote_count":"6","comment_id":"326072","poster":"std2021"},{"content":"Why is C not correct?","timestamp":"1701527460.0","comments":[{"timestamp":"1704168540.0","comment_id":"1111579","poster":"Hisayuki","upvote_count":"1","content":"Because the COPY from S3 will be done by Redshift Cluster"}],"comment_id":"1086256","poster":"jitesh_k","upvote_count":"1"},{"comment_id":"1063347","timestamp":"1699223220.0","content":"Selected Answer: D\nWhy it is not D ? It seems to be a question about Unload-Copy operation. The COPY operation requires access keys and secret keys.","upvote_count":"1","poster":"paws_brasil"},{"timestamp":"1651283100.0","poster":"novice_expert","content":"Selected Answer: B\nB. new IAM role with read-only access to the Amazon S3 bucket and include the assume role action. \nModify the Amazon Redshift cluster to add the IAM role.\n\n\nProvide authentication for your cluster to access Amazon S3 on your behalf to load the sample data. You provide authentication by referencing the IAM role that you created and set as the default for your cluster","upvote_count":"4","comment_id":"594787"},{"timestamp":"1645050120.0","upvote_count":"3","content":"B\nhttps://docs.aws.amazon.com/redshift/latest/gsg/rs-gsg-create-sample-db.html\nProvide authentication for your cluster to access Amazon S3 on your behalf to load the sample data. You provide authentication by referencing the IAM role that you created and set as the default for your cluster","poster":"user0001","comment_id":"548965"},{"poster":"GMartinelli","timestamp":"1638268560.0","content":"Selected Answer: B\nOption B","comment_id":"490589","upvote_count":"3"},{"comment_id":"428109","content":"B. To best protect your sensitive data and safeguard your AWS access credentials, we recommend creating an IAM role and attaching it to your cluster. For more information about providing access permissions, see Permissions to access other AWS resources.\n\nIn this step, you create a new IAM role that allows Amazon Redshift to load data from Amazon S3 buckets. An IAM role is an IAM identity that you can create in your account that has specific permissions. In the next step, you attach the role to your cluster","timestamp":"1636261800.0","upvote_count":"1","poster":"ChauPhan"},{"comment_id":"427445","poster":"guru_ji","timestamp":"1635518100.0","content":"anyone here already cleared the exam ?\nHow much %age Q we will get in real exam, any idea ?","upvote_count":"2"},{"poster":"gelsm","timestamp":"1635336120.0","content":"Answer: B\nhttps://docs.aws.amazon.com/redshift/latest/gsg/rs-gsg-create-an-iam-role.html\n\"Now that you have created the new role, your next step is to attach it to your cluster. You can attach the role when you launch a new cluster or you can attach it to an existing cluster. In the next step, you attach the role to a new cluster.\"","upvote_count":"2","comment_id":"415946"},{"comment_id":"408527","content":"Correct answer is B. Good practice is to create an IAM Role with read only permission to S3 and attach this role to Redshift cluster for COPY jobs from S3 to Redshift cluster","timestamp":"1634691000.0","upvote_count":"2","poster":"Hits_23"},{"comment_id":"358921","poster":"Aesthet","comments":[{"poster":"Aesthet","comment_id":"358955","upvote_count":"3","timestamp":"1633274340.0","content":"Role-based access control\nhttps://docs.aws.amazon.com/redshift/latest/dg/copy-usage_notes-access-permissions.html"}],"timestamp":"1633210260.0","content":"B\nhttps://docs.aws.amazon.com/redshift/latest/gsg/rs-gsg-create-an-iam-role.html","upvote_count":"1"},{"comments":[{"poster":"[Removed]","upvote_count":"5","comment_id":"351617","content":"you can modify the redshift cluster to associal an IAM role. C is not correct","timestamp":"1633037580.0"}],"poster":"manan728","timestamp":"1632948120.0","content":"C is the correct one. You cannot modify the redshift cluster to add IAM role to it. But you can add the role to the user who's trying to get access to S3","comment_id":"342288","upvote_count":"1"}],"exam_id":22,"answers_community":["B (88%)","13%"],"url":"https://www.examtopics.com/discussions/amazon/view/48681-exam-aws-certified-database-specialty-topic-1-question-90/","answer_images":[],"answer_ET":"B"}],"exam":{"name":"AWS Certified Database - Specialty","isMCOnly":false,"isImplemented":true,"numberOfQuestions":359,"lastUpdated":"11 Apr 2025","isBeta":false,"provider":"Amazon","id":22},"currentPage":70},"__N_SSP":true}