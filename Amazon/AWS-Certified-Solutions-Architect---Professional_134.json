{"pageProps":{"questions":[{"id":"7YNueB3lZcWjvENN8Wiq","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/36444-exam-aws-certified-solutions-architect-professional-topic-1/","unix_timestamp":1604827680,"answer_images":[],"answer_description":"","question_id":666,"discussion":[{"timestamp":"1632224160.0","comment_id":"215122","upvote_count":"16","content":"B seems to be correct, spot instances for ECS cluster and Reserved instances for RDB","poster":"liono","comments":[{"poster":"kirrim","comment_id":"463602","content":"Agree, more info:\n\nA with RDS on spot instances is automatically ruled out\nB is feasible by using a \"Diversified\" allocation strategy when setting up the Spot provisioning ECS cluster\nC is feasible but more expensive to do RDS on-demand instances than RDS RI as in B, and uses EFS instead of S3 to store the documents, not as cost effective\nD is feasible but more expensive to do RDS on-demand instances than RDS RI as in B\n\nhttps://aws.amazon.com/ec2/spot/containers-for-less/get-started/\nhttps://aws.amazon.com/ec2/spot/instance-advisor/","timestamp":"1635792780.0","upvote_count":"2"}]},{"upvote_count":"6","timestamp":"1634570460.0","comment_id":"284429","poster":"Ebi","content":"B for sure"},{"poster":"Amsa","comment_id":"716013","upvote_count":"1","content":"Selected Answer: B\nB is correct, straight from Don Bonso's test","timestamp":"1668166500.0"},{"content":"I was skeptical of spot instance on ECS until i read this;\nAmazon Elastic Container Service (ECS) supports Automated Spot Instance Draining, a new capability that reduces service interruptions due to Spot termination for ECS workloads. This feature will enable ECS customers to safely manage any interruptions of ECS tasks running on Spot instances due to termination of the underlying EC2 Spot instance.","upvote_count":"4","poster":"KiraguJohn","comment_id":"639912","timestamp":"1659244380.0"},{"timestamp":"1642515120.0","upvote_count":"3","poster":"frankzeng","content":"https://aws.amazon.com/about-aws/whats-new/2019/09/amazon-ecs-supports-automated-draining-for-spot-instances-running-ecs-services/","comment_id":"526695"},{"timestamp":"1640940600.0","comment_id":"513960","poster":"cldy","content":"B is correct.","upvote_count":"1"},{"timestamp":"1640172180.0","upvote_count":"2","poster":"Ni_yot","content":"B for me. you want to use reserved instances as its cost effective.","comment_id":"507023"},{"content":"B for sure","upvote_count":"1","timestamp":"1638767340.0","poster":"vramchn","comment_id":"494911"},{"timestamp":"1638421380.0","upvote_count":"1","poster":"AzureDP900","comment_id":"492153","content":"B Is right"},{"poster":"andylogan","comment_id":"446433","timestamp":"1635703800.0","content":"It's B","upvote_count":"1"},{"timestamp":"1635623580.0","upvote_count":"1","content":"BBB\n---","poster":"tgv","comment_id":"436538"},{"content":"method of elimination - \n\nchoosing B as answer. \nA and C are eliminated - due to cron usage\nD - eliminated due to on-demand instance where DB cost can be reduced by reserved instances (seems it needs to be run for several years).","poster":"Suresh108","comment_id":"433388","timestamp":"1635529800.0","upvote_count":"2"},{"upvote_count":"2","comment_id":"413471","timestamp":"1635221460.0","content":"I'll go with B","poster":"WhyIronMan"},{"timestamp":"1635144360.0","comment_id":"411504","content":"B, AWS RDS Reserved instances more cost efficent as it says the company will operate more then 7 years.","upvote_count":"1","poster":"Kopa"},{"content":"it's B","upvote_count":"2","comment_id":"357145","timestamp":"1634964780.0","poster":"Waiweng"},{"upvote_count":"2","poster":"Kian1","content":"going with B","timestamp":"1634749560.0","comment_id":"294263"},{"timestamp":"1634201280.0","poster":"kopper2019","upvote_count":"2","content":"B is the correct one spot instances and Reserved for DB","comment_id":"269339"},{"poster":"Bulti","upvote_count":"2","content":"Answer is B and not X because in rare circumstances would you chose on demand RDS instances as usually RDS databases are required 24/7","comment_id":"257044","timestamp":"1633705080.0"},{"timestamp":"1633567380.0","upvote_count":"1","content":"B BECAUSE OF THE READ REPLICAS WHICH IS AN AWS MYSQL BEST PRACTICE ... LOOK OUT FOR THIS IN EXAM IF YOUR UNSURE BETWEEN ANSWERS GUYS","comment_id":"256894","poster":"petebear55"},{"timestamp":"1633553460.0","upvote_count":"1","comment_id":"241702","poster":"T14102020","content":"For sure B."},{"content":"RDS on reserved instances makes sense.\nECS on spot fleet is cheaper but not reliable for critical tasks. It is suitable for \"stateless or task-based scenarios that simply run as long as they need to run and are easily replaced with subsequent identical processes\".\nQuestion does not mention type of workload.\nAlthough between B and C, B makes sense due to RDS reserved instance and no Lambda.","upvote_count":"2","comment_id":"240620","poster":"Cantaloupe","timestamp":"1633319760.0"},{"upvote_count":"3","poster":"jackdryan","content":"I'll go with B","comment_id":"233419","timestamp":"1632631380.0"},{"poster":"Kelvin1477","content":"B\nThis feature will enable ECS customers to safely manage any interruptions of ECS tasks running on Spot instances due to termination of the underlying EC2 Spot instance. Automated Spot Instance Draining will automatically place Spot instances in “DRAINING” state upon the receipt of two minute interruption notice","comment_id":"225328","upvote_count":"4","timestamp":"1632515580.0"}],"exam_id":32,"answers_community":["B (100%)"],"answer":"B","choices":{"B":"Create an ECS cluster using a fleet of Spot Instances, with Spot Instance draining enabled. Provision the database and its read replicas in Amazon RDS using Reserved Instances. Store the documents in a secured Amazon S3 bucket with a lifecycle policy to move the documents that are older than 3 months to Amazon S3 Glacier, then delete the documents from Amazon S3 Glacier that are more than 7 years old.","C":"Create an ECS cluster using On-Demand Instances. Provision the database and its read replicas in Amazon RDS using On-Demand Instances. Store the documents in Amazon EFS. Create a cron job to move the documents that are older than 3 months to Amazon S3 Glacier. Create an AWS Lambda function to delete the documents in S3 Glacier that are older than 7 years.","D":"Create an ECS cluster using a fleet of Spot Instances with Spot Instance draining enabled. Provision the database and its read replicas in Amazon RDS using On-Demand Instances. Store the documents in a secured Amazon S3 bucket with a lifecycle policy to move the documents that are older than 3 months to Amazon S3 Glacier, then delete the documents in Amazon S3 Glacier after 7 years.","A":"Create an ECS cluster using On-Demand Instances. Provision the database and its read replicas in Amazon RDS using Spot Instances. Store the documents in an encrypted EBS volume, and create a cron job to delete the documents after 7 years."},"timestamp":"2020-11-08 10:28:00","answer_ET":"B","topic":"1","question_images":[],"question_text":"A company wants to run a serverless application on AWS. The company plans to provision its application in Docker containers running in an Amazon ECS cluster.\nThe application requires a MySQL database and the company plans to use Amazon RDS. The company has documents that need to be accessed frequently for the first 3 months, and rarely after that. The document must be retained for 7 years.\nWhat is the MOST cost-effective solution to meet these requirements?"},{"id":"RXoR17FaoNzASWVSM6DB","answer_description":"","timestamp":"2020-11-08 10:42:00","exam_id":32,"answer_images":[],"answers_community":["C (100%)"],"topic":"1","choices":{"C":"Create an AWS Glue crawler and custom classifier based on the data feed formats and build a table definition to match. Trigger an AWS Lambda function on file delivery to start an AWS Glue ETL job to transform the entire record according to the processing and transformation requirements. Define the output format as JSON. Once complete, have the ETL job send the results to another S3 bucket for internal processing.","B":"Trigger an AWS Lambda function on file delivery that extracts each record and writes it to an Amazon SQS queue. Configure an AWS Fargate container application to automatically scale to a single instance when the SQS queue contains messages. Have the application process each record, and transform the record into JSON format. When the queue is empty, send the results to another S3 bucket for internal processing and scale down the AWS Fargate instance.","A":"Trigger an AWS Lambda function on file delivery that extracts each record and writes it to an Amazon SQS queue. Trigger another Lambda function when new messages arrive in the SQS queue to process the records, writing the results to a temporary location in Amazon S3. Trigger a final Lambda function once the SQS queue is empty to transform the records into JSON format and send the results to another S3 bucket for internal processing.","D":"Create an AWS Glue crawler and custom classifier based upon the data feed formats and build a table definition to match. Perform an Amazon Athena query on file delivery to start an Amazon EMR ETL job to transform the entire record according to the processing and transformation requirements. Define the output format as JSON. Once complete, send the results to another S3 bucket for internal processing and scale down the EMR cluster."},"isMC":true,"question_images":[],"answer":"C","url":"https://www.examtopics.com/discussions/amazon/view/36445-exam-aws-certified-solutions-architect-professional-topic-1/","unix_timestamp":1604828520,"answer_ET":"C","discussion":[{"comment_id":"215129","timestamp":"1632075300.0","upvote_count":"23","content":"C seems to be correct\nhttps://docs.aws.amazon.com/glue/latest/dg/trigger-job.html","poster":"liono","comments":[{"content":"I agree. A) can bring cost problems and concurrency limits in lambda. Furthermore, Glue already solves these issues with much less development.","upvote_count":"4","comment_id":"222732","poster":"fabianjanu","timestamp":"1632591000.0"}]},{"upvote_count":"7","comment_id":"343299","poster":"blackgamer","content":"C is the correct answer. \n\nhttps://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Glue_Event-driven-ETL-Pipelines.e24d59bb79a9e24cdba7f43ffd234ec0482a60e2.png","comments":[{"timestamp":"1635829620.0","poster":"kirrim","content":"Beautiful diagram!\n\nJust in case the URL for that image gets modifed, scroll down to \"Use Cases\" on the home page for Glue: https://aws.amazon.com/glue/","comment_id":"463607","upvote_count":"1"}],"timestamp":"1635273780.0"},{"poster":"milofficial","timestamp":"1678688580.0","content":"Selected Answer: C\nC is correct","upvote_count":"1","comment_id":"837678"},{"comment_id":"615444","content":"I'm going with C.","poster":"CloudHell","timestamp":"1655054520.0","upvote_count":"1"},{"poster":"cldy","upvote_count":"1","comment_id":"497320","timestamp":"1639024740.0","content":"C. Create an AWS Glue crawler and custom classifier based on the data feed formats and build a table definition to match. Trigger an AWS Lambda function on file delivery to start an AWS Glue ETL job to transform the entire record according to the processing and transformation requirements. Define the output format as JSON. Once complete, have the ETL job send the results to another S3 bucket for internal processing."},{"poster":"AzureDP900","upvote_count":"1","timestamp":"1638445680.0","content":"c is correct\nYou can use a Glue crawler to populate the AWS Glue Data Catalog with tables. The Lambda function can be triggered\nusing S3 event notifications when object create events occur. The Lambda function will then trigger the Glue ETL job\nto transform the records masking the sensitive data and modifying the output format to JSON. This solution meets all\nrequirements.","comment_id":"492412"},{"content":"C is correct","poster":"AzureDP900","upvote_count":"1","timestamp":"1638052080.0","comment_id":"488537"},{"upvote_count":"2","comment_id":"484100","content":"Selected Answer: C\nhttps://aws.amazon.com/glue/","poster":"acloudguru","timestamp":"1637578680.0"},{"upvote_count":"1","timestamp":"1635625440.0","content":"It's C","poster":"andylogan","comment_id":"446436"},{"comment_id":"436540","content":"CCC\n---","timestamp":"1635502320.0","upvote_count":"1","poster":"tgv"},{"comment_id":"413923","poster":"WhyIronMan","timestamp":"1635373020.0","upvote_count":"2","content":"I'll go with C"},{"comment_id":"366856","poster":"mustpassla","content":"D, a use case of Glue crawler.","upvote_count":"2","timestamp":"1635333540.0"},{"timestamp":"1635328740.0","poster":"Waiweng","content":"it's C","comment_id":"357149","upvote_count":"2"},{"comment_id":"334613","content":"going with C","upvote_count":"2","timestamp":"1635231600.0","poster":"KnightVictor"},{"timestamp":"1635111000.0","upvote_count":"1","content":"i think D","comment_id":"318659","poster":"eji"},{"timestamp":"1634848140.0","poster":"wasabidev","content":"C for me","upvote_count":"1","comment_id":"313456"},{"content":"I will go with C","upvote_count":"2","timestamp":"1634665440.0","comment_id":"294266","poster":"Kian1"},{"poster":"Ebi","timestamp":"1634234580.0","comment_id":"288178","upvote_count":"3","content":"I go with C"},{"timestamp":"1634177940.0","poster":"Bulti","comment_id":"257047","upvote_count":"1","content":"C is correct"},{"upvote_count":"1","content":"Correct answer is C","poster":"T14102020","timestamp":"1633964160.0","comment_id":"241713"},{"timestamp":"1633814640.0","poster":"Cantaloupe","content":"ETL is perfect for this type of transformations\nD does not make sense to use Athena to query when we have AWS Glue crawler. EMR is for big data analytics. Glue is much better for simpler tasks.\nChoosing C","comment_id":"240613","upvote_count":"3"},{"comment_id":"233426","upvote_count":"2","poster":"jackdryan","timestamp":"1633651140.0","content":"I'll go with C"},{"comment_id":"228078","poster":"micahdevops","timestamp":"1633105260.0","content":"support c","upvote_count":"1"},{"upvote_count":"1","comment_id":"225332","poster":"Kelvin1477","content":"I choose C\nYou will still need to solve the tranformation if you write own custom Lambda function\nGlue can solve that: https://docs.aws.amazon.com/glue/latest/dg/built-in-transforms.html","timestamp":"1632922500.0"}],"question_id":667,"question_text":"A financial services company receives a regular data feed from its credit card servicing partner. Approximately 5,000 records are sent every 15 minutes in plaintext, delivered over HTTPS directly into an Amazon S3 bucket with server-side encryption. This feed contains sensitive credit card primary account number\n(PAN) data. The company needs to automatically mask the PAN before sending the data to another S3 bucket for additional internal processing. The company also needs to remove and merge specific fields, and then transform the record into JSON format. Additionally, extra feeds are likely to be added in the future, so any design needs to be easily expandable.\nWhich solutions will meet these requirements?"},{"id":"wuitFDYL1otFGfJI3odQ","exam_id":32,"url":"https://www.examtopics.com/discussions/amazon/view/47094-exam-aws-certified-solutions-architect-professional-topic-1/","answer":"BD","timestamp":"2021-03-14 20:56:00","question_id":668,"choices":{"E":"Use the existing CodeCommit repository for the AWS KMS key template. Create an IAM policy to allow commits to the new repository and attach it to the data protection team's users. Modify the existing CodePipeline pipeline to use a custom IAM role and to perform KMS key updates using CloudFormation. Modify the KMS key policy to allow the CodePipeline IAM role to modify the key policy.","B":"Create an AWS KMS key that follows the CloudFront service role to generate data keys for encryption Configure S3 default encryption to use KMS managed keys (SSE-KMS) on the log storage bucket using the new KMS key Modify the KMS key policy to allow the log processing service to perform decrypt operations.","C":"Configure S3 default encryption to use AWS KMS managed keys (SSE-KMS) on the log storage bucket using the AWS Managed S3 KMS key. Modify the KMS key policy to allow the CloudFront service role to generate data keys for encryption Modify the KMS key policy to allow the log processing service to perform decrypt operations.","D":"Create a new CodeCommit repository for the AWS KMS key template. Create an IAM policy to allow commits to the new repository and attach it to the data protection team's users. Create a new CodePipeline pipeline with a custom IAM role to perform KMS key updates using CloudFormation Modify the KMS key policy to allow the CodePipeline IAM role to modify the key policy.","A":"Create an AWS KMS key that allows the AWS Logs Delivery account to generate data keys for encryption Configure S3 default encryption to use server-side encryption with KMS managed keys (SSE-KMS) on the log storage bucket using the new KMS key. Modify the KMS key policy to allow the log processing service to perform decrypt operations."},"unix_timestamp":1615751760,"answers_community":["BD (100%)"],"isMC":true,"answer_ET":"AD","topic":"1","question_text":"A media company is serving video files stored in Amazon S3 using Amazon CloudFront. The development team needs access to the logs to diagnose faults and perform service monitoring. The log files from CloudFront may contain sensitive information about users.\nThe company uses a log processing service to remove sensitive information before making the logs available to the development team. The company has the following requirements for the unprocessed logs:\n✑ The logs must be encrypted at rest and must be accessible by the log processing service only.\n✑ Only the data protection team can control access to the unprocessed log files.\n✑ AWS CloudFormation templates must be stored in AWS CodeCommit.\n✑ AWS CodePipeline must be triggered on commit to perform updates made to CloudFormation templates.\nCloudFront is already writing the unprocessed logs to an Amazon S3 bucket, and the log processing service is operating against this S3 bucket.\n//IMG//\n\nWhich combination of steps should a solutions architect take to meet the company's requirements? (Choose two.)","question_images":["https://www.examtopics.com/assets/media/exam-media/04241/0044800005.png"],"answer_images":[],"discussion":[{"comment_id":"339725","timestamp":"1633827420.0","poster":"Mrflip","comments":[{"poster":"LCC92","timestamp":"1634252400.0","upvote_count":"3","comment_id":"360408","content":"From the link Meflip gives:\nIf you enabled server-side encryption for your Amazon S3 bucket using AWS KMS-managed keys (SSE-KMS) with a customer-managed Customer Master Key (CMK), you must add the following to the key policy for your CMK to enable writing log files to the bucket. You cannot use the default CMK because CloudFront won't be able to upload the log files to the bucket.\n{\n \"Sid\": \"Allow CloudFront Flow Logs to use the key\",\n \"Effect\": \"Allow\",\n \"Principal\": {\n \"Service\": \"delivery.logs.amazonaws.com\"\n },\n \"Action\": \"kms:GenerateDataKey*\",\n \"Resource\": \"*\"\n}"}],"content":"AD\n`There is no such Role called Cloudfront service link role. Cloudfront uses the awslogsdelivery to deliver logs to s3 bucket -> https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html#AccessLogsBucketAndFileOwnership","upvote_count":"13"},{"timestamp":"1633521660.0","content":"B & D : These are correct options —\n\nIf you enabled server-side encryption for your Amazon S3 bucket using AWS KMS-managed keys (SSE-KMS) with a customer-managed Customer Master Key (CMK), you must add the following to the key policy for your CMK to enable writing log files to the bucket. You cannot use the default CMK because CloudFront won't be able to upload the log files to the bucket.\n\nURL : https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html#AccessLogsKMSPermissions","comments":[{"timestamp":"1634760540.0","upvote_count":"1","content":"According to that link and the paragraph you have pasted, the correct answers are C & D","poster":"pablobairat","comment_id":"427998","comments":[{"comment_id":"601479","content":"C is wrong. You cannot modify a key policy of a AWS managed KMS key.\nhttps://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#aws-managed-cmk","poster":"bobsmith2000","timestamp":"1652514900.0","upvote_count":"3"}]}],"comment_id":"329642","upvote_count":"11","poster":"SD13"},{"content":"Selected Answer: BD\nfuk u that's why","timestamp":"1697120220.0","comment_id":"1041790","poster":"kiwtirApp","upvote_count":"1"},{"content":"according to AWS:\nIf the S3 bucket for your standard logs uses server-side encryption with AWS KMS keys (SSE-KMS) using a customer managed key, you must add the following statement to the key policy for your customer managed key. This allows CloudFront to write log files to the bucket. (You can’t use SSE-KMS with the AWS managed key because CloudFront won’t be able to write log files to the bucket.)\nwith this I go with A&D","upvote_count":"2","timestamp":"1638608340.0","poster":"ryu10_09","comments":[{"poster":"bobsmith2000","upvote_count":"3","timestamp":"1652514960.0","content":"There's no such thing as \"AWS Logs Delivery account\".\nIt's a service","comment_id":"601480"}],"comment_id":"493599"},{"upvote_count":"3","timestamp":"1636002660.0","content":"A,D\nAWS Logs Delivery account + new repository","comment_id":"459410","poster":"student22"},{"content":"It's A D as tgv's comment","poster":"andylogan","timestamp":"1635887760.0","comment_id":"446455","upvote_count":"1"},{"timestamp":"1635569340.0","poster":"Kopa","upvote_count":"2","content":"Coorect A, D","comment_id":"437832"},{"comment_id":"436547","upvote_count":"6","content":"AAA DDD\n---\nCloudFront service role doesn't exist. It uses \"delivery.logs.amazonaws.com\" which is the \"awslogsdelivery account\"\n---> https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html","poster":"tgv","timestamp":"1635565440.0"},{"comments":[{"poster":"blackgamer","comment_id":"436050","content":"Change to A& D after reading this document. \n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/encrypt-log-data-kms.html","upvote_count":"1","timestamp":"1635424140.0"}],"comment_id":"433967","poster":"blackgamer","timestamp":"1634932200.0","upvote_count":"1","content":"B &has D for me."},{"poster":"denccc","comment_id":"431563","upvote_count":"1","content":"B and D","timestamp":"1634817960.0"},{"comment_id":"413929","upvote_count":"2","timestamp":"1634391900.0","poster":"WhyIronMan","content":"I'll go with B,D"},{"content":"it's A&D, no such thing as cloudfront service role","timestamp":"1634301240.0","comments":[{"timestamp":"1634306220.0","comment_id":"397854","upvote_count":"5","content":"Whichever service delivers logs to S3 needs to have permission to use the CMK. In this case CloudFront delivers the logs to AWS Logs Delivery account. Then AWS Logs Delivery account delivers the logs to S3. In this case, CloudFront doesn't encrypt the logs - the AWS Logs Delivery account does. CloudFront isn't even aware of the fact that the logs are being encrypted.","poster":"DashL"}],"poster":"Waiweng","comment_id":"383935","upvote_count":"7"},{"comment_id":"377960","upvote_count":"3","poster":"ss160700","content":"A & D - CloudFront will use \"Service\": \"delivery.logs.amazonaws.com\" to log to S3. Need action\": \"kms:GenerateDataKey*\" to the principal.","comments":[{"upvote_count":"1","comment_id":"601481","timestamp":"1652515380.0","content":"Isn't it a service? A states that it's account, but in documentation it's shown as service","poster":"bobsmith2000"}],"timestamp":"1634284860.0"},{"poster":"[Removed]","content":"why not E?","comment_id":"360577","timestamp":"1634270400.0","upvote_count":"3"},{"content":"Thanks for the link SD13. My first choice was AD, but BD seems right.","upvote_count":"2","timestamp":"1633733220.0","comment_id":"337960","poster":"CarisB"},{"upvote_count":"1","poster":"Pupu86","timestamp":"1633349820.0","comment_id":"328579","content":"Option C is correct as SSE-S3 is needed minimally to encrypt at rest and reduce unnecessary cost of SSE-KMS."},{"content":"BD\nFor me because\nA seems not right because it using AWS SSE you can't use your own key","timestamp":"1633251360.0","poster":"M_Asep","upvote_count":"1","comment_id":"323800"},{"timestamp":"1633157580.0","upvote_count":"3","content":"BD for me","comment_id":"318666","poster":"eji"},{"upvote_count":"1","content":"Correct Option : A D","poster":"SD13","comment_id":"316631","timestamp":"1632962520.0"},{"timestamp":"1632381060.0","comment_id":"313451","content":"AD for me","poster":"wasabidev","upvote_count":"2"},{"upvote_count":"2","comment_id":"310815","poster":"sek12324","timestamp":"1632112380.0","content":"CE\nThe first time you create a CodeCommit repository in a new AWS Region in your AWS account, CodeCommit creates an AWS managed CMK (the aws/codecommit key) in that same AWS Region in AWS Key Management Service (AWS KMS). This key is used only by CodeCommit (the aws/codecommit key). It is stored in your AWS account. CodeCommit uses this AWS managed CMK to encrypt and decrypt the data in this and all other CodeCommit repositories within that region in your AWS account."}],"answer_description":""},{"id":"HPLOgXgywBxuiem4lSDm","answer_images":[],"choices":{"C":"Create a separate ALB for each device type. Create one Auto Scaling group behind each ALB. Use Amazon Route 53 to route to different ALBs depending on the User-Agent HTTP header.","B":"Move content to Amazon S3. Create an Amazon CloudFront distribution to serve content out of the S3 bucket. Use Lambda@Edge to load different resources based on the User-Agent HTTP header.","D":"Move content to Amazon S3. Create an Amazon CloudFront distribution to serve content out of the S3 bucket. Use the User-Agent HTTP header to load different content.","A":"Create separate Auto Scaling groups based on device types. Switch to Network Load Balancer (NLB). Use the User-Agent HTTP header in the NLB to route to a different set of EC2 instances."},"question_id":669,"discussion":[{"poster":"liono","comment_id":"215145","comments":[{"poster":"dolphina02","upvote_count":"1","content":"I'll say.","comment_id":"216290","timestamp":"1632385560.0"}],"upvote_count":"17","content":"B seems to fulfill the requirements","timestamp":"1632249180.0"},{"upvote_count":"1","comment_id":"773407","timestamp":"1673524560.0","poster":"syaldram","content":"Selected Answer: B\nThe correct answer is B. I always associate gaming servers with Network Load Balances (NLB)s."},{"comment_id":"648520","content":"B is obs","upvote_count":"1","timestamp":"1660840200.0","poster":"Ni_yot"},{"upvote_count":"1","poster":"pal40sg","timestamp":"1646471160.0","content":"Selected Answer: B\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/dynamically-route-viewer-requests-to-any-origin-using-lambdaedge/","comment_id":"561322"},{"content":"Selected Answer: B\nit is B","poster":"shotty1","timestamp":"1643133000.0","upvote_count":"1","comment_id":"532309"},{"comment_id":"521494","poster":"pititcu667","upvote_count":"1","content":"Selected Answer: B\nB seems correct","timestamp":"1641901200.0"},{"comment_id":"492424","timestamp":"1638446340.0","upvote_count":"1","content":"it is B","poster":"AzureDP900"},{"upvote_count":"1","poster":"andylogan","timestamp":"1635961140.0","comment_id":"446458","content":"It's B"},{"comment_id":"436548","poster":"tgv","timestamp":"1635928560.0","content":"BBB\n---","upvote_count":"1"},{"timestamp":"1635889140.0","poster":"blackgamer","comment_id":"433973","upvote_count":"2","content":"Answer is B. Refer below on the explanation.\n\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/dynamically-route-viewer-requests-to-any-origin-using-lambdaedge/"},{"comment_id":"416964","content":"For those who confuse between B and D. Check this link https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html#lambda-examples-redirecting-examples","timestamp":"1635883260.0","poster":"tiffanny","upvote_count":"3","comments":[{"upvote_count":"2","poster":"kirrim","timestamp":"1636145880.0","comment_id":"463613","content":"Totally agree on B, and great document!\n\nHere's the exact fragment URL on that page to the code to redirect based on device type:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html#lambda-examples-vary-on-device-type"}]},{"comment_id":"413935","content":"I'll go with B","poster":"WhyIronMan","upvote_count":"1","timestamp":"1635774960.0"},{"comment_id":"385602","poster":"hk436","content":"B for sure.!","upvote_count":"1","timestamp":"1635509100.0"},{"timestamp":"1635388320.0","poster":"Waiweng","comment_id":"357152","content":"it's B","upvote_count":"4"},{"poster":"blackgamer","upvote_count":"1","comment_id":"343309","content":"The answer is B.","timestamp":"1635286260.0"},{"poster":"wasabidev","timestamp":"1635160620.0","content":"B is correct","upvote_count":"3","comment_id":"313438"},{"upvote_count":"1","timestamp":"1634804940.0","content":"will go with B","poster":"Kian1","comment_id":"294269"},{"comment_id":"284430","upvote_count":"3","poster":"Ebi","timestamp":"1634772180.0","content":"I go with B"},{"timestamp":"1633885440.0","upvote_count":"1","poster":"Bulti","comment_id":"257048","content":"B is correct"},{"upvote_count":"1","comment_id":"253293","content":"B is the answer. User-agent is from http header which in layer 7.","timestamp":"1633760220.0","poster":"gaussye"},{"content":"B is correct","upvote_count":"1","poster":"T14102020","timestamp":"1633442760.0","comment_id":"241739"},{"upvote_count":"4","poster":"Cantaloupe","comment_id":"240602","content":"A is wrong. NLB is layer 4 and cannot filter on user-agent\nB is the answer \nhttps://docs.aws.amazon.com/lambda/latest/dg/lambda-edge.html","timestamp":"1633376880.0"},{"upvote_count":"2","poster":"cloudgc","comment_id":"234120","comments":[{"comment_id":"234571","timestamp":"1633193040.0","comments":[{"upvote_count":"2","content":"How D fulfill the requirement?","timestamp":"1634354700.0","comment_id":"271413","poster":"01037"},{"comment_id":"343794","content":"\"You can configure CloudFront to CACHE objects...\" It says \"cache\" not \"to load\" tho","timestamp":"1635368820.0","upvote_count":"1","poster":"thai"},{"comment_id":"275790","poster":"shammous","upvote_count":"2","content":"option B is a more sophisticated solution than option D, even if D is possible: \"You can configure CloudFront to cache objects based on values in the User-Agent header, but we don't recommend it. The User-Agent header has many possible values, and caching based on those values would cause CloudFront to forward significantly more requests to your origin.\"\nRef: Same link shared by cloudgc above\nD is actually OK as the content to deliver depends only on the \"device type\". If more complicated criteria was involved, Lambda@Edge would be needed...\nThis is a bit tricky and any thoughts here are welcome.","timestamp":"1634491980.0"}],"upvote_count":"1","poster":"jackdryan","content":"Very good link. Both B and D seem to fulfill the requirement"}],"timestamp":"1632469380.0","content":"Confused B or D - https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/RequestAndResponseBehaviorCustomOrigin.html#request-custom-user-agent-header"},{"upvote_count":"3","timestamp":"1632449280.0","content":"I'll go with B","poster":"jackdryan","comment_id":"233428"},{"upvote_count":"3","timestamp":"1632389640.0","comment_id":"217020","content":"B: Implementation details here https://blog.frankfu.com.au/2019/03/05/modifying-user-agent-based-on-bot-or-human-for-cloudfront-via-lambda-edge/","poster":"bbnbnuyh"}],"answer_ET":"B","exam_id":32,"timestamp":"2020-11-05 02:26:00","question_text":"A company's service for video game recommendations has just gone viral. The company has new users from all over the world. The website for the service is hosted on a set of Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). The website consists of static content with different resources being loaded depending on the device type.\nUsers recently reported that the load time for the website has increased. Administrators are reporting high loads on the EC2 instances that host the service.\nWhich set actions should a solutions architect take to improve response times?","url":"https://www.examtopics.com/discussions/amazon/view/36118-exam-aws-certified-solutions-architect-professional-topic-1/","answer_description":"","answer":"B","question_images":[],"topic":"1","answers_community":["B (100%)"],"isMC":true,"unix_timestamp":1604539560},{"id":"viNVDUdeC2SYX9os8FCg","answer_images":[],"timestamp":"2020-11-08 11:21:00","answer":"B","topic":"1","answers_community":["B (100%)"],"answer_ET":"B","question_id":670,"exam_id":32,"discussion":[{"content":"Come on Guys !! .. Ive seen the discussions below ... if you can not get this one then you are no where near to taking the exam !! .. the problem here is a WRITE issue .. so for that we would use SQS to help hold the solution until it is ready to be written ... Elastichche is for caching solution .. so we would use that in READ situation .... ANSWER IS B .. really concerned why the incorrect answer appears in the answer box","comment_id":"234274","poster":"petebear55","timestamp":"1633396920.0","upvote_count":"47","comments":[{"comment_id":"239253","content":"Agree 100%, these questions are the easy ones...","timestamp":"1633568340.0","upvote_count":"7","poster":"PAUGURU"},{"comment_id":"439580","poster":"student22","upvote_count":"6","content":"Well said! Answers in the answer box make me worried. It's good that we have a great community here.","timestamp":"1636202220.0"},{"content":"maybe you should not take the exam yourself. havn't you heard of write-through cache before:\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html#Strategies.WriteThrough","poster":"ryu10_09","timestamp":"1637655120.0","comment_id":"484821","comments":[{"timestamp":"1660490280.0","content":"I guess you kind of just proved his point.","comment_id":"646792","upvote_count":"3","poster":"gerhardbl"},{"poster":"redipa","timestamp":"1654283340.0","upvote_count":"3","comment_id":"611240","content":"caching is for read speed. write-through cache doesn't increase write speed to the DB, it just means any new writes to the DB are also written to the cache along the way."}],"upvote_count":"2"}]},{"poster":"smartassX","upvote_count":"31","content":"B is the answer! SQS with Lambda.","comments":[{"poster":"beso","content":"SQS with lambda is eventually consistent, we are looking here for strong consistent which is option A?","upvote_count":"1","comments":[{"timestamp":"1634591340.0","poster":"petebear55","comment_id":"256913","content":"\"large and unpredictable volumes of traffic periodically\" beso a can not be the answer","upvote_count":"3"}],"timestamp":"1632866880.0","comment_id":"220986"},{"comment_id":"256906","content":"WELL DONE","poster":"petebear55","timestamp":"1634506500.0","upvote_count":"1"}],"comment_id":"217625","timestamp":"1632661500.0"},{"upvote_count":"1","comment_id":"763656","content":"Selected Answer: B\nI'll go with B","timestamp":"1672656840.0","poster":"evargasbrz"},{"timestamp":"1665383700.0","content":"Selected Answer: B\nB.. SQS to capture new writes before storing in DB will help\nD is wrong as it only provide enhancement during READ operation.","comment_id":"690771","poster":"skywalker","upvote_count":"1"},{"upvote_count":"1","comment_id":"622834","timestamp":"1656281040.0","poster":"kangtamo","content":"Selected Answer: B\nAgree with B: SQS."},{"comments":[{"timestamp":"1667330460.0","comment_id":"709398","upvote_count":"2","poster":"Cal88","content":"They don’t wanna introduce change to the “data mode” so no change to the DB type.\nNo one said anything about changing the way to process the writes.\nif you choose A , what capacity will you chose to scale up?\nRemember its unpredictable traffic which a really important keyword in the question.\nB is the best way to achieve what is asked in the question"}],"comment_id":"606297","timestamp":"1653335100.0","upvote_count":"1","poster":"user0001","content":"A :because they dont want to introduce change \"A solutions architect must provide a solution that does not alter the underlying data architecture\"\nB would be better if they do allow changes"},{"timestamp":"1638490800.0","comment_id":"492861","upvote_count":"1","content":"D is NOT correct. The write-through strategy adds data or updates data in the cache whenever data is written to the database. Every write involves two trips:\nA write to the cache\nA write to the database\nWhich adds latency to the process.","poster":"vbal"},{"timestamp":"1638446520.0","content":"going with B","poster":"AzureDP900","comment_id":"492430","upvote_count":"1"},{"upvote_count":"1","comment_id":"484822","poster":"ryu10_09","content":"I would still go with using memcached and the write-through capabilities for it:https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html#Strategies.WriteThrough\nSQS is also an option here","timestamp":"1637655180.0"},{"timestamp":"1636123920.0","upvote_count":"1","poster":"tgv","content":"BBB\n---","comment_id":"436550"},{"comment_id":"433975","timestamp":"1636100400.0","content":"B is the answer.","upvote_count":"1","poster":"blackgamer"},{"comment_id":"431531","timestamp":"1635867120.0","poster":"tiffanny","upvote_count":"1","content":"lol but in real life, aws user are using A option. LOL because they are too lazy to change the infra."},{"poster":"mustpassla","comment_id":"366868","content":"B, use SQS to ensures that submissions are not dropped. A is not correct as scaling up manually cant help as the volumes of traffic are unpredictable.","upvote_count":"4","timestamp":"1635841440.0"},{"upvote_count":"2","poster":"Waiweng","timestamp":"1635770880.0","content":"it's B","comment_id":"357156"},{"comment_id":"313431","timestamp":"1635371760.0","upvote_count":"2","poster":"wasabidev","content":"B. in my opinion, one of the easiest question"},{"content":"going for B","timestamp":"1635222660.0","poster":"Kian1","comment_id":"294273","upvote_count":"3"},{"content":"B for sure","timestamp":"1634839980.0","upvote_count":"4","comment_id":"284431","poster":"Ebi"},{"comment_id":"283701","timestamp":"1634671560.0","upvote_count":"4","content":"B\nThis is literally the use case for SQS why are people fighting about it?","poster":"Trap_D0_r"},{"content":"I think its B\nsince the sign up page parameters would be unidentical and doesnt lead to elastic cache( read only)","upvote_count":"2","poster":"blackVoid","timestamp":"1634663880.0","comment_id":"257255"},{"content":"B is correct. Remember with least amount of changes and unpredictable spike in volume it's better to go with a decoupled arch such as SQS.","upvote_count":"2","timestamp":"1634639580.0","comments":[{"upvote_count":"1","comment_id":"606298","content":"they dont want change so B is not right","poster":"user0001","timestamp":"1653335160.0"}],"comment_id":"257049","poster":"Bulti"},{"timestamp":"1634280480.0","comment_id":"241764","upvote_count":"3","content":"For sure B. \nQuestion is about write issue.","poster":"T14102020"},{"content":"B - Load is unpredictable and periodic which negates option A which says 'anticipated demand'.","upvote_count":"3","timestamp":"1633370340.0","poster":"cloudgc","comment_id":"234130"},{"poster":"jackdryan","comment_id":"233431","content":"I'll go with B","timestamp":"1633367100.0","upvote_count":"5"},{"upvote_count":"9","content":"B.\n\nA is wrong because the volumes of traffic are unpredictable and you may not scale up appropriately.\n\nC is wrong because changing to Dynamo is changing the underlying data model (relational to non-relational)\n\nD is wrong because using ElastiCache would not \"increase write capacity to the DB instance\". Additionally even with high availability, in the event that the primary node in the Memcached cluster goes down, there can be data loss.\n\nB can scale and would not be subject to such data loss.","poster":"avland","comment_id":"225955","timestamp":"1633329840.0"},{"comments":[{"content":"if write capacity hit, no guaranteed that txn will be committed, sqs can guaranteed that at least 1 delivery","poster":"Kelvin1477","comment_id":"225336","timestamp":"1632894900.0","upvote_count":"1"},{"content":"A is incorrect. Intuitively, A should be more easier and less effort but the question has the keyword 'unpredictable' but in the answer it said 'anticipated demand'.","timestamp":"1635449520.0","comment_id":"314289","poster":"bachdx","upvote_count":"1"}],"poster":"XRiddlerX","upvote_count":"1","content":"Answer is A\n\nAlthough management will expect large and unpredictable volumes of traffic periodically, the solution architect needs to build a solution that when a user submits the information it HAS to be committed to the database. This to me is a transnational based architecture. Therefore, the only option to support that requirement, despite the unpredictable volumes of traffic, is Answer A.\n\nB is INCORRECT as it supports a decoupled system and eventual consistency.\nC is INCORRECT because this will change the data model which doesn't support the managements requirement\nD IS INCORRECT because you use ElasticCache for reads NOT writes.","timestamp":"1632768060.0","comment_id":"219137"},{"comment_id":"215153","content":"D\nAmazon ElastiCache is an ideal front-end for data stores like Amazon RDS or Amazon DynamoDB, providing a high-performance middle tier for applications with extremely high request rates and/or low latency requirements.","timestamp":"1632261180.0","poster":"liono","upvote_count":"2","comments":[{"timestamp":"1632659280.0","upvote_count":"4","content":"writes not reads","comment_id":"216475","poster":"keos"},{"comment_id":"219216","timestamp":"1632808620.0","content":"\"build a solution that does not change the underlying data model\". Why are you moving from SQL to NoSQL?","upvote_count":"2","poster":"cpd"},{"timestamp":"1634363340.0","comment_id":"256904","content":"yOU SHOULD BE LOOKING FOR A SOLUTION THAT WILL HELP WITH WRITES NOT READS .. SO GO FOR SQS .... READ THE QUESTION IS THE BEST ADVICE IVE BEEN GIVEN FOR THE EXAM .. I SUGGEST YOU DO THIS AGAIN","poster":"petebear55","upvote_count":"2"}]}],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/36447-exam-aws-certified-solutions-architect-professional-topic-1/","answer_description":"","unix_timestamp":1604830860,"question_text":"A company is planning a large event where a promotional offer will be introduced. The company's website is hosted on AWS and backed by an Amazon RDS for\nPostgreSQL DB instance. The website explains the promotion and includes a sign-up page that collects user information and preferences. Management expects large and unpredictable volumes of traffic periodically, which will create many database writes. A solutions architect needs to build a solution that does not change the underlying data model and ensures that submissions are not dropped before they are committed to the database.\nWhich solution meets these requirements?","question_images":[],"choices":{"B":"Use Amazon SQS to decouple the application and database layers. Configure an AWS Lambda function to write items from the queue into the database.","C":"Migrate to Amazon DynamoDB and manage throughput capacity with automatic scaling.","A":"Immediately before the event, scale up the existing DB instance to meet the anticipated demand. Then scale down after the event.","D":"Use Amazon ElastiCache for Memcached to increase write capacity to the DB instance."}}],"exam":{"isImplemented":true,"lastUpdated":"11 Apr 2025","provider":"Amazon","isBeta":false,"name":"AWS Certified Solutions Architect - Professional","numberOfQuestions":1019,"isMCOnly":false,"id":32},"currentPage":134},"__N_SSP":true}