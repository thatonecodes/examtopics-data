{"pageProps":{"questions":[{"id":"yVyHf3paEDM4WLQt9zX6","url":"https://www.examtopics.com/discussions/amazon/view/84875-exam-aws-certified-solutions-architect-associate-saa-c03/","topic":"1","unix_timestamp":1665327720,"answer_images":[],"question_images":[],"question_id":576,"choices":{"D":"Set up an AWS Direct Connect connection between the on-premises network and AWS. Deploy an S3 File Gateway on premises. Create a public virtual interface (VIF) to connect to the S3 File Gateway. Create an S3 bucket. Create a new NFS file share on the S3 File Gateway. Point the new file share to the S3 bucket. Transfer the data from the existing NFS file share to the S3 File Gateway.","C":"Deploy an S3 File Gateway on premises. Create a public service endpoint to connect to the S3 File Gateway. Create an S3 bucket. Create a new NFS file share on the S3 File Gateway. Point the new file share to the S3 bucket. Transfer the data from the existing NFS file share to the S3 File Gateway.","A":"Create an S3 bucket. Create an IAM role that has permissions to write to the S3 bucket. Use the AWS CLI to copy all files locally to the S3 bucket.","B":"Create an AWS Snowball Edge job. Receive a Snowball Edge device on premises. Use the Snowball Edge client to transfer data to the device. Return the device so that AWS can import the data into Amazon S3."},"answer_description":"","timestamp":"2022-10-09 17:02:00","exam_id":31,"answers_community":["B (87%)","12%"],"answer":"B","isMC":true,"discussion":[{"comment_id":"695468","content":"Selected Answer: B\nLet's analyse this:\n\nB. On a Snowball Edge device you can copy files with a speed of up to 100Gbps. 70TB will take around 5600 seconds, so very quickly, less than 2 hours. The downside is that it'll take between 4-6 working days to receive the device and then another 2-3 working days to send it back and for AWS to move the data onto S3 once it reaches them. Total time: 6-9 working days. Bandwidth used: 0.\n\nC. File Gateway uses the Internet, so maximum speed will be at most 1Gbps, so it'll take a minimum of 6.5 days and you use 70TB of Internet bandwidth.\n\nD. You can achieve speeds of up to 10Gbps with Direct Connect. Total time 15.5 hours and you will use 70TB of bandwidth. However, what's interesting is that the question does not specific what type of bandwidth? Direct Connect does not use your Internet bandwidth, as you will have a dedicate peer to peer connectivity between your on-prem and the AWS Cloud, so technically, you're not using your \"public\" bandwidth. \n\nThe requirements are a bit too vague but I think that B is the most appropriate answer, although D might also be correct if the bandwidth usage refers strictly to your public connectivity.","timestamp":"1727009400.0","upvote_count":"122","poster":"Gatt","comments":[{"poster":"YDUYGU","timestamp":"1743949020.0","comment_id":"1558267","content":"DX Lead times are often longer than I month to establish a new connection.That’s why D is the wrong answer on the other hand.","upvote_count":"1"},{"comment_id":"723182","poster":"abhishek_m89","content":"and it says, \"The total storage is 70 TB and is no longer growing\". Thats why it should be B.","upvote_count":"6","timestamp":"1669009080.0"},{"upvote_count":"10","timestamp":"1667995740.0","comments":[{"poster":"pentium75","timestamp":"1703420460.0","upvote_count":"4","content":"It does, because option D says \"SET UP a DirectConnect connection\", not \"use an existing DirectConnect connection\".","comment_id":"1104576"}],"comment_id":"714543","poster":"Gatt","content":"I will add that the question does not specify if the company already has DA in place or not. If they don't have DA in place, it will take a long time (weeks) for DA connectivity to be setup. Another point for B here, as Snowball is much quicker from this perspective."},{"poster":"OBIOHAnze","timestamp":"1716421500.0","upvote_count":"3","comment_id":"1216086","content":"B is the correct answer because the migration needs to be completed as soon as possible with limited bandwidth"}]},{"timestamp":"1665327720.0","content":"Selected Answer: B\nAs using the least possible network bandwidth.","upvote_count":"35","comment_id":"690292","poster":"tuloveu"},{"comment_id":"1400755","upvote_count":"1","content":"Selected Answer: B\nAWS snowball is ideal to transfer TB-PB of data from an on-prem connection to AWS cloud with little amount of internet connection required","poster":"melvis8","timestamp":"1742420340.0"},{"upvote_count":"1","poster":"MGKYAING","timestamp":"1735139760.0","comment_id":"1331613","content":"Selected Answer: B\nAWS Snowball Edge is specifically designed for scenarios requiring the transfer of large datasets (terabytes or petabytes) with minimal network bandwidth usage.\nIt is ideal for one-time, large-scale migrations like the 70 TB of video files in this case.\nFor ongoing, smaller-scale data transfers, solutions like S3 File Gateway or AWS DataSync may be more appropriate."},{"poster":"Sjb_009","content":"Selected Answer: B\nThe least possible network bandwidth says it all","timestamp":"1734626040.0","upvote_count":"2","comment_id":"1329067"},{"content":"B is Correct Answer","poster":"OmarRefaat","upvote_count":"1","comment_id":"1299880","timestamp":"1729313280.0"},{"poster":"Andreshere","upvote_count":"2","content":"Selected Answer: B\nThe question states that the storage is no longer growing. This implies that we don’t need to make any kind of data synchronization. Additionally, the total storage is 70 TB, which is a large amount of data. This implies high transfer costs. So, we can discard A, C and D options.\nCorrect option: A. \nA Snowball device is a physical storage device which supports large data transfers. It is commonly used for transporting huge amounts of data from on-premises to AWS. Concretely, Snowball Edge is suitable for data transfers up to 80 TB. The transport times are between 1 and 2 weeks, so in case that we have hundreds of terabytes of data, we get them earlier than using Internet.\nIn case that we need to transfer petabytes of data, it is recommended to use AWS Snowmobile, which is a physical track that transports data, up to 10 PB.","comments":[{"content":"The correct answer is B not A, i misswrite that.","upvote_count":"2","timestamp":"1704733140.0","comment_id":"1116816","poster":"Andreshere"}],"timestamp":"1726903560.0","comment_id":"1116815"},{"comment_id":"1286413","upvote_count":"1","poster":"Sandy4v","content":"Selected Answer: B\nB is the correct answer","timestamp":"1726756620.0"},{"comment_id":"1286103","content":"I choose B\nThe key is:\n- The total storage is 70 TB and is no longer growing. \n- Using the least possible network bandwidth.\n\nNo longer Growing mean 1 time migrate, no need file gw\nLeast Possible Nw Bandwith --> Snowball Edge","timestamp":"1726714020.0","upvote_count":"2","poster":"KerasHanog"},{"content":"Ans B. Agree: Snowball Edge is designed for these types of operations; its more robust and secure because the operation is completed (relatively) quickly. Ans C doesn't really fly.","comment_id":"1263406","poster":"PaulGa","timestamp":"1723281840.0","upvote_count":"1"},{"timestamp":"1723135260.0","content":"Select answer: B\nWhy? The essence of the S3 File Gateway is to provide a seamless interface for on-premises apps to store and retrieve data in Amazon S3 using standard protocols such as NFS and SMB. On the other hand,\nAs I write this, the first AWS Official use case for Snowball is to migrate data especially when network conditions are limited.\n\nThe question is a bit tricky. But applying simple logical linguistic analysis, \"as soon as possible\" coupled with \"the least network bandwidth possible\" means the question's focal point is network bandwidth. So, whatever the least network bandwidth is, it`s corresponding time to get the data into AWS S3 bucket is the value for \"as soon as possible\".","upvote_count":"3","poster":"Johnoppong101","comment_id":"1262615"},{"comment_id":"1259420","content":"Selected Answer: B\nSnowball transfers data faster than the internet, and in this case, as the size of the data is large, so this would be the best option","poster":"Supriya_T","upvote_count":"1","timestamp":"1722521220.0"},{"timestamp":"1721656500.0","poster":"PR5577","upvote_count":"1","comment_id":"1253117","content":"Selected Answer: B\nQuestion specifically mentions minimal use of Network bandwidth. Storage gateway are mostly for using cloud storage on-premises. Usually data is copied one time using DataSync or other services like Snow devices."},{"content":"Selected Answer: B\nMinimising network bandwidth","comment_id":"1245991","poster":"jhg96","upvote_count":"1","timestamp":"1720684080.0"},{"poster":"ChymKuBoy","content":"Selected Answer: B\nB for sure","upvote_count":"1","timestamp":"1717565280.0","comment_id":"1224532"},{"timestamp":"1717054500.0","poster":"Ishu_","content":"Selected Answer: B\nThrough Snowball edge, large amount of data can be transferred to AWS faster and securely.","upvote_count":"1","comment_id":"1221444"},{"upvote_count":"1","content":"Selected Answer: B\nB is the only solution which satisfies the bandwidth requirement.","poster":"DrvnkMario","comment_id":"1218293","timestamp":"1716638580.0"}],"answer_ET":"B","question_text":"A company uses NFS to store large video files in on-premises network attached storage. Each video file ranges in size from 1 MB to 500 GB. The total storage is 70 TB and is no longer growing. The company decides to migrate the video files to Amazon S3. The company must migrate the video files as soon as possible while using the least possible network bandwidth.\nWhich solution will meet these requirements?"},{"id":"aUfSJEqlTOQSuBXAxlQj","question_images":[],"answer":"C","answers_community":["C (100%)"],"timestamp":"2022-10-11 09:57:00","question_text":"A company has a website hosted on AWS. The website is behind an Application Load Balancer (ALB) that is configured to handle HTTP and HTTPS separately. The company wants to forward all requests to the website so that the requests will use HTTPS.\nWhat should a solutions architect do to meet this requirement?","url":"https://www.examtopics.com/discussions/amazon/view/85121-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_images":[],"unix_timestamp":1665475020,"isMC":true,"exam_id":31,"question_id":577,"choices":{"A":"Update the ALB's network ACL to accept only HTTPS traffic.","B":"Create a rule that replaces the HTTP in the URL with HTTPS.","C":"Create a listener rule on the ALB to redirect HTTP traffic to HTTPS.","D":"Replace the ALB with a Network Load Balancer configured to use Server Name Indication (SNI)."},"discussion":[{"comments":[{"comment_id":"751255","timestamp":"1671555780.0","poster":"Buruguduystunstugudunstuy","upvote_count":"17","content":"Option A. Updating the ALB's network ACL to accept only HTTPS traffic is not a valid solution because the network ACL is used to control inbound and outbound traffic at the subnet level, not at the listener level.\n\nOption B. Creating a rule that replaces the HTTP in the URL with HTTPS is not a valid solution because this would not redirect the traffic to the HTTPS listener.\n\nOption D. Replacing the ALB with a Network Load Balancer configured to use Server Name Indication (SNI) is not a valid solution because it would not address the requirement to redirect HTTP traffic to HTTPS."}],"timestamp":"1671555720.0","poster":"Buruguduystunstugudunstuy","content":"Selected Answer: C\nC. Create a listener rule on the ALB to redirect HTTP traffic to HTTPS.\n\nTo meet the requirement of forwarding all requests to the website so that the requests will use HTTPS, a solutions architect can create a listener rule on the ALB that redirects HTTP traffic to HTTPS. This can be done by creating a rule with a condition that matches all HTTP traffic and a rule action that redirects the traffic to the HTTPS listener. The HTTPS listener should already be configured to accept HTTPS traffic and forward it to the target group.","upvote_count":"30","comment_id":"751254"},{"poster":"masetromain","comment_id":"694562","timestamp":"1665730800.0","content":"Selected Answer: C\nAnswer C : \nhttps://docs.aws.amazon.com/fr_fr/elasticloadbalancing/latest/application/create-https-listener.html\nhttps://aws.amazon.com/fr/premiumsupport/knowledge-center/elb-redirect-http-to-https-using-alb/","upvote_count":"15"},{"upvote_count":"1","poster":"satyaammm","comment_id":"1335236","content":"Selected Answer: C\nRedirecting the network on to HTTPS is the requirement here.","timestamp":"1735750620.0"},{"upvote_count":"5","poster":"cookieMr","content":"Selected Answer: C\nA. Network ACLs operate at subnet level and control inbound and outbound traffic. Updating the network ACL alone will not enforce the redirection of HTTP to HTTPS.\n\nB. This approach would require modifying application code or server configuration to perform URL rewrite. It is not an optimal solution as it adds complexity and potential maintenance overhead. Moreover, it does not leverage the ALB's capabilities for handling HTTP-to-HTTPS redirection.\n\nD. While NLB can handle SSL/TLS termination using SNI for routing requests to different services, replacing the ALB solely to enforce HTTP-to-HTTPS redirection would be an unnecessary and more complex solution.\n\nTherefore, the recommended approach is to create a listener rule on the ALB to redirect HTTP traffic to HTTPS. By configuring a listener rule, you can define a redirect action that automatically directs HTTP requests to their corresponding HTTPS versions.","comment_id":"929415","timestamp":"1727760360.0"},{"comment_id":"1282092","poster":"PaulGa","content":"Selected Answer: C\nAns C - don't re-invent; just re-direct","upvote_count":"1","timestamp":"1726057680.0"},{"poster":"awsgeek75","comment_id":"1122635","content":"Selected Answer: C\nhttps://repost.aws/knowledge-center/elb-redirect-http-to-https-using-alb\n\nSteps 6-8 tells exactly how to do this:\n\"6. Select a load balancer, and then choose HTTP Listener.\n7. Under Rules, choose View/edit rules.\n8. Choose Edit Rule to modify the existing default rule to redirect all HTTP requests to HTTPS. Or, insert a rule between the existing rules (if appropriate for your use case).\"","timestamp":"1705246800.0","upvote_count":"3"},{"poster":"Ruffyit","content":"C. Create a listener rule on the ALB to redirect HTTP traffic to HTTPS.","upvote_count":"3","timestamp":"1698383640.0","comment_id":"1055127"},{"upvote_count":"3","comment_id":"1048901","poster":"AWSStudyBuddy","content":"Selected Answer: C\nThis solution meets all of the requirements:\n\nForward all requests to the website so that the requests will use HTTPS: The ALB can be configured to redirect all HTTP traffic to HTTPS.\nThe other options are not as good for this scenario:\n\nA. Updating the ALB's network ACL to accept only HTTPS traffic will prevent users from accessing the website using HTTP.\nB. Creating a rule that replaces the HTTP in the URL with HTTPS will not prevent users from accessing the website using HTTP.\nD. Replacing the ALB with a Network Load Balancer configured to use Server Name Indication (SNI) is not necessary because the ALB can be configured to redirect all HTTP traffic to HTTPS.","timestamp":"1697813580.0"},{"upvote_count":"3","timestamp":"1696296240.0","comment_id":"1023519","content":"I hate this question description \"The company wants to forward all requests to the website so that the requests will use HTTPS.\"","poster":"Tom123456ac"},{"timestamp":"1691607180.0","poster":"Guru4Cloud","upvote_count":"8","comment_id":"976959","content":"Selected Answer: C\nThe best solution is to create a listener rule on the Application Load Balancer (ALB) to redirect HTTP traffic to HTTPS (option C).\n\nHere is why:\n\nALB listener rules allow you to redirect traffic from one listener port (e.g. 80 for HTTP) to another (e.g. 443 for HTTPS). This achieves the goal to forward all requests over HTTPS.\nNetwork ACLs control traffic at the subnet level and cannot distinguish between HTTP and HTTPS requests to implement a redirect (option A incorrect).\nReplacing HTTP with HTTPS in the URL happens at the client side. It does not redirect at the ALB (option B incorrect).\nNetwork Load Balancers work at the TCP level and do not understand HTTP or HTTPS protocols. So they cannot redirect in this manner (option D incorrect)."},{"content":"Option C is the correct answer","timestamp":"1689781020.0","comment_id":"956773","poster":"miki111","upvote_count":"1"},{"poster":"Abrar2022","upvote_count":"1","content":"A solutions architect should create listen rules to direct http traffic to https.","timestamp":"1684399380.0","comment_id":"900945"},{"comment_id":"870877","poster":"cheese929","upvote_count":"2","timestamp":"1681558320.0","content":"Selected Answer: C\nC is correct. Traffic redirection will solve it."},{"upvote_count":"5","comment_id":"853240","poster":"elearningtakai","content":"Selected Answer: C\nThis rule can be created in the following way:\n1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/.\n2. In the navigation pane, choose Load Balancers.\n3. Select the ALB and choose Listeners.\n4. Choose View/edit rules and then choose Add rule.\n5. In the Add Rule dialog box, choose HTTPS.\n6. In the Default action dialog box, choose Redirect to HTTPS.\n7. Choose Save rules.\nThis listener rule will redirect all HTTP requests to HTTPS, ensuring that all traffic is encrypted.","timestamp":"1680007500.0"},{"content":"Selected Answer: C\nConfigure an HTTPS listener on the ALB: This step involves setting up an HTTPS listener on the ALB and configuring the security policy to use a secure SSL/TLS protocol and cipher suite.\n\nCreate a redirect rule on the ALB: The redirect rule should be configured to redirect all incoming HTTP requests to HTTPS. This can be done by creating a redirect rule that redirects HTTP requests on port 80 to HTTPS requests on port 443.\n\nUpdate the DNS record: The DNS record for the website should be updated to point to the ALB's DNS name, so that all traffic is routed through the ALB.\n\nVerify the configuration: Once the configuration is complete, the website should be tested to ensure that all requests are being redirected to HTTPS. This can be done by accessing the website using HTTP and verifying that the request is redirected to HTTPS.","timestamp":"1678467720.0","upvote_count":"2","comment_id":"835247","poster":"mell1222"},{"comment_id":"749364","timestamp":"1671415560.0","content":"Selected Answer: C\nOption C","poster":"career360guru","upvote_count":"1"},{"content":"C \nTo redirect HTTP traffic to HTTPS, a solutions architect should create a listener rule on the ALB to redirect HTTP traffic to HTTPS. Option A is not correct because network ACLs do not have the ability to redirect traffic. Option B is not correct because it does not redirect traffic, it only replaces the URL. Option D is not correct because a Network Load Balancer does not have the ability to handle HTTPS traffic.","poster":"Shasha1","upvote_count":"3","timestamp":"1670940420.0","comment_id":"744107"},{"comment_id":"723579","content":"C is correct","poster":"Wpcorgan","upvote_count":"1","timestamp":"1669039560.0"},{"timestamp":"1665475020.0","comment_id":"691880","content":"Selected Answer: C\nAnswer C: https://aws.amazon.com/premiumsupport/knowledge-center/elb-redirect-http-to-https-using-alb/","upvote_count":"5","poster":"hanhdroid"}],"answer_description":"","answer_ET":"C","topic":"1"},{"id":"eYBHiMJXlRi1TbeldVN3","isMC":true,"answer_ET":"A","topic":"1","answer":"A","exam_id":31,"url":"https://www.examtopics.com/discussions/amazon/view/121205-exam-aws-certified-solutions-architect-associate-saa-c03/","unix_timestamp":1695408960,"question_text":"A company is planning to migrate a TCP-based application into the company's VPC. The application is publicly accessible on a nonstandard TCP port through a hardware appliance in the company's data center. This public endpoint can process up to 3 million requests per second with low latency. The company requires the same level of performance for the new public endpoint in AWS.\n\nWhat should a solutions architect recommend to meet this requirement?","timestamp":"2023-09-22 20:56:00","choices":{"A":"Deploy a Network Load Balancer (NLB). Configure the NLB to be publicly accessible over the TCP port that the application requires.","B":"Deploy an Application Load Balancer (ALB). Configure the ALB to be publicly accessible over the TCP port that the application requires.","D":"Deploy an Amazon API Gateway API that is configured with the TCP port that the application requires. Configure AWS Lambda functions with provisioned concurrency to process the requests.","C":"Deploy an Amazon CloudFront distribution that listens on the TCP port that the application requires. Use an Application Load Balancer as the origin."},"question_id":578,"answers_community":["A (100%)"],"question_images":[],"answer_description":"","answer_images":[],"discussion":[{"upvote_count":"10","timestamp":"1711263780.0","content":"Selected Answer: A\nSince the company requires the same level of performance for the new public endpoint in AWS.\n\n\n\nA Network Load Balancer functions at the fourth layer of the Open Systems Interconnection (OSI) model. It can handle millions of requests per second. After the load balancer receives a connection request, it selects a target from the target group for the default rule. It attempts to open a TCP connection to the selected target on the port specified in the listener configuration.\n\nLink; \nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html","comment_id":"1015494","poster":"Sugarbear_01"},{"timestamp":"1716798360.0","comment_id":"1081400","content":"Selected Answer: A\nTCP = NLB","poster":"TariqKipkemei","upvote_count":"6"},{"content":"Selected Answer: A\nB: Is wrong as ALB is not going to help with TCP traffic\nC: CloudFront is CDN. There is no content here\nD: API Gateway is for HTTP web/API stuff, not custom TCP port applicationns","poster":"awsgeek75","timestamp":"1720733520.0","upvote_count":"3","comment_id":"1120288"},{"timestamp":"1711140960.0","content":"Selected Answer: A\nNLBs handle millions of requests per second. NLBs can handle general TCP traffic.","poster":"taustin2","comment_id":"1014416","upvote_count":"4"}]},{"id":"7UThSIqV7908o58GOpqo","timestamp":"2023-09-22 22:27:00","url":"https://www.examtopics.com/discussions/amazon/view/121210-exam-aws-certified-solutions-architect-associate-saa-c03/","answer":"B","choices":{"A":"Create a DB snapshot of the RDS for PostgreSQL DB instance to populate a new Aurora PostgreSQL DB cluster.","B":"Create an Aurora read replica of the RDS for PostgreSQL DB instance. Promote the Aurora read replicate to a new Aurora PostgreSQL DB cluster.","D":"Use the pg_dump utility to back up the RDS for PostgreSQL database. Restore the backup to a new Aurora PostgreSQL DB cluster.","C":"Use data import from Amazon S3 to migrate the database to an Aurora PostgreSQL DB cluster."},"topic":"1","unix_timestamp":1695414420,"answer_ET":"B","discussion":[{"comment_id":"1015247","poster":"Guru4Cloud","content":"Selected Answer: B\nThe key reasons are:\n\nAurora read replicas allow setting up replication from RDS PostgreSQL to Aurora PostgreSQL with minimal downtime.\nOnce replication is set up, the read replica can be promoted to a full standalone Aurora DB cluster with little to no downtime.\nThis approach leverages AWS's managed replication between the source RDS PostgreSQL instance and Aurora. It avoids having to manually create backups and restore data.\nUsing DB snapshots or pg_dump backups requires manually restoring data which increases downtime and operational overhead.\nData import from S3 would require exporting, uploading and then importing data which adds overhead.","timestamp":"1695495780.0","comments":[{"content":"can share the link? thanks","poster":"JA2018","timestamp":"1732381320.0","comment_id":"1316741","upvote_count":"1"}],"upvote_count":"7"},{"timestamp":"1695534180.0","upvote_count":"5","poster":"Sugarbear_01","comments":[{"comment_id":"1015509","poster":"Sugarbear_01","upvote_count":"2","timestamp":"1695534720.0","content":"Using ( 4 - using logical replication) RDS for PostgreSQL and Aurora PostgreSQL instance to migrate data off minimal downtime. But is not part of the option in the answer. Which makes answer B the best solution."}],"content":"Answer [B]\n\nThere are five options for migrating data from your existing Amazon RDS for PostgreSQL database to an Amazon Aurora PostgreSQL-Compatible DB cluster.\n1-Using a snapshot\n2-Using an Aurora read replica\n3-Using a pg_dump utility\n4-Using logical replication\n5-Using a data import from Amazon S3\n\n(2-Using an Aurora read replica) \n The Aurora read replica option minimizes downtime during a migration. Which is what the question demand so answer B; is the correct ; \nhttps://repost.aws/knowledge-center/aurora-postgresql-migrate-from-rds","comment_id":"1015503"},{"comment_id":"1126764","timestamp":"1705676880.0","poster":"Firdous586","upvote_count":"2","content":"B is correct as the question says least down time and data loss"},{"content":"Selected Answer: B\n\"Use an RDS for PostgreSQL DB instance as the basis for a new Aurora PostgreSQL DB cluster by using an Aurora read replica. The Aurora read replica is available for migrating only within the same AWS Region and account. The Aurora read replica option minimizes downtime during a migration. You can promote the new cluster when you have zero (0) replication lag between the primary RDS instance and the Aurora read replica.\"\nhttps://repost.aws/knowledge-center/aurora-postgresql-migrate-from-rds","upvote_count":"3","poster":"awsgeek75","comment_id":"1120593","timestamp":"1705049820.0"},{"upvote_count":"5","comment_id":"1111769","content":"Selected Answer: B\nNot A: Would work but have some (though minor) downtime\nB: \"The Aurora read replica option minimizes downtime during a migration\"\nNot C: \"If your data is stored using Amazon Simple Storage Service (Amazon S3)\" ... in this case it is not\nNot D: \"If ... you don't have downtime considerations, you can use this option\"\nhttps://repost.aws/knowledge-center/aurora-postgresql-migrate-from-rds","timestamp":"1704190080.0","poster":"pentium75"},{"content":"Selected Answer: B\nACD will have delta changes issue. Which means, RDS snapshot/export at 2pm, upload/import the table into Aurora, configure and populated completed by 6pm. This created a 4-hour gap of delta changes","upvote_count":"2","timestamp":"1703066940.0","poster":"Cyberkayu","comment_id":"1101425"},{"comments":[{"comment_id":"1111766","content":"I thought that too but B is correct: https://repost.aws/knowledge-center/aurora-postgresql-migrate-from-rds","poster":"pentium75","upvote_count":"2","timestamp":"1704189960.0"}],"timestamp":"1702478280.0","poster":"aws94","comment_id":"1095542","content":"Selected Answer: A\nplease focus, we have RDS not Aurora, I don't know how you vote to create an Aurora read replica to migrate an RDS to Aurora.","upvote_count":"1"},{"timestamp":"1701081000.0","poster":"TariqKipkemei","comment_id":"1081402","content":"Selected Answer: B\nLEAST operational overhead = read replica","upvote_count":"2"},{"upvote_count":"2","timestamp":"1699307220.0","comment_id":"1064283","poster":"potomac","content":"Selected Answer: B\nA,B,C are all valid option.\nBut B: The Aurora read replica option minimizes downtime during a migration."},{"upvote_count":"4","timestamp":"1697984760.0","poster":"thanhnv142","comment_id":"1050747","content":"B is correct guys. Lets see what we got here:\nC and D is not correct of course. We have to consider A and B. \nA: migration using a snapshot: this would, of course, introduce heavy data loss and down time\nB: migration using read replica: nearly no dataloss and downtime."},{"poster":"RRya","upvote_count":"1","comments":[],"timestamp":"1697493000.0","content":"Selected Answer: A\nRDS PostgreSQL to Aurora PostgreSQL:\n• Option 1: DB Snapshots from RDS PostgreSQL restored as PostgreSQL Aurora DB\n• Option 2: Create an Aurora Read Replica from your RDS PostgreSQL, and when the replication lag is 0, promote it as its own DB cluster (can take time and cost $)","comment_id":"1045299"},{"upvote_count":"1","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Migrating.html","comment_id":"1015747","timestamp":"1695558240.0","poster":"Jay2k23"},{"timestamp":"1695414420.0","comment_id":"1014488","poster":"taustin2","upvote_count":"2","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Migrating.html"}],"answer_description":"","answer_images":[],"isMC":true,"exam_id":31,"question_id":579,"question_text":"A company runs its critical database on an Amazon RDS for PostgreSQL DB instance. The company wants to migrate to Amazon Aurora PostgreSQL with minimal downtime and data loss.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","answers_community":["B (88%)","12%"],"question_images":[]},{"id":"YDFAkkQxN7bGbJmAoAia","isMC":true,"question_images":[],"answers_community":["C (100%)"],"question_text":"A company's infrastructure consists of hundreds of Amazon EC2 instances that use Amazon Elastic Block Store (Amazon EBS) storage. A solutions architect must ensure that every EC2 instance can be recovered after a disaster.\n\nWhat should the solutions architect do to meet this requirement with the LEAST amount of effort?","answer":"C","url":"https://www.examtopics.com/discussions/amazon/view/121212-exam-aws-certified-solutions-architect-associate-saa-c03/","timestamp":"2023-09-22 22:44:00","answer_ET":"C","choices":{"A":"Take a snapshot of the EBS storage that is attached to each EC2 instance. Create an AWS CloudFormation template to launch new EC2 instances from the EBS storage.","B":"Take a snapshot of the EBS storage that is attached to each EC2 instance. Use AWS Elastic Beanstalk to set the environment based on the EC2 template and attach the EBS storage.","C":"Use AWS Backup to set up a backup plan for the entire group of EC2 instances. Use the AWS Backup API or the AWS CLI to speed up the restore process for multiple EC2 instances.","D":"Create an AWS Lambda function to take a snapshot of the EBS storage that is attached to each EC2 instance and copy the Amazon Machine Images (AMIs). Create another Lambda function to perform the restores with the copied AMIs and attach the EBS storage."},"exam_id":31,"answer_description":"","topic":"1","unix_timestamp":1695415440,"answer_images":[],"discussion":[{"upvote_count":"12","timestamp":"1727117640.0","content":"Selected Answer: C\nThe key reasons are:\n\nAWS Backup automates backup of resources like EBS volumes. It allows defining backup policies for groups of resources. This removes the need to manually create backups for each resource.\nThe AWS Backup API and CLI allow programmatic control of backup plans and restores. This enables restoring hundreds of EC2 instances programmatically after a disaster instead of manually.\nAWS Backup handles cleanup of old backups based on policies to minimize storage costs.","comment_id":"1015237","poster":"Guru4Cloud"},{"comment_id":"1324442","upvote_count":"1","poster":"LeonSauveterre","content":"Selected Answer: C\nA - Manually managing templates for hundreds of EC2 instances is TOO MUCH WORK.\nB - Beanstalk is designed for managing application environments, not backups.\nC - AWS Backup API or CLI simplifies bulk restore operations. YES.\nD - Never create a lambda function when there's already to tool to do that and you need minimal effort. This is true for almost all the questions in this list.","timestamp":"1733819100.0"},{"content":"Selected Answer: C\nLEAST amount of effort = AWS Backup","comment_id":"1081405","poster":"TariqKipkemei","timestamp":"1732703520.0","upvote_count":"3"},{"content":"for the question, I would choose C as well, AWS Backup of the EC2, but design, why would anything of importance be on the Ec2 that would need to be restored? Shouldn't any critical or important data be on the EBS volumes in this example or similar location?","comments":[{"timestamp":"1733819340.0","comment_id":"1324446","poster":"LeonSauveterre","content":"There are actually scenarios where an EC2 instance itself might still need restoration, even when EBS is being used for storage.\n\n1. Some workloads may have specific configurations or dependencies baked into the EC2 instance itself, like custom softwares and OS-level configs.\n2. Some applications may be running on EC2 relies on temporary but critical data stored on the instance's root volume.\n3. Some stateful applications might store temporary state information locally on the EC2 instance. If this state isn’t externalized (like DynamoDB, Redis, or EFS), then recovery is needed.","upvote_count":"1"}],"upvote_count":"2","timestamp":"1732079820.0","poster":"Chiquitabandita","comment_id":"1075160"},{"poster":"taustin2","upvote_count":"3","content":"Selected Answer: C\nGoing with Backup. Can restore programmatically using Backup API.","comment_id":"1014495","timestamp":"1727037840.0"}],"question_id":580}],"exam":{"isMCOnly":true,"id":31,"lastUpdated":"11 Apr 2025","provider":"Amazon","name":"AWS Certified Solutions Architect - Associate SAA-C03","isImplemented":true,"isBeta":false,"numberOfQuestions":1019},"currentPage":116},"__N_SSP":true}