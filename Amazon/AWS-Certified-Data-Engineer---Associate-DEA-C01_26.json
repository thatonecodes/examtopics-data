{"pageProps":{"questions":[{"id":"0x7ffq2mLrDlC49omsho","question_text":"A data engineer needs to join data from multiple sources to perform a one-time analysis job. The data is stored in Amazon DynamoDB, Amazon RDS, Amazon Redshift, and Amazon S3.\nWhich solution will meet this requirement MOST cost-effectively?","exam_id":21,"question_images":[],"isMC":true,"question_id":126,"url":"https://www.examtopics.com/discussions/amazon/view/131712-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","topic":"1","answer_ET":"C","answer":"C","answers_community":["C (100%)"],"answer_description":"","unix_timestamp":1705801980,"choices":{"B":"Copy the data from DynamoDB, Amazon RDS, and Amazon Redshift into Amazon S3. Run Amazon Athena queries directly on the S3 files.","C":"Use Amazon Athena Federated Query to join the data from all data sources.","A":"Use an Amazon EMR provisioned cluster to read from all sources. Use Apache Spark to join the data and perform the analysis.","D":"Use Redshift Spectrum to query data from DynamoDB, Amazon RDS, and Amazon S3 directly from Redshift."},"discussion":[{"content":"Selected Answer: C\nI would go for C because Federated Query is typical for this porpouse. Besides, we don't need to add/duplicate resources in S3. But I see that, becasuse Athena is more optimized for S3, it can be considered a tricky question, since there can be more trade-offs to consider, such as data governance that are easier if data is centralized in S3 in my opinion.","timestamp":"1727739420.0","poster":"lucas_rfsb","upvote_count":"7","comment_id":"1187087"},{"upvote_count":"4","comment_id":"1227006","content":"Selected Answer: C\nServerless Processing: Athena is a serverless query service, meaning you only pay for the queries you run. This eliminates the need to provision and manage compute resources like in EMR clusters,\nmaking it ideal for one-time jobs.\nFederated Query Capability: Athena Federated Query allows you to directly query data from various sources like DynamoDB, RDS, Redshift, and S3 without physically moving the data. This eliminates data movement costs and simplifies the analysis process.\nReduced Cost for Large Datasets: Compared to copying data to S3, which can be expensive for large datasets, Athena Federated Query avoids unnecessary data movement, reducing overall costs.","timestamp":"1733715960.0","poster":"pypelyncar"},{"timestamp":"1726881960.0","content":"Amazon Athena Federated Query allows you to query data from multiple federated data sources including relational databases, NoSQL databases, and object stores directly from Athena. While this might seem like an efficient way to join data from different sources without the need for copying data into Amazon S3, it's essential to consider the cost implications.\n\nAWS documentation on Amazon Athena Federated Query [1] explains that while Federated Query enables you to query data from external data sources without data movement, it does not eliminate data transfer costs. Depending on the data sources involved (such as Amazon RDS, DynamoDB, etc.), there might be data transfer costs associated with querying data directly from these sources.\n\n[1] Amazon Athena Federated Query Documentation: https://docs.aws.amazon.com/athena/latest/ug/federated-data-sources.html","upvote_count":"2","poster":"certplan","comment_id":"1178946"},{"upvote_count":"1","comment_id":"1178945","timestamp":"1726881900.0","poster":"certplan","content":"1. Data Storage Costs: Storing data in Amazon S3 is generally cheaper compared to the other AWS storage options like Amazon Redshift or Amazon RDS.\n\n2. Compute Costs: Amazon: Athena is a serverless query service that allows you to query data directly from S3 without the need for provisioning or managing infrastructure. You only pay for the queries you run, which can be more cost-effective compared to provisioning an EMR cluster (option A) or using Redshift Spectrum (option D), both of which involve compute resources that you might not fully utilize.\n\n3. Data Transfer Costs: Option B involves copying the data once into S3, and then there are no additional data transfer costs for querying the data using Athena. In contrast, options A and D would involve data transfer costs as data is moved between different services.\n\nAmazon Athena Pricing: https://aws.amazon.com/athena/pricing/ \nAmazon S3 Pricing: https://aws.amazon.com/s3/pricing/"},{"comment_id":"1178944","timestamp":"1726881840.0","content":"Point:\n\"perform a one-time analysis job\"\n\nOption C (Amazon Athena Federated Query) might seem appealing, but it's generally more suited for querying data from external sources without copying the data into S3. However, since the data is already within AWS services, copying it to S3 and using Athena directly would likely be more cost-effective.","poster":"certplan","upvote_count":"1"},{"poster":"[Removed]","comments":[{"comment_id":"1171085","content":"Agree. C","upvote_count":"2","poster":"GiorgioGss","timestamp":"1726060260.0"}],"timestamp":"1721519580.0","content":"Selected Answer: C\nYou can query these sources by using Federated Queries, which is a native feature of Athena. The other options may increase costs and operational overhead, as they use more than one service to achieve the same result\n\nhttps://docs.aws.amazon.com/athena/latest/ug/connectors-available.html","upvote_count":"4","comment_id":"1127578"}],"answer_images":[],"timestamp":"2024-01-21 02:53:00"},{"id":"fZERnQM2VWOyXoFLR6Bl","answers_community":["BD (100%)"],"isMC":true,"answer_ET":"BD","answer_description":"","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/131713-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_id":127,"answer_images":[],"timestamp":"2024-01-21 03:03:00","discussion":[{"content":"Selected Answer: BD\nHDFS is not recommended for persistent storage because once a cluster is terminated, all HDFS data is lost. Also, long-running workloads can fill the disk space quickly. Thus, S3 is the best option since it's highly available, durable, and scalable.\n\nAWS Graviton-based instances cost up to 20% less than comparable x86-based Amazon\nEC2 instances: https://aws.amazon.com/ec2/graviton/","poster":"[Removed]","upvote_count":"9","comment_id":"1127581","timestamp":"1721520180.0","comments":[{"timestamp":"1723364220.0","content":"If you are using instance storage this is true, but you can use EBS instead of instance storage.\nEBS has better performance than s3 for HDFS. This is the keyword from question, so EBS > S3\n\nI would rather select AD.","poster":"BartoszGolebiowski24","upvote_count":"1","comment_id":"1147133"}]},{"timestamp":"1743168180.0","comment_id":"1411307","content":"Selected Answer: BD\nCost effective + high reliability > S3\nGravitation > Low cost","poster":"sam_pre","upvote_count":"1"},{"poster":"ttpro1995","upvote_count":"1","comment_id":"1330987","content":"Selected Answer: BD\nRule of thumb: pick the AWS in-house solution provided for that service.\nGraviton is aws processor, and also EMRFS on S3.","timestamp":"1735002720.0"},{"comment_id":"1227007","timestamp":"1733716260.0","poster":"pypelyncar","upvote_count":"3","content":"Selected Answer: BD\ns3 no question.\nGraviton=> Cost-Effectiveness: Graviton instances are ARM-based instances specifically designed for cloud workloads.\nThey offer significant cost savings compared to x86-based instances while delivering comparable or better performance for many Apache Spark workloads.\nPerformance: Graviton instances are optimized for Spark workloads and can deliver the same level of performance as x86-based instances in many cases. Additionally, EMR offers performance-optimized versions of Spark built for Graviton instances."},{"timestamp":"1728843300.0","comment_id":"1195088","poster":"okechi","comments":[{"upvote_count":"4","comment_id":"1198441","content":"E is incorrect, Spot instances does not provide high reliability as required by the company.","poster":"chris_spencer","timestamp":"1729326120.0"}],"upvote_count":"1","content":"My answer is BE"},{"upvote_count":"2","poster":"certplan","timestamp":"1726882980.0","comment_id":"1178953","content":"A. - AWS recommends using Amazon S3 as a persistent data store for Amazon EMR due to its scalability, durability, and cost-effectiveness. Storing data in HDFS would require managing and maintaining additional infrastructure, which may incur higher costs in terms of storage, management, and scalability compared to using Amazon S3. AWS documentation emphasizes the benefits of integrating Amazon EMR with Amazon S3 for cost optimization and efficiency.\n\nD. - While Graviton instances may offer cost savings in certain scenarios, they might not always be the most cost-effective option depending on the specific workload requirements and availability of compatible software. x86-based instances are more commonly supported by a broader range of software and frameworks, which could result in better performance and compatibility in some cases. Additionally, AWS documentation on instance types and pricing can provide insights into the cost-effectiveness of Graviton instances compared to x86-based instances."},{"comments":[{"timestamp":"1728024900.0","upvote_count":"1","poster":"nyaopoko","comment_id":"1189142","content":"yes BD is answer"}],"poster":"GiorgioGss","timestamp":"1726033980.0","comment_id":"1170872","content":"Selected Answer: BD\nB and D.","upvote_count":"3"}],"choices":{"E":"Use Spot Instances for all primary nodes.","B":"Use Amazon S3 as a persistent data store.","A":"Use Hadoop Distributed File System (HDFS) as a persistent data store.","D":"Use Graviton instances for core nodes and task nodes.","C":"Use x86-based instances for core nodes and task nodes."},"topic":"1","exam_id":21,"answer":"BD","question_text":"A company is planning to use a provisioned Amazon EMR cluster that runs Apache Spark jobs to perform big data analysis. The company requires high reliability. A big data team must follow best practices for running cost-optimized and long-running workloads on Amazon EMR. The team must find a solution that will maintain the company's current level of performance.\nWhich combination of resources will meet these requirements MOST cost-effectively? (Choose two.)","unix_timestamp":1705802580},{"id":"YzpsleuQTEBuBqIgJ5dR","answer_ET":"C","topic":"1","question_text":"A company wants to implement real-time analytics capabilities. The company wants to use Amazon Kinesis Data Streams and Amazon Redshift to ingest and process streaming data at the rate of several gigabytes per second. The company wants to derive near real-time insights by using existing business intelligence (BI) and analytics tools.\nWhich solution will meet these requirements with the LEAST operational overhead?","exam_id":21,"unix_timestamp":1707218280,"choices":{"C":"Create an external schema in Amazon Redshift to map the data from Kinesis Data Streams to an Amazon Redshift object. Create a materialized view to read data from the stream. Set the materialized view to auto refresh.","A":"Use Kinesis Data Streams to stage data in Amazon S3. Use the COPY command to load data from Amazon S3 directly into Amazon Redshift to make the data immediately available for real-time analysis.","D":"Connect Kinesis Data Streams to Amazon Kinesis Data Firehose. Use Kinesis Data Firehose to stage the data in Amazon S3. Use the COPY command to load the data from Amazon S3 to a table in Amazon Redshift.","B":"Access the data from Kinesis Data Streams by using SQL queries. Create materialized views directly on top of the stream. Refresh the materialized views regularly to query the most recent stream data."},"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/133048-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_images":[],"discussion":[{"poster":"blackgamer","upvote_count":"8","timestamp":"1711723800.0","comment_id":"1185507","content":"Selected Answer: C\nThe answer is C. It can provide near real-time insight analysis. Refer the article from AWS - https://aws.amazon.com/blogs/big-data/real-time-analytics-with-amazon-redshift-streaming-ingestion/"},{"comments":[{"timestamp":"1726334160.0","upvote_count":"6","comment_id":"1283725","poster":"markill123","content":"Redshift cannot create external schemas that map directly to Kinesis Data Streams. You would still need an intermediary step, such as Firehose or S3, to handle data ingestion. Additionally, maintaining auto-refreshing materialized views directly from a stream isn't feasible with Redshift."}],"timestamp":"1710555300.0","poster":"helpaws","upvote_count":"7","comment_id":"1174660","content":"Selected Answer: C\nKey word here is near real-time. If it's involve S3 and COPY, it's not gonna be near real-time"},{"content":"Selected Answer: C\nhttps://aws.amazon.com/blogs/big-data/real-time-analytics-with-amazon-redshift-streaming-ingestion/#:~:text=Before%20the%20launch,the%20data%20stream.","upvote_count":"1","comment_id":"1410789","poster":"melligeri","timestamp":"1743064380.0"},{"upvote_count":"2","content":"Selected Answer: D\n✅ Use Kinesis Data Firehose to load data into Redshift via S3 for the simplest and most scalable solution.\n✅ Firehose automatically batches, transforms, and loads data with no manual intervention required.\n✅ Achieves near real-time analytics with minimal operational effort.","poster":"Rpathak4","comment_id":"1402198","timestamp":"1742722260.0"},{"upvote_count":"1","comment_id":"1398891","timestamp":"1742049960.0","content":"Selected Answer: D\nCreating an external schema and using materialized views directly on top of Kinesis Data Streams is also not an ideal choice because this approach can add complexity and doesn't leverage fully managed solutions like Kinesis Data Firehose. The manual management of data refresh rates adds operational overhead.","poster":"MephiboshethGumani"},{"content":"Selected Answer: C\nRefer to the article from AWS - https://aws.amazon.com/blogs/big-data/real-time-analytics-with-amazon-redshift-streaming-ingestion/","comment_id":"1388256","poster":"Eltanany","upvote_count":"1","timestamp":"1741861260.0"},{"comment_id":"1364713","content":"Selected Answer: D\noption D provides a streamlined, efficient, and low-overhead approach to achieving real-time analytics with the specified technologies.","timestamp":"1741058700.0","upvote_count":"1","poster":"jesusmoh"},{"upvote_count":"2","timestamp":"1736687040.0","comment_id":"1339500","content":"Selected Answer: D\nA: Kinesis Data Streams to stage data in Amazon S3. not really easy, \nB: sql directly to Kinesis Data Streams : functionality not exist\nC : external schema from redshift to Kinesis Data Streams : functionality not exist\nD : near real-time = Kinesis Data Firehose","poster":"plutonash"},{"comment_id":"1337845","poster":"subbie","upvote_count":"1","timestamp":"1736316780.0","content":"Selected Answer: C\nhttps://aws.amazon.com/blogs/big-data/real-time-analytics-with-amazon-redshift-streaming-ingestion/"},{"poster":"subbie","content":"Selected Answer: B\nhttps://aws.amazon.com/blogs/big-data/real-time-analytics-with-amazon-redshift-streaming-ingestion/","comment_id":"1337844","timestamp":"1736316720.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1734879180.0","content":"Selected Answer: A\nA for me\nC - Redshift does not natively support direct mapping to Kinesis Data Streams. Some extra configs are needed.\nD - There will be a 60s latency when using Firehose, so it's \"Near\" real time not real time.","poster":"haby","comment_id":"1330421"},{"comment_id":"1329012","timestamp":"1734621120.0","poster":"HagarTheHorrible","upvote_count":"1","content":"Selected Answer: D\nRedshift does not natively support direct mapping to Kinesis Data Streams. Materialized views cannot directly query streaming data from Kinesis."},{"content":"Selected Answer: C\nSee https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion-getting-started.html","upvote_count":"1","poster":"altonh","timestamp":"1733304300.0","comment_id":"1321791"},{"content":"Selected Answer: D\nD could be the most standard way to handle this case. How to use C to implement it is questionable for me.","upvote_count":"2","comments":[{"upvote_count":"1","poster":"Asmunk","timestamp":"1731866820.0","comment_id":"1313684","content":"https://docs.aws.amazon.com/streams/latest/dev/using-other-services-redshift.html"}],"timestamp":"1730884680.0","poster":"Asen_Cat","comment_id":"1307752"},{"upvote_count":"1","content":"Selected Answer: C\nAmazon Redshift can automatically refresh materialized views with up-to-date data from its base tables when materialized views are created with or altered to have the autorefresh option. Amazon Redshift autorefreshes materialized views as soon as possible after base tables changes.\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/materialized-view-refresh.html","timestamp":"1730883480.0","poster":"heavenlypearl","comment_id":"1307740"},{"comment_id":"1306078","content":"Firehose is Near-Real time, you can set your buffer size and stream to either Redshift or S3 directly. Since Redshift is not in the option, use s3...","poster":"royalrum","upvote_count":"1","timestamp":"1730533500.0"},{"poster":"Shatheesh","content":"Selected Answer: D\nKinesis Data Streams , option D using Kinesis Data Firehose is a fully managed service that automatically handles the ingestion of data","comment_id":"1302529","upvote_count":"1","timestamp":"1729787160.0"},{"poster":"markill123","upvote_count":"4","timestamp":"1726334220.0","comment_id":"1283727","content":"Selected Answer: D\nHere’s why D is the best choice:\n\nKinesis Data Firehose is a fully managed service that automatically handles the ingestion of data from Kinesis Data Streams and stages it in S3, which significantly reduces operational overhead compared to managing custom data ingestion pipelines.\nS3 as a staging area: Using Amazon S3 as a staging location allows for flexible data management, high durability, and direct loading into Redshift without needing to manage complex buffering or data handling processes.\nCOPY command: The COPY command in Amazon Redshift is highly optimized for loading large datasets efficiently, making it a common and effective method to load bulk data from S3 into Redshift for near real-time analysis.\nFirehose to Redshift: Firehose can automatically buffer, batch, and transform data before loading it into Redshift, reducing manual intervention and ensuring data is readily available for real-time analytics."},{"content":"Selected Answer: D\nOption C has an issue: Redshift does not natively support direct querying or mapping of Kinesis Data Streams. D is the only correct option.","upvote_count":"2","timestamp":"1725675720.0","poster":"shammous","comment_id":"1279873"},{"poster":"V0811","comment_id":"1260946","timestamp":"1722837780.0","content":"Selected Answer: D\nOption D","upvote_count":"2"},{"upvote_count":"1","timestamp":"1720086960.0","comment_id":"1241955","poster":"bakarys","content":"Selected Answer: A\nOption A (using Kinesis Data Streams to stage data in Amazon S3 and loading it directly into Amazon Redshift) is the most straightforward and efficient approach. It minimizes operational overhead and ensures immediate availability of data for analysis.\nOptions B and C introduce additional complexity and may not provide the same level of efficiency"},{"poster":"d8945a1","upvote_count":"2","content":"Selected Answer: C\nMVs in Redshift with auto refresh is the best option for near real time.","timestamp":"1715053200.0","comment_id":"1207701"},{"content":"Selected Answer: C\nUsing materialized views with auto-refresh directly on a Redshift external schema of Kinesis Data Stream offers the most streamlined and efficient approach for near real-time insights using existing BI tools.","poster":"Christina666","timestamp":"1712958900.0","upvote_count":"3","comment_id":"1194553"},{"comment_id":"1181036","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion-getting-started.html\n\nC is correct. (KDS -> Redshift)\nD is wrong as it has more operational overhead (KDS -> KDF -> S3 -> Redshift)","timestamp":"1711213380.0","upvote_count":"5","poster":"fceb2c1"},{"upvote_count":"2","timestamp":"1711029480.0","comment_id":"1178959","poster":"certplan","content":"1. Amazon Kinesis Data Firehose: It's designed to reliably load streaming data into data lakes and data stores with minimal configuration and management overhead. It handles tasks like buffering, scaling, and delivering data to destinations like Amazon S3 and Amazon Redshift automatically.\n\n2. Amazon S3 as a staging area: Storing data in Amazon S3 provides a scalable and durable solution for data storage without needing to manage infrastructure. It also allows for easy integration with other AWS services and existing BI and analytics tools.\n\n3. Amazon Redshift: While Redshift requires some setup and management, loading data from Amazon S3 using the COPY command is a straightforward process. Once data is loaded into Redshift, existing BI and analytics tools can query the data directly, enabling near real-time insights.\n\n4. Minimal operational overhead: This solution minimizes operational overhead because much of the management tasks, such as scaling, buffering, and delivery of data, are handled by Amazon Kinesis Data Firehose. Additionally, using Amazon S3 as a staging area simplifies data storage and integration with other services."},{"upvote_count":"1","content":"By considering the characteristics and capabilities of each AWS service and approach, along with insights from AWS documentation, it becomes evident that option D offers the most streamlined and operationally efficient solution for the scenario described.\n\nThis idea/concept is also straight out of the Amazon Solutions Architect course material.","poster":"certplan","timestamp":"1710993420.0","comment_id":"1178960"},{"content":"Point: \"Which solution will meet these requirements with the LEAST operational overhead?\"\n\nC. - This approach involves creating an external schema in Amazon Redshift to map data from Kinesis Data Streams, which adds complexity compared to directly loading data from Amazon S3 using Amazon Kinesis Data Firehose.\n - While materialized views with auto-refresh can provide near real-time insights, managing them and ensuring proper synchronization with the streaming data source may require more operational effort.\n - AWS documentation for Amazon Redshift primarily focuses on traditional data loading methods and querying, with limited guidance on integrating with real-time data sources like Kinesis Data Streams.","timestamp":"1710993360.0","comment_id":"1178958","poster":"certplan","upvote_count":"1"},{"content":"Selected Answer: D\nI think D. It could be C but because of \"LEAST operational overhead\" I will go with D.","poster":"GiorgioGss","comment_id":"1170877","upvote_count":"3","timestamp":"1710144000.0"},{"content":"Both ChatGPT and I are thinking D is correct (100%)","upvote_count":"1","timestamp":"1707218280.0","comments":[{"content":"I think this is true. \nI could not find any sources that AWS Kinesis Data Stream can stream data directly into s3 without a middle step with AWS Kinesis Data Firehose. \n\nThe AWS Kinesis Data Firehose is near real-time service.\n\nAnyway, I think the answer is D because the other 3 options are not better at all.","comment_id":"1147142","timestamp":"1707647460.0","upvote_count":"1","comments":[{"poster":"BartoszGolebiowski24","comments":[{"content":"thank you, i was leaning towards D but this article helps","comment_id":"1246395","upvote_count":"1","timestamp":"1720745520.0","poster":"LR2023"}],"comment_id":"1147148","upvote_count":"5","timestamp":"1707648120.0","content":"However, after some investigation, I found out that Amazon Kinesis Data Streams provides a way to ingest stream data directly into an Amazon Redshift cluster.\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion.html\n\nMaterialized views with auto-refresh enabled will continuously ingest new data from the stream as it arrives, keeping the view updated with the latest data in real time.\n\nSo I think the correct answer is C. \n\n\n\nThe COPY command also supports loading data from streaming sources like Kinesis Data Streams or Kinesis Data Firehose. When used with these services, COPY provides a way to ingest real-time streaming data into Redshift tables. \nBut this solution is not an option for this question."}],"poster":"BartoszGolebiowski24"}],"poster":"Aesthet","comment_id":"1142075"}],"answer_description":"","isMC":true,"question_id":128,"answer":"C","answers_community":["C (56%)","D (39%)","4%"],"timestamp":"2024-02-06 12:18:00"},{"id":"VWEmgZx5KhS4j9NJkKmY","topic":"1","answer_description":"","unix_timestamp":1706951640,"isMC":true,"question_text":"A company uses an Amazon QuickSight dashboard to monitor usage of one of the company's applications. The company uses AWS Glue jobs to process data for the dashboard. The company stores the data in a single Amazon S3 bucket. The company adds new data every day.\nA data engineer discovers that dashboard queries are becoming slower over time. The data engineer determines that the root cause of the slowing queries is long-running AWS Glue jobs.\nWhich actions should the data engineer take to improve the performance of the AWS Glue jobs? (Choose two.)","answer_images":[],"answer":"AB","question_images":[],"answers_community":["AB (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/132734-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","timestamp":"2024-02-03 10:14:00","discussion":[{"timestamp":"1722669240.0","comments":[{"content":"How does partitioning data in S3 improve the performance of AWS Glue jobs? Partitioning data s3 improve the query performance, but the question was the action should the DE take to improve the performance of AWS Glue jobs !","upvote_count":"1","comment_id":"1331231","timestamp":"1735074000.0","poster":"MLOPS_eng"},{"upvote_count":"1","comment_id":"1193981","timestamp":"1728673020.0","poster":"Leo87656789","comments":[{"poster":"DevoteamAnalytix","timestamp":"1730641800.0","upvote_count":"2","content":"Here you can find 5 different Worker types:\nhttps://docs.aws.amazon.com/glue/latest/dg/add-job.html","comment_id":"1206085"},{"content":"It looks like there are various worker types in AWS Glue actually. I'll go with AB as well. \n\n\"With AWS Glue, you only pay for the time your ETL job takes to run. There are no resources to manage, no upfront costs, and you are not charged for startup or shutdown time. You are charged an hourly rate based on the number of Data Processing Units (or DPUs) used to run your ETL job. A single Data Processing Unit (DPU) is also referred to as a worker. AWS Glue comes with three worker types to help you select the configuration that meets your job latency and cost requirements. Workers come in Standard, G.1X, G.2X, and G.025X configurations.\"\n\nhttps://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html","upvote_count":"2","poster":"tgv","comment_id":"1219996","timestamp":"1732778100.0"}],"content":"I would also go for A, B.\nBut there are no worker types in AWS Glue. You can only increase the DPU."}],"poster":"rralucard_","content":"Selected Answer: AB\nA. Partition the data that is in the S3 bucket. Organize the data by year, month, and day.\n\n • Partitioning data in Amazon S3 can significantly improve query performance. By organizing the data by year, month, and day, AWS Glue and Amazon QuickSight can scan only the relevant partitions of data, which reduces the amount of data read and processed. This approach is particularly effective for time-series data, where queries often target specific time ranges.\n\nB. Increase the AWS Glue instance size by scaling up the worker type.\n\n • Scaling up the worker type can provide more computational resources to the AWS Glue jobs, enabling them to process data faster. This can be especially beneficial when dealing with large datasets or complex transformations. It’s important to monitor the performance improvements and cost implications of scaling up.","comment_id":"1139119","upvote_count":"10"},{"upvote_count":"2","poster":"certplan","content":"1. **Partition the Data in Amazon S3**: \n - AWS documentation on optimizing Amazon S3 performance: https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance.html\n - AWS Glue documentation on partitioning data for AWS Glue jobs: https://docs.aws.amazon.com/glue/latest/dg/how-it-works.html#how-partitioning-works\n - Best practices for partitioning in Amazon S3: https://docs.aws.amazon.com/AmazonS3/latest/userguide/best-practices-partitioning.html\n\n2. **Optimizing AWS Glue Job Settings**:\n - AWS Glue documentation on optimizing job performance: https://docs.aws.amazon.com/glue/latest/dg/best-practices.html\n - AWS Glue documentation on scaling AWS Glue job resources: https://docs.aws.amazon.com/glue/latest/dg/monitor-profile-glue-job-cloudwatch-metrics.html\n\nBy referring to these documentation resources, the data engineer can gain insights into best practices and recommendations provided by AWS for optimizing AWS Glue jobs, thereby justifying the suggested actions to address the issue of slowing job performance.","comment_id":"1178965","timestamp":"1726884360.0"}],"question_id":129,"answer_ET":"AB","exam_id":21,"choices":{"A":"Partition the data that is in the S3 bucket. Organize the data by year, month, and day.","C":"Convert the AWS Glue schema to the DynamicFrame schema class.","E":"Modify the IAM role that grants access to AWS glue to grant access to all S3 features.","D":"Adjust AWS Glue job scheduling frequency so the jobs run half as many times each day.","B":"Increase the AWS Glue instance size by scaling up the worker type."}},{"id":"PaiMHb1V6C6Hjp02dRBS","question_images":[],"answers_community":["C (100%)"],"unix_timestamp":1707050760,"answer_description":"","choices":{"C":"Map state","B":"Choice state","A":"Parallel state","D":"Wait state"},"isMC":true,"answer_ET":"C","exam_id":21,"question_id":130,"url":"https://www.examtopics.com/discussions/amazon/view/132773-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","topic":"1","timestamp":"2024-02-04 13:46:00","question_text":"A data engineer needs to use AWS Step Functions to design an orchestration workflow. The workflow must parallel process a large collection of data files and apply a specific transformation to each file.\nWhich Step Functions state should the data engineer use to meet these requirements?","answer":"C","discussion":[{"upvote_count":"1","content":"Selected Answer: C\nClearly is mapping state","poster":"GabrielSGoncalves","comment_id":"1274297","timestamp":"1724895600.0"},{"timestamp":"1717899060.0","content":"Selected Answer: C\nThe Map state allows you to define a single execution path for processing a collection of data items in parallel.\nThis aligns perfectly with the data engineer's requirement of parallel processing a large collection of data files","poster":"pypelyncar","comment_id":"1227011","upvote_count":"3"},{"content":"Selected Answer: C\nto execute in parallel","upvote_count":"1","timestamp":"1717549140.0","poster":"FunkyFresco","comment_id":"1224455"},{"timestamp":"1716424440.0","content":"Selected Answer: C\nC is Correct\nTo meet the requirement of parallel processing a large collection of data files and applying a specific transformation to each file, the data engineer should use the Map state in AWS Step Functions.\nThe Map state is specifically designed to run a set of tasks in parallel for each element in a collection or array. Each element (in this case, each data file) is processed independently and in parallel, allowing the workflow to take advantage of parallel processing.","comment_id":"1216115","poster":"sveni1502","upvote_count":"3"},{"poster":"lucas_rfsb","timestamp":"1711933740.0","upvote_count":"1","comment_id":"1187116","content":"Selected Answer: C\nC, Map state is correct"},{"poster":"Aesthet","comment_id":"1142096","upvote_count":"1","content":"With Step Functions, you can orchestrate large-scale parallel workloads to perform tasks, such as on-demand processing of semi-structured data. These parallel workloads let you concurrently process large-scale data sources stored in Amazon S3. For example, you might process a single JSON or CSV file that contains large amounts of data. Or you might process a large set of Amazon S3 objects.\n\nTo set up a large-scale parallel workload in your workflows, include a Map state in Distributed mode.","timestamp":"1707219360.0"},{"timestamp":"1707218520.0","comment_id":"1142077","poster":"Aesthet","content":"C is correct.\nMap state is designed precisely for the requirement described. It allows you to iterate over a collection of items, processing each item individually. The Map state can automatically manage the iteration and execute the specified transformation on each item in parallel, making it the perfect choice for parallel processing of a large collection of data files.","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: C\nThe Map state is specifically designed for processing a collection of items (like data files) in parallel. It allows you to apply a transformation or a set of steps to each item in the input array independently.\nThe Map state automatically iterates over each item in the array and performs the defined steps. This makes it ideal for scenarios where you need to process a large number of files in a similar manner, as in your requirement.","timestamp":"1707050760.0","poster":"rralucard_","comment_id":"1140093"}],"answer_images":[]}],"exam":{"isImplemented":true,"isMCOnly":true,"name":"AWS Certified Data Engineer - Associate DEA-C01","id":21,"isBeta":false,"lastUpdated":"11 Apr 2025","provider":"Amazon","numberOfQuestions":207},"currentPage":26},"__N_SSP":true}