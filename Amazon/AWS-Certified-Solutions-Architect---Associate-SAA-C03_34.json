{"pageProps":{"questions":[{"id":"uVyXJbyukWlHfqdpmDOA","isMC":true,"unix_timestamp":1665448380,"exam_id":31,"topic":"1","discussion":[{"upvote_count":"17","content":"Selected Answer: B\nThe storage solution that will meet these requirements most cost-effectively is B: Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Glacier Deep Archive after 1 month.\n\nAmazon S3 Glacier Deep Archive is a secure, durable, and extremely low-cost Amazon S3 storage class for long-term retention of data that is rarely accessed and for which retrieval times of several hours are acceptable. It is the lowest-cost storage option in Amazon S3, making it a cost-effective choice for storing backup files that are not accessed after 1 month.\n\nYou can use an S3 Lifecycle configuration to automatically transition objects from S3 Standard to S3 Glacier Deep Archive after 1 month. This will minimize the storage costs for the backup files that are not accessed frequently.","timestamp":"1672172820.0","poster":"Buruguduystunstugudunstuy","comments":[{"comments":[{"poster":"JA2018","upvote_count":"1","comment_id":"1308807","timestamp":"1731073920.0","content":"If the AZ selected for the One Zone-IA option is whacked offline, high chances of data loss."},{"upvote_count":"6","content":"Also S3 Standard-IA & One Zone-IA stores the data for max of 30 days and not indefinitely.","timestamp":"1673516880.0","comments":[{"poster":"MatAlves","comment_id":"1287998","timestamp":"1727070420.0","content":"you want the MOST cost-effectively solution. Glacier DA is the cheapest for archival.","upvote_count":"1"}],"poster":"vgchan","comment_id":"773324"}],"content":"Option A, configuring S3 Intelligent-Tiering to automatically migrate objects, is not a good choice because it is not designed for long-term storage and does not offer the cost benefits of S3 Glacier Deep Archive.\n\nOption C, transitioning objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) after 1 month, is not a good choice because it is not the lowest-cost storage option and would not provide the cost benefits of S3 Glacier Deep Archive.\n\nOption D, transitioning objects from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 1 month, is not a good choice because it is not the lowest-cost storage option and would not provide the cost benefits of S3 Glacier Deep Archive.","timestamp":"1672172820.0","upvote_count":"7","comment_id":"759038","poster":"Buruguduystunstugudunstuy"}],"comment_id":"759037"},{"timestamp":"1665793380.0","upvote_count":"7","comment_id":"695067","poster":"ninjawrz","content":"B: Transition to Glacier deep archive for cost efficiency"},{"timestamp":"1739823960.0","upvote_count":"1","poster":"Vandaman","comment_id":"1357969","content":"Selected Answer: B\nDefinitely B. Files are not accessed after 1 month and need to be kept indefinitely, so Glacier Deep Archive is the best solution."},{"upvote_count":"1","poster":"satyaammm","timestamp":"1735575180.0","content":"Selected Answer: B\nGlacier archive is most suitable here as the data doesn't needs to be accessed anymore but stored indefinitely.","comment_id":"1334243"},{"timestamp":"1731094200.0","content":"I think the answer is C. This is because, the files were accessed frequently for the first month, so transitioning to Glacier Deep Archive after 1 month would result in higher retrieval costs for the first month of frequent access, making it a less optimal solution. However, since the files are accessed frequently for the first month and then not accessed after that, transitioning them to S3 Standard-IA after 1 month is the cost-effective choice.","poster":"LayManCloud_2050","upvote_count":"1","comment_id":"1308892"},{"poster":"PaulGa","timestamp":"1723579080.0","comment_id":"1265332","upvote_count":"1","content":"Selected Answer: B\nAns B - Glacier: the files will not be accessed after 1 month; they just need to be retained"},{"comment_id":"1253066","upvote_count":"1","timestamp":"1721650440.0","content":"Selected Answer: B\nShould apply S3 lifecycle to move not accessed file after 1 month to S3 Glacier Deep Archive.","poster":"DavidNgTan"},{"comment_id":"1201722","content":"Selected Answer: C\nSince the files are not accessed after 1 month but need to be kept indefinitely, transitioning them to S3 Standard-Infrequent Access (S3 Standard-IA) would be the best choice.","timestamp":"1714014480.0","poster":"Solomon2001","upvote_count":"1"},{"content":"not accessed == Galcier -- easy one","comment_id":"1187777","upvote_count":"1","timestamp":"1712020500.0","poster":"JohnZh"},{"content":"Selected Answer: B\nA: Possible but expensive \nCD: One zone so no guarantee of being stored indefinitely.\nB: S3GDA is cost effective indefinite storage","comment_id":"1122038","poster":"awsgeek75","upvote_count":"1","timestamp":"1705178400.0"},{"poster":"A_jaa","comment_id":"1121634","content":"Selected Answer: B\nAnswer-B","upvote_count":"1","timestamp":"1705149420.0"},{"comment_id":"1099086","upvote_count":"2","content":"It's can't be B!!\nbecause objects that are archived to S3 Glacier Instant Retrieval and S3 Glacier Flexible Retrieval are charged for a minimum storage duration of 90 days, and S3 Glacier Deep Archive has a minimum storage duration of 180 days.","timestamp":"1702831260.0","comments":[{"poster":"RichWil","content":"In AWS S3 Standard, there is no minimum storage duration requirement before you can move an object to S3 Glacier Deep Archive. You can transition objects at any time using S3 Lifecycle Policies.\n\nHowever, once an object is stored in S3 Glacier Deep Archive, it has a minimum storage duration of 180 days. If the object is deleted or overwritten before 180 days, you will be charged for the full 180-day period.","timestamp":"1739669340.0","comment_id":"1357116","upvote_count":"1"},{"timestamp":"1703424660.0","comment_id":"1104619","comments":[{"upvote_count":"2","timestamp":"1704717660.0","poster":"RNess","comment_id":"1116607","content":"I mean, that min duration **before** can move to S3 Glacier Flexible Retrieval or S3 Glacier Deep Archive"}],"poster":"pentium75","upvote_count":"1","content":"But the items must be kept forever, so where's the issue with that?"}],"poster":"RNess"},{"comment_id":"1091406","content":"Selected Answer: B\nB because since the files should be kept but never accessed we can put them in Deep Archive","timestamp":"1702088820.0","upvote_count":"1","poster":"ddement0r"},{"comment_id":"1054451","content":"Amazon S3 Glacier Deep Archive is a secure, durable, and extremely low-cost Amazon S3 storage class for long-term retention of data that is rarely accessed and for which retrieval times of several hours are acceptable","timestamp":"1698315360.0","poster":"Ruffyit","upvote_count":"1"},{"comment_id":"1022190","upvote_count":"1","timestamp":"1696157940.0","content":"Selected Answer: B\nAnswer is B","poster":"AhmedAbdelhedi"},{"comment_id":"992733","content":"Selected Answer: B\nB as these files will be stored indefinitely after 1 month","poster":"sujanakakarla","timestamp":"1693276200.0","upvote_count":"1"},{"poster":"TariqKipkemei","timestamp":"1690949400.0","content":"Selected Answer: B\nFiles are accessed frequently for 1 month = S3 Standard. Files are not accessed after 1 month and must be kept indefinitely at low costs = S3 Glacier Deep Archive. \nNo requirement for low Ops but S3 Lifecycle to the rescue...whoooosh!","comment_id":"969648","upvote_count":"1"},{"poster":"Guru4Cloud","upvote_count":"1","timestamp":"1689946680.0","content":"Selected Answer: B\nOption B (Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Glacier Deep Archive after 1 month) is the most cost-effective storage solution for this specific scenario. It allows you to maintain accessibility for the initial 1 month while achieving significant cost savings in the long term.","comment_id":"958561"},{"content":"Option B is the right answer for this.","poster":"miki111","comment_id":"953562","timestamp":"1689532620.0","upvote_count":"1"},{"comment_id":"950561","timestamp":"1689244560.0","content":"Selected Answer: B\nCorrect answer is B","upvote_count":"1","poster":"Kaab_B"},{"poster":"Debmalya_aws","upvote_count":"1","comments":[{"comment_id":"949191","timestamp":"1689095700.0","poster":"bingusbongus","content":"You absolutely can.","upvote_count":"2"}],"timestamp":"1688980800.0","comment_id":"947942","content":"It will be C. Can not move to Glacier directly from standard using Lifecycle"},{"comment_id":"926576","content":"Selected Answer: B\nS3 Glacier Deep Archive is designed for long-term archival storage with very low storage costs. It offers the lowest storage prices among the storage classes in Amazon S3. However, it's important to note that accessing data from S3 Glacier Deep Archive has a significant retrieval time, ranging from several minutes to hours, which may not be suitable if you require immediate access to the backup files.\n\nIf the files need to be accessed frequently within the first month but not after that, transitioning them to S3 Glacier Deep Archive using an S3 Lifecycle configuration can provide cost savings. However, keep in mind that retrieving the files from S3 Glacier Deep Archive will have a significant time delay.","timestamp":"1687079100.0","upvote_count":"3","poster":"cookieMr"},{"comment_id":"903733","poster":"MostafaWardany","content":"Selected Answer: B\nB is the correct answer","timestamp":"1684733760.0","upvote_count":"1"},{"timestamp":"1684310220.0","content":"Selected Answer: B\nB is correct answer","comment_id":"899831","upvote_count":"1","poster":"beginnercloud"},{"content":"Transition to Glacier storage for cost efficient and can be queries in 5-7 hours time","timestamp":"1682966040.0","poster":"Rahulbit34","upvote_count":"1","comment_id":"886600"},{"upvote_count":"3","comment_id":"863643","poster":"PhucVuu","content":"Selected Answer: B\nKeywords:\n- The files are accessed frequently for 1 month.\n- Files are NOT accessed after 1 month.\n\nA: Incorrect - We know the pattern (accessed frequently for 1 month, NOT accessed after 1 month) so we can configure it manually to make the cost reduce as much as possible.\nB: Correct - Glacier Deep Archive is the most cost-effective for file which rarely use\nC: Incorrect - Standard-Infrequent Access good for in Infrequent Access but not good for rarely(never) use.\nD: Incorrect - One Zone-Infrequent Access can reduce more cost compare to Standard-Infrequent Access but it is not the best way compare to Glacier Deep Archive.","timestamp":"1680854520.0"},{"poster":"enc_0343","upvote_count":"1","content":"The answer is B. \"S3 Glacier Deep Archive is Amazon S3’s lowest-cost storage class and supports long-term retention and digital preservation for data that may be accessed once or twice in a year.\" See here: https://aws.amazon.com/s3/storage-classes/","timestamp":"1677700320.0","comment_id":"826228"},{"poster":"KittieHearts","upvote_count":"1","content":"Selected Answer: B\nFiles are only required to be kept up to 7 years for businesses to Deep archive is the most cost optimal as well as useful in this scenario.","timestamp":"1677296340.0","comment_id":"821156"},{"comment_id":"750320","content":"Selected Answer: B\nGlacier deep archive = lowest cost (accessed once or twice a year)","poster":"pazabal","upvote_count":"2","timestamp":"1671492360.0"},{"timestamp":"1671442260.0","upvote_count":"1","poster":"Myxa","content":"Selected Answer: B\nCorrect answer: B","comment_id":"749673"},{"poster":"NikaCZ","comment_id":"747498","upvote_count":"1","content":"Selected Answer: B\nTransition to Glacier is cost effective.","timestamp":"1671215580.0"},{"poster":"Hacar","timestamp":"1670922660.0","upvote_count":"1","comment_id":"743798","content":"Selected Answer: B\nB is the answer."},{"timestamp":"1670873640.0","poster":"Certified101","content":"Selected Answer: D\nAmazon S3 Glacier Deep Archive – for long term storage: Minimum storage duration of 180 days","upvote_count":"1","comment_id":"743263"},{"comment_id":"736576","poster":"AlaN652","timestamp":"1670308980.0","upvote_count":"1","content":"Selected Answer: B\nSince deep archive is the cheapest storage option"},{"comment_id":"735266","content":"Selected Answer: B\nDeep archive is cheaper","timestamp":"1670173260.0","upvote_count":"2","poster":"Gil80"},{"poster":"ENNYBOLA","timestamp":"1669867020.0","comment_id":"732223","comments":[{"upvote_count":"1","content":"Nah pretty sure its minimum storage time 180 days. Meaning you can't remove it from glacier storage for half a year, but you can put it into glacier whenever you want.","timestamp":"1676159040.0","poster":"lofzee","comment_id":"805756"}],"content":"i thought it can only go to deep archive after 90 days?","upvote_count":"2"},{"poster":"Wpcorgan","comment_id":"723496","timestamp":"1669035240.0","upvote_count":"1","content":"B is correct"},{"content":"BBBBBBBBB","upvote_count":"1","comment_id":"723196","timestamp":"1669010280.0","poster":"Tsho"},{"poster":"renekton","content":"Selected Answer: B\nB is the correct answer","timestamp":"1668748020.0","upvote_count":"1","comment_id":"721070"},{"comment_id":"713932","upvote_count":"2","poster":"xeun88","timestamp":"1667922060.0","content":"B is correct"},{"poster":"GameDad09","content":"Selected Answer: B\nB is the correct one.","upvote_count":"2","comment_id":"697673","timestamp":"1666037460.0"},{"content":"BBBBBBBBBB","comment_id":"696468","poster":"queen101","upvote_count":"2","timestamp":"1665948720.0"},{"content":"Selected Answer: B\nsure for B","upvote_count":"2","poster":"BoboChow","timestamp":"1665550860.0","comment_id":"692650"},{"timestamp":"1665448380.0","poster":"Ralston40","comment_id":"691619","upvote_count":"4","content":"The answer is B"}],"question_images":[],"choices":{"A":"Configure S3 Intelligent-Tiering to automatically migrate objects.","B":"Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Glacier Deep Archive after 1 month.","C":"Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) after 1 month.","D":"Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 1 month."},"answers_community":["B (96%)","2%"],"answer_description":"","answer_ET":"B","question_text":"A company is storing backup files by using Amazon S3 Standard storage. The files are accessed frequently for 1 month. However, the files are not accessed after 1 month. The company must keep the files indefinitely.\nWhich storage solution will meet these requirements MOST cost-effectively?","answer_images":[],"timestamp":"2022-10-11 02:33:00","question_id":166,"answer":"B","url":"https://www.examtopics.com/discussions/amazon/view/85092-exam-aws-certified-solutions-architect-associate-saa-c03/"},{"id":"JhluEZcEXCKr227UTB4g","timestamp":"2023-01-14 19:57:00","unix_timestamp":1673722620,"choices":{"A":"Remove the two NAT instances and replace them with two NAT gateways in the same Availability Zone.","B":"Use Auto Scaling groups with Network Load Balancers for the NAT instances in different Availability Zones.","D":"Replace the two NAT instances with Spot Instances in different Availability Zones and deploy a Network Load Balancer.","C":"Remove the two NAT instances and replace them with two NAT gateways in different Availability Zones."},"question_id":167,"answer_description":"","answer_images":[],"question_text":"A company is concerned that two NAT instances in use will no longer be able to support the traffic needed for the company’s application. A solutions architect wants to implement a solution that is highly available, fault tolerant, and automatically scalable.\n\nWhat should the solutions architect recommend?","url":"https://www.examtopics.com/discussions/amazon/view/95322-exam-aws-certified-solutions-architect-associate-saa-c03/","answer":"C","question_images":[],"exam_id":31,"answer_ET":"C","answers_community":["C (97%)","3%"],"isMC":true,"topic":"1","discussion":[{"content":"Selected Answer: C\nfyi yall in most cases nat instances are a bad thing because their customer managed while nat gateways are AWS Managed. So in this case I already know to get rid of the nat instances the reason its c is because it wants high availability meaning different AZs","timestamp":"1692576060.0","upvote_count":"7","comment_id":"816103","poster":"Bhrino"},{"comment_id":"1107435","content":"Selected Answer: C\nSee https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html","upvote_count":"7","poster":"pentium75","timestamp":"1719544620.0"},{"comment_id":"1358272","upvote_count":"1","content":"Selected Answer: C\nNAT Gateways in 2 different AZ's is the most suitable option here.","timestamp":"1739880420.0","poster":"satyaammm"},{"comment_id":"1084912","poster":"MiniYang","timestamp":"1717205580.0","comments":[],"upvote_count":"1","content":"Selected Answer: B\nHighly available, fault tolerant and automatically scalable=> Autoscaling and Diffrent AZ"},{"poster":"TariqKipkemei","timestamp":"1711094040.0","content":"Selected Answer: C\nHighly available, fault tolerant, and automatically scalable = two NAT gateways in different Availability Zones","upvote_count":"3","comment_id":"1013644"},{"content":"Selected Answer: C\nRemove the two NAT instances and replace them with two NAT gateways in different Availability Zones","timestamp":"1706458620.0","upvote_count":"2","comment_id":"965616","poster":"Undisputed"},{"timestamp":"1703771340.0","content":"Selected Answer: C\nThis recommendation ensures high availability and fault tolerance by distributing the NAT gateways across multiple AZs. NAT gateways are managed AWS services that provide scalable and highly available outbound NAT functionality. By deploying NAT gateways in differentAZs, the company can achieve redundancy and avoid a single point of failure. This solution also provides automatic scaling to handle increasing traffic without manual intervention.\n\nOption A is incorrect because placing both NAT gateways in the same Availability Zone does not provide fault tolerance.\n\nOption B is incorrect because using Auto Scaling groups with Network Load Balancers is not the recommended approach for NAT instances.\n\nOption D is incorrect because Spot Instances are not suitable for critical infrastructure components like NAT instances.","upvote_count":"5","comment_id":"936604","poster":"cookieMr"},{"comment_id":"915180","upvote_count":"2","content":"Selected Answer: C\nHA: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html\nScalability: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html","timestamp":"1701769200.0","poster":"Axeashes"},{"poster":"Theodorz","timestamp":"1691884080.0","content":"Could anybody teach me why the B cannot be correct answer? This solution also seems providing Scalability(Auto Scaling Group), High Availability(different AZ), and Fault Tolerance(NLB & AZ). \n\nI honestly think that C is not enough, because each NAT gateway can provide a few scalability, but the bandwidth limit is clearly explained in the document. The C exactly mentioned \"two NAT gateways\" so the number of NAT is fixed, which will reach its limit soon.","comment_id":"806986","upvote_count":"3","comments":[{"content":"Option B proposes to use an Auto Scaling group with Network Load Balancers to continue using the existing two NAT instances. However, NAT instances do not support automatic failover without a script, unlike NAT gateways which provide this functionality. Additionally, using Network Load Balancers to balance traffic between NAT instances adds more complexity to the solution.\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html","comments":[{"content":"Thx for your explanation!","comment_id":"1105273","timestamp":"1719311940.0","upvote_count":"1","poster":"mwwt2022"}],"comment_id":"812777","poster":"KZM","upvote_count":"3","timestamp":"1692339300.0"}]},{"content":"C. If you have resources in multiple Availability Zones and they share one NAT gateway, and if the NAT gateway’s Availability Zone is down, resources in the other Availability Zones lose internet access. To create an Availability Zone-independent architecture, create a NAT gateway in each Availability Zone and configure your routing to ensure that resources use the NAT gateway in the same Availability Zone. https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html#nat-gateway-basics","timestamp":"1690170840.0","comment_id":"786182","upvote_count":"3","poster":"JayBee65"},{"comment_id":"778471","upvote_count":"2","timestamp":"1689553020.0","content":"Selected Answer: C\nReplace NAT Instances with Gateway","poster":"techhb"},{"comment_id":"775847","timestamp":"1689353820.0","upvote_count":"3","poster":"mhmt4438","content":"Selected Answer: C\nCorrect answer is C"}]},{"id":"SR8Usd8dm9qwm5P3Wk0B","question_text":"An application runs on an Amazon EC2 instance that has an Elastic IP address in VPC A. The application requires access to a database in VPC B. Both VPCs are in the same AWS account.\n\nWhich solution will provide the required access MOST securely?","exam_id":31,"question_images":[],"answers_community":["B (91%)","9%"],"discussion":[{"upvote_count":"14","content":"A is correct. B will work but is not the most secure method, since it will allow everything in VPC A to talk to everything in VPC B and vice versa, not at all secure. A on the other hand will only allow the application (since you select it's IP address) to talk to the application server in VPC A - you are allowing only the required connectivity. See the link for this exact use case: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html","timestamp":"1674540060.0","comments":[{"comments":[{"content":":)))))))))","comment_id":"846704","upvote_count":"1","poster":"test_devops_aws","timestamp":"1679464140.0"},{"comment_id":"864617","content":"he must be the security engineer lolol :D\n\n\"Jaybee\" - Please dont ever say that traffic over the public internet is secure :D","comments":[{"content":"https://aws.amazon.com/vpc/faqs/","upvote_count":"1","comment_id":"1317305","timestamp":"1732505520.0","poster":"swakan"}],"timestamp":"1680951240.0","upvote_count":"5","poster":"datz"},{"content":"Both VPCs are in the \"SAME AWS ACCOUNT\" and the requirement specifies allowing traffic from the *PUBLIC IP of the APPLICATION SERVER*. In this case the traffic remains inside the AWS infrastructure or will it go through the public internet?","upvote_count":"2","comments":[{"timestamp":"1703740800.0","upvote_count":"2","comment_id":"1107437","poster":"pentium75","content":"Answer A (not \"the requirement\") specifies \"allowing traffic from the public IP\", which is for sure NOT the \"most secure\" option."}],"poster":"graveend","timestamp":"1691781360.0","comment_id":"978906"}],"poster":"mhmt4438","upvote_count":"17","timestamp":"1675006980.0","content":"\" allows all traffic from the public IP address\" Nice bro niceee This is absolutely the most secure method at all. :)))","comment_id":"791784"}],"comment_id":"786187","poster":"JayBee65"},{"comment_id":"1077999","poster":"DUBURA","timestamp":"1700706540.0","content":"Selected Answer: B\nB. Configure a VPC peering connection between VPC A and VPC B.\n\nThe most secure solution is to configure a VPC peering connection between the two VPCs. This allows private communication between the application server and the database, without exposing resources to the public internet. \n\nOption A exposes the database to the public internet by allowing inbound traffic from a public IP address.\n\nOption C makes the database instance itself public, which is insecure.\n\nOption D adds complexity with a proxy that is not needed when a VPC peering connection can enable private communication between VPCs.\n\nSo option B is the most secure while allowing the necessary connectivity between the application server and the database in the separate VPCs.","upvote_count":"12"},{"poster":"FlyingHawk","timestamp":"1737433920.0","content":"Selected Answer: B\nIt should be B, plus the DB security g A. group that allows all traffic from the security group of the application server in VPC A","comment_id":"1344004","upvote_count":"1"},{"timestamp":"1716412200.0","upvote_count":"2","comment_id":"1216037","content":"Well this is a tricky one!!! Are we going to assume that the database in VPC B is in private subnet? In that case configuring security group to allow the traffic coming from Elastic IP of VPC A will not work. And if we use peering, the resources that live in the same subnet as the EC2 instance in VPC A will have access to the database? So what would we say to this? Is moving traffic through the public AWS space is safer than allowing access to the DB to other resources in VPC A?..... I don't know what to think","poster":"Jazz888"},{"timestamp":"1700641440.0","comment_id":"1077123","poster":"Ruffyit","content":"When you establish peering relationships between VPCs across different AWS Regions, resources in the VPCs (for example, EC2 instances and Lambda functions) in different AWS Regions can communicate with each other using private IP addresses, without using a gateway, VPN connection, or network appliance. The traffic remains in the private IP space. All inter-Region traffic is encrypted with no single point of failure, or bandwidth bottleneck. Traffic always stays on the global AWS backbone, and never traverses the public internet, which reduces threats, such as common exploits, and DDoS attacks. Inter-Region VPC peering provides a simple and cost-effective way to share resources between regions or replicate data for geographic redundancy.","upvote_count":"2"},{"upvote_count":"3","poster":"rlamberti","timestamp":"1698065400.0","comment_id":"1051838","content":"Selected Answer: B\nMost secure = not leaving AWS network.\nVPC peering is the way."},{"upvote_count":"2","comment_id":"1013648","content":"Selected Answer: B\nVPC to VPC comms = VPC peering","poster":"TariqKipkemei","timestamp":"1695362220.0"},{"timestamp":"1693814400.0","poster":"Sutariya","comment_id":"998342","content":"B is correct : Setup VPC peering and connect Application from VPC A to connect with VPC B in private subnet so DB instace always secure with internet.","upvote_count":"2"},{"poster":"_d1rk_","comment_id":"985764","timestamp":"1692533880.0","content":"Am I missing something or simply A is wrong because, without VPC peering (or other inter-connection sharing mechanisms such as Transit Gateway or VPN), VPC A and VPC B cannot communicate each other?","upvote_count":"1","comments":[{"upvote_count":"1","content":"can use vpc endpoints but no option use that","timestamp":"1693793520.0","comment_id":"998100","poster":"jacob_ho"}]},{"comment_id":"972803","poster":"A1975","content":"Selected Answer: B\nWhen you establish peering relationships between VPCs across different AWS Regions, resources in the VPCs (for example, EC2 instances and Lambda functions) in different AWS Regions can communicate with each other using private IP addresses, without using a gateway, VPN connection, or network appliance. The traffic remains in the private IP space. All inter-Region traffic is encrypted with no single point of failure, or bandwidth bottleneck. Traffic always stays on the global AWS backbone, and never traverses the public internet, which reduces threats, such as common exploits, and DDoS attacks. Inter-Region VPC peering provides a simple and cost-effective way to share resources between regions or replicate data for geographic redundancy.\n\nhttps://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html","timestamp":"1691223540.0","upvote_count":"4"},{"timestamp":"1688391120.0","content":"Selected Answer: B\nWith peering, we EC2 can communicate with RDS. RDS SG can have inbound from EC2 IP rather than VPC CIDR for more security","poster":"animefan1","comment_id":"941882","upvote_count":"2"},{"upvote_count":"2","poster":"maggie135","content":"Selected Answer: B\nVPC peering uses AWS network.","timestamp":"1688195100.0","comment_id":"939686"},{"upvote_count":"5","comment_id":"936607","content":"Selected Answer: B\nBy configuring a VPC peering connection between VPC A and VPC B, you can establish private and secure communication between the EC2 instance in VPC A and the database in VPC B. VPC peering allows traffic to flow between the two VPCs using private IP addresses, without the need for public IP addresses or exposing the database to the internet.\n\nOption A is not the best solution as it requires allowing all traffic from the public IP address of the application server, which can be less secure.\n\nOption C involves making the DB instance publicly accessible, which introduces security risks by exposing the database directly to the internet.\n\nOption D adds unnecessary complexity by launching an additional EC2 instance in VPC B and proxying all requests through it, which is not the most efficient and secure approach in this scenario.","timestamp":"1687953060.0","poster":"cookieMr"},{"comment_id":"923723","upvote_count":"1","timestamp":"1686805680.0","poster":"joechen2023","content":"Selected Answer: B\nI don't like A because the security group setting is wrong as it set up to allow all public IP addresses. If the security group setting is correct, then I will go for A\nI don't like B because it need to set up security group as well on top of peering.\nfor exam purpose only, I will go with the least worst choice which is B"},{"timestamp":"1686634680.0","comment_id":"921953","upvote_count":"1","content":"Selected Answer: A\nThe keywords are: \"access MOST securely\", hence the option A meets these requirements.","poster":"Bmarodi"},{"comment_id":"921286","content":"Selected Answer: A\nEach VPC security group rule makes it possible for a specific source to access a DB instance in a VPC that is associated with that VPC security group. The source can be a range of addresses (for example, 203.0.113.0/24), or another VPC security group. \n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide","poster":"smartegnine","timestamp":"1686561780.0","upvote_count":"1"},{"content":"Selected Answer: B\nMost secure = VPC peering","poster":"MostafaWardany","comment_id":"916451","timestamp":"1686068220.0","upvote_count":"2"},{"content":"Selected Answer: B\nI vote for option B.","comment_id":"910299","upvote_count":"2","timestamp":"1685454960.0","poster":"Bmarodi"},{"timestamp":"1684931280.0","upvote_count":"2","comment_id":"905880","content":"Selected Answer: B\nBBBB. A is not secure","poster":"Piccalo"},{"poster":"channn","comment_id":"854059","content":"Selected Answer: A\npeering is not secure to B as no more control on access from A to B","timestamp":"1680069900.0","upvote_count":"1"},{"upvote_count":"4","timestamp":"1675509540.0","comment_id":"797866","poster":"JohnnyBG","content":"Selected Answer: B\nB But what a crappy question/answers ..."},{"content":"Answer is B,\nA is not the answer <--it is not SECURE to have your traffic flow out from the internet to database.","timestamp":"1675469700.0","upvote_count":"5","poster":"kerl","comment_id":"797500"},{"upvote_count":"2","poster":"PoomJanT","timestamp":"1675254780.0","comment_id":"795099","content":"Selected Answer: B\nShould B)"},{"poster":"raf123123","timestamp":"1674903540.0","comment_id":"790537","upvote_count":"3","content":"Selected Answer: B\nAnswer: B"},{"timestamp":"1674896040.0","poster":"focus_23","comment_id":"790422","upvote_count":"3","content":"Selected Answer: B\nA) not possible, DB instance not have a public ip."},{"comment_id":"789272","timestamp":"1674786000.0","content":"Selected Answer: A\nAgreeing with JayBee65. See link for exact solution:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_SettingUp.html#CHAP_SettingUp.SecurityGroup\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_SettingUp.html#CHAP_SettingUp.SecurityGroup","poster":"Training4aBetterLife","upvote_count":"2"},{"poster":"AHUI","content":"Ans: B \nhttps://aws.amazon.com/premiumsupport/knowledge-center/rds-connectivity-instance-subnet-vpc/\nMy DB instance can't be accessed by an Amazon EC2 instance from a different VPC\nCreate a VPC peering connection between the VPCs. A VPC peering connection allows two VPCs to communicate with each other using private IP addresses.\n\n1. Create and accept a VPC peering connection.\n\nImportant: If the VPCs are in the same AWS account, be sure that the IPv4 CIDR blocks don't overlap. For more information, see VPC peering limitations.\n\n2. Update both route tables.\n\n3. Update your security groups to reference peer VPC groups.\n\n4. Activate DNS resolution support for your VPC peering connection.\n\n5. On the Amazon Elastic Compute Cloud (Amazon EC2) instance, test the VPC peering connection by using a networking utility. See the following example:","timestamp":"1673723760.0","upvote_count":"2","comment_id":"775858"},{"poster":"mhmt4438","upvote_count":"3","comments":[{"timestamp":"1674540120.0","content":"This is absolutely NOT the most secure method at all.","comment_id":"786190","poster":"JayBee65","upvote_count":"1","comments":[{"upvote_count":"2","timestamp":"1705746840.0","poster":"LoXoL","content":"Why? \n\nWith VPC Peering you can privately connect two VPCs using AWS’ network.","comment_id":"1127200"}]}],"timestamp":"1673722860.0","comment_id":"775850","content":"Selected Answer: B\nB. Configure a VPC peering connection between VPC A and VPC B.\n\nThe most secure solution to provide access to the database in VPC B from the application running on an EC2 instance in VPC A is to configure a VPC peering connection between the two VPCs. This will allow the application to access the database using the private IP addresses, and will not require any public IP addresses or Internet access. The traffic will be confined to the VPCs, and can be further secured with security group rules."}],"topic":"1","answer_description":"","choices":{"C":"Make the DB instance publicly accessible. Assign a public IP address to the DB instance.","A":"Create a DB instance security group that allows all traffic from the public IP address of the application server in VPC A.","D":"Launch an EC2 instance with an Elastic IP address into VPC B. Proxy all requests through the new EC2 instance.","B":"Configure a VPC peering connection between VPC A and VPC B."},"url":"https://www.examtopics.com/discussions/amazon/view/95323-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_images":[],"timestamp":"2023-01-14 20:01:00","question_id":168,"answer_ET":"B","answer":"B","unix_timestamp":1673722860,"isMC":true},{"id":"nYUTchVhniSi2vYf5C3Y","answer_description":"","discussion":[{"upvote_count":"16","comment_id":"936614","content":"Selected Answer: C\nBy publishing VPC flow logs to CloudWatch Logs and creating metric filters to detect RDP or SSH access, the operations team can configure an CloudWatch metric alarm to notify them when the alarm is triggered. This will provide the desired notification when RDP or SSH access to an environment is established.\n\nOption A is incorrect because CloudWatch Application Insights is not designed for detecting RDP or SSH access.\n\nOption B is also incorrect because configuring an IAM instance profile with the AmazonSSMManagedInstanceCore policy does not directly address the requirement of notifying the operations team when RDP or SSH access occurs.\n\nOption D is wrong beacuse configuring an EventBridge rule to listen for EC2 Instance State-change Notification events and using an SNS topic as a target will notify the operations team about changes in the instance state, such as starting or stopping instances. However, it does not specifically detect or notify when RDP or SSH access is established, which is the requirement stated in the question.","poster":"cookieMr","timestamp":"1703772000.0"},{"upvote_count":"9","timestamp":"1689680400.0","comments":[{"content":"https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs-records-examples.html#flow-log-example-accepted-rejected \n\nAdding this to support that VPC flow logs can be used to cvapture Accepted or Rejected SSH and RDP traffic.","comment_id":"792699","poster":"NitiATOS","comments":[{"content":"I don't think C would be an acceptable solution ... the request is to be notified WHEN a SSH and/or RDP connection is established so it requires real-time monitoring and that is something the C solution does not provide ... I would select A as a correct answer","upvote_count":"1","comment_id":"909598","timestamp":"1701287040.0","poster":"ruqui"}],"timestamp":"1690710660.0","upvote_count":"4"}],"content":"Selected Answer: C\nhttps://aws.amazon.com/blogs/security/how-to-monitor-and-visualize-failed-ssh-access-attempts-to-amazon-ec2-linux-instances/","poster":"Vickysss","comment_id":"780048"},{"timestamp":"1733395140.0","poster":"0xE8D4A51000","content":"Selected Answer: D\nSee https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-instance-state-changes.html","comment_id":"1224609","upvote_count":"1"},{"poster":"pentium75","timestamp":"1720337400.0","content":"Selected Answer: C\nC sounds complex, but is the only answer that can work.\nNot A - Application Insights has nothing to do with SSH/RDP access to the OS; also we need a notification, not an OpsItem\nNot B - Just attaching a role does not create a notification\nNot D - Establishing SSH/RDP access is not a \"state change\" that would trigger this","upvote_count":"3","comment_id":"1115677"},{"comments":[{"timestamp":"1719546240.0","upvote_count":"3","content":"C: Could work but it seems overkill to capture VPC flow logs just to detect SSH and RDP traffic. Also it is not real-time, and it's unclear how and when exactly the state transitions and notifications will be triggered. At best you'd get notification few minutes AFTER (not \"when\") \"access has been established\". Still, is has most similarity with the recommended approach to detect failed connections: https://aws.amazon.com/tr/blogs/security/how-to-monitor-and-visualize-failed-ssh-access-attempts-to-amazon-ec2-linux-instances/\n\nD: Won't work because establishment of a connection is not an instance state change.","comment_id":"1107456","poster":"pentium75"}],"poster":"pentium75","timestamp":"1719546240.0","content":"Selected Answer: C\nA bit clueless here. AWS-recommended approach involves the CloudWatch Logs Agent on each EC2 instance, but that is not involved in any of the answers.\n\nA: Sounds good at first read, but \"CloudWatch Application Insights\" cannot detect RDP or SSH access. \n\nB: Would allow RDP or SSH access via Systems Manager, but would NOT prevent access without Systems Manager; also we'd need to configure notifications in Systems Manager which is not mentioned here.","upvote_count":"3","comment_id":"1107455"},{"comment_id":"1077145","upvote_count":"2","content":"https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs-records-examples.html#flow-log-example-accepted-rejected\n\nAdding this to support that VPC flow logs can be used to cvapture Accepted or Rejected SSH and RDP traffic.","poster":"Ruffyit","timestamp":"1716360900.0"},{"comment_id":"1013651","timestamp":"1711094520.0","poster":"TariqKipkemei","content":"Selected Answer: C\nPublish VPC flow logs to Amazon CloudWatch Logs. Create required metric filters. Create an Amazon CloudWatch metric alarm with a notification action for when the alarm is in the ALARM state","upvote_count":"2"},{"poster":"Bmarodi","upvote_count":"4","timestamp":"1702458000.0","content":"Selected Answer: C\nVPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data can be published to the following locations: Amazon CloudWatch Logs, Amazon S3, or Amazon Kinesis Data Firehose. After you create a flow log, you can retrieve and view the flow log records in the log group, bucket, or delivery stream that you configured.\n\nFlow logs can help you with a number of tasks, such as:\n\n Diagnosing overly restrictive security group rules\n\n Monitoring the traffic that is reaching your instance\n\n Determining the direction of the traffic to and from the network interfaces\nRef link: https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html","comment_id":"922024"},{"timestamp":"1701979680.0","content":"Selected Answer: C\nseems like c:\nhttps://aws.amazon.com/tr/blogs/security/how-to-monitor-and-visualize-failed-ssh-access-attempts-to-amazon-ec2-linux-instances/","upvote_count":"2","comments":[{"content":"This link does not mention VPC flow logs at all.","upvote_count":"1","timestamp":"1719545700.0","comment_id":"1107446","poster":"pentium75"}],"comment_id":"917491","poster":"cokutan"},{"comment_id":"917403","upvote_count":"2","comments":[{"timestamp":"1702931280.0","comment_id":"926852","upvote_count":"1","poster":"markw92","content":"D is wrong. EC2 instance state change is only for pending, running etc. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-instance-state-changes.html you can't have state change of ssh or rdp."}],"timestamp":"1701973680.0","content":"Selected Answer: D\nD. Configure an Amazon EventBridge rule to listen for events of type EC2 Instance State-change Notification. Configure an Amazon Simple Notification Service (Amazon SNS) topic as a target. Subscribe the operations team to the topic. This setup allows the EventBridge rule to capture instance state change events, such as when RDP or SSH access is established. The rule can then send notifications to the specified SNS topic, which is subscribed by the operations team.","poster":"ChrisAn"},{"content":"Selected Answer: C\nC: \n\nhttps://www.youtube.com/watch?v=KAe3Eju59OU","timestamp":"1696764000.0","comment_id":"864639","upvote_count":"2","poster":"datz"},{"timestamp":"1693539060.0","poster":"Abhineet9148232","content":"Selected Answer: C\nhttps://aws.amazon.com/blogs/security/how-to-monitor-and-visualize-failed-ssh-access-attempts-to-amazon-ec2-linux-instances/","comment_id":"825538","upvote_count":"2"},{"poster":"bullrem","upvote_count":"1","comment_id":"787059","content":"Selected Answer: A\nA. Configuring Amazon CloudWatch Application Insights to create AWS Systems Manager OpsItems when RDP or SSH access is detected would be the most appropriate solution in this scenario. This would allow the operations team to be notified when RDP or SSH access has been established and provide them with the necessary information to take action if needed. Additionally, Amazon CloudWatch Application Insights would allow for monitoring and troubleshooting of the system in real-time.","timestamp":"1690232460.0"},{"comment_id":"785696","upvote_count":"5","poster":"Training4aBetterLife","timestamp":"1690128780.0","content":"Selected Answer: C\nEC2 Instance State-change Notifications are not the same as RDP or SSH established connection notifications. Use Amazon CloudWatch Logs to monitor SSH access to your Amazon EC2 Linux instances so that you can monitor rejected (or established) SSH connection requests and take action."},{"comment_id":"783962","timestamp":"1689999300.0","comments":[{"timestamp":"1690171920.0","upvote_count":"2","poster":"JayBee65","comment_id":"786196","content":"I completely agree with the logic here, but I'm thinking C, since I believe you will need to \"Create required metric filters\" in order to detect RDP or SSH access, and this is not specified in the question, see https://docs.aws.amazon.com/systems-manager/latest/userguide/OpsCenter-create-OpsItems-from-CloudWatch-Alarms.html"}],"poster":"alexleely","upvote_count":"3","content":"Selected Answer: A\nThe Answer can be A or C depending on the requirement if it requires real-time notification.\nA: Allows the operations team to be notified in real-time when access is established, and also provides visibility into the access events through the OpsItems.\n\nC: The logs will need to be analyzed and metric filters applied to detect access, and then the alarm will trigger based on that analysis. This method could have a delay in providing notifications. Thus, not the best solution if real-time notification is required.\n\nWhy not D: RDP or SSH access does not cause an EC2 instance to have a state change. The state change events that Amazon EventBridge can listen for include stopping, starting, and terminated instances, which do not apply to RDP or SSH access. But RDP or SSH connection to an EC2 instance does generate an event in the system, such as a log entry which can be used to notify the Operation team. Since its a log, you would require a service that monitors logs like CloudTrail, VPC Flow logs, or AWS Systems Manager Session Manager."},{"content":"Selected Answer: C\nIt's C fam. RDP or SSH connections won't change the state of the EC2 instance, so D doesn't make sense.","poster":"owlminus","timestamp":"1689787800.0","comment_id":"781534","upvote_count":"5"},{"comment_id":"776222","upvote_count":"3","comments":[{"poster":"alanp","comment_id":"777563","timestamp":"1689496860.0","upvote_count":"2","content":"Are state changes pending:\nrunning\nstopping\nstopped\nshutting-down\nterminated\n\nhttps://aws.amazon.com/blogs/security/how-to-monitor-and-visualize-failed-ssh-access-attempts-to-amazon-ec2-linux-instances/"}],"poster":"forzadejan","timestamp":"1689397260.0","content":"D. Configure an Amazon EventBridge rule to listen for events of type EC2 Instance State-change Notification. Configure an Amazon Simple Notification Service (Amazon SNS) topic as a target. Subscribe the operations team to the topic.\n\nEC2 instances sends events to the EventBridge when state change occurs, such as when a new RDP or SSH connection is established, you can use EventBridge to configure a rule that listens for these events and trigger an action, like sending an email or SMS, when the connection is detected. The operations team can be notified by subscribing to the Amazon Simple Notification Service (Amazon SNS) topic, which can be configured as the target of the EventBridge rule."},{"upvote_count":"3","comments":[{"content":"um, isn't \"EC2 Instance State-change\" like running, terminated, or stopped?","comment_id":"839103","poster":"CapJackSparrow","timestamp":"1694707080.0","upvote_count":"2"}],"comment_id":"775857","poster":"mhmt4438","timestamp":"1689354900.0","content":"Selected Answer: D\nConfigure an Amazon EventBridge rule to listen for events of type EC2 Instance State-change Notification. Configure an Amazon Simple Notification Service (Amazon SNS) topic as a target. Subscribe the operations team to the topic. This approach allows you to set up a rule that listens for state change events on the EC2 instances, specifically for when RDP or SSH access is established, and trigger a notification via Amazon SNS to the operations team. This way they will be notified when RDP or SSH access to an environment has been established."}],"timestamp":"2023-01-14 20:15:00","url":"https://www.examtopics.com/discussions/amazon/view/95324-exam-aws-certified-solutions-architect-associate-saa-c03/","topic":"1","answer_images":[],"unix_timestamp":1673723700,"isMC":true,"choices":{"C":"Publish VPC flow logs to Amazon CloudWatch Logs. Create required metric filters. Create an Amazon CloudWatch metric alarm with a notification action for when the alarm is in the ALARM state.","B":"Configure the EC2 instances with an IAM instance profile that has an IAM role with the AmazonSSMManagedInstanceCore policy attached.","A":"Configure Amazon CloudWatch Application Insights to create AWS Systems Manager OpsItems when RDP or SSH access is detected.","D":"Configure an Amazon EventBridge rule to listen for events of type EC2 Instance State-change Notification. Configure an Amazon Simple Notification Service (Amazon SNS) topic as a target. Subscribe the operations team to the topic."},"question_images":[],"answer":"C","question_id":169,"question_text":"A company runs demonstration environments for its customers on Amazon EC2 instances. Each environment is isolated in its own VPC. The company’s operations team needs to be notified when RDP or SSH access to an environment has been established.","answers_community":["C (84%)","Other"],"answer_ET":"C","exam_id":31},{"id":"DA8rWlLtEwCgsTee6L6p","exam_id":31,"answer":"AB","url":"https://www.examtopics.com/discussions/amazon/view/95084-exam-aws-certified-solutions-architect-associate-saa-c03/","choices":{"A":"Ensure the root user uses a strong password.","C":"Store root user access keys in an encrypted Amazon S3 bucket.","E":"Apply the required permissions to the root user with an inline policy document.","D":"Add the root user to a group containing administrative permissions.","B":"Enable multi-factor authentication to the root user."},"unix_timestamp":1673639940,"isMC":true,"answer_images":[],"question_text":"A solutions architect has created a new AWS account and must secure AWS account root user access.\n\nWhich combination of actions will accomplish this? (Choose two.)","timestamp":"2023-01-13 20:59:00","answer_description":"","answer_ET":"AB","discussion":[{"content":"Selected Answer: AB\nA. Setting a strong password for the root user is an essential security measure to prevent unauthorized access.\n\nB. Enabling MFA adds an extra layer of security by requiring an additional authentication factor, such as a code from a mobile app or a hardware token, in addition to the password.\n\nC. Root user access keys should be avoided whenever possible, and it is best to use IAM users with restricted permissions instead.\n\nD. The root user already has unrestricted access to all resources and services in the account, so granting additional administrative permissions could increase the risk of unauthorized actions.\n\nE. Instead, it is recommended to create IAM users with appropriate permissions and use those users for day-to-day operations, while keeping the root user secured and only using it for necessary administrative tasks.","upvote_count":"9","poster":"cookieMr","timestamp":"1703772180.0","comment_id":"936616"},{"comment_id":"1077148","poster":"Ruffyit","content":"Ensure the root user uses a strong password. Enable multi-factor authentication to the root user.","upvote_count":"2","timestamp":"1716361080.0"},{"content":"Selected Answer: AB\nEnsure the root user uses a strong password. Enable multi-factor authentication to the root user.","timestamp":"1711094580.0","poster":"TariqKipkemei","comment_id":"1013654","upvote_count":"2"},{"comment_id":"928270","timestamp":"1703067720.0","content":"Selected Answer: AB\nOptions A & B are the CORRECT answers.","poster":"DiscussionMonke","upvote_count":"2"},{"upvote_count":"2","comment_id":"910405","poster":"Bmarodi","timestamp":"1701367440.0","content":"Selected Answer: AB\nOptions A & B are the right answers."},{"timestamp":"1699259580.0","content":"Selected Answer: AB\nSee https://docs.aws.amazon.com/SetUp/latest/UserGuide/best-practices-root-user.html","poster":"luisgu","comment_id":"890537","upvote_count":"2"},{"poster":"Kunj7","upvote_count":"2","comment_id":"854913","content":"Selected Answer: AB\nA and B are the correct answers: \n\nOption A: A strong password is always required for any AWS account you create, and should not be shared or stored anywhere as there is always a risk. \n\nOption B: This is following AWS best practice, by enabling MFA on your root user which provides another layer of security on the account and unauthorised access will be denied if the user does not have the correct password and MFA.","timestamp":"1696018500.0"},{"comment_id":"837656","poster":"WherecanIstart","content":"Selected Answer: AB\nAB are the right answers.","upvote_count":"2","timestamp":"1694575560.0"},{"comment_id":"831704","poster":"fkie4","timestamp":"1694072100.0","content":"This is probably the hardest question in AWS history","upvote_count":"3"},{"poster":"ProfXsamson","upvote_count":"4","comment_id":"793882","timestamp":"1690787160.0","content":"Selected Answer: AB\nAB is the only feasible answer here."},{"content":"Selected Answer: BE\nB. Enabling multi-factor authentication for the root user provides an additional layer of security to ensure that only authorized individuals are able to access the root user account.\nE. Applying the required permissions to the root user with an inline policy document ensures that the root user only has the necessary permissions to perform the necessary tasks, and not any unnecessary permissions that could potentially be misused.","comment_id":"787079","poster":"bullrem","upvote_count":"2","comments":[{"content":"E is wrong because you can't attach permissions or policies to the root user. \nA is right because MFA alone won't help too much if the password is \"123\".","poster":"pentium75","upvote_count":"3","timestamp":"1719546480.0","comment_id":"1107461"},{"upvote_count":"1","poster":"bullrem","comments":[{"timestamp":"1697052000.0","poster":"[Removed]","content":"Strong passwords + multi factor is the counter to brute force...","comment_id":"867612","upvote_count":"1"}],"content":"The other options are not sufficient to secure the root user access because:\nA. A strong password alone is not enough to protect against potential security threats such as phishing or brute force attacks.\nC. Storing the root user access keys in an encrypted S3 bucket does not address the root user's authentication process.\nD. Adding the root user to a group with administrative permissions does not address the root user's authentication process and does not provide an additional layer of security.","timestamp":"1690233900.0","comment_id":"787080"},{"content":"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html","upvote_count":"1","comment_id":"787081","poster":"bullrem","timestamp":"1690233960.0"}],"timestamp":"1690233900.0"},{"poster":"Pindol","content":"Selected Answer: AB\nAB obviusly","comment_id":"786803","timestamp":"1690216560.0","upvote_count":"2"},{"comment_id":"782173","upvote_count":"1","timestamp":"1689845820.0","content":"Selected Answer: AB\nRoot user already has admin, so D is not correct","poster":"david76x"},{"comment_id":"777866","upvote_count":"2","timestamp":"1689514140.0","content":"Selected Answer: AB\nAB are correct","poster":"Aninina"},{"comment_id":"777692","content":"Selected Answer: AB\nD is incorrect as root user already has full admin access.","poster":"wmp7039","upvote_count":"2","timestamp":"1689506580.0"},{"comment_id":"776928","poster":"swolfgang","content":"Selected Answer: AB\nD. Add the root user to a group containing administrative permissions. >>its not about security,actually its unsecure so >> a&B","timestamp":"1689438840.0","upvote_count":"2"},{"timestamp":"1689435720.0","comments":[{"content":"root user is literally the super admin account. What more permissions could you possible give to the root user by adding it to admin group?","timestamp":"1719842400.0","upvote_count":"1","comment_id":"1111332","poster":"awsgeek75"}],"poster":"raf123123","content":"Selected Answer: BD\nBD is correct","comment_id":"776882","upvote_count":"2"},{"comments":[{"content":"D is wrong because the root user is outside of IAM, thus you can't put him into a group. Also he does not need \"administrative permissions\" as he has those anyway.","timestamp":"1719546540.0","comment_id":"1107464","poster":"pentium75","upvote_count":"1"},{"comment_id":"786200","upvote_count":"1","timestamp":"1690171980.0","poster":"JayBee65","content":"What would D achieve exactly??? :)"}],"upvote_count":"2","content":"Selected Answer: BD\nhttps://www.examtopics.com/discussions/amazon/view/21794-exam-aws-certified-solutions-architect-associate-saa-c02/","poster":"mhmt4438","timestamp":"1689355320.0","comment_id":"775861"},{"comment_id":"774814","upvote_count":"2","poster":"kbaruu","timestamp":"1689271140.0","content":"Selected Answer: AB\nhttps://docs.aws.amazon.com/accounts/latest/reference/best-practices-root-user.html\n\n* Enable AWS multi-factor authentication (MFA) on your AWS account root user. For more information, see Using multi-factor authentication (MFA) in AWS in the IAM User Guide.\n\n* Never share your AWS account root user password or access keys with anyone.\n\n* Use a strong password to help protect access to the AWS Management Console. For information about managing your AWS account root user password, see Changing the password for the root user."}],"question_id":170,"answers_community":["AB (86%)","10%"],"topic":"1","question_images":[]}],"exam":{"lastUpdated":"11 Apr 2025","isBeta":false,"numberOfQuestions":1019,"id":31,"isMCOnly":true,"name":"AWS Certified Solutions Architect - Associate SAA-C03","isImplemented":true,"provider":"Amazon"},"currentPage":34},"__N_SSP":true}