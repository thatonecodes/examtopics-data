{"pageProps":{"questions":[{"id":"EpzQ7dmDfF81wjILH8rj","question_images":[],"topic":"1","answer_ET":"D","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/59932-exam-aws-certified-solutions-architect-professional-topic-1/","answer_images":[],"question_id":746,"isMC":true,"discussion":[{"content":"D. - \"A usage plan specifies who can access one or more deployed API stages and methods—and also how much and how fast they can access them. The plan uses API keys to identify API clients and meters access to the associated API stages for each key. It also lets you configure throttling limits and quota limits that are enforced on individual client API keys.\" https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html","poster":"mericov","comment_id":"428144","upvote_count":"18","timestamp":"1633780680.0"},{"timestamp":"1634820300.0","poster":"Jupi","comment_id":"431889","upvote_count":"15","content":"A - wrong. You can use WAF to protect your api gateway directly without cloudfront\nB - wrong. You can use WAF to protect your api gateway directly without cloudfront\nc - wrong. You can use api gateway resource policis to allow users from specified aws account, from specified IP ranges or CIDR blocks or from specified VPCs or VPC endpoints. request limit is not part of resource policies. \nd - correct. API gateway usage plans can limit the API access and be sure that the usage does not exceed thrsholds we define."},{"upvote_count":"2","content":"Selected Answer: D\nA(wrong): OAI is used only for S3.\nB(wrong): This is not possible to define a WAF web ACL rule to block clients that submit more than five requests per day, because\n\n\"\nA rate-based rule tracks the rate of requests for each originating IP address, and triggers the rule action on IPs with rates that go over a limit. You set the limit as the number of requests per 5-minute time span......\nThe following caveats apply to AWS WAF rate-based rules:\nThe minimum rate that you can set is 100.\nAWS WAF checks the rate of requests every 30 seconds, and counts requests for the prior five minutes each time. Because of this, it's possible for an IP address to send requests at too high a rate for 30 seconds before AWS WAF detects and blocks it.\nAWS WAF can block up to 10,000 IP addresses. If more than 10,000 IP addresses send high rates of requests at the same time, AWS WAF will only block 10,000 of them.\n\"\nhttps://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-rate-based.html","timestamp":"1665894420.0","comment_id":"695997","poster":"tomosabc1"},{"comment_id":"649137","timestamp":"1660961580.0","upvote_count":"2","content":"CDN is also a way to prevent DDoS, this question focus to \"500 unique IP addresses worldwide\" so 500 IPs come from multiple Region in the world, so if you use Cloudfront, you will distribute DDoS Attack traffic to nearest PoPs and apply Rate Limiting on this PoPs.\n\n----> Answer is B","poster":"Kyperos"},{"upvote_count":"1","timestamp":"1660279800.0","comment_id":"645744","poster":"Jughead","content":"Selected Answer: D\nD is the answer"},{"timestamp":"1641225000.0","comment_id":"515885","poster":"RVivek","content":"Why Not B ? Adding Cloud front provides AWS Shield service which is a free DDoS protection.https://aws.amazon.com/shield/?whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc","upvote_count":"1"},{"poster":"AzureDP900","upvote_count":"3","content":"Read both docs and choose your option. I am going with D\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-resource-policies.html","timestamp":"1639003680.0","comment_id":"497220"},{"timestamp":"1638949800.0","poster":"cldy","content":"D. Create an AWS WAF web ACL with a rule to allow access to the IP addresses used by the six partners. Associate the web ACL with the API. Create a usage plan with a request limit and associate it with the API. Create an API key and add it to the usage plan.","upvote_count":"2","comment_id":"496618"},{"comment_id":"448686","timestamp":"1636282980.0","poster":"andylogan","upvote_count":"2","content":"It's D"},{"poster":"tgv","upvote_count":"2","content":"DDD\n---","timestamp":"1635174540.0","comment_id":"435415"},{"comment_id":"434567","upvote_count":"2","timestamp":"1635129720.0","poster":"blackgamer","content":"D is the answer"},{"upvote_count":"3","poster":"denccc","content":"I'll go with D","timestamp":"1634454360.0","comment_id":"431696"},{"poster":"neta1o","timestamp":"1634356320.0","upvote_count":"3","content":"+1 to D, seems like usage plans support the referenced rate limits where resource policies don't.","comment_id":"431476"},{"timestamp":"1634266320.0","comment_id":"431081","upvote_count":"6","content":"My vote - D \nAB - Ignored as Cloudfront not required as its regional based resource. It is gonna add costs. Also WAF can directly sit on top of APIGW\nC - Incorrect as resource policies are used to restrict access and not to provide limit. Request limit is done with usage plan.","poster":"vjawscert"},{"content":"I think is C, resource policy allows control the IP source: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-resource-policies.html","timestamp":"1633870320.0","poster":"zolthar_z","comments":[{"poster":"rb39","comment_id":"501690","content":"but you cannot define a limit in a resource policy, it's just allow/deny access","upvote_count":"2","timestamp":"1639518300.0"}],"comment_id":"430228","upvote_count":"4"},{"comment_id":"427502","upvote_count":"4","poster":"pkboy78","content":"I think it is C.","timestamp":"1632238500.0"}],"answer":"D","unix_timestamp":1629381060,"question_text":"A company is creating a REST API to share information with six of its partners based in the United States. The company has created an Amazon API Gateway\nRegional endpoint. Each of the six partners will access the API once per day to post daily sales figures.\nAfter initial deployment, the company observes 1,000 requests per second originating from 500 different IP addresses around the world. The company believes this traffic is originating from a botnet and wants to secure its API while minimizing cost.\nWhich approach should the company take to secure its API?","choices":{"A":"Create an Amazon CloudFront distribution with the API as the origin. Create an AWS WAF web ACL with a rule to block clients that submit more than five requests per day. Associate the web ACL with the CloudFront distribution. Configure CloudFront with an origin access identity (OAI) and associate it with the distribution. Configure API Gateway to ensure only the OAI can run the POST method.","B":"Create an Amazon CloudFront distribution with the API as the origin. Create an AWS WAF web ACL with a rule to block clients that submit more than five requests per day. Associate the web ACL with the CloudFront distribution. Add a custom header to the CloudFront distribution populated with an API key. Configure the API to require an API key on the POST method.","D":"Create an AWS WAF web ACL with a rule to allow access to the IP addresses used by the six partners. Associate the web ACL with the API. Create a usage plan with a request limit and associate it with the API. Create an API key and add it to the usage plan.","C":"Create an AWS WAF web ACL with a rule to allow access to the IP addresses used by the six partners. Associate the web ACL with the API. Create a resource policy with a request limit and associate it with the API. Configure the API to require an API key on the POST method."},"timestamp":"2021-08-19 15:51:00","answers_community":["D (100%)"],"exam_id":32},{"id":"ZdAanUPFHPH2snDCfAMZ","url":"https://www.examtopics.com/discussions/amazon/view/59927-exam-aws-certified-solutions-architect-professional-topic-1/","question_text":"A company is running its AWS infrastructure across two AWS Regions. The company has four VPCs in the eu-west-1 Region and has two VPCs in the us-east-1\nRegion. The company also has an on-premises data center in Europe that has two AWS Direct Connect connections in eu-west-1.\nThe company needs a solution in which Amazon EC2 instances in each VPC can connect to each other by using private IP addresses. Servers in the on-premises data center also must be able to connect to those VPCs by using private IP addresses.\nWhat is the MOST cost-effective solution that meets these requirements?","isMC":true,"discussion":[{"comments":[{"upvote_count":"1","poster":"MikelH93","comment_id":"894162","content":"i go A first but after reading this article i go B \n\nhttps://cloudonaut.io/advanved-aws-networking-pitfalls-that-you-should-avoid/","timestamp":"1683734820.0","comments":[{"comment_id":"1327218","upvote_count":"1","poster":"mnsait","content":"Thanks for sharing the article. Good insights.\n\nAnd same here. I chose A first, then after going through the discussions and the article here, changing my answer to B.","timestamp":"1734337740.0"}]}],"comment_id":"431905","poster":"Jupi","upvote_count":"15","timestamp":"1633779000.0","content":"A - In correct. It will work, but there is cost for each transit gateway\nB - Correct. https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/transit-gateway-vs-vpc-peering.html\n\"Lower cost — With VPC peering you only pay for data transfer charges. Transit Gateway has an hourly charge per attachment in addition to the data transfer fees. \"\nC - public VIFs is for public IP\nD - for transit gateways, you need transit VIFs, not private VIFs.."},{"content":"A is the only correct answer\nWhy not B? \n1) VPC peering edge to edge is not possible also its not trasetive with VPN / DC - how on-prem servers will be able to communicate with VPCs on another region?\nhttps://docs.aws.amazon.com/vpc/latest/peering/invalid-peering-configurations.html#edge-to-edge-vgw\n2) you cant associate VPCs to a direct connect GW only virtual private GW or transit GW\nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways-intro.html\nC - uses public VIFs cant be correct\nD - will not route traffic between regions as it uses private VIFs and not transit VIFs\nFor A its documented architecture - Two DCs, Two VIFs, DX GW and inter-region transit peering, here:\nhttps://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/aws-dx-dxgw-with-aws-transit-gateway-multi-regions-and-aws-public-peering.html","timestamp":"1657653840.0","comment_id":"630658","upvote_count":"8","poster":"asfsdfsdf"},{"comment_id":"1137005","upvote_count":"1","poster":"marszalekm","content":"In B it states: \"Associate each VPC with the Direct Connect gateway.\" you can't do that directly, you must use VGW. If this is implied, then B is of course the best.","timestamp":"1706724240.0"},{"timestamp":"1696774920.0","comment_id":"1028048","upvote_count":"1","content":"Selected Answer: A\nits A\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-aws-transit-gateway.html","poster":"nhorcajada"},{"timestamp":"1685046720.0","upvote_count":"2","content":"Selected Answer: B\nCan someone enlighted me for it seems for me that one private VIF mapped into one direct connect gateway, so \"Create two private VIFs, and attach them to a single Direct Connect gateway\" seems strange for me.\nI searched in the internet only find one direct connection support 50 private VIFs and 500 VPCs, but I didn't find the relationship between direct connection gateway and private VIF( not direct connection and private VIF )","comments":[{"timestamp":"1685047080.0","upvote_count":"1","poster":"Jesuisleon","content":"or how many direct connection gateways can one single direct connection have ?","comment_id":"906869"}],"comment_id":"906867","poster":"Jesuisleon"},{"poster":"Heer","comment_id":"793419","upvote_count":"2","timestamp":"1675118280.0","content":"For those who have selected B :\n\nThis solution would be effective in allowing EC2 instances in each VPC to connect to each other using private IP addresses, and for the on-premises data center to connect to the VPCs using private IP addresses. However, this solution may not be the most cost-effective option, as the cost of the two private VIFs and the Direct Connect gateway can add up. Additionally, there may be additional latency when communicating between VPCs in different Regions due to the cross-Region peering.\n\n\nOption A is the right answer here \nThe most cost-effective solution for this scenario is to create a transit gateway in each Region and connect all the VPCs to the transit gateways. You can then configure a Direct Connect connection to each transit gateway and route all the traffic between the VPCs through the transit gateways. This will allow all instances in the VPCs to connect to each other and to the on-premises data center by using private IP addresses, and it will minimize the overall cost of the solution."},{"upvote_count":"2","comment_id":"763814","poster":"evargasbrz","content":"Selected Answer: B\nI'll go with B - it's the most cost-effective solution comparing to A","timestamp":"1672673880.0"},{"timestamp":"1667493780.0","content":"Selected Answer: B\nClearly B choice, most cost-effective","upvote_count":"2","comment_id":"710658","poster":"alxjandroleiva"},{"content":"Selected Answer: A\nAWS Transit Gateway + transit VIF","poster":"JohnPi","upvote_count":"3","comment_id":"690931","timestamp":"1665394140.0","comments":[{"upvote_count":"1","content":"option B cost less (you do not pay the transit gateway) but is missing the VGW (each VGW is attached to a VPC)","comment_id":"699846","timestamp":"1666265580.0","poster":"JohnPi"}]},{"comment_id":"676228","poster":"pinhead900","upvote_count":"4","timestamp":"1663855380.0","content":"Selected Answer: A\nyou can ONLY associate a transit gateway or a virtual private gateway to the direct connect gateways, therefore B cannot be correct\n https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways-intro.html"},{"poster":"hilft","content":"B. Forum is right. \nThis is a terrible question. I would go for a real-world environment. As an architect, I would always go for transit gateway because peering over peering got scalability issues.","timestamp":"1658617200.0","upvote_count":"1","comment_id":"635789"},{"comment_id":"585405","timestamp":"1649877060.0","content":"Selected Answer: B\nB for cost-effective solution and it make use of DC gateway","poster":"bkrish","upvote_count":"1"},{"upvote_count":"1","poster":"Ni_yot","timestamp":"1645988280.0","comment_id":"557532","content":"Will go with B as well. We must use private VIFs for connectivity. https://aws.amazon.com/premiumsupport/knowledge-center/public-private-interface-dx/"},{"poster":"futen0326","comment_id":"552549","upvote_count":"1","content":"Selected Answer: B\nNot enough VPCs for A to be correct, we're going for cost-efficiency.. B will work fine here.","timestamp":"1645423200.0"},{"comment_id":"539772","poster":"Clandestine60","upvote_count":"1","content":"Selected Answer: B\nB is the answer. cross-region vpc peering handles inter-vpc communication and 2 private VIFs are all thats needed for the ON-prem DC to the differnt VPC connections. 1 private VIF for 1 direct connect connection and the 2nd private VIF for the second DConnect connection. This is possible because we are using a DConnect gateway. with DC gateway, we just need a single private VIF for connectivity to multiple VPCs. \nDetails Here: https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/direct-connect.html","timestamp":"1643900220.0"},{"poster":"HellGate","upvote_count":"1","content":"Selected Answer: D\nAnswer is D. \n\nThis question ask on Transit Gateway Association. https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways-intro.html","comment_id":"538584","timestamp":"1643797440.0"},{"poster":"ByomkeshDas","timestamp":"1642391220.0","comment_id":"525445","content":"Seems option B has lowest cost. But there they have said to create only 2 Private VIFs. But there are total 4+2=6 VPCs in the two regions. So it require 6 private VIFs not 2. Otherwise option A is correct.","upvote_count":"2"},{"comments":[{"upvote_count":"1","timestamp":"1647643680.0","content":"A Direct Connect Gateway can connect to up to 10 VPCs (via VGWs) globally in any AWS account over a single private VIF. so it's B.","comment_id":"570782","poster":"Bigbearcn"}],"poster":"andypham","comment_id":"471112","upvote_count":"3","content":"B is NOT correct, because VPC peering is point to point. So if you want to connect 6 total vpc to on-premises, you need 6 private VIFs, not only 2.","timestamp":"1636056840.0"},{"poster":"andylogan","timestamp":"1635795900.0","upvote_count":"1","content":"It's B - Lower cos","comment_id":"448725"},{"upvote_count":"7","comment_id":"444920","timestamp":"1635093000.0","poster":"anandkl80","content":"Answer: B\n\nWhat is the MOST cost-effective solution that meets these requirements?\n\nWhile this makes TGW a good default for most network architectures, VPC peering is still a valid choice due to the following advantages it has over TGW:\n\nLower cost — With VPC peering you only pay for data transfer charges. Transit Gateway has an hourly charge per attachment in addition to the data transfer fees.\n\nLatency — Unlike VPC peering, Transit Gateway is an additional hop between VPCs."},{"upvote_count":"1","comment_id":"441046","content":"VPC peering is cheaper and additionally you don't pay for transfer within the region if vpcs are in the same zone.","timestamp":"1634886960.0","poster":"DerekKey"},{"upvote_count":"2","comment_id":"435471","content":"BBB\n---\nBoth A & B are correct but the question is asking to be cost-efficient.\nAccording to AWS, you have Lower cost — With VPC peering\nhttps://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/transit-gateway-vs-vpc-peering.html","poster":"tgv","timestamp":"1634609640.0"},{"comments":[{"timestamp":"1634014920.0","content":"Nobody cares about the overhead which option B brings. The request is to be cost-efficient.\nBoth A & B are correct. The question is which of them is more cost-efficient","comment_id":"435454","poster":"tgv","upvote_count":"1"}],"poster":"sergioandreslq","upvote_count":"2","timestamp":"1633917120.0","comment_id":"433903","content":"A:\nIt is the most cost-efficient, AWS recommends using TGW instead of peering when you need to connect many VPCs, In this case, 2 regions plus on-premise servers.\nthe overhead to maintain the peering is very expensive and they will count for IT resources that are part of the TCO. addition, many point to generate issues.\nTransit VIF to connect TGW with DX Gateway.\nB: Too much overhead to manage the peering connection."},{"upvote_count":"3","content":"Initially I would have said D since it is the best solution but since the question wants the most cost effective it would be B. VPC peering only has data transfer charge vs Transit Gateway hourly charge. https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/transit-gateway-vs-vpc-peering.html","timestamp":"1633649700.0","comment_id":"431481","poster":"neta1o"},{"content":"Why not C?","upvote_count":"1","poster":"Cotter","timestamp":"1633068060.0","comment_id":"431215"},{"content":"B is the most cost effective","timestamp":"1632673920.0","poster":"near22","comments":[{"content":"did you considered also how many peers you need for this? six in eu-west-1 and one in us-east-1. And another eight for cross-Region peering.","comments":[{"content":"That is why B is the solution. 6 VPC is not many and it is cost effective to do such a way. Not a clean design, but cost effective","poster":"blackgamer","timestamp":"1634001960.0","comment_id":"434571","upvote_count":"2"}],"timestamp":"1632931860.0","upvote_count":"1","poster":"mericov","comment_id":"429941"}],"comment_id":"428642","upvote_count":"3"},{"poster":"mericov","timestamp":"1632424200.0","content":"A. Transit VIF is used when you attaching via a DX GW to a transit GW,","upvote_count":"6","comment_id":"428153"},{"comment_id":"427493","content":"I think it is D","poster":"pkboy78","upvote_count":"2","comments":[{"content":"I think it is A","poster":"pkboy78","upvote_count":"1","comment_id":"431051","timestamp":"1632979740.0"}],"timestamp":"1632112500.0"}],"answer":"B","question_images":[],"unix_timestamp":1629380400,"answer_description":"","exam_id":32,"choices":{"D":"Create an AWS Transit Gateway in each Region, and attach each VPC to the transit gateway in that Region. Create cross-Region peering between the transit gateways. Create two private VIFs, and attach them to a single Direct Connect gateway. Associate each VPC with the Direct Connect gateway.","A":"Create an AWS Transit Gateway in each Region, and attach each VPC to the transit gateway in that Region. Create cross-Region peering between the transit gateways. Create two transit VIFs, and attach them to a single Direct Connect gateway. Associate each transit gateway with the Direct Connect gateway.","B":"Create VPC peering between each VPC in the same Region. Create cross-Region peering between each VPC in different Regions. Create two private VIFs, and attach them to a single Direct Connect gateway. Associate each VPC with the Direct Connect gateway.","C":"Create VPC peering between each VPC in the same Region. Create cross-Region peering between each VPC in different Regions. Create two public VIFs that are configured to route AWS IP addresses globally to on-premises servers."},"question_id":747,"topic":"1","answer_ET":"B","answer_images":[],"timestamp":"2021-08-19 15:40:00","answers_community":["B (50%)","A (44%)","6%"]},{"id":"T1tZMRzUu2f9kNjNu2I7","isMC":true,"timestamp":"2021-08-19 15:53:00","answer_ET":"B","unix_timestamp":1629381180,"exam_id":32,"answers_community":["B (83%)","D (17%)"],"discussion":[{"comment_id":"435451","timestamp":"1633404060.0","content":"BBB\n---\nKey words: \"A solutions architect PLANS to use the AWS Schema Conversion Tool and\nAWS Database Migration Service (AWS DMS) for the migration\" so running the conversion report for Oracle to Aurora MySQL hasn't been done yet \nhttps://docs.aws.amazon.com/dms/latest/sbs/chap-rdsoracle2aurora.html\n\nA: AWS actually recommends to: \"drop primary key indexes, secondary indexes, referential integrity constraints, and data manipulation language (DML) triggers. Or you can delay their creation until after the full load tasks are complete\" --> https://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html\nC: M5 doesn't exist --> https://docs.aws.amazon.com/dms/latest/userguide/CHAP_ReplicationInstance.Types.html\nD: You can't disable automated backups on Aurora. The backup retention period for Aurora is managed by the DB cluster --> https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html","comments":[{"timestamp":"1634212560.0","poster":"Coffeinerd","upvote_count":"1","content":"B seems the most logical, we are talking about migration with no downtime, not about increase speed of loading data, so B is the best here.","comment_id":"439689"}],"poster":"tgv","upvote_count":"13"},{"timestamp":"1634049720.0","comment_id":"439102","content":"Following the replies here it seems that no answer can be correct (see tgv + Jupi)\nMy guess: there is a typo in answer B - it should be AWS SCT in the first part of the answer. Then it all makes sense.","upvote_count":"10","poster":"TomPaschenda"},{"content":"Selected Answer: B\nI'll go with B, but It seems like the answers don't match the question or there are some typos or mistakes. I think the first tool in B should be SCT.","comment_id":"763815","timestamp":"1672674120.0","upvote_count":"2","poster":"evargasbrz"},{"upvote_count":"2","poster":"bobsmith2000","timestamp":"1653231540.0","content":"It seems like the answers don't match the question or there are some typos or mistakes","comment_id":"605579"},{"content":"B is my answer","timestamp":"1646714820.0","comment_id":"563026","upvote_count":"1","poster":"Kuang"},{"poster":"Bigbearcn","comment_id":"539244","timestamp":"1643843520.0","upvote_count":"1","content":"Selected Answer: D\nIt's D."},{"upvote_count":"1","comment_id":"498483","content":"B. Use AWS SCT to run the conversion report for Oracle to Aurora MySQL. Remediate any issues. Then use AWS DMS to migrate the data.","poster":"cldy","timestamp":"1639128540.0","comments":[{"timestamp":"1734344040.0","upvote_count":"1","content":"Yes, this seems to be the right set of things to do. You run SCT first and get the conversion report. Based on the report, take remediate action for those that are not automatically handled. Then use DMS to migrate the data.","poster":"mnsait","comment_id":"1327266"}]},{"content":"B is right","upvote_count":"1","poster":"AzureDP900","timestamp":"1639003980.0","comment_id":"497222"},{"comment_id":"488895","upvote_count":"3","poster":"acloudguru","content":"Selected Answer: B\nThe first tool in B should be SCT, then B will be the answer. Anyway this is the simplest migration question i've met so far ,hope I can have it in my exam","timestamp":"1638081540.0"},{"content":"It's B","upvote_count":"1","comment_id":"448727","poster":"andylogan","timestamp":"1635711540.0"},{"upvote_count":"1","poster":"Kopa","timestamp":"1634706000.0","comment_id":"444477","content":"I will go for B"},{"upvote_count":"2","timestamp":"1633198680.0","comment_id":"435430","poster":"tgv","content":"GUYS, You can't disable automated backups on Aurora\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html\nIt cannot be D","comments":[{"comment_id":"436082","timestamp":"1633948200.0","poster":"network_zeal","upvote_count":"2","content":"good catch. B seems to be right answer. yes, schema Conversion is done by SCT but SCT is a part of DMS."},{"comment_id":"440218","poster":"student22","upvote_count":"1","content":"Yes, the link clearly states that:\n\"You can't disable automated backups on Aurora. The backup retention period for Aurora is managed by the DB cluster.\"\nAs TomPaschenda has mentioned, it looks like there's a typo in Answer B which should be SCT in the first part.","timestamp":"1634258580.0"}]},{"content":"Only D seems to make sense.","upvote_count":"1","poster":"blackgamer","comment_id":"434572","timestamp":"1633172460.0"},{"timestamp":"1633031280.0","poster":"Jupi","comment_id":"431916","content":"A - Incorrect This will be done by DMS by default.\nB - Incorrect - Conversion reports are by Schema Conversion tool\nC - Incorrect AWS DMS creates the replication instance on an Amazon EC2 instance. AWS DMS currently supports the T2, T3, C4, C5, R4, and R5 Amazon EC2 instance classes for replication instances:\nD - Correct. This will ensure migration is faster.","upvote_count":"1"},{"poster":"Cotter","upvote_count":"1","comment_id":"431218","content":"OK the most sound is D.","timestamp":"1632640380.0"},{"comments":[{"upvote_count":"1","comment_id":"441051","poster":"DerekKey","timestamp":"1634647500.0","content":"This is a general source not Aurora MySQL specific:\nYou can't disable automated backups on Aurora. The backup retention period for Aurora is managed by the DB cluster.\nD is WRONG"}],"comment_id":"428519","poster":"pablobairat","upvote_count":"1","content":"D\nSource: https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Troubleshooting.html#CHAP_Troubleshooting.General.SlowTask","timestamp":"1632530160.0"},{"poster":"mericov","content":"D - When migrating to an Amazon RDS database, it's a good idea to turn off backups and Multi-AZ on the target until you're ready to cut over. Similarly, when migrating to systems other than Amazon RDS, turning off any logging on the target until after cutover is usually a good idea. https://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html","comments":[{"content":"WRONG we have Aurora MySQL","comment_id":"441055","upvote_count":"1","timestamp":"1634678160.0","poster":"DerekKey"}],"timestamp":"1632430800.0","upvote_count":"1","comment_id":"428162"},{"upvote_count":"1","timestamp":"1632173580.0","comment_id":"427506","content":"It should be D","poster":"pkboy78"}],"question_text":"A company runs an application that gives users the ability to search for videos and related information by using keywords that are curated from content providers.\nThe application data is stored in an on-premises Oracle database that is 800 GB in size.\nThe company wants to migrate the data to an Amazon Aurora MySQL DB instance. A solutions architect plans to use the AWS Schema Conversion Tool and\nAWS Database Migration Service (AWS DMS) for the migration. During the migration, the existing database must serve ongoing requests. The migration must be completed with minimum downtime.\nWhich solution will meet these requirements?","answer":"B","answer_description":"","question_images":[],"topic":"1","question_id":748,"answer_images":[],"choices":{"A":"Create primary key indexes, secondary indexes, and referential integrity constraints in the target database before starting the migration process.","B":"Use AWS DMS to run the conversion report for Oracle to Aurora MySQL. Remediate any issues. Then use AWS DMS to migrate the data.","D":"Turn off automatic backups and logging of the target database until the migration and cutover processes are complete.","C":"Use the M5 or C5 DMS replication instance type for ongoing replication."},"url":"https://www.examtopics.com/discussions/amazon/view/59933-exam-aws-certified-solutions-architect-professional-topic-1/"},{"id":"RAdPAeSGUr9sd1c5CICg","url":"https://www.examtopics.com/discussions/amazon/view/59934-exam-aws-certified-solutions-architect-professional-topic-1/","answer":"A","answer_description":"","unix_timestamp":1629381600,"choices":{"B":"Enable AWS CloudTrail logging. Specify an Amazon S3 bucket as the destination for the logs.","D":"Create an Amazon CloudWatch log group. Configure Amazon SES to send logs to the log group.","C":"Use Amazon Athena to query the logs in the Amazon S3 bucket for recipient, subject, and time sent.","E":"Use Amazon Athena to query the logs in Amazon CloudWatch for recipient, subject, and time sent.","A":"Create an Amazon SES configuration set with Amazon Kinesis Data Firehose as the destination. Choose to send logs to an Amazon S3 bucket."},"answers_community":["A (50%)","C (33%)","D (17%)"],"answer_images":[],"question_text":"A travel company built a web application that uses Amazon Simple Email Service (Amazon SES) to send email notifications to users. The company needs to enable logging to help troubleshoot email delivery issues. The company also needs the ability to do searches that are based on recipient, subject, and time sent.\nWhich combination of steps should a solutions architect take to meet these requirements? (Choose two.)","exam_id":32,"answer_ET":"A","isMC":true,"topic":"1","timestamp":"2021-08-19 16:00:00","question_images":[],"discussion":[{"poster":"mericov","comments":[{"poster":"RVivek","comments":[{"content":"\"Athena supports creating tables and querying data from CSV, TSV, custom-delimited, and JSON formats; data from Hadoop-related formats: ORC, Apache Avro and Parquet; logs from Logstash, AWS CloudTrail logs, and Apache WebServer logs.\"","timestamp":"1667887800.0","upvote_count":"2","comment_id":"713520","poster":"Byrney"}],"content":"KinesisFirehose data type is JSON. Athena query wll not work on that. \nD& E id the answer. https://docs.aws.amazon.com/ses/latest/dg/monitor-using-event-publishing.html","comment_id":"545647","timestamp":"1644639480.0","upvote_count":"1"}],"timestamp":"1632596580.0","content":"A / C - https://docs.aws.amazon.com/ses/latest/dg/monitor-sending-activity.html","comment_id":"428174","upvote_count":"10"},{"poster":"denccc","content":"B and C","timestamp":"1632608400.0","comments":[{"comment_id":"429142","timestamp":"1632609780.0","content":"Change to A and C: To enable you to track your email sending at a granular level, you can set up Amazon SES to publish email sending events to Amazon CloudWatch, Amazon Kinesis Data Firehose, or Amazon Simple Notification Service based on characteristics that you define. https://docs.aws.amazon.com/ses/latest/dg/monitor-using-event-publishing.html","upvote_count":"1","comments":[{"content":"FYI https://docs.aws.amazon.com/ses/latest/DeveloperGuide/using-configuration-sets.html","timestamp":"1632646680.0","comment_id":"431699","upvote_count":"1","poster":"denccc"}],"poster":"denccc"}],"upvote_count":"8","comment_id":"428324"},{"content":"Selected Answer: A\nA+C is the best answer:\nhttps://aws.amazon.com/blogs/messaging-and-targeting/analyzing-amazon-ses-event-data-with-aws-analytics-services/","upvote_count":"1","poster":"dev112233xx","comment_id":"879411","timestamp":"1682344980.0"},{"timestamp":"1675119240.0","poster":"Heer","comment_id":"793428","upvote_count":"1","content":"Option B & D are also relevant here and can be used for logging and then querying based on based on recipient, subject, and time sent.\n\nBut the cost effect solution is Option A & C"},{"poster":"evargasbrz","timestamp":"1672674540.0","content":"I'll go with A and C \nhttps://docs.aws.amazon.com/ses/latest/dg/event-publishing-add-event-destination.html\nhttps://docs.aws.amazon.com/ses/latest/dg/event-publishing-add-event-destination-firehose.html\nhttps://docs.aws.amazon.com/ses/latest/dg/monitor-sending-activity.html","comment_id":"763819","upvote_count":"1","comments":[{"content":"BTW: It's also possible to use Amazon Athena to query logs in CloudWatch Logs \nhttps://docs.aws.amazon.com/athena/latest/ug/connectors-cloudwatch.html","comment_id":"763827","upvote_count":"1","timestamp":"1672674900.0","poster":"evargasbrz"}]},{"comment_id":"700011","poster":"pek77","content":"https://docs.aws.amazon.com/ses/latest/dg/event-publishing-add-event-destination.html\n\nAC\n\nIf you simply want a running total of each type of event (for example, so that you can set an alarm when the total gets too high), you can use CloudWatch.\n\nIf you want detailed event records that you can output to another service such as Amazon OpenSearch Service or Amazon Redshift for analysis, you can use Kinesis Data Firehose.","upvote_count":"1","timestamp":"1666275240.0"},{"comment_id":"666674","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/athena/latest/ug/connectors-cloudwatch.html","poster":"Yashar1691","upvote_count":"1","timestamp":"1662965460.0"},{"content":"https://docs.aws.amazon.com/ses/latest/dg/event-publishing-add-event-destination-cloudwatch.html\nEmail Header – Amazon SES retrieves the dimension name and value from a header in the email. Note\nYou can't use any of the following email headers as the Dimension Name: Received, To, From, DKIM-Signature, CC, message-id, or Return-Path...so A&C","poster":"JonnyB1001","comment_id":"631773","timestamp":"1657892880.0","upvote_count":"1"},{"upvote_count":"2","timestamp":"1652289720.0","poster":"bobsmith2000","content":"It's AC.\n\n\"The event destination that you choose depends on the level of detail you want about the events, and the way you want to receive the event information. If you simply want a running total of each type of event (for example, so that you can set an alarm when the total gets too high), you can use CloudWatch.\n\nIf you want detailed event records that you can output to another service such as Amazon OpenSearch Service or Amazon Redshift for analysis, you can use Kinesis Data Firehose.\n\nIf you want to receive notifications when certain events occur, you can use Amazon SNS.\"\nSource:\nhttps://docs.aws.amazon.com/ses/latest/dg/event-publishing-add-event-destination.html","comment_id":"600237"},{"upvote_count":"2","poster":"Hasitha99","content":"Selected Answer: A\nAnswer: A, C https://docs.aws.amazon.com/ses/latest/DeveloperGuide/using-configuration-sets.html","comment_id":"586897","timestamp":"1650129000.0"},{"comment_id":"565088","poster":"Yasyas86","timestamp":"1646955540.0","upvote_count":"1","content":"Answer is A/C \nhttps://aws.amazon.com/getting-started/hands-on/build-serverless-real-time-data-processing-app-lambda-kinesis-s3-dynamodb-cognito-athena/4/#:~:text=Amazon%20Athena%20allows%20us%20to,to%20an%20Amazon%20S3%20bucket."},{"poster":"RVivek","timestamp":"1644637920.0","comments":[{"comment_id":"545645","content":"Answer is D & E.\nKinesis Firehose delivers Data in JSON format . To run Athena query JSON format should be conveted using Lambda function","timestamp":"1644639120.0","upvote_count":"1","poster":"RVivek"}],"upvote_count":"1","comment_id":"545634","content":"A& C will work\nD& E also work \nNow whic combination is the best soloutiion ?"},{"timestamp":"1639004220.0","content":"A,C is right","comment_id":"497225","poster":"AzureDP900","upvote_count":"2"},{"timestamp":"1638869940.0","content":"A. Create an Amazon SES configuration set with Amazon Kinesis Data Firehose as the destination. Choose to send logs to an Amazon S3 bucket.\nC. Use Amazon Athena to query the logs in the Amazon S3 bucket for recipient, subject, and time sent.","upvote_count":"1","comment_id":"495837","poster":"cldy"},{"upvote_count":"2","content":"Selected Answer: C\nCHoose A and C","comment_id":"483069","poster":"acloudguru","timestamp":"1637477760.0"},{"timestamp":"1636450800.0","comment_id":"474714","upvote_count":"1","poster":"Kopa","content":"A and C\n\nhttps://docs.aws.amazon.com/ses/latest/dg/event-publishing-retrieving-firehose.html"},{"content":"It's A, C","timestamp":"1635282780.0","comment_id":"448731","poster":"andylogan","upvote_count":"1"},{"comments":[{"upvote_count":"3","comment_id":"448730","timestamp":"1634134680.0","poster":"andylogan","content":"Quote from your link \"Using the information collected by CloudTrail, you can determine what request was made to Amazon SES, the source IP address from which the request was made, who made the request, when it was made, and so on\"\nSo CloudTrail doesn't log email content, answer should be A, C instead of B"}],"poster":"AkanshaR91","content":"Answer should be B and C.\nReference-\nhttps://aws.amazon.com/about-aws/whats-new/2015/05/amazon-ses-is-now-integrated-with-cloudtrail/","upvote_count":"1","comment_id":"441766","timestamp":"1633807980.0"},{"upvote_count":"2","comment_id":"440219","timestamp":"1633499340.0","content":"A,C\nSES --> Kinesis Firehose --> S3 --> Query with Athena","poster":"student22"},{"comment_id":"439922","timestamp":"1633201020.0","upvote_count":"3","poster":"Liongeek","content":"Ans is A and C . WHY?\n\nYou can both publish logs and metric to CloudWatch and Kinesis Data Firehose, but ONLY can publish detailed event records to Kinesis Data Firehose.\nAnd of course, once in Firehose you can put the logs in S3 and analyze them with Athena\n\nOficial Ref: https://docs.aws.amazon.com/ses/latest/dg/event-publishing-add-event-destination.html"},{"content":"AAA CCC\n---\nB: CloudTrail doesn't log this kind of info","poster":"tgv","timestamp":"1633171380.0","upvote_count":"2","comment_id":"435486"},{"poster":"blackgamer","comment_id":"434592","upvote_count":"1","content":"A and C seems to be right.","timestamp":"1632830940.0"},{"timestamp":"1632687840.0","comment_id":"432846","poster":"hdomingo","content":"A & C - B is incorrect, CloudTrail don´t logging this kind of info (Amazon SES only delivers management events to CloudTrail): https://docs.aws.amazon.com/es_es/ses/latest/DeveloperGuide/logging-using-cloudtrail.html","upvote_count":"2"},{"content":"I think it is B and C","upvote_count":"1","comment_id":"427508","timestamp":"1632157260.0","poster":"pkboy78"}],"question_id":749},{"id":"ULU7UwxMNDmCAKYQsmJw","answer_description":"","unix_timestamp":1629382260,"answer":"CDE","url":"https://www.examtopics.com/discussions/amazon/view/59937-exam-aws-certified-solutions-architect-professional-topic-1/","topic":"1","isMC":true,"answer_images":[],"answer_ET":"CDE","question_id":750,"discussion":[{"content":"C,D,E is right!!","poster":"iillii","timestamp":"1646890620.0","upvote_count":"4","comment_id":"564563"},{"comment_id":"497228","upvote_count":"2","content":"C,D,E is right","timestamp":"1639004460.0","poster":"AzureDP900"},{"upvote_count":"1","timestamp":"1638969120.0","comments":[{"upvote_count":"2","content":"SES is a service that helps you send/receive emails, not a service that could subscribe an event.","timestamp":"1653424920.0","comment_id":"606900","poster":"lingxian"}],"content":"Why not B?","comment_id":"496835","poster":"Meghaaaa"},{"comment_id":"496544","upvote_count":"2","content":"C. Deploy EC2 instances in an Auto Scaling group. Configure the launch template to deploy instances without key pairs. Configure Amazon CloudWatch Logs to capture system access logs. Create an Amazon CloudWatch alarm that is based on the logs to detect when a user logs in to an EC2 instance.\nD. Configure an Amazon Simple Notification Service (Amazon SNS) topic to send a message to the security team when an alarm is activated.\nE. Turn on AWS CloudTrail logs for all AWS Regions. Configure Amazon CloudWatch alarms to provide an alert when an AwsConsoleSignin event is detected.","poster":"cldy","timestamp":"1638940740.0"},{"upvote_count":"2","timestamp":"1638040920.0","content":"C,D,E is correct answer","comment_id":"488462","poster":"AzureDP900"},{"timestamp":"1637541300.0","upvote_count":"2","poster":"acloudguru","content":"Selected Answer: CDE\nF is not right, cloudwatch dashboard does not have such way to meet the requirement","comment_id":"483758"},{"upvote_count":"2","content":"yep C.D.E","comment_id":"474732","poster":"Kopa","timestamp":"1636453560.0"},{"timestamp":"1635299220.0","content":"It's C D E","poster":"andylogan","comment_id":"448734","upvote_count":"2"},{"upvote_count":"3","content":"CCC DDD EEE\n---","timestamp":"1635099060.0","comment_id":"435488","poster":"tgv"},{"upvote_count":"1","poster":"blackgamer","timestamp":"1634946780.0","comment_id":"434594","content":"CDE is the answer."},{"upvote_count":"1","poster":"Cotter","comment_id":"431229","timestamp":"1633796460.0","content":"Sure for C,D and E."},{"comment_id":"429151","content":"Also going for CDE","upvote_count":"1","poster":"denccc","timestamp":"1632848760.0"},{"poster":"pablobairat","comment_id":"428522","timestamp":"1632723600.0","content":"C,D,E it is","upvote_count":"3"},{"timestamp":"1632380760.0","upvote_count":"2","comment_id":"427514","poster":"pkboy78","content":"I think it is C, D and E"}],"timestamp":"2021-08-19 16:11:00","choices":{"D":"Configure an Amazon Simple Notification Service (Amazon SNS) topic to send a message to the security team when an alarm is activated.","C":"Deploy EC2 instances in an Auto Scaling group. Configure the launch template to deploy instances without key pairs. Configure Amazon CloudWatch Logs to capture system access logs. Create an Amazon CloudWatch alarm that is based on the logs to detect when a user logs in to an EC2 instance.","A":"Turn on AWS CloudTrail logs in the application's primary AWS Region. Use Amazon Athena to query the logs for AwsConsoleSignIn events.","B":"Configure Amazon Simple Email Service (Amazon SES) to send email to the security team when an alarm is activated.","E":"Turn on AWS CloudTrail logs for all AWS Regions. Configure Amazon CloudWatch alarms to provide an alert when an AwsConsoleSignIn event is detected.","F":"Deploy EC2 instances in an Auto Scaling group. Configure the launch template to delete the key pair after launch. Configure Amazon CloudWatch Logs for the system access logs. Create an Amazon CloudWatch dashboard to show user logins over time."},"exam_id":32,"answers_community":["CDE (100%)"],"question_text":"A company is launching a new web application on Amazon EC2 instances. Development and production workloads exist in separate AWS accounts.\nAccording to the company's security requirements, only automated configuration tools are allowed to access the production account. The company's security team wants to receive immediate notification if any manual access to the production AWS account or EC2 instances occurs.\nWhich combination of actions should a solutions architect take in the production account to meet these requirements? (Choose three.)","question_images":[]}],"exam":{"isImplemented":true,"lastUpdated":"11 Apr 2025","isBeta":false,"provider":"Amazon","numberOfQuestions":1019,"name":"AWS Certified Solutions Architect - Professional","isMCOnly":false,"id":32},"currentPage":150},"__N_SSP":true}