{"pageProps":{"questions":[{"id":"s9ZU6ZSRb3BwfdA3XaN5","url":"https://www.examtopics.com/discussions/amazon/view/151931-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","unix_timestamp":1732500540,"question_id":71,"timestamp":"2024-11-25 03:09:00","choices":{"D":"Define and create the calculated field in the dashboard.","B":"Define and create the calculated field in the analysis.","A":"Define and create the calculated field in the dataset.","C":"Define and create the calculated field in the visual."},"question_images":[],"answer_ET":"A","isMC":true,"answers_community":["A (100%)"],"question_text":"A retail company is expanding its operations globally. The company needs to use Amazon QuickSight to accurately calculate currency exchange rates for financial reports. The company has an existing dashboard that includes a visual that is based on an analysis of a dataset that contains global currency values and exchange rates.\n\nA data engineer needs to ensure that exchange rates are calculated with a precision of four decimal places. The calculations must be precomputed. The data engineer must materialize results in QuickSight super-fast, parallel, in-memory calculation engine (SPICE).\n\nWhich solution will meet these requirements?","answer_images":[],"topic":"1","exam_id":21,"answer":"A","discussion":[{"upvote_count":"1","content":"Selected Answer: A\nA: Sí, porque al crear el campo calculado en el dataset se precomputan los valores y se materializan en SPICE, asegurando precisión y rapidez.\nB: No, porque los campos calculados en el análisis se calculan en tiempo de consulta, no se precomputan.\nC: No, porque los campos calculados en la visual se generan al renderizar, no se almacenan en SPICE.\nD: No, porque los campos calculados en el dashboard tampoco se precomputan en SPICE.","poster":"italiancloud2025","timestamp":"1739917440.0","comment_id":"1358485"},{"comment_id":"1317293","poster":"emupsx1","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/quicksight/latest/user/adding-a-calculated-field-analysis.html","timestamp":"1732500540.0","upvote_count":"2"}],"answer_description":""},{"id":"SlSnhzKaZhFQ9RtXZC7D","topic":"1","answer":"B","question_id":72,"unix_timestamp":1730002860,"isMC":true,"answers_community":["B (50%)","A (42%)","8%"],"question_images":[],"discussion":[{"poster":"bad1ccc","timestamp":"1743667980.0","comment_id":"1426531","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/athena/latest/ug/federated-queries.html","upvote_count":"1"},{"content":"Selected Answer: D\nThe requirement is to aggregate the data in S3. Only option has exclusively called this out. So Ans D is correct","comment_id":"1399527","upvote_count":"1","timestamp":"1742186460.0","poster":"Palee"},{"comment_id":"1341172","upvote_count":"1","poster":"MerryLew","timestamp":"1736963700.0","content":"Selected Answer: A\nAthena can be used to build certain types of data pipelines, particularly when the primary focus is on ad-hoc analysis and querying large datasets stored in S3 without the need for complex data transformations, but for more intricate data processing and heavy ETL operations, other AWS services like Glue are often more suitable due to their dedicated data processing capabilities."},{"upvote_count":"1","content":"Selected Answer: A\nGlue is the right tool \nto build pipeline","poster":"Eeshav15","timestamp":"1736726820.0","comment_id":"1339738"},{"poster":"michele_scar","timestamp":"1731684300.0","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/athena/latest/ug/connectors-available.html","comment_id":"1312689","upvote_count":"2"},{"poster":"Eleftheriia","upvote_count":"3","content":"Selected Answer: B\nWould it be B\n\"If you have data in sources other than Amazon S3, you can use Athena Federated Query to query the data in place or build pipelines that extract data from multiple data sources and store them in Amazon S3. With Athena Federated Query, you can run SQL queries across data stored in relational, non-relational, object, and custom data sources.\"\n\nhttps://docs.aws.amazon.com/athena/latest/ug/connect-to-a-data-source.html","comment_id":"1311830","timestamp":"1731579660.0"},{"content":"Correct Answer: B \n\nUse the Amazon Athena federated query connectors for Amazon Redshift, Teradata, and BigQuery to build the pipeline in Athena. Write a SQL query to read from all the data sources, join the data, and run a Merge operation on the data lake Iceberg table.","poster":"kupo777","timestamp":"1730811000.0","upvote_count":"3","comment_id":"1307370"},{"poster":"ae35a02","timestamp":"1730122560.0","upvote_count":"3","comment_id":"1303981","content":"Selected Answer: A\nAWS GLUE has native connectors to Redshift, BigQuery and Terradata, and integrates with Iceberg format.\nAthena is not for building Pipelines, AppFlow is for transfering data from Saas applications"},{"timestamp":"1730002860.0","content":"Answer:A","poster":"Parandhaman_Margan","upvote_count":"2","comment_id":"1303481"}],"exam_id":21,"choices":{"C":"Use the native Amazon Redshift connector, the Java Database Connectivity (JDBC) connector for Teradata, and the open source Apache Spark BigQuery connector to build the pipeline in Amazon EMR. Write code in PySpark to join the data. Run a Merge operation on the data lake Iceberg table.","B":"Use the Amazon Athena federated query connectors for Amazon Redshift, Teradata, and BigQuery to build the pipeline in Athena. Write a SQL query to read from all the data sources, join the data, and run a Merge operation on the data lake Iceberg table.","D":"Use the native Amazon Redshift, Teradata, and BigQuery connectors in Amazon Appflow to write data to Amazon S3 and AWS Glue Data Catalog. Use Amazon Athena to join the data. Run a Merge operation on the data lake Iceberg table.","A":"Use native Amazon Redshift, Teradata, and BigQuery connectors to build the pipeline in AWS Glue. Use native AWS Glue transforms to join the data. Run a Merge operation on the data lake Iceberg table."},"url":"https://www.examtopics.com/discussions/amazon/view/150342-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","timestamp":"2024-10-27 05:21:00","answer_description":"","answer_ET":"B","answer_images":[],"question_text":"A company has three subsidiaries. Each subsidiary uses a different data warehousing solution. The first subsidiary hosts its data warehouse in Amazon Redshift. The second subsidiary uses Teradata Vantage on AWS. The third subsidiary uses Google BigQuery.\n\nThe company wants to aggregate all the data into a central Amazon S3 data lake. The company wants to use Apache Iceberg as the table format.\n\nA data engineer needs to build a new pipeline to connect to all the data sources, run transformations by using each source engine, join the data, and write the data to Iceberg.\n\nWhich solution will meet these requirements with the LEAST operational effort?"},{"id":"hNX977I3u2ABaqOSSfm9","answer":"B","question_images":[],"answers_community":["B (100%)"],"isMC":true,"choices":{"D":"Create an IAM user that has an access key to access the DynamoDB table. Use Kubernetes secrets that are mounted in a volume of the EKS duster nodes to store the user access key data.","C":"Create an IAM user that has an access key to access the DynamoDB table. Use environment variables in the EKS containers to store the IAM user access key data.","B":"Attach an IAM role to the EKS worker nodes, Grant the IAM role access to DynamoDUse the IAM role to set up IAM roles service accounts (IRSA) functionality.","A":"Store the AWS credentials in an Amazon S3 bucket. Grant the EKS containers access to the S3 bucket to retrieve the credentials."},"timestamp":"2024-11-22 23:28:00","question_id":73,"question_text":"A company is building a data stream processing application. The application runs in an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The application stores processed data in an Amazon DynamoDB table.\n\nThe company needs the application containers in the EKS cluster to have secure access to the DynamoDB table. The company does not want to embed AWS credentials in the containers.\n\nWhich solution will meet these requirements?","topic":"1","answer_description":"","answer_ET":"B","url":"https://www.examtopics.com/discussions/amazon/view/151854-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","exam_id":21,"answer_images":[],"unix_timestamp":1732314480,"discussion":[{"content":"Selected Answer: B\nhttps://docs.aws.amazon.com/eks/latest/userguide/create-node-role.html","upvote_count":"1","poster":"emupsx1","comment_id":"1317296","timestamp":"1732502460.0"},{"upvote_count":"1","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html","poster":"jacob_nz","comment_id":"1316514","timestamp":"1732314480.0"}]},{"id":"jUDcrdWkegc5Gtu2XjoQ","answer":"B","answer_images":[],"isMC":true,"choices":{"D":"Create an AWS Direct Connect connection to the on-premises data center. Store the application keys in AWS Secrets Manager. Create Amazon S3 buckets that contain presigned URLS that have one-day expiration dates.","C":"Create a security group in a public subnet. Configure the security group to allow only connections from the CIDR blocks that correspond to the data producer. Create Amazon S3 buckets than contain presigned URLS that have one-day expiration dates.","A":"Instruct the new data producer to create Amazon Machine Images (AMIs) on Amazon Elastic Container Service (Amazon ECS) to store the code base of the application. Create security groups in a public subnet that allow connections only to the on-premises data center.","B":"Create an AWS Direct Connect connection to the on-premises data center. Store the service account credentials in AWS Secrets manager."},"timestamp":"2024-11-25 03:48:00","discussion":[{"poster":"Ell89","content":"Selected Answer: B\nB. all others contain partial nonsense","upvote_count":"1","timestamp":"1740692400.0","comment_id":"1362734"},{"poster":"MerryLew","content":"Selected Answer: B\nFor secure connections without cost constraints, always think Direct Connect.","upvote_count":"1","comment_id":"1341179","timestamp":"1736964240.0"},{"upvote_count":"2","content":"Selected Answer: B\nDirect Connect + Secret Manager","poster":"emupsx1","timestamp":"1732502880.0","comment_id":"1317300"}],"unix_timestamp":1732502880,"question_text":"A data engineer needs to onboard a new data producer into AWS. The data producer needs to migrate data products to AWS.\n\nThe data producer maintains many data pipelines that support a business application. Each pipeline must have service accounts and their corresponding credentials. The data engineer must establish a secure connection from the data producer's on-premises data center to AWS. The data engineer must not use the public internet to transfer data from an on-premises data center to AWS.\n\nWhich solution will meet these requirements?","url":"https://www.examtopics.com/discussions/amazon/view/151933-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_images":[],"answers_community":["B (100%)"],"exam_id":21,"topic":"1","answer_description":"","question_id":74,"answer_ET":"B"},{"id":"AIadvPaUTXGHkEUE739o","answers_community":["AC (54%)","AB (38%)","8%"],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/150401-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","topic":"1","question_id":75,"answer_images":[],"answer":"AC","answer_ET":"AC","discussion":[{"comment_id":"1362736","poster":"Ell89","timestamp":"1740692520.0","upvote_count":"1","content":"Selected Answer: AC\n• A leverages the event-driven capability of Glue Crawlers.\n • C uses AWS Lambda for direct and real-time updates to the Data Catalog.\n • This combination ensures incremental updates are made only when changes occur, reducing costs and operational complexity."},{"content":"Selected Answer: AB\n(A) S3 Event-Based Crawler: Automatically triggers incremental catalog updates whenever new data arrives in the S3 bucket, reducing the need for custom code and manual intervention.\n\n(B) Time-Based Schedule: Periodically runs the crawler to catch any missed events and keep the data catalog accurate and up to date.\n\nUsing both methods minimizes operational overhead while ensuring comprehensive and reliable incremental updates.","upvote_count":"1","poster":"YUICH","comment_id":"1348841","timestamp":"1738197420.0"},{"timestamp":"1735224720.0","content":"Selected Answer: AB\nCheck out the design pattern documentation for this case. There's no need for Lambda here, so option C should be excluded. Option B seems viable, along with option A (A is the obvious choice for me).\n\nhttps://aws.amazon.com/blogs/big-data/run-aws-glue-crawlers-using-amazon-s3-event-notifications/","upvote_count":"1","poster":"axantroff","comment_id":"1331957"},{"poster":"michele_scar","content":"Selected Answer: AC\nB and D are wrong due too \"Manually\" and \"Scheduling\".\nE is too much for this use case","timestamp":"1731684720.0","comment_id":"1312692","upvote_count":"3"},{"content":"Selected Answer: AC\n- Option A suggests creating an S3 event-based AWS Glue crawler to consume events from the SQS queue. This option is appropriate as it allows the crawler to automatically respond to events, thereby reducing manual intervention and ensuring timely updates to the Data Catalog\n\n- Option C involves using an AWS Lambda function to directly update the Data Catalog based on S3 events received from the SQS queue. This is a strong candidate as it automates the update process without the need for manual scheduling or intervention, thus minimizing operational overhead. AWS Glue Crawlers can consume events from an SQS queue: https://docs.aws.amazon.com/glue/latest/dg/crawler-s3-event-notifications.html","comment_id":"1308007","timestamp":"1730912040.0","upvote_count":"3","poster":"tucobbad"},{"poster":"pikuantne","comment_id":"1305400","content":"Selected Answer: AB\nBased on this article (Option 1 for the architecture) it should be AB:\n\n1. Run the crawler on a schedule.\n2. Crawler polls for object create events in the SQS queue\n3a. If there are events, crawler updates the Data Catalog\n3b. If not, crawler stops","timestamp":"1730376360.0","upvote_count":"3"},{"comments":[{"upvote_count":"2","timestamp":"1730911920.0","comment_id":"1308006","content":"Answer is A and C\nIn fact, AWS Glue Crawlers can consume events indeed: https://docs.aws.amazon.com/glue/latest/dg/crawler-s3-event-notifications.html","poster":"tucobbad"}],"content":"Selected Answer: BC\nAWS Glue Crawlers can not consupe events from an SQS queue\nD introduce a manual operation\nE introduce more complexity\nso BC","poster":"ae35a02","timestamp":"1730123640.0","comment_id":"1303992","upvote_count":"1"}],"question_images":[],"isMC":true,"choices":{"C":"Use an AWS Lambda function to directly update the Data Catalog based on S3 events that the SQS queue receives.","B":"Define a time-based schedule to run the AWS Glue crawler, and perform incremental updates to the Data Catalog.","A":"Create an S3 event-based AWS Glue crawler to consume events from the SQS queue.","E":"Use AWS Step Functions to orchestrate the process of updating the Data Catalog based on S3 events that the SQS queue receives.","D":"Manually initiate the AWS Glue crawler to perform updates to the Data Catalog when there is a change in the S3 bucket."},"unix_timestamp":1730123640,"timestamp":"2024-10-28 14:54:00","question_text":"A data engineer configured an AWS Glue Data Catalog for data that is stored in Amazon S3 buckets. The data engineer needs to configure the Data Catalog to receive incremental updates.\n\nThe data engineer sets up event notifications for the S3 bucket and creates an Amazon Simple Queue Service (Amazon SQS) queue to receive the S3 events.\n\nWhich combination of steps should the data engineer take to meet these requirements with LEAST operational overhead? (Choose two.)","exam_id":21}],"exam":{"name":"AWS Certified Data Engineer - Associate DEA-C01","provider":"Amazon","isMCOnly":true,"id":21,"isBeta":false,"isImplemented":true,"numberOfQuestions":207,"lastUpdated":"11 Apr 2025"},"currentPage":15},"__N_SSP":true}