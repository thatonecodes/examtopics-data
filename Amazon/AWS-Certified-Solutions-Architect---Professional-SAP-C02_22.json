{"pageProps":{"questions":[{"id":"iF0gPWK7lkwWkwnOkCgG","answer":"B","discussion":[{"comment_id":"1165044","upvote_count":"2","timestamp":"1709495940.0","poster":"a54b16f","content":"Selected Answer: B\nC is missing \"bidirectional S3 Cross-Region Replication\""},{"upvote_count":"3","timestamp":"1700792820.0","content":"Selected Answer: B\nB is always a better option. C is possible but less preferred. \nIrrespective of B or C application will need modification to deploy in 2nd region as Bucket URL has to be change in application.","poster":"career360guru","comment_id":"1078982"},{"upvote_count":"3","comments":[{"content":"just B","timestamp":"1724908980.0","upvote_count":"1","poster":"helloworldabc","comment_id":"1274356"},{"timestamp":"1703846340.0","content":"Option C includes \"Set up S3 Cross-Region Replication (CRR) from the original S3 bucket to the new S3 bucket\". By that the application in the new region will have access to the files from the \"old\" and the new region, and the application running in the \"old\" region only has access to the data of the \"old\" region, as no bidirectional CRR is being set up. That doesn't make a lot of sense. Option B contains bidirectional CRR which keeps both buckets in sync.","poster":"carpa_jo","upvote_count":"3","comment_id":"1108590"}],"timestamp":"1696696620.0","comment_id":"1027541","content":"Selected Answer: C\nAn S3 Multi-Region Access Point is a global endpoint that provides access to data in one or more S3 buckets. To create an S3 Multi-Region Access Point, you must specify a set of S3 buckets that you want to include in the Multi-Region Access Point. You must also configure routing rules to determine which requests are routed to which S3 buckets.\n\nOnce you have created an S3 Multi-Region Access Point, you must modify your application to use the Multi-Region Access Point endpoint instead of the S3 bucket endpoints. This requires changes to your application code and configuration.\n\nOption C does not require the creation of an S3 Multi-Region Access Point. Instead, you can simply deploy the application in two Regions and configure the application to use the S3 bucket endpoints in each Region. This is a simpler and more straightforward approach, which reduces operational overhead.","poster":"Russs99"},{"poster":"MasterP007","upvote_count":"4","timestamp":"1691689620.0","content":"Selected Answer: B\nOption B creates a new S3 bucket in a second Region and sets up bidirectional S3 Cross-Region Replication (CRR) between the original S3 bucket and the new S3 bucket. S3 CRR is a feature that enables automatic, asynchronous copying of objects across S3 buckets in different AWS Regions. You can use S3 CRR to keep your data synchronized across Regions for lower latency, compliance, security, disaster recovery, and regional efficiency.","comment_id":"977967"},{"content":"Selected Answer: B\nThe answer is B","timestamp":"1690688340.0","upvote_count":"1","comment_id":"966822","poster":"azizmo"},{"upvote_count":"1","content":"Selected Answer: B\nit's a B","poster":"nicecurls","timestamp":"1688889240.0","comment_id":"946969"},{"upvote_count":"2","content":"Selected Answer: B\nits a B","poster":"NikkyDicky","comments":[{"timestamp":"1689297780.0","content":"the \"stored in a single Amazon S3 bucket\" comment is confusing though. have to assume new versionn will have buckets in each region","upvote_count":"3","comment_id":"951130","poster":"NikkyDicky"}],"comment_id":"944074","timestamp":"1688585160.0"},{"poster":"phattran","upvote_count":"3","comment_id":"943833","timestamp":"1688566980.0","content":"Selected Answer: B\nS3 CRR prefer S3 Multi-Region Access Point"},{"content":"B sounds right for deploying in 2 different regions though.","upvote_count":"1","poster":"YodaMaster","comment_id":"943808","timestamp":"1688565480.0"},{"timestamp":"1688565360.0","content":"this question seems incomplete?","poster":"YodaMaster","upvote_count":"1","comment_id":"943806"},{"timestamp":"1685156700.0","upvote_count":"3","comment_id":"907708","content":"B, enable the S3 sync","poster":"Masonyeoh"},{"poster":"Roontha","content":"Answer : B\nhttps://aws.amazon.com/s3/features/multi-region-access-points/","comment_id":"907678","timestamp":"1685149560.0","upvote_count":"2"}],"isMC":true,"timestamp":"2023-05-27 03:06:00","answer_ET":"B","answer_images":[],"choices":{"D":"Set up an S3 gateway endpoint with the S3 bucket as an origin. Deploy the application to a second Region. Modify the application to use the new S3 gateway endpoint. Use S3 Intelligent-Tiering on the S3 bucket.","B":"Create a new S3 bucket in a second Region. Set up bidirectional S3 Cross-Region Replication (CRR) between the original S3 bucket and the new S3 bucket. Configure an S3 Multi-Region Access Point that uses both S3 buckets. Deploy a modified application to both Regions.","A":"Set up an Amazon CloudFront distribution with the S3 bucket as an origin. Deploy the application to a second Region Modify the application to use the CloudFront distribution. Use AWS Global Accelerator to access the data in the S3 bucket.","C":"Create a new S3 bucket in a second Region Deploy the application in the second Region. Configure the application to use the new S3 bucket. Set up S3 Cross-Region Replication (CRR) from the original S3 bucket to the new S3 bucket."},"answers_community":["B (84%)","C (16%)"],"exam_id":33,"url":"https://www.examtopics.com/discussions/amazon/view/110338-exam-aws-certified-solutions-architect-professional-sap-c02/","question_text":"A company hosts an application on AWS. The application reads and writes objects that are stored in a single Amazon S3 bucket. The company must modify the application to deploy the application in two AWS Regions.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","unix_timestamp":1685149560,"topic":"1","question_id":106,"answer_description":"","question_images":[]},{"id":"uXl3TCOEO5bpIoufmJTs","topic":"1","isMC":true,"timestamp":"2023-05-27 03:08:00","question_text":"An online gaming company needs to rehost its gaming platform on AWS. The company's gaming application requires high performance computing (HPC) processing and has a leaderboard that changes frequently. An Ubuntu instance that is optimized for compute generation hosts a Node.js application for game display. Game state is tracked in an on-premises Redis instance.\n\nThe company needs a migration strategy that optimizes application performance.\n\nWhich solution will meet these requirements?","unix_timestamp":1685149680,"discussion":[{"timestamp":"1701054480.0","poster":"Roontha","upvote_count":"12","comment_id":"907679","content":"Answer : C\n\nhttps://aws.amazon.com/blogs/database/building-a-real-time-gaming-leaderboard-with-amazon-elasticache-for-redis/"},{"content":"Selected Answer: C\nElastic Cache for Redis, C or D.\nBoth are on demand, we cant use spot\nTie breaker is the instance type c5.","upvote_count":"5","timestamp":"1701312420.0","comment_id":"910677","poster":"rbm2023"},{"poster":"Win007","content":"D is the write answer","comment_id":"1224643","timestamp":"1733399160.0","upvote_count":"1"},{"poster":"voccer","upvote_count":"1","content":"Answer: C\nB/c: not use spot instance","comment_id":"1123472","timestamp":"1721048700.0"},{"poster":"career360guru","timestamp":"1716517080.0","upvote_count":"1","comment_id":"1079021","content":"Selected Answer: C\nOption C"},{"comment_id":"1006335","upvote_count":"1","poster":"dkcloudguru","timestamp":"1710322380.0","content":"Agree with option C"},{"poster":"SK_Tyagi","content":"Selected Answer: C\nAgree with C.","comment_id":"985980","upvote_count":"1","timestamp":"1708461120.0"},{"content":"Correct C.","timestamp":"1706130420.0","poster":"ggrodskiy","comment_id":"961951","upvote_count":"1"},{"upvote_count":"1","timestamp":"1704490020.0","poster":"NikkyDicky","comment_id":"944075","content":"Selected Answer: C\nC for sure"},{"poster":"YodaMaster","content":"Selected Answer: C\nC is the way","comment_id":"943810","timestamp":"1704470400.0","upvote_count":"1"},{"upvote_count":"1","poster":"Alabi","comment_id":"931106","timestamp":"1703293320.0","content":"Selected Answer: C\nC for sure"},{"poster":"F_Eldin","timestamp":"1701354480.0","comment_id":"910230","content":"Selected Answer: C\nA, B : Wrong. Spot instances. B: OpeSearch instead of Redis\nD: Wrong, DynamoDB instead of Redis","upvote_count":"2"},{"upvote_count":"2","poster":"andreitugui","content":"Selected Answer: C\nThe answer is C as compute optimized instance is required c5, and ElastiCache is the for Redis.","timestamp":"1701325860.0","comment_id":"909880"},{"content":"Agree with C","timestamp":"1701061560.0","upvote_count":"2","poster":"Masonyeoh","comment_id":"907709"}],"exam_id":33,"url":"https://www.examtopics.com/discussions/amazon/view/110339-exam-aws-certified-solutions-architect-professional-sap-c02/","answers_community":["C (100%)"],"answer_images":[],"choices":{"C":"Create an Auto Scaling group of c5.large Amazon EC2 On-Demand Instances behind an Application Load Balancer. Use an Amazon ElastiCache for Redis cluster to maintain the leaderboard.","D":"Create an Auto Scaling group of m5.large Amazon EC2 On-Demand Instances behind an Application Load Balancer. Use an Amazon DynamoDB table to maintain the leaderboard.","A":"Create an Auto Scaling group of m5.large Amazon EC2 Spot Instances behind an Application Load Balancer. Use an Amazon ElastlCache for Redis cluster to maintain the leaderboard.","B":"Create an Auto Scaling group of c5.large Amazon EC2 Spot Instances behind an Application Load Balancer. Use an Amazon OpenSearch Service cluster to maintain the leaderboard."},"answer_description":"","answer":"C","question_id":107,"question_images":[],"answer_ET":"C"},{"id":"PUvwXcOcq0tZF7pcax8U","exam_id":33,"timestamp":"2023-05-27 05:16:00","answer_description":"","topic":"1","answers_community":["CE (45%)","BE (44%)","7%"],"answer_ET":"CE","choices":{"E":"Store the timesheet submission data in Amazon S3. Use Amazon Athena and Amazon QuickSight to generate the reports using Amazon S3 as the data source.","A":"Deploy the application to Amazon EC2 On-Demand Instances with load balancing across multiple Availability Zones. Use scheduled Amazon EC2 Auto Scaling to add capacity before the high volume of submissions on Fridays.","B":"Deploy the application in a container using Amazon Elastic Container Service (Amazon ECS) with load balancing across multiple Availability Zones. Use scheduled Service\nAuto Scaling to add capacity before the high volume of submissions on Fridays.","D":"Store the timesheet submission data in Amazon Redshift. Use Amazon QuickSight to generate the reports using Amazon Redshift as the data source.","C":"Deploy the application front end to an Amazon S3 bucket served by Amazon CloudFront. Deploy the application backend using Amazon API Gateway with an AWS Lambda proxy integration."},"unix_timestamp":1685157360,"answer":"CE","question_images":[],"discussion":[{"comments":[{"upvote_count":"1","timestamp":"1737510660.0","poster":"altonh","comment_id":"1344543","content":"C implies that the front end is static while the back end is dynamic."},{"timestamp":"1692559980.0","comment_id":"986033","upvote_count":"1","content":"It looks like BE are the best options. While deploying the frontend to S3 and using API Gateway with Lambda for the backend is a good architectural approach, it might not directly address the requirement for load scaling and scheduling.","poster":"Gmail78"}],"comment_id":"928404","content":"Selected Answer: BE\ni'm going with BE.\nA not correct with EC2 instances to mantain.\nC is not correct because we cannot host webapplication on S3 (only static contents)\nD too much effort for Redshift","timestamp":"1687260960.0","upvote_count":"17","poster":"emiliocb4"},{"content":"Selected Answer: CE\nA. EC2 on-demand instances don't make sense to accept timesheet entries\nB. ECS can be done but they want to minimise operational overhead where option C sounds better/simple\nC. Sounds simple enough to use s3. I choose this.\nD. I already chose s3 so this doesn't apply + redshift seems overkill\nE.This goes with Option C\nSo answer C and E","upvote_count":"11","timestamp":"1688566140.0","comment_id":"943821","poster":"YodaMaster"},{"comment_id":"1335361","timestamp":"1735793760.0","content":"Selected Answer: CE\nC is better option than B bcz it is managed/serverless than ECS (without mentioning Fargate)","upvote_count":"1","poster":"JaffaDaffa"},{"timestamp":"1735793700.0","content":"Selected Answer: CE\nB is not right option as ECS management overhead unless specified with Fargate Launch type.","poster":"JaffaDaffa","upvote_count":"1","comment_id":"1335360"},{"comment_id":"1324546","poster":"deepakR20","content":"Selected Answer: BE\nKey parameter is \" with most of the submissions occurring on Friday.\" hence BE is the right answer","upvote_count":"1","timestamp":"1733836140.0"},{"content":"Selected Answer: BE\nE is apparently.\nI would go for B rather than C. Even serverless lambda is best suited for minimizing operational overhead; however the point is \"mobile devices\"; the mobile application is the frontend => \"Deploy the application front end to an Amazon S3 bucket served by Amazon CloudFront\" from C does not make sense.","timestamp":"1733108460.0","upvote_count":"1","poster":"LuongTo","comment_id":"1320766"},{"poster":"youonebe","comment_id":"1317493","timestamp":"1732540620.0","content":"Selected Answer: CE\nCE-CE-CE","upvote_count":"1"},{"upvote_count":"1","timestamp":"1731411540.0","comment_id":"1310547","content":"Selected Answer: BE\ni'm going with BE.","poster":"zersa"},{"comments":[{"timestamp":"1733107500.0","poster":"LuongTo","comment_id":"1320761","upvote_count":"2","content":"C feasible. Frontend (html, js) will be in S3 with Cloudfront, lambda for backend. lambda is the best approach for \"minimizing operational overhead\". The \"requirement\" here is about high-availability not DR which requires multi-region"}],"comment_id":"1310167","upvote_count":"1","content":"Selected Answer: BE\nHigh availability = multiple availability zones (clue in the answer, B)\nCannot be C as it's a web application so cannot be on S3. Lambda not suitable either because Lambdas only run in a single chosen region of deployment.","poster":"Halliphax","timestamp":"1731338400.0"},{"upvote_count":"1","poster":"0b43291","content":"Selected Answer: CE\nOption A (EC2 On-Demand Instances with Auto Scaling) requires managing and scaling EC2 instances, which adds operational overhead compared to a serverless approach.\n Option B (Amazon ECS with Service Auto Scaling) also requires managing and scaling container instances, which adds operational overhead compared to a serverless approach.\n Option D (Amazon Redshift) is a data warehousing solution better suited for large-scale analytics workloads, which may be overkill for the given requirements and introduce unnecessary complexity and cost.\n\nBy choosing the combination of options C and E, the solutions architect can implement a highly available, scalable, and cost-effective solution with minimal operational overhead, leveraging the benefits of serverless computing, object storage, and managed analytics services.","comment_id":"1308922","timestamp":"1731098520.0"},{"content":"Selected Answer: CE\nOptions A and B involve managing EC2 instances or containers, which would require more operational effort than a fully serverless solution with C and E.","upvote_count":"1","timestamp":"1729353120.0","poster":"sashenka","comment_id":"1300066"},{"comment_id":"1295227","timestamp":"1728492660.0","content":"Selected Answer: BE\nIt's B & E, the question says that timeshares will need ro be submitted. Therefore making it dynamic. \n\nSelecting option C to use S3 to host webapp can't work because S3 can host static sites.","poster":"AWSum1","upvote_count":"1"},{"comment_id":"1269591","timestamp":"1724169900.0","poster":"Syre","content":"Selected Answer: BE\nC is Incorrect","upvote_count":"1"},{"poster":"kpcert","upvote_count":"2","content":"Selected Answer: BE\nVoting for BE","comment_id":"1229300","timestamp":"1718208540.0"},{"content":"Selected Answer: BE\nC is not correct. We need Lambda, not Lambda Proxy. BTW, APIGW + Lambda for the unknown load, in this case we knew that high load on Friday","upvote_count":"2","timestamp":"1718048820.0","comment_id":"1228079","poster":"trungtd"},{"content":"Selected Answer: BE\nAnswer is B and E.\nWe already know which on friday a lot of people will submit timesheet, so scheduled autoscaling is perfect.\nFinally S3 + Athena are enough. No need of Redshift database to run report.","timestamp":"1716272820.0","poster":"red_panda","upvote_count":"4","comment_id":"1214746"},{"poster":"mifune","content":"Selected Answer: CE\nBetween A or B I don't see any difference regarding minimizing the operational overhead. The right answer can be either. That leads me to think that the right answer is C (indeed, it's full serverless!)\n\nC, E","comment_id":"1213630","timestamp":"1716100020.0","upvote_count":"1"},{"content":"c is simple but not scalable one\ni feel C and B are almost same operation overhead. \ni will go for BE","timestamp":"1714452840.0","upvote_count":"2","poster":"43c89f4","comment_id":"1204334"},{"comment_id":"1178534","poster":"Russs99","content":"Selected Answer: BE\nOption C is wrong. The requirement states that the data must be stored in a format that allows payroll administrators to run monthly reports.\nAmazon S3 and Amazon API Gateway do not inherently provide the necessary data storage and querying capabilities for generating reports.","timestamp":"1710954780.0","upvote_count":"2"},{"comment_id":"1165047","upvote_count":"1","timestamp":"1709496180.0","poster":"a54b16f","content":"Selected Answer: CE\nMinimal admin effort"},{"poster":"career360guru","content":"Selected Answer: CE\nC & E is most operationally efficient. Redshift cluster needs more operational effort to manage.","comment_id":"1079026","upvote_count":"2","timestamp":"1700800440.0"},{"content":"Why do I feel C somehow tricky because it says deploy backend using APIGW with Lamda proxy integration, and doesnt mention a Lambda function to process data? \"Lamda proxy integration\" only means an option to tick in configuration of APIGW, no?","timestamp":"1698826020.0","upvote_count":"2","comments":[{"comment_id":"1306446","timestamp":"1730618460.0","content":"Lambda Proxy Integration is often the go-to choice for quickly developing flexible, serverless APIs, reducing the configuration overhead while maximizing control over request and response handling.\n\nFor cases where requests need to be modified at the API Gateway level before reaching Lambda, a non-proxy integration might be more appropriate.","upvote_count":"1","poster":"Daniel76"},{"poster":"kejam","upvote_count":"2","content":"Agreed. C is a misdirect. You don't need Lambda Proxy. APIGW can integrate with S3 API directly.\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/integrating-api-with-aws-services-s3.html","timestamp":"1704858300.0","comment_id":"1118117"}],"comment_id":"1059446","poster":"joleneinthebackyard"},{"comment_id":"997313","content":"C & E. \"minimizing operational overhead\" is the deciding factor between C and B. operating and managing ECS and ALB would be more cumbersome versus a more serverless approach like APIGW, Lambda, and S3.","poster":"CloudHandsOn","upvote_count":"1","timestamp":"1693716480.0"},{"poster":"SK_Tyagi","content":"Selected Answer: CE\nLeast Operational overhead","comment_id":"985990","timestamp":"1692556740.0","upvote_count":"3"},{"comment_id":"965794","poster":"easytoo","content":"b-e-b-e-b-e-b-e","timestamp":"1690575180.0","upvote_count":"2"},{"comment_id":"961932","timestamp":"1690224360.0","upvote_count":"1","content":"Correct CE.","poster":"ggrodskiy"},{"poster":"NikkyDicky","upvote_count":"4","comment_id":"944077","timestamp":"1688585460.0","content":"Selected Answer: CE\nCE. front-end app like angular can be hosted inn s3 and CF"},{"timestamp":"1688566380.0","upvote_count":"5","content":"oh and this from AWS which seems familiar to the question. https://aws.amazon.com/blogs/architecture/create-dynamic-contact-forms-for-s3-static-websites-using-aws-lambda-amazon-api-gateway-and-amazon-ses/","comment_id":"943826","poster":"YodaMaster"},{"upvote_count":"4","comment_id":"929965","timestamp":"1687383960.0","content":"Selected Answer: BE\nOlabiba.ai said:\nOption B suggests deploying the application in a container using Amazon ECS with load balancing across multiple Availability Zones. This ensures high availability and scalability by distributing the workload across multiple instances and zones. Using scheduled Service Auto Scaling allows for adding capacity before the high volume of submissions on Fridays, ensuring the application can handle the increased load.\n\nOption E suggests storing the timesheet submission data in Amazon S3, which provides a highly durable and scalable storage solution. Amazon Athena can be used to query the data directly from S3, and Amazon QuickSight can be used to generate the monthly reports using S3 as the data source. This combination allows for efficient data storage and reporting without the need for additional infrastructure or operational overhead.\n\nBy implementing these steps, you can achieve a highly available and scalable infrastructure while minimizing operational overhead.","poster":"Jackhemo"},{"comment_id":"914769","upvote_count":"2","content":"Selected Answer: CE\nC and E will require least operational overhead.","timestamp":"1685891820.0","poster":"hitesh24"},{"comment_id":"913764","timestamp":"1685807460.0","upvote_count":"3","content":"Selected Answer: AE\nI prefer A to C, as I didn't see why Cloudfront is necesary in C.\nthe mainstream is from mobile to AWS environment while cloudfront is used to cache files for a user to the nearest edge location. The question emphasize the Friday burst, but C doesn't address this scenario purposely. I think A is better than C.","poster":"Jesuisleon"},{"upvote_count":"3","poster":"Darkhorse_79","comments":[{"content":"May be there are consistent verifications there for example job number check, number of projects check or completion check( some fields must be filled), when all checks passed, the forms can be saved in S3.","timestamp":"1685807700.0","comment_id":"913768","upvote_count":"1","poster":"Jesuisleon"}],"content":"Selected Answer: CE\nSubmitting timesheets is likely a pretty static site setup,, javascript based, why would you deploy a full EC2 or ECS platform when you can host it on S3 easily","timestamp":"1685737140.0","comment_id":"913036"},{"timestamp":"1685718060.0","upvote_count":"2","content":"Selected Answer: CE\nminimizing operational overhead, then No EC2, NO ECS. a lot of operational work to maintain it.","comment_id":"912900","poster":"nexus2020"},{"timestamp":"1685421720.0","poster":"andreitugui","content":"Selected Answer: CE\nC. By deploying the application front end to an Amazon S3 bucket served by Amazon CloudFront, you can benefit from the scalability, high availability, and low latency of the S3 and CloudFront services. This combination allows for efficient content delivery and a smooth user experience on mobile devices.\n\nUsing Amazon API Gateway with an AWS Lambda proxy integration for the backend enables serverless execution and eliminates the need for managing and scaling infrastructure. It provides an efficient way to handle the timesheet submissions and i","upvote_count":"4","comment_id":"909888"},{"content":"Selected Answer: AE\nAnswer should be A E.\nB (Wrong): Why I need to auto scale EC2 instances while scaling is managed by ECS.\nD (Wrong): Why I need to run a complete Redshift environment to generate a report. Athena can do the job.","timestamp":"1685298180.0","comment_id":"908807","comments":[{"content":"CloudFront is not necessary in this scenario. I didn't see the necessarity to cache forms in Cloudfront.","timestamp":"1685807820.0","comments":[{"upvote_count":"1","content":"not forms but reports","comment_id":"962769","timestamp":"1690292100.0","poster":"rxhan"}],"comment_id":"913771","upvote_count":"1","poster":"Jesuisleon"}],"upvote_count":"3","poster":"AMEJack"},{"upvote_count":"1","comment_id":"908532","content":"Selected Answer: AD\nI am thinking AD \nA is less operation overhead than B. as B is not using Fargate deployment, so you need deploy EC2 anyway. and they you need manage ECS on top EC2. more layers and more operation overhead. \nD: we should use redshift for report, not from S3 bucket","timestamp":"1685269680.0","poster":"ShinLi"},{"poster":"Nash101","content":"Answer A & E","upvote_count":"1","timestamp":"1685266260.0","comment_id":"908467"},{"comments":[{"upvote_count":"2","timestamp":"1685887320.0","comment_id":"914712","poster":"Roontha","content":"changed my answer to C, E after thoroughly reviewing involved aws services and their use cases along with following blog post\n\nB is not correct. Answers are C,E\n\nC is very less operational overhead...fully serverless, also use case is simple timesheet application. Option B (it could come either serverless (fargate) or server (EC2), but you have to have maintain cluster)\n\nhttps://medium.com/@lakshmanLD/lambda-proxy-vs-lambda-integration-in-aws-api-gateway-3a9397af0e6d"}],"content":"Answer : B, E","comment_id":"908099","upvote_count":"1","timestamp":"1685204160.0","poster":"Roontha"},{"timestamp":"1685157360.0","comment_id":"907721","upvote_count":"3","comments":[{"content":"agree with BD, but i don't know why A is wrong. I know B is better and cheaper, but A is having less operation overhead, right?","comment_id":"908527","poster":"ShinLi","timestamp":"1685269560.0","upvote_count":"1","comments":[{"content":"B is not correct. Answers are C,E\n\nC is very less operational overhead...fully serverless, also use case is simple timesheet application. Option B (it could come either serverless (fargate) or server (EC2), but you have to have maintain cluster)\n\nhttps://medium.com/@lakshmanLD/lambda-proxy-vs-lambda-integration-in-aws-api-gateway-3a9397af0e6d","poster":"Roontha","timestamp":"1685887260.0","upvote_count":"1","comment_id":"914709"}]},{"poster":"AMEJack","comment_id":"908808","timestamp":"1685298360.0","upvote_count":"1","content":"The question is saying \"minimizing operational overhead\", Redshift is to minimize the operation overhead?! It should be Athena."}],"content":"Selected Answer: BD\nhttps://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html","poster":"Masonyeoh"}],"url":"https://www.examtopics.com/discussions/amazon/view/110346-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_images":[],"isMC":true,"question_id":108,"question_text":"A solutions architect is designing an application to accept timesheet entries from employees on their mobile devices. Timesheets will be submitted weekly, with most of the submissions occurring on Friday. The data must be stored in a format that allows payroll administrators to run monthly reports. The infrastructure must be highly available and scale to match the rate of incoming data and reporting requests.\n\nWhich combination of steps meets these requirements while minimizing operational overhead? (Choose two.)"},{"id":"wvPZzBlyYGDFCgidY4TE","exam_id":33,"question_id":109,"answer":"ADF","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/110347-exam-aws-certified-solutions-architect-professional-sap-c02/","question_images":[],"answer_ET":"ADF","unix_timestamp":1685157600,"choices":{"D":"Configure Amazon S3 to send object deletion events to an Amazon EventBridge event bus that publishes to an Amazon Simple Notification Service (Amazon SNS) topic.","A":"Configure AWS CloudTrail to log S3 data events.","B":"Configure S3 server access logging for the S3 bucket.","F":"Configure a new S3 bucket to store the logs with an S3 Lifecycle policy.","C":"Configure Amazon S3 to send object deletion events to Amazon Simple Email Service (Amazon SES).","E":"Configure Amazon S3 to send the logs to Amazon Timestream with data storage tiering."},"isMC":true,"timestamp":"2023-05-27 05:20:00","discussion":[{"comment_id":"997925","upvote_count":"12","timestamp":"1693770660.0","poster":"cmoreira","content":"Selected Answer: ADF\nADF\nA or B work, but docs recomment cloud trail:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html"},{"comment_id":"1092194","upvote_count":"7","timestamp":"1702167960.0","poster":"kaby1987","content":"Selected Answer: ADF\nADF are correct choices."},{"comments":[{"poster":"altonh","comment_id":"1361398","content":"The requirement is for the most cost-effective.\nA - You will pay for the data event delivered to S3\nC - No integration to SES\nE - Paying more for Timestream","timestamp":"1740474900.0","upvote_count":"1"}],"content":"Selected Answer: BDF\nThe requirement is for the most cost-effective.\nA - You will pay for the data event delivered to S3\nC - No integration to SES\nD - Paying more for Timestream","poster":"altonh","comment_id":"1361397","upvote_count":"1","timestamp":"1740474900.0"},{"comment_id":"1355761","content":"Selected Answer: ADF\nThe conflict is between AWS Cloudtrail (A) and S3 Server Access Log (B). \nB is incorrect because S3 Server access logs track requests at the bucket level, not object-level operations (e.g., deletions). \n\nA is correct because CloudTrail is required for detailed tracking including object level..","timestamp":"1739390400.0","poster":"820b83f","upvote_count":"1"},{"content":"Selected Answer: BDF\nBDF flows well. ADF, however, does not provide details on how you will store the logs in the new S3 bucket.","timestamp":"1737511020.0","upvote_count":"1","poster":"altonh","comment_id":"1344548"},{"comment_id":"1281805","content":"\"We recommend that you use CloudTrail for logging bucket-level and object-level actions for your Amazon S3 resources.\"","timestamp":"1726008120.0","upvote_count":"3","poster":"HSong"},{"content":"Selected Answer: ADE\nADFis right answer.","upvote_count":"1","timestamp":"1718979060.0","comment_id":"1234453","poster":"dragongoseki"},{"poster":"Helpnosense","content":"Selected Answer: BDF\nBoth A and B can log s3 activities. Difference is A real time log but cost more. B log has delay but cheaper. The requirement is the most cost-effective so choose B to meet this requirement.","timestamp":"1718930160.0","upvote_count":"4","comment_id":"1234133"},{"content":"Selected Answer: ADF\nADF logs everything, BDF doesnt.","poster":"seetpt","timestamp":"1714666920.0","comment_id":"1205706","upvote_count":"3"},{"timestamp":"1712001000.0","upvote_count":"3","poster":"titi_r","comment_id":"1187642","content":"Selected Answer: BDF\nBDF meet the requirements."},{"comment_id":"1168218","upvote_count":"5","timestamp":"1709832240.0","content":"Selected Answer: ADF\nProbably B is cheaper but A is safer and more accurate and remember the \"The company must log ALL activities for objects\"\n\nAccording to this https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerLogs.html#LogDeliveryBestEffort\n\n\"The log record for a particular request might be delivered long after the request was actually processed, or it might not be delivered at all. \"\n\nso for me is A not B","poster":"liquen14"},{"poster":"Russs99","comment_id":"1151609","upvote_count":"4","timestamp":"1708038480.0","content":"Selected Answer: BDF\nGiven the requirement to log all activities for objects in an S3 bucket and keep logs for 5 years, combined with a focus on cost-effectiveness, S3 server access logging (Option B) would indeed be a cheaper solution for capturing basic access logs. However, for advanced auditing and compliance requirements where detailed API call tracking is needed, CloudTrail's data event logging provides valuable insights that S3 access logs do not."},{"timestamp":"1706458860.0","content":"Selected Answer: BDF\nB is cheaper than A\nAWS CloudTrail (A) - Management events (first delivery) are free; data events incur a fee, in addition to storage of logs\nS3 Server Logs (B) - No other cost in addition to storage of logs\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html#:~:text=S3%20Server%20Logs-,Price,-Management%20events%20(first","comment_id":"1134266","upvote_count":"1","poster":"ninomfr64"},{"comment_id":"1132904","content":"Selected Answer: ADF\nFor capturing object-level events, such as object deletions, you would typically use Amazon S3 Event Notifications or enable AWS CloudTrail data events for S3.","timestamp":"1706303880.0","poster":"gagol14","upvote_count":"4"},{"poster":"Jane1234YIP","comment_id":"1121290","content":"S3 server access logging does not capture object-level events like object deletions. so I will go ADF.","comments":[{"timestamp":"1705154220.0","content":"wrong. check \"operation\" in https://docs.aws.amazon.com/AmazonS3/latest/userguide/LogFormat.html\nBDF","upvote_count":"5","comment_id":"1121699","poster":"cox1960"}],"timestamp":"1705119060.0","upvote_count":"3"},{"poster":"adelynllllllllll","timestamp":"1704030900.0","comment_id":"1110608","content":"BDF\nBecause it asked for cost-effective.","upvote_count":"1"},{"comment_id":"1104624","content":"Selected Answer: BDF\nB is better than A because S3 server logs -- > Cost efficient and get more log information (Lifecycle, Authentication info)\nLink: https://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html","timestamp":"1703425140.0","upvote_count":"2","poster":"mosalahs"},{"upvote_count":"1","content":"Selected Answer: BDF\nMy Choice","poster":"tuh22","comment_id":"1100901","timestamp":"1703010960.0"},{"timestamp":"1700879460.0","upvote_count":"3","content":"ADF are correct choices.\n\nUsing server access logging provides basic access logs for requests made to the S3 bucket, but it is not as comprehensive for auditing purposes as CloudTrail and can result in a large volume of data, increasing costs.","comments":[{"content":"S3 Server Access log is cheaper as you only pay for the storage of logs, while CloudTrail Data Event incur into additional cost + storage of logs.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html\n\nS3 Server Access log - You can use server access logs for the following purposes:\nPerforming security and access audits\nLearning about your customer base\nUnderstanding your Amazon S3 bill\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/LogFormat.html","timestamp":"1706458980.0","upvote_count":"1","comment_id":"1134267","poster":"ninomfr64"}],"comment_id":"1079685","poster":"heatblur"},{"poster":"career360guru","comment_id":"1079030","upvote_count":"3","timestamp":"1700800740.0","content":"Selected Answer: BDF\nB D F are the right options"},{"poster":"BECAUSE","content":"Selected Answer: BDF\nB,D,F is more cost effective","comment_id":"1074866","upvote_count":"2","timestamp":"1700419860.0"},{"comment_id":"1073819","content":"Selected Answer: BDF\ns3 access logs are more cost-effective","poster":"severlight","upvote_count":"2","timestamp":"1700287980.0"},{"comment_id":"1073634","content":"Selected Answer: ADF\nADF\nBoth A and B could work, but cloudtrail is much more precise than S3 logs access.","upvote_count":"5","timestamp":"1700254860.0","poster":"Mikado211"},{"poster":"richguo","upvote_count":"2","timestamp":"1699060680.0","content":"Selected Answer: BDF\nB - Enabling Amazon S3 server access logging\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-server-access-logging.html\n\nD - Tutorial: Send a notification when an Amazon S3 object is created\nhttps://docs.aws.amazon.com/eventbridge/latest/userguide/eb-s3-object-created-tutorial.html\n\nF - without doubt","comment_id":"1061856"},{"content":"Why do people talk about Server access logs cannot send to Eventbridge while there is no requirement for that? The access log only need to store in S3. \"Delete event to be send to EventBridge\" is another configuration!","timestamp":"1698823740.0","comment_id":"1059428","poster":"joleneinthebackyard","upvote_count":"1"},{"content":"Selected Answer: ADF\nS3 Server ACcess log can send logs in a particular format to another S3. It cannot send to Eventbridge. Also documentation mention using cloudtrail as the recommended solution.","comment_id":"1058915","poster":"Sab","upvote_count":"3","timestamp":"1698762720.0"},{"poster":"KungLjao","comment_id":"1058829","content":"Selected Answer: ADF\nCloudtrail is recommended over s3 server access logs","upvote_count":"3","timestamp":"1698759360.0"},{"poster":"KCjoe","comment_id":"1054819","content":"Selected Answer: ADF\nCannot be B, because access log can only send to S3 bucket.","upvote_count":"2","timestamp":"1698343620.0"},{"upvote_count":"1","content":"Selected Answer: BDF\nB,D,F, s3 server access logs are sufficient enough unless we have any specific scenarios to go for Cloudtrail","comment_id":"1027490","timestamp":"1696692780.0","poster":"Mahakali"},{"upvote_count":"2","content":"A, D, F\n\nYou can record the actions that are taken by users, roles, or AWS services on Amazon S3 resources and maintain log records for auditing and compliance purposes. To do this, you can use server-access logging, AWS CloudTrail logging, or a combination of both. We recommend that you use CloudTrail for logging bucket-level and object-level actions for your Amazon S3 resources.\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html","timestamp":"1693534500.0","poster":"study_aws1","comment_id":"995601"},{"poster":"Soweetadad","upvote_count":"6","comment_id":"993662","timestamp":"1693363080.0","comments":[{"content":"https://repost.aws/knowledge-center/s3-audit-deleted-missing-objects\nagree with ADF","poster":"vn_thanhtung","upvote_count":"1","timestamp":"1693556760.0","comment_id":"995852"}],"content":"Selected Answer: ADF\nADF. Please read this \"We recommend that you use CloudTrail for logging bucket-level and object-level actions for your Amazon S3 resources.\"\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html"},{"upvote_count":"2","timestamp":"1692557160.0","comment_id":"985993","content":"Selected Answer: BDF\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/cloudtrail-logging.html\nAWS CloudTrail logs provide a record of actions taken by a user, role, or an AWS service in Amazon S3, while Amazon S3 server access logs provide detailed records for the requests that are made to an S3 bucket.","poster":"SK_Tyagi"},{"timestamp":"1692555600.0","poster":"xav1er","comment_id":"985972","content":"Selected Answer: BDF\nI would go with BDF, B bacause server access logging can be sufficient for logging all activities on objects (not bucket itself) and is more cost-effective than cloud-trail. Close one tho :)","upvote_count":"4"},{"content":"Selected Answer: BDF\nQuestion says \"The company must log all activities for objects in the S3 bucket.\" Special attention to \"the S3 bucket.\" Now, add to the requirement \"MOST cost-effectively.\"\n\nThis only should make you choose B instead of A. But we can go a bit further if we look here \"The company must log all activities\". You can check the documentation, but Olabiba can save you time with this question:\n\n - Me: What S3 activities are logged using CloudTrail?\n - Olabiba: By default, CloudTrail logs S3 bucket-level API calls that were made in the last 90 days. These bucket-level calls include events such as CreateBucket, DeleteBucket, PutBucketLifecycle, PutBucketPolicy, and more. \nHowever, CloudTrail does not log requests made to objects within the S3 bucket. If you need more detailed logging for object-level activities, you can enable S3 server access logging.","poster":"chico2023","upvote_count":"3","comment_id":"984198","timestamp":"1692336480.0"},{"upvote_count":"2","poster":"easytoo","comment_id":"965797","content":"b-d-f----b-d-f-----b-d-f","timestamp":"1690575600.0"},{"timestamp":"1690224180.0","poster":"ggrodskiy","comment_id":"961929","content":"Correct ADF.","upvote_count":"1"},{"comment_id":"961765","poster":"totopopo","content":"Selected Answer: BDF\nA and B have both the capability to log events for objects. But B is definitely more cost effective and sticks better with other options (like A would miss something like \"and send trails to S3\").","timestamp":"1690213020.0","upvote_count":"3"},{"poster":"lxrdm","comment_id":"954851","timestamp":"1689649320.0","upvote_count":"2","content":"Selected Answer: BDF\nTrack all object activities is to use S3 access logs and store the log into another bucket"},{"content":"Selected Answer: ADF\nkey words:\nlog all data events-> cloudtrail\nsend email for \"delete events\"-> event bridge rule-> ses\nretain for 5 yrs-> lifecycle","upvote_count":"4","timestamp":"1688767500.0","poster":"Christina666","comment_id":"946022"},{"timestamp":"1688585700.0","comment_id":"944082","poster":"NikkyDicky","upvote_count":"2","content":"Selected Answer: ADF\nIts ADF"},{"content":"Selected Answer: BDF\nB makes more sense than A (for MOST cost-effective). They both track object deletions.\nBy default, CloudTrail records bucket-level events. To get logs for object-level operations like GetObject, DeleteObject, and PutObject, you must configure object-level logging. Object-level logging incurs additional charges, so be sure to review the pricing for CloudTrail data events.\nhttps://repost.aws/knowledge-center/s3-audit-deleted-missing-objects","timestamp":"1688567520.0","upvote_count":"2","poster":"YodaMaster","comment_id":"943842"},{"upvote_count":"4","content":"Selected Answer: ADF\nB : Wrong. Server logging has limited features compared to cloudtrail\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html\nCloudTrail data events can be set for all the S3 buckets for the AWS account or just for some folder in S3 bucket. Whereas, S3 server access logs would be set at individual bucket level\n\nC: Wrong. You need Lambda to connect events to SES . SNS and eventbridge\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-how-to-event-types-and-destinations.html#supported-notification-event-types\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/EventBridge.html\nhttps://medium.com/@anirsom2012/send-email-notification-on-aws-s3-events-186acd25a401\n\nE: Wrong, Timestreams is an overkill.","poster":"SkyZeroZx","timestamp":"1688335800.0","comment_id":"941270"},{"comment_id":"941109","upvote_count":"1","content":"Both AWS CloudTrail and Amazon S3 Server Logs capture Object operations and Bucket operations, however, for CloudTrail, data events incur a fee, in addition to storage of logs. For Amazon S3 Server Logs, no other cost in addition to storage of logs. \nThus, given this question asks how to solve MOST cost-effectively, we must conclude B, not A.\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html","timestamp":"1688318040.0","poster":"dkx"},{"timestamp":"1687973340.0","content":"Selected Answer: ADF\nIt must be A and not B. With B will send logs about object access. But in order to send notifications to evenbridge the delete event needs to be published in Cloudtrail, so option A is needed","poster":"javitech83","comment_id":"936957","upvote_count":"3"},{"timestamp":"1687599000.0","poster":"Maria2023","upvote_count":"2","comments":[{"poster":"Maria2023","upvote_count":"2","comment_id":"932401","timestamp":"1687599060.0","content":"I've accidentally triggered wrong choices - it's BDF"}],"content":"Selected Answer: ADF\nAwful wording here. You do not actually set up S3 to send events - you create a rule in EventBridge that responds to a trigger that is \"detail-type\": \"Object Deleted\". I personally couldn't find any way to make Server access logging trigger anything.","comment_id":"932398"},{"timestamp":"1685892300.0","content":"Selected Answer: BDF\nThe question asks about all Object logs, and object logs are covert by S3 Server Access Logging.","upvote_count":"2","comment_id":"914772","poster":"hitesh24","comments":[{"content":"plus, the fact that Cloudtrail is an over-kill\nServer access logging makes more sense here","poster":"chikorita","comment_id":"922270","timestamp":"1686663840.0","upvote_count":"1"}]},{"timestamp":"1685495880.0","comment_id":"910698","poster":"y0eri","content":"Selected Answer: BDF\nCheapest option -> S3 server access logging (https://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html)\nEvent Bridge (https://docs.aws.amazon.com/AmazonS3/latest/userguide/EventBridge.html)\nLifecycle policy for 5 year retention","upvote_count":"3"},{"comment_id":"910280","poster":"F_Eldin","timestamp":"1685453400.0","content":"Selected Answer: ADF\nB : Wrong. Server logging has limited features compared to cloudtrail \nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html\nCloudTrail data events can be set for all the S3 buckets for the AWS account or just for some folder in S3 bucket. Whereas, S3 server access logs would be set at individual bucket level\n\nC: Wrong. You need Lambda to connect events to SES . SNS and eventbridge \nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-how-to-event-types-and-destinations.html#supported-notification-event-types \nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/EventBridge.html\nhttps://medium.com/@anirsom2012/send-email-notification-on-aws-s3-events-186acd25a401\n\nE: Wrong, Timestreams is an overkill.","upvote_count":"4"},{"upvote_count":"2","comment_id":"909893","content":"Selected Answer: ADF\nA)Logs needed to record all objects actions, cloudtrail will need to store these logs in a bucket, since the bucket data is sensitive, you need to separate buckets, as having cloudtrail writing objects into same data sensitive bucket will add additional object logs from cloudtrail putObject actions and could mess up things. Best practice to keep cloudtrail logs in a dedicate s3 bucket.\nD) needed to send notifications to security team, eventbridge with sns topic is needed.\nF) New bucket is needed as explained to point A, to store cloudtrail logs, also put a lifecycle policy on the logging bucket for the 5 year retention request.\n\nC-Wrong, s3 can't directly send events to ses\nE- Timestream is a time series databse.","poster":"andreitugui","comments":[{"poster":"Arnaud92","comment_id":"970318","timestamp":"1690989540.0","content":"Between A and B, B is cheapest. Look at the question. That's why it's BDF","upvote_count":"1"}],"timestamp":"1685422620.0"},{"timestamp":"1685380680.0","upvote_count":"1","comment_id":"909576","poster":"dbaroger","content":"Selected Answer: BCF\nB = https://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-server-access-logging.html\nC = Send Email \nF = https://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-cloudtrail-logging-for-s3.html"},{"upvote_count":"1","timestamp":"1685203620.0","poster":"Roontha","content":"Agreed with ADF","comment_id":"908089"},{"content":"Selected Answer: ADF\nADF, sgreed","comments":[{"poster":"ShinLi","upvote_count":"1","comments":[{"upvote_count":"2","poster":"Jesuisleon","content":"They needs to cache log files( contains access log for objects in S3 ) for 5 years not the object in S3 for 5 years","timestamp":"1685810100.0","comment_id":"913807"}],"timestamp":"1685269920.0","content":"agree with ADF, but F is not looks too good. I know we need lifecycle policy to delete the files after 5 years, but why we need create a new S3 bucket? why not apply lifecycle policy on existing bucket?","comment_id":"908537"}],"upvote_count":"4","poster":"Masonyeoh","timestamp":"1685157600.0","comment_id":"907724"}],"answer_images":[],"question_text":"A company is storing sensitive data in an Amazon S3 bucket. The company must log all activities for objects in the S3 bucket and must keep the logs for 5 years. The company's security team also must receive an email notification every time there is an attempt to delete data in the S3 bucket.\n\nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose three.)","topic":"1","answers_community":["ADF (60%)","BDF (38%)","1%"]},{"id":"cIqpMkKi5on2LupIZBa7","answers_community":["AD (100%)"],"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/110379-exam-aws-certified-solutions-architect-professional-sap-c02/","topic":"1","answer":"AD","isMC":true,"answer_images":[],"exam_id":33,"unix_timestamp":1685201040,"discussion":[{"comment_id":"997698","upvote_count":"25","timestamp":"1693752600.0","content":"Selected Answer: AD\nThere is no correct answer. NONE.\nA.Direct Connect gateway are global. You dont create them in a \"region\"\nB. Not needed, since you have DX-GW.\nC. Cant establish site-to-site VPN over private VIF. You do it over public or transit (recommended).\nD. Yes, should use private VIF, but for access to AWS public resources, not the other VPCs.\nE. VPC peering wont allow Onprem to access other VPCs via peering.\n\nBest Answer is DX-Gateway AND Public VIF (A and D). However they're both wrong.\n\nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways-intro.html","comments":[{"comment_id":"1341959","timestamp":"1737080880.0","upvote_count":"1","content":"Vote D.\nYou can access the AWS public resources if you create a public VIF well. By setting the AWS site-to-set VPN, one of AWS's public resources, you can leverage this VPN to connect to the multiple VPC accordingly.","poster":"GabrielShiao"}],"poster":"cmoreira"},{"upvote_count":"12","timestamp":"1685201040.0","content":"Answer : A, D\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-site-to-site-vpn.html","poster":"Roontha","comment_id":"908070"},{"comment_id":"1346865","upvote_count":"1","timestamp":"1737885060.0","poster":"Zac15","content":"Selected Answer: AD\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-direct-connect-for-amazon-connect/virtual-interfaces-vif.html"},{"poster":"gfhbox0083","upvote_count":"1","comment_id":"1247140","content":"Selected Answer: AD\nA, D for sure.\nMust have access to AWS public services.","timestamp":"1720851780.0"},{"upvote_count":"1","content":"Selected Answer: AD\nA and D","timestamp":"1700801220.0","comment_id":"1079032","poster":"career360guru"},{"content":"Selected Answer: AD\nits AD","comment_id":"944086","timestamp":"1688586060.0","upvote_count":"1","poster":"NikkyDicky"},{"comment_id":"941273","timestamp":"1688336460.0","content":"Selected Answer: AD\nAnswer : A, D\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-site-to-site-vpn.html","poster":"SkyZeroZx","upvote_count":"1"},{"timestamp":"1687874520.0","comment_id":"935459","poster":"pupsik","upvote_count":"2","content":"Selected Answer: AD\ngot to use Public VIN in order to connect to AWS Services via Direct Connect."},{"comment_id":"929834","timestamp":"1687373820.0","content":"a-d-a-d-a-d-a-d","poster":"easytoo","upvote_count":"1"},{"comment_id":"913826","upvote_count":"5","poster":"Jesuisleon","timestamp":"1685811660.0","content":"Agree Roontha.\nFor E, \"Create a private VIF with the existing Direct Connect connection to connect to the peered VPCs\" is wrong. private VIF can only connect to the vpc which is in the same region with direct connection, you can't extend private VIF to the VPCs in other 2 regions."},{"upvote_count":"3","comment_id":"910671","timestamp":"1685493180.0","content":"Selected Answer: AD\nagree with A and D tks to Roontha","poster":"rbm2023"},{"comment_id":"909908","upvote_count":"1","poster":"andreitugui","content":"Selected Answer: AD\nAnswer is A,D","timestamp":"1685424300.0"}],"question_id":110,"answer_description":"","timestamp":"2023-05-27 17:24:00","answer_ET":"AD","choices":{"A":"Create a Direct Connect gateway in the Region that is closest to the data center. Attach the Direct Connect connection to the Direct Connect gateway. Use the Direct Connect gateway to connect the VPCs in the other two Regions.","E":"Use VPC peering to establish a connection between the VPCs across the Regions Create a private VIF with the existing Direct Connect connection to connect to the peered VPCs.","C":"Create a private VIF. Establish an AWS Site-to-Site VPN connection over the private VIF to the VPCs in the other two Regions.","D":"Create a public VIF. Establish an AWS Site-to-Site VPN connection over the public VIF to the VPCs in the other two Regions.","B":"Set up additional Direct Connect connections from the on-premises data center to the other two Regions."},"question_text":"A company is building a hybrid environment that includes servers in an on-premises data center and in the AWS Cloud. The company has deployed Amazon EC2 instances in three VPCs. Each VPC is in a different AWS Region. The company has established an AWS Direct. Connect connection to the data center from the Region that is closest to the data center.\n\nThe company needs the servers in the on-premises data center to have access to the EC2 instances in all three VPCs. The servers in the on-premises data center also must have access to AWS public services.\n\nWhich combination of steps will meet these requirements with the LEAST cost? (Choose two.)"}],"exam":{"id":33,"isImplemented":true,"isMCOnly":true,"numberOfQuestions":529,"isBeta":false,"lastUpdated":"11 Apr 2025","name":"AWS Certified Solutions Architect - Professional SAP-C02","provider":"Amazon"},"currentPage":22},"__N_SSP":true}