{"pageProps":{"questions":[{"id":"6sBBkvNIUQVr0MuUDEzv","answer_description":"","unix_timestamp":1597137840,"exam_id":32,"answer":"D","choices":{"D":"Configure a CodeCommit trigger to invoke an AWS Lambda function to scan new code submissions for credentials. If credentials are found, disable them in AWS IAM and notify the user.","B":"Use a scheduled AWS Lambda function to download and scan the application code from CodeCommit. If credentials are found, generate new credentials and store them in AWS KMS.","A":"Run a script nightly using AWS Systems Manager Run Command to search for credentials on the development instances. If found, use AWS Secrets Manager to rotate the credentials.","C":"Configure Amazon Macie to scan for credentials in CodeCommit repositories. If credentials are found, trigger an AWS Lambda function to disable the credentials and notify the user."},"isMC":true,"answer_ET":"D","url":"https://www.examtopics.com/discussions/amazon/view/28051-exam-aws-certified-solutions-architect-professional-topic-1/","discussion":[{"comments":[{"timestamp":"1634147880.0","upvote_count":"3","comment_id":"276942","poster":"rcher","content":"Sample code here https://github.com/aws-samples/discover-sensitive-data-in-aws-codecommit-with-aws-lambda/tree/main/src/handlers\n\nRunning regex after all hehe"}],"upvote_count":"28","poster":"Nemer","content":"D. CodeCommit trigger with Lambda.\nhttps://docs.aws.amazon.com/lambda/latest/dg/services-codecommit.html","timestamp":"1632413040.0","comment_id":"155266"},{"upvote_count":"10","content":"C. Macie can be used with CodeCommit. \nhttps://docs.aws.amazon.com/codecommit/latest/userguide/data-protection.html","comments":[{"poster":"misterfaust","comments":[{"poster":"Gmail78","comment_id":"200428","content":"which it exclude C from my understanding...D is then the answer","comments":[{"comment_id":"207319","content":"Macie can only scan S3 buckets. D is the answer","poster":"bbnbnuyh","timestamp":"1633437660.0","upvote_count":"3"}],"timestamp":"1633144980.0","upvote_count":"1"}],"upvote_count":"1","content":"\"Use advanced managed security services such as Amazon Macie, which assists in discovering and securing personal data that is stored in Amazon S3.\"","timestamp":"1633125480.0","comment_id":"196258"},{"upvote_count":"5","poster":"ymengxing","comments":[{"upvote_count":"4","content":"CodeCommit may use S3 on the back end (and it also uses DynamoDB on the back end) but I don't think they're stored in buckets that you can see or point Macie to. In fact, there are even solutions out there describing how to copy your repo from CodeCommit into S3 to back it up: https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automate-event-driven-backups-from-codecommit-to-amazon-s3-using-codebuild-and-cloudwatch-events.html\n\nD: AWS has an exact architecture for doing this: https://aws.amazon.com/blogs/compute/discovering-sensitive-data-in-aws-codecommit-with-aws-lambda-2/","timestamp":"1636291620.0","poster":"kirrim","comment_id":"461654"}],"content":"That's right!\nAWS CodeCommit stores your repositories in Amazon S3 and Amazon DynamoDB.\nSo use Macie.\nSee https://aws.amazon.com/codecommit/features/\nHigh Availability and Durability.","comment_id":"425718","timestamp":"1635353520.0"}],"timestamp":"1633117260.0","poster":"MMARTINEZ85","comment_id":"192999"},{"timestamp":"1693198740.0","comment_id":"991804","upvote_count":"1","content":"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify-lambda.html","poster":"Simon523"},{"upvote_count":"3","content":"Selected Answer: D\nMacke is only for S3","comment_id":"644386","timestamp":"1660028760.0","poster":"Santo99"},{"content":"Amazon Macie is only used for S3. Hence, D seems good :):)","comment_id":"536097","poster":"cannottellname","upvote_count":"2","timestamp":"1643542980.0"},{"timestamp":"1642811640.0","upvote_count":"1","content":"D - https://aws.amazon.com/blogs/compute/discovering-sensitive-data-in-aws-codecommit-with-aws-lambda-2/","poster":"tkanmani76","comment_id":"529505"},{"content":"D is right answer, I think this question in Neal Davis practice tests","comment_id":"494873","poster":"AzureDP900","upvote_count":"1","timestamp":"1638764160.0"},{"comment_id":"490842","poster":"ryu10_09","upvote_count":"1","timestamp":"1638290760.0","content":"Selected Answer: D\nhttps://aws.amazon.com/blogs/compute/discovering-sensitive-data-in-aws-codecommit-with-aws-lambda-2/"},{"timestamp":"1636080780.0","poster":"nodogoshi","content":"D. Amazon Macie is for S3 Service, not for CodeCommit.\nhttps://docs.aws.amazon.com/codecommit/latest/userguide/data-protection.html\n”Use advanced managed security services such as Amazon Macie, which assists in discovering and securing personal data that is stored in Amazon S3.”\n[stored in Amazon S3.]","comment_id":"450519","upvote_count":"1"},{"timestamp":"1635839160.0","content":"For D, there is a blog post describing that exact solution: https://aws.amazon.com/blogs/compute/discovering-sensitive-data-in-aws-codecommit-with-aws-lambda-2/\nFor C: I dont think Macie works directly with CodeCommit","poster":"TomPaschenda","upvote_count":"4","comments":[{"upvote_count":"1","poster":"student22","timestamp":"1635971220.0","comment_id":"441855","content":"Good link, thanks. \nAnswer is D"}],"comment_id":"441598"},{"upvote_count":"2","poster":"Suresh108","comment_id":"439343","timestamp":"1635823380.0","content":"I am choosing DDDDDD. \n\nhttps://aws.amazon.com/blogs/compute/discovering-sensitive-data-in-aws-codecommit-with-aws-lambda-2/"},{"upvote_count":"1","comment_id":"436859","content":"D\nNot C - Macie is for s3","timestamp":"1635496620.0","poster":"student22"},{"poster":"WhyIronMan","content":"I'll go with D","upvote_count":"1","comment_id":"411893","timestamp":"1635284340.0"},{"poster":"Kopa","content":"Only D is a promptly and immediate solution regarding security.","timestamp":"1635247080.0","comment_id":"405271","upvote_count":"1"},{"content":"D is answer. C is not relevant , it is to scan S3.","upvote_count":"1","comment_id":"351571","timestamp":"1634892420.0","poster":"blackgamer"},{"upvote_count":"2","comment_id":"350259","content":"it's D","timestamp":"1634810220.0","poster":"Waiweng"},{"timestamp":"1634691840.0","content":"Cannot be D. It will check only for newly commited code, not for old code, which is required. I pick A.","comment_id":"332108","upvote_count":"1","poster":"PredaOvde"},{"poster":"Pupu86","content":"Answer is D.\nCodeCommit itself is a repository. \n\nUsing Mercie means you are saving your code artefacts to S3 instead.","upvote_count":"1","timestamp":"1634559060.0","comment_id":"323235"},{"upvote_count":"2","comment_id":"297235","timestamp":"1634492820.0","content":"Answer is C \nHow will you scan your code with lambda ? \npls take a look at this link , Macie is used on s3 :\n https://aws.amazon.com/blogs/security/classify-sensitive-data-in-your-environment-using-amazon-macie/","poster":"tipzzz"},{"timestamp":"1634202360.0","comment_id":"292559","poster":"Kian1","upvote_count":"1","content":"going with D"},{"timestamp":"1634145240.0","content":"D is the best answer","upvote_count":"3","comment_id":"272724","poster":"Ebi"},{"poster":"kopper2019","upvote_count":"1","timestamp":"1634136180.0","content":"D, Macie is only for S3\nUse advanced managed security services such as Amazon Macie, which assists in discovering and securing personal data that is stored in Amazon S3.","comment_id":"269255"},{"upvote_count":"1","content":"Correct answer is D. Not C because Macie only works with S3 and not with CodeCommit.","comment_id":"253626","poster":"Bulti","timestamp":"1633725660.0"},{"comment_id":"244586","content":"Correct is C. Macie","poster":"T14102020","timestamp":"1633588500.0","upvote_count":"1"},{"poster":"rscloud","content":"D. Amazon Macie is for S3","upvote_count":"1","timestamp":"1633575120.0","comment_id":"242051"},{"comment_id":"231659","timestamp":"1633559820.0","upvote_count":"3","content":"I'll go with D","poster":"jackdryan"},{"poster":"smartassX","upvote_count":"2","timestamp":"1633519440.0","comment_id":"216125","content":"C macie can scan for credentials using regex and classify the content and its risk factor. Trigger lambda to disable the credentials and notify the user!"},{"content":"D. is the best answer. Macie support only S3.","poster":"wsw","upvote_count":"1","comment_id":"174013","timestamp":"1632544500.0"},{"comment_id":"169697","content":"D. I believe Macie is for S3 storage.","poster":"Momon","upvote_count":"1","timestamp":"1632541140.0"},{"upvote_count":"1","timestamp":"1632529680.0","comments":[{"content":"Macie is for keep a check on confidential data being exposed","upvote_count":"2","comment_id":"183287","timestamp":"1633093380.0","poster":"sam422"},{"poster":"petebear55","comment_id":"227685","content":"Amazon Macie is for use with S3","timestamp":"1633521840.0","upvote_count":"1"},{"comment_id":"254162","timestamp":"1634112180.0","upvote_count":"1","content":"Yes but for s3 Macie is used for ... typical Amazon Red Herring","poster":"petebear55"}],"poster":"ashp","content":"why not C ? Amazon Macie is built to check for security issues. \n Just not clear that is why I am asking","comment_id":"161041"},{"upvote_count":"1","timestamp":"1632431040.0","content":"D is right option.","poster":"Anila_Dhharisi","comment_id":"157147"}],"topic":"1","timestamp":"2020-08-11 11:24:00","question_images":[],"answer_images":[],"question_id":561,"answers_community":["D (100%)"],"question_text":"During an audit, a security team discovered that a development team was putting IAM user secret access keys in their code and then committing it to an AWS\nCodeCommit repository. The security team wants to automatically find and remediate instances of this security vulnerability.\nWhich solution will ensure that the credentials are appropriately secured automatically?"},{"id":"0igGUnpa2zlVmZGVnU7i","unix_timestamp":1597138680,"answer_description":"","choices":{"B":"Implement automated testing using AWS CodeBuild in a test environment. Use CloudFormation change sets to evaluate changes before deployment. Use AWS CodeDeploy to leverage blue/green deployment patterns to allow evaluations and the ability to revert changes, if needed.","C":"Use plugins for the integrated development environment (IDE) to check the templates for errors, and use the AWS CLI to validate that the templates are correct. Adapt the deployment code to check for error conditions and generate notifications on errors. Deploy to a test environment and execute a manual test plan before approving the change for production.","A":"Adapt the deployment scripts to detect and report CloudFormation error conditions when performing deployments. Write test plans for a testing team to execute in a non-production environment before approving the change for production.","D":"Use AWS CodeDeploy and a blue/green deployment pattern with CloudFormation to replace the user data deployment scripts. Have the operators log in to running instances and go through a manual test plan to verify the application is running as expected."},"answer":"B","discussion":[{"comment_id":"155279","upvote_count":"21","poster":"Nemer","timestamp":"1632246960.0","content":"B. Why do manual testing in option D when it can be automated with CodeBuild? CF Change sets to preview changes, and CodeDeploy b/g deployment with ASG.\n\nhttps://aws.amazon.com/blogs/devops/performing-bluegreen-deployments-with-aws-codedeploy-and-auto-scaling-groups/"},{"upvote_count":"1","comment_id":"760226","poster":"evargasbrz","timestamp":"1672254120.0","content":"Selected Answer: B\nI'll go with B"},{"content":"of course, it's B.","upvote_count":"1","timestamp":"1658872560.0","comment_id":"637642","poster":"hilft"},{"content":"Selected Answer: B\nit is B","upvote_count":"2","comment_id":"610602","poster":"xyzman","timestamp":"1654174620.0"},{"timestamp":"1639146540.0","comment_id":"498696","poster":"cldy","content":"B. Implement automated testing using AWS CodeBuild in a test environment. Use CloudFormation change sets to evaluate changes before deployment. Use AWS CodeDeploy to leverage blue/green deployment patterns to allow evaluations and the ability to revert changes, if needed.","upvote_count":"3"},{"content":"B is right , this is straight forward question","upvote_count":"2","poster":"AzureDP900","comment_id":"494874","timestamp":"1638764340.0"},{"timestamp":"1638101220.0","comment_id":"489106","poster":"acloudguru","content":"Selected Answer: B\ncodebuild can provide automatic test","upvote_count":"3"},{"upvote_count":"1","content":"B. automated testing always best","timestamp":"1636183740.0","comment_id":"450520","poster":"nodogoshi"},{"timestamp":"1635966000.0","poster":"jobe42","comment_id":"412619","content":"B: fully automated, with ChangeSets, all other answers have way to much room for human errors.","upvote_count":"2"},{"content":"I'll go with B","comment_id":"411905","poster":"WhyIronMan","timestamp":"1635736320.0","upvote_count":"1"},{"comment_id":"351578","timestamp":"1635351720.0","poster":"blackgamer","content":"It is B.","upvote_count":"1"},{"timestamp":"1635180660.0","poster":"Waiweng","comment_id":"350263","upvote_count":"3","content":"it's B"},{"content":"going with B","poster":"Kian1","timestamp":"1634437740.0","upvote_count":"1","comment_id":"292561"},{"content":"I will go with B","upvote_count":"3","timestamp":"1633702260.0","poster":"Ebi","comment_id":"272725"},{"poster":"kopper2019","comment_id":"258983","timestamp":"1633667220.0","content":"Answer is B","upvote_count":"2"},{"poster":"Bulti","content":"B is correct.","timestamp":"1633645320.0","comment_id":"253628","upvote_count":"2"},{"content":"Correct is B. automated testing using AWS CodeBuild","upvote_count":"1","comment_id":"244589","timestamp":"1633347720.0","poster":"T14102020"},{"upvote_count":"2","content":"I'll go with B","poster":"jackdryan","comment_id":"231661","timestamp":"1633069020.0"},{"upvote_count":"1","poster":"CYL","comment_id":"208643","timestamp":"1632446460.0","content":"B. When one adopts go infrastructure as code, we need to test the infrastructure code as well via automated testing, and revert to original if things are not performing correctly."},{"timestamp":"1632286680.0","comment_id":"157151","upvote_count":"3","content":"B is right option. As CodeBuild will suffice the testing requirement and CodeDeploy for Blue/Green deployments.","poster":"Anila_Dhharisi"}],"answer_ET":"B","answers_community":["B (100%)"],"timestamp":"2020-08-11 11:38:00","question_id":562,"isMC":true,"exam_id":32,"url":"https://www.examtopics.com/discussions/amazon/view/28056-exam-aws-certified-solutions-architect-professional-topic-1/","question_images":[],"question_text":"A company is using AWS CodePipeline for the CI/CD of an application to an Amazon EC2 Auto Scaling group. All AWS resources are defined in AWS\nCloudFormation templates. The application artifacts are stored in an Amazon S3 bucket and deployed to the Auto Scaling group using instance user data scripts.\nAs the application has become more complex, recent resource changes in the CloudFormation templates have caused unplanned downtime.\nHow should a solutions architect improve the CI/CD pipeline to reduce the likelihood that changes in the templates will cause downtime?","topic":"1","answer_images":[]},{"id":"HW1uLzqR560qBaWt1mFC","topic":"1","unix_timestamp":1597219620,"question_text":"A financial services company is moving to AWS and wants to enable developers to experiment and innovate while preventing access to production applications.\nThe company has the following requirements:\n✑ Production workloads cannot be directly connected to the internet.\n✑ All workloads must be restricted to the us-west-2 and eu-central-1 Regions.\n✑ Notification should be sent when developer sandboxes exceed $500 in AWS spending monthly.\nWhich combination of actions needs to be taken to create a multi-account structure that meets the company's requirements? (Choose three.)","answer_images":[],"timestamp":"2020-08-12 10:07:00","choices":{"B":"Create accounts for each production workload within an organization in AWS Organizations. Place the production accounts within an organizational unit (OU). Create an SCP with a Deny rule on the attach an internet gateway action. Create an SCP with a Deny rule to prevent use of the default VPC. Attach the SCPs to the OU for the production accounts.","C":"Create a SCP containing a Deny Effect for cloudfront:*, iam:*, route53:*, and support:* with a StringNotEquals condition on an aws:RequestedRegion condition key with us-west-2 and eu-central-1 values. Attach the SCP to the organization's root.","F":"Create accounts for each development workload within an organization in AWS Organizations. Place the development accounts within an organizational unit (OU). Create a budget within AWS Budgets for each development account to monitor and report on monthly spending exceeding $500.","D":"Create an IAM permission boundary containing a Deny Effect for cloudfront:*, iam:*, route53:*, and support:* with a StringNotEquals condition on an aws:RequestedRegion condition key with us-west-2 and eu-central-1 values. Attach the permission boundary to an IAM group containing the development and production users.","A":"Create accounts for each production workload within an organization in AWS Organizations. Place the production accounts within an organizational unit (OU). For each account, delete the default VPC. Create an SCP with a Deny rule for the attach an internet gateway and create a default VPC actions. Attach the SCP to the OU for the production accounts.","E":"Create accounts for each development workload within an organization in AWS Organizations. Place the development accounts within an organizational unit (OU). Create a custom AWS Config rule to deactivate all IAM users when an account's monthly bill exceeds $500."},"question_id":563,"answer_description":"","question_images":[],"answer":"ACF","url":"https://www.examtopics.com/discussions/amazon/view/28238-exam-aws-certified-solutions-architect-professional-topic-1/","exam_id":32,"discussion":[{"comments":[{"upvote_count":"9","timestamp":"1635318960.0","comments":[{"content":"Good point. \nAnswer: ACF","upvote_count":"1","timestamp":"1636010160.0","comment_id":"436878","poster":"student22"}],"content":"B is wrong for one simple reason. You can delete the default VPC and create a new one. The new one will have a new arn so the SCP will not have effect on it.\nJust denying to create an IG does not prevent to create a new default VPC with the IG attached.\nFrom here: https://docs.aws.amazon.com/vpc/latest/userguide/default-vpc.html\n\"Amazon creates the above resources on your behalf. IAM policies do not apply to these actions because you do not perform these actions. For example, if you have an IAM policy that denies the ability to call CreateInternetGateway, and then you call CreateDefaultVpc, the internet gateway in the default VPC is still created.\"\n\nIn conclusion, ACF","poster":"pablobairat","comment_id":"427526"},{"content":"I get the point... both questions want to reflect equivalent actions, but for me the redaction of B is very confusing...\n\n\"and create a default VPC actions. Create an SCP with a Deny rule to prevent use of the default VPC\"\n\nObviousy it can be understood that \"create default vpc actions\" means the default vpc for the prod environment....\n\nAnd when it is said that...\"Create an SCP with a Deny rule to prevent use of the default VPC\"... It can be understood that it is talking about th original \"default VPC\" no the new one... isn´t it?\n\nIn any case It is too much \"It can be understood\"... So I go for ACF, nobody will use never that VPC so I for me it has more sense cleaning the entire network structure of prod (consdering B syntax).","timestamp":"1632531600.0","upvote_count":"1","poster":"ipindado2020","comment_id":"183261"}],"poster":"Nemer","upvote_count":"25","comment_id":"156252","content":"BCF - Production and dev accounts in separate OUs, AWS Budget for notifications. \nBetween A & B, deleting default VPC seems excessive. SCP should be able to prevent using it. Not 100% sure.","timestamp":"1632070860.0"},{"upvote_count":"23","poster":"Ebi","comment_id":"272740","content":"ACF is the right answer.\nB can not be the answer, there is no way to have one single SCP at OU or root level to deny using of default VPC in each account","timestamp":"1634117400.0","comments":[{"timestamp":"1634270640.0","content":"Touche","upvote_count":"2","comment_id":"297432","poster":"gpark"},{"timestamp":"1664949360.0","comment_id":"686621","upvote_count":"2","content":"Should be ADF. As there could be other types of workload which could be in other org, e.g. sandbox workloads in CTO org, etc. The question doesn't imply there are only two orgs in this company","poster":"heany"}]},{"comments":[{"content":"ACF you cannot \"Create an SCP with a Deny rule to prevent use of the default VPC\"","upvote_count":"1","poster":"JohnPi","comment_id":"698405","timestamp":"1666115040.0"}],"upvote_count":"1","content":"Selected Answer: BCF\nBCF\nA does not scale","comment_id":"698395","timestamp":"1666114200.0","poster":"JohnPi"},{"timestamp":"1665053760.0","upvote_count":"2","comment_id":"687689","content":"Selected Answer: ACF\nThe answer should be ACF.\n\nB(wrong): \"Create an SCP with a Deny rule to prevent use of the default VPC.\" It is impossible to do this.\nD(wrong): Permission boundary can only be attached to user or role, rather than IAM group.\nE(wrong): Obviously wrong. AWS Budgets should be used.","poster":"tomosabc1"},{"upvote_count":"1","comment_id":"678668","poster":"Azerty1313","timestamp":"1664099160.0","content":"C isn't recommended see: https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html#scp-warning-testing-effect"},{"content":"Selected Answer: ACF\nAWS strongly recommends that you don't attach SCPs to the root of your organization without thoroughly testing the impact that the policy has on accounts. \n\nBut if tested why not?","timestamp":"1663869180.0","poster":"AwsBRFan","upvote_count":"2","comment_id":"676441"},{"comment_id":"637040","upvote_count":"1","poster":"hilft","content":"It's BDF.\nDon't mess around with IGW\nAWS don't recommend SCP on root account","timestamp":"1658798100.0"},{"content":"Selected Answer: ACF\nCannot find how to \"Deny rule to prevent use of the default VPC\"","poster":"aandc","comment_id":"626471","timestamp":"1656834840.0","upvote_count":"2"},{"content":"Selected Answer: ACF\nVote ACF","timestamp":"1649090940.0","upvote_count":"3","poster":"roka_ua","comment_id":"580846"},{"timestamp":"1645701000.0","poster":"futen0326","upvote_count":"1","content":"D instead of C. You don't have to attach an SCP to the root, it's bad practice, you can be a little more granular with D. It works better for the requirement.","comment_id":"555204"},{"poster":"tkanmani76","upvote_count":"1","comment_id":"523258","content":"A - Why not B ? Tried searching SCP for VPC - we can deny creation of default VPC (CreateDefaultVpc), there are none to stop using it. So only way is to delete.\nD - Why not C ? Per AWS it is not a good practice to attach SCP to root.\nF - No contention with E here.","timestamp":"1642125180.0"},{"content":"I have to revisit this question and confirm between ACF vs BCF","comment_id":"494876","timestamp":"1638764700.0","poster":"AzureDP900","upvote_count":"1"},{"comment_id":"486517","timestamp":"1637829240.0","comments":[{"comment_id":"486519","content":"I change my mind. i have checked and you can delete default VPC","poster":"ryu10_09","timestamp":"1637829300.0","upvote_count":"1"}],"upvote_count":"1","poster":"ryu10_09","content":"why A, you cannot delete the default VPC. so A is not valid. It is BCF"},{"content":"A,C,F should be","timestamp":"1637068080.0","poster":"Kopa","comment_id":"479386","upvote_count":"1"},{"poster":"near22","timestamp":"1636091760.0","comment_id":"446329","upvote_count":"1","content":"ADF\nfor c, AWS don't recommend apply SCP to root"},{"poster":"littlecurly","timestamp":"1635465900.0","upvote_count":"1","comment_id":"427955","content":"B,D,F\nD denies the root to the global services including IAM, which doesn't make sense..."},{"timestamp":"1635031800.0","comment_id":"416493","poster":"student2020","content":"ACF is the answer\nThere is no action to prevent use of default VPC\nhttps://docs.aws.amazon.com/AWSEC2/latest/APIReference/OperationList-query-vpc.html","upvote_count":"3"},{"poster":"WhyIronMan","upvote_count":"3","timestamp":"1634910900.0","comment_id":"412483","content":"I'll go with B,C,F"},{"upvote_count":"3","comment_id":"390458","timestamp":"1634820240.0","poster":"tushar321","content":"Why not D. Why to apply SCP at root level which is not a best practice ?"},{"upvote_count":"2","comment_id":"368536","timestamp":"1634735700.0","poster":"[Removed]","content":"Why D instead of C?"},{"comments":[{"poster":"viet1991","content":"No, You can create new default VPC after delete old one.","comment_id":"375384","upvote_count":"1","timestamp":"1634793540.0"},{"content":"From an AWS Security Training which I attended: Delete the default VPC! You do not need it anymore. If you find any AWS service which relies on the default VPC, please contact us, so we can change the services.","comment_id":"412625","timestamp":"1635007740.0","poster":"jobe42","upvote_count":"1"}],"upvote_count":"1","timestamp":"1634733780.0","poster":"tvs","comment_id":"352342","content":"Deleting default VPC might create issues ? https://aws.amazon.com/premiumsupport/knowledge-center/deleted-default-vpc/"},{"timestamp":"1634520660.0","upvote_count":"1","comment_id":"351581","poster":"blackgamer","content":"BCF seems to be better option. To delete default VPC for each new account is not a good solution."},{"comment_id":"350272","timestamp":"1634492040.0","upvote_count":"4","poster":"Waiweng","content":"it's B,C,F"},{"comment_id":"333820","content":"You can create a baseline SCP to deny usage of default VPC and tag it to production OU instead of manually deleting the default VPC (containing IGW) every time a new production account is created. \n\nit is a much neater solution so BCF","poster":"Pupu86","upvote_count":"1","timestamp":"1634451360.0"},{"upvote_count":"2","timestamp":"1634232540.0","content":"going with ACF","comment_id":"292562","poster":"Kian1"},{"timestamp":"1633748940.0","upvote_count":"2","poster":"petebear55","comment_id":"254231","content":"Having looked at the answers i chose BCF before reading the below .... I am happy to stay with my answers as this is one of those questions where one can just choose as he thinks best ... well done to every one else whom chose BCF as well. NOTE THIS QUESTION OR VERY SIMILAR IS ON THE EXAM"},{"upvote_count":"2","content":"A,C,F is the right answer. Deleting the default VPC in an account that should not be exposed to the internet is a standard procedure followed by most Enterprise IT companies.","timestamp":"1633725720.0","poster":"Bulti","comment_id":"253679"},{"upvote_count":"2","timestamp":"1633711500.0","comment_id":"251348","content":"ACF - The problem with B is that every time you need to add a new production workload, you would need to update your SCP. With A however, you would just delete the default VPC and you are good to go.","poster":"darthvoodoo"},{"upvote_count":"2","timestamp":"1633640940.0","poster":"taoteching1","content":"I'll go with ACF, deleting the default VPC is standard operating procedure for well architected infrastructure design in Dec 2020 - https://www.reddit.com/r/aws/comments/fshd0f/do_you_delete_the_default_vpc/","comment_id":"250029"},{"comment_id":"244599","content":"Correct is BCF. Witout delect deafult VPC + SCP +AWS Budget","poster":"T14102020","timestamp":"1633396980.0","upvote_count":"1"},{"timestamp":"1633393680.0","upvote_count":"4","poster":"jackdryan","content":"I'll go with B,C,F","comment_id":"231664"},{"upvote_count":"1","content":"A, C, F. Matches the requirements in the questions. AWS Budget to guard on over expenditure. SCP to control usage outside allowed region.","timestamp":"1633066200.0","comment_id":"208645","poster":"CYL"},{"content":"I will go with ACF , I could not find any SCP policy with a Deny rule to prevent use of the default VPC","comment_id":"201497","poster":"Mikey123","timestamp":"1633050900.0","comments":[{"content":"This is how you can write a deny rule for running ec2 in a specific VPC {\n \"Version\": \"2012-10-17\",\n \"Statement\": [{\n \"Effect\": \"Deny\",\n \"Action\": \"ec2:RunInstances\",\n \"Resource\": \"arn:aws:ec2:region:account:subnet/*\",\n \"Condition\": {\n \"StringEquals\": {\n \"ec2:Vpc\": \"arn:aws:ec2:region:account:vpc/vpc-11223344556677889\"\n }\n }\n }","upvote_count":"2","comment_id":"216333","timestamp":"1633151160.0","poster":"smartassX"}],"upvote_count":"8"},{"comment_id":"199590","poster":"Calig","comments":[{"content":"on a second thought \"Create an SCP with a Deny rule to prevent use of the default VPC.\" is more efficient than deleting default VPC, so change to BCF","poster":"Calig","upvote_count":"1","comment_id":"199593","timestamp":"1632944220.0"}],"timestamp":"1632724200.0","upvote_count":"1","content":"A C F - A is better because it totally remove the ability to further have internet access (via creating IGW or creating default VPC). Note that deleting default VPC does not means you cannot recreate another VPC for private networking (just not default VPC which include IGW)..."},{"comment_id":"185195","content":"B for sure. If you delete default VPC, If you delete a default VPC, you might experience problems launching Amazon Elastic Compute Cloud (Amazon EC2) instances in that Region.\nAs per https://aws.amazon.com/premiumsupport/knowledge-center/deleted-default-vpc","timestamp":"1632709260.0","comments":[{"comment_id":"263781","content":"What kind of problem?","upvote_count":"1","timestamp":"1633894980.0","poster":"01037"}],"upvote_count":"1","poster":"perio"},{"comments":[{"upvote_count":"2","comment_id":"163869","timestamp":"1632327120.0","poster":"balisongjam","content":"Brother, no. You might need the VPC in prod. The only requirment is that you can't connect to the internet, therefore SCP to stop IG creation is sufficient."},{"comment_id":"176642","comments":[{"poster":"Phat","upvote_count":"3","timestamp":"1632391740.0","content":"Changed to BCF","comment_id":"179052"}],"timestamp":"1632378780.0","poster":"Phat","upvote_count":"1","content":"I think LunchTime has reason"},{"poster":"sam422","comment_id":"182193","upvote_count":"1","content":"Delete Default VPC? IGW is the one which makes VPC public? so B makes sense","timestamp":"1632472200.0"}],"comment_id":"160167","content":"A, C, F is correct.\nA and not B is correct as an SCP does not ensure that the default VPC cannot be accessed. As per https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_type-auth.html\n “SCPs affect only principals that are managed by accounts that are part of the organization. SCPs don't affect resource-based policies directly. They also don't affect users or roles from accounts outside the organization.”. Therefore the default VPC needs to be deleted, making A, and not B correct","poster":"LunchTime","timestamp":"1632191580.0","upvote_count":"7"},{"comment_id":"157160","upvote_count":"3","poster":"Anila_Dhharisi","content":"BCF are right options. Instead of deleting the default VPC better not to give access to use default VPC which is in option B.","timestamp":"1632083640.0"}],"answers_community":["ACF (90%)","10%"],"answer_ET":"ACF","isMC":true},{"id":"gTfIy8hnWNb8KQ8tBS5L","timestamp":"2020-08-13 18:30:00","answer":"B","url":"https://www.examtopics.com/discussions/amazon/view/28502-exam-aws-certified-solutions-architect-professional-topic-1/","exam_id":32,"topic":"1","answer_ET":"B","answer_description":"","question_images":[],"discussion":[{"comments":[{"poster":"hilft","upvote_count":"1","comment_id":"638934","content":"i think B is better than A","timestamp":"1659055500.0"},{"content":"I don't think web app combined with ALB. My understanding is access heavy using NLB, calculation heavy using ALB.","timestamp":"1635219000.0","poster":"oscargee","upvote_count":"3","comment_id":"363053"},{"timestamp":"1642929120.0","content":"why not C?","upvote_count":"2","poster":"GeniusMikeLiu","comment_id":"530403"}],"poster":"Nemer","comment_id":"157479","timestamp":"1632695760.0","content":"B. Web app needs ALB. Multi-AZ deployment should address HA. Retain deletion policy to not delete the db with the stack.","upvote_count":"22"},{"poster":"Ebi","timestamp":"1634536740.0","upvote_count":"5","comment_id":"272743","content":"I will go with B"},{"comment_id":"1128030","poster":"3a632a3","timestamp":"1705860060.0","content":"Selected Answer: B\nI was stuck on this one for a while as both A and B are possible. The biggest issue with B is it doesn't have detail on the app deployment. But, I realize it is asking for a \"scalable\" solution. Both the ALB and NLB can scale, BUT an RDS instance cannot scale like an Aurora MySQL cluster which can have 15 read replicas.","upvote_count":"1"},{"upvote_count":"1","poster":"Vadbro7","content":"More voted answers are correct or the suggested answer please help","timestamp":"1691913300.0","comment_id":"979847"},{"timestamp":"1681856100.0","upvote_count":"2","poster":"romiao106","content":"Selected Answer: B\nNLB is wayy more expensive.","comment_id":"874149"},{"poster":"robertohyena","upvote_count":"3","comment_id":"744836","content":"Selected Answer: B\nAnswer: B.\nNot A: we will not use NLB for web app\nNot C: Beanstalk is region service. It CANNOT \"automatically scaling web server environment that spans two separate Regions\"\nNot D: spot instances cant meet 'highly available'","timestamp":"1671003960.0"},{"comment_id":"703302","timestamp":"1666638660.0","upvote_count":"3","content":"Selected Answer: B\nBBB - Web App need ALB not NLB","poster":"Blair77"},{"upvote_count":"2","comments":[{"timestamp":"1705858200.0","comment_id":"1128010","content":"You can use an A record with an alias\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html","upvote_count":"1","poster":"3a632a3"}],"content":"Selected Answer: A\nHave to use CNAME record on R53 to maping with ALB. C is wrong","timestamp":"1665985740.0","poster":"ToanVN1988","comment_id":"696957"},{"content":"200,000 accesses per day is really no big deal, no reason to use NLB","timestamp":"1665201360.0","upvote_count":"1","poster":"WayneYi","comment_id":"689011"},{"comment_id":"659231","content":"Question forcus on HA \nAmazon Aurora is designed for spead storage on three AZ => HA more than RDS only","timestamp":"1662292260.0","poster":"kadev","upvote_count":"1"},{"content":"B is best option for the requirement","poster":"Sathish1412","timestamp":"1661932260.0","comment_id":"654870","upvote_count":"1"},{"upvote_count":"2","comments":[{"upvote_count":"1","comment_id":"654546","timestamp":"1661904180.0","poster":"Sathish1412","content":"You are correct!"}],"poster":"MikeyJ","content":"Selected Answer: B\nNLB in A is overkill.\n\ndaily demands of 200,000 users < Network Load Balancer is capable of handling millions of requests per second while maintaining ultra-low latencies.\nhttps://aws.amazon.com/elasticloadbalancing/network-load-balancer/","comment_id":"646634","timestamp":"1660463460.0"},{"comment_id":"628870","upvote_count":"1","timestamp":"1657302420.0","poster":"Millari","content":"A. EB is already .NET ready\nAWS Elastic Beanstalk for .NET makes it easier to deploy, manage, and scale your ASP.NET web applications that use Amazon Web Services.\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_NET.html\n\nyou can also AAdding an Amazon RDS DB instance to your .NET application environment"},{"upvote_count":"1","comment_id":"624375","timestamp":"1656469260.0","poster":"TechX","comments":[{"content":"I'd start thinking about a NLB if I had 200K users per second, not per day.","timestamp":"1660483860.0","upvote_count":"1","poster":"gerhardbl","comment_id":"646760"}],"content":"Selected Answer: A\nA for me, we have 200.000 users which is heavy access, NLB will go over ALB"},{"upvote_count":"3","comment_id":"622814","timestamp":"1656276900.0","content":"Selected Answer: B\nB sounds better.","poster":"kangtamo"},{"content":"Selected Answer: B\nIt's B.","upvote_count":"2","timestamp":"1644367320.0","comment_id":"543447","poster":"Bigbearcn"},{"content":"Selected Answer: A\nYou can deploy system with Beanstalk since it has its source code of .Net. And there's no DR requirement (D).","poster":"HellGate","upvote_count":"2","comment_id":"543106","timestamp":"1644331260.0"},{"comment_id":"494878","poster":"AzureDP900","content":"B is right","timestamp":"1638764940.0","upvote_count":"1"},{"upvote_count":"3","timestamp":"1636807260.0","poster":"kaleen_bhaiya","content":"Answer is A\nCouple of reasons; 1) NLB is high performing and 2) You cannot have an A record for Route 53 alias, ALB doesn't have IP (A type) so answer would be NLB. Let me know if I am missing anything.","comment_id":"477445"},{"comment_id":"412494","content":"I'll go with B","poster":"WhyIronMan","timestamp":"1635720480.0","upvote_count":"2"},{"poster":"blackgamer","timestamp":"1635115440.0","comment_id":"351584","comments":[{"poster":"Rocketeer","content":"NLB will act as a passthrough for the traffic and hence will work for http or https","comment_id":"675096","timestamp":"1663761360.0","upvote_count":"1","comments":[{"content":"Never mind. I think we need to use ALB for http or https traffic.","timestamp":"1663761720.0","comment_id":"675105","upvote_count":"1","poster":"Rocketeer"}]}],"upvote_count":"5","content":"I will go with B. A is incorrect as NLB doesn't have listener for Http and Https, it only works at layer 4 TCP and TLS only."},{"upvote_count":"4","timestamp":"1635051240.0","poster":"Waiweng","content":"it's B","comment_id":"350278"},{"comment_id":"331534","content":"A\nI will filter out any answer with \"spanning three azs\". LB supports >=2 AZs. The question doesn't specify the region. AWS still has certain region with 2 AZs, in BJS for instance.","poster":"Sunflyhome","timestamp":"1635006780.0","comments":[{"comment_id":"436881","poster":"student22","timestamp":"1635974700.0","comments":[{"content":"Changing my answer to B.","timestamp":"1636297920.0","comment_id":"458154","upvote_count":"1","poster":"student22"}],"upvote_count":"1","content":"A for me too.\nNLB is good for heavy use."}],"upvote_count":"3"},{"upvote_count":"2","timestamp":"1634656560.0","comment_id":"292563","poster":"Kian1","content":"going with B"},{"timestamp":"1634312340.0","content":"B is the right answer. I agree with Nemer.","poster":"Bulti","comment_id":"253688","upvote_count":"3"},{"comment_id":"244607","upvote_count":"3","timestamp":"1634102880.0","poster":"T14102020","comments":[{"timestamp":"1634207760.0","content":"+ Retain Delete policy","poster":"T14102020","upvote_count":"2","comment_id":"244632"}],"content":"Correct is B. Aurora + without Beanstalk as it cannot support multiregions"},{"poster":"jackdryan","content":"I'll go with B","comment_id":"231669","timestamp":"1633591980.0","upvote_count":"4"},{"upvote_count":"4","content":"Who keeps putting the INCORRECT answer in the ANSWER BOX ... they really need to sort this out .. only the correct answer should go in there...","timestamp":"1633339620.0","poster":"petebear55","comment_id":"227694"},{"content":"I would go for B .. because of its ability to Scale etc","upvote_count":"1","poster":"petebear55","timestamp":"1633277040.0","comment_id":"227692"},{"content":"Answer is option B, using NLB for a Web application does not seem right in option A.","timestamp":"1633116480.0","poster":"Sagardonthineni","upvote_count":"1","comments":[{"content":"Also option B is using Aurora cluster which is better in performance than option A RDS Mysql.","poster":"Sagardonthineni","upvote_count":"1","comment_id":"209741","timestamp":"1633180200.0"}],"comment_id":"209735"},{"poster":"CYL","timestamp":"1633103880.0","comment_id":"208648","upvote_count":"1","content":"B has the highest availability setup."},{"upvote_count":"3","comment_id":"184920","content":"I think Answer is A, with only Application moved to AWS and Database on premise, A makes sense","comments":[{"comment_id":"263785","upvote_count":"2","poster":"01037","content":"Placing RDS into elastic beanstalk isn't best practice for production.","timestamp":"1634531040.0"}],"poster":"sam422","timestamp":"1633083180.0"},{"comment_id":"174026","content":"B.\nC. is wrong because it also mention a cross-Region read replica, so the solution will mostly be unusable in read-only in one of the region.","poster":"wsw","timestamp":"1632841260.0","upvote_count":"3"},{"content":"Why not C? as the demand for 200000 users and may increase in future.","upvote_count":"2","comments":[{"comment_id":"161637","timestamp":"1632720720.0","content":"Elastic beanstalk are regional . All Beanstalk resources of one environment are located only in one region. This means you cannot run an instance in Frankfurt and one in Ohio, but you can run those in multiple availability zones for one region..This means B is correct","poster":"TK2019","upvote_count":"9","comments":[{"poster":"sam422","upvote_count":"2","content":"Thanks Mate for explaining","timestamp":"1633055160.0","comment_id":"183305"}]}],"comment_id":"157822","poster":"Anila_Dhharisi","timestamp":"1632716280.0"}],"answer_images":[],"question_id":564,"isMC":true,"question_text":"A company is hosting a three-tier web application in an on-premises environment. Due to a recent surge in traffic that resulted in downtime and a significant financial impact, company management has ordered that the application be moved to AWS. The application is written in .NET and has a dependency on a MySQL database. A solutions architect must design a scalable and highly available solution to meet the demand of 200,000 daily users.\nWhich steps should the solutions architect take to design an appropriate solution?","unix_timestamp":1597336200,"choices":{"B":"Use AWS CloudFormation to launch a stack containing an Application Load Balancer (ALB) in front of an Amazon EC2 Auto Scaling group spanning three Availability Zones. The stack should launch a Multi-AZ deployment of an Amazon Aurora MySQL DB cluster with a Retain deletion policy. Use an Amazon Route 53 alias record to route traffic from the company's domain to the ALB.","D":"Use AWS CloudFormation to launch a stack containing an Application Load Balancer (ALB) in front of an Amazon ECS cluster of Spot instances spanning three Availability Zones. The stack should launch an Amazon RDS MySQL DB instance with a Snapshot deletion policy. Use an Amazon Route 53 alias record to route traffic from the company's domain to the ALB.","A":"Use AWS Elastic Beanstalk to create a new application with a web server environment and an Amazon RDS MySQL Multi-AZ DB instance. The environment should launch a Network Load Balancer (NLB) in front of an Amazon EC2 Auto Scaling group in multiple Availability Zones. Use an Amazon Route 53 alias record to route traffic from the company's domain to the NLB.","C":"Use AWS Elastic Beanstalk to create an automatically scaling web server environment that spans two separate Regions with an Application Load Balancer (ALB) in each Region. Create a Multi-AZ deployment of an Amazon Aurora MySQL DB cluster with a cross-Region read replica. Use Amazon Route 53 with a geoproximity routing policy to route traffic between the two Regions."},"answers_community":["B (76%)","A (24%)"]},{"id":"907rPin8O1eitAD7mnk0","topic":"1","unix_timestamp":1617257940,"answer_images":[],"question_text":"An International company has deployed a multi-tier web application that relies on DynamoDB in a single region. For regulatory reasons they need disaster recovery capability in a separate region with a Recovery Time Objective of 2 hours and a Recovery Point Objective of 24 hours. They should synchronize their data on a regular basis and be able to provision me web application rapidly using CloudFormation.\nThe objective is to minimize changes to the existing web application, control the throughput of DynamoDB used for the synchronization of data and synchronize only the modified elements.\nWhich design would you choose to meet these requirements?","timestamp":"2021-04-01 08:19:00","choices":{"A":"Use AWS data Pipeline to schedule a DynamoDB cross region copy once a day, create a ג€Lastupdatedג€ attribute in your DynamoDB table that would represent the timestamp of the last update and use it as a filter.","D":"Send also each Ante into an SQS queue in me second region; use an auto-scaling group behind the SQS queue to replay the write in the second region.","C":"Use AWS data Pipeline to schedule an export of the DynamoDB table to S3 in the current region once a day then schedule another task immediately after it that will import data from S3 to DynamoDB in the other region.","B":"Use EMR and write a custom script to retrieve data from DynamoDB in the current region using a SCAN operation and push it to DynamoDB in the second region."},"question_id":565,"answer_description":"","question_images":[],"answer":"A","url":"https://www.examtopics.com/discussions/amazon/view/48615-exam-aws-certified-solutions-architect-professional-topic-1/","exam_id":32,"discussion":[{"poster":"student22","upvote_count":"1","comment_id":"1307321","timestamp":"1730803620.0","content":"Selected Answer: A\nA is the only solution that supports the requirement \"synchronize only the modified elements\"."},{"comment_id":"1266662","content":"A. Use AWS data Pipeline to schedule a DynamoDB cross region copy once a day, create a Lastupdated attribute in your DynamoDB table that would represent the timestamp of the last update and use it as a filter.","upvote_count":"1","timestamp":"1723752120.0","poster":"amministrazione"},{"timestamp":"1704397080.0","poster":"tototo12","upvote_count":"1","comment_id":"1114028","content":"Selected Answer: A\nanswer A"},{"comment_id":"963912","poster":"TravelKo","content":"Selected Answer: A\nI will go with option A. We don't know number of changes in a day from the question. it could be 0 or millions. Why to export the whole table?","timestamp":"1690386120.0","upvote_count":"1"},{"comment_id":"927771","timestamp":"1687196280.0","poster":"SkyZeroZx","upvote_count":"1","content":"Selected Answer: C\nThe best design for this scenario is C.\n\nThis design meets all of the requirements:\n\nIt uses AWS Data Pipeline to schedule an export of the DynamoDB table to S3 in the current region once a day. This ensures that the data is synchronized on a regular basis.\nIt schedules another task immediately after it that will import data from S3 to DynamoDB in the other region. This ensures that the data is synchronized in both regions.\nIt uses a LastUpdated attribute in the DynamoDB table to represent the timestamp of the last update. This allows the Data Pipeline to only export the modified elements.\nIt uses S3 as a staging area. This helps to control the throughput of DynamoDB and prevents the table from becoming overloaded.","comments":[{"timestamp":"1687196280.0","comment_id":"927773","upvote_count":"1","poster":"SkyZeroZx","content":"This design would allow the company to meet their disaster recovery requirements while minimizing changes to the existing web application. The data would be synchronized on a regular basis, and only the modified elements would be exported. The throughput of DynamoDB would be controlled, and the data could be replayed even if the auto-scaling group is scaled down.\n\nHere are some additional considerations for this design:\n\nThe frequency of the data synchronization should be based on the regulatory requirements.\nThe size of the DynamoDB table should be considered when determining the capacity of the S3 bucket.\nThe Data Pipeline job should be configured to retry if it fails."},{"upvote_count":"1","comment_id":"927774","content":"Both ChatGPT and Bard agree that C seems to me the most viable option as explained","poster":"SkyZeroZx","timestamp":"1687196340.0"}]},{"comment_id":"902749","upvote_count":"1","content":"Selected Answer: A\nI think A.","poster":"Jesuisleon","timestamp":"1684613340.0"},{"upvote_count":"3","content":"I think A. \nRequirements: \"to synchronize only updated parts.\"\nA. CRR: continuously replicate changes from a DynamoDB table.\nC. export/import : backup the entire contents of DynamoDB table.","poster":"hahaaaaa","timestamp":"1659066420.0","comment_id":"638971"},{"timestamp":"1650896280.0","comment_id":"591765","comments":[{"comment_id":"686457","upvote_count":"2","poster":"wassb","content":"I believe that wont satisfy the RPO of 24h","timestamp":"1664921340.0"}],"content":"Selected Answer: C\nIt can't be A! There's no such an option in data pipeline templates any more! It's not mentioned in documentation as well. \nThere are two options in templates:\n1) Export DynamoDB table to S3\n2) Import DynamoDB backup data from S3\nThat's it! \nSo the only thing that has left is C. Import and export","upvote_count":"1","poster":"bobsmith2000"},{"content":"A. Use AWS data Pipeline to schedule a DynamoDB cross region copy once a day, create a ג€Lastupdatedג€ attribute in your DynamoDB table that would represent the timestamp of the last update and use it as a filter.","comment_id":"497628","timestamp":"1639046880.0","upvote_count":"2","poster":"cldy"},{"upvote_count":"1","comments":[{"poster":"tiana528","timestamp":"1638776520.0","upvote_count":"3","comment_id":"494967","content":"dynamodb global table is active-active, this question asks for a way of disaster recovery which is not active-active."}],"comment_id":"485481","timestamp":"1637708460.0","content":"why no use dynamoDB global table?","poster":"acloudguru"},{"timestamp":"1636222980.0","poster":"rockc","comment_id":"428885","upvote_count":"1","content":"Here are more details: https://aws.amazon.com/blogs/aws/copy-dynamodb-data-between-regions-using-the-aws-data-pipeline/"},{"upvote_count":"1","content":"https://aws.amazon.com/about-aws/whats-new/2013/09/12/announcing-dynamodb-cross-region-copy-feature-in-aws-data-pipeline/","timestamp":"1634197860.0","comment_id":"427469","poster":"nwk"},{"content":"A needs to modify the web application to update Lastupdated attribute, doesn't it?","comments":[{"timestamp":"1633849620.0","upvote_count":"2","poster":"Yahowmy","comment_id":"391931","content":"I believe the only modification here is to the DynamoDB table."},{"content":"I think C is the best option here.","poster":"01037","upvote_count":"2","comment_id":"349470","timestamp":"1633716720.0"}],"comment_id":"349469","timestamp":"1633372680.0","upvote_count":"1","poster":"01037"},{"comment_id":"340767","upvote_count":"2","content":"A.\nSeems right","timestamp":"1633129500.0","poster":"Malcnorth59"},{"upvote_count":"2","comment_id":"333960","timestamp":"1632840900.0","poster":"ppshein","content":"A is for exact answer I believe."},{"timestamp":"1632245040.0","poster":"cldy","upvote_count":"2","content":"A.\nmeets the requirements.","comment_id":"325566"}],"answers_community":["A (67%)","C (33%)"],"answer_ET":"A","isMC":true}],"exam":{"lastUpdated":"11 Apr 2025","provider":"Amazon","isImplemented":true,"numberOfQuestions":1019,"id":32,"isMCOnly":false,"isBeta":false,"name":"AWS Certified Solutions Architect - Professional"},"currentPage":113},"__N_SSP":true}