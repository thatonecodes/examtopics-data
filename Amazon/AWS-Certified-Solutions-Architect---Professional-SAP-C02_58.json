{"pageProps":{"questions":[{"id":"RJ1g1BTCHLTy6yIqJFlV","answers_community":["B (100%)"],"choices":{"D":"Use the AWS Migration Hub import tool to load the details of the company’s on-premises environment. Generate a report by using Migration Hub Strategy Recommendations.","B":"Use AWS Migration Hub and install the AWS Application Discovery Agent on the servers. Deploy the Migration Hub Strategy Recommendations application data collector. Generate a report by using Migration Hub Strategy Recommendations.","C":"Use AWS Migration Hub and run the AWS Application Discovery Service Agentless Collector on the servers. Group the servers and databases by using AWS Application Migration Service. Generate a report by using Migration Hub Strategy Recommendations.","A":"Use Migration Evaluator to request an evaluation of the environment from AWS. Use the AWS Application Discovery Service Agentless Collector to import the details into a Migration Evaluator Quick Insights report."},"question_text":"A company is planning to migrate to the AWS Cloud. The company hosts many applications on Windows servers and Linux servers. Some of the servers are physical, and some of the servers are virtual. The company uses several types of databases in its on-premises environment. The company does not have an accurate inventory of its on-premises servers and applications.\n\nThe company wants to rightsize its resources during migration. A solutions architect needs to obtain information about the network connections and the application relationships. The solutions architect must assess the company’s current environment and develop a migration plan.\n\nWhich solution will provide the solutions architect with the required information to develop the migration plan?","question_images":[],"answer_images":[],"answer_description":"","exam_id":33,"discussion":[{"upvote_count":"10","timestamp":"1723472520.0","content":"Selected Answer: B\nAlways remember. If you want to find data for migration that is related to\n1. Network, system performance, running process, etc\n2. The current on-prem load that you need to find has physical servers in it.\nAlways use an Application discovery agent.\nso A and C are out (since they use agentless discovery which is only used for on-prem VMs)\nBetween B and D: D is wrong the question itself mentions we are not aware of the current load so import data is not possible.\n\nCorrect ans is B","comment_id":"1148347","poster":"saggy4"},{"upvote_count":"5","poster":"ayadmawla","timestamp":"1717940340.0","content":"Selected Answer: B\nThe Discovery Agent captures system configuration, system performance, running processes, and details of the network connections between systems.\n\nThe Agentless Collector is only installed as an OVA on the VMware vCenter so it doesn't apply to all servers.\n\nhttps://aws.amazon.com/application-discovery/faqs/","comment_id":"1091883"},{"comment_id":"1161296","poster":"thotwielder","comments":[{"upvote_count":"3","timestamp":"1726246680.0","poster":"pangchn","content":"coz the company don't have a detailed list of servers to be imported","comment_id":"1172834"}],"timestamp":"1724818380.0","content":"Why not D?\nAWS Migration Hub (Migration Hub) import allows you to import details of your on-premises environment directly into Migration Hub without using the Application Discovery Service Agentless Collector (Agentless Collector) or AWS Application Discovery Agent (Discovery Agent)\nhttps://docs.aws.amazon.com/application-discovery/latest/userguide/discovery-import.html","upvote_count":"1"},{"timestamp":"1720564380.0","upvote_count":"1","content":"Selected Answer: B\nOption B","comment_id":"1117969","poster":"career360guru"},{"comment_id":"1117953","poster":"career360guru","content":"Selected Answer: B\nOption B","upvote_count":"1","timestamp":"1720563000.0"},{"content":"Selected Answer: B\nAnswer: B","timestamp":"1717493880.0","poster":"J0n102","upvote_count":"1","comment_id":"1087607"},{"poster":"shaaam80","comment_id":"1083065","timestamp":"1716943560.0","upvote_count":"2","content":"Selected Answer: B\nAnswer B. Application Discovery service agent installed on all servers and VMs to gather information."},{"upvote_count":"3","poster":"devalenzuela86","timestamp":"1716379980.0","content":"Selected Answer: B\nB for sure","comment_id":"1077428"},{"comment_id":"1077001","poster":"cypkir","upvote_count":"3","timestamp":"1716352380.0","content":"Selected Answer: B\nAnswer: B"}],"topic":"1","question_id":286,"answer":"B","timestamp":"2023-11-22 07:33:00","answer_ET":"B","isMC":true,"unix_timestamp":1700634780,"url":"https://www.examtopics.com/discussions/amazon/view/126825-exam-aws-certified-solutions-architect-professional-sap-c02/"},{"id":"OQvU7H1laN1bAWsox70K","question_id":287,"question_images":[],"question_text":"A financial services company sells its software-as-a-service (SaaS) platform for application compliance to large global banks. The SaaS platform runs on AWS and uses multiple AWS accounts that are managed in an organization in AWS Organizations. The SaaS platform uses many AWS resources globally.\n\nFor regulatory compliance, all API calls to AWS resources must be audited, tracked for changes, and stored in a durable and secure data store.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/126920-exam-aws-certified-solutions-architect-professional-sap-c02/","isMC":true,"answer_description":"","choices":{"C":"Create a new AWS CloudTrail trail in the organization's management account. Create a new Amazon S3 bucket with versioning turned on to store the logs. Deploy the trail for all accounts in the organization. Enable MFA delete and encryption on the S3 bucket.","B":"Create a new AWS CloudTrail trail in each member account of the organization. Create new Amazon S3 buckets to store the logs. Deploy the trail to all AWS Regions. Enable MFA delete and encryption on the S3 buckets.","A":"Create a new AWS CloudTrail trail. Use an existing Amazon S3 bucket in the organization's management account to store the logs. Deploy the trail to all AWS Regions. Enable MFA delete and encryption on the S3 bucket.","D":"Create a new AWS CloudTrail trail in the organization's management account. Create a new Amazon S3 bucket to store the logs. Configure Amazon Simple Notification Service (Amazon SNS) to send log-file delivery notifications to an external management system that will track the logs. Enable MFA delete and encryption on the S3 bucket."},"timestamp":"2023-11-22 15:19:00","exam_id":33,"answers_community":["C (95%)","5%"],"answer_ET":"C","answer":"C","unix_timestamp":1700662740,"discussion":[{"poster":"Totoroha","timestamp":"1700739780.0","comment_id":"1078381","upvote_count":"11","content":"i thinks C is correct answer"},{"upvote_count":"10","content":"Selected Answer: C\nC is correct. Do not get fooled by the phrase \"deploy the trail for all accounts\" to think that a trail is created in each account – it means that the new organisational-level trail is _configured_ to capture data for all accounts.","poster":"Dgix","comment_id":"1178447","comments":[{"content":"thanks for that info, someone tends to be misled by those phrases.","upvote_count":"1","comment_id":"1355678","poster":"juanife","timestamp":"1739378220.0"}],"timestamp":"1710948720.0"},{"timestamp":"1731795060.0","upvote_count":"1","comment_id":"1313290","poster":"AzureDP900","content":"Option C: Create a new AWS CloudTrail trail in the organization's management account. Create a new Amazon S3 bucket with versioning turned on to store the logs. Deploy the trail for all accounts in the organization. Enable MFA delete and encryption on the S3 bucket.\nThe management account can act as a central hub for logging and auditing.\nUsing an existing S3 bucket in the management account reduces operational overhead compared to creating multiple buckets across different accounts.\nVersioning turned on ensures that old log versions are not automatically deleted, providing an additional layer of compliance."},{"upvote_count":"1","content":"I will go with D as the correct answer because C has versioning turned on which is not necessary in this case. You can configure a trail to use Amazon SNS topic and be notifies when cloud trail publishes new log files to the Amazon S3 bucket.","comments":[{"comment_id":"1320305","timestamp":"1732985580.0","upvote_count":"1","poster":"dv1","content":"Yes, S3 versioning is not necessary for the org trail to function, but it is a good practice to have in case of accidental deletion of events in the bucket. Option D with SNS is irrelevant."}],"timestamp":"1727624760.0","poster":"Chungies","comment_id":"1291208"},{"poster":"career360guru","comment_id":"1117978","content":"Selected Answer: C\nOption C","timestamp":"1704847320.0","upvote_count":"1"},{"content":"Selected Answer: C\nA: Should always create new bucket for cloudtrail\nB: When you create an organization trail, a trail with the name that you give it is created in every AWS account that belongs to your organization.\nC: Correct\nD: For several reasons, use SNS only to notify admin, not to use email as a external mgmt system","timestamp":"1704740340.0","poster":"MegalodonBolado","upvote_count":"3","comment_id":"1116907"},{"poster":"duriselvan","comment_id":"1109470","upvote_count":"1","timestamp":"1703916120.0","content":"D ans :- https://docs.aws.amazon.com/awscloudtrail/latest/userguide/configure-sns-notifications-for-cloudtrail.html"},{"poster":"J0n102","comment_id":"1087605","upvote_count":"1","timestamp":"1701689760.0","content":"Selected Answer: C\nAnswer: C"},{"poster":"ProMax","content":"Selected Answer: C\nC is correct","timestamp":"1701000480.0","comment_id":"1080617","upvote_count":"3"},{"content":"Selected Answer: C\ni thinks C is correct answer","poster":"oomwowww","comment_id":"1080532","upvote_count":"3","timestamp":"1700984820.0"},{"comments":[{"comment_id":"1079831","upvote_count":"3","content":"Yes, C is the correct","timestamp":"1700900040.0","poster":"devalenzuela86"}],"content":"Selected Answer: A\nA for sure","upvote_count":"1","timestamp":"1700662740.0","comment_id":"1077435","poster":"devalenzuela86"}],"answer_images":[]},{"id":"mTue23mPohmRpxxVOR4f","timestamp":"2023-11-23 12:45:00","answer_images":[],"answer_description":"","exam_id":33,"isMC":true,"topic":"1","question_text":"A company is deploying a distributed in-memory database on a fleet of Amazon EC2 instances. The fleet consists of a primary node and eight worker nodes. The primary node is responsible for monitoring cluster health, accepting user requests, distributing user requests to worker nodes, and sending an aggregate response back to a client. Worker nodes communicate with each other to replicate data partitions.\n\nThe company requires the lowest possible networking latency to achieve maximum performance.\n\nWhich solution will meet these requirements?","choices":{"D":"Launch compute optimized EC2 instances in a spread placement group.","B":"Launch compute optimized EC2 instances in a partition placement group.","A":"Launch memory optimized EC2 instances in a partition placement group.","C":"Launch memory optimized EC2 instances in a cluster placement group."},"answer_ET":"C","url":"https://www.examtopics.com/discussions/amazon/view/127001-exam-aws-certified-solutions-architect-professional-sap-c02/","discussion":[{"timestamp":"1701226380.0","content":"Selected Answer: C\nAnswer C. Memory optimized in Cluster placement group for low latency replication between worker nodes.","comment_id":"1083068","upvote_count":"6","poster":"shaaam80"},{"timestamp":"1700739900.0","poster":"Totoroha","comment_id":"1078384","content":"Option C.","upvote_count":"5"},{"upvote_count":"2","timestamp":"1731794760.0","comment_id":"1313288","poster":"AzureDP900","content":"C is right \nCluster placement groups (CPGs) are designed to place related resources into the same Availability Zone (AZ), which reduces latency and improves network performance. Since this is a distributed in-memory database with multiple nodes that need to communicate quickly, using a CPG for memory-optimized EC2 instances would be an ideal solution.\nBy launching memory-optimized EC2 instances in a cluster placement group:\n\n\n\nYou can reduce latency by minimizing network hops between nodes\nYou can improve communication efficiency between nodes"},{"content":"Selected Answer: C\nGod I wish all SCP questions are like this.\nEasy to read.\nEasy to answer.\nEasy to move on to next question without spending extra time to read through all comments, ask google/chatGPT and read AWS doc to make sure the community vote is correct","poster":"kgpoj","timestamp":"1723456260.0","comment_id":"1264585","upvote_count":"3"},{"poster":"career360guru","content":"Selected Answer: C\nOption C","comment_id":"1117980","timestamp":"1704847440.0","upvote_count":"1"},{"content":"Selected Answer: C\nC is the correct answer for sure","poster":"Russs99","comment_id":"1087885","upvote_count":"1","timestamp":"1701717420.0"},{"poster":"J0n102","upvote_count":"1","timestamp":"1701689340.0","comment_id":"1087603","content":"Selected Answer: C\nAnswer: C, I guess memory optimized is the obvious way to go and \nCluster placement group provides the lowest possible networking latency"},{"upvote_count":"3","comment_id":"1079829","poster":"devalenzuela86","content":"Selected Answer: C\nC is ok","timestamp":"1700899800.0"}],"unix_timestamp":1700739900,"answer":"C","question_id":288,"question_images":[],"answers_community":["C (100%)"]},{"id":"DmaEIISuQ09LaWdhpL54","question_id":289,"unix_timestamp":1700663460,"answer_description":"","question_text":"A company maintains information on premises in approximately 1 million.csv files that are hosted on a VM. The data initially is 10 TB in size and grows at a rate of 1 TB each week. The company needs to automate backups of the data to the AWS Cloud.\n\nBackups of the data must occur daily. The company needs a solution that applies custom filters to back up only a subset of the data that is located in designated source directories. The company has set up an AWS Direct Connect connection.\n\nWhich solution will meet the backup requirements with the LEAST operational overhead?","question_images":[],"answers_community":["C (83%)","B (17%)"],"timestamp":"2023-11-22 15:31:00","url":"https://www.examtopics.com/discussions/amazon/view/126921-exam-aws-certified-solutions-architect-professional-sap-c02/","discussion":[{"comments":[{"upvote_count":"2","poster":"PAUGURU","timestamp":"1701453960.0","comment_id":"1085407","content":"The only problem with C is that a data sync is not a backup. If you delete a file, the sync will delete the file on AWS, but with backups you can restore it from yesterday's backup. So I think it's B.","comments":[{"timestamp":"1704651180.0","upvote_count":"2","content":"I agree AWS DataSync is not a dedicated backup solution but it can be used for data replication that serves as a backup, it's essential to understand its limitations and distinctions compared to a comprehensive backup service:\nWhen to Use DataSync for Backup-Like Purposes:\n\nInitial Data Transfer: It's efficient for bulk migration of large datasets to AWS storage services.\nIncremental Updates: It excels at replicating ongoing changes to keep a copy of data in AWS, serving as a near-real-time backup.\nCost-Effective Replication: It's often more cost-effective than traditional backup tools for ongoing data replication, especially for large datasets.","comment_id":"1116064","poster":"vibzr2023"}]}],"content":"Selected Answer: C\nBecause of: The company needs a solution that applies custom filters to back up only a subset of the data that is located in designated source directories.","comment_id":"1078203","upvote_count":"14","poster":"VasDev","timestamp":"1700725620.0"},{"comment_id":"1091889","content":"Selected Answer: C\nFor me there are two cues: \n1- \"custom filters\" which are available in Datasync\n2- AWS Backup does not back up to S3, rather to a Storage Vault.","poster":"ayadmawla","timestamp":"1702137120.0","upvote_count":"6"},{"content":"Selected Answer: C\nAutomatización: DataSync permite programar tareas periódicas, como respaldos diarios.\n\nFiltros personalizados: Puedes configurar filtros para incluir solo ciertos archivos o directorios específicos.\n\nCompatibilidad: DataSync está diseñado para grandes cantidades de archivos (como 1 millón de archivos .csv).\n\nRendimiento: Se integra con Direct Connect, aprovechando la conexión de alta capacidad.\n\nBajo overhead operativo: Solo debes instalar un agente una vez y definir tareas desde la consola de AWS. Es administrado completamente por AWS.","comment_id":"1409617","timestamp":"1742808420.0","poster":"eesa","upvote_count":"1"},{"content":"Option C is a good choice for this scenario.\n\n\nUsing AWS DataSync to replicate the data to Amazon S3 daily (Option C) meets all the requirements:\n\n\n\nIt provides an automated solution for backing up data to the cloud.\n\nIt allows you to apply custom filters to back up only a subset of the data located in designated source directories.\n\nSince DataSync is designed for scalable and efficient data transfer, it should provide better performance for large datasets.\n\n\nHere are some benefits of using AWS DataSync:\n\n\n\nEasy setup and configuration\n\nAutomatic replication to Amazon S3\n\nScalable and efficient data transfer\n\nSupports custom filters for selective data backup","poster":"AzureDP900","timestamp":"1731794640.0","upvote_count":"1","comment_id":"1313287"},{"comment_id":"1182606","timestamp":"1711381620.0","content":"Selected Answer: C\nOption C: How To \n https://docs.aws.amazon.com/datasync/latest/userguide/create-s3-location.html","upvote_count":"1","poster":"TonytheTiger"},{"comment_id":"1117985","content":"Selected Answer: C\nOption C - Due to filtering requirement.","timestamp":"1704848220.0","poster":"career360guru","upvote_count":"2"},{"content":"Answer: C\nOption B: AWS Backup offers centralized backup management, but it might not support custom filtering for specific files or directories as granularly as DataSync.","comment_id":"1116059","timestamp":"1704651000.0","poster":"vibzr2023","upvote_count":"1"},{"comments":[{"content":"based on the FQA, AWS Backup can only back up on-premises \"Storage Gateway\" volumes and \"VMware virtual machines\".","poster":"motica0418","timestamp":"1703522400.0","comment_id":"1105405","upvote_count":"1"}],"timestamp":"1703180100.0","upvote_count":"1","comment_id":"1102803","content":"B AWS Backup can do backup from on-premise (https://aws.amazon.com/backup/faqs/ Can I use AWS Backup to back up on-premises data?)","poster":"yuliaqwerty"},{"poster":"awsamar","upvote_count":"3","timestamp":"1702588920.0","content":"Selected Answer: B\nB correct. Because Datasync is not for backup","comment_id":"1096842"},{"upvote_count":"2","poster":"Russs99","content":"Selected Answer: C\nas to option B, AWS Backup doesn't natively support direct backups of on-premises data into Amazon S3.","timestamp":"1701718140.0","comment_id":"1087899"},{"comment_id":"1087601","upvote_count":"2","content":"Selected Answer: C\nAnswer: C","poster":"J0n102","timestamp":"1701689040.0"},{"timestamp":"1701227220.0","comment_id":"1083074","poster":"shaaam80","content":"Selected Answer: C\nAnswer C - with Datasync custom filters can be created to select what data needs to be backed up / replicated.","upvote_count":"2"},{"content":"Selected Answer: B\nB for sure","timestamp":"1700663460.0","upvote_count":"3","comment_id":"1077441","poster":"devalenzuela86","comments":[{"content":"For sure you ate wrong.","poster":"igor12ghsj577","upvote_count":"4","timestamp":"1705755240.0","comment_id":"1127273"}]}],"topic":"1","answer_images":[],"exam_id":33,"isMC":true,"answer":"C","choices":{"D":"Use an AWS Snowball Edge device for the initial backup. Use AWS DataSync for incremental backups to Amazon S3 daily.","C":"Install the AWS DataSync agent as a VM that runs on the on-premises hypervisor. Configure a DataSync task to replicate the data to Amazon S3 daily.","B":"Create a backup plan in AWS Backup to back up the data to Amazon S3. Schedule the backup plan to run daily.","A":"Use the Amazon S3 CopyObject API operation with multipart upload to copy the existing data to Amazon S3. Use the CopyObject API operation to replicate new data to Amazon S3 daily."},"answer_ET":"C"},{"id":"awEp2ljsz5SHrDlGC2SN","question_text":"A company’s solutions architect is reviewing a web application that runs on AWS. The application references static assets in an Amazon S3 bucket in the us-east-1 Region. The company needs resiliency across multiple AWS Regions. The company already has created an S3 bucket in a second Region.\nWhich solution will meet these requirements with the LEAST operational overhead?","exam_id":33,"discussion":[{"comment_id":"742310","content":"C is correct.\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html","poster":"zhangyu20000","timestamp":"1670809500.0","upvote_count":"19"},{"comment_id":"1275479","content":"C. Configure replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in the second Region. Set up an Amazon CloudFront distribution with an origin group that contains the two S3 buckets as origins.","upvote_count":"1","poster":"amministrazione","timestamp":"1725093540.0"},{"comment_id":"1264093","upvote_count":"1","timestamp":"1723377300.0","content":"Selected Answer: C\nWhy not D ? D. Configure replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in the second Region. If failover is required, update the application code to load S3 objects from the S3 bucket in the second Region:\n\nManual Failover: This option involves manual updates to the application code in the event of a failover, which adds operational overhead and complexity. CloudFront provides automatic failover and load balancing, making it a more streamlined solution.","poster":"MAZIADI"},{"upvote_count":"1","comment_id":"1199405","poster":"sarlos","content":"C IS THE answer","timestamp":"1713660540.0"},{"comment_id":"1175375","upvote_count":"1","poster":"gofavad926","timestamp":"1710628980.0","content":"Selected Answer: C\nC is correct"},{"poster":"VerRi","upvote_count":"1","timestamp":"1708685580.0","comment_id":"1157063","content":"Selected Answer: C\nStraightforward"},{"poster":"8608f25","comment_id":"1145131","content":"Selected Answer: C\nOption C is the most efficient solution because it leverages S3’s built-in replication feature to automatically replicate objects to a second bucket in another Region, ensuring that the data is resiliently stored across multiple Regions. By using Amazon CloudFront with an origin group containing both S3 buckets, the application benefits from CloudFront’s global content delivery network, which improves load times and provides a built-in failover mechanism. This setup minimizes operational overhead while achieving the desired resiliency and performance improvements.\nOption C provides a seamless, automated solution for achieving resiliency across multiple AWS Regions with minimal operational effort, leveraging AWS services designed for replication, content delivery, and failover.","upvote_count":"1","timestamp":"1707440820.0"},{"content":"C is correct because,\n\nYou can server Dynamic Websites with Static Content with CDN by having origins for both and in your webserver app refer to DNS for s3 origin from CF to deliver static content. For webserver on EC2 (Custom Origins can be used). \nSo in above scenario, if you would like to have resiliency. Add another S3 Origin with bucket in different region. Create Origin Group with both S3 Origins. Set priority on Origins and select 4XX and 5XX error codes for failover. You can use DNS returned for Origin Group from Cloud front in your web app and that would do automatic failover with least overheads. \n\nD also solves the purpose, but you will need to build failover mechanism in your app. However, with above Cloudfront Origin group is taking care of that for you.","comment_id":"1135479","upvote_count":"1","poster":"Vaibs099","timestamp":"1706586120.0"},{"comment_id":"1101442","upvote_count":"2","poster":"ninomfr64","content":"Selected Answer: C\nAll options does the job, but:\nA would require code maintenance and managing public hosted zone -> No\nB would require Lambda and CloudFront operations -> No\nC would require only CloudFront operations -> Yes\nD requires a lot of work for failover that appears to be manual -> No","timestamp":"1703068500.0"},{"poster":"subbupro","comment_id":"1088483","timestamp":"1701780420.0","content":"C is mostly correct, A is not correct - B and D required the code changes. C will take care of the cloud front orgin failover.","upvote_count":"1"},{"content":"C is good","upvote_count":"1","comment_id":"1080932","poster":"abeb","timestamp":"1701021660.0"},{"upvote_count":"1","comment_id":"1069007","content":"Selected Answer: C\nobvious","timestamp":"1699847820.0","poster":"severlight"},{"comment_id":"1021438","poster":"totten","timestamp":"1696075680.0","upvote_count":"4","content":"Selected Answer: C\nHere's why Option C is the most suitable choice:\n\nReplication: Amazon S3 Cross-Region replication is designed to replicate objects from one S3 bucket to another in a different Region. This ensures data resiliency across Regions with minimal operational overhead. Once configured, replication happens automatically.\n\nCloudFront: Setting up an Amazon CloudFront distribution with an origin group containing the two S3 buckets allows you to use a single CloudFront distribution to serve content from both Regions. CloudFront provides low-latency access to your content, and using an origin group allows for failover if one of the S3 buckets becomes unavailable.","comments":[{"poster":"totten","comment_id":"1021439","timestamp":"1696075680.0","upvote_count":"1","content":"Option A suggests configuring the application to write each object to both S3 buckets, which can result in higher operational overhead and may not provide immediate failover capabilities.\n\nOption B involves creating a Lambda function to copy objects, which adds complexity and requires code maintenance for each object written to the S3 bucket in us-east-1.\n\nOption D relies on manual updates to the application code for failover, which is less automated and could result in higher operational overhead.\n\nTherefore, Option C is the most efficient and operationally streamlined solution to achieve data resiliency and availability across multiple AWS Regions."}]},{"content":"Selected Answer: C\nC, LEAST operational overhead","poster":"Simon523","comment_id":"1003661","upvote_count":"1","timestamp":"1694317680.0"},{"content":"Selected Answer: C\nC should incur the least operational cost while D still requires the cx to update the code in whatever way they deem as appropriate","comment_id":"993479","upvote_count":"1","timestamp":"1693339620.0","poster":"TWOCATS"},{"content":"Selected Answer: C","comment_id":"981620","timestamp":"1692101160.0","poster":"Karamen","upvote_count":"1"},{"content":"Selected Answer: C\nIts completely asking CRR Right one is C","poster":"xplusfb","comment_id":"976966","timestamp":"1691608260.0","upvote_count":"1"},{"poster":"Brightalw","timestamp":"1691474520.0","content":"Selected Answer: D\nEB support .Net. and from question, it was ordered to move the app from on-premises to AWS. EB is more appropriated for this case.","upvote_count":"1","comment_id":"975252"},{"content":"Selected Answer: C\nCCCCCCCCCCCCCC","upvote_count":"1","poster":"Jonalb","comment_id":"946343","timestamp":"1688807160.0"},{"upvote_count":"1","timestamp":"1688379600.0","content":"Selected Answer: C\nits a C correct answ...","comment_id":"941711","poster":"Jonalb"},{"upvote_count":"1","timestamp":"1688004300.0","comment_id":"937371","poster":"NikkyDicky","content":"C no doubt"},{"timestamp":"1687871280.0","poster":"hglopes","upvote_count":"1","comment_id":"935384","content":"Selected Answer: A\nWith A you achieve better overall resiliency because if a region goes down you can still write to the other bucket and ensure all webapp features. Also does not require adding cloudfront if they don't use it already leading to less operational overhead. it may however decrease performance in writing to s3 writing and perhaps data consistency issues in the future"},{"timestamp":"1687773180.0","upvote_count":"2","comment_id":"934268","poster":"Jonalb","content":"Selected Answer: C\nOption C is the most suitable solution with the least operational overhead compared to option D because it leverages the built-in replication functionality of Amazon S3.\n\nIn option C, by configuring replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in the second Region, the replication process is handled automatically by Amazon S3. This ensures that the static assets are consistently synchronized between the two regions without the need for manual intervention or custom code.\n\nOn the other hand, option D suggests configuring replication on the S3 bucket in us-east-1 and updating the application code to load objects from the second Region in case of failover. While this option can achieve resiliency across multiple regions, it introduces additional complexity and operational overhead."},{"timestamp":"1685649240.0","upvote_count":"1","content":"Selected Answer: C\nC is the correct answer.\nMore information at https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html","comment_id":"912362","poster":"AmalArul"},{"comment_id":"892963","content":"Selected Answer: C\nC is the Only option as per the requirement","upvote_count":"1","timestamp":"1683625860.0","poster":"gameoflove"},{"comment_id":"887868","timestamp":"1683058320.0","poster":"rbm2023","content":"Selected Answer: C\nC is the most suitable, because it will use both buckets as CF distribution","upvote_count":"1"},{"poster":"Sin_ha","comment_id":"869820","content":"The solution that will meet the requirements with the least operational overhead is to configure replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in the second Region and set up an Amazon CloudFront distribution with an origin group that contains the two S3 buckets as origins. Therefore, the correct answer is C.","upvote_count":"3","timestamp":"1681428780.0"},{"content":"Selected Answer: C\nS3 + Cloudfront","comment_id":"852968","poster":"mfsec","upvote_count":"2","timestamp":"1679989140.0"},{"comment_id":"851750","timestamp":"1679895060.0","poster":"Cloud_noob","content":"Selected Answer: C\nyou can configure Amazon CloudFront to use two different Amazon S3 buckets from different regions as the origin for your content.\n\nTo do this, you would need to create two separate Amazon S3 bucket origins in your CloudFront distribution settings, each one pointing to a different S3 bucket in a different region.\n\nWhen creating the CloudFront distribution, you can add multiple origins to the distribution configuration. You can specify the origin domain name for each origin, which will correspond to the domain name of the S3 bucket you want to use as the origin.\n\nYou can also specify the origin protocol policy, which determines whether CloudFront uses HTTP or HTTPS to communicate with the origin.\n\nKeep in mind that you will need to configure cross-region replication between the two S3 buckets in order to keep the content in both buckets synchronized. Additionally, you will need to make sure that both S3 buckets are publicly accessible or that CloudFront has the appropriate permissions to access the buckets.","upvote_count":"2"},{"comment_id":"793573","content":"Selected Answer: C\nModifying any existing application code IS a operational overhead.","poster":"jooncco","upvote_count":"3","timestamp":"1675133700.0"},{"timestamp":"1674213420.0","comment_id":"782160","upvote_count":"4","content":"Selected Answer: C\nThis solution will meet the requirements with the least operational overhead as it allows the company to use Amazon CloudFront to automatically distribute the static assets of the web application across multiple regions, and if the primary S3 bucket in us-east-1 becomes unavailable, CloudFront will automatically route the traffic to the secondary S3 bucket in the second region. This solution eliminates the need for additional Lambda function or updating the application code for failover.","poster":"ask4cloud"},{"poster":"masetromain","upvote_count":"3","comment_id":"774835","content":"Selected Answer: C\nC. Configure replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in the second Region. Set up an Amazon CloudFront distribution with an origin group that contains the two S3 buckets as origins. This option provides automatic replication of objects across the two S3 buckets, and CloudFront automatically routes requests to the nearest origin, providing low latency and high availability for the application. This solution requires minimal operational overhead to maintain as the replication and failover is handled automatically by S3 and CloudFront.","timestamp":"1673642220.0"},{"poster":"VVish","upvote_count":"2","comment_id":"766972","timestamp":"1672945740.0","content":"C - LEAST operational overhead"},{"comment_id":"759778","timestamp":"1672228800.0","content":"Selected Answer: C\ninvolves updating the application code to load S3 objects from the second region in case of a failover, which is not necessary if you are using CloudFront with an origin group as in option C.","upvote_count":"3","poster":"aimik"},{"upvote_count":"4","timestamp":"1670955600.0","poster":"masetromain","content":"Selected Answer: C\nAnswer C","comment_id":"744348"}],"answer_images":[],"question_images":[],"choices":{"C":"Configure replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in the second Region. Set up an Amazon CloudFront distribution with an origin group that contains the two S3 buckets as origins.","B":"Create an AWS Lambda function to copy objects from the S3 bucket in us-east-1 to the S3 bucket in the second Region. Invoke the Lambda function each time an object is written to the S3 bucket in us-east-1. Set up an Amazon CloudFront distribution with an origin group that contains the two S3 buckets as origins.","A":"Configure the application to write each object to both S3 buckets. Set up an Amazon Route 53 public hosted zone with a record set by using a weighted routing policy for each S3 bucket. Configure the application to reference the objects by using the Route 53 DNS name.","D":"Configure replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in the second Region. If failover is required, update the application code to load S3 objects from the S3 bucket in the second Region."},"question_id":290,"unix_timestamp":1670809500,"topic":"1","answer_description":"","timestamp":"2022-12-12 02:45:00","answers_community":["C (95%)","2%"],"answer":"C","answer_ET":"C","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/91102-exam-aws-certified-solutions-architect-professional-sap-c02/"}],"exam":{"isImplemented":true,"id":33,"lastUpdated":"11 Apr 2025","isMCOnly":true,"name":"AWS Certified Solutions Architect - Professional SAP-C02","provider":"Amazon","isBeta":false,"numberOfQuestions":529},"currentPage":58},"__N_SSP":true}