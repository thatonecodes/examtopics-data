{"pageProps":{"questions":[{"id":"Ijuy4KKaYfmUlAAs6IzB","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/150357-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","exam_id":21,"answer_ET":"AC","question_text":"A company is building an inventory management system and an inventory reordering system to automatically reorder products. Both systems use Amazon Kinesis Data Streams. The inventory management system uses the Amazon Kinesis Producer Library (KPL) to publish data to a stream. The inventory reordering system uses the Amazon Kinesis Client Library (KCL) to consume data from the stream. The company configures the stream to scale up and down as needed.\n\nBefore the company deploys the systems to production, the company discovers that the inventory reordering system received duplicated data.\n\nWhich factors could have caused the reordering system to receive duplicated data? (Choose two.)","answer_images":[],"question_id":56,"topic":"1","discussion":[{"upvote_count":"4","timestamp":"1731676560.0","poster":"michele_scar","comment_id":"1312636","content":"Selected Answer: AC\nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-duplicates.html"},{"upvote_count":"4","timestamp":"1730032020.0","content":"Answer is AC https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-duplicates.html\nConsumer can add duplicates due to network timeouts.\nProducer can consume duplicates due to shards and record processor related changes.","comment_id":"1303579","poster":"0c2d840"}],"choices":{"A":"The producer experienced network-related timeouts.","D":"The AggregationEnabled configuration property was set to true.","B":"The streamâ€™s value for the IteratorAgeMilliseconds metric was too high.","C":"There was a change in the number of shards, record processors, or both.","E":"The max_records configuration property was set to a number that was too high."},"unix_timestamp":1730032020,"timestamp":"2024-10-27 13:27:00","answer":"AC","answer_description":"","question_images":[],"answers_community":["AC (100%)"]},{"id":"xi9dI2CJANuDzaVKXp3p","answer_description":"","question_text":"A data engineer needs to securely transfer 5 TB of data from an on-premises data center to an Amazon S3 bucket. Approximately 5% of the data changes every day. Updates to the data need to be regularly proliferated to the S3 bucket. The data includes files that are in multiple formats. The data engineer needs to automate the transfer process and must schedule the process to run periodically.\nWhich AWS service should the data engineer use to transfer the data in the MOST operationally efficient way?","exam_id":21,"answer_images":[],"unix_timestamp":1705747920,"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/131676-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_id":57,"answers_community":["A (100%)"],"question_images":[],"discussion":[{"poster":"TonyStark0122","timestamp":"1706821020.0","comment_id":"1137927","upvote_count":"11","content":"A. AWS DataSync\n\nExplanation:\nAWS DataSync is a managed data transfer service that simplifies and accelerates moving large amounts of data online between on-premises storage and Amazon S3, EFS, or FSx for Windows File Server. DataSync is optimized for efficient, incremental, and reliable transfers of large datasets, making it suitable for transferring 5 TB of data with daily updates."},{"poster":"sam_pre","upvote_count":"1","timestamp":"1742831880.0","content":"Selected Answer: A\nDataSync perfectly fit for this requirement","comment_id":"1409704"},{"upvote_count":"1","timestamp":"1724137980.0","comment_id":"1269247","content":"Aseems correct.\n\nAWS Direct Connect is a networking service, nothing to be realted to sync data between on-premises and cloud storage services, as DataSync does (\" online service that automates and accelerates moving data between on premises and AWS Storage services.\").","poster":"San_Juan"},{"content":"Selected Answer: A\nDataSync, locations, tasks, is all what you need.","comment_id":"1226984","poster":"pypelyncar","upvote_count":"2","timestamp":"1717893540.0"},{"upvote_count":"1","content":"Selected Answer: A\nis datasync","comment_id":"1212618","timestamp":"1715902860.0","poster":"FunkyFresco"},{"poster":"augustino0890","content":"A. AWS DataSync \nAWS DataSync is a data transfer service specifically designed to simplify and accelerate moving large volumes of data between on-premises storage systems and AWS storage services like S3.","timestamp":"1714116900.0","upvote_count":"2","comment_id":"1202447"},{"poster":"KelvinPun","content":"Selected Answer: A\nThat's the job of DataSync","comment_id":"1198383","timestamp":"1713509220.0","upvote_count":"1"},{"timestamp":"1712872560.0","upvote_count":"1","content":"A - DataSync is build for this use case","comment_id":"1194023","poster":"Rafaaws"},{"content":"Selected Answer: A\nTypical DataSync use case","poster":"milofficial","timestamp":"1705747920.0","comment_id":"1127211","upvote_count":"2"}],"timestamp":"2024-01-20 11:52:00","answer_ET":"A","choices":{"D":"Amazon S3 Transfer Acceleration","A":"AWS DataSync","B":"AWS Glue","C":"AWS Direct Connect"},"answer":"A","topic":"1"},{"id":"lYc77WFV6k4IUz7q6KHt","choices":{"D":"Use AWS Database Migration Service (AWS DMS) to capture changed records in the operational systems. Publish the changes to an Amazon DynamoDB table in a different AWS region from the source database. Build Amazon QuickSight dashboards that track the orders.","B":"Use AWS Glue to build ingestion pipelines from the operational systems into Amazon DynamoDBuild dashboards in Amazon QuickSight that track the orders.","C":"Use AWS Database Migration Service (AWS DMS) to capture changed records in the operational systems. Publish the changes to an Amazon DynamoDB table in a different AWS region from the source database. Build Grafana dashboards that track the orders.","A":"Use AWS Glue to build ingestion pipelines from the operational systems into Amazon Redshift Build dashboards in Amazon QuickSight that track the orders."},"answer_ET":"A","answer":"A","question_id":58,"isMC":true,"answer_description":"","timestamp":"2024-11-25 02:20:00","answer_images":[],"answers_community":["A (70%)","D (30%)"],"discussion":[{"poster":"MerryLew","upvote_count":"2","content":"Selected Answer: A\nDynamoDB is not designed to support relational databases. Redshift, however is.","timestamp":"1736962440.0","comment_id":"1341153"},{"comment_id":"1339657","poster":"pepedaruiz999","content":"Selected Answer: A\nDynamoDB is not relational data base","timestamp":"1736710980.0","upvote_count":"3"},{"timestamp":"1735124580.0","poster":"axantroff","comment_id":"1331523","content":"Selected Answer: D\nIDK, it feels like from DEV overhead D > A","upvote_count":"1"},{"content":"Selected Answer: D\nUsing AWS DMS for real-time change data capture (CDC) and publishing the changes to DynamoDB, followed by building QuickSight dashboards, is the most efficient solution with the least development overhead for this use case","upvote_count":"2","comment_id":"1330181","timestamp":"1734811140.0","poster":"kailu"},{"upvote_count":"2","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/build-an-etl-service-pipeline-to-load-data-incrementally-from-amazon-s3-to-amazon-redshift-using-aws-glue.html","timestamp":"1732497600.0","comment_id":"1317270","poster":"emupsx1"}],"topic":"1","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/151928-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_text":"An ecommerce company operates a complex order fulfilment process that spans several operational systems hosted in AWS. Each of the operational systems has a Java Database\nConnectivity (JDBC)-compliant relational database where the latest processing state is captured.\n\nThe company needs to give an operations team the ability to track orders on an hourly basis across the entire fulfillment process.\n\nWhich solution will meet these requirements with the LEAST development overhead?","unix_timestamp":1732497600,"exam_id":21},{"id":"CUuGCxeHOl7aKo8miAEm","choices":{"B":"SQL","A":"Gremlin","D":"SPARQL","E":"Spark SQL","C":"ANSI SQL"},"exam_id":21,"question_images":[],"unix_timestamp":1732489620,"timestamp":"2024-11-25 00:07:00","url":"https://www.examtopics.com/discussions/amazon/view/151917-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","discussion":[{"timestamp":"1732489620.0","comment_id":"1317232","upvote_count":"4","content":"Selected Answer: AD\nhttps://docs.aws.amazon.com/neptune/latest/userguide/access-graph-queries.html","poster":"emupsx1"}],"answers_community":["AD (100%)"],"question_text":"A data engineer needs to use Amazon Neptune to develop graph applications.\n\nWhich programming languages should the engineer use to develop the graph applications? (Choose two.)","answer_description":"","question_id":59,"isMC":true,"topic":"1","answer_images":[],"answer":"AD","answer_ET":"AD"},{"id":"sk217wzPN9YJHjd5pp8r","discussion":[{"comment_id":"1341155","timestamp":"1736962560.0","upvote_count":"1","content":"Selected Answer: A\nThe fan out feature allows consumers to receive data from a stream with dedicated throughput","poster":"MerryLew"},{"poster":"AgboolaKun","timestamp":"1731174480.0","upvote_count":"2","content":"Selected Answer: A\nThe correct answer is A.\n\nHere is why:\n\nAmazon Kinesis Data Streams is designed for real-time streaming data.\n \nThe PutRecords API is efficient for sending multiple records to Kinesis in a single call, which is good for optimizing throughput from mobile devices.\n\nThe enhanced fan-out feature allows multiple consumers to read from the same stream with dedicated throughput for each consumer, which meets the requirement of dedicated throughput for each internal consumer.\n\nThis option uses uses Kinesis Data Streams for real-time processing, optimizes throughput from mobile devices with the PutRecords API, and provides dedicated throughput for each consumer using the enhanced fan-out feature.","comment_id":"1309159"},{"timestamp":"1730373660.0","poster":"pikuantne","content":"Selected Answer: A\nA is best, but I think it was supposed to be a SHARD for each consumer.\nB - doesn't make any sense\nC - Firehose does not have enhanced fan-out afaik\nD - does not have the dedicated throughput as it doesn't use enhanced fan-out with KDS","upvote_count":"1","comment_id":"1305382"},{"comments":[{"comment_id":"1289248","content":"Sorry meant to select option C","timestamp":"1727315220.0","upvote_count":"1","poster":"LR2023"}],"poster":"LR2023","timestamp":"1727315160.0","comment_id":"1289247","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/streams/latest/dev/kpl-with-firehose.html\n\nKPL does work with firehose","upvote_count":"1"},{"timestamp":"1726704840.0","content":"Selected Answer: A\nSeems to be A - Since KPL does not work into firehouse and only streams, and additionally the dedicated throughput is solved through fan-out","poster":"Fawk","comment_id":"1286067","upvote_count":"2"}],"question_id":60,"question_images":[],"timestamp":"2024-09-19 02:14:00","topic":"1","choices":{"A":"Configure the mobile app to call the PutRecords API operation to send data to Amazon Kinesis Data Streams. Use the enhanced fan-out feature with a stream for each internal consumer.","C":"Configure the mobile app to use the Amazon Kinesis Producer Library (KPL) to send data to Amazon Kinesis Data Firehose. Use the enhanced fan-out feature with a stream for each internal consumer.","B":"Configure the mobile app to call the PutRecordBatch API operation to send data to Amazon Kinesis Data Firehose. Submit an AWS Support case to turn on dedicated throughput for the companyâ€™s AWS account. Allow each internal consumer to access the stream.","D":"Configure the mobile app to call the PutRecords API operation to send data to Amazon Kinesis Data Streams. Host the stream-processing application for each internal consumer on Amazon EC2 instances. Configure auto scaling for the EC2 instances."},"answer_description":"","question_text":"A mobile gaming company wants to capture data from its gaming app. The company wants to make the data available to three internal consumers of the data. The data records are approximately 20 KB in size.\n\nThe company wants to achieve optimal throughput from each device that runs the gaming app. Additionally, the company wants to develop an application to process data streams. The stream-processing application must have dedicated throughput for each internal consumer.\n\nWhich solution will meet these requirements?","answer":"A","exam_id":21,"answer_ET":"A","answer_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/147824-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","unix_timestamp":1726704840,"answers_community":["A (86%)","14%"]}],"exam":{"provider":"Amazon","name":"AWS Certified Data Engineer - Associate DEA-C01","numberOfQuestions":207,"isImplemented":true,"isMCOnly":true,"lastUpdated":"11 Apr 2025","isBeta":false,"id":21},"currentPage":12},"__N_SSP":true}