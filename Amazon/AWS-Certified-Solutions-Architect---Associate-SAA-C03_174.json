{"pageProps":{"questions":[{"id":"FEL3WehLxH7MVzmMJzhT","discussion":[{"comment_id":"1223552","poster":"Scheldon","content":"Selected Answer: A\nAnswerA\n\nI think only option A have any sense. It is cheap (no cost), it is secure (traffic is not going to public network).\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.html","upvote_count":"3","timestamp":"1733232360.0"},{"timestamp":"1729267320.0","poster":"Hkayne","comment_id":"1198015","content":"Selected Answer: A\nAws gateway will have no cost because the trafic will stay on aws infrastructure.","upvote_count":"2"},{"timestamp":"1728447360.0","comment_id":"1191943","poster":"Tanidanindo","upvote_count":"2","content":"Selected Answer: A\nGateway endpoint will minimize data transfer costs"},{"comment_id":"1188996","content":"Selected Answer: A\nA- gateway endpoint for S3","timestamp":"1728002160.0","poster":"Awsbeginner87","upvote_count":"2"},{"upvote_count":"1","poster":"xBUGx","content":"Selected Answer: A\ngateway endpoint for Amazon S3","timestamp":"1727991000.0","comment_id":"1188917"},{"content":"Selected Answer: A\nGateway endpoint is free https://digitalcloud.training/vpc-interface-endpoint-vs-gateway-endpoint-in-aws/.","timestamp":"1727971620.0","poster":"AlvinC2024","comment_id":"1188752","upvote_count":"1"}],"question_images":[],"choices":{"D":"Create one NAT gateway for each Availability Zone in public subnets. In each of the route tables for the private subnets, add a default route that points to the NAT gateway in the same Availability Zone.","C":"Create an AWS PrivateLink interface endpoint for Amazon S3 in the VPIn the route tables for the private subnets, add an entry for the interface endpoint.","B":"Create a single NAT gateway in a public subnet. In the route tables for the private subnets, add a default route that points to the NAT gateway.","A":"Create a gateway endpoint for Amazon S3 in the VPC. In the route tables for the private subnets, add an entry for the gateway endpoint."},"question_text":"A solutions architect is creating an application. The application will run on Amazon EC2 instances in private subnets across multiple Availability Zones in a VPC. The EC2 instances will frequently access large files that contain confidential information. These files are stored in Amazon S3 buckets for processing. The solutions architect must optimize the network architecture to minimize data transfer costs.\n\nWhat should the solutions architect do to meet these requirements?","timestamp":"2024-04-03 18:07:00","question_id":866,"unix_timestamp":1712160420,"exam_id":31,"answer_images":[],"answer_ET":"A","url":"https://www.examtopics.com/discussions/amazon/view/137828-exam-aws-certified-solutions-architect-associate-saa-c03/","answer":"A","isMC":true,"topic":"1","answer_description":"","answers_community":["A (100%)"]},{"id":"2OijGuVtLiO6AbP157cd","question_id":867,"answer_images":[],"isMC":true,"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/138185-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_ET":"A","answer":"A","question_images":[],"unix_timestamp":1712636220,"topic":"1","exam_id":31,"answers_community":["A (80%)","B (20%)"],"choices":{"C":"Provision an Amazon DocumentDB (with MongoDB compatibility) instance with a memory optimized instance type. Monitor Amazon CloudWatch for performance-related issues. Change the instance class if necessary.","B":"Provision an Amazon RDS for MySQL DB instance with General Purpose SSD storage. Place an Amazon ElastiCache cluster in front of the DB instance. Configure the application to query ElastiCache instead.","D":"Provision an Amazon Elastic File System (Amazon EFS) file system in General Purpose performance mode. Monitor Amazon CloudWatch for IOPS bottlenecks. Change to Provisioned Throughput performance mode if necessary.","A":"Provision an Amazon RDS for MySQL DB instance with Provisioned IOPS SSD storage. Monitor write operation metrics by using Amazon CloudWatch. Adjust the provisioned IOPS if necessary."},"timestamp":"2024-04-09 06:17:00","question_text":"A company wants to relocate its on-premises MySQL database to AWS. The database accepts regular imports from a client-facing application, which causes a high volume of write operations. The company is concerned that the amount of traffic might be causing performance issues within the application.\n\nHow should a solutions architect design the architecture on AWS?","discussion":[{"content":"Selected Answer: A\nA or B. Can't be B because there is high volume of write no need for Elasticache","poster":"Hkayne","upvote_count":"5","comment_id":"1198017","timestamp":"1713456360.0"},{"poster":"GOTJ","comment_id":"1356939","upvote_count":"1","timestamp":"1739638620.0","content":"Selected Answer: B\nEven though option \"A\" makes sense, everybody discarding option \"B\" is focusing on improving the import process performance itself by using ElastiCache, which is basically correct. However, there is not a clear indicator that the “client-facing application” ONLY performs high volumes of write operations. In fact, this can be pointless (a “write only” database?).\n\n In my opinion, the main concern of the company is how THE REST of the application processes might be affected while this write-intensive operations run in the database. And those operations surely includes reads. \n\nUsing an ElastiCache in front of the RDB instance or creating a read replica to redirect reads to might help with performance of THE REST of application when write-intensive imports processes are executing, as the query result might be cached.\n\nMy vote goes to “B”"},{"comment_id":"1231325","content":"Selected Answer: A\nhigh volume of write operation -> Provisioned IOPS SSD storage","poster":"KennethNg923","upvote_count":"3","timestamp":"1718536500.0"},{"timestamp":"1717150320.0","poster":"Scheldon","comment_id":"1222064","content":"Selected Answer: A\nAnswer A\nFor sure we cannot choose generar purpose IOPS SSD hence I would choos provisioned one. addtionally it is a good idea to monitor performance with CloudWatch and adjust setup(provisioned IOPS) if there will be a need.","upvote_count":"4"},{"upvote_count":"3","poster":"sandordini","timestamp":"1714402800.0","comment_id":"1204075","content":"Selected Answer: B\nThe most effective strategy for coping with that limit is to supplement disk-based databases with in-memory caching (Elasticache for Redis, Write-through strategy) I'd go for B...","comments":[{"comment_id":"1320459","poster":"JA2018","timestamp":"1733030460.0","upvote_count":"1","content":"why I would not choose #B? While ElastiCache can be used for caching read-heavy workloads, it's not the best choice for a database with high write operations as it is primarily designed for fast reads from a cache."},{"timestamp":"1715654700.0","poster":"Mr_Marcus","content":"If it were changes to existing data, maybe. The scenario specifically says data imports. Going with \"A\".","comment_id":"1211186","upvote_count":"2"}]},{"content":"Selected Answer: A\nAmazon RDS for MySQL DB instance with Provisioned IOPS SSD storage","timestamp":"1712636220.0","upvote_count":"4","poster":"Tanidanindo","comment_id":"1191946"}]},{"id":"zsTkDkE65Ke9jhcDcEji","question_images":[],"question_id":868,"url":"https://www.examtopics.com/discussions/amazon/view/138010-exam-aws-certified-solutions-architect-associate-saa-c03/","topic":"1","discussion":[{"comment_id":"1190187","timestamp":"1728180660.0","content":"Selected Answer: D\n\"ensure that third parties do not have access to the data before the data is encrypted and sent to AWS\"","poster":"xBUGx","upvote_count":"8"},{"upvote_count":"6","comment_id":"1215549","timestamp":"1732270020.0","content":"Selected Answer: D\n\"Amazon S3 managed encryption key\" (SSE-S3) is a server-side encryption. Therefore it is not a client-side encryption. To encrypt the data before sending to S3, it has to be client-side encryption.","poster":"f07ed8f"},{"upvote_count":"3","timestamp":"1732200060.0","poster":"Hkayne","content":"Selected Answer: D\nMust encrypt the data on client side before uploading it to S3","comment_id":"1214935"}],"answer":"D","question_text":"A company runs an application in the AWS Cloud that generates sensitive archival data files. The company wants to rearchitect the application's data storage. The company wants to encrypt the data files and to ensure that third parties do not have access to the data before the data is encrypted and sent to AWS. The company has already created an Amazon S3 bucket.\n\nWhich solution will meet these requirements?","choices":{"A":"Configure the S3 bucket to use client-side encryption with an Amazon S3 managed encryption key. Configure the application to use the S3 bucket to store the archival files.","B":"Configure the S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Configure the application to use the S3 bucket to store the archival files.","C":"Configure the S3 bucket to use dual-layer server-side encryption with AWS KMS keys (SSE-KMS). Configure the application to use the S3 bucket to store the archival files.","D":"Configure the application to use client-side encryption with a key stored in AWS Key Management Service (AWS KMS). Configure the application to store the archival files in the S3 bucket."},"isMC":true,"answer_ET":"D","answers_community":["D (100%)"],"exam_id":31,"answer_images":[],"answer_description":"","timestamp":"2024-04-06 04:11:00","unix_timestamp":1712369460},{"id":"Rgi5jgMDQoieJl0ZiZbK","answer_ET":"B","topic":"1","question_text":"A company uses Amazon RDS with default backup settings for its database tier. The company needs to make a daily backup of the database to meet regulatory requirements. The company must retain the backups for 30 days.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","url":"https://www.examtopics.com/discussions/amazon/view/139172-exam-aws-certified-solutions-architect-associate-saa-c03/","question_id":869,"answers_community":["B (100%)"],"answer_description":"","answer_images":[],"answer":"B","choices":{"A":"Write an AWS Lambda function to create an RDS snapshot every day.","C":"Use AWS Systems Manager Maintenance Windows to modify the RDS backup retention period.","D":"Create a manual snapshot every day by using the AWS CLI. Modify the RDS backup retention period.","B":"Modify the RDS database to have a retention period of 30 days for automated backups."},"exam_id":31,"isMC":true,"discussion":[{"upvote_count":"5","poster":"Hkayne","timestamp":"1713526680.0","comment_id":"1198572","content":"Selected Answer: B\nBy default, Amazon RDS creates and saves automated backups of your DB instance securely in Amazon S3 for a user-specified retention period. You can set the backup retention period from 1 to 35 days. The maximum retention period currently available for automated snapshots is 35 days. When automated backups are turned on for your DB Instance, Amazon RDS automatically performs a full, daily snapshot of your data and captures transaction logs."},{"content":"Selected Answer: B\nIts B\n\"Amazon RDS performs a full daily backup of your data during a backup window that you define when you create the DB instance. You can configure a retention period of up to 35 days for the automated backup.\"","upvote_count":"2","poster":"EdricHoang","timestamp":"1719048180.0","comment_id":"1235291"}],"question_images":[],"timestamp":"2024-04-19 13:38:00","unix_timestamp":1713526680},{"id":"PHFNHaeO2PUjrEw6VjEo","answer_images":[],"answer":"C","discussion":[{"poster":"mk168898","timestamp":"1729651560.0","content":"I was thinking between B and C. Saw that DAX is for Dynamo DB which is different from the question Aurora DB. \nSo i picked C","comment_id":"1301838","upvote_count":"2"},{"upvote_count":"4","content":"Selected Answer: C\nRead Replica for multiple users access and read the data","poster":"KennethNg923","comment_id":"1231329","timestamp":"1718536800.0"},{"content":"Answer C, wording is that read queries are slowing down write queries -> we need to optimize for read queries -> we need to add read replicas.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Performance.html#Aurora.Managing.Performance.ReadScaling","upvote_count":"4","timestamp":"1718197920.0","comment_id":"1229166","poster":"ug56c"},{"content":"Selected Answer: B\nwrite queries=>High performance of read & Write Dynamo, I think DAX is a better solution.","timestamp":"1717735800.0","upvote_count":"1","poster":"sheilawu","comments":[{"comment_id":"1240810","upvote_count":"2","poster":"Rhydian25","content":"Dynamo is NoSQL...","timestamp":"1719925740.0"}],"comment_id":"1225905"},{"timestamp":"1717155480.0","poster":"Scheldon","content":"Selected Answer: C\nAnswerC\nA - We can create only RO replica of DB hence this is not possible.\nD - Redshift is not for a DB from my understanding but more like analitics tools\nB - Could be a thinkg but there is no point of Read-Only from DAX and write to DB cluster. beside it is hard to say if it would be cost effective as we are paying per hour not R/W requests as it is with replica.\nHence I would go with C","comment_id":"1222105","upvote_count":"2"}],"isMC":true,"exam_id":31,"answer_ET":"C","choices":{"D":"Create an Amazon Redshift cluster. Copy the users' data to the Redshift cluster. Update the application to connect to the Redshift cluster and to perform read-only queries on the Redshift cluster.","A":"Create a second Aurora DB cluster. Configure a copy job to replicate the users’ data to the new database. Update the application to use the second database to read the data.","B":"Create an Amazon DynamoDB Accelerator (DAX) cluster in front of the existing Aurora DB cluster. Update the application to use the DAX cluster for read-only queries. Write data directly to the Aurora DB cluster.","C":"Create an Aurora read replica in the existing Aurora DB cluster. Update the application to use the replica endpoint for read-only queries and to use the cluster endpoint for write queries."},"topic":"1","question_images":[],"timestamp":"2024-05-31 13:38:00","question_text":"A company that runs its application on AWS uses an Amazon Aurora DB cluster as its database. During peak usage hours when multiple users access and read the data, the monitoring system shows degradation of database performance for the write queries. The company wants to increase the scalability of the application to meet peak usage demands.\n\nWhich solution will meet these requirements MOST cost-effectively?","url":"https://www.examtopics.com/discussions/amazon/view/141661-exam-aws-certified-solutions-architect-associate-saa-c03/","answers_community":["C (86%)","14%"],"unix_timestamp":1717155480,"question_id":870,"answer_description":""}],"exam":{"lastUpdated":"11 Apr 2025","id":31,"numberOfQuestions":1019,"name":"AWS Certified Solutions Architect - Associate SAA-C03","isBeta":false,"provider":"Amazon","isMCOnly":true,"isImplemented":true},"currentPage":174},"__N_SSP":true}