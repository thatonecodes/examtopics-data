{"pageProps":{"questions":[{"id":"LFstZv6LOrzcl2rCxeyL","answer":"C","discussion":[{"comment_id":"1387448","upvote_count":"1","content":"Selected Answer: C\nC: It is more cost-effectively, because we reserve slots for the critical/specific Lambda function (configuration instead of more resources).\n\nWith answer B we increase concurrency allocation, but it will increase costs, additionally if more Lambda functions are created this issue will ocurr again","timestamp":"1741701060.0","poster":"daed09"},{"content":"Selected Answer: C\nC: Sí, la concurrencia reservada aísla la Lambda del impacto de otras funciones, siendo la opción más rentable.","timestamp":"1739918160.0","poster":"italiancloud2025","comment_id":"1358492","upvote_count":"1"}],"question_id":121,"answer_ET":"C","answer_description":"","answers_community":["C (100%)"],"isMC":true,"question_text":"A company has an application that uses an Amazon API Gateway REST API and an AWS Lambda function to retrieve data from an Amazon DynamoDB instance. Users recently reported intermittent high latency in the application's response times. A data engineer finds that the Lambda function experiences frequent throttling when the company's other Lambda functions experience increased invocations.\n\nThe company wants to ensure the API's Lambda function operate without being affected by other Lambda functions.\n\nWhich solution will meet this requirement MOST cost-effectively?","unix_timestamp":1739918160,"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/156780-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","topic":"1","timestamp":"2025-02-18 23:36:00","question_images":[],"exam_id":21,"choices":{"D":"Increase the Lambda function timeout and allocated memory.","A":"Increase the number of read capacity unit (RCU) in DynamoDB.","B":"Configure provisioned concurrency for the Lambda function.","C":"Configure reserved concurrency for the Lambda function."}},{"id":"Zgk6zC4rpHb1WSSIuZ1T","choices":{"A":"Use an Amazon Kinesis Data Firehose delivery stream to process the dataset. Create an AWS Lambda transform function to identify the PII. Use an AWS SDK to obfuscate the PII. Set the S3 data lake as the target for the delivery stream.","C":"Use the Detect PII transform in AWS Glue Studio to identify the PII. Create a rule in AWS Glue Data Quality to obfuscate the PII. Use an AWS Step Functions state machine to orchestrate a data pipeline to ingest the data into the S3 data lake.","D":"Ingest the dataset into Amazon DynamoDB. Create an AWS Lambda function to identify and obfuscate the PII in the DynamoDB table and to transform the data. Use the same Lambda function to ingest the data into the S3 data lake.","B":"Use the Detect PII transform in AWS Glue Studio to identify the PII. Obfuscate the PII. Use an AWS Step Functions state machine to orchestrate a data pipeline to ingest the data into the S3 data lake."},"unix_timestamp":1706824500,"answers_community":["B (68%)","C (30%)","2%"],"question_id":122,"answer_description":"","answer":"B","topic":"1","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/132653-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","timestamp":"2024-02-01 22:55:00","discussion":[{"upvote_count":"12","comments":[{"poster":"Eleftheriia","upvote_count":"1","content":"Yes, and regarding the \"Create a rule in AWS Glue Data Quality to obfuscate the PII. \" which is included in answer C, it cannot be done like this because in the aws glue console there is a section, \"detect sensitive data\" and then \"types of sensitive information to detect\". Therefore through this console you can obfuscate PII. \nRelevant tutorial: https://www.youtube.com/watch?v=-TZZBfcnxBw","timestamp":"1733212920.0","comment_id":"1321285"}],"timestamp":"1710777840.0","poster":"milofficial","content":"Selected Answer: B\nHow does Data Quality obfuscate PII? You can do this directly in Glue Studio: https://docs.aws.amazon.com/glue/latest/dg/detect-PII.html","comment_id":"1176613"},{"timestamp":"1719081060.0","content":"Selected Answer: B\nOption C involves additional steps and complexity with creating rules in AWS Glue Data Quality, which adds more operational effort compared to directly using AWS Glue Studio's capabilities.","upvote_count":"5","comment_id":"1235549","poster":"Khooks"},{"content":"Selected Answer: B\nActually it is B. No need to create a rule in AWS Glue.","comment_id":"1411897","timestamp":"1743292620.0","upvote_count":"1","poster":"Kalyso"},{"content":"Selected Answer: C\nB. Use the Detect PII transform in AWS Glue Studio to identify the PII. Obfuscate the PII. Use an AWS Step Functions state machine to orchestrate a data pipeline to ingest the data into the S3 data lake. Detect PII transform only detects. Obfuscate the PII ok but how ? Answer C explain how","poster":"plutonash","timestamp":"1736683980.0","comment_id":"1339468","upvote_count":"1"},{"content":"Selected Answer: C\nWhy C is better than B:\nObfuscation clarity: Option C explicitly mentions using a Glue Data Quality rule to obfuscate PII, while option B does not specify how obfuscation is implemented.\nAccuracy: Glue Data Quality provides a more structured way to handle obfuscation compared to relying solely on Glue Studio's PII detection.\nThus, C is the most accurate and operationally efficient solution.","timestamp":"1736604960.0","poster":"Udyan","upvote_count":"1","comment_id":"1339178"},{"upvote_count":"1","comment_id":"1282335","content":"The keyt","timestamp":"1726092060.0","poster":"markill123"},{"content":"Selected Answer: B\nB provides a streamlined, mostly visual approach using purpose-built tools for data processing and PII handling, making it the solution with the least operational effort.","upvote_count":"2","poster":"antun3ra","comment_id":"1262275","timestamp":"1723080600.0"},{"timestamp":"1722109560.0","content":"Selected Answer: C\nhttps://aws.amazon.com/blogs/big-data/automated-data-governance-with-aws-glue-data-quality-sensitive-data-detection-and-aws-lake-formation/","comments":[{"upvote_count":"1","poster":"portland","comment_id":"1256455","content":"Actually it is B","timestamp":"1722110280.0"}],"poster":"portland","comment_id":"1256447","upvote_count":"1"},{"content":"Selected Answer: B\nhttps://docs.aws.amazon.com/glue/latest/dg/detect-PII.html","upvote_count":"2","poster":"qwertyuio","comment_id":"1246544","timestamp":"1720765800.0"},{"upvote_count":"1","timestamp":"1719789600.0","content":"Selected Answer: C\nanwser is C","poster":"bakarys","comment_id":"1239905"},{"content":"I don't think we need to use much more services to fulfill these requirements. Just AWS Glue is enough, it can detect and obfuscate PII data already.\nSource: https://docs.aws.amazon.com/glue/latest/dg/detect-PII.html#choose-action-pii","comment_id":"1231172","timestamp":"1718499240.0","poster":"bigfoot1501","upvote_count":"3"},{"comment_id":"1213614","content":"Selected Answer: C\nWe cannot directly handle PII with Glue Studio, and Glue Data Quality can be used to handle PII.","upvote_count":"3","timestamp":"1716097140.0","poster":"VerRi"},{"poster":"Just_Ninja","content":"Selected Answer: A\nA very easy was is to use the SDK to identify PII.\n\nhttps://docs.aws.amazon.com/code-library/latest/ug/comprehend_example_comprehend_DetectPiiEntities_section.html","comment_id":"1208166","upvote_count":"1","timestamp":"1715140500.0"},{"comment_id":"1206501","upvote_count":"3","content":"Selected Answer: C\nThe transform Detect PII in AWS Glue Studio is specifically used to identify personally identifiable information (PII) within the data. It can detect and flag this information, but on its own, it does not perform the obfuscation or removal of these details.\n\nTo effectively obfuscate or alter the identified PII, an additional transformation would be necessary. This could be accomplished in several ways, such as:\n\nWriting a custom script within the same AWS Glue job using Python or Scala to modify the PII data as needed.\nUsing AWS Glue Data Quality, if available, to create rules that automatically obfuscate or modify the data identified as PII. AWS Glue Data Quality is a newer tool that helps improve data quality through rules and transformations, but whether it's needed will depend on the functionality's availability and the specificity of the obfuscation requirements","timestamp":"1714826340.0","poster":"kairosfc"},{"upvote_count":"2","content":"Answer is option C. Period","comment_id":"1195086","poster":"okechi","timestamp":"1713031800.0"},{"comment_id":"1186108","poster":"arvehisa","timestamp":"1711804380.0","content":"Selected Answer: B\nB is correct.\nC: glue data quality cannot obfuscate the PII\nD: need to write code but the question is the \"LEAST operational effort\"","upvote_count":"4"},{"poster":"certplan","upvote_count":"2","timestamp":"1710960240.0","comment_id":"1178611","content":"In python ---\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom pyspark.sql import SparkSession\n\n# Initialize Spark session\nspark = SparkSession.builder \\\n .appName(\"Example Glue Job\") \\\n .getOrCreate()\n\n# Initialize Glue context\nglueContext = GlueContext(SparkContext.getOrCreate())\n\n# Retrieve Glue job arguments\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\n\n# Define your EMR step\nemr_step = [\n {\n \"Name\": \"My EMR Step\",\n \"ActionOnFailure\": \"CONTINUE\",\n \"HadoopJarStep\": {\n \"Jar\": \"s3://your-bucket/emr-scripts/your_script.jar\",\n \"Args\": [\n \"arg1\",\n \"arg2\"\n ]\n }\n }\n]\n\n# Execute the EMR step\nresponse = glueContext.start_job_run(args['JOB_NAME'], job_run_args={'--extra-py-files': 'your_script.py'})\nprint(response)"},{"content":"B. Utilizes AWS Glue Studio for PII detection, AWS Step Functions for orchestration, and S3 for storage. Glue Studio simplifies PII detection, and Step Functions can streamline the data pipeline orchestration, potentially reducing operational effort compared to option A.\n\nC. Similar to option B, but it additionally includes AWS Glue Data Quality for obfuscating PII. This might add a bit more complexity but can also streamline the process if Glue Data Quality offers convenient features for PII obfuscation.","timestamp":"1710958440.0","comment_id":"1178582","poster":"certplan","upvote_count":"2"},{"poster":"jellybella","content":"Selected Answer: B\nAWS Glue Data Quality is a feature that automatically validates the quality of the data during a Glue job run, but it's not typically used for data obfuscation.","comment_id":"1176241","timestamp":"1710734220.0","upvote_count":"4"},{"comment_id":"1167996","timestamp":"1709816280.0","upvote_count":"1","poster":"GiorgioGss","content":"Selected Answer: C\nhttps://dev.to/awscommunity-asean/validating-data-quality-with-aws-glue-databrew-4df4\nhttps://docs.aws.amazon.com/glue/latest/dg/detect-PII.html"},{"comment_id":"1147090","poster":"BartoszGolebiowski24","content":"I think this is A.\nWe ingest data to s3 with a PPI transformation. \nWe do not need to use glue, or step function here in that case.","timestamp":"1707642540.0","upvote_count":"1","comments":[{"upvote_count":"1","content":"But in the other case, if this is a one-time operation, Answer: C should be better.\nThe phrase \"ingestion\" case me think, that this is the stream of data. \n\nTo sum up.\nOne time: Answer C.\nStream: Answer A.","timestamp":"1707642660.0","comment_id":"1147093","poster":"BartoszGolebiowski24"}]},{"timestamp":"1707049140.0","content":"Selected Answer: C\nOption C seems to be the best solution to meet the requirement with the least operational effort. It leverages AWS Glue Studio for PII detection, AWS Glue Data Quality for obfuscation, and AWS Step Functions for orchestration, minimizing the need for custom coding and manual processes.","upvote_count":"2","poster":"rralucard_","comment_id":"1140072"},{"content":"C. Use the Detect PII transform in AWS Glue Studio to identify the PII. Create a rule in AWS Glue Data Quality to obfuscate the PII. Use an AWS Step Functions state machine to orchestrate a data pipeline to ingest the data into the S3 data lake","poster":"TonyStark0122","comment_id":"1137955","upvote_count":"1","timestamp":"1706824500.0"}],"answer_images":[],"exam_id":21,"question_text":"A data engineer must use AWS services to ingest a dataset into an Amazon S3 data lake. The data engineer profiles the dataset and discovers that the dataset contains personally identifiable information (PII). The data engineer must implement a solution to profile the dataset and obfuscate the PII.\nWhich solution will meet this requirement with the LEAST operational effort?","isMC":true,"answer_ET":"B"},{"id":"x7gOZZm0eUap17diRR3h","answer_images":[],"timestamp":"2024-01-21 02:40:00","answer_description":"","discussion":[{"upvote_count":"15","poster":"valuedate","comment_id":"1215631","content":"Selected Answer: B\nGlue Workflow only orchestrate crawlers and glue jobs","timestamp":"1716371040.0"},{"upvote_count":"7","content":"Selected Answer: B\nFor me it's B because I did not found a possibility how Glue can trigger/orchestrate EMR processes OOTB.\nBut with StepFunction there is a way: https://aws.amazon.com/blogs/big-data/orchestrate-amazon-emr-serverless-jobs-with-aws-step-functions/","poster":"DevoteamAnalytix","comment_id":"1206035","timestamp":"1714731480.0"},{"poster":"Rpathak4","upvote_count":"1","timestamp":"1742720940.0","content":"Selected Answer: A\nWhy Not the Other Options?\n\nB. AWS Step Functions More flexible but requires manual setup of states and transitions for Glue & EMR. Higher operational overhead than Glue Workflows.\nC. AWS Lambda Lambda is not ideal for long-running ETL workflows. Best suited for lightweight data transformations or event-driven tasks.\nD. Amazon MWAA (Apache Airflow) More control but requires cluster management and custom DAGs. Higher maintenance than Glue Workflows.","comment_id":"1402185"},{"upvote_count":"1","timestamp":"1742294640.0","comment_id":"1400074","content":"Selected Answer: B\nThe company wants to improve the existing architecture so A cannot be the right choice","poster":"Palee"},{"comment_id":"1339477","poster":"plutonash","upvote_count":"1","timestamp":"1736685000.0","content":"Selected Answer: B\nit is interesting to choose A for minimum effort but only step functions can trigger the work both on EMR and on GLUE jobs"},{"content":"Selected Answer: B\nWe have both Glue job and EMR job, so we need Step Functions to connect those. \nAirflow can do it, but required more dev work.","timestamp":"1735002420.0","upvote_count":"2","comment_id":"1330983","poster":"ttpro1995"},{"comment_id":"1292212","upvote_count":"1","timestamp":"1727845140.0","content":"Selected Answer: A\nglue workflows is part of the glue ecosystem so its provides seamless integration with minimal changes","poster":"Adrifersilva"},{"timestamp":"1727829540.0","content":"Answer A, Glue workflows","poster":"Shatheesh","comment_id":"1292155","upvote_count":"1"},{"upvote_count":"1","comment_id":"1271802","content":"Selected Answer: A\nGlue workflows are managed services and best for considering least operational overhead.","timestamp":"1724525220.0","poster":"Shanmahi"},{"timestamp":"1722835320.0","poster":"V0811","comment_id":"1260937","upvote_count":"1","content":"Selected Answer: A\nAWS Glue Workflows are specifically designed for orchestrating ETL jobs in AWS Glue. They allow you to define and manage complex workflows that include multiple jobs and triggers, all within the AWS Glue environment.Integration: AWS Glue workflows seamlessly integrate with other AWS Glue components, making it easier to manage ETL processes without the need for external orchestration tools.Minimal Operational Overhead: Since AWS Glue is a fully managed service, using Glue workflows will reduce the operational overhead compared to managing separate orchestrators or building custom solutions.While D. Amazon Managed Workflows for Apache Airflow (Amazon MWAA) is also a good choice for more complex orchestration, it may involve more management overhead compared to the more straightforward AWS Glue workflows. Thus, AWS Glue workflows provide the least operational overhead given the context of this scenario."},{"comment_id":"1241793","upvote_count":"1","poster":"HunkyBunky","content":"Selected Answer: B\nB - because AWS Glue can't trigger EMR","timestamp":"1720064880.0"},{"timestamp":"1716726540.0","upvote_count":"3","comment_id":"1218976","poster":"FunkyFresco","content":"Selected Answer: B\nEMR in workflows , i dont think so"},{"content":"Selected Answer: B\nThere is no way for Glue Workflow to trigger EMR","timestamp":"1716098400.0","comment_id":"1213623","upvote_count":"4","poster":"VerRi"},{"poster":"acoshi","timestamp":"1714407060.0","upvote_count":"2","comment_id":"1204111","content":"Selected Answer: A\nhttps://aws.amazon.com/blogs/big-data/orchestrate-an-etl-pipeline-using-aws-glue-workflows-triggers-and-crawlers-with-custom-classifiers/"},{"poster":"lucas_rfsb","timestamp":"1711918020.0","upvote_count":"6","comment_id":"1187012","content":"Selected Answer: A\nSince it seems to me that this pipeline is complex, with multiple workflows, I would go for Glue workflows."},{"upvote_count":"3","comment_id":"1184835","timestamp":"1711638420.0","content":"Yo me voy por la D) Amazon MWAA porque Glue Workflows solo admite Jobs de Glue y Step Function puede fucionar pero no son workflows de datos. Amazon MWAA son workflows de datos y esta integrado tanto con Glue como EMR: https://aws.amazon.com/blogs/big-data/simplify-aws-glue-job-orchestration-and-monitoring-with-amazon-mwaa/","poster":"jasango"},{"content":"Here's an example of how you can use AWS Glue to initiate an EMR (Elastic MapReduce) job:\n\nLet's assume you have an AWS Glue job that performs ETL tasks on data stored in Amazon S3. You want to leverage EMR for a specific task within this job, such as running a complex Spark job.\n\n1. Define a Glue Job: Create an AWS Glue job using the AWS Glue console, SDK, or CLI. Define the input and output data sources, as well as the transformations you want to apply.\n\n2. Incorporate EMR Step: Within the Glue job script, include a section where you define an EMR step. An EMR step is a unit of work that performs a specific task on an EMR cluster.\n\nCode follows in the next entry...","upvote_count":"2","timestamp":"1710960240.0","comment_id":"1178609","poster":"certplan"},{"content":"Selected Answer: B\norchestrating = step function","timestamp":"1710138720.0","comment_id":"1170845","poster":"GiorgioGss","upvote_count":"5"},{"comment_id":"1140077","timestamp":"1707049440.0","upvote_count":"3","content":"Selected Answer: A\nOption A, AWS Glue Workflows, seems to be the best solution to meet the requirements with the least operational overhead. It offers a seamless integration with the company's existing AWS Glue and Amazon EMR setup, providing a managed and straightforward way to orchestrate their ETL workflows without extensive additional setup or manual intervention.","comments":[{"content":"Can you provide an example of Glue initiating an EMR job? Or somewhere in the documents? AFAIK, Glue workflows are only to be used for Glue related things e.g. pull data, transform it, and store it somewhere else (ETL). Executing commands on behalf of other services can be done using boto in glue, but it feels weird using Glue like that when you have step functions which are designed for orchestrating different services.","poster":"ottarg","comment_id":"1166796","upvote_count":"3","timestamp":"1709677260.0"}],"poster":"rralucard_"},{"content":"Glue Work flows","upvote_count":"2","timestamp":"1706824740.0","poster":"TonyStark0122","comment_id":"1137956"},{"timestamp":"1705801200.0","content":"Selected Answer: B\nOrchestrating different AWS services is a typical use case for Step Functions: https://docs.aws.amazon.com/step-functions/latest/dg/connect-emr.html\nhttps://docs.aws.amazon.com/step-functions/latest/dg/connect-glue.html","comment_id":"1127573","upvote_count":"4","poster":"[Removed]"}],"answer":"B","question_images":[],"answers_community":["B (74%)","A (26%)"],"question_text":"A company maintains multiple extract, transform, and load (ETL) workflows that ingest data from the company's operational databases into an Amazon S3 based data lake. The ETL workflows use AWS Glue and Amazon EMR to process data.\nThe company wants to improve the existing architecture to provide automated orchestration and to require minimal manual effort.\nWhich solution will meet these requirements with the LEAST operational overhead?","answer_ET":"B","choices":{"C":"AWS Lambda functions","A":"AWS Glue workflows","B":"AWS Step Functions tasks","D":"Amazon Managed Workflows for Apache Airflow (Amazon MWAA) workflows"},"unix_timestamp":1705801200,"topic":"1","isMC":true,"question_id":123,"exam_id":21,"url":"https://www.examtopics.com/discussions/amazon/view/131710-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/"},{"id":"B3DNZ3VldhpQJPc7I1ku","answer_ET":"C","answers_community":["C (58%)","B (42%)"],"unix_timestamp":1706825100,"timestamp":"2024-02-01 23:05:00","answer_images":[],"question_images":[],"question_id":124,"topic":"1","question_text":"A company currently stores all of its data in Amazon S3 by using the S3 Standard storage class.\nA data engineer examined data access patterns to identify trends. During the first 6 months, most data files are accessed several times each day. Between 6 months and 2 years, most data files are accessed once or twice each month. After 2 years, data files are accessed only once or twice each year.\nThe data engineer needs to use an S3 Lifecycle policy to develop new data storage rules. The new storage solution must continue to provide high availability.\nWhich solution will meet these requirements in the MOST cost-effective way?","discussion":[{"comment_id":"1174656","timestamp":"1726862820.0","comments":[{"upvote_count":"2","poster":"ttpro1995","content":"The requirement does not state how fast data need to be retrieval. So, pick Glacier deep for even more cost saving.","comments":[{"content":"question states: \"must continue to provide high availability.\" So NOT Deep Archive","poster":"JimOGrady","comment_id":"1410026","upvote_count":"1","timestamp":"1742905800.0","comments":[{"poster":"sam_pre","timestamp":"1743164700.0","comment_id":"1411291","content":"High availability and fast retrieval are two different things","upvote_count":"1"}]}],"timestamp":"1735002540.0","comment_id":"1330985"}],"upvote_count":"19","poster":"helpaws","content":"Selected Answer: B\n\"S3 Glacier Flexible Retrieval delivers low-cost storage, up to 10% lower cost (than S3 Glacier Instant Retrieval), for archive data that is accessed 1-2 times per year and is retrieved asynchronously\"\n\nSource: https://aws.amazon.com/s3/storage-classes/glacier/"},{"poster":"WarPig666","upvote_count":"7","timestamp":"1734052440.0","comment_id":"1325939","content":"Selected Answer: C\nFlexible retrieval will be higher cost than deep archive. If records only need to be retrieved once or twice a year, this doesn't mean they need to be instantly available."},{"poster":"Rpathak4","timestamp":"1742721300.0","content":"Selected Answer: C\nWhy Not the Other Options?\n\nA. S3 One Zone-IA → Glacier Flexible Retrieval ❌ One Zone-IA is risky (data loss if the AZ fails). Glacier Flexible Retrieval is more expensive than Deep Archive.\n\nB. S3 Standard-IA → Glacier Flexible Retrieval ❌ Glacier Flexible Retrieval is not the cheapest long-term storage. Deep Archive costs much less.\n\nD. S3 One Zone-IA → Glacier Deep Archive ❌ One Zone-IA lacks high availability (single AZ failure = data loss). S3 Standard-IA is safer.","upvote_count":"1","comment_id":"1402188"},{"content":"Selected Answer: C\nGlacier deep archive has the same availability as flexible retrieval and there's no retrieval time requirement so C is the most cost effective that meets the requirements.","upvote_count":"2","timestamp":"1739910420.0","comment_id":"1358447","poster":"anonymous_learner_2"},{"comment_id":"1345000","poster":"luigiDDD","content":"Selected Answer: C\nC is the most cost effective","timestamp":"1737589020.0","upvote_count":"2"},{"timestamp":"1736685780.0","content":"Selected Answer: B\n\"data files are accessed only once or twice each year\", this is \"S3 Glacier Flexible Retrieval\" definition","comment_id":"1339487","upvote_count":"1","poster":"plutonash"},{"comment_id":"1339184","timestamp":"1736605680.0","poster":"Udyan","upvote_count":"2","content":"Selected Answer: C\nIs it mentioned in question that Retrieval time is constraint, no, so, if any engineer need to access data, say May and November, so he/she can wait for 2-3 days to get data, as in the long run, they have an year to analyze the data so, deep archive will save costs only."},{"upvote_count":"2","comment_id":"1339183","content":"Selected Answer: C\nThis question was in Stephen Maarek Udemy practice questions too, here concern not given for extraction time so, just see cost friendlyness, thus, C over B","poster":"Udyan","timestamp":"1736605320.0"},{"poster":"HagarTheHorrible","timestamp":"1734620460.0","content":"Selected Answer: B\ndeep archive doesn't make sense","upvote_count":"1","comment_id":"1329009"},{"upvote_count":"1","content":"Selected Answer: B\nFor once or twice a year it is flexible retrieval.","comment_id":"1322650","poster":"Eleftheriia","timestamp":"1733470020.0"},{"poster":"jk15997","upvote_count":"2","timestamp":"1733429460.0","comment_id":"1322512","content":"Selected Answer: C\nThere is no requirement for the retrieval time."},{"comment_id":"1321709","upvote_count":"4","poster":"altonh","content":"Selected Answer: C\nThere is no requirement for the retrieval time. So this is more cost-effective.","timestamp":"1733289720.0"},{"poster":"iamwatchingyoualways","content":"Selected Answer: C\nNo instant access is mentioned. Most Cost effective.","timestamp":"1732642380.0","comment_id":"1318218","upvote_count":"3"},{"comment_id":"1313062","timestamp":"1731762060.0","content":"Selected Answer: B\nOption B is the correct answer because it balances cost-effectiveness and availability:\n\n S3 Standard-IA offers cost savings for infrequently accessed data while maintaining high availability across multiple zones.\n S3 Glacier Flexible Retrieval is a good balance for archiving with occasional access needs.","upvote_count":"2","poster":"truongnguyen86"},{"upvote_count":"1","content":"Selected Answer: B\nB\nHigh availability means the need for readily available service. \nS3 Standard-IA deliver 99.9% availability vs S3 One Zone-IA deliver 99.5% availability\nS3 Glacier Flexible Retrieval has configurable retrieval times, from minutes to hours, with free bulk retrievals. \nBut with S3 Glacier Deep Archive it's retrieval time is within 12 hours\nhttps://aws.amazon.com/s3/storage-classes/","comment_id":"1308362","timestamp":"1730985180.0","poster":"lsj900605"},{"comment_id":"1303392","upvote_count":"1","content":"Selected Answer: B\nB due to flex","poster":"LrdKanien","timestamp":"1729969500.0"},{"comment_id":"1292794","upvote_count":"3","content":"Selected Answer: C\nIt's C, the request is about \"High availability\" not \"Less time to retrieve the data\". The other request is \"most cost effective\" so deleting A and D for the HA, remains B and C that both satisfy the HA. Now choose the most cost effective --> C","poster":"michele_scar","timestamp":"1727960940.0"},{"upvote_count":"1","poster":"theloseralreadytaken","content":"Selected Answer: B\nGalcier Deep Archive take longer retrieval time (hours to days) than Glacier Flexible Retrieval.","timestamp":"1727168100.0","comment_id":"1288506"},{"poster":"GZMartinelli","content":"Selected Answer: B\nShould be B. The questions asks for high availability, when a file is in Glacier Deep Archive, it takes more time to be available to use.","timestamp":"1726618680.0","comment_id":"1285515","upvote_count":"1"},{"comment_id":"1279871","comments":[{"content":"Where does it say instantly once or twice a year? It says it is used once or twice a year.","upvote_count":"1","poster":"cgapperi","timestamp":"1734975720.0","comment_id":"1330894","comments":[{"upvote_count":"1","content":"It does say, however, \"The new storage solution must continue to provide high availability.\" THIS is the key statement.","comment_id":"1330896","poster":"cgapperi","timestamp":"1734975840.0"}]}],"timestamp":"1725674520.0","upvote_count":"1","poster":"shammous","content":"Selected Answer: B\nTo retrieve data from the S3 Glacier Deep Archive, you need more than 12 hours! The scenario mentions that we still need to access data instantly once or twice yearly. S3 Glacier Flexible Retrieval is more appropriate in this case."},{"content":"• S3 Glacier Instant Retrieval delivers the lowest cost storage, up to 68% lower cost (than S3 Standard-Infrequent Access), for long-lived data that is accessed once per quarter and requires millisecond retrieval.\n• S3 Glacier Flexible Retrieval delivers low-cost storage, up to 10% lower cost (than S3 Glacier Instant Retrieval), for archive data that is accessed 1-2 times per year and is retrieved asynchronously. \n• S3 Glacier Deep Archive delivers the lowest cost storage, up to 75% lower cost (than S3 Glacier Flexible Retrieval), for long-lived archive data that is accessed less than once per year and is retrieved asynchronously.","comment_id":"1264964","poster":"sachin","timestamp":"1723521240.0","upvote_count":"1"},{"comment_id":"1252338","poster":"andrologin","upvote_count":"2","timestamp":"1721552520.0","content":"Selected Answer: B\nBased on this link https://aws.amazon.com/s3/storage-classes/glacier/\nGlacier Flexible Retrieval is cheaper that Instant Retrieval"},{"poster":"LR2023","upvote_count":"2","timestamp":"1720731900.0","content":"questions doesnt provide clairty on when data is accessed does it need to made available instantly or not. Deep archive times are longer.","comment_id":"1246317"},{"timestamp":"1720498980.0","poster":"imymoco","comment_id":"1244675","content":"B https://aws.amazon.com/jp/about-aws/whats-new/2021/11/amazon-s3-glacier-storage-class-amazon-s3-glacier-flexible-retrieval/","upvote_count":"1"},{"comment_id":"1235554","upvote_count":"1","poster":"Khooks","content":"Selected Answer: C\nAnswer should be C.\nB: While this option transitions to S3 Glacier Flexible Retrieval after 2 years, which provides quicker retrieval times than Glacier Deep Archive, it is more expensive. Given the infrequent access pattern after 2 years, the additional cost is not justified.","timestamp":"1719081420.0"},{"poster":"Fexo","upvote_count":"1","timestamp":"1718188500.0","comment_id":"1228948","content":"Selected Answer: B\nAnswer is B, because Object needs to be retrieved once / twice monthly , hence GFR\nS3 Glacier Flexible Retrieval delivers low-cost storage, up to 10% lower cost (than S3 Glacier Instant Retrieval), for archive data that is accessed 1-2 times per year and is retrieved asynchronously\n\nhttps://aws.amazon.com/s3/storage-classes/glacier/"},{"upvote_count":"1","comment_id":"1219400","content":"Selected Answer: C\nI will go with C because Glacier Flexible Retrieval is way more expensive than Glacier Deep Archive.","timestamp":"1716798240.0","poster":"tgv"},{"upvote_count":"2","content":"Selected Answer: C\nHA and cost effective. \nHere is no hint in the question for instant access..","timestamp":"1715141340.0","comment_id":"1208177","poster":"Just_Ninja"},{"poster":"Christina666","comment_id":"1194551","content":"Selected Answer: C\nHA - C","timestamp":"1712958120.0","upvote_count":"1"},{"upvote_count":"4","content":"Selected Answer: C\nSince it was requested high availability, then can't be Standard One Zone. And in the end of 2 years it was asked for the most cost effective, than Glacier Deep Archive.","poster":"lucas_rfsb","timestamp":"1711921260.0","comment_id":"1187042"},{"content":"Selected Answer: C\n2 requirements: 1)highly available 2) cost-effective. no mention about load time so C is correct.","comment_id":"1186112","poster":"arvehisa","upvote_count":"2","timestamp":"1711805040.0"},{"content":"Selected Answer: C\nThe question mention \"the most cost-effective way\". C is most cost-effective and still highly available. The requirement doesn't indicate the retrieval time requirement.","upvote_count":"2","comment_id":"1185478","timestamp":"1711722540.0","poster":"blackgamer"},{"timestamp":"1710543300.0","comment_id":"1174561","upvote_count":"1","content":"C is wrong. Deep archive requires very long to load.","poster":"CalvinL4"},{"upvote_count":"4","poster":"GiorgioGss","comment_id":"1170846","content":"Selected Answer: C\nThe new storage solution must continue to provide high availability = A and C out.\nAfter 2 years, data files are accessed only once or twice each year. && MOST cost-effective way = C. C is more cost-effective than B.","timestamp":"1710139020.0","comments":[{"comment_id":"1170848","poster":"GiorgioGss","content":"I mean A and D out. sry.","timestamp":"1710139020.0","upvote_count":"1"}]},{"comment_id":"1164402","upvote_count":"2","content":"Will go with B.","timestamp":"1709419440.0","poster":"CalvinL4"},{"poster":"TonyStark0122","timestamp":"1706825100.0","content":"Option B: Transition objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 6 months. Transfer objects to S3 Glacier Flexible Retrieval after 2 years.","comment_id":"1137958","upvote_count":"3"}],"url":"https://www.examtopics.com/discussions/amazon/view/132654-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","exam_id":21,"choices":{"C":"Transition objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 6 months. Transfer objects to S3 Glacier Deep Archive after 2 years.","A":"Transition objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 6 months. Transfer objects to S3 Glacier Flexible Retrieval after 2 years.","D":"Transition objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 6 months. Transfer objects to S3 Glacier Deep Archive after 2 years.","B":"Transition objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 6 months. Transfer objects to S3 Glacier Flexible Retrieval after 2 years."},"isMC":true,"answer_description":"","answer":"C"},{"id":"SPEX1gRxOyP0BIkQx8Ux","question_images":[],"isMC":true,"choices":{"D":"Unload a copy of the data from the ETL cluster to an Amazon S3 bucket every week. Create an Amazon Redshift Spectrum table based on the content of the ETL cluster.","B":"Create materialized views based on the sales team's requirements. Grant the sales team direct access to the ETL cluster.","A":"Set up the sales team BI cluster as a consumer of the ETL cluster by using Redshift data sharing.","C":"Create database views based on the sales team's requirements. Grant the sales team direct access to the ETL cluster."},"question_id":125,"answers_community":["A (70%)","D (30%)"],"answer_description":"","discussion":[{"timestamp":"1711805580.0","comment_id":"1186113","poster":"arvehisa","content":"Selected Answer: A\nA: redshift data sharing:\nhttps://docs.aws.amazon.com/redshift/latest/dg/data_sharing_intro.html\nWith data sharing, you can securely and easily share live data across Amazon Redshift clusters.\nB: materialized view is only within 1 redshift cluster, across different tables","upvote_count":"5"},{"poster":"lucas_rfsb","timestamp":"1711926180.0","content":"Selected Answer: D\nIn my opinion using Redshift Data Sharing will consume less resources. 'D' envolves using a S3 bucket.","comment_id":"1187071","comments":[{"poster":"lucas_rfsb","content":"Sorry I wanted to select A but did D","upvote_count":"7","timestamp":"1711926240.0","comment_id":"1187072"}],"upvote_count":"5"},{"content":"Seems that the performance of the critical ETL cluster should not be affected when using data sharing, so the answer is likely A:\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/data_sharing_intro.html\n\nSupporting different kinds of business-critical workloads – Use a central extract, transform, and load (ETL) cluster that shares data with multiple business intelligence (BI) or analytic clusters. This approach provides read workload isolation and chargeback for individual workloads. You can size and scale your individual workload compute according to the workload-specific requirements of price and performance.\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/considerations.html\nThe performance of the queries on shared data depends on the compute capacity of the consumer clusters.","timestamp":"1726806660.0","comment_id":"1286657","upvote_count":"2","poster":"motk123"},{"timestamp":"1725404160.0","poster":"wimalik","comment_id":"1277860","content":"A as Redshift data sharing allows you to share live data across Redshift clusters without having to duplicate the data. This feature enables the sales team to access the data from the ETL cluster directly without interrupting the critical analysis tasks or overloading the ETL cluster's resources. The sales team can join this shared data with their own data in the BI cluster efficiently.","upvote_count":"1"},{"poster":"San_Juan","comment_id":"1273861","timestamp":"1724825820.0","upvote_count":"1","content":"Selected Answer: D\n\"The solution must minimize usage of the computing resources of the ETL cluster.\" That is key. You shouldn't use ETL cluster, so unload data to S3 and run queries in a separate Redshift Spectrum database. ETL cluster do nothing meanwhile."},{"timestamp":"1716099240.0","poster":"VerRi","comment_id":"1213627","upvote_count":"3","content":"Selected Answer: A\nTypetical Redshift data sharing use case"},{"poster":"valuedate","comment_id":"1209382","upvote_count":"2","timestamp":"1715346000.0","content":"key words: \"weekly\"\n\"The solution must minimize usage of the computing resources of the ETL cluster.\"\n\nAnswer:D"},{"timestamp":"1715052300.0","comment_id":"1207697","poster":"d8945a1","content":"Selected Answer: A\nTypical usecase of datasharing in Redshift. \n\nThe question mentions that - 'team needs to join data from the ETL cluster with data that is in the sales team's BI cluster.' This is possible with datashare.","upvote_count":"4"},{"content":"Selected Answer: D\nThe spectrum table is accessed from the sales cluster with zero impact on the ETL cluster.","poster":"jasango","upvote_count":"3","comment_id":"1184845","timestamp":"1711639800.0"},{"comment_id":"1178917","timestamp":"1710986160.0","poster":"certplan","content":"Options A, B, and C involve granting the sales team direct access to the ETL cluster, which could potentially impact the performance of the ETL cluster and interfere with its critical analysis tasks. Option D provides a more isolated and scalable approach by leveraging Amazon S3 and Redshift Spectrum for data sharing while minimizing the usage of the ETL cluster's computing resources.\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum-sharing-data.html \nhttps://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-design-tables.html","upvote_count":"1"},{"upvote_count":"5","content":"Selected Answer: A\nInitially I would go with B but that definitely will use more resource.","poster":"GiorgioGss","timestamp":"1710139260.0","comment_id":"1170851"},{"upvote_count":"4","content":"Selected Answer: A\nTo share data between Redshift clusters and meet the requirements of sharing ETL cluster data with the sales team without interrupting critical analysis tasks and minimizing the usage of the ETL cluster's computing resources, Redshift Data Sharing is the way to go\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/data_sharing_intro.html\n\n\"Supporting different kinds of business-critical workloads – Use a central extract, transform, and load (ETL) cluster that shares data with multiple business intelligence (BI) or analytic clusters. This approach provides read workload isolation and chargeback for individual workloads. You can size and scale your individual workload compute according to the workload-specific requirements of price and performance\"","poster":"[Removed]","timestamp":"1705801620.0","comment_id":"1127576"}],"answer_ET":"A","answer":"A","unix_timestamp":1705801620,"exam_id":21,"topic":"1","question_text":"A company maintains an Amazon Redshift provisioned cluster that the company uses for extract, transform, and load (ETL) operations to support critical analysis tasks. A sales team within the company maintains a Redshift cluster that the sales team uses for business intelligence (BI) tasks.\nThe sales team recently requested access to the data that is in the ETL Redshift cluster so the team can perform weekly summary analysis tasks. The sales team needs to join data from the ETL cluster with data that is in the sales team's BI cluster.\nThe company needs a solution that will share the ETL cluster data with the sales team without interrupting the critical analysis tasks. The solution must minimize usage of the computing resources of the ETL cluster.\nWhich solution will meet these requirements?","answer_images":[],"timestamp":"2024-01-21 02:47:00","url":"https://www.examtopics.com/discussions/amazon/view/131711-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/"}],"exam":{"isBeta":false,"id":21,"provider":"Amazon","numberOfQuestions":207,"name":"AWS Certified Data Engineer - Associate DEA-C01","isMCOnly":true,"isImplemented":true,"lastUpdated":"11 Apr 2025"},"currentPage":25},"__N_SSP":true}