{"pageProps":{"questions":[{"id":"USBKVOgD0kG3hUYdwgxs","answers_community":["A (100%)"],"topic":"1","answer_ET":"A","exam_id":14,"discussion":[{"timestamp":"1732079460.0","comment_id":"1315047","poster":"PHD_CHENG","content":"Selected Answer: A\nA is correct","upvote_count":"6"},{"comment_id":"1355421","content":"Selected Answer: A\nIn the context of generative AI models, tokens are the smallest units of text that the model processes. A token could represent an entire word, a subword, or even a single character, depending on how the model is tokenized. Tokens are the basic building blocks for both input and output in natural language processing (NLP) tasks, such as text generation or translation.","poster":"Jessiii","upvote_count":"4","timestamp":"1739330700.0"}],"choices":{"D":"Tokens are the specific prompts or instructions given to a generative AI model to generate output.","C":"Tokens are the pre-trained weights of a generative AI model that are fine-tuned for specific tasks.","B":"Tokens are the mathematical representations of words or concepts used in generative AI models.","A":"Tokens are the basic units of input and output that a generative AI model operates on, representing words, subwords, or other linguistic units."},"question_images":[],"question_text":"What are tokens in the context of generative AI models?","answer_images":[],"timestamp":"2024-11-20 06:11:00","url":"https://www.examtopics.com/discussions/amazon/view/151661-exam-aws-certified-ai-practitioner-aif-c01-topic-1-question/","unix_timestamp":1732079460,"answer":"A","question_id":136,"isMC":true,"answer_description":""},{"id":"0XwsEYRRtmwRqCzcl3rh","answer_ET":"A","question_text":"A company wants to assess the costs that are associated with using a large language model (LLM) to generate inferences. The company wants to use Amazon Bedrock to build generative AI applications.\nWhich factor will drive the inference costs?","url":"https://www.examtopics.com/discussions/amazon/view/151662-exam-aws-certified-ai-practitioner-aif-c01-topic-1-question/","answer_description":"","topic":"1","discussion":[{"timestamp":"1739330700.0","content":"Selected Answer: A\nIn the context of using Amazon Bedrock and generative AI models, inference costs are typically driven by the number of tokens consumed during the input and output processing.\n\nNumber of tokens consumed refers to how many tokens (words, subwords, characters) the model processes during inference (both input and output). More tokens mean higher processing and hence higher costs.","comment_id":"1355422","upvote_count":"1","poster":"Jessiii"},{"poster":"OnePG","comment_id":"1351426","content":"Selected Answer: A\nA. Number of tokens consumed. More tokens used = higher cost. \n\nAll other affects training costs, not inference costs. Correct answer is A","upvote_count":"2","timestamp":"1738678620.0"},{"poster":"85b5b55","content":"Selected Answer: A\nNo. of tokens consumed while processing. Tokens are the basic units of input and output that a generative AI model operates on, representing words, subwords, or other linguistic units.","comment_id":"1349798","upvote_count":"1","timestamp":"1738401360.0"},{"comment_id":"1315048","content":"Selected Answer: A\nA is correct. Token is the basic unit of generative AI model","upvote_count":"3","timestamp":"1732079520.0","poster":"PHD_CHENG"}],"answers_community":["A (100%)"],"exam_id":14,"answer_images":[],"unix_timestamp":1732079520,"answer":"A","isMC":true,"question_images":[],"timestamp":"2024-11-20 06:12:00","question_id":137,"choices":{"A":"Number of tokens consumed","C":"Amount of data used to train the LLM","D":"Total training time","B":"Temperature value"}},{"id":"YAZRz6NXxfsjGyhV30DR","discussion":[{"upvote_count":"1","poster":"Jessiii","comment_id":"1355423","timestamp":"1739330760.0","content":"Selected Answer: C\nTo manage the flow of data from Amazon S3 to SageMaker Studio notebooks securely and efficiently, configuring SageMaker to use a VPC (Virtual Private Cloud) with an S3 endpoint is the best solution. This setup ensures that data transfer between SageMaker and S3 happens within the AWS network, without going through the public internet, which improves security and performance.\n\nS3 VPC endpoint: An S3 VPC endpoint allows secure, private access to Amazon S3 from resources in your VPC, enabling SageMaker to securely retrieve data stored in S3 buckets without leaving the AWS network. This setup helps manage data flow efficiently."},{"comment_id":"1355129","content":"Selected Answer: C\nS3 gateway endpoint should be a default in every VPC.","poster":"chris_spencer","timestamp":"1739304060.0","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: C\nDeploy and run the Amazon SageMaker Studio on VPC and Connect to S3 using S3 Gateway endpoint.","timestamp":"1738401540.0","poster":"85b5b55","comment_id":"1349800"},{"upvote_count":"1","poster":"Amitst","content":"Selected Answer: C\nC. Configure SageMaker to use a VPC with an S3 endpoint.","timestamp":"1733386080.0","comment_id":"1322274"}],"choices":{"D":"Configure SageMaker to use S3 Glacier Deep Archive.","C":"Configure SageMaker to use a VPC with an S3 endpoint.","B":"Use Amazon Macie to monitor SageMaker Studio.","A":"Use Amazon Inspector to monitor SageMaker Studio."},"exam_id":14,"isMC":true,"timestamp":"2024-12-05 09:08:00","url":"https://www.examtopics.com/discussions/amazon/view/152547-exam-aws-certified-ai-practitioner-aif-c01-topic-1-question/","unix_timestamp":1733386080,"question_images":[],"answer_ET":"C","answer_images":[],"question_id":138,"answer_description":"","answer":"C","question_text":"A company is using Amazon SageMaker Studio notebooks to build and train ML models. The company stores the data in an Amazon S3 bucket. The company needs to manage the flow of data from Amazon S3 to SageMaker Studio notebooks.\nWhich solution will meet this requirement?","answers_community":["C (100%)"],"topic":"1"},{"id":"Wqd3cxkfG8IploDcmGfs","answer_ET":"A","choices":{"C":"Amazon Elastic File System (Amazon EFS)","B":"Amazon Elastic Block Store (Amazon EBS)","D":"AWS Snowcone","A":"Amazon S3"},"answers_community":["A (100%)"],"answer":"A","unix_timestamp":1732079580,"answer_images":[],"timestamp":"2024-11-20 06:13:00","discussion":[{"poster":"Jessiii","content":"Selected Answer: A\nAmazon S3 (Simple Storage Service) is the ideal solution for storing datasets that will be used by Amazon Bedrock for model validation. It is a scalable, durable, and secure storage service that is commonly used to store large datasets, including those used for machine learning model training, validation, and inference.\n\nIn the case of Amazon Bedrock, the company would typically upload the new validation dataset to an S3 bucket, which can then be accessed by Bedrock to validate the model's responses against the new data.","upvote_count":"1","comment_id":"1355424","timestamp":"1739330820.0"},{"upvote_count":"2","comment_id":"1349802","content":"Selected Answer: A\nAmazon S3 is the best option for storing (Object Storage) the datasets that Amazon Bedrock uses for customer queries.","timestamp":"1738401720.0","poster":"85b5b55"},{"upvote_count":"2","poster":"PHD_CHENG","timestamp":"1732079580.0","comment_id":"1315049","content":"Selected Answer: A\nA is correct"}],"answer_description":"","exam_id":14,"topic":"1","question_id":139,"question_text":"A company has a foundation model (FM) that was customized by using Amazon Bedrock to answer customer queries about products. The company wants to validate the model's responses to new types of queries. The company needs to upload a new dataset that Amazon Bedrock can use for validation.\nWhich AWS service meets these requirements?","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/151663-exam-aws-certified-ai-practitioner-aif-c01-topic-1-question/","isMC":true},{"id":"CnvcXKGeZ1KDSsCPUilD","unix_timestamp":1735355040,"choices":{"A":"Prompted persona switches","C":"Ignoring the prompt template","B":"Exploiting friendliness and trust","D":"Extracting the prompt template"},"discussion":[{"upvote_count":"1","poster":"kopper2019","comment_id":"1358105","content":"D. Extracting the prompt template","timestamp":"1739850120.0"},{"content":"Selected Answer: D\nExtracting the prompt template refers to a situation where the attacker tries to reveal or access the underlying structure or instructions used to configure the behavior of the large language model (LLM). This type of attack can expose how the model has been trained or how it responds to certain inputs, effectively giving the attacker insight into how the LLM has been directed to generate responses.\n\nThis type of attack could potentially lead to misuse, such as causing the model to behave in unintended ways, or even allow an attacker to manipulate the behavior of the model by crafting specific inputs based on the extracted prompt template.","comment_id":"1355425","poster":"Jessiii","timestamp":"1739330880.0","upvote_count":"1"},{"upvote_count":"1","poster":"dspd","comment_id":"1348067","content":"Selected Answer: D\nD. Extracting the prompt template","timestamp":"1738086420.0"},{"timestamp":"1737849540.0","upvote_count":"1","comment_id":"1346734","content":"Selected Answer: B\nB. Exploiting friendliness and trust\nExploiting friendliness and trust involves manipulating the LLM to respond in a way that appears friendly or trustworthy, potentially causing it to deviate from its intended behavior. This type of attack directly exposes how the LLM has been configured to interact with users, often leading it to provide information or make decisions that align more closely with the attacker's intentions rather than its original programming.","poster":"AzureDP900"},{"poster":"Moon","upvote_count":"2","content":"Selected Answer: D\nD: Extracting the prompt template\n\nExplanation:\nExtracting the prompt template is a prompting attack where an attacker intentionally crafts inputs to reveal the underlying configuration or instructions (prompt template) used to guide the large language model (LLM). This exposes the internal behavior or design of the model, potentially revealing sensitive or proprietary information about how the LLM is configured.\n\nWhy not the other options?\nA: Prompted persona switches:\nThis attack involves manipulating the LLM to adopt a different persona or role than intended but does not directly expose the prompt template.","comment_id":"1335100","timestamp":"1735695540.0"},{"poster":"aws_Tamilan","timestamp":"1735355040.0","comment_id":"1332725","content":"Selected Answer: D\nD. Extracting the prompt template\n\nExplanation:\nExtracting the prompt template is a prompting attack where the attacker directly attempts to reveal the underlying configured behavior or instructions of the large language model (LLM). This can expose sensitive configurations, system instructions, or contextual prompts that guide the model's behavior.","upvote_count":"1"}],"answer_ET":"D","url":"https://www.examtopics.com/discussions/amazon/view/153534-exam-aws-certified-ai-practitioner-aif-c01-topic-1-question/","question_text":"Which prompting attack directly exposes the configured behavior of a large language model (LLM)?","question_id":140,"answer_images":[],"answer_description":"","exam_id":14,"answers_community":["D (83%)","B (17%)"],"isMC":true,"question_images":[],"answer":"D","timestamp":"2024-12-28 04:04:00","topic":"1"}],"exam":{"id":14,"isMCOnly":false,"lastUpdated":"11 Apr 2025","name":"AWS Certified AI Practitioner AIF-C01","isImplemented":true,"isBeta":false,"numberOfQuestions":154,"provider":"Amazon"},"currentPage":28},"__N_SSP":true}