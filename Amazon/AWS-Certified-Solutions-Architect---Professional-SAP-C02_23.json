{"pageProps":{"questions":[{"id":"h0kD2HngJAK0IYxXPjlx","answers_community":["ACD (74%)","6%","6%","6%"],"choices":{"A":"Enable AWS Config in all accounts","F":"Use AWS Security Hub to deploy AWS WAF rules in all accounts for all CloudFront distributions","E":"Use AWS Shield Advanced to deploy AWS WAF rules in all accounts for all CloudFront distributions","B":"Enable Amazon GuardDuty in all accounts","C":"Enable all features for the organization","D":"Use AWS Firewall Manager to deploy AWS WAF rules in all accounts for all CloudFront distributions"},"question_text":"A company is using an organization in AWS Organizations to manage hundreds of AWS accounts. A solutions architect is working on a solution to provide baseline protection for the Open Web Application Security Project (OWASP) top 10 web application vulnerabilities. The solutions architect is using AWS WAF for all existing and new Amazon CloudFront distributions that are deployed within the organization.\n\nWhich combination of steps should the solutions architect take to provide the baseline protection? (Choose three.)","answer":"ACD","timestamp":"2023-05-27 15:39:00","answer_images":[],"answer_ET":"ACD","question_images":[],"isMC":true,"unix_timestamp":1685194740,"exam_id":33,"discussion":[{"content":"My Answer A,C,D\n\nhttps://aws.amazon.com/blogs/security/using-aws-firewall-manager-and-waf-to-protect-your-web-applications-with-master-rules-and-application-specific-rules/\n\ncan someone post the link if you feel my answer is incorrect","poster":"Roontha","comment_id":"908022","upvote_count":"16","comments":[{"comments":[{"upvote_count":"20","timestamp":"1685883300.0","comment_id":"914614","poster":"Roontha","content":"@ShinLi,\n\nC is must requirement in order leverage AWS Firewall Manager according to aws.\n\nPrerequisites\nAWS Firewall Manager has the following prerequisites:\n\nAWS Organizations: Your organization must be using AWS Organizations to manage your accounts, and All Features must be enabled. For more information, see Creating an Organization and Enabling All Features in Your Organization.\nA firewall administrator AWS Account: You must designate one of the AWS accounts in your organization as the administrator for AWS Firewall Manager. This gives the account permission to deploy AWS WAF rules across the organization.\nAWS Config: You must enable AWS Config for all of the accounts in your organization so that AWS Firewall Manager can detect newly created resources. To enable AWS Config for all of the accounts in your organization, you can use the Enable AWS Config template on the StackSets Sample Templates page. For more information, see Getting Started with AWS Config."}],"upvote_count":"1","comment_id":"908567","poster":"ShinLi","content":"why you pickup C? why we need enable all the features?","timestamp":"1685272140.0"}],"timestamp":"1685194740.0"},{"content":"Selected Answer: ACD\nAWS Firewall Manager has the following prerequisites:\nAWS Organizations: Your organization must be using AWS Organizations to manage your accounts, and All Features must be enabled.\nA firewall administrator AWS Account: You must designate one of the AWS accounts in your organization as the administrator for AWS Firewall Manager. \nAWS Config: You must enable AWS Config for all of the accounts in your organization so that AWS Firewall Manager can detect newly created resources.\nReference: https://aws.amazon.com/blogs/security/using-aws-firewall-manager-and-waf-to-protect-your-web-applications-with-master-rules-and-application-specific-rules/","timestamp":"1725770340.0","upvote_count":"2","poster":"sakibmas","comment_id":"1280207"},{"upvote_count":"1","comment_id":"1190991","content":"Selected Answer: ACD\nACD is the correct combination to establish a base line security when deploying within the organization in AWS Organization.","timestamp":"1712498100.0","poster":"Russs99"},{"upvote_count":"2","comment_id":"1088508","poster":"shaaam80","content":"Selected Answer: ACD\nAnswer - ACD \nPrerequisites - AWS Config and All Features should be enabled in the organization.","timestamp":"1701782460.0"},{"timestamp":"1700801760.0","comment_id":"1079039","content":"Selected Answer: ACD\nA, C, D","upvote_count":"1","poster":"career360guru"},{"poster":"severlight","upvote_count":"3","timestamp":"1700289780.0","content":"Selected Answer: ACD\nAWS config must be enabled in all accounts to identify new resources so AWS Firewall manager works properly","comment_id":"1073833"},{"upvote_count":"2","timestamp":"1690576020.0","content":"a-c-d----a-c-d----a-c-d\nGuardDuty, Shield Advanced, and Security Hub provide other security capabilities but are not directly related to deploying WAF rules across all accounts and distributions.","comment_id":"965799","poster":"easytoo"},{"upvote_count":"1","poster":"NikkyDicky","timestamp":"1688586180.0","comment_id":"944089","content":"Selected Answer: ACD\nits ACD"},{"content":"Selected Answer: ACD\nD is clear. A and C are needed for D to work\n\nhttps://aws.amazon.com/es/blogs/security/centrally-manage-aws-waf-api-v2-and-aws-managed-rules-at-scale-with-firewall-manager/#:~:text=Firewall%20Manager%20prerequisites","comment_id":"936969","timestamp":"1687974000.0","poster":"javitech83","upvote_count":"1"},{"content":"Selected Answer: ACD\nACD\nLink reference : https://aws.amazon.com/es/blogs/security/centrally-manage-aws-waf-api-v2-and-aws-managed-rules-at-scale-with-firewall-manager/#:~:text=Firewall%20Manager%20prerequisites","comment_id":"933688","upvote_count":"3","poster":"SkyZeroZx","timestamp":"1687703640.0"},{"upvote_count":"1","content":"baseline for OWASP = b-d-f","comment_id":"929846","timestamp":"1687375200.0","poster":"easytoo"},{"comment_id":"928430","poster":"emiliocb4","upvote_count":"4","timestamp":"1687262220.0","content":"Selected Answer: ACD\nbaseline protection vconfiguration.\nA to evaluate the configurations of AWS resources\nC enabling all features required by Firewall manager\nD to enable the waf rules"},{"poster":"Jonalb","upvote_count":"1","content":"Selected Answer: ABD\nEnable AWS Config in all accounts: AWS Config provides a detailed view of the configuration of AWS resources within an organization. By enabling AWS Config, the solutions architect can track and monitor the configuration of CloudFront distributions and ensure that they adhere to the desired baseline configuration, including AWS WAF settings.\n\nEnable Amazon GuardDuty in all accounts: Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior within AWS accounts. Enabling GuardDuty in all accounts allows for real-time threat detection and alerts related to potential web application vulnerabilities.","timestamp":"1686653520.0","comment_id":"922157"},{"poster":"SVGoogle89","timestamp":"1686068640.0","upvote_count":"2","comment_id":"916456","content":"Prerequisites for using AWS Firewall Manager\nYour account must be a member of AWS Organizations\nYour AWS account must be a member of an organization in the AWS Organizations service, and the organization must have all features enabled.\n\nYour account must be the AWS Firewall Manager administrator\nTo configure Firewall Manager policies, your account must be set as the AWS Firewall Manager administrator account, in the Settings pane.\n\nYou must have AWS Config enabled for your accounts and Regions\nYou must enable AWS Config for each of your AWS Organizations member accounts and for each AWS Region that contains resources that you want to protect using AWS Firewall Manager."},{"content":"Selected Answer: ACD\nA,C,D is right answer.\nInfact My initial choice is B,C,D.\nAfter I rewatch neal Davis' video, GuardDuty is intelligent thread detection service based ML,\nit does continuous monitoring for : 1) CloudTrail Management events; 2) CloudTrail S3 Data Events;3)VPC Flow Logs 4) DNS logs. so guardduty is not right in this scenario.","poster":"Jesuisleon","upvote_count":"3","timestamp":"1685812560.0","comment_id":"913831"},{"poster":"chathur","timestamp":"1685807580.0","content":"Selected Answer: ACD\nThe tutorial is here.\n\nhttps://aws.amazon.com/blogs/security/centrally-manage-aws-waf-api-v2-and-aws-managed-rules-at-scale-with-firewall-manager/#:~:text=Firewall%20Manager%20prerequisites","comments":[{"upvote_count":"1","poster":"Gmail78","comment_id":"986040","timestamp":"1692560580.0","content":"I assume if you want to secure AWS you need Guard duty enabled, it also interact with AWS WAF: https://aws.amazon.com/blogs/security/how-to-use-amazon-guardduty-and-aws-web-application-firewall-to-automatically-block-suspicious-hosts/"}],"comment_id":"913766","upvote_count":"1"},{"timestamp":"1685757240.0","poster":"Rajivjain","upvote_count":"2","comment_id":"913154","content":"Selected Answer: BDE\nUpdating My Vote to BDE \nEnabling Amazon GuardDuty will help monitor and detect malicious activity.\nDeploying WAF rules via Firewall Manager or Shield Advanced will filter incoming traffic and block common attack patterns. These steps can help protect against many of the most common web application security risks identified by OWASP.\nA (Enable AWS Config) is not directly related to providing baseline protection for web applications against OWASP's top 10 vulnerabilities. \nC (Enable All Features) is too broad and does not specifically address web application security. \nF (Use Security Hub) does not have a native capability to deploy WAF rules at scale."},{"comment_id":"911134","timestamp":"1685531760.0","poster":"MnqobiZulu","content":"ACD.......","upvote_count":"1"},{"comment_id":"910659","poster":"rbm2023","timestamp":"1685491620.0","comments":[{"timestamp":"1685491680.0","upvote_count":"1","content":"Option F – Again, security hub is an organization wide feature, while AWS Security Hub can provide visibility into security findings related to web application vulnerabilities detected by AWS WAF, it is AWS WAF that provides the actual protection by inspecting and filtering web traffic to your applications.\neliminating A – if you enable ALL FEATURES, you do not need this option.\neliminating B – if you enable ALL FEATURES, you do not need this option. \neliminating E - AWS Shield Advanced is a DDoS","comment_id":"910660","poster":"rbm2023"}],"content":"Selected Answer: CDF\nIt is a combination of steps, and you need to choose THREE, remember that all options should be related to the Organization wide management, so:\nOption C – You need to take advantage of managing security services using Organization, hence enable all features is required in this case we have hundreds of accounts.\nOption D – In the same Link above Firewall Manager is one of the listed services, you can centrally configure and manage AWS WAF rules across accounts in your organization.","upvote_count":"2"},{"content":"Selected Answer: ABD\nA,D are correct, based on this: https://aws.amazon.com/pt/blogs/security/using-aws-firewall-manager-and-waf-to-protect-your-web-applications-with-master-rules-and-application-specific-rules/\nI'm in doubt about the third option, but I would go with option B because Guarduty integrates with the WAF: https://aws.amazon.com/pt/blogs/security/how-to-use-amazon-guardduty-and-aws-web-application-firewall-to-automatically-block-suspicious-hosts/\n\nSo A, B, D are my answers.","upvote_count":"1","poster":"Urameshi","comment_id":"909771","timestamp":"1685401440.0"},{"upvote_count":"1","comment_id":"908871","content":"Selected Answer: BDF\nto provide baseline protection.","poster":"Rajivjain","timestamp":"1685307600.0"},{"content":"BDF is right, considering taking to provide the baseline protection.","poster":"Rajivjain","upvote_count":"1","timestamp":"1685307480.0","comment_id":"908870"},{"comment_id":"908837","timestamp":"1685301960.0","poster":"AMEJack","content":"Selected Answer: ACD\nA: Config should be enable\nC: All features should be enabled\nD: Firewall Manager","upvote_count":"4"}],"topic":"1","question_id":111,"url":"https://www.examtopics.com/discussions/amazon/view/110372-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_description":""},{"id":"NlMpSfRFIQnxS2TtpeJ3","answer":"C","timestamp":"2022-12-10 17:54:00","url":"https://www.examtopics.com/discussions/amazon/view/90937-exam-aws-certified-solutions-architect-professional-sap-c02/","choices":{"B":"Deploy a new API Gateway API and Lambda functions in another Region. Change the Route 53 DNS record to a multivalue answer. Add both API Gateway APIs to the answer. Enable target health monitoring. Convert the DynamoDB tables to global tables.","A":"Deploy a new set of Lambda functions in a new Region. Update the API Gateway API to use an edge-optimized API endpoint with Lambda functions from both Regions as targets. Convert the DynamoDB tables to global tables.","C":"Deploy a new API Gateway API and Lambda functions in another Region. Change the Route 53 DNS record to a failover record. Enable target health monitoring. Convert the DynamoDB tables to global tables.","D":"Deploy a new API Gateway API in a new Region. Change the Lambda functions to global functions. Change the Route 53 DNS record to a multivalue answer. Add both API Gateway APIs to the answer. Enable target health monitoring. Convert the DynamoDB tables to global tables."},"isMC":true,"topic":"1","unix_timestamp":1670691240,"question_images":[],"answers_community":["C (96%)","2%"],"question_text":"A company is providing weather data over a REST-based API to several customers. The API is hosted by Amazon API Gateway and is integrated with different AWS Lambda functions for each API operation. The company uses Amazon Route 53 for DNS and has created a resource record of weather.example.com. The company stores data for the API in Amazon DynamoDB tables. The company needs a solution that will give the API the ability to fail over to a different AWS Region.\nWhich solution will meet these requirements?","answer_ET":"C","discussion":[{"comments":[{"upvote_count":"3","content":"Step1 - set up resources - Route 53 failover DNS records for the domain names","poster":"leehjworking","timestamp":"1682398860.0","comment_id":"879957"}],"poster":"robertohyena","comment_id":"741377","content":"C.\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/dns-failover.html","upvote_count":"15","timestamp":"1670726340.0"},{"poster":"c73bf38","upvote_count":"9","content":"The best solution to give the API the ability to fail over to a different AWS Region would be option C:\n\nC. Deploy a new API Gateway API and Lambda functions in another Region. Change the Route 53 DNS record to a failover record. Enable target health monitoring. Convert the DynamoDB tables to global tables.\n\nThis solution involves deploying a new API Gateway API and Lambda functions in another region. The company should also convert the DynamoDB tables to global tables to enable cross-region replication of the data. Then, the company should change the Route 53 DNS record to a failover record and enable target health monitoring to automatically route traffic to the new region in the event of a failure or outage in the primary region.","comment_id":"809115","timestamp":"1727061000.0"},{"comment_id":"1359615","upvote_count":"1","poster":"teeee123","timestamp":"1740109980.0","content":"Selected Answer: C\nI think C"},{"upvote_count":"1","comment_id":"1304295","content":"Selected Answer: C\nFailover routing policy – Use when you want to configure active-passive failover.","timestamp":"1730178540.0","poster":"TariqKipkemei"},{"timestamp":"1727061060.0","comment_id":"774664","content":"Selected Answer: C\nThe solution that will meet these requirements is option C:\n\nDeploy a new API Gateway API and Lambda functions in another Region.\nChange the Route 53 DNS record to a failover record.\nEnable target health monitoring.\nConvert the DynamoDB tables to global tables.\n\nThis solution will allow the API to failover to a different region, by using Route 53 failover record. The failover record will direct traffic to the primary API endpoint (the one in the primary region) as long as it is healthy. If the primary endpoint becomes unavailable, traffic will be directed to the secondary endpoint (the one in the secondary region). Additionally, by converting the DynamoDB tables to global tables, the data will be available in both regions, which is required for the failover scenario. Target health monitoring can be used to monitor the health of the API Gateway, and when it is determined that the primary endpoint is unavailable, the traffic will be directed to the secondary endpoint.","poster":"masetromain","upvote_count":"3"},{"content":"Selected Answer: C\nI also agree with C. But not sure why not B, B is actually pretty good option. No, that I have experience in this specific case; what I normally see is Active/Standby. But option B sounds good because, in theory, we need to have both regions running the current code (Lambda) and if an outage happens we are sure both work, and we don't have stale config/code in the failover region. Sometimes multi-answer does not return the best endpoint for the use case, so that could be something against this solution.","comments":[{"poster":"princajen","upvote_count":"1","content":"Multivalue is used for load balancing, not failover.","timestamp":"1736880840.0","comment_id":"1340497"}],"timestamp":"1727061060.0","upvote_count":"3","comment_id":"870524","poster":"Sarutobi"},{"upvote_count":"1","content":"Selected Answer: B\nThe answer is B.\nA: There is no Route 53, so it cannot be switched in the event of a failure.\nC: It's good to change to a failover record, but compared to other questions, there is no step to add a DNS record answer, so you can't switch to a new region.\nD: The global function is meaningless.\n\nB: A health check is additionally set, and failover is possible because the corresponding records are not returned in the event of a region failure.\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-configuring.html","poster":"edder","timestamp":"1727061060.0","comment_id":"1078033"},{"content":"Selected Answer: C\nNot A. \"edge-optimized API endpoint\" make use of CloudFront to optimize global each, however API Gateway instance is deployed in a single region thus no ability to fail over to a different AWS Region\nNot B. \"Route 53 DNS record to a multivalue\" implements a active-active scenario, while we are requested to have fail over\nNot D. I am not aware of \"global function\" also \"Route 53 DNS record to a multivalue\" is not the best fit (see above)\n\nThus C. is correct has it come with all the required pieces","comment_id":"1091660","poster":"ninomfr64","timestamp":"1727061060.0","upvote_count":"3"},{"comment_id":"1100014","poster":"atirado","timestamp":"1727061060.0","content":"Selected Answer: C\nOption A - Does not provide a way to fail over to a new region but rather a way for API gateway to respond from the region closest to the client\n\nOption B - Does not provide a way to fail over to a new region because when the main region is healthy name resolution will provide 2 possible regions to connect to\n\nOption C - Provides a way to fail over to a new region through the use of a Route 53 failover record and health monitoring and deployment in another region\n\nOption D - Does not provide a way to fail over to a new region because when the main region is healthy name resolution will provide 2 possible regions to connect to","upvote_count":"5"},{"poster":"higashikumi","comment_id":"1211994","upvote_count":"3","timestamp":"1727061000.0","content":"Selected Answer: C\nTo achieve automatic failover for the weather API, the company should deploy a duplicate API Gateway and Lambda functions in a secondary AWS region, then configure a Route 53 failover record that points to both endpoints. This failover record, combined with health checks, will automatically redirect traffic to the secondary region if the primary one fails. Additionally, converting DynamoDB tables to global tables ensures data availability in both regions, allowing the secondary API to function seamlessly during a failover."},{"timestamp":"1725089880.0","comment_id":"1275427","content":"D. Deploy a new API Gateway API in a new Region. Change the Lambda functions to global functions. Change the Route 53 DNS record to a multivalue answer. Add both API Gateway APIs to the answer. Enable target health monitoring. Convert the DynamoDB tables to global tables.","poster":"amministrazione","upvote_count":"1"},{"timestamp":"1718542620.0","content":"Selected Answer: D\nThe changes of A and C are too much, breaking the original security design.\nB is wrong because answer B doesn't mention deny SCP on root level is changed. Allow on OU will not win because when allow and deny the same service, explicit deny always wins for the sake of security concerns.","comment_id":"1231376","poster":"Helpnosense","upvote_count":"1"},{"upvote_count":"2","poster":"lighthouse85","timestamp":"1717203540.0","content":"Selected Answer: C\nC, failover health","comment_id":"1222403"},{"upvote_count":"1","poster":"gofavad926","timestamp":"1710601440.0","comment_id":"1175043","content":"Selected Answer: C\nC, failover record, this is the typical failover configuration on route53. Be careful, chatgpt suggests the option B \"multivalue answer\""},{"poster":"MoT0ne","upvote_count":"3","content":"Selected Answer: C\nChoosing C cause you want the API GW and Lambda functions work as a combination behind the DNS with failover, can think of Route53 here as a CDN provider like Cloudflare","comment_id":"1171665","timestamp":"1710246180.0"},{"upvote_count":"1","timestamp":"1701019620.0","comment_id":"1080900","content":"C is correct","poster":"abeb"},{"poster":"severlight","comment_id":"1067705","content":"Selected Answer: C\nfailover is required","upvote_count":"1","timestamp":"1699690500.0"},{"upvote_count":"2","timestamp":"1696205520.0","poster":"Jean_PA","comment_id":"1022704","content":"Selected Answer: C\nC is correct."},{"poster":"ansgohar","comment_id":"1018712","content":"Selected Answer: C\nC. Deploy a new API Gateway API and Lambda functions in another Region. Change the Route 53 DNS record to a failover record. Enable target health monitoring. Convert the DynamoDB tables to global tables.","timestamp":"1695810960.0","upvote_count":"1"},{"poster":"Simon523","comment_id":"993081","content":"Selected Answer: C\nhttps://thewebspark.com/2020/07/14/handling-multi-region-fail-over-with-amazon-route-53-tutorial/","timestamp":"1693313040.0","upvote_count":"1"},{"poster":"dimitry_khan_arc","content":"Selected Answer: C\nC is my choice","upvote_count":"1","timestamp":"1692908580.0","comment_id":"989475"},{"comment_id":"978769","poster":"whenthan","content":"Selected Answer: C\nhttps://d1.awsstatic.com/events/reinvent/2019/REPEAT_1_Best_practices_for_building_multi-region,_active-active_serverless_applications_SVS337-R1.pdf","upvote_count":"1","timestamp":"1691768280.0"},{"poster":"stevegod0","comment_id":"965625","timestamp":"1690555500.0","content":"C is correct.","upvote_count":"1"},{"timestamp":"1687800240.0","content":"Selected Answer: C\nIt's C","poster":"NikkyDicky","upvote_count":"1","comment_id":"934670"},{"poster":"cheese929","comment_id":"915993","upvote_count":"1","timestamp":"1686035340.0","content":"Selected Answer: C\nC is correct"},{"timestamp":"1683909660.0","poster":"RunkieMax","content":"Selected Answer: C\nC fit the best the question","comment_id":"896071","upvote_count":"1"},{"upvote_count":"1","timestamp":"1679976720.0","poster":"mfsec","comment_id":"852753","content":"Selected Answer: C\nC is good here"},{"content":"Selected Answer: C\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/dns-failover.html","comment_id":"831693","poster":"kiran15789","timestamp":"1678180380.0","upvote_count":"1"},{"comment_id":"819832","upvote_count":"1","timestamp":"1677191040.0","content":"Selected Answer: C\nEasy one :)","poster":"dev112233xx"},{"timestamp":"1676056200.0","upvote_count":"1","content":"Selected Answer: C\nC is correct.","comment_id":"804659","poster":"Sarutobi"},{"timestamp":"1670691240.0","comment_id":"741120","upvote_count":"4","content":"Selected Answer: C\nI agree with answer C. this is the correct use case of road 53 DNS failover record","poster":"masetromain"}],"exam_id":33,"answer_images":[],"question_id":112,"answer_description":""},{"id":"iz6hPg6peCQYUGteFQHh","choices":{"D":"Create an Amazon S3 bucket. Configure the S3 bucket to use the S3 Glacier Deep Archive storage class as default. Configure the S3 bucket for website hosting. Create an S3 interface endpoint. Configure the S3 bucket to allow access only through that endpoint.","C":"Launch an Amazon EC2 instance that runs a web server Attach an Amazon Elastic Block Store (Amazon EBS) volume to store the archived data. Use the Cold HDD (sc1) volume type. Configure the instance security groups to allow access only from private networks.","A":"Create an Amazon S3 bucket. Configure the S3 bucket to use the S3 One Zone-Infrequent Access (S3 One Zone-IA) storage class as default. Configure the S3 bucket for website hosting. Create an S3 interface endpoint. Configure the S3 bucket to allow access only through that endpoint.","B":"Launch an Amazon EC2 instance that runs a web server. Attach an Amazon Elastic File System (Amazon EFS) file system to store the archived data in the EFS One Zone-Infrequent Access (EFS One Zone-IA) storage class Configure the instance security groups to allow access only from private networks."},"topic":"1","answers_community":["A (64%)","D (34%)","2%"],"exam_id":33,"discussion":[{"timestamp":"1671558360.0","comments":[{"poster":"tman22","timestamp":"1671558720.0","comment_id":"751300","upvote_count":"18","content":"Nevermind, I go for D.\nIt should be technically possible - and mostly dependent on the intranet web application logic - It could present users with the ability to start file retrieval, for then to later access the data."}],"upvote_count":"39","content":"A - Glacier Deep Archive can't be used for web hosting, regardless if the company says retrieval time is no concern.","poster":"tman22","comment_id":"751295"},{"content":"A is correct. HA is not required here. \nD use Glacier deep archive that need hours to access that will cause time out for web","upvote_count":"22","poster":"zhangyu20000","comment_id":"742868","timestamp":"1670852880.0"},{"content":"Selected Answer: D\nS3 Glacier Deep Archive will be the lowest cost.","comment_id":"1367776","upvote_count":"1","timestamp":"1741499580.0","poster":"BennyMao"},{"upvote_count":"1","timestamp":"1740281640.0","comment_id":"1360395","poster":"eberhe900","content":"Selected Answer: D\nThe question does not require website hosting. Its focus is on securely storing documents so employees can access them via VPN without exposing them to the public Internet. So Glacier Deep Archive is more cost-saving"},{"comment_id":"1324480","upvote_count":"2","timestamp":"1733825040.0","poster":"ahhatem","content":"Selected Answer: D\nBuckets do not have a storage class... objects do! You can enable web hosting on any bucket and retrieve the objects (when available) over HTTP, whether the objects are available instantly or not that is another question. For glacier, you will have to request a restore and wait but when the object is eventually ready, you can download it directly over HTTP.\nSo, the answer is D."},{"timestamp":"1733639400.0","comment_id":"1323410","content":"Selected Answer: A\nS3 glacier can be used for hosting as it requires additional steps to unarchive objects!","poster":"Heman31in","upvote_count":"1"},{"upvote_count":"1","comment_id":"1323166","timestamp":"1733586720.0","content":"Selected Answer: A\nNo, Amazon S3 Glacier Deep Archive cannot be used for web hosting.\n\nReasons:\nGlacier Deep Archive is not designed for real-time access:\n\nGlacier Deep Archive is intended for long-term, infrequently accessed archival data. Retrieval times can range from 12 hours to 48 hours, making it unsuitable for serving content dynamically or in real-time through a website.\nNo direct web hosting capabilities:\n\nS3 web hosting requires immediate availability of objects stored in the bucket. S3 Glacier Deep Archive is designed for delayed access, so objects stored in this class cannot be served directly.\nStorage class retrieval process:\n\nObjects stored in Glacier or Glacier Deep Archive require a retrieval job to be initiated before they are available for access. This process is asynchronous and incompatible with the demands of a web-hosting use case.","poster":"wem"},{"upvote_count":"1","poster":"TariqKipkemei","timestamp":"1730954880.0","content":"Selected Answer: D\nkeywords:\n'Archive documents, low requests, low availability and speed, LOWEST cost' = S3 Glacier Deep Archive","comment_id":"1308234"},{"comment_id":"1300913","upvote_count":"1","content":"Selected Answer: A\nonly A & D make sense, and since website hosting is also a requirement, I'll go with A.","poster":"TewatiaAmit","timestamp":"1729506600.0"},{"comments":[{"comment_id":"967733","content":"S3 interface endpoint doesn't support web hosting.\nThe question does not say large files, but large number of archived documents, which could be small-sized. Hence EFS OZ-IA (being cheaper than SC1) could be the right answer.","upvote_count":"1","timestamp":"1690776240.0","poster":"MRL110"}],"comment_id":"810181","content":"Selected Answer: A\nThe requirements are to store a large number of archived documents that are not publicly accessible, and make them available to employees through a corporate intranet. As the number of requests is low and speed of retrieval is not a concern, we can use the low-cost S3 One Zone-Infrequent Access (S3 One Zone-IA) storage class. We can configure the S3 bucket for website hosting and create an S3 interface endpoint to allow access to the documents only through the corporate intranet. This solution is the lowest cost as it eliminates the need to launch and manage EC2 instances.\n\nOption B and C involve launching an EC2 instance which increases the operational overhead and is more expensive than using S3. Also, EFS One Zone-IA storage class is not recommended for storing large files.\n\nOption D involves using the S3 Glacier Deep Archive storage class which is intended for long-term archival storage of data and not suitable for retrieving data frequently.","upvote_count":"4","timestamp":"1727062380.0","poster":"c73bf38"},{"poster":"zejou1","comment_id":"835602","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html\nStore large number of archived docs, and available through corp intranet. \nCopies of data held on physical media elsewhere (could be re-created). \nRequests low (but it doesn't say RARE so think monthly/quarterly).\n\"AVAILABILITY\" and speed of retrieval are not concerns.\n\nIt is A, yes Glacier is \"cheaper\", but I have to leave the archives for at least 180 days, would be available on corp intranet and it is more cost-effective if I want to migrate the data to Glacier if I monitor use and see it is \"rarely\" touched and know I have to hold it due to regulatory for at minimal 180 days.","upvote_count":"4","timestamp":"1727062380.0"},{"comment_id":"858617","content":"Selected Answer: A\nWhile D is most probably the cheapest solution.\nWhen you try to download an object in deep archive you get a warning that it is not possible. You need to retrieve it: got to actions and restore which will need at least 12 hours for *deep* archive.\nOnly after that you can access the document. \nThe answer D says enable webhosting: thats afaik not going to work but users will end up in above mentioned warning.\nTherefore we need to go for A which is not as cheap but users can access the documents.","poster":"hobokabobo","upvote_count":"4","timestamp":"1727062380.0"},{"timestamp":"1727062380.0","content":"Selected Answer: A\nGiven the requirements and the need for the lowest cost solution, the best option would be:\n\nA. Create an Amazon S3 bucket. Configure the S3 bucket to use the S3 One Zone-Infrequent Access (S3 One Zone-IA) storage class as default. Configure the S3 bucket for website hosting. Create an S3 interface endpoint. Configure the S3 bucket to allow access only through that endpoint.\nOptions B and C involve launching EC2 instances which would add unnecessary complexity and cost since the company's priority is to minimize costs. Additionally, option D involves using the S3 Glacier Deep Archive storage class which is intended for long-term archival data and has longer retrieval times, making it less suitable for the given requirements.","comment_id":"993816","poster":"bur4an","upvote_count":"3"},{"content":"Selected Answer: A\nAnswer A. \nB and C are not relevant.\nD is close to create confusion but can't be used as an option for 2 reasons:\n1. You can't create a S3 bucket with Glacier deep archive as a default storage class. Need lifecycle transition from any other S3 classes. \n2. S3 Glacier deep archive can't be used for website hosting.","timestamp":"1727062320.0","comment_id":"1087663","comments":[{"upvote_count":"2","poster":"_Jassybanga_","timestamp":"1707270780.0","content":"1 You can create - read here https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html\n2> Yes you are correct on this - So will go with answer A too..","comment_id":"1142883"}],"upvote_count":"2","poster":"shaaam80"},{"timestamp":"1727062320.0","comment_id":"1101101","content":"Selected Answer: A\nOption A - This option will work and S3 One Zone is a cheap storage solution for a large number of documents\n\nOption B - This option might not work: Nothing is said in the question about whether the Client VPN is connecting to a private subnet. Moreover, EFS might not be a cheap storage solution for a large number of documents\n\nOption C - This option might not work: Nothing is said in the question about whether the Client VPN is connecting to a private subnet. Moreover, EBS Cold HDD might not be a cheap storage solution for a large number of documents\n\nOption D - This option will not work: S3 Deep Glacier Vaults cannot be configured for static hosting. You would need to write an application for accessing the archives.","poster":"atirado","upvote_count":"2"},{"content":"Selected Answer: A\npay attention to \"copies of data that is held on physical media elsewhere\", this is hint for one zone. Using Glacier is possible in theory, but won't work out of box. Need to develop a whole new application to submit unarchive request when user request a file, wait for up to 48 hour, create the s3 link, notify the user and ask user to come back to view the file. This is ANOTHER application","timestamp":"1727062320.0","upvote_count":"8","comment_id":"1143787","comments":[{"poster":"24Gel","timestamp":"1710378060.0","upvote_count":"2","content":"I agree, \"copies of data that is held on physical media elsewhere\", this is hint for one zone, \n\nHowever, it could be multiple zone as well.\n\nAvailability and speed of retrieval are not concerns of the company.\n\nSo I go with D","comment_id":"1173006"},{"upvote_count":"1","timestamp":"1710550740.0","poster":"kz407","content":"This! It's also worth mentioning that, the application we have to develop for option D, will be very difficult, if not impossible to be hosted in S3, because it will be a stateful application.","comment_id":"1174604"}],"poster":"a54b16f"},{"content":"Selected Answer: A\nFor me is A.\nI'm not sure that S3 Glacier Deep Archive can be used as website. Also more than 12 hours to retrieve is so much for documental systems (also if is not a concern the speed up).\nGoing with A","upvote_count":"1","timestamp":"1727062320.0","comment_id":"1177065","poster":"red_panda"},{"content":"Selected Answer: A\nObjects in Glacier Deep Archive needs to be 'restored'. A click on simple static website will not make AWS API call to restore the object and make it available.","comment_id":"1185644","poster":"Smart","upvote_count":"2","timestamp":"1727062320.0"},{"poster":"TonytheTiger","timestamp":"1727062320.0","upvote_count":"1","comment_id":"1187511","content":"Selected Answer: D\nOption D: 2 major points for the company. 1. Availability and speed of retrieval are NOT concerns of the company. 2. Meets these requirements at the LOWEST cost. Only S3 Glacier Deep Archive gives the company those requirement. The questions doesn't state how fast the employees need to access the files but the company does, see point 1. S3 Glacier Deep Archive is the lowest-cost storage option in AWS. Standard-IA and S3 One Zone-IA objects are available for millisecond access\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html"},{"poster":"Helpnosense","content":"Selected Answer: A\nThe answer is A.\nD is wrong because the data stored on Glacier Deep Archive can't be accessed directly without initiating a retrieval request to restore the data to either S3 Standard or S3 Standard-IA first. Needless to say, use it as static web site.","comment_id":"1231488","upvote_count":"1","timestamp":"1727062320.0"},{"upvote_count":"1","comment_id":"1276498","content":"A - Even Deep Glacier is lowest cost and company says retrieval time is no concern, Deep Glacier cannot be used for web hosting.","poster":"Eduman","timestamp":"1725260040.0"},{"poster":"amministrazione","comment_id":"1275452","upvote_count":"1","timestamp":"1725091680.0","content":"A. Create an Amazon S3 bucket. Configure the S3 bucket to use the S3 One Zone-Infrequent Access (S3 One Zone-IA) storage class as default. Configure the S3 bucket for website hosting. Create an S3 interface endpoint. Configure the S3 bucket to allow access only through that endpoint."},{"content":"Selected Answer: A\nA because : No web static hosting on Glacier deep archive + Glacier is the cheapest but can be more expensive than One zone-IA if the employees retrieve the document (retrieval costs are high) + timeout on the front-end because it will take hours to retrieve the file.","comment_id":"1263587","poster":"MAZIADI","timestamp":"1723309560.0","upvote_count":"1"},{"upvote_count":"2","content":"I went with D so did ChatGPT yet the majority of folks have chosen A ...How do we know what is the exact answer...I see why One Zone IA should be used but I am not confident, please help","comments":[{"poster":"8693a49","content":"You cannot have a website on Glacier, so D is clearly wrong. To retrieve documents from Glacier you need to first call 'restore' on it. The object becomes available after considerable amount of time in standard storage class a for a limited duration. This wouldn't work on a static website.\n\nI suppose technically you could build a non-static application to manage restoring files for users, but it's awkward, and the solution would likely cost more due to development costs. Glacier's purpose is to store data that you never want to see again, but there is a 0.001% chance you might actually need it at some point. It is comparable to tape storage.\n\nChatGPT cannot answer these questions accurately because it is unable to reason.","comment_id":"1257573","timestamp":"1722268860.0","upvote_count":"4"}],"timestamp":"1718831580.0","poster":"5ehjry6sktukliyliuliykutjhy","comment_id":"1233211"},{"content":"The cheapest storage tier in Amazon S3 that can be used for static web hosting is the Amazon S3 Standard - Infrequent Access (S3 Standard-IA). While it offers lower costs compared to the S3 Standard storage class, it is designed for data that is accessed less frequently but still requires rapid access when needed.","comment_id":"1232503","comments":[{"poster":"cnethers","content":"Here's a quick comparison of the relevant S3 storage classes:\n\n Amazon S3 Standard:\n Designed for frequently accessed data.\n Low latency and high throughput performance.\n Suitable for websites with dynamic content and frequent access.","comment_id":"1232510","timestamp":"1718726520.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"1232507","content":"Amazon S3 Glacier and S3 Glacier Deep Archive:\n\n Much cheaper storage options.\n Designed for long-term archival and infrequently accessed data.\n Retrieval times range from minutes (S3 Glacier) to hours (S3 Glacier Deep Archive).\n Not suitable for static web hosting due to high latency in data retrieval.","timestamp":"1718726460.0","poster":"cnethers"}],"timestamp":"1718726400.0","upvote_count":"1","poster":"cnethers"},{"content":"A for sure.\nS3 One Zone-IA is ideal for customers who want a lower-cost option for infrequently accessed data but do not require the availability and resilience of S3.","comment_id":"1226517","upvote_count":"1","timestamp":"1717820940.0","poster":"gfhbox0083"},{"content":"Selected Answer: A\nCannot Glacier","timestamp":"1717237200.0","comment_id":"1222575","poster":"lighthouse85","upvote_count":"1"},{"comment_id":"1205466","poster":"titi_r","upvote_count":"1","content":"How can “A” or “D” be correct even though interface endpoint (PrivateLink) for S3 does NOT support Website endpoints!?\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html#privatelink-limitations","timestamp":"1714648020.0"},{"poster":"MoT0ne","comment_id":"1172643","timestamp":"1710340740.0","content":"Selected Answer: D\nLOWEST cost when compared with A","upvote_count":"1"},{"poster":"gustori99","comment_id":"1136953","content":"B and C does not make sense. \n\nBUT A and D also contains nonsense information. It is not possible to configure a bucket to use S3 One Zone-IA storage class or Glacier storage class as default. You can only specify a different storage class during upload. Also configuring bucket for website hosting does not make sense because a website endpoint is only accessible from the public internet (if the bucket policy allows it) and it is not supported for interface endpoint.","upvote_count":"2","timestamp":"1706718180.0"},{"content":"B and C does not make sense. \n\nBUT A and B also contains nonsense information. It is not possible to configure a bucket to use S3 One Zone-IA storage class or Glacier storage class as default. Standard storage class is always default and cannot be changed. You can only specify a different storage class during upload. Also lifecycle policy cannot help because it allows transition to S3 One-Zone-IA only after 30 days. Configuring bucket for website hosting does not make sense because a website endpoint is only accessible from the public internet (if the bucket policy allows it) and it is not supported for interface endpoint.","poster":"gustori99","upvote_count":"2","timestamp":"1706718120.0","comment_id":"1136952"},{"comments":[{"timestamp":"1710378540.0","comment_id":"1173010","content":"this should not be a concern here.\n\nYou cannot create a deep archive bucket, when you create a bucket, you either create a normal bucket or a single zone bucket, then you can configure it to use deep archive in it.","poster":"24Gel","upvote_count":"2"}],"poster":"liux99","comment_id":"1113137","content":"Confusion here is A and D. D is cheaper but is not viable. You cannot use S3 bucket of Deep Glacier class for web hosting.","timestamp":"1704316680.0","upvote_count":"2"},{"upvote_count":"2","timestamp":"1703522700.0","comment_id":"1105407","content":"I think I'll go for A when I take the exam, but, like most people, I'm on the fence.","poster":"Jay_2pt0_1"},{"poster":"ninomfr64","comment_id":"1098257","content":"Selected Answer: D\nIn the exam I would go for D, but both A and D have an issue: you do not need to enable static website hosting on the bucket, as this is only for public website endpoints. However, having static website hosting enabled doesn't prevent you from access the bucket using API. \n\nSee https://aws.amazon.com/blogs/networking-and-content-delivery/hosting-internal-https-static-websites-with-alb-s3-and-privatelink/#:~:text=You%20do%20not%20need%20to%20enable%20static%20website%20hosting%20on%20the%20bucket%2C%20as%20this%20is%20only%20for%20public%20website%20endpoints.%20Requests%20to%20the%20bucket%20will%20be%20going%20through%20a%20private%20REST%20API%20instead.","timestamp":"1702738740.0","upvote_count":"2"},{"poster":"924641e","content":"Tricky but answer D would provide the LOWEST cost vs answer A. Answer A would be the best design balance between cost and use for end-users.","upvote_count":"1","timestamp":"1702428480.0","comment_id":"1095049"},{"poster":"ixdb","upvote_count":"1","content":"Selected Answer: D\nS3 bucket does not support to set a default storage class. You can create lifecycle rule with Day 0 to move to Glactier deep archive class and enable web hosting. You can do it on aws console.","timestamp":"1702393800.0","comment_id":"1094650"},{"timestamp":"1701459120.0","comment_id":"1085452","content":"Selected Answer: D\nIf the company needs to access these documents occasionally and can tolerate several hours of retrieval time, Option D (Glacier Deep Archive) would be the most cost-effective solution.\nHowever, if the company requires faster access to these documents (even if infrequently), Option A (S3 One Zone-IA) would be a better choice, balancing cost with the need for more immediate access.\nSince the original statement indicates that \"availability and speed of retrieval are not concerns,\" Option D (Glacier Deep Archive) aligns more closely with these stipulations, offering the lowest cost solution at the expense of longer retrieval times. However, if the retrieval time becomes a concern at any point, switching to S3 One Zone-IA (Option A) could provide a middle ground between cost and accessibility.","upvote_count":"1","poster":"geekos"},{"comment_id":"1084844","timestamp":"1701389520.0","content":"Selected Answer: A\nS3 Glacier Deep Archive storage is primarily intended for data archiving purposes. However, it's important to note that in many organizations, only backup administrators have access to retrieve data, and it's not typically designed for direct user access. Additionally, using Deep Archive for web hosting is not feasible due to its intended use case, which focuses on long-term data retention rather than immediate or user-initiated access","upvote_count":"1","poster":"Hit1979"},{"poster":"edder","content":"Selected Answer: D\nThe answer is D.\nB,C: No need to use EC2.\nA: There is no need to use IA as it states that availability and speed of retrieval are not concerns of the company.\n\nD: If necessary, you can restore the object to an S3 bucket and retrieve it with web hosting.\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/restoring-objects.html","comment_id":"1079210","upvote_count":"1","timestamp":"1700825100.0"},{"content":"Selected Answer: A\nB and C are distractors. Between A and D, A satisfies all the requirements and since data is available physically somewhere else and so it can be stored in One-zone IA storage class. D would have been the best option cost wise if the bucket is not used for website hosting. Since Glacier class can't be used for website hosting it eliminates option D. Right answer is A.","comment_id":"1078458","poster":"jainparag1","upvote_count":"1","timestamp":"1700747520.0"},{"comment_id":"1077883","content":"Selected Answer: D\nD because the Consulting times is Low, that means the files will not be checked frequently.","timestamp":"1700689380.0","poster":"epartida","upvote_count":"1"},{"comment_id":"1069632","upvote_count":"1","timestamp":"1699900140.0","content":"In such divide in correct answer, what should be picked and remembered for exam?","poster":"srs27"},{"upvote_count":"1","content":"Selected Answer: A\nI would answer A instead of D for the reason that some copies are already stored somewhere and we don't need durability across AZs, and there is retrieval cost of the archive. However, this makes me doubt 'The speed of retrieval are not concern'.","poster":"severlight","timestamp":"1699768020.0","comment_id":"1068324"},{"comment_id":"1060557","timestamp":"1698928800.0","poster":"senthilsekaran","upvote_count":"3","content":"Will go with Option D as it has clearly mentioned company want to move archived documents"},{"comment_id":"1060371","content":"D: as Glacier deep archival is the lowest cost\nA: one-zone IA only cost 20% cheaper. \n\nboth can work with retrieval of data without concern on speed but D will be the cheapest no matter what.","poster":"Pupu86","upvote_count":"1","timestamp":"1698911220.0"},{"content":"A. if it were D, retrieval cost from web would become higher than A. And also in Glacier we call vault not bucket.","timestamp":"1696657200.0","poster":"rlf","upvote_count":"2","comments":[{"comment_id":"1078454","poster":"jainparag1","timestamp":"1700747220.0","content":"Retrieval is very rare and time is not a concern. I'll go for D. One zone IA is not for storing archived documents.","upvote_count":"1"}],"comment_id":"1027107"},{"timestamp":"1693764720.0","poster":"career360guru","comment_id":"997864","content":"Option A is the only cost effective solution.\nDeep Archive can't be used for Web-Hosting. Anyone who thinks that is possible should try it once before selecting that option.","comments":[{"timestamp":"1702739040.0","upvote_count":"2","content":"You can create an S3 bucket and configure a lifecycle policy to move any files after 0 days to Glacier Deep Archive, then you enable static web site hosting (I just did it)\n\nHere the point is that static web site hosting is only for public endpoint, thus in this scenario going for A or D you would only access documents using S3 api.","comment_id":"1098260","poster":"ninomfr64"}],"upvote_count":"3"},{"comment_id":"991278","upvote_count":"1","content":"Option D: \nThis option involves creating an Amazon S3 bucket and configuring it to use the S3 Glacier Deep Archive storage class as default. This storage class is designed for long-term storage of data that is rarely accessed and can be restored within several hours, offering the lowest cost storage for different access patterns. The S3 bucket is configured for website hosting and an S3 interface endpoint is created","timestamp":"1693120800.0","poster":"dkcloudguru"},{"upvote_count":"2","timestamp":"1692752580.0","content":"Selected Answer: A\nlarge documents storage - s3 and availability and speed of retrieval are no concerns and lowest cost...","comment_id":"987854","poster":"whenthan"},{"poster":"Simon523","timestamp":"1692679260.0","content":"Selected Answer: D\nI think the key words are \"Availability and speed of retrieval are not concerns\" & \"LOWEST cost\".\nOf course user cannot directly access the file, for it require 12 hours to retrieval files, but cause the time is not concern, so I select \"D\".","comment_id":"987073","upvote_count":"2"},{"poster":"b3llman","timestamp":"1691480700.0","content":"A - Glacier Deep Archive will take too long and it will hit the request timeout limit for S3.","upvote_count":"1","comment_id":"975307"},{"upvote_count":"1","comment_id":"972023","content":"Selected Answer: A\nThis is so tricky, but I would also go with A. \nThe only reason I go with A is that answer D has \"Configure the S3 bucket for website hosting\". This part doesn't make sense (unless they were storing other types of static content) as objects archived in Glacier DA have to be restored first. Seriously. If it wasn't that, I would go D.","poster":"chico2023","timestamp":"1691146140.0"},{"comment_id":"963311","timestamp":"1690342260.0","comments":[{"content":"Also, website access is not possible with interface-endpoints.\n(https://repost.aws/questions/QUu19UpXsTRnaPcg5biU54RA/s3-interface-endpoint)","timestamp":"1690345620.0","comments":[{"timestamp":"1702740780.0","poster":"ninomfr64","content":"Correct, but enabling static web site doesn't prevent you to access objects via S3 api (scenario doesn't require uses to access content via web site)\n\nAlso, B requires to run an EC2 that is making B and C more expensive than A and D","comment_id":"1098279","upvote_count":"1"}],"upvote_count":"1","comment_id":"963325","poster":"MRL110"},{"upvote_count":"1","content":"us-east-1\n\nEFS - One Zone-Infrequent Access Storage (GB-Month) $0.0133\nEBS sc1 - $0.015 per GB-month of provisioned storage\nthat pricing is very close...","timestamp":"1692215520.0","comment_id":"982960","poster":"Greyeye"}],"poster":"MRL110","content":"Since there is cost associated with One Zone-IA retrieval as well as interface endpoints, this should be B considering EFS One Zone-IA is cheaper than EBS SC1.","upvote_count":"2"},{"upvote_count":"1","poster":"Russs99","content":"Selected Answer: A\nAs to D, S3 Glacier Deep Archive storage should not be used as the default storage for any daily usage. It is designed for long-term archiving of data that is rarely accessed. The default retrieval time for S3 Glacier Deep Archive items is 12 hours, which is too slow for most daily usage.","timestamp":"1690124460.0","comment_id":"960551"},{"timestamp":"1689589740.0","comment_id":"954087","upvote_count":"1","poster":"khksoma","content":"It is A.\nhttps://tutorialsdojo.com/amazon-s3-vs-glacier/"},{"content":"Selected Answer: D\ni dont see anything saying you cant use web hosting with Deep archive. I believe the web hosting is seperate from the storage class.","upvote_count":"1","timestamp":"1688853000.0","comment_id":"946756","poster":"Magoose","comments":[{"comment_id":"962426","poster":"hirenshah005","upvote_count":"1","timestamp":"1690268580.0","content":"You are wrong Sir, Deep Archive can not make public objects even they are in Intranet"}]},{"content":"Selected Answer: D\nAbout enabling Website hosting in a S3 Bucket, remember that retrieving objects from Glacier Deep Archive will temporarily make the objects available into a Standard S3 bucket (which you can enable with Website hosting) https://docs.aws.amazon.com/AmazonS3/latest/userguide/restoring-objects.html","upvote_count":"2","poster":"Mom305","timestamp":"1688411940.0","comment_id":"942162"},{"upvote_count":"1","content":"Selected Answer: A\nA\nD is not usable","timestamp":"1687915620.0","poster":"NikkyDicky","comment_id":"935961"},{"poster":"dkx","comment_id":"934819","upvote_count":"2","content":"S3 can be used to host static web content, while Glacier cannot. In S3, users create buckets. In Glacier, users create archives and vaults.\nhttps://tutorialsdojo.com/amazon-s3-vs-glacier/","timestamp":"1687817280.0"},{"poster":"Jonalb","timestamp":"1687461180.0","content":"Selected Answer: D\nDDDDDDDDDDDDDD sorry guys","upvote_count":"1","comment_id":"930950"},{"upvote_count":"1","poster":"Jonalb","comment_id":"929207","content":"Selected Answer: A\nThe number of requests will be low!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\nAAAAAAAAAAAAAAAAAAAAAAA","timestamp":"1687335960.0"},{"poster":"[Removed]","comment_id":"928358","content":"Selected Answer: A\nD is ruled out by the need for no public access, so even though A is more expensive, it's the lowest cost suitable solution.","upvote_count":"1","timestamp":"1687257840.0"},{"poster":"Maria2023","upvote_count":"5","comment_id":"927688","content":"Selected Answer: A\nI vote for A, mostly based on that sentence \"Create an Amazon S3 bucket. Configure the S3 bucket to use the S3 Glacier Deep Archive storage class as default\" - I do not believe you can configure S3 bucket to use Glacier Deep Archive storage class as default. You need to set up lifecycle rules to transfer data to glacier. Plus the \"website hosting\" part","timestamp":"1687192380.0"},{"poster":"Jackhemo","comment_id":"926849","content":"Selected Answer: A\nbased on olabiba.ai:\nased on the requirements and the need for the lowest cost solution, the most suitable option would be:\n\nA. Create an Amazon S3 bucket. Configure the S3 bucket to use the S3 One Zone-Infrequent Access (S3 One Zone-IA) storage class as default. Configure the S3 bucket for website hosting. Create an S3 interface endpoint. Configure the S3 bucket to allow access only through that endpoint.\n\nThis option allows you to store the archived documents in an S3 bucket with the One Zone-Infrequent Access storage class, which is cost-effective for long-term storage. By configuring the S3 bucket for website hosting, you can make the documents accessible through the corporate intranet. Creating an S3 interface endpoint ensures secure access through the VPC, and by configuring the S3 bucket to allow access only through that endpoint, you ensure that the data is not accessible to the public.","timestamp":"1687112640.0","upvote_count":"1"},{"content":"S3 Glacier Deep Archive is a cost-effective and easy-to-manage alternative to tape. S3 Glacier Deep Archive delivers the lowest cost storage, up to 75% lower cost (than S3 Glacier Flexible Retrieval), for long-lived archive data that is accessed less than ONCE per year and is retrieved asynchronously. so Answer is A","comment_id":"918912","upvote_count":"2","timestamp":"1686287460.0","poster":"tromyunpak"},{"comment_id":"918724","content":"a-a-a-a-a-a-a-a-a-a-a-a-a","upvote_count":"2","timestamp":"1686264060.0","poster":"easytoo"},{"content":"Selected Answer: A\nI think A makes sense than D.\nDon't think s3 glacier deep tier is suitable for web hosting.","timestamp":"1685537040.0","upvote_count":"1","comment_id":"911264","poster":"Jesuisleon"},{"timestamp":"1684923240.0","upvote_count":"1","comment_id":"905748","content":"Selected Answer: D\nlower cost and no concerning on retrieval time","poster":"emiliocb4"},{"timestamp":"1684856640.0","poster":"rtguru","upvote_count":"1","comment_id":"905058","content":"The correct answer is D"},{"content":"Selected Answer: D\nD LOWEST Cost for me","poster":"SkyZeroZx","timestamp":"1684371420.0","comment_id":"900634","upvote_count":"1"},{"poster":"gonzjo52","content":"Nunca se menciono un sitio web, se dijo \"intranet\" a través de la VPN, por lo que no es necesario una web estaticas en el bucket. Elijo la opción D","upvote_count":"1","timestamp":"1684047480.0","comment_id":"897329"},{"upvote_count":"2","comment_id":"896158","poster":"karma4moksha","timestamp":"1683915960.0","content":"Option D: Create an Amazon S3 bucket. Configure the S3 bucket to use the S3 Glacier Deep Archive storage class as default. Configure the S3 bucket for website hosting. Create an S3 interface endpoint. Configure the S3 bucket to allow access only through that endpoint.\n\nThis option would be less expensive than using other S3 storage classes, but it would still be more expensive than using S3 One Zone-IA storage. Additionally, using the S3 Glacier Deep Archive storage class would make retrieval of the documents slow and expensive, which does not meet the company's requirements. Therefore, this option is not the best fit for the company's requirements.Hence A"},{"upvote_count":"2","content":"No doubt that the answer is D\nGlacier Deep Archive storage. Frontend access is not the main point of the question.","poster":"AWS_Sam","timestamp":"1683848400.0","comment_id":"895487"},{"comment_id":"892109","timestamp":"1683546780.0","upvote_count":"1","content":"Selected Answer: D\nD is the correct option as A is One Zone-Infrequent Access (S3 One Zone-IA) storage class which is not HA","poster":"gameoflove"},{"timestamp":"1682433000.0","content":"Selected Answer: A\nGlacier can't handle web hosting. It's a trick question.","comment_id":"880483","upvote_count":"5","poster":"Maja1"},{"comment_id":"869848","content":"if the question says archiving, most of the time the answer is glacier","poster":"devopsy","timestamp":"1681431300.0","upvote_count":"1"},{"comment_id":"861786","content":"Selected Answer: D\nWhen tackling AWS questions, always note the key requirement which is the LOWEST cost. It even says number of requests is low, availability and speed are not of concern.\n\nAlso don't make assumptions of the methods retrieval, it can always be a frontend to trigger restore and then once restore is complete, notify user to download. Frontend is not the main point of the question, here we want to provide solution for storage archival at CHEAPEST cost.","timestamp":"1680706140.0","poster":"OnePunchExam","upvote_count":"8"},{"comment_id":"858645","content":"Selected Answer: D\nWithin a single bucket, the object for website hosting is not necessarily a Glacier Deep Archive storage class.\nSince the purpose is to store archived documents, you can assume long-term storage.","timestamp":"1680424260.0","poster":"Asagumo","upvote_count":"1"},{"poster":"dev112233xx","timestamp":"1680039060.0","content":"Selected Answer: A\nA makes more sense than D.. Deep Archive retrieval time is 12 hours and I’m not sure it’s possible to host static website in such long retrieval time!","comment_id":"853723","upvote_count":"3"},{"timestamp":"1679064060.0","content":"Selected Answer: A\nA is the only correct. I looked up the AWS docs...\nS3 Glacier Deep Archive is a completely separate service that does not support web hosting.","poster":"vherman","comment_id":"842076","upvote_count":"5"},{"timestamp":"1679001060.0","upvote_count":"8","poster":"Dimidrol","comment_id":"841361","content":"A , i created bucket with web hosting and put some html pages in glacier deep archive and had 403 error, operation invalid for object storage class"},{"timestamp":"1678830120.0","upvote_count":"1","comment_id":"839324","content":"D - S3 One Zone-IA is for data that is accessed less frequently, but requires rapid access when needed. Question says availability and speed of retrieval are not concerns of the company.","poster":"Damijo"},{"timestamp":"1678623780.0","comments":[],"content":"Selected Answer: D\nAvailability and speed of retrieval are not concerns of the company.\nbut they did not mention high durability which is not provided by OneZone-IA","upvote_count":"1","comment_id":"836995","poster":"vherman"},{"content":"D is c orrect","timestamp":"1678606800.0","poster":"limjieson","comment_id":"836782","upvote_count":"1"},{"content":"Selected Answer: A\nwill go with A considering following hints\n1) data is copy of somethign stored else where (hints to One zone)\n2) traffic is low (but it still exist)\n3) minimum storage duration \n\nD might alos be correct but i will select A in exam","timestamp":"1678186560.0","upvote_count":"5","poster":"kiran15789","comment_id":"831781"},{"comment_id":"821756","poster":"cudbyanc","content":"Selected Answer: D\nThis solution provides cost-effective storage for the archived documents using the S3 One Zone-Infrequent Access (S3 One Zone-IA) storage class, which is the lowest cost storage option for infrequently accessed data in a single availability zone. Hosting the S3 bucket as a website enables easy access to the documents through the intranet, and creating an S3 interface endpoint ensures that access is only possible through the VPN attached to the VPC. Additionally, S3 provides built-in security features, such as bucket policies and access control lists (ACLs), to control access to the data.","upvote_count":"2","timestamp":"1677351720.0"},{"upvote_count":"4","comment_id":"819340","poster":"Sarutobi","comments":[{"upvote_count":"3","timestamp":"1677871320.0","content":"https://www.linkedin.com/pulse/s3-standard-more-cost-effective-than-glacier-jon-bonso;\nDefinitely A: Glacier has the highest minimum storage duration, which is 180 days, it becomes cost prohibitive if you factor in retrieval costs","poster":"Ajani","comment_id":"828292","comments":[{"comment_id":"872905","poster":"Sarutobi","upvote_count":"1","content":"Exactly, lol.","timestamp":"1681750860.0"}]}],"timestamp":"1677165780.0","content":"Selected Answer: A\nI will use A, but the question does not specify how often the files are retrieved. If they are retrieved frequently A for sure if they aren't then D."},{"upvote_count":"1","poster":"God_Is_Love","comments":[{"content":"I'm on the fence on this question, Option A is offering a Single AZ S3 bucket with infrequent access that has the feature to enable web hosting. I can't find a web hosting feature with any of the archive classes unless the archive is restored and transitioned back to the standard class.","timestamp":"1677279300.0","upvote_count":"2","comment_id":"821007","poster":"c73bf38"}],"timestamp":"1677086700.0","content":"Tricky one - Glacier storage class has different levels which can fetch documents quickly with instant retrieval too. so many people go for A but answer is D to save more!. - https://aws.amazon.com/s3/storage-classes/glacier/","comment_id":"818102"},{"poster":"PSPaul","upvote_count":"1","comment_id":"817967","timestamp":"1677078780.0","content":"D is good!\nKeyword is \"speed of retrieval are not concerns\"\nSo, Glacier Deep Archive is the choice."},{"content":"Selected Answer: D\nLowest cost gives the hint. it should be option D.","comment_id":"816632","timestamp":"1676987220.0","poster":"saurabh1805","upvote_count":"1"},{"comment_id":"816630","content":"Lowest cost gives the hint. it should be option D.","poster":"saurabh1805","upvote_count":"1","timestamp":"1676987160.0"},{"upvote_count":"1","comment_id":"816315","content":"Selected Answer: D\nThe employees can connect via intranet point to note is it's not via web application, so ppl can wait 12hours to get the documents for the lowest storage cost","poster":"kiran15789","timestamp":"1676964180.0"},{"poster":"c73bf38","content":"Confused by why everyone thinks it's D, reading the doc it says the use case and the minimum archive period is 90 days.\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/restoring-objects.html\n\nNumber of days you plan to keep objects archived – S3 Glacier Flexible Retrieval and S3 Glacier Deep Archive are long-term archival solutions. The minimal storage duration period is 90 days for the S3 Glacier Flexible Retrieval storage class and 180 days for S3 Glacier Deep Archive. Deleting data that is archived to Amazon S3 Glacier doesn't incur charges if the objects you delete are archived for more than the minimal storage duration period. If you delete or overwrite an archived object within the minimal duration period, Amazon S3 charges a prorated early deletion fee. For information about the early deletion fee, see the","timestamp":"1676957640.0","comment_id":"816234","upvote_count":"1"},{"comment_id":"809617","upvote_count":"3","poster":"brfc","timestamp":"1676469300.0","content":"I was going for D but due to retrieval costs I'm now leaning towards A"},{"poster":"Sara_swa","comment_id":"806989","upvote_count":"1","content":"B - since 'The data must not be accessible to the public' and EFS one zone IA is cheaper than EBS sc1","timestamp":"1676253300.0"},{"upvote_count":"1","timestamp":"1675642800.0","poster":"oatif","comment_id":"799281","content":"Selected Answer: A\nD does not make any sense 12-48 hours of data retrieval time is absurd - hence A."},{"comments":[{"upvote_count":"2","comment_id":"798118","poster":"cloudman","timestamp":"1675525260.0","content":"Read this : The number of requests will be low. Availability and speed of retrieval are not concerns of the company & LOWEST COST I see its D"},{"timestamp":"1676797380.0","poster":"anita_student","upvote_count":"2","content":"You can't use Glacier (in any flavor) for website hosting","comment_id":"813886"}],"poster":"DWsk","timestamp":"1675435800.0","comment_id":"797116","upvote_count":"1","content":"I'm really unsure on this one. I can't find a definitive answer whether you can use glacier to host web content. It would make sense you can't because you need to restore the data before retrieving it, but theoretically it could be possible with application logic.\nThis question feels like its a gotcha that is using Glacier as a red herring for CHEAPEST option but really wants you to use One Zone IA."},{"poster":"Shahul75","upvote_count":"2","timestamp":"1675353600.0","content":"Selected Answer: C\nIt should C.\ntaking of the wrong ones, \n* S3 interface doesn't support website endpoints\n* EFS One Zone-IA is expensive than SC1\nOnly one is left, which is \"C\"","comment_id":"796195"},{"comment_id":"780186","poster":"bititan","upvote_count":"4","content":"Selected Answer: A\nweb hosting not possible with deep archive objects. so I go for option A. Question is not about archival solution. it's about accessing data from vpc based application whilst maintaining lowest","timestamp":"1674057840.0"},{"content":"Selected Answer: D\nThe S3 Glacier Deep Archive storage class is the lowest-cost storage class offered by Amazon S3, and it is designed for archival data that is accessed infrequently and for which retrieval time of several hours is acceptable. S3 interface endpoint for the VPC ensures that access to the bucket is only from resources within the VPC and this will meet the requirement of not being accessible to the public. And also, S3 bucket can be configured for website hosting, and this will allow employees to access the documents through the corporate intranet.\nUsing an EC2 instance and a file system or block store would be more expensive and unnecessary because the number of requests to the data will be low and availability and speed of retrieval are not concerns.\nAdditionally, using Amazon S3 bucket will provide durability, scalability and availability of data.","comment_id":"774740","upvote_count":"3","poster":"masetromain","timestamp":"1673631780.0"},{"upvote_count":"4","comment_id":"743069","content":"Selected Answer: D\nThe number of requests will be low. Availability and speed of retrieval are not concerns of the company.\nWhich solution will meet these requirements at the LOWEST cost?\n\nI go with D","comments":[{"upvote_count":"2","comments":[{"timestamp":"1671537960.0","content":"Yes we can use one bucket with different storage class to store objects as per s3 lifecycle policy.","upvote_count":"1","comment_id":"750827","poster":"bjct"}],"content":"one bucket with deep glacier by default, can this bucket use web hosting?","poster":"zhangyu20000","timestamp":"1670976060.0","comment_id":"744568"},{"timestamp":"1673631720.0","upvote_count":"1","comment_id":"774738","content":"The S3 Glacier Deep Archive storage class is the lowest-cost storage class offered by Amazon S3, and it is designed for archival data that is accessed infrequently and for which retrieval time of several hours is acceptable. S3 interface endpoint for the VPC ensures that access to the bucket is only from resources within the VPC and this will meet the requirement of not being accessible to the public. And also, S3 bucket can be configured for website hosting, and this will allow employees to access the documents through the corporate intranet.\nUsing an EC2 instance and a file system or block store would be more expensive and unnecessary because the number of requests to the data will be low and availability and speed of retrieval are not concerns.\nAdditionally, using Amazon S3 bucket will provide durability, scalability and availability of data.","poster":"masetromain"}],"poster":"masetromain","timestamp":"1670864880.0"}],"isMC":true,"answer_description":"","question_text":"A company is planning to store a large number of archived documents and make the documents available to employees through the corporate intranet. Employees will access the system by connecting through a client VPN service that is attached to a VPC. The data must not be accessible to the public.\nThe documents that the company is storing are copies of data that is held on physical media elsewhere. The number of requests will be low. Availability and speed of retrieval are not concerns of the company.\nWhich solution will meet these requirements at the LOWEST cost?","url":"https://www.examtopics.com/discussions/amazon/view/91211-exam-aws-certified-solutions-architect-professional-sap-c02/","timestamp":"2022-12-12 14:48:00","question_id":113,"answer_images":[],"answer":"A","answer_ET":"A","unix_timestamp":1670852880,"question_images":[]},{"id":"b0NE7eCwbonf5J8pGkRW","choices":{"E":"The company's IdP defines SAML assertions that properly map users or groups. In the company to IAM roles with appropriate permissions.","B":"The IAM roles created for the federated users' or federated groups' trust policy have set the SAML provider as the principal.\nB. Test users are not in the AWSFederatedUsers group in the company's IdP.","D":"The on-premises IdP's DNS hostname is reachable from the AWS environment VPCs.","C":"The web portal calls the AWS STS AssumeRoleWithSAML API with the ARN of the SAML provider, the ARN of the IAM role, and the SAML assertion from IdP.","A":"The IAM user's permissions policy has allowed the use of SAML federation for that user."},"discussion":[{"comments":[{"timestamp":"1701210480.0","content":"Ref: BDF https://www.examtopics.com/discussions/amazon/view/36355-exam-aws-certified-solutions-architect-professional-topic-1/","comment_id":"908859","poster":"Rajivjain","upvote_count":"3"}],"content":"Kindly correct the Answers' sequence. A to F","upvote_count":"23","timestamp":"1701210180.0","comment_id":"908854","poster":"Rajivjain"},{"poster":"andreitugui","upvote_count":"21","timestamp":"1701335640.0","comment_id":"910004","content":"B) The IAM roles created for the federated users' or federated groups' trust policy have set the SAML provider as the principal.\nD) The web portal calls the AWS STS AssumeRoleWithSAML API with the ARN of the SAML provider, the ARN of the IAM role, and the SAML assertion from IdP.\nF)The company's IdP defines SAML assertions that properly map users or groups. In the company to IAM roles with appropriate permissions."},{"content":"B1,C,E","comment_id":"1209577","poster":"sarlos","upvote_count":"6","timestamp":"1731296400.0"},{"timestamp":"1717014240.0","content":"Selected Answer: BCE\nFor sure - BCE","comment_id":"1083830","upvote_count":"3","poster":"37b2ab7"},{"upvote_count":"3","content":"Selected Answer: BCE\nB1, C, E","comment_id":"1073834","poster":"severlight","timestamp":"1716008160.0"},{"upvote_count":"1","poster":"dkcloudguru","timestamp":"1710327720.0","content":"BDF is correct","comment_id":"1006384"},{"timestamp":"1709470260.0","comment_id":"997540","upvote_count":"2","poster":"CloudHandsOn","content":"Selected Answer: BCE\nB,C, & E was my first choice"},{"comment_id":"992872","poster":"Gmail78","content":"C- STS AssumerolewithSAML\nB1- Define trust policy for IAM assumed by the principal\nE - SAML Assertion","upvote_count":"3","timestamp":"1709197980.0"},{"poster":"SK_Tyagi","timestamp":"1708463760.0","upvote_count":"1","content":"Selected Answer: BD\nBDF is correct","comment_id":"986016"},{"comment_id":"976202","poster":"anttan","content":"Should be BEF, right? \nD. The web portal calls the AWS STS AssumeRoleWithSAML API with the ARN of the SAML provider, the ARN of the IAM role, and the SAML assertion from IdP. This is already being done by the federated identity web portal.\n\nSo E) The on-premises IdP's DNS hostname is reachable from the AWS environment VPCs. The on-premises IdP's DNS hostname must be reachable from the AWS environment VPCs. This is because the AWS STS AssumeRoleWithSAML API will need to be able to resolve the DNS hostname of the IdP in order to retrieve the SAML assertion.","upvote_count":"2","timestamp":"1707456600.0"},{"comment_id":"965047","timestamp":"1706397720.0","content":"Selected Answer: B\nBDF is the right answers","poster":"breadops","upvote_count":"2"},{"timestamp":"1706099160.0","content":"Correct BCE.","poster":"ggrodskiy","upvote_count":"1","comment_id":"961463"},{"comment_id":"957112","upvote_count":"1","content":"Selected Answer: BD\nAdmin The Order from the Question is not right.. Answer is BDF!","timestamp":"1705730460.0","poster":"Just_Ninja"},{"content":"Selected Answer: BCE\nB (the 1st B, as there are two in this version of question) CE","comment_id":"944091","poster":"NikkyDicky","timestamp":"1704491340.0","upvote_count":"2"},{"poster":"easytoo","timestamp":"1703208960.0","content":"it's B-D-F Jeff.","upvote_count":"2","comment_id":"929997"},{"timestamp":"1701049200.0","poster":"Roontha","content":"Answer : B, C, E","comments":[{"comment_id":"907635","upvote_count":"4","timestamp":"1701049440.0","content":"Sorry...it is BDF\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html","poster":"Roontha"}],"upvote_count":"2","comment_id":"907631"}],"exam_id":33,"answer":"BCE","topic":"1","question_text":"A solutions architect has implemented a SAML 2.0 federated identity solution with their company's on-premises identity provider (IdP) to authenticate users' access to the AWS environment. When the solutions architect tests authentication through the federated identity web portal, access to the AWS environment is granted. However, when test users attempt to authenticate through the federated identity web portal, they are not able to access the AWS environment.\n\nWhich items should the solutions architect check to ensure identity federation is properly configured? (Choose three.)","timestamp":"2023-05-27 01:40:00","url":"https://www.examtopics.com/discussions/amazon/view/110334-exam-aws-certified-solutions-architect-professional-sap-c02/","question_images":[],"answer_ET":"BCE","answers_community":["BCE (71%)","14%","14%"],"unix_timestamp":1685144400,"question_id":114,"answer_images":[],"answer_description":"","isMC":true},{"id":"wNOSsAn0304far4iIGgn","timestamp":"2023-05-27 01:37:00","exam_id":33,"answer_ET":"A","discussion":[{"timestamp":"1685158560.0","upvote_count":"6","content":"Selected Answer: A\nUsing RDS Proxy, you can handle unpredictable surges in database traffic. Otherwise, these surges might cause issues due to oversubscribing connections or creating new connections at a fast rate. RDS Proxy establishes a database connection pool and reuses connections in this pool. This approach avoids the memory and CPU overhead of opening a new database connection each time. To protect the database against oversubscription, you can control the number of database connections that are created.","comment_id":"907727","poster":"Masonyeoh"},{"upvote_count":"1","content":"Selected Answer: A\nTo handle the overloaded connections and keep the secrets in the Amazon Secret Manager.","poster":"85b5b55","timestamp":"1740036900.0","comment_id":"1359137"},{"upvote_count":"2","timestamp":"1703856120.0","poster":"carpa_jo","comment_id":"1108706","content":"Selected Answer: A\nUse replicas to scale read, this use-case is about writing so C & D are out.\nSecret manager offers rotation, parameter store doesn't.\nSo its A."},{"upvote_count":"1","comment_id":"1093641","timestamp":"1702310280.0","poster":"duriselvan","content":"D. Aurora Replica with Parameter Store:\n\nPros:\nImproves database capacity and reduces load on the primary instance.\nParameter Store provides centralized configuration management.\nCons:\nManually rotating credentials in Parameter Store poses security risks.","comments":[{"poster":"helloworldabc","upvote_count":"1","content":"just A","comment_id":"1269938","timestamp":"1724225820.0"}]},{"timestamp":"1700720040.0","content":"Selected Answer: A\nOption A","upvote_count":"2","comment_id":"1078115","poster":"career360guru"},{"timestamp":"1698818940.0","comment_id":"1059385","content":"Selected Answer: A\nstraight A. love these questions 😂","poster":"joleneinthebackyard","upvote_count":"1"},{"poster":"NikkyDicky","comment_id":"944092","upvote_count":"1","content":"Selected Answer: A\neasy A","timestamp":"1688586660.0"},{"comment_id":"935487","upvote_count":"1","content":"Selected Answer: A\nAgree with other explanations here.","timestamp":"1687876860.0","poster":"pupsik"},{"upvote_count":"3","content":"Selected Answer: A\nAgree with A \nRotate the keys using Secrets Manager, Param store does not cover it.\nRDS Proxy is exactly to solve the issues with overloaded connection because is a connection pool component.","poster":"rbm2023","timestamp":"1685487840.0","comment_id":"910634"},{"content":"Answer : A\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.html","timestamp":"1685144220.0","upvote_count":"4","comment_id":"907627","poster":"Roontha"}],"question_text":"A solutions architect needs to improve an application that is hosted in the AWS Cloud. The application uses an Amazon Aurora MySQL DB instance that is experiencing overloaded connections. Most of the application’s operations insert records into the database. The application currently stores credentials in a text-based configuration file.\n\nThe solutions architect needs to implement a solution so that the application can handle the current connection load. The solution must keep the credentials secure and must provide the ability to rotate the credentials automatically on a regular basis.\n\nWhich solution will meet these requirements?","choices":{"D":"Create an Aurora Replica. Store the connection credentials in AWS Systems Manager Parameter Store.","C":"Create an Aurora Replica. Store the connection credentials as a secret in AWS Secrets Manager","A":"Deploy an Amazon RDS Proxy layer. In front of the DB instance. Store the connection credentials as a secret in AWS Secrets Manager.","B":"Deploy an Amazon RDS Proxy layer in front of the DB instance. Store the connection credentials in AWS Systems Manager Parameter Store"},"answer_images":[],"answer_description":"","question_images":[],"unix_timestamp":1685144220,"topic":"1","answer":"A","url":"https://www.examtopics.com/discussions/amazon/view/110333-exam-aws-certified-solutions-architect-professional-sap-c02/","question_id":115,"isMC":true,"answers_community":["A (100%)"]}],"exam":{"numberOfQuestions":529,"isBeta":false,"provider":"Amazon","isMCOnly":true,"name":"AWS Certified Solutions Architect - Professional SAP-C02","isImplemented":true,"id":33,"lastUpdated":"11 Apr 2025"},"currentPage":23},"__N_SSP":true}