{"pageProps":{"questions":[{"id":"YmY438cqCU9osJArXwKS","answer_description":"","question_images":[],"question_text":"A data scientist is building a forecasting model for a retail company by using the most recent 5 years of sales records that are stored in a data warehouse. The dataset contains sales records for each of the company’s stores across five commercial regions. The data scientist creates a working dataset with StoreID. Region. Date, and Sales Amount as columns. The data scientist wants to analyze yearly average sales for each region. The scientist also wants to compare how each region performed compared to average sales across all commercial regions.\n\nWhich visualization will help the data scientist better understand the data trend?","answers_community":["D (100%)"],"exam_id":26,"isMC":true,"choices":{"C":"Create an aggregated dataset by using the Pandas GroupBy function to get average sales for each year for each region. Create a bar plot of average sales for each region. Add an extra bar in each facet to represent average sales.","B":"Create an aggregated dataset by using the Pandas GroupBy function to get average sales for each year for each store. Create a bar plot, colored by region and faceted by year, of average sales for each store. Add a horizontal line in each facet to represent average sales.","D":"Create an aggregated dataset by using the Pandas GroupBy function to get average sales for each year for each region. Create a bar plot, faceted by year, of average sales for each region. Add a horizontal line in each facet to represent average sales.","A":"Create an aggregated dataset by using the Pandas GroupBy function to get average sales for each year for each store. Create a bar plot, faceted by year, of average sales for each store. Add an extra bar in each facet to represent average sales."},"answer":"D","discussion":[{"comment_id":"1235774","upvote_count":"1","content":"Selected Answer: D\nIt is the best way","timestamp":"1719137520.0","poster":"pandkast"},{"content":"Selected Answer: D\nD: visualization provides insights into regional sales trends over time and allows for comparisons between regions and the overall average.","upvote_count":"2","comment_id":"1169794","poster":"AIWave","timestamp":"1710015600.0"},{"upvote_count":"3","timestamp":"1707527820.0","content":"D. Create an aggregated dataset by using the Pandas GroupBy function to get average sales for each year for each region. Create a bar plot, faceted by year, of average sales for each region. Add a horizontal line in each facet to represent average sales. This visualization allows the data scientist to compare yearly average sales across regions and see how each region's performance relates to the overall average, providing clear insights into trends and deviations.","comment_id":"1145881","poster":"Alice1234"},{"poster":"kyuhuck","timestamp":"1707302100.0","content":"Selected Answer: D\nExplanation:\nThe best visualization for this task is to create a bar plot, faceted by year, of average sales for each\nregion and add a horizontal line in each facet to represent average sales. This way, the data scientist\ncan easily compare the yearly average sales for each region with the overall average sales and see the\nIT Certification Guaranteed, The Easy Way!\n170\ntrends over time. The bar plot also allows the data scientist to see the relative performance of each\nregion within each year and across years. The other options are less effective because they either do\nnot show the yearly trends, do not show the overall average sales, or do not group the data by\nregion.\nReferences:\npandas.DataFrame.groupby - pandas 2.1.4 documentation\npandas.DataFrame.plot.bar - pandas 2.1.4 documentation\nMatplotlib - Bar Plot - Online Tutorials Library","upvote_count":"2","comment_id":"1143241"}],"topic":"1","answer_images":[],"answer_ET":"D","url":"https://www.examtopics.com/discussions/amazon/view/133255-exam-aws-certified-machine-learning-specialty-topic-1/","timestamp":"2024-02-07 11:35:00","unix_timestamp":1707302100,"question_id":231},{"id":"SgdDiAbre9pyBcwbQliF","timestamp":"2024-02-07 11:36:00","url":"https://www.examtopics.com/discussions/amazon/view/133256-exam-aws-certified-machine-learning-specialty-topic-1/","answer_images":[],"answer_ET":"C","exam_id":26,"discussion":[{"content":"Selected Answer: D\nfrom copilot - Amazon Lookout for Equipment is specifically designed for predictive maintenance and can automatically detect anomalies in sensor data, making it a suitable choice for this scenario. It minimizes the need for manual intervention and leverages advanced machine learning models to identify and handle outliers efficiently.","upvote_count":"1","poster":"MultiCloudIronMan","comment_id":"1288076","timestamp":"1727091120.0"},{"comment_id":"1204742","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-analyses.html#data-wrangler-time-series-anomaly-detection","poster":"Peter_Hsieh","upvote_count":"2","timestamp":"1714518000.0"},{"content":"Selected Answer: C\nData Wrangler can do it all","timestamp":"1711140300.0","comment_id":"1180309","upvote_count":"1","poster":"vkbajoria"},{"upvote_count":"2","poster":"AIWave","timestamp":"1710016080.0","content":"Anomaly detection visualization feature in SageMaker Data Wrangler is designed to identify outliers in the dataset based on sensor data parameters such as temperature and pressure. By visually inspecting the anomalies, the ML specialist can easily identify and remove outliers using transformations within Data Wrangler data flows, minimizing operational overhead.","comment_id":"1169803"},{"poster":"Adzz","comment_id":"1160617","timestamp":"1709039340.0","upvote_count":"1","content":"Selected Answer: C\nGoing with C"},{"upvote_count":"1","poster":"delfoxete","content":"Selected Answer: C\nagree with C","comment_id":"1143859","timestamp":"1707344400.0"},{"content":"Selected Answer: C\nAmazon SageMaker Data Wrangler is a tool that helps data scientists and ML developers to prepare\ndata for ML. One of the features of Data Wrangler is the anomaly detection visualization, which uses\nan unsupervised ML algorithm to identify outliers in the dataset based on statistical properties. The\nML specialist can use this feature to quickly explore the sensor data and find any anomalous values\nthat may affect the model performance. The ML specialist can then add a transformation to a Data\nWrangler data flow to remove the outliers from the dataset. The data flow can be exported as a\nscript or a pipeline to automate the data preparation process. This option requires the least\noperational overhead compared to the other options.\nReferences:\nAmazon SageMaker Data Wrangler - Amazon Web Services (AWS)\nAnomaly Detection Visualization - Amazon SageMaker\nTransform Data - Amazon SageMaker","comment_id":"1143243","poster":"kyuhuck","timestamp":"1707302160.0","upvote_count":"1"}],"question_images":[],"question_id":232,"isMC":true,"question_text":"A company uses sensors on devices such as motor engines and factory machines to measure parameters, temperature and pressure. The company wants to use the sensor data to predict equipment malfunctions and reduce services outages.\n\nMachine learning (ML) specialist needs to gather the sensors data to train a model to predict device malfunctions. The ML specialist must ensure that the data does not contain outliers before training the model.\n\nHow can the ML specialist meet these requirements with the LEAST operational overhead?","answer_description":"","answer":"C","choices":{"C":"Use an Amazon SageMaker Data Wrangler anomaly detection visualization to find outliers in the dataset. Add a transformation to a Data Wrangler data flow to remove outliers.","B":"Use an Amazon SageMaker Data Wrangler bias report to find outliers in the dataset. Use a Data Wrangler data flow to remove outliers based on the bias report.","D":"Use Amazon Lookout for Equipment to find and remove outliers from the dataset.","A":"Load the data into an Amazon SageMaker Studio notebook. Calculate the first and third quartile. Use a SageMaker Data Wrangler data flow to remove only values that are outside of those quartiles."},"answers_community":["C (86%)","14%"],"topic":"1","unix_timestamp":1707302160},{"id":"C0kvC3PpIJQxSZdXYsJu","url":"https://www.examtopics.com/discussions/amazon/view/133088-exam-aws-certified-machine-learning-specialty-topic-1/","answer_images":[],"discussion":[{"upvote_count":"1","timestamp":"1727684760.0","content":"Selected Answer: C\nStandards calling for PCA.\nhttps://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-data-wrangler-for-dimensionality-reduction/","comment_id":"1291479","poster":"AMEJack"},{"timestamp":"1727094660.0","poster":"MultiCloudIronMan","upvote_count":"1","comment_id":"1288110","content":"Selected Answer: B\nPCA Requires minmax scaling."},{"poster":"Peter_Hsieh","comment_id":"1204739","content":"Selected Answer: B\nWith support for PCA in Data Wrangler, you can now easily reduce the dimensionality of a high dimensional data set in only a few clicks. You can access PCA by selecting Dimensionality Reduction from the “Add step” workflow.\nhttps://aws.amazon.com/about-aws/whats-new/2022/10/amazon-sagemaker-data-wrangler-reduce-dimensionality-pca/","timestamp":"1714516980.0","upvote_count":"1"},{"comment_id":"1180307","timestamp":"1711140180.0","poster":"vkbajoria","content":"Selected Answer: B\nPCA requires scaling => use min-max scaler","upvote_count":"1"},{"comment_id":"1169810","timestamp":"1710016740.0","content":"Selected Answer: B\nA: No - PCA requires feature scaling to remove dominance of high value variables\nB: Yes - Scaling addresses the issue of features with different ranges + PCA does feature reduction\nC: No - manual removal may lead to removal of important features\nD: No - manual removal may lead to removal of important features","upvote_count":"2","poster":"AIWave"},{"poster":"vkbajoria","timestamp":"1709519940.0","comment_id":"1165250","content":"Selected Answer: C\nStandard scaler is better for PCA","upvote_count":"2"},{"content":"Selected Answer: B\nC performs a standard transformation and D removes variables with low correlations which will delete important features","timestamp":"1707245700.0","upvote_count":"2","comment_id":"1142491","poster":"delfoxete"}],"question_id":233,"question_images":[],"timestamp":"2024-02-06 19:55:00","topic":"1","answer":"B","unix_timestamp":1707245700,"answers_community":["B (70%)","C (30%)"],"answer_ET":"B","isMC":true,"choices":{"B":"Load the data into Amazon SageMaker Data Wrangler. Scale the data with a Min Max Scaler transformation step. Use the SageMaker built-in algorithm for PCA on the scaled dataset to transform the data.","A":"Use the Amazon SageMaker built-in algorithm for PCA on the dataset to transform the data.","C":"Reduce the dimensionality of the dataset by removing the features that have the highest correlation. Load the data into Amazon SageMaker Data Wrangler. Perform a Standard Scaler transformation step to scale the data. Use the SageMaker built-in algorithm for PCA on the scaled dataset to transform the data.","D":"Reduce the dimensionality of the dataset by removing the features that have the lowest correlation. Load the data into Amazon SageMaker Data Wrangler. Perform a Min Max Scaler transformation step to scale the data. Use the SageMaker built-in algorithm for PCA on the scaled dataset to transform the data."},"question_text":"A data scientist obtains a tabular dataset that contains 150 correlated features with different ranges to build a regression model. The data scientist needs to achieve more efficient model training by implementing a solution that minimizes impact on the model’s performance. The data scientist decides to perform a principal component analysis (PCA) preprocessing step to reduce the number of features to a smaller set of independent features before the data scientist uses the new features in the regression model.\n\nWhich preprocessing step will meet these requirements?","exam_id":26,"answer_description":""},{"id":"ATg68AWiCk94yjsEFyFL","answers_community":["BD (100%)"],"question_id":234,"topic":"1","discussion":[{"comment_id":"1263232","upvote_count":"1","timestamp":"1723239600.0","content":"Selected Answer: BD\nK means and PCA","poster":"KariDam0909"},{"timestamp":"1711139940.0","upvote_count":"1","comment_id":"1180306","content":"Selected Answer: BD\nK means and PCA","poster":"vkbajoria"},{"content":"Selected Answer: BD\nClassic clustering problem\nA: No - LDA is for topic modelling\nB: Yes - K-means is a clustering algorithm\nC: No - applies to images\nD: Yes - PCA makes sure only relevant features are selected\nE: No - FM is supervised regression/classification recommendation algorithm for sparse data","timestamp":"1710017160.0","upvote_count":"1","comment_id":"1169814","poster":"AIWave"},{"poster":"Alice1234","content":"B. K-means: This algorithm is effective for clustering customers into distinct groups based on similarities across their features, which can reveal segments more likely to respond to marketing campaigns.\n\nD. Principal Component Analysis (PCA): Given the high dimensionality of the dataset, PCA can reduce the number of variables to a manageable size while retaining most of the variance, making the dataset more tractable for clustering algorithms like K-means.","comment_id":"1145882","upvote_count":"3","timestamp":"1707528360.0"},{"upvote_count":"2","comment_id":"1142486","content":"Selected Answer: BD\nk-means for clustering due to no comments regarding labels in the data and also PCA in order to reduce the amount of features","poster":"delfoxete","timestamp":"1707245460.0"}],"answer_images":[],"question_images":[],"answer_description":"","unix_timestamp":1707245460,"isMC":true,"question_text":"An online retailer collects the following data on customer orders: demographics, behaviors, location, shipment progress, and delivery time. A data scientist joins all the collected datasets. The result is a single dataset that includes 980 variables.\n\nThe data scientist must develop a machine learning (ML) model to identify groups of customers who are likely to respond to a marketing campaign.\n\nWhich combination of algorithms should the data scientist use to meet this requirement? (Choose two.)","choices":{"B":"K-means","C":"Semantic segmentation","A":"Latent Dirichlet Allocation (LDA)","E":"Factorization machines (FM)","D":"Principal component analysis (PCA)"},"answer_ET":"BD","exam_id":26,"url":"https://www.examtopics.com/discussions/amazon/view/133087-exam-aws-certified-machine-learning-specialty-topic-1/","timestamp":"2024-02-06 19:51:00","answer":"BD"},{"id":"VSVWI3sfp45HSYWra8tg","url":"https://www.examtopics.com/discussions/amazon/view/14807-exam-aws-certified-machine-learning-specialty-topic-1/","exam_id":26,"timestamp":"2020-02-24 15:28:00","answer_images":[],"answer_ET":"B","isMC":true,"choices":{"B":"Parquet files","A":"CSV files","C":"Compressed JSON","D":"RecordIO"},"discussion":[{"poster":"gaku1016","timestamp":"1663753620.0","content":"Answer is B. Athena is best in Parquet format.","upvote_count":"23","comment_id":"54593"},{"timestamp":"1665636780.0","poster":"emailtorajivk","content":"You can improve the performance of your query by compressing, partitioning, or converting your data into columnar formats. Amazon Athena supports open source columnar data formats such as Apache Parquet and Apache ORC. Converting your data into a compressed, columnar format lowers your cost and improves query performance by enabling Athena to scan less data from S3 when executing your query","comment_id":"66280","upvote_count":"13"},{"content":"Selected Answer: B\nAmazon Athena performs best when querying columnar storage formats like Apache Parquet. Given that 1 TB of data is generated every minute, optimizing storage format is critical for query performance and cost efficiency.\n\nWhy Parquet (B) is the Best Choice?\nColumnar Storage:\n\nParquet stores data by columns instead of rows, allowing Athena to scan only the needed columns, reducing the amount of data read.\nCompression Efficiency:\n\nParquet automatically compresses data more efficiently than CSV or JSON.\nSmaller file sizes = Faster queries + Lower costs.\nEfficient Query Performance:\n\nParquet supports predicate pushdown, meaning queries can skip irrelevant rows without scanning the entire dataset.\nOptimized for Big Data & Athena:\n\nDesigned for big data workloads in Athena, Redshift Spectrum, and Presto.\nWorks well with S3 partitioning to improve query speed.","upvote_count":"2","poster":"JonSno","timestamp":"1739837220.0","comment_id":"1358041"},{"poster":"loict","comment_id":"1006571","timestamp":"1726228740.0","upvote_count":"2","content":"Selected Answer: B\nA. NO - slower \nB. YES - Parquet native in Aethena/Presto\nC. NO - Compressed JSON\nD. NO - no built-in support"},{"content":"Selected Answer: B\naccording to:\nhttps://dzone.com/articles/how-to-be-a-hero-with-powerful-parquet-google-and\nthe query run time over parquet file was 6.78 seconds while it was 236 seconds on the same data but stored on csv file which mean that parquet file is 34x faster than csv file","poster":"teka112233","upvote_count":"1","timestamp":"1724196780.0","comment_id":"986122"},{"content":"Selected Answer: B\nB it is","poster":"apprehensive_scar","comment_id":"540170","timestamp":"1675481820.0","upvote_count":"3"},{"comments":[{"poster":"AddiWei","comment_id":"544111","timestamp":"1675978980.0","upvote_count":"5","content":"Because you must explore data very quickly using SQL in order to run EDA / analyze data for ML purposes. Those explorations can inform on selecting features that can be used for modeling purposes."}],"comment_id":"329623","timestamp":"1666283940.0","upvote_count":"3","content":"Answer is B. https://aws.amazon.com/tw/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/\nBut why does this question relate to Machine Learning?","poster":"benson2021"}],"question_images":[],"question_id":235,"question_text":"A monitoring service generates 1 TB of scale metrics record data every minute. A Research team performs queries on this data using Amazon Athena. The queries run slowly due to the large volume of data, and the team requires better performance.\nHow should the records be stored in Amazon S3 to improve query performance?","answer_description":"","answer":"B","topic":"1","unix_timestamp":1582554480,"answers_community":["B (100%)"]}],"exam":{"name":"AWS Certified Machine Learning - Specialty","id":26,"isMCOnly":false,"isImplemented":true,"lastUpdated":"11 Apr 2025","numberOfQuestions":369,"isBeta":false,"provider":"Amazon"},"currentPage":47},"__N_SSP":true}