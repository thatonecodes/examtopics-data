{"pageProps":{"questions":[{"id":"9JQYTlR64pXnVDSsJF7h","unix_timestamp":1651501020,"exam_id":26,"choices":{"A":"Classes C and D are too similar.","D":"The model is overfitting for classes B and E.","C":"The data distribution is skewed.","B":"The dataset is too small for holdout cross-validation."},"question_id":66,"answer":"A","question_text":"A data scientist is training a text classification model by using the Amazon SageMaker built-in BlazingText algorithm. There are 5 classes in the dataset, with 300 samples for category A, 292 samples for category B, 240 samples for category C, 258 samples for category D, and 310 samples for category E.\nThe data scientist shuffles the data and splits off 10% for testing. After training the model, the data scientist generates confusion matrices for the training and test sets.\n//IMG//\n\n//IMG//\n\nWhat could the data scientist conclude form these results?","url":"https://www.examtopics.com/discussions/amazon/view/75080-exam-aws-certified-machine-learning-specialty-topic-1/","isMC":true,"answers_community":["A (76%)","B (18%)","6%"],"answer_ET":"A","timestamp":"2022-05-02 16:17:00","question_images":["https://www.examtopics.com/assets/media/exam-media/04145/0009500001.png","https://www.examtopics.com/assets/media/exam-media/04145/0009600001.png"],"answer_description":"","topic":"1","discussion":[{"comment_id":"600026","upvote_count":"8","timestamp":"1668167640.0","content":"Isn't it A? the model doesn't classify C & D well.","poster":"LydiaGom"},{"comment_id":"606632","timestamp":"1669293600.0","content":"Selected Answer: A\nthe correct answer should be A, the model is clearly unable to tell C and D apart\n\nthe reason why B is incorrect is subtle - there is holdout validation or cross-validation, but not holdout cross-validation; while I think it would be more reasonable to use CV with such a small dataset rather than holdout, the answer is mixing terms and therefore should be wrong\n\nalso, the test set confusion matrix is still pretty comparable to the train set one, so I wouldn't say there is objective evidence to claim holdout is a wrong choice here","upvote_count":"7","poster":"dolorez"},{"upvote_count":"1","timestamp":"1733909580.0","comment_id":"1228305","content":"Selected Answer: A\nI would go for A as well.","poster":"Antoh1978"},{"poster":"tueo","content":"Selected Answer: A\nI think option A is correct as C & D are behaving similarly.","comment_id":"1196079","upvote_count":"1","timestamp":"1729003200.0"},{"timestamp":"1725289140.0","upvote_count":"2","poster":"vkbajoria","comment_id":"1164231","content":"I think the answer is D.\nA => C, D are similar in train but the testing results contradict that. There are many As and Bs for C"},{"upvote_count":"1","poster":"kyuhuck","content":"Selected Answer: D\nThese results indicate that the model is overfitting for classes B and E, meaning that it is memorizing\nthe specific features of these classes in the training data, but failing to capture the general features\nthat are applicable to the test data. Overfitting is a common problem in machine learning, where the\nmodel performs well on the training data, but poorly on the test data3. Some possible causes of\noverfitting are:\nThe model is too complex or has too many parameters for the given data. This makes the model\nflexible enough to fit the noise and outliers in the training data, but reduces its ability to generalize to\nnew data","timestamp":"1723098240.0","comment_id":"1144273"},{"poster":"DimLam","upvote_count":"3","timestamp":"1713443880.0","comments":[{"comment_id":"1046929","content":"Also because of random peeking of test set entries, we got the wrong proportions of labels between train and test sets. \nSo the answer can be even C","poster":"DimLam","upvote_count":"1","timestamp":"1713445080.0"}],"content":"Selected Answer: B\nActually, both A and D are true. It would be an easy one if we had to choose two answers. But we need to choose only one. \n\nSo how to make sure that the person who created this question thought about A only?\n\nAlso if we take a look into the test confusion matrix. We can see that the A class also missed with C class at the same rate as the C and D classes.\n\nI would even say that here the model is generally overfitted. I would go for B","comment_id":"1046906"},{"poster":"kaike_reis","upvote_count":"1","timestamp":"1707494220.0","comment_id":"976717","comments":[{"poster":"DimLam","timestamp":"1713445200.0","upvote_count":"1","comment_id":"1046930","content":"But on the test set it's even confused between A and C classes"}],"content":"Selected Answer: A\nLetter A is correct. The model gets confused between (C) and (D) in training and testing."},{"content":"Selected Answer: B\nHold-out\n\nHold-out is when you split up your dataset into a ‘train’ and ‘test’ set. The training set is what the model is trained on, and the test set is used to see how well that model performs on unseen data. A common split when using the hold-out method is using 80% of data for training and the remaining 20% of the data for testing.\n\nHold-out\n\nHold-out is when you split up your dataset into a ‘train’ and ‘test’ set. The training set is what the model is trained on, and the test set is used to see how well that model performs on unseen data. A common split when using the hold-out method is using 80% of data for training and the remaining 20% of the data for testing.\nRefere: \nhttps://medium.com/@eijaz/holdout-vs-cross-validation-in-machine-learning-7637112d3f8f","upvote_count":"1","timestamp":"1706602080.0","poster":"rockyykrish","comment_id":"966907"},{"content":"Selected Answer: A\nModel in unable to tell c&D","timestamp":"1706549760.0","upvote_count":"1","comment_id":"966501","poster":"Mickey321"},{"upvote_count":"3","content":"D - Training accuracies of B and E are higher than those of test, whereas A has similar accuracy in both. For C and D, test accuracy has actually improved.","poster":"DD4","comment_id":"679192","comments":[{"comment_id":"888694","upvote_count":"1","content":"B and C has below 50% accuracy. D has 98% in train and 86% accuracy in test.\nAnd you are telling me, the take away is overfitting of D,\nSeriously???","timestamp":"1699033020.0","poster":"ZSun"}],"timestamp":"1679781780.0"},{"comment_id":"608610","content":"I think the answer is A. The model doesn't perform well on class C and D in both training and testing dataset. I don't think B is relevant to the question(cross-validation is not mentioned in the question)","poster":"tgaos","upvote_count":"3","timestamp":"1669697700.0"},{"timestamp":"1669628520.0","content":"Selected Answer: A\nWhat means holdout cross validation. There should be holdout validation vs cross validation","upvote_count":"2","comment_id":"608296","poster":"exam887"},{"comment_id":"596077","content":"B should be the correct answer","upvote_count":"2","timestamp":"1667405820.0","poster":"bluer1"}],"answer_images":[]},{"id":"14LXDO0mKNEIGsJyxrT3","unix_timestamp":1651173300,"exam_id":26,"answer":"BDE","question_id":67,"choices":{"F":"Data augmentation","E":"Feature importance with a tree-based classifier","C":"Data binning","B":"Correlation plot with heat maps","A":"Data scaling with standardization and normalization","D":"Univariate selection"},"question_text":"A company that manufactures mobile devices wants to determine and calibrate the appropriate sales price for its devices. The company is collecting the relevant data and is determining data features that it can use to train machine learning (ML) models. There are more than 1,000 features, and the company wants to determine the primary features that contribute to the sales price.\nWhich techniques should the company use for feature selection? (Choose three.)","url":"https://www.examtopics.com/discussions/amazon/view/74806-exam-aws-certified-machine-learning-specialty-topic-1/","isMC":true,"answer_ET":"BDE","answers_community":["BDE (100%)"],"timestamp":"2022-04-28 21:15:00","question_images":[],"answer_description":"","topic":"1","answer_images":[],"discussion":[{"upvote_count":"18","timestamp":"1667322840.0","poster":"ckkobe24","content":"Selected Answer: BDE\ni will go for B, D and E. B and D for me are like doing partial regression and corr plot can actually tell you briefly how well the univariate is correlated with your target and i guess that also apply for D.. and E , feature importance ranking that's what feature selection strategy want from my POV. And for Data Binning is data enrichment just like augmentations , but then the question was saying they want to do feature selection over 1k+ variables which implies they actually care more about which variable(s) can contribute more on determining the price ?","comment_id":"595638"},{"timestamp":"1692255000.0","content":"Selected Answer: BDE\nB. Correlation plot with heat maps: This technique can be used to identify the relationship between each feature and the target variable (sales price). By creating a correlation plot with heat maps, the company can quickly visualize the strength and direction of the relationship between each feature and the target variable.\n\nD. Univariate selection: This technique can be used to select the features that have the strongest relationship with the target variable. It involves analyzing each feature independently and selecting the ones that have the highest correlation with the target variable.\n\nE. Feature importance with a tree-based classifier: This technique can be used to determine the most important features that contribute to the target variable. By using a tree-based classifier such as Random Forest or Gradient Boosting, the company can rank the importance of each feature and select the ones that have the highest importance.","poster":"AjoseO","comment_id":"811653","upvote_count":"9"},{"timestamp":"1718632800.0","comment_id":"1099057","content":"Selected Answer: BDE\nFor feature selection in machine learning, you can use the following techniques:\nB. Correlation plot with heat maps:\nCorrelation analysis helps identify relationships between features and the target variable. A heat map can visually represent the correlation matrix, helping to identify highly correlated features.\nD. Univariate selection:\nUnivariate selection methods evaluate the relationship between each feature and the target variable independently. Common techniques include statistical tests such as chi-squared tests, ANOVA, or mutual information.\nE. Feature importance with a tree-based classifier:\nTree-based classifiers, such as decision trees or random forests, can provide feature importance scores. These scores help identify which features contribute the most to the predictive performance of the model.","upvote_count":"1","poster":"aquanaveen"},{"poster":"aquanaveen","timestamp":"1718505840.0","upvote_count":"1","content":"Selected Answer: BDE\nA, C and F are not feature selection techniques.","comment_id":"1097902"},{"content":"Selected Answer: BDE\nBDE seem to be the only viable feature selection methods here","comment_id":"1082424","timestamp":"1716884820.0","upvote_count":"1","poster":"endeesa"},{"upvote_count":"1","content":"Selected Answer: BDE\nThose are the only ones for FS.","timestamp":"1707494400.0","poster":"kaike_reis","comment_id":"976718"},{"comment_id":"966503","content":"Selected Answer: BDE\nthe most appropriate feature selection techniques for the company to determine the primary features contributing to the sales price are B (correlation plot with heat maps), D (univariate selection), and E (feature importance with a tree-based classifier).","poster":"Mickey321","timestamp":"1706550000.0","upvote_count":"1"},{"timestamp":"1692171180.0","poster":"wolfsong","upvote_count":"3","comment_id":"810509","content":"BDE as stated here: https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e"},{"timestamp":"1673700300.0","upvote_count":"4","poster":"Morsa","comment_id":"631300","content":"BDE for me"},{"content":"Selected Answer: BDE\nthrowing my weight behind B D E. Correlation with heatmaps help us eliminate multicollinearity, Univariate testing helps us see which ones are correlated with the target, same as feature importances of tree-based algorithms.","timestamp":"1671832020.0","upvote_count":"3","poster":"ovokpus","comment_id":"621258"},{"timestamp":"1666984500.0","content":"CDF for me","comment_id":"594001","poster":"bluer1","upvote_count":"1"}]},{"id":"EnjcFdZHabbZdRSjpDWE","timestamp":"2020-01-20 12:07:00","isMC":true,"answer":"D","question_text":"During mini-batch training of a neural network for a classification problem, a Data Scientist notices that training accuracy oscillates.\nWhat is the MOST likely cause of this issue?","question_id":68,"question_images":[],"answer_images":[],"answer_ET":"D","answers_community":["D (100%)"],"discussion":[{"comment_id":"54555","content":"Answer is D.\nShould the weight be increased or reduced so that the error is smaller than the current value? You need to examine the amount of change to know that. Therefore, we differentiate and check whether the slope of the tangent is positive or negative, and update the weight value in the direction to reduce the error. The operation is repeated over and over so as to approach the optimal solution that is the goal. The width of the update amount is important at this time, and is determined by the learning rate.","upvote_count":"17","poster":"gaku1016","timestamp":"1632979980.0"},{"content":"maybe D ?","upvote_count":"8","comment_id":"40970","poster":"ozan11","timestamp":"1632732780.0"},{"upvote_count":"1","content":"Selected Answer: D\nD. The learning rate is very high. \n\nExplanation:\nWhen the learning rate is too high, the optimization process may overshoot the optimal weights in parameter space. Instead of gradually converging, the model updates weights in a highly unstable manner, causing fluctuations in training accuracy. The network fails to settle into a minimum because the updates are too aggressive.","timestamp":"1739730120.0","poster":"JonSno","comment_id":"1357328"},{"comment_id":"803143","timestamp":"1727164440.0","upvote_count":"3","content":"Selected Answer: D\nA high learning rate can cause oscillations in the training accuracy because the optimizer makes large updates to the model parameters in each iteration, which can cause overshooting the optimal values. This can result in the model oscillating back and forth across the optimal solution.","poster":"AjoseO"},{"poster":"Mickey321","comment_id":"973116","timestamp":"1727164440.0","upvote_count":"2","content":"Selected Answer: D\nIf the learning rate is too high, the model weights may overshoot the optimal values and bounce back and forth around the minimum of the loss function. This can cause the training accuracy to oscillate and prevent the model from converging to a stable solution. The training accuracy is the proportion of correct predictions made by the model on the training data."},{"poster":"Rejju","comment_id":"1004481","content":"When the learning rate is set too high, it can lead to oscillations or divergence during training. Here's why:\n\nHigh Learning Rate: A high learning rate means that the model's parameters are updated by a large amount in each training step. This can cause the model to overshoot the optimal parameter values, leading to instability in training.\n\nOscillations: If the learning rate is excessively high, the model's updates can become unstable, causing it to oscillate back and forth between parameter values. This oscillation can prevent the model from converging to an optimal solution.\n\nTo address this issue, you can try reducing the learning rate. It's often necessary to experiment with different learning rates to find the one that works best for your specific problem and dataset. Learning rate scheduling techniques, such as reducing the learning rate over time, can also help stabilize training.","timestamp":"1727164440.0","upvote_count":"2"},{"comment_id":"927271","poster":"CKS1210","content":"Answer is A.\nA high learning rate means that the model parameters are being updated by large magnitudes in each iteration. As a result, the optimization process may struggle to converge to the optimal solution, leading to erratic behavior and fluctuations in training accuracy.","timestamp":"1687160700.0","upvote_count":"1"},{"comment_id":"916716","content":"Selected Answer: D\nIf learning rate is high, the accuracy is fluctuated because the value of loss function moves back and forth over the global minimum.","poster":"soonmo","timestamp":"1686094920.0","upvote_count":"1"},{"content":"Selected Answer: D\nThe big learning rating overshoot in true minima.","upvote_count":"2","poster":"Valcilio","comment_id":"832681","timestamp":"1678263540.0"},{"comment_id":"776653","content":"Selected Answer: D\nD Learning rate is too high. Textbook example of learning rate being too high. Lower Learning_rate will take more iterations, or longer to train, but will settle in place.","upvote_count":"1","timestamp":"1673792280.0","poster":"Tomatoteacher"},{"upvote_count":"1","content":"12-sep exam","comment_id":"667393","poster":"Shailendraa","timestamp":"1663009680.0"},{"timestamp":"1657085700.0","upvote_count":"1","comment_id":"627729","poster":"Sam1610","content":"D: per supuesto"},{"content":"A company sells thousands of products on a public website and wants to automatically identify\nproducts with potential durability problems. The company has 1.000 reviews with date, star rating,\nreview text, review summary, and customer email fields, but many reviews are incomplete and\nhave empty fields. Each review has already been labeled with the correct durability result.\nA machine learning specialist must train a model to identify reviews expressing concerns over\nproduct durability. The first model needs to be trained and ready to review in 2 days.\nWhat is the MOST direct approach to solve this problem within 2 days?\nA.\nTrain a custom classifier by using Amazon Comprehend.\nB.\nBuild a recurrent neural network (RNN) in Amazon SageMaker by using Gluon and Apache\nMXNet.\nC.\nTrain a built-in BlazingText model using Word2Vec mode in Amazon SageMaker.\nD.\nUse a built-in seq2seq model in Amazon SageMaker.","timestamp":"1646771040.0","poster":"missionml","upvote_count":"1","comments":[{"poster":"missionml","content":"Is A valid option?","timestamp":"1646771280.0","comment_id":"563521","upvote_count":"1"}],"comment_id":"563518"},{"comment_id":"374004","timestamp":"1636048680.0","upvote_count":"1","content":"D is correct. big batch size make local minia.","poster":"btsql"},{"upvote_count":"1","timestamp":"1635171900.0","poster":"jeetss1","comment_id":"355530","content":"it is a multiple answer question and answer should be both A and D"},{"timestamp":"1634602320.0","content":"Answer is D 100%; learning rate too high will cause such an event","poster":"syu31svc","upvote_count":"3","comment_id":"165065"},{"content":"The answer is D, from the Coursera deep learning specialization (course 2 - improving Deep NN)","poster":"deep_n","timestamp":"1634518980.0","upvote_count":"2","comment_id":"89325"},{"comment_id":"65631","content":"If the learning rate is too small, it will take very long time to get to the bottom.If the learning rate is too big, it could get oscillate away from the bottom. If training a neural net and you find that the loss or accuracy is speeding to infinity the learning rate is too high.","poster":"emailtorajivk","timestamp":"1633061040.0","upvote_count":"3"}],"exam_id":26,"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/12378-exam-aws-certified-machine-learning-specialty-topic-1/","answer_description":"","choices":{"A":"The class distribution in the dataset is imbalanced.","B":"Dataset shuffling is disabled.","D":"The learning rate is very high.","C":"The batch size is too big."},"unix_timestamp":1579518420},{"id":"0YA4PbtG2igCL38Fu5Ri","discussion":[{"comment_id":"594621","timestamp":"1651253640.0","upvote_count":"18","content":"Selected Answer: C\nAnswer is C, CNN-QR and DeepAR accepts related time series data (weather data, number of people on property, etc.,)","poster":"spaceexplorer"},{"content":"Selected Answer: C\nOption C: Convolutional Neural Network - Quantile Regression (CNN-QR). This algorithm is well-suited for handling complex datasets with multiple features, such as historical power consumption, weather, number of individuals, and public holidays, providing accurate and robust forecasts.","poster":"MultiCloudIronMan","timestamp":"1727184660.0","comment_id":"1288620","upvote_count":"1"},{"upvote_count":"1","poster":"Stokvisss","content":"Answer is C, CNN-QR and DeepAR accepts related time series data (weather data, number of people on property, etc.,). Classic forecasting methods, such as ARIMA or exponential smoothing (ETS), fit a single model to each individual time series. In contrast, DeepAR+ creates a global model (one model for all the time series) with the potential benefit of learning across time series.\n\nSource: https://aws.amazon.com/blogs/machine-learning/making-accurate-energy-consumption-predictions-with-amazon-forecast/","comment_id":"1161359","timestamp":"1709104680.0"},{"content":"Selected Answer: D\nAs per https://docs.aws.amazon.com/forecast/latest/dg/aws-forecast-choosing-recipes.html\n\nA. NO - no as powerful as NN\nB. NO - no as powerful as NN\nC. NO - works best with 100's of time series \nD. YES - best for strong seasonnability, expected for power","comment_id":"1011237","poster":"loict","timestamp":"1695123540.0","upvote_count":"1"},{"poster":"jyrajan69","content":"Based on this only CNN-QR can accept historical data\n\nhttps://www.examtopics.com/exams/amazon/aws-certified-machine-learning-specialty/view/32/","comment_id":"1001400","timestamp":"1694080380.0","upvote_count":"2"},{"timestamp":"1676623620.0","content":"Selected Answer: C\nCNN-QR is a deep learning algorithm that can model complex relationships between the inputs and outputs, such as the weather and public holidays, with historical power consumption data. CNN-QR has been shown to be effective in generating accurate predictions in many different types of forecasting use cases, including demand forecasting.\n\nETS (Exponential Smoothing) is a classical time series algorithm that is often used for forecasting. It can be effective for simple time series data that have regular patterns, but may not be sufficient to handle the complexity of the given data.\n\nARIMA (Autoregressive Integrated Moving Average) is another classical time series algorithm that can model complex patterns in data. However, it may be difficult to use in cases where there are many different inputs and the relationships between the inputs and outputs are complex.","poster":"AjoseO","upvote_count":"4","comment_id":"811650"},{"upvote_count":"3","content":"Selected Answer: C\nARIMA & ES are both base time series algos that are available. DeeoAR+ & CNN-QR are refined and able to utilize external data as well to complement the time series data available","timestamp":"1667923800.0","poster":"aScientist","comment_id":"713963"},{"poster":"matteocal","timestamp":"1658835660.0","comment_id":"637374","content":"Selected Answer: C\nC, as explained here:\nhttps://docs.aws.amazon.com/forecast/latest/dg/aws-forecast-choosing-recipes.html","upvote_count":"4"},{"content":"Selected Answer: A\nAccording to the link below, it is either ARIMA or DeepAR.\n\nSo A is the answer here\n\nhttps://aws.amazon.com/blogs/machine-learning/making-accurate-energy-consumption-predictions-with-amazon-forecast/","upvote_count":"3","poster":"ovokpus","comment_id":"624278","timestamp":"1656451980.0"},{"poster":"edvardo","content":"Given the provided data, I would discard A and B.\n\nAmazon Forecast CNN-QR, Convolutional Neural Network - Quantile Regression, is a proprietary machine learning algorithm for forecasting scalar (one-dimensional) time series\n\nI would choose D, Prophet. https://docs.aws.amazon.com/forecast/latest/dg/aws-forecast-recipe-prophet.html\nHow Prophet Works\nProphet is especially useful for datasets that:\nContain an extended time period (months or years) of detailed historical observations (hourly, daily, or weekly)\nHave multiple strong seasonalities\nInclude previously known important, but irregular, events\nHave missing data points or large outliers\nHave non-linear growth trends that are approaching a limit\nProphet is an additive regression model with a piecewise linear or logistic growth curve trend. It includes a yearly seasonal component modeled using Fourier series and a weekly seasonal component modeled using dummy variables.","comments":[{"timestamp":"1667923740.0","content":"Prophet wont be able to use the additional data that is available in the question","upvote_count":"1","poster":"aScientist","comment_id":"713961"},{"timestamp":"1655457360.0","poster":"f4bi4n","upvote_count":"6","content":"Prophet doesn't accept historical-related time series, so it won't work here\nhttps://docs.aws.amazon.com/forecast/latest/dg/aws-forecast-choosing-recipes.html#comparing-algos","comment_id":"617638"}],"comment_id":"596797","upvote_count":"2","timestamp":"1651667400.0"}],"answers_community":["C (88%)","9%"],"answer":"C","question_text":"A power company wants to forecast future energy consumption for its customers in residential properties and commercial business properties. Historical power consumption data for the last 10 years is available. A team of data scientists who performed the initial data analysis and feature selection will include the historical power consumption data and data such as weather, number of individuals on the property, and public holidays.\nThe data scientists are using Amazon Forecast to generate the forecasts.\nWhich algorithm in Forecast should the data scientists use to meet these requirements?","answer_description":"","topic":"1","question_images":[],"exam_id":26,"timestamp":"2022-04-29 19:34:00","answer_images":[],"unix_timestamp":1651253640,"url":"https://www.examtopics.com/discussions/amazon/view/74920-exam-aws-certified-machine-learning-specialty-topic-1/","isMC":true,"choices":{"A":"Autoregressive Integrated Moving Average (AIRMA)","C":"Convolutional Neural Network - Quantile Regression (CNN-QR)","B":"Exponential Smoothing (ETS)","D":"Prophet"},"question_id":69,"answer_ET":"C"},{"id":"o5zXGg9mRLAQpUz9jr3G","unix_timestamp":1651500600,"url":"https://www.examtopics.com/discussions/amazon/view/75078-exam-aws-certified-machine-learning-specialty-topic-1/","timestamp":"2022-05-02 16:10:00","question_id":70,"isMC":true,"choices":{"D":"Use the audio transcripts to create a training dataset and build an Amazon Transcribe custom language model. Analyze the transcripts and update the training dataset with a manually corrected version of transcripts where product names are not being transcribed correctly. Create an updated custom language model.","A":"Use a voice-driven Amazon Lex bot to perform the ASR customization. Create customer slots within the bot that specifically identify each of the required product names. Use the Amazon Lex synonym mechanism to provide additional variations of each product name as mis-transcriptions are identified in development.","B":"Use Amazon Transcribe to perform the ASR customization. Analyze the word confidence scores in the transcript, and automatically create or update a custom vocabulary file with any word that has a confidence score below an acceptable threshold value. Use this updated custom vocabulary file in all future transcription tasks.","C":"Create a custom vocabulary file containing each product name with phonetic pronunciations, and use it with Amazon Transcribe to perform the ASR customization. Analyze the transcripts and manually update the custom vocabulary file to include updated or additional entries for those names that are not being correctly identified."},"answer_images":[],"answers_community":["C (71%)","D (29%)"],"exam_id":26,"topic":"1","question_images":[],"question_text":"A company wants to use automatic speech recognition (ASR) to transcribe messages that are less than 60 seconds long from a voicemail-style application. The company requires the correct identification of 200 unique product names, some of which have unique spellings or pronunciations.\nThe company has 4,000 words of Amazon SageMaker Ground Truth voicemail transcripts it can use to customize the chosen ASR model. The company needs to ensure that everyone can update their customizations multiple times each hour.\nWhich approach will maximize transcription accuracy during the development phase?","answer":"C","answer_description":"","discussion":[{"timestamp":"1668844080.0","poster":"siju13","upvote_count":"12","comment_id":"603623","content":"Selected Answer: C\nAnswer is C.\n\nhttps://aws.amazon.com/blogs/machine-learning/build-a-custom-vocabulary-to-enhance-speech-to-text-transcription-accuracy-with-amazon-transcribe/"},{"timestamp":"1692254640.0","comments":[{"content":"Thank you AjoseO for all these detailed explanations! They are very useful!","comment_id":"819896","poster":"Siyuan_Zhu","upvote_count":"2","comments":[{"content":"say thank you to chat gpt","upvote_count":"2","comment_id":"970148","poster":"ccpmad","timestamp":"1706883900.0"}],"timestamp":"1692826800.0"},{"content":"D is an ideal answer however, the question ask for \"The company needs to ensure that everyone can update their customizations multiple times each hour\".\nTo retrain custom model each hour when we have changes, will be tedious and time consuming.\nI go with c, where we can ask everyone to just update the config file.","poster":"drcok87","timestamp":"1693656540.0","upvote_count":"8","comment_id":"826964"}],"comment_id":"811646","upvote_count":"5","poster":"AjoseO","content":"Selected Answer: D\nOption D involves using the available audio transcripts to create a training dataset and building a custom language model with Amazon Transcribe. \n\nThis approach provides a high degree of control over the transcription process and the ability to fine-tune the model to the specific vocabulary and pronunciation requirements of the company. Analyzing the transcripts and updating the training dataset with corrected versions is a crucial step in improving transcription accuracy. \n\nIt enables the model to learn from mistakes and to incorporate the unique spelling and pronunciation of the 200 required product names."},{"upvote_count":"1","comment_id":"1411161","timestamp":"1743134280.0","poster":"Carpediem78","comments":[{"content":"why? i think it's C","poster":"ef12052","timestamp":"1743827640.0","upvote_count":"1","comment_id":"1513144"}],"content":"Selected Answer: D\nThe company requires the correct identification of 200 unique product names, some of which have unique spellings or pronunciations.\n-> Use the audio transcripts to create a training dataset and build an Amazon Transcribe custom language model. Analyze the transcripts and update the training dataset with a manually corrected version of transcripts where product names are not being transcribed correctly. Create an updated custom language model."},{"content":"Selected Answer: C\n-Creating a custom vocabulary file allows you to explicitly define the correct pronunciation of each product name.\n-Manually updating the custom vocabulary file based on these observations allows you to continuously improve the ASR system.\n- As new product names or variations emerge, you can easily add them to the custom vocabulary file without retraining the entire ASR model.","poster":"AIWave","timestamp":"1724545140.0","comment_id":"1158291","upvote_count":"2"},{"comment_id":"997260","poster":"Shenannigan","upvote_count":"2","timestamp":"1709441040.0","content":"Selected Answer: C\nD was my initial choice however looking at the requirement \"The company needs to ensure that everyone can update their customizations multiple times each hour.\" I changed my mind due to having to retrain the model with new vocabulary. C gives you the ability to update the vocabulary and have it take effect immediately"},{"comment_id":"976723","timestamp":"1707495060.0","content":"Selected Answer: C\nAnswer is C\nD would required to build a model. It's well known the quantity of products, so it's not necessary.","upvote_count":"1","poster":"kaike_reis"},{"poster":"Mickey321","content":"Selected Answer: D\nthe best approach to maximize transcription accuracy during the development phase is to use the audio transcripts to create a training dataset and build an Amazon Transcribe custom language model. Analyze the transcripts and update the training dataset with a manually corrected version of transcripts where product names are not being transcribed correctly. Create an updated custom language model.","timestamp":"1706560860.0","upvote_count":"1","comment_id":"966588"},{"comment_id":"752648","content":"Why not D though?","poster":"DeepakPg","upvote_count":"1","timestamp":"1687366320.0"},{"comment_id":"737431","poster":"Yongs","content":"I think C is correct.","timestamp":"1686107340.0","upvote_count":"2"},{"poster":"bluer1","timestamp":"1667405400.0","comment_id":"596074","content":"A? any thought?","upvote_count":"1"}],"answer_ET":"C"}],"exam":{"isMCOnly":false,"isImplemented":true,"provider":"Amazon","lastUpdated":"11 Apr 2025","isBeta":false,"id":26,"numberOfQuestions":369,"name":"AWS Certified Machine Learning - Specialty"},"currentPage":14},"__N_SSP":true}