{"pageProps":{"questions":[{"id":"c1Ksrxu1J60DIPqLqEQK","answer":"C","url":"https://www.examtopics.com/discussions/amazon/view/99791-exam-aws-certified-solutions-architect-associate-saa-c03/","isMC":true,"discussion":[{"timestamp":"1728703800.0","content":"Selected Answer: C\n'On the first day of every month at midnight' = Scheduled scaling policy","upvote_count":"4","poster":"TariqKipkemei","comment_id":"1041320"},{"content":"Selected Answer: C\nBy configuring a scheduled scaling policy, the EC2 Auto Scaling group can proactively launch additional EC2 instances before the CPU utilization peaks to 100%. This will ensure that the application can handle the workload during the month-end financial calculation batch, and avoid any disruption or downtime.\n\nConfiguring a simple scaling policy based on CPU utilization or adding Amazon CloudFront distribution or Amazon ElastiCache will not directly address the issue of handling the monthly peak workload.","timestamp":"1711812180.0","comment_id":"855798","poster":"elearningtakai","upvote_count":"4"},{"poster":"Steve_4542636","comment_id":"828430","timestamp":"1709499840.0","content":"Selected Answer: C\nIf the scaling were based on CPU or memory, it requires a certain amount of time above that threshhold, 5 minutes for example. That would mean the CPU would be at 100% for five minutes.","upvote_count":"3"},{"comments":[{"comment_id":"818812","upvote_count":"2","content":"Scheduled scaling policies allow you to schedule EC2 instance scaling events in advance based on a specified time and date. You can use this feature to plan for anticipated traffic spikes or seasonal changes in demand. By setting up scheduled scaling policies, you can ensure that you have the right number of instances running at the right time, thereby optimizing performance and reducing costs.\n\nTo set up a scheduled scaling policy in EC2 Auto Scaling, you need to specify the following:\n\nStart time and date: The date and time when the scaling event should begin.\n\nDesired capacity: The number of instances that you want to have running after the scaling event.\n\nRecurrence: The frequency with which the scaling event should occur. This can be a one-time event or a recurring event, such as daily or weekly.","timestamp":"1708665540.0","poster":"LuckyAro"}],"comment_id":"818811","timestamp":"1708665360.0","poster":"LuckyAro","upvote_count":"3","content":"Selected Answer: C\nC: Configure an EC2 Auto Scaling scheduled scaling policy based on the monthly schedule is the best option because it allows for the proactive scaling of the EC2 instances before the monthly batch run begins. This will ensure that the application is able to handle the increased workload without experiencing downtime. The scheduled scaling policy can be configured to increase the number of instances in the Auto Scaling group a few hours before the batch run and then decrease the number of instances after the batch run is complete. This will ensure that the resources are available when needed and not wasted when not needed.\n\nThe most appropriate solution to handle the increased workload during the monthly batch run and avoid downtime would be to configure an EC2 Auto Scaling scheduled scaling policy based on the monthly schedule."},{"poster":"bdp123","content":"Selected Answer: C\nC is the correct answer as traffic spike is known","upvote_count":"2","comment_id":"818225","timestamp":"1708626660.0"},{"timestamp":"1708343340.0","upvote_count":"3","content":"ANSWER - C\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-scheduled-scaling.html","comment_id":"814024","poster":"jennyka76"},{"comment_id":"812945","poster":"Neha999","timestamp":"1708260000.0","upvote_count":"2","content":"C as the schedule of traffic spike is known beforehand."}],"exam_id":31,"answer_description":"","question_id":281,"topic":"1","answer_ET":"C","timestamp":"2023-02-18 13:40:00","answer_images":[],"unix_timestamp":1676724000,"question_text":"A company’s application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. On the first day of every month at midnight, the application becomes much slower when the month-end financial calculation batch runs. This causes the CPU utilization of the EC2 instances to immediately peak to 100%, which disrupts the application.\n\nWhat should a solutions architect recommend to ensure the application is able to handle the workload and avoid downtime?","answers_community":["C (100%)"],"choices":{"D":"Configure Amazon ElastiCache to remove some of the workload from the EC2 instances.","B":"Configure an EC2 Auto Scaling simple scaling policy based on CPU utilization.","C":"Configure an EC2 Auto Scaling scheduled scaling policy based on the monthly schedule.","A":"Configure an Amazon CloudFront distribution in front of the ALB."},"question_images":[]},{"id":"U5TYbqhbRDvlcoh8ty56","question_images":[],"answer":"A","answer_images":[],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/99703-exam-aws-certified-solutions-architect-associate-saa-c03/","discussion":[{"content":"Selected Answer: A\nSFTP, FTP - think \"Transfer\" during test time","poster":"Steve_4542636","comment_id":"828431","upvote_count":"17","timestamp":"1677877560.0"},{"comment_id":"1045703","poster":"wsdasdasdqwdaw","comments":[{"content":"1. AWS Transfer Family: Fully managed service that allows customers to transfer files over SFTP, FTPS, and FTP directly into and out of Amazon S3. \n\n2. Eliminates the need to manage any infrastructure for file transfer, which reduces operational overhead. \n\n3. You can also configure the service to use an existing Active Directory for authentication, =>>> no changes need to be made to the customer's application.\n\n4. Rule of thumbs for AWS exams: Opt for AWS fully managed services. :p :p :p","poster":"JA2018","comment_id":"1315324","upvote_count":"1","timestamp":"1732114560.0"}],"upvote_count":"2","content":"LEAST operational overhead => A, D is much more operational overhead","timestamp":"1697530920.0"},{"timestamp":"1697081940.0","poster":"TariqKipkemei","upvote_count":"2","comment_id":"1041324","content":"Selected Answer: A\nSFTP, No changes to the customer’s application? = AWS Transfer Family"},{"comment_id":"999486","content":"Transfer family is used for SFTP","poster":"Guru4Cloud","timestamp":"1693916580.0","upvote_count":"2"},{"poster":"live_reply_developers","timestamp":"1689941400.0","upvote_count":"2","content":"SFTP -> transfer family","comment_id":"958449"},{"timestamp":"1685108700.0","comment_id":"907400","upvote_count":"2","poster":"antropaws","content":"Selected Answer: A\nA no doubt. Why the system gives B as the correct answer?"},{"timestamp":"1683125820.0","content":"Selected Answer: A\njust A","upvote_count":"2","comment_id":"888647","poster":"lht"},{"content":"Selected Answer: A\nAWS Transfer Family","comment_id":"818818","timestamp":"1677129960.0","poster":"LuckyAro","upvote_count":"3"},{"comment_id":"818817","timestamp":"1677129840.0","upvote_count":"3","poster":"LuckyAro","content":"AWS Transfer Family is a fully managed service that allows customers to transfer files over SFTP, FTPS, and FTP directly into and out of Amazon S3. It eliminates the need to manage any infrastructure for file transfer, which reduces operational overhead. Additionally, the service can be configured to use an existing Active Directory for authentication, which means that no changes need to be made to the customer's application."},{"timestamp":"1677090720.0","upvote_count":"2","content":"Selected Answer: A\nTransfer family is used for SFTP","comment_id":"818226","poster":"bdp123"},{"content":"Selected Answer: A\nusing AWS Batch to LEAST operational overhead\nand have SFTP to no changes to the customer’s application\n\nhttps://aws.amazon.com/vi/blogs/architecture/managed-file-transfer-using-aws-transfer-family-and-amazon-s3/","poster":"TungPham","timestamp":"1676769960.0","comment_id":"813638","upvote_count":"3"},{"upvote_count":"4","poster":"Bhawesh","timestamp":"1676663040.0","content":"Selected Answer: A\nA. Set up AWS Transfer Family with SFTP for Amazon S3. Configure integrated Active Directory authentication.\n\nhttps://docs.aws.amazon.com/transfer/latest/userguide/directory-services-users.html","comment_id":"812291"}],"isMC":true,"question_text":"A company wants to give a customer the ability to use on-premises Microsoft Active Directory to download files that are stored in Amazon S3. The customer’s application uses an SFTP client to download the files.\n\nWhich solution will meet these requirements with the LEAST operational overhead and no changes to the customer’s application?","topic":"1","answer_ET":"A","question_id":282,"unix_timestamp":1676663040,"timestamp":"2023-02-17 20:44:00","exam_id":31,"answers_community":["A (100%)"],"choices":{"D":"Set up a Windows Amazon EC2 instance with SFTP to connect the on-premises client with Amazon S3. Integrate AWS Identity and Access Management (IAM).","C":"Set up AWS DataSync to synchronize between the on-premises location and the S3 location by using AWS IAM Identity Center (AWS Single Sign-On).","B":"Set up AWS Database Migration Service (AWS DMS) to synchronize the on-premises client with Amazon S3. Configure integrated Active Directory authentication.","A":"Set up AWS Transfer Family with SFTP for Amazon S3. Configure integrated Active Directory authentication."}},{"id":"ES3Cv3f56rw6HeckyaJP","url":"https://www.examtopics.com/discussions/amazon/view/99686-exam-aws-certified-solutions-architect-associate-saa-c03/","question_id":283,"answer_ET":"B","question_images":[],"answer":"B","discussion":[{"comments":[{"timestamp":"1726335240.0","comment_id":"1283729","upvote_count":"2","content":"Me too((( terrible question","poster":"elmyth"},{"poster":"Guru4Cloud","comment_id":"999475","content":"Me too","upvote_count":"4","timestamp":"1693916220.0"},{"poster":"lostmagnet001","timestamp":"1707339240.0","comment_id":"1143814","upvote_count":"1","content":"the same here!"}],"timestamp":"1685610540.0","upvote_count":"63","content":"readed the question 5 times, didn't understood a thing :(","comment_id":"911923","poster":"danielklein09"},{"upvote_count":"13","timestamp":"1676654700.0","comment_id":"812163","poster":"bdp123","content":"Selected Answer: B\nEnabling Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot allows you to\nquickly create a new Amazon Machine Image (AMI) from a snapshot, which can help reduce the\ninitialization latency when provisioning new instances. Once the AMI is provisioned, you can replace\nthe AMI in the Auto Scaling group with the new AMI. This will ensure that new instances are launched\nfrom the updated AMI and are able to meet the increased demand quickly."},{"content":"Selected Answer: B\nThe question focuses on reducing initialization latency when launching large EC2 instances in an Auto Scaling group during sudden demand spikes.\n\nSo B with fast snapshot is obviously the choice. But without the choices listed there, I would simply integrate a Warm Pool for the Auto Scaling group with pre-initialized instances based on an optimized AMI :)","poster":"LeonSauveterre","comment_id":"1319610","upvote_count":"2","timestamp":"1732866720.0"},{"content":"Selected Answer: B\nThe question wording is pretty weird but the only thing of value is latency during initialisation which makes B the correct option.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-fast-snapshot-restore.html\n\nA only helps with creating the AMI\nC and D will probably work (ambiguous language) but won't handle initialising latency issues.","poster":"awsgeek75","timestamp":"1704306060.0","upvote_count":"6","comment_id":"1113017"},{"timestamp":"1703919360.0","content":"Selected Answer: B\nFast Snapshot Restore (FSR)\n• Force full initialization of snapshot to have no\nlatency on the first use","poster":"farnamjam","upvote_count":"2","comment_id":"1109490"},{"timestamp":"1703863440.0","comment_id":"1108841","upvote_count":"4","poster":"pentium75","content":"Selected Answer: B\n\"Fast snapshot restore\" = pre-warmed snapshot\nAMI from such a snapshot is pre-warmed AMI"},{"comment_id":"1105050","poster":"master9","content":"Selected Answer: D\nAmazon Data Lifecycle Manager (DLM) is a feature of Amazon EBS that automates the creation, retention, and deletion of snapshots, which are used to back up your Amazon EBS volumes. With DLM, you can protect your data by implementing a backup strategy that aligns with your business requirements.\n\nYou can create lifecycle policies to automate snapshot management. Each policy includes a schedule of when to create snapshots, a retention rule with a defined period to retain each snapshot, and a set of Amazon EBS volumes to assign to the policy.\n\nThis service helps simplify the management of your backups, ensure compliance, and reduce costs.","comments":[{"upvote_count":"2","poster":"pentium75","comment_id":"1108839","content":"We're not asked to \"simplify the management of our backups, ensure compliance, and reduce costs\", we're asked to \"provide minimum initialization latency\" for an auto-scaling group.","timestamp":"1703863320.0"},{"comment_id":"1105051","upvote_count":"1","poster":"master9","content":"Sorry, its \"C\" and not \"D\"","timestamp":"1703485080.0"}],"timestamp":"1703484960.0","upvote_count":"1"},{"timestamp":"1703006880.0","upvote_count":"1","comment_id":"1100855","content":"Selected Answer: B\nb is correct","poster":"Nisarg2121"},{"comments":[{"content":"Option A (Use aws ec2 register-image command and AWS Step Functions): While this approach can be used to automate the creation of an AMI and update the Auto Scaling group, it may not offer the same level of optimization for initialization latency as Amazon EBS fast snapshot restore.\n\n Option C (Enable AMI creation and define lifecycle rules in Amazon Data Lifecycle Manager, create a Lambda function): While Amazon DLM can help manage the lifecycle of your AMIs, it might not provide the same level of speed and responsiveness needed for sudden increases in demand.\n\n Option D (Use Amazon EventBridge and AWS Backup): AWS Backup is primarily designed for backup and recovery, and it might not be as optimized for quickly provisioning instances in response to sudden demand spikes. EventBridge can be used for event-driven architectures, but in this context, it might introduce unnecessary complexity.","timestamp":"1701320820.0","poster":"meowruki","upvote_count":"2","comment_id":"1084007"}],"timestamp":"1701320760.0","content":"B. Enable Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot. Provision an AMI by using the snapshot. Replace the AMI in the Auto Scaling group with the new AMI.\n\nHere's the reasoning:\n\n Amazon EBS Fast Snapshot Restore: This feature allows you to quickly create new EBS volumes (and subsequently AMIs) from snapshots. Fast Snapshot Restore optimizes the initialization process by pre-warming the snapshots, reducing the time it takes to create volumes from those snapshots.\n\n Provision an AMI using the snapshot: By using fast snapshot restore, you can efficiently provision an AMI from the pre-warmed snapshot, minimizing the initialization latency.\n\n Replace the AMI in the Auto Scaling group: This allows you to update the instances in the Auto Scaling group with the new AMI efficiently, ensuring that the new instances are launched with minimal delay.","comment_id":"1084006","poster":"meowruki","upvote_count":"2"},{"poster":"TariqKipkemei","upvote_count":"2","content":"Selected Answer: B\nEnable Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot. Provision an AMI by using the snapshot. Replace the AMI in the Auto Scaling group with the new AMI","timestamp":"1697082300.0","comment_id":"1041328"},{"poster":"kambarami","upvote_count":"1","timestamp":"1695294480.0","comment_id":"1013009","content":"Pleaw3 reword 5he question. Can not understand a thing!"},{"upvote_count":"3","comment_id":"999480","timestamp":"1693916460.0","content":"Selected Answer: B\nEnable EBS fast snapshot restore on a snapshot\nCreate an AMI from the snapshot\nReplace the AMI used by the Auto Scaling group with this new AMI\n\nThe key points:\n\n° Need to launch large EC2 instances quickly from an AMI in an Auto Scaling group\n ° Looking to minimize instance initialization latency","poster":"Guru4Cloud"},{"content":"Selected Answer: B\nB most def","timestamp":"1685108880.0","comment_id":"907404","poster":"antropaws","upvote_count":"2"},{"poster":"elearningtakai","comment_id":"854588","upvote_count":"3","content":"Selected Answer: B\nB: \"EBS fast snapshot restore\": minimizes initialization latency. This is a good choice.","timestamp":"1680103740.0"},{"upvote_count":"3","poster":"Zox42","comment_id":"851650","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-fast-snapshot-restore.html","timestamp":"1679886900.0"},{"content":"Keyword, minimize initilization latency == snapshot. A and B have snapshots in them, but B is the one that makes sense.\nC has DLP that can create machines from AMI, but that does not talk about latency and snapshots.","timestamp":"1677215340.0","poster":"geekgirl22","comment_id":"820125","upvote_count":"4"},{"upvote_count":"2","timestamp":"1677130320.0","comment_id":"818822","poster":"LuckyAro","content":"Selected Answer: B\nEnabling Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot allows for rapid restoration of EBS volumes from snapshots. This reduces the time required to create an AMI from a snapshot, which is useful for quickly provisioning large Amazon EC2 instances.\n\nProvisioning an AMI by using the fast snapshot restore feature is a fast and efficient way to create an AMI. Once the AMI is created, it can be replaced in the Auto Scaling group without any downtime or disruption to running instances."},{"poster":"bdp123","comment_id":"816690","timestamp":"1676990940.0","upvote_count":"2","content":"Selected Answer: B\nEnabling Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot allows you to\nquickly create a new Amazon Machine Image (AMI) from a snapshot, which can help reduce the\ninitialization latency when provisioning new instances. Once the AMI is provisioned, you can replace\nthe AMI in the Auto Scaling group with the new AMI. This will ensure that new instances are launched from the updated AMI and are able to meet the increased demand quickly."},{"content":"Selected Answer: C\nProvision an AMI by using the snapshot => not sure because SnapShot only backup a EBS, AMI is backup a cluster\n. Replace the AMI in the Auto Scaling group with the new AMI. => for what ??\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html\n\nAmazon Data Lifecycle Manager helps automate snapshot and AMI management","poster":"TungPham","comment_id":"813670","timestamp":"1676773440.0","upvote_count":"2"},{"timestamp":"1676747220.0","comment_id":"813417","poster":"jennyka76","content":"agree with answer - B","upvote_count":"2"},{"poster":"kpato87","upvote_count":"3","comment_id":"813109","content":"Selected Answer: B\nOption B is the most suitable solution for this use case, as it enables Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot, which significantly reduces the time required for creating an AMI from the snapshot. The fast snapshot restore feature enables Amazon EBS to pre-warm the EBS volumes associated with the snapshot, which reduces the time required to initialize the volumes when launching instances from the AMI.","timestamp":"1676731140.0"},{"content":"https://www.examtopics.com/discussions/amazon/view/82400-exam-aws-certified-solutions-architect-associate-saa-c02/","comment_id":"812942","poster":"Neha999","upvote_count":"1","timestamp":"1676723700.0"}],"answer_description":"","answers_community":["B (94%)","4%"],"isMC":true,"exam_id":31,"topic":"1","timestamp":"2023-02-17 18:25:00","unix_timestamp":1676654700,"answer_images":[],"question_text":"A company is experiencing sudden increases in demand. The company needs to provision large Amazon EC2 instances from an Amazon Machine Image (AMI). The instances will run in an Auto Scaling group. The company needs a solution that provides minimum initialization latency to meet the demand.\n\nWhich solution meets these requirements?","choices":{"C":"Enable AMI creation and define lifecycle rules in Amazon Data Lifecycle Manager (Amazon DLM). Create an AWS Lambda function that modifies the AMI in the Auto Scaling group.","D":"Use Amazon EventBridge to invoke AWS Backup lifecycle policies that provision AMIs. Configure Auto Scaling group capacity limits as an event source in EventBridge.","B":"Enable Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot. Provision an AMI by using the snapshot. Replace the AMI in the Auto Scaling group with the new AMI.","A":"Use the aws ec2 register-image command to create an AMI from a snapshot. Use AWS Step Functions to replace the AMI in the Auto Scaling group."}},{"id":"kAHFFAVQuNeq2H5lgsQr","question_images":[],"choices":{"A":"Create a new AWS Key Management Service (AWS KMS) encryption key. Use AWS Secrets Manager to create a new secret that uses the KMS key with the appropriate credentials. Associate the secret with the Aurora DB cluster. Configure a custom rotation period of 14 days.","C":"Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system in all EC2 instances of the application tier. Restrict the access to the file on the file system so that the application can read the file and that only super users can modify the file. Implement an AWS Lambda function that rotates the key in Aurora every 14 days and writes new credentials into the file.","B":"Create two parameters in AWS Systems Manager Parameter Store: one for the user name as a string parameter and one that uses the SecureString type for the password. Select AWS Key Management Service (AWS KMS) encryption for the password parameter, and load these parameters in the application tier. Implement an AWS Lambda function that rotates the password every 14 days.","D":"Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon S3 bucket that the application uses to load the credentials. Download the file to the application regularly to ensure that the correct credentials are used. Implement an AWS Lambda function that rotates the Aurora credentials every 14 days and uploads these credentials to the file in the S3 bucket."},"url":"https://www.examtopics.com/discussions/amazon/view/99790-exam-aws-certified-solutions-architect-associate-saa-c03/","timestamp":"2023-02-18 13:31:00","exam_id":31,"answer_description":"","answers_community":["A (100%)"],"answer_images":[],"answer":"A","discussion":[{"timestamp":"1680183000.0","content":"Selected Answer: A\nAWS Secrets Manager allows you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. With this service, you can automate the rotation of secrets, such as database credentials, on a schedule that you choose. The solution allows you to create a new secret with the appropriate credentials and associate it with the Aurora DB cluster. You can then configure a custom rotation period of 14 days to ensure that the credentials are automatically rotated every two weeks, as required by the IT security guidelines. This approach requires the least amount of operational effort as it allows you to manage secrets centrally without modifying your application code or infrastructure.","comment_id":"855804","poster":"elearningtakai","upvote_count":"6"},{"timestamp":"1676746200.0","content":"Answer is A\nTo implement password rotation lifecycles, use AWS Secrets Manager. You can rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle using Secrets Manager.\nhttps://aws.amazon.com/blogs/security/how-to-use-aws-secrets-manager-rotate-credentials-amazon-rds-database-types-oracle/","poster":"jennyka76","comment_id":"813403","upvote_count":"5"},{"upvote_count":"1","poster":"MJ45","timestamp":"1735080720.0","comment_id":"1331260","content":"Selected Answer: A\nMost questions related to rotating credentials - AWS Secrets Manager\nAWS System Parameter Store CANNOT rotate credentials."},{"poster":"LeonSauveterre","timestamp":"1732866960.0","comment_id":"1319613","upvote_count":"1","content":"Selected Answer: A\nWell... I would simply rule out all the options with \"Implement an AWS Lambda function\" given that we need a solution with least operational effort. Why manually compose codes to rotate when you can achieve the same goal automatically?"},{"content":"Selected Answer: A\nCreate a new AWS Key Management Service (AWS KMS) encryption key. Use AWS Secrets Manager to create a new secret that uses the KMS key with the appropriate credentials. Associate the secret with the Aurora DB cluster. Configure a custom rotation period of 14 days","upvote_count":"3","comment_id":"1041331","timestamp":"1697082480.0","poster":"TariqKipkemei"},{"timestamp":"1693915620.0","poster":"Guru4Cloud","content":"Selected Answer: A\nUse AWS Secrets Manager to store the Aurora credentials as a secret\nEncrypt the secret with a KMS key\nConfigure 14 day automatic rotation for the secret\nAssociate the secret with the Aurora DB cluster\nThe key points:\n\nAurora MySQL credentials must be encrypted and rotated every 14 days\nWant to minimize operational effort","comment_id":"999468","upvote_count":"3"},{"timestamp":"1680104040.0","comment_id":"854595","upvote_count":"2","content":"Selected Answer: A\nA: AWS Secrets Manager. Simply this supported rotate feature, and secure to store credentials instead of EFS or S3.","poster":"elearningtakai"},{"content":"Selected Answer: A\nVoting A","comment_id":"828433","poster":"Steve_4542636","upvote_count":"2","timestamp":"1677878340.0"},{"timestamp":"1677132000.0","upvote_count":"2","comment_id":"818838","poster":"LuckyAro","content":"Selected Answer: A\nA proposes to create a new AWS KMS encryption key and use AWS Secrets Manager to create a new secret that uses the KMS key with the appropriate credentials. Then, the secret will be associated with the Aurora DB cluster, and a custom rotation period of 14 days will be configured. AWS Secrets Manager will automate the process of rotating the database credentials, which will reduce the operational effort required to meet the IT security guidelines."},{"poster":"Neha999","upvote_count":"2","timestamp":"1676723460.0","comment_id":"812940","content":"A\nhttps://www.examtopics.com/discussions/amazon/view/59985-exam-aws-certified-solutions-architect-associate-saa-c02/"}],"question_text":"A company hosts a multi-tier web application that uses an Amazon Aurora MySQL DB cluster for storage. The application tier is hosted on Amazon EC2 instances. The company’s IT security guidelines mandate that the database credentials be encrypted and rotated every 14 days.\n\nWhat should a solutions architect do to meet this requirement with the LEAST operational effort?","unix_timestamp":1676723460,"question_id":284,"topic":"1","answer_ET":"A","isMC":true},{"id":"NTkslZxkzPeB8BM7XXDh","unix_timestamp":1676745780,"timestamp":"2023-02-18 19:43:00","topic":"1","question_text":"A company has deployed a web application on AWS. The company hosts the backend database on Amazon RDS for MySQL with a primary DB instance and five read replicas to support scaling needs. The read replicas must lag no more than 1 second behind the primary DB instance. The database routinely runs scheduled stored procedures.\n\nAs traffic on the website increases, the replicas experience additional lag during periods of peak load. A solutions architect must reduce the replication lag as much as possible. The solutions architect must minimize changes to the application code and must minimize ongoing operational overhead.\n\nWhich solution will meet these requirements?","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/99871-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_ET":"A","answers_community":["A (90%)","10%"],"question_images":[],"discussion":[{"poster":"fkie4","content":"i hate this kind of question","upvote_count":"61","comment_id":"833678","timestamp":"1678346880.0"},{"comment_id":"842418","poster":"asoli","content":"Selected Answer: A\nUsing Cache required huge changes in the application. Several things need to change to use cache in front of the DB in the application. So, option B is not correct.\nAurora will help to reduce replication lag for read replica","upvote_count":"13","timestamp":"1679104140.0"},{"timestamp":"1718068740.0","comment_id":"1228203","content":"Selected Answer: A\nYou need to read the question carefully.\nThe solutions architect must minimize changes to the application code = therefore A\nIf this question without this statement, B will be a better choice.","poster":"sheilawu","upvote_count":"3"},{"upvote_count":"4","timestamp":"1712230740.0","comment_id":"1189271","content":"minimize ongoing operational overhead = Not B\nUsing ElastiCache require app change","poster":"JackyCCK"},{"comment_id":"1113023","comments":[{"poster":"JA2018","timestamp":"1732114980.0","comment_id":"1315329","content":"from the same URL:\n\nIs Amazon Aurora MySQL compatible?\nAmazon Aurora is drop-in compatible with existing MySQL open-source databases and adds support for new releases regularly. This means you can easily migrate MySQL databases to and from Aurora using standard import/export tools or snapshots. It also means that most of the code, applications, drivers, and tools you already use with MySQL databases today can be used with Aurora with little or no change. This makes it easy to move applications between the two engines.\n\nYou can see the current Amazon Aurora MySQL release compatibility information in the documentation.","upvote_count":"1"}],"poster":"awsgeek75","content":"Selected Answer: A\nAWS Aurora and Native Functions are least application changes while providing better performance and minimum latency. https://aws.amazon.com/rds/aurora/faqs/\n\nB, C, D require lots of changes to the application so relatively speaking A is least code change and least maintenance/operational overhead.","timestamp":"1704306420.0","upvote_count":"9"},{"upvote_count":"5","content":"Selected Answer: A\nA: Minimal changes to the application code, < 1 second lag\nB: Does not address the replication lag issue at all, requires code changes and adds overhead\nC: Moving from managed RDS to self-managed database on EC2 is ADDING, not minimizing, overhead, PLUS it does not address the replication lag issue\nD: DynamoDB is a NoSQL DB, would require MASSIVE changes to application code and probably even application logic","comment_id":"1108848","poster":"pentium75","timestamp":"1703863860.0"},{"poster":"Murtadhaceit","upvote_count":"3","content":"Selected Answer: A\nimho, B is not valid because it involves extra coding and the question specifically mentions no more coding. Therefore, replacing the current db with another one is not considered as more coding.","timestamp":"1701863760.0","comment_id":"1089283"},{"timestamp":"1697082660.0","content":"Selected Answer: A\nMigrate the database to Amazon Aurora MySQL. Replace the read replicas with Aurora Replicas, and configure Aurora Auto Scaling. Replace the stored procedures with Aurora MySQL native functions","upvote_count":"2","poster":"TariqKipkemei","comment_id":"1041335"},{"timestamp":"1693915200.0","poster":"Guru4Cloud","comment_id":"999459","content":"Selected Answer: A\nMigrate the RDS MySQL database to Amazon Aurora MySQL\nUse Aurora Replicas for read scaling instead of RDS read replicas\nConfigure Aurora Auto Scaling to handle load spikes\nReplace stored procedures with Aurora MySQL native functions","upvote_count":"2"},{"timestamp":"1687375980.0","poster":"MrAWSAssociate","upvote_count":"3","comments":[{"content":"... but migrating their ENTIRE prod database and its replicas to a new platform is not a heavy change?","timestamp":"1692581820.0","comment_id":"986156","poster":"aaroncelestin","upvote_count":"3"}],"content":"Selected Answer: A\nFirst, Elasticache involves heavy change on application code. The question mentioned that \"he solutions architect must minimize changes to the application code\". Therefore B is not suitable and A is more appropriate for the question requirement.","comment_id":"929852"},{"poster":"KMohsoe","content":"Selected Answer: B\nWhy not B? Please explain to me.","upvote_count":"2","timestamp":"1684742100.0","comments":[{"upvote_count":"2","comment_id":"1017695","timestamp":"1695728760.0","poster":"Terion","content":"It wouldn't have the most up to date info since it must no lag in relation to the main DB"},{"poster":"pentium75","comment_id":"1108845","timestamp":"1703863680.0","upvote_count":"2","content":"How would adding a cache \"reduce the replication lag\" between the primary instance and the read replicas? Plus, it would require \"changes to the application code\" that we want to avoid. The \"AWS Lambda functions\" would create \"ongoing operational overhead\" that we're also asked to avoid."}],"comment_id":"903833"},{"comments":[{"upvote_count":"1","timestamp":"1710817440.0","poster":"njufi","content":"I agree with your explanation. Additionally, considering the requirement that \"the read replicas must lag no more than 1 second behind the primary DB instance,\" it's crucial to ensure that Elasticache for Redis also maintains this tight synchronization window. This implies that the main RDS instance would need to synchronize an additional database, potentially exacerbating lag during peak times rather than alleviating it.","comment_id":"1176954"}],"content":"Option A is the most appropriate solution for reducing replication lag without significant changes to the application code and minimizing ongoing operational overhead. Migrating the database to Amazon Aurora MySQL allows for improved replication performance and higher scalability compared to Amazon RDS for MySQL. Aurora Replicas provide faster replication, reducing the replication lag, and Aurora Auto Scaling ensures that there are enough Aurora Replicas to handle the incoming traffic. Additionally, Aurora MySQL native functions can replace the stored procedures, reducing the load on the database and improving performance.\n\nOption B is not the best solution since adding an ElastiCache for Redis cluster does not address the replication lag issue, and the cache may not have the most up-to-date information. Additionally, replacing the stored procedures with AWS Lambda functions adds additional complexity and may not improve performance.","upvote_count":"5","comment_id":"835621","poster":"kaushald","timestamp":"1678505760.0"},{"content":"Selected Answer: B\na,b are confusing me..\ni would like to go with b..","upvote_count":"1","comments":[{"comment_id":"835276","poster":"bangfire","timestamp":"1678470060.0","content":"Option B is incorrect because it suggests using ElastiCache for Redis as a caching layer in front of the database, but this would not necessarily reduce the replication lag on the read replicas. Additionally, it suggests replacing the stored procedures with AWS Lambda functions, which may require significant changes to the application code.","comments":[{"timestamp":"1679743080.0","poster":"lizzard812","upvote_count":"2","comment_id":"850066","content":"Yes and moreover Redis requires app refactoring which is a solid operational overhead"}],"upvote_count":"6"}],"comment_id":"834510","poster":"[Removed]","timestamp":"1678405020.0"},{"content":"Selected Answer: B\nBy using ElastiCache you avoid a lot of common issues you might encounter. ElastiCache is a database caching solution. ElastiCache Redis per se, supports failover and Multi-AZ. And Most of all, ElastiCache is well suited to place in front of RDS.\n\nMigrating a database such as option A, requires operational overhead.","upvote_count":"2","poster":"Nel8","comments":[{"comment_id":"1108846","poster":"pentium75","upvote_count":"2","timestamp":"1703863740.0","content":"Database migration is one-time work, NOT \"operational overhead\". Plus, RDS for MySQL to Aurora with MySQL compatibility is not a big deal, and \"minimizes changes to the application code\" as requested."}],"comment_id":"825374","timestamp":"1677625680.0"},{"poster":"bdp123","content":"Selected Answer: A\nAurora can have up to 15 read replicas - much faster than RDS\nhttps://aws.amazon.com/rds/aurora/","comment_id":"818235","comments":[{"poster":"ChrisG1454","comments":[{"poster":"ChrisG1454","upvote_count":"1","timestamp":"1678514520.0","comment_id":"835690","content":"You can invoke an Amazon Lambda function from an Amazon Aurora MySQL-Compatible Edition DB cluster with the \"native function\"....\n\nhttps://docs.amazonaws.cn/en_us/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Integrating.Lambda.html"}],"comment_id":"830682","timestamp":"1678093980.0","content":"\" As a result, all Aurora Replicas return the same data for query results with minimal replica lag. This lag is usually much less than 100 milliseconds after the primary instance has written an update \"\n\nReference: \nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html","upvote_count":"3"}],"timestamp":"1677091320.0","upvote_count":"5"},{"content":"Answer - A\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PostgreSQL.Replication.ReadReplicas.html\n---------------------------------------------------------------------------------------\nYou can scale reads for your Amazon RDS for PostgreSQL DB instance by adding read replicas to the instance. As with other Amazon RDS database engines, RDS for PostgreSQL uses the native replication mechanisms of PostgreSQL to keep read replicas up to date with changes on the source DB. For general information about read replicas and Amazon RDS, see Working with read replicas.","comment_id":"813395","timestamp":"1676745780.0","poster":"jennyka76","upvote_count":"3"}],"question_id":285,"answer_description":"","answer":"A","choices":{"C":"Migrate the database to a MySQL database that runs on Amazon EC2 instances. Choose large, compute optimized EC2 instances for all replica nodes. Maintain the stored procedures on the EC2 instances.","A":"Migrate the database to Amazon Aurora MySQL. Replace the read replicas with Aurora Replicas, and configure Aurora Auto Scaling. Replace the stored procedures with Aurora MySQL native functions.","D":"Migrate the database to Amazon DynamoDB. Provision a large number of read capacity units (RCUs) to support the required throughput, and configure on-demand capacity scaling. Replace the stored procedures with DynamoDB streams.","B":"Deploy an Amazon ElastiCache for Redis cluster in front of the database. Modify the application to check the cache before the application queries the database. Replace the stored procedures with AWS Lambda functions."},"exam_id":31,"isMC":true}],"exam":{"name":"AWS Certified Solutions Architect - Associate SAA-C03","id":31,"lastUpdated":"11 Apr 2025","provider":"Amazon","isMCOnly":true,"isBeta":false,"numberOfQuestions":1019,"isImplemented":true},"currentPage":57},"__N_SSP":true}