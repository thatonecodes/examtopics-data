{"pageProps":{"questions":[{"id":"XWb2jT81HAyyxoFsRmCs","unix_timestamp":1678453440,"answer_images":[],"answers_community":["C (100%)"],"question_id":316,"choices":{"C":"Automated backups","D":"Multi-AZ deployments","B":"Manual snapshots","A":"Read replicas"},"discussion":[{"poster":"Uzbekistan","comment_id":"1164946","upvote_count":"4","content":"Selected Answer: C\nAmazon RDS provides automated backups, which can be configured to take regular snapshots of the database instance. By enabling automated backups and setting the retention period to 30 days, the company can ensure that it retains backups for up to 30 days. Additionally, Amazon RDS allows for point-in-time recovery within the retention period, enabling the restoration of the database to its state from any point within the last 30 days, including 5 minutes before any change. This feature provides the required capability to recover from accidental data loss incidents.","timestamp":"1725375360.0"},{"timestamp":"1709325360.0","upvote_count":"4","content":"Selected Answer: C\nAutomated backups allow you to recover your database to any point in time within your specified retention period, which can be up to 35 days. The recovery process creates a new Amazon RDS instance with a new endpoint, and the process takes time proportional to the size of the database. Automated backups are enabled by default and occur daily during the backup window. This feature provides an easy and convenient way to recover from data loss incidents such as the one described in the scenario.","poster":"Guru4Cloud","comment_id":"996348"},{"comment_id":"855863","poster":"elearningtakai","content":"Selected Answer: C\nOption C, Automated backups, will meet the requirement. Amazon RDS allows you to automatically create backups of your DB instance. Automated backups enable point-in-time recovery (PITR) for your DB instance down to a specific second within the retention period, which can be up to 35 days. By setting the retention period to 30 days, the company can restore the database to its state from up to 5 minutes before any change within the last 30 days.","comments":[{"comment_id":"926318","upvote_count":"2","content":"I selected C as well, but still don't know how the automatic backup could have a copy 5 minutes before any change. AWS doc states \"Automated backups occur daily during the preferred backup window. \" https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html. \nI think the answer maybe A, as read replica will be kept sync and then restore from the read replica. could an expert help?","poster":"joechen2023","timestamp":"1702861140.0","comments":[{"comment_id":"1113204","poster":"awsgeek75","upvote_count":"2","timestamp":"1720041000.0","content":"\"the company wants the ability to restore the database to its state from 5 minutes before any change\"\nThe automatic backup takes a backup every 5 minutes. This means it can restore the database to 5 minutes in the past."},{"upvote_count":"2","poster":"TheFivePips","timestamp":"1724712420.0","comment_id":"1160170","content":"Automated backups enable point-in-time recovery (PITR) for your DB instance down to a specific second within the retention period, which can be up to 35 days"}]}],"timestamp":"1696083900.0","upvote_count":"4"},{"timestamp":"1696037340.0","upvote_count":"3","content":"Selected Answer: C\nC: Automated Backups\n\nhttps://aws.amazon.com/rds/features/backup/","comment_id":"855147","poster":"gold4otas"},{"timestamp":"1695533880.0","content":"Selected Answer: C\nAutomated Backups...","poster":"WherecanIstart","comment_id":"849089","upvote_count":"3"},{"content":"Selected Answer: C\nccccccccc","upvote_count":"1","timestamp":"1694343840.0","poster":"[Removed]","comment_id":"835005"}],"question_text":"A company runs a web application that is backed by Amazon RDS. A new database administrator caused data loss by accidentally editing information in a database table. To help recover from this type of incident, the company wants the ability to restore the database to its state from 5 minutes before any change within the last 30 days.\n\nWhich feature should the solutions architect include in the design to meet this requirement?","exam_id":31,"answer_ET":"C","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/102127-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_description":"","answer":"C","timestamp":"2023-03-10 14:04:00","isMC":true,"topic":"1"},{"id":"sM6jrbtvn9waMPByvhil","question_id":317,"answer_description":"","answers_community":["D (85%)","C (15%)"],"isMC":true,"choices":{"C":"Apply fine-grained IAM permissions to the premium content in the DynamoDB table.","A":"Enable API caching and throttling on the API Gateway API.","B":"Set up AWS WAF on the API Gateway API. Create a rule to filter users who have a subscription.","D":"Implement API usage plans and API keys to limit the access of users who do not have a subscription."},"url":"https://www.examtopics.com/discussions/amazon/view/102128-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_ET":"D","topic":"1","discussion":[{"comment_id":"996340","content":"Selected Answer: D\nImplementing API usage plans and API keys is a straightforward way to restrict access to specific users or groups based on subscriptions. It allows you to control access at the API level and doesn't require extensive changes to your existing architecture. This solution provides a clear and manageable way to enforce access restrictions without complicating other parts of the application","timestamp":"1709324640.0","upvote_count":"10","poster":"Guru4Cloud"},{"comment_id":"1344849","content":"Selected Answer: C\nSegurança robusta:\n\nA lógica de controle de acesso é aplicada no nível do banco de dados, garantindo que usuários não autorizados não consigam acessar os dados premium, mesmo que tentem burlar o aplicativo.","poster":"Rcosmos","upvote_count":"1","timestamp":"1737561180.0"},{"timestamp":"1725376260.0","comment_id":"1164956","upvote_count":"1","content":"Selected Answer: C\nChat GPT said: \nOption C, \"Apply fine-grained IAM permissions to the premium content in the DynamoDB table,\" would likely involve the least operational overhead.\nHere's why:\nGranular Control: IAM permissions allow you to control access at a very granular level, including specific actions (e.g., GetItem, PutItem) on individual resources (e.g., DynamoDB tables).\nIntegration with Cognito: IAM policies can be configured to allow access based on the identity of the user authenticated through Cognito. You can create IAM roles or policies that grant access to users with specific attributes or conditions, such as having a subscription.\nMinimal Configuration Changes: This solution primarily involves configuring IAM policies for access control in DynamoDB, which can be done with minimal changes to the existing application architecture.","poster":"Uzbekistan"},{"poster":"awsgeek75","comment_id":"1113208","comments":[{"upvote_count":"2","timestamp":"1720041540.0","poster":"awsgeek75","content":"Also, option A is for performance and not for security\noption B, WAF cannot control access based on subscription without massive custom coding which will be a big operational overhead","comment_id":"1113210"},{"poster":"awsgeek75","content":"I had to chose D but must have clicked C incorrectly. It is D as my explanation is about D not C! C is the wrong answer.","upvote_count":"2","comment_id":"1126089","timestamp":"1721317800.0"}],"timestamp":"1720041420.0","upvote_count":"2","content":"Selected Answer: C\nC is correct as per the link and doc:\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html#apigateway-usage-plans-best-practices\n\nD: API keys cannot be used to limit access and this can only be done via methods defined in above link"},{"comments":[{"upvote_count":"2","timestamp":"1720041360.0","poster":"awsgeek75","comment_id":"1113206","content":"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html#apigateway-usage-plans-best-practices\n\nCorrect link"}],"timestamp":"1716555240.0","poster":"lipi0035","comment_id":"1079380","content":"In the same document https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html if you scroll down, it says `Don't use API keys for authentication or authorization to control access to your APIs. If you have multiple APIs in a usage plan, a user with a valid API key for one API in that usage plan can access all APIs in that usage plan. Instead, to control access to your API, use an IAM role, a Lambda authorizer, or an Amazon Cognito user pool.`\n\nIn the same document at the bottom, it says \"If you're using a developer portal to publish your APIs, note that all APIs in a given usage plan are subscribable, even if you haven't made them visible to your customers.\"\n\nI go with C","upvote_count":"2"},{"upvote_count":"2","poster":"TariqKipkemei","timestamp":"1713327120.0","content":"Selected Answer: D\nAfter you create, test, and deploy your APIs, you can use API Gateway usage plans to make them available as product offerings for your customers. You can configure usage plans and API keys to allow customers to access selected APIs, and begin throttling requests to those APIs based on defined limits and quotas. These can be set at the API, or API method level.\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html#:~:text=Creating%20and%20using-,usage%20plans,-with%20API%20keys","comment_id":"1045511"},{"content":"D\nOption D involves implementing API usage plans and API keys. By associating specific API keys with users who have a valid subscription, you can control access to the premium content.","timestamp":"1703131500.0","comment_id":"928964","poster":"marufxplorer","upvote_count":"2"},{"timestamp":"1698848580.0","comment_id":"886218","poster":"kruasan","content":"Selected Answer: D\nA. This would not actually limit access based on subscriptions. It helps optimize and control API usage, but does not address the core requirement.\nB. This could work by checking user subscription status in the WAF rule, but would require ongoing management of WAF and increases operational overhead.\nC. This is a good approach, using IAM permissions to control DynamoDB access at a granular level based on subscriptions. However, it would require managing IAM permissions which adds some operational overhead.\nD. This option uses API Gateway mechanisms to limit API access based on subscription status. It would require the least amount of ongoing management and changes, minimizing operational overhead. API keys could be easily revoked/changed as subscription status changes.","upvote_count":"4"},{"timestamp":"1697526720.0","upvote_count":"3","comment_id":"872434","poster":"imvb88","content":"CD both possible but D is more suitable since it mentioned in https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html \n\nA,B not relevant."},{"upvote_count":"3","poster":"elearningtakai","content":"Selected Answer: D\nThe solution that will meet the requirement with the least operational overhead is to implement API Gateway usage plans and API keys to limit access to premium content for users who do not have a subscription.\nOption A is incorrect because API caching and throttling are not designed for authentication or authorization purposes, and it does not provide access control.\nOption B is incorrect because although AWS WAF is a useful tool to protect web applications from common web exploits, it is not designed for authorization purposes, and it might require additional configuration, which increases the operational overhead.\nOption C is incorrect because although IAM permissions can restrict access to data stored in a DynamoDB table, it does not provide a mechanism for limiting access to specific content based on the user subscription. Moreover, it might require a significant amount of additional IAM permissions configuration, which increases the operational overhead.","timestamp":"1696084080.0","comment_id":"855864"},{"poster":"klayytech","timestamp":"1695576780.0","comment_id":"849580","content":"Selected Answer: D\nTo meet the requirement with the least operational overhead, you can implement API usage plans and API keys to limit the access of users who do not have a subscription. This way, you can control access to your API Gateway APIs by requiring clients to submit valid API keys with requests. You can associate usage plans with API keys to configure throttling and quota limits on individual client accounts.","upvote_count":"3"},{"poster":"techhb","content":"answer is D ,if looking for least overhead \nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html\nC will achieve it but operational overhead is high.","comment_id":"838774","upvote_count":"3","timestamp":"1694682780.0"},{"content":"Selected Answer: D\nBoth C&D are valid solution\nAccording to ChatGPT:\n\"Applying fine-grained IAM permissions to the premium content in the DynamoDB table is a valid approach, but it requires more effort in managing IAM policies and roles for each user, making it more complex and adding operational overhead.\"","upvote_count":"2","comment_id":"837818","poster":"quentin17","timestamp":"1694591700.0"},{"content":"Selected Answer: D\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html","upvote_count":"4","comment_id":"836940","timestamp":"1694510640.0","poster":"Karlos99"},{"poster":"[Removed]","upvote_count":"1","content":"Selected Answer: C\nccccccccc","comments":[{"poster":"pentium75","upvote_count":"2","timestamp":"1719756660.0","comment_id":"1109823","content":"\"Fine-grained permissions\" for only two groups of users, hell no.\n\"IAM permissions\" for customers, also no."}],"timestamp":"1694344020.0","comment_id":"835007"}],"timestamp":"2023-03-10 14:07:00","exam_id":31,"answer_images":[],"question_images":[],"unix_timestamp":1678453620,"question_text":"A company’s web application consists of an Amazon API Gateway API in front of an AWS Lambda function and an Amazon DynamoDB database. The Lambda function handles the business logic, and the DynamoDB table hosts the data. The application uses Amazon Cognito user pools to identify the individual users of the application. A solutions architect needs to update the application so that only users who have a subscription can access premium content.\n\nWhich solution will meet this requirement with the LEAST operational overhead?","answer":"D"},{"id":"QzbDhTnCQXExIiZsrgiH","exam_id":31,"unix_timestamp":1678454280,"timestamp":"2023-03-10 14:18:00","answers_community":["A (98%)","2%"],"answer":"A","question_images":[],"topic":"1","discussion":[{"content":"Selected Answer: A\nNLBs allow UDP traffic (ALBs don't support UDP)\nGlobal Accelerator uses Anycast IP addresses and its global network to intelligently route users to the optimal endpoint\nUsing NLBs as Global Accelerator endpoints provides improved availability and DDoS protection.","timestamp":"1693592340.0","upvote_count":"12","comment_id":"996337","poster":"Guru4Cloud"},{"content":"Selected Answer: A\nC - D: Cloudfront don't support UDP/TCP\nB: Global accelerator don't support ALB\nA is correct","timestamp":"1684827840.0","upvote_count":"5","poster":"lucdt4","comment_id":"904696"},{"content":"Selected Answer: C\nCloudFront's origin can be on-premises sources. \n\nCheck this out: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/DownloadDistS3AndCustomOrigins.html#concept_CustomOrigin\n\n\"A custom origin is an HTTP server, for example, a web server. The HTTP server can be an Amazon EC2 instance or an HTTP server that you host somewhere else. \"","upvote_count":"1","timestamp":"1732120620.0","poster":"JA2018","comment_id":"1315392"},{"poster":"sandordini","content":"Selected Answer: A\nNon-HTTP, Massive performance: NLB, UDP: AWS Global Accelerator","comment_id":"1195632","upvote_count":"3","timestamp":"1713115080.0"},{"comment_id":"1109828","timestamp":"1703953140.0","content":"Selected Answer: A\nNeither ALB (B+D) nor CloudFront (C+D) do support UDP.","poster":"pentium75","upvote_count":"5"},{"poster":"TariqKipkemei","content":"Selected Answer: A\nUDP = NLB and Global Accelerator","timestamp":"1697516100.0","upvote_count":"4","comment_id":"1045512"},{"upvote_count":"4","comment_id":"942737","poster":"live_reply_developers","content":"Selected Answer: A\nNLB + GA support UDP/TCP","timestamp":"1688473080.0"},{"comment_id":"938564","poster":"Gooniegoogoo","upvote_count":"3","timestamp":"1688071200.0","content":"good reference https://blog.cloudcraft.co/alb-vs-nlb-which-aws-load-balancer-fits-your-needs/"},{"comment_id":"883067","poster":"SkyZeroZx","timestamp":"1682632020.0","upvote_count":"4","content":"Selected Answer: A\nUDP = NBL \nUDP = GLOBAL ACCELERATOR \nUPD NOT WORKING WITH CLOUDFRONT\nANS IS A"},{"content":"Selected Answer: A\nMore discussions at: https://www.examtopics.com/discussions/amazon/view/51508-exam-aws-certified-solutions-architect-associate-saa-c02/","upvote_count":"1","timestamp":"1679771100.0","comment_id":"850390","poster":"MssP"},{"comment_id":"845479","comments":[{"content":"It could be valid but I think A is better. Uses the AWS global network to optimize the path from users to applications, improving the performance of TCP and UDP traffic","comment_id":"850383","upvote_count":"2","timestamp":"1679770740.0","poster":"MssP"},{"comment_id":"872869","content":"Latency based routing is already using in the application, so AWS global network will optimize the path from users to applications.","timestamp":"1681747260.0","upvote_count":"2","poster":"Shrestwt"}],"timestamp":"1679368860.0","upvote_count":"2","content":"Why is C not correct - does anyone know?","poster":"Grace83"},{"timestamp":"1678736280.0","upvote_count":"4","poster":"FourOfAKind","content":"Selected Answer: A\nUDP == NLB\nMust be hosted on-premises != CloudFront","comment_id":"838231","comments":[{"poster":"imvb88","comment_id":"872440","timestamp":"1681716120.0","upvote_count":"2","content":"actually CloudFront's origin can be on-premises. Source: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/DownloadDistS3AndCustomOrigins.html#concept_CustomOrigin\n\n\"A custom origin is an HTTP server, for example, a web server. The HTTP server can be an Amazon EC2 instance or an HTTP server that you host somewhere else. \""}]},{"poster":"[Removed]","upvote_count":"3","comment_id":"835016","content":"Selected Answer: A\naaaaaaaa","timestamp":"1678454280.0"}],"answer_images":[],"question_text":"A company is using Amazon Route 53 latency-based routing to route requests to its UDP-based application for users around the world. The application is hosted on redundant servers in the company's on-premises data centers in the United States, Asia, and Europe. The company’s compliance requirements state that the application must be hosted on premises. The company wants to improve the performance and availability of the application.\n\nWhat should a solutions architect do to meet these requirements?","answer_ET":"A","choices":{"A":"Configure three Network Load Balancers (NLBs) in the three AWS Regions to address the on-premises endpoints. Create an accelerator by using AWS Global Accelerator, and register the NLBs as its endpoints. Provide access to the application by using a CNAME that points to the accelerator DNS.","D":"Configure three Application Load Balancers (ALBs) in the three AWS Regions to address the on-premises endpoints. In Route 53, create a latency-based record that points to the three ALBs, and use it as an origin for an Amazon CloudFront distribution. Provide access to the application by using a CNAME that points to the CloudFront DNS.","C":"Configure three Network Load Balancers (NLBs) in the three AWS Regions to address the on-premises endpoints. In Route 53, create a latency-based record that points to the three NLBs, and use it as an origin for an Amazon CloudFront distribution. Provide access to the application by using a CNAME that points to the CloudFront DNS.","B":"Configure three Application Load Balancers (ALBs) in the three AWS Regions to address the on-premises endpoints. Create an accelerator by using AWS Global Accelerator, and register the ALBs as its endpoints. Provide access to the application by using a CNAME that points to the accelerator DNS."},"isMC":true,"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/102131-exam-aws-certified-solutions-architect-associate-saa-c03/","question_id":318},{"id":"3NgI6wr3QeSqvrwbR2HM","answer_description":"","answer_ET":"A","discussion":[{"timestamp":"1707559800.0","poster":"lostmagnet001","upvote_count":"10","content":"Selected Answer: A\ni get confused, the question saids \"NEW\" users... if you apply this password policy it would affect all the users in the AWS account....","comment_id":"1146031"},{"content":"The question is for new users, answer A is not exact for that case.","timestamp":"1695479940.0","upvote_count":"9","poster":"angel_marquina","comment_id":"1015006"},{"upvote_count":"1","timestamp":"1737562380.0","poster":"Rcosmos","content":"Selected Answer: U\nPor que as outras opções não são adequadas?\nB. Definir uma política de senha para cada usuário do IAM:\n\nO IAM não permite políticas de senha configuradas individualmente para usuários. As políticas de senha são aplicadas a nível da conta.","comment_id":"1344860"},{"comment_id":"1320061","upvote_count":"1","timestamp":"1732942260.0","poster":"LeonSauveterre","content":"Selected Answer: A\nA: Overall password policy for the entire AWS account - affects new users immediately, but not old users immediately. Next time the old users change their passwords, the new rule will apply then.\n\nB: Logically wrong. If you already have a user, then that user must've already have a password. So when you individually set a password policy for these users, they are only affected next time they change their passwords, just like I mentioned above about option A. Plus, this is really tiresome because this is user-wise, and there might be too many users, and the work you must do manually is increasing horribly."},{"poster":"[Removed]","content":"Selected Answer: B\nBecause its mentioned \"all new users\"","timestamp":"1717247460.0","comment_id":"1222662","comments":[{"content":"Ignore the above, seems to be custom policy can help in the case, so A should be right","upvote_count":"1","comment_id":"1222667","timestamp":"1717248180.0","poster":"[Removed]"}],"upvote_count":"1"},{"content":"Selected Answer: A\nYou can set a custom password policy on your AWS account to specify complexity requirements and mandatory rotation periods for your IAM users' passwords. When you create or change a password policy, most of the password policy settings are enforced the next time your users change their passwords. However, some of the settings are enforced immediately.\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_passwords_account-policy.html#:~:text=Setting%20an%20account-,password%20policy,-for%20IAM%20users","comment_id":"1045515","timestamp":"1697516460.0","upvote_count":"4","poster":"TariqKipkemei"},{"comment_id":"849582","content":"Selected Answer: A\nTo accomplish this, the solutions architect should set an overall password policy for the entire AWS account. This policy will apply to all IAM users in the account, including new users.","timestamp":"1679686860.0","poster":"klayytech","upvote_count":"4"},{"content":"Selected Answer: A\nSet overall password policy ...","poster":"WherecanIstart","timestamp":"1679279040.0","comment_id":"844435","upvote_count":"3"},{"timestamp":"1678884960.0","comment_id":"839887","poster":"kampatra","content":"Selected Answer: A\nA is correct","upvote_count":"2"},{"poster":"[Removed]","timestamp":"1678454460.0","content":"Selected Answer: A\naaaaaaa","comment_id":"835017","upvote_count":"5"}],"isMC":true,"question_images":[],"question_text":"A solutions architect wants all new users to have specific complexity requirements and mandatory rotation periods for IAM user passwords.\n\nWhat should the solutions architect do to accomplish this?","answer_images":[],"answers_community":["A (94%)","3%"],"timestamp":"2023-03-10 14:21:00","exam_id":31,"answer":"A","question_id":319,"choices":{"B":"Set a password policy for each IAM user in the AWS account.","A":"Set an overall password policy for the entire AWS account.","D":"Attach an Amazon CloudWatch rule to the Create_newuser event to set the password with the appropriate requirements.","C":"Use third-party vendor software to set password requirements."},"unix_timestamp":1678454460,"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/102132-exam-aws-certified-solutions-architect-associate-saa-c03/"},{"id":"StBRy4SaTZl2sMM0S9bO","question_text":"A company has migrated an application to Amazon EC2 Linux instances. One of these EC2 instances runs several 1-hour tasks on a schedule. These tasks were written by different teams and have no common programming language. The company is concerned about performance and scalability while these tasks run on a single instance. A solutions architect needs to implement a solution to resolve these concerns.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","choices":{"D":"Create an Amazon Machine Image (AMI) of the EC2 instance that runs the tasks. Create an Auto Scaling group with the AMI to run multiple copies of the instance.","B":"Convert the EC2 instance to a container. Use AWS App Runner to create the container on demand to run the tasks as jobs.","C":"Copy the tasks into AWS Lambda functions. Schedule the Lambda functions by using Amazon EventBridge (Amazon CloudWatch Events).","A":"Use AWS Batch to run the tasks as jobs. Schedule the jobs by using Amazon EventBridge (Amazon CloudWatch Events)."},"question_images":[],"answer_images":[],"answer":"A","question_id":320,"exam_id":31,"answer_ET":"A","isMC":true,"unix_timestamp":1678454580,"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/102133-exam-aws-certified-solutions-architect-associate-saa-c03/","discussion":[{"comments":[{"poster":"FourOfAKind","upvote_count":"32","comments":[{"content":"can't be C","comment_id":"838236","upvote_count":"9","poster":"FourOfAKind","timestamp":"1694626980.0"}],"timestamp":"1694626980.0","comment_id":"838235","content":"But the question states \"several 1-hour tasks on a schedule\", and the maximum runtime for Lambda is 15 minutes, so it can't be A."},{"poster":"JTruong","content":"Lambda can only execute job under 15 mins* so C can't be the answer","timestamp":"1719859980.0","comment_id":"1111427","upvote_count":"6"},{"comment_id":"838281","upvote_count":"12","timestamp":"1694631180.0","content":"It’s not because time limit of lambda is 15 minutes","poster":"smgsi"},{"timestamp":"1713427140.0","poster":"wsdasdasdqwdaw","upvote_count":"7","comment_id":"1046733","content":"AWS Batch - As a fully managed service, AWS Batch helps you to run batch computing workloads of any scale. AWS Batch automatically provisions compute resources and optimizes the workload distribution based on the quantity and scale of the workloads. With AWS Batch, there's no need to install or manage batch computing software, so you can focus your time on analyzing results and solving problems. https://docs.aws.amazon.com/batch/latest/userguide/what-is-batch.html ---> I am voting for A, C would have been OK if the time was within 15 minutes."}],"comment_id":"837495","timestamp":"1694559060.0","upvote_count":"10","content":"Selected Answer: C\nquestion said \"These tasks were written by different teams and have no common programming language\", and key word \"scalable\". Only Lambda can fulfil these. Lambda can be done in different programming languages, and it is scalable","poster":"fkie4"},{"comment_id":"1109833","timestamp":"1719757620.0","poster":"pentium75","upvote_count":"9","content":"\"Running on a schedule\" = Batch\nNot C due Lambda < 15 min\nNot D, auto-scaling doesn't make sense for things running on a schedule"},{"content":"Selected Answer: D\nAnswer = D\n\"performance and scalability while these tasks run on a single instance\" They gave me a legacy application and want it to autoscale for performace. They dont want it to run on a single EC2 instance. Shouldn't I make an AMI and provision multiple EC2 instances in an autoscaling group ? I could put an ALB in front of it. I wont have to deal with \"uncommon programming languages\" inside the application... Just a thought..","upvote_count":"2","poster":"foha2012","timestamp":"1721860920.0","comment_id":"1131209"},{"content":"Selected Answer: A\nAWS Batch is for jobs running at schedule on EC2. so option A\nB is operational overhead\nC Lambda is 15 mins max execution\nD Scaling is not a requirement","timestamp":"1720041900.0","comment_id":"1113214","poster":"awsgeek75","upvote_count":"7"},{"timestamp":"1717052340.0","comment_id":"1084125","upvote_count":"4","poster":"meowruki","content":"Selected Answer: A\nAWS Batch: AWS Batch is a fully managed service for running batch computing workloads. It dynamically provisions the optimal quantity and type of compute resources based on the volume and specific resource requirements of the batch jobs. It allows you to run tasks written in different programming languages with minimal operational overhead."},{"content":"Selected Answer: A\nThe tast working for hour but lambda function timeout is 15 minutes. So vote A.","comment_id":"1076012","timestamp":"1716257160.0","poster":"hungta","upvote_count":"2"},{"comment_id":"1041906","upvote_count":"2","poster":"youdelin","timestamp":"1712938800.0","content":"I know guys are stressed out trying to figure this exam out okay, but no matter what people say, with or without reasoning, at least put your mouth clean. Going like AAA is an issue, but talking shi* on him just because he didn't write down the reasoning is your fault."},{"poster":"Guru4Cloud","upvote_count":"6","comment_id":"996330","content":"Selected Answer: A\nIt can run heterogeneous workloads and tasks without needing to convert them to a common format.\nAWS Batch manages the underlying compute resources - no need to manage containers, Lambda functions or Auto Scaling groups.","timestamp":"1709323620.0"},{"timestamp":"1706843340.0","content":"AWS Lambda function can only be run for 15 mins","upvote_count":"2","poster":"zjcorpuz","comment_id":"969532"},{"content":"Selected Answer: A\nmaximum runtime for Lambda is 15 minutes, hence A","upvote_count":"3","comment_id":"947500","timestamp":"1704834300.0","poster":"jaydesai8"},{"timestamp":"1701264660.0","comment_id":"909340","upvote_count":"2","content":"Selected Answer: A\nI also go with A.","poster":"antropaws"},{"upvote_count":"1","poster":"omoakin","timestamp":"1701220020.0","content":"C. Copy the tasks into AWS Lambda functions. Schedule the Lambda functions by using Amazon EventBridge (Amazon CloudWatch Events)","comments":[{"comment_id":"909611","poster":"ruqui","timestamp":"1701287640.0","content":"wrong, Lambda maximum runtime is 15 minutes and the tasks run for an hour","upvote_count":"3"}],"comment_id":"908908"},{"poster":"KMohsoe","upvote_count":"2","comment_id":"906337","content":"Selected Answer: A\nB and D out!\nA and C let's think! \nAWS Lambda functions are time limited.\nSo, Option A","timestamp":"1700893740.0"},{"content":"AAAAAAAAAAAAAAAAA\nbecause lambda only run within 15 minutes","comment_id":"904703","poster":"lucdt4","upvote_count":"3","timestamp":"1700733180.0"},{"comment_id":"900725","content":"Selected Answer: A\nAnswer is A.\nCould have been C but AWS Lambda functions can be only configured to run up to 15 minutes per execution. While the task in question need an 1hour to run,","poster":"TariqKipkemei","upvote_count":"5","timestamp":"1700289900.0"},{"poster":"luisgu","comments":[{"content":"Things 'running on a schedule' = Batch, not autoscaling","timestamp":"1719757380.0","poster":"pentium75","upvote_count":"2","comment_id":"1109829"}],"comment_id":"894598","timestamp":"1699688040.0","upvote_count":"2","content":"Selected Answer: D\nquestion is asking for the LEAST operational overhead. With batch, you have to create the compute environment, create the job queue, create the job definition and create the jobs --> more operational overhead than creating an ASG"},{"comment_id":"878591","timestamp":"1698077520.0","upvote_count":"2","content":"Selected Answer: A\nA not C\nThe maximum AWS Lambda function run time is 15 minutes. If a Lambda function runs for longer than 15 minutes, it will be terminated by AWS Lambda. This limit is in place to prevent the Lambda environment from becoming stale and to ensure that resources are available for other functions. If a task requires more than 15 minutes to complete, a different AWS service or architecture may be better suited for the use case.","poster":"WELL_212"},{"upvote_count":"1","content":"Selected Answer: C\nCCCCCCCCCC","poster":"neosis91","comments":[],"comment_id":"876168","timestamp":"1697858820.0"},{"upvote_count":"1","poster":"neosis91","comment_id":"875773","timestamp":"1697817960.0","content":"Selected Answer: A\nAAAAAAAAA"},{"comment_id":"870922","timestamp":"1697374080.0","upvote_count":"3","poster":"udo2020","content":"It must be A!\nIn general, AWS Lambda can be more cost-effective for smaller, short-lived tasks or for event-driven computing use cases. For long running or computation heavy tasks, AWS Batch can be more cost-effective, as it allows you to provision and manage a more robust computing environment."},{"comment_id":"869908","upvote_count":"2","timestamp":"1697249220.0","poster":"Strib","content":"Selected Answer: B\nI think the problem is that: 1. Amount 1-hour execution. 2. No one common language. So I think the better is B."},{"timestamp":"1697205540.0","comment_id":"869474","poster":"ErfanKh","content":"Selected Answer: A\nA for me, Lambda has 15 minute time out cant be C","upvote_count":"2"},{"upvote_count":"2","comments":[{"comment_id":"909614","poster":"ruqui","content":"wrong! if you setup an AMI that is configured to run all the jobs, then all the instances of the ASG will be running all the jobs at the same time!! this solution won't address any scalability and performance problems","timestamp":"1701287880.0","upvote_count":"4"}],"content":"D is the answer. [The best solution is to create an AMI of the EC2 instance, and then use it as a template for which to launch additional instances using an Auto Scaling Group. This removes the issues of performance, scalability, and redundancy by allowing the EC2 instances to automatically scale and be launched across multiple Availability Zones.]from udemy","comment_id":"865272","poster":"dangoooooo","timestamp":"1696824900.0"},{"upvote_count":"2","content":"Selected Answer: A\nI am leaning towards A because:\n1. Each individual job runs for about 1 hr., not ideal for lambda.\n2. The concern is performance/scalability. If we break these multiple jobs into individual tasks and let AWS batch handle them, we might have less operational overhead to maintain and use the scalability power of AWS batch - Ec2 scaling. \n3. The other options do not address the issue of breaking down multiple jobs running on the same machine. I feel that the programming language keyword is here to confuse us. \n\nGL","comment_id":"858854","timestamp":"1696251780.0","poster":"kraken21"},{"timestamp":"1695756360.0","poster":"klayytech","upvote_count":"2","comment_id":"851435","content":"Selected Answer: A\nLambda functions are short lived; the Lambda max timeout is 900 seconds (15 minutes). This can be difficult to manage and can cause issues in production applications. We'll take a look at AWS Lambda timeout limits, timeout errors, monitoring timeout errors, and how to apply best practices to handle them effectively"},{"timestamp":"1695661380.0","upvote_count":"5","poster":"MssP","content":"Selected Answer: A\nruns several 1-hour tasks -> No way for Lambda. A is the option.","comment_id":"850387"},{"timestamp":"1695577860.0","upvote_count":"1","poster":"klayytech","comment_id":"849586","content":"Selected Answer: C\nTo meet the requirements with the least operational overhead, you can copy the tasks into AWS Lambda functions and schedule the Lambda functions by using Amazon EventBridge (Amazon CloudWatch Events). This solution is cost-effective and requires minimal operational overhead."},{"content":"C, AWS Lambda natively provides a Runtime API which allows you to use any additional programming languages to author your functions..","poster":"Santosh43","timestamp":"1695472080.0","comment_id":"848306","upvote_count":"1"},{"upvote_count":"3","content":"Selected Answer: D\nI'm leaning towards D. Having an ASG with several EC2 instances will scale and improve performance","comment_id":"844437","poster":"WherecanIstart","timestamp":"1695169680.0"},{"poster":"CapJackSparrow","comment_id":"841272","timestamp":"1694882880.0","content":"um, D looks pretty solid... I can't anything on Batch jobs that talk about acceptable programming languages.","upvote_count":"1"},{"content":"Selected Answer: A\nhttps://aws.plainenglish.io/aws-lambda-or-aws-batch-making-the-right-choice-for-your-workload-8d38162350af","timestamp":"1694683200.0","comment_id":"838777","upvote_count":"4","poster":"techhb"},{"content":"https://www.examtopics.com/discussions/amazon/view/84704-exam-aws-certified-solutions-architect-associate-saa-c02/","timestamp":"1694656620.0","poster":"aragon_saa","comment_id":"838538","upvote_count":"2"},{"comments":[{"upvote_count":"14","comment_id":"837494","poster":"fkie4","content":"A my S. show some reasons next time","timestamp":"1694558940.0"}],"content":"Selected Answer: A\naaaaaaaa","timestamp":"1694344980.0","poster":"[Removed]","upvote_count":"6","comment_id":"835020"}],"timestamp":"2023-03-10 14:23:00","answer_description":"","answers_community":["A (72%)","C (16%)","9%"]}],"exam":{"lastUpdated":"11 Apr 2025","provider":"Amazon","id":31,"name":"AWS Certified Solutions Architect - Associate SAA-C03","isImplemented":true,"numberOfQuestions":1019,"isBeta":false,"isMCOnly":true},"currentPage":64},"__N_SSP":true}