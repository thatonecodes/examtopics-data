{"pageProps":{"questions":[{"id":"vKL0jlacSaU2dBW0AO2m","answer_images":[],"choices":{"B":"Sam local generate-event","D":"Sam local start-api","C":"Sam local start-lambda","A":"Sam local invoke"},"answer_description":"","timestamp":"2023-10-06 03:00:00","exam_id":24,"question_id":111,"answer_ET":"D","answer":"D","question_images":[],"discussion":[{"upvote_count":"9","comment_id":"1026632","poster":"Digo30sp","timestamp":"1712407800.0","content":"Selected Answer: D\nThe correct answer is (D).\n\nThe AWS SAM CLI sam local start-api subcommand is used to start a local API Gateway instance. This allows you to test your REST API locally before deploying it to the production environment.\n\nThe other subcommands will not meet the developer's requirements:\n\nLocal invocation of Sam is used to invoke a Lambda function locally.\nSam's local event generation is used to generate a local event file to be used to invoke a Lambda function locally.\nSam local start-lambda is used to start a local instance of a Lambda function."},{"poster":"65703c1","timestamp":"1732381860.0","upvote_count":"1","content":"Selected Answer: D\nD is the correct answer.","comment_id":"1216676"},{"upvote_count":"1","timestamp":"1725334560.0","poster":"KarBiswa","comment_id":"1164514","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-cli-command-reference-sam-local-start-api.html"},{"comment_id":"1040462","timestamp":"1712829000.0","poster":"dilleman","upvote_count":"3","content":"Selected Answer: D\nD is correct"},{"timestamp":"1712365200.0","poster":"fordiscussionstwo","upvote_count":"4","comment_id":"1026109","content":"DDDDDDDDDDD"}],"isMC":true,"answers_community":["D (100%)"],"question_text":"A developer is creating a new REST API by using Amazon API Gateway and AWS Lambda. The development team tests the API and validates responses for the known use cases before deploying the API to the production environment.\n\nThe developer wants to make the REST API available for testing by using API Gateway locally.\n\nWhich AWS Serverless Application Model Command Line Interface (AWS SAM CLI) subcommand will meet these requirements?","url":"https://www.examtopics.com/discussions/amazon/view/122621-exam-aws-certified-developer-associate-dva-c02-topic-1/","topic":"1","unix_timestamp":1696554000},{"id":"FoGiNhz1Gidhb6EZyrEA","answer":"D","answer_description":"","answer_ET":"D","topic":"1","choices":{"B":"Use the resource policies of the SQS queue in the main account to give each account permissions to write to that SQS queue. Add to the Amazon EventBridge event bus of each account an EventBridge rule that matches all EC2 instance lifecycle events. Add the SQS queue in the main account as a target of the rule.","D":"Configure the permissions on the main account event bus to receive events from all accounts. Create an Amazon EventBridge rule in each account to send all the EC2 instance lifecycle events to the main account event bus. Add an EventBridge rule to the main account event bus that matches all EC2 instance lifecycle events. Set the SQS queue as a target for the rule.","C":"Write an AWS Lambda function that scans through all EC2 instances in the company accounts to detect EC2 instance lifecycle changes. Configure the Lambda function to write a notification message to the SQS queue in the main account if the function detects an EC2 instance lifecycle change. Add an Amazon EventBridge scheduled rule that invokes the Lambda function every minute.","A":"Configure Amazon EC2 to deliver the EC2 instance lifecycle events from all accounts to the Amazon EventBridge event bus of the main account. Add an EventBridge rule to the event bus of the main account that matches all EC2 instance lifecycle events. Add the SQS queue as a target of the rule."},"question_text":"A company is running Amazon EC2 instances in multiple AWS accounts. A developer needs to implement an application that collects all the lifecycle events of the EC2 instances. The application needs to store the lifecycle events in a single Amazon Simple Queue Service (Amazon SQS) queue in the company's main AWS account for further processing.\nWhich solution will meet these requirements?","question_id":112,"answers_community":["D (80%)","B (15%)","2%"],"url":"https://www.examtopics.com/discussions/amazon/view/102782-exam-aws-certified-developer-associate-dva-c02-topic-1/","exam_id":24,"isMC":true,"unix_timestamp":1678957020,"question_images":[],"discussion":[{"comment_id":"845391","comments":[{"content":"thanks a lot","comment_id":"971600","poster":"jipark","timestamp":"1691112960.0","upvote_count":"1"}],"poster":"Untamables","upvote_count":"20","timestamp":"1679359440.0","content":"Selected Answer: D\nThe correct answer is D.\nAmazon EC2 instances can send the state-change notification events to Amazon EventBridge.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-instance-state-changes.html\nAmazon EventBridge can send and receive events between event buses in AWS accounts.\nhttps://docs.aws.amazon.com/eventbridge/latest/userguide/eb-cross-account.html"},{"comment_id":"890859","poster":"geekdamsel","upvote_count":"11","content":"This question came in exam. Correct answer is D.","timestamp":"1683389940.0"},{"timestamp":"1738671240.0","content":"Selected Answer: C\nEither C or D. Down side of C: Needs to trigger everyone min but very flexible. D: is completely automated but giving access to other aws account is not safe.","comment_id":"1351332","poster":"7948dca","upvote_count":"1"},{"poster":"sumanshu","comment_id":"1329447","timestamp":"1734700080.0","comments":[{"timestamp":"1734700260.0","content":"A) Eliminated - Amazon EC2 itself does not send lifecycle events directly to EventBridge in another account. The EC2 lifecycle events would be published to EventBridge within the account where the EC2 instance resides.\n\nThe key here is using EventBridge cross-account event bus access. Amazon EventBridge allows for event buses to receive events from other AWS accounts if the appropriate permissions are set.","poster":"sumanshu","comment_id":"1329449","upvote_count":"1","comments":[{"upvote_count":"1","comment_id":"1329468","poster":"sumanshu","comments":[{"upvote_count":"1","poster":"sumanshu","timestamp":"1734701580.0","comment_id":"1329469","content":"C) Eliminated - more maintenance overhead of LAMBDA"}],"content":"B) Will work as Option B is implying a direct route from each AWS account’s EventBridge to the main account’s SQS queue, bypassing the main account's EventBridge event bus - Eliminated - as No centralized events, which is in Option D","timestamp":"1734701520.0"}]}],"content":"Selected Answer: D\nhttps://docs.aws.amazon.com/us_en/eventbridge/latest/userguide/eb-cross-account.html#:~:text=You%20can%20configure%20EventBridge%20to%20send%20and%20receive,events%20from%20the%20event%20bus%20in%20your%20account.","upvote_count":"1"},{"poster":"trieudo","timestamp":"1733913660.0","content":"Selected Answer: D\n==> Discard C: lamdba scans ==> it will be delay by scan all data\n==> Discard A: 'matches all EC2 instance' ==> hard to maintain, when updating many times can occurs\n==> Discard B: it works, but it maybe have some security problem when pushing raw data (not clean) into SQS. It also doesn't take advantage of \n\nD: By configuring the main account's event bus to accept events from other accounts and adding rules in those accounts to forward lifecycle events, this solution achieves secure and efficient centralization. EventBridge then routes the events to an SQS queue in the main account for further processing.","upvote_count":"2","comment_id":"1324981"},{"content":"Selected Answer: D\nhttps://docs.aws.amazon.com/eventbridge/latest/userguide/eb-cross-account.html","comment_id":"1248763","upvote_count":"1","poster":"Anandesh","timestamp":"1721118120.0"},{"content":"Selected Answer: D\nD is correct answer.","upvote_count":"1","timestamp":"1717257360.0","poster":"NagaoShingo","comment_id":"1222763"},{"upvote_count":"1","timestamp":"1716295380.0","poster":"65703c1","comment_id":"1214940","content":"D is the correct answer."},{"poster":"xdkonorek2","content":"Selected Answer: D\nTried to implement both B and D\nIt's tricky, because B could be possible but you can't select cross-account SQS as target to the rule, option D is 100% correct","timestamp":"1702747140.0","upvote_count":"1","comment_id":"1098347"},{"timestamp":"1699075140.0","poster":"dongocanh272","content":"Selected Answer: D\nMy answer is D","upvote_count":"2","comment_id":"1061908"},{"comment_id":"1027586","timestamp":"1696703640.0","content":"Selected Answer: D\nAnswer C is correct","upvote_count":"1","poster":"Digo30sp"},{"upvote_count":"5","comment_id":"964057","timestamp":"1690393440.0","poster":"TeeTheMan","content":"Selected Answer: B\nSeems to me the correct answer is B. The current most voted answer is B, but can someone explain why it’s better than B? I think B is better because it has fewer steps. The events go straight from each account into the queue. Unlike in D which has the intermediate step of the event bus of the main account. Also, why would you want to pollute the event bus of the main account with events from other accounts when it isn’t necessary?"},{"upvote_count":"2","timestamp":"1689356400.0","comments":[{"content":"Sorry Im wrong, AWS allow to send and receive Amazon EventBridge events between AWS accounts. https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-cross-account.html\n\nBoth B and D works, but D is more centralized","upvote_count":"4","timestamp":"1689357900.0","poster":"KillThemWithKindness","comment_id":"951732"}],"poster":"KillThemWithKindness","comment_id":"951712","content":"B\nAnswer A is incorrect because Amazon EventBridge events can't be sent directly from one account's event bus to another.\n\nAnswer C is incorrect because it's unnecessary and inefficient to use Lambda to periodically scan all EC2 instances for lifecycle changes. Amazon EventBridge can capture these events automatically as they occur.\n\nAnswer D is incorrect because it is not possible to configure the main account event bus to receive events from all accounts directly, and Amazon EventBridge events can't be sent directly from one account's event bus to another. The EventBridge rules need to be set up in the accounts where the events are generated."},{"timestamp":"1685441100.0","comment_id":"910136","content":"Selected Answer: D\nThe correct answer is D.\nhttps://docs.aws.amazon.com/eventbridge/latest/userguide/eb-cross-account.html","poster":"ezredame","upvote_count":"2"},{"comment_id":"890673","content":"Selected Answer: A\nOption D is not the best solution because it involves configuring the permissions on the main account's EventBridge event bus to receive events from all accounts, which can lead to potential security risks. Allowing other AWS accounts to send events to the main account's EventBridge event bus can potentially open up a security vulnerability, as it increases the attack surface area for the main account.\n\nOn the other hand, option A is the best solution because it involves using Amazon EventBridge, which is a serverless event bus that can be used to route events between AWS services or AWS accounts. By configuring Amazon EC2 to deliver the EC2 instance lifecycle events from all accounts to the Amazon EventBridge event bus of the main account, and adding the SQS queue as a target of the rule, the application can collect all the lifecycle events of the EC2 instances in a single queue in the main account without compromising the security posture of the AWS environment.","poster":"Bibay","timestamp":"1683368220.0","upvote_count":"1"},{"content":"Selected Answer: B\nB solution meets all da requirements. By using resource policies, you can grant permissions for other accounts to write to the SQS queue in the main account. \nThen, you create EventBridge rules in each account dat match EC2 lifecycle events and use da main account's SQS queue as a target for these rules. It's da best choice for dis scenario.","comment_id":"883792","poster":"ihebchorfi","upvote_count":"1","timestamp":"1682703780.0"},{"poster":"MrTee","upvote_count":"2","timestamp":"1682377920.0","comment_id":"879760","content":"Selected Answer: D\nThis solution allows the collection of all the lifecycle events of the EC2 instances from multiple AWS accounts and stores them in a single Amazon SQS queue in the company’s main AWS account for further processing"},{"content":"For Option C using lambda does not seem to be a good solution as we would have to trigger lambda on some schedule and it will has less granularity in time.\n\nFor D. Why would we be matching EC2 instance lifecycle events in Main account event bus and not in each account event bus and reducing overhead for main account","upvote_count":"1","comment_id":"852935","poster":"shahs10","timestamp":"1679987820.0"},{"comment_id":"840850","poster":"good_","content":"I think the answer to this question is also A.","upvote_count":"4","timestamp":"1678964460.0"},{"timestamp":"1678957860.0","content":"Answer A: This makes more sense and a simplified solution.","upvote_count":"5","comment_id":"840738","poster":"haaris786"},{"poster":"aragon_saa","upvote_count":"4","content":"D\nhttps://www.examtopics.com/discussions/amazon/view/96209-exam-aws-certified-developer-associate-topic-1-question-396/","comment_id":"840715","timestamp":"1678957020.0"}],"timestamp":"2023-03-16 09:57:00","answer_images":[]},{"id":"RPK3yMK7q5wljxjCPHJ1","answer_description":"","choices":{"C":"On an Amazon RDS DB instance, create a table that contains columns for title, release year, and genre. Configure the title as the primary key.","B":"Create an Amazon DynamoDB table. Configure the table with a primary key that consists of the genre as the partition key and the release year as the sort key. Create a global secondary index that uses the title as the partition key.","A":"Create an Amazon DynamoDB table. Configure the table with a primary key that consists of the title as the partition key and the release year as the sort key. Create a global secondary index that uses the genre as the partition key and the title as the sort key.","D":"On an Amazon RDS DB instance, create a table where the primary key is the title and all other data is encoded into JSON format as one additional column."},"exam_id":24,"unix_timestamp":1679401800,"discussion":[{"poster":"Bibay","timestamp":"1683378540.0","comment_id":"890753","content":"Selected Answer: A\nA. Create an Amazon DynamoDB table. Configure the table with a primary key that consists of the title as the partition key and the release year as the sort key. Create a global secondary index that uses the genre as the partition key and the title as the sort key.\n\nThis option is the best choice for the given requirements. By using DynamoDB, the developer can store the movie information in a flexible and scalable NoSQL database. The primary key can be set to the title and release year, allowing for efficient retrieval of information about a specific movie. The global secondary index can be created using the genre as the partition key, allowing for efficient retrieval of information about all movies in a specific genre. Additionally, the use of a NoSQL database like DynamoDB allows for the flexible storage of additional properties about the cast and crew, as each movie can have different properties without affecting the structure of the database.","upvote_count":"16"},{"comment_id":"1329905","upvote_count":"2","poster":"sumanshu","content":"Selected Answer: A\nB) Eliminated - No efficient way to query by title + release year since the primary key is genre + release year, and the GSI only has the title\n\nC) Eliminated - Adding new columns for properties like assistant director or animal trainer becomes cumbersome and violates flexibility.\n\nD) Eliminated - JSON in RDS is harder to query","timestamp":"1734774180.0"},{"timestamp":"1734092040.0","poster":"trieudo","upvote_count":"2","content":"Selected Answer: A\nkeyword: (title + release year), title, genre, each film has differential cast and crew\n\n=> Discard C,D: when it store film by structure && option C must scan all data that is not primary key(title), seem be not flexible && option D is horrible way, when storing data as json structure, take time to extract this json\n==> discard B: querry (title + release year) must scan all item without combining them in primary key(partition key and sort key) at primary table or GSI","comment_id":"1326112"},{"content":"Selected Answer: A\nThis question was on my exam July 23rd, 2024. Answer is A","timestamp":"1721754780.0","comment_id":"1253808","poster":"ahadh7621","upvote_count":"3"},{"upvote_count":"1","poster":"sid4510","timestamp":"1719982140.0","content":"A is mostly correct , but I do see one problem there because in one year there can be same title movies can come which invalidate our primary key having title as partition key and year as a. Sort. key","comment_id":"1241161"},{"content":"Selected Answer: A\nA is the correct answer.","poster":"65703c1","comment_id":"1215018","timestamp":"1716301260.0","upvote_count":"1"},{"upvote_count":"2","poster":"leonardoliveros","comment_id":"1072044","content":"Selected Answer: A\nIf you create a primary key with title(pk) and release(sk) date you corvered two scenaries, and also you need a GSI by last scenary with genre so you should creating a GSI with genre (pk) and title (sk)","timestamp":"1700094300.0"},{"timestamp":"1693741140.0","content":"Selected Answer: A\nGo with A.\nNoSQL is good when data attributes are inconsistent -> DynamoDB\nPrimary key should be unique, go with title + release year.","poster":"Tony88","upvote_count":"4","comment_id":"997570"},{"timestamp":"1691613780.0","comment_id":"977016","upvote_count":"4","content":"As the schema for each entry of data into the database is not the same all the time, We would require a NoSQL database. So, RDS DB instance is ruled out. The answer is between A and B.\n\nAs we would need the partition key to be as unique as possible, we would like to have the title of the movie as the partition key. Because having the partition key as the genre will create a hot partition problem and our data stored in the DynamoDB will be skewed. \n\nSo option A is the answer.","poster":"jayvarma"},{"timestamp":"1680683640.0","upvote_count":"2","comment_id":"861890","poster":"Krok","content":"Selected Answer: A\nIt's A - I totally agree. It's a single appropriate solution. But in my opinion genre isn't a quite good option as GSI partition key - it isn't high distribution and we can get a hot partition."},{"comment_id":"853271","poster":"shahs10","timestamp":"1680008880.0","upvote_count":"2","content":"Selected Answer: A\nOption A because we have to search on the basis of title so it is better to partition by title. Also we have to search by genre so it is good option to make GSI using genre as partition key"},{"comment_id":"845936","content":"Selected Answer: A\nThe correct answer is A.\nAmazon DynamoDB is suited for storing inconsistent attributes data across items.\nOption B is wrong. This solution does not help get items with the condition of the combination, title and release year.","poster":"Untamables","timestamp":"1679401800.0","upvote_count":"3"}],"timestamp":"2023-03-21 13:30:00","question_text":"A developer wants to store information about movies. Each movie has a title, release year, and genre. The movie information also can include additional properties about the cast and production crew. This additional information is inconsistent across movies. For example, one movie might have an assistant director, and another movie might have an animal trainer.\nThe developer needs to implement a solution to support the following use cases:\nFor a given title and release year, get all details about the movie that has that title and release year.\nFor a given title, get all details about all movies that have that title.\nFor a given genre, get all details about all movies in that genre.\nWhich data store configuration will meet these requirements?","question_id":113,"answer_ET":"A","question_images":[],"answer_images":[],"answer":"A","topic":"1","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/103468-exam-aws-certified-developer-associate-dva-c02-topic-1/","answers_community":["A (100%)"]},{"id":"E0qizkU2BTYQTB3eMlQb","answer":"A","url":"https://www.examtopics.com/discussions/amazon/view/122622-exam-aws-certified-developer-associate-dva-c02-topic-1/","timestamp":"2023-10-06 03:01:00","isMC":true,"question_text":"A company has a serverless application on AWS that uses a fleet of AWS Lambda functions that have aliases. The company regularly publishes new Lambda function by using an in-house deployment solution. The company wants to improve the release process and to use traffic shifting. A newly published function version should initially make available only to a fixed percentage of production users.\n\nWhich solution will meet these requirements?","discussion":[{"content":"Selected Answer: A\nThe correct answer is (A).\n\nWeighted aliases allow you to route traffic to different versions of a function based on weights that you assign. This allows you to implement a canary deployment, where you initially route a small percentage of your traffic to the new version of the function, and then gradually increase the percentage as you gain confidence in the new version.","timestamp":"1696596780.0","comments":[{"upvote_count":"3","content":"If we need Canary deployment, then why not B ?\nHow you will use A in automated deployment?","comments":[{"timestamp":"1727499960.0","comment_id":"1290509","poster":"albert_kuo","content":"While canary deployments are a valid strategy for gradual rollouts, Lambda doesn't have a built-in \"canary deployment type.\" This option is misleading and not applicable in the context of Lambda functions.","upvote_count":"1"},{"comment_id":"1254032","timestamp":"1721783100.0","upvote_count":"2","content":"the key word is \"fixed percentage\"","poster":"albert_kuo"}],"comment_id":"1152513","poster":"rimaSamir","timestamp":"1708169400.0"}],"upvote_count":"8","comment_id":"1026633","poster":"Digo30sp"},{"poster":"65703c1","timestamp":"1716477540.0","upvote_count":"1","content":"Selected Answer: A\nA is the correct answer.","comment_id":"1216681"},{"content":"Selected Answer: B\nI am struggling to see how the correct answer isn't canary. please feel free to enlighten me as I am at a loss how this question description is anything but canary","poster":"DeaconStJohn","comments":[{"timestamp":"1710927120.0","upvote_count":"1","poster":"DeaconStJohn","content":"is this question a case of what naming convention is used within Lambda service. i.e. Canary deployments via Weighted Aliases.","comments":[{"upvote_count":"1","content":"Still very on the fence with this one. \nMy key take aways are that the question says an in house deployment solution and not \"codedeploy.\"\nBy using weighted aliases we are in fact performing a canary deployments.\n\nBitch of a question.","poster":"DeaconStJohn","comment_id":"1178110","comments":[{"poster":"ahadh7621","upvote_count":"2","content":"My thought process is this. They can't use Canary Deployment because that is specifically for AWS CodeDeploy: https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html\n\nThey are using an in-house deployment method, so AWS canary deployments aren't applicable. They can, however, use routing configuration on an alias to send a portion of traffic to a second function version. For example, you can reduce the risk of deploying a new version by configuring the alias to send most of the traffic to the existing version, and only a small percentage of traffic to the new version.\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html#configuring-alias-routing","comment_id":"1250053","timestamp":"1721266680.0"}],"timestamp":"1710927480.0"}],"comment_id":"1178094"}],"comment_id":"1178086","upvote_count":"1","timestamp":"1710926820.0"},{"comment_id":"1164078","poster":"KarBiswa","upvote_count":"2","timestamp":"1709378520.0","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html#configuring-alias-routing:~:text=function%20version.%20For%20example%2C%20you%20can%20reduce%20the%20risk%20of%20deploying%20a%20new%20version%20by%20configuring%20the%20alias%20to%20send%20most%20of%20the%20traffic%20to%20the%20existing%20version%2C%20and%20only%20a%20small%20percentage%20of%20traffic%20to%20the%20new%20version."},{"poster":"NijeshT","timestamp":"1701443460.0","content":"Answer is A.\nweighted aliases offer fixed, predefined percentages","upvote_count":"4","comment_id":"1085304"},{"upvote_count":"4","comment_id":"1026112","timestamp":"1696554060.0","poster":"fordiscussionstwo","content":"AAAAAAAAAAA"}],"answers_community":["A (92%)","8%"],"question_images":[],"topic":"1","answer_description":"","choices":{"D":"Configure a linear deployment type for Lambda.","C":"Configure routing on the new versions by using environment variables.","A":"Configure routing on the alias of the new function by using a weighted alias.","B":"Configure a canary deployment type for Lambda."},"unix_timestamp":1696554060,"exam_id":24,"answer_images":[],"question_id":114,"answer_ET":"A"},{"id":"XprBaQexAFDzAIrnK8jf","choices":{"D":"Use Amazon ElastiCache for Redis to offload read requests from the main database.","B":"Replicate the data to Amazon DynamoDSet up a DynamoDB Accelerator (DAX) cluster.","A":"Use Amazon ElastiCache for Memcached to offload read requests from the main database.","C":"Configure the Amazon RDS instances to use Multi-AZ deployment with one standby instance. Offload read requests from the main database to the standby instance."},"timestamp":"2023-10-06 03:03:00","question_images":[],"isMC":true,"answers_community":["A (100%)"],"answer":"A","question_id":115,"answer_description":"","question_text":"A company has an application that stores data in Amazon RDS instances. The application periodically experiences surges of high traffic that cause performance problems. During periods of peak traffic, a developer notices a reduction in query speed in all database queries.\n\nThe team’s technical lead determines that a multi-threaded and scalable caching solution should be used to offload the heavy read traffic. The solution needs to improve performance.\n\nWhich solution will meet these requirements with the LEAST complexity?","answer_images":[],"exam_id":24,"url":"https://www.examtopics.com/discussions/amazon/view/122623-exam-aws-certified-developer-associate-dva-c02-topic-1/","discussion":[{"content":"Selected Answer: A\nWhen deciding between Memcached and Redis, here are a few questions to consider:\n\nIs object caching your primary goal, for example to offload your database? If so, use Memcached.\n\nhttps://docs.aws.amazon.com/whitepapers/latest/scale-performance-elasticache/memcached-vs.-redis.html","timestamp":"1696968360.0","poster":"kashtelyan","upvote_count":"8","comment_id":"1039844"},{"timestamp":"1727500980.0","poster":"albert_kuo","comment_id":"1290514","content":"Selected Answer: A\noption A (using ElastiCache for Memcached) provides the best balance of meeting the requirements (multi-threaded, scalable caching to improve performance) while maintaining the least complexity.","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: A\nA is the correct answer.","comment_id":"1216695","timestamp":"1716478620.0","poster":"65703c1"},{"upvote_count":"1","content":"Selected Answer: A\nA. If you're looking for a multi-threaded solution, then ElastiCache for Memcached (not Redis) is the solution.","timestamp":"1707168060.0","comment_id":"1141495","poster":"tsdsmth"},{"comment_id":"1128913","upvote_count":"3","poster":"SerialiDr","content":"Selected Answer: A\nA. Use Amazon ElastiCache for Memcached to offload read requests from the main database.\n\nElastiCache for Memcached is a good fit for this scenario. It's a high-performance, distributed, in-memory caching system that can easily scale to manage surges in read traffic. It's simple to set up and integrate with an existing RDS instance.\nD. Use Amazon ElastiCache for Redis to offload read requests from the main database.\n\nElastiCache for Redis also offers high performance and is capable of handling surges in read traffic. Redis provides more advanced data structures and features compared to Memcached, like persistence, built-in replication, and support for complex data types. However, it might be more complex to set up and manage than Memcached, depending on the use case.","timestamp":"1705946280.0"},{"upvote_count":"3","timestamp":"1696597140.0","comment_id":"1026637","poster":"Digo30sp","content":"Selected Answer: A\nThe correct answer is (A).\n\nAmazon ElastiCache for Memcached is a scalable, multithreaded caching solution that can be used to offload heavy read traffic from Amazon RDS instances. ElastiCache for Memcached is easy to configure and manage, making it a low-effort solution to meet technical lead requirements."},{"poster":"fordiscussionstwo","timestamp":"1696554180.0","content":"AAAAAAAAA","comment_id":"1026116","upvote_count":"3"}],"answer_ET":"A","unix_timestamp":1696554180,"topic":"1"}],"exam":{"id":24,"numberOfQuestions":551,"isMCOnly":true,"isImplemented":true,"lastUpdated":"11 Apr 2025","isBeta":false,"name":"AWS Certified Developer - Associate DVA-C02","provider":"Amazon"},"currentPage":23},"__N_SSP":true}