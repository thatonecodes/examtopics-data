{"pageProps":{"questions":[{"id":"X0M46sTxfm6GimSKVQ72","unix_timestamp":1669650180,"answer_ET":"A","timestamp":"2022-11-28 16:43:00","url":"https://www.examtopics.com/discussions/amazon/view/89136-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_images":[],"question_images":[],"discussion":[{"timestamp":"1670149020.0","upvote_count":"39","content":"Selected Answer: A\nChanging my vote to A. After reviewing a Udemy course of SAA-C03, it seems that A (multi-AZ and Clusters) is sufficient for HA.","comment_id":"734986","comments":[{"content":"what number of class ?","poster":"berks","comment_id":"755449","upvote_count":"5","timestamp":"1671950820.0"},{"content":"Here AWS defines HA, and uses the word cluster - AWS has several methods for achieving HA through both approaches, such as through a scalable, load balanced cluster or assuming an active–standby pair. - https://docs.aws.amazon.com/whitepapers/latest/real-time-communication-on-aws/high-availability-and-scalability-on-aws.html","timestamp":"1698491760.0","poster":"AAAWrekng","upvote_count":"2","comment_id":"1056140"}],"poster":"Gil80"},{"content":"Selected Answer: C\nThe question states that it is a critical app and it has to be HA. A could be the answer, but it's in the same AZ, so if the entire region fails, it doesn't cater for the HA requirement. \n\nHowever, the likelihood of a failure in two different regions at the same time is 0. Therefore, to me it seems that C is the better option to cater for HA requirement.\n\nIn addition, C does state like A that the DB app is installed on an EC2 instance.","comment_id":"730227","comments":[{"upvote_count":"6","comment_id":"1179051","content":"Option C proposes launching two EC2 instances in different AWS Regions and setting up database replication, with failover to a second Region. While this solution does provide geographic redundancy, it may introduce higher latency due to cross-region communication and data replication. Additionally, failover to a different Region typically involves more complex configurations and longer recovery times compared to failover within the same Region.\n\nWhile Option C may offer a level of redundancy, it might not provide the same level of responsiveness and automatic failover capabilities as Option A, which leverages Availability Zones within the same Region. In scenarios where low latency and rapid failover are critical, Option A is often preferred. However, if geographic redundancy is a top priority and the potential trade-offs in latency and failover time are acceptable, Option C could still be a viable solution.","poster":"Burrito69","timestamp":"1711008480.0"},{"comment_id":"737806","upvote_count":"8","timestamp":"1670413860.0","content":"but for C you need communication between the two VPC, which increase the complexity. With a should be enough for HA","poster":"javitech83"},{"upvote_count":"7","timestamp":"1677537840.0","comment_id":"824211","poster":"Steve_4542636","content":"The question doesn't ask which option is the most HA. It asks what meets the requirements."},{"timestamp":"1684029360.0","comment_id":"897192","poster":"aussiehoa","content":"Design for region failure? may as well design for AWS failure and put replica in GCP and Azure :v","upvote_count":"12","comments":[{"poster":"jjcode","comment_id":"1089960","upvote_count":"3","timestamp":"1701926340.0","content":"this is reading comprehension exam not a practical exam."},{"poster":"Kp88","comments":[{"content":"yep lol, even in the other questions, for HA you can use Multi-AZ","timestamp":"1696235880.0","poster":"cyber_bedouin","comment_id":"1022936","upvote_count":"3"}],"timestamp":"1690349640.0","content":"And on-prem in multiple DCs and one in mars too :D","upvote_count":"10","comment_id":"963393"}]}],"upvote_count":"29","timestamp":"1669717260.0","poster":"Gil80"},{"timestamp":"1742573220.0","poster":"SirDNS","comment_id":"1401629","upvote_count":"1","content":"Selected Answer: A\nFailover between Regions is typically slower than within a Region."},{"timestamp":"1740808560.0","content":"Selected Answer: A\nCross-region setups are primarily used for disaster recovery rather than high availability due to higher latency and complexity. T","poster":"Anastesas","comment_id":"1363415","upvote_count":"1"},{"upvote_count":"2","timestamp":"1726642560.0","comment_id":"1285619","content":"Selected Answer: A\nI am voting for A.","poster":"bignatov"},{"timestamp":"1722691620.0","content":"Selected Answer: A\nIt's a due to being in the same region different AZ for latency purposes.","comment_id":"1260311","poster":"1e22522","upvote_count":"2"},{"content":"Selected Answer: A\nA provides HA with 2 EC2 in two AZ with database replication","timestamp":"1720378860.0","upvote_count":"3","comment_id":"1243984","poster":"jatric"},{"poster":"ChymKuBoy","upvote_count":"1","comment_id":"1237201","timestamp":"1719378900.0","content":"C for sure"},{"content":"What does \"Configure the EC2 instances as a cluster\" mean? The only \"EC2 cluster\" that I am aware of is a \"cluster placement group\". If that's the case, then all EC2 instances in that cluster must be in the same AZ. So option A would be invalid then.","timestamp":"1719360240.0","comment_id":"1237134","poster":"Duckydoo","upvote_count":"2"},{"poster":"shil_31","upvote_count":"2","timestamp":"1717626660.0","comment_id":"1225031","content":"Selected Answer: A\nOption B uses an AMI for backup and CloudFormation for automation, but doesn't provide high availability or automatic failover.\nOption C launches instances in different regions, which may not be necessary and may increase costs.\nOption D uses EC2 automatic recovery, which can recover an instance, but doesn't provide high availability or automatic failover."},{"timestamp":"1717388340.0","poster":"k_","content":"Selected Answer: C\nThe term \"disruptive event\" implies it requires DR, HA is not sufficient.","comment_id":"1223418","upvote_count":"3"},{"upvote_count":"2","content":"Selected Answer: A\nExplanation:\n\nOption C provides a solution that ensures high availability by deploying EC2 instances in different AWS Regions. By setting up database replication and failing over the database to a second Region, you ensure automatic failover if a disruptive event occurs in one Region.\n\nOptions A and B focus on high availability within a single AWS Region but don't address automatic failover to a different Region in case of a disruptive event.\n\nOption D uses EC2 automatic recovery, but it doesn't provide a solution for automatic failover to a different Region, which is necessary for ensuring high availability in case of a Region-wide failure.","poster":"Solomon2001","timestamp":"1714881600.0","comment_id":"1206773"},{"timestamp":"1713792300.0","comment_id":"1200208","content":"C - I am a littel be wonder reading same explanation, becouse exist a vary big differance beetwen instance cluster and dababase cluster.","upvote_count":"1","poster":"hardy1234567"},{"content":"Selected Answer: A\nOption A suggests deploying two EC2 instances, each in a different Availability Zone within the same AWS Region. This ensures high availability by distributing the instances across multiple physically isolated locations. By installing the database on both EC2 instances and configuring them as a cluster, you create a highly available database setup where one instance can seamlessly take over if the other instance fails.\n\nAdditionally, setting up database replication between the instances ensures data consistency and redundancy. If one instance fails, the other instance can continue serving requests without interruption.","poster":"Uzbekistan","timestamp":"1711322760.0","upvote_count":"4","comment_id":"1182032"},{"poster":"derekz","timestamp":"1708937880.0","content":"Selected Answer: A\nA is for HA. D is for DR","upvote_count":"2","comment_id":"1159505"},{"upvote_count":"1","poster":"MrPCarrot","comment_id":"1137517","content":"Perfect Answer is A","timestamp":"1706783700.0"},{"upvote_count":"8","timestamp":"1706613480.0","poster":"thewalker","content":"Selected Answer: A\nA. \nHigh Availability - multiple Zones. \nDisaster Recovery - multiple Regions.","comment_id":"1135717"},{"timestamp":"1706238120.0","content":"C \"if a disruptive event occurs\"","comment_id":"1132224","poster":"Mkenya08","upvote_count":"2"},{"content":"Answer is C, because question mentioned about disruptive event occurs. when the whole region failed, it can not cover the scenario for HA","upvote_count":"3","poster":"andyngkh86","timestamp":"1705794780.0","comment_id":"1127551"},{"content":"Selected Answer: A\nA is more correct to support both HA and Failover.\nC is only for Failover, not HA during the traffic.","upvote_count":"2","poster":"vip2","timestamp":"1705227900.0","comment_id":"1122453"},{"poster":"Priyapani","upvote_count":"1","timestamp":"1704952800.0","comment_id":"1119394","content":"Selected Answer: C\nC will be answer\nAs it is a critical application"},{"content":"Selected Answer: C\nOnly A & C are possible solutions. However A is not suitable when the region fails while this is critical application => C is the answer","timestamp":"1704390840.0","comment_id":"1113946","upvote_count":"1","poster":"upliftinghut"},{"content":"Selected Answer: A\nBetween A and C, A meets the high availability requirements. C seems an overkill and also requires manual failover as the option does not mention how the failover switch is setup (like using CloudFormation or some events etc)","comment_id":"1110896","poster":"awsgeek75","upvote_count":"3","timestamp":"1704067800.0"},{"upvote_count":"7","timestamp":"1703660580.0","comment_id":"1106630","content":"Selected Answer: A\nB and D are using a single instance and involve restoring an old AMI in case of failure, that is anything but HA\n\nA is correct, different AZs meet AWS' definition of HA, and the \"cluster\" should take care of the automatic failover. \n\nC involves manual failover which does not meet the requirements.","poster":"pentium75"},{"comment_id":"1102094","poster":"Sumith4112","content":"Selected Answer: C\nPractically speaking, A should be the answer. Because, if you do, C, DB replication across two reason is not less expensive. Also, C says, \"fail over the DB to a second region\" . This is means, it is manual. So, I believe A is the right answer.","timestamp":"1703123340.0","comments":[{"upvote_count":"2","content":"Then why did you select C?","comment_id":"1106627","poster":"pentium75","timestamp":"1703660460.0"}],"upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: A\nfor high availability and load distribution, Option A is the most suitable choice by deploying EC2 instances in different AZs within the same AWS Region and configuring them as a cluster with database replication.","comment_id":"1095626","poster":"ale_brd_111","timestamp":"1702483380.0"},{"content":"Selected Answer: A\nC has got more votes as well. However, I think, since C says, \"fail over the database to a second region\". it seems to be manual. So, going with A.","poster":"Sumith4112","timestamp":"1702270800.0","comment_id":"1093091","upvote_count":"3"},{"poster":"yayaayzo","content":"CORRECT ANS IS AAAA\nA. Launch two EC2 instances, each in a different Availability Zone in the same AWS Region. Install the database on both EC2 instances. Configure the EC2 instances as a cluster. Set up database replication.\n\nExplanation:\n\nDifferent Availability Zones: Deploying the EC2 instances in different Availability Zones ensures redundancy and availability in case one Availability Zone experiences issues.\n\nCluster Configuration and Database Replication: Configuring the EC2 instances as a cluster with database replication allows for automatic failover. If one instance becomes unavailable, the other can take over seamlessly.","upvote_count":"2","comment_id":"1092779","timestamp":"1702235640.0"},{"poster":"ansagr","content":"Selected Answer: A\nConfiguring the EC2 instances as a cluster enhances high availability by creating a collaborative and redundant environment for the database. In a clustered setup, both EC2 instances work together, sharing the workload and maintaining synchronized copies of the database. If one instance fails due to a disruptive event, the other instance can seamlessly take over, ensuring continuous availability of the database.","upvote_count":"2","comment_id":"1092271","timestamp":"1702179420.0"},{"timestamp":"1701874620.0","poster":"Masakichen","upvote_count":"2","comment_id":"1089465","content":"The solution architect should choose A. Launch two EC2 instances, each located in a different availability zone within the same AWS region. Install the database on both EC2 instances. Configure the EC2 instances as a cluster. Set up database replication."},{"content":"Selected Answer: C\nsince we're using EC2 and manually setting everything, what do \"create cluster\" mean in answer A?\nyou can set replication between 2 databases set up in 2 servers without any cluster, just open ports and set up replication\nalso spreading ec2 between regions is higher availability","upvote_count":"1","timestamp":"1699734600.0","comment_id":"1068093","poster":"xdkonorek2"},{"content":"Both A and C provide HA and failover. There is a slight difference between multi-az and multi-region failover: multi-region replication is intended for DR and it is not natively supported. A is the answer.","comment_id":"1064311","poster":"liux99","timestamp":"1699310100.0","upvote_count":"3"},{"upvote_count":"2","content":"Selected Answer: A\nhigh avaliable and cluster are key words. So answer is A","comment_id":"1060860","poster":"nnncppp","timestamp":"1698956220.0"},{"poster":"Pankaj_007","timestamp":"1698853500.0","comment_id":"1059869","content":"The database must be highly available and must fail over automatically if a disruptive event occurs. --> In cluster setup failover happens automatically , so for me A is correct","upvote_count":"2"},{"poster":"rlamberti","comment_id":"1051257","content":"Selected Answer: A\nAnswer is A\nCommunication between regions may cause DB cluster disruption because of latency - remember that the question doesn't pointed the DB engine. So, using two AZs will keep HA without the latency potential issue.","timestamp":"1698019740.0","upvote_count":"3"},{"content":"Selected Answer: C\nDR should be in different regions.","comment_id":"1042640","timestamp":"1697198760.0","upvote_count":"1","poster":"tom_cruise"},{"timestamp":"1696577220.0","content":"Option A is correct - and why not Option B ( as Automatic fail over will be miss )","upvote_count":"1","poster":"prabhjot","comment_id":"1026333","comments":[{"poster":"prabhjot","upvote_count":"1","timestamp":"1696577400.0","comment_id":"1026335","content":"nd why not Option C( as Automatic fail over will be miss )"}]},{"timestamp":"1695218160.0","comment_id":"1012382","upvote_count":"3","poster":"hieulam","content":"Selected Answer: C\nif a disruptive event occurs ==> Assuming that occurs in the region, not in the availability zone only, thus, C is correct."},{"comment_id":"1008146","poster":"TariqKipkemei","content":"Selected Answer: A\nTechnically, both option A and C provide HA. But I would go with A because its less complex and less costly on replication costs.","upvote_count":"3","timestamp":"1694755380.0"},{"upvote_count":"1","timestamp":"1694156820.0","comment_id":"1002223","poster":"kanha1996","content":"A is the anser"},{"comment_id":"984726","upvote_count":"4","poster":"Guru4Cloud","content":"Selected Answer: C\nLaunch two EC2 instances, each in a different AWS Region. Install the database on both EC2 instances. Set up database replication. Fail over the database to a second Region.\n\nThe key reasons are:\n\nCross-region redundancy provides the highest level of availability and disaster recovery. If one entire region goes down, the database can fail over across regions.\nDatabase replication ensures data is consistent between regions at all times.\nManual failover gives the flexibility to fail over on-demand in case of regional issues.","timestamp":"1692382860.0"},{"content":"Selected Answer: C\nA \"critical application\" should be protected against a regional outage. This sounds like overkill but is absolutely commonplace and used frequently for truly critical applications.","poster":"n43u435b543ht2b","timestamp":"1691537160.0","comment_id":"976087","upvote_count":"4"},{"content":"Selected Answer: C\nQuestion keyword \"disruptive event\", \"highly available\", \"failover automatically\".\n\n\"Different Region\" is least condition for against \"disruptive event\", not \"different Availability Zone\".\n\nTypo in the question: \"failover automatically\", not \"fail over automatically\".","poster":"james2033","comment_id":"957066","upvote_count":"4","timestamp":"1689821700.0"},{"timestamp":"1689563280.0","content":"Should be C\nCluster share same rack(hardware) in EC2 and its a logical grouping of instances within a single Availability Zone. Hence does not provide HA.","poster":"vini15","comment_id":"953797","upvote_count":"3"},{"timestamp":"1689129120.0","comment_id":"949431","poster":"sosda","upvote_count":"1","content":"Selected Answer: C\nCluster = single AZ = not HA"},{"poster":"Mia2009687","comment_id":"945232","timestamp":"1688701140.0","content":"Selected Answer: C\nFor A- Configure the EC2 instances as a cluster. Set up database replication\nI don't understand why we need EC2 cluster while we require database cluster only.","upvote_count":"2"},{"comment_id":"939150","timestamp":"1688135880.0","content":"Selected Answer: C\nOption C addresses this requirement by launching two EC2 instances in different AWS Regions, installing the database on both instances, setting up database replication, and enabling failover to the second region. In this configuration, if one region becomes unavailable, the application can seamlessly fail over to the database instance in the second region, ensuring high availability and continuity of operations.","poster":"sbnpj","upvote_count":"1"},{"poster":"Ezekiel2517","comments":[{"content":"Agree, that was my reason to discard A. You can't cluster EC2 when they are separate AZ's. I haven't worked on a server cluster for a long time and when I last did, the nodes were in same Data centre (AZ) long before AWS was a thing. Perhaps I'm missing some AWS feature, so please prove us wrong","timestamp":"1696838040.0","upvote_count":"1","comment_id":"1028604","poster":"Jeffab"}],"content":"For those who say A is correct, explain how to set up an EC2 cluster in a multi-AZ environment.\n\nSpoiler alert: it is not technically doable. Ergo, the only viable remaining option is C, as full of flaws as it may be... I choose the lesser evil.","comment_id":"937795","upvote_count":"2","timestamp":"1688026320.0"},{"poster":"cookieMr","upvote_count":"4","timestamp":"1687784280.0","content":"Selected Answer: A\nBy launching two EC2 instances in different Availability Zones and configuring them as a cluster with database replication, the database can achieve high availability and automatic failover. If one instance or Availability Zone becomes unavailable, the other instance can continue serving the application without interruption.\n\nB. Launching a single EC2 instance and using an AMI for backup and provisioning automation does not provide automatic failover or high availability.\n\nC. Launching EC2 instances in different AWS Regions and setting up database replication is a multi-Region setup, which can provide disaster recovery capabilities but does not provide automatic failover within a single Region.\n\nD. Using EC2 automatic recovery can recover the instance if it fails due to hardware issues, but it does not provide automatic failover or high availability across multiple instances or Availability Zones.","comment_id":"934469"},{"content":"Selected Answer: C\nCluster EC2s cannot span between AZs, which invalidates option A.","comments":[{"comment_id":"902466","poster":"AbdulMalik_Y","timestamp":"1684573320.0","upvote_count":"1","content":"that's what i thought !!!"}],"comment_id":"901752","upvote_count":"10","poster":"antropaws","timestamp":"1684484040.0"},{"timestamp":"1683430140.0","comments":[{"poster":"markw92","content":"You can't cluster EC2 when they are separate AZ's. This invalidate answer A. You have to look carefully and read each word carefully.","upvote_count":"4","comment_id":"926272","timestamp":"1687034880.0"}],"upvote_count":"2","poster":"cheese929","comment_id":"891131","content":"Selected Answer: A\nA is correct. Meets the requirements"},{"upvote_count":"2","content":"answer is C.. since multi region infrastructure provides more HA than multi AZ.","timestamp":"1682741820.0","comment_id":"884030","poster":"Dinya_jui"},{"comment_id":"870610","upvote_count":"1","timestamp":"1681533660.0","content":"Selected Answer: C\nbetter choice for HA : different region is better than AZ .","poster":"shinejh0528"},{"timestamp":"1681119480.0","upvote_count":"5","content":"Selected Answer: A\nTo be \"highly available\" it's sufficient to configure a multi-AZ (Availability Zone) instance.\nNOT multi-region.","comment_id":"866154","poster":"jdr75"},{"poster":"maver144","content":"How could you setup cluster for EC2 in different regions as it requires instances to be placed in the same AZs.","timestamp":"1680504720.0","comment_id":"859607","upvote_count":"2"},{"content":"Selected Answer: A\nIt has to be A ,as this is asking for HA nor DR .If it had been DR ,we can think of entire region failure which will make us to think bout having another instance in another region .","timestamp":"1680170940.0","poster":"ashu131","upvote_count":"5","comment_id":"855591"},{"poster":"k33","upvote_count":"1","comment_id":"850682","timestamp":"1679799000.0","content":"Selected Answer: A\nAnswer : A"},{"comment_id":"846663","upvote_count":"5","content":"Selected Answer: A\nFor the once wondering between A and C.\n\"..Configure the EC2 instances as a cluster\" > this give you the automatic failover to the second DB. C point to manual failover making the answer incorrect.","poster":"bgsanata","timestamp":"1679458620.0"},{"timestamp":"1678765620.0","poster":"samu010203","upvote_count":"2","content":"Selected Answer: A\nlooks like A","comment_id":"838532"},{"comment_id":"824207","poster":"Steve_4542636","timestamp":"1677537600.0","upvote_count":"2","content":"Selected Answer: A\nWhere should the database be stored? It should be stored on an EBS which doesn't support multi-region failover."},{"upvote_count":"6","poster":"Lonojack","comment_id":"819726","content":"Selected Answer: A\nHigh availability = Availability Zone\nDisaster Recovery = Multi-Region\n“DISRUPTIVE” DOES NOT suggest DISASTER!","timestamp":"1677184920.0"},{"content":"Selected Answer: A\nVoted for A after some consulatio with more experienced AWS architect... Clue over here is that region failover must be done automatically","poster":"Michal_L_95","comment_id":"817173","upvote_count":"2","timestamp":"1677013860.0"},{"comments":[{"timestamp":"1696838760.0","upvote_count":"2","comment_id":"1028614","poster":"Jeffab","content":"I'm confused, why are you referencing an article on Elastic Container Service (ECS) Cluster? There is no mention of ECS in question, only EC2 running a database."}],"poster":"bdp123","upvote_count":"2","content":"Selected Answer: A\nECS Spread placement strategy\nECS groups available capacity used to place Tasks into ECS Clusters with ECS Tasks being launched into an ECS Cluster. An ECS Clusters configured to use EC2 will have EC2 Instances registered with it and each EC2 instance resides in a single Availability Zone. You should be ensuring that you have EC2 instances registered with your Cluster from multiple Availability Zones. \nhttps://aws.amazon.com/blogs/containers/amazon-ecs-availability-best-practices/#:~:text=An%20ECS%20Clusters%20configured%20to,Cluster%20from%20multiple%20Availability%20Zones.","comment_id":"811084","timestamp":"1676580000.0"},{"comment_id":"810231","poster":"KZM","timestamp":"1676519820.0","content":"It is \"A\".\nMulti-AZ in the same region is enough with the requirements for HA and failover.\nIt is not \"C\". The cross regions may have higher latency.","upvote_count":"2"},{"comment_id":"809822","upvote_count":"1","poster":"Ouk","content":"Selected Answer: C\nFailover so multiple region C","timestamp":"1676483880.0"},{"poster":"Yelizaveta","content":"Selected Answer: A\nHigh availability means: multi-AZ.\nDR (Disaster Recovery) means, it could it would be multi-Regions as it talks about disruptive events.\nBut because the keyword is \"High Availability\" and you have a multi-region for the database this will not be highly available as there will be additional latency issues and data consistency issues as databases are in the different regions.","upvote_count":"3","timestamp":"1676278440.0","comment_id":"807210"},{"content":"Selected Answer: A\nAnswer is A","upvote_count":"2","comment_id":"797008","poster":"aakashkumar1999","timestamp":"1675426020.0"},{"timestamp":"1674986460.0","poster":"mhmt4438","comment_id":"791534","upvote_count":"1","content":"Selected Answer: A\nDefinitely A"},{"timestamp":"1674355500.0","upvote_count":"2","comment_id":"783898","content":"Option C does not fully meet the requirement of automatic failover in case of a disruptive event. While it does have the database replicated in two regions, there is no mention of automatic failover in the event of a disruption. Additionally, it would also have additional latency and data consistency issues as the databases are in different regions. Option A and D are better solutions as they have automatic failover mechanisms in place in case of disruptive events.","poster":"bullrem"},{"comments":[{"upvote_count":"1","comment_id":"784960","poster":"jainparag1","timestamp":"1674446880.0","content":"That's good for HA but not for DR which is the ask here. Correct answer is C."}],"content":"Selected Answer: A\nThe correct answer is A. \n(Configure the EC2 instances as a cluster) Cluster consist of one or more DB instances and a cluster volume that manages the data for those DB instances. Cluster Volume is a VIRTUAL DATABASE storage volume that spans multiple Availability Zones, with each Availability Zone having a copy of the DB cluster data. https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.html","timestamp":"1673886840.0","comment_id":"777933","poster":"imisioluwa","upvote_count":"4"},{"comment_id":"772182","poster":"DavidNamy","content":"Selected Answer: A\nA. Launch two EC2 instances, each in a different Availability Zone in the same AWS Region. Install the database on both EC2 instances. Configure the EC2 instances as a cluster. Set up database replication.","upvote_count":"2","timestamp":"1673422380.0"},{"timestamp":"1673286960.0","poster":"Mahadeva","upvote_count":"1","content":"Selected Answer: C\nAny disruptive event (in Question) means there is a damage that has to be reversed. Malicious attacks, inadvertent user or config events, etc. \n\nHigh availability is not disaster recovery: https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/high-availability-is-not-disaster-recovery.html\n\n\nAny DR plan should address both geographical and component (of a workload) recovery. Multi-Region plan is better. Component-wise a cluster (option A) of EC2 is not the best solution. Because, typically a cluster is tightly-coupled, components can be in same-Rack, same-AZ, which is also not the best failure recovery when disruption strikes.","comment_id":"770721"},{"content":"Selected Answer: C\nUsually, when you see \"Which solution will meet these requirements?\", it’s mean only one answer is valid, and all other have incorrect solution.\nI'll vote for C, 'cause in A \"Install the database on both EC2 instances. Configure the EC2 instances as a cluster.\". They don’t say like \"Configure databases as a cluster.\". So it's a cluster placement group, that can’t be created in different AZ's","timestamp":"1672912140.0","upvote_count":"4","poster":"HayLLlHuK","comments":[{"content":"This article explains that you do set up the EC2 instances as a cluster: https://aws.amazon.com/premiumsupport/knowledge-center/ec2-windows-sql-server-always-on-cluster/, so I'd say A. C is manual so does not meet automatic requirment.","poster":"JayBee65","timestamp":"1673180220.0","upvote_count":"1","comment_id":"769370"}],"comment_id":"766484"},{"poster":"theonlyprince","comment_id":"765084","timestamp":"1672781880.0","content":"Selected Answer: C\n\"A cluster placement group is a logical grouping of instances within a single Availability Zone.\"\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\n\n'A' can't be right as per the above link since the EC2 instances have to be in the same AZ for clustering.","upvote_count":"4"},{"content":"A = Answer correct\nNot sure why people are choosing C. \nIn Cluster mode, would failover to the next available instance in the cluster automatically which is part of the requirement for the solution. \nC states \"fail over the database to a second region\" Which I believe is manual","comment_id":"763842","upvote_count":"2","timestamp":"1672676580.0","poster":"Mindvision"},{"content":"Selected Answer: A\nA: to meet \"disruptive fail over.\"","comment_id":"759141","upvote_count":"2","poster":"PassNow1234","timestamp":"1672182120.0"},{"content":"Selected Answer: A\nThe correct answer is A: Launch two EC2 instances, each in a different Availability Zone in the same AWS Region. Install the database on both EC2 instances. Configure the EC2 instances as a cluster. Set up database replication.\n\nTo ensure high availability for the database, the company should launch two EC2 instances in different Availability Zones within the same AWS Region. This will ensure that the database is available even if one Availability Zone becomes unavailable due to a disruptive event.\n\nThe company should then install the database on both EC2 instances and configure them as a cluster. This will allow the database to automatically failover to the other EC2 instance if one instance becomes unavailable.\n\nFinally, the company should set up database replication between the two EC2 instances. This will ensure that the data is constantly being synchronized between the two instances so that the database remains consistent and up-to-date.","upvote_count":"2","timestamp":"1671829860.0","comments":[{"content":"Option B is incorrect because using an AMI to back up the data will not provide high availability for the database.\n\nOption C is incorrect because launching the EC2 instances in different AWS Regions will not provide automatic failover for the database.\n\nOption D is incorrect because using EC2 automatic recovery will not provide high availability for the database. It will only recover the instance if it becomes unavailable due to a hardware or infrastructure issue, not if there is a disruptive event.","upvote_count":"1","poster":"Buruguduystunstugudunstuy","comment_id":"754550","timestamp":"1671829920.0"}],"comment_id":"754548","poster":"Buruguduystunstugudunstuy"},{"upvote_count":"3","comment_id":"752619","content":"Selected Answer: C\nShould be C as cluster placement group is in the same AZ","poster":"alect096","timestamp":"1671645000.0"},{"poster":"Khodjaev","comment_id":"750873","timestamp":"1671540360.0","upvote_count":"2","content":"Looks Answer A is enough!!!\nTip\nWithin each AWS Region, Availability Zones (AZs) represent locations that are distinct from each other to provide isolation in case of outages. We recommend that you distribute the primary instance and reader instances in your DB cluster over multiple Availability Zones to improve the availability of your DB cluster. That way, an issue that affects an entire Availability Zone doesn't cause an outage for your cluster.\n\nYou can set up a Multi-AZ cluster by making a simple choice when you create the cluster. The choice is simple whether you use the AWS Management Console, the AWS CLI, or the Amazon RDS API. You can also make an existing Aurora cluster into a Multi-AZ cluster by adding a new reader instance and specifying a different Availability Zone."},{"content":"The more I read the question I am leaning towards ans A vs C because failover to diff region is NOT automatic. Even though its critical application i think we one should consider automatic failover requirement.","comment_id":"748818","poster":"Kirrr","timestamp":"1671365580.0","upvote_count":"1"},{"comment_id":"748448","timestamp":"1671314640.0","content":"The answer is C. The EC2 instances can not be configured as a cluster if there are in different AZ as A said.","poster":"bammy","upvote_count":"4"},{"poster":"Nefareus","content":"Selected Answer: C\nDefinition of cluster placement groups: low-latency performance in a single AZ. Therefore it can not be A","upvote_count":"6","timestamp":"1671291720.0","comment_id":"748171"},{"upvote_count":"3","poster":"career360guru","timestamp":"1671264000.0","content":"Selected Answer: A\nOption A is better option than Option C.\nOption C may have higher operational overhead & complexity than option A as setting up a cross region DB replication and automatic failover.","comment_id":"747856"},{"comments":[{"timestamp":"1673180340.0","content":"This shows that you can do it: https://aws.amazon.com/premiumsupport/knowledge-center/ec2-windows-sql-server-always-on-cluster/","comment_id":"769373","poster":"JayBee65","upvote_count":"2"}],"poster":"romko","comment_id":"746957","content":"Selected Answer: C\nA is incorrect by definition: it says \"Launch two EC2 instances, each in a different Availability Zone, Configure the EC2 instances as a cluster\"\n\naccording to documentation: \"A cluster placement group is a logical grouping of instances within a single Availability Zone\"\n\nso it's not possible to place two EC2 into same cluster, such as they are in different AZs.\n\nSo answer is C.","timestamp":"1671179280.0","upvote_count":"7"},{"comments":[{"comment_id":"748806","timestamp":"1671364860.0","upvote_count":"2","content":"This is my question also. I don't think failover to diff region can be setup automatically. Manual intervention is needed to failover to another region where as in same region failover between AZ would be automatic. Please advise if I am wrong.","poster":"Kirrr"}],"poster":"nexus2020","comment_id":"745307","content":"Selected Answer: A\nC: Set up database replication. Fail over the database to a second Region.\n\nHow do we meet this: \"must fail over automatically \" in C?","timestamp":"1671037380.0","upvote_count":"2"},{"upvote_count":"2","content":"Selected Answer: C\nI would say C, A could be a solution, but here we are speaking a Disruptive event. Having DB in 2 regions seems better than AZ in same region. Moreover, the question doesn't refer to 'LEAST OPERATIONAL OVERHEAD\" so for me we don't care about complexity here we care about having best HA solution for a critical app.","comment_id":"744118","poster":"invaderfr","timestamp":"1670941140.0"},{"comment_id":"743691","content":"Selected Answer: A\nTwo regions can still fail at the same time. That's when connectivity and communication between both instances come in, and with them being in the same region but different AZs, the failover and backups would happen more efficiently","upvote_count":"2","timestamp":"1670915280.0","poster":"Marge_Simpson"},{"content":"C \nThis solution will ensure high availability and will automatically fail over if a disruptive event occurs. It will also allow the company to replicate the database across multiple AWS Regions, which will provide additional protection against disruptive events.A is not correct because setting up a cluster of EC2 instances in the same Availability Zone will not provide the required level of high availability. The database must be replicated across multiple AWS Regions to ensure high availability","poster":"Shasha1","timestamp":"1670833920.0","upvote_count":"1","comment_id":"742555"},{"upvote_count":"1","comment_id":"742535","poster":"lapaki","content":"Selected Answer: A\nA. Clustering for HA","timestamp":"1670832000.0","comments":[{"comment_id":"744349","upvote_count":"1","content":"if it was Disaster Recovery i will chose C 2 region , but it asking fro fail over so Multi AZ ! Not sure though :(","timestamp":"1670955660.0","poster":"Rn22"},{"content":"A is super wrong. Clustering places instances into a low-latency group in a single AZ. This answer contradicts itself in my opinion, so I am fairly certain the answer is C","timestamp":"1671094200.0","upvote_count":"1","comment_id":"745861","poster":"vadiminski_a"}]},{"comment_id":"741906","poster":"chaficco21","upvote_count":"4","content":"Answer is A. Literally stated here https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-sql-server/ec2-sql-ha.html","timestamp":"1670777400.0"},{"upvote_count":"4","comment_id":"737808","poster":"javitech83","timestamp":"1670413920.0","content":"Selected Answer: A\nIt is A. C introduces much more complexity as connectivity will be needed between the two VPCs"},{"poster":"Rameez1","upvote_count":"2","timestamp":"1670175960.0","content":"Selected Answer: C\nOption C meets the criteria of HA with data replication in 2nd region.","comment_id":"735310"},{"comment_id":"731959","timestamp":"1669840380.0","upvote_count":"5","content":"I think A is best answer - cluster manage switch to remaining DB automatically. With C, you have to switch manually to new DB server in case of failover. For me, A is better answer.","poster":"Studen15"},{"poster":"Ekie","timestamp":"1669732140.0","comment_id":"730506","upvote_count":"5","content":"I am going with A as it mentioned clearly setting up a Cluster which is the way to go to have HA when DB is installed in EC2"},{"timestamp":"1669691460.0","comment_id":"729874","upvote_count":"2","poster":"kvsomu","content":"A is the answer"},{"upvote_count":"3","content":"Selected Answer: A\n\"The company needs to use Amazon EC2 for the application’s database\"\nSo we have to setup replication of database ourself on EC2","comment_id":"729336","comments":[{"comment_id":"731063","upvote_count":"1","content":"I read the question and got confused now. In most cases, multi-az is enough for HA requirement but it is \"a critical application\", so maybe it makes sense to set up replication in another region...","poster":"leonnnn","timestamp":"1669780020.0"},{"timestamp":"1669698120.0","comment_id":"729924","comments":[{"timestamp":"1669736400.0","content":"Multi-AZ can handle HA in most cases.","poster":"leonnnn","upvote_count":"1","comment_id":"730568"}],"upvote_count":"1","poster":"KADSM","content":"As the question mentioned disruptive event, entire region may be down. So thinking of HA with another region."},{"comment_id":"730224","content":"So why not C?\n\" Launch two EC2 instances, each in a different AWS Region. Install the database on both EC2 instances. Set up database replication. Fail over the database to a second Region.\"\n\nThe question states that it has to be HA. If you have a failure in the same region, both AZ fails, however, the likelihood of a failure in two different regions at the same time is 0.\nIn addition, C does state like A that the DB app is installed on an EC2 instance.\n\nTo me, C seems more logical. I could be wrong, but your explanation ignores the HA request.","upvote_count":"1","comments":[{"comment_id":"731052","comments":[{"timestamp":"1669779600.0","content":"> when ha down\ntypo... it should be \"when region down\"","poster":"leonnnn","upvote_count":"1","comment_id":"731058"}],"upvote_count":"1","content":"multi-az handles HA in most cases. ofcuz multi-region can keep ha when ha down, but it cost too much, and it happens in very rare case.","timestamp":"1669779480.0","poster":"leonnnn"}],"poster":"Gil80","timestamp":"1669717080.0"}],"poster":"leonnnn","timestamp":"1669650180.0"}],"answer":"A","answers_community":["A (62%)","C (38%)"],"exam_id":31,"isMC":true,"question_id":126,"choices":{"C":"Launch two EC2 instances, each in a different AWS Region. Install the database on both EC2 instances. Set up database replication. Fail over the database to a second Region.","A":"Launch two EC2 instances, each in a different Availability Zone in the same AWS Region. Install the database on both EC2 instances. Configure the EC2 instances as a cluster. Set up database replication.","B":"Launch an EC2 instance in an Availability Zone. Install the database on the EC2 instance. Use an Amazon Machine Image (AMI) to back up the data. Use AWS CloudFormation to automate provisioning of the EC2 instance if a disruptive event occurs.","D":"Launch an EC2 instance in an Availability Zone. Install the database on the EC2 instance. Use an Amazon Machine Image (AMI) to back up the data. Use EC2 automatic recovery to recover the instance if a disruptive event occurs."},"topic":"1","answer_description":"","question_text":"A company needs to run a critical application on AWS. The company needs to use Amazon EC2 for the application’s database. The database must be highly available and must fail over automatically if a disruptive event occurs.\n\nWhich solution will meet these requirements?"},{"id":"0cu1P7p7V22FeFl1bVkx","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/89138-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_images":[],"answer":"C","answers_community":["C (97%)","2%"],"choices":{"C":"Move the EC2 instances into an Auto Scaling group. Configure the order system to send messages to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the EC2 instances to consume messages from the queue.","A":"Move the EC2 instances into an Auto Scaling group. Create an Amazon EventBridge (Amazon CloudWatch Events) rule to target an Amazon Elastic Container Service (Amazon ECS) task.","B":"Move the EC2 instances into an Auto Scaling group behind an Application Load Balancer (ALB). Update the order system to send messages to the ALB endpoint.","D":"Create an Amazon Simple Notification Service (Amazon SNS) topic. Create an AWS Lambda function, and subscribe the function to the SNS topic. Configure the order system to send messages to the SNS topic. Send a command to the EC2 instances to process the messages by using AWS Systems Manager Run Command."},"topic":"1","discussion":[{"upvote_count":"15","timestamp":"1708287840.0","poster":"Guru4Cloud","content":"Selected Answer: C\nThe key reasons are:\n\nUsing an Auto Scaling group ensures the EC2 instances that process orders are highly available and scalable.\nWith SQS, the orders are decoupled from the instances that process them via asynchronous queuing.\nIf instances fail or go down, the orders remain in the queue until new instances can pick them up. This provides automated resilience.\nAny failed processing can retry by resending messages back to the queue","comment_id":"984727"},{"comment_id":"1110899","content":"Selected Answer: C\nA uses ECS tasks for something which makes no sense.\nB does not solve the reliable processing of orders\nC SQS for sending a message and processing it reliable\nD is like reinventing SQS with SNS and Lambda mumbo jumbo","upvote_count":"5","timestamp":"1719785760.0","poster":"awsgeek75"},{"comment_id":"1356919","timestamp":"1739635260.0","upvote_count":"1","content":"Selected Answer: C\nSQS queues are the most suitable here.","poster":"satyaammm"},{"comment_id":"1089962","content":"How does SNS capture the requests after the application fails? Those messages are ephemeral by nature and will not hold the data like SQS would. In theory one could create a subscription based service using SNS to stream the data to a service that could store the request, but why...","poster":"jjcode","comments":[{"timestamp":"1719464700.0","content":"That's one of the reasons why D is wrong (not to mention the \"Systems Manager Run Command\" nonsense).","comment_id":"1106631","comments":[{"timestamp":"1719785580.0","upvote_count":"2","content":"I stopped reading option D after SNS and Lambda.... it was sounding nonsense. SQS is default reliability delivery system for me.","comment_id":"1110898","poster":"awsgeek75"}],"upvote_count":"3","poster":"pentium75"}],"upvote_count":"2","timestamp":"1717730700.0"},{"timestamp":"1716799200.0","comment_id":"1081413","poster":"pavospam","content":"Selected Answer: C\nit's C... 4 answers wrong I have found","upvote_count":"1"},{"poster":"Ruffyit","content":"C.\nOption D suggests using Amazon SNS and AWS Lambda, which can be part of an event-driven architecture but may not be the best fit for ensuring the automatic processing of orders during system outages. It relies on an additional AWS Systems Manager Run Command step, which adds complexity and may not be as reliable as using SQS for queuing messages.","comment_id":"1067132","upvote_count":"2","timestamp":"1715327280.0"},{"upvote_count":"2","poster":"David_Ang","content":"Selected Answer: C\n\"C\" because they need to store the request and then be process by the system if it fails, SNS does not have that capacity. another mistake from the admin","timestamp":"1714065300.0","comment_id":"1053909"},{"poster":"vijaykamal","content":"Selected Answer: C\nOption D suggests using Amazon SNS and AWS Lambda, which can be part of an event-driven architecture but may not be the best fit for ensuring the automatic processing of orders during system outages. It relies on an additional AWS Systems Manager Run Command step, which adds complexity and may not be as reliable as using SQS for queuing messages.","upvote_count":"2","timestamp":"1711604820.0","comment_id":"1019386"},{"comment_id":"1008148","upvote_count":"2","content":"Selected Answer: C\nMove the EC2 instances into an Auto Scaling group. Configure the order system to send messages to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the EC2 instances to consume messages from the queue.","poster":"TariqKipkemei","timestamp":"1710487620.0"},{"timestamp":"1708264440.0","upvote_count":"2","poster":"Guru4Cloud","content":"Selected Answer: C\nC is the correct answer.\n\nUsing an Auto Scaling group with EC2 instances behind a load balancer provides high availability and scalability.\n\nSending the orders to an SQS queue decouples the ordering system from the processing system. The EC2 instances can poll the queue for new orders and process them even during an outage. Any failed orders will go back to the queue for reprocessing.","comment_id":"984454"},{"comment_id":"934477","content":"Selected Answer: C\nBy moving the EC2 into an ASG and configuring them to consume messages from an SQS, the system can decouple the order processing from the order system itself. This allows the system to handle failures and automatically process orders even if the order system or EC2 experience outages.\n\nA. Using an ASG with an EventBridge rule targeting an ECS task does not provide the necessary decoupling and message queueing for automatic order processing during outages.\n\nB. Moving the EC2 instances into an ASG behind an \nALB does not address the need for message queuing and automatic processing during outages.\n\nD. Using SNS and Lambda can provide notifications and orchestration capabilities, but it does not provide the necessary message queueing and consumption for automatic order processing during outages. Additionally, using Systems Manager Run Command to send commands for order processing adds complexity and does not provide the desired level of automation.","poster":"cookieMr","timestamp":"1703602980.0","upvote_count":"4"},{"timestamp":"1703405400.0","content":"D is so unnecessary .... this confuses people","comments":[{"upvote_count":"4","poster":"cookieMr","comment_id":"934473","content":"Thx Allmightly for voting system! Answers provided by the site (and not by community) are 20% wrong.","timestamp":"1703602920.0"}],"poster":"pisica134","comment_id":"932243","upvote_count":"2"},{"content":"The answer D is so complex and unnecessary. Why moderator is not providing an explanation of answers when there are heavy conflicts. These kind of answers put your knowledge in question which is not good going into the exam.","timestamp":"1702853580.0","comment_id":"926276","poster":"markw92","upvote_count":"1","comments":[{"content":"The \"Correct Answers\" for this exam are obviously determined by picking a random letter between A and D.","comment_id":"1106632","poster":"pentium75","timestamp":"1719464760.0","upvote_count":"2"}]},{"poster":"gx2222","comment_id":"863024","upvote_count":"3","content":"Selected Answer: C\nTo meet the company's requirements of having a resilient solution that can process orders automatically in case of a system outage, the solutions architect needs to implement a fault-tolerant architecture. Based on the given scenario, a potential solution is to move the EC2 instances into an Auto Scaling group and configure the order system to send messages to an Amazon Simple Queue Service (Amazon SQS) queue. The EC2 instances can then consume messages from the queue.","timestamp":"1696601580.0"},{"upvote_count":"2","poster":"k33","timestamp":"1695696720.0","comment_id":"850684","content":"Selected Answer: C\nAnswer : C"},{"timestamp":"1692116280.0","content":"Selected Answer: C\nC. Move the EC2 instances into an Auto Scaling group. Configure the order system to send messages to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the EC2 instances to consume messages from the queue.\n\nTo meet the requirements of the company, a solutions architect should ensure that the system is resilient and can process orders automatically in the event of a system outage. To achieve this, moving the EC2 instances into an Auto Scaling group is a good first step. This will enable the system to automatically add or remove instances based on demand and availability.","comment_id":"809837","comments":[{"poster":"nickolaj","timestamp":"1692116340.0","upvote_count":"3","comment_id":"809839","content":"However, it's also necessary to ensure that orders are not lost if a system outage occurs. To achieve this, the order system can be configured to send messages to an Amazon Simple Queue Service (Amazon SQS) queue. SQS is a highly available and durable messaging service that can help ensure that messages are not lost if the system fails.\n\nFinally, the EC2 instances can be configured to consume messages from the queue, process the orders and then store them in the database on Amazon RDS. This approach ensures that orders are not lost and can be processed automatically if a system outage occurs. Therefore, option C is the correct answer.","comments":[{"content":"Option A is incorrect because it suggests creating an Amazon EventBridge rule to target an Amazon Elastic Container Service (ECS) task. While this may be a valid solution in some cases, it is not necessary in this scenario.\n\nOption B is incorrect because it suggests moving the EC2 instances into an Auto Scaling group behind an Application Load Balancer (ALB) and updating the order system to send messages to the ALB endpoint. While this approach can provide resilience and scalability, it does not address the issue of order processing and the need to ensure that orders are not lost if a system outage occurs.\n\nOption D is incorrect because it suggests using Amazon Simple Notification Service (SNS) to send messages to an AWS Lambda function, which will then send a command to the EC2 instances to process the messages by using AWS Systems Manager Run Command. While this approach may work, it is more complex than necessary and does not take advantage of the durability and availability of SQS.","upvote_count":"3","poster":"nickolaj","comment_id":"809840","timestamp":"1692116340.0"}]}],"poster":"nickolaj","upvote_count":"3"},{"comment_id":"779230","poster":"LuckyAro","content":"Selected Answer: C\nMy question is; can orders be sent directly into an SQS queue ? How about the protocol for management of the messages from the queue ? can EC2 instances be programmed to process them like Lambda ?","upvote_count":"1","timestamp":"1689612720.0"},{"timestamp":"1687668900.0","content":"Selected Answer: D\nI choose D","comment_id":"755451","poster":"berks","comments":[{"upvote_count":"2","poster":"pentium75","content":"and manually send commands through Systems Manager whenever a new order appears?","comment_id":"1106634","timestamp":"1719464820.0"}],"upvote_count":"1"},{"timestamp":"1687546260.0","content":"Selected Answer: C\nTo meet the requirements of the company, a solution should be implemented that can automatically process orders if a system outage occurs. Option C meets these requirements by using an Auto Scaling group and Amazon Simple Queue Service (SQS) to ensure that orders can be processed even if a system outage occurs.\n\nIn this solution, the EC2 instances are placed in an Auto Scaling group, which ensures that the number of instances can be automatically scaled up or down based on demand. The ordering system is configured to send messages to an SQS queue, which acts as a buffer and stores the messages until they can be processed by the EC2 instances. The EC2 instances are configured to consume messages from the queue and process them. If a system outage occurs, the messages in the queue will remain available and can be processed once the system is restored.","comment_id":"754535","upvote_count":"3","poster":"Buruguduystunstugudunstuy"},{"content":"Selected Answer: A\nc is right","comment_id":"754457","poster":"techhb","upvote_count":"1","timestamp":"1687541040.0"},{"content":"C. Move the EC2 instances into an Auto Scaling group. Configure the order system to send messages to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the EC2 instances to consume messages from the queue.","upvote_count":"2","comment_id":"748294","poster":"NikaCZ","timestamp":"1687020060.0"},{"comment_id":"746960","timestamp":"1686897120.0","upvote_count":"3","content":"Selected Answer: C\nC, decouple applications and functionality, give ability to reprocess message if failed due to networking issue or overloaded other systems","poster":"romko"},{"poster":"Shasha1","comment_id":"742566","content":"C\nConfiguring the EC2 instances to consume messages from the SQS queue will ensure that the instances can process orders automatically, even if a system outage occurs.\ne.","upvote_count":"2","timestamp":"1686552660.0"},{"comment_id":"730985","timestamp":"1685403000.0","content":"SQS order","upvote_count":"1","poster":"TonyghostR05"},{"content":"Selected Answer: C\nC. SQS meets this requirement.","upvote_count":"2","timestamp":"1685348700.0","comment_id":"730233","poster":"Gil80"},{"comment_id":"730067","poster":"learner2023","content":"Selected Answer: C\nC is the right answer","upvote_count":"1","timestamp":"1685339820.0"},{"comment_id":"729875","content":"C is the answer","upvote_count":"1","timestamp":"1685322780.0","poster":"kvsomu"},{"comment_id":"729869","poster":"Nigma","content":"Answer : C","timestamp":"1685321700.0","upvote_count":"1"},{"content":"Selected Answer: C\nAnswer: C due to SQS","comment_id":"729790","poster":"Mee6","upvote_count":"1","timestamp":"1685315040.0"},{"comment_id":"729686","poster":"TMM369","timestamp":"1685307180.0","upvote_count":"2","content":"Selected Answer: C\nC - system to send messages to an Amazon Simple Queue Service (Amazon SQS)"},{"timestamp":"1685284380.0","poster":"Gabs90","upvote_count":"3","content":"Selected Answer: C\nC - https://www.examtopics.com/discussions/amazon/view/81087-exam-aws-certified-solutions-architect-associate-saa-c02/","comment_id":"729427"},{"content":"Selected Answer: C\nSQS is better than SNS to meet this requirement","comment_id":"729342","timestamp":"1685281800.0","poster":"leonnnn","upvote_count":"2"}],"question_id":127,"question_text":"A company’s order system sends requests from clients to Amazon EC2 instances. The EC2 instances process the orders and then store the orders in a database on Amazon RDS. Users report that they must reprocess orders when the system fails. The company wants a resilient solution that can process orders automatically if a system outage occurs.\n\nWhat should a solutions architect do to meet these requirements?","answer_ET":"C","isMC":true,"question_images":[],"timestamp":"2022-11-28 16:50:00","exam_id":31,"unix_timestamp":1669650600},{"id":"4LkeOFHOmEXf5I0NBLy7","exam_id":31,"answer_ET":"D","unix_timestamp":1669650960,"question_text":"A company runs an application on a large fleet of Amazon EC2 instances. The application reads and writes entries into an Amazon DynamoDB table. The size of the DynamoDB table continuously grows, but the application needs only data from the last 30 days. The company needs a solution that minimizes cost and development effort.\n\nWhich solution meets these requirements?","answer_description":"","isMC":true,"timestamp":"2022-11-28 16:56:00","discussion":[{"poster":"Gil80","content":"Selected Answer: D\nchanging my answer to D after researching a bit.\n\nThe DynamoDB TTL feature allows you to define a per-item timestamp to determine when an item is no longer needed. Shortly after the date and time of the specified timestamp, DynamoDB deletes the item from your table without consuming any write throughput.","timestamp":"1669922280.0","upvote_count":"42","comment_id":"733004"},{"poster":"satyaammm","timestamp":"1739635320.0","upvote_count":"1","comment_id":"1356921","content":"Selected Answer: D\nTTL is the most suitable here."},{"content":"Selected Answer: D\nAlways bet on the TTL","upvote_count":"2","timestamp":"1722691860.0","poster":"1e22522","comment_id":"1260315"},{"comment_id":"1243301","timestamp":"1720258980.0","poster":"Nawaff","upvote_count":"1","comments":[{"upvote_count":"3","poster":"Anthony_Rodrigues","content":"It's to minimize development effort.\nC is not the correct one because it needs to enable Dynamo Streams and configure them to send to Lambda, then create the Lambda code for deleting items.\nBesides that, the Lambda is triggered only on new items, and since it's ephemeral, it would have to query items older than 30 days and then delete them.\nWith D, you just need to make two small changes, and it's done—no extra service, no overhead, and no concern about failing to remove.","comment_id":"1271349","timestamp":"1724428740.0"}],"content":"Selected Answer: C\nI would day C\nBecause D requires extending the application to add the timestamp attribute.\nWhich is by itself a development effort."},{"poster":"Hkayne","timestamp":"1714240740.0","upvote_count":"2","comment_id":"1203219","content":"Selected Answer: D\nD is the correct answer"},{"content":"Selected Answer: D\nD is the best answer, dynamostreams is not suitable for this use cases","comment_id":"1191535","timestamp":"1712574240.0","poster":"soufiyane","upvote_count":"2"},{"timestamp":"1712251140.0","upvote_count":"3","poster":"Uzbekistan","comment_id":"1189469","content":"Option D is the most suitable solution to meet the company's requirements while minimizing cost and development effort.\n\nTTL (Time to Live) Attribute: DynamoDB provides a feature called Time to Live (TTL), which allows you to automatically delete items from a table after a specified period. By adding a TTL attribute to each item with a value of the current timestamp plus 30 days, you can let DynamoDB automatically delete items older than 30 days. This eliminates the need for manual deletion efforts or periodic stack redeployment.\n\nMinimal Development Effort\nCost-Effective"},{"content":"Selected Answer: D\nuse ttl","upvote_count":"2","timestamp":"1710037980.0","poster":"scar0909","comment_id":"1170006"},{"comments":[{"upvote_count":"2","comment_id":"1124578","timestamp":"1705447140.0","content":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html","poster":"awsgeek75"}],"content":"Selected Answer: D\nA and B don't solve anything.\nBetween C and D, C requires more cost due to Lambda executions. D uses the TTL built-in feature so it won't cost extra. Also, D does not require extra development and is a matter of configuration. In old-school developer speak, don't write code if your DBA can do some work!","timestamp":"1704068460.0","upvote_count":"3","comment_id":"1110900","poster":"awsgeek75"},{"timestamp":"1695010260.0","content":"Selected Answer: D\nDynamoDB Time to Live was designed to handle this kind of requirement where an item is no longer needed. TTL is provided at no extra cost as a means to reduce stored data volumes by retaining only the items that remain current for your workload’s needs","poster":"TariqKipkemei","comment_id":"1010238","upvote_count":"3"},{"timestamp":"1692383340.0","comment_id":"984729","poster":"Guru4Cloud","upvote_count":"3","content":"Selected Answer: D\nD. Extend the application to add an attribute that has a value of the current timestamp plus 30 days to each new item that is created in the table. Configure DynamoDB to use the attribute as the TTL attribute.\n\nThe main reasons are:\n\nUsing DynamoDB's built-in TTL functionality is the most direct way to handle data expiration.\nIt avoids the complexity of triggers, streams, and lambda functions in option C.\nModifying the application code to add the TTL attribute is relatively simple and minimizes operational overhead"},{"poster":"cookieMr","timestamp":"1687784700.0","comment_id":"934480","upvote_count":"3","content":"Selected Answer: D\nBy adding a TTL attribute to the DynamoDB table and setting it to the current timestamp plus 30 days, DynamoDB will automatically delete the items that are older than 30 days. This solution eliminates the need for manual deletion or additional infrastructure components.\n\nA. Redeploying the CloudFormation stack every 30 days and deleting the original stack introduces unnecessary complexity and operational overhead.\n\nB. Using an EC2 instance with a monitoring application and a script to delete items older than 30 days adds additional infrastructure and maintenance efforts.\n\nC. Configuring DynamoDB Streams to invoke a Lambda function to delete items older than 30 days adds complexity and requires additional development and operational effort compared to using the built-in TTL feature of DynamoDB."},{"poster":"pisica134","upvote_count":"2","timestamp":"1687587060.0","comment_id":"932246","content":"D: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html"},{"poster":"Abrar2022","content":"Amazon DynamoDB Time to Live (TTL) allows you to define a per-item timestamp to determine when an item is no longer needed.","upvote_count":"4","timestamp":"1685268660.0","comment_id":"908498"},{"upvote_count":"2","timestamp":"1684157880.0","comment_id":"898333","content":"Selected Answer: D\nC is incorrect because it can take more than 15 minutes to delete the old data. Lambda won't work","poster":"studynoplay"},{"upvote_count":"2","timestamp":"1683644760.0","comment_id":"893223","content":"Selected Answer: D\nClear case for TTL - every object gets deleted after a certain period of time","poster":"Konb"},{"content":"Selected Answer: D\nUse DynamoDB TTL feature to achieve this..","poster":"rushi0611","comment_id":"890431","timestamp":"1683342600.0","upvote_count":"2"},{"content":"Selected Answer: D\nC is absurd. DynamoDB usually is a RDS with high iops (read/write operations on tables), executing a Lambda function eachtime you insert a item will not be cost-effective.It's much better create such a field the question propose, and manage the delete with a SQL delete sentence:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.DeleteData.html","comment_id":"866165","upvote_count":"2","timestamp":"1681120200.0","poster":"jdr75"},{"content":"Selected Answer: D\nAmazon DynamoDB Time to Live (TTL) allows you to define a per-item timestamp to determine when an item is no longer needed. Shortly after the date and time of the specified timestamp, DynamoDB deletes the item from your table without consuming any write throughput. TTL is provided at no extra cost as a means to reduce stored data volumes by retaining only the items that remain current for your workload’s needs.\n\nTTL is useful if you store items that lose relevance after a specific time.","upvote_count":"2","timestamp":"1673982660.0","comment_id":"779243","poster":"LuckyAro"},{"content":"Selected Answer: D\nD: This solution is more efficient and cost-effective than alternatives that would require additional resources and maintenance.","timestamp":"1673427300.0","poster":"DavidNamy","upvote_count":"2","comment_id":"772255"},{"content":"Selected Answer: D\nD DyanmoDB TTL will expire the items\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html","timestamp":"1672167180.0","upvote_count":"2","poster":"anonymouscloudguy","comment_id":"758940"},{"timestamp":"1671828120.0","comments":[{"comment_id":"754529","upvote_count":"2","timestamp":"1671828180.0","content":"Option A involves using AWS CloudFormation to redeploy the solution every 30 days, but this would require significant development effort and could cause downtime for the application. \n\nOption B involves using an EC2 instance and a monitoring application to delete items that are older than 30 days, but this requires additional infrastructure and maintenance effort. \n\nOption C involves using DynamoDB Streams and a Lambda function to delete items that are older than 30 days, but this requires additional infrastructure and maintenance effort.","poster":"Buruguduystunstugudunstuy"}],"upvote_count":"3","poster":"Buruguduystunstugudunstuy","content":"Selected Answer: D\nTo minimize cost and development effort, a solution that requires minimal changes to the existing application and infrastructure would be the most appropriate. Option D meets these requirements by using DynamoDB's Time-To-Live (TTL) feature, which allows you to specify an attribute on each item in a table that has a timestamp indicating when the item should expire.\n\nIn this solution, the application is extended to add an attribute that has a value of the current timestamp plus 30 days to each new item that is created in the table. DynamoDB is then configured to use this attribute as the TTL attribute, which causes items to be automatically deleted from the table when their TTL value is reached. This solution requires minimal changes to the existing application and infrastructure and does not require any additional resources or a complex setup.","comment_id":"754528"},{"content":"Selected Answer: D\nTTL does the trick","poster":"techhb","upvote_count":"2","timestamp":"1671821100.0","comment_id":"754427"},{"poster":"kvenikoduru","content":"Selected Answer: D\nAmazon DynamoDB Time to Live (TTL) allows you to define a per-item timestamp to determine when an item is no longer needed. Shortly after the date and time of the specified timestamp, DynamoDB deletes the item from your table without consuming any write throughput. - check this link https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html","upvote_count":"2","timestamp":"1671724740.0","comment_id":"753455"},{"upvote_count":"2","comment_id":"748729","poster":"prethesh","content":"Selected Answer: D\nhttps://aws.amazon.com/about-aws/whats-new/2017/02/amazon-dynamodb-now-supports-automatic-item-expiration-with-time-to-live-ttl/","timestamp":"1671358140.0"},{"poster":"career360guru","content":"Selected Answer: D\nOption D - Right answer","upvote_count":"2","timestamp":"1671264420.0","comment_id":"747860"},{"upvote_count":"2","content":"Selected Answer: D\nDynamoDB has the TTL (Time to Live) functionality that gives you the option to set the duration you want a particular data to persist in the table.\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/ttl-dynamodb/","poster":"Baba_Eni","timestamp":"1671110520.0","comment_id":"746109"},{"upvote_count":"1","comment_id":"742581","content":"C \n Amazon DynamoDB Streams to invoke an AWS Lambda function when a new item is created in the table. The Lambda function can then be configured to delete items in the table that are older than 30 days. This solution minimizes cost and development effort because it uses existing AWS services and does not require any additional infrastructure or code development. \nOption D is not correct for me, it is because, DynamoDB Time-to-Live (TTL) is not the most effective solution for minimizing cost and development effort. While DynamoDB TTL can be used to automatically delete items in a table after a certain amount of time, it requires manual configuration of the TTL attribute for each item in the table. This solution would require additional development effort to add the TTL attribute to the application, and it may not be feasible if the application is already running.","timestamp":"1670835780.0","poster":"Shasha1","comments":[{"poster":"JayBee65","comment_id":"771108","upvote_count":"2","content":"This is inefficient:\nThe function would run every time an item was added, would generate costs each time it ran, and typically would not need to delete an item, since the first execution of the day would delete the items over 30 days old.\nIt would also require development effort to create the lambda function.","timestamp":"1673331060.0"}]},{"content":"Selected Answer: D\n\"AWS Lambda is charging its users by the number of requests for their functions and by the duration, which is the time the code needs to execute.\" As the questions notes \"A LARGE FLEET OF EC2\", could rack up lots of money from using lambda calls to delete from tables. TTL is \"FREE\" to use and it also removes data from the table. so \"D\" would be the best solution.","poster":"bmofo","upvote_count":"2","timestamp":"1670782260.0","comment_id":"741966"},{"comment_id":"741279","poster":"Uhrien","upvote_count":"3","timestamp":"1670711400.0","content":"Selected Answer: D\nThis answer seems to be D.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html"},{"upvote_count":"3","timestamp":"1670414640.0","poster":"javitech83","comment_id":"737822","content":"Selected Answer: D\nD is correct. For C I think developing a lambda has more effort than including an attribute, that would be 2 lines code. And of course cheaper than invoking a lambda for each single entry, which has no sense."},{"content":"Selected Answer: D\n\"Amazon DynamoDB Time to Live (TTL) allows you to define a per-item timestamp to determine when an item is no longer needed. Shortly after the date and time of the specified timestamp, DynamoDB deletes the item from your table without consuming any write throughput. TTL is provided at no extra cost as a means to reduce stored data volumes by retaining only the items that remain current for your workload’s needs.\"","upvote_count":"4","comment_id":"737233","timestamp":"1670362500.0","poster":"emohar01"},{"timestamp":"1670334900.0","content":"Selected Answer: C\nC because even if TTL should be ok, the goal is to reduce cost, so if you reduce DB size you'll reduce the cost.","poster":"invaderfr","comment_id":"736872","comments":[{"timestamp":"1672357800.0","poster":"FNJ1111","content":"the goal is also to \"minimize development effort\" and lambda functions are development effort. So it's D.","comment_id":"761565","upvote_count":"2"}],"upvote_count":"1"},{"timestamp":"1669842780.0","poster":"mj98","comment_id":"731989","content":"Ans is C","upvote_count":"1"},{"upvote_count":"1","poster":"TonyghostR05","content":"For my opinion, C is the answer","comment_id":"730988","timestamp":"1669771920.0"},{"poster":"[Removed]","content":"Selected Answer: C\noriginally thought D but this article changed my mind\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/time-to-live-ttl-streams.html#:~:text=Using%20DynamoDB%20Streams%20and%20Lambda%20to%20archive%20TTL%20deleted%20items","timestamp":"1669749300.0","comment_id":"730769","comments":[{"poster":"JayBee65","upvote_count":"3","comment_id":"771109","content":"Why does it change your mind? C does not necessarily delete items after 30 days - the lambda function will only run if new data is added. The function will run every time new data is added, regardless of whether old data needs to be deleted, since the addition of new data and the deletion of old data are not in any way related. C is a poor choice in my mind. If you think otherwise, please explain why.","timestamp":"1673331360.0"},{"upvote_count":"2","comment_id":"1106636","timestamp":"1703661000.0","poster":"pentium75","content":"So there is \"a large fleet of EC2 instances\", probably they are writing tens of thousands, if not millions, of new records per day. And EVERY TIME they write a single new record - probably millions of times per day - you would invoke a Lambda function to identify and delete items older than 30 days?","comments":[{"upvote_count":"2","poster":"foha2012","content":"LOL! Good one !","timestamp":"1705796940.0","comment_id":"1127557"}]}],"upvote_count":"2"},{"timestamp":"1669727160.0","comment_id":"730412","upvote_count":"2","content":"Selected Answer: D\nD is the answer","poster":"Heyang"},{"upvote_count":"2","comments":[],"timestamp":"1669726920.0","poster":"Gil80","content":"Additional info to my previous vote for C. The question asks to minimize development effort. D says: \" Extend the application to add an attribute that has a value of the current timestamp...\"\nExtending the application increases development effort, so it's another reason why D is not the answer.\n\nLastly, Amazon will always aspire to have the customers use their solution, so they would want you to use DynamoDB instead of developing (extending the application) your own solution.","comment_id":"730407"},{"upvote_count":"1","poster":"Gil80","content":"Selected Answer: C\nI was thinking D at first because you never really want to delete data, but then at the end of the question it says \"The company needs a solution that minimizes cost...\"\n\nSo in this case, I believe cost reduction will happen when we reduce DynamoDB size, therefore C seems correct.","timestamp":"1669717680.0","comment_id":"730235"},{"content":"D is the answer","comments":[{"content":"Why? how does D factor in \"minimizes cost...\" ?","timestamp":"1669726200.0","upvote_count":"1","poster":"Gil80","comment_id":"730389"}],"poster":"kvsomu","upvote_count":"3","comment_id":"729876","timestamp":"1669691640.0"},{"timestamp":"1669690800.0","comment_id":"729870","upvote_count":"2","content":"Answer : D","poster":"Nigma"},{"timestamp":"1669650960.0","poster":"leonnnn","comment_id":"729349","content":"Selected Answer: C\nC is correct.","upvote_count":"1"}],"answers_community":["D (95%)","5%"],"question_id":128,"url":"https://www.examtopics.com/discussions/amazon/view/89140-exam-aws-certified-solutions-architect-associate-saa-c03/","choices":{"D":"Extend the application to add an attribute that has a value of the current timestamp plus 30 days to each new item that is created in the table. Configure DynamoDB to use the attribute as the TTL attribute.","A":"Use an AWS CloudFormation template to deploy the complete solution. Redeploy the CloudFormation stack every 30 days, and delete the original stack.","B":"Use an EC2 instance that runs a monitoring application from AWS Marketplace. Configure the monitoring application to use Amazon DynamoDB Streams to store the timestamp when a new item is created in the table. Use a script that runs on the EC2 instance to delete items that have a timestamp that is older than 30 days.","C":"Configure Amazon DynamoDB Streams to invoke an AWS Lambda function when a new item is created in the table. Configure the Lambda function to delete items in the table that are older than 30 days."},"answer":"D","topic":"1","answer_images":[],"question_images":[]},{"id":"7szji5twI9OOY0U3cpKp","exam_id":31,"discussion":[{"timestamp":"1673427600.0","upvote_count":"17","poster":"DavidNamy","content":"Selected Answer: BE\nB. Rehost the application in AWS Elastic Beanstalk with the .NET platform in a Multi-AZ deployment.\nE. Use AWS Database Migration Service (AWS DMS) to migrate from the Oracle database to Oracle on Amazon RDS in a Multi-AZ deployment.\n\nRehosting the application in Elastic Beanstalk with the .NET platform can minimize development changes. Multi-AZ deployment of Elastic Beanstalk will increase the availability of application, so it meets the requirement of high availability.\n\nUsing AWS Database Migration Service (DMS) to migrate the database to Amazon RDS Oracle will ensure compatibility, so the application can continue to use the same database technology, and the development team can use their existing skills. It also migrates to a managed service, which will handle the availability, so the team do not have to worry about it. Multi-AZ deployment will increase the availability of the database.","comment_id":"772259"},{"comment_id":"754518","upvote_count":"7","content":"Selected Answer: BE\nB. Rehost the application in AWS Elastic Beanstalk with the .NET platform in a Multi-AZ deployment.\nE. Use AWS Database Migration Service (AWS DMS) to migrate from the Oracle database to Oracle on Amazon RDS in a Multi-AZ deployment.\n\nTo minimize development changes while moving the application to AWS and to ensure a high level of availability, the company can rehost the application in AWS Elastic Beanstalk with the .NET platform in a Multi-AZ deployment. This will allow the application to run in a highly available environment without requiring any changes to the application code.\n\nThe company can also use AWS Database Migration Service (AWS DMS) to migrate the Oracle database to Oracle on Amazon RDS in a Multi-AZ deployment. This will allow the company to maintain the existing database platform while still achieving a high level of availability.","timestamp":"1671827100.0","poster":"Buruguduystunstugudunstuy"},{"content":"Selected Answer: BE\nBE = least effort approach.. basically a lift and shift which is what the questions is asking for","comment_id":"1220199","poster":"lofzee","upvote_count":"2","timestamp":"1716897000.0"},{"poster":"hardy1234567","timestamp":"1713793380.0","comment_id":"1200212","content":"d - incorrect at all. Doesn't exist way for migration oracle to dinamoDB.","upvote_count":"2"},{"poster":"awsgeek75","upvote_count":"3","timestamp":"1704068820.0","comment_id":"1110901","content":"Selected Answer: BE\nE for minimizing development changes by using same Oracle engine but in highly available deployment.\nC and D require platform change so it won't work as it increases development.\nA is also development work of converting .Net to .Net core Lambda functions. May not even be possible.\nB is simple lift and shift\nBE is correct"},{"content":"Selected Answer: BE\nDynamoDB is NoSQL - E it out\nReplatform requires considerable overhead - C is out\nLambda function is for running code for short duration - A is out\nAnswer - BE","upvote_count":"6","timestamp":"1695873360.0","comment_id":"1019392","poster":"vijaykamal"},{"upvote_count":"2","poster":"TariqKipkemei","content":"Selected Answer: BE\nMinimize development changes + High availability = AWS Elastic Beanstalk and Oracle on Amazon RDS in a Multi-AZ deployment","timestamp":"1695010560.0","comment_id":"1010239"},{"content":"Selected Answer: B\nB) Rehost the application in AWS Elastic Beanstalk with the .NET platform in a Multi-AZ deployment.\n\nE) Use AWS Database Migration Service (AWS DMS) to migrate from the Oracle database to Oracle on Amazon RDS in a Multi-AZ deployment.\n\nThe reasons are:\n\n° Rehosting in Elastic Beanstalk allows lifting and shifting the .NET application with minimal code changes. Multi-AZ deployment provides high availability.\n° Using DMS to migrate the Oracle data to RDS Oracle in Multi-AZ deployment minimizes changes for the database while achieving high availability.\n° Together this \"lift and shift\" approach minimizes refactoring needs while providing HA on AWS.","timestamp":"1692383640.0","comment_id":"984731","poster":"Guru4Cloud","upvote_count":"2"},{"timestamp":"1687784880.0","content":"Selected Answer: BE\nB. This allows the company to migrate the application to AWS without significant code changes while leveraging the scalability and high availability provided by EBS's Multi-AZ deployment.\n\nE. This enables the company to migrate the Oracle database to RDS while maintaining compatibility with the existing application and leveraging the Multi-AZ deployment for high availability.\n\nA. would require significant development changes and may not provide the same level of compatibility as rehosting or replatforming options.\n\nC. would still require changes to the application and the underlying infrastructure, whereas rehosting with EBS minimizes the need for modification.\n\nD. would likely require significant changes to the application code, as DynamoDB is a NoSQL database with a different data model compared to Oracle.","upvote_count":"4","comment_id":"934484","poster":"cookieMr"},{"timestamp":"1687035660.0","comment_id":"926280","poster":"markw92","upvote_count":"3","content":"Answer is BE. No idea why D was chosen. That requires development work and question clearly states minimize development changes, changing db from Oracle to DynamoDB is LOT of development."},{"content":"Selected Answer: BE\nB + E are the anwers that fulfil the requirements.","upvote_count":"2","poster":"Bmarodi","timestamp":"1685019120.0","comment_id":"906667"},{"timestamp":"1683430080.0","poster":"cheese929","content":"Selected Answer: BE\nB and E","upvote_count":"2","comment_id":"891130"},{"timestamp":"1683083700.0","poster":"Nikhilcy","content":"why not C?","comments":[{"upvote_count":"2","timestamp":"1724992260.0","poster":"AWSSURI","comment_id":"1274906","content":"Retire(simplest) < Retain < Relocate < Rehost < Repurchase < Replatform < Re-architect/Refactor (most complex)....So by this order rehosting less complex than replatform thats why we go for BE"},{"comment_id":"916529","timestamp":"1686074520.0","poster":"AlankarJ","content":"It runs on a windows server, shifting the whole this to Linux based EC2 would be extra work and of no sense","upvote_count":"3"}],"comment_id":"888114","upvote_count":"2"},{"upvote_count":"2","poster":"k33","content":"Selected Answer: BE\nAnswer : BE","timestamp":"1679799240.0","comment_id":"850687"},{"comments":[{"poster":"gustavtd","content":"Because that needs some development,","upvote_count":"3","comment_id":"763987","timestamp":"1672693020.0"}],"content":"Why A is wrong?","poster":"waiyiu9981","comment_id":"763662","timestamp":"1672658400.0","upvote_count":"1"},{"upvote_count":"2","timestamp":"1671820080.0","content":"Selected Answer: BE\nB&E Option ,because D is for No-Sql","comment_id":"754418","comments":[{"comment_id":"771111","content":"And requires additional development effort","timestamp":"1673331480.0","poster":"JayBee65","upvote_count":"2"}],"poster":"techhb"},{"content":"B&E Option","upvote_count":"2","poster":"career360guru","timestamp":"1671264600.0","comment_id":"747861"},{"upvote_count":"4","comment_id":"732200","poster":"dcyberguy","timestamp":"1669864080.0","content":"B- According to the AWS documentation, the simplest way to migrate .NET applications to AWS is to repost the applications using either AWS Elastic Beanstalk or Amazon EC2.\nE - RDS with Oracle is a no-brainer"},{"upvote_count":"4","timestamp":"1669749480.0","poster":"[Removed]","comment_id":"730772","content":"Selected Answer: BE\nsame as everyone else"},{"content":"B E should be correct. Question says \"Minimize development changes\" - so should go for same oracle DB","poster":"KADSM","timestamp":"1669698720.0","upvote_count":"2","comment_id":"729928"},{"comment_id":"729788","upvote_count":"2","content":"Selected Answer: BE\nB for Minimal Development(Elastic BeanStalk)\nE for RDS with Oracle","poster":"Mee6","timestamp":"1669683660.0"},{"comment_id":"729429","timestamp":"1669653420.0","poster":"Gabs90","upvote_count":"2","content":"Selected Answer: BE\nhttps://www.examtopics.com/discussions/amazon/view/67840-exam-aws-certified-solutions-architect-associate-saa-c02/"},{"timestamp":"1669650960.0","poster":"leonnnn","comment_id":"729350","content":"Selected Answer: BE\nB E is correct","upvote_count":"2"},{"content":"Selected Answer: BE\nB and E \nOracle to RDS","upvote_count":"3","timestamp":"1669644600.0","comment_id":"729200","poster":"Nigma"},{"content":"Selected Answer: BE\nmigrate to oracle on RDS is easy compare DynamoDB","poster":"asthman","upvote_count":"2","timestamp":"1669639320.0","comment_id":"729090"}],"question_images":[],"topic":"1","unix_timestamp":1669639320,"answer_images":[],"timestamp":"2022-11-28 13:42:00","choices":{"C":"Replatform the application to run on Amazon EC2 with the Amazon Linux Amazon Machine Image (AMI).","A":"Refactor the application as serverless with AWS Lambda functions running .NET Core.","D":"Use AWS Database Migration Service (AWS DMS) to migrate from the Oracle database to Amazon DynamoDB in a Multi-AZ deployment.","E":"Use AWS Database Migration Service (AWS DMS) to migrate from the Oracle database to Oracle on Amazon RDS in a Multi-AZ deployment.","B":"Rehost the application in AWS Elastic Beanstalk with the .NET platform in a Multi-AZ deployment."},"answer":"BE","answer_description":"","question_text":"A company has a Microsoft .NET application that runs on an on-premises Windows Server. The application stores data by using an Oracle Database Standard Edition server. The company is planning a migration to AWS and wants to minimize development changes while moving the application. The AWS application environment should be highly available.\n\nWhich combination of actions should the company take to meet these requirements? (Choose two.)","isMC":true,"answer_ET":"BE","question_id":129,"answers_community":["BE (97%)","3%"],"url":"https://www.examtopics.com/discussions/amazon/view/89068-exam-aws-certified-solutions-architect-associate-saa-c03/"},{"id":"QxjwoPVVCEQzTBmgl8lr","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/89078-exam-aws-certified-solutions-architect-associate-saa-c03/","exam_id":31,"question_id":130,"answer":"D","isMC":true,"timestamp":"2022-11-28 15:14:00","question_text":"A company runs a containerized application on a Kubernetes cluster in an on-premises data center. The company is using a MongoDB database for data storage. The company wants to migrate some of these environments to AWS, but no code changes or deployment method changes are possible at this time. The company needs a solution that minimizes operational overhead.\n\nWhich solution meets these requirements?","answers_community":["D (100%)"],"question_images":[],"choices":{"D":"Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for compute and Amazon DocumentDB (with MongoDB compatibility) for data storage.","B":"Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for compute and Amazon DynamoDB for data storage","A":"Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 worker nodes for compute and MongoDB on EC2 for data storage.","C":"Use Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 worker nodes for compute and Amazon DynamoDB for data storage."},"topic":"1","answer_ET":"D","answer_images":[],"unix_timestamp":1669644840,"discussion":[{"content":"Selected Answer: D\nIf you see MongoDB, just go ahead and look for the answer that says DocumentDB.","comment_id":"743697","poster":"Marge_Simpson","timestamp":"1686633180.0","upvote_count":"39"},{"timestamp":"1708288980.0","comment_id":"984734","poster":"Guru4Cloud","content":"Selected Answer: D\nOption D is the correct solution that meets all the requirements:\n º Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for compute and Amazon DocumentDB (with MongoDB compatibility) for data storage.\nThe key reasons are:\n º EKS allows running the Kubernetes environment on AWS without changes.\n º Using Fargate removes the need to provision and manage EC2 instances. \n º DocumentDB provides MongoDB compatibility so the data layer is unchanged.","upvote_count":"10"},{"comment_id":"1195101","upvote_count":"2","timestamp":"1728846060.0","poster":"MehulKapadia","content":"Selected Answer: D\nApplications are already containerized. Amazon EKS is fully managed kubernetes service.\nFarGate = Less overhead of managing infrastructure.\nAmazon DocumentDB is MongoDB Compatible.\n \nAnswer D"},{"poster":"LoXoL","timestamp":"1721209020.0","content":"Selected Answer: D\nno brainer says D","upvote_count":"2","comment_id":"1124912"},{"upvote_count":"2","timestamp":"1705727100.0","poster":"james2033","content":"Selected Answer: D\nQuestion keyword \"containerized application\", \"Kubernetes cluster\", \"no changes or deployment method changes\". Choose C, not D.\n\nBut \"minimizes operational overhead\", choose D.","comment_id":"957074"},{"upvote_count":"4","comment_id":"934493","timestamp":"1703603580.0","content":"Selected Answer: D\nThis solution allows the company to leverage EKS to manage the K8s cluster and Fargate to handle the compute resources without requiring manual management of EC2 worker nodes. The use of DocumentDB provides a fully managed MongoDB-compatible database service in AWS.\n\nA. would require managing and scaling the EC2 instances manually, which increases operational overhead.\n\nB. would require significant changes to the application code as DynamoDB is a NoSQL database with a different data model compared to MongoDB.\n\nC. would also require code changes to adapt to DynamoDB's different data model, and managing EC2 worker nodes increases operational overhead.","poster":"cookieMr"},{"upvote_count":"2","comment_id":"906675","timestamp":"1700924460.0","poster":"Bmarodi","content":"Selected Answer: D\nThe solution meets these requirements is option D."},{"comment_id":"898340","poster":"studynoplay","content":"Selected Answer: D\nminimizes operational overhead = Serverless (Fargate)\nMongoDB = DocumentDB","timestamp":"1700063340.0","upvote_count":"2"},{"poster":"Buruguduystunstugudunstuy","timestamp":"1687544460.0","content":"Selected Answer: D\nTo minimize operational overhead and avoid making any code or deployment method changes, the company can use Amazon Elastic Kubernetes Service (EKS) with AWS Fargate for computing and Amazon DocumentDB (with MongoDB compatibility) for data storage. This solution allows the company to run the containerized application on EKS without having to manage the underlying infrastructure or make any changes to the application code.\n\nAWS Fargate is a fully-managed container execution environment that allows you to run containerized applications without the need to manage the underlying EC2 instances.\n\nAmazon DocumentDB is a fully-managed document database service that supports MongoDB workloads, allowing the company to use the same database platform as in their on-premises environment without having to make any code changes.","upvote_count":"5","comment_id":"754508"},{"comment_id":"754411","content":"Selected Answer: D\nReason A &B Elimnated as its Kubernates\nwhy D read here https://containersonaws.com/introduction/ec2-or-aws-fargate/","poster":"techhb","timestamp":"1687537020.0","upvote_count":"3"},{"poster":"career360guru","comment_id":"747863","upvote_count":"3","timestamp":"1686982260.0","content":"Selected Answer: D\nOption D"},{"content":"DDDDDDD","timestamp":"1685581080.0","comment_id":"732195","upvote_count":"2","poster":"dcyberguy"},{"content":"Selected Answer: D\nhttps://www.examtopics.com/discussions/amazon/view/67897-exam-aws-certified-solutions-architect-associate-saa-c02/","poster":"Gabs90","upvote_count":"2","comment_id":"729430","timestamp":"1685284740.0"},{"comment_id":"729351","content":"Selected Answer: D\nD meets the requirements","timestamp":"1685282220.0","upvote_count":"2","poster":"leonnnn"},{"comment_id":"729203","timestamp":"1685276040.0","upvote_count":"3","content":"Selected Answer: D\nD\nEKS because of Kubernetes so A and B are eliminated\nnot C because of MongoDB and Fargate is more expensive","poster":"Nigma"}]}],"exam":{"provider":"Amazon","isMCOnly":true,"isBeta":false,"id":31,"numberOfQuestions":1019,"lastUpdated":"11 Apr 2025","isImplemented":true,"name":"AWS Certified Solutions Architect - Associate SAA-C03"},"currentPage":26},"__N_SSP":true}