{"pageProps":{"questions":[{"id":"4fa0HZZldheX5IhMJKC3","exam_id":26,"answer":"A","discussion":[{"upvote_count":"6","poster":"Mickey321","timestamp":"1724098500.0","content":"Selected Answer: A\nBatch Transform can efficiently handle this workload by splitting the files into mini-batches and distributing them across multiple instances. Batch Transform can also scale down the instances when there are no files to process, so you only pay for the duration that the instances are actively processing files.\nBatch Transform is more cost-effective than Asynchronous Inference because Asynchronous Inference is designed for workloads with large payload sizes (up to 1GB) and long processing times (up to 15 minutes) that need near real-time responses. Asynchronous Inference queues incoming requests and processes them asynchronously, returning an output location as a response. Asynchronous Inference also autoscales the instance count to zero when there are no requests to process. However, Asynchronous Inference charges you for both request processing and request queuing time, which may be higher than Batch Transform for your use case.","comment_id":"985429"},{"upvote_count":"3","poster":"loict","timestamp":"1726065720.0","comment_id":"1004909","content":"Selected Answer: A\nA. YES - Batch Transform can pick up new files from S3\nB. NO - no need for asynch, high-throughpt queues\nC. NO - Processing is not for model inference\nD. NO - no need for scaling through multiple endpoints"},{"content":"Selected Answer: A\nKey point is data is collected every hour. seems like a batch solution is most cost effective","upvote_count":"3","comment_id":"844728","poster":"blanco750","timestamp":"1710929100.0"},{"content":"Selected Answer: A\nI will go with A. The Async inference seems promising but the size of telemetry file is not known.\n\nAs per https://docs.aws.amazon.com/sagemaker/latest/dg/inference-cost-optimization.html\n\n\"Use batch inference for workloads for which you need inference for a large set of data for processes that happen offline (that is, you don’t need a persistent endpoint). You pay for the instance for the duration of the batch inference job\". As you pay for the batch job duration, cost should not be an issue with Batch transform.\n\n\"Use asynchronous inference for asynchronous workloads that process up to 1 GB of data (such as text corpus, image, video, and audio) that are latency insensitive and cost sensitive. With asynchronous inference, you can control costs by specifying a fixed number of instances for the optimal processing rate instead of provisioning for the peak. You can also scale down to zero to save additional costs.\"","comment_id":"837831","upvote_count":"3","timestamp":"1710324960.0","poster":"pan_b"},{"comment_id":"829299","poster":"oso0348","upvote_count":"2","content":"Selected Answer: A\nBased on the requirements and constraints given in the scenario, the MOST cost-effective solution for the company to use to run the model across the telemetry for all the devices is SageMaker Batch Transform.\n\nSageMaker Batch Transform is a cost-effective solution for performing offline inference, as it allows for large amounts of data to be processed at a lower cost compared to real-time inference. In this case, the telemetry data for each device is collected hourly and can be processed in batches using SageMaker Batch Transform. This can help to reduce the cost of inference, as the data is not being processed in real-time and can be processed offline.","timestamp":"1709579820.0"},{"poster":"Siyuan_Zhu","timestamp":"1708820820.0","content":"Selected Answer: B\nB -- based on what Drcock87 said, \nas well as this: \"Amazon SageMaker Asynchronous Inference is a new capability in SageMaker that queues incoming requests and processes them asynchronously. Compared to Batch Transform Asynchronous Inference provides immediate access to the results of the inference job rather than waiting for the job to complete\"","comment_id":"821049","comments":[{"upvote_count":"3","content":"I still think its A because:\n- \"The 4-day telemetry of each device is collected in a separate file and is placed in an Amazon S3 bucket once every hour.\" Which means this is use-case where data is available upfront for inferencing.\n- Also, unlike async the batch transform does not keep an active endpoint all the time. async is similar to real-time inference, used when you need inference right-away; the question is not asking for real-time inference.","timestamp":"1709349840.0","poster":"drcok87","comment_id":"826466"}],"upvote_count":"1"},{"content":"Real-Time Inference is suitable for workloads where payload sizes are up to 6MB and need to be processed with low latency requirements in the order of milliseconds or seconds.\n\nServerless Inference: Serverless inference is ideal when you have intermittent or unpredictable traffic patterns.\n\nBatch transform is ideal for offline predictions on large batches of data that is available upfront.\n\nWe are introducing Amazon SageMaker Asynchronous Inference, a new inference option in Amazon SageMaker that queues incoming requests and processes them asynchronously. This option is ideal for inferences with large payload sizes (up to 1GB) and/or long processing times (up to 15 minutes) that need to be processed as requests arrive. Asynchronous inference enables you to save on costs by autoscaling the instance count to zero when there are no requests to process, so you only pay when your endpoint is processing requests.\n\na","comments":[{"comment_id":"826469","poster":"drcok87","upvote_count":"1","content":"Real-time inference is suitable for workloads where payload sizes are up to 6MB and need to be processed with low latency requirements in the order of milliseconds or seconds. Batch transform is ideal for offline predictions on large batches of data that is available upfront. The new asynchronous inference option is ideal for workloads where the request sizes are large (up to 1GB) and inference processing times are in the order of minutes (up to 15 minutes). Example workloads for asynchronous inference include running predictions on high resolution images generated from a mobile device at different intervals during the day and providing responses within minutes of receiving the request.","timestamp":"1709350020.0"}],"timestamp":"1707609000.0","comment_id":"804870","poster":"drcok87","upvote_count":"2"}],"question_images":[],"unix_timestamp":1676073000,"answers_community":["A (94%)","6%"],"question_id":136,"question_text":"A manufacturing company wants to monitor its devices for anomalous behavior. A data scientist has trained an Amazon SageMaker scikit-learn model that classifies a device as normal or anomalous based on its 4-day telemetry. The 4-day telemetry of each device is collected in a separate file and is placed in an Amazon S3 bucket once every hour. The total time to run the model across the telemetry for all devices is 5 minutes.\n\nWhat is the MOST cost-effective solution for the company to use to run the model across the telemetry for all the devices?","timestamp":"2023-02-11 00:50:00","choices":{"B":"SageMaker Asynchronous Inference","A":"SageMaker Batch Transform","C":"SageMaker Processing","D":"A SageMaker multi-container endpoint"},"isMC":true,"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/98764-exam-aws-certified-machine-learning-specialty-topic-1/","topic":"1","answer_ET":"A","answer_images":[]},{"id":"zSM05KzvVF8UTdCrS6V1","topic":"1","question_id":137,"timestamp":"2023-02-06 18:27:00","answer":"D","answer_images":[],"unix_timestamp":1675704420,"choices":{"B":"Calculate the principal component analysis (PCA) components. Create a line plot of the number of components against the explained variance. The optimal value of k is the number of PCA components after which the curve starts decreasing in a linear fashion.","A":"Calculate the principal component analysis (PCA) components. Run the k-means clustering algorithm for a range of k by using only the first two PCA components. For each value of k, create a scatter plot with a different color for each cluster. The optimal value of k is the value where the clusters start to look reasonably separated.","C":"Create a t-distributed stochastic neighbor embedding (t-SNE) plot for a range of perplexity values. The optimal value of k is the value of perplexity, where the clusters start to look reasonably separated.","D":"Run the k-means clustering algorithm for a range of k. For each value of k, calculate the sum of squared errors (SSE). Plot a line chart of the SSE for each value of k. The optimal value of k is the point after which the curve starts decreasing in a linear fashion."},"isMC":true,"discussion":[{"content":"Selected Answer: D\nThe Answer is D","timestamp":"1707240420.0","upvote_count":"5","comment_id":"800001","poster":"Amit11011996"},{"upvote_count":"4","comment_id":"986354","poster":"Mickey321","content":"Selected Answer: D\nOption D uses the elbow method, which is a popular and well-known method for determining the optimal value of k for k-means clustering1. It plots the sum of squared errors (SSE) for different values of k, and looks for the point where the SSE starts to decrease in a linear fashion. This point is called the elbow, and it indicates that adding more clusters does not improve the model significantly2.","timestamp":"1724232540.0"},{"poster":"oso0348","content":"Selected Answer: D\nThe Sum of square shows variation within each cluster","comment_id":"829293","timestamp":"1709579580.0","upvote_count":"3"},{"poster":"AjoseO","comment_id":"815949","content":"Selected Answer: D\nD. Run the k-means clustering algorithm for a range of k. For each value of k, calculate the sum of squared errors (SSE). Plot a line chart of the SSE for each value of k. The optimal value of k is the point after which the curve starts decreasing in a linear fashion.\n\nThe sum of squared errors (SSE) measures the total variation within each cluster, and the optimal value of k is typically the point where the SSE begins to level off or decrease sharply. Plotting the SSE against the number of clusters (k) allows the data scientist to identify the optimal number of clusters based on where the SSE curve starts decreasing linearly.","upvote_count":"4","timestamp":"1708467120.0"},{"timestamp":"1707611280.0","comment_id":"804883","poster":"drcok87","content":"d https://towardsdatascience.com/explain-ml-in-a-simple-way-k-means-clustering-e925d019743b","upvote_count":"2"}],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/98201-exam-aws-certified-machine-learning-specialty-topic-1/","question_text":"A company wants to segment a large group of customers into subgroups based on shared characteristics. The company’s data scientist is planning to use the Amazon SageMaker built-in k-means clustering algorithm for this task. The data scientist needs to determine the optimal number of subgroups (k) to use.\n\nWhich data visualization approach will MOST accurately determine the optimal value of k?","question_images":[],"answer_ET":"D","answers_community":["D (100%)"],"exam_id":26},{"id":"T1yFok8MZJZQ3DUP3uFY","answer_images":[],"answers_community":["AB (100%)"],"timestamp":"2023-02-11 01:32:00","isMC":true,"question_id":138,"question_images":[],"exam_id":26,"answer":"AB","answer_ET":"AB","unix_timestamp":1676075520,"question_text":"A data scientist at a financial services company used Amazon SageMaker to train and deploy a model that predicts loan defaults. The model analyzes new loan applications and predicts the risk of loan default. To train the model, the data scientist manually extracted loan data from a database. The data scientist performed the model training and deployment steps in a Jupyter notebook that is hosted on SageMaker Studio notebooks. The model's prediction accuracy is decreasing over time.\n\nWhich combination of steps is the MOST operationally efficient way for the data scientist to maintain the model's accuracy? (Choose two.)","discussion":[{"upvote_count":"1","content":"Selected Answer: AB\nA. YES - fully automated pipeline\nB. YES - triggers the pipeline A as needed\nC. NO - email notification does not allow automation\nD. NO - manual steps required, not operationaly efficient\nE. NO - we need another step to trigger the Lambda","comment_id":"1004925","timestamp":"1726066500.0","poster":"loict"},{"comment_id":"986521","timestamp":"1724245260.0","content":"Selected Answer: AB\nOption A uses SageMaker Pipelines to create an automated workflow that extracts fresh data, trains the model, and deploys a new version of the model. This option is operationally efficient because it eliminates the need for manual intervention and ensures that your model is always up to date with the latest data. You can also use SageMaker Pipelines to orchestrate your workflow using a graphical interface or a Python SDK1.\nOption B configures SageMaker Model Monitor with an accuracy threshold to check for model drift. Model drift occurs when the statistical properties of the target variable change over time, which can affect the performance of your model2.","poster":"Mickey321","upvote_count":"1"},{"poster":"Mllb","upvote_count":"2","content":"Selected Answer: AB\nhttps://aws.amazon.com/blogs/machine-learning/automate-model-retraining-with-amazon-sagemaker-pipelines-when-drift-is-detected/","timestamp":"1711958340.0","comment_id":"857719"},{"content":"Selected Answer: AB\nRetrain the model when the accuracy is decreasing is the most recommended way to take of your models.","poster":"Valcilio","timestamp":"1709989140.0","upvote_count":"2","comment_id":"833973"},{"poster":"oso0348","upvote_count":"2","comment_id":"829291","timestamp":"1709579400.0","content":"Selected Answer: AB\nThe MOST operationally efficient way for the data scientist to maintain the model's accuracy would be to choose options A and B:\n\nA. Use SageMaker Pipelines to create an automated workflow that extracts fresh data, trains the model, and deploys a new version of the model.\n\nUsing SageMaker Pipelines allows the data scientist to automate the entire workflow from data extraction to model deployment. This ensures that the model is trained and deployed on the latest data automatically without the need for manual intervention. The data scientist can set up the pipeline to run on a schedule or trigger it based on certain events.\n\nB. Configure SageMaker Model Monitor with an accuracy threshold to check for model drift. Initiate an Amazon CloudWatch alarm when the threshold is exceeded. Connect the workflow in SageMaker Pipelines with the CloudWatch alarm to automatically initiate retraining."},{"poster":"AjoseO","content":"Selected Answer: AB\nUsing SageMaker Pipelines to create an automated workflow that extracts fresh data, trains the model, and deploys a new version of the model is an efficient way to automate the process of model retraining and deployment.\n\nConfiguring SageMaker Model Monitor with an accuracy threshold to check for model drift and initiating an Amazon CloudWatch alarm when the threshold is exceeded is an efficient way to monitor the accuracy of the deployed model and initiate retraining when necessary. This approach helps to maintain the accuracy of the model over time.","comment_id":"815953","timestamp":"1708467180.0","upvote_count":"2"},{"content":"Selected Answer: AB\nhttps://aws.amazon.com/blogs/machine-learning/automate-model-retraining-with-amazon-sagemaker-pipelines-when-drift-is-detected/","comment_id":"809146","poster":"Jerry84","timestamp":"1707975300.0","upvote_count":"2"},{"upvote_count":"1","poster":"drcok87","comment_id":"804885","timestamp":"1707611520.0","content":"B first and then A"}],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/98765-exam-aws-certified-machine-learning-specialty-topic-1/","topic":"1","choices":{"C":"Store the model predictions in Amazon S3. Create a daily SageMaker Processing job that reads the predictions from Amazon S3, checks for changes in model prediction accuracy, and sends an email notification if a significant change is detected.","B":"Configure SageMaker Model Monitor with an accuracy threshold to check for model drift. Initiate an Amazon CloudWatch alarm when the threshold is exceeded. Connect the workflow in SageMaker Pipelines with the CloudWatch alarm to automatically initiate retraining.","E":"Export the training and deployment code from the SageMaker Studio notebooks into a Python script. Package the script into an Amazon Elastic Container Service (Amazon ECS) task that an AWS Lambda function can initiate.","D":"Rerun the steps in the Jupyter notebook that is hosted on SageMaker Studio notebooks to retrain the model and redeploy a new version of the model.","A":"Use SageMaker Pipelines to create an automated workflow that extracts fresh data, trains the model, and deploys a new version of the model."}},{"id":"BIKO313wNTuIkmZ0440E","exam_id":26,"answer":"BEF","question_images":["https://img.examtopics.com/aws-certified-machine-learning-specialty/image1.png"],"discussion":[{"poster":"loict","content":"Selected Answer: BEF\nA. NO - reducing data will not help in a better model; the more the merrier :-)\nB. YES - It can address non-linearity in the full spectrum\nC. NO - reducing data will not help in a better model; the more the merrier :-)\nD. NO - residual is not constant when price > 50\nE. YES - that can help make non-linear data linear\nF. YES - it can capture more complex relationships","timestamp":"1726071960.0","comment_id":"1004978","upvote_count":"4"},{"content":"Selected Answer: BEF\nOption E suggests that you examine the input data, and apply non-linear data transformations where appropriate. This option is helpful because it can reduce the non-linearity in your data and make it more suitable for a linear model. For example, you can apply a logarithmic, square root, or inverse transformation to your price variable and see if it improves the fit of your model1. You can also use the Box-Cox transformation, which is a method that automatically finds the best transformation for your data2.\nOption F suggests that you use a non-linear model instead of a linear model. This option is also helpful because it can capture the non-linear relationship between price and sales that is evident in your residual plot.\nOption B suggests that you create two different models for different sections of the data. This option is also helpful because it can account for the different behavior of your data at different price ranges.","comment_id":"986529","timestamp":"1724245980.0","poster":"Mickey321","upvote_count":"1"},{"upvote_count":"1","poster":"Peng001","timestamp":"1720839180.0","comment_id":"950285","content":"Selected Answer: BEF\nThe linear model y = ax + b works well for x < 50, but for x > 50 the residual increases linearly, meaning that the slope linear model increases, i.e., y = a'x + b' with a' != a.\n\nOffset will not help. Downsampling will not help either."},{"comments":[{"comments":[{"content":"It appears on 2023-April-03","poster":"Mllb","upvote_count":"2","timestamp":"1712682120.0","comment_id":"865694"}],"timestamp":"1711958700.0","content":"Then, BEF","upvote_count":"3","poster":"Mllb","comment_id":"857725"}],"poster":"Mllb","content":"The linear model doesn't capture the data complexity","timestamp":"1711958640.0","comment_id":"857724","upvote_count":"1"},{"timestamp":"1710246660.0","poster":"stjokerli","content":"Selected Answer: BEF\nAs per wolfsong said","comment_id":"837001","upvote_count":"4"},{"upvote_count":"1","comments":[{"poster":"Chelseajcole","comment_id":"826224","timestamp":"1709322300.0","content":"Bde should be the answer","upvote_count":"1"}],"content":"Selected Answer: CDE\nTwo models , add a constant or in-put data transformation","comment_id":"826223","timestamp":"1709322240.0","poster":"Chelseajcole"},{"poster":"AjoseO","timestamp":"1708467000.0","upvote_count":"1","comments":[{"timestamp":"1708558680.0","upvote_count":"1","content":"If D is the answer of this question, Isn't B the another answer too?\nSuppose that the initial linear model means y = aX + b, then D means y = a(X - C) + b --> y = aX + b' (when Price > 50)\nI think that D means we would use two different linear models for different sections (Price = 50) of the data.","comment_id":"817283","poster":"GiyeonShin"},{"upvote_count":"11","comment_id":"816832","poster":"wolfsong","content":"A good residual plot is a flat line at y = 0. So...\n- Not sure if D is right. If you offset by a constant value, you're just moving the plot up or down. You'd have to add a term like K*Price, where price > 50 and K > 0, for you to flatten that curve beyond Price > 50.\n- Also unsure about C. The variance looks fairly good for Price < 50 as it's mostly around zero which is what you want. The problem is the residual value at Price > 50 which goes way off.\n\nI'd go with B, E & F:\nE: obvious\nF: use non-linear model instead as it will remove the kink in the plot\nB: Not an answer I like, but if you can't use a nonlinear model, you need to use a piecewise-linear model that separates the data in two. Something like this: https://towardsdatascience.com/piecewise-linear-regression-model-what-is-it-and-when-can-we-use-it-93286cfee452","timestamp":"1708532640.0"}],"content":"Selected Answer: CDE\nThe residual plot shows that the linear model is not fitting the data well, with a clear pattern indicating that the model is underfitting. To improve the accuracy of the predictions, the ML engineer should take the following actions:\n\nC. Downsample the data in sections where Price < 50: This could be an option since there seems to be a higher variance in the residuals in the region where Price < 50.\n\nD. Offset the input data by a constant value where Price > 50: This could be an option since there seems to be a systematic bias in the residuals in the region where Price > 50.\n\n\nE. Examine the input data, and apply non-linear data transformations where appropriate: This is necessary since the residual plot shows that the linear model is not capturing the non-linear relationships in the data.","comment_id":"815941"}],"answers_community":["BEF (83%)","CDE (17%)"],"unix_timestamp":1676931000,"question_id":139,"question_text":"A retail company wants to create a system that can predict sales based on the price of an item. A machine learning (ML) engineer built an initial linear model that resulted in the following residual plot:\n\n//IMG//\n\n\nWhich actions should the ML engineer take to improve the accuracy of the predictions in the next phase of model building? (Choose three.)","timestamp":"2023-02-20 23:10:00","isMC":true,"choices":{"C":"Downsample the data in sections where Price < 50.","D":"Offset the input data by a constant value where Price > 50.","F":"Use a non-linear model instead of a linear model.","B":"Create two different models for different sections of the data.","A":"Downsample the data uniformly to reduce the amount of data.","E":"Examine the input data, and apply non-linear data transformations where appropriate."},"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/100146-exam-aws-certified-machine-learning-specialty-topic-1/","topic":"1","answer_ET":"BEF","answer_images":[]},{"id":"J5dLb5F6XQOClYOKgXfH","answer_ET":"A","url":"https://www.examtopics.com/discussions/amazon/view/98308-exam-aws-certified-machine-learning-specialty-topic-1/","unix_timestamp":1675775820,"question_images":[],"topic":"1","answer_images":[],"question_text":"A data scientist at a food production company wants to use an Amazon SageMaker built-in model to classify different vegetables. The current dataset has many features. The company wants to save on memory costs when the data scientist trains and deploys the model. The company also wants to be able to find similar data points for each test data point.\n\nWhich algorithm will meet these requirements?","choices":{"D":"Principal component analysis (PCA) with the algorithm mode set to random","C":"K-means","B":"Linear learner with early stopping","A":"K-nearest neighbors (k-NN) with dimension reduction"},"answers_community":["A (63%)","C (37%)"],"exam_id":26,"question_id":140,"answer":"A","discussion":[{"content":"Selected Answer: A\nto be able to find similar data points for each test data point.","timestamp":"1739843100.0","comment_id":"1358079","poster":"Carpediem78","upvote_count":"2"},{"upvote_count":"1","timestamp":"1718411040.0","comment_id":"1230724","content":"Selected Answer: C\nK-means unsupervised learning","poster":"sheetalconect"},{"comment_id":"1196365","content":"It is A\n\nMemory Efficiency: K-nearest neighbors (k-NN) doesn't require storing a model with learned parameters, as it's an instance-based learning algorithm. It simply memorizes the training dataset. Therefore, it saves on memory costs compared to models with learned parameters like linear learners.\n\nDimension Reduction: By employing dimension reduction techniques like Principal Component Analysis (PCA) in conjunction with k-NN, you can reduce the dimensionality of the dataset, which helps in saving memory costs. This makes k-NN with dimension reduction a suitable choice when memory efficiency is a concern.\n\nSimilar Data Points: K-nearest neighbors naturally provides a measure of similarity between data points. Given a test data point, it finds the k nearest neighbors in the training data. This fulfills the requirement of being able to find similar data points for each test data point.","upvote_count":"1","poster":"ddaanndann","timestamp":"1713246780.0"},{"poster":"JonSno","upvote_count":"1","timestamp":"1713116280.0","content":"A. K-nearest neighbors (k-NN) with dimension reduction ( KNN - Useful for Classification task for different types of veggies based on features + Dimensionality reductions like PCA can be applied prior to KNN to reduce no.of features in dataset , thereby saving memory costs during training and model deployment - also remove noise and data redundancy )","comment_id":"1195636"},{"timestamp":"1712365740.0","poster":"Gmishra","content":"Selected Answer: C\nhttps://www.linkedin.com/advice/3/what-difference-between-knn-k-means-skills-computer-science-cx1hc","comment_id":"1190170","upvote_count":"1"},{"upvote_count":"2","poster":"AIWave","timestamp":"1709328900.0","content":"Selected Answer: C\nThis is un unsupervised clustering problem not a classification one (A). k-means is a better choice from memory efficiency perspective.","comment_id":"1163776"},{"timestamp":"1701444240.0","upvote_count":"2","content":"Selected Answer: C\nWhile A is most voted comment, but knn is really high on memory usage as it stores the data points information to make predictions. Just voting for it because it mentions dimensionality reduction is obtuse. C is the next most probable candidate that fits the bill on every account.","poster":"akgarg00","comment_id":"1085307"},{"comment_id":"1050524","timestamp":"1697975880.0","upvote_count":"2","poster":"DimLam","content":"Selected Answer: A\nShould be A, as only A is can be used for classification, finding similar data points and dimensionality reduction"},{"upvote_count":"2","poster":"loict","timestamp":"1694494680.0","content":"Selected Answer: A\nA. YES - K-NN will find the similar datapoints, and dimension reduction will save memory\nB. NO - Linear learner is for regression or classification, not finding similar data points\nC. NO - K-means is for unsupervised clustering, not find closest data ponits\nD. NO - Principal component analysis (PCA) with the algorithm mode set to random","comment_id":"1005384"},{"upvote_count":"1","poster":"kaike_reis","content":"Selected Answer: A\nC doesn't solve the \"too many features\" problem + It's well defined the vegetable classes.\nA is the way","timestamp":"1692040800.0","comment_id":"981039"},{"comment_id":"844775","upvote_count":"2","content":"Selected Answer: A\nKNN to reduce dimensionality which may help reduce memory utilisation.","timestamp":"1679309280.0","poster":"blanco750"},{"upvote_count":"1","poster":"Valcilio","content":"Selected Answer: A\nIt's A, needs to reduct the dimensionality of the dataset.","timestamp":"1678229160.0","comment_id":"832344"},{"comment_id":"826225","poster":"Chelseajcole","upvote_count":"2","timestamp":"1677700020.0","content":"Selected Answer: A\nThey wanna less feature"},{"comment_id":"823188","timestamp":"1677472620.0","content":"I will go with A.\n\nC is not valid, as K-means is a clustering algorithm that can group similar data points together. However, it does not perform classification, and it is not clear how it addresses the memory cost and similarity search requirements mentioned in the question.","upvote_count":"1","poster":"lmimi"},{"comment_id":"821345","content":"Selected Answer: C\nIt should be C,\nbecause it is unsupervised classification problem.","comments":[{"comment_id":"1050522","content":"Not sure that classification is unsupervised problem","timestamp":"1697975700.0","upvote_count":"1","poster":"DimLam"}],"timestamp":"1677317580.0","upvote_count":"1","poster":"Amit11011996"},{"upvote_count":"2","timestamp":"1676930460.0","comments":[{"poster":"blanco750","comment_id":"844772","timestamp":"1679309040.0","upvote_count":"1","content":"agree. By reducing the number of dimensions, you may achieve comparable analysis results using less memory and in a shorter amount of time."}],"poster":"AjoseO","content":"Selected Answer: A\noption A suggests using the k-nearest neighbors (k-NN) algorithm with dimension reduction. The k-NN algorithm can be used for classification tasks and dimension reduction can help reduce memory costs. Additionally, k-NN can be used for finding similar data points.\n\nK-NN is a simple algorithm that works well with high-dimensional data and can find similar data points.","comment_id":"815926"},{"comment_id":"814282","content":"https://ealizadeh.com/blog/knn-and-kmeans/#:~:text=unsupervised%20learning%20algorithm.-,K%20in%20K%2DMeans%20refers%20to%20the%20number%20of%20clusters,using%20different%20values%20for%20K.\nKNN does use lot of memory because in lazy learning it stores (memorizes) the training dataset. AWS sagemaker however has an improved version of this algorithm.\nBecause the questions does not mention we have labels, we cannot use supervised learning\n\nK-means : unsupervised and helps us to classify different vegetables based on their many features. This also find \"similar data points for each test data point\"\n\nc","timestamp":"1676825640.0","upvote_count":"1","poster":"drcok87"},{"upvote_count":"3","timestamp":"1676440260.0","comment_id":"809160","poster":"Jerry84","content":"Selected Answer: A\n\"Training with the k-NN algorithm has three steps: sampling, dimension reduction, and index building. Sampling reduces the size of the initial dataset so that it fits into memory. For dimension reduction, the algorithm decreases the feature dimension of the data to reduce the footprint of the k-NN model in memory and inference latency.\"\n\" The main objective of k-NN's training is to construct the index. The index enables efficient lookups of distances between points whose values or class labels have not yet been determined and the k nearest points to use for inference.\"\n\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/k-nearest-neighbors.html"},{"content":"Selected Answer: C\nshould be C","upvote_count":"3","poster":"Aninina","comment_id":"800917","timestamp":"1675775820.0"}],"isMC":true,"answer_description":"","timestamp":"2023-02-07 14:17:00"}],"exam":{"provider":"Amazon","id":26,"isMCOnly":false,"isBeta":false,"isImplemented":true,"numberOfQuestions":369,"lastUpdated":"11 Apr 2025","name":"AWS Certified Machine Learning - Specialty"},"currentPage":28},"__N_SSP":true}