{"pageProps":{"questions":[{"id":"VyVNa2DysJy5nPZLPYMD","answer_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/5005-exam-aws-certified-solutions-architect-professional-topic-1/","answer_description":"","answer":"A","question_text":"A bank is re-architecting its mainframe-based credit card approval processing application to a cloud-native application on the AWS cloud.\nThe new application will receive up to 1,000 requests per second at peak load. There are multiple steps to each transaction, and each step must receive the result of the previous step. The entire request must return an authorization response within less than 2 seconds with zero data loss. Every request must receive a response. The solution must be Payment Card Industry Data Security Standard (PCI DSS)-compliant.\nWhich option will meet all of the bank's objectives with the LEAST complexity and LOWEST cost while also meeting compliance requirements?","answers_community":["A (50%)","C (29%)","D (21%)"],"choices":{"C":"Deploy the application on Amazon EC2 on Dedicated Instances. Use an Elastic Load Balancer in front of a farm of application servers in an Auto Scaling group to handle incoming requests. Scale out/in based on a custom Amazon CloudWatch metric for the number of inbound requests per second after measuring the capacity of a single instance.","A":"Create an Amazon API Gateway to process inbound requests using a single AWS Lambda task that performs multiple steps and returns a JSON object with the approval status. Open a support case to increase the limit for the number of concurrent Lambdas to allow room for bursts of activity due to the new application.","B":"Create an Application Load Balancer with an Amazon ECS cluster on Amazon EC2 Dedicated Instances in a target group to process incoming requests. Use Auto Scaling to scale the cluster out/in based on average CPU utilization. Deploy a web service that processes all of the approval steps and returns a JSON object with the approval status.","D":"Create an Amazon API Gateway to process inbound requests using a series of AWS Lambda processes, each with an Amazon SQS input queue. As each step completes, it writes its result to the next step's queue. The final step returns a JSON object with the approval status. Open a support case to increase the limit for the number of concurrent Lambdas to allow room for bursts of activity due to the new application."},"timestamp":"2019-09-10 16:06:00","question_images":[],"unix_timestamp":1568124360,"discussion":[{"timestamp":"1632128280.0","comments":[{"timestamp":"1649252220.0","content":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html \n\"Allocate tasks to multiple worker nodes: process a high number of credit card validation requests.\"","comments":[{"comment_id":"710914","poster":"sindra","upvote_count":"1","content":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/standard-queues.html \nIncline to D","timestamp":"1667538240.0"}],"comment_id":"581844","poster":"Yamchi","upvote_count":"6"},{"poster":"heany","upvote_count":"2","content":"only problem is that : A Lambda function may run for up to 15 minutes (this is called the Lambda timeout), meaning Lambda is not suited to long-running processes ' . According to the question, the app 'will receive up to 1,000 requests per second at peak load' which means the lambda function will need to keep running. So, both A and D don't look right. that leaves only C.","comment_id":"692487","timestamp":"1665530820.0"}],"comment_id":"10453","content":"Seems like D would be a better choice","poster":"dpvnme","upvote_count":"28"},{"comment_id":"13657","comments":[{"poster":"petebear55","content":"a will cause too much bottle neck","comment_id":"247557","upvote_count":"3","timestamp":"1635019680.0"},{"timestamp":"1632650100.0","poster":"PacoDerek","upvote_count":"3","content":"SQS guarantee at-least-One delivery","comments":[{"comment_id":"46227","content":"The entire request must return an authorization response within less than 2 seconds with zero data loss.","poster":"ashp","timestamp":"1632664080.0","upvote_count":"2"}],"comment_id":"43527"},{"upvote_count":"4","poster":"Amitv2706","comment_id":"380550","content":"if SQS is used we still have an option to keep storing the message as failed lambda will not delete it ideally.\n\nBut with A - Where that message/req will go if lambda fails ?\n\nSeems D is better choice","timestamp":"1636115040.0"}],"content":"A\nA: The process must complete within 2 seconds. This sounds like what Lambda can do.\nB\\C: Not the most cost efficient compared to Lambda. Scaling may also not be fast enough.\nD: SQS may have data loss due to DLQ? SQS also does not process in order so this may be another problem unless you use FIFO.","poster":"donathon","timestamp":"1632441660.0","upvote_count":"15"},{"upvote_count":"1","poster":"sumaju","comment_id":"1078445","content":"Selected Answer: C\nTo be PCI DSS compliant dedicated instance is mandatory, so only option C is correct irrespective of any other approach.","timestamp":"1700745120.0"},{"upvote_count":"1","timestamp":"1687456140.0","poster":"SkyZeroZx","content":"Selected Answer: A\nA is less overhead than B for use of SQS and in sync invokation of lambdas in 2 seconds","comment_id":"930861"},{"timestamp":"1681679280.0","upvote_count":"1","comment_id":"872189","poster":"romiao106","content":"This is a use case for step function. I will go with D because a series of lambda processes this appears to be describing a step function"},{"timestamp":"1681387980.0","content":"Selected Answer: A\nA seems better than D\nLEAST complexity and LOWEST cost == API Gateway + Lambda\n\nD is wrong! SQS is Asynchronous so the response from API Gateway will be just a 200 http code (or whatever you decide to return). the question stated that response should be immediate which makes D answer invalid","comment_id":"869380","upvote_count":"2","poster":"dev112233xx"},{"upvote_count":"1","timestamp":"1676017620.0","comment_id":"804115","content":"ChatGPT response :\nOption 2 of creating an Amazon API Gateway to process inbound requests using a single AWS Lambda task is a better option for meeting the bank's objectives with the least complexity and lowest cost while also meeting compliance requirements. This is because it avoids the complexity and overhead of multiple Lambda functions, SQS queues, and coordination of the results between the steps. The single Lambda function can be designed to handle all the steps in the processing of a credit card approval transaction, which would make the implementation simpler and more efficient. Additionally, by having a single Lambda function, it is easier to ensure that the response time of less than 2 seconds and zero data loss requirements are met.\n\nOption A looks more relevant","poster":"Heer"},{"content":"Selected Answer: C\nA-> 1000 request per seconds and less than 2 secs to complete, so you can achieve 2000 lambdas running. You need to request a limit increase.\nBetween C and D, but the question emphasis is on LEAST complexity and LOWEST cost, so C looks better.","upvote_count":"2","poster":"evargasbrz","timestamp":"1672231500.0","comment_id":"759822"},{"timestamp":"1671154860.0","comment_id":"746676","content":"Selected Answer: A\n1000 request per seconds and less than 2 secs response time lambda fits the bill","upvote_count":"1","poster":"myco"},{"poster":"timmysixstrings","timestamp":"1669996200.0","upvote_count":"1","content":"Selected Answer: A\nEC2 is too slow to scale, so that rules out B/C. \nThe question emphasis is on LEAST complexity and LOWEST cost, so I think A the best option. having individual lambdas for each step will increase the complexity and having SQS in between each will increase both complexity and cost","comment_id":"733886"},{"poster":"AjayPrajapati","comment_id":"713221","upvote_count":"2","content":"Selected Answer: A\nEC2 scale in/out is slow. SQS can be slow too and it is asych.","timestamp":"1667844120.0"},{"poster":"Rocketeer","comment_id":"673468","content":"A for me.\nUsing SQS makes it asynch. How do you respond back to the API call ?\nAlso multiple lambdas and SQS will be slower than option A. Step function is a better option.","upvote_count":"4","comments":[{"content":"\"Using SQS makes it asynch. How do you respond back to the API call ?\"\nA good point! This definitely rules option D out.","upvote_count":"1","comment_id":"684167","timestamp":"1664623980.0","poster":"tomosabc1"}],"timestamp":"1663604940.0"},{"upvote_count":"1","poster":"dcdcdc3","content":"D is preferred as it is more robust, with cloud native services etc, but 1000 requests per second with API GW may make this much more expensive, maybe this is why C is proposed","comment_id":"670211","timestamp":"1663267020.0"},{"content":"D is the only choice here","comment_id":"637709","timestamp":"1658883420.0","poster":"hilft","upvote_count":"1"},{"timestamp":"1653469260.0","comment_id":"607128","poster":"bobsmith2000","upvote_count":"1","content":"Selected Answer: D\nNo data loss, full compliance to pci"},{"comment_id":"560059","comments":[{"comment_id":"576082","upvote_count":"1","poster":"czarno","timestamp":"1648371300.0","content":"not only you specifically selected C, then you also went on to comment D... and reference to the previous question.\nOther than that I think it is D... APIGW + Lambda + SQS"}],"timestamp":"1646311920.0","upvote_count":"1","poster":"pal40sg","content":"Selected Answer: C\nD. gp3 can't set IOPS"},{"poster":"cldy","content":"D is correct.","comment_id":"513353","timestamp":"1640867640.0","upvote_count":"1"},{"timestamp":"1639218720.0","content":"D. Create an Amazon API Gateway to process inbound requests using a series of AWS Lambda processes, each with an Amazon SQS input queue. As each step completes, it writes its result to the next stepג€™s queue. The final step returns a JSON object with the approval status. Open a support case to increase the limit for the number of concurrent Lambdas to allow room for bursts of activity due to the new application.","poster":"cldy","comment_id":"499330","upvote_count":"1"},{"comment_id":"490413","timestamp":"1638247920.0","upvote_count":"2","content":"Selected Answer: D\nD, as SQS is also PCI DSS compliance \nhttps://aws.amazon.com/compliance/services-in-scope/","poster":"acloudguru"},{"poster":"AWSum1","comment_id":"450657","timestamp":"1636269600.0","upvote_count":"4","content":"A \n\nMy simple understanding:\nMultiple Lambda functions for each step can add up to 300ms/step"},{"content":"A\n\nWhy not D? The question is asking for the least complex working solution.","comment_id":"441676","timestamp":"1636225980.0","poster":"student22","upvote_count":"1"},{"upvote_count":"1","timestamp":"1636183260.0","content":"It should be 'C' since emphasis is on compliance with PCI-DSS as long as the standard allows the app to be on shared tenants.","poster":"mustafa1p","comment_id":"435669"},{"comment_id":"431202","timestamp":"1636152660.0","content":"i think A is more complex because of all functions are included in one lambda, also it does not offer low cost as it will run all the time, while several lambda they will process the function once and pass to the other lambda. Im more on D then A.","poster":"Kopa","upvote_count":"1"},{"content":"D is wrong - the answer is not mentioning step functions","comment_id":"425210","comments":[{"comment_id":"425211","content":"https://d1.awsstatic.com/whitepapers/compliance/pci-dss-compliance-on-aws.pdf","poster":"DerekKey","comments":[{"poster":"Salmariaz","content":"AWS SQS is PCI complaint\nhttps://aws.amazon.com/compliance/services-in-scope/","timestamp":"1636275660.0","upvote_count":"1","comment_id":"467665"}],"upvote_count":"2","timestamp":"1636145880.0"}],"timestamp":"1636145100.0","upvote_count":"1","poster":"DerekKey"},{"poster":"WhyIronMan","comment_id":"411135","upvote_count":"3","timestamp":"1636117860.0","content":"I'll go with D"},{"content":"D works best which address zero data loss by implementing message queuing. SQS is a pull delivery system, so processing the queue is dependent on the lambda function. \nLambda responses are double digit ms thus could deliver sub-2s responses; require limit increase as multiple lambdas could be running for each request.","poster":"Waiweng","upvote_count":"6","timestamp":"1636070940.0","comment_id":"348232"},{"poster":"blackgamer","upvote_count":"3","timestamp":"1635941880.0","content":"I will go with A as it is less complicated","comment_id":"346716"},{"content":"A - is the correct answer based on the objective of LEAST complexity and LOWEST cost","upvote_count":"3","poster":"Ziegler","comment_id":"330414","timestamp":"1635727740.0"},{"poster":"kiev","content":"I would go for A. It is simple and cost effective. For those going for C, how does multiple lambda and sqs help you with keeping cost down?","comment_id":"294502","timestamp":"1635703200.0","upvote_count":"2"},{"comment_id":"291710","content":"will go with A","timestamp":"1635589980.0","upvote_count":"2","poster":"Kian1"},{"comment_id":"280288","timestamp":"1635533940.0","upvote_count":"8","content":"I go with A, there is no way multiple lambda functions with multiple reads/writes to SQS can return in less than 2 seconds.\nAlso without step functions there is no way to return output to API call when multiple functions are called one after another","poster":"Ebi"},{"comment_id":"259202","timestamp":"1635357660.0","poster":"01037","upvote_count":"1","content":"D.\nThough it is more complex than A, the A can NOT meet the requirement of \"zero data loss\", which is the question is asking for."},{"content":"I am changing my answer to A from D. This is because I am not sure how the workflow creating using a series of Lambda functions would return a response back to API Gateway if SQS is inserted in between the steps implemented by Lambda. Without the use of Step Functions I don't see how a response can be returned to API Gateway synchronously.","comment_id":"257834","timestamp":"1635260700.0","upvote_count":"4","comments":[{"comment_id":"269026","timestamp":"1635385080.0","upvote_count":"2","poster":"Bulti","comments":[{"poster":"certainly","upvote_count":"1","content":"so your Answer is back to D again. which i think is better over A for \"zero data lost\"","comment_id":"332661","timestamp":"1635920520.0"}],"content":"After I went through the link I got an idea on how I could invoke the transactions using Lambda asynchronously but still return a synchronous response from the first Lambda back to API Gateway. https://stackoverflow.com/questions/56374587/invoking-a-step-function-synchronously-from-a-lambda-function#:~:text=3%20Answers&text=AWS%20Step%20Functions%20are%20only,you%20might%20find%20Activities%20useful."}],"poster":"Bulti"},{"poster":"Bulti","upvote_count":"2","comment_id":"252259","content":"Answer is D. The throughput of standard SQS is unlimited. Although the request processing may appear asynchronous, finishing all steps in 2 sec won't be an issue. From the perspective of setting up a solution, A might be most easy but it is not reliable. It will pose issues when operating this solution in terms of handling failures and losing data. Therefore I think D is the right answer. If a DLQ was specified as part of option A, then I would have voted for Option A. But I think the correct answer is D.","timestamp":"1635242580.0"},{"poster":"petebear55","upvote_count":"1","timestamp":"1635193620.0","comment_id":"247564","content":"A is two simplistic despite the question asking for 'least' PCI compliant standards are very strict ... and when it talks about messaging we should look at the cloud alternative to message processing tool known as IBM MQ .. so one should look at d which meets all the requirements and also follows AWS best practice of decoupling processes.."},{"poster":"superbart","timestamp":"1634928180.0","upvote_count":"2","comment_id":"247391","content":"\"a series of AWS Lambda processes, each with an Amazon SQS input queue.\" \nThis means that there is one SQS per Lambda. This allows Standard SQS to be used instead of slower FIFO SQS.\nThe answer is A.","comments":[{"poster":"superbart","timestamp":"1634964360.0","content":"My mistake, Answer is D","comment_id":"247393","upvote_count":"2"}]},{"timestamp":"1634892420.0","comment_id":"243536","poster":"T14102020","upvote_count":"1","content":"Correct is A. one Lambda without SQS = LEAST complexity"},{"comment_id":"241948","content":"The question asks for LEAST complexity so D is not feasible, if you have five steps you would put up 5 lambdas and 5 queues? Answer is A.","upvote_count":"1","poster":"PAUGURU","timestamp":"1634764200.0"},{"poster":"MeepMeep","content":"AAAAAAAAAAAAAA","upvote_count":"1","timestamp":"1634683560.0","comment_id":"238651"},{"timestamp":"1634608440.0","comment_id":"230842","comments":[{"comment_id":"234201","upvote_count":"4","timestamp":"1634671200.0","poster":"jackdryan","content":"On second revision, changing it to D because of the data loss prevention requirement"}],"poster":"jackdryan","content":"I'll go with A","upvote_count":"2"},{"poster":"Edgecrusher77","timestamp":"1634557320.0","content":"Difficult to choose between A & D\nBut as previsously mentionned, decoupling every step in the transaction is the key here, in term of DESIGN, so I will go for D","upvote_count":"2","comment_id":"228468"},{"content":"A & B seems to be valid and equivalent...but A should have the lowest cost (B cluster is running all day)\n\nD for me it has no sense... it is an authorization request scenario synchronous (2 seconds)... if the request is queued with SQS through the API the response of the API will just include an HTTP 202 accepted without the approval response, that should be sent later to the api cient in some how...","comment_id":"183035","upvote_count":"3","timestamp":"1634546820.0","poster":"ipindado2020"},{"comment_id":"175476","timestamp":"1634512320.0","content":"A\nFor D, adding the SQS chain, you are adding the additional latency","poster":"Ganfeng","upvote_count":"2"},{"content":"D is correct, each transaction has multiple step, each step return json authorization request within 2 sec, no data loss.","timestamp":"1634394780.0","comment_id":"151834","poster":"fullaws","upvote_count":"2"},{"content":"I would go with A for least complexity, which is hard requirement\nWhile SQS in D seems to add durability, it is not exactly like this. If some step of the lambda will fail, it will fail also in D, between the SQS steps. But having multiple steps, lambda invocations, SQS (queuing means potential waits)... makes it challenging to meet the 2 sec requirement \nAlso it is possible to make the Lambda from A to persist the request at the beginning in a Dynamo for example, and have a process looking for orphaned Dynamo entries just like one would do with SQS dead letter queues","upvote_count":"1","comment_id":"143300","timestamp":"1634382000.0","poster":"MultiAZ"},{"upvote_count":"4","timestamp":"1634344140.0","poster":"inf","comments":[{"content":"Coding notes:\n- write each Lambda function to check timestamp - if submission timestamp > 2s ago, send a failed JSON response. \n- write lambda function to keep the lambda instances warm (or 2019+ use provisioned concurrency) to process faster\n- if all Lambda code was in a single Lambda, and that Lambda failed, we could lose that data. By storing it in a queue its protected till its processed\n\nThis is my take - may be wrong, but A seems too easy.","upvote_count":"2","timestamp":"1634372340.0","poster":"inf","comment_id":"136930"}],"content":"Answer: D\nNotes:\n- an authorisation response, successful or *unsuccessful*, response must occur within 2s.\n- EC2 instances without redundant storage cannot prevent data loss\n- SQS data is redundant across multiple SQS servers\n\nA - incorrect - data loss possible, also Lambda already supports 1000 RPS, thus no need to increase for this app.\nB - incorrect - data loss possible, may also not send a response if the instance crashes\nC - incorrect - data loss possible, may also not send a response if the instance crashes\nD - correct - SQS for redundancy, Lambda responses are double digit ms thus could deliver sub-2s responses; require limit increase as multiple lambdas could be running for each request.","comment_id":"136926"},{"timestamp":"1634248380.0","upvote_count":"1","poster":"NikkyDicky","comment_id":"133181","content":"D more likely. nothing in the question indicated each step taking a long time, so there is no reason to believe multiple Lambda calls will take over 2s."},{"upvote_count":"6","content":"Between A & D -> D is better\nThere are multiple steps to each transaction and every step needs a response .\nAssume there are n steps and if we put every step in single lamda , and failure occurs in (n-1)th step , again the lamda has to repeat from start which might overshoot 2sec eta.\nSince anyway every transaction needs a response , we can persist the response (which is input to next step) and if any failure occurs , restart from that step instead of from the start.\nI think decoupling every step in the transaction is the key here","comment_id":"108245","timestamp":"1634082600.0","poster":"JohnyGaddar"},{"content":"i think it is A, since lambda is a function why add extra steps by putting the result of one function into an SQS queue for the next lambda to pick it up and perform its steps. Why not just program one lambda to do everything?","comment_id":"107475","poster":"oatif","upvote_count":"1","timestamp":"1633900800.0"},{"timestamp":"1633761000.0","upvote_count":"2","content":"I am inclining towards D, becuase the requirement is \"Every request must receive a response\" and SQS delivers it.","comments":[{"comment_id":"107450","timestamp":"1633874640.0","upvote_count":"1","poster":"Ibranthovic","content":"But by using multiple Lambda, it may not finish each request within 2 seconds."},{"content":"that's the very reason why A works best.","comment_id":"107526","upvote_count":"1","timestamp":"1634028540.0","comments":[{"comment_id":"110428","timestamp":"1634195940.0","poster":"meenu2225","upvote_count":"1","content":"Yes, that's correct, A does looks like a better option."}],"poster":"Wira"}],"poster":"meenu2225","comment_id":"106366"},{"poster":"NKnab","upvote_count":"1","timestamp":"1633722360.0","content":"A is the correct answer. Lambda does steps sequentially.","comment_id":"102151"},{"comment_id":"98600","timestamp":"1633561500.0","poster":"JAWS1600","upvote_count":"1","content":"D Will not finish in 2 seconds. \nA is better choice. \nBC, are too costly - Dedicated and ELB"},{"comments":[{"timestamp":"1633358400.0","comment_id":"95136","upvote_count":"1","content":"It is a credit card application not a bank debit or credit so duplication is not as catastrophic","poster":"RogerRabbit"}],"poster":"AShahine21","timestamp":"1633285320.0","upvote_count":"3","content":"B is a perfect solution - but it is the LEAST complexity and LOWEST cost\nSo it is between A and D. The problem of D is that \nSQS standard: Duplication ( repeat a bank transaction multiple time ???!) - Not acceptable\nSQS FIFO : 300mes/sec ( the peak load is around 1000 per second) - Not acceptable\nSo the right answer is A","comment_id":"94749"},{"upvote_count":"1","comment_id":"94388","content":"I like A because of its simplicity however the Lambda function itself would be pretty complex And there is no mention of managing state or storage throughout the function thus in my mind putting the data more at risk and making troubleshooting more difficult. D on the other hand protects the data better using q's and decouple all the processing thus making the entire process less complex.....My only concern is with all the lambda funtions and q's....completing the process within 2 seconds? I think A has a better chance of meeting that requirement but D does the rest of it more effectively.","poster":"Merlin1","timestamp":"1633245720.0"},{"upvote_count":"5","content":"I was inclining towards D because of message persistency; however multiple lambdas = multiples cold starts = more than 2 seconds. Option B & C is not least complex or cost-effective. Option A works fine.","timestamp":"1633149960.0","poster":"Smart","comment_id":"77082"},{"comment_id":"57370","content":"PCI-DSS requires encryption at rest and transit (among many other things), so any solution which doesn't offer this, is off: SQS, ELB, Lambda , API Gateway and EC2 allow this. To allow for ZERO data loss, we need to store the data in all its stages into reliable persistent storage (EC2 can die, so that's a No-Go), so that leaves only A and D...Out of these two I'd choose D over A as it documents data in each stage. SQS having Dead Letter Queues is just fine - in case data couldn't be properly processed, it's archived there.","comments":[{"comments":[{"content":"The following link shows SQS is PCI compliant.\nhttps://aws.amazon.com/compliance/services-in-scope/\nI prefer D also","upvote_count":"1","comment_id":"62631","poster":"paulwang","timestamp":"1632887580.0"},{"poster":"virtual","timestamp":"1633053360.0","content":"SQS is pci-dss compliant: (see pci tab)\nhttps://aws.amazon.com/compliance/services-in-scope/","upvote_count":"1","comment_id":"63436"},{"timestamp":"1633055760.0","content":"https://aws.amazon.com/compliance/services-in-scope/\nhttps://aws.amazon.com/blogs/security/aws-adds-16-more-services-to-its-pci-dss-compliance-program/\n\nSQS is PCI-DSS compliance server","comment_id":"63485","poster":"flyingoncloud","upvote_count":"3"}],"content":"option D has an SQS queue though, which is not PCI-DSS compliant, based on your comment above.","upvote_count":"1","timestamp":"1632756780.0","poster":"n1ch0las","comment_id":"61304"},{"upvote_count":"1","timestamp":"1633388880.0","content":"All these are PCI compliant.\nhttps://aws.amazon.com/compliance/services-in-scope/","comment_id":"98598","poster":"JAWS1600"},{"timestamp":"1635066000.0","poster":"petebear55","content":"correct ... A will lead to bottleneck and does not meet the aws best practice of decoupling processes","comment_id":"247559","upvote_count":"1"}],"timestamp":"1632748560.0","upvote_count":"6","poster":"MrP"},{"upvote_count":"1","timestamp":"1632745380.0","content":"Should be A","comment_id":"51685","poster":"amog"},{"comments":[{"upvote_count":"1","content":"Two much risk of bottleneck with this","timestamp":"1634984400.0","poster":"petebear55","comment_id":"247555"}],"content":"A is better","comment_id":"12659","upvote_count":"3","poster":"awsec2","timestamp":"1632363660.0"},{"comment_id":"11148","poster":"huhupai","timestamp":"1632335940.0","content":"Does D meet response less than 2 seconds requirement?","comments":[{"poster":"ipindado2020","timestamp":"1634539620.0","upvote_count":"1","content":"\"The entire request must return an authorization response\"...\nusings sqs will be asynchronous.","comment_id":"183032"}],"upvote_count":"2"}],"topic":"1","answer_ET":"A","exam_id":32,"question_id":491},{"id":"rpeQlgNMBJMlRMYLEG88","answer":"A","question_text":"A Solutions Architect is migrating a 10 TB PostgreSQL database to Amazon RDS for PostgreSQL. The company's internet link is 50 MB with a VPN in the\nAmazon VPC, and the Solutions Architect needs to migrate the data and synchronize the changes before the cutover. The cutover must take place within an 8-day period.\nWhat is the LEAST complex method of migrating the database securely and reliably?","isMC":true,"answer_images":[],"answer_description":"","discussion":[{"content":"Answer is A.\nB: Not possible. Because transferring 10TB over 50Mbps will take 17 days at least.\nC: Use DMS to copy not database dump.\nD: You don’t need SCT since there is no need for conversion.\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.Process.html","timestamp":"1632084600.0","comment_id":"11485","upvote_count":"27","comments":[{"upvote_count":"11","poster":"94xychen","comment_id":"111161","comments":[{"timestamp":"1645500840.0","content":"Question clearly states \"50 megabits per second\" which is Mbps. MBps is mega bytes per second.\n\nScary how people are testing for a cert like this, and don't even know the difference","upvote_count":"4","poster":"futen0326","comment_id":"553412","comments":[{"comment_id":"697470","upvote_count":"1","timestamp":"1666017360.0","content":"But the question does say \"MB\":\n\"The company's internet link is 50 MB with a VPN in the\nAmazon VPC...\"","poster":"redipa"}]}],"content":"B: It's 10TB over 50MB... not 50Mbps","timestamp":"1633885740.0"},{"poster":"mostafasookar","content":"A is right \nMegabits per second (Mbps) so it will take 17 days IF it is Megabytes per second (MBps) so B it will work","timestamp":"1650171480.0","upvote_count":"1","comment_id":"587055"},{"poster":"Amac1979","timestamp":"1678216140.0","content":"https://aws.amazon.com/blogs/storage/enable-large-scale-database-migrations-with-aws-dms-and-aws-snowball/","upvote_count":"1","comment_id":"832215"}],"poster":"donathon"},{"upvote_count":"15","comments":[{"content":"It will take 17.6606 days using that tool, NOT 2 days","comments":[{"timestamp":"1632947580.0","comment_id":"31759","upvote_count":"2","comments":[{"comment_id":"568537","upvote_count":"1","timestamp":"1647364080.0","content":"it's 50 Mb, not 50MB, if you check one more time.","poster":"syscao"},{"upvote_count":"7","poster":"thirstylion","content":"Its Mbps (megabits) so 17 days.","comment_id":"479132","timestamp":"1637030460.0"}],"content":"I checked again, the speed is 50 MB not 50 Mb so it will take 2. Please try again.","poster":"9Ow30"},{"upvote_count":"2","poster":"9Ow30","comments":[{"timestamp":"1633090620.0","comment_id":"32567","content":"Ahh, it's 2.20758 days. Correct. \nSo answer is B","poster":"cinopi","upvote_count":"5"}],"timestamp":"1633002960.0","content":"The company's internet link is 50 MB with a VPN","comment_id":"31760"}],"poster":"cinopi","upvote_count":"4","timestamp":"1632759960.0","comment_id":"30718"},{"timestamp":"1656248400.0","content":"MB is not the same as Mb\nB is for Byte\nb is for bit","poster":"robsonchirara","comment_id":"622558","upvote_count":"2"}],"comment_id":"29999","timestamp":"1632734700.0","poster":"9Ow30","content":"B\nUsing the calculator here http://www.calctool.org/CALC/prof/computing/transfer_time it will take 2 days to transfer 10TB over 50MB line."},{"content":"Selected Answer: B\nA is wrong cause DMS can't load data from S3. DMS is the simplest way here and can transfer 10TB within 3days as calculated below.","timestamp":"1702500840.0","comment_id":"1095818","poster":"DavidC","upvote_count":"1"},{"timestamp":"1693153020.0","comment_id":"991563","upvote_count":"1","poster":"TravelKo","content":"You can't use AWS DMS from S3. So A is ruled out. Option - B is the right answer you can set it with minimum effort. 50MBX60X60= ~180GBX24/Per day"},{"poster":"romiao106","timestamp":"1681679400.0","content":"Agree with donation. A is the answer","upvote_count":"1","comment_id":"872190"},{"comment_id":"720072","upvote_count":"1","content":"Internet is measured in bit per second NOT Bytes per second. Don't know why the question would say MegaBytes...","timestamp":"1668639900.0","poster":"desertlotus1211"},{"upvote_count":"1","timestamp":"1667980260.0","poster":"whuzzup","content":"Selected Answer: A\nA. Snowball copies existing data + additional synchronization","comment_id":"714368"},{"poster":"AjayPrajapati","comment_id":"713245","timestamp":"1667846700.0","content":"Selected Answer: A\nA is right. Snowball can combine with DMS and S3 for faster migration. SCT is not required because it is like to like DB","upvote_count":"1"},{"poster":"mnizamu","timestamp":"1667175960.0","comment_id":"708093","upvote_count":"2","content":"It says very clearly that \"The company's internet link is 50 MB with a VPN.\" Therefore, transferring 10TB of data over a 50 MB link will take 2 days 7 hours 33 mins 20 sec. Therefore, the answer should be B. Calculation: 8 bits/1 byte x 50 Megabytes/s = 400 Megabits/s"},{"timestamp":"1666712520.0","poster":"kharakbeer","content":"Selected Answer: A\nThe internet speed is calculated by bitpersecond and NOT Bytepersecond. B is wrong the answer is A as you don't need SCT when migrating from on-prem postgreSQL to AWS PostgreSQL","comment_id":"703995","upvote_count":"1"},{"timestamp":"1666093080.0","upvote_count":"1","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.html","comment_id":"698176","poster":"Vizz5585"},{"upvote_count":"1","comment_id":"693208","timestamp":"1665588720.0","poster":"joanneli77","content":"50 MB is not 50 mb. The author wrote the question wrong. One is 800% different from the other. Remember networks are measured in 'b'its not 'B'ytes, even though almost all other data is capital B. In either case, data transfer will take too long."},{"timestamp":"1665047520.0","upvote_count":"2","poster":"JohnPi","comment_id":"687633","content":"Selected Answer: D\nWhen you're using an Edge device, the data migration process has the following stages:\n\n-You use the AWS Schema Conversion Tool (AWS SCT) to extract the data locally and move it to an Edge device.\n-You ship the Edge device or devices back to AWS.\n-After AWS receives your shipment, the Edge device automatically loads its data into an Amazon S3 bucket.\n-AWS DMS takes the files and migrates the data to the target data store. If you are using change data capture (CDC), those updates are written to the Amazon S3 bucket and then applied to the target data store."},{"content":"Selected Answer: A\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.Process.html","comment_id":"687426","poster":"[Removed]","upvote_count":"1","timestamp":"1665028200.0"},{"poster":"tomosabc1","content":"Selected Answer: B\nThe answer is B.\nWe can figure out the answer by ruling out the wrong ones.\nA,D(wrong): Neither AWS DMS nor AWS Schema Conversion Tool can be used to copying on premise DB to Showball device.\nhttps://aws.amazon.com/dms/schema-conversion-tool/\nC(wrong): Log Shipping is for SQL server on EC2, rather than AWS RDS for PostgreSQL. \nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/migration-sql-server/ec2-log-shipping.html\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_SQLServer.html#SQLServer.Concepts.General.FeatureNonSupport","comments":[{"content":"You are wrong , DMS can be used with snowball to migrate databases from on-premise to AWS\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.html","timestamp":"1667149860.0","poster":"Cal88","comment_id":"707934","upvote_count":"1"}],"timestamp":"1664640180.0","comment_id":"684334","upvote_count":"1"},{"poster":"dcdcdc3","timestamp":"1663268340.0","content":"MB means nothing really. If it is MBps then B is correct, If it is Mbps D is correct (Use SCT):\nhttps://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_DMSIntegration.html\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.html","comments":[{"poster":"Cal88","timestamp":"1667149920.0","content":"Why do you need schema conversion if you are migrating to the same DB\nI think answer A is correct Snowball + DMS","comment_id":"707936","upvote_count":"1"}],"comment_id":"670237","upvote_count":"2"},{"upvote_count":"1","poster":"jerrykid","comment_id":"648759","content":"C is correct. DMS now support S3\nThe only way to transfer data into Snowball is dump data and copy by Snowball agent, not DMS or SCT. Both DMS and SCT is service and hosted in AWS.","timestamp":"1660889100.0"},{"upvote_count":"1","comment_id":"625648","timestamp":"1656663600.0","poster":"she1989","content":"Answer is D: https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_Source.PostgreSQL.html"},{"poster":"azurehunter","upvote_count":"3","content":"Answer is D\nA is wrong because DMS is cloud service and it cannot extract database offline. It requires VPN or DX for data migration and replication. So, the only solution is to use SCT to extract the data and schemas prior to copy to Snowball.","comment_id":"614262","timestamp":"1654819560.0"},{"upvote_count":"6","comment_id":"553437","poster":"johnnsmith","timestamp":"1645503180.0","content":"D is the correct answer. https://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.htmlWhen you're using an Edge device, the data migration process has the following stages:\n\nYou use the AWS Schema Conversion Tool (AWS SCT) to extract the data locally and move it to an Edge device.\nYou ship the Edge device or devices back to AWS.\nAfter AWS receives your shipment, the Edge device automatically loads its data into an Amazon S3 bucket.\nAWS DMS takes the files and migrates the data to the target data store. If you are using change data capture (CDC), those updates are written to the Amazon S3 bucket and then applied to the target data store."},{"poster":"AMKazi","timestamp":"1644451380.0","comment_id":"544173","content":"Ans is B least complication way. \nCalculation: 50 mbps is 50x60 seconds =3000 mb per minute.\n3000 x 60 minutes = 180000 mb per hour\n18000 x 24 hours = 4320000 mb per day = 4.32 Terabyte per day\nso in 8 days you can transfer 4.32 x 8 = 34.56 terabytes","upvote_count":"2"},{"upvote_count":"1","timestamp":"1643910900.0","content":"Selected Answer: A\nA. megabits = Mb","comment_id":"539870","poster":"zoliv"},{"comment_id":"531879","content":"B is not reliable, anything can happen anytime also speeds are not consistent. For the exam as well, we should use AWS resources as much as possible :P. A seems much more reliable than B.","poster":"cannottellname","upvote_count":"1","timestamp":"1643088660.0"},{"comment_id":"526029","content":"interesting. I tired out the calculator and its showing as 17 days. So the answer is A. The confusion is the connection speed. it clearly states '50 megabits per second' VPN connection. There is only one to read this I see","poster":"Ni_yot","upvote_count":"1","timestamp":"1642447260.0"},{"comment_id":"515797","upvote_count":"1","content":"50 Megabits =\n6.25 Megabytes / 6.25MB per second . 10TB need 19 days to finish transfer. So A is right.","poster":"GeniusMikeLiu","timestamp":"1641217560.0"},{"comment_id":"495082","upvote_count":"1","timestamp":"1638789780.0","content":"Answer is A. \n10 TB for 8 days not possible with 50mbps vpn SCT is not needed besides its only for schema conversion not copying data","poster":"KiraguJohn"},{"poster":"AzureDP900","comments":[{"timestamp":"1640288040.0","upvote_count":"1","comment_id":"508108","poster":"AzureDP900","content":"changing my answer as B."}],"content":"I'll go with D","timestamp":"1638736200.0","upvote_count":"2","comment_id":"494638"},{"upvote_count":"1","poster":"student22","timestamp":"1636151760.0","content":"B\n---\nNo need of snowball because there's a 50MB link (not 50Mbps)","comment_id":"457554"},{"comment_id":"450665","upvote_count":"2","timestamp":"1636136640.0","content":"Stuck between B and D \n\nSnowball edge because \nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.html\n\nBut it take between 4 - 6 days to receive a edge device. Then you still need to copy data, send it back to AWS and sync\n\nB \nBecause well if its 50mbps and not 50mb its going to take just over 2 days","poster":"AWSum1"},{"poster":"nodogoshi","comment_id":"449850","upvote_count":"2","content":"B. 50 MB connecttion enough for dms migration.","timestamp":"1636060080.0"},{"content":"If 50 MB speed, then the answer is B. If 50 Mbps , then A.","comment_id":"435409","poster":"blackgamer","timestamp":"1635963780.0","upvote_count":"2"},{"comments":[{"timestamp":"1636015920.0","upvote_count":"1","content":"Changing to A","poster":"denccc","comment_id":"436461"}],"timestamp":"1635929460.0","content":"D: read the steps here: https://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.html","upvote_count":"2","comment_id":"426474","poster":"denccc"},{"timestamp":"1635852060.0","upvote_count":"4","poster":"DerekKey","comment_id":"425227","content":"A is wrong - DMS can only use Snowball Edge\nB should be correct - 50MB broadband connection (50MB/sec)\nC is wrong - \"the LEAST complex method\" and \"log shipping to synchronize changes\"\nD is wrong - SCT works only with schema. Alone it can not \"copy the database\""},{"comment_id":"415701","timestamp":"1635837300.0","content":"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.Limitations.html\n\nIn some cases, an error can occur when loading from the local database to the Edge device or when loading data from Amazon S3 to the target database. In some of these cases, the error is recoverable and the task can restart. If AWS DMS can't recover from the error, the migration stops. If this happens, contact AWS Support.\n\nSince we only have 8 days, there is no room for much troubleshooting. Wondering if Snowball solution is risky in this case.","poster":"TiredDad","upvote_count":"1"},{"timestamp":"1635805140.0","content":"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.Limitations.html\n\nIn some cases, an error can occur when loading from the local database to the Edge device or when loading data from Amazon S3 to the target database. In some of these cases, the error is recoverable and the task can restart. If AWS DMS can't recover from the error, the migration stops. If this happens, contact AWS Support.","upvote_count":"1","comment_id":"415699","poster":"TiredDad"},{"comment_id":"414030","poster":"student2020","timestamp":"1635801600.0","content":"D\nWhen you're using an Edge device, the data migration process has the following stages:\n1. You use the AWS Schema Conversion Tool (AWS SCT) to extract the data locally and move it to an Edge device.\n2. You ship the Edge device or devices back to AWS.\n3. After AWS receives your shipment, the Edge device automatically loads its data into an Amazon S3 bucket.\n4. AWS DMS takes the files and migrates the data to the target data store. If you are using change data\ncapture (CDC), those updates are written to the Amazon S3 bucket and then applied to the target datastore.\npg 534 https://docs.aws.amazon.com/dms/latest/userguide/dmsug.pdf#CHAP_LargeDBs.SBS","upvote_count":"4"},{"upvote_count":"3","content":"I'll go with D\n\nWhen you're using an Edge device, the data migration process has the following stages:\n\nYou use the AWS Schema Conversion Tool (AWS SCT) to extract the data locally and move it to an Edge device.\n\nYou ship the Edge device or devices back to AWS.\n\nAfter AWS receives your shipment, the Edge device automatically loads its data into an Amazon S3 bucket.\n\nAWS DMS takes the files and migrates the data to the target data store. If you are using change data capture (CDC), those updates are written to the Amazon S3 bucket and then applied to the target data store.\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.html","comment_id":"411140","timestamp":"1635719760.0","poster":"WhyIronMan"},{"timestamp":"1635697800.0","upvote_count":"2","comment_id":"410493","content":"Depends.. what is a 50 MB line? 50 Mbps then A, if 50 MB/s, then B","poster":"jobe42"},{"upvote_count":"1","comment_id":"382567","content":"is VPN secure and reliable for such huge amount of data?","timestamp":"1635676980.0","poster":"SheldonHofstadter"},{"comment_id":"377152","poster":"pradhyumna","content":"my choice is D. While 50 MB is good to transfer the DB with out a SnowBall. it may not be a reliable process. DMS appears to have far more limitations and SCT must be part of the solution to ensure a reliable and consistent db migration https://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html#CHAP_BestPractices.OnGoingReplication","timestamp":"1635661980.0","upvote_count":"2"},{"timestamp":"1635614580.0","poster":"Waiweng","comment_id":"348237","content":"it's A","upvote_count":"1"},{"timestamp":"1635553020.0","poster":"kiev","content":"We are over doing this. Transfer of 10TB with snowball? Hmm, B is the answer.","comment_id":"294511","upvote_count":"1"},{"upvote_count":"1","comment_id":"291724","timestamp":"1635524460.0","poster":"Kian1","content":"will go with A"},{"poster":"Ebi","comment_id":"279658","upvote_count":"5","timestamp":"1635516540.0","content":"Key in here is LEAST complex method, using Snowball is fairly complex specially when is used with DMS\nWith 50MB internet transferring 10TB takes a bit more than 2 days, \nI will go with B"},{"poster":"shammous","upvote_count":"1","timestamp":"1635468780.0","content":"It should be B:\nThe VPN connection (50MBps) should allow the transfer of 10 TB in ~53h.\nDMS will insurance both the transfer and synchronization and would be the LEAST COMPLEX option (Ref: https://aws.amazon.com/getting-started/hands-on/move-to-managed/migrate-postgresql-to-amazon-rds/)","comment_id":"279076"},{"upvote_count":"1","poster":"gookseang","comment_id":"278600","timestamp":"1635466200.0","content":"should be A"},{"poster":"newme","content":"A or B (if it's really 50MBps)\nSnowball (Edge) + DMS has to be a better choice than Snowball only, though more expensive.\nI believe \"AWS Snowball device\" here includes both the Snowball and the Snowball Edge.\nBecause the title is \"AWS Snowball Device Differences\" in below reference.\nhttps://docs.aws.amazon.com/snowball/latest/ug/device-differences.html","timestamp":"1635285600.0","upvote_count":"2","comments":[{"comment_id":"259815","upvote_count":"2","poster":"01037","comments":[{"content":"AWS Schema Converstion Tool (SCT) supports a range of database and data warehouse conversions which are listed here. Note that SCT can be used to:\n\nCopy a database schema from a source to a target\nConvert a database or data warehouse schema\nAnalyze a database to determine the conversion complexity\nAnalyze a database to determine any possible restrictions to running on Amazon RDS\nAnalyze a database to determine if a license downgrade is possible\nConvert embedded SQL code in an application\nMigrate data warehouse data to Amazon Redshift\n\nhttps://aws.amazon.com/dms/faqs/","timestamp":"1635427800.0","comment_id":"276837","poster":"rcher","upvote_count":"1"}],"content":"Maybe D or B.\nAfter reading\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.html\nit seems, AWS SCT is always needed to work with DMS.","timestamp":"1635369180.0"}],"comment_id":"259395"},{"upvote_count":"2","comment_id":"258023","content":"B Should be the answer\n\nIt will take 2.2069 days to transfer 10TB over 50MB link, to be precise it takes 3178.91 minutes","poster":"Santya","timestamp":"1635210960.0"},{"timestamp":"1635209700.0","upvote_count":"1","poster":"Bulti","content":"Now assuming they actually meant \"Snowball Edge\" as opposed to \"Snowball\", among the remaining options, C can be easily discarded. AWS documentation on SCT suggests that you still need SCT to convert from PostGresSQL to RDS PostGresSQL as mentioned here-> https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_Source.PostgreSQL.html and also with Snowball Edge you need both DMS and SCT to migrate the data into the Snowball Edge device as mentioned here->https://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.Process.html and therefore I think D is the right answer.","comment_id":"252270"},{"poster":"Bulti","timestamp":"1635089160.0","upvote_count":"2","comment_id":"252269","content":"This is the most misleading question I have found so far. The fact that it mentions 50MB as opposed to 50Mbps which is a standard way of describing an internet bandwidth is confusing and on top of that the use of word \"Snowball\" instead of \"Snowball Edge\" which is the right service to use for Large Database migration in conjuction with SCT and DMS also confuses me as well. If I were to chose between the 2 lessor evils i.e. which one is definite error on their part I would go with 50MB vs 50Mbps. And therefore I will rule out B straightaway."},{"upvote_count":"1","poster":"petebear55","timestamp":"1635083040.0","comment_id":"247584","content":"its D .. but the question should probably have mentioned snowball edge rather tahn snowball https://aws.amazon.com/blogs/database/new-aws-dms-and-aws-snowball-integration-enables-mass-database-migrations-and-migrations-of-large-databases/"},{"comment_id":"231278","timestamp":"1634999160.0","upvote_count":"1","content":"Answer is A or D:\nA: states use AWS DMS, this is good\nB: states use AWS Schemal Conversion Tool to copy the database, this is not possible. Schema conversion only converts the DB schema not copy data.\nA is answer","poster":"SamuelK"},{"comments":[{"poster":"jackdryan","timestamp":"1635058560.0","content":"On second thought changing to A.","upvote_count":"4","comment_id":"234215"}],"upvote_count":"3","poster":"jackdryan","timestamp":"1634909100.0","content":"I'll go with D","comment_id":"230850"},{"poster":"kopper2019","timestamp":"1634895060.0","content":"if 50 means 500mb link yeah B is ok but if it is refers to 50 link A is the answer","upvote_count":"2","comment_id":"215543"},{"timestamp":"1634885040.0","comment_id":"210282","poster":"abhi123489","upvote_count":"3","content":"B it should be. 50MB is more than enough to move 10TB. The solution will be simple as well. \nPer min = 50*60 = 3000 MB (3GB)\nPer Hour = 3000 * 60 = 180000 MB (180GB)\nPer Day = 180 GB * 24 = 4320 GB (4.3 TB)"},{"comments":[{"timestamp":"1634856000.0","poster":"AlwaysLearning2020","upvote_count":"1","comment_id":"197481","content":"Or network bandwidth ;-)"}],"timestamp":"1634719560.0","comment_id":"197480","content":"The one who set the question has properly never ordered a Internet link before when telcom- lingo said 50-Meg, they are refering to 50Mbps, 50 Megabits per sec, NOT 50 Megabytes per sec. Bandwidth are sold in Kbps, Mbps, Gbps, NOT KB, MB, GB, this are Amazon used for Data transfer, not telecom-bandwidth.","upvote_count":"1","poster":"AlwaysLearning2020"},{"poster":"AlwaysLearning2020","timestamp":"1634659260.0","comment_id":"197469","content":"Assuming network efficiency is 100%, a 50Mbps link and for 8 days, it can only transfer a max an approx. 4.12TB. ;-)","upvote_count":"1"},{"content":"does migration over VPN (B) meet the reliable requirement?","timestamp":"1634610840.0","poster":"cthd","upvote_count":"2","comment_id":"183781"},{"comment_id":"183040","timestamp":"1634601840.0","poster":"ipindado2020","upvote_count":"1","content":"B is the way...\n10TB -> 50MB/S = 2,2 days\n\nYou will not get 100% of performance of the max data transfer rate... but seems reasonable that 25% will be available for this task (just with the 25% will be below 8 days)."},{"poster":"Jaihanuman","timestamp":"1634599500.0","content":"How to access the aws questions : this link is broken plz help https://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-professional/","comment_id":"181298","upvote_count":"1"},{"poster":"Krish812003","upvote_count":"1","timestamp":"1634584020.0","comment_id":"180916","content":"As mentioned by \"9Ow30\" from the same link he provided to calculate the duration, the answer is B. It is MB not Mbps, so if you calculate 10TB of Data with 50MB it takes 2.20758 days"},{"timestamp":"1634546700.0","poster":"fullaws","content":"A is correct, no need aws sct","upvote_count":"1","comment_id":"158551"},{"poster":"fullaws","content":"D is correct, SCT help to copy on-premise schemas to rds with same db engine. Beside that RDS do not support log shipping. https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_Welcome.html","timestamp":"1634529420.0","comment_id":"151898","upvote_count":"1"},{"timestamp":"1634518200.0","upvote_count":"1","comment_id":"145275","poster":"Anila_Dhharisi","content":"As its 10TB size of data and requirement is least complex method of migration, it should be migrated using Snowball. So option A , C & D are related to Snowball. D cannot be chosen as we do have RDS PostgreSQL database. SCT is used when there is no availability of database supported by RDS. Now its between A & C. In the question, they're using VPN which needs to be synchronized with the log . So option C is correct."},{"timestamp":"1634390760.0","content":"For everyone saying snowball does not support DMS:\n\nhttps://aws.amazon.com/about-aws/whats-new/2017/11/aws-database-migration-service-adds-support-for-aws-snowball/#:~:text=AWS%20Database%20Migration%20Service%20Adds%20Support%20for%20AWS%20Snowball,-Posted%20On%3A%20Nov&text=AWS%20Database%20Migration%20Service%20(DMS,petabyte%2Dscale%20data%20transport%20solution.\n\nSnowball is not the way to go any more. It is Snowball Edge that AWS provides. Also, only major difference between snowball and snowball edge is the compute capability on Edge.\n\nB it is.","comment_id":"141152","poster":"IAmNotLambda","comments":[{"poster":"sweand","content":"There is no snowball mentioned in B. The 50 MB speed might be type, as conventional speed will be 50 Mbps. So snowball needed here.","upvote_count":"1","timestamp":"1634422680.0","comment_id":"143803"}],"upvote_count":"1"},{"timestamp":"1634278440.0","poster":"sweand","content":"Its C. A & D are incorrect because snowball dont support DMS/SCT but snowball edge does. \nB is correct depending on internet speed. The conventional speed will be in Mbps not in MB.","upvote_count":"1","comment_id":"139110"},{"content":"C acceptable,,,??","comment_id":"138384","timestamp":"1634235960.0","upvote_count":"1","poster":"noisonnoiton"},{"content":"Part 2:\nAnswer: D\nA - incorrect - SCT must be used when migrating databases from on-prem using a Snowball device. \nB - incorrect - VPN over the internet would not be considered \"reliable\", and (maybe, to a lesser extent) based on other exam questions, also can't be considered secure over the Internet \nC - incorrect - log shipping is not possible from an on-prem (non-rds) database to Postgresql RDS. If it were, it would be complicated vs DMS with SCT. \nD - correct - reliable, secure, SCT required when using DMS with Snowball devices as the data transport medium.","comments":[{"upvote_count":"2","poster":"khksoma","timestamp":"1634382960.0","comment_id":"140205","content":"I agree..\nhttps://aws.amazon.com/blogs/storage/enable-large-scale-database-migrations-with-aws-dms-and-aws-snowball/"}],"comment_id":"137200","timestamp":"1634212320.0","poster":"inf","upvote_count":"1"},{"comments":[{"upvote_count":"1","poster":"AlwaysLearning2020","content":"50MB? Per what? Per second, per hour or per day?","comment_id":"197482","timestamp":"1634879880.0"}],"poster":"inf","upvote_count":"1","timestamp":"1634189820.0","comment_id":"137197","content":"Answer: D\n(hesitantly, please prove me wrong,)\nAssumptions\n- 50MB is 50MB and not 50Mbps\n- Snowball devices aka Snowball Edge takes approximately 7 days to ship from your DC to AWS and have data uploaded. Cutting it fine but doable\n- Transfer reliably and securely - not something you can say about a VPN over the Internet.\n- Snowball is the service, a Snowball device is Snowball Edge\n- DMS with a Snowball device must use the SCT for the migration (https://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.html)\n- We need DMS (CDC) or another 3rd party app to do a live sync from on-premise to RDS PostgreSQL - log shipping is not an option (if I'm wrong please post a link)"},{"poster":"NikkyDicky","comment_id":"133198","upvote_count":"1","timestamp":"1634096160.0","content":"C for homogenous data migration AWS recommends native tools (https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-on-premises-postgresql-database-to-amazon-rds-for-postgresql.html)","comments":[{"comment_id":"134704","upvote_count":"1","content":"Link you refer shows DMS so B","comments":[{"poster":"NikkyDicky","comment_id":"135890","content":"for those too lazy to read the whole document, here is the excerpt I was referring to:\n\"We recommend that you use native PostgreSQL database migration tools under the following conditions: you have a homogeneous migration, where you are migrating from a database with the same database engine as the target database; you are migrating an entire database; and the native tools allow you to migrate your system with minimal downtime. \"","timestamp":"1634177580.0","upvote_count":"1"}],"timestamp":"1634120880.0","poster":"sami777"}]},{"upvote_count":"1","poster":"oatif","timestamp":"1634090040.0","content":"the answer is B, as it is the fastest sol'n.","comment_id":"130306"},{"content":"With 50MB. Answer is B.\nhttp://www.calctool.org/CALC/prof/computing/transfer_time","timestamp":"1634037900.0","upvote_count":"4","comment_id":"114328","poster":"roger8978"},{"comments":[{"upvote_count":"2","comment_id":"137179","timestamp":"1634183880.0","content":"The terminology \"Snowball device\" implies a \"Snowball Edge device\". They are one in the same given there is no other device other than a Snowball Edge . \n\"AWS Snowball now refers to the service overall, and Snowball Edge are the current types of devices that the service uses – sometimes referred to generically as AWS Snowball devices\"\nhttps://aws.amazon.com/snowball/faqs/#:~:text=Snowball%20Edge%20is%20an%20edge,for%20use%20in%20edge%20locations.\nAnswer: D","poster":"inf"}],"upvote_count":"1","timestamp":"1633859640.0","poster":"Merlin1","comment_id":"106223","content":"Exactly... if it’s snowball edge!"},{"upvote_count":"1","comment_id":"105368","timestamp":"1633773120.0","poster":"sunilrch","content":"D is the correct Answer"},{"poster":"Oleksandr","comment_id":"104263","content":"Answer is D, and it is described exactly step-by-step here: https://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.html","timestamp":"1633738740.0","upvote_count":"4"},{"poster":"Merlin1","timestamp":"1633674360.0","content":"There are issues with this ? Is it 50MB or 50Mb? That changes the answer....also is it SnowBall or Snowball Ede? Its not listed in the answers otherwise D would be correct. SCT IS used to extract postgre data to Snowball Edge...It seems to be that this is the answer the question is built for....However Edge is not mentioned which invalidates D. And of course again the 50MB or 50Mb effects the answer as well....Guess only the correct wording can answer this one for sure. Seems like they are looking for D, but if that's the case then there are issues with the questions syntax.","upvote_count":"1","comment_id":"97680"},{"content":"Whoever is saying D, he should re-check his information.\nThere is no need for SCT, as the client is migration PostgreSQL to PostgreSQL!\nA is not a good answer, You need Snowball edge.\nit is between B and C, and honestly, I'm not able to understand why we should know the internet speed/size calculation for this exam without having a calculator!\nB - the calculation shows around 2 days, However, is it reliable to use VPN connection for 10TB data transfer? I'm not sure.\n\nC is a complete solution and respect all requirements.","comment_id":"94760","timestamp":"1633656960.0","poster":"AShahine21","upvote_count":"6"},{"timestamp":"1633655520.0","comment_id":"88736","content":"I prefer C.\n\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-on-premises-postgresql-database-to-amazon-rds-for-postgresql.html\n\nno SCT is needed and DMS is more complex compared to 'native data dump'\n\nAlso;\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/PostgreSQL.Procedural.Importing.html\n\"We recommend that you use native PostgreSQL database migration tools under the following conditions:\nYou have a homogeneous migration, where you are migrating from a database with the same database engine as the target database.\nYou are migrating an entire database.\nThe native tools allow you to migrate your system with minimal downtime.\"","upvote_count":"2","poster":"[Removed]"},{"content":"Answer is B. 50MB is not 50mbps.","upvote_count":"2","poster":"fw","comment_id":"85761","timestamp":"1633651440.0"},{"poster":"koalasy","upvote_count":"1","timestamp":"1633634220.0","content":"agree answer is B. we can not judge TYPO. \nleast complex method is B. and no schema conversion is needed as it is posgresQL to RDS posgresSql. \nall the other methods are complex, they do not meet the requirement.","comment_id":"84371"},{"upvote_count":"2","timestamp":"1633581540.0","poster":"Joeylee","comment_id":"76715","content":"Answer should be D. SCT is required to extract data from source DB.\nB is incorrect. \n1, Internet speed 50MB looks like a typo. Convention is 50Mb.\n2, using DMS is 2 more stages - full load and ongoing replication."},{"upvote_count":"6","content":"Looking at most of the comments, I think B is most appropriate. \n\nA & D is out because regular Snowball Device is not supported by DMS. D is completely out as we don't need SCT (used for heterogenous migration) at all here. With assumption that it is 50 MB (not 50 Mb), B will work out nicely - looking for Least Complex solution. If it is 50 Mb (not 50 MB), C will work as B will take much longer. \n\nNow, if it were Snowball 'Edge' Device for options A & D, it still wouldn't change anything. Option D - SCT is still not needed. Option A works however, once database is loaded and available in S3, the replication task is automatic from there on - no need to load in RDS exclusively. Option B & C won't rely on Snowball Edge Device, the conclusion would be same from above paragraph.","timestamp":"1633508460.0","poster":"Smart","comment_id":"71478"},{"comment_id":"64257","poster":"Jshuen","content":"Answer should be D, the below link explain the detail process flow, SCT is required for using Snowball Edge,\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.Process.html","timestamp":"1633499760.0","upvote_count":"1"},{"upvote_count":"2","timestamp":"1633476120.0","comment_id":"53534","content":"I Agree with @Moon, it's C. \"Be sure to request a Snowball Edge device (Snowball Edge Storage Optimized), because regular Snowball devices are not supported for AWS DMS. \"","poster":"SamuelK"},{"upvote_count":"2","comment_id":"51688","timestamp":"1633469460.0","content":"Should be B","poster":"amog"},{"timestamp":"1633450140.0","poster":"M2","content":"answer is A\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.html\nhttps://aws.amazon.com/blogs/database/new-aws-dms-and-aws-snowball-integration-enables-mass-database-migrations-and-migrations-of-large-databases/","upvote_count":"2","comment_id":"48468"},{"comment_id":"46234","content":"Answer B.\n1. VPN safe\n2. It will take two days to transfer data ( Bit Vs Byte) also easy to transfer\n3. Snowball will take time to get and send back.","upvote_count":"1","poster":"ashp","timestamp":"1633412280.0"},{"poster":"PacoDerek","comment_id":"43580","timestamp":"1633239720.0","upvote_count":"1","content":"should be C\nA/D using DMS or SCT to land data on S3, which are not possible. cause the only way u can do with a snowball is snowball client.\nB: its too long time\nhttps://docs.aws.amazon.com/snowball/latest/ug/using-device.html#snowball-data-transfer"},{"comment_id":"42712","poster":"AWSPro24","comments":[{"poster":"sweand","timestamp":"1634235660.0","comment_id":"137566","content":"Snow ball edge does but in options we dont have snowball edge. i think its C.","upvote_count":"1"}],"timestamp":"1633195020.0","upvote_count":"4","content":"I believe the answer is D. It appears SCT actually does the copying. \n\"You use the AWS Schema Conversion Tool (AWS SCT) to extract the data locally and move it to an Edge device.\" (https://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.html)\n\nWhile the DMS agent acts as the DBs mirror or clone you might say\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.Process.html"},{"timestamp":"1633101180.0","poster":"siasiasia","comment_id":"37308","content":"it says LEAST complex method, and the bandwidth allows it (ofcourse if it is 50 MB/s) so B","upvote_count":"2"},{"content":"C is the answer","timestamp":"1632836160.0","poster":"dojo","comment_id":"30743","upvote_count":"4"},{"comment_id":"19343","comments":[{"upvote_count":"2","content":"Can't be B as the question mentioned only 8 days of cut over. Using B will take 10 days assuming the link is 100% use for replication and no other production load running.","timestamp":"1632596520.0","poster":"skywalker","comment_id":"19664"},{"poster":"all_past","upvote_count":"1","content":"34,560,000 mb is not 34TB, 8 bits = 1 Byte, it will be only 4,320,000 MB.","comments":[{"comments":[{"poster":"94xychen","upvote_count":"1","content":"It's still 50MB.","timestamp":"1633987560.0","comment_id":"111162"}],"upvote_count":"4","timestamp":"1632693180.0","poster":"all_past","content":"Whoops, while I posted this message, I saw it's 50MB in the question, then seems this answer is possible.","comment_id":"21924"}],"timestamp":"1632644280.0","comment_id":"21923"},{"poster":"AlwaysLearning2020","timestamp":"1634702100.0","upvote_count":"1","comment_id":"197476","content":"50Mbps = 50 x 1024 x 1024 = 52,428,800 bits/sec\n = 52,428,800 / 8 = 6,553,600 bytes/sec\n = 6,553,600 / 1024 = 6,400 KB/sec\n = 6,400 / 1024 = 6.25 MB/sec\n = 6.25MB * 60 * 60 * 24 * 8 = 4,320,000 MB\n = 4,320,000 / 1024 / 1024 = 4.12 TB for 8 days"}],"timestamp":"1632469800.0","content":"B should be possible 50 mbps * 60 * 60 * 24 * 8 = 34,560,000 mb in 8 days aprox 34 TB only need 10. Therefore B","poster":"Warrenn","upvote_count":"6"},{"comments":[{"comments":[{"content":"but in the question, it not mention about Aurora.","comment_id":"174021","poster":"Phat","upvote_count":"1","timestamp":"1634576520.0"}],"upvote_count":"2","content":"Answer D","timestamp":"1632436980.0","comment_id":"19247","poster":"skywalker"}],"content":"https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_Welcome.html\n\nCheck up the table with source and target DB..\n\n\nYou can migrate schema and data from MySQL to an Aurora MySQL DB cluster without using AWS SCT. For more information… But for Postgres to Aurora PostgresDB you need SCT prior DMS.","poster":"skywalker","upvote_count":"1","timestamp":"1632403500.0","comment_id":"19246"},{"upvote_count":"6","content":"Answer is D, https://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.html","poster":"huhupai","timestamp":"1632241800.0","comment_id":"17524"},{"upvote_count":"1","timestamp":"1632181980.0","content":"why not D:\nThe process of using AWS DMS and AWS Snowball Edge incorporates both on-premises applications and Amazon-managed services. We use the terms local and remote to distinguish these components.\n\nLocal components include the following:\n\nAWS SCT\n\nAWS DMS Agent (a local version of AWS DMS that works on-premises)\n\nSnowball Edge devices\n\nRemote components include the following:\n\nAmazon S3\n\nAWS DMS","poster":"chaudh","comments":[{"poster":"AWS2020","upvote_count":"2","timestamp":"1632259200.0","comment_id":"18189","content":"I am not sure about this. While the steps are correct but the the word \" Snowball Edge\" is not mentioned. So I think the Answer is C.","comments":[{"comment_id":"18195","poster":"AWS2020","timestamp":"1632261120.0","content":"Also, SCT used to convert PostgreSQL to Amazon Aurora, MySQL","upvote_count":"1"}]}],"comment_id":"16241"},{"poster":"Moon","timestamp":"1632163200.0","upvote_count":"5","comment_id":"13288","content":"Answer is \"C\".\nReferring to AWS: \"Be sure to request a Snowball Edge device (Snowball Edge Storage Optimized), because regular Snowball devices are not supported for AWS DMS. \"\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.SBS.create-snowball-job.html"},{"comment_id":"12550","timestamp":"1632153660.0","poster":"SivaG","upvote_count":"6","content":"It Seems AWS snow ball edge will only support for DMS , not just snowball...isn't the answer C"},{"upvote_count":"3","content":"a should be","comment_id":"11020","timestamp":"1632083940.0","poster":"awsec2"}],"question_id":492,"question_images":[],"topic":"1","answers_community":["A (50%)","D (30%)","B (20%)"],"url":"https://www.examtopics.com/discussions/amazon/view/5148-exam-aws-certified-solutions-architect-professional-topic-1/","timestamp":"2019-09-13 20:03:00","answer_ET":"A","exam_id":32,"choices":{"C":"Order an AWS Snowball device and copy a database dump to the device. After the data has been copied to Amazon S3, import it to the Amazon RDS instance. Set up log shipping over a VPN to synchronize changes before the cutover.","A":"Order an AWS Snowball device and copy the database using the AWS DMS. When the database is available in Amazon S3, use AWS DMS to load it to Amazon RDS, and configure a job to synchronize changes before the cutover.","D":"Order an AWS Snowball device and copy the database by using the AWS Schema Conversion Tool. When the data is available in Amazon S3, use AWS DMS to load it to Amazon RDS, and configure a job to synchronize changes before the cutover.","B":"Create an AWS DMS job to continuously replicate the data from on premises to AWS. Cutover to Amazon RDS after the data is synchronized."},"unix_timestamp":1568397780},{"id":"XzJGHz9s3KqHPohxAHg7","answer_description":"Reference:\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html","question_images":[],"timestamp":"2019-09-10 11:00:00","unix_timestamp":1568106000,"answer":"B","question_text":"A Solutions Architect must update an application environment within AWS Elastic Beanstalk using a blue/green deployment methodology. The Solutions Architect creates an environment that is identical to the existing application environment and deploys the application to the new environment.\nWhat should be done next to complete the update?","url":"https://www.examtopics.com/discussions/amazon/view/4996-exam-aws-certified-solutions-architect-professional-topic-1/","question_id":493,"discussion":[{"upvote_count":"23","comment_id":"10420","timestamp":"1632320820.0","poster":"awsec2","content":"b. https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html"},{"comment_id":"759830","timestamp":"1672231860.0","poster":"evargasbrz","content":"Selected Answer: B\nit's B","upvote_count":"1"},{"poster":"cldy","upvote_count":"2","comment_id":"496788","timestamp":"1638964740.0","content":"B. Select the Swap Environment URLs option"},{"timestamp":"1638736260.0","comment_id":"494640","poster":"AzureDP900","content":"I'll go with B","upvote_count":"1"},{"upvote_count":"2","timestamp":"1635840780.0","content":"I'll go with B","poster":"WhyIronMan","comment_id":"411142"},{"comment_id":"371360","content":"The Answer is B, https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/swap-the-environment-of-an-elastic-beanstalk-application.html","poster":"zolthar_z","upvote_count":"2","timestamp":"1635696720.0"},{"upvote_count":"1","timestamp":"1635492780.0","content":"it's B","comment_id":"348238","poster":"Waiweng"},{"content":"B for sure","upvote_count":"1","timestamp":"1634986620.0","comment_id":"316508","poster":"awsexamprep47"},{"poster":"Kian1","comment_id":"291725","upvote_count":"2","content":"going with B","timestamp":"1634859060.0"},{"poster":"Ebi","content":"B is the answer","upvote_count":"3","timestamp":"1634660220.0","comment_id":"279659"},{"comments":[{"content":"That's the method for changing the CNAME entry if you have full control over the environment. In EB you can't change the CNAME record yourself, EB controls that. You have to tell EB what environment it should point the CNAME record to. That is done by changing the FQDN (technically not a URL, but AWS still calls it a URL) for the environment:\n\nhttps://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/swap-the-environment-of-an-elastic-beanstalk-application.html","poster":"kirrim","upvote_count":"2","timestamp":"1636276440.0","comment_id":"459705"}],"comment_id":"279098","content":"answer is D : \nhttps://medium.com/@kumargaurav1247/blue-green-deployment-introduction-68b01d471dde","poster":"tipzzz","upvote_count":"1","timestamp":"1634651040.0"},{"timestamp":"1634649600.0","comment_id":"252274","poster":"Bulti","upvote_count":"1","content":"Answer is B. You need to swap Environment URLs"},{"content":"B : When an application is developed and deployed to an AWS Elastic Beanstalk environment, having two separate, but identical, environments—blue and green—increases availability and reduces risk. In this Quick Start architecture, the blue environment is the production environment that normally handles live traffic. The CI/CD pipeline architecture creates a clone (green) of the live Elastic Beanstalk environment (blue). The pipeline then swaps the URLs between the two environments.","poster":"spring21","timestamp":"1634635680.0","comment_id":"245013","upvote_count":"3"},{"comment_id":"243542","timestamp":"1634626020.0","poster":"T14102020","upvote_count":"1","content":"Correct is B. Swap Environment URLs"},{"content":"I'll go with B","poster":"jackdryan","upvote_count":"2","comment_id":"230852","timestamp":"1634566200.0"},{"comments":[{"content":"B is correct.","comment_id":"177455","upvote_count":"1","poster":"Phat","timestamp":"1634412360.0"},{"upvote_count":"1","comment_id":"247593","timestamp":"1634647800.0","content":"despite everyone disagreeing with us i'm inclined to go a,long with u","poster":"petebear55"}],"timestamp":"1634334660.0","poster":"AWSKrish","upvote_count":"3","content":"D: Please note it is B/G deployment and once Updating DNS is saffice. Wondering SWAP would do traffic in 2 directions once new env is ready that is not needed,","comment_id":"152161"},{"upvote_count":"1","comment_id":"151917","content":"B is correct","poster":"fullaws","timestamp":"1634034240.0"},{"upvote_count":"1","poster":"NikkyDicky","timestamp":"1633902360.0","comment_id":"133201","content":"B for sure"},{"content":"Answer is B: I have done this many times","upvote_count":"1","comment_id":"104265","timestamp":"1633410300.0","poster":"Oleksandr"},{"content":"B is correct","poster":"FreeSwan","comment_id":"94672","upvote_count":"1","timestamp":"1633251480.0"},{"content":"Agree B","timestamp":"1632767280.0","upvote_count":"1","poster":"amog","comment_id":"51690"},{"upvote_count":"2","timestamp":"1632702180.0","comment_id":"44450","content":"B is correct","poster":"dumma"},{"timestamp":"1632666600.0","upvote_count":"2","comment_id":"30744","content":"B is the answer","poster":"dojo"},{"content":"@awsec2, Yes,I agree with you totally.I also found the the same link as you.","upvote_count":"1","comment_id":"10545","timestamp":"1632399720.0","poster":"One_picese"}],"topic":"1","exam_id":32,"choices":{"C":"Replace the Auto Scaling launch configuration","B":"Select the Swap Environment URLs option","A":"Redirect to the new environment using Amazon Route 53","D":"Update the DNS records to point to the green environment"},"answer_ET":"B","answer_images":[],"isMC":true,"answers_community":["B (100%)"]},{"id":"nOtXLWleeQx1uRPBznOg","answers_community":["D (80%)","A (20%)"],"answer":"D","choices":{"C":"Install Logstash on servers, send logs to Amazon S3 and use Amazon Athena to identify errors, use sendmail to notify the Operations team of errors.","B":"Install an AWS X-Ray agent on servers, send logs to AWS Lambda and analyze them to identify errors, use Amazon CloudWatch Events to notify the Operations team of errors.","A":"Install Amazon Kinesis Agent on servers, send logs to Amazon Kinesis Data Streams and use Amazon Kinesis Data Analytics to identify errors, create an Amazon CloudWatch alarm to notify the Operations team of errors","D":"Install the Amazon CloudWatch agent on servers, send logs to Amazon CloudWatch Logs and use metric filters to identify errors, create a CloudWatch alarm to notify the Operations team of errors."},"unix_timestamp":1568770380,"answer_description":"","answer_ET":"D","question_text":"A company has a legacy application running on servers on premises. To increase the application's reliability, the company wants to gain actionable insights using application logs. A Solutions Architect has been given following requirements for the solution:\n✑ Aggregate logs using AWS.\n✑ Automate log analysis for errors.\n✑ Notify the Operations team when errors go beyond a specified threshold.\nWhat solution meets the requirements?","isMC":true,"timestamp":"2019-09-18 03:33:00","question_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/5338-exam-aws-certified-solutions-architect-professional-topic-1/","exam_id":32,"answer_images":[],"question_id":494,"discussion":[{"poster":"donathon","content":"D\nA: Amazon Kinesis Data Analytics used for data analytics.\nB: Cannot be implemented on premise.\nC: Athena is servers SQL based query system. Should use SNS instead of sendmail.\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html","timestamp":"1632169740.0","comment_id":"11496","upvote_count":"25","comments":[{"upvote_count":"1","poster":"AWSPro24","comment_id":"42689","timestamp":"1632553500.0","content":"I agree with A. It says \"Log Analysis\" not \"Log Analytics\" CloudWatch Logs can do the job. https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html","comments":[{"poster":"AWSPro24","comments":[{"upvote_count":"3","content":"Interesting. It seems that CloudWatch Logs is even built on top of Kinesis https://forums.aws.amazon.com/thread.jspa?threadID=157966","timestamp":"1632583620.0","poster":"AWSPro24","comment_id":"42694"},{"content":"Here is what A is missing. Collecting the logs from on-prem servers and sending to kinesis. Option A does not provide solution for that piece.","upvote_count":"2","timestamp":"1633733880.0","poster":"JAWS1600","comment_id":"98572"},{"upvote_count":"2","comment_id":"409037","content":"The key thing is \"on premises\". There is a Kinesis Agent for Windows, which runs on-prem. Since there is no mention of server type, so, it is safe to assume that the kinesis agent can't be installed on those servers. But CloudWatch agents can be installed both on most OS.","timestamp":"1635848760.0","poster":"DashL"}],"comment_id":"42691","timestamp":"1632570000.0","content":"Sorry I meant D. Also the CloudWatch landing page includes the term \"actionable insights\" \n https://aws.amazon.com/cloudwatch/\n\nI don't see any reason why A would not work but it seems like overkill for just error counting.","upvote_count":"2"}]},{"comment_id":"18211","timestamp":"1632350160.0","upvote_count":"3","comments":[{"poster":"LunchTime","content":"This is obviously a close call between options A and D. Both appear to fulfill the requirements. However, AWS2020 makes a great point – what if the error is not specified in the error filter? Consequently, option D may not report on some errors. Also, slides 17 and 18 in the following AWS presentation would lead me to believe “A” is the answer they are looking for on the exam. https://www.slideshare.net/AmazonWebServices/realtime-application-monitoring-with-amazon-kinesis-and-amazon-cloudwatch-aws-online-tech-talks","comment_id":"125070","timestamp":"1634102460.0","upvote_count":"4"}],"poster":"AWS2020","content":"I think the Answer is A. D may work if we only need to identify the error not analyze it and you would need to put error filter such as 400x, 500x. What happens if we have error tat is not specified in metric filter"}]},{"timestamp":"1632174360.0","comments":[{"timestamp":"1635813300.0","comment_id":"330391","poster":"SD13","comments":[{"content":"Kinesis agent can forward logs to AWS cloudwatch as per info at this blog\nhttps://aws.amazon.com/blogs/big-data/collect-parse-transform-and-stream-windows-events-logs-and-metrics-using-amazon-kinesis-agent-for-microsoft-windows/","poster":"Student1950","upvote_count":"1","timestamp":"1644004680.0","comment_id":"540621"}],"upvote_count":"4","content":"Kinesis agent can not forward logs to cloudwatch, how the cloudwatch alarm will be triggered? option A is missing this part"}],"comment_id":"13285","poster":"Moon","content":"I would for with A.\nhttps://docs.aws.amazon.com/kinesis-agent-windows/latest/userguide/what-is-kinesis-agent-windows.html\nhttps://medium.com/@khandelwal12nidhi/build-log-analytic-solution-on-aws-cc62a70057b2","upvote_count":"14"},{"comment_id":"930868","poster":"SkyZeroZx","upvote_count":"2","timestamp":"1687456440.0","content":"Selected Answer: D\nD\nA: Amazon Kinesis Data Analytics used for data analytics.\nB: Cannot be implemented on premise.\nC: Athena is servers SQL based query system. Should use SNS instead of sendmail.\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html"},{"timestamp":"1669998900.0","upvote_count":"2","comment_id":"733914","poster":"timmysixstrings","content":"Selected Answer: D\nI think the answer is D, even though A would work too. \nA is a more complex and Kinesis Data Analytics is built for Analytics. while it might be possible to repurpose this for automated log analysis, it's not ideal. CloudWatch Metrics is purpose built for automated log analysis."},{"comment_id":"713258","poster":"AjayPrajapati","content":"Selected Answer: A\nA - its a legacy app and log format might not be something that cloud watch would detect things as \"error\". A, you have flexibility to define more logic and do more \"Automation\"","timestamp":"1667847600.0","upvote_count":"1"},{"upvote_count":"2","content":"The answer is D:\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html","timestamp":"1659375060.0","poster":"ibrahimsow","comment_id":"640783"},{"poster":"RVivek","comment_id":"544641","content":"D.\nThe key is \"Notify the Operations team when errors go beyond a specified threshold\" \nThat says \" metric filters \" and a defined threshold","timestamp":"1644505380.0","upvote_count":"3"},{"poster":"Student1950","content":"As per the following AWS blog from 2018, correct answer seems to be A\nhttps://aws.amazon.com/blogs/big-data/collect-parse-transform-and-stream-windows-events-logs-and-metrics-using-amazon-kinesis-agent-for-microsoft-windows/","timestamp":"1644004560.0","comment_id":"540620","upvote_count":"1"},{"timestamp":"1638736380.0","upvote_count":"2","content":"I'll go with D","comment_id":"494642","poster":"AzureDP900"},{"content":"D. Install the Amazon CloudWatch agent on servers, send logs to Amazon CloudWatch Logs and use metric filters to identify errors, create a CloudWatch alarm to notify the Operations team of errors.","comment_id":"494162","upvote_count":"1","poster":"cldy","timestamp":"1638693060.0"},{"comment_id":"491482","poster":"AzureDP900","timestamp":"1638351360.0","content":"D is right, there is no need of kinesis data analytics for this .","upvote_count":"2"},{"comment_id":"436464","poster":"denccc","content":"I go for D","timestamp":"1636060860.0","upvote_count":"3"},{"poster":"blackgamer","upvote_count":"1","content":"I think A is better suited than D. \nCloudwatch agent can only install on Amazon Linux 2. Since this is legacy applicaion, I am assuming they are on different OS. Kinesis agent can install Redhat Linux 7 , so it is more reasonable here.","timestamp":"1635970200.0","comment_id":"421625"},{"poster":"WhyIronMan","upvote_count":"2","content":"I'll go with D","timestamp":"1635908280.0","comment_id":"411143"},{"poster":"Waiweng","content":"it's D","comment_id":"348243","upvote_count":"3","timestamp":"1635831120.0"},{"content":"D is the answer.\nCW Agent can be installed on On-Prem servers","timestamp":"1635747900.0","upvote_count":"1","poster":"awsexamprep47","comment_id":"316510"},{"comment_id":"291729","poster":"Kian1","content":"going with D","upvote_count":"2","timestamp":"1635649260.0"},{"comments":[{"comments":[{"poster":"Rocketeer","upvote_count":"1","timestamp":"1661901540.0","content":"Part of the requirement is to do aggregations which can be done in kinesis data analytics. Hence I am leaning towards A.","comment_id":"654510"}],"poster":"Ebi","content":"A is not the answer, first of all we don't need real-time, Kinesis is a very good use case of real time log analysis, second we don't need Kinesis Analytics, only automation is required is capturing errors which can be done using CW metric, lastly Kinesis agent although can be installed on-premise server but has more limited OS support compared to CW agent, so for a legacy app is not a good choice:\n\nfor Kinesis agent: \"Your operating system must be either Amazon Linux AMI with version 2015.09 or later, or Red Hat Enterprise Linux version 7 or later.\"\nhttps://docs.aws.amazon.com/streams/latest/dev/writing-with-agents.html#download-install\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html","timestamp":"1635588960.0","comment_id":"286697","upvote_count":"8"}],"timestamp":"1635586800.0","comment_id":"279667","upvote_count":"5","content":"D is the answer","poster":"Ebi"},{"content":"go D ~","upvote_count":"1","poster":"gookseang","timestamp":"1635581400.0","comment_id":"278608"},{"upvote_count":"1","poster":"01037","content":"D.\nBoth A and D meet the requirement, but D is simpler so is better.","timestamp":"1635448680.0","comment_id":"259869"},{"timestamp":"1635423420.0","comment_id":"252282","content":"Between A and D, I think the correct answer is A. This is a typically Data Analytics Specialty exam question as well. Kinesis Data stream for log aggregation and Kinesis Data Analytics for Log analysis and Cloud Watch for alerting checks all boxes. Always look for Big data Analytics services in the options offered for these questions.","poster":"Bulti","upvote_count":"3"},{"poster":"petebear55","timestamp":"1635349380.0","upvote_count":"1","comment_id":"247601","content":"A .. when it mentions automation .. you should look at kinesis ... see how shifty Amazon are throwing the usual Red herring in the answers by putting D in there"},{"poster":"T14102020","upvote_count":"1","comments":[{"poster":"consultsk","comment_id":"253566","timestamp":"1635432060.0","content":"Exclude Kinesis? - I agree with what @Bulti mentioned. @Moon also said correctly. I go with 'A'.","upvote_count":"1"}],"content":"Correct is D. CloudWatch agent + Without Kinesis","comment_id":"243546","timestamp":"1635249960.0"},{"poster":"jackdryan","content":"I'll go with D*","timestamp":"1635181200.0","upvote_count":"4","comment_id":"230864"},{"timestamp":"1635122100.0","content":"D is the answer!\nA: Kinesis agent send logs to cloudwatch !! \n“ cloudwatch.emitMetrics \nEnables the agent to emit metrics to CloudWatch if set (true).\n\nDefault: true\n“","upvote_count":"2","comment_id":"215559","poster":"smartassX"},{"content":"https://aws.amazon.com/blogs/big-data/implement-serverless-log-analytics-using-amazon-kinesis-analytics/ this makes me to choose A","timestamp":"1635052920.0","comment_id":"212389","upvote_count":"1","poster":"Sagardonthineni"},{"upvote_count":"1","poster":"sam422","timestamp":"1634914500.0","content":"I go with D, how do we send application logs on-premise to kinesis? D make sense","comment_id":"188004"},{"poster":"proxyolism","upvote_count":"1","content":"I thought first D as answer but I changed my mind to A.\nD cannot analyze errors. A only can.\n\nand kinesis agent canbe run to other os like linux.\nhttps://docs.aws.amazon.com/streams/latest/dev/writing-with-agents.html\n\nand it can run on on-premise too.\nhttps://github.com/awslabs/amazon-kinesis-agent","timestamp":"1634878800.0","comment_id":"183507","comments":[{"comment_id":"188006","upvote_count":"1","content":"Did you see kinesis for on premise in any aws documents?","poster":"sam422","timestamp":"1635002820.0"}]},{"comments":[{"poster":"Phat","upvote_count":"1","timestamp":"1634787420.0","comment_id":"174035","content":"can or cannot? with your comment, I think you refer D.","comments":[{"content":"D\nKinesis Agent does support OS other than windows, but the legacy application is where I'm concerned. If I had to chose, I would go with D.","upvote_count":"2","comment_id":"179330","poster":"Nkem","timestamp":"1634825460.0"}]}],"content":"A\nYou can't install Kinesis Agent on a legacy machine. It is also only available for Windows.","poster":"Nkem","upvote_count":"1","comment_id":"169958","timestamp":"1634776260.0"},{"content":"A is correct, Actionable insight is the point.... https://aws.amazon.com/kinesis/data-analytics/","comment_id":"151933","upvote_count":"1","poster":"fullaws","timestamp":"1634711100.0"},{"timestamp":"1634530500.0","content":"I think it is A, because CloudWatch-Agent support more operating systems.\nhttps://docs.aws.amazon.com/zh_cn/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html","comments":[{"timestamp":"1634684700.0","content":"typo, it is D.","comment_id":"137903","upvote_count":"1","poster":"ricoyao"}],"comment_id":"137902","poster":"ricoyao","upvote_count":"1"},{"content":"Answer: A\nVery close between A & D - both can achieve logging and alerting, but KDA can provide insight\nA - correct - does everything we need. Logs are aggregated, notifications sent. Importantly it analyses/queries logs\nB - incorrect - doesn't make sense\nC - incorrect - Athena isn't automated - requires some external service/code/pipeline. sendmail isn't an AWS service - doesn't state how to alert then invoke sendmail\nD - incorrect - doesn't analyse the logs, only searches for known patterns. Provides no insight into the legacy application.\n\nDifferences between A and D\n1. Kinesis Data Analytics can query and analyse the logs, whereas Cloudwatch metrics look for known patterns - limited\n2. Cloudwatch Agent is compatible with more legacy O/S's. Kinesis Agent for Linux supports RHEL7 and AWS Linux AMI, Kinesis Agent for Windows supports older Windows O/S running .NET 4.6 e.g. 2008 R2+. Question states \"legacy application\", not legacy \"o/s\" - thus we can disregard point 2. Too obscure.","comment_id":"137558","poster":"inf","upvote_count":"3","timestamp":"1634306220.0"},{"comment_id":"133205","poster":"NikkyDicky","timestamp":"1634147100.0","content":"D. close call with A, but there is nothing in A to indicate HOW CloudWatch alarm gets generated from kinesis analytics output","upvote_count":"2"},{"upvote_count":"2","timestamp":"1634028240.0","comment_id":"107964","poster":"JAWS1600","content":"Support D\n Option A is aggregating but not storing the aggregated logs anywhere."},{"comment_id":"107465","poster":"oatif","content":"D for me since cloud watch's whole purpose is to analyze and report on instances and their data. Where is Kinesis is for data streaming, collection and analysis.","upvote_count":"1","timestamp":"1633856580.0"},{"poster":"Oleksandr","content":"I think it's A. https://d0.awsstatic.com/Projects/P4113850/aws-projects_build-log-analytics-solution.pdf","upvote_count":"1","timestamp":"1633787640.0","comment_id":"104271"},{"poster":"RogerRabbit","content":"Answer should be D. For A, the logs are sent to Kinesis. There is no discussion of how the logs will get to CloudWatch in order to set an alarm. In order for A to be correct there needs to be a Lambda function or something that sends the errors to CloudWatch after analysis.","comment_id":"94994","upvote_count":"2","timestamp":"1633705740.0","comments":[{"upvote_count":"3","poster":"RogerRabbit","timestamp":"1634070180.0","content":"Correcting myself... Kinesis Data Analytics data is automatically sent to CloudWatch.\nhttps://docs.aws.amazon.com/kinesisanalytics/latest/dev/monitoring-cloudwatch.html","comment_id":"114832"}]},{"timestamp":"1633657200.0","content":"A\nhttps://aws.amazon.com/kinesis/data-analytics/\none of application is for log analytics","comment_id":"82101","poster":"AmazonAu","upvote_count":"1"},{"comments":[{"content":"Not saying D is wrong, but Kinesis Agent for Windows is available for RHEL7/Amazon Linux .","upvote_count":"1","poster":"inf","timestamp":"1634276520.0","comment_id":"137545"}],"timestamp":"1633363920.0","comment_id":"79947","poster":"fw","content":"I'll go with D. For A, kinesis agent is only available for Windows, while Cloudwatch agent is available for both Windows & Linux. We don't know what OS this application server is on.","upvote_count":"1"},{"comment_id":"76729","poster":"Joeylee","timestamp":"1633234140.0","upvote_count":"1","content":"I will go with A. D is very suspicious as the server is on premise with unknown type, CloudWatch agent installation is in question."},{"timestamp":"1633231800.0","content":"D satisfies all the requirements with low cost and ease to setup.\nWhy choose A when there isn't any requirement for real-time analysis?","poster":"AdamSmith","upvote_count":"4","comment_id":"72792"},{"poster":"virtual","comment_id":"63513","timestamp":"1633086780.0","content":"Although D seems to be a classic working solution using Cloudwatch, I think the spirit of the question is response A with several Kinesis features (Agent, streams, analytics).","comments":[{"timestamp":"1633094160.0","content":"I agree that D is classic option (that's my choice). I also agree option A works well too (https://aws.amazon.com/solutions/real-time-insights-account-activity/\nhttps://d1.awsstatic.com/amazonkinesis/Product-Page-Diagram_Kinesis-Data-Analytics-Real-Time-Log-Analytics.d577a64060cc1e594c3c5c4a66feb1cc6e26a397.png)\n\nHowever, what about log retention? Do we need long-term log retention? For what purpose? Just error notifications? Logs aggregated through Kinesis Data Stream can be retained maximally for 7 days. CloudWatch Logs can be retained infinitely. \n\nIf Kinesis were dumping logs into S3 at the time, it would be optimal for real-time monitoring (which is not requested as well). I would go for Option D.","upvote_count":"5","poster":"Smart","comment_id":"71497"}],"upvote_count":"1"},{"timestamp":"1632990060.0","content":"It is B for me.\nTo run the X-Ray daemon locally, on-premises, or on other AWS services, download it from Amazon S3, run it, and then give it permission to upload segment documents to X-Ray.\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-daemon.html","comment_id":"61056","comments":[{"content":"sorry, its D\nB sends logs to lambda (?)","upvote_count":"1","comment_id":"61058","timestamp":"1633054680.0","poster":"mandrakenet"},{"upvote_count":"1","comment_id":"247596","content":"And for me .. i and you will be going against the grain ... but aws are shits and put answers like d in to throw you off as they are so obvious ... but aws are not that generous and would not give such an easy answer ... its B","poster":"petebear55","timestamp":"1635256680.0"}],"poster":"mandrakenet","upvote_count":"2"},{"comment_id":"53434","upvote_count":"1","content":"Sorry, I mean A provides the required automation of log analysis for errors","timestamp":"1632868800.0","poster":"Gorha"},{"comment_id":"53431","timestamp":"1632797220.0","content":"D provides the required automation of log analysis for errors","upvote_count":"1","poster":"Gorha"},{"poster":"M2","timestamp":"1632719280.0","upvote_count":"3","comment_id":"48462","content":"ok so here is the catch. its not realtime streaming app so answer is D. if it was realtime streaming then A is better"},{"upvote_count":"2","comment_id":"44451","timestamp":"1632639960.0","poster":"dumma","content":"D is correct"},{"poster":"VMHarry","upvote_count":"1","timestamp":"1632431220.0","content":"if D, how to notify the Operations team when errors go beyond a specified threshold","comment_id":"32952","comments":[{"upvote_count":"2","comment_id":"37309","timestamp":"1632500580.0","content":"Cloudwatch Alarm and SNS","poster":"siasiasia"}]},{"comment_id":"27890","timestamp":"1632390660.0","upvote_count":"2","poster":"Scunningham99","content":"agree with a"},{"poster":"Scunningham99","content":"agree with a","comment_id":"27889","upvote_count":"2","timestamp":"1632360840.0"},{"content":"I go with A. Kinesis Data Analytics is used to analyze log data flow, and can aggregate data.","upvote_count":"2","timestamp":"1632268860.0","poster":"larryaws","comment_id":"17616"},{"timestamp":"1632173520.0","poster":"awsdog","upvote_count":"3","content":"Automate log analysis for errors. -> need error Analytics \ni choose \"A\"","comment_id":"13059"}]},{"id":"QxoBsO0oBdWQufIjDRnA","discussion":[{"upvote_count":"11","timestamp":"1635120540.0","content":"B and D","comment_id":"348250","poster":"Waiweng"},{"content":"I go with B and D\n\n\"AWS Shield Standard automatically protects your Amazon Route 53 Hosted Zones from infrastructure layer DDoS attacks\"\nhttps://aws.amazon.com/shield/?nc1=h_ls&whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc\n\n\"AWS WAF can be deployed on Amazon CloudFront, the Application Load Balancer (ALB), Amazon API Gateway, and AWS AppSync.\"\nhttps://aws.amazon.com/waf/faqs/","upvote_count":"7","poster":"CarisB","comment_id":"327273","timestamp":"1633972320.0"},{"poster":"evargasbrz","content":"Selected Answer: BD\nB and D","timestamp":"1672232340.0","upvote_count":"1","comment_id":"759844"},{"poster":"janvandermerwer","content":"Selected Answer: BD\nMost likely answers:\nB and D\nA - NLB Doesn't support WAF\nC - ASG direct doesn't support waf - needs an ALB/Cloudfront in front\n\nE - Shield is included by default anyway","upvote_count":"2","comment_id":"715096","timestamp":"1668072960.0"},{"timestamp":"1666093500.0","upvote_count":"1","content":"Selected Answer: BD\nB and D","poster":"Vizz5585","comment_id":"698181"},{"comment_id":"627753","timestamp":"1657090440.0","poster":"TechX","content":"Selected Answer: BD\n100% BD","upvote_count":"2"},{"poster":"KiraguJohn","timestamp":"1655789760.0","comment_id":"619606","upvote_count":"1","content":"I have a problem with D because we have not been told whether the web content is static or dynamic. Can we use Cloudfront on a dynamic web content?"},{"upvote_count":"2","comment_id":"597743","timestamp":"1651843920.0","content":"Selected Answer: BD\nB. Route 53 and AWS Shields helps in mitigating the flood of DDoS attacks\nD. Cloudfront and WAF also aid in preventing DDoS attacks","poster":"tartarus23"},{"content":"Selected Answer: BD\nVote BD","comment_id":"580858","upvote_count":"1","timestamp":"1649092560.0","poster":"roka_ua"},{"poster":"shotty1","upvote_count":"2","comment_id":"532773","timestamp":"1643191740.0","content":"Selected Answer: BD\nit is BD"},{"comment_id":"494643","timestamp":"1638736440.0","content":"Selected Answer: BD\nB and D is the answer","poster":"AzureDP900","upvote_count":"4"},{"timestamp":"1636169280.0","poster":"tonikus","upvote_count":"2","content":"how on Earth could E got marked as an answer?\nIt's B and D","comment_id":"450312"},{"content":"I'll go with B, D","poster":"WhyIronMan","upvote_count":"2","timestamp":"1635867420.0","comment_id":"411145"},{"comment_id":"380566","content":"B and D","upvote_count":"1","timestamp":"1635323160.0","poster":"Amitv2706"},{"comment_id":"346738","timestamp":"1634640000.0","upvote_count":"1","poster":"blackgamer","content":"B and D is the answer"},{"comment_id":"317728","content":"B and D is the answer","upvote_count":"2","poster":"nitinz","timestamp":"1633926780.0"},{"upvote_count":"1","timestamp":"1633384320.0","poster":"awsexamprep47","comment_id":"316515","content":"B&D is the answer"},{"upvote_count":"4","content":"i go with B, D.\n\n- AWS Shield\n Amazon CloudFront distributions\n Amazon Route 53 hosted zones\n AWS Global Accelerator accelerators\n Application load balancers\n Elastic Load Balancing (ELB) load balancers\n Amazon Elastic Compute Cloud (Amazon EC2) Elastic IP addresses\n- AWS WAF\n Amazon CloudFront\n Amazon API Gateway REST API\n Application Load Balancer \n AWS AppSync GraphQL API\n\n\ni refer this link :\nhttps://docs.aws.amazon.com/ko_kr/waf/latest/developerguide/waf-chapter.html \nhttps://docs.aws.amazon.com/ko_kr/waf/latest/developerguide/shield-chapter.html","timestamp":"1632759840.0","poster":"JJu","comment_id":"316205"},{"upvote_count":"1","content":"i think BD","comment_id":"313579","timestamp":"1632495420.0","poster":"didek1986"},{"content":"I go with BC","timestamp":"1632114420.0","upvote_count":"3","poster":"Nguyenhau","comment_id":"309728"}],"answers_community":["BD (100%)"],"topic":"1","answer_images":[],"exam_id":32,"timestamp":"2021-03-13 14:35:00","answer":"BD","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/46884-exam-aws-certified-solutions-architect-professional-topic-1/","answer_ET":"BD","question_text":"What combination of steps could a Solutions Architect take to protect a web workload running on Amazon EC2 from DDoS and application layer attacks? (Choose two.)","choices":{"E":"Create and use an internet gateway in the VPC and use AWS Shield.","C":"Put the EC2 instances in an Auto Scaling group and configure AWS WAF on it.","D":"Create and use an Amazon CloudFront distribution and configure AWS WAF on it.","B":"Migrate the DNS to Amazon Route 53 and use AWS Shield.","A":"Put the EC2 instances behind a Network Load Balancer and configure AWS WAF on it."},"isMC":true,"question_id":495,"unix_timestamp":1615642500,"answer_description":""}],"exam":{"provider":"Amazon","numberOfQuestions":1019,"isImplemented":true,"isBeta":false,"isMCOnly":false,"name":"AWS Certified Solutions Architect - Professional","id":32,"lastUpdated":"11 Apr 2025"},"currentPage":99},"__N_SSP":true}