{"pageProps":{"questions":[{"id":"QPcI3xzJZSZSokArKV4a","isMC":true,"question_images":[],"answer":"A","answers_community":["A (85%)","D (15%)"],"discussion":[{"poster":"sambb","comments":[{"poster":"b3llman","comment_id":"975319","upvote_count":"1","timestamp":"1691481780.0","content":"file storage gateway can be installed on EC2 and it is exactly used for accessing S3 from EC2 as a file system"},{"upvote_count":"1","comment_id":"1027849","timestamp":"1696757460.0","content":"It's used a lot, I've used it for customers to access and analyze data imported via Snowball from Windows machines.","poster":"Chainshark"},{"poster":"dqwsmwwvtgxwkvgcvc","content":"There is one S3 file gateway\n\nhttps://aws.amazon.com/storagegateway/file/s3/","timestamp":"1692318900.0","upvote_count":"1","comment_id":"984076"},{"comment_id":"1016409","timestamp":"1695614040.0","content":"https://aws.amazon.com/blogs/storage/new-enhancements-for-moving-data-between-amazon-fsx-for-lustre-and-amazon-s3/","upvote_count":"3","poster":"Tofu13"}],"comment_id":"823462","upvote_count":"20","timestamp":"1677489420.0","content":"Selected Answer: A\nA: Lazy loading is cost-effective because only a subset of data is used at every job\nB: There are hundreds of EC2 instances using the volume which is not possible (one EBS volume is limited to 16 nitro instances attached)\nC: Batching would load too much data\nD: storage gateway is used for on premises data access, I don't know is you can install a gateway in AWS, but Amazon would never advise this"},{"comment_id":"972239","content":"Answer: D\n\nI think the main point here is to understand what they mean by \"The file system must provide high performance access to the needed data\" while \"provide the LARGEST overall cost reduction\"?\n\nFor answer A, we have to remember that lazy load is SLOW for the first time you try to access the file (as it is being fetched from S3), BUT, as we are talking about hundreds of instances, then it might be OK. S3 Intelligent-Tiering, although doesn't seem to fit much, the part that says \"The job runs once monthly, reads a subset of the files from the shared file system\", indicates that at least part of the 200TB of data won't be accessed, which helps not going for answer C, for example.\n\nMy only issue with answer D is that Storage Gateway can be slower than FSx for Lustre, HOWEVER, what is the cost X performance ratio they are seeking here? We can guess that costs trumps maximum performance here: \"Which solution will provide the LARGEST overall cost reduction\" and, as Storage Gateway is way cheaper than FSx for Lustre per TB, it's safe to say that D is the most correct answer.","upvote_count":"15","timestamp":"1691158500.0","poster":"chico2023"},{"upvote_count":"1","comment_id":"1332989","timestamp":"1735394460.0","poster":"SIJUTHOMASP","content":"Selected Answer: D\nI lean more towards D but I am not sure whether the Gateway is only intended for on-premise as few are mentioning here. If that is not the case then the right option is D."},{"content":"Selected Answer: D\nFSx for Lustre, is only for Linux where in the question is Linux noted. It's states only EC2 instance not which OS is on it!","comment_id":"1323456","upvote_count":"1","poster":"zaxxon","timestamp":"1733648820.0"},{"comment_id":"1316158","timestamp":"1732250100.0","poster":"TariqKipkemei","content":"Selected Answer: A\n'The job runs once monthly', 'cost reduction' = S3 Intelligent-Tiering storage class, lazy loading.\n'Scalable file system', 'shared file system', 'data-intensive ' = Amazon FSx for Lustre","upvote_count":"1"},{"upvote_count":"2","timestamp":"1731891180.0","content":"Selected Answer: A\nBy choosing Option A, the company can leverage the cost-effectiveness of Amazon S3 Intelligent-Tiering for storage and the high performance of Amazon FSx for Lustre for temporary file access, while minimizing the overall cost by creating and deleting the file system only when needed.\nOption B (using Amazon EBS Multi-Attach) is not ideal because EBS volumes are designed for persistent storage, and attaching and detaching a large volume to multiple instances can be time-consuming and potentially disruptive.\nOption C (using Amazon FSx for Lustre with batch loading) is less cost-effective than Option A because batch loading requires loading the entire 200 TB of data into the file system, which can be expensive and time-consuming.\nOption D (using AWS Storage Gateway File Gateway) is not the most cost-effective solution because the File Gateway is designed for on-premises file storage integration and may not provide the same level of performance as FSx for Lustre for this data-intensive workload.","poster":"0b43291","comment_id":"1313779"},{"comment_id":"1275461","timestamp":"1725092580.0","upvote_count":"1","content":"A. Migrate the data from the existing shared file system to an Amazon S3 bucket that uses the S3 Intelligent-Tiering storage class. Before the job runs each month, use Amazon FSx for Lustre to create a new file system with the data from Amazon S3 by using lazy loading. Use the new file system as the shared storage for the duration of the job. Delete the file system when the job is complete.","poster":"amministrazione"},{"content":"A or D : confusion. I wish they can provide explanation about their answers when it is not the most voted one","timestamp":"1723362960.0","poster":"MAZIADI","upvote_count":"1","comment_id":"1263904"},{"timestamp":"1718567520.0","upvote_count":"2","poster":"Helpnosense","comment_id":"1231526","content":"I vote D instead A because the requirement in the question is \"modifies the data on the shared file system\" Fsx imported data from s3 and lost the relationship to s3 after import is done Without explicitly copy back to s3, the change stays on shared file system only. Answer A solution doesn't provide a step to copy the modification back to s3. \nStorage gateway presents s3 storage to the OS as shared file system. Any modification on the shared file system will be automatically saved on s3."},{"content":"Selected Answer: A\nA: Lazy loading is cost-effective because only a subset of data is used at every job","timestamp":"1710611580.0","poster":"gofavad926","upvote_count":"1","comment_id":"1175168"},{"comment_id":"1174721","upvote_count":"3","timestamp":"1710561360.0","poster":"kz407","content":"Selected Answer: A\nProblem with D is that, AWS Storage GW and File GW are solutions for integrating on-premise storage with AWS storage solutions, particularly (but not limited to) S3.\n\nhttps://aws.amazon.com/storagegateway/\nhttps://aws.amazon.com/storagegateway/file\n\nCompute resources are residing in AWS, so having Storage GW and File GW won't solve a thing.\nAs far as option B is concerned, it comes down to the limitations of EBS (such as the max block size, and max number of instance that can be attached etc). Also, attaching and detaching of the EBS volumes seems a bit complicated too. On top of that, EBS does not offer the cost optimizations offered by S3 Intelligent Tiering. The question clearly mentions that only a subset of the data will be used. Intelligent tiering ensures a substantial cost optimization over time.\nHence, the answer should be A."},{"timestamp":"1710311700.0","upvote_count":"1","poster":"kspendli","content":"Option D, migrating the data to an Amazon S3 bucket and using AWS Storage Gateway, seems to provide the largest overall cost reduction while meeting the requirements of high-performance access during the job run and minimizing costs when the storage is not actively being used. Therefore, Option D is the most suitable choice.","comment_id":"1172282"},{"poster":"anubha.agrahari","timestamp":"1709678640.0","comment_id":"1166805","upvote_count":"2","content":"Selected Answer: A\nhttps://aws.amazon.com/blogs/storage/new-enhancements-for-moving-data-between-amazon-fsx-for-lustre-and-amazon-s3/"},{"content":"Selected Answer: A\nOption A - This option might work. However, AWS FSx for Lustre does not have a feature called \"lazy loading\" - its default behavior is to load a file from S3 when it is first accessed (restore). It can provide high-performance as needed though nothing is said in the question about whether a slow initial load time due to restore operations could be an issue. S3 Intelligent-Tiering minimizes storage costs.\n\nOption B - This option will provide a high-performance storage option. However, storage in EBS is expensive compared to other AWS storage services\n\nOption C - This option might work. However, AWS FSx for Luster does not have a feature called \"batch loading\". Files can be pre-loaded issuing a hsm-restore command. S3 Standard is a cheap storage option yet not the cheapest option in S3\n\nOption D - This option does not work as described in the option","upvote_count":"2","poster":"atirado","comment_id":"1101268","timestamp":"1703050620.0","comments":[{"comment_id":"1135016","content":"Actually AWS FSx for Lustre does not have a direct feature 'Lazy loading' but the question is the support of that when Amazon FSx will import the objects in our S3 bucket as files, and “lazy-load” the file contents from S3 when first access the files.. Any data processing job on Lustre with S3 as an input data source can be started without Lustre doing a full download of the dataset\nfirst - Data is lazy loaded: only the data that is\nactually processed is loaded, meaning you can\ndecrease your costs and latency","poster":"AimarLeo","upvote_count":"1","timestamp":"1706536380.0"}]},{"upvote_count":"1","comment_id":"1099547","poster":"ninomfr64","content":"Not B because using EBS still involves EC2 instances that are expensive (the instances that host the shared file system run continuously). Also, multi-attach is supported only for io1/oi2 EBS disk types that are expensive;\nNot C as batch loading does not exists in the doc/console, I think they might refer to the option to pre-populate the data using lfs hsm_restore command as mentioned here https://docs.aws.amazon.com/fsx/latest/LustreGuide/preload-file-contents-hsm-dra.html. This would be a more expensive option\nNot D as Storage Gateway provide less performance than FSx for Lustre and it requires at least an EC2 instance and this will introduce additional cost\n\nAA is a viable option as S3 is cheaper storage, FSx for Lustre provides performance. Lazy loading allows to actually move in the filesystem data that is actually used and intelligent tiering make sure those files that are not used are moved to less expensive S3 storage tiers.","timestamp":"1702889220.0"},{"comments":[{"comment_id":"1167440","content":"\"Only a subset of data is accessed each run\" So that means after 30 days data can tier down so yes there is cost savings in using INT","timestamp":"1709753640.0","poster":"e4bc18e","upvote_count":"1"}],"upvote_count":"1","comment_id":"1088386","poster":"subbupro","content":"Intelligent tiering is not required, because the job would be running for every month, so there is no purpose for intelligent tiering, The question is having cost impact also one of the option. So go with option D.","timestamp":"1701772080.0"},{"poster":"Japanese1","upvote_count":"5","comments":[{"comment_id":"1173403","poster":"kz407","timestamp":"1710420060.0","upvote_count":"1","content":"You can configure a DRA for automatic import only, for automatic export only, or for both. \"A data repository association configured with both automatic import and automatic export propagates data in both directions between the file system and the linked S3 bucket. As you make changes to data in your S3 data repository, FSx for Lustre detects the changes and then automatically imports the changes to your file system. As you create, modify, or delete files, FSx for Lustre automatically exports the changes to Amazon S3 asynchronously once your application finishes modifying the file.\"\n\nhttps://docs.aws.amazon.com/fsx/latest/LustreGuide/create-dra-linked-data-repo.html"},{"poster":"grire974","upvote_count":"1","timestamp":"1704847260.0","comment_id":"1117975","content":"Oh yeh - of course; if you delete the FSx volume; the changes are lost."}],"comment_id":"1061153","timestamp":"1698997080.0","content":"Selected Answer: D\nFunctional requirements should be met before non-functional requirements.\nIn the first place, only option D allows the application to change the data in the shared file during the monthly job execution. With options A and C, data changes made during the job are discarded after the job runs.\nOn top of that, although D is inferior to A in performance, it meets the requirements because it is the cheapest."},{"timestamp":"1693384260.0","poster":"bur4an","upvote_count":"1","content":"Selected Answer: A\nA. Migrate the data from the existing shared file system to an Amazon S3 bucket that uses the S3 Intelligent-Tiering storage class. Before the job runs each month, use Amazon FSx for Lustre to create a new file system with the data from Amazon S3 by using lazy loading. Use the new file system as the shared storage for the duration of the job. Delete the file system when the job is complete.\n\nOption B (using Amazon EBS) would result in higher costs due to the continuous usage of large EBS volumes. Similarly, option C involves creating a new FSx for Lustre file system with batch loading, which may not be as cost-effective as lazy loading.\n\nOption D (using AWS Storage Gateway) would involve additional complexity and potentially higher costs compared to directly utilizing S3 and FSx for Lustre.","comment_id":"993849"},{"upvote_count":"1","poster":"dqwsmwwvtgxwkvgcvc","timestamp":"1692321360.0","comment_id":"984110","content":"Selected Answer: D\n@chico already explain the logic behind, @sambb chose A because S3 file gateway wasn't clear to him"},{"comment_id":"966828","timestamp":"1690689060.0","poster":"chiajy","upvote_count":"1","content":"Question mentioned \"The file system must provide high performance access to the needed data for the duration of the 72-hour run.\" Assuming S3 Intelligent-Tiering don't move data into Archive Access tiers(which are optional) [Ref: docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering-overview.html] . Thus, need to ensure data is always in storage tiers that provide \"low latency and high throughput performance.\". As S3 Intelligent-Tiering delivers automatic storage cost savings, Answer A can be the potential answer."},{"timestamp":"1690208520.0","upvote_count":"4","poster":"waoo","content":"A一定是错的，因为数据都是不常访问的，如果放到s3的智能存储中，会默认变成冷数据，再被访问时，需要重新激活，需要付出很高的成本","comment_id":"961700"},{"content":"ption C. Migrate the data from the existing shared file system to an Amazon S3 bucket that uses the S3 Standard storage class. Before the job runs each month, use Amazon FSx for Lustre to create a new file system with the data from Amazon S3 by using batch loading. Use the new file system as the shared storage for the duration of the job. Delete the file system when the job is complete.","timestamp":"1688634240.0","upvote_count":"1","comment_id":"944479","poster":"Asamara"},{"content":"Selected Answer: A\nA benefit of FSx for Lustre. \nIntegrates seamlessly with Amazon S3 (connect your S3 data sets to your FSx for Lustre file system, run your analyses, write results back to S3, and delete your file system) ...","poster":"rxhan","timestamp":"1688254320.0","upvote_count":"2","comment_id":"940359"},{"content":"Selected Answer: A\nA?\nC would load unneeded data.\nThe only potential issue with A is that lazy loading may impact high performance access, which is also requirement","poster":"NikkyDicky","upvote_count":"1","comment_id":"937083","timestamp":"1687980840.0"},{"timestamp":"1687336500.0","poster":"Jonalb","upvote_count":"1","comment_id":"929213","content":"Selected Answer: A\nS3 Intelligent-Tiering !!!! ITS A"},{"upvote_count":"5","poster":"hitesh24","timestamp":"1685639700.0","content":"By migrating the data to an S3 bucket with the S3 Intelligent-Tiering storage class, you can take advantage of cost optimization. S3 Intelligent-Tiering automatically moves data between two access tiers: frequent access and infrequent access, based on usage patterns. This ensures that data is stored cost-effectively while providing high performance when needed.\n\nUsing Amazon FSx for Lustre to create a new file system with the data from Amazon S3 using lazy loading allows for efficient access to the required subset of files during the monthly job. The file system is created on-demand and the data is loaded only when accessed, which helps reduce costs as you only pay for the storage and compute resources used during the job.\n\nDeleting the file system when the job is complete ensures that you are not incurring any additional costs for the shared storage when it is not needed.\n\nTherefore, option A provides the largest overall cost reduction while still meeting the performance requirements for the monthly job.","comment_id":"912300"},{"poster":"gameoflove","upvote_count":"2","content":"Selected Answer: D\nD is the most logical answer","comment_id":"892131","timestamp":"1683547560.0"},{"upvote_count":"3","poster":"jj22222","comment_id":"863903","timestamp":"1680873060.0","content":"Selected Answer: A\nS3 intelligent tiering"},{"timestamp":"1679980620.0","upvote_count":"1","poster":"mfsec","content":"Selected Answer: A\nA is the best choice","comment_id":"852805"},{"poster":"cudbyanc","timestamp":"1679164020.0","upvote_count":"2","content":"Selected Answer: A\ndefinitely A","comment_id":"843056"},{"comment_id":"831797","upvote_count":"1","timestamp":"1678187640.0","content":"Selected Answer: A\nLazy loading is cost-effective because only a subset of data is used at every job","poster":"kiran15789"},{"comment_id":"820277","poster":"hobokabobo","upvote_count":"1","content":"Selected Answer: A\nA: provides High performance Access\nB: EBS is by far more expensive than s3.\nC: Lustre with Lazy Loading(A) is Cheaper than Batch loading\nD: might be cheaper than A but does not provide High performance Access.","timestamp":"1677229500.0"},{"comments":[{"upvote_count":"1","comment_id":"820251","poster":"hobokabobo","content":"I think you are wrong about the S3 file gateway:\nhttps://docs.aws.amazon.com/filegateway/latest/files3/create-gateway-file.html#connect-to-amazon-s3-file","timestamp":"1677226680.0"}],"poster":"God_Is_Love","comment_id":"818544","upvote_count":"1","content":"My Logical answer : D has blunder in it. Storage gateway is not a storage solution. its just a gateway for large data transfers usual usecase of on premises. Block storage is not fit as data is being modified here in this\nusecase. So B is wrong. am guessing data analytics usecase here for high performant compute which Lustre provides. that leaves A or C. The tricky word here is Overall cost storage saving comparing S3 Intelligent tier vs Standard tier. Intelligent tier can recognise that data is not being touched\nfor a month and it will use its intelligence to move into other cheaper storage class and gets to work whenever that high performance job needs to be started. So A fits well.","timestamp":"1677108660.0"},{"comment_id":"815190","timestamp":"1676894460.0","poster":"spd","upvote_count":"3","content":"Selected Answer: A\nFSx for Lustre with lazy loading should work","comments":[{"upvote_count":"1","timestamp":"1684802460.0","poster":"mKrishna","content":"Question has Data intense. Lazy load takes time to download file to FSx","comment_id":"904447"}]},{"content":"Can someone please explain why the correct answer is A, and not C?\nA will actually come out more expensive, as (because you load the file each month) the files will never transition out from S3 standard, and you also pay for the inteligent-tiering.","comment_id":"813907","poster":"anita_student","upvote_count":"1","timestamp":"1676799000.0","comments":[{"upvote_count":"3","poster":"God_Is_Love","comment_id":"818548","timestamp":"1677109080.0","content":"Hey Anita - Intelligent tier will observe that data is not touched for a month right, so data can actually be moved into other cheaper storage based on usage patterns. it is cheaper compared to standard S3. AWS says Since the launch of S3 Intelligent-Tiering in 2018, customers have saved $750 million from adopting S3 Intelligent-Tiering when compared to S3 Standard. Read thru this as this article says S3 intelligent tiering provides automatic save storage costs- https://aws.amazon.com/s3/storage-classes/intelligent-tiering/","comments":[{"timestamp":"1677443640.0","comment_id":"822937","upvote_count":"3","content":"This job runs 1x every month, so the data has no opportunity to transition into cheaper storage tier.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering-overview.html\n\" For a low monthly object monitoring and automation charge, S3 Intelligent-Tiering monitors access patterns and automatically moves objects to the Infrequent Access tier when they have not been accessed for 30 consecutive days.\"\nSo basically all you achieve with using intelligent tiering is keeping the data in standard storage for 30 days, moving the data to infrequent access and moving it back to standard the same day or the next. Seems pretty pointless to me","poster":"anita_student"},{"content":"OK, just noticed only a subset of the files are read during the monthly job, so in that case A is correct.","timestamp":"1677444780.0","comment_id":"822954","poster":"anita_student","upvote_count":"1"}]}]},{"timestamp":"1674985080.0","comment_id":"791518","upvote_count":"3","comments":[{"comment_id":"803427","timestamp":"1675957740.0","content":"I think we could assume it because it deliberately uses FSx for Lustre linked to S3.","poster":"moota","upvote_count":"1"}],"poster":"Musk","content":"I go for D. In A, we are missing a critical step, exporting the resutls back from FSX to S3. Instead, we are deleting the FSX file system, which means that results and modifications are lost. Please check https://aws.amazon.com/blogs/storage/new-enhancements-for-moving-data-between-amazon-fsx-for-lustre-and-amazon-s3/ which explains that."},{"comments":[{"upvote_count":"1","timestamp":"1677014100.0","comment_id":"817178","content":"yeah there is but its not feasible here as there is hundreds of instance.\n\nMulti-Attach enabled volumes can be attached to up to 16 Linux instances built on the Nitro System that are in the same Availability Zone. You can attach a volume that is Multi-Attach enabled to Windows instances, but the operating system does not recognize the data on the volume that is shared between the instances, which can result in data inconsistency.","poster":"saurabh1805"}],"content":"Option A is Correct. \nAs for Option B - is there such a thing as EBS Multi-attach!","upvote_count":"1","comment_id":"786710","poster":"MasterP007","timestamp":"1674579300.0"},{"upvote_count":"3","content":"Selected Answer: A\nThis solution would provide the largest overall cost reduction while meeting the requirements. By migrating the data to an S3 bucket using the S3 Intelligent-Tiering storage class, the company can take advantage of the automatic cost optimization provided by the storage class, which can result in significant cost savings. Additionally, by using Amazon FSx for Lustre to create a new file system with the data from Amazon S3, the company can take advantage of the high-performance access to the needed data for the duration of the 72-hour run. When the job is complete, the company can delete the file system, further reducing costs.\n\nOption B, C and D may provide some cost savings over the current solution, but the savings would be less significant than the option A, because of the cost of the storage, the cost of the data transfer, and the cost of the storage gateway, the solution using the S3 and FSx for Lustre is the most cost-effective while meeting the requirements.","timestamp":"1673637720.0","poster":"masetromain","comment_id":"774796"},{"timestamp":"1671922140.0","comment_id":"755230","upvote_count":"2","content":"Selected Answer: A\nusing FSx for Lustre with lazy loading allows you to only pay for the data that is accessed during the job, rather than paying for the entire file system upfront.","poster":"cloudfever"},{"poster":"masetromain","content":"Selected Answer: A\nhttps://www.examtopics.com/discussions/amazon/view/80991-exam-aws-certified-solutions-architect-professional-topic-1/","comment_id":"744182","upvote_count":"4","timestamp":"1670944320.0"}],"answer_ET":"A","topic":"1","timestamp":"2022-12-13 16:12:00","unix_timestamp":1670944320,"exam_id":33,"question_text":"A company is running a data-intensive application on AWS. The application runs on a cluster of hundreds of Amazon EC2 instances. A shared file system also runs on several EC2 instances that store 200 TB of data. The application reads and modifies the data on the shared file system and generates a report. The job runs once monthly, reads a subset of the files from the shared file system, and takes about 72 hours to complete. The compute instances scale in an Auto Scaling group, but the instances that host the shared file system run continuously. The compute and storage instances are all in the same AWS Region.\nA solutions architect needs to reduce costs by replacing the shared file system instances. The file system must provide high performance access to the needed data for the duration of the 72-hour run.\nWhich solution will provide the LARGEST overall cost reduction while meeting these requirements?","answer_description":"","choices":{"D":"Migrate the data from the existing shared file system to an Amazon S3 bucket. Before the job runs each month, use AWS Storage Gateway to create a file gateway with the data from Amazon S3. Use the file gateway as the shared storage for the job. Delete the file gateway when the job is complete.","B":"Migrate the data from the existing shared file system to a large Amazon Elastic Block Store (Amazon EBS) volume with Multi-Attach enabled. Attach the EBS volume to each of the instances by using a user data script in the Auto Scaling group launch template. Use the EBS volume as the shared storage for the duration of the job. Detach the EBS volume when the job is complete","C":"Migrate the data from the existing shared file system to an Amazon S3 bucket that uses the S3 Standard storage class. Before the job runs each month, use Amazon FSx for Lustre to create a new file system with the data from Amazon S3 by using batch loading. Use the new file system as the shared storage for the duration of the job. Delete the file system when the job is complete.","A":"Migrate the data from the existing shared file system to an Amazon S3 bucket that uses the S3 Intelligent-Tiering storage class. Before the job runs each month, use Amazon FSx for Lustre to create a new file system with the data from Amazon S3 by using lazy loading. Use the new file system as the shared storage for the duration of the job. Delete the file system when the job is complete."},"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/91447-exam-aws-certified-solutions-architect-professional-sap-c02/","question_id":146},{"id":"IozXdlr8LVEn7ZiYe6oP","timestamp":"2023-06-23 20:53:00","question_images":[],"question_id":147,"answers_community":["CD (42%)","BD (38%)","AD (18%)","3%"],"discussion":[{"timestamp":"1688340540.0","upvote_count":"14","comment_id":"941297","comments":[{"content":"just CD","timestamp":"1724459160.0","comment_id":"1271475","upvote_count":"1","poster":"helloworldabc"}],"poster":"SkyZeroZx","content":"Selected Answer: AD\nA By sharing the subnets that host the service provider applications using AWS Resource Access Manager (RAM), the service consumer applications can be deployed in the same organization's accounts. This allows the traffic between the service consumer and service provider applications to stay within the organization's network, reducing data transfer charges.\nD By using the Availability Zone-specific endpoint service's local DNS name, the service consumer compute resources can directly access the service provider applications within the same Availability Zone. This eliminates the need for cross-Availability Zone data transfer, thus reducing data transfer charges."},{"upvote_count":"8","comment_id":"1001632","content":"Selected Answer: CD\n- **C. Turn off cross-zone load balancing for the Network Load Balancer in all service provider application deployments.**\n- **D. Ensure that service consumer compute resources use the Availability Zone-specific endpoint service by using the endpoint's local DNS name.**","timestamp":"1694095620.0","poster":"xav1er"},{"content":"Selected Answer: CD\nNormally when data leaves AZ to another, there is a cost associated","timestamp":"1732565280.0","upvote_count":"1","poster":"youonebe","comment_id":"1317744"},{"content":"This is why C is correct:\n\n\n\"For ALB and CLB, there is no cross-AZ data transfer charges within the same VPC. But for NLB, if the client and target are in one AZ, but the NLB is in another AZ, there will be a zone-in and zone-out which is $0.02.\"","comment_id":"1306171","upvote_count":"2","poster":"sam2ng","timestamp":"1730553000.0"},{"content":"Selected Answer: CD\nB is not an option: While placing resources in the same organization might simplify management, it does not inherently reduce data transfer charges. Data transfer costs between AWS Organizations accounts are typically not impacted by being in the SAME OR DIFFERENT organizations, especially when using PrivateLink.","upvote_count":"1","comment_id":"1294463","poster":"JoeTromundo","timestamp":"1728334020.0"},{"content":"Selected Answer: CD\nC D is correct one\nFor C, Cross-zone load balancing can distribute traffic across multiple AZs, which increases data transfer costs between AZs. Disabling cross-zone load balancing ensures that traffic remains within the same AZ, reducing the associated data transfer charges. This is particularly important for applications using AWS PrivateLink, as it will help keep data transfers within the same AZ as much as possible.","upvote_count":"1","timestamp":"1720630320.0","poster":"vip2","comment_id":"1245612"},{"upvote_count":"3","poster":"michele_scar","comment_id":"1218336","timestamp":"1716642720.0","content":"Selected Answer: CD\nB is useless because if you place the resource in the same org but in different AZs you will pay the same as different org in different AZs. So B is uncorrect (like A and E).\nRemains C and D as a solution that should reduce costs."},{"timestamp":"1714722180.0","content":"Selected Answer: BD\nBD for me","comment_id":"1205978","poster":"seetpt","upvote_count":"2"},{"comment_id":"1193401","upvote_count":"1","content":"B - allows data transfer between linked accounts to be free of charge.\nD - ensures traffic stays within the same AZ as much as possible, minimizing inter-AZ data transfer costs.\n\nCD - Save money.","timestamp":"1712806920.0","poster":"4555894"},{"upvote_count":"4","poster":"VerRi","timestamp":"1711897200.0","content":"Selected Answer: BD\n\"The company manages two organisations in AWS Organizations,\" which means they have one organisation for service providers and one more for consumers. \n\nA. Since applications are created in the provider organisation, sharing the subnet with other accounts within the same organisation has no effect. \nB. Combining provider and consumer into one organisation is the first move for Option D.\nC. Cross-zone load balancing does not change the amount of data traffic passing through the NLB, it affects how that traffic is distributed across the targets. \nD. AZ-specific endpoint helps to reduce data transfer charges because it keeps the traffic in a single AZ and is designed for intra-regional communication within the same account or organization. \nE. WTF","comment_id":"1186854"},{"timestamp":"1711044060.0","upvote_count":"5","comment_id":"1179482","comments":[{"content":"Turning off cross-zone load balancing can reduce inter-AZ data transfer costs. With cross-zone load balancing disabled, a Network Load Balancer (NLB) only routes requests to targets in the same Availability Zone as the load balancer node that received the request. This setup reduces the data transferred across Availability Zones, thereby reducing costs.","comment_id":"1184445","timestamp":"1711579980.0","poster":"mav3r1ck","upvote_count":"3"}],"poster":"Dgix","content":"Selected Answer: BD\nIt's B and D.\n\nA. Sharing subnets does not directly reduce data transfer charges.\nC. Turning off cross-zone load balancing does not impact data transfer costs between VPC endpoints and service consumers.\nE. A Savings Plan reduces costs for compute usage, not specifically for data transfer charges."},{"timestamp":"1710262380.0","poster":"ajeeshb","comment_id":"1171838","content":"Selected Answer: CD\nAnswer: C, D","upvote_count":"3"},{"comment_id":"1149483","poster":"marszalekm","timestamp":"1707849900.0","upvote_count":"2","content":"https://docs.aws.amazon.com/ram/latest/userguide/shareable.html \"Can share with only AWS accounts in its own organization.\" ec2:Subnet"},{"comment_id":"1146015","content":"Selected Answer: CD\nAnswer is CD\nD) Obvious option, This approach minimizes data transfer costs by ensuring that traffic between service consumers and service providers stays within the same Availability Zone\nC) Only after setting up your NLB, you can create a VPC Endpoint Service (VPC-E) that is powered by AWS PrivateLink. Cross-zone lb feature is optional for NLB since 2018 so, turning off cross-zone load balancing can help ensure that data does not unnecessarily cross Availability Zones, thereby once again reducing data transfer costs\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html\n\n\n\nB) Incorrect: putting the workloads into 1 org - would not make any effect on billing neither, unless you change the topology profoundly and move away the VPCE solution - but we are not talking about Re-architecting, we are looking to provide guidelines\nA) Incorrect: RAM can be used only within 1 organization\nE) Incorrect: there is no a such flavor of Saving plans, AWS provides 3 Compute, EC Instance and SageMaker Saving plans","comments":[{"upvote_count":"1","content":"You can also share with specific AWS accounts by account ID, regardless of whether the account is part of an organization.","comment_id":"1174187","poster":"JOKERO","timestamp":"1710495180.0"}],"poster":"Wardove","upvote_count":"6","timestamp":"1707557760.0"},{"comment_id":"1134119","timestamp":"1706450100.0","comments":[{"comment_id":"1134120","poster":"LazyAutonomy","timestamp":"1706450160.0","comments":[{"content":"C - appears incorrect, but the reason has nothing to do with \"compromising high availability\". As pointed out by @elmoh, cross-zone load balancing isn't enabled by default in NLBs anyway. See https://docs.aws.amazon.com/elasticloadbalancing/latest/network/network-load-balancers.html#cross-zone-load-balancing. Even if cross-zone load balancing was enabled by default in NLBs, this option doesn't cover the Gateway Load Balancer VPC endpoint service use case.","upvote_count":"2","comment_id":"1134121","poster":"LazyAutonomy","timestamp":"1706450160.0"}],"content":"B - appears correct. @Just_Ninja's explanation nails it. If you use Organizations and you create accounts, then in each member account, the logical identifiers for each availability zone (e.g. \"eu-central-1a\") are guaranteed to map to the same AZ Physical ID (e.g. \"euc1-az3\") for all accounts within the Organization. In other words, it's likely that AZ \"eu-central-1a\" for accounts in OrgABC is not the same as AZ \"eu-central-1a\" for accounts in OrgXYZ. That's a problem if you're trying to eliminate unnecessary cross-zone traffic. Without this, you could instruct developers to use AZ-specific DNS names and still end up with the same huge bill :-)","upvote_count":"1"}],"poster":"LazyAutonomy","upvote_count":"5","content":"Selected Answer: BD\nHoly bageezus, never seen a discussion thread so divided.\n\n@NikkyDicky is spot on - cross zone traffic is indeed where the money is going. I think we all know that.\n\nA - appears incorrect, we cannot share subnets between accounts in different AWS Orgs. Even if you could, or even if you chose A+B, it would be impractical to assume all other workloads could be deployed in service provider subnets. Would probably run out of IPs. And even if the subnets were huge and we didn't run out of IPs, there is no mechanism in A to guide developers deploying their workloads to reduce or prevent cross-AZ traffic. You could share the subnets and deploy all provider/consumer workloads in the same set of subnets and still end up with the same huge bill :-)"},{"upvote_count":"3","poster":"tmlong18","content":"Selected Answer: CD\nI go with C & D.\nData transfer cost base on physical distance.(cross AZ, cross region, internal)\nA & B - shared VPC doesn't distribute traffic to inter-az","comment_id":"1122401","timestamp":"1705222260.0"},{"poster":"Jay_2pt0_1","comment_id":"1107231","content":"This question is poorly framed. I go with A & D, not because they are great, but because the others are terrible. You should not have to move into the same org (that can't be the answer). Also, we won't compromise HA, so that can't be the answer either.","upvote_count":"3","timestamp":"1703713740.0"},{"content":"Selected Answer: AB\nThe question is badly framed. \nFirst, we need define the \"Data transfer\". Does it mean cross AZ data transfer or cross account data transfer? \nI assume there isn't private network connectivity between the two parties, because they are not even in the same organization, and there is not statement saying they are connected to each other with peering or transit gateway or VPN. So I assume the \"Data transfer\" is cross organization data transfer, which highly possible is internet data transfer cost. So, A and B will be the best answer.\nIf the question designer meant the cross AZ data transfer and forgot to mention there is already private network connectivity created between the two VPC, C and D might be the best answer. But we can't assume something without any evidence, right?","timestamp":"1703226480.0","poster":"bjexamprep","comments":[{"upvote_count":"1","comment_id":"1122404","timestamp":"1705222440.0","content":"AWS PrivateLink is private network and support cross account VPC","poster":"tmlong18"}],"comment_id":"1103189","upvote_count":"2"},{"poster":"ayadmawla","timestamp":"1702566300.0","comment_id":"1096583","upvote_count":"1","content":"Selected Answer: AB\nRead B + A\nReduce the multi-organisation setup into a single one and then use Resource Sharing. Simple"},{"upvote_count":"1","timestamp":"1702134240.0","comment_id":"1091870","poster":"duriselvan","content":"D. Ensure that service consumer compute resources use the Availability Zone-specific endpoint service by using the endpoint's local DNS name.\n\nThis guideline encourages service consumer applications to utilize the local Availability Zone endpoint for the service provider application. This significantly reduces data transfer charges as communication happens within the same Availability Zone, avoiding inter-Availability Zone data transfer fees.\n\nE. Create a Savings Plan that provides adequate coverage for the organization's planned inter-Availability Zone data transfer usage.\n\nWhile not eliminating data transfer charges altogether, a Savings Plan can be beneficial if inter-Availability Zone communication is unavoidable. By committing to a consistent data transfer usage level, the company can receive a discount on its data transfer charges, leading to cost savings."},{"upvote_count":"3","content":"Selected Answer: BD\nAnswers: B and D \nNot C, this is compromising on the application's high availability.\nA - RAM cannot share resources across orgs\nE - not a relevant answer","comment_id":"1084523","poster":"shaaam80","timestamp":"1701359880.0"},{"timestamp":"1700890080.0","upvote_count":"2","content":"Selected Answer: BD\nB and D","poster":"jpes","comment_id":"1079751"},{"comment_id":"1078878","timestamp":"1700778900.0","upvote_count":"3","content":"Selected Answer: BD\nB and D","poster":"career360guru"},{"upvote_count":"3","content":"Selected Answer: CD\nThe key is to understand how Azure bills for data transfer. \nAs long as there are inter-availability zones transfer, there will be ingress/egress charges","timestamp":"1700737860.0","comment_id":"1078363","poster":"Pupu86"},{"content":"Selected Answer: CD\none company to organizations, c - saves costs for resource providers, and d saves costs for consumers, but both of them save costs for the company.\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-privatelink/creating-highly-available-endpoint-services.html\nso, such weird set up isn't random :)","upvote_count":"3","poster":"severlight","comments":[{"poster":"severlight","upvote_count":"1","comment_id":"1073915","timestamp":"1700303820.0","content":"* two orgs"}],"comment_id":"1073914","timestamp":"1700303820.0"},{"comment_id":"1069621","content":"Selected Answer: CD\nCannot be A because AWS EC2 Subnets cannot be shared with AWS Accounts outside Organizations. \nhttps://docs.aws.amazon.com/ram/latest/userguide/shareable.html","poster":"Sab","upvote_count":"3","timestamp":"1699899540.0"},{"timestamp":"1698360540.0","comment_id":"1054943","content":"Selected Answer: BD\nCannot be A, becuase they are in different orgnanizations, cannot use RAM if they are in different org.","poster":"KCjoe","upvote_count":"5"},{"poster":"rlf","upvote_count":"1","comment_id":"1043964","timestamp":"1697359140.0","content":"CD. \nB is wrong because 'placing the service provider applications and the service consumer applications in AWS accounts in the same organization' requires rejoining aws accounts between organizations."},{"timestamp":"1694096160.0","content":"i beleieve C is wrong\n\nCross-zone load balancing\nBy default, each load balancer node distributes traffic across the registered targets in its Availability Zone only. If you turn on cross-zone load balancing, each load balancer node distributes traffic across the registered targets in all enabled Availability Zones.","comment_id":"1001638","poster":"[Removed]","upvote_count":"1"},{"timestamp":"1693045080.0","comment_id":"990640","content":"Isn't C and D achieving the same outcome?\n- In C Provider Account turns off cross zone load balancing to ensure traffic stays in requested AZ\n- In D, Consumer Account is using specific provider AZ endpoints to ensure traffic stays in requested AZ\nIf this is the case, C seems a lower maintenance solution?","poster":"hglopes","upvote_count":"2"},{"timestamp":"1692873540.0","upvote_count":"3","content":"Selected Answer: BD\nB: allows data transfer between linked accounts to be free of charge;\nD: reduces data transfer charges across AZs","comment_id":"989094","poster":"aviathor"},{"content":"Selected Answer: BD\nThat just feels right","upvote_count":"3","poster":"SK_Tyagi","timestamp":"1692583440.0","comment_id":"986164"},{"poster":"chikorita","comments":[{"content":"okay guys!!!\nI tested this and can confirm that using you can share RAM resources across \"External AWS IDs\"\nPlease refer --> https://medium.com/@vanchi811/how-to-share-resources-with-multiple-accounts-using-aws-resource-access-manager-ram-b131d76b2641\nso changing my answer to AD (which is also most voted option presently) ;)","poster":"chikorita","timestamp":"1694524740.0","comment_id":"1005792","upvote_count":"1"},{"poster":"chikorita","upvote_count":"1","content":"typo up there :(\ni choose ****BD***","comment_id":"985025","timestamp":"1692428760.0"}],"upvote_count":"4","content":"wow! the comment section is divided\nhere's my honest take:\nit definitely not A and D. why?\nA: can't share resources using RAM across AWS Org\nE: ain't no way Savings Plan helps with \"data transfer charges\"\n\nI am left w BCD\nB: allows data transfer between linked accounts to be free of charge; rightly mentioned by @easytoo\nC: disabling \"cross-zone load balancing\" would saves money real quick\nD: reduces data transfer charges across AZs\n\nnow, D is for sure the correct answer!!!\ni am confused between B and C\n \ntho a good Solution Architect should not compromise on High Availability which arises if we opt for C!!!\nSo, i choose BA :)","timestamp":"1692428700.0","comment_id":"985024"},{"upvote_count":"3","content":"Selected Answer: BD\nB. Place the service provider applications and the service consumer applications in AWS accounts in the same organization. This helps avoid or minimize data transfer charges when communicating between accounts within the same AWS Organization.\n\nD. Ensure that service consumer compute resources use the Availability Zone-specific endpoint service by using the endpoint's local DNS name. This helps keep data transfer within the same Availability Zone, reducing data transfer charges.","comments":[{"timestamp":"1694108640.0","comment_id":"1001803","upvote_count":"2","content":"can you provide a link to prove that B is true?\nB. Place the service provider applications and the service consumer applications in AWS accounts in the same organization. This helps avoid or minimize data transfer charges when communicating between accounts within the same AWS Organization.","poster":"kjcncjek","comments":[{"upvote_count":"1","comment_id":"1064124","timestamp":"1699293180.0","poster":"yorkicurke","content":"this should do it. good luck.\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/resolve-dns-names-of-network-load-balancer-nodes-to-limit-cross-zone-traffic/"}]}],"timestamp":"1692104940.0","poster":"magmichal05","comment_id":"981693"},{"timestamp":"1690709820.0","comment_id":"967004","content":"Selected Answer: CD\nSubnets can't be shared across organizations:\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/vpc-sharing-a-new-approach-to-multiple-accounts-and-vpc-management/#:~:text=In%20AWS%20RAM%2C%20we%20can,within%20the%20same%20AWS%20Organization.","upvote_count":"4","poster":"MRL110"},{"poster":"ggrodskiy","content":"Correct BD.","upvote_count":"1","timestamp":"1690116780.0","comment_id":"960447"},{"poster":"Just_Ninja","content":"Selected Answer: BD\nWhy i choose B and D?\nB: If you use Oranisations and you create accounts, then the AZ-1a is the same Physical ID like it in the Child Account! \nOtherwise it´s possible that my AZ-1a and your AZ-1a in the same Region can have different Availability Zone ID´s for example euc1-az2 or euc1-az1.\n(If you are not sure about it, check your VPC Subnets there you can find it.)\n\nD: Ensure that service consumer compute resources use the Availability Zone-specific endpoint service by using the endpoint's local DNS name.\nExplanation: By using the endpoint's local DNS name, the service consumer's requests will be directed to the VPC endpoint within the same Availability Zone, reducing the data transfer across Availability Zones. This can help minimize the data transfer charges associated with using resources across different Availability Zones.","timestamp":"1690025640.0","upvote_count":"2","comment_id":"959443"},{"timestamp":"1689727620.0","comment_id":"956072","comments":[{"timestamp":"1690091400.0","upvote_count":"1","content":"Small typo....(subnets can be shared only within a organization). Rest all remains same.","poster":"study_aws1","comment_id":"960120"}],"upvote_count":"1","poster":"study_aws1","content":"A & B can form a combination here, not A & D (subnets cannot be shared only within a organization). But the question here is asking for Data Transfer costs reduction, not whole scale changes in Organization approach. \nWith this, C) & D) forms the right combination for the question."},{"poster":"kiss22","upvote_count":"2","comment_id":"953129","timestamp":"1689492540.0","content":"The answer is C and D, it can't be A because there are two AWS Organizations involved."},{"content":"Selected Answer: CD\ncross-zone traffic is where money going","poster":"NikkyDicky","upvote_count":"4","comment_id":"944864","timestamp":"1688663040.0"},{"timestamp":"1688050140.0","comment_id":"938274","poster":"BasselBuzz","upvote_count":"3","content":"Selected Answer: AD\nA By sharing the subnets that host the service provider applications using AWS Resource Access Manager (RAM), the service consumer applications can be deployed in the same organization's accounts. This allows the traffic between the service consumer and service provider applications to stay within the organization's network, reducing data transfer charges.\nD By using the Availability Zone-specific endpoint service's local DNS name, the service consumer compute resources can directly access the service provider applications within the same Availability Zone. This eliminates the need for cross-Availability Zone data transfer, thus reducing data transfer charges."},{"poster":"pupsik","comments":[{"timestamp":"1690849560.0","upvote_count":"1","poster":"easytoo","comment_id":"968631","content":"Changed to B-D now pupsik. \nPlease see amendment to my previous answer."}],"content":"Selected Answer: AD\nExplanation by @easytoo is spot on.","upvote_count":"1","comment_id":"935658","timestamp":"1687887780.0"},{"poster":"PhuocT","content":"Selected Answer: BD\nB and D seem reasonable choices","comment_id":"932788","upvote_count":"3","timestamp":"1687625760.0"},{"upvote_count":"4","comment_id":"932477","poster":"SmileyCloud","content":"Selected Answer: CD\nCD saves money. You'll loose HA, but you will save money.\nhttps://www.examtopics.com/discussions/amazon/view/69648-exam-aws-certified-solutions-architect-professional-topic-1/","timestamp":"1687605180.0"},{"timestamp":"1687581000.0","content":"its CD","upvote_count":"2","poster":"ozelllll","comment_id":"932160"},{"upvote_count":"2","content":"Selected Answer: AD\nA. Use AWS Resource Access Manager to share the subnets that host the service provider applications with other accounts in the organization. This allows the service consumer applications to access the service provider applications directly within the same organization, without incurring data transfer charges.\n\nD. Ensure that service consumer compute resources use the Availability Zone-specific endpoint service by using the endpoint's local DNS name. This ensures that data transfer between the service consumer applications and the service provider applications stays within the same Availability Zone, reducing data transfer charges.","comments":[{"comment_id":"995610","poster":"Gabehcoud","content":"How can we share a subnet outside the organization? The question says \"there are 2 AWS organizations\".","timestamp":"1693535520.0","upvote_count":"1"}],"timestamp":"1687546380.0","comment_id":"931877","poster":"easytoo"}],"choices":{"C":"Turn off cross-zone load balancing for the Network Load Balancer in all service provider application deployments.","A":"Use AWS Resource Access Manager to share the subnets that host the service provider applications with other accounts in the organization.","B":"Place the service provider applications and the service consumer applications in AWS accounts in the same organization.","E":"Create a Savings Plan that provides adequate coverage for the organization's planned inter-Availability Zone data transfer usage.","D":"Ensure that service consumer compute resources use the Availability Zone-specific endpoint service by using the endpoint's local DNS name."},"topic":"1","answer":"CD","question_text":"A company's solutions architect is analyzing costs of a multi-application environment. The environment is deployed across multiple Availability Zones in a single AWS Region. After a recent acquisition, the company manages two organizations in AWS Organizations. The company has created multiple service provider applications as AWS PrivateLink-powered VPC endpoint services in one organization. The company has created multiple service consumer applications in the other organization.\n\nData transfer charges are much higher than the company expected, and the solutions architect needs to reduce the costs. The solutions architect must recommend guidelines for developers to follow when they deploy services. These guidelines must minimize data transfer charges for the whole environment.\n\nWhich guidelines meet these requirements? (Choose two.)","url":"https://www.examtopics.com/discussions/amazon/view/113131-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_ET":"CD","unix_timestamp":1687546380,"answer_description":"","exam_id":33,"answer_images":[],"isMC":true},{"id":"F4UjcnSmyC2kiEkumxqf","answer":"A","answer_ET":"A","url":"https://www.examtopics.com/discussions/amazon/view/112879-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_description":"","question_images":[],"unix_timestamp":1687396380,"isMC":true,"topic":"1","discussion":[{"content":"Selected Answer: A\nFile Gateway == SMB , NFS\n Volumes Gateway == iSCSI \n Tape Gateway = VTL","comment_id":"941300","timestamp":"1704245580.0","upvote_count":"32","poster":"SkyZeroZx"},{"upvote_count":"1","comment_id":"1333545","timestamp":"1735483140.0","content":"Selected Answer: A\nGuys, options B and C are exactly same. :)","poster":"SIJUTHOMASP"},{"content":"Ans D \nhe most cost-effective solution for moving the backups to S3 is D. Deploy an AWS Storage Gateway volume gateway, create an SMB file share, and automate data copies to S3.\n\nHere's why:\n\nCost-effectiveness: Volume gateways use Amazon EBS volumes for local storage, which is typically more cost-effective than Amazon FSx for Windows File Server for storing large amounts of data. Additionally, this approach avoids the need for additional backups within Amazon FSx, further reducing costs.\n\nDirect Connect utilization: Leveraging the existing Direct Connect connection optimizes network bandwidth for transferring data to S3, minimizing latency and potential data transfer charges.\n\nAutomated backups: Automating copies of the nightly exports to S3 ensures reliable backups and minimizes manual intervention.","poster":"duriselvan","comment_id":"1092589","timestamp":"1718026080.0","upvote_count":"1"},{"comment_id":"1078881","timestamp":"1716496800.0","content":"Selected Answer: A\nOption A","poster":"career360guru","upvote_count":"2"},{"comments":[{"upvote_count":"2","timestamp":"1714735320.0","content":"But the answer A never mentions S3 file gateway?","comment_id":"1061460","poster":"NolaHOla"}],"content":"Selected Answer: A\nif you read the end of the following link's paragraph, its right there in documentation;\nhttps://aws.amazon.com/storagegateway/features/#Gateway_Types\nunder \" Amazon S3 File Gateway \"\nCustomers can use Amazon S3 File Gateway to back up on-premises file data as objects in Amazon S3 (including Microsoft SQL Server and Oracle databases and logs), and for hybrid cloud workflows using data generated by on-premises applications for processing by AWS services such as machine learning or big data analytics.","comment_id":"1060748","timestamp":"1714665420.0","upvote_count":"1","poster":"yorkicurke"},{"timestamp":"1708489080.0","comment_id":"986167","upvote_count":"1","poster":"SK_Tyagi","content":"Selected Answer: A\nhttps://aws.amazon.com/storagegateway/features/"},{"timestamp":"1707079920.0","poster":"rafael796","upvote_count":"1","content":"Selected Answer: A\nfile gateway = most cheap","comment_id":"972401"},{"content":"Selected Answer: A\nA - SMB mount = file gwy","comment_id":"945021","poster":"NikkyDicky","timestamp":"1704583980.0","upvote_count":"2"},{"timestamp":"1703641620.0","comment_id":"934874","upvote_count":"2","poster":"RockyLeon","content":"Selected Answer: A\nfile gateway -> used to store file inside s3\n volume gateway -> used to store file in on-premises using iSCSI connectivity"},{"upvote_count":"2","timestamp":"1703523900.0","comment_id":"933709","poster":"Jackhemo","content":"Using Olabiba.ai to learn not to find an answer:\n\nJack: Labiba, what is the Microsoft SQL Server database export is it block or file?\n\noLabiba: The Microsoft SQL Server database export is typically a block-level backup. It captures the data at the database level, including the schema, tables, and records, and stores it in a binary format. This allows for efficient backup and restoration of the database.\n\nIn summary, if you primarily need file-level access to your backups, File Gateway is a better choice. If you require block-level storage and want to optimize for low-latency access, Volume Gateway is a better fit.\n\nLet me know if you know the answer now."},{"comment_id":"932833","timestamp":"1703448600.0","poster":"Maria2023","content":"Selected Answer: A\nFile Gateway could be mapped as SMB file share and used by the database or other automation to transfer database backups. Volume Gateway is more used to perform volume snapshots on the on-premise system so I don't believe it's a sustainable approach here.","upvote_count":"3"},{"upvote_count":"2","poster":"SmileyCloud","content":"Selected Answer: A\nIt's A (file gateway). Volume gateway is iSCSI.","comment_id":"932529","timestamp":"1703427360.0"},{"content":"Selected Answer: D\nolabiba.ai says D\nOption D: Using an AWS Storage Gateway volume gateway allows you to write the nightly database exports to an SMB file share on the volume gateway, which can be stored locally and automatically backed up to an S3 bucket. This solution is cost-effective as it utilizes the existing Direct Connect connection and requires minimal additional infrastructure.","comment_id":"931952","upvote_count":"2","timestamp":"1703373420.0","poster":"Jackhemo"},{"poster":"easytoo","comment_id":"931881","upvote_count":"2","timestamp":"1703365020.0","content":"d-d-d-d-d-d\n\nBy deploying an AWS Storage Gateway volume gateway within the VPC connected to the Direct Connect connection, the company can leverage the high-speed, low-latency connection to transfer the nightly database exports to the SMB file share on the volume gateway. This allows for efficient and reliable data transfer.\n\nAutomating copies of this data from the SMB file share to an S3 bucket provides a cost-effective solution for storing the backups in more robust cloud storage on Amazon S3. The company can take advantage of the durability, scalability, and cost-effectiveness of S3 for long-term storage."},{"upvote_count":"2","timestamp":"1703275080.0","content":"Selected Answer: A\nBetween A and D:\n\nwrite to local drive can also be a network drive mapped to the windows server. therefore SME file share is enough (A), D is Block level, for sure will cost more.\n\nthe File Gateway is designed for file-level access and presents Amazon S3 storage as a file share, while the Volume Gateway provides block-level access and appears as local block storage volumes. The choice between the two depends on the specific needs and requirements of your applications and data access patterns.","poster":"nexus2020","comment_id":"930873","comments":[{"comment_id":"1103729","timestamp":"1719084420.0","upvote_count":"1","content":"The backend of storage gateway is actually S3 storage, which means both volume gateway and file gateway share the same cost for storage. \nAnd the gateway cost is the same according to aws pricing: https://aws.amazon.com/storagegateway/pricing/. \nso where did you get the \"D is Block level, for sure will cost more\"?","poster":"bjexamprep"}]},{"timestamp":"1703214780.0","comment_id":"930037","upvote_count":"1","comments":[{"content":"I might be wrong with my theory. Going with A","timestamp":"1703915220.0","upvote_count":"1","poster":"bhanus","comment_id":"938781"},{"comment_id":"931954","timestamp":"1703373660.0","content":"Use olabiba.ai. It is better.","upvote_count":"1","poster":"Jackhemo","comments":[{"poster":"PhuocT","upvote_count":"2","timestamp":"1703404800.0","content":"Q: are you using openAI as your AI engine?\nolabiba.com: Yes, I am powered by OpenAI's advanced AI technology. It allows me to understand and respond to your messages in a conversational manner. OpenAI provides the foundation for my capabilities, but the Olabiba team has also customized and trained me to better suit your needs. So, feel free to ask me anything or share your thoughts!","comment_id":"932238"}]},{"upvote_count":"1","poster":"gd1","timestamp":"1703371260.0","content":"Volume will iSCSI so hat is out. Therefor A is correct","comment_id":"931941"}],"content":"I am between A and D. ChatGpt says A. But The reason why I think D is because, the question says backups are written to local drive(which means its a volume on onpremises machine). So I thought a volume can be attached to volume gateway. But ChatGPT says In terms of cost-effectiveness and simplicity, option A is a better choice. It involves using an AWS Storage Gateway file gateway, which directly stores the data as objects in Amazon S3 without the need for on-premises storage. This eliminates the complexity and costs associated with maintaining an on-premises volume gateway.","poster":"bhanus"}],"choices":{"A":"Create a new S3 bucket. Deploy an AWS Storage Gateway file gateway within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to the new SMB file share.","D":"Create a new S3 bucket. Deploy an AWS Storage Gateway volume gateway within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to the new SMB file share on the volume gateway, and automate copies of this data to an S3 bucket.","B":"Create an Amazon FSx for Windows File Server Single-AZ file system within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to an SMB file share on the Amazon FSx file system. Enable nightly backups.","C":"Create an Amazon FSx for Windows File Server Multi-AZ file system within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to an SMB file share on the Amazon FSx file system. Enable nightly backups."},"question_id":148,"answer_images":[],"answers_community":["A (96%)","4%"],"timestamp":"2023-06-22 03:13:00","exam_id":33,"question_text":"A company has an on-premises Microsoft SQL Server database that writes a nightly 200 GB export to a local drive. The company wants to move the backups to more robust cloud storage on Amazon S3. The company has set up a 10 Gbps AWS Direct Connect connection between the on-premises data center and AWS.\n\nWhich solution meets these requirements MOST cost-effectively?"},{"id":"PX2wormaxhheUCVMSRR1","url":"https://www.examtopics.com/discussions/amazon/view/112880-exam-aws-certified-solutions-architect-professional-sap-c02/","timestamp":"2023-06-22 03:15:00","unix_timestamp":1687396500,"question_images":[],"isMC":true,"question_id":149,"answer_ET":"B","question_text":"A company needs to establish a connection from its on-premises data center to AWS. The company needs to connect all of its VPCs that are located in different AWS Regions with transitive routing capabilities between VPC networks. The company also must reduce network outbound traffic costs, increase bandwidth throughput, and provide a consistent network experience for end users.\n\nWhich solution will meet these requirements?","discussion":[{"content":"Selected Answer: B\nIn fact site to site VPN would be more affordable than deploying a Direct Connect leased line. However, AWS also wants to market their product by stating that there is a need to increase throughput (site to site only can achieve max of 1.25Gbps) and consistent user experience (AWS Direct Connect > Site-to-Site VPN) so B would be a better choice.","upvote_count":"9","poster":"Pupu86","timestamp":"1700740380.0","comment_id":"1078389"},{"comment_id":"1246031","content":"B, for sure.\nFor a consistent network experience","poster":"gfhbox0083","timestamp":"1720689000.0","upvote_count":"1"},{"content":"Selected Answer: B\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-aws-transit-gateway.html","timestamp":"1710524400.0","comment_id":"1174429","poster":"TonytheTiger","upvote_count":"1"},{"comment_id":"1078883","content":"Selected Answer: B\nOption B may not be most cost-effective best option in terms of performance.","upvote_count":"3","timestamp":"1700779380.0","poster":"career360guru"},{"comment_id":"1058748","content":"Anyone can explain that why Site to Site VPN not valid?","upvote_count":"1","poster":"joleneinthebackyard","timestamp":"1698754440.0","comments":[{"timestamp":"1717057620.0","poster":"fartosh","comment_id":"1221462","content":"The company wants to increase bandwidth throughput, which is gained by establishing Direct Connect.","upvote_count":"2"}]},{"content":"what if the situation is 1 AWS account, different VPC's across different regions? Can we still use a TGW?","timestamp":"1692439740.0","comment_id":"985105","upvote_count":"1","poster":"Gabehcoud"},{"upvote_count":"1","poster":"hexie","comment_id":"945541","content":"Selected Answer: B\nB.\nCant be D because TGW doesnt support transitive connections, so if users connect to a VPN it invalidate this options.\nA and C are skippable on the first phrase.","timestamp":"1688723280.0"},{"timestamp":"1688679300.0","poster":"NikkyDicky","comment_id":"945023","content":"Selected Answer: B\nB no doubt","upvote_count":"1"},{"timestamp":"1687716540.0","comments":[{"poster":"rxhan","content":"Mr. copy and paste","timestamp":"1690643220.0","upvote_count":"3","comment_id":"966490"}],"content":"Selected Answer: B\ndirect connect + vpc = direct connect gw + TGW. so B","comment_id":"933804","poster":"SkyZeroZx","upvote_count":"3"},{"comment_id":"932836","content":"Selected Answer: B\nTransit gateway is a regional service but you can peer different TGs in different regions\nhttps://aws.amazon.com/about-aws/whats-new/2019/12/aws-transit-gateway-supports-inter-region-peering/","upvote_count":"1","poster":"Maria2023","timestamp":"1687630800.0"},{"content":"Selected Answer: B\nB. No need for D and S2S VPN.","poster":"SmileyCloud","comment_id":"932543","timestamp":"1687610040.0","upvote_count":"1"},{"comment_id":"931135","content":"BBBBBBBBBBB?","upvote_count":"1","poster":"aragon_saa","timestamp":"1687479600.0"},{"poster":"nexus2020","upvote_count":"3","comment_id":"930917","content":"Selected Answer: B\ndirect connect + vpc = direct connect gw + TGW. so B","timestamp":"1687458540.0"}],"answer_description":"","exam_id":33,"answer":"B","answers_community":["B (100%)"],"answer_images":[],"choices":{"A":"Create an AWS Site-to-Site VPN connection between the on-premises data center and a new central VPC. Create VPC peering connections that initiate from the central VPC to all other VPCs.","C":"Create an AWS Site-to-Site VPN connection between the on-premises data center and a new central VPUse a transit gateway with dynamic routing. Connect the transit gateway to all other VPCs.","B":"Create an AWS Direct Connect connection between the on-premises data center and AWS. Provision a transit VIF, and connect it to a Direct Connect gateway. Connect the Direct Connect gateway to all the other VPCs by using a transit gateway in each Region.","D":"Create an AWS Direct Connect connection between the on-premises data center and AWS. Establish an AWS Site-to-Site VPN connection between all VPCs in each Region. Create VPC peering connections that initiate from the central VPC to all other VPCs."},"topic":"1"},{"id":"BKgKYfj2d7uf6iiW7YOx","answer":"D","answer_ET":"D","url":"https://www.examtopics.com/discussions/amazon/view/112882-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_description":"","question_images":[],"unix_timestamp":1687396680,"topic":"1","isMC":true,"discussion":[{"comment_id":"930046","timestamp":"1687396860.0","content":"Selected Answer: D\nD - Cross account role should be created in destination(member) account. The role has trust entity to master account.","upvote_count":"6","poster":"bhanus"},{"timestamp":"1702449120.0","upvote_count":"1","poster":"duriselvan","content":"A is ans\nA. Create an IAM user and a cross-account role in the management account. Configure the cross-account role with least privilege access to the member accounts.\n\nHere's why:\n\nCross-account roles: Provide a secure and managed way for users or services in one AWS account to access resources in another account.\nLeast privilege access: Configure the cross-account role with the minimum permissions needed to stop or terminate resources in the member accounts, minimizing potential security risks.\nCentralized control: Maintaining user credentials and access in the management account simplifies centralized management and auditing.","comment_id":"1095202","comments":[{"upvote_count":"1","poster":"helloworldabc","timestamp":"1724460000.0","content":"just D","comment_id":"1271476"}]},{"upvote_count":"2","content":"Selected Answer: D\nOption D","comment_id":"1078885","timestamp":"1700779560.0","poster":"career360guru"},{"timestamp":"1692721740.0","content":"Hmm, seems like alot of work. What if the question was, In the management account, a solutions architect needs to create an IAM user that can stop or terminate resources in 100 organization or member accounts? Asked AI \"Using AWS Organizations, can you create both IAM user and permission sets in the management account for accessing managed organization resources?\" The answer was Yes.","upvote_count":"1","poster":"skyhiker","comment_id":"987607"},{"content":"Selected Answer: D\nits a D","timestamp":"1688679420.0","comment_id":"945024","upvote_count":"2","poster":"NikkyDicky"},{"timestamp":"1687703820.0","content":"Selected Answer: D\nOne user is sufficient and you need cross-account role.","poster":"SmileyCloud","comment_id":"933691","upvote_count":"4"},{"timestamp":"1687453080.0","upvote_count":"2","poster":"MoussaNoussa","comment_id":"930810","content":"D - Cross account role should be created in destination(member) account. The role has trust entity to master account."},{"poster":"bhanus","comment_id":"930044","content":"Selected Answer: D\nD - Cross account role should be created in destination account(which is member account) and trust policy should be there","upvote_count":"3","timestamp":"1687396680.0"}],"choices":{"B":"Create an IAM user in each member account. In the management account, create a cross-account role that has least privilege access. Grant the IAM users access to the cross-account role by using a trust policy.","D":"Create an IAM user in the management account. In the member accounts, create cross-account roles that have least privilege access. Grant the IAM user access to the roles by using a trust policy.","A":"Create an IAM user and a cross-account role in the management account. Configure the cross-account role with least privilege access to the member accounts.","C":"Create an IAM user in the management account. In the member accounts, create an IAM group that has least privilege access. Add the IAM user from the management account to each IAM group in the member accounts."},"question_id":150,"answer_images":[],"answers_community":["D (100%)"],"timestamp":"2023-06-22 03:18:00","exam_id":33,"question_text":"A company is migrating its development and production workloads to a new organization in AWS Organizations. The company has created a separate member account for development and a separate member account for production. Consolidated billing is linked to the management account. In the management account, a solutions architect needs to create an IAM user that can stop or terminate resources in both member accounts.\n\nWhich solution will meet this requirement?"}],"exam":{"isBeta":false,"name":"AWS Certified Solutions Architect - Professional SAP-C02","isImplemented":true,"id":33,"isMCOnly":true,"provider":"Amazon","numberOfQuestions":529,"lastUpdated":"11 Apr 2025"},"currentPage":30},"__N_SSP":true}