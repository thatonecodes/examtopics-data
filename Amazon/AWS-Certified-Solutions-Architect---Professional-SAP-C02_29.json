{"pageProps":{"questions":[{"id":"IEfL0ikGcJChuNga5rfn","answers_community":["B (100%)"],"discussion":[{"timestamp":"1703299440.0","content":"Selected Answer: B\nA. Incorrect: RI's Supports only EC2 instances. \nB. Correct: Compute savings plan supports EC2, Fargate and Lambda. Applied in Organization's management account.\nC. Incorrect: RI's Supports only EC2 instances and Changes to be applied at Organizations management account.\nD. Incorrect: Instance Saving plan supports only EC2.","comment_id":"931144","upvote_count":"14","poster":"elanelans"},{"upvote_count":"1","content":"Selected Answer: B\nB - \"Compute Savings Plans provide the most flexibility and help to reduce your costs by up to 66%. These plans automatically apply to EC2 instance usage regardless of instance family, size, AZ, Region, OS or tenancy, and also apply to Fargate or Lambda usage.\n\nhttps://aws.amazon.com/savingsplans/compute-pricing/","comment_id":"1189423","timestamp":"1728054900.0","poster":"titi_r"},{"content":"Answer B. Compute Savings plan covers EC2, Fargate & Lambda. Instance Savings plan only for EC2 instances.","comment_id":"1084486","upvote_count":"3","poster":"shaaam80","timestamp":"1717075020.0"},{"upvote_count":"1","comment_id":"1078858","content":"Selected Answer: B\nOption B","poster":"career360guru","timestamp":"1716494160.0"},{"comment_id":"944240","poster":"NikkyDicky","upvote_count":"1","content":"Selected Answer: B\nits a B","timestamp":"1704514500.0"},{"upvote_count":"2","timestamp":"1703531580.0","poster":"SkyZeroZx","comment_id":"933779","content":"Selected Answer: B\nA. Incorrect: RI's Supports only EC2 instances.\nB. Correct: Compute savings plan supports EC2, Fargate and Lambda. Applied in Organization's management account.\nC. Incorrect: RI's Supports only EC2 instances and Changes to be applied at Organizations management account.\nD. Incorrect: Instance Saving plan supports only EC2."},{"content":"Selected Answer: B\nB, magic keywords - Management account and Compute savings Plan.","comment_id":"931139","poster":"SmileyCloud","timestamp":"1703298360.0","upvote_count":"1"},{"timestamp":"1703268780.0","poster":"nexus2020","comment_id":"930769","upvote_count":"1","content":"Selected Answer: B\nCompute Savings plan is made for this usage type"},{"content":"Selected Answer: B\nB- compute savings plans covers all ec2, fargate, lambda.","timestamp":"1703202360.0","poster":"bhanus","upvote_count":"1","comment_id":"929966"}],"answer_ET":"B","answer_images":[],"unix_timestamp":1687383960,"question_text":"A company runs many workloads on AWS and uses AWS Organizations to manage its accounts. The workloads are hosted on Amazon EC2. AWS Fargate. and AWS Lambda. Some of the workloads have unpredictable demand. Accounts record high usage in some months and low usage in other months.\n\nThe company wants to optimize its compute costs over the next 3 years. A solutions architect obtains a 6-month average for each of the accounts across the organization to calculate usage.\n\nWhich solution will provide the MOST cost savings for all the organization's compute usage?","question_id":141,"choices":{"A":"Purchase Reserved Instances for the organization to match the size and number of the most common EC2 instances from the member accounts.","D":"Purchase an EC2 Instance Savings Plan for each member account from the management account based on EC2 usage data from the last 6 months.","B":"Purchase a Compute Savings Plan for the organization from the management account by using the recommendation at the management account level.","C":"Purchase Reserved Instances for each member account that had high EC2 usage according to the data from the last 6 months."},"exam_id":33,"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/112866-exam-aws-certified-solutions-architect-professional-sap-c02/","timestamp":"2023-06-21 23:46:00","question_images":[],"topic":"1","answer":"B","answer_description":""},{"id":"72Qw58iADC8sMOIBspjX","answers_community":["A (100%)"],"question_text":"A company has hundreds of AWS accounts. The company uses an organization in AWS Organizations to manage all the accounts. The company has turned on all features.\n\nA finance team has allocated a daily budget for AWS costs. The finance team must receive an email notification if the organization's AWS costs exceed 80% of the allocated budget. A solutions architect needs to implement a solution to track the costs and deliver the notifications.\n\nWhich solution will meet these requirements?","answer_description":"","exam_id":33,"topic":"1","answer_images":[],"discussion":[{"content":"Selected Answer: A\nA. Makes sense.\nB. Trusted advisor not required.\nC. Control Tower not required.\nD. Budgets can be managed in Org's Mgmt account itself.","comment_id":"931150","upvote_count":"9","timestamp":"1687482600.0","poster":"elanelans"},{"comment_id":"1360092","upvote_count":"1","content":"Selected Answer: A\nAWS Budgets can help for daily tracking and notify through SNS.","timestamp":"1740218340.0","poster":"85b5b55"},{"timestamp":"1723199160.0","upvote_count":"2","poster":"kgpoj","content":"Selected Answer: A\nA: AWS Budgets + SNS = Easy budget (daily) tracking and alerts\n\nB: Trusted Advisor is for recommendations, not daily budgets.\n\nC: Control Tower is for governance, not budget alerts\n\nD: Complex setup with no added value over AWS Budgets","comment_id":"1262860"},{"poster":"career360guru","content":"Selected Answer: A\nOption A","comment_id":"1078859","timestamp":"1700776620.0","upvote_count":"1"},{"upvote_count":"3","timestamp":"1688810340.0","poster":"nicecurls","content":"Selected Answer: A\nofc it's Ahttps://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-professional-sap-c02/view/#","comment_id":"946367"},{"poster":"NikkyDicky","timestamp":"1688609760.0","comment_id":"944241","upvote_count":"2","content":"Selected Answer: A\nstraight A"},{"comments":[{"upvote_count":"6","poster":"rxhan","content":"you copy and paste other people answers","comment_id":"966378","timestamp":"1690631640.0"}],"upvote_count":"2","comment_id":"941293","poster":"SkyZeroZx","timestamp":"1688339880.0","content":"Selected Answer: A\nA. Makes sense.\nB. Trusted advisor not required.\nC. Control Tower not required.\nD. Budgets can be managed in Org's Mgmt account itself."},{"comment_id":"931852","poster":"easytoo","content":"a-a-a-a-a-a-a","timestamp":"1687543980.0","upvote_count":"1"},{"poster":"SmileyCloud","timestamp":"1687519860.0","comment_id":"931525","content":"Selected Answer: A\nThis one is simple. A","upvote_count":"1"},{"timestamp":"1687450680.0","content":"Selected Answer: A\nA, simple one","comment_id":"930775","poster":"nexus2020","upvote_count":"1"},{"upvote_count":"1","poster":"MoussaNoussa","timestamp":"1687443120.0","comment_id":"930653","content":"A is the answer"},{"timestamp":"1687384020.0","upvote_count":"1","poster":"bhanus","content":"Selected Answer: A\nA is the answer","comment_id":"929968"}],"isMC":true,"unix_timestamp":1687384020,"question_id":142,"answer":"A","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/112867-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_ET":"A","timestamp":"2023-06-21 23:47:00","choices":{"A":"In the organization's management account, use AWS Budgets to create a budget that has a daily period. Add an alert threshold and set the value to 80%. Use Amazon Simple Notification Service (Amazon SNS) to notify the finance team.","D":"Configure the member accounts to save a daily AWS Cost and Usage Report to an Amazon S3 bucket in the organization's management account. Use Amazon EventBridge to schedule a daily Amazon Athena query to calculate the organization’s costs. Configure Athena to send an Amazon CloudWatch alert if the total costs are more than 80% of the allocated budget. Use Amazon Simple Notification Service (Amazon SNS) to notify the finance team.","C":"Register the organization with AWS Control Tower. Activate the optional cost control (guardrail). Set a control (guardrail) parameter of 80%. Configure control (guardrail) notification preferences. Use Amazon Simple Notification Service (Amazon SNS) to notify the finance team.","B":"In the organization’s management account, set up the organizational view feature for AWS Trusted Advisor. Create an organizational view report for cost optimization. Set an alert threshold of 80%. Configure notification preferences. Add the email addresses of the finance team."}},{"id":"Q33xG7hAMjq86LVkh3wR","answer":"C","topic":"1","answer_ET":"C","question_text":"A company provides auction services for artwork and has users across North America and Europe. The company hosts its application in Amazon EC2 instances in the us-east-1 Region. Artists upload photos of their work as large-size. high-resolution image files from their mobile phones to a centralized Amazon S3 bucket created in the us-east-1 Region. The users in Europe are reporting slow performance for their image uploads.\n\nHow can a solutions architect improve the performance of the image upload process?","url":"https://www.examtopics.com/discussions/amazon/view/112868-exam-aws-certified-solutions-architect-professional-sap-c02/","question_id":143,"discussion":[{"upvote_count":"28","poster":"chico2023","timestamp":"1708275660.0","comments":[{"content":"Kudos to you for such a great explanation!","comment_id":"1107223","poster":"Jay_2pt0_1","upvote_count":"2","timestamp":"1719517260.0"}],"comment_id":"984601","content":"Selected Answer: C\nMain point of the question: \"The users in Europe are reporting slow performance for their image uploads.\"\nHow do we improve performance? If we look on the latency side, sure, S3 Transfer Acceleration (option C), but the question puts another variable to our scenario: \"Artists upload photos of their work as large-size. high-resolution image files from their mobile phones...\"\nIf you just look at that above, you would switch to A as we can improve upload with multipart.\n\nHere comes the plot twist \"The users in Europe are reporting slow performance for their image uploads.\" - Meaning, in \"Europe\", not in the \"NA\". Of course! The bucket in the US... So yeah, question really bad, not objective (in my pov) and with lots of interpretations, but C would help them with the perception of performance in this context."},{"poster":"kpcert","comment_id":"1227984","content":"Selected Answer: C\nBetween A and C, I would choose C - Transfer Acceleration, as this issue is focusing on improving the upload performance across the region","timestamp":"1733854320.0","upvote_count":"1"},{"poster":"rohan0411","comment_id":"1227636","timestamp":"1733807700.0","upvote_count":"1","content":"Why not B ?"},{"upvote_count":"3","content":"Selected Answer: C\nOption C. As the users in Europe only are facing this issue. A would improve upload performance overall for both US and Europe.","timestamp":"1716494820.0","poster":"career360guru","comment_id":"1078862"},{"content":"I believe this question should rightfully be a multi-choice question where A and C are the answer together to solve this problem statement\n\nhttps://aws.amazon.com/blogs/compute/uploading-large-objects-to-amazon-s3-using-multipart-upload-and-transfer-acceleration/","timestamp":"1716450120.0","upvote_count":"2","comment_id":"1078300","poster":"Pupu86"},{"comment_id":"987479","content":"I would choose A. Why does C say \"Configure the buckets [more than one] to use S3 Transfer Acceleration?\nSometimes you have to hate how these questions and answers are worded.","upvote_count":"1","poster":"skyhiker","timestamp":"1708616820.0","comments":[{"poster":"skyhiker","upvote_count":"1","timestamp":"1708617120.0","content":"C would be the answer if the 's' was removed. Will to go with C.","comment_id":"987485"}]},{"content":"Selected Answer: A\nI have some doubts about this question, it makes more sense to use multipart upload to split the file and gain upload speed. AWS Transfer Accelerator seems to be applied to reduce delay.http\ns://aws.amazon.com/pt/blogs/compute/uploading-large-objects-to-amazon-s3-using-multipart-upload-and-transfer-acceleration/","comment_id":"967535","timestamp":"1706658060.0","upvote_count":"1","poster":"RGR21"},{"comment_id":"960715","content":"Correct C.","upvote_count":"1","poster":"ggrodskiy","timestamp":"1706040780.0"},{"poster":"NikkyDicky","timestamp":"1704561720.0","upvote_count":"1","content":"Selected Answer: C\nC\nwould be good in combination with A, but better as a standalone choice","comment_id":"944793"},{"poster":"Christina666","upvote_count":"1","timestamp":"1704461100.0","comment_id":"943657","content":"Selected Answer: C\nupload performance-> transfer acceleration"},{"comment_id":"938310","upvote_count":"1","content":"Selected Answer: C\ncorrect is C","timestamp":"1703871060.0","poster":"javitech83"},{"comment_id":"935642","comments":[{"poster":"YodaMaster","comment_id":"944321","upvote_count":"4","content":"Using your link, the tests mentioned show C is faster\nSingle upload with transfer acceleration 40% faster\nMultipart upload without transfer acceleration 38% faster","timestamp":"1704522600.0"},{"poster":"SeemaDataReader","comment_id":"1130117","content":"Reading carefully into the blog looks like the author did some maths wrong. \nMultipart upload took 43s which is 40% faster than base of 72s\nTransfer acceleration took 45s which is 38% faster than base of 72s. \nSo based on this multipart gives better performance","timestamp":"1721774880.0","comments":[{"timestamp":"1739574000.0","comment_id":"1356616","content":"Double check your math brother..","poster":"shmoeee","upvote_count":"1"}],"upvote_count":"1"}],"poster":"pupsik","timestamp":"1703705340.0","upvote_count":"1","content":"Selected Answer: A\nTransfer Acceleration doesn't guarantee a significant increase in upload speed.\nA multi-part upload on other hand does, because it uploads multiple smaller chunks of the files in parallel.\n\nIdeally multi-part upload and Transfer Accelerator should be deployed together. If we had to pick only one of the two, multi-part upload would result in better performance.\n\nhttps://aws.amazon.com/blogs/compute/uploading-large-objects-to-amazon-s3-using-multipart-upload-and-transfer-acceleration/"},{"poster":"SkyZeroZx","content":"Selected Answer: C\nC. https://aws.amazon.com/s3/transfer-acceleration/","timestamp":"1703531940.0","comment_id":"933784","upvote_count":"1"},{"content":"Selected Answer: C\nC. https://aws.amazon.com/s3/transfer-acceleration/","poster":"SmileyCloud","timestamp":"1703338980.0","upvote_count":"2","comment_id":"931534"},{"content":"C of course","timestamp":"1703261640.0","upvote_count":"1","comment_id":"930654","poster":"MoussaNoussa"},{"timestamp":"1703202720.0","comment_id":"929975","poster":"bhanus","upvote_count":"1","content":"Selected Answer: C\nC - Transfer acceleration. S3 Transfer Acceleration utilizes the Amazon CloudFront global network of edge locations to accelerate the transfer of data to and from S3 buckets. By enabling S3 Transfer Acceleration on the centralized S3 bucket, the users in Europe will experience faster uploads as their data will be routed through the closest CloudFront edge location."}],"answer_description":"","question_images":[],"choices":{"C":"Configure the buckets to use S3 Transfer Acceleration.","A":"Redeploy the application to use S3 multipart uploads.","D":"Create an Auto Scaling group for the EC2 instances and create a scaling policy.","B":"Create an Amazon CloudFront distribution and point to the application as a custom origin."},"isMC":true,"exam_id":33,"answer_images":[],"timestamp":"2023-06-21 23:52:00","unix_timestamp":1687384320,"answers_community":["C (95%)","5%"]},{"id":"bVyqzHCaXQAHscfdBBh6","answer":"D","topic":"1","answer_ET":"D","question_text":"A company wants to containerize a multi-tier web application and move the application from an on-premises data center to AWS. The application includes web. application, and database tiers. The company needs to make the application fault tolerant and scalable. Some frequently accessed data must always be available across application servers. Frontend web servers need session persistence and must scale to meet increases in traffic.\n\nWhich solution will meet these requirements with the LEAST ongoing operational overhead?","url":"https://www.examtopics.com/discussions/amazon/view/112869-exam-aws-certified-solutions-architect-professional-sap-c02/","question_id":144,"discussion":[{"content":"Selected Answer: D\nA looked good until \"store session data in SQS\".","comment_id":"935650","timestamp":"1687887300.0","upvote_count":"23","poster":"pupsik"},{"poster":"SkyZeroZx","upvote_count":"15","content":"Selected Answer: D\nwhat a worst ques\nA - Why do you need SQS to store web sever session data. SQS is for decoupling services\nB - EBS multi attach is for SAME availibility zone. The ques says multipel availibility zones\nC - Why do you need EFS to store web sever session data. Its damn expensive\nD - Better answer- But again why need for EKS.\nIf I were to choose one option, its D as its better compared to ABC","comment_id":"933788","timestamp":"1687713780.0"},{"timestamp":"1728333060.0","upvote_count":"3","poster":"JoeTromundo","content":"Selected Answer: D\nBy exclusion of the other options, the least worst answer is D.","comment_id":"1294460"},{"upvote_count":"1","timestamp":"1725681900.0","poster":"liuliangzhou","comment_id":"1279878","content":"Selected Answer: D\nB,D can do it all.\nMultiple EC2 accesses a single storage using EFS and EBS, with priority given to EFS file storage.\nDynamoDB can also be used for session storage.\nhttps://docs.aws.amazon.com/aws-sdk-php/v2/guide/feature-dynamodb-session-handler.html"},{"comment_id":"1204440","content":"one of the poor Question... so answer we give poor... its D. because i cant choose ABC","upvote_count":"4","timestamp":"1714468260.0","poster":"43c89f4"},{"content":"Selected Answer: B\nI think that the issue of multi-attach EBS in one AZ is dealt with by the manner in which it is explained. It is the EC2 that are distributed in Multi-AZ not the EBS. Just my pov.","comment_id":"1089076","poster":"ayadmawla","upvote_count":"3","timestamp":"1701850380.0"},{"poster":"career360guru","timestamp":"1700777520.0","comment_id":"1078864","upvote_count":"2","content":"Selected Answer: D\nOption D, Though C is also possible but Multi-attach EBS has higher operational overhead."},{"timestamp":"1696075380.0","poster":"covabix879","comment_id":"1021434","upvote_count":"1","content":"Selected Answer: D\nDue to operational efficiency D is better choice compared to B."},{"poster":"task_7","upvote_count":"1","comment_id":"1020607","timestamp":"1695975300.0","content":"Selected Answer: D\ndeployments carry ReplicaSets \nDynamoDB table for session data"},{"timestamp":"1694646780.0","content":"Selected Answer: C\nThere is a requirement for fault tolerance. I feel 'C\" satisfies that as it has replicasets. Option D does not talk about it","poster":"rsn","upvote_count":"1","comment_id":"1007014"},{"content":"Now i'll have to go with B. Check out what alabiba says to question, \"Can aws sqs be used to store web server session data?\"\nalabiba \"No, AWS SQS (Simple Queue Service) is not typically used for storing web server session data. SQS is a message queuing service that is designed for reliable and scalable message communication between distributed systems. For storing session data, it is more common to use dedicated session storage solutions such as databases (e.g., Amazon DynamoDB) or in-memory caches (e.g., Redis).\"","comments":[{"timestamp":"1694578140.0","content":"problem with option B is \" Multi-Attach on EC2 instances that are distributed across multiple Availability Zones\"; please note that multi-attach can only span since AZ\noption D is correct","poster":"chikorita","upvote_count":"2","comment_id":"1006177"}],"poster":"skyhiker","timestamp":"1692887460.0","upvote_count":"2","comment_id":"989267"},{"content":"Selected Answer: D\nD - best of the worst","upvote_count":"7","poster":"NikkyDicky","comment_id":"944796","timestamp":"1688657220.0"},{"timestamp":"1688618520.0","comment_id":"944327","poster":"YodaMaster","upvote_count":"2","content":"Selected Answer: D\nA looked good until \"store session data in SQS\"."},{"upvote_count":"3","content":"A looked good until \"store session data in SQS\".","comment_id":"939715","poster":"Henrytml","timestamp":"1688196600.0"},{"timestamp":"1688052900.0","upvote_count":"2","poster":"javitech83","content":"Selected Answer: D\nA looked good until \"store session data in SQS\".","comment_id":"938318"},{"timestamp":"1687628820.0","poster":"Maria2023","content":"Selected Answer: A\nFargate is the service, the only question remains the storage. Amazon EBS Multi-Attach is single-az service, so remains A. Even though I am not very confident with SQS caching web service sessions.","comment_id":"932825","upvote_count":"1"},{"upvote_count":"3","timestamp":"1687585320.0","content":"Agree, this is a worst question\nD is best choice for this question, but I would prefer to change EKS to ECS Fargate for compute and Elasticache for Redis for session.","comment_id":"932208","poster":"PhuocT"},{"poster":"gd1","upvote_count":"1","content":"Gpt 4.0 - Answer is D","comment_id":"931922","timestamp":"1687551360.0"},{"timestamp":"1687544280.0","comments":[{"comment_id":"931865","upvote_count":"1","content":"prefer d-d-d-d-d-d-d-d","poster":"easytoo","timestamp":"1687545000.0"}],"poster":"easytoo","content":"c-c-c-c-c-c","upvote_count":"1","comment_id":"931854"},{"comments":[{"timestamp":"1687524180.0","comment_id":"931597","upvote_count":"3","content":"Actually, it's D. Multi-Attach is the same AZ as someone pointed out.","poster":"SmileyCloud"}],"timestamp":"1687524120.0","upvote_count":"1","comment_id":"931593","poster":"SmileyCloud","content":"Selected Answer: B\nA comes with the least operational overhead, but storing session data in SQS is wrong.\nSession data can either be stored in ElastiCache or DynamoDB, so it's either B or D.\nI am going with B because ECS on EC2 is probably less demanding in terms of operations than EKS."},{"comment_id":"931142","content":"D is the best. Multi-attach is a single az feature","poster":"jubileu84","timestamp":"1687480920.0","upvote_count":"1"},{"upvote_count":"2","timestamp":"1687451760.0","poster":"nexus2020","comment_id":"930791","content":"Selected Answer: A\nQuestion is badly formated, I agree on that. But the question is asking: which one has the LEAST ongoing operational overhead. \n\nin general, EC2 has more operational task, and EKS has even more than Fargate. \n\nso A is the LEAST ongoing operational overhead?"},{"content":"the best answer is D. but using EKS here is a bad choice","poster":"MoussaNoussa","upvote_count":"2","timestamp":"1687443720.0","comment_id":"930663"},{"upvote_count":"4","timestamp":"1687384860.0","content":"what a worst ques\nA - Why do you need SQS to store web sever session data. SQS is for decoupling services\nB - EBS multi attach is for SAME availibility zone. The ques says multipel availibility zones\nC - Why do you need EFS to store web sever session data. Its damn expensive\nD - Better answer- But again why need for EKS. \nIf I were to choose one option, its D as its better compared to ABC","poster":"bhanus","comment_id":"929977"}],"answer_description":"","question_images":[],"choices":{"B":"Run the application on Amazon Elastic Container Service (Amazon ECS) on Amazon EC2. Use Amazon ElastiCache for Redis to cache frontend web server session data. Use Amazon Elastic Block Store (Amazon EBS) with Multi-Attach on EC2 instances that are distributed across multiple Availability Zones.","A":"Run the application on Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. Use Amazon Elastic File System (Amazon EFS) for data that is frequently accessed between the web and application tiers. Store the frontend web server session data in Amazon Simple Queue Service (Amazon SQS).","C":"Run the application on Amazon Elastic Kubernetes Service (Amazon EKS). Configure Amazon EKS to use managed node groups. Use ReplicaSets to run the web servers and applications. Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system across all EKS pods to store frontend web server session data.","D":"Deploy the application on Amazon Elastic Kubernetes Service (Amazon EKS). Configure Amazon EKS to use managed node groups. Run the web servers and application as Kubernetes deployments in the EKS cluster. Store the frontend web server session data in an Amazon DynamoDB table. Create an Amazon Elastic File System (Amazon EFS) volume that all applications will mount at the time of deployment."},"isMC":true,"exam_id":33,"answer_images":[],"timestamp":"2023-06-22 00:01:00","unix_timestamp":1687384860,"answers_community":["D (88%)","6%"]},{"id":"TOw6WCJHRIJARazk0BH0","question_images":[],"discussion":[{"comments":[{"poster":"yorkicurke","upvote_count":"3","timestamp":"1698946680.0","comment_id":"1060728","content":"the following link maybe helpful for some;\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html#CHAP_Target.S3.Limitations"}],"content":"Selected Answer: C\nC. The proper way is to use AWS DMS, but the answer here uses S3 (???) which will take forever. So the answer is C.","poster":"SmileyCloud","upvote_count":"17","timestamp":"1687527540.0","comment_id":"931659"},{"poster":"Ganshank","upvote_count":"12","comment_id":"991340","content":"C\nhttps://aws.amazon.com/blogs/database/part-3-migrating-to-amazon-rds-for-sql-server-using-transactional-replication-with-native-backup-and-restore/","timestamp":"1693128360.0"},{"content":"Selected Answer: C\nBy using native database high availability tools and replication methods, you can achieve near-zero downtime during the migration process. The other options may not provide the same level of seamless data replication and minimal downtime as the native SQL Server replication tools.\n\nOption B: Use AWS Database Migration Service (AWS DMS) to rehost the database. Set Amazon S3 as a target. Set up change data capture (CDC) replication. When the source and destination are fully synchronized, load the data from Amazon S3 into an Amazon RDS for Microsoft SQL Server DB instance.\n\n While AWS DMS can be used for migrations, it introduces additional complexity compared to native SQL Server replication tools.\n Staging data in Amazon S3 and then loading into the target RDS instance can cause downtime during the final cutover.\n Native replication tools can directly replicate data to the target RDS instance without an intermediate storage solution.","comment_id":"1315456","poster":"0b43291","upvote_count":"4","timestamp":"1732129080.0"},{"comment_id":"1265969","poster":"jefnmet","timestamp":"1723668660.0","content":"B look correct to me","upvote_count":"1","comments":[{"timestamp":"1724458980.0","upvote_count":"1","poster":"helloworldabc","comment_id":"1271474","content":"just C"}]},{"content":"Selected Answer: B\nB is the AWS way of doing a DB migration. C might work and could be better in some cases, but because the DBs are legacy you might run into compatibility issues or limitations with the tooling. If the databases are very large you really want to use B because you need to ship the bulk of the data with Snowball.","upvote_count":"1","comment_id":"1259256","poster":"8693a49","timestamp":"1722499980.0"},{"timestamp":"1721805540.0","content":"Selected Answer: C\nC. Correct, Near-Zero Downtime.\nA. In-Place Upgrade and Migration to Aurora: This involves multiple steps and the potential for increased downtime during the cutover process. \nSchema Conversion: Depending on the complexity of the legacy system, converting schemas and ensuring compatibility with Amazon Aurora can be challenging and time-consuming.\nB. Intermediate Storage in Amazon S3: Adds complexity\nTwo-Step Process: First replicating to Amazon S3 and then loading into Amazon RDS adds additional steps and potential points of failure.","upvote_count":"1","poster":"CAIYasia","comment_id":"1254195"},{"comment_id":"1243374","timestamp":"1720271580.0","upvote_count":"2","poster":"grandcanyon","content":"Selected Answer: B\nIn option C - \"Connect the source system to an Amazon RDS for Microsoft SQL Server DB instance\", should be target, not source"},{"poster":"trungtd","upvote_count":"1","comment_id":"1228192","timestamp":"1718066640.0","content":"Selected Answer: C\nUse AWS Database Migration Service (AWS DMS) to \"rehost\" the database???? \nHow you can \"rehost\" database with DMS"},{"upvote_count":"2","poster":"michele_scar","timestamp":"1716642240.0","comment_id":"1218334","content":"Selected Answer: B\nDMS should be the better service for this use case"},{"content":"Selected Answer: C\nAnswer: C.","upvote_count":"1","comment_id":"1208739","poster":"titi_r","timestamp":"1715239620.0"},{"upvote_count":"2","timestamp":"1714526760.0","poster":"BrijMohan08","comment_id":"1204794","content":"Selected Answer: A\nAWS Application Migration Service (MGN) is a highly automated lift-and-shift (rehost) solution that simplifies the migration of applications to AWS. It supports near-zero downtime migrations by continuously replicating the source servers to AWS.\n\nRepointing the applications to Amazon Aurora Serverless satisfies the migration to the modern data architecture."},{"comment_id":"1186864","poster":"svenkata18","upvote_count":"1","timestamp":"1711899120.0","content":"Why not A as the question the it should rearchitected from legacy"},{"poster":"JOKERO","content":"Native database high availability (HA) tools include the Always On or distributed availability group clusters in Microsoft SQL Server and Oracle’s Data Guard replications. This approach requires a major effort to set up across extended, cross-site HA clusters, and might cause some performance degradation because of the longer latency to achieve fully synchronous active/active deployments. However, this method provides the closest to near-zero downtime during the cutover.","upvote_count":"4","timestamp":"1710494760.0","comment_id":"1174183"},{"upvote_count":"1","timestamp":"1705929900.0","comment_id":"1128673","poster":"ftaws","content":"What is \"native database high availability tools\"????"},{"poster":"tmlong18","comment_id":"1124829","timestamp":"1705483380.0","content":"Selected Answer: C\nB. Use AWS Database Migration Service (AWS DMS) to rehost the database.\n\nThis action is not 'rehost'","upvote_count":"1"},{"content":"C:\nUse distributed AG, it will work.\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-sql-server-to-aws-using-distributed-availability-groups.html","upvote_count":"1","comment_id":"1111510","poster":"adelynllllllllll","timestamp":"1704153600.0"},{"poster":"duriselvan","comment_id":"1095346","content":"A. Use AWS Application Migration Service and the AWS Schema Conversion Tool (AWS SCT). Perform an in-place upgrade before the migration. Export the migrated data to Amazon Aurora Serverless after cutover. Repoint the applications to Amazon Aurora.\n\nHere's why this approach offers the best advantages:\n\nMinimal downtime: In-place upgrade and cutover minimize downtime compared to traditional database migrations.\nModernization: AWS Schema Conversion Tool helps modernize legacy schema structures during migration.\nServerless architecture: Amazon Aurora Serverless simplifies management and scales effortlessly.\nApplication compatibility: Repointing applications directly to Aurora minimizes disruption.","timestamp":"1702460940.0","upvote_count":"4"},{"timestamp":"1701850560.0","content":"Selected Answer: C\nAgree with C; B was goo until it talked about S3 :(","upvote_count":"4","poster":"ayadmawla","comment_id":"1089078"},{"comment_id":"1084514","upvote_count":"3","content":"Selected Answer: B\nAnswer B.","timestamp":"1701359460.0","poster":"shaaam80"},{"upvote_count":"3","comment_id":"1081021","timestamp":"1701033540.0","content":"Selected Answer: B\nb - always on feature","poster":"geekgirl007"},{"content":"Selected Answer: B\nBoth B and C are possible but I would go with DMS.","timestamp":"1700778120.0","poster":"career360guru","upvote_count":"3","comment_id":"1078872"},{"upvote_count":"1","timestamp":"1700302620.0","content":"Selected Answer: C\nC. https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-sql-server/always-on.html","comment_id":"1073905","poster":"severlight"},{"content":"In this post, we showed you how to configure transactional replication with native backup and restore that replicates data from an on-premises SQL Server or SQL Server on an EC2 instance. You can use this strategy to migrate your large mission-critical workloads to an RDS for SQL Server instance with minimal to near-zero downtime. \n\n\nC ans","comment_id":"1014854","upvote_count":"1","timestamp":"1695469020.0","poster":"duriselvan"},{"upvote_count":"4","comment_id":"989673","timestamp":"1692940200.0","content":"Selected Answer: B\nOnly B can do it.\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PostgreSQL.S3Import.html","poster":"billtran"},{"timestamp":"1692065520.0","upvote_count":"4","comment_id":"981220","poster":"longngo0924","content":"Selected Answer: B\nCorrect answer is B.\nhttps://repost.aws/questions/QUw-bHxHYITuC3IqDwgyx6fw/is-it-possible-to-use-aws-rds-sql-server-as-an-aag-target-from-on-premise-primary"},{"comment_id":"966432","comments":[{"content":"So what is your answer then ?","poster":"PhilTheAnimal","upvote_count":"1","timestamp":"1691497560.0","comment_id":"975647"}],"content":"RDS SQL Server is a managed service, so it will not be possible to add RDS Instance as an extension node to your on-premise primary instance. However, you can connect directly from your on-premise App to hosted RDS SQL Server instance in AWS. Alternatively, if you need RDS as a DR node for your on-premises primary, you can use an option like DMS (Database Migration Service) to set up on-going replication to RDS.","upvote_count":"1","poster":"rxhan","timestamp":"1690637100.0"},{"comment_id":"962671","upvote_count":"6","timestamp":"1690286460.0","content":"Selected Answer: B\nCorrect B\nAlways On Availability Groups (AG) cannot be used between an on-premise SQL Server database and Amazon RDS for SQL Server on AWS. Always On Availability Groups is a feature specific to SQL Server Enterprise Edition and requires Windows Failover Clustering-based network connectivity.\n\nAmazon RDS for SQL Server supports both Standard and Enterprise editions of SQL Server, but it does not support the cluster failover features required for Always On Availability Groups.\n\nSo only DMS is the correct answer","poster":"Arnaud92"},{"content":"Correct B.","comment_id":"960699","timestamp":"1690134600.0","upvote_count":"1","poster":"ggrodskiy"},{"poster":"Bengi","upvote_count":"2","content":"B - Refer to \nhttps://docs.aws.amazon.com/dms/latest/sbs/chap-manageddatabases.sqlserveralwayson.html","timestamp":"1689681300.0","comment_id":"955351"},{"timestamp":"1688657340.0","comment_id":"944799","poster":"NikkyDicky","upvote_count":"1","content":"Selected Answer: C\nC for no downtime"},{"poster":"Jackhemo","comment_id":"932890","upvote_count":"1","timestamp":"1687635480.0","content":"Olabiba says \"C\"...so it is C."},{"timestamp":"1687551720.0","upvote_count":"1","comment_id":"931928","content":"GPT 4.0 is B","poster":"gd1"},{"content":"c-c-c-c-c-c\nMicrosoft SQL Server provides native high availability tools such as Always On Availability Groups or database mirroring. These tools enable real-time data replication and failover capabilities, allowing for minimal downtime during the migration process.","timestamp":"1687544460.0","poster":"easytoo","comment_id":"931858","upvote_count":"2"},{"poster":"nexus2020","comment_id":"930862","timestamp":"1687456200.0","content":"Selected Answer: C\nNative Database High Availability Tools: AWS provides native high availability tools for Microsoft SQL Server, such as database mirroring, Always On Availability Groups, and transactional replication. These tools are designed to minimize downtime during the migration process and ensure data consistency and integrity.","upvote_count":"5"}],"choices":{"B":"Use AWS Database Migration Service (AWS DMS) to rehost the database. Set Amazon S3 as a target. Set up change data capture (CDC) replication. When the source and destination are fully synchronized, load the data from Amazon S3 into an Amazon RDS for Microsoft SQL Server DB instance.","C":"Use native database high availability tools. Connect the source system to an Amazon RDS for Microsoft SQL Server DB instance. Configure replication accordingly. When data replication is finished, transition the workload to an Amazon RDS for Microsoft SQL Server DB instance.","D":"Use AWS Application Migration Service. Rehost the database server on Amazon EC2. When data replication is finished, detach the database and move the database to an Amazon RDS for Microsoft SQL Server DB instance. Reattach the database and then cut over all networking.","A":"Use AWS Application Migration Service and the AWS Schema Conversion Tool (AWS SCT). Perform an in-place upgrade before the migration. Export the migrated data to Amazon Aurora Serverless after cutover. Repoint the applications to Amazon Aurora."},"question_text":"A solutions architect is planning to migrate critical Microsoft SQL Server databases to AWS. Because the databases are legacy systems, the solutions architect will move the databases to a modern data architecture. The solutions architect must migrate the databases with near-zero downtime.\n\nWhich solution will meet these requirements?","answer_ET":"C","answer_images":[],"answers_community":["C (55%)","B (42%)","3%"],"question_id":145,"url":"https://www.examtopics.com/discussions/amazon/view/112974-exam-aws-certified-solutions-architect-professional-sap-c02/","answer":"C","topic":"1","unix_timestamp":1687456200,"timestamp":"2023-06-22 19:50:00","answer_description":"","isMC":true,"exam_id":33}],"exam":{"id":33,"isBeta":false,"numberOfQuestions":529,"lastUpdated":"11 Apr 2025","name":"AWS Certified Solutions Architect - Professional SAP-C02","isImplemented":true,"provider":"Amazon","isMCOnly":true},"currentPage":29},"__N_SSP":true}