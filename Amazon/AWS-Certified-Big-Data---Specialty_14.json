{"pageProps":{"questions":[{"id":"IQ3JXNFcRGZpBG9HltDo","answer_images":[],"discussion":[{"poster":"mattyb123","content":"answer ADF","timestamp":"1632459720.0","upvote_count":"6","comment_id":"8313"},{"upvote_count":"1","timestamp":"1635611400.0","comment_id":"122768","poster":"alopazo","content":"ADF. All steps are here\n\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#configure-snapshot-copy-grant"},{"comment_id":"103769","timestamp":"1635594240.0","upvote_count":"1","content":"ABD is the right answer","poster":"k115"},{"timestamp":"1634978520.0","content":"According to https://docs.aws.amazon.com/redshift/latest/APIReference/API_CreateSnapshotCopyGrant.html answer is ADF","poster":"szszsz","upvote_count":"2","comment_id":"77057"},{"content":"ADF is correct Answer https://docs.aws.amazon.com/redshift/latest/mgmt/managing-snapshots-console.html#xregioncopy-kms-encrypted-snapshot","timestamp":"1634519100.0","upvote_count":"2","poster":"YashBindlish","comment_id":"74759"},{"comment_id":"74089","content":"looks its ABD.","timestamp":"1634403000.0","upvote_count":"2","poster":"susan8840"},{"content":"my selection ADF","comment_id":"52385","timestamp":"1634010960.0","upvote_count":"2","poster":"san2020"},{"timestamp":"1633653720.0","upvote_count":"1","poster":"kalpanareddy","content":"looks its ABD. any thoughts \nhttps://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html","comment_id":"43449"},{"comment_id":"40217","content":"find the step by step AWS Redshift cross-region copy snapshot through the console (Question ask for CLI but both has same steps)\nhttps://www.youtube.com/watch?v=9DepoiBOe6o","poster":"ME2000","timestamp":"1633472760.0","upvote_count":"1"},{"comments":[{"upvote_count":"4","content":"ADF is correct","comment_id":"19414","timestamp":"1633085280.0","poster":"cybe001"},{"upvote_count":"1","timestamp":"1633123920.0","comment_id":"30989","poster":"shwang","content":"I choose ACD as well."},{"content":"It's F not C, because the snapshot in the destination region should be encrypted with a key that exists in that region.","comment_id":"99326","upvote_count":"1","poster":"Corram","timestamp":"1635469020.0"}],"comment_id":"19413","content":"I choose ACD.","timestamp":"1632855300.0","poster":"cybe001","upvote_count":"2"},{"comment_id":"8651","content":"Same question is on https://www.examtopics.com/exams/amazon/aws-certified-big-data-specialty/view/8/ Question#36. This is the correct question and answer","timestamp":"1632701760.0","upvote_count":"2","comments":[{"content":"You asked ACD in that question )\nWhy ADF here?","upvote_count":"2","timestamp":"1633592220.0","poster":"practicioner","comments":[{"poster":"Corram","comment_id":"101707","timestamp":"1635546960.0","upvote_count":"1","content":"Question#36 has a bug since F is missing and C is closest to F That's why mattyb123 went for ACD there I suppose. ADF is correct."}],"comment_id":"41065"}],"poster":"mattyb123"}],"question_text":"An Amazon Redshift Database is encrypted using KMS. A data engineer needs to use the AWS CLI to create a KMS encrypted snapshot of the database in another AWS region.\nWhich three steps should the data engineer take to accomplish this task? (Choose three.)","exam_id":17,"topic":"2","question_id":66,"answers_community":[],"answer_description":"","answer_ET":"Explanation","timestamp":"2019-08-26 08:47:00","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/4092-exam-aws-certified-big-data-specialty-topic-2-question-1/","answer":"Explanation","choices":{"E":"In the destination region, enable cross-region replication and specify the name of the copy grant created.","D":"In the source region, enable cross-region replication and specify the name of the copy grant created.","B":"Copy the existing KMS key to the destination region.","A":"Create a new KMS key in the destination region.","F":"Use CreateSnapshotCopyGrant to allow Amazon Redshift to use the KMS key created in the destination region. ADF","C":"Use CreateSnapshotCopyGrant to allow Amazon Redshift to use the KMS key from the source region."},"unix_timestamp":1566802020,"question_images":[]},{"id":"caY3gpqArFIFhYWAYzhF","exam_id":17,"answer_description":"","timestamp":"2019-08-26 05:43:00","topic":"2","discussion":[{"comment_id":"124369","timestamp":"1636228080.0","upvote_count":"1","poster":"yogesh88","content":"Just attempted exam, This is typo. In the exam, s3distcp was properly mentioned. \nI selected D"},{"upvote_count":"1","timestamp":"1635357900.0","content":"There are no S3DistCOP tools (It is rather called S3DistCp), so the answer is B","comment_id":"105856","poster":"awane"},{"timestamp":"1635083040.0","upvote_count":"1","content":"D is the right answer","comment_id":"103803","poster":"k115"},{"upvote_count":"1","poster":"srirampc","timestamp":"1634663340.0","content":"D. since the transfer is from on-premise hadoop S3DistCp would work from hadoop to S3.","comment_id":"82611"},{"timestamp":"1634519640.0","comment_id":"80549","poster":"viru","content":"D\nhttps://forums.aws.amazon.com/thread.jspa?threadID=120522\nhttps://d0.awsstatic.com/whitepapers/aws-amazon-emr-best-practices.pdf","upvote_count":"1"},{"poster":"Bulti","comment_id":"77541","content":"Answer D: \nhttps://blog.ippon.tech/aws-white-paper-in-5-minutes-or-less-best-practices-for-amazon-emr/","upvote_count":"3","timestamp":"1633650900.0"},{"comment_id":"74099","upvote_count":"2","poster":"susan8840","content":"B makes senses. not D since S3DistCp is used to copy large amounts of data from Amazon S3 into HDFS. the issue here is from on-premise to S3","timestamp":"1633569720.0","comments":[{"comment_id":"338046","poster":"DerekKey","upvote_count":"1","content":"bot true: \nYou can also use S3DistCp to copy data from HDFS to Amazon S3. \nS3DistCp is more scalable and efficient for parallel copying large numbers of objects across buckets and across AWS accounts.\nS3DistCp is the same as the Hadoop binary DistCp, except it takes advantage of multi-part upload to S3 for larger files. Hadoop is optimized for large file blocks, so it is usually best to use S3DistCp for copying HDFS files from an external data center or local disk to S3 to take advantage of this optimization.","timestamp":"1636278300.0"}]},{"content":"my selection D","timestamp":"1632879540.0","upvote_count":"3","comment_id":"52396","poster":"san2020"},{"poster":"bigdatalearner","comment_id":"13809","timestamp":"1632822660.0","content":"D. S3DistCop is the right answer and it's used for moving big data from Hadoop to s3 , S3 to hadopp or from s3 to s3 and at backend it uses map reduce job. Multipart upload is not the best choice here","upvote_count":"4"},{"poster":"jlpl","comment_id":"8758","timestamp":"1632593580.0","content":"Vote D for now","upvote_count":"4","comments":[{"upvote_count":"5","poster":"mattyb123","comment_id":"9828","timestamp":"1632775080.0","content":"Answer is D. There is a typo."}]},{"comments":[{"timestamp":"1632092220.0","comment_id":"8289","poster":"mattyb123","content":"Anyone disagree with B. I did select this answer last time but the numbers seem to add up?","upvote_count":"1","comments":[{"comments":[{"comment_id":"66929","upvote_count":"2","content":"More info: http://www.thecloudxperts.co.uk/moving-large-amounts-of-data-from-hdfs-data-center-to-amazon-s3-using-s3distcp\n\nTwo Important tools to move data—S3DistCp and DistCp—can help you move data stored on your local (data center) HDFS storage to Amazon S3.","poster":"kttttt","timestamp":"1633367460.0"}],"comment_id":"8317","content":"By reviewing https://d0.awsstatic.com/whitepapers/aws-amazon-emr-best-practices.pdf looks like D is the correct answer.","timestamp":"1632122760.0","upvote_count":"2","poster":"mattyb123"}]}],"content":"Confirmed. \n1.https://toolstud.io/data/filesize.php?speed=5&speed_unit=Gbps&duration=12&duration_unit=hours&compare=harddisk\n2.https://aws.amazon.com/snowball/faqs/#Q.3A_How_long_does_it_take_to_transfer_my_data.3F: Q: When should I consider using Snowball instead of AWS Direct Connect?\nAWS Direct Connect provides you with dedicated, fast connections from your premises to the AWS network. If you need to transfer large quantities of data to AWS on an ongoing basis, AWS Direct Connect might be the right choice.","poster":"mattyb123","timestamp":"1632076860.0","upvote_count":"1","comment_id":"8288"}],"answer":"B","choices":{"C":"Use Amazon S3 transfer acceleration capability to transfer data to Amazon S3 over AWS Direct Connect.","A":"Use an offline copy method, such as an AWS Snowball device, to copy and transfer data to Amazon S3.","D":"Setup S3DistCop tool on the on-premises Hadoop environment to transfer data to Amazon S3 over AWS Direct Connect.","B":"Configure a multipart upload for Amazon S3 on AWS Java SDK to transfer data over AWS Direct Connect."},"question_text":"An organization currently runs a large Hadoop environment in their data center and is in the process of creating an alternative Hadoop environment on AWS, using Amazon EMR.\nThey generate around 20 TB of data on a monthly basis. Also on a monthly basis, files need to be grouped and copied to Amazon S3 to be used for the Amazon\nEMR environment. They have multiple S3 buckets across AWS accounts to which data needs to be copied. There is a 10G AWS Direct Connect setup between their data center and AWS, and the network team has agreed to allocate 50% of AWS Direct Connect bandwidth to data transfer. The data transfer cannot take more than two days.\nWhat would be the MOST efficient approach to transfer data to AWS on a monthly basis?","unix_timestamp":1566790980,"url":"https://www.examtopics.com/discussions/amazon/view/4076-exam-aws-certified-big-data-specialty-topic-2-question-10/","isMC":true,"question_id":67,"answer_ET":"B","answers_community":[],"answer_images":[],"question_images":[]},{"id":"N794CfknfTHlwqx0CgBf","question_id":68,"question_images":[],"answer_images":[],"choices":{"A":"Use multiple Amazon EBS volumes on Amazon EMR to store processed data and scale out the Amazon EMR cluster as needed.","D":"use Amazon Kinesis Data Firehose and, instead of using Amazon EMR, stream logs directly into Amazon Elasticsearch Service.","B":"Use the EMR File System and Amazon S3 to store processed data and scale out the Amazon EMR cluster as needed.","C":"Use Amazon DynamoDB to store processed data and scale out the Amazon EMR cluster as needed."},"answer":"D","timestamp":"2019-08-26 06:02:00","unix_timestamp":1566792120,"isMC":true,"answer_ET":"D","question_text":"An organization is developing a mobile social application and needs to collect logs from all devices on which it is installed. The organization is evaluating the\nAmazon Kinesis Data Streams to push logs and Amazon EMR to process data. They want to store data on HDFS using the default replication factor to replicate data among the cluster, but they are concerned about the durability of the data. Currently, they are producing 300 GB of raw data daily, with additional spikes during special events. They will need to scale out the Amazon EMR cluster to match the increase in streamed data.\nWhich solution prevents data loss and matches compute demand?","answers_community":[],"url":"https://www.examtopics.com/discussions/amazon/view/4077-exam-aws-certified-big-data-specialty-topic-2-question-11/","topic":"2","answer_description":"","exam_id":17,"discussion":[{"content":"my selection B","upvote_count":"6","comment_id":"52397","timestamp":"1636190460.0","poster":"san2020"},{"comments":[{"timestamp":"1636142220.0","comment_id":"36878","content":"Agreed, B is the correct answer.","upvote_count":"2","poster":"Kuntazulu"}],"content":"I would say B without no doubt","poster":"antoneti","upvote_count":"1","comment_id":"28595","timestamp":"1635472440.0"},{"upvote_count":"2","comment_id":"8637","timestamp":"1632786360.0","content":"B, anyone?","comments":[{"timestamp":"1633660320.0","comments":[{"poster":"Jialu","comment_id":"9848","upvote_count":"2","content":"I vote B","timestamp":"1635348120.0"}],"poster":"mattyb123","upvote_count":"4","content":"I went with B","comment_id":"9829"}],"poster":"jlpl"},{"comment_id":"8290","timestamp":"1632233520.0","upvote_count":"2","comments":[{"comment_id":"8934","comments":[{"timestamp":"1633288560.0","upvote_count":"1","poster":"mattyb123","content":"https://docs.aws.amazon.com/elasticsearch-service/latest/developerguide/sizing-domains.html refers to a lot smaller reference data then what the question states. When searching for compute resources on AWS elasticsearch isnt a service that appears.","comment_id":"9380"}],"poster":"mattyb123","content":"Keyword for EMR is usually spike or spikey so i am assuming EMR is required in the answer","upvote_count":"3","timestamp":"1633121220.0"}],"poster":"mattyb123","content":"Thoughts on B? Prevents data loss (durable data store S3 with S3 versioning) Compute demand (EMR)"}]},{"id":"3bceSDfhLli2JltETbTI","topic":"2","answers_community":[],"question_text":"An advertising organization uses an application to process a stream of events that are received from clients in multiple unstructured formats.\nThe application does the following:\n✑ Transforms the events into a single structured format and streams them to Amazon Kinesis for real-time analysis.\n✑ Stores the unstructured raw events from the log files on local hard drivers that are rotated and uploaded to Amazon S3.\nThe organization wants to extract campaign performance reporting using an existing Amazon redshift cluster.\nWhich solution will provide the performance data with the LEAST number of operations?","answer":"B","exam_id":17,"answer_images":[],"question_images":[],"discussion":[{"upvote_count":"6","comment_id":"75908","poster":"Bulti","content":"Not A – No use loading unstructured data in multiple formats to RedShift via Kinesis Firehouse agent.\nNot B- Creating External table using RedShift Spectrum will be an issue against unstructured data in multiple formats.\nNot C - Not a good choice. Never seen Lambda talking to RedShift and why would you use it when KFH directly connect to RedShift.\nCorrect Option is D- Because it loads structured data in a single format to RedShift.","timestamp":"1633749960.0"},{"comment_id":"338057","timestamp":"1636177740.0","content":"Correct D: least number of operations","upvote_count":"1","poster":"DerekKey"},{"upvote_count":"2","timestamp":"1634915280.0","poster":"jove","content":"D seems more reasonable. I go with D","comment_id":"143727"},{"comments":[{"upvote_count":"1","timestamp":"1634643780.0","content":"A is about log files which are unstructured. Not a good idea to move the log files to Redshift.","comment_id":"143726","poster":"jove"}],"timestamp":"1634306760.0","upvote_count":"1","poster":"Bulti","content":"Option A also talks about shipping structured data from the application using Kinesis Firehouse agent to Redshift. So using the same Agent it's possible to stream the data to both Kinesis Data Streams for analyiss and KFH to deliver it to Redshift. It seems like the most direct option.","comment_id":"78797"},{"comment_id":"74103","upvote_count":"1","content":"B. the question is asking how to consume the data in Redshift not how to get/input the data which is already in place","timestamp":"1633644840.0","poster":"susan8840"},{"content":"The unstructured date is already transformed to single dtructured format prior to putting into Kinesis. So I will go with D for LEAST number of operations. B = spectrum is not needed","comment_id":"72965","timestamp":"1633533780.0","upvote_count":"1","poster":"Zinty"},{"content":"To handle the unstructured data structure, Kinesis Data Firehose can invoke Lambda function to do data transformation and format conversion, so it's D","comment_id":"53251","poster":"zhengtoronto","upvote_count":"2","timestamp":"1633533120.0"},{"comment_id":"52398","poster":"san2020","timestamp":"1633483560.0","content":"my selection D","upvote_count":"2"},{"content":"answer is D. The key here is multiple unstructured formats. You can't define an external table with multiple source formats.","poster":"mars2","upvote_count":"3","comment_id":"45966","timestamp":"1633429920.0"},{"upvote_count":"1","timestamp":"1633345800.0","comment_id":"36882","poster":"Kuntazulu","content":"A. FH to Redshift is direct..."},{"comment_id":"33372","comments":[{"timestamp":"1635510240.0","poster":"DerekKey","comment_id":"338055","content":"Transforms the events into a single structured format and streams them to Amazon Kinesis for real-time analysis.","upvote_count":"1"}],"content":"For unstructured data combine Redshift with S3 is basic. Because Redshift is not for unstructured data.","timestamp":"1633306020.0","upvote_count":"4","poster":"sriansri"},{"poster":"cybe001","timestamp":"1632674040.0","upvote_count":"2","comment_id":"19447","content":"I go with D. Fire Hose can read the Structured data from Kinesis Stream and store it in Redshift."},{"upvote_count":"1","timestamp":"1632604860.0","poster":"Zire","content":"The problem with B is fine if the data was structured since we could use redshidt spectrum to create external tables pointing to S3 . For this I'd go with D. At least as solution it is correct","comment_id":"14326"},{"comment_id":"13810","upvote_count":"2","poster":"bigdatalearner","timestamp":"1632389040.0","content":"B is the right answer","comments":[{"upvote_count":"3","comments":[{"upvote_count":"1","comment_id":"31263","comments":[{"poster":"DerekKey","content":"Transforms the events into a single structured format and streams them to Amazon Kinesis for real-time analysis.","comment_id":"338056","upvote_count":"1","timestamp":"1635539100.0"}],"timestamp":"1633261080.0","content":"refereed FAQ, unstructured data in s3 could be the external table of redshift, So it is B","poster":"shwang"}],"content":"How can B be the answer when is says 'point the table to the unstructured data'? The answer is D.","poster":"d00ku","comment_id":"19904","timestamp":"1632916680.0"}]},{"comments":[{"timestamp":"1632169500.0","upvote_count":"2","comments":[{"timestamp":"1634308560.0","comment_id":"143725","content":"Amazon Redshift now supports writing to external tables in Amazon S3 : \nhttps://aws.amazon.com/about-aws/whats-new/2020/06/amazon-redshift-now-supports-writing-to-external-tables-in-amazon-s3/","upvote_count":"1","poster":"jove"}],"content":"Amazon Redshift Spectrum uses external tables to query data that is stored in Amazon S3. You can query an external table using the same SELECT syntax you use with other Amazon Redshift tables. External tables are read-only. You can't write to an external table.\n1.https://docs.aws.amazon.com/redshift/latest/dg/r_CREATE_EXTERNAL_TABLE.html 2.https://blog.openbridge.com/10-simple-tips-that-help-you-quickly-find-success-adopting-amazon-redshift-spectrum-810db089abbe\n3.https://docs.aws.amazon.com/redshift/latest/dg/c-spectrum-external-tables.html\n4.https://docs.aws.amazon.com/redshift/latest/dg/r_CREATE_EXTERNAL_TABLE.html","poster":"mattyb123","comment_id":"8793"},{"content":"I think its D due to FH being able to automatically copy/write the data to redshift. Where if you were using redshift spectrum you can only create read only external tables and you would need to write the SQL to create the external table.","comment_id":"9382","poster":"mattyb123","timestamp":"1632221100.0","upvote_count":"4"}],"poster":"mattyb123","timestamp":"1632148620.0","upvote_count":"4","comment_id":"8291","content":"Thoughts on D?"}],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/4078-exam-aws-certified-big-data-specialty-topic-2-question-12/","timestamp":"2019-08-26 06:05:00","answer_ET":"B","answer_description":"","unix_timestamp":1566792300,"question_id":69,"choices":{"B":"Create an external table in Amazon Redshift and point it to the S3 bucket where the unstructured raw events are stored.","A":"Install the Amazon Kinesis Data Firehose agent on the application servers and use it to stream the log files directly to Amazon Redshift.","C":"Write an AWS Lambda function that triggers every hour to load the new log files already in S3 to Amazon redshift.","D":"Connect Amazon Kinesis Data Firehose to the existing Amazon Kinesis stream and use it to stream the event directly to Amazon Redshift."}},{"id":"bvcZnMemAGkHgt0ieS3Z","topic":"2","question_images":[],"answer_images":[],"unix_timestamp":1566792420,"answer_ET":"C","answer_description":"","question_id":70,"url":"https://www.examtopics.com/discussions/amazon/view/4079-exam-aws-certified-big-data-specialty-topic-2-question-13/","answer":"C","discussion":[{"comments":[{"comment_id":"11135","content":"Transparent Data Encryption is for HDFS not s3, B may be the correct answer https://poonamkucheriya.wordpress.com/2019/01/11/how-to-implement-sql-standard-based-hive-authorization-in-emr-hive/","comments":[{"poster":"apertus","comment_id":"11137","upvote_count":"1","timestamp":"1633155720.0","content":"Transparent Data Encryption is for HDFS: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-data-encryption-options.html"}],"timestamp":"1633109280.0","poster":"apertus","upvote_count":"1"}],"upvote_count":"5","poster":"mattyb123","comment_id":"8292","content":"It's A. \nhttps://aws.amazon.com/blogs/big-data/implementing-authorization-and-auditing-using-apache-ranger-on-amazon-emr/","timestamp":"1632069600.0"},{"content":"Every is saying B \nbut https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hive-differences.html\nit says \"Hive authorization : \nAmazon EMR supports Hive authorization for HDFS but not for EMRFS and Amazon S3. Amazon EMR clusters run with authorization disabled by default.\"","timestamp":"1655975280.0","upvote_count":"1","comment_id":"620861","poster":"ru4aws"},{"timestamp":"1636232280.0","poster":"DerekKey","content":"Answer A:\nTransparent Encryption - Security configurations offer settings to enable security for data in-transit and data at-rest in Amazon Elastic Block Store (Amazon EBS) storage volumes and EMRFS data in Amazon S3.\nRanger - EMR steps are used to perform the following: Install and configure Ranger HDFS and Hive plugins","upvote_count":"1","comment_id":"338083"},{"content":"it's A..\n\nApache Ranger has the following goals:\n\nCentralized security administration to manage all security related tasks in a central UI or using REST APIs.\nFine grained authorization to do a specific action and/or operation with Hadoop component/tool and managed through a central administration tool\nStandardize authorization method across all Hadoop components.\nEnhanced support for different authorization methods - Role based access control, attribute based access control etc.\nCentralize auditing of user access and administrative actions (security related) within all the components of Hadoop.","poster":"rohitsingh","timestamp":"1636159260.0","comment_id":"188735","upvote_count":"1"},{"timestamp":"1635838080.0","poster":"askaron","comment_id":"133855","upvote_count":"2","content":"B is the only one to address \"SQL base authorization\":\nhttps://cwiki.apache.org/confluence/display/Hive/SQL+Standard+Based+Hive+Authorization\nWhich is basically the right to execute a SELECT statement, as required by the question.\nAnswer A is tempting, but why installing an additional ec2 instance if it goes without doing so?"},{"poster":"jkoffee","upvote_count":"1","timestamp":"1635630360.0","comment_id":"110174","content":"B my choice\nhttps://aws.amazon.com/fr/blogs/big-data/encrypt-data-at-rest-and-in-flight-on-amazon-emr-with-security-configurations/"},{"upvote_count":"2","content":"B.\nA: didn’t mention data encryption on S3.\nC: KMS is a key management service to managed the encryption keys. No matter what encryption you use, you will can always KMS to manage the keys. so you can say use KMS to manage encryption keys, but not use KMS for encryption of data.\nD: security group is not helping in giving permission to select statement.","poster":"freedomeox","timestamp":"1635474660.0","comment_id":"107269"},{"timestamp":"1635406380.0","content":"B is the correct answer","comment_id":"103815","upvote_count":"2","poster":"k115"},{"content":"This is a good example which confirms that A is the answer-> https://noise.getoto.net/2016/12/02/implementing-authorization-and-auditing-using-apache-ranger-on-amazon-emr/.\nThe key is that the question is about EMR HDFS but some of the choices offered talk about EMRFS which may lead to picking up a wrong answer. As this is AWS EMR, we are looking at an HDFS cluster that needs to be protected. Transparent Data Encryption meets the at rest and in transit requirements. So now authorizing access to Hive tables is best offered using Apache ranger as shown in the blog at the above link.","comment_id":"75928","upvote_count":"3","comments":[{"upvote_count":"2","comment_id":"101772","content":"B is correct.\n\"store sensitive information on Amazon S3 and process it through Amazon EMR\" <- this is EMRFS at its best. Option A does not help with encrypting data in S3, nor in transit when it gets loaded from S3 to EMR, which the question clearly asks for.","poster":"Corram","timestamp":"1634916240.0"}],"poster":"Bulti","timestamp":"1634398140.0"},{"poster":"Zinty","content":"I dont think B is correct - Amazon EMR supports Hive Authorization(Storage Based Authorization, SQL Standards Based Authorization in HiveServer2) for HDFS but not for EMRFS and Amazon S3. Amazon EMR clusters run with authorization disabled by default.","upvote_count":"1","comment_id":"72977","comments":[{"comment_id":"101892","content":"This article written by AWS employees indicates otherwise stating \"The EMRFS authorization feature specifically applies to access by using HiveServer2.\" https://idk.dev/best-practices-for-securing-amazon-emr/","poster":"Corram","upvote_count":"1","timestamp":"1635349440.0"}],"timestamp":"1634365260.0"},{"timestamp":"1634086320.0","comment_id":"52399","poster":"san2020","upvote_count":"2","content":"my selection B"},{"content":"It's B\nhttps://cwiki.apache.org/confluence/display/Hive/LanguageManual+Authorization","upvote_count":"2","poster":"aws123","comment_id":"41626","timestamp":"1634064780.0"},{"content":"Why not C? Is it wrong?","timestamp":"1634006580.0","comment_id":"33373","poster":"sriansri","upvote_count":"1"},{"comments":[{"upvote_count":"1","content":"Yes, C is the answer. Using KMS IAM roles to control data access is a good pattern that control access to users using the data.","comments":[{"content":"WRONG: IAM roles are not for protecting data in transit","poster":"DerekKey","timestamp":"1636218480.0","upvote_count":"1","comment_id":"338063"}],"comment_id":"82620","poster":"srirampc","timestamp":"1634816040.0"}],"timestamp":"1633739460.0","upvote_count":"1","poster":"shwang","comment_id":"31092","content":"why nobody has a think about C?"},{"comments":[{"upvote_count":"1","comment_id":"24281","poster":"s3an","comments":[{"upvote_count":"1","content":"First of all\n\"transit for traffic between Amazon S3 and EMRFS.\" is Invalid, because EMRFS is already on S3\nSecondly, you can use different IAM roles for EMRFS requests to Amazon S3 based on cluster users, groups, or the location of EMRFS data in Amazon S3. (invalid - authorization on HiveServer2)\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-fs.html\nTherefore the correct answer is C","comment_id":"38607","poster":"ME2000","timestamp":"1634027760.0"}],"timestamp":"1633717740.0","content":"B is the right answer, as Kerberos authentication in D will not limit access to SELECT statements, it's simply an authentication mechanism"}],"timestamp":"1633418700.0","comment_id":"24279","upvote_count":"2","content":"A is wrong. https://aws.amazon.com/blogs/aws/new-at-rest-and-in-transit-encryption-for-amazon-emr/ \"We already offer several data encryption options for EMR including server and client side encryption for Amazon S3 with EMRFS and Transparent Data Encryption for HDFS. While these solutions do a good job of protecting data at rest, they do not address data stored in temporary files or data that is in flight, moving between job steps. Each of these encryption options must be individually enabled and configured, making the process of implementing encryption more tedious that it need be\"","poster":"s3an"},{"upvote_count":"2","timestamp":"1633160460.0","comment_id":"19448","poster":"cybe001","content":"I choose B"},{"comments":[{"upvote_count":"1","poster":"mattyb123","content":"Please view the big data exam preparation course on aws. It is mentioned quite heavily and the use case matches https://www.aws.training/Details/Curriculum?id=21332","comment_id":"8698","comments":[{"upvote_count":"1","content":"https://www.aws.training/Details/Curriculum?id=21332 -> can not open for some reason, login with credential","comment_id":"8871","comments":[{"poster":"mattyb123","timestamp":"1633085460.0","upvote_count":"1","comment_id":"8909","content":"It's a free course. You just need to sign in with your amazon/aws account or APN account to access the training."}],"timestamp":"1632670260.0","poster":"jlpl"}],"timestamp":"1632514740.0"}],"upvote_count":"1","poster":"jlpl","timestamp":"1632368040.0","comment_id":"8638","content":"Apache Ranger is not AWS product, might not a right choice"}],"choices":{"D":"Configure Security Group on Amazon EMR. Create an Amazon VPC endpoint for Amazon S3. Configure HiveServer2 to use Kerberos authentication on the cluster.","C":"Use AWS KMS for encryption of data. Configure and attach multiple roles with different permissions based on the different user needs.","A":"Configure Transparent Data Encryption on Amazon EMR. Create an Amazon EC2 instance and install Apache Ranger. Configure the authorization on the cluster to use Apache Ranger.","B":"Configure data encryption at rest for EMR File System (EMRFS) on Amazon S3. Configure data encryption in transit for traffic between Amazon S3 and EMRFS. Configure storage and SQL base authorization on HiveServer2."},"isMC":true,"timestamp":"2019-08-26 06:07:00","answers_community":[],"exam_id":17,"question_text":"An organization needs to store sensitive information on Amazon S3 and process it through Amazon EMR. Data must be encrypted on Amazon S3 and Amazon\nEMR at rest and in transit. Using Thrift Server, the Data Analysis team users HIVE to interact with this data. The organization would like to grant access to only specific databases and tables, giving permission only to the SELECT statement.\nWhich solution will protect the data and limit user access to the SELECT statement on a specific portion of data?"}],"exam":{"isMCOnly":true,"lastUpdated":"11 Apr 2025","isBeta":false,"id":17,"name":"AWS Certified Big Data - Specialty","provider":"Amazon","numberOfQuestions":85,"isImplemented":true},"currentPage":14},"__N_SSP":true}