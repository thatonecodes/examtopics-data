{"pageProps":{"questions":[{"id":"jdF2V1IHrIh9F64AgOdg","question_id":76,"answers_community":["C (100%)"],"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/151935-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_images":[],"discussion":[{"upvote_count":"2","timestamp":"1734518760.0","comment_id":"1328393","content":"Selected Answer: C\nC LEAST operational overhead","poster":"7a1d491"},{"timestamp":"1732504860.0","poster":"emupsx1","upvote_count":"1","content":"https://aws.amazon.com/blogs/big-data/set-up-alerts-and-orchestrate-data-quality-rules-with-aws-glue-data-quality/","comment_id":"1317302"}],"topic":"1","choices":{"D":"Create AWS Lambda functions that run data quality queries on the columns data type and the presence of null values. Orchestrate the ETL pipeline by using an AWS Step Functions workflow that runs the Lambda functions. Configure the Step Functions workflow to send an email notification that informs the data engineer about the incomplete datasets to the SNS topic.","C":"Create data quality checks on the source datasets that the daily reports use. Create data quality actions by using AWS Glue workflows to confirm the completeness and consistency of the datasets. Configure the data quality actions to create an event in Amazon EventBridge if a dataset is incomplete. Configure EventBridge to send the event that informs the data engineer about the incomplete datasets to the Amazon SNS topic.","B":"Create data quality checks on the source datasets that the daily reports use. Create a new Amazon EMR cluster. Use Apache Spark SQL to create Apache Spark jobs in the EMR cluster that run data quality queries on the columns data type and the presence of null values. Orchestrate the ETL pipeline by using an AWS Step Functions workflow. Configure the workflow to send an email notification that informs the data engineer about the incomplete datasets to the SNS topic.","A":"Create data quality checks for the source datasets that the daily reports use. Create a new AWS managed Apache Airflow cluster. Run the data quality checks by using Airflow tasks that run data quality queries on the columns data type and the presence of null values. Configure Airflow Directed Acyclic Graphs (DAGs) to send an email notification that informs the data engineer about the incomplete datasets to the SNS topic."},"answer_ET":"C","answer":"C","unix_timestamp":1732504860,"isMC":true,"timestamp":"2024-11-25 04:21:00","answer_description":"","exam_id":21,"question_text":"A company uses AWS Glue Data Catalog to index data that is uploaded to an Amazon S3 bucket every day. The company uses a daily batch processes in an extract, transform, and load (ETL) pipeline to upload data from external sources into the S3 bucket.\n\nThe company runs a daily report on the S3 data. Some days, the company runs the report before all the daily data has been uploaded to the S3 bucket. A data engineer must be able to send a message that identifies any incomplete data to an existing Amazon Simple Notification Service (Amazon SNS) topic.\n\nWhich solution will meet this requirement with the LEAST operational overhead?"},{"id":"UPm9nHOaC1Ol2lifCyzb","topic":"1","exam_id":21,"question_images":[],"question_id":77,"answer_ET":"C","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/150885-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer":"C","answers_community":["C (100%)"],"discussion":[{"content":"Selected Answer: C\nIt's the only answer that match least operation and masking information","comment_id":"1312694","poster":"michele_scar","timestamp":"1731684840.0","upvote_count":"3"},{"poster":"tucobbad","comment_id":"1308012","upvote_count":"4","content":"Selected Answer: C\nTo me, it seems C is the best approach as Redshift has Dynamic Data Masking feature:\nhttps://docs.aws.amazon.com/redshift/latest/dg/t_ddm.html","timestamp":"1730912460.0"}],"answer_images":[],"choices":{"C":"Create a separate Amazon Redshift database role for each team. Define masking policies that apply for each team separately. Attach appropriate masking policies to each team role.","D":"Move the customer data to an Amazon S3 bucket. Use AWS Lake Formation to create a data lake. Use fine-grained security capabilities to grant each team appropriate permissions to access the data.","A":"Create a separate Redshift cluster for each team. Load only the required data for each team. Restrict access to clusters based on the teams.","B":"Create views that include required fields for each of the data requirements. Grant the teams access only to the view that each team requires."},"timestamp":"2024-11-06 18:01:00","question_text":"A company stores customer data that contains personally identifiable information (PII) in an Amazon Redshift cluster. The company's marketing, claims, and analytics teams need to be able to access the customer data.\n\nThe marketing team should have access to obfuscated claim information but should have full access to customer contact information. The claims team should have access to customer information for each claim that the team processes. The analytics team should have access only to obfuscated PII data.\n\nWhich solution will enforce these data access requirements with the LEAST administrative overhead?","unix_timestamp":1730912460,"isMC":true},{"id":"t5GGzFQalaXm9CLcW8en","question_images":[],"answers_community":["A (100%)"],"answer_images":[],"unix_timestamp":1732505940,"answer_description":"","topic":"1","question_text":"A financial company recently added more features to its mobile app. The new features required the company to create a new topic in an existing Amazon Managed Streaming for Apache Kafka (Amazon MSK) cluster.\n\nA few days after the company added the new topic, Amazon CloudWatch raised an alarm on the RootDiskUsed metric for the MSK cluster.\n\nHow should the company address the CloudWatch alarm?","exam_id":21,"url":"https://www.examtopics.com/discussions/amazon/view/151937-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","timestamp":"2024-11-25 04:39:00","question_id":78,"isMC":true,"answer_ET":"A","discussion":[{"comment_id":"1341184","poster":"MerryLew","timestamp":"1736964720.0","content":"Selected Answer: A\n\"RootDiskUsed\" is the percentage of the percentage of root disk used by the broker. Expanding storage and enabling automatic scaling seems like the best bet.","upvote_count":"1"},{"timestamp":"1732505940.0","poster":"emupsx1","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/msk/latest/developerguide/metrics-details.html","upvote_count":"2","comment_id":"1317307"}],"choices":{"D":"Specify the Target Volume-in-GiB parameter for the existing topic.","C":"Update the MSK broker instance to a larger instance type. Restart the MSK cluster.","A":"Expand the storage of the MSK broker. Configure the MSK cluster storage to expand automatically.","B":"Expand the storage of the Apache ZooKeeper nodes."},"answer":"A"},{"id":"8N1yJt5DkQQRKJ9K6abi","answers_community":["AD (89%)","7%"],"question_id":79,"exam_id":21,"question_text":"A data engineer is building a data pipeline on AWS by using AWS Glue extract, transform, and load (ETL) jobs. The data engineer needs to process data from Amazon RDS and MongoDB, perform transformations, and load the transformed data into Amazon Redshift for analytics. The data updates must occur every hour.\nWhich combination of tasks will meet these requirements with the LEAST operational overhead? (Choose two.)","url":"https://www.examtopics.com/discussions/amazon/view/131679-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","isMC":true,"discussion":[{"poster":"rralucard_","upvote_count":"7","content":"Selected Answer: AD\nAWS Glue triggers provide a simple and integrated way to schedule ETL jobs. By configuring these triggers to run hourly, the data engineer can ensure that the data processing and updates occur as required without the need for external scheduling tools or custom scripts. This approach is directly integrated with AWS Glue, reducing the complexity and operational overhead.\nAWS Glue supports connections to various data sources, including Amazon RDS and MongoDB. By using AWS Glue connections, the data engineer can easily configure and manage the connectivity between these data sources and Amazon Redshift. This method leverages AWS Glueâ€™s built-in capabilities for data source integration, thus minimizing operational complexity and ensuring a seamless data flow from the sources to the destination (Amazon Redshift).","timestamp":"1705903080.0","comment_id":"1128365"},{"comment_id":"1226989","timestamp":"1717894140.0","upvote_count":"6","poster":"pypelyncar","content":"Selected Answer: AD\nA. Configure AWS Glue triggers to run the ETL jobs every hour.\n Reduced Code Complexity: Glue triggers eliminate the need to write custom code for scheduling ETL jobs. This simplifies the pipeline and reduces maintenance overhead.\n Scalability and Integration: Glue triggers work seamlessly with Glue ETL jobs, ensuring efficient scheduling and execution within the Glue ecosystem.\nD. Use AWS Glue connections to establish connectivity between the data sources and Amazon Redshift.\n Pre-Built Connectors: Glue connections offer pre-built connectors for various data sources like RDS and Redshift. This eliminates the need for manual configuration and simplifies data source access within the ETL jobs.\n Centralized Management: Glue connections are centrally managed within the Glue service, streamlining connection management and reducing operational overhead."},{"upvote_count":"2","comment_id":"1356172","timestamp":"1739458260.0","poster":"saransh_001","content":"Selected Answer: AD\nA. AWS Glue provides a built-in mechanism to trigger ETL jobs at scheduled intervals, such as every hour. Using Glue triggers minimizes the need for additional custom code or services, reducing operational overhead.\nD. AWS Glue connections simplify the process of establishing secure and reliable connections to various data sources (Amazon RDS, MongoDB) and the destination (Amazon Redshift). This approach reduces the need for manually configuring connection settings and makes the ETL pipeline easier to maintain."},{"comment_id":"1270594","timestamp":"1726895340.0","poster":"San_Juan","content":"Selected Answer: AC\nA. because the question is saying that the jobs are build in Glue, and must run every hour.\nC. because you can run the jobs as Lambda functions every hour.\n\nB. discarted, because the question is saying that \"DE\" is using Glue, DataBrew is for cleaning data without code, but it seems that the \"DE\" is writing code for transforming the data.\nD. Discarted, because the connections are not directly related to the question, that it is saying that you should run every hour Glue jobs, and the connections doesn't seem relevant.\nE. Discarted, because is saying that the data source is RDS and MongoDB, not Redshift, so you cannot use the Redshift Data API for getting the data and transform it.","upvote_count":"1"},{"timestamp":"1723394640.0","content":"AE \nD is not valid. as it shoyld be \nUse AWS Glue connections to establish connectivity between the data sources (including Amazon Redshift) and Glue Job","poster":"sachin","comment_id":"1264231","upvote_count":"1","comments":[{"timestamp":"1723623840.0","content":"An AWS Glue connection is a setting that allows an AWS Glue job to access a data source. This allows you to connect to databases such as RDS, MongoDB, etc. However, this opinion states that this connection is not used to load data directly into Redshift, and that Glue jobs must use the COPY command to load data into Redshift, which is inappropriate. However, since Glue jobs can process data and load it directly into Redshift, it is a bit of a stretch to consider option D as unconditionally wrong.","poster":"samadal","comment_id":"1265569","upvote_count":"1"}]},{"poster":"DevoteamAnalytix","upvote_count":"3","content":"Selected Answer: AD\nI was not sure about A - But in AWS console => Glue => Triggers => Add Trigger I have found the Trigger type: \"Schedule - Fire the trigger on a timer.\"","comment_id":"1206008","timestamp":"1714727520.0"},{"timestamp":"1711913040.0","comment_id":"1186975","content":"Selected Answer: CD\nI found this question actually confusing. In which step the transformation would be implemented itself? I can be wrong, but with Glue triggers we would only run the job, but not the transformation logic itself. In this way, I would go in C and D","poster":"lucas_rfsb","upvote_count":"1"},{"content":"Selected Answer: AD\nNot a clear question - B would kinda make sense - but AD seems to be more correct","comment_id":"1176597","timestamp":"1710775680.0","poster":"milofficial","upvote_count":"3"},{"timestamp":"1709813760.0","content":"Selected Answer: AD\nA - this is obvious and D -https://docs.aws.amazon.com/glue/latest/dg/console-connections.html","upvote_count":"4","comment_id":"1167963","poster":"GiorgioGss"},{"upvote_count":"3","content":"A. Configure AWS Glue triggers to run the ETL jobs every hour.\nD. Use AWS Glue connections to establish connectivity between the data sources and Amazon Redshift.\n\nExplanation:\n\nOption A: Configuring AWS Glue triggers allows the ETL jobs to be scheduled and run automatically every hour without the need for manual intervention. This reduces operational overhead by automating the data processing pipeline.\n\nOption D: Using AWS Glue connections simplifies connectivity between the data sources (Amazon RDS and MongoDB) and Amazon Redshift. Glue connections abstract away the details of connection configuration, making it easier to manage and maintain the data pipeline.","comment_id":"1137937","timestamp":"1706821500.0","poster":"TonyStark0122"},{"timestamp":"1705748340.0","content":"Selected Answer: AB\nLambda triggers for Glue jobs make me dizzy","poster":"milofficial","comment_id":"1127218","upvote_count":"2"}],"unix_timestamp":1705748340,"question_images":[],"timestamp":"2024-01-20 11:59:00","answer_ET":"AD","choices":{"D":"Use AWS Glue connections to establish connectivity between the data sources and Amazon Redshift.","E":"Use the Redshift Data API to load transformed data into Amazon Redshift.","B":"Use AWS Glue DataBrew to clean and prepare the data for analytics.","A":"Configure AWS Glue triggers to run the ETL jobs every hour.","C":"Use AWS Lambda functions to schedule and run the ETL jobs every hour."},"answer_description":"","answer_images":[],"answer":"AD","topic":"1"},{"id":"U7V6JGtJgE5QlSKO9IfW","answer_images":[],"answer":"B","answers_community":["B (100%)"],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/151938-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_text":"A data engineer needs to build an enterprise data catalog based on the company's Amazon S3 buckets and Amazon RDS databases. The data catalog must include storage format metadata for the data in the catalog.\n\nWhich solution will meet these requirements with the LEAST effort?","topic":"1","question_id":80,"unix_timestamp":1732506180,"isMC":true,"timestamp":"2024-11-25 04:43:00","answer_ET":"B","question_images":[],"discussion":[{"poster":"imymoco","content":"https://docs.aws.amazon.com/glue/latest/dg/add-classifier.html","timestamp":"1735080480.0","upvote_count":"1","comment_id":"1331258"},{"poster":"emupsx1","comment_id":"1317309","upvote_count":"1","timestamp":"1732506180.0","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/glue/latest/dg/catalog-and-crawler.html"}],"choices":{"C":"Use Amazon Macie to build a data catalog and to identify sensitive data elements. Collect the data format information from Macie.","D":"Use scripts to scan data elements and to assign data classifications based on the format of the data.","A":"Use an AWS Glue crawler to scan the S3 buckets and RDS databases and build a data catalog. Use data stewards to inspect the data and update the data catalog with the data format.","B":"Use an AWS Glue crawler to build a data catalog. Use AWS Glue crawler classifiers to recognize the format of data and store the format in the catalog."},"exam_id":21}],"exam":{"numberOfQuestions":207,"provider":"Amazon","name":"AWS Certified Data Engineer - Associate DEA-C01","lastUpdated":"11 Apr 2025","isImplemented":true,"id":21,"isBeta":false,"isMCOnly":true},"currentPage":16},"__N_SSP":true}