{"pageProps":{"questions":[{"id":"0eB6v0km0mHG2W6WW4mB","answer_description":"","answer_ET":"A","unix_timestamp":1675155900,"exam_id":28,"choices":{"A":"Throughput Optimized HDD (st1)","B":"Provisioned IOPS SSD (io2)","D":"Cold HDD (sc1)","C":"General Purpose SSD (gp3)"},"answers_community":["A (63%)","D (31%)","6%"],"question_images":[],"discussion":[{"comment_id":"800646","upvote_count":"7","timestamp":"1675754220.0","poster":"trashi","content":"Selected Answer: D\nD\nhttps://aws.amazon.com/ebs/cold-hdd/"},{"upvote_count":"5","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/sap/latest/sap-netweaver/backup-and-recovery.html","comment_id":"827498","timestamp":"1677795900.0","poster":"Deepk12493"},{"content":"Selected Answer: A\nAnswer is A. \n\nD is not suitable because sc1 provides low throughput and high latency. The backup action could impact production","poster":"awsmonster","timestamp":"1707384360.0","upvote_count":"1","comment_id":"1144320"},{"content":"SQL native tools to take backup on disk: Backup requires high throughput compared to IOPS. We recommend using Throughput Optimized HDD (st1) which provides maximum throughput of 500 MB/s per volume. Once the backup completes on disk, you can use scripts to move it to an Amazon S3 bucket","poster":"leotoras","upvote_count":"2","timestamp":"1706666280.0","comment_id":"1136317"},{"comment_id":"1027108","poster":"SONALID","upvote_count":"2","timestamp":"1696657380.0","content":"Answer is A\nSQL native tools to take backup on disk: Backup requires high throughput compared to IOPS. We recommend using Throughput Optimized HDD (st1) which provides maximum throughput of 500 MB/s per volume. Once the backup completes on disk, you can use scripts to move it to an Amazon S3 bucket."},{"poster":"G4Exams","timestamp":"1694398200.0","upvote_count":"2","comment_id":"1004400","content":"A and D are possible but where does the requirement say to have high throughput ?! So more likely D that is more cost effective."},{"content":"Selected Answer: A\nA: For workloads involving frequent, large, and sequential I/O operations, such as log processing and big data workloads, the st1 volume type is an economical and high-performance choice. Since database backups involve sequential write operations, Throughput Optimized HDD (st1) volumes, which are designed for large, sequential I/O workloads, will meet these requirements most cost-effectively.","timestamp":"1690967220.0","upvote_count":"4","poster":"kaishin0527","comment_id":"969965"},{"poster":"[Removed]","timestamp":"1690015380.0","upvote_count":"1","content":"Selected Answer: C\nWhen unsure go for gp3\nhttps://aws.amazon.com/blogs/storage/maximizing-microsoft-sql-server-performance-with-amazon-ebs/","comment_id":"959363","comments":[{"poster":"[Removed]","content":"Changing to A","comment_id":"960210","upvote_count":"1","timestamp":"1690098960.0"}]},{"comment_id":"918664","content":"a-a-a-a-a-a-a-a","poster":"easytoo","timestamp":"1686256080.0","upvote_count":"1"},{"content":"Selected Answer: D\nhttps://docs.aws.amazon.com/sap/latest/sap-hana/cold-data-tiering-options.html#sap-archiving\n\nFor archiving, another option is to use the Amazon Elastic Block Store (Amazon EBS) sc1 volume type as the underlying storage type for your archive file system. Amazon EBS sc1 volumes are inexpensive block storage and are designed for less frequently accessed workloads like data archiving. To increase durability and availability of your archived data, we recommend that you copy the data to Amazon S3 for backup and Amazon S3 Glacier for long term retention.\n\nSC1 IS CHEAPEST","upvote_count":"3","comment_id":"912184","poster":"mawsman","timestamp":"1685630520.0"},{"comment_id":"869069","timestamp":"1681360920.0","content":"Selected Answer: A\n\"For SQL Server database backup, you can use one of the following methods:\nSQL native tools to take backup on disk: Backup requires high throughput compared to IOPS. We recommend using Throughput Optimized HDD (st1) which provides maximum throughput of 500 MB/s per volume.\"","upvote_count":"1","poster":"GiorgioGss"},{"comment_id":"848095","timestamp":"1679569140.0","poster":"Kimzia","content":"Selected Answer: A\n(st1) volumes for SAP HANA to perform file-based backup. This volume type provides low-cost magnetic storage designed for large sequential workloads.","upvote_count":"2"},{"upvote_count":"2","poster":"blanco750","timestamp":"1678197720.0","comment_id":"831971","content":"Selected Answer: A\nThe question could be more clear mentioning throughput requirements but as other have mentioned, a couple of places mentions st1 as recommended HDD volume for backups as it gives high througput and for SAP backups high throughput is required."},{"comments":[{"poster":"trashi","timestamp":"1675934040.0","upvote_count":"3","comment_id":"803024","content":"Why A? sc1 is cheaper as said in https://aws.amazon.com/ebs/cold-hdd/ and a supported option for storing backups."}],"comment_id":"802276","timestamp":"1675875660.0","poster":"schalke04","content":"Selected Answer: A\nA is correct.","upvote_count":"3"},{"poster":"Hyperdanny","content":"I am voting for D. This is not a Hana scenario though, but I don't see a reason to use throughput optimized data for backups in this scenario. Cold is cheaper.","upvote_count":"3","timestamp":"1675701960.0","comment_id":"799969"},{"poster":"Nots","comment_id":"798808","timestamp":"1675600440.0","upvote_count":"4","content":"Selected Answer: A\nI'm torn between A and D.\nThere is also a description that st1 is an option when backing up to EBS as a local backup.\nhttps://docs.aws.amazon.com/wellarchitected/latest/sap-lens/best-practice-14-2.html"},{"poster":"forexamweb","comment_id":"794522","timestamp":"1675191900.0","content":"Selected Answer: D\nD maybe\nhttps://docs.aws.amazon.com/sap/latest/sap-hana/cold-data-tiering-options.html#sap-archiving","upvote_count":"1"},{"timestamp":"1675155900.0","comment_id":"793880","poster":"kk8s","upvote_count":"1","content":"Selected Answer: C\nC for me gp3.\nhttps://aws.amazon.com/ebs/volume-types/"}],"question_text":"A company is running SAP ERP Central Component (SAP ECC) with a Microsoft SQL Server database on AWS. A solutions architect must attach an additional 1 TB Amazon Elastic Block Store (Amazon EBS) volume. The company needs to write the SQL Server database backups to this EBS volume before moving the database backups to Amazon S3 for long-term storage.\nWhich EBS volume type will meet these requirements MOST cost-effectively?","timestamp":"2023-01-31 10:05:00","url":"https://www.examtopics.com/discussions/amazon/view/97393-exam-aws-certified-sap-on-aws-specialty-pas-c01-topic-1/","topic":"1","answer_images":[],"isMC":true,"answer":"A","question_id":71},{"id":"fIVZFUAXBJOoBsauha3U","unix_timestamp":1675173720,"answer_images":[],"choices":{"A":"Reduce the global_allocation_limit parameter to 1,120 GiB.","E":"Change to a supported compute optimized instance type for SAP HANA.","C":"Move to a scale-out architecture for SAP HANA with at least three x1. 16xlarge instances.","D":"Modify the Amazon Elastic Block Store (Amazon EBS) volume type from General Purpose to Provisioned IOPS for all SAP HANA data volumes.","B":"Migrate the SAP HANA database to an EC2 High Memory instance with a larger number of available vCPUs."},"answers_community":["BE (71%)","14%","14%"],"isMC":true,"timestamp":"2023-01-31 15:02:00","answer_ET":"BE","question_images":[],"answer_description":"","discussion":[{"upvote_count":"1","comments":[{"upvote_count":"1","comment_id":"1184429","content":"\"For SAP HANA databases that run in memory, memory optimized (r*, x*, u*) are your only options.\" https://docs.aws.amazon.com/wellarchitected/latest/sap-lens/best-practice-13-2.html","poster":"koki2847","timestamp":"1711577940.0"}],"comment_id":"1184416","content":"Selected Answer: BC\nSAP HANA does not certify compute optimized instances. So E is not preferable I guess. https://docs.aws.amazon.com/sap/latest/general/sap-hana-aws-ec2.html","timestamp":"1711576920.0","poster":"koki2847"},{"timestamp":"1698070320.0","poster":"acethetest1000","content":"Selected Answer: BE\nI think it relates to SAP HANA best practices as it recommends scaling up as much as possible before scaling out.\n\nAs AWS works based on t-shirt sizes and HANA demands memory optimized the next step is High Memory instance type, which in turn will increase the CPU amount.","upvote_count":"2","comment_id":"1051943"},{"timestamp":"1694398740.0","poster":"G4Exams","content":"Selected Answer: BE\nB and E.","upvote_count":"1","comment_id":"1004403"},{"upvote_count":"1","poster":"kaishin0527","comment_id":"969954","content":"Selected Answer: BE\nB,E: This problem is about high CPU usage. The SAP HANA database is not fully utilizing the available memory, but CPU utilization is reaching 100% during peak query times. This suggests that the workload is CPU-bound. Therefore, you can alleviate this issue by adding more CPU resources, either by moving to a larger High Memory instance or switching to a compute-optimized instance type that has more vCPUs available.","timestamp":"1690966260.0"},{"upvote_count":"1","content":"Selected Answer: BE\nOptions B,E","poster":"[Removed]","timestamp":"1690015740.0","comments":[{"comment_id":"959369","content":"Focus on the probable root cause of the performance issue, which is high CPU utilization.","upvote_count":"1","timestamp":"1690015800.0","poster":"[Removed]"}],"comment_id":"959368"},{"poster":"easytoo","upvote_count":"2","timestamp":"1686245280.0","content":"b-c-b-c-b-c","comment_id":"918514"},{"poster":"Shaktimaan","upvote_count":"2","comments":[{"comment_id":"1051940","poster":"acethetest1000","upvote_count":"1","content":"The option B reads: move a High Memory instance which starts with 224vCPU and 3TB of memory: https://aws.amazon.com/ec2/instance-types/#memory-optimized","timestamp":"1698070140.0"}],"comment_id":"833583","content":"A - WRONG, since no need to reduce memory.\nB - WRONG, there is no ec2 instance larger than x1.32xlarge in X1 family.\nC. RIGHT, Will add additional 1.5 times more CPU and Memory.\nD. WRONG, No I/O issue as mentioned.\nE. RIGHT, best way to get more compute and save on memory.","timestamp":"1678334580.0"},{"content":"Selected Answer: CE\nNot 100% sure but eliminating the wrong ones\nA. Reduce the global_allocation_limit parameter to 1,120 GiB. WRONG. Memory has nothing to do with High CPU here so this won't help\nB. Migrate the SAP HANA database to an EC2 High Memory instance with a larger number of available vCPUs. Wrong. we need CPU optimized not memory\nC. Move to a scale-out architecture for SAP HANA with at least three x1. 16xlarge instances. Scaling out actually helps in cases of high CPU utilization\nD. Modify the Amazon Elastic Block Store (Amazon EBS) volume type from General Purpose to Provisioned IOPS for all SAP HANA data volumes. Issue is not related to high DISK I/O as it clearly says I/O wait is not increasing\nE. Change to a supported compute optimized instance type for SAP HANA. this is probabaly the most correct option. use CPU optimized instances","poster":"blanco750","upvote_count":"1","comment_id":"831993","timestamp":"1678198920.0"},{"content":"I am voting for B/C: Change to a more powerful instance type (High Memory type) or scale out .","upvote_count":"3","timestamp":"1675725000.0","poster":"Hyperdanny","comment_id":"800353"},{"comments":[{"comment_id":"828504","upvote_count":"1","timestamp":"1677894420.0","poster":"matakuyy2","content":"I don't think D is the answer.\nI don't think D is the answer, because it says \"I/O wait times are not increasing\", so I don't think I/O is the problem."},{"comment_id":"813816","upvote_count":"2","content":"can't choice B. Because the documentation says \"but when that is not an option (such as scaling up a database instance), have a process in place to do so manually.\"\nSo i choose C and D","timestamp":"1676791500.0","poster":"everydaysmile"}],"comment_id":"794247","upvote_count":"4","timestamp":"1675173720.0","poster":"kk8s","content":"C,D\n\nhttps://docs.aws.amazon.com/wellarchitected/latest/sap-lens/best-practice-16-5.html"}],"answer":"BE","question_text":"Business users are reporting timeouts during periods of peak query activity on an enterprise SAP HANA data mart. An SAP system administrator has discovered that at peak volume, the CPU utilization increases rapidly to 100% for extended periods on the x1.32xlarge Amazon EC2 instance where the database is installed. However, the SAP HANA database is occupying only 1,120 GiB of the available 1,952 GiB on the instance. I/O wait times are not increasing. Extensive query tuning and system tuning have not resolved this performance problem.\nWhich solutions should the SAP system administrator use to improve the performance? (Choose two.)","url":"https://www.examtopics.com/discussions/amazon/view/97448-exam-aws-certified-sap-on-aws-specialty-pas-c01-topic-1/","exam_id":28,"topic":"1","question_id":72},{"id":"Y8vZuIJHo8e4XOu19gRD","answer_ET":"B","answer":"B","question_text":"A company is moving to the AWS Cloud gradually. The company has multiple SAP landscapes on VMware. The company already has sandbox, development, and QA systems on AWS. The company’s production system is still running on premises. The company has 2 months to cut over the entire landscape to the AWS Cloud.\nThe company has adopted a hybrid architecture for the next 2 months and needs to synchronize its shared file systems between the landscapes. These shared file systems include /trans directory mounts, /software directory mounts, and third-party integration mounts. In the on-premises landscape, the company has NFS mounts between the servers. On the AWS infrastructure side, the company is using Amazon Elastic File System (Amazon EFS) to share the common files.\nAn SAP solutions architect needs to design a solution to schedule transfer of these shared files bidirectionally four times each day. The data transfer must be encrypted.\nWhich solution will meet these requirements?","timestamp":"2023-02-08 18:06:00","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/98445-exam-aws-certified-sap-on-aws-specialty-pas-c01-topic-1/","discussion":[{"comments":[{"comment_id":"952964","content":"https://aws.amazon.com/datasync/faqs/","timestamp":"1689479280.0","upvote_count":"1","poster":"[Removed]"}],"content":"Selected Answer: B\nB. The solution is to install an AWS DataSync agent on the on-premises VMware platform. Use the DataSync endpoint to synchronize between the on-premises NFS server and Amazon EFS on AWS.\n\nAWS DataSync is an online data movement and discovery service that simplifies data migration and helps you quickly, easily, and securely move your file or object data to, from, and between AWS storage services. DataSync can copy data to and from Network File System (NFS) file servers, Server Message Block (SMB) file servers, Amazon S3 buckets, Amazon EFS file systems, and AWS Snowcone.\n\nRsync is a utility for efficiently transferring and synchronizing files between a computer and an external hard drive and across networked computers by comparing the modification times and sizes of files. It is not designed for transferring data between on-premises servers and AWS.","poster":"[Removed]","comment_id":"952963","upvote_count":"3","timestamp":"1689479280.0"},{"timestamp":"1686245400.0","poster":"easytoo","comment_id":"918517","upvote_count":"1","content":"b-b-b-b-b-b-b"},{"timestamp":"1675875960.0","content":"Selected Answer: B\nB: AWS DataSync is an online data transfer service that simplifies, automates, and accelerates moving data between storage systems and services.","poster":"schalke04","upvote_count":"4","comment_id":"802281"}],"answer_description":"","question_id":73,"answer_images":[],"question_images":[],"answers_community":["B (100%)"],"isMC":true,"choices":{"C":"Order an AWS Snowcone device. Use the Snowcone device to transfer data between the on-premises servers and AWS.","D":"Set up a separate AWS Direct Connect connection for synchronization between the on-premises servers and AWS.","A":"Write an rsync script. Schedule the script through cron for four times each day in the on-premises VMware servers to transfer the data from on premises to AWS.","B":"Install an AWS DataSync agent on the on-premises VMware platform. Use the DataSync endpoint to synchronize between the on-premises NFS server and Amazon EFS on AWS."},"unix_timestamp":1675875960,"exam_id":28},{"id":"4dxOYGYl2A2963yBIGaK","answer_ET":"C","answer":"C","topic":"1","question_text":"A company is planning to move to AWS. The company wants to set up sandbox and test environments on AWS to perform proofs of concept (POCs). Development and production environments will remain on premises until the POCs are completed.\nAt the company’s on-premises location, SAProuter is installed on the same server as SAP Solution Manager. The company uses SAP Solution Manager to monitor the entire landscape. The company uses SAProuter to connect to SAP Support. The on-premises SAP Solution Manager instance must monitor the performance and server metrics of the newly created POC systems on AWS. The existing SAProuter must be able to report any issues to SAP.\nWhat should an SAP solutions architect do to set up this hybrid infrastructure MOST cost-effectively?","answer_description":"","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/98373-exam-aws-certified-sap-on-aws-specialty-pas-c01-topic-1/","choices":{"B":"Install a new SAP Solution Manager instance and a new SAProuter instance in the AWS environment. Install the Amazon CloudWatch agent on all on-premises instances. Push the monitoring data to the new SAP Solution Manager instance. Connect all on-premises systems and POC systems on AWS to the new SAP Solution Manager instance and the new SAProuter instance. Remove the on-premises SAP Solution Manager instance and the on-premises SAProuter instance. Use the new instances on AWS.","A":"Install a new SAP Solution Manager instance and a new SAProuter instance in the AWS environment. Connect the POC systems to these new instances. Use these new instances in parallel with the on-premises SAP Solution Manager instance and the on-premises SAProuter instance.","D":"Add the POC systems on AWS to the existing SAP Transport Management System that is configured in the on-premises SAP systems.","C":"Use AWS Site-to-Site VPN to connect the on-premises network to the AWS environment. Connect the POC systems on AWS to the on-premises SAP Solution Manager instance and the on-premises SAProuter instance."},"timestamp":"2023-02-07 22:52:00","exam_id":28,"answer_images":[],"unix_timestamp":1675806720,"isMC":true,"answers_community":["C (100%)"],"discussion":[{"timestamp":"1706714880.0","upvote_count":"1","comment_id":"1136910","content":"Selected Answer: C\nIt reads: the existing SAP Router must remain, hence I don't believe SolMan/SAP Router should be migrated. Besides, the company will start paying for another instance on AWS.","poster":"acethetest1000"},{"content":"Selected Answer: C\nC is the most cost effective in terms of infrastructure and services.","comment_id":"1112883","poster":"acethetest1000","upvote_count":"1","timestamp":"1704295020.0"},{"poster":"[Removed]","upvote_count":"1","timestamp":"1690016520.0","content":"Selected Answer: C\nGoing C","comment_id":"959372"},{"timestamp":"1678199760.0","poster":"blanco750","upvote_count":"2","comment_id":"832006","content":"Selected Answer: C\nC\nhttps://docs.aws.amazon.com/sap/latest/general/overview-sap-planning.html#figure-4"},{"comment_id":"802282","poster":"schalke04","content":"Selected Answer: C\nC makes sense","upvote_count":"2","timestamp":"1675876080.0"},{"comment_id":"801485","poster":"Hyperdanny","upvote_count":"3","timestamp":"1675806720.0","content":"I am voting C:\n\nhttps://docs.aws.amazon.com/sap/latest/general/overview-router-hybrid.html"}],"question_id":74},{"id":"h1WeifozF681nzM8SWX0","isMC":true,"topic":"1","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/98371-exam-aws-certified-sap-on-aws-specialty-pas-c01-topic-1/","question_text":"An SAP solutions architect is using AWS Systems Manager Distributor to install the AWS Data Provider for SAP on production SAP application servers and SAP HANA database servers. The SAP application servers and the SAP HANA database servers are running on Red Hat Enterprise Linux.\nThe SAP solutions architect chooses instances manually in Systems Manager Distributor and schedules installation. The installation fails with an access and authorization error related to Amazon CloudWatch and Amazon EC2 instances. There is no error related to AWS connectivity.\nWhat should the SAP solutions architect do to resolve the error?","unix_timestamp":1675805640,"question_images":[],"question_id":75,"answers_community":["C (100%)"],"answer_description":"","exam_id":28,"answer":"C","discussion":[{"comment_id":"801473","content":"c - https://docs.aws.amazon.com/sap/latest/general/data-provider-troubleshooting.html","poster":"ohcn","upvote_count":"5","timestamp":"1675805640.0"},{"upvote_count":"1","content":"Selected Answer: C\nMost likely C","poster":"G4Exams","timestamp":"1694399040.0","comment_id":"1004404"},{"content":"Selected Answer: C\nC: The AWS Data Provider for SAP requires access to Amazon CloudWatch and Amazon EC2 instances. It retrieves this access by assuming an IAM role that is attached to the EC2 instances. If this role is missing or doesn't have the appropriate permissions, the AWS Data Provider for SAP installation will fail with an access and authorization error. To resolve this error, you must create an IAM role and attach the appropriate policy to it, then attach this role to the EC2 instances.","comment_id":"969951","timestamp":"1690966020.0","poster":"kaishin0527","upvote_count":"1"},{"comment_id":"802285","upvote_count":"4","content":"Selected Answer: C\nC correct","timestamp":"1675876200.0","poster":"schalke04"},{"upvote_count":"3","timestamp":"1675807260.0","comment_id":"801495","poster":"Hyperdanny","content":"I am voting for C. The Data Provider doesn't seem to have the right authorizations available."}],"timestamp":"2023-02-07 22:34:00","answer_ET":"C","choices":{"D":"Wait until Systems Manager Agent is fully installed and ready to use on the EC2 instances. Use Systems Manager Patch Manager to perform the installation.","C":"Create an IAM role. Attach the appropriate policy to the role. Attach the role to the appropriate EC2 instances.","A":"Install the CloudWatch agent on the servers before installing the AWS Data Provider for SAP.","B":"Download the AWS Data Provider for SAP installation package from AWS Marketplace. Use an operating system super user to install the agent manually or through a script."}}],"exam":{"isMCOnly":true,"id":28,"isBeta":false,"name":"AWS Certified SAP on AWS - Specialty PAS-C01","provider":"Amazon","numberOfQuestions":130,"isImplemented":true,"lastUpdated":"11 Apr 2025"},"currentPage":15},"__N_SSP":true}