{"pageProps":{"questions":[{"id":"P2ON0ECNRy7k0rDkxllw","answer_ET":"D","answer_description":"","question_images":[],"exam_id":21,"question_text":"A data engineer needs to debug an AWS Glue job that reads from Amazon S3 and writes to Amazon Redshift. The data engineer enabled the bookmark feature for the AWS Glue job.\nThe data engineer has set the maximum concurrency for the AWS Glue job to 1.\n\nThe AWS Glue job is successfully writing the output to Amazon Redshift. However, the Amazon S3 files that were loaded during previous runs of the AWS Glue job are being reprocessed by subsequent runs.\n\nWhat is the likely reason the AWS Glue job is reprocessing the files?","url":"https://www.examtopics.com/discussions/amazon/view/143046-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer":"D","discussion":[{"poster":"lool","upvote_count":"8","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/glue/latest/dg/glue-troubleshooting-errors.html#error-job-bookmarks-reprocess-data","timestamp":"1720273200.0","comment_id":"1243389"},{"content":"Selected Answer: D\nA \"commit\" statement within your AWS Glue job script is absolutely required to update the job bookmark and properly track processed data, preventing the reprocessing of old data when running the job again; essentially, if you don't include the commit statement, the job will not remember where it left off and may process data multiple times. For more information about job.commit(), please reference this documentation - https://docs.aws.amazon.com/glue/latest/dg/glue-troubleshooting-errors.html#error-job-bookmarks-reprocess-data","timestamp":"1731000120.0","poster":"AgboolaKun","upvote_count":"2","comment_id":"1308487"},{"content":"Selected Answer: D\nIt's B the right answer","timestamp":"1729603860.0","poster":"rsmf","upvote_count":"2","comment_id":"1301595"},{"upvote_count":"2","content":"Selected Answer: A\nCommit statements are relevant to transactional operations in databases like Redshift but are not related to S3 bookmarks or Glueâ€™s tracking mechanism for processed files.","timestamp":"1729408020.0","comment_id":"1300318","poster":"mohamedTR"},{"timestamp":"1728036180.0","comment_id":"1293090","poster":"proserv","upvote_count":"2","content":"Selected Answer: D\nEnsure that your job run script ends with the following commit:\n\njob.commit()\n\nWhen you include this object, AWS Glue records the timestamp and path of the job run. If you run the job again with the same path, AWS Glue processes only the new files. If you don't include this object and job bookmarks are enabled, the job reprocesses the already processed files along with the new files and creates redundancy in the job's target data store.\n\nhttps://docs.aws.amazon.com/glue/latest/dg/glue-troubleshooting-errors.html#error-job-bookmarks-reprocess-data"},{"content":"Selected Answer: A\nI would go with A option","timestamp":"1726603740.0","comment_id":"1285443","upvote_count":"1","poster":"azure_bimonster"},{"timestamp":"1725591480.0","comment_id":"1279301","poster":"EJGisME","content":"Selected Answer: A\nA. The AWS Glue job does not have the s3:GetObjectAcl permission that is required for bookmarks to work correctly.","upvote_count":"1"},{"poster":"mzansikiller","upvote_count":"1","timestamp":"1724020380.0","content":"Selected Answer: A\nAnswer A\n\nthis is a job bookmarks permissions issue","comment_id":"1268231"},{"content":"Selected Answer: A\nFor AWS Glue bookmarks to function correctly, the job needs the necessary permissions to read and write bookmark data, including the s3:GetObjectAcl permission. If these permissions are not correctly set, the job may not be able to track which files have already been processed, leading to reprocessing of previously processed files.","poster":"antun3ra","upvote_count":"4","comment_id":"1262573","timestamp":"1723131840.0"},{"timestamp":"1721276160.0","upvote_count":"2","poster":"andrologin","content":"Selected Answer: D\nAWS Glue Job requires the commit statement to save the last successful run/processing","comment_id":"1250093"},{"content":"Selected Answer: D\nFor me - D looks correct","upvote_count":"3","timestamp":"1719929220.0","comment_id":"1240833","poster":"HunkyBunky"},{"comment_id":"1239451","comments":[{"comment_id":"1250092","poster":"andrologin","upvote_count":"1","content":"It is the commit statement that ensures AWS saves the last successful processing","timestamp":"1721276100.0"},{"poster":"HunkyBunky","content":"I've not found any information that s3:GetObjectACL is necessary for Glue bookmarks, so I'm pretty sure that A is wrong","timestamp":"1719979680.0","upvote_count":"1","comment_id":"1241138"}],"upvote_count":"3","content":"Selected Answer: A\nThe commit statement (Option D) is not required for AWS Glue jobs. AWS Glue commits any open transactions to the database when all the script statements finish running.","timestamp":"1719706860.0","poster":"Alagong"},{"comment_id":"1239061","timestamp":"1719641700.0","poster":"Bmaster","upvote_count":"4","content":"D is good\n\nhttps://docs.aws.amazon.com/glue/latest/dg/glue-troubleshooting-errors.html#error-job-bookmarks-reprocess-data"}],"answer_images":[],"question_id":11,"timestamp":"2024-06-29 08:15:00","choices":{"B":"The maximum concurrency for the AWS Glue job is set to 1.","C":"The data engineer incorrectly specified an older version of AWS Glue for the Glue job.","A":"The AWS Glue job does not have the s3:GetObjectAcl permission that is required for bookmarks to work correctly.","D":"The AWS Glue job does not have a required commit statement."},"isMC":true,"answers_community":["D (61%)","A (39%)"],"unix_timestamp":1719641700,"topic":"1"},{"id":"JG1Sy4gdmRfIa3jod11G","answer_ET":"B","answer_description":"","question_images":[],"exam_id":21,"url":"https://www.examtopics.com/discussions/amazon/view/143047-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_text":"An ecommerce company wants to use AWS to migrate data pipelines from an on-premises environment into the AWS Cloud. The company currently uses a third-party tool in the on-premises environment to orchestrate data ingestion processes.\n\nThe company wants a migration solution that does not require the company to manage servers. The solution must be able to orchestrate Python and Bash scripts. The solution must not require the company to refactor any code.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","answer":"B","answer_images":[],"discussion":[{"upvote_count":"2","comment_id":"1242444","poster":"HunkyBunky","timestamp":"1720149240.0","content":"Selected Answer: B\nB - because company want to use same tool on premises and least operational overhead"},{"comment_id":"1241595","poster":"didorins","content":"Selected Answer: C\n\"The company wants a migration solution that does not require the company to manage servers.\". How is it Amazon Managed Workflows for Apache Airflow and not Step Functions when Step Functions is the serverless of the two ?","comments":[{"timestamp":"1720095720.0","content":"\"All of the components contained in the outer box (in the image below) appear as a single Amazon MWAA environment in your account. The Apache Airflow Scheduler and Workers are AWS Fargate (Fargate) containers that connect to the private subnets in the Amazon VPC for your environment. Each environment has its own Apache Airflow metadatabase managed by AWS that is accessible to the Scheduler and Workers Fargate containers via a privately-secured VPC endpoint.\"\n\nhttps://docs.aws.amazon.com/mwaa/latest/userguide/what-is-mwaa.html\n\nSo MWAA is hosted on ECS Fargate - serverless. Supports Bash and Python. B is correct.","poster":"didorins","upvote_count":"4","comment_id":"1242064"}],"upvote_count":"1","timestamp":"1720032120.0"},{"upvote_count":"3","content":"Selected Answer: B\nAn ecommerce company wants to use AWS to migrate data pipelines from an on-premises environment into the AWS Cloud. The company currently uses a third-party tool in the on-premises environment to orchestrate data ingestion processes.\n\nThe company wants a migration solution that does not require the company to manage servers. The solution must be able to orchestrate Python and Bash scripts. The solution must not require the company to refactor any code.\n\nWhich solution will meet these requirements with the LEAST operational overhead?\n\nA. AWS Lambda\nB. Amazon Managed Workflows for Apache Airflow (Amazon MVVAA)\nC. AWS Step Functions\nD. AWS Glue","poster":"Ja13","timestamp":"1720021440.0","comment_id":"1241497"},{"comment_id":"1240834","poster":"HunkyBunky","upvote_count":"1","content":"Selected Answer: B\nB - best fits in task requirements","timestamp":"1719929340.0"},{"comment_id":"1239064","upvote_count":"2","timestamp":"1719642000.0","content":"My Choice is B","poster":"Bmaster"}],"question_id":12,"timestamp":"2024-06-29 08:20:00","choices":{"D":"AWS Glue","B":"Amazon Managed Workflows for Apache Airflow (Amazon MVVAA)","C":"AWS Step Functions","A":"AWS Lambda"},"isMC":true,"answers_community":["B (86%)","14%"],"unix_timestamp":1719642000,"topic":"1"},{"id":"l6atByHKAMPCauQHRONx","question_id":13,"answer_description":"","isMC":true,"timestamp":"2024-01-18 09:47:00","exam_id":21,"url":"https://www.examtopics.com/discussions/amazon/view/131473-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer":"C","topic":"1","unix_timestamp":1705567620,"choices":{"B":"Compress the .csv files by using Snappy compression.","A":"Change the data format from .csv to JSON format. Apply Snappy compression.","C":"Change the data format from .csv to Apache Parquet. Apply Snappy compression.","D":"Compress the .csv files by using gzip compression."},"question_images":[],"answer_ET":"C","discussion":[{"poster":"milofficial","comment_id":"1125641","comments":[{"comment_id":"1127566","poster":"[Removed]","upvote_count":"1","timestamp":"1705800120.0","content":"Hahahaha! I believe that this kind of question is only for the beta calibration purpose. They won't be in the final exam version."}],"timestamp":"1705567620.0","upvote_count":"11","content":"Selected Answer: C\nIf the exam would only have these kinds of questions everyone would be blessed"},{"poster":"TonyStark0122","timestamp":"1706818620.0","content":"C. Change the data format from .csv to Apache Parquet. Apply Snappy compression.\n\nExplanation:\nApache Parquet is a columnar storage format optimized for analytical queries. It is highly efficient for query performance, especially when queries involve selecting specific columns, as it allows for column pruning and predicate pushdown optimizations.","comment_id":"1137892","upvote_count":"6"},{"upvote_count":"1","content":"Selected Answer: C\nC is correct","comment_id":"1410025","timestamp":"1742905740.0","poster":"Scotty_Nguyen"},{"comment_id":"1254504","upvote_count":"1","poster":"GabrielSGoncalves","content":"Selected Answer: C\nC is the way to do It based on best practices recommended by AWS (https://aws.amazon.com/pt/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/)","timestamp":"1721834520.0"},{"content":"Selected Answer: C\nC is correct","poster":"hnk","timestamp":"1715490600.0","upvote_count":"1","comment_id":"1210121"},{"timestamp":"1715294400.0","upvote_count":"1","comment_id":"1209089","content":"Selected Answer: C\nswitching to Apache Parquet format with Snappy compression offers the most significant improvement in Athena query performance, especially for queries that select specific columns","poster":"k350Secops"},{"comment_id":"1207115","poster":"d8945a1","timestamp":"1714958580.0","upvote_count":"1","content":"Selected Answer: C\nParquet is columnar storage and the question specifies that users performs most queries by selecting a specific column."},{"content":"Selected Answer: C\nhttps://aws.amazon.com/jp/blogs/news/top-10-performance-tuning-tips-for-amazon-athena/","timestamp":"1712580360.0","upvote_count":"2","poster":"wa212","comment_id":"1191578"},{"upvote_count":"1","poster":"Alcee","comment_id":"1158040","timestamp":"1708792620.0","content":"C easy"}],"question_text":"A data engineer needs Amazon Athena queries to finish faster. The data engineer notices that all the files the Athena queries use are currently stored in uncompressed .csv format. The data engineer also notices that users perform most queries by selecting a specific column.\nWhich solution will MOST speed up the Athena query performance?","answers_community":["C (100%)"],"answer_images":[]},{"id":"NN6uiHStJJsoMvogsNkU","answer":"B","discussion":[{"upvote_count":"1","poster":"andrologin","comment_id":"1254560","content":"Selected Answer: B\nAWS DMS allows for change data capture that will have the destination updated at near real time with changes from the source database","timestamp":"1721844240.0"},{"comment_id":"1254524","content":"Selected Answer: B\nShould B. Makes most sense.","upvote_count":"1","timestamp":"1721837760.0","poster":"Fredrik1"},{"comment_id":"1253127","content":"Chatgpt\nOption B (AWS DMS) is the most suitable with the least development effort. AWS DMS supports continuous data replication with CDC capabilities, making it well-suited for near real-time data integration from MySQL to Amazon Redshift. It handles schema conversion and simplifies the setup process compared to custom development or scheduled ETL jobs. Given the existing AWS Direct Connect, AWS DMS can efficiently replicate MySQL updates to Redshift with minimal latency, meeting the company's requirement for near real-time insights integration. Therefore, option B is the correct choice.","timestamp":"1721657400.0","poster":"Chelseajcole","upvote_count":"1"},{"comment_id":"1239075","upvote_count":"3","poster":"Bmaster","timestamp":"1719643320.0","content":"B is good. DMS+CDC...\n\nhttps://aws.amazon.com/ko/blogs/apn/change-data-capture-from-on-premises-sql-server-to-amazon-redshift-target/"}],"choices":{"D":"Run scheduled AWS DataSync tasks to synchronize data from the MySQL database. Set Amazon Redshift as the destination for the tasks.","C":"Use the Amazon AppFlow SDK to build a custom connector for the MySQL database to continuously replicate the database changes. Set Amazon Redshift as the destination for the connector.","A":"Run a scheduled AWS Glue extract, transform, and load (ETL) job to get the MySQL database updates by using a Java Database Connectivity (JDBC) connection. Set Amazon Redshift as the destination for the ETL job.","B":"Run a full load plus CDC task in AWS Database Migration Service (AWS DMS) to continuously replicate the MySQL database changes. Set Amazon Redshift as the destination for the task."},"answer_description":"","answer_images":[],"exam_id":21,"timestamp":"2024-06-29 08:42:00","unix_timestamp":1719643320,"url":"https://www.examtopics.com/discussions/amazon/view/143051-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_text":"A retail company stores data from a product lifecycle management (PLM) application in an on-premises MySQL database. The PLM application frequently updates the database when transactions occur.\n\nThe company wants to gather insights from the PLM application in near real time. The company wants to integrate the insights with other business datasets and to analyze the combined dataset by using an Amazon Redshift data warehouse.\n\nThe company has already established an AWS Direct Connect connection between the on-premises infrastructure and AWS.\n\nWhich solution will meet these requirements with the LEAST development effort?","answers_community":["B (100%)"],"question_id":14,"answer_ET":"B","isMC":true,"question_images":[],"topic":"1"},{"id":"MYA0pP4zVyD0unCKwb2K","timestamp":"2024-06-29 08:52:00","isMC":true,"answer_images":[],"topic":"1","answer_ET":"C","answer":"C","discussion":[{"comment_id":"1241500","content":"Selected Answer: C\nC. Amazon Athena\n\nHere's why Amazon Athena is suitable:\n\nServerless: Amazon Athena is a serverless query service that allows you to run SQL queries directly on data stored in Amazon S3 without the need to manage infrastructure.\nPartitioning: Athena supports querying data by partitioning, which can significantly improve query performance by limiting the amount of data scanned.\nACID Properties: Although Amazon S3 itself does not provide ACID properties, Amazon Athena ensures consistency in query results and durability of the data stored in S3 through its managed query execution.\nCost-effective: With Amazon Athena, you only pay for the queries you run and the amount of data scanned, making it a cost-effective choice compared to managing infrastructure or using dedicated services like Amazon Redshift Spectrum or Amazon EMR.","poster":"Ja13","timestamp":"1720021680.0","upvote_count":"5"},{"upvote_count":"2","content":"Selected Answer: C\nC. Amazon Athena","poster":"EJGisME","timestamp":"1726114200.0","comment_id":"1282428"},{"poster":"EJGisME","content":"Selected Answer: C\nAmazon Redshift Spectrum is not serverless.","timestamp":"1725608640.0","comment_id":"1279466","upvote_count":"2"},{"comment_id":"1250809","poster":"andrologin","timestamp":"1721361660.0","content":"Selected Answer: B\nAthena is cost effective as it only charges for queries run","upvote_count":"2"},{"timestamp":"1719929880.0","poster":"HunkyBunky","comment_id":"1240838","content":"C - cheapest solution in this case","upvote_count":"1"},{"upvote_count":"1","content":"C is good.","poster":"Bmaster","comment_id":"1239081","timestamp":"1719643920.0"}],"url":"https://www.examtopics.com/discussions/amazon/view/143053-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","exam_id":21,"answer_description":"","unix_timestamp":1719643920,"question_text":"A marketing company uses Amazon S3 to store clickstream data. The company queries the data at the end of each day by using a SQL JOIN clause on S3 objects that are stored in separate buckets.\n\nThe company creates key performance indicators (KPIs) based on the objects. The company needs a serverless solution that will give users the ability to query data by partitioning the data. The solution must maintain the atomicity, consistency, isolation, and durability (ACID) properties of the data.\n\nWhich solution will meet these requirements MOST cost-effectively?","answers_community":["C (82%)","B (18%)"],"question_images":[],"choices":{"C":"Amazon Athena","A":"Amazon S3 Select","D":"Amazon EMR","B":"Amazon Redshift Spectrum"},"question_id":15}],"exam":{"numberOfQuestions":207,"lastUpdated":"11 Apr 2025","provider":"Amazon","isMCOnly":true,"isBeta":false,"id":21,"isImplemented":true,"name":"AWS Certified Data Engineer - Associate DEA-C01"},"currentPage":3},"__N_SSP":true}