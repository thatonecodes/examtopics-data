{"pageProps":{"questions":[{"id":"eBkb3ssamRqN1UPPLzkV","answer_description":"","answer_ET":"D","question_images":[],"answer":"D","answer_images":[],"choices":{"A":"Publish flow logs to Amazon CloudWatch Logs. Use Amazon Athena for analytics.","B":"Publish flow logs to Amazon CloudWatch Logs. Use an Amazon OpenSearch Service cluster for analytics.","C":"Publish flow logs to Amazon S3 in text format. Use Amazon Athena for analytics.","D":"Publish flow logs to Amazon S3 in Apache Parquet format. Use Amazon Athena for analytics."},"timestamp":"2024-06-15 09:59:00","exam_id":21,"question_text":"An online retail company has an application that runs on Amazon EC2 instances that are in a VPC. The company wants to collect flow logs for the VPC and analyze network traffic.\n\nWhich solution will meet these requirements MOST cost-effectively?","url":"https://www.examtopics.com/discussions/amazon/view/142567-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_id":201,"isMC":true,"discussion":[{"comments":[{"upvote_count":"1","poster":"koki2847","timestamp":"1721402400.0","content":"https://aws.amazon.com/about-aws/whats-new/2021/10/amazon-vpc-flow-logs-parquet-hive-prefixes-partitioned-files/","comment_id":"1251277"}],"content":"Selected Answer: D\nFlow Logs can be published to S3 in Parquet format: https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs-s3.html#flow-logs-s3-path","comment_id":"1230819","poster":"tgv","upvote_count":"6","timestamp":"1718438340.0"},{"comment_id":"1234755","poster":"PGGuy","upvote_count":"5","timestamp":"1718993160.0","content":"Selected Answer: D\nPublishing flow logs to Amazon S3 in Apache Parquet format and using Amazon Athena for analytics (D) is the most cost-effective solution. This approach minimizes storage costs due to the efficient compression of Parquet, and optimizes query performance and cost in Athena due to the reduced data size and optimized columnar storage."},{"comments":[{"timestamp":"1740521700.0","comment_id":"1361603","content":"how would text be more cost effective than columnar?","poster":"Ell89","upvote_count":"1"}],"comment_id":"1258718","content":"The question says clearly most cost effective, so on comparison between C and D, has to be C","timestamp":"1722414180.0","poster":"jyrajan69","upvote_count":"2"},{"comment_id":"1249669","poster":"LR2023","content":"Selected Answer: B\nFlow logs acn be published to S3 but then option D sas in Parquet format - it is not automatically converted into parquet....\nhttps://aws.amazon.com/solutions/implementations/centralized-logging-with-opensearch/","upvote_count":"1","timestamp":"1721225280.0"},{"poster":"HunkyBunky","timestamp":"1719227520.0","comment_id":"1236241","upvote_count":"2","content":"Selected Answer: D\nApache parquet and S3 = most cost-effective solution"}],"answers_community":["D (93%)","7%"],"unix_timestamp":1718438340,"topic":"1"},{"id":"j0er2KlKvcfUvm5wf8Zt","timestamp":"2024-06-15 10:01:00","topic":"1","question_images":[],"exam_id":21,"question_id":202,"isMC":true,"answer":"A","unix_timestamp":1718438460,"url":"https://www.examtopics.com/discussions/amazon/view/142568-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answers_community":["A (100%)"],"answer_ET":"A","answer_description":"","choices":{"B":"Change the distribution style of the store location table to KEY distribution based on the column that has the highest dimension.","C":"Add a join column named store_id into the sort key for all the tables.","A":"Change the distribution style of the store location table from EVEN distribution to ALL distribution.","D":"Upgrade the Redshift reserved node to a larger instance size in the same instance family."},"answer_images":[],"question_text":"A retail company stores transactions, store locations, and customer information tables in four reserved ra3.4xlarge Amazon Redshift cluster nodes. All three tables use even table distribution.\n\nThe company updates the store location table only once or twice every few years.\n\nA data engineer notices that Redshift queues are slowing down because the whole store location table is constantly being broadcast to all four compute nodes for most queries. The data engineer wants to speed up the query performance by minimizing the broadcasting of the store location table.\n\nWhich solution will meet these requirements in the MOST cost-effective way?","discussion":[{"timestamp":"1721159520.0","content":"Selected Answer: A\nALL distribution is optimal for slowly changing dimension tables and generally small in size to allow for optimal joins.","comment_id":"1249195","poster":"andrologin","upvote_count":"2"},{"comment_id":"1240742","poster":"bakarys","content":"Selected Answer: A\nThe most cost-effective solution to speed up the query performance by minimizing the broadcasting of the store location table would be:\n\nA. Change the distribution style of the store location table from EVEN distribution to ALL distribution.\n\nIn Amazon Redshift, the ALL distribution style replicates the entire table to all nodes in the cluster, which eliminates the need to redistribute the data when executing a query. This can significantly improve query performance. Given that the store location table is updated only once or twice every few years, the overhead of maintaining the replicated data would be minimal. This makes it a cost-effective solution for improving the query performance.","timestamp":"1719920400.0","upvote_count":"2"},{"comment_id":"1234756","content":"Selected Answer: A\nChanging the distribution style of the store location table to ALL distribution (A) is the most cost-effective solution. It directly addresses the issue of broadcasting by ensuring the entire table is available on each node, significantly improving join performance without incurring substantial additional costs.","poster":"PGGuy","timestamp":"1718993220.0","upvote_count":"4"},{"content":"Selected Answer: A\nUsing ALL distribution means the table is replicated to all nodes, eliminating the need for broadcasting during queries. Since the store location table is updated infrequently, this will significantly speed up queries without incurring frequent update costs.","timestamp":"1718438460.0","upvote_count":"2","poster":"tgv","comment_id":"1230821"}]},{"id":"rkajapRRoYHwJVHKbNfW","question_id":203,"answer_images":[],"answer":"B","timestamp":"2024-06-15 10:02:00","answer_description":"","isMC":true,"discussion":[{"content":"Selected Answer: B\nRegex Patterns for everyone's reference\n\n. : Matches any single character.\n* : Matches zero or more of the preceding element.\n+ : Matches one or more of the preceding element.\n[abc] : Matches any of the enclosed characters.\n[^abc] : Matches any character not enclosed.\n^ : Matches the start of a string.\n$ : Matches the end of a string.\n| : Logical OR operator.\n(abc) : Matches 'abc' and remembers the match.\n\nAnswer is B","poster":"chrispchrisp","upvote_count":"7","comment_id":"1253703","timestamp":"1721743200.0"},{"comment_id":"1249199","content":"Selected Answer: B\nRegex patterns:\n^ - used to capture the start of the text/string\n| - used as an OR operator","timestamp":"1721159580.0","poster":"andrologin","upvote_count":"1"},{"poster":"bakarys","timestamp":"1719920520.0","comment_id":"1240748","content":"Selected Answer: B\nB. Select * from Sales where city_name ~ ‘^(San|El)*’;\n\nThis query uses a regular expression pattern with the ~ operator. The caret ^ at the beginning of the pattern indicates that the match must start at the beginning of the string. (San|El) matches either “San” or “El”, and * means zero or more of the preceding element. So this query will return all rows where city_name starts with either “San” or “El”.","upvote_count":"1"},{"poster":"HunkyBunky","content":"Selected Answer: B\nB - becuase of regexp","timestamp":"1718932980.0","comment_id":"1234148","upvote_count":"1"},{"poster":"JohnYang","comment_id":"1232616","content":"Selected Answer: B\n^ asserts the position at the start of the string.\n(San|El) matches either \"San\" or \"El\".","upvote_count":"3","timestamp":"1718748120.0"},{"poster":"tgv","comment_id":"1230822","content":"Selected Answer: B\n~: This operator indicates the use of a regular expression.\n^: This symbol signifies the start of the string.\n(San|El): This pattern matches strings that start with either \"San\" or \"El\".","timestamp":"1718438520.0","upvote_count":"2"}],"question_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/142569-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","choices":{"D":"Select * from Sales where city_name ~ ‘^(San&El)*’;","B":"Select * from Sales where city_name ~ ‘^(San|El)*’;","A":"Select * from Sales where city_name ~ ‘$(San|El)*’;","C":"Select * from Sales where city_name ~’$(San&El)*’;"},"answers_community":["B (100%)"],"exam_id":21,"unix_timestamp":1718438520,"answer_ET":"B","question_text":"A company has a data warehouse that contains a table that is named Sales. The company stores the table in Amazon Redshift. The table includes a column that is named city_name. The company wants to query the table to find all rows that have a city_name that starts with \"San\" or \"El\".\n\nWhich SQL query will meet this requirement?"},{"id":"qcP7hpvIXqnPUhakugeB","answer_description":"","question_text":"A company needs to send customer call data from its on-premises PostgreSQL database to AWS to generate near real-time insights. The solution must capture and load updates from operational data stores that run in the PostgreSQL database. The data changes continuously.\n\nA data engineer configures an AWS Database Migration Service (AWS DMS) ongoing replication task. The task reads changes in near real time from the PostgreSQL source database transaction logs for each table. The task then sends the data to an Amazon Redshift cluster for processing.\n\nThe data engineer discovers latency issues during the change data capture (CDC) of the task. The data engineer thinks that the PostgreSQL source database is causing the high latency.\n\nWhich solution will confirm that the PostgreSQL database is the source of the high latency?","exam_id":21,"choices":{"B":"Verify that logical replication of the source database is configured in the postgresql.conf configuration file.","C":"Enable Amazon CloudWatch Logs for the DMS endpoint of the source database. Check for error messages.","D":"Use Amazon CloudWatch to monitor the DMS task. Examine the CDCLatencySource metric to identify delays in the CDC from the source database.","A":"Use Amazon CloudWatch to monitor the DMS task. Examine the CDCIncomingChanges metric to identify delays in the CDC from the source database."},"discussion":[{"content":"Selected Answer: D\nCDCLatencySource Metric: This metric measures the latency between the source database and the DMS task. It shows how long it takes for changes to be read from the source database's transaction logs.\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Monitoring.html#CHAP_Monitoring.Metrics","timestamp":"1718439060.0","comment_id":"1230825","upvote_count":"5","poster":"tgv"},{"upvote_count":"1","comment_id":"1241838","timestamp":"1720069920.0","content":"Selected Answer: D\nonly D makes sense","poster":"HunkyBunky"},{"upvote_count":"1","comment_id":"1235875","timestamp":"1719157080.0","content":"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Troubleshooting_Latency.html\nA high CDCLatencySource metric indicates that the process of capturing changes from the source is delayed.\nAnswer is D","poster":"sdas1"}],"topic":"1","answer_images":[],"question_id":204,"answer_ET":"D","answer":"D","isMC":true,"timestamp":"2024-06-15 10:11:00","url":"https://www.examtopics.com/discussions/amazon/view/142571-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answers_community":["D (100%)"],"unix_timestamp":1718439060,"question_images":[]},{"id":"O9JuY09dDIEtmoCbGzly","answer_ET":"C","topic":"1","exam_id":21,"answer_images":[],"timestamp":"2024-06-15 09:31:00","choices":{"A":"Use Amazon Kinesis Data Streams and Amazon Kinesis Data Firehose to deliver the data to the S3 bucket. Use the default buffer interval for Kinesis Data Firehose.","B":"Use Amazon Kinesis Data Streams to deliver the data to the S3 bucket. Configure the stream to use 5 provisioned shards.","C":"Use Amazon Kinesis Data Streams and call the Kinesis Client Library to deliver the data to the S3 bucket. Use a 5 second buffer interval from an application.","D":"Use Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) and Amazon Kinesis Data Firehose to deliver the data to the S3 bucket. Use a 5 second buffer interval for Kinesis Data Firehose."},"url":"https://www.examtopics.com/discussions/amazon/view/142561-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer":"C","discussion":[{"comment_id":"1230839","timestamp":"1718440560.0","upvote_count":"8","poster":"tgv","content":"Selected Answer: C\nC - This option ensures low latency by using a short buffer interval (5 seconds). The use of KCL allows for customized processing logic and timely delivery of data to S3. This makes it a strong candidate for minimal latency.\n\nD - While this option provides low latency with a 5-second buffer interval, it introduces unnecessary complexity by using Apache Flink for what seems to be a straightforward data ingestion task. This option is overkill for the given use case and may add more operational overhead than necessary."},{"content":"Selected Answer: D\nKinesis Data Streams cannot deliver directly to S3. Data has to go through Firehose. A is correct but is not lowest latency. I would go with D, as we can set the buffer interval to a low value. We do not need Flink, tho. That's a bit confusing.","poster":"artworkad","timestamp":"1718447280.0","comment_id":"1230890","upvote_count":"5"},{"content":"Selected Answer: A\nWhy could not be A?\nhttps://aws.amazon.com/blogs/big-data/optimize-downstream-data-processing-with-amazon-data-firehose-and-amazon-emr-running-apache-spark/\nIt uses Data Firehose + Kinesis Data Streams","timestamp":"1732270380.0","upvote_count":"2","poster":"Eleftheriia","comment_id":"1316237"},{"comment_id":"1303498","poster":"Parandhaman_Margan","content":"Answer:D","timestamp":"1730008380.0","upvote_count":"1"},{"upvote_count":"2","poster":"andrologin","comment_id":"1254085","content":"Selected Answer: C\nUse data streams and KCL, option A would be right but the default buffer for Firehose does not allow it to be correct. D adds extra components that are not needed for delivery of data.","timestamp":"1721792520.0"},{"comment_id":"1249712","poster":"LR2023","upvote_count":"2","timestamp":"1721228520.0","comments":[{"content":"A can not correct as it's said \"Use the default buffer interval for Kinesis Data Firehose\" wich is 300 secs","upvote_count":"2","comment_id":"1260972","timestamp":"1722845940.0","poster":"teo2157"}],"content":"Selected Answer: A\nhttps://aws.amazon.com/about-aws/whats-new/2023/12/amazon-kinesis-data-firehose-zero-buffering/"},{"comment_id":"1245768","upvote_count":"1","poster":"4bc91ae","content":"its C - option D uses 1/ Analytics which summarizes data and gence has delay then passses to 2/ Firehose for deliver and Firehose doesnt say its using zero buffering","timestamp":"1720652100.0"},{"comment_id":"1236436","timestamp":"1719243720.0","upvote_count":"1","content":"Firehose uses multi-part upload for S3 destination when you configure a buffer time interval less than 60 seconds to offer lower latencies. Due to multi-part upload for S3 destination, you will see some increase in S3 PUT API costs if you choose a buffer time interval less than 60 seconds.","poster":"sdas1"},{"poster":"GHill1982","timestamp":"1718436660.0","comments":[{"poster":"tgv","timestamp":"1718534940.0","content":"Fyi, Firehose now supports 0 buffering: https://aws.amazon.com/about-aws/whats-new/2023/12/amazon-kinesis-data-firehose-zero-buffering/","upvote_count":"3","comment_id":"1231308","comments":[{"timestamp":"1719244560.0","poster":"sdas1","comment_id":"1236446","content":"As per option A, \"Use the default buffer interval for Kinesis Data Firehose\". Default buffer interval for Kinesis Data Firehose is 300seconds where S3 is the destination. \nFlink is not required here. Hence, option D is not suitable.","upvote_count":"2","comments":[{"comment_id":"1236447","poster":"sdas1","content":"https://docs.aws.amazon.com/firehose/latest/dev/buffering-hints.html","timestamp":"1719244620.0","upvote_count":"1"}]}]}],"content":"Selected Answer: C\nI think the answer is C. Kinesis Data Firehose has a minimum buffer interval of 60 seconds (1 minute) or 1 MB of data.","comment_id":"1230804","upvote_count":"3"}],"answer_description":"","question_images":[],"answers_community":["C (59%)","D (23%)","A (18%)"],"question_id":205,"question_text":"A lab uses IoT sensors to monitor humidity, temperature, and pressure for a project. The sensors send 100 KB of data every 10 seconds. A downstream process will read the data from an Amazon S3 bucket every 30 seconds.\n\nWhich solution will deliver the data to the S3 bucket with the LEAST latency?","unix_timestamp":1718436660,"isMC":true}],"exam":{"isBeta":false,"lastUpdated":"11 Apr 2025","provider":"Amazon","numberOfQuestions":207,"name":"AWS Certified Data Engineer - Associate DEA-C01","id":21,"isMCOnly":true,"isImplemented":true},"currentPage":41},"__N_SSP":true}