{"pageProps":{"questions":[{"id":"tTFTgRkcVKeBgDWAyhLi","question_id":356,"isMC":true,"question_images":[],"answer_ET":"BD","question_text":"A Data Scientist needs to analyze employment data. The dataset contains approximately 10 million observations on people across 10 different features. During the preliminary analysis, the Data Scientist notices that income and age distributions are not normal. While income levels shows a right skew as expected, with fewer individuals having a higher income, the age distribution also shows a right skew, with fewer older individuals participating in the workforce.\nWhich feature transformations can the Data Scientist apply to fix the incorrectly skewed data? (Choose two.)","exam_id":26,"answers_community":["BD (86%)","14%"],"url":"https://www.examtopics.com/discussions/amazon/view/43728-exam-aws-certified-machine-learning-specialty-topic-1/","choices":{"B":"Numerical value binning","D":"Logarithmic transformation","A":"Cross-validation","E":"One hot encoding","C":"High-degree polynomial transformation"},"answer":"BD","answer_description":"","answer_images":[],"timestamp":"2021-02-02 00:46:00","discussion":[{"comment_id":"298225","comments":[{"upvote_count":"7","comment_id":"368313","content":"Agree with B &D\nB binning for age\nD for make income in normal dist","timestamp":"1651621560.0","poster":"OmarSaadEldien"},{"comment_id":"307000","timestamp":"1650475620.0","poster":"omar_bahrain","content":"agree B&D. both are strategies to eliminate the effect of skewing","upvote_count":"6"}],"timestamp":"1650137640.0","upvote_count":"31","poster":"seanLu","content":"I would go with B,D. Refer to quantile binning and log transform below.\nhttps://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b"},{"timestamp":"1648030860.0","content":"SHOULD BE C,D","upvote_count":"10","comment_id":"281499","poster":"Joe_Zhang"},{"timestamp":"1744186140.0","upvote_count":"1","content":"Selected Answer: B\nBinning involves grouping numerical values into discrete intervals or bins. While it can simplify the representation of a feature and potentially make the distribution appear less skewed in a histogram, it doesn't fundamentally change the underlying skewness of the continuous data. It discretizes the data rather than transforming its distribution.","poster":"Togy","comment_id":"1559175"},{"content":"Selected Answer: BD\nD. Logarithmic Transformation: Addresses the right-skewed income and age distributions. The log function compresses large values, reducing the impact of outliers and making the distributions closer to normal.\n\nB. Numerical Value Binning: Useful for the age distribution. By grouping ages into bins (e.g., 20-29, 30-39, etc.), you reduce the impact of the right skew caused by fewer older individuals. While it doesn't achieve a perfectly normal distribution, it often makes the feature more interpretable and manageable for modeling.","poster":"VR10","timestamp":"1724169060.0","comment_id":"1154893","upvote_count":"1"},{"timestamp":"1713095580.0","content":"Selected Answer: BD\nB and D","upvote_count":"1","poster":"AmeeraM","comment_id":"1043392"},{"comment_id":"991947","content":"Selected Answer: BD\nAgree with B &D B binning for age D for make income in normal dist","upvote_count":"1","timestamp":"1709118360.0","poster":"Mickey321"},{"poster":"Shailendraa","upvote_count":"2","comment_id":"664939","timestamp":"1678404480.0","content":"BD is correct"},{"content":"A and E, it asks incorrectly","comment_id":"616403","poster":"[Removed]","upvote_count":"1","timestamp":"1671059820.0"},{"poster":"Sivadharan","comment_id":"602352","timestamp":"1668567120.0","upvote_count":"3","content":"Selected Answer: BD\nB & D. Reasonable explanation in below discussion."},{"timestamp":"1658545620.0","upvote_count":"1","comment_id":"530264","poster":"angnam","content":"BD\nWith age, always do quantile binning\nWith skewed data, always use log."},{"content":"B because we have skewed data with few exeptions\nD log transform can change distribution of data\nnot C - because there is no indicaiton in the text, that data is following any of the HIGH DEGREE polynomial distribution like x^ 10","poster":"Juka3lj","comment_id":"342706","timestamp":"1651334640.0","upvote_count":"5"},{"upvote_count":"4","content":"should be c and d","comment_id":"324944","poster":"Vita_Rasta84444","timestamp":"1650849420.0"},{"timestamp":"1650615420.0","poster":"achiko","upvote_count":"3","comment_id":"318522","content":"polynomial transformations can also be used for skewed data. https://machinelearningmastery.com/polynomial-features-transforms-for-machine-learning/"},{"comment_id":"287893","content":"It seems the ans are C,D\nhttps://anshikaaxena.medium.com/how-skewed-data-can-skrew-your-linear-regression-model-accuracy-and-transfromation-can-help-62c6d3fe4c53","timestamp":"1649326800.0","poster":"jiadong","upvote_count":"5"}],"unix_timestamp":1612223160,"topic":"1"},{"id":"KOoKuC0qQKSRpEAJTgKt","question_images":[],"discussion":[{"content":"I think C will be answer, because we even don't know how many layers now, so apply L1,L2 and dropouts layer will be first resort to solve overfitting. If it still does not work, then to reduce layers","poster":"knightknt","timestamp":"1681987320.0","comment_id":"588565","upvote_count":"11"},{"comments":[{"poster":"DimLam","content":"Why do you think it works only for regression problems? L1/L2 regularizations are just adding penalties to loss functions. I don't see any problems with applying it to DL model","comment_id":"1057387","timestamp":"1730269140.0","upvote_count":"1"}],"timestamp":"1725723600.0","poster":"mamun4105","comment_id":"1001700","content":"D: D is the correct answer. C could be the answer only if it is a regression problem. You cannot apply L1 (Lasso regression) and L2 (Ridge regression) to classification problems. However, you can use dropout here.","upvote_count":"1"},{"timestamp":"1724842740.0","content":"Selected Answer: C\nC Regulization","upvote_count":"2","poster":"Mickey321","comment_id":"992070"},{"timestamp":"1722624600.0","upvote_count":"1","comment_id":"970469","content":"Selected Answer: C\nif you see overfit think regularization.","poster":"kaike_reis"},{"timestamp":"1712228040.0","poster":"Khalil11","comment_id":"860911","upvote_count":"2","content":"Selected Answer: C\nC is the correct answer: The overfitting problem can be addressed by applying regularization techniques such as L1 or L2 regularization and dropouts. Regularization techniques add a penalty term to the cost function of the model, which helps to reduce the complexity of the model and prevent it from overfitting to the training data. Dropouts randomly turn off some of the neurons during training, which also helps to prevent overfitting."},{"upvote_count":"2","poster":"Valcilio","content":"Selected Answer: C\nD can work, but C is a better answer!","comment_id":"830202","timestamp":"1709664960.0"},{"content":"C and D both seems to be correct but, seems like removing layer is first step in to optimization\nhttps://www.kaggle.com/general/175912\nd","upvote_count":"2","poster":"drcok87","comment_id":"813424","timestamp":"1708283760.0"},{"comment_id":"804844","upvote_count":"1","timestamp":"1707605640.0","poster":"AjoseO","content":"Selected Answer: C\nC. Apply L1 or L2 regularization and dropouts to the training\" because regularization can help reduce overfitting by adding a penalty to the loss function for large weights, preventing the model from memorizing the training data. \n\nDropout is a regularization technique that randomly drops out neurons during the training process, further reducing the risk of overfitting."},{"comment_id":"767925","poster":"albu44","timestamp":"1704562140.0","content":"Selected Answer: D\n\"The first step when dealing with overfitting is to decrease the complexity of the model. To decrease the complexity, we can simply remove layers or reduce the number of neurons to make the network smaller.\"\nhttps://www.kdnuggets.com/2019/12/5-techniques-prevent-overfitting-neural-networks.html","upvote_count":"1"},{"timestamp":"1702149180.0","comment_id":"740374","upvote_count":"4","poster":"Peeking","comments":[{"upvote_count":"1","comment_id":"970470","content":"the problem is overfitting, not HP Tuning.","comments":[{"timestamp":"1734221340.0","comment_id":"1326638","poster":"Shakespeare","content":"Can be used for overfitting as well, but the problem does not say it is a deep learning algorithm being used so C would be more appropriate.","upvote_count":"1"}],"timestamp":"1722624660.0","poster":"kaike_reis"}],"content":"Selected Answer: D\nDeep learning tuning order:\n1. Number of layers\n2. Number of neurons (indirectly implements dropout)\n3. L1/L2 regularization\n4. Dropout"},{"poster":"Parth12","upvote_count":"3","timestamp":"1689959220.0","comment_id":"634722","content":"Selected Answer: C\nHere we are looking to reduce the Overfitting to improve the generalization. In order to do so, L1(or Lasso) regression has always been a good aide.","comments":[{"content":"This is not a regression problem at all.","poster":"mamun4105","timestamp":"1725723480.0","comment_id":"1001698","upvote_count":"1"}]},{"poster":"mtp1993","upvote_count":"3","timestamp":"1687986120.0","comment_id":"624257","content":"Selected Answer: C\nC, Regularization and dropouts should be the first attempt"},{"upvote_count":"3","comment_id":"621187","content":"Selected Answer: C\nYes, C is right here. Regularization and Dropouts","poster":"ovokpus","timestamp":"1687543020.0"},{"comment_id":"592309","upvote_count":"4","content":"Selected Answer: C\nC is the answer","poster":"Abdelrahman_Omran","timestamp":"1682505180.0"}],"answer":"C","isMC":true,"answers_community":["C (81%)","D (19%)"],"question_text":"A web-based company wants to improve its conversion rate on its landing page. Using a large historical dataset of customer visits, the company has repeatedly trained a multi-class deep learning network algorithm on Amazon SageMaker. However, there is an overfitting problem: training data shows 90% accuracy in predictions, while test data shows 70% accuracy only.\nThe company needs to boost the generalization of its model before deploying it into production to maximize conversions of visits to purchases.\nWhich action is recommended to provide the HIGHEST accuracy model for the company's test and validation data?","choices":{"A":"Increase the randomization of training data in the mini-batches used in training","D":"Reduce the number of layers and units (or neurons) from the deep learning network","B":"Allocate a higher proportion of the overall data to the training dataset","C":"Apply L1 or L2 regularization and dropouts to the training"},"question_id":357,"answer_images":[],"answer_ET":"C","timestamp":"2022-04-20 12:42:00","answer_description":"","exam_id":26,"topic":"1","unix_timestamp":1650451320,"url":"https://www.examtopics.com/discussions/amazon/view/73881-exam-aws-certified-machine-learning-specialty-topic-1/"},{"id":"Xz5GL1xa8zlKs7daLhjU","isMC":true,"discussion":[{"poster":"ac71","content":"A is correct. tSNE can do segmentation or grouping as well. Refer: https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1","comment_id":"282373","timestamp":"1663894440.0","upvote_count":"21"},{"timestamp":"1666842780.0","content":"A is definitely the correct answer. \nPay attention to what the question is asking:\n\"whether there are natural groupings for these columns across all customers and visualize the results as quickly as possible\"\n\nThe key point is to visualize the \"groupings\"(exactly what t-SNE scatter plot does, it visualize high-dimensional data points on 2D space). \nThe question does not ask to visualize how many groups you would classify (K-Means Elbow Plot does not visualize the groupings, it is used to determine the optimal # of groups=K).","poster":"SophieSu","comment_id":"298745","upvote_count":"18"},{"comment_id":"992139","content":"Selected Answer: A\noption A","poster":"Mickey321","timestamp":"1724847120.0","upvote_count":"1"},{"comments":[{"timestamp":"1727880540.0","content":"Elbow plot helps you identify the correct number of clusters during K-Means clustering. The clustering happens basis of all the features and thus group employees. This is to help your understanding. And the correct answer however is still tSNE becuase the question focuses on identifying relationships/similarities between the features / columns in the dataset. The correct answer is A","comment_id":"1023182","poster":"windy9","upvote_count":"1"}],"poster":"kaike_reis","content":"B doesn't even answer the question: how are you going to see your customer groups in an elbow plot","comment_id":"970474","timestamp":"1722624840.0","upvote_count":"1"},{"poster":"kaike_reis","upvote_count":"1","comment_id":"970473","timestamp":"1722624780.0","content":"Selected Answer: A\nEuclidean Distance suffers for high dimensional data. tSNE can suffers as well, but from my perspective is the correct one."},{"upvote_count":"2","poster":"Sylzys","content":"Selected Answer: A\nElbow plot will not help visualize groups, only try to predict an optimal number of clusters.\nI think A is a better choice here","timestamp":"1710242820.0","comment_id":"836942"},{"content":"Selected Answer: A\nA. \n\nThe t-SNE algorithm is a popular tool for visualizing high-dimensional datasets, as it can transform high-dimensional data into a 2D scatter plot, which makes it easier to visualize and understand the relationships between data points. \n\nThe scatter plot produced by t-SNE can be interpreted as a map that reveals the structure of the data, showing whether there are natural groupings or clusters within the data. \n\nOption A is the quickest and simplest way to visualize the data in a meaningful way, allowing the Specialist to gain insights into the data more efficiently.","timestamp":"1707605760.0","comment_id":"804846","poster":"AjoseO","upvote_count":"3"},{"timestamp":"1700710200.0","content":"A is correct","comment_id":"724861","poster":"minkhant19","upvote_count":"1"},{"timestamp":"1694546160.0","poster":"Shailendraa","upvote_count":"3","comment_id":"667395","content":"12-sep exam"},{"poster":"Morsa","upvote_count":"2","content":"Selected Answer: A\nA as k-means elbow is erroneous. It does not helping here. Scatter plot and t-sne is the right answer","timestamp":"1689441420.0","comment_id":"631881"},{"content":"Selected Answer: A\nAn elbow plot (B) will not give you what the question is asking for. A scatter plot will, and t-SNE is first for visualizing before dimensionality reduction.","comment_id":"624328","timestamp":"1687996800.0","upvote_count":"2","poster":"ovokpus"},{"upvote_count":"1","poster":"Sadgamaya","content":"A is correct as k means suffer from curse of dimensionality and t-she will be a better option.","timestamp":"1679967240.0","comment_id":"576528"},{"content":"Selected Answer: A\nThe B,C,D plots are meaningless wrt the problem —> A","upvote_count":"2","timestamp":"1678826880.0","poster":"Mircuz","comment_id":"567932"},{"upvote_count":"1","poster":"Mircuz","timestamp":"1678288980.0","content":"Selected Answer: B\nt-SNE suffers curse of dimensionality and is indicated for small datasets","comment_id":"563347"},{"content":"Additionally the numeric features don't require \"embedding\". I think they meant to write \"standardize\"","timestamp":"1675901040.0","comment_id":"543425","poster":"AddiWei","upvote_count":"1"},{"content":"Rooting for A","timestamp":"1675405260.0","upvote_count":"1","comment_id":"539437","poster":"apprehensive_scar"},{"poster":"bitsplease","comment_id":"528801","timestamp":"1674247560.0","content":"B & D are wrong--because data contains \"thousands of columns\" and using k-means with euclidean suffers from \"curse of dimensionality\"\n\nThus leaving A & C, you CANNOT viz clusters/groups/segments in a line graph so correct answer is A","upvote_count":"1"},{"content":"B is correct\nhttps://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b\nAccording to this For tsne it is recommended to use other dimensional reduction methods to first reduce it to a reasonable amount when number of dimesions is too high","upvote_count":"1","comment_id":"403784","timestamp":"1667242920.0","poster":"t_k_r"},{"timestamp":"1666973040.0","comments":[{"timestamp":"1667759160.0","poster":"Madwyn","upvote_count":"2","comment_id":"427035","content":"tSNE, like PCA, is unsupervised, no labels are needed. It's used for high dimension data visualisation, which is for human to see the grouping.\n\nSo A."}],"upvote_count":"1","content":"B.\ntSNE need to have grouped labels for visualization. Here looking for whether there is group or not","comment_id":"352167","poster":"orangechickencombo"},{"upvote_count":"1","content":"A is correct answer","timestamp":"1666920000.0","poster":"Juka3lj","comment_id":"342717"},{"content":"B looks like a good answer to me","poster":"cnethers","timestamp":"1666150140.0","comment_id":"284186","upvote_count":"3"},{"timestamp":"1663973400.0","content":"Based on the fact following fact :\n\nhttps://machinelearningmastery.com/clustering-algorithms-with-python/\n\nExamples of Clustering Algorithms\n- Library Installation\n- Clustering Dataset\n- Affinity Propagation\n- Agglomerative Clustering\n- BIRCH\n- DBSCAN\n- K-Means\n- Mini-Batch K-Means\n- Mean Shift\n- OPTICS\n- Spectral Clustering\n- Gaussian Mixture Model\n\n+\n\nhttps://predictivehacks.com/k-means-elbow-method-code-for-python/\nThe Elbow method is a very popular technique and the idea is to run k-means clustering for a range of clusters k (let’s say from 1 to 10) and for each value, we are calculating the sum of squared distances from each point to its assigned center(distortions).","upvote_count":"4","comment_id":"284185","poster":"cnethers"}],"exam_id":26,"question_images":[],"question_id":358,"question_text":"A Machine Learning Specialist is given a structured dataset on the shopping habits of a company's customer base. The dataset contains thousands of columns of data and hundreds of numerical columns for each customer. The Specialist wants to identify whether there are natural groupings for these columns across all customers and visualize the results as quickly as possible.\nWhat approach should the Specialist take to accomplish these tasks?","answer_ET":"A","url":"https://www.examtopics.com/discussions/amazon/view/43864-exam-aws-certified-machine-learning-specialty-topic-1/","choices":{"B":"Run k-means using the Euclidean distance measure for different values of k and create an elbow plot.","C":"Embed the numerical features using the t-distributed stochastic neighbor embedding (t-SNE) algorithm and create a line graph.","A":"Embed the numerical features using the t-distributed stochastic neighbor embedding (t-SNE) algorithm and create a scatter plot.","D":"Run k-means using the Euclidean distance measure for different values of k and create box plots for each numerical column within each cluster."},"answers_community":["A (93%)","7%"],"unix_timestamp":1612320900,"topic":"1","answer":"A","answer_description":"","answer_images":[],"timestamp":"2021-02-03 03:55:00"},{"id":"2JDGeqa9zNfhaDshiQx5","question_images":[],"answer":"A","discussion":[{"comment_id":"35227","poster":"JayK","content":"Answer is A. The answer to this question is about Pipe mode from S3. The only options are A and C. As AWS Glue cannot be use to create models which is option C.\nThe correct answer is A","timestamp":"1633745160.0","upvote_count":"31"},{"content":"Answer is A.","poster":"liangfb","upvote_count":"13","comment_id":"32898","timestamp":"1633730340.0"},{"upvote_count":"3","poster":"JonSno","content":"Selected Answer: A\nTraining locally on a small dataset ensures the training script and model parameters are working correctly.\nAmazon SageMaker training jobs allow direct access to S3 data without downloading everything.\nPipe input mode efficiently streams data from S3 to the training instance, reducing disk space requirements and speeding up training.","comment_id":"1357053","timestamp":"1739654460.0"},{"content":"Selected Answer: A\nOnly Pipe mode can stream data from S3","upvote_count":"1","comment_id":"1327520","timestamp":"1734367620.0","poster":"reginav"},{"content":"Selected Answer: A\nThe reason for this choice is that Pipe input mode is a feature of Amazon SageMaker that allows you to stream data directly from an Amazon S3 bucket to your training instances without downloading it first1. This way, you can avoid the time and space limitations of loading a large dataset onto your notebook instance. Pipe input mode also offers faster start times and better throughput than File input mode, which downloads the entire dataset before training1.","timestamp":"1727164080.0","comment_id":"973043","upvote_count":"3","poster":"Mickey321"},{"poster":"loict","content":"Selected Answer: A\nA. YES - pipe mode is best to start inference before the entire data is transferred; the only drawback is if multiple training jobs are done in sequence (eg. different hyperparamater), the data will be downloaded again\nB. NO - we want to use SageMaker first for initial training\nC. NO - We first want to test things in SageMaker\nD. NO - the SageMaker notebook will not use the AMI so the testing done is useless","comment_id":"1006312","upvote_count":"1","timestamp":"1727164080.0"},{"upvote_count":"1","comment_id":"1143919","timestamp":"1707351960.0","poster":"kyuhuck","content":"Selected Answer: B\nB. Generate daily precision-recall data in Amazon QuickSight, and publish the results in a dashboard shared with the Business team.\n\nThis solution leverages QuickSight's managed service capabilities for both data processing and visualization, which should minimize the coding effort required to provide the Business team with the necessary insights. However, it's important to note that QuickSight's ability to calculate the precision-recall data depends on its support for the necessary statistical functions or the availability of such calculations in the dataset. If QuickSight cannot perform these calculations directly, option C might be necessary, despite the increased effort."},{"upvote_count":"1","content":"Selected Answer: A\nI think it should be a","poster":"Venkatesh_Babu","comment_id":"961675","timestamp":"1690207380.0"},{"poster":"Valcilio","comment_id":"832664","content":"Selected Answer: A\nIt's A, pipe mode is for dealing with very big data.","timestamp":"1678263000.0","upvote_count":"2"},{"poster":"yemauricio","upvote_count":"2","timestamp":"1671462180.0","content":"Selected Answer: A\nA, PIPE is to do that sort of modeling","comment_id":"749951"},{"content":"When data is already in S3 and next it should move to Sagemaker.. so option A is suitable","poster":"Shailendraa","timestamp":"1662907500.0","upvote_count":"1","comment_id":"666210"},{"timestamp":"1635810900.0","comment_id":"398857","content":"Answer is A. B, C & D can be dropped because there is no integration from/to Sage Maker train job (model).","upvote_count":"1","poster":"Huy"},{"poster":"cloud_trail","comment_id":"278020","timestamp":"1635588480.0","content":"Gotta be A. You need to use Pipe mode but Glue cannot train a model.","upvote_count":"2"},{"poster":"bobdylan1","upvote_count":"1","timestamp":"1635207600.0","content":"AAAAAAAAAAa","comment_id":"255392"},{"comment_id":"173417","poster":"Willnguyen22","timestamp":"1634403300.0","content":"ans is A","upvote_count":"1"},{"upvote_count":"3","poster":"GeeBeeEl","comment_id":"150011","content":"Will you run AWS Deep Learning AMI for all cases where the data is very large in S3? Also what role is Glue playing here? Is there a transformation? These are the two issues for options B C and D. I believe they do not represent what is required to satisfy the requirements in the question. The answer definitely requires the pipe mode, but not with Glue. I go with A https://aws.amazon.com/blogs/machine-learning/using-pipe-input-mode-for-amazon-sagemaker-algorithms/","timestamp":"1634063340.0"},{"timestamp":"1633913880.0","comment_id":"98672","poster":"roytruong","content":"go for A","upvote_count":"2"},{"upvote_count":"2","timestamp":"1633913820.0","poster":"PRC","content":"Agree with A.","comment_id":"65512"},{"content":"A is correct","comment_id":"37757","upvote_count":"4","poster":"cybe001","timestamp":"1633849380.0"},{"poster":"PhilipAWS","comment_id":"26883","upvote_count":"2","timestamp":"1633214340.0","content":"Answer is D not A","comments":[{"poster":"hailiang","timestamp":"1634677560.0","comment_id":"180168","content":"oh, come on","upvote_count":"8"}]},{"upvote_count":"10","poster":"cmm103","comment_id":"26328","content":"https://aws.amazon.com/blogs/machine-learning/using-pipe-input-mode-for-amazon-sagemaker-algorithms/","timestamp":"1632284160.0"}],"isMC":true,"question_text":"A Machine Learning Specialist is developing a custom video recommendation model for an application. The dataset used to train this model is very large with millions of data points and is hosted in an Amazon S3 bucket. The Specialist wants to avoid loading all of this data onto an Amazon SageMaker notebook instance because it would take hours to move and will exceed the attached 5 GB Amazon EBS volume on the notebook instance.\nWhich approach allows the Specialist to use all the data to train the model?","answers_community":["A (93%)","7%"],"question_id":359,"choices":{"C":"Use AWS Glue to train a model using a small subset of the data to confirm that the data will be compatible with Amazon SageMaker. Initiate a SageMaker training job using the full dataset from the S3 bucket using Pipe input mode.","D":"Load a smaller subset of the data into the SageMaker notebook and train locally. Confirm that the training code is executing and the model parameters seem reasonable. Launch an Amazon EC2 instance with an AWS Deep Learning AMI and attach the S3 bucket to train the full dataset.","A":"Load a smaller subset of the data into the SageMaker notebook and train locally. Confirm that the training code is executing and the model parameters seem reasonable. Initiate a SageMaker training job using the full dataset from the S3 bucket using Pipe input mode.","B":"Launch an Amazon EC2 instance with an AWS Deep Learning AMI and attach the S3 bucket to the instance. Train on a small amount of the data to verify the training code and hyperparameters. Go back to Amazon SageMaker and train using the full dataset"},"answer_images":[],"answer_ET":"A","timestamp":"2019-12-03 19:03:00","answer_description":"","exam_id":26,"topic":"1","unix_timestamp":1575396180,"url":"https://www.examtopics.com/discussions/amazon/view/9656-exam-aws-certified-machine-learning-specialty-topic-1/"},{"id":"yZUwh2yDmkbNlTl4mvBW","answer_ET":"C","url":"https://www.examtopics.com/discussions/amazon/view/43866-exam-aws-certified-machine-learning-specialty-topic-1/","question_id":360,"discussion":[{"upvote_count":"23","poster":"[Removed]","comments":[{"timestamp":"1704806880.0","poster":"Sneep","upvote_count":"4","comment_id":"770417","content":"It's definitely C. The fact that this site indicates A is a clear sign that answers are just randomly selected, it would make zero sense to spot-instance the master node for an EMR cluster. Make sure you look at discussions for all of these questions."}],"content":"Answer is C. https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html","timestamp":"1665418200.0","comment_id":"282390"},{"poster":"SophieSu","upvote_count":"10","timestamp":"1666058220.0","content":"C is the correct answer. \n\"Long-Running Clusters and Data Warehouses\nIf you are running a persistent Amazon EMR cluster that has a predictable variation in computational capacity, such as a data warehouse, you can handle peak demand at lower cost with Spot Instances. You can launch your master and core instance groups as On-Demand Instances to handle the normal capacity and launch task instance groups as Spot Instances to handle your peak load requirements.\"","comment_id":"298747"},{"upvote_count":"1","poster":"teka112233","content":"Selected Answer: C\nAccording to :https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html\nThe task nodes process data but do not hold persistent data in HDFS. If they terminate because the Spot price has risen above your maximum Spot price, no data is lost and the effect on your cluster is minimal.\n\nWhen you launch one or more task instance groups as Spot Instances, Amazon EMR provisions as many task nodes as it can, using your maximum Spot price. This means that if you request a task instance group with six nodes, and only five Spot Instances are available at or below your maximum Spot price, Amazon EMR launches the instance group with five nodes, adding the sixth later if possible.","comment_id":"991036","timestamp":"1724704620.0"},{"timestamp":"1712228100.0","content":"Selected Answer: C\nThe correct answer is C","poster":"Khalil11","comment_id":"860912","upvote_count":"1"},{"poster":"Sylzys","content":"Selected Answer: C\nI don't get why the wrong answer are still not updated after more than 1 year of everyone showing docs proving answer C..","comments":[{"poster":"gusta_dantas","content":"1 and a half year and still wrong.. Incredible!","comment_id":"960545","upvote_count":"2","timestamp":"1721746560.0"}],"comment_id":"837077","upvote_count":"3","timestamp":"1710251880.0"},{"poster":"AjoseO","timestamp":"1707606180.0","comment_id":"804848","upvote_count":"1","content":"Selected Answer: C\nLong-running clusters and data warehouses\n\nIf you are running a persistent Amazon EMR cluster that has a predictable variation in computational capacity, such as a data warehouse, you can handle peak demand at lower cost with Spot Instances. \n\nYou can launch your primary and core instance groups as On-Demand Instances to handle the normal capacity and launch the task instance group as Spot Instances to handle your peak load requirements."},{"content":"Selected Answer: C\nOnly task nodes can be deleted without losing data.","timestamp":"1702226220.0","comment_id":"741106","upvote_count":"1","poster":"SK27"},{"poster":"Twist3d","upvote_count":"1","content":"C, If you want to cut cost on an EMR cluster in the most efficient way, use spot instances on the task nodes because it, task nodes do not store data so no risk of data loss","timestamp":"1702150740.0","comment_id":"740391"},{"poster":"ovokpus","timestamp":"1687647000.0","comment_id":"621898","upvote_count":"3","content":"Selected Answer: C\nFor Long running jobs, you do not want to compromise the Master node(sudden termination) or the core nodes (HDFS data loss).\n\nSpot Instances on 20 task nodes are enough cost savings without compromising the job.\n\nHence, C"},{"comments":[{"timestamp":"1687584660.0","comment_id":"621457","upvote_count":"1","comments":[{"comment_id":"621458","poster":"Jump09","upvote_count":"1","content":"In the question , there are no specific conditions mentioned except the concern with the COST, thus I think the answer should be A.","timestamp":"1687584720.0"}],"poster":"Jump09","content":"Adding the related reference from the AWS documentation:\nMaster node on a Spot Instance\nThe master node controls and directs the cluster. When it terminates, the cluster ends, so you should only launch the master node as a Spot Instance if you are running a cluster where sudden termination is acceptable. This might be the case if you are testing a new application, have a cluster that periodically persists data to an external store such as Amazon S3, or are running a cluster where cost is more important than ensuring the cluster's completion."}],"poster":"Jump09","upvote_count":"1","comment_id":"621455","timestamp":"1687584480.0","content":"If your primary concern is the cost, then you can run the master node on spot instances."},{"timestamp":"1666639440.0","content":"Answer: C. https://aws.amazon.com/getting-started/hands-on/optimize-amazon-emr-clusters-with-ec2-spot/\nAmazon recommends using On-Demand instances for Master and Core nodes unless you are launching highly ephemeral workloads.","poster":"benson2021","comment_id":"336092","upvote_count":"5"},{"poster":"xpada001","timestamp":"1666233780.0","comment_id":"311502","upvote_count":"3","content":"Answer should be C."},{"poster":"ac71","comment_id":"282375","comments":[{"poster":"MahEid","timestamp":"1667481960.0","comment_id":"450114","upvote_count":"1","content":"Answer is C \nyou should only run core nodes on Spot Instances /*when partial HDFS data loss is tolerable*/\nQuestion is what \"Should\" be launched as spot instance"}],"upvote_count":"3","content":"Only master node is incorrect. Either use all on spot or only task or core on spot. As per: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html\n\nBetter to use only task node on spot for long running tasks/jobs","timestamp":"1665206820.0"}],"topic":"1","unix_timestamp":1612321260,"question_text":"A Machine Learning Specialist is planning to create a long-running Amazon EMR cluster. The EMR cluster will have 1 master node, 10 core nodes, and 20 task nodes. To save on costs, the Specialist will use Spot Instances in the EMR cluster.\nWhich nodes should the Specialist launch on Spot Instances?","exam_id":26,"answer_description":"","isMC":true,"answers_community":["C (100%)"],"answer":"C","timestamp":"2021-02-03 04:01:00","choices":{"C":"Any of the task nodes","A":"Master node","D":"Both core and task nodes","B":"Any of the core nodes"},"question_images":[],"answer_images":[]}],"exam":{"isBeta":false,"numberOfQuestions":369,"provider":"Amazon","isMCOnly":false,"lastUpdated":"11 Apr 2025","id":26,"name":"AWS Certified Machine Learning - Specialty","isImplemented":true},"currentPage":72},"__N_SSP":true}