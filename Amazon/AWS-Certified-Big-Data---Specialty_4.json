{"pageProps":{"questions":[{"id":"Vme2L4JPrU4SeDE6sb5H","answer_description":"","answer_ET":"A","answer":"A","isMC":true,"answers_community":[],"question_text":"A company is using Amazon Machine Learning as part of a medical software application. The application will predict the most likely blood type for a patient based on a variety of other clinical tests that are available when blood type knowledge is unavailable.\nWhat is the appropriate model choice and target attribute combination for this problem?","topic":"1","question_images":[],"discussion":[{"timestamp":"1634564760.0","comments":[{"timestamp":"1635709200.0","upvote_count":"1","poster":"freedomeox","comment_id":"105555","content":"in my opinion, the \"multi class target attribute\" in D means the prediction result can be multi-class, eg, the blood type can be B and O, which didn't make sense here. That's why I rule out D. KNN can be used to do multi-class classification, thus A is a general but still correct answer."},{"poster":"agm84","upvote_count":"6","content":"Amazon Machine Learning does not have KNN. KNN is available in sagemaker. Its A","comment_id":"98387","timestamp":"1634953200.0"}],"comment_id":"79532","content":"Answer is D. The target label is unknown from other clinical trial data. KNN is the best model to predict that multi-classfication targert label based on other clinical trial data.","upvote_count":"1","poster":"Bulti"},{"timestamp":"1634347920.0","content":"my selection A","comment_id":"52336","upvote_count":"2","poster":"san2020"},{"upvote_count":"2","timestamp":"1633078620.0","content":"KNN is simplest ML, blood type classification is serious. so the answer should be A.","comment_id":"13509","poster":"pkfe"},{"poster":"exams","upvote_count":"2","timestamp":"1632694740.0","content":"A - Multi-class looks appropriate","comment_id":"11523"}],"answer_images":[],"choices":{"D":"K-Nearest Neighbors model with a multi-class target attribute.","B":"Regression model with a numeric target attribute.","A":"Multi-class classification model with a categorical target attribute.","C":"Binary Classification with a categorical target attribute."},"unix_timestamp":1568779500,"timestamp":"2019-09-18 06:05:00","url":"https://www.examtopics.com/discussions/amazon/view/5345-exam-aws-certified-big-data-specialty-topic-1-question-23/","exam_id":17,"question_id":16},{"id":"ujPenxPnVvqnS3wThh4w","timestamp":"2019-08-13 04:46:00","answer_description":"","answers_community":[],"isMC":true,"exam_id":17,"question_id":17,"discussion":[{"timestamp":"1635734460.0","poster":"Bulti","content":"Answer : B\nNot A- query filters on customer ID and hence the distribution key will not segregate the workload between small and large customers.\nNot C- RDS is not a good choice for aggregation.\nNot D – You could route the largest customers to a dedicated RedShift cluster. But I am not sure why I would raise the concurrency of the multi-tenant cluster to accommodate the remaining customers when these customers are already on the cluster and consists of 20% of the volume. For that reason I wouldn’t chose D.\nB is the correct answer- Depending upon the company the logged in user belongs to, you can assign the query to a specific query group. Each of the large customer can be assigned to one query group each and all small customers can be assigned to another query group. This allow the large customers to get their own query queue with a dedicated share of the 25 node hardware to run their queries while smaller customers share a single queue and therefore a portion of the 25 node hardware.","comment_id":"76259","upvote_count":"8"},{"timestamp":"1635703200.0","poster":"YashBindlish","comment_id":"74502","upvote_count":"1","content":"Correct Answer is B"},{"comment_id":"64965","content":"Redshift enforces a query concurrency limit of 15 on a cluster.Queries in are executed in a queue, by default there is one queue per query cluster which can run up to five concurrent queries. Users can modify the configuration to allow up to 15 queries per queue and a maximum of 8 queues.The concurrent queries for a cluster across queues is limited to a maximum of 15. Users cannot modify this configuration.\n\nEven after you configure WLM queues , and no matter how big you cluster is, concurrent queries that can be run are limited to 15.\n\nD is the best sofar.","upvote_count":"2","comments":[{"timestamp":"1636226160.0","content":"If you create new queue, then you can increase concurrency level while default queue can have only 5 concurrency queries. Across the queue, you can have 50 concurrent queue. That's why the answer is B.","upvote_count":"1","poster":"skytango","comment_id":"135425"}],"timestamp":"1635667740.0","poster":"Averageguy"},{"upvote_count":"1","content":"my selection B","poster":"san2020","comment_id":"52337","timestamp":"1635548340.0"},{"poster":"yuriy_ber","content":"I'd say D - not sure but I think WLM approach wouldn't work here in multi-tenant architecture since our users are not IAM users and can not be used in queues. Futhermore it's not about long-running and short-running queues but about \"big\" and \"small\" cutomers.","timestamp":"1635454200.0","upvote_count":"1","comment_id":"30353"},{"timestamp":"1635306600.0","content":"B seems correct. https://docs.aws.amazon.com/redshift/latest/dg/c_workload_mngmt_classification.html","comment_id":"24837","upvote_count":"1","poster":"s3an"},{"poster":"cybe001","timestamp":"1634912400.0","comment_id":"19231","content":"B is correct. The question asks for a solution in which large customer workload \"do NOT interfere\" with smaller customer. It doesn't tell multi-tenant separation. Performance is important in serving the large customers.","upvote_count":"1"},{"upvote_count":"1","comment_id":"18254","timestamp":"1634714340.0","poster":"asadao","content":"D is correct"},{"comment_id":"8639","timestamp":"1633514340.0","content":"the answer is b. to manage workload at redshift to create queu for different query type.","upvote_count":"2","poster":"muhsin","comments":[{"comments":[{"poster":"mattyb123","timestamp":"1633612080.0","content":"Read from pg 27. https://d1.awsstatic.com/whitepapers/Multi_Tenant_SaaS_Storage_Strategies.pdf\nFrom my understanding of the article it is best for SaaS providers to use the silo approach. Meaning Answer D is correct. Anyone disagree?","upvote_count":"2","comment_id":"8922"}],"upvote_count":"1","poster":"mattyb123","content":"Thanks for the explanation this now makes more sense than creating an additional cluster.","comment_id":"8717","timestamp":"1633570200.0"},{"timestamp":"1634312220.0","comment_id":"9530","content":"@muhsin why b over d?","upvote_count":"1","comments":[{"timestamp":"1634320440.0","poster":"mattyb123","comment_id":"9826","upvote_count":"1","content":"I selected B in the exam. When i reread the question, it states SaaS service. Also the WLM use case works here as mention by @muhsin as we are trying to separate long and short queries.","comments":[{"upvote_count":"2","comment_id":"11525","content":"I agree with B","poster":"exams","timestamp":"1634374440.0"}]}],"poster":"mattyb123"}]},{"timestamp":"1632226500.0","content":"Thoughts on B?","poster":"mattyb123","comment_id":"6650","comments":[{"poster":"muhsin","upvote_count":"2","timestamp":"1633286340.0","comment_id":"7961","content":"the problem is that they don't share the same databases. If they have same DB, then we need to create user group and query group to prevent long run queries from consuming all resources."},{"content":"larger customer workloads do NOT interfere with the smaller customer workloads . B still share the workload","comment_id":"6926","poster":"Jialu","comments":[{"timestamp":"1633038600.0","comment_id":"7029","upvote_count":"1","poster":"mattyb123","content":"thanks"}],"timestamp":"1632803460.0","upvote_count":"2"}],"upvote_count":"1"}],"question_text":"A data engineer is running a DWH on a 25-node Redshift cluster of a SaaS service. The data engineer needs to build a dashboard that will be used by customers. Five big customers represent 80% of usage, and there is a long tail of dozens of smaller customers. The data engineer has selected the dashboarding tool.\nHow should the data engineer make sure that the larger customer workloads do NOT interfere with the smaller customer workloads?","answer_images":[],"answer_ET":"D","topic":"1","question_images":[],"unix_timestamp":1565664360,"answer":"D","url":"https://www.examtopics.com/discussions/amazon/view/3519-exam-aws-certified-big-data-specialty-topic-1-question-24/","choices":{"B":"Place the largest customers into a single user group with a dedicated query queue and place the rest of the customers into a different query queue.","C":"Push aggregations into an RDS for Aurora instance. Connect the dashboard application to Aurora rather than Redshift for faster queries.","A":"Apply query filters based on customer-id that can NOT be changed by the user and apply distribution keys on customer-id.","D":"Route the largest customers to a dedicated Redshift cluster. Raise the concurrency of the multi-tenant Redshift cluster to accommodate the remaining customers."}},{"id":"kRCohyHOfa3Fs9vWwHDF","url":"https://www.examtopics.com/discussions/amazon/view/3520-exam-aws-certified-big-data-specialty-topic-1-question-25/","timestamp":"2019-08-13 04:53:00","question_id":18,"unix_timestamp":1565664780,"exam_id":17,"answer_ET":"A","isMC":true,"question_text":"An Amazon Kinesis stream needs to be encrypted.\nWhich approach should be used to accomplish this task?","answer_images":[],"discussion":[{"upvote_count":"2","content":"A is correct","timestamp":"1636059060.0","poster":"Mikilo","comment_id":"95573"},{"comment_id":"52339","timestamp":"1635911100.0","poster":"san2020","content":"my selection A","upvote_count":"4"},{"content":"Agree with A\n https://docs.aws.amazon.com/firehose/latest/dev/encryption.html","upvote_count":"2","timestamp":"1634724900.0","comment_id":"42648","poster":"kalpanareddy"},{"comment_id":"11526","timestamp":"1634553240.0","poster":"exams","content":"Agree with A","upvote_count":"2"},{"content":"A is correct answer","poster":"pra276","comment_id":"7527","upvote_count":"2","timestamp":"1634369040.0"},{"timestamp":"1633881000.0","poster":"Jialu","upvote_count":"2","comment_id":"7391","content":"A is correct answer"},{"comment_id":"6651","poster":"mattyb123","timestamp":"1633838640.0","content":"https://aws.amazon.com/blogs/big-data/encrypt-and-decrypt-amazon-kinesis-records-using-aws-kms/","upvote_count":"2"}],"answer":"A","topic":"1","choices":{"B":"Use a partition key to segment the data by MD5 hash function, which makes it undecipherable while in transit.","A":"Perform a client-side encryption of the data before it enters the Amazon Kinesis stream on the producer.","D":"Use a shard to segment the data, which has built-in functionality to make it indecipherable while in transit.","C":"Perform a client-side encryption of the data before it enters the Amazon Kinesis stream on the consumer."},"answer_description":"Reference: https://docs.aws.amazon.com/firehose/latest/dev/encryption.html","question_images":[],"answers_community":[]},{"id":"khKKgmbmiUIy4P9vZmWK","exam_id":17,"answer":"C","question_text":"An online photo album app has a key design feature to support multiple screens (e.g, desktop, mobile phone, and tablet) with high-quality displays. Multiple versions of the image must be saved in different resolutions and layouts.\nThe image-processing Java program takes an average of five seconds per upload, depending on the image size and format. Each image upload captures the following image metadata: user, album, photo label, upload timestamp.\nThe app should support the following requirements:\n✑ Hundreds of user image uploads per second\n✑ Maximum image upload size of 10 MB\n✑ Maximum image metadata size of 1 KB\n✑ Image displayed in optimized resolution in all supported screens no later than one minute after image upload\nWhich strategy should be used to meet these requirements?","answer_images":[],"answer_ET":"C","unix_timestamp":1567954560,"question_id":19,"isMC":true,"answer_description":"","timestamp":"2019-09-08 16:56:00","choices":{"C":"Upload image with metadata to Amazon S3, use Lambda function to run the image processing and save the images output to Amazon S3 and metadata to the app repository DB.","B":"Write image and metadata RDS with BLOB data type. Use AWS Data Pipeline to run the image processing and save the image output to Amazon S3 and metadata to the app repository DB.","D":"Write image and metadata to Amazon Kinesis. Use Amazon Elastic MapReduce (EMR) with Spark Streaming to run image processing and save the images output to Amazon S3 and metadata to app repository DB.","A":"Write images and metadata to Amazon Kinesis. Use a Kinesis Client Library (KCL) application to run the image processing and save the image output to Amazon S3 and metadata to the app repository DB."},"topic":"1","answers_community":[],"url":"https://www.examtopics.com/discussions/amazon/view/4906-exam-aws-certified-big-data-specialty-topic-1-question-26/","question_images":[],"discussion":[{"comment_id":"76255","poster":"Bulti","content":"Answer : C – record size limitation of 1MB in Kinesis takes that option A out. Moreover using Lambda to do image processing on S3 file upload event and writing those formats back into S3 bucket and the metadata into a repository DB like Dynamo is a standard practice.","timestamp":"1634888520.0","upvote_count":"6"},{"comment_id":"27854","timestamp":"1632865080.0","upvote_count":"5","content":"@antoneti -- Why not Kinesis -- Kinesis is for real-time streaming. You need to deal with Shards. Single shard has 1Mb limit. This question is about image upload and transformation. Maximum image size is 10Gb. It is easier to deal with S3 for image upload, lambda for transformation and put the metadata into something like DynamoDB. So, I think the answer is C.","poster":"rusu"},{"comment_id":"52340","poster":"san2020","upvote_count":"2","content":"my selection C","timestamp":"1634780940.0"},{"content":"Answer is B,\nUploading images to S3 then use Lambda to resize is a popular solution.\nhttps://aws.amazon.com/jp/blogs/compute/resize-images-on-the-fly-with-amazon-s3-aws-lambda-and-amazon-api-gateway/","timestamp":"1633681200.0","comments":[{"timestamp":"1634624820.0","content":"Well so you mean to say Answer is \"C\"(I feel the same) as \"B\" does not seem to resonate with what you mentioned above .?","upvote_count":"3","poster":"JonSno","comment_id":"51842"}],"comment_id":"46429","upvote_count":"1","poster":"AdamSmith"},{"timestamp":"1632993240.0","content":"Kinesis is out because maximum record size of 1 MB but Lambda has also limitation of payload (6 MB) and big payloads on lambda can cause timeout problems. Any ideas? Split images before sending them to Lambda?","upvote_count":"2","comment_id":"28273","poster":"yuriy_ber","comments":[{"content":"the payload limit is just on the invocation. Since you already uploaded the file on s3, you just need to tell lambda where to find it. No need to send it again to lambda. C is correct.","upvote_count":"1","timestamp":"1636066980.0","poster":"MichRox","comment_id":"111436"}]},{"poster":"antoneti","upvote_count":"1","timestamp":"1632767880.0","comment_id":"25119","content":"and why not used Kinesis?"},{"upvote_count":"1","comment_id":"11527","timestamp":"1632497640.0","poster":"exams","content":"c it is"},{"timestamp":"1632129480.0","content":"https://aws.amazon.com/blogs/big-data/building-and-maintaining-an-amazon-s3-metadata-index-without-servers/","poster":"Hitu","comment_id":"10196","upvote_count":"1"}]},{"id":"RV1WpJUJCoEDmjbfxtpE","question_images":[],"exam_id":17,"unix_timestamp":1567955820,"answer":"D","answer_ET":"D","isMC":true,"choices":{"B":"Identify the largest dimension table and designate the key of this dimension table as the distribution key of the ORDERS table.","C":"Identify the smallest dimension table and designate the key of this dimension table as the distribution key of the ORDERS table.","A":"Identify the largest and most frequently joined dimension table and ensure that it and the ORDERS table both have EVEN distribution.","D":"Identify the largest and the most frequently joined dimension table and designate the key of this dimension table as the distribution key of the ORDERS table."},"timestamp":"2019-09-08 17:17:00","question_id":20,"answers_community":[],"url":"https://www.examtopics.com/discussions/amazon/view/4907-exam-aws-certified-big-data-specialty-topic-1-question-27/","discussion":[{"timestamp":"1635682920.0","poster":"san2020","content":"my selection D","upvote_count":"5","comment_id":"52341"},{"comment_id":"40596","content":"I'm sure D is a right choice","poster":"practicioner","timestamp":"1634747460.0","upvote_count":"1"},{"poster":"VB","comment_id":"11902","timestamp":"1634574180.0","content":"it is D","upvote_count":"2"},{"comments":[{"poster":"exams","content":"I think D","upvote_count":"1","comment_id":"11528","timestamp":"1634250420.0"}],"upvote_count":"3","timestamp":"1633145340.0","comment_id":"10203","content":"A or D?\n\nIn a typical star schema, the fact table has foreign key relationships with multiple dimension tables, so you need to choose one of the dimensions. You would choose the foreign key for the largest frequently joined dimension as a distribution key in the fact table and the primary key in the dimension table. Make sure that the distribution keys chosen result in relatively even distribution for both tables, and if the distribution is skewed, use a different dimension. Then analyze the remaining dimensions to determine if a distribution style of ALL, KEY, or EVEN is appropriate.","poster":"Hitu"},{"comment_id":"10199","poster":"Hitu","timestamp":"1632751320.0","upvote_count":"2","content":"https://aws.amazon.com/blogs/big-data/optimizing-for-star-schemas-and-interleaved-sorting-on-amazon-redshift/"}],"question_text":"A customer needs to determine the optimal distribution strategy for the ORDERS fact table in its Redshift schema. The ORDERS table has foreign key relationships with multiple dimension tables in this schema.\nHow should the company determine the most appropriate distribution key for the ORDERS table?","answer_images":[],"answer_description":"Reference:\nhttps://aws.amazon.com/blogs/big-data/optimizing-for-star-schemas-and-interleaved-sorting-on- amazon-redshift/","topic":"1"}],"exam":{"name":"AWS Certified Big Data - Specialty","numberOfQuestions":85,"id":17,"isImplemented":true,"provider":"Amazon","lastUpdated":"11 Apr 2025","isMCOnly":true,"isBeta":false},"currentPage":4},"__N_SSP":true}