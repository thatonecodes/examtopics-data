{"pageProps":{"questions":[{"id":"RUGYggXY12iOYBwu7v4U","answer_ET":"B","answers_community":["B (96%)","4%"],"choices":{"C":"Custom entity recognition","D":"Built-in models","B":"Custom classification with multi-label mode","A":"Custom classification with multi-class mode"},"question_id":96,"url":"https://www.examtopics.com/discussions/amazon/view/74925-exam-aws-certified-machine-learning-specialty-topic-1/","isMC":true,"answer":"B","exam_id":26,"answer_description":"","question_images":[],"discussion":[{"content":"Selected Answer: B\nThe answer is B. In multi-label mode, individual classes represent different categories, but these categories are not mutually exclusive while individual classes are mutually exclusive in multi-class mode","upvote_count":"13","timestamp":"1682793780.0","poster":"spaceexplorer","comment_id":"594648"},{"comment_id":"596441","timestamp":"1683139380.0","poster":"Ivandrago","upvote_count":"10","content":"B - In simple language, it is tagging"},{"content":"Selected Answer: C\nA. NO - multi-class means more than binomial/2 classes possible targets, but still the document belongs to only 1\nB. YES - multiple class can be assigned (eg. using SoftMax for different probabilities)\nC. NO - it is about assigning Entities to terms in the input documents, not classifying the documents\nD. NO - you need to customize the classes","poster":"loict","upvote_count":"1","timestamp":"1725778320.0","comment_id":"1002212"},{"content":"Selected Answer: B\nAnswer b","comment_id":"989056","poster":"Mickey321","upvote_count":"1","timestamp":"1724493540.0"},{"poster":"Chelseajcole","upvote_count":"5","comment_id":"825255","timestamp":"1709153160.0","content":"Selected Answer: B\nIn multi-label classification, individual classes represent different categories, but these categories are somehow related and are not mutually exclusive. As a result, each document has at least one class assigned to it, but can have more. For example, a movie can simply be an action movie, or it can be an action movie, a science fiction movie, and a comedy, all at the same time.\n\nIn multi-class classification, each document can have one and only one class assigned to it. The individual classes are mutually exclusive. For example, a movie can be classed as a documentary or as science fiction, but not both at the same time."},{"poster":"Peeking","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/comprehend/latest/dg/prep-classifier-data-multi-label.html","timestamp":"1702256460.0","upvote_count":"1","comment_id":"741339"},{"timestamp":"1694545080.0","comment_id":"667382","upvote_count":"2","poster":"Shailendraa","content":"12-sep exam"},{"poster":"matteocal","timestamp":"1690443060.0","comment_id":"637898","content":"Selected Answer: B\nB. Multi-label:\nhttps://docs.aws.amazon.com/comprehend/latest/dg/prep-classifier-data-multi-label.html","upvote_count":"2"},{"timestamp":"1689338040.0","poster":"Morsa","upvote_count":"1","content":"Selected Answer: B\nWritten in the technical guide related but not mutually exclusive","comment_id":"631359"}],"answer_images":[],"timestamp":"2022-04-29 20:43:00","unix_timestamp":1651257780,"question_text":"A retail company collects customer comments about its products from social media, the company website, and customer call logs. A team of data scientists and engineers wants to find common topics and determine which products the customers are referring to in their comments. The team is using natural language processing (NLP) to build a model to help with this classification.\nEach product can be classified into multiple categories that the company defines. These categories are related but are not mutually exclusive. For example, if there is mention of \"Sample Yogurt\" in the document of customer comments, then \"Sample Yogurt\" should be classified as \"yogurt,\" \"snack,\" and \"dairy product.\"\nThe team is using Amazon Comprehend to train the model and must complete the project as soon as possible.\nWhich functionality of Amazon Comprehend should the team use to meet these requirements?","topic":"1"},{"id":"5GLQR5QeBXDAkek8RKBs","question_text":"A data engineer is using AWS Glue to create optimized, secure datasets in Amazon S3. The data science team wants the ability to access the ETL scripts directly from Amazon SageMaker notebooks within a VPC. After this setup is complete, the data science team wants the ability to run the AWS Glue job and invoke the\nSageMaker training job.\nWhich combination of steps should the data engineer take to meet these requirements? (Choose three.)","question_images":[],"answers_community":["BCF (70%)","BDF (20%)","10%"],"url":"https://www.examtopics.com/discussions/amazon/view/74999-exam-aws-certified-machine-learning-specialty-topic-1/","choices":{"C":"Create SageMaker notebooks by using the AWS Glue development endpoint.","F":"Create an IAM policy and an IAM role for the SageMaker notebooks.","A":"Create a SageMaker development endpoint in the data science team's VPC.","E":"Attach a decryption policy to the SageMaker notebooks.","D":"Create SageMaker notebooks by using the SageMaker console.","B":"Create an AWS Glue development endpoint in the data science team's VPC."},"answer":"BCF","question_id":97,"unix_timestamp":1651352760,"answer_images":[],"answer_description":"","isMC":true,"topic":"1","discussion":[{"comment_id":"595291","poster":"spaceexplorer","timestamp":"1651352760.0","content":"Selected Answer: BCF\nBCF; This tutorial doc says so: https://docs.aws.amazon.com/glue/latest/dg/dev-endpoint-tutorial-sage.html","upvote_count":"12"},{"upvote_count":"5","poster":"tgaos","comment_id":"613125","timestamp":"1654672320.0","content":"Selected Answer: BCF\nBCF is correct"},{"comments":[{"comment_id":"1331723","poster":"587df71","upvote_count":"1","content":"https://docs.aws.amazon.com/glue/latest/dg/dev-endpoint-tutorial-sage.html\nit says ...\n\"Select the check box next to the name of a development endpoint that you want to use, and on the Action menu, choose Create SageMaker notebook\"","timestamp":"1735166820.0"}],"timestamp":"1727717280.0","content":"Selected Answer: BDF\n\"C\" is definitely wrong SageMaker cannot be created by using the AWS Glue development endpoint.\nExplanation: This option is not correct. SageMaker notebooks are created using the SageMaker console or APIs, not through AWS Glue development endpoints.","comment_id":"1291631","poster":"MultiCloudIronMan","upvote_count":"2"},{"timestamp":"1725995880.0","poster":"GS_77","upvote_count":"1","content":"Selected Answer: BDF\nBDF is correct","comment_id":"1281729"},{"content":"Selected Answer: BDF\nwhen creating notebook why do we need Glue development endpoint? it should be D","upvote_count":"3","comment_id":"1194351","poster":"vkbajoria","timestamp":"1712927460.0"},{"poster":"loict","upvote_count":"3","timestamp":"1694156640.0","comments":[{"content":"Sorry, B is true. Scripts must be accessible they say. Then A is wrong ?","comment_id":"1002219","timestamp":"1694156700.0","upvote_count":"2","poster":"loict"}],"content":"Selected Answer: ADF\nA. YES - By requirement, the notebook must be in a VPC\nB. NO - Data is already in S3, we do not need to know it was made with AWS Glue\nC. NO - Data is already in S3 thanks to AWS Glue, no runtime relationship with SageMaker\nD. YES - need to create the Notebooks at some point\nE. NO - no need to decrypt, it is about ACL\nF. YES - notebooks need to be able to read S3","comment_id":"1002217"},{"upvote_count":"1","poster":"Mickey321","timestamp":"1692873540.0","content":"Selected Answer: BCF\nCreating a SageMaker development endpoint in the data science team's VPC will allow the data science team to access the ETL scripts and the AWS Glue job from within their VPC.\nCreating an IAM policy and an IAM role for the SageMaker notebooks will allow the data science team to access the ETL scripts and the AWS Glue job with the appropriate permissions.\nCreating SageMaker notebooks by using the SageMaker console will allow the data science team to easily create and manage the SageMaker notebooks.","comment_id":"989096"},{"upvote_count":"3","timestamp":"1677617400.0","poster":"Chelseajcole","comment_id":"825262","content":"Selected Answer: BCF\nIn the AWS Glue console, choose Dev endpoints to navigate to the development endpoints list.\nSelect the check box next to the name of a development endpoint that you want to use, and on the Action menu, choose Create SageMaker notebook.\nFill out the Create and configure a notebook page as follows:\nEnter a notebook name.\nUnder Attach to development endpoint, verify the development endpoint.\nCreate or choose an AWS Identity and Access Management (IAM) role."}],"timestamp":"2022-04-30 23:06:00","answer_ET":"BCF","exam_id":26},{"id":"lQfliQRsvLSpiDuozdP5","answer":"C","question_text":"A data engineer needs to provide a team of data scientists with the appropriate dataset to run machine learning training jobs. The data will be stored in Amazon S3. The data engineer is obtaining the data from an Amazon Redshift database and is using join queries to extract a single tabular dataset. A portion of the schema is as follows:\n\nTransactionTimestamp (Timestamp)\nCardName (Varchar)\nCardNo (Varchar)\n\nThe data engineer must provide the data so that any row with a CardNo value of NULL is removed. Also, the TransactionTimestamp column must be separated into a TransactionDate column and a TransactionTime column. Finally, the CardName column must be renamed to NameOnCard.\n\nThe data will be extracted on a monthly basis and will be loaded into an S3 bucket. The solution must minimize the effort that is needed to set up infrastructure for the ingestion and transformation. The solution also must be automated and must minimize the load on the Amazon Redshift cluster.\n\nWhich solution meets these requirements?","timestamp":"2022-11-28 03:05:00","question_id":98,"url":"https://www.examtopics.com/discussions/amazon/view/89019-exam-aws-certified-machine-learning-specialty-topic-1/","answer_description":"","exam_id":26,"discussion":[{"comment_id":"728732","timestamp":"1701137100.0","content":"Selected Answer: C\nagreed with C","upvote_count":"12","poster":"ystotest"},{"upvote_count":"1","poster":"2eb8df0","content":"Selected Answer: C\nIts always Glue","timestamp":"1741701240.0","comment_id":"1387449"},{"poster":"akgarg00","comment_id":"1081234","content":"Selected Answer: C\nThe answer was between C and D, but we are suppose to minimize use of Redshift cluster, answer is C. And B are too much effort, so not to be done as per constraints of question.","timestamp":"1732686660.0","upvote_count":"2"},{"comment_id":"1006050","poster":"teka112233","timestamp":"1726177440.0","content":"Selected Answer: C\nSimply the requirements are a full ETL process where data will be extracted from Redshift (E), then transformed by renaming, removing null values, or even separating the first column So (T), and finally load data to S3(L)\nall that with the least overhead, which make the AWS Glue ideal for these requirements","upvote_count":"1"},{"upvote_count":"1","poster":"loict","comment_id":"1002228","content":"Selected Answer: C\nA. NO - AWS Glue (serverless) is a simpler option than EMR to run Spark jobs\nB. NO - Spark is a better option for datapipelines, it avoids the need for intermediary files\nC. YES - Spark and AWS Glue best combination\nD. NO - Amazon Redshift Spectrum is a \"Lake House\" architecture, meant to run SQL against against both DW & S3; here, we want to query only from the DW","timestamp":"1725779460.0"},{"poster":"Mickey321","content":"Selected Answer: C\nThe reason is that this solution can leverage the existing capabilities of AWS Glue, which is a fully managed service that can help users create, run, and manage ETL (extract, transform, and load) workflows. According to the web search results, AWS Glue can connect to various data sources and destinations, such as Amazon Redshift and Amazon S3, and use Apache Spark as the underlying processing engine. AWS Glue can also provide various built-in transforms that can perform common data manipulation operations, such as filtering, mapping, renaming, or joining. Moreover, AWS Glue can support scheduling and automation of ETL jobs using triggers or workflows.","comment_id":"989120","upvote_count":"1","timestamp":"1724498580.0"},{"poster":"Mickey321","comment_id":"989117","upvote_count":"1","timestamp":"1724498040.0","content":"Selected Answer: C\nagree with C"},{"timestamp":"1723385280.0","content":"Selected Answer: C\nC\nReason: we want to minimize infrastructure effort, so we should prioritize serverless solutions, we want something automated and minimize the load on the Redshift cluster. That said, Letter A is wrong as it uses a managed service (EMR) just like Letter B (EC2). Letter D brings Redshift Spectrum, however the base is not in S3, but Redshift! So, it's discarded this option, since we use this service to move data from S3 → Redshift using SQL. Letter C is correct.","comment_id":"978693","poster":"kaike_reis","upvote_count":"1"},{"upvote_count":"2","poster":"Jerry84","timestamp":"1705477320.0","comment_id":"778666","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-python-transforms.html"}],"question_images":[],"answer_ET":"C","answer_images":[],"choices":{"C":"Set up an AWS Glue job that has the Amazon Redshift cluster as the source and the S3 bucket as the destination. Use the built-in transforms Filter, Map, and RenameField to perform the required transformations. Schedule the job to run monthly.","D":"Use Amazon Redshift Spectrum to run a query that writes the data directly to the S3 bucket. Create an AWS Lambda function to run the query monthly.","B":"Set up an Amazon EC2 instance with a SQL client tool, such as SQL Workbench/J, to query the data from the Amazon Redshift cluster directly Export the resulting dataset into a file. Upload the file into the S3 bucket. Perform these tasks monthly.","A":"Set up an Amazon EMR cluster. Create an Apache Spark job to read the data from the Amazon Redshift cluster and transform the data. Load the data into the S3 bucket. Schedule the job to run monthly."},"answers_community":["C (100%)"],"unix_timestamp":1669601100,"isMC":true,"topic":"1"},{"id":"X5e8NalrFGOHklFBA0Jg","question_images":[],"answer_images":[],"choices":{"D":"Use a COPY instruction in the Dockerfile to copy the training program to the /opt/ml/train directory.","A":"Specify the server argument in the ENTRYPOINT instruction in the Dockerfile.","B":"Specify the training program in the ENTRYPOINT instruction in the Dockerfile.","C":"Include the path to the training data in the docker build command when packaging the container."},"answers_community":["B (92%)","8%"],"question_text":"A machine learning (ML) specialist wants to bring a custom training algorithm to Amazon SageMaker. The ML specialist implements the algorithm in a Docker container that is supported by SageMaker.\n\nHow should the ML specialist package the Docker container so that SageMaker can launch the training correctly?","answer_description":"","question_id":99,"topic":"1","unix_timestamp":1669534740,"discussion":[{"comment_id":"728008","poster":"VinceCar","timestamp":"1685165940.0","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-dockerfile.html","upvote_count":"5"},{"comment_id":"1160180","timestamp":"1724714040.0","poster":"AIWave","upvote_count":"1","content":"Selected Answer: D\nThe /opt/ml directory is the default directory where SageMaker expects the training script and other related files to be located. The script at location above is triggered by setting environment variable SAGEMAKER_PROGRAM and *not* through an ENTRYPOINT in docker file"},{"comment_id":"1002235","timestamp":"1709889480.0","upvote_count":"1","content":"Selected Answer: B\nA. NO - There is no server here, we do training not inference\nB. YES\nC. NO - path to training data is externally provided, not hardcoded in the image\nD. NO - /opt/ml/train is the working directory of the ENTRYPOINT","poster":"loict"},{"upvote_count":"2","timestamp":"1708781760.0","content":"Selected Answer: B\nAmazon SageMaker supports bringing custom training algorithms by using Docker containers, which are software packages that can contain all the dependencies and configurations needed to run an application. Dockerfile is a text file that contains the instructions for building a Docker image, which is a snapshot of a Docker container. ENTRYPOINT is an instruction in the Dockerfile that specifies the default executable or command that will run when the container is started. By specifying the training program in the ENTRYPOINT instruction, the ML specialist can ensure that Amazon SageMaker can run the training program automatically when it creates and runs a Docker container for the training job.","comment_id":"989132","poster":"Mickey321"},{"upvote_count":"4","comment_id":"745273","poster":"wjohnny","timestamp":"1686753120.0","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/adapt-training-container.html\n\nIn Step 2, it is mentioned to use this instruction on dockerfile:\n\n# Defines train.py as script entrypoint\nENV SAGEMAKER_PROGRAM train.py"}],"timestamp":"2022-11-27 08:39:00","exam_id":26,"isMC":true,"answer":"B","answer_ET":"B","url":"https://www.examtopics.com/discussions/amazon/view/88925-exam-aws-certified-machine-learning-specialty-topic-1/"},{"id":"prmeF95R19ojfc0dBwbv","question_id":100,"url":"https://www.examtopics.com/discussions/amazon/view/88731-exam-aws-certified-machine-learning-specialty-topic-1/","answer_description":"","timestamp":"2022-11-25 18:39:00","answer_ET":"AD","answer":"AD","isMC":true,"answers_community":["AD (84%)","Other"],"discussion":[{"timestamp":"1669398000.0","comment_id":"726985","content":"Selected Answer: AD\nAD are the right answer","upvote_count":"6","poster":"GauravLahotiML"},{"comment_id":"1288638","poster":"MultiCloudIronMan","upvote_count":"2","timestamp":"1727186580.0","content":"Selected Answer: AD\nRandom Cut Forest (RCF) (Option D): RCF is an unsupervised algorithm designed for anomaly detection. It can identify unusual patterns in the data without requiring labeled examples of fraudulent transactions1.\nIP Insights (Option A): IP Insights is another unsupervised algorithm that can detect anomalies based on IP address usage patterns. It is particularly useful for identifying suspicious activities related to IP addresses"},{"timestamp":"1694157660.0","poster":"loict","content":"Selected Answer: AD\nA. YES - IP Insights works unsupervised on IP addresses; builtin algorithm\nB. NO - k-NN is unsupervised clustering, does not help with anomalities\nC. NO - Linear learner is supervised\nD. YES - Random Cut Forest (RCF) is unsupervised anomalities\nE. NO - XGBoost is supervised","comment_id":"1002239","upvote_count":"2"},{"upvote_count":"2","content":"Selected Answer: AD\nIP Insights is an unsupervised learning algorithm that learns the usage patterns of IP addresses. It can capture associations between IP addresses and various entities, such as user IDs or account numbers. It can also identify anomalous events, such as a user attempting to log in from an unusual IP address, or an account that is creating resources from a suspicious IP address1.\n\nRandom Cut Forest (RCF) is another unsupervised algorithm for detecting anomalous data points within a dataset. It can handle arbitrary-dimensional input and scale well with respect to number of features, data set size, and number of instances. It can detect anomalies such as unexpected spikes in time series data, breaks in periodicity, or unclassifiable data points2.","comment_id":"989146","poster":"Mickey321","timestamp":"1692877740.0"},{"poster":"SANDEEP_AWS","upvote_count":"1","comments":[{"timestamp":"1679259300.0","poster":"blanco750","upvote_count":"1","content":"It expects CSV format and the question mentions data is in CSV format so IP Insights is correct","comment_id":"844243"}],"timestamp":"1678455120.0","comment_id":"835032","content":"Selected Answer: CD\nCan't be A, as we don't have data in the format expected for IP Insights algorithm(https://docs.aws.amazon.com/sagemaker/latest/dg/ip-insights-training-data-formats.html)."},{"content":"Selected Answer: AD\nA and D are correct. \nA. IP Insights for Pattern recognition.\nD. Random Cut Forest (RCF) for Anomaly detection\n\nB,C,E are normally Supervised learning algorithm which are against the wordings \"There is no label ...\"","poster":"Jerry84","timestamp":"1673942880.0","comment_id":"778684","upvote_count":"2"},{"poster":"Peeking","comment_id":"741402","timestamp":"1670732400.0","content":"Selected Answer: AD\nC is not part of the answer. IP insight because the data contain IP address. RCF because the data is unlabeled and anomaly is being detected for fraud.","upvote_count":"3"},{"comment_id":"730358","timestamp":"1669723800.0","content":"Selected Answer: AD\nAD are correct","poster":"Amit11011996","upvote_count":"1"},{"poster":"ystotest","upvote_count":"3","comment_id":"728737","timestamp":"1669601280.0","content":"Selected Answer: AD\napprently AD"},{"timestamp":"1669397940.0","upvote_count":"3","comment_id":"726983","content":"Selected Answer: AC\nAC is the correct answer to detect anomalies","poster":"GauravLahotiML"}],"question_text":"An ecommerce company wants to use machine learning (ML) to monitor fraudulent transactions on its website. The company is using Amazon SageMaker to research, train, deploy, and monitor the ML models.\n\nThe historical transactions data is in a .csv file that is stored in Amazon S3. The data contains features such as the user's IP address, navigation time, average time on each page, and the number of clicks for each session. There is no label in the data to indicate if a transaction is anomalous.\n\nWhich models should the company use in combination to detect anomalous transactions? (Choose two.)","topic":"1","unix_timestamp":1669397940,"question_images":[],"answer_images":[],"exam_id":26,"choices":{"D":"Random Cut Forest (RCF)","A":"IP Insights","B":"K-nearest neighbors (k-NN)","C":"Linear learner with a logistic function","E":"XGBoost"}}],"exam":{"isBeta":false,"lastUpdated":"11 Apr 2025","name":"AWS Certified Machine Learning - Specialty","provider":"Amazon","isMCOnly":false,"numberOfQuestions":369,"isImplemented":true,"id":26},"currentPage":20},"__N_SSP":true}