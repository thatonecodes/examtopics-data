{"pageProps":{"questions":[{"id":"0itMMVAprAddu51rVr3l","answer_ET":"A","question_id":41,"discussion":[{"timestamp":"1719944820.0","comment_id":"1240965","content":"Why is not D a solution? If the developers in the secondary region can configure primary region's codecommit repository as a remote repository in the AWS Cloud9 environment they can do development and do all git functions remote.","upvote_count":"1","poster":"zijo"},{"comments":[{"comments":[{"comment_id":"1235111","content":"Actually, I meant to say I lean towards \"D\" (using Cloud9 as remote development environment)","upvote_count":"2","comments":[{"comment_id":"1235127","timestamp":"1719033840.0","upvote_count":"1","content":"Also want to add that \"D\" would would work fine if you presume that the \"git\" \"mirror\" is also being done (though additional undefined step in the solution). Nothing says \"D\" is the complete solution. The ONLY requirement here is to provide developers a remote environment to develop in.","poster":"Gomer"}],"timestamp":"1719032460.0","poster":"Gomer"}],"timestamp":"1719032040.0","upvote_count":"1","poster":"Gomer","comment_id":"1235106","content":"The specific requirement here isn't \"disaster recovery capability\" and \"ability to switch over its daily operations to a secondary AWS Region.\" That is just being investigated.\nThe specific requirement is to \"provide the capability for the company to develop code in the secondary Region.\"\n\"If the company needs to use the secondary Region, developers can add an additional remote URL to their local Git configuration.\"\n\nTo me this sounds to like the specific requirement here is only to provide developers with a complete remote development environment (not to provide a DR solution)\nIf that is true, then using Cloud9 web development environment (includes git, etc.) with same local CodeCommit repo is acceptiable\nI'm not a developer, but the specific criteria wording and logic make me lean towards \"C\""},{"upvote_count":"2","poster":"Gomer","timestamp":"1719032100.0","comment_id":"1235107","content":"Flow:\n\nAWS Example: CodeCommit(action) ----------------> Lambda -> Fargate task (\"git clone --mirror\" local repo, \"git remote set-url --push origin\" destination repo) -> CodeCommit(remote repo)\n\nSolution \"B\": CodeCommit(action) -> EventBridge -> Lambda -> Fargate task (\"git clone --mirror\" local repo, \"git remote set-url --push origin\" destination repo) -> S3(remote bucket)\n\nReferences:\nhttps://aws.amazon.com/blogs/devops/replicate-aws-codecommit-repository-between-regions-using-aws-fargate/\nhttps://aws.amazon.com/cloud9/"}],"timestamp":"1719031920.0","poster":"Gomer","content":"A: (NO) \"Create an AWS CodeBuild project to perform a Git mirror operation of the primary Region's CodeCommit repository to the secondary Region's CodeCommit repository.\"\nCodeBuild doesn't have the ability to do a \"git mirror\" operation itself. All online examples have CodeCommit actions calling Lambda (directly or through EventWatch) which calls fargate (or EC2) which does the actual git mirror\n\nA: (NO) \"Create an AWS Lambda function that invokes the CodeBuild project.\nThe is exactly the reverse from online examples\n\nB: (NO) \"Create an AWS Fargate task to perform a Git mirror operation of the primary Region's CodeCommit repository and copy the result to the S3 bucket.\"\nDoes it really make sense to use a \"git\" mirror operations copy from a CodeCommit repo to an S3 bucket? All online examples using \"git\" \"mirror\" have CodeCommit repo as remote target.","upvote_count":"1","comment_id":"1235105"},{"comments":[{"upvote_count":"2","comment_id":"1145497","timestamp":"1707485040.0","poster":"thanhnv142","content":"A is correct: < develop code in the secondary Region>: code commit cannot automatically clone cross-region.Must use a tool to do this duplication task\nB: Using S3 as a secondary repo is incorrect\nC and D: no mention of using codecommit as the secondary repo"}],"poster":"thanhnv142","timestamp":"1707275460.0","content":"A is correct: <A DevOps engineer must provide the capability for the company to develop code in the secondary Region> means code commit","upvote_count":"3","comment_id":"1142915"},{"comment_id":"1118996","upvote_count":"2","timestamp":"1704916140.0","poster":"yuliaqwerty","content":"Selected Answer: A\nAgree answer is A"},{"poster":"svjl","comments":[{"content":"This part of B is incorrect. - It should use Code commit instead of S3. \"Create an AWS Fargate task to perform a Git mirror operation of the primary Region's CodeCommit repository and copy the result to the S3 bucket. \"","timestamp":"1702836060.0","poster":"bnagaraja9099","upvote_count":"1","comment_id":"1099120"}],"content":"B- It does the replication out of the box and meets the requirements\nhttps://aws.amazon.com/blogs/devops/replicate-aws-codecommit-repository-between-regions-using-aws-fargate/","upvote_count":"2","comment_id":"1091139","timestamp":"1702045140.0"},{"poster":"vandergun","comment_id":"1074444","content":"Selected Answer: A\nA is at least operation and cost","upvote_count":"2","timestamp":"1700368860.0"},{"comment_id":"1014728","content":"Selected Answer: A\nThis solution meets all of the company's requirements:\n\nIt allows developers to add an additional remote URL to their local Git configuration to develop code in the secondary Region.\nIt is automated: the EventBridge rule will automatically invoke the Lambda function whenever a merge event occurs in the primary Region's CodeCommit repository.\nIt is reliable: the CodeBuild project will use Git to ensure that a perfect copy of the primary Region's CodeCommit repository is created in the secondary Region.","upvote_count":"4","timestamp":"1695453060.0","poster":"Dushank"},{"timestamp":"1695212640.0","comment_id":"1012299","content":"Selected Answer: A\nhttps://dev.to/apatil88/replicate-aws-codecommit-repositories-between-regions-using-codebuild-and-codepipeline-5fh1","upvote_count":"4","poster":"RVivek"},{"timestamp":"1693573620.0","content":"Selected Answer: B\nB is right. https://aws.amazon.com/cn/blogs/devops/replicate-aws-codecommit-repository-between-regions-using-aws-fargate/","upvote_count":"2","poster":"ixdb","comment_id":"996073","comments":[{"comment_id":"1014750","poster":"zendevloper","timestamp":"1695456360.0","content":"B is wrong because it uses S3. Developers need a valid git remote URL.\nCorrect answer is A","upvote_count":"3"}]}],"question_images":[],"answer_description":"","choices":{"D":"Create an AWS Cloud9 environment and a CodeCommit repository in the secondary Region. Configure the primary Region's CodeCommit repository as a remote repository in the AWS Cloud9 environment. Connect the secondary Region's CodeCommit repository to the AWS Cloud9 environment.","A":"Create a CodeCommit repository in the secondary Region. Create an AWS CodeBuild project to perform a Git mirror operation of the primary Region's CodeCommit repository to the secondary Region's CodeCommit repository. Create an AWS Lambda function that invokes the CodeBuild project. Create an Amazon EventBridge rule that reacts to merge events in the primary Region's CodeCommit repository. Configure the EventBridge rule to invoke the Lambda function.","C":"Create an AWS CodeArtifact repository in the secondary Region. Create an AWS CodePipeline pipeline that uses the primary Regionâ€™s CodeCommit repository for the source action. Create a cross-Region stage in the pipeline that packages the CodeCommit repository contents and stores the contents in the CodeArtifact repository when a pull request is merged into the CodeCommit repository.","B":"Create an Amazon S3 bucket in the secondary Region. Create an AWS Fargate task to perform a Git mirror operation of the primary Region's CodeCommit repository and copy the result to the S3 bucket. Create an AWS Lambda function that initiates the Fargate task. Create an Amazon EventBridge rule that reacts to merge events in the CodeCommit repository. Configure the EventBridge rule to invoke the Lambda function."},"answer":"A","timestamp":"2023-09-01 15:07:00","isMC":true,"question_text":"A company is examining its disaster recovery capability and wants the ability to switch over its daily operations to a secondary AWS Region. The company uses AWS CodeCommit as a source control tool in the primary Region.\n\nA DevOps engineer must provide the capability for the company to develop code in the secondary Region. If the company needs to use the secondary Region, developers can add an additional remote URL to their local Git configuration.\n\nWhich solution will meet these requirements?","url":"https://www.examtopics.com/discussions/amazon/view/119654-exam-aws-certified-devops-engineer-professional-dop-c02/","topic":"1","answer_images":[],"exam_id":23,"answers_community":["A (86%)","14%"],"unix_timestamp":1693573620},{"id":"BsMScFG6FJgCZBH97Khu","exam_id":23,"answer_description":"","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/119637-exam-aws-certified-devops-engineer-professional-dop-c02/","unix_timestamp":1693558800,"discussion":[{"comment_id":"1146396","timestamp":"1707578040.0","upvote_count":"5","content":"Selected Answer: A\nA is the solution which will allow testing without any such consequence","poster":"Ramdi1"},{"content":"Selected Answer: A\nA correct as allow testing before real deployment. \nhttps://aws.amazon.com/blogs/devops/enhancing-automated-database-continuous-integration-with-aws-codebuild-and-amazon-rds-database-snapshot/","upvote_count":"2","poster":"jamesf","timestamp":"1722236760.0","comment_id":"1257303"},{"content":"Selected Answer: A\nA is correct: This option allow testing before real deployment\nB: < Deploy the application to production> : this would not allow testing before changes are made\nC: <Create a snapshot of the DB cluster before deploying the application>: This means the same as B - would not allow testing before changes are made\nD: <Ensure that the DB cluster is a Multi-AZ deployment. Deploy the application with the updates> - This deploy the app before testing, so it is incoorect","poster":"thanhnv142","upvote_count":"4","timestamp":"1707486180.0","comment_id":"1145513"},{"content":"A is correct: <The DevOps team uses continuous integration to periodically verify that the application works> and <The DevOps team needs to test the changes before the changes are deployed to the production database> means codebuild","comment_id":"1142916","timestamp":"1707275580.0","poster":"thanhnv142","upvote_count":"1"},{"comment_id":"1014735","content":"This solution meets all of the company's requirements:\n\nIt allows the DevOps team to test the changes before they are deployed to the production database.\nIt is automated: the CodeBuild buildspec file will automatically restore the DB cluster from a snapshot, run the integration tests, and drop the restored database after verification.\nIt is reliable: the CodeBuild buildspec file will ensure that the integration tests are run against a copy of the production database.","upvote_count":"1","timestamp":"1695454200.0","poster":"Dushank"},{"upvote_count":"4","comment_id":"996075","poster":"ixdb","content":"Selected Answer: A\nA is right. All others will change the prod db.","timestamp":"1693573680.0"},{"content":"I think it is A\n\nhttps://aws.amazon.com/blogs/devops/enhancing-automated-database-continuous-integration-with-aws-codebuild-and-amazon-rds-database-snapshot/","comment_id":"995869","upvote_count":"2","timestamp":"1693558800.0","poster":"traveller37"}],"answer_images":[],"timestamp":"2023-09-01 11:00:00","choices":{"D":"Ensure that the DB cluster is a Multi-AZ deployment. Deploy the application with the updates. Fail over to the standby instance if verification fails.","A":"Use a buildspec file in AWS CodeBuild to restore the DB cluster from a snapshot of the production database, run integration tests, and drop the restored database after verification.","B":"Deploy the application to production. Configure an audit log of data control language (DCL) operations to capture database activities to perform if verification fails.","C":"Create a snapshot of the DB cluster before deploying the application. Use the Update requires:Replacement property on the DB instance in AWS CloudFormation to deploy the application and apply the changes."},"question_text":"A DevOps team is merging code revisions for an application that uses an Amazon RDS Multi-AZ DB cluster for its production database. The DevOps team uses continuous integration to periodically verify that the application works. The DevOps team needs to test the changes before the changes are deployed to the production database.\n\nWhich solution will meet these requirements?","topic":"1","question_id":42,"answer_ET":"A","answer":"A","answers_community":["A (100%)"],"question_images":[]},{"id":"MPWs86BHbjJnqBR0PHDQ","discussion":[{"upvote_count":"14","comment_id":"995872","timestamp":"1693558920.0","content":"I think C:\nhttps://aws.amazon.com/blogs/security/automatically-block-suspicious-traffic-with-aws-network-firewall-and-amazon-guardduty/","poster":"traveller37","comments":[{"comment_id":"996920","content":"Sorry i means B","timestamp":"1693662060.0","upvote_count":"1","comments":[{"upvote_count":"1","content":"You mean C?","poster":"denccc","comment_id":"1062927","timestamp":"1699191420.0"}],"poster":"traveller37"}]},{"poster":"RVivek","upvote_count":"11","timestamp":"1695419280.0","comment_id":"1014529","content":"Selected Answer: C\nC is correct . Only Network Firewall can block traffic at VPC level. \nA only updates the list , no blocking action\nB- WAF and Web ACL can block only HTTPS traffic for a API/VPC endpoint/ Cloudfron distribution not for enire VPC"},{"content":"Selected Answer: C\nC, AWS Network Firewall can block traffic at VPC level.\nhttps://aws.amazon.com/blogs/security/automatically-block-suspicious-traffic-with-aws-network-firewall-and-amazon-guardduty/","timestamp":"1722237000.0","poster":"jamesf","upvote_count":"2","comment_id":"1257304"},{"content":"Selected Answer: C\nB blocks traffic at the http/https web traffic layer not for VPC layer","poster":"zijo","timestamp":"1719952620.0","upvote_count":"1","comment_id":"1241035"},{"upvote_count":"3","poster":"thanhnv142","content":"Selected Answer: C\nC is correct: <a solution to automatically deny traffic> means network FW. \nA: irrelevant\nB: We need network fw, not WAF\nD: irrelevant","comment_id":"1142918","timestamp":"1707275760.0"},{"comment_id":"1080310","upvote_count":"1","content":"hmmm\nis this the last question as of now(25th Nov 23)","timestamp":"1700943360.0","poster":"yorkicurke"},{"poster":"Dushank","comment_id":"1014738","timestamp":"1695454680.0","upvote_count":"6","content":"Selected Answer: C\nHere's the rationale for choosing this option:\n\nAWS Network Firewall:\nAWS Network Firewall is designed to provide centralized network traffic inspection and filtering. It's a suitable choice for implementing network-level controls.\n\nLambda Function for Automation:\nCreating a Lambda function to trigger the creation of a Drop action rule in the firewall policy allows for automated response based on Security Hub findings. This enables you to take immediate action when suspicious sources are detected.\n\nSpecific Action (Drop):\nThe Drop action rule is effective for denying traffic from suspicious sources, effectively controlling access and preventing unwanted traffic.\n\nThis approach aligns well with the requirement to automatically deny traffic when GuardDuty identifies a new suspicious source, enhancing security in the multi-tenant VPC environment."},{"upvote_count":"1","comment_id":"1012316","timestamp":"1695213300.0","content":"Selected Answer: B\nA only will upadte threat list. the requirement is to block the taffic.\nB is corerect. Also it is event driven immditae action","poster":"RVivek"},{"poster":"vladik820","timestamp":"1695030780.0","comment_id":"1010447","content":"Selected Answer: A\nA is right","upvote_count":"1"}],"choices":{"A":"Create a GuardDuty threat list. Configure GuardDuty to reference the list. Create an AWS Lambda function that will update the threat list. Configure the Lambda function to run in response to new Security Hub findings that come from GuardDuty.","C":"Configure a firewall in AWS Network Firewall. Create an AWS Lambda function that will create a Drop action rule in the firewall policy. Configure the Lambda function to run in response to new Security Hub findings that come from GuardDuty.","B":"Configure an AWS WAF web ACL that includes a custom rule group. Create an AWS Lambda function that will create a block rule in the custom rule group. Configure the Lambda function to run in response to new Security Hub findings that come from GuardDuty.","D":"Create an AWS Lambda function that will create a GuardDuty suppression rule. Configure the Lambda function to run in response to new Security Hub findings that come from GuardDuty."},"question_images":[],"question_id":43,"answer_ET":"C","answer_images":[],"isMC":true,"topic":"1","exam_id":23,"answers_community":["C (92%)","4%"],"answer":"C","unix_timestamp":1693558920,"question_text":"A company manages a multi-tenant environment in its VPC and has configured Amazon GuardDuty for the corresponding AWS account. The company sends all GuardDuty findings to AWS Security Hub.\n\nTraffic from suspicious sources is generating a large number of findings. A DevOps engineer needs to implement a solution to automatically deny traffic across the entire VPC when GuardDuty discovers a new suspicious source.\n\nWhich solution will meet these requirements?","timestamp":"2023-09-01 11:02:00","url":"https://www.examtopics.com/discussions/amazon/view/119638-exam-aws-certified-devops-engineer-professional-dop-c02/","answer_description":""},{"id":"Dp85bhOb7D4cycHRiLcK","answer_description":"","isMC":true,"discussion":[{"content":"Selected Answer: BD\nB and D are correct: <update the infrastructure to ensure that only the Lambda functionâ€™s execution role> means we need to ensure that lambda's IAM role has sufficient permissions and KMS policy allows Lambda's IAM role\nA: cannot update default key\nC: <allows the account's root principal to decrypt> this against the principal of least privilege\nE: irrelevant","upvote_count":"5","poster":"thanhnv142","comment_id":"1142920","timestamp":"1707276060.0"},{"comment_id":"1288282","upvote_count":"1","poster":"heff_bezos","content":"Selected Answer: BD\nIf default keys are the same as the AWS managed keys, then the answer is B. You cannot modify the \"default\" key's policy to allow access only from the Lambda execution role.","timestamp":"1727116980.0"},{"poster":"jamesf","content":"Selected Answer: BD\nI go for BD","comment_id":"1257309","timestamp":"1722237420.0","upvote_count":"2"},{"poster":"4555894","timestamp":"1710005820.0","content":"Selected Answer: BD\nThe requirement is to update the infrastructure to ensure that only the Lambda functionâ€™s execution\nrole can access the values in Secrets Manager. The solution must apply the principle of least\nprivilege, which means granting the minimum permissions necessary to perform a task.","comment_id":"1169684","upvote_count":"2"},{"upvote_count":"1","poster":"hotblooded","comment_id":"1139961","timestamp":"1707040140.0","content":"Selected Answer: AD\n{\n\"Version\": \"2012-10-17\",\n\"Id\": \"key-consolepolicy-2\",\n\"Statement\": [\n{\n\"Sid\": \"Allow use of the key\",\n\"Effect\": \"Allow\",\n\"Principal\": {\"AWS\": [\n\"arn:aws:iam::111122223333:role/KeyCreatorRole\"\n]},\n\"Action\": [\n\"kms:Encrypt\",\n\"kms:Decrypt\",\n\"kms:ReEncrypt*\",\n\"kms:GenerateDataKey*\",\n\"kms:DescribeKey\"\n],\n\"Resource\": here arn of secret manager\n}\n]\n}\n\nI think A is correct answer , why to create CMK as customer is using default KMS"},{"timestamp":"1701354600.0","content":"Selected Answer: BD\nI think B:D","comment_id":"1084451","upvote_count":"4","poster":"zolthar_z"},{"comments":[{"upvote_count":"1","comments":[{"upvote_count":"1","content":"Or we can ad below condition also \n\n\"Condition\": {\n \"StringEquals\": {\n \"kms:CallerAccount\": \"111122223333\",\n \"kms:ViaService\": \"secretsmanager.us-west-2.amazonaws.com\"\n }\n }","comment_id":"1139959","timestamp":"1707039960.0","poster":"hotblooded"}],"content":"{\n \"Version\": \"2012-10-17\",\n \"Id\": \"key-consolepolicy-2\",\n \"Statement\": [\n {\n \"Sid\": \"Allow use of the key\",\n \"Effect\": \"Allow\",\n \"Principal\": {\"AWS\": [\n \"arn:aws:iam::111122223333:role/KeyCreatorRole\"\n ]},\n \"Action\": [\n \"kms:Encrypt\",\n \"kms:Decrypt\",\n \"kms:ReEncrypt*\",\n \"kms:GenerateDataKey*\",\n \"kms:DescribeKey\"\n ],\n \"Resource\": here arn of secret manager\n }\n ]\n}\n\nI think A is correct answer , why to create CMK as customer is using default KMS","timestamp":"1707039720.0","poster":"hotblooded","comment_id":"1139956"}],"upvote_count":"4","poster":"radev","timestamp":"1700731500.0","content":"Selected Answer: BD\nB, D\nA is incorrect because updating the default KMS key for Secrets Manager to allow only the Lambda function's execution role to decrypt would grant access to all other resources using the default key, which violates the principle of least privilege.\n\nC is incorrect because allowing the account's root principal to decrypt the secret would grant unnecessary access to the secret, which violates the principle of least privilege.\n\nE is incorrect because removing all KMS permissions from the Lambda function's execution role would prevent the Lambda function from decrypting the secret, which is required for it to function properly.","comment_id":"1078286"},{"content":"Selected Answer: BD\nI vote B,D","poster":"vandergun","timestamp":"1700658900.0","comment_id":"1077401","upvote_count":"2"}],"timestamp":"2023-11-22 14:15:00","exam_id":23,"answer_ET":"BD","question_text":"A company uses AWS Secrets Manager to store a set of sensitive API keys that an AWS Lambda function uses. When the Lambda function is invoked the Lambda function retrieves the API keys and makes an API call to an external service. The Secrets Manager secret is encrypted with the default AWS Key Management Service (AWS KMS) key.\n\nA DevOps engineer needs to update the infrastructure to ensure that only the Lambda functionâ€™s execution role can access the values in Secrets Manager. The solution must apply the principle of least privilege.\n\nWhich combination of steps will meet these requirements? (Choose two.)","answers_community":["BD (95%)","5%"],"question_id":44,"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/126917-exam-aws-certified-devops-engineer-professional-dop-c02/","unix_timestamp":1700658900,"choices":{"C":"Create a KMS customer managed key that trusts Secrets Manager and allows the account's root principal to decrypt. Update Secrets Manager to use the new customer managed key","E":"Remove all KMS permissions from the Lambda functionâ€™s execution role","B":"Create a KMS customer managed key that trusts Secrets Manager and allows the Lambda function's execution role to decrypt. Update Secrets Manager to use the new customer managed key","D":"Ensure that the Lambda functionâ€™s execution role has the KMS permissions scoped on the resource level. Configure the permissions so that the KMS key can encrypt the Secrets Manager secret","A":"Update the default KMS key for Secrets Manager to allow only the Lambda functionâ€™s execution role to decrypt"},"question_images":[],"topic":"1","answer":"BD"},{"id":"Nipu8sHPKVGfXxLrFzoe","question_text":"A company's DevOps engineer is creating an AWS Lambda function to process notifications from an Amazon Simple Notification Service (Amazon SNS) topic. The Lambda function will process the notification messages and will write the contents of the notification messages to an Amazon RDS Multi-AZ DB instance.\n\nDuring testing, a database administrator accidentally shut down the DB instance. While the database was down the company lost several of the SNS notification messages that were delivered during that time.\n\nThe DevOps engineer needs to prevent the loss of notification messages in the future.\n\nWhich solutions will meet this requirement? (Choose two.)","answer_description":"","topic":"1","answer_images":[],"discussion":[{"poster":"vandergun","timestamp":"1700742540.0","upvote_count":"11","comment_id":"1078424","content":"Selected Answer: CD\nThe two solutions that will meet the requirement of preventing the loss of notification messages in the future are:\n\nD. Subscribe an Amazon Simple Queue Service (Amazon SQS) queue to the SNS topic. Configure the Lambda function to process messages from the SQS queue.\n\nThis solution will ensure that notification messages are delivered to the SQS queue even if the Lambda function is unavailable or the RDS DB instance is down. The Lambda function can then process the messages from the SQS queue at its own pace.\n\nC. Configure an Amazon Simple Queue Service (Amazon SQS) dead-letter queue for the SNS topic.\n\nThis solution will ensure that notification messages that cannot be delivered to the RDS DB instance are not lost. Instead, they will be moved to a dead-letter queue. The DevOps engineer can then manually process the messages from the dead-letter queue."},{"comment_id":"1084455","timestamp":"1701354840.0","upvote_count":"5","content":"Selected Answer: CD\nC:D , D is a best practice for this scenario, C because you can send failed SNS o SQS Dead letter queue, https://docs.aws.amazon.com/sns/latest/dg/sns-dead-letter-queues.html","poster":"zolthar_z"},{"content":"Selected Answer: BD\nC does not make sense. Adding a DLQ to an SNS topic will capture messages that failed to be sent to the consumer, due to an issue with SNS. We are looking to capture messages that couldn't be processed due to a issue at the consumer side (RDS database to be exact, but the Lambda function will be aware of this). \nHence there are two solutions:\n1. configure a SQS between the SNS topic and the Lambda function \n2. configure a DLQ as a destination for failure for the Lambda function (raise an exceuption in the Lambda code)","comment_id":"1353164","poster":"ce0df07","upvote_count":"1","timestamp":"1738972680.0"},{"upvote_count":"1","timestamp":"1734249060.0","poster":"CHRIS12722222","comment_id":"1326745","content":"Selected Answer: BD\nCorrect answer\n\nhttps://www.youtube.com/watch?v=rYFAdRCibyc"},{"poster":"weixing","timestamp":"1728542040.0","upvote_count":"1","comment_id":"1295440","content":"BD\nC. Dead-letter queues can only be added to SNS subscriptions, not to topics."},{"comment_id":"1253092","poster":"h432ng","content":"AD.\n\nC is wrong, \"Configuring an Amazon SNS dead-letter queue for a subscription\" not for SNS topic\nA is correct, with Dynamodb, admin can no longer \"accidentally shut down the DB instance.\"\n\nA fixes the root cause. With D an SQS is there, no need for DLQ for SNS. If lambda process data from SQS, what is SNS DLQ help here?","timestamp":"1721653380.0","upvote_count":"2"},{"upvote_count":"4","timestamp":"1707276420.0","content":"Selected Answer: CD\nC and D are correct: <. While the database was down the company lost several of the SNS notification messages that were delivered during that time> means dead-letter queue in SQS and output SNS to SQS to store dead-letter queue","comment_id":"1142922","poster":"thanhnv142"},{"content":"Selected Answer: BD\nB & D are correct","poster":"zain1258","timestamp":"1701183120.0","comment_id":"1082667","comments":[{"poster":"Gomer","content":"Here's what I get when you break it down graphically between CD and BC:\n\nCD: SNS > SQS|DLQ) > Lambda > RDS\nBD: SNS > SQS > Lambda > SQS > RDS\nThe DLQ is just there to handle any SNS messages that have errors and can't be processed. There is no way you want/need two SQS queues in series (on either side of the Lambda). The ONLY thing you need to add for the requirements is queue to hold stuff while DB is down. The DLQ just makes sure even an messed up message data is captured for later review. Only C&D make any sense here.","comment_id":"1236588","upvote_count":"2","timestamp":"1719258900.0"}],"upvote_count":"1"}],"unix_timestamp":1700742540,"question_id":45,"exam_id":23,"answers_community":["CD (87%)","13%"],"isMC":true,"timestamp":"2023-11-23 13:29:00","choices":{"E":"Replace the SNS topic with an Amazon EventBridge event bus. Configure an EventBridge rule on the new event bus to invoke the Lambda function for each event.","D":"Subscribe an Amazon Simple Queue Service (Amazon SQS) queue to the SNS topic. Configure the Lambda function to process messages from the SQS queue.","B":"Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination of the Lambda function.","C":"Configure an Amazon Simple Queue Service (Amazon SQS) dead-letter queue for the SNS topic.","A":"Replace the RDS Multi-AZ DB instance with an Amazon DynamoDB table."},"url":"https://www.examtopics.com/discussions/amazon/view/127016-exam-aws-certified-devops-engineer-professional-dop-c02/","question_images":[],"answer":"CD","answer_ET":"CD"}],"exam":{"isBeta":false,"isImplemented":true,"provider":"Amazon","id":23,"numberOfQuestions":355,"lastUpdated":"11 Apr 2025","name":"AWS Certified DevOps Engineer - Professional DOP-C02","isMCOnly":true},"currentPage":9},"__N_SSP":true}