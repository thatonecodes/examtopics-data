{"pageProps":{"questions":[{"id":"yAraMqvjoibFegGE5tVr","exam_id":20,"question_text":"A company uses the Amazon Kinesis SDK to write data to Kinesis Data Streams. Compliance requirements state that the data must be encrypted at rest using a key that can be rotated. The company wants to meet this encryption requirement with minimal coding effort.\nHow can these requirements be met?","discussion":[{"comment_id":"179490","upvote_count":"20","poster":"KoMo","content":"B.\nhttps://docs.aws.amazon.com/streams/latest/dev/what-is-sse.html","timestamp":"1633651500.0"},{"upvote_count":"6","poster":"cloudlearnerhere","comment_id":"713085","content":"Selected Answer: B\nCorrect answer is B as Kinesis Data Streams supports data at rest encryption using Server-Side encryption. Data is encrypted before persisting and decrypted before being read by the consumers and requires no changes to producers and consumers.\n\nOptions A & C are wrong as it would require coding effort.\n\nOption D is wrong as the default key cannot be rotated.","timestamp":"1667830140.0"},{"poster":"Debi_mishra","timestamp":"1685213820.0","content":"B is correct. D is wrong - AWS Managed keys are rotated but as per AWS not as per customer. Here customer want rotation capability and they might want to do it number of times in a year.","upvote_count":"1","comment_id":"908170"},{"content":"B: I passed the test","timestamp":"1682957640.0","poster":"pk349","upvote_count":"1","comment_id":"886467"},{"timestamp":"1658806260.0","comment_id":"637105","content":"Selected Answer: B\nSelected Answer: B","poster":"rocky48","upvote_count":"1"},{"content":"Selected Answer: B\nIt should be B. B describes the method to encrypt your data at rest inside your Kinesis Data Streams. \nOption D is the only valid alternative given the constraints, but there is no valid method to rotate this key yourself. So I would argue this key is not rotatable.","upvote_count":"1","timestamp":"1655814360.0","poster":"Ramshizzle","comment_id":"619803"},{"comment_id":"597947","content":"Selected Answer: B\nVote for B.\nI think \"rotatable key\" means you can rotate manually, it should be CMK, not AWS managed key.\nD said \"using the default KMS key\", it is saying to use AWS managed key. So it's not right.","timestamp":"1651890000.0","poster":"MWL","upvote_count":"2"},{"poster":"Teraxs","comments":[{"poster":"MWL","upvote_count":"2","timestamp":"1651889880.0","comment_id":"597945","content":"I think \"rotatable key\" means you can rotate manually, it should be CMK, not AWS managed key."}],"upvote_count":"2","comment_id":"597271","timestamp":"1651749300.0","content":"Selected Answer: D\nI'd say D:\nA and C are out because they involve more coding.\nB would work, but key rotation of CMK is disabled by default and the answer did not say to enable it (but mentions creation and alias, so that was likely left out on purpose)\nD works, is the simplest and the key is rotated by default (no every year, used to be every 3 years)\nParagraph \"Customer managaged keys\" \n in https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html#rotate-aws-owned-keys"},{"poster":"jrheen","timestamp":"1651360560.0","upvote_count":"1","content":"Answer - B","comment_id":"595342"},{"comment_id":"483320","upvote_count":"1","timestamp":"1637503980.0","poster":"aws2019","content":"B is the right answer"},{"poster":"lostsoul07","content":"B is the right answer","timestamp":"1636002900.0","upvote_count":"2","comment_id":"280822"},{"upvote_count":"3","comment_id":"280185","timestamp":"1635634080.0","poster":"jay1ram2","content":"The Answer is B. You cannot rotate \"AWS Managed CMK\" i.e. Default keys. It is automatically rotated every 3 years. \n\nhttps://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html"},{"content":"\"key that can be rotated\" it is talking about CMK. B must be the answer.","poster":"Subho_in","timestamp":"1635268680.0","upvote_count":"2","comment_id":"268297"},{"comment_id":"250797","upvote_count":"3","poster":"gtourkas","content":"Just checked the Kinesis console. You can select either the default or the CMK for encryption at rest. Since rotation can be set for the CMK, B is the answer.","timestamp":"1635217560.0"},{"comments":[{"upvote_count":"1","content":"No, there is no such thing as default key for KDS. It's B","poster":"sly_tail","comment_id":"856031","timestamp":"1680199080.0"}],"upvote_count":"2","comment_id":"248310","content":"D. \n\nthe FAQ's https://aws.amazon.com/kinesis/data-streams/faqs/#kinesis-encryption \nquestion : \n\"What is server-side encryption\"\nIt mentions \"Server-side encryption for Kinesis Data Streams automatically encrypts data using a user specified AWS KMS master key (CMK) \"","poster":"Shivibaheti","timestamp":"1635162600.0"},{"comment_id":"239028","poster":"Draco31","content":"B.\nit's written here: https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html\nYou cannot rotate key that you did not create","upvote_count":"3","timestamp":"1634890740.0"},{"content":"It should be B. \nHere is the link,\nhttps://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html","upvote_count":"1","comment_id":"194528","poster":"Sent1","timestamp":"1634049000.0"},{"timestamp":"1634012280.0","poster":"aceit","content":"It should be D.\nFor every encrypted stream, the Kinesis service calls the AWS KMS service approximately every five minutes to create a new data encryption key. In a 30-day month, each encrypted stream generates approximately 8,640 KMS API requests and here it 's talking about both custom KMS master keys and the (Default) aws/kinesis customer master key (CMK). \nhttps://docs.aws.amazon.com/streams/latest/dev/costs-performance.html","upvote_count":"2","comment_id":"182377"},{"timestamp":"1633886220.0","comments":[{"content":"I don't think so. \nFrom the doc you posted:\n\"Key rotation in AWS KMS is a cryptographic best practice that is designed to be transparent and easy to use. AWS KMS supports optional automatic key rotation only for customer managed CMKs.\" \nSo you have to create one first.","poster":"crispogioele","timestamp":"1636033020.0","upvote_count":"1","comment_id":"282796","comments":[{"poster":"Ryo0w0o","comment_id":"716686","timestamp":"1668257760.0","content":"D seems to be correct.\nAccording to the doc, \"you can enable automatic key rotation for an existing KMS key.\"\nSo, auto-rotation could be applied to the default key. This is the minimal effort.","upvote_count":"1"}]}],"poster":"giocal","content":"D\nKMS allows you to rotate key (https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html#rotate-keys-manually)","upvote_count":"3","comment_id":"182325"},{"comments":[{"timestamp":"1633499100.0","poster":"pmjcr","content":"D does not provide key rotation as required by the question so I believe B is the answer.","comment_id":"176095","upvote_count":"2"}],"timestamp":"1633422720.0","poster":"Paitan","content":"B and D both works. But default rotation period using KMS is 1 year. So maybe B is the right choice. But I am still not fully convinced.","upvote_count":"1","comment_id":"175660"},{"upvote_count":"3","timestamp":"1633112880.0","comment_id":"171469","poster":"Marc34","content":"It's B."},{"comments":[{"timestamp":"1632233580.0","poster":"awssp12345","comment_id":"169389","content":"I agree","upvote_count":"1","comments":[{"comments":[{"upvote_count":"12","comments":[{"poster":"awssp12345","comment_id":"173039","upvote_count":"2","content":"makes sense. Changing answer to B.","timestamp":"1633337040.0"}],"content":"I think its B. Because the question they mentioned about \"a key that can be rotated\". With default KMS CMK it will be rotated only every 3 years. But customer managed CMK you can manage the key rotation...Thoughts?","timestamp":"1633058580.0","comment_id":"170010","poster":"Sent1"}],"timestamp":"1632934800.0","comment_id":"169559","content":"Using the AWS managed CMK will be minimal effort.","upvote_count":"1","poster":"awssp12345"}]},{"upvote_count":"2","comment_id":"214610","poster":"LMax","comments":[{"upvote_count":"3","timestamp":"1634398560.0","poster":"LMax","content":"I have probably missed key rotation requirement. With that in mind changing my answer to B","comment_id":"214611"}],"content":"It's D.\n\"You can now encrypt data in your Amazon Kinesis streams using server-side encryption and AWS Key Management Service (KMS) keys. Server-side encryption makes it easy to meet strict data management requirements by encrypting your data at rest within Kinesis Streams.\"\nhttps://aws.amazon.com/about-aws/whats-new/2017/07/amazon-kinesis-streams-introduces-server-side-encryption/#:~:text=You%20can%20now%20encrypt%20data,at%20rest%20within%20Kinesis%20Streams.&text=For%20details%2C%20see%20the%20AWS%20KMS%20pricing%20page.","timestamp":"1634236200.0"}],"poster":"testtaker3434","timestamp":"1632221760.0","content":"I will go with D","upvote_count":"4","comment_id":"167888"}],"url":"https://www.examtopics.com/discussions/amazon/view/29733-exam-aws-certified-data-analytics-specialty-topic-1-question/","isMC":true,"answer_images":[],"unix_timestamp":1598569260,"choices":{"A":"Create a customer master key (CMK) in AWS KMS. Assign the CMK an alias. Use the AWS Encryption SDK, providing it with the key alias to encrypt and decrypt the data.","B":"Create a customer master key (CMK) in AWS KMS. Assign the CMK an alias. Enable server-side encryption on the Kinesis data stream using the CMK alias as the KMS master key.","D":"Enable server-side encryption on the Kinesis data stream using the default KMS key for Kinesis Data Streams.","C":"Create a customer master key (CMK) in AWS KMS. Create an AWS Lambda function to encrypt and decrypt the data. Set the KMS key ID in the function's environment variables."},"answer_ET":"B","question_id":131,"answer_description":"","topic":"1","question_images":[],"answers_community":["B (83%)","D (17%)"],"answer":"B","timestamp":"2020-08-28 01:01:00"},{"id":"aTbkXBKtXSgqMCPnFPjF","question_text":"A streaming application is reading data from Amazon Kinesis Data Streams and immediately writing the data to an Amazon S3 bucket every 10 seconds. The application is reading data from hundreds of shards. The batch interval cannot be changed due to a separate requirement. The data is being accessed by Amazon\nAthena. Users are seeing degradation in query performance as time progresses.\nWhich action can help improve query performance?","unix_timestamp":1597235160,"question_id":132,"answers_community":["A (100%)"],"answer_images":[],"isMC":true,"answer":"A","timestamp":"2020-08-12 14:26:00","exam_id":20,"choices":{"C":"Add more memory and CPU capacity to the streaming application.","B":"Increase the number of shards in Kinesis Data Streams.","A":"Merge the files in Amazon S3 to form larger files.","D":"Write the files to multiple S3 buckets."},"topic":"1","answer_ET":"A","discussion":[{"content":"It should be A, large number of small files ins3 will slow down reads","poster":"abhineet","timestamp":"1632178980.0","comments":[{"upvote_count":"5","comment_id":"156523","timestamp":"1632825000.0","content":"Yeap, I agree its A.","poster":"testtaker3434"},{"upvote_count":"2","timestamp":"1640076600.0","poster":"[Removed]","comment_id":"505932","content":"You can speed up your queries dramatically by compressing your data, provided that files are splittable or of an optimal size (optimal S3 file size is between 200MB-1GB). Smaller data sizes mean less network traffic between Amazon S3 to Athena."}],"upvote_count":"41","comment_id":"156405"},{"poster":"Paitan","comment_id":"175251","upvote_count":"10","content":"Merge the files in Amazon S3 to form larger files will definitely increase read performance. So option A is the right choice.","timestamp":"1633746180.0"},{"timestamp":"1709077500.0","comment_id":"1161056","upvote_count":"1","poster":"chinmayj213","content":"Everyone is saying A, which is write but why because 1000's shard and per shard capacity is 1 mb , So 1000's of files per second . which require merge to improve the query performance."},{"poster":"NikkyDicky","content":"Selected Answer: A\nA for sure","timestamp":"1690840260.0","upvote_count":"1","comment_id":"968517"},{"poster":"pk349","content":"A: I passed the test","upvote_count":"2","comments":[{"poster":"Priya_angre","content":"what is right answer","timestamp":"1684082160.0","upvote_count":"1","comment_id":"897716"}],"timestamp":"1682946240.0","comment_id":"886257"},{"content":"A. This bit of AWS documentation: https://docs.aws.amazon.com/athena/latest/ug/performance-tuning-s3-throttling.html says \"If possible, avoid having a large number of small files. Amazon S3 has a limit of 5500 requests per second, and your Athena queries share this same limit. If you scan millions of small objects in a single query, your query will likely be throttled by Amazon S3.\"","timestamp":"1680249840.0","comment_id":"856761","poster":"Aina","upvote_count":"1"},{"poster":"AwsNewPeople","comment_id":"835870","content":"A. Merge the files in Amazon S3 to form larger files.\n\nTo improve query performance when using Amazon Athena to access data from an Amazon S3 bucket, the streaming application should merge the files in S3 to form larger files. When the streaming application writes data to S3 every 10 seconds, it creates small files, which can lead to a large number of small files over time. This can lead to performance degradation in Athena queries as more small files mean more metadata needs to be scanned, and more file operations are required to read data. By merging small files into larger files, the number of files in the bucket can be reduced, which can significantly improve Athena query performance.\n\nIncreasing the number of shards in Kinesis Data Streams, adding more memory and CPU capacity to the streaming application, or writing files to multiple S3 buckets are not directly related to the issue of degraded query performance in Athena.","timestamp":"1678532940.0","upvote_count":"4"},{"comment_id":"829096","poster":"itsme1","content":"Selected Answer: A\ns3 has a limit of 5500 requests per second, combining reduces the requests\n\nhttps://docs.aws.amazon.com/athena/latest/ug/performance-tuning.html","timestamp":"1677945600.0","upvote_count":"1"},{"timestamp":"1667563140.0","upvote_count":"7","poster":"cloudlearnerhere","content":"Selected Answer: A\nCorrect answer is A as merging files to form a bigger file can help optimize and improve query performance.\nOption B is wrong as increasing shards would only increase the ingestion flow.\n\nOptions C & D are wrong as it does not improve Athena's query performance.","comment_id":"711099"},{"content":"Selected Answer: A\nMerging small files into larger files will reduce the number of compute activities and speed up the process","poster":"MultiCloudIronMan","comment_id":"694918","upvote_count":"2","timestamp":"1665766860.0"},{"upvote_count":"2","timestamp":"1662420900.0","content":"https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/","poster":"Abep","comment_id":"660623"},{"upvote_count":"2","timestamp":"1658291040.0","poster":"rocky48","content":"Selected Answer: A\nAnswer should be A","comment_id":"633826"},{"upvote_count":"3","poster":"ru4aws","comment_id":"631419","content":"Selected Answer: A\nA \nas merging small files into one large file will result in less meta data to maintain for the Data Catalog to maintain which results in Athena to scan data faster","timestamp":"1657808760.0"},{"upvote_count":"2","poster":"dushmantha","timestamp":"1655622960.0","comment_id":"618570","content":"Things can be done to increase performance of Athena are use columnar formats, use small number of large files, use partitions. So the answer should be A."},{"comment_id":"605272","content":"Selected Answer: A\nAnswer should be A","timestamp":"1653206220.0","upvote_count":"1","poster":"Bik000"},{"content":"Selected Answer: A\nAnswer is A","upvote_count":"3","poster":"moon2351","comment_id":"570134","timestamp":"1647563460.0"},{"poster":"RSSRAO","upvote_count":"3","comment_id":"546392","timestamp":"1644748260.0","content":"Selected Answer: A\nA is the correct answer. merge small files into larger files works as expected"},{"poster":"Donell","upvote_count":"2","timestamp":"1635896940.0","comment_id":"392652","content":"Q5 Options:\n\nA. Grant permissions in Amazon Redshift to allow the marketing Amazon Redshift user to access the three promotion columns of the advertising external table.\n\nB. Create an Amazon Redshift Spectrum IAM role with permissions for Lake Formation. Attach it to the Amazon Redshift cluster.\n\nC. Create an Amazon Redshift Spectrum IAM role with permissions for the marketing S3 bucket. Attach it to the Amazon Redshift cluster.\n\nD. Create an external schema in Amazon Redshift by using the Amazon Redshift Spectrum 1AM role. Grant usage to the marketing Amazon Redshift user.\n\nE. Grant permissions in Lake Formation to allow the Amazon Redshift Spectrum role to access the three promotion columns of the advertising table.\n\nF. Grant permissions in Lake Formation to allow the marketing IAM group to access the three promotion columns of the advertising table.","comments":[{"timestamp":"1636250520.0","content":"Can you please tell the answer for Q5 which you have posted below","comment_id":"440364","upvote_count":"1","poster":"priyajit_nanda"},{"poster":"Olga2022","comment_id":"484160","content":"I think it should be B, D, E\nhttps://aws.amazon.com/about-aws/whats-new/2019/08/amazon-redshift-spectrum-now-supports-column-level-access-control-with-aws-lake-formation/","upvote_count":"5","timestamp":"1637582220.0"}]},{"timestamp":"1635849480.0","poster":"Donell","upvote_count":"2","comment_id":"392651","content":"Q5) A company is providing analytics services to its sales and marketing departments. The departments can access the data only through their business intelligence (Bl) tools, which run queries on Amazon Redshift using an Amazon Redshift internal user to connect. Each department is assigned a user in the Amazon Redshift database with the permissions needed for that department. The marketing data analysts must be granted direct access to the advertising table, which is stored in Apache Parquet format in the marketing S3 bucket of the company data lake. The company data lake is managed by AWS Lake Formation. Finally, access must be limited to the three promotion columns in the table.\nWhich combination of steps will meet these requirements? (Select THREE)"},{"comments":[{"comment_id":"492905","timestamp":"1638498360.0","poster":"lakeswimmer","content":"A - Lake formation reduces operational overhead","upvote_count":"7"}],"content":"Real Exam question AWS DAS-C01: Do comment your answers:\nQ4) A company has a data lake on AWS that ingests sources of data from multiple business units and uses Amazon Athena for queries. The storage layer is Amazon S3 using the AWS Glue Data Catalog. The company wants to make the data available to its data scientists and business analysts. However, the company first needs to manage data access for Athena based on user roles and responsibilities.\nWhat should the company do to apply these access controls with the LEAST operational overhead?\n\nA. Define security policy-based rules for the users and applications by role in AWS Lake Formation.\nB. Define security policy-based rules for the users and applications by role in AWS Identity and Access Management (IAM).\nC. Define security policy-based rules for the tables and columns by role in AWS Glue.\nD. Define security policy-based rules for the tables and columns by role in AWS Identity and Access Management (IAM).","upvote_count":"2","timestamp":"1635675120.0","poster":"Donell","comment_id":"392650"},{"comments":[{"poster":"Olga2022","timestamp":"1637585520.0","comment_id":"484216","comments":[{"comment_id":"498274","poster":"mdjsrk","comments":[{"comment_id":"557194","timestamp":"1645948140.0","content":"I think the answer for this qst resides in \"What is the MOST cost-effective way\"","upvote_count":"1","poster":"simo40010"}],"timestamp":"1639109340.0","upvote_count":"2","content":"Why not B? Automatic WLM manages up to 8 queues and allows concurrency scaling as well"},{"upvote_count":"1","comment_id":"639982","poster":"godspeed1204","content":"Concurrency scaling is supported only by RA3 nodes.\n\"Amazon Redshift supports concurrency scaling for write operations on only Amazon Redshift RA3 nodes, specifically ra3.16xlarge, ra3.4xlarge, and ra3.xlplus. Concurrency scaling for write operations isn't supported on other node types.\"","timestamp":"1659255060.0","comments":[{"upvote_count":"1","content":"For \"write\" operations. In this case, is a read-only query.","poster":"flanfranco","timestamp":"1678043160.0","comment_id":"830215"}]}],"upvote_count":"8","content":"Seems D\nhttps://docs.aws.amazon.com/redshift/latest/dg/concurrency-scaling.html"}],"comment_id":"392649","content":"Q3) A company is designing a data warehouse to support business intelligence reporting. Users will access the executive dashboard heavily each Monday and Friday morning for 1 hour. These read-only queries will run on the active Amazon Redshift cluster, Which runs on dc2.8xlarge compute nodes 24 hours a day, 7 days a week. There are three queues set up in workload management: Dashboard, ETL, and System. The Amazon Redshift cluster needs to process the queries without wait time.\nWhat is the MOST cost-effective way to ensure that the cluster these queries?\n\nA. Perform a classic resize to place the cluster in read-only mode while adding an additional node to the cluster.\nB. Enable automatic workloadmanagement.\nC. Perform an elastic resize to add an additional node to thecluster.\nD. Enable concurrency scaling for the Dashboard workloadqueue.","poster":"Donell","timestamp":"1635507480.0","upvote_count":"1"},{"timestamp":"1635282780.0","comments":[{"timestamp":"1638498960.0","content":"Option A\nhttps://aws.amazon.com/about-aws/whats-new/2019/11/amazon-emr-announces-one-click-access-to-persistent-spark-history-server/","comment_id":"492909","poster":"lakeswimmer","upvote_count":"4"}],"poster":"Donell","content":"Q2 options: \nA. Using the EMR console, select the cluster that is terminated and start the Spark history server. When the server is up, open it to troubleshoot the failed Spark jobs.\nB. Create a new EMR cluster and set the EMR log location to be the same as the terminated cluster’s log location. When the cluster is ready, use the Spark user interface to troubleshoot the failed jobs.\nC. Find the EMR cluster ID from the EMR console or the AWS CLI, and use that ID to find the Amazon CloudWatch metrics to troubleshoot the failed Spark jobs.\nD. Select the cluster that was terminated with the failed Spark jobs. Using the clone functionality, create a new EMR cluster by cloning the terminated cluster. Then check the Spark history server to troubleshoot the failed Spark jobs.","upvote_count":"3","comment_id":"392648"},{"timestamp":"1635205320.0","comment_id":"392647","upvote_count":"1","poster":"Donell","content":"(Posting another real exam questionAWS DAS-C01.Do comment your answers.Note that question and answer options are posted in 2 parts,since unable to post everything at once):\nQ2) A gaming company uses Amazon EMR to run its ETL workloads. When a new file is uploaded to an Amazon S3 bucket, the company executes Apache Spark jobs and terminates the EMR cluster immediately. Recently, some spark jobs are failing and logs that are stored in Amazon S3 are not providing enough insight.\nHow can the company troubleshoot its failed Spark jobs?"},{"upvote_count":"1","poster":"Donell","timestamp":"1635106560.0","comment_id":"385996","content":"Answer: A. Merge the files in Amazon S3 to form larger files."},{"upvote_count":"1","timestamp":"1635092580.0","content":"A is correct","poster":"gunjan4392","comment_id":"378640"},{"poster":"leliodesouza","comment_id":"359107","timestamp":"1634791140.0","upvote_count":"1","content":"The answer is A."},{"timestamp":"1634685000.0","poster":"lostsoul07","upvote_count":"3","comment_id":"274232","content":"A is the right answer"},{"content":"A is correct","upvote_count":"3","poster":"BillyC","comment_id":"216833","timestamp":"1634530140.0"},{"comment_id":"211523","content":"A is correct answer","poster":"sanjaym","upvote_count":"4","timestamp":"1634520720.0"},{"upvote_count":"3","poster":"tsaran1913","timestamp":"1634002320.0","content":"Answer is A. https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/","comment_id":"202241"},{"poster":"Mahesh22","upvote_count":"1","comment_id":"199523","timestamp":"1633949100.0","comments":[{"poster":"jove","timestamp":"1634075100.0","upvote_count":"3","comment_id":"210737","content":"Nope. The correct answer is A for sure."}],"content":"A- Merge to S3 is costly operation with so many API calls and it will hit performance further\nB- Increase the number of shard will increase inflow and the application will become slow further\nD. Multiple s3 bucket definetly not solution \nLeft with C ?"},{"timestamp":"1633257600.0","upvote_count":"4","content":"Tricky question as Bucketing/Partition can also help... but I think the answer is still A as optimising filesize is definitely one of the highly recommended.\nhttps://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/","poster":"zeronine","comment_id":"159518"},{"content":"D. See top 10 performance tuning tips for Athena “2. Bucket your data”.\nA dropped because merging more files cannot make Athena consume all data in its memory.\nB dropped due to this option literally increasing the input speed, not the output query efficiency.\nC dropped due to the question asking query performance (Athena) degradation, not the streaming application.\nhttps://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/#OptimizeFileSizes\nhttps://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL+BucketedTables","poster":"zanhsieh","comments":[{"poster":"skar21","upvote_count":"5","content":"Bucketing data is different than multiple S3 buckets. D is not correct.","timestamp":"1633304460.0","comment_id":"160274"}],"comment_id":"159191","timestamp":"1633090740.0","upvote_count":"1"},{"timestamp":"1633049040.0","poster":"singh100","comment_id":"159158","content":"Agree with A. Too many small files degrade read performance","upvote_count":"3"}],"question_images":[],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/28263-exam-aws-certified-data-analytics-specialty-topic-1-question/"},{"id":"Q0KFBATqLoMV5A6OD57e","question_text":"A company wants to enrich application logs in near-real-time and use the enriched dataset for further analysis. The application is running on Amazon EC2 instances across multiple Availability Zones and storing its logs using Amazon CloudWatch Logs. The enrichment source is stored in an Amazon DynamoDB table.\nWhich solution meets the requirements for the event collection and enrichment?","answer":"A","answer_images":[],"timestamp":"2020-08-29 22:15:00","choices":{"C":"Configure the application to write the logs locally and use Amazon Kinesis Agent to send the data to Amazon Kinesis Data Streams. Configure a Kinesis Data Analytics SQL application with the Kinesis data stream as the source. Join the SQL application input stream with DynamoDB records, and then store the enriched output stream in Amazon S3 using Amazon Kinesis Data Firehose.","B":"Export the raw logs to Amazon S3 on an hourly basis using the AWS CLI. Use AWS Glue crawlers to catalog the logs. Set up an AWS Glue connection for the DynamoDB table and set up an AWS Glue ETL job to enrich the data. Store the enriched data in Amazon S3.","D":"Export the raw logs to Amazon S3 on an hourly basis using the AWS CLI. Use Apache Spark SQL on Amazon EMR to read the logs from Amazon S3 and enrich the records with the data from DynamoDB. Store the enriched data in Amazon S3.","A":"Use a CloudWatch Logs subscription to send the data to Amazon Kinesis Data Firehose. Use AWS Lambda to transform the data in the Kinesis Data Firehose delivery stream and enrich it with the data in the DynamoDB table. Configure Amazon S3 as the Kinesis Data Firehose delivery destination."},"discussion":[{"upvote_count":"38","content":"The answer is A - Since they are already using CloudWatch Logs, it makes sense to send the CW logs to KFH which will invoke lambda and send data to S3 for further analysis.","comments":[{"poster":"[Removed]","timestamp":"1641535500.0","upvote_count":"3","comment_id":"518790","content":"KDA can refrer s3 only for data enrichment."}],"poster":"awssp12345","timestamp":"1632228720.0","comment_id":"169554"},{"upvote_count":"10","comment_id":"713087","timestamp":"1667830260.0","content":"Selected Answer: A\nCorrect answer is A as CloudWatch logs can be integrated with Kinesis Data Firehose using subscription filters, the data can be enriched using Lambda doing a lookup on the DynamoDB tables and data storage to S3.\n\nOptions B & D are wrong as exporting the logs would not provide near-real-time data handling.\n\nOption C is wrong as using Kinesis Data Stream for data collection with agents would increase overhead, also Kinesis Data Analytics does not support DynamoDB as a reference data source for enrichment.","poster":"cloudlearnerhere"},{"comment_id":"886469","poster":"pk349","content":"A: I passed the test","timestamp":"1682957700.0","upvote_count":"1"},{"content":"Selected Answer: A\nSelected Answer: A","comment_id":"637101","timestamp":"1658806020.0","poster":"rocky48","upvote_count":"1"},{"comment_id":"623993","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html#FirehoseExample. B, D are obviously wrong. DynamoDB is not for joining.","poster":"GarfieldBin","comments":[{"comment_id":"744436","upvote_count":"1","timestamp":"1670961960.0","content":"Agree!","poster":"renfdo"}],"upvote_count":"2","timestamp":"1656423300.0"},{"timestamp":"1653204480.0","content":"Selected Answer: A\nMy Answer is A","poster":"Bik000","comment_id":"605230","upvote_count":"1"},{"poster":"ShilaP","upvote_count":"2","timestamp":"1647781260.0","comment_id":"571601","content":"A is the right answer"},{"comments":[{"upvote_count":"1","content":"You can uncompress inside the Lambda without any issue. So answer A works.","poster":"Ramshizzle","timestamp":"1655808600.0","comment_id":"619771"}],"upvote_count":"2","content":"The answer is C - Since CW logs is written in gzip format to Kinesis Firehose. In the Lambda function the uncompress command should be applied first. https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html#FirehoseExample","comment_id":"523499","timestamp":"1642158960.0","poster":"ses13"},{"comments":[{"poster":"Ramshizzle","timestamp":"1655808660.0","upvote_count":"1","comment_id":"619772","content":"Lambda does the enrichment and so the Lambda needs to read the DynamoDB table. This is perfectly fine."}],"comment_id":"493884","poster":"arun004","upvote_count":"1","timestamp":"1638642660.0","content":"How KDF refer data stored in DynamoDB ? i don't think it is possible"},{"timestamp":"1637503560.0","content":"A is the right answer","upvote_count":"1","poster":"aws2019","comment_id":"483315"},{"upvote_count":"1","content":"Why not D ?\nhttps://aws.amazon.com/es/blogs/big-data/analyze-your-data-on-amazon-dynamodb-with-apache-spark/","poster":"Gekko","comment_id":"380303","timestamp":"1636013340.0","comments":[{"poster":"Dr_Kiko","timestamp":"1636156380.0","comment_id":"423964","content":"because hourly is not near realtime","upvote_count":"4"}]},{"content":"A is the right answer","timestamp":"1635630540.0","comment_id":"280825","poster":"lostsoul07","upvote_count":"4"},{"timestamp":"1635506100.0","upvote_count":"2","poster":"Lucas88","comment_id":"272709","content":"The thing that confuses me about A is the combination of Firehose and Lambda, is it even possible to add a Lambda processing step in Firehose?","comments":[{"timestamp":"1635564900.0","upvote_count":"3","content":"Never mind, it is indeed possible!","poster":"Lucas88","comment_id":"272710"}]},{"upvote_count":"1","content":"Here Key word ' multiple Availability Zones' is crucial. We need to export the log from differnet zones.","timestamp":"1634841960.0","poster":"SA_206","comment_id":"267354"},{"comments":[{"upvote_count":"1","timestamp":"1634282400.0","comments":[{"content":"Ignore C <<< KDA join Dynamo not possible . ( reference data on S3 would have worked ) \nA works well <<<<","poster":"angadaws","upvote_count":"2","timestamp":"1634478780.0","comment_id":"237735"}],"comment_id":"237732","content":"K Agent ->KDS-> KDA join Dynamo -> KDF->s3","poster":"angadaws"}],"upvote_count":"3","comment_id":"237731","poster":"angadaws","content":"C - is possible too -> KDS-> KDA join Dynamo -> KDF->s3\nhttps://docs.aws.amazon.com/firehose/latest/dev/writing-with-agents.html\nhttps://aws.amazon.com/blogs/big-data/joining-and-enriching-streaming-data-on-amazon-kinesis/\n\nboth solution can be near real time . \nA - only because CW logs is used ?? Tricky Question","timestamp":"1633941720.0"},{"upvote_count":"1","timestamp":"1632947640.0","poster":"LMax","comment_id":"214809","content":"Only A provides near-real-time. Rest would have much longer delay."},{"poster":"syu31svc","comment_id":"191944","timestamp":"1632936780.0","content":"Answer is A\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html#FirehoseExample","upvote_count":"2"},{"upvote_count":"3","content":"I will also go with option A.","poster":"Paitan","comment_id":"175662","timestamp":"1632452220.0"},{"upvote_count":"4","content":"I will go with A","comment_id":"169985","poster":"testtaker3434","timestamp":"1632354480.0"}],"answer_description":"","unix_timestamp":1598732100,"answers_community":["A (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/30049-exam-aws-certified-data-analytics-specialty-topic-1-question/","question_id":133,"exam_id":20,"answer_ET":"A","topic":"1","isMC":true,"question_images":[]},{"id":"Xl4i6y1W7W0ulgS07XYf","answer_images":[],"question_images":[],"answer":"B","url":"https://www.examtopics.com/discussions/amazon/view/30053-exam-aws-certified-data-analytics-specialty-topic-1-question/","discussion":[{"comment_id":"169965","content":"It's B.\nhttps://aws.amazon.com/jp/about-aws/whats-new/2020/03/announcing-column-level-access-control-for-amazon-redshift/","poster":"esuaaaa","upvote_count":"22","timestamp":"1633714500.0","comments":[{"timestamp":"1638542040.0","content":"B it is\nFor row level access I guess only option to create views, any thoughts?","comment_id":"493181","poster":"lakeswimmer","upvote_count":"2"},{"content":"grant select(cust_name, cust_phone) on cust_profile to user1;","upvote_count":"2","comment_id":"505805","timestamp":"1640058180.0","poster":"lakediver"},{"timestamp":"1678386060.0","comment_id":"834262","content":"B talks about providing access at a user level rather than a group. It this not an operational overhead?","upvote_count":"1","poster":"rsn"}]},{"upvote_count":"6","poster":"cloudlearnerhere","timestamp":"1667830380.0","content":"Selected Answer: B\nCorrect answer is B as Redshift supports column-level access control, which works best with the table-level access control without having to implement views.\n\nOption A is wrong as it increases maintenance overhead.\n\nOption C is wrong as IAM policy does not help provide column-level access control.\n\n\nOption D is wrong as using Redshift column-level access control is better than views.","comment_id":"713090"},{"timestamp":"1682957760.0","comment_id":"886470","comments":[{"upvote_count":"1","comment_id":"887143","content":"got how many questions from the dump?","poster":"anjuvinayan","timestamp":"1683015000.0"}],"upvote_count":"1","poster":"pk349","content":"B: I passed the test"},{"poster":"akashm99101001com","upvote_count":"1","content":"Selected Answer: B\nGRANT can be used to assume an IAM role as well which covers options C as well.\nhttps://docs.aws.amazon.com/redshift/latest/dg/r_GRANT-usage-notes.html#r_GRANT-usage-notes-assumerole","comment_id":"844037","comments":[{"timestamp":"1679246940.0","comment_id":"844044","poster":"akashm99101001com","content":"\"The data in the table will eventually be referenced by several existing queries that run many times a day.\"\nIf the view is based on a complex query that joins many tables or performs many calculations, it can be slow to query. If the view is based on a large amount of data, it can also be slow to query.","upvote_count":"2"}],"timestamp":"1679246340.0"},{"content":"B is correct According to Stephane Maarek course on Udemy\nSince March 2020, Amazon Redshift supports column-level access control for data in Redshift. Customers can use column-level GRANT and REVOKE statements to help meet their security and compliance needs.\n\nRedshift's table-level access controls for the data in Redshift are already in use by many customers, but they also want the ability to control access in more detail. You can now control access to columns without having to implement view-based access control or use another system. Column-level access control is available in all Amazon Redshift regions.\n\nGRANT command defines access privileges for a user or user group. Privileges include access options such as being able to read data in tables and views, write data, create tables, and drop tables. Use this command to give specific privileges for a table, database, schema, function, procedure, language, or column.","poster":"nharaz","upvote_count":"4","comment_id":"708603","timestamp":"1667237820.0"},{"content":"Selected Answer: B\nB. Column level access control is available in redshift.","poster":"aefuen1","comment_id":"708118","timestamp":"1667179920.0","upvote_count":"1"},{"timestamp":"1665904380.0","content":"Selected Answer: C\nthe key is lowest maintenance overhead!!\nif you grant access permission using SQL, you will facing with endless maintenance","comment_id":"696051","poster":"LukeTran3206","upvote_count":"1"},{"timestamp":"1659432600.0","upvote_count":"1","poster":"dushmantha","content":"Selected Answer: C\nI will choose \"C\". Because its easy for me to grant read only access for any user for non sensitive data. And to allow only auditers to access sensitive data. Not other way around as given in \"B\".","comment_id":"641193"},{"content":"Selected Answer: B\nIts B, remember that create a view is not a good practice and might have leak of data. The best practice is to GRANT","timestamp":"1658610120.0","comment_id":"635749","poster":"carbita","upvote_count":"2"},{"comment_id":"634919","timestamp":"1658451720.0","content":"Selected Answer: B\nB is the right answer.","upvote_count":"1","poster":"rocky48"},{"timestamp":"1653205260.0","poster":"Bik000","content":"Selected Answer: B\nAnswer is B","comment_id":"605250","upvote_count":"1"},{"upvote_count":"1","comment_id":"605050","content":"B and C are very similar. The only advantage for B is that there is a single role assigned to auditors to access all the columns. While in case of C, auditors will access few columns via public role and few senstive columns via another role created specific to them.","timestamp":"1653177120.0","poster":"certificationJunkie"},{"content":"Answer - B","upvote_count":"1","poster":"jrheen","timestamp":"1651352880.0","comment_id":"595293"},{"comment_id":"590219","timestamp":"1650655620.0","poster":"ay12","upvote_count":"2","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/redshift/latest/dg/r_GRANT.html"},{"poster":"aws2019","content":"B is the ans","timestamp":"1637379780.0","comment_id":"482247","upvote_count":"1"},{"upvote_count":"1","poster":"Marcinha","comments":[{"timestamp":"1636129320.0","comment_id":"443561","poster":"Marcinha","content":"Changed for D","upvote_count":"1","comments":[{"timestamp":"1636261560.0","upvote_count":"1","comment_id":"443563","poster":"Marcinha","content":"Changed for B"}]}],"comment_id":"441487","timestamp":"1635718080.0","content":"It's D. Much easier to create a view than to insert in 1 new table."},{"poster":"mickies9","timestamp":"1635580800.0","content":"Why not C? all other team should have access to the table except for the sensitive data columns right? Options B is providing Audit team permission to the table and then granting access to the column again. Is that second step even necessary?","comments":[{"upvote_count":"1","poster":"dushmantha","content":"That's what I thought too. Allowing a very limited set of users to access sensitive columns is much easier","comment_id":"641189","timestamp":"1659432240.0"}],"comment_id":"421028","upvote_count":"2"},{"content":"Answer B.\nColumn level access control is available in Redshift using GRANT command in SQL.","timestamp":"1635320040.0","poster":"Donell","upvote_count":"1","comment_id":"387986"},{"comment_id":"354696","poster":"Heer","content":"ANSWER:OPTION B\nEXPLAINATION:Using 'Grant' Redshift can control the access to the columns in the table .Which lead to filtering out option C and D.Option A is using 'Grant'to provide read only access to already filtered data (which is second table) and the question say that 'ensure that only members of the auditing group can read the columns containing sensitive data.' which is satisfied by Option B.","upvote_count":"3","timestamp":"1635283860.0"},{"content":"why not C?","upvote_count":"2","poster":"KingD","comment_id":"331003","timestamp":"1635079560.0"},{"timestamp":"1634744340.0","comment_id":"307112","poster":"dkp","upvote_count":"1","content":"https://aws.amazon.com/blogs/big-data/achieve-finer-grained-data-security-with-column-level-access-control-in-amazon-redshift/\nB is correct"},{"content":"B is the right answer","timestamp":"1634617920.0","comment_id":"280832","upvote_count":"2","poster":"lostsoul07"},{"content":"D is not an option:\nhttps://aws.amazon.com/about-aws/whats-new/2020/03/announcing-column-level-access-control-for-amazon-redshift/\n\"Many customers already use Redshift’s table-level access control for data in Redshift, and also want the ability to control access at a finer granularity. Now they can get control access to columns without having to implement views-based access control or use another system.\"","timestamp":"1634330400.0","upvote_count":"1","poster":"LMax","comment_id":"214642","comments":[{"poster":"LMax","timestamp":"1634555340.0","comment_id":"214646","content":"A is kind of similar to D, but even more complicated to implement.\nSo it could be only B or C.\nAfter checking this article:\nhttps://aws.amazon.com/about-aws/whats-new/2020/03/announcing-column-level-access-control-for-amazon-redshift/\nagree that B is the simliest.","upvote_count":"1"}]},{"comment_id":"175671","poster":"Paitan","comments":[{"poster":"Paitan","upvote_count":"1","comment_id":"177442","comments":[{"upvote_count":"2","poster":"goodh32","comment_id":"187261","content":"new view is like new object where all existing object reference in app will need to be changed. So, option D is not a valid choice. B is correct as the same object is being accessed","timestamp":"1633805700.0"}],"content":"Sorry, I meant this can be also achieved by option D (not C). But the easiest way to do is option B.","timestamp":"1633796220.0"}],"timestamp":"1633744680.0","content":"In Redshift we can have column level access control. So will chose option B.\nHowever this can be also done using option C as well.","upvote_count":"3"},{"upvote_count":"1","timestamp":"1633230900.0","comment_id":"169618","content":"Thoughts on D any one?","poster":"awssp12345"}],"topic":"1","question_text":"A company uses Amazon Redshift as its data warehouse. A new table has columns that contain sensitive data. The data in the table will eventually be referenced by several existing queries that run many times a day.\nA data analyst needs to load 100 billion rows of data into the new table. Before doing so, the data analyst must ensure that only members of the auditing group can read the columns containing sensitive data.\nHow can the data analyst meet these requirements with the lowest maintenance overhead?","exam_id":20,"answer_description":"","choices":{"C":"Load all the data into the new table and grant all users read-only permissions to non-sensitive columns. Attach an IAM policy to the auditing group with explicit ALLOW access to the sensitive data columns.","A":"Load all the data into the new table and grant the auditing group permission to read from the table. Load all the data except for the columns containing sensitive data into a second table. Grant the appropriate users read-only permissions to the second table.","D":"Load all the data into the new table and grant the auditing group permission to read from the table. Create a view of the new table that contains all the columns, except for those considered sensitive, and grant the appropriate users read-only permissions to the table.","B":"Load all the data into the new table and grant the auditing group permission to read from the table. Use the GRANT SQL command to allow read-only access to a subset of columns to the appropriate users."},"unix_timestamp":1598742600,"timestamp":"2020-08-30 01:10:00","isMC":true,"answer_ET":"B","answers_community":["B (88%)","13%"],"question_id":134},{"id":"ZKYwCgA5oGTn35WJfkBi","answer_description":"","question_text":"A banking company wants to collect large volumes of transactional data using Amazon Kinesis Data Streams for real-time analytics. The company uses\nPutRecord to send data to Amazon Kinesis, and has observed network outages during certain times of the day. The company wants to obtain exactly once semantics for the entire processing pipeline.\nWhat should the company do to obtain these characteristics?","answer_images":[],"discussion":[{"timestamp":"1632348480.0","upvote_count":"17","comments":[{"poster":"awssp12345","content":"me too!","upvote_count":"2","timestamp":"1632814500.0","comment_id":"169563"}],"content":"Agree with A.","comment_id":"167890","poster":"testtaker3434"},{"comment_id":"181227","content":"A. \nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-duplicates.html\n\"Applications that need strict guarantees should embed a primary key within the record to remove duplicates later when processing.\"","upvote_count":"9","poster":"vicks316","timestamp":"1632991320.0"},{"timestamp":"1742117580.0","poster":"Palee","content":"Selected Answer: D\nAns D\nOption A doesn't talk about \"Obtain exactly once semantics for the entire processing pipeline","upvote_count":"1","comment_id":"1399182"},{"poster":"daisyli","content":"D \nApache Flink provides a powerful API to transform, aggregate, and enrich events, and supports exactly-once semantics. Apache Flink is therefore a good foundation for the core of your streaming architecture.\nhttps://aws.amazon.com/blogs/big-data/streaming-etl-with-apache-flink-and-amazon-kinesis-data-analytics/","comment_id":"1061454","upvote_count":"2","timestamp":"1699016940.0"},{"upvote_count":"1","comment_id":"886473","timestamp":"1682957820.0","poster":"pk349","content":"A: I passed the test"},{"timestamp":"1677501060.0","content":"Selected Answer: A\nA - exactly what is requested in the description","upvote_count":"1","poster":"enoted","comment_id":"823651"},{"comment_id":"713092","timestamp":"1667830500.0","upvote_count":"3","content":"Selected Answer: A\nCorrect answer is A as producer retries can result in duplicates in Kinesis Data Streams and must be handled by the producer by using a unique key for each message.\n\nThere are two primary reasons why records may be delivered more than one time to your Amazon Kinesis Data Streams application: producer retries and consumer retries. Your application must anticipate and appropriately handle processing individual records multiple times.\n\nOptions B & C are wrong as they would not handle the exactly-once processing semantics.\n\n\nOption D is wrong as although Apache Flink and Spark Streaming would work, it would need a complete change in the current application.","poster":"cloudlearnerhere"},{"comment_id":"634369","upvote_count":"1","content":"Selected Answer: A\nSelected Answer: A","timestamp":"1658380320.0","poster":"rocky48"},{"content":"application should be idempotent. Can be achieved by including a primary key in the record. Hence, A is correct answer","upvote_count":"1","poster":"certificationJunkie","timestamp":"1653641100.0","comment_id":"607996"},{"comment_id":"600385","poster":"MWL","content":"Selected Answer: A\nThe problem is the question is, the producer may put the records several times. Donell explained this very well. \nKDA, Flink or Spark can only make sure 'Exactly once' for every record in stream. If the record is duplicated by producer, they will process exactly once for every record in stream.\nSo the answer should be A.","upvote_count":"2","timestamp":"1652315700.0"},{"timestamp":"1651349580.0","content":"Answer : A","upvote_count":"1","comment_id":"595255","poster":"jrheen"},{"upvote_count":"1","content":"A is right","timestamp":"1637185380.0","comment_id":"480294","poster":"aws2019"},{"upvote_count":"8","timestamp":"1635845460.0","comment_id":"388050","poster":"Donell","content":"Answer A. Design the application so it can remove duplicates during processing by embedding a unique ID in each record.\nProducer Retries\nConsider a producer that experiences a network-related timeout after it makes a call to PutRecord, but before it can receive an acknowledgement from Amazon Kinesis Data Streams. The producer cannot be sure if the record was delivered to Kinesis Data Streams. Assuming that every record is important to the application, the producer would have been written to retry the call with the same data. If both PutRecord calls on that same data were successfully committed to Kinesis Data Streams, then there will be two Kinesis Data Streams records. Although the two records have identical data, they also have unique sequence numbers. Applications that need strict guarantees should embed a primary key within the record to remove duplicates later when processing. Note that the number of duplicates due to producer retries is usually low compared to the number of duplicates due to consumer retries.\n\nReference: https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-duplicates.html"},{"comment_id":"354715","poster":"Heer","comments":[{"content":"The problem is the question is, the producer may put the records several times. Donell explained this very well. So the answer should be A.\nFlink or Spark can only make sure 'Exactly once' for every record in stream. If the record is duplicated by producer, they don't work.","poster":"MWL","upvote_count":"1","comment_id":"600383","timestamp":"1652315580.0"}],"timestamp":"1635396540.0","content":"ANSWER:D\nKDS and KDF has 'exactly once' semantics .Option A is a fail safe mechanism when there is a choppy network while sending data to KDS with PutRecord.\nApache Flink and Apache Spark both guarantees 'Exactly once' semantics and which is what is the requirement as per the question .","upvote_count":"4"},{"content":"A is the right answer","timestamp":"1635363360.0","poster":"lostsoul07","comment_id":"280827","upvote_count":"3"},{"comment_id":"239041","comments":[{"poster":"liyungho","content":"According to Flink kinesis connector doc -- https://ci.apache.org/projects/flink/flink-docs-stable/dev/connectors/kinesis.html, in the Kinesis producer section, \nit states \"Note that the producer is not participating in Flink’s checkpointing and doesn’t provide exactly-once processing guarantees. Also, the Kinesis producer does not guarantee that records are written in order to the shards (See here and here for more details).\nIn case of a failure or a resharding, data will be written again to Kinesis, leading to duplicates. This behavior is usually called “at-least-once” semantics.\"\nSo I think the answer is A.","upvote_count":"2","comment_id":"239781","timestamp":"1634938440.0"},{"comment_id":"256681","poster":"omar_bahrain","upvote_count":"1","content":"There are documents that link streaming (Kafka/Kinesis) and EMR/Spark/Flink to remove deduplication in realtime. \nhttps://blog.griddynamics.com/in-stream-deduplication-with-spark-amazon-kinesis-and-s3/ \n\nin addition to the difficulty of changing running application, I would say D is a good potential candidate","timestamp":"1635257340.0"}],"content":"A was a good choice until i search EMR Flink...: \nhttps://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-flink.html\nApache Flink is a streaming dataflow engine that you can use to run real-time stream processing on high-throughput data sources. Flink supports event time semantics for out-of-order events, exactly-once semantics, backpressure control, and APIs optimized for writing both streaming and batch applications. \nCan be connected to KDS.\nSo i will pick up D","upvote_count":"2","timestamp":"1633767720.0","poster":"Draco31"},{"comment_id":"191945","poster":"syu31svc","upvote_count":"1","timestamp":"1633642920.0","content":"Link provided supports A as the answer"},{"upvote_count":"1","content":"I will also go with option A.","poster":"Paitan","comment_id":"175664","timestamp":"1632817620.0"}],"choices":{"B":"Rely on the processing semantics of Amazon Kinesis Data Analytics to avoid duplicate processing of events.","A":"Design the application so it can remove duplicates during processing be embedding a unique ID in each record.","D":"Rely on the exactly one processing semantics of Apache Flink and Apache Spark Streaming included in Amazon EMR.","C":"Design the data producer so events are not ingested into Kinesis Data Streams multiple times."},"answers_community":["A (88%)","13%"],"question_id":135,"answer_ET":"A","answer":"A","exam_id":20,"url":"https://www.examtopics.com/discussions/amazon/view/29735-exam-aws-certified-data-analytics-specialty-topic-1-question/","isMC":true,"topic":"1","question_images":[],"unix_timestamp":1598569440,"timestamp":"2020-08-28 01:04:00"}],"exam":{"provider":"Amazon","id":20,"isBeta":false,"isImplemented":true,"name":"AWS Certified Data Analytics - Specialty","lastUpdated":"11 Apr 2025","numberOfQuestions":164,"isMCOnly":true},"currentPage":27},"__N_SSP":true}