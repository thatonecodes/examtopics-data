{"pageProps":{"questions":[{"id":"uc2PHdJpy1nwsoqmxIrP","answer":"B","question_id":156,"question_text":"A company is developing a gene reporting device that will collect genomic information to assist researchers with collecting large samples of data from a diverse population. The device will push 8 KB of genomic data every second to a data platform that will need to process and analyze the data and provide information back to researchers. The data platform must meet the following requirements:\n\n• Provide near-real-time analytics of the inbound genomic data\n• Ensure the data is flexible, parallel, and durable\n• Deliver results of processing to a data warehouse\n\nWhich strategy should a solutions architect use to meet these requirements?","answer_ET":"B","url":"https://www.examtopics.com/discussions/amazon/view/112889-exam-aws-certified-solutions-architect-professional-sap-c02/","discussion":[{"poster":"shaaam80","comment_id":"1084542","upvote_count":"8","timestamp":"1701361800.0","content":"Selected Answer: B\nAnswer B. \nOption A might be close enough, near-real time, which is Firehose, but the target is RDS but the ask is for Datawarehouse (Redshift)"},{"comment_id":"1225429","timestamp":"1717672080.0","upvote_count":"1","content":"Dis the right Answer","poster":"Win007","comments":[{"upvote_count":"1","comment_id":"1271482","timestamp":"1724460600.0","content":"just B","poster":"helloworldabc"}]},{"poster":"bjexamprep","upvote_count":"1","timestamp":"1713489660.0","content":"Selected Answer: D\nKinesis client is a library. Users need to write an application with the Kinesis Client Library to use it. \nBoth A and B states “analyze the data with Kinesis clients” without mentioning how the application is written and deployed. So, both A and B are out, cause the deployment model is the key of the question to satisfy the requirement.\nC has an incorrect statement “analyze the data from Amazon SQS with Kinesis”\nD is a feasible solution.","comment_id":"1198261","comments":[{"comment_id":"1271481","upvote_count":"1","content":"just B","timestamp":"1724460540.0","poster":"helloworldabc"},{"upvote_count":"1","poster":"jopaca1216","timestamp":"1721595840.0","content":"SQS is not near real time","comment_id":"1252718"}]},{"comment_id":"1179487","content":"Selected Answer: B\nCorrect answer is B.","timestamp":"1711044360.0","poster":"Dgix","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: B\n'parallel'","comment_id":"1122424","poster":"tmlong18","timestamp":"1705224420.0"},{"comment_id":"1078898","poster":"career360guru","upvote_count":"1","timestamp":"1700780700.0","content":"Selected Answer: B\nOption B"},{"upvote_count":"1","comment_id":"945041","poster":"NikkyDicky","content":"Selected Answer: B\nB for sure","timestamp":"1688681280.0"},{"timestamp":"1687809420.0","content":"Selected Answer: B\nB. Real-time is either firehose (A) or streams (B). But they require a data warehouse and that's RedShift not RDS.","comment_id":"934759","poster":"SmileyCloud","upvote_count":"4"},{"poster":"easytoo","content":"b=b=b=b=b=b=b","upvote_count":"1","timestamp":"1687547940.0","comment_id":"931897"},{"comment_id":"931559","poster":"nexus2020","content":"Selected Answer: B\nB is the one for real time","timestamp":"1687522140.0","upvote_count":"1"},{"upvote_count":"2","comment_id":"930854","poster":"MoussaNoussa","content":"Answer B is the right one","timestamp":"1687455660.0"},{"comment_id":"930065","timestamp":"1687398840.0","content":"Selected Answer: B\nB is correct\nB - Kinesis Data Streams is a real-time streaming service and provide near-real-time analytics. Also the question \"Deliver results of processing to a data warehouse\" and this option has redshift cluster which is a powerful data warehousing solution that can handle large-scale analytics workloads.\n\nA - incorrect because Kinesis Data Firehose is NOT ideal for near-real-time analytics and may introduce some latency in the data processing pipeline. Additionally, saving the results to an Amazon RDS instance may not provide the scalability and flexibility required for processing and analyzing large volumes of genomic data.","comments":[{"timestamp":"1688098680.0","poster":"bhanus","content":"Between A and B, B is better because questions asks for data warehousing capabilities. So option B has Redshift which is correct answer.","upvote_count":"1","comment_id":"938801"},{"upvote_count":"2","poster":"bhanus","timestamp":"1687801920.0","comment_id":"934690","content":"What a worst framed ques. The ques says \"NEAR real time\" which means its Kinesis data firehose. But this option has RDS which is not good for analysis"}],"upvote_count":"4","poster":"bhanus"}],"isMC":true,"answer_description":"","timestamp":"2023-06-22 03:54:00","answer_images":[],"question_images":[],"exam_id":33,"choices":{"C":"Use Amazon S3 to collect the inbound device data, analyze the data from Amazon SQS with Kinesis, and save the results to an Amazon Redshift cluster.","A":"Use Amazon Kinesis Data Firehose to collect the inbound sensor data, analyze the data with Kinesis clients, and save the results to an Amazon RDS instance.","B":"Use Amazon Kinesis Data Streams to collect the inbound sensor data, analyze the data with Kinesis clients, and save the results to an Amazon Redshift cluster using Amazon EMR.","D":"Use an Amazon API Gateway to put requests into an Amazon SQS queue, analyze the data with an AWS Lambda function, and save the results to an Amazon Redshift cluster using Amazon EMR."},"topic":"1","answers_community":["B (95%)","5%"],"unix_timestamp":1687398840},{"id":"UlTJqR9VlhUidMIlh7mL","topic":"1","answers_community":["C (100%)"],"question_id":157,"question_text":"A company is developing a new service that will be accessed using TCP on a static port. A solutions architect must ensure that the service is highly available, has redundancy across Availability Zones, and is accessible using the DNS name my.service.com, which is publicly accessible. The service must use fixed address assignments so other companies can add the addresses to their allow lists.\nAssuming that resources are deployed in multiple Availability Zones in a single Region, which solution will meet these requirements?","choices":{"B":"Create an Amazon ECS cluster and a service definition for the application. Create and assign public IP addresses for the ECS cluster. Create a Network Load Balancer (NLB) and expose the TCP port. Create a target group and assign the ECS cluster name to the NLCreate a new A record set named my.service.com, and assign the public IP addresses of the ECS cluster to the record set. Provide the public IP addresses of the ECS cluster to the other companies to add to their allow lists.","D":"Create an Amazon ECS cluster and a service definition for the application. Create and assign public IP address for each host in the cluster. Create an Application Load Balancer (ALB) and expose the static TCP port. Create a target group and assign the ECS service definition name to the ALB. Create a new CNAME record set and associate the public IP addresses to the record set. Provide the Elastic IP addresses of the Amazon EC2 instances to the other companies to add to their allow lists.","C":"Create Amazon EC2 instances for the service. Create one Elastic IP address for each Availability Zone. Create a Network Load Balancer (NLB) and expose the assigned TCP port. Assign the Elastic IP addresses to the NLB for each Availability Zone. Create a target group and register the EC2 instances with the NLB. Create a new A (alias) record set named my.service.com, and assign the NLB DNS name to the record set.","A":"Create Amazon EC2 instances with an Elastic IP address for each instance. Create a Network Load Balancer (NLB) and expose the static TCP port. Register EC2 instances with the NLB. Create a new name server record set named my.service.com, and assign the Elastic IP addresses of the EC2 instances to the record set. Provide the Elastic IP addresses of the EC2 instances to the other companies to add to their allow lists."},"isMC":true,"answer":"C","question_images":[],"exam_id":33,"answer_description":"","unix_timestamp":1670944500,"timestamp":"2022-12-13 16:15:00","discussion":[{"poster":"God_Is_Love","upvote_count":"12","timestamp":"1677113760.0","comment_id":"818607","content":"Logical answer : Non http port like TCP should hint to NLB immediately.(ALB does not fit here) Sharing IP address of EC2 is not apt\nwhether it is from individual EC2 instances or those from ECS cluster.this eliminates A,B.D, infact the NLB's address which stays in front of / associates to ec2 instances need to be shared. So, only solution is C"},{"timestamp":"1673638140.0","comment_id":"774801","content":"Selected Answer: C\nA more appropriate solution would be option C. Create an Amazon EC2 instances for the service. Create one Elastic IP address for each Availability Zone. Create a Network Load Balancer (NLB) and expose the assigned TCP port. Assign the Elastic IP addresses to the NLB for each Availability Zone. Create a target group and register the EC2 instances with the NLB. Create a new A (alias) record set named my.service.com, and assign the NLB DNS name to the record set. As it uses the NLB as the resource in the A-record, traffic will be routed through the NLB, and it will automatically route the traffic to the healthy instances based on the health checks and also it provides the fixed address assignments as the other companies can add the NLB's Elastic IP addresses to their allow lists.","upvote_count":"6","poster":"masetromain"},{"content":"keywords:\n'TCP', 'fixed address', 'regional' = NLB","comment_id":"1316162","upvote_count":"1","comments":[{"timestamp":"1732251180.0","comment_id":"1316163","upvote_count":"1","poster":"TariqKipkemei","content":"Answer is C"}],"poster":"TariqKipkemei","timestamp":"1732251120.0"},{"content":"Selected Answer: C\nBy choosing Option C, the company can meet the requirements of high availability, redundancy across Availability Zones, accessibility through a DNS name (my.service.com), and fixed IP address assignments that can be added to allow lists by other companies.\n\nOption A is not suitable because it involves creating Elastic IP addresses for each EC2 instance, which can become difficult to manage and does not provide the desired DNS name accessibility.\nOption B is not appropriate because it uses an Amazon ECS cluster with public IP addresses, which may not provide the desired fixed IP addresses for allow listing by other companies.\nOption D is not the correct choice because it uses an Application Load Balancer (ALB), which is designed for HTTP/HTTPS traffic and may not be the best fit for a TCP-based service. Additionally, it involves creating public IP addresses for each host in the ECS cluster, which can be complex and may not provide the desired fixed IP addresses for allow listing.","timestamp":"1731891600.0","upvote_count":"1","poster":"0b43291","comment_id":"1313780"},{"content":"C. Create Amazon EC2 instances for the service. Create one Elastic IP address for each Availability Zone. Create a Network Load Balancer (NLB) and expose the assigned TCP port. Assign the Elastic IP addresses to the NLB for each Availability Zone. Create a target group and register the EC2 instances with the NLB. Create a new A (alias) record set named my.service.com, and assign the NLB DNS name to the record set.","upvote_count":"1","comment_id":"1275464","timestamp":"1725092640.0","poster":"amministrazione"},{"comment_id":"1266330","content":"Ec2+NLB","timestamp":"1723713780.0","poster":"Ashu_0007","upvote_count":"1"},{"timestamp":"1713846000.0","content":"A looks ok\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html","poster":"Alawi_Amazon_AWS","upvote_count":"1","comment_id":"1200470"},{"upvote_count":"1","comment_id":"1175172","content":"Selected Answer: C\nC: NLB with elastic IPs","poster":"gofavad926","timestamp":"1710611820.0"},{"comment_id":"1134252","poster":"Vaibs099","upvote_count":"1","timestamp":"1706457840.0","content":"C is the right answer - Few key points - TCP static Port (go with NLB), IP Whitelisting required which can only be done with NLB. ALB doesn't support static IPs. And sharing Static (Elastic) IPs of instances of no use when using NLB. We need to share NLB Elsatic IPs from Multi AZs and create DNS record for NLB Domain Name to Domain entry."},{"poster":"sammyhaj","timestamp":"1704381000.0","content":"https://repost.aws/knowledge-center/elb-attach-elastic-ip-to-public-nlb","upvote_count":"1","comment_id":"1113795"},{"upvote_count":"1","timestamp":"1693807260.0","comment_id":"998242","content":"Selected Answer: C\nOther option haven't mention multi AZ","poster":"Simon523"},{"timestamp":"1688674800.0","upvote_count":"1","poster":"Christina666","content":"Selected Answer: C\nStatic IP-> NLB","comment_id":"944992"},{"timestamp":"1687983480.0","content":"Selected Answer: C\nI suppose C, although you can;'t do this with A record, only alias","upvote_count":"1","comment_id":"937111","poster":"NikkyDicky"},{"comment_id":"926389","poster":"SkyZeroZx","content":"Selected Answer: C\nCreate one Elastic IP address for each Availability Zone.","timestamp":"1687056960.0","upvote_count":"2"},{"upvote_count":"1","content":"C is the only option that talks about more than one AZ.","poster":"AWS_Sam","timestamp":"1683857160.0","comment_id":"895589"},{"timestamp":"1679980740.0","upvote_count":"2","poster":"mfsec","comment_id":"852809","content":"Selected Answer: C\nCreate Amazon EC2 instances for the service. Create one Elastic IP address for each Availability Zone."},{"comment_id":"831802","poster":"kiran15789","timestamp":"1678187820.0","content":"Selected Answer: C\nIP address using NLB","upvote_count":"1"},{"content":"Selected Answer: C\nC looks correct.","comment_id":"817197","poster":"saurabh1805","upvote_count":"2","timestamp":"1677015360.0"},{"upvote_count":"1","poster":"zozza2023","content":"Selected Answer: C\nC. NLB with one Elastic IP per AZ to handle TCP traffic. Alias record set named my.service.com.\nhttps://www.examtopics.com/discussions/amazon/view/28045-exam-aws-certified-solutions-architect-professional-topic-1/","timestamp":"1675085220.0","comment_id":"792845"},{"comment_id":"791541","poster":"Musk","content":"Selected Answer: C\nC looks correct. I did not read the rest.","upvote_count":"1","timestamp":"1674987060.0"},{"upvote_count":"2","timestamp":"1670944500.0","content":"Selected Answer: C\nhttps://www.examtopics.com/discussions/amazon/view/28045-exam-aws-certified-solutions-architect-professional-topic-1/","comment_id":"744188","poster":"masetromain"}],"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/91449-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_ET":"C"},{"id":"k1lCQMwCCaGGC2WB7z26","isMC":true,"answer_images":[],"timestamp":"2023-06-22 19:48:00","choices":{"C":"Use a global table within Amazon DynamoDB so data can be accessed in the two selected Regions.","B":"Use an Amazon Route 53 failover routing policy for failover from the primary Region to the disaster recovery Region. Set Time to Live (TTL) to 30 seconds.","F":"Use Auto Scaling groups for the web and application layers across multiple Availability Zones in the Regions. Use Spot Instances for the required resources.","A":"Use an Amazon Route 53 weighted routing policy set to 100/0 across the two selected Regions. Set Time to Live (TTL) to 1 hour.","E":"Implement a hot standby model using Auto Scaling groups for the web and application layers across multiple Availability Zones in the Regions. Use zonal Reserved Instances for the minimum number of servers and On-Demand Instances for any additional resources.","D":"Back up data from an Amazon DynamoDB table in the primary Region every 60 minutes and then write the data to Amazon S3. Use S3 cross-Region replication to copy the data from the primary Region to the disaster recovery Region. Have a script import the data into DynamoDB in a disaster recovery scenario."},"unix_timestamp":1687456080,"answer":"BCE","answer_ET":"BCE","topic":"1","answers_community":["BCE (94%)","6%"],"url":"https://www.examtopics.com/discussions/amazon/view/112973-exam-aws-certified-solutions-architect-professional-sap-c02/","question_images":[],"question_id":158,"exam_id":33,"answer_description":"","question_text":"A solutions architect needs to define a reference architecture for a solution for three-tier applications with web. application, and NoSQL data layers. The reference architecture must meet the following requirements:\n\n• High availability within an AWS Region\n• Able to fail over in 1 minute to another AWS Region for disaster recovery\n• Provide the most efficient solution while minimizing the impact on the user experience\n\nWhich combination of steps will meet these requirements? (Choose three.)","discussion":[{"content":"Selected Answer: BCE\nnot sure how these answers are generated, poor quality!\nCorrect answer - BCE\nHot standby, DynamoDB Global tables, Route53 failover routing policy.","poster":"shaaam80","comment_id":"1084548","upvote_count":"5","timestamp":"1732984380.0"},{"comment_id":"1078899","poster":"career360guru","content":"Selected Answer: BCE\nB, C and E","upvote_count":"2","timestamp":"1732403340.0"},{"poster":"career360guru","comment_id":"1077515","content":"Selected Answer: BCE\nFDE is incorrect.\nBCE are right options","timestamp":"1732291200.0","upvote_count":"1"},{"poster":"NikkyDicky","comment_id":"945042","upvote_count":"2","timestamp":"1720303860.0","content":"Selected Answer: BCE\nBCE for sure"},{"content":"Selected Answer: BCE\nA and D must be wrong. They cannot meet the performance requirement.\nF is not good. Spot Instances are not reliable.","poster":"Piccaso","timestamp":"1720083660.0","upvote_count":"1","comment_id":"942554"},{"content":"Selected Answer: BCE\nBCE is correct","upvote_count":"1","timestamp":"1719964860.0","comment_id":"941318","poster":"SkyZeroZx"},{"timestamp":"1719679200.0","comment_id":"938375","poster":"javitech83","upvote_count":"1","content":"Selected Answer: BCE\nBCE is correct"},{"timestamp":"1719432060.0","content":"Selected Answer: ACE\nA - Failover Rt 53\nC - Global DynamoDB tables to take care of regional replication\nE - Minimum EC2 across regions with reserved and on-demand","poster":"SmileyCloud","comments":[{"content":"Sorry BCE.","timestamp":"1719432120.0","poster":"SmileyCloud","comment_id":"934763","upvote_count":"3"}],"upvote_count":"1","comment_id":"934762"},{"timestamp":"1719340080.0","comment_id":"933817","poster":"SkyZeroZx","content":"To meet the requirements of high availability within an AWS Region, failover to another AWS Region for disaster recovery, and provide an efficient solution while minimizing user impact, the following three steps should be taken:\n\nStep B: Use an Amazon Route 53 failover routing policy for failover from the primary Region to the disaster recovery Region. Set Time to Live (TTL) to 30 seconds.\n\nBy using the failover routing policy in Amazon Route 53, you can configure DNS failover between the primary and disaster recovery Regions. This allows traffic to be redirected to the disaster recovery Region in the event of a failure in the primary Region.","comments":[{"upvote_count":"2","timestamp":"1719340080.0","poster":"SkyZeroZx","comment_id":"933819","content":"Step E: Implement a hot standby model using Auto Scaling groups for the web and application layers across multiple Availability Zones in the Regions. Use zonal Reserved Instances for the minimum number of servers and On-Demand Instances for any additional resources.\n\nBy implementing a hot standby model with Auto Scaling groups across multiple Availability Zones in both the primary and disaster recovery Regions, you can ensure high availability within the Region. Using zonal Reserved Instances for the minimum number of servers helps optimize costs, while On-Demand Instances provide flexibility for additional resource provisioning."}],"upvote_count":"1"},{"upvote_count":"1","comment_id":"933816","content":"Selected Answer: BCE\nB, C and E","timestamp":"1719340020.0","poster":"SkyZeroZx"},{"poster":"PhuocT","content":"B, C and E","upvote_count":"1","timestamp":"1719210780.0","comment_id":"932257"},{"timestamp":"1719146100.0","poster":"nexus2020","content":"Selected Answer: BCE\nBCE here as well\nA: 1 hour is too long\nD: just use global table....\nF: hot spot?","comment_id":"931584","upvote_count":"1"},{"comment_id":"930860","content":"BCE is the right answer","poster":"MoussaNoussa","timestamp":"1719078480.0","upvote_count":"2"}]},{"id":"kepEbS0BREBGi9BZLxew","answer_ET":"B","timestamp":"2023-06-23 14:48:00","isMC":true,"topic":"1","answers_community":["B (85%)","Other"],"answer":"B","question_id":159,"discussion":[{"upvote_count":"8","content":"Selected Answer: B\nOption B is correct. Feeltwise Option C requires edge agent to collect the data --> Higher operational overhead to migrate as this will need changes in customer application customer has today.","poster":"career360guru","comment_id":"1077591","timestamp":"1700673480.0"},{"upvote_count":"6","poster":"SK_Tyagi","comment_id":"986180","content":"Selected Answer: B\nThe confusion seem to be b/w IoTCore and FleetWise (B & C), however for anomaly detection one uses Kinesis Data Analytics(KDA) and other uses Glue ML algorithms. Least overhead is using Random Cut Forest in (KDA) as compared to Glue","timestamp":"1692586680.0"},{"timestamp":"1712839380.0","comment_id":"1193778","content":"Selected Answer: C\nOption C: How To \n\nhttps://aws.amazon.com/blogs/iot/best-practices-for-ingesting-data-from-devices-using-aws-iot-core-and-or-amazon-kinesis/","comments":[{"upvote_count":"1","timestamp":"1712839440.0","content":"Sorry \" Option B NOT C \"","comment_id":"1193781","poster":"TonytheTiger"}],"upvote_count":"1","poster":"TonytheTiger"},{"comments":[{"content":"just B","upvote_count":"2","poster":"helloworldabc","timestamp":"1724460780.0","comment_id":"1271484"}],"timestamp":"1706784840.0","comment_id":"1137544","content":"Selected Answer: C\nans is C","poster":"sunny","upvote_count":"1"},{"timestamp":"1704281160.0","content":"Selected Answer: D\nAnswer is D.\nAWS Lookout - Automatically detect anomalies within metrics and identify their root causes.\nhttps://aws.amazon.com/lookout-for-metrics/","poster":"learnwithaniket","comment_id":"1112704","upvote_count":"1"},{"comment_id":"1107239","poster":"Jay_2pt0_1","timestamp":"1703714700.0","upvote_count":"1","content":"Agree with @duriselvan - Fleetwise is made for this and Glue has machine learning modules"},{"poster":"duriselvan","timestamp":"1702106640.0","content":"C ans :-\nAWS IoT FleetWise: This managed service simplifies vehicle data collection and management, reducing operational overhead compared to other options.\nKinesis data stream: This serverless stream allows processing data in real-time, eliminating the need for custom code.\nKinesis Data Firehose: This service automatically stores data in S3, reducing manual intervention.\nGlue machine learning transforms: These built-in features enable anomaly detection directly within Glue, eliminating the need for separate ML models and infrastructure","comment_id":"1091490","upvote_count":"3"},{"poster":"shaaam80","content":"Selected Answer: B\nAnswer B. Straightforward\nC might sound like a good option with Fleetwise, but Glue for anamoly detection?? Also talks about Kinesis integration with Fleetwise not sure. \nFleetwise also needs a Edge agent to communicate with AWS IoT Fleetwise","timestamp":"1701362340.0","comment_id":"1084554","upvote_count":"5"},{"content":"Selected Answer: B\nits a B...oye! :)","upvote_count":"2","timestamp":"1699007940.0","comment_id":"1061358","poster":"yorkicurke"},{"upvote_count":"3","timestamp":"1697124360.0","poster":"totten","comment_id":"1041873","content":"Selected Answer: B\nHere's why option B is the best choice:\n\nSimplicity: This solution leverages AWS IoT Core and Amazon Kinesis Data Firehose, which are fully managed services, making it a simple and low-overhead option.\n\nReal-time Data Streaming: AWS IoT Core efficiently receives the vehicle data using the MQTT protocol, and Kinesis Data Firehose streams the data to Amazon S3. This supports data streaming in real-time.\n\nEasy Anomaly Detection: Amazon Kinesis Data Analytics can easily be set up to process the streaming data in real-time to detect anomalies.\n\nScalability: This architecture is designed to handle a growing number of vehicles and high data volumes, ensuring scalability without operational overhead.\n\nData Storage: Data is reliably stored in Amazon S3, eliminating concerns about on-premises storage limitations."},{"upvote_count":"1","content":"Selected Answer: B\nI agree with everyone. Even olabiba agrees. It's B.","poster":"chico2023","comment_id":"976805","timestamp":"1691595420.0"},{"upvote_count":"2","poster":"NikkyDicky","timestamp":"1688682540.0","comment_id":"945046","content":"Selected Answer: B\nit's a B\nC - there is no Fleetwise to Kinesis integration"},{"comment_id":"934767","upvote_count":"2","poster":"SmileyCloud","content":"Selected Answer: B\nA - too complex\nB - It's B. You se IoT Code, Kinesis Firehose and Kinesis Data Analytics for anomalies https://docs.aws.amazon.com/kinesisanalytics/latest/dev/app-anomaly-detection.html\nC - IoT FleetWise is a perfect use case but this solution does not detect anomalies. You need Lookout for this as described here. https://docs.aws.amazon.com/kinesisanalytics/latest/dev/app-anomaly-detection.html\nD - This is also possible, but the use case for RabbitMQ is different.","timestamp":"1687810680.0"},{"content":"c-c-c-c-c-c-c","timestamp":"1687782720.0","poster":"easytoo","comment_id":"934447","upvote_count":"1"},{"poster":"SkyZeroZx","content":"Selected Answer: B\nB for me opinion i need use Amazon Kinesis Data Analytics for detect anomalies\nC sounds goood but i don't know how AWS Glue detect anomalies , usually use case is ETL","comment_id":"933822","timestamp":"1687718220.0","upvote_count":"1"},{"timestamp":"1687638840.0","poster":"Jackhemo","upvote_count":"1","content":"Selected Answer: B\nOlabiba says 'B'.","comment_id":"932908"},{"upvote_count":"3","content":"Selected Answer: B\nAWS IoT Core provides a good way to handle data from IoT devices like these smart vehicles, especially as the MQTT protocol is used. Amazon Kinesis Data Firehose can capture, transform, and load streaming data into data lakes, data stores, and analytics services. It can handle large volumes of data from hundreds of thousands of sources, and it can scale automatically. Amazon Kinesis Data Analytics makes it easy to analyze streaming data in real-time with Java, SQL, or Apache Flink, without having to learn new programming languages or processing frameworks. It could be used to analyze the streaming data and detect anomalies","poster":"gd1","comment_id":"932036","timestamp":"1687566660.0"},{"content":"Selected Answer: B\nB. Use AWS IoT Core to receive the vehicle data. Configure rules to route data to an Amazon Kinesis Data Firehose delivery stream that stores the data in Amazon S3. Create an Amazon Kinesis Data Analytics application that reads from the delivery stream to detect anomalies.\n\nExplanation:\nThis solution leverages AWS IoT Core, which is designed for handling IoT device communication and data ingestion. The vehicle data is received by AWS IoT Core and routed using rules to an Amazon Kinesis Data Firehose delivery stream. Kinesis Data Firehose can handle high volumes of data and seamlessly store it in Amazon S3, ensuring scalability for peak traffic. To detect anomalies, an Amazon Kinesis Data Analytics application can be created to analyze the data from the delivery stream. This solution requires the least operational overhead as it leverages managed services and provides scalability and analytics capabilities for the growing volume of vehicle data.","timestamp":"1687564680.0","upvote_count":"1","poster":"Alabi","comment_id":"932016"},{"timestamp":"1687524480.0","comment_id":"931603","comments":[{"comment_id":"982421","timestamp":"1692181560.0","upvote_count":"3","content":"FleetWise only allows you to store data in TimeStream or S3 (more recently), hence the choice of Iot Core","comments":[{"poster":"carpa_jo","comment_id":"1109197","content":"Source: https://docs.aws.amazon.com/iot-fleetwise/latest/developerguide/data-ingestion.html\n\"AWS IoT FleetWise then delivers the data to a Timestream table or Amazon S3 bucket.\"","upvote_count":"2","timestamp":"1703888100.0"}],"poster":"mashandpie"}],"content":"Selected Answer: C\nAWS IoT FleetWise makes it easier for you to collect, transform, and transfer vehicle data to the cloud in near real time and use that data to improve...","poster":"nexus2020","upvote_count":"3"}],"url":"https://www.examtopics.com/discussions/amazon/view/113094-exam-aws-certified-solutions-architect-professional-sap-c02/","question_text":"A company manufactures smart vehicles. The company uses a custom application to collect vehicle data. The vehicles use the MQTT protocol to connect to the application. The company processes the data in 5-minute intervals. The company then copies vehicle telematics data to on-premises storage. Custom applications analyze this data to detect anomalies.\n\nThe number of vehicles that send data grows constantly. Newer vehicles generate high volumes of data. The on-premises storage solution is not able to scale for peak traffic, which results in data loss. The company must modernize the solution and migrate the solution to AWS to resolve the scaling challenges.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","choices":{"C":"Use AWS IoT FleetWise to collect the vehicle data. Send the data to an Amazon Kinesis data stream. Use an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Use the built-in machine learning transforms in AWS Glue to detect anomalies.","D":"Use Amazon MQ for RabbitMQ to collect the vehicle data. Send the data to an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Use Amazon Lookout for Metrics to detect anomalies.","B":"Use AWS IoT Core to receive the vehicle data. Configure rules to route data to an Amazon Kinesis Data Firehose delivery stream that stores the data in Amazon S3. Create an Amazon Kinesis Data Analytics application that reads from the delivery stream to detect anomalies.","A":"Use AWS IoT Greengrass to send the vehicle data to Amazon Managed Streaming for Apache Kafka (Amazon MSK). Create an Apache Kafka application to store the data in Amazon S3. Use a pretrained model in Amazon SageMaker to detect anomalies."},"answer_images":[],"unix_timestamp":1687524480,"answer_description":"","question_images":[],"exam_id":33},{"id":"wVbqFrZZGrva2DD1BWlo","exam_id":33,"answer_images":[],"timestamp":"2023-06-22 18:19:00","url":"https://www.examtopics.com/discussions/amazon/view/112965-exam-aws-certified-solutions-architect-professional-sap-c02/","question_text":"During an audit, a security team discovered that a development team was putting IAM user secret access keys in their code and then committing it to an AWS CodeCommit repository. The security team wants to automatically find and remediate instances of this security vulnerability.\n\nWhich solution will ensure that the credentials are appropriately secured automatically?","discussion":[{"comment_id":"934771","content":"Selected Answer: D\nA - AWS Secrets Manager can't rotate the credentials if they are part of the code\nB - You don't store creds in KMS, that's the job of Secrets Manager\nC - Macie can do S3 only. CodeCommit backend is also S3 but it's transparent for us, so you can't use Macie.\nD - Correct. See this use case https://aws.amazon.com/blogs/compute/discovering-sensitive-data-in-aws-codecommit-with-aws-lambda-2/","upvote_count":"14","poster":"SmileyCloud","timestamp":"1703629560.0"},{"timestamp":"1719399420.0","content":"D https://aws.amazon.com/blogs/compute/discovering-sensitive-data-in-aws-codecommit-with-aws-lambda-2/","upvote_count":"1","comment_id":"1106020","poster":"yuliaqwerty"},{"content":"Using lambda to trigger a scan is retrospectively ineffective as Azure can do so with DevOps Organization advanced security (which does code scanning) and provide you an option to remediate if targets are found.","poster":"Pupu86","upvote_count":"1","timestamp":"1716463560.0","comment_id":"1078448"},{"timestamp":"1716393540.0","content":"Selected Answer: D\nD is right option.","upvote_count":"1","poster":"career360guru","comment_id":"1077629"},{"poster":"joleneinthebackyard","comment_id":"1058542","upvote_count":"2","timestamp":"1714455300.0","content":"Selected Answer: D\nMacie only does S3 -> C is out\nScheduled or nightly script will only detect the problem after a while so damage might has already done --> A, B is out\nPlus KMS doesnt do secrets\nD looks valid technically"},{"poster":"ggrodskiy","content":"Correct C.\nMacie can scan for credentials in CodeCommit repositories. According to the AWS documentation, Macie supports scanning for credentials in CodeCommit repositories and triggering actions based on the findings. You can use Macie to discover sensitive data such as AWS access keys, AWS secret access keys, private keys, and more in your CodeCommit repositories. You can also configure Macie to send notifications, invoke Lambda functions, or publish findings to AWS Security Hub when it detects sensitive data in CodeCommit repositories. For more information, see Data protection in AWS CodeCommithttps://docs.aws.amazon.com/macie/latest/user/what-is-macie.html and Amazon Macie | AWS Bloghttps://aws.amazon.com/blogs/aws/category/amazon-macie/.https://docs.aws.amazon.com/macie/latest/user/what-is-macie.html: https://docs.aws.amazon.com/codecommit/latest/userguide/data-protection.htmlhttps://aws.amazon.com/blogs/aws/category/amazon-macie/: https://aws.amazon.com/blogs/aws/category/amazon-macie/","timestamp":"1705965660.0","upvote_count":"1","comment_id":"959836"},{"content":"Selected Answer: D\nD - https://aws.amazon.com/blogs/compute/discovering-sensitive-data-in-aws-codecommit-with-aws-lambda-2/","poster":"NikkyDicky","comment_id":"945049","upvote_count":"2","timestamp":"1704587520.0"},{"comment_id":"934966","poster":"River007","upvote_count":"1","content":"D can resolve the code that already commit to codecommit","timestamp":"1703650320.0","comments":[{"timestamp":"1703779740.0","poster":"RockyLeon","content":"D says Codecommit trigger to scan new code submissions.... \nhow already commit code will scan ?","comments":[{"timestamp":"1703779860.0","upvote_count":"1","poster":"RockyLeon","content":"whereas question did not ask for existing code","comment_id":"936744"}],"upvote_count":"1","comment_id":"936741"}]},{"upvote_count":"1","poster":"SkyZeroZx","comment_id":"933824","content":"Selected Answer: D\nMacie sounds good but not is use case is only scans S3.\nThen D is more apropiate in this case , similar question in this exam practice on Tutoriales Dojo","timestamp":"1703536740.0"},{"comment_id":"932863","poster":"Maria2023","timestamp":"1703452320.0","upvote_count":"2","content":"Selected Answer: D\nMacie would be a great choice but at the moment it only scans S3. And even if CodeCommit ends in S3 (according to the AWS documentation) it is not visible for us and therefore I don't believe we an configure Macie to scan. At the moment Lambda remains the best choice"},{"poster":"gd1","upvote_count":"1","content":"Selected Answer: D\nNeed auto-disable and D does it","timestamp":"1703385420.0","comment_id":"932038"},{"poster":"Alabi","upvote_count":"1","content":"Selected Answer: D\nD. Configure a CodeCommit trigger to invoke an AWS Lambda function to scan new code submissions for credentials. If credentials are found, disable them in AWS IAM and notify the user.\n\nExplanation:\nThis solution leverages a CodeCommit trigger to automatically invoke an AWS Lambda function whenever new code is submitted to the repository. The Lambda function can scan the code for credentials and if found, take appropriate actions such as disabling those credentials in AWS IAM and notifying the user. This approach ensures that the security vulnerability is automatically identified and remediated as part of the development process, providing a proactive security measure.","comment_id":"932018","timestamp":"1703383260.0"},{"upvote_count":"4","timestamp":"1703343240.0","content":"Selected Answer: D\nI would go with D. reason is ABC are all post event action, meaning the creditential are already leaked AFTER the code submition. \n\nonly D would prevent it from happeninng by doing a check BEFORE it get submitted.","poster":"nexus2020","comment_id":"931605"},{"poster":"MoussaNoussa","comment_id":"930879","content":"option D is the correct one of course","upvote_count":"3","timestamp":"1703275500.0"},{"comments":[{"content":"change it to D as it would prevent it from happeninng by doing a check BEFORE it get submitted.","timestamp":"1703917500.0","comment_id":"938810","poster":"bhanus","upvote_count":"1"}],"comment_id":"930779","content":"Selected Answer: C\nC - https://docs.aws.amazon.com/macie/latest/user/managed-data-identifiers.html#managed-data-identifiers-credentials","upvote_count":"2","timestamp":"1703269140.0","poster":"bhanus"}],"answer_ET":"D","topic":"1","answers_community":["D (93%)","7%"],"question_id":160,"answer":"D","answer_description":"","choices":{"B":"Use a scheduled AWS Lambda function to download and scan the application code from CodeCommit. If credentials are found, generate new credentials and store them in AWS KMS.","A":"Run a script nightly using AWS Systems Manager Run Command to search for credentials on the development instances. If found, use AWS Secrets Manager to rotate the credentials","C":"Configure Amazon Macie to scan for credentials in CodeCommit repositories. If credentials are found, trigger an AWS Lambda function to disable the credentials and notify the user.","D":"Configure a CodeCommit trigger to invoke an AWS Lambda function to scan new code submissions for credentials. If credentials are found, disable them in AWS IAM and notify the user."},"unix_timestamp":1687450740,"isMC":true,"question_images":[]}],"exam":{"isBeta":false,"name":"AWS Certified Solutions Architect - Professional SAP-C02","lastUpdated":"11 Apr 2025","numberOfQuestions":529,"id":33,"isImplemented":true,"provider":"Amazon","isMCOnly":true},"currentPage":32},"__N_SSP":true}