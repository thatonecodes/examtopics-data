{"pageProps":{"questions":[{"id":"A6MjwjXmN48RlxehWJ5p","answer_images":[],"question_text":"A company’s developers want a secure way to gain SSH access on the company's Amazon EC2 instances that run the latest version of Amazon Linux. The developers work remotely and in the corporate office.\n\nThe company wants to use AWS services as a part of the solution. The EC2 instances are hosted in a VPC private subnet and access the internet through a NAT gateway that is deployed in a public subnet.\n\nWhat should a solutions architect do to meet these requirements MOST cost-effectively?","url":"https://www.examtopics.com/discussions/amazon/view/132957-exam-aws-certified-solutions-architect-associate-saa-c03/","timestamp":"2024-02-05 22:06:00","answer_description":"","question_id":761,"unix_timestamp":1707167160,"topic":"1","answer_ET":"D","choices":{"C":"Create a bastion host in the public subnet of the VPConfigure the security groups and SSH keys of the bastion host to only allow connections and SSH authentication from the developers’ corporate and remote networks. Instruct the developers to connect through the bastion host by using SSH to reach the EC2 instances.","A":"Create a bastion host in the same subnet as the EC2 instances. Grant the ec2:CreateVpnConnection IAM permission to the developers. Install EC2 Instance Connect so that the developers can connect to the EC2 instances.","B":"Create an AWS Site-to-Site VPN connection between the corporate network and the VPC. Instruct the developers to use the Site-to-Site VPN connection to access the EC2 instances when the developers are on the corporate network. Instruct the developers to set up another VPN connection for access when they work remotely.","D":"Attach the AmazonSSMManagedInstanceCore IAM policy to an IAM role that is associated with the EC2 instances. Instruct the developers to use AWS Systems Manager Session Manager to access the EC2 instances."},"exam_id":31,"question_images":[],"discussion":[{"timestamp":"1727031120.0","poster":"alawada","content":"Selected Answer: D\nAWS Systems Manager Session Manager is a service that enables you to securely connect to your EC2 instances without using SSH keys or bastion hosts. You can use Session Manager to access your instances through the AWS Management Console, the AWS CLI, or the AWS SDKs. Session Manager uses IAM policies and roles to control who can access which instances. By attaching the AmazonSSMManagedlnstanceCore IAM policy to an IAM role that is associated with the EC2 instances, you grant the Session Manager service the necessary permissions to perform actions on your instances. You also need to attach another IAM policy to the developers' IAM users or roles that allows them to start sessions to the instances.","comment_id":"1180316","upvote_count":"7"},{"poster":"Andy_09","content":"Option D","comment_id":"1141487","upvote_count":"5","timestamp":"1722884760.0"},{"comment_id":"1331049","poster":"LeonSauveterre","timestamp":"1735024740.0","upvote_count":"1","content":"Selected Answer: D\nA - Bastion hosts are typically placed in public subnets (not private subnets). Also, see option C below.\nB - Would work, but this is too complex and too expensive.\nC - An additional EC2 instance (bastion host) = instance running costs + endless maintenance.\nD - God yes. No need to manage SSH keys, and access is encrypted by default."},{"comment_id":"1182950","poster":"TruthWS","upvote_count":"3","content":"Option D","timestamp":"1727311680.0"},{"upvote_count":"2","poster":"Mikado211","timestamp":"1727212020.0","comment_id":"1182023","content":"Selected Answer: D\nSSM is always the recommended way of connection for EC2 \"using ssh\".\nIt's the most cost effective and the most secure way of doing the job."},{"timestamp":"1724338920.0","comments":[{"upvote_count":"3","content":"it doesn`t meet requirements MOST cost-effectively","poster":"pila21","comment_id":"1176874","timestamp":"1726693620.0"}],"content":"Why not C?","poster":"iczcezar","upvote_count":"2","comment_id":"1156559"},{"poster":"kempes","comment_id":"1143888","timestamp":"1723064160.0","upvote_count":"3","content":"Selected Answer: D\nOption D"}],"answers_community":["D (100%)"],"isMC":true,"answer":"D"},{"id":"AK1UysyIbT95UMfN1BFt","answer_images":[],"question_images":[],"isMC":true,"answer":"C","question_text":"A pharmaceutical company is developing a new drug. The volume of data that the company generates has grown exponentially over the past few months. The company's researchers regularly require a subset of the entire dataset to be immediately available with minimal lag. However, the entire dataset does not need to be accessed on a daily basis. All the data currently resides in on-premises storage arrays, and the company wants to reduce ongoing capital expenses.\n\nWhich storage solution should a solutions architect recommend to meet these requirements?","discussion":[{"comment_id":"1141809","timestamp":"1707198600.0","upvote_count":"11","poster":"Andy_09","content":"Option C"},{"comment_id":"1144817","timestamp":"1707417780.0","upvote_count":"7","content":"B. Deploying an AWS Storage Gateway file gateway with an Amazon S3 bucket as the target storage would require the entire dataset to be stored in Amazon S3, which might not be cost-effective considering that only a subset of the data needs to be accessed regularly. Additionally, accessing data directly from S3 might introduce latency. so correct option is C bcz AWS Storage Gateway volume gateway with cached volumes allows the company to keep frequently accessed data locally on-premises while storing the entire dataset in Amazon S3. This solution provides immediate access to the subset of data with minimal lag, as frequently accessed data is cached locally. It also reduces ongoing capital expenses as it leverages Amazon S3 storage, which is cost-effective.","comments":[{"timestamp":"1720155000.0","content":"Both B and C store the entire data set in Amazon S3 and the AWS Storage Gateway file gateway also supports local caching. I do not know if it is B or C for me.","upvote_count":"2","comment_id":"1242482","poster":"capepenguin"}],"poster":"hajra313"},{"upvote_count":"1","poster":"nadeerm","timestamp":"1741350480.0","content":"Selected Answer: B\nVolume Gateway is designed for block storage (EBS volumes), not file storage. It is not the best fit for this use case, which involves file-based data access.","comment_id":"1366251"},{"comment_id":"1236393","poster":"Scheldon","upvote_count":"3","timestamp":"1719240000.0","content":"Selected Answer: C\nAnswerC\n\nDeploying an AWS Storage Gateway volume gateway with cached volumes will allow to store all data in AWS but the most frequently accessed data will be stored/cached localy (on-premises) = low latency for most used data while all data will be stored in the cloud.\n\nhttps://docs.aws.amazon.com/storagegateway/latest/vgw/StorageGatewayConcepts.html#storage-gateway-cached-concepts"},{"poster":"BatVanyo","comment_id":"1200048","upvote_count":"4","content":"A storage guy here.. the question is not clear enough to give a definitive answer between B and C, as both can do the job.\nAn \"on-prem storage array\" can be any of the three:\n- File array (serving any file protocol, e.g. NFS/SMB) -> requiring a file gateway (supports caching of the most recently used data)\n- Block array (iSCSI/Fibre Channel) -> requiring a volume gateway (supports cached volumes most recently used data)\n- Combo (providing both File and Block protocols)\n\nSomething is clearly missing in the question in order to give a definitive answer between B and C.","timestamp":"1713774960.0"},{"comment_id":"1189524","content":"Selected Answer: C\nstorage arrays = Volume Gateway","poster":"mohammadthainat","upvote_count":"3","timestamp":"1712261400.0"},{"upvote_count":"5","comment_id":"1173761","content":"Selected Answer: C\nstorage array, also known as a disk array so AWS Storage Gateway volume.\nits a trap","poster":"lenotc","timestamp":"1710452040.0"},{"timestamp":"1710390180.0","upvote_count":"3","comment_id":"1173095","content":"Selected Answer: C\nC is correct. Using AWS Storage Gateway volume gateway with cached volumes provide local access to the file.","poster":"MattBJ"},{"poster":"ninasgx","upvote_count":"4","comment_id":"1159736","content":"Selected Answer: C\nrequire a subset of the entire dataset => cached volumes","timestamp":"1708952400.0"},{"comment_id":"1158042","upvote_count":"2","timestamp":"1708792800.0","content":"Selected Answer: C\nThe company's researchers regularly require a subset of the entire dataset to be immediately available with minimal laghttps://docs.aws.amazon.com/storagegateway/latest/vgw/WhatIsStorageGateway.html","poster":"osmk"}],"topic":"1","answer_description":"","exam_id":31,"timestamp":"2024-02-06 06:50:00","choices":{"D":"Configure an AWS Site-to-Site VPN connection from the on-premises environment to AWS. Migrate data to an Amazon Elastic File System (Amazon EFS) file system.","A":"Run AWS DataSync as a scheduled cron job to migrate the data to an Amazon S3 bucket on an ongoing basis.","C":"Deploy an AWS Storage Gateway volume gateway with cached volumes with an Amazon S3 bucket as the target storage. Migrate the data to the Storage Gateway appliance.","B":"Deploy an AWS Storage Gateway file gateway with an Amazon S3 bucket as the target storage. Migrate the data to the Storage Gateway appliance."},"answer_ET":"C","answers_community":["C (95%)","5%"],"unix_timestamp":1707198600,"url":"https://www.examtopics.com/discussions/amazon/view/132996-exam-aws-certified-solutions-architect-associate-saa-c03/","question_id":762},{"id":"2eW5kWpqupGCpZ2SWXXy","answer":"A","choices":{"B":"Use AWS Backup for the table.","D":"Turn on streams on the table to capture a log of all changes to the table in the last 24 hours. Store a copy of the stream in an Amazon S3 bucket.","A":"Configure point-in-time recovery for the table.","C":"Use an AWS Lambda function to make an on-demand backup of the table every hour."},"unix_timestamp":1707167460,"url":"https://www.examtopics.com/discussions/amazon/view/132960-exam-aws-certified-solutions-architect-associate-saa-c03/","answers_community":["A (100%)"],"question_images":[],"question_text":"A company has a business-critical application that runs on Amazon EC2 instances. The application stores data in an Amazon DynamoDB table. The company must be able to revert the table to any point within the last 24 hours.\n\nWhich solution meets these requirements with the LEAST operational overhead?","discussion":[{"upvote_count":"13","content":"Option A","poster":"Andy_09","comment_id":"1141490","timestamp":"1707167460.0"},{"comment_id":"1173096","content":"Selected Answer: A\nA is correct. One of the highlight features of DynamoDB.","poster":"MattBJ","timestamp":"1710390300.0","upvote_count":"5"},{"timestamp":"1719240240.0","poster":"Scheldon","upvote_count":"3","comment_id":"1236395","content":"Selected Answer: A\nAnswerA\nPoint-in-time recovery helps protect your DynamoDB tables from accidental write or delete operations. With point-in-time recovery, you don't have to worry about creating, maintaining, or scheduling on-demand backups. For example, suppose that a test script writes accidentally to a production DynamoDB table. With point-in-time recovery, you can restore that table to any point in time during the last 35 days. After you enable point-in-time recovery, you can restore to any point in time from five minutes before the current time until 35 days ago. DynamoDB maintains incremental backups of your table.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecovery.html"},{"content":"Selected Answer: A\noption A","poster":"1dd","timestamp":"1709869200.0","comment_id":"1168532","upvote_count":"3"},{"poster":"asdfcdsxdfc","comment_id":"1166064","timestamp":"1709591340.0","content":"Selected Answer: A\nA looks correct","upvote_count":"3"},{"content":"Selected Answer: A\nOption A","timestamp":"1708789320.0","comment_id":"1158019","upvote_count":"3","poster":"_mavik_"}],"question_id":763,"answer_images":[],"topic":"1","timestamp":"2024-02-05 22:11:00","isMC":true,"answer_ET":"A","exam_id":31,"answer_description":""},{"id":"8FJ50WVcFDNuAa279KbZ","exam_id":31,"discussion":[{"comment_id":"1144811","timestamp":"1707417300.0","poster":"hajra313","content":"option b bcz option c is WS AppSync is not the most appropriate solution for file processing.\noption d While Amazon Simple Notification Service (SNS) can be used to trigger actions based on S3 events, it's not directly involved in processing files .option c :Kinesis is typically used for real-time data streaming and analytics, which may not be needed for simple file processing tasks such as extracting metadata.","upvote_count":"5"},{"upvote_count":"5","timestamp":"1707326160.0","comment_id":"1143566","content":"Selected Answer: B\nB seems to be make most sense to me.","poster":"mestule"},{"poster":"Lin878","comment_id":"1239311","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/EventNotifications.html","timestamp":"1719669540.0","upvote_count":"3"},{"upvote_count":"2","timestamp":"1719240420.0","content":"Selected Answer: B\nAnswerB","poster":"Scheldon","comment_id":"1236397"},{"timestamp":"1711141260.0","upvote_count":"3","poster":"alawada","content":"Selected Answer: B\nThe problem with C is how it sends the data to S3, if it was Firehose it would make sense. I waka for B.","comment_id":"1180325"},{"poster":"MattBJ","timestamp":"1710390420.0","content":"Selected Answer: B\nB is correct. The most cost effective option.","comment_id":"1173100","upvote_count":"3"},{"content":"option B","poster":"jaswantn","timestamp":"1708133640.0","upvote_count":"2","comment_id":"1152321"},{"upvote_count":"2","timestamp":"1707347280.0","content":"Option D","comment_id":"1143893","poster":"kempes"},{"content":"Option D","comment_id":"1141814","upvote_count":"1","timestamp":"1707199020.0","poster":"Andy_09"}],"answer_description":"","choices":{"C":"Configure Amazon Kinesis Data Streams to process and send data to Amazon S3. Invoke an AWS Lambda function to process the files.","A":"Configure AWS CloudTrail trails to log S3 API calls. Use AWS AppSync to process the files.","D":"Configure an Amazon Simple Notification Service (Amazon SNS) topic to process the files uploaded to Amazon S3. Invoke an AWS Lambda function to process the files.","B":"Configure an object-created event notification within the S3 bucket to invoke an AWS Lambda function to process the files."},"question_id":764,"answer_ET":"B","timestamp":"2024-02-06 06:57:00","question_images":[],"answer":"B","unix_timestamp":1707199020,"isMC":true,"answer_images":[],"topic":"1","question_text":"A company hosts an application used to upload files to an Amazon S3 bucket. Once uploaded, the files are processed to extract metadata, which takes less than 5 seconds. The volume and frequency of the uploads varies from a few files each hour to hundreds of concurrent uploads. The company has asked a solutions architect to design a cost-effective architecture that will meet these requirements.\n\nWhat should the solutions architect recommend?","url":"https://www.examtopics.com/discussions/amazon/view/132997-exam-aws-certified-solutions-architect-associate-saa-c03/","answers_community":["B (100%)"]},{"id":"XzTJOYY3KxEdiP0shrDG","question_id":765,"answer_description":"","answer_ET":"C","unix_timestamp":1666024680,"isMC":true,"discussion":[{"poster":"123jhl0","content":"Selected Answer: C\n(A) - You don't need to deploy an EC2 instance to host an API - Operational overhead\n(B) - Same as A\n(**C**) - Is the answer\n(D) - AWS Glue gets data from S3, not from API GW. AWS Glue could do ETL by itself, so don't need lambda. Non sense. https://aws.amazon.com/glue/","comment_id":"697556","comments":[{"poster":"Futurebones","comment_id":"895383","timestamp":"1683834960.0","comments":[{"timestamp":"1696311900.0","comment_id":"1023631","upvote_count":"9","content":"It is because they assume that Kinesis Data Firehose built-in transformations are not enough. So you have to use specific lambda transformation. Please refer to this link : https://aws.amazon.com/kinesis/data-firehose/#:~:text=Amazon%20Kinesis%20Data%20Firehose%20is,data%20stores%2C%20and%20analytics%20services.","poster":"Remy_d"}],"content":"I don''t understand is why we should use Lambda in between to transform data. To me, Kinesis data firehose is enough as it is an extract, transform, and load (ETL) service.","upvote_count":"5"}],"timestamp":"1666024680.0","upvote_count":"44"},{"content":"Selected Answer: C\nThe company needs an API = Amazon API Gateway API \nA real-time data ingestion = Amazon Kinesis data stream\nA process that transforms data = AWS Lambda functions\nKinesis Data Firehose delivery stream to send the data to Amazon S3\nA storage solution for the data = Amazon S3","upvote_count":"30","poster":"TariqKipkemei","timestamp":"1692075300.0","comment_id":"981294"},{"timestamp":"1726398360.0","content":"Selected Answer: C\nAns C - the API is given so just configure Kinesis stream/Firehouse to use it","comment_id":"1284071","upvote_count":"1","poster":"PaulGa"},{"poster":"awsgeek75","timestamp":"1705255260.0","upvote_count":"3","comment_id":"1122741","content":"Selected Answer: C\nC is least operational overhead\n\nA: EC2 is overhead in this scenario\nB: Same as A\nD: Glue is not real time data streaming"},{"content":"Selected Answer: C\nIt looks overengineered, but as it works, let's go for the C","poster":"Mikado211","upvote_count":"5","comment_id":"1086863","timestamp":"1701613920.0"},{"timestamp":"1698462480.0","comment_id":"1055954","upvote_count":"4","content":"The company needs an API = Amazon API Gateway API\nA real-time data ingestion = Amazon Kinesis data stream\nA process that transforms data = AWS Lambda functions\nKinesis Data Firehose delivery stream to send the data to Amazon S3\nA storage solution for the data = Amazon S3","poster":"Ruffyit"},{"comment_id":"1042685","poster":"peekingpicker","upvote_count":"1","timestamp":"1697201940.0","content":"Selected Answer: D\n\"a real-time data ingestion\"\nisn't firehose not realtime ? Kinesis FireHose is \"Near\" Real-time . It has 60 seconds gap. I think it should be D","comments":[{"comment_id":"1047263","timestamp":"1697667240.0","poster":"rlamberti","upvote_count":"5","content":"The real-time part (data ingestion) will be performed by Kinesis Data Stream and API Gateway. After this, the transformation and storage of the data don't need to be in real-time, since it was already ingested, so Kinesis Firehose + Lambda is perfect. C makes sense to me."}]},{"poster":"Guru4Cloud","content":"Selected Answer: C\nOption C provides the least operational overhead to meet the requirements:\n\nAPI Gateway provides the API\nKinesis Data Streams ingests the real-time data\nLambda functions transform the data\nFirehose delivers the data to S3 storage\nThe key advantages are:\n\nServerless architecture requires minimal operational overhead\nFully managed ingestion, processing and storage services\nNo need to manage EC2 instances","upvote_count":"2","comment_id":"978014","timestamp":"1691694840.0"},{"upvote_count":"3","comment_id":"951274","poster":"diabloexodia","timestamp":"1689314160.0","content":"Requirements:\nAPI- API gateway\nReal time data ingestion - AWS Kinesis data stream\nETL(Extract Transform Load) - Kinesis Firehose\nStorage- S3"},{"content":"Selected Answer: C\nC - is the answer","timestamp":"1688220900.0","poster":"tamefi5512","comment_id":"940041","upvote_count":"1"},{"timestamp":"1687364880.0","comment_id":"929728","content":"Selected Answer: C\nC. By leveraging these services together, you can achieve a real-time data ingestion architecture with minimal operational overhead. The data flows from the API Gateway to the Kinesis data stream, undergoes transformations with Lambda, and is then sent to S3 via the Kinesis Data Firehose delivery stream for storage.\n\nA. This adds operational overhead as you need to handle EC2 management, scaling, and maintenance. It is less efficient compared to using a serverless solution like API Gateway.\n\nB. It requires deploying and managing an EC2 to host the API and configuring Glue. This adds operational overhead, including EC2 management and potential scalability limitations.\n\nD. It still requires managing and configuring Glue, which adds operational overhead. Additionally, it may not be the most efficient solution as Glue is primarily used for ETL scenarios, and in this case, real-time data transformation is required.","upvote_count":"2","poster":"cookieMr"},{"upvote_count":"2","timestamp":"1684564500.0","content":"Selected Answer: D\nI am gonna choose D for this.\nKinesis Data Stream + Data Firehose will adds up to the operational overhead, plus it is \"Near real-time\", not a real time solution.\nLambda functions scale automatically, so no management of scaling/compute resources is needed.\nAWS Glue handles the data storage in S3, so no management of that is needed.","poster":"winzzhhzzhh","comment_id":"902389"},{"upvote_count":"2","timestamp":"1679043420.0","content":"Gotta love all those chatgpt answers y'all are throwing at us.\n\nKinesis Firehose is NEAR real-time, not real-time like your bots tell you.","poster":"UnluckyDucky","comment_id":"841817","comments":[{"comment_id":"1105232","upvote_count":"2","content":"Stem is about \"real-time data INGESTION\", not real-time processing.","poster":"pentium75","timestamp":"1703504220.0"}]},{"comment_id":"783885","timestamp":"1674353820.0","content":"Selected Answer: C\noption C is the best solution. It uses Amazon Kinesis Data Firehose which is a fully managed service for delivering real-time streaming data to destinations such as Amazon S3. This service requires less operational overhead as compared to option A, B, and D. Additionally, it also uses Amazon API Gateway which is a fully managed service for creating, deploying, and managing APIs. These services help in reducing the operational overhead and automating the data ingestion process.","poster":"bullrem","upvote_count":"1"},{"comments":[{"upvote_count":"2","comments":[{"poster":"Buruguduystunstugudunstuy","content":"Option A is incorrect because it involves deploying an EC2 instance to host an API, which adds operational overhead in the form of maintaining and securing the instance.\n\nOption B is incorrect because it involves deploying an EC2 instance to host an API and disabling source/destination checking on the instance. Disabling source/destination checking can make the instance vulnerable to attacks, which adds operational overhead in the form of securing the instance.","comments":[{"content":"Option D is incorrect because it involves using AWS Glue to send the data to Amazon S3, which adds operational overhead in the form of maintaining and securing the AWS Glue infrastructure.\n\nOverall, Option C is the best choice because it uses fully managed services for the API, data transformation, and data delivery, which minimizes operational overhead.","timestamp":"1671590760.0","upvote_count":"2","poster":"Buruguduystunstugudunstuy","comment_id":"751807"}],"upvote_count":"2","comment_id":"751806","timestamp":"1671590760.0"}],"content":"Finally, Option C uses AWS Lambda, which is a fully managed service for running code in response to events. With AWS Lambda, you don't have to worry about the operational overhead of setting up and maintaining a server to run the data transformation code.\n\nOverall, Option C provides a fully managed solution for real-time data ingestion with minimal operational overhead.","comment_id":"751805","poster":"Buruguduystunstugudunstuy","timestamp":"1671590700.0"}],"timestamp":"1671597540.0","content":"Selected Answer: C\nOption C is the solution that meets the requirements with the least operational overhead.\n\nIn Option C, you can use Amazon API Gateway as a fully managed service to create, publish, maintain, monitor, and secure APIs. This means that you don't have to worry about the operational overhead of deploying and maintaining an EC2 instance to host the API.\n\nOption C also uses Amazon Kinesis Data Firehose, which is a fully managed service for delivering real-time streaming data to destinations such as Amazon S3. With Kinesis Data Firehose, you don't have to worry about the operational overhead of setting up and maintaining a data ingestion infrastructure.","poster":"Buruguduystunstugudunstuy","comment_id":"751804","upvote_count":"1"},{"content":"Selected Answer: C\nOption C","poster":"career360guru","upvote_count":"1","timestamp":"1671415140.0","comment_id":"749359"},{"upvote_count":"1","comment_id":"747699","content":"Selected Answer: C\nOption C","timestamp":"1671237180.0","poster":"career360guru"},{"upvote_count":"1","comment_id":"723663","poster":"Wpcorgan","timestamp":"1669042860.0","content":"C is correct"},{"timestamp":"1666710540.0","content":"Selected Answer: C\nC is correct answer","poster":"Cristian93","comment_id":"703977","upvote_count":"2"}],"timestamp":"2022-10-17 18:38:00","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/85740-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_images":[],"question_images":[],"answers_community":["C (97%)","3%"],"exam_id":31,"choices":{"A":"Deploy an Amazon EC2 instance to host an API that sends data to an Amazon Kinesis data stream. Create an Amazon Kinesis Data Firehose delivery stream that uses the Kinesis data stream as a data source. Use AWS Lambda functions to transform the data. Use the Kinesis Data Firehose delivery stream to send the data to Amazon S3.","B":"Deploy an Amazon EC2 instance to host an API that sends data to AWS Glue. Stop source/destination checking on the EC2 instance. Use AWS Glue to transform the data and to send the data to Amazon S3.","C":"Configure an Amazon API Gateway API to send data to an Amazon Kinesis data stream. Create an Amazon Kinesis Data Firehose delivery stream that uses the Kinesis data stream as a data source. Use AWS Lambda functions to transform the data. Use the Kinesis Data Firehose delivery stream to send the data to Amazon S3.","D":"Configure an Amazon API Gateway API to send data to AWS Glue. Use AWS Lambda functions to transform the data. Use AWS Glue to send the data to Amazon S3."},"answer":"C","question_text":"A company needs to configure a real-time data ingestion architecture for its application. The company needs an API, a process that transforms data as the data is streamed, and a storage solution for the data.\nWhich solution will meet these requirements with the LEAST operational overhead?"}],"exam":{"isMCOnly":true,"isBeta":false,"id":31,"lastUpdated":"11 Apr 2025","numberOfQuestions":1019,"name":"AWS Certified Solutions Architect - Associate SAA-C03","isImplemented":true,"provider":"Amazon"},"currentPage":153},"__N_SSP":true}