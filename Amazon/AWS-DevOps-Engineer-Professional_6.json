{"pageProps":{"questions":[{"id":"zuUjH58xQXKHhfVRvtA2","timestamp":"2022-12-23 14:58:00","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/92580-exam-aws-devops-engineer-professional-topic-1-question-121/","answer_description":"","answer":"B","discussion":[{"poster":"saggy4","upvote_count":"5","comment_id":"759899","timestamp":"1672235400.0","content":"Selected Answer: B\nB is the correct answer here is the link to the docs explaining just that https://aws.amazon.com/blogs/devops/using-codedeploy-environment-variables/"},{"timestamp":"1677982200.0","comment_id":"829549","content":"Selected Answer: B\nCheck the 2nd example in this doc - https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html","poster":"bgc1","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: B\nWhy is C suggested as \"correct answer\" ?","poster":"Piccaso","timestamp":"1676452800.0","comment_id":"809308"},{"content":"Selected Answer: B\nhttps://aws.amazon.com/blogs/devops/using-codedeploy-environment-variables/","poster":"Bulti","upvote_count":"3","comment_id":"787096","timestamp":"1674604800.0"},{"timestamp":"1672069980.0","comment_id":"757618","upvote_count":"1","content":"Selected Answer: B\nB the correct one","poster":"tartealacreme"},{"upvote_count":"2","content":"B - Correct","timestamp":"1671803880.0","poster":"Imstack","comment_id":"754271"}],"answer_ET":"B","question_text":"A company is adopting AWS CodeDeploy to automate its application deployments for a Java-Apache Tomcat application with an Apache webserver The development team started with a proof of concept, created a deployment group for a developer environment, and performed functional tests within the application After completion, the team will create additional deployment groups for staging and production.\n\nThe current log level is configured within the Apache settings, but the team wants to change this configuration dynamically when the deployment occurs, so that they can set different log level configurations depending on the deployment group without having a different application revision for each group.\n\nHow can these requirements be met with the LEAST management overhead and without requiring different script versions for each deployment group?","answers_community":["B (100%)"],"topic":"1","isMC":true,"exam_id":35,"question_id":26,"choices":{"C":"Create a CodeDeploy custom environment variable for each environment Then place a script into the application revision that checks this environment variable to identify which deployment group the instance is part of. Use this information to configure the log level settings. Reference this script as part of the ValidateService lifecycle hook in the appspec.yml file.","D":"Create a script that uses the CodeDeploy environment variable DEPLOYMENT_GROUP_ID to identify which deployment group the instance is part of to configure the log level settings. Reference this script as part of the Install lifecycle hook in the appspec.yml file.","A":"Tag the Amazon EC2 instances depending on the deployment group. Then place a script into the application revision that calls the metadata service and the EC2 API to identify which deployment group the instance is part of. Use this information to configure the log level settings. Reference the script as part of the Afterinstall lifecycle hook in the appspec.yml file.","B":"Create a script that uses the CodeDeploy environment variable DEPLOYMENT_GROUP_NAME to identify which deployment group the instance is part of. Use this information to configure the log level settings. Reference this script as part of the Beforelnstall lifecycle hook in the appspec.yml file."},"unix_timestamp":1671803880,"answer_images":[]},{"id":"O0M0HNBZwalfnAfTyXOk","question_text":"A company is running a custom-built application that processes records. All the components run on Amazon EC2 instances that run in an Auto Scaling group. Each record's processing is a multistep sequential action that is compute-intensive. Each step is always completed in 5 minutes or less.\n\nA limitation of the current system is that if any steps fail, the application has to reprocess the record from the beginning. The company wants to update the architecture so that the application must reprocess only the failed steps.\n\nWhat is the MOST operationally efficient solution that meets these requirements?","isMC":true,"exam_id":35,"answer_images":[],"topic":"1","discussion":[{"comment_id":"857367","poster":"easytoo","timestamp":"1680288840.0","upvote_count":"1","content":"its D baby."},{"upvote_count":"1","comment_id":"804429","poster":"Piccaso","content":"Selected Answer: D\nA, haha ...\nB, I think I tried this method, headache ....\nC, haha","timestamp":"1676039760.0"},{"timestamp":"1674604980.0","comment_id":"787097","poster":"Bulti","content":"Selected Answer: D\nD is the right answer","upvote_count":"1"},{"timestamp":"1671804060.0","poster":"Imstack","comment_id":"754274","upvote_count":"4","content":"D DDDDDDD"}],"url":"https://www.examtopics.com/discussions/amazon/view/92581-exam-aws-devops-engineer-professional-topic-1-question-122/","answer_ET":"D","answer_description":"","question_id":27,"answer":"D","question_images":[],"answers_community":["D (100%)"],"choices":{"C":"Create a web application to pass records to an Amazon Kinesis data stream. Decouple the processing by using the Kinesis data stream and AWS Lambda functions.","B":"Perform the processing steps by using logic in the application. Convert the application code to run in a container. Use AWS Fargate to manage the container instances. Configure the container to invoke itself to pass the state from one step to the next.","D":"Create a web application to pass records to AWS Step Functions. Decouple the processing into Step Functions tasks and AWS Lambda functions.","A":"Create a web application to write records to Amazon S3. Use S3 Event Notifications to publish to an Amazon Simple Notification Service (Amazon SNS) topic. Use an EC2 instance to poll Amazon SNS and start processing. Save intermediate results to Amazon S3 to pass on to the next step."},"unix_timestamp":1671804060,"timestamp":"2022-12-23 15:01:00"},{"id":"k6Lbum7jQ2Gi4w2SjQPG","topic":"1","answer":"CD","url":"https://www.examtopics.com/discussions/amazon/view/92419-exam-aws-devops-engineer-professional-topic-1-question-123/","question_images":[],"timestamp":"2022-12-22 08:31:00","choices":{"D":"Use AWS Backup to take a snapshot of the DB instance every hour and to copy the snapshot to the failover Region.","C":"Upon failover, launch the AWS CloudFormation template in the failover Region with the DB snapshot ID as an input parameter. When the stack creation is complete, change the DNS records to point to the failover Region's ALB.","E":"Create an Amazon EventBridge (Amazon CloudWatch Events) event that invokes an AWS Lambda function to copy the RDS automated snapshot to the failover Region.","B":"Upon failover, update the AWS CloudFormation stack in the failover Region to increase the desired number of instances in the Auto Scaling group. When the stack update is complete, change the DNS records to point to the failover Region's ALB.","A":"Launch an RDS DB instance in the failover Region. Use AWS Database Migration Service (AWS DMS) to configure ongoing replication from the source database."},"answer_images":[],"answer_ET":"CD","unix_timestamp":1671694260,"exam_id":35,"question_text":"A DevOps engineer is designing a multi-Region disaster recovery strategy for an application. The application requires an RPO of 1 hour and requires an RTO of 4 hours. The application is deployed with an AWS CloudFormation template that creates an Application Load Balancer (ALB), Amazon EC2 instances in an Auto Scaling group and an Amazon RDS Multi-AZ DB instance with 20 GB of allocated storage. The AMI of the application instance does not contain data and has been copied to the destination Region.\n\nWhich combination of actions will meet the recovery objectives at the LOWEST cost? (Choose two.)","discussion":[{"timestamp":"1672236960.0","comment_id":"759920","upvote_count":"5","poster":"saggy4","content":"Selected Answer: CD\nAnswer is C and D.\nB is wrong because from the question there is just AMI in the disaster region, there is no Autoscaling group for which we can increase the count."},{"content":"It is often cheaper to set up and autoreplicate a DB and leave it running than to create it failure-time. Also, copying snapshots interregionally every hour is not cheap. And restoring them can take more than 4 hours.\n\nTherefore: AB.","comment_id":"1048824","upvote_count":"1","timestamp":"1697808240.0","poster":"Dgix"},{"content":"C,D baby.","comment_id":"853901","upvote_count":"1","timestamp":"1680055320.0","poster":"easytoo"},{"upvote_count":"1","timestamp":"1677982680.0","poster":"bgc1","content":"Selected Answer: CD\nCD combination is the only one that fulfills RPO of 1 hour at lowest cost.","comment_id":"829561"},{"content":"Selected Answer: CD\nD vs E\nWe need an RPO of 1 hour, we need backups younger or equal to 1 hour!\n\n\"E. Create an Amazon EventBridge (Amazon CloudWatch Events) event that invokes an AWS Lambda function to copy the RDS automated snapshot to the failover Region.\"\n-> The CW Event should be a scheduled one. In addition, by default, this automatic snapshot occurs only once each day during a 30-minute backup window. We cannot guarantee the RPO.\n\nTherefore D is the correct one.","comment_id":"824990","upvote_count":"2","poster":"SHoKMaSTeR","timestamp":"1677599640.0"},{"timestamp":"1676041320.0","comment_id":"804453","comments":[{"comment_id":"810657","content":"Sorry for the above ....\nB is not correct, because it does not recover the dataset.\nI changed my mind from \"BD\" to \"CD\".","poster":"Piccaso","timestamp":"1676551800.0","upvote_count":"1"}],"content":"Selected Answer: BD\nA, does not match the requirement of \"LOWEST cost\".\nC and E ..... do not make sense.","upvote_count":"2","poster":"Piccaso"},{"content":"C and D is correct. The same CloudFormation template should run upon failover to provision the same resources as in the primary with the exception that the DB storage is restored from the AWS backup taken every hour using the DB Snapshot ID.","timestamp":"1674605640.0","poster":"Bulti","upvote_count":"1","comment_id":"787102"},{"upvote_count":"4","poster":"obaf1","comment_id":"753083","timestamp":"1671697260.0","content":"Answer: C,D\nB is wrong because it's an Auto Scaling Group, therefore there's no need to manually increase number of instances in the failover region."},{"comment_id":"753056","content":"Answer: C,D\nB is wring because it's an Auto Scaling Group and there's no need to manually increase number of instances in the failover region","timestamp":"1671694260.0","upvote_count":"3","poster":"obaf1"}],"answer_description":"","question_id":28,"answers_community":["CD (80%)","BD (20%)"],"isMC":true},{"id":"4D4hUM6aGcM54d1BQCIn","answer_description":"","answer_ET":"D","topic":"1","question_id":29,"discussion":[{"timestamp":"1676042100.0","upvote_count":"2","comment_id":"804476","content":"Selected Answer: D\n\"Immediately terminate\" is not a good practice found in A B C","poster":"Piccaso"},{"content":"Selected Answer: D\nRequirement: with the least amount of downtime\nB - wrong - this is not how you do it with the beanstalk - you also don't have to change cname in this case (single environment) - when you perform this type of update the app can be unavailable for some time\nD - correct","upvote_count":"3","timestamp":"1675537680.0","comment_id":"798277","poster":"DerekKey"},{"timestamp":"1674609180.0","upvote_count":"3","comment_id":"787137","comments":[{"comment_id":"795677","upvote_count":"1","poster":"Bulti","content":"B is correct as it clearly indicates how 2 environments will be provisioned and application deployed using EB. D on the other hand doesn't mention anything about app deployment, only about creating the environment using CloudFormation. Cloudformation cannot deploy an application.","timestamp":"1675302960.0"}],"poster":"Bulti","content":"Selected Answer: D\nD is correct because as mentioned in the question there is already a CloudFormation template to provision the resources so why switch to EB. But I have to admit that this question is tricky. The answer could be B as well."},{"timestamp":"1674455820.0","content":"Option A - use A record, which is used with IP addresses\nOption C - OpsWorks - not needed\nOption B - with EB you use CNAME to swap between TWO environments\nOption D - fits well","comment_id":"785043","poster":"kopper2019","upvote_count":"2"},{"timestamp":"1673896920.0","content":"Selected Answer: D\nhttps://www.examtopics.com/discussions/amazon/view/8099-exam-aws-devops-engineer-professional-topic-1-question-94/","upvote_count":"2","comment_id":"778146","poster":"Oleg_gol"},{"comments":[{"content":"Why not Z ?","poster":"Piccaso","comment_id":"814270","upvote_count":"1","timestamp":"1676824920.0"}],"timestamp":"1672339260.0","upvote_count":"2","comment_id":"761372","poster":"Kapello10","content":"Selected Answer: B\nB is the correct answer"},{"comment_id":"759931","content":"Selected Answer: D\nD is correct\nElastic Beanstalk will be a much of a hassle as we will need to move the existing system in EC2 hence B and C are wrong\nBetween A and D, A tries deletes the older env immediately and does not change the R53 entries","timestamp":"1672237500.0","poster":"saggy4","upvote_count":"4"}],"url":"https://www.examtopics.com/discussions/amazon/view/93073-exam-aws-devops-engineer-professional-topic-1-question-124/","answers_community":["D (88%)","13%"],"isMC":true,"choices":{"C":"Use a single AWS Elastic Beanstalk environment and an AWS OpsWorks environment to deploy the staging and production environments. Update the environment by uploading the ZIP file with the new application code into the Elastic Beanstalk environment deployed with the OpsWorks stack. Validate the traffic in the new environment and immediately terminate the old environment if tests are successful.","A":"Use CloudFormation to deploy an additional staging environment and configure the Route 53 DNS with weighted records. During cutover change the Route 53 A record weights to achieve an even traffic distribution between the two environments. Validate the traffic in the new environment and immediately terminate the old environment if tests are successful.","D":"Use AWS CloudFormation to deploy an additional staging environment, and configure the Route 53 DNS with weighted records. During cutover, increase the weight distribution to have more traffic directed to the new staging environment as workloads are successfully validated. Keep the old production environment in place until the new staging environment handles all traffic.","B":"Use a single AWS Elastic Beanstalk environment to deploy the staging and production environments. Update the environment by uploading the ZIP file with the new application code. Swap the Elastic Beanstalk environment CNAME. Validate the traffic in the new environment and immediately terminate the old environment if tests are successful."},"exam_id":35,"answer_images":[],"answer":"D","question_images":[],"question_text":"A company runs a three-tier web application in its production environment, which is built on a single AWS CloudFormation template made up of Amazon EC2 instances behind an ELB Application Load Balancer. The instances run in an EC2 Auto Scaling group across multiple Availability Zones. Data is stored in an Amazon RDS Multi-AZ DB instance with read replicas. Amazon Route 53 manages the application's public DNS record.\n\nA DevOps engineer must create a workflow to mitigate a failed software deployment by rolling back changes in the production environment when a software cutover occurs for new application software.\n\nWhat steps should the engineer perform to meet these requirements with the LEAST amount of downtime?","timestamp":"2022-12-28 15:25:00","unix_timestamp":1672237500},{"id":"tLWedvtoCvf6NEpJbULh","timestamp":"2022-12-22 07:40:00","url":"https://www.examtopics.com/discussions/amazon/view/92414-exam-aws-devops-engineer-professional-topic-1-question-125/","question_images":[],"answer_description":"","answer":"B","discussion":[{"timestamp":"1679965920.0","poster":"easytoo","comments":[{"upvote_count":"1","poster":"Lalo","comment_id":"1024933","timestamp":"1696432620.0","content":"Correct IS BBBBBB:\nFor Amazon S3 buckets, you must delete all objects in the bucket for deletion to succeed.\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html"}],"comment_id":"852633","content":"I'm going for A:\nOption A uses the built-in CloudFormation functionality of DeletionPolicy to ensure that the S3 bucket is removed when the CloudFormation stack is deleted. This is a simple and well-documented feature that is widely used in CloudFormation templates.","upvote_count":"2"},{"poster":"Bulti","comment_id":"787138","content":"Selected Answer: B\nB is the right answer.","timestamp":"1674609300.0","upvote_count":"1"},{"content":"Selected Answer: B\nThe Correct Option is B","comment_id":"759936","timestamp":"1672237620.0","poster":"saggy4","upvote_count":"2"},{"content":"BBBBBBBBB","timestamp":"1671804960.0","comment_id":"754280","poster":"Imstack","upvote_count":"2"},{"upvote_count":"1","content":"It's B. You can create a lambda function to clean up your bucket and invoke your lambda from your CloudFormation stack using a CustomResource.","comment_id":"753027","timestamp":"1671691200.0","poster":"SuriSagar"}],"answer_ET":"B","question_text":"An IT team has built an AWS CloudFormation template so others in the company can quickly and reliably deploy and terminate an application. The template creates an Amazon EC2 instance with a user data script to install the application and an Amazon S3 bucket that the application uses to serve static webpages while it is running.\n\nAll resources should be removed when the CloudFormation stack is deleted. However, the team observes that CloudFormation reports an error during stack deletion, and the S3 bucket created by the stack is not deleted.\n\nHow can the team resolve the error in the MOST efficient manner to ensure that all resources are deleted without errors?","answers_community":["B (100%)"],"topic":"1","isMC":true,"exam_id":35,"question_id":30,"choices":{"D":"Replace the EC2 and S3 bucket resources with a single AWS OpsWorks Stacks resource. Define a custom recipe for the stack to create and delete the EC2 instance and the S3 bucket.","B":"Add a custom resource with an AWS Lambda function with the DependsOn attribute specifying the S3 bucket, and an IAM role. Write the Lambda function to delete all objects from the bucket when RequestType is Delete.","C":"Identify the resource that was not deleted. From the S3 console, empty the S3 bucket and then delete it.","A":"Add a DeletionPolicy attribute to the S3 bucket resource, with the value Delete forcing the bucket to be removed when the stack is deleted."},"unix_timestamp":1671691200,"answer_images":[]}],"exam":{"provider":"Amazon","name":"AWS DevOps Engineer Professional","isBeta":false,"id":35,"lastUpdated":"11 Apr 2025","numberOfQuestions":208,"isImplemented":true,"isMCOnly":false},"currentPage":6},"__N_SSP":true}