{"pageProps":{"questions":[{"id":"b2NJTxWfjRRoyUJfycfq","isMC":true,"exam_id":33,"topic":"1","unix_timestamp":1707150720,"answer_description":"","question_images":[],"answer":"C","question_id":351,"answer_ET":"C","choices":{"A":"Create an AWS Lambda function that can validate the serial number. Create an AWS IoT Core provisioning template. Include the SerialNumber parameter in the Parameters section. Add the Lambda function as a pre-provisioning hook. During manufacturing, call the RegisterThing API operation and specify the template and parameters.","C":"Create an AWS Lambda function that can validate the serial number. Create an AWS IoT Core provisioning template. Include the SerialNumber parameter in the Parameters section. Add the Lambda function as a pre-provisioning hook. Register the CA with AWS IoT Core, specify the provisioning template, and set the allow-auto-registration parameter.","B":"Create an AWS Step Functions state machine that can validate the serial number. Create an AWS IoT Core provisioning template. Include the SerialNumber parameter in the Parameters section. Specify the Step Functions state machine to validate parameters. Call the StartThingRegistrationTask API operation during installation.","D":"Create an AWS IoT Core provisioning template. Include the SerialNumber parameter in the Parameters section. Include parameter validation in the template. Provision a claim certificate and a private key for each device that uses the CA. Grant AWS IoT Core service permissions to update AWS IoT things during provisioning."},"url":"https://www.examtopics.com/discussions/amazon/view/132878-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_images":[],"discussion":[{"timestamp":"1707191160.0","comment_id":"1141734","poster":"kejam","content":"Selected Answer: C\nIn addition to validating the bootstrap certificate presented by devices, Fleet Provisioning also provides Lambda-based provisioning hooks that enable appropriate validation for pertinent device attributes. Examples of device attributes could include a serial number ...\n\nhttps://aws.amazon.com/blogs/iot/how-to-automate-onboarding-of-iot-devices-to-aws-iot-core-at-scale-with-fleet-provisioning/","upvote_count":"8"},{"timestamp":"1732491300.0","comment_id":"1317245","poster":"TomTom","content":"Selected Answer: A\nOption A, meet the requirements. Why Not C, because C mentioned as Auto Provisioing, while the requirements is to have control.","upvote_count":"1"},{"timestamp":"1713553260.0","comment_id":"1198815","upvote_count":"1","poster":"titi_r","content":"Selected Answer: C\nCorrect ans - C."},{"poster":"career360guru","content":"Selected Answer: C\nOption C","comment_id":"1169324","upvote_count":"1","timestamp":"1709970480.0"},{"comment_id":"1162157","upvote_count":"1","content":"https://docs.aws.amazon.com/iot/latest/developerguide/iot-provision.html","poster":"duriselvan","timestamp":"1709175240.0"},{"timestamp":"1709175180.0","poster":"duriselvan","comment_id":"1162156","content":"AWS provides several different ways to provision a device and install unique client certificates on it. This section describes each way and how to select the best one for your IoT solution. These options are described in detail in the whitepaper titled Device Manufacturing and Provisioning with X.509 Certificates in AWS IoT Core.","upvote_count":"1"},{"comment_id":"1162155","content":"cANSGiven the requirements, Option C is the most suitable solution:\n\nIt combines serial number validation using a Lambda function.\nThe pre-provisioning hook ensures validation before registration.\nThe allow-auto-registration parameter provides fine-grained control over auto-registration.","timestamp":"1709175060.0","poster":"duriselvan","upvote_count":"3"},{"timestamp":"1707839880.0","content":"Selected Answer: D\nDevices can be manufactured with a provisioning claim certificate and private key (which are special purpose credentials) embedded in them. If these certificates are registered with AWS IoT, the service can exchange them for unique device certificates that the device can use for regular operations\nhttps://docs.aws.amazon.com/iot/latest/developerguide/provision-wo-cert.html#claim-based","comment_id":"1149333","poster":"ele","upvote_count":"2"},{"poster":"duriselvan","content":"C is ans","upvote_count":"1","timestamp":"1707669000.0","comment_id":"1147501"},{"upvote_count":"2","content":"Correct Answer is D","timestamp":"1707150720.0","comment_id":"1141281","poster":"alexis123456"}],"question_text":"A retail company is mounting IoT sensors in all of its stores worldwide. During the manufacturing of each sensor, the company’s private certificate authority (CA) issues an X.509 certificate that contains a unique serial number. The company then deploys each certificate to its respective sensor.\n\nA solutions architect needs to give the sensors the ability to send data to AWS after they are installed. Sensors must not be able to send data to AWS until they are installed.\n\nWhich solution will meet these requirements?","answers_community":["C (77%)","D (15%)","8%"],"timestamp":"2024-02-05 17:32:00"},{"id":"hUrG66fN6HPcRUErX9e9","url":"https://www.examtopics.com/discussions/amazon/view/132879-exam-aws-certified-solutions-architect-professional-sap-c02/","unix_timestamp":1707151440,"question_images":[],"discussion":[{"content":"Selected Answer: B\nThey use Jenkins. X-Ray is for debugging not unit testing. Seamless deploys and rollbacks mean blue/green deployments. That leaves Answer B:\n\nhttps://aws.amazon.com/blogs/devops/setting-up-a-ci-cd-pipeline-by-integrating-jenkins-with-aws-codebuild-and-aws-codedeploy/","poster":"kejam","comment_id":"1141744","upvote_count":"5","timestamp":"1707192360.0"},{"upvote_count":"2","poster":"AzureDP900","timestamp":"1731365340.0","comment_id":"1310361","content":"The correct answer is B.\nThis solution meets the requirements:\nReceive notifications for bad builds: Using GitHub webhooks to trigger the CodePipeline pipeline ensures that build failures are detected and notified via Amazon SNS topic.\nZero downtime during deployments: Using a blue/green deployment with AWS CodeDeploy allows for zero downtime deployments, as changes are made to the new environment while the old one remains available.\nSeamless rollbacks in case of major issues: The blue/green deployment architecture enables seamless rollbacks by simply switching traffic back to the previous version."},{"poster":"career360guru","comment_id":"1169327","content":"Selected Answer: B\nOption B","upvote_count":"1","timestamp":"1709970780.0"},{"content":"Selected Answer: B\nB, no-brainer","poster":"ele","upvote_count":"1","comment_id":"1149341","timestamp":"1707840300.0"},{"comment_id":"1147208","content":"Selected Answer: B\nAnswer is B","timestamp":"1707654000.0","poster":"aabdlmouna","upvote_count":"1"},{"comment_id":"1142332","timestamp":"1707234480.0","poster":"master9","content":"Selected Answer: B\nAWS CodeBuild can be used to conduct unit testing. CodeBuild is a managed service that compiles your source code, runs tests, and produces deployable application artifacts. You can create reports in CodeBuild that contain details about tests that are run during builds. These tests can include unit tests, configuration tests, and functional tests","upvote_count":"2"},{"poster":"onlyvimal2103","upvote_count":"1","comment_id":"1142092","content":"Correct Answer B \nhttps://aws.amazon.com/about-aws/whats-new/2018/05/aws-codepipeline-supports-push-events-from-github-via-webhooks/\nhttps://docs.aws.amazon.com/codebuild/latest/userguide/jenkins-plugin.html","timestamp":"1707219240.0"},{"timestamp":"1707151440.0","upvote_count":"1","comment_id":"1141283","poster":"alexis123456","content":"Correct Answer is C"}],"topic":"1","choices":{"C":"Use GitHub websockets to trigger the CodePipeline pipeline. Use AWS X-Ray for unit testing and static code analysis. Send alerts to an Amazon SNS topic for any bad builds. Deploy in a blue/green deployment using AWS CodeDeploy.","A":"Use GitHub websockets to trigger the CodePipeline pipeline. Use the Jenkins plugin for AWS CodeBuild to conduct unit testing. Send alerts to an Amazon SNS topic for any bad builds. Deploy in an in-place, all-at-once deployment configuration using AWS CodeDeploy.","B":"Use GitHub webhooks to trigger the CodePipeline pipeline. Use the Jenkins plugin for AWS CodeBuild to conduct unit testing. Send alerts to an Amazon SNS topic for any bad builds. Deploy in a blue/green deployment using AWS CodeDeploy.","D":"Use GitHub webhooks to trigger the CodePipeline pipeline. Use AWS X-Ray for unit testing and static code analysis. Send alerts to an Amazon SNS topic for any bad builds. Deploy in an in-place, all-at-once deployment configuration using AWS CodeDeploy."},"answer_ET":"B","question_text":"A startup company recently migrated a large ecommerce website to AWS. The website has experienced a 70% increase in sales. Software engineers are using a private GitHub repository to manage code. The DevOps team is using Jenkins for builds and unit testing. The engineers need to receive notifications for bad builds and zero downtime during deployments. The engineers also need to ensure any changes to production are seamless for users and can be rolled back in the event of a major issue.\n\nThe software engineers have decided to use AWS CodePipeline to manage their build and deployment process.\n\nWhich solution will meet these requirements?","answers_community":["B (100%)"],"timestamp":"2024-02-05 17:44:00","answer_description":"","isMC":true,"question_id":352,"exam_id":33,"answer_images":[],"answer":"B"},{"id":"ihNsoTu9SzTLkEi6Y7Wm","url":"https://www.examtopics.com/discussions/amazon/view/132884-exam-aws-certified-solutions-architect-professional-sap-c02/","topic":"1","isMC":true,"timestamp":"2024-02-05 17:53:00","answer_ET":"B","answer_description":"","answers_community":["B (83%)","A (17%)"],"unix_timestamp":1707151980,"exam_id":33,"answer_images":[],"question_id":353,"discussion":[{"upvote_count":"5","timestamp":"1707198060.0","comment_id":"1141798","poster":"kejam","content":"Selected Answer: B\nAnswer B: LEAST operational effort and fine grained approach. \nhttps://aws.amazon.com/blogs/apn/optimizing-cost-per-tenant-visibility-in-saas-solutions/\n\nRCU and WCU metrics are already logged in CloudWatch.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html"},{"poster":"TomTom","comment_id":"1324412","timestamp":"1733813220.0","content":"Selected Answer: A\nOption A is the preferred solution for providing a granular view of DynamoDB costs for each tenant with the least operational effort.\n\nAdvantages of Option A:\n* Utilizes AWS's built-in tagging capabilities for cost allocation, which is straightforward to implement and maintain.\n* Provides detailed cost reports in AWS Cost Explorer without requiring additional custom code or complex calculations.\n\nDisadvantages of Option B:\nWhile it offers precise tracking of resource consumption, it significantly increases operational complexity and maintenance requirements.\n\nThus, for a balance of granularity and minimal operational effort, Option A is the optimal choice.","upvote_count":"2","comments":[{"upvote_count":"2","poster":"altonh","comment_id":"1358053","timestamp":"1739839500.0","content":"Tenants do not have their own tables. They share tables."}]},{"poster":"Chicote","upvote_count":"1","timestamp":"1729946700.0","content":"Selected Answer: B\nes B, seguro","comment_id":"1303271"},{"content":"Selected Answer: B\nOption B","upvote_count":"1","timestamp":"1709971500.0","poster":"career360guru","comment_id":"1169332"},{"timestamp":"1708649460.0","poster":"zzyy","comment_id":"1156813","content":"Selected Answer: B\nAnswer B","upvote_count":"1"},{"content":"Selected Answer: B\nAnswer is B","upvote_count":"1","timestamp":"1707653940.0","poster":"aabdlmouna","comment_id":"1147207"},{"timestamp":"1707373980.0","comment_id":"1144156","poster":"07c2d2a","upvote_count":"4","content":"AWS Cost Explorer API vs cost calculator is really all you need to consider here.","comments":[{"timestamp":"1707374100.0","poster":"07c2d2a","comment_id":"1144158","content":"API can automate it, cost calculator is a manual process and never ideal for something like this.","upvote_count":"2"}]},{"upvote_count":"1","content":"Selected Answer: B\nAnswer is B","comment_id":"1143468","timestamp":"1707319920.0","poster":"TheCloudGuruu"},{"timestamp":"1707151980.0","content":"Correct Answer is A","upvote_count":"1","comment_id":"1141292","poster":"alexis123456"}],"choices":{"B":"Configure the Lambda functions to log the tenant ID and the number of RCUs and WCUs consumed from DynamoDB for each transaction to Amazon CloudWatch Logs. Deploy another Lambda function to calculate the tenant costs by using the logged capacity units and the overall DynamoDB cost from the AWS Cost Explorer API. Create an Amazon EventBridge rule to invoke the calculation Lambda function on a schedule.","C":"Create a new partition key that associates DynamoDB items with individual tenants. Deploy a Lambda function to populate the new column as part of each transaction. Deploy another Lambda function to calculate the tenant costs by using Amazon Athena to calculate the number of tenant items from DynamoDB and the overall DynamoDB cost from the AWS CUR. Create an Amazon EventBridge rule to invoke the calculation Lambda function on a schedule.","D":"Deploy a Lambda function to log the tenant ID, the size of each response, and the duration of the transaction call as custom metrics to Amazon CloudWatch Logs. Use CloudWatch Logs Insights to query the custom metrics for each tenant. Use AWS Pricing Calculator to obtain the overall DynamoDB costs and to calculate the tenant costs.","A":"Associate a new tag that is named tenant ID with each table in DynamoDB. Activate the tag as a cost allocation tag in the AWS Billing and Cost Management console. Deploy new Lambda function code to log the tenant ID in Amazon CloudWatch Logs. Use the AWS CUR to separate DynamoDB consumption cost for each tenant ID."},"question_text":"A software as a service (SaaS) company has developed a multi-tenant environment. The company uses Amazon DynamoDB tables that the tenants share for the storage layer. The company uses AWS Lambda functions for the application services.\n\nThe company wants to offer a tiered subscription model that is based on resource consumption by each tenant. Each tenant is identified by a unique tenant ID that is sent as part of each request to the Lambda functions. The company has created an AWS Cost and Usage Report (AWS CUR) in an AWS account. The company wants to allocate the DynamoDB costs to each tenant to match that tenant's resource consumption.\n\nWhich solution will provide a granular view of the DynamoDB cost for each tenant with the LEAST operational effort?","answer":"B","question_images":[]},{"id":"HIxly3AWx4Gf8FRGIAi5","isMC":true,"exam_id":33,"unix_timestamp":1707152340,"topic":"1","answer_description":"","question_images":[],"question_id":354,"answer":"A","answer_ET":"A","choices":{"A":"Create a new AWS account that is accessible only to the security team through an assumed role. Create an S3 bucket in the new account. Enable S3 Versioning and S3 Object Lock. Configure a default retention period of 1 year. Set up replication from the existing S3 bucket to the new S3 bucket. Create an S3 Batch Replication job to copy all existing data.","D":"Enable Amazon GuardDuty with the S3 protection feature for the account and the AWS Region. Add an S3 Lifecycle rule to delete objects after 1 year.","C":"Explicitly deny bucket creation from all users and roles except for an AWS Service Catalog launch constraint role. Define a Service Catalog product for the creation of the S3 bucket to force S3 Versioning and MFA Delete to be enabled. Authorize users to launch the product when they need to create an S3 bucket.","B":"Use the s3-bucket-versioning-enabled AWS Config managed rule. Configure an automatic remediation action that uses an AWS Lambda function to enable S3 Versioning and MFA Delete on noncompliant resources. Add an S3 Lifecycle rule to delete objects after 1 year."},"url":"https://www.examtopics.com/discussions/amazon/view/132886-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_images":[],"discussion":[{"comment_id":"1146018","upvote_count":"8","timestamp":"1707558360.0","content":"Selected Answer: A\nS3 Object Lock - prevents objects from being deleted or overwritten for a fixed amount of time or indefinitely, adding a layer of protection against malicious or accidental deletion.\nReplication - to a new account limits the risk of a single point of compromise; even if attackers gain access to the original account, they cannot alter or delete the locked objects in the replicated bucket.\nVersioning - keeps multiple versions of an object in an S3 bucket, providing additional security and recovery options.","poster":"nharaz"},{"content":"Selected Answer: D\nOption D is the only option that addresses security risk. Option A is not addressing this - Replicating existing bucket to another bucket does not eliminate the risk due to original bucket credential leak.","poster":"career360guru","upvote_count":"5","comment_id":"1169336","timestamp":"1709972160.0"},{"comment_id":"1312336","poster":"0b43291","timestamp":"1731623160.0","content":"Selected Answer: A\nOption A, the company can effectively isolate sensitive data in a separate, secure account with strict access controls, while ensuring that both existing and future data are protected against unauthorized access, deletion, or modification, even if the original account's credentials are compromised.\nThe other options do not provide the same level of protection or have limitations:\n Option B relies on AWS Config and automatic remediation, which may not be effective if the attacker gains access to the account and disables or modifies these configurations.\n Option C focuses on controlling bucket creation but does not address the protection of existing data or objects in the current bucket.\n Option D relies on Amazon GuardDuty, which is a threat detection service and does not provide the same level of data protection as S3 Versioning and Object Lock.","upvote_count":"2"},{"content":"The correct answer to this question is Option C. Explicitly denying bucket creation from all users and roles except for an AWS Service Catalog launch constraint role, defining a Service Catalog product for the creation of the S3 bucket, and forcing S3 Versioning and MFA Delete ensures that existing and future objects in the S3 bucket are protected. This option provides explicit access controls for S3 bucket creation and forces S3 versioning and MFA Delete on noncompliant resources, making it the most suitable solution to address the security concerns of the company. Option A is not the best choice because creating a new AWS account can introduce complexity and create a single point of failure. While Options B and D offer some benefits, they do not provide explicit access controls for S3 bucket creation, which is essential for protecting sensitive data.","poster":"AzureDP900","timestamp":"1731364560.0","comment_id":"1310355","upvote_count":"1"},{"timestamp":"1730651820.0","comment_id":"1306573","poster":"AloraCloud","content":"Eliminate C & D","upvote_count":"1"},{"content":"Selected Answer: A\nA\nassume role to provide short-term credential","comment_id":"1244200","timestamp":"1720426620.0","upvote_count":"1","poster":"vip2"},{"comment_id":"1184033","timestamp":"1711540020.0","poster":"TonytheTiger","content":"Selected Answer: A\nOption A: Amazon S3 now allows you to enable S3 Object Lock for existing buckets with just a few clicks and to enable S3 Replication for buckets using S3 Object Lock\n\nhttps://aws.amazon.com/about-aws/whats-new/2023/11/amazon-s3-enabling-object-lock-buckets/#:~:text=To%20lock%20existing%20objects%2C%20you,of%20objects%20at%20a%20time.","upvote_count":"2"},{"upvote_count":"2","content":"Selected Answer: A\nThe question is, as so often, misleading. None of the alternatives deal with _access_, only with modification.","comment_id":"1170554","poster":"Dgix","timestamp":"1710096900.0"},{"comments":[{"comments":[{"comment_id":"1276259","timestamp":"1725232680.0","upvote_count":"1","poster":"Daniel76","content":"Attacker can just take the data and leave it intact. The damage is done."}],"timestamp":"1716840480.0","upvote_count":"1","comment_id":"1219827","content":"creating new account accessed by security team members is action taken to avoid the risk through leaked long-term credentials of existing account so Option A","poster":"9f02c8d"}],"comment_id":"1163226","upvote_count":"2","content":"The question is looking for solution for “concerned that an attacker could gain access to the AWS account through leaked long-term credentials”. \nNone of the answer is addressing the concern of “Access” Through “leaked long-term credentials”.\nThe is question doesn’t mention anything about data loss concerns, while, all the answers are providing protection for deleting the data.","timestamp":"1709267400.0","poster":"bjexamprep"},{"upvote_count":"2","poster":"TheCloudGuruu","timestamp":"1707320220.0","content":"Selected Answer: D\nAnswer is D. It's the only one that specifically addresses the issue. The question never said only the security team needs access.","comments":[{"upvote_count":"1","timestamp":"1707374400.0","poster":"07c2d2a","comment_id":"1144162","content":"The answer is a. It's the only one that prevents the data from being deleted by attackers that get access using long term credential. GuardDuty is a monitoring system. By itself, it doesn't actually stop anything from happening. It also likely wouldn't catch use of existing long-term credentials as malicious."},{"timestamp":"1707558480.0","content":"Enabling GuardDuty with S3 protection and adding a lifecycle rule to delete objects after 1 year focuses on monitoring for threats and managing object lifecycle but:\n\nDoes not prevent the deletion or alteration of objects by an attacker who has gained access.\nS3 protection in GuardDuty helps identify suspicious access patterns but after-the-fact rather than preventing unauthorized changes.","upvote_count":"1","comment_id":"1146019","poster":"nharaz"}],"comment_id":"1143472"},{"poster":"kejam","comment_id":"1141804","upvote_count":"2","timestamp":"1707198540.0","content":"Selected Answer: A\nhttps://repost.aws/knowledge-center/s3-cross-account-replication-object-lock"},{"comment_id":"1141708","poster":"duriselvan","upvote_count":"3","content":"A ans :\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/security-best-practices.html","timestamp":"1707186000.0"},{"content":"Correct Answer is A","poster":"alexis123456","comment_id":"1141295","timestamp":"1707152340.0","upvote_count":"1"}],"question_text":"A company has an application that stores data in a single Amazon S3 bucket. The company must keep all data for 1 year. The company’s security team is concerned that an attacker could gain access to the AWS account through leaked long-term credentials.\n\nWhich solution will ensure that existing and future objects in the S3 bucket are protected?","answers_community":["A (71%)","D (29%)"],"timestamp":"2024-02-05 17:59:00"},{"id":"qBE0OAQnOBoisJoHpgHR","exam_id":33,"answer_ET":"A","answer":"A","answer_description":"","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/132896-exam-aws-certified-solutions-architect-professional-sap-c02/","answers_community":["A (100%)"],"isMC":true,"question_text":"A company needs to improve the security of its web-based application on AWS. The application uses Amazon CloudFront with two custom origins. The first custom origin routes requests to an Amazon API Gateway HTTP API. The second custom origin routes traffic to an Application Load Balancer (ALB). The application integrates with an OpenID Connect (OIDC) identity provider (IdP) for user management.\n\nA security audit shows that a JSON Web Token (JWT) authorizer provides access to the API. The security audit also shows that the ALB accepts requests from unauthenticated users.\n\nA solutions architect must design a solution to ensure that all backend services respond to only authenticated users.\n\nWhich solution will meet this requirement?","timestamp":"2024-02-05 19:19:00","choices":{"A":"Configure the ALB to enforce authentication and authorization by integrating the ALB with the IdP. Allow only authenticated users to access the backend services.","D":"Enable AWS CloudTrail to log all requests that come to the ALB. Create an AWS Lambda function to analyze the logs and block any requests that come from unauthenticated users.","B":"Modify the CloudFront configuration to use signed URLs. Implement a permissive signing policy that allows any request to access the backend services.","C":"Create an AWS WAF web ACL that filters out unauthenticated requests at the ALB level. Allow only authenticated traffic to reach the backend services."},"question_id":355,"unix_timestamp":1707157140,"answer_images":[],"discussion":[{"comment_id":"1141810","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html","upvote_count":"10","timestamp":"1707198660.0","poster":"kejam"},{"comment_id":"1310350","timestamp":"1731363960.0","poster":"AzureDP900","upvote_count":"1","content":"Option A is right, this solution meets the requirement of ensuring that all backend services respond to only authenticated users:\n1) Authentication at the load balancer level: By configuring the ALB to integrate with the OIDC IdP, you can enforce authentication and authorization for incoming requests.\n2) Preventing unauthenticated requests: The ALB will reject any requests from unauthenticated users, ensuring that only authenticated users can access the backend services."},{"poster":"career360guru","upvote_count":"1","timestamp":"1709972340.0","content":"Selected Answer: A\nOption A","comment_id":"1169338"},{"comment_id":"1160756","content":"Selected Answer: A\nA is right","poster":"a54b16f","timestamp":"1709047080.0","upvote_count":"2"},{"comment_id":"1143474","content":"Selected Answer: A\nAnswer is A","timestamp":"1707320280.0","poster":"TheCloudGuruu","upvote_count":"2"},{"upvote_count":"3","poster":"alexis123456","content":"correct Answer is A","comment_id":"1141327","timestamp":"1707157140.0"}],"question_images":[]}],"exam":{"isBeta":false,"lastUpdated":"11 Apr 2025","provider":"Amazon","id":33,"numberOfQuestions":529,"isMCOnly":true,"name":"AWS Certified Solutions Architect - Professional SAP-C02","isImplemented":true},"currentPage":71},"__N_SSP":true}