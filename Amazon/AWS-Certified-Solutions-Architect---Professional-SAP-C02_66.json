{"pageProps":{"questions":[{"id":"Y7f63XawMJ2d80ernqx6","question_text":"A car rental company has built a serverless REST API to provide data to its mobile app. The app consists of an Amazon API Gateway API with a Regional endpoint, AWS Lambda functions, and an Amazon Aurora MySQL Serverless DB cluster. The company recently opened the API to mobile apps of partners. A significant increase in the number of requests resulted, causing sporadic database memory errors.\n\nAnalysis of the API traffic indicates that clients are making multiple HTTP GET requests for the same queries in a short period of time. Traffic is concentrated during business hours, with spikes around holidays and other events.\n\nThe company needs to improve its ability to support the additional usage while minimizing the increase in costs associated with the solution.\n\nWhich strategy meets these requirements?","question_images":[],"choices":{"C":"Modify the Aurora Serverless DB cluster configuration to increase the maximum amount of available memory.","D":"Enable throttling in the API Gateway production stage. Set the rate and burst values to limit the incoming calls.","A":"Convert the API Gateway Regional endpoint to an edge-optimized endpoint. Enable caching in the production stage.","B":"Implement an Amazon ElastiCache for Redis cache to store the results of the database calls. Modify the Lambda functions to use the cache."},"unix_timestamp":1700636760,"answer_images":[],"answers_community":["B (50%)","A (49%)","1%"],"answer_description":"","question_id":326,"timestamp":"2023-11-22 08:06:00","url":"https://www.examtopics.com/discussions/amazon/view/126849-exam-aws-certified-solutions-architect-professional-sap-c02/","exam_id":33,"isMC":true,"discussion":[{"comment_id":"1114007","content":"Option A is correct\nWhile A and B do the job but the question says \"minimizing the increase in costs associated with the solution\".. I'll go with A coz Edge-optimized endpoints cache responses at edge locations closer to users, significantly reducing the number of requests reaching the database and Lambda functions.. While Option B -- While ElastiCache for Redis a good caching solution, it adds complexity and cost compared to edge caching.","poster":"vibzr2023","timestamp":"1704395520.0","upvote_count":"10"},{"timestamp":"1704913140.0","poster":"career360guru","upvote_count":"10","comment_id":"1118952","content":"Selected Answer: A\nOption A because B is more expensive than A"},{"content":"Selected Answer: D\nWhy it is not D. Question mentions Sporadic requests for short period of time. it is good to implement throttling to reduce error rates. Question does not ask about cache/performance. Besides it could be all new requests so cache would not necessary help","upvote_count":"1","comment_id":"1418607","poster":"sergza888","timestamp":"1743534960.0"},{"upvote_count":"1","comment_id":"1409967","timestamp":"1742893560.0","content":"Selected Answer: B\nEliminates redundant database queries: Since multiple HTTP GET requests query the same data in a short period, caching the results in ElastiCache for Redis significantly reduces load on Aurora Serverless.\n\nImproves performance: Redis provides low-latency access to frequently requested data, ensuring fast responses.\n\nReduces costs: By offloading queries from Aurora, this minimizes the need for additional database memory or scaling.\n\nHandles traffic spikes effectively: Redis stores precomputed results, which helps in managing high traffic surges during peak hours.","poster":"eesa"},{"timestamp":"1742248620.0","comment_id":"1399859","upvote_count":"1","poster":"Deztroyer88","content":"Selected Answer: B\nOption A is for mostly for latency and doesn't reduce the load on the DB."},{"timestamp":"1736095560.0","content":"Selected Answer: A\nOption A","poster":"deepakR20","upvote_count":"1","comment_id":"1336815"},{"content":"Selected Answer: B\nOption B","timestamp":"1735533480.0","upvote_count":"1","comment_id":"1333918","poster":"ollyone"},{"poster":"henrikhmkhitaryan59","comment_id":"1325262","timestamp":"1733952180.0","upvote_count":"1","content":"Selected Answer: A\nTaking cost as the primary focus and assuming simple edge caching will suffice, Option A is the better choice."},{"upvote_count":"1","comment_id":"1324712","poster":"Spike2020","content":"Selected Answer: A\nAPI Gateway caching reduces database load by serving repeated requests\nCaching is ideal for handling identical GET requests\nEdge-optimized endpoints improve performance for distributed users\nNo code changes required","timestamp":"1733857380.0"},{"poster":"alexbraila","comments":[{"content":"And, indeed, as mentioned by asquared16, Neal Davis has a similar question in one of his exams and he also picked A.\n\nOverall explanation\nAn edge-optimized API endpoint is best for geographically distributed clients. API requests are routed to the nearest CloudFront Point of Presence (POP). For mobile clients this is a good use case for this type of endpoint. The Regional endpoint is best suited to traffic coming from within the Region only.\nYou can enable API caching in Amazon API Gateway to cache your endpoint's responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API.\n\nWhy not B:\nThis will increase costs associated with the solution as the ElastiCache cluster could be expensive.","timestamp":"1733311200.0","upvote_count":"1","poster":"alexbraila","comment_id":"1321826"}],"upvote_count":"1","comment_id":"1321822","content":"Selected Answer: A\nThe exact same question is here:\n\nhttps://repost.aws/questions/QU4xZPFTZ3TASRtyDteJBM7Q/amazon-elasticache-vs-api-gateway-edge-optimized-endpoint\n\nI am voting for A, but I am still not convinced","timestamp":"1733311020.0"},{"poster":"AzureDP900","upvote_count":"1","comment_id":"1313209","comments":[{"poster":"AzureDP900","timestamp":"1731786540.0","comment_id":"1313210","content":"*Option B","upvote_count":"1"}],"timestamp":"1731786540.0","content":"ption B (using ElastiCache for Redis) provides a more targeted solution that specifically addresses the issue of frequent requests and database memory errors. By caching frequently accessed data in ElastiCache, you can reduce the load on the Aurora Serverless DB cluster and improve performance."},{"content":"Selected Answer: B\nBoth options have their merits and can potentially address the issue to some extent. However, Option B (ElastiCache for Redis cache) is generally considered a more robust and targeted solution for caching database query results and addressing the root cause of redundant queries.\n\nIf the application data is highly dynamic or personalized, and cache invalidation is a significant concern, Option B may be the preferred choice, as it allows for more granular control over the caching logic within the application code.\n\nUltimately, the decision may depend on factors such as the nature of the data, the complexity of the caching requirements, the team's familiarity with the technologies involved, and the overall architectural preferences of the organization.","timestamp":"1731537540.0","upvote_count":"1","comment_id":"1311653","poster":"0b43291"},{"comment_id":"1303588","upvote_count":"1","timestamp":"1730034360.0","content":"Selected Answer: A\nOption A: Edge-Optimized Endpoint with API Gateway Caching\nThis is the most suitable solution because:\nAPI Gateway caching can store frequently accessed query results at edge locations, reducing latency and database load14\nEdge-optimized endpoints serve responses from locations closer to clients, improving performance4\nIt's more cost-effective compared to implementing ElastiCache or increasing database resources4\nThe pricing for HTTP API requests is very economical at $1 per million requests for the first 300 million1","poster":"sashenka"},{"timestamp":"1728592920.0","content":"Selected Answer: B\nFor those who are saying that option A is the correct one, why should API Gateway Regional Endpoint be converted to Edge-Optimized Endpoint, if caching can be enabled on either? Also, Between the two options, Option B tends to have a lower cost increase in the long term, especially because ElastiCache allows for more direct control over costs and can be adjusted to meet demand. Option A may result in significant data transfer costs, as switching to an Edge-Optimized endpoint involves CloudFront usage costs, which can increase rapidly as traffic grows. Therefore, if the goal is to minimize the increase in costs, Option B (using ElastiCache for Redis) is likely the best choice.,","upvote_count":"4","comment_id":"1295738","poster":"JoeTromundo"},{"comment_id":"1269491","timestamp":"1724159340.0","upvote_count":"3","content":"Selected Answer: A\nWell, I'm convinced with B too, but apparently Neal Davis chose A in one of his exams so..","poster":"asquared16"},{"comment_id":"1214190","poster":"Zas1","content":"Selected Answer: B\nMinimizing is not the most cheaper.","upvote_count":"2","timestamp":"1716191340.0"},{"timestamp":"1715673000.0","comment_id":"1211293","upvote_count":"7","poster":"red_panda","content":"Selected Answer: B\nFor me it's B. We are using a REGIONAL (Not Global) API Endpoint.\nTake in mind that the errors are in Database layer, so it's not a problem to APIs, but with Redis Cache for sure we solve it. For me it's the best choice."},{"upvote_count":"3","timestamp":"1712017740.0","comment_id":"1187755","poster":"AlbertC","content":"Selected Answer: B\nA doesn't resolve the problems. It is B."},{"upvote_count":"3","poster":"VerRi","timestamp":"1711063260.0","comment_id":"1179727","content":"Selected Answer: A\nB is expensive"},{"upvote_count":"1","comment_id":"1111852","timestamp":"1704197280.0","content":"kEY WORK MOBILE APP b IS ANY\nhttps://aws.amazon.com/elasticache/redis/","poster":"duriselvan"},{"comment_id":"1109578","content":"Selected Answer: A\nAPI Gateway can take care of caching and it should be the cheaper solution compared to ElastiCache for Redis. That why I go with A.","upvote_count":"4","poster":"carpa_jo","timestamp":"1703929200.0"},{"upvote_count":"6","timestamp":"1703695680.0","content":"Selected Answer: A\nThe main option is \"clients are making multiple HTTP GET requests for the same queries in a short period of time.\" Enable Cache from APIGW https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html\n\nOption B is workable solution but will add more cost","poster":"mosalahs","comment_id":"1107011"},{"content":"Selected Answer: B\nIts B \nFor those choosing A, a change between regional and edge API is not required but API caching is. The problem is that A doesn't explain \"how\" which is explained in B.","upvote_count":"4","timestamp":"1703088480.0","poster":"ayadmawla","comment_id":"1101726"},{"upvote_count":"4","content":"Selected Answer: B\nShould be B :\"Analysis of the API traffic indicates that clients are making multiple HTTP GET requests for the same queries in a short period of time\" . Same query with same result should be cached.","timestamp":"1702593360.0","poster":"GaryQian","comments":[{"comment_id":"1209767","poster":"mifune","timestamp":"1715431020.0","content":"It's what is doing the answer A as well. Where's the problem?","upvote_count":"1"}],"comment_id":"1096879"},{"comments":[{"comment_id":"1094109","timestamp":"1702355460.0","poster":"Russs99","content":"The problem being solved in this scenario is not a latency related, but catching, therefore, i am sticking with pick of B over A","upvote_count":"3"}],"timestamp":"1702353780.0","content":"Selected Answer: B\nOption A suggest Converting the API Gateway endpoint and enabling caching is not as effective for this scenario because edge-optimized endpoints are primarily for global distribution. This application is regional","comment_id":"1094103","poster":"Russs99","upvote_count":"4"},{"timestamp":"1701514500.0","upvote_count":"4","poster":"PAUGURU","content":"Selected Answer: B\nI'd say B, solution A can reduce latency using Edge API, moreover the caching part is too vague, what does it mean enable caching in the production? It means exactly solution B.","comment_id":"1086076"},{"upvote_count":"2","poster":"dutchy1988","comment_id":"1084322","timestamp":"1701345660.0","content":"multiple HTTP GET requests for the same queries -> leaning towards caching.\nAlso remark that company requires minimize costs, redis is out since you will have to spend money while there is need for it during low loads. A is the best solution here."},{"comment_id":"1083980","timestamp":"1701317160.0","upvote_count":"2","content":"Selected Answer: B\nB is right answer","poster":"ProMax"},{"timestamp":"1701310920.0","comment_id":"1083925","upvote_count":"2","content":"Selected Answer: A\nAnswer A. User Edge-optimized API GW endpoint. B would work but with increased costs and overhead for Elasticache for Redis.","poster":"shaaam80"},{"upvote_count":"3","timestamp":"1701025560.0","comment_id":"1080976","content":"Selected Answer: A\nA is the right answer.\n\nUsing ElastiCache will work, but you'll be required to spin up resources which you'll be billed for, even during slow periods when it's not needed. Better to use the caching that is a part of APIGW.","poster":"heatblur"},{"upvote_count":"2","poster":"Jonalb","comment_id":"1078061","timestamp":"1700713440.0","content":"Selected Answer: B\nB. Implementar um cache do Amazon ElastiCache for Redis para armazenar os resultados das chamadas de banco de dados. Modifique as funções do Lambda para usar o cache."},{"comment_id":"1077909","upvote_count":"3","poster":"thala","timestamp":"1700695260.0","content":"Selected Answer: A\nhttps://www.examtopics.com/discussions/amazon/view/47753-exam-aws-certified-solutions-architect-professional-topic-1/"},{"comment_id":"1077776","poster":"devalenzuela86","upvote_count":"3","timestamp":"1700684340.0","content":"Selected Answer: B\nB for sure","comments":[{"content":"By implementing an Amazon ElastiCache for Redis cache to store the results of the database calls and modifying the Lambda functions to use the cache, the car rental company can reduce the number of requests to the database and improve the performance of the API. This solution is cost-effective because it reduces the load on the database and minimizes the number of requests to the database.","upvote_count":"1","timestamp":"1700778660.0","comment_id":"1078877","poster":"devalenzuela86"}]},{"comment_id":"1077049","poster":"cypkir","upvote_count":"3","content":"Selected Answer: A\nAnswer: A","timestamp":"1700636760.0"}],"topic":"1","answer":"B","answer_ET":"B"},{"id":"u1vgSGtNIFUgenAk7dHP","timestamp":"2023-11-22 08:07:00","exam_id":33,"discussion":[{"poster":"shaaam80","timestamp":"1701311520.0","content":"Selected Answer: B\nAnswer B - Company has created a DB schema on AWS. So next logical step is to use DMS for DB migration over the Private VIF. VPC Endpoint is also used for DMS.","comment_id":"1083929","upvote_count":"5"},{"upvote_count":"1","comment_id":"1313206","content":"B is right","timestamp":"1731786060.0","poster":"AzureDP900"},{"comment_id":"1118954","poster":"career360guru","upvote_count":"1","content":"Selected Answer: B\nOption B","timestamp":"1704913260.0"},{"timestamp":"1702593600.0","upvote_count":"2","comment_id":"1096882","content":"Selected Answer: B\nShould be B\nAll other options are Loading data into S3 then copy again to DB . Way to slow","poster":"GaryQian"},{"upvote_count":"2","timestamp":"1702584600.0","content":"Selected Answer: B\nB: Definitly DMS","poster":"FuriouZ","comment_id":"1096789"},{"poster":"GabrielDeBiasi","timestamp":"1701171120.0","content":"Selected Answer: B\ndatabase migration AND least possible downtime? AWS DMS","upvote_count":"3","comment_id":"1082490"},{"content":"Selected Answer: B\nB. Use o AWS Database Migration Service (AWS DMS) para migrar os dados para a AWS. Crie uma instância de replicação DMS em uma sub-rede privada. Crie endpoints VPC para AWS DMS. Configure uma tarefa DMS para copiar dados do banco de dados local para a instância de banco de dados usando carga total mais captura de dados de alteração (CDC). Use a chave padrão do AWS Key Management Service (AWS KMS) para criptografia em repouso. Use TLS para criptografia em trânsito.","upvote_count":"2","poster":"Jonalb","timestamp":"1700713200.0","comment_id":"1078060"},{"comment_id":"1077911","poster":"thala","content":"Selected Answer: B\nhttps://www.examtopics.com/discussions/amazon/view/89247-exam-aws-certified-solutions-architect-professional-topic-1/","upvote_count":"1","timestamp":"1700695440.0"},{"comment_id":"1077788","content":"Selected Answer: B\nAnswer B","poster":"devalenzuela86","upvote_count":"1","timestamp":"1700685180.0"},{"poster":"cypkir","content":"Selected Answer: B\nAnswer: B","comment_id":"1077051","timestamp":"1700636820.0","upvote_count":"1"}],"answer_ET":"B","isMC":true,"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/126850-exam-aws-certified-solutions-architect-professional-sap-c02/","question_text":"A company is migrating an on-premises application and a MySQL database to AWS. The application processes highly sensitive data, and new data is constantly updated in the database. The data must not be transferred over the internet. The company also must encrypt the data in transit and at rest.\n\nThe database is 5 TB in size. The company already has created the database schema in an Amazon RDS for MySQL DB instance. The company has set up a 1 Gbps AWS Direct Connect connection to AWS. The company also has set up a public VIF and a private VIF. A solutions architect needs to design a solution that will migrate the data to AWS with the least possible downtime.\n\nWhich solution will meet these requirements?","answer_images":[],"topic":"1","question_id":327,"choices":{"B":"Use AWS Database Migration Service (AWS DMS) to migrate the data to AWS. Create a DMS replication instance in a private subnet. Create VPC endpoints for AWS DMS. Configure a DMS task to copy data from the on-premises database to the DB instance by using full load plus change data capture (CDC). Use the AWS Key Management Service (AWS KMS) default key for encryption at rest. Use TLS for encryption in transit.","C":"Perform a database backup. Use AWS DataSync to transfer the backup files to Amazon S3. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3) for encryption at rest. Use TLS for encryption in transit. Import the data from Amazon S3 to the DB instance.","A":"Perform a database backup. Copy the backup files to an AWS Snowball Edge Storage Optimized device. Import the backup to Amazon S3. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3) for encryption at rest. Use TLS for encryption in transit. Import the data from Amazon S3 to the DB instance.","D":"Use Amazon S3 File Gateway. Set up a private connection to Amazon S3 by using AWS PrivateLink. Perform a database backup. Copy the backup files to Amazon S3. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3) for encryption at rest. Use TLS for encryption in transit. Import the data from Amazon S3 to the DB instance."},"answers_community":["B (100%)"],"answer":"B","unix_timestamp":1700636820,"answer_description":""},{"id":"O44xdTQdCOESzDAGucPy","choices":{"A":"Provision an AWS Storage Gateway file gateway NFS file share that is attached to an Amazon S3 bucket. Mount the NFS file share on each EC2 instance in the cluster.","D":"Provision a new Amazon Elastic File System (Amazon EFS) file system that uses Max I/O performance mode. Mount the EFS file system on each EC2 instance in the cluster.","B":"Provision a new Amazon Elastic File System (Amazon EFS) file system that uses General Purpose performance mode. Mount the EFS file system on each EC2 instance in the cluster.","C":"Provision a new Amazon Elastic Block Store (Amazon EBS) volume that uses the io2 volume type. Attach the EBS volume to all of the EC2 instances in the cluster."},"exam_id":33,"question_text":"Accompany is deploying a new cluster for big data analytics on AWS. The cluster will run across many Linux Amazon EC2 instances that are spread across multiple Availability Zones.\n\nAll of the nodes in the cluster must have read and write access to common underlying file storage. The file storage must be highly available, must be resilient, must be compatible with the Portable Operating System Interface (POSIX), and must accommodate high levels of throughput.\n\nWhich storage solution will meet these requirements?","isMC":true,"answer_ET":"D","answers_community":["D (73%)","B (28%)"],"answer_images":[],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/126958-exam-aws-certified-solutions-architect-professional-sap-c02/","answer":"D","question_images":[],"question_id":328,"discussion":[{"comments":[{"upvote_count":"1","comment_id":"1198090","timestamp":"1713464580.0","poster":"titi_r","comments":[{"content":"Yes, that's the reason the description is saying that the EC2 are spread in multiple AZs. It's option \"D\" the correct one","upvote_count":"2","timestamp":"1715436420.0","comment_id":"1209823","poster":"mifune"}],"content":"It's \"B\", not \"D\".\n\n\"For workloads that require high throughput and IOPS, use Regional file systems configured with the General Purpose performance mode and Elastic throughput.\nNote: To achieve the maximum 250,000 read IOPS for frequently accessed data, the file system must use Elastic throughput.\nNote: Elastic throughput is available only for file systems that use the General Purpose performance mode.\nMax I/O mode is not supported for One Zone file systems or file systems that use Elastic throughput.\"\n-\nhttps://docs.aws.amazon.com/efs/latest/ug/performance.html"}],"poster":"J0n102","upvote_count":"17","content":"Selected Answer: D\n- General purpose performance mode (default)\nIdeal for latency-sensitive use cases.\n- Max I/O mode\nCan scale to higher levels of aggregate throughput and operations per second with a tradeoff of slightly higher latencies for file operations.","comment_id":"1086701","timestamp":"1701593940.0"},{"content":"Selected Answer: D\nD\nIn contrast, Max I/O file systems are suitable for workloads such as data analytics, media processing, and machine learning. These workloads need to perform parallel operations from hundreds or even thousands of containers and require the highest possible aggregate throughput and IOPS\n2 keywords matching the question, Throughput and Data analytic\nhttps://docs.aws.amazon.com/AmazonECS/latest/bestpracticesguide/storage-efs.html","timestamp":"1710888840.0","poster":"pangchn","upvote_count":"9","comment_id":"1177799"},{"upvote_count":"1","timestamp":"1731785940.0","content":"D is right","poster":"AzureDP900","comment_id":"1313205"},{"poster":"0b43291","timestamp":"1731544380.0","content":"Selected Answer: D\nOption D is the correct answer because Amazon Elastic File System (Amazon EFS) with Max I/O performance mode meets all the requirements for the big data analytics cluster. EFS provides highly available, resilient, and POSIX-compatible file storage replicated across multiple Availability Zones. Max I/O mode offers high levels of throughput and IOPS required for high-performance workloads like big data analytics. By mounting the EFS file system on each EC2 instance in the cluster, all nodes can access the common file storage with read and write capabilities, enabling seamless collaboration and data sharing across the distributed cluster.\n\nOption B (Amazon EFS General Purpose performance mode): The General Purpose performance mode is suitable for most file system workloads but may not provide the high levels of throughput required for big data analytics workloads.","comment_id":"1311664","upvote_count":"1"},{"timestamp":"1730087220.0","upvote_count":"1","poster":"Danm86","comment_id":"1303813","content":"B is the correct answer, Max I/O mode is the legacy version available, for best performance AWS recommends using default mode which is general purpose. \nhttps://docs.aws.amazon.com/efs/latest/ug/performance.html#performancemodes"},{"comment_id":"1206037","content":"Selected Answer: D\nD for me","upvote_count":"1","poster":"seetpt","timestamp":"1714731840.0"},{"comment_id":"1198091","content":"Selected Answer: B\nB - correct.","poster":"titi_r","timestamp":"1713464700.0","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: D\nfor analytics workload","poster":"CMMC","comment_id":"1179771","timestamp":"1711072980.0"},{"content":"Selected Answer: D\nD looks correct","poster":"yog927","comment_id":"1175565","upvote_count":"3","timestamp":"1710647220.0"},{"timestamp":"1710092400.0","upvote_count":"3","poster":"career360guru","comment_id":"1170493","content":"Selected Answer: D\nOption D because of High Throughput requirement"},{"comment_id":"1169104","upvote_count":"2","poster":"liquen14","timestamp":"1709931360.0","content":"Selected Answer: B\nfrom https://docs.aws.amazon.com/efs/latest/ug/performance.html#performancemodes: \n\n\"Due to the higher per-operation latencies with Max I/O, we recommend using General Purpose performance mode for all file systems.\""},{"timestamp":"1708757460.0","poster":"cf9e355","upvote_count":"3","comment_id":"1157689","content":"Selected Answer: D\nPerformance\n......'' In contrast, Max I/O file systems are suitable for workloads such as data analytics, media processing, and machine learning. \".........\nref:\nhttps://docs.aws.amazon.com/AmazonECS/latest/bestpracticesguide/storage-efs.html"},{"comment_id":"1153499","poster":"marszalekm","content":"Selected Answer: B\nIOPS is something different than throughput","timestamp":"1708282200.0","upvote_count":"1"},{"comment_id":"1133497","poster":"Exams22","timestamp":"1706373600.0","content":"Selected Answer: B\nIOPS is not throughput... General Purpose performance mode has a higher throughput","upvote_count":"2"},{"upvote_count":"2","content":"Selected Answer: D\n\"Max I/O performance mode has higher per-operation latencies than General Purpose performance mode. For faster performance, we recommend always using General Purpose performance mode\"\n\nNo performance requirement but high I/O in the question.","poster":"tmlong18","comment_id":"1123929","timestamp":"1705388760.0"},{"poster":"career360guru","content":"Selected Answer: D\nOption D as Maximum Throughput is primary requirement here.","timestamp":"1704913500.0","comment_id":"1118963","upvote_count":"3"},{"content":"Selected Answer: B\nB seems more appropriate.","poster":"Mosn","upvote_count":"1","comment_id":"1115287","timestamp":"1704556500.0"},{"upvote_count":"1","poster":"vibzr2023","content":"think Both B and D are correct.. But I'll go with B coz the hierarchy is general purpose then Max I/O..\n- A. Storage Gateway: While providing NFS access to S3, it's not optimized for high throughput or real-time access like EFS, making it less suitable for big data analytics workloads.\n- C. EBS Volume: EBS volumes can only be attached to a single EC2 instance at a time, limiting their use for shared storage across multiple instances.\n- D. EFS Max I/O: While offering the highest throughput, it's more expensive than General Purpose mode and might not be necessary for all big data workloads.","timestamp":"1704318480.0","comment_id":"1113159"},{"comment_id":"1111875","timestamp":"1704198480.0","content":"D ans\nWhat latency, throughput, and IOPS performance can I expect for my Amazon EFS file system?\nThe expected performance for your Amazon EFS file system depends on its specific configuration (for instance, storage class and thoroughput mode) and the specific file system operation type (read or write). Please see the File System Performance documentation for more information on expected latency , maximum throughput, and maximum IOPS performance for Amazon EFS file systems.","poster":"duriselvan","upvote_count":"1"},{"comment_id":"1111014","upvote_count":"1","poster":"NOZOMI","content":"https://docs.aws.amazon.com/efs/latest/ug/performance.html#performancemodes\nImportant\nDue to the higher per-operation latencies with Max I/O, we recommend using General Purpose performance mode for all file systems.","timestamp":"1704095220.0"},{"upvote_count":"1","content":"I vote for B","poster":"yuliaqwerty","comment_id":"1107068","timestamp":"1703701320.0"},{"content":"Selected Answer: B\nAnswer is B - See: https://docs.aws.amazon.com/efs/latest/ug/performance.html\nWith Elastic Througput, EFS Standard can achieve up to 250,000 Reads and 50,000 writes. Elastic Throughput is not supported on Max I/O\n\nMax I/O performance mode has higher per-operation latencies than General Purpose performance mode. For faster performance, we recommend always using General Purpose performance mode. For more information, see Performance modes.","comment_id":"1101734","timestamp":"1703089440.0","upvote_count":"5","poster":"ayadmawla"},{"timestamp":"1702792920.0","poster":"erenbiku1","content":"Selected Answer: B\nMax I/O performance mode has higher per-operation latencies than General Purpose performance mode. For faster performance, we recommend always using General Purpose performance mode. For more information, see Performance modes.","comment_id":"1098687","upvote_count":"1"},{"poster":"GaryQian","content":"Selected Answer: D\nKey word: high level of throughput \nBut AWS seems to sell their 'general purpose' EFS eveywhere.","timestamp":"1702593780.0","comment_id":"1096883","upvote_count":"2"},{"upvote_count":"3","content":"Selected Answer: D\nD because throughput is more important than latency","timestamp":"1702584720.0","poster":"FuriouZ","comment_id":"1096793"},{"comment_id":"1095742","timestamp":"1702493220.0","poster":"MegalodonBolado","content":"Selected Answer: B\nAmazon EFS offers two performance modes, General Purpose and Max I/O.\n\n* General Purpose mode has the lowest per-operation latency and is the Performance mode for file systems. One Zone file systems always use the General Purpose Performance mode. For faster performance, we recommend always using General Purpose Performance mode.\n\n* Max I/O mode is a previous generation performance type that is designed for highly parallelized workloads that can tolerate higher latencies than the General Purpose mode. Max I/O mode is not supported for One Zone file systems or file systems that use Elastic throughput.\n\nhttps://docs.aws.amazon.com/efs/latest/ug/performance.html#performancemodes","upvote_count":"3"},{"comment_id":"1083937","upvote_count":"3","content":"Selected Answer: D\nAnswer D. High levels of throughput - Max I/O. \nIf the ask was reduced latency, then General Purpose would have sufficed.","timestamp":"1701312120.0","poster":"shaaam80"},{"upvote_count":"4","content":"Selected Answer: B\nMax/IO is legacy, and you should always use General Purpose performance mode for faster performance.","timestamp":"1701242280.0","poster":"tfl","comment_id":"1083190"},{"poster":"salazar35","content":"Selected Answer: B\n\"For faster performance, we recommend always using General Purpose Performance mode.\"","timestamp":"1701195660.0","comment_id":"1082827","upvote_count":"1"},{"upvote_count":"2","comment_id":"1080979","timestamp":"1701025980.0","content":"Selected Answer: D\nD is the right answer. EFS is required as is Max I/O. This mode is optimized for applications where tens, hundreds, or thousands of EC2 instances are accessing the file system. It's designed for high levels of aggregate throughput and operations per second, making it ideal for big data and analytics applications.","poster":"heatblur"},{"upvote_count":"2","comment_id":"1078059","content":"Selected Answer: D\nD. Provisione um novo sistema de arquivos Amazon Elastic File System (Amazon EFS) que use o modo de desempenho Max I/O. Monte o sistema de arquivos EFS em cada instância do EC2 no cluster.","poster":"Jonalb","timestamp":"1700712960.0"},{"timestamp":"1700695740.0","comment_id":"1077915","poster":"thala","content":"Selected Answer: D\nhttps://www.examtopics.com/discussions/amazon/view/88074-exam-aws-certified-solutions-architect-professional-topic-1/","upvote_count":"2","comments":[{"poster":"devalenzuela86","timestamp":"1700778120.0","content":"Read all the comments in the question.","comment_id":"1078873","upvote_count":"1"},{"content":"B is the most appropriate because it allows you to have a highly available, resilient, POSIX-compatible, and high throughput file storage that can be accessed by all nodes in the big data analytics cluster running on AWS. By provisioning a new Amazon EFS file system that uses General Purpose performance mode, you can ensure that the file storage is optimized for a wide variety of workloads and can accommodate high levels of throughput. By mounting the EFS file system on each EC2 instance in the cluster, you can ensure that all nodes in the cluster have read and write access to the common underlying file storage.","comment_id":"1078871","timestamp":"1700778060.0","poster":"devalenzuela86","upvote_count":"2"}]},{"content":"Selected Answer: B\nAnswer B","comment_id":"1077789","timestamp":"1700685360.0","poster":"devalenzuela86","upvote_count":"1"}],"unix_timestamp":1700685360,"topic":"1","timestamp":"2023-11-22 21:36:00"},{"id":"b2Xq1zdKDrndlBAxkmmT","discussion":[{"timestamp":"1701171540.0","poster":"GabrielDeBiasi","content":"Selected Answer: D\nOne thing we can learn here is if you see \"aurora serverless VERSION 1\" -> migrate away from this","upvote_count":"11","comment_id":"1082503"},{"poster":"heatblur","content":"Selected Answer: D\nD is the answer. \n\nConvert the Aurora Serverless v1 database to a standard Aurora MySQL global database extending across the source and target regions, launch the solution in the target region, and configure the two regional solutions to work in an active-passive configuration. This approach provides the necessary speed for recovery and data replication to meet the strict RTO and RPO.\n\nAurora Serverless v1 doesn't support read replicas, cross region replicas, or global databases.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.html#aurora-serverless.limitations","comment_id":"1080984","upvote_count":"7","timestamp":"1701026400.0"},{"upvote_count":"1","timestamp":"1744033140.0","comment_id":"1558584","poster":"hiberus","content":"Selected Answer: B\nWhy not B? What is the problem of lauching de runbook with SAM?"},{"comment_id":"1324459","timestamp":"1733821440.0","poster":"Spike2020","upvote_count":"1","content":"Selected Answer: B\nMore cost-effective (no constantly running passive environment)\nUses SAM runbook for automated, consistent deployment"},{"content":"Selected Answer: B\nWhy not B?\nOption B can fulfill the requirement for RTO of 5 mins and RPO of 1 min.\nOption D Active-Passive require manual intervention and introduce additional complexity.","poster":"TomTom","comment_id":"1320763","timestamp":"1733107920.0","upvote_count":"1"},{"upvote_count":"2","comment_id":"1290206","poster":"Syre","timestamp":"1727460360.0","content":"Selected Answer: B\nB is indeed a better choice here because it focuses on using a global database with automated deployment, which is critical for achieving both the RTO and RPO requirements efficiently."},{"content":"Selected Answer: D\nOption D","upvote_count":"1","timestamp":"1704913860.0","comment_id":"1118967","poster":"career360guru"},{"upvote_count":"4","timestamp":"1701195840.0","content":"Selected Answer: D\nD will provide \"RTO of 5 minutes and an RPO of 1 minute\"","comment_id":"1082832","poster":"salazar35"},{"timestamp":"1700712780.0","comment_id":"1078057","poster":"Jonalb","content":"Selected Answer: D\nD. Altere o banco de dados Aurora Serverless v1 para um banco de dados global Aurora MySQL padrão que se estende pela região de origem e pela região de destino. Inicie a solução na região de destino. Configure as duas soluções regionais para funcionarem em uma configuração ativa-passiva.","upvote_count":"2"},{"poster":"thala","comment_id":"1077918","upvote_count":"4","comments":[{"upvote_count":"1","content":"D is incorrect because changing the Aurora Serverless v1 database to a standard Aurora MySQL global database that extends across the source Region and the target Region and launching the solution in the target Region does not meet the RTO and RPO requirements.","poster":"devalenzuela86","timestamp":"1700777580.0","comment_id":"1078865"}],"content":"Selected Answer: D\nOption D (Change to Aurora MySQL Global Database and Launch Solution in Target Region with Active-Passive Configuration) is the most suitable solution. It addresses both the database replication and application layer readiness in the target region, meeting the specified RTO and RPO requirements.","timestamp":"1700696100.0"},{"comment_id":"1077795","comments":[{"poster":"devalenzuela86","upvote_count":"1","comments":[{"timestamp":"1701312720.0","upvote_count":"2","content":"Aurora Serverless v1 db does not support replicas.","comment_id":"1083943","poster":"shaaam80"}],"comment_id":"1078863","content":"To design a disaster recovery (DR) strategy that can recover the solution in another AWS Region with an RTO of 5 minutes and an RPO of 1 minute, the best solution would be to create a read replica of the Aurora Serverless v1 database in the target Region. Then, use AWS SAM to create a runbook to deploy the solution to the target Region. Finally, promote the read replica to primary in case of disaster.","timestamp":"1700777520.0"}],"upvote_count":"2","content":"Selected Answer: A\nA for sure","poster":"devalenzuela86","timestamp":"1700685720.0"},{"comment_id":"1077056","timestamp":"1700637120.0","content":"Selected Answer: D\nAnswer: D","upvote_count":"1","poster":"cypkir"}],"topic":"1","choices":{"D":"Change the Aurora Serverless v1 database to a standard Aurora MySQL global database that extends across the source Region and the target Region. Launch the solution in the target Region. Configure the two Regional solutions to work in an active-passive configuration.","C":"Create an Aurora Serverless v1 DB cluster that has multiple writer instances in the target Region. Launch the solution in the target Region. Configure the two Regional solutions to work in an active-passive configuration.","B":"Change the Aurora Serverless v1 database to a standard Aurora MySQL global database that extends across the source Region and the target Region. Use AWS SAM to create a runbook to deploy the solution to the target Region.","A":"Create a read replica of the Aurora Serverless v1 database in the target Region. Use AWS SAM to create a runbook to deploy the solution to the target Region. Promote the read replica to primary in case of disaster."},"answer_ET":"D","timestamp":"2023-11-22 08:12:00","answer_description":"","unix_timestamp":1700637120,"isMC":true,"question_images":[],"exam_id":33,"question_id":329,"answer":"D","answers_community":["D (81%)","Other"],"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/126851-exam-aws-certified-solutions-architect-professional-sap-c02/","question_text":"A company hosts a software as a service (SaaS) solution on AWS. The solution has an Amazon API Gateway API that serves an HTTPS endpoint. The API uses AWS Lambda functions for compute. The Lambda functions store data in an Amazon Aurora Serverless v1 database.\n\nThe company used the AWS Serverless Application Model (AWS SAM) to deploy the solution. The solution extends across multiple Availability Zones and has no disaster recovery (DR) plan.\n\nA solutions architect must design a DR strategy that can recover the solution in another AWS Region. The solution has an RTO of 5 minutes and an RPO of 1 minute.\n\nWhat should the solutions architect do to meet these requirements?"},{"id":"o7bFpm5NOSBULDfnPzi2","answer":"A","topic":"1","exam_id":33,"url":"https://www.examtopics.com/discussions/amazon/view/126852-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_description":"","question_images":[],"answers_community":["A (98%)","2%"],"isMC":true,"timestamp":"2023-11-22 08:16:00","unix_timestamp":1700637360,"answer_images":[],"question_text":"A company owns a chain of travel agencies and is running an application in the AWS Cloud. Company employees use the application to search for information about travel destinations. Destination content is updated four times each year.\n\nTwo fixed Amazon EC2 instances serve the application. The company uses an Amazon Route 53 public hosted zone with a multivalue record of travel.example.com that returns the Elastic IP addresses for the EC2 instances. The application uses Amazon DynamoDB as its primary data store. The company uses a self-hosted Redis instance as a caching solution.\n\nDuring content updates, the load on the EC2 instances and the caching solution increases drastically. This increased load has led to downtime on several occasions. A solutions architect must update the application so that the application is highly available and can handle the load that is generated by the content updates.\n\nWhich solution will meet these requirements?","answer_ET":"A","choices":{"A":"Set up DynamoDB Accelerator (DAX) as in-memory cache. Update the application to use DAX. Create an Auto Scaling group for the EC2 instances. Create an Application Load Balancer (ALB). Set the Auto Scaling group as a target for the ALB. Update the Route 53 record to use a simple routing policy that targets the ALB's DNS alias. Configure scheduled scaling for the EC2 instances before the content updates.","B":"Set up Amazon ElastiCache for Redis. Update the application to use ElastiCache. Create an Auto Scaling group for the EC2 instances. Create an Amazon CloudFront distribution, and set the Auto Scaling group as an origin for the distribution. Update the Route 53 record to use a simple routing policy that targets the CloudFront distribution’s DNS alias. Manually scale up EC2 instances before the content updates.","C":"Set up Amazon ElastiCache for Memcached. Update the application to use ElastiCache. Create an Auto Scaling group for the EC2 instances. Create an Application Load Balancer (ALB). Set the Auto Scaling group as a target for the ALB. Update the Route 53 record to use a simple routing policy that targets the ALB's DNS alias. Configure scheduled scaling for the application before the content updates.","D":"Set up DynamoDB Accelerator (DAX) as in-memory cache. Update the application to use DAX. Create an Auto Scaling group for the EC2 instances. Create an Amazon CloudFront distribution, and set the Auto Scaling group as an origin for the distribution. Update the Route 53 record to use a simple routing policy that targets the CloudFront distribution's DNS alias. Manually scale up EC2 instances before the content updates."},"discussion":[{"comments":[{"timestamp":"1723548600.0","upvote_count":"1","poster":"kgpoj","comments":[{"poster":"juanife","upvote_count":"1","content":"the thing is (as other people already said as well) AWS is evaluating not only our technical knowledge about its Cloud services but also how he manage to solve technical scenarios while using english language","comment_id":"1355676","timestamp":"1739377980.0"}],"comment_id":"1265124","content":"Couldn't agree more.\n\nThey can just ask us, hey for DynamoDB's cache, should you use DAX or ElasticCache or Memcached? Is CloudFront Distribution designed for fixing sudden traffic spike?\n\nThen we can just say: 1. DAX; 2. no. :D"}],"upvote_count":"34","comment_id":"1084909","timestamp":"1701401100.0","poster":"heatblur","content":"Selected Answer: A\nThe length of these questions should be a crime...."},{"timestamp":"1704316680.0","poster":"vibzr2023","content":"Option A correct... other options\nB. ElastiCache for Redis: While a good caching solution, DAX is specifically optimized for DynamoDB, making it a better choice in this context.\nC. ElastiCache for Memcached: Memcached is not as feature-rich as Redis and lacks DAX's DynamoDB integration.\nD. CloudFront: While useful for content delivery, it's not the primary solution for handling database load and scaling EC2 instances.","upvote_count":"9","comment_id":"1113136"},{"poster":"AzureDP900","upvote_count":"1","timestamp":"1731785640.0","content":"A and D are similar however manually scale up instances can be eliminated easily. A is right","comment_id":"1313202"},{"comment_id":"1269449","upvote_count":"1","content":"That felt like reading a eulogy with a gun to my face.","timestamp":"1724156760.0","poster":"asquared16"},{"timestamp":"1720946520.0","poster":"gfhbox0083","comment_id":"1247701","upvote_count":"1","content":"Selected Answer: A\nA, for sure.\nNo need for CF in the case of content updates"},{"upvote_count":"1","comment_id":"1178511","timestamp":"1710952920.0","poster":"Dgix","content":"Selected Answer: A\nA: Correct. Utilizes DAX for DynamoDB caching, Auto Scaling for EC2, and ALB for traffic distribution; aligns with best practices.\n\nB Incorrect. CloudFront is not optimal for dynamic content load handling; manual scaling is less efficient than scheduled scaling."},{"comment_id":"1118974","upvote_count":"1","timestamp":"1704914400.0","poster":"career360guru","content":"Selected Answer: A\nOption A"},{"comment_id":"1083948","upvote_count":"3","timestamp":"1701313380.0","poster":"shaaam80","content":"Selected Answer: A\nAnswer - A. use DAX, in memory cache of DynamoDB.\nB is wrong - manually scale up & Autoscaling group as origin for the CF distro"},{"timestamp":"1701017040.0","comment_id":"1080820","poster":"salazar35","upvote_count":"3","content":"Selected Answer: A\nA - Update issue no need CloudFront here"},{"poster":"Jonalb","comment_id":"1078034","timestamp":"1700710200.0","upvote_count":"3","content":"Selected Answer: A\nA. Configure o DynamoDB Accelerator (DAX) como cache na memória. Atualize o aplicativo para usar o DAX. Crie um grupo do Auto Scaling para as instâncias do EC2. Crie um balanceador de carga de aplicativo (ALB). Defina o grupo do Auto Scaling como destino para o ALB. Atualize o registro do Route 53 para usar uma política de roteamento simples que tenha como alvo o alias DNS do ALB. Configure o escalonamento programado para as instâncias do EC2 antes das atualizações de conteúdo."},{"comment_id":"1077924","upvote_count":"3","poster":"thala","content":"Selected Answer: A\nhttps://www.examtopics.com/discussions/amazon/view/70883-exam-aws-certified-solutions-architect-professional-topic-1/","timestamp":"1700696640.0"},{"poster":"devalenzuela86","upvote_count":"1","comments":[{"content":"Yes, A is correct","timestamp":"1700777100.0","comment_id":"1078861","upvote_count":"1","poster":"devalenzuela86"}],"content":"Selected Answer: B\nB is correct","timestamp":"1700686440.0","comment_id":"1077804"},{"comment_id":"1077060","content":"Selected Answer: A\nAnswer: A","upvote_count":"1","poster":"cypkir","timestamp":"1700637360.0"}],"question_id":330}],"exam":{"provider":"Amazon","id":33,"lastUpdated":"11 Apr 2025","numberOfQuestions":529,"name":"AWS Certified Solutions Architect - Professional SAP-C02","isMCOnly":true,"isImplemented":true,"isBeta":false},"currentPage":66},"__N_SSP":true}