{"pageProps":{"questions":[{"id":"SWK9lHvUnCisrOtOnlEM","answers_community":["A (57%)","D (43%)"],"choices":{"A":"Create an Amazon Aurora MySQL replica of the RDS for MySQL DB instance. Pause application writes to the RDS DB instance. Promote the Aurora Replica to a standalone DB cluster. Reconfigure the application to use the Aurora database and resume writes. Add eu-west-1 as a secondary Region to the 06 cluster. Enable write forwarding on the DB cluster. Deploy the application in eu-west-1. Configure the application to use the Aurora MySQL endpoint in eu- west-1.","C":"Copy the most recent snapshot from the RDS for MySQL DB instance to eu-west-1. Create a new RDS for MySQL DB instance in eu-west-1 from the snapshot. Configure MySQL logical replication from us-east-1 to eu-west-1. Enable write forwarding on the DB cluster. Deploy the application in eu-west-1. Configure the application to use the RDS for MySQL endpoint in eu-west-1.","D":"Convert the RDS for MySQL DB instance to an Amazon Aurora MySQL DB cluster. Add eu-west-1 as a secondary Region to the DB cluster. Enable write forwarding on the DB cluster. Deploy the application in eu-west-1. Configure the application to use the Aurora MySQL endpoint in eu-west-1.","B":"Add a cross-Region replica in eu-west-1 for the RDS for MySQL DB instance. Configure the replica to replicate write queries back to the primary DB instance. Deploy the application in eu-west-1. Configure the application to use the RDS for MySQL endpoint in eu-west-1."},"answer_ET":"A","answer_images":[],"discussion":[{"timestamp":"1720987200.0","comment_id":"1247954","content":"Selected Answer: A\nA, First a read replica has to be created and that could be promoted to a cluster. second forward writes","upvote_count":"1","poster":"WhyIronMan"},{"timestamp":"1704828000.0","poster":"3a632a3","comment_id":"1117777","content":"Technically, none of these solutions meet the requirement. Aurora Global Databases still have the issue of lag and stale data depending on the configuration. There is only one writer that replicates the data to the read replica in the second region. See the following in the documentation to understand write forwarding latency: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database-write-forwarding-ams.html#aurora-global-database-write-forwarding-isolation-ams\nOption C comes the closest. They may mean Group Replication. You can have each instance perform their own read and write operations. But there isn't a concept of write forwarding. However, there are a lot of limitations for doing this. \nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/mysql-active-active-clusters.html","upvote_count":"1"},{"timestamp":"1677830160.0","comment_id":"827747","content":"Selected Answer: A\nhttps://aws.amazon.com/getting-started/hands-on/migrate-rdsmysql-to-auroramysql/#:~:text=2.1%20%2D%20Open%20the%20Amazon%20RDS,choose%20Create%20Aurora%20read%20replica.\nFirst a read replica has to be created and that could be promoted to a cluster ..","poster":"andras","upvote_count":"1"},{"content":"Selected Answer: D\nhesitated between A and D but sence A it saying pause the write so ...","comment_id":"792078","poster":"zozza2023","upvote_count":"2","timestamp":"1675027620.0"},{"timestamp":"1673541900.0","upvote_count":"2","content":"Selected Answer: D\nThe solution that meets these requirements is to convert the RDS for MySQL DB instance to an Amazon Aurora MySQL DB cluster and adding eu-west-1 as a secondary Region to the DB cluster, this will enable write forwarding on the DB cluster which will allow the customers in Europe and the customers in the US to have access to the same data and write to the database and see updates from the other group in real time. Also, deploying the application in eu-west-1 and configuring the application to use the Aurora MySQL endpoint in eu-west-1, will ensure that customers in Europe will have low latency and not stale data. Aurora uses a native replication feature to propagate writes to all Aurora replicas, regardless of their location and this makes it the best option for this use case.","poster":"masetromain","comment_id":"773706"},{"upvote_count":"1","timestamp":"1671712860.0","content":"Selected Answer: A\n\"A\" is the one.","comment_id":"753285","poster":"Kende"},{"upvote_count":"1","comment_id":"715082","content":"Selected Answer: A\nD seems to be wrong - To \"convert\" the RDS instance to Aurora, you'll need to restore from a snapshot - OR, deploy a read replica as per question A.\n\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Migrating.RDSMySQL.html","timestamp":"1668071820.0","poster":"janvandermerwer","comments":[{"comment_id":"773711","content":"This option creates an Aurora replica of the RDS for MySQL DB instance and promoting it to a standalone DB cluster and adding eu-west-1 as a secondary Region to the cluster, then enabling write forwarding on the DB cluster. While this will allow the customers in Europe and the customers in the US to have access to the same data and write to the database, this solution may not provide the best performance for customers in Europe as it requires the application to be reconfigured and writes to be paused during this process, which may cause delays and stale data. Additionally, this solution requires manual intervention to promote the replica, which may be prone to errors and increase the risk of data inconsistencies.","timestamp":"1673542140.0","upvote_count":"1","poster":"masetromain"}]},{"content":"Only 'A' will work","comment_id":"714844","poster":"sjpd10","timestamp":"1668043860.0","upvote_count":"1"},{"comment_id":"708083","upvote_count":"1","timestamp":"1667171100.0","poster":"Tokyo344","content":"Selected Answer: A\nI think A\n\nhttps://aws.amazon.com/blogs/database/best-practices-for-migrating-rds-for-mysql-databases-to-amazon-aurora/"},{"content":"Selected Answer: A\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Migrating.RDSMySQL.Replica.html","poster":"fdoxxx","comment_id":"702290","timestamp":"1666539540.0","comments":[{"content":"Amazon Aurora MySQL replica of the RDS for MySQL DB instance. How to do that?","upvote_count":"1","timestamp":"1666852620.0","comment_id":"705297","poster":"ToanVN1988"}],"upvote_count":"2"},{"comment_id":"702283","upvote_count":"1","poster":"kharakbeer","content":"Selected Answer: D\nVery good answer","timestamp":"1666538820.0"},{"upvote_count":"1","comment_id":"689816","timestamp":"1665278700.0","poster":"skywalker","content":"I am confuse if answer is A.. coz ..\n\nCreate an Amazon Aurora MySQL replica of the RDS for MySQL DB instance (At this stage, Aurora should be empty since no migration yet) . Pause application writes to the RDS DB instance. Promote the Aurora Replica to a standalone DB cluster. Reconfigure the application to use the Aurora database and resume writes. ...... What happen to the existing data in RDS? No need to migrate?? \nCannot be B and C.... \n\nD. wise.. you can convert RDSD to mySQLDB direcly.. but here it didn't mentioned direct conversion.. if they mentioned migrate RDS to AuroraDB... then the whole statement would be wrong. \n\nAgain.. badlly worded option here."},{"comment_id":"688810","upvote_count":"3","poster":"Malluchan","timestamp":"1665160140.0","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Migrating.RDSMySQL.Replica.html"},{"content":"Selected Answer: A\nA\nYou cannot convert RDS to Aurora, you need to create an Aurora Replica first and promote to a standalone DB cluster","timestamp":"1664787300.0","poster":"JohnPi","upvote_count":"2","comments":[{"upvote_count":"1","poster":"JohnPi","comment_id":"702306","content":"https://aws.amazon.com/getting-started/hands-on/migrate-rdsmysql-to-auroramysql/","timestamp":"1666541280.0"},{"timestamp":"1666014840.0","content":"you should not sit the exam, if that is what you think","comment_id":"697447","poster":"Bilal_M","upvote_count":"4"}],"comment_id":"685328"},{"timestamp":"1664322480.0","poster":"vbloise","upvote_count":"4","comment_id":"681305","content":"Selected Answer: D\nIt's D: \nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database-write-forwarding.html"},{"comment_id":"669689","timestamp":"1663231320.0","upvote_count":"1","content":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Migrating.RDSMySQL.Replica.html","poster":"Cloudxie"}],"question_id":941,"unix_timestamp":1663231320,"isMC":true,"answer":"A","exam_id":32,"question_text":"A company has deployed its database on an Amazon RDS for MySQL DB instance in the us-east-1 Region. The company needs to make its data available to customers in Europe. The customers in Europe must have access to the same data as customers in the United States (US) and will not tolerate high application latency or stale data. The customers in Europe and the customers in the US need to write to the database. Both groups of customers need to see updates from the other group in real time.\nWhich solution will meet these requirements?","answer_description":"","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/82256-exam-aws-certified-solutions-architect-professional-topic-1/","timestamp":"2022-09-15 10:42:00","question_images":[]},{"id":"f7dUEKGr4fis7btwrmgZ","question_id":942,"topic":"1","answers_community":["C (100%)"],"choices":{"C":"Enable Dynamo DB auto scaling for the table.","D":"Purchase 1-year reserved capacity that is sufficient to cover the peak load for 4 hours each day.","B":"Change the DynamoDB table to use on-demand capacity.","A":"Reduce the provisioned RCUs and WCUs."},"unix_timestamp":1661952960,"timestamp":"2022-08-31 15:36:00","isMC":true,"exam_id":32,"answer_ET":"C","answer":"C","answer_description":"","discussion":[{"poster":"AwsBRFan","upvote_count":"6","content":"Selected Answer: C\nhttps://aws.amazon.com/blogs/database/amazon-dynamodb-auto-scaling-performance-and-cost-optimization-at-any-scale/\n\"As you can see, there are compelling reasons to use DynamoDB auto scaling with actively changing traffic. Auto scaling responds quickly and simplifies capacity management, which lowers costs by scaling your table’s provisioned capacity and reducing operational overhead.\"","comment_id":"664149","timestamp":"1662690000.0","comments":[{"timestamp":"1673542320.0","poster":"masetromain","upvote_count":"1","comment_id":"773718","content":"While option A. Reduce the provisioned RCUs and WCUs, will help in reducing the cost, it may not be effective during the peak hours and the company may experience throttling and slow response times.\n\nOption B. Change the DynamoDB table to use on-demand capacity, can be a good option but it may end up being more expensive than auto scaling during peak hours.\n\nOption D. Purchase 1-year reserved capacity that is sufficient to cover the peak load for 4 hours each day, will ensure that the company has enough capacity during peak hours, but it may be an over-provisioning during the off-peak hours and end up being more expensive."}]},{"timestamp":"1689709620.0","content":"Correct C.","comment_id":"955849","poster":"ggrodskiy","upvote_count":"1"},{"comment_id":"773719","timestamp":"1673542380.0","upvote_count":"1","content":"While option A. Reduce the provisioned RCUs and WCUs, will help in reducing the cost, it may not be effective during the peak hours and the company may experience throttling and slow response times.\n\nOption B. Change the DynamoDB table to use on-demand capacity, can be a good option but it may end up being more expensive than auto scaling during peak hours.\n\nOption D. Purchase 1-year reserved capacity that is sufficient to cover the peak load for 4 hours each day, will ensure that the company has enough capacity during peak hours, but it may be an over-provisioning during the off-peak hours and end up being more expensive.","poster":"masetromain"},{"timestamp":"1667728620.0","content":"Why not D? Key is cost-efficient with predictible usage. You can reserve RCU and WCU: https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-reservation-models/amazon-dynamodb-reservations.html","poster":"Dozer","comment_id":"712234","upvote_count":"3"},{"timestamp":"1667245380.0","content":"Selected Answer: C\nCCC Auto Scaling !","upvote_count":"1","poster":"Blair77","comment_id":"708694"},{"timestamp":"1666539780.0","poster":"fdoxxx","comment_id":"702293","upvote_count":"1","content":"Selected Answer: C\nAutoScaling will adjust RCU and WCU to current needs - not only 4h peak"},{"content":"The Q says 'predictable model with high sales traffic for 4 hours daily' and 'meets these requirements MOST cost-effectively' - how can it be auto-scaling ? \n\nWith On-demand, DynamoDB would scale when needed (4 hr window). With auto-scaling, aren't you billed for the provisioned capacity ?\n\nI think the answer is 'B'","poster":"sjpd10","timestamp":"1666364220.0","comments":[{"timestamp":"1668044940.0","comment_id":"714856","content":"'C'\n\nAuto-scaling adjusts RCU & WCU based on the requirement for those 4 hrs of PEAK.","upvote_count":"1","poster":"sjpd10"}],"upvote_count":"3","comment_id":"701008"},{"poster":"aloha123","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.Console.html","comment_id":"655127","upvote_count":"3","timestamp":"1661952960.0"}],"answer_images":[],"question_text":"A company operates quick-service restaurants. The restaurants follow a predictable model with high sales traffic for 4 hours daily. Sales traffic is lower outside of those peak hours.\nThe point of sale and management platform is deployed in the AWS Cloud and has a backend that is based on Amazon DynamoDB. The database table uses provisioned throughput mode with 100,000 RCUs and 80,000 WCUs to match known peak resource consumption.\nThe company wants to reduce its DynamoDB cost and minimize the operational overhead for the IT staff.\nWhich solution meets these requirements MOST cost-effectively?","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/78709-exam-aws-certified-solutions-architect-professional-topic-1/"},{"id":"PbM1b2ARNWMuoIBEbtGj","answers_community":["A (50%)","C (50%)"],"answer_description":"","question_text":"An administrator is using Amazon CloudFormation to deploy a three tier web application that consists of a web tier and application tier that will utilize Amazon\nDynamoDB for storage when creating the CloudFormation template.\nWhich of the following would allow the application instance access to the DynamoDB tables without exposing API credentials?","choices":{"D":"Create an identity and Access Management user in the CloudFormation template that has permissions to read and write from the required DynamoDB table, use the GetAtt function to retrieve the Access and secret keys and pass them to the application instance through user-data.","B":"Use the Parameter section in the Cloud Formation template to nave the user input Access and Secret Keys from an already created IAM user that has me permissions required to read and write from the required DynamoDB table.","A":"Create an Identity and Access Management Role that has the required permissions to read and write from the required DynamoDB table and associate the Role to the application instances by referencing an instance profile.","C":"Create an Identity and Access Management Role that has the required permissions to read and write from the required DynamoDB table and reference the Role in the instance profile property of the application instance."},"discussion":[{"content":"A: \"...and associate the Role to the application instances by referencing an instance profile\"\n\nC: \"...and reference the Role in the instance profile property of the application instance\"\n\nThe correct answer is not C, as the instance profile property of an instance doesn't refer to an IAM Role, it refers to an Instance Profile, which is a resource in its own right.\n\nTherefore, the correct answer is A.","comments":[{"poster":"ExtHo","timestamp":"1634233020.0","content":"It would be A if you were doing it directly in EC2. However, since it says you have to use CloudFormation then you have to add the reference to the profile in the template. This means the answer is C.\n\nWhen we doing with CloudFormation Template we need to update under properties. See following.\n\nType: AWS::IAM::InstanceProfile\nProperties:\nInstanceProfileName: String\nPath: String\nRoles:\n- String","upvote_count":"15","comments":[{"upvote_count":"2","poster":"01037","timestamp":"1635775500.0","content":"Yes it's C","comment_id":"370481"}],"comment_id":"331661"}],"comment_id":"324347","upvote_count":"7","timestamp":"1632220560.0","poster":"Dgix"},{"upvote_count":"1","timestamp":"1742141700.0","content":"Selected Answer: C\nThe correct answer is C. You've got to take a closer look at the difference between A & C","poster":"codeScalable","comment_id":"1399285"},{"upvote_count":"1","comment_id":"1266995","timestamp":"1723806180.0","poster":"amministrazione","content":"A. Create an Identity and Access Management Role that has the required permissions to read and write from the required DynamoDB table and associate the Role to the application instances by referencing an instance profile."},{"timestamp":"1707850440.0","comment_id":"1149490","poster":"JPA210","upvote_count":"1","content":"Selected Answer: A\nI would go to A like TigerInTheCloud say: I prefer A. Normally we, at least me, use the term \"attach\" or \"associate\". AWS CLI iam subcommand 'add-role-to-instance-profile' and ec2 subcommand 'associate-iam-instance-profile' perform the task. Also, just checked, term, associate not reference, is used in AWS document.\nC is not wrong , is true that we reference the Role in the instance profile, but we just do not use that term. We use attach or associate."},{"upvote_count":"1","comment_id":"918833","content":"Selected Answer: C\nThe correct answer is option C: Create an Identity and Access Management (IAM) Role that has the required permissions to read and write from the required DynamoDB table and reference the Role in the instance profile property of the application instance.\n\nBy creating an IAM Role and associating it with the instance profile of the application instances, you can grant the necessary permissions to access DynamoDB without exposing API credentials. The IAM Role acts as a set of temporary security credentials that can be assumed by the instances. This approach follows the recommended security best practices of AWS.","timestamp":"1686277860.0","poster":"SkyZeroZx"},{"poster":"TigerInTheCloud","timestamp":"1672005480.0","content":"Selected Answer: A\nEnglish test :-)\nI prefer A. Normally we, at least me, use the term \"attach\" or \"associate\". AWS CLI iam subcommand 'add-role-to-instance-profile' and ec2 subcommand 'associate-iam-instance-profile' perform the task.\nAlso, just checked, term, associate not reference, is used in AWS document, https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html","comments":[{"upvote_count":"1","timestamp":"1672042260.0","content":"Question ask for Cloudformation template and not Cli. Makes all the difference.","comment_id":"757220","poster":"hobokabobo"}],"upvote_count":"1","comment_id":"756033"},{"upvote_count":"2","poster":"cldy","content":"C. Create an Identity and Access Management Role that has the required permissions to read and write from the required DynamoDB table and reference the Role in the instance profile property of the application instance.","comment_id":"496790","timestamp":"1638964920.0"}],"answer_images":[],"topic":"1","question_id":943,"unix_timestamp":1617116520,"answer_ET":"C","answer":"A","exam_id":32,"timestamp":"2021-03-30 17:02:00","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/48522-exam-aws-certified-solutions-architect-professional-topic-1/","isMC":true},{"id":"NhKFzCSm7ZCi0QsP6Jnq","timestamp":"2022-09-05 13:25:00","question_text":"A company is using AWS Organizations to manage multiple accounts. Due to regulatory requirements, the company wants to restrict specific member accounts to certain AWS Regions, where they are permitted to deploy resources. The resources in the accounts must be tagged, enforced based on a group standard, and centrally managed with minimal configuration.\nWhat should a solutions architect do to meet these requirements?","question_images":[],"isMC":true,"question_id":944,"answer_ET":"D","url":"https://www.examtopics.com/discussions/amazon/view/80310-exam-aws-certified-solutions-architect-professional-topic-1/","answer_images":[],"choices":{"C":"Associate the specific member accounts with the root. Apply a tag policy and an SCP using conditions to limit Regions.","A":"Create an AWS Config rule in the specific member accounts to limit Regions and apply a tag policy.","B":"From the AWS Billing and Cost Management console, in the management account, disable Regions for the specific member accounts and apply a tag policy on the root.","D":"Associate the specific member accounts with a new OU. Apply a tag policy and an SCP using conditions to limit Regions."},"answers_community":["D (100%)"],"exam_id":32,"unix_timestamp":1662377100,"answer_description":"","discussion":[{"comments":[{"content":"Link for an example:\nhttps://aws.amazon.com/es/blogs/mt/implement-aws-resource-tagging-strategy-using-aws-tag-policies-and-service-control-policies-scps/","poster":"joancarles","comment_id":"685512","timestamp":"1664799180.0","upvote_count":"1"}],"upvote_count":"8","timestamp":"1662377100.0","poster":"SGES","content":"D - Agreed putting those member accounts with OU then use tagging policy and SCP based conditions to achieve required compliance.","comment_id":"660117"},{"content":"Correct D.","comment_id":"949827","timestamp":"1689167880.0","poster":"ggrodskiy","upvote_count":"1"},{"comment_id":"773726","content":"Selected Answer: D\nBy applying a tag policy and an SCP using conditions to limit Regions, the architect can ensure that resources in the specific member accounts are tagged and deployed only in the allowed regions, which will meet the regulatory requirements.\n\nOption A: Create an AWS Config rule in the specific member accounts to limit Regions and apply a tag policy, while this will limit the regions and apply a tag policy, it does not provide centralized management and enforcement of the restriction.\n\nOption B: From the AWS Billing and Cost Management console, in the management account, disable Regions for the specific member accounts and apply a tag policy on the root, this will disable regions for the specific member accounts and apply a tag policy, however, it does not provide a way to enforce the restriction or provide centralized management.","comments":[{"timestamp":"1673542680.0","comments":[{"comment_id":"773728","upvote_count":"1","content":"In contrast, creating a new organizational unit (OU) in the management account and associating the specific member accounts with that OU, as in option D, provides a clear boundary and a logical grouping of the member accounts that are subject to the regulatory compliance requirements. This approach makes it easier to identify which accounts are subject to the requirements, and it makes it more straightforward to manage and monitor compliance.","poster":"masetromain","timestamp":"1673542680.0"}],"comment_id":"773727","upvote_count":"1","content":"Option C: Associate the specific member accounts with a new OU. Apply a tag policy and an SCP using conditions to limit Regions, this will limit the regions and apply a tag policy, but it does not provide centralized management and enforcement of the restriction.\n\nWhile option C, associating the specific member accounts with the root account, apply a tag policy and an SCP using conditions to limit Regions, is one way to achieve the regulatory compliance requirements, it may not be the most optimal solution. This approach doesn't provide a clear boundary between the member accounts that are subject to the regulatory compliance requirements and other member accounts that are not. Also, it could be harder to identify which accounts are subject to the requirements, as it could be lost in the bigger scope of the root account, this can make it more difficult to manage and monitor compliance.","poster":"masetromain"}],"upvote_count":"1","poster":"masetromain","timestamp":"1673542680.0"},{"poster":"akash_it","upvote_count":"2","content":"Selected Answer: D\nAgree with comments","timestamp":"1664437080.0","comment_id":"682447"}],"answer":"D","topic":"1"},{"id":"xZPtuaK9kPE2S7t2nr3n","topic":"1","discussion":[{"poster":"SGES","content":"Answer is B\nThe objects in the bucket are encrypted therefore IAM role must have permission for decryption","upvote_count":"10","timestamp":"1662198420.0","comment_id":"658296"},{"timestamp":"1675019340.0","poster":"zozza2023","upvote_count":"1","comment_id":"791966","content":"Selected Answer: B\nError 403= meaning a missing permissons to s3"},{"comments":[{"poster":"masetromain","content":"Option A: Ensure that blocking all public access has not been enabled in the S3 bucket, while this is important, this is not the cause of the issue since the developer is using an S3 endpoint inside a VPC, not public access.\n\nOption C: Verify that the IAM role has the correct trust relationship configured, while this is important, this is not the cause of the issue since the developer is able to assume the correct IAM role.\n\nOption D: Check that local firewall rules are not preventing access to the S3 endpoint, While this is important, this is not the cause of the issue since the developer is able to assume the correct IAM role, and the S3 bucket policy and the NACL are also valid.","upvote_count":"1","timestamp":"1673542860.0","comment_id":"773733"}],"upvote_count":"1","content":"Selected Answer: B\nB. Verify that the IAM role has permission to decrypt the referenced KMS key.\n\nThe developer is receiving an Error 403: Access Denied message when trying to download an object from the S3 bucket, this means that the developer has the necessary permissions to access the S3 bucket but something else is preventing the access. Since the bucket is encrypted with an AWS KMS key and the developer is assuming the correct IAM role, it is likely that the issue is related to the KMS key. The solutions architect should verify that the IAM role has the correct permissions to decrypt the referenced KMS key. Without the correct permissions to decrypt the key, the developer will not be able to access the object even though they have the necessary permissions to access the S3 bucket.","poster":"masetromain","timestamp":"1673542800.0","comment_id":"773732"},{"timestamp":"1667652840.0","upvote_count":"1","content":"Selected Answer: B\nB it is","comment_id":"711753","poster":"Ni_yot"},{"comment_id":"664894","content":"Selected Answer: B\nB also here","timestamp":"1662749880.0","upvote_count":"3","poster":"AwsBRFan"}],"question_text":"A developer reports receiving an Error 403: Access Denied message when they try to download an object from an Amazon S3 bucket. The S3 bucket is accessed using an S3 endpoint inside a VPC, and is encrypted with an AWS KMS key. A solutions architect has verified that the developer is assuming the correct IAM role in the account that allows the object to be downloaded. The S3 bucket policy and the NACL are also valid.\nWhich additional step should the solutions architect take to troubleshoot this issue?","answer":"B","answer_ET":"B","choices":{"A":"Ensure that blocking all public access has not been enabled in the S3 bucket.","C":"Verify that the IAM role has the correct trust relationship configured.","D":"Check that local firewall rules are not preventing access to the S3 endpoint.","B":"Verify that the IAM role has permission to decrypt the referenced KMS key."},"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/79735-exam-aws-certified-solutions-architect-professional-topic-1/","exam_id":32,"timestamp":"2022-09-03 11:47:00","answer_images":[],"isMC":true,"unix_timestamp":1662198420,"question_id":945,"answers_community":["B (100%)"],"question_images":[]}],"exam":{"lastUpdated":"11 Apr 2025","isMCOnly":false,"name":"AWS Certified Solutions Architect - Professional","isBeta":false,"provider":"Amazon","numberOfQuestions":1019,"id":32,"isImplemented":true},"currentPage":189},"__N_SSP":true}