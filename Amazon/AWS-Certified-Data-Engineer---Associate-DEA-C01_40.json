{"pageProps":{"questions":[{"id":"lF89nQz3ngxh6PM1Yc1R","answers_community":["C (100%)"],"isMC":true,"discussion":[{"timestamp":"1718899680.0","comment_id":"1233772","poster":"FunkyFresco","upvote_count":"5","content":"Selected Answer: C\nuse EXPLAIN ANALIZE \nhttps://docs.aws.amazon.com/athena/latest/ug/athena-explain-statement.html"},{"comment_id":"1241795","content":"Selected Answer: C\nexplain analyze + select * from table","upvote_count":"1","timestamp":"1720065180.0","poster":"HunkyBunky"},{"poster":"tgv","upvote_count":"4","content":"Selected Answer: C\nA - Only partially meets the requirements as it does not include computational costs.\nB - Incorrect syntax and does not meet the requirements.\nC - Fully meets the requirements by providing both the execution plan and the computational costs.\nD - Incorrect syntax and does not meet the requirements.","timestamp":"1718437560.0","comment_id":"1230811"}],"url":"https://www.examtopics.com/discussions/amazon/view/142563-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","timestamp":"2024-06-15 09:46:00","topic":"1","answer_images":[],"choices":{"A":"EXPLAIN SELECT * FROM sales;","D":"EXPLAIN FROM sales;","B":"EXPLAIN ANALYZE FROM sales;","C":"EXPLAIN ANALYZE SELECT * FROM sales;"},"exam_id":21,"question_text":"A data engineer wants to improve the performance of SQL queries in Amazon Athena that run against a sales data table.\n\nThe data engineer wants to understand the execution plan of a specific SQL statement. The data engineer also wants to see the computational cost of each operation in a SQL query.\n\nWhich statement does the data engineer need to run to meet these requirements?","question_images":[],"answer_ET":"C","unix_timestamp":1718437560,"answer_description":"","question_id":196,"answer":"C"},{"id":"ypALFrZLCZIyw8BIdHLU","answer_images":[],"choices":{"C":"Choose the STANDARD execution class in the Glue job properties.","D":"Choose the latest version in the GlueVersion field in the Glue job properties.","A":"Choose the FLEX execution class in the Glue job properties.","B":"Use the Spot Instance type in Glue job properties."},"timestamp":"2024-02-01 05:29:00","question_images":[],"exam_id":21,"unix_timestamp":1706761740,"answer":"A","question_text":"A data engineer needs to schedule a workflow that runs a set of AWS Glue jobs every day. The data engineer does not require the Glue jobs to run or finish at a specific time.\nWhich solution will run the Glue jobs in the MOST cost-effective way?","answers_community":["A (100%)"],"topic":"1","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/132628-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer_description":"","discussion":[{"content":"Selected Answer: A\nThe FLEX execution class leverages spare capacity within the AWS infrastructure to run Glue jobs at a discounted price compared to the standard execution class. Since the data engineer doesn't have specific time constraints, utilizing spare capacity is ideal for cost savings.\nToday's date its a checkbox in order to spare capacity and will mean we dont know when is going to finish, which is recommended to increase a timeout","comment_id":"1226790","upvote_count":"7","poster":"pypelyncar","timestamp":"1717858680.0"},{"upvote_count":"6","content":"A. Choose the FLEX execution class in the Glue job properties.\n\nExplanation:\nThe FLEX execution class in AWS Glue allows jobs to use idle resources within the Glue service, which can significantly reduce costs compared to the STANDARD execution class. With FLEX, Glue jobs run when resources are available, which is a cost-effective approach for jobs that don't need to be completed within a specific timeframe.","timestamp":"1706818200.0","comment_id":"1137887","poster":"TonyStark0122"},{"poster":"GabrielSGoncalves","content":"Selected Answer: A\nFLEX is how you lower Glue cost when you dont have urgency to run ETLs.","upvote_count":"1","timestamp":"1721834280.0","comment_id":"1254500"},{"comment_id":"1209087","timestamp":"1715293320.0","content":"Selected Answer: A\nAs its said the FLEX job comes cheaper that hiring a spot instance","upvote_count":"3","poster":"k350Secops"},{"content":"Selected Answer: A\nI'd go with A","upvote_count":"1","timestamp":"1712087880.0","poster":"lucas_rfsb","comment_id":"1188287"},{"timestamp":"1706761740.0","comment_id":"1137269","upvote_count":"5","poster":"lalitjhawar","content":"A\nFlex allows you to optimize your costs on your non-urgent or non-time sensitive data integration workloads such as testing, and one-time data loads. With Flex, AWS Glue jobs run on spare compute capacity instead of dedicated hardware. The start and runtimes of jobs using Flex can vary because spare compute resources arenâ€™t readily available and can be reclaimed during the run of a job\n\nhttps://aws.amazon.com/blogs/big-data/introducing-aws-glue-flex-jobs-cost-savings-on-etl-workloads/"}],"answer_ET":"A","question_id":197},{"id":"XL69xrSDiTPxT7DGhuc4","question_text":"A company plans to provision a log delivery stream within a VPC. The company configured the VPC flow logs to publish to Amazon CloudWatch Logs. The company needs to send the flow logs to Splunk in near real time for further analysis.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","unix_timestamp":1718437680,"timestamp":"2024-06-15 09:48:00","exam_id":21,"question_id":198,"answer":"B","url":"https://www.examtopics.com/discussions/amazon/view/142564-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answers_community":["B (100%)"],"question_images":[],"choices":{"C":"Create an Amazon Kinesis Data Firehose delivery stream to use Splunk as the destination. Create an AWS Lambda function to send the flow logs from CloudWatch Logs to the delivery stream.","B":"Create an Amazon Kinesis Data Firehose delivery stream to use Splunk as the destination. Create a CloudWatch Logs subscription filter to send log events to the delivery stream.","D":"Configure an Amazon Kinesis Data Streams data stream to use Splunk as the destination. Create an AWS Lambda function to send the flow logs from CloudWatch Logs to the data stream.","A":"Configure an Amazon Kinesis Data Streams data stream to use Splunk as the destination. Create a CloudWatch Logs subscription filter to send log events to the data stream."},"topic":"1","answer_ET":"B","isMC":true,"answer_images":[],"discussion":[{"poster":"tgv","timestamp":"1718437680.0","content":"Selected Answer: B\nKinesis Data Firehose has built-in support for Splunk as a destination, making the integration straightforward. Using a CloudWatch Logs subscription filter directly to Firehose simplifies the data flow, eliminating the need for additional Lambda functions or custom integrations.","upvote_count":"6","comment_id":"1230814"},{"content":"Selected Answer: B\nCreating an Amazon Kinesis Data Firehose delivery stream to use Splunk as the destination and creating a CloudWatch Logs subscription filter to send log events to the delivery stream would meet these requirements with the least operational overhead.\n\nAmazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics services. It can capture, transform, and deliver streaming data to Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, generic HTTP endpoints, and service providers like Splunk.\n\nCloudWatch Logs subscription filters allow you to send real-time log events to Kinesis Data Firehose and are ideal for scenarios where you want to forward the logs to other services for further analysis.\n\nOptions A and D involve Kinesis Data Streams, which would require additional management and operational overhead. Option C involves creating a Lambda function, which also adds operational overhead. Therefore, option B is the best choice.","upvote_count":"4","timestamp":"1720130400.0","comment_id":"1242310","poster":"bakarys"}],"answer_description":""},{"id":"Vn7FPeskwiBIURn1NckS","unix_timestamp":1718437860,"isMC":true,"discussion":[{"upvote_count":"6","timestamp":"1720386720.0","comment_id":"1244008","content":"Selected Answer: A\nCorrect Solution:\nA. Set up AWS Lake Formation. Define security policy-based rules for the users and applications by IAM role in Lake Formation.\n\nExplanation:\nAWS Lake Formation: This service simplifies and automates the process of securing and managing data lakes. It allows you to define fine-grained access control policies at the database, table, and column levels.\nSecurity Policy-Based Rules: Lake Formation allows you to create policies that specify which users or roles have access to specific data, including column-level access controls. This makes it easier to manage access based on roles and responsibilities.","poster":"Ja13"},{"upvote_count":"1","timestamp":"1734697260.0","comment_id":"1329435","poster":"HagarTheHorrible","content":"Selected Answer: A\nA lake formation for any fine-grained access"},{"upvote_count":"1","content":"Selected Answer: A\nA - Lake formation","comment_id":"1234146","poster":"HunkyBunky","timestamp":"1718932680.0"},{"content":"Selected Answer: A\nLake Formation supports fine-grained access control, including column-level permissions.","poster":"tgv","comment_id":"1230815","upvote_count":"4","timestamp":"1718437860.0"}],"answer":"A","question_text":"A company has a data lake on AWS. The data lake ingests sources of data from business units. The company uses Amazon Athena for queries. The storage layer is Amazon S3 with an AWS Glue Data Catalog as a metadata repository.\n\nThe company wants to make the data available to data scientists and business analysts. However, the company first needs to manage fine-grained, column-level data access for Athena based on the user roles and responsibilities.\n\nWhich solution will meet these requirements?","answer_images":[],"answer_description":"","answers_community":["A (100%)"],"topic":"1","exam_id":21,"question_id":199,"choices":{"B":"Define an IAM resource-based policy for AWS Glue tables. Attach the same policy to IAM user groups.","A":"Set up AWS Lake Formation. Define security policy-based rules for the users and applications by IAM role in Lake Formation.","D":"Create a resource share in AWS Resource Access Manager (AWS RAM) to grant access to IAM users.","C":"Define an IAM identity-based policy for AWS Glue tables. Attach the same policy to IAM roles. Associate the IAM roles with IAM groups that contain the users."},"url":"https://www.examtopics.com/discussions/amazon/view/142565-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","timestamp":"2024-06-15 09:51:00","question_images":[],"answer_ET":"A"},{"id":"n5PPm7nmYeLrZ1gXh6P1","question_text":"A company has developed several AWS Glue extract, transform, and load (ETL) jobs to validate and transform data from Amazon S3. The ETL jobs load the data into Amazon RDS for MySQL in batches once every day. The ETL jobs use a DynamicFrame to read the S3 data.\n\nThe ETL jobs currently process all the data that is in the S3 bucket. However, the company wants the jobs to process only the daily incremental data.\n\nWhich solution will meet this requirement with the LEAST coding effort?","unix_timestamp":1718438040,"exam_id":21,"timestamp":"2024-06-15 09:54:00","answer":"B","question_id":200,"url":"https://www.examtopics.com/discussions/amazon/view/142566-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answers_community":["B (100%)"],"question_images":[],"choices":{"B":"Enable job bookmarks for the ETL jobs to update the state after a run to keep track of previously processed data.","A":"Create an ETL job that reads the S3 file status and logs the status in Amazon DynamoDB.","C":"Enable job metrics for the ETL jobs to help keep track of processed objects in Amazon CloudWatch.","D":"Configure the ETL jobs to delete processed objects from Amazon S3 after each run."},"topic":"1","answer_ET":"B","isMC":true,"answer_images":[],"discussion":[{"poster":"tgv","upvote_count":"8","content":"Selected Answer: B\nAWS Glue job bookmarks are designed to handle incremental data processing by automatically tracking the state.","comment_id":"1230816","timestamp":"1718438040.0"},{"upvote_count":"1","comment_id":"1249191","poster":"andrologin","content":"Selected Answer: B\nAWS Glue Bookmarks can be used to pin where the data processing last stopped hence help with incremental processing.","timestamp":"1721159220.0"},{"timestamp":"1720065120.0","comment_id":"1241794","content":"Selected Answer: B\nB - bookmarks is a key","upvote_count":"1","poster":"HunkyBunky"},{"upvote_count":"3","poster":"bakarys","content":"Selected Answer: B\nThe solution that will meet this requirement with the least coding effort is Option B: Enable job bookmarks for the ETL jobs to update the state after a run to keep track of previously processed data.\n\nAWS Glue job bookmarks help ETL jobs to keep track of data that has already been processed during previous runs. By enabling job bookmarks, the ETL jobs can skip the processed data and only process the new, incremental data. This feature is designed specifically for this use case and requires minimal coding effort.\n\nOptions A, C, and D would require additional coding and operational effort. Option A would require creating a new ETL job and managing a DynamoDB table. Option C would involve setting up job metrics and CloudWatch, which doesnâ€™t directly address processing incremental data. Option D would involve deleting data from S3 after processing, which might not be desirable if the original data needs to be retained. Therefore, Option B is the most suitable solution.","timestamp":"1719919620.0","comment_id":"1240723"}],"answer_description":""}],"exam":{"isBeta":false,"isImplemented":true,"provider":"Amazon","isMCOnly":true,"lastUpdated":"11 Apr 2025","numberOfQuestions":207,"name":"AWS Certified Data Engineer - Associate DEA-C01","id":21},"currentPage":40},"__N_SSP":true}