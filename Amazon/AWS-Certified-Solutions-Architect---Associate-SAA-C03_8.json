{"pageProps":{"questions":[{"id":"2wfzAugG9MBMZ3V7vfNx","choices":{"A":"Use AWS Fargate on Amazon Elastic Container Service (Amazon ECS) to run the containerized web application with Service Auto Scaling. Use an Application Load Balancer to distribute the incoming requests.","B":"Use two Amazon EC2 instances to host the containerized web application. Use an Application Load Balancer to distribute the incoming requests.","C":"Use AWS Lambda with a new code that uses one of the supported languages. Create multiple Lambda functions to support the load. Use Amazon API Gateway as an entry point to the Lambda functions.","D":"Use a high performance computing (HPC) solution such as AWS ParallelCluster to establish an HPC cluster that can process the incoming requests at the appropriate scale."},"answer_ET":"A","isMC":true,"answers_community":["A (100%)"],"unix_timestamp":1666186680,"exam_id":31,"discussion":[{"timestamp":"1666186680.0","comment_id":"699063","content":"Selected Answer: A\nLess operational overhead means A: Fargate (no EC2), move the containers on ECS, autoscaling for growth and ALB to balance consumption.\nB - requires configure EC2\nC - requires add code (developpers)\nD - seems like the most complex approach, like re-architecting the app to take advantage of an HPC platform.","upvote_count":"20","poster":"123jhl0"},{"comment_id":"1337592","poster":"satyaammm","timestamp":"1736261640.0","content":"Selected Answer: A\nAWS Fargate is a serverless compute engine and is fully managed by AWS. So it is more suitable here as it provides less operational overhead.","upvote_count":"1"},{"content":"Selected Answer: A\nAnd A - its containers so use FarGate, add some auto-scaling...","comment_id":"1284695","timestamp":"1726490760.0","upvote_count":"2","poster":"PaulGa"},{"content":"Selected Answer: A\nA makes sense","comment_id":"1253276","poster":"jaradat02","timestamp":"1721681040.0","upvote_count":"1"},{"content":"Selected Answer: A\nkey = LEAST operational overhead\n\nFargate a serverless service fully managed by aws\nhttps://docs.aws.amazon.com/AmazonECS/latest/userguide/what-is-fargate.html#:~:text=AWS%20Fargate%20is,optimize%20cluster%20packing.","comment_id":"1061009","upvote_count":"3","poster":"cosmiccliff","timestamp":"1698971640.0"},{"content":"Less operational overhead means A: Fargate (no EC2), move the containers on ECS, autoscaling for growth and ALB to balance consumption.","timestamp":"1698599640.0","upvote_count":"2","comment_id":"1057012","poster":"Ruffyit"},{"content":"Selected Answer: A\nLEAST operational overhead = AWS Fargate","upvote_count":"2","comment_id":"992780","timestamp":"1693284600.0","poster":"TariqKipkemei"},{"upvote_count":"3","content":"Selected Answer: A\nA is the best solution to meet the requirements with the least operational overhead. The key reasons are:\n\nAWS Fargate removes the need to provision and manage servers. Fargate will automatically scale the application based on demand. This removes a significant operational burden.\nUsing ECS along with Fargate provides a managed orchestration layer to easily run and scale the containerized application.\nThe Application Load Balancer handles distribution of traffic without additional effort.\nNo code changes are required to move the application to Fargate. The containers can run as-is.","comment_id":"981907","poster":"Guru4Cloud","timestamp":"1692122880.0"},{"content":"Selected Answer: A\nA is the correct answer.\n\nAWS Fargate removes the need to provision and manage servers, allowing you to focus on deploying and running applications. Fargate will scale compute capacity up and down automatically based on application load. This removes the operational overhead of managing servers.","timestamp":"1692119760.0","poster":"Guru4Cloud","upvote_count":"1","comment_id":"981866"},{"comment_id":"956567","upvote_count":"1","timestamp":"1689766920.0","poster":"james2033","content":"Selected Answer: A\nExisting: \"containerized web-app\", \"minimum code changes + minimum development effort\" --> AWS Fargate + Amazon Elastic Container Services (ECS). Easy question."},{"timestamp":"1689246900.0","upvote_count":"2","content":"A\nFargate, ECS, ASG, ALB….What else one will need for a nice sleep?","poster":"MNotABot","comment_id":"950602"},{"comment_id":"930415","content":"Selected Answer: A\nOption A (AWS Fargate on Amazon ECS with Service Auto Scaling) is the best choice as it provides a serverless and managed environment for your containerized web application. It requires minimal code changes, offers automatic scaling, and utilizes an Application Load Balancer for request distribution.\n\nOption B (Amazon EC2 instances with an Application Load Balancer) requires manual management of EC2 instances, resulting in more operational overhead compared to option A.\n\nOption C (AWS Lambda with API Gateway) may require significant code changes and restructuring, introducing complexity and potentially increasing development effort.\n\nOption D (AWS ParallelCluster) is not suitable for a containerized web application and involves significant setup and configuration overhead.","upvote_count":"3","timestamp":"1687432200.0","poster":"cookieMr"},{"comment_id":"907115","timestamp":"1685082900.0","upvote_count":"1","content":"Selected Answer: A\nAWS Fargate is a technology that you can use with Amazon ECS to run containers without having to manage servers or clusters of Amazon EC2 instances. With Fargate, you no longer have to provision, configure, or scale clusters of virtual machines to run containers. This removes the need to choose server types, decide when to scale your clusters, or optimize cluster packing.","poster":"Jeeva28"},{"poster":"studynoplay","upvote_count":"1","timestamp":"1683406020.0","content":"Selected Answer: A\nLeast Operational Overhead = Serverless","comment_id":"890997"},{"content":"Selected Answer: A\nAWS Fargate is a technology that you can use with Amazon ECS to run containers without having to manage servers on clusters of Amazon EC2 instances. With Fargate, you no longer have to provision, configure, or scale of virtual machines to run containers. \n\nhttps://docs.aws.amazon.com/AmazonECS/latest/userguide/what-is-fargate.html","timestamp":"1678797120.0","upvote_count":"1","poster":"airraid2010","comment_id":"838825"},{"content":"A is correct","timestamp":"1675914180.0","poster":"Chalamalli","upvote_count":"1","comment_id":"802802"},{"upvote_count":"3","poster":"Buruguduystunstugudunstuy","comment_id":"759152","timestamp":"1672183500.0","content":"Selected Answer: A\nThe best solution to meet the requirements with the least operational overhead is Option A: Use AWS Fargate on Amazon Elastic Container Service (Amazon ECS) to run the containerized web application with Service Auto Scaling. Use an Application Load Balancer to distribute the incoming requests."},{"timestamp":"1671395700.0","poster":"career360guru","comment_id":"749167","upvote_count":"1","content":"Selected Answer: A\nOption A has minimum operational overhead and almost no application code changes."},{"content":"A is correct","timestamp":"1669059600.0","poster":"Wpcorgan","comment_id":"723858","upvote_count":"1"},{"upvote_count":"3","content":"Selected Answer: A\nAgreed with A,\nlambda will work too but requires more operational overhead (more chores)\n\nwith A, you are just moving from an on-prem container to AWS container","poster":"Six_Fingered_Jose","comment_id":"704823","timestamp":"1666798440.0"}],"question_images":[],"answer":"A","question_text":"A company hosts a containerized web application on a fleet of on-premises servers that process incoming requests. The number of requests is growing quickly. The on-premises servers cannot handle the increased number of requests. The company wants to move the application to AWS with minimum code changes and minimum development effort.\nWhich solution will meet these requirements with the LEAST operational overhead?","topic":"1","answer_description":"","answer_images":[],"question_id":36,"timestamp":"2022-10-19 15:38:00","url":"https://www.examtopics.com/discussions/amazon/view/85913-exam-aws-certified-solutions-architect-associate-saa-c03/"},{"id":"EiYs84fWhoh6wKV5v4ix","unix_timestamp":1666186140,"answer_ET":"C","answer_description":"","exam_id":31,"question_id":37,"answer_images":[],"question_text":"A company uses 50 TB of data for reporting. The company wants to move this data from on premises to AWS. A custom application in the company’s data center runs a weekly data transformation job. The company plans to pause the application until the data transfer is complete and needs to begin the transfer process as soon as possible.\nThe data center does not have any available network bandwidth for additional workloads. A solutions architect must transfer the data and must configure the transformation job to continue to run in the AWS Cloud.\nWhich solution will meet these requirements with the LEAST operational overhead?","topic":"1","discussion":[{"comment_id":"699059","content":"Selected Answer: C\nA. Use AWS DataSync to move the data. Create a custom transformation job by using AWS Glue. - No BW available for DataSync, so \"asap\" will be weeks/months (?)\nB. Order an AWS Snowcone device to move the data. Deploy the transformation application to the device. - Snowcone will just store 14TB (SSD configuration).\n**C**. Order an AWS Snowball Edge Storage Optimized device. Copy the data to the device. Create a custom transformation job by using AWS Glue. - SnowBall can store 80TB (ok), takes around 1 week to move the device (faster than A), and AWS Glue allows to do ETL jobs. This is the answer.\nD. Order an AWS Snowball Edge Storage Optimized device that includes Amazon EC2 compute. Copy the data to the device. Create a new EC2 instance on AWS to run the transformation application. - Same as C, but the ETL job requires the deployment/configuration/maintenance of an EC2 instance, while Glue is serverless. This means D has more operational overhead than C.","timestamp":"1666186140.0","upvote_count":"74","poster":"123jhl0","comments":[{"upvote_count":"6","poster":"jdr75","comment_id":"862728","content":"I agree. When it said \"with least Operational overhead\" , it does not takes in account \"migration activities\" neccesary to reach the \"final photo/scenario\". In \"operational overhead\" schema, you're situated in a \"final scenario\" and you've only take into account how do you operate it, and if the operation of that scheme is ALIGHTED (least effort to operate than original scenario), that's the desired state.","timestamp":"1680762540.0"},{"poster":"remand","content":"I disagree on D. transformation job is already in place.so, all you have to do is deploy and run on ec2. \nC takes more effort to build Glue process, like reinventing the wheel . this is unnecessary","upvote_count":"9","comments":[{"upvote_count":"2","poster":"xxichlas","timestamp":"1714105500.0","content":"the differene between snowball with ec2 and without ec2 is $200 (for us east 1). fair to assume aws glue will not be more than that","comment_id":"1202382"}],"timestamp":"1675286760.0","comment_id":"795533"}]},{"poster":"goodmail","comments":[{"upvote_count":"3","content":"AWS Glue is not part of SnowBall Edge AWS services it can run within. Check it out here : https://docs.aws.amazon.com/snowball/latest/developer-guide/whatisedge.html","comment_id":"1161673","poster":"happpieee","timestamp":"1709128080.0"},{"content":"Because AWS Glue means less \"operational (!) overhead\" than running an EC2 instance.","comment_id":"1105738","poster":"pentium75","timestamp":"1703570520.0","upvote_count":"6"}],"timestamp":"1673682360.0","comment_id":"775166","content":"Selected Answer: D\nWhy C? This answer misses the part between SnowBall and AWS Glue. \nD at least provides a full-step solution that copies data in snowball device, and installs the custom application in device's EC2 to do the transformation job.","upvote_count":"17"},{"timestamp":"1738197960.0","upvote_count":"1","comment_id":"1348844","poster":"zdi561","content":"Selected Answer: C\nOne requirement is to run transformation with the data in aws, that is the job of glue"},{"poster":"architect_kags","content":"Selected Answer: D\nSince the transformation application already there, Snowball Edge devices with compute capability allow to run applications directly on the device. This means the transformation job can start locally on the Snowball Edge device if needed or directly after the data transfer to AWS, minimizing delays. AWS Glue for transformation adds additional refactoring work and operational overhead.","upvote_count":"3","timestamp":"1733013420.0","comment_id":"1320404"},{"poster":"tom_cruise","content":"Selected Answer: D\nThe only way C can work is by using this: Amazon S3 adapter — Use for programmatic data transfer in to and out of AWS using the Amazon S3 API for Snowball Edge, but it is not indicated in the answer. If it includes the step to transfer the data from the Snowball to S3, then C would be a better answer.","upvote_count":"2","comment_id":"1318753","timestamp":"1732721220.0"},{"poster":"0de7d1b","content":"Selected Answer: D\nApplication still need EC2 instance along data transfer with Snowball","timestamp":"1732214820.0","comment_id":"1315964","upvote_count":"2"},{"comment_id":"1284730","upvote_count":"3","poster":"PaulGa","content":"Selected Answer: D\nAns D - but I suspect it should be Ans C because, as stated by 123jhlo (1yr 11mth ago), \nD is not serverless: \n\n\"**C**. Order an AWS Snowball Edge Storage Optimized device. Copy the data to the device. Create a custom transformation job by using AWS Glue. - SnowBall can store 80TB (ok), takes around 1 week to move the device (faster than A), and AWS Glue allows to do ETL jobs.\" \nD. Order an AWS Snowball Edge Storage Optimized device that includes Amazon EC2 compute. Copy the data to the device. Create a new EC2 instance on AWS to run the transformation application. - Same as C, but the ETL job requires the deployment/configuration/maintenance of an EC2 instance, while Glue is serverless. This means D has more operational overhead than C.\"","timestamp":"1726493160.0"},{"poster":"bignatov","upvote_count":"3","timestamp":"1724408760.0","comment_id":"1271232","content":"Selected Answer: D\nA and D are wrong. D is the correct answer. \nWhy not C:\n\nAWS Snowball Edge is a physical device designed for transferring large amounts of data to and from AWS. It includes some compute capabilities, such as running AWS Lambda functions, AWS IoT Greengrass, and EC2 instances, but it does not support AWS Glue.\n\nAWS Glue is a fully managed ETL (Extract, Transform, Load) service that runs within the AWS Cloud. It is designed to work with data stored in AWS services like Amazon S3, Amazon RDS, and Amazon Redshift, among others. AWS Glue is not available on edge devices like Snowball Edge.","comments":[{"upvote_count":"1","content":"Yes, AWS Glue can connect to AWS Snowball Edge. AWS Snowball Edge is a data transfer and edge computing device that allows you to move large amounts of data into and out of AWS. You can use AWS Glue to create ETL jobs that read data from Snowball Edge and write it to other AWS services like Amazon S3, Amazon Redshift, or Amazon RDS. --- FROM Copilot","poster":"Lexxo","comment_id":"1358220","timestamp":"1739871840.0"},{"comment_id":"1271234","poster":"bignatov","timestamp":"1724408820.0","upvote_count":"1","content":"just correction A and B are wrong and D is the correct answer."}]},{"content":"gotta be C surely..... LEAST operational overhead.\nEC2 = operational overhead.\nAWS Glue = managed service with transformation capabilities.","comment_id":"1219623","upvote_count":"2","poster":"lofzee","timestamp":"1716821580.0"},{"upvote_count":"4","content":"The q states that the custom job must remain, so glue is out. Seems like D is the only option as DataSync needs bandwidth.","poster":"f761d0e","timestamp":"1713275100.0","comment_id":"1196619"},{"timestamp":"1708071600.0","content":"Selected Answer: C\nUsing an EC2 instance instead of a managed service like AWS Glue will include more operational overhead for the organization.","poster":"vip2","comment_id":"1151822","upvote_count":"2"},{"timestamp":"1706843220.0","poster":"Femmyte","comment_id":"1138074","upvote_count":"3","content":"Selected Answer: D\nThe answer is D because of the following key points\n1. A custom application in the company’s data center runs a weekly data transformation job. Which means that the company already has an application that runs the transformation.\n2. A solutions architect must transfer the data and must configure the transformation job to continue to run in the AWS Cloud. This shows that the only responsibility of the architect is to transfer the data and configure the existing application to run on the EC2 the architect is going to deploy."},{"upvote_count":"2","poster":"awsgeek75","comment_id":"1124372","timestamp":"1705424760.0","content":"Selected Answer: C\nA: Cannot be done because no bandwidth\nB: Snowcone is probably to small\nD: Doable by EC2 is overhead for transformation when Glue is an option\n\nC: Is correct as Snowball Edge Storage Optimised device is good for storage and Glue can transform once the data is available"},{"content":"Selected Answer: C\nC best suit for:\nETL jobs with LEAST operational overhead.\nFor my understanding, we need here to avoir operation or maintenance burden of the solution","timestamp":"1704803700.0","poster":"bujuman","upvote_count":"1","comment_id":"1117470"},{"content":"Selected Answer: D\nwe use snowball to copy 50 PB\n\"The company plans to pause the application until the data transfer is complete \"\nand least over head \" hence C would be reinventing the weel","timestamp":"1701076440.0","upvote_count":"1","poster":"Shalen","comment_id":"1081340"},{"poster":"slimen","upvote_count":"3","timestamp":"1698990240.0","content":"Selected Answer: D\nwhich is faster?\n- setup a glue cluster and adapt it to do the same analytical stuff as the original app\n- simpley run the same app in an EC2 instance?","comment_id":"1061107"},{"timestamp":"1698808680.0","poster":"mach2022","comment_id":"1059341","content":"How are we going to run the custom application using glue? that means more time to adapt the process instead of just running the app in ec2","upvote_count":"2"},{"upvote_count":"7","timestamp":"1697587800.0","content":"Selected Answer: D\nNot A. AWS DataSync requires an internet connection & the question states no available bandwidth\nNot B. SnowCone only has a max of 14 TB with an SSD, and the data is 50 TB\nNot C. Snowball Edge doesn't support Glue\nSupported services: https://docs.aws.amazon.com/snowball/latest/developer-guide/whatisedge.html\n\nSo the answer must be D, as Snowball Edge Storage Optimized does support EC2 & can store 80 TB for the version that support compute resources","poster":"GB_12345","comment_id":"1046439"},{"timestamp":"1693285200.0","content":"Selected Answer: C\nSnowball Edge has storage and compute capabilities, can be used to support workload in offline locations. \n\nTechnically option D will work but with the overhead of EC2, negating the requirement for LEAST ops.","upvote_count":"1","poster":"TariqKipkemei","comments":[],"comment_id":"992787"},{"timestamp":"1692120180.0","content":"Selected Answer: C\nThe Snowball Edge Storage Optimized device allows transferring a large amount of data without using network bandwidth.\nOnce the data is copied to the Snowball, AWS Glue can be used to create a custom ETL job to transform the data, avoiding the need to reconfigure the existing on-premises application.\nThis meets the requirements to transfer the data with minimal operational overhead and configure the data transformation job to run in AWS","upvote_count":"1","comment_id":"981873","poster":"Guru4Cloud"},{"upvote_count":"2","timestamp":"1689767100.0","poster":"james2033","comment_id":"956569","content":"Selected Answer: C\nAWS Glue for ETL (Extract, Transform, Load) https://docs.aws.amazon.com/glue/latest/dg/how-it-works.html is good for this case (transformation). Keyword \"50 TB\", \"AWS Snowball\". Choose C. Easy question."},{"comments":[{"poster":"rcarmin","timestamp":"1689015600.0","content":"D answer says Snowball Edge STORAGE Optimized, which supports 80TB. 39.5TB is for the Snowball Edge COMPUTE Optimized.","comment_id":"948338","upvote_count":"3"}],"content":"Selected Answer: C\nA - no bandwith, option out\nB - snowcone SSD has max 14TB of capacity\nC - is correct one here\nD - cannot use Compute optimized as max capactiy for this snowball is 39.5TB, and only that's why ;-)\nhttps://docs.aws.amazon.com/snowball/latest/developer-guide/device-differences.html","upvote_count":"4","poster":"small_zipgeniuis","timestamp":"1688404680.0","comment_id":"942076"},{"comments":[{"poster":"rcarmin","comment_id":"948339","timestamp":"1689015720.0","content":"IMHO, that's the least CONFIGURATION overhead, not operational. After you configure Glue, the operation should be easier than maintaining the EC2 and the transformation job.","upvote_count":"2"}],"comment_id":"939024","content":"Selected Answer: D\n\"A custom application in the company’s data center runs a weekly data transformation job.\"\n\n\"A solutions architect must transfer the data and must configure the transformation job to continue to run in the AWS Cloud.\"\n\nLEAST operational overhead -> just take app and put on EC2, instead of configuring Glue","poster":"live_reply_developers","upvote_count":"2","timestamp":"1688122620.0"},{"timestamp":"1687433100.0","comment_id":"930427","content":"Option A (AWS DataSync with AWS Glue) involves using AWS DataSync for data transfer, which requires available network bandwidth. Since the data center has no additional network bandwidth, this option is not suitable.\n\nOption B (AWS Snowcone device with deployment) is designed for smaller workloads and may not have enough storage capacity for transferring 50 TB of data. Additionally, deploying the transformation application on the Snowcone device could introduce complexity and operational overhead.\n\nOption D (AWS Snowball Edge with EC2 compute) involves transferring the data using a Snowball Edge device and then creating a new EC2 instance in AWS to run the transformation application. This option adds additional complexity and operational overhead of managing an EC2 instance.\n\nIn comparison, option C offers a straightforward and efficient approach. The Snowball Edge Storage Optimized device can handle the large data transfer without relying on network bandwidth. Once the data is transferred, AWS Glue can be used to create the transformation job, ensuring the continuity of the application's processing in the AWS Cloud.","comments":[{"content":"My thoughts exactly. I think people are missunderstanding CONFIG for OPERATION overhead.","comment_id":"948342","timestamp":"1689015840.0","upvote_count":"2","poster":"rcarmin"}],"upvote_count":"5","poster":"cookieMr"},{"comment_id":"919155","content":"Selected Answer: C\nCorrectly answer is C.\n\n“The data center does not have any available network bandwidth for additional workloads.”","upvote_count":"1","timestamp":"1686304140.0","poster":"beginnercloud"},{"content":"Option is C. \n“The data center does not have any available network bandwidth for additional workloads.” \n D is new EC instance is need to created. So I choose option C.","comment_id":"898730","timestamp":"1684185840.0","poster":"KMohsoe","upvote_count":"1"},{"poster":"studynoplay","comment_id":"891009","upvote_count":"5","content":"Selected Answer: C\nLEAST operational overhead = Serverless = Glue","timestamp":"1683407160.0"},{"timestamp":"1682812080.0","upvote_count":"1","comment_id":"884776","content":"Selected Answer: D\nExist \" A custom application in the company’s data center runs a weekly data transformation job\"\nBecause existing previous app rebuild with Glue is more effort \n\nAns D","poster":"SkyZeroZx"},{"upvote_count":"2","comment_id":"874892","poster":"darn","timestamp":"1681921680.0","content":"Selected Answer: C\nD is far too manual, lots of overhead"},{"poster":"Robrobtutu","content":"Selected Answer: C\nI'm voting C and not D because creating a new EC2 instance in Snowball to run the transformation application has more overhead than running Glue. Another thing to consider is that answer C does not mandate us to install Glue in Snowball, we can run Glue after the data has been uploaded from Snowball to AWS.","comment_id":"872845","upvote_count":"3","timestamp":"1681744260.0"},{"content":"Selected Answer: C\nC has less operational overhead than D. Managing EC2 has higher operational overhead than serverless AWS Glue","upvote_count":"3","timestamp":"1679600820.0","comment_id":"848620","poster":"Bang3R"},{"poster":"StuMoz","upvote_count":"3","content":"I was originally going to vote for C, however it is D because of 2 reasons. 1) AWS love to promote their own products, so Glue is most likely and 2) because Glue presents the least operational overhead moving forward as it is serverless unlike an EC2 instance which requires patching, feeding and watering","comments":[{"comment_id":"872822","timestamp":"1681743060.0","upvote_count":"5","poster":"Robrobtutu","content":"Answer C uses Glue, answer D uses EC2, so I believe you probably meant you're voting for C."}],"timestamp":"1678108020.0","comment_id":"830822"},{"timestamp":"1678063740.0","poster":"Dody","upvote_count":"2","comment_id":"830463","content":"Selected Answer: C\nUsing the EC2 instance created on the Snowball Edge for the transformation job will do it once , However the solution architect must configure the transformation job to continue to run in the AWS Cloud so it's AWS Glue"},{"comment_id":"820118","poster":"AlmeroSenior","timestamp":"1677214260.0","comments":[{"timestamp":"1681744080.0","upvote_count":"3","content":"The answer does not say you have to run Glue inside snowball edge, it just says you would use Glue, which could be after snowball edge reaches Amazon facilities and the data is uploaded to AWS.","comment_id":"872842","poster":"Robrobtutu"}],"content":"Selected Answer: D\nLets not forget that even a compute optimized Snowball cannot run Glue . Basically a NAS with S3 and EC2 is what you get so cant be C ( unless you run storage on prem and Glue in cloud with a dx/vpn )","upvote_count":"2"},{"content":"Is it possible to use AWS Glue service on snowball edge?","comment_id":"818757","timestamp":"1677123480.0","poster":"habibi03336","upvote_count":"1"},{"content":"Selected Answer: D\nperfect fit is D","comment_id":"795535","timestamp":"1675286820.0","upvote_count":"1","poster":"remand"},{"upvote_count":"4","comment_id":"791756","timestamp":"1675004940.0","content":".... and the AI maven says : \n\nA solution that would meet these requirements with the least operational overhead is to use AWS Snowball Edge. Snowball Edge is a data transfer device that can transfer large amounts of data into and out of the AWS cloud with minimal network bandwidth requirements. Additionally, Snowball Edge can run custom scripts on the device, so the transformation job can be configured to continue running during the transfer. Once the transfer is complete, the data can be loaded into an AWS storage service such as Amazon S3. This solution would minimize operational overhead by allowing for a parallel transfer and processing of data, rather than requiring the application to be paused.","poster":"G3"},{"poster":"aba2s","comment_id":"763644","upvote_count":"2","content":"Selected Answer: C\nOption B is incorrect. Although you can use AWS DataSync to automate and accelerate data transfer from on-premises to AWS storage services, it’s not capable of replicating existing applications running on your server.\nOption B is incorrect as AWS Snowcone supports data collection and data processing using AWS compute services but supports only 8 TB of HDD-based hard disk. It’s not the best option for transferring 50 TB of data, as it will require multiple iterations of offline data transfer. \nI will go for C as it seem to have less operational overhead.","timestamp":"1672655400.0"},{"timestamp":"1672214640.0","poster":"NV305","content":"Selected Answer: C\nc only\nGlue is serverless. This means D has more operational overhead than C.","comment_id":"759547","upvote_count":"4"},{"content":"Selected Answer: C\nOption C involves using AWS Lambda to process the photos and storing the photos in Amazon S3, which can handle a large amount of data and scale to meet the needs of the growing user base. Retaining DynamoDB to store the metadata allows the application to continue to use a fast and highly available database for this purpose.","poster":"DavidNamy","timestamp":"1671783180.0","upvote_count":"1","comment_id":"754019"},{"poster":"DavidNamy","content":"Selected Answer: D\nOption D, ordering an AWS Snowball Edge Storage Optimized device that includes Amazon EC2 compute, is the most efficient solution because it allows you to both transfer the data and run the transformation application on the same device, reducing the operational overhead required.","comment_id":"754013","comments":[{"timestamp":"1681743720.0","upvote_count":"2","comment_id":"872836","content":"The answer states you would create an extra EC2 instance to run the application. Wouldn't this mean it would have more operational overhead than Glue?","poster":"Robrobtutu"}],"upvote_count":"2","timestamp":"1671782940.0"},{"poster":"Buruguduystunstugudunstuy","content":"Selected Answer: D\nThe solution that will meet these requirements with the least operational overhead is Option D: (Order an AWS Snowball Edge Storage Optimized device that includes Amazon EC2 compute. Copy the data to the device. Create a new EC2 instance on AWS to run the transformation application.)\n\nAWS Snowball Edge Storage Optimized devices are used to transfer large amounts of data quickly and securely to and from the cloud. They come with onboard storage and compute capabilities, which allows you to perform data processing tasks on the device itself before transferring the data to the cloud. This means that you can copy the data to the device and then use the device's computing capabilities to run the transformation application directly on the device, without having to pause the application or transfer it to the cloud.","upvote_count":"4","timestamp":"1671662160.0","comment_id":"752823","comments":[{"timestamp":"1681743780.0","upvote_count":"2","comment_id":"872837","content":"The answer states you would create an extra EC2 instance to run the application, you wouldn't be using the existing the existing compute. Wouldn't this mean it would have more operational overhead than Glue?","poster":"Robrobtutu"},{"upvote_count":"1","comment_id":"752824","timestamp":"1671662220.0","poster":"Buruguduystunstugudunstuy","content":"Option A, using AWS DataSync to move the data and creating a custom transformation job using AWS Glue, would require more operational overhead as it involves setting up and configuring multiple services. \n\nOption B, ordering an AWS Snowcone device and deploying the transformation applied to the device, would also involve setting up and configuring multiple services and may not have sufficient computing capabilities to run the transformation application. \n\nOption C, ordering an AWS Snowball Edge Storage Optimized device and creating a custom transformation job using AWS Glue, would involve setting up and configuring multiple services and would not have the onboard compute capabilities to run the transformation application directly on the device."}]},{"timestamp":"1671396120.0","comment_id":"749171","poster":"career360guru","content":"Selected Answer: D\nOption D is right as there is a need to copy and transfer the customer job also along with Data. Option C may not work as it requires custom job that needs to be re-written. So fastest and least operational overhead for migration is D only.","upvote_count":"1"},{"timestamp":"1670879820.0","comment_id":"743346","upvote_count":"1","content":"Selected Answer: D\nA, B are obviously to be crossed out as others have mentioned. \n\nI choose D as they have a custom application that runs data transformation so it would be simplest to just install it on Snowball Edge which comes with an EC2.\n\nThey have a custom transformation application, hence I think using AWS Glue is not a good choice. You would need to tweak AWS Glue to do the job like their custom application ( more operational overhead).","poster":"[Removed]"},{"poster":"mj98","upvote_count":"2","content":"Selected Answer: D\nI would say D because they have a custom application?","timestamp":"1669834980.0","comment_id":"731883"},{"poster":"ocbn3wby","timestamp":"1669540740.0","comments":[{"content":"Also, the question clearly states they want to \"pause the application\" and not transform it to something more efficient.","poster":"ocbn3wby","upvote_count":"1","comment_id":"728088","timestamp":"1669540740.0"}],"upvote_count":"2","content":"Selected Answer: D\nI would stick to D answer. \n\nIn real life - this is what it would have happened. Maybe after the migration, the existing ETL application would be refactored to AWS services. But this takes development time vs \"lift and shift\" approach. \n\n\nEdge Storage optimized offers EC2 compute functionality (with AMI directly integrated) https://docs.aws.amazon.com/snowball/latest/developer-guide/device-differences.html","comment_id":"728087"},{"comment_id":"727090","content":"Selected Answer: D\nD is correct\nI'm a bit split on this, because C is also a good answer, especially if you consider that the operational overhead. On the other hand, it seems more operationally efficient to just run the custom ETL job that already exists on the on-prem server on an EC2 instance instead of using AWS Glue. The part of the question that says they already have a \"custom application\" tells me that it should stay 1:1 to make the migration easier.","poster":"rewdboy","upvote_count":"1","timestamp":"1669411560.0","comments":[{"upvote_count":"1","timestamp":"1669452420.0","comment_id":"727397","comments":[{"upvote_count":"3","timestamp":"1669540560.0","comment_id":"728082","content":"You can search this online. \nhttps://docs.aws.amazon.com/snowball/latest/developer-guide/device-differences.html","poster":"ocbn3wby"}],"content":"As I know Snowball Edge dosn't come with EC2. So the answer should be C","poster":"JayanKuruwita"},{"poster":"ocbn3wby","upvote_count":"1","timestamp":"1669540680.0","content":"In real life - this is what it would have happened. Maybe after the migration, the existing ETL application would be refactored to AWS services (Glue). But this takes development time vs \"lift and shift\" approach. \n\n\nI would stick to answer D.","comment_id":"728085"}]},{"content":"C is correct","timestamp":"1669059720.0","upvote_count":"1","comment_id":"723860","poster":"Wpcorgan"},{"poster":"Six_Fingered_Jose","timestamp":"1666798980.0","content":"Selected Answer: C\nagree with C \nGlue is least effort","comment_id":"704833","upvote_count":"2"}],"url":"https://www.examtopics.com/discussions/amazon/view/85912-exam-aws-certified-solutions-architect-associate-saa-c03/","timestamp":"2022-10-19 15:29:00","choices":{"A":"Use AWS DataSync to move the data. Create a custom transformation job by using AWS Glue.","D":"Order an AWS Snowball Edge Storage Optimized device that includes Amazon EC2 compute. Copy the data to the device. Create a new EC2 instance on AWS to run the transformation application.","B":"Order an AWS Snowcone device to move the data. Deploy the transformation application to the device.","C":"Order an AWS Snowball Edge Storage Optimized device. Copy the data to the device. Create a custom transformation job by using AWS Glue."},"answer":"C","question_images":[],"isMC":true,"answers_community":["C (64%)","D (36%)"]},{"id":"AzreGJJrQj6jehRZin0L","question_id":38,"isMC":true,"choices":{"C":"Use AWS Lambda to process the photos. Store the photos in Amazon S3. Retain DynamoDB to store the metadata.","B":"Use Amazon Kinesis Data Firehose to process the photos and to store the photos and metadata.","A":"Use AWS Lambda to process the photos. Store the photos and metadata in DynamoDB.","D":"Increase the number of EC2 instances to three. Use Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volumes to store the photos and metadata."},"question_text":"A company has created an image analysis application in which users can upload photos and add photo frames to their images. The users upload images and metadata to indicate which photo frames they want to add to their images. The application uses a single Amazon EC2 instance and Amazon DynamoDB to store the metadata.\nThe application is becoming more popular, and the number of users is increasing. The company expects the number of concurrent users to vary significantly depending on the time of day and day of week. The company must ensure that the application can scale to meet the needs of the growing user base.\nWhich solution meats these requirements?","answers_community":["C (100%)"],"answer_images":[],"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/85189-exam-aws-certified-solutions-architect-associate-saa-c03/","exam_id":31,"answer_description":"","discussion":[{"upvote_count":"48","content":"Selected Answer: C\nDo not store images in databases ;)... correct answer should be C","comment_id":"692358","timestamp":"1665515940.0","poster":"MXB05"},{"content":"Selected Answer: C\nSolution C offloads the photo processing to Lambda. Storing the photos in S3 ensures scalability and durability, while keeping the metadata in DynamoDB allows for efficient querying of the associated information.\n\nOption A does not provide an appropriate solution for storing the photos, as DynamoDB is not suitable for storing large binary data like images.\n\nOption B is more focused on real-time streaming data processing and is not the ideal service for processing and storing photos and metadata in this use case.\n\nOption D involves manual scaling and management of EC2 instances, which is less flexible and more labor-intensive compared to the serverless nature of Lambda. It may not efficiently handle the varying number of concurrent users and can introduce higher operational overhead.\n\nIn conclusion, option C provides the best solution for scaling the application to meet the needs of the growing user base by leveraging the scalability and durability of Lambda, S3, and DynamoDB.","timestamp":"1687434360.0","upvote_count":"16","comment_id":"930450","poster":"cookieMr"},{"timestamp":"1735578120.0","comment_id":"1334269","upvote_count":"1","poster":"michaelmorar","content":"Selected Answer: C\nBad question - it does not specify a requirement for storing the photos nor does it specify where the old application does so. C is the best answer absent further requirements."},{"comment_id":"1284734","upvote_count":"2","content":"Selected Answer: C\nAns C - as well explained by cookieMr (1yr 2mth ago): \n\"Solution C offloads the photo processing to Lambda. Storing the photos in S3 ensures scalability and durability, while keeping the metadata in DynamoDB allows for efficient querying of the associated information.\n\nOption A does not provide an appropriate solution for storing the photos, as DynamoDB is not suitable for storing large binary data like images.\n\n...option C provides the best solution for scaling the application to meet the needs of the growing user base by leveraging the scalability and durability of Lambda, S3, and DynamoDB.\"","poster":"PaulGa","timestamp":"1726493460.0"},{"content":"Selected Answer: C\nDynamoDB should not be used for storing images and files in general. Answer should be C.","comment_id":"1282004","upvote_count":"2","poster":"pranavff_examtopics_1993","timestamp":"1726038360.0"},{"comment_id":"1280146","upvote_count":"2","timestamp":"1725748800.0","content":"DynamoDB is not designed for storing large objects like photos. Amazon S3 is the correct storage service for photos.","poster":"zliang14"},{"timestamp":"1722101100.0","upvote_count":"3","comment_id":"1256388","content":"Selected Answer: C\ns3 bucket is good option to store images","poster":"snk27"},{"content":"Selected Answer: C\nC is correct answer.","poster":"Gape4","comment_id":"1231592","timestamp":"1718584500.0","upvote_count":"1"},{"poster":"f761d0e","upvote_count":"2","comment_id":"1196622","content":"C. DB is for data, not for photos. Kinesis doesn't store, it processes streaming.","timestamp":"1713275340.0"},{"comment_id":"1193736","content":"Selected Answer: C\nDynamoDB items max size is 400kb. so A cannot be right answer.\n\nCorrect Answer is C","timestamp":"1712835720.0","upvote_count":"2","poster":"MehulKapadia"},{"poster":"farnamjam","timestamp":"1703734260.0","content":"Selected Answer: C\nMax size for DDB entry is 400KB.","upvote_count":"2","comment_id":"1107360"},{"comment_id":"1057077","upvote_count":"2","poster":"aptx4869","timestamp":"1698604980.0","content":"Selected Answer: C\nImages (Object) should go in S3 and metadata should go in database (DynamoDB)"},{"comment_id":"1057031","content":"Solution C offloads the photo processing to Lambda. Storing the photos in S3 ensures scalability and durability, while keeping the metadata in DynamoDB allows for efficient querying of the associated information.","poster":"Ruffyit","upvote_count":"2","timestamp":"1698600540.0"},{"upvote_count":"1","comment_id":"1055484","content":"Selected Answer: C\nSolution C","poster":"Ferna","timestamp":"1698404160.0"},{"poster":"David_Ang","comment_id":"1042863","content":"Selected Answer: C\ni think is only a confusion of the admin, because it has more sense to store the photos in a S3 bucket is logic.","upvote_count":"1","timestamp":"1697219400.0"},{"upvote_count":"1","timestamp":"1697028000.0","poster":"tom_cruise","comment_id":"1040645","content":"Selected Answer: C\nA does not store data."},{"comment_id":"993696","content":"Selected Answer: C\nI stopped at option C","timestamp":"1693368780.0","upvote_count":"1","poster":"TariqKipkemei"},{"comment_id":"990890","timestamp":"1693062420.0","upvote_count":"1","poster":"sand444","content":"Selected Answer: C\nc is correct"},{"timestamp":"1692279300.0","poster":"Abdou1604","upvote_count":"1","comment_id":"983683","content":"DynamoDB can technically store images as binary data (BLOBs)"},{"comment_id":"952213","content":"Selected Answer: C\nWhy one would store photos in DB","upvote_count":"2","timestamp":"1689412260.0","poster":"RajkumarTatipaka"},{"upvote_count":"8","comment_id":"950618","content":"This one is in exam","poster":"MNotABot","timestamp":"1689248640.0"},{"comment_id":"919156","poster":"beginnercloud","timestamp":"1686304200.0","upvote_count":"1","content":"Selected Answer: C\nOption C is the best."},{"timestamp":"1684864260.0","content":"Selected Answer: C\nC is the correct answer, A can't store images in DB","comment_id":"905126","poster":"MostafaWardany","upvote_count":"1"},{"content":"Selected Answer: C\nGo for C which is able to scale","timestamp":"1682754660.0","upvote_count":"1","comment_id":"884136","poster":"cheese929"},{"poster":"TheAbsoluteTruth","comment_id":"858961","comments":[{"content":"Si Señior Siarra!","poster":"cookieMr","timestamp":"1687433280.0","upvote_count":"1","comment_id":"930432"}],"timestamp":"1680448680.0","upvote_count":"3","content":"Selected Answer: C\nLa opción A no es la solución más adecuada para manejar la carga potencialmente alta de usuarios simultáneos, ya que las instancias de Lambda tienen un límite de tiempo de ejecución y la carga alta puede causar un retraso significativo en la respuesta de la aplicación. Además, no se proporciona una solución escalable para almacenar las imágenes.\n\nLa opción C proporciona una solución escalable para el procesamiento y almacenamiento de imágenes y metadatos. La aplicación puede utilizar AWS Lambda para procesar las fotos y almacenar las imágenes en Amazon S3, que es un servicio de almacenamiento escalable y altamente disponible. Los metadatos pueden almacenarse en DynamoDB, que es un servicio de base de datos escalable y de alto rendimiento que puede manejar una gran cantidad de solicitudes simultáneas."},{"content":"C!\nLa opción A no es la solución más adecuada para manejar la carga potencialmente alta de usuarios simultáneos, ya que las instancias de Lambda tienen un límite de tiempo de ejecución y la carga alta puede causar un retraso significativo en la respuesta de la aplicación. Además, no se proporciona una solución escalable para almacenar las imágenes.\n\nLa opción C proporciona una solución escalable para el procesamiento y almacenamiento de imágenes y metadatos. La aplicación puede utilizar AWS Lambda para procesar las fotos y almacenar las imágenes en Amazon S3, que es un servicio de almacenamiento escalable y altamente disponible. Los metadatos pueden almacenarse en DynamoDB, que es un servicio de base de datos escalable y de alto rendimiento que puede manejar una gran cantidad de solicitudes simultáneas.","upvote_count":"1","timestamp":"1680448620.0","comment_id":"858960","poster":"TheAbsoluteTruth"},{"comment_id":"834981","upvote_count":"1","timestamp":"1678452240.0","content":"C is the answer","poster":"rdss11"},{"content":"Selected Answer: C\nmost optimal solution","poster":"Sdraju","comment_id":"828434","timestamp":"1677878940.0","upvote_count":"1"},{"timestamp":"1672656300.0","comment_id":"763649","content":"Selected Answer: C\nC","upvote_count":"1","poster":"aba2s"},{"poster":"DavidNamy","content":"Selected Answer: C\nOption C involves using AWS Lambda to process the photos and storing the photos in Amazon S3, which can handle a large amount of data and scale to meet the needs of the growing user base. Retaining DynamoDB to store the metadata allows the application to continue to use a fast and highly available database for this purpose.","timestamp":"1671783600.0","comment_id":"754023","upvote_count":"1"},{"timestamp":"1671783480.0","poster":"DavidNamy","content":"Selected Answer: C\nAccording to the well-designed framework, option C is the safest and most efficient option.","upvote_count":"1","comment_id":"754021"},{"poster":"a070112","content":"Static content, C","timestamp":"1671691680.0","upvote_count":"2","comment_id":"753036"},{"poster":"Buruguduystunstugudunstuy","timestamp":"1671662640.0","comment_id":"752827","upvote_count":"3","content":"Selected Answer: C\nC. Use AWS Lambda to process the photos. Store the photos in Amazon S3. Retain DynamoDB to store the metadata.\n\nThis solution meets the requirements because it uses AWS Lambda to process the photos, which can automatically scale to meet the needs of the growing user base. The photos can be stored in Amazon S3, which is a highly scalable and durable object storage service. DynamoDB can be retained to store the metadata, which can also scale to meet the needs of the growing user base. This solution allows the application to scale to meet the needs of the growing user base, while also ensuring that the photos and metadata are stored in a scalable and durable manner."},{"content":"Selected Answer: C\nOption C","timestamp":"1671396360.0","poster":"career360guru","comment_id":"749177","upvote_count":"1"},{"upvote_count":"1","poster":"lighrz","comment_id":"737379","content":"Selected Answer: C\nphoto needs to be on S3","timestamp":"1670381460.0"},{"timestamp":"1669411920.0","poster":"rewdboy","upvote_count":"4","comment_id":"727096","content":"Selected Answer: C\nC for sure\nI was originally leaning toward A because it seemed like a simpler setup to keep the images and metadata in the same service, but DynamoDB has a record limit of 64KB, so S3 would be better for image storage and then DynamoDB for metadata"},{"content":"C is correct","upvote_count":"1","timestamp":"1669059780.0","comment_id":"723861","poster":"Wpcorgan"},{"timestamp":"1668415740.0","upvote_count":"1","content":"Selected Answer: C\nphoto needs to be on S3","comment_id":"717820","poster":"Pamban"},{"upvote_count":"1","content":"Selected Answer: C\nphotos should be stored on S3","timestamp":"1668171240.0","poster":"mabotega","comment_id":"716057"},{"poster":"Six_Fingered_Jose","timestamp":"1666799280.0","upvote_count":"2","comment_id":"704836","content":"Selected Answer: C\nagree with C,\nStoring image in DB wont be very scalable compared to S3\nmetadata does not take up much space and is more efficiently stored in DB"},{"comment_id":"702797","poster":"tubtab","upvote_count":"1","content":"Selected Answer: C\ncccccccccc","timestamp":"1666597620.0"}],"topic":"1","answer":"C","unix_timestamp":1665515940,"answer_ET":"C","timestamp":"2022-10-11 21:19:00"},{"id":"S3Bwgp74tKDSzF30UAD7","answer_images":[],"question_images":[],"isMC":true,"topic":"1","choices":{"B":"Configure the security group for the EC2 instances to restrict outbound traffic so that only traffic to the S3 prefix list is permitted.","C":"Move the EC2 instances to private subnets. Create a VPC endpoint for Amazon S3, and link the endpoint to the route table for the private subnets.","A":"Create a NAT gateway. Configure the route table for the public subnets to send traffic to Amazon S3 through the NAT gateway.","D":"Remove the internet gateway from the VPC. Set up an AWS Direct Connect connection, and route traffic to Amazon S3 over the Direct Connect connection."},"unix_timestamp":1666266900,"timestamp":"2022-10-20 13:55:00","question_id":39,"answer_description":"","discussion":[{"comment_id":"930466","poster":"cookieMr","content":"Selected Answer: C\nOption A (creating a NAT gateway) would not meet the requirement since it still involves sending traffic to S3 over the internet. NAT gateway is used for outbound internet connectivity from private subnets, but it doesn't provide a private route for accessing S3.\n\nOption B (configuring security groups) focuses on controlling outbound traffic using security groups. While it can restrict outbound traffic, it doesn't provide a private route for accessing S3.\n\nOption D (setting up Direct Connect) involves establishing a dedicated private network connection between the on-premises environment and AWS. While it offers private connectivity, it is more suitable for hybrid scenarios and not necessary for achieving private access to S3 within the VPC.\n\nIn summary, option C provides a straightforward solution by moving the EC2 instances to private subnets, creating a VPC endpoint for S3, and linking the endpoint to the route table for private subnets. This ensures that file transfer traffic between the EC2 instances and S3 remains within the private network without going over the internet.","timestamp":"1687434840.0","upvote_count":"16"},{"comment_id":"752849","upvote_count":"6","content":"Selected Answer: C\nThe correct answer is C. Move the EC2 instances to private subnets. Create a VPC endpoint for Amazon S3, and link the endpoint to the route table for the private subnets.\n\nTo meet the new requirement of transferring files over a private route, the EC2 instances should be moved to private subnets, which do not have direct access to the internet. This ensures that the traffic for file transfers does not go over the internet.\n\nTo enable the EC2 instances to access Amazon S3, a VPC endpoint for Amazon S3 can be created. VPC endpoints allow resources within a VPC to communicate with resources in other services without the traffic being sent over the internet. By linking the VPC endpoint to the route table for the private subnets, the EC2 instances can access Amazon S3 over a private connection within the VPC.","comments":[{"comments":[{"upvote_count":"2","timestamp":"1673360400.0","poster":"Kayamables","content":"How about the question of moving the instances across subnets. Because according to AWS you can't do it. https://aws.amazon.com/premiumsupport/knowledge-center/move-ec2-instance/#:~:text=It%27s%20not%20possible%20to%20move,%2C%20Availability%20Zone%2C%20or%20VPC.\nKindly clarify. Maybe I miss something.","comment_id":"771502","comments":[{"comment_id":"1105740","timestamp":"1703570820.0","content":"You can't just change the subnet in instance settings, but this article mentions how you CAN move the instance manually.","upvote_count":"2","poster":"pentium75"}]}],"timestamp":"1671664800.0","upvote_count":"1","content":"Option A (Create a NAT gateway) would not work, as a NAT gateway is used to allow resources in private subnets to access the internet, while the requirement is to prevent traffic from going over the internet.\n\nOption B (Configure the security group for the EC2 instances to restrict outbound traffic) would not achieve the goal of routing traffic over a private connection, as the traffic would still be sent over the internet.\n\nOption D (Remove the internet gateway from the VPC and set up an AWS Direct Connect connection) would not be necessary, as the requirement can be met by simply creating a VPC endpoint for Amazon S3 and routing traffic through it.","poster":"Buruguduystunstugudunstuy","comment_id":"752851"}],"poster":"Buruguduystunstugudunstuy","timestamp":"1671664740.0"},{"timestamp":"1739968080.0","upvote_count":"1","content":"Selected Answer: C\nC is the best Answer. The requirement is for file transfers to take a private route and not be sent over the internet. This eliminates A and B. D is for Hybrid solutions.","comment_id":"1358728","poster":"Vandaman"},{"upvote_count":"2","timestamp":"1726494000.0","poster":"PaulGa","comment_id":"1284746","content":"Selected Answer: D\nAns C - I was going for Ans D... \n...but as well explained by Buruguduystunstugudunstuy (1 year, 8 mth ago), C is simpler: \n\"Option C: Move the EC2 instances to private subnets. Create a VPC endpoint for Amazon S3, and link the endpoint to the route table for the private subnets.\nTo meet the new requirement of transferring files over a private route, the EC2 instances should be moved to private subnets, which do not have direct access to the internet. This ensures that the traffic for file transfers does not go over the internet.\n\"Option D (Remove the internet gateway from the VPC and set up an AWS Direct Connect connection) would not be necessary, as the requirement can be met by simply creating a VPC endpoint for Amazon S3 and routing traffic through it.\""},{"content":"Selected Answer: C\nC is the correct answer.","upvote_count":"1","poster":"jaradat02","timestamp":"1721682300.0","comment_id":"1253285"},{"comment_id":"1057034","poster":"Ruffyit","timestamp":"1698600600.0","upvote_count":"1","content":"C. Move the EC2 instances to private subnets. Create a VPC endpoint for Amazon S3, and link the endpoint to the route table for the private subnets."},{"poster":"TariqKipkemei","comment_id":"993700","upvote_count":"1","timestamp":"1693369080.0","content":"Selected Answer: C\nMove the EC2 instances to private subnets. Create a VPC endpoint for Amazon S3, and link the endpoint to the route table for the private subnets."},{"content":"Selected Answer: C\nlink VPC endpoint in route tables ---- EC2 instance to communicate S3 with a private connection in VPC","comment_id":"990901","upvote_count":"2","poster":"sand444","timestamp":"1693062720.0"},{"timestamp":"1671783600.0","comment_id":"754022","upvote_count":"3","poster":"DavidNamy","content":"Selected Answer: C\nAccording to the well-designed framework, option C is the safest and most efficient option."},{"upvote_count":"1","poster":"career360guru","timestamp":"1671396480.0","content":"Selected Answer: C\nOption C","comment_id":"749180"},{"upvote_count":"5","content":"C is correct. \nThere is no requirement for public access from internet. \n\nApplication must be moved in Private subnet. This is a prerequisite in using VPC endpoints with S3\nhttps://aws.amazon.com/blogs/storage/managing-amazon-s3-access-with-vpc-endpoints-and-s3-access-points/","timestamp":"1669543500.0","comment_id":"728120","poster":"ocbn3wby"},{"content":"C is correct","upvote_count":"1","comment_id":"723862","poster":"Wpcorgan","timestamp":"1669059900.0"},{"poster":"Jtic","content":"Selected Answer: C\nUse VPC endpoint","comment_id":"717723","upvote_count":"1","timestamp":"1668404280.0"},{"poster":"Jtic","upvote_count":"1","timestamp":"1668369240.0","comments":[{"content":"Use VPC endpoint","comment_id":"717490","timestamp":"1668369240.0","poster":"Jtic","upvote_count":"1"}],"comment_id":"717489","content":"Selected Answer: C\nUser VPC endpoint and make the EC2 private"},{"content":"Selected Answer: C\nVPC endpoint is the best choice to route S3 traffic without traversing internet. Option A alone can't be used as NAT Gateway requires an Internet gateway for outbound internet traffic. Option B would still require traversing through internet and option D is also not a suitable solution","comment_id":"711940","upvote_count":"4","timestamp":"1667672100.0","poster":"backbencher2022"}],"answer":"C","answer_ET":"C","answers_community":["C (95%)","5%"],"url":"https://www.examtopics.com/discussions/amazon/view/86031-exam-aws-certified-solutions-architect-associate-saa-c03/","question_text":"A medical records company is hosting an application on Amazon EC2 instances. The application processes customer data files that are stored on Amazon S3. The EC2 instances are hosted in public subnets. The EC2 instances access Amazon S3 over the internet, but they do not require any other network access.\nA new requirement mandates that the network traffic for file transfers take a private route and not be sent over the internet.\nWhich change to the network architecture should a solutions architect recommend to meet this requirement?","exam_id":31},{"id":"tnQ5vhZfJS5mrvRTeJTG","answer_ET":"AD","answer":"AD","answers_community":["AD (89%)","5%"],"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/85996-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_images":[],"answer_description":"","question_images":[],"question_text":"A company uses a popular content management system (CMS) for its corporate website. However, the required patching and maintenance are burdensome. The company is redesigning its website and wants anew solution. The website will be updated four times a year and does not need to have any dynamic content available. The solution must provide high scalability and enhanced security.\nWhich combination of changes will meet these requirements with the LEAST operational overhead? (Choose two.)","choices":{"B":"Deploy an AWS WAF web ACL in front of the website to provide HTTPS functionality.","E":"Create the new website. Deploy the website by using an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer.","A":"Configure Amazon CloudFront in front of the website to use HTTPS functionality.","D":"Create the new website and an Amazon S3 bucket. Deploy the website on the S3 bucket with static website hosting enabled.","C":"Create and deploy an AWS Lambda function to manage and serve the website content."},"timestamp":"2022-10-20 10:00:00","exam_id":31,"discussion":[{"poster":"palermo777","content":"A -> We can configure CloudFront to require HTTPS from clients (enhanced security) https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html\nD -> storing static website on S3 provides scalability and less operational overhead, then configuration of Application LB and EC2 instances (hence E is out)\n\nB is out since AWS WAF Web ACL does not to provide HTTPS functionality, but to protect HTTPS only.","comment_id":"700270","timestamp":"1666296540.0","upvote_count":"36"},{"comments":[{"poster":"Lalo","upvote_count":"3","content":"Amazon CloudFront is for Securely deliver content with low latency and high transfer speeds\nBut what about the SQLinjection XSS attacks? we use WAF and olso use HTTPS\nhttps://www.f5.com/glossary/web-application-firewall-waf#:~:text=A%20WAF%20protects%20your%20web,and%20what%20traffic%20is%20safe. \nWAF protects your web apps by filtering, monitoring, and blocking any malicious HTTP/S traffic traveling to the web application, and prevents any unauthorized data from leaving the app.\nAnswer is WAF Not Cloudfront","comment_id":"919663","timestamp":"1686350580.0"},{"content":"does not need to have any dynamic content available","poster":"aussiehoa","timestamp":"1683575880.0","comment_id":"892484","upvote_count":"2"}],"timestamp":"1666799640.0","upvote_count":"10","comment_id":"704842","content":"Selected Answer: AD\nagree with A and D\n\nstatic website -> obviously S3, and S3 is super scalable\nCDN -> CloudFront obviously as well, and with HTTPS security is enhanced.\n\nB does not make sense because you are not replacing the CDN with anything,\nE works too but takes too much effort and compared to S3, S3 still wins in term of scalability. plus why use EC2 when you are only hosting static website","poster":"Six_Fingered_Jose"},{"comment_id":"1253287","timestamp":"1721682720.0","content":"Selected Answer: AD\nA and D is the safest combination.","poster":"jaradat02","upvote_count":"2"},{"poster":"David_Ang","upvote_count":"3","comment_id":"1042866","timestamp":"1697219640.0","content":"Selected Answer: AD\nthese answers are the most common use case for real companies, is like the answers that have more sense"},{"upvote_count":"2","content":"Selected Answer: AD\nWeb Application Firewall creates rules to block attacks, but it does not create HTTPS. It can only allow HTTPS inbound traffic.","comment_id":"1040663","poster":"tom_cruise","timestamp":"1697028780.0"},{"comment_id":"993702","content":"Selected Answer: AD\nScalability, enhanced security and less operational overhead = CloudFront with HTTPS\nScalability and less operational overhead = S3 bucket with static website hosting","poster":"TariqKipkemei","upvote_count":"3","timestamp":"1693369380.0"},{"upvote_count":"2","content":"Selected Answer: AD\nA. Amazon CloudFront provides scalable content delivery with HTTPS functionality, meeting security and scalability requirements.\n\nD. Deploying the website on an Amazon S3 bucket with static website hosting reduces operational overhead by eliminating server maintenance and patching.\n\nWhy other options are incorrect:\n\nB. AWS WAF does not provide HTTPS functionality or address patching and maintenance.\n\nC. Using AWS Lambda introduces complexity and does not directly address patching and maintenance.\n\nE. Managing EC2 instances and an Application Load Balancer increases operational overhead and does not minimize patching and maintenance tasks.\n\nIn summary, configuring Amazon CloudFront for HTTPS and deploying on Amazon S3 with static website hosting provide security, scalability, and reduced operational overhead.","poster":"cookieMr","comment_id":"930478","timestamp":"1687435560.0"},{"poster":"beginnercloud","comment_id":"919157","upvote_count":"2","timestamp":"1686304380.0","content":"Selected Answer: AD\nAD\n\nA for enhanced security D for static content"},{"poster":"studynoplay","upvote_count":"4","comment_id":"891396","content":"Selected Answer: AD\nLEAST operational overhead = Serverless\nhttps://aws.amazon.com/serverless/","timestamp":"1683464700.0"},{"upvote_count":"1","timestamp":"1682950140.0","poster":"angolateoria","content":"AD misses the operational part, how can the app work without a lambda function, an EC2 instance or something?","comment_id":"886366"},{"comment_id":"874896","poster":"darn","timestamp":"1681921860.0","upvote_count":"2","content":"Selected Answer: AD\npeople do not seem to get the LEAST OPERATIONAL OVERHEAD statement, many people keep voting for options that bring far too Op work"},{"content":"Selected Answer: AD\nA for enhanced security\nD for static content","timestamp":"1681038780.0","upvote_count":"3","comment_id":"865449","poster":"channn"},{"timestamp":"1679059260.0","upvote_count":"2","poster":"Erbug","content":"Since Amazon S3 is unlimited and you pay as you go so it means there will be no limit to scale as long as your data is going to grow, so D is one of the correct answers and another correct answer is A, because of this: https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html\n\nso my answer is AD.","comment_id":"842001"},{"timestamp":"1677186660.0","upvote_count":"1","poster":"ManOnTheMoon","content":"I vote A & C for the reason being least operational overhead.","comment_id":"819760"},{"timestamp":"1676187780.0","poster":"Yelizaveta","comment_id":"806077","upvote_count":"4","content":"Selected Answer: AD\nHere a perfect explanation: \nhttps://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/"},{"upvote_count":"1","poster":"Abdel42","comment_id":"788807","timestamp":"1674743520.0","content":"Selected Answer: AD\nSimple and secure"},{"upvote_count":"2","comment_id":"779090","content":"Selected Answer: AD\nD. Create the new website and an Amazon S3 bucket. Deploy the website on the S3 bucket with static website hosting enabled.\nA. Configure Amazon CloudFront in front of the website to use HTTPS functionality.\n\nBy deploying the website on an S3 bucket with static website hosting enabled, the company can take advantage of the high scalability and cost-efficiency of S3 while also reducing the operational overhead of managing and patching a CMS.\nBy configuring Amazon CloudFront in front of the website, it will automatically handle the HTTPS functionality, this way the company can have a secure website with very low operational overhead.","poster":"remand","timestamp":"1673974560.0"},{"comments":[{"poster":"pentium75","upvote_count":"2","content":"You're asked for a \"combination of changes\", not for two alternatives. D already covers the hosting part, now we need the security which is A.","comment_id":"1105744","timestamp":"1703570940.0"},{"timestamp":"1671665460.0","poster":"Buruguduystunstugudunstuy","comment_id":"752865","upvote_count":"1","content":"Why other options are not correct?\n\nOption A (using Amazon CloudFront) and Option B (using an AWS WAF web ACL) would provide HTTPS functionality but would require additional configuration and maintenance to ensure that they are set up correctly and remain secure.\n\nOption E (using an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer) would provide high scalability, but it would require more operational overhead because it involves managing and maintaining EC2 instances."}],"content":"Selected Answer: CD\nKEYWORD: LEAST operational overhead\n\nD. Create the new website and an Amazon S3 bucket. Deploy the website on the S3 bucket with static website hosting enabled.\n\nC. Create and deploy an AWS Lambda function to manage and serve the website content.\n\nOption D (using Amazon S3 with static website hosting) would provide high scalability and enhanced security with minimal operational overhead because it requires little maintenance and can automatically scale to meet increased demand.\n\nOption C (using an AWS Lambda function) would also provide high scalability and enhanced security with minimal operational overhead. AWS Lambda is a serverless compute service that runs your code in response to events and automatically scales to meet demand. It is easy to set up and requires minimal maintenance.","upvote_count":"3","timestamp":"1671665340.0","poster":"Buruguduystunstugudunstuy","comment_id":"752863"},{"content":"Selected Answer: AD\nA and D","timestamp":"1671396660.0","comment_id":"749182","poster":"career360guru","upvote_count":"2"},{"comment_id":"741452","poster":"AlaN652","content":"Selected Answer: AD\nA: for high availability and security through cloudfront HTTPS\nD: Scalable storge solution and support of static hosting","timestamp":"1670738580.0","upvote_count":"2"},{"poster":"Wpcorgan","comment_id":"723863","timestamp":"1669060020.0","upvote_count":"1","content":"A and D"},{"content":"Selected Answer: AD\nCloudfront can do the WAF part so i chose A and D","poster":"PS_R","comment_id":"714448","upvote_count":"3","timestamp":"1667988000.0"},{"content":"Selected Answer: AD\nInitially I thought B) WAF for HTTP to HTTPS redirect, but then I found CloudFront can do it so A) adds performance/scale and security. https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https.html","comment_id":"714015","poster":"Bevemo","timestamp":"1667930520.0","upvote_count":"4"},{"comments":[{"timestamp":"1703571060.0","upvote_count":"1","comment_id":"1105747","poster":"pentium75","content":"Web ACL requires CloudFront in the first place. \"You can protect Amazon CloudFront, Amazon API Gateway, Application Load Balancer, AWS AppSync, Amazon Cognito, AWS App Runner, and AWS Verified Access resources.\""}],"comment_id":"705749","poster":"ManoAni","timestamp":"1666889880.0","upvote_count":"2","content":"Selected Answer: BD\nFor enhanced security B, and they mentioned patching is burdensome so if its E, then they must patch the EC2 instances. So hosting in S3 is ideal as it is static content."},{"comment_id":"699671","poster":"rob74","content":"Selected Answer: BE\n. The solution must provide high scalability and enhanced security\nAWS WAF--> For enhanced security\nhigh scalability -->behind an Application Load Balancer.","comments":[{"comment_id":"728123","content":"Please provide informed answers. You are truly correct, but in this case, there is no specific need to host the website/cms on EC2 + ALB. \n\nIt only requires static website - which can be achieved with scalable S3.","timestamp":"1669543980.0","upvote_count":"6","poster":"ocbn3wby"}],"timestamp":"1666252800.0","upvote_count":"1"}],"question_id":40,"isMC":true,"unix_timestamp":1666252800}],"exam":{"isMCOnly":true,"id":31,"lastUpdated":"11 Apr 2025","isImplemented":true,"provider":"Amazon","numberOfQuestions":1019,"isBeta":false,"name":"AWS Certified Solutions Architect - Associate SAA-C03"},"currentPage":8},"__N_SSP":true}