{"pageProps":{"questions":[{"id":"PLSvK93YV5oSzsBerwut","exam_id":35,"question_text":"A company has multiple development teams sharing one AWS account. The development team's manager wants to be able to automatically stop Amazon EC2 instances and receive notifications if resources are idle and not tagged as production resources.\nWhich solution will meet these requirements?","timestamp":"2021-03-16 03:23:00","answer":"C","unix_timestamp":1615861380,"answer_ET":"C","question_images":[],"topic":"1","discussion":[{"content":"I'll go with C\n\nReferences:\nhttps://docs.aws.am\nazon.com/awssupport/latest/user/cloudwatch-ta.html\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/events/EventTypes.html#trusted-advisor-event-types\nhttps://aws.amazon.com/premiumsupport/technology/trusted-advisor/","upvote_count":"24","comments":[{"comment_id":"350876","poster":"JohnnieWalker","timestamp":"1635320040.0","content":"I will go with C too.\nhttps://gist.github.com/sudharsans/af23ee7e8919947af83ceb269a40d8db\nhttps://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-events-ta.html","upvote_count":"2"}],"comment_id":"326044","poster":"WhyIronMan","timestamp":"1635078420.0"},{"comment_id":"744694","timestamp":"1670990280.0","upvote_count":"6","poster":"saggy4","content":"Selected Answer: C\nA - No such cloudwatch event which identifies idle EC2 instances\nB - No such AWS System Manager Event that lets you know the idle resources\nC - Correct Answer. Trusted Advisor Checks lets you know idle EC2 instances\nD - Amazon Inspector is for managing vulnerabilities"},{"poster":"DucSiu","comment_id":"1078540","timestamp":"1700754060.0","content":"C because Trusted Advisor have check Low CPU.","upvote_count":"1"},{"timestamp":"1675603080.0","upvote_count":"3","content":"Selected Answer: A\nD is eliminated, Amazon Inspector is used to scan vulnerabilities. \nC is eliminated, AWS Trusted Advisor checks accounts.\nB is eliminated, AWS Systems Manager manages resource on AWS and on-premises.","comment_id":"798848","poster":"Piccaso","comments":[{"content":"The links offered by JohnnieWalker changed my mind from A to C","upvote_count":"1","poster":"Piccaso","timestamp":"1675603380.0","comment_id":"798853"}]},{"upvote_count":"2","content":"C is the correct answer. Trusted Advisor is used to diagnose issues with your infrastructure including idle EC2 instances.","comment_id":"778334","poster":"Bulti","timestamp":"1673906880.0"},{"comment_id":"761512","timestamp":"1672350600.0","content":"C for sure","poster":"saeidp","upvote_count":"2"},{"upvote_count":"2","timestamp":"1635015840.0","comment_id":"311934","comments":[{"comments":[{"timestamp":"1673539440.0","poster":"[Removed]","comment_id":"773685","content":"CloudWatch does not have a built-in event that specifically identifies the idle status of EC2 instances, agreed. However, you can create a custom event using CloudWatch Events and a Lambda function to identify idle instances by monitoring for specific conditions that indicate an idle status, such as low CPU/network usage","upvote_count":"1","comments":[{"upvote_count":"1","timestamp":"1675603440.0","poster":"Piccaso","comment_id":"798854","content":"I agree. However, the option C is doing what you suggested, but using Trust Advisor instead of CloudWatch. A did not say that it will use CloudWatch Event + Lambda function"}]}],"content":"This makes A wrong:\nCloudWatch Events rule to filter for Amazon EC2 >>>INSTANCE STATUS CHECKS AND IDENTIFY IDLE<<< EC2 instances\n????\n\nThere are two types of status checks: system status checks and instance status checks, AND NONE of them has the \"Idle\" status\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-system-instance-status-check.html#types-of-instance-status-checks","poster":"WhyIronMan","upvote_count":"4","comment_id":"326039","timestamp":"1635048480.0"}],"content":"A - CW event for ec2 state and lambda to stop ec2","poster":"rscloud"}],"answer_images":[],"isMC":true,"choices":{"D":"Use a scheduled Amazon CloudWatch Events rule to target Amazon Inspector events for idle EC2 instances. Use the CloudWatch Events rule to target the AWS Lambda function to stop non-production instances and send notifications.","A":"Use a scheduled Amazon CloudWatch Events rule to filter for Amazon EC2 instance status checks and identify idle EC2 instances. Use the CloudWatch Events rule to target an AWS Lambda function to stop non-production instances and send notifications.","B":"Use a scheduled Amazon CloudWatch Events rule to filter AWS Systems Manager events and identify idle EC2 instances and resources. Use the CloudWatch Events rule to target an AWS Lambda function to stop non-production instances and send notifications.","C":"Use a scheduled Amazon CloudWatch Events rule to target a custom AWS Lambda function that runs AWS Trusted Advisor checks. Create a second CloudWatch Events rule to filter events from Trusted Advisor to trigger a Lambda function to stop idle non-production instances and send notifications."},"answers_community":["C (67%)","A (33%)"],"url":"https://www.examtopics.com/discussions/amazon/view/47270-exam-aws-devops-engineer-professional-topic-1-question-42/","question_id":146,"answer_description":""},{"id":"KKtmzxxa9gs9RvK58iDU","answer_ET":"D","url":"https://www.examtopics.com/discussions/amazon/view/28583-exam-aws-devops-engineer-professional-topic-1-question-43/","question_images":[],"timestamp":"2020-08-14 21:16:00","answer":"D","discussion":[{"content":"D. \nLifecycle hook can trigger lambda to check DB status: https://docs.aws.amazon.com/autoscaling/ec2/userguide/configuring-lifecycle-hook-notifications.html","upvote_count":"13","comment_id":"163382","timestamp":"1633033440.0","poster":"halfway"},{"upvote_count":"2","content":"Selected Answer: D\nA and B are just weird.\nC is eliminated, AWS Step Functions is for workflow of complex business logic.","poster":"Piccaso","timestamp":"1675604280.0","comment_id":"798865"},{"comment_id":"778336","upvote_count":"1","content":"D is the correct answer.","poster":"Bulti","timestamp":"1673907060.0"},{"timestamp":"1669439280.0","comment_id":"727271","poster":"Jtic","upvote_count":"2","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/configuring-lifecycle-hook-notifications.html"},{"timestamp":"1635785580.0","upvote_count":"1","poster":"certking","content":"repopulating table needs to happen at each deployment, not at each ASG scale-in/out event. with option D, when the ASG scales out, it pauses and triggers the DB table repopulating. is that what we want?","comment_id":"447320"},{"content":"Only D can be the answer","timestamp":"1634201940.0","poster":"gmandala","comment_id":"253071","upvote_count":"1"},{"upvote_count":"3","comment_id":"224218","poster":"jackdryan","timestamp":"1633652040.0","content":"I'll go with D"},{"timestamp":"1633625460.0","upvote_count":"1","poster":"xlFireman","comment_id":"219284","content":"D is the answer"},{"upvote_count":"1","poster":"ChauPhan","comment_id":"209492","timestamp":"1633501140.0","content":"Go with D"},{"timestamp":"1632547200.0","content":"Assuming we want to 'guarantee' that database is ready before web tier requires some verification likely through Lambda. \n\nA & B is out. D is applicable but it doesn't quite guarantee. Also, lifecycle hook is not triggering any Lambda for verification. \n\nC is correct. Assuming we are using CodePipeline, after CodeBuild, we invoke Stepfunctions that will periodically check table population through lambda.","comment_id":"158267","upvote_count":"3","poster":"Smart","comments":[{"upvote_count":"3","comment_id":"160150","timestamp":"1633012320.0","poster":"AKD","content":"C does not ensure the portal isn't coming online. ASG Life cycle hooks are the only way to make it pause, monitor the state of DB and allow it to proceed later. All of this is made through Lambda.\nSo D is the right answer."}]}],"isMC":true,"question_id":147,"topic":"1","answer_description":"","question_text":"An n-tier application requires a table in an Amazon RDS MySQL DB instance to be dropped and repopulated at each deployment. This process can take several minutes and the web tier cannot come online until the process is complete. Currently, the web tier is configured in an Amazon EC2 Auto Scaling group, with instances being terminated and replaced at each deployment. The MySQL table is populated by running a SQL query through an AWS CodeBuild job.\nWhat should be done to ensure that the web tier does not come online before the database is completely configured?","exam_id":35,"unix_timestamp":1597432560,"choices":{"C":"Use AWS Step Functions to monitor and maintain the state of data population. Mark the database in service before continuing with the deployment.","B":"Modify the launch configuration of the Auto Scaling group to pause user data execution for 600 seconds, allowing the table to be populated.","A":"Use Amazon Aurora as a drop-in replacement for RDS MySQL. Use snapshots to populate the table with the correct data.","D":"Use an EC2 Auto Scaling lifecycle hook to pause the configuration of the web tier until the table is populated."},"answer_images":[],"answers_community":["D (100%)"]},{"id":"l6fqpjzVWL1iWa4lhs1w","exam_id":35,"question_text":"A highly regulated company has a policy that DevOps Engineers should not log in to their Amazon EC2 instances except in emergencies. If a DevOps Engineer does log in, the Security team must be notified within 15 minutes of the occurrence.\nWhich solution will meet these requirements?","timestamp":"2020-08-11 13:10:00","answer":"B","unix_timestamp":1597144200,"answer_ET":"B","question_images":[],"topic":"1","discussion":[{"comment_id":"155372","upvote_count":"14","timestamp":"1633771980.0","content":"https://aws.amazon.com/blogs/security/how-to-monitor-and-visualize-failed-ssh-access-attempts-to-amazon-ec2-linux-instances/ \n\nB","comments":[{"content":"Thanks for the link","poster":"saeidp","upvote_count":"1","timestamp":"1672352880.0","comment_id":"761535"},{"content":"This is not about failed access.","upvote_count":"1","comment_id":"233477","poster":"svjl","timestamp":"1634647440.0","comments":[{"upvote_count":"3","comments":[{"comment_id":"509765","poster":"GreatFunana","timestamp":"1640546100.0","content":"Excuse me, the technique listed in here could easily be adapted for successful logins*","upvote_count":"2"}],"poster":"GreatFunana","comment_id":"509763","timestamp":"1640546100.0","content":"Bruh. Stop being that person who just read the URL string and not bothered to read or at least skim the article. The technique listed in here could easily be adapted for login attempts."}]},{"poster":"bnagaraja9099","timestamp":"1635024060.0","upvote_count":"1","comment_id":"280115","content":"Thanks ofr the link"},{"poster":"MikeyJ","content":"A CloudWatch Logs agent runs on each EC2 instance. The agents are configured to send SSH logs from the EC2 instance to a log stream identified by an instance ID.\nLog streams are aggregated into a log group. As a result, one log group contains all the logs you want to analyze from one or more instances.\nYou apply metric filters to a log group in order to search for specific keywords. When the metric filter finds specific keywords, the filter counts the occurrences of the keywords in a time-based sliding window. If the occurrence of a keyword exceeds the CloudWatch alarm threshold, an alarm is triggered.","upvote_count":"2","comment_id":"666619","timestamp":"1662961860.0"}],"poster":"jxp09"},{"comment_id":"1046235","timestamp":"1697564100.0","poster":"YR4591","content":"Selected Answer: B\nB, Cloud watch agent can collect OS logs included ssh logins","upvote_count":"1"},{"timestamp":"1694359440.0","poster":"frizzolo","comment_id":"1004086","content":"vote for B","upvote_count":"1"},{"timestamp":"1682949240.0","comment_id":"886336","upvote_count":"2","content":"Selected Answer: B\nB is more suitable answer for this scenario.","poster":"ParagSanyashiv"},{"poster":"skkakarla","timestamp":"1676292960.0","upvote_count":"1","content":"The SSH example given in other comments is only for Linux. The question asks about EC2, so we have to factor in other types as well. So C seems the better option, as with cloudtrail we are still able to meet the 15 min timeline","comment_id":"807400"},{"timestamp":"1675604820.0","comments":[{"poster":"Piccaso","upvote_count":"1","timestamp":"1676294700.0","content":"I am in the second round, now I pick B.","comment_id":"807429"}],"upvote_count":"1","comment_id":"798869","poster":"Piccaso","content":"Selected Answer: C\nA and B are eliminated, because it is error-prone to install something on each EC2 instance.\nD is more error-prone."},{"timestamp":"1673907840.0","comment_id":"778344","upvote_count":"1","content":"Answer is B. Within 15 min is the key. Cloudtrail logs cannot be analyzed within 15 min as it doesn't get to CloudWatch log within 15 min.","poster":"Bulti"},{"timestamp":"1672352820.0","comment_id":"761534","content":"B is correct\nA CloudWatch Logs agent runs on each EC2 instance. The agents are configured to send SSH logs from the EC2 instance to a log stream identified by an instance ID.\nhttps://aws.amazon.com/blogs/security/how-to-monitor-and-visualize-failed-ssh-access-attempts-to-amazon-ec2-linux-instances/","poster":"saeidp","upvote_count":"2"},{"upvote_count":"2","comments":[{"upvote_count":"1","comment_id":"818941","timestamp":"1677138720.0","content":"yes, this is the point","poster":"thuyeinaung"}],"poster":"whlq","content":"B\nC is incorrect because CloudTrail logs can take up to 15 minutes to record an event.","timestamp":"1671439860.0","comment_id":"749643"},{"timestamp":"1670993040.0","upvote_count":"4","comment_id":"744719","poster":"saggy4","content":"Selected Answer: B\nA - Inspector is for managing vulnerabilities\nB - Correct Answer, Cloudwatch agent can scan through the EC2 instances for SSH login logs\nC - Logging into an AWS EC2 instances is not traced by CloudTrail (Only if we use SSM but nothing is mentioned about it in the question).\nD - Too much work involved to do things"},{"poster":"SatenderRathee","upvote_count":"1","comment_id":"741907","timestamp":"1670777460.0","content":"Selected Answer: C\nThe correct solution is C. Setting up AWS CloudTrail with Amazon CloudWatch Logs, subscribing CloudWatch Logs to Amazon Kinesis, and attaching an AWS Lambda function to Kinesis to parse the logs and determine if they contain user logins will meet the requirements of the question. This solution will allow the Security team to be notified within 15 minutes of any user logins on the EC2 instances."},{"upvote_count":"3","timestamp":"1668350700.0","comment_id":"717372","poster":"flavins","content":"Selected Answer: B\nB is correct because is simple and fast. C is an over kill and complicated"},{"timestamp":"1667746920.0","poster":"nebojsaMa","comment_id":"712390","content":"C as the explenation says : https://docs.aws.amazon.com/IAM/latest/UserGuide/security-logging-and-monitoring.html","upvote_count":"1"},{"timestamp":"1662464520.0","comments":[{"upvote_count":"3","timestamp":"1662728940.0","poster":"MichaelExam","content":"C should be wrong. Since the log of login to EC2 instance belong OS level. Cloudtrail don't log OS level event.","comment_id":"664595"}],"poster":"SamHan","content":"Selected Answer: C\nAns: C\nLogin requests will be logged only in cloudtrail not cloudwatch","comment_id":"661176","upvote_count":"3"},{"poster":"blueorca","upvote_count":"4","content":"B is correct, as the question states within 15 minutes. If it says immediately then it would be C. There might be multiple options that can do it, we always need to pick the best one, which includes lowest possible cost as well.","timestamp":"1644742200.0","comment_id":"546334"},{"upvote_count":"2","poster":"certking","timestamp":"1636214460.0","comment_id":"447316","content":"B would be a perfect answer if it mentioned creating a CloudWatch Alarm: having a CloudWatch metric filter along doesn't trigger anything.\n\nI think C is technically possible but involves too much overhead such as building your Lambda function and paying for Kinesis. It's preventing the wheels"},{"upvote_count":"3","timestamp":"1634909580.0","comment_id":"261933","poster":"GVGREAT","content":"Answer is B\nhttps://medium.com/@matthewleearthur/alerting-on-successful-ec2-ssh-logins-6b97ccfb33eb"},{"content":"Why not C? We are only interested in ssh log in. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitor-with-cloudtrail.html","timestamp":"1634846520.0","poster":"gmandala","upvote_count":"1","comment_id":"252706"},{"timestamp":"1634830560.0","comment_id":"233484","comments":[{"upvote_count":"1","timestamp":"1635345840.0","comment_id":"401643","content":"I think only B can work.\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/events/EventTypes.html\n\nCloudwatch Events can trigger Amazon Inspector, but Amazon Inspector does not generate cloudwatch events. So the methodology described in A wouldn't work. Also, the only metrics posted by inspector to cloudwatch are those you have listed, and successful logins are not among them.","poster":"MBJames"},{"upvote_count":"1","timestamp":"1640545920.0","poster":"GreatFunana","content":"Bruh. Have you ever made simple alerts with CloudWatch? You can simply filter patterns with regular CloudWatch metrics/events:\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html","comment_id":"509761"}],"upvote_count":"4","poster":"svjl","content":"A is correct: \nAmazon Inspector automatically publishes metrics data on your assessments to Amazon CloudWatch. If you are a CloudWatch user, your Inspector assessment statistics will automatically be populated to CloudWatch. The Inspector metrics that are currently available are: number of assessment runs, agents targeted, and findings generated. For more details, see the Amazon Inspector documentation for details on the assessment metrics published to CloudWatch, then you have the messages.\n\nB is totally wrong, you will create an alert if you filter the logs from CloudWatch insights not from a metric filter.\nC, D wrong for this context."},{"poster":"jackdryan","timestamp":"1634443080.0","comment_id":"224214","upvote_count":"4","content":"I'll go with B"},{"upvote_count":"2","comment_id":"209451","content":"B is correct","poster":"ChauPhan","timestamp":"1634390880.0"},{"comments":[{"poster":"svjl","comment_id":"233478","upvote_count":"1","timestamp":"1634779200.0","content":"This is not about failed access, but when a log in occur"}],"comment_id":"175908","timestamp":"1633831500.0","poster":"rejuvenate","content":"B\nA CloudWatch Logs agent runs on each EC2 instance. The agents are configured to send SSH logs from the EC2 instance to a log stream identified by an instance ID.\nLog streams are aggregated into a log group. As a result, one log group contains all the logs you want to analyze from one or more instances.\nYou apply metric filters to a log group in order to search for specific keywords. When the metric filter finds specific keywords, the filter counts the occurrences of the keywords in a time-based sliding window. If the occurrence of a keyword exceeds the CloudWatch alarm threshold, an alarm is triggered.\nAn IAM policy defines a role that gives the EC2 servers permission to create logs in a log group and send log events (new log entries) from EC2 to log groups. This role is then assumed by the application servers.\nCloudWatch alarms notify users when a specified threshold has been crossed. For example, you can set an alarm to trigger when more than 2 failed SSH connections happen in a 5-minute period. \nhttps://aws.amazon.com/blogs/security/how-to-monitor-and-visualize-failed-ssh-access-attempts-to-amazon-ec2-linux-instances/","upvote_count":"4"}],"answer_images":[],"isMC":true,"choices":{"D":"Set up a script on each Amazon EC2 instance to push all logs to Amazon S3. Set up an S3 event to trigger an AWS Lambda function, which triggers an Amazon Athena query to run. The Athena query checks for logins and sends the output to the Security team using Amazon SNS.","B":"Install the Amazon CloudWatch agent on each EC2 instance. Configure the agent to push all logs to Amazon CloudWatch Logs and set up a CloudWatch metric filter that searches for user logins. If a login is found, send a notification to the Security team using Amazon SNS.","A":"Install the Amazon Inspector agent on each EC2 instance. Subscribe to Amazon CloudWatch Events notifications. Trigger an AWS Lambda function to check if a message is about user logins. If it is, send a notification to the Security team using Amazon SNS.","C":"Set up AWS CloudTrail with Amazon CloudWatch Logs. Subscribe CloudWatch Logs to Amazon Kinesis. Attach AWS Lambda to Kinesis to parse and determine if a log contains a user login. If it does, send a notification to the Security team using Amazon SNS."},"answers_community":["B (67%)","C (33%)"],"url":"https://www.examtopics.com/discussions/amazon/view/28082-exam-aws-devops-engineer-professional-topic-1-question-44/","answer_description":"","question_id":148},{"id":"PF7sUpl1pr5k3vFpiB9X","answer_description":"","question_text":"A DevOps engineer has automated a web service deployment by using AWS CodePipeline with the following steps:\n1. An AWS CodeBuild project compiles the deployment artifact and runs unit tests.\n2. An AWS CodeDeploy deployment group deploys the web service to Amazon EC2 instances in the staging environment.\n3. A CodeDeploy deployment group deploys the web service to EC2 instances in the production environment.\nThe quality assurance (QA) team requests permission to inspect the build artifact before the deployment to the production environment occurs. The QA team wants to run an internal penetration testing tool to conduct manual tests. The tool will be invoked by a REST API call.\nWhich combination of actions should the DevOps engineer take to fulfill this request? (Choose two.)","discussion":[{"content":"A and E - https://www.examtopics.com/discussions/amazon/view/49433-exam-aws-devops-engineer-professional-topic-1-question-190/","timestamp":"1661982720.0","upvote_count":"13","poster":"ohcn","comment_id":"655520"},{"timestamp":"1697564340.0","content":"Selected Answer: AE\nAE\nCode pipeline itself can't send api requests, it should use either codebuild or lambda. In addition, approval step can be set in codepipline and not in code build.","upvote_count":"1","comment_id":"1046239","poster":"YR4591"},{"upvote_count":"1","comment_id":"886341","timestamp":"1682949480.0","content":"Selected Answer: AE\nAE are most suitable option here.","poster":"ParagSanyashiv"},{"poster":"hoomaan","content":"Agree with ohcn, AE is correct.","timestamp":"1677388200.0","upvote_count":"1","comment_id":"822076"},{"comment_id":"803912","poster":"bihani","upvote_count":"1","timestamp":"1675996920.0","content":"Selected Answer: AE\nagree with ohcn"},{"timestamp":"1675607340.0","comments":[{"upvote_count":"1","poster":"hoomaan","timestamp":"1677388320.0","comment_id":"822081","content":"I am not sure about B, as we cannot add approval to buildspec.yaml\nhttps://www.reddit.com/r/aws/comments/x7e0na/in_the_buildspecyml_for_aws_codebuild_is_it/"}],"upvote_count":"1","content":"Selected Answer: BE\nDo not understand why C is in the correct answer.\nBetween A and B. A looks straightforward. \nHowever, I prefer B, because it is more automatic and integrated.","poster":"Piccaso","comment_id":"798892"},{"timestamp":"1673907960.0","comment_id":"778346","content":"A & E are the right answers","upvote_count":"1","poster":"Bulti"},{"poster":"PepsNick","comment_id":"771222","content":"Selected Answer: AE\nAE. Here is information about adding a manual approval step which is required. https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html\nYou call a lambda function in parallel to the approval to trigger the API.","timestamp":"1673341440.0","upvote_count":"2"},{"upvote_count":"1","poster":"saeidp","content":"A and E","comment_id":"761539","timestamp":"1672353360.0"},{"poster":"Chinta","timestamp":"1671702600.0","content":"AE is the correct option","upvote_count":"1","comment_id":"753160"},{"upvote_count":"2","comment_id":"674776","poster":"Goozian","content":"Selected Answer: AE\nA and E","timestamp":"1663734420.0"},{"content":"E for sure, not sure about A","timestamp":"1662372660.0","comment_id":"660031","poster":"costin","upvote_count":"2"}],"timestamp":"2022-08-31 23:52:00","answer_ET":"AE","unix_timestamp":1661982720,"answer_images":[],"answer":"AE","question_images":[],"question_id":149,"url":"https://www.examtopics.com/discussions/amazon/view/78792-exam-aws-devops-engineer-professional-topic-1-question-45/","choices":{"D":"Update the pipeline to directly call the REST API for the penetration testing tool.","B":"Modify the buildspec.yml file for the compilation stage to require manual approval before completion.","C":"Update the CodeDeploy deployment groups so that they require manual approval to proceed.","E":"Update the pipeline to invoke a Lambda function that calls the REST API for the penetration testing tool.","A":"Insert a manual approval action between the test actions and deployment actions of the pipeline."},"exam_id":35,"topic":"1","isMC":true,"answers_community":["AE (88%)","13%"]},{"id":"s8kwGilfi6hTiH3EaY7P","question_text":"A DevOps Engineer manages a large commercial website that runs on Amazon EC2. The website uses Amazon Kinesis Data Streams to collect and process web logs. The DevOps Engineer manages the Kinesis consumer application, which also runs on Amazon EC2.\nSudden increases of data cause the Kinesis consumer application to fall behind, and the Kinesis data streams drop records before the records can be processed.\nThe DevOps Engineer must implement a solution to improve stream handling.\nWhich solution meets these requirements with the MOST operational efficiency?","answer_description":"","discussion":[{"content":"B - https://www.examtopics.com/discussions/amazon/view/8544-exam-aws-devops-engineer-professional-topic-1-question-129/","timestamp":"1661983080.0","upvote_count":"7","comment_id":"655526","poster":"ohcn"},{"comment_id":"1323685","content":"Selected Answer: C\nc is correct \nBenefits of the Lambda Approach:\nFully managed, serverless processing.\nScales automatically with incoming data.\nReduces operational overhead and simplifies management.\nProvides a robust solution for real-time data processing.","upvote_count":"1","timestamp":"1733682420.0","poster":"Simba84"},{"timestamp":"1728587040.0","content":"Selected Answer: C\nC - Lambda is more operationally efficient and it out scales.\nIt runs in parallel (multi-thread) to process Kinesis records, which would resolve the issue of the consumer falling behind.","poster":"kempl","comment_id":"1295722","upvote_count":"1"},{"poster":"Venki_dev","content":"Selected Answer: C\nAns is C , \nkey here is \"MOST operational efficiency\" , when compare EC2 vs Lambda , Lambda are more operationally efficient, with EC2 you need to operationally manage it like patching etc etc. though B will also fit , but Operationally efficient would be C\n\nwith regard to application falling behind it can be achieved by lambda ParallelizationFactor , see AWS Documentation \nwhen you set ParallelizationFactor to 2, you can have 200 concurrent Lambda invocations at maximum to process 100 Kinesis data shards (though in practice, you may see a different values for the ConcurrentExecutions metric). This helps scale up the processing throughput when the data volume is volatile and the IteratorAge is high.\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html","comment_id":"1226745","upvote_count":"2","timestamp":"1717853940.0"},{"poster":"failexamonly","comment_id":"1123947","upvote_count":"1","timestamp":"1705390140.0","content":"Selected Answer: C\nmore efficient than b"},{"upvote_count":"2","poster":"hahaha1","timestamp":"1677470580.0","content":"Selected Answer: B\nB is the correct one.","comment_id":"823159"},{"poster":"thuyeinaung","comment_id":"818948","timestamp":"1677139080.0","upvote_count":"4","content":"Selected Answer: B\nI think the answer is B."},{"upvote_count":"3","comments":[{"upvote_count":"1","poster":"Venki_dev","content":"you are right , and lambda ParallelizationFactor can help to solve this problem and Lambda or operationally more efficient than EC2. So answer is C and not B","timestamp":"1717854120.0","comment_id":"1226748"}],"timestamp":"1675607880.0","content":"Selected Answer: B\nI changed my mind to B.\nThe throughput problem is not from Kinesis Data Streams, but from the EC2 hosting Kinesis Data Streams.\nSo we need to scale up the EC2 instances.","comment_id":"798900","poster":"Piccaso"},{"comments":[],"upvote_count":"3","comment_id":"798894","poster":"Piccaso","content":"Selected Answer: D\nWe need to use the in-built auto-scaling mechanism in Kinesis Data Streams to solve the problem.","timestamp":"1675607580.0"},{"timestamp":"1673908920.0","content":"Answer is B. We need to scale the Kinesis consumer application horizontally based on the CloudWatch metric. Answer cannot be C because replacing the Kinesis Consumer app with Lambda does not make the solution operationally efficient. Lambda could also throttle as the data is streamed faster into Kinesis Data Stream. Answer is not D because just my increasing the shards will not result in efficient processing on the consumer side. We also need to increase the no of instances and therefore KCL workers to propose the data from additional shards.","poster":"Bulti","comment_id":"778348","upvote_count":"3"},{"poster":"PepsNick","upvote_count":"1","comment_id":"771225","timestamp":"1673341620.0","content":"Selected Answer: B\nB. They are asking for operational efficiency. Converting the application to lambda may not be possible and is not efficient."},{"poster":"ddev3737","content":"Why not D?\nD. Increase the number of shards in the Kinesis Data Streams to increase the overall throughput so that the consumer application can process the data faster.\n\nIncreasing the number of shards in the Kinesis Data Streams is the most operationally efficient solution because it allows the consumer application to process the data faster by increasing the overall throughput of the data stream. This will allow the consumer application to keep up with sudden increases in data and prevent records from being dropped. It also does not require any changes to the consumer application itself, which can be more operationally efficient than modifying the application or converting it to a Lambda function. Additionally, increasing the number of shards does not require the use of additional resources such as EC2 instances or EMR clusters, which can be more expensive and complex to manage.","comments":[{"poster":"m4r0ck","comment_id":"956923","content":"the problem is not with the throughput as it can increase. the problem is when this happens, the processing app falls behind which means it's a consumer issue. B is the correct answer in this case","upvote_count":"1","timestamp":"1689800640.0"}],"upvote_count":"3","comment_id":"770083","timestamp":"1673246040.0"},{"comment_id":"762525","timestamp":"1672471200.0","poster":"saeidp","upvote_count":"4","content":"I go with C. Most efficient (C: because of the sudden spikes) vs fastest approach (B)"},{"comment_id":"760607","content":"Selected Answer: B\nRefer to ohcn's comment.","poster":"AkaAka4","upvote_count":"1","timestamp":"1672291680.0"},{"comment_id":"753172","content":"Go with B","upvote_count":"1","timestamp":"1671703080.0","poster":"Chinta"},{"content":"Selected Answer: C\nkinesis stream + lambda is better solution","timestamp":"1670675040.0","comment_id":"740961","poster":"SatenderRathee","upvote_count":"2"},{"upvote_count":"3","comment_id":"735072","content":"Selected Answer: C\n1. https://docs.aws.amazon.com/streams/latest/dev/lambda-consumer.html\n2. https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html","poster":"Maygam","timestamp":"1670156700.0"},{"content":"Selected Answer: C\nI would go with C. Lambda is always more efficient and cheaper than EC2. https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html","comment_id":"722258","poster":"alinato","timestamp":"1668892800.0","comments":[{"comment_id":"732182","upvote_count":"5","content":"Agree. As the question is asking for MOST operational efficiency, then answer is C.\nIf the question is asking for FASTEST method to improve, then answer is B","poster":"kyozanuro","timestamp":"1669862040.0"}],"upvote_count":"4"},{"upvote_count":"2","content":"Selected Answer: B\nOnly B can't be false!","timestamp":"1663734840.0","comment_id":"674782","poster":"Goozian"}],"timestamp":"2022-08-31 23:58:00","answer_ET":"C","unix_timestamp":1661983080,"answer_images":[],"answer":"C","question_images":[],"question_id":150,"url":"https://www.examtopics.com/discussions/amazon/view/78795-exam-aws-devops-engineer-professional-topic-1-question-46/","choices":{"C":"Convert the Kinesis consumer application to run as an AWS Lambda function. Configure the Kinesis Data Streams as the event source for the Lambda function to process the data streams.","A":"Modify the Kinesis consumer application to store the logs durably in Amazon S3. Use Amazon EMR to process the data directly on Amazon S3 to derive customer insights. Store the results in Amazon S3.","B":"Horizontally scale the Kinesis consumer application by adding more EC2 instances based on the Amazon CloudWatch GetRecords.IteratorAgeMilliseconds metric. Increase the retention period of the Kinesis Data Streams.","D":"Increase the number of shards in the Kinesis Data Streams to increase the overall throughput so that the consumer application processes data faster."},"exam_id":35,"isMC":true,"topic":"1","answers_community":["C (47%)","B (43%)","10%"]}],"exam":{"isMCOnly":false,"numberOfQuestions":208,"isImplemented":true,"lastUpdated":"11 Apr 2025","id":35,"name":"AWS DevOps Engineer Professional","provider":"Amazon","isBeta":false},"currentPage":30},"__N_SSP":true}