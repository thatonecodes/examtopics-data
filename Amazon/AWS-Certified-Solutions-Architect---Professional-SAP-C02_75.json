{"pageProps":{"questions":[{"id":"YU0k3GvDCOO9scr8Krlo","discussion":[{"poster":"AzureDP900","upvote_count":"1","comment_id":"1310271","content":"Option B is actually a very good choice. Creating an Amazon Elastic Kubernetes Service (EKS) cluster that has managed node groups allows for a seamless migration of the existing Kubernetes deployment to AWS, while maintaining the flexibility and scalability of EKS.","timestamp":"1731352800.0"},{"poster":"AloraCloud","comment_id":"1298560","timestamp":"1729055460.0","content":"B is an AWS Solutions/Sales team to a customer answer","upvote_count":"1"},{"comment_id":"1235728","upvote_count":"3","poster":"Spike2020","timestamp":"1719129660.0","content":"B is best to manage, but is it easiest to migrate? you still need to adjust manifest file for managed node groups and ECR repo. D is lift and shift."},{"comment_id":"1184144","poster":"TonytheTiger","content":"Selected Answer: B\nOption B: Some additional migration to EKS info\n(1) https://aws.amazon.com/blogs/architecture/field-notes-migrating-a-self-managed-kubernetes-cluster-on-ec2-to-amazon-eks/\n\n(2) https://aws.amazon.com/blogs/containers/migrating-from-self-managed-kubernetes-to-amazon-eks-here-are-some-key-considerations/","timestamp":"1711547940.0","upvote_count":"2"},{"content":"Selected Answer: B\nOption B","poster":"career360guru","upvote_count":"1","comment_id":"1169836","timestamp":"1710019020.0"},{"timestamp":"1707482700.0","upvote_count":"1","poster":"TheCloudGuruu","content":"Selected Answer: B\nB is the best option","comment_id":"1145459"},{"content":"Selected Answer: B\nIt is B","upvote_count":"1","comment_id":"1144492","timestamp":"1707399660.0","poster":"arberod"},{"content":"Selected Answer: B\nAnswer is B - because only in that case - we don't need to do any changes in application\n\nA - is out, because we will need to create deployments for many micro-services\nC - is out, because we will need to create ecs deployments for many micro-services\nD - is out, because it will require a lot of overhead and efforts for self-managed K8S setup","timestamp":"1707302460.0","comment_id":"1143245","poster":"HunkyBunky","upvote_count":"3"},{"poster":"kejam","content":"Selected Answer: B\nAnswer B: LEAST effort to migrate\nMinor changes to the manifest files seems like the least amount of work compared to what needs to be done in the other answers.","comment_id":"1142927","timestamp":"1707277320.0","upvote_count":"4"},{"upvote_count":"3","content":"Correct answer is B","timestamp":"1707184920.0","comment_id":"1141698","poster":"alexis123456"}],"unix_timestamp":1707184920,"answer":"B","question_images":[],"question_id":371,"choices":{"D":"Rebuild the on-premises Kubernetes cluster by hosting the cluster on Amazon EC2 instances. Migrate the open source container image repository to the EC2 instances. Deploy the manifests from on premises to the new cluster on AWS. Deploy an open source PostgreSQL database on the new cluster.","B":"Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster that has managed node groups. Copy the application containers to a new Amazon Elastic Container Registry (Amazon ECR) repository. Deploy the manifests from on premises to the EKS cluster. Create an Amazon Aurora PostgreSQL DB cluster.","C":"Create an Amazon Elastic Container Service (Amazon ECS) cluster that has an Amazon EC2 capacity pool. Copy the application containers to a new Amazon Elastic Container Registry (Amazon ECR) repository. Register each container image as a new task definition. Configure ECS services for each task definition to match the original Kubernetes deployments. Create an Amazon Aurora PostgreSQL DB cluster.","A":"Create an AWS App Runner service. Connect the App Runner service to the open source container image repository. Deploy the manifests from on premises to the App Runner service. Create an Amazon RDS for PostgreSQL database."},"answer_ET":"B","url":"https://www.examtopics.com/discussions/amazon/view/132985-exam-aws-certified-solutions-architect-professional-sap-c02/","answers_community":["B (100%)"],"answer_description":"","exam_id":33,"topic":"1","isMC":true,"timestamp":"2024-02-06 03:02:00","answer_images":[],"question_text":"A company wants to migrate its website to AWS. The website uses microservices and runs on containers that are deployed in an on-premises, self-managed Kubernetes cluster. All the manifests that define the deployments for the containers in the Kubernetes deployment are in source control.\n\nAll data for the website is stored in a PostgreSQL database. An open source container image repository runs alongside the on-premises environment.\n\nA solutions architect needs to determine the architecture that the company will use for the website on AWS.\n\nWhich solution will meet these requirements with the LEAST effort to migrate?"},{"id":"TXXnj7RBSIik4UnTqcbZ","answer":"D","choices":{"A":"Migrate storage of the contest entries to Amazon DynamoDB. Create a DynamoDB Accelerator (DAX) cluster. Rewrite the code to run as Amazon Elastic Container Service (Amazon ECS) containers that use the Fargate launch type. At the end of the contest, delete the DynamoDB table.","D":"Migrate the storage of the contest entries to Amazon DynamoDB. Rewrite the code as AWS Lambda functions. Set the DynamoDB TTL attribute on each entry to expire each entry at the end of the contest.","B":"Migrate the storage of the contest entries to Amazon Redshift. Rewrite the code as AWS Lambda functions. At the end of the contest, delete the Redshift cluster.","C":"Add an Amazon ElastiCache for Redis cluster in front of the RDS DB instances to cache the contest entries. Rewrite the code to run as Amazon Elastic Container Service (Amazon ECS) containers that use the Fargate launch type. Set the ElastiCache TTL attribute on each entry to expire each entry at the end of the contest."},"isMC":true,"answer_description":"","timestamp":"2024-02-07 02:20:00","answer_images":[],"answers_community":["D (65%)","A (35%)"],"answer_ET":"D","topic":"1","question_id":372,"question_images":[],"unix_timestamp":1707268800,"question_text":"A company uses a mobile app on AWS to run online contests. The company selects a winner at random at the end of each contest. The contests run for variable lengths of time. The company does not need to retain any data from a contest after the contest is finished.\n\nThe company uses custom code that is hosted on Amazon EC2 instances to process the contest data and select a winner. The EC2 instances run behind an Application Load Balancer and store contest entries on Amazon RDS DB instances. The company must design a new architecture to reduce the cost of running the contests.\n\nWhich solution will meet these requirements MOST cost-effectively?","url":"https://www.examtopics.com/discussions/amazon/view/133225-exam-aws-certified-solutions-architect-professional-sap-c02/","discussion":[{"comment_id":"1207896","upvote_count":"6","timestamp":"1715089200.0","content":"Selected Answer: D\nLambda for choosing a winner (it's a short-term task) and TTL (known before the contest starts) make sense.","poster":"m1xa"},{"upvote_count":"1","comment_id":"1347219","timestamp":"1737950160.0","content":"Selected Answer: A\nThis is what I hate AWS keep using impractical examples or unclear assumptions on their exam, it just train a lot of people just know yelling: \"lambda is cheap, lambda is efficiency\".","poster":"tonytam1991"},{"poster":"SIJUTHOMASP","comment_id":"1332098","content":"Selected Answer: D\nHere cost is the only concern and not performance, option A can be ruled out because no DAX is required. Simply Dynamo DB in option D would be more than sufficient.","upvote_count":"1","timestamp":"1735242000.0"},{"timestamp":"1731352440.0","comment_id":"1310266","content":"Option D \nMigrate Storage: Migrating storage from RDS DB instances (which require a significant upfront cost) to DynamoDB (a fully managed NoSQL database service with low usage-based costs) will significantly reduce the company's expenses.\nLambda Functions: Rewriting code as AWS Lambda functions (serverless computing for code execution) will eliminate the need for EC2 instances, reducing costs even further. Lambda functions are also ideal for tasks that run infrequently or have short durations.\nDynamoDB TTL Attribute: By using DynamoDB's TTL attribute to expire each entry at the end of the contest, the company can automatically clean up unused data without requiring manual deletion.","poster":"AzureDP900","upvote_count":"1"},{"comment_id":"1250863","timestamp":"1721366700.0","upvote_count":"2","content":"Selected Answer: D\nOption D is the most cost Efficient","poster":"Moghite"},{"timestamp":"1720416960.0","upvote_count":"1","comment_id":"1244103","content":"Selected Answer: A\nremove entire DynamoDB and DAX is used for reduce RCU","poster":"vip2"},{"comment_id":"1236889","content":"Should be A. With Variable lengths of time, Lambda is not the right choice.","upvote_count":"2","timestamp":"1719316740.0","poster":"Training"},{"timestamp":"1718196840.0","comment_id":"1229135","content":"Selected Answer: D\n\"the contests run for variable lengths of time\" does not mean that those time periods are not known. We do not need a fixed value for TTL but can use Lambda to change timestamp depending on each contest.","upvote_count":"3","poster":"trungtd"},{"timestamp":"1717256460.0","comment_id":"1222760","content":"Option D, The option A doesn't fulfill the requirement of cost effectiveness as there is no reason to use DAX","upvote_count":"2","poster":"9f02c8d"},{"comment_id":"1211957","content":"Selected Answer: A\nCorrect option is A here.\nFirst of all, we don't know how much time the contest will last as per requirements, so fix the TTL it's a mistake.\nSecond point, we can delete the entire table as we didn't the file after the end of the context, so no data to retain.\nFinally, for web contest, we don't know how much users will be online, and providing a cache layer might be a good solution.","upvote_count":"2","poster":"red_panda","timestamp":"1715779440.0"},{"comment_id":"1200176","poster":"titi_r","timestamp":"1713788700.0","content":"Selected Answer: D\nD - correct.","upvote_count":"2"},{"comment_id":"1196994","content":"D. There is no in memory cache requirement here for DAX. So omitting A","timestamp":"1713330960.0","upvote_count":"2","poster":"tushar321"},{"upvote_count":"3","content":"Selected Answer: A\nThe question is looking for “MOST cost-effectively”. I assume DAX + DynamoDB is cheaper than DynamoDB only. Cause DAX should be able to reduce the RCU cost and improve performance. This is an online contest, which means the RCU could be very high.\n\"the contests run for variable lengths of time\" means you can't set a fix TTL for the record entry. And even you can have a fix time, the record being submitted 1 min before the deadline will still be kept for the TTL and keep generating cost.","timestamp":"1713059640.0","poster":"bjexamprep","comment_id":"1195208"},{"comment_id":"1190638","poster":"pangchn","upvote_count":"4","timestamp":"1712441100.0","content":"Selected Answer: A\nVote for A here\nreason as specified by zouwelaar"},{"content":"Selected Answer: A\nYou are forgetting that the contests run for variable lengths of time. So Lambda and TTL are out.","comments":[{"upvote_count":"2","content":"Assuming each contest still has a set time from the start, D is most cost efficient, TTL is set based on each contest time. Lambda is only used to add/fetch entries and select random winner, runtime is minimal. I go with D here","poster":"w3ap0nx","timestamp":"1712511540.0","comment_id":"1191099"}],"upvote_count":"4","comment_id":"1188473","timestamp":"1712121480.0","poster":"zouwelaar"},{"comment_id":"1169840","content":"Selected Answer: D\nOption D","upvote_count":"2","timestamp":"1710019140.0","poster":"career360guru"},{"upvote_count":"3","content":"Time To Live (TTL) for DynamoDB is a cost-effective method for deleting items that are no longer relevant. TTL allows you to define a per-item expiration timestamp that indicates when an item is no longer needed. DynamoDB automatically deletes expired items within a few days of their expiration time, without consuming write throughput.","comment_id":"1163191","poster":"duriselvan","timestamp":"1709262540.0"},{"timestamp":"1707482760.0","content":"Selected Answer: D\nLambda to save cost","poster":"TheCloudGuruu","comment_id":"1145460","upvote_count":"4"},{"content":"Selected Answer: D\nD is the most cost-effective solution. It leverages DynamoDB for efficient, scalable storage with automatic data expiration via TTL and AWS Lambda for flexible, event-driven processing. This setup minimizes costs by using resources only when needed and automatically scaling to match demand without the need for manual intervention or over-provisioning.","comment_id":"1145327","timestamp":"1707467040.0","upvote_count":"4","poster":"nharaz"},{"upvote_count":"2","poster":"kejam","timestamp":"1707277560.0","content":"Selected Answer: D\nAnswer D:\nSeems to be the MOST cost-effective solution","comment_id":"1142928"},{"poster":"master9","content":"Selected Answer: D\nhe most cost-effective solution would be to use AWS Lambda for processing the contest data and selecting a winner. AWS Lambda is a serverless compute service that runs your code in response to events and automatically manages the underlying compute resources.\nDynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It’s a fully managed, multiregion, multimaster, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications","timestamp":"1707268800.0","comment_id":"1142873","upvote_count":"2"}],"exam_id":33},{"id":"9CH2VQ05LnduV1gtMn6Y","answers_community":["A (75%)","D (25%)"],"answer":"A","answer_images":[],"topic":"1","question_text":"A company has implemented a new security requirement. According to the new requirement, the company must scan all traffic from corporate AWS instances in the company's VPC for violations of the company's security policies. As a result of these scans, the company can block access to and from specific IP addresses.\n\nTo meet the new requirement, the company deploys a set of Amazon EC2 instances in private subnets to serve as transparent proxies. The company installs approved proxy server software on these EC2 instances. The company modifies the route tables on all subnets to use the corresponding EC2 instances with proxy software as the default route. The company also creates security groups that are compliant with the security policies and assigns these security groups to the EC2 instances.\n\nDespite these configurations, the traffic of the EC2 instances in their private subnets is not being properly forwarded to the internet.\n\nWhat should a solutions architect do to resolve this issue?","url":"https://www.examtopics.com/discussions/amazon/view/132986-exam-aws-certified-solutions-architect-professional-sap-c02/","choices":{"B":"Add a rule to the security group that is assigned to the proxy EC2 instances to allow all traffic between instances that have this security group. Assign this security group to all EC2 instances in the VPC.","A":"Disable source/destination checks on the EC2 instances that run the proxy software.","C":"Change the VPCs DHCP options set. Set the DNS server options to point to the addresses of the proxy EC2 instances.","D":"Assign one additional elastic network interface to each proxy EC2 instance. Ensure that one of these network interfaces has a route to the private subnets. Ensure that the other network interface has a route to the internet."},"discussion":[{"upvote_count":"9","content":"Selected Answer: A\nAnswer A:\nProxies like NATs will need SrcDestCheck disabled\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html#EIP_Disable_SrcDestCheck","poster":"kejam","timestamp":"1707278700.0","comment_id":"1142934"},{"comment_id":"1312399","upvote_count":"1","poster":"0b43291","timestamp":"1731643800.0","content":"Selected Answer: A\nIn an Amazon VPC, the source/destination check is a security feature that ensures that an instance cannot be used as a network gateway or router to forward traffic between resources. By default, this check is enabled on all EC2 instances.\n\nWhen you want to use an EC2 instance as a transparent proxy or network appliance to forward traffic between resources, you need to disable the source/destination check on that instance. This allows the instance to receive and forward traffic that is not destined for itself.\n\nThe other options provided would not resolve the issue:\n\n Option B (adding a security group rule) would not enable the proxy instances to forward traffic, as the source/destination check is a separate network configuration.\n Option C (changing DHCP options) would not affect the ability of the proxy instances to forward traffic.\n Option D (adding additional network interfaces) is not necessary, as the issue is related to the source/destination check and not the network interface configuration."},{"poster":"AzureDP900","content":"Option A resolves the issue by allowing the traffic of the EC2 instances in their private subnets to be properly forwarded to the internet.","comment_id":"1310265","timestamp":"1731352200.0","upvote_count":"1"},{"comment_id":"1297327","upvote_count":"1","poster":"chris_spencer","content":"With A i am missing the route to the internet via a NAT Gateway or NAT Instance via a ENI, with D i miss the scr/dst check","timestamp":"1728893580.0"},{"content":"Selected Answer: D\n\"the company deploys a set of Amazon EC2 instances in private subnets to serve as transparent proxies.\" How could a Proxy in a private Subnet communicate with the Internet? So we need a second network card with connection to an IGW. Anwser D","comment_id":"1289365","timestamp":"1727341740.0","upvote_count":"2","poster":"ahrentom"},{"comment_id":"1190001","upvote_count":"2","timestamp":"1712334360.0","content":"Selected Answer: D\nWhile disabling security checks might seem like a solution, it's not recommended for production environments as it weakens security. The issue lies in routing, not security","poster":"Russs99"},{"poster":"TheCloudGuruu","comment_id":"1145464","content":"Selected Answer: A\nAnswer is A, proxy","timestamp":"1707482880.0","upvote_count":"1"},{"comment_id":"1143242","upvote_count":"1","poster":"HunkyBunky","timestamp":"1707302160.0","content":"Selected Answer: A\nAnswer is - A"},{"poster":"alexis123456","content":"Correct Answer is A","comment_id":"1141700","timestamp":"1707185100.0","upvote_count":"3"}],"timestamp":"2024-02-06 03:05:00","isMC":true,"answer_description":"","question_id":373,"answer_ET":"A","question_images":[],"exam_id":33,"unix_timestamp":1707185100},{"id":"NunBsF4TqQ4ZUbbjft4i","answer_ET":"C","answers_community":["C (81%)","Other"],"unix_timestamp":1707185160,"choices":{"C":"Create a new CloudFormation template that strictly provisions the existing VPC resources and configuration. From the CloudFormation console, create a new stack by importing the Existing resources.","A":"Create a new AWS Cloud Development Kit (AWS CDK) stack that strictly provisions the existing VPC resources and configuration. Use AWS CDK to import the VPC into the stack and to manage the VPC.","D":"Create a new CloudFormation template that creates the VPC. Use the AWS Serverless Application Model (AWS SAM) CLI to import the VPC.","B":"Create a CloudFormation stack set that creates the VPC. Use the stack set to import the VPC into the stack."},"question_text":"A company is running its solution on AWS in a manually created VPC. The company is using AWS CloudFormation to provision other parts of the infrastructure. According to a new requirement, the company must manage all infrastructure in an automatic way.\n\nWhat should the company do to meet this new requirement with the LEAST effort?","question_images":[],"answer_images":[],"topic":"1","question_id":374,"exam_id":33,"isMC":true,"discussion":[{"comment_id":"1145607","content":"Selected Answer: C\nD - SAM cannot used for importing and currently we are already using Cloudformation\nB - Stacksets used to create multiple stacks and currently we are using Cloudformation\nA - CDK , we will need to change all the entire stack from Cloudformation to CDK\nC - We can import existing resources in Cloudformation: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/resource-import.html","upvote_count":"10","timestamp":"1707493440.0","poster":"saggy4"},{"upvote_count":"1","timestamp":"1731351960.0","poster":"AzureDP900","comment_id":"1310263","content":"Option C provides a straightforward way to manage the existing infrastructure in an automatic way with minimal effort."},{"upvote_count":"2","timestamp":"1728677280.0","comment_id":"1296244","content":"Selected Answer: C\n\"Use the stack set to import the VPC into the stack.\"\nCloudFormation stack sets are designed for deploying stacks across multiple accounts and regions. They are NOT meant for importing existing resources into a single VPC or stack.","poster":"JoeTromundo"},{"poster":"titi_r","comment_id":"1200192","upvote_count":"2","timestamp":"1713789780.0","content":"Selected Answer: C\nC - correct."},{"upvote_count":"2","poster":"titi_r","content":"Selected Answer: C\nC - correct.","timestamp":"1713789720.0","comment_id":"1200191"},{"upvote_count":"3","comment_id":"1186171","timestamp":"1711814460.0","poster":"VerRi","content":"Selected Answer: C\nB means to create a stack set just for VPC, we don't need a stack set to handle just 1 resource"},{"timestamp":"1710020220.0","upvote_count":"1","comment_id":"1169854","poster":"career360guru","content":"Selected Answer: B\nOption B"},{"content":"Selected Answer: C\nI discarded B, because IMO stack sets are not needed.","comment_id":"1152797","upvote_count":"3","poster":"marszalekm","timestamp":"1708201500.0"},{"content":"Selected Answer: B\nCreate a CloudFormation stack","comment_id":"1145466","upvote_count":"1","poster":"TheCloudGuruu","timestamp":"1707482940.0"},{"content":"Selected Answer: B\nagree B","timestamp":"1707400140.0","comment_id":"1144504","poster":"arberod","comments":[{"upvote_count":"1","comment_id":"1151280","content":"Changed to C","timestamp":"1708029060.0","poster":"arberod"}],"upvote_count":"1"},{"timestamp":"1707301740.0","content":"Selected Answer: C\nI guess C\n\nA - is out, because CDK not allow to import any exists resources\nB - is out, becuase StackSets are used only for create multiple stacks and manage them from a single stack\nD - is out, because AWS SAM cli - can't be used for import resources in CF","poster":"HunkyBunky","upvote_count":"4","comment_id":"1143233"},{"poster":"kejam","comment_id":"1142938","content":"Selected Answer: B\nAnswer B: Because CloudFormation is already in use.\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/resource-import.html","upvote_count":"2","timestamp":"1707279000.0"},{"comments":[],"upvote_count":"1","timestamp":"1707278880.0","comment_id":"1142937","poster":"kejam","content":"Selected Answer: A\nAnswer A:\nhttps://aws.amazon.com/blogs/devops/how-to-import-existing-resources-into-aws-cdk-stacks/\nhttps://docs.aws.amazon.com/cdk/v2/guide/cli.html#cli-import"},{"timestamp":"1707185160.0","poster":"alexis123456","upvote_count":"2","content":"Correct Answer is B","comment_id":"1141701"}],"answer_description":"","timestamp":"2024-02-06 03:06:00","url":"https://www.examtopics.com/discussions/amazon/view/132987-exam-aws-certified-solutions-architect-professional-sap-c02/","answer":"C"},{"id":"wXkCi2IAZvLaRvH7ACkb","answers_community":["C (100%)"],"topic":"1","unix_timestamp":1707185280,"answer_description":"","answer_images":[],"question_text":"A company has developed a new release of a popular video game and wants to make it available for public download. The new release package is approximately 5 GB in size. The company provides downloads for existing releases from a Linux-based, publicly facing FTP site hosted in an on-premises data center. The company expects the new release will be downloaded by users worldwide. The company wants a solution that provides improved download performance and low transfer costs, regardless of a user's location.","question_images":[],"timestamp":"2024-02-06 03:08:00","discussion":[{"upvote_count":"3","content":"Option C provides a scalable, secure, and cost-effective solution for serving the game files worldwide.\nHere's why:\n• Amazon Route 53: Provides fast and reliable DNS resolution for users to access the game files.\n• Amazon S3 bucket: Stores the game files in a highly available and durable storage system. Upload the game files (5 GB) to the S3 bucket, which will be charged at the standard rate (approximately $0.023 per GB).\n• Amazon CloudFront: A content delivery network (CDN) that caches frequently accessed resources like the game files. This reduces the latency experienced by users worldwide and provides better download performance.","timestamp":"1731351720.0","poster":"AzureDP900","comment_id":"1310260"},{"upvote_count":"2","content":"Selected Answer: C\nOption C","comment_id":"1169857","poster":"career360guru","timestamp":"1710020400.0"},{"timestamp":"1707483360.0","content":"Selected Answer: C\nC. S3 is the best option, no need for requestor pays","comment_id":"1145471","upvote_count":"2","poster":"TheCloudGuruu"},{"upvote_count":"2","content":"Selected Answer: C\nIt is C","poster":"arberod","comment_id":"1144509","timestamp":"1707400320.0"},{"upvote_count":"2","comment_id":"1142940","poster":"kejam","timestamp":"1707279180.0","content":"Selected Answer: C\nAnswer C:"},{"content":"Correct Answer is C","poster":"alexis123456","upvote_count":"2","timestamp":"1707185280.0","comment_id":"1141702"}],"choices":{"D":"Configure Amazon Route 53 and an Amazon S3 bucket for website hosting. Upload the game files to the S3 bucket. Set Requester Pays for the S3 bucket. Publish the game download URL for users to download the package.","B":"Store the game files on Amazon EFS volumes that are attached to Amazon EC2 instances within an Auto Scaling group. Configure an FTP service on each of the EC2 instances. Use an Application Load Balancer in front of the Auto Scaling group. Publish the game download URL for users to download the package.","C":"Configure Amazon Route 53 and an Amazon S3 bucket for website hosting. Upload the game files to the S3 bucket. Use Amazon CloudFront for the website. Publish the game download URL for users to download the package.","A":"Store the game files on Amazon EBS volumes mounted on Amazon EC2 instances within an Auto Scaling group. Configure an FTP service on the EC2 instances. Use an Application Load Balancer in front of the Auto Scaling group. Publish the game download URL for users to download the package."},"url":"https://www.examtopics.com/discussions/amazon/view/132988-exam-aws-certified-solutions-architect-professional-sap-c02/","question_id":375,"isMC":true,"exam_id":33,"answer_ET":"C","answer":"C"}],"exam":{"id":33,"name":"AWS Certified Solutions Architect - Professional SAP-C02","isBeta":false,"lastUpdated":"11 Apr 2025","isImplemented":true,"isMCOnly":true,"numberOfQuestions":529,"provider":"Amazon"},"currentPage":75},"__N_SSP":true}