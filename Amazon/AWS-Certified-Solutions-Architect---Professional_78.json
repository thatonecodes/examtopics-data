{"pageProps":{"questions":[{"id":"iOkmRfevrNkbWBP7DElc","question_images":[],"exam_id":32,"question_id":386,"topic":"1","discussion":[{"upvote_count":"45","poster":"Moon","content":"I do support answer \"B\".\nThe question does not request availability on different AZ.\nThe solution can tolerate 4 hours RTO. Therefore, CloudFront for Redshift, and backup stored in S3.\nhttps://aws.amazon.com/blogs/big-data/building-multi-az-or-multi-region-amazon-redshift-clusters/\nA: Redshift can not work on Multi AZ!\nC: Kinesis not meant for cross region population!\nD: is a perfect answer, but not cost effective. More redundant than required!","timestamp":"1632316140.0","comment_id":"14421","comments":[{"comment_id":"392312","content":"Option C can't be a solution because it is cost effective. But the link provided by Moon (https://aws.amazon.com/blogs/big-data/building-multi-az-or-multi-region-amazon-redshift-clusters/) brings up a very interesting point: The link outlines a solution where Kinesis Streams can be used to deliver data to a Redshift Cluster either in a different AZ or in a different different Region.\nRedshift can be a destination for Kinesis Firehose (https://docs.aws.amazon.com/firehose/latest/dev/create-destination.html#create-destination-redshift).\nIf Kinesis Streams can be cross regional, then why Firehose can't be?\nHowever I couldn't find any documentation on how Kinesis (Either Streams or Firehose) can be used for cross region population?","timestamp":"1635970620.0","upvote_count":"2","poster":"DashL"}]},{"upvote_count":"10","comment_id":"15613","timestamp":"1632390300.0","poster":"Marcos","content":"Agree with B.\n\nhttps://aws.amazon.com/redshift/faqs/?nc1=h_ls\n\nQ: What happens to my data warehouse cluster availability and data durability if my data warehouse cluster's Availability Zone (AZ) has an outage?\nIf your Amazon Redshift data warehouse cluster's Availability Zone becomes unavailable, you will not be able to use your cluster until power and network access to the AZ are restored. Your data warehouse cluster's data is preserved so you can start using your Amazon Redshift data warehouse as soon as the AZ becomes available again. In addition, you can also choose to restore any existing snapshots to a new AZ in the same Region. Amazon Redshift will restore your most frequently accessed data first so you can resume queries as quickly as possible."},{"timestamp":"1702134060.0","comment_id":"1091866","content":"now Redshift can support Multi AZ!\n-----------\nUnlike single-AZ deployments, customers can now improve availability of Redshift by running their data warehouse in a multi-AZ deployment. A multi-AZ deployment allows you to run your data warehouse in multiple AWS Availability Zones (AZ) simultaneously and continue operating in unforeseen failure scenarios. No application changes are required to maintain business continuity since the Multi-AZ deployment is managed as a single data warehouse with one endpoint. Multi-AZ deployments reduce recovery time by guaranteeing capacity to automatically recover and are intended for customers with business-critical analytics applications that require the highest levels of availability and resiliency to AZ failures. This also allows customers to implement a solution that is more compliant with the recommendations of the Reliability Pillar of the AWS Well-Architected Framework. To learn more about Amazon Redshift Multi-AZ refer here.","upvote_count":"1","poster":"Chris22usa"},{"comment_id":"942929","upvote_count":"1","poster":"SkyZeroZx","timestamp":"1688484720.0","content":"Selected Answer: B\nI do support answer \"B"},{"content":"first thought B and most comments say B stating my thoughts - most cost effective , s2 backup, and CFN template to build infra quickly if required","upvote_count":"1","timestamp":"1666386660.0","comment_id":"701175","poster":"mrgreatness"},{"timestamp":"1639113960.0","content":"B. Ensure that the Amazon Redshift cluster creation has been templated using AWS CloudFormation so it can easily be launched in another Availability Zone and data populated from the automated Redshift back-ups stored in Amazon S3.","poster":"cldy","comment_id":"498317","upvote_count":"1"},{"comment_id":"493765","poster":"AzureDP900","timestamp":"1638631620.0","content":"I will go with B , It is lowest cost.","upvote_count":"1"},{"timestamp":"1636191540.0","content":"It's B","comment_id":"450323","upvote_count":"1","poster":"andylogan"},{"comment_id":"441878","upvote_count":"2","content":"Currently, Amazon Redshift only supports Single-AZ deployments. You can run data warehouse clusters in multiple AZ's by loading data into two Amazon Redshift data warehouse clusters in separate AZs from the same set of Amazon S3 input files. With Redshift Spectrum, you can spin up multiple clusters across AZs and access data in Amazon S3 without having to load it into your cluster. In addition, you can also restore a data warehouse cluster to a different AZ from your data warehouse cluster snapshots.","timestamp":"1636089120.0","poster":"nwk"},{"timestamp":"1636061880.0","content":"BBB\n---","upvote_count":"1","poster":"tgv","comment_id":"437810"},{"poster":"WhyIronMan","content":"I'll go with B","timestamp":"1635910560.0","comment_id":"351892","upvote_count":"1"},{"upvote_count":"2","comment_id":"343737","content":"agree with B","poster":"Waiweng","timestamp":"1635769800.0"},{"comment_id":"289493","timestamp":"1635574080.0","upvote_count":"2","content":"go for B","poster":"Kian1"},{"timestamp":"1635515940.0","poster":"Ebi","content":"I go with B","comment_id":"283966","upvote_count":"3"},{"comment_id":"263438","content":"I'll go with B","poster":"sanjaym","timestamp":"1635433740.0","upvote_count":"2"},{"timestamp":"1634847300.0","comment_id":"243327","content":"B\nA: Redshift cluster doesn't support Multi-AZ\nC/D: Both need to deploy multiple identical Redshift clusters into two AZ/Regions, which double the cost.","poster":"newme","comments":[{"poster":"newme","comment_id":"243329","content":"Only that, how to know 4 fours is enough for CloudFormation to start up new Redshift cluster?","timestamp":"1635260400.0","upvote_count":"2"}],"upvote_count":"2"},{"comment_id":"242005","timestamp":"1634378760.0","upvote_count":"1","poster":"T14102020","content":"B is correct as most cost-effective."},{"content":"I'll go with B","comment_id":"228799","poster":"jackdryan","timestamp":"1634237940.0","upvote_count":"2"},{"content":"Although X is an ideal option it is not cost effective. The best option which is also cost effective is B.","timestamp":"1634141220.0","poster":"Bulti","upvote_count":"5","comment_id":"227978"},{"timestamp":"1634079180.0","upvote_count":"1","poster":"vjt","content":"B it is.","comment_id":"215772"},{"timestamp":"1633841520.0","poster":"Ebi","upvote_count":"1","content":"Answer is B","comment_id":"211832"},{"poster":"fullaws","content":"B is correct, no need regional backup as AWS had 99.99 region uptime. Redshift had 99.9% uptime which allow 8 hour of downtime, Redshift AZ recovery is enough for this case.","timestamp":"1633636740.0","comment_id":"148905","upvote_count":"2"},{"comment_id":"134180","upvote_count":"2","poster":"NikkyDicky","content":"B. cross-region is not required","timestamp":"1633345800.0"},{"timestamp":"1633246680.0","comment_id":"133228","poster":"mat2020","upvote_count":"2","content":"answer :b"},{"comment_id":"127953","timestamp":"1633154400.0","content":"B seems the only cost effective and viable solution","upvote_count":"2","poster":"Snarfhound"},{"timestamp":"1633144620.0","upvote_count":"1","comment_id":"127515","content":"C is correct.","poster":"HoganYu"},{"upvote_count":"1","comment_id":"105472","content":"Option B","poster":"meenu2225","timestamp":"1632955740.0"},{"poster":"jv1","comment_id":"100427","upvote_count":"3","timestamp":"1632942780.0","content":"B \nhttps://aws.amazon.com/blogs/big-data/automate-amazon-redshift-cluster-creation-using-aws-cloudformation/"},{"poster":"NKnab","upvote_count":"1","timestamp":"1632914220.0","comment_id":"99809","content":"https://aws.amazon.com/blogs/big-data/building-multi-az-or-multi-region-amazon-redshift-clusters/ - c is correct"},{"content":"B. \nC and D are not cost-efficient, as they are requiring two identical clusters. A is not technically possible. Only remaining option is B :-) .","poster":"JAWS1600","timestamp":"1632666600.0","upvote_count":"1","comment_id":"90915"},{"comments":[{"content":"Yes, four hours is the key.\nBut how to know 4 hours is enough?","poster":"newme","upvote_count":"1","timestamp":"1634804760.0","comment_id":"243313"}],"comment_id":"90871","upvote_count":"1","timestamp":"1632664320.0","poster":"qianhaopower","content":"Four hours is the key, have enough time to launch new stack from CloudFormation"},{"upvote_count":"2","timestamp":"1632512460.0","comment_id":"30062","content":"I will go with B, as the question asks \"the cluster can either operate or be restored within four hours.\" so, not talking about another region.","poster":"JayK"},{"timestamp":"1632476760.0","content":"I will go for B.\nUsing cfn it can quickly create a new database within 4 hours time which has been asked.","comment_id":"28497","upvote_count":"5","poster":"9Ow30"},{"timestamp":"1632457200.0","upvote_count":"2","comment_id":"17544","content":"Q: Does Amazon Redshift support Multi-AZ Deployments?\n\nCurrently, Amazon Redshift only supports Single-AZ deployments. You can run data warehouse clusters in multiple AZ's by loading data into two Amazon Redshift data warehouse clusters in separate AZs from the same set of Amazon S3 input files. With Redshift Spectrum, you can spin up multiple clusters across AZs and access data in Amazon S3 without having to load it into your cluster. In addition, you can also restore a data warehouse cluster to a different AZ from your data warehouse cluster snapshots.","poster":"TechGuru"},{"timestamp":"1632193500.0","comments":[{"comment_id":"44228","poster":"arunkumar","timestamp":"1632577680.0","upvote_count":"5","content":"D is not cost-effective."}],"comment_id":"14050","content":"D\nA\\B: There is no region replication.\nC: Kinesis cannot stream across region. https://engineering.opsgenie.com/cross-region-replication-of-kinesis-streams-4a62f3bb269d\nD: This seems most probable. https://aws.amazon.com/blogs/big-data/a-zero-administration-amazon-redshift-database-loader/","poster":"donathon","upvote_count":"1"}],"answer_images":[],"choices":{"C":"Use Amazon Kinesis Data Firehose to collect the data ahead of ingestion into Amazon Redshift and create clusters using AWS CloudFormation in another region and stream the data to both clusters.","D":"Create two identical Amazon Redshift clusters in different regions (one as the primary, one as the secondary). Use Amazon S3 cross-region replication from the primary to secondary region, which triggers an AWS Lambda function to populate the cluster in the secondary region.","B":"Ensure that the Amazon Redshift cluster creation has been templated using AWS CloudFormation so it can easily be launched in another Availability Zone and data populated from the automated Redshift back-ups stored in Amazon S3.","A":"Ensure that the Amazon Redshift cluster has been set up to make use of Auto Scaling groups with the nodes in the cluster spread across multiple Availability Zones."},"answer_description":"Q: What happens to my data warehouse cluster availability and data durability if my data warehouse cluster's Availability Zone (AZ) has an outage?\nIf your Amazon Redshift data warehouse cluster's Availability Zone becomes unavailable, you will not be able to use your cluster until power and network access to the AZ are restored. Your data warehouse cluster's data is preserved so you can start using your Amazon Redshift data warehouse as soon as the AZ becomes available again. In addition, you can also choose to restore any existing snapshots to a new AZ in the same Region. Amazon Redshift will restore your most frequently accessed data first so you can resume queries as quickly as possible.\nReference:\nhttps://aws.amazon.com/redshift/faqs/?nc1=h_ls","answer_ET":"B","answers_community":["B (100%)"],"answer":"B","isMC":true,"question_text":"A Solutions Architect has been asked to look at a company's Amazon Redshift cluster, which has quickly become an integral part of its technology and supports key business process. The Solutions Architect is to increase the reliability and availability of the cluster and provide options to ensure that if an issue arises, the cluster can either operate or be restored within four hours.\nWhich of the following solution options BEST addresses the business need in the most cost-effective manner?","timestamp":"2019-10-07 02:47:00","url":"https://www.examtopics.com/discussions/amazon/view/6185-exam-aws-certified-solutions-architect-professional-topic-1/","unix_timestamp":1570409220},{"id":"0soGBXd1EOjuhgfl8hhp","choices":{"C":"Only allow launching of EC2 instances using a centralized DevOps team, which is given work packages via notifications from an internal ticketing system. Users make requests for resources using this ticketing tool, which has manual information security approval steps to ensure that EC2 instances are only launched from approved AMIs.","E":"Use a scheduled AWS Lambda function to scan through the list of running instances within the virtual private cloud (VPC) and determine if any of these are based on unapproved AMIs. Publish a message to an SNS topic to inform Information Security that this occurred and then shut down the instance.","A":"Use IAM policies to restrict the ability of users or other automated entities to launch EC2 instances based on a specific set of pre-approved AMIs, such as those tagged in a specific way by Information Security.","B":"Use regular scans within Amazon Inspector with a custom assessment template to determine if the EC2 instance that the Amazon Inspector Agent is running on is based upon a pre-approved AMI. If it is not, shut down the instance and inform Information Security by email that this occurred.","D":"Use AWS Config rules to spot any launches of EC2 instances based on non-approved AMIs, trigger an AWS Lambda function to automatically terminate the instance, and publish a message to an Amazon SNS topic to inform Information Security that this occurred."},"url":"https://www.examtopics.com/discussions/amazon/view/5102-exam-aws-certified-solutions-architect-professional-topic-1/","question_id":387,"answer_ET":"AD","discussion":[{"upvote_count":"32","timestamp":"1632255960.0","content":"AD\nA: https://aws.amazon.com/premiumsupport/knowledge-center/restrict-launch-tagged-ami/\nB: Amazon Inspector is an automated security assessment service that helps you test the network accessibility of your Amazon EC2 instances and the security state of your applications running on the instances.\nC: Delays the deployment.\nD: This ensure that the compliance is enforced.\nE: Not effective.","poster":"donathon","comments":[{"comment_id":"331141","timestamp":"1635687300.0","poster":"SD13","comments":[{"timestamp":"1636068120.0","upvote_count":"1","content":"I agree...\n\nStraight from Jon Bonso exam:\n\nThe option that says: Set up IAM policies to restrict the ability of users to launch EC2 instances based on a specific set of pre-approved AMIs which were tagged by the Security team is incorrect because setting up an IAM Policy will totally restrict the development team from launching EC2 instances with unapproved AMIs which could impact their CI/CD process. The scenario clearly says that the solution should not have any interruption in the company's development process.","poster":"memester","comments":[{"timestamp":"1636114500.0","upvote_count":"2","content":"Jon Bonso is also wrong then. Because Option-D will also have disruption. So left with E is only good choice? Both memester and SD13, please tell us ur answer when reply. I am still stick to A D.","comment_id":"454913","poster":"StelSen"}],"comment_id":"405595"}],"content":"A will impact the development process.","upvote_count":"2"}],"comment_id":"13329"},{"timestamp":"1632186600.0","comment_id":"10744","content":"a, d\nhttps://aws.amazon.com/premiumsupport/knowledge-center/restrict-launch-tagged-ami/","comments":[{"upvote_count":"2","poster":"dpvnme","content":"Yes, I think A&D are the best choices here","comment_id":"11465","timestamp":"1632201300.0"}],"upvote_count":"11","poster":"awsec2"},{"comment_id":"709271","upvote_count":"1","content":"Selected Answer: AD\nE doesn't sound correct. why to have new lamda to scan all VM. remember lambda has limited run time of 15 minute. what if you have ton of VMs.","timestamp":"1667319600.0","poster":"AjayPrajapati","comments":[{"upvote_count":"1","content":"The Lambda doesn't scan the VMs themselves, it scans a *list* of all the VMs and compares each entry with the approved AMI list. That won't take anything like 15 minutes.","comment_id":"712676","timestamp":"1667774100.0","poster":"Byrney"}]},{"comment_id":"701179","upvote_count":"1","timestamp":"1666387320.0","content":"Answer is 100% A & D -- you can specify tags in conditions, so have a tag that only allows RunInstance with a specific AMI. Its definitely A & D -- D because Config is perfect solution. I'm 100% certain of this","poster":"mrgreatness"},{"comment_id":"676047","content":"Selected Answer: DE\nD&E Security team is incorrect because setting up an IAM Policy will totally restrict the development team from launching EC2 instances with unapproved AMIs","upvote_count":"1","timestamp":"1663846140.0","poster":"JohnPi"},{"upvote_count":"1","comment_id":"670674","content":"D , E is correct","timestamp":"1663321980.0","poster":"akash_it"},{"upvote_count":"2","timestamp":"1662139560.0","content":"Selected Answer: DE\nA will block development.\n\nOnly options that make sense are D and E.","comment_id":"657655","poster":"epomatti"},{"timestamp":"1648594800.0","content":"Selected Answer: AD\nAD are right","comment_id":"577918","poster":"jj22222","upvote_count":"3"},{"upvote_count":"1","comment_id":"554430","poster":"futen0326","content":"D E\n\nIf you're already settled on using D why would you also use A? You've already taken care of the launch requirement, now we must solve the potential issue of unapproved AMIs that are may be past launch, but running.","timestamp":"1645616340.0"},{"poster":"cannottellname","upvote_count":"1","content":"AAAAAA DDDDDD","timestamp":"1644297360.0","comment_id":"542869"},{"poster":"tkanmani76","upvote_count":"1","content":"A and D - refer link for A - https://aws.amazon.com/premiumsupport/knowledge-center/restrict-launch-tagged-ami/","timestamp":"1640171340.0","comment_id":"507008"},{"timestamp":"1638631860.0","upvote_count":"1","poster":"AzureDP900","comment_id":"493768","content":"AD is right"},{"content":"It's A D","timestamp":"1636095360.0","poster":"andylogan","upvote_count":"1","comment_id":"450329"},{"poster":"DerekKey","timestamp":"1636072320.0","content":"D&E\nA is WRONG - since it will stall agile continuous integration and deployment process that cannot be stalled by the solution\nB - incorrect\nC - refer to A","comment_id":"409163","upvote_count":"2"},{"timestamp":"1636016340.0","comment_id":"351894","upvote_count":"2","poster":"WhyIronMan","content":"I'll go with A,D"},{"upvote_count":"2","comment_id":"343738","content":"will go for A,D","poster":"Waiweng","timestamp":"1635993540.0"},{"comment_id":"335273","timestamp":"1635925740.0","upvote_count":"1","content":"D, E is the best for me.","poster":"ppshein"},{"upvote_count":"4","comment_id":"333048","content":"A and D. \n\nThe question asks for *minimal* impact on development (which rules out C). But it doesn't ask for *no* impact. Enforcing a rule like this will always have *some* impact - otherwise what's the point?","poster":"sarah_t","timestamp":"1635828600.0"},{"timestamp":"1635567240.0","poster":"anandbabu","content":"correct answer is DE","upvote_count":"2","comment_id":"330538"},{"poster":"ExtHo","comment_id":"322533","timestamp":"1635537960.0","upvote_count":"2","comments":[{"poster":"sarah_t","content":"A would only restrict the choice of instance but not prevent launching instances. \n\nE would terminate instances that were launched before the rule went into effect --> major impact!","upvote_count":"3","comment_id":"333059","timestamp":"1635867120.0"}],"content":"DE are correct choices \nA: is incorrect because setting up an IAM Policy will totally restrict the development team from launching EC2 instances with unapproved AMIs which could impact their CI/CD process. The scenario clearly says that the solution should not have any interruption in the company's development process.\n\nB: is incorrect because the Amazon Inspector service is just an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. It does not have the capability to detect EC2 instances that are using unapproved AMIs, unlike AWS Config.\n\nC: is incorrect because having manual information security approval will impact the development process. A better solution is to implement an automated process using AWS Config and a scheduled AWS Lambda function."},{"content":"AD\nhttps://aws.amazon.com/blogs/devops/aws-config-checking-for-compliance-with-new-managed-rule-options/\n\"AWS Config rules can now check that running instances are using approved Amazon Machine Images, or AMIs. You can specify a list of approved AMI by ID or provide a tag to specify the list of AMI Ids.\"","upvote_count":"1","comment_id":"291173","timestamp":"1635454500.0","poster":"AJBA"},{"comment_id":"291098","poster":"wind","content":"I'll go with AD.","timestamp":"1635440940.0","upvote_count":"1"},{"timestamp":"1635379500.0","content":"will go with A&D","poster":"Kian1","upvote_count":"2","comment_id":"289502"},{"comment_id":"283975","upvote_count":"3","timestamp":"1635294360.0","content":"AD is my answer","poster":"Ebi"},{"content":"I'll go with AD","upvote_count":"2","timestamp":"1635086280.0","poster":"sanjaym","comment_id":"263446"},{"timestamp":"1634907420.0","poster":"vipgcp","comment_id":"253878","upvote_count":"2","content":"Ans: A&E\nA - It's not stalling the Dev Process, It's enforcing to use right IAM\nB - Inspector is not meant for this\nC - This will stall Dev process\nD - In case Dev launched non approved AMI then it's TERMINATING the instance - which is not good, there is a possibility Dev make it approve later as Custom image\nE - D and E are quite similar, Lamda is ok instead of Config rule but it's Shutting down an instance."},{"content":"A & D. A is a preventive control and D is a respond control. Using easy avail services to implement. These controls are to ensure the AMI are approved and that do not disrupt the development. Terminating the instance will just ensure the EBS volume are removed and no charges will be there.","timestamp":"1634857320.0","poster":"happpieee","upvote_count":"3","comment_id":"247086"},{"poster":"newme","upvote_count":"1","timestamp":"1634840460.0","content":"I'll go for A&D.\n\"deployment process that cannot be stalled by the solution\" is really an annoying requirement.\nIt doesn't provide specific definition of what could be stalling development process.\n\nB: I don't think Amazon Inspector can check what AMI is the EC2 instance is using.\nC: It's still manual work, miss could happen.\nE: It needs additional development work of Lambda. I think It works but not the best way.","comment_id":"243339"},{"poster":"T14102020","timestamp":"1634811060.0","upvote_count":"2","content":"Correct is DE. You can't interrupt the CI/CD process so you have to first let the instance launch and then delete it","comment_id":"242013"},{"comment_id":"229241","content":"Config uses system manager to execute a remediation action 100% no doubt about it. \nConfig doesn't have an action to execute Lambda by default as a remediation 100% no doubt about it\n\nin both D and E we will have to write a function as there is no default remediation as mentioned.\n\nshutting down the instance is better than terminating it which can't be done straight forward anyway \n\nthe environment is a dev environment shutting down will not stall the dev process as nothing on the dev instances will be lost.\n\nchecked all the previous results step by step on the interface.\ni'll go with A&E","timestamp":"1634654940.0","poster":"YouYouYou","upvote_count":"1"},{"upvote_count":"3","timestamp":"1634584140.0","content":"I'll go with A,D","comment_id":"228800","poster":"jackdryan"},{"upvote_count":"3","comments":[{"comment_id":"270498","upvote_count":"3","content":"I changed my answer to A, D. E requires development. A and D accomplishes both, no disruption to the CI/CD process and least impact to Development.","timestamp":"1635100860.0","poster":"Bulti"}],"content":"D,E is the right answer. a is incorrect because it is impactful to the development process, B is not a possible solution, c impacts the current CI/CD process.","timestamp":"1634575800.0","poster":"Bulti","comment_id":"228055"},{"timestamp":"1634573940.0","upvote_count":"2","content":"Answer id DE","poster":"Ebi","comment_id":"211833"},{"upvote_count":"2","poster":"Ganfeng","comment_id":"179213","content":"Support DE","timestamp":"1634536020.0"},{"timestamp":"1634344500.0","content":"This wording is confusing,think question means by least impact on process not about shutting down or terminating VM s,but continuing the Agile Process \nagile continuous integration and deployment process that cannot be stalled by the solution \nwould be in line with A& D","comment_id":"174897","poster":"kaush","upvote_count":"1"},{"content":"Correct Answer D&E\nA - incorrect. “IAM policies to restrict” does not address “process that cannot be stalled by the solution”\nB - incorrect. Amazon Inspector running inside EC2 is meant for security scanning.\nC - Incorrect. This option talks about changing the process where there is already a process in place - “Development team has an agile continuous integration”\nD - Correct. The best option.\nE - Correct. Little clumsy but workable solution.","upvote_count":"4","poster":"Neive","comment_id":"171822","timestamp":"1634329860.0"},{"content":"A and D is correct, \"compliance enforced\" & \"least impact for development process, not least impact to production\". Well in real world, E will be a better choice than D in term of shutdown an instance instead of terminate.","comment_id":"148907","upvote_count":"1","poster":"fullaws","timestamp":"1634294940.0"},{"upvote_count":"1","content":"AD for sure","poster":"NikkyDicky","comment_id":"134184","timestamp":"1634129040.0"},{"content":"D,E\nA: I believe all AMIs must be custom to have tags assigned. So, A option assumes that AWS default AMIs can't be utilized since they don't have tags.","timestamp":"1633940160.0","poster":"demon42","upvote_count":"2","comment_id":"114137"},{"upvote_count":"1","content":"d is not correct, you cannot chain config with lambda directly .Need cloudwatch events in between.","comment_id":"104391","comments":[{"upvote_count":"1","poster":"pgarg","comment_id":"104394","timestamp":"1633914420.0","content":"sorry , d is correct. Found this :- A Lambda function is custom code that you upload to AWS Lambda, and it is invoked by events that are published to it by an event source. If the Lambda function is associated with a Config rule, AWS Config invokes it when the rule's trigger occurs. The Lambda function then evaluates the configuration information that is sent by AWS Config, and it returns the evaluation results."}],"timestamp":"1633839900.0","poster":"pgarg"},{"comments":[{"content":"setting up an IAM Policy will totally restrict the development team from launching EC2 instances with unapproved AMIs which could impact the CI/CD process","comment_id":"103077","timestamp":"1633838700.0","poster":"rrhaber","upvote_count":"1"}],"comment_id":"102155","content":"The answer is A,D","upvote_count":"2","timestamp":"1633812840.0","poster":"Ibranthovic"},{"poster":"meenu2225","timestamp":"1633744080.0","upvote_count":"1","content":"Its A,D. There is a specific rule in AWS config which addresses this AMI related alert.","comment_id":"101977"},{"timestamp":"1633723500.0","comment_id":"100876","content":"DE \nD: A config rule will have 'approved-amis-by-id' to evaluate compliance without interrupting the CICD process.\nE: A scheduled lambda function will scan through instances, it's an automated process i.e. no manual interruption :)","upvote_count":"4","poster":"rrhaber"},{"upvote_count":"2","timestamp":"1633553940.0","content":"Finally once you put A in place why would you need D?","comment_id":"94895","poster":"Merlin1"},{"comment_id":"94893","poster":"Merlin1","upvote_count":"1","content":"Except the lambda function is scheduled....My bad, the scheduled part is redundant but Its hard for me to sign off on anything that autoterminates an instance without understanding the impact first.","timestamp":"1633551180.0"},{"timestamp":"1633408980.0","comment_id":"94892","poster":"Merlin1","content":"I kind of agree with sb333. Mission accomplished with A! Now all that remains is dealing with any existing systems....so anything \"regular scheduled\" are unnecessary. And in the real world you just don't terminate running instances without putting your job at risk. So E is the only one that doesn't term the instance. It stops the instance and allows it it be started back up if it happens ti be running something critical and gives the team time to remediate the issue without a \"no going back\" solution","upvote_count":"1"},{"poster":"JAWS1600","content":"A D. For those who are thinking of C, please pay attention to the \"Deployment process that cannot be stalled by the solution\". This means our solution should NOT shutdown EC2 instance.","comments":[{"timestamp":"1635178380.0","comment_id":"279971","poster":"CanBe","upvote_count":"1","content":"C does not shutdown anything. Please read again. It goes by prior verification. Except for A and C, all other options shutdown the instances."}],"upvote_count":"1","timestamp":"1633390560.0","comment_id":"90925"},{"poster":"amtest123","upvote_count":"8","content":"D & E,\nYou can't interupt the CI/CD process so you have to first let the instance launch and then delete it","comment_id":"83505","timestamp":"1633386420.0"},{"content":"I like A and C as both of these have the \"least\" impact on the development process. BDE all either shutdown or terminate the instance. That is impactful I think.","poster":"jgtran","timestamp":"1633338960.0","upvote_count":"3","comment_id":"79945"},{"comment_id":"64291","poster":"ghostrider8001","comments":[{"content":"Agreed. C is not quite optimal but then shutting down/terminating instances without notifying Dev team would be quite abrupt and impactful.","poster":"Smart","timestamp":"1633031460.0","comment_id":"69932","upvote_count":"2","comments":[{"upvote_count":"1","comment_id":"69934","content":"Recognize that DevSecOps requires good communication among them. https://aws.amazon.com/devops/what-is-devops/","timestamp":"1633338240.0","poster":"Smart"}]}],"timestamp":"1632989940.0","content":"The question is looking for a solution \"deployment process that cannot be stalled by the solution and least impact on development process \" B D and E shuts down the instance. Left with only A and C","upvote_count":"2"},{"comment_id":"60322","timestamp":"1632891660.0","comments":[{"upvote_count":"1","timestamp":"1635065700.0","content":"Inspector is not meant for checking AMI -> https://aws.amazon.com/inspector/faqs/\n\n\"Amazon Inspector is an automated security assessment service that helps you test the network accessibility of your Amazon EC2 instances and the security state of your applications running on the instances.\"","comment_id":"260576","poster":"sarofi"}],"content":"I think D is ok with AWS Config but why not consider B as it seems Amazon inspector also allow to realize assessments on configs, with agents ...\nI would go for BD","poster":"virtual","upvote_count":"1"},{"content":"Should be A,D","timestamp":"1632783120.0","upvote_count":"2","comment_id":"45274","poster":"amog"},{"comment_id":"41217","timestamp":"1632625740.0","content":"my choice is A,D","upvote_count":"2","poster":"markpark"},{"comments":[{"comments":[{"content":"A , D/E\nDont think the point about AWS config directly triggering lambda is addressed in the link you have provided. Config i dont think can invoke a funct directly it needs to trigger an event which inturn can invoke a function. Since so many of you mention D im conflicted between D and E for my second answer.. happy to receive any suggestion","poster":"dman","comment_id":"99990","upvote_count":"1","timestamp":"1633582020.0"}],"upvote_count":"1","timestamp":"1632609000.0","content":"A & D are the correct answers. To address sb333's analysis for D, AWS Config can trigger a Lambda function (https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_develop-rules_getting-started.html) and a Lambda function can terminate an EC2 instance (https://aws.amazon.com/premiumsupport/knowledge-center/start-stop-lambda-cloudwatch/). E could also work, but AWS Config (i.e., answer D) would likely pick up the issue sooner and hence terminate the instance sooner making that a better solution.","comment_id":"36056","poster":"LunchTime"}],"upvote_count":"1","poster":"sb333","content":"I believe it's A & E. There are two things that need to be addressed. One is stopping new instances from being launched based on unapproved AMIs. The other is addressing instances that are already running that are running unapproved instance. The requirement is to also have the LEAST impact. \"A\" is correct for preventing new instances. \"E\" has the least impact on currently running instances, as they will be shut down and then Security can be notified and those can be addressed as to what to do with them. Nothing should be getting past answer \"A\" unless it's an instance that was already running prior to addressing unapproved AMIs. \"D\" is a tempting answer, but it terminates the instances. Also, there are no direct triggers in AWS Config for Lambda that I'm aware of. Remediation from AWS Config is done using AWS Systems Manager Automation, not Lambda. \"E\" is easily done using Lambda and can shut off the instances, which really is the least impactful until those EC2 instances can be resolved. Terminating existing instances causes the MOST impact.","comment_id":"35485","timestamp":"1632590220.0"},{"upvote_count":"3","comment_id":"28499","content":"I like A and C\nAlthough C, delays but it still keeps things secure.\nD is taking action post launch, so I am not very convinced with that approach to keep things secure.","poster":"9Ow30","timestamp":"1632574860.0"},{"comment_id":"24467","upvote_count":"6","content":"IMHO DE is the answer coz “process that cannot be stalled by the solution. Hence “A” is out.","poster":"Rockeye","timestamp":"1632516240.0"},{"comment_id":"14358","poster":"Moon","upvote_count":"8","content":"A & D.\nE: could work, but it will interrupt running systems, while \"D\" shut down a JUST launched instances.","timestamp":"1632308760.0"}],"unix_timestamp":1568275920,"topic":"1","exam_id":32,"isMC":true,"answer":"AD","question_images":[],"answers_community":["AD (57%)","DE (43%)"],"question_text":"A company prefers to limit running Amazon EC2 instances to those that were launched from AMIs pre-approved by the Information Security department. The\nDevelopment team has an agile continuous integration and deployment process that cannot be stalled by the solution.\nWhich method enforces the required controls with the LEAST impact on the development process? (Choose two.)","answer_description":"","timestamp":"2019-09-12 10:12:00","answer_images":[]},{"id":"gRyJojQc6ZuNPVwsrTnu","exam_id":32,"answer":"C","question_id":388,"choices":{"C":"Configure the Auto Scaling group to send an SNS notification of the launch of a new instance to the trusted key management service. Have the Key management service generate a signed certificate and send it directly to the newly launched instance.","B":"Embed a certificate into the Amazon Machine Image that is used by the Auto Scaling group. Have the launched instances generate a certificate signature request with the instance's assigned instance-id to the key management service for signature.","A":"Configure an IAM Role that grants access to an Amazon S3 object containing a signed certificate and configure the Auto Scaling group to launch instances with this role. Have the instances bootstrap get the certificate from Amazon S3 upon first boot.","D":"Configure the launched instances to generate a new certificate upon first boot. Have the Key management service poll the Auto Scaling group for associated instances and send new instances a certificate signature (hat contains the specific instance-id."},"unix_timestamp":1568233320,"url":"https://www.examtopics.com/discussions/amazon/view/5074-exam-aws-certified-solutions-architect-professional-topic-1/","answer_images":[],"timestamp":"2019-09-11 22:22:00","isMC":true,"question_images":[],"discussion":[{"comment_id":"10676","content":"I would go with C","comments":[{"poster":"Warrenn","timestamp":"1632297180.0","upvote_count":"4","comment_id":"17400","content":"agree C. A is incorrect and will not work"},{"comment_id":"18815","poster":"skywalker","timestamp":"1632317820.0","upvote_count":"4","content":"Agreed to be C"}],"timestamp":"1632068280.0","poster":"dpvnme","upvote_count":"8"},{"upvote_count":"6","comment_id":"22956","content":"C is option making some sense. However it should be ACM for certificates. KMS is for keys","timestamp":"1632951480.0","poster":"examacc"},{"timestamp":"1723734000.0","upvote_count":"1","content":"C. Configure the Auto Scaling group to send an SNS notification of the launch of a new instance to the trusted key management service. Have the Key management service generate a signed certificate and send it directly to the newly launched instance.","comment_id":"1266509","poster":"amministrazione"},{"content":"Selected Answer: D\nThe ec2 create the certificate that includes its instanceid put unsigned certificate into sns and the customer key management system poll sns for new request and sign the certificate.\n\nThis is not AWS kms but a customer key system so it could be written to poll requests","upvote_count":"1","poster":"a6a3d55","timestamp":"1716321240.0","comment_id":"1215209"},{"content":"Selected Answer: B\nThe second time I read this question, I believe the answer should B.\nA. instance-id is randomly generated by amazon internal algorithm, you can't get all possible instance-id in advance before you start an instance. So A is out.\nC. \"Have the Key management service generate a signed certificate\" is apparently wrong. Still don't know why people don't notice it. KMS only does encryptions/decryptions, for x 509 certificate , you need ACM to generate.\nD \" upon first boot\" I suspect whether you can get instance-id in booting time via script in user data and \" Have the Key management service poll the Auto Scaling group\" looks strange to me, as far as I know, it seems no poll function from KMS.","upvote_count":"1","poster":"Jesuisleon","timestamp":"1685390520.0","comments":[{"timestamp":"1686416280.0","content":"Ok, the third time I read this question, I change to C.\n\"an x 509 certificates must Designed by the customer's Key management service\" here customer's key management service is not KMS but a certificate management service so it can generate x509 certificate. This question is REALLY badly worded question, the one who devised this question mixing encryption/decryption key with certificates !","comment_id":"920195","upvote_count":"2","poster":"Jesuisleon"}],"comment_id":"909687"},{"upvote_count":"1","poster":"Jesuisleon","timestamp":"1684615980.0","content":"I don't understand how C can be correct. KMS can not generate x509 certificate ! it should be acm!","comment_id":"902764"},{"content":"Another question that Sucks! Not only configurations needed with Option C, I think it may need some programming work or even your Key management service cannot send it to EC2.","timestamp":"1673494920.0","upvote_count":"1","comment_id":"773098","poster":"jhonivy"},{"poster":"TigerInTheCloud","timestamp":"1671290700.0","upvote_count":"1","comment_id":"748153","content":"Selected Answer: C\nSame as cannottellname and TechX"},{"content":"Selected Answer: C\nAnswer: C\nExplanation:\nThe certificate must be signed by the customers key management service and this is the only option. Using S3 wont have it unique, embedding in AMI wont make it unique, Generating a new certificate by itself would defeat the requirement of getting it signed by customers key management service.\nA – Accessing from S3 was fine but how can the file be unique when every time autoscaling generates different instances and instance-id.. Thats not predictable\nB – Embedding a certificate in AMI cannot make the certificate unique.\nD – As the EC2 instances must generate unique X.509 certificate and this must be specific to the instance id. The EC2 instance can generate the certificate itself BUT it is clearly mentioned that the certificate must be signed by the customers key management service and not self signed.","upvote_count":"4","comment_id":"625039","poster":"TechX","timestamp":"1656567060.0"},{"poster":"cannottellname","comment_id":"538579","upvote_count":"4","content":"Option C\n\nJust to be clear, this question doesn't talk about AWS KMS but a customer's key management service (something like internal Certificate Authority). The requirement asks about unique certificate to be assigned for each EC2 instance. A, B doesn't guarantee a unique certificate. D doesn't make sense as CA won't poll for a CSR (Certificate Signining Request) and Sign it. This leaves with Option C which is possible with SNS alerting the customer's key management service or CA with details about instance-id and CA can generate a Certificate, Sign It and send back to the associated instance.\n\nSaw a comment that AWS SNS doesn't send notification to KMS which is correct as KMS service is not integrated with SNS but the question is about Customer's Key Management Service and not AWS KMS. AFAIK, AWS KMS doesn't deal with X509 SSL certificates. It only deals with Cryptographic Keys","timestamp":"1643797200.0"},{"content":"C is correct , A & B not handling to generate cert with instance-id.","comment_id":"418267","poster":"blackgamer","upvote_count":"1","timestamp":"1635978300.0"},{"poster":"01037","timestamp":"1635576960.0","content":"I understand why B can't be the answer, since the very first request violates the policy.\nBut how does the Key management service send signed certificate directly to the newly launched instance?","comments":[{"poster":"01037","timestamp":"1635890940.0","content":"Is customer's Key management service AWS KMS? Or is it inside any VPC? Nothing mentioned about it.\nIf it is, AWS KMS isn't inside any VPC, the B doesn't violates the policy.","comment_id":"345015","upvote_count":"1"}],"upvote_count":"2","comment_id":"345011"},{"content":"How does SNS send notifications to KMS?","timestamp":"1635388380.0","upvote_count":"4","comment_id":"255403","poster":"lin404"},{"upvote_count":"2","timestamp":"1635335820.0","content":"To meet the requirements, It has to be C.\nA. Not unique for each instance.\nB. First request is too insecure.\nD. New instance may not have unique certificate before the first request","poster":"newme","comment_id":"210328"},{"poster":"cpal012","comment_id":"194659","content":"Cant be 'C'. KMS doesnt generate certificates, ACM does.","timestamp":"1634960280.0","upvote_count":"4"},{"timestamp":"1634826420.0","poster":"srknbngl","comment_id":"190282","content":"C is correct","upvote_count":"1"},{"content":"Answer is B. When an EC2 instance that is started communicates with the Trusted Customer managed KMS service, it can use client ID and client secret to send the Certificate Signature request.","timestamp":"1634789040.0","upvote_count":"2","comment_id":"170443","poster":"Bulti"},{"comment_id":"151858","content":"you do not need Auto-scaling group for SNS.\nB","timestamp":"1634774400.0","poster":"pddddd","upvote_count":"3"},{"comment_id":"143850","content":"C is correct, A & B is not specific to instance, D CA does not have feature of poll CSR and signed. SNS cant work with KMS however, the \"trust key management\" here refer to third party.","upvote_count":"2","timestamp":"1634531640.0","poster":"fullaws"},{"timestamp":"1634436420.0","poster":"noisonnoiton","comment_id":"131047","content":"go with C","upvote_count":"2"},{"comment_id":"129133","poster":"sirmie_slim","upvote_count":"2","timestamp":"1634252520.0","content":"C has to be the answer"},{"content":"I believe it is B. \n\n1. as Musk mentions, it is the only option that creates a unique certificate. \n\n2. it does not violate the requirements since, although the first request is unsigned, it is not a request to a VPC resource. The first unsigned request is to the Customer's key management service which is external to AWS. The question states: \"every outbound connection from these instances to any other service within the customers Virtual Private Cloud must be authenticated\"","comments":[{"poster":"newme","upvote_count":"3","timestamp":"1635031380.0","content":"C also has unique certificate for each instance\n\nThough the question doesn't mention it, the first request if B is too unsure","comment_id":"210322"}],"comment_id":"86668","poster":"RogerRabbit","upvote_count":"2","timestamp":"1634012640.0"},{"timestamp":"1633953120.0","comment_id":"75183","poster":"Joeylee","content":"I would go with C","upvote_count":"4"},{"poster":"sergza","content":"I Do not like C. I has a lot of async interactions which could lead to server without certs and how their KMS is going to send signed cert directly back. \"B\" i guess is the better option you send CSR request during boot time","upvote_count":"2","comment_id":"63595","timestamp":"1633677960.0"},{"poster":"BillyC","upvote_count":"2","content":"C is Correct!","comment_id":"49668","timestamp":"1633410720.0"},{"timestamp":"1633105860.0","content":"I think it's B. It's the only one where the server creates a certificate request unique to a server.","comment_id":"41429","upvote_count":"1","poster":"Musk"},{"timestamp":"1633035540.0","upvote_count":"3","comment_id":"37727","poster":"amog","content":"Answer is C\nA - seems to be wrong because if a single certificate is stored in S3, it would have a single key pair and a single signature which would be duplicated across all instances. Does not fulfill the instance-id uniqueness\n\nB - similar issue of duplicate private key but more importantly, the first outbound request from the instance to the KMS (for certificate signing) will not be signed, which violates the policy\n\nD - A new certificate is generated upon each instance spin-up. Without sending out a signing request (with the signature of the newly generated key-pair), how can the KMS send a valid signed certificate."},{"poster":"lavy","timestamp":"1632975900.0","content":"C - http://jayendrapatil.com/tag/iam/","comment_id":"35206","upvote_count":"2"}],"answer_ET":"C","question_text":"An AWS customer is deploying an application mat is composed of an AutoScaling group of EC2 Instances.\nThe customers security policy requires that every outbound connection from these instances to any other service within the customers Virtual Private Cloud must be authenticated using a unique x 509 certificate that contains the specific instance-id.\nIn addition, an x 509 certificates must Designed by the customer's Key management service in order to be trusted for authentication.\nWhich of the following configurations will support these requirements?","topic":"1","answers_community":["C (71%)","14%","14%"],"answer_description":""},{"id":"IOZsud0g1Yn0E2gyIUsi","answer_description":"","unix_timestamp":1568279880,"answers_community":["BD (100%)"],"exam_id":32,"question_id":389,"isMC":true,"timestamp":"2019-09-12 11:18:00","choices":{"B":"Configure an Amazon CloudWatch Events rule that invokes an AWS Lambda function to secure the S3 bucket.","C":"Use the S3 bucket permissions for AWS Trusted Advisor and configure a CloudWatch event to notify by using Amazon SNS.","A":"Turn on object-level logging for Amazon S3. Turn on Amazon S3 event notifications to notify by using an Amazon SNS topic when a PutObject API call is made with a public-read permission.","E":"Schedule a recursive Lambda function to regularly change all object permissions inside the S3 bucket.","D":"Turn on object-level logging for Amazon S3. Configure a CloudWatch event to notify by using an SNS topic when a PutObject API call with public-read permission is detected in the AWS CloudTrail logs."},"discussion":[{"comment_id":"13331","upvote_count":"29","timestamp":"1632331440.0","poster":"donathon","content":"A: There is a possibility that the event may be missed using this method. Amazon S3 event notifications typically deliver events in seconds but can sometimes take a minute or longer. On very rare occasions, events might be lost. https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\nB: This actively remediate the public access. https://aws.amazon.com/blogs/security/how-to-detect-and-automatically-remediate-unintended-permissions-in-amazon-s3-object-acls-with-cloudwatch-events/\nC: This is possible but not complete. This Trusted Advisor check doesn't monitor for bucket policies that override bucket ACLs.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/secure-s3-resources/\nhttps://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/#security\nhttps://willhamill.com/2018/02/19/get-alerts-when-an-s3-bucket-is-made-public-in-your-aws-account\nD: https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/log-s3-data-events.html\nE: Not feasible.","comments":[{"timestamp":"1632338760.0","content":"Answer is BD BTW.","comment_id":"14051","upvote_count":"17","poster":"donathon"},{"comment_id":"14356","upvote_count":"6","timestamp":"1632478200.0","poster":"Moon","content":"Good resources... Support \"B & D\" too."},{"comment_id":"343739","timestamp":"1635718500.0","poster":"Waiweng","content":"B D it's good","upvote_count":"5"},{"upvote_count":"2","poster":"awsenthu","timestamp":"1633885980.0","content":"cloudtrail will take 15min to deliver the logs, my take is A and B\nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html","comment_id":"110050"},{"poster":"wassb","timestamp":"1665216480.0","content":"Trusted Advisor can check if the S3 bucket is public not objects insided","comment_id":"689124","upvote_count":"1"}]},{"upvote_count":"14","comment_id":"10753","content":"b,d\nhttps://aws.amazon.com/blogs/security/how-to-detect-and-automatically-remediate-unintended-permissions-in-amazon-s3-object-acls-with-cloudwatch-events/","timestamp":"1632308760.0","poster":"awsec2","comments":[{"poster":"dpvnme","upvote_count":"4","timestamp":"1632315840.0","content":"yes, A is not possible with S3 event notification","comment_id":"11467"}]},{"comment_id":"925711","upvote_count":"1","poster":"SkyZeroZx","timestamp":"1686970140.0","content":"Selected Answer: BD\nB D it's good"},{"timestamp":"1643489100.0","content":"A is not correct, the permission can't be detected by event notification. Event format is here:https://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-content-structure.html","comment_id":"535706","upvote_count":"7","poster":"niu_tim"},{"content":"I will go with BD","poster":"AzureDP900","upvote_count":"1","timestamp":"1638632040.0","comment_id":"493769"},{"comment_id":"492868","timestamp":"1638492000.0","poster":"vbal","content":"demand of this 'How can the existence of a public S3 item be recognized' mean D & not C","upvote_count":"1"},{"poster":"vbal","upvote_count":"1","content":"B&C ; https://aws.amazon.com/about-aws/whats-new/2018/02/aws-trusted-advisors-s3-bucket-permissions-check-is-now-free/","comment_id":"492864","timestamp":"1638491520.0"},{"comment_id":"450332","upvote_count":"1","poster":"andylogan","content":"It's B D","timestamp":"1636049520.0"},{"comment_id":"382170","upvote_count":"1","timestamp":"1635925740.0","poster":"amithbti416","content":"B and D\nAmazon CloudWatch Events to detect PutObject and PutObjectAcl API calls in near real time and helps ensure that the objects remain private by making automatic PutObjectAcl calls, when necessary."},{"timestamp":"1635906180.0","poster":"WhyIronMan","comment_id":"351895","content":"I'll go with B,D","upvote_count":"1"},{"upvote_count":"2","timestamp":"1635507000.0","poster":"ExtHo","content":"Any Thought on A,D\nmany peoples referring as \"On very rare occasions, events might be lost\" to rule out A on \nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html \n\nI don't found event might be lost \nImportant\nAmazon S3 event notifications are designed to be delivered at least once. Typically, event notifications are delivered in seconds but can sometimes take a minute or longer.","comment_id":"322562"},{"upvote_count":"1","comment_id":"291219","timestamp":"1635194520.0","poster":"AJBA","content":"Definitely BD"},{"content":"will go with B&D","upvote_count":"2","timestamp":"1635115800.0","poster":"Kian1","comment_id":"289503"},{"poster":"Ebi","content":"BD are the right ones","upvote_count":"3","comment_id":"283176","timestamp":"1634921280.0"},{"comment_id":"263530","upvote_count":"1","timestamp":"1634875200.0","poster":"sanjaym","content":"I'll go with BD"},{"content":"Correct is BD.","timestamp":"1634785800.0","poster":"T14102020","upvote_count":"2","comment_id":"242015"},{"content":"I'll go with B,D","timestamp":"1634763780.0","comment_id":"229399","poster":"jackdryan","upvote_count":"3"},{"timestamp":"1634712960.0","upvote_count":"2","comment_id":"229258","poster":"YouYouYou","content":"https://aws.amazon.com/blogs/security/how-to-detect-and-automatically-remediate-unintended-permissions-in-amazon-s3-object-acls-with-cloudwatch-events/\n\nanswer is B&D"},{"comments":[{"timestamp":"1634878380.0","poster":"shammous","upvote_count":"1","comment_id":"279916","content":"A doesn't fail to detect and and send an alert. It just might take some time to do it. That's why D is prefered."}],"poster":"Bulti","upvote_count":"3","content":"B and D are the correct answers. While D detects the condition using cloud watch events and SNS topic , B actually performs the remediation. Others are incorrect because they fall to do either of the 2 things required in the question.","timestamp":"1634706840.0","comment_id":"228060"},{"upvote_count":"3","timestamp":"1634706360.0","comment_id":"183623","poster":"exergeng","content":"cause A uses a public read detection, which is late than put detection , is not really good.\nchoose B,D"},{"poster":"Ganfeng","content":"B D my pick","timestamp":"1634624940.0","upvote_count":"1","comment_id":"179222"},{"comment_id":"148910","content":"B and D is correct, s3 event notification had the possibility of event missed (not reliable).","upvote_count":"2","timestamp":"1634614080.0","poster":"fullaws"},{"poster":"learner4ever","upvote_count":"1","content":"B and D","timestamp":"1634299440.0","comment_id":"145304"},{"poster":"NikkyDicky","content":"BD for sure","timestamp":"1634199840.0","comment_id":"134194","upvote_count":"1"},{"upvote_count":"2","poster":"sosolotus","timestamp":"1634193480.0","content":"BC \nhttps://aws.amazon.com/about-aws/whats-new/2018/02/aws-trusted-advisors-s3-bucket-permissions-check-is-now-free/","comment_id":"115982"},{"comment_id":"102158","timestamp":"1633252980.0","comments":[{"upvote_count":"2","content":"Is it correct that it wil take 15 odd minutes to deliver the logs, but still A,D are the correct option. With S3 event notification you can create the event for Put (s3:ObjectCreated:Put) but you cannot be notified based on public-read permission. For this you will have to use Cloudtrail.","comments":[{"comment_id":"109905","timestamp":"1633832100.0","poster":"meenu2225","upvote_count":"1","content":"Typo, I man B and D are correct."}],"comment_id":"105480","poster":"meenu2225","timestamp":"1633808760.0"}],"content":"I will go with A and B\nD is counting on Cloudtrail logs, which could take up to 15 min to write the log","poster":"Ibranthovic","upvote_count":"1"},{"upvote_count":"4","content":"I think there is some error/typo in options. I believe Option D should be 'PutObjectACL API' otherwise it won't work. It focuses only on object creation process - what if someone changes the objectACL after creation? We also have to ignore overriding bucket policy's public access. Otherwise, none of the combinations would work. \n\nConsider this: The question is asking for: \n1. Immediate notification if any object is public. \n2. Automatic remediation (in future)\n3. Overall, per scenario, no object should be public. \nThings that will allow object-level public access: ObjectACL and Overriding Bucket Policy.","comments":[{"timestamp":"1633021920.0","content":"Correction: PutObjectACL in addtion to PutObject API.","poster":"Smart","upvote_count":"1","comment_id":"75466"}],"comment_id":"75465","poster":"Smart","timestamp":"1632951300.0"},{"upvote_count":"1","poster":"ghostrider8001","timestamp":"1632771540.0","comment_id":"64295","content":"@sparkf1 B takes care of the remedy"},{"timestamp":"1632755400.0","poster":"amog","content":"Should be B,D","upvote_count":"2","comment_id":"45275"},{"timestamp":"1632655800.0","poster":"sparkf1","comment_id":"36956","content":"Answer E for \"automatically remediated in the future\"","upvote_count":"1"},{"timestamp":"1632559620.0","content":"continued...\n\nB. Will work\nThis would need to be done in conjunction with answer “A” or “D”.\n\nC. Will work but AWS Trusted adviser is NOT event driven, i.e., it must be scheduled. This means running it on some frequency, the quickest refresh is every 5 minutes. This is not as efficient as using an event driven mechanism, such at answer “B”. It also leaves open the possibility of a security even occurring between executions of AWS Trusted Adviser.\n\nD. Will work and does not have the drawback of “A”\nSee https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/log-s3-data-events.html.\n\nE. Will work, but not efficient.\nBetween schedule executions there is a risk of having a security event occur.","comment_id":"36080","poster":"LunchTime","upvote_count":"5"},{"upvote_count":"6","timestamp":"1632484320.0","content":"Good analysis and resources provided by donathon. I thought I would add a bit more elaboration though\nB & D are the correct answers.\nFirstly, the situation is discussed in this article: https://aws.amazon.com/premiumsupport/knowledge-center/secure-s3-resources/\nUnfortunately, the article does not line up perfectly with the answer options given. Regardless…\nA. Will work, but there is an issue…\nAs per https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\n“Amazon S3 event notifications typically deliver events in seconds but can sometimes take a minute or longer. On very rare occasions, events might be lost.” \nConsequently, security events might be missed using this approach and there may be a window of several minutes or longer until the issue is identified.","comment_id":"36079","poster":"LunchTime"}],"question_images":[],"answer_images":[],"answer":"BD","url":"https://www.examtopics.com/discussions/amazon/view/5105-exam-aws-certified-solutions-architect-professional-topic-1/","topic":"1","question_text":"A Company had a security event whereby an Amazon S3 bucket with sensitive information was made public. Company policy is to never have public S3 objects, and the Compliance team must be informed immediately when any public objects are identified.\nHow can the presence of a public S3 object be detected, set to trigger alarm notifications, and automatically remediated in the future? (Choose two.)","answer_ET":"BD"},{"id":"nLcHlT8I4pdrjUsYJVy8","answer_images":[],"discussion":[{"comments":[{"poster":"MrCarter","content":"that is incorrect","comment_id":"394565","upvote_count":"4","timestamp":"1635988920.0"},{"poster":"Frank1","comment_id":"21718","upvote_count":"21","content":"Need to keep host header as cloudfront and elb is using the SAME ssl certificate.\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront-to-custom-origin.html search \"host\"","timestamp":"1632757740.0","comments":[{"poster":"uopspop","comment_id":"22169","upvote_count":"6","content":"Thanks a lot. This explains why A is incorrect. \nI support D to be the answer, then.","timestamp":"1632831960.0"},{"comment_id":"75513","poster":"Smart","upvote_count":"3","content":"^Thanks - https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/http-502-bad-gateway.html#ssl-negotitation-failure","timestamp":"1633005420.0"},{"content":"? This is why A is correct. \nThe article mentioned explicitly states that ONLY if you pass through the Host Header that the certificate must match the domain in the host header. Thus, if you *exclude* the host header, CloudFront does not care about the name in the origin certificate. So don't include the host header.\n\n\"In addition, if you configured CloudFront to forward the Host header to your origin, the origin must respond with a certificate matching the domain in the Host header.\"\n(therefore just exclude the host header)","comments":[{"content":"Sorry, upvoted by mistake. \nYou were wrong about this. If you remove the original HOST header, Cloudfront will add it back with the hostname of the origin. Since the HOST header no longer matches with the certificate, SSL handshake will fail at ALB. So, keeping the original HOST header is a must.","upvote_count":"10","comment_id":"171348","poster":"b3llman","timestamp":"1633762200.0"}],"upvote_count":"4","comment_id":"131226","poster":"inf","timestamp":"1633188180.0"}]}],"upvote_count":"16","timestamp":"1632514200.0","poster":"HazemYousry","comment_id":"12532","content":"A - Only session cookie and the Authorization headers to be kept and other headers can be removed"},{"comment_id":"44331","upvote_count":"10","comments":[{"comment_id":"394566","timestamp":"1636050000.0","upvote_count":"2","poster":"MrCarter","content":"nope, D is the correct answer"}],"poster":"dumma","content":"A is correct https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html the key requirements are to increase cache hit ratio, and not\nbreaking SSL between CloudFront and the ALB. Breaking up the origin to static and\ndynamic would help. Application needs session and authorization headers for dynamic\ncontent but can be skipped for static content and neither need the user agent or host.","timestamp":"1632972660.0"},{"timestamp":"1687374720.0","upvote_count":"1","content":"Selected Answer: D\nD - Host to not be removed","comment_id":"929840","poster":"SkyZeroZx"},{"poster":"DarthYoda","upvote_count":"2","content":"Selected Answer: D\nD seems to be right","timestamp":"1668283620.0","comment_id":"716905"},{"poster":"robsonchirara","upvote_count":"1","content":"D - Removing the host header will break the TLS handshake. Static content is probably not being served by the ALB, maybe s3. Therefore no need to send many headers as this is affecting the cache hit ratio.","timestamp":"1667661660.0","comment_id":"711846"},{"timestamp":"1666938960.0","comment_id":"706203","content":"Selected Answer: D\nD - Host to not be removed","upvote_count":"2","poster":"dmscountera"},{"upvote_count":"2","content":"D IS SURE 100%","poster":"Sizuma","timestamp":"1661956800.0","comment_id":"655195"},{"comment_id":"632990","timestamp":"1658144160.0","poster":"Student1950","upvote_count":"7","content":"I vote for D. Explanation:\nExisting configuration is workings with Host Header forwarding - means both CloudFront and ALB are configured with same SSL certificates (same host name definition in SSL cert).\nIf you remove host header, CloudFront will add Custom Origin host (hostname defined in ALB) to the host header (host potion of URL). When this request reaches ALB, the request will be failed at ALB as SSL hostname defined in ALB SSL certificate will not match with host portion of URL hence Host Header is required when we have same SSL certificate deployed on CloudFront and ALB. This works if ALB has its own SSL certificate matching its own host name definition which means CloudFront, and ALB have different SSL certificates."},{"content":"D looks right","upvote_count":"2","poster":"jj22222","timestamp":"1642893840.0","comment_id":"530124"},{"upvote_count":"3","comment_id":"493770","timestamp":"1638632520.0","content":"D is right\nRemove the User-Agent HTTP header from the whitelist headers section on both of the cache behaviors. There is no need to remove Host header.","poster":"AzureDP900"},{"poster":"acloudguru","upvote_count":"4","content":"Selected Answer: D\nD, seperate static and dynamic web to increase cache hit","comment_id":"490333","timestamp":"1638237360.0"},{"comment_id":"450604","timestamp":"1636256340.0","upvote_count":"1","content":"It's D","poster":"andylogan"},{"content":"Going for D","poster":"Kopa","upvote_count":"1","timestamp":"1636234620.0","comment_id":"446588"},{"timestamp":"1636211100.0","comment_id":"406616","upvote_count":"1","content":"D Correct","poster":"Akhil254"},{"content":"Correct answer is D\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html\nCreate separate cache behaviors for static and dynamic content, and configure CloudFront to forward cookies to your origin only for dynamic content.\nHost header is required for both cache behaviors not to break the SSL connection with the ALB.","upvote_count":"9","timestamp":"1636140120.0","comment_id":"403470","poster":"student2020","comments":[{"content":"User-agent header results in too much variation in each request and therefore lots of cache misses. Removing this header will improve the cache hit ratio.\nTry to avoid caching based on request headers that have large numbers of unique values.\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html","upvote_count":"4","poster":"student2020","timestamp":"1636140300.0","comment_id":"403472"}]},{"upvote_count":"9","poster":"Radhaghosh","timestamp":"1635936720.0","comment_id":"362377","content":"Correct Answer - D\nSince it's distribution both Static & Dynamic content. You should have two cache behaviors. So Option B & C is eliminated. Now between A & D, Host HTTP headers is required, and you can't remove. So only Valid Option is D\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/understanding-the-cache-key.html"},{"timestamp":"1635924300.0","content":"going with D, verified in Neal Davis sample questions","poster":"KnightVictor","upvote_count":"4","comment_id":"356471"},{"poster":"vivektiwari","content":"Correct Answer: D.\nRemoving the host header will result in failed flow between CloudFront and ALB, because they have same certificate.","upvote_count":"2","timestamp":"1635799560.0","comment_id":"356184"},{"timestamp":"1635739200.0","comment_id":"354811","upvote_count":"2","poster":"WhyIronMan","content":"I'll go with D"},{"upvote_count":"1","poster":"blackgamer","timestamp":"1635606480.0","content":"D is making more sense than B.","comment_id":"344422"},{"timestamp":"1635337020.0","content":"I'll support D","comment_id":"343743","poster":"Waiweng","upvote_count":"2"},{"content":"B is my answer","upvote_count":"1","timestamp":"1635209400.0","comment_id":"327760","poster":"aishvary123"},{"poster":"Kian1","upvote_count":"3","comment_id":"289512","timestamp":"1635094020.0","content":"will go with D"},{"comment_id":"284879","timestamp":"1635027300.0","content":"I ll go with D. If you remove the host header, HTTPS will be impacted. So A is incorrect.","upvote_count":"1","poster":"bnagaraja9099"},{"content":"My choice is D","comment_id":"283185","poster":"Ebi","upvote_count":"1","timestamp":"1634951820.0"},{"poster":"consultsk","upvote_count":"1","timestamp":"1634440380.0","content":"Only one option to be chosen. I prefer to go with 'A'. There are a few links already provided to support this answer.","comment_id":"252214"},{"upvote_count":"1","timestamp":"1634411520.0","poster":"newme","content":"AD.\n\nauthorization and session tracking are needed, so B&C are both wrong.\nThen only A&D left.\nBut what I don't understand is, how A or D improves cache hit?","comment_id":"243585"},{"timestamp":"1634185860.0","poster":"T14102020","comment_id":"242018","content":"D is correct. Remove cookies and user-agent","upvote_count":"1"},{"comment_id":"229401","content":"I'll go with D","upvote_count":"3","timestamp":"1634180340.0","poster":"jackdryan"},{"content":"D is correct as hostname is required to perform TLS with ALB. Also host name header can be revised to cache content when the origin is other than S3 as per cf developers guide.","timestamp":"1633987860.0","upvote_count":"3","comment_id":"228460","comments":[{"comment_id":"270657","upvote_count":"2","poster":"Bulti","timestamp":"1634946480.0","content":"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/understanding-the-cache-key.html"}],"poster":"Bulti"},{"timestamp":"1633921680.0","poster":"exergeng","upvote_count":"1","content":"If host http header is removed, SNI of ALB will not get the hostname needed.Thus,TLS hand shake error may occur.\nrefer https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-https-connection-fails/?nc1=h_ls\nchoose D","comment_id":"184130"},{"content":"D is correct","timestamp":"1633893960.0","upvote_count":"1","comment_id":"179223","poster":"Ganfeng"},{"timestamp":"1633628580.0","content":"is User-agent header instead of http header","upvote_count":"1","comment_id":"148927","poster":"fullaws"},{"upvote_count":"1","content":"D is correct, origin other than s3 can exclude host header. static page exclude authenticaiton http header, and session. Default exclude HTTP header","timestamp":"1633628520.0","comment_id":"148926","poster":"fullaws"},{"timestamp":"1633596300.0","poster":"learner4ever","content":"D is the only answer","upvote_count":"1","comment_id":"145305"},{"upvote_count":"1","poster":"NikkyDicky","comment_id":"134200","content":"D more likely","timestamp":"1633445640.0"},{"comment_id":"99787","poster":"ripntear","upvote_count":"2","content":"A: Host headers are only for S3 not ALB - so host header needs to be removed.","comments":[{"poster":"dayody","comment_id":"113057","upvote_count":"1","content":"so the answer is D","timestamp":"1633160940.0"}],"timestamp":"1633115580.0"},{"content":"Not sure. Could be D","upvote_count":"2","comment_id":"45278","poster":"amog","timestamp":"1633005360.0"},{"poster":"dojo","timestamp":"1632932340.0","upvote_count":"2","comment_id":"31123","content":"D is the answer"},{"comment_id":"19077","comments":[{"upvote_count":"1","poster":"9Ow30","comment_id":"30357","content":"Answer is D","timestamp":"1632928740.0"}],"upvote_count":"3","poster":"awsgcpazure","timestamp":"1632757560.0","content":"This was a single selection!!! Good Luck ^^"},{"content":"AD\nA - Breaking up the origin to static and dynamic would help. The application needs session and authorization headers, and neither need the user agent or host.\nB - The application requires the authorization header\nC - The application requires the session header\nD - Almost the same as A except the host header is kept, meaning that it will affect the cache hit.","comment_id":"14053","poster":"donathon","upvote_count":"4","timestamp":"1632638700.0"},{"upvote_count":"1","poster":"awspro","timestamp":"1632631620.0","content":"What's the real?\nCould you explain for me?","comment_id":"13530"},{"content":"Not sure why it's double choice. But if it's single, I'd go with D","poster":"dpvnme","comment_id":"11499","timestamp":"1632418320.0","upvote_count":"2"},{"content":"why bd","upvote_count":"2","timestamp":"1632242820.0","poster":"awsec2","comment_id":"10754"}],"answer_description":"","question_id":390,"question_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/5106-exam-aws-certified-solutions-architect-professional-topic-1/","timestamp":"2019-09-12 11:25:00","unix_timestamp":1568280300,"exam_id":32,"choices":{"B":"Remove the User-Agent and Authorization HTTP headers from the whitelist headers section of the cache behavior. Then update the cache behavior to use presigned cookies for authorization.","D":"Create two cache behaviors for static and dynamic content. Remove the User-Agent HTTP header from the whitelist headers section on both of the cache behaviors. Remove the session cookie from the whitelist cookies section and the Authorization HTTP header from the whitelist headers section for cache behavior configured for static content.","A":"Create two cache behaviors for static and dynamic content. Remove the User-Agent and Host HTTP headers from the whitelist headers section on both of the cache behaviors. Remove the session cookie from the whitelist cookies section and the Authorization HTTP header from the whitelist headers section for cache behavior configured for static content.","C":"Remove the Host HTTP header from the whitelist headers section and remove the session cookie from the whitelist cookies section for the default cache behavior. Enable automatic object compression and use Lambda@Edge viewer request events for user authorization."},"answer":"D","answer_ET":"D","answers_community":["D (100%)"],"topic":"1","question_text":"A company is using an Amazon CloudFront distribution to distribute both static and dynamic content from a web application running behind an Application Load\nBalancer. The web application requires user authorization and session tracking for dynamic content. The CloudFront distribution has a single cache behavior configured to forward the Authorization, Host, and User-Agent HTTP whitelist headers and a session cookie to the origin. All other cache behavior settings are set to their default value.\nA valid ACM certificate is applied to the CloudFront distribution with a matching CNAME in the distribution settings. The ACM certificate is also applied to the\nHTTPS listener for the Application Load Balancer. The CloudFront origin protocol policy is set to HTTPS only. Analysis of the cache statistics report shows that the miss rate for this distribution is very high.\nWhat can the Solutions Architect do to improve the cache hit rate for this distribution without causing the SSL/TLS handshake between CloudFront and the\nApplication Load Balancer to fail?"}],"exam":{"isMCOnly":false,"name":"AWS Certified Solutions Architect - Professional","isImplemented":true,"numberOfQuestions":1019,"lastUpdated":"11 Apr 2025","id":32,"provider":"Amazon","isBeta":false},"currentPage":78},"__N_SSP":true}