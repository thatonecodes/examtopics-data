{"pageProps":{"questions":[{"id":"W2CUhsUKZOeahhdDA8QU","question_images":[],"isMC":true,"choices":{"C":"Use AWS IoT to send the data from devices to Amazon Kinesis Data Streams with the IoT rules engine. Use one Kinesis Data Firehose stream attached to a Kinesis stream to batch and stream the data partitioned by date. Use another Kinesis Firehose stream attached to the same Kinesis stream to filter out the required fields to ingest into Elasticsearch for real-time analytics.","B":"Create a Direct Connect connection between AWS and the on-premises data center and copy the data to Amazon S3 using S3 Acceleration. Use Amazon Athena to query the data.","D":"Use AWS IoT to send the data from devices to Amazon Kinesis Data Streams with the IoT rules engine. Use one Kinesis Data Firehose stream attached to a Kinesis stream to stream the data into an Amazon S3 bucket partitioned by date. Attach an AWS Lambda function with the same Kinesis stream to filter out the required fields for ingestion into Amazon DynamoDB for real-time analytics.","E":"use multiple AWS Snowball Edge devices to transfer data to Amazon S3, and use Amazon Athena to query the data.","A":"use AWS IoT to send data from devices to an Amazon SQS queue, create a set of workers in an Auto Scaling group and read records in batch from the queue to process and save the data. Fan out to an Amazon SNS queue attached with an AWS Lambda function to filter the request dataset and save it to Amazon Elasticsearch Service for real-time analytics."},"exam_id":17,"answer":"AD","answer_ET":"AD","timestamp":"2019-08-26 06:59:00","unix_timestamp":1566795540,"discussion":[{"content":"A is too slow.\nSo between B and E for the transportation problem.\nQuick math, 10G bit ~ 1G byte/sec ~ 73 TB/day ~ 2 PB/month\nYou should remember this because you will need this math in many other exams too.\nSo E.\n\nBetween C and D for the storage and analysis.\nSee unstructured text => Think DynamoDB\n\nConclusion D, E","comments":[{"upvote_count":"1","poster":"Reza215r","timestamp":"1635808860.0","comment_id":"107291","content":"you don't really need the math to choose between B or E for two obvious reasons. Direct connect would take around two months to be established. 2. The question states \"one time upload\". You don't need a Direct connect for a one off use."},{"comment_id":"135193","content":"DynamoDB is not for real-time analytics....D is wrong. It must be C,E.","timestamp":"1636046460.0","upvote_count":"1","poster":"hdesai"}],"comment_id":"47645","poster":"AdamSmith","timestamp":"1633897380.0","upvote_count":"8"},{"timestamp":"1632507900.0","upvote_count":"7","content":"It's D and E ... SQS can not hold more than 256B for a value.. but the question says \"each record is 10KB\", so SQS is out, so A is out. To copy the 10PB of data, you need multiple Snowball Edge, so E is in. Direct Connect (B) and stream data partitioned by date (C) are also not valid. So, the answer should be D and E.","comment_id":"12683","comments":[{"poster":"VB","upvote_count":"3","content":"sorry.. SQS limit is 256 KB, and not Bytes.","comment_id":"13202","timestamp":"1632612180.0"},{"upvote_count":"4","comment_id":"21825","comments":[{"poster":"sriansri","timestamp":"1632877620.0","comment_id":"33411","upvote_count":"1","content":"Option D says partition bucket by date. Only 100 buckets allowed for an AWS Account. So the answer should be C & E"}],"content":"agreed, for realtime on new data need \"stream\" not \"batch\" (D), for existing data need to use Snowball (E).","poster":"chaudh","timestamp":"1632642780.0"}],"poster":"VB"},{"poster":"DerekKey","upvote_count":"1","comment_id":"338152","content":"A is wrong - set of workers in autoscaling group? Exactly what will happen there? (\"read records in batch from the queue\")\nB is wrong - will take more than 1 month to get established and working \nC is correct - with Kinesis Firehose you can transform data (using Lambda)\nD is wrong - DynamoDB is not used for Real time Analytics which typically involve large range scans and complex operations such as grouping and aggregation (\"selective data is loaded while generating the long-term trend with ANSI SQL queries through JDBC for visualization\")\nE is correct","timestamp":"1636138920.0"},{"content":"D\nE\nhttps://aws.amazon.com/fr/getting-started/hands-on/build-serverless-real-time-data-processing-app-lambda-kinesis-s3-dynamodb-cognito-athena/\nLook at the architecture ... inspiring","upvote_count":"1","comment_id":"110226","timestamp":"1636044600.0","poster":"jkoffee"},{"content":"C look good if store in s3, as data in firehose expire in 24hours.\nfor D, DynamoDB for real-time analytics, weird.","timestamp":"1634927640.0","poster":"examinfo","comment_id":"100380","upvote_count":"2","comments":[{"content":"C states \"Use Firehose to ingest into Elasticsearch for real-time analytics\" -> Firehose can only do near-realtime and thus C must be false.","upvote_count":"2","comment_id":"101794","timestamp":"1635681840.0","poster":"Corram"}]},{"comments":[{"upvote_count":"1","timestamp":"1634826300.0","poster":"Corram","content":"Agreed that DynamoDB is not suited for real-time analytics by itself. However, you may build real-time dashboards based on the content of a DynamoDB table, see for example here: https://aws.amazon.com/blogs/startups/building-dynamic-dashboards-using-lambda-and-dynamodb-streams-part-1/","comment_id":"99582"}],"upvote_count":"1","poster":"agm84","comment_id":"96767","timestamp":"1634603640.0","content":"E for transferring data.\nBoth C and D have inconsistency regarding real-time analytics. FH has 60 seconds delay and DynamoDB is not suited for real-time analytics. So in this scenario i prefer dealing with FH not being truly real-time than having real-time analytics over DynamoDB table. Then C and E."},{"timestamp":"1634570580.0","upvote_count":"5","poster":"Bulti","comment_id":"78828","content":"D is the answer for real-time analytics;None of the other options offer real-time analytics. ElasticSearch is for near real-time analytics.\nE is the other answer for transfering PB data."},{"upvote_count":"1","poster":"jiedee","timestamp":"1634536260.0","comment_id":"72275","content":"DE\nA-sqs is typically for a decouple scenario, not for big data\nB- far from real-time\nC-firehose has 60 seconds latency, again not real-time"},{"poster":"san2020","content":"my selection DE","timestamp":"1634313360.0","comment_id":"52407","upvote_count":"1"},{"poster":"sriansri","comment_id":"33409","comments":[{"content":"even with 1 bucket you can still do the partitioning - just with the object keys","comment_id":"33846","timestamp":"1632961020.0","poster":"hailiang","upvote_count":"4"},{"comment_id":"38638","content":"Kinesis stream can't filter out the required fields","timestamp":"1632977640.0","poster":"ME2000","upvote_count":"1"}],"timestamp":"1632760140.0","content":"Option D says partition bucket by date. Only 100 buckets allowed for an AWS Account. So the answer should be C & E","upvote_count":"2"},{"comment_id":"11218","content":"Should be D, E. E for large data transfer. D for real-time dashboard as C is near real-time","upvote_count":"6","poster":"apertus","comments":[{"comment_id":"45389","content":"\"but all fields must be available for long-term generation\"\nWhat's why C, because Dynamo DB has some limitations for this kind of analysis","timestamp":"1633613580.0","upvote_count":"1","poster":"practicioner"}],"timestamp":"1632478380.0"},{"timestamp":"1632361560.0","upvote_count":"2","comment_id":"8641","content":"Cost effective and real time, lamba and s3 are good candidates A,D?","comments":[{"comment_id":"9700","upvote_count":"4","timestamp":"1632454140.0","content":"Im going to go with C & E. I selected A,D last time. C as FH allows batching and custom transformations to obtain the date data to connect to the near real time dashboard. Also C uses multiple FH streams. E as its a PB data transport solution so you can load data from the SAN into S3 within a couple of days and will allow queries from Athena.","poster":"mattyb123"}],"poster":"jlpl"},{"timestamp":"1632092100.0","poster":"mattyb123","content":"Thoughts on B and D? B due to Athena is mentioned with ANSI SQL queries through JDBC.","comment_id":"8297","upvote_count":"1","comments":[{"comment_id":"8298","content":"Apologies B & C. has visualization aspect from Kibana in elastic search.","poster":"mattyb123","upvote_count":"2","timestamp":"1632178200.0"}]}],"answer_images":[],"topic":"2","answers_community":[],"question_id":76,"question_text":"An organization has 10,000 devices that generate 100 GB of telemetry data per day, with each record size around 10 KB. Each record has 100 fields, and one field consists of unstructured log data with a \"String\" data type in the English language. Some fields are required for the real-time dashboard, but all fields must be available for long-term generation.\nThe organization also has 10 PB of previously cleaned and structured data, partitioned by Date, in a SAN that must be migrated to AWS within one month.\nCurrently, the organization does not have any real-time capabilities in their solution. Because of storage limitations in the on-premises data warehouse, selective data is loaded while generating the long-term trend with ANSI SQL queries through JDBC for visualization. In addition to the one-time data loading, the organization needs a cost-effective and real-time solution.\nHow can these requirements be met? (Choose two.)","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/4084-exam-aws-certified-big-data-specialty-topic-2-question-19/"},{"id":"72XI6IIThNu559A2bthE","answer_images":[],"question_id":77,"question_text":"How should an Administrator BEST architect a large multi-layer Long Short-Term Memory (LSTM) recurrent neural network (RNN) running with MXNET on\nAmazon EC2? (Choose two.)","choices":{"A":"Use data parallelism to partition the workload over multiple devices and balance the workload within the GPUs.","B":"Use compute-optimized EC2 instances with an attached elastic GPU.","C":"Use general purpose GPU computing instances such as G3 and P3.","D":"Use processing parallelism to partition the workload over multiple storage devices and balance the workload within the GPUs."},"answer_ET":"AC","isMC":true,"exam_id":17,"question_images":[],"unix_timestamp":1566802920,"timestamp":"2019-08-26 09:02:00","url":"https://www.examtopics.com/discussions/amazon/view/4093-exam-aws-certified-big-data-specialty-topic-2-question-2/","topic":"2","answers_community":[],"answer":"AC","answer_description":"","discussion":[{"poster":"san2020","timestamp":"1635495660.0","content":"my selection AC","upvote_count":"3","comment_id":"52387"},{"poster":"mattyb123","upvote_count":"1","comment_id":"8930","timestamp":"1635118860.0","comments":[{"content":"Question is missing an answer which states Model Parallelism. Answer is Its Model Parallelism and the instance type G3 and P3.","timestamp":"1635368340.0","poster":"mattyb123","comment_id":"9827","upvote_count":"8"}],"content":"Anyone got any further thoughts on this one?"},{"poster":"mattyb123","comment_id":"8315","comments":[{"poster":"mattyb123","content":"https://mxnet.incubator.apache.org/versions/master/faq/distributed_training.html","comment_id":"8655","comments":[{"comment_id":"8656","poster":"mattyb123","content":"Data Parallelism vs Model Parallelism\nBy default, MXNet uses data parallelism to partition the workload over multiple devices. Assume there are n devices. Then each one will receive a copy of the complete model and train it on 1/n of the data. The results such as gradients and updated model are communicated across these devices.\nMXNet also supports model parallelism. In this approach, each device holds onto only part of the model. This proves useful when the model is too large to fit onto a single device.\nAnswer doesn't list Model Parallelism as that would be correct when using large models maybe this is a typo?","upvote_count":"2","timestamp":"1633164600.0","comments":[{"upvote_count":"1","content":"but data or model parallelism which is better is still a hot debate in ML, if both options appear then I really don't know which one is better...","comment_id":"106653","timestamp":"1635991140.0","poster":"freedomeox"},{"content":"https://aws.amazon.com/blogs/machine-learning/reducing-deep-learning-inference-cost-with-mxnet-and-amazon-elastic-inference/ Mentions increased performance with EI elastic GPUs on compute ec2 instances. However answer doesn't refer to Amazon Elastic Inference.","comments":[{"timestamp":"1634242380.0","comment_id":"8755","upvote_count":"1","content":"@mattyb123: r u sitting in exam any time soon?","poster":"jlpl","comments":[{"comment_id":"8773","upvote_count":"2","poster":"mattyb123","content":"Yes, sitting it very soon again. Hence i want some feedback to talk through answers as this exam is hard compared to the CSA one.","timestamp":"1634797860.0"}]}],"upvote_count":"1","poster":"mattyb123","timestamp":"1633664940.0","comment_id":"8658"}]}],"timestamp":"1633127640.0","upvote_count":"1"}],"upvote_count":"1","timestamp":"1632819960.0","content":"answer is correct. https://aws.amazon.com/blogs/machine-learning/parallelizing-across-multiple-cpu-gpus-to-speed-up-deep-learning-inference-at-the-edge/"}]},{"id":"oQqufumqBOikzH7naaf8","answers_community":[],"answer_ET":"C","answer":"C","answer_images":[],"question_images":[],"topic":"2","question_id":78,"isMC":true,"timestamp":"2019-08-31 00:54:00","choices":{"B":"Create an Amazon RDS Aurora table, with Amazon_ID as the primary key for each user. The application uses amazon.com web identity federation to get a token that is used to assume an IAM role. Use IAM database authentication by using rds:db-tag IAM authentication policy and GRANT Amazon RDS row- level read permission per user.","D":"Create an Amazon DynamoDB table, with Amazon_ID as the partition key. The application uses amazon.com web identity federation to assume an IAM role from AWS STS in the Role, use IAM condition context key dynamodb:LeadingKeys with IAM substitution variables $ {www.amazon.com:user_id} and allow the required DynamoDB API operations in IAM JSON policy Action element for reading the records.","C":"Create an Amazon DynamoDB table, with Amazon_ID as the partition key. The application uses amazon.com web identity federation to get a token that is used to assume an IAM role from AWS STS in the Role, use IAM condition context key dynamodb:LeadingKeys with IAM substitution variables $ and allow the required DynamoDB API operations in IAM JSON policy Action element for reading the records. {www.amazon.com:user_id}","A":"Create an Amazon RDS Aurora table, with Amazon_ID as the primary key. The application uses amazon.com web identity federation to get a token that is used to assume an IAM role from AWS STS. Use IAM database authentication by using the rds:db-tag IAM authentication policy and GRANT Amazon RDS row-level read permission per user."},"answer_description":"","discussion":[{"poster":"Bulti","comment_id":"78857","timestamp":"1636145100.0","upvote_count":"6","content":"For the question on Dynamo DB that @mattyb123 has posted here, the answer is A and not B. You should always use IAM role and attach an IAM policy to that role on EC2 instance to be able to access both Dynamo DB and DAX."},{"comment_id":"140578","content":"\"A\" - Because , Dynamo blends more with DAX cannot think of elasticache. Role is Must","upvote_count":"1","poster":"Venky_2020","timestamp":"1636182000.0"},{"poster":"fansely92","comment_id":"66469","timestamp":"1635944100.0","upvote_count":"1","content":"Thank you for helping me and everyone"},{"poster":"san2020","timestamp":"1635658200.0","content":"my selection C","comment_id":"52409","upvote_count":"2"},{"upvote_count":"1","timestamp":"1634992020.0","poster":"ME2000","content":"The issues boil down to IAM user VS IAM role.\nAnd IAM user on the EC2 instance is an invalid option\nTherefore A is the correct answer.","comment_id":"38643"},{"timestamp":"1634338260.0","content":"What's the answer finally?","comment_id":"20209","comments":[{"comment_id":"20300","poster":"d00ku","timestamp":"1634601900.0","content":"C should be the correct one.","upvote_count":"2"}],"upvote_count":"1","poster":"harry_123"},{"comment_id":"16950","content":"C is the right answer : as token is mentioned and it's very important while it's missing in option D. see below article\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WIF.html","poster":"bigdatalearner","timestamp":"1634124480.0","upvote_count":"1"},{"content":"C is correct for question 20 as per the following link:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WIF.html\nreview the Web Identity Federation Overview section.","timestamp":"1633331280.0","upvote_count":"1","poster":"mattyb123","comment_id":"9389","comments":[{"upvote_count":"1","content":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/specifying-conditions.html additional reading","timestamp":"1633608780.0","poster":"mattyb123","comment_id":"9390"}]},{"comments":[{"content":"@revs1610 thanks for the feedback. A few of us have been working on the correct solutions by writing the reasons why in the comments throughout the past 17 pages, as we have also noticed mistakes with the answers. If you can spare some time we would appreciate you take a look at some of the comments and assist where we may be wrong.","poster":"mattyb123","timestamp":"1633260780.0","upvote_count":"1","comment_id":"9310"}],"content":"@mattyb123, i took the exam recently and it looks most of the questions are from the thread, and the answer i was provided is the one which is specified in the comment section. even though i passed some answers are wrong. i request you take some more time to review each questions before you guys go with final exam.","poster":"revs1610","upvote_count":"3","timestamp":"1633191300.0","comment_id":"9253"},{"poster":"revs1610","comment_id":"9196","timestamp":"1632457440.0","comments":[{"comment_id":"9221","content":"thanks @revs1610 i think you are right with B after reviewing that link. Answer has to be either B or D due to the RCU/WCU calculation.\n1.https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.client.create-user-policy.html Mentions providing ec2 user permissions to use DAX application client to secure the the ec2 instance instead of assuming a role or service role which makes sense for the answer to be B. Only reason i didn't think of DAX was the millisecond requirement which Elasticache performs.","poster":"mattyb123","timestamp":"1632900600.0","upvote_count":"1"}],"content":"with the requirement \"Minimal changes to application code to improve performance using write-through cache\", the answer could be B instead of D. base on the https://www.quora.com/What-is-the-difference-between-ElastiCache-and-DynamoDB-Accelerator-in-AWS","upvote_count":"2"},{"comment_id":"9055","poster":"mattyb123","timestamp":"1632170760.0","content":"An organization is designing an Amazon DynamoDB table for an application that must meet the following requirements:\n- item size is 40 KB\n- Read/Write ratio 2000/500 sustained, respectively\n- Heavily read-oriented and requires low latencies in the order of milliseconds\n- The application runs on an Amazon Ec2 instance\n- Access to the DynamoDB table must be secure within the VPC\n- Minimal changes to application code to improve performance using write-through cache\nWhich design options will BEST meet these requirements?\nA. Size the DynamoDB table with 10000 RCU/20000 WCUs, implement the DynamoDB Accelerator (DAX) for read performance, use VPC endpoints for DynamoDB, and implement an IAM role on the EC2 Instance to secure DynamoDB access.\nB. Size the DynamoDB table with 20000 RCU/20000 WCUs, implement the DynamoDB Accelerator (DAX) for read performance, leverage VPC endpoints for DynamoDB, and implement an IAM user on the EC2 Instance to secure DynamoDB access.","comments":[{"timestamp":"1632205140.0","upvote_count":"2","poster":"mattyb123","comments":[{"poster":"d00ku","comments":[{"upvote_count":"3","content":"agree,\nnever select an answer that give a user to EC2 instead of a role.\nAlso always think DAX when seeing milliseconds.\nAlso VPC so there must be an Endpoint.\nThe only answer that satisfies all of the above is A.","timestamp":"1635407940.0","poster":"AdamSmith","comment_id":"47648"}],"timestamp":"1634218800.0","comment_id":"19917","content":"Default dynamo behavious (DAX included) is eventual consistent reads. Question does not mention strongly consistent reads. RCU should be 10k not 20k. DAX will work in this scenario. EC2 should use IAM role not user. Answer A seems correct as dynamo needs VPC endpoint.","upvote_count":"7"},{"comment_id":"184738","content":"A. Table with 10000 RCU/20000 WCU implies eventually consistent model which has a lower latency compared to strongly consistent (https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html). And of course DAX integrates better with DynamoDB with millisecond to microsecond latency.","timestamp":"1636205040.0","poster":"vicks316","upvote_count":"1"},{"poster":"mattyb123","upvote_count":"1","content":"I think answer is D due to. \n1 provides milliseconds, \n2. provides write through cache \n3. RCU 40/4=10 10*2000 = 20000\n4. WCU 40*500= 20000\nOnly part i dont like is the ec2 instance implementing a user not a role.","timestamp":"1632298980.0","comment_id":"9057","comments":[{"upvote_count":"1","poster":"VB","content":"If you want to write through cache to dynamodb.. don't we need DAX? I think the answer is B for this question. I don't think you can update dynamo-db through ElastiCache. Please correct me if I am wrong.","timestamp":"1633998540.0","comment_id":"12685"}]}],"comment_id":"9056","content":"C. Size the DynamoDB table with 10000 RCU/20000 WCUs, implement Amazon ElastiCache for read performance, set up a NAT gateway on VPC for the EC2 instance to access DynamoDB, and implement an IAM role on the EC2 Instance to secure DynamoDB access.\nD. Size the DynamoDB table with 20000 RCU/20000 WCUs, implement Amazon ElastiCache for read performance, leverage VPC endpoints for DynamoDB, and implement an IAM user on the EC2 Instance to secure DynamoDB access."}],"upvote_count":"3"}],"question_text":"An organization is designing a public web application and has a requirement that states all application users must be centrally authenticated before any operations are permitted. The organization will need to create a user table with fast data lookup for the application in which a user can read only his or her own data. All users already have an account with amazon.com.\nHow can these requirements be met?","exam_id":17,"unix_timestamp":1567205640,"url":"https://www.examtopics.com/discussions/amazon/view/4403-exam-aws-certified-big-data-specialty-topic-2-question-20/"},{"id":"XA0A6YG08JDZKdMwRXEb","discussion":[{"content":"D pretty cool!\nhttps://docs.aws.amazon.com/quicksight/latest/user/working-with-stories.html","poster":"jkoffee","upvote_count":"2","timestamp":"1636015320.0","comment_id":"110087"},{"content":"my selection C","comment_id":"52389","comments":[{"comment_id":"101712","upvote_count":"1","poster":"Corram","timestamp":"1634422260.0","content":"Your other selection is more appropriate.","comments":[{"timestamp":"1635741300.0","content":"I really like this bot...","upvote_count":"5","comment_id":"106656","poster":"freedomeox"}]}],"timestamp":"1633781640.0","poster":"san2020","upvote_count":"1"},{"timestamp":"1633677840.0","poster":"san2020","content":"my selection D","upvote_count":"3","comment_id":"52388"},{"upvote_count":"4","timestamp":"1633071720.0","poster":"shandy","comment_id":"9917","content":"Answer is right. https://docs.aws.amazon.com/quicksight/latest/user/working-with-stories.html"}],"isMC":true,"unix_timestamp":1567775760,"answers_community":[],"choices":{"B":"Use a pivot table as a visual option to display measured values and weekly aggregate data as a row dimension.","C":"Use a dashboard option to create an analysis of the data for each week and apply filters to visualize the data change.","A":"Use the analysis option for data captured in each week and view the data by a date range.","D":"Use a story option to preserve multiple iterations of an analysis and play the iterations sequentially."},"timestamp":"2019-09-06 15:16:00","topic":"2","url":"https://www.examtopics.com/discussions/amazon/view/4807-exam-aws-certified-big-data-specialty-topic-2-question-3/","question_id":79,"exam_id":17,"answer":"D","answer_ET":"D","answer_description":"","question_text":"An organization is soliciting public feedback through a web portal that has been deployed to track the number of requests and other important data. As part of reporting and visualization, AmazonQuickSight connects to an Amazon RDS database to virtualize data. Management wants to understand some important metrics about feedback and how the feedback has changed over the last four weeks in a visual representation.\nWhat would be the MOST effective way to represent multiple iterations of an analysis in Amazon QuickSight that would show how the data has changed over the last four weeks?","question_images":[],"answer_images":[]},{"id":"5CIB5cwPmB5hGSq0dVUd","topic":"2","choices":{"D":"Set up Apache Hive metastore on an Amazon EC2 instance and run a scheduled bash script that connects to data sources to populate the metastore.","A":"Set up Amazon DynamoDB as the data catalog and run a scheduled AWS Lambda function that connects to data sources to populate the DynamoDB table.","B":"Use an Amazon database as the data catalog and run a scheduled AWS Lambda function that connects to data sources to populate the database.","C":"Use AWS Glue Data Catalog as the data catalog and schedule crawlers that connect to data sources to populate the catalog."},"discussion":[{"content":"my selection C","poster":"san2020","comment_id":"52390","upvote_count":"3","timestamp":"1633981380.0"},{"comment_id":"47565","poster":"venkataws","timestamp":"1633375740.0","content":"Does these kind of straight forward questions appear in the exam ? Anyone who have wrote the exam can confirm. Thanks.","upvote_count":"1"},{"poster":"I_heart_shuffle_girls","timestamp":"1632941580.0","upvote_count":"2","comment_id":"34176","content":"C is correct https://docs.aws.amazon.com/glue/latest/dg/populate-data-catalog.html"},{"comment_id":"19415","upvote_count":"4","content":"C is correct","poster":"cybe001","timestamp":"1632143880.0"}],"answer_description":"","exam_id":17,"answer":"C","answer_ET":"C","url":"https://www.examtopics.com/discussions/amazon/view/7724-exam-aws-certified-big-data-specialty-topic-2-question-4/","answers_community":[],"answer_images":[],"isMC":true,"unix_timestamp":1572981180,"question_id":80,"timestamp":"2019-11-05 20:13:00","question_text":"An organization is setting up a data catalog and metadata management environment for their numerous data stores currently running on AWS. The data catalog will be used to determine the structure and other attributes of data in the data stores. The data stores are composed of Amazon RDS databases, Amazon\nRedshift, and CSV files residing on Amazon S3. The catalog should be populated on a scheduled basis, and minimal administration is required to manage the catalog.\nHow can this be accomplished?","question_images":[]}],"exam":{"id":17,"numberOfQuestions":85,"provider":"Amazon","isImplemented":true,"name":"AWS Certified Big Data - Specialty","isMCOnly":true,"lastUpdated":"11 Apr 2025","isBeta":false},"currentPage":16},"__N_SSP":true}