{"pageProps":{"questions":[{"id":"2DZQqX15bRyFAHJdpb2A","choices":{"A":"Use the AnalyzeDocument API action. Set the FeatureTypes parameter to SIGNATURES. Return the confidence scores for each page.","B":"Use the Prediction API call on the documents. Return the signatures and confidence scores for each page.","C":"Use the StartDocumentAnalysis API action to detect the signatures. Return the confidence scores for each page.","D":"Use the GetDocumentAnalysis API action to detect the signatures. Return the confidence scores for each page."},"question_text":"A law firm handles thousands of contracts every day. Every contract must be signed. Currently, a lawyer manually checks all contracts for signatures.\n\nThe law firm is developing a machine learning (ML) solution to automate signature detection for each contract. The ML solution must also provide a confidence score for each contract page.\n\nWhich Amazon Textract API action can the law firm use to generate a confidence score for each page of each contract?","answer_description":"","topic":"1","question_images":[],"question_id":256,"answer_images":[],"unix_timestamp":1710336900,"url":"https://www.examtopics.com/discussions/amazon/view/135917-exam-aws-certified-machine-learning-specialty-topic-1/","exam_id":26,"answer_ET":"A","answer":"A","answers_community":["A (100%)"],"timestamp":"2024-03-13 14:35:00","discussion":[{"content":"Selected Answer: A\nhttps://docs.aws.amazon.com/textract/latest/dg/API_AnalyzeDocument.html","comment_id":"1400457","poster":"ef12052","timestamp":"1742379000.0","upvote_count":"1"},{"poster":"ggrodskiy","content":"A. The AnalyzeDocument API action in Amazon Textract allows for the analysis of various features within a document, including signatures. By setting the FeatureTypes parameter to SIGNATURES, the law firm can instruct Textract to specifically focus on detecting signatures within the contracts. Additionally, Textract provides confidence scores for detected elements, including signatures. Therefore, by using this action and specifying the FeatureTypes parameter, the law firm can receive confidence scores for each page of each contract, facilitating the automation of signature detection.","upvote_count":"2","timestamp":"1729947420.0","comment_id":"1202595"},{"upvote_count":"1","poster":"AIWave","comment_id":"1173680","timestamp":"1726337580.0","content":"Selected Answer: A\nAnalyzeDocument Signatures feature automatically detects and extracts signature images from the document and comes with a confidence score (ranging from 0 to 100) for each detected signature"},{"comment_id":"1172598","poster":"Peter_Hsieh","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/textract/latest/dg/API_AnalyzeDocument.html","timestamp":"1726227300.0","upvote_count":"3"}],"isMC":true},{"id":"YAl3FpaHnaPjgGJK9tJ2","question_id":257,"question_text":"A gaming company has launched an online game where people can start playing for free, but they need to pay if they choose to use certain features. The company needs to build an automated system to predict whether or not a new user will become a paid user within 1 year. The company has gathered a labeled dataset from 1 million users.\nThe training dataset consists of 1,000 positive samples (from users who ended up paying within 1 year) and 999,000 negative samples (from users who did not use any paid features). Each data sample consists of 200 features including user age, device, location, and play patterns.\nUsing this dataset for training, the Data Science team trained a random forest model that converged with over 99% accuracy on the training set. However, the prediction results on a test dataset were not satisfactory\nWhich of the following approaches should the Data Science team take to mitigate this issue? (Choose two.)","answer_description":"","answer_images":[],"choices":{"B":"Include a copy of the samples in the test dataset in the training dataset.","C":"Generate more positive samples by duplicating the positive samples and adding a small amount of noise to the duplicated data.","E":"Change the cost function so that false positives have a higher impact on the cost value than false negatives.","D":"Change the cost function so that false negatives have a higher impact on the cost value than false positives.","A":"Add more deep trees to the random forest to enable the model to learn more features."},"answers_community":["CD (100%)"],"isMC":true,"timestamp":"2019-12-16 02:56:00","answer":"CD","topic":"1","exam_id":26,"answer_ET":"CD","question_images":[],"unix_timestamp":1576461360,"url":"https://www.examtopics.com/discussions/amazon/view/10409-exam-aws-certified-machine-learning-specialty-topic-1/","discussion":[{"comment_id":"45319","poster":"Phong","upvote_count":"30","timestamp":"1632532080.0","content":"I think it should be CD\nC: because we need a balance dataset\nD: The number of positive samples is large so model tends to predict 0 (negative) for all cases leading to False Negative problem. We should minimize that.\nMy opinion"},{"upvote_count":"24","timestamp":"1632572040.0","content":"I think it should be CD\nC: because we need a balance dataset\nD: The number of negative samples is large so model tends to predict 0 (negative) for all cases leading to False Negative problem. We should minimize that.\nMy opinion","comment_id":"45322","poster":"Phong"},{"comment_id":"1358042","upvote_count":"2","content":"Selected Answer: CD\nWhy These Are the Best Choices?\nC. Generate more positive samples by duplicating the positive samples and adding a small amount of noise to the duplicated data. \n\nBalances the dataset by increasing the number of positive samples.\nAdding noise prevents overfitting and helps the model generalize better.\nAlternative: Use SMOTE (Synthetic Minority Over-sampling Technique) to generate synthetic positive examples.\nD. Change the cost function so that false negatives have a higher impact on the cost value than false positives. \n\nSince missing a potential paying user (false negative) is more critical than misclassifying a non-paying user, adjusting the cost function to penalize false negatives more will improve recall for paid users.\nMethods:\nUse weighted loss functions (e.g., weighted cross-entropy).\nAdjust class weights in random forest or another algorithm.\nUse AUC-ROC or F1-score instead of accuracy for evaluation.","timestamp":"1739837520.0","poster":"JonSno"},{"timestamp":"1730339220.0","poster":"dinITExam","upvote_count":"1","content":"Think C and D","comment_id":"1305270"},{"poster":"John_Pongthorn","timestamp":"1645415940.0","upvote_count":"5","comment_id":"552499","content":"Selected Answer: CD\nC,D is correct (percentage of the positive class is key to decide which case we are interested in)\nThis question, positive class (Pay) is 0.01% as compared to 99.99( not pay) , as a result, we have to pay attention to Pay because if we miss 0.01% out, we didn't get revenue. it is a false negative.\n\nIn contrast to these questions, it positive class (Pay) is 40% as compared to negative class (60% not pay), it is avoidable to emphasize on 40% ( if model predict as payment but in reality customer neglect), we won't get revenue the amount from false positive)"},{"upvote_count":"1","comment_id":"540718","content":"I think is CD","timestamp":"1644023340.0","poster":"apprehensive_scar"},{"comment_id":"278236","poster":"cloud_trail","content":"C and D. Hopefully, no one honestly thinks that B is a good answer. Never expose test data to the training set or vice versa. C is right because of the highly imbalanced training set. D is right because you want to minimize false negatives, maximize true positives, maximize recall of the positive class. I'm not sure why anyone's worried about precision in this case.","timestamp":"1636212600.0","upvote_count":"4"},{"comment_id":"274151","timestamp":"1636126020.0","upvote_count":"8","poster":"felbuch","content":"CD\n\nThe model has 99% accuracy because it's simply predicting that everyone's a negative. Since almost everyone's a negative, it will get almost everyone right.\nSo we need to penalize the model for predicting that someone is a negative when it is not (i.e. penalize false negatives). So that's D.\n\nAlso, it would be really nice to have more positives -- one way to do that is to follow option C."},{"comment_id":"241673","timestamp":"1635667140.0","upvote_count":"1","content":"CD 100%","poster":"engomaradel"},{"content":"CD\n\nC:imbalance of test (1000 positive, 999000 negative = 0.1% positive) thus C to increase that\n\nD :also to reduce generalizing, since everyone says no, the model would generalize to no, but increasing the penalty of a false negative would reduce generalizing..","upvote_count":"2","comment_id":"226008","timestamp":"1635208200.0","poster":"ybad"},{"timestamp":"1634651820.0","comment_id":"211266","poster":"Omar_Cascudo","content":"It is needed to diminish the FP, because they are player predicted to pay and in reality will not pay. So FP should impact the cost metric more. CE should be the answer.","upvote_count":"2"},{"content":"CD are correct for sure.","comment_id":"167671","timestamp":"1634596500.0","poster":"bidds","upvote_count":"3"},{"comment_id":"144937","content":"It is C,E... we want to find all paying customers, which are positives, so we have to punish incorrectly finding negatives, which is E","poster":"hans1234","timestamp":"1634461020.0","upvote_count":"2"},{"comment_id":"123351","content":"CD\n\nalthough i am worried about the noise being introduced as it could skew the data nevertheless no better answer is given","timestamp":"1634380260.0","poster":"Wira","upvote_count":"2"},{"content":"CD\nWe need high recall so that we do not miss many Positive cases. In that case we need to have less False Negative(FN) therefore it should have high impact on cost function.","upvote_count":"3","poster":"aws_razor","timestamp":"1634156520.0","comment_id":"94788"},{"poster":"roytruong","content":"in my view, CD are answers\nC: of course, handle the imbalanced dataset\nD: right now, model accuracy is 99%, it means model predict everything is negative leading to FN problem, so we need to minimize it more in cost function","upvote_count":"3","comment_id":"84792","timestamp":"1633600980.0"},{"poster":"wuha5086","upvote_count":"8","timestamp":"1633450440.0","comment_id":"77606","content":"CD, FN are valuable players, we should care more on FN"},{"comment_id":"60126","content":"Is my assumption right here?\n\n ACTUAL\n --------------------------------------\nP PAY NPAY \nR --------------------------------------\nE PAY TP FP \nD \nI NPAY FN TN \nC -----------------------------------","poster":"VB","timestamp":"1633352220.0","upvote_count":"1"},{"timestamp":"1633222920.0","comment_id":"58390","upvote_count":"5","poster":"AKT","content":"C,D correct"},{"content":"#1 Imbalanced data - needs balancing\nAgreed with C\n\n# Imbalanced Cost of FP & FN\nWe have a higher probability of FN and lower probability of FP\nTo balance that increase the sensiitivity/weight towards FP\n\nSo the do E\n\nAnswer is CE","comment_id":"57322","poster":"rajs","timestamp":"1632826080.0","upvote_count":"9"},{"content":"CD: should take care of False Negatives over False Positives","upvote_count":"9","timestamp":"1632695640.0","comment_id":"51466","poster":"JayK"},{"poster":"JayK","upvote_count":"14","content":"Answer should be CE. C because the problem here is of an imbalanced dataset and for that positive samples need to be included.\nE because of the Precision","comment_id":"35279","comments":[{"content":"Agreed with CE but confused about your suggestion of \"Precision\"\n\nPrecision = (TP)/(TP+FP)\nSo are you saying to increase precision adjust FP","timestamp":"1633203660.0","comment_id":"57323","poster":"rajs","upvote_count":"3"}],"timestamp":"1632308580.0"},{"timestamp":"1632291060.0","upvote_count":"2","content":"I think B possibly not correct","poster":"heihei","comment_id":"34425"},{"comment_id":"29915","poster":"heihei","content":"if the Precision of model is more cared, it should be E not D","timestamp":"1632165060.0","upvote_count":"1"}]},{"id":"3TOOOmemrXFwU6SMHKRA","url":"https://www.examtopics.com/discussions/amazon/view/136026-exam-aws-certified-machine-learning-specialty-topic-1/","topic":"1","exam_id":26,"discussion":[{"comment_id":"1331458","timestamp":"1735116360.0","content":"Selected Answer: ADE\nXGBoost is primarily designed for tabular data. For a task involving image data, XGBoost would not be a suitable choice because it does not have the capability to directly process images or learn from the spatial relationships inherent in image data.\nIn the context of the problem described, a more suitable approach would involve using a deep learning model, e.g., CNNs. Refer to Question 258 where we choose ResNet-50 instead of. XGBoost","upvote_count":"1","poster":"LeoD"},{"content":"Selected Answer: ADE\nADE ◊Recognition is for general identification, this requires a specific training with only 1% data available.","upvote_count":"1","timestamp":"1732993200.0","poster":"ciscochamps","comment_id":"1320352"},{"content":"A,B,F\nYou can not use XGBoost for object recognition directly. You need to integrate it with CNN. \nYou can use Rekognition custom for this scenario","comment_id":"1306239","poster":"spinatram","timestamp":"1730564760.0","upvote_count":"1"},{"timestamp":"1729415940.0","poster":"MultiCloudIronMan","upvote_count":"1","comment_id":"1300378","content":"Selected Answer: ADE\nRecognition is for general identification, this requires a specific training with only 1% data available."},{"timestamp":"1721002200.0","content":"Selected Answer: ADE\nDetecting corrosion is too specialized a task for Rekognition - trying transfer learning would literally mean re-training.\n\nAWS blog explaining suitability of Object Detection/Semantic Segmentation (though they prefer the color classification approach using XGBoost): https://aws.amazon.com/blogs/machine-learning/rust-detection-using-machine-learning-on-aws/\n\nThe enhancement step may be needed to contrast enhance/sharpen images.","poster":"cloudera3","comment_id":"1248006","upvote_count":"1"},{"poster":"3eb0542","comment_id":"1209033","upvote_count":"1","timestamp":"1715279100.0","content":"Selected Answer: ABE\nThe combination of steps that would meet the requirements is indeed A, B, and E"},{"timestamp":"1710723960.0","content":"Selected Answer: ABE\nThis is object detection problem, which Rekognitation can do since they have 0.1% corrosion photos, augmentation is necessary","upvote_count":"2","comment_id":"1176153","poster":"vkbajoria"},{"timestamp":"1710472320.0","comment_id":"1174009","upvote_count":"3","poster":"butaman","content":"Selected Answer: ADE\nA. Use an object detection algorithm: This can help identify corrosion areas in a photo.\nE. Perform image augmentation on photos with corrosion: This can improve the model’s ability to generalize by increasing the diversity of the training data.\nD. Use an XGBoost algorithm: This can classify the severity of the corrosion after the areas of corrosion have been identified. It’s effective for multi-class classification problems."},{"content":"Selected Answer: ABE\nA: Yes - train a model that identifies corrosion areas within the photos\nB: Yes - identify and label objects, including corrosion, in the images.\nC: No - Not for classification\nD: No - XGBoost doesn't work on images\nE: Yes - rotation, scaling, and flipping, can enhance the model’s ability to generalize and improve its performance\nF: not required","timestamp":"1710448620.0","comment_id":"1173697","poster":"AIWave","upvote_count":"2"},{"upvote_count":"1","poster":"KAST_424","timestamp":"1710441120.0","comment_id":"1173633","content":"Selected Answer: ADE\nObject detection algorithm can be trained to identify corrosion rather than too customize Rekognition"}],"question_id":258,"answer_images":[],"question_images":[],"answers_community":["ADE (62%)","ABE (38%)"],"answer_description":"","answer":"ADE","unix_timestamp":1710441120,"timestamp":"2024-03-14 19:32:00","choices":{"A":"Use an object detection algorithm to train a model to identify corrosion areas of a photo.","F":"Perform image augmentation on photos that do not contain corrosion.","C":"Use a k-means clustering algorithm to train a model to classify the severity of corrosion in a photo.","B":"Use Amazon Rekognition with label detection on the photos.","D":"Use an XGBoost algorithm to train a model to classify the severity of corrosion in a photo.","E":"Perform image augmentation on photos that contain corrosion."},"isMC":true,"question_text":"A company that operates oil platforms uses drones to photograph locations on oil platforms that are difficult for humans to access to search for corrosion.\n\nExperienced engineers review the photos to determine the severity of corrosion. There can be several corroded areas in a single photo. The engineers determine whether the identified corrosion needs to be fixed immediately, scheduled for future maintenance, or requires no action. The corrosion appears in an average of 0.1% of all photos.\n\nA data science team needs to create a solution that automates the process of reviewing the photos and classifying the need for maintenance.\n\nWhich combination of steps will meet these requirements? (Choose three.)","answer_ET":"ADE"},{"id":"PcdnMeeqEK9liNJk5Ted","answer_description":"","question_text":"A company maintains a 2 TB dataset that contains information about customer behaviors. The company stores the dataset in Amazon S3. The company stores a trained model container in Amazon Elastic Container Registry (Amazon ECR).\n\nA machine learning (ML) specialist needs to score a batch model for the dataset to predict customer behavior. The ML specialist must select a scalable approach to score the model.\n\nWhich solution will meet these requirements MOST cost-effectively?","unix_timestamp":1710449760,"question_images":[],"choices":{"A":"Score the model by using AWS Batch managed Amazon EC2 Reserved Instances. Create an Amazon EC2 instance store volume and mount it to the Reserved Instances.","C":"Score the model by using an Amazon SageMaker notebook on Amazon EC2 Reserved Instances. Create an Amazon EBS volume and mount it to the Reserved Instances.","D":"Score the model by using Amazon SageMaker notebook on Amazon EC2 Spot Instances. Create an Amazon Elastic File System (Amazon EFS) file system and mount it to the Spot Instances.","B":"Score the model by using AWS Batch managed Amazon EC2 Spot Instances. Create an Amazon FSx for Lustre volume and mount it to the Spot Instances."},"exam_id":26,"answer_images":[],"answer":"B","topic":"1","answer_ET":"B","url":"https://www.examtopics.com/discussions/amazon/view/136031-exam-aws-certified-machine-learning-specialty-topic-1/","answers_community":["B (100%)"],"timestamp":"2024-03-14 21:56:00","discussion":[{"content":"Selected Answer: B\nB seems to be most cost effective","poster":"vkbajoria","upvote_count":"1","timestamp":"1727694720.0","comment_id":"1186107"},{"comment_id":"1174012","timestamp":"1726362960.0","content":"Selected Answer: B\nThe most cost-effective solution is Option B: Use AWS Batch with Amazon EC2 Spot Instances and Amazon FSx for Lustre. This approach leverages the efficiency of AWS Batch, the cost benefits of Spot Instances, and the high-performance of FSx for Lustre, making it ideal for scoring a batch model. However, Spot Instances can be interrupted, so they’re best for flexible workloads.","poster":"butaman","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: B\nAWS Batch managed Amazon EC2 Spot Instances - cost effective!\nFSx for Lustre - large volume (2 TB) data","comment_id":"1173713","timestamp":"1726340160.0","poster":"AIWave"}],"question_id":259,"isMC":true},{"id":"zufJikKD1tUSDXEoZ249","isMC":true,"discussion":[{"poster":"MultiCloudIronMan","upvote_count":"2","comment_id":"1300380","content":"Selected Answer: D\nEfficient Resource Allocation: Hyperband is designed to allocate computational resources efficiently by dynamically stopping underperforming configurations and focusing on the more promising ones","timestamp":"1729416420.0"},{"comment_id":"1176144","poster":"vkbajoria","content":"Selected Answer: D\nHyperband is he answer","upvote_count":"1","timestamp":"1710723180.0"},{"content":"Selected Answer: D\nThe best technique for the data scientist’s requirements is Hyperband. It’s designed for a large number of experiments, stops low-performance models early, and allocates more resources to high-performance models. This reduces computational time compared to Grid Search, Random Search, and Bayesian Optimization which don’t have these features.","timestamp":"1710472440.0","poster":"butaman","comment_id":"1174010","upvote_count":"1"},{"comment_id":"1173732","content":"Selected Answer: D\nHyperband involves training multiple models with different hyperparameter configurations , eliminating poorly performing ones and allocating resources to promising ones.","upvote_count":"1","poster":"AIWave","timestamp":"1710450540.0"},{"comment_id":"1173636","poster":"KAST_424","timestamp":"1710441480.0","content":"Selected Answer: D\n. Hyperband: This technique is a bandit-based approach that allocates resources efficiently by running multiple configurations in parallel with varying durations. It eliminates poorly performing configurations early and focuses resources on promising ones, making it ideal for minimizing compute time.","upvote_count":"2"}],"answer_ET":"D","answer_description":"","answer":"D","timestamp":"2024-03-14 19:38:00","unix_timestamp":1710441480,"choices":{"B":"Random search","D":"Hyperband","A":"Grid search","C":"Bayesian optimization"},"answer_images":[],"topic":"1","exam_id":26,"answers_community":["D (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/136027-exam-aws-certified-machine-learning-specialty-topic-1/","question_images":[],"question_id":260,"question_text":"A data scientist is implementing a deep learning neural network model for an object detection task on images. The data scientist wants to experiment with a large number of parallel hyperparameter tuning jobs to find hyperparameters that optimize compute time.\n\nThe data scientist must ensure that jobs that underperform are stopped. The data scientist must allocate computational resources to well-performing hyperparameter configurations. The data scientist is using the hyperparameter tuning job to tune the stochastic gradient descent (SGD) learning rate, momentum, epoch, and mini-batch size.\n\nWhich technique will meet these requirements with LEAST computational time?"}],"exam":{"id":26,"isImplemented":true,"numberOfQuestions":369,"name":"AWS Certified Machine Learning - Specialty","isMCOnly":false,"lastUpdated":"11 Apr 2025","provider":"Amazon","isBeta":false},"currentPage":52},"__N_SSP":true}