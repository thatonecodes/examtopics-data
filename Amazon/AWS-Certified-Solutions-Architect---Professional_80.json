{"pageProps":{"questions":[{"id":"6bOm41Pj0w9rqvrZNRlv","question_id":396,"answer_images":[],"question_images":[],"timestamp":"2019-09-12 13:52:00","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/5115-exam-aws-certified-solutions-architect-professional-topic-1/","topic":"1","choices":{"D":"Back up all the data to Amazon S3 in the production region. Set up cross-region replication of this S3 bucket to another region and set up a lifecycle policy in the second region to immediately move this data to Amazon Glacier.","C":"Back up all the data to Amazon Glacier in the production region. Set up cross-region replication of this data to Amazon Glacier in the disaster recovery region. Set up a lifecycle policy to delete any data older than 60 days.","B":"Back up all the data to Amazon S3 in the disaster recovery region. Use a lifecycle policy to move this data to Amazon Glacier in the production region immediately. Only the data is replicated; remove the data from the S3 bucket in the disaster recovery region.","A":"Back up all the data to a large Amazon EBS volume attached to the backup media server in the production region. Run automated scripts to snapshot these volumes nightly, and copy these snapshots to the disaster recovery region."},"unix_timestamp":1568289120,"question_text":"A company is finalizing the architecture for its backup solution for applications running on AWS. All of the applications run on AWS and use at least two Availability\nZones in each tier.\nCompany policy requires IT to durably store nightly backups for all its data in at least two locations: production and disaster recovery. The locations must be in different geographic regions. The company also needs the backup to be available to restore immediately at the production data center, and within 24 hours at the disaster recovery location. All backup processes must be fully automated.\nWhat is the MOST cost-effective backup solution that will meet all requirements?","isMC":true,"exam_id":32,"answer":"D","discussion":[{"timestamp":"1632350400.0","upvote_count":"40","content":"D\nA: Not sustainable. EBS has a 16TiB limit.\nB: No backup at the production region.\nC: Glacier does not allow restore immediately.","poster":"donathon","comment_id":"12832"},{"content":"Support \"D\" answer.\nS3 in production for immediate recovery. Glacier for Disaster with 24 hours recovery.","poster":"Moon","comment_id":"14294","upvote_count":"15","timestamp":"1632352620.0"},{"timestamp":"1697113620.0","upvote_count":"1","content":"A. You need an immediate restore, it's only possible with snapshots. No matter how \"attractive\" D looks, it doesn't fit the requirement.","comment_id":"1041722","poster":"kondratyevmn"},{"comment_id":"895386","timestamp":"1683835200.0","content":"Selected Answer: D\nD , easy one","poster":"mimadour21698","upvote_count":"2"},{"timestamp":"1653570180.0","poster":"bobsmith2000","comment_id":"607663","content":"Selected Answer: D\nno-brainer","upvote_count":"3"},{"comment_id":"577279","upvote_count":"2","content":"Selected Answer: D\nD meets all requirements and is the most cost efficient.","poster":"Mimek","timestamp":"1648534800.0"},{"poster":"AMKazi","upvote_count":"1","comment_id":"534185","timestamp":"1643326440.0","content":"D: meets all requirements\nA: Expensive and not needed to back up all data to PROD\nB: Does not meeting immediate access requirement in PROD \nC: Same as B"},{"timestamp":"1643125320.0","poster":"pititcu667","upvote_count":"1","content":"Selected Answer: D\nd meets all the checkboxes","comment_id":"532242"},{"timestamp":"1640944320.0","poster":"cldy","comment_id":"514009","upvote_count":"1","content":"D is correct."},{"comment_id":"507178","poster":"Ni_yot","upvote_count":"1","timestamp":"1640184180.0","content":"D for sure"},{"timestamp":"1638634140.0","poster":"AzureDP900","upvote_count":"1","comment_id":"493787","content":"D without any doubt"},{"content":"It's D","upvote_count":"1","comment_id":"450616","timestamp":"1636264200.0","poster":"andylogan"},{"content":"I'll go with D","timestamp":"1636151460.0","upvote_count":"1","poster":"WhyIronMan","comment_id":"409493"},{"upvote_count":"3","timestamp":"1635966960.0","comment_id":"362405","poster":"Radhaghosh","content":"Key phrase of the question \"the backup to be available to restore immediately at the production data center, and within 24 hours at the disaster recovery location.\" and Cost effective solution. So it has to be S3 and S3 Standard (storage tire) for Production backup region. Both the condition available only in Option D.\nSo Correct Answer is D"},{"timestamp":"1635776400.0","comment_id":"344446","poster":"blackgamer","content":"The answer is D for sure.","upvote_count":"1"},{"poster":"Waiweng","comment_id":"343952","timestamp":"1635725520.0","upvote_count":"3","content":"it's D"},{"upvote_count":"1","timestamp":"1635468060.0","poster":"KnightVictor","comment_id":"329223","content":"Should be D"},{"timestamp":"1635461760.0","upvote_count":"1","poster":"alisyech","comment_id":"322386","content":"i choose D"},{"upvote_count":"2","timestamp":"1635366120.0","content":"Going with D","comment_id":"289542","poster":"Kian1"},{"poster":"Ebi","content":"Defo D","comment_id":"283339","timestamp":"1635239160.0","upvote_count":"4"},{"poster":"sanjaym","content":"D without any doubt.","upvote_count":"3","timestamp":"1635154140.0","comment_id":"266526"},{"comment_id":"264470","content":"Answer is D. \n\nStoring in EBS is not a cost effective solution and also there can be issues with sizing if the data size is huge. Also, automating using scripts might not be a sustainable solution.","timestamp":"1635018360.0","upvote_count":"1","poster":"halfdeaf"},{"content":"Correct answer is D.\nNeed to make the backup available immediately in the production environment and to be able to restore the service within 24 hrs in DR which is possible to do from Glacier there.","poster":"T14102020","comment_id":"242384","upvote_count":"2","timestamp":"1634991060.0"},{"poster":"jackdryan","timestamp":"1634959200.0","content":"I'll go with D","upvote_count":"3","comment_id":"229414"},{"upvote_count":"1","timestamp":"1634950260.0","content":"Answer is D because of the need to make the backup available immediately in the production environment and to be able to restore the service within 24 hrs in DR which is possible to do from Glacier there.","comment_id":"228553","poster":"Bulti"},{"timestamp":"1634664240.0","upvote_count":"1","comment_id":"226243","content":"Answer is D","poster":"lostri"},{"poster":"Edgecrusher77","upvote_count":"2","timestamp":"1634225640.0","content":"D for sure!\nWhat's wrong with this dump ?? Come on !...","comment_id":"225782"},{"comment_id":"215845","content":"D it is.","poster":"vjt","timestamp":"1634014380.0","upvote_count":"1"},{"content":"D is correct","comment_id":"148955","upvote_count":"2","timestamp":"1633957560.0","poster":"fullaws"},{"timestamp":"1633495200.0","content":"D it is","poster":"learner4ever","upvote_count":"2","comment_id":"145313"},{"upvote_count":"3","timestamp":"1633414620.0","content":"D acceptable","comment_id":"136266","poster":"noisonnoiton"},{"poster":"NikkyDicky","timestamp":"1633240980.0","comment_id":"134219","upvote_count":"1","content":"D most likely"},{"poster":"JAWS1600","timestamp":"1632958560.0","content":"NO way A would work in this case. here is why- Backup to EBS attached to Backup server in \"production environment\". Now if the production environment is down. - You will have to create a backup server in DR, and then restore this EBS snapshot to the backup server. That is lot longer process than what D offers. D is stating to move the data from S3 to Glacier immediately. The restore from Glacier can be expedited which can be availble within 1-5 minutes. Also requirement says data should be available immediately - Yes S3 cross region replication make it availble immediately with asyc replication ( with latency ) . In option A they are copying data nightly , which means data is not availble until the nightly copy is done. D is the right choice.","comment_id":"91440","upvote_count":"2"},{"comment_id":"48229","poster":"wolke89","content":"A is not fully automated I say D","upvote_count":"2","timestamp":"1632766440.0"},{"timestamp":"1632543780.0","comment_id":"45292","poster":"amog","upvote_count":"3","content":"Should be D\n24h in disaster region is RPO, not RTO"},{"poster":"Khanh","content":"D is good but it is not cost effective. I think A is OK","upvote_count":"1","timestamp":"1632503460.0","comment_id":"40570"},{"upvote_count":"4","comment_id":"36820","content":"CONTINUED FROM ABOVE..\nHowever, having said that, this solution is effective only if there is a single EC2 instance that is being backed up as EBS can only be mounted to a single EC2 instance. My argument would be more compelling if answer \"A\" had the backup being to EFS, which can support mounting to multiple EC2 instances, instead of EBS. \n\nAlso, donathon has a good point in that the 16 TiB limit of EBS is also a consideration.\n\nGiven everything, I would say \"D\" is still the best answer but I don't think its as cut and dry as I first thought. \n\nI'd be interested in comments on the factors I mentioned.","timestamp":"1632447480.0","poster":"LunchTime"},{"timestamp":"1632440460.0","comment_id":"36819","poster":"LunchTime","comments":[{"timestamp":"1633114860.0","comment_id":"121639","upvote_count":"2","poster":"Charbelm","content":"\"backup to be available to restore immediately\" means the backup should be available immediately (S3 not Glacier) but it doesn't mean that the restore should be immediate (RTO close to zero)\nFor this, I still go with D"}],"content":"To throw a monkey wrench into things, if you interpret \"backup to be available to restore immediately\" as meaning the RTO must be \"immediate (i.e., as close to zero as possible) then answer \"A\" would meet the requirements best. It's weird to specify the \"backup\" itself needing to be available immediately instead of the RTO being as close to zero as possible (i.e., immediately available).\n\nGoing with that assumption making the RTO as small as possible, backing up to an EBS (i.e., answer \"A\") in the production region would enable you to directly mount the backup onto a new EC2 instance that could replace a failed EC2 production instance. This would be the fastest means to get applications up and running again. If instead, you back up to S3 in the production region you still need to copy that data from S3 to something that can be mounted to an EC2 instance (i.e., either and EBS or EFS volume). \nCONTINUED BELOW...","upvote_count":"1"},{"comments":[{"content":"yeah I would go with D here","timestamp":"1632143100.0","upvote_count":"1","comment_id":"11510","poster":"dpvnme"}],"comment_id":"10788","timestamp":"1632078840.0","poster":"awsec2","content":"why not 'd'","upvote_count":"1"}],"answer_ET":"D","answers_community":["D (100%)"]},{"id":"ahXRU3g9eXFwYjy6OWUX","discussion":[{"upvote_count":"28","timestamp":"1632532320.0","comment_id":"14280","poster":"Moon","content":"I do support Answer \"C\".\nFile gateway is limited by performance its gateway instance, whether EC2 or On-premises, Cache will get filled up fast if not properly configured, For large number of EC2 instances EFS scales better. So, bottom line is File Storage gateway is for legacy applications and you have to add cost of large gateway instances before comparing it to same quantity of EFS storage.\nhttps://www.reddit.com/r/aws/comments/82pyop/storage_gateway_vs_efs/"},{"upvote_count":"8","content":"B\nThis ensures the content stays on-premise but of cause sacrifice the fact that the EC2 will be accessing slight stale data via the S3 bucket because there is a replication time lag. Key is “without delaying the content refresh process”. I assume the content is on premise and the important fact is EFS is private only, it cannot be exposed to public and hence how would EFS be able to update from those “sources”?\nA\\C\\D: This will delay the content refresh process. Because of the propagation delay tied to data traveling over long distances, the network latency of an AWS Direct Connect connection between your on-premises data center and your Amazon VPC can be tens of milliseconds. If your file operations are serialized, the latency of the AWS Direct Connect connection directly impacts your read and write throughput. In essence, the volume of data you can read or write during a period of time is bounded by the amount of time it takes for each read and write operation to complete. https://docs.aws.amazon.com/efs/latest/ug/performance-onpremises.html","poster":"donathon","comments":[{"poster":"cmm103","comment_id":"13479","content":"Yes, B.\nhttps://docs.aws.amazon.com/storagegateway/latest/userguide/GettingStartedAccessFileShare.html","upvote_count":"1","timestamp":"1632185160.0"},{"timestamp":"1632217980.0","upvote_count":"19","content":"Since direct connect already available we can expose EFS to on-premise.\nFrom EFS FAQ's\nQ: How do I access an EFS file system from servers in my on-premises datacenter?\n\nTo access EFS file systems from on-premises, you must have an AWS Direct Connect or AWS VPN connection between your on-premises datacenter and your Amazon VPC.","comment_id":"13961","poster":"leeo"},{"comment_id":"22323","poster":"Frank1","content":"I am not sure how to \"mount the same Storage Gateway bucket to each web server Amazon EC2 instance\". AWS never officially support mounting S3 bucket in EC2","upvote_count":"10","comments":[{"comment_id":"280447","comments":[{"poster":"Hasitha99","timestamp":"1649407260.0","upvote_count":"2","comment_id":"582769","content":"Storage gateway can store data in S3 , Glacier or EBS.\nSo the answer is B.\nRef: https://aws.amazon.com/blogs/storage/cloud-storage-in-minutes-with-aws-storage-gateway/"}],"poster":"shammous","timestamp":"1635238800.0","content":"\"... mount the same Storage Gateway bucket to each web server Amazon EC2\" should ring bells as there is no such thing as \"Storage Gateway bucket\" but \"S3 bucket\". B is not valid, C is.","upvote_count":"4"}],"timestamp":"1632638640.0"},{"upvote_count":"14","content":"C:\nI believe answer \"B\" would be good if there is no \"frequent updates\" from various sources.\nAlso, File Storage Gateway requires physical device to be placed on premises, while the source of updates are from various locations. Therefore, the physical mounting of File Storage Gateway shall be placed on different places \"various Sources\" - as said in the question.","timestamp":"1632538200.0","comments":[{"timestamp":"1636635840.0","upvote_count":"1","content":"It says \"refreshed several times a day from various sources\" not \"frequent updates\". Makes a lot difference.","comment_id":"476215","poster":"Kopa"}],"poster":"Moon","comment_id":"14293"},{"content":"It should be C,\nAs there's Direct Connection between AWS and On-permise, EFS could work here.\nhttps://docs.aws.amazon.com/efs/latest/ug/efs-onpremises.html","poster":"Ibranthovic","timestamp":"1632436260.0","comment_id":"14150","upvote_count":"8"}],"comment_id":"12841","timestamp":"1632183660.0"},{"comment_id":"926218","comments":[{"comment_id":"996483","upvote_count":"1","content":"Should be C","poster":"vn_thanhtung","timestamp":"1693615260.0"}],"timestamp":"1687031160.0","upvote_count":"1","content":"Selected Answer: B\nThe correct answer is B.\n\nThis solution meets all of the requirements:\n\nIt allows the company to scale the web infrastructure up and down in response to load.\nIt keeps the content refresh process running without delay.\nIt is secure and reliable.\nThe other solutions are not as effective.\n\nSolution A would require the company to synchronize the content between the NAS server and the Amazon EBS volume on a regular basis. This could delay the content refresh process.\nSolution C would require the company to expose an Amazon EFS share to on-premises users. This could pose a security risk.\nSolution D would require the company to update the web server instances from the NAS server on a nightly basis. This could delay the content refresh process.","poster":"SkyZeroZx"},{"upvote_count":"3","poster":"JohnPi","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/efs/latest/ug/efs-onpremises.html","timestamp":"1664973840.0","comment_id":"686880"},{"comment_id":"626568","timestamp":"1656853980.0","upvote_count":"1","content":"Selected Answer: B\nvote for B","poster":"aandc"},{"comment_id":"582770","timestamp":"1649407320.0","content":"Selected Answer: B\nStorage gateway can store data in S3 , Glacier or EBS.\nSo the answer is B.\nRef: https://aws.amazon.com/blogs/storage/cloud-storage-in-minutes-with-aws-storage-gateway/","poster":"Hasitha99","upvote_count":"1"},{"upvote_count":"3","poster":"RVD","content":"Selected Answer: C\nAns: C","comment_id":"569430","timestamp":"1647482460.0"},{"content":"C's problem is without mentioning how the data transition/migration.\nB does mention \"replicate content to AWS\".\n\nQuestion asked \"transition its online infrastructure to AWS without causing a delay in the process of content refreshment\".\nSo Storage Gateway is way to go as my understanding.","poster":"sTeVe86","upvote_count":"3","comment_id":"543818","timestamp":"1644416340.0"},{"poster":"AzureDP900","comment_id":"507299","timestamp":"1640198220.0","upvote_count":"1","content":"Answer is C"},{"poster":"cldy","timestamp":"1638772980.0","upvote_count":"1","comment_id":"494946","content":"C. Expose an Amazon EFS share to on-premises users to serve as the NAS serve. Mount the same EFS share to the web server Amazon EC2 instances to serve the content."},{"timestamp":"1638634980.0","content":"C is right","poster":"AzureDP900","upvote_count":"1","comment_id":"493792"},{"comment_id":"484876","upvote_count":"1","content":"I'd go with C. with DX EFS can be accessed from on premises","timestamp":"1637658300.0","poster":"backfringe"},{"timestamp":"1636299600.0","content":"It's C","upvote_count":"1","poster":"andylogan","comment_id":"450617"},{"comment_id":"440867","content":"Answer is C\n\nWhy not B? EFS will reflect the changes to files immediately during the day as opposed to File Gateway which could be slower. EFS will also scale up/down nicely. There no cost restriction in the question.","upvote_count":"2","poster":"student22","timestamp":"1636184100.0"},{"poster":"AWS_Noob","timestamp":"1636028820.0","content":"Looking at the question , it says a Centralized File share to multiple web servers. EFS does this","upvote_count":"2","comment_id":"430243"},{"upvote_count":"1","content":"C Correct","poster":"Akhil254","comment_id":"406639","timestamp":"1635863760.0"},{"comment_id":"362413","timestamp":"1635761280.0","content":"Important phrase in the question \"without delaying the content refresh process\". But the phrase \"scale resources up and down in response to load\" kind of tricks to forcefully think for ELB & Auto scaling. The actual point is to refresh the content without delay you need a shared file server mount point for both On-prem and AWS. which EFS can prove. \nCorrect Answer is C","upvote_count":"2","poster":"Radhaghosh"},{"content":"C for sure","upvote_count":"5","timestamp":"1635693780.0","poster":"Waiweng","comment_id":"343959"},{"timestamp":"1635512880.0","upvote_count":"2","content":"will go for C","poster":"Kian1","comment_id":"289548"},{"timestamp":"1635451320.0","comment_id":"284939","upvote_count":"3","poster":"bnagaraja9099","content":"I'll go with C. The First part of option B is right. But second part of Option B is not right. So going with C."},{"upvote_count":"3","comment_id":"283340","content":"C is the only correct answer in here","poster":"Ebi","timestamp":"1635318060.0"},{"comment_id":"266530","comments":[{"timestamp":"1635148500.0","comments":[{"poster":"rcher","comment_id":"276604","comments":[{"comment_id":"399953","content":"Please try to think twice before belittling someone. He was asking how in case of C the existing data in NAS will end up in EFS. Option B addresses that, C - not.","poster":"vimgoru24","upvote_count":"1","timestamp":"1635783180.0"}],"upvote_count":"2","timestamp":"1635176580.0","content":"Please try to google yourself next time.\n\nEFS are accessible within a VPC (VPC resource , albeit exposed with an ENI), and with a DX connected, you will be able to access it from your on-prem network\n\nhttps://docs.aws.amazon.com/efs/latest/ug/efs-onpremises.html"}],"comment_id":"270273","poster":"Sourabh1703","content":"There is no explanation in C on how data will be available from on-premise , that make me believe B is better.","upvote_count":"1"}],"timestamp":"1635144180.0","upvote_count":"1","content":"I'll go with C","poster":"sanjaym"},{"comment_id":"242389","upvote_count":"1","content":"Correct answer is C. File gateway wasn't support auto refresh.","poster":"T14102020","timestamp":"1635111420.0"},{"comment_id":"229420","timestamp":"1635100320.0","upvote_count":"2","content":"I'll go with C","poster":"jackdryan"},{"timestamp":"1635075240.0","content":"This question might be outdate. \nFor B, file gateway wasn't support auto refresh, but from July 2020, it does support cache refresh from 5min to 30 days refresh cache. \nFor C, there is no concern about update content.\nConsider the exam has been there for more than a year, I would choose C for sure if I met this question.","poster":"ting_66","upvote_count":"1","comment_id":"228561"},{"timestamp":"1635034740.0","content":"C is the correct answer. The key is to refresh the content in AWS without delay. Although B will do the job, replication will introduce delay when using File Storage Gateway. On the other hand with C, all sources will directly write the data to the EFS mount which is also mounted on individual EC2 instances. The questions says the solution should have the ability to auto-scale. It doesn't mention that the solution requires auto-scaling. Hence C is still correct as the solution can be auto-scaled using ASG in the future if there is a need to.","poster":"Bulti","upvote_count":"5","comment_id":"228560"},{"timestamp":"1635029460.0","content":"Answer is B","upvote_count":"1","poster":"lostri","comment_id":"226242"},{"comment_id":"225788","timestamp":"1634884020.0","poster":"Edgecrusher77","upvote_count":"1","content":"C is the best option, but doesn't adress the scalability of the web tier, which adressed by A... And A doesn't adress the files content / NAS properly..."},{"comment_id":"220488","timestamp":"1634665800.0","poster":"SamuelK","upvote_count":"1","content":"B is the correct answer"},{"poster":"vjt","upvote_count":"1","timestamp":"1634518020.0","content":"The issue with file gateway using AWS Storage Gateway is that you wiould run into performance issues once the local cache fills up and then the application has to source the data from S3 which is the underlying object storage. this architecture wont be much scalable in comparison to just mounting EFS directly. \nC is correct for sure.","comment_id":"215851"},{"timestamp":"1634379060.0","comment_id":"206264","upvote_count":"1","comments":[{"content":"There is an another article I found.\nhttps://docs.aws.amazon.com/efs/latest/ug/how-it-works.html#how-it-works-direct-connect","poster":"porlarowl","upvote_count":"1","comment_id":"206266","timestamp":"1634404260.0"}],"poster":"porlarowl","content":"I thought the answer is B, but I found a article below titled 'Amazon EFS Update – On-Premises Access via Direct Connect'.\nSo the answer is may be C.\nhttps://aws.amazon.com/ko/blogs/aws/amazon-efs-update-on-premises-access-via-direct-connect-vpc/"},{"content":"Content refreshed several times at day.... then u can only work at night. Answer \"D\".","upvote_count":"2","comment_id":"201072","poster":"Joanale","timestamp":"1634368560.0"},{"timestamp":"1634214780.0","poster":"Neive","content":"A = Incorrect, EBS is strictly no.\nB = Incorrect. “On the AWS side, mount the same Storage Gateway bucket to each web server Amazon EC2 instance to serve the content” - It is technically not possible to mount the s3 bucket where file gateway storing the files.\nC= Correct. This is technically possible.\nD = Incorrect. Files on the NAS is refreshed several times a day. With this option infra on AWS will have stale data.","comments":[{"comment_id":"185701","upvote_count":"1","timestamp":"1634333760.0","poster":"sk2022","content":"“On the AWS side, mount the same Storage Gateway bucket to each web server Amazon EC2 instance to serve the content” - It is technically not possible to mount the s3 bucket where file gateway storing the files.\n\nWhy? This is a perfectly valid option."}],"comment_id":"172026","upvote_count":"3"},{"poster":"fullaws","upvote_count":"2","timestamp":"1634143680.0","comment_id":"148964","content":"C is the correct answer, as they are moving to AWS, there is not needed for the company to spend money to increase the size of their gateway instance just to migrate those data to prevent content delay. The situation also felt well as they already had direct connect. EFS will satisfied."},{"timestamp":"1634076000.0","poster":"learner4ever","comment_id":"145314","content":"C it is","upvote_count":"1"},{"upvote_count":"1","timestamp":"1634072820.0","poster":"noisonnoiton","comment_id":"136898","content":"C acceptable"},{"upvote_count":"1","comment_id":"134225","timestamp":"1634041740.0","content":"C for sure","poster":"NikkyDicky"},{"comment_id":"98606","upvote_count":"1","poster":"JAWS1600","content":"C. close choices are B and C. B is wrong -\"mount same storage bucket\" . You cannot mount a stoarge bucket. You can only access a storage bucket or mount a NFS. Tricky question","timestamp":"1633852860.0"},{"timestamp":"1633824420.0","upvote_count":"2","poster":"JohnyGaddar","comment_id":"96471","content":"A - ebs cannot be attached to multiple instances\n B- file gateway is limited by performance its gateway instance. EFS scales better \n D - will create delay \n C - correct ( https://docs.aws.amazon.com/efs/latest/ug/efs-onpremises.html)"},{"comment_id":"89097","timestamp":"1633485660.0","upvote_count":"1","content":"Storage Gateway Bucket? there is no such thing.It it said Storage getway file share then ok....but you cant mount a bucket. Answer has to be C","poster":"Merlin1"},{"content":"The answer is C. \nhttps://aws.amazon.com/blogs/aws/amazon-efs-update-on-premises-access-via-direct-connect-vpc/\n To use this feature for migration, you simply attach an EFS file system to your on-premises servers, copy your data to it, and then process it in the cloud as desired, leaving your data in AWS for the long term.","poster":"fw","timestamp":"1633483440.0","upvote_count":"3","comment_id":"77633"},{"timestamp":"1633400460.0","content":"C is the proper recommended way but then where is the old data? Is it already migrated to EFS? Option B's mounting bucket doesn't make sense.","upvote_count":"1","comment_id":"75880","poster":"Smart"},{"timestamp":"1633379160.0","comment_id":"75578","poster":"Joeylee","content":"C would be the answer since direct connect has been provided","upvote_count":"1"},{"timestamp":"1633320600.0","comment_id":"70195","content":"One important requirement has not been taken care so far which is to scale up and down the resources and these resources are EC2 instances and this is only possible by using ASG with option D.","poster":"hammad","upvote_count":"1"},{"timestamp":"1633049280.0","poster":"Jshuen","content":"I will support C also, because file gateway needs to be setup with an instance as host platform, and cannot mount directly to the S3 bucket, then will eliminate the choice of B","comment_id":"59103","upvote_count":"2"},{"timestamp":"1633007940.0","comment_id":"57688","poster":"mandrakenet","upvote_count":"4","content":"C is correct\nD: \"mount the same Storage Gateway bucket\" you cannot \"mount\" s3 on EC2"},{"poster":"amog","comment_id":"45293","content":"Support C","timestamp":"1632994560.0","upvote_count":"2"},{"content":"support B","upvote_count":"1","poster":"markpark","timestamp":"1632984060.0","comment_id":"41239"},{"content":"Just for completeness of my comments, answer \"A\" is not valid as an EBS volume cannot be mounted to more then one EC2 instance at a time (you need to use EFS instead). Answer \"D\" is not correct as using a nightly update process would delay the content refresh process, which is a no-no based on the requirements.","upvote_count":"1","poster":"LunchTime","timestamp":"1632902820.0","comments":[{"upvote_count":"2","timestamp":"1633882020.0","comment_id":"118643","content":"Given the additional comments, I've switch my opinion to C being correct instead of B.","poster":"LunchTime"}],"comment_id":"37170"},{"timestamp":"1632878460.0","comment_id":"37169","content":"CONTINUE FROM ABOVE...\n\nHowever, if my hypothesis is wrong and instead ALL of the application tiers can be moved to AWS then \"C\" would be a better answer. That would eliminate all elements of the application from being on-premise. The feeds to the application from \"other sources\" would just connect to AWS regardless whether those sources were on-premise or not. Answer \"C\" then provides the cleanest and most performance architecture as there is no replication of data since the \"sources\" are writing directly to EFS. Answer \"B\" (i.e., AWS Storage Gateway) would still work but would not be as performance.","poster":"LunchTime","upvote_count":"3"},{"timestamp":"1632862500.0","upvote_count":"3","content":"CONTINUED FROM ABOVE...\n\nUsers is different from \"various sources\" as in the question stating \"...the content is refreshed several times a day from VARIOUS SOURCES...\" So I don't believe \"users\" in answer \"C\" refers to the systems that are providing the data from the \"various sources\". Consequently, I believe the question is stating that the refresh process is NOT to be changed in the migration to AWS. \n\nIf this thought process is correct, then answer \"C\" would also mean the NAS Server is still in place on-premise and being updated with the data from various sources. You would then also need a mechanism to synchronize the on-premise NAS server with the new EFS NAS server sitting in AWS. That would not be the cleanest architecture and \"B\" would be a better choice as AWS Storage Gateway automatically takes care of the synchronization with AWS.\nCONTINUED BELOW...","comment_id":"37168","poster":"LunchTime"},{"poster":"LunchTime","timestamp":"1632802920.0","content":"I support answer B. Although its an EXTREMELY tough decision between B & C, as the other comments indicate.\n\nMy reasoning for leaning towards B instead of C is as follows: Firstly, I'm interpreting the wording \"migrate the web infrastructure\" and \"without DELAYING THE CONTENT REFRESH\" in the question statement \"How can the company MIGRATE THE WEB INFRASTRUCTURE to AWS WITHOUT DELAYING THE CONTENT REFRESH process?\" to infer that only the web tier and application tier are being migrated to AWS and NOT the data tier. If all the tiers were to be migrated to AWS, why would they mention the \"without delaying the content refresh process\"? The wording of \"without delaying the content refresh process\" would not be needed as that requirement would be inferred, I would think. That means we are looking at a hybrid solution. Obviously, this is debatable. I feel that the wording of answer \"C\" supports this hypothesis via using the wording \"Exposing an Amazon EFS share to on-premise USERS...\". \nCONTINUE BELOW...","comment_id":"37166","upvote_count":"2"},{"content":"Answer is C.","upvote_count":"2","comment_id":"34794","poster":"arunkumar","timestamp":"1632737040.0"},{"comment_id":"31129","content":"Support B\nIts possible, See URL\nhttps://docs.aws.amazon.com/storagegateway/latest/userguide/create-gateway-file.html","upvote_count":"2","timestamp":"1632664860.0","poster":"dojo"},{"content":"C is the correct answer","poster":"HazemYousry","timestamp":"1632153660.0","upvote_count":"2","comment_id":"12551"},{"poster":"awsec2","timestamp":"1632151140.0","content":"c should be","upvote_count":"5","comment_id":"10758"}],"question_id":397,"topic":"1","exam_id":32,"url":"https://www.examtopics.com/discussions/amazon/view/5108-exam-aws-certified-solutions-architect-professional-topic-1/","answer":"C","isMC":true,"answers_community":["C (67%)","B (33%)"],"question_text":"A company has an existing on-premises three-tier web application. The Linux web servers serve content from a centralized file share on a NAS server because the content is refreshed several times a day from various sources. The existing infrastructure is not optimized and the company would like to move to AWS in order to gain the ability to scale resources up and down in response to load. On-premises and AWS resources are connected using AWS Direct Connect.\nHow can the company migrate the web infrastructure to AWS without delaying the content refresh process?","timestamp":"2019-09-12 12:29:00","question_images":[],"choices":{"C":"Expose an Amazon EFS share to on-premises users to serve as the NAS serve. Mount the same EFS share to the web server Amazon EC2 instances to serve the content.","D":"Create web server Amazon EC2 instances on AWS in an Auto Scaling group. Configure a nightly process where the web server instances are updated from the NAS server.","B":"Create an on-premises file gateway using AWS Storage Gateway to replace the NAS server and replicate content to AWS. On the AWS side, mount the same Storage Gateway bucket to each web server Amazon EC2 instance to serve the content.","A":"Create a cluster of web server Amazon EC2 instances behind a Classic Load Balancer on AWS. Share an Amazon EBS volume among all instances for the content. Schedule a periodic synchronization of this volume and the NAS server."},"answer_images":[],"answer_description":"","unix_timestamp":1568284140,"answer_ET":"C"},{"id":"3l66y5V1RH8nLpriyOf9","discussion":[{"comments":[{"timestamp":"1633577220.0","upvote_count":"1","poster":"pra276","comments":[{"upvote_count":"1","poster":"LunchTime","timestamp":"1633781340.0","comment_id":"37220","content":"The example pra276 provides does not support answer B as the example requires the use of Kinesis in addition to CloudWatch Log Streams."},{"content":"B is incorrect because of CW Event, which is irrelevant.","comments":[{"timestamp":"1651943160.0","poster":"user0001","content":"for real-time Kinesis Data Streams","upvote_count":"1","comment_id":"598217"}],"timestamp":"1635694320.0","upvote_count":"6","poster":"ppshein","comment_id":"345396"}],"comment_id":"22888","content":"See this for answer B: \nhttps://aws.amazon.com/blogs/architecture/stream-amazon-cloudwatch-logs-to-a-centralized-account-for-audit-and-analysis/"},{"timestamp":"1665121860.0","upvote_count":"1","comments":[{"timestamp":"1669673400.0","comment_id":"729647","content":"from the link below. Forwarding CW events to other accounts can achieve near real time \nhttps://aws.amazon.com/blogs/aws/new-cross-account-delivery-of-cloudwatch-events/","upvote_count":"1","poster":"heany"}],"poster":"heany","comment_id":"688351","content":"Should be B. You will need data stream to send logs cross accounts. But for event, you can send directly to another account. https://medium.com/version-1/centralised-logs-and-alarms-from-multiple-aws-accounts-e8ef02750340"}],"upvote_count":"40","timestamp":"1633403160.0","poster":"donathon","content":"C\nThe solution uses Amazon Kinesis Data Streams and a log destination to set up an endpoint in the logging account to receive streamed logs and uses Amazon Kinesis Data Firehose to deliver log data to the Amazon Simple Storage Solution (S3) bucket. Application accounts will subscribe to stream all (or part) of their Amazon CloudWatch logs to a defined destination in the logging account via subscription filters.\nhttps://aws.amazon.com/blogs/architecture/central-logging-in-multi-account-environments/\nA: Does not satisfy “near real time”.","comment_id":"12842"},{"content":"I support answer \"C\".\nC: the solution is having a proper logging steps:\nCloudWatch (application Account) --> Kinesis Data Stream (Logging Account) --> Kinesis Firehose (Logging Account) --> S3 (Logging Account)","upvote_count":"28","comment_id":"14276","poster":"Moon","timestamp":"1633515720.0"},{"poster":"SkyZeroZx","content":"Selected Answer: C\n\"subscribe an Amazon Kinesis Data Firehose stream to Amazon CloudWatch Events\" in letter B is more complex and incorrect \nBecause letter C is more apropiate","upvote_count":"1","comment_id":"926228","timestamp":"1687031760.0"},{"poster":"BKhan","upvote_count":"1","timestamp":"1675086120.0","comment_id":"792875","content":"Selected Answer: C\nShould be C"},{"comment_id":"494908","timestamp":"1638767100.0","upvote_count":"2","poster":"cldy","content":"C. Create Amazon Kinesis Data Streams in the logging account, subscribe the stream to CloudWatch Logs streams in each application AWS account, configure an Amazon Kinesis Data Firehose delivery stream with the Data Streams as its source, and persist the log data in an Amazon S3 bucket inside the logging AWS account."},{"upvote_count":"2","poster":"AzureDP900","comment_id":"493796","timestamp":"1638635400.0","content":"real time is keyword here and I will go with C"},{"poster":"backfringe","upvote_count":"2","content":"I'd go with C\nCloudWatch (application Account) --> Kinesis Data Stream (Logging Account) --> Kinesis Firehose (Logging Account) --> S3 (Logging Account)","comment_id":"484021","timestamp":"1637572440.0"},{"content":"Selected Answer: C\nit's C. Near real-time = Amazon Kinesis Data Streams","comment_id":"483862","poster":"fadhilmukh","upvote_count":"2","timestamp":"1637555820.0"},{"poster":"andylogan","comment_id":"450618","upvote_count":"1","content":"It's C","timestamp":"1636232160.0"},{"comment_id":"409513","timestamp":"1635965280.0","upvote_count":"1","poster":"WhyIronMan","content":"I'll go with C"},{"timestamp":"1635793320.0","comment_id":"362417","poster":"Radhaghosh","content":"For Centralized login you need Kinesis Data Stream. (using CloudWatch Destination) \nhttps://aws.amazon.com/solutions/implementations/centralized-logging/\n\nCorrect Answer is C","upvote_count":"4"},{"content":"Go for C","upvote_count":"3","timestamp":"1635685620.0","comment_id":"343964","poster":"Waiweng"},{"timestamp":"1635675120.0","comment_id":"289552","poster":"Kian1","content":"going with C","upvote_count":"2"},{"poster":"LB","timestamp":"1635667200.0","upvote_count":"1","content":"https://aws.amazon.com/blogs/architecture/stream-amazon-cloudwatch-logs-to-a-centralized-account-for-audit-and-analysis/","comment_id":"287865"},{"upvote_count":"6","content":"Answer is C","comment_id":"283342","timestamp":"1635584340.0","poster":"Ebi"},{"comment_id":"266550","poster":"sanjaym","content":"I'll go with C","timestamp":"1635539700.0","upvote_count":"1"},{"content":"Correct answer is C.\nCloudWatch (application Account) --> Kinesis Data Stream (Logging Account) --> Kinesis Firehose (Logging Account) --> S3 (Logging Account)","upvote_count":"1","timestamp":"1635474420.0","comment_id":"242394","poster":"T14102020"},{"upvote_count":"2","comment_id":"229422","poster":"jackdryan","timestamp":"1635319680.0","content":"I'll go with C"},{"upvote_count":"6","content":"Correct answer is C. You will need to create a log destination pointing to the kinesis data stream as the target and then use that destination in the subscription filter of the CloudWatch log streams in the application account to flow the events from the application account to the logging account in near real-time. B is not possible as CloudWatch log stream cannot be a destination of a subscription filter on another CloudWatch log stream. Only supported destination is Kinesis Data Stream or Kinesis Data Firehouse or Lambda.","poster":"Bulti","timestamp":"1635022740.0","comment_id":"228634"},{"comment_id":"215853","upvote_count":"1","timestamp":"1634740620.0","poster":"vjt","content":"C for sure."},{"upvote_count":"2","timestamp":"1634666160.0","comment_id":"148969","poster":"fullaws","content":"C is correct"},{"poster":"learner4ever","comment_id":"145300","content":"C it is","upvote_count":"2","timestamp":"1634630760.0"},{"comment_id":"136901","content":"C acceptable","timestamp":"1634294580.0","poster":"noisonnoiton","upvote_count":"1"},{"timestamp":"1634252760.0","comment_id":"134229","poster":"NikkyDicky","content":"C makes sense","upvote_count":"1"},{"upvote_count":"1","timestamp":"1634016060.0","comments":[{"poster":"Smart","timestamp":"1634017440.0","content":"Option D is where Agents are publishing events directly to Kinesis - which doesn't happen! Alternatively, I was thinking if agents can put data into another account's CW. (https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Agent-common-scenarios.html#CloudWatch-Agent-send-to-different-AWS-account)","comment_id":"84376","upvote_count":"2"},{"content":"Correction. The choice is C.","upvote_count":"1","timestamp":"1634139720.0","comment_id":"85327","poster":"fw"}],"poster":"fw","content":"My choice is D.\nCloudwatch logs -> Kinesis Firehose -> Lambda to process -> Kinesis Firehose -> S3 is a logical workflow for this.","comment_id":"82606"},{"timestamp":"1633984260.0","comment_id":"45296","poster":"amog","content":"Should be C","upvote_count":"2"},{"upvote_count":"3","timestamp":"1633840080.0","poster":"LunchTime","comment_id":"37223","content":"I agree that \"C\" is the correct answer.\nA: The export being \"hourly\" does not meet the \"near real-time\" requirement.\nB: CloudWatch log streams won’t work since Kinesis streams are currently the only resource supported as a destination for cross-account subscriptions as per https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs//CrossAccountSubscriptions.html.\nC: Kinesis streams are currently the only resource supported as a destination for cross-account subscriptions as per the same reference as answer \"B\".\nD: Same problem as answer \"B\" has."},{"comment_id":"34797","content":"C is the answer.\n\nhttps://aws.amazon.com/blogs/architecture/central-logging-in-multi-account-environments/","upvote_count":"1","poster":"arunkumar","timestamp":"1633758000.0"},{"comments":[{"upvote_count":"3","poster":"dpvnme","content":"the reference talks about C","comment_id":"11513","timestamp":"1633100760.0"}],"poster":"Xiaoyao2000","timestamp":"1633042140.0","comment_id":"11145","content":"B\nhttps://aws.amazon.com/blogs/architecture/central-logging-in-multi-account-environments/","upvote_count":"2"},{"upvote_count":"5","poster":"awsec2","content":"why not c","comment_id":"10760","timestamp":"1632493860.0"}],"question_id":398,"question_text":"A company has multiple AWS accounts hosting IT applications. An Amazon CloudWatch Logs agent is installed on all Amazon EC2 instances. The company wants to aggregate all security events in a centralized AWS account dedicated to log storage.\nSecurity Administrators need to perform near-real-time gathering and correlating of events across multiple AWS accounts.\nWhich solution satisfies these requirements?","topic":"1","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/5109-exam-aws-certified-solutions-architect-professional-topic-1/","answer_ET":"C","isMC":true,"answer_images":[],"exam_id":32,"answers_community":["C (100%)"],"unix_timestamp":1568284500,"timestamp":"2019-09-12 12:35:00","choices":{"A":"Create a Log Audit IAM role in each application AWS account with permissions to view CloudWatch Logs, configure an AWS Lambda function to assume the Log Audit role, and perform an hourly export of CloudWatch Logs data to an Amazon S3 bucket in the logging AWS account.","D":"Configure CloudWatch Logs agents to publish data to an Amazon Kinesis Data Firehose stream in the logging AWS account, use an AWS Lambda function to read messages from the stream and push messages to Data Firehose, and persist the data in Amazon S3.","B":"Configure CloudWatch Logs streams in each application AWS account to forward events to CloudWatch Logs in the logging AWS account. In the logging AWS account, subscribe an Amazon Kinesis Data Firehose stream to Amazon CloudWatch Events, and use the stream to persist log data in Amazon S3.","C":"Create Amazon Kinesis Data Streams in the logging account, subscribe the stream to CloudWatch Logs streams in each application AWS account, configure an Amazon Kinesis Data Firehose delivery stream with the Data Streams as its source, and persist the log data in an Amazon S3 bucket inside the logging AWS account."},"question_images":[],"answer":"C"},{"id":"AC4BnmtzhOmrhbd7Bklm","unix_timestamp":1581534600,"answers_community":["D (100%)"],"choices":{"B":"A web tier deployed across 3 AZs with 2 EC2 (Elastic Compute Cloud) instances in each AZ inside an Auto Scaling Group behind an ELB (elastic load balancer) and an application tier deployed across 3 AZs with 2 EC2 instances in each AZ inside an Auto Scaling Group behind an ELB and one RDS (Relational Database Service) Instance deployed with read replicas in the two other AZs.","D":"A web tier deployed across 3 AZs with 2 EC2 (Elastic Compute Cloud) instances in each AZ Inside an Auto Scaling Group behind an ELB (elastic load balancer). And an application tier deployed across 3 AZs with 2 EC2 instances in each AZ inside an Auto Scaling Group behind an ELB and a Multi-AZ RDS (Relational Database services) deployment.","A":"A web tier deployed across 2 AZs with 3 EC2 (Elastic Compute Cloud) instances in each AZ inside an Auto Scaling Group behind an ELB (elastic load balancer), and an application tier deployed across 2 AZs with 3 EC2 instances in each AZ inside an Auto Scaling Group behind an ELB and one RDS (Relational Database Service) instance deployed with read replicas in the other AZ.","C":"A web tier deployed across 2 AZs with 3 EC2 (Elastic Compute Cloud) instances in each AZ inside an Auto Scaling Group behind an ELB (elastic load balancer) and an application tier deployed across 2 AZs with 3 EC2 instances m each AZ inside an Auto Scaling Group behind an ELS and a Multi-AZ RDS (Relational Database Service) deployment."},"topic":"1","answer_ET":"D","isMC":true,"question_text":"Your company runs a customer facing event registration site This site is built with a 3-tier architecture with web and application tier servers and a MySQL database The application requires 6 web tier servers and 6 application tier servers for normal operation, but can run on a minimum of 65% server capacity and a single MySQL database.\nWhen deploying this application in a region with three availability zones (AZs) which architecture provides high availability?","question_id":399,"answer_images":[],"answer":"D","timestamp":"2020-02-12 20:10:00","exam_id":32,"question_images":[],"answer_description":"","discussion":[{"content":"Yes, I think we can consider what remains if we loose one AZ, that is you retrieve your 65% capacity, so D seems correct","upvote_count":"12","poster":"virtual","comments":[{"content":"Agree, this is the differentiating point from C. \nD is correct.","poster":"mnsait","comment_id":"1309880","upvote_count":"1","timestamp":"1731310380.0"}],"comment_id":"54295","timestamp":"1632570420.0"},{"content":"Selected Answer: D\nD.\nHigh availability == Multi-AZ RDS (not read replicas - C)","poster":"student22","comment_id":"1307227","upvote_count":"1","timestamp":"1730788920.0"},{"upvote_count":"1","comment_id":"1266510","content":"D. A web tier deployed across 3 AZs with 2 EC2 (Elastic Compute Cloud) instances in each AZ Inside an Auto Scaling Group behind an ELB (elastic load balancer). And an application tier deployed across 3 AZs with 2 EC2 instances in each AZ inside an Auto Scaling Group behind an ELB and a Multi-AZ RDS (Relational Database services) deployment.","poster":"amministrazione","timestamp":"1723734120.0"},{"comment_id":"1017955","upvote_count":"1","comments":[{"comment_id":"1309879","content":"Note that 65% of minimum capacity is required. So with 2 AZ, you lose 50% of the capacity in case one AZ goes down. \n\nTherefore, answer is D.","timestamp":"1731310320.0","poster":"mnsait","upvote_count":"1"}],"content":"Option C is the correct answer, it has covered high availibility and Multi-AZ RDS. I am not sure why most of the people mentioned D as correct answer, a good architect will always thing for the solution which very cost effective as well as less complex and solve the purpose. Option A, Option B, and Option D do provide some level of redundancy and load balancing, but they don't spread both the web and application tiers across AZs. Option B and Option D spread the web tier across all three AZs, which might not be necessary for this architecture and could increase costs without a significant increase in availability. Option A lacks Multi-AZ RDS for the database, which is crucial for database availability in the event of a failure.","timestamp":"1695743700.0","poster":"rockyogi12345"},{"comment_id":"901534","content":"I think b is the correct answer","poster":"rtguru","timestamp":"1684449000.0","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: D\nD is the right answer","poster":"Bill_Wiiliam","timestamp":"1657191660.0","comment_id":"628326"},{"upvote_count":"1","timestamp":"1640278500.0","content":"high availability = Multi-AZ ; D","comment_id":"508049","poster":"vbal"},{"comment_id":"497766","content":"D. A web tier deployed across 3 AZs with 2 EC2 (Elastic Compute Cloud) instances in each AZ Inside an Auto Scaling Group behind an ELB (elastic load balancer). And an application tier deployed across 3 AZs with 2 EC2 instances in each AZ inside an Auto Scaling Group behind an ELB and a Multi-AZ RDS (Relational Database services) deployment.","timestamp":"1639057800.0","poster":"cldy","upvote_count":"1"},{"upvote_count":"2","poster":"acloudguru","timestamp":"1637741580.0","content":"Selected Answer: D\nto operate at 65%, have 3 AZs with 2 instances each + RDS with Multi-AZ.","comment_id":"485728"},{"comment_id":"406217","content":"D Correct","timestamp":"1636044420.0","poster":"Akhil254","upvote_count":"1"},{"content":"How is availability calculated?","comment_id":"346239","comments":[{"upvote_count":"1","poster":"01037","timestamp":"1635888540.0","content":"I'll chiose D though","comment_id":"346240"}],"timestamp":"1635754200.0","poster":"01037","upvote_count":"1"},{"content":"D for sure","comment_id":"346237","poster":"01037","upvote_count":"1","timestamp":"1634954400.0"},{"poster":"cldy","comment_id":"324690","upvote_count":"1","timestamp":"1634907480.0","content":"D.\nto operate at 65%, have 3 AZs with 2 instances each + RDS with Multi-AZ."},{"poster":"srknbngl","comment_id":"190284","upvote_count":"1","timestamp":"1634787540.0","content":"D is correct"},{"comment_id":"131096","content":"go with D","poster":"noisonnoiton","timestamp":"1634633400.0","upvote_count":"1"},{"content":"You can follow this link https://aws.amazon.com/rds/features/multi-az/ and scroll down to this section \"Multi-AZ deployments, multi-region deployments, and read replicas\" to confirm the main purpose for each of them. So, D is a right choice.","upvote_count":"1","timestamp":"1634493240.0","poster":"AWSChia","comment_id":"126609"},{"timestamp":"1634429340.0","poster":"AWSChia","upvote_count":"1","content":"D is correct. Since you must have \"...a minimum of 65% server capacity and a single database\"","comment_id":"126591"},{"timestamp":"1634400840.0","comment_id":"116952","content":"D: Read Replica does not create High Availability where as Multi-AZ RDS.","upvote_count":"2","poster":"manoj101"},{"upvote_count":"1","content":"I will go with D","comment_id":"109325","poster":"KwasiB","timestamp":"1634249460.0"},{"comment_id":"96963","upvote_count":"1","timestamp":"1633966800.0","poster":"JAWS1600","content":"Reading it again. I think answer is A. \nThis is what changed my mind. They are sending tapes to offsite. So we need to find a solution which replaces tapes. A is the only option which provides Glacier, which is alternative to tape solution as per AWS documentation.\nYou can directly backup to glacier using API calls ( not via console). See the following console\nhttps://www.msp360.com/resources/blog/compare-amazon-glacier-direct-upload-and-glacier-upload-through-amazon-s3/"},{"poster":"JAWS1600","upvote_count":"2","content":"It is B. Yes I agreed with option D , it is using Multi AZ, but that leverages only two AZs. If we loose both these AZs, The DB layer is gone. With Option B, it is using Read replica. RR can be promoted to Active DB, in case 1 or 2 AZs go down.","comment_id":"94004","timestamp":"1633265220.0"},{"upvote_count":"3","content":"D:\nA web tier deployed across 3 AZs with 2 EC2 (Elastic Compute Cloud) instances in each AZ Inside an Auto \nScaling Group behind an ELB (elastic load balancer). And an application tier deployed across 3 AZs with 2 EC2 \ninstances In each AZ inside an Auto Scaling Group behind an ELB. And a Multi-AZ RDS (Relational Database \nservices) deployment.\n\nWhy every question has broken answer?","comment_id":"75184","poster":"Joeylee","timestamp":"1632925080.0"},{"comments":[{"comment_id":"68608","content":"I think it is because D is using Multi-AZ and B is using read replicas.","upvote_count":"5","poster":"kakashi","timestamp":"1632691680.0"}],"upvote_count":"3","content":"Is D Correct?","comment_id":"49669","timestamp":"1632473520.0","poster":"BillyC"}],"url":"https://www.examtopics.com/discussions/amazon/view/13939-exam-aws-certified-solutions-architect-professional-topic-1/"},{"id":"lS4uWM0Wnk8qiMa9ADv1","url":"https://www.examtopics.com/discussions/amazon/view/5158-exam-aws-certified-solutions-architect-professional-topic-1/","answer_images":[],"discussion":[{"comment_id":"12843","content":"B\nhttps://aws.amazon.com/about-aws/whats-new/2017/11/aws-lambda-supports-traffic-shifting-and-phased-deployments-with-aws-codedeploy/","upvote_count":"31","timestamp":"1632358020.0","poster":"donathon"},{"content":"I support answer \"B\".\nSAM support CodeDeploy, which can be used for Lambda Core versioning.","poster":"Moon","timestamp":"1632406860.0","upvote_count":"14","comment_id":"14269"},{"timestamp":"1687377720.0","content":"Selected Answer: B\nB, SAM is typical solution, simple question, hope I can have it in my exam","upvote_count":"1","poster":"SkyZeroZx","comment_id":"929881"},{"upvote_count":"1","poster":"mimadour21698","timestamp":"1683836640.0","comment_id":"895396","content":"Selected Answer: B\nB is the answer."},{"poster":"evargasbrz","timestamp":"1672154880.0","content":"Selected Answer: B\nYou can set traffic shifting using the AWS Management Console, AWS CLI, and SDKs. This feature is available in all regions that support Lambda.\n\nAWS CodeDeploy leverages Lambda’s traffic shifting capabilities to automate the gradual rollout of new function versions.\nhttps://aws.amazon.com/about-aws/whats-new/2017/11/aws-lambda-supports-traffic-shifting-and-phased-deployments-with-aws-codedeploy/","upvote_count":"1","comment_id":"758709"},{"upvote_count":"1","content":"Selected Answer: B\nVote for B","timestamp":"1666426320.0","comment_id":"701408","poster":"SangVu"},{"comment_id":"542618","poster":"jj22222","content":"Selected Answer: B\nB looks right","timestamp":"1644260640.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1642068660.0","content":"Selected Answer: B\nSAM is just better.","poster":"pititcu667","comment_id":"522769"},{"content":"B is correct.","timestamp":"1640866980.0","poster":"cldy","upvote_count":"1","comment_id":"513346"},{"comment_id":"499311","poster":"cldy","timestamp":"1639217520.0","upvote_count":"1","content":"B. Use AWS SAM and built-in AWS CodeDeploy to deploy the new Lambda version, gradually shift traffic to the new version, and use pre-traffic and post-traffic test functions to verify code. Rollback if Amazon CloudWatch alarms are triggered."},{"upvote_count":"1","comment_id":"493798","content":"B is right","poster":"AzureDP900","timestamp":"1638635640.0"},{"content":"Selected Answer: B\nB, SAM is typical solution, simple question, hope I can have it in my exam","poster":"acloudguru","timestamp":"1638243180.0","comment_id":"490391","upvote_count":"1"},{"upvote_count":"1","content":"B\nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html","timestamp":"1636074240.0","comment_id":"459390","poster":"uninit"},{"timestamp":"1635953340.0","comment_id":"450712","poster":"andylogan","content":"It's B","upvote_count":"1"},{"content":"B. Why not use SAM?","upvote_count":"1","comment_id":"447899","timestamp":"1635826740.0","poster":"nodogoshi"},{"timestamp":"1635777300.0","comment_id":"409521","content":"I'll go with B","poster":"WhyIronMan","upvote_count":"1"},{"content":"B Correct","upvote_count":"1","poster":"Akhil254","comment_id":"406645","timestamp":"1635522540.0"},{"content":"B. That codedeploy is made of.","upvote_count":"1","timestamp":"1635469320.0","comment_id":"389359","poster":"Kopa"},{"content":"AWS SAM is for Serverless deployment and the method is Canary. So B is most appropriate and valid answer","upvote_count":"1","poster":"Radhaghosh","timestamp":"1635434520.0","comment_id":"362422"},{"comment_id":"344690","content":"IT should be B. \nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html","upvote_count":"1","poster":"blackgamer","timestamp":"1634946360.0"},{"content":"It's B,","timestamp":"1634908260.0","poster":"Waiweng","upvote_count":"3","comment_id":"343967"},{"comment_id":"327924","poster":"aishvary123","upvote_count":"1","timestamp":"1634857680.0","content":"D is my answer"},{"upvote_count":"2","content":"B.\nD. not because \"reduce the time to detect and revert when errors are identified\"","timestamp":"1634433420.0","comment_id":"310899","poster":"wasabidev"},{"timestamp":"1634363700.0","poster":"Kian1","comment_id":"289555","upvote_count":"1","content":"going with B"},{"comment_id":"283346","poster":"Ebi","content":"B for sure","timestamp":"1634308500.0","upvote_count":"4"},{"timestamp":"1634219340.0","upvote_count":"1","content":"B for sure.","poster":"sanjaym","comment_id":"266553"},{"content":"Answer is B. This is a classic use case of Serverless Application Model (SAM).","comment_id":"264499","poster":"halfdeaf","timestamp":"1634173500.0","upvote_count":"2"},{"timestamp":"1634172960.0","poster":"T14102020","upvote_count":"1","content":"Correct answer is B. SAM and CodeDeploy.","comment_id":"242396"},{"timestamp":"1634079000.0","comment_id":"229424","poster":"jackdryan","upvote_count":"3","content":"I'll go with B"},{"content":"Answer is B. SAM and CodeDeploy is the standard for deploying Serverless services such as Lambda. Besides the detection of the errors using CloudWatch log events to rollback can be automated through this process.","comment_id":"228637","poster":"Bulti","upvote_count":"3","timestamp":"1634038260.0"},{"upvote_count":"1","content":"Should be B","poster":"Edgecrusher77","timestamp":"1633864980.0","comment_id":"225793"},{"comment_id":"215856","poster":"vjt","content":"B is correct.","timestamp":"1633807860.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"162599","poster":"tj515153","content":"D also looks correct, any idea?","timestamp":"1633690020.0"},{"timestamp":"1633453980.0","poster":"fullaws","content":"B is correct","comment_id":"148972","upvote_count":"1"},{"timestamp":"1633346640.0","upvote_count":"1","content":"B it is","poster":"learner4ever","comment_id":"145303"},{"poster":"noisonnoiton","content":"B acceptable","timestamp":"1633098540.0","upvote_count":"1","comment_id":"136903"},{"timestamp":"1632986640.0","content":"B for sure","poster":"NikkyDicky","comment_id":"134230","upvote_count":"1"},{"poster":"sunilrch","content":"B is the perfect Answer","timestamp":"1632906540.0","upvote_count":"1","comment_id":"106795"},{"comment_id":"45297","timestamp":"1632780000.0","poster":"amog","upvote_count":"2","content":"It's B"},{"upvote_count":"4","comment_id":"28634","poster":"Scunningham99","timestamp":"1632708000.0","content":"b is correct \nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html"},{"comments":[{"comment_id":"11517","poster":"dpvnme","timestamp":"1632140100.0","content":"it is b","upvote_count":"1"}],"timestamp":"1632107460.0","comment_id":"11056","content":"why not b","upvote_count":"1","poster":"awsec2"}],"question_id":400,"exam_id":32,"choices":{"C":"Refactor the AWS CLI scripts into a single script that deploys the new Lambda version. When deployment is completed, the script tests execute. If errors are detected, revert to the previous Lambda version.","D":"Create and deploy an AWS CloudFormation stack that consists of a new API Gateway endpoint that references the new Lambda version. Change the CloudFront origin to the new API Gateway endpoint, monitor errors and if detected, change the AWS CloudFront origin to the previous API Gateway endpoint.","B":"Use AWS SAM and built-in AWS CodeDeploy to deploy the new Lambda version, gradually shift traffic to the new version, and use pre-traffic and post-traffic test functions to verify code. Rollback if Amazon CloudWatch alarms are triggered.","A":"Create and deploy nested AWS CloudFormation stacks with the parent stack consisting of the AWS CloudFront distribution and API Gateway, and the child stack containing the Lambda function. For changes to Lambda, create an AWS CloudFormation change set and deploy; if errors are triggered, revert the AWS CloudFormation change set to the previous version."},"isMC":true,"answer":"B","topic":"1","timestamp":"2019-09-14 12:02:00","answer_description":"","answers_community":["B (100%)"],"answer_ET":"B","question_images":[],"unix_timestamp":1568455320,"question_text":"A company has a serverless application comprised of Amazon CloudFront, Amazon API Gateway, and AWS Lambda functions. The current deployment process of the application code is to create a new version number of the Lambda function and run an AWS CLI script to update. If the new function version has errors, another CLI script reverts by deploying the previous working version of the function. The company would like to decrease the time to deploy new versions of the application logic provided by the Lambda functions, and also reduce the time to detect and revert when errors are identified.\nHow can this be accomplished?"}],"exam":{"name":"AWS Certified Solutions Architect - Professional","id":32,"isImplemented":true,"isMCOnly":false,"numberOfQuestions":1019,"isBeta":false,"lastUpdated":"11 Apr 2025","provider":"Amazon"},"currentPage":80},"__N_SSP":true}