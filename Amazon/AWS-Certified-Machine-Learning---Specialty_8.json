{"pageProps":{"questions":[{"id":"uZCgbbR5JRLZNc6PuKnw","unix_timestamp":1612570020,"timestamp":"2021-02-06 01:07:00","answer":"B","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/44093-exam-aws-certified-machine-learning-specialty-topic-1/","answer_ET":"B","isMC":true,"question_text":"A data scientist must build a custom recommendation model in Amazon SageMaker for an online retail company. Due to the nature of the company's products, customers buy only 4-5 products every 5-10 years. So, the company relies on a steady stream of new customers. When a new customer signs up, the company collects data on the customer's preferences. Below is a sample of the data available to the data scientist.\n//IMG//\n\nHow should the data scientist split the dataset into a training and test set for this use case?","choices":{"C":"Identify the 10% of users with the least interaction data. Split off all interaction data from these users for the test set.","B":"Identify the most recent 10% of interactions for each user. Split off these interactions for the test set.","D":"Randomly select 10% of the users. Split off all interaction data from these users for the test set.","A":"Shuffle all interaction data. Split off the last 10% of the interaction data for the test set."},"exam_id":26,"answer_images":[],"answers_community":["B (53%)","D (47%)"],"topic":"1","discussion":[{"poster":"[Removed]","content":"I would select B, straight from this AWS example: https://aws.amazon.com/blogs/machine-learning/building-a-customized-recommender-system-in-amazon-sagemaker/","upvote_count":"26","comment_id":"284454","comments":[{"timestamp":"1653328260.0","comment_id":"485378","upvote_count":"3","content":"the blog didn't mentioned anything about sample selection. how is B arrived?","poster":"ttsun"}],"timestamp":"1647742860.0"},{"upvote_count":"8","content":"I think the answer is D because customers by only 4-5 products every 5-10 years so it doesn't make sense to get 10% interactions for each user as a test set.","comments":[{"poster":"jrff","upvote_count":"2","timestamp":"1682778960.0","comment_id":"707251","content":"Yes, agree. Answer should be D"},{"timestamp":"1683697560.0","content":"B. Recommendation should use the historcial to predict the furture action. B is using the older records to prediect the newer records. D is using 90% user to predict other 10%, 90% is irrelevant to other 10%.","upvote_count":"2","comment_id":"715013","poster":"VinceCar"}],"timestamp":"1651425600.0","poster":"NicZ1111","comment_id":"440143"},{"content":"There is no difference between A and D, so I prefer B as the answer","timestamp":"1732488780.0","poster":"kawaimahiro","comment_id":"1217817","upvote_count":"3"},{"poster":"kyuhuck","content":"Selected Answer: D\nThe best way to split the dataset into a training and test set for this use case is to randomly select\n10% of the users and split off all interaction data from these users for the test set. This is because the\ncompany relies on a steady stream of new customers, so the test set should reflect the behavior of\nnew customers who have not been seen by the model before. The other options are not suitable\nbecause they either mix old and new customers in the test set (A and B), or they bias the test set\ntowards users with less interaction data . References:\nAmazon SageMaker Developer Guide: Train and Test Datasets\nAmazon Personalize Developer Guide: Preparing and Importing Data","upvote_count":"1","timestamp":"1723388580.0","comment_id":"1147524"},{"timestamp":"1720778100.0","comment_id":"1120728","content":"Selected Answer: D\nPrimary concern is to evaluate the model's performance on completely new users then option D would be more appropriate.","upvote_count":"2","poster":"praveenaws"},{"content":"I'd also take time into consideration, since even for such long-lived products there might be trends or regulations or whatever that make customers prefer one over the other. => A,D are out\n\nC will not give you a test set of desired size => out\n\n=> B","poster":"u_b","upvote_count":"2","comment_id":"1072550","timestamp":"1715867100.0"},{"upvote_count":"2","content":"Selected Answer: D\nIf the primary concern is to evaluate the model's performance on completely new users (which seems to be the case for the company in question), then option D would be more appropriate.","timestamp":"1714598160.0","poster":"sonoluminescence","comment_id":"1060114"},{"poster":"DimLam","timestamp":"1714451460.0","comment_id":"1058487","upvote_count":"2","content":"Selected Answer: D\nI would choose D.\n\nAccording to the question, because of the product nature, the company doesn't rely on customer-product historical interactions for recommendations. It relies on customer explicit preferences, which are gathered on the first sign-up.\n\nThe company wants to make recommendations for these new users. It is the main source of revenue for the company.\n\nTo conduct thorough testing company needs to simulate the new users, not existing ones.\n\nTo do it we need to randomly choose some percentage of users and remove all of their transactions from the train set. And use their transactions only in test."},{"timestamp":"1711776060.0","poster":"Rejju","content":"Selected Answer: B\nBy selecting the most recent interactions for each user, you are simulating the scenario of having new customers in your test set. This method allows you to assess how well the model generalizes to both existing and new users.","comment_id":"1021230","upvote_count":"3"},{"poster":"loict","timestamp":"1710773340.0","upvote_count":"3","comment_id":"1010602","content":"Selected Answer: D\nA. NO - the data is denormalized and users' preferences are present in multiple rows in the interactions; if we split off interactions, we introduce leakage as the same user will be present in train & test\nA. NO - the data is denormalized and users' preferences are present in multiple rows in the interactions; if we split off based on the interaction, we introduce leakage as the same user will be present in train & test\nC. NO - bias\nD. YES - no bias and user based"},{"content":"Selected Answer: B\nA NO introduces a bias in the training set (old interactions) vs. test set (new interactions)\nC NO will have a very sparse test set\nB NO the same user will be present in the training and test set; we want a user-based model, not an interaction-based one, so a user should belong to only one set\nD YES - last remaining option.","timestamp":"1709800080.0","upvote_count":"3","poster":"loict","comment_id":"1001235"},{"upvote_count":"2","poster":"Mickey321","comment_id":"993213","content":"Selected Answer: B\nChanging to B","timestamp":"1709227800.0"},{"upvote_count":"2","poster":"Mickey321","comment_id":"991244","timestamp":"1709021280.0","content":"Selected Answer: D\nBetween B and D but the issue is 4-5 transaction every 5-10 years. Hence last 10% transaction is difficult. So going for D"},{"timestamp":"1702710180.0","content":"Selected Answer: B\nI would select B as it is time series data. Order might be important. So for each user, last 10% of transactions ordered by date could be a good answer.","comment_id":"924859","upvote_count":"2","poster":"AmitGSL"},{"upvote_count":"2","content":"Selected Answer: D\nYou want different users in training and in testing datasets, which is C or D. In addition, B is wrong since you cannot take 10% of 4-5 transactions per customer. Actually, between B, C and D, only in D you can get exactly 10%.","comment_id":"883570","timestamp":"1698505560.0","poster":"cox1960"},{"comment_id":"807828","poster":"AjoseO","timestamp":"1691955720.0","upvote_count":"1","content":"Selected Answer: B\nThis method is appropriate because it takes into account the unique buying behavior of each customer and is likely to reflect the latest preferences of the customer. It ensures that the test set contains a representative sample of the most recent customer preferences, which is important in this use case where customer preferences change infrequently over time."},{"comment_id":"711211","content":"Selected Answer: B\nB makes the most business sense. Since customers buy products every 4-5 years, it makes sense to be able to predict future sales from really old data. splitting the test set to be only recent interactions is the best way to test model performance from historically 'recent' data","timestamp":"1683203220.0","poster":"aScientist","upvote_count":"1"},{"upvote_count":"1","timestamp":"1657926540.0","poster":"vetaal","content":"Selected Answer: B\nIt's a time sensitive dataset and hence B makes most sense as test set.","comment_id":"524538"},{"content":"Selected Answer: B\nI think it is a problem of leakage, so B is the correct answer\nhttps://www.datarobot.com/wiki/target-leakage/","timestamp":"1657255980.0","poster":"gggsrs","comment_id":"519356","upvote_count":"3"},{"timestamp":"1650420720.0","upvote_count":"1","poster":"Juka3lj","content":"B is correct, takes all users into consideration\nD is wrong because it takes only 10% of total number of users.","comment_id":"342992"}],"question_images":["https://www.examtopics.com/assets/media/exam-media/04145/0007800001.png"],"question_id":36},{"id":"N15EHnGG9bvLnsbUZo5z","exam_id":26,"answer":"ADE","answer_images":[],"choices":{"B":"Use SCPs to restrict access to SageMaker.","E":"Restrict notebook presigned URLs to specific IPs used by the company.","F":"Protect data with encryption at rest and in transit. Use AWS Key Management Service (AWS KMS) to manage encryption keys.","D":"Enable network isolation for training jobs and models.","A":"Connect to SageMaker by using a VPC interface endpoint powered by AWS PrivateLink.","C":"Disable root access on the SageMaker notebook instances."},"topic":"1","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/44155-exam-aws-certified-machine-learning-specialty-topic-1/","answers_community":["ADE (61%)","ADF (30%)","6%"],"question_id":37,"discussion":[{"upvote_count":"37","comments":[{"poster":"khchan123","comment_id":"1076428","content":"ADE for sure. F is for encryption and not data egress.","upvote_count":"2","timestamp":"1700582340.0"},{"poster":"scuzzy2010","content":"I agree with ADF. SCP is to control access to a service, it's not related to securing data.","comment_id":"306771","upvote_count":"3","timestamp":"1634667180.0"}],"poster":"SophieSu","content":"ADF - the concepts in ADF are explained in detail on the official Amazon Exam Readiness Exam Readiness: AWS Certified Machine Learning - Specialty. Amazon official materials do not mention other concepts in BCE.","timestamp":"1633847040.0","comment_id":"299359"},{"comment_id":"425588","timestamp":"1635671940.0","upvote_count":"28","content":"As per official document only 4 ways to do data egress Enforcing deployment in VPC,Enforcing network isolation,Restricting notebook pre-signed URLs to IPs,Disabling internet access\nCorrect Ans - ADE\n\n\nRead Controlling data egress section\nLink - https://aws.amazon.com/blogs/machine-learning/millennium-management-secure-machine-learning-using-amazon-sagemaker/","poster":"rahulw230"},{"content":"Selected Answer: ADF\nADF : O\nBCE : X","poster":"Carpediem78","comment_id":"1409911","timestamp":"1742884200.0","upvote_count":"1","comments":[{"comment_id":"1413953","timestamp":"1743395880.0","poster":"ef12052","content":"https://aws.amazon.com/blogs/machine-learning/millennium-management-secure-machine-learning-using-amazon-sagemaker/","upvote_count":"1"}]},{"timestamp":"1742073900.0","poster":"Togy","upvote_count":"1","content":"Selected Answer: ABD\nCorrect Choices and Reasoning:\n\nA. Connect to SageMaker by using a VPC interface endpoint powered by AWS PrivateLink: Keeps traffic within the VPC.\nB. Use SCPs to restrict access to SageMaker: Limits authorized actions and services.\nD. Enable network isolation for training jobs and models: Prevents network access during training and inference.\nTherefore, the three mechanisms that the ML engineer can use to control data egress from SageMaker are A. Connect to SageMaker by using a VPC interface endpoint powered by AWS PrivateLink, B. Use SCPs to restrict access to SageMaker, and D. Enable network isolation for training jobs and models.","comment_id":"1399054"},{"comment_id":"1316483","upvote_count":"1","content":"Selected Answer: ADF\nA. Connect to SageMaker by using a VPC interface endpoint powered by AWS PrivateLink\n\nPrivateLink ensures that communication between SageMaker and other AWS services happens entirely within the AWS network, avoiding exposure to the public internet.\nThis reduces the risk of unintended data egress.\nD. Enable network isolation for training jobs and models\n\nEnabling network isolation ensures that containers used for training jobs and models cannot make outbound network connections.\nThis prevents accidental or malicious data egress.\nF. Protect data with encryption at rest and in transit. Use AWS Key Management Service (AWS KMS) to manage encryption keys\n\nEncrypting data ensures its security even if it is inadvertently accessed or stored improperly.\nKMS allows centralized and secure management of encryption keys.","poster":"KarinaAsh","timestamp":"1732306980.0"},{"upvote_count":"1","timestamp":"1714539780.0","content":"Selected Answer: ADE\nF - it takes care of data sitting in sagemaker env which is encrypted but E ensures that the srvices or its resources cannot be accessed outside of the allowed IP's","poster":"rookiee1111","comment_id":"1204864"},{"upvote_count":"1","comments":[{"poster":"vkbajoria","comment_id":"1187627","upvote_count":"1","content":"I changed my selection It is truly ADE. I read the link provided by rahulw230","timestamp":"1711998180.0"}],"content":"My vote for ADF","comment_id":"1164145","timestamp":"1709388360.0","poster":"vkbajoria"},{"poster":"AIWave","timestamp":"1708444440.0","content":"Selected Answer: ABD\nA = VPC endpoints are well know safety mechanism in SM so traffic doesn’t leave AWS\nB = service control policy can restrict access at org level\nD = Network isolation limits training model access only to S3","upvote_count":"1","comment_id":"1154830"},{"comment_id":"1147527","timestamp":"1707671220.0","poster":"kyuhuck","content":"Selected Answer: ADF\nTo control data egress from SageMaker, the ML engineer can use the following mechanisms:\nConnect to SageMaker by using a VPC interface endpoint powered by AWS PrivateLink. This allows\nthe ML engineer to access SageMaker services and resources without exposing the traffic to the\npublic internet. This reduces the risk of data leakage and unauthorized access1 Enable network\nisolation for training jobs and models.","upvote_count":"1"},{"content":"Question is wrong A, B, E and D are all valid to a point.","upvote_count":"1","comment_id":"1060129","poster":"sonoluminescence","timestamp":"1698882360.0"},{"poster":"jyrajan69","comment_id":"1001084","timestamp":"1694048520.0","upvote_count":"1","content":"The more I see it, the more likely I will go with ABD, the only answers than address the data egress issue"},{"timestamp":"1693812300.0","comment_id":"998291","content":"For those who are sure that is E, please explain how you can use pre-signed urls to restrict IP's, from my understanding it is a time based access to your S3 objects, you can policies to control access, like SCP (Service Control Policy), Isolation is definitely one option so that leaves F (Encrypting in transit and Encrypting objects) as the only possible solution as BDF","upvote_count":"1","poster":"jyrajan69"},{"upvote_count":"1","poster":"Mickey321","timestamp":"1693116900.0","comment_id":"991251","content":"Selected Answer: ADF\nA and D are for sure. The challenge between E and F. E restrict access to the notebook hence indirectly control who access it and can access data but encrypting the data is more direct way to protect the egress of the data. hence leaning more towards F"},{"poster":"mawsman","upvote_count":"3","timestamp":"1681754400.0","comment_id":"872941","content":"Selected Answer: ADE\nNot F because the que4stion is \"to control data egress\". F (encryption) is not egress control."},{"poster":"codehive","timestamp":"1680908460.0","comment_id":"864283","upvote_count":"2","content":"Selected Answer: ADF\nA, D, F are the mechanisms that the ML engineer can use to control data egress from SageMaker. B, C, and E do not directly control data egress from SageMaker. SCPs restrict access to AWS services, disabling root access on the SageMaker notebook instances improves security, and restricting notebook presigned URLs to specific IPs used by the company adds another layer of security, but none of these mechanisms control data egress from SageMaker."},{"timestamp":"1680598080.0","comment_id":"860820","upvote_count":"2","content":"Selected Answer: ADE\nhttps://aws.amazon.com/blogs/machine-learning/millennium-management-secure-machine-learning-using-amazon-sagemaker/","poster":"Mllb"},{"content":"Selected Answer: ADF\nAre the correct","comment_id":"860065","upvote_count":"2","poster":"Mllb","timestamp":"1680534840.0"},{"timestamp":"1667572500.0","content":"Selected Answer: DEF\nAccording to the subheadings in this case study:\nhttps://aws.amazon.com/blogs/machine-learning/millennium-management-secure-machine-learning-using-amazon-sagemaker/ \n\nThe relevant options are:\nControlling data egress:\nEnforcing deployment in VPC: This does not require a VPN to be enabled\nEnforcing network isolation\nRestricting notebook pre-signed URLs to IPs\nDisabling internet access\n\nEnforcing encryption:\nEnforcing job encryption: sagemaker:VolumeKmsKey","poster":"aScientist","upvote_count":"1","comment_id":"711217"},{"comment_id":"600432","timestamp":"1652326260.0","upvote_count":"2","content":"Selected Answer: ADF\nI think option E \"Restrict notebook presigned URLs to specific IPs used by the company. \" is ambiguous. The official blog post at https://aws.amazon.com/blogs/machine-learning/millennium-management-secure-machine-learning-using-amazon-sagemaker/ does indicate that the objective can be achieved by limiting access to the notebook by blocking some IPs. The point is not presigned URLs, but blocking IPs. The option E however puts more emphasis on presigned URLs.","poster":"DJiang"},{"timestamp":"1642995540.0","upvote_count":"6","comment_id":"531036","poster":"vetaal","content":"Selected Answer: ADE\nPer thr following link, it is ADE - see section on controlling data egress - https://aws.amazon.com/blogs/machine-learning/millennium-management-secure-machine-learning-using-amazon-sagemaker/"},{"timestamp":"1641680700.0","poster":"geekgirl007","content":"Selected Answer: ADE\nthe answer is ADE. see this doc \"controlling data egress\" section; https://aws.amazon.com/blogs/machine-learning/millennium-management-secure-machine-learning-using-amazon-sagemaker/","upvote_count":"7","comment_id":"519803"},{"comment_id":"519143","timestamp":"1641580080.0","poster":"hess","upvote_count":"1","content":"Selected Answer: ADE\nSCP only restrict access to the service not the data."},{"upvote_count":"3","comment_id":"435826","timestamp":"1636095600.0","content":"C D E - read the article mentioned below","poster":"Dr_Kiko"},{"poster":"Huy","content":"BDE yes. \nA is for Sagemaker access while F is for data encryption. They are not specific for the data egress control.","timestamp":"1635193620.0","comment_id":"401795","upvote_count":"4"},{"comment_id":"376381","upvote_count":"2","poster":"btsql","content":"ADF is myanswer. SCP is just secure copy.","timestamp":"1635131400.0"},{"upvote_count":"3","content":"ADF\nF: Protect data with encryption at rest and in transit\nA: Internetwork Traffic Privacy\nD: Infrastructure Security\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/data-protection.html","poster":"DSJingguo","timestamp":"1635084000.0","comment_id":"344645"},{"upvote_count":"4","poster":"DonRichy","comment_id":"301011","comments":[{"poster":"DonRichy","timestamp":"1634312880.0","content":"Sorry , its BDE and not ADE","upvote_count":"6","comment_id":"301012"}],"timestamp":"1634052720.0","content":"Answer is ADE, check below:\nhttps://aws.amazon.com/blogs/machine-learning/millennium-management-secure-machine-learning-using-amazon-sagemaker/"},{"poster":"astonm13","upvote_count":"7","content":"I think it is ADE","comment_id":"291846","timestamp":"1633612080.0"},{"comment_id":"287128","content":"B. Use SCPs to restrict access to SageMaker. (policy to restrict data access)\nD. Enable network isolation for training jobs and models. (protecting the data through isolation)\nF. Protect data with encryption at rest and in transit. Use AWS Key Management Service (AWS KMS) to manage encryption keys. (Protecting the data through encryption)\n\nThe ask is to protect the data not who can access the data, suttle difference","upvote_count":"7","poster":"cnethers","timestamp":"1633452960.0"},{"timestamp":"1633085280.0","content":"bde\nhttps://aws.amazon.com/blogs/machine-learning/millennium-management-secure-machine-learning-using-amazon-sagemaker/","poster":"jiadong","upvote_count":"8","comment_id":"285304"}],"question_images":[],"unix_timestamp":1612675980,"answer_ET":"ADE","timestamp":"2021-02-07 06:33:00","question_text":"A financial services company wants to adopt Amazon SageMaker as its default data science environment. The company's data scientists run machine learning\n(ML) models on confidential financial data. The company is worried about data egress and wants an ML engineer to secure the environment.\nWhich mechanisms can the ML engineer use to control data egress from SageMaker? (Choose three.)","answer_description":""},{"id":"wdkxccpFYOpwZKkxeBMN","exam_id":26,"discussion":[{"upvote_count":"49","comment_id":"589630","content":"I would choose C.","poster":"knightknt","timestamp":"1650579780.0"},{"comment_id":"621404","poster":"ovokpus","timestamp":"1656036900.0","content":"Answer here is C. Glue, Athena and Quicksight are serverless and need little code (only SQL)","upvote_count":"12"},{"upvote_count":"2","timestamp":"1727625180.0","poster":"MultiCloudIronMan","comment_id":"1291213","content":"C is the right answer"},{"upvote_count":"2","poster":"ArunRav","comment_id":"1218228","timestamp":"1716635220.0","content":"Answer is C, all serverless"},{"comment_id":"1187957","timestamp":"1712051220.0","content":"I woul choose C as well","poster":"Noname3562","upvote_count":"2"},{"timestamp":"1701103020.0","poster":"endeesa","upvote_count":"1","comment_id":"1081784","content":"In the presence of AWS Glue, with a goal to minimise coding efforts. C is the correct answer"},{"timestamp":"1700141640.0","content":"I also chose C.\nA has code/infra overhead of EMR.\nB is wrong b/c you dont query S3 with redshift\nD is overhead from orchestrating lambda jobs with step funcs","comment_id":"1072436","poster":"u_b","upvote_count":"3"},{"timestamp":"1699863960.0","comment_id":"1069151","content":"AWS Glue CROWLER for data discovery","poster":"qsergii","upvote_count":"1"},{"timestamp":"1698395940.0","content":"C is correct","upvote_count":"2","comment_id":"1055285","poster":"Snape"},{"upvote_count":"1","timestamp":"1694832840.0","poster":"jopaca1216","content":"The correct is C","comment_id":"1008809"},{"timestamp":"1693117080.0","comment_id":"991254","content":"Why no voting option?\nIt is option C","upvote_count":"4","poster":"Mickey321"},{"upvote_count":"2","content":"C is correct","timestamp":"1690964340.0","comment_id":"969928","poster":"kazivebtak"},{"poster":"ADVIT","content":"I think it's C","timestamp":"1688524440.0","comment_id":"943248","upvote_count":"1"},{"comment_id":"918688","upvote_count":"1","poster":"mixonfreddy","timestamp":"1686259380.0","content":"Answer is C, all serverless"},{"upvote_count":"1","poster":"Ahmedhadi_","timestamp":"1681736700.0","content":"answer is c as data sources varies alot so requires glue crawler","comment_id":"872706"},{"content":"C Is correct, you use Glue for ingestion","upvote_count":"2","poster":"mite_gvg","timestamp":"1681580100.0","comment_id":"871169"},{"comment_id":"864286","comments":[{"poster":"codehive","upvote_count":"1","comment_id":"864288","content":"Option A is not an ideal choice because Amazon EMR is a heavy-weight service and requires more infrastructure management than AWS Glue.","timestamp":"1680908880.0"}],"timestamp":"1680908820.0","poster":"codehive","content":"Option C is the most suitable choice to meet the given requirements. AWS Glue is a fully managed extract, transform, and load (ETL) service that allows users to discover, enrich, and transform data easily, without the need for extensive coding. It supports different data sources, schema detection, and schema evolution, which makes it an ideal choice for the given scenario. Amazon Athena, a serverless interactive query service, allows users to run standard SQL queries against data stored in Amazon S3, which makes it easy to analyze the enriched and transformed data. Amazon QuickSight is a cloud-based business intelligence service that can connect to various data sources, including Amazon Athena, to create interactive dashboards and reports, which makes it a suitable choice for gaining insights from the data.","upvote_count":"1"},{"content":"Go with C here","timestamp":"1677180480.0","upvote_count":"1","poster":"Siyuan_Zhu","comment_id":"819631"},{"timestamp":"1675366020.0","content":"Answer is C","upvote_count":"2","poster":"damaldon","comment_id":"796368"},{"comment_id":"723801","poster":"GauravLahotiML","timestamp":"1669052580.0","content":"C is thr correct answer","upvote_count":"2"},{"content":"I think A. EMR to handle volume and velocity and variety","comment_id":"700676","poster":"prathapsingh","timestamp":"1666341840.0","upvote_count":"1"},{"content":"A: too much infrastructure & code\nB: Data intake with KDA is non-sense & too much infrastructure & code\nC: glue, code but no infra admin, same for athena and quicksight\nD: could work but multiple Lambda produces a code overhead and there is no data ingest in the question","comment_id":"618702","timestamp":"1655644020.0","upvote_count":"1","poster":"f4bi4n"}],"url":"https://www.examtopics.com/discussions/amazon/view/74067-exam-aws-certified-machine-learning-specialty-topic-1/","answer_ET":"A","topic":"1","answer_description":"","question_id":38,"answer_images":[],"question_text":"A company needs to quickly make sense of a large amount of data and gain insight from it. The data is in different formats, the schemas change frequently, and new data sources are added regularly. The company wants to use AWS services to explore multiple data sources, suggest schemas, and enrich and transform the data. The solution should require the least possible coding effort for the data flows and the least possible infrastructure management.\nWhich combination of AWS services will meet these requirements?\nA.\n✑ Amazon EMR for data discovery, enrichment, and transformation\n✑ Amazon Athena for querying and analyzing the results in Amazon S3 using standard SQL\n✑ Amazon QuickSight for reporting and getting insights\nB.\n✑ Amazon Kinesis Data Analytics for data ingestion\n✑ Amazon EMR for data discovery, enrichment, and transformation\n✑ Amazon Redshift for querying and analyzing the results in Amazon S3\nC.\n✑ AWS Glue for data discovery, enrichment, and transformation\n✑ Amazon Athena for querying and analyzing the results in Amazon S3 using standard SQL\n✑ Amazon QuickSight for reporting and getting insights\nD.\n✑ AWS Data Pipeline for data transfer\n✑ AWS Step Functions for orchestrating AWS Lambda jobs for data discovery, enrichment, and transformation\n✑ Amazon Athena for querying and analyzing the results in Amazon S3 using standard SQL\n✑ Amazon QuickSight for reporting and getting insights","answers_community":[],"timestamp":"2022-04-22 00:23:00","question_images":[],"answer":"A","isMC":false,"unix_timestamp":1650579780},{"id":"ZelchYzz3XKrBgUgXXFg","answer_ET":"C","url":"https://www.examtopics.com/discussions/amazon/view/76354-exam-aws-certified-machine-learning-specialty-topic-1/","question_id":39,"isMC":true,"topic":"1","question_text":"A company is converting a large number of unstructured paper receipts into images. The company wants to create a model based on natural language processing\n(NLP) to find relevant entities such as date, location, and notes, as well as some custom entities such as receipt numbers.\nThe company is using optical character recognition (OCR) to extract text for data labeling. However, documents are in different structures and formats, and the company is facing challenges with setting up the manual workflows for each document type. Additionally, the company trained a named entity recognition (NER) model for custom entity detection using a small sample size. This model has a very low confidence score and will require retraining with a large dataset.\nWhich solution for text extraction and entity detection will require the LEAST amount of effort?","answer_description":"","choices":{"A":"Extract text from receipt images by using Amazon Textract. Use the Amazon SageMaker BlazingText algorithm to train on the text for entities and custom entities.","C":"Extract text from receipt images by using Amazon Textract. Use Amazon Comprehend for entity detection, and use Amazon Comprehend custom entity recognition for custom entity detection.","D":"Extract text from receipt images by using a deep learning OCR model from the AWS Marketplace. Use Amazon Comprehend for entity detection, and use Amazon Comprehend custom entity recognition for custom entity detection.","B":"Extract text from receipt images by using a deep learning OCR model from the AWS Marketplace. Use the NER deep learning model to extract entities."},"answers_community":["C (100%)"],"answer_images":[],"answer":"C","unix_timestamp":1653682080,"question_images":[],"exam_id":26,"timestamp":"2022-05-27 22:08:00","discussion":[{"comment_id":"608190","poster":"exam_prep","timestamp":"1669586880.0","upvote_count":"15","content":"C is the correct answer. You definitely need Amazon Textract service which eliminate options B & D. Between A & C - Comprehend will quicker."},{"upvote_count":"1","timestamp":"1727818200.0","comment_id":"1187675","poster":"vkbajoria","content":"Selected Answer: C\nTextract and Comprehend will do the job"},{"upvote_count":"1","comment_id":"1167796","timestamp":"1725690960.0","poster":"james2033","content":"Selected Answer: C\nKeywords 'Amazon Textract' and 'Amazon Comprehend'"},{"poster":"Mickey321","timestamp":"1709025000.0","upvote_count":"1","content":"Selected Answer: C\nC indeed due to least effort","comment_id":"991273"},{"content":"Selected Answer: C\nC is correct","comment_id":"975877","timestamp":"1707420060.0","poster":"kaike_reis","upvote_count":"1"},{"content":"I think C","timestamp":"1704429420.0","comment_id":"943249","upvote_count":"1","poster":"ADVIT"},{"content":"Selected Answer: C\nI go for C","comment_id":"832495","poster":"alp_ileri","timestamp":"1694136660.0","upvote_count":"2"},{"content":"Selected Answer: C\nC is the best answer, textract is to extract data from documents and comprehend to understand the filling, objective or origin of a file.","upvote_count":"1","poster":"Valcilio","timestamp":"1694001780.0","comment_id":"830865"},{"timestamp":"1690998480.0","poster":"damaldon","comment_id":"796384","content":"C is correct, you can extract Entity information easily with Comprehend.\nhttps://aws.amazon.com/comprehend/features/","upvote_count":"4"}]},{"id":"b8LeI1zWfPNaRiu2XPjD","answer_ET":"B","question_id":40,"url":"https://www.examtopics.com/discussions/amazon/view/74974-exam-aws-certified-machine-learning-specialty-topic-1/","question_text":"A company is building a predictive maintenance model based on machine learning (ML). The data is stored in a fully private Amazon S3 bucket that is encrypted at rest with AWS Key Management Service (AWS KMS) CMKs. An ML specialist must run data preprocessing by using an Amazon SageMaker Processing job that is triggered from code in an Amazon SageMaker notebook. The job should read data from Amazon S3, process it, and upload it back to the same S3 bucket.\nThe preprocessing code is stored in a container image in Amazon Elastic Container Registry (Amazon ECR). The ML specialist needs to grant permissions to ensure a smooth data preprocessing workflow.\nWhich set of actions should the ML specialist take to meet these requirements?","topic":"1","isMC":true,"answer_description":"","choices":{"B":"Create an IAM role that has permissions to create Amazon SageMaker Processing jobs. Attach the role to the SageMaker notebook instance. Create an Amazon SageMaker Processing job with an IAM role that has read and write permissions to the relevant S3 bucket, and appropriate KMS and ECR permissions.","D":"Create an IAM role that has permissions to create Amazon SageMaker Processing jobs. Attach the role to the SageMaker notebook instance. Set up an S3 endpoint in the default VPC. Create Amazon SageMaker Processing jobs with the access key and secret key of the IAM user with appropriate KMS and ECR permissions.","C":"Create an IAM role that has permissions to create Amazon SageMaker Processing jobs and to access Amazon ECR. Attach the role to the SageMaker notebook instance. Set up both an S3 endpoint and a KMS endpoint in the default VPC. Create Amazon SageMaker Processing jobs from the notebook.","A":"Create an IAM role that has permissions to create Amazon SageMaker Processing jobs, S3 read and write access to the relevant S3 bucket, and appropriate KMS and ECR permissions. Attach the role to the SageMaker notebook instance. Create an Amazon SageMaker Processing job from the notebook."},"answers_community":["B (55%)","A (45%)"],"answer_images":[],"answer":"B","unix_timestamp":1651328700,"question_images":[],"exam_id":26,"timestamp":"2022-04-30 16:25:00","discussion":[{"comments":[{"comment_id":"955557","upvote_count":"3","timestamp":"1689692400.0","content":"Why should the IAM permission be assigned to create S3, when the data is already stored in S3? It only require permission to read and write data in S3. I believe A is incorrect.","poster":"kukreti18"}],"content":"Selected Answer: A\nA; IAM assigned to SageMaker Notebook instance can be passed to other SageMaker jobs like training; processing, automl, etc.,","upvote_count":"12","comment_id":"595076","timestamp":"1651328700.0","poster":"spaceexplorer"},{"content":"Selected Answer: A\nChecked this on CoPilot","poster":"MultiCloudIronMan","comment_id":"1299663","timestamp":"1729254660.0","upvote_count":"1","comments":[{"poster":"MultiCloudIronMan","upvote_count":"3","timestamp":"1730457120.0","comment_id":"1305740","content":"I changed my mind its 'B' Option B is generally better because it provides a more secure and controlled approach to managing permissions. By separating the roles, you can ensure that the SageMaker notebook instance has only the permissions it needs to create processing jobs, while the processing job itself has the specific permissions required to access the S3 bucket, KMS, and ECR. This separation of duties enhances security and minimizes the risk of over-permissioning any single role."}]},{"comment_id":"1292595","upvote_count":"3","poster":"MJSY","timestamp":"1727919780.0","content":"Selected Answer: B\nA is not correct, for safety and principle of least privilege, you should decouple the role of each service."},{"content":"Selected Answer: B\nbased on answers from here","timestamp":"1717458180.0","upvote_count":"1","comment_id":"1223832","poster":"Chiquitabandita"},{"comment_id":"1201914","poster":"F1Fan","timestamp":"1714042860.0","content":"Selected Answer: A\nOption A:\nThe IAM role is created with the necessary permissions to create Amazon SageMaker Processing jobs, read and write data to the relevant S3 bucket, and access the KMS CMKs and ECR container image.\nThe IAM role is attached to the SageMaker notebook instance, which allows the notebook to assume the role and create the Amazon SageMaker Processing job with the necessary permissions.\nThe Amazon SageMaker Processing job is created from the notebook, which ensures that the job has the necessary permissions to read data from S3, process it, and upload it back to the same S3 bucket.\n\nOption B is close, but it's not entirely correct. It mentions creating an IAM role with permissions to create Amazon SageMaker Processing jobs, but it doesn't mention attaching the role to the SageMaker notebook instance. This is a crucial step, as it allows the notebook to assume the role and create the Amazon SageMaker Processing job with the necessary permissions.","upvote_count":"2"},{"upvote_count":"2","comment_id":"1147531","timestamp":"1707671700.0","poster":"kyuhuck","content":"Selected Answer: B\nThe correct solution for granting permissions for data preprocessing is to use the following steps:Create an IAM role that has permissions to create Amazon SageMaker Processing jobs. Attach therole to the SageMaker notebook instance. This role allows the ML specialist to run Processing jobsfrom the notebook code1 Create an Amazon SageMaker Processing job with an IAM role that hasread and write permissions to the relevant S3 bucket, and appropriate KMS and ECR permissions.This role allows the Processing job to access the data in the encrypted S3 bucket, decrypt it with theKMS CMK, and pull the container image from ECR23 The other options are incorrect because theyeither miss some permissions or use unnecessary steps. For example"},{"upvote_count":"2","poster":"CloudHandsOn","timestamp":"1706042940.0","comment_id":"1130041","content":"Selected Answer: B\nLeast priv."},{"timestamp":"1704814380.0","poster":"CloudHandsOn","comment_id":"1117604","content":"Selected Answer: B\nA. Create an IAM role with S3, KMS, ECR permissions and SageMaker Processing job creation permissions. Attach it to the SageMaker notebook instance: This option seems comprehensive as it includes all necessary permissions. However, attaching this role directly to the SageMaker notebook instance would not be sufficient for the Processing job itself. The Processing job needs its own role with appropriate permissions.\n\nB. Create two IAM roles: one for the SageMaker notebook with permissions to create Processing jobs, and another for the Processing job itself with S3, KMS, and ECR permissions: This option is more aligned with best practices. The notebook instance and the Processing job have different roles tailored to their specific needs. This separation ensures that each service has only the permissions necessary for its operation, following the principle of least privilege.","upvote_count":"3"},{"content":"The processing job may not run on the notebook instance. AWS will provide resources to execute the job.\nSo A is wrong.\nB.","upvote_count":"3","poster":"rav009","timestamp":"1704684120.0","comment_id":"1116364"},{"content":"Selected Answer: B\nIf we follow the principle of Least Privillege, B is correct. The notebook instance does not need access to S3 and KMS given that it is only needed to trigger the processsing Job.","poster":"endeesa","comment_id":"1081788","timestamp":"1701103500.0","upvote_count":"2"},{"content":"Not A b/c it does not indicate perms given to the Job via IAM role.\n=> I went with B.","timestamp":"1700142060.0","comment_id":"1072445","upvote_count":"1","poster":"u_b"},{"content":"Selected Answer: B\nMy answer is B. The notebook instance doesn't need access to S3 and ECR. This access is needed for Processing Job only.\n\nAnd as a best practice of least privilege I'll choose B","comment_id":"1058497","timestamp":"1698734940.0","poster":"DimLam","upvote_count":"4"},{"poster":"Rejju","upvote_count":"3","timestamp":"1696048620.0","content":"Selected Answer: B\nwhere permissions are granted to the SageMaker Processing job itself and not to the notebook instance. This approach offers better security and control over permissions, making it the preferred choice for running SageMaker Processing jobs with the required access to S3, KMS, and ECR. ( Follows the principle of least privilege and have more control over permissions.","comment_id":"1021249"},{"timestamp":"1694069760.0","content":"Selected Answer: A\nIt says \"Amazon SageMaker Processing job that is triggered from code in an Amazon SageMaker notebook.\" - so A or C. There is no need to create an S3 endpoint (C), that is only to allow traffic over the internet.\n\nSo A.","poster":"loict","upvote_count":"1","comment_id":"1001254"},{"content":"Selected Answer: B\nConfusing between A and B. Leaning to B The main difference between A and B is the IAM role that is attached to the SageMaker notebook instance. In A, the role has permissions to access the data, the container image, and the KMS CMK. In B, the role only has permissions to create SageMaker Processing jobs. This means that in A, the notebook instance can potentially access or modify the data or the image without using a Processing job, which is not desirable. In B, the notebook instance can only create Processing jobs, and the Processing jobs themselves have a separate IAM role that grants them access to the data, the image, and the KMS CMK. This way, the data and the image are only accessed by the Processing jobs, which are more secure and controlled than the notebook instance.","comment_id":"991279","upvote_count":"4","timestamp":"1693120860.0","poster":"Mickey321"},{"poster":"kaike_reis","content":"Selected Answer: A\nLetters C and D are wrong, as they bring VPC, something that is not mentioned in the problem. Letter A is correct, since Letter B asks for the creation of two different IAM roles.","comments":[{"upvote_count":"1","poster":"DimLam","comment_id":"1058492","timestamp":"1698734760.0","content":"What is the problem with creating two different IAM roles?"}],"timestamp":"1691515680.0","comment_id":"975883","upvote_count":"1"},{"poster":"ccpmad","upvote_count":"1","timestamp":"1690796580.0","comment_id":"967941","content":"Selected Answer: A\nOption A ensures that the role has the necessary permissions to access the required resources (S3, KMS, ECR) and that the notebook has the ability to create a processing job in SageMaker seamlessly. It also follows the principle of \"least privilege\" by granting only the necessary permissions to perform the task without exposing more access than required."},{"timestamp":"1688525340.0","comment_id":"943255","poster":"ADVIT","upvote_count":"1","content":"Probably A is simpler than B.\nPer https://docs.aws.amazon.com/sagemaker/latest/dg/security-iam-awsmanpol.html#security-iam-awsmanpol-AmazonSageMakerFullAccess\nOne IAM Role can do everything.","comments":[]},{"content":"Selected Answer: B\nB is least privilege, since the notebook only needs access to sagemaker processing and the processing instance needs access to S3, KMS and ECR.","poster":"mawsman","timestamp":"1681756560.0","comment_id":"872981","upvote_count":"4"},{"upvote_count":"3","poster":"luckybme","content":"Selected Answer: B\nI think the answer is B. You need to pass a role to the DataProcessing job and the other options don't mention anything about passing a role to the call that submits the job.","timestamp":"1675793340.0","comment_id":"801246"},{"content":"Selected Answer: A\n\"Passing Roles\nActions like passing a role between services are a common function within SageMaker.\"\n\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html","timestamp":"1674010200.0","upvote_count":"3","comment_id":"779542","poster":"Jerry84"},{"content":"D. Sagemaker need to access private S3. Only sagemaker with VPC endpoint is able to create a VPC link with S3 private link.","poster":"jrff","comment_id":"707264","timestamp":"1667056080.0","upvote_count":"1"},{"timestamp":"1660045200.0","comments":[{"content":"I think attach the endpoint in the default VPC is a problem for private","poster":"tsangckl","timestamp":"1669687800.0","upvote_count":"1","comment_id":"729850"}],"poster":"V_B_","content":"Why not D? Seems that it should work","comment_id":"644477","upvote_count":"1"},{"comment_id":"607037","timestamp":"1653452760.0","content":"Selected Answer: A\nAnswer is A","upvote_count":"4","poster":"azi_2021"},{"content":"Option A has its own merits by assigning required IAM permissions once to the notebook and then run different processing job without worrying about the permissions.","upvote_count":"1","poster":"exam_prep","comment_id":"602695","timestamp":"1652735580.0"},{"comment_id":"602694","upvote_count":"1","comments":[{"comment_id":"617853","poster":"f4bi4n","timestamp":"1655487840.0","upvote_count":"1","content":"I wanted to go also with B BUT the ECR could be the problem. \nIn this example (https://docs.aws.amazon.com/sagemaker/latest/dg/build-your-own-processing-container.html) they use also a role for the job. But at the top they are writing:\n\"Ensure that your SageMaker IAM role can pull the image from Amazon ECR\"\nSo probably A"}],"poster":"exam_prep","timestamp":"1652735460.0","content":"Why not B? What if you want to run multiple jobs from the same sagemaker notebook with different access requirements?"}]}],"exam":{"isBeta":false,"name":"AWS Certified Machine Learning - Specialty","isImplemented":true,"numberOfQuestions":369,"lastUpdated":"11 Apr 2025","provider":"Amazon","id":26,"isMCOnly":false},"currentPage":8},"__N_SSP":true}