{"pageProps":{"questions":[{"id":"TrlAATKnZ8YU7V3uPnmG","question_images":[],"question_text":"A company releases a new application in a new AWS account. The application includes an AWS Lambda function that processes messages from an Amazon Simple Queue Service (Amazon SQS) standard queue. The Lambda function stores the results in an Amazon S3 bucket for further downstream processing. The Lambda function needs to process the messages within a specific period of time after the messages are published. The Lambda function has a batch size of 10 messages and takes a few seconds to process a batch of messages.\n\nAs load increases on the application's first day of service, messages in the queue accumulate at a greater rate than the Lambda function can process the messages. Some messages miss the required processing timelines. The logs show that many messages in the queue have data that is not valid. The company needs to meet the timeline requirements for messages that have valid data.\n\nWhich solution will meet these requirements?","topic":"1","choices":{"D":"Keep the Lambda function's batch size the same. Configure the Lambda function to report failed batch items. Configure an SQS dead-letter queue.","B":"Reduce the Lambda function's batch size. Increase the SQS message throughput quota. Request a Lambda concurrency increase in the AWS Region.","C":"Increase the Lambda function's batch size. Configure S3 Transfer Acceleration on the S3 bucket. Configure an SQS dead-letter queue.","A":"Increase the Lambda function's batch size. Change the SQS standard queue to an SQS FIFO queue. Request a Lambda concurrency increase in the AWS Region."},"exam_id":23,"answer":"D","url":"https://www.examtopics.com/discussions/amazon/view/137361-exam-aws-certified-devops-engineer-professional-dop-c02/","answer_ET":"D","answer_images":[],"answers_community":["D (93%)","7%"],"discussion":[{"poster":"c3518fc","content":"Selected Answer: D\nConfigure a dead-letter queue to avoid creating a snowball anti-pattern in your serverless application’s architecture. For more information, see the Avoiding snowball anti-patterns section of this guide.\n\nConfigure your Lambda function event source mapping to make only the failed messages visible. To do this, you must include the value ReportBatchItemFailures in the FunctionResponseTypes list when configuring your event source mapping. https://docs.aws.amazon.com/prescriptive-guidance/latest/lambda-event-filtering-partial-batch-responses-for-sqs/best-practices-partial-batch-responses.html","upvote_count":"5","comment_id":"1202768","timestamp":"1714156680.0"},{"poster":"hk0308","timestamp":"1735092660.0","comment_id":"1331321","content":"Selected Answer: C\nWe have to increase the batch size to speed up the processing. \nD makes no sense since it will not speed up the processing in anyway.\nA cannot be right since it used FIFO queue which will reduce lambda concurrency.","upvote_count":"1"},{"poster":"VerRi","timestamp":"1730944200.0","comment_id":"1308202","content":"Selected Answer: D\nA. 1 failure within a batch will cause all messages in that batch to fail, blocking other tasks and delaying overall processing","upvote_count":"2"},{"content":"Selected Answer: D\nD for me","upvote_count":"2","timestamp":"1714654800.0","poster":"seetpt","comment_id":"1205550"},{"upvote_count":"2","poster":"dkp","comment_id":"1194719","content":"answer D seems more approprite","timestamp":"1712991960.0"},{"comment_id":"1190292","timestamp":"1712391960.0","content":"I am torn between option A or D","upvote_count":"1","poster":"Ola2234"},{"timestamp":"1711795800.0","poster":"WhyIronMan","comment_id":"1186050","upvote_count":"4","content":"Selected Answer: D\nD,\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/lambda-event-filtering-partial-batch-responses-for-sqs/best-practices-partial-batch-responses.html"},{"poster":"ogerber","comment_id":"1184235","content":"its D, 100%","timestamp":"1711558680.0","upvote_count":"2"}],"answer_description":"","timestamp":"2024-03-27 17:58:00","question_id":131,"unix_timestamp":1711558680,"isMC":true},{"id":"wSEWZmKZ57ENdnipem5i","unix_timestamp":1711528680,"isMC":true,"answer_ET":"ABF","answer_description":"","timestamp":"2024-03-27 09:38:00","discussion":[{"timestamp":"1723879740.0","content":"Selected Answer: ABF\nA: Kinesis Enhanced fan-out is an Amazon Kinesis Data Streams feature that enables consumers to receive records from a data stream with dedicated throughput of up to 2 MB of data per second per shard. A consumer that uses enhanced fan-out doesn't have to contend with other consumers that are receiving data from the stream.\n\nB: Reserved concurrency – This represents the maximum number of concurrent instances allocated to your function. When a function has reserved concurrency, no other function can use that concurrency. Reserved concurrency is useful for ensuring that your most critical functions always have enough concurrency to handle incoming requests.\n\nF: The data capacity of your stream is a function of the number of shards that you specify for the stream. The total capacity of the stream is the sum of the capacities of its shards.","poster":"GripZA","upvote_count":"2","comment_id":"1267534"},{"poster":"seetpt","comment_id":"1205553","timestamp":"1714654860.0","upvote_count":"2","content":"Selected Answer: ABF\nABF for me"},{"timestamp":"1714157280.0","upvote_count":"4","content":"Selected Answer: ABF\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html","comment_id":"1202774","poster":"c3518fc"},{"content":"ABF or ACF","timestamp":"1712394480.0","poster":"Ola2234","comment_id":"1190319","upvote_count":"1"},{"upvote_count":"3","content":"Selected Answer: ABF\nA,B,F,\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html","comment_id":"1186045","timestamp":"1711795500.0","poster":"WhyIronMan"},{"comment_id":"1183941","timestamp":"1711528680.0","upvote_count":"4","content":"Selected Answer: ABF\nhttps://aws.amazon.com/about-aws/whats-new/2019/11/aws-lambda-supports-parallelization-factor-for-kinesis-and-dynamodb-event-sources/","poster":"Seoyong"}],"answer_images":[],"choices":{"E":"Turn off the ReportBatchItemFailures setting in the Lambda event source mapping.","B":"Increase the ParallelizationFactor setting in the Lambda event source mapping.","D":"Increase the batch size in the Kinesis data stream.","F":"Increase the number of shards in the Kinesis data stream.","C":"Configure reserved concurrency for the Lambda function that processes the logs.","A":"Create a data stream consumer with enhanced fan-out. Set the Lambda function that processes the logs as the consumer."},"question_images":[],"question_text":"A company has an application that runs on AWS Lambda and sends logs to Amazon CloudWatch Logs. An Amazon Kinesis data stream is subscribed to the log groups in CloudWatch Logs. A single consumer Lambda function processes the logs from the data stream and stores the logs in an Amazon S3 bucket.\n\nThe company’s DevOps team has noticed high latency during the processing and ingestion of some logs.\n\nWhich combination of steps will reduce the latency? (Choose three.)","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/137340-exam-aws-certified-devops-engineer-professional-dop-c02/","exam_id":23,"answers_community":["ABF (100%)"],"question_id":132,"answer":"ABF"},{"id":"xcW5y2kn4x0SMx18AACv","unix_timestamp":1711559340,"isMC":true,"answer_ET":"B","answer_description":"","timestamp":"2024-03-27 18:09:00","discussion":[{"poster":"WhyIronMan","comment_id":"1186043","upvote_count":"5","timestamp":"1727685540.0","content":"Selected Answer: B\nB\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws_deny-ip.html"},{"comment_id":"1205554","poster":"seetpt","content":"Selected Answer: B\nB 100%","timestamp":"1730559660.0","upvote_count":"3"},{"poster":"c3518fc","timestamp":"1729968840.0","comment_id":"1202777","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws_deny-ip.html","upvote_count":"4"},{"upvote_count":"4","content":"Selected Answer: B\nanswer b\nuses an SCP within AWS Organizations to deny source IP addresses that are outside of the company’s IP address range, providing a centralized and organization-wide control over AWS actions based on source IP addresses for all accounts and resources within the organization.","poster":"dkp","timestamp":"1728804540.0","comment_id":"1194744"},{"poster":"ogerber","timestamp":"1727449740.0","comment_id":"1184240","content":"its B, 100%","upvote_count":"2"}],"answer_images":[],"choices":{"A":"Configure AWS Firewall Manager for the organization. Create an AWS Network Firewall policy that allows only source traffic from the company's IP address range. Set the policy scope to all accounts in the organization.","D":"In Organizations, create an SCP that allows source IP addresses that are inside of the company’s IP address range. Attach the SCP to the organization's root.","B":"In Organizations, create an SCP that denies source IP addresses that are outside of the company’s IP address range. Attach the SCP to the organization's root.","C":"Configure Amazon GuardDuty for the organization. Create a GuardDuty trusted IP address list for the company's IP range. Activate the trusted IP list for the organization."},"question_text":"A company operates sensitive workloads across the AWS accounts that are in the company's organization in AWS Organizations. The company uses an IP address range to delegate IP addresses for Amazon VPC CIDR blocks and all non-cloud hardware.\n\nThe company needs a solution that prevents principals that are outside the company’s IP address range from performing AWS actions in the organization's accounts.\n\nWhich solution will meet these requirements?","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/137362-exam-aws-certified-devops-engineer-professional-dop-c02/","topic":"1","exam_id":23,"answers_community":["B (100%)"],"question_id":133,"answer":"B"},{"id":"qIETT0wJprJtoOQT1AsJ","unix_timestamp":1711559640,"url":"https://www.examtopics.com/discussions/amazon/view/137363-exam-aws-certified-devops-engineer-professional-dop-c02/","answer_ET":"ADF","topic":"1","question_text":"A company deploys an application in two AWS Regions. The application currently uses an Amazon S3 bucket in the primary Region to store data.\n\nA DevOps engineer needs to ensure that the application is highly available in both Regions. The DevOps engineer has created a new S3 bucket in the secondary Region. All existing and new objects must be in both S3 buckets. The application must fail over between the Regions with no data loss.\n\nWhich combination of steps will meet these requirements with the MOST operational efficiency? (Choose three.)","timestamp":"2024-03-27 18:14:00","isMC":true,"question_id":134,"answer_images":[],"answer":"ADF","exam_id":23,"discussion":[{"upvote_count":"6","poster":"that1guy","content":"Selected Answer: ADF\nADF, \"All existing and new objects must be in BOTH S3 buckets.\" this requires two-way replication.","comment_id":"1212887","timestamp":"1715949240.0"},{"upvote_count":"5","timestamp":"1721624100.0","content":"Selected Answer: ADF\nNote: Application deployed to both regions, bi-directional replication will be required","comment_id":"1252830","poster":"d9iceguy"},{"comment_id":"1308215","timestamp":"1730945940.0","upvote_count":"2","content":"Selected Answer: ADF\nPoor wording. An active-active solution is recommended for HA, but bidirectional replication means CRR * 2. 'a two-way replication rule' is quite misleading","poster":"VerRi"},{"content":"Selected Answer: ACF\nNot D because it states creating the two-way replication on the source bucket and you need to configure it on both to work:\n\nWhen two-way replication is set up, a replication rule from the source bucket (DOC-EXAMPLE-BUCKET-1) to the bucket containing the replicas (DOC-EXAMPLE-BUCKET-2) is created. Then, a second replication rule from the bucket containing the replicas (DOC-EXAMPLE-BUCKET-2) to the source bucket (DOC-EXAMPLE-BUCKET-1) is created.","timestamp":"1726122360.0","upvote_count":"2","poster":"aws_god","comment_id":"1282491"},{"poster":"[Removed]","content":"Selected Answer: ADF\nADF here, because there is no mention of two way replication in C","timestamp":"1724158200.0","comment_id":"1269469","upvote_count":"2"},{"poster":"Trex247","upvote_count":"3","content":"Selected Answer: ADF\nI think it's ADF check out this:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/mrap-create-two-way-replication-rules.html","timestamp":"1723844820.0","comment_id":"1267343"},{"upvote_count":"2","timestamp":"1723528800.0","content":"Selected Answer: ACF\nTwo-way replication is possible using CRR.\n\n\"Replication is configured via rules. There is no rule for bi-directional replication. You will however setup a rule to replicate from the S3 bucket in the east AWS region to the west bucket, and you will setup a second rule to replicate going the opposite direction. These two rules will enable bi-directional replication across AWS regions.\"\n\n- https://catalog.workshops.aws/well-architected-reliability/en-US/4-failure-management/1-backup/20-bidirectional-replication-for-s3/2-configure-replication","poster":"everydaysmile","comment_id":"1265010"},{"upvote_count":"1","timestamp":"1722489120.0","comments":[{"comment_id":"1330438","content":"There is !\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/mrap-create-two-way-replication-rules.html#:~:text=page%2C%20choose%20the-,Replicate%20objects%20among%20all%20specified%20buckets,-template.%20The%20Replicate","poster":"CHRIS12722222","timestamp":"1734881880.0","upvote_count":"1"}],"comment_id":"1259201","poster":"hzaki","content":"there is nothing called (Create a two-way replication rule on the source S3 bucket), the two-way replication is configured separately in each region per each bucket, that's why option D is incorrect."},{"poster":"jamesf","upvote_count":"3","content":"Selected Answer: ADF\nOption D: two-way replication required","comment_id":"1258196","timestamp":"1722341880.0"},{"comment_id":"1255751","content":"Selected Answer: ADF\nD - for failover between regions. Any data stored on secondary bucket post failover operations needs to be replicated as well","poster":"auxwww","timestamp":"1722003660.0","upvote_count":"4"},{"poster":"xdkonorek2","comment_id":"1242682","content":"Selected Answer: ADF\n\"The application must fail over between the Regions with no data loss.\"\nC is not enough, because if we failover to region B and then to A application couldn't access data that was created in region B in the meantime","upvote_count":"4","timestamp":"1720173300.0"},{"content":"Two-Way Replication Rule is bidirectional, meaning objects are replicated from bucket A to bucket B and from bucket B to bucket A. This ensures that both buckets always contain the same data.\nS3 Cross-Region Replication (CRR) is unidirectional, meaning it replicates objects from a source bucket to a destination bucket. Changes made in the destination bucket do not propagate back to the source bucket.\nSo D, not C","timestamp":"1719896760.0","comment_id":"1240531","upvote_count":"2","poster":"6ef9a08"},{"poster":"seetpt","content":"Selected Answer: ACF\nACF for me","comment_id":"1205555","timestamp":"1714654920.0","upvote_count":"1"},{"comments":[{"content":"I agree. Does anyone have any reason why it wouldn't be B?","upvote_count":"2","poster":"Jay_2pt0_1","timestamp":"1715588280.0","comment_id":"1210763"}],"upvote_count":"3","poster":"MalonJay","content":"ADF\nThe secondary also needs to replicate to the primary.","comment_id":"1204783","timestamp":"1714525320.0"},{"comment_id":"1194755","timestamp":"1712993880.0","poster":"dkp","upvote_count":"2","content":"Selected Answer: ACF\nanswer acf"},{"timestamp":"1712486280.0","poster":"fdoxxx","comment_id":"1190901","content":"Selected Answer: ACF\nACF for sure. A - we need a replication role with principles for S3 Batch Operation, replicate job, and S3. C - will replicate all new objects, F - will replicate existing objects","upvote_count":"2"},{"upvote_count":"2","timestamp":"1711559640.0","poster":"ogerber","content":"Selected Answer: ACF\nits ACF for me","comment_id":"1184242"}],"choices":{"C":"Create an S3 Cross-Region Replication (CRR) rule on the source S3 bucket. Configure the rule to use the IAM role for Amazon S3 to replicate to the target S3 bucket.","A":"Create a new IAM role that allows the Amazon S3 and S3 Batch Operations service principals to assume the role that has the necessary permissions for S3 replication.","B":"Create a new IAM role that allows the AWS Batch service principal to assume the role that has the necessary permissions for S3 replication.","F":"Create an operation in S3 Batch Operations to replicate the contents of the source S3 bucket to the target S3 bucket. Configure the operation to use the IAM role for Amazon S3.","E":"Create an AWS Batch job that has an AWS Fargate orchestration type. Configure the job to use the IAM role for AWS Batch. Specify a Bash command to use the AWS CLI to synchronize the contents of the source S3 bucket and the target S3 bucket","D":"Create a two-way replication rule on the source S3 bucket. Configure the rule to use the IAM role for Amazon S3 to replicate to the target S3 bucket."},"answers_community":["ADF (73%)","ACF (28%)"],"answer_description":"","question_images":[]},{"id":"3ZY0bQGlbd43T2zTUyyC","choices":{"C":"Identify the resource that was not deleted. Manually empty the S3 bucket and then delete it.","B":"Add a custom resource with an AWS Lambda function with the DependsOn attribute specifying the S3 bucket, and an IAM role. Write the Lambda function to delete all objects from the bucket when RequestType is Delete.","D":"Replace the EC2 and S3 bucket resources with a single AWS OpsWorks Stacks resource. Define a custom recipe for the stack to create and delete the EC2 instance and the S3 bucket.","A":"Add a DelelionPolicy attribute to the S3 bucket resource, with the value Delete forcing the bucket to be removed when the stack is deleted."},"topic":"1","question_images":[],"isMC":true,"question_text":"An IT team has built an AWS CloudFormation template so others in the company can quickly and reliably deploy and terminate an application. The template creates an Amazon EC2 instance with a user data script to install the application and an Amazon S3 bucket that the application uses to serve static webpages while it is running.\nAll resources should be removed when the CloudFormation stack is deleted. However, the team observes that CloudFormation reports an error during stack deletion, and the S3 bucket created by the stack is not deleted.\nHow can the team resolve the error in the MOST efficient manner to ensure that all resources are deleted without errors?","exam_id":23,"answers_community":["B (100%)"],"answer_ET":"B","timestamp":"2023-04-05 04:53:00","discussion":[{"timestamp":"1722217020.0","content":"B is correct: \n- Cant delete S3 so must check S3\n- There are several DeletionPolition option in ACF: delete, retain, snapshot. For S3, even if there is delete flag, S3 can only be deleted if all objects are removed\nA: wrong - add delete flag to deleteionpolicy cant forcing deletion of S3\nC: should not manually do the task\nD: should not swap to AWS opsworks","poster":"thanhnv142","comment_id":"1134636","upvote_count":"7"},{"timestamp":"1702676940.0","content":"B. As per the AWS DeletionPolicy Options documentation it says, \"For Amazon S3 buckets, you must delete all objects in the bucket for deletion to succeed.\"\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html","comment_id":"924549","poster":"n_d1","upvote_count":"7"},{"comment_id":"1225936","upvote_count":"1","poster":"HarryLy","timestamp":"1733561040.0","content":"Selected Answer: B\nCloudformation does not have any behavior to force delete not empty bucket, need to invoke a custom lambda function to delete it"},{"timestamp":"1731319200.0","content":"Selected Answer: B\nKeyword \"Custom Resource\"","upvote_count":"1","comment_id":"1209651","poster":"c3518fc"},{"poster":"madperro","timestamp":"1702064160.0","comment_id":"918531","content":"Selected Answer: B\nB is a correct answer. A is wrong, you can't delete a bucket that has any objects.","upvote_count":"1"},{"content":"Selected Answer: B\nB. Add a custom resource with an AWS Lambda function with the DependsOn attribute specifying the S3 bucket, and an IAM role. Write the Lambda function to delete all objects from the bucket when RequestType is Delete.","upvote_count":"2","comment_id":"886916","poster":"haazybanj","timestamp":"1698893340.0"},{"upvote_count":"1","content":"B is the correct answer","timestamp":"1697316300.0","comment_id":"870469","poster":"alce2020"},{"poster":"ele","content":"Selected Answer: B\nBecause it's B. CFN will not delete non-empty bucket. It must be emptied first. Custom resource will do it.","comment_id":"864011","upvote_count":"3","timestamp":"1696691460.0"},{"upvote_count":"1","content":"Why not A?","comment_id":"861700","comments":[{"upvote_count":"1","comments":[{"content":"As per the linked article: \"For Amazon S3 buckets, you must delete all objects in the bucket for deletion to succeed.\"","timestamp":"1721203800.0","poster":"tallmantim","upvote_count":"2","comment_id":"1124877"}],"content":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html\ndeletion policy seems fine as well ...","poster":"tycho","comment_id":"879421","timestamp":"1698156720.0"}],"timestamp":"1696474380.0","poster":"lqpO_Oqpl"}],"url":"https://www.examtopics.com/discussions/amazon/view/105243-exam-aws-certified-devops-engineer-professional-dop-c02/","answer":"B","answer_description":"","question_id":135,"answer_images":[],"unix_timestamp":1680663180}],"exam":{"id":23,"lastUpdated":"11 Apr 2025","provider":"Amazon","numberOfQuestions":355,"isBeta":false,"isMCOnly":true,"isImplemented":true,"name":"AWS Certified DevOps Engineer - Professional DOP-C02"},"currentPage":27},"__N_SSP":true}