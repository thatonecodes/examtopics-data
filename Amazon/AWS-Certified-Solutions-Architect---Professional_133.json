{"pageProps":{"questions":[{"id":"EtBbAfxnYVaKK9mGUmzZ","question_images":[],"discussion":[{"upvote_count":"27","poster":"bbnbnuyh","comments":[{"comment_id":"240677","poster":"Cantaloupe","timestamp":"1632659160.0","upvote_count":"6","content":"Yes. There is burst option but it can be exhausted\n\"When using General Purpose SSD storage, your DB instance receives an initial I/O credit balance of 5.4 million I/O credits. This initial credit balance is enough to sustain a burst performance of 3,000 IOPS for 30 minutes.\""}],"comment_id":"216342","content":"A. 100G GP2 is going to give roughtly 300 IOPS which is too low","timestamp":"1632211800.0"},{"upvote_count":"11","comment_id":"284497","content":"Answer is A,\nKey is \"on the second day\", so all the credit have been used by then","timestamp":"1633685640.0","poster":"Ebi"},{"comment_id":"634589","poster":"5kk","timestamp":"1658406900.0","upvote_count":"1","content":"Selected Answer: A\nA looks good."},{"timestamp":"1651400340.0","content":"Selected Answer: A\nA looks good","comment_id":"595552","poster":"pankajrawat","upvote_count":"1"},{"poster":"AzureDP900","upvote_count":"1","comment_id":"492149","timestamp":"1638420720.0","content":"A is right"},{"poster":"backfringe","upvote_count":"1","comment_id":"488872","content":"I'd go with A","timestamp":"1638079800.0"},{"comments":[{"upvote_count":"2","poster":"Cal88","comment_id":"709100","timestamp":"1667304840.0","content":"An easy way to eliminate C is to read the question carefully\n“The application server logs show no evidence of database connectivity issues.“\nSo there are no connectivity issues \nI really hope that I read the questions carefully in my exam and not jump to any conclusions quickly"}],"comment_id":"488626","content":"Before even reading answers my best bet was A. I am with you guys, I am not sure why they want to fool us with C. This is one of the reason knowing concepts is very important rather than depending on answers :)","poster":"AzureDP900","timestamp":"1638061020.0","upvote_count":"1"},{"comment_id":"482977","upvote_count":"1","poster":"acloudguru","content":"Selected Answer: A\nThere is burst option but it can be exhausted\n\"When using General Purpose SSD storage, your DB instance receives an initial I/O credit balance of 5.4 million I/O credits. This initial credit balance is enough to sustain a burst performance of 3,000 IOPS for 30 minutes.\"","timestamp":"1637459940.0"},{"upvote_count":"1","comment_id":"446233","poster":"andylogan","timestamp":"1636234440.0","content":"It's A - exhausted all its initial I/O credits on the second day"},{"content":"AAA\n---","poster":"tgv","comment_id":"435260","upvote_count":"1","timestamp":"1636072800.0"},{"poster":"blackgamer","comment_id":"433928","content":"A is the answer.","upvote_count":"1","timestamp":"1635895980.0"},{"timestamp":"1635011820.0","upvote_count":"1","comment_id":"416873","poster":"jobe42","content":"A Emerging similar happened Touch us.. So I've learned shout EBS burstable balance"},{"comment_id":"413467","poster":"WhyIronMan","upvote_count":"1","timestamp":"1634980200.0","content":"I'll go with A"},{"comment_id":"366780","timestamp":"1634244720.0","upvote_count":"1","poster":"mustpassla","content":"A, a typical SAP IOPS related question"},{"comment_id":"357133","poster":"Waiweng","content":"it's A","timestamp":"1633834500.0","upvote_count":"3"},{"content":"going with A","comment_id":"294241","poster":"Kian1","upvote_count":"3","timestamp":"1633704720.0"},{"upvote_count":"2","comment_id":"256986","timestamp":"1633684140.0","content":"A is the right answer. It must have exhausted all its I/O credits due to the marketing event and now operates with 300 IOPS which is pretty low for that event.","poster":"Bulti"},{"timestamp":"1633394040.0","poster":"petebear55","comment_id":"256875","upvote_count":"1","content":"Changed to A after seeing this https://aws.amazon.com/blogs/database/how-to-use-cloudwatch-metrics-to-decide-between-general-purpose-or-provisioned-iops-for-your-rds-database/"},{"poster":"petebear55","content":"I believe the answer is C based on real time experience","upvote_count":"1","comments":[{"timestamp":"1651965720.0","comment_id":"598377","upvote_count":"1","content":"the answer is C, \nA is wrong, we have no info to prove that .","poster":"user0001"},{"content":"Issue happened in the \"second day\" so A is more likely.","timestamp":"1633761900.0","poster":"LisX","comment_id":"336454","upvote_count":"1"},{"comment_id":"433364","content":"The application server logs show no evidence of database connectivity issues.\n\nso eliminating C. I am going for A.","upvote_count":"1","poster":"Suresh108","timestamp":"1635414660.0"}],"timestamp":"1633137780.0","comment_id":"256870"},{"upvote_count":"1","poster":"T14102020","comment_id":"241610","timestamp":"1633068720.0","content":"For sure answer is A"},{"upvote_count":"2","comment_id":"233385","timestamp":"1632494760.0","poster":"jackdryan","content":"I'll go with A"},{"content":"A for sure","upvote_count":"1","poster":"gookseang","timestamp":"1632420000.0","comment_id":"219526"},{"poster":"keos","content":"A, most likely","upvote_count":"2","comment_id":"216416","timestamp":"1632327360.0"}],"url":"https://www.examtopics.com/discussions/amazon/view/36627-exam-aws-certified-solutions-architect-professional-topic-1/","choices":{"A":"It exhausted the I/O credit balance due to provisioning low disk storage during the setup phase.","D":"It exhausted the network bandwidth available to the RDS for MySQL DB instance.","C":"It exhausted the maximum number of allowed connections to the database instance.","B":"It caused the data in the tables to change frequently, requiring indexes to be rebuilt to optimize queries."},"timestamp":"2020-11-10 04:28:00","answer_description":"","answer":"A","isMC":true,"exam_id":32,"unix_timestamp":1604978880,"question_text":"An ecommerce website running on AWS uses an Amazon RDS for MySQL DB instance with General Purpose SSD storage. The developers chose an appropriate instance type based on demand, and configured 100 GB of storage with a sufficient amount of free space.\nThe website was running smoothly for a few weeks until a marketing campaign launched. On the second day of the campaign, users reported long wait times and time outs. Amazon CloudWatch metrics indicated that both reads and writes to the DB instance were experiencing long response times. The CloudWatch metrics show 40% to 50% CPU and memory utilization, and sufficient free storage space is still available. The application server logs show no evidence of database connectivity issues.\nWhat could be the root cause of the issue with the marketing campaign?","topic":"1","question_id":661,"answer_ET":"A","answer_images":[],"answers_community":["A (100%)"]},{"id":"js16KGhzk8j4wN5s0pcV","choices":{"B":"Create an AWS Snowball import job. Export a backup of the Oracle data warehouse. Copy the exported data to the Snowball device. Return the Snowball device to AWS. Create an Amazon RDS for Oracle database and restore the backup file to that RDS instance. Create an AWS DMS task to migrate the data from the RDS for Oracle database to Amazon Redshift. Copy daily incremental backups from Oracle in the data center to the RDS for Oracle database over the internet. Verify the data migration is complete and perform the cut over to Amazon Redshift.","A":"Install Oracle database software on an Amazon EC2 instance. Configure VPN connectivity between AWS and the company's data center. Configure the Oracle database running on Amazon EC2 to join the Oracle Real Application Clusters (RAC). When the Oracle database on Amazon EC2 finishes synchronizing, create an AWS DMS ongoing replication task to migrate the data from the Oracle database on Amazon EC2 to Amazon Redshift. Verify the data migration is complete and perform the cut over to Amazon Redshift.","D":"Create an AWS Snowball import job. Configure a server in the company's data center with an extraction agent. Use AWS SCT to manage the extraction agent and convert the Oracle schema to an Amazon Redshift schema. Create a new project in AWS SCT using the registered data extraction agent. Create a local task and an AWS DMS task in AWS SCT with replication of ongoing changes. Copy data to the Snowball device and return the Snowball device to AWS. Allow AWS DMS to copy data from Amazon S3 to Amazon Redshift. Verify that the data migration is complete and perform the cut over to Amazon Redshift.","C":"Install Oracle database software on an Amazon EC2 instance. To minimize the migration time, configure VPN connectivity between AWS and the company's data center by provisioning a 1 Gbps AWS Direct Connect connection. Configure the Oracle database running on Amazon EC2 to be a read replica of the data center Oracle database. Start the synchronization process between the company's on-premises data center and the Oracle database on Amazon EC2. When the Oracle database on Amazon EC2 is synchronized with the on-premises database, create an AWS DMS ongoing replication task to migrate the data from the Oracle database read replica that is running on Amazon EC2 to Amazon Redshift. Verify the data migration is complete and perform the cut over to Amazon Redshift."},"discussion":[{"content":"D is correct, you need Snowball for the size of the DB, SCT for converting from oracle to redshift and DMS for migration job\nhttps://aws.amazon.com/getting-started/hands-on/migrate-oracle-to-amazon-redshift/","poster":"liono","comments":[{"comments":[{"content":"great explain!","comment_id":"697536","poster":"sangkhuu","timestamp":"1666021560.0","upvote_count":"1"}],"content":"50TB to transfer\n- Transmitting over 50Mbps VPN ~ 90 days, not going to work\n- Transmitting over 1Gbps DX ~ 4.3 days, but ~ 60 days to provision circuit, not going to work\n\nA and C are automatically ruled out\n\n- Transmitting via Snowball (Edge) ~ 3-5 days, can hold up to 80TB usable disk, feasible\n\nBetween B and D, difference is around whether to use SCT and DMS to Snowball in your datacenter, then move to AWS. Or, copy to Snowball in data center, move to AWS, then do DMS WITHOUT SCT within AWS. Clearly, you need SCT to go from Oracle to Redshift, so it has to be D\n\nhttps://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/agents.dw.html","timestamp":"1636235460.0","comment_id":"463587","poster":"kirrim","upvote_count":"8"}],"upvote_count":"27","comment_id":"214578","timestamp":"1632122760.0"},{"content":"D. SCT is a must for converting Oracle DATA WAREHOUSE to Redshift","upvote_count":"5","comment_id":"274472","poster":"hkwong","timestamp":"1633485600.0"},{"poster":"sumaju","timestamp":"1702600800.0","content":"None of the options are correct here. It should be\n1. Ship the 1st full backup using snowball.\n2. Incremental backup using internet as data volume will be small.\nNeed SCT to move from Oracle to Redshift. So D is closer, but missed the incremental backup transportation.","comment_id":"1096939","upvote_count":"1"},{"poster":"Anhdd","timestamp":"1654148700.0","comment_id":"610455","content":"Selected Answer: A\nI wonder that if we choose Snow Ball as the solution. It's perfect, but the time when we ship back to AWS (1 week maybe), on-premis data are not being synced. While the question requires that \"guarantee that small daily updates are synced with the Amazon Redshift data warehouse\" ? How this can be complete?","upvote_count":"1"},{"content":"Selected Answer: D\nmark D","poster":"leoluo2020","timestamp":"1647690240.0","upvote_count":"3","comment_id":"571030"},{"comment_id":"499894","content":"For a Oracle database running in the AWS Cloud on \"Target architecture\" SCT is not printed.\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-oracle-database-to-amazon-redshift-using-aws-dms-and-aws-sct.html\nWith D the changes of the month will be lost.","poster":"palace","timestamp":"1639300140.0","upvote_count":"1"},{"upvote_count":"2","comment_id":"495732","timestamp":"1638862440.0","content":"D. Create an AWS Snowball import job. Configure a server in the companyג€™s data center with an extraction agent. Use AWS SCT to manage the extraction agent and convert the Oracle schema to an Amazon Redshift schema. Create a new project in AWS SCT using the registered data extraction agent. Create a local task and an AWS DMS task in AWS SCT with replication of ongoing changes. Copy data to the Snowball device and return the Snowball device to AWS. Allow AWS DMS to copy data from Amazon S3 to Amazon Redshift. Verify that the data migration is complete and perform the cut over to Amazon Redshift.","poster":"cldy"},{"upvote_count":"1","content":"I go with D","timestamp":"1638337260.0","comment_id":"491300","poster":"backfringe"},{"poster":"AzureDP900","timestamp":"1638061260.0","content":"I just thought about D and all the candidates mentioned same. I am getting ready for exam :)","comment_id":"488628","upvote_count":"1"},{"timestamp":"1637467380.0","content":"Selected Answer: D\nsince it is Oracle to Redshit, it needs SCT. scan for the key word SCT and answer is D. 50T through network is impossible for A to finish in 30days, snowball is a must.","upvote_count":"1","comment_id":"483003","poster":"acloudguru"},{"poster":"moon2351","upvote_count":"1","content":"Answer is D","comment_id":"449201","timestamp":"1636008960.0"},{"poster":"andylogan","upvote_count":"2","content":"It's D - for Oracle to Redshit, it needs SCT","comment_id":"446234","timestamp":"1635910800.0"},{"timestamp":"1635659820.0","poster":"DerekKey","upvote_count":"1","comment_id":"439131","content":"Requirements:\n1. \"the data warehouse only receives minor daily updates and is primarily used for reading and reporting\"\n2. \"ensure that the minor daily changes have been synchronized with the\nAmazon Redshift data warehouse\"\n\nD - how would you make it working if Snowball will travel to AWS for 3-4 days and 1 day more will be spent on restoring database?"},{"content":"DDD\n---","comment_id":"436535","timestamp":"1635530940.0","upvote_count":"1","poster":"tgv"},{"comment_id":"433934","upvote_count":"1","poster":"blackgamer","timestamp":"1635512520.0","content":"Only D makes sense, but the solution is not written very clearly."},{"upvote_count":"1","content":"This is question is so long and big as 50TB. :D\n\nsince it is Oracle to Redshit, it needs SCT. scan for the key word SCT and answer is D.","timestamp":"1635424020.0","poster":"Suresh108","comment_id":"433366"},{"poster":"denccc","comment_id":"428281","upvote_count":"1","timestamp":"1635120420.0","content":"D: https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/agents.dw.html"},{"poster":"WhyIronMan","timestamp":"1634692860.0","content":"I'll go with D","upvote_count":"1","comment_id":"413468"},{"content":"B\nFor all of those answering D, SCT and DMS are installed on-prem. So, what happens when the Snowball device send back to Amazon? It takes a week for the Snowball device to reach Amazon. So, how data is synced up between on-prem and AWS during that week?\nOnly B provides a solution for continuous data synchronization till the cutoff happens.","upvote_count":"1","comment_id":"397831","poster":"DashL","comments":[{"poster":"DerekKey","timestamp":"1635637320.0","upvote_count":"1","comments":[{"timestamp":"1649336400.0","upvote_count":"1","content":"Finally sombody is paying attention.\nBTW, not sure why they need to move data to EC2 as it can be directly converted to Redshift.... I mean it's not optimal idea.","comment_id":"582449","poster":"sashsz"}],"content":"I agree with you. The only element that i dont agree is one week for a snowball transfer back to AWS. t should tak 3 days :)","comment_id":"439115"}],"timestamp":"1634446380.0"},{"upvote_count":"1","content":"B\nFor all of those answering D, SCT and DMS are installed on-prem. So, what happens when the Snowball device send back to Amazon? It takes a week for the Snowball device to reach Amazon. So, how data is synced up between on-prem and AWS during that week?\nOnly B provides a solution for continuous data synchronization till the cutoff happens.","comments":[{"content":"B. Does not work. Need SCT. SCT schema can be installed on Redshift as soon as it’s run. Data sync / Snowball delivery can follow.","poster":"Pb55","upvote_count":"2","timestamp":"1634541600.0","comment_id":"398316"}],"comment_id":"397830","timestamp":"1634326560.0","poster":"DashL"},{"upvote_count":"4","timestamp":"1634155320.0","poster":"Waiweng","comment_id":"357138","content":"it's D"},{"comment_id":"294245","timestamp":"1633825560.0","poster":"Kian1","upvote_count":"2","content":"going with D"},{"comments":[{"comment_id":"301190","upvote_count":"1","content":"D is correct, only data sync using 50Mbps link.","timestamp":"1634077620.0","poster":"jayakumarchellam"}],"poster":"PredaOvde","upvote_count":"1","content":"If there is a 50Mbps connection speed, it's possible to migrate ~0.5TB per day, which means ~15TB in 30 days. The requirement is 50TB in 30 days, which means D is not an option. I vote B instead.","timestamp":"1633632780.0","comment_id":"293228"},{"timestamp":"1633624200.0","comment_id":"284498","poster":"Ebi","content":"I go with D","upvote_count":"4"},{"comments":[{"poster":"Bulti","timestamp":"1633396500.0","upvote_count":"4","content":"SnowBall Edge and Snowball are all referred to as Snowball Device so I will change the answer to D.","comment_id":"269712"}],"poster":"Bulti","comment_id":"257001","timestamp":"1632895860.0","upvote_count":"1","content":"I think the answer is B. The only reason I won't go with D is because it doesn't mention Snowball Edge but instead just Snowball. To use DMS and SCT, Snowball Edge is required and just Snowball Device won't work. There was another question similar to this between 200 and 300 and there was a similar confusion."},{"poster":"T14102020","timestamp":"1632712800.0","upvote_count":"1","content":"D is correct answer.","comment_id":"241622"},{"comment_id":"234029","upvote_count":"3","timestamp":"1632681660.0","poster":"cloudgc","content":"D - https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/agents.dw.html"},{"comment_id":"233387","upvote_count":"2","content":"I'll go with D","timestamp":"1632371940.0","poster":"jackdryan"},{"comment_id":"219534","content":"D for sure","upvote_count":"1","poster":"gookseang","timestamp":"1632358860.0"}],"answer":"D","exam_id":32,"unix_timestamp":1604749620,"url":"https://www.examtopics.com/discussions/amazon/view/36364-exam-aws-certified-solutions-architect-professional-topic-1/","answer_ET":"D","answers_community":["D (80%)","A (20%)"],"isMC":true,"timestamp":"2020-11-07 12:47:00","topic":"1","answer_description":"","answer_images":[],"question_id":662,"question_text":"A solutions architect has been assigned to migrate a 50 TB Oracle data warehouse that contains sales data from on-premises to Amazon Redshift. Major updates to the sales data occur on the final calendar day of the month. For the remainder of the month, the data warehouse only receives minor daily updates and is primarily used for reading and reporting. Because of this, the migration process must start on the first day of the month and must be complete before the next set of updates occur. This provides approximately 30 days to complete the migration and ensure that the minor daily changes have been synchronized with the\nAmazon Redshift data warehouse. Because the migration cannot impact normal business network operations, the bandwidth allocated to the migration for moving data over the internet is 50 Mbps. The company wants to keep data migration costs low.\nWhich steps will allow the solutions architect to perform the migration within the specified timeline?","question_images":[]},{"id":"Uu85ZujYgNg4q1L4htMv","answer":"AD","answer_images":[],"isMC":true,"question_id":663,"timestamp":"2020-11-08 10:18:00","answer_ET":"AD","topic":"1","exam_id":32,"discussion":[{"content":"Question is asking for: RTO of 30 minutes and an RPO of 5\nRPO RTO -> mode\n24 24hr -> backup\n12 4hr -> pilot light\n1.4 15min -> warm standup\n15min 5min -> active-active\nB because of above ^\nD is obvious","poster":"cpd","upvote_count":"28","comments":[{"comment_id":"439476","content":"A,D\nRPO/RTO is for the data tier.","timestamp":"1636049100.0","poster":"student22","upvote_count":"4"},{"poster":"tgv","comments":[{"poster":"MikeyJ","comment_id":"647064","upvote_count":"1","content":"My thinking too. If it hadn't specifically mentioned costs I would have said B.","timestamp":"1660547700.0"}],"comment_id":"435264","content":"As this might be the general best practice, the question is asking to optimize costs and I think we can easily achieve the RTO / RPO with option A","upvote_count":"4","timestamp":"1635851160.0"}],"timestamp":"1632530940.0","comment_id":"219199"},{"content":"I go with AD","timestamp":"1634604300.0","comment_id":"284426","poster":"Ebi","upvote_count":"23"},{"comment_id":"821297","timestamp":"1677313380.0","upvote_count":"2","content":"Selected Answer: AD\nPilot light (RPO in minutes, RTO in tens of minutes): Provision a copy of your core workload infrastructure in the recovery Region. Replicate your data into the recovery Region and create backups of it there. Resources required to support data replication and backup, such as databases and object storage, are always on. Other elements such as application servers or serverless compute are not deployed, but can be created when needed with the necessary configuration and application code.\nhttps://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/rel_planning_for_recovery_disaster_recovery.html","poster":"andras"},{"comment_id":"761098","content":"Selected Answer: BD\nI'll go with B and D","poster":"evargasbrz","upvote_count":"1","timestamp":"1672322940.0"},{"timestamp":"1665296220.0","poster":"JohnPi","upvote_count":"2","content":"Selected Answer: BD\nhttps://aws.amazon.com/blogs/architecture/disaster-recovery-dr-architecture-on-aws-part-iii-pilot-light-and-warm-standby/","comment_id":"689933"},{"comment_id":"687148","upvote_count":"3","timestamp":"1664995560.0","content":"I will go with A/D.\nThe question highlights \"optimizing costs\". B - Hot standby would work but is more expensive.\nRTO and RPO is for data and D covers that.","poster":"psou7"},{"content":"Because of \"The application and web layers are stateless\" so dont have any data stored in EC2 Instance. If application dont have multiple deployments in a day, option A is cost effective. Cross-Region Aurora will effect to RPO/RTO and meet requirements. --> AD are best options!","poster":"Kyperos","timestamp":"1660873980.0","upvote_count":"3","comment_id":"648685"},{"content":"C and D is right answer.\nA is non sense 24 hours \"DAILY\"","upvote_count":"1","timestamp":"1658974320.0","comment_id":"638388","poster":"hilft"},{"content":"A is wrong. daily snapshot won't be enough for 30min/5min","comment_id":"638387","upvote_count":"1","poster":"hilft","timestamp":"1658974260.0"},{"upvote_count":"1","timestamp":"1656729000.0","poster":"aandc","content":"AD, RTO & RPO only for Data tier,","comment_id":"625902"},{"comment_id":"623796","upvote_count":"5","content":"Selected Answer: AD\nAD for me\nB will work but it's too expensive cause you have active-active model, and the RTO and RPO within only minute, while the question say that it's can up to 30 minutes. A will work and more cost effective","timestamp":"1656398940.0","poster":"TechX"},{"poster":"gorodetsky","upvote_count":"3","timestamp":"1647783720.0","content":"Selected Answer: BD\nB,D https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/plan-for-disaster-recovery-dr.html","comment_id":"571623"},{"upvote_count":"2","content":"I go with BD\nhttps://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/plan-for-disaster-recovery-dr.html","poster":"good_tea","comment_id":"555729","timestamp":"1645759380.0"},{"poster":"lifebegins","content":"Answer is B & D\nHot Standby is the correct answer: \n\nBecause, if we have fleet of EC2 Instances, which are stateless, why even we are taking snapshots. Suppost, if we have 5 instance in app later, 10 instances in BL, what is the use of taking the snapshot of the disk of App Layes which is stateless, instead of that, we can maintain thin layer of Hot Standby 1 instance in Web, 1 instance in BL behind autoscaling group with Cross Replication of Aurora, we can bring the entire layer with in few minutes by standing up the instance by Cloud Formation with the DR database:\n\nhttps://www.wellarchitectedlabs.com/reliability/disaster-recovery/workshop_4/\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database-disaster-recovery.html","upvote_count":"2","timestamp":"1644489000.0","comment_id":"544451"},{"content":"Will go with B and D. The host standby solves the 30min RTO and the DB cross replication solves the 5min RPO","upvote_count":"2","comment_id":"527871","timestamp":"1642622220.0","poster":"Ni_yot"},{"timestamp":"1641680040.0","poster":"CloudChef","content":"B and D as stated on Digital Cloud Training.","comment_id":"519794","upvote_count":"2"},{"comment_id":"488633","upvote_count":"1","content":"I'll go with A,D","poster":"AzureDP900","timestamp":"1638061860.0"},{"content":"The prob with A to be copied this snapshot in 5 min?","upvote_count":"2","timestamp":"1637232180.0","comment_id":"480615","poster":"Kopa"},{"content":"BBB D. Same exact question appears on the Tutorials Dojo test which has moderators and good credibility and their answer is [B]D with the explanation that it may take longer than 30 min to get the snapshot back up and running especially if there is manual intervention. Picture that a regional outage happens in the middle of the night on a weekend or holiday. What is the likelihood that someone can get an EC2 instance restored from a snapshot and fully operational from the time the region goes out. If you were the person responsible for that 30 min RTO SLA would you put YOUR JOB on the line? I too was inclined to select AD but the more I understand the explanation I can agree with BD.","comments":[{"timestamp":"1642446780.0","poster":"ppandey96","upvote_count":"1","comment_id":"526026","content":"RPO and RTO is for data layer not application"},{"timestamp":"1636837440.0","poster":"sashenka","comment_id":"477741","content":"And to make the 30 min RTO window even more critical is this, \"application and web layers are stateless and run on an Amazon EC2 fleet of instances\". Can one truly recover an EC2 fleet for both the Web and App tiers(we don't know how many but it sounds like more than a couple\" from a snapshot within 30 min from the time we have a failure?","upvote_count":"1","comments":[{"upvote_count":"1","timestamp":"1636838700.0","poster":"sashenka","comments":[{"content":"Obviously, you don't understand what ppandey96 mean by his/her comment. Please read carefully.","upvote_count":"1","poster":"tomosabc1","timestamp":"1665604200.0","comment_id":"693354"}],"content":"Take a look at the following guidance:\nhttps://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/plan-for-disaster-recovery-dr.html\nWhat [A][D] describe is a Pilot light (RPO in minutes, RTO in hours) which DOES NOT MEET THE REQUIREMENTS: Replicate your data from one region to another and provision a copy of your core workload infrastructure. Resources required to support data replication and backup such as databases and object storage are always on. Other elements such as application servers are loaded with application code and configurations, but are switched off and are only used during testing or when Disaster Recovery failover is invoked.\n\nWhat's needed here is Warm standby (RPO in seconds, RTO in minutes) but since that is not available the only valid option is [B][D] .","comment_id":"477754"}]}],"comment_id":"477738","poster":"sashenka","timestamp":"1636837200.0","upvote_count":"3"},{"comment_id":"468379","timestamp":"1636146540.0","content":"Why not E instead of B to ensure there is a copy of the web AMI/snapshot in another region? It's a lot easier to configure AWS backup and can set custom schedules etc","upvote_count":"1","poster":"undecided"},{"poster":"andylogan","comment_id":"446246","upvote_count":"1","timestamp":"1636101960.0","content":"It's A D - 30 min to recover the snapshot."},{"timestamp":"1636057200.0","poster":"student22","comment_id":"442396","upvote_count":"2","content":"A,D\nB will work but A is more cost effective."},{"timestamp":"1635979620.0","poster":"DerekKey","comments":[{"upvote_count":"1","content":"Trouble is it's not ONE server! \"Application and web layers are stateless and run on an Amazon EC2 FLEET of instances\". Go ahead and restore 20-30 EC2 instances from a snapshot and see how long it takes.","comment_id":"477742","poster":"sashenka","timestamp":"1636837800.0"}],"upvote_count":"2","content":"A correct - if an application will be changed it is good to have its most current release\nB wrong - not needed, even for largest instances initialization of a new server will be shorter than 10 minutes\nC wrong - doesn't mention another Region btw. have you ever done snapshot on a database with this size. Restrictions:\n- automatic backup frequency - 1 day\n- manual snapshots - 100 cluster snapshots and 100 db snapshots. It can be adjusted but the answer is not mentioning it. \nD correct\nE wrong - is not usable in this scenario","comment_id":"439153"},{"content":"AAA DDD\n---","comment_id":"435262","timestamp":"1635785340.0","upvote_count":"1","poster":"tgv"},{"comment_id":"428282","upvote_count":"1","content":"A and D","timestamp":"1635697440.0","poster":"denccc"},{"poster":"bbcl","upvote_count":"2","content":"1.\"The application has an RTO of 30 minutes and an RPO of 5 minutes for the data tier\" only! so A and B is not related to requirement.\n2. with C cross-Region Aurora Replica of the database will have entire copy of DB (by Arura is not selected tables) to another region so that conflict to E: Create an AWS Backup job to replicate data to another Region that is not necessary!\n3. if only replication is not able to restore from data corruption because BAD data is sync to read replica. 5mins a backup snapshot is needed.\nanswer should be CD","timestamp":"1635572160.0","comment_id":"414612"},{"timestamp":"1635537960.0","content":"while optimizing costs\n\nI'll go with A,D","poster":"WhyIronMan","upvote_count":"3","comment_id":"413469"},{"content":"I support A,D. \n\nA more cost efficent and lot of time 30 min to recover the snapshot.","upvote_count":"1","timestamp":"1635488520.0","poster":"Kopa","comment_id":"411462"},{"upvote_count":"5","content":"web and apps Tier - stateless\nRPO and RTO for data Tier \nso answer is AD","comment_id":"382528","poster":"tvs","timestamp":"1635430200.0"},{"timestamp":"1635416160.0","upvote_count":"1","comment_id":"377390","poster":"zolthar_z","content":"The answer is A and D, the cost is for the EC2 instances, is cheaper a snapshot than a cold infrastructure. The 5 minutes RPO is for the DB."},{"content":"it's B and D","poster":"Waiweng","comment_id":"357141","timestamp":"1635339840.0","upvote_count":"4"},{"poster":"Kayode","comment_id":"338339","content":"The answer is BD","upvote_count":"3","timestamp":"1634951520.0"},{"comment_id":"295576","content":"Hot standby (Multi-site) will have the highest costs. Solutions is asking to optimise costs. Cross region Snapshots will suffice for a 30min RTO. Creating snapshots of Aurora every 5 minutes will not work as it could take hours to copy it cross region. Correct answer is A and D.","poster":"Joaster","comments":[{"comment_id":"339565","upvote_count":"1","poster":"mijeko8879","content":"this is a 3 teer application, NOT just EC2 ! -> B, D","timestamp":"1634953920.0","comments":[{"poster":"mijeko8879","content":".. 3 tier .. (sorry for the typo)","upvote_count":"1","comment_id":"339566","timestamp":"1635098700.0"}]}],"upvote_count":"1","timestamp":"1634696040.0"},{"content":"will go with AD","upvote_count":"4","poster":"Kian1","timestamp":"1634605920.0","comment_id":"294252"},{"timestamp":"1634525580.0","comment_id":"281101","content":"I'm confused as there is no guarantee if choosing A) to automatically detect and use those instances.... so not sure if A) or B)... I think it's clear D).... I'd go by optimizing costs only by A) and D)....","poster":"Mr_D130","upvote_count":"1"},{"content":"I would prefer C, E:\nC - 5 mins RPO; \nE - cross region disaster recovery","comment_id":"264051","upvote_count":"1","comments":[{"comments":[{"timestamp":"1634335440.0","comment_id":"271402","poster":"01037","upvote_count":"1","content":"But D may be a faster way to recovery, but there is no reference saying that how long it takes."},{"timestamp":"1634393160.0","content":"I don't understand A. Why is daily snapshot needed?","poster":"01037","comment_id":"271404","upvote_count":"2"}],"comment_id":"271398","timestamp":"1634324280.0","poster":"01037","upvote_count":"1","content":"Agree. most cost effective.\n\nOnly one concern, that how long it'll take to bring DB up.\nWeb tier is stateless, so as long as the AMI exists in DR Region, I think it can be setup within 30 minutes."}],"timestamp":"1634108820.0","poster":"MichaelHuang"},{"comment_id":"257019","upvote_count":"6","poster":"Bulti","content":"I will go with A and D. A instead of B due to the cost and through automation and due to statelessness of the ap and web tier it may be possible to recover the app in 30 mins.","timestamp":"1633740420.0"},{"content":"Agree with cpd. Answer BD for 5 min RTO","comment_id":"241629","upvote_count":"1","timestamp":"1633246560.0","poster":"T14102020"},{"comment_id":"240626","upvote_count":"1","poster":"Cantaloupe","content":"Between C and D\n\"Amazon Aurora automatically maintains 6 copies of your data across 3 Availability Zones and will automatically attempt to recover your database in a healthy AZ with no data loss. In the unlikely event your data is unavailable within Amazon Aurora storage, you can restore from a DB Snapshot or perform a point-in-time restore operation to a new instance. Note that the latest restorable time for a point-in-time restore operation can be up to 5 minutes in the past.\"\n\nObviously no need to take snapshots as it's automatic\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html\n\nBut then what's point of Replica.","timestamp":"1632904380.0","comments":[{"content":"A: daily snapshot of stateless apps does not make sense\nB: what's 'hot standby'. There are 'warm standby' and 'Pilot light'","upvote_count":"2","poster":"Cantaloupe","timestamp":"1633137960.0","comment_id":"240632"}]},{"timestamp":"1632716700.0","content":"I'll go with A,D","comments":[{"upvote_count":"4","comments":[{"poster":"gookseang","upvote_count":"1","content":"I agree with you ~~","timestamp":"1634509320.0","comment_id":"278738"}],"poster":"arulrajjayaraj","content":"A & D - Key point of requirement is - RTO & RPO only for Data tier","timestamp":"1633231320.0","comment_id":"240652"}],"upvote_count":"3","comment_id":"233396","poster":"jackdryan"},{"content":"B & D since daily ec2 snapshots dont guarentee RTO of 30 mins","comments":[{"content":"Mate the EC2 incidents dont change .. its the db .. so daily ec2 back up is more than enough","comment_id":"256881","upvote_count":"1","timestamp":"1633659420.0","poster":"petebear55"},{"comment_id":"229340","comments":[{"comment_id":"433377","timestamp":"1635730800.0","upvote_count":"1","content":"what happens if app code was updated 6 hours back?? \n\nI am going for BD.","poster":"Suresh108"},{"comment_id":"239237","timestamp":"1632731160.0","poster":"PAUGURU","content":"How can you know if you need to go to DR?? It doesn't state that you have someone monitoring the service 24x7. If a region fails e.g. at 3 AM in 30 minutes you have to be up and ready, with snapshots it will take you a few hours just to realize the region is gone and you have to activate DR, then you start launching instances, if you are up and running by 9 AM you are lucky. \nIt's B and D to guarantee RTO of 30 mins.","upvote_count":"1","comments":[{"upvote_count":"1","content":"Daily backups will capture any application changes made by the developers (although daily is probably overkill). As stated by Santya, the actual DR process can be automated with Lambda and an ALB using health checks. That's not part of the question, though. A & D make the most sense.","comment_id":"283698","poster":"Trap_D0_r","timestamp":"1634533440.0"},{"timestamp":"1633811940.0","comment_id":"257192","content":"My to cents.. B & D are correct.. question asks for optimizing costs..\nYou can automate the restore job i.e. to build the app servers from snapshots via cloudformation.. you test this before and when DR event is called out, run the cloudformation. Even execution of cloudformation can be automated..","upvote_count":"1","poster":"Santya"},{"timestamp":"1634086980.0","poster":"Santya","upvote_count":"1","content":"My two cents.. A & D are correct.. question asks for optimizing costs..\nYou can automate the restore job i.e. to build the app servers from snapshots via cloudformation.. you test this before and when DR event is called out, run the cloudformation. Even execution of cloudformation can be automated..","comment_id":"257196"}]}],"content":"Nope...the web and app instances are stateless. This means they don't hold any data. You can use the latest snapshot to restore. Also there is no RPO for the web and app tier. Cost is the key here. A & D are correct answers.","poster":"darthvoodoo","upvote_count":"6","timestamp":"1632576780.0"}],"timestamp":"1632536040.0","poster":"bbnbnuyh","upvote_count":"1","comment_id":"220863"},{"comment_id":"219120","poster":"XRiddlerX","content":"Answer is A and D\nI actually tried to restore a 20TB Aurora DB to another region and it took about 1.5 hours to create the cluster and restore the DB. I found this AWS premium support article confirming it can take hours. With that, the RTO requirement would not be met.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/aurora-mysql-slow-snapshot-restore/#:~:text=This%20can%20take%20up%20to,copies%20on%20your%20three%20AZs.","timestamp":"1632432300.0","upvote_count":"3"},{"comments":[{"upvote_count":"1","poster":"MarkDillon1075","comment_id":"217289","content":"cost is the key. snapshots are cheaper","timestamp":"1632295860.0"},{"upvote_count":"1","content":"I would normally agree but this question refers to Auroa which is a bit different ... answer B is a typical Red herring answer Amazon throw in there to trip up us poor persecuted students of Aws","timestamp":"1633544160.0","comment_id":"256878","poster":"petebear55"}],"comment_id":"216459","upvote_count":"2","poster":"keos","content":"given stateless EC2 instances, a hot stand may be better","timestamp":"1632149520.0"},{"content":"A and D seems to be correct","timestamp":"1632069420.0","poster":"liono","upvote_count":"5","comment_id":"215119"}],"url":"https://www.examtopics.com/discussions/amazon/view/36443-exam-aws-certified-solutions-architect-professional-topic-1/","question_images":[],"question_text":"A solutions architect is designing a disaster recovery strategy for a three-tier application. The application has an RTO of 30 minutes and an RPO of 5 minutes for the data tier. The application and web tiers are stateless and leverage a fleet of Amazon EC2 instances. The data tier consists of a 50 TB Amazon Aurora database.\nWhich combination of steps satisfies the RTO and RPO requirements while optimizing costs? (Choose two.)","choices":{"E":"Create an AWS Backup job to replicate data to another Region.","A":"Create daily snapshots of the EC2 instances and replicate the snapshots to another Region.","C":"Create snapshots of the Aurora database every 5 minutes.","B":"Deploy a hot standby of the application to another Region.","D":"Create a cross-Region Aurora Replica of the database."},"unix_timestamp":1604827080,"answer_description":"","answers_community":["AD (54%)","BD (46%)"]},{"id":"w1LkmLBDt6noqNeR666F","question_text":"A company has a primary Amazon S3 bucket that receives thousands of objects every day. The company needs to replicate these objects into several other S3 buckets from various AWS accounts. A solutions architect is designing a new AWS Lambda function that is triggered when an object is created in the main bucket and replicates the object into the target buckets. The objects do not need to be replicated in real time. There is concern that this function may impact other critical\nLambda functions due to Lambda's regional concurrency limit.\nHow can the solutions architect ensure this new Lambda function will not impact other critical Lambda functions?","answer":"A","isMC":true,"answer_images":[],"topic":"1","exam_id":32,"answer_ET":"A","question_images":[],"discussion":[{"upvote_count":"25","timestamp":"1632070140.0","comment_id":"215118","content":"A is correct, \nhttps://aws.amazon.com/blogs/compute/managing-aws-lambda-function-concurrency/","comments":[{"poster":"cpd","upvote_count":"1","content":"Thank you, very nice blog.","timestamp":"1632179820.0","comment_id":"219205"},{"upvote_count":"2","comment_id":"246063","timestamp":"1632474660.0","content":"Thankyou, very informative blog!\nA is correct","poster":"rscloud"},{"upvote_count":"1","timestamp":"1660981920.0","comment_id":"649327","content":"But those other Lambda functions would now be running in different accounts where the target buckets are, hence there would be no impact on the 'key' Lambdas in the main account. Also by using Lambda to process SQS, it will pull multiple messages off the queue at once, instead of firing up a new concurrent Lambda for every object that needs to be copied. Lastly, if you set a Reserved Concurrency limit on the Lambdas that process these large quantities of S3 uploads, and the limit is hit, you will start losing data and your buckets will be out of sync. I think just using Reserved Concurrency is too simple here; they are looking for one step further.","poster":"gerhardbl"},{"upvote_count":"2","poster":"kirrim","timestamp":"1636262160.0","comment_id":"463596","content":"Agree!\n\nAnother document supporting A:\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html\n\n\"Your function can't scale out of control – Reserved concurrency also limits your function from using concurrency from the unreserved pool, which caps its maximum concurrency. You can reserve concurrency to prevent your function from using all the available concurrency in the Region, or from overloading downstream resources.\""}],"poster":"liono"},{"comments":[{"upvote_count":"2","timestamp":"1644288960.0","poster":"cannottellname","comment_id":"542829","content":"I believe SQS queue + Lamda Reserve will help solve the issue. Not keeping Lambda limit/reserve have a chance that concurrency is fully utilized by this only."}],"content":"I choose C. Let me explain why I don't think A is the right approach: if you set Reserved concurrency to let's say 200, I make myself 2 questions:\n1) 800 instances will remain available for other lambdas. How do you guarantee 800 is enough for other lambdas? Perhaps 999 lambdas was answering in a timely manner to all the requests and adding a new lambda will break everything.\n2) Assigning 200 Reserved instances to the new Lambda does not guarantee that will be enough for the new Lambda. Maybe it requires 500? \n\nSo, adding SQS queue resolve both of the problems. Thoughts?","comment_id":"289791","timestamp":"1632669720.0","poster":"PredaOvde","upvote_count":"15"},{"content":"Selected Answer: A\nProblem with option D is it will trigger every time a message is received in SQS, which is similar to trigger Lambda function through s3 event notification. If Lambda is designed to process in Batch, then D will be more appropriate, but there will be a risk of Lambda timeout. So A should be a better choice.","upvote_count":"1","comment_id":"1096955","timestamp":"1702602600.0","poster":"sumaju"},{"comment_id":"944120","upvote_count":"2","content":"Selected Answer: A\nA\naccording to this youtube explanation:\nhttps://youtu.be/kNtOhcpju6g","poster":"SkyZeroZx","timestamp":"1688591160.0"},{"poster":"dev112233xx","comment_id":"872784","timestamp":"1681741560.0","content":"Selected Answer: A\nA\naccording to this youtube explanation:\nhttps://youtu.be/kNtOhcpju6g","upvote_count":"1"},{"comment_id":"773403","timestamp":"1673523960.0","content":"Selected Answer: A\nI am going to go with A on this one. Reserved concurrency because even with SQS you will still use up the allowed concurrency.","poster":"syaldram","upvote_count":"2"},{"content":"Selected Answer: A\nThis is what reserved concurrency is for","comment_id":"721419","upvote_count":"2","timestamp":"1668789480.0","poster":"timmysixstrings"},{"content":"Selected Answer: A\nStraight from Jon Bonso tests and correct answer is A.\nExplanation:\n\nConcurrency is the number of requests that your function is serving at any given time. When your function is invoked, Lambda allocates an instance of it to process the event. When the function code finishes running, it can handle another request. If the function is invoked again while a request is still being processed, another instance is allocated, which increases the function's concurrency. Concurrency is subject to a Regional quota that is shared by all functions in a Region.\n\nThere are two types of concurrency available:\n\nReserved concurrency – Reserved concurrency creates a pool of requests that can only be used by its function, and also prevents its function from using unreserved concurrency.\n\nProvisioned concurrency – Provisioned concurrency initializes a requested number of execution environments so that they are prepared to respond to your function's invocations.","comment_id":"716006","poster":"Amsa","upvote_count":"2","timestamp":"1668166080.0"},{"timestamp":"1662386820.0","upvote_count":"2","content":"Selected Answer: C\n\"The objects do not need to be processed in real-time\" - SQS gives you the option to process in batches, you can also delay delivery to ensure Lambda isn't swarmed with multiple concurrent executions from the object upload.","comment_id":"660250","poster":"astalavista1"},{"comment_id":"624998","content":"Key point, addition of this new Lambda function has no adverse effect on other key Lambda functions? So how does C address that? Only A provides and answer with reserved concurrency","upvote_count":"2","poster":"jyrajan69","timestamp":"1656556380.0"},{"content":"Answer is A.\n\nC is wrong because the Lamdba functions which read the message from SQS may scale out to 1000 if hundreds of thousands of upload occur in a very short time. It will impact the other Lamdba functions.\n\nRefer to https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html","poster":"azurehunter","upvote_count":"1","timestamp":"1655029860.0","comment_id":"615262"},{"poster":"jj22222","upvote_count":"2","timestamp":"1648585260.0","comment_id":"577853","content":"Selected Answer: C\nC. Configure S3 event notifications to add events to an Amazon SQS queue in a separate account. Create the new Lambda function in the same account as the SQS queue and trigger the function when a message arrives in the queue."},{"comment_id":"547895","timestamp":"1644945120.0","poster":"pititcu667","upvote_count":"1","content":"Selected Answer: A\nI think A is correct."},{"comment_id":"532393","timestamp":"1643142540.0","poster":"shotty1","upvote_count":"2","content":"Selected Answer: C\nI think this is C"},{"timestamp":"1642675980.0","upvote_count":"2","content":"C is right - The concurrency space available is common for all the functions in the region. By reserving concurrency for a function we ensure no other function can use that concurrency space. However this limits the ability to use the concurrency from open pool.","poster":"tkanmani76","comment_id":"528338"},{"poster":"cldy","content":"C. Configure S3 event notifications to add events to an Amazon SQS queue in a separate account. Create the new Lambda function in the same account as the SQS queue and trigger the function when a message arrives in the queue.","comment_id":"498694","timestamp":"1639146420.0","upvote_count":"1"},{"poster":"AzureDP900","comment_id":"492152","timestamp":"1638421140.0","content":"I am going with C","upvote_count":"1"},{"timestamp":"1638248040.0","upvote_count":"2","comment_id":"490415","poster":"kaleen_bhaiya","content":"Selected Answer: C\nAnswer is C, only C assures to copy all the. files, if you throttle the Lambda many of the copy requests will fail. And there is no need to have synchronous copying of data."},{"poster":"student22","upvote_count":"1","content":"A\n---\nThe question mentions \"The objects do not need to be replicated in real time.\", hinting at A.\nC is too much work.","timestamp":"1636246800.0","comment_id":"450780"},{"poster":"andylogan","timestamp":"1636209840.0","upvote_count":"1","comment_id":"446428","content":"It's A"},{"comment_id":"439170","poster":"DerekKey","content":"Question is:\n\"How can the solutions architect ensure this new Lambda function will not impact other critical Lambda functions?\"\nWILL NOT IMPACT\nAny usage of the new replication lambda in current account will impact critical Lambda functions.\nC is CORRECT in my opinion","upvote_count":"2","timestamp":"1636046640.0"},{"comment_id":"435267","timestamp":"1636011060.0","upvote_count":"1","poster":"tgv","content":"AAA\n---"},{"comment_id":"433953","upvote_count":"2","timestamp":"1635999360.0","poster":"blackgamer","content":"C is the answer to me."},{"content":"Hard one but will go for A","upvote_count":"1","timestamp":"1635987180.0","poster":"denccc","comment_id":"431561"},{"comment_id":"422281","upvote_count":"1","content":"Not an answer choice but wouldn't the correct solution for this be to use S3 bucket replication with multiple destination buckets instead of Lambda? https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html","poster":"neta1o","timestamp":"1635649860.0"},{"upvote_count":"2","comment_id":"413470","poster":"WhyIronMan","timestamp":"1635428580.0","content":"I'll go with A"},{"comment_id":"407390","content":"C Correct","upvote_count":"2","poster":"Akhil254","timestamp":"1635345300.0"},{"comments":[{"comment_id":"397840","content":"I was first leaning towards Ans C. But after reading it carefully, it says that \"trigger the function when a message arrives in the queue\". That means if there is a flood of messages, then the lambda will start consuming message at full throttle and consume all the concurrency. So, a perfect solution might need a mechanism like the messages from the queue will be consumed, say every x millisec.\nConsidering this, the optimal answer would be A","poster":"DashL","upvote_count":"2","timestamp":"1634476320.0"}],"comment_id":"396068","poster":"MrCarter","timestamp":"1634278800.0","upvote_count":"2","content":"THIS IS JUST ONE OF THOSE QUESTIONS, ISN'T IT. BOTH A AND C MAKE SENSE.\nI'm leaning towards C in order to have the concurrency limit be in the other asccounts where the data is being replicated to. \nIf the q said between different regions within the same account, then A would be preferable"},{"timestamp":"1634068500.0","content":"Should be C . Because in A 1. how do you determine how much concurrency need for replication lambda 2. if replication lambda fails due to concurrecy limit how it will be copy items which already arrived in s3. ?","comments":[{"poster":"tvs","comment_id":"382531","timestamp":"1634178480.0","upvote_count":"1","content":"I mean the object that missed to replicated due to concurrency error."}],"poster":"tvs","comment_id":"381022","upvote_count":"2"},{"content":"it's C","poster":"Waiweng","upvote_count":"4","timestamp":"1633481280.0","comment_id":"357143"},{"timestamp":"1633120140.0","comment_id":"314277","comments":[{"content":"C is incorrect it's possible and you will have the concurrency limit on the separate AWS account all for the new Lambda function. However, this requires more work and the creation of another AWS account. Setting a concurrency limit is recommended as it can be used to limit the number of executions of a particular function.","timestamp":"1633412760.0","poster":"ExtHo","comment_id":"329180","upvote_count":"1"},{"poster":"Gladabhi","timestamp":"1635231780.0","comment_id":"398908","upvote_count":"1","content":"C is incomplete. Data in account X and Lambda in Y will need some cross account permission that is not included in the ans."}],"poster":"bachdx","content":"Between A and C I believe C is correct. Here is my reasoning:\nA: If the Lambda reserved concurrency is decrease to, let's say, 1, what happens to all the message that cannot be process? I believe the Lambda will throw a throttle error and the mesages is discarded.\nC: However, if SQS is used, all the message is pushed into a queue and the number of concurrent invocation will be scaled accordingly to the number of free concurrency invocations that has left. And S3 Event can be sent to SQS in another account so cross account is not a problem.","upvote_count":"5"},{"content":"C. SQS can be configured with a batch number to decrease lambda invocations","poster":"wasabidev","upvote_count":"1","timestamp":"1632969780.0","comment_id":"313516"},{"timestamp":"1632770040.0","comment_id":"301840","poster":"kiev","upvote_count":"2","content":"A is the right answer and again from Neal Davis"},{"timestamp":"1632733200.0","poster":"Kian1","upvote_count":"1","comment_id":"294257","content":"going with C","comments":[{"timestamp":"1632757440.0","upvote_count":"1","poster":"Kian1","content":"on second thought, I changed to A","comment_id":"295693"}]},{"timestamp":"1632634320.0","poster":"Ebi","content":"A although is correct, it can still impact critical functions.\nI go with C, there is zero chance for impacting existing functions. \nMy personal preference is to use SQS but in same account and increasing total limit for that account.","comment_id":"284428","upvote_count":"5"},{"upvote_count":"3","content":"The question specifically asks what needs to be done to the lambda function in question. Option C even though sounds right is ignoring the above ask and asking us to create a new lambda function in each of the S3 destination accounts. Therefore A makes more sense where we are limiting concurrency by lambda function.","timestamp":"1632588120.0","comment_id":"257027","comments":[{"upvote_count":"1","timestamp":"1632606300.0","comment_id":"271407","content":"Good Point","poster":"01037"},{"content":"Where does it say to create Lambda function in EACH of the destination accounts. In only says to create it in one separate account that has the SQS queue.\nUltimately C is better solution because:\n1) you can use batching to limit number of concurrently excused Lambda functions\n2) even if concurrency limit in the separate account will be reached temporarily Lambda Service will keep retrying to poll the messages from the queue (until retry limit is reached) and feed it to Lambda functions - this will in almost guarantee processing of all events/messages other that some really exceptionally large and long surges in traffic","timestamp":"1633475760.0","comment_id":"331718","poster":"RedKane","upvote_count":"2"}],"poster":"Bulti"},{"comment_id":"257026","poster":"Bulti","upvote_count":"3","content":"The question specifically asks want to an be done to the lambda function I question. Option C even though sounds right is ignoring the above ask and asking us to create a new lambda function in each of the S3 destination accounts. Therefore A makes more sense where we are limiting concurrency by lambda function.","timestamp":"1632560100.0"},{"content":"For sure it is reserved concurrency. Correct answer is A.","poster":"T14102020","timestamp":"1632350160.0","comment_id":"241699","upvote_count":"1"},{"comment_id":"233416","upvote_count":"3","poster":"jackdryan","content":"I'll go with A","timestamp":"1632247080.0"},{"comment_id":"228451","content":"C, not need to be replicated in real time","poster":"NNHAN","upvote_count":"3","timestamp":"1632221580.0"},{"comments":[{"content":"A - https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html\n\nWhen a function has reserved concurrency, no other function can use that concurrency. >>>Reserved concurrency also limits the maximum concurrency for the function<<<, and applies to the function as a whole, including versions and aliases.","upvote_count":"2","poster":"cloudgc","comment_id":"234069","timestamp":"1632254220.0"}],"poster":"keos","upvote_count":"1","comment_id":"216463","timestamp":"1632108540.0","content":"A is wrong, \"set to the new lambda..\""}],"choices":{"D":"Ensure the new Lambda function implements an exponential backoff algorithm. Monitor existing critical Lambda functions with Amazon CloudWatch alarms for the Throttles Lambda metric.","B":"Increase the execution timeout of the new Lambda function to 5 minutes. Monitor existing critical Lambda functions with Amazon CloudWatch alarms for the Throttles Lambda metric.","C":"Configure S3 event notifications to add events to an Amazon SQS queue in a separate account. Create the new Lambda function in the same account as the SQS queue and trigger the function when a message arrives in the queue.","A":"Set the new Lambda function reserved concurrency limit to ensure the executions do not impact other critical Lambda functions. Monitor existing critical Lambda functions with Amazon CloudWatch alarms for the Throttles Lambda metric."},"unix_timestamp":1604827080,"answer_description":"","timestamp":"2020-11-08 10:18:00","url":"https://www.examtopics.com/discussions/amazon/view/36442-exam-aws-certified-solutions-architect-professional-topic-1/","answers_community":["A (58%)","C (42%)"],"question_id":664},{"id":"zU3FeMu1qzzcm8H4XI0u","isMC":true,"timestamp":"2020-01-12 08:32:00","answer_images":[],"answer":"B","answer_description":"","question_id":665,"question_images":[],"exam_id":32,"topic":"1","answer_ET":"B","answers_community":["B (75%)","C (25%)"],"choices":{"B":"Configure your DirectConnect router with a higher BGP priority man your VPN router, verify network traffic is leveraging Directconnect and then delete your existing VPN connection.","A":"Delete your existing VPN connection to avoid routing loops configure your DirectConnect router with the appropriate settings and verity network traffic is leveraging DirectConnect.","D":"Configure your DirectConnect router, update your VPC route tables to point to the DirectConnect connection, configure your VPN connection with a higher BGP priority, and verify network traffic is leveraging the DirectConnect connection.","C":"Update your VPC route tables to point to the DirectConnect connection configure your DirectConnect router with the appropriate settings verify network traffic is leveraging DirectConnect and then delete the VPN connection."},"unix_timestamp":1578814320,"question_text":"Your company previously configured a heavily used, dynamically routed VPN connection between your on-premises data center and AWS. You recently provisioned a DirectConnect connection and would like to start using the new connection.\nAfter configuring DirectConnect settings in the AWS Console, which of the following options win provide the most seamless transition for your users?","url":"https://www.examtopics.com/discussions/amazon/view/11799-exam-aws-certified-solutions-architect-professional-topic-1/","discussion":[{"content":"Answer is B\nWe can have only 1 VGW on VPC, so no need to configure route in VPC anymore\nhttps://acloud.guru/forums/aws-certified-solutions-architect-professional/discussion/-KWVDow4aXPEdVfmBcZD/after-configuring-directconnect-settings-in-the-aws-console-which-of-the-followi","comment_id":"37944","timestamp":"1632360780.0","comments":[{"timestamp":"1632950460.0","content":"What does higher priority mean? In networking sometimes this means less preferable. Anyway the important here is the traffic has to use the same path (DX) from the VPC to on-premise and from on-premise to the VPC.","upvote_count":"1","poster":"kakashi","comment_id":"69605"},{"timestamp":"1635863220.0","upvote_count":"4","poster":"01037","content":"I think it's B.\nIt's client side configuration.\nVPC automatically prioritize DX, but client side doesn't, so it needs the adjustment.","comment_id":"360251"}],"poster":"amog","upvote_count":"13"},{"content":"Definitely not C. This is a common test trap. You cant point to a DC connection, only the VGW. Who sets the answers on this site?","upvote_count":"5","comment_id":"195809","poster":"cpal012","timestamp":"1634178120.0"},{"content":"B. Configure your DirectConnect router with a higher BGP priority man your VPN router, verify network traffic is leveraging Directconnect and then delete your existing VPN connection.","timestamp":"1723754640.0","upvote_count":"1","poster":"amministrazione","comment_id":"1266683"},{"comment_id":"751795","timestamp":"1671589860.0","upvote_count":"1","poster":"TigerInTheCloud","content":"Selected Answer: B\nAWS prefers DX, but return traffic from outside AWS needs to set a higher BGP priority to use DX"},{"upvote_count":"2","poster":"kzqc","timestamp":"1667760960.0","content":"Selected Answer: B\nB. Updating VPC routing table is useless here because AWS will always use DC over VPN when sending traffic to on-premise. But changing DC router and vpn router (both on-prem) will affect return traffic from on-prem to AWS. Higher BGP priority really means high BGP local preference. \nhttps://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/vpn-connection-as-a-backup-to-aws-dx-connection-example.html","comment_id":"712584"},{"timestamp":"1658702820.0","upvote_count":"1","poster":"hilft","comment_id":"636273","content":"I will go for C. Still super confused. Networking specialty"},{"upvote_count":"1","content":"Selected Answer: C\nAnswer: C\nExplanation:\nDirect Connect takes priority over Dynamically configured VPN connections.","comment_id":"627215","poster":"TechX","timestamp":"1656989520.0"},{"content":"Your company previously configured a heavily used, dynamically routed VPN connection between your on-premises data center and AWS. You\nrecently provisioned a DirectConnect connection and would like to start using the new connection.\nAfter configuring DirectConnect settings in the AWS Console, which of the following options win provide the most seamless transition for your\nusers?","upvote_count":"2","poster":"RVivek","comments":[{"timestamp":"1641039360.0","upvote_count":"1","comment_id":"514500","content":"So even if you dont perfrom steps the mentioned in option B it is going to be take care sutomatically","poster":"RVivek"},{"poster":"RVivek","timestamp":"1641040920.0","content":"copy paste mistake. Answer is as B is not required \nQ. Can I use AWS Direct Connect and a VPN Connection to the same VPC simultaneously?\nYes. However, only in fail-over scenarios. The Direct Connect path will always be preferred, when established, regardless of AS path prepending.\nReference:\nhttps://aws.amazon.com/directconnect/faqs/","upvote_count":"2","comment_id":"514506"}],"comment_id":"514498","timestamp":"1641039180.0"},{"poster":"FERIN_01","timestamp":"1636018800.0","comment_id":"424070","content":"Not sure on high BGP Priority, as BGP Priority numbers got different meaning. But option C. for sure, will work. Also need not delete VPN as it can be used alternative if direct link goes down","upvote_count":"2"},{"comment_id":"353511","timestamp":"1635592980.0","content":"B is missing 'update the VPC route table', so C is correct","poster":"pt8","upvote_count":"1","comments":[{"timestamp":"1635939420.0","comment_id":"377608","poster":"tvs","content":"BGP does that.","upvote_count":"2"}]},{"comment_id":"330058","upvote_count":"1","content":"B is right. Becuase only BGP makes priorities in Direct Connect","poster":"hihismkskks","timestamp":"1635494640.0"},{"poster":"RomanTsai","timestamp":"1635380400.0","upvote_count":"1","comment_id":"297186","content":"Answer is B"},{"timestamp":"1634843520.0","poster":"wind","upvote_count":"1","comment_id":"289490","content":"C is correct, you don't need to care BGP."},{"timestamp":"1634793780.0","upvote_count":"1","poster":"MHKyaw","comment_id":"269444","content":"Keyword in question is \" After Configuring DirectConnect settings,\". So, we don't need to configure DirectConnect again. \nI go with C."},{"timestamp":"1634345580.0","content":"Answer is B\n\"Important: Be sure that Direct Connect is the preferred route from your end, and not over VPN when the Direct Connect virtual interface is up in order to avoid asymmetric routing; this might cause traffic to be dropped. We always prefer a Direct Connect connection over VPN routes\"\nQuoted from AWS. On the client side, the client's router must prefer DX to avoid traffix going in through VPC while going out through DX","poster":"qkhanhpro","upvote_count":"1","comment_id":"230784"},{"timestamp":"1633776120.0","comment_id":"182412","upvote_count":"1","content":"Agree with B","poster":"ipindado2020"},{"content":"Answer is B\nYou don't need to make any change in the routing table, because BGP will automatically handle any needed changes in the route tables","comment_id":"176294","upvote_count":"3","poster":"ashendy","timestamp":"1633665060.0"},{"comment_id":"155292","timestamp":"1633621380.0","upvote_count":"4","content":"Its not possible to point router table to direct connect connection, but vgw. Therefore B","poster":"sensor"},{"upvote_count":"1","timestamp":"1633311720.0","poster":"RyanGhavidel","comment_id":"152178","content":"C - there is no BGP priority and also VPC rt prefers DX routes automatically"},{"poster":"fullaws","content":"B is correct, VGW will propagate route into VPC route table, and Direct connect route is high priority than IPsec route","upvote_count":"3","comment_id":"144007","timestamp":"1633276500.0"},{"timestamp":"1633240980.0","comment_id":"117997","content":"C is correct.\n\nVPC routing table still need to be corrected to point to DX before deleting VPN connection. Though BGP weight can be an option but once you delete VPN the routing need to be corrected any way. Any thoughts?","poster":"manoj101","upvote_count":"4"},{"poster":"arsimi","upvote_count":"4","timestamp":"1633144560.0","content":"C\nhttps://jayendrapatil.com/tag/direct-connect/","comment_id":"81404"},{"upvote_count":"2","poster":"Gorha","timestamp":"1632899460.0","content":"B is correct!","comment_id":"54098"}]}],"exam":{"numberOfQuestions":1019,"isImplemented":true,"id":32,"lastUpdated":"11 Apr 2025","provider":"Amazon","name":"AWS Certified Solutions Architect - Professional","isBeta":false,"isMCOnly":false},"currentPage":133},"__N_SSP":true}