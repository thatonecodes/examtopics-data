{"pageProps":{"questions":[{"id":"7JN89KKIJaXC9t3zixFc","isMC":true,"question_id":131,"discussion":[{"poster":"AjoseO","comment_id":"811805","content":"Selected Answer: B\nAmazon Forecast is an AWS service that uses machine learning to build accurate time-series forecasts. It provides several built-in algorithms that support holiday featurization, and the DeepAR+ algorithm can handle the seasonality and correlation with other products with minimal development effort. With Amazon Forecast, the data scientist can easily configure the forecast horizon, select the appropriate forecast frequency, and configure the model training to incorporate the available historical data. Using Amazon SageMaker Processing to enrich the data with holiday information may require more development effort and does not offer the same level of automation and integration as Amazon Forecast.","timestamp":"1676633880.0","upvote_count":"5"},{"timestamp":"1732398120.0","comment_id":"1316814","poster":"KarinaAsh","upvote_count":"2","content":"Selected Answer: B\nWhile ARIMA is a classic time series forecasting method, it might not capture complex patterns (like seasonality and related product sales) as effectively as DeepAR+"},{"upvote_count":"1","timestamp":"1713113820.0","comment_id":"1195626","content":"B - DeepAR _ Algo with Holidya featurization in Amazon Forecast --> works better than Option A as Option A may be suboptimal but totally doable. DeepAR shines in multiple correlated time series and meant for seasonal data patterns","poster":"JonSno"},{"upvote_count":"3","content":"Selected Answer: A\nA. YES - fully managed solution\nB. NO - DeepAR+ is more for multiple time series (\"In many applications, however, you have many similar time series across a set of cross-sectional units\"- https://docs.aws.amazon.com/forecast/latest/dg/aws-forecast-recipe-deeparplus.html)\nC. NO - SageMaker DeepAR is too low-level \nD. NO - Gluon is too low-level","poster":"loict","comment_id":"1004889","timestamp":"1694441340.0"},{"content":"Selected Answer: A\nOption B is a great choice but requires more development effort over A which is also a great choice. Since the question asked for Least Development I am going with A","comment_id":"998072","timestamp":"1693791300.0","upvote_count":"2","poster":"Shenannigan"},{"poster":"Mickey321","content":"Selected Answer: B\nDeep AR can understand seasonal effect","upvote_count":"1","timestamp":"1692473100.0","comment_id":"985399"},{"comment_id":"844688","upvote_count":"2","content":"Selected Answer: B\nB is correct","timestamp":"1679304120.0","poster":"blanco750"},{"upvote_count":"2","content":"Selected Answer: B\nDeepAR accepts exogenorous regressors different from ARIMA and can understand seasonal effects, ARIMA can't do it too.","timestamp":"1678227840.0","comment_id":"832340","poster":"Valcilio"},{"timestamp":"1678131540.0","comment_id":"831199","upvote_count":"2","content":"Selected Answer: B\nIt is deepAR","poster":"Chelseajcole"},{"timestamp":"1677958500.0","comment_id":"829325","poster":"oso0348","content":"Selected Answer: A\nOption B is a good choice, as the DeepAR+ algorithm is specifically designed for forecasting in time series data with seasonality and long-term dependencies. However, it may require more development effort compared to the ARIMA algorithm.","upvote_count":"1"},{"comment_id":"806609","upvote_count":"2","content":"b\nhttps://docs.aws.amazon.com/forecast/latest/dg/holidays.html\nhttps://docs.aws.amazon.com/whitepapers/latest/time-series-forecasting-principles-with-amazon-forecast/appendix-a-faqs.html","timestamp":"1676223120.0","poster":"drcok87"}],"answer_ET":"B","choices":{"B":"Use Amazon Forecast with Holidays featurization and the built-in DeepAR+ algorithm to train the model.","C":"Use Amazon SageMaker Processing to enrich the data with holiday information. Train the model by using the SageMaker DeepAR built-in algorithm.","D":"Use Amazon SageMaker Processing to enrich the data with holiday information. Train the model by using the Gluon Time Series (GluonTS) toolkit.","A":"Use Amazon Forecast with Holidays featurization and the built-in autoregressive integrated moving average (ARIMA) algorithm to train the model."},"exam_id":26,"url":"https://www.examtopics.com/discussions/amazon/view/98990-exam-aws-certified-machine-learning-specialty-topic-1/","answer":"B","answer_images":[],"unix_timestamp":1676223120,"answers_community":["B (70%)","A (30%)"],"question_images":[],"topic":"1","timestamp":"2023-02-12 18:32:00","answer_description":"","question_text":"A data scientist at a retail company is forecasting sales for a product over the next 3 months. After preliminary analysis, the data scientist identifies that sales are seasonal and that holidays affect sales. The data scientist also determines that sales of the product are correlated with sales of other products in the same category.\n\nThe data scientist needs to train a sales forecasting model that incorporates this information.\n\nWhich solution will meet this requirement with the LEAST development effort?"},{"id":"zZEA3EZmIg1HnroEgRoy","isMC":true,"answer_images":[],"discussion":[{"content":"Selected Answer: B\noversample the minority class","timestamp":"1707001560.0","upvote_count":"9","comment_id":"797474","poster":"blt23"},{"comment_id":"1082826","poster":"endeesa","content":"Selected Answer: B\nUndersampling not an option for already limited observations. SMOTE clearly MOST promising first action before trying to balance classes","timestamp":"1732818000.0","upvote_count":"1"},{"content":"Do we need to stratify the non-failure events by machine type?","poster":"sukye","comment_id":"1073613","upvote_count":"1","timestamp":"1731874920.0"},{"comment_id":"829316","timestamp":"1709580600.0","content":"Selected Answer: B\nB. Oversample the failure cases by using the Synthetic Minority Oversampling Technique (SMOTE).\n\nSince the number of failure cases is relatively small, oversampling the failure cases using techniques like SMOTE can help balance the class distribution and prevent the model from being biased towards the majority class. SMOTE creates synthetic samples for the minority class by interpolating new samples between existing ones. This will help improve the model's accuracy in predicting failure cases. Adjusting class weights (A) or undersampling (C, D) may not be as effective in this scenario.","upvote_count":"3","poster":"oso0348"},{"upvote_count":"2","content":"Selected Answer: B\nThe data provided is imbalanced, with only 100 failure cases out of 10,000 event samples. Therefore, it is important to address this imbalance to improve the accuracy of the predictive maintenance model.","poster":"AjoseO","comment_id":"815904","timestamp":"1708465020.0"}],"timestamp":"2023-02-04 00:06:00","answer_description":"","unix_timestamp":1675465560,"question_text":"A company is building a predictive maintenance model for its warehouse equipment. The model must predict the probability of failure of all machines in the warehouse. The company has collected 10,000 event samples within 3 months. The event samples include 100 failure cases that are evenly distributed across 50 different machine types.\n\nHow should the company prepare the data for the model to improve the model's accuracy?","answers_community":["B (100%)"],"answer":"B","answer_ET":"B","question_id":132,"exam_id":26,"topic":"1","question_images":[],"choices":{"D":"Undersample the non-failure events by using the Synthetic Minority Oversampling Technique (SMOTE).","A":"Adjust the class weight to account for each machine type.","B":"Oversample the failure cases by using the Synthetic Minority Oversampling Technique (SMOTE).","C":"Undersample the non-failure events. Stratify the non-failure events by machine type."},"url":"https://www.examtopics.com/discussions/amazon/view/97866-exam-aws-certified-machine-learning-specialty-topic-1/"},{"id":"1PL9QsfgvPu8jCf0p7kP","timestamp":"2023-02-11 00:32:00","exam_id":26,"discussion":[{"poster":"AjoseO","content":"Selected Answer: C\nC. Train an Amazon SageMaker Neural Topic Model (NTM) model to generate the product categories.\n\nThe task is to build a machine learning model to categorize documents for all the company's products. Among the given options, training an Amazon SageMaker Neural Topic Model (NTM) model would be the most efficient and effective solution.\n\nAn NTM model can identify topics in text data and group similar documents into specific categories, making it a suitable model for document categorization. With an NTM model, the data scientist would not need to define product categories beforehand, as the model would automatically group similar documents into topics. This saves time and resources compared to the other options.","comments":[{"comment_id":"972205","timestamp":"1691156220.0","poster":"ccpmad","comments":[{"content":"Thanks to ChatGPT and also thanks Ajose O for saving our time looking for some evidence or a proof to the right answer \nAjose you made some good work bringing this clarification for us, so Thank you so much, Gracias amigo :)","timestamp":"1694792100.0","upvote_count":"3","poster":"teka112233","comment_id":"1008561"}],"upvote_count":"1","content":"thank you chatgpt"}],"upvote_count":"11","timestamp":"1676929080.0","comment_id":"815905"},{"content":"Selected Answer: C\nA. NO - no need to build a custom model\nB. NO - k-means is supervised model\nC. YES - unsupervised clustering algorithm\nD. NO - Blazing Text will do word embedding, not classification","upvote_count":"5","comments":[{"content":"No, k-means is an unsupervised learning algorithm. I","poster":"wendaz","upvote_count":"5","comment_id":"1048264","timestamp":"1697749860.0"}],"poster":"loict","timestamp":"1694442720.0","comment_id":"1004904"},{"upvote_count":"1","timestamp":"1718326740.0","comment_id":"1230161","poster":"sheetalconect","content":"Selected Answer: B\nC. Train an Amazon SageMaker Neural Topic Model (NTM) model to generate the product categories. -- option doesn't talk about any classification activity"},{"timestamp":"1692475860.0","comment_id":"985425","poster":"Mickey321","upvote_count":"1","content":"Selected Answer: C\nNeural Topic Model (NTM) is one of the built-in algorithms of Amazon SageMaker that can perform topic modeling on text data. Topic modeling is a technique that can discover latent topics or themes from a collection of documents. Topic modeling can be used for document categorization by assigning each document to one or more topics based on its content."},{"content":"Selected Answer: C\nBlazing Text is only for supervised problems.","poster":"kaike_reis","timestamp":"1692039540.0","upvote_count":"1","comment_id":"981027"},{"upvote_count":"2","comment_id":"872734","timestamp":"1681738740.0","content":"Selected Answer: D\nAssign pre-defined categories to documents in a corpus: categorize books in a library into academic disciplines - BlazingText algorithm\n\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/algos.html","poster":"mawsman","comments":[{"timestamp":"1681764780.0","content":"Reading it again - C","poster":"mawsman","upvote_count":"3","comment_id":"873084"}]},{"comment_id":"829310","timestamp":"1677957900.0","poster":"oso0348","comments":[{"timestamp":"1678158120.0","upvote_count":"4","poster":"zshafi13","comments":[{"comments":[{"content":"Good catch. The problem with B is the fact that is an incomplete question: \"Tokenize the data and transform the data into tabular data\" how are you going to do this conrad?","comment_id":"981028","poster":"kaike_reis","upvote_count":"2","timestamp":"1692039600.0"}],"comment_id":"963204","poster":"vbal","timestamp":"1690329660.0","content":"what is K-means?","upvote_count":"1"}],"comment_id":"831512","content":"\"no predefined product categories\" -> unsupervised learning, C."}],"upvote_count":"1","content":"Selected Answer: B\nOption C is wrong because it suggests using a Neural Topic Model (NTM) to categorize documents. While NTM can be used to discover the underlying topics in a corpus of documents, it may not be the most efficient solution for categorizing documents for specific products. NTM is more suited for unsupervised learning problems where the goal is to discover the underlying themes or topics of the document corpus.\n\nIn this scenario, the data scientist needs to categorize documents based on predefined product categories. Therefore, a supervised learning algorithm like a text classification model would be more suitable. Amazon SageMaker Blazing Text algorithm provides an efficient and scalable solution for text classification problems."},{"upvote_count":"1","poster":"drcok87","timestamp":"1676071920.0","content":"No predefined product category: topic modeling with NTM or LDA (Organize a set of documents into topics (not known in advance): tag a document as belonging to a medical category based on the terms used in the document.) \n\nPredefined product category: topic modeling with blazing text (categorize books in a library into academic disciplines)\n\nc","comment_id":"804861"}],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/98762-exam-aws-certified-machine-learning-specialty-topic-1/","answers_community":["C (82%)","Other"],"answer":"C","choices":{"A":"Build a custom clustering model. Create a Dockerfile and build a Docker image. Register the Docker image in Amazon Elastic Container Registry (Amazon ECR). Use the custom image in Amazon SageMaker to generate a trained model.","C":"Train an Amazon SageMaker Neural Topic Model (NTM) model to generate the product categories.","D":"Train an Amazon SageMaker Blazing Text model to generate the product categories.","B":"Tokenize the data and transform the data into tabular data. Train an Amazon SageMaker k-means model to generate the product categories."},"question_id":133,"answer_images":[],"question_text":"A company stores its documents in Amazon S3 with no predefined product categories. A data scientist needs to build a machine learning model to categorize the documents for all the company's products.\n\nWhich solution will meet these requirements with the MOST operational efficiency?","answer_description":"","unix_timestamp":1676071920,"question_images":[],"answer_ET":"C","topic":"1"},{"id":"aPVZrZufOaGLcOSVoTJz","answer":"A","choices":{"D":"Use Amazon Lookout for Vision.","C":"Use the Amazon SageMaker Object Detection algorithm.","A":"Use Amazon Rekognition.","B":"Use a custom convolutional neural network (CNN)."},"url":"https://www.examtopics.com/discussions/amazon/view/98763-exam-aws-certified-machine-learning-specialty-topic-1/","answer_ET":"A","isMC":true,"discussion":[{"upvote_count":"1","comment_id":"985426","poster":"Mickey321","content":"Selected Answer: A\nAmazon Rekognition can handle large-scale and high-quality images with low latency and high accuracy. You can use Amazon Rekognition to process images from various sources, such as cameras, webcams, or media files. You can also use Amazon Rekognition to process images in real time or in batch mode.","timestamp":"1724098380.0"},{"comment_id":"832341","poster":"Valcilio","comments":[{"poster":"angus","content":"https://aws.amazon.com/rekognition/","comment_id":"837921","upvote_count":"3","timestamp":"1710333780.0"}],"upvote_count":"3","timestamp":"1709850780.0","content":"Selected Answer: A\nIt's A, rekgonition has methods to do text detection."},{"timestamp":"1708502340.0","poster":"GiyeonShin","upvote_count":"2","content":"Selected Answer: A\nOCR : Amazon Rekognition","comment_id":"816333"},{"timestamp":"1708465380.0","poster":"AjoseO","content":"Selected Answer: A\nRekognition's Text Detection feature allows you to easily extract text from images and videos without the need to create a custom model or perform complex training. It's a fully managed service that provides accurate and scalable text detection, recognition, and analysis capabilities. Additionally, Rekognition provides a simple API and SDKs for integrating text detection functionality into your applications.\n\nhttps://docs.aws.amazon.com/rekognition/latest/dg/text-detection.html?pg=ln&sec=ft","upvote_count":"1","comment_id":"815912"},{"poster":"drcok87","content":"a - LEAST operational overhead","comment_id":"804863","timestamp":"1707608160.0","upvote_count":"1"}],"question_text":"A sports analytics company is providing services at a marathon. Each runner in the marathon will have their race ID printed as text on the front of their shirt. The company needs to extract race IDs from images of the runners.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","answer_description":"","topic":"1","answers_community":["A (100%)"],"timestamp":"2023-02-11 00:36:00","question_id":134,"unix_timestamp":1676072160,"question_images":[],"exam_id":26,"answer_images":[]},{"id":"JqjnneKieY6wwfAgB5Z1","choices":{"D":"Insert an Amazon Kinesis Data Analytics stream downstream of the Kinesis Data Firehose stream that transforms raw record attributes into simple transformed values using SQL.","A":"Require that the stores to switch to capturing their data locally on AWS Storage Gateway for loading into Amazon S3, then use AWS Glue to do the transformation.","B":"Deploy an Amazon EMR cluster running Apache Spark with the transformation logic, and have the cluster run each day on the accumulating records in Amazon S3, outputting new/transformed records to Amazon S3.","C":"Spin up a fleet of Amazon EC2 instances with the transformation logic, have them transform the data records accumulating on Amazon S3, and output the transformed records to Amazon S3."},"answer_images":[],"answer_ET":"D","discussion":[{"comments":[{"poster":"mawsman","comment_id":"93333","timestamp":"1664611920.0","content":"Best explanation here, kudos.","upvote_count":"4"},{"poster":"kakalotka","comment_id":"352819","upvote_count":"2","content":"I can't find any information that indicate Kinesis data analytics taking data from firehose","timestamp":"1667346360.0"}],"content":"D is correct. Question has '\"simple transformations, and some attributes will be combined\" and Least development effort. Kinesis analytics can get data from Firehose, transform and write to S3\nhttps://docs.aws.amazon.com/kinesisanalytics/latest/java/examples-s3.html","poster":"cybe001","comment_id":"37849","timestamp":"1664223660.0","upvote_count":"48"},{"poster":"Huy","comment_id":"398952","timestamp":"1667715600.0","content":"The best way to transform data is before it arrives to S3 so D should be best answer. But D is not completed. It should have another Firehose to deliver results to S3.","upvote_count":"9"},{"comment_id":"1358020","upvote_count":"1","timestamp":"1739833200.0","poster":"JonSno","content":"Selected Answer: D\nD. Insert an Amazon Kinesis Data Analytics stream downstream of the Kinesis Data Firehose stream that transforms raw record attributes into simple transformed values using SQL.\nExplanation:\nSince the data is already flowing through Amazon Kinesis Data Firehose, the least development effort solution is to use Amazon Kinesis Data Analytics, which supports SQL-based transformations on streaming data without requiring new infrastructure.\n\nWhy is this the best choice?\nNo major architectural changes – Data continues flowing from stores into Kinesis Data Firehose and then to Amazon S3.\nSimple SQL transformations – Since the changes are simple (e.g., attribute combinations), SQL is sufficient.\nLow operational overhead – No need to manage clusters or instances.\nReal-time processing – Transformed records immediately enter Amazon S3 for training."},{"timestamp":"1718843340.0","upvote_count":"1","comment_id":"928056","content":"Ans is D\nAmazon Kinesis Data Analytics provides a serverless option for real-time data processing using SQL queries. In this case, by inserting a Kinesis Data Analytics stream downstream of the Kinesis Data Firehose stream, the retail chain can easily perform the required simple transformations on the ingested purchasing records.","poster":"CKS1210"},{"content":"Selected Answer: D\nThe best answer is to use a lambda, but the letter D can do it very good too in the absence of the lambda option.","timestamp":"1709886300.0","upvote_count":"2","poster":"Valcilio","comment_id":"832689"},{"upvote_count":"1","poster":"cloud_trail","timestamp":"1667122680.0","comment_id":"278148","content":"I go with D. A tough question, though. And C are definitely out. They key to the question is that it does not say that the transformed data needs to be stored again in S3. It just needs to be sent to the model for training after being transformed. So a Kinesis Data Analytics stream is appropriate to do the transformation."},{"upvote_count":"6","comment_id":"260699","poster":"harmanbirstudy","content":"Legacy data -- Firehose -- Kinesis Analytics -- S3.This happens in near real time before the data ends up in S3.\n--Legacy data -- Firehose -- S3 is already happening (mentioned in first line in question), adding Kinesis Data Analytics to do simple transformation joins using SQL on the incoming data is the LEAST amount of work needed.\nKinesis Data analytics can write o S3. here is the AWS link with working example.Even Though Udemy tutorial said it cannot write directly to S3 :) .\n\nhttps://docs.aws.amazon.com/kinesisanalytics/latest/java/examples-s3.html","timestamp":"1667060340.0"},{"upvote_count":"1","poster":"gamaX","timestamp":"1667028480.0","comment_id":"234900","content":"It seems that LEAST developmnet effort:\nhttps://aws.amazon.com/fr/blogs/big-data/preprocessing-data-in-amazon-kinesis-analytics-with-aws-lambda/\n\nand GRETAST development effort:\nhttps://aws.amazon.com/fr/blogs/big-data/optimizing-downstream-data-processing-with-amazon-kinesis-data-firehose-and-amazon-emr-running-apache-spark/"},{"upvote_count":"1","poster":"HaiHN","timestamp":"1666784760.0","comment_id":"200528","comments":[{"poster":"h_sahu","comments":[{"comment_id":"278150","content":"With option A, you would be changing the legacy data ingestion, a huge development effort. Remember, you're talking about 20,000 stores.","poster":"cloud_trail","timestamp":"1667281740.0","upvote_count":"2"}],"timestamp":"1667008980.0","content":"I believe, kinesis should be used only in case of live data stream and this is not the case here. So as per me D shouldn't be the answer. I think A should be the answer as AWS storage gateway is something which is used alongwith on premise applications to move data to s3. Then glue can be used to transform the data.","upvote_count":"1","comment_id":"224718"}],"content":"It's D\n\nhttps://aws.amazon.com/blogs/big-data/preprocessing-data-in-amazon-kinesis-analytics-with-aws-lambda/\n\n\"In some scenarios, you may need to enhance your streaming data with additional information, before you perform your SQL analysis. Kinesis Analytics gives you the ability to use data from Amazon S3 in your Kinesis Analytics application, using the Reference Data feature. However, you cannot use other data sources from within your SQL query.\""},{"content":"It is D.","upvote_count":"1","comment_id":"144851","poster":"hans1234","timestamp":"1666397580.0"},{"timestamp":"1665574980.0","content":"I think the answer is D, because require the LEAST amount of development effort.","poster":"dikers","comment_id":"110018","upvote_count":"1"},{"upvote_count":"2","poster":"roytruong","timestamp":"1665461640.0","content":"it's D, kinesis analytic can easily connect with firehose","comment_id":"98695"},{"timestamp":"1664565600.0","upvote_count":"2","poster":"dreemswang","comment_id":"77920","content":"why not A. it seems good to me","comments":[{"content":"\"require stores to capture data locally using S3 gateway\" - for 20k stores this creates a HUUUGE operational overhead and development effort, definitely wrong","comment_id":"104794","poster":"ExamTaker123456789","upvote_count":"3","timestamp":"1665564360.0"}]},{"upvote_count":"6","comment_id":"65577","poster":"PRC","timestamp":"1664523540.0","content":"D is correct...rest all need some kind of manual intervention as well as they are not simple..Firehose allows transformation as well as moving into S3"},{"comments":[{"content":"Its D, because with KDA you can transform the data with SQL while with EMR you need to write code, considering the requirement of \"least development effort\", so D","poster":"hailiang","comment_id":"181260","timestamp":"1666551060.0","upvote_count":"3"}],"comment_id":"50163","upvote_count":"3","poster":"devsean","content":"I think the answer is B. D would be correct if they didn't want to transform the legacy data from before the switch, but it seems like they do. Choosing D would mean that you'd have to use an EC2 instance or something else to transform the legacy data along with adding the Kinesis data analytics functionality. Also, there is no real-time requirement so daily transformation is fine.","timestamp":"1664409300.0"},{"poster":"devsean","content":"I think the answer is B. D would be correct if they didn't want to transform the legacy data from before the switch, but it seems like they do. Choosing D would mean that you'd have to use an EC2 instance or something else to transform the legacy data along with adding the Kinesis data analytics functionality. Also, there is no real-time requirement so daily transformation is fine.","comment_id":"50162","timestamp":"1664311800.0","upvote_count":"7","comments":[{"poster":"ADVIT","upvote_count":"1","content":"\"LEAST amount of development effort\" , EMR is no complicated to LEAST","timestamp":"1719522300.0","comment_id":"935838"},{"timestamp":"1713560280.0","poster":"ZSun","upvote_count":"1","content":"If the question is \"least cost\" then B, but the question is \"least develope effort, then you want to keep original architeture. I agree that for daily ETL instead of real-time, and large dataset, B is better option.","comment_id":"875098"},{"upvote_count":"1","poster":"HaiHN","content":"You can use Lambda instead of EC2. So D should be OK.\nhttps://aws.amazon.com/blogs/big-data/preprocessing-data-in-amazon-kinesis-analytics-with-aws-lambda/","comment_id":"200565","timestamp":"1666812840.0"}]},{"poster":"am7","timestamp":"1664165220.0","upvote_count":"1","content":"can be B","comment_id":"36803"},{"timestamp":"1663863360.0","upvote_count":"3","content":"Amazon Kinesis Data Analytics can not send data to S3 directly - it needs something like Kinesis Data Firehose after it.","comment_id":"27182","poster":"vetal"}],"exam_id":26,"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/9826-exam-aws-certified-machine-learning-specialty-topic-1/","question_text":"A retail chain has been ingesting purchasing records from its network of 20,000 stores to Amazon S3 using Amazon Kinesis Data Firehose. To support training an improved machine learning model, training records will require new but simple transformations, and some attributes will be combined. The model needs to be retrained daily.\nGiven the large number of stores and the legacy data ingestion, which change will require the LEAST amount of development effort?","topic":"1","answer_description":"","answer":"D","answers_community":["D (100%)"],"timestamp":"2019-12-06 11:59:00","isMC":true,"unix_timestamp":1575629940,"question_id":135}],"exam":{"isMCOnly":false,"isBeta":false,"isImplemented":true,"provider":"Amazon","lastUpdated":"11 Apr 2025","id":26,"name":"AWS Certified Machine Learning - Specialty","numberOfQuestions":369},"currentPage":27},"__N_SSP":true}