{"pageProps":{"questions":[{"id":"h7K8AGczwqxpdLmsyN7q","answers_community":[],"timestamp":"2019-08-13 07:45:00","url":"https://www.examtopics.com/discussions/amazon/view/3524-exam-aws-certified-big-data-specialty-topic-1-question-41/","topic":"1","answer":"B","exam_id":17,"answer_description":"","question_images":[],"question_id":36,"unix_timestamp":1565675100,"answer_images":[],"isMC":true,"question_text":"An organization uses Amazon Elastic MapReduce(EMR) to process a series of extract-transform-load (ETL) steps that run in sequence. The output of each step must be fully processed in subsequent steps but will not be retained.\nWhich of the following techniques will meet this requirement most efficiently?","discussion":[{"content":"Answer is C. AWS Data Pipepline works well for sequence of ETL processing.\nhttps://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html\n\nAWS Data Pipeline is a web service that you can use to automate the movement and transformation of data. With AWS Data Pipeline, you can define data-driven workflows, so that tasks can be dependent on the successful completion of previous tasks. You define the parameters of your data transformations and AWS Data Pipeline enforces the logic that you've set up.","timestamp":"1635208560.0","poster":"Jayraam","upvote_count":"9","comment_id":"68002"},{"timestamp":"1633904880.0","upvote_count":"8","comments":[{"upvote_count":"1","content":"With AWS Data Pipeline, you can define data-driven workflows, so that tasks can be dependent on the successful completion of previous tasks. https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html","comment_id":"62849","poster":"reg9","timestamp":"1634799120.0"}],"content":"C should be the correct answer. The question never mentioned anything about keeping the final output in s3. ETL might be to-and-from any other database. And D only says load data to be processed in HDFS, and not really the output of each process.","poster":"s3an","comment_id":"24869"},{"content":"only 2 logical answers are C & D. Out if which Data Pipeline does not have the ability to share data on its own in between steps (it has to be stored somewhere). My choice is D as HDFS is ephemeral, data is lost of cluster termination","comments":[{"poster":"vicks316","comment_id":"183311","upvote_count":"1","timestamp":"1636005180.0","content":"Exactly what I was going to say, \"Data Pipeline does not have the ability to share data on its own in between steps (it has to be stored somewhere)\". Spot on, D from my perspective."}],"timestamp":"1635795180.0","upvote_count":"2","poster":"Royk2020","comment_id":"150165"},{"comment_id":"75427","content":"C & D will both do the job. However I think D is more efficient than C so you do not have to deal with starting and terminating a transient EMR cluster on each intermediate step. AWS Data pipeline is being introduced to confuse us because it is the service used to execute a series of job in a sequence. However I think D is the right answer as its more efficient. The reason why the final output is persisted to S3 is because we cannot lose it as is it the result of all the map/reduce processing we did on the cluster. So when the cluster is terminate we don't want to lose the results of our Map/reduce processing we did on the data fed to the cluster for processing.","timestamp":"1635286680.0","poster":"Bulti","upvote_count":"5"},{"timestamp":"1635235740.0","comment_id":"74736","poster":"YashBindlish","upvote_count":"1","content":"Correct Answer is D you can efficiently copy large amounts of data from Amazon S3 into HDFS where subsequent steps in your EMR clusters can process it. You can also use S3DistCp to copy data between Amazon S3 buckets or from HDFS to Amazon S3"},{"upvote_count":"1","timestamp":"1634770260.0","comment_id":"52360","content":"my selection C","poster":"san2020"},{"poster":"balajisush0312","content":"A.B and D support retain.However, C doesn't. So far me C is correct","upvote_count":"1","timestamp":"1634721480.0","comment_id":"51974"},{"poster":"ExamTopicSteven","comment_id":"48686","content":"Moreover, \"Datapipe line launches an Amazon EMR cluster for each scheduled interval, submits jobs as steps to the cluster, and terminates the cluster after tasks have completed.\" By this, data is not retained. So C looks good for me.\nDo you think the \"final output\" in D, means final output of each step. final output = output here.","timestamp":"1634280720.0","upvote_count":"2"},{"upvote_count":"1","content":"\"D. Load the data to be processed into HDFS\" = retained?. So D is not right? Moreover, the question looks is only about ETL, did not mention what we should do regarding \"final output\"\nC. Define the ETL steps as SEPARATE AWS Data Pipeline activities. SEPARATE means \"not ratained\", doesn't it?","timestamp":"1634236740.0","poster":"ExamTopicSteven","comment_id":"48684"},{"comment_id":"47205","timestamp":"1634079480.0","content":"@mattyb123 or anyone here- did you cleared the exam recently? need reviews on most of the contradictory answers","poster":"sam3787","upvote_count":"1"},{"timestamp":"1634013060.0","content":"output will not be retained. do we still require S3? not sure of the correct answer here","upvote_count":"1","poster":"sam3787","comment_id":"46777"},{"upvote_count":"4","timestamp":"1633992840.0","poster":"jay1ram2","content":"Correct Answer is D\n\nThe main ask is efficiency. \n\nA) EMRFS provides consistency. However copying intermediate results into S3 is not an efficient approach. \nB) s3n is a old protocol so it is not efficient\nC) Using data pipelines activity for each step is just orchestration and may not guarantee efficiency. \nD) Using HDFS for intermediate steps ensures that the data is replicated and stored within EMR core nodes and is the most efficient way to store data for processing in EMR (even compared to S3). Storing the final result in S3 provides durability and may or may not matter from the context of this question.","comment_id":"38763"},{"poster":"ME2000","comment_id":"37976","content":"Answer is D\nHere we go...\nScalability and Flexibility\nAdditionally, Amazon EMR provides the flexibility to use several file systems for your input, output, and intermediate data. For example, you might choose the Hadoop Distributed File System (HDFS) which runs on the master and core nodes of your cluster for processing data that you do not need to store beyond your clusterâ€™s lifecycle. You might choose the EMR File System (EMRFS) to use Amazon S3 as a data layer for applications running on your cluster so that you can separate your compute and storage, and persist data outside of the lifecycle of your cluster. \nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-overview-benefits.html","timestamp":"1633944960.0","upvote_count":"2"},{"content":"I would choose D since intermittent output saving it in HDFS is best option for chain of ETL jobs.","upvote_count":"1","timestamp":"1633792380.0","poster":"Raju_k","comment_id":"22319"},{"poster":"cybe001","comment_id":"19253","content":"My answer is D","upvote_count":"2","timestamp":"1633536180.0"},{"upvote_count":"1","content":"B is correct","timestamp":"1633467540.0","comment_id":"18260","poster":"asadao"},{"upvote_count":"2","poster":"M2","comment_id":"16056","content":"answer should D as it is writing only final output only in s3","timestamp":"1633432080.0"},{"timestamp":"1632980160.0","comment_id":"15682","upvote_count":"1","poster":"Zire","content":"My Opinion:\n Key phrase in this question: Steps that run in sequence. \nQuestion: Which of the following techniques will meet this requirement most efficiently? C: Data pipeline.\nA and B are not required since the question says: ...but will not be retained. \nAny thoughts?"},{"timestamp":"1632970380.0","comment_id":"13822","poster":"bigdatalearner","content":"@mattyb123 what's the answer here?","upvote_count":"1"},{"timestamp":"1632723540.0","content":"D seems quite correct.\n\nHDFS is a distributed, scalable, and portable file system for Hadoop. An advantage of HDFS is data awareness between the Hadoop cluster nodes managing the clusters and the Hadoop cluster nodes managing the individual steps. For more information, see Hadoop documentation.\n\nHDFS is used by the master and core nodes. One advantage is that it's fast; a disadvantage is that it's ephemeral storage which is reclaimed when the cluster ends. It's best used for caching the results produced by intermediate job-flow steps.","comment_id":"10437","upvote_count":"8","poster":"alotofjeff"},{"timestamp":"1632322380.0","comments":[{"timestamp":"1632583080.0","upvote_count":"4","poster":"muhsin","content":"this explanation is in AWS under EMRFS. s3n old version. so the answer is A\n'Previously, Amazon EMR used the S3 Native FileSystem with the URI scheme, s3n. While this still works, we recommend that you use the s3 URI scheme for the best performance, security, and reliability.'","comment_id":"8727","comments":[{"poster":"mattyb123","content":"reference link: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-file-systems.html","timestamp":"1632678540.0","comment_id":"8787","upvote_count":"2","comments":[{"timestamp":"1632876600.0","upvote_count":"2","content":"Should be D. \"HDFS is used by the master and core nodes... It's best used for caching the results produced by intermediate job-flow steps.\" mentioned in your reference link","comment_id":"11044","poster":"apertus"}]}]}],"poster":"muhsin","content":"most efficient way is s3n uri and then emrfs","comment_id":"6896","upvote_count":"2"},{"poster":"mattyb123","comment_id":"6673","content":"Thoughts on C?","upvote_count":"2","timestamp":"1632255360.0","comments":[{"comment_id":"6674","upvote_count":"3","poster":"mattyb123","timestamp":"1632256680.0","content":"or thoughts on D? D seems like a more simplified answer"}]}],"answer_ET":"B","choices":{"C":"Define the ETL steps as separate AWS Data Pipeline activities.","A":"Use the EMR File System (EMRFS) to store the outputs from each step as objects in Amazon Simple Storage Service (S3).","D":"Load the data to be processed into HDFS, and then write the final output to Amazon S3.","B":"Use the s3n URI to store the data to be processed as objects in Amazon S3."}},{"id":"ryF5UBolOjmBzLyuF2cI","answers_community":[],"choices":{"D":"Collect both sensor data and emergency services events with Amazon Kinesis Firehose and use Amazon Redshift for analysis.","A":"Collect the sensor data with Amazon Kinesis Firehose and store it in Amazon Redshift for analysis. Collect emergency services events with Amazon SQS and store in Amazon DynampDB for analysis.","C":"Collect both sensor data and emergency services events with Amazon Kinesis Streams and use DynamoDB for analysis.","B":"Collect the sensor data with Amazon SQS and store in Amazon DynamoDB for analysis. Collect emergency services events with Amazon Kinesis Firehose and store in Amazon Redshift for analysis."},"question_text":"The department of transportation for a major metropolitan area has placed sensors on roads at key locations around the city. The goal is to analyze the flow of traffic and notifications from emergency services to identify potential issues and to help planners correct trouble spots.\nA data engineer needs a scalable and fault-tolerant solution that allows planners to respond to issues within\n30 seconds of their occurrence.\nWhich solution should the data engineer choose?","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/3420-exam-aws-certified-big-data-specialty-topic-1-question-42/","question_images":[],"question_id":37,"discussion":[{"timestamp":"1634572080.0","content":"Answer : A \nA- Is the correct answer because alerting in 30 seconds cannot happen on data collected from sensors without further analysis which needs to be done using some transformation in KFH/Lambda and some aggregation in RedShift. Alerting in 30 seconds can happen on notification from emergency services only and for that a real-time collection solution such as SQS will be fine. It is also scalable and fault-tolerant as well.\nB- Not a good choice because emergency events needs to be notified within 30 seconds and KFH/Redshift combo doesnâ€™t do that.\nC- KS is not auto scalable / fully managed plus I donâ€™t see how data collected from the sensors can be analyzed and reported on within 30 seconds using a storage service such as Dynamo DB.\nD- KFH is not a real-time solution to store and alert on emergency services in 30 sec.","comment_id":"76089","comments":[{"poster":"Corram","timestamp":"1634597160.0","comment_id":"100988","upvote_count":"2","content":"I like the points you make and I also wonder how data collected from sensors can be analyzed so quickly, but to quote from jay1ram2's comment: \"The main asks are scalability, fault tolerance, and 30 seconds SLA for all data (It does not say just emergency services data).\" So in my opinion it is C and the sensors just provide really useful data that almost literally say \"problem\" or \"no problem\" out of the box.\nLastly, auto-scalability and being fully managed was not asked for; only scalability, and KS is manually scalable."}],"upvote_count":"9","poster":"Bulti"},{"timestamp":"1632206880.0","upvote_count":"7","content":"is it C. This will ensure the real time aspect? FH has a 60 second delay","comment_id":"6434","poster":"mattyb123","comments":[{"timestamp":"1632703560.0","comments":[{"poster":"mattyb123","content":"Anyone agree or disagree with A. Or is it C?","comments":[{"comment_id":"11542","upvote_count":"1","poster":"exams","content":"I think C","timestamp":"1633264320.0"}],"comment_id":"7034","upvote_count":"1","timestamp":"1632757320.0"}],"poster":"mattyb123","content":"Only reason why i think it could be A also is the mention of SQS for emergency services. Just with the FH delay there is 30 seconds un accounted for.","comment_id":"6902","upvote_count":"1"}]},{"comment_id":"220759","timestamp":"1635328800.0","poster":"beedle","content":"it cannot be A because AF cannot store data sirectly to redshift. It needs to use S3 copy command. Please read the docs. Its gottabe c.","upvote_count":"1"},{"comments":[{"poster":"DerekKey","comment_id":"337439","upvote_count":"1","content":"The questions asks for scalable not autp scalable","timestamp":"1635443520.0"}],"poster":"Phoenyx89","upvote_count":"1","timestamp":"1635322380.0","comment_id":"175227","content":"it is A. C is not suitable because DS is not auto scalable and the text says that only the alerts must be lower than 30 seconds."},{"content":"I think It could be A, look at this movie: https://www.youtube.com/watch?v=J8GJ4b4jLGI","comments":[{"poster":"notcloudguru","timestamp":"1635044280.0","comment_id":"120877","upvote_count":"1","content":"he said every 3 minutes data is sent, ?"}],"poster":"matthew95","comment_id":"106987","timestamp":"1634919780.0","upvote_count":"1"},{"comment_id":"74737","upvote_count":"2","poster":"YashBindlish","content":"Realtime to Kinesis Streams - Correct Answer is C","timestamp":"1634442060.0"},{"poster":"san2020","upvote_count":"4","content":"my selection C","comment_id":"52361","timestamp":"1634348820.0"},{"content":"Firehose has the minimum delay of 60 seconds so C is the only correct choice.","poster":"AdamSmith","upvote_count":"4","timestamp":"1634347680.0","comment_id":"46703"},{"comment_id":"39917","poster":"ME2000","timestamp":"1633910220.0","upvote_count":"1","content":"The IoT Core rules engine is not currently compatible with SQS FIFO queues: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html#FIFO-compatibility"},{"upvote_count":"5","comment_id":"38772","timestamp":"1633815000.0","poster":"jay1ram2","content":"My answer is C\n\nThe main asks are scalability, fault tolerance, and 30 seconds SLA for all data (It does not say just emergency services data). \n\nA, B, D) Uses firehose which has 60 seconds minimum buffer. \nC) Both Kinesis and DynamoDB have sub-second SLAs. As for the scalability of kinesis, even though it does auto scale, it can be manually scaled with no downtime. Both are managed serverless and have built-in fault tolerance."},{"comment_id":"19830","poster":"BigEv","timestamp":"1633585320.0","content":"I will go with C. \nhttps://sookocheff.com/post/aws/comparing-kinesis-and-sqs/","upvote_count":"1"},{"upvote_count":"1","timestamp":"1633390140.0","content":"I agree with C","poster":"asadao","comment_id":"18261"},{"content":"\"..A data engineer needs a scalable and fault-tolerant solution...\" .. Is Auto Scaling enabled in Kinesis Streams in this scenario? ..I don't think so.. and we know Kinesis Firehose is scalable because \"..It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration...\" and it also has 60 sec minimum delay and can not be used for Emergency Services. So, in my opinion, A is correct. Firehose for non-emergency and SQS for emergency because both are managed services.","timestamp":"1633273620.0","poster":"VB","comment_id":"11990","comments":[{"content":"I also agree with A. DynamoDB is more suitable for real-time analysis (OLTP) and Redshift for data warehouse (OLAP) analysis.","upvote_count":"2","timestamp":"1633418700.0","poster":"cybe001","comment_id":"19254"}],"upvote_count":"5"},{"comment_id":"7567","content":"Answer is A","upvote_count":"1","comments":[{"comment_id":"8309","timestamp":"1633038540.0","upvote_count":"2","poster":"mattyb123","content":"Why do you think its A?"}],"poster":"pra276","timestamp":"1632964200.0"},{"poster":"jlpl","comment_id":"7558","upvote_count":"1","content":"C is correct answer","timestamp":"1632904320.0"}],"answer_description":"","unix_timestamp":1565419020,"answer_ET":"A","timestamp":"2019-08-10 08:37:00","answer":"A","answer_images":[],"topic":"1","exam_id":17},{"id":"pjCRY9cXGcV8V2ZvH2Cx","choices":{"B":"Use AWS QuickSight to connect it to data stored in Amazon S3 to obtain the necessary business insight. Plot the churn trend graph to extrapolate churn likelihood for existing customers.","D":"Use a Redshift cluster to COPY the data from Amazon S3. Create a User Defined Function in Redshift that computes the likelihood of churn.","A":"Use the Amazon Machine Learning service to build the binary classification model based on the dataset stored in Amazon S3. The model will be used regularly to predict churn attribute for existing customers.","C":"Use EMR to run the Hive queries to build a profile of a churning customer. Apply a profile to existing customers to determine the likelihood of churn."},"question_id":38,"answers_community":[],"isMC":true,"answer_images":[],"topic":"1","question_text":"A telecommunications company needs to predict customer churn (i.e., customers who decide to switch to a competitor). The company has historic records of each customer, including monthly consumption patterns, calls to customer service, and whether the customer ultimately quit the service. All of this data is stored in\nAmazon S3. The company needs to know which customers are likely going to churn soon so that they can win back their loyalty.\nWhat is the optimal approach to meet these requirements?","exam_id":17,"answer_ET":"B","answer":"B","discussion":[{"timestamp":"1632274860.0","content":"it is absolutely A. churn prediction is a machine learning algorithm. Quicksight provide a visual analysis.","upvote_count":"11","comment_id":"6897","poster":"muhsin"},{"upvote_count":"6","comment_id":"6678","content":"Thoughts on A?","poster":"mattyb123","timestamp":"1632214200.0"},{"upvote_count":"1","poster":"YashBindlish","content":"Prediction it has to be Machine Learning. So i will go with A","comment_id":"74738","timestamp":"1635747540.0"},{"comment_id":"54143","poster":"Sandip_ece","upvote_count":"1","timestamp":"1635731580.0","content":"I will go with A"},{"content":"my selection A","upvote_count":"3","poster":"san2020","timestamp":"1635591960.0","comment_id":"52362"},{"timestamp":"1634923500.0","upvote_count":"3","poster":"sriansri","comment_id":"33163","content":"This is obvious A is the correct answer.\nhttps://aws.amazon.com/blogs/machine-learning/predicting-customer-churn-with-amazon-machine-learning/"},{"content":"I would go with A.","timestamp":"1634128620.0","poster":"Raju_k","upvote_count":"1","comment_id":"22321"},{"poster":"cybe001","upvote_count":"1","timestamp":"1633480140.0","content":"It is A. You may use QuickSight to visually analyze the data points using Scatter Plot and find out if a customer is going to leave or not. It is not an \"Optimal Solution\". Binary classification ML is appropriate for it.","comment_id":"19310"},{"upvote_count":"1","comments":[{"upvote_count":"1","comment_id":"18444","content":"answers only referenced Quicksight not Quicksight ML so will stick with A ..u have a point though","poster":"DJTau","timestamp":"1633448820.0"},{"comment_id":"253719","content":"I dont believe you can still use Quicksight ML to attain your use case, going through the video you can only attain the following use cases via Quicksight ML:- \n- Anomaly Detection (Random Forest)\n- Forecasting --> Regression over time -->Time-series Analysis --> Estimating values of certain attributes over time. \n- What if Analysis --> If You change the value of a certain attribute you can see its effect on your analysis being reflected over your dataset\n- Auto-Narratives --> Summarizing your Graphs in words by parsing through your dataset.\nThus, I believe A is the correct answer herein!","upvote_count":"1","timestamp":"1636244280.0","poster":"MihirB"}],"content":"https://aws.amazon.com/quicksight/features-ml/?nc=sn&loc=2&dn=2\nIt talks about using quicksight to discover trends, like whether someone is going to flip or not.","timestamp":"1633212000.0","poster":"L33","comment_id":"14511"},{"content":"who put answer B as correct answer here ? any admin of this website can correct the obvious wrong answers provided ?","comments":[{"poster":"Corram","timestamp":"1635792840.0","upvote_count":"2","comment_id":"100991","content":"also curious who did this :D to honor san2020 let me write:\n\nmy selection A"}],"upvote_count":"2","timestamp":"1633157640.0","poster":"bigdatalearner","comment_id":"13823"},{"upvote_count":"1","content":"For A, the company needs to have all data including previous customers, i.e. they should have both old and new customers in S3.. then they can build a binary classification (yes/no) to decide.","poster":"VB","timestamp":"1633125240.0","comment_id":"11991"},{"upvote_count":"1","comment_id":"7568","poster":"pra276","content":"Answer is A","timestamp":"1632738360.0"},{"timestamp":"1632383700.0","upvote_count":"1","content":"A is correct answer","comment_id":"7393","poster":"Jialu"}],"timestamp":"2019-08-13 08:04:00","question_images":[],"answer_description":"","unix_timestamp":1565676240,"url":"https://www.examtopics.com/discussions/amazon/view/3525-exam-aws-certified-big-data-specialty-topic-1-question-43/"},{"id":"k9s3BY1VLpLIaSVkD5rO","question_images":[],"question_text":"A system needs to collect on-premises application spool files into a persistent storage layer in AWS. Each spool file is 2 KB. The application generates 1 M files per hour. Each source file is automatically deleted from the local server after an hour.\nWhat is the most cost-efficient option to meet these requirements?","url":"https://www.examtopics.com/discussions/amazon/view/3426-exam-aws-certified-big-data-specialty-topic-1-question-44/","question_id":39,"answer_ET":"C","answer_description":"","answers_community":[],"answer_images":[],"answer":"C","topic":"1","timestamp":"2019-08-10 14:21:00","exam_id":17,"choices":{"A":"Write file contents to an Amazon DynamoDB table.","D":"Copy files to Amazon S3 infrequent Access Storage.","C":"Write file contents to Amazon ElastiCache.","B":"Copy files to Amazon S3 Standard Storage."},"isMC":true,"discussion":[{"upvote_count":"23","comments":[{"comment_id":"115948","content":"Yes correct,\nfrom S3 FAQ\nRequest Example:\nAssume you transfer 10,000 files into Amazon S3 and transfer 20,000 files out of Amazon S3 each day during the month of March. Then, you delete 5,000 files on March 31st.\nTotal PUT requests = 10,000 requests x 31 days = 310,000 requests\nTotal GET requests = 20,000 requests x 31 days = 620,000 requests\nTotal DELETE requests = 5,000Ã—1 day = 5,000 requests\n\nAssuming your bucket is in the US East (Northern Virginia) Region, the Request fees are calculated below:\n310,000 PUT Requests: 310,000 requests x $0.005/1,000 = $1.55\n620,000 GET Requests: 620,000 requests x $0.004/10,000 = $0.25\n5,000 DELETE requests = 5,000 requests x $0.00 (no charge) = $0.00","upvote_count":"1","timestamp":"1635189840.0","poster":"notcloudguru"}],"poster":"jay1ram2","content":"Correct Answer is A\n\nThe main asks are persistence and cost-efficiency. \n\nLet us calculate the storage and R/W numbers\n2KB/File * 1 Million files/Hr * 24 Hrs * 30 Days = 1 Month of data\n2/(1024 * 1024) * 1,000,000 * 24 * 30 = 1,373 GB of data/month\n1,000,000 * 24 * 30 = 720 Million writes/month or ~278 writes/second.\n\nA) The cost of storing 1.37 TB of data in DynamoDB is $370. On-demand cost of writing ~278/sec into DynamoDB is ~600 a month with a total cost of ~$1000. It is persistence storage. \nB) The cost of storing 1.37 TB of data in S3 is ~$31. Put request cost of 720 Million objects ~$3600 a month with aa total cost of ~$3631.\nC) Storing 720 Million records with total size 1.3 TB in Elasticache memory will cost more than $10K/month along with snapshot cost for persistence. \nD) S3 IA costs are even higher than standard costs given that the minimum size requirement/object is 128KB.\n\nA is the most cost-efficient of all along with persistence SLA.","comment_id":"38784","timestamp":"1634414040.0"},{"upvote_count":"10","content":"for N.virginia\nDynamoDB per GB 0.25/month + RCU \nS3 Standard per GB 0.023/month\nS3 Standard IA per GB /0.0125 but bill minimum 128KB for each object\nS3 Standard is cheaper","comment_id":"10442","poster":"alotofjeff","timestamp":"1632607980.0"},{"comment_id":"155896","timestamp":"1636157580.0","poster":"skytango","upvote_count":"1","content":"This article can be a clue. The answer could be A due to too many small files. https://www.reddit.com/r/aws/comments/5haamf/has_anyone_done_the_costs_math_on_s3_vs_dynamodb/"},{"content":"A is correct : Too many small files.. S3 is not a good choice. DynamoDB is the right option.","comment_id":"141486","upvote_count":"1","timestamp":"1636082220.0","poster":"jove"},{"poster":"askaron","upvote_count":"1","timestamp":"1636033740.0","content":"D. You can run a batch job e.g. every 30 minutes to gzip new files into one gz file and upload to S3 IA. All requirements met, only 48 API S3 PUT calls per day, and the file will for sure be bigger than 128kb.","comment_id":"132837"},{"timestamp":"1635239280.0","comment_id":"131570","poster":"jsr2017","content":"B is the correct, for dynamo you also need to pay by request.\nAlso you can call once per minute","upvote_count":"3"},{"upvote_count":"1","comment_id":"74739","content":"Correct Answer is A","poster":"YashBindlish","timestamp":"1634890320.0"},{"timestamp":"1634753340.0","poster":"Sandip_ece","comment_id":"54144","content":"As the question is asking to store files on AWS, I will go with Option - B","upvote_count":"2"},{"comment_id":"52363","timestamp":"1634441940.0","poster":"san2020","upvote_count":"2","content":"my selection A"},{"comment_id":"33506","timestamp":"1634270700.0","upvote_count":"1","content":"D would give you 30 days of storage per file, it doesn't say you have to keep the files, dynamoDB with TTL is a better option?","poster":"miguel80"},{"comments":[{"comment_id":"33650","content":"need to consider put api cost here. the cost of putting 1M small object per hour to s3 will kill your customer at the end of the month","timestamp":"1634365860.0","upvote_count":"3","poster":"hailiang"}],"timestamp":"1634201220.0","comment_id":"33502","content":"S3 Standard is most cost efficient.\n2M Kb files per hour which leads to 48GB per day and 1152 GB per month\nFor this S3 cost 28 USD\nDynamoDB cost 308 USD","poster":"sriansri","upvote_count":"2"},{"content":"I would agree with @pra276 and select A because too many small object put requests to S3 increases cost compared to DynamoDB.","poster":"Raju_k","upvote_count":"1","timestamp":"1634171160.0","comment_id":"22322"},{"timestamp":"1633840260.0","poster":"BigEv","upvote_count":"3","comment_id":"19831","comments":[{"poster":"d00ku","upvote_count":"8","timestamp":"1633860540.0","comment_id":"19891","content":"S3 IA charges on items min size 128kb. So even if the objects in the question have 2kb they will be charged for 128kb each. This results in higher costs compared to standard S3."}],"content":"I choose D\nThe question is asking for \"persistent storage layer\", but DynamoDB is naturally a database. So I think S3 should be option. However, the question does not clearly mention how frequently the stored data need be accessed, so I will go with S3 IA which is less expensive than S3 standard."},{"timestamp":"1633799820.0","upvote_count":"5","content":"D, Persistence and Cheap solution. The question didn't tell how frequent the files will be accessed. So D instead of B.","comment_id":"19315","poster":"cybe001"},{"comment_id":"18262","poster":"asadao","timestamp":"1633252860.0","upvote_count":"1","content":"B correct"},{"upvote_count":"3","comment_id":"15054","timestamp":"1633144380.0","poster":"M2","content":"Answer is B as it is frequently accessed storage and cheaper"},{"poster":"Zire","upvote_count":"4","comment_id":"14153","content":"I'd look at the two key phrases in this question: a persistent storage layer AND most cost-efficient option. That leads me to D","timestamp":"1632903720.0"},{"content":"A seems right to me as DynamoDb can delete data automatically using TTL once time expired. any thoughts ?","comment_id":"13824","timestamp":"1632688980.0","comments":[{"content":"1. the data is deleted from the local sever only, AWS shall be used for persistant storage\n2. consider searching this thing called \"S3 Lifecycle Rules\", it's basically the analogon to DDB TTL.","upvote_count":"2","comment_id":"100994","poster":"Corram","timestamp":"1635125820.0"}],"upvote_count":"2","poster":"bigdatalearner"},{"timestamp":"1632553920.0","content":"Answer is A: Here what is the most cost effective solution. For data that is less in size dynamoDB would be a better option than s3. You may check here https://calculator.s3.amazonaws.com/index.html.","comments":[{"comments":[{"comment_id":"9667","upvote_count":"5","poster":"mattyb123","content":"A is correct. With smaller object size and total storage size per month, DynamoDB is a more cost effective solution. S3 would be a better choice if the object size and storage size per month was much larger.","timestamp":"1632579660.0"}],"upvote_count":"1","poster":"muhsin","comment_id":"9641","content":"there will be charges for indexing. I think S3 is always best option","timestamp":"1632575220.0"}],"comment_id":"7570","poster":"pra276","upvote_count":"5"},{"comment_id":"6451","upvote_count":"2","timestamp":"1632257340.0","poster":"jlpl","comments":[{"upvote_count":"2","timestamp":"1632435540.0","content":"Agreed. Probably would use some form of Kinesis to stream it and store it in S3","poster":"mattyb123","comment_id":"6682"}],"content":"B? s3 cost effective, ?"}],"unix_timestamp":1565439660},{"id":"nxlnb7hi2ODiK1zVFQnh","question_text":"An administrator receives about 100 files per hour into Amazon S3 and will be loading the files into Amazon\nRedshift. Customers who analyze the data within Redshift gain significant value when they receive data as quickly as possible. The customers have agreed to a maximum loading interval of 5 minutes.\nWhich loading approach should the administrator use to meet this objective?","answer_description":"","choices":{"C":"Load the cluster when the administrator has an event multiple of files relative to Cluster Slice Count, or 5 minutes, whichever comes first.","A":"Load each file as it arrives because getting data into the cluster as quickly as possibly is the priority.","D":"Load the cluster when the number of files is less than the Cluster Slice Count.","B":"Load the cluster as soon as the administrator has the same number of files as nodes in the cluster."},"question_images":[],"answer_images":[],"unix_timestamp":1568782860,"answer":"C","question_id":40,"url":"https://www.examtopics.com/discussions/amazon/view/5350-exam-aws-certified-big-data-specialty-topic-1-question-45/","timestamp":"2019-09-18 07:01:00","discussion":[{"upvote_count":"5","content":"To verify that C is the right answer refer to https://aws.amazon.com/blogs/big-data/best-practices-for-micro-batch-loading-on-amazon-redshift/","timestamp":"1635745440.0","comment_id":"75444","poster":"Bulti"},{"content":"C is correct","poster":"Mikilo","comment_id":"95572","upvote_count":"1","timestamp":"1636269720.0"},{"content":"my selection C","timestamp":"1634823360.0","comment_id":"52364","upvote_count":"3","poster":"san2020"},{"comment_id":"13825","upvote_count":"3","content":"It's easy and obvious and answer is C","poster":"bigdatalearner","timestamp":"1633949880.0"},{"timestamp":"1633333500.0","poster":"exams","upvote_count":"3","comment_id":"11543","content":"I agree with C"}],"isMC":true,"exam_id":17,"answer_ET":"C","answers_community":[],"topic":"1"}],"exam":{"isBeta":false,"provider":"Amazon","isMCOnly":true,"isImplemented":true,"id":17,"name":"AWS Certified Big Data - Specialty","lastUpdated":"11 Apr 2025","numberOfQuestions":85},"currentPage":8},"__N_SSP":true}