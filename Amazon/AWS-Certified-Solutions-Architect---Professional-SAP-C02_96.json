{"pageProps":{"questions":[{"id":"30mLsLM6Wtblp0d1GebN","exam_id":33,"timestamp":"2024-07-01 14:51:00","isMC":true,"choices":{"B":"Create an S3 access point in the data scientists' AWS account for the data lake.","D":"Update the VPC route table to route S3 traffic to an S3 access point.","C":"Update the EC2 instance role. Add a policy with a condition that allows the s3:GetObject action when the value for the s3:DataAccessPointArn condition key is a valid access point ARN.","E":"Add an S3 bucket policy with a condition that allows the s3:GetObject action when the value for the s3:DataAccessPointArn condition key is a valid access point ARN.","A":"Create a gateway VPC endpoint for Amazon S3 in the data scientists’ VPC."},"question_text":"A company is collecting data from a large set of IoT devices. The data is stored in an Amazon S3 data lake. Data scientists perform analytics on Amazon EC2 instances that run in two public subnets in a VPC in a separate AWS account.\n\nThe data scientists need access to the data lake from the EC2 instances. The EC2 instances already have an assigned role with permissions to access Amazon S3.\nAccording to company policies, only authorized networks are allowed to have access to the IoT data.\n\nWhich combination of steps should a solutions architect take to meet these requirements? (Choose two.)","answer_description":"","question_id":476,"answer":"BE","discussion":[{"timestamp":"1741559220.0","content":"Selected Answer: AE\nA gateway VPC endpoint allows EC2 instances to access S3 privately without using the public internet.\nE. The S3 bucket policy ensures that only authorized access via the S3 access point is permitted.\n B is wrong because S3 Access Points are tied to the bucket’s AWS account, not the requester's AWS account.\nThe access point should be created in the same AWS account as the S3 data lake, not in the data scientists’ account.","poster":"Deztroyer88","upvote_count":"2","comment_id":"1373435"},{"timestamp":"1733648460.0","upvote_count":"3","poster":"Spike2020","content":"Selected Answer: AE\nA: Gateway VPC endpoints provide secure access to S3 without requiring internet access. Can be used in a multi-account setting.\nE: Bucket policies can restrict access to specific VPC endpoints.\nNot B: While S3 access points can be useful, they're not necessary in this scenario where the primary requirement is network-level access control.","comment_id":"1323451"},{"poster":"AzureDP900","comment_id":"1313351","content":"B: Creating an S3 access point in the data scientists' AWS account provides a secure and controlled way to expose the data lake to EC2 instances. The access point allows you to manage who can access the bucket, and you can configure the bucket policy to include conditions that restrict access.\n\nE: Adding an S3 bucket policy with a condition that allows the s3:GetObject action when the value for the s3:DataAccessPointArn condition key is a valid access point ARN provides additional security and control over who can access the data lake. This ensures that only authorized networks (in this case, the data scientists' AWS account) can access the bucket.","timestamp":"1731810480.0","upvote_count":"2"},{"poster":"doobc","content":"BE. https://aws.amazon.com/blogs/storage/setting-up-cross-account-amazon-s3-access-with-s3-access-points/","timestamp":"1730806440.0","upvote_count":"1","comment_id":"1307344"},{"upvote_count":"2","comment_id":"1304489","content":"I feel the combination of A,B and E would be the correct answer","poster":"sam2ng","timestamp":"1730211180.0"},{"poster":"kgpoj","upvote_count":"4","comment_id":"1283548","content":"This question is really bad.\n\nIt feels like if A is selected, then E needs to be adjusted to enable access between VPC endpoints and the bucket directly\n\nOr if B is selected, then B needs to be reworded to say creating access point in data lake account, then E would be valid without any modification","timestamp":"1726302180.0"},{"content":"Selected Answer: BE\nB & E are correct options. A isn't correct because gateway VPC endpoint doesn't work outside of VPC. In this question, we are talking about 2 different accounts which implies 2 different VPCs as well","poster":"backbencher2022","timestamp":"1724008740.0","upvote_count":"4","comment_id":"1268201"},{"timestamp":"1723863360.0","poster":"kgpoj","comment_id":"1267414","content":"Selected Answer: AE\nS3 Access Point should be created in destination account.\n\nYou need VPC endpoint to keep the network private.\n\nThis question might just assumed that the S3 access point is already created in destination account","upvote_count":"2"},{"comments":[{"timestamp":"1726309320.0","upvote_count":"1","content":"You can also create a cross-account access point that's associated with a bucket in another AWS account, as long as you know the bucket name and the bucket owner's account ID. However, creating cross-account access points doesn't grant you access to data in the bucket until you are granted permissions from the bucket owner. The bucket owner must grant the access point owner's account (your account) access to the bucket through the bucket policy. For more information, see Granting permissions for cross-account access points.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/create-access-points.html","comment_id":"1283583","poster":"paultantony"}],"upvote_count":"3","poster":"zolthar_z","timestamp":"1722870420.0","comment_id":"1261095","content":"Selected Answer: BE\nS3 access point is used If you want to share your bucket with other accounts"},{"poster":"dzidis","content":"Selected Answer: BE\nGateway endpoint do not work cross account, so BE.\nowever, gateway endpoints do not allow access from on-premises networks, from peered VPCs in other AWS Regions, or through a transit gateway. For those scenarios, you must use an interface endpoint, which is available for an additional cost.\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html","timestamp":"1722400740.0","upvote_count":"3","comment_id":"1258563"},{"comment_id":"1255312","poster":"RotterDam","timestamp":"1721954520.0","comments":[{"comments":[{"content":"I just try it. You can create a cross-account s3 access point","poster":"hanson1028","comment_id":"1274391","timestamp":"1724915100.0","upvote_count":"1"}],"content":"B is completely wrong because the access point is created in the wrong account.\n\nYou need the access point to be created in the source s3 bucket account","timestamp":"1724135220.0","poster":"kgpoj","comment_id":"1269181","upvote_count":"2"}],"upvote_count":"3","content":"Selected Answer: BE\nAnyone who is picking A/E - please realize DataAccessPointArn ONLY WORKS when there is an access point created. \nA does NOT mention creating an Access Point. B is completely possible and combine with E restricts all traffic coming from the VPC that has the acccess point mentioned in B. \nB+E is the correct answer"},{"content":"a,d gateway VPC endpoint needs config route table","poster":"luuthang2011","comment_id":"1254179","timestamp":"1721803320.0","upvote_count":"1"},{"comment_id":"1251577","timestamp":"1721457120.0","upvote_count":"1","poster":"vip2","content":"Selected Answer: AE\nA, E are correct"},{"comments":[{"timestamp":"1732489980.0","upvote_count":"1","content":"B, E \nCreating a gateway VPC endpoint for Amazon S3 does allow direct access from the EC2 instances to the data lake, but it does not ensure that only authorized networks can access the IoT data.","poster":"AzureDP900","comment_id":"1317235"}],"poster":"gfhbox0083","comment_id":"1247396","content":"Selected Answer: AE\nA, E for sure.\nOnly authorized networks are allowed to have access to the IoT data.","upvote_count":"2","timestamp":"1720891020.0"},{"comment_id":"1245533","upvote_count":"3","poster":"c22ddd8","content":"Selected Answer: BE\nNeed access from different AWS account with restrictions. So it is BE","timestamp":"1720621320.0"},{"poster":"Alagong","comment_id":"1241696","upvote_count":"4","timestamp":"1720046580.0","content":"Selected Answer: AE\nA. This step ensures that the traffic between the EC2 instances and the S3 data lake does not traverse the public internet, thereby meeting security requirements and reducing latency.\nE. This step ensures that the access to the data lake is restricted according to company policies. It leverages an S3 bucket policy to enforce access control based on specific conditions, thereby providing an additional layer of security."},{"timestamp":"1720046520.0","upvote_count":"1","poster":"Alagong","comment_id":"1241694","content":"A. This step ensures that the traffic between the EC2 instances and the S3 data lake does not traverse the public internet, thereby meeting security requirements and reducing latency.\nC. This step ensures that the access to the data lake is restricted according to company policies. It leverages an S3 bucket policy to enforce access control based on specific conditions, thereby providing an additional layer of security."},{"content":"B\nS3 access points allow fine-grained control of access policies and network settings for specific S3 buckets.\nE\ns3:DataAccessPointArn must be used to set permissions on the S3 bucket side for going through the access point. role settings in C do not have settings to determine the access point on the bucket side.","comment_id":"1240149","upvote_count":"2","timestamp":"1719838260.0","poster":"kupo777"}],"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/143132-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_images":[],"topic":"1","answer_ET":"BE","unix_timestamp":1719838260,"answers_community":["BE (53%)","AE (47%)"]},{"id":"roCiX7X2vTSauGEZfULl","answers_community":["A (91%)","9%"],"isMC":true,"question_text":"A company wants to migrate its website to AWS. The website uses containers that are deployed in an on-premises, self-managed Kubernetes cluster. All data for the website is stored in an on-premises PostgreSQL database.\n\nThe company has decided to migrate the on-premises Kubernetes cluster to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The EKS cluster will use EKS managed node groups with a static number of nodes. The company will also migrate the on-premises database to an Amazon RDS for PostgreSQL database.\n\nA solutions architect needs to estimate the total cost of ownership (TCO) for this workload before the migration.\n\nWhich solution will provide the required TCO information?","answer_images":[],"answer_description":"","discussion":[{"timestamp":"1732489620.0","poster":"AzureDP900","content":"A \nMigration Evaluator is a tool provided by AWS that helps estimate the costs of migrating an on-premises workload to AWS, including both the migration cost and the estimated TCO (Total Cost of Ownership) for the new AWS resources.\nBy using Migration Evaluator, the solutions architect can:\nImport data about the existing on-premises Kubernetes cluster and PostgreSQL database.\nConfigure a scenario that accurately reflects the planned migration to Amazon EKS and Amazon RDS.\nExport a Quick Insights report that provides an estimate of the TCO for the migrated workload.","upvote_count":"2","comment_id":"1317231"},{"poster":"kgpoj","comment_id":"1267416","content":"Selected Answer: A\nQuick catch: when you see TCO, think about Migration Evaluator","timestamp":"1723863900.0","upvote_count":"4"},{"content":"Selected Answer: A\nBest Answer: A. Request access to Migration Evaluator. Run the Migration Evaluator Collector and import the data. Configure a scenario. Export a Quick Insights report from Migration Evaluator.\nReasoning:\nComprehensive TCO Analysis: Migration Evaluator is specifically designed to assess migration projects and provides detailed cost estimates.\nAccurate Data: By collecting data from the on-premises environment, Migration Evaluator can generate more accurate cost estimates.\nScenario Modeling: The ability to configure scenarios allows for testing different migration options and their associated costs.\nQuick Insights Report: This provides a summarized overview of the potential TCO.","poster":"Chakanetsa","timestamp":"1721717940.0","comment_id":"1253459","upvote_count":"3"},{"content":"Selected Answer: A\nB:Estimate AWS service costs \nA: Assess current environment and plan migration to AWS","comment_id":"1251576","poster":"vip2","upvote_count":"1","timestamp":"1721457060.0"},{"content":"Selected Answer: A\nOption B is incorrect because AWS Database Migration Service (AWS DMS) is used for migrating databases, not for estimating the TCO. Additionally, the AWS Pricing Calculator alone cannot provide a comprehensive TCO analysis for a complex migration scenario involving Kubernetes and databases.\nOption C is incorrect because AWS Application Migration Service is primarily used for migrating and modernizing applications, not for estimating the TCO of a migration.","poster":"mark_232323","timestamp":"1721188260.0","comment_id":"1249362","upvote_count":"2"},{"comment_id":"1245726","poster":"Moumita","content":"Selected Answer: B\nOption B (AWS DMS assessment + AWS Pricing Calculator) is typically more appropriate and practical.","comments":[{"upvote_count":"1","comment_id":"1246915","timestamp":"1720802460.0","content":"https://docs.aws.amazon.com/whitepapers/latest/how-aws-pricing-works/aws-pricingtco-tools.html","poster":"Moumita"}],"upvote_count":"1","timestamp":"1720642320.0"},{"timestamp":"1719836100.0","upvote_count":"3","content":"A\nMigration Evaluator is used to estimate TCO.","poster":"kupo777","comment_id":"1240134"}],"topic":"1","choices":{"C":"Initialize AWS Application Migration Service. Add the on-premises servers as source servers. Launch a test instance. Output a TCO report from Application Migration Service.","B":"Launch AWS Database Migration Service (AWS DMS) for the on-premises database. Generate an assessment report. Create an estimate in AWS Pricing Calculator for the costs of the EKS migration.","A":"Request access to Migration Evaluator. Run the Migration Evaluator Collector and import the data. Configure a scenario. Export a Quick Insights report from Migration Evaluator.","D":"Access the AWS Cloud Economics Center webpage to assess the AWS Cloud Value Framework. Create an AWS Cost and Usage report from the Cloud Value Framework."},"question_id":477,"answer_ET":"A","question_images":[],"timestamp":"2024-07-01 14:15:00","url":"https://www.examtopics.com/discussions/amazon/view/143130-exam-aws-certified-solutions-architect-professional-sap-c02/","answer":"A","exam_id":33,"unix_timestamp":1719836100},{"id":"ZmkY58JsZboCAaP4QTNs","answers_community":["AE (91%)","9%"],"isMC":true,"question_text":"An events company runs a ticketing platform on AWS. The company’s customers configure and schedule their events on the platform. The events result in large increases of traffic to the platform. The company knows the date and time of each customer’s events.\n\nThe company runs the platform on an Amazon Elastic Container Service (Amazon ECS) cluster. The ECS cluster consists of Amazon EC2 On-Demand Instances that are in an Auto Scaling group. The Auto Scaling group uses a predictive scaling policy.\n\nThe ECS cluster makes frequent requests to an Amazon S3 bucket to download ticket assets. The ECS cluster and the S3 bucket are in the same AWS Region and the same AWS account. Traffic between the ECS cluster and the S3 bucket flows across a NAT gateway.\n\nThe company needs to optimize the cost of the platform without decreasing the platform's availability.\n\nWhich combination of steps will meet these requirements? (Choose two.)","answer_images":[],"answer_description":"","discussion":[{"upvote_count":"7","content":"If you made it this far you will pass.. Good luck everyone! This is a great service.","comment_id":"1317275","timestamp":"1732497900.0","poster":"0b43291"},{"poster":"wbedair","comment_id":"1239922","timestamp":"1719793500.0","upvote_count":"5","content":"Selected Answer: AE\nOptions A and E will meet the requirements most cost-effectively by leveraging the predictability of the workload of known customer events to optimize scaling operations and reducing data transfer costs"},{"comment_id":"1401764","upvote_count":"1","content":"Selected Answer: AE\nTomorrow is the exam. To everyone who has made it this far, good luck and do your best!","poster":"itsjunukim","timestamp":"1742619600.0"},{"timestamp":"1732489320.0","comment_id":"1317230","poster":"AzureDP900","content":"A,E \nBy creating a gateway VPC endpoint for the S3 bucket (option A), you can reduce latency and improve performance by routing traffic directly through Amazon's network, rather than relying on the NAT gateway.\n\n\nAnd by replacing the predictive scaling policy with scheduled scaling policies for the scheduled events (option E), you can avoid scaling instances during low-traffic periods, which would reduce costs and prevent unnecessary charges.","upvote_count":"1"},{"poster":"Chakanetsa","upvote_count":"1","timestamp":"1727425620.0","content":"Selected Answer: AB\nA. Create a gateway VPC endpoint for the S3 bucket: This will allow the ECS cluster to access the S3 bucket directly without the need for traffic to flow through the NAT gateway, reducing costs associated with NAT data transfer.\n\nB. Add another ECS capacity provider that uses an Auto Scaling group of Spot Instances: By introducing Spot Instances with the same weight as On-Demand Instances, the company can take advantage of the cost savings from Spot Instances while maintaining the ability to scale with On-Demand Instances when needed.\n\nThese steps will reduce network traffic costs and take advantage of lower-cost compute options without compromising platform availability.","comment_id":"1289886"},{"poster":"c22ddd8","timestamp":"1720179960.0","comment_id":"1242738","content":"Selected Answer: AE\nSince it is scheduled event , E is correct ans","upvote_count":"2"},{"poster":"vip2","comment_id":"1242715","upvote_count":"2","content":"Selected Answer: AE\nA: no charge for S3 access\nE: know details of date and time --- can scheduled for saving cost","timestamp":"1720177380.0"},{"poster":"kupo777","timestamp":"1719835080.0","comment_id":"1240132","content":"A\nFor S3 communication in the same region, the communication fee is waived by using the gateway VPC endpoint.\nB\nAvailability is reduced when spot instances are used.\nC\nUsing on-demand capacity reservation increases costs.\nD\nUsing S3 Transfer Acceleration increases costs.\nE\nScheduled scaling policies allow resources to be used according to events.\n\nThe answers are A and E.","upvote_count":"3"}],"topic":"1","question_id":478,"choices":{"A":"Create a gateway VPC endpoint for the S3 bucket.","D":"Enable S3 Transfer Acceleration on the S3 bucket.","B":"Add another ECS capacity provider that uses an Auto Scaling group of Spot Instances. Configure the new capacity provider strategy to have the same weight as the existing capacity provider strategy.","E":"Replace the predictive scaling policy with scheduled scaling policies for the scheduled events.","C":"Create On-Demand Capacity Reservations for the applicable instance type for the time period of the scheduled scaling policies."},"answer_ET":"AE","question_images":[],"timestamp":"2024-07-01 02:25:00","url":"https://www.examtopics.com/discussions/amazon/view/143123-exam-aws-certified-solutions-architect-professional-sap-c02/","answer":"AE","unix_timestamp":1719793500,"exam_id":33},{"id":"A4xiNlt6DbwCWV4C1HkJ","answers_community":["C (100%)"],"isMC":true,"question_text":"A company has an organization in AWS Organizations that has a large number of AWS accounts. One of the AWS accounts is designated as a transit account and has a transit gateway that is shared with all of the other AWS accounts. AWS Site-to-Site VPN connections are configured between all of the company’s global offices and the transit account. The company has AWS Config enabled on all of its accounts.\n\nThe company’s networking team needs to centrally manage a list of internal IP address ranges that belong to the global offices. Developers will reference this list to gain access to their applications securely.\n\nWhich solution meets these requirements with the LEAST amount of operational overhead?","answer_images":[],"answer_description":"","discussion":[{"content":"Selected Answer: C\nThe correct answer is option C. In this solution, a VPC prefix list is created in the transit account with all of the internal IP address ranges, and then shared to all of the other accounts using AWS Resource Access Manager. This allows for central management of the IP address ranges, and eliminates the need for manual updates to security group rules in each account. This solution also allows for compliance checks to be run using AWS Config and for any non-compliant security groups to be automatically remediated.\n\nOption A is not correct because it would require manual updates to the JSON file and would also require developers to manually update their security group rules, which would lead to operational overhead.\n\nOption B is not correct because it would require the creation of a new AWS Config managed rule and it would also require manual updates to the security group rules in each account.\n\nOption D is not correct because it would require manual updates to the security group in the transit account and it would also lead to operational overhead.","timestamp":"1673697180.0","comment_id":"775319","poster":"masetromain","upvote_count":"24","comments":[{"comments":[{"content":"I doubt all the security groups in the accounts will use the same CIDR ranges. They just need a way to centrally manage the CIDR prefixes. The question did not say that everyone has to comply and any non-compliant resources needs to be remdiated.","timestamp":"1704767520.0","comment_id":"1117162","poster":"chicagobeef","upvote_count":"2"}],"upvote_count":"1","content":"I agree that option C is probable the best one, but B is also correct, there is no manual updates to the SG, the remediation is automated in ASW Config. In option C you also need to manual update the prefix list, no? Imagine a new CIDR appears in the offices.","timestamp":"1704191100.0","poster":"jpa8300","comment_id":"1111776"}]},{"upvote_count":"1","comment_id":"1322880","timestamp":"1733509860.0","content":"Selected Answer: C\nA VPC Prefix List is a reusable, user-defined resource in Amazon Virtual Private Cloud (VPC) that contains a collection of IP address ranges. These ranges can represent destinations or sources for traffic, and the prefix list can be referenced in various configurations like security groups, route tables, or network ACLs.","poster":"Aritra88"},{"poster":"Tiger4Code","upvote_count":"1","comment_id":"1319967","timestamp":"1732922880.0","content":"Selected Answer: C\nC: in the shared account create a VPC Prefix list, share it using RAM, then SGs can reference it"},{"poster":"amministrazione","content":"C. In the transit account, create a VPC prefix list with all of the internal IP address ranges. Use AWS Resource Access Manager to share the prefix list with all of the other accounts. Use the shared prefix list to configure security group rules in the other accounts.","upvote_count":"1","timestamp":"1725094800.0","comment_id":"1275504"},{"poster":"ninomfr64","comment_id":"1114507","timestamp":"1704459600.0","upvote_count":"2","content":"Selected Answer: C\nNot A. This requires to maintain the JSON file, SNS topic in each account, Lambda to update SG. This is a lot of work, also not clear what accounts holds the S3 with the JSON\nNot B. I was not able to spot a managed AWS Config rule that could help in this case https://docs.aws.amazon.com/config/latest/developerguide/managed-rules-by-aws-config.html (but I do not recall managed rule by hart and this doesn't sound like a remote use case, so in the exam this could trick me)","comments":[{"poster":"ninomfr64","upvote_count":"2","content":"Not D. You can reference a VPC SG in other account VPCs when you have VPC peering in place, this is not mentioned in the scenario https://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-security-groups.html. Since there is a Transit Gateway involved it is unlikely to have VPC peering and the resources in a VPC attached to a transit gateway cannot access the security groups of a different VPC that is also attached to the same transit gateway https://docs.aws.amazon.com/vpc/latest/tgw/tgw-vpc-attachments.html (this option initially was not bad for me)\n\nC works well as prefix lists are created exactly for this purpose https://docs.aws.amazon.com/vpc/latest/userguide/managed-prefix-lists.html","comment_id":"1114509","timestamp":"1704459600.0"}]},{"content":"Selected Answer: C\nC for sure","comment_id":"940986","upvote_count":"1","poster":"NikkyDicky","timestamp":"1688309520.0"},{"upvote_count":"1","timestamp":"1686592560.0","poster":"Asds","comment_id":"921629","content":"Selected Answer: C\nDefinitely prefix"},{"content":"Selected Answer: C\nprefix list and RAM","poster":"mfsec","comment_id":"853201","timestamp":"1680004200.0","upvote_count":"2"},{"comment_id":"841375","upvote_count":"2","poster":"dev112233xx","timestamp":"1679003280.0","content":"Selected Answer: C\nC makes sense ✅"},{"upvote_count":"2","poster":"zozza2023","comment_id":"793200","timestamp":"1675105020.0","content":"Selected Answer: C\nhttps://www.examtopics.com/discussions/amazon/view/82131-exam-aws-certified-solutions-architect-professional-topic-1/"},{"comment_id":"781017","poster":"AjayD123","content":"Selected Answer: C\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/simplify-network-routing-and-security-administration-with-vpc-prefix-lists/#:~:text=A%20Prefix%20List%20is%20a,Resource%20Access%20Manager%20(RAM).","timestamp":"1674124200.0","upvote_count":"4"}],"topic":"1","question_id":479,"choices":{"B":"Create a new AWS Config managed rule that contains all of the internal IP address ranges. Use the rule to check the security groups in each of the accounts to ensure compliance with the list of IP address ranges. Configure the rule to automatically remediate any noncompliant security group that is detected.","A":"Create a JSON file that is hosted in Amazon S3 and that lists all of the internal IP address ranges. Configure an Amazon Simple Notification Service (Amazon SNS) topic in each of the accounts that can be invoked when the JSON file is updated. Subscribe an AWS Lambda function to the SNS topic to update all relevant security group rules with the updated IP address ranges.","C":"In the transit account, create a VPC prefix list with all of the internal IP address ranges. Use AWS Resource Access Manager to share the prefix list with all of the other accounts. Use the shared prefix list to configure security group rules in the other accounts.","D":"In the transit account, create a security group with all of the internal IP address ranges. Configure the security groups in the other accounts to reference the transit account’s security group by using a nested security group reference of “/sg-1a2b3c4d”."},"answer_ET":"C","question_images":[],"timestamp":"2023-01-14 12:53:00","url":"https://www.examtopics.com/discussions/amazon/view/95209-exam-aws-certified-solutions-architect-professional-sap-c02/","answer":"C","unix_timestamp":1673697180,"exam_id":33},{"id":"36mtrrASZxzM9mFVvDOH","topic":"1","unix_timestamp":1673697360,"question_text":"A company runs a new application as a static website in Amazon S3. The company has deployed the application to a production AWS account and uses Amazon CloudFront to deliver the website. The website calls an Amazon API Gateway REST API. An AWS Lambda function backs each API method.\n\nThe company wants to create a CSV report every 2 weeks to show each API Lambda function’s recommended configured memory, recommended cost, and the price difference between current configurations and the recommendations. The company will store the reports in an S3 bucket.\n\nWhich solution will meet these requirements with the LEAST development time?","url":"https://www.examtopics.com/discussions/amazon/view/95211-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_description":"","question_id":480,"answer":"B","answer_images":[],"isMC":true,"choices":{"D":"Purchase the AWS Business Support plan for the production account. Opt in to AWS Compute Optimizer for AWS Trusted Advisor checks. In the Trusted Advisor console, schedule a job to export the cost optimization checks to a .csv file. Store the file in an S3 bucket every 2 weeks.","C":"Opt in to AWS Compute Optimizer. Set up enhanced infrastructure metrics. Within the Compute Optimizer console, schedule a job to export the Lambda recommendations to a .csv file. Store the file in an S3 bucket every 2 weeks.","B":"Opt in to AWS Compute Optimizer. Create a Lambda function that calls the ExportLambdaFunctionRecommendations operation. Export the .csv file to an S3 bucket. Create an Amazon EventBridge rule to schedule the Lambda function to run every 2 weeks.","A":"Create a Lambda function that extracts metrics data for each API Lambda function from Amazon CloudWatch Logs for the 2-week period. Collate the data into tabular format. Store the data as a .csv file in an S3 bucket. Create an Amazon EventBridge rule to schedule the Lambda function to run every 2 weeks."},"exam_id":33,"discussion":[{"timestamp":"1673697360.0","content":"Selected Answer: B\nThe correct answer is B. Opting in to AWS Compute Optimizer and creating a Lambda function that calls the ExportLambdaFunctionRecommendations operation is the least development time solution. This option allows you to use the built-in AWS Compute Optimizer service to extract metrics data and export it as a CSV file, which can then be stored in an S3 bucket.\n\nOption A is not correct because it requires the development of a Lambda function that extracts metrics data and collates it into tabular format, which adds development time. Option C is not correct because it requires the setup of enhanced infrastructure metrics, which adds development time. Option D is not correct because it requires purchasing the AWS Business Support plan and using the Trusted Advisor console, which adds development time.","comment_id":"775322","upvote_count":"23","poster":"masetromain"},{"comment_id":"793211","upvote_count":"9","poster":"zozza2023","timestamp":"1675105380.0","content":"Selected Answer: B\nAWS compute optimizer+ lambda"},{"comment_id":"1322879","poster":"Aritra88","content":"Selected Answer: B\nAnswer B\nSolution Steps\n1. Use AWS Compute Optimizer for Lambda Recommendations\nAWS Compute Optimizer provides recommendations for Lambda functions, including:\n* Recommended memory size to improve performance or reduce cost.\n* Current and recommended cost comparisons.\nYou can query AWS Compute Optimizer using the AWS Management Console, AWS CLI, or SDKs to retrieve the necessary data for your report.\n\n2. Automate Data Retrieval\nSet up an AWS Lambda function to automate the process:\n1. Query Compute Optimizer:\n * Use the GetLambdaFunctionRecommendations API to retrieve:\n * Current memory size\n * Recommended memory size\n * Current and recommended cost","upvote_count":"1","timestamp":"1733509860.0"},{"upvote_count":"1","timestamp":"1725094920.0","content":"B. Opt in to AWS Compute Optimizer. Create a Lambda function that calls the ExportLambdaFunctionRecommendations operation. Export the .csv file to an S3 bucket. Create an Amazon EventBridge rule to schedule the Lambda function to run every 2 weeks.","comment_id":"1275505","poster":"amministrazione"},{"timestamp":"1722349500.0","content":"Why would anyone need to memorize whether Compute Optimizer reports can be scheduled from the UI or must be done through API calls? This is so unnecessary *rolls eyes","poster":"8693a49","upvote_count":"3","comment_id":"1258273"},{"comment_id":"1170085","poster":"khchan123","timestamp":"1710053340.0","comments":[{"upvote_count":"2","comment_id":"1277156","timestamp":"1725335400.0","poster":"helloworldabc","content":"just B"}],"upvote_count":"3","content":"Selected Answer: C\nThe correct answer is C.\nOption A involves creating a custom Lambda function to extract metrics data from CloudWatch Logs and generate the CSV report, which would require more development time compared to using the Compute Optimizer service.\n\nOption B is partially correct, as it involves using Compute Optimizer and a Lambda function, but it misses the ability to schedule recurring exports directly within the Compute Optimizer console.\n\nOption D suggests using AWS Trusted Advisor, which is a service for monitoring best practices and resources, but it does not provide the specific Lambda function memory and cost recommendations required in this scenario."},{"poster":"8608f25","timestamp":"1707512760.0","comment_id":"1145794","upvote_count":"1","content":"Selected Answer: B\nOption B is the most efficient and straightforward solution. By opting into AWS Compute Optimizer, the company can leverage AWS’s service for recommendations on optimal AWS resource configurations based on utilization metrics. Using the ExportLambdaFunctionRecommendations operation allows for automating the retrieval of the desired optimization data with minimal code. Scheduling this operation with an Amazon EventBridge rule to run every 2 weeks and exporting the results directly to a CSV file in an S3 bucket meets all the stated requirements with minimal development effort."},{"upvote_count":"2","comments":[],"content":"Selected Answer: C\nNot A. This requires some serious development, also not 100% sure CW Logs alone provides all the required info.\nNot B. This requires some coding to to call the ExportLambdaFunctionRecommendations API\nNot D. To create CSV reports (organizational view reports) in Trusted Advisor you need to enable Trusted Advisor in you organization, and AWS Organization is not mentioned in the scenario https://docs.aws.amazon.com/awssupport/latest/user/organizational-view.html \n\nC is the right solution as it allows to schedule report with the required info with no development https://docs.aws.amazon.com/compute-optimizer/latest/ug/exporting-recommendations.html. This is was misleading for me as it mentions to set up enhanced infrastructure metrics that is only available for EC2, but you can do it without development (you can do it from console), this add cost but the ask focus on development effort.","timestamp":"1704359760.0","comment_id":"1113479","poster":"ninomfr64"},{"poster":"AWSCertification2024","comment_id":"1112095","timestamp":"1704215520.0","content":"Selected Answer: B\nB is correct\nNot C because Enhanced infrastructure metrics is a paid feature of Compute Optimizer that applies to Amazon EC2 instances and instances that are part of Auto Scaling groups.","upvote_count":"3"},{"timestamp":"1700857980.0","poster":"enk","comment_id":"1079596","content":"Selected Answer: D\nLambda = development. Option D has no development. If you are not familiar with dev'ing - publishing a simple Lambda function can require you to wrap all the Node.js or Python or whatever programming language libraries with it in order to execute correctly within AWS Lambda. Configuring Trusted Advisor (GUI) or scheduling a job is NOT considered Development.","upvote_count":"3"},{"upvote_count":"5","comment_id":"1049909","content":"Selected Answer: D\nBasic plan of Trusted Advisor only has 7 core checks. Business plan has all these, so with LEAST development, it must be business plan.\nCheck categories\n**Cost optimization**\nPerformance\nSecurity\nFault tolerance\nService limits","timestamp":"1697925360.0","poster":"KCjoe"},{"content":"B. \nOption C is not correct because \"Enhanced infrastructure metrics is a paid feature of Compute Optimizer that applies to Amazon EC2 instances.\" \nhttps://docs.aws.amazon.com/compute-optimizer/latest/ug/enhanced-infrastructure-metrics.html","upvote_count":"1","comment_id":"1028330","timestamp":"1696805820.0","poster":"rlf"},{"content":"Selected Answer: B\nComputer Optimizer could generate Export for Lambda Functions one-time. In order to schedule every 2 weeks, EventBridge Scheduler/Schedule Rule should be used.","timestamp":"1694477400.0","upvote_count":"4","comment_id":"1005269","poster":"awsent"},{"timestamp":"1694476440.0","poster":"awsent","content":"Answer: B\nhttps://aws.amazon.com/blogs/compute/optimizing-aws-lambda-cost-and-performance-using-aws-compute-optimizer/","comment_id":"1005257","upvote_count":"1"},{"comment_id":"1004503","poster":"Simon523","upvote_count":"4","timestamp":"1694415300.0","content":"Selected Answer: B\nAWS Compute Optimizer helps avoid overprovisioning and underprovisioning four types of AWS resources—Amazon Elastic Compute Cloud (EC2) instance types, Amazon Elastic Block Store (EBS) volumes, Amazon Elastic Container Service (ECS) services on AWS Fargate, and AWS Lambda functions—based on your utilization data."},{"poster":"NikkyDicky","comment_id":"940996","upvote_count":"1","content":"Selected Answer: B\nits a B","timestamp":"1688310180.0"},{"upvote_count":"3","content":"B - https://docs.aws.amazon.com/compute-optimizer/latest/APIReference/API_ExportLambdaFunctionRecommendations.html","timestamp":"1685521080.0","comment_id":"910986","poster":"EricZhang"},{"comment_id":"896702","upvote_count":"3","timestamp":"1683985020.0","poster":"karma4moksha","content":"Option D i would say as purchasing business support and truster advisor is money but not development time."},{"poster":"Pete987","timestamp":"1679576460.0","comment_id":"848225","upvote_count":"2","content":"Answer D\n\nA. Not the least effort\nB: There is no mention of the need of creating Lambda for exporting recommendations here: https://docs.aws.amazon.com/compute-optimizer/latest/ug/exporting-recommendations.html\n\nC: This would have been correct but \"Enhanced infrastructure metrics\" setting is only for ec2: https://docs.aws.amazon.com/compute-optimizer/latest/ug/enhanced-infrastructure-metrics.html\n\nD: Trusted Advisor can be used.https://docs.aws.amazon.com/awssupport/latest/user/get-started-with-aws-trusted-advisor.html"},{"content":"Selected Answer: B\nB \nhttps://docs.aws.amazon.com/compute-optimizer/latest/ug/exporting-recommendations.html","comment_id":"841383","poster":"dev112233xx","comments":[{"poster":"ninomfr64","timestamp":"1704359940.0","upvote_count":"1","content":"but this proves correctness of answer C, instead B would make use of ExportLambdaFunctionRecommendations API to export thus requesting some little development https://docs.aws.amazon.com/compute-optimizer/latest/APIReference/API_ExportLambdaFunctionRecommendations.html","comment_id":"1113482"}],"timestamp":"1679004240.0","upvote_count":"3"},{"comments":[{"comment_id":"783100","timestamp":"1674287280.0","content":"I correct answer C to B.\nAWS compute optimizer itself cannot make recommendation file by oneself.\nIt need simple lambda.","poster":"masssa","upvote_count":"3","comments":[{"timestamp":"1704359820.0","upvote_count":"1","poster":"ninomfr64","content":"you can export recommendations https://docs.aws.amazon.com/compute-optimizer/latest/ug/exporting-recommendations.html","comment_id":"1113481"}]}],"poster":"masssa","upvote_count":"1","comment_id":"783095","timestamp":"1674286800.0","content":"Selected Answer: C\nI vote C.\nAWS compute optimizer can make lambda recommendation without any development.\nhttps://docs.aws.amazon.com/compute-optimizer/latest/ug/view-lambda-recommendations.html"},{"comment_id":"781019","upvote_count":"3","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/compute-optimizer/latest/APIReference/API_ExportLambdaFunctionRecommendations.html","poster":"AjayD123","timestamp":"1674124560.0"},{"poster":"zhangyu20000","comment_id":"776695","timestamp":"1673795040.0","comments":[{"content":"That's the old way of doing it. The new way does not require the creation of Lambda. Compute optimizer takes care of it","timestamp":"1679576580.0","upvote_count":"1","poster":"Pete987","comment_id":"848228"}],"upvote_count":"2","content":"B is correct\nhttps://aws.amazon.com/blogs/compute/optimizing-aws-lambda-cost-and-performance-using-aws-compute-optimizer/"}],"timestamp":"2023-01-14 12:56:00","question_images":[],"answer_ET":"B","answers_community":["B (79%)","12%","9%"]}],"exam":{"numberOfQuestions":529,"isMCOnly":true,"isImplemented":true,"provider":"Amazon","lastUpdated":"11 Apr 2025","isBeta":false,"id":33,"name":"AWS Certified Solutions Architect - Professional SAP-C02"},"currentPage":96},"__N_SSP":true}