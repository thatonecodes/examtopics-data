{"pageProps":{"questions":[{"id":"ky6QSgZrC2ZRAkqsyibm","unix_timestamp":1741779960,"answer_images":[],"answer":"D","answer_description":"","exam_id":27,"isMC":true,"question_id":11,"topic":"1","answer_ET":"D","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/168918-exam-aws-certified-machine-learning-engineer-associate-mla/","discussion":[{"timestamp":"1743405360.0","upvote_count":"1","comment_id":"1414098","poster":"eesa","content":"Selected Answer: C\nC. Enable Amazon CloudWatch metrics. Observe the ModelSetupTime metric in the SageMaker namespace.\n\n ✅ Correct: This metric will show if model startup time is contributing to the latency.\n\n ✅ Directly confirms or denies the cold start hypothesis.\n\n\nModelSetupTime \nThe time it takes to launch new compute resources for a serverless endpoint. The time can vary depending on the model size, how long it takes to download the model, and the start-up time of the container.\n\nUnits: Microseconds\n\nValid statistics: Average, Min, Max, Sample Count, Percentiles\n\n\n\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html"},{"upvote_count":"2","timestamp":"1741779960.0","poster":"chris_spencer","content":"Selected Answer: D\nModelLoadingWaitTime metric \nmeasures the time taken to load the model","comment_id":"1387844"}],"timestamp":"2025-03-12 12:46:00","answers_community":["D (67%)","C (33%)"],"question_text":"An ML engineer has deployed an Amazon SageMaker model to a serverless endpoint in production. The model is invoked by the InvokeEndpoint API operation.\n\nThe model's latency in production is higher than the baseline latency in the test environment. The ML engineer thinks that the increase in latency is because of model startup time.\n\nWhat should the ML engineer do to confirm or deny this hypothesis?","choices":{"D":"Enable Amazon CloudWatch metrics. Observe the ModelLoadingWaitTime metric in the SageMaker namespace.","A":"Schedule a SageMaker Model Monitor job. Observe metrics about model quality.","C":"Enable Amazon CloudWatch metrics. Observe the ModelSetupTime metric in the SageMaker namespace.","B":"Schedule a SageMaker Model Monitor job with Amazon CloudWatch metrics enabled."}},{"id":"80etNzbSlPSEV3HSRGKf","answer_ET":"A","answer_description":"","answer":"A","timestamp":"2025-03-20 16:34:00","isMC":true,"question_id":12,"answer_images":[],"exam_id":27,"discussion":[{"timestamp":"1742484840.0","poster":"ygn4ei","upvote_count":"1","content":"Selected Answer: A\ncorrect","comment_id":"1401152"}],"topic":"1","choices":{"B":"Create a custom Amazon Elastic Container Registry (Amazon ECR) image that contains the custom script. Push the ECR image to a Docker registry. Attach the Docker image to a SageMaker Studio domain. Select the kernel to run as part of the SageMaker notebook.","C":"Create a custom package index repository. Use AWS CodeArtifact to manage the installation of the custom script. Set up AWS PrivateLink endpoints to connect CodeArtifact to the SageMaker instance. Install the script.","D":"Store the custom script in Amazon S3. Create an AWS Lambda function to install the custom script on new SageMaker notebooks. Configure Amazon EventBridge to invoke the Lambda function when a new SageMaker notebook is initialized.","A":"Create a lifecycle configuration script to install the custom script when a new SageMaker notebook is created. Attach the lifecycle configuration to every new SageMaker notebook as part of the creation steps."},"question_images":[],"question_text":"A company must install a custom script on any newly created Amazon SageMaker notebook instances.\n\nWhich solution will meet this requirement with the LEAST operational overhead?","answers_community":["A (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/169510-exam-aws-certified-machine-learning-engineer-associate-mla/","unix_timestamp":1742484840},{"id":"4KTpsrUUGb0Ji7AgIJOn","answer_description":"","topic":"1","question_id":13,"unix_timestamp":1741851780,"exam_id":27,"url":"https://www.examtopics.com/discussions/amazon/view/168979-exam-aws-certified-machine-learning-engineer-associate-mla/","isMC":true,"question_text":"A company is building a real-time data processing pipeline for an ecommerce application. The application generates a high volume of clickstream data that must be ingested, processed, and visualized in near real time. The company needs a solution that supports SQL for data processing and Jupyter notebooks for interactive analysis.\n\nWhich solution will meet these requirements?","answer_ET":"D","discussion":[{"comment_id":"1401616","timestamp":"1742570520.0","poster":"eesa","content":"Selected Answer: D\n✅ Why Option D is correct:\nRequirements Recap:\n\n Real-time ingestion of high-volume clickstream data\n SQL-based data processing\n Jupyter notebooks for interactive analysis\n Near real-time visualization\n\nAmazon MSK + Managed Flink (Apache Flink):\n\n MSK is ideal for handling high-throughput, real-time event streams like clickstream data.\n Amazon Managed Service for Apache Flink:\n Provides stream processing with low latency\n Supports SQL via Apache Flink SQL\n Can integrate with Jupyter Notebooks via Apache Zeppelin or SageMaker notebooks (for interactive analysis)\n Flink dashboard allows for real-time data visualization and monitoring\n\nThis stack is purpose-built for real-time streaming analytics, supports SQL for processing, and integrates well with notebooks and dashboards.","upvote_count":"2"},{"comment_id":"1401156","timestamp":"1742485080.0","poster":"ygn4ei","upvote_count":"1","content":"Selected Answer: B\ncorrect"},{"timestamp":"1741851780.0","comment_id":"1388216","content":"Selected Answer: C\nShould be C.\n\nAWS Glue with PySpark supports SQL-like transformations and can be integrated with Jupyter notebooks.\n\nD is incorrect because Apache Flink for processing does not natively support SQL for data processing.","upvote_count":"1","poster":"chris_spencer"}],"timestamp":"2025-03-13 08:43:00","question_images":[],"answer_images":[],"answers_community":["D (50%)","C (25%)","B (25%)"],"choices":{"C":"Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to ingest the data. Use AWS Glue with PySpark to process the data. Store the processed data in Amazon S3. Use Amazon QuickSight to visualize the data.","A":"Use Amazon Data Firehose to ingest the data. Create an AWS Lambda function to process the data. Store the processed data in Amazon S3. Use Amazon QuickSight to visualize the data.","B":"Use Amazon Kinesis Data Streams to ingest the data. Use Amazon Data Firehose to transform the data. Use Amazon Athena to process the data. Use Amazon QuickSight to visualize the data.","D":"Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to ingest the data. Use Amazon Managed Service for Apache Flink to process the data. Use the built-in Flink dashboard to visualize the data."},"answer":"D"},{"id":"6EXbGxav4aaI0MAFm60f","answer_images":[],"question_text":"A medical company needs to store clinical data. The data includes personally identifiable information (PII) and protected health information (PHI).\n\nAn ML engineer needs to implement a solution to ensure that the PII and PHI are not used to train ML models.\n\nWhich solution will meet these requirements?","choices":{"A":"Store the clinical data in Amazon S3 buckets. Use AWS Glue DataBrew to mask the PII and PHI before the data is used for model training.","C":"Use Amazon Comprehend to detect and mask the PII before the data is used for model training. Use Amazon Comprehend Medical to detect and mask the PHI before the data is used for model training.","B":"Upload the clinical data to an Amazon Redshift database. Use built-in SQL stored procedures to automatically classify and mask the PII and PHI before the data is used for model training.","D":"Create an AWS Lambda function to encrypt the PII and PHI. Program the Lambda function to save the encrypted data to an Amazon S3 bucket for model training."},"question_id":14,"answers_community":["C (100%)"],"isMC":true,"exam_id":27,"timestamp":"2025-03-20 16:39:00","answer_description":"","unix_timestamp":1742485140,"answer":"C","topic":"1","answer_ET":"C","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/169514-exam-aws-certified-machine-learning-engineer-associate-mla/","discussion":[{"comment_id":"1401158","poster":"ygn4ei","content":"Selected Answer: C\ncorrect","timestamp":"1742485140.0","upvote_count":"1"}]},{"id":"GgxU9UoWBuxOmDd8zvsI","question_images":[],"isMC":true,"answer_ET":"C","topic":"1","question_text":"Case study -\nAn ML engineer is developing a fraud detection model on AWS. The training dataset includes transaction logs, customer profiles, and tables from an on-premises MySQL database. The transaction logs and customer profiles are stored in Amazon S3.\nThe dataset has a class imbalance that affects the learning of the model's algorithm. Additionally, many of the features have interdependencies. The algorithm is not capturing all the desired underlying patterns in the data.\nThe training dataset includes categorical data and numerical data. The ML engineer must prepare the training dataset to maximize the accuracy of the model.\nWhich action will meet this requirement with the LEAST operational overhead?","question_id":15,"answer_description":"","answer":"C","timestamp":"2024-11-27 15:41:00","choices":{"B":"Use AWS Glue to transform the numerical data into categorical data.","A":"Use AWS Glue to transform the categorical data into numerical data.","D":"Use Amazon SageMaker Data Wrangler to transform the numerical data into categorical data.","C":"Use Amazon SageMaker Data Wrangler to transform the categorical data into numerical data."},"exam_id":27,"unix_timestamp":1732718460,"url":"https://www.examtopics.com/discussions/amazon/view/152147-exam-aws-certified-machine-learning-engineer-associate-mla/","discussion":[{"comment_id":"1333612","poster":"ninomfr64","upvote_count":"1","timestamp":"1735491000.0","content":"Selected Answer: C\nYou need to transform category to numeric as ML model works with numbers, thus it is either A or C. Data Wrangler provides a builtin transformation to encode categorical data - https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.html#data-wrangler-transform-cat-encode while Glue doesn't provide a managed transformation for encoding data - https://docs.aws.amazon.com/glue/latest/dg/edit-jobs-transforms.html"},{"upvote_count":"1","comment_id":"1332466","poster":"Pofmagic","content":"Selected Answer: C\nData Wrangler can be used for encoding categorical data, i.e. the process of creating a numerical representation for categories. Categorical encoding encodes categorical data that is in string format into arrays of integers. Data Wrangler supports ordinal and a one-hot encoding, also similarity encoding (more advanced). \nhttps://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.html#data-wrangler-transform-cat-encode\n\nAWS Glue also has Data science recipe steps for One Hot Encoding and Categorical Mapping. \nhttps://docs.aws.amazon.com/databrew/latest/dg/recipe-actions.data-science.html\n\nHowever Data Wrangler is more user-friendly with visual and natural language interfaces for less operational overhead","timestamp":"1735313100.0"},{"timestamp":"1732718460.0","poster":"GiorgioGss","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.html","upvote_count":"3","comment_id":"1318722"}],"answer_images":[],"answers_community":["C (100%)"]}],"exam":{"isMCOnly":false,"id":27,"isImplemented":true,"name":"AWS Certified Machine Learning Engineer - Associate MLA-C01","lastUpdated":"11 Apr 2025","provider":"Amazon","numberOfQuestions":106,"isBeta":false},"currentPage":3},"__N_SSP":true}