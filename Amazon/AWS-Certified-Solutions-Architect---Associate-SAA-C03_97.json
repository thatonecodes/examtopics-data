{"pageProps":{"questions":[{"id":"6RG5szQB5OJ1Vjtp3i31","answer_description":"","choices":{"B":"Create a static website hosted in Amazon CloudFront that invokes AWS Step Functions to resize the images and store the images in an Amazon RDS database.","D":"Create a dynamic website hosted on an automatically scaling Amazon Elastic Container Service (Amazon ECS) cluster that creates a resize job in Amazon Simple Queue Service (Amazon SQS). Set up an image-resizing program that runs on an Amazon EC2 instance to process the resize jobs.","A":"Create a static website hosted in Amazon S3 that invokes AWS Lambda functions to resize the images and store the images in an Amazon S3 bucket.","C":"Create a dynamic website hosted on a web server that runs on an Amazon EC2 instance. Configure a process that runs on the EC2 instance to resize the images and store the images in an Amazon S3 bucket."},"answer_ET":"A","answer":"A","url":"https://www.examtopics.com/discussions/amazon/view/109713-exam-aws-certified-solutions-architect-associate-saa-c03/","question_images":[],"isMC":true,"question_text":"A social media company wants to allow its users to upload images in an application that is hosted in the AWS Cloud. The company needs a solution that automatically resizes the images so that the images can be displayed on multiple device types. The application experiences unpredictable traffic patterns throughout the day. The company is seeking a highly available solution that maximizes scalability.\n\nWhat should a solutions architect do to meet these requirements?","timestamp":"2023-05-19 14:20:00","topic":"1","unix_timestamp":1684498800,"question_id":481,"answer_images":[],"answers_community":["A (100%)"],"exam_id":31,"discussion":[{"poster":"cloudenthusiast","content":"Selected Answer: A\nBy using Amazon S3 and AWS Lambda together, you can create a serverless architecture that provides highly scalable and available image resizing capabilities. Here's how the solution would work:\n\nSet up an Amazon S3 bucket to store the original images uploaded by users.\nConfigure an event trigger on the S3 bucket to invoke an AWS Lambda function whenever a new image is uploaded.\nThe Lambda function can be designed to retrieve the uploaded image, perform the necessary resizing operations based on device requirements, and store the resized images back in the S3 bucket or a different bucket designated for resized images.\nConfigure the Amazon S3 bucket to make the resized images publicly accessible for serving to users.","upvote_count":"18","comment_id":"901899","timestamp":"1684498800.0"},{"comment_id":"1351888","upvote_count":"1","poster":"surajkrishnamurthy","timestamp":"1738762320.0","content":"Selected Answer: A\nIs Static Website Read only ?\nEventhough I have selected Answer as A"},{"comment_id":"1322986","poster":"LeonSauveterre","timestamp":"1733538780.0","content":"Selected Answer: A\nOK. A is better, although I chose D originally.\n\nI guess it shouldn't be D simply because it overkills for this scenario and introduces higher operational overhead (even though option D can handle more complex workloads than Lambda if needed).","upvote_count":"1"},{"content":"How can end user upload an image to S3 bucket with static hosting. I believe it should be dynamic website (Answer D)","comment_id":"1221972","poster":"cnureddy","upvote_count":"2","timestamp":"1717136160.0"},{"poster":"mr123dd","content":"image = static = S3 or cloudfront\nbut image is unstructured data so you dont store it in a relational database like RDS\nand Step Function is not for processing\nSo A","upvote_count":"3","comment_id":"1113390","timestamp":"1704352920.0"},{"upvote_count":"2","content":"Selected Answer: A\nThis meets all the key requirements:\n\nS3 static website provides high availability and auto scaling to handle unpredictable traffic\nLambda functions invoked from the S3 site can resize images on the fly\nStoring images in S3 buckets provides durability, scalability and high throughput\nServerless approach with S3 and Lambda maximizes scalability and availability","poster":"Guru4Cloud","timestamp":"1692786840.0","comment_id":"988193"},{"poster":"TariqKipkemei","content":"Selected Answer: A\nScalability = S3, Lamda\nautomatically resize images = Lambda","timestamp":"1689826020.0","upvote_count":"3","comment_id":"957113"}]},{"id":"Ol5rZl9hgeefBr8rjahe","topic":"1","answer_description":"","unix_timestamp":1684326300,"question_text":"A company is running a microservices application on Amazon EC2 instances. The company wants to migrate the application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster for scalability. The company must configure the Amazon EKS control plane with endpoint private access set to true and endpoint public access set to false to maintain security compliance. The company must also put the data plane in private subnets. However, the company has received error notifications because the node cannot join the cluster.\n\nWhich solution will allow the node to join the cluster?","answer_ET":"B","answer_images":[],"exam_id":31,"answer":"B","question_images":[],"timestamp":"2023-05-17 14:25:00","isMC":true,"answers_community":["B (58%)","A (42%)"],"question_id":482,"choices":{"A":"Grant the required permission in AWS Identity and Access Management (IAM) to the AmazonEKSNodeRole IAM role.","B":"Create interface VPC endpoints to allow nodes to access the control plane.","C":"Recreate nodes in the public subnet. Restrict security groups for EC2 nodes.","D":"Allow outbound traffic in the security group of the nodes."},"discussion":[{"upvote_count":"21","timestamp":"1684749600.0","comments":[{"timestamp":"1715186040.0","poster":"TwinSpark","content":"correct i was going for B, but A looks better. \nhttps://docs.aws.amazon.com/eks/latest/userguide/cluster-endpoint.html\n\"When you enable endpoint private access for your cluster, Amazon EKS creates a Route 53 private hosted zone on your behalf and associates it with your cluster's VPC. This private hosted zone is managed by Amazon EKS, and it doesn't appear in your account's Route 53 resources. \"","comment_id":"1208474","upvote_count":"1"},{"poster":"h0ng97_spare_002","comments":[{"upvote_count":"1","comment_id":"1316561","timestamp":"1732337100.0","poster":"JA2018","content":"which implies AmazonEKSNodeRole IAM role had already been configured..... that leaves answer B as the only viable choice"}],"comment_id":"1182999","upvote_count":"4","timestamp":"1711426560.0","content":"https://docs.aws.amazon.com/eks/latest/userguide/create-node-role.html#:~:text=Before,launched\n\n\"Before you can launch nodes and register them into a cluster, you must create an IAM role for those nodes to use when they are launched.\""}],"content":"Selected Answer: A\nCheck this : https://docs.aws.amazon.com/eks/latest/userguide/create-node-role.html\n\nAlso, EKS does not require VPC endpoints. This is not the right use case for EKS","comment_id":"903908","poster":"y0"},{"comment_id":"901902","timestamp":"1684498920.0","content":"Selected Answer: B\nBy creating interface VPC endpoints, you can enable the necessary communication between the Amazon EKS control plane and the nodes in private subnets. This solution ensures that the control plane maintains endpoint private access (set to true) and endpoint public access (set to false) for security compliance.","upvote_count":"20","poster":"cloudenthusiast"},{"poster":"Dantecito","content":"Selected Answer: B\nB. \"The company has received error notifications because the node cannot join the cluster\" it's a network connectivity issue, not permissions.","timestamp":"1739308440.0","upvote_count":"1","comment_id":"1355188"},{"content":"Selected Answer: B\nSince the company has configured the Amazon EKS control plane with private access enabled (true) and public access disabled (false), the EKS API server is only accessible through private network routes. The worker nodes, which are in private subnets, must be able to communicate with the EKS control plane for proper functionality.\n\nWhen the nodes cannot join the cluster, the most common cause is that they are unable to reach the EKS API server. In this case, since the control plane is private-only, AWS recommends using interface VPC endpoints (AWS PrivateLink) to enable private communication between the nodes and the control plane.","timestamp":"1738202640.0","poster":"4729e6c","upvote_count":"1","comment_id":"1348872"},{"poster":"Salilgen","content":"Selected Answer: A\nIMO answer is A.\nIn private cluster you need interface VPC endpoints to necessary connect to some AWS services (https://docs.aws.amazon.com/eks/latest/userguide/private-clusters.html) but EKS node always connect to control plane by EKS owned ENI. \nSee this: it is very clear.\nhttps://keetmalin.medium.com/eks-cluster-network-architecture-for-worker-nodes-635e067c8c2a","upvote_count":"2","timestamp":"1735052700.0","comment_id":"1331161"},{"poster":"LeonSauveterre","content":"Selected Answer: B\nThe question is trying to tell us:\n1. \"private access = true\" and \"public access = false\". So the control plane endpoint is private and only accessible from within the VPC.\n2. Nodes (data plane) are in private subnets.\n\nOption A is NECESSARY but lack of permissions would generally cause authorization errors, not connectivity errors (that lead to failure of joining the cluster). Only after you got this right, you would likely receive errors about joining. So apparently we have already configured auth correctly, meaning A is not the answer.\n\nOption C exposes the nodes to the internet. Wrong.\n\nOption D is important for nodes to communicate with AWS services (because of allowing outbound traffic), but itâ€™s not sufficient if the required VPC interface endpoints are not even there.","comment_id":"1322992","timestamp":"1733539860.0","upvote_count":"1"},{"comment_id":"1241609","content":"Selected Answer: A\nAmazonEKSNodeRole IAM role\n\nhttps://docs.aws.amazon.com/eks/latest/userguide/create-node-role.html","upvote_count":"2","timestamp":"1720033440.0","poster":"a7md0"},{"timestamp":"1719623520.0","poster":"emakid","comment_id":"1238977","upvote_count":"3","content":"Selected Answer: B\nWhen Amazon EKS nodes cannot join the cluster, especially when the control plane is set to private access only, the issue typically revolves around networking and connectivity. When the EKS control plane is configured with private access only, the nodes must communicate with the control plane over private IP addresses. Creating VPC endpoints (specifically, com.amazonaws.<region>.eks) allows traffic between the EKS nodes and the control plane to be routed privately within the VPC, which resolves the connectivity issue."},{"upvote_count":"2","content":"Selected Answer: B\nI think is B.","timestamp":"1719283860.0","poster":"Gape4","comment_id":"1236675"},{"upvote_count":"2","timestamp":"1718566440.0","content":"Selected Answer: B\nError they have mentioned is at network level. They are not saying authorisation is failed rather noce is enable to connect to cluster aka connectivity issue. So answer it must be B","comment_id":"1231517","poster":"MandAsh"},{"poster":"Rocconno","comment_id":"1228404","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/eks/latest/userguide/private-clusters.html\n\"Any self-managed nodes must be deployed to subnets that have the VPC interface endpoints that you require. If you create a managed node group, the VPC interface endpoint security group must allow the CIDR for the subnets, or you must add the created node security group to the VPC interface endpoint security group.\"","timestamp":"1718103780.0","upvote_count":"2"},{"timestamp":"1715237700.0","poster":"stalk98","upvote_count":"1","content":"I Think is A","comment_id":"1208725"},{"content":"Selected Answer: B\nB is good to go","timestamp":"1714203600.0","poster":"trinh_le","comment_id":"1202993","upvote_count":"3"},{"poster":"JackyCCK","comment_id":"1190724","timestamp":"1712456760.0","upvote_count":"2","content":"S3/DynamoDB - VPC endpoint, other service should use interface endpoint so B is incorrect"},{"content":"Selected Answer: B\nBecause of these two assertions:\n- Amazon EKS control plane with endpoint private access set to true and endpoint public access set to false to maintain security compliance.\n( The company must also put the data plane in private subnets.\nThe best answer is related to Networking, Private Subnets (EKS Ctr Plane is strictly private and Data Plane stick under private subnets) and not related to EKS autodeployment that sure need an IAM policy. So according to me, answer B is the best answer.","poster":"bujuman","timestamp":"1712219700.0","comment_id":"1189179","upvote_count":"3"},{"poster":"potomac","comment_id":"1063418","timestamp":"1699230540.0","upvote_count":"2","content":"Selected Answer: A\nBefore can launch nodes and register nodes into a EKS cluster, must create an IAM role for those nodes to use when they are launched."},{"content":"A is correct:\nTo deploy a new EKS cluster:\n1. Need to have a VPC and at least 2 subnets\n2. An IAM role that have permission to create and describe EKS cluster","timestamp":"1697891640.0","upvote_count":"3","comment_id":"1049481","poster":"thanhnv142"},{"timestamp":"1697891160.0","upvote_count":"2","comment_id":"1049479","content":"A is good to go. B is not correct because they already setup connection to control plane.","comments":[{"poster":"pentium75","content":"\"They already setup connection to control plane\" where did you read that?","upvote_count":"3","timestamp":"1704107880.0","comment_id":"1111109"}],"poster":"thanhnv142"},{"timestamp":"1693238820.0","poster":"Bennyboy789","comment_id":"992369","upvote_count":"3","content":"Selected Answer: B\nIn Amazon EKS, nodes need to communicate with the EKS control plane. When the Amazon EKS control plane endpoint access is set to private, you need to create interface VPC endpoints in the VPC where your nodes are running. This allows the nodes to access the control plane privately without needing public internet access."},{"comment_id":"989496","upvote_count":"3","content":"Selected Answer: A\nThis should be an associate-level question.\n\nhttps://repost.aws/knowledge-center/eks-worker-nodes-cluster\nhttps://docs.aws.amazon.com/eks/latest/userguide/create-node-role.html","timestamp":"1692911400.0","comments":[{"timestamp":"1692911400.0","poster":"Smart","content":"This should NOT be an associate-level question","upvote_count":"8","comment_id":"989497"}],"poster":"Smart"},{"timestamp":"1692785580.0","content":"Selected Answer: B\nSince the EKS control plane has public access disabled and is in private subnets, the EKS nodes in the private subnets need interface VPC endpoints to reach the control plane API.\n\nCreating these interface endpoints allows the EKS nodes to communicate with the control plane privately within the VPC to join the cluster.","comments":[{"comment_id":"988190","upvote_count":"3","timestamp":"1692786060.0","comments":[{"upvote_count":"2","comment_id":"988191","timestamp":"1692786120.0","content":"Reason why, not A\nWhile security groups and IAM permissions are important considerations for networking and authentication, they alone won't resolve the issue of nodes not being able to join the cluster when the control plane is configured for private access.","poster":"Guru4Cloud"}],"poster":"Guru4Cloud","content":"Why B\nPrivate Control Plane: You've configured the Amazon EKS control plane with private endpoint access, which means the control plane is not accessible over the public internet.\n\nVPC Endpoints: When the control plane is set to private access, you need to set up VPC endpoints for the Amazon EKS service so that the nodes in your private subnets can communicate with the EKS control plane without going through the public internet. These are known as interface VPC endpoints."}],"upvote_count":"4","comment_id":"988182","poster":"Guru4Cloud"},{"poster":"0628atv","upvote_count":"3","content":"Selected Answer: A\nbecause the node cannot join the cluster.","comment_id":"953153","timestamp":"1689495000.0"},{"comment_id":"947104","timestamp":"1688898300.0","upvote_count":"2","content":"Selected Answer: A\nA. When it comes to troubleshooting, First thing to do is to check the if the proper permissions are given to the roles. Since the question doesn't mention any procedure how they configure/created the eks cluster and nodes, you need to check on the policies and it is also a requirement on creating EKS\n\nYou can check this site https://docs.aws.amazon.com/eks/latest/userguide/troubleshooting.html\nhttps://docs.aws.amazon.com/eks/latest/userguide/create-node-role.html","poster":"Iragmt"},{"timestamp":"1688836260.0","upvote_count":"2","poster":"jaydesai8","comment_id":"946636","content":"Selected Answer: B\nAs mention in the link below \n\nKubernetes API requests within your cluster's VPC (such as node to control plane communication) use the private VPC endpoint.\nhttps://docs.aws.amazon.com/eks/latest/userguide/cluster-endpoint.html\n\nAnswer is B"},{"poster":"narddrer","comment_id":"946634","timestamp":"1688836020.0","content":"Selected Answer: B\nQuestion is more about Private and public endpoint for nodes, more about routing and registering than accessing. \nas per the link https://docs.aws.amazon.com/eks/latest/userguide/cluster-endpoint.html","upvote_count":"2"},{"content":"Selected Answer: B\nGoing with B here:\n--> https://docs.aws.amazon.com/eks/latest/userguide/vpc-interface-endpoints.html","upvote_count":"2","timestamp":"1688707680.0","comment_id":"945336","poster":"VellaDevil"},{"comment_id":"927713","timestamp":"1687193640.0","upvote_count":"2","comments":[{"poster":"CVliner","upvote_count":"3","timestamp":"1687434420.0","comments":[{"upvote_count":"1","timestamp":"1717382760.0","content":"(A) does not fit for the security of nodes; it's the permission of the node to join the cluster NOT the protection of the node. (A) is correct. Before you can launch nodes and register them into a cluster, you must create an IAM role for those nodes to use when they are launched.\n\nhttps://docs.aws.amazon.com/eks/latest/userguide/create-node-role.html\n\nWe have to write IAM role name eksClusterRole in order for the cluster to create load balancers. \n\nhttps://docs.aws.amazon.com/eks/latest/userguide/service_IAM_role.html","poster":"NSA_Poker","comment_id":"1223389"}],"content":"Please be noted, that A fits only for security for nodes (not cluster) For cluster we have to write IAM role name eksClusterRole. https://docs.aws.amazon.com/eks/latest/userguide/service_IAM_role.html","comment_id":"930454"}],"poster":"vrevkov","content":"Selected Answer: A\nThis is A because the control plane and data plane nodes are in the same VPC and data plane nodes don't need any interface VPC endpoints, but they definitely need to have IAM role with correct permissions.\nhttps://docs.aws.amazon.com/eks/latest/userguide/create-node-role.html"},{"poster":"antropaws","content":"Selected Answer: A\nThe question is: \n\nWhich solution will allow the node to join the cluster?\n\nThe answer is A:\n\nAmazon EKS node IAM role\n\nNodes receive permissions for these API calls through an IAM instance profile and associated policies. Before you can launch nodes and register them into a cluster, you must create an IAM role for those nodes to use when they are launched. This requirement applies to nodes launched with the Amazon EKS optimized AMI provided by Amazon, or with any other node AMIs that you intend to use.\n\nhttps://docs.aws.amazon.com/eks/latest/userguide/create-node-role.html","timestamp":"1685965980.0","comment_id":"915405","upvote_count":"6"},{"poster":"elmogy","comment_id":"909057","timestamp":"1685337900.0","upvote_count":"5","content":"Selected Answer: B\nKubernetes API requests within your cluster's VPC (such as node to control plane communication) use the private VPC endpoint.\n\nhttps://docs.aws.amazon.com/eks/latest/userguide/cluster-endpoint.html"},{"content":"Selected Answer: B\nb for me","timestamp":"1684326300.0","upvote_count":"4","poster":"nosense","comment_id":"900118"}],"url":"https://www.examtopics.com/discussions/amazon/view/109534-exam-aws-certified-solutions-architect-associate-saa-c03/"},{"id":"AJlSeclyX9wMwDgVMMdu","question_id":483,"question_text":"A company is migrating an on-premises application to AWS. The company wants to use Amazon Redshift as a solution.\n\nWhich use cases are suitable for Amazon Redshift in this scenario? (Choose three.)","choices":{"B":"Supporting client-side and server-side encryption","F":"Creating a secondary replica of the cluster by using the AWS Management Console","D":"Caching data to reduce the pressure on the backend database","E":"Scaling globally to support petabytes of data and tens of millions of requests per minute","A":"Supporting data APIs to access data with traditional, containerized, and event-driven applications","C":"Building analytics workloads during specified hours and when the application is not active"},"topic":"1","answer":"BCE","url":"https://www.examtopics.com/discussions/amazon/view/109535-exam-aws-certified-solutions-architect-associate-saa-c03/","exam_id":31,"answers_community":["BCE (60%)","ACE (18%)","7%","Other"],"discussion":[{"poster":"elmogy","timestamp":"1685338680.0","comments":[{"comment_id":"1111113","upvote_count":"2","poster":"pentium75","timestamp":"1704108960.0","content":"\"Data APIs are not typically used with Redshift\" -> \"With the Data API, you can programmatically access data in your Amazon Redshift cluster from different AWS services such as AWS Lambda, Amazon SageMaker notebooks, AWS Cloud9, and also your on-premises applications using the AWS SDK. This allows you to build cloud-native, containerized, serverless, web-based, and event-driven applications on the AWS Cloud.\""}],"upvote_count":"21","comment_id":"909062","content":"Selected Answer: BCE\nAmazon Redshift is a data warehouse solution, so it is suitable for:\n-Supporting encryption (client-side and server-side)\n-Handling analytics workloads, especially during off-peak hours when the application is less active\n-Scaling to large amounts of data and high query volumes for analytics purposes\n\nThe following options are incorrect because:\nA) Data APIs are not typically used with Redshift. It is more for running SQL queries and analytics.\nD) Redshift is not typically used for caching data. It is for analytics and data warehouse purposes.\nF) Redshift clusters do not create replicas in the management console. They are standalone clusters. you could create DR cluster from snapshot and restore to another region (automated or manual) but I do not think this what is meant in this option."},{"content":"Selected Answer: BCE\nB. Supporting client-side and server-side encryption: Amazon Redshift supports both client-side and server-side encryption for improved data security.\n\nC. Building analytics workloads during specified hours and when the application is not active: Amazon Redshift is optimized for running complex analytic queries against very large datasets, making it a good choice for this use case.\n\nE. Scaling globally to support petabytes of data and tens of millions of requests per minute: Amazon Redshift is designed to handle petabytes of data, and to deliver fast query and I/O performance for virtually any size dataset.","comment_id":"902962","poster":"Rob1L","timestamp":"1684651380.0","upvote_count":"6"},{"content":"Selected Answer: ABC\nChatgpt says ABC.\nD. Not for caching.\nE. It is not optimized for high-volume like millions of requests per minute.\nF. Does not natively support creating a secondary replica of the cluster.","timestamp":"1739309640.0","upvote_count":"1","poster":"Dantecito","comment_id":"1355198"},{"comment_id":"1322999","content":"Selected Answer: BCE\nYOU MUST KNOW: Amazon Redshift is a cloud-based data warehouse service designed for analytics and reporting on large datasets, optimized for complex queries and analytical workloads rather than transactional or real-time applications.\n\nAfter you memorized that, option A is incorrect. If you want to work on operational workloads, then Aurora, Amazon RDS, DynamoDB, and such, are better choices.\n\nOption D: Caching is not a core use case for Redshift. You can choose DAX (Amazon DynamoDB Accelerator) or ElastiCache to mitigate database pressure.\n\nOption E: I choose that. BUT, Redshift is not designed for *high-concurrency* transactional workloads like \"10s of mils of reqs/min\", although it CAN handle massive datasets. Well... It seems partially true to me compared to other definitely wrong options.","upvote_count":"2","timestamp":"1733540520.0","poster":"LeonSauveterre"},{"content":"Selected Answer: ACE\nB is not correct, how it can do encryption at client side ?","poster":"rpmaws","comment_id":"1282383","upvote_count":"2","timestamp":"1726100640.0"},{"comment_id":"1241047","timestamp":"1719954840.0","content":"Found this related to A -- but specific to Redshift Serverless - but should qualify as a Redshift use case\nThe Data API enables you to seamlessly access data from Redshift Serverless with all types of traditional, cloud-native, and containerized serverless web service-based applications and event-driven applications.\nhttps://www.amazonaws.cn/en/blog-selection/use-the-amazon-redshift-data-api-to-interact-with-amazon-redshift-serverless/","upvote_count":"1","poster":"3bdf1cc"},{"timestamp":"1717386180.0","content":"Selected Answer: BCE\nThe following are obviously incorrect:\n(D) Redshift is not as suitable as ElastiCache for caching.\n(F) A secondary replica of the cluster is not supported.\n\nThe debate is between BCE & ACE or simplified, between A & C. \n(A) is incorrect bc there is a difference btw Amazon Redshift Data API & API Gateway. API Gateway supports containerized and serverless workloads, as well as web applications. Amazon Redshift Data API is a built in API to access Redshift data with web servicesâ€“based applications, including AWS Lambda, Amazon SageMaker notebooks, and AWS Cloud9.\nhttps://aws.amazon.com/blogs/big-data/build-a-serverless-analytics-application-with-amazon-redshift-and-amazon-api-gateway/\n\n(B) is correct. You have the following options of protecting data at rest in Amazon Redshift. Use server-side encryption OR use client-side encryption\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/security-encryption.html","upvote_count":"2","comment_id":"1223400","poster":"NSA_Poker"},{"content":"Redshift is OLAP(online analytical processing) so D is wrong, \"when the application is not active\"","comments":[{"poster":"JackyCCK","content":"*C is wrong","comment_id":"1190432","upvote_count":"1","timestamp":"1712412600.0"}],"upvote_count":"2","timestamp":"1712412480.0","comment_id":"1190431","poster":"JackyCCK"},{"upvote_count":"3","poster":"awsgeek75","timestamp":"1704918720.0","content":"Selected Answer: ACE\nA, C, E are for data and Redshift is data warehouse.\nB is too generic of a choice\nD caching is not the main purpose of Redshift\nF replication is not main use of Redshift \n\nCE are easy\nBetween AB, I chose A because Redshift supports data API and client-side encryption is not Redshift specific","comment_id":"1119013"},{"upvote_count":"2","content":"Selected Answer: ABD\nA: source https://aws.amazon.com/blogs/big-data/using-the-amazon-redshift-data-api-to-interact-with-amazon-redshift-clusters/\nB: source: https://docs.aws.amazon.com/redshift/latest/mgmt/security-encryption.html\nC: not sure, but you can configure scheduled queries, but the remark \" and when the application is not active \" , that is not relevant.\nD: source https://docs.aws.amazon.com/redshift/latest/dg/c_challenges_achieving_high_performance_queries.html\nE: Scaling globally is not supported; redshift is only a regional service.\nF: only read replica is supported. So not a secondary replica of the cluster.","poster":"1rob","comment_id":"1118546","timestamp":"1704890040.0"},{"comment_id":"1111114","poster":"pentium75","upvote_count":"2","content":"Selected Answer: ABD\nA: https://aws.amazon.com/de/blogs/big-data/get-started-with-the-amazon-redshift-data-api/\nB: https://docs.aws.amazon.com/redshift/latest/mgmt/security-encryption.html\nD: https://docs.aws.amazon.com/redshift/latest/dg/c_challenges_achieving_high_performance_queries.html#result-caching\n\nNot C: Redshift is a Data Warehouse; you can use that for analytics, but it is not directly related to an \"application\"\nNot E: \"Petabytes of data\" yes, but \"tens of millions of requests per minute\" is not a typical feature of Redshift\nNor F: Replicas are not a Redshift feature","timestamp":"1704109020.0"},{"upvote_count":"2","poster":"TariqKipkemei","comment_id":"1066970","timestamp":"1699596120.0","content":"Selected Answer: ACE\nTechnically both options A and B apply, this is from the links below:\n\nA. You can access your Amazon Redshift database using the built-in Amazon Redshift Data API. \nhttps://docs.aws.amazon.com/redshift/latest/mgmt/data-api.html#:~:text=in%20Amazon%20Redshift-,Data%20API,-.%20Using%20this%20API\n\nB. You can encrypt data client-side and upload the encrypted data to Amazon Redshift. In this case, you manage the encryption process, the encryption keys, and related tools.\n\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/security-encryption.html#:~:text=Use-,client%2Dside,-encryption%20%E2%80%93%20You%20can"},{"upvote_count":"3","comments":[{"poster":"potomac","content":"change to ABD\n\nTo reduce query runtime and improve system performance, Amazon Redshift caches the results of certain types of queries in memory on the leader node. When a user submits a query, Amazon Redshift checks the results cache for a valid, cached copy of the query results. If a match is found in the result cache, Amazon Redshift uses the cached results and doesn't run the query. Result caching is transparent to the user.","comment_id":"1063430","upvote_count":"1","timestamp":"1699232400.0"}],"content":"Selected Answer: ABC\nAmazon Redshift provides a Data API that you can use to painlessly access data from Amazon Redshift with all types of traditional, cloud-native, and containerized, serverless web services-based and event-driven applications.\n\nAmazon Redshift supports up to 500 concurrent queries per cluster, which may be expanded by adding more nodes to the cluster.","poster":"potomac","comment_id":"1063423","timestamp":"1699231380.0"},{"poster":"Guru4Cloud","upvote_count":"3","comment_id":"988165","content":"Selected Answer: BCE\nThe key use cases for Amazon Redshift that fit this scenario are:\n\nB) Redshift supports both client-side and server-side encryption to protect sensitive data.\n\nC) Redshift is well suited for running batch analytics workloads during off-peak times without affecting OLTP systems.\n\nE) Redshift can scale to massive datasets and concurrent users to support large analytics workloads.","timestamp":"1692784560.0"},{"poster":"cd93","comment_id":"987875","upvote_count":"3","timestamp":"1692758220.0","content":"Selected Answer: BCD\nWhy E lol? It's a data warehouse! it has no need to support millions of requests, it is not mentioned anywhere (https://aws.amazon.com/redshift/features)\n\nIn fact Redshift editor supports max 500 connections and workgroup support max 2000 connections at once, see it's quota page\nRedshift has a cache layer, D is correct"},{"poster":"mrsoa","upvote_count":"2","content":"Selected Answer: BCE\nBCE, For B this is why \n\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/security-encryption.html","timestamp":"1690472340.0","comment_id":"964859"},{"poster":"james2033","timestamp":"1689664320.0","comments":[{"timestamp":"1689664380.0","comment_id":"955086","content":"Typo, I want said \"C and E are next chosen correct answers.\"","upvote_count":"2","poster":"james2033"}],"upvote_count":"2","content":"Selected Answer: ACE\nQuote: \"The Data API enables you to seamlessly access data from Redshift Serverless with all types of traditional, cloud-native, and containerized serverless web service-based applications and event-driven applications.\" at https://aws.amazon.com/blogs/big-data/use-the-amazon-redshift-data-api-to-interact-with-amazon-redshift-serverless/ (28/4/2023). Choose A. B and C are next chosen correct answers.","comment_id":"955084"},{"timestamp":"1689495240.0","poster":"0628atv","upvote_count":"2","content":"Selected Answer: ACE\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/welcome.html","comment_id":"953161"},{"timestamp":"1684636320.0","content":"CEF for me","poster":"omoakin","upvote_count":"2","comment_id":"902855"},{"timestamp":"1684415040.0","poster":"Efren","upvote_count":"1","content":"A seems correct \n\nThe Data API enables you to seamlessly access data from Redshift Serverless with all types of traditional, cloud-native, and containerized serverless web service-based applications and event-driven applications.","comment_id":"901228"},{"comments":[{"content":"U mean ACE rite?","comment_id":"901620","comments":[{"content":"Yeah not sure, but i would say ACE","upvote_count":"1","timestamp":"1684653240.0","poster":"Efren","comment_id":"902985"}],"upvote_count":"1","poster":"y0","timestamp":"1684467180.0"}],"comment_id":"901224","poster":"Efren","content":"BCE for me","upvote_count":"1","timestamp":"1684414860.0"},{"timestamp":"1684326420.0","upvote_count":"2","poster":"nosense","comments":[],"comment_id":"900123","content":"Selected Answer: ACF\nb it's working, but not primary"}],"answer_ET":"BCE","answer_images":[],"question_images":[],"isMC":true,"answer_description":"","timestamp":"2023-05-17 14:27:00","unix_timestamp":1684326420},{"id":"urUjlItKoxSyxPC7Klul","exam_id":31,"unix_timestamp":1684499520,"question_text":"A company provides an API interface to customers so the customers can retrieve their financial information. Ð•he company expects a larger number of requests during peak usage times of the year.\n\nThe company requires the API to respond consistently with low latency to ensure customer satisfaction. The company needs to provide a compute host for the API.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","answer_description":"","answer_ET":"B","choices":{"B":"Use Amazon API Gateway and AWS Lambda functions with provisioned concurrency.","D":"Use Amazon API Gateway and AWS Lambda functions with reserved concurrency.","A":"Use an Application Load Balancer and Amazon Elastic Container Service (Amazon ECS).","C":"Use an Application Load Balancer and an Amazon Elastic Kubernetes Service (Amazon EKS) cluster."},"discussion":[{"content":"Selected Answer: B\nIn the context of the given scenario, where the company wants low latency and consistent performance for their API during peak usage times, it would be more suitable to use provisioned concurrency. By allocating a specific number of concurrent executions, the company can ensure that there are enough function instances available to handle the expected load and minimize the impact of cold starts. This will result in lower latency and improved performance for the API.","poster":"cloudenthusiast","upvote_count":"13","comment_id":"901914","timestamp":"1684499520.0"},{"timestamp":"1693253700.0","upvote_count":"7","poster":"Bennyboy789","content":"Selected Answer: B\nProvisioned - minimizing cold starts and providing low latency.","comment_id":"992594"},{"poster":"LeonSauveterre","upvote_count":"2","timestamp":"1733541300.0","comment_id":"1323004","content":"Selected Answer: B\nConcurrency of lambda function, provisioned vs reserved:\n\nReserved Concurrency:\n1. Desired instances can start without interference from other functions\n2. Guarantees available execution capacity for critical functions\n\nProvisioned Concurrency:\n1. Also guarantees consistent performance\n2. [!!] Allows you to pre-warm a specific number of function instances to reduce cold start latency"},{"content":"Selected Answer: A\nhttps://docs.aws.amazon.com/lambda/latest/dg/lambda-concurrency.html#reserved-and-provisioned\n\nConsistency decreases if you exceed your provisioned instance. Lets say you have 1000 (default) provisioned instances and the load is 1500. The new 500 will have to wait until the first 1000 concurrent calls finish. This is solved by increasing the provisioned concurrency to 1500.","timestamp":"1704919080.0","comment_id":"1119015","poster":"awsgeek75","upvote_count":"2"},{"poster":"1rob","comments":[{"comments":[{"upvote_count":"4","timestamp":"1703849160.0","poster":"pdragon1981","content":"Sorry I understand bad the text, coorect answer is B, as for my understanding now the host is the device that the costumer needs to connect with API Gateway, bellow explains well the logic\nhttps://aws.amazon.com/api-gateway/","comment_id":"1108632"}],"comment_id":"1108629","poster":"pdragon1981","timestamp":"1703848860.0","upvote_count":"3","content":"Exactly, inicially I was thinking on B but if company must provide a host I would say that only option A is feasible"}],"timestamp":"1701412380.0","comment_id":"1084994","content":"Selected Answer: A\nSo I have my doubts here. The question also states ;\"The company needs to provide a compute host for the API.\" Imho this implies to have some sort of physical host which has to be provided by the customer. Translating this further to aws this would mean an EC2 instance. And then when I would go for ECS in stead of EKS.\nPlease share your opinion.","upvote_count":"7"},{"poster":"Guru4Cloud","upvote_count":"3","timestamp":"1692784020.0","comment_id":"988158","content":"Selected Answer: B\nThis option provides the least operational overhead:\n\nAPI Gateway handles the API requests and integration with Lambda\nLambda automatically scales compute without managing servers\nProvisioned concurrency ensures consistent low latency by keeping functions initialized\nNo need to manage containers or orchestration platforms as with ECS/EKS"},{"comment_id":"957122","timestamp":"1689827760.0","upvote_count":"2","poster":"TariqKipkemei","content":"Selected Answer: B\nThe company requires the API to respond consistently with low latency to ensure customer satisfaction especially during high peak periods, there is no mention of cost efficient. Hence provisioned concurrency is the best option. \nProvisioned concurrency is the number of pre-initialized execution environments you want to allocate to your function. These execution environments are prepared to respond immediately to incoming function requests. Configuring provisioned concurrency incurs charges to your AWS account.\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html#:~:text=for%20a%20function.-,Provisioned%20concurrency,-%E2%80%93%20Provisioned%20concurrency%20is"},{"content":"Selected Answer: B\nAWS Lambda provides a highly scalable and distributed infrastructure that automatically manages the underlying compute resources. It automatically scales your API based on the incoming request load, allowing it to respond consistently with low latency, even during peak times. AWS Lambda takes care of infrastructure provisioning, scaling, and resource management, allowing you to focus on writing the code for your API logic.","comment_id":"909740","timestamp":"1685397000.0","upvote_count":"4","poster":"MirKhobaeb"}],"answer_images":[],"answer":"B","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/109719-exam-aws-certified-solutions-architect-associate-saa-c03/","question_images":[],"question_id":484,"isMC":true,"answers_community":["B (78%)","A (23%)"],"timestamp":"2023-05-19 14:32:00"},{"id":"xNzYJry5ppZfWSgKl4sI","question_id":485,"question_text":"A company wants to send all AWS Systems Manager Session Manager logs to an Amazon S3 bucket for archival purposes.\n\nWhich solution will meet this requirement with the MOST operational efficiency?","choices":{"B":"Install the Amazon CloudWatch agent. Push all logs to a CloudWatch log group. Export the logs to an S3 bucket from the group for archival purposes.","A":"Enable S3 logging in the Systems Manager console. Choose an S3 bucket to send the session data to.","C":"Create a Systems Manager document to upload all server logs to a central S3 bucket. Use Amazon EventBridge to run the Systems Manager document against all servers that are in the account daily.","D":"Install an Amazon CloudWatch agent. Push all logs to a CloudWatch log group. Create a CloudWatch logs subscription that pushes any incoming log events to an Amazon Kinesis Data Firehose delivery stream. Set Amazon S3 as the destination."},"topic":"1","answer":"A","url":"https://www.examtopics.com/discussions/amazon/view/109536-exam-aws-certified-solutions-architect-associate-saa-c03/","exam_id":31,"answers_community":["A (95%)","5%"],"discussion":[{"upvote_count":"8","comment_id":"1106134","timestamp":"1703604420.0","content":"Selected Answer: A\nsend logs to Amazon S3 from AWS Systems Manager Session Manager. Here are the steps to do so:\n\nEnable S3 Logging: Open the AWS Systems Manager console. In the navigation pane, choose Session Manager. Choose the Preferences tab, and then choose Edit. Select the check box next to Enable under S3 logging.\n\nCreate an S3 Bucket: To store the Session Manager logs, create an S3 bucket to hold the audit logs from the Session Manager interactive shell usage.\n\nConfigure IAM Role: AWS Systems Manager Agent (SSM Agent) uses the same AWS Identity and Access Management (IAM) role to activate itself and upload logs to Amazon S3. You can use either an IAM instance profile thatâ€™s attached to an Amazon Elastic Compute Cloud (Amazon EC2) instance or the IAM role thatâ€™s configured for the Default Host Management Configuration.","poster":"master9"},{"content":"Selected Answer: A\noption A does not involve CloudWatch, while option D does. Therefore, in terms of operational overhead, option A would generally have less complexity and operational overhead compared to option D.\n\nOption A simply enables S3 logging in the Systems Manager console, allowing you to directly send session logs to an S3 bucket. This approach is straightforward and requires minimal configuration.\n\nOn the other hand, option D involves installing and configuring the Amazon CloudWatch agent, creating a CloudWatch log group, setting up a CloudWatch Logs subscription, and configuring an Amazon Kinesis Data Firehose delivery stream to store logs in an S3 bucket. This requires additional setup and management compared to option A.\n\nSo, if minimizing operational overhead is a priority, option A would be a simpler and more straightforward choice.","upvote_count":"5","poster":"cloudenthusiast","comment_id":"901923","timestamp":"1684499760.0"},{"content":"A, You can choose to store session log data in a specified Amazon Simple Storage Service (Amazon S3) bucket for debugging and troubleshooting purposes.\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-logging.html#session-manager-logging-s3","poster":"pujithacg8","upvote_count":"2","timestamp":"1722794700.0","comment_id":"1260761"},{"comment_id":"1119019","content":"Selected Answer: A\nMost efficient is A because it is a direct option in SM logging. \nB can work but is more operational overhead as you end up using CloudWatch (not sure how but making assumption based on language of option)\nC is definitely too much work\nD Way too many moving parts","upvote_count":"3","timestamp":"1704919440.0","poster":"awsgeek75"},{"upvote_count":"2","poster":"potomac","timestamp":"1699232820.0","comment_id":"1063433","content":"Selected Answer: A\nYou can choose to store session log data in a specified Amazon Simple Storage Service (Amazon S3) bucket for debugging and troubleshooting purposes."},{"poster":"deechean","comment_id":"996146","content":"Selected Answer: A\nYou can config the log archived to S3 in the Session Manager - > preference tab. Another option is CloudWatch log. \nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-logging.html#session-manager-logging-s3","timestamp":"1693580100.0","upvote_count":"2"},{"upvote_count":"3","comment_id":"988151","content":"Selected Answer: A\nÂ°Simplicity - Enabling S3 logging requires just a simple configuration in the Systems Manager console to specify the destination S3 bucket. No other services need to be configured.\n Â°Direct integration - Systems Manager has native support to send session logs to S3 through this feature. No need for intermediary services.\n Â°Automated flow - Once S3 logging is enabled, the session logs automatically flow to the S3 bucket without manual intervention.\n Â°Easy management - The S3 bucket can be managed independently for log storage and archival purposes without impacting Systems Manager.\n Â°Cost-effectiveness - No charges for intermediate CloudWatch or Kinesis services. Just basic S3 storage costs.\n Â°Minimal overhead - No ongoing management of complex pipeline of services. Direct logs to S3 minimizes overhead.","timestamp":"1692783720.0","poster":"Guru4Cloud"},{"comment_id":"957127","poster":"TariqKipkemei","content":"Selected Answer: A\nWith the MOST operational efficiency then option A is best.\nOtherwise B is also an option with a little bit more ops than option A.\n\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-logging.html","upvote_count":"2","timestamp":"1689828480.0"},{"poster":"Zox42","content":"Selected Answer: A\nAnswer A. https://aws-labs.net/winlab5-manageinfra/sessmgrlog.html","upvote_count":"2","comment_id":"946785","timestamp":"1688859300.0"},{"content":"Selected Answer: A\nGPT argued for D.\n\nB could be an option, by installing a logging package on alle managed systems/ECs etc. https://docs.aws.amazon.com/systems-manager/latest/userguide/distributor-working-with-packages-deploy.html\n\nHowever, as it mentions the \"Session manager logs\" I would tend towards A.","poster":"Zuit","upvote_count":"2","timestamp":"1687960680.0","comment_id":"936731"},{"poster":"MrAWSAssociate","timestamp":"1687247340.0","upvote_count":"2","content":"Selected Answer: A\nIt should be \"A\".\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-logging.html","comment_id":"928253"},{"comment_id":"922810","timestamp":"1686724980.0","poster":"secdgs","content":"Selected Answer: A\nIt have menu to Enable S3 Logging.\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-logging.html#session-manager-logging-s3","upvote_count":"2"},{"timestamp":"1686125940.0","upvote_count":"1","content":"Selected Answer: B\nBBBBBBBBB","comments":[{"content":"\"Install the CloudWatch agent\" where?","comment_id":"1111125","poster":"pentium75","timestamp":"1704109860.0","upvote_count":"2"}],"poster":"Markie999","comment_id":"917002"},{"upvote_count":"1","poster":"Bill1000","comment_id":"916656","comments":[{"content":"To log session data using Amazon S3 (console)\n\nOpen the AWS Systems Manager console at https://console.aws.amazon.com/systems-manager/.\nIn the navigation pane, choose Session Manager.\nChoose the Preferences tab, and then choose Edit.\nSelect the check box next to Enable under S3 logging.","comment_id":"948750","upvote_count":"3","poster":"baba365","timestamp":"1689057840.0"},{"comment_id":"927728","upvote_count":"1","poster":"vrevkov","timestamp":"1687194360.0","content":"But where do you want to install the Amazon CloudWatch agent in case of B?"}],"content":"Selected Answer: B\nThe option 'A' says \"Enable S3 logging in the Systems Manager console.\" This means that you will enable the logs !! FOR !! S3 events and its is not what the question asks. My vote is for Option B, based on this article: https://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html","timestamp":"1686087000.0"},{"upvote_count":"1","poster":"omoakin","timestamp":"1685503920.0","comment_id":"910770","content":"DDDDDD"},{"content":"Option D is definetely not right,\nIts optiom B","timestamp":"1684672440.0","comment_id":"903210","poster":"Anmol_1010","upvote_count":"2"},{"comments":[{"comment_id":"905675","upvote_count":"5","content":"Question may not be very clear. A should be the answer. Below link is the documetation:\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-logging.html#session-manager-logging-s3","poster":"[Removed]","timestamp":"1684918980.0"}],"upvote_count":"1","poster":"omoakin","comment_id":"902858","timestamp":"1684637460.0","content":"Chat GPT says option A is incorrect cos it requires enabling S3 logging in the system manager console only logs information about the systems manager service not the session logs\nSays correct answer is B"},{"content":"Selected Answer: A\nA MOST operational efficiency?","comment_id":"900132","timestamp":"1684326900.0","poster":"nosense","upvote_count":"4"}],"answer_ET":"A","answer_images":[],"question_images":[],"isMC":true,"answer_description":"","timestamp":"2023-05-17 14:35:00","unix_timestamp":1684326900}],"exam":{"lastUpdated":"11 Apr 2025","numberOfQuestions":1019,"isMCOnly":true,"id":31,"provider":"Amazon","isBeta":false,"isImplemented":true,"name":"AWS Certified Solutions Architect - Associate SAA-C03"},"currentPage":97},"__N_SSP":true}