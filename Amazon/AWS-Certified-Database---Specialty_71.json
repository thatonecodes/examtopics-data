{"pageProps":{"questions":[{"id":"e3djWsOf6HFUjLMuQE81","timestamp":"2021-04-08 14:20:00","answer_images":[],"choices":{"B":"Use a DynamoDB global table replica in another Region. Enable point-in-time recovery for both tables.","C":"Use a DynamoDB Accelerator table in another Region. Enable point-in-time recovery for the table.","A":"Create a DynamoDB stream that is processed by an AWS Lambda function that copies the data to a DynamoDB table in another Region.","D":"Create an AWS Backup plan and assign the DynamoDB table as a resource."},"unix_timestamp":1617884400,"question_text":"A database specialist at a large multi-national financial company is in charge of designing the disaster recovery strategy for a highly available application that is in development. The application uses an Amazon DynamoDB table as its data store. The application requires a recovery time objective (RTO) of 1 minute and a recovery point objective (RPO) of 2 minutes.\nWhich operationally efficient disaster recovery strategy should the database specialist recommend for the DynamoDB table?","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/49602-exam-aws-certified-database-specialty-topic-1-question-91/","isMC":true,"answers_community":["B (93%)","7%"],"exam_id":22,"topic":"1","answer_description":"","answer":"B","question_id":351,"answer_ET":"B","discussion":[{"content":"B DynamoDB global tables","upvote_count":"16","poster":"shantest1","timestamp":"1632330360.0","comment_id":"331165"},{"comment_id":"378678","upvote_count":"6","content":"BBBBBBBBBBB\n\nglobal tables are multi master replication enabled solution.\nit meets the requirement.","poster":"Suresh108","timestamp":"1635784380.0"},{"comment_id":"973838","upvote_count":"2","poster":"IhorK","content":"Selected Answer: B\n- A single Amazon DynamoDB global table can only have one replica table per AWS Region.\n- You can enable point-in-time recovery on each replica of a global table.\nhttps://aws.amazon.com/dynamodb/global-tables/?nc1=h_ls","timestamp":"1691326200.0"},{"poster":"lollyj","comment_id":"751618","timestamp":"1671577620.0","content":"Selected Answer: B\nOnly logical answer. Backups take too long to restore. Global tables are synchronously updated","upvote_count":"1"},{"upvote_count":"2","comments":[{"comment_id":"693423","poster":"Jiang_aws1","timestamp":"1665614220.0","content":"B is correct. Global table is multi master & it is sync up with few sec","upvote_count":"2"}],"poster":"sachin","comment_id":"626205","content":"It is asking for DR soulution. My vote is for A . If you need less RPO and RTO setup Streams and Kinesis, Lambda and you can achive the given RPO RTO, \nRPO is 5 mins in usual automated backups . B could be true but in global tables also RPO is 5 mins","timestamp":"1656775440.0"},{"upvote_count":"2","poster":"Dantas","content":"Selected Answer: B\nTwo-minute recovery point objective (RPO).","comment_id":"609812","timestamp":"1654015320.0"},{"comment_id":"595135","upvote_count":"4","poster":"novice_expert","content":"Selected Answer: B\nx A. Create a DynamoDB stream that is processed by an AWS Lambda function that copies the data to a DynamoDB table in another Region. (good if B is wrong)\nB. Use a DynamoDB global table replica in another Region. Enable point-in-time recovery for both tables. (PITR is distraction btw PITR takes 5 min recovery, but global table will auto failover to good region, right?)\nx C. DynamoDB Accelerator (its for read caching)\nx D. Create an AWS Backup plan and assign the DynamoDB table as a resource. (takes 1 hour)","timestamp":"1651333500.0"},{"comment_id":"593603","timestamp":"1651127760.0","content":"Selected Answer: B\nB because of RTO and RPO limits defined. D is good solution doesn't fit limits. DynamoDB Accellerator is distractor for DR question.","poster":"marcoeu","upvote_count":"2"},{"comment_id":"529995","timestamp":"1642874160.0","comments":[{"content":"to get RPO of 2 minutes you will have to create a backup plan that backs up < 2 minutes!! Thats not what backup is for","comment_id":"561636","poster":"RotterDam","upvote_count":"5","timestamp":"1646512380.0"}],"content":"Selected Answer: D\nA, B and C are not DR solutions for Dynamo DB. Global Tables is multi region replication.\nD meets the objective . You can set up scheduled backups for Amazon DynamoDB using AWS Backup.. https://aws.amazon.com/blogs/database/set-up-scheduled-backups-for-amazon-dynamodb-using-aws-backup/","poster":"Sandy1971","upvote_count":"1"},{"poster":"Shunpin","upvote_count":"2","comment_id":"510774","timestamp":"1640661600.0","content":"Selected Answer: B\nI will go to B.\nhttps://s3.amazonaws.com/solutions-reference/multi-region-application-architecture/latest/multi-region-application-architecture.pdf"},{"timestamp":"1637065140.0","poster":"toppic26","upvote_count":"4","comment_id":"479356","content":"A is the answer. \nD is wrong. Restore takes nearly 1 hour https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/backuprestore_HowItWorks.html\nB is wrong. RPO is about 5 mins.\nA is true. If you need less RPO and RTO setup Streams and Kinesis, Lambda whatever"},{"poster":"Sp230","upvote_count":"1","comment_id":"478085","comments":[{"upvote_count":"1","poster":"toppic26","comment_id":"479351","timestamp":"1637064360.0","content":"But restore is \"less than one hour\" aws says https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/backuprestore_HowItWorks.html"}],"timestamp":"1636890300.0","content":"The question requires 2 min RPO. Using PITR RPO is about 5 min. I think D makes more sense here"},{"content":"BBBBB https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GlobalTables.html","timestamp":"1635994680.0","poster":"Scunningham99","upvote_count":"2","comment_id":"448969"},{"upvote_count":"4","comment_id":"358959","comments":[{"poster":"gelsm","comment_id":"359308","timestamp":"1634988960.0","upvote_count":"1","content":"HI Aesthet, How are you sure about this? :) Thanks!"}],"poster":"Aesthet","content":"B final answer","timestamp":"1633881960.0"}]},{"id":"nMPRyAqWEKKNicGZNPtZ","topic":"1","answer_ET":"D","timestamp":"2021-04-01 20:37:00","question_id":352,"discussion":[{"content":"Selected Answer: D\non-premises MySQL server -> mysqldump utility -> snapshot -> copy to S3 ->MySQL Utility on EC2 -> import to AWS RDS MySQL -> Establish replication into the new DB instance using MySQL replication -> Stop application access to the on-premises MySQL server -> let the remaining transactions replicate over. -> Point the application to the DB instance.","poster":"novice_expert","comment_id":"595237","upvote_count":"5","timestamp":"1651347960.0"},{"comment_id":"975950","content":"It says RDS so B, C (EC2 option) is out.","timestamp":"1691521560.0","poster":"aws2023a","upvote_count":"1"},{"comment_id":"920174","content":"I feel correct answer is B as DMS is involved","poster":"megramlak","upvote_count":"1","timestamp":"1686414300.0"},{"poster":"lollyj","comment_id":"751631","upvote_count":"2","content":"Selected Answer: D\nWas a toss between B&D. DMS task threw me off but I don't see how you can get snapshot to an EC2 instance from option B","timestamp":"1671578640.0"},{"upvote_count":"1","comment_id":"729472","content":"Selected Answer: B\nCorrect answer is B\nI don't get why so many people chose D. How is extra copy to S3 bucket reduce downtime?","poster":"vkruger","comments":[{"timestamp":"1671578580.0","comment_id":"751630","poster":"lollyj","content":"Installing new SQL software on an EC2 just for the sake of migration threw me off. Getting that much data through an s3 gateway endpoint would get the data from on-prem to the VPC but directly on to an EC2 instance seems ridic to me. So answer is D","upvote_count":"1"}],"timestamp":"1669656960.0"},{"poster":"damaldon","timestamp":"1634386020.0","comment_id":"419396","content":"D looks correct as per this liink:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MySQL.Procedural.Importing.NonRDSRepl.html","upvote_count":"3"},{"poster":"Suresh108","comment_id":"378681","timestamp":"1634192940.0","content":"DDDDDDDD","comments":[{"poster":"Suresh108","timestamp":"1634367840.0","upvote_count":"1","comment_id":"380865","content":"Ignore all EC2 based answers - method of elimination - left with D"}],"upvote_count":"1"},{"upvote_count":"2","poster":"Aesthet","comment_id":"358962","timestamp":"1633721100.0","content":"D\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MySQL.Procedural.Importing.External.Repl.html"},{"comment_id":"326632","upvote_count":"2","timestamp":"1633409100.0","content":"D: Ans","poster":"shantest1"},{"comment_id":"326074","timestamp":"1632420900.0","content":"Option D","poster":"std2021","upvote_count":"2"}],"answer_description":"","question_text":"A small startup company is looking to migrate a 4 TB on-premises MySQL database to AWS using an Amazon RDS for MySQL DB instance.\nWhich strategy would allow for a successful migration with the LEAST amount of downtime?","answer":"D","choices":{"A":"Deploy a new RDS for MySQL DB instance and configure it for access from the on-premises data center. Use the mysqldump utility to create an initial snapshot from the on-premises MySQL server, and copy it to an Amazon S3 bucket. Import the snapshot into the DB instance utilizing the MySQL utilities running on an Amazon EC2 instance. Immediately point the application to the DB instance.","B":"Deploy a new Amazon EC2 instance, install the MySQL software on the EC2 instance, and configure networking for access from the on-premises data center. Use the mysqldump utility to create a snapshot of the on-premises MySQL server. Copy the snapshot into the EC2 instance and restore it into the EC2 MySQL instance. Use AWS DMS to migrate data into a new RDS for MySQL DB instance. Point the application to the DB instance.","C":"Deploy a new Amazon EC2 instance, install the MySQL software on the EC2 instance, and configure networking for access from the on-premises data center. Use the mysqldump utility to create a snapshot of the on-premises MySQL server. Copy the snapshot into an Amazon S3 bucket and import the snapshot into a new RDS for MySQL DB instance using the MySQL utilities running on an EC2 instance. Point the application to the DB instance.","D":"Deploy a new RDS for MySQL DB instance and configure it for access from the on-premises data center. Use the mysqldump utility to create an initial snapshot from the on-premises MySQL server, and copy it to an Amazon S3 bucket. Import the snapshot into the DB instance using the MySQL utilities running on an Amazon EC2 instance. Establish replication into the new DB instance using MySQL replication. Stop application access to the on-premises MySQL server and let the remaining transactions replicate over. Point the application to the DB instance."},"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/48683-exam-aws-certified-database-specialty-topic-1-question-92/","exam_id":22,"unix_timestamp":1617302220,"answer_images":[],"answers_community":["D (88%)","13%"],"question_images":[]},{"id":"v3xW3o9WkMxyyqUgbARS","answer_images":[],"answer":"A","topic":"1","timestamp":"2021-04-01 20:40:00","url":"https://www.examtopics.com/discussions/amazon/view/48684-exam-aws-certified-database-specialty-topic-1-question-93/","answer_ET":"A","answers_community":["A (72%)","D (28%)"],"unix_timestamp":1617302400,"question_images":[],"question_text":"A software development company is using Amazon Aurora MySQL DB clusters for several use cases, including development and reporting. These use cases place unpredictable and varying demands on the Aurora DB clusters, and can cause momentary spikes in latency. System users run ad-hoc queries sporadically throughout the week. Cost is a primary concern for the company, and a solution that does not require significant rework is needed.\nWhich solution meets these requirements?","question_id":353,"answer_description":"","choices":{"B":"Upgrade one of the DB clusters to a larger size, and consolidate development and reporting activities on this larger DB cluster.","C":"Use existing DB clusters and stop/start the databases on a routine basis using scheduling tools.","A":"Create new Aurora Serverless DB clusters for development and reporting, then migrate to these new DB clusters.","D":"Change the DB clusters to the burstable instance family."},"discussion":[{"upvote_count":"15","comment_id":"356251","poster":"Zhongkai","content":"Check https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.DBInstanceClass.html - t2 and t3 (8vCPU 32GB RAM) series are mostly weaker than r4/r5 (96 vCPU 768GB RAM) series. Changing from r to t series is not a good idea. Furthermore, \"unpredictable\" indicates \"Aurora Serverless\". Hence I will go with A.","timestamp":"1633383180.0"},{"upvote_count":"7","poster":"novice_expert","comment_id":"594641","timestamp":"1651256940.0","content":"Selected Answer: A\n\"unexpected and variable demands, adhoc searched\" => \"Aurora Serverless\""},{"timestamp":"1695145500.0","comment_id":"1011586","upvote_count":"1","poster":"Germaneli","content":"Selected Answer: A\nSkipping B+c as distractors.\nTorn between A+D -\nD is not applicable as \"we recommend using the [burstable] T DB instance classes only for development, test, or other nonproduction servers\".\nSwitching to burstable classes is not an option here for PROD reporting.\nHence A.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.BestPractices.html#AuroraMySQL.BestPractices.T2Medium"},{"upvote_count":"4","timestamp":"1675036320.0","content":"Selected Answer: D\nQuestion says without significant rework. Option A would require migration. So option D is correct. \nhttps://aws.amazon.com/rds/instance-types/","poster":"anantarb","comment_id":"792179"},{"timestamp":"1671579060.0","comment_id":"751642","upvote_count":"1","content":"Selected Answer: D\nA - Serverless will require moving existing DB although you are saving on cost. \nD. Can respond to traffic spikes, requires very little change and the costs for the spikes won't be much because it isn't too frequent. The burst credits can be used.","poster":"lollyj"},{"comment_id":"566802","timestamp":"1647174000.0","content":"Selected Answer: A\nhttps://aws.amazon.com/rds/aurora/serverless/\n\nAmazon Aurora Serverless is a simple, cost-effective option for infrequent, intermittent, or unpredictable workloads.","upvote_count":"4","poster":"Dantas"},{"comment_id":"555572","poster":"tugboat","content":"Selected Answer: A\nad-hoc searches = Serverless\n\nhttps://aws.amazon.com/rds/aurora/faqs/?nc=sn&loc=6\nQ: Can I migrate an existing Aurora DB cluster to Aurora Serverless?\nYes, you can restore a snapshot taken from an existing Aurora provisioned cluster into an Aurora Serverless DB Cluster (and vice versa).","timestamp":"1645737540.0","upvote_count":"1"},{"content":"D \nA is good if the cluster experience no activities which is not the case","poster":"user0001","timestamp":"1645029720.0","upvote_count":"2","comment_id":"548774"},{"content":"Answer: A\nThe company's key concern is cost, and a solution that does not need extensive rework is required. Converting Aurora Cluster to Serverless does not require extensive rework.","upvote_count":"1","timestamp":"1641840780.0","comment_id":"521050","poster":"awsmonster"},{"upvote_count":"2","comment_id":"509673","timestamp":"1640532660.0","content":"\"resulting in brief latency spikes\" Answer is D","poster":"SMAZ"},{"poster":"jove","upvote_count":"2","timestamp":"1636844040.0","content":"If the key concern is the cost, the right option is Aurora Serverless.. Answer is A","comment_id":"477844"},{"upvote_count":"5","timestamp":"1636160940.0","content":"D less rework - A is too much work","comment_id":"448989","poster":"Scunningham99"},{"timestamp":"1634691540.0","comment_id":"439654","poster":"guru_ji","content":"Answer: D\n\n**does not require significant rework is needed**","upvote_count":"2"},{"upvote_count":"2","content":"D. no significant work.","poster":"CW0106","comment_id":"438186","timestamp":"1634411040.0"},{"content":"A. Aurora Serverless is most saving cost. You pay what you query","upvote_count":"1","comment_id":"430460","timestamp":"1634212860.0","poster":"ChauPhan"},{"timestamp":"1633652340.0","poster":"Hits_23","content":"Answer is A. Serverless aurora cluster can be cost effective in the varying workload and dev/test environment where services are not required 24 * 7","comment_id":"408561","upvote_count":"1"},{"content":"A final answer","comment_id":"358968","poster":"Aesthet","upvote_count":"3","timestamp":"1633463520.0"},{"poster":"[Removed]","upvote_count":"4","comment_id":"347904","content":"Will go for Option D because the solution that does not require significant rework is needed. Option A requires migration-related work.","timestamp":"1632430080.0"},{"timestamp":"1632332160.0","content":"Will go with A","comment_id":"345354","poster":"Thatxamguy","upvote_count":"3"},{"poster":"tapjungle","comment_id":"342946","upvote_count":"2","content":"Answer A","timestamp":"1632322560.0"},{"timestamp":"1632205320.0","poster":"std2021","content":"Option D","comment_id":"326076","upvote_count":"4"}],"exam_id":22,"isMC":true},{"id":"tRoRw0es37O3W06btesx","answer_images":[],"topic":"1","answer":"D","timestamp":"2021-04-02 15:02:00","url":"https://www.examtopics.com/discussions/amazon/view/48808-exam-aws-certified-database-specialty-topic-1-question-94/","answer_ET":"D","unix_timestamp":1617368520,"answers_community":["D (67%)","C (33%)"],"question_images":[],"question_text":"A database specialist is building a system that uses a static vendor dataset of postal codes and related territory information that is less than 1 GB in size. The dataset is loaded into the application's cache at start up. The company needs to store this data in a way that provides the lowest cost with a low application startup time.\nWhich approach will meet these requirements?","question_id":354,"answer_description":"","discussion":[{"content":"D, key words \" static vendor dataset\" & \"lowest cost\"","poster":"TonyGe","comment_id":"415852","timestamp":"1635421020.0","upvote_count":"7"},{"comment_id":"1121596","timestamp":"1705148160.0","poster":"MultiAZ","content":"Selected Answer: D\nD gives the lower cost. It also meets the requirements - S3 can do tons of IOPS when needed. Most importantly, the data is loaded on startup and then used by the app in-memory, not used constantly - so there is no need for DynamoDB.","upvote_count":"1"},{"poster":"IhorK","timestamp":"1691329740.0","upvote_count":"2","content":"Selected Answer: C\nDynamoDB in on-demand capacity mode price:\n - DynamoDB Standard table class, Read Request Units (RRU) - $0.25 per MILLION read request units.\n - Data storage - First 25 GB stored per month is free using the DynamoDB Standard table class.\n- On-demand backup - Cold Backup Storage* $0.03 per GB-month (The information is static, you can do without a backup).\nhttps://aws.amazon.com/dynamodb/pricing/on-demand/\nAmazon S3 pricing:\nS3 Standard - General purpose storage for any type of data, typically used for frequently accessed data. First 50 TB / Month - $0.023 per GB\nhttps://aws.amazon.com/s3/pricing/\nApplication does not start so often, I would choose answer C. DynamoDB will give this information much faster than reading from S3. \nThe price of $0.25 per MILLION read request is cheaper than S3.","comment_id":"973887"},{"upvote_count":"2","timestamp":"1687512780.0","poster":"[Removed]","content":"Selected Answer: C\nthe lowest cost & a low application startup time -> Dynamo DB with OnDemand capacity mode. \nOption D (S3) will cause longer startup time in my opinion","comment_id":"931417"},{"poster":"satishstechie","upvote_count":"1","comment_id":"757635","timestamp":"1672070880.0","content":"Selected Answer: D\nsimple option"},{"poster":"lollyj","content":"Selected Answer: D\nStatic small files. S3 is your best bet for costs","upvote_count":"2","timestamp":"1671579600.0","comment_id":"751654"},{"poster":"novice_expert","timestamp":"1651341900.0","upvote_count":"4","comment_id":"595190","content":"Selected Answer: D\nx A. RDS (costly for 1 GB data)\nx B. Amazon Aurora Serverless. (costly for 1 GB data)\nx C. Use Amazon DynamoDB in on-demand capacity mode.\n (postal codes and associated territorial data that is less than 1 GB in size => key value data, \nminimizing application launch time => dynamoDB, but caching is required at application and not database)\n\nD. Use Amazon S3 and load the data from flat files. (most cost-effective)"},{"timestamp":"1636275180.0","poster":"Scunningham99","comment_id":"448990","content":"D is the answer","upvote_count":"2"},{"upvote_count":"1","content":"Let us see in this way, 1 GB file on boot uploading in to application time < (1 GB file from s3 to Application + load time). So definitely it is not D, I will go with B","timestamp":"1635873120.0","comment_id":"433365","poster":"aws4myself"},{"timestamp":"1635201540.0","poster":"gelsm","upvote_count":"1","comment_id":"404413","content":"I agree with D based on the link below:\nhttps://www.sumologic.com/insight/s3-cost-optimization/\nFor example, for 1 GB file stored on S3 with 1 TB of storage provisioned, you are billed for 1 GB only. In a lot of other services such as Amazon EC2, Amazon Elastic Block Storage (Amazon EBS) and Amazon DynamoDB you pay for provisioned capacity. For example, in the case of Amazon EBS disk you pay for the size of 1 TB of disk even if you just save 1 GB file. This makes managing S3 cost easier than many other services including Amazon EBS and Amazon EC2. On S3 there is no risk of over-provisioning and no need to manage disk utilization."},{"comment_id":"378688","poster":"Suresh108","upvote_count":"2","timestamp":"1635189840.0","content":"DDDDDDDDDDD. lowest cost. \n\n--> this says Application cache and NOT DB cache.\n\napplicationג€™s cache at start up"},{"comment_id":"375442","upvote_count":"2","poster":"AM","timestamp":"1635123420.0","content":"OR D is also a possibility"},{"poster":"AM","timestamp":"1634929080.0","comment_id":"375441","upvote_count":"1","content":"I will go with B. Will cover lowesr cost."},{"timestamp":"1634758500.0","content":"C or D\nI choose D","comment_id":"358972","poster":"Aesthet","upvote_count":"2"},{"comment_id":"326639","timestamp":"1633198800.0","upvote_count":"1","comments":[{"comment_id":"331156","poster":"shantest1","timestamp":"1634378880.0","content":"May be D:\nApplication Cache may be enough. Load from S3 straight into Application Cache","upvote_count":"6"}],"content":"C: ?\nlowest start up time asked","poster":"shantest1"}],"choices":{"C":"Use Amazon DynamoDB in on-demand capacity mode.","A":"Use an Amazon RDS DB instance. Shut down the instance once the data has been read.","D":"Use Amazon S3 and load the data from flat files.","B":"Use Amazon Aurora Serverless. Allow the service to spin resources up and down, as needed."},"exam_id":22,"isMC":true},{"id":"1qIvlkO6DvmEAx9PXyRy","topic":"1","unix_timestamp":1617303420,"exam_id":22,"question_images":[],"discussion":[{"comment_id":"331910","upvote_count":"9","timestamp":"1633457580.0","poster":"shantest1","content":"A. Answer\nUse EMR to copy to S3 and use EMR to create new table."},{"comment_id":"378711","upvote_count":"5","timestamp":"1634284140.0","poster":"Suresh108","content":"AAAA for me as well. \n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/back-up-dynamodb-s3/\n\nEMR to backup and restore"},{"comment_id":"915459","upvote_count":"3","content":"Selected Answer: A\nA. Use Amazon EMR to export the data from the current DynamoDB table to Amazon S3. Then use Amazon EMR again to import the data from Amazon S3 into a new DynamoDB table with the new partition key. Most Voted\nxB. DynamoDB is not a valid source for AWS DMS\nxC. The partition key cannot be modified\nxD. Although you can choose to restore a DynamoDB table without the secondary indexes, the partition key cannot be modifier during restore","timestamp":"1685970180.0","poster":"aviathor"},{"timestamp":"1678001040.0","upvote_count":"1","content":"Selected Answer: C\nTo apply the new partition key to all existing and new data in a DynamoDB table, the AWS CLI can be used to update the table and modify the partition key","comment_id":"829699","poster":"ninjalight25"},{"comment_id":"752624","comments":[{"content":"Dynamodb can be a target , but not a source -- Hence A","timestamp":"1672282560.0","comment_id":"760541","upvote_count":"1","poster":"Kanwar_89"}],"content":"Selected Answer: B\nI believe DMS works with DynamoDB and is a simpler solution compared to EMR. Dynamodb can be a source for DMS.\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.DynamoDB.html","upvote_count":"2","timestamp":"1671645480.0","poster":"lollyj"},{"content":"Is it possible to change partition key in DynamoDB? No. Once the table is setup, you cannot modify its Key Schema. You can only provision a new table, move data there, and then remove the first table","upvote_count":"5","timestamp":"1657426500.0","comment_id":"629427","poster":"Chirantan"},{"comment_id":"594504","upvote_count":"3","poster":"novice_expert","timestamp":"1651240680.0","content":"Selected Answer: A\nhttps://aws.amazon.com/premiumsupport/knowledge-center/back-up-dynamodb-s3/\n\nDynamoDB to S3 is possible by EMR, AWS Glue, or AWS Data Pipeline."},{"comment_id":"554954","upvote_count":"3","poster":"tugboat","timestamp":"1645660140.0","content":"Selected Answer: A\nDynamoDB cannot be a source for DMS. \n\nPer -https://dynobase.com/dynamodb-keys/\nIs it possible to change partition key in DynamoDB?\nNo. Once the table is setup, you cannot modify its Key Schema. You can only provision a new table, move data there, and then remove the first table.\n\n\nPer - https://aws.amazon.com/premiumsupport/knowledge-center/back-up-dynamodb-s3/\nTo customize the process of creating backups, you can use use Amazon EMR, AWS Glue, or AWS Data Pipeline.\n\nSo, using EMR is the only valid option."},{"content":"D\nhttps://docs.aws.amazon.com/cli/latest/reference/dynamodb/restore-table-from-backup.html","timestamp":"1645654620.0","poster":"kped21","upvote_count":"1","comment_id":"554918","comments":[{"content":"this will recreate the same partition key","poster":"RotterDam","upvote_count":"1","timestamp":"1646574600.0","comment_id":"562051"}]},{"poster":"Aesthet","upvote_count":"3","content":"A, other options are not possible","timestamp":"1634141460.0","comment_id":"358980"},{"upvote_count":"2","poster":"std2021","comment_id":"326086","timestamp":"1632893100.0","comments":[{"comment_id":"476881","poster":"toppic26","content":"DynamoDB doesnt seem to be a source for DMS. https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.html","timestamp":"1636714800.0","upvote_count":"4"}],"content":"I'll go with B. \nrestore-table-from-backup doesn't seem to be able to change partition key, not sure here"}],"url":"https://www.examtopics.com/discussions/amazon/view/48686-exam-aws-certified-database-specialty-topic-1-question-95/","answer_ET":"A","question_text":"A database specialist needs to review and optimize an Amazon DynamoDB table that is experiencing performance issues. A thorough investigation by the database specialist reveals that the partition key is causing hot partitions, so a new partition key is created. The database specialist must effectively apply this new partition key to all existing and new data.\nHow can this solution be implemented?","answer_images":[],"answers_community":["A (75%)","B (17%)","8%"],"answer":"A","choices":{"D":"Use the AWS CLI to back up the DynamoDB table. Then use the restore-table-from-backup command and modify the partition key.","C":"Use the AWS CLI to update the DynamoDB table and modify the partition key.","A":"Use Amazon EMR to export the data from the current DynamoDB table to Amazon S3. Then use Amazon EMR again to import the data from Amazon S3 into a new DynamoDB table with the new partition key.","B":"Use AWS DMS to copy the data from the current DynamoDB table to Amazon S3. Then import the DynamoDB table to create a new DynamoDB table with the new partition key."},"question_id":355,"isMC":true,"timestamp":"2021-04-01 20:57:00","answer_description":""}],"exam":{"lastUpdated":"11 Apr 2025","id":22,"name":"AWS Certified Database - Specialty","isImplemented":true,"numberOfQuestions":359,"isBeta":false,"isMCOnly":false,"provider":"Amazon"},"currentPage":71},"__N_SSP":true}