{"pageProps":{"questions":[{"id":"QLADUPPuXvb7GlxGltOp","isMC":true,"exam_id":32,"answer_description":"","topic":"1","question_images":[],"question_text":"The following policy can be attached to an IAM group. It lets an IAM user in that group access a \"home directory\" in AWS S3 that matches their user name using the console.\n{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Action\": [\"s3:*\"],\n\"Effect\": \"Allow\",\n\"Resource\": [\"arn:aws:s3:::bucket-name\"],\n\"Condition\":{\"StringLike\":{\"s3:prefix\":[\"home/${aws:username}/*\"]}}\n},\n{\n\"Action\":[\"s3:*\"],\n\"Effect\":\"Allow\",\n\"Resource\": [\"arn:aws:s3:::bucket-name/home/${aws:username}/*\"]\n}\n]\n}","choices":{"A":"True","B":"False"},"unix_timestamp":1569253200,"answers_community":["B (100%)"],"question_id":121,"discussion":[{"upvote_count":"17","timestamp":"1632212940.0","poster":"Moon","comment_id":"12298","comments":[{"comment_id":"83889","upvote_count":"3","timestamp":"1633591440.0","poster":"fw","content":"The link indeed is very helpful. It shows how to configure policies so that user can use console to upload/download objects from S3 to his own directory.\nBasically two more blocks are needed (in addition to two blocks listed in this question):\nBlock 1: Allow required Amazon S3 console permissions\nBlock 2: Allow listing objects in root and home folders"},{"poster":"nitinz","upvote_count":"1","comments":[{"comment_id":"323075","timestamp":"1635794640.0","poster":"cldy","upvote_count":"4","content":"\"s:*\" covers everything, so the correct answer is A TRUE."}],"comment_id":"314396","content":"that ink is super helpful, many config are missing from it, hence B","timestamp":"1635068460.0"},{"timestamp":"1633194720.0","content":"I thought this link was super helpful. Here's a quote:\n\n\"The ListAllMyBuckets action grants David permission to list all the buckets in the AWS account, which is required for navigating to buckets in the Amazon S3 console (and as an aside, you currently can’t selectively filter out certain buckets, so users must have permission to list all buckets for console access). The console also does a GetBucketLocation call when users initially navigate to the Amazon S3 console, which is why David also requires permission for that action. Without these two actions, David will get an access denied error in the console.\"","comment_id":"43386","poster":"sarah1","upvote_count":"6","comments":[{"content":"Thank you Sarah. Really good and concise summary: \nIs missing: \nList on resource: all buckets\nGetBucketLocation on resource: all buckets","upvote_count":"1","comment_id":"671306","timestamp":"1663395840.0","poster":"FAB1975"}]}],"content":"Answer B:\nexplanation:\nhttps://aws.amazon.com/blogs/security/writing-iam-policies-grant-access-to-user-specific-folders-in-an-amazon-s3-bucket/"},{"poster":"tan9","timestamp":"1632701400.0","content":"User has no permission to list the ancestor directories, so that they won't be possible to navigate into their own home directory. And full working policy can be found at: https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_home-directory-console.html","comment_id":"27625","upvote_count":"9","comments":[{"poster":"Musk","content":"But if they type the right URL they get in. I don't see why it would not work. The question is not whether the user can navigate down to the right path.\nI think it's TRUE","timestamp":"1632805860.0","upvote_count":"6","comment_id":"33776"}]},{"content":"B. False","poster":"amministrazione","comment_id":"1266411","timestamp":"1723723080.0","upvote_count":"1"},{"timestamp":"1721095740.0","poster":"Narendragpt","comment_id":"1248626","upvote_count":"2","content":"A is True ."},{"content":"Selected Answer: B\nAnswer B:\nexplanation:\nhttps://aws.amazon.com/blogs/security/writing-iam-policies-grant-access-to-user-specific-folders-in-an-amazon-s3-bucket/","poster":"SkyZeroZx","timestamp":"1686336960.0","upvote_count":"1","comment_id":"919582"},{"timestamp":"1666865700.0","upvote_count":"1","comment_id":"705432","poster":"sjpd10","content":"The second action provides access to all S3 buckets unlike the first with a 'Condition'"},{"upvote_count":"1","poster":"roka_ua","comment_id":"580848","timestamp":"1649091240.0","content":"Selected Answer: B\nVote B"},{"timestamp":"1639819320.0","poster":"tkanmani76","upvote_count":"1","comment_id":"504129","content":"Answer A - In the same link being discussed here the last block is as below - which provides * for all S3 Actions and take to his home page. \n{\n \"Sid\": \"AllowAllS3ActionsInUserFolder\",\n \"Action\":[\"s3:*\"],\n \"Effect\":\"Allow\",\n \"Resource\": [\"arn:aws:s3:::my-company/home/${aws:username}/*\"]\n }","comments":[{"poster":"RVivek","timestamp":"1640754600.0","comment_id":"511806","comments":[{"upvote_count":"1","poster":"tkanmani76","content":"Agree, stand corrected - Option B","comment_id":"523249","timestamp":"1642123680.0"}],"content":"It still gives permission only to \"my-company/home/${aws:username}/*\" , Permission to list all buckets is necessary to navigate to this folder","upvote_count":"1"}]},{"comment_id":"330868","upvote_count":"3","timestamp":"1635869400.0","content":"B.\n\"Using the console\" is the key.\nIf only program access is needed, then it's enough.","poster":"01037"},{"upvote_count":"1","comment_id":"322610","poster":"cldy","timestamp":"1635079140.0","content":"A. \nShouldn't s:* cover all S3 actions?","comments":[{"timestamp":"1704519900.0","content":"The issue here is not with the action, but with which resource is applied. In our case, it's applied to the folder's content, not the folder itself.","upvote_count":"1","comment_id":"1114988","poster":"shammous"}]},{"poster":"srknbngl","comment_id":"190111","upvote_count":"1","timestamp":"1634371680.0","content":"Correct Answer: B"},{"upvote_count":"4","poster":"fullaws","comment_id":"143355","timestamp":"1634322300.0","content":"B is correct (s3:ListAllMyBuckets, s3:GetBucketLocation need to Resource *)"},{"upvote_count":"2","content":"go with B","poster":"noisonnoiton","comment_id":"123088","timestamp":"1634264160.0"},{"poster":"awssp12345","comment_id":"117592","upvote_count":"4","timestamp":"1633963680.0","content":"Should s3:* cover all the s3 permissions including list?"},{"comment_id":"49639","poster":"BillyC","timestamp":"1633567080.0","upvote_count":"2","content":"Yes B is Correct!"},{"timestamp":"1632957660.0","poster":"amog","upvote_count":"5","content":"Answer is B\n\"using the console.\". They can use this policy to access home directory by URL, but not the console","comment_id":"36360"},{"content":"Missing quite a few configs, like console access, root and home folder access, etc","timestamp":"1632657960.0","comment_id":"26944","upvote_count":"1","poster":"Danao"},{"timestamp":"1632604200.0","comment_id":"21136","upvote_count":"1","content":"False because either one should be present. Both satisfy the requirement","poster":"Tamili"}],"answer":"B","answer_images":[],"timestamp":"2019-09-23 17:40:00","url":"https://www.examtopics.com/discussions/amazon/view/5609-exam-aws-certified-solutions-architect-professional-topic-1/","answer_ET":"B"},{"id":"EYhP9GJo7mUz7en5X3lO","answer_ET":"A","answers_community":["A (100%)"],"choices":{"C":"Amazon ElastiCache supports Memcached and Hazelcast.","A":"Amazon ElastiCache supports Memcached and Redis.","B":"Amazon ElastiCache supports Redis and WinCache.","D":"Amazon ElastiCache supports Memcached only."},"answer_description":"The cache engines supported by Amazon ElastiCache are Memcached and Redis.\nReference:\nhttp://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/SelectEngine.html","unix_timestamp":1600098720,"url":"https://www.examtopics.com/discussions/amazon/view/31279-exam-aws-certified-solutions-architect-professional-topic-1/","topic":"1","exam_id":32,"isMC":true,"question_id":122,"question_text":"Which of the following cache engines does Amazon ElastiCache support?","question_images":[],"discussion":[{"upvote_count":"5","comments":[{"timestamp":"1636261500.0","content":"Why not hazelcast?","comment_id":"382259","poster":"bamjive06","upvote_count":"1"}],"comment_id":"376482","content":"A is correct.\nWhat a question(笑)","poster":"01037","timestamp":"1635424560.0"},{"comment_id":"1267126","upvote_count":"1","timestamp":"1723814160.0","poster":"amministrazione","content":"A. Amazon ElastiCache supports Memcached and Redis.","comments":[{"timestamp":"1723845660.0","comment_id":"1267347","content":"This exam sap-c01","upvote_count":"1","poster":"aragon_saa"}]},{"upvote_count":"2","timestamp":"1647096900.0","comment_id":"566216","poster":"foxrj21","content":"Selected Answer: A\nA is correct!"},{"timestamp":"1638684480.0","upvote_count":"1","poster":"cldy","comment_id":"494100","content":"A. Amazon ElastiCache supports Memcached and Redis."},{"upvote_count":"2","poster":"acloudguru","comment_id":"479173","content":"I think the website made a mistake , they mixed up the Devops question in this SAP questions.","timestamp":"1637037480.0"},{"timestamp":"1633562760.0","poster":"BillyC","comment_id":"179388","content":"A is correct","upvote_count":"2"}],"answer_images":[],"timestamp":"2020-09-14 17:52:00","answer":"A"},{"id":"FEGbENJ60dPPXnHlGMxQ","topic":"1","question_id":123,"answer_description":"To define multiple schedules for different activities in the same pipeline, in AWS Data Pipeline, you should define multiple schedule objects in your pipeline definition file and associate the desired schedule to the correct activity via its schedule field. As an example of this, it could allow you to define a pipeline in which log files are stored in Amazon S3 each hour to drive generation of an aggregate report once a day.\nReference:\nhttps://aws.amazon.com/datapipeline/faqs/","discussion":[{"comment_id":"1267127","upvote_count":"1","content":"C. Defining multiple schedule objects in your pipeline definition file and associating the desired schedule to the correct activity via its schedule field","timestamp":"1723814160.0","poster":"amministrazione"},{"content":"C. Defining multiple schedule objects in your pipeline definition file and associating the desired schedule to the correct activity via its schedule field.\n\nTo define multiple AWS Data Pipeline schedules for different activities within the same pipeline, you can define multiple schedule objects in your pipeline definition file and associate each schedule with the appropriate activity using its schedule field.\n\nEach activity in the pipeline can have its own schedule defined by specifying the schedule object name in the schedule field of the activity definition. By associating the desired schedule object with each activity, you can configure different schedules for different activities within the same pipeline.","timestamp":"1686672180.0","comment_id":"922363","upvote_count":"1","poster":"SkyZeroZx"},{"content":"C. Defining multiple schedule objects in your pipeline definition file and associating the desired schedule to the correct activity via its schedule field","timestamp":"1638867240.0","comment_id":"495792","upvote_count":"1","poster":"cldy"},{"timestamp":"1635773160.0","comment_id":"376483","upvote_count":"1","content":"C is correct!","poster":"01037"},{"timestamp":"1633603260.0","poster":"BillyC","comment_id":"179394","upvote_count":"1","content":"C is correct!"}],"question_images":[],"answer_ET":"C","answer":"C","timestamp":"2020-09-14 17:57:00","question_text":"You have been given the task to define multiple AWS Data Pipeline schedules for different activities in the same pipeline.\nWhich of the following would successfully accomplish this task?","answers_community":["C (100%)"],"unix_timestamp":1600099020,"answer_images":[],"choices":{"B":"Defining multiple pipeline definitions in your schedule objects file and associating the desired schedule to the correct activity via its schedule field","D":"Defining multiple schedule objects in the schedule field","C":"Defining multiple schedule objects in your pipeline definition file and associating the desired schedule to the correct activity via its schedule field","A":"Creating multiple pipeline definition files"},"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/31280-exam-aws-certified-solutions-architect-professional-topic-1/","exam_id":32},{"id":"L882M90w15aumV0iJNXr","unix_timestamp":1600099020,"isMC":true,"timestamp":"2020-09-14 17:57:00","question_text":"In a VPC, can you modify a set of DHCP options after you create them?","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/31281-exam-aws-certified-solutions-architect-professional-topic-1/","answer_description":"After you create a set of DHCP options, you can't modify them. If you want your VPC to use a different set of DHCP options, you must create a new set and associate them with your VPC. You can also set up your VPC to use no DHCP options at all.\nReference:\nhttp://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_DHCP_Options.html","question_images":[],"answers_community":[],"question_id":124,"answer_ET":"C","discussion":[{"content":"C. No, you can't modify a set of DHCP options after you create them.","upvote_count":"1","comment_id":"1267129","timestamp":"1723814220.0","poster":"amministrazione"},{"poster":"cldy","timestamp":"1638705000.0","content":"C. No, you can't modify a set of DHCP options after you create them.","comment_id":"494285","upvote_count":"2"},{"upvote_count":"2","content":"Yes it's C, but its sorta dumb because you can achieve the functionality of changing the DHCP options set by switching to a new one with new options, then delete the old one which I think is what you should remember. One of those \"learn the doc by heart question\"...because feature has been implemented this way instead of that one ... okay","timestamp":"1635628080.0","poster":"robertomartinez","comment_id":"415242"},{"content":"C\nNo you can NOT","poster":"01037","upvote_count":"1","timestamp":"1634879700.0","comment_id":"376484"},{"timestamp":"1634309400.0","comment_id":"316499","content":"C is correct Answer:\nWhen you create a VPC, we automatically create a set of DHCP options and associate them with the VPC. You can configure your own DHCP options set for your VPC.\n\nChanging DHCP options\nAfter you create a set of DHCP options, you can't modify them. If you want your VPC to use a different set of DHCP options, you must create a new set and associate them with your VPC. You can also set up your VPC to use no DHCP options at all.","upvote_count":"2","poster":"ExtHo"},{"comment_id":"179395","timestamp":"1633590420.0","upvote_count":"1","content":"C is correct!","poster":"BillyC"}],"choices":{"D":"Yes, you can modify a set of DHCP options within 24 hours after creation.","A":"Yes, you can modify a set of DHCP options within 48 hours after creation and there are no VPCs associated with them.","B":"Yes, you can modify a set of DHCP options any time after you create them.","C":"No, you can't modify a set of DHCP options after you create them."},"answer_images":[],"exam_id":32,"answer":"C"},{"id":"k6VcG0ze6YuX7fuy0TWE","isMC":true,"exam_id":32,"topic":"1","question_images":[],"answer_description":"If a IAM user is trying to perform some action on an object belonging to another AWS user's bucket, S3 will verify whether the owner of the IAM user has given sufficient permission to him. It also verifies the policy for the bucket as well as the policy defined by the object owner.\nReference:\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-auth-workflow-object-operation.html","question_text":"A bucket owner has allowed another account's IAM users to upload or access objects in his bucket. The IAM user of Account A is trying to access an object created by the IAM user of account B. What will happen in this scenario?","choices":{"B":"AWS S3 will verify proper rights given by the owner of Account A, the bucket owner as well as by the IAM user B to the object","C":"The bucket policy may not be created as S3 will give error due to conflict of Access Rights","D":"It is not possible that the IAM user of one account accesses objects of the other IAM user","A":"It is not possible to give permission to multiple IAM users"},"unix_timestamp":1600099200,"answers_community":[],"question_id":125,"discussion":[{"comment_id":"1267130","timestamp":"1723814280.0","poster":"amministrazione","upvote_count":"1","content":"B. AWS S3 will verify proper rights given by the owner of Account A, the bucket owner as well as by the IAM user B to the object"},{"timestamp":"1639060140.0","comment_id":"497794","content":"B. AWS S3 will verify proper rights given by the owner of Account A, the bucket owner as well as by the IAM user B to the object","poster":"cldy","upvote_count":"1"},{"poster":"01037","timestamp":"1635920100.0","content":"B\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-auth-workflow-bucket-operation.html\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-auth-workflow-object-operation.html","upvote_count":"1","comment_id":"376508"},{"timestamp":"1635395100.0","comment_id":"179396","poster":"BillyC","content":"is correct","upvote_count":"1"}],"answer":"B","answer_images":[],"timestamp":"2020-09-14 18:00:00","url":"https://www.examtopics.com/discussions/amazon/view/31282-exam-aws-certified-solutions-architect-professional-topic-1/","answer_ET":"B"}],"exam":{"isImplemented":true,"name":"AWS Certified Solutions Architect - Professional","lastUpdated":"11 Apr 2025","numberOfQuestions":1019,"isMCOnly":false,"provider":"Amazon","isBeta":false,"id":32},"currentPage":25},"__N_SSP":true}