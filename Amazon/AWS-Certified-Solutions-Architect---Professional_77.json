{"pageProps":{"questions":[{"id":"a0BnKBk3rFVI22AxvP4f","timestamp":"2019-09-29 13:04:00","question_text":"A Solutions Architect must create a cost-effective backup solution for a company's 500MB source code repository of proprietary and sensitive applications. The repository runs on Linux and backs up daily to tape. Tape backups are stored for 1 year.\nThe current solution is not meeting the company's needs because it is a manual process that is prone to error, expensive to maintain, and does not meet the need for a Recovery Point Objective (RPO) of 1 hour or Recovery Time Objective (RTO) of 2 hours. The new disaster recovery requirement is for backups to be stored offsite and to be able to restore a single file if needed.\nWhich solution meets the customer's needs for RTO, RPO, and disaster recovery with the LEAST effort and expense?","answer":"B","answer_description":"","isMC":true,"unix_timestamp":1569755040,"answers_community":["B (83%)","A (17%)"],"exam_id":32,"topic":"1","answer_images":[],"answer_ET":"B","discussion":[{"upvote_count":"70","comment_id":"14443","content":"Tough question. I do support answer \"B\".\nEventhough there is no cross region replication, but that is not a requirement in the question. The requirement is just an offsite (AWS) disaster recovery. Therefore, a single copy in AWS would make the deal.\nAlso, there is a tricky requirement of restoring a SINGLE FILE! \nThe snapshot of Storage Gateway (cached or stored, or tape) are storing the backup as a whole, and not as files! that mean to restore, you need to build a snapshot, and mount the snapshot into an EC2, then restore the files. Therefore, it needs effort to restore, un-like the (File storage), which store the files as they are in S3 bucket! Where pulling a file is very straight forward.\nhttps://d1.awsstatic.com/whitepapers/aws-storage-gateway-file-gateway-for-hybrid-architectures.pdf\nA: nightly backup, does not meet the requirement. Plus restoration effort.\nC & D: both are working solutions, and valid too. however restoration effort, and cost is higher than B.\nAgain, it is a tough decision.","timestamp":"1632391080.0","comments":[{"content":"very good explaination. I think the restore file requirement eliminates C and D","comment_id":"21349","upvote_count":"4","timestamp":"1632480780.0","poster":"Frank1"},{"upvote_count":"3","timestamp":"1632714480.0","content":"Moon is the man.","comment_id":"86788","poster":"easytoo"},{"content":"On answer \"B\" I was concerned about the RTO because of the Glacier Service, but if they use expedited retrievals the information will be available between 1 to 5 minutes. So, for me the correct answer is \"B\".","timestamp":"1633640340.0","upvote_count":"3","poster":"7thGuest","comment_id":"150512"}],"poster":"Moon"},{"content":"C\nIn the stored mode, your primary data is stored locally and your entire dataset is available for low-latency access while asynchronously backed up to AWS. Because you already have a full copy on premise, you do not need too many versions on S3 and it can go straight to glacier. You will need to mount the snapshot for single file recovery though. https://aws.amazon.com/storagegateway/faqs/?nc=sn&loc=6\nA: Run backup nightly: does not meet RPO of 1 hour.\nB: Does not address the RPO of 1 hour. Versioning needs to be set to hourly. Not the least expense and no cross-region replication.\nD: Because this is a cache copy, during restore, you will need to download the whole volume from S3 which may exceed the 2 hour RTO.","comments":[{"timestamp":"1632205260.0","content":"Thank you so much","poster":"awspro","upvote_count":"1","comment_id":"13398"},{"comment_id":"636864","content":"I went to B because C didn't mention about hourly backup. It's true that version can handle... Thanks","upvote_count":"1","poster":"hilft","timestamp":"1658770620.0"},{"timestamp":"1632247620.0","comment_id":"14047","content":"Answer changed to B.\nhttps://aws.amazon.com/storagegateway/faqs/?nc=sn&loc=6\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html\nA: Run backup nightly: does not meet RPO of 1 hour.\nB: Although it does not have hourly snapshot, it has versioning configured. This is better for file based recovery. The question only needs the backup to be stored offsite so this actually does satisfy the requirement.\nC: Because this uses cross region replication, it has 2 copies and double the cost.\nD: Because this is a cache copy, during restore, you will need to download the whole volume from S3 which may exceed the 2 hour RTO.","upvote_count":"29","poster":"donathon"}],"poster":"donathon","timestamp":"1632098580.0","comment_id":"13220","upvote_count":"12"},{"poster":"mimadour21698","upvote_count":"1","content":"Selected Answer: B\nGo on B","timestamp":"1683811740.0","comment_id":"895105"},{"upvote_count":"1","comment_id":"709091","content":"Selected Answer: B\n\"able to restore a single file if needed\" = must use file gateway --- Configure the local source code repository to synchronize files to an AWS Storage Gateway file Amazon gateway to store backup copies in an Amazon S3 Standard bucket.","poster":"Jonfernz","timestamp":"1667304420.0"},{"poster":"JohnPi","content":"Selected Answer: B\nB\nrequirement: \"to be able to restore a single file if needed.\"","timestamp":"1664961240.0","comment_id":"686736","upvote_count":"1"},{"poster":"aandc","content":"Selected Answer: B\nturn to B after reading all comments","upvote_count":"1","timestamp":"1656828300.0","comment_id":"626441"},{"upvote_count":"2","comments":[{"upvote_count":"1","poster":"superuser784","comment_id":"708125","timestamp":"1667180580.0","content":"you are not storing the whole backup, in this case the solution the solution will store each file"}],"poster":"cannottellname","comment_id":"550818","content":"How can we use Amazon Storage File Gateway for Tape Backup? I believe options are incorrect here.","timestamp":"1645264440.0"},{"comment_id":"532714","timestamp":"1643187000.0","content":"Selected Answer: B\nI selected B because restore of individual files will be easiest with this solution","upvote_count":"1","poster":"shotty1"},{"content":"Definitively B : because A : because VT are presented to app bck as iscsi when tape is ejected from app bck, gateway mark data as ro and archive them to glacier on s3, it's not \nthe LEAST amount of work and expense","upvote_count":"1","timestamp":"1642458600.0","comment_id":"526161","poster":"weurseuk"},{"timestamp":"1641988980.0","content":"Selected Answer: A\nwith the LEAST amount of work and expense? Since they already have a solution that backs up to tape A) makes more sense. I know that B is a better option but it requires work.","upvote_count":"1","comments":[{"poster":"aandc","content":"A -> nightly backup, RPO cannot be meet","timestamp":"1656828360.0","upvote_count":"1","comment_id":"626442"}],"poster":"pititcu667","comment_id":"522122"},{"comment_id":"493436","timestamp":"1638577680.0","poster":"AzureDP900","content":"B seems reasonable in this case","upvote_count":"1"},{"comment_id":"450311","timestamp":"1636299000.0","content":"It's B - versioning","poster":"andylogan","upvote_count":"1"},{"content":"BBB\n---","poster":"tgv","comment_id":"437806","upvote_count":"1","timestamp":"1636290240.0"},{"comment_id":"411390","upvote_count":"1","timestamp":"1636152300.0","content":"my pick is B","poster":"aishvary123"},{"timestamp":"1636047180.0","content":"I will go with answer B, file is always available in cache of File Storage Gateway which is synced up with S3 bucket.","poster":"Kukkuji","comment_id":"369471","upvote_count":"1"},{"timestamp":"1635983400.0","comment_id":"351880","content":"I'll go with B","poster":"WhyIronMan","upvote_count":"2"},{"content":"definitely B","poster":"Waiweng","upvote_count":"1","comment_id":"343724","timestamp":"1635695220.0"},{"timestamp":"1635556260.0","content":"B is correct. Note: \"restore a single file\"","poster":"wind","upvote_count":"1","comment_id":"291016"},{"comment_id":"289483","timestamp":"1635490260.0","poster":"Kian1","upvote_count":"2","content":"Going with B"},{"poster":"Ebi","content":"Answer is B, all other solutions needs to restore entire volume not file","timestamp":"1635404280.0","upvote_count":"3","comment_id":"283938"},{"timestamp":"1634847840.0","content":"The key item is to retrieve single files, only file gateway provides this","comment_id":"270185","upvote_count":"2","poster":"Sourabh1703"},{"content":"I'll go with B","poster":"sanjaym","comment_id":"263420","timestamp":"1634652720.0","upvote_count":"1"},{"content":"Correct answer is B. Key is not cache, is not tape, is not S3 Glacier","poster":"T14102020","upvote_count":"1","comment_id":"241981","timestamp":"1634378280.0"},{"poster":"newme","upvote_count":"1","comment_id":"240972","timestamp":"1634348460.0","content":"I'll go with B, though I'm not sure about RTO, I can't find any references about transfer speed of Storage Gateway and S3.\n\nA: not meet the requirement of one-hour RPO\nC: also not sure about RTO. There is cross-region replication, so it costs more than B.\nD: basically same as C, but more work is needed and there is no backup deletion, maybe because the answer isn't complete."},{"poster":"jackdryan","upvote_count":"3","content":"I'll go with B","comment_id":"228787","timestamp":"1634309880.0"},{"poster":"Bulti","timestamp":"1634301600.0","comment_id":"227853","content":"It's going to be lot easier to maintain the current source code repository rather than replacing it. Also restoring a single file will be lot faster when files are stored in an S3 bucket as objects rather than when stored as EBS snapshots. So the answer is B.","upvote_count":"2"},{"poster":"vjt","comment_id":"215758","upvote_count":"1","content":"B should be the answer.","timestamp":"1634260200.0"},{"upvote_count":"2","poster":"Ebi","timestamp":"1634229180.0","content":"My answer is B","comment_id":"211807","comments":[{"comment_id":"344403","upvote_count":"1","poster":"Learnerinprogess","timestamp":"1635961020.0","content":"Any way to get access to your email id for further checking on how can the exam prep be done?"}]},{"comment_id":"179491","timestamp":"1634226420.0","poster":"Diadem27","content":"C for sure.","upvote_count":"3"},{"upvote_count":"1","comment_id":"176738","poster":"Utpal","timestamp":"1634173020.0","content":"\"backups to be stored offsite \" eliminates the option C"},{"timestamp":"1633731840.0","comment_id":"174817","content":"B is correct\n\nrequirement, \"to be able to restore a single file if needed.\" -- only file gateway provides this and other volume/Tap based, you need to restore the entire snapshot","upvote_count":"3","poster":"Ganfeng"},{"upvote_count":"1","poster":"garud","comment_id":"150503","content":"I would go for C. Note that B says to configure the current repo (which means it proposes to use the current repo as is). As per the question, the company is spending a lot to maintain this which means replacing this repo with AWS native one, is a better option. C satisfies this requirement.","timestamp":"1633567920.0"},{"timestamp":"1633482540.0","comments":[{"poster":"Phat","content":"File gateway means B. I think just a typo from you.","timestamp":"1633684200.0","upvote_count":"1","comment_id":"172518"}],"content":"C is correct, file gateway is better in this case, restore single file instead of whole snapshot (RTO), versioning instead of hourly snapshot (RPO).","comment_id":"148888","upvote_count":"4","poster":"fullaws"},{"timestamp":"1633452300.0","poster":"NikkyDicky","comment_id":"134163","upvote_count":"1","content":"B more likely"},{"comment_id":"133663","content":"go with B\nrestore a single file","poster":"noisonnoiton","upvote_count":"2","timestamp":"1633372980.0"},{"content":"answer: B","comment_id":"133236","poster":"mat2020","upvote_count":"1","timestamp":"1633163220.0"},{"comment_id":"125995","upvote_count":"1","content":"B:\nC is not possible as with archive snapshots to Amazon Glacier RPO of 1 HR will not be possible.","timestamp":"1633131960.0","poster":"manoj101"},{"content":"The requirement here is for a backup solution and not storage solution\nI see 2 problems with (B)\n- It is not a backup solution but actually a storage solution (in S3)\n- In case of regional failure there is no DR plan\n\nComing to (C):\nIt is a backup solution and also covers the scenario of regional failure\n\nWhat do you guys think?","comment_id":"113598","poster":"rsn","upvote_count":"1","timestamp":"1633129320.0"},{"timestamp":"1632823500.0","comment_id":"105464","poster":"meenu2225","content":"Correct answer B. It makes more sense out of the other lot.","upvote_count":"2"},{"poster":"LunchTime","timestamp":"1632503220.0","upvote_count":"3","content":"C is not correct as setting the snapshot frequency to 1 hour does not mean that the snapshot will have completed in the 1 hour (i.e., it will take some time to copy the 500 MB file or its delta) which means the 1 hour RPO could be missed. Also, if anything goes wrong with creating the snapshot the 1 hour RPO could be missed as well. Consequently, B is the best answer.","comment_id":"34663"},{"upvote_count":"1","comment_id":"13139","timestamp":"1632079800.0","content":"why b?","poster":"awspro"}],"question_images":[],"choices":{"D":"Replace the local source code repository storage with a Storage Gateway cached volume. Create a snapshot schedule to take hourly snapshots. Use an Amazon CloudWatch Events schedule expression rule to run an hourly AWS Lambda task to copy snapshots from US-EAST -1 to US-WEST-2.","A":"Replace local tapes with an AWS Storage Gateway virtual tape library to integrate with current backup software. Run backups nightly and store the virtual tapes on Amazon S3 standard storage in US-EAST-1. Use cross-region replication to create a second copy in US-WEST-2. Use Amazon S3 lifecycle policies to perform automatic migration to Amazon Glacier and deletion of expired backups after 1 year.","B":"Configure the local source code repository to synchronize files to an AWS Storage Gateway file Amazon gateway to store backup copies in an Amazon S3 Standard bucket. Enable versioning on the Amazon S3 bucket. Create Amazon S3 lifecycle policies to automatically migrate old versions of objects to Amazon S3 Standard - Infrequent Access, then Amazon Glacier, then delete backups after 1 year.","C":"Replace the local source code repository storage with a Storage Gateway stored volume. Change the default snapshot frequency to 1 hour. Use Amazon S3 lifecycle policies to archive snapshots to Amazon Glacier and remove old snapshots after 1 year. Use cross-region replication to create a copy of the snapshots in US-WEST-2."},"url":"https://www.examtopics.com/discussions/amazon/view/5834-exam-aws-certified-solutions-architect-professional-topic-1/","question_id":381},{"id":"ra8dImrFvivQbowCHjSN","discussion":[{"comment_id":"10721","poster":"awsec2","upvote_count":"32","comments":[{"timestamp":"1632116220.0","comments":[{"timestamp":"1632821700.0","comment_id":"69913","poster":"Smart","comments":[{"comment_id":"69917","poster":"Smart","content":"nvm. interface has changed. now happens through CW Events.","timestamp":"1632980940.0","upvote_count":"1"}],"upvote_count":"3","content":"lambda can run on schedule basis."},{"comment_id":"13404","content":"in B, 'Configure cron expression Amazon CloudWatch Events rules to trigger\nthe Lambda functions.'","timestamp":"1632222240.0","upvote_count":"8","poster":"pudak"}],"content":"lambda cannot trigger itself. step function is the best choice here","poster":"dpvnme","upvote_count":"3","comment_id":"11453"}],"timestamp":"1632092820.0","content":"b.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/schedule-elastic-beanstalk-stop-restart/"},{"poster":"donathon","comments":[{"upvote_count":"1","poster":"text","comment_id":"24844","content":"hi donathon, would you explain why you prefer B better than C","timestamp":"1632250980.0","comments":[{"poster":"student22","comment_id":"461723","timestamp":"1636098720.0","content":"As newme has mentioned below, \"Invoke Step Functions daily\" sounds like manually.","upvote_count":"2"}]},{"upvote_count":"1","comment_id":"29492","content":"I will choose C as the solution should be \"a one-stop scheduler solution for all teams\".\nAnd Step Functions is eligible since it is:\n1. it translates your workflow into a state machine diagram that is easy to understand, easy to explain to others, and easy to change.\n2. With Step Functions, you can craft long-running workflows such as machine learning model training, report generation, and *IT automation*.","poster":"tan9","timestamp":"1632396180.0"}],"comment_id":"14048","timestamp":"1632224700.0","upvote_count":"15","content":"B\nhttps://aws.amazon.com/premiumsupport/knowledge-center/schedule-elastic-beanstalk-stop-restart/"},{"content":"We have to keep the operational costs low. both options have their advantages and disadvantages. Option B requires more development effort but has minimal operational overhead and cost. Option D is easier to set up but may result in additional costs. Option B involves developing AWS Lambda functions that will start and stop the Elastic Beanstalk environment. The Lambda functions will be triggered by cron expression Amazon CloudWatch Events rules. This option has minimal operational overhead and cost. However, it requires the development of custom code, which may be time-consuming and may require additional resources.\nOption D involves configuring a time-based Auto Scaling group that will scale up an Amazon EC2 instance in the morning and terminate it at the end of the day. This option requires minimal custom code and is easy to set up. However, it may result in additional costs due to the use of EC2 instances and may not be as cost-effective as Option B.","timestamp":"1686990300.0","poster":"TECHNOWARRIOR","comment_id":"925864","upvote_count":"1"},{"content":"Selected Answer: B\nIt's B - cron expression Amazon CloudWatch Events rules","upvote_count":"1","comment_id":"925706","poster":"SkyZeroZx","timestamp":"1686968820.0"},{"comment_id":"584453","upvote_count":"1","content":"Selected Answer: B\nAgree with B","timestamp":"1649722200.0","poster":"lisiuyiu"},{"upvote_count":"1","timestamp":"1640129460.0","content":"B is right","poster":"AzureDP900","comment_id":"506505"},{"upvote_count":"1","poster":"AzureDP900","content":"B is right","timestamp":"1638577740.0","comment_id":"493437"},{"timestamp":"1636137060.0","content":"going for B","poster":"Kopa","upvote_count":"1","comment_id":"467433"},{"timestamp":"1636072920.0","comment_id":"450313","upvote_count":"1","content":"It's B - cron expression Amazon CloudWatch Events rules","poster":"andylogan"},{"poster":"DerekKey","comment_id":"409109","timestamp":"1636022160.0","upvote_count":"1","content":"cron expression Amazon CloudWatch Events rules - you have to create 2 - one for increase the other for decrease e.g.\n\"cron(0 6 * * MON-FRI *)\"\n\"cron(0 22 * * MON-FRI *)\""},{"poster":"WhyIronMan","comment_id":"351883","upvote_count":"2","timestamp":"1636020960.0","content":"I'll go with B"},{"poster":"Waiweng","content":"it's B","timestamp":"1635959520.0","upvote_count":"2","comment_id":"343728"},{"comment_id":"291028","timestamp":"1635950340.0","content":"B is correct.","poster":"wind","upvote_count":"1"},{"upvote_count":"1","content":"Answer B\nRef: https://aws.amazon.com/premiumsupport/knowledge-center/schedule-elastic-beanstalk-stop-restart/","timestamp":"1635867300.0","comment_id":"290775","poster":"Mansur"},{"upvote_count":"2","timestamp":"1635846540.0","poster":"Kian1","content":"going with B","comment_id":"289484"},{"content":"Answer is B\nRe \"It should also be able to handle the increased use of Elastic Beanstalk environments among different teams\", Lambda can go through all EB environments and stop them all","poster":"Ebi","upvote_count":"5","timestamp":"1635662820.0","comment_id":"283943"},{"poster":"sanjaym","timestamp":"1635405780.0","upvote_count":"2","comment_id":"263427","content":"I'll go with B"},{"poster":"newme","comment_id":"242427","timestamp":"1635387540.0","upvote_count":"2","content":"B.\nA: additional EC2 cost\nC: \"Invoke Step Functions daily\" sounds like manually.\nD: I think it's the best solution for what the CFO wants to do. But it doesn't meet the requirement of spinning up and terminating environments."},{"content":"Correct answer is D. Key is Auto Scaling group.","poster":"T14102020","comments":[{"comment_id":"716896","poster":"DarthYoda","content":"No. Why would you do this\" put the Elastic Beanstalk environment start command in the EC2 instance user data\"","timestamp":"1668282120.0","upvote_count":"1"}],"upvote_count":"1","timestamp":"1635371940.0","comment_id":"241986"},{"timestamp":"1635347700.0","content":"i'll go with B\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html\nhighly available solution with a precision of 1 minute can be achieved, one stop shop for all teams can be achieved and lambda can start and stop given the correct role is in place most complete answer is B","upvote_count":"1","poster":"YouYouYou","comment_id":"229168"},{"poster":"jackdryan","content":"I'll go with B","timestamp":"1635302100.0","comment_id":"228793","upvote_count":"3"},{"timestamp":"1635096420.0","comment_id":"227953","poster":"Bulti","content":"B is the correct answer","upvote_count":"3"},{"upvote_count":"2","poster":"Dhananjaya","content":"B\nhttps://aws.amazon.com/premiumsupport/knowledge-center/schedule-elastic-beanstalk-stop-restart/","comment_id":"226876","timestamp":"1635002100.0"},{"content":"B for sure","poster":"vjt","timestamp":"1634860920.0","comment_id":"215759","upvote_count":"1"},{"comment_id":"211809","upvote_count":"2","timestamp":"1634778180.0","content":"Definitely B","poster":"Ebi"},{"upvote_count":"2","content":"Answer is B.\nD is totally off topic: You wouldn’t want to control EB through the User Data of an EC2 instance that’s auto-scaling. And even if it could technically be possible, it wouldn’t work at the end of the day to stop EB as User Data are only used at instance start. On top of that, Auto-Scaling would only apply to that \"scheduler\" instance and not to the EB instance.","poster":"wsw","timestamp":"1634707980.0","comment_id":"183545"},{"upvote_count":"1","comments":[{"comment_id":"181453","timestamp":"1634610900.0","poster":"sam422","content":"D increases cost as it require EC2 instance","upvote_count":"1"}],"comment_id":"177655","poster":"Gallux","timestamp":"1634589540.0","content":"D\nOnly D increase use and only D terminate intances"},{"poster":"Neive","comment_id":"171743","content":"B is correct","upvote_count":"1","timestamp":"1634483340.0"},{"upvote_count":"1","comment_id":"148891","poster":"fullaws","content":"B is correct","timestamp":"1634453280.0"},{"poster":"NikkyDicky","upvote_count":"1","content":"B for sure","timestamp":"1634215320.0","comment_id":"134171"},{"comment_id":"133668","content":"go with B\nhttps://aws.amazon.com/premiumsupport/knowledge-center/schedule-elastic-beanstalk-stop-restart/","upvote_count":"2","timestamp":"1634028780.0","poster":"noisonnoiton"},{"poster":"mat2020","timestamp":"1633995840.0","content":"answer :b","comment_id":"133235","upvote_count":"1"},{"poster":"hello_aws","comments":[{"content":"D needs an EC2, which increases cost","upvote_count":"1","comment_id":"181455","poster":"sam422","timestamp":"1634662380.0"}],"comment_id":"127297","content":"D. as it says: \"It should also be able to handle the increased use of Elastic Beanstalk environments among different teams\"","timestamp":"1633983420.0","upvote_count":"2"},{"comment_id":"115745","upvote_count":"2","timestamp":"1633975620.0","poster":"ramikhreim","content":"https://aws.amazon.com/premiumsupport/knowledge-center/schedule-elastic-beanstalk-stop-restart/ Explanation why b is the answer"},{"comment_id":"105466","poster":"meenu2225","timestamp":"1633848840.0","upvote_count":"2","content":"B is the right answer, I have implemented similar lambdas for RDS, EC2 using cloudwatch alerts."},{"timestamp":"1633741320.0","poster":"NiruzCorp","content":"Anser is B. See https://aws.amazon.com/premiumsupport/knowledge-center/schedule-elastic-beanstalk-stop-restart/","comment_id":"97237","upvote_count":"2"},{"content":"It is D","poster":"Ibranthovic","upvote_count":"3","comment_id":"93197","timestamp":"1633604460.0"},{"timestamp":"1633595340.0","comments":[{"poster":"JAWS1600","comments":[{"comment_id":"227935","poster":"binhdx","timestamp":"1635030420.0","content":"I don't think D is correct, cause min instance on timebased scaling is 1. ( not 0)","upvote_count":"2"}],"timestamp":"1633651380.0","comment_id":"93996","content":"The following statement makes D invalid -\n\"and put the Elastic Beanstalk environment start command in the EC2 instance user date\"\nThere is no \"user date\" \nWe dont need to put start command in Ec2\nOtherwise I agree D would have been the cheapest option , as Auto Scaling does not cost anything. In contrast to option B, there will be cost for running Lambda.","upvote_count":"1"}],"comment_id":"81097","content":"I vote for D. ABC only mention their solution to start and stop the EB environment. Only D will terminate the instance and it is part of the requirement.","poster":"jgtran","upvote_count":"3"},{"poster":"Smart","timestamp":"1633117860.0","comment_id":"75370","content":"Agree with rest - B. C can be achieve the same task as B through longer route. Underneath activity task, there will be need to trigger lambda function. Also, recognize there is no simple start/stop process - it is more about terminate and rebuild environments. \n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/schedule-elastic-beanstalk-stop-restart/","upvote_count":"2"},{"content":"Question is asking start and terminate. Option D is the only with start and terminate all other options ABC are about start and stop only.","timestamp":"1633117200.0","upvote_count":"4","comment_id":"70092","poster":"hammad"},{"content":"Answer is B","upvote_count":"2","comment_id":"45262","poster":"amog","timestamp":"1632805080.0"},{"poster":"dumma","upvote_count":"1","comment_id":"42693","timestamp":"1632755940.0","content":"B is correct, Option C is wrong as although Step Functions would work, they need to be scheduled to be invoked using either CloudWatch Events or other mechanism, which is missing in the answer."},{"content":"For B&C, How to handle the increased use of Elastic Beanstalk environments among different teams?\nSo my answer is D","timestamp":"1632697140.0","comment_id":"36527","poster":"simonyu","upvote_count":"2"},{"upvote_count":"1","poster":"CSharpPro","comment_id":"34804","content":"Ans: B\nActivity Tasks need to be run on a Server, either VM or EC2. This increases costs.","timestamp":"1632596580.0"},{"timestamp":"1632396900.0","content":"B? C? \nIn B: its clear that cron job is scheduled. (does it meet the requirement of One-stop soliton??)\nin C: Saying \"Invoke step function daily\" , did not metion to configure Cloudwatch event.","upvote_count":"1","poster":"JayK","comment_id":"30035"}],"topic":"1","exam_id":32,"answer":"B","unix_timestamp":1568269920,"question_id":382,"answer_description":"Reference:\nhttps://aws.amazon.com/premiumsupport/knowledge-center/schedule-elastic-beanstalk-stop-restart/","answer_ET":"B","answer_images":[],"question_text":"A company CFO recently analyzed the company's AWS monthly bill and identified an opportunity to reduce the cost for AWS Elastic Beanstalk environments in use. The CFO has asked a Solutions Architect to design a highly available solution that will spin up an Elastic Beanstalk environment in the morning and terminate it at the end of the day.\nThe solution should be designed with minimal operational overhead and to minimize costs. It should also be able to handle the increased use of Elastic Beanstalk environments among different teams, and must provide a one-stop scheduler solution for all teams to keep the operational costs low.\nWhat design will meet these requirements?","isMC":true,"answers_community":["B (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/5090-exam-aws-certified-solutions-architect-professional-topic-1/","choices":{"D":"Configure a time-based Auto Scaling group. In the morning, have the Auto Scaling group scale up an Amazon EC2 instance and put the Elastic Beanstalk environment start command in the EC2 instance user data. At the end of the day, scale down the instance number to 0 to terminate the EC2 instance.","C":"Develop an AWS Step Functions state machine with ג€waitג€ as its type to control the start and stop time. Use the activity task to start and stop the Elastic Beanstalk environment. Create a role for Step Functions to allow it to start and stop the Elastic Beanstalk environment. Invoke Step Functions daily.","A":"Set up a Linux EC2 Micro instance. Configure an IAM role to allow the start and stop of the Elastic Beanstalk environment and attach it to the instance. Create scripts on the instance to start and stop the Elastic Beanstalk environment. Configure cron jobs on the instance to execute the scripts.","B":"Develop AWS Lambda functions to start and stop the Elastic Beanstalk environment. Configure a Lambda execution role granting Elastic Beanstalk environment start/stop permissions, and assign the role to the Lambda functions. Configure cron expression Amazon CloudWatch Events rules to trigger the Lambda functions."},"question_images":[],"timestamp":"2019-09-12 08:32:00"},{"id":"ML3p9LCVbbX4wpbtHyC1","topic":"1","discussion":[{"poster":"Moon","comment_id":"14437","content":"My answers are \"A & C\".\nThe key point in the question is that it request to \"asses, audit & monitor\". Therefore, any answer that contains terminations for instances/services shall be eliminated. \nSo, \"D & E\": are out because they are taking actions.\n\"B\": does not make sense!\nA: Config rules, are very useful tool for compliancy.\nC: Cloud Trail is also great tool for auditing.","comments":[{"upvote_count":"2","comment_id":"29494","content":"A&C. I have the same viewpoint to Moon.\n\nOption C encrypted CloudTrail logs in addition to what option E do, this is considered as a best practice here: https://docs.aws.amazon.com/en_pv/awscloudtrail/latest/userguide/best-practices-security.html","timestamp":"1633132080.0","poster":"tan9"},{"timestamp":"1660223760.0","poster":"pixepe","upvote_count":"1","comment_id":"645491","content":"Correct - A,C.\n\nE is incorrect - As cloudtrail can't publish on sns topic on unauthorized usage. Per AWS, \"You can be notified when CloudTrail publishes new log files to your Amazon S3 bucket. You manage notifications using Amazon Simple Notification Service (Amazon SNS).\" https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-find-log-files.html"},{"timestamp":"1633123020.0","poster":"9Ow30","content":"Yes A and C are good. Just tell what is asked and nothing extra. So we can ignore the action answers.","comment_id":"28494","upvote_count":"5"}],"timestamp":"1632864480.0","upvote_count":"42"},{"poster":"donathon","comment_id":"13226","content":"AE\nB\\D: Cloudwatch cannot monitor API changes.\nC: Both C and E is doable but I feel E is better because it revert changes and hence ensures the environment is always in compliance.\nhttps://aws.amazon.com/blogs/security/how-to-audit-your-aws-resources-for-security-compliance-by-using-custom-aws-config-rules/\nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudwatch-alarms-for-cloudtrail.html#cloudwatch-alarms-for-cloudtrail-cloudtrail-changes\nhttps://aws.amazon.com/blogs/security/how-to-automatically-revert-and-receive-notifications-about-changes-to-your-amazon-vpc-security-groups/","upvote_count":"11","timestamp":"1632662700.0","comments":[{"content":"i believe A,C. ans E , no such operation \"Evaluate the usage of Lambda functions to automatically revert non-authorized changes in AWS resources\". and since aws config already restrict the operation, it can't be any non-authorized changes","timestamp":"1633221600.0","upvote_count":"4","comment_id":"41932","poster":"PacoDerek"},{"timestamp":"1635369060.0","upvote_count":"1","content":"B\\D: Cloudwatch cannot monitor API changes.\nGood point","poster":"newme","comment_id":"242506"},{"timestamp":"1635942300.0","comment_id":"409115","poster":"DerekKey","content":"donathon - you are wrong - Cloudwatch can monitor API changes using CloudTrail integration (you enable it on each trail)","upvote_count":"1"},{"comment_id":"392296","upvote_count":"3","timestamp":"1635888960.0","poster":"DashL","content":"Why do you want to revert the changes back (as mentioned in E) when the requirement is only to \" continuously assess, audit, and monitor\"?"}]},{"upvote_count":"1","content":"Selected Answer: AC\nA && C\nKey words - 'continuously' assess, audit, and monitor the 'configurations'\nA is probably correct - although rules should be triggered on resource configuration change not periodically\nB is wrong - no explanation needed\nC is correct - you can search trails history\nD is wrong - requires CloudTrail trails with CloudWatch integration not mentioned here\nE is wrong - it says unauthorized API activities - they are not looking for such functionality","timestamp":"1686969060.0","comment_id":"925707","poster":"SkyZeroZx"},{"timestamp":"1666384080.0","comment_id":"701151","content":"A and C 100% -- I built a solution like this.","upvote_count":"1","poster":"mrgreatness"},{"timestamp":"1639212240.0","upvote_count":"2","content":"A. Use AWS Config rules to periodically audit changes to AWS resources and monitor the compliance of the configuration. Develop AWS Config custom rules using AWS Lambda to establish a test-driven development approach, and further automate the evaluation of configuration changes against the required controls.\nC. Use AWS CloudTrail events to assess management activities of all AWS accounts. Ensure that CloudTrail is enabled in all accounts and available AWS services. Enable trails, encrypt CloudTrail event log files with an AWS KMS key, and monitor recorded activities with CloudWatch Logs.","poster":"cldy","comment_id":"499212"},{"upvote_count":"1","timestamp":"1638577920.0","comment_id":"493439","content":"AC is right","poster":"AzureDP900"},{"upvote_count":"1","poster":"andylogan","timestamp":"1636253340.0","comment_id":"450315","content":"It's A C"},{"timestamp":"1636095720.0","content":"A,C\n\nWhy C instead of D? Because here no requirement to take reactive actions, and C secures the logs better.","poster":"student22","upvote_count":"1","comment_id":"440746"},{"comment_id":"409136","content":"A nad C\nKey words - 'continuously' assess, audit, and monitor the 'configurations'\nA is probably correct - although rules should be triggered on resource configuration change not periodically\nB is wrong - no explanation needed\nC is correct - you can search trails history\nD is wrong - requires CloudTrail trails with CloudWatch integration not mentioned here\nE is wrong - it says unauthorized API activities - they are not looking for such functionality","timestamp":"1636084200.0","upvote_count":"3","poster":"DerekKey"},{"poster":"student2020","content":"Answer C has this last statement \"and monitor recorded activities with CloudWatch Logs\". CloudWatch logs does not record activities from CloudTrail. I think this eliminates C.","timestamp":"1635914640.0","comment_id":"403397","upvote_count":"1","comments":[{"comment_id":"403407","poster":"student2020","content":"Edit - After testing in AWS console, you can actually and monitor recorded activities with CloudWatch Logs. This answer is correct. A and C seem to be the best options..","upvote_count":"1","timestamp":"1635938460.0"}]},{"comment_id":"351886","content":"I'll go with A,C","upvote_count":"2","timestamp":"1635867840.0","poster":"WhyIronMan"},{"poster":"Waiweng","content":"A and C","timestamp":"1635827220.0","comment_id":"343731","upvote_count":"3"},{"content":"A & C for sure","poster":"alisyech","upvote_count":"1","timestamp":"1635658260.0","comment_id":"322391"},{"comment_id":"291065","poster":"wind","content":"go with AC.","timestamp":"1635635280.0","upvote_count":"1"},{"comment_id":"289488","poster":"Kian1","content":"will go with A,C","upvote_count":"2","timestamp":"1635604920.0"},{"content":"AC is my choice","timestamp":"1635569160.0","poster":"Ebi","upvote_count":"3","comment_id":"283947"},{"comment_id":"263432","content":"I'll go with AC. Initially I thought AE. C and E both correct but C is more relevant as E is reverting changes which is not in requirement.","poster":"sanjaym","timestamp":"1635564240.0","upvote_count":"2"},{"content":"AC is correct answer","timestamp":"1635302940.0","poster":"T14102020","comment_id":"241994","upvote_count":"1"},{"comment_id":"232242","poster":"petebear55","content":"I'll go for A and D .. A is agreed .. however like me only a few have gone for D .. but the solution requires MONITORING .. so i would go for D ....E is for recording IP'/API so not really in the requirement","upvote_count":"1","timestamp":"1635289920.0"},{"upvote_count":"3","comment_id":"228795","poster":"jackdryan","content":"I'll go with A,C","timestamp":"1635220860.0"},{"timestamp":"1635179040.0","comment_id":"227961","poster":"Bulti","upvote_count":"3","content":"Answer is A and C. The key is that the ask is to audit, asses and monitor but no where in the question you are asked to take any action otherwise A and E were good option if we were expected to automatically take actions if bad configuration somehow escapees the rules defined using AWS config"},{"upvote_count":"1","comment_id":"215766","content":"A and C for sure.","poster":"vjt","timestamp":"1635146280.0"},{"poster":"Ebi","upvote_count":"1","timestamp":"1635124020.0","comment_id":"211814","content":"AC my answer"},{"poster":"ishuiyutian","upvote_count":"1","comment_id":"207730","timestamp":"1634990520.0","content":"Answer: A & C"},{"comment_id":"200864","poster":"ashendy","upvote_count":"4","content":"AD\nhttps://stelligent.com/2019/02/25/continuous-compliance-on-aws-using-aws-config-rules/","timestamp":"1634885760.0"},{"content":"A,C\nA - Correct. Meets “continuously assess, audit, and monitor the configurations of AWS resources”\nB - Incorrect. CloudWatch Logs agent runs on EC2 only.\nC - Correct. CloudTrail captures AWS API calls including calls for “teams to provision resources”\nD - “automatically revert non-authorized changes in AWS resources” is not an ask\nE - “revert non-authorized changes” is not an ask","upvote_count":"3","comment_id":"171762","timestamp":"1634866800.0","poster":"Neive"},{"content":"A and C, assess, audit, monitor, refer Moon","timestamp":"1634753460.0","upvote_count":"1","poster":"fullaws","comment_id":"148894"},{"comment_id":"134176","content":"AC most likely","upvote_count":"1","timestamp":"1634703900.0","poster":"NikkyDicky"},{"timestamp":"1634674380.0","comment_id":"133233","content":"Answer: A & C","upvote_count":"1","poster":"mat2020"},{"poster":"meenu2225","comment_id":"105469","upvote_count":"1","content":"A, C are the correct options. No doubt about Option A, Doubt is between D and C, C encrypts the logs which is a primary requirement for logs to protect it from unautorized access. Thus, A, C","timestamp":"1634634060.0"},{"timestamp":"1634507280.0","upvote_count":"3","content":"Ask in the question is \"Continuously assess, audit, and monitor the configurations of AWS resources\" \nD and E are also REMEDIATING with lambda, which is not the requirement. \nC is also \"encrypting\" cloud trail which is not the ask. However it is industry standard ( which is one of the business requirements)\nB is Invalid\nSo logically it does make sense A and C are the right choices.","comment_id":"98252","poster":"JAWS1600"},{"upvote_count":"1","timestamp":"1634349300.0","poster":"cert123","comment_id":"79515","content":"B . https://aws.amazon.com/premiumsupport/knowledge-center/schedule-elastic-beanstalk-stop-restart/"},{"content":"I go with A & C","poster":"Joeylee","upvote_count":"2","comment_id":"75522","timestamp":"1634186940.0"},{"timestamp":"1634051520.0","content":"A is valid. AWS Config exists for this very reason. \nB is invalid. Regardless if it can or cannot monitor APIs directly or indirectly, there is a lot of overhead and only can monitor APIs made through application. \nD is possible. I like near real-time monitoring. Again, lot of overhead which can be simplified through AWS Config and easily monitored. \nE is confusing (just like me at this point). There is no direct integration between CloudTrail & SNS - you will have to use Lambda Functions. In order for lambda functions to trigger, you will need cloudwatch events so, at this point, there is no need of cloudtrail at all. This is, now, similar to option D.\nC is valid. We need to store this logs safely. It helps with business requirements - follow compliance standards and AWS best practices. \nWill keep it simple and go with A & C.","comment_id":"75386","poster":"Smart","upvote_count":"5"},{"timestamp":"1633985580.0","content":"CD IMO\nA: Definately not as AWS config rules are continuesly not periodically \nB: Simply wrong\nC: AWS CloudTrail is exactly desgined for the purpose\nD: Right answer\nE: ClouldTrail just logs, and the requirement is for compliance check not for access check. besides unauthorized API call should be blocked in the first place.","poster":"Ming","upvote_count":"1","comment_id":"53634","comments":[{"content":"That's even better! There are certain managed config rules are periodic. Also, custom config rules can be periodically defined.","poster":"Smart","timestamp":"1634019060.0","upvote_count":"1","comment_id":"75376"}]},{"poster":"Ming","comment_id":"53633","upvote_count":"1","timestamp":"1633918740.0","content":"CD IMO\nA: Definately not as AWS config rules are continuesly not periodically \nB: Simply wrong\nC: AWS CloudTrail is exactly desgined for the purpose\nD: Right answer\nE: ClouldTrail just logs, and the requirement is for compliance check not for access check. besides unauthorized API call should be blocked in the first place."},{"timestamp":"1633581360.0","upvote_count":"2","poster":"amog","content":"Should be A,C","comment_id":"45271"},{"comment_id":"44188","upvote_count":"1","poster":"Denis_H","timestamp":"1633409760.0","content":"AC\nThe AWS Config console shows the compliance status of your rules and resources. You can see how your AWS resources comply overall with your desired configurations, and learn which specific resources are noncompliant. You can also use the AWS CLI, the AWS Config API, and AWS SDKs to make requests to the AWS Config service for compliance information.\nhttps://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html\n\nBy default, the log files delivered by CloudTrail to your bucket are encrypted by Amazon server-side encryption with Amazon S3-managed encryption keys (SSE-S3). To provide a security layer that is directly manageable, you can instead use server-side encryption with AWS KMS–managed keys (SSE-KMS) for your CloudTrail log files.\nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/encrypting-cloudtrail-log-files-with-aws-kms.html\nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/best-practices-security.html\nhttps://aws.amazon.com/blogs/security/how-to-audit-your-aws-resources-for-security-compliance-by-using-custom-aws-config-rules/"},{"poster":"money","timestamp":"1633146900.0","comment_id":"41359","content":"A,D\n\nThe key here is continually. AWS configs home page literally says continually assess, audit, and monitor. So that's the first choice. \n\nB - CloudWatch Logs Agent doesn't monitor API changes\nC - Manual processes of looking at CloudTrail events\nD - Good: Near real time, event patterns allow for specificity, Lambda helps keep compliance, and SNS speeds up response time\nE - Manual again. Relies on SNS to notify team of unauthorized API activities.","upvote_count":"3"},{"upvote_count":"2","timestamp":"1633146300.0","content":"A is correct. Between C and D: \nC: saying \"Enable trail\", it should be \"create trail\" then only those logs are vaailable in S3 and Cloudwatch logs. \nD: The question did not ask about action to be taken and also nothing regarding notification\nSo, If the C has \"Create trail\", then its A & C","comment_id":"30041","poster":"JayK"},{"content":"This was a single selection!!! Good Luck ^^","upvote_count":"1","comments":[{"content":"sorry. question 35","upvote_count":"1","poster":"awsgcpazure","comment_id":"19076","timestamp":"1633082220.0"}],"timestamp":"1633068060.0","poster":"awsgcpazure","comment_id":"19075"},{"upvote_count":"1","content":"I prefer AW, it is security-sensitive businesses to AWS so any non compliant resources should be terminated.","timestamp":"1632901740.0","comment_id":"17520","comments":[{"comment_id":"17521","poster":"TechGuru","comments":[{"timestamp":"1633060200.0","poster":"AWS2020","upvote_count":"1","comment_id":"18413","content":"the question didnt ask for action. A doesnt take any action, so i think A and C"}],"timestamp":"1633056420.0","content":"Sorry AE","upvote_count":"1"}],"poster":"TechGuru"},{"poster":"manhmaluc","comment_id":"13942","upvote_count":"1","content":"A E\n\ndue to E will reverts wrong config aws resources","timestamp":"1632736260.0"},{"content":"a, c\nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudwatch-alarms-for-cloudtrail.html","timestamp":"1632637920.0","poster":"huhupai","upvote_count":"5","comment_id":"13016"},{"timestamp":"1632287280.0","comment_id":"11039","poster":"dpvnme","comments":[{"poster":"dpvnme","comments":[{"poster":"awspro","content":"why are you change D to E ?","upvote_count":"1","comment_id":"13008","timestamp":"1632487860.0"}],"upvote_count":"3","comment_id":"11454","timestamp":"1632392460.0","content":"Changing my answer to A&E"}],"content":"A & D\nhttps://aws.amazon.com/blogs/aws/new-cloudwatch-events-track-and-respond-to-changes-to-your-aws-resources/","upvote_count":"1"},{"comment_id":"10734","upvote_count":"4","timestamp":"1632221700.0","poster":"awsec2","comments":[{"timestamp":"1632272880.0","poster":"awsec2","upvote_count":"1","comment_id":"10735","content":"A, b sorry"}],"content":"a,c \n\nhttps://aws.amazon.com/blogs/security/how-to-audit-your-aws-resources-for-security-compliance-by-using-custom-aws-config-rules/"}],"unix_timestamp":1568274240,"timestamp":"2019-09-12 09:44:00","choices":{"D":"Use the Amazon CloudWatch Events near-real-time capabilities to monitor system events patterns, and trigger AWS Lambda functions to automatically revert non-authorized changes in AWS resources. Also, target Amazon SNS topics to enable notifications and improve the response time of incident responses.","C":"Use AWS CloudTrail events to assess management activities of all AWS accounts. Ensure that CloudTrail is enabled in all accounts and available AWS services. Enable trails, encrypt CloudTrail event log files with an AWS KMS key, and monitor recorded activities with CloudWatch Logs.","B":"Use Amazon CloudWatch Logs agent to collect all the AWS SDK logs. Search the log data using a pre-defined set of filter patterns that matches mutating API calls. Send notifications using Amazon CloudWatch alarms when unintended changes are performed. Archive log data by using a batch export to Amazon S3 and then Amazon Glacier for a long-term retention and auditability.","A":"Use AWS Config rules to periodically audit changes to AWS resources and monitor the compliance of the configuration. Develop AWS Config custom rules using AWS Lambda to establish a test-driven development approach, and further automate the evaluation of configuration changes against the required controls.","E":"Use CloudTrail integration with Amazon SNS to automatically notify unauthorized API activities. Ensure that CloudTrail is enabled in all accounts and available AWS services. Evaluate the usage of Lambda functions to automatically revert non-authorized changes in AWS resources."},"question_id":383,"answer_images":[],"question_images":[],"answers_community":["AC (100%)"],"answer_ET":"AC","exam_id":32,"question_text":"A company plans to move regulated and security-sensitive businesses to AWS. The Security team is developing a framework to validate the adoption of AWS best practices and industry-recognized compliance standards. The AWS Management Console is the preferred method for teams to provision resources.\nWhich strategies should a Solutions Architect use to meet the business requirements and continuously assess, audit, and monitor the configurations of AWS resources? (Choose two.)","answer_description":"Reference:\nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudwatch-alarms-for-cloudtrail.html https://docs.aws.amazon.com/en_pv/awscloudtrail/latest/userguide/best-practices-security.html","url":"https://www.examtopics.com/discussions/amazon/view/5098-exam-aws-certified-solutions-architect-professional-topic-1/","isMC":true,"answer":"AC"},{"id":"ZeE6wjEzdqNMslm0oNZy","answer_ET":"D","unix_timestamp":1569825180,"isMC":true,"answer":"D","question_images":[],"exam_id":32,"question_id":384,"timestamp":"2019-09-30 08:33:00","discussion":[{"timestamp":"1632209400.0","comment_id":"13228","poster":"donathon","upvote_count":"35","content":"D\nA\\C: Too slow\nB: Too expensive. Usually for petabyte workloads.\nhttps://aws.amazon.com/snowball/faqs/"},{"content":"How should I choose between Snowmobile and Snowball?\n\nTo migrate large datasets of 10PB or more in a single location, you should use Snowmobile. For datasets less than 10PB or distributed in multiple locations, you should use Snowball. In addition, you should evaluate the amount of available bandwidth in your network backbone. If you have a high speed backbone with hundreds of Gb/s of spare throughput, then you can use Snowmobile to migrate the large datasets all at once. If you have limited bandwidth on your backbone, you should consider using multiple Snowballs to migrate the data incrementally.","timestamp":"1632935700.0","comment_id":"48546","poster":"Averageguy","upvote_count":"18"},{"upvote_count":"1","timestamp":"1683815460.0","comment_id":"895150","poster":"mimadour21698","content":"Selected Answer: D\nD based on answers"},{"poster":"mrgreatness","content":"Okay sorry has to be D, to transfer 400TB over a 1GiB line it would take approx 40 days. So, in my opinion, the questions is bad as it doesn't properly consider the Snowball shipping times , but safest bet here is D. Final Answer D!","upvote_count":"1","timestamp":"1666385400.0","comment_id":"701163"},{"upvote_count":"1","comment_id":"701159","timestamp":"1666384920.0","poster":"mrgreatness","content":"As a default, Snowball uses standard shipping of two- five days. You cannot choose expedited shipping at this time -- from FAQ"},{"content":"D - The Snowball services can typically transfer up to 100 TBs in about a week. So Order 5, we got 10 days here. seems to make sense! However, how long will it take to be delivered and then sent back hmmm","poster":"mrgreatness","comment_id":"701156","timestamp":"1666384620.0","upvote_count":"1"},{"poster":"hilft","content":"Between B and D.\nD","comment_id":"637684","timestamp":"1658879400.0","upvote_count":"1"},{"poster":"Weninka","content":"I would go with A, as B & D will most probably not fit in the time frame - the questions states the data should be migrated within 10 days. The delivery & return of the snowball devices will take at least 2 days and you're left with 8 (in best case scenario) to upload the data on/off the devices. According to this documentation 400TB might take ~ 6 days to get uploaded.\n https://docs.aws.amazon.com/snowball/latest/developer-guide/BestPractices.html\nUtilizing Transfer manager and s3 client multipart parallel uploads seems like a better option for me.\nhttps://aws.amazon.com/blogs/developer/introducing-amazon-s3-transfer-manager-in-the-aws-sdk-for-java-2-x/\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html","upvote_count":"1","comment_id":"593012","comments":[{"poster":"mrgreatness","content":"I was thinking exactly the same. Shipping can be 2-5 days, and then sending back..would it make the 10 day timeframe","comment_id":"701160","upvote_count":"1","timestamp":"1666384980.0"}],"timestamp":"1651052040.0"},{"timestamp":"1643833200.0","comment_id":"539161","content":"One Snowball = 80TB. All you need is 5 80TB Snowballs. Much more cost effective than Snowmobile.","upvote_count":"1","poster":"Jonfernz"},{"poster":"cldy","content":"D. Request multiple AWS Snowball devices to be delivered to the data center. Load the data concurrently into these devices and send it back. Have AWS download that data to the Amazon S3 bucket. Sync the new data that was generated while migration was in flight.","timestamp":"1639213740.0","comment_id":"499239","upvote_count":"2"},{"content":"It's D","comment_id":"450318","timestamp":"1636130160.0","poster":"andylogan","upvote_count":"2"},{"poster":"WhyIronMan","upvote_count":"2","comment_id":"351889","timestamp":"1636066980.0","content":"I'll go with D"},{"content":"Go for Snowball D","timestamp":"1635945180.0","comment_id":"343735","upvote_count":"3","poster":"Waiweng"},{"upvote_count":"2","content":"will go with D","poster":"Kian1","comment_id":"289489","timestamp":"1635811080.0"},{"timestamp":"1635791940.0","poster":"Ebi","comment_id":"283956","content":"Answer is D","upvote_count":"4"},{"upvote_count":"3","content":"I'll go with D","poster":"sanjaym","comment_id":"263435","timestamp":"1635767340.0"},{"timestamp":"1635358740.0","comment_id":"245136","content":"D\ndata <10pb","upvote_count":"1","poster":"rscloud"},{"timestamp":"1635262200.0","comment_id":"241998","content":"Correct answer is D. \nTo migrate large datasets of 10PB or more in a single location, you should use Snowmobile. For datasets less than 10PB or distributed in multiple locations, you should use Snowball.","upvote_count":"1","poster":"T14102020"},{"poster":"jackdryan","content":"I'll go with D","upvote_count":"3","comment_id":"228796","timestamp":"1635162660.0"},{"poster":"Bulti","upvote_count":"1","timestamp":"1634358240.0","comment_id":"227964","content":"Answer is D as the data is less than 10 PB"},{"content":"D for sure.","upvote_count":"1","timestamp":"1634344320.0","poster":"vjt","comment_id":"215769"},{"poster":"Ebi","timestamp":"1634336940.0","content":"400TB of data cannot be transferred over 1Gbps network in 10 days even if 100% utilized.\nThere is no Snowmobile with 1PB storage.\n\nOnly valid answer is D","upvote_count":"1","comment_id":"211827"},{"timestamp":"1633992660.0","poster":"Paitan","comment_id":"198386","upvote_count":"1","content":"Option D."},{"comment_id":"148895","content":"D is correct","timestamp":"1633690500.0","upvote_count":"1","poster":"fullaws"},{"content":"D for sure","timestamp":"1633351680.0","comment_id":"134177","poster":"NikkyDicky","upvote_count":"1"},{"upvote_count":"1","content":"go with D\nSnowmobile is overfit","poster":"noisonnoiton","comment_id":"133686","timestamp":"1632983940.0"},{"content":"answer D","timestamp":"1632937080.0","comment_id":"133230","upvote_count":"1","poster":"mat2020"},{"timestamp":"1632884280.0","content":"As per https://www.edureka.co/blog/aws-snowball-and-snowmobile-tutorial/ \"...AWS recommends using a Snowball when the data to be transferred is less than 10 PB or else to use a Snowmobile.\"","upvote_count":"1","comment_id":"34678","poster":"LunchTime"},{"comment_id":"14433","poster":"Moon","timestamp":"1632657120.0","content":"Answer is \"D\". As said by Donathon.\nA & C: too slow, requires more than 100 days for migration.\nB: is too expensive.","upvote_count":"5"}],"question_text":"A company is running a high-user-volume media-sharing application on premises. It currently hosts about 400 TB of data with millions of video files. The company is migrating this application to AWS to improve reliability and reduce costs.\nThe Solutions Architecture team plans to store the videos in an Amazon S3 bucket and use Amazon CloudFront to distribute videos to users. The company needs to migrate this application to AWS within 10 days with the least amount of downtime possible. The company currently has 1 Gbps connectivity to the Internet with\n30 percent free capacity.\nWhich of the following solutions would enable the company to migrate the workload to AWS and meet all of the requirements?","choices":{"C":"Use an Amazon S3 client to transfer data from the data center to the Amazon S3 bucket over the Internet. Use the throttling feature to ensure the Amazon S3 client does not use more than 30 percent of available Internet capacity.","A":"Use a multi-part upload in Amazon S3 client to parallel-upload the data to the Amazon S3 bucket over the Internet. Use the throttling feature to ensure that the Amazon S3 client does not use more than 30 percent of available Internet capacity.","D":"Request multiple AWS Snowball devices to be delivered to the data center. Load the data concurrently into these devices and send it back. Have AWS download that data to the Amazon S3 bucket. Sync the new data that was generated while migration was in flight.","B":"Request an AWS Snowmobile with 1 PB capacity to be delivered to the data center. Load the data into Snowmobile and send it back to have AWS download that data to the Amazon S3 bucket. Sync the new data that was generated while migration was in flight."},"answer_description":"Reference:\nhttps://www.edureka.co/blog/aws-snowball-and-snowmobile-tutorial/","topic":"1","answers_community":["D (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/5853-exam-aws-certified-solutions-architect-professional-topic-1/","answer_images":[]},{"id":"VXMAbDWyoxMQYX1zYqNv","answer_description":"","answer_ET":"C","answer":"C","exam_id":32,"question_text":"A company has developed a new billing application that will be released in two weeks. Developers are testing the application running on 10 EC2 instances managed by an Auto Scaling group in subnet 172.31.0.0/24 within VPC A with CIDR block 172.31.0.0/16. The Developers noticed connection timeout errors in the application logs while connecting to an Oracle database running on an Amazon EC2 instance in the same region within VPC B with CIDR block 172.50.0.0/16.\nThe IP of the database instance is hard-coded in the application instances.\nWhich recommendations should a Solutions Architect present to the Developers to solve the problem in a secure way with minimal maintenance and overhead?","url":"https://www.examtopics.com/discussions/amazon/view/5258-exam-aws-certified-solutions-architect-professional-topic-1/","timestamp":"2019-09-16 21:36:00","topic":"1","unix_timestamp":1568662560,"choices":{"B":"Create and attach internet gateways for both VPCs. Configure default routes to the internet gateways for both VPCs. Assign an Elastic IP for each Amazon EC2 instance in VPC A","D":"Create an additional Amazon EC2 instance for each VPC as a customer gateway; create one virtual private gateway (VGW) for each VPC, configure an end- to-end VPC, and advertise the routes for 172.50.0.0/16","C":"Create a VPC peering connection between the two VPCs and add a route to the routing table of VPC A that points to the IP address range of 172.50.0.0/16","A":"Disable the SrcDestCheck attribute for all instances running the application and Oracle Database. Change the default route of VPC A to point ENI of the Oracle Database that has an IP address assigned within the range of 172.50.0.0/16"},"answer_images":[],"discussion":[{"poster":"donathon","comment_id":"13229","timestamp":"1632664680.0","upvote_count":"34","content":"C\nA: It does not goes through NAT so this is not the solution.\nB: It does not need to go through internet. This is not secured.\nD: This is VPN which is not suitable. Peering should be used."},{"upvote_count":"5","timestamp":"1632791520.0","comment_id":"45273","content":"Answer is C","poster":"amog"},{"content":"100% C","upvote_count":"1","timestamp":"1666386240.0","comment_id":"701172","poster":"mrgreatness"},{"upvote_count":"1","timestamp":"1666198380.0","comment_id":"699197","poster":"Blair77","content":"Selected Answer: C\nAn easy one! C is right!"},{"timestamp":"1658623740.0","poster":"hilft","comment_id":"635828","upvote_count":"1","content":"most secure.\nB."},{"poster":"AzureDP900","content":"C is right","comment_id":"493440","timestamp":"1638578160.0","upvote_count":"1"},{"upvote_count":"1","poster":"andylogan","timestamp":"1636110060.0","comment_id":"450322","content":"It's C"},{"comment_id":"447097","timestamp":"1636083240.0","upvote_count":"1","content":"C is Correct","poster":"moon2351"},{"upvote_count":"4","timestamp":"1635934680.0","content":"Answer C is missing the routes in VPC B.\n\nYou also need to add a route to the routing table of VPC B that points to the IP address of range of 172.31.0.0/24, otherwise, there is no route for return traffic from VPC B to A.\n\nThat's to say, C is most likely answer but not the full solution.","comment_id":"423430","poster":"walkwolf3"},{"content":"I'll go with C","poster":"WhyIronMan","comment_id":"351890","timestamp":"1635514200.0","upvote_count":"1"},{"comment_id":"343736","poster":"Waiweng","upvote_count":"2","timestamp":"1634893200.0","content":"it's C"},{"upvote_count":"2","comment_id":"289491","content":"will go with C","timestamp":"1634651700.0","poster":"Kian1"},{"content":"Easy one, answer is C","comment_id":"283957","poster":"Ebi","upvote_count":"3","timestamp":"1634521680.0"},{"timestamp":"1634454600.0","poster":"sanjaym","content":"I'll go with C","comment_id":"263436","upvote_count":"2"},{"timestamp":"1634418780.0","poster":"jayakumarchellam","comments":[{"content":"There is no such information. Rather opposite.","upvote_count":"3","poster":"DerekKey","comment_id":"409149","timestamp":"1635752040.0"}],"content":"C is wrong - Peering already happened , problem is timeout . autoscale instance required time to route to Database IP address","upvote_count":"1","comment_id":"262345"},{"timestamp":"1634317860.0","poster":"SachinJha","upvote_count":"1","content":"Though C looks appropriate but not sure how that resolves this problem without EIP: \"The IP of the database instance is hard-coded\"","comment_id":"258940"},{"poster":"T14102020","content":"C is correct. Key is VPC peering","upvote_count":"1","comment_id":"242000","timestamp":"1634265240.0"},{"comment_id":"228797","timestamp":"1634143740.0","poster":"jackdryan","content":"I'll go with C","upvote_count":"3"},{"upvote_count":"4","comment_id":"227967","poster":"Bulti","content":"C is the right answer given all the other options suck more than C which feels incomplete.","timestamp":"1634000100.0"},{"content":"C for sure.","upvote_count":"1","timestamp":"1633552500.0","poster":"vjt","comment_id":"215770"},{"content":"C is correct","timestamp":"1633477860.0","comment_id":"148896","upvote_count":"3","poster":"fullaws"},{"poster":"NikkyDicky","content":"C for sure","upvote_count":"3","comment_id":"134179","timestamp":"1633354260.0"},{"timestamp":"1633275600.0","upvote_count":"1","comments":[{"content":"That is really weird that it is located in another VPC honestly","poster":"Ibranthovic","comment_id":"93207","timestamp":"1633318260.0","comments":[{"comment_id":"409153","timestamp":"1635754740.0","content":"No it is not weird. Since DEV VPC is only for development and database can be used for different purposes.","poster":"DerekKey","upvote_count":"1"}],"upvote_count":"1"},{"timestamp":"1633352880.0","content":"Indeed and how can you configure it on a VPC and even a /16!?!?... with this setup you will get all your routing messed up... If this is real in the exam it really sucks","poster":"pmjcr","upvote_count":"5","comment_id":"110850"},{"comment_id":"155508","timestamp":"1633541220.0","content":"You're right, but using RFC1918 ranges is only a recommendation, it's not a restriction so there's nothing stopping you form using 172.50.0.0/16.","upvote_count":"2","poster":"Stec1980"}],"content":"172.50 is not a private subnet, you can't reach it without internet.","poster":"Ibranthovic","comment_id":"93205"},{"poster":"Denis_H","comment_id":"44251","upvote_count":"1","content":"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html","timestamp":"1632743460.0"},{"upvote_count":"3","content":"My concern with C is that it doesn't include the step of adding a route to the routing table of VPC B as well (to route back to VPC A). So does that mean that D is the only answer that works, even though there is more overhead and maintenance than C?","timestamp":"1632309900.0","comment_id":"12383","comments":[{"comments":[{"comment_id":"83917","poster":"Smart","comments":[{"upvote_count":"1","poster":"Smart","timestamp":"1633253700.0","comment_id":"83918","comments":[{"content":"I think you have missed: secure way with minimal maintenance and overhead","poster":"DerekKey","timestamp":"1635877320.0","upvote_count":"2","comment_id":"409155"}],"content":"Although, recommended way would be to used VPC peering."}],"upvote_count":"1","content":"Agree. Option C is incomplete - it is going to give same error as in scenario; while option D assumes that IGWs exist. I am inclining towards D.","timestamp":"1633047780.0"}],"poster":"sb333","comment_id":"12388","upvote_count":"2","timestamp":"1632383760.0","content":"Looking at answer D, it's poorly written as well. If D is possible to setup in this way (can't find anything pointing to that as a valid architecture they way it's suggested), it doesn't talk about IGW which would be needed. So I would think C is the closest best answer, but missing a step of adding the route to VPC B route table as well, is either intentional to make the answer wrong or is a poorly written answer by leaving out a crucial step. :("}],"poster":"sb333"},{"upvote_count":"4","poster":"piotr","comments":[{"content":"Agree. need peering","upvote_count":"2","poster":"dpvnme","timestamp":"1632283380.0","comment_id":"11456"}],"comment_id":"11314","timestamp":"1632277620.0","content":"DB is in another VPC so peering needed? Answer C."}],"answers_community":["C (100%)"],"question_images":[],"question_id":385,"isMC":true}],"exam":{"lastUpdated":"11 Apr 2025","isBeta":false,"id":32,"numberOfQuestions":1019,"isImplemented":true,"isMCOnly":false,"name":"AWS Certified Solutions Architect - Professional","provider":"Amazon"},"currentPage":77},"__N_SSP":true}