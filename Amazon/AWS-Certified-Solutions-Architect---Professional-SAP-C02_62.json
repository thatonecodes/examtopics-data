{"pageProps":{"questions":[{"id":"xFM5dSAHMpKkhbxitIcU","answer_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/126837-exam-aws-certified-solutions-architect-professional-sap-c02/","question_id":306,"answer_ET":"D","exam_id":33,"answers_community":["D (100%)"],"question_text":"A live-events company is designing a scaling solution for its ticket application on AWS. The application has high peaks of utilization during sale events. Each sale event is a one-time event that is scheduled. The application runs on Amazon EC2 instances that are in an Auto Scaling group. The application uses PostgreSQL for the database layer.\n\nThe company needs a scaling solution to maximize availability during the sale events.\n\nWhich solution will meet these requirements?","topic":"1","unix_timestamp":1700635800,"answer_description":"","timestamp":"2023-11-22 07:50:00","choices":{"B":"Use a scheduled scaling policy for the EC2 instances. Host the database on an Amazon RDS for PostgreSQL Mulli-AZ DB instance with automatically scaling read replicas. Create an Amazon EventBridge rule that invokes an AWS Lambda function to create a larger read replica before a sale event. Fail over to the larger read replica. Create another EventBridge rule that invokes another Lambda function to scale down the read replica after the sale event.","C":"Use a predictive scaling policy for the EC2 instances. Host the database on an Amazon RDS for PostgreSQL MultiAZ DB instance with automatically scaling read replicas. Create an AWS Step Functions state machine to run parallel AWS Lambda functions to pre-warm the database before a sale event. Create an Amazon EventBridge rule to invoke the state machine.","A":"Use a predictive scaling policy for the EC2 instances. Host the database on an Amazon Aurora PostgreSQL Serverless v2 Multi-AZ DB instance with automatically scaling read replicas. Create an AWS Step Functions state machine to run parallel AWS Lambda functions to pre-warm the database before a sale event. Create an Amazon EventBridge rule to invoke the state machine.","D":"Use a scheduled scaling policy for the EC2 instances. Host the database on an Amazon Aurora PostgreSQL Multi-AZ DB cluster. Create an Amazon EventBridge rule that invokes an AWS Lambda function to create a larger Aurora Replica before a sale event. Fail over to the larger Aurora Replica. Create another EventBridge rule that invokes another Lambda function to scale down the Aurora Replica after the sale event."},"question_images":[],"discussion":[{"content":"Selected Answer: D\nD is the best answer.\n\nIt leverages scheduled scaling for EC2 instances, which is ideal for handling predictable, high-traffic event peaks. Amazon Aurora PostgreSQL is a high-performance database solution that provides the reliability needed for such critical operations. The use of a larger Aurora Replica during the event and scaling down afterward allows for efficient resource utilization, aligning the database capacity with the fluctuating demand.\n\nWhile it introduces some complexity in terms of manual replica management, this approach offers a good balance between performance, reliability, and cost-effectiveness, making it well-suited for the described scenario.","comment_id":"1084802","upvote_count":"8","timestamp":"1701381360.0","poster":"heatblur"},{"content":"Selected Answer: D\nAnswer: D","timestamp":"1700635800.0","comment_id":"1077029","poster":"cypkir","upvote_count":"5"},{"poster":"AzureDP900","timestamp":"1731791400.0","content":"D is best option","upvote_count":"1","comment_id":"1313256"},{"timestamp":"1719657180.0","comment_id":"1239216","upvote_count":"1","poster":"gfhbox0083","content":"D for sure."},{"poster":"duriselvan","comment_id":"1146968","content":"key point is Create an Amazon EventBridge - Monitor Application Auto Scaling events with Amazon EventBridge \nAmazon EventBridge, formerly called CloudWatch Events, helps you monitor events that are specific to Application Auto Scaling and initiate target actions that use other AWS services. Events from AWS services are delivered to EventBridge in near real time.\nhttps://docs.aws.amazon.com/autoscaling/application/userguide/monitoring-eventbridge.html","upvote_count":"2","timestamp":"1707622020.0"},{"timestamp":"1704928860.0","comments":[{"poster":"tmlong18","upvote_count":"3","timestamp":"1705375500.0","comment_id":"1123830","content":"Aurora auto scaling requires some time to adjust, and cannot handle sudden spikes in traffic.\nAuto scaling is more suitable for gradually increasing traffic."}],"comment_id":"1119144","upvote_count":"4","content":"Selected Answer: D\nBad question design. \nAurora support auto scaling, so the answer should have Aurora autoscaling. But the predictive scaling for ASG in A and C is obviously wrong. And B is using Lambda function to fail over while Aurora already has this feature. Which leaves D the only possible answer. \nWho the hell designed this stupid answers.","poster":"bjexamprep"},{"upvote_count":"1","timestamp":"1704900600.0","poster":"career360guru","comment_id":"1118751","content":"Selected Answer: D\nB or D are the possible choices. D is better choice as it uses Aurora engine that has better availability and scaling performance."},{"comment_id":"1087309","upvote_count":"2","content":"Selected Answer: D\nleverages scheduled scaling and Aurora PostgreSQL is high-performance database","timestamp":"1701662160.0","poster":"J0n102"},{"upvote_count":"4","poster":"devalenzuela86","comment_id":"1077560","timestamp":"1700671200.0","content":"Selected Answer: D\nAnswer D"}],"answer":"D"},{"id":"Tr44taYoDFGEqapQ3DHQ","answer_ET":"ADE","answers_community":["ADE (53%)","DEF (26%)","11%","5%"],"discussion":[{"poster":"heatblur","comment_id":"1080377","upvote_count":"9","content":"Selected Answer: ADE\nADE\n\nOption D: Create an AWS Direct Connect connection and a Direct Connect gateway between the on-premises network and the target AWS network.\nOption E: During configuration of the replication servers, select the option to use private IP addresses for data replication.\nOption A: could be considered if the private subnets are used without the NAT gateways, ensuring internal-only network access","timestamp":"1700951400.0"},{"content":"Selected Answer: DEF\nhttps://docs.aws.amazon.com/drs/latest/userguide/quick-start-guide-gs.html\n\n(E) Data routing and throttling controls how data flows from the external server to the replication servers. If you choose not to use a private IP, your replication servers will be automatically assigned a public IP and data will flow over the public internet. Check \"Use private IP for data replication\".\n\n(F) On Default DRS launch settings, check \"Copy private IP\". This way all other servers can transparently reach the recovered server.\n\n(D) Architects could use VPN or AWS DC, but \"...The company does not want this solution to consume all available network bandwidth because other applications require bandwidth.\", preferably use AWS Direct Connect.","poster":"MegalodonBolado","timestamp":"1704306300.0","comment_id":"1113021","upvote_count":"7"},{"comment_id":"1399793","content":"Selected Answer: CDE\nC - This ensures that replication traffic does not travel over the public internet, meeting the security requirement.\nD- Direct Connect provides a dedicated, high-bandwidth, and lower-latency connection to AWS, ensuring replication traffic does not consume all available internet bandwidth.\nE- Ensures that replication occurs over private connectivity rather than the public internet, aligning with the security requirement.","upvote_count":"1","timestamp":"1742239320.0","poster":"Deztroyer88"},{"comment_id":"1311092","upvote_count":"1","poster":"0b43291","content":"Selected Answer: ADE\nBy following these steps, you can meet the requirements of configuring a cloud backup of the on-premises intranet application using AWS Elastic Disaster Recovery, ensuring that replication traffic does not travel through the public internet, preventing the application from being accessible from the internet (since it's deployed in private subnets), and not consuming all available network bandwidth (since you're using a dedicated Direct Connect connection).","timestamp":"1731473160.0"},{"content":"Selected Answer: DEF\n> By default, data is sent from the source servers to the replication servers over the public internet, using the public IP that was automatically assigned to the replication servers. Transferred data is always encrypted in transit. \n\n> Choose the box to the left of the Use private IP for data replication... option if you want to route the replicated data from your source servers to the staging area subnet through a private network with a VPN, AWS Direct Connect, VPC peering, or another type of existing private connection.","poster":"that1guy","comment_id":"1301188","timestamp":"1729530840.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1728919440.0","comment_id":"1297706","content":"Why it cannot be the following:\n\n• A. Create a VPC that has at least two private subnets, two NAT gateways, and a virtual private gateway. - NAT Gateway not necessary\n• B. Create a VPC that has at least two public subnets, a virtual private gateway, and an internet gateway. - IGW not required\n• C. Create an AWS Site-to-Site VPN connection between the on-premises network and the target AWS network. - You need bandwidth so that teh solution does not impact other applications","poster":"AloraCloud"},{"upvote_count":"3","poster":"Syre","timestamp":"1727455080.0","comment_id":"1290159","content":"Selected Answer: ACE\nDirect Connect is an overkill for such a solution. You cant set it all up just to do DR."},{"upvote_count":"2","poster":"ShenYuying","timestamp":"1725601200.0","comment_id":"1279382","content":"Regarding Option A, I'm not sure why there should be at least 2 subnets in the VPC. When configuring the Elastic Disaster Recovery, you only need to choose 1 subnet as target area. Besides, NAT is not needed here. \nFor Option F, you can choose \"Copy private IP\" to match source server's IP address, but this is not a must, it is an optional choice, you don't need to choose it to meet the question's requirement.\nI'm really confused"},{"comment_id":"1269640","upvote_count":"1","content":"Those who picked A, why would you need the NAT gateways?!","poster":"asquared16","timestamp":"1724179920.0"},{"timestamp":"1723510620.0","upvote_count":"1","content":"I am super confused about A\n\nA says Virtual Private Gateway, which is for Site-to-Site VPNs. Why do we need this ???","poster":"kgpoj","comment_id":"1264887"},{"upvote_count":"2","poster":"vip2","timestamp":"1720508280.0","comment_id":"1244747","content":"Selected Answer: DEF\nreplication traffic does not travel through the public internet. --> Not A \nmust not be accessible from the internet --> Not B\nThe company does not want this solution to consume all available network bandwidth --> not C, it requires D as dedicated network\nE and F during the Disaster Recovery step 3 and 4 as described as link below,\nhttps://docs.aws.amazon.com/drs/latest/userguide/quick-start-guide-gs.html"},{"timestamp":"1706692860.0","content":"We don't need to connect internet, why we need NAT gateway in A?","comments":[{"content":"the question says not accessible from internet \nNAT gateway is for inbound to internet and not internet -> inbound","comment_id":"1194846","timestamp":"1713002100.0","upvote_count":"2","poster":"drake2020"},{"upvote_count":"1","poster":"marszalekm","comments":[{"comment_id":"1150452","timestamp":"1707939300.0","content":"Thats the only info I found, however this doesn't exactly answer your question.","upvote_count":"1","poster":"marszalekm"}],"comment_id":"1150451","content":"https://docs.aws.amazon.com/drs/latest/userguide/Network-Requirements.html\nThere are two ways to establish direct connectivity to the Internet for the VPC of the staging area, as described in the VPC FAQ\n1. Public IP address + Internet gateway\n2. Private IP address + NAT instance","timestamp":"1707939120.0"}],"upvote_count":"4","poster":"ftaws","comment_id":"1136577"},{"content":"How about A,C,E?\nA. Create an intranet application and other application in a private subnet.\nIntranet applications connect to a private gateway(one).\nOther applications connect to the NAT gateway(one).\nEliminates traffic interference.\nC. Site-to-Site VPN connect to private gateway.\nE. Replicates private IP.","upvote_count":"4","timestamp":"1706094600.0","poster":"zhooon","comment_id":"1130507","comments":[{"upvote_count":"1","content":"Can not backup for other application through Site-to-Site VPN.\nIt is correct Option D. 'Direct Connect gateway'\nA, D, E","poster":"zhooon","timestamp":"1706277300.0","comment_id":"1132559"},{"poster":"zhooon","comment_id":"1130528","content":"Can other applications communicate with the Internet through the NAT gateway?","upvote_count":"1","timestamp":"1706096460.0"}]},{"comment_id":"1118761","poster":"career360guru","timestamp":"1704901020.0","content":"Selected Answer: ADE\nA, D and E","upvote_count":"2"},{"comment_id":"1103579","poster":"yuliaqwerty","content":"Answer ADE","timestamp":"1703266740.0","upvote_count":"1"},{"poster":"shaaam80","content":"Selected Answer: ADE\nAnswer ADE","timestamp":"1701865860.0","comment_id":"1089322","upvote_count":"2"},{"comment_id":"1087307","poster":"J0n102","timestamp":"1701661500.0","upvote_count":"5","content":"Selected Answer: ADE\nDX is needed as it Provides a dedicated, private network connection that can be managed to avoid consuming all available network bandwidth"},{"timestamp":"1701427260.0","content":"Selected Answer: BDE\nNot Option - A, I don't see the point of creating NAT gateways.","comments":[{"content":"mb, answer should A,D,E","comment_id":"1086681","upvote_count":"1","timestamp":"1701591060.0","poster":"SHASHANK32"}],"comment_id":"1085146","upvote_count":"1","poster":"SHASHANK32"},{"upvote_count":"1","comment_id":"1083629","comments":[{"comment_id":"1083630","upvote_count":"1","poster":"shaaam80","timestamp":"1701273600.0","content":"Direct connect not needed as there is no ask for a dedicated connection or high speed.","comments":[{"content":"Question states: \"The company does not want this solution to consume all available network bandwidth because other applications require bandwidth.\"\n\nUsage of a VPN relies on the companies bandwidth and could very easily consume most of it. They'd need a dedicated connection (aka Direct Connect) to meet this requirement.","upvote_count":"3","timestamp":"1701381600.0","poster":"heatblur","comment_id":"1084805"}]}],"content":"Answer - ACE \nVPC with 2 private subnets and 2 NAT gateways for application and replication traffic which has to be private\nSite to Site VPN - for secure connection between Onprem and Customer VPC so both replication and application traffic does not flow over public internet\nChoosing private IP address for replication.","timestamp":"1701273540.0","poster":"shaaam80"},{"content":"Selected Answer: ADE\nI guess ADE","timestamp":"1701076080.0","poster":"HunkyBunky","upvote_count":"1","comment_id":"1081332"},{"poster":"devalenzuela86","content":"Selected Answer: AEF\nCreating a VPC with at least two public subnets and an internet gateway (Option B) would allow the application to be accessible from the internet, which is not a requirement. Creating an AWS Site-to-Site VPN connection (Option C) or an AWS Direct Connect connection (Option D) would allow the replication traffic to be routed through a private network, but these options are not required since Option A already provides a private network 1. answer AEF","upvote_count":"1","timestamp":"1700836920.0","comment_id":"1079348"},{"poster":"devalenzuela86","content":"Selected Answer: ACE\nACE for sure","upvote_count":"1","comment_id":"1077565","timestamp":"1700671560.0"},{"content":"Selected Answer: BDE\nAnswer: B D E","timestamp":"1700635800.0","upvote_count":"1","poster":"cypkir","comment_id":"1077030"}],"answer":"ADE","unix_timestamp":1700635800,"url":"https://www.examtopics.com/discussions/amazon/view/126838-exam-aws-certified-solutions-architect-professional-sap-c02/","exam_id":33,"question_text":"A company runs an intranet application on premises. The company wants to configure a cloud backup of the application. The company has selected AWS Elastic Disaster Recovery for this solution.\n\nThe company requires that replication traffic does not travel through the public internet. The application also must not be accessible from the internet. The company does not want this solution to consume all available network bandwidth because other applications require bandwidth.\n\nWhich combination of steps will meet these requirements? (Choose three.)","question_images":[],"question_id":307,"isMC":true,"timestamp":"2023-11-22 07:50:00","answer_description":"","topic":"1","choices":{"E":"During configuration of the replication servers, select the option to use private IP addresses for data replication.","C":"Create an AWS Site-to-Site VPN connection between the on-premises network and the target AWS network.","D":"Create an AWS Direct Connect connection and a Direct Connect gateway between the on-premises network and the target AWS network.","A":"Create a VPC that has at least two private subnets, two NAT gateways, and a virtual private gateway.","B":"Create a VPC that has at least two public subnets, a virtual private gateway, and an internet gateway.","F":"During configuration of the launch settings for the target servers, select the option to ensure that the Recovery instance’s private IP address matches the source server's private IP address."},"answer_images":[]},{"id":"mdRko99LNZZHolYxNOnz","topic":"1","answer":"D","choices":{"A":"Use AWS Step Functions to process the S3 event that occurs when a user stores an image. Run an AWS Lambda function that resizes the image in place and replaces the original file in the S3 bucket. Create an S3 Lifecycle expiration policy to expire all stored images after 6 months.","C":"Use S3 Event Notifications to invoke an AWS Lambda function when a user stores an image. Use the Lambda function to resize the image in place and to store the original file in the S3 bucket. Create an S3 Lifecycle policy to move all stored images to S3 Standard-Infrequent Access (S3 Standard-IA) after 6 months.","B":"Use Amazon EventBridge to process the S3 event that occurs when a user uploads an image. Run an AWS Lambda function that resizes the image in place and replaces the original file in the S3 bucket. Create an S3 Lifecycle expiration policy to expire all stored images after 6 months.","D":"Use Amazon Simple Queue Service (Amazon SQS) to process the S3 event that occurs when a user stores an image. Run an AWS Lambda function that resizes the image and stores the resized file in an S3 bucket that uses S3 Standard-Infrequent Access (S3 Standard-IA). Create an S3 Lifecycle policy to move all stored images to S3 Glacier Deep Archive after 6 months."},"answer_description":"","discussion":[{"comments":[{"content":"Agreed with B \n using Amazon EventBridge, you can meet the company's requirements most cost-effectively:\nhandle significant variance in demand\nBe reliable at enterprise scale\nRerun processing jobs in the event of failure (not explicitly required but ensures reliability)\nMove stored images to a colder storage class after 6 months to reduce costs.","upvote_count":"1","timestamp":"1731790800.0","comment_id":"1313244","poster":"AzureDP900"},{"upvote_count":"4","content":"How do you rerun for failure with option B?\n\nSQS can handle \"rerun\", hence D","timestamp":"1723517340.0","comment_id":"1264935","poster":"kgpoj"}],"content":"Selected Answer: B\nConsidering the requirements, Option B (Amazon EventBridge with AWS Lambda and S3 Lifecycle Expiration Policy) seems to be the most cost-effective and appropriate solution. It combines the scalability and flexibility of AWS Lambda for image processing with the straightforward event handling of Amazon EventBridge, and appropriately manages the image lifecycle with an S3 expiration policy. While Option C is also a strong contender, the misalignment of the lifecycle policy with the requirement makes Option B a better fit. Option A might be more suitable for complex workflows but is likely not needed for this scenario, and Option D includes unnecessary long-term archival steps.","comment_id":"1077891","timestamp":"1700691780.0","upvote_count":"16","poster":"thala"},{"timestamp":"1703267040.0","poster":"yuliaqwerty","upvote_count":"12","comment_id":"1103588","comments":[{"comment_id":"1297712","content":"Yes it is ....\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/EventBridge.html","upvote_count":"3","timestamp":"1728920100.0","poster":"AloraCloud"}],"content":"B is for sure\nA no because Step Function is not in list of s3 event destinations https://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-how-to-event-types-and-destinations.html\nC and D has option for storing data longer than 6 months which is not required"},{"comment_id":"1336045","content":"Selected Answer: B\nOption B (Amazon EventBridge with AWS Lambda and S3 Lifecycle Expiration Policy) seems to be the most cost-effective and appropriate solution. It combines the scalability and flexibility of AWS Lambda for image processing with the straightforward event handling of Amazon EventBridge, and appropriately manages the image lifecycle with an S3 expiration policy.","upvote_count":"1","poster":"Chakanetsa","timestamp":"1735913940.0"},{"poster":"SIJUTHOMASP","comment_id":"1331707","content":"Selected Answer: D\nInvoking Lambda directly won't give any resiliency - so C is not a choice. Since the Event Bridge solution tries to replace original file - its rule out. Step function can't be destined from S3 - A is out. Hence the right answer is D through SQS meeting the need for re-run.","timestamp":"1735160760.0","upvote_count":"1"},{"comment_id":"1325255","upvote_count":"1","content":"Selected Answer: D\nOptions A and B imply replacing the original image, which will cause an execution loop.\nOption C doesn't allow rerunning failed jobs.\nTherefore, only option D meets all the requirements.","poster":"henrikhmkhitaryan59","timestamp":"1733951580.0"},{"content":"Selected Answer: D\nThis is a very malformed question. It should be D because SQS can handle failure. But then the archiving policy is not requested. So all options are not optimal in my opinion.","upvote_count":"1","timestamp":"1733735940.0","poster":"Spike2020","comment_id":"1323942"},{"timestamp":"1733400180.0","poster":"nimbus_00","comment_id":"1322345","upvote_count":"1","content":"Selected Answer: B\nArchiving and replaying events with Amazon EventBridge\nhttps://aws.amazon.com/blogs/compute/archiving-and-replaying-events-with-amazon-eventbridge/"},{"timestamp":"1732889820.0","comment_id":"1319751","upvote_count":"1","poster":"TomTom","content":"Selected Answer: D\nOption D is most cost effective"},{"upvote_count":"1","timestamp":"1732810620.0","poster":"youonebe","comment_id":"1319339","content":"Selected Answer: D\nneed to rerun failed jobs, so D"},{"poster":"0b43291","timestamp":"1731473820.0","content":"Selected Answer: D\nDifficult one. Both options B and D meet the specific requirement of storing the files in an Amazon S3 bucket for up to 6 months.\n\nHowever, when considering the additional requirements of being reliable at enterprise scale, having the ability to rerun processing jobs in the event of failure, and being the most cost-effective solution, option D with Amazon SQS, AWS Lambda, and the S3 Lifecycle policy to transition to Glacier Deep Archive is still the better choice.\n\nNo Rerun of jobs with B. Only D","upvote_count":"1","comment_id":"1311097"},{"poster":"Halliphax","upvote_count":"1","timestamp":"1731235200.0","comment_id":"1309400","content":"Selected Answer: B\n\"Store the images in S3 for six months\" - leaves only option B. \n\nOptions C & D mean keeping the images in S3 forever and that's not the more cost effective option compared to just deleting the files as the question implies is a requirement."},{"upvote_count":"2","content":"Selected Answer: D\nYou’ve got to have a buffer for reruns!\nFor those concerned about the 6 months TTL in S3 remember glacier isn’t S3.","timestamp":"1731226260.0","comment_id":"1309338","poster":"nimbus_00"},{"comment_id":"1306086","content":"Selected Answer: B\nC and D are out, for keeping data more than 6 months.\nA is out, due to S3 event destination does not include step function, which is anyway seldom use for one step action.\nEventbridge does support retry if event fail to go off:","poster":"Daniel76","upvote_count":"1","timestamp":"1730534460.0"},{"comment_id":"1302511","poster":"TewatiaAmit","content":"Selected Answer: D\nSQS ensured that any failed jobs can be retried.","timestamp":"1729783440.0","upvote_count":"2"},{"poster":"mkgiz","comment_id":"1288893","content":"Selected Answer: D\n\"ability to rerun processing jobs in the event of failure\"","upvote_count":"3","timestamp":"1727241660.0"},{"timestamp":"1724125320.0","comment_id":"1269037","poster":"2aa2222","content":"Let’s break down the question into some decisive pieces:\n1.Millions of customers will use solution  this to me has to be a robust queuing solution (like SQS, not eventbridge, not step-function)\n2.Store the files in an Amazon S3 bucket for up to 6 months This doesn’t talk about deleting the files. It says store in “an” S3 bucket for 6-months, which means it can definitely go to another “MOST cost-effective” bucket, i.e. Glacier Deep Archive\n3.Solution must handle significant variance in demand  “significant” variance can be interpreted as infrequent usage.\n4.Solution must also be reliable /ability to rerun processing in the event of failure – Only SQS can achieve this.\nMy verdict: Answer = D","upvote_count":"4"},{"timestamp":"1723957140.0","upvote_count":"2","content":"Selected Answer: D\n1. The solution must also be reliable at enterprise scale and have the ability to rerun processing jobs in the event of failure.\n Use SQS , option A / B / C are not include AWS SQS.\n2. The cost-effective solution , option D contains S3 Glacier Deep Archive to reduce S3 storage costs","comment_id":"1267926","poster":"felon124"},{"comment_id":"1265580","comments":[{"timestamp":"1730885760.0","poster":"Miquella_The_Rizzler","upvote_count":"1","comment_id":"1307760","content":"That is not true at all you can definitely check logic for comparing the upload date to prevent infinite loop, not to mention we can implement all kind of logic to assign UUID for image. \nThis is their use case:\nEventBridge Replay when: \n - You need to reprocess a batch of historical events\n - Original event order matters\n - After fixing system-wide issues\n - For disaster recovery scenarios\nSQS Acknowledgment when:\n - You need per-message processing guarantees\n - Want automatic retry handling\nNeed individual message tracking"}],"poster":"ff32d79","timestamp":"1723624980.0","content":"Selected Answer: B\nI go for B but this question is completely wrong. First of all, if you modify in the same bucket you are going to have pontential infinite loop... which means 3 answers are out. But why would you save things in glacier when they can be deleted? About ReRun, in EventBridge you can replay...","upvote_count":"1"},{"content":"Selected Answer: D\nEverybody is focused on choosing the MOST cost-effective option, but there's also this requirement:\n\"The solution must also be reliable at enterprise scale and have the ability to rerun processing jobs in the event of failure.\"\n\nwhich I believe it can be achieved only by option D","comment_id":"1265195","poster":"tgv","upvote_count":"1","timestamp":"1723558800.0"},{"comment_id":"1244045","timestamp":"1720402080.0","upvote_count":"1","content":"I think this question is warding wrong. If we look at the requirement \"store the files in an Amazon S3 bucket for up to 6 months.\" and decide that objects can be deleted after 6 months, C and D are excluded.\nBut is that true?\nWould AWS create a problem involving such an elementary mistake?","poster":"053081f"},{"upvote_count":"1","timestamp":"1719968340.0","content":"Selected Answer: A\nA is the answer","comment_id":"1241093","poster":"awsaz"},{"upvote_count":"4","poster":"Helpnosense","content":"Selected Answer: D\nVote D because the requirement \"rerun processing jobs in the event of failure.\" Glacier Deep archive is also really cost-effective","comment_id":"1238436","timestamp":"1719534180.0"},{"content":"Option D is right answer as it gets the batch files with significant variance in demand","upvote_count":"1","poster":"9f02c8d","timestamp":"1716746820.0","comment_id":"1219097"},{"comment_id":"1214119","poster":"teo2157","timestamp":"1716180900.0","content":"Selected Answer: A\nGoing for A as it's the only option that achieve reprocessing, B could be a good answer but it doesn´t allow any reporcessing.","upvote_count":"2"},{"comment_id":"1211125","content":"Selected Answer: A\na: step function for 'the ability to rerun processing jobs in the event of failure'.\nc,d out for keeping the files after 6 months.\nb: not able to rerun failures..","poster":"thotwielder","upvote_count":"2","timestamp":"1715642340.0"},{"timestamp":"1713624780.0","comments":[{"timestamp":"1715022720.0","content":"That is wrong lambda functions alone can be used to resize images, there is no need for step functions, there is even an article on this from 6 YEARS AGO on how to resize images just with lambda https://aws.amazon.com/blogs/compute/resize-images-on-the-fly-with-amazon-s3-aws-lambda-and-amazon-api-gateway/","comment_id":"1207539","poster":"e4bc18e","upvote_count":"2"}],"comment_id":"1199200","upvote_count":"3","content":"Correct: A\nAn S3 event is created by eventbridge and can trigger a step function.\nImage processing cannot be done a lambda function alone. Step function is preferred for image processing. You also need the ability to rerun the jobs.\nhttps://docs.aws.amazon.com/step-functions/latest/dg/use-cases-data-processing.html\nhttps://docs.aws.amazon.com/step-functions/latest/dg/tutorial-cloudwatch-events-s3.html","poster":"trap"},{"timestamp":"1712486520.0","comment_id":"1190903","poster":"FF2024","upvote_count":"2","content":"B is the answer.\nA is wrong - AWS Step Function cannot directly be invoked by S3 Event Notification."},{"upvote_count":"1","content":"B is the best answer (cheapest option) compared to A as step functions requires event notification (for trigger) which is typically done using event bridge .","timestamp":"1712234640.0","comment_id":"1189324","poster":"SKS"},{"upvote_count":"3","content":"Selected Answer: B\nAnother poorly worded question. First of all, C and D can be eliminated since they keep files after the 6 month period. Then, both A and B are valid. Both will use a Lambda to do the work, but the state changes in the Step Function will incur a very slight cost. Personally, I'd choose A to get control of retries, etc, but the MOST cost-effective alternative is B.","timestamp":"1710950640.0","comment_id":"1178484","poster":"Dgix"},{"upvote_count":"1","comment_id":"1167180","timestamp":"1709731860.0","content":"Selected Answer: B\nC & D are automatically eliminated as the images don't need to be stored beyond 6 months.\n\nStep Function cannot be invoked for an S3 Event, thus EventBridge.","poster":"hogtrough"},{"content":"Selected Answer: B\nBy default, EventBridge retries sending the event for 24 hours and up to 185 times with an exponential back off and jitter, or randomized delay. If an event isn't delivered after all retry attempts are exhausted, the event is dropped and EventBridge doesn't continue to process it. To avoid losing events after they fail to be delivered to a target, you can configure a dead-letter queue (DLQ) and send all failed events to it for processing later.","poster":"igor12ghsj577","timestamp":"1707930600.0","comment_id":"1150359","upvote_count":"2"},{"comment_id":"1150194","upvote_count":"1","timestamp":"1707913080.0","poster":"arberod","content":"Selected Answer: B\nIt is B"},{"poster":"ele","comment_id":"1146000","content":"Selected Answer: A\nA is the most cost effective, and Stepfunction can retry on failure. Only missing is what will invoke STF. Still I vote for A.","timestamp":"1707556020.0","upvote_count":"1"},{"timestamp":"1706696940.0","poster":"LazyAutonomy","upvote_count":"9","comments":[{"comment_id":"1140417","timestamp":"1707071820.0","upvote_count":"1","poster":"chelbsik","content":"Well, actually there is no requirement for archiving files either, so you just waste money on it in this case. And none of these solutions describes jobs rerun, including D, which means it works in all 4."}],"content":"Selected Answer: D\nA, B and C - there's no mechanism to retry failed jobs, so these options don't meet the mandatory requirement: \"The solution must also be reliable at enterprise scale and have the ability to rerun processing jobs in the event of failure.\"\n\nThere's no mandatory requirement to delete the files after 6 months.\n\nD - meets the mandatory requirements.","comment_id":"1136628"},{"timestamp":"1704931320.0","comment_id":"1119163","poster":"bjexamprep","comments":[{"poster":"Shmon3ymon3y","upvote_count":"1","comment_id":"1170013","timestamp":"1710038820.0","content":"S3 event notifications cannot directly target step functions. They can only invoke SNS, SQS, Lambda, or Eventbridge"}],"content":"Selected Answer: A\nBad question design.\n\nThe question doesn't mention storing the images after 6 months, so the images should be discarded after 6 months. So, C and D are out.\nEventBridge invokes Lambda, and if the Lambda fails, it won't start a new Lambda to rerun the process, cause the event is gone. So, B is out.\nA is missing sth as well. Step Function is not on the list of S3 event, so EventBridge should be here. And Step Function is too heavy comparing with SQS for this job. \nComparing all the answers, I will choose A, even it is not ideal.","upvote_count":"2"},{"timestamp":"1704907800.0","comment_id":"1118874","content":"Selected Answer: B\nOption B","poster":"career360guru","upvote_count":"1"},{"poster":"MegalodonBolado","content":"Selected Answer: B\nB is sufficient and most cost effective. \nEventBridge can retry using the RetryPolicy.","timestamp":"1704307560.0","upvote_count":"1","comment_id":"1113035"},{"comment_id":"1110318","upvote_count":"1","poster":"NOZOMI","timestamp":"1704004740.0","content":"https://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-how-to-event-types-and-destinations.html"},{"timestamp":"1702519920.0","comments":[{"upvote_count":"1","content":"You should re-read the answers, none of them suggest storing original images - they are all replacing them.","poster":"chelbsik","comment_id":"1140418","timestamp":"1707071880.0","comments":[{"upvote_count":"1","poster":"chelbsik","timestamp":"1707072000.0","comment_id":"1140419","content":"I was wrong, C says that explicitly, D describes it vaguely.\nThe point is - both C and D are wrong."}]}],"poster":"GaryQian","upvote_count":"5","content":"Selected Answer: D\nI will vote for D. The question mentioned backend service will resize the large images and store them in S3. It doesn't say S3 needs to store original iamges. Both A & B will save original iamges. D looks good for all requirements ( retry, 6months ,etc.. )","comment_id":"1095953"},{"upvote_count":"2","timestamp":"1701866580.0","comment_id":"1089339","content":"Selected Answer: A\nAnswer A. Step functions seem best fit than Eventbridge as it has the ability to retry failed steps.","poster":"shaaam80"},{"timestamp":"1701681720.0","comments":[],"upvote_count":"2","poster":"dutchy1988","content":"Allthough Eventbridge CAN retry jobs, it relies implicitly on a standard SQS resource. -> another resource -> more costs. So A seems the best fit.","comment_id":"1087482"},{"comment_id":"1085856","poster":"PAUGURU","timestamp":"1701501420.0","content":"Selected Answer: A\nC and D are out because there is no need to keep images after 6 months. Step function is the solution for retry, as stated on AWS description: \"AWS Step Functions automatically handles errors and exceptions with built-in try/catch and retry\". So answer is A for me.","upvote_count":"3"},{"upvote_count":"3","poster":"HunkyBunky","comment_id":"1085020","content":"Selected Answer: D\nOnly option D provides a ability for rerun processing jobs in the event of failure.\n\nB - don't provide a way for rerun job, EventBridge able to do retry only if the message are not sent to consumer, but it not able to rerun job in case of failure. All other options not provide rerun ability at all. I'm agree that option D is not ideal with DeepArchive, but only this option provide a way for rerun job in case of failure.","timestamp":"1701414780.0"},{"comment_id":"1083865","content":"Selected Answer: A\n\"have the ability to rerun processing jobs in the event of failure\"... How we can handle this requirement? I believe step functions triggered by the event in S3 through Eventbridge would do the trick for the retry mechanism. Also, only A and B matches the \"up to 6 months\" retention requirement","poster":"tfl","upvote_count":"3","timestamp":"1701300120.0"},{"upvote_count":"2","comments":[{"content":"Switching to A. AWS Step functions have a built in mechanism to retry failed steps","poster":"shaaam80","upvote_count":"1","comment_id":"1089336","timestamp":"1701866520.0"}],"timestamp":"1701277200.0","content":"Selected Answer: B\nAnswer B","comment_id":"1083656","poster":"shaaam80"},{"upvote_count":"1","poster":"devalenzuela86","comment_id":"1077567","content":"Selected Answer: C\nAnswer C","timestamp":"1700671800.0"},{"timestamp":"1700635860.0","upvote_count":"1","poster":"cypkir","content":"Selected Answer: C\nAnswer: C","comment_id":"1077032"}],"answer_ET":"D","question_text":"A company that provides image storage services wants to deploy a customer-facing solution to AWS. Millions of individual customers will use the solution. The solution will receive batches of large image files, resize the files, and store the files in an Amazon S3 bucket for up to 6 months.\n\nThe solution must handle significant variance in demand. The solution must also be reliable at enterprise scale and have the ability to rerun processing jobs in the event of failure.\n\nWhich solution will meet these requirements MOST cost-effectively?","answers_community":["D (43%)","B (37%)","A (18%)","2%"],"answer_images":[],"question_id":308,"unix_timestamp":1700635860,"exam_id":33,"isMC":true,"timestamp":"2023-11-22 07:51:00","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/126839-exam-aws-certified-solutions-architect-professional-sap-c02/"},{"id":"LvLQlKwGTjVXR0KH9Og0","answers_community":["C (90%)","10%"],"topic":"1","exam_id":33,"discussion":[{"poster":"heatblur","timestamp":"1700952060.0","comment_id":"1080381","upvote_count":"9","content":"Selected Answer: C\nC appears to be the most suitable solution. The combination of consolidated billing, a comprehensive tagging strategy using Tag Editor, and the purchase of Compute Savings Plans provides a balanced approach. This solution offers a centralized view and management of costs, ensures accurate cost allocation through tagging, and maintains flexibility in compute resource selection with the Compute Savings Plans. The Compute Savings Plans are particularly beneficial as they provide savings not only on EC2 instances but also on AWS Fargate and AWS Lambda, offering a broader range of applicability than EC2 Instance Savings Plans."},{"comment_id":"1410627","poster":"barracouto","content":"Selected Answer: C\nThe answers are annoying because billing is automatically consolidated with Organizations","timestamp":"1743029220.0","upvote_count":"1"},{"content":"c is right\nBy configuring AWS Organizations to use consolidated billing, implementing a tagging strategy, using Tag Editor to apply tags (although it's not strictly necessary), and purchasing Compute Savings Plans, you can meet the company's requirements of reducing compute costs, managing costs effectively, and improving visibility into billing for individual departments while maintaining operational flexibility.","poster":"AzureDP900","comment_id":"1313239","timestamp":"1731790380.0","upvote_count":"1"},{"content":"Selected Answer: C\nOption C.","comment_id":"1118879","poster":"career360guru","upvote_count":"2","timestamp":"1704908160.0"},{"upvote_count":"1","timestamp":"1704477660.0","content":"Answer: C\nOption A: Lacks consolidated billing, limiting cost visibility and potential discounts.\nOption B: SCPs are primarily for compliance enforcement, not tag application.\nOption D: Misses consolidated billing's benefits for cost visibility and management.","comment_id":"1114711","poster":"vibzr2023"},{"upvote_count":"2","poster":"shaaam80","timestamp":"1701277440.0","comment_id":"1083661","content":"Selected Answer: C\nAnswer C. Compute Savings plan. Tagging resources in each account using Tag editor & Consolidated Billing to view billing across the accounts."},{"comments":[{"timestamp":"1701069720.0","content":"Compute Savings Plans - cover Amazon EC2, AWS Lambda, and AWS Fargate usage = operational flexibility","comment_id":"1081274","poster":"HunkyBunky","upvote_count":"2"}],"upvote_count":"4","poster":"HunkyBunky","comment_id":"1081273","timestamp":"1701069600.0","content":"Selected Answer: C\nAnswer: C\nBecause for apply Tags to already created resources - you need to use Tag editor."},{"poster":"George88","timestamp":"1700959980.0","upvote_count":"4","comment_id":"1080409","content":"Answer: C\nCompute Savings Plans covers more resources than EC2 Instance Savings Plans.\nYou use Tag Editor to apply tags, not SCPs."},{"timestamp":"1700672040.0","upvote_count":"2","poster":"devalenzuela86","comment_id":"1077573","content":"Selected Answer: B\nB for sure"}],"unix_timestamp":1700672040,"answer_description":"","choices":{"A":"Use AWS Budgets for each department. Use Tag Editor to apply tags to appropriate resources. Purchase EC2 Instance Savings Plans.","D":"Use AWS Budgets for each department. Use SCPs to apply tags to appropriate resources. Purchase Compute Savings Plans.","B":"Configure AWS Organizations to use consolidated billing. Implement a tagging strategy that identifies departments. Use SCPs to apply tags to appropriate resources. Purchase EC2 Instance Savings Plans.","C":"Configure AWS Organizations to use consolidated billing. Implement a tagging strategy that identifies departments. Use Tag Editor to apply tags to appropriate resources. Purchase Compute Savings Plans."},"answer":"C","url":"https://www.examtopics.com/discussions/amazon/view/126935-exam-aws-certified-solutions-architect-professional-sap-c02/","timestamp":"2023-11-22 17:54:00","question_id":309,"question_text":"A company has an organization in AWS Organizations that includes a separate AWS account for each of the company’s departments. Application teams from different departments develop and deploy solutions independently.\n\nThe company wants to reduce compute costs and manage costs appropriately across departments. The company also wants to improve visibility into billing for individual departments. The company does not want to lose operational flexibility when the company selects compute resources.\n\nWhich solution will meet these requirements?","answer_images":[],"question_images":[],"isMC":true,"answer_ET":"C"},{"id":"td7JwlCjLCqnWKLsSxMz","discussion":[{"poster":"thala","upvote_count":"5","timestamp":"1700692140.0","content":"Selected Answer: C\nConsidering the primary concern of improving upload performance for large files while maintaining secure access for authenticated users, Option C (Enable S3 Transfer Acceleration and use it in the presigned URL) is the most suitable solution. It directly addresses the issue of slow uploads for large objects by leveraging CloudFront’s edge locations for accelerated data transfer to S3, and it works seamlessly with the existing mechanism of generating presigned URLs for authenticated users.","comment_id":"1077893"},{"upvote_count":"5","poster":"tmlong18","comment_id":"1123879","content":"Selected Answer: C\nA is wrong.\nThe limit of API Gateway payload is 10MB","timestamp":"1705382340.0"},{"upvote_count":"1","poster":"AzureDP900","timestamp":"1731790080.0","comment_id":"1313236","content":"C is right\nS3 Transfer Acceleration: This feature can significantly improve the performance of large file uploads by leveraging AWS' global network and caching capabilities.\n\nPresigned URL with S3 Transfer Acceleration: By including the S3 Transfer Acceleration endpoint in the presigned URL, you can take advantage of the acceleration to reduce upload times.\n\nS3 multipart upload API: Using the S3 multipart upload API allows for resumable uploads, which is essential for large files that may be interrupted during transfer."},{"timestamp":"1723518000.0","comment_id":"1264941","content":"Selected Answer: C\nC\n\"Most users are reporting slow upload times for objects larger than 100 MB.\"\nStraight to S3 Transfer Acceleration.","upvote_count":"2","poster":"kgpoj"},{"poster":"nharaz","comments":[{"poster":"nharaz","upvote_count":"1","comment_id":"1139660","timestamp":"1707002460.0","content":"Sorry I mean = C"}],"upvote_count":"1","content":"Selected Answer: D\nPresigned URLs still ensure that only authenticated users can upload content, as the generation of a presigned URL requires valid AWS credentials. The URL is temporary and grants the bearer permission to perform the action defined in the URL, in this case, a PUT operation to upload an object","timestamp":"1707002400.0","comment_id":"1139659"},{"timestamp":"1704908700.0","upvote_count":"1","poster":"career360guru","comment_id":"1118892","content":"Selected Answer: C\nOption C"},{"poster":"MegalodonBolado","upvote_count":"5","content":"Selected Answer: C\nhttps://aws.amazon.com/blogs/compute/uploading-large-objects-to-amazon-s3-using-multipart-upload-and-transfer-acceleration/\n\n(C)","comment_id":"1113669","timestamp":"1704371280.0"},{"upvote_count":"1","content":"C has the most votes currently. How does C ensure that only authenticated users are allowed to post content?","poster":"carpa_jo","comment_id":"1110618","timestamp":"1704031200.0","comments":[{"poster":"MegalodonBolado","timestamp":"1704371580.0","content":"S3TA supports presigned URL. The only problem the architect mest solve is the slow upload. Multipart upload can overcome TCP speed limitations and S3TA reduces latency.\n\nSee the link in my vote","upvote_count":"2","comment_id":"1113672"}]},{"timestamp":"1703267460.0","poster":"yuliaqwerty","comment_id":"1103595","upvote_count":"1","content":"C is the easiest"},{"upvote_count":"4","poster":"ayadmawla","comment_id":"1092677","timestamp":"1702228320.0","content":"Selected Answer: A\nAnswer is A to secure the API. \nhttps://aws.amazon.com/blogs/compute/uploading-to-amazon-s3-directly-from-a-web-or-mobile-application/#:~:text=Adding%20authentication%20to%20the%20upload%20process&text=You%20can%20restrict%20access%20to,as%20Amazon%20Cognito%20or%20Auth0."},{"comment_id":"1089346","poster":"shaaam80","content":"Selected Answer: C\nAnswer C","timestamp":"1701867240.0","upvote_count":"1"},{"content":"Selected Answer: C\nAnswer: C","comment_id":"1077033","timestamp":"1700635980.0","poster":"cypkir","upvote_count":"2"}],"exam_id":33,"answers_community":["C (81%)","Other"],"url":"https://www.examtopics.com/discussions/amazon/view/126840-exam-aws-certified-solutions-architect-professional-sap-c02/","isMC":true,"question_id":310,"timestamp":"2023-11-22 07:53:00","unix_timestamp":1700635980,"answer_description":"","answer_ET":"C","answer":"C","topic":"1","question_text":"A company has a web application that securely uploads pictures and videos to an Amazon S3 bucket. The company requires that only authenticated users are allowed to post content. The application generates a presigned URL that is used to upload objects through a browser interface. Most users are reporting slow upload times for objects larger than 100 MB.\n\nWhat can a solutions architect do to improve the performance of these uploads while ensuring only authenticated users are allowed to post content?","answer_images":[],"question_images":[],"choices":{"A":"Set up an Amazon API Gateway with an edge-optimized API endpoint that has a resource as an S3 service proxy. Configure the PUT method for this resource to expose the S3 PutObject operation. Secure the API Gateway using a COGNITO_USER_POOLS authorizer. Have the browser interface use API Gateway instead of the presigned URL to upload objects.","C":"Enable an S3 Transfer Acceleration endpoint on the S3 bucket. Use the endpoint when generating the presigned URL. Have the browser interface upload the objects to this URL using the S3 multipart upload API.","B":"Set up an Amazon API Gateway with a regional API endpoint that has a resource as an S3 service proxy. Configure the PUT method for this resource to expose the S3 PutObject operation. Secure the API Gateway using an AWS Lambda authorizer. Have the browser interface use API Gateway instead of the presigned URL to upload objects.","D":"Configure an Amazon CloudFront distribution for the destination S3 bucket. Enable PUT and POST methods for the CloudFront cache behavior. Update the CloudFront origin to use an origin access identity (OAI). Give the OAI user 3: PutObject permissions in the bucket policy. Have the browser interface upload objects using the CloudFront distribution."}}],"exam":{"name":"AWS Certified Solutions Architect - Professional SAP-C02","provider":"Amazon","id":33,"isImplemented":true,"isMCOnly":true,"numberOfQuestions":529,"isBeta":false,"lastUpdated":"11 Apr 2025"},"currentPage":62},"__N_SSP":true}