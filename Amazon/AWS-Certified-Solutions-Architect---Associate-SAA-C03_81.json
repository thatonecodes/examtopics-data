{"pageProps":{"questions":[{"id":"vqpTUCuN95j4LmyeoLx1","answer":"C","question_text":"A company hosts a multi-tier web application on Amazon Linux Amazon EC2 instances behind an Application Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The company observes that the Auto Scaling group launches more On-Demand Instances when the application's end users access high volumes of static web content. The company wants to optimize cost.\n\nWhat should a solutions architect do to redesign the application MOST cost-effectively?","answer_description":"","topic":"1","question_images":[],"timestamp":"2023-05-16 18:58:00","answer_images":[],"unix_timestamp":1684256280,"exam_id":31,"answers_community":["C (100%)"],"answer_ET":"C","choices":{"D":"Create an AWS Lambda function behind an Amazon API Gateway API to host the static website contents.","A":"Update the Auto Scaling group to use Reserved Instances instead of On-Demand Instances.","B":"Update the Auto Scaling group to scale by launching Spot Instances instead of On-Demand Instances.","C":"Create an Amazon CloudFront distribution to host the static web contents from an Amazon S3 bucket."},"url":"https://www.examtopics.com/discussions/amazon/view/109423-exam-aws-certified-solutions-architect-associate-saa-c03/","discussion":[{"upvote_count":"4","comment_id":"1115889","content":"Selected Answer: C\nC: Cost effective static content scaling = CloudFront\nA and B scale instances so not the best use of money for static content\nD Probably most expensive way of service static content at scale as you'll be charged for Lambda execution also","timestamp":"1720354020.0","poster":"awsgeek75"},{"comment_id":"1115875","poster":"mwwt2022","upvote_count":"3","content":"Selected Answer: C\nstatic content -> CloudFront","timestamp":"1720352820.0"},{"upvote_count":"4","timestamp":"1708866900.0","content":"Selected Answer: C\nimplementing CloudFront to serve static content is the most cost-optimal architectural change for this use case.","comment_id":"989985","poster":"Guru4Cloud"},{"timestamp":"1705563540.0","content":"Selected Answer: C\nKeyword \"Amazon CloudFront\", \"high volumes of static web content\", choose C.","comment_id":"954987","upvote_count":"3","poster":"james2033"},{"timestamp":"1702533300.0","comment_id":"922732","content":"Selected Answer: C\nstatic web content = Amazon CloudFront","poster":"TariqKipkemei","upvote_count":"2"},{"content":"Selected Answer: C\nStatic Web Content = S3 Always.\nCloudFront = Closer to the users locations since it will cache in the Edge nodes.","timestamp":"1701719880.0","comment_id":"914854","upvote_count":"3","poster":"alexandercamachop"},{"content":"By leveraging Amazon CloudFront, you can cache and serve the static web content from edge locations worldwide, reducing the load on your EC2 instances. This can help lower the number of On-Demand Instances required to handle high volumes of static web content requests. Storing the static content in an Amazon S3 bucket and using CloudFront as a content delivery network (CDN) improves performance and reduces costs by reducing the load on your EC2 instances.","upvote_count":"4","comment_id":"901189","poster":"cloudenthusiast","timestamp":"1700316780.0"},{"upvote_count":"3","timestamp":"1700202540.0","comment_id":"899733","poster":"Efren","content":"Selected Answer: C\nStatic content, cloudFront plus S3"},{"comment_id":"899377","content":"Selected Answer: C\nc for me","poster":"nosense","upvote_count":"2","timestamp":"1700161080.0"}],"isMC":true,"question_id":401},{"id":"xuXcGc1Q5iidoc9lRmoc","isMC":true,"answer_ET":"D","topic":"1","question_images":[],"answer_images":[],"unix_timestamp":1684412400,"discussion":[{"upvote_count":"18","comment_id":"901192","poster":"cloudenthusiast","timestamp":"1700317200.0","content":"Selected Answer: D\nBy utilizing Lake Formation's tag-based access control, you can define tags and tag-based policies to grant selective access to the required data for the engineering team accounts. This approach allows you to control access at a granular level without the need to copy or move the data to a common account or manage permissions individually in each account. It provides a centralized and scalable solution for securely sharing data across accounts with minimal operational overhead."},{"content":"Selected Answer: D\n(B) uses the CLI command that has many options: principal, TableName, ColumnNames, LFTag etc providing a way to manage granular access permissions for different users at the table and column level. That way you don't give full access to the all the data. The problem with (B) is to implement this in each account has a lot more operational overhead than (D).","poster":"NSA_Poker","upvote_count":"2","timestamp":"1733088960.0","comment_id":"1222871"},{"poster":"awsgeek75","upvote_count":"4","timestamp":"1720354440.0","content":"Selected Answer: D\nD: Selective data = tagging \nA and B gives full access to all the data\nC is possible but with complex operational overhead as you have to publish your data to the Data Exchange. (this is based on my limited knowledge so happy to be corrected)","comment_id":"1115898"},{"comment_id":"989982","timestamp":"1708866540.0","content":"Selected Answer: D\nD is the correct option with the least operational overhead.\n\nUsing Lake Formation tag-based access control allows granting cross-account permissions to access data in other accounts based on tags, without having to copy data or configure individual permissions in each account.\n\nThis provides a centralized, tag-based way to share selective data across accounts to authorized users with least operational overhead.","upvote_count":"4","poster":"Guru4Cloud"},{"upvote_count":"4","content":"Selected Answer: D\nhttps://aws.amazon.com/blogs/big-data/securely-share-your-data-across-aws-accounts-using-aws-lake-formation/","comment_id":"904320","poster":"luisgu","timestamp":"1700686860.0"}],"url":"https://www.examtopics.com/discussions/amazon/view/109647-exam-aws-certified-solutions-architect-associate-saa-c03/","exam_id":31,"answer":"D","choices":{"B":"Use the Lake Formation permissions Grant command in each account where the data is stored to allow the required engineering team users to access the data.","D":"Use Lake Formation tag-based access control to authorize and grant cross-account permissions for the required data to the engineering team accounts.","A":"Copy the required data to a common account. Create an IAM access role in that account. Grant access by specifying a permission policy that includes users from the engineering team accounts as trusted entities.","C":"Use AWS Data Exchange to privately publish the required data to the required engineering team accounts."},"question_id":402,"timestamp":"2023-05-18 14:20:00","answer_description":"","answers_community":["D (100%)"],"question_text":"A company stores several petabytes of data across multiple AWS accounts. The company uses AWS Lake Formation to manage its data lake. The company's data science team wants to securely share selective data from its accounts with the company's engineering team for analytical purposes.\n\nWhich solution will meet these requirements with the LEAST operational overhead?"},{"id":"i8O4z0rkwsRkcJlep5gp","answer_images":[],"timestamp":"2023-05-16 19:01:00","discussion":[{"content":"Selected Answer: A\nThe question asks for \"a cost-effective solution [ONLY TO] to minimize upload and download latency and maximize performance\", not for the actual application. And the 'cost-effective solution to minimize upload and download latency and maximize performance' is S3 Transfer Acceleration. Obviously there is more required to host the app, but that is not asked for.","poster":"pentium75","timestamp":"1704029640.0","comment_id":"1110590","upvote_count":"14"},{"content":"Selected Answer: A\nThe question is focused on large downloads and uploads. S3 Transfer Acceleration is what fits. CloudFront is for caching which cannot be used when the data is unique. They aren't as concerned with regular web traffic. \n\nAmazon S3 Transfer Acceleration can speed up content transfers to and from Amazon S3 by as much as 50-500% for long-distance transfer of larger objects.","upvote_count":"7","poster":"chris0975","comment_id":"1051129","timestamp":"1698010320.0"},{"comments":[{"upvote_count":"1","comment_id":"1343052","timestamp":"1737299520.0","content":"I agree with your point !!","poster":"vincent2023"}],"comment_id":"1303467","content":"Selected Answer: C\nS3 can't \"host an application\" , so even tho S3 Transfer acceleration would optimize the performances of the download/upload , still it doesn't host the app. the question is poorly written i guess","upvote_count":"1","timestamp":"1730000160.0","poster":"Ben_88"},{"upvote_count":"3","comment_id":"1270023","content":"Selected Answer: A\nA for sure","timestamp":"1724236080.0","poster":"ChymKuBoy"},{"content":"Selected Answer: A\nNot C, D No requirements to scale the application itself so EC2 is not applicable. \nB is for caching so not sure how/if that helps the upload speed for global users\nA is correct as Transfer Accelerator is best for uploading and downloading unique items near the user's region/location","comment_id":"1115902","upvote_count":"6","poster":"awsgeek75","timestamp":"1704637080.0"},{"upvote_count":"3","content":"Selected Answer: A\nfor datas greater tham 1 GB, s3 transfer acceleration is the best","poster":"tosuccess","timestamp":"1704340740.0","comment_id":"1113325"},{"comment_id":"1100192","timestamp":"1702950420.0","upvote_count":"5","content":"Selected Answer: A\nApplication users will be able to download and upload UNIQUE data up to gigabytes in size\n\nThus all caching related solution dont work.","poster":"Cyberkayu"},{"content":"Selected Answer: A\nDownloading data upto gigabytes in size - Cloudfront is a content delivery service that acts as an edge caching layer for images and other data. Not a service that minimizes upload and download latency.","timestamp":"1700273280.0","poster":"Goutham4981","comment_id":"1073764","upvote_count":"2"},{"timestamp":"1699209480.0","comments":[{"upvote_count":"2","comment_id":"1110588","content":"A doesn't mention EC2 or EKS or ECS or Elastic Beanstalk or Lambda. Where does the \"scalable web application\" run?","timestamp":"1704029460.0","comments":[{"poster":"JA2018","comment_id":"1316328","timestamp":"1732283700.0","upvote_count":"1","content":"hmm... could be a red herring?"}],"poster":"pentium75"}],"content":"Selected Answer: A\nThe question is focused on large downloads and uploads. S3 Transfer Acceleration is what fits. CloudFront is for caching which cannot be used when the data is unique. They aren't as concerned with regular web traffic.\n\nC didn't mention S3. Where the data is stored?","upvote_count":"4","comment_id":"1063176","poster":"potomac"},{"upvote_count":"2","poster":"beast2091","timestamp":"1698903180.0","comment_id":"1060284","content":"It is A.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/transfer-acceleration.html"},{"content":"It is A as the Transfer Acceleration will minimize upload and download latency.\nIf you choose C, where would the files be stored? There is no mention of any S3. Will it be stored inside the EC2? That's why I didn't go for C","poster":"danielmakita","upvote_count":"5","comment_id":"1056388","timestamp":"1698519480.0"},{"upvote_count":"4","comment_id":"1054452","content":"Selected Answer: C\nAmazon S3 with Transfer Acceleration (option A) is designed for speeding up uploads to Amazon S3, and it's not used for hosting scalable web applications. It doesn't mention using EC2 instances for hosting the application.","timestamp":"1698315420.0","poster":"Sindokuhlep"},{"content":"Selected Answer: C\nMy answer is C","poster":"canonlycontainletters1","comment_id":"1054303","upvote_count":"1","timestamp":"1698298800.0"},{"timestamp":"1697814420.0","comment_id":"1048911","poster":"thanhnv142","upvote_count":"1","content":"C because A is for upload data to S3, not for web app"},{"comment_id":"1040569","content":"Selected Answer: C\nThe correct answer is C!!! It is not A, because \n- Amazon S3 with Transfer Acceleration (option A) is designed for speeding up uploads to Amazon S3, and it's not used for hosting scalable web applications. It doesn't mention using EC2 instances for hosting the application.","timestamp":"1697023500.0","poster":"DamyanG","upvote_count":"3"},{"poster":"Victory007","upvote_count":"1","content":"Selected Answer: C\nAmazon CloudFront is a global content delivery network (CDN) that delivers web content to users with low latency and high transfer speeds. It does this by caching content at edge locations around the world, which are closer to the users than the origin server.\nBy using Amazon EC2 with Auto Scaling and Amazon CloudFront, the company can create a scalable and high-performance web application that is accessible to users from different geographic regions of the world.","comment_id":"1028411","timestamp":"1696817340.0"},{"timestamp":"1696429440.0","content":"Selected Answer: A\nI believe it would be A - my thinking maybe wrong but im just thinking specifically about the S3 put allows upto 5gb not sure about cloudfront. Second way of thinking is that cached content on edge locations but would it not have to go to source still to retrieve if another person wants to download that content in a different part of the world?","comment_id":"1024894","upvote_count":"3","poster":"Ramdi1"},{"content":"C,\n1. Cloudfront cache data at edge, which provide better performance for read. Global Accelerator will always goto origin for content. \n2. Cloudfront can also help performance for dynamic content, which is good for Web app","upvote_count":"1","comment_id":"1021601","timestamp":"1696086600.0","poster":"bsbs1234"},{"timestamp":"1695713640.0","poster":"Ramdi1","upvote_count":"2","comment_id":"1017520","content":"Selected Answer: C\nI think C is correct the question mentions geographic locations and cloudfront had 500 + edge locations. Gigabytes in size - s3 has a max limit of a 5gb put - even though the question does not say 5gb or less just something to think about and s3 cant hold dynamic content"},{"poster":"garuta","upvote_count":"2","timestamp":"1695685920.0","content":"Selected Answer: A\nS3TA shortens the distance between client applications and AWS servers that acknowledge PUTS and GETS to Amazon S3 using our global network of hundreds of CloudFront Edge Locations. We automatically route your uploads and downloads through the closest Edge Locations to your application.","comment_id":"1017261"},{"content":"Selected Answer: C\nC is correct","comment_id":"1012459","poster":"nnecode","timestamp":"1695223860.0","upvote_count":"1"},{"content":"Selected Answer: C\nI think C is correct as it provides caching at edge which minimizes latency","comment_id":"1009008","poster":"CHOTADON","upvote_count":"1","timestamp":"1694851980.0"},{"timestamp":"1693360740.0","comment_id":"993641","upvote_count":"1","content":"Selected Answer: C\nShould be C, I will never host a \"scalable application\" using S3. They might be fast in data transfer but that is not the whole point","poster":"Hades2231"},{"timestamp":"1693117020.0","poster":"junsu123","comment_id":"991252","content":"Selected Answer: C\nIt's my first time writing a comment, but I think C is the answer here.\nUsing Amazon S3 with Transfer Acceleration can help speed up data transfer, but it may not be the best solution for hosting web applications. S3 is primarily an object storage service, and dynamic processing to host web applications can be limited.","upvote_count":"2"},{"timestamp":"1692911880.0","comments":[{"timestamp":"1692961500.0","content":"Final Ans: C\nRevisiting the question and answering.\n° Amazon CloudFront is a content delivery network (CDN) that allows caching content at edge locations closer to users. This minimizes latency for download and upload.\n\n° This means that the content will be served from servers that are closer to the user, which will reduce the amount of time it takes for the content to be delivered. Distributing content to multiple servers, which can help to handle spikes in traffic","comment_id":"989976","poster":"Guru4Cloud","upvote_count":"4"}],"comment_id":"989501","upvote_count":"2","content":"Selected Answer: A\nUse Amazon S3 with Transfer Acceleration to host the application.","poster":"Guru4Cloud"},{"comment_id":"984445","upvote_count":"1","poster":"hachiri","timestamp":"1692359280.0","content":"Selected Answer: C\n** data up to gigabytes in size **\n\nQ: How should I choose between S3 Transfer Acceleration and Amazon CloudFront’s PUT/POST?\n\nS3 Transfer Acceleration optimizes the TCP protocol and adds additional intelligence between the client and the S3 bucket, making S3 Transfer Acceleration a better choice if a higher throughput is desired. If you have objects that are smaller than 1 GB or if the data set is less than 1 GB in size, you should consider using Amazon CloudFront's PUT/POST commands for optimal performance.\n\nhttps://aws.amazon.com/s3/faqs/?nc1=h_ls"},{"timestamp":"1691409180.0","poster":"ersin13","upvote_count":"2","content":"You have to be aware of Application users will be able to download and upload unique data up to gigabytes in size.You can not use cloudfront uniqe data so answer is A","comment_id":"974656"},{"content":"Selected Answer: C\nThe question is vague. A is good for a static website and C is good for a dynamic one. I go with C.","comment_id":"967335","upvote_count":"1","poster":"jayce5","timestamp":"1690737600.0"},{"content":"Autoscaling group can not use as origin of cloud front so A","comment_id":"966716","upvote_count":"3","poster":"Kp88","timestamp":"1690674300.0"},{"upvote_count":"1","poster":"live_reply_developers","content":"Selected Answer: C\nOption A is not appropriate because Amazon S3 with Transfer Acceleration helps in faster transfers of files over long distances between the client and the bucket, but it's not designed for hosting web applications.","timestamp":"1690198440.0","comment_id":"961535"},{"content":"Selected Answer: A\nQuote \"S3TA shortens the distance between client applications and AWS servers that acknowledge PUTS and GETS to Amazon S3 using our global network of hundreds of CloudFront Edge Locations.\" at https://aws.amazon.com/s3/transfer-acceleration/","poster":"james2033","comment_id":"954991","upvote_count":"3","timestamp":"1689659220.0"},{"content":"Selected Answer: C\nPretty tricky question:\nA seems right for the up and download: however, first sentence mentions: \"hosting a web application on AWS\" -> S3 is alright for static content, but for the web app we should prefer a compute service like EC2.","timestamp":"1687797060.0","comment_id":"934628","poster":"Zuit","upvote_count":"2"},{"upvote_count":"2","comment_id":"922738","timestamp":"1686716280.0","comments":[{"comment_id":"1058459","content":"The question talks about reducing latencies on uploads and downloads. Amazon S3 Transfer Acceleration can speed up content transfers to and from Amazon S3 by as much as 50-500% for long-distance transfer of larger objects. Customers who have either web or mobile applications with widespread users or applications hosted far away from their S3 bucket can experience long and variable upload and download speeds over the Internet. S3 Transfer Acceleration (S3TA) reduces the variability in Internet routing, congestion and speeds that can affect transfers, and logically shortens the distance to S3 for remote applications. S3TA improves transfer performance by routing traffic through Amazon CloudFront’s globally distributed Edge Locations and over AWS backbone networks, and by using network protocol optimizations. \n\nBut the answer is not framed correctly as the application still needs to be hosted on s3 and not S3TA.","poster":"TariqKipkemei","upvote_count":"3","timestamp":"1698729060.0"}],"poster":"TariqKipkemei","content":"Selected Answer: A\nA fits this scenario"},{"content":"Selected Answer: A\nAmazon S3 (Simple Storage Service) is a highly scalable object storage service provided by AWS. It allows you to store and retrieve any amount of data from anywhere on the web. With Amazon S3, you can host static websites, store and deliver large media files, and manage data for backup and restore.\n\nTransfer Acceleration is a feature of Amazon S3 that utilizes the AWS global infrastructure to accelerate file transfers to and from Amazon S3. It uses optimized network paths and parallelization techniques to speed up data transfer, especially for large files and over long distances.\n\nBy using Amazon S3 with Transfer Acceleration, the web application can benefit from faster upload and download speeds, reducing latency and improving overall performance for users in different geographic regions. This solution is cost-effective as it leverages the existing Amazon S3 infrastructure and eliminates the need for additional compute resources.","timestamp":"1685901720.0","upvote_count":"2","poster":"alexandercamachop","comment_id":"914856"},{"comment_id":"914199","content":"How on earth is it C? \n\nTransfer Acceleration is for optimizing file transfers to and from Amazon S3, whereas Amazon CloudFront is bringing content closer to the end user. \n\nI feel good knowing that most of the people here are new to AWS.","poster":"Abrar2022","upvote_count":"2","timestamp":"1685859840.0"},{"comment_id":"909723","content":"CCCCCCCCCCC\nUse Amazon EC2 with Auto Scaling and Amazon CloudFront to host the application","upvote_count":"2","poster":"omoakin","timestamp":"1685394540.0"},{"upvote_count":"2","timestamp":"1684591440.0","poster":"EA100","content":"Answer - C\nC. Use Amazon EC2 with Auto Scaling and Amazon CloudFront to host the application.\n\nUsing Amazon EC2 with Auto Scaling allows for scalability and the ability to handle varying levels of demand for the web application. Auto Scaling ensures that the appropriate number of EC2 instances are provisioned based on the workload, enabling efficient resource utilization and cost optimization.\n\nAmazon CloudFront can be used as a content delivery network (CDN) to cache and deliver static and dynamic content closer to the end users, reducing latency and improving performance. By leveraging CloudFront, the web application can benefit from faster content delivery to users in different geographic regions.\n\nSo, option C is the correct choice in this situation to minimize latency, maximize performance, and achieve cost-effectiveness.","comment_id":"902615"},{"timestamp":"1684542180.0","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/ja_jp/AmazonS3/latest/userguide/transfer-acceleration.html","comment_id":"902286","poster":"hiroohiroo","upvote_count":"3","comments":[{"content":"Use Amazon S3 with Transfer Acceleration is not the best choice because Amazon S3 is primarily a storage service and may not be optimized for hosting dynamic web applications.","upvote_count":"1","comment_id":"910134","poster":"karbob","timestamp":"1685441100.0"}]},{"poster":"cloudenthusiast","upvote_count":"3","comments":[{"content":"Transfer Acceleration is focused on optimizing file transfers to and from Amazon S3, whereas Auto Scaling with Amazon CloudFront is a more suitable combination for hosting a scalable web application with global accessibility.","comment_id":"910142","upvote_count":"2","poster":"karbob","timestamp":"1685441760.0"}],"comment_id":"901196","timestamp":"1684412820.0","content":"Since S3 Transfer Acceleration is leveraging CloudFront's global network of edge location so C is not needed."},{"timestamp":"1684297860.0","upvote_count":"3","comment_id":"899735","poster":"Efren","content":"Selected Answer: A\nS3 Transfer acceleration is precisely for this. agreed with nosense"},{"timestamp":"1684256460.0","poster":"nosense","comment_id":"899383","upvote_count":"3","content":"Selected Answer: A\ni WILL Go with A."}],"question_text":"A company wants to host a scalable web application on AWS. The application will be accessed by users from different geographic regions of the world. Application users will be able to download and upload unique data up to gigabytes in size. The development team wants a cost-effective solution to minimize upload and download latency and maximize performance.\n\nWhat should a solutions architect do to accomplish this?","unix_timestamp":1684256460,"choices":{"C":"Use Amazon EC2 with Auto Scaling and Amazon CloudFront to host the application.","B":"Use Amazon S3 with CacheControl headers to host the application.","D":"Use Amazon EC2 with Auto Scaling and Amazon ElastiCache to host the application.","A":"Use Amazon S3 with Transfer Acceleration to host the application."},"answers_community":["A (75%)","C (25%)"],"answer":"A","answer_ET":"A","topic":"1","isMC":true,"question_id":403,"url":"https://www.examtopics.com/discussions/amazon/view/109424-exam-aws-certified-solutions-architect-associate-saa-c03/","exam_id":31,"question_images":[],"answer_description":""},{"id":"45olhDdlSEQkbmL5NEWz","question_text":"A company has hired a solutions architect to design a reliable architecture for its application. The application consists of one Amazon RDS DB instance and two manually provisioned Amazon EC2 instances that run web servers. The EC2 instances are located in a single Availability Zone.\n\nAn employee recently deleted the DB instance, and the application was unavailable for 24 hours as a result. The company is concerned with the overall reliability of its environment.\n\nWhat should the solutions architect do to maximize reliability of the application's infrastructure?","url":"https://www.examtopics.com/discussions/amazon/view/109426-exam-aws-certified-solutions-architect-associate-saa-c03/","topic":"1","answer_images":[],"answer_description":"","unix_timestamp":1684256580,"exam_id":31,"choices":{"C":"Create an additional DB instance along with an Amazon API Gateway and an AWS Lambda function. Configure the application to invoke the Lambda function through API Gateway. Have the Lambda function write the data to the two DB instances.","D":"Place the EC2 instances in an EC2 Auto Scaling group that has multiple subnets located in multiple Availability Zones. Use Spot Instances instead of On-Demand Instances. Set up Amazon CloudWatch alarms to monitor the health of the instances Update the DB instance to be Multi-AZ, and enable deletion protection.","B":"Update the DB instance to be Multi-AZ, and enable deletion protection. Place the EC2 instances behind an Application Load Balancer, and run them in an EC2 Auto Scaling group across multiple Availability Zones.","A":"Delete one EC2 instance and enable termination protection on the other EC2 instance. Update the DB instance to be Multi-AZ, and enable deletion protection."},"answer_ET":"B","discussion":[{"timestamp":"1720354860.0","comment_id":"1115906","poster":"awsgeek75","content":"Option E: Sack the employee who did this :)","upvote_count":"10"},{"content":"Selected Answer: B\nB is correct. HA ensured by DB in Mutli-AZ and EC2 in AG","comment_id":"899386","upvote_count":"5","poster":"nosense","timestamp":"1700161380.0"},{"comment_id":"1196845","timestamp":"1729116360.0","content":"Selected Answer: B\nA: delete one instance, why?. Although takes care of reliability of DB instance however not EC2.\nB. seems perfect as takes care of reliability of both EC2 as well as DB\nC. DB instance's reliability is not taken care of\nD. seems to be trying to address cost alongside reliability of EC2 and DB.","upvote_count":"3","poster":"wizcloudifa"},{"timestamp":"1720355160.0","content":"Selected Answer: B\nA: Deleting one EC2 instance makes no sense. Why would you do that?\nC: API Gateway, Lambda etc are all nice but they don't solve the problem of DB instance deletion\nD: EC2 subnet blah blah, what? The problem is reliability, not networking!\n\nB is correct as it solves the DB deletion issue and increases reliability by Multi AZ scaling of EC2 instances","upvote_count":"5","poster":"awsgeek75","comment_id":"1115909"},{"poster":"Guru4Cloud","comment_id":"989464","content":"Selected Answer: B\nThe key points:\n ° RDS Multi-AZ and deletion protection provide high availability for the database.\n ° The load balancer and Auto Scaling group across AZs give high availability for EC2.\n ° Options A, C, D have limitations that would reduce reliability vs option B.","timestamp":"1708812000.0","upvote_count":"3"},{"poster":"TariqKipkemei","timestamp":"1702534800.0","comment_id":"922739","upvote_count":"2","content":"Selected Answer: B\nUpdate the DB instance to be Multi-AZ, and enable deletion protection. Place the EC2 instances behind an Application Load Balancer, and run them in an EC2 Auto Scaling group across multiple Availability Zones"},{"poster":"antropaws","content":"Selected Answer: B\nB for sure.","upvote_count":"2","timestamp":"1701769260.0","comment_id":"915183"},{"poster":"alexandercamachop","timestamp":"1701720300.0","upvote_count":"2","content":"Selected Answer: B\nIt is the only one with High Availability. \nAmazon RDS with Multi AZ\nEC2 with Auto Scaling Group in Multi Az","comment_id":"914857"},{"content":"same question from \nhttps://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-associate-saa-c02/\nlong time ago and still same option B","upvote_count":"3","poster":"omoakin","comment_id":"901482","timestamp":"1700346000.0"}],"answer":"B","answers_community":["B (100%)"],"question_id":404,"question_images":[],"isMC":true,"timestamp":"2023-05-16 19:03:00"},{"id":"BLaPeKQV9zIMWrhvrY9W","answer_ET":"A","answers_community":["A (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/109403-exam-aws-certified-solutions-architect-associate-saa-c03/","choices":{"D":"Back up the data on tapes. Ship the tapes to an AWS data center. Mount a target Amazon S3 bucket on the on-premises file system.","B":"Back up the data to AWS Snowball Edge Storage Optimized devices. Ship the devices to an AWS data center. Mount a target Amazon S3 bucket on the on-premises file system.","A":"Create an AWS DataSync agent in the corporate data center. Create a data transfer task Start the transfer to an Amazon S3 bucket.","C":"Use rsync to copy the data directly from local storage to a designated Amazon S3 bucket over the Direct Connect connection."},"question_text":"A company is storing 700 terabytes of data on a large network-attached storage (NAS) system in its corporate data center. The company has a hybrid environment with a 10 Gbps AWS Direct Connect connection.\n\nAfter an audit from a regulator, the company has 90 days to move the data to the cloud. The company needs to move the data efficiently and without disruption. The company still needs to be able to access and update the data during the transfer window.\n\nWhich solution will meet these requirements?","answer_images":[],"isMC":true,"unix_timestamp":1684239480,"timestamp":"2023-05-16 14:18:00","topic":"1","exam_id":31,"discussion":[{"poster":"wRhlH","comments":[{"comment_id":"1179121","poster":"Maru86","timestamp":"1711017240.0","upvote_count":"4","content":"The question explicitly mentioned \"devices\", also Snowball Edge Storage Optimized is 80TB HDD. So it is possible, but the answer is A because we can transfer with DataSync in 6.5 days."},{"timestamp":"1687679820.0","upvote_count":"1","comment_id":"933364","comments":[{"timestamp":"1689830220.0","comment_id":"957142","content":"This account is wrong but I get your point. It is wrong cause 10Gb/s is not the same as 10GB/s (Gigabits vs Gigabytes). However, the correct count is 864Tb/8 = 108TB per day. In one week you should've transferred all the data.","poster":"siGma182","comments":[{"content":"folks, you had to consider the typical TCP overheads for such over-the-wire data transfers.... it could be as high as 70% of the total available bandwidth due to a wide variety of factors","comment_id":"1316335","poster":"JA2018","upvote_count":"1","timestamp":"1732284060.0"},{"upvote_count":"2","comment_id":"1179112","poster":"Maru86","content":"That's right, 1 GB = 8 Gb. Essentially we have a speed of 1.25GB/s.","timestamp":"1711016640.0"}],"upvote_count":"8"}],"poster":"smartegnine","content":"10GBs * 24*60*60 =864,000 GB estimate around 864 TB a day, 2 days will transfer all data. But for snowball at least 4 days for delivery to the data center."}],"timestamp":"1686558300.0","upvote_count":"11","comment_id":"921248","content":"For those who wonders why not B. Snowball Edge Storage Optimized device for data transfer is up to 100TB\nhttps://docs.aws.amazon.com/snowball/latest/developer-guide/device-differences.html"},{"poster":"hsinchang","comment_id":"962719","content":"Selected Answer: A\nAccess during the transfer window -> DataSync","timestamp":"1690288860.0","upvote_count":"9"},{"timestamp":"1738579620.0","comment_id":"1350831","content":"Selected Answer: A\n700TB with a Bandwidth of 10Gbps takes approximately 7 days, 3 Hours, 2 Minutes, 7 Seconds\nAs per the below link>>\nhttps://expedient.com/knowledgebase/tools-and-calculators/file-transfer-time-calculator/\nSo Option A is Correct & using AWS Datasync facilitates further access to data still present in the OnPrem to access and S3 provides with unlimited data storage capacity","poster":"surajkrishnamurthy","upvote_count":"1"},{"timestamp":"1731017040.0","comment_id":"1308565","content":"A- Porque La compañía todavía necesita poder acceder y actualizar los datos durante la ventana de transferencia\nB-Aunque esta podria ser una opcion porque dice de enviar los dispositivosal al centro de datos que serian mas o menos entre 9 y 10 dispositivos ya que el almacenamiento de storage optimized es de 80 TB el problema esta en que con Snowball Edge Storage Optimized no se puede acceder y actuializar a los datos durante la transferencia","poster":"Danilus","upvote_count":"1","comments":[{"content":"From Google Translate:\n\nA- Because The company still needs to be able to access and update the data during the transfer window\n\nB-Although this could be an option because it says that sending the devices to the data center would be more or less between 9 and 10 devices since the storage optimized storage is 80 TB, the problem is that with Snowball Edge Storage Optimized it does not Data can be accessed and updated during transfer","poster":"JA2018","upvote_count":"3","timestamp":"1732284180.0","comment_id":"1316336"}]},{"upvote_count":"1","comment_id":"1308564","content":"Selected Answer: A\nA- Porque La compañía todavía necesita poder acceder y actualizar los datos durante la ventana de transferencia\nB-Aunque esta podria ser una opcion porque dice de enviar los dispositivosal al centro de datos que serian mas o menos entre 9 y 10 dispositivos ya que el almacenamiento de storage optimized es de 80 TB el problema esta en que con Snowball Edge Storage Optimized no se puede acceder y actuializar a los datos durante la transferencia","timestamp":"1731016980.0","poster":"Danilus"},{"content":"Selected Answer: A\nFinally, a company with good bandwidth.","comment_id":"1283030","poster":"MatAlves","timestamp":"1726207980.0","upvote_count":"4"},{"content":"Selected Answer: A\n(B) is incorrect bc although Mountpoint for S3 is possible for on-premises NAS, this is not as efficient as AWS DataSync. Data updates made during the transfer window would have to be resolved later.","timestamp":"1717275000.0","comment_id":"1222899","poster":"NSA_Poker","upvote_count":"2"},{"upvote_count":"1","poster":"Burrito69","content":"I will put simple calculation for oyu guys to just store in your head to quickly answer: \nfor 10GBPS its 1.25GBPS becasue its bits to bytes.\nfor one minute its 75GBPS\nfor one hour its 4500 GBPS\nfor one day its 10.8 TB\n\nso you can calculate easily if you just store these numbers in your head. lets say if question is 1GBPS DirectConnect that meand everything above should be divide by 8. cool","timestamp":"1711606740.0","comment_id":"1184591","comments":[{"timestamp":"1732284300.0","poster":"JA2018","content":"folks, you had to consider the typical TCP overheads for such over-the-wire data transfers.... it could be as high as 70% of the total available bandwidth due to a wide variety of factors\n\nin a worst case scenario, we could be looking @ 3.24 TB/ day","comment_id":"1316338","upvote_count":"1"}]},{"comment_id":"1115911","content":"Selected Answer: A\nCritical requirement: \"The company needs to move the data efficiently and without disruption.\" \nB: Causes disruption\nC: I don't think that is possible without a gateway kind of thing\nD: Tape backups? \" Mount a target Amazon S3 bucket on the on-premises file system\"? This requires some gateway which is not mentioned\n\nA is the answer as DataSync allows transfer without disruption and with 10Gbps, it can be done in 90 days.","upvote_count":"3","poster":"awsgeek75","timestamp":"1704637860.0"},{"timestamp":"1692906900.0","poster":"Guru4Cloud","content":"Selected Answer: A\nAWS DataSync can efficiently transfer large datasets from on-premises NAS to Amazon S3 over Direct Connect.\n\nDataSync allows accessing and updating the data continuously during the transfer process.","upvote_count":"5","comment_id":"989461"},{"upvote_count":"3","content":"Selected Answer: A\nAWS DataSync is a secure, online service that automates and accelerates moving data between on premises and AWS Storage services.","poster":"TariqKipkemei","timestamp":"1686798960.0","comment_id":"923666"},{"content":"A\nhttps://www.examtopics.com/discussions/amazon/view/46492-exam-aws-certified-solutions-architect-associate-saa-c02/#:~:text=Exam%20question%20from,Question%20%23%3A%20385","comment_id":"901487","timestamp":"1684441800.0","poster":"omoakin","upvote_count":"2"},{"content":"Selected Answer: A\nBy leveraging AWS DataSync in combination with AWS Direct Connect, the company can efficiently and securely transfer its 700 terabytes of data to an Amazon S3 bucket without disruption. The solution allows continued access and updates to the data during the transfer window, ensuring business continuity throughout the migration process.","timestamp":"1684413180.0","upvote_count":"4","comment_id":"901200","poster":"cloudenthusiast"},{"content":"Selected Answer: A\nA for me, bcs egde storage up to 100tb","comment_id":"899170","upvote_count":"5","poster":"nosense","timestamp":"1684239480.0"}],"question_id":405,"question_images":[],"answer_description":"","answer":"A"}],"exam":{"provider":"Amazon","lastUpdated":"11 Apr 2025","numberOfQuestions":1019,"isImplemented":true,"name":"AWS Certified Solutions Architect - Associate SAA-C03","isBeta":false,"isMCOnly":true,"id":31},"currentPage":81},"__N_SSP":true}