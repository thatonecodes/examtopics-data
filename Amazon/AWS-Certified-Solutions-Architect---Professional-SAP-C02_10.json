{"pageProps":{"questions":[{"id":"b2xqDcBYA4WYp3UDlRJf","url":"https://www.examtopics.com/discussions/amazon/view/91139-exam-aws-certified-solutions-architect-professional-sap-c02/","question_images":[],"timestamp":"2022-12-12 10:00:00","isMC":true,"question_id":46,"topic":"1","exam_id":33,"question_text":"A company is running an application on several Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer. The load on the application varies throughout the day, and EC2 instances are scaled in and out on a regular basis. Log files from the EC2 instances are copied to a central Amazon S3 bucket every 15 minutes. The security team discovers that log files are missing from some of the terminated EC2 instances.\nWhich set of actions will ensure that log files are copied to the central S3 bucket from the terminated EC2 instances?","unix_timestamp":1670835600,"answers_community":["B (100%)"],"answer_description":"","choices":{"A":"Create a script to copy log files to Amazon S3, and store the script in a file on the EC2 instance. Create an Auto Scaling lifecycle hook and an Amazon EventBridge rule to detect lifecycle events from the Auto Scaling group. Invoke an AWS Lambda function on the autoscaling:EC2_INSTANCE_TERMINATING transition to send ABANDON to the Auto Scaling group to prevent termination, run the script to copy the log files, and terminate the instance using the AWS SDK.","B":"Create an AWS Systems Manager document with a script to copy log files to Amazon S3. Create an Auto Scaling lifecycle hook and an Amazon EventBridge rule to detect lifecycle events from the Auto Scaling group. Invoke an AWS Lambda function on the autoscaling:EC2_INSTANCE_TERMINATING transition to call the AWS Systems Manager API SendCommand operation to run the document to copy the log files and send CONTINUE to the Auto Scaling group to terminate the instance.","D":"Create an AWS Systems Manager document with a script to copy log files to Amazon S3. Create an Auto Scaling lifecycle hook that publishes a message to an Amazon Simple Notification Service (Amazon SNS) topic. From the SNS notification, call the AWS Systems Manager API SendCommand operation to run the document to copy the log files and send ABANDON to the Auto Scaling group to terminate the instance.","C":"Change the log delivery rate to every 5 minutes. Create a script to copy log files to Amazon S3, and add the script to EC2 instance user data. Create an Amazon EventBridge rule to detect EC2 instance termination. Invoke an AWS Lambda function from the EventBridge rule that uses the AWS CLI to run the user-data script to copy the log files and terminate the instance."},"discussion":[{"poster":"masetromain","comment_id":"774711","content":"Selected Answer: B\nB. Create an AWS Systems Manager document with a script to copy log files to Amazon S3. Create an Auto Scaling lifecycle hook and an Amazon EventBridge rule to detect lifecycle events from the Auto Scaling group. Invoke an AWS Lambda function on the autoscaling:EC2_INSTANCE_TERMINATING transition to call the AWS Systems Manager API SendCommand operation to run the document to copy the log files and send CONTINUE to the Auto Scaling group to terminate the instance. This approach will use the Auto Scaling lifecycle hook to execute the script that copies log files to S3, before the instance is terminated, ensuring that all log files are copied from the terminated instances.","timestamp":"1727062020.0","upvote_count":"12"},{"timestamp":"1670955840.0","poster":"rtgfdv3","comment_id":"744354","upvote_count":"8","content":"Selected Answer: B\nhttps://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/"},{"timestamp":"1734305460.0","content":"Selected Answer: B\nThis is most accurate with redundancy as EventBridge can directly invoke SSM Document and you don't need Lambda function.","upvote_count":"1","poster":"pk0619","comment_id":"1327101"},{"upvote_count":"1","poster":"Shanmahi","content":"Selected Answer: B\nB using systems manager","timestamp":"1732919520.0","comment_id":"1319954"},{"comment_id":"1100415","upvote_count":"6","poster":"atirado","content":"Selected Answer: B\nOption A - This option might not work: Preventing ASG termination could create further trouble and there is no guarantee the script will run if the instance happens to be unhealthy\nOption B - This option could work: Running the script from the SSM API guarantees the script will run, using EventBridge to capture the ASG termination event provides a perfect place to hook in the call to SSM which will also pause the termination until the script runs. Then CONTINUE allows the ASG termination to continue.\nOption C - This option does not work because it does not solve the problem: Terminating instances within the 15 minute window causes log files to be lost. \nOption D - This option might not work: It does not rely on EventBridge to detect the ASG termination event. It also could create further trouble because no other actions will be performed due to sending ABANDON though nothing is said about other actions in the question","timestamp":"1727062080.0"},{"timestamp":"1727062080.0","content":"Selected Answer: B\nA- Wrong because prevent termination is not needed. \nC- Wrong because 5-minute frequency creates an overhead or delay . Using user data for the script adds complexity\nD- Wrong because SNS","poster":"F_Eldin","upvote_count":"3","comment_id":"888774"},{"content":"Selected Answer: B\nB is the right answer due to Auto Scaling lifecycle hook and an Amazon EventBridge rule to detect lifecycle events from the Auto Scaling group. Invoke an AWS Lambda function on the autoscaling:EC2_INSTANCE_TERMINATING transition to call the AWS Systems Manager API SendCommand operation to run the document to copy the log files and send","poster":"gameoflove","comment_id":"892063","upvote_count":"1","timestamp":"1727062080.0"},{"poster":"cattle_rei","content":"Selected Answer: B\nI think this is B. It could be A as well, but B is better solution because the document with SM can be re-utilized with other instances. Also A would require using a custom image with the script or user data to create the script, so more points of failure.","upvote_count":"1","timestamp":"1727062020.0","comment_id":"992977"},{"timestamp":"1727062020.0","poster":"ansgohar","upvote_count":"1","comment_id":"1019698","content":"Selected Answer: B\nB. Create an AWS Systems Manager document with a script to copy log files to Amazon S3. Create an Auto Scaling lifecycle hook and an Amazon EventBridge rule to detect lifecycle events from the Auto Scaling group. Invoke an AWS Lambda function on the autoscaling:EC2_INSTANCE_TERMINATING transition to call the AWS Systems Manager API SendCommand operation to run the document to copy the log files and send CONTINUE to the Auto Scaling group to terminate the instance."},{"upvote_count":"1","timestamp":"1725090900.0","comment_id":"1275443","poster":"amministrazione","content":"B. Create an AWS Systems Manager document with a script to copy log files to Amazon S3. Create an Auto Scaling lifecycle hook and an Amazon EventBridge rule to detect lifecycle events from the Auto Scaling group. Invoke an AWS Lambda function on the autoscaling:EC2_INSTANCE_TERMINATING transition to call the AWS Systems Manager API SendCommand operation to run the document to copy the log files and send CONTINUE to the Auto Scaling group to terminate the instance."},{"comment_id":"1175118","poster":"gofavad926","content":"Selected Answer: B\nB is the correct answer","timestamp":"1710607920.0","upvote_count":"1"},{"timestamp":"1699764600.0","poster":"severlight","comment_id":"1068302","upvote_count":"2","content":"Selected Answer: B\nboth abandon and continue will lead to instance termination, the difference is abandon will prevent from running other lifycycle hooks"},{"poster":"cattle_rei","content":"I think this is B. It could be A as well, but B is better solution because the document with SM can be re-utilized with other instances. Also A would require using a custom image with the script or user data to create the script, so more points of failure.","timestamp":"1693304340.0","comment_id":"992976","upvote_count":"1"},{"poster":"softarts","timestamp":"1692340620.0","content":"Selected Answer: B\nd is wrong, shouldn't be \"ABANDON\"","comment_id":"984229","upvote_count":"2"},{"timestamp":"1687914420.0","upvote_count":"1","content":"Selected Answer: B\nit's a B","poster":"NikkyDicky","comment_id":"935948"},{"poster":"2aldous","timestamp":"1681842420.0","upvote_count":"3","comment_id":"873995","content":"Selected Answer: B\nB.\nSmart solution :)"},{"content":"Selected Answer: B\nSystems manager + eventbridge","timestamp":"1679979360.0","comment_id":"852789","upvote_count":"4","poster":"mfsec"},{"timestamp":"1678184880.0","content":"Selected Answer: B\nhttps://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/","comment_id":"831759","poster":"kiran15789","upvote_count":"2"},{"poster":"Untamables","content":"Selected Answer: B\nB\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html","timestamp":"1672202340.0","comment_id":"759354","upvote_count":"4"},{"poster":"masetromain","comments":[{"timestamp":"1670866020.0","upvote_count":"2","comment_id":"743097","poster":"masetromain","content":"I'm wrong the answer is B\n\nhttps://www.examtopics.com/discussions/amazon/view/69532-exam-aws-certified-solutions-architect-professional-topic-1/"}],"comment_id":"743021","content":"I find answer C correct.\nbut can at the same time that an instance is terminated run a lambda function that executes the script?","upvote_count":"1","timestamp":"1670862000.0"},{"content":"B is correct\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/tutorial-lifecycle-hook-lambda.html","comment_id":"742820","upvote_count":"2","timestamp":"1670851140.0","poster":"zhangyu20000"},{"upvote_count":"4","comment_id":"742575","timestamp":"1670835600.0","poster":"Raj40","content":"Selected Answer: B\nCorrect answer B"}],"answer_images":[],"answer":"B","answer_ET":"B"},{"id":"ynxgQFBfjzFtcXswXpBW","answer":"D","topic":"1","exam_id":33,"isMC":true,"timestamp":"2023-01-16 15:22:00","unix_timestamp":1673878920,"choices":{"B":"Set up Amazon Simple Email Service (Amazon SES) to send email messages. Store the email template in an Amazon S3 bucket. Create an AWS Lambda function to retrieve the template from the S3 bucket and to merge the customer data from the application with the template. Use an SDK in the Lambda function to send the email message.","C":"Set up an SMTP server on Amazon EC2 instances by using an AMI from the AWS Marketplace. Store the email template in Amazon Simple Email Service (Amazon SES) with parameters for the customer data. Create an AWS Lambda function to call the SES template and to pass customer data to replace the parameters. Use the AWS Marketplace SMTP server to send the email message.","A":"Set up an SMTP server on Amazon EC2 instances by using an AMI from the AWS Marketplace. Store the email template in an Amazon S3 bucket. Create an AWS Lambda function to retrieve the template from the S3 bucket and to merge the customer data from the application with the template. Use an SDK in the Lambda function to send the email message.","D":"Set up Amazon Simple Email Service (Amazon SES) to send email messages. Store the email template on Amazon SES with parameters for the customer data. Create an AWS Lambda function to call the SendTemplatedEmail API operation and to pass customer data to replace the parameters and the email destination."},"discussion":[{"poster":"God_Is_Love","upvote_count":"12","timestamp":"1694311320.0","content":"Selected Answer: D\nSendTemplatedEmail\nSendEmail\nSendRawEmail are email api methods used in SES","comment_id":"834622"},{"comment_id":"778069","content":"Selected Answer: D\nThe correct answer is D.\n\nIn this solution, the company can use Amazon SES to send email messages, which will minimize operational overhead as SES is a fully managed service that handles sending and receiving email messages. The company can store the email template on Amazon SES with parameters for the customer data and use an AWS Lambda function to call the SendTemplatedEmail API operation, passing in the customer data to replace the parameters and the email destination. This solution eliminates the need to set up and manage an SMTP server on EC2 instances, which can be costly and time-consuming.\n\nOption A and B are not correct because it requires to set up an SMTP server on EC2 instances, which is not necessary and will increase operational overhead.\nOption C is not correct because it stores the email template in Amazon SES with parameters for the customer data which is not possible.","upvote_count":"11","poster":"masetromain","timestamp":"1689523320.0","comments":[{"upvote_count":"7","comments":[{"poster":"titi_r","timestamp":"1726604820.0","content":"ChatGPT also is saying \"Option A and B are not correct because it requires to set up an SMTP server on EC2 instances\", but those options are \"A\" and \"C\", not \"A\" and \"B\". Seems there is some mismatch with the options.","upvote_count":"2","comment_id":"1176090"}],"content":"Ok, so according to chatgpt C is not correct because \"Option C is not correct because it stores the email template in Amazon SES with parameters for the customer data which is not possible.\"\nHowever, D says exactly the same - so D is not correct as well?\nDo not fully trust chatgp","poster":"Maria2023","timestamp":"1703263680.0","comment_id":"930701"}]},{"content":"Selected Answer: D\nS3 Buckets is not needed to store template","poster":"[Removed]","comment_id":"1209644","upvote_count":"1","timestamp":"1731317580.0"},{"upvote_count":"1","timestamp":"1719181920.0","content":"Selected Answer: D\nOption D","comment_id":"1104384","poster":"career360guru"},{"upvote_count":"1","comment_id":"985167","poster":"SK_Tyagi","content":"Selected Answer: D\nD - Can send templated email with request parameters","timestamp":"1708351380.0"},{"timestamp":"1704984720.0","comment_id":"948960","poster":"Jonalb","content":"Selected Answer: D\nDDDDDDDD","upvote_count":"1"},{"upvote_count":"1","timestamp":"1704315780.0","comment_id":"942147","poster":"NikkyDicky","content":"Selected Answer: D\nits a d"},{"comments":[{"poster":"carpa_jo","comment_id":"1107811","upvote_count":"1","timestamp":"1719574800.0","content":"D is correct.\nRegarding your concerns about email templates on SES with parameters see: https://docs.aws.amazon.com/ses/latest/dg/send-personalized-email-api.html"},{"timestamp":"1708351260.0","upvote_count":"1","poster":"SK_Tyagi","content":"https://docs.aws.amazon.com/ses/latest/APIReference-V2/API_CreateEmailTemplate.html","comment_id":"985166","comments":[{"poster":"pk0619","comment_id":"1329763","upvote_count":"1","timestamp":"1734741420.0","content":"There can be variables in the template."}]}],"comment_id":"930708","content":"Selected Answer: B\nI vote for B due to the fact that I cannot see an option to \"Store the email template on Amazon SES with parameters for the customer data\" Other than that it looks like a good option but it's just not working","poster":"Maria2023","timestamp":"1703263860.0","upvote_count":"1"},{"timestamp":"1702935540.0","upvote_count":"1","comment_id":"926877","content":"Selected Answer: D\nkeyword = SendTemplatedEmail API","poster":"SkyZeroZx"},{"timestamp":"1695795840.0","content":"Selected Answer: D\nTemplate - easy one.","upvote_count":"1","poster":"mfsec","comment_id":"851791"},{"comment_id":"792231","upvote_count":"3","poster":"zozza2023","content":"Selected Answer: D\nD should be the answer","timestamp":"1690672020.0"},{"poster":"zhangyu20000","upvote_count":"2","comment_id":"777765","timestamp":"1689510120.0","content":"D is correct - https://docs.aws.amazon.com/ses/latest/APIReference/API_SendTemplatedEmail.html"}],"answer_description":"","answers_community":["D (97%)","3%"],"question_text":"A software as a service (SaaS) based company provides a case management solution to customers A3 part of the solution. The company uses a standalone Simple Mail Transfer Protocol (SMTP) server to send email messages from an application. The application also stores an email template for acknowledgement email messages that populate customer data before the application sends the email message to the customer.\n\nThe company plans to migrate this messaging functionality to the AWS Cloud and needs to minimize operational overhead.\n\nWhich solution will meet these requirements MOST cost-effectively?","question_id":47,"question_images":[],"answer_ET":"D","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/95557-exam-aws-certified-solutions-architect-professional-sap-c02/"},{"id":"CnIuRFc6SzP1W3yXwcsP","answer_description":"","answer_images":[],"answer":"C","unix_timestamp":1673880300,"question_id":48,"exam_id":33,"discussion":[{"content":"Selected Answer: C\nThe correct answer is C. The company can solve the problem by configuring scale-in protection for the instances during processing. This will ensure that the instances are not terminated while they are processing videos. This will prevent the messages from moving to the dead-letter queue and ensure that videos are processed properly.\n\nOption A is incorrect because turning on termination protection for the EC2 instances will not solve the problem as it will impact the ability of the Auto Scaling group to scale instances in and out based on the number of videos in the queue.\n\nOption B is incorrect because the company has specified a visibility timeout of 1 hour, which is enough time for the instances to process a video and there is no need to update the timeout to 3 hours.\n\nOption D is incorrect because the company has set the maxReceiveCount to 1 and changing it to 0 will not solve the problem. maxReceiveCount allowed range is 1 to 1000.","poster":"masetromain","timestamp":"1673892240.0","upvote_count":"28","comments":[{"content":"fully agree, option d is inocrrect because 0 is an invalida value for maxReceiveCount","poster":"[Removed]","upvote_count":"1","comment_id":"957578","timestamp":"1689858240.0"},{"poster":"Bwitch","content":"ChatGPT confirms this reasoning.","upvote_count":"8","comment_id":"905821","timestamp":"1684928640.0"}],"comment_id":"778073"},{"poster":"venvig","content":"Selected Answer: C\nRefer https://aws.amazon.com/blogs/aws/new-instance-protection-for-auto-scaling/\nFrom the above link, \"an instance might be handling a long-running work task, perhaps pulled from an SQS queue. Protecting the instance from termination will avoid wasted work\" - This is what the question is also alluding to.\nThis is how one would make use of the functionality.\nYou change the protection status of one or more instances by calling the SetInstanceProtection function. If you wanted to use this function to protect long-running, queue-driven worker processes from scale-in termination, you could set up your application as follows (this is pseudocode):\n\nwhile (true)\n{\n SetInstanceProtection(False);\n Work = GetNextWorkUnit();\n SetInstanceProtection(True);\n ProcessWorkUnit(Work);\n SetInstanceProtection(False);\n}","timestamp":"1693690740.0","upvote_count":"6","comment_id":"997146"},{"poster":"Jorkaef","content":"Correct is C:\n\n B. 3-hour visibility timeout\n\nToo long for 30-minute processing\nCould delay reprocessing of failed messages\nDoesn't address root cause\n\nC. Scale-in protection during processing\n\nPrevents instance termination while processing\nAllows message processing to complete\nPrevents message return to queue\nStops premature scale-in\n✓ CORRECT","upvote_count":"1","comment_id":"1313761","timestamp":"1731883620.0"},{"poster":"Jorkaef","comment_id":"1311049","content":"B is correct;\nupdating the visibility timeout to 3 hours (option B) is the most appropriate solution as it gives enough time for the messages to be processed without being prematurely marked as failures.","timestamp":"1731466440.0","upvote_count":"1"},{"comment_id":"1227632","upvote_count":"1","poster":"trungtd","timestamp":"1717987260.0","content":"Selected Answer: D\nD is a typo"},{"content":"Selected Answer: D\nIf Option D is a typo, then D","timestamp":"1708914060.0","upvote_count":"2","poster":"VerRi","comment_id":"1159303"},{"content":"B.\nThe best solution for this problem is to update the visibility timeout for the SQS queue to 3 hours. This is because when the visibility timeout is set to 1 hour, it means that if the EC2 instance doesn't process the message within an hour, it will be moved to the dead-letter queue. By increasing the visibility timeout to 3 hours, this should give the EC2 instance enough time to process the message before it gets moved to the dead-letter queue. Additionally, configuring scale-in protection for the EC2 instances during processing will help to ensure that the instances are not terminated while the messages are being processed.","timestamp":"1706394960.0","upvote_count":"3","comment_id":"1133699","poster":"Greanny"},{"timestamp":"1705132680.0","content":"Selected Answer: D\nOption D is a typo.\nI seen the same question in udemy but the Option D is 10","upvote_count":"4","comment_id":"1121397","poster":"tmlong18"},{"content":"Selected Answer: C\nOption C is correct.","timestamp":"1703378700.0","poster":"career360guru","upvote_count":"2","comment_id":"1104389"},{"timestamp":"1700119140.0","upvote_count":"1","poster":"severlight","content":"Selected Answer: C\nsetting MaxReceiveCount to 0 doesn't make and send and it impossible, because messages would be send to DLQ without any attempt to consume them from source queue","comment_id":"1072243"},{"poster":"Russs99","comment_id":"1013302","content":"Selected Answer: D\nchecked 4 AI, C is definitely not the correct answer: Option C: Configuring scale-in protection for the instances during processing will not prevent messages from being moved to the dead-letter queue if they cannot be processed on the first attempt.","timestamp":"1695316980.0","upvote_count":"1"},{"content":"Selected Answer: C\nGoing with C only because D has value of maxReceiveCount set to 0","timestamp":"1692447780.0","upvote_count":"2","comment_id":"985187","poster":"SK_Tyagi"},{"timestamp":"1689634200.0","poster":"rtguru","upvote_count":"1","comment_id":"954677","content":"I go with C"},{"comments":[{"upvote_count":"1","comment_id":"1171284","content":"But this for a queue to use with lambda. Here it is EC2 in ASG","poster":"ajeeshb","timestamp":"1710186360.0"}],"comment_id":"943292","poster":"YodaMaster","content":"Selected Answer: B\nB.\nAWS \"recommends setting your queue's visibility timeout to six times your function timeout\" which makes 3 hours perfect.\nsource: https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html","upvote_count":"2","timestamp":"1688530980.0"},{"content":"Selected Answer: C\nC more likely","comment_id":"942153","upvote_count":"1","poster":"NikkyDicky","timestamp":"1688411460.0"},{"content":"I couldn't find any way to configure scale-in protection for the instances during processing except to do it manually, which is going to be an insane exercise. Eventually, that can be done by the application as part of the processing but I would then expect some more context in the answer.","poster":"Maria2023","upvote_count":"1","timestamp":"1687447620.0","comment_id":"930738"},{"upvote_count":"6","poster":"dev112233xx","comment_id":"904001","content":"Selected Answer: D\nD makes sense\nI think D answer has a typo! probably they didn't copy the text properly\nhttps://repost.aws/knowledge-center/lambda-retrying-valid-sqs-messages","timestamp":"1684755720.0"},{"timestamp":"1684621500.0","upvote_count":"1","poster":"F_Eldin","comment_id":"902798","content":"Selected Answer: D\nOption C, configuring scale-in protection for the instances during processing, is not directly related to the problem. Scale-in protection prevents instances from being terminated during an Auto Scaling event, but it does not address the issue of messages being moved to the dead-letter queue without successful processing.\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-instance-protection.html\n\nI think D is tyoe and should read :\nD. Update the redrive policy and set maxReceiveCount to 10."},{"poster":"Parsons","comment_id":"881752","upvote_count":"2","content":"Selected Answer: D\nD should be \"set maxReceiveCount to 10.\" It, Maybe a typo.\n\nExplanation:\nThis setting ensures that any message that failed to be processed will be sent back to the queue to be picked up by other consumers and re-processed.\n\n---\nWhy C is incorrect?\nWell, the Auto Scaling group responds to the number of messages on the queue, scale-in protection is not cost-effective when there are no messages on the SQS queue.","timestamp":"1682521320.0","comments":[{"timestamp":"1684619280.0","upvote_count":"3","poster":"rbm2023","comment_id":"902792","content":"there are no errors in the application logs, this leave us to believe that the instances are being terminated by the auto scaling during the processing of the videos. any workaround in the SQS layer might not sove the problem"}]},{"poster":"pauloC","comment_id":"879197","content":"Selected Answer: C\nI couldn't find SQS guidance for EC2 but there is for Lambda. We recommend setting your queue's visibility timeout to six times your function timeout, plus the value of MaximumBatchingWindowInSeconds . This allows time for your Lambda function to process each batch of events and to retry in the event of a throttling error.\nI think you can apply this here as the process takes 30 minutes and 3 hours is 6X this.\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html","upvote_count":"1","timestamp":"1682329560.0"},{"content":"Selected Answer: C\nScale-in protection","poster":"mfsec","comments":[{"upvote_count":"1","comment_id":"851798","content":"he visibility timeout might still need to be adjusted, but the scale-in protection is the primary solution to prevent instances from being terminated during processing, which would cause the messages to end up in the dead-letter queue.","timestamp":"1679898600.0","poster":"mfsec"}],"upvote_count":"1","comment_id":"851794","timestamp":"1679898420.0"},{"comment_id":"822796","poster":"kiran15789","upvote_count":"4","timestamp":"1677435300.0","comments":[{"upvote_count":"1","timestamp":"1684330440.0","comment_id":"900194","poster":"Jesuisleon","content":"Although your answer is right, your description for maxReceiveCount is WRONG. \namxReceiveCount represents the max count if app fails to deal with the message, after that the message is ALWAYS moved to dead-letter queue.\nsee \" The maxReceiveCount is the number of times a consumer tries receiving a message from a queue without deleting it before being moved to the dead-letter queue. \"\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html"}],"content":"Selected Answer: C\nSetting maxReceiveCount to 0 in the redrive policy of an Amazon SQS queue means that if a message is not successfully processed by any of the consumers after one attempt, the message will be deleted from the queue immediately instead of being moved to the dead-letter queue."},{"timestamp":"1676835960.0","content":"Selected Answer: C\nC Correct Answer","upvote_count":"1","poster":"spd","comment_id":"814460"},{"comments":[{"upvote_count":"1","content":"moderator dont approve, I figured it out.","comment_id":"812090","poster":"c73bf38","timestamp":"1676649000.0","comments":[{"comment_id":"812094","poster":"c73bf38","content":"B is the correct option.\n\nThe issue seems to be that the videos are taking longer than the visibility timeout to process, so they are being sent to the dead-letter queue even though they are still being processed. By updating the visibility timeout for the SQS queue to 3 hours, the videos will have more time to process before being sent to the dead-letter queue, which should solve the problem.","timestamp":"1676649120.0","upvote_count":"2","comments":[{"upvote_count":"1","content":"Interesting point, I understood the problem in a different way. I think the problem is that while an Ec2 Instance is still working on the video, there was a scale-in event and that instance was selected for termination. I will use personally lifecycle hooks, option C defeats the purpose of AutoScaling in some way like you said is a workaround.","timestamp":"1677847380.0","comment_id":"827964","poster":"Sarutobi"}]}]}],"timestamp":"1676648400.0","content":"Selected Answer: D\nI'm conflicted on this question, D updating the redrive sounds like the best solution because it's addressing the root cause. C is a workaround, not solving the problem of processing the videos.","comment_id":"812080","poster":"c73bf38","upvote_count":"1"},{"upvote_count":"1","comment_id":"792235","content":"Selected Answer: C\nfor me, should be C","poster":"zozza2023","timestamp":"1675041060.0"},{"content":"C is correct\nA: termination protection of EC2 will impact ASG\nB: only take 30 minutes, no need for 3 hour\nC: ASG is based on queue depth, ASG will scale in when queue length is 0. But maxreceivecount is set to 1.\nD: maxreceivecount allowed range is 1 to 1000","timestamp":"1673880300.0","poster":"zhangyu20000","upvote_count":"4","comment_id":"777793"}],"answer_ET":"C","url":"https://www.examtopics.com/discussions/amazon/view/95559-exam-aws-certified-solutions-architect-professional-sap-c02/","choices":{"C":"Configure scale-in protection for the instances during processing","A":"Turn on termination protection tor the EC2 Instances","D":"Update the redrive policy and set maxReceiveCount to 0.","B":"Update the visibility timeout for the SQS queue to 3 hours"},"question_images":[],"question_text":"A company is processing videos in the AWS Cloud by Using Amazon EC2 instances in an Auto Scaling group. It takes 30 minutes to process a video Several EC2 instances scale in and out depending on the number of videos in an Amazon Simple Queue Service (Amazon SQS) queue.\n\nThe company has configured the SQS queue with a redrive policy that specifies a target dead-letter queue and a maxReceiveCount of 1. The company has set the visibility timeout for the SQS queue to 1 hour. The company has set up an Amazon CloudWatch alarm to notify the development team when there are messages in the dead-letter queue.\n\nSeveral times during the day. the development team receives notification that messages are in the dead-letter queue and that videos have not been processed property. An investigation finds no errors m the application logs.\n\nHow can the company solve this problem?","timestamp":"2023-01-16 15:45:00","topic":"1","answers_community":["C (71%)","D (26%)","3%"],"isMC":true},{"id":"XJOBm0WdeQX9rlgcY2SU","answer_ET":"C","question_text":"A company has developed APIs that use Amazon API Gateway with Regional endpoints. The APIs call AWS Lambda functions that use API Gateway authentication mechanisms. After a design review, a solutions architect identifies a set of APIs that do not require public access.\n\nThe solutions architect must design a solution to make the set of APIs accessible only from a VPC. All APIs need to be called with an authenticated user\n\nWhich solution will meet these requirements with the LEAST amount of effort?","timestamp":"2023-01-16 15:54:00","choices":{"B":"Remove the DNS entry that is associated with the API in API Gateway. Create a hosted zone in Amazon Route 53. Create a CNAME record in the hosted zone. Update the API in API Gateway with the CNAME record. Use the CNAME record to call the API from the VPC.","C":"Update the API endpoint from Regional to private in API Gateway. Create an interface VPC endpoint in the VPCreate a resource policy, and attach it to the API. Use the VPC endpoint to call the API from the VPC.","D":"Deploy the Lambda functions inside the VPC Provision an EC2 instance, and install an Apache server. From the Apache server, call the Lambda functions. Use the internal CNAME record of the EC2 instance to call the API from the VPC.","A":"Create an internal Application Load Balancer (ALB). Create a target group. Select the Lambda function to call. Use the ALB DNS name to call the API from the VPC."},"answer":"C","question_id":49,"discussion":[{"upvote_count":"11","comments":[{"poster":"altonh","timestamp":"1736992320.0","content":"Agree. The proper solution should be:\nCreate a new private API GW and move those private APIs to this newly created API GW.","upvote_count":"1","comment_id":"1341311"},{"poster":"toma","timestamp":"1719353520.0","upvote_count":"2","content":"there is only set of APIs that do not require public access, you dont need all APIs private access? so it could be that the answer is A?","comment_id":"1237112"}],"comment_id":"1093079","timestamp":"1702268640.0","poster":"bjexamprep","content":"Selected Answer: C\nBad question design. None of the answers is correct.\nNone of the answers mentions how to satisfy the requirement of \"All APIs need to be called with an authenticated user\".\nAnother requirement \"make the set of APIs accessible only from a VPC\". \"the set\" doesn't mean the whole set. Here \"the set\" means a part of the whole set. \nA: The set of APIs are still publicly accessible.\nB: Removing DNS entry doesn't remove the public accessibility.\nC: This is making the whole set of APIs private. If this answer can be specific to \"the set\" APIs, this could be a good answer.\nD: Using EC2 instances is always a bad answer."},{"content":"Selected Answer: C\nshould be C as on the question has said 'no need for public IP\" ==> private in API gateway = VPC endpoint","comment_id":"792240","upvote_count":"9","poster":"zozza2023","timestamp":"1675041540.0"},{"content":"All given answers are not ideal.. the closet one is C BUT.. .when mentioning the requirement to have only 'a set of API to be private' means 'not all'.. turning the endpoint from public to private will turn all to Private ,, which is not fully correct as per the question.. I suppose the given answer or question missing an info.. or AWS starts playing with AI","upvote_count":"3","timestamp":"1706785020.0","comment_id":"1137547","poster":"AimarLeo"},{"poster":"carpa_jo","comment_id":"1107833","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-private-apis.html","upvote_count":"1","timestamp":"1703772540.0"},{"upvote_count":"1","content":"Selected Answer: C\nOption C","poster":"career360guru","timestamp":"1703378880.0","comment_id":"1104391"},{"upvote_count":"1","comment_id":"997151","timestamp":"1693691760.0","poster":"venvig","content":"Selected Answer: C\nRefer https://aws.amazon.com/blogs/compute/introducing-amazon-api-gateway-private-endpoints/"},{"content":"Answer is C as explain in https://repost.aws/knowledge-center/api-gateway-vpc-connections","poster":"Explorer_30","timestamp":"1693678680.0","upvote_count":"1","comment_id":"997058"},{"poster":"SK_Tyagi","comment_id":"985196","upvote_count":"1","timestamp":"1692448140.0","content":"Selected Answer: C\nRegional to Private fits the use-case"},{"timestamp":"1689947160.0","comment_id":"958567","content":"the best possible answer from all the options is C","poster":"rtguru","upvote_count":"1"},{"poster":"NikkyDicky","upvote_count":"2","comment_id":"942161","content":"Selected Answer: C\nit's C, although it begs the questions about APIs that need to stay public...","timestamp":"1688411940.0"},{"timestamp":"1679898660.0","content":"Selected Answer: C\nC. Update the API endpoint from Regional to private in API Gateway.","poster":"mfsec","comment_id":"851799","upvote_count":"1"},{"content":"Selected Answer: C\nThe correct answer is C. Update the API endpoint from Regional to private in API Gateway. Create an interface VPC endpoint in the VPC. Create a resource policy, and attach it to the API. Use the VPC endpoint to call the API from the VPC.\nThis solution will meet the requirements with the least amount of effort because it utilizes the built-in features of API Gateway and VPC to restrict access to the API. With this method, no additional infrastructure or configurations are necessary.\nA and B are not correct because they would require additional infrastructure and configurations.\nD is not correct because it would require provisioning an EC2 instance and installing an Apache server, introducing additional complexity and management overhead.","upvote_count":"5","comment_id":"778075","poster":"masetromain","timestamp":"1673892300.0"},{"upvote_count":"1","comment_id":"777804","content":"C is correct","timestamp":"1673880840.0","poster":"zhangyu20000"}],"unix_timestamp":1673880840,"answer_description":"","exam_id":33,"answers_community":["C (100%)"],"topic":"1","isMC":true,"answer_images":[],"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/95561-exam-aws-certified-solutions-architect-professional-sap-c02/"},{"id":"ULk1gb6HbNOJZ6mcewIe","answers_community":["BD (96%)","2%"],"exam_id":33,"answer_description":"","discussion":[{"poster":"sambb","content":"Selected Answer: BD\nA: Global Accelerator can't have an s3 bucket as endpoint\nC: People are complaining about time to retreive maps. Transfert acceleration is used to accelerate PUT requests to an s3 bucket located in a distant region.\nE: An accelerator as cloudfront origin does not make much sense, because cloudfront is already using the AWS network. Global Accelerator is usually for Layer 4 networking and/or static anycast IPs","comment_id":"832753","upvote_count":"19","timestamp":"1678267140.0"},{"timestamp":"1673892360.0","poster":"masetromain","upvote_count":"8","content":"Selected Answer: BD\nB is correct because it involves creating a new S3 bucket in the us-east-1 region and configuring cross-Region replication to synchronize from the existing S3 bucket in eu-west-1. This will allow users in us-east-1 to access the weather maps from a closer location, improving performance.\n\nD is correct because it involves using Lambda@Edge to modify requests from North America to use the S3 bucket in us-east-1. This will also allow users in us-east-1 to access the weather maps from a closer location, improving performance.\n\nA and E are not correct because they do not involve creating a new S3 bucket in us-east-1, which is necessary for improving performance for the users in that region. C is not correct because it involves using the S3 Transfer Acceleration endpoint, which is a different service and not necessary for this scenario.","comment_id":"778077"},{"upvote_count":"1","timestamp":"1734430080.0","comment_id":"1327844","comments":[{"content":"The correct answer implies a CloudFront with multiple origins, i.e. pointing to two (2) S3 buckets and using Lambda@Edge to decide which origin to go to.","comment_id":"1341312","poster":"altonh","timestamp":"1736993340.0","upvote_count":"1"}],"content":"Selected Answer: BD\nAlthough, D is not really correct. You should be using \"s3 multi-region access point\". It is designed specifically for this scenario.","poster":"ahhatem"},{"content":"Selected Answer: BD\nBD\nC using S3 Transfer Acceleration is good but this answer option itself is wrong due to the statement that pointing to a regional endpoint, where it doesn't exist. Once enable, it is just a global endpoint URL\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/transfer-acceleration-examples.html","timestamp":"1712206740.0","upvote_count":"1","poster":"pangchn","comment_id":"1189098"},{"content":"Selected Answer: AC\nIf you want to improve latency , you always look for Global Accelerator fro the readings and Transfer accelerator for the updates. \nYes, it is possible to configure AWS Global Accelerator to distribute traffic from an S3 bucket in one AWS Region (eu-west-1) to endpoint groups in another AWS Region (us-east-1) for TCP ports 80 and 443. This configuration can be useful for improving the performance and availability of your S3 bucket for users in both regions.\nThis way you sabe money in the storage, you don't need to duplicate the storage. And for persons that chose option D, if you update the bucket there, those objects will not be replicated to the other region since replication works only in one way.","poster":"jpa8300","timestamp":"1704645840.0","upvote_count":"1","comment_id":"1116018","comments":[{"poster":"helloworldabc","content":"just BD","comment_id":"1277121","timestamp":"1725333720.0","upvote_count":"1"}]},{"content":"Selected Answer: BD\nOption B & D","poster":"career360guru","timestamp":"1703379180.0","comment_id":"1104392","upvote_count":"1"},{"timestamp":"1702269060.0","content":"Selected Answer: BD\nThis is not a good question design. Does that mean the application use CloudFront in EU and does not use CloudFront in the US? How weird it is!!!","comment_id":"1093082","poster":"bjexamprep","upvote_count":"3"},{"timestamp":"1701344640.0","upvote_count":"4","content":"Selected Answer: BD\nExactly case from this blog post https://aws.amazon.com/blogs/networking-and-content-delivery/dynamically-route-viewer-requests-to-any-origin-using-lambdaedge/","comment_id":"1084305","poster":"Jrhp"},{"upvote_count":"2","comment_id":"958585","timestamp":"1689948060.0","poster":"rtguru","content":"BD, I was initially looking at BE, I think global accelerator is used more for write requests."},{"comment_id":"942169","content":"Selected Answer: BD\nBD makes more ense","upvote_count":"2","timestamp":"1688412300.0","poster":"NikkyDicky"},{"upvote_count":"1","comment_id":"941154","poster":"SmileyCloud","timestamp":"1688322300.0","content":"Selected Answer: BD\nhttps://godof.cloud/dynamic-origin-s3-spa/\nUse case"},{"comment_id":"853902","timestamp":"1680055380.0","comments":[{"upvote_count":"2","poster":"Eshu2009","comment_id":"853906","timestamp":"1680055980.0","content":"Q: Can I use AWS Global Accelerator for object storage with Amazon S3?\n\nA: You can use Amazon S3 Multi-Region Access Points to get the benefits of Global Accelerator for object storage. S3 Multi-Region Access Points use Global Accelerator transparently to provide a single global endpoint to access a data set that spans multiple S3 buckets in different AWS Regions. This allows you to build multi-region applications with the same simple architecture used in a single region, and then to run those applications anywhere in the world. Application requests made to an S3 Multi-Region Access Point’s global endpoint automatically route over the AWS global network to the S3 bucket with the lowest network latency. This allows applications to automatically avoid congested network segments on the public internet, improving application performance and reliability."}],"content":"BE- global accelerators improve performance by providing edge location for onboarding traffic.","poster":"Eshu2009","upvote_count":"3"},{"timestamp":"1679899020.0","content":"Selected Answer: BD\nIll go with BD","poster":"mfsec","upvote_count":"1","comment_id":"851805"},{"poster":"kiran15789","upvote_count":"4","comment_id":"822808","timestamp":"1677435720.0","content":"Selected Answer: BD\nSince only one additional region we dont need global accelerators"},{"timestamp":"1677075900.0","poster":"bititan","content":"Selected Answer: BC\nS3 transfer acceleration is more efficient","upvote_count":"1","comment_id":"817913"},{"poster":"zozza2023","content":"Selected Answer: BD\nA and E are not correct as there isn't a need to use aws global accel","timestamp":"1675041720.0","upvote_count":"2","comment_id":"792247"},{"comment_id":"777805","upvote_count":"1","timestamp":"1673880900.0","content":"BD is correct","poster":"zhangyu20000"}],"answer":"BD","question_text":"A weather service provides high-resolution weather maps from a web application hosted on AWS in the eu-west-1 Region. The weather maps are updated frequently and stored in Amazon S3 along with static HTML content. The web application is fronted by Amazon CloudFront.\n\nThe company recently expanded to serve users in the us-east-1 Region, and these new users report that viewing their respective weather maps is slow from time to time.\n\nWhich combination of steps will resolve the us-east-1 performance issues? (Choose two.)","unix_timestamp":1673880900,"question_images":[],"answer_ET":"BD","timestamp":"2023-01-16 15:55:00","answer_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/95562-exam-aws-certified-solutions-architect-professional-sap-c02/","choices":{"D":"Use Lambda@Edge to modify requests from North America to use the S3 bucket in us-east-1.","A":"Configure the AWS Global Accelerator endpoint for the S3 bucket in eu-west-1. Configure endpoint groups for TCP ports 80 and 443 in us-east-1.","E":"Configure the AWS Global Accelerator endpoint for us-east-1 as an origin on the CloudFront distribution. Use Lambda@Edge to modify requests from North America to use the new origin.","C":"Use Lambda@Edge to modify requests from North America to use the S3 Transfer Acceleration endpoint in us-east-1.","B":"Create a new S3 bucket in us-east-1. Configure S3 cross-Region replication to synchronize from the S3 bucket in eu-west-1."},"isMC":true,"question_id":50}],"exam":{"lastUpdated":"11 Apr 2025","isMCOnly":true,"provider":"Amazon","isImplemented":true,"numberOfQuestions":529,"isBeta":false,"name":"AWS Certified Solutions Architect - Professional SAP-C02","id":33},"currentPage":10},"__N_SSP":true}