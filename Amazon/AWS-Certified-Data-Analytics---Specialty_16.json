{"pageProps":{"questions":[{"id":"AIgOJDGlQBG2hLOTHwGQ","answer_ET":"B","unix_timestamp":1596997440,"question_text":"A large company receives files from external parties in Amazon EC2 throughout the day. At the end of the day, the files are combined into a single file, compressed into a gzip file, and uploaded to Amazon S3. The total size of all the files is close to 100 GB daily. Once the files are uploaded to Amazon S3, an\nAWS Batch program executes a COPY command to load the files into an Amazon Redshift cluster.\nWhich program modification will accelerate the COPY process?","exam_id":20,"discussion":[{"comment_id":"159281","content":"B. Split your data into files so that the number of files is a multiple of the number of slices in your cluster. That way Amazon Redshift can divide the data evenly among the slices.","timestamp":"1632455640.0","upvote_count":"23","poster":"singh100"},{"upvote_count":"7","poster":"Shraddha","comment_id":"383744","timestamp":"1636270800.0","content":"B : \nThis is a textbook question. Sequential loading vs. parallel loading.\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/t_splitting-data-files.html"},{"content":"files -> EC2 -> merge files at the end of the day single file compressed -> s3 (100GB daily)\n\nA: The copy command for a large file (100GB) is slow and not effective as redshift will try to distribute the processing across the cluster and only after this division will the copy be carried out.\n\nB: Files must be large enough to run on only one slice of the node. With this pre-processing done, the master node does not need to worry about \"allocating memory\" to copy this file. If each slice processes a file, the transfer speed will be optimal.\n\nC: It's a smart option, but not the most effective. The number of slices is directly related to the number of nodes, but if the division is made thinking only about the number of nodes, it is possible to make the mistake of executing the COPY command for files that are too large.\n\nD: The dist style must be done after loading the data.","upvote_count":"1","poster":"GCPereira","comment_id":"1102982","timestamp":"1703193960.0"},{"comment_id":"989803","poster":"nroopa","upvote_count":"1","content":"Option B","timestamp":"1692950700.0"},{"content":"Selected Answer: B\nI think B","timestamp":"1690914480.0","upvote_count":"1","poster":"NikkyDicky","comment_id":"969298"},{"comments":[{"upvote_count":"1","content":"Agree I passed the test two times in a row.","timestamp":"1699962720.0","poster":"roymunson","comment_id":"1070327"}],"poster":"pk349","upvote_count":"1","content":"B: I passed the test","timestamp":"1682947260.0","comment_id":"886285"},{"comment_id":"750683","upvote_count":"1","poster":"SamQiu","content":"Why can't I use Option D?","timestamp":"1671526440.0"},{"poster":"[Removed]","content":"b\nhttps://docs.aws.amazon.com/glue/latest/dg/cross-account-access.html\nGranting access to Data Catalog resources across accounts enables your extract, transform, and load (ETL) jobs to query and join data from different accounts.","upvote_count":"1","timestamp":"1671205140.0","comment_id":"747340"},{"upvote_count":"7","content":"Selected Answer: B\nB is correct as the COPY command loads the data in parallel from multiple files, dividing the workload among the nodes in your cluster. When you load all the data from a single large file, Amazon Redshift is forced to perform a serialized load, which is much slower. Split your load data files so that the files are about equal size, between 1 MB and 1 GB after compression. For optimum parallelism, the ideal size is between 1 MB and 125 MB after compression. The number of files should be a multiple of the number of slices in your cluster.","comment_id":"711285","timestamp":"1667583300.0","poster":"cloudlearnerhere"},{"timestamp":"1664080980.0","content":"Selected Answer: B\nGZIP cannot be split. So first split the files and then gzip the slices. Also, it is recommended to have number of files equal to a number which is multiple of total slices in Redshift cluster, so that copy command can engage all worked nodes parallelly and evenly distribute the load.","upvote_count":"1","comment_id":"678425","poster":"Arka_01"},{"upvote_count":"1","poster":"renfdo","timestamp":"1663018320.0","content":"Selected Answer: B\nB is the right answer","comment_id":"667476"},{"comment_id":"638495","timestamp":"1658986800.0","upvote_count":"1","content":"Selected Answer: B\nB is the right answer","poster":"rocky48"},{"comment_id":"274266","upvote_count":"1","content":"B is the right answer","poster":"lostsoul07","timestamp":"1636263120.0"},{"content":"B is correct for me","poster":"BillyC","upvote_count":"1","comment_id":"216807","timestamp":"1635934800.0"},{"content":"B is the answer.","poster":"sanjaym","comment_id":"204756","upvote_count":"1","timestamp":"1635321180.0"},{"content":"Option B, By using a single full file forces copy to do a serial load. Splitting the files in multiple of number of slices in cluster and compressing them is ideal for better performance of copy","poster":"Karan_Sharma","timestamp":"1633746600.0","upvote_count":"2","comment_id":"177503"},{"content":"B is the right option.","comment_id":"175273","poster":"Paitan","upvote_count":"2","timestamp":"1633244160.0"},{"comment_id":"160759","poster":"abhineet","content":"B is right","upvote_count":"2","timestamp":"1632997320.0"},{"upvote_count":"5","timestamp":"1632406320.0","content":"Agree it is B.","poster":"testtaker3434","comment_id":"153887"}],"timestamp":"2020-08-09 20:24:00","answer_description":"","answer_images":[],"choices":{"C":"Split the number of files so they are equal to a multiple of the number of compute nodes in the Amazon Redshift cluster. Gzip and upload the files to Amazon S3. Run the COPY command on the files.","A":"Upload the individual files to Amazon S3 and run the COPY command as soon as the files become available.","B":"Split the number of files so they are equal to a multiple of the number of slices in the Amazon Redshift cluster. Gzip and upload the files to Amazon S3. Run the COPY command on the files.","D":"Apply sharding by breaking up the files so the distkey columns with the same values go to the same file. Gzip and upload the sharded files to Amazon S3. Run the COPY command on the files."},"answers_community":["B (100%)"],"question_id":76,"topic":"1","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/27800-exam-aws-certified-data-analytics-specialty-topic-1-question/","isMC":true,"answer":"B"},{"id":"wxx95Y8JgzA89aDhmcOH","question_id":77,"answer_ET":"A","answer_images":[],"topic":"1","isMC":true,"exam_id":20,"timestamp":"2020-08-09 20:38:00","answers_community":["A (88%)","13%"],"url":"https://www.examtopics.com/discussions/amazon/view/27804-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer":"A","question_images":[],"discussion":[{"poster":"JohnWick2020","upvote_count":"40","timestamp":"1633539000.0","content":"Answer is A - \"Use an AD connector and SSO in a corporate environment\",\nKey point of question is to “Secure access from its on-premise AD to Quicksight\".\n• Quicksight Enterprise edition allows for connecting to AD / using AD groups, SSO, row-level security, encryption at rest ... etc","comment_id":"162034"},{"comment_id":"175239","poster":"Paitan","upvote_count":"16","timestamp":"1633556400.0","content":"Option A. Quicksight Enterprise edition allows for connecting through AD connector."},{"comment_id":"1279441","poster":"[Removed]","timestamp":"1725606360.0","upvote_count":"10","content":"Option A. Quicksight Enterprise edition allows for connecting through AD connector."},{"timestamp":"1704812940.0","content":"Considering the need to secure access from an on-premises Active Directory to Amazon QuickSight, Option A (Use an Active Directory Connector and Single Sign-On (SSO) in a Corporate Network Environment) is the most appropriate. It directly addresses the requirement of integrating QuickSight with the company’s Active Directory, allowing for controlled and secure access to QuickSight dashboards using corporate credentials. This approach ensures that access to QuickSight is managed in alignment with the company's existing security policies and user management practices. While it doesn't explicitly mention securing data, integrating QuickSight with SSO and Active Directory is a crucial step in overall access security, especially in a corporate environment.","upvote_count":"1","poster":"fagilom","comment_id":"1117597"},{"comment_id":"1043491","content":"Selected Answer: A\nA for sure","upvote_count":"1","timestamp":"1697290740.0","poster":"gofavad926"},{"comment_id":"970366","upvote_count":"1","content":"Selected Answer: A\nIts a me A","timestamp":"1690993380.0","poster":"MLCL"},{"comment_id":"968508","poster":"NikkyDicky","content":"Selected Answer: A\nit's an A","timestamp":"1690839060.0","upvote_count":"1"},{"comment_id":"918874","poster":"papercome","timestamp":"1686281880.0","content":"Selected Answer: D\nIs this single selection? Both A and D seems correct to me.","upvote_count":"1"},{"poster":"pk349","upvote_count":"2","comment_id":"886249","timestamp":"1682945820.0","content":"A: I passed the test"},{"timestamp":"1682062620.0","poster":"anjuvinayan","upvote_count":"3","comment_id":"876260","content":"S3 cannot be placed inside VPC. Redshift can be placed inside VPC. So here connecting Quicksight to S3 doesn't need anything else. Quicksight enterprise edition allows AD connection and answer is A"},{"content":"A. Use an Active Directory connector and single sign-on (SSO) in a corporate network environment is the correct solution to secure access from an on-premises Active Directory to Amazon QuickSight. This solution allows the users to log in with their existing corporate credentials and enables IT administrators to manage access to Amazon QuickSight using their on-premises Active Directory. Amazon QuickSight supports various authentication methods, including SSO, OpenID Connect, and SAML 2.0, making it easy to integrate with existing authentication solutions.","comment_id":"835895","upvote_count":"2","timestamp":"1678534260.0","poster":"AwsNewPeople"},{"content":"Answer is A \"Use an AD connector and SSO in a corporate environment\"","comment_id":"810923","timestamp":"1676565660.0","upvote_count":"1","poster":"Vicious000"},{"poster":"ran_gun","content":"The question target on How should the data be secured?\nso in this case, it should be B right? option A talks only about the connecting via Active Directory connector; i doubt does it focus on the actual question","timestamp":"1674020580.0","upvote_count":"3","comment_id":"779640"},{"timestamp":"1667480760.0","content":"Correct answer is A as QuickSight access using Active Directory can be implemented using an Active Directory connector and single sign-on (SSO) in a corporate network environment.\nOptions B, C & D are wrong as they do not target the QuickSight authentication requirement but focus on QuickSight access to S3 and Redshift.","poster":"cloudlearnerhere","upvote_count":"2","comment_id":"710510"},{"poster":"Rejju","comment_id":"705203","upvote_count":"3","content":"The Answer seems to be A, but wondering why the answer mentioned as B. it worries me!","timestamp":"1666843020.0"},{"poster":"Hruday","comment_id":"649761","upvote_count":"2","content":"Selected Answer: A\nA is the answer","timestamp":"1661090160.0"},{"upvote_count":"1","timestamp":"1661090100.0","comment_id":"649760","poster":"Hruday","content":"A is the answer"},{"content":"Selected Answer: A\nAnswer is A","upvote_count":"1","poster":"Bik000","timestamp":"1653205380.0","comment_id":"605253"},{"poster":"Shammy45","content":"Selected Answer: A\nActive Directory with Enterprise edition","timestamp":"1652784000.0","upvote_count":"1","comment_id":"602883"},{"content":"Answer - A","timestamp":"1651351560.0","upvote_count":"1","poster":"jrheen","comment_id":"595282"},{"comment_id":"482242","poster":"aws2019","upvote_count":"2","timestamp":"1637378820.0","content":"A is the ans"},{"content":"A should be the answer.\nQuestion is about connecting from on-prem to Quicksight\nhttps://docs.aws.amazon.com/quicksight/latest/user/aws-directory-service.html","upvote_count":"6","poster":"Ashoks","timestamp":"1636214340.0","comment_id":"378473"},{"upvote_count":"2","timestamp":"1636117020.0","content":"The answer is A.","comment_id":"359089","poster":"leliodesouza"},{"timestamp":"1635855180.0","poster":"mcri","comment_id":"316249","upvote_count":"3","content":"My answer is A."},{"comment_id":"293955","timestamp":"1635496020.0","upvote_count":"3","poster":"Pradhan","content":"My answer is A."},{"upvote_count":"5","timestamp":"1635258900.0","content":"A.\n\nB, C. QuickSight doesn't need an VPC endpoint to connect to S3. QuickSight uses a manifest file to connect to S3.\nD. QuickSight doesn't have any security group.","poster":"Exia","comment_id":"284591"},{"poster":"marcelpinheiro","timestamp":"1635228780.0","comment_id":"278829","upvote_count":"4","content":"CORRECT: Option A\n\nUser Management for Standard Edition\nIn Standard edition, you can invite an AWS Identity and Access Management (IAM) user and allow that user to use their credentials to access Amazon QuickSight. Alternatively, you can invite any person with an email address to create an Amazon QuickSight–only user account. When you create a user account, Amazon QuickSight sends email to that user inviting them to activate their account.\n\nUser Management for Enterprise Edition\nIn Enterprise edition, you can select one or more Microsoft Active Directory active directory groups in AWS Directory Service for administrative access. All users in these groups are authorized to sign in to Amazon QuickSight as administrators. You can also select one or more Microsoft Active Directory active directory groups in AWS Directory Service for user access. All users in these groups are authorized to sign in to Amazon QuickSight as users."},{"comment_id":"274205","timestamp":"1635097860.0","poster":"lostsoul07","upvote_count":"1","content":"A is the right answer"},{"upvote_count":"4","comment_id":"259772","timestamp":"1634871240.0","poster":"AdityaB","content":"A is the right answer \nhttps://docs.aws.amazon.com/quicksight/latest/user/directory-integration.html"},{"content":"Answer is A","timestamp":"1634796360.0","upvote_count":"1","poster":"mendelthegreat","comment_id":"255248"},{"content":"A is correct","poster":"BillyC","upvote_count":"1","comment_id":"216842","timestamp":"1634712060.0"},{"comment_id":"190952","upvote_count":"1","timestamp":"1634569680.0","content":"Answer is A\nhttps://docs.aws.amazon.com/quicksight/latest/user/external-identity-providers.html","poster":"syu31svc"},{"content":"Now, i am confused.\nIs the answer B as per examtopics answer of A, or D as per comments here.?","upvote_count":"1","poster":"oluakins","comments":[{"content":"The correct answer is A. In most cases the examtopics answers are not correct. Always go through the discussions and then take your final call.","timestamp":"1634190600.0","comments":[{"upvote_count":"2","timestamp":"1635521700.0","comment_id":"316248","poster":"mcri","content":"Ok, I see through the discussion, but my hope was that \"a super-useres\" someone \"that know\" can be a ligthouse.... So maybe if examtopic found explaination correct can be upgrade the answer or not?\nEvery body can write here... people like me that are studyng and for this reason can accidentally write a wrong conclusion.... so I think that if the moderator con filter only the write answer and upgrade the answer field is better (this is my opinion). Regards"}],"upvote_count":"2","comment_id":"179648","poster":"Paitan"}],"timestamp":"1634006700.0","comment_id":"178773"},{"comment_id":"178759","content":"Option A because of the Active Directory usecase","poster":"tesla_elon","upvote_count":"1","timestamp":"1633983300.0"},{"timestamp":"1633380360.0","poster":"singh100","content":"Answer A. https://aws.amazon.com/blogs/security/how-to-connect-your-on-premises-active-directory-to-aws-using-ad-connector/","comment_id":"161444","upvote_count":"1"},{"timestamp":"1633286100.0","comment_id":"161039","upvote_count":"1","content":"My answer is A: https://docs.aws.amazon.com/singlesignon/latest/userguide/connectonpremad.html","poster":"carol1522"},{"content":"My vote is for A. Please refer to ‘Using Active Directory with Amazon QuickSight’ documentation on AWS website","poster":"AjNapa","upvote_count":"3","comment_id":"160038","timestamp":"1633058700.0"},{"content":"My answer is D","comments":[{"content":"Actually A : https://aws.amazon.com/premiumsupport/knowledge-center/quicksight-enterprise-account/","comment_id":"159495","timestamp":"1632688440.0","upvote_count":"2","poster":"zeronine"}],"upvote_count":"1","poster":"zeronine","comment_id":"159494","timestamp":"1632604980.0"},{"timestamp":"1632510840.0","comment_id":"153897","content":"Agree its B","upvote_count":"3","comments":[{"poster":"testtaker3434","upvote_count":"2","comment_id":"160803","content":"Going through the docs, options A seems a better choice.","timestamp":"1633216920.0"}],"poster":"testtaker3434"}],"choices":{"A":"Use an Active Directory connector and single sign-on (SSO) in a corporate network environment.","D":"Place Amazon QuickSight and Amazon Redshift in the security group and use an Amazon S3 endpoint to connect Amazon QuickSight to Amazon S3.","B":"Use a VPC endpoint to connect to Amazon S3 from Amazon QuickSight and an IAM role to authenticate Amazon Redshift.","C":"Establish a secure connection by creating an S3 endpoint to connect Amazon QuickSight and a VPC endpoint to connect to Amazon Redshift."},"question_text":"A financial company hosts a data lake in Amazon S3 and a data warehouse on an Amazon Redshift cluster. The company uses Amazon QuickSight to build dashboards and wants to secure access from its on-premises Active Directory to Amazon QuickSight.\nHow should the data be secured?","unix_timestamp":1596998280,"answer_description":""},{"id":"fjWEQEM3CtnIgw9b1EYl","question_text":"A large ride-sharing company has thousands of drivers globally serving millions of unique customers every day. The company has decided to migrate an existing data mart to Amazon Redshift. The existing schema includes the following tables.\n✑ A trips fact table for information on completed rides.\n✑ A drivers dimension table for driver profiles.\n✑ A customers fact table holding customer profile information.\nThe company analyzes trip details by date and destination to examine profitability by region. The drivers data rarely changes. The customers data frequently changes.\nWhat table design provides optimal query performance?","url":"https://www.examtopics.com/discussions/amazon/view/28720-exam-aws-certified-data-analytics-specialty-topic-1-question/","isMC":true,"answer_images":[],"choices":{"C":"Use DISTSTYLE KEY (destination) for the trips table and sort by date. Use DISTSTYLE ALL for the drivers table. Use DISTSTYLE EVEN for the customers table.","B":"Use DISTSTYLE EVEN for the trips table and sort by date. Use DISTSTYLE ALL for the drivers table. Use DISTSTYLE EVEN for the customers table.","A":"Use DISTSTYLE KEY (destination) for the trips table and sort by date. Use DISTSTYLE ALL for the drivers and customers tables.","D":"Use DISTSTYLE EVEN for the drivers table and sort by date. Use DISTSTYLE ALL for both fact tables."},"answer_ET":"C","discussion":[{"comments":[{"content":"Not sure how the 2 links gave you this answer! Not saying your suggestion is wrong","poster":"GeeBeeEl","comment_id":"179150","upvote_count":"2","timestamp":"1632943620.0"}],"comment_id":"159228","timestamp":"1632085500.0","poster":"zanhsieh","upvote_count":"38","content":"C.\nDrivers’ data -> ALL, Customer’s data -> EVEN, Trips table -> KEY (destination) & sort by date\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_choosing_dist_sort.html\nhttps://slideshare.net/AmazonWebServices/deep-dive-on-amazon-redshift-80877515"},{"content":"IMO, it should be B.. Reasons: Distributing the data on destination might cause a data skew which we don't want. If there is no clear dist key for a fact, it's better to dist it evenly.","timestamp":"1633629060.0","comment_id":"206014","poster":"jove","upvote_count":"12"},{"content":"Selected Answer: C\nC is right","poster":"NikkyDicky","upvote_count":"1","timestamp":"1690914660.0","comment_id":"969299"},{"timestamp":"1690178520.0","comment_id":"961191","content":"C is the correct answer!","upvote_count":"1","poster":"penguins2"},{"content":"C: I passed the test","upvote_count":"2","timestamp":"1682947320.0","comment_id":"886286","poster":"pk349"},{"timestamp":"1667583420.0","poster":"cloudlearnerhere","comment_id":"711287","content":"Selected Answer: C\nCorrect answer is C as as the trip would be queries on destination and date, the trips table needs a DISTSTYLE KEY (destination) for the trips table and sort by date. As the drivers data rarely changes DISTSTYLE ALL can be applied for the drivers table, which will maintain a copy per node. Also as customers data changes frequently,","upvote_count":"5"},{"timestamp":"1664081160.0","poster":"Arka_01","upvote_count":"8","content":"Selected Answer: C\nUse All Distribution for rarely changing tables, as they are copied to all slices. Use Even distribution to frequently changing and large tables, as Redshift engine can randomly distribute them to different data slices. Use diststyle key for the tables where you know a join key.","comment_id":"678426"},{"upvote_count":"1","timestamp":"1659816840.0","comment_id":"643508","content":"Answer is C","poster":"rocky48"},{"upvote_count":"1","comment_id":"604861","poster":"Bik000","content":"Selected Answer: C\nAnswer is C","timestamp":"1653132720.0"},{"upvote_count":"2","comment_id":"599479","poster":"MWL","content":"Selected Answer: C\nC should be right.\nThe data will be analized by destination. And the requirement doesn't mention that it need to join trip and customer/driver table. So, using DISTSTYLE KEY (destination) for the trips should improve the performance of trip data.\nFor A, there are millons of customers, so using ALL for that will cause too much copy data.","timestamp":"1652173440.0"},{"timestamp":"1641634620.0","upvote_count":"3","content":"I think answer is B. Destination is not as unique a key and can cause skew with key partition. So even for the trips seems right. Drivers is updated rarely, so suitable for All. Cust is updated often so Even is good for it too.","poster":"Shivanikats","comment_id":"519415"},{"comment_id":"402866","poster":"Bambur","upvote_count":"4","timestamp":"1636126200.0","content":"The answer is B. We don't know which key (with high cardinality) to use for fact table for even distribution so we should chose even diststyle and use fields that should be used to access data as sort key.. Rare updated dimension table good candidate for diststyle all, and another one frequently updated should have even diststyle."},{"poster":"Huy","upvote_count":"4","content":"FACT table in DW is central table and will be queried the most. Because we query trips by destination and date therefore the higher cardinality is destination -> use Destination as key. Drivers is dimension table and data is small so can be ALL to speedup the join. Customer data frequently changes and we are not sure which columns should be joined so EVEN is safe.","comment_id":"388070","timestamp":"1635160440.0"},{"comment_id":"386254","timestamp":"1635129360.0","upvote_count":"1","poster":"Donell","content":"Answer C"},{"comment_id":"383748","timestamp":"1635098880.0","poster":"Shraddha","upvote_count":"3","content":"Ans C..This is a textbook question. However, if there is an answer where both customer and driver tables are EVEN, I would go for it. ALL does not quite give benefits over EVEN.\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_choosing_dist_sort.html"},{"poster":"gunjan4392","upvote_count":"1","content":"C seems okay","timestamp":"1634974740.0","comment_id":"383716"},{"upvote_count":"2","content":"C in my opnion is the best choice, because the Trip table is a fact table so it will join with the other tables, like customer and driver tables. The table customer is frequently updated, so in this case ALL is not recommended.","timestamp":"1634780640.0","comment_id":"359524","poster":"ariane_tateishi"},{"comment_id":"274267","poster":"lostsoul07","content":"C is the right answer","timestamp":"1634359260.0","upvote_count":"2"},{"timestamp":"1634343720.0","poster":"saabji","content":"C. ALL only for drivers table. This leaves B & C. KEY better for trips table as most Queries are on selected cols","upvote_count":"2","comment_id":"251074"},{"timestamp":"1633866240.0","poster":"BillyC","content":"C is correct for me","upvote_count":"2","comment_id":"216806"},{"comment_id":"207796","timestamp":"1633765680.0","content":"B and C are identical except for the word (destination)\nThey both have DISTSTYLE ALL for the non changing table. Maybe C is the answer but not for the reasons given","upvote_count":"1","poster":"[Removed]"},{"upvote_count":"1","content":"Answer is C\nFrom link: https://docs.aws.amazon.com/redshift/latest/dg/c_choosing_dist_sort.html\nB and D are ruled out\n\"ALL distribution is appropriate only for relatively slow moving tables; that is, tables that are not updated frequently or extensively.\"","timestamp":"1633396560.0","poster":"syu31svc","comment_id":"191375"},{"timestamp":"1633076400.0","poster":"GeeBeeEl","upvote_count":"1","content":"B and D do not refer to destination and if profitability is to be calculated by destination/region, I assume they are probably incorrect. All options show sorting is by date. DISTYLE ALL good for drivers table, but not customers table -- Driv table does not change, but customers table change, this knocks out A","comments":[{"comment_id":"179162","upvote_count":"3","poster":"GeeBeeEl","timestamp":"1633250220.0","comments":[{"comments":[{"timestamp":"1634747280.0","comment_id":"342278","poster":"sivajiboss","content":"Example for key distribution given in this seem to be antipattern for redshift. It doesnt use the redshift parallelism capability if you load up all data into one node.","upvote_count":"1"}],"timestamp":"1633366620.0","content":"Answer is C --- sorry forgot to conclude","poster":"GeeBeeEl","upvote_count":"2","comment_id":"186292"}],"content":"Here are the references, https://www.matillion.com/resources/blog/aws-redshift-performance-choosing-the-right-distribution-styles/#:~:text=The%20distribution%20style%20is%20how,you%20want%20to%20distribute%20it%E2%80%A6 https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html"}],"comment_id":"179159"},{"poster":"Paitan","timestamp":"1632941340.0","upvote_count":"2","content":"C is the right option here.","comment_id":"175275"},{"poster":"Saaho","content":"It is nowhere mentioned that TRIPS table is being joined with the dimension tables and if at all it is choosing destination as SORT KEY doesn't make sense as we will most likely be joining with other dim tables using driver/cust id.We use KEY for tables which participate on joins on the key specified.\nSo I think B might be the answer","upvote_count":"2","comments":[{"comment_id":"165324","timestamp":"1632832140.0","upvote_count":"2","poster":"Saaho","content":"I agree,Its not mentioned that trips table participates in any join or uses destination as key for join,so even distribution makes sense,B is right"},{"timestamp":"1633057560.0","poster":"GeeBeeEl","upvote_count":"1","content":"Saaho, even though its not mentioned check practices https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html","comment_id":"179154"}],"comment_id":"162306","timestamp":"1632590460.0"},{"comment_id":"160760","timestamp":"1632511860.0","upvote_count":"3","comments":[{"upvote_count":"1","poster":"GeeBeeEl","content":"How did you arrive at this?","timestamp":"1632972240.0","comment_id":"179153"}],"content":"C is my pick","poster":"abhineet"},{"upvote_count":"2","comments":[{"upvote_count":"2","content":"Sorry, I mean C.","comment_id":"161685","poster":"carol1522","timestamp":"1632560940.0"}],"comment_id":"160154","content":"Maybe is B, because is not recommended use distyle ALL when the table is frequently updated.","timestamp":"1632439680.0","poster":"carol1522"},{"content":"my answer is C - DISTSTYLE ALL is a good choice for the dimension that doesn't change often","upvote_count":"2","timestamp":"1632177120.0","poster":"zeronine","comment_id":"159813"}],"topic":"1","question_images":[],"unix_timestamp":1597580940,"question_id":78,"timestamp":"2020-08-16 14:29:00","answer":"C","answers_community":["C (100%)"],"answer_description":"","exam_id":20},{"id":"4ROQMeV3IeJNvxYnaxRh","answer_ET":"B","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/28785-exam-aws-certified-data-analytics-specialty-topic-1-question/","answers_community":["B (100%)"],"exam_id":20,"topic":"1","question_text":"Three teams of data analysts use Apache Hive on an Amazon EMR cluster with the EMR File System (EMRFS) to query data stored within each teams Amazon\nS3 bucket. The EMR cluster has Kerberos enabled and is configured to authenticate users from the corporate Active Directory. The data is highly sensitive, so access must be limited to the members of each team.\nWhich steps will satisfy the security requirements?","choices":{"A":"For the EMR cluster Amazon EC2 instances, create a service role that grants no access to Amazon S3. Create three additional IAM roles, each granting access to each team's specific bucket. Add the additional IAM roles to the cluster's EMR role for the EC2 trust policy. Create a security configuration mapping for the additional IAM roles to Active Directory user groups for each team.","B":"For the EMR cluster Amazon EC2 instances, create a service role that grants no access to Amazon S3. Create three additional IAM roles, each granting access to each team's specific bucket. Add the service role for the EMR cluster EC2 instances to the trust policies for the additional IAM roles. Create a security configuration mapping for the additional IAM roles to Active Directory user groups for each team.","C":"For the EMR cluster Amazon EC2 instances, create a service role that grants full access to Amazon S3. Create three additional IAM roles, each granting access to each team's specific bucket. Add the service role for the EMR cluster EC2 instances to the trust polices for the additional IAM roles. Create a security configuration mapping for the additional IAM roles to Active Directory user groups for each team.","D":"For the EMR cluster Amazon EC2 instances, create a service role that grants full access to Amazon S3. Create three additional IAM roles, each granting access to each team's specific bucket. Add the service role for the EMR cluster EC2 instances to the trust polices for the base IAM roles. Create a security configuration mapping for the additional IAM roles to Active Directory user groups for each team."},"answer_images":[],"question_images":[],"discussion":[{"poster":"jack42","timestamp":"1633829040.0","content":"No doubt its B. If you will have full access on Ec2 instance role and no role match then it will fall back to the default role [When a cluster application makes a request to Amazon S3 through EMRFS, EMRFS evaluates role mappings in the top-down order that they appear in the security configuration. If a request made through EMRFS doesn’t match any identifier, EMRFS falls back to using the service role for cluster EC2 instances.] Also this is tested fully and its more secure then any other options.","comment_id":"196708","upvote_count":"30"},{"timestamp":"1636096560.0","poster":"Shraddha","content":"Ans B :\nThis is a textbook question. Basically you:\n\ncreate a new EMR service role, removing default permission from original service role which is too permissive with s3:*\ncreate some new roles to allow access to respective s3 buckets\nEMRFS by default will assume EMR service role, which means it gets all access to S3, but can be configured to assume an additional role created by user\nTo be able to do that, user-created roles needs to trust EMR service role (because EMRFS will assume that role first)\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-emrfs-iam-roles.html","comment_id":"383752","upvote_count":"11"},{"poster":"tsangckl","content":"Bling choose A \nfor the below explanation \nOption A is correct because it ensures that each team only has access to its own S3 bucket. By creating a service role that grants no access to Amazon S3 for the EMR cluster EC2 instances, you prevent unauthorized access. Then, by creating additional IAM roles that grant access to each team’s specific bucket and adding these roles to the EMR role for the EC2 trust policy, you ensure that each team can access only its own data. Finally, by creating a security configuration mapping for the additional IAM roles to Active Directory user groups for each team, you ensure that only the members of each team can access their own data.\nOther options are not the best solutions for this scenario. For example, Options B, C, and D involve adding the service role for the EMR cluster EC2 instances to the trust policies for the additional IAM roles or the base IAM roles, which could potentially allow unauthorized access to the S3 buckets.","timestamp":"1711428480.0","comment_id":"1183025","upvote_count":"1"},{"upvote_count":"1","poster":"NikkyDicky","timestamp":"1690914900.0","comment_id":"969301","content":"Selected Answer: B\nB is right"},{"content":"B: I passed the test","poster":"pk349","upvote_count":"2","timestamp":"1682947380.0","comment_id":"886287"},{"comment_id":"711292","content":"Selected Answer: B\nCorrect answer is B as the EMR service role should be provided with no access and the mapping defined for security configuration for using IAM roles mapped to groups.\n\nOption A is wrong as the service role for the EMR cluster EC2 instances should be updated to the trust policies for the additional IAM roles.\n\nOptions C & D are wrong as the EMR service role should have no access.","upvote_count":"5","poster":"cloudlearnerhere","timestamp":"1667583780.0"},{"timestamp":"1665367200.0","comment_id":"690646","poster":"rav009","upvote_count":"1","content":"Selected Answer: B\nB\nB is right, the service role need assume the additional roles, which means add it to the trust policy of the additional roles.\nA is the opposite."},{"comment_id":"640258","timestamp":"1659307380.0","poster":"Grimreaper69","content":"isnt b and c the same?","comments":[{"content":"B - create a service role that grants no access to Amazon S3.\nC- create a service role that grants FULL access to Amazon S3\nB is the right choice","comment_id":"642174","poster":"rudramadhu","timestamp":"1659590700.0","upvote_count":"2"}],"upvote_count":"1"},{"poster":"rocky48","timestamp":"1658805420.0","upvote_count":"1","content":"Selected Answer: B\nAnswer B","comment_id":"637093"},{"upvote_count":"1","content":"Selected Answer: B\nAnswer is B","comment_id":"604822","timestamp":"1653130200.0","poster":"Bik000"},{"comment_id":"482901","timestamp":"1637450940.0","upvote_count":"1","poster":"aws2019","content":"Answer B"},{"timestamp":"1636201800.0","comment_id":"386260","poster":"Donell","upvote_count":"2","content":"Answer B"},{"comment_id":"285863","upvote_count":"1","content":"When a cluster application makes a request to Amazon S3 through EMRFS, EMRFS evaluates role mappings in the top-down order that they appear in the security configuration. If a request made through EMRFS doesn’t match any identifier, EMRFS falls back to using the service role for cluster EC2 instances. For this reason, we recommend that the policies attached to this role limit permissions to Amazon S3. For more information, see Service Role for Cluster EC2 Instances (EC2 Instance Profile).","poster":"Antfoot","timestamp":"1635964200.0"},{"poster":"Exia","comment_id":"285296","timestamp":"1635611280.0","content":"C. We need additional IAM roles.\n\nA, B. If EC2 service role has no access to Amazon S3, no one on this EC2 can access S3 at any level. Besides, S3 deny unauthorized access by default.\nD. We need additional IAM roles.","upvote_count":"4"},{"comment_id":"274269","content":"B is the right answer","upvote_count":"3","timestamp":"1635407100.0","poster":"lostsoul07"},{"timestamp":"1635305160.0","comment_id":"272303","content":"Would go with A.\nBy default, no privilege given in the \"default\" instance profile.\nPrivileges are given through dedicated roles and policies for each domain.\nThen allow these roles to be assumed by the EMR Service Role.\nEMR will then use the appropriate IAM roles based on to the role mapping definition.","poster":"lvi","upvote_count":"1"},{"comment_id":"260203","content":"B is correct as of https://aws.amazon.com/de/blogs/big-data/build-a-multi-tenant-amazon-emr-cluster-with-kerberos-microsoft-active-directory-integration-and-emrfs-authorization/","upvote_count":"4","timestamp":"1634981820.0","poster":"blubb"},{"timestamp":"1634841360.0","poster":"Draco31","comment_id":"237214","upvote_count":"4","content":"Go for B. \nC and D grants full access to EMR so dropped.\nA will allow the 3 additionnal IAM role to assume the EMR Role wich has no access to S3, so useless. \nB however will allows EMR role to assume the 3 additionnal IAM role with different bucket access"},{"comment_id":"216797","upvote_count":"4","timestamp":"1634413320.0","content":"B is correct! SURE","poster":"BillyC"},{"poster":"passtest100","comment_id":"215275","comments":[{"poster":"liyungho","timestamp":"1634526660.0","upvote_count":"3","comment_id":"233663","content":"The difference between A and B is whether we add the IAM role to the trust policies for EMR roles or we add the EMR roles to the trust policy for the IAM role. According to https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html, in the \"How do roles for EC2 instances work?\" section, it said \"The administrator uses IAM to create the Get-pics role. In the role's trust policy, the administrator specifies that only EC2 instances can assume the role.\" It seems that B is the right approach."}],"upvote_count":"1","timestamp":"1634361960.0","content":"SHOULD BE A:\nAccording to the links: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-emrfs-iam-roles.html\n1 the default service role for the ec2 should be limited in any case that the IAM role does not match any request. \n2 to use the IAM role to access the S3 through EMRFS is to put the IAM role into the trust policy of EMRFS. \nOption A works for this."},{"content":"Yes B makes sense after reading this (https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-iam-role-for-ec2.html); its ok if you have no access to EMR service role. IAM role take care of read /write access to S3 and other AWS Services","timestamp":"1634127900.0","comment_id":"213974","poster":"PareshRane","upvote_count":"4"},{"timestamp":"1633815360.0","comment_id":"191381","poster":"syu31svc","upvote_count":"1","content":"A and B make no sense to me as you are denying access to S3\nC makes logical sense as you are creating policies for newly created roles"},{"timestamp":"1632654540.0","comment_id":"175514","poster":"Paitan","content":"I will go with option B.","upvote_count":"2"},{"content":"Answer is B according to https://docs.amazonaws.cn/en_us/emr/latest/ManagementGuide/emr-emrfs-iam-roles.html","upvote_count":"3","comments":[{"upvote_count":"1","content":"The link you quoted did not suggest B at all, A and B grants no access to S3. EMRFS uses data in S3, how is it supposed to get access to data if service role does not grant access to S3","poster":"GeeBeeEl","timestamp":"1633310520.0","comment_id":"179318","comments":[{"poster":"Paitan","upvote_count":"6","comment_id":"179668","timestamp":"1633360920.0","content":"Granting no access to S3 doesn't mean its an explicit deny. Since the service role is attached to the three individual team roles each of which individually has the required access to their respective buckets, the requirement will be met."},{"poster":"Ikram_BN","comment_id":"180960","timestamp":"1633774740.0","content":"Create three additional IAM roles, each granting access to each team's specific bucket.","upvote_count":"1"}]}],"timestamp":"1632603900.0","poster":"awssp12345","comment_id":"164661"},{"timestamp":"1632602160.0","comment_id":"160772","content":"looks like B, tough one","poster":"abhineet","upvote_count":"2"},{"comments":[{"comment_id":"179313","timestamp":"1633167420.0","upvote_count":"1","poster":"GeeBeeEl","content":"No, not B at all....... A and B grants no access to S3"}],"content":"I think B.. Any thoughts?","upvote_count":"3","comment_id":"159876","poster":"zeronine","timestamp":"1632553080.0"},{"timestamp":"1632297900.0","comment_id":"159595","poster":"cloud4gr8","comments":[{"poster":"GeeBeeEl","timestamp":"1632967560.0","upvote_count":"1","content":"No, not B at all....... A and B grants no access to S3","comment_id":"179312"}],"upvote_count":"2","content":"Is it B? any comments?\nAs C says granting full permissions to S3."}],"answer":"B","timestamp":"2020-08-17 05:26:00","question_id":79,"unix_timestamp":1597634760,"answer_description":""},{"id":"VVv1QrcLAXHlglFW1Q6C","timestamp":"2020-08-09 20:36:00","answer_ET":"ACE","answers_community":["ACE (88%)","6%"],"isMC":true,"question_images":[],"question_text":"A company is planning to create a data lake in Amazon S3. The company wants to create tiered storage based on access patterns and cost objectives. The solution must include support for JDBC connections from legacy clients, metadata management that allows federation for access control, and batch-based ETL using PySpark and Scala. Operational management should be limited.\nWhich combination of components can meet these requirements? (Choose three.)","topic":"1","question_id":80,"choices":{"F":"Amazon EMR with Apache Hive, using an Amazon RDS with MySQL-compatible backed metastore","D":"Amazon EMR with Apache Hive for JDBC clients","C":"AWS Glue for Scala-based ETL","E":"Amazon Athena for querying data in Amazon S3 using JDBC drivers","B":"Amazon EMR with Apache Spark for ETL","A":"AWS Glue Data Catalog for metadata management"},"answer_description":"","exam_id":20,"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/27803-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer":"ACE","discussion":[{"comment_id":"155695","poster":"Prodip","timestamp":"1632377100.0","upvote_count":"48","content":"I will go with A,C,E . Glue can do both pyspark and scala based ETL. Glue for Metadata and JDBC drivers to connect Athena from outside of AWS. \nServer less . so, Operational management is limited","comments":[{"upvote_count":"4","timestamp":"1633381380.0","poster":"abhineet","comment_id":"160775","content":"ya i thought so too, ACE for me"}]},{"content":"Each word has meaning, So I will go with ABD, A metadata management that allows federation for access control, B- batch-based ETL using PySpark, D-JDBC connections from legacy clients. Not-C- because it mentioned only scala but questions mentioned scala operation is limited, E- you need JDBC to connect clinet not the Athena","upvote_count":"6","comments":[{"timestamp":"1634293920.0","poster":"Mahesh22","upvote_count":"1","content":"Correct. ABD is right","comment_id":"199749"},{"timestamp":"1634839920.0","comment_id":"207376","poster":"vanireddy","upvote_count":"1","content":"I agree with this. Correct is ABD."},{"comments":[{"upvote_count":"1","comment_id":"734884","poster":"shammous","timestamp":"1670136120.0","content":"I mean AWS Glue (not ETL) is a serverless service and you don't need to provision it."}],"content":"EMR=Operationd overhead. ETL does it all and it is a managed service. ACE is better answer","upvote_count":"1","comment_id":"734880","poster":"shammous","timestamp":"1670135820.0"},{"content":"if we select B,D (EMR-spark,Hive-jdbc),does it not make more sense to use Emr-Hive-datastore(F),instead of glue-catalog(A),limiting operational management.\n- making BDF more appropriate.","upvote_count":"1","timestamp":"1667801580.0","poster":"abgz887","comment_id":"712836"}],"timestamp":"1634084640.0","poster":"jack42","comment_id":"195317"},{"content":"Bing\nOption A is correct because AWS Glue Data Catalog provides a unified metadata repository across a variety of data sources and data formats, and it integrates with Amazon S3, Amazon RDS, Amazon Athena, Amazon Redshift, and others.\nOption B is correct because Amazon EMR with Apache Spark supports PySpark and Scala for batch-based ETL processing.\nOption E is correct because Amazon Athena supports SQL queries and can be integrated with JDBC drivers, allowing legacy clients to execute queries.","upvote_count":"1","timestamp":"1711429020.0","comment_id":"1183034","poster":"tsangckl"},{"timestamp":"1708506960.0","upvote_count":"1","content":"Selected Answer: ABE\nI will go with A. AWS Glue Data Catalog, B.Amazon EMR with Apache Spark, E. Amazon Athena aligns well with the company's requirements for a data lake architecture, offering a balance of performance, cost-efficiency, and ease of management.\n\nWhile C is also a viable option for ETL processes, it's more aligned with serverless ETL jobs and might not be as flexible for Scala as Amazon EMR with Apache Spark. D and F could provide JDBC connectivity and metadata management but are more operationally intensive and less integrated with S3 tiered storage strategies compared to using Athena with the Glue Data Catalog.","poster":"NarenKA","comment_id":"1155384"},{"upvote_count":"1","content":"Why are we saying C ? C just says \"Scala\" ETL, even though Glue supports both pyspark and scala and AWS managed, the option specifically mentions \"Scala based\". Requirement is for both Scala and Pyspark that directly points to EMR. answer should be ABE.. about operational management, it says \"limited\", and EMR can qualify with it. using glue there is 'no' operational overhead.","poster":"geekfrosty","comment_id":"979709","timestamp":"1691895000.0"},{"upvote_count":"1","comment_id":"969303","poster":"NikkyDicky","content":"Selected Answer: ACE\nACE it","timestamp":"1690915020.0"},{"timestamp":"1682947440.0","content":"ACE: I passed the test","poster":"pk349","upvote_count":"3","comment_id":"886289"},{"poster":"cloudlearnerhere","content":"Selected Answer: ACE\nCorrect answers are A, C & E\n\nOption A as Glue Data Catalog provides metadata management with the federation for access control.\n\nOption C as AWS Glue supports both serverless PySpark and Scala-based ETL with the least operational overhead.\n\nOption E as Athena can be used for querying S3 data. Athena can be connected using JDBC drivers from the external legacy clients.\n\nOptions B, D & E is wrong as using EMR and RDS would increase the operational management and cost.","comment_id":"711294","timestamp":"1667583840.0","upvote_count":"5"},{"timestamp":"1664081520.0","content":"Selected Answer: ACE\nAs operation management should be less, so all EMR related options are invalid, as EMR needs management of underlying EC2 instances","comment_id":"678432","upvote_count":"2","poster":"Arka_01"},{"comment_id":"660755","content":"Selected Answer: ACE\nA. *Less* operational overhead compared to F (selected)\nB. High operational overhead, when compared to \"C\" AWS Glue based Scala\nC. *Less* operational overhead compared to \"B\" EMR PySpark (selected)\nD. Higher operational overhead when compared to \"E\" Athena. https://docs.aws.amazon.com/emr/latest/ReleaseGuide/HiveJDBCDriver.html\nE. *Less* operational overhead when compared to \"D\" EMR Hive JDBC (selected)\nF. Higher operational overhead when compared to \"A\" Glue metadata","upvote_count":"2","timestamp":"1662434520.0","poster":"Abep"},{"poster":"rocky48","content":"Selected Answer: ACE\nI will go with A,C,E","comment_id":"643502","timestamp":"1659816360.0","upvote_count":"2"},{"content":"Selected Answer: ABC\nD and F are wrong because the question never mentions Hive. E is not right, since Athena don't need JDBC to query S3. C is right because AWS Glue can be used for Scala-based ETL. \nA is right because Glue can connect on-premises DB through JDBC. https://aws.amazon.com/blogs/big-data/how-to-access-and-analyze-on-premises-data-stores-using-aws-glue/. B is right because Apache Spark can support PySpark.","poster":"GarfieldBin","timestamp":"1656054900.0","comment_id":"621483","upvote_count":"1"},{"comment_id":"604859","timestamp":"1653132600.0","poster":"Bik000","upvote_count":"3","content":"Selected Answer: ACE\nMy Answer is A, C & E"},{"timestamp":"1644736800.0","poster":"Japanese1","content":"D, F are clearly wrong.\nD : JDBC connection from older clients is required, NOT from Athena.\nF : It is redundant to use RDS as a metastore. And there is no requirement for MySQL-compatible backed metastore.\nI'm torn between B and C. B is predominant in terms of cost constraints, but I am doubtful.","comment_id":"546278","upvote_count":"1"},{"content":"Answer: A,C,E \nEMR has operational overhead.","upvote_count":"2","comment_id":"386266","timestamp":"1636069200.0","poster":"Donell"},{"content":"Answer: A,C,E \nEMR has operational overhead.","upvote_count":"3","poster":"Donell","timestamp":"1635984600.0","comment_id":"386265"},{"timestamp":"1635979440.0","comment_id":"383815","poster":"Shraddha","upvote_count":"4","content":"Ans - ACE\nNote: This is a free score question. Anything EMR comparing to serverless Glue / Athena is operational overhead. Also remember Glue can do PySpark and Scala, and Athena can do JDBC."},{"timestamp":"1635851820.0","poster":"leliodesouza","upvote_count":"1","comment_id":"360930","content":"A, C and E are the correct answers."},{"poster":"ariane_tateishi","timestamp":"1635753360.0","content":"A,C,E should be the right answer, considering the two requisites: \"The company wants to create tiered storage based on access patterns and cost objectives\" and \"minimize the operational management\".","comment_id":"359679","upvote_count":"3"},{"comment_id":"274270","timestamp":"1635661440.0","poster":"lostsoul07","upvote_count":"3","content":"A, C, E is the right answer"},{"content":"I think it should be A,C,E","upvote_count":"1","timestamp":"1635595740.0","poster":"DAQIANG","comment_id":"247286"},{"comment_id":"236025","timestamp":"1635459720.0","upvote_count":"1","poster":"SumaD2020","content":"I will also go with A, C, E. Anyone gave test recently?"},{"upvote_count":"1","content":"I think its ACE. The question requires one combined solution. So its Either a solution with Athena or with EMR/Hive. It is not both.","poster":"hans1234","timestamp":"1635319980.0","comment_id":"226727"},{"poster":"BillyC","upvote_count":"1","content":"A, C and E are correct for me","timestamp":"1635126060.0","comment_id":"216796"},{"poster":"jove","content":"IMO, they should be ACE","upvote_count":"1","comment_id":"206539","timestamp":"1634794740.0"},{"timestamp":"1634734320.0","poster":"sanjaym","content":"I think it should be ACD and NOT ACE. because Athena doesn't need JDBC to connect S3.","comment_id":"204779","comments":[{"upvote_count":"1","comment_id":"210955","timestamp":"1635015120.0","poster":"jove","content":"No it does not but legacy clients need support for JDBC connections to connect Athena. The requirement is \"The solution must include support for JDBC connections from legacy clients\""}],"upvote_count":"1"},{"timestamp":"1634688600.0","comments":[{"comment_id":"206538","poster":"jove","content":"If you have a legacy client, Athena offers a JDBC connection : https://docs.aws.amazon.com/athena/latest/ug/connect-with-jdbc.html","upvote_count":"2","timestamp":"1634763780.0"},{"content":"The sentence should be \"...pyspark and scala. Operational management...\", rather than \" .. scala operational management ...\". There should be a period \".\" between the two words scala and Operational, or the first word \"O\" of \"Operational\" should not be capital.","comment_id":"233679","upvote_count":"1","timestamp":"1635382620.0","poster":"liyungho"}],"comment_id":"204726","content":"option E says \"Amazon Athena for querying data in Amazon S3 using JDBC drivers\" - i dont think athena queries S3 using JDBD. \nFor JDBC have to rely on Apache Hive... I would go with ABD..\nalso question says \"Scala Operational management should be limited\" so i drop C","poster":"Warrior001","upvote_count":"1"},{"timestamp":"1634056560.0","poster":"askblr","upvote_count":"1","content":"Confused between ABE and ACE. Why Option \"B\" is ruled out? JDBC is supported for both Hive and Athena connectivity rite?","comment_id":"193296"},{"poster":"syu31svc","comment_id":"191382","upvote_count":"1","timestamp":"1634029200.0","content":"ACE\nIf ETL is concerned then Glue would be the preferred choice\nLink https://docs.aws.amazon.com/athena/latest/ug/connect-with-jdbc.html mentions \"You can use a JDBC connection to connect Athena to business intelligence tools and other applications, such as SQL Workbench. To do this, download, install, and configure the Athena JDBC driver, using the following links on Amazon S3\""},{"timestamp":"1633641240.0","poster":"Karan_Sharma","upvote_count":"1","content":"Option A,C,E: Glue for batch ETL - it can do both scala & python. Glue datacatalog for metadata. Athena for JDBC access, it works on S3 so tiered based storage can be enabled.","comment_id":"179068"},{"timestamp":"1633545480.0","content":"A, C and E for me. AWS Glu is one of Amazon's most preferred service in data analytics.","upvote_count":"1","comment_id":"175516","poster":"Paitan"},{"upvote_count":"1","content":"B is incorrect because you will need to write and manage scala code","comment_id":"160777","timestamp":"1633421460.0","poster":"abhineet"},{"poster":"carol1522","comment_id":"160168","upvote_count":"1","timestamp":"1633057860.0","content":"Doubt between ACE or ABE"},{"comment_id":"159869","upvote_count":"1","poster":"zeronine","comments":[{"poster":"zeronine","content":"I meant ACE. B was typo..","timestamp":"1633487640.0","upvote_count":"2","comment_id":"161387"}],"timestamp":"1632471360.0","content":"My answers are : A.B.E"},{"poster":"singh100","content":"I agree with A,C,E.","timestamp":"1632451380.0","upvote_count":"3","comment_id":"159296"},{"poster":"testtaker3434","timestamp":"1632356340.0","content":"Im in doubt between ABD or ABF.\n\nNOT C, I can do the same with B and support PySpark and Scala \nNOT E, because you dont use JDBC to connect with S3.","comment_id":"153895","upvote_count":"3"}],"unix_timestamp":1596998160}],"exam":{"numberOfQuestions":164,"lastUpdated":"11 Apr 2025","provider":"Amazon","id":20,"name":"AWS Certified Data Analytics - Specialty","isMCOnly":true,"isImplemented":true,"isBeta":false},"currentPage":16},"__N_SSP":true}