{"pageProps":{"questions":[{"id":"2JW812quKFpMnrLnw7oq","exam_id":23,"answer_images":[],"discussion":[{"content":"Selected Answer: C\nAgree C is correct","comment_id":"1109396","upvote_count":"6","poster":"PrasannaBalaji","timestamp":"1719713700.0"},{"poster":"c3518fc","upvote_count":"5","timestamp":"1729704120.0","comment_id":"1200870","content":"Selected Answer: C\nThis solution provides the following benefits:\n\nAutomation: The entire process, from code push to testing and deployment, is automated, reducing manual effort and increasing developer velocity.\nIntegration: By using AWS CodePipeline, CodeBuild, and CodeDeploy, you leverage fully managed services that are designed to work together seamlessly.\nIncremental Deployment: The CodeDeployDefault.LambdaLinear10PercentEvery3Minutes option ensures a smooth and controlled migration of traffic to the new versions of your microservices, minimizing the risk of downtime or disruption."},{"timestamp":"1726494300.0","content":"Selected Answer: C\nC is correct","comment_id":"1175059","poster":"DanShone","upvote_count":"3"},{"upvote_count":"3","poster":"Shasha1","content":"C \nThere is no 'pre-commit' hook option in the Lambda deployment hook (Canary); only 'before allowing traffic' and 'after allowing traffic' options are available. Therefore, the 'LambdaLinear10PercentEvery3Minutes' option, which is a canary deployment method, enables a linear deployment strategy, gradually shifting traffic to the new versions at a rate of 10% every 3 minutes.\nhttps://medium.com/@Da_vidgf/canary-deployments-in-serverless-applications-b0f47fa9b409","comment_id":"1167250","timestamp":"1725627240.0"},{"content":"if it is canary, why not a?","comment_id":"1148408","poster":"Chelseajcole","upvote_count":"1","timestamp":"1723474560.0"},{"content":"A is correct: canary deployment","comment_id":"1143421","timestamp":"1723034880.0","poster":"thanhnv142","upvote_count":"1"},{"upvote_count":"4","content":"Selected Answer: C\nc is correct","comment_id":"1125157","timestamp":"1721228700.0","poster":"twogyt"},{"timestamp":"1720800420.0","upvote_count":"4","content":"Selected Answer: C\nB is wrong, why would you trigger a pipeline when TEST code is pushed","poster":"a54b16f","comment_id":"1120992"},{"timestamp":"1719661800.0","upvote_count":"4","poster":"csG13","comment_id":"1108734","content":"Selected Answer: C\nAnswer is C, canary deployment"},{"poster":"PrasannaBalaji","comment_id":"1108647","content":"Selected Answer: B\nB is correct","upvote_count":"1","timestamp":"1719654780.0"}],"answer_description":"","question_images":[],"question_id":56,"unix_timestamp":1703850780,"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/129671-exam-aws-certified-devops-engineer-professional-dop-c02/","isMC":true,"question_text":"A company has microservices running in AWS Lambda that read data from Amazon DynamoDB. The Lambda code is manually deployed by developers after successful testing. The company now needs the tests and deployments be automated and run in the cloud. Additionally, traffic to the new versions of each microservice should be incrementally shifted over time after deployment.\n\nWhat solution meets all the requirements, ensuring the MOST developer velocity?","answers_community":["C (96%)","4%"],"timestamp":"2023-12-29 12:53:00","answer_ET":"C","choices":{"C":"Create an AWS CodePipeline configuration and set up the source code step to trigger when code is pushed. Set up the build step to use AWS CodeBuild to run the tests. Set up an AWS CodeDeploy configuration to deploy, then select the CodeDeployDefault.LambdaLinear10PercentEvery3Minutes option.","D":"Use the AWS CLI to set up a post-commit hook that uploads the code to an Amazon S3 bucket after tests have passed Set up an S3 event trigger that runs a Lambda function that deploys the new version. Use an interval in the Lambda function to deploy the code over time at the required percentage.","B":"Create an AWS CodeBuild configuration that triggers when the test code is pushed. Use AWS CloudFormation to trigger an AWS CodePipeline configuration that deploys the new Lambda versions and specifies the traffic shift percentage and interval.","A":"Create an AWS CodePipeline configuration and set up a post-commit hook to trigger the pipeline after tests have passed. Use AWS CodeDeploy and create a Canary deployment configuration that specifies the percentage of traffic and interval."},"answer":"C"},{"id":"zUF2bTLRow4mOCUzrqmK","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/105514-exam-aws-certified-devops-engineer-professional-dop-c02/","question_id":57,"answer_ET":"C","question_images":[],"answer_description":"","topic":"1","choices":{"B":"Set up a NAT gateway. Deploy the EC2 instances to a private subnet. Update the private subnet's route table to use the NAT gateway as the default route.","D":"Create a security group for the application instances and allow only outbound traffic to the artifact repository. Remove the security group rule once the install is complete.","C":"Publish the application artifacts to an Amazon S3 bucket and create a VPC endpoint for S3. Assign an IAM instance profile to the EC2 instances so they can read the application artifacts from the S3 bucket.","A":"Launch the instances in a public subnet with Elastic IP addresses attached. Once the application is installed and running, run a script to disassociate the Elastic IP addresses afterwards."},"answer":"C","discussion":[{"upvote_count":"8","comment_id":"1101209","timestamp":"1703046120.0","content":"Selected Answer: C\nC - in the answer \nThough we can use both B and C , since we only want to download to package at the time of initialization . So there is no need to have continuous access to internet . Therefore, it is cheap and optimal to use S3 .","poster":"z_inderjot"},{"timestamp":"1722959940.0","comment_id":"1261753","upvote_count":"1","poster":"namtp","content":"Selected Answer: C\nC is the correct answer.\nno access to the internet but connect to aws services => private endpoint"},{"upvote_count":"3","timestamp":"1706415600.0","content":"C is correct: all other options utilize internet connections","comment_id":"1133817","poster":"thanhnv142"},{"poster":"harithzainudin","upvote_count":"3","timestamp":"1702190820.0","content":"Selected Answer: C\nC is the correct one.\n\nall other option will allow internet access which is not compliance with the reqs","comment_id":"1092307"},{"timestamp":"1700245980.0","upvote_count":"3","poster":"zolthar_z","content":"Selected Answer: C\nC: Can't be B because with the NAT the EC2 still has internet access","comment_id":"1073537"},{"upvote_count":"2","poster":"robertohyena","comments":[{"poster":"rowanwally","content":"is the dump still valid?","timestamp":"1699916820.0","comment_id":"1069831","upvote_count":"1"}],"comment_id":"1067894","timestamp":"1699712820.0","content":"C is the correct answer.\n\nA B D are not correct.\nKeywords:\n- requires the instances to run with no access to the internet"},{"poster":"bosmanx","content":"Selected Answer: C\nB is incorrect, the new policy is \"no access to the internet\"","timestamp":"1699170900.0","upvote_count":"2","comment_id":"1062711"},{"content":"C is the answer. B would enable internet access from the instance.","poster":"DevopsNoob","comment_id":"1056190","upvote_count":"1","timestamp":"1698496380.0"},{"poster":"Ffida","upvote_count":"1","comment_id":"1020802","timestamp":"1695993120.0","content":"C is correct and B, which is specifically for NAT. in question they have asked that no internet access from the instance, so If we enable NAT then from outside no one can access the instance but internet will be accessible on the instance using NAT."},{"timestamp":"1695574200.0","content":"C is correct\nB: \"instances to run with no access to the internet.\" so you can not use NAT","poster":"ataince","upvote_count":"1","comment_id":"1016036"},{"timestamp":"1693965480.0","upvote_count":"2","poster":"DaddyDee","content":"C is the answer, you can use artifacts in s3 with vpc endpoints. With a gateway endpoint, you can access Amazon S3 from your VPC, without requiring an internet gateway or NAT device for your VPC, and with no additional cost.\nhttps://repost.aws/knowledge-center/ec2-systems-manager-vpc-endpoints\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html","comment_id":"1000084"},{"comment_id":"991560","poster":"rahulsingha2112","timestamp":"1693152600.0","content":"C is correct as solution required no internet access","upvote_count":"1"},{"content":"Correct C.","poster":"ggrodskiy","upvote_count":"1","timestamp":"1691717040.0","comment_id":"978180"},{"comment_id":"918169","content":"Selected Answer: C\nC is the answer. B gives the instances access to the Internet.","upvote_count":"1","poster":"madperro","timestamp":"1686222720.0"},{"upvote_count":"1","timestamp":"1685472060.0","poster":"rdoty","comment_id":"910538","content":"Selected Answer: C\nDef C, all others include access to the internet"},{"poster":"ProfXsamson","content":"This is supposed to be a Choose two answer. BC","timestamp":"1685073360.0","upvote_count":"1","comment_id":"907022"},{"content":"NAT GW for me","timestamp":"1684821120.0","comment_id":"904605","upvote_count":"1","poster":"Akaza"},{"poster":"lunt","comment_id":"902105","timestamp":"1684513200.0","upvote_count":"2","comments":[{"poster":"Fco_Javier","upvote_count":"1","timestamp":"1691657340.0","content":"Indeed, you can connect to a private repository in S3, in which you have previously included the application artifacts, through a VPC endpoints (adding it as a destination in the routing table for traffic destined from the VPC to Amazon S3). This VPC endpoint can be: an \"gateway endpoint\" or an \"interface endpoint\"\n\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html","comment_id":"977444"}],"content":"Selected Answer: C\n\"no access to internet\" > no access means ACD are out.\nPoint of this question to make you think you must have Internet to complete the install, thsi is not true as you can always download the packages & DL from a repo or S3.\nC is correct."},{"content":"Selected Answer: C\nC is a secure way of achieving the scenario and will match the security compliance.","timestamp":"1683523620.0","comment_id":"891831","upvote_count":"1","poster":"ParagSanyashiv"},{"timestamp":"1682986080.0","poster":"haazybanj","content":"Selected Answer: C\nThe correct answer is C.\n\nCreating a VPC endpoint for Amazon S3 allows the EC2 instances to access the application artifacts in the S3 bucket without going through the internet, thus meeting the new security requirement of running the instances with no internet access. Assigning an IAM instance profile to the EC2 instances allows them to read the application artifacts from the S3 bucket.","upvote_count":"1","comment_id":"886841"},{"content":"Selected Answer: C\nThe question states - \"no internet access\". for me that only leaves C.","comment_id":"882593","upvote_count":"2","timestamp":"1682596020.0","poster":"Mail1964"},{"upvote_count":"3","comment_id":"870454","comments":[{"poster":"Dimidrol","comment_id":"877553","content":"Very strange, if you have requirement to run instance without internet you","upvote_count":"1","timestamp":"1682189460.0"}],"timestamp":"1681503420.0","poster":"alce2020","content":"Option B is the best solution as it provides a secure way for the instances to access the internet and the application artifacts without compromising security. By deploying the instances in a private subnet and setting up a NAT gateway, the instances can access the internet through the NAT gateway, which acts as a proxy, while not having direct access to the internet. The NAT gateway allows outbound internet connectivity, but inbound traffic is not allowed, so it complies with the new security rule"},{"poster":"henryyvr","content":"Selected Answer: C\nwith nat gateway we will have access to the internet","timestamp":"1681420260.0","upvote_count":"2","comment_id":"869773"},{"comment_id":"864641","upvote_count":"2","content":"Selected Answer: C\nC for me, with nat gateway we will have access to the internet","timestamp":"1680952920.0","poster":"Dimidrol"},{"content":"Option B is the correct solution to install the application while complying with the new rule.\nBy setting up a NAT gateway and deploying the EC2 instances to a private subnet, the instances will not have direct access to the internet, which satisfies the new security requirement.","poster":"kassem77","timestamp":"1680895680.0","upvote_count":"1","comment_id":"864193"},{"content":"Selected Answer: B\nuse Nat GW to access Internet","poster":"ele","comment_id":"863910","upvote_count":"3","timestamp":"1680873540.0","comments":[{"timestamp":"1685456520.0","upvote_count":"2","comment_id":"910311","content":"No access to the Internet is a requirement. NAT gateway is accessing the Internet. So that invalidates B.","poster":"bcx"}]}],"question_text":"To run an application, a DevOps engineer launches an Amazon EC2 instance with public IP addresses in a public subnet. A user data script obtains the application artifacts and installs them on the instances upon launch. A change to the security classification of the application now requires the instances to run with no access to the internet. While the instances launch successfully and show as healthy, the application does not seem to be installed.\nWhich of the following should successfully install the application while complying with the new rule?","timestamp":"2023-04-07 15:19:00","unix_timestamp":1680873540,"exam_id":23,"answers_community":["C (91%)","9%"],"answer_images":[]},{"id":"tZOywWM5lhO3pjHk9Gfc","answer_images":[],"answers_community":["C (100%)"],"question_images":[],"question_id":58,"topic":"1","answer":"C","url":"https://www.examtopics.com/discussions/amazon/view/129690-exam-aws-certified-devops-engineer-professional-dop-c02/","answer_description":"","timestamp":"2023-12-29 14:57:00","choices":{"B":"Create two AWS CodePipeline configurations for test and production environments. Configure the production pipeline to have a manual approval step. Create a CodeCommit repository for each environment. Set up each CodePipeline to retrieve the source code from the appropriate repository. Set up the deployment step to deploy the Lambda functions with AWS CloudFormation.","D":"Create an AWS CodeBuild configuration for test and production environments. Configure the production pipeline to have a manual approval step. Create one CodeCommit repository with a branch for each environment. Push the Lambda function code to an Amazon S3 bucket. Set up the deployment step to deploy the Lambda functions from the S3 bucket.","C":"Create two AWS CodePipeline configurations for test and production environments. Configure the production pipeline to have a manual approval step. Create one CodeCommit repository with a branch for each environment. Set up each CodePipeline to retrieve the source code from the appropriate branch in the repository. Set up the deployment step to deploy the Lambda functions with AWS CloudFormation.","A":"Configure a new AWS CodePipeline service. Create a CodeCommit repository for each environment. Set up CodePipeline to retrieve the source code from the appropriate repository. Set up the deployment step to deploy the Lambda functions with AWS CloudFormation."},"question_text":"A company is building a web and mobile application that uses a serverless architecture powered by AWS Lambda and Amazon API Gateway. The company wants to fully automate the backend Lambda deployment based on code that is pushed to the appropriate environment branch in an AWS CodeCommit repository.\n\nThe deployment must have the following:\n\n• Separate environment pipelines for testing and production\n• Automatic deployment that occurs for test environments only\n\nWhich steps should be taken to meet these requirements?","answer_ET":"C","discussion":[{"content":"Selected Answer: C\nC is correct: <Separate environment pipelines for testing and production> means codepipeline <code that is pushed to the appropriate environment branch in an AWS CodeCommit repository.> means code CodeCommit\nA: no mention of creating Separate env for test and dev\nB: <Create a CodeCommit repository for each environment> should not do this. We should create a branch for each env\nD: no mention of code pipelines","comment_id":"1143426","poster":"thanhnv142","timestamp":"1723035180.0","upvote_count":"6"},{"upvote_count":"3","content":"Selected Answer: C\nBy creating two CodePipeline configurations, using a single CodeCommit repository with branches for each environment, and deploying Lambda functions with CloudFormation, this solution meets the requirements while following best practices for source code management, continuous delivery, and infrastructure as code.","poster":"c3518fc","comment_id":"1200876","timestamp":"1729704540.0"},{"content":"Selected Answer: C\nC is correct\nFirst, A&B both are in-correct: As a basic policy - do not create a repo for the same code for multiple environments. Always create a branch from the same repo. The strategy is wrong for A&B.\nNow C&D: D uses Lambda function with s3, whereas C uses code pipeline to store and build. Using code pipeline is a smart choice rather than using S3 as a code pipeline that offers better branching strategy and controls. I will go with ‘C”.","comment_id":"1109399","poster":"PrasannaBalaji","timestamp":"1719713940.0","upvote_count":"3"},{"content":"Selected Answer: C\nIt’s C - unique env and also distinct resources in aws codepipeline would result to pull from both repos on every update of either repo.","timestamp":"1719662220.0","upvote_count":"2","poster":"csG13","comment_id":"1108740"}],"isMC":true,"unix_timestamp":1703858220,"exam_id":23},{"id":"oWmJ1z3Rpsu05SqWb1rM","discussion":[{"comment_id":"1108785","content":"Selected Answer: D\nI go with D, simply because with Fargate you have very limited access to the OS.","poster":"csG13","timestamp":"1703860560.0","upvote_count":"7"},{"comment_id":"1179218","content":"Selected Answer: D\nWhile option A is a strong candidate due to its serverless nature and ease of deployment, option D is the most suitable solution given the need for specific software versions, OS-level tuning, and the requirement for a scalable and fault-tolerant infrastructure. Option D provides the necessary control over the software environment and infrastructure configuration, along with the benefits of automation and scalability.","timestamp":"1711024800.0","upvote_count":"6","poster":"CloudHandsOn"},{"comment_id":"1353259","upvote_count":"1","timestamp":"1738996620.0","poster":"ce0df07","content":"Selected Answer: B\nOption B is clearly the preferred option:\n- Uses Elastic Beanstalk web server tier with load balancing\n- Provides managed Tomcat platform\n- Supports configuration files for software installation\n- Includes auto-scaling and health monitoring\n- Uses CodePipeline for automated deployments\n- Appropriate for web applications\nOption A: Containerization created unnecessary overhead and it will turn up more expensive\nOption C: Worker tier is for background processes, not suitable for web applications\nOption D: \n- More manual configuration required\n- Less managed service features\n- No built-in platform support\n- More complex to maintain"},{"comment_id":"1335777","content":"Selected Answer: D\nAWS Fargate does not allow direct tuning of Linux OS-level parameters because it is a fully managed, serverless compute engine for containers. With Fargate, AWS abstracts the underlying infrastructure, including the operating system, and does not expose granular OS-level configurations.","poster":"zijo","upvote_count":"2","timestamp":"1735855680.0"},{"upvote_count":"2","timestamp":"1722914700.0","comment_id":"1261362","content":"Selected Answer: D\nkeywords: \n- specific versions of Apache Tomcat, HAProxy, and Varnish Cache \n- operating system-level parameters \n\nNot A as Fargate is serverless","poster":"jamesf"},{"poster":"didek1986","upvote_count":"5","comment_id":"1197157","timestamp":"1713349380.0","content":"Selected Answer: D\nKey point: The application's operating system-level parameters require tuning"},{"content":"Selected Answer: D\nill go with D","poster":"dkp","comment_id":"1194832","timestamp":"1713000600.0","upvote_count":"3"},{"upvote_count":"3","comment_id":"1194795","comments":[{"comment_id":"1257355","upvote_count":"1","timestamp":"1722243240.0","content":"Me too, I still go for D but seen like new update for AWS Fargate have some change","poster":"jamesf"}],"content":"I doubt this question will come up since both A and D are correct: https://aws.amazon.com/blogs/containers/announcing-additional-linux-controls-for-amazon-ecs-tasks-on-aws-fargate/\n\nthey need to add some words to the question tat force you to choose either A or D","poster":"devakram","timestamp":"1712996340.0"},{"comment_id":"1183348","upvote_count":"1","timestamp":"1711460940.0","content":"Selected Answer: A\nAnswer A","poster":"stoy123"},{"content":"Selected Answer: D\nD - The application's operating system-level parameters require tuning - means option D is the only answer. A,B and C dont allow this os level tuning","comments":[{"poster":"c3518fc","upvote_count":"1","content":"Today we’re excited to announce that customers can now tune Linux kernel parameters in ECS tasks on AWS Fargate. Tuning Linux kernel parameters can help customers optimize their network throughput when running containerized network proxies or achieve higher levels of workload resilience by terminating stale connections. This launch provides parity for ECS tasks launched on AWS Fargate and Amazon EC2 container instances. https://aws.amazon.com/blogs/containers/announcing-additional-linux-controls-for-amazon-ecs-tasks-on-aws-fargate/","comment_id":"1200882","timestamp":"1713893880.0"}],"timestamp":"1710600660.0","poster":"DanShone","upvote_count":"3","comment_id":"1175038"},{"content":"Selected Answer: A\nAWS Fargate can tune linux kernel parameters.\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html","comment_id":"1165241","upvote_count":"1","poster":"dzn","timestamp":"1709518500.0"},{"content":"Selected Answer: D\nD is correct: <The application's operating system-level parameters require tuning> means the user need controls over os-level\nA: AWS Fargate and Docker dont provide os-level controls\nB and C: Beanstalk does not support Varnish Cache and no os-level controls provided","comment_id":"1145551","poster":"thanhnv142","timestamp":"1707489300.0","upvote_count":"5"},{"timestamp":"1706189460.0","upvote_count":"2","poster":"sksegha","content":"Selected Answer: A\nThe question only needs Linux OS, its not asking for any extra customizations. AWS fargate allows you to choose the linux platform - and you can package the specific packages in a docker container.","comment_id":"1131700"},{"content":"Selected Answer: D\nIt need specific versions of Apache, so, B and C are out, it requires OS level customization, so no Fargate (A)","timestamp":"1705083240.0","comment_id":"1120997","poster":"a54b16f","upvote_count":"4"},{"upvote_count":"5","poster":"zolthar_z","content":"Selected Answer: D\nD: you can't customize fargate OS, Also the docker images only ocntains the application software","comment_id":"1109922","timestamp":"1703964720.0"}],"isMC":true,"question_id":59,"answer_images":[],"question_text":"A DevOps engineer wants to find a solution to migrate an application from on premises to AWS. The application is running on Linux and needs to run on specific versions of Apache Tomcat, HAProxy, and Varnish Cache to function properly. The application's operating system-level parameters require tuning. The solution must include a way to automate the deployment of new application versions. The infrastructure should be scalable and faulty servers should be replaced automatically.\n\nWhich solution should the DevOps engineer use?","url":"https://www.examtopics.com/discussions/amazon/view/129698-exam-aws-certified-devops-engineer-professional-dop-c02/","choices":{"A":"Upload the application as a Docker image that contains all the necessary software to Amazon ECR. Create an Amazon ECS cluster using an AWS Fargate launch type and an Auto Scaling group. Create an AWS CodePipeline pipeline that uses Amazon ECR as a source and Amazon ECS as a deployment provider.","B":"Upload the application code to an AWS CodeCommit repository with a saved configuration file to configure and install the software. Create an AWS Elastic Beanstalk web server tier and a load balanced-type environment that uses the Tomcat solution stack. Create an AWS CodePipeline pipeline that uses CodeCommit as a source and Elastic Beanstalk as a deployment provider.","C":"Upload the application code to an AWS CodeCommit repository with a set of .ebextensions files to configure and install the software. Create an AWS Elastic Beanstalk worker tier environment that uses the Tomcat solution stack. Create an AWS CodePipeline pipeline that uses CodeCommit as a source and Elastic Beanstalk as a deployment provider.","D":"Upload the application code to an AWS CodeCommit repository with an appspec.yml file to configure and install the necessary software. Create an AWS CodeDeploy deployment group associated with an Amazon EC2 Auto Scaling group. Create an AWS CodePipeline pipeline that uses CodeCommit as a source and CodeDeploy as a deployment provider."},"unix_timestamp":1703860560,"answer_ET":"D","question_images":[],"exam_id":23,"answers_community":["D (89%)","9%"],"topic":"1","answer":"D","answer_description":"","timestamp":"2023-12-29 15:36:00"},{"id":"dcASUGgUdPCZa651zI9X","timestamp":"2023-12-29 13:03:00","question_text":"A DevOps engineer is using AWS CodeDeploy across a fleet of Amazon EC2 instances in an EC2 Auto Scaling group. The associated CodeDeploy deployment group, which is integrated with EC2 Auto Scaling, is configured to perform in-place deployments with CodeDeployDefault.OneAtATime. During an ongoing new deployment, the engineer discovers that, although the overall deployment finished successfully, two out of five instances have the previous application revision deployed. The other three instances have the newest application revision.\n\nWhat is likely causing this issue?","answer":"D","topic":"1","answer_images":[],"question_id":60,"isMC":true,"question_images":[],"choices":{"B":"A failed AfterInstall lifecycle event hook caused the CodeDeploy agent to roll back to the previous version on the affected instances.","A":"The two affected instances failed to fetch the new deployment.","D":"EC2 Auto Scaling launched two new instances while the new deployment had not yet finished, causing the previous version to be deployed on the affected instances.","C":"The CodeDeploy agent was not installed in two affected instances."},"url":"https://www.examtopics.com/discussions/amazon/view/129672-exam-aws-certified-devops-engineer-professional-dop-c02/","answers_community":["D (92%)","8%"],"exam_id":23,"answer_ET":"D","answer_description":"","unix_timestamp":1703851380,"discussion":[{"comment_id":"1145576","poster":"thanhnv142","content":"Selected Answer: D\nD is correct: In-place deployment search for all available agents at the time of the deployment and update the app version. If there are new instances launched after the search, they would be omitted and they fetch the lastest app version available, which is the previous revision\nA and B: If this happened, the other three would be affected as well\nC: If code deploy agents were not installed, no version would be installed on the two instances","upvote_count":"6","timestamp":"1723207860.0"},{"timestamp":"1722004800.0","comments":[{"poster":"govindrk","comment_id":"1148014","timestamp":"1723454880.0","upvote_count":"1","content":"A. The explanation provided for D summarizes A- The two affected instances failed to fetch the new deployment."}],"poster":"promo286","content":"Selected Answer: D\nD. EC2 Auto Scaling launched two new instances while the new deployment had not yet finished, causing the previous version to be deployed on the affected instances.\n\nExplanation:\n\nIn an EC2 Auto Scaling group, when a deployment is in progress and new instances are launched, they may receive the previous version of the application if the deployment has not yet completed. This is because the new instances join the Auto Scaling group and need to fetch the latest revision during the deployment process. If the deployment has not finished when the new instances are launched, they will fetch the current revision available in the Auto Scaling group, which might be the previous version.","comment_id":"1132663","upvote_count":"5"},{"timestamp":"1734903480.0","poster":"youonebe","content":"Selected Answer: B\nB is correct.\n\nThe AfterInstall lifecycle event is executed after the application revision is installed on the EC2 instance. If an error occurs during this phase, the CodeDeploy agent can trigger a rollback to the previous application version. Given that the deployment was successful for three instances and not for the others, it’s possible that the AfterInstall hook failed on the two instances that still have the previous version. In this case, the CodeDeploy agent would have automatically rolled back to the last successful application revision for those two instances.\n\nwhy D is wrong? new instances launched by Auto Scaling would have the current revision applied during the deployment process, not the previous revision. Therefore, it’s unlikely that the new instances would have the previous version unless something else occurred.","upvote_count":"1","comment_id":"1330587"},{"timestamp":"1723911960.0","content":"Selected Answer: B\n, B. A failed AfterInstall lifecycle event hook caused the CodeDeploy agent to roll back to the previous version on the affected instances seems to be the most plausible explanation. It accounts for the scenario where the deployment was successful overall, but specific instances reverted to the previous application revision due to issues encountered during post-installation steps. It's important for the DevOps engineer to review the deployment logs, especially focusing on lifecycle event hooks and their outcomes, to confirm this hypothesis and take corrective actions.","poster":"kyuhuck","upvote_count":"1","comment_id":"1152746"},{"upvote_count":"4","content":"Selected Answer: D\nonly D makes sense","timestamp":"1720800900.0","comment_id":"1120999","poster":"a54b16f"},{"timestamp":"1719664740.0","poster":"csG13","content":"Selected Answer: D\nGot to be D. The rest choices would impact the entire ASG and not only two out of the five intances.","upvote_count":"5","comment_id":"1108787"},{"content":"Selected Answer: D\nD is correct \nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-auto-scaling.html#troubleshooting-auto-scaling-provision-termination-loo","upvote_count":"3","timestamp":"1719655380.0","comment_id":"1108654","poster":"PrasannaBalaji"}]}],"exam":{"isImplemented":true,"numberOfQuestions":355,"isMCOnly":true,"provider":"Amazon","isBeta":false,"id":23,"name":"AWS Certified DevOps Engineer - Professional DOP-C02","lastUpdated":"11 Apr 2025"},"currentPage":12},"__N_SSP":true}