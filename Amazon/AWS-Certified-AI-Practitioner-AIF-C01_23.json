{"pageProps":{"questions":[{"id":"WmLOy6x3PeHWPczNnClX","answer_description":"","answer_ET":"C","timestamp":"2024-11-01 19:10:00","answers_community":["C (79%)","A (21%)"],"isMC":true,"exam_id":14,"answer":"C","choices":{"C":"Asynchronous inference","A":"Real-time inference","D":"Batch transform","B":"Serverless inference"},"url":"https://www.examtopics.com/discussions/amazon/view/150626-exam-aws-certified-ai-practitioner-aif-c01-topic-1-question/","unix_timestamp":1730484600,"topic":"1","answer_images":[],"question_images":[],"question_id":111,"question_text":"A company uses Amazon SageMaker for its ML pipeline in a production environment. The company has large input data sizes up to 1 GB and processing times up to 1 hour. The company needs near real-time latency.\nWhich SageMaker inference option meets these requirements?","discussion":[{"content":"Selected Answer: C\nReal-Time Inference: Immediate responses for high-traffic, low-latency applications.\n>> Asynchronous Inference: Near real-time for large payloads and longer processing.\nBatch Transform: Large-scale, offline processing without real-time needs.\nServerless Inference: Low-latency inference for intermittent or unpredictable traffic without managing infrastructure.","poster":"jove","upvote_count":"9","comment_id":"1307477","timestamp":"1730827860.0"},{"comment_id":"1558328","poster":"tcl08","content":"Selected Answer: A\nAsynchronous inference processes requests in the background, returning a response ID and allowing the client to check for results later, while real-time inference delivers redictions with minimal delay, suitable for interactive applications","upvote_count":"1","timestamp":"1743963240.0"},{"content":"Selected Answer: C\nExplicação:A inferência assíncrona do Amazon SageMaker é ideal quando:\nOs dados de entrada são grandes (por exemplo, até 1 GB),O tempo de processamento Pode ser longo (até 1 hora por solicitação),E você ainda precisa de respostas com latência razoável, mas não exige resposta instantânea como em APIs síncronas.\nEla permite que você envie a solicitação, continue processando outras tarefas e recupere o resultado quando estiver pronto, o que evita timeouts comuns em inferência síncrona.","upvote_count":"1","timestamp":"1743946140.0","poster":"Rcosmos","comment_id":"1558251"},{"content":"Selected Answer: C\nHere the keyword is near “real time latency “\nAsynchronous inference. queues incoming requests and processes them asynchronously. This option is ideal for requests with large payload sizes (up to 1GB), long processing times (up toAsynchronous Inference one hour), and near real-time latency requirements","comment_id":"1362272","poster":"Amar949499","timestamp":"1740600780.0","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: C\nAmazon SageMaker Asynchronous Inference is a capability in SageMaker AI that queues incoming requests and processes them asynchronously. This option is ideal for requests with large payload sizes (up to 1GB), long processing times (up to one hour), and near real-time latency requirements","timestamp":"1740413040.0","comment_id":"1361105","poster":"Nopnov"},{"upvote_count":"1","content":"Selected Answer: A\nReal-time inference in Amazon SageMaker is designed for low-latency, high-throughput applications where predictions need to be made immediately after data is processed. Since the company requires near real-time latency for their ML pipeline and has processing times of up to 1 hour and input sizes up to 1 GB, real-time inference is the most suitable option.\nWith real-time inference, you can deploy your trained models as an API endpoint and get predictions on demand, ensuring low latency. This is ideal for situations where you need immediate responses after submitting the data.","poster":"JJwin","comment_id":"1357034","timestamp":"1739651460.0"},{"timestamp":"1739494140.0","comment_id":"1356325","poster":"Willdoit","content":"Selected Answer: A\nThe company requires near real-time latency, which means the model needs to respond quickly to inference requests. Real-time inference in Amazon SageMaker is designed for low-latency applications where predictions are needed in milliseconds to seconds. \nC. Asynchronous inference – Useful for large requests that take minutes or hours to process, but it is not real-time.","upvote_count":"2"},{"upvote_count":"2","content":"Selected Answer: C\nAsynchronous inference in Amazon SageMaker is ideal when you have large input data sizes (like the 1 GB mentioned) and relatively long processing times (like up to 1 hour). While real-time inference typically offers lower latency, it may struggle with large datasets or complex models that require more processing time. In contrast, asynchronous inference can handle large inputs and longer processing times without needing immediate results. It processes the data and provides the results later, which might be acceptable if your requirement for near real-time latency can be slightly relaxed (for instance, if results can be retrieved within minutes rather than immediately).","timestamp":"1739324820.0","comment_id":"1355324","poster":"Jessiii"},{"comment_id":"1334524","content":"Selected Answer: C\nC: Asynchronous inference\n\nExplanation:\nAsynchronous inference in Amazon SageMaker is specifically designed to handle large payloads (up to 1 GB) and long processing times (up to 1 hour). It decouples request submission from processing, allowing the client to submit a request and receive a response later when the inference is complete. This makes it suitable for use cases where real-time responses are not strictly required, but near real-time results are needed.","poster":"Moon","timestamp":"1735606560.0","upvote_count":"3"},{"timestamp":"1735111740.0","upvote_count":"2","content":"Selected Answer: C\nWhenever \"near real-time latency\" - asynchronous inference","poster":"Aryan_10","comment_id":"1331419"},{"upvote_count":"3","comment_id":"1320992","timestamp":"1733151780.0","content":"Selected Answer: C\nC is right. \nAmazon SageMaker Asynchronous Inference is a capability in SageMaker that queues incoming requests and processes them asynchronously. This option is ideal for requests with large payload sizes (up to 1GB), long processing times (up to one hour), and near real-time latency requirements. Asynchronous Inference enables you to save on costs by autoscaling the instance count to zero when there are no requests to process, so you only pay when your endpoint is processing requests.","poster":"wmj"},{"timestamp":"1732784640.0","poster":"wangyang_0622","comment_id":"1319128","content":"Selected Answer: A\nI think answer A is the correct one as the customer wants to have real-time inference, right?","upvote_count":"2"},{"comments":[{"comment_id":"1309457","upvote_count":"2","timestamp":"1731246000.0","poster":"cuzzindavid","content":"After looking at this...yes Asynchronous is appropriate"}],"poster":"cuzzindavid","comment_id":"1309456","content":"Key word \"real-time latency\"","timestamp":"1731245880.0","upvote_count":"1"},{"poster":"sachin_koenig","content":"Asynchronous inference\n\nPDF\nRSS\nAmazon SageMaker Asynchronous Inference is a capability in SageMaker that queues incoming requests and processes them asynchronously. This option is ideal for requests with large payload sizes (up to 1GB), long processing times (up to one hour), and near real-time latency requirements. Asynchronous Inference enables you to save on costs by autoscaling the instance count to zero when there are no requests to process, so you only pay when your endpoint is processing requests.","timestamp":"1730628120.0","comment_id":"1306476","upvote_count":"3"},{"upvote_count":"2","content":"Amazon SageMaker Asynchronous Inference would be the appropriate option. Here’s why:\n\n • Handles Large Payloads: Asynchronous Inference is designed to handle large input payloads (up to several GBs) that are typically not suited for real-time, low-latency processing.\n • Long Processing Times: It supports inference requests that can take minutes to hours to complete, making it ideal for models that require significant processing time.\n • Near Real-Time Response: While it does not provide millisecond-level latency like real-time endpoints, it offers a more scalable and efficient solution for near real-time use cases where the response time can range from seconds to minutes.","timestamp":"1730484600.0","poster":"galliaj","comment_id":"1305945"}]},{"id":"4x7iagZkbi6T8yoCKOlG","answer_description":"","answer_ET":"A","timestamp":"2024-11-08 06:47:00","answers_community":["A (100%)"],"isMC":true,"exam_id":14,"answer":"A","choices":{"A":"Implement moderation APIs.","B":"Retrain the model with a general public dataset.","D":"Automate user feedback integration.","C":"Perform model validation."},"url":"https://www.examtopics.com/discussions/amazon/view/150996-exam-aws-certified-ai-practitioner-aif-c01-topic-1-question/","unix_timestamp":1731044820,"topic":"1","answer_images":[],"question_images":[],"question_id":112,"question_text":"A company has built a chatbot that can respond to natural language questions with images. The company wants to ensure that the chatbot does not return inappropriate or unwanted images.\nWhich solution will meet these requirements?","discussion":[{"timestamp":"1739329440.0","poster":"Jessiii","upvote_count":"1","comment_id":"1355395","content":"Selected Answer: A\nA. Implement moderation APIs: Moderation APIs can help screen the content generated by the chatbot to ensure that inappropriate or unwanted images are not returned to users. These APIs can identify and filter out offensive, explicit, or harmful content before it's shown to the user. This is the most direct and effective way to ensure content is appropriate."},{"upvote_count":"1","poster":"Moon","timestamp":"1735683840.0","content":"Selected Answer: A\nA: Implement moderation APIs.\n\nExplanation:\nModeration APIs are designed to detect and filter inappropriate or unwanted content, such as images, text, or videos. By integrating moderation APIs into the chatbot workflow, the company can screen and block inappropriate images before they are returned to users. This ensures compliance with ethical standards and avoids exposing users to harmful or unwanted content.","comment_id":"1335055"},{"upvote_count":"2","comment_id":"1326851","poster":"kyo","content":"Selected Answer: A\nAmazon Rekognition moderation APIs can help you automatically identify and filter inappropriate content in images and videos, reducing the need for manual human review. This can significantly improve efficiency and reduce costs while maintaining high standards of content moderation. For details, please refer to the following document:\nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html","timestamp":"1734266700.0"},{"comment_id":"1309206","poster":"jove","upvote_count":"2","content":"Selected Answer: A\nModeration APIs are designed to filter and flag inappropriate or unwanted content, ensuring that the chatbot does not return harmful or unsuitable images. These APIs can scan images before they are returned to the user and block or flag any content that violates the company’s guidelines.","timestamp":"1731187020.0"},{"comment_id":"1308649","upvote_count":"2","content":"A. Implement moderation APIs.\n\nModeration APIs can help filter out inappropriate or unwanted images by analyzing and moderating content before it is returned to users. This ensures that the chatbot maintains safe and appropriate interactions, reducing the risk of inappropriate images being shown.","timestamp":"1731044820.0","poster":"dehkon"}]},{"id":"ed5xsHn1MRQGN7DV97vn","topic":"1","answer_description":"","answer_ET":"B","isMC":true,"question_text":"An AI practitioner is using an Amazon Bedrock base model to summarize session chats from the customer service department. The AI practitioner wants to store invocation logs to monitor model input and output data.\nWhich strategy should the AI practitioner use?","answer_images":[],"answer":"B","question_images":[],"answers_community":["B (100%)"],"choices":{"C":"Configure AWS Audit Manager as the logs destination for the model.","B":"Enable invocation logging in Amazon Bedrock.","D":"Configure model invocation logging in Amazon EventBridge.","A":"Configure AWS CloudTrail as the logs destination for the model."},"unix_timestamp":1731417960,"question_id":113,"timestamp":"2024-11-12 14:26:00","exam_id":14,"url":"https://www.examtopics.com/discussions/amazon/view/151144-exam-aws-certified-ai-practitioner-aif-c01-topic-1-question/","discussion":[{"poster":"Jessiii","upvote_count":"1","timestamp":"1739329440.0","content":"Selected Answer: B\nB. Enable invocation logging in Amazon Bedrock: Amazon Bedrock provides a built-in option to enable invocation logging. This feature allows you to capture and store detailed logs for model invocations, including input and output data, which helps you monitor and analyze the performance and behavior of the model.","comment_id":"1355396"},{"content":"Selected Answer: B\nB: Enable invocation logging in Amazon Bedrock.\n\nExplanation:\nAmazon Bedrock provides the ability to log model invocations, including input and output data, for monitoring and troubleshooting purposes. By enabling invocation logging in Amazon Bedrock, the AI practitioner can store logs securely and use them to analyze model behavior and performance.","comment_id":"1335057","timestamp":"1735684260.0","poster":"Moon","upvote_count":"1"},{"content":"Selected Answer: B\nB - The question mentions using an Amazon Bedrock base model, and invocation logging is a feature specifically designed for Amazon Bedrock.","comment_id":"1310601","poster":"Blair77","timestamp":"1731417960.0","upvote_count":"2"}]},{"id":"uGznUpRJNCf2CkrdUFUY","question_images":[],"question_id":114,"answer_images":[],"question_text":"A company is building an ML model to analyze archived data. The company must perform inference on large datasets that are multiple GBs in size. The company does not need to access the model predictions immediately.\nWhich Amazon SageMaker inference option will meet these requirements?","answer_ET":"A","url":"https://www.examtopics.com/discussions/amazon/view/151124-exam-aws-certified-ai-practitioner-aif-c01-topic-1-question/","topic":"1","timestamp":"2024-11-11 21:50:00","unix_timestamp":1731358200,"answer_description":"","isMC":true,"answer":"A","exam_id":14,"answers_community":["A (78%)","D (22%)"],"choices":{"A":"Batch transform","C":"Serverless inference","D":"Asynchronous inference","B":"Real-time inference"},"discussion":[{"poster":"Willdoit","content":"Selected Answer: A\nBatch transform is ideal for scenarios where you need to perform inference on large datasets, but the predictions are not needed immediately.","upvote_count":"1","comment_id":"1356354","timestamp":"1739503560.0"},{"poster":"Jessiii","timestamp":"1739329500.0","comment_id":"1355397","upvote_count":"1","content":"Selected Answer: A\nA. Batch transform: This is the best option for performing inference on large datasets that are stored in bulk and do not require immediate access to predictions. Batch transform allows you to process large amounts of data (such as multiple GBs of archived data) in batches, without the need for real-time responses. You can submit data in large volumes, and SageMaker processes the data and returns the results once the job completes."},{"timestamp":"1738189140.0","content":"Selected Answer: A\nA. Batch transform ✅\n\nExplanation:\nBatch Transform is ideal for processing large datasets in bulk when immediate responses are not needed.\nIt supports multiple GB-sized datasets and can handle inference without requiring an endpoint to be always active.\nSince the company is working with archived data and does not need real-time predictions, batch processing is the most efficient and cost-effective choice.","poster":"ExamTopicsPrepare","upvote_count":"1","comment_id":"1348796"},{"content":"Selected Answer: D\nasynchronous inference is the most appropriate choice for the company's specific needs, as it provides a balance between processing large datasets and not requiring immediate results.","upvote_count":"2","comments":[{"poster":"djeong95","content":"Amazon SageMaker Asynchronous Inference is a capability in SageMaker AI that queues incoming requests and processes them asynchronously. This option is ideal for requests with large payload sizes (up to 1GB), long processing times (up to one hour), and near real-time latency requirements. Asynchronous Inference enables you to save on costs by autoscaling the instance count to zero when there are no requests to process, so you only pay when your endpoint is processing requests. \n\nA is more suitable here.","timestamp":"1738588080.0","comment_id":"1350877","upvote_count":"1"}],"comment_id":"1338374","poster":"viejito","timestamp":"1736432940.0"},{"content":"Selected Answer: A\nBatch transform is specifically designed to handle large volumes of data, including datasets that are multiple GBs in size. This aligns perfectly with the company's requirement to perform inference on large datasets.","upvote_count":"3","poster":"Blair77","timestamp":"1731418260.0","comment_id":"1310607"},{"timestamp":"1731358200.0","content":"Selected Answer: A\nInfo on Batch Transform matches up with the details of 'large datsets' and 'don't need projections immediately. https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html","poster":"GriffXX","upvote_count":"1","comment_id":"1310321"}]},{"id":"N3YkUADl0Uz0ZaKdgbMm","timestamp":"2024-11-21 10:50:00","isMC":true,"discussion":[{"content":"Selected Answer: A\nA. Embeddings: Embeddings are numerical representations of words, phrases, or even entire documents. These representations capture the semantic meaning of the real-world objects and concepts and are used by AI and NLP models to improve their understanding of textual information. They help the model interpret and process language in a more meaningful way.","poster":"Jessiii","upvote_count":"1","comment_id":"1355398","timestamp":"1739329500.0"},{"content":"Selected Answer: A\nA. Embeddings\n\nExplanation:\nEmbeddings are numerical representations of real-world objects, words, phrases, or concepts in a continuous vector space. They enable AI and Natural Language Processing (NLP) models to understand and process textual information by capturing the semantic relationships and contextual meanings of words and phrases.","poster":"may2021_r","timestamp":"1735486380.0","upvote_count":"2","comment_id":"1333570"},{"comment_id":"1315748","poster":"eesa","timestamp":"1732182600.0","content":"Explanation:\n\n Embeddings are numerical representations of real-world objects, concepts, or textual data. In AI and NLP, embeddings map words, phrases, or even entire documents to a high-dimensional vector space. This allows models to capture semantic relationships and improve understanding of the textual information","upvote_count":"1"}],"exam_id":14,"question_text":"Which term describes the numerical representations of real-world objects and concepts that AI and natural language processing (NLP) models use to improve understanding of textual information?","topic":"1","answer_description":"","question_id":115,"answer_ET":"A","url":"https://www.examtopics.com/discussions/amazon/view/151750-exam-aws-certified-ai-practitioner-aif-c01-topic-1-question/","choices":{"B":"Tokens","D":"Binaries","C":"Models","A":"Embeddings"},"answers_community":["A (100%)"],"answer_images":[],"answer":"A","question_images":[],"unix_timestamp":1732182600}],"exam":{"isBeta":false,"lastUpdated":"11 Apr 2025","isImplemented":true,"numberOfQuestions":154,"name":"AWS Certified AI Practitioner AIF-C01","id":14,"isMCOnly":false,"provider":"Amazon"},"currentPage":23},"__N_SSP":true}