{"pageProps":{"questions":[{"id":"FtHtUXo2jd73q5lpkC8L","exam_id":33,"question_images":[],"topic":"1","answer_ET":"D","unix_timestamp":1673645100,"url":"https://www.examtopics.com/discussions/amazon/view/95095-exam-aws-certified-solutions-architect-professional-sap-c02/","answers_community":["D (91%)","9%"],"isMC":true,"choices":{"C":"Create a script to automatically gather the server information from the on-premises hosts. Use the AWS CLI to run the put-resource-attributes command to store the detailed server data in AWS Migration Hub. Query the data directly in the Migration Hub console.","D":"Deploy the AWS Application Discovery Agent to each on-premises server. Configure Data Exploration in AWS Migration Hub. Use Amazon Athena to run predefined queries against the data in Amazon S3.","A":"Deploy and configure the AWS Agentless Discovery Connector virtual appliance on the on-premises hosts. Configure Data Exploration in AWS Migration Hub. Use AWS Glue to perform an ETL job against the data. Query the data by using Amazon S3 Select.","B":"Export only the VM performance information from the on-premises hosts. Directly import the required data into AWS Migration Hub. Update any missing information in Migration Hub. Query the data by using Amazon QuickSight."},"answer_description":"","question_id":401,"timestamp":"2023-01-13 22:25:00","answer_images":[],"discussion":[{"timestamp":"1673982720.0","poster":"masetromain","content":"Selected Answer: D\nThe correct answer is D: Deploy the AWS Application Discovery Agent to each on-premises server. Configure Data Exploration in AWS Migration Hub. Use Amazon Athena to run predefined queries against the data in Amazon S3.\n\nHere is why the other choices are not correct:\n\nA. Deploy and configure the AWS Agentless Discovery Connector virtual appliance on the on-premises hosts. Configure Data Exploration in AWS Migration Hub. Use AWS Glue to perform an ETL job against the data. Query the data by using Amazon S3 Select. - AWS Agentless Discovery Connector will help in discovering and inventory servers but it does not provide the same level of detailed metrics as the AWS Application Discovery Agent, it also does not cover process information.","comments":[{"poster":"masetromain","timestamp":"1673982780.0","comments":[{"comment_id":"779248","upvote_count":"5","poster":"masetromain","timestamp":"1673982780.0","content":"D. Deploy the AWS Application Discovery Agent to each on-premises server. Configure Data Exploration in AWS Migration Hub. Use Amazon Athena to run predefined queries against the data in Amazon S3. - This is the correct answer as it covers all the requirements mentioned in the question, it will allow collecting the detailed metrics, including process information and it provides a way to query and analyze the data using Amazon Athena."}],"upvote_count":"6","content":"B. Export only the VM performance information from the on-premises hosts. Directly import the required data into AWS Migration Hub. Update any missing information in Migration Hub. Query the data by using Amazon QuickSight. - It does not cover process information and it's not the best way to collect the required data, it's not efficient and it might miss some important information.\n\nC. Create a script to automatically gather the server information from the on-premises hosts. Use the AWS CLI to run the put-resource-attributes command to store the detailed server data in AWS Migration Hub. Query the data directly in the Migration Hub console. - this solution might not be very reliable and it does not cover process information, also it does not provide a way to query and analyze the data.","comment_id":"779247"}],"upvote_count":"47","comment_id":"779246"},{"content":"Selected Answer: D\nChoosing between A and D. For A, how can S3 select query?","upvote_count":"6","timestamp":"1674045840.0","poster":"icassp","comments":[{"comment_id":"798538","poster":"oatif","upvote_count":"4","content":"I think A is a better solution because the Agentless discovery connector is custom-made for the VMware environment. It will save us time and collect all the necessary data we need. Installing a Discovery agent in every server would be very time-consuming. S3 select allows simple select operations against your raw data. I don't think we need athena for","timestamp":"1675560240.0","comments":[{"comment_id":"1159851","poster":"djeong95","timestamp":"1708958820.0","upvote_count":"1","content":"As written by jainparag1, S3 Select is definitely the wrong solution here. As you said, it only allows for very simple select operations. Athena is a better way to go once you have configured the Migration hub settings correctly."},{"content":"A is horrible. You can write only simple SQLs using S3 select. But here you need a sophisticated solution to query these special metrics. D is satisfying all the requirements.","poster":"jainparag1","upvote_count":"3","comment_id":"1079139","timestamp":"1700815560.0"}]}],"comment_id":"779995"},{"upvote_count":"1","content":"A is correct","timestamp":"1731284820.0","comment_id":"1309754","poster":"Jorkaef"},{"timestamp":"1726561740.0","content":"Selected Answer: D\nIf precise information about each running Process is required, it is necessary to consider using Agent-based Discovery.","comment_id":"1285096","upvote_count":"2","poster":"liuliangzhou"},{"timestamp":"1725094440.0","poster":"amministrazione","upvote_count":"1","content":"D. Deploy the AWS Application Discovery Agent to each on-premises server. Configure Data Exploration in AWS Migration Hub. Use Amazon Athena to run predefined queries against the data in Amazon S3.","comment_id":"1275494"},{"upvote_count":"1","comment_id":"1260605","timestamp":"1722765480.0","poster":"Jason666888","content":"Selected Answer: D\nD for sure"},{"upvote_count":"3","poster":"vip2","content":"Selected Answer: D\nsee https://docs.aws.amazon.com/application-discovery/latest/userguide/what-is-appdiscovery.html\n\nfor VMs hosted on VMware, you can use both the Agentless Collector and Discovery Agent to perform discovery simultaneously. \n\nAgentless Collector captures system performance information and resource utilization for each VM running in the vCenter, regardless of what operating system is in use. However, it cannot “look inside” each of the VMs, and as such, cannot figure out what processes are running on each VM nor what network connections exist. Therefore, if you need this level of detail and want to take a closer look at some of your existing VMs in order to assist in planning your migration, you can install the Discovery Agent on an as-needed basis.","timestamp":"1715754660.0","comment_id":"1211788"},{"comment_id":"1175413","timestamp":"1710632040.0","content":"Selected Answer: D\nD is correct","poster":"gofavad926","upvote_count":"1"},{"upvote_count":"1","timestamp":"1709094540.0","poster":"whichonce","content":"Selected Answer: A\nDefinetely A\n\nhttps://docs.aws.amazon.com/application-discovery/latest/userguide/agentless-collector-data-collected-vmware.html\n\nVmware supports agentless connector with AWS, and data can be imported ove Migration Hub","comment_id":"1161256"},{"poster":"8608f25","comment_id":"1145735","upvote_count":"1","content":"Selected Answer: D\nOption D is the most efficient and streamlined solution for the requirements. Deploying the AWS Application Discovery Agent on each on-premises server allows for detailed collection of server metrics, including CPU usage, RAM usage, operating system details, and running processes. By configuring Data Exploration in AWS Migration Hub, the collected data can be analyzed and queried effectively. Using Amazon Athena for querying enables powerful SQL-based exploration of the data stored in Amazon S3, offering a flexible and scalable way to analyze the migration readiness and planning data.\n\nIt is not option C because, Option C involves creating a custom script to gather server information and using the AWS CLI to store data in AWS Migration Hub. While this approach could potentially work, it requires significant manual effort to develop, deploy, and maintain the scripts across 1,000 servers, which is not ideal for minimizing operational overhead.","timestamp":"1707507120.0"},{"poster":"ninomfr64","upvote_count":"1","content":"Selected Answer: D\nNot A - as AWS Agentless Discovery Connector does not provide processes visibility\nNot B - as Migration Hub Import functionality does not support process datahttps://docs.aws.amazon.com/cli/latest/reference/mgh/put-resource-attributes.html, also I do not see how to query with QuickSight as there is not direct integration with Migration Hub to my knowledge\nNot C - as it seems that put-resource-attributes command does not support process data https://docs.aws.amazon.com/cli/latest/reference/mgh/put-resource-attributes.html \n\nD is correct as Discovery Agent collects the required data including processes, Data Exploration in Migration Hub allows to use Amazon Athena and comes with pre-defined queries as well. https://docs.aws.amazon.com/application-discovery/latest/userguide/explore-data.html","timestamp":"1703581080.0","comment_id":"1105839"},{"content":"Selected Answer: D\nhttps://docs.aws.amazon.com/application-discovery/latest/userguide/explore-data.html","timestamp":"1700914380.0","poster":"edder","upvote_count":"1","comment_id":"1079987"},{"comment_id":"980349","upvote_count":"1","content":"Selected Answer: D\nThe agent-based collector can collect data related to running processes which is not available to the Agentless Collector. \n\nCheck out for yourself in the FAQs:\nhttps://aws.amazon.com/application-discovery/faqs/","timestamp":"1691969460.0","poster":"punkbuster"},{"timestamp":"1691610540.0","content":"Selected Answer: A\nAs far as i learned for VM based envs we can go with agentless. And we can use a OVA image via collect the metrics and so on. im going with A . https://docs.aws.amazon.com/application-discovery/latest/userguide/agentless-data-collected.html","upvote_count":"2","comment_id":"976987","poster":"xplusfb"},{"comments":[{"comment_id":"973023","timestamp":"1691239380.0","poster":"chico2023","upvote_count":"1","content":"=== Agentless Collector\nQ: What data does the Agentless Collector capture?\nThe Agentless Collector is delivered as an Open Virtual Appliance (OVA) package that can be deployed to a VMware host. The type of data collected will depend on the capabilities that you configure. If the credentials are provided to connect to vCenter, the Agentless Collector will collect VM inventory, configuration, and performance history data such as CPU, memory, and disk usage. If credentials are provided to connect to databases such as Oracle, SQL Server, MySQL, or PostgreSQL, the Agentless Collector will collect version, edition, and schema data. Server and database information is uploaded to the Application Discovery Service data store. Database information can be sent to AWS DMS Fleet Advisor for analysis."}],"content":"Selected Answer: D\nAnswer: D\n\nThe requirement: \"the company wants to gather server metrics such as CPU details, RAM usage, operating system information, and running processes.\"\n\nFrom https://aws.amazon.com/application-discovery/faqs/:\n\n=== AWS Application Discovery Service Discovery Agent \nQ: What data does the AWS Application Discovery Service Discovery Agent capture?\nThe Discovery Agent captures system configuration, system performance, running processes, and details of the network connections between systems.","timestamp":"1691239380.0","poster":"chico2023","upvote_count":"1","comment_id":"973022"},{"timestamp":"1691110380.0","upvote_count":"1","content":"Selected Answer: D\nI prefer D","poster":"CuteRunRun","comment_id":"971579"},{"upvote_count":"1","timestamp":"1690988880.0","poster":"ggrodskiy","comment_id":"970314","content":"Correct A.\n\n\nD uses agent-based discovery, which requires installing an agent on each on-premises server. This can be cumbersome and intrusive for a large number of servers. It also does not explain how to use AWS Glue to perform an ETL job against the data."},{"upvote_count":"1","content":"Selected Answer: D\nit's a D","timestamp":"1688306460.0","comment_id":"940950","poster":"NikkyDicky"},{"upvote_count":"5","content":"Selected Answer: D\nInitially, I went for A but the Discovery Connector only seems to collect information from the hypervisor, which excludes memory usage, processes etc. So I end up with D. Note to myself and a reminder to everyone - read the questions carefully, this is not associate exam.","poster":"Maria2023","timestamp":"1687262760.0","comment_id":"928438"},{"upvote_count":"1","timestamp":"1687112760.0","content":"Selected Answer: A\nThe key is the VMWare environment, for that the obvious solution is A. IMHO.","poster":"bcx","comment_id":"926851"},{"poster":"mfsec","comment_id":"853131","upvote_count":"2","content":"Selected Answer: D\nD is the answer because agentless cant grab everything","timestamp":"1680001080.0"},{"poster":"dev112233xx","upvote_count":"5","comment_id":"840437","timestamp":"1678926360.0","content":"Selected Answer: D\nA is wrong.. because Agentless can’t collect processes .. only CPU/RAM and disk IO"},{"timestamp":"1677930660.0","comments":[{"upvote_count":"2","poster":"Ajani","comment_id":"828891","content":"Going with D; Agentless discovery Connector does not gather process information; \"THE\" on premises HOSTs(physical servers?) will be running on esxi server.\nYou can deploy Discovery agent on Server(VM) . I might be overthinking it.","timestamp":"1677931440.0"}],"poster":"Ajani","content":"If you have virtual machines (VMs) that are running in the VMware vCenter environment, you can use the Agentless Collector to collect system information without having to install an agent on each VM. Instead, you load this on-premises appliance into vCenter and allow it to discover all of its hosts and VMs.\n\nAgentless Collector captures system performance information and resource utilization for each VM running in the vCenter, regardless of what operating system is in use. However, it cannot “look inside” each of the VMs, and as such, cannot figure out what processes are running on each VM nor what network connections exist.","upvote_count":"1","comment_id":"828871"},{"upvote_count":"2","timestamp":"1677589980.0","content":"Selected Answer: D\nWith the agentless collector you cannot get running processes on the VMs, and you cannot export the data to CSV or to Athena for further querying","comment_id":"824823","poster":"sambb"},{"upvote_count":"2","poster":"God_Is_Love","timestamp":"1677339360.0","comment_id":"821596","content":"Even though question does not ask for least operational effort, performance, HA etc, the solution needs to be thinking those in mind. deploying on each server is not practically good solution. So D cannot be answer. Instead, an appliance which does this discovery job is good which is right there in A. Moreover A is exclusively for VMWare use case. I choose A"},{"comments":[{"comment_id":"819902","content":"S3 Select supports querying one file at a time. With Amazon Athena, you can perform SQL against any number of objects, or even entire bucket paths.","upvote_count":"2","poster":"c73bf38","timestamp":"1677195900.0"}],"content":"Selected Answer: A\nAnswer is A.\nThe AWS Agentless Discovery Connector is used when performing migration of servers in vmware clusters. S3 Select can be used to query.\nAWS SA's would only recommend installing the agent on each on-prem server for physical hosts, not vmware server.","timestamp":"1677148800.0","poster":"monkeyfish","comment_id":"819077","upvote_count":"1"},{"upvote_count":"3","timestamp":"1674967560.0","content":"D will be correct in my opinion.","poster":"pravi1","comment_id":"791396"},{"upvote_count":"3","content":"D Since Agentless Collector can't collect process https://docs.aws.amazon.com/application-discovery/latest/userguide/what-is-appdiscovery.html","poster":"silkroad78","comment_id":"778768","timestamp":"1673949600.0","comments":[]},{"timestamp":"1673645100.0","poster":"masetromain","comment_id":"774871","upvote_count":"3","content":"Selected Answer: A\nThe correct solution is A. Deploy and configure the AWS Agentless Discovery Connector virtual appliance on the on-premises hosts. Configure Data Exploration in AWS Migration Hub. Use AWS Glue to perform an ETL job against the data. Query the data by using Amazon S3 Select.\n\nThis solution allows the company to gather detailed server metrics from the on-premises hosts by deploying the Agentless Discovery Connector virtual appliance. The data can then be imported into AWS Migration Hub for further analysis. The company can then use AWS Glue to perform an ETL job on the data and query it using Amazon S3 Select for further analysis."}],"answer":"D","question_text":"A company is planning to migrate 1,000 on-premises servers to AWS. The servers run on several VMware clusters in the company’s data center. As part of the migration plan, the company wants to gather server metrics such as CPU details, RAM usage, operating system information, and running processes. The company then wants to query and analyze the data.\n\nWhich solution will meet these requirements?"},{"id":"WqqIXLRgd76PEvu6vnqo","choices":{"D":"Modify the web-crawling process to store results in an Amazon Aurora Serverless MySQL instance.","E":"Modify the web-crawling process to store results in Amazon S3.","C":"Modify the web-crawling process to store results in Amazon Neptune.","B":"Convert the web-crawling process into an AWS Lambda function. Configure the Lambda function to pull URLs from the SQS queue.","A":"Use m5.8xlarge instances instead of t2.micro instances for the web-crawling process. Reduce the number of instances in the fleet by 50%."},"answer_images":[],"answers_community":["BE (100%)"],"answer_ET":"BE","answer_description":"","timestamp":"2024-03-19 13:19:00","discussion":[{"poster":"AzureDP900","comment_id":"1309622","timestamp":"1731271320.0","content":"Option B involves converting the web-crawling process into an AWS Lambda function, which will allow the company to pay only for the compute time consumed by the function. This approach can help reduce costs compared to running EC2 instances.\nConverting the web-crawling process to a Lambda function also eliminates the need to maintain and monitor a fleet of EC2 instances, reducing operational costs.\nOption E involves modifying the web-crawling process to store results in Amazon S3, which is an object storage service that can store large amounts of data. This approach can help reduce costs compared to storing data on EFS or other block-based file systems.\nStoring results in S3 also provides high availability and durability for the data, meeting the requirements of a production-grade system.","upvote_count":"1"},{"upvote_count":"3","comment_id":"1182028","poster":"pangchn","timestamp":"1711322160.0","content":"Selected Answer: BE\nBE\nlamda + S3\nthe process don't need a database"},{"timestamp":"1710967440.0","comment_id":"1178690","content":"Selected Answer: BE\nA is utter rubbish - scaling out is not what we need\nB is optimal in terms of cost\nC and D involve fairly expensive databases not suitable for this use case. Moreover, Neptune must run in a VPC.\nE is optimal in terms of accessibility and cost","upvote_count":"3","poster":"Dgix"},{"timestamp":"1710850740.0","upvote_count":"1","content":"Selected Answer: BE\nuse lambda instead of a fleet of EC2, and store the results into cost-effective S3","poster":"CMMC","comment_id":"1177294"}],"exam_id":33,"url":"https://www.examtopics.com/discussions/amazon/view/136615-exam-aws-certified-solutions-architect-professional-sap-c02/","topic":"1","question_id":402,"isMC":true,"answer":"BE","unix_timestamp":1710850740,"question_text":"A company is running a web-crawling process on a list of target URLs to obtain training documents for machine learning training algorithms. A fleet of Amazon EC2 t2.micro instances pulls the target URLs from an Amazon Simple Queue Service (Amazon SQS) queue. The instances then write the result of the crawling algorithm as a .csv file to an Amazon Elastic File System (Amazon EFS) volume. The EFS volume is mounted on all instances of the fleet.\n\nA separate system adds the URLs to the SQS queue at infrequent rates. The instances crawl each URL in 10 seconds or less.\n\nMetrics indicate that some instances are idle when no URLs are in the SQS queue. A solutions architect needs to redesign the architecture to optimize costs.\n\nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.)","question_images":[]},{"id":"pyFOB81cPAbADnrX3iC0","answer_images":[],"question_text":"A company needs to migrate its website from an on-premises data center to AWS. The website consists of a load balancer, a content management system (CMS) that runs on a Linux operating system, and a MySQL database.\n\nThe CMS requires persistent NFS-compatible storage for a file system. The new solution on AWS must be able to scale from 2 Amazon EC2 instances to 30 EC2 instances in response to unpredictable traffic increases. The new solution also must require no changes to the website and must prevent data loss.\n\nWhich solution will meet these requirements?","question_images":[],"timestamp":"2024-03-19 13:59:00","answers_community":["A (93%)","7%"],"answer_description":"","isMC":true,"question_id":403,"unix_timestamp":1710853140,"exam_id":33,"answer_ET":"A","answer":"A","discussion":[{"content":"Option A involves creating an Amazon Elastic File System (Amazon EFS) file system, which provides a highly available and scalable file system that can be accessed by multiple EC2 instances.\nDeploying the CMS to AWS Elastic Beanstalk with an Application Load Balancer and an Auto Scaling group will allow the website to scale from 2 EC2 instances to 30 EC2 instances in response to unpredictable traffic increases, meeting the first requirement.\nThe use of EFS ensures that the file system is preserved across instance replacements or terminations during scale-in operations, preventing data loss.","timestamp":"1731270900.0","upvote_count":"1","comment_id":"1309619","poster":"AzureDP900"},{"content":"Selected Answer: A\nC is wrong because lifehook cannot mount EFS","timestamp":"1712019240.0","poster":"spencer_sharp","comment_id":"1187767","upvote_count":"4"},{"comment_id":"1186580","timestamp":"1711859580.0","upvote_count":"2","poster":"VerRi","content":"Selected Answer: A\nB and D are out because NFS->EFS\nC scale-in lifecycle hook to mount the EFS?????"},{"content":"Selected Answer: A\nA is correct","comment_id":"1184507","timestamp":"1711592280.0","upvote_count":"1","poster":"yog927"},{"content":"Selected Answer: A\nA\nEBS is out first.\nFor C, the NLB is weired but couldn't say its wrong. The scale-in policy to mount EFS is wrong, since mounting task should happens during scale-out process.","upvote_count":"3","comment_id":"1182710","poster":"pangchn","timestamp":"1711392000.0"},{"content":"Selected Answer: A\nB and D are out because Amazon EBS is not NFS-compatible\nC is out because scale-in lifecycle hook triggers when the instance is about to terminate - no point of mounting the EFS file system here\n\nReferences:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html","comment_id":"1181478","upvote_count":"2","timestamp":"1711279620.0","poster":"lasithasilva709"},{"poster":"ahmadraufsyahputra","upvote_count":"2","comment_id":"1178873","content":"A because I think Network Load Balancer is not the answer for this case","timestamp":"1710980760.0"},{"timestamp":"1710968760.0","content":"Selected Answer: A\nB and D are out because of EBS Multi-Attach volumes not working across AZs and have a max number of 16 instances in one zone.\n\nA is the correct answer because of no code changes (yes!).\nC is not optimal because of the NLB which isn't optimal as it doesn't support HTTP/HTTPS as such, working on the TCP level and doesn't do path-based routing. Also, having to set up autoscaling explicitly adds overhead.\n\nTherefore, A.","poster":"Dgix","comment_id":"1178705","upvote_count":"1"},{"poster":"CMMC","upvote_count":"1","timestamp":"1710853920.0","comment_id":"1177340","content":"Selected Answer: C\nChange to #C since #A could involve website changes"},{"poster":"CMMC","comment_id":"1177327","upvote_count":"1","timestamp":"1710853140.0","content":"Selected Answer: A\nEFS for persistent storage, Beanstalk for deploying with ALB and auto-scaling"}],"url":"https://www.examtopics.com/discussions/amazon/view/136621-exam-aws-certified-solutions-architect-professional-sap-c02/","choices":{"D":"Create an Amazon Elastic Block Store (Amazon EBS) Multi-Attach volume. Create a launch template and an Auto Scaling group to launch EC2 instances to support the CMS. Create an Application Load Balancer to distribute traffic. Create an Amazon ElastiCache for Redis cluster to support the MySQL database. Use EC2 user data to attach the EBS volume to the EC2 instances.","B":"Create an Amazon Elastic Block Store (Amazon EBS) Multi-Attach volume. Deploy the CMS to AWS Elastic Beanstalk with a Network Load Balancer and an Auto Scaling group. Use .ebextensions to mount the EBS volume to the EC2 instances. Create an Amazon RDS for MySQL database in the Elastic Beanstalk environment.","C":"Create an Amazon Elastic File System (Amazon EFS) file system. Create a launch template and an Auto Scaling group to launch EC2 instances to support the CMS. Create a Network Load Balancer to distribute traffic. Create an Amazon Aurora MySQL database. Use an EC2 Auto Scaling scale-in lifecycle hook to mount the EFS file system to the EC2 instances.","A":"Create an Amazon Elastic File System (Amazon EFS) file system. Deploy the CMS to AWS Elastic Beanstalk with an Application Load Balancer and an Auto Scaling group. Use .ebextensions to mount the EFS file system to the EC2 instances. Create an Amazon Aurora MySQL database that is separate from the Elastic Beanstalk environment."},"topic":"1"},{"id":"Ly5VbiX9KuZck4VfbxVq","url":"https://www.examtopics.com/discussions/amazon/view/136624-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_ET":"C","isMC":true,"answer":"C","question_images":[],"question_text":"A company needs to implement disaster recovery for a critical application that runs in a single AWS Region. The application's users interact with a web frontend that is hosted on Amazon EC2 instances behind an Application Load Balancer (ALB). The application writes to an Amazon RDS for MySQL DB instance. The application also outputs processed documents that are stored in an Amazon S3 bucket.\n\nThe company’s finance team directly queries the database to run reports. During busy periods, these queries consume resources and negatively affect application performance.\n\nA solutions architect must design a solution that will provide resiliency during a disaster. The solution must minimize data loss and must resolve the performance problems that result from the finance team's queries.\n\nWhich solution will meet these requirements?","answer_description":"","choices":{"C":"Create a read replica of the RDS DB instance in a separate Region. Instruct the finance team to run queries against the read replica. Create AMIs of the EC2 instances that host the application frontend. Copy the AMIs to the separate Region. Use S3 Cross-Region Replication (CRR) from the original S3 bucket to a new S3 bucket in the separate Region. During a disaster, promote the read replica to a standalone DB instance. Launch EC2 instances from the AMIs and create an ALB to present the application to end users. Configure the application to point to the new S3 bucket.","B":"Launch additional EC2 instances that host the application in a separate Region. Add the additional instances to the existing ALIn the separate Region, create a read replica of the RDS DB instance. Instruct the finance team to run queries against the read replica. Use S3 Cross-Region Replication (CRR) from the original S3 bucket to a new S3 bucket in the separate Region. During a disaster, promote the read replica to a standalone DB instance. Configure the application to point to the new S3 bucket and to the newly promoted read replica.","A":"Migrate the database to Amazon DynamoDB and use DynamoDB global tables. Instruct the finance team to query a global table in a separate Region. Create an AWS Lambda function to periodically synchronize the contents of the original S3 bucket to a new S3 bucket in the separate Region. Launch EC2 instances and create an ALB in the separate Region. Configure the application to point to the new S3 bucket.","D":"Create hourly snapshots of the RDS DB instance. Copy the snapshots to a separate Region. Add an Amazon ElastiCache cluster in front of the existing RDS database. Create AMIs of the EC2 instances that host the application frontend. Copy the AMIs to the separate Region. Use S3 Cross-Region Replication (CRR) from the original S3 bucket to a new S3 bucket in the separate Region. During a disaster, restore the database from the latest RDS snapshot. Launch EC2 instances from the AMIs and create an ALB to present the application to end users. Configure the application to point to the new S3 bucket."},"question_id":404,"timestamp":"2024-03-19 14:07:00","exam_id":33,"answer_images":[],"topic":"1","discussion":[{"poster":"pangchn","timestamp":"1711392540.0","comment_id":"1182716","upvote_count":"7","content":"Selected Answer: C\nC\nA is out since periodic lambda will have data loss\nB is out since ALB is regional service. Can't add EC2 to ALB if in different region\nD is out since hourly backup will have data loss"},{"poster":"AzureDP900","upvote_count":"1","timestamp":"1731270600.0","content":"Option C involves creating a read replica of the RDS DB instance in a separate Region, which addresses the performance issue caused by the finance team's queries. This approach also ensures that the database is available during a disaster.\nAdditionally, this option uses S3 Cross-Region Replication (CRR) to synchronize data across Regions, ensuring minimal data loss in case of a disaster.\nThe use of a read replica and CRR provides a more efficient solution compared to options A or D, which involve synchronizing entire databases or restoring from snapshots.","comment_id":"1309615"},{"timestamp":"1728368340.0","comment_id":"1294596","poster":"ahrentom","content":"Selected Answer: B\nI´ll go with B, because\n1. C did not promote the Read Replica into a Standalone instance.\n2. C did not redirect S3 traffic to the separate region, if the main region fails.\nCost is not in focus, so we can preinstall the needed EC2 instances.","upvote_count":"1","comments":[{"content":"\"Launch additional EC2 instances that host the application in a separate Region. Add the additional instances to the existing ALB\"\n\nYou cannot add instances from the new region to the existing region.","timestamp":"1740031980.0","comment_id":"1359105","poster":"altonh","upvote_count":"1"},{"upvote_count":"1","timestamp":"1728368340.0","poster":"ahrentom","content":"https://aws.amazon.com/de/blogs/aws/amazon-rds-for-mysql-promote-read-replica/","comment_id":"1294598"}]},{"timestamp":"1711280700.0","comment_id":"1181485","upvote_count":"1","poster":"lasithasilva709","content":"Selected Answer: C\nA is out because relational database is suited here\nD is out because ElastiCache is not required and hourly snapshots of the RDS DB instance would not minimise data loss\nB is out because as per the requirements (no RTO is mentioned), there is no need to launch EC2 instances in DR site and keep them idle"},{"timestamp":"1710969060.0","poster":"Dgix","comment_id":"1178713","upvote_count":"1","content":"Selected Answer: C\nC is the answer."},{"comment_id":"1177966","content":"Selected Answer: C\nThis solution involves creating a Read Replica of the RDS DB instance in another region and directing the finance team to execute queries on it, minimizing application performance impact. AMIs of EC2 instances are created and copied for rapid deployment. S3 Cross-Region Replication ensures data safety. In a disaster, the Read Replica becomes a standalone DB, and EC2 instances from AMIs with a new ALB serve the application, all reconfigured to the new S3 bucket. This approach addresses disaster recovery, minimizes data loss, and mitigates query-induced performance issues with minimal application changes.","poster":"txxxxxf","upvote_count":"2","timestamp":"1710915240.0"},{"comment_id":"1177334","upvote_count":"1","poster":"CMMC","content":"Selected Answer: C\nRead recplica for reporting, CRR to replicate S3 in another region, launch EC2 from AMI and ALB and promote the read replicate in the separate region furing DR","timestamp":"1710853620.0"}],"answers_community":["C (92%)","8%"],"unix_timestamp":1710853620},{"id":"GqWfdZaz3Tiz6IcZfmbw","question_images":[],"answer":"A","url":"https://www.examtopics.com/discussions/amazon/view/136627-exam-aws-certified-solutions-architect-professional-sap-c02/","answers_community":["A (89%)","11%"],"choices":{"B":"Create a VPC Endpoint Service that accepts HTTP or HTTPS traffic, host it behind an Application Load Balancer, and make the service available over DX.","D":"Attach a NAT gateway to the VPC, and ensure that network access control and security group rules allow the relevant inbound and outbound traffic.","A":"Create a VPC Endpoint Service that accepts TCP traffic, host it behind a Network Load Balancer, and make the service available over DX.","C":"Attach an internet gateway to the VPC, and ensure that network access control and security group rules allow the relevant inbound and outbound traffic."},"answer_images":[],"discussion":[{"content":"Selected Answer: A\nA is the correct option. There is no direct support for ALB with Private Link / VPC Endpoint service. ALB can be a target group for NLB so, we can use ALB with NLB but not ALB directly. Check this page for more details - https://aws.amazon.com/about-aws/whats-new/2021/09/application-load-balancer-aws-privatelink-static-ip-addresses-network-load-balancer/","upvote_count":"3","poster":"backbencher2022","comment_id":"1272943","timestamp":"1724694540.0"},{"comments":[{"upvote_count":"1","timestamp":"1726372980.0","comment_id":"1283901","poster":"kgpoj","content":"VPC Endpoint doesn't directly work with ALB, so B is wrong"}],"comment_id":"1270039","poster":"asquared16","upvote_count":"2","timestamp":"1724238420.0","content":"What do we know that makes B not a valid answer? It feels like the question is missing something."},{"content":"A, for sure.\nConnectivity cannot traverse the internet","poster":"gfhbox0083","upvote_count":"2","timestamp":"1720768980.0","comment_id":"1246563"},{"upvote_count":"2","comment_id":"1223921","timestamp":"1717478100.0","content":"Selected Answer: A\nA, VPC endpoint used with NLB","poster":"trungtd"},{"upvote_count":"2","timestamp":"1711860120.0","poster":"VerRi","content":"Selected Answer: A\nVPC endpoint + NLB = PrivateLink","comment_id":"1186581"},{"content":"Selected Answer: A\nA, VPC endpoint used with NLB","poster":"yog927","timestamp":"1711591380.0","comment_id":"1184504","upvote_count":"1"},{"timestamp":"1711393620.0","comment_id":"1182729","poster":"pangchn","content":"Selected Answer: A\nA\nThis is a privatelink scenrio. Can't find a hard evidence but the Privatelink seem can only work with NLB. If need ALB, it will be Privatelink -> NLB -> ALB\none evidence is the link lasithasilva709 posted\nanother evidence is compare of ALB/NLB\nhttps://aws.amazon.com/elasticloadbalancing/features/?nc=sn&loc=2&dn=1\n3rd evidence\nhttps://aws.amazon.com/about-aws/whats-new/2021/09/application-load-balancer-aws-privatelink-static-ip-addresses-network-load-balancer/","upvote_count":"4","comments":[{"content":"Also in question only mentioned services but doesn't mention port, where TCP (NLB) can cover all ports but HTTP/HTTPS (ALB) is restricted","timestamp":"1712470860.0","comment_id":"1190794","upvote_count":"2","poster":"pangchn"}]},{"poster":"lasithasilva709","comment_id":"1181501","content":"Selected Answer: A\nMy understanding is that NLB should be used for a VPC endpoint service.\n\nHere are some resources:\n1. To use AWS PrivateLink, create a Network Load Balancer for your application in your VPC, and create a VPC endpoint service configuration pointing to that load balancer. \nhttps://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/aws-privatelink.html\n2. https://aws.amazon.com/blogs/networking-and-content-delivery/application-load-balancer-type-target-group-for-network-load-balancer/","timestamp":"1711281600.0","upvote_count":"2"},{"timestamp":"1711150560.0","upvote_count":"2","comment_id":"1180414","content":"Answer is A.\nMany services is a key word , option B is for http and https.","poster":"AWSPro1234"},{"poster":"Dgix","comment_id":"1178721","timestamp":"1710969540.0","upvote_count":"2","content":"Selected Answer: B\nB is just a safe as A — TCP is not inherently safer. However, HTTPS and HTTP are much more commonly used when providing services to other companies. As we don't have any information as to the nature of the service, a safer bet (pun intended) is B."},{"poster":"CMMC","upvote_count":"2","timestamp":"1710854520.0","content":"Selected Answer: A\n#C & #D are out given the connectivity cannot traverse the internet. #A enables secure VPC endpoint to privately expose to other companies' VPCs without traversing the internet, and TCP to provide more controlled and secure comm protocol for sensitive data","comment_id":"1177347"}],"question_id":405,"isMC":true,"timestamp":"2024-03-19 14:22:00","unix_timestamp":1710854520,"topic":"1","answer_ET":"A","question_text":"A company has many services running in its on-premises data center. The data center is connected to AWS using AWS Direct Connect (DX) and an IPSec VPN. The service data is sensitive and connectivity cannot traverse the internet. The company wants to expand into a new market segment and begin offering its services to other companies that are using AWS.\n\nWhich solution will meet these requirements?","exam_id":33,"answer_description":""}],"exam":{"lastUpdated":"11 Apr 2025","isMCOnly":true,"id":33,"provider":"Amazon","name":"AWS Certified Solutions Architect - Professional SAP-C02","numberOfQuestions":529,"isBeta":false,"isImplemented":true},"currentPage":81},"__N_SSP":true}