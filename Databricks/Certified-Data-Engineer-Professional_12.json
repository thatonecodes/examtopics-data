{"pageProps":{"questions":[{"id":"XgOHpzpY9N90kKH3o5rv","answer":"A","answer_ET":"A","exam_id":163,"timestamp":"2024-10-20 17:51:00","isMC":true,"choices":{"B":"Spark cannot capture the topic and partition fields from a Kafka source.","D":"Updating the table schema will invalidate the Delta transaction log metadata.","C":"Updating the table schema requires a default value provided for each field added.","A":"New fields will not be computed for historic records."},"question_images":[],"question_id":56,"unix_timestamp":1729439460,"answers_community":["A (100%)"],"answer_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/149871-exam-certified-data-engineer-professional-topic-1-question/","question_text":"A data pipeline uses Structured Streaming to ingest data from Apache Kafka to Delta Lake. Data is being stored in a bronze table, and includes the Kafka-generated timestamp, key, and value. Three months after the pipeline is deployed, the data engineering team has noticed some latency issues during certain times of the day.\n\nA senior data engineer updates the Delta Table's schema and ingestion logic to include the current timestamp (as recorded by Apache Spark) as well as the Kafka topic and partition. The team plans to use these additional metadata fields to diagnose the transient processing delays.\n\nWhich limitation will the team face while diagnosing this problem?","discussion":[{"timestamp":"1737023280.0","upvote_count":"1","content":"Selected Answer: A\nA is correct: the old records are lost as no history was saved for the new toppic.","poster":"RandomForest","comment_id":"1341639"},{"upvote_count":"2","timestamp":"1729439460.0","content":"Selected Answer: A\nThere is no way to reprocess history/old records to populate these values after 3 months as Kafka does not necessarily preserve them so long. This is the function of the Raw or Bronze table. Also, the other answers just don't make sense.","poster":"m79590530","comment_id":"1300552"}],"answer_description":"","topic":"1"},{"id":"HtlNTKELevtfjte3LlSs","timestamp":"2023-08-27 18:26:00","exam_id":163,"answer_description":"","choices":{"A":"Apply the churn model to all rows in the customer_churn_params table, but implement logic to perform an upsert into the predictions table that ignores rows where predictions have not changed.","B":"Convert the batch job to a Structured Streaming job using the complete output mode; configure a Structured Streaming job to read from the customer_churn_params table and incrementally predict against the churn model.","E":"Replace the current overwrite logic with a merge statement to modify only those records that have changed; write logic to make predictions on the changed records identified by the change data feed.","C":"Calculate the difference between the previous model predictions and the current customer_churn_params on a key identifying unique customers before making new predictions; only make predictions on those customers not in the previous predictions.","D":"Modify the overwrite logic to include a field populated by calling spark.sql.functions.current_timestamp() as data are being written; use this field to identify records written on a particular date."},"unix_timestamp":1693153560,"isMC":true,"answer_images":[],"answer_ET":"E","answers_community":["E (80%)","D (20%)"],"question_images":[],"question_text":"A table in the Lakehouse named customer_churn_params is used in churn prediction by the machine learning team. The table contains information about customers derived from a number of upstream sources. Currently, the data engineering team populates this table nightly by overwriting the table with the current valid values derived from upstream data sources.\nThe churn prediction model used by the ML team is fairly stable in production. The team is only interested in making predictions on records that have changed in the past 24 hours.\nWhich approach would simplify the identification of these changed records?","question_id":57,"answer":"E","url":"https://www.examtopics.com/discussions/databricks/view/119189-exam-certified-data-engineer-professional-topic-1-question/","topic":"1","discussion":[{"content":"E is right answer","poster":"Eertyy","upvote_count":"6","timestamp":"1693153560.0","comment_id":"991565"},{"poster":"AlHerd","comment_id":"1410830","timestamp":"1743069300.0","upvote_count":"1","content":"Selected Answer: E\nE.\n\nWhile both D and E look right D only adds a timestamp but doesn’t track whether the record content actually changed, leading to false positives."},{"timestamp":"1740529620.0","poster":"Tedet","content":"Selected Answer: D\nEvaluation:\n\nAdding a current_timestamp() field to each record during the overwrite allows you to track when each record was written.\nThis makes it easy to identify records that have been updated or inserted recently by filtering on this timestamp field (e.g., filtering for records written in the past 24 hours).\nThis approach simplifies identifying recently changed records because you can easily filter for the most recent data and then run churn predictions only on those records.\nConclusion: This is a simple and efficient solution. It allows you to track changes by using a timestamp, making it easy to filter and predict only on changed records without complex logic.","upvote_count":"1","comment_id":"1361629"},{"content":"Selected Answer: E\nA, B, and C don't make sense. Adding a timestamp with an overwrite logic that overwrites everything does not make sense - all records would have a timestamp from the last night. That would be not helpful in identifying what changed.\n\nE is correct. Only write changes, use CDF to identify the changes and apply the model.","upvote_count":"2","comment_id":"1334760","poster":"arekm","timestamp":"1735645440.0"},{"timestamp":"1733982060.0","content":"Selected Answer: E\nWhile both E and D are correct. \nE is more accurate, given the scenario","comment_id":"1325421","poster":"Sriramiyer92","upvote_count":"1"},{"timestamp":"1733352720.0","comment_id":"1322129","poster":"janeZ","upvote_count":"1","content":"Selected Answer: D\nD is the right answer"},{"comment_id":"1261373","timestamp":"1722915660.0","comments":[{"content":"You are 100pc correct Melik3. Reason being consequences of E are below.\nA merge statement ensures that only the records that have changed are updated, but it doesn’t directly address how to identify which records have changed within the last 24 hours.\nUsing a change data feed can help track changes, but it may not be the most efficient method unless the infrastructure is set up for real-time change tracking.\nThe complexity of managing and using the change data feed for just 24-hour changes might introduce unnecessary overhead.\nConclusion: This is a good option, but it could be more complex to implement than simply adding a current_timestamp() field.","upvote_count":"1","poster":"Tedet","timestamp":"1740529680.0","comment_id":"1361630"},{"content":"\"write logic to make predictions on the CHANGED records identified by the change data feed\". the only thing partially wrong about E is that it has never been stated that the table has a change data feed enables.","upvote_count":"1","poster":"benni_ale","timestamp":"1729839180.0","comment_id":"1302788"}],"poster":"Melik3","content":"I don't understand why E is correct. With E we are updating only data needed but we are then doing prediction on the whole table which means that we are doing again predictions on not changing records which is not efficient","upvote_count":"1"},{"content":"E is the correct one. By removing overwrite with merge, this will lead to an UPSERT causing updating only the data needed ( When Matched Upate + When not mached insert clauses). Then, with the CDC the capability of identifying is also satisfied.","poster":"leopedroso1","comment_id":"1153470","timestamp":"1708279200.0","upvote_count":"2"},{"upvote_count":"1","content":"correct ans is E","poster":"AziLa","timestamp":"1705869420.0","comment_id":"1128119"},{"poster":"kz_data","comment_id":"1118585","timestamp":"1704893460.0","upvote_count":"1","content":"Selected Answer: E\nE is correct"},{"comment_id":"1044819","poster":"sturcu","upvote_count":"3","timestamp":"1697448480.0","content":"Selected Answer: E\nE is Correct"}]},{"id":"KbuffBvQypr9k487pUNa","isMC":true,"discussion":[{"content":"Selected Answer: B\nCorrect Answer: B\nThe Change Data Feed (CDF) feature in Delta Lake enables reading only the changes (inserts and updates) to a Delta table. This would allow the function to focus on new or modified data since the last trigger, making it ideal for processing only the new records that have not been processed yet. This directly meets the requirement for identifying and manipulating new records efficiently.","poster":"Freyr","timestamp":"1717275180.0","comment_id":"1222900","upvote_count":"7","comments":[{"upvote_count":"1","comment_id":"1335552","content":"It is missing the starting version. You either use readStream without the version or read (aka batch) with the version. In here in B the version is missing. Since the source table is append only, it is perfect for streaming, making A the right choice.","timestamp":"1735821300.0","poster":"arekm"},{"comment_id":"1266800","content":"We are ingesting data from the folder with a parquet in the bronze table. It doesn't make any sense to use the CDF feature for bronze table )","upvote_count":"1","timestamp":"1723778460.0","comments":[{"timestamp":"1723779000.0","content":"I've changed my opinion. Yes, B looks as correct answer","upvote_count":"2","poster":"practicioner","comment_id":"1266803"}],"poster":"practicioner"}]},{"upvote_count":"1","comment_id":"1410192","content":"Selected Answer: A\nany tables can become a stream in Databricks, therefore answer A.\nIt cannot be answer B as the table named bronze does not have CDF enabled","poster":"thierryb","timestamp":"1742943060.0"},{"comment_id":"1399933","upvote_count":"1","content":"Selected Answer: B\nThe function new_records() is meant to retrieve new records that have not yet been processed in the pipeline.\n\nDelta Lake provides a feature called the Change Data Feed (CDF), which allows you to track changes (inserts, updates, and deletes) in a Delta table.\n\nBy using .option(\"readChangeFeed\", \"true\"), the function can read only the new changes from the Delta table named \"bronze\".\n\nThis ensures that only unprocessed records are returned, aligning with the pipeline's requirement.","poster":"mohadjhamad","timestamp":"1742261520.0"},{"timestamp":"1735821360.0","content":"Selected Answer: A\nA - append only table works for streaming, B is missing the starting version, C - timestamp during the insertion will be different from the one during the next step, D - that was a \"like\" type query (or maybe a substring on the file name == the directory), then yes - actual file names only start with the pattern presented, but are longer.","comment_id":"1335554","poster":"arekm","upvote_count":"1"},{"poster":"UrcoIbz","comment_id":"1329146","upvote_count":"2","timestamp":"1734639060.0","content":"Selected Answer: A\nOption A seems to be the correct answer. \n\nOption B seems not to be the right one, because is missing the version. Based on the documentation 'Change data feed also supports batch execution, which requires specifying a starting version'. \n\nhttps://learn.microsoft.com/en-us/azure/databricks/delta/delta-change-data-feed#batch\n\nOption D , in my opinion, is not correct, as the function definition above is not having any input parameter."},{"comment_id":"1300554","poster":"m79590530","upvote_count":"3","timestamp":"1729439820.0","content":"Selected Answer: A\nCorrect answer is A as we have append-only mode writes which are ideal for simple Structured Streaming as a next step ;)"},{"comment_id":"1297617","timestamp":"1728912720.0","poster":"shaojunni","content":"Selected Answer: A\ndelta table returns new records in streaming read.","upvote_count":"3"},{"comment_id":"1289438","timestamp":"1727354220.0","content":"Selected Answer: B\nB. Set the skipChangeCommits flag to true on raw_iot\n\nLet's break down the requirements and explain why this is the best solution:\n\nRetain manually deleted or updated records in raw_iot: The skipChangeCommits flag, when set to true, tells Delta Live Tables (DLT) to ignore any manual changes (updates or deletes) made to the table outside of the pipeline. This means that even if records are manually deleted or updated in the raw_iot table, these changes won't be reflected in the table when the pipeline runs again.\nRecompute downstream bpm_stats table: By default, DLT will recompute downstream tables when their upstream dependencies change. Since bpm_stats is based on raw_iot, it will naturally be recomputed when the pipeline updates, without any special configuration.\nWhy the other options are not correct:\n\nA. Setting pipelines.reset.allowed to false on raw_iot would prevent the table from being reset, but it wouldn't address the requirement to retain manually deleted or updated records.","upvote_count":"1","poster":"pk07"},{"upvote_count":"1","comment_id":"1288304","timestamp":"1727122500.0","content":"Selected Answer: D\nYou have to know the CDF's current version and last processed the version in order to get not processed records. B does not provide those versions. It will just return content from the bronze table with CDF turned on. D is only possible solution.","poster":"shaojunni"},{"content":"I did not test it. But i think D is wrong as it filtering agenst directory path using ==","upvote_count":"2","poster":"HelixAbdu","timestamp":"1722086700.0","comment_id":"1256313"},{"timestamp":"1717004760.0","upvote_count":"4","content":"Selected Answer: D\nSeems D","comment_id":"1221167","poster":"MDWPartners"}],"timestamp":"2024-05-29 19:46:00","exam_id":163,"answer_description":"","unix_timestamp":1717004760,"url":"https://www.examtopics.com/discussions/databricks/view/141557-exam-certified-data-engineer-professional-topic-1-question/","answer_ET":"A","choices":{"B":"return spark.read.option(\"readChangeFeed\", \"true\").table (\"bronze\")","A":"return spark.readStream.table(\"bronze\")","D":"","C":""},"answer_images":[],"question_text":"A nightly job ingests data into a Delta Lake table using the following code:\n\n//IMG//\n\n\nThe next step in the pipeline requires a function that returns an object that can be used to manipulate new records that have not yet been processed to the next table in the pipeline.\n\nWhich code snippet completes this function definition?\n\ndef new_records():","answers_community":["A (42%)","B (38%)","D (21%)"],"question_id":58,"topic":"1","answer":"A","question_images":["https://img.examtopics.com/certified-data-engineer-professional/image65.png"]},{"id":"sBVcM5aRGgJymYoG10jt","topic":"1","question_text":"A junior data engineer is working to implement logic for a Lakehouse table named silver_device_recordings. The source data contains 100 unique fields in a highly nested JSON structure.\n\nThe silver_device_recordings table will be used downstream to power several production monitoring dashboards and a production model. At present, 45 of the 100 fields are being used in at least one of these applications.\n\nThe data engineer is trying to determine the best approach for dealing with schema declaration given the highly-nested structure of the data and the numerous fields.\n\nWhich of the following accurately presents information about Delta Lake and Databricks that may impact their decision-making process?","unix_timestamp":1718208780,"answers_community":["D (100%)"],"answer":"D","answer_ET":"D","url":"https://www.examtopics.com/discussions/databricks/view/142393-exam-certified-data-engineer-professional-topic-1-question/","isMC":true,"discussion":[{"upvote_count":"3","comment_id":"1230599","content":"Selected Answer: D\nAgree with propopsed answer, D","poster":"Isio05","timestamp":"1718387760.0"},{"poster":"hpkr","comment_id":"1229304","upvote_count":"2","content":"Selected Answer: D\nD is correct","timestamp":"1718208780.0"}],"exam_id":163,"answer_description":"","answer_images":[],"timestamp":"2024-06-12 18:13:00","question_id":59,"question_images":[],"choices":{"B":"Because Delta Lake uses Parquet for data storage, data types can be easily evolved by just modifying file footer information in place.","C":"Schema inference and evolution on Databricks ensure that inferred types will always accurately match the data types used by downstream systems.","D":"Because Databricks will infer schema using types that allow all observed data to be processed, setting types manually provides greater assurance of data quality enforcement.","A":"The Tungsten encoding used by Databricks is optimized for storing string data; newly-added native support for querying JSON strings means that string types are always most efficient."}},{"id":"1mK7PdKlAa3uk5QVShAn","isMC":true,"answer_ET":"B","choices":{"C":"No computation will occur until enriched_itemized_orders_by_account is queried; upon query materialization, results will be calculated using the current valid version of data in each of the three tables referenced in the join logic.","D":"An incremental job will detect if new rows have been written to any of the source tables; if new rows are detected, all results will be recalculated and used to overwrite the enriched_itemized_orders_by_account table.","A":"A batch job will update the enriched_itemized_orders_by_account table, replacing only those rows that have different values than the current version of the table, using accountID as the primary key.","B":"The enriched_itemized_orders_by_account table will be overwritten using the current valid version of data in each of the three tables referenced in the join logic."},"exam_id":163,"timestamp":"2024-06-12 18:15:00","question_id":60,"question_images":["https://img.examtopics.com/certified-data-engineer-professional/image68.png"],"answer_description":"","unix_timestamp":1718208900,"answers_community":["B (100%)"],"answer_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/142394-exam-certified-data-engineer-professional-topic-1-question/","question_text":"The data engineering team maintains the following code:\n\n//IMG//\n\n\nAssuming that this code produces logically correct results and the data in the source tables has been de-duplicated and validated, which statement describes what will occur when this code is executed?","discussion":[{"timestamp":"1718208900.0","comment_id":"1229305","poster":"hpkr","content":"Selected Answer: B\nB is correct","upvote_count":"3"}],"topic":"1","answer":"B"}],"exam":{"isImplemented":true,"isMCOnly":true,"isBeta":false,"name":"Certified Data Engineer Professional","numberOfQuestions":200,"id":163,"provider":"Databricks","lastUpdated":"12 Apr 2025"},"currentPage":12},"__N_SSP":true}