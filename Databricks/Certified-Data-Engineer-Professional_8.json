{"pageProps":{"questions":[{"id":"qrrNao8PuTik19HKaZVV","isMC":true,"discussion":[{"timestamp":"1717263420.0","comment_id":"1222837","content":"Selected Answer: B\nCorrect Answer: B. \nIn the question it is mentioned that the schema evolution is enabled. This option states that the new nested field is added to the target schema, and existing records not matching the new schema format are populated with NULL for the newly added columns. This behavior aligns with how schema evolution functions in Delta Lake, dynamically adapting the schema to include new fields.","poster":"Freyr","upvote_count":"5"},{"timestamp":"1735824660.0","upvote_count":"1","poster":"HairyTorso","content":"Selected Answer: B\nSeems like B is correct if below config is enabled:\n\nSET spark.databricks.delta.schema.autoMerge.enabled = true;","comment_id":"1335572"},{"poster":"pppppppppie","content":"because B says for unmatched records. but my new records has 1001 which is existing record. we should give ans what will happen with this new record. it will update the record in target with NULL as coupon","timestamp":"1725416460.0","upvote_count":"1","comment_id":"1277909"},{"timestamp":"1725416340.0","content":"It has to be D","poster":"pppppppppie","upvote_count":"1","comment_id":"1277905"},{"upvote_count":"4","content":"Selected Answer: B\nschema evolution is enabled, so B.","poster":"MDWPartners","timestamp":"1717001700.0","comment_id":"1221139"}],"question_images":["https://img.examtopics.com/certified-data-engineer-professional/image56.png","https://img.examtopics.com/certified-data-engineer-professional/image57.png","https://img.examtopics.com/certified-data-engineer-professional/image58.png"],"url":"https://www.examtopics.com/discussions/databricks/view/141552-exam-certified-data-engineer-professional-topic-1-question/","unix_timestamp":1717001700,"exam_id":163,"answers_community":["B (100%)"],"choices":{"B":"The new nested Field is added to the target schema, and dynamically read as NULL for existing unmatched records.","D":"The new nested field is added to the target schema, and files underlying existing records are updated to include NULL values for the new field.","A":"The update throws an error because changes to existing columns in the target schema are not supported.","C":"The update is moved to a separate \"rescued\" column because it is missing a column expected in the target schema."},"question_id":36,"answer_description":"","answer_ET":"B","answer_images":[],"topic":"1","answer":"B","timestamp":"2024-05-29 18:55:00","question_text":"The following table consists of items found in user carts within an e-commerce website.\n\n//IMG//\n\n\nThe following MERGE statement is used to update this table using an updates view, with schema evolution enabled on this table.\n\n//IMG//\n\n\nHow would the following update be handled?\n\n//IMG//"},{"id":"KCFKe3rJ5QnVm0ksNseR","timestamp":"2024-06-01 19:44:00","topic":"1","choices":{"C":"Deduplicate records in each batch by pk_id and overwrite the target table.","B":"Use merge into to insert, update, or delete the most recent entry for each pk_id into a table, then propagate all changes throughout the system.","A":"Iterate through an ordered set of changes to the table, applying each in turn to create the current state of the table, (insert, update, delete), timestamp of change, and the values.","D":"Use Delta Lake’s change data feed to automatically process CDC data from an external system, propagating all changes to all dependent tables in the Lakehouse."},"answer":"D","question_images":[],"question_text":"An upstream system is emitting change data capture (CDC) logs that are being written to a cloud object storage directory. Each record in the log indicates the change type (insert, update, or delete) and the values for each field after the change. The source table has a primary key identified by the field pk_id.\n\nFor auditing purposes, the data governance team wishes to maintain a full record of all values that have ever been valid in the source system. For analytical purposes, only the most recent value for each record needs to be recorded. The Databricks job to ingest these records occurs once per hour, but each individual record may have changed multiple times over the course of an hour.\n\nWhich solution meets these requirements?","discussion":[{"upvote_count":"1","comment_id":"1553215","timestamp":"1743896880.0","poster":"a85becd","content":"Selected Answer: B\nDelta Lake's Change Data Feed (CDF) only works on Delta tables, as it is a feature explicitly tied to Delta Lake's architecture. In the scenario described in the question, the source is an external system writing CDC logs to cloud object storage, and there is no mention of the data being stored in Delta tables initially."},{"content":"Selected Answer: D\nIf its exact duplicate of Question #13 then we are missing a good choice of :\n\"E. Ingest all log information into a bronze table; use MERGE INTO to insert, update, or delete the most recent entry for each pk_id into a silver table to recreate the current table state.\"","comment_id":"1332257","timestamp":"1735280580.0","upvote_count":"3","poster":"OnlyPraveen"},{"comment_id":"1319318","upvote_count":"2","poster":"e904bf4","timestamp":"1732808040.0","content":"Selected Answer: B\nThe right answer is B because CDF only works on delta tables and here the source is external"},{"timestamp":"1730267880.0","comment_id":"1304874","content":"There is no right answer. Closest is B only after the CDC logs are ingested to a Bronze table and then use merge into a silver table.\n\nWhy not D? Because CDF only works on delta tables and not on external CDC logs.","poster":"Huepig","upvote_count":"3"},{"upvote_count":"1","comment_id":"1256275","content":"The MERGE INTO statement in Delta Lake is a powerful feature designed to handle Change Data Capture (CDC) data efficiently. This approach meets both the auditing and analytical requirements.\n\nCDF is not enabled by default. So these data is not generated by it to handel them.","comments":[{"poster":"practicioner","content":"I'd agree, but there is \"a full record of all values that have ever been valid in the source system\". After deleting records we can still use time-travel options... But after vacuuming audit team will be dissapinted","comment_id":"1266364","upvote_count":"1","timestamp":"1723717500.0"}],"timestamp":"1722080580.0","poster":"HelixAbdu"},{"upvote_count":"2","poster":"Ati1362","timestamp":"1719228480.0","content":"Selected Answer: D\nagree with D","comment_id":"1236283"},{"upvote_count":"2","content":"Selected Answer: D\nDelta Lake provides built-in change data feed functionality.\nIt captures changes (inserts, updates, deletes) and propagates them to dependent tables.\nBy using Delta Lake, you can maintain historical records and propagate changes efficiently.","comment_id":"1224761","poster":"BrianNguyen95","timestamp":"1717596000.0"},{"poster":"Freyr","comment_id":"1222841","content":"Selected Answer: D\nCorrect Answer: D \nDelta Lake’s change data feed feature is specifically designed to handle CDC scenarios. It processes data from external systems, tracking all changes (inserts, updates, deletes) and maintaining a detailed history of these changes. This feature allows for keeping a comprehensive log while also ensuring the most recent state is correctly reflected in analytical tables.","timestamp":"1717263840.0","upvote_count":"3"}],"unix_timestamp":1717263840,"answer_ET":"D","answers_community":["D (77%)","B (23%)"],"answer_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/141743-exam-certified-data-engineer-professional-topic-1-question/","question_id":37,"exam_id":163,"isMC":true,"answer_description":""},{"id":"ZYlCl9a4PBryUdOQnQXj","answer_description":"","question_id":38,"topic":"1","question_images":[],"exam_id":163,"choices":{"C":"Overwrite the account_current table with each batch using the results of a query against the account_history table grouping by user_id and filtering for the max value of last_updated.","D":"Filter records in account_history using the last_updated field and the most recent hour processed, as well as the max last_login by user_id write a merge statement to update or insert the most recent value for each user_id.","B":"Use Auto Loader to subscribe to new files in the account_history directory; configure a Structured Streaming trigger available job to batch update newly detected files into the account_current table.","A":"Filter records in account_history using the last_updated field and the most recent hour processed, making sure to deduplicate on username; write a merge statement to update or insert the most recent value for each username."},"answer_ET":"D","timestamp":"2024-06-01 21:07:00","answer":"D","question_text":"An hourly batch job is configured to ingest data files from a cloud object storage container where each batch represent all records produced by the source system in a given hour. The batch job to process these records into the Lakehouse is sufficiently delayed to ensure no late-arriving data is missed. The user_id field represents a unique key for the data, which has the following schema:\n\nuser_id BIGINT, username STRING, user_utc STRING, user_region STRING, last_login BIGINT, auto_pay BOOLEAN, last_updated BIGINT\n\nNew records are all ingested into a table named account_history which maintains a full record of all data in the same schema as the source. The next table in the system is named account_current and is implemented as a Type 1 table representing the most recent value for each unique user_id.\n\nWhich implementation can be used to efficiently update the described account_current table as part of each hourly batch job assuming there are millions of user accounts and tens of thousands of records processed hourly?","answers_community":["D (71%)","14%","14%"],"url":"https://www.examtopics.com/discussions/databricks/view/141744-exam-certified-data-engineer-professional-topic-1-question/","discussion":[{"upvote_count":"1","poster":"benni_ale","comment_id":"1307253","timestamp":"1730795100.0","content":"Selected Answer: D\nB or D ... associate course tells us to use auto loader when bilions of rows..."},{"comment_id":"1288816","timestamp":"1727219340.0","content":"Selected Answer: D\nD seems like the best option","upvote_count":"1","poster":"RyanAck24"},{"content":"Selected Answer: B\nA, D both wrong. They only take data from the latest update. It is too narrow. Same user_id can have several updates within an hour to update different fields. So use auto loader to apply all the updates within an hour is the only correct answer.","timestamp":"1727106600.0","upvote_count":"1","comment_id":"1288190","poster":"shaojunni"},{"upvote_count":"1","comment_id":"1268169","content":"Selected Answer: A\nAnswer is A. You're meeting all the requirements with less overhead. It's only updating on the most recent record, so duplicates are handled.\n\nAnswer D is too much overhead. They're doing a full table scan for all records, which as the question stated, is millions of records.","comments":[{"content":"User Id would be a better column to merge into with, username might not be distinct","poster":"Onobhas01","comment_id":"1276941","timestamp":"1725297540.0","upvote_count":"1"}],"timestamp":"1724000940.0","poster":"fe3b2fc"},{"upvote_count":"3","comment_id":"1222863","poster":"Freyr","content":"Selected Answer: D\nCorrect Answer: D\nSimilar to Option A, but specifically designed around the user_id, which is the primary key. This approach ensures that the account_current is always up-to-date with the latest information per user based on the primary key, reducing the risk of duplicate information and ensuring the integrity of the data with respect to the unique identifier.","timestamp":"1717268820.0"}],"answer_images":[],"unix_timestamp":1717268820,"isMC":true},{"id":"grzKEzJUKeZKovXuBFwA","topic":"1","answer_ET":"A","question_images":["https://img.examtopics.com/certified-data-engineer-professional/image59.png","https://img.examtopics.com/certified-data-engineer-professional/image60.png"],"isMC":true,"choices":{"B":"Use Structured Streaming to configure a live dashboard against the products_per_order table within a Databricks notebook.","A":"Populate the dashboard by configuring a nightly batch job to save the required values as a table overwritten with each update.","D":"Use the Delta Cache to persist the products_per_order table in memory to quickly update the dashboard with each query.","C":"Define a view against the products_per_order table and define the dashboard against this view."},"question_id":39,"answer":"A","answer_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/141553-exam-certified-data-engineer-professional-topic-1-question/","exam_id":163,"unix_timestamp":1717002900,"answer_description":"","answers_community":["A (100%)"],"discussion":[{"comment_id":"1341580","timestamp":"1737018360.0","poster":"RandomForest","upvote_count":"1","content":"Selected Answer: A\nA is correct: The data should only be refreshed once a day so nightly load will do just fine."},{"content":"E\n#69 in professional one","poster":"17kyu","comments":[{"timestamp":"1728872880.0","comment_id":"1297177","content":"C Define a viwe","upvote_count":"1","poster":"17kyu"}],"timestamp":"1728872820.0","comment_id":"1297175","upvote_count":"1"},{"comment_id":"1221146","content":"Selected Answer: A\nSeems correct","poster":"MDWPartners","timestamp":"1717002900.0","upvote_count":"2"}],"timestamp":"2024-05-29 19:15:00","question_text":"The business intelligence team has a dashboard configured to track various summary metrics for retail stores. This includes total sales for the previous day alongside totals and averages for a variety of time periods. The fields required to populate this dashboard have the following schema:\n\n//IMG//\n\n\nFor demand forecasting, the Lakehouse contains a validated table of all itemized sales updated incrementally in near real-time. This table, named products_per_order, includes the following fields:\n\n//IMG//\n\n\nBecause reporting on long-term sales trends is less volatile, analysts using the new dashboard only require data to be refreshed once daily. Because the dashboard will be queried interactively by many users throughout a normal business day, it should return results quickly and reduce total compute associated with each materialization.\n\nWhich solution meets the expectations of the end users while controlling and limiting possible costs?"},{"id":"Oo2uVxBuPVvr2gJ6ZBlk","topic":"1","answers_community":["C (100%)"],"url":"https://www.examtopics.com/discussions/databricks/view/149836-exam-certified-data-engineer-professional-topic-1-question/","question_images":[],"answer_images":[],"choices":{"A":"Apply the churn model to all rows in the customer_churn_params table, but implement logic to perform an upsert into the predictions table that ignores rows where predictions have not changed.","C":"Replace the current overwrite logic with a merge statement to modify only those records that have changed; write logic to make predictions on the changed records identified by the change data feed.","D":"Modify the overwrite logic to include a field populated by calling spark.sql.functions.current_timestamp() as data are being written; use this field to identify records written on a particular date.","B":"Convert the batch job to a Structured Streaming job using the complete output mode; configure a Structured Streaming job to read from the customer_churn_params table and incrementally predict against the churn model."},"unix_timestamp":1729413780,"isMC":true,"answer":"C","question_id":40,"question_text":"A Delta lake table with CDF enabled table in the Lakehouse named customer_churn_params is used in churn prediction by the machine learning team. The table contains information about customers derived from a number of upstream sources. Currently, the data engineering team populates this table nightly by overwriting the table with the current valid values derived from upstream data sources.\n\nThe churn prediction model used by the ML team is fairly stable in production. The team is only interested in making predictions on records that have changed in the past 24 hours.\n\nWhich approach would simplify the identification of these changed records?","exam_id":163,"discussion":[{"upvote_count":"1","poster":"RandomForest","timestamp":"1737018600.0","comment_id":"1341585","content":"Selected Answer: C\nC is correct: Use the fact that CDF is enabled to implement a SCD Type 1 UPSERT logic."},{"comment_id":"1300362","upvote_count":"3","poster":"m79590530","content":"Selected Answer: C\nWhy use overwrite for a table that has CDF enabled? Best is to modify it in a SCD Type 1 style by using UPSERTs via the MERGE INTO logic and then/also use its CDF system fields as the means to filter which records are current and which not by selecting only the records that have 'insert' or 'update_postimage' in their _change_type CDF system column.","timestamp":"1729413780.0"}],"answer_ET":"C","timestamp":"2024-10-20 10:43:00","answer_description":""}],"exam":{"numberOfQuestions":200,"lastUpdated":"12 Apr 2025","id":163,"name":"Certified Data Engineer Professional","isImplemented":true,"isMCOnly":true,"provider":"Databricks","isBeta":false},"currentPage":8},"__N_SSP":true}