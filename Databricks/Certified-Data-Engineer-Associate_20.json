{"pageProps":{"questions":[{"id":"qHHJA9X6TZbWVXs5sxtj","question_text":"A dataset has been defined using Delta Live Tables and includes an expectations clause:\nCONSTRAINT valid_timestamp EXPECT (timestamp > '2020-01-01') ON VIOLATION DROP ROW\nWhat is the expected behavior when a batch of data containing data that violates these constraints is processed?","question_id":96,"discussion":[{"poster":"XiltroX","comment_id":"858825","upvote_count":"17","content":"Selected Answer: C\nI am simply appalled by the number of wrong answers in this series of questions. The statement in the question already says \"ON VIOLATE DROP ROW\" which means if condition is violated, there will be nothing saved to quarantine table and a log of all invalid entries will be recoded. All invalid data that doesn't meet condition will be dropped. \nSo C is the correct answer.","timestamp":"1680437880.0"},{"content":"Selected Answer: C\nC is correct","timestamp":"1680617460.0","comment_id":"861148","upvote_count":"5","poster":"rafahb"},{"poster":"806e7d2","upvote_count":"2","content":"Selected Answer: C\nIn Delta Live Tables, expectations are used to enforce data quality rules. In this specific case, the expectation is that the timestamp column should be greater than '2020-01-01'. When a batch of data is processed, if a record violates this expectation, the following happens:\n\nDrop the violating rows: The rows that don't meet the expectation (timestamp > '2020-01-01') will be dropped from the dataset.\nLogging of the violation: The fact that these rows were dropped due to the violation will be recorded in the event log for audit and tracking purposes.\nThis ensures that only valid data (according to the expectation) is loaded into the final dataset, while invalid data is tracked.","timestamp":"1731960360.0","comment_id":"1314198"},{"upvote_count":"1","content":"Selected Answer: C\n100% C","timestamp":"1726816080.0","poster":"Stefan94","comment_id":"1286697"},{"content":"Selected Answer: C\nC is the correct answer.","poster":"gdc.moser","comment_id":"1274043","timestamp":"1724848140.0","upvote_count":"1"},{"content":"Selected Answer: C\nC is the correct answer. The DROP ROW clause will cause them to NOT be added to the destination; only marked in the log.","timestamp":"1720469280.0","poster":"3fbc31b","comment_id":"1244556","upvote_count":"1"},{"timestamp":"1714290360.0","comment_id":"1203448","upvote_count":"1","content":"Selected Answer: C\nC is correct","poster":"benni_ale"},{"content":"Selected Answer: C\nC is correct","upvote_count":"1","timestamp":"1704753660.0","poster":"SerGrey","comment_id":"1117088"},{"poster":"Garyn","timestamp":"1703906400.0","upvote_count":"3","comment_id":"1109363","content":"Selected Answer: C\nC. Records that violate the expectation are dropped from the target dataset and recorded as invalid in the event log.\n\nExplanation:\n\nThe defined expectation specifies that if the timestamp is not greater than '2020-01-01', the row will be considered in violation of the constraint.\nThe ON VIOLATION DROP ROW clause states that rows that violate the constraint will be dropped from the target dataset.\nAdditionally, the expectation clause will log these violations in the event log, indicating which records did not meet the specified constraint criteria.\nThis behavior ensures that the rows failing the defined constraint are not included in the target dataset and are logged as invalid in the event log for reference or further investigation, maintaining data integrity within the dataset based on the specified constraints."},{"timestamp":"1700072940.0","upvote_count":"1","poster":"Huroye","content":"who choses these answers? The correct answer is C. The record is dropped. This is not about the default behavior. It is explicit.","comment_id":"1071782"},{"comment_id":"1058871","upvote_count":"1","content":"Selected Answer: C\nRight answer: C\nInvalid rows will be dropped as requested by the constraint and flagged as such in log files. If you need a quarantine table, you'll have to write more code.","timestamp":"1698761580.0","poster":"DavidRou"},{"poster":"vctrhugo","content":"Selected Answer: C\nC. Records that violate the expectation are dropped from the target dataset and recorded as invalid in the event log.\n\nWith the defined constraint and expectation clause, when a batch of data is processed, any records that violate the expectation (in this case, where the timestamp is not greater than '2020-01-01') will be dropped from the target dataset. These dropped records will also be recorded as invalid in the event log, allowing for auditing and tracking of the data quality issues without causing the entire job to fail.","comment_id":"998000","timestamp":"1693779720.0","upvote_count":"2"},{"content":"Selected Answer: C\nhttps://docs.databricks.com/en/delta-live-tables/expectations.html","poster":"AndreFR","comment_id":"985347","upvote_count":"2","timestamp":"1692467520.0"},{"upvote_count":"1","content":"C\nWhen a batch of data is processed in Delta Live Tables and contains data that violates the defined expectations or constraints, the expected behavior is that the records violating the expectation are dropped from the target dataset. Additionally, these violated records are recorded as invalid in the event log.","comment_id":"946489","poster":"Atnafu","timestamp":"1688821380.0"},{"content":"Selected Answer: C\nC is correct","timestamp":"1688738460.0","comment_id":"945751","poster":"mehroosali","upvote_count":"1"},{"content":"B is correct. This question is number 35 on the practice test on databricks patner academy. https://partner-academy.databricks.com/ correct answer is \"Records that violate the expectation are added to the target dataset and recorded as invalid in the event log\"","timestamp":"1681387860.0","upvote_count":"2","comment_id":"869376","comments":[{"content":"Sorry, D","upvote_count":"1","comment_id":"869706","poster":"SHINGX","timestamp":"1681414380.0","comments":[{"poster":"SHINGX","content":"I was wrong, the ON VIOLATION DROP ROW makes C the correct answer","upvote_count":"5","timestamp":"1681451700.0","comment_id":"869988"}]}],"poster":"SHINGX"},{"content":"Selected Answer: C\noption C","timestamp":"1680512220.0","comment_id":"859705","upvote_count":"4","poster":"surrabhi_4"}],"answers_community":["C (100%)"],"isMC":true,"unix_timestamp":1680437880,"answer":"C","choices":{"D":"Records that violate the expectation are added to the target dataset and recorded as invalid in the event log.","B":"Records that violate the expectation are added to the target dataset and flagged as invalid in a field added to the target dataset.","A":"Records that violate the expectation are dropped from the target dataset and loaded into a quarantine table.","C":"Records that violate the expectation are dropped from the target dataset and recorded as invalid in the event log.","E":"Records that violate the expectation cause the job to fail."},"answer_description":"","answer_ET":"C","topic":"1","exam_id":162,"answer_images":[],"timestamp":"2023-04-02 14:18:00","url":"https://www.examtopics.com/discussions/databricks/view/104855-exam-certified-data-engineer-associate-topic-1-question-32/","question_images":[]},{"id":"7TMIYRliPFNxzXswLgmw","answer_images":[],"answer":"B","answer_description":"","discussion":[{"content":"Selected Answer: B\nB. CREATE STREAMING LIVE TABLE should be used when data needs to be processed incrementally. The CREATE STREAMING LIVE TABLE syntax is used to create tables that read data incrementally, while the CREATE LIVE TABLE syntax is used to create tables that read data in batch mode. Delta Live Tables support both streaming and batch modes of processing data. When the data is streamed and needs to be processed incrementally, CREATE STREAMING LIVE TABLE should be used.","comment_id":"861990","upvote_count":"7","timestamp":"1680691140.0","poster":"4be8126"},{"upvote_count":"6","content":"Selected Answer: B\nB is the correct answer.","comment_id":"858827","timestamp":"1680437940.0","poster":"XiltroX"},{"upvote_count":"1","timestamp":"1730560080.0","comment_id":"1306208","content":"B. CREATE STREAMING LIVE TABLE should be used when data needs to be processed Streaming is used for incremental data.","poster":"ajay1709"},{"upvote_count":"1","timestamp":"1723178400.0","content":"Selected Answer: B\nB. CREATE STREAMING LIVE TABLE should be used when data needs to be processed incrementally.\nThis syntax is used to define a Delta Live Table that processes data incrementally as new data arrives, which is essential for handling streaming data or large datasets that need to be processed in chunks rather than all at once.","poster":"80370eb","comment_id":"1262766"},{"content":"Selected Answer: B\nB is correct","upvote_count":"1","comment_id":"1117090","poster":"SerGrey","timestamp":"1704753720.0"},{"timestamp":"1699367520.0","poster":"awofalus","content":"Selected Answer: B\nB is correct","comment_id":"1064881","upvote_count":"1"},{"timestamp":"1693779780.0","poster":"vctrhugo","comment_id":"998001","content":"Selected Answer: B\nB. CREATE STREAMING LIVE TABLE should be used when data needs to be processed incrementally.\n\nThe CREATE STREAMING LIVE TABLE syntax is used when you want to create Delta Live Tables (DLT) tables that are designed for processing data incrementally. This is typically used when your data pipeline involves streaming or incremental data updates, and you want the table to stay up to date as new data arrives. It allows you to define tables that can handle data changes incrementally without the need for full table refreshes.\n\nSo, option B correctly describes when to use CREATE STREAMING LIVE TABLE over CREATE LIVE TABLE in the context of Delta Live Tables.","upvote_count":"3"},{"timestamp":"1686751440.0","content":"This is old version question, currently, Databricks only have Streaming Table (Create Live Table). The previous Streaming live table and Live table already combined.","upvote_count":"2","poster":"ZSun","comments":[{"upvote_count":"3","comment_id":"928534","timestamp":"1687273740.0","poster":"lgkofficialwork","content":"is this dump valid for v3?"}],"comment_id":"923275"},{"upvote_count":"4","content":"Selected Answer: B\noption B","comment_id":"859706","poster":"surrabhi_4","timestamp":"1680512280.0"}],"question_images":[],"question_text":"Which of the following describes when to use the CREATE STREAMING LIVE TABLE (formerly CREATE INCREMENTAL LIVE TABLE) syntax over the CREATE LIVE TABLE syntax when creating Delta Live Tables (DLT) tables using SQL?","timestamp":"2023-04-02 14:19:00","exam_id":162,"unix_timestamp":1680437940,"answers_community":["B (100%)"],"url":"https://www.examtopics.com/discussions/databricks/view/104856-exam-certified-data-engineer-associate-topic-1-question-33/","question_id":97,"answer_ET":"B","isMC":true,"topic":"1","choices":{"E":"CREATE STREAMING LIVE TABLE should be used when the previous step in the DLT pipeline is static.","A":"CREATE STREAMING LIVE TABLE should be used when the subsequent step in the DLT pipeline is static.","C":"CREATE STREAMING LIVE TABLE is redundant for DLT and it does not need to be used.","D":"CREATE STREAMING LIVE TABLE should be used when data needs to be processed through complicated aggregations.","B":"CREATE STREAMING LIVE TABLE should be used when data needs to be processed incrementally."}},{"id":"W6NTZ2EikiTdVVnfQAuQ","answer_ET":"E","choices":{"C":"Databricks SQL","A":"Unity Catalog","E":"Auto Loader","D":"Data Explorer","B":"Delta Lake"},"discussion":[{"content":"Selected Answer: E\nAuto Loader is a feature in Databricks that is specifically designed to efficiently ingest new files incrementally from cloud storage directories. It can handle the scenario where files accumulate in a shared directory, and you want to ingest only the new files since the previous run without reprocessing the entire dataset.\n\nAuto Loader uses file notification services to track new files that appear in the directory, enabling incremental processing of those files as they are added.\nIt also supports schema inference and automatically manages the state of the files that have been processed, so you don't need to manually track which files have been ingested.","upvote_count":"1","timestamp":"1731960840.0","poster":"806e7d2","comment_id":"1314202"},{"poster":"80370eb","timestamp":"1723178700.0","content":"Selected Answer: E\nE. Auto Loader\nAuto Loader is designed to incrementally ingest new data files as they appear in a directory, making it ideal for scenarios where files accumulate and need to be ingested without reprocessing previously ingested files. It automatically tracks which files have already been processed, ensuring that only new files are ingested with each pipeline run.","upvote_count":"1","comment_id":"1262767"},{"content":"Selected Answer: E\nE is correct","poster":"benni_ale","upvote_count":"1","timestamp":"1714290540.0","comment_id":"1203449"},{"comment_id":"1117091","poster":"SerGrey","timestamp":"1704753840.0","content":"Selected Answer: E\nE is correct","upvote_count":"1"},{"content":"the data engineer needs to identify which files are new since the previous run. This seems to be an analysis effort. If that is the case, and I might be wrong, then DB SQL is the correct answer.","poster":"Huroye","timestamp":"1700073540.0","upvote_count":"1","comment_id":"1071789"},{"comment_id":"1058949","content":"Selected Answer: E\nAutoloader can help if you want to ingest data incrementally.","poster":"DavidRou","timestamp":"1698763800.0","upvote_count":"1"},{"poster":"AndreFR","content":"Selected Answer: E\nAuto Loader incrementally and efficiently processes new data files as they arrive in cloud storage without any additional setup.\n\nhttps://docs.databricks.com/en/ingestion/auto-loader/index.html","comment_id":"985350","timestamp":"1692467820.0","upvote_count":"2"},{"comment_id":"859707","upvote_count":"3","poster":"surrabhi_4","timestamp":"1680512280.0","content":"Selected Answer: E\noption E"},{"poster":"XiltroX","comment_id":"858828","content":"Selected Answer: E\nE is the correct answer.","timestamp":"1680438000.0","upvote_count":"4"}],"answer_description":"","question_text":"A data engineer is designing a data pipeline. The source system generates files in a shared directory that is also used by other processes. As a result, the files should be kept as is and will accumulate in the directory. The data engineer needs to identify which files are new since the previous run in the pipeline, and set up the pipeline to only ingest those new files with each run.\nWhich of the following tools can the data engineer use to solve this problem?","question_id":98,"answers_community":["E (100%)"],"exam_id":162,"question_images":[],"answer":"E","unix_timestamp":1680438000,"isMC":true,"url":"https://www.examtopics.com/discussions/databricks/view/104857-exam-certified-data-engineer-associate-topic-1-question-34/","timestamp":"2023-04-02 14:20:00","answer_images":[],"topic":"1"},{"id":"b1t1aijUmn7cwVZsueFv","exam_id":162,"question_id":99,"url":"https://www.examtopics.com/discussions/databricks/view/104858-exam-certified-data-engineer-associate-topic-1-question-35/","answer":"E","topic":"1","discussion":[{"upvote_count":"3","comment_id":"1203451","poster":"benni_ale","timestamp":"1730109120.0","content":"Selected Answer: E\nE - Aggregations are performed from silver to gold"},{"comments":[],"timestamp":"1722487860.0","comment_id":"1137339","upvote_count":"1","content":"Answer shoiuld be A , for writestream data should be stream only and not static","poster":"agAshish"},{"content":"Selected Answer: E\nE is correct","comment_id":"1117092","upvote_count":"2","poster":"SerGrey","timestamp":"1720471500.0"},{"upvote_count":"4","comment_id":"1066559","timestamp":"1715266380.0","content":"The best practice is to use \"Complete\" as output mode instead of \"append\" when working with aggregated tables. Since gold layer is work final aggregated tables, the only option with output mode as complete is option E.","poster":"surya_lolla"},{"poster":"DavidRou","timestamp":"1714481460.0","upvote_count":"1","comment_id":"1058952","content":"Selected Answer: E\nE is the right answer. The \"gold layer\" is used to store aggregated clean data, E is the only answer in wich aggregation is performed."},{"content":"Selected Answer: E\nE as we're doing an aggregation and we're rewriting the whole table and not just appending.","comment_id":"1026390","poster":"tocs","upvote_count":"2","timestamp":"1712395260.0"},{"content":"E is correct as it includes group by as well by store.","comment_id":"971585","upvote_count":"2","poster":"GhaneshK","timestamp":"1707015660.0"},{"comment_id":"861162","upvote_count":"3","poster":"rafahb","timestamp":"1696429200.0","content":"Selected Answer: E\nE option"},{"comment_id":"859711","content":"Selected Answer: E\nOption E","timestamp":"1696323900.0","poster":"surrabhi_4","upvote_count":"3"},{"content":"E is the correct answer.","timestamp":"1696249260.0","comment_id":"858829","poster":"XiltroX","upvote_count":"4"}],"timestamp":"2023-04-02 14:21:00","answer_images":[],"answer_ET":"E","question_text":"Which of the following Structured Streaming queries is performing a hop from a Silver table to a Gold table?","answers_community":["E (100%)"],"unix_timestamp":1680438060,"question_images":[],"answer_description":"","isMC":true,"choices":{"B":"","D":"","A":"","E":"","C":""}},{"id":"30ntmlnPGYfBKJRuMwga","answer_images":[],"topic":"1","answer":"D","question_images":[],"answer_description":"","discussion":[{"content":"Selected Answer: D\nD. They can navigate to the DLT pipeline page, click on each table, and view the data quality statistics.\n\nTo identify the table in a Delta Live Tables (DLT) pipeline where data is being dropped due to quality concerns, the data engineer can navigate to the DLT pipeline page, click on each table in the pipeline, and view the data quality statistics. These statistics often include information about records dropped, violations of expectations, and other data quality metrics. By examining the data quality statistics for each table in the pipeline, the data engineer can determine at which table the data is being dropped.","comment_id":"998004","timestamp":"1693780200.0","poster":"vctrhugo","upvote_count":"16"},{"timestamp":"1725199440.0","upvote_count":"1","poster":"CID2024","comment_id":"1276104","content":"The correct answer is:\nD. They can navigate to the DLT pipeline page, click on each table, and view the data quality statistics.\nDelta Live Tables provides detailed data quality statistics for each table in the pipeline. By navigating to the DLT pipeline page and clicking on each table, the data engineer can view these statistics and determine at which table the records are being dropped due to quality concerns. This allows them to identify and address the specific issues causing the data to be dropped."},{"timestamp":"1720469460.0","content":"Selected Answer: D\nCorrect answer is \"D\".","comment_id":"1244558","upvote_count":"1","poster":"3fbc31b"},{"poster":"benni_ale","comment_id":"1203454","timestamp":"1714291080.0","upvote_count":"1","content":"Selected Answer: D\nI would say D but I have never really tested it, still other solutions smell wrong"},{"upvote_count":"1","content":"D is correct \nBy clicking on each table in the DLT pipeline page, the data engineer may be able to access data quality statistics, error logs, or other information related to dropped records. This can help them pinpoint at which table in the pipeline the data is being dropped.","timestamp":"1706770380.0","comment_id":"1137342","poster":"agAshish"},{"upvote_count":"2","content":"Selected Answer: D\nE is for when an error occur. But pipeline is defined to drop some records that will not result on error","comment_id":"1089200","poster":"Diewrine","timestamp":"1701859200.0"},{"comment_id":"1064887","poster":"awofalus","content":"Selected Answer: D\nD is correct","upvote_count":"1","timestamp":"1699367940.0"},{"timestamp":"1688822760.0","poster":"Atnafu","comment_id":"946516","upvote_count":"2","content":"E\nWhen records are dropped due to quality concerns in a DLT pipeline, the errors are logged in the event log. The data engineer can navigate to the DLT pipeline page and click on the “Error” button to view the present errors. The errors will show the table where the records were dropped.\nOption A: Setting up separate expectations for each table will not help the data engineer determine which table is dropping the records.\n\nOption B: The data engineer cannot determine which table is dropping the records without looking at the event log.\n\nOption C: Setting up DLT to notify the data engineer via email when records are dropped will not help the data engineer determine which table is dropping the records.\n\nOption D: Viewing the data quality statistics for each table will not help the data engineer determine which table is dropping the records.","comments":[{"upvote_count":"1","poster":"DavidRou","content":"Don't you have to select a table generated in a single step of the pipeline to access the errors through the buttton though? Probably D is the right one here","timestamp":"1698764820.0","comment_id":"1058970"}]},{"poster":"prasioso","comment_id":"896653","content":"Selected Answer: D\nThink answer is D.\nThe pipeline is configured to drop invalid records, i.e. a SQL equivalent query with a ON VIOLATION DROP ROW clause. This will not result in a failed pipeline execution because there are no errors. Instead, you'd have to go to each table and review the quality charactistics.","comments":[{"comments":[{"timestamp":"1730207580.0","poster":"peadar_pa","comment_id":"1304461","content":"No D is correct for the reasons stated by everyone else","upvote_count":"1"}],"comment_id":"946816","poster":"Atnafu","upvote_count":"2","timestamp":"1688865120.0","content":"Option D is incorrect because viewing the data quality statistics for each table will not help the data engineer identify which table is dropping the records. The data quality statistics will show the overall quality of the data in each table, but they will not show which table is dropping the records.\n\nFor example, if the data quality statistics for a table show that 10% of the records are invalid, this does not mean that 10% of the records are being dropped. The invalid records could be being updated, inserted, or deleted."}],"timestamp":"1683979140.0","upvote_count":"4"},{"content":"Is this for v2 or v3","upvote_count":"3","poster":"[Removed]","timestamp":"1682345400.0","comment_id":"879419"},{"comment_id":"862401","timestamp":"1680721020.0","poster":"XiltroX","upvote_count":"4","content":"Selected Answer: D\nThe correct answer is D"}],"unix_timestamp":1680721020,"question_id":100,"question_text":"A data engineer has three tables in a Delta Live Tables (DLT) pipeline. They have configured the pipeline to drop invalid records at each table. They notice that some data is being dropped due to quality concerns at some point in the DLT pipeline. They would like to determine at which table in their pipeline the data is being dropped.\nWhich of the following approaches can the data engineer take to identify the table that is dropping the records?","exam_id":162,"url":"https://www.examtopics.com/discussions/databricks/view/105325-exam-certified-data-engineer-associate-topic-1-question-36/","choices":{"E":"They can navigate to the DLT pipeline page, click on the “Error” button, and review the present errors.","C":"They can set up DLT to notify them via email when records are dropped.","B":"They cannot determine which table is dropping the records.","A":"They can set up separate expectations for each table when developing their DLT pipeline.","D":"They can navigate to the DLT pipeline page, click on each table, and view the data quality statistics."},"answer_ET":"D","isMC":true,"timestamp":"2023-04-05 20:57:00","answers_community":["D (100%)"]}],"exam":{"numberOfQuestions":169,"isBeta":false,"isImplemented":true,"provider":"Databricks","lastUpdated":"12 Apr 2025","isMCOnly":true,"id":162,"name":"Certified Data Engineer Associate"},"currentPage":20},"__N_SSP":true}