{"pageProps":{"questions":[{"id":"9pPsQ1dW9Xiuc8b3xZgi","question_text":"A junior data engineer on your team has implemented the following code block.\n\n//IMG//\n\n\nThe view new_events contains a batch of records with the same schema as the events Delta table. The event_id field serves as a unique key for this table.\n\nWhen this query is executed, what will happen with new records that have the same event_id as an existing record?","exam_id":163,"question_id":106,"isMC":true,"topic":"1","unix_timestamp":1742461200,"answers_community":["B (100%)"],"answer":"B","timestamp":"2025-03-20 10:00:00","answer_images":[],"discussion":[{"poster":"Kyries","upvote_count":"1","content":"Selected Answer: B\nB is correct","comment_id":"1400957","timestamp":"1742461200.0"}],"answer_ET":"B","answer_description":"","url":"https://www.examtopics.com/discussions/databricks/view/169466-exam-certified-data-engineer-professional-topic-1-question/","question_images":["https://img.examtopics.com/certified-data-engineer-professional/image87.png"],"choices":{"A":"They are merged.","C":"They are updated.","D":"They are inserted.","B":"They are ignored."}},{"id":"4ROJiPnXNj0Ib6RtBiA3","isMC":true,"question_text":"A Structured Streaming job deployed to production has been experiencing delays during peak hours of the day. At present, during normal execution, each microbatch of data is processed in less than 3 seconds. During peak hours of the day, execution time for each microbatch becomes very inconsistent, sometimes exceeding 30 seconds. The streaming write is currently configured with a trigger interval of 10 seconds.\nHolding all other variables constant and assuming records need to be processed in less than 10 seconds, which adjustment will meet the requirement?","question_images":[],"discussion":[{"upvote_count":"9","comment_id":"1114342","poster":"RafaelCFC","content":"Selected Answer: E\nI believe this is a case of the least bad option, not exactly the best option possible.\n\n- A is wrong because in Streaming you very rarely have any executors idle, as all cores are engaged in processing the window of data;\n- B is wrong because triggering every 30s will not meet the 10s target processing interval;\n- C is wrong in two manners: increasing shuffle partitions to any number above the number of available cores in the cluster will worsen performance in streaming; also, the checkpoint folder has no connection with trigger time.\n- D is wrong because, keeping all other things the same as described by the problem, keeping the trigger time as 10s will not change the underlying conditions of the delay (i.e.: too much data to be processed in a timely manner).\n\nE is the only option that might improve processing time.","timestamp":"1704442020.0","comments":[{"comment_id":"1334780","content":"With one addition to A explanation - micro-batches are sequential by design.","poster":"arekm","timestamp":"1735647540.0","upvote_count":"1"}]},{"timestamp":"1735647600.0","upvote_count":"1","content":"Selected Answer: E\nAnswer E, see explanation by RafaelCFC.","comment_id":"1334781","poster":"arekm"},{"comment_id":"1334272","upvote_count":"1","comments":[{"upvote_count":"1","poster":"arekm","content":"Structured streaming processes batches in sequence. It does so since it guarantees exactly once processing, see: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html","timestamp":"1735647480.0","comment_id":"1334778"}],"poster":"ASRCA","timestamp":"1735578360.0","content":"Selected Answer: A\nOption A emphasizes utilizing idle executors to begin processing the next batch while longer-running tasks from previous batches finish. This approach can help maintain a steady flow of data processing and reduce the likelihood of bottlenecks."},{"poster":"Thameur01","timestamp":"1733312040.0","content":"Selected Answer: B\nIf microbatch execution occasionally exceeds 30 seconds, a trigger interval of 5 seconds would cause multiple batches to queue up while the previous batch is still running. This would exacerbate the delays and potentially lead to backpressure and failure.\nB is the best option in this case.\nIf we assume for sure that execution time should be less than 10s, then in that case a 5s interval will make more sense and E should be the best answer.","upvote_count":"1","comment_id":"1321828"},{"upvote_count":"2","comments":[{"timestamp":"1722869520.0","poster":"wdeleersnyder","content":"Ooops, I mean, D.","upvote_count":"2","comment_id":"1261092"}],"timestamp":"1722869460.0","comment_id":"1261091","content":"In Databricks Runtime 11.3 LTS and above, the Trigger.Once setting is deprecated. Databricks recommends you use Trigger.AvailableNow for all incremental batch processing workloads.\n\nhttps://docs.databricks.com/en/structured-streaming/triggers.html\n\nDoesn't seem like E is a valid and recommended option given that it is deprecated.","poster":"wdeleersnyder"},{"comment_id":"1222470","upvote_count":"2","timestamp":"1717211940.0","poster":"imatheushenrique","content":"Considering the best option for performance gain is:\nE. Decrease the trigger interval to 5 seconds; triggering batches more frequently may prevent records from backing up and large batches from causing spill."},{"timestamp":"1707886260.0","comment_id":"1149836","content":"Selected Answer: E\nE is the answer. \nEnable the settings uses the 128 MB as the target file size\nhttps://learn.microsoft.com/en-us/azure/databricks/delta/tune-file-size","upvote_count":"2","poster":"ojudz08"},{"upvote_count":"2","poster":"DAN_H","comment_id":"1136828","content":"Selected Answer: E\nE is correct as A is wrong because in Streaming you very rarely have any executors idle","timestamp":"1706710500.0"},{"poster":"kz_data","comment_id":"1118618","timestamp":"1704895620.0","upvote_count":"1","content":"Selected Answer: E\nI think is E is correct"},{"comment_id":"1105127","timestamp":"1703495580.0","content":"Selected Answer: E\ncorrect answer is E","upvote_count":"1","poster":"ervinshang"},{"content":"Only C. Even if you trigger more frequently you decrease both load and time for this load. E doesn't change anything.","comment_id":"1065030","upvote_count":"1","poster":"ofed","timestamp":"1699378320.0"},{"comment_id":"1040333","poster":"sturcu","timestamp":"1697010840.0","upvote_count":"4","content":"Selected Answer: E\nChanging trigger interval to \"one\" will cause this to be a \"batch\" and will not execute in microbranches. This will not help at all"},{"content":"correct answer is E","comment_id":"1013349","poster":"Eertyy","upvote_count":"1","timestamp":"1695322380.0"},{"poster":"azurearch","timestamp":"1694243700.0","comment_id":"1003002","upvote_count":"1","content":"sorry, the caveat is holding all other variables constant.. that means we are not allowed to change trigger intervals. is C the answer then"},{"comment_id":"1002969","content":"what if in between those 5 seconds trigger interval if there are more records, that would still increase the time it takes to process.. i doubt E is correct. I will go with answer D. it is not to execute all queries within 10 secs. it is to execute trigger now batch every 10 seconds.","timestamp":"1694239920.0","poster":"azurearch","upvote_count":"1"},{"upvote_count":"1","timestamp":"1694233860.0","poster":"azurearch","content":"A option also is about setting trigger interval to 5 seconds, just to understand.. why its not the answer","comment_id":"1002913"},{"upvote_count":"2","poster":"cotardo2077","content":"Selected Answer: E\nfor sure E","comment_id":"999175","timestamp":"1693896120.0"},{"upvote_count":"2","poster":"Eertyy","timestamp":"1693393740.0","content":"correct anwer is E","comment_id":"994000"},{"comment_id":"970010","content":"correct answer is E. D means a job will need to acquire resources in 10s which is impossible without serverless","upvote_count":"4","poster":"asmayassineg","timestamp":"1690970040.0"}],"answers_community":["E (92%)","4%"],"choices":{"B":"Increase the trigger interval to 30 seconds; setting the trigger interval near the maximum execution time observed for each batch is always best practice to ensure no records are dropped.","A":"Decrease the trigger interval to 5 seconds; triggering batches more frequently allows idle executors to begin processing the next batch while longer running tasks from previous batches finish.","D":"Use the trigger once option and configure a Databricks job to execute the query every 10 seconds; this ensures all backlogged records are processed with each batch.","E":"Decrease the trigger interval to 5 seconds; triggering batches more frequently may prevent records from backing up and large batches from causing spill.","C":"The trigger interval cannot be modified without modifying the checkpoint directory; to maintain the current stream state, increase the number of shuffle partitions to maximize parallelism."},"unix_timestamp":1690970040,"answer_description":"","timestamp":"2023-08-02 11:54:00","answer_images":[],"topic":"1","exam_id":163,"answer_ET":"E","answer":"E","url":"https://www.examtopics.com/discussions/databricks/view/117090-exam-certified-data-engineer-professional-topic-1-question/","question_id":107},{"id":"9lFLULRyWdwqJwqp7rDR","url":"https://www.examtopics.com/discussions/databricks/view/152709-exam-certified-data-engineer-professional-topic-1-question/","question_images":[],"answers_community":["D (75%)","C (25%)"],"answer_description":"","exam_id":163,"unix_timestamp":1733678220,"choices":{"D":"Maintain data quality rules in a Delta table outside of this pipeline's target schema, providing the schema name as a pipeline parameter.","C":"Maintain data quality rules in a separate Databricks notebook that each DLT notebook or file can import as a library.","B":"Use global Python variables to make expectations visible across DLT notebooks included in the same pipeline.","A":"Add data quality constraints to tables in this pipeline using an external job with access to pipeline configuration files."},"answer_images":[],"timestamp":"2024-12-08 18:17:00","isMC":true,"question_text":"A team of data engineers are adding tables to a DLT pipeline that contain repetitive expectations for many of the same data quality checks. One member of the team suggests reusing these data quality rules across all tables defined for this pipeline.\n\nWhat approach would allow them to do this?","discussion":[{"poster":"benni_ale","content":"Selected Answer: D\nhttps://docs.databricks.com/en/delta-live-tables/expectations.html\n \"You can maintain data quality rules separately from your pipeline implementations. Databricks recommends storing the rules in a Delta table with each rule categorized by a tag.\"","comment_id":"1323650","upvote_count":"5","timestamp":"1733678220.0"},{"poster":"lakime","timestamp":"1742903820.0","upvote_count":"1","comment_id":"1410019","content":"Selected Answer: C\nInitially C, currently D"},{"comment_id":"1335576","upvote_count":"1","timestamp":"1735825380.0","poster":"arekm","content":"Selected Answer: D\nD is what Databricks suggests as of now"},{"timestamp":"1733742840.0","content":"Selected Answer: C\nTo reuse repetitive data quality rules across multiple tables in a Delta Live Tables (DLT) pipeline, the most efficient approach is to maintain these rules in a separate notebook or Python module and import them where needed. This promotes code reusability, maintainability, and consistency","upvote_count":"1","poster":"Thameur01","comment_id":"1324005"}],"question_id":108,"answer":"D","topic":"1","answer_ET":"D"},{"id":"kq9QC6lcVBqdorDO5UCR","discussion":[{"poster":"mohadjhamad","content":"Selected Answer: C\nWhy a view works: By creating a view that performs a left outer join between validation_copy (the expected source of records) and report, missing records can be identified where the report.key is NULL.\nReferencing the view in DLT: The view can then be used in an expectation rule within the report table to ensure all expected records are present.","upvote_count":"1","timestamp":"1742477400.0","comment_id":"1401039"},{"timestamp":"1733679660.0","comment_id":"1323665","upvote_count":"1","content":"Selected Answer: A\nArgubly a better solution would be Define a materialised view that performs a left outer join on validation_copy and report, and define an expectation that no report key values are null : https://docs.databricks.com/en/delta-live-tables/expectations.html?utm_source=chatgpt.com#perform-advanced-validation-with-delta-live-tables-expectations","poster":"benni_ale"}],"isMC":true,"answers_community":["A (50%)","C (50%)"],"answer":"A","question_images":["https://img.examtopics.com/certified-data-engineer-professional/image94.png"],"answer_images":[],"topic":"1","answer_ET":"A","url":"https://www.examtopics.com/discussions/databricks/view/152712-exam-certified-data-engineer-professional-topic-1-question/","choices":{"D":"Define a function that performs a left outer join on validation_copy and report, and check against the result in a DLT expectation for the report table","C":"Define a view that performs a left outer join on validation_copy and report, and reference this view in DLT expectations for the report table","A":"Define a temporary table that performs a left outer join on validation_copy and report, and define an expectation that no report key values are null","B":"Define a SQL UDF that performs a left outer join on two tables, and check if this returns null values for report key values in a DLT expectation for the report table"},"question_text":"A user wants to use DLT expectations to validate that a derived table report contains all records from the source, included in the table validation_copy.\n\nThe user attempts and fails to accomplish this by adding an expectation to the report table definition.\n\n//IMG//\n\n\nWhich approach would allow using DLT expectations to validate all expected records are present in this table?","exam_id":163,"unix_timestamp":1733679660,"answer_description":"","question_id":109,"timestamp":"2024-12-08 18:41:00"},{"id":"HAFezWFahsKdX3S28ocQ","unix_timestamp":1735391340,"topic":"1","question_text":"A user new to Databricks is trying to troubleshoot long execution times for some pipeline logic they are working on. Presently, the user is executing code cell-by-cell, using display() calls to confirm code is producing the logically correct results as new transformations are added to an operation. To get a measure of average time to execute, the user is running each cell multiple times interactively.\n\nWhich of the following adjustments will get a more accurate measure of how code is likely to perform in production?","answer":"B","question_images":[],"answers_community":["B (75%)","D (25%)"],"choices":{"A":"The Jobs UI should be leveraged to occasionally run the notebook as a job and track execution time during incremental code development because Photon can only be enabled on clusters launched for scheduled jobs.","B":"The only way to meaningfully troubleshoot code execution times in development notebooks is to use production-sized data and production-sized clusters with Run All execution.","D":"Calling display() forces a job to trigger, while many transformations will only add to the logical query plan; because of caching, repeated execution of the same logic does not provide meaningful results.","C":"Production code development should only be done using an IDE; executing code against a local build of open source Spark and Delta Lake will provide the most accurate benchmarks for how code will perform in production."},"isMC":true,"timestamp":"2024-12-28 14:09:00","question_id":110,"answer_description":"","answer_ET":"B","exam_id":163,"answer_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/153569-exam-certified-data-engineer-professional-topic-1-question/","discussion":[{"upvote_count":"1","comment_id":"1402027","timestamp":"1742674500.0","content":"Selected Answer: B\nB is the right answer","poster":"suranga4"},{"poster":"mohadjhamad","upvote_count":"1","timestamp":"1742477580.0","content":"Selected Answer: D\nCalling display() forces a job to trigger,","comment_id":"1401041"},{"timestamp":"1736202420.0","content":"Selected Answer: B\nB is the right answer.","comment_id":"1337372","poster":"mouthwash","upvote_count":"1"},{"comment_id":"1335580","timestamp":"1735825740.0","poster":"arekm","upvote_count":"1","content":"Selected Answer: B\nB - this is a repeated question. You need production like data & environment to be able to troubleshoot the performance issues. display() does introduce overhead, but we use it here to investigate, where the problems start to surface. We still need the production like env and data first."}]}],"exam":{"isMCOnly":true,"provider":"Databricks","isImplemented":true,"numberOfQuestions":200,"id":163,"name":"Certified Data Engineer Professional","lastUpdated":"12 Apr 2025","isBeta":false},"currentPage":22},"__N_SSP":true}