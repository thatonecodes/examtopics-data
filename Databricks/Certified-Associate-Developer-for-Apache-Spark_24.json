{"pageProps":{"questions":[{"id":"m4chOUr38dp328FMpJBh","unix_timestamp":1688702220,"topic":"1","question_images":[],"question_id":116,"answer":"C","answers_community":["C (60%)","E (40%)"],"answer_images":[],"question_text":"Which of the following code blocks fails to return a DataFrame reverse sorted alphabetically based on column division?","url":"https://www.examtopics.com/discussions/databricks/view/114376-exam-certified-associate-developer-for-apache-spark-topic-1/","exam_id":161,"timestamp":"2023-07-07 05:57:00","choices":{"B":"storesDF.orderBy([\"division\"], ascending = [0])","E":"storesDF.sort(desc(\"division\"))","D":"storesDF.sort(\"division\", ascending – False)","A":"storesDF.orderBy(\"division\", ascending – False)","C":"storesDF.orderBy(col(\"division\").asc())"},"answer_ET":"C","isMC":true,"answer_description":"","discussion":[{"upvote_count":"2","poster":"58470e1","comment_id":"1312712","content":"Selected Answer: E\ndoesn't reverse sorted alphabetically mean descending????","timestamp":"1731688140.0"},{"upvote_count":"1","content":"Selected Answer: C\nC is the right answer. \nSee code below with Spark 3.5.1\n\n# Sort a DataFrame by a column in reverse alphabetical order\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, asc, desc\nfrom pyspark.errors import PySparkTypeError\n\nspark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n\ndata = [\n (0, 43161, \"A\"),\n (1, 51200, \"A\"),\n (2, None, \"B\"),\n (3, 78367, \"B\"),\n (4, None, \"C\"),\n ]\nstoresDF = spark.createDataFrame(data, [\"storeID\", \"sqft\", \"division\"])\n \nstoresDF.orderBy(\"division\", ascending = False).show()\nstoresDF.orderBy([\"division\"], ascending = [0]).show()\nstoresDF.orderBy(col(\"division\").asc()).show()\nstoresDF.sort(\"division\", ascending = False).show()\nstoresDF.sort(desc(\"division\")).show()","poster":"jds0","timestamp":"1721717520.0","comment_id":"1253458"},{"upvote_count":"2","comment_id":"1192457","timestamp":"1712689380.0","content":"Selected Answer: C\nC is the correct answer","poster":"dbdantas"},{"content":"C is the right answer because it returns the dataframe in ascending order.","comment_id":"1168357","poster":"azurearch","timestamp":"1709845800.0","upvote_count":"2"},{"poster":"mahmoud_salah30","content":"b i tesetd ut","timestamp":"1704006120.0","upvote_count":"1","comment_id":"1110335"},{"timestamp":"1702265820.0","upvote_count":"1","content":"C. It is the only option \"not returning\" the dataframe in descending(reverse) order.\nAll other formats are returning the descending order. \nIn Oprtion E, if we import the desc function,. it will not throw error and will return the dataframe in descending order.","comment_id":"1093059","poster":"vishnuas"},{"upvote_count":"1","content":"E. storesDF.sort(desc(\"division\"))\n\nOption E correctly uses the desc function to specify the descending order for sorting. Thank you for providing additional information and clarification.","timestamp":"1699480260.0","poster":"outwalker","comment_id":"1065987"},{"timestamp":"1690877940.0","content":"Option A and D is giving errors. ~ cannot be used in ascending. Right way is to use ascending=False. Most relevant option is C which is sorting the data in ascending order , Option A, D have some typos it should be = instead of ~.","upvote_count":"1","comment_id":"968879","poster":"singh100"},{"comment_id":"968875","timestamp":"1690877520.0","upvote_count":"3","poster":"singh100","content":"C is the answer. Only C will make the data in ascending order. Tested the code."},{"upvote_count":"3","poster":"zozoshanky","timestamp":"1690731060.0","comment_id":"967243","content":"E is right"},{"poster":"ItsAB","comment_id":"947158","content":"It's C: storesDF.orderBy(col(\"division\").asc()) => storesDF.orderBy(col(\"division\").desc())","upvote_count":"1","timestamp":"1688901420.0"},{"content":"Option E is right answer","upvote_count":"2","timestamp":"1688702220.0","poster":"lakhan0309","comment_id":"945258"}]},{"id":"EcxJdEqW3PnxxrOnW5lb","answer":"E","timestamp":"2023-04-26 13:14:00","answer_ET":"E","discussion":[{"poster":"4be8126","comment_id":"881530","timestamp":"1729941240.0","content":"Selected Answer: E\nThe answer is E.\n\nOption A returns a 10% sample, not a 15% sample as requested.\n\nOption B is incorrect because sampleBy() is used to perform stratified sampling based on a column's values.\n\nOption C is incorrect because the first argument should be set to False to prevent sampling with replacement.\n\nOption D is incorrect because the sample() method without arguments will return a 50% sample of the DataFrame.\n\nOption E is the correct answer as it returns a sample of 15% of the DataFrame without replacement.","upvote_count":"3"}],"url":"https://www.examtopics.com/discussions/databricks/view/107566-exam-certified-associate-developer-for-apache-spark-topic-1/","unix_timestamp":1682507640,"answer_description":"","question_images":[],"question_text":"Which of the following code blocks returns a 15 percent sample of rows from DataFrame storesDF without replacement?","choices":{"A":"storesDF.sample(fraction = 0.10)","C":"storesDF.sample(True, fraction = 0.10)","D":"storesDF.sample()","B":"storesDF.sampleBy(fraction = 0.15)","E":"storesDF.sample(fraction = 0.15)"},"exam_id":161,"question_id":117,"isMC":true,"answers_community":["E (100%)"],"topic":"1","answer_images":[]},{"id":"CEWfmwVGrOW7jensYhFy","url":"https://www.examtopics.com/discussions/databricks/view/107567-exam-certified-associate-developer-for-apache-spark-topic-1/","isMC":true,"question_text":"Which of the following code blocks returns all the rows from DataFrame storesDF?","topic":"1","answer_ET":"B","answer":"B","answer_images":[],"unix_timestamp":1682507700,"question_id":118,"question_images":[],"answers_community":["B (100%)"],"choices":{"A":"storesDF.head()","B":"storesDF.collect()","D":"storesDF.take()","E":"storesDF.show()","C":"storesDF.count()"},"discussion":[{"timestamp":"1698318900.0","comment_id":"881531","poster":"4be8126","upvote_count":"5","content":"Selected Answer: B\nAnswer: B\n\nExplanation:\n\nhead() returns the first n rows of the DataFrame. By default, it returns the first 5 rows.\ncollect() returns an array of Row objects that represent the entire DataFrame.\ncount() returns the number of rows in the DataFrame.\ntake(n) returns the first n rows of the DataFrame as an array of Row objects.\nshow() prints the first 20 rows of the DataFrame in a tabular form.\nOnly collect() returns all the rows from the DataFrame."},{"comment_id":"1110438","timestamp":"1719735300.0","poster":"mahmoud_salah30","upvote_count":"1","content":"b correct"}],"exam_id":161,"answer_description":"","timestamp":"2023-04-26 13:15:00"},{"id":"4kWDDJs4saqWUFmHF1eY","answers_community":["D (100%)"],"timestamp":"2023-04-26 13:17:00","isMC":true,"answer_ET":"D","topic":"1","answer_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/107568-exam-certified-associate-developer-for-apache-spark-topic-1/","question_id":119,"exam_id":161,"question_text":"Which of the following code blocks applies the function assessPerformance() to each row of DataFrame storesDF?","choices":{"D":"[assessPerformance(row) for row in storesDF.collect()]","B":"[assessPerformance() for row in storesDF]","A":"[assessPerformance(row) for row in storesDF.take(3)]","C":"storesDF.collect().apply(lambda: assessPerformance)","E":"[assessPerformance(row) for row in storesDF]"},"unix_timestamp":1682507820,"question_images":[],"answer_description":"","discussion":[{"upvote_count":"2","comment_id":"1254330","content":"Selected Answer: D\nOption D is correct. \nSee example code below:\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n\ndata = [\n (0, 43161, \"A\"),\n (1, 51200, \"A\"),\n (2, None, \"B\"),\n (3, 78367, \"B\"),\n (4, None, \"C\"),\n ]\nstoresDF = spark.createDataFrame(data, [\"storeID\", \"sqft\", \"division\"])\n\ndef myFunction(row):\n return row[0]\n\n[myFunction(row) for row in storesDF.collect()]","poster":"jds0","timestamp":"1721818080.0"},{"upvote_count":"1","timestamp":"1686088620.0","comment_id":"916666","poster":"ZSun","content":"There are many way to apply a function to dataframe.\n1. apply, as shown in option D. but it should be apply(assessPerformance)\n2. list comprehension: for row in df.collect()\n3. foreach\n4. map, but for RDD majorly"},{"poster":"4be8126","upvote_count":"2","timestamp":"1682507820.0","content":"Selected Answer: D\nThe correct answer is D.\n\nExplanation:\n\nOption A uses the take() method to extract three rows from the DataFrame, but it applies the assessPerformance() function to each row outside of the DataFrame context.\n\nOption B attempts to apply the assessPerformance() function to each row, but it doesn't reference the row object in any way.\n\nOption C tries to apply the assessPerformance() function to the entire DataFrame but does so using an incorrect syntax.\n\nOption D correctly applies the assessPerformance() function to each row of the DataFrame using a list comprehension over the result of the collect() method.\n\nOption E is similar to D, but it will iterate over rows individually instead of using the collect() method to retrieve all rows at once. While this is still a valid approach, it may be less efficient.","comment_id":"881532"}],"answer":"D"},{"id":"3FD7W1o2uMOQGKaOLuOl","question_id":120,"discussion":[{"content":"A. Task","timestamp":"1722409500.0","upvote_count":"1","poster":"singh100","comment_id":"967843"},{"poster":"Mohitsain","content":"Selected Answer: A\nThe correct answer is A. Task.","comment_id":"935301","upvote_count":"1","timestamp":"1719488400.0"},{"poster":"TmData","timestamp":"1718635680.0","comment_id":"926047","content":"Selected Answer: A\nThe correct answer is A. Task.\n\nExplanation: In the Spark execution hierarchy, the most granular level is the task. A task represents a unit of work that is executed on a partitioned portion of the data in parallel. Tasks are created by the Spark driver program and assigned to individual executors for execution. Each task operates on a subset of the data and performs a specific operation defined by the Spark application, such as a transformation or an action.","upvote_count":"2"},{"comment_id":"871366","upvote_count":"1","timestamp":"1713222720.0","content":"Selected Answer: A\nA is correct","poster":"evertonllins"}],"topic":"1","timestamp":"2023-04-16 01:12:00","answer":"A","answer_ET":"A","unix_timestamp":1681600320,"choices":{"B":"Executor","C":"Node","A":"Task","E":"Slot","D":"Job"},"answer_images":[],"question_text":"Which of the following is the most granular level of the Spark execution hierarchy?","exam_id":161,"answers_community":["A (100%)"],"answer_description":"","isMC":true,"url":"https://www.examtopics.com/discussions/databricks/view/106305-exam-certified-associate-developer-for-apache-spark-topic-1/","question_images":[]}],"exam":{"id":161,"provider":"Databricks","numberOfQuestions":185,"isImplemented":true,"isBeta":false,"lastUpdated":"12 Apr 2025","name":"Certified Associate Developer for Apache Spark","isMCOnly":true},"currentPage":24},"__N_SSP":true}