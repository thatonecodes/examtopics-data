{"pageProps":{"questions":[{"id":"g8lN536cndw1XpBJjrxb","unix_timestamp":1695598560,"answer":"B","exam_id":163,"question_images":["https://img.examtopics.com/certified-data-engineer-professional/image13.png"],"answer_images":[],"isMC":true,"timestamp":"2023-09-25 01:36:00","choices":{"E":"They are deleted.","B":"They are ignored.","D":"They are inserted.","A":"They are merged.","C":"They are updated."},"answer_ET":"B","answers_community":["B (100%)"],"question_id":121,"discussion":[{"comment_id":"1351480","timestamp":"1738685100.0","upvote_count":"1","content":"Selected Answer: B\nmerge will work if no match","poster":"Ashish7singh2020"},{"upvote_count":"2","content":"Selected Answer: B\nNo WHEN MATCHED section in MERGE, hence no action on those records, hence ignore - answer B.","poster":"arekm","comment_id":"1334806","timestamp":"1735649700.0"},{"poster":"Shakmak","upvote_count":"1","timestamp":"1732990800.0","comment_id":"1320336","content":"Selected Answer: B\nB is a correct Answer"},{"timestamp":"1717212720.0","content":"B. They are ignored.\nBecause there is not mention so there is no WHEN statement for this condition","poster":"imatheushenrique","upvote_count":"2","comment_id":"1222480"},{"content":"B is correct","poster":"PrashantTiwari","upvote_count":"2","comment_id":"1145270","timestamp":"1707460140.0"},{"comment_id":"1118705","timestamp":"1704897660.0","poster":"kz_data","content":"Selected Answer: B\nB is correct","upvote_count":"1"},{"timestamp":"1697357400.0","comment_id":"1043947","content":"Selected Answer: B\nIgnored","upvote_count":"1","poster":"alexvno"},{"content":"Selected Answer: B\nThe answer is correct. \"If none of the WHEN MATCHED conditions evaluate to true for a source and target row pair that matches the merge_condition, then the target row is left unchanged.\" \nhttps://docs.databricks.com/en/sql/language-manual/delta-merge-into.html#:~:text=If%20none%20of%20the%20WHEN%20MATCHED%20conditions%20evaluate%20to%20true%20for%20a%20source%20and%20target%20row%20pair%20that%20matches%20the%20merge_condition%2C%20then%20the%20target%20row%20is%20left%20unchanged.","timestamp":"1695598560.0","comment_id":"1016232","upvote_count":"3","poster":"rairaix"}],"question_text":"A junior data engineer on your team has implemented the following code block.\n//IMG//\n\nThe view new_events contains a batch of records with the same schema as the events Delta table. The event_id field serves as a unique key for this table.\nWhen this query is executed, what will happen with new records that have the same event_id as an existing record?","topic":"1","url":"https://www.examtopics.com/discussions/databricks/view/121337-exam-certified-data-engineer-professional-topic-1-question/","answer_description":""},{"id":"48efBnqEFsEgo5q6TEhT","answer":"B","question_images":["https://img.examtopics.com/certified-data-engineer-professional/image14.png"],"timestamp":"2023-08-02 12:40:00","answer_ET":"B","exam_id":163,"answer_description":"","choices":{"A":"Each time the job is executed, newly updated records will be merged into the target table, overwriting previous values with the same primary keys.","E":"Each time the job is executed, only those records that have been inserted or updated since the last execution will be appended to the target table, giving the desired result.","B":"Each time the job is executed, the entire available history of inserted or updated records will be appended to the target table, resulting in many duplicate entries.","D":"Each time the job is executed, the differences between the original and current versions are calculated; this may result in duplicate entries for some records.","C":"Each time the job is executed, the target table will be overwritten using the entire history of inserted or updated records, giving the desired result."},"isMC":true,"topic":"1","answer_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/117098-exam-certified-data-engineer-professional-topic-1-question/","discussion":[{"content":"Selected Answer: B\nAs update type is insert and update so B is correct option.","comment_id":"1363233","upvote_count":"1","timestamp":"1740774000.0","poster":"Chugs"},{"content":"Selected Answer: B\nB, Spark.read reads the entire table every time processed. If it was readstream then E would be answer","comment_id":"1356324","poster":"asdsadasdas","upvote_count":"3","timestamp":"1739493960.0"},{"timestamp":"1738685160.0","comment_id":"1351481","poster":"Ashish7singh2020","upvote_count":"1","content":"Selected Answer: B\nsince start version is 0"},{"comment_id":"1306485","timestamp":"1730630520.0","poster":"akashdesarda","upvote_count":"1","content":"Selected Answer: B\nThe starting version is 0, that means in every version entire data will be fetched. It is then append."},{"timestamp":"1722687720.0","content":"Correct Answer: B (not E)\nAlthough it was pretty obvious to me, I still wrote the code to check and yes, it will append the entire change during every write since starting version is mentioned as 0. \nIf in doubt, code it yourselves","comment_id":"1260280","poster":"faraaz132","upvote_count":"2"},{"comment_id":"1223829","upvote_count":"3","timestamp":"1717456740.0","poster":"imatheushenrique","content":"(\"startingVersion\", 0) that means the entiry history of table will be read so B."},{"content":"B is correct","timestamp":"1707460260.0","poster":"PrashantTiwari","comment_id":"1145271","upvote_count":"2"},{"poster":"kz_data","comment_id":"1118714","content":"Selected Answer: B\nB is correct","timestamp":"1704897780.0","upvote_count":"2"},{"content":"Selected Answer: B\nCorrect B","comment_id":"1111099","upvote_count":"1","timestamp":"1704104880.0","poster":"5ffcd04"},{"timestamp":"1702109940.0","comment_id":"1091521","content":"Selected Answer: B\ncorrect answer is B.","upvote_count":"1","poster":"azurelearn2020"},{"poster":"[Removed]","upvote_count":"1","comments":[],"timestamp":"1701718680.0","comment_id":"1087907","content":"Selected Answer: E\nConsidering that we are talking about Change Data Feed and the code is filtering by[ \"update_postimage\", \"insert\" ] the column \"_change_type\", I would go with the option E.\n\nReference:\nhttps://docs.delta.io/latest/delta-change-data-feed.html#:~:text=_change_type,update_preimage%20%2C%20update_postimage"},{"comments":[{"poster":"[Removed]","comment_id":"1087910","content":"I'm with you, follow the reference:\n\nhttps://docs.delta.io/latest/delta-change-data-feed.html#:~:text=_change_type,update_preimage%20%2C%20update_postimage","timestamp":"1701718920.0","upvote_count":"1"}],"poster":"jyothsna12496","content":"why is it Not E. It gets newly inserted or updated records","upvote_count":"1","timestamp":"1697646180.0","comment_id":"1047064"},{"comment_id":"1040450","poster":"sturcu","timestamp":"1697017260.0","content":"Selected Answer: B\ncorrect","upvote_count":"1"},{"poster":"azurearch","content":"B is the right answer, sorry.","timestamp":"1694316900.0","upvote_count":"2","comment_id":"1003658"},{"poster":"azurearch","timestamp":"1694176740.0","content":"answer is A, because there is a filter as asmayassineg said. Filter filters only existing records from change feed","comment_id":"1002467","upvote_count":"1"},{"comment_id":"970047","poster":"asmayassineg","timestamp":"1690972860.0","upvote_count":"2","content":"sorry, answer is correct B."},{"poster":"asmayassineg","comments":[{"comment_id":"1131934","upvote_count":"1","content":"there is also insert in the filter.","timestamp":"1706206800.0","poster":"mht3336"},{"content":"it's B:\nReading table’s changes, captured by CDF, using spark.read means that you are reading them as a static source. So, each time you run the query, all table’s changes (starting from the specified startingVersion) will be read.","upvote_count":"6","comment_id":"988249","timestamp":"1692790800.0","poster":"taif12340"}],"timestamp":"1690972800.0","comment_id":"970046","content":"Answer is A, since the df is filtering on updated records using update_postimage filter","upvote_count":"2"}],"question_id":122,"question_text":"A junior data engineer seeks to leverage Delta Lake's Change Data Feed functionality to create a Type 1 table representing all of the values that have ever been valid for all rows in a bronze table created with the property delta.enableChangeDataFeed = true. They plan to execute the following code as a daily job:\n//IMG//\n\nWhich statement describes the execution and results of running the above query multiple times?","unix_timestamp":1690972800,"answers_community":["B (92%)","8%"]},{"id":"9IZ0tsUD7ka1ifgNnTIp","answer_description":"","answer":"E","answer_images":[],"isMC":true,"question_id":123,"question_images":[],"unix_timestamp":1699170900,"discussion":[{"content":"Selected Answer: E\nE is the right answer, as the table in bronze can be replayed again when required.","comment_id":"1558223","poster":"kishanu","timestamp":"1743932520.0","upvote_count":"1"},{"comment_id":"1363433","upvote_count":"1","timestamp":"1740812400.0","poster":"Tedet","content":"Selected Answer: A\nConsidering the Databricks documentation on change feed and your need to process new records that have not been processed yet, Option A might actually be a better fit since you're looking for a streaming solution that can continuously monitor new records. The change feed (Option D) works for batch processing changes from a specific version, which isn't ideal for real-time streaming."},{"timestamp":"1735741800.0","poster":"HairyTorso","upvote_count":"1","content":"Selected Answer: E\nE lgtm","comment_id":"1335204"},{"timestamp":"1733810040.0","content":"Selected Answer: E\nWhen we design pipeline, we will have to make sure data from source will be present there in the raw layer/bronze layer and the transformation we make should be done in refine and enterprise layer so by this way we can tackle this kind of situation where the necessary column was not replicated in previous runs of pipeline and we can create new column based on raw data we have.","comment_id":"1324397","upvote_count":"3","poster":"Anithec0der"},{"upvote_count":"3","timestamp":"1717461600.0","content":"Medallion Architecture is named in E. (Ingesting all raw data and metadata from Kafka to a bronze Delta table creates a permanent, replayable history of the data state.)","comment_id":"1223839","poster":"imatheushenrique"},{"upvote_count":"2","content":"Selected Answer: E\nE is correct","timestamp":"1707891180.0","poster":"ojudz08","comment_id":"1149866"},{"upvote_count":"1","timestamp":"1706794020.0","content":"Selected Answer: E\nI think E is correct","comment_id":"1137643","poster":"DAN_H"},{"poster":"kz_data","content":"Selected Answer: E\nI think E is correct","comment_id":"1118730","upvote_count":"1","timestamp":"1704898740.0"},{"upvote_count":"2","timestamp":"1699170900.0","content":"Selected Answer: E\nLooks good - E","poster":"alexvno","comment_id":"1062710"}],"choices":{"E":"Ingesting all raw data and metadata from Kafka to a bronze Delta table creates a permanent, replayable history of the data state.","C":"Delta Lake automatically checks that all fields present in the source data are included in the ingestion layer.","D":"Data can never be permanently dropped or deleted from Delta Lake, so data loss is not possible under any circumstance.","B":"Delta Lake schema evolution can retroactively calculate the correct value for newly added fields, as long as the data was in the original source.","A":"The Delta log and Structured Streaming checkpoints record the full history of the Kafka producer."},"topic":"1","answer_ET":"E","url":"https://www.examtopics.com/discussions/databricks/view/125387-exam-certified-data-engineer-professional-topic-1-question/","timestamp":"2023-11-05 08:55:00","answers_community":["E (92%)","8%"],"question_text":"A new data engineer notices that a critical field was omitted from an application that writes its Kafka source to Delta Lake. This happened even though the critical field was in the Kafka source. That field was further missing from data written to dependent, long-term storage. The retention threshold on the Kafka service is seven days. The pipeline has been in production for three months.\nWhich describes how Delta Lake can help to avoid data loss of this nature in the future?","exam_id":163},{"id":"4umAXcyONTdzTWREgmr2","answers_community":["D (100%)"],"url":"https://www.examtopics.com/discussions/databricks/view/117385-exam-certified-data-engineer-professional-topic-1-question-3/","question_images":[],"topic":"1","timestamp":"2023-08-05 07:43:00","answer_images":[],"isMC":true,"answer":"D","question_id":124,"answer_description":"","discussion":[{"comment_id":"972714","timestamp":"1691214180.0","content":"the answer given is correct:\nMaximum concurrent runs: Set to 1. There must be only one instance of each query concurrently active.\nRetries: Set to Unlimited.\nhttps://docs.databricks.com/en/structured-streaming/query-recovery.html","upvote_count":"11","poster":"8605246"},{"timestamp":"1743780840.0","upvote_count":"1","content":"Selected Answer: D\nCant be all purpose general compute","poster":"codebender","comment_id":"1473795"},{"timestamp":"1738700580.0","content":"Selected Answer: D\nJob cluster autoterminates, and you want retries for recover","comment_id":"1351570","poster":"EelkeV","upvote_count":"1"},{"upvote_count":"2","timestamp":"1727523900.0","comment_id":"1290638","poster":"akashdesarda","content":"Selected Answer: D\nUse databricks jobs as it as native integration with Streaming use case. See the example Job here https://docs.databricks.com/en/structured-streaming/query-recovery.html#configure-structured-streaming-jobs-to-restart-streaming-queries-on-failure"},{"poster":"imatheushenrique","timestamp":"1717547340.0","content":"D. Cluster: New Job Cluster;\nRetries: Unlimited;\nMaximum Concurrent Runs: 1","comment_id":"1224434","upvote_count":"1"},{"timestamp":"1717208760.0","comment_id":"1222434","poster":"imatheushenrique","content":"D. Cluster: New Job Cluster;\nRetries: Unlimited;\nMaximum Concurrent Runs: 1","upvote_count":"1"},{"timestamp":"1712586900.0","comment_id":"1191628","content":"D is correct\nhttps://docs.databricks.com/en/structured-streaming/query-recovery.html","upvote_count":"1","poster":"juliom6"},{"timestamp":"1705862100.0","content":"Correct Ans is D","poster":"AziLa","upvote_count":"1","comment_id":"1128056"},{"poster":"Jay_98_11","timestamp":"1705147680.0","content":"Selected Answer: D\nD is correct","upvote_count":"1","comment_id":"1121587"},{"comment_id":"1102665","upvote_count":"1","poster":"kz_data","timestamp":"1703172120.0","content":"Selected Answer: D\nD is correct"},{"upvote_count":"1","content":"Selected Answer: D\nD is correct","comment_id":"1044664","timestamp":"1697431740.0","poster":"sturcu"}],"question_text":"When scheduling Structured Streaming jobs for production, which configuration automatically recovers from query failures and keeps costs low?","answer_ET":"D","exam_id":163,"choices":{"D":"Cluster: New Job Cluster;\nRetries: Unlimited;\nMaximum Concurrent Runs: 1","E":"Cluster: Existing All-Purpose Cluster;\nRetries: None;\nMaximum Concurrent Runs: 1","C":"Cluster: Existing All-Purpose Cluster;\nRetries: Unlimited;\nMaximum Concurrent Runs: 1","B":"Cluster: New Job Cluster;\nRetries: None;\nMaximum Concurrent Runs: 1","A":"Cluster: New Job Cluster;\nRetries: Unlimited;\nMaximum Concurrent Runs: Unlimited"},"unix_timestamp":1691214180},{"id":"orSGipt9wuE4e50JXHlo","answer":"A","question_text":"A nightly job ingests data into a Delta Lake table using the following code:\n//IMG//\n\nThe next step in the pipeline requires a function that returns an object that can be used to manipulate new records that have not yet been processed to the next table in the pipeline.\nWhich code snippet completes this function definition?\ndef new_records():","question_images":["https://img.examtopics.com/certified-data-engineer-professional/image15.png"],"answer_images":[],"question_id":125,"answers_community":["A (46%)","E (28%)","D (24%)","1%"],"topic":"1","exam_id":163,"isMC":true,"choices":{"A":"return spark.readStream.table(\"bronze\")","C":"","B":"return spark.readStream.load(\"bronze\")","D":"return spark.read.option(\"readChangeFeed\", \"true\").table (\"bronze\")","E":""},"unix_timestamp":1694644500,"answer_description":"","timestamp":"2023-09-14 00:35:00","answer_ET":"A","discussion":[{"comment_id":"1068885","content":"Selected Answer: D\n# not providing a starting version/timestamp will result in the latest snapshot being fetched first\nspark.readStream.format(\"delta\") \\\n .option(\"readChangeFeed\", \"true\") \\\n .table(\"myDeltaTable\")\nPlease refer:\nhttps://docs.databricks.com/en/delta/delta-change-data-feed.html","comments":[{"comment_id":"1334815","poster":"arekm","upvote_count":"2","timestamp":"1735651020.0","content":"Answer D would require specifying the start and (optionally) the end version for reading data from CDF. So D does not seem to be correct."},{"upvote_count":"2","timestamp":"1728581820.0","poster":"shaojunni","comment_id":"1295691","content":"readChangeFeed is disabled by default."},{"upvote_count":"3","poster":"t_d_v","timestamp":"1724009460.0","comments":[{"comments":[{"upvote_count":"1","poster":"arekm","timestamp":"1735651320.0","comment_id":"1334819","content":"CDF without a stream requires a starting version at the minimum."}],"content":"You can read Delta Lake Change Data Feed without using a stream. You can use batch queries to read the change data feed by setting the readChangeFeed option to true.","upvote_count":"2","comment_id":"1272414","timestamp":"1724648100.0","poster":"GHill1982"}],"content":"There is no stream in option D","comment_id":"1268203"}],"upvote_count":"12","timestamp":"1699825620.0","poster":"AzureDE2522"},{"upvote_count":"8","content":"In my opinion E is not correct because we do not see parameters pass within to the function (year, month and day)... the function is def new_records():","timestamp":"1700139720.0","poster":"Laraujo2022","comment_id":"1072418"},{"content":"Selected Answer: A\nOption A is best because it creates a streaming source that reads only new appended data from the \"bronze\" table incrementally. Even if ingestion is done in batch, using spark.readStream.table(\"bronze\") lets downstream processing treat the table as a live data stream.","upvote_count":"2","timestamp":"1743083460.0","comment_id":"1410910","poster":"AlHerd"},{"comment_id":"1363432","timestamp":"1740811980.0","content":"Selected Answer: D\nExplanation: This is the best option for Delta Lake, as it uses the readChangeFeed option. This option is specifically designed to read only the new changes (insertions, updates, or deletions) since the last read, which is exactly what is needed when you want to handle new records that have not yet been processed. This ensures that only records that are new or changed since the last read are returned.\nConclusion: This is the correct choice, as it ensures that only new records are read.","poster":"Tedet","upvote_count":"1"},{"poster":"asdsadasdas","content":"Selected Answer: A\n\"manipulate new records that have *not yet been processed* to the next table \" readstream can incrementally pick data yet to be processed. with D the issue is spark.read it will read the entire table","timestamp":"1739495220.0","comment_id":"1356327","upvote_count":"1","comments":[{"timestamp":"1739495460.0","poster":"asdsadasdas","content":"Batch (read) Reads all available CDF history starting from the earliest retained version May load too much data or fail if old versions are deleted\nStreaming (readStream) Starts from the latest version unless a checkpoint exists","upvote_count":"1","comment_id":"1356328"}]},{"timestamp":"1739082240.0","comment_id":"1353735","upvote_count":"2","poster":"shaswat1404","content":"Selected Answer: E\nin option A and B assume steaming ingestiopn but ingestion is in batch mode \nin option C current_timestamp is used which is dynamic and changes every time the query is executed therefore it wont correctly filter records injested in the last batch\nin option D it only works if delta.enableChangeDataFeed = true was set on the table before the ingestion (its disabled by default and given query does not set this option as true) therefore this option is in valid\noption E is correct as it correctly filters from the most recent batch as it uses file path to retrieve only data from the latest ingestion column source_file was created specifically for this purpose ensuring the function returns onle new records.."},{"content":"Selected Answer: A\nYou can read data from the delta table using structured streaming. You have 2 options:\n- without CDF - only process new rows (without updates and deletes)\n- with CDF - all changes to the data, i.e. insert, update, delete.\n\nAnswer A uses the first option. However, in the question they talk about \"new records\". So using streaming for new records is OK. Answer A is correct.","timestamp":"1735651200.0","comment_id":"1334816","comments":[],"upvote_count":"2","poster":"arekm"},{"upvote_count":"1","content":"Selected Answer: E\nNew records will be filtered for D /","comment_id":"1327625","poster":"sgerin","timestamp":"1734381780.0"},{"poster":"temple1305","upvote_count":"1","content":"Selected Answer: D\nNew records will be filtered for D - \nexample https://delta.io/blog/2023-07-14-delta-lake-change-data-feed-cdf/","timestamp":"1734294960.0","comment_id":"1327051"},{"content":"Selected Answer: A\nAnswer A. A better approach would involve streaming directly from the Delta table (Option A), possibly along with using metadata like ingest_time to track new records more accurately.\nIt might be better to rely on the streaming process itself rather than trying to filter based on the file path (option E).","poster":"AlejandroU","upvote_count":"1","timestamp":"1734161520.0","comment_id":"1326380"},{"timestamp":"1733313540.0","upvote_count":"1","comment_id":"1321841","content":"Selected Answer: E\nUsing the source_file metadata field allows you to filter new records ingested from specific files.\nE is the most robust and reliable option for tracking and working with new records in this batch ingestion pipeline.","poster":"Thameur01"},{"upvote_count":"1","comment_id":"1319134","poster":"benni_ale","content":"Selected Answer: E\nI tried myself but none really works","timestamp":"1732785120.0"},{"timestamp":"1729510740.0","poster":"cbj","comment_id":"1300941","upvote_count":"2","content":"Selected Answer: A\nOthers can't ensure data not being processed. e.g. if the code not run for one day and run next day, C or E will mis process one day's data."},{"content":"Selected Answer: A\nsince \"bronze\" table is a delta table, readStream() only returns new data.","poster":"shaojunni","comment_id":"1295700","timestamp":"1728582960.0","upvote_count":"4"},{"upvote_count":"2","comment_id":"1295163","content":"Selected Answer: E\nIf the job runs only once per day, then option E could indeed be a valid and effective solution. Here's why:\n\nDaily Execution: Since the job runs once per day, all records ingested on that day would be new and unprocessed.\nSource File Filtering: The filter condition col(\"source_file\").like(f\"/mnt/daily_batch/{year}/{month}/{day}\") would select only the records that were ingested from the current day's batch file.\nSimplicity: This approach is straightforward and doesn't require maintaining additional state (like last processed version or timestamp).\nReliability: As long as the daily batch files are consistently named and placed in the correct directory structure, this method will reliably capture all new records for that day.","poster":"pk07","timestamp":"1728479040.0"},{"content":"Selected Answer: A\nA is correct by Elimination. As stated by Alaverdi in another comment. Reads delta table as a stream and processes only newly arrived records. \n\nB excluded because of incorrect syntax\n\nC excluded, will be an empty result, as ingestion time (which comes as a param in the other method) is compared with current timestamp\n\nD excluded because of syntax error, should be : spark.read.option(\"readChangeFeed\", \"true\").option(\"startingVersion\", 1).table(\"bronze\")\n\nE excluded, will be an empty result, because “source_file” give a filename, while f\"/mnt \n/daily_batch/{year}/{month}/{day}\" gives a folder name","timestamp":"1727274780.0","comment_id":"1289057","poster":"AndreFR","upvote_count":"7"},{"timestamp":"1724010420.0","content":"Selected Answer: C\nActually it's hard to choose between C and E, as both are a bit incorrect:\nOption E - seems like it will be an empty result, as file name is compared with folder name\nOption C - seems like it will be an empty result, as ingestion time (which comes as a param in the other method) is compared with current timestamp.\n\nOn the other hand, if new_records method had an ingestion time param, then the task would be obvious. Also considering the very first line which imports current_timestamp, let me say it's C :))","poster":"t_d_v","comment_id":"1268210","upvote_count":"1"},{"comment_id":"1267042","content":"Selected Answer: D\nD is correct","upvote_count":"1","timestamp":"1723810740.0","poster":"partha1022"},{"comment_id":"1260286","timestamp":"1722688140.0","poster":"faraaz132","content":"Selected Answer: E\nCorrect Answer : E\nSince, it selects only those records which have been loaded on the specified date and these records are not processed yet. This is what we want\n\nNot A : It reads all records even the ones previously processed since bronze table keeps historic data.\n\nNot D : It is no where mentioned that change data feed is enabled, nor is it present in the code snippet. This is where we have to be careful with self- assumption","upvote_count":"3"},{"timestamp":"1722370320.0","poster":"aiwithqasim","comment_id":"1258389","content":"Option D. return spark.read.option(\"readChangeFeed\", \"true\").table (\"bronze\")\n\nThe following code snippet is from https://delta.io/blog/2023-07-14-delta-lake-change-data-feed-cdf/ where the writer explained what will happen if we give \"readChangeFeed\", and \"true\". It will include all the details from the respective mentioned version.\n\nIn our in option D starting version is not described it will pick the latest record. Please refer to the doc https://docs.databricks.com/en/delta/delta-change-data-feed.html and find \"By default, the stream returns the latest snapshot of the table when the stream first starts as an INSERT and future changes as change data.\"\n(\n spark.read.format(\"delta\")\n .option(\"readChangeFeed\", \"true\")\n .option(\"startingVersion\", 0)\n .table(\"people\")\n .show(truncate=False)\n)","upvote_count":"3"},{"poster":"zhiva","timestamp":"1719469080.0","comment_id":"1237907","content":"Selected Answer: A\nBoth E and A can be correct but in the definition of the function there are no input parameters. This means we can't use them correctly in returned statement only with the given information in the question. This is why I vote for A","upvote_count":"2"},{"timestamp":"1717461900.0","poster":"imatheushenrique","content":"The E option makes more sense because all the partition would be filtered.\nCan't be the options that use CDF because theres no readChangeFeed option in dataframe read","comment_id":"1223840","upvote_count":"1"},{"content":"Selected Answer: E\nSince the ingest_daily_batch function writes to the \"bronze\" table in batch mode using spark.read and write operations, we should not use readStream to read from it in the subsequent function.","timestamp":"1711558080.0","comment_id":"1184233","upvote_count":"2","poster":"arik90"},{"upvote_count":"1","poster":"alexvno","timestamp":"1710329520.0","comment_id":"1172494","content":"Selected Answer: E\nProbable E, but still filename not specified only folder path"},{"upvote_count":"2","poster":"vikram12apr","comment_id":"1168704","timestamp":"1709890740.0","content":"Selected Answer: E\nPlease read the question again .\nit is asking to get the data from bronze table to the some downstream table.\nNow as its a append only daily nightly job\nthe filter on file name will give the new data available in bronze table which is still not flown down the pipeline."},{"upvote_count":"1","content":"D is correct.\nhttps://delta.io/blog/2023-07-14-delta-lake-change-data-feed-cdf/\nCDF can be enabled on non-streaming Delta table.. \"delta\" is default table format.","poster":"agreddy","timestamp":"1708528860.0","comment_id":"1155635"},{"comment_id":"1149873","timestamp":"1707891960.0","upvote_count":"1","content":"Selected Answer: D\nthe question here is how to manipulate new records that have not yet been processed to the next table, since the data has been ingested into the bronze table you need to check whether or not the data ingested daily is already there in the silver table, so I think answer is D. Enabling change data feed allows to track row-level changes between delta table versions \n\nhttps://docs.databricks.com/en/delta/delta-change-data-feed.html","poster":"ojudz08"},{"comment_id":"1141033","poster":"guillesd","upvote_count":"1","content":"the problem here is that both A and E are correct. E just follows the previous filtering logic while A uses the readStream method which will have to maintain a checkpoint. But both can work","timestamp":"1707133500.0"},{"poster":"DAN_H","upvote_count":"4","content":"Selected Answer: A\nA as Structured Streaming incrementally reads Delta tables. While a streaming query is active against a Delta table, new records are processed idempotently as new table versions commit to the source table.","timestamp":"1706794500.0","comment_id":"1137653"},{"content":"Selected Answer: A\nA is Correct","timestamp":"1706697180.0","comment_id":"1136640","poster":"adenis","upvote_count":"1"},{"content":"A is Correct","poster":"adenis","comment_id":"1136638","timestamp":"1706697120.0","upvote_count":"1"},{"content":"Selected Answer: E\ncan't be D since no read option in CDF. https://docs.databricks.com/en/delta/delta-change-data-feed.html","poster":"Jay_98_11","comments":[{"comment_id":"1131945","content":"spark.read.format(\"delta\") \\\n .option(\"readChangeFeed\", \"true\") \\\n .option(\"startingVersion\", 0) \\\n .option(\"endingVersion\", 10) \\\n .table(\"myDeltaTable\")","upvote_count":"1","poster":"mht3336","timestamp":"1706207280.0"}],"timestamp":"1705171200.0","comment_id":"1121964","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: E\nE addresses the desired filtering, while keeping with the logic of the first step being a batch job, and has no code errors.","poster":"RafaelCFC","timestamp":"1704444240.0","comment_id":"1114361"},{"comment_id":"1095606","poster":"alaverdi","content":"Selected Answer: A\nIn my opinion A is the correct answer. You read delta table as a stream and process only newly arrived records. This is maintained while writing the stream with the state stored in checkpoint location.\nspark.readStream.table(\"bronze\")\n .writeStream\n .format(\"delta\")\n .outputMode(\"append\")\n .option(\"checkpointLocation\", \"/path/to/checkpoints/\")\n .toTable(\"silver\")","timestamp":"1702481760.0","upvote_count":"5"},{"timestamp":"1697860680.0","upvote_count":"1","comment_id":"1049249","content":"E is correct.\nD use invalid option refer to see sample in https://docs.databricks.com/en/delta/delta-change-data-feed.html .\nA , B didn't filter ,so it will gather whole table data.\nE uses the knew value to filter .","poster":"chokthewa"},{"upvote_count":"2","timestamp":"1697450040.0","content":"Selected Answer: E\nwe filter on the file_path","poster":"sturcu","comment_id":"1044826"},{"timestamp":"1695475740.0","content":"Yes it is E because D, there is o mention that delta.enableChangeDataFeed is true. Also there is no read option in CDF. it is table changes (batch) or readStream. URL https://docs.databricks.com/en/delta/delta-change-data-feed.html","poster":"MarceloManhaes","comment_id":"1014938","upvote_count":"1"},{"upvote_count":"2","poster":"hammer_1234_h","comment_id":"1009485","content":"it could be E, but not D","timestamp":"1694914620.0"},{"comment_id":"1006999","timestamp":"1694644500.0","content":"the answer should be A \nhttps://docs.databricks.com/en/structured-streaming/delta-lake.html#delta-table-as-a-source","poster":"hammer_1234_h","upvote_count":"1"}],"url":"https://www.examtopics.com/discussions/databricks/view/120700-exam-certified-data-engineer-professional-topic-1-question/"}],"exam":{"lastUpdated":"12 Apr 2025","isBeta":false,"id":163,"numberOfQuestions":200,"provider":"Databricks","name":"Certified Data Engineer Professional","isImplemented":true,"isMCOnly":true},"currentPage":25},"__N_SSP":true}