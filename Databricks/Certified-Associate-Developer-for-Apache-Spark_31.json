{"pageProps":{"questions":[{"id":"s2rbX7ttUrh64lqkjhdi","answer_images":[],"discussion":[{"content":"should be A","comment_id":"1189845","timestamp":"1728122400.0","poster":"Sowwy1","upvote_count":"2"}],"answer_description":"","unix_timestamp":1712268000,"timestamp":"2024-04-05 00:00:00","choices":{"A":"1. withColumn\n2. col\n3. cast\n4. StringType()","B":"1. withColumn\n2. cast\n3. col\n4. StringType()","D":"1. withColumn\n2. cast\n3. col\n4. StringType","E":"1. withColumn\n2. col\n3. cast\n4. StringType","C":"1. newColumn\n2. col\n3. cast\n4. StringType()"},"question_id":151,"answer_ET":"A","exam_id":161,"topic":"1","url":"https://www.examtopics.com/discussions/databricks/view/137956-exam-certified-associate-developer-for-apache-spark-topic-1/","answers_community":[],"question_text":"The code block shown below should return a new DataFrame from DataFrame storesDF where column storeId is of the type string. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.\n\nCode block:\n\nstoresDF.__1__(\"storeId\", __2__(\"storeId\").__3__(__4__)","question_images":[],"answer":"A","isMC":true},{"id":"ilHmgIDVoGCFzI3S9buC","discussion":[{"poster":"Sowwy1","comment_id":"1189846","content":"should be C","upvote_count":"2","timestamp":"1728122400.0"}],"choices":{"A":"storesDF.withColumn(\"modality\", lit(PHYSICAL))","E":"storesDF.withColumn(\"modality\", \"PHYSICAL\")","D":"storesDF.withColumn(\"modality\", StringType(\"PHYSICAL\"))","B":"storesDF.withColumn(\"modality\", col(\"PHYSICAL\"))","C":"storesDF.withColumn(\"modality\", lit(\"PHYSICAL\"))"},"question_id":152,"exam_id":161,"answer_images":[],"topic":"1","question_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/137957-exam-certified-associate-developer-for-apache-spark-topic-1/","isMC":true,"timestamp":"2024-04-05 00:00:00","answer_ET":"C","answer_description":"","question_text":"Which of the following code blocks returns a new DataFrame from DataFrame storesDF where column modality is the constant string \"PHYSICAL\"? Assume DataFrame storesDF is the only defined language variable.","unix_timestamp":1712268000,"answer":"C","answers_community":[]},{"id":"Kl2udW8f9LKVS2HyyCkp","answer":"E","unix_timestamp":1686163320,"question_id":153,"url":"https://www.examtopics.com/discussions/databricks/view/111436-exam-certified-associate-developer-for-apache-spark-topic-1/","exam_id":161,"answer_description":"","answers_community":["E (100%)"],"answer_images":[],"topic":"1","question_images":[],"answer_ET":"E","discussion":[{"content":"E is correct.","poster":"singh100","comment_id":"967863","upvote_count":"1","timestamp":"1722411420.0"},{"upvote_count":"2","comment_id":"926052","timestamp":"1718635920.0","poster":"TmData","content":"Selected Answer: E\nThe correct answer is E. By default, DataFrames will be split into 200 unique partitions when data is being shuffled.\n\nExplanation: The spark.sql.shuffle.partitions configuration parameter in Spark determines the number of partitions to use when shuffling data. When a shuffle operation occurs, such as during DataFrame joins or aggregations, data needs to be redistributed across partitions based on a specific key. The spark.sql.shuffle.partitions value defines the default number of partitions to be used during such shuffling operations."},{"content":"E. By default, DataFrames will be split into 200 unique partitions when data is being shuffled.\n\nThe spark.sql.shuffle.partitions configuration parameter determines the number of partitions that are used when shuffling data for joins or aggregations. The default value is 200, which means that by default, when a shuffle operation occurs, the data will be divided into 200 partitions. This allows the tasks to be distributed across the cluster and processed in parallel, improving performance.\n\nHowever, the optimal number of shuffle partitions depends on the specific details of your cluster and data. If the number is too small, then each partition will be large, and the tasks may take a long time to run. If the number is too large, then there will be many small tasks, and the overhead of scheduling and processing all these tasks can degrade performance. Therefore, tuning this parameter to match your specific use case can help optimize the performance of your Spark applications.","upvote_count":"2","timestamp":"1717785720.0","comment_id":"917527","poster":"sumand"}],"choices":{"D":"By default, all DataFrames in Spark, including existing DataFrames, will be split into 200 unique segments for parallelization.","A":"By default, all DataFrames in Spark will be spit to perfectly fill the memory of 200 executors.","C":"By default, Spark will only read the first 200 partitions of DataFrames to improve speed.","B":"By default, new DataFrames created by Spark will be split to perfectly fill the memory of 200 executors.","E":"By default, DataFrames will be split into 200 unique partitions when data is being shuffled."},"isMC":true,"timestamp":"2023-06-07 20:42:00","question_text":"The default value of spark.sql.shuffle.partitions is 200. Which of the following describes what that means?"},{"id":"WnVBTVzG3WtskbfrPXHS","url":"https://www.examtopics.com/discussions/databricks/view/116795-exam-certified-associate-developer-for-apache-spark-topic-1/","question_id":154,"question_text":"The code block shown below contains an error. The code block is intended to return a new DataFrame where column managerName from DataFrame storesDF is split at the space character into column managerFirstName and column managerLastName. Identify the error.\n\nA sample of DataFrame storesDF is displayed below:\n\n//IMG//\n\n\nCode block:\n\nstoresDF.withColumn(\"managerFirstName\", col(\"managerName\").split(\" \").getItem(0))\n.withColumn(\"managerLastName\", col(\"managerName\").split(\" \").getItem(1))","exam_id":161,"choices":{"D":"The split() operation comes from the imported functions object. It accepts a Column object and split character as arguments. It is not a method of a Column object.","A":"The index values of 0 and 1 are not correct â€“ they should be 1 and 2, respectively.","B":"The index values of 0 and 1 should be provided as second arguments to the split() operation rather than indexing the result.","E":"The withColumn operation cannot be called twice in a row.","C":"The split() operation comes from the imported functions object. It accepts a string column name and split character as arguments. It is not a method of a Column object."},"timestamp":"2023-07-30 19:35:00","answer":"C","isMC":true,"unix_timestamp":1690738500,"topic":"1","question_images":["https://img.examtopics.com/certified-associate-developer-for-apache-spark/image8.png"],"answers_community":["C (100%)"],"answer_description":"","discussion":[{"upvote_count":"2","timestamp":"1738241460.0","content":"In my opinion both C and D are correct because split function from package pyspark.sql.functions accepts both column object or string as a first parameter","comment_id":"1349058","poster":"tmz1"},{"poster":"Sowwy1","timestamp":"1712767740.0","content":"D. The split() operation comes from the imported functions object. It accepts a Column object and split character as arguments. It is not a method of a Column object.","comment_id":"1193168","upvote_count":"1"},{"content":"Answer C\npyspark.sql.functions provides a function split() to split DataFrame string Column into multiple columns.\nhttps://sparkbyexamples.com/pyspark/pyspark-split-dataframe-column-into-multiple-columns/","upvote_count":"1","comment_id":"1159609","poster":"Ahlo","timestamp":"1708944600.0"},{"content":"Selected Answer: C\nI think it is C \ndata = [\n (\"John Smith\",), \n (\"Jane Doe\",),\n (\"Mike Johnson\",)\n]\n\ndf = spark.createDataFrame(data, [\"managerName\"])\n\ndf.show()\n\ndf = df.withColumn(\"managerFirstName\", split(col(\"managerName\"), \" \").getItem(0)) \\\n .withColumn(\"managerLastName\", split(col(\"managerName\"), \" \").getItem(1))\n\ndf.show()","comments":[{"content":"in your example, your are using split( col(\"managerName\"), ... ) and not split(\"managerName\", ...) <- means that answer is D","poster":"cd6a625","comment_id":"1244318","upvote_count":"1","timestamp":"1720439220.0"}],"timestamp":"1699372920.0","comment_id":"1064972","poster":"newusername","upvote_count":"2"},{"poster":"zozoshanky","upvote_count":"1","comment_id":"967351","content":"Can be C as an answer too.","comments":[{"comments":[{"timestamp":"1714956060.0","content":"Yes, I agree with you, D is correct we have to pass a column as an object","comment_id":"1207107","upvote_count":"1","poster":"65bd33e"}],"poster":"cookiemonster42","timestamp":"1690833180.0","content":"But you have to pass a column as an object, not a string. you have to use col() expression. So D is the right one.","upvote_count":"4","comment_id":"968465"}],"timestamp":"1690738500.0"}],"answer_ET":"D","answer_images":[]},{"id":"MSbSoXGbmNKdSn0lW9Zc","answer_description":"","unix_timestamp":1690833480,"answer":"D","topic":"1","question_images":["https://img.examtopics.com/certified-associate-developer-for-apache-spark/image9.png"],"question_id":155,"answer_images":[],"exam_id":161,"answers_community":["D (100%)"],"url":"https://www.examtopics.com/discussions/databricks/view/116934-exam-certified-associate-developer-for-apache-spark-topic-1/","choices":{"C":"1. withColumn\n2. \"storeSlogan\"\n3. regexp_replace\n4. col(\"storeSlogan\")\n5. \"\\\"\"\n6. \"'\"","A":"1. withColumn\n2. \"storeSlogan\"\n3. regexp_extract\n4. col(\"storeSlogan\")\n5. \"\\\"\"\n6. \"'\"","E":"1. withColumn\n2. \"storeSlogan\"\n3. regexp_extract\n4. col(\"storeSlogan\")\n5. \"'\"\n6. \"\\\"\"","D":"1. withColumn\n2. \"storeSlogan\"\n3. regexp_replace\n4. col(\"storeSlogan\")\n5. \"'\"\n6. \"\\\"\"","B":"1. newColumn\n2. storeSlogan\n3. regexp_extract\n4. col(storeSlogan)\n5. \"\\\"\"\n6. \"'\""},"question_text":"The code block shown below should return a new DataFrame where single quotes in column storeSlogan have been replaced with double quotes. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.\n\nA sample of DataFrame storesDF is below:\n\n//IMG//\n\n\nCode block:\n\nstoresDF.__1__(__2__, __3__(__4__, __5__, __6__))","discussion":[{"comment_id":"1365254","upvote_count":"1","timestamp":"1741147800.0","content":"Selected Answer: D\nIn Python, the expression '\"' (double quote inside single quotes) or \\\" (escaped double quote) both represent a double quote (\").\n\nIn the correct answer D, the replacement part is written as:\n\nregexp_replace(col(\"storeSlogan\"), \"'\", \"\\\"\")\nThe first argument (\"storeSlogan\") refers to the column to be modified.\nThe second argument (\"'\") specifies the single quote character to be replaced.\nThe third argument (\"\\\"\" or '\"') represents the double quote character that will replace the single quote.\nSince Python uses backslashes (\\) to escape special characters, \\\" ensures that the double quote is correctly interpreted as part of the replacement string.\n\nSo, in short:\nâœ… ' \" ' and ' \\\" ' are equivalent in this case, both representing a double quote.","poster":"ARUNKUMARKRISHNASAMY"},{"upvote_count":"1","poster":"azure_bimonster","comment_id":"1145490","content":"Selected Answer: D\nTo me it would like this:\n\nstoresDF.withColumn(\"storeSlogan\", regexp_replace(col(\"storeSlogan\"), \" ' \", \"\\\"\"))","timestamp":"1723202160.0"},{"timestamp":"1706738280.0","upvote_count":"2","poster":"cookiemonster42","content":"Selected Answer: D\nD is correct, we need a replacement function with first argument \"'\"","comment_id":"968467"}],"answer_ET":"D","timestamp":"2023-07-31 21:58:00","isMC":true}],"exam":{"provider":"Databricks","isBeta":false,"lastUpdated":"12 Apr 2025","id":161,"isImplemented":true,"numberOfQuestions":185,"name":"Certified Associate Developer for Apache Spark","isMCOnly":true},"currentPage":31},"__N_SSP":true}