{"pageProps":{"questions":[{"id":"qlOV5Egbf5GobqM3iS2Q","question_images":[],"question_text":"The data engineering team has been tasked with configuring connections to an external database that does not have a supported native connector with Databricks. The external database already has data security configured by group membership. These groups map directly to user groups already created in Databricks that represent various teams within the company.\n\nA new login credential has been created for each group in the external database. The Databricks Utilities Secrets module will be used to make these credentials available to Databricks users.\n\nAssuming that all the credentials are configured correctly on the external database and group membership is properly configured on Databricks, which statement describes how teams can be granted the minimum necessary access to using these credentials?","answers_community":["C (100%)"],"answer":"C","isMC":true,"url":"https://www.examtopics.com/discussions/databricks/view/141560-exam-certified-data-engineer-professional-topic-1-question/","answer_images":[],"unix_timestamp":1717006140,"answer_description":"","topic":"1","choices":{"C":"\"Read\" permissions should be set on a secret scope containing only those credentials that will be used by a given team.","A":"No additional configuration is necessary as long as all users are configured as administrators in the workspace where secrets have been added.","D":"\"Manage\" permissions should be set on a secret scope containing only those credentials that will be used by a given team.","B":"\"Read\" permissions should be set on a secret key mapped to those credentials that will be used by a given team."},"exam_id":163,"discussion":[{"poster":"hpkr","upvote_count":"3","timestamp":"1718211240.0","comment_id":"1229345","content":"Selected Answer: C\nC is correct. Read permission on secret scope should work here."},{"timestamp":"1717277220.0","content":"Selected Answer: C\nCorrect Answer: C\nThis option is the best practice for managing access to sensitive data. By creating a secret scope dedicated to each team and setting \"Read\" permissions on the scope, you ensure that only the intended team members can access their respective credentials. This method aligns with security best practices by tightly controlling access based on group membership and reducing the risk of unauthorized access.","comment_id":"1222914","upvote_count":"3","poster":"Freyr"},{"poster":"MDWPartners","timestamp":"1717006140.0","content":"Selected Answer: C\nSeems C","comment_id":"1221175","upvote_count":"2"}],"question_id":76,"answer_ET":"C","timestamp":"2024-05-29 20:09:00"},{"id":"fQWGrzJTwOcpJ8q6tSlP","isMC":true,"answer":"C","exam_id":163,"answers_community":["C (100%)"],"choices":{"B":"It is retained for 30 days, during which time you can deliver job run logs to DBFS or S3","C":"It is retained for 60 days, during which you can export notebook run results to HTML","D":"It is retained for 60 days, after which logs are archived","A":"It is retained until you export or delete job run logs"},"answer_images":[],"question_images":[],"answer_description":"","question_text":"What is the retention of job run history?","topic":"1","url":"https://www.examtopics.com/discussions/databricks/view/141707-exam-certified-data-engineer-professional-topic-1-question/","answer_ET":"C","unix_timestamp":1717205580,"discussion":[{"content":"Selected Answer: C\nFor most Databricks workspaces, the job run history is retained for 60 days.","upvote_count":"3","comment_id":"1230652","timestamp":"1718397360.0","poster":"Elotrovert"},{"timestamp":"1718211300.0","poster":"hpkr","content":"Selected Answer: C\nC is correct","upvote_count":"2","comment_id":"1229346"}],"timestamp":"2024-06-01 03:33:00","question_id":77},{"id":"ZGMzTysKhRCXh2U2tBZ0","answers_community":["C (100%)"],"topic":"1","exam_id":163,"unix_timestamp":1717205400,"url":"https://www.examtopics.com/discussions/databricks/view/141706-exam-certified-data-engineer-professional-topic-1-question/","isMC":true,"timestamp":"2024-06-01 03:30:00","answer_images":[],"question_id":78,"answer_ET":"C","choices":{"D":"Because the REST API was used for job creation and triggering runs, user identity will not be captured in the audit logs.","A":"Because the REST API was used for job creation and triggering runs, a Service Principal will be automatically used to identify these events.","C":"Because these events are managed separately, User A will have their identity associated with the job creation events and User B will have their identity associated with the job run events.","B":"Because User A created the jobs, their identity will be associated with both the job creation events and the job run events."},"question_text":"A data engineer, User A, has promoted a new pipeline to production by using the REST API to programmatically create several jobs. A DevOps engineer, User B, has configured an external orchestration tool to trigger job runs through the REST API. Both users authorized the REST API calls using their personal access tokens.\n\nWhich statement describes the contents of the workspace audit logs concerning these events?","answer_description":"","discussion":[{"comment_id":"1222416","upvote_count":"5","content":"C, because the users has their own personal access tokens.","timestamp":"1717205400.0","poster":"imatheushenrique"},{"content":"Selected Answer: C\nThere is some ambiguity in this question, as has not been explained how this orchestration tool works.\n\nIf the tool just call the job (run-now), this event is tracking the user who 'triggers an on-demand job run'.\n\nBut we have other options, like modify/edit a job configuration, and set the trigger. If this situation happens, and the 'run as' is not populated, when the job is triggered automatically, there is not tracked who run the job. Then, the job run is associated to the owner of the job.\n\nI'm assuming the the orchestration tool is doing a run-now, so option C should be the right option.\n\nhttps://docs.databricks.com/api/azure/workspace/jobs/getrun\nhttps://docs.databricks.com/en/admin/account-settings/audit-logs.html#jobs","timestamp":"1734647400.0","comment_id":"1329185","comments":[{"comment_id":"1399201","timestamp":"1742121900.0","upvote_count":"1","poster":"73109a1","content":"PATs for production not a good idea afaik."}],"poster":"UrcoIbz","upvote_count":"1"},{"poster":"Jugiboss","upvote_count":"1","timestamp":"1729841040.0","comment_id":"1302797","content":"Selected Answer: C\nC is correct, same question earlier."},{"poster":"m79590530","content":"Selected Answer: C\nThere is creator_user_name property of a job as well as runs_as property and both can contain different identities assigned.","timestamp":"1729445460.0","comment_id":"1300611","upvote_count":"1"}],"question_images":[],"answer":"C"},{"id":"qKh9sW3dFmBGiZk1bIpp","question_images":[],"topic":"1","answer_description":"","question_id":79,"answers_community":["A (80%)","E (20%)"],"answer":"A","isMC":true,"unix_timestamp":1693388400,"discussion":[{"comment_id":"999084","timestamp":"1693891800.0","comments":[{"content":"Your source doesn't support your answer. It doesn't mention anything about autotuning to increase the speed of merges","timestamp":"1737759600.0","upvote_count":"1","comment_id":"1346256","poster":"meatpoof"}],"content":"Selected Answer: A\nhttps://docs.databricks.com/en/delta/tune-file-size.html#autotune-table 'Autotune file size based on workload'","upvote_count":"13","poster":"cotardo2077"},{"timestamp":"1743933960.0","poster":"kishanu","comment_id":"1558229","content":"Selected Answer: E\nDatabricks Auto Optimize and Auto Compaction features are designed to optimize file sizes dynamically for better performance and efficiency in Delta Lake. These features do not use a fixed target file size like 1 GB, but instead autotune file sizes based on partition-level characteristics.\n\nIn this case:\n\nEach partition has at least 1 GB of data, and the overall table is large (10+ TB), but...\nYou see many small files <64 MB, which seems suboptimal at first.\nHowever, Databricks may intentionally use smaller file sizes within partitions when:\n\nThe data change rate is high (as in a streaming CDC feed).\nSmaller file sizes help with faster read times, reduced shuffle, and quicker MERGE operations during structured streaming.\nThe amount of new data added per batch or microbatch is small, leading to many smaller files, especially when auto compaction determines this improves job performance at runtime.\nThis makes option E the most accurate description of what's happening.","upvote_count":"1"},{"content":"Selected Answer: A\nAn always-on Structured Streaming job that applies updates from a Change Data Capture (CDC) feed uses frequent MERGE operations to apply changes (inserts, updates, deletes) to the Delta table.\n\nBecause these MERGE operations are constant and high-frequency, Databricks may autotune to a smaller target file size to reduce the duration and overhead of each merge. This behaviour is described explicitly in the documentation.\n\nSo, with this in view, the correct answer is A","poster":"AlHerd","upvote_count":"1","timestamp":"1743072300.0","comment_id":"1410846"},{"poster":"EZZALDIN","comment_id":"1410279","content":"Selected Answer: E\nThe primary goal of Auto Optimize and Auto Compaction in a streaming job isn’t specifically to reduce MERGE duration. Instead, these features adjust file sizes based on the incremental volume of data being ingested in each micro‐batch within a partition. Even though each partition contains around 1 GB of data (from the original OPTIMIZE), the streaming job writes small batches that are compacted into smaller files (often under 64 MB) because that’s the amount of new data per batch.\n\nSo, Option E is more accurate: Databricks auto-tunes the target file size based on the amount of data in each partition (from each micro-batch), not specifically to speed up MERGE operations.","timestamp":"1742975700.0","upvote_count":"1"},{"content":"Selected Answer: E\nOption E is more accurate because Delta Lake’s Auto Optimize and Auto Compaction are designed to adjust file sizes based on the streaming data partitioning, which inherently leads to smaller files over time. The system auto-tunes file sizes as new, incremental data is ingested and partitioned.\nOption A is plausible, but optimizing file sizes for MERGE operations is not the core focus of Auto Optimize in this case. The system’s auto-tuning mechanism is more about managing file sizes based on the streaming data's partition size and maintaining efficient reads/writes, rather than directly optimizing for MERGE performance.","comment_id":"1362384","upvote_count":"3","timestamp":"1740623340.0","poster":"Tedet"},{"poster":"Tedet","upvote_count":"1","timestamp":"1740622620.0","content":"Selected Answer: A\nOptions Behavior\nauto (recommended) Tunes target file size while respecting other autotuning functionality. Requires Databricks Runtime 10.4 LTS or above.\nlegacy Alias for true. Requires Databricks Runtime 10.4 LTS or above.\ntrue Use 128 MB as the target file size. No dynamic sizing.\nfalse Turns off auto compaction. Can be set at the session level to override auto compaction for all Delta tables modified in the workload.","comment_id":"1362381"},{"poster":"rollno1","upvote_count":"1","content":"Selected Answer: E\nMERGE operations are not the main update mechanism in this scenario—it’s an incremental stream update, not batch MERGE. Larger partitions often result in smaller file sizes because:\nFrequent incremental writes cause small batch updates.\nCompaction happens at the partition level, not globally.","comment_id":"1357039","timestamp":"1739652660.0"},{"timestamp":"1722917820.0","upvote_count":"4","poster":"Melik3","comment_id":"1261408","content":"Selected Answer: A\nIt is important here to understand the difference between the partition size and the data files. the partition size is 1GB which is caused by OPTIMIZE and also expected. In each partition are data files. Databricks did an attuning to these datafile and resized them to a small size to be able to do MERGE statements efficiently that's why A is the correct answer"},{"content":"One of the purposes of a optimize execution is the gain in merge oprations, so:\nA. Databricks has autotuned to a smaller target file size to reduce duration of MERGE operations","timestamp":"1717211340.0","poster":"imatheushenrique","upvote_count":"1","comment_id":"1222463"},{"timestamp":"1707557400.0","poster":"RiktRikt007","content":"how A is correct ? While Databricks does have autotuning capabilities, it primarily considers the table size. In this case, the table is over 10 TB, which would typically lead to a target file size of 1 GB, not under 64 MB.","comment_id":"1146010","upvote_count":"2"},{"upvote_count":"2","poster":"PrashantTiwari","comment_id":"1145242","timestamp":"1707454380.0","content":"The target file size is based on the current size of the Delta table. For tables smaller than 2.56 TB, the autotuned target file size is 256 MB. For tables with a size between 2.56 TB and 10 TB, the target size will grow linearly from 256 MB to 1 GB. For tables larger than 10 TB, the target file size is 1 GB.\nCorrect answer is A"},{"upvote_count":"1","content":"correct ans is A","timestamp":"1705869720.0","comment_id":"1128123","poster":"AziLa"},{"poster":"Jay_98_11","content":"Selected Answer: A\nA is correct","upvote_count":"2","timestamp":"1705169940.0","comment_id":"1121944"},{"poster":"kz_data","upvote_count":"1","content":"Selected Answer: A\ncorrect answer is A","comment_id":"1118598","timestamp":"1704894360.0"},{"content":"Selected Answer: A\nAuto Optimize reduces file size less than 128MB to facilitate quick merge","upvote_count":"1","poster":"BIKRAM063","comment_id":"1060812","timestamp":"1698951360.0"},{"poster":"sen411","content":"E is the right answer, because the question is why there are small files","comment_id":"1049739","upvote_count":"1","timestamp":"1697913000.0"},{"poster":"sturcu","timestamp":"1697009760.0","upvote_count":"1","content":"Selected Answer: A\nCorrect","comment_id":"1040308"},{"timestamp":"1694282820.0","poster":"azurearch","comment_id":"1003404","content":"A is correct answer","upvote_count":"1"},{"comment_id":"993911","comments":[{"comment_id":"1013272","content":"option A is correct answer as , option E is the likely explanation for the smaller file sizes","upvote_count":"1","timestamp":"1695314160.0","poster":"Eertyy"}],"poster":"Eertyy","timestamp":"1693388400.0","content":"E is right answer","upvote_count":"4"}],"exam_id":163,"answer_images":[],"choices":{"D":"Databricks has autotuned to a smaller target file size based on the overall size of data in the table","A":"Databricks has autotuned to a smaller target file size to reduce duration of MERGE operations","E":"Databricks has autotuned to a smaller target file size based on the amount of data in each partition","C":"Bloom filter indices calculated on the table are preventing file compaction","B":"Z-order indices calculated on the table are preventing file compaction"},"timestamp":"2023-08-30 11:40:00","url":"https://www.examtopics.com/discussions/databricks/view/119377-exam-certified-data-engineer-professional-topic-1-question/","question_text":"A production workload incrementally applies updates from an external Change Data Capture feed to a Delta Lake table as an always-on Structured Stream job. When data was initially migrated for this table, OPTIMIZE was executed and most data files were resized to 1 GB. Auto Optimize and Auto Compaction were both turned on for the streaming production job. Recent review of data files shows that most data files are under 64 MB, although each partition in the table contains at least 1 GB of data and the total table size is over 10 TB.\nWhich of the following likely explains these smaller file sizes?","answer_ET":"A"},{"id":"4S9uSaSZHs3IdoxyIszC","isMC":true,"answer_images":[],"question_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/141705-exam-certified-data-engineer-professional-topic-1-question/","unix_timestamp":1717205280,"answers_community":["D (100%)"],"topic":"1","question_id":80,"discussion":[{"comment_id":"1300612","upvote_count":"1","timestamp":"1729445580.0","poster":"m79590530","content":"Selected Answer: D\nCluster lifecycle events are visible in the Cluster Event Log"},{"timestamp":"1717205280.0","content":"Its possible to see the metricks of compute with Ganglia, but the question is about a timeline so D, Cluster Event Log seems correct.","poster":"imatheushenrique","comment_id":"1222413","upvote_count":"3"}],"answer_ET":"D","answer":"D","choices":{"A":"Workspace audit logs","D":"Cluster Event Log","B":"Driver's log file","C":"Ganglia"},"timestamp":"2024-06-01 03:28:00","question_text":"A distributed team of data analysts share computing resources on an interactive cluster with autoscaling configured. In order to better manage costs and query throughput, the workspace administrator is hoping to evaluate whether cluster upscaling is caused by many concurrent users or resource-intensive queries.\n\nIn which location can one review the timeline for cluster resizing events?","exam_id":163,"answer_description":""}],"exam":{"isBeta":false,"lastUpdated":"12 Apr 2025","isImplemented":true,"id":163,"provider":"Databricks","isMCOnly":true,"numberOfQuestions":200,"name":"Certified Data Engineer Professional"},"currentPage":16},"__N_SSP":true}