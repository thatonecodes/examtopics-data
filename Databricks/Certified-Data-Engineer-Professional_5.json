{"pageProps":{"questions":[{"id":"GpmOmrRHzmL8f1JMFaQQ","choices":{"C":"User A’s email address will always appear in this field, as they still own the underlying notebooks.","D":"Once User C takes \"Owner\" privileges, their email address will appear in this field; prior to this, User B’s email address will appear in this field.","B":"User B’s email address will always appear in this field, as their credentials are always used to trigger the run.","A":"Once User C takes \"Owner\" privileges, their email address will appear in this field; prior to this, User A’s email address will appear in this field.","E":"User C will only ever appear in this field if they manually trigger the job, otherwise it will indicate User B."},"timestamp":"2024-06-05 19:41:00","answer_description":"","answers_community":["B (37%)","C (30%)","A (26%)","7%"],"answer":"B","url":"https://www.examtopics.com/discussions/databricks/view/141951-exam-certified-data-engineer-professional-topic-1-question/","answer_ET":"B","answer_images":[],"isMC":true,"unix_timestamp":1717609260,"question_id":21,"discussion":[{"upvote_count":"1","content":"Selected Answer: C\nIt should be User A.\nGo to any existing workflow which has a few runs and see the JSON. It will have the creator_user_name as the creator of the workflow. For the user who has triggered the job, the field name is \"run_as_user_name\"","timestamp":"1744086420.0","poster":"kishanu","comment_id":"1558795"},{"timestamp":"1744086360.0","content":"Selected Answer: C\nIt should be User A.\nGo to any existing workflow which has a few runs and see the JSON. It will have the creator_user_name as the creator of the workflow. For the user who has triggered the job, the field name is \"run_as_user_name\"","poster":"kishanu","comment_id":"1558794","upvote_count":"1"},{"comment_id":"1335511","content":"Selected Answer: B\nB - the person that triggered the job. Since we are using User B personal access token, it is going to say User B's email address.","upvote_count":"1","timestamp":"1735815240.0","poster":"arekm"},{"timestamp":"1734202380.0","content":"Selected Answer: A\nUser A was OWNER, B just run job, doesn't became OWNER, then User C became OWNER - so case \"A\"","comments":[{"timestamp":"1734213000.0","poster":"temple1305","content":"Sorry - really B \nThe creator_user_name field in the Databricks REST API response for job runs reflects the user who triggered the job, not the owner of the job or the creator of the underlying notebooks. Since User B's external orchestration tool triggers the job runs using their personal access token, the field will always indicate User B's email address as the \"creator\" of those job runs.","comment_id":"1326618","upvote_count":"6"}],"poster":"temple1305","comment_id":"1326592","upvote_count":"2"},{"content":"Selected Answer: E\ncreator_user_name reflects the user who triggered the job run, not the job owner or the notebook creator.\nFor programmatically triggered runs, the field will reflect the user whose personal access token was used (User B in this case).\nIf User C manually triggers a job, the field will show User C’s email address for that specific run.","poster":"Thameur01","comment_id":"1322705","comments":[{"comment_id":"1335512","timestamp":"1735815360.0","poster":"arekm","content":"The statement is correct, however the question does not mention manually triggering the job. Only the monitoring solution that retrieves the data from the REST API. Given the circumstances, it is going to be User B's email address.","upvote_count":"1"}],"timestamp":"1733482740.0","upvote_count":"2"},{"content":"Selected Answer: B\nIt's B.","comment_id":"1310027","timestamp":"1731328200.0","poster":"cf56faf","upvote_count":"2"},{"timestamp":"1731084060.0","content":"ownership of job an be assigned to user not for a group. Option A is correct.","poster":"smashit","upvote_count":"1","comment_id":"1308865"},{"comment_id":"1303883","timestamp":"1730104560.0","content":"Selected Answer: A\nSeems correct.","upvote_count":"2","poster":"Jugiboss"},{"comment_id":"1303649","content":"Selected Answer: C\nThe creator_user_name field in the run information returned by the REST API reflects the user who originally created the job. Since User A created the jobs using their personal access token, their email address will always appear in this field, regardless of who triggers the job runs or who takes ownership of the job later.","poster":"Ananth4Sap","upvote_count":"2","timestamp":"1730044620.0"},{"poster":"pk07","timestamp":"1727201220.0","upvote_count":"2","content":"Selected Answer: C\nC based on previous comment","comment_id":"1288733"},{"upvote_count":"1","content":"Selected Answer: C\nJob creator can't be changed. Owner can be changed. So creator_user_name field always return who created the job: User A. But UserA no longer owns the job. So Answer C is partially correct.","comment_id":"1287015","timestamp":"1726857180.0","poster":"shaojunni"},{"upvote_count":"1","timestamp":"1726857120.0","content":"Selected Answer: C\nJob creator can't be changed. Owner can be changed. So creator_user_name field always return owner: User A. But UserA no longer owns the job. So Answer C is partially correct.","comment_id":"1287013","poster":"shaojunni"},{"upvote_count":"3","content":"Selected Answer: A\nWhen you create a job your role is IS OWNER and RUN AS. So when you trigger a job, it will run as the RUN AS entity. And it should be user A if someone dosen't have changed it","timestamp":"1723987680.0","poster":"jlocke","comment_id":"1268069"},{"content":"Selected Answer: B\nAnswer B","comment_id":"1264142","upvote_count":"2","poster":"946a1af","timestamp":"1723380060.0"},{"comment_id":"1244663","upvote_count":"2","content":"Selected Answer: B\nShould be the DevOps email address","timestamp":"1720496880.0","poster":"c00ccb7"},{"content":"Selected Answer: B\nthe creator_user_name field reflects the user who triggered the job run","timestamp":"1720449000.0","poster":"c00ccb7","upvote_count":"3","comment_id":"1244387"},{"comment_id":"1231438","timestamp":"1718554080.0","upvote_count":"2","poster":"fcfb11c","content":"Answer: E"},{"upvote_count":"1","timestamp":"1717609260.0","poster":"Deb9753","comment_id":"1224897","content":"Answer: C"}],"exam_id":163,"question_images":[],"topic":"1","question_text":"A data engineer, User A, has promoted a pipeline to production by using the REST API to programmatically create several jobs. A DevOps engineer, User B, has configured an external orchestration tool to trigger job runs through the REST API. Both users authorized the REST API calls using their personal access tokens.\n\nA workspace admin, User C, inherits responsibility for managing this pipeline. User C uses the Databricks Jobs UI to take \"Owner\" privileges of each job. Jobs continue to be triggered using the credentials and tooling configured by User B.\n\nAn application has been configured to collect and parse run information returned by the REST API. Which statement describes the value returned in the creator_user_name field?"},{"id":"hOe3tum4fOyACZdtA2SY","url":"https://www.examtopics.com/discussions/databricks/view/146882-exam-certified-data-engineer-professional-topic-1-question/","topic":"1","question_id":22,"exam_id":163,"timestamp":"2024-09-04 04:29:00","question_text":"A member of the data engineering team has submitted a short notebook that they wish to schedule as part of a larger data pipeline. Assume that the commands provided below produce the logically correct results when run as presented.\n\n//IMG//\n\n\nWhich command should be removed from the notebook before scheduling it as a job?","unix_timestamp":1725416940,"question_images":["https://img.examtopics.com/certified-data-engineer-professional/image51.png"],"answers_community":["D (100%)"],"answer_images":[],"choices":{"B":"Cmd 3","C":"Cmd 4","A":"Cmd 2","D":"Cmd 5"},"isMC":true,"answer_description":"","answer":"D","discussion":[{"upvote_count":"1","timestamp":"1735815480.0","content":"Selected Answer: D\nD - Cmd 5 which is display(finalDF) is costly, produces a nicely formatted HTML table which is unnecessary during the production run. On top that there is a limit on how much output the notebook can generate. This causes me some problems in the past for legacy code.","comment_id":"1335514","poster":"arekm"},{"timestamp":"1731084240.0","upvote_count":"1","poster":"smashit","comment_id":"1308866","content":"There is a limitation when you use display command inside a job. if the output exceeds 20MB it will throw an error. may be that was the reason."},{"upvote_count":"2","content":"Selected Answer: D\nThe display function is built specifically for the notebook environment and will not work for Spark. Should you want to print contacts of DF in Spark then replace it with df.show()","timestamp":"1725416940.0","poster":"csrazdan","comment_id":"1277913"}],"answer_ET":"D"},{"id":"p9avsuaKbjCHlLgkLFte","question_images":[],"isMC":true,"question_text":"Which statement regarding Spark configuration on the Databricks platform is true?","unix_timestamp":1721458680,"exam_id":163,"answers_community":["D (100%)"],"answer":"D","question_id":23,"discussion":[{"upvote_count":"1","timestamp":"1730026620.0","comment_id":"1303555","content":"Selected Answer: D\nthis question repeats afaik","poster":"nedlo"},{"poster":"Melik3","content":"Selected Answer: D\nthis looks correct","timestamp":"1725895260.0","comment_id":"1281031","upvote_count":"2"},{"content":"Selected Answer: D\nShould be D","upvote_count":"3","poster":"vexor3","timestamp":"1721458680.0","comment_id":"1251581"}],"choices":{"C":"When the same Spark configuration property is set for an interactive cluster and a notebook attached to that cluster, the notebook setting will always be ignored.","D":"Spark configuration properties set for an interactive cluster with the Clusters UI will impact all notebooks attached to that cluster.","A":"The Databricks REST API can be used to modify the Spark configuration properties for an interactive cluster without interrupting jobs currently running on the cluster.","B":"Spark configurations set within a notebook will affect all SparkSessions attached to the same interactive cluster."},"timestamp":"2024-07-20 08:58:00","answer_ET":"D","topic":"1","answer_description":"","url":"https://www.examtopics.com/discussions/databricks/view/144252-exam-certified-data-engineer-professional-topic-1-question/","answer_images":[]},{"id":"a98naUArnjZOxNxythsl","unix_timestamp":1694933100,"timestamp":"2023-09-17 08:45:00","question_images":["https://img.examtopics.com/certified-data-engineer-professional/image9.png"],"topic":"1","question_text":"A junior data engineer has configured a workload that posts the following JSON to the Databricks REST API endpoint 2.0/jobs/create.\n//IMG//\n\nAssuming that all configurations and referenced resources are available, which statement describes the result of executing this workload three times?","discussion":[{"content":"Selected Answer: C\nYou can totally create 3 jobs with the same name using the UI. REST API is no different. Since no schedule information is in the json, it will not run.","comment_id":"1334747","poster":"arekm","timestamp":"1735643160.0","upvote_count":"4"},{"upvote_count":"1","comment_id":"1322348","content":"Selected Answer: C\nAnswer C. 3 new jobs with the same name will be created with each API call, but they won't be executed unless further configuration is made for scheduling or triggering.","timestamp":"1733400540.0","poster":"AlejandroU"},{"poster":"akashdesarda","upvote_count":"1","content":"Selected Answer: C\nDatabricks will keep on creating new jobs if you keep running create rept api. Each will have the same name but a different ID. Also no trigger/schedule is mentioned so they wont run.","timestamp":"1727536920.0","comment_id":"1290686"},{"content":"C. Three new jobs named \"Ingest new data\" will be defined in the workspace, but no jobs will be executed.","timestamp":"1717547580.0","comment_id":"1224443","poster":"imatheushenrique","upvote_count":"1"},{"upvote_count":"3","timestamp":"1716132840.0","comment_id":"1213860","content":"Selected Answer: C\nLearnt new thing : DBX can have duplicate job names (Job ID will be different). So three jobs will be created with three job ids but it will not run as no schedule is mentioned.","poster":"coercion"},{"comment_id":"1167343","poster":"franciscodm","content":"C for sure","timestamp":"1709742300.0","upvote_count":"1"},{"comment_id":"1130892","timestamp":"1706115840.0","poster":"spaceexplorer","content":"Selected Answer: C\nCorrect answer is C","upvote_count":"1"},{"poster":"Jay_98_11","upvote_count":"1","content":"Selected Answer: C\nC is correct","comment_id":"1121659","timestamp":"1705150140.0"},{"content":"Selected Answer: C\nCorrect answer is C","upvote_count":"1","poster":"kz_data","timestamp":"1704891900.0","comment_id":"1118561"},{"comment_id":"1040267","timestamp":"1697007180.0","content":"Selected Answer: C\ndatabricks jobs create will create a new job with the same name each time it is run.\nIn order to overwrite the extsting job you need to run databricks jobs reset","poster":"sturcu","upvote_count":"3"},{"comment_id":"1016751","upvote_count":"3","poster":"bob_","timestamp":"1695641820.0","content":"Answer is correct. The 3 API calls create 3 jobs with the same name but different job ids. There is no schedule defined so will not execute."},{"comments":[{"poster":"Eertyy","upvote_count":"2","timestamp":"1695308640.0","comment_id":"1013207","content":"to add more: when you execute this workload three times, it will define three new jobs in the workspace, each with the name \"Ingest new data.\" These jobs can be scheduled to run daily or at a specified frequency, depending on how they are configured."},{"comment_id":"1038958","timestamp":"1696879860.0","content":"Ok. So Tell me the schedule these Jobs will run?\nDont know? Why? Maybe because it is not specified, or even configured. So the answer is correct. Create 3 Jobs but none will be executed.","upvote_count":"2","poster":"Starvosxant"}],"upvote_count":"2","poster":"Eertyy","comment_id":"1013199","timestamp":"1695308340.0","content":"correct answer is A, because an api can create can create same job with same name if executed thrice"},{"upvote_count":"2","content":"therefore answer: D","comment_id":"1009629","poster":"mwyopme","timestamp":"1694933100.0"},{"poster":"vsydoriak99","content":"Because the create command was run 3 times. Databricks can have several jobs with the same name","comment_id":"1009958","timestamp":"1694969100.0","upvote_count":"3"}],"answer_images":[],"answer_description":"","question_id":24,"answer":"C","exam_id":163,"answers_community":["C (100%)"],"choices":{"B":"The logic defined in the referenced notebook will be executed three times on new clusters with the configurations of the provided cluster ID.","C":"Three new jobs named \"Ingest new data\" will be defined in the workspace, but no jobs will be executed.","A":"Three new jobs named \"Ingest new data\" will be defined in the workspace, and they will each run once daily.","E":"The logic defined in the referenced notebook will be executed three times on the referenced existing all purpose cluster.","D":"One new job named \"Ingest new data\" will be defined in the workspace, but it will not be executed."},"isMC":true,"answer_ET":"C","url":"https://www.examtopics.com/discussions/databricks/view/120855-exam-certified-data-engineer-professional-topic-1-question/"},{"id":"UdmfC9UX1k3VQehKAkw8","answer":"B","answer_description":"","question_images":[],"answer_images":[],"question_text":"The business reporting team requires that data for their dashboards be updated every hour. The total processing time for the pipeline that extracts, transforms, and loads the data for their pipeline runs in 10 minutes.\n\nAssuming normal operating conditions, which configuration will meet their service-level agreement requirements with the lowest cost?","question_id":25,"choices":{"D":"Schedule a job to execute the pipeline once an hour on a dedicated interactive cluster","C":"Schedule a Structured Streaming job with a trigger interval of 60 minutes","B":"Schedule a job to execute the pipeline once an hour on a new job cluster","A":"Configure a job that executes every time new data lands in a given directory"},"exam_id":163,"url":"https://www.examtopics.com/discussions/databricks/view/149442-exam-certified-data-engineer-professional-topic-1-question/","unix_timestamp":1728959940,"discussion":[{"content":"Selected Answer: B\nKey words: updated every hour, pipeline runs in 10 minutes - Simple job cluster should do the job.","comment_id":"1326398","poster":"Sriramiyer92","timestamp":"1734167460.0","upvote_count":"1"},{"content":"Selected Answer: B\nThe correct answer is B. Schedule a job to execute the pipeline once an hour on a new job cluster.\n\nExplanation:\nIn this scenario, the business reporting team needs the data to be updated every hour, and the processing time for the pipeline takes 10 minutes. To meet this requirement with the lowest cost, the best option is to schedule the job to run once per hour using a new job cluster.\n\nA job cluster is created specifically for the duration of the job, and once the job finishes, the cluster is terminated. This is cost-efficient because resources are only consumed while the job is running, and the cluster does not stay active when it is not needed.","upvote_count":"1","poster":"Colje","timestamp":"1728959940.0","comment_id":"1297917","comments":[{"comment_id":"1335521","content":"I agree with B. However, C would also work from the cost perspective provided you set the timeout to a low value. That would still be more resource consumption than B, which disposes of the cluster as soon as the job is done. Moreover, C does not mention \"job cluster\" which is kind of an implied best practice. However, always better to be explicit.","poster":"arekm","timestamp":"1735815840.0","upvote_count":"1"}]}],"answer_ET":"B","isMC":true,"topic":"1","timestamp":"2024-10-15 04:39:00","answers_community":["B (100%)"]}],"exam":{"id":163,"isBeta":false,"lastUpdated":"12 Apr 2025","numberOfQuestions":200,"name":"Certified Data Engineer Professional","isMCOnly":true,"isImplemented":true,"provider":"Databricks"},"currentPage":5},"__N_SSP":true}