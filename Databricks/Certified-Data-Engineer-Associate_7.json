{"pageProps":{"questions":[{"id":"MI5jKvqslZrq06vTK1eX","unix_timestamp":1716408180,"question_text":"A dataset has been defined using Delta Live Tables and includes an expectations clause:\n\nCONSTRAINT valid_timestamp EXPECT (timestamp > '2020-01-01') ON VIOLATION DROP ROW\n\nWhat is the expected behavior when a batch of data containing data that violates these constraints is processed?","answers_community":["C (83%)","A (17%)"],"topic":"1","timestamp":"2024-05-22 22:03:00","discussion":[{"upvote_count":"2","poster":"9d4d68a","content":"Repeated, Correct answer is C","comment_id":"1272972","timestamp":"1724696340.0"},{"poster":"vigaro","comment_id":"1236566","upvote_count":"2","content":"Selected Answer: C\nON VIOLATION DROP ROW","timestamp":"1719256740.0"},{"content":"Selected Answer: C\nit's C","poster":"31cadd7","upvote_count":"2","comment_id":"1230490","timestamp":"1718368560.0"},{"comment_id":"1220760","timestamp":"1716970260.0","content":"C. Records that violate the expectation are dropped from the target dataset and recorded as invalid in the event log.\n\nWhen a constraint defined using the EXPECT clause is violated, Delta Live Tables will drop the records that violate the expectation from the target dataset. Additionally, information about the dropped records and the reason for their exclusion will be recorded in the event log for audit and monitoring purposes. This ensures that only valid data meeting the specified constraints is included in the target dataset.","upvote_count":"3","poster":"d39c1db"},{"timestamp":"1716408300.0","comment_id":"1215994","upvote_count":"1","poster":"PreranaC","content":"Selected Answer: C\nC should be correct, A is for ON VIOLATION FAIL UPDATE"},{"poster":"PreranaC","timestamp":"1716408180.0","comment_id":"1215992","comments":[{"timestamp":"1716661380.0","comment_id":"1218504","poster":"MDWPartners","upvote_count":"2","content":"i don't agree, it shouldn't make the job to fail."}],"upvote_count":"1","content":"Selected Answer: A\nA should be correct"}],"choices":{"B":"Records that violate the expectation are added to the target dataset and flagged as invalid in a field added to the target dataset.","A":"Records that violate the expectation cause the job to fail.","C":"Records that violate the expectation are dropped from the target dataset and recorded as invalid in the event log.","D":"Records that violate the expectation are added to the target dataset and recorded as invalid in the event log."},"isMC":true,"answer_ET":"C","question_id":31,"exam_id":162,"answer_description":"","question_images":[],"answer_images":[],"answer":"C","url":"https://www.examtopics.com/discussions/databricks/view/141060-exam-certified-data-engineer-associate-topic-1-question-126/"},{"id":"hlo6no0YikmL1mMhHYQO","exam_id":162,"timestamp":"2024-08-26 20:16:00","answer_ET":"D","answers_community":["D (100%)"],"topic":"1","answer":"D","answer_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/146479-exam-certified-data-engineer-associate-topic-1-question-127/","question_text":"A data engineer has a Job with multiple tasks that runs nightly. Each of the tasks runs slowly because the clusters take a long time to start.\n\nWhich action can the data engineer perform to improve the start up time for the clusters used for the Job?","answer_description":"","isMC":true,"discussion":[{"comment_id":"1327323","poster":"MultiCloudIronMan","content":"Selected Answer: D\nThe correct answer is D. They can use clusters that are from a cluster pool. Using clusters from a cluster pool can significantly reduce the start-up time because the clusters are pre-configured and ready to be used, which eliminates the need to wait for new clusters to be created and started.","timestamp":"1734351240.0","upvote_count":"1"},{"content":"Selected Answer: D\npools are a set of idle, ready-to-use instances hence minimizing start-up times","timestamp":"1729248060.0","poster":"RandomForest","upvote_count":"1","comment_id":"1299624"},{"comment_id":"1272971","poster":"9d4d68a","upvote_count":"1","timestamp":"1724696160.0","content":"Repeated, Correct"}],"question_id":32,"question_images":[],"choices":{"A":"They can use endpoints available in Databricks SQL","C":"They can configure the clusters to autoscale for larger data sizes","D":"They can use clusters that are from a cluster pool","B":"They can use jobs clusters instead of all-purpose clusters"},"unix_timestamp":1724696160},{"id":"HmfSB6m4BqIbGg8ykrPX","answer_images":[],"discussion":[{"upvote_count":"1","content":"Selected Answer: C\nNo, a new notebook needs to be run prior to the original task meaning the original task depends on the new notebook, hence C.","timestamp":"1742835360.0","poster":"Billybob0604","comment_id":"1409737"},{"comment_id":"1282733","upvote_count":"1","timestamp":"1726157580.0","content":"Selected Answer: B\nB is correct. the new task needs to be the dependancy.","poster":"CommanderBigMac"},{"comment_id":"1272970","poster":"9d4d68a","upvote_count":"1","timestamp":"1724696160.0","content":"Correct Answer: B\n\nExplanation: To set up the new task to run a new notebook prior to the original task in a single-task Job, the data engineer can use the following approach: In the existing Job, create a new task that corresponds to the new notebook that needs to be run. Set up the new task with the appropriate configuration, specifying the notebook to be executed and any necessary parameters or dependencies. Once the new task is created, designate it as a dependency of the original task in the Job configuration. This ensures that the new task is executed before the original task."},{"timestamp":"1717941960.0","upvote_count":"1","comment_id":"1227362","content":"Selected Answer: B\nAnswer is B.\nNew task is prior than the original task.","poster":"hussamAlHunaiti"},{"poster":"PreranaC","content":"Selected Answer: B\nB is correct","comment_id":"1215997","timestamp":"1716408480.0","upvote_count":"1"},{"timestamp":"1716124080.0","poster":"nmosq","comment_id":"1213790","content":"B is correct, \"needs to run prior to the original task\"","upvote_count":"1"},{"content":"Selected Answer: B\nB is correct","comment_id":"1213719","poster":"BharaniRaj","upvote_count":"1","timestamp":"1716114660.0"},{"upvote_count":"1","comment_id":"1209951","content":"B is correct, as new task runs first","timestamp":"1715457780.0","poster":"Kunka"},{"poster":"Ivan_Petrov","upvote_count":"1","comment_id":"1209378","timestamp":"1715345760.0","content":"B is correct"}],"question_images":[],"question_text":"A data engineer has a single-task Job that runs each morning before they begin working. After identifying an upstream data issue, they need to set up another task to run a new notebook prior to the original task.\n\nWhich approach can the data engineer use to set up the new task?","exam_id":162,"answer_description":"","unix_timestamp":1715345760,"url":"https://www.examtopics.com/discussions/databricks/view/140290-exam-certified-data-engineer-associate-topic-1-question-128/","answers_community":["B (80%)","C (20%)"],"isMC":true,"timestamp":"2024-05-10 14:56:00","choices":{"A":"They can clone the existing task in the existing Job and update it to run the new notebook.","B":"They can create a new task in the existing Job and then add it as a dependency of the original task.","C":"They can create a new task in the existing Job and then add the original task as a dependency of the new task.","D":"They can create a new job from scratch and add both tasks to run concurrently."},"answer":"B","topic":"1","question_id":33,"answer_ET":"B"},{"id":"d23UJBhkUBBtyZFmFl3D","isMC":true,"question_text":"A single Job runs two notebooks as two separate tasks. A data engineer has noticed that one of the notebooks is running slowly in the Jobâ€™s current run. The data engineer asks a tech lead for help in identifying why this might be the case.\n\nWhich approach can the tech lead use to identify why the notebook is running slowly as part of the Job?","timestamp":"2024-08-26 20:10:00","topic":"1","answer":"C","answer_ET":"C","discussion":[{"content":"Selected Answer: C\nThe correct answer is C. They can navigate to the Runs tab in the Jobs UI and click on the active run to review the processing notebook. This approach allows the tech lead to directly access and review the notebook that is currently running, helping to identify any issues causing it to run slowly.","comment_id":"1327324","upvote_count":"1","timestamp":"1734351480.0","poster":"MultiCloudIronMan"},{"content":"Selected Answer: C\nQuestion states it is Running slowly, nothing is wrong with the job itself, so the Run needs to be checked.","comment_id":"1282734","poster":"CommanderBigMac","upvote_count":"1","timestamp":"1726157700.0"},{"upvote_count":"1","content":"Repeated, Correct","poster":"9d4d68a","comment_id":"1272964","timestamp":"1724695800.0"}],"question_images":[],"choices":{"B":"They can navigate to the Tasks tab in the Jobs UI and click on the active run to review the processing notebook.","D":"They can navigate to the Tasks tab in the Jobs UI to immediately review the processing notebook.","C":"They can navigate to the Runs tab in the Jobs UI and click on the active run to review the processing notebook.","A":"They can navigate to the Runs tab in the Jobs UI to immediately review the processing notebook."},"answers_community":["C (100%)"],"exam_id":162,"answer_images":[],"question_id":34,"answer_description":"","url":"https://www.examtopics.com/discussions/databricks/view/146477-exam-certified-data-engineer-associate-topic-1-question-129/","unix_timestamp":1724695800},{"id":"zU0SrUhh0MdpRvtWVmW6","exam_id":162,"answer":"C","timestamp":"2023-04-01 16:04:00","answer_description":"","question_images":[],"unix_timestamp":1680357840,"choices":{"E":"USE DATABASE customer360;","B":"DROP DATABASE customer360;","C":"DESCRIBE DATABASE customer360;","A":"DESCRIBE LOCATION customer360;","D":"ALTER DATABASE customer360 SET DBPROPERTIES ('location' = '/user'};"},"answer_ET":"C","answers_community":["C (100%)"],"answer_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/databricks/view/104750-exam-certified-data-engineer-associate-topic-1-question-13/","discussion":[{"upvote_count":"8","timestamp":"1693770120.0","poster":"vctrhugo","content":"Selected Answer: C\nC. DESCRIBE DATABASE customer360;\n\nTo retrieve the location of a database named \"customer360\" in a database management system like Hive or Databricks, you can use the DESCRIBE DATABASE command followed by the database name. This command will provide information about the database, including its location.","comment_id":"997916"},{"timestamp":"1723103640.0","content":"Selected Answer: C\nC. DESCRIBE DATABASE customer360;\nthis will show the location of the databaase.","comment_id":"1262398","poster":"80370eb","upvote_count":"1"},{"upvote_count":"1","timestamp":"1714230360.0","comment_id":"1203172","content":"Selected Answer: C\nC is correct","poster":"benni_ale"},{"comment_id":"1177193","poster":"Itmma","upvote_count":"1","content":"Selected Answer: C\nC is correct","timestamp":"1710841860.0"},{"timestamp":"1704322200.0","upvote_count":"1","poster":"SerGrey","content":"Selected Answer: C\nCorrect answer is C","comment_id":"1113193"},{"upvote_count":"1","timestamp":"1699361460.0","poster":"awofalus","comment_id":"1064793","content":"Selected Answer: C\nCorrect :C"},{"timestamp":"1695698040.0","poster":"KalavathiP","content":"Selected Answer: C\nC is correct","comment_id":"1017351","upvote_count":"1"},{"upvote_count":"1","timestamp":"1691723220.0","poster":"Akshay67364","content":"Option C","comment_id":"978232"},{"poster":"Gowthamr02","content":"Option C","upvote_count":"1","comment_id":"972704","timestamp":"1691213400.0"},{"upvote_count":"2","timestamp":"1682053560.0","content":"Option C -\nhttps://spark.apache.org/docs/3.0.0-preview/sql-ref-syntax-aux-describe-database.html","poster":"Varma_Saraswathula","comment_id":"876210"},{"upvote_count":"2","comment_id":"859666","content":"Selected Answer: C\noption c","timestamp":"1680509220.0","poster":"surrabhi_4"},{"content":"Selected Answer: C\nMuy facil","upvote_count":"2","comment_id":"858863","poster":"knivesz","timestamp":"1680441360.0"},{"poster":"XiltroX","comment_id":"857992","upvote_count":"3","content":"Selected Answer: C\nCorrect answer","timestamp":"1680357840.0"}],"isMC":true,"question_id":35,"question_text":"Which of the following commands will return the location of database customer360?"}],"exam":{"id":162,"provider":"Databricks","name":"Certified Data Engineer Associate","isMCOnly":true,"lastUpdated":"12 Apr 2025","isBeta":false,"isImplemented":true,"numberOfQuestions":169},"currentPage":7},"__N_SSP":true}