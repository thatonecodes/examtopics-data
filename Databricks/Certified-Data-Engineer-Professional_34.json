{"pageProps":{"questions":[{"id":"a4QZRzjWXsISJCJ1pN5z","answer_ET":"B","isMC":true,"answers_community":["B (77%)","D (18%)","5%"],"discussion":[{"timestamp":"1727238720.0","upvote_count":"6","comment_id":"1182254","content":"Selected Answer: B\ndatabricks fs cp dist/<â€¦>.whl dbfs:/some/place/appropriate","poster":"arik90"},{"comments":[{"upvote_count":"1","content":"The question is about upload - answer B.","poster":"arekm","timestamp":"1735803780.0","comment_id":"1335392"}],"upvote_count":"1","poster":"AlejandroU","timestamp":"1734501900.0","content":"Selected Answer: D\nAnswer D. The Databricks CLI libraries command is used to manage libraries, including installing custom Python wheels. Specifically, the install subcommand can be used to install a wheel. In contrast, Option B. fs: This command interacts with the Databricks File System (DBFS) to manage files, but it is primarily used for basic file operations (like cp, ls, rm), not specifically for uploading libraries.","comment_id":"1328294"},{"comment_id":"1160168","timestamp":"1724712060.0","poster":"Curious76","content":"Selected Answer: D\nHere's how you can use the libraries command to upload your wheel:\nBash\n\ndatabricks libraries upload --file <path_to_wheel_file> --name <library_name>","upvote_count":"1"},{"timestamp":"1723705380.0","poster":"ojudz08","upvote_count":"1","comment_id":"1150831","content":"Selected Answer: C\nthis is a bit tricky, question is asked to upload custom Python Wheel, you can use fs command, but since it'll be used in production job, job command might be needed to perform databricks jobs operations?\nhttps://docs.databricks.com/en/dev-tools/cli/commands.html"},{"comment_id":"1136011","poster":"Somesh512","content":"Selected Answer: B\nIts asking to upload to DBFS and not install on cluster","timestamp":"1722355260.0","upvote_count":"2"},{"comment_id":"1084732","timestamp":"1717092300.0","upvote_count":"3","poster":"petrv","content":"Selected Answer: B\nthe question is about copying the file not about installing."},{"timestamp":"1716735540.0","content":"Selected Answer: B\nAnswer B is corrent:\n\"... which Databricks CLI command can be used to upload a custom Python Wheel to object storage mounted with the DBFS ...\"\nThe question asks, how to upload the wheel. Not install it or configure it in a job. \nhttps://docs.databricks.com/en/archive/dev-tools/cli/dbfs-cli.html","poster":"Enduresoul","comment_id":"1080851","upvote_count":"4"},{"upvote_count":"2","content":"Selected Answer: B\nThe Databricks CLI fs command is used for interacting with the Databricks File System (DBFS). You can use it to put files into DBFS, which includes uploading custom Python Wheels to a directory in DBFS. The fs command has subcommands like cp that can be used to copy files from your local file system to DBFS, which is backed by an object storage mounted with dbutils.fs.mount().\n\ndatabricks fs cp my_package.whl dbfs:/mnt/my-mount-point/my_package.whl","comment_id":"1076509","timestamp":"1716306060.0","poster":"aragorn_brego"},{"upvote_count":"1","comment_id":"1054639","poster":"mouad_attaqi","content":"Selected Answer: D\nIt is done using the command: databricks libraries install","timestamp":"1714139160.0"},{"poster":"sturcu","upvote_count":"1","timestamp":"1713973860.0","content":"Selected Answer: D\nyou can add a library section to the jobs command, but you can install a wheel with the library command","comments":[{"timestamp":"1713973920.0","poster":"sturcu","comment_id":"1053025","upvote_count":"1","content":"/api/2.0/libraries/install"}],"comment_id":"1053023"}],"choices":{"D":"libraries","C":"jobs","A":"configure","B":"fs","E":"workspace"},"url":"https://www.examtopics.com/discussions/databricks/view/124574-exam-certified-data-engineer-professional-topic-1-question/","unix_timestamp":1698162660,"answer":"B","answer_images":[],"exam_id":163,"topic":"1","question_text":"Assuming that the Databricks CLI has been installed and configured correctly, which Databricks CLI command can be used to upload a custom Python Wheel to object storage mounted with the DBFS for use with a production job?","question_id":166,"answer_description":"","timestamp":"2023-10-24 17:51:00","question_images":[]},{"id":"BiVI8jUjHORB8Pz7IHFU","unix_timestamp":1703860560,"topic":"1","question_id":167,"question_images":["https://img.examtopics.com/certified-data-engineer-professional/image32.png","https://img.examtopics.com/certified-data-engineer-professional/image33.png"],"answer_images":[],"answer":"A","discussion":[{"upvote_count":"13","timestamp":"1719664560.0","comments":[{"comment_id":"1130315","poster":"Def21","timestamp":"1721797260.0","content":"E - a view, could be an option but it would require computation every time used.","upvote_count":"1"}],"comment_id":"1108784","content":"Selected Answer: A\nlooks like A to me, as long as they only need the data for the aggregates based on the previous day only","poster":"dmov"},{"comment_id":"1332579","upvote_count":"1","content":"Selected Answer: D\nDelta cache avoids having to read data from the table every time it's queried during the day.","timestamp":"1735325280.0","poster":"srinivasa"}],"choices":{"E":"Define a view against the products_per_order table and define the dashboard against this view.","B":"Use Structured Streaming to configure a live dashboard against the products_per_order table within a Databricks notebook.","D":"Use the Delta Cache to persist the products_per_order table in memory to quickly update the dashboard with each query.","A":"Populate the dashboard by configuring a nightly batch job to save the required values as a table overwritten with each update.","C":"Configure a webhook to execute an incremental read against products_per_order each time the dashboard is refreshed."},"answer_ET":"A","url":"https://www.examtopics.com/discussions/databricks/view/129697-exam-certified-data-engineer-professional-topic-1-question/","answer_description":"","isMC":true,"question_text":"The business intelligence team has a dashboard configured to track various summary metrics for retail stores. This includes total sales for the previous day alongside totals and averages for a variety of time periods. The fields required to populate this dashboard have the following schema:\n\n//IMG//\n\n\nFor demand forecasting, the Lakehouse contains a validated table of all itemized sales updated incrementally in near real-time. This table, named products_per_order, includes the following fields:\n\n//IMG//\n\n\nBecause reporting on long-term sales trends is less volatile, analysts using the new dashboard only require data to be refreshed once daily. Because the dashboard will be queried interactively by many users throughout a normal business day, it should return results quickly and reduce total compute associated with each materialization.\n\nWhich solution meets the expectations of the end users while controlling and limiting possible costs?","timestamp":"2023-12-29 15:36:00","answers_community":["A (93%)","7%"],"exam_id":163},{"id":"418bK9t6VeYfcKLEbtG4","unix_timestamp":1693216020,"topic":"1","question_images":["https://img.examtopics.com/certified-data-engineer-professional/image3.png"],"question_id":168,"answer_images":[],"answer":"A","discussion":[{"timestamp":"1694080800.0","content":"Selected Answer: A\nYou need:\n- Batch operation since it is at most once a day\n- Append, since you need to keep track of past predictions\n\nA is the correct answer. You don't need to specify \"format\" when you use saveAsTable.","upvote_count":"13","poster":"thxsgod","comment_id":"1001405"},{"comment_id":"1294572","poster":"benni_ale","timestamp":"1728365100.0","upvote_count":"1","content":"Selected Answer: A\nBatch, Append"},{"upvote_count":"1","poster":"coercion","timestamp":"1716130500.0","content":"Selected Answer: A\ndefault table format is delta so no need to specify the format.\nAs per the requirement, \"append\" mode is required to maintain the history. Default mode is \"ErrorIfExists\"","comment_id":"1213850"},{"timestamp":"1705148100.0","comment_id":"1121595","content":"Selected Answer: A\nA is correct","upvote_count":"1","poster":"Jay_98_11"},{"comment_id":"1102680","upvote_count":"1","poster":"kz_data","timestamp":"1703172960.0","content":"Selected Answer: A\nA is correct"},{"content":"Selected Answer: A\nCorrect","upvote_count":"1","timestamp":"1697005080.0","poster":"sturcu","comment_id":"1040240"},{"upvote_count":"1","comment_id":"1040239","content":"Correct","poster":"sturcu","timestamp":"1697005020.0"},{"upvote_count":"2","comments":[{"content":"First: the default node Databricks saves tables IS Delta Format. So no reason why you say it wouldn't benefit from Lakehouse features.\nSecond: the default write mode is Error, means that if you try to write to a location and that already exists there, it will prone a Error. And the question specify that you gonna write once a day.\nYou better revisit basic topics before continue to the professional level certification, or buy the dump entirely.","poster":"Starvosxant","timestamp":"1696879440.0","upvote_count":"4","comment_id":"1038955"},{"comment_id":"1013181","timestamp":"1695307140.0","comments":[{"content":"Its also said they want to compare past values as well, so mode needs to be append. By default is error mode.","poster":"pradyumn9999","timestamp":"1698058260.0","comment_id":"1051722","upvote_count":"4"}],"content":"Here's why:\nA. saves the data as a managed table, which may not be efficient for large-scale data or frequent updates. It doesn't utilize Delta Lake capabilities.\nC.is used for streaming operations, not batch processing. Also, using \"overwrite\" as output mode will replace the existing data each time, which is not suitable for keeping historical predictions.\nD.is similar to option A but with \"overwrite\" mode. It will replace the entire table each time, which is not suitable for maintaining a historical record of predictions.\n\nE. is also for streaming operations and not for batch processing. Additionally, it uses the \"table\" method, which is not typically used for writing batch data into Delta Lake tables.\nOption B is suitable for batch processing, writes data in Delta Lake format, and allows you to efficiently maintain a historical record of predictions while minimizing compute costs.","upvote_count":"3","poster":"Eertyy"}],"timestamp":"1695307020.0","comment_id":"1013178","content":"answer is B","poster":"Eertyy"},{"timestamp":"1693216080.0","upvote_count":"1","comment_id":"992014","content":"Selected answer is wrong, not write Format is specified in A.","poster":"buggumaster"},{"poster":"buggumaster","timestamp":"1693216020.0","upvote_count":"1","comment_id":"992013","content":"Selected answer is wrong, not writeMode is specified in A."}],"choices":{"A":"preds.write.mode(\"append\").saveAsTable(\"churn_preds\")","E":"","B":"preds.write.format(\"delta\").save(\"/preds/churn_preds\")","C":"","D":""},"answer_ET":"A","url":"https://www.examtopics.com/discussions/databricks/view/119221-exam-certified-data-engineer-professional-topic-1-question-7/","answer_description":"","isMC":true,"question_text":"The data science team has created and logged a production model using MLflow. The following code correctly imports and applies the production model to output the predictions as a new DataFrame named preds with the schema \"customer_id LONG, predictions DOUBLE, date DATE\".\n//IMG//\n\nThe data science team would like predictions saved to a Delta Lake table with the ability to compare all predictions across time. Churn predictions will be made at most once per day.\nWhich code block accomplishes this task while minimizing potential compute costs?","answers_community":["A (100%)"],"timestamp":"2023-08-28 11:47:00","exam_id":163},{"id":"X61fAqNn7kVxeq2PS6Lf","unix_timestamp":1698163320,"topic":"1","question_images":[],"question_id":169,"answer_images":[],"answer":"A","discussion":[{"poster":"aragorn_brego","timestamp":"1700589840.0","content":"Selected Answer: A\nThis strategy aims to control the size of the output Parquet files without shuffling the data. The spark.sql.files.maxPartitionBytes parameter sets the maximum size of a partition that Spark will read. By setting it to 512 MB, you are aligning the read partition size with the desired output file size. Since the transformations are narrow (meaning they do not require shuffling), the number of partitions should roughly correspond to the number of output files when writing out to Parquet, assuming the data is evenly distributed and there is no data expansion during processing.","upvote_count":"10","comment_id":"1076528"},{"content":"Selected Answer: D\nD is the only one that does the trick.\n\nNote, we can not do shuffling.\n\nWrong answers:\n\nA: spark.sql.files.maxPartitionBytes is about reading, not writing.(The maximum number of bytes to pack into a single partition when reading files. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC. )\n\nB: spark.sql.adaptive.advisoryPartitionSizeInBytes takes effect while shuffling and sorting does not make sense (The advisory size in bytes of the shuffle partition during adaptive optimization (when spark.sql.adaptive.enabled is true). It takes effect when Spark coalesces small shuffle partitions or splits skewed shuffle partition.)\n\nC: Would work but spark.sql.adaptive.advisoryPartitionSizeInBytes would need shuffling.\n\nE. spark.sql.shuffle.partitions (Configures the number of partitions to use when shuffling data for joins or aggregations.) is not about writing.","comment_id":"1130323","poster":"Def21","comments":[{"poster":"arekm","upvote_count":"1","comment_id":"1335395","timestamp":"1735804260.0","content":"D does repartition, which the question says we should try to avoid."},{"timestamp":"1733711700.0","comment_id":"1323835","upvote_count":"1","content":"spark.sql.files.maxPartitionBytes is not just for reading files","poster":"carlosmps"},{"poster":"azurefan777","upvote_count":"4","comment_id":"1308675","content":"Answer D is wrong -> repartition does perform shuffling in Spark. When you use repartition, Spark redistributes the data across the specified number of partitions, which requires moving data between nodes to achieve the new partitioning. Answer A should be correct","timestamp":"1731055920.0"}],"timestamp":"1706080200.0","upvote_count":"6"},{"comments":[{"comment_id":"1330244","timestamp":"1734833280.0","upvote_count":"2","content":"Given the requirement to avoid shuffling, Option A is the most suitable choice. By setting spark.sql.files.maxPartitionBytes to 512 MB, you influence the partitioning during the read phase, which can help in achieving the desired file sizes during the write operation. However, it's important to note that this approach may not guarantee exact file sizes, and some variability may occur. If achieving precise file sizes is critical and shuffling is permissible, Option D would be the preferred strategy.","poster":"AlejandroU"}],"content":"Selected Answer: D\nAnswer D. Explicitly repartitioning to 2,048 partitions ensures that the output files are close to the desired size of 512 MB, provided the data distribution is relatively even.\nRepartitioning directly addresses the problem by controlling the number of partitions, which directly affects the output file size\nWhy not option A ? Misinterpretation of spark.sql.files.maxPartitionBytes in Option A:\nThe assessment incorrectly states that this configuration controls the maximum size of files when writing to Parquet. This setting controls the size of partitions when reading data, not during writing.","timestamp":"1734832620.0","comment_id":"1330235","upvote_count":"1","poster":"AlejandroU"},{"timestamp":"1733686860.0","poster":"temple1305","comment_id":"1323738","upvote_count":"1","content":"Selected Answer: C\nspark.sql.adaptive.advisoryPartitionSizeInBytes - \nThe advisory size in bytes of the shuffle partition during adaptive optimization (when spark.sql.adaptive.enabled is true). It takes effect when Spark coalesces small shuffle partitions or splits skewed shuffle partition.\nAnd then we do coalesce - without shuffle - so have to work!"},{"content":"Selected Answer: A\nI though D, but default num of partitions is 200, so you cant do coalesce (2048) (you cant increase numb of partitions through coalesce), so its not possible to do it without repartitioning and shuffle. Only A can be done without Shuffle","poster":"nedlo","timestamp":"1729950720.0","upvote_count":"2","comment_id":"1303293"},{"content":"Option A\nspark.sql.files.maxPartitionBytes controls the maximum size of partitions during reading on the Spark cluster, and that reducing this value could lead to more partitions and thus potentially more output files. The key point is that it works best when no shuffles occur, which aligns with the scenario of having narrow transformations only.","upvote_count":"2","timestamp":"1726413600.0","comment_id":"1284189","poster":"sdas1","comments":[{"comment_id":"1284190","timestamp":"1726413660.0","poster":"sdas1","content":"Given that no shuffle occurs and you're aiming to control the file sizes during output, adjusting spark.sql.files.maxPartitionBytes could help indirectly by determining the partition size for reading. Since the number of input partitions can influence the size of the output files when no shuffle occurs, the partition size may closely match the size of the files being written out.","upvote_count":"1","comments":[{"poster":"sdas1","upvote_count":"1","comments":[{"poster":"sdas1","comment_id":"1284192","content":"Thus, Option A is also a valid strategy:\n\nSet spark.sql.files.maxPartitionBytes to 512 MB, process the data with narrow transformations, and write to Parquet.\nBy reducing the value of spark.sql.files.maxPartitionBytes, you ensure more partitions are created during the read phase, leading to output files closer to the desired size, assuming the transformations are narrow and no shuffling occurs.","upvote_count":"1","timestamp":"1726413660.0"}],"content":"If the transformations remain narrow, then Spark won't repartition the data unless explicitly instructed to do so (e.g., through a repartition or coalesce operation). In this case, using spark.sql.files.maxPartitionBytes to adjust the read partition size to 512 MB could indirectly control the number of output files and ensure they align with the target file size.","timestamp":"1726413660.0","comment_id":"1284191"}]}]},{"comments":[{"content":"hey, 1TB=1000GB=1^6MB.","poster":"hal2401me","comment_id":"1171610","upvote_count":"4","timestamp":"1710241260.0"}],"poster":"vikram12apr","content":"Selected Answer: A\nD is not correct as it will create 2048 target files of 0.5 MB each\nOnly A will do the job as it will read this file in 2 partition ( 1 TB = 512*2 MB) and as we are not doing any shuffling(not mentioned in option) it will create those many partition file i.e 2 part files","comment_id":"1169434","timestamp":"1709984520.0","upvote_count":"1"},{"poster":"hal2401me","comment_id":"1167675","upvote_count":"1","content":"Selected Answer: D\nChatGPT says D: This strategy directly addresses the desired part-file size by repartitioning the data. It avoids shuffling during narrow transformations.\nRecommended for achieving the desired part-file size without unnecessary shuffling.","timestamp":"1709789520.0"},{"upvote_count":"1","content":"Selected Answer: D\nD is mot suitable.","poster":"Curious76","comment_id":"1162035","timestamp":"1709161440.0"},{"content":"Selected Answer: A\nThis approach ensures that each partition will be approximately the target part-file size, which can improve the efficiency of the data write. It also avoids the need for a shuffle operation, which can be expensive in terms of performance.","upvote_count":"3","timestamp":"1707334860.0","poster":"vctrhugo","comment_id":"1143743"},{"timestamp":"1706707740.0","poster":"adenis","comment_id":"1136782","upvote_count":"1","content":"Selected Answer: C\nÐ¡ is correct"},{"upvote_count":"2","poster":"spaceexplorer","timestamp":"1706133960.0","content":"Selected Answer: A\nRest of the answers trigger shuffles","comment_id":"1131156"},{"timestamp":"1704141660.0","content":"Selected Answer: A\nA is correct. \nThe question states Which strategy will yield the best performance without shuffling data.\nThe other options involve shuffling either manually or through AQE","upvote_count":"2","poster":"divingbell17","comment_id":"1111424"},{"content":"C is correct answer","timestamp":"1703059320.0","poster":"911land","comment_id":"1101365","upvote_count":"1"},{"comment_id":"1099580","content":"Selected Answer: A\n- spark.sql.files.maxPartitionBytes: 128MB (The maximum number of bytes to pack into a single partition when reading files. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.)","poster":"alexvno","upvote_count":"1","timestamp":"1702893060.0"},{"comment_id":"1084755","content":"Selected Answer: C\nHere's a breakdown of the reasons:\n\nspark.sql.adaptive.advisoryPartitionSizeInBytes: This configuration parameter is designed to provide advisory partition sizes for the adaptive query execution framework. It can help in controlling the partition sizes without triggering unnecessary shuffling.\n\ncoalesce(2048): Coalescing to a specific number of partitions after the narrow transformations allows you to control the number of output files without triggering a shuffle. This helps achieve the target part-file size without incurring the overhead of a full shuffle.\n\nSetting a specific target: The strategy outlines the goal of achieving a target part-file size of 512 MB, which aligns with the requirement.","upvote_count":"3","poster":"petrv","timestamp":"1701376320.0"},{"upvote_count":"1","poster":"ocaj90","timestamp":"1699984980.0","comment_id":"1070716","content":"obviously D. It allows you to control both the number of partitions and the final part-file size, which aligns with the requirements. Option B shuffles partitions, which is not allowed."},{"content":"Selected Answer: B\nThe number of output files saved to the disk is equal to the number of partitions in the Spark executors when the write operation is performed.","poster":"sturcu","timestamp":"1698163320.0","comment_id":"1053027","upvote_count":"2"}],"choices":{"D":"Ingest the data, execute the narrow transformations, repartition to 2,048 partitions (1TB* 1024*1024/512), and then write to parquet.","E":"Set spark.sql.shuffle.partitions to 512, ingest the data, execute the narrow transformations, and then write to parquet.","C":"Set spark.sql.adaptive.advisoryPartitionSizeInBytes to 512 MB bytes, ingest the data, execute the narrow transformations, coalesce to 2,048 partitions (1TB*1024*1024/512), and then write to parquet.","A":"Set spark.sql.files.maxPartitionBytes to 512 MB, ingest the data, execute the narrow transformations, and then write to parquet.","B":"Set spark.sql.shuffle.partitions to 2,048 partitions (1TB*1024*1024/512), ingest the data, execute the narrow transformations, optimize the data by sorting it (which automatically repartitions the data), and then write to parquet."},"answer_ET":"A","url":"https://www.examtopics.com/discussions/databricks/view/124575-exam-certified-data-engineer-professional-topic-1-question/","answer_description":"","isMC":true,"question_text":"A data ingestion task requires a one-TB JSON dataset to be written out to Parquet with a target part-file size of 512 MB. Because Parquet is being used instead of Delta Lake, built-in file-sizing features such as Auto-Optimize & Auto-Compaction cannot be used.\n\nWhich strategy will yield the best performance without shuffling data?","timestamp":"2023-10-24 18:02:00","answers_community":["A (57%)","D (24%)","Other"],"exam_id":163},{"id":"NlfQlHKNB3JTkMFHr8hQ","unix_timestamp":1698210600,"timestamp":"2023-10-25 07:10:00","question_images":["https://img.examtopics.com/certified-data-engineer-professional/image34.png"],"answers_community":["A (100%)"],"answer_images":[],"question_id":170,"answer":"A","discussion":[{"upvote_count":"9","content":"Selected Answer: A\nTo handle late-arriving data in a streaming aggregation, you need to specify a watermark, which tells the streaming query how long to wait for late data. The withWatermark method is used for this purpose in Spark Structured Streaming. It defines the threshold for how late the data can be relative to the latest data that has been seen in the same window.","timestamp":"1700590020.0","comment_id":"1076530","poster":"aragorn_brego"},{"timestamp":"1698210600.0","comment_id":"1053462","content":"Selected Answer: A\nwithWatermark.\nThere sliding window is doe through the window function","upvote_count":"9","poster":"sturcu"},{"timestamp":"1723665660.0","comment_id":"1265967","upvote_count":"1","poster":"71dfab9","content":"Selected Answer: A\nThe withWatermark method is used in streaming DataFrames when processing real-time data streams. This method helps in managing stateful operations, such as aggregations, by specifying a time column to use for watermarking. Watermarking is a mechanism to handle late data (data that arrives later than expected) by defining a threshold time window beyond which late data is considered too late to be included in aggregations.\n\nThe slidingWindow function mentioned in D is not a standard function in Databricks or Apache Spark."},{"timestamp":"1699519500.0","comment_id":"1066244","upvote_count":"3","poster":"Dileepvikram","content":"Answer is A"}],"answer_ET":"A","answer_description":"","exam_id":163,"choices":{"C":"await(\"event_time + â€˜10 minutes'\")","A":"withWatermark(\"event_time\", \"10 minutes\")","B":"awaitArrival(\"event_time\", \"10 minutes\")","D":"slidingWindow(\"event_time\", \"10 minutes\")","E":"delayWrite(\"event_time\", \"10 minutes\")"},"topic":"1","question_text":"A junior data engineer has been asked to develop a streaming data pipeline with a grouped aggregation using DataFrame df. The pipeline needs to calculate the average humidity and average temperature for each non-overlapping five-minute interval. Incremental state information should be maintained for 10 minutes for late-arriving data.\n\nStreaming DataFrame df has the following schema:\n\n\"device_id INT, event_time TIMESTAMP, temp FLOAT, humidity FLOAT\"\n\nCode block:\n\n//IMG//\n\n\nChoose the response that correctly fills in the blank within the code block to complete this task.","isMC":true,"url":"https://www.examtopics.com/discussions/databricks/view/124595-exam-certified-data-engineer-professional-topic-1-question/"}],"exam":{"id":163,"isBeta":false,"isMCOnly":true,"numberOfQuestions":200,"provider":"Databricks","isImplemented":true,"lastUpdated":"12 Apr 2025","name":"Certified Data Engineer Professional"},"currentPage":34},"__N_SSP":true}