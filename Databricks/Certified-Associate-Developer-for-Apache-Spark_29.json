{"pageProps":{"questions":[{"id":"btH18Zh08m8vVDJmRI3K","timestamp":"2023-05-03 12:05:00","unix_timestamp":1683108300,"url":"https://www.examtopics.com/discussions/databricks/view/108385-exam-certified-associate-developer-for-apache-spark-topic-1/","isMC":true,"answer":"E","discussion":[{"content":"Selected Answer: E\nE is correct. The \"format\" parameter should be used instead of \"source\" (default \"parquet\"):\n\nhttps://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.load.html\n\nformat: str, optional\n optional string for format of the data source. Default to ‘parquet’.","upvote_count":"1","timestamp":"1731608820.0","poster":"juliom6","comment_id":"1070729"},{"content":"Selected Answer: E\nI would go for E","comment_id":"1064832","poster":"newusername","upvote_count":"1","timestamp":"1730987160.0"},{"poster":"Singh_Sumit","timestamp":"1727674200.0","upvote_count":"1","comment_id":"1021272","content":"spark.read.load(PARQUET_PATH,format='parquet')\n\nLoad is valid, if provided with format."},{"content":"Selected Answer: E\nIntention is to read a parquet at the file path filePath into a DataFrame","timestamp":"1723726020.0","upvote_count":"2","comment_id":"981674","poster":"Ram459"},{"upvote_count":"2","comment_id":"968202","content":"The parameters for load() function are: path, format, schema, **options\nA. Overall it makes sense, but do we really need to use schema?\nB. There is load operation, that's FALSE\nC. read is used without parenthesis, FALSE\nD. It should indeed, but there's no source parameter, FALSE\nE. That's true, but we need to put quotes for the filePath, then it's FALSE\n\nMakes it A, but the question is really strange and not clear.","timestamp":"1722437400.0","poster":"cookiemonster42","comments":[{"timestamp":"1722438180.0","content":"UPD - parquet already has schema in it, it's not needed, then, I don't know what the answer is then","upvote_count":"2","comment_id":"968216","poster":"cookiemonster42"}]},{"comment_id":"930966","timestamp":"1719084840.0","content":"Selected Answer: E\nAnswer should be E. Removing source and default is 'parquet' anyway. However, it is not ideal to use load, rather the respective method.\n\nhttps://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrameReader.load.html?highlight=dataframereader%20load#pyspark.sql.DataFrameReader.load","upvote_count":"3","poster":"Larrave"},{"content":"1. pyspark.sql.SparkSession.read Returns a DataFrameReader\nhttps://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.read.html#pyspark.sql.SparkSession.read\n2. we check this DataFrameReader, it contains both \"load\" and \"parquet\" methods.\n2.1. for load, load(path, format, schema)\nhttps://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.load.html#pyspark.sql.DataFrameReader.load\nTherefore, the answer is A or E.\nTypically parquet contains schema information. \nI do not like this question, because if reading a parquet file, directly use spark.read.parquet()","comment_id":"916504","upvote_count":"2","timestamp":"1717695060.0","poster":"ZSun"},{"comments":[{"timestamp":"1737978120.0","content":"the statement \"there is no load() operation\" in answer B is clearly wrong as this operation exists in pyspark. E is correct answer - instead of source parameter, you should use format to achieve the same result","poster":"tmz1","upvote_count":"1","comment_id":"1347372"}],"upvote_count":"4","content":"Selected Answer: B\nThe correct code block to read a parquet file would be \n\nspark.read.parquet(filePath).","timestamp":"1714730700.0","poster":"4be8126","comment_id":"888376"}],"choices":{"D":"The filePath argument to the load() operation should be quoted.","E":"There is no source parameter to the load() operation – it can be removed.","B":"There is no load() operation – it should be parquet() instead.","A":"There is no source parameter to the load() operation – the schema parameter should be used instead.","C":"The spark.read operation should be followed by parentheses to return a DataFrameReader object."},"exam_id":161,"question_id":141,"question_text":"The code block shown below contains an error. The code block intended to read a parquet at the file path filePath into a DataFrame. Identify the error.\nCode block:\nspark.read.load(filePath, source – \"parquet\")","question_images":[],"answers_community":["E (64%)","B (36%)"],"answer_ET":"E","answer_images":[],"topic":"1","answer_description":""},{"id":"2grY7Tnn9wmz64ZxYQIW","answers_community":["A (100%)"],"answer_images":[],"topic":"1","question_images":[],"discussion":[{"content":"Selected Answer: A\nThe most likely operation to result in a shuffle is:\n\nA. DataFrame.join()\n\nExplanation: A shuffle operation in Spark involves redistributing and reorganizing data across partitions. It typically occurs when data needs to be rearranged or merged based on a specific key or condition. DataFrame joins involve combining two DataFrames based on a common key column, and this operation often requires data to be shuffled to ensure that matching records are located on the same executor or partition. The shuffle process involves exchanging data between nodes or executors in the cluster, which can incur significant data movement and network communication overhead.","poster":"TmData","timestamp":"1718635860.0","upvote_count":"2","comment_id":"926050"},{"upvote_count":"2","comment_id":"879327","poster":"4be8126","timestamp":"1713962280.0","content":"Selected Answer: A\nThe operation that is most likely to result in a shuffle is DataFrame.join().\n\nJoin operation requires data to be combined from two different sources based on a common key, and this typically involves a reorganization of the data such that the data with the same keys are co-located in the same executor. This process is known as a shuffle operation, which can be a performance-intensive operation, especially for large datasets.\n\nThe other DataFrame operations such as filter(), union(), where() or drop() do not require data to be shuffled across the nodes."}],"answer_description":"","unix_timestamp":1682339880,"answer":"A","isMC":true,"timestamp":"2023-04-24 14:38:00","url":"https://www.examtopics.com/discussions/databricks/view/107314-exam-certified-associate-developer-for-apache-spark-topic-1/","answer_ET":"A","choices":{"D":"DataFrame.where()","E":"DataFrame.drop()","B":"DataFrame.filter()","C":"DataFrame.union()","A":"DataFrame.join()"},"question_text":"Which of the following operations is most likely to result in a shuffle?","exam_id":161,"question_id":142},{"id":"e04eaea1H2UXCp5mHAoi","question_images":[],"answer_ET":"C","answer":"C","unix_timestamp":1685596920,"exam_id":161,"choices":{"B":"2, 4, 1","D":"2, 5, 1","C":"3, 5, 1","E":"3, 4, 1","A":"3, 5, 6"},"question_text":"In what order should the below lines of code be run in order to read a JSON file at the file path filePath into a DataFrame with the specified schema schema?\nLines of code:\n1. .json(filePath, schema = schema)\n2. .storesDF\n3. .spark \\\n4. .read() \\\n5. .read \\\n6. .json(filePath, format = schema)","discussion":[{"upvote_count":"1","comment_id":"1145045","timestamp":"1723145040.0","poster":"azure_bimonster","content":"Selected Answer: C\nwe use the following structure: spark.read.json(filePath, schema=schemaName)"},{"poster":"juliom6","comment_id":"1070739","timestamp":"1715704680.0","upvote_count":"2","content":"Selected Answer: C\nC is correct:\n\nhttps://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.json.html\n\njson function does not have a \"format\" parameter."},{"content":"storesDF = spark.read.json(filePath, schema = schema)\nC","timestamp":"1701957960.0","upvote_count":"3","poster":"ZSun","comment_id":"917193"},{"comment_id":"911758","comments":[{"timestamp":"1719938640.0","comment_id":"1112149","content":"This is so wrong.. in order to read a table you need to use spark.read.json / parquet / Table.","upvote_count":"1","poster":"pnev"}],"content":"Selected Answer: B\n2. .storesDF: This line is unrelated to reading the JSON file and can be disregarded.\n.read(): This line invokes the DataFrameReader's read() method to create a DataFrameReader object.\n.json(filePath, schema=schema): This line uses the DataFrameReader object to read the JSON file at the specified filePath into a DataFrame with the provided schema.","poster":"Jtic","upvote_count":"1","timestamp":"1701415320.0"}],"answers_community":["C (75%)","B (25%)"],"answer_description":"","question_id":143,"topic":"1","answer_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/databricks/view/110744-exam-certified-associate-developer-for-apache-spark-topic-1/","timestamp":"2023-06-01 07:22:00"},{"id":"NS832IBTo4W2xETsiKFK","unix_timestamp":1697658060,"url":"https://www.examtopics.com/discussions/databricks/view/123987-exam-certified-associate-developer-for-apache-spark-topic-1/","discussion":[{"comment_id":"1310800","poster":"58470e1","timestamp":"1731434700.0","upvote_count":"1","content":"D is correct."},{"upvote_count":"2","content":"Selected Answer: D\nCorrect is D","timestamp":"1699365720.0","comment_id":"1064853","poster":"newusername"},{"poster":"juadaves","comment_id":"1047190","comments":[],"content":"B\n\nhttps://stackoverflow.com/questions/30520428/what-is-the-difference-between-memory-only-and-memory-and-disk-caching-level-in","upvote_count":"1","timestamp":"1697658060.0"}],"answer_images":[],"exam_id":161,"question_id":144,"choices":{"E":"MEMORY_ONLY","C":"MEMORY_AND_DISK","B":"MEMORY_AND_DISK_SER","D":"MEMORY_AND_DISK_2","A":"MEMORY_ONLY_2"},"question_text":"Which of the following storage levels should be used to store as much data as possible in memory on two cluster nodes while storing any data that does not fit in memory on disk to be read in when needed?","answer_description":"","topic":"1","answer_ET":"D","answer":"D","isMC":true,"timestamp":"2023-10-18 21:41:00","answers_community":["D (100%)"],"question_images":[]},{"id":"KHUp3UHTYx7wPskx75Nc","exam_id":161,"answer":"B","question_text":"Which of the following Spark properties is used to configure the maximum size of an automatically broadcasted DataFrame when performing a join?","timestamp":"2023-09-16 02:43:00","topic":"1","question_id":145,"question_images":[],"choices":{"B":"spark.sql.autoBroadcastJoinThreshold","C":"spark.sql.shuffle.partitions","A":"spark.sql.broadcastTimeout","D":"spark.sql.inMemoryColumnarStorage.batchSize","E":"spark.sql.adaptive.skewedJoin.enabled"},"discussion":[{"upvote_count":"1","content":"Selected Answer: B\nThe correct answer is B. spark.sql.autoBroadcastJoinThreshold. This property in Apache Spark is used to configure the maximum size (in bytes) of a table that will be broadcast to all worker nodes when performing a join. If the size of the table is below this threshold, it will be broadcasted, which can significantly speed up join operations.","timestamp":"1726447380.0","comment_id":"1008778","poster":"thanab"}],"answer_images":[],"answer_ET":"B","isMC":true,"unix_timestamp":1694824980,"url":"https://www.examtopics.com/discussions/databricks/view/120817-exam-certified-associate-developer-for-apache-spark-topic-1/","answer_description":"","answers_community":["B (100%)"]}],"exam":{"numberOfQuestions":185,"isBeta":false,"isMCOnly":true,"id":161,"name":"Certified Associate Developer for Apache Spark","provider":"Databricks","lastUpdated":"12 Apr 2025","isImplemented":true},"currentPage":29},"__N_SSP":true}