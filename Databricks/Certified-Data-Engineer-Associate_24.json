{"pageProps":{"questions":[{"id":"QHc4A85C4hythhACPdEM","choices":{"A":"INSERT INTO my_table VALUES ('a1', 6, 9.4)","D":"UPDATE my_table VALUES ('a1', 6, 9.4)","E":"UPDATE VALUES ('a1', 6, 9.4) my_table","B":"my_table UNION VALUES ('a1', 6, 9.4)","C":"INSERT VALUES ( 'a1' , 6, 9.4) INTO my_table"},"url":"https://www.examtopics.com/discussions/databricks/view/124370-exam-certified-data-engineer-associate-topic-1-question-50/","isMC":true,"exam_id":162,"answer":"A","answer_images":[],"question_images":[],"answer_ET":"A","answers_community":["A (100%)"],"unix_timestamp":1697995560,"question_id":116,"question_text":"A data engineer has been given a new record of data:\n\nid STRING = 'a1'\nrank INTEGER = 6\nrating FLOAT = 9.4\n\nWhich of the following SQL commands can be used to append the new record to an existing Delta table my_table?","timestamp":"2023-10-22 19:26:00","answer_description":"","topic":"1","discussion":[{"poster":"80370eb","comment_id":"1263467","timestamp":"1723293180.0","upvote_count":"1","content":"Selected Answer: A\ninsert into table comment"},{"poster":"benni_ale","timestamp":"1714368900.0","comment_id":"1203822","upvote_count":"1","content":"Selected Answer: A\nA is correct"},{"poster":"azure_bimonster","content":"Selected Answer: A\nA is correct because syntax is correct","timestamp":"1705766940.0","upvote_count":"2","comment_id":"1127372"},{"timestamp":"1704810600.0","comment_id":"1117568","content":"Selected Answer: A\nA is correct","poster":"Annelijn","upvote_count":"2"},{"poster":"meow_akk","timestamp":"1697995620.0","upvote_count":"3","content":"Ans A : check the correct syntax for insert into","comment_id":"1050931"}]},{"id":"lBgbN1YHqmWQOjBzxobo","answer":"B","answers_community":["B (100%)"],"choices":{"E":"VACUUM","C":"COMPACTION","A":"REDUCE","D":"REPARTITION","B":"OPTIMIZE"},"question_text":"A data engineer has realized that the data files associated with a Delta table are incredibly small. They want to compact the small files to form larger files to improve performance.\n\nWhich of the following keywords can be used to compact the small files?","question_id":117,"isMC":true,"answer_ET":"B","unix_timestamp":1697810220,"answer_images":[],"topic":"1","discussion":[{"poster":"kishanu","timestamp":"1697810220.0","content":"Selected Answer: B\nOPTIMIZE can be used to club small files into 1 and improve performance.","comment_id":"1048842","upvote_count":"5"},{"upvote_count":"1","poster":"80370eb","timestamp":"1723293000.0","content":"Selected Answer: B\nThe OPTIMIZE command in Databricks is used to compact small files into larger ones, improving the performance of Delta tables.","comment_id":"1263464"},{"comment_id":"1262789","poster":"80370eb","content":"Selected Answer: B\nThe OPTIMIZE command in Delta Lake merges small files into larger files, which can help improve query performance and manage storage more efficiently.","upvote_count":"1","timestamp":"1723184280.0"},{"timestamp":"1714368960.0","comment_id":"1203823","poster":"benni_ale","upvote_count":"1","content":"Selected Answer: B\nB is correct"},{"content":"Selected Answer: B\nOPTIMIZE is the correct answer. Compacting small files using the OPTIMIZE command improves table performance such as by combining multiple small files into larger ones.","poster":"UGOTCOOKIES","upvote_count":"2","timestamp":"1706230140.0","comment_id":"1132184"},{"content":"Selected Answer: B\nOPTIMIZE would help in this scenario","poster":"azure_bimonster","upvote_count":"2","comment_id":"1127373","timestamp":"1705767000.0"},{"comment_id":"1094881","timestamp":"1702410060.0","content":"Selected Answer: B\nIts B https://docs.databricks.com/en/delta/optimize.html","upvote_count":"2","poster":"nedlo"},{"content":"Ans B : optimize is used to compact small files which in turn improves perf","upvote_count":"3","comment_id":"1050932","timestamp":"1697995620.0","poster":"meow_akk"}],"url":"https://www.examtopics.com/discussions/databricks/view/124126-exam-certified-data-engineer-associate-topic-1-question-51/","timestamp":"2023-10-20 15:57:00","question_images":[],"answer_description":"","exam_id":162},{"id":"OjcWDz5SyNJlXdIMDH7e","discussion":[{"comment_id":"1263465","timestamp":"1723293120.0","poster":"80370eb","upvote_count":"1","content":"Selected Answer: C\nDelta Lake builds on top of the Parquet file format, adding features like ACID transactions, versioning, and more, while leveraging Parquet's efficient columnar storage capabilities."},{"upvote_count":"1","content":"Selected Answer: C\nDelta Lake tables use the Parquet file format for storing data, which is a columnar storage format optimized for performance and efficient data processing.","poster":"80370eb","timestamp":"1723184340.0","comment_id":"1262790"},{"comment_id":"1203824","content":"Selected Answer: C\nParquet for data and JSON for metadata","timestamp":"1714368960.0","poster":"benni_ale","upvote_count":"1"},{"poster":"azure_bimonster","timestamp":"1705767060.0","content":"Selected Answer: C\nParquet","upvote_count":"1","comment_id":"1127374"},{"upvote_count":"1","comment_id":"1088506","content":"Selected Answer: C\nParquet format because its columnar format, much faster alternative to CSV because it supports partition pruning for example. No such file format as \"Delta\"","timestamp":"1701782220.0","poster":"nedlo"},{"upvote_count":"3","comment_id":"1057636","poster":"kishore1980","content":"Selected Answer: C\nParquet format is correct","timestamp":"1698667920.0"},{"poster":"kishanu","timestamp":"1698466500.0","upvote_count":"1","comment_id":"1055975","content":"Selected Answer: C\nParquet it is"},{"comment_id":"1052553","timestamp":"1698125580.0","comments":[{"poster":"kishanu","comment_id":"1055974","timestamp":"1698466440.0","upvote_count":"4","content":"Buddy it should be Parquet, hence C"}],"content":"Selected Answer: B\nparquet format","upvote_count":"1","poster":"SD5713"},{"comment_id":"1050939","timestamp":"1697995800.0","upvote_count":"1","poster":"meow_akk","content":"so i think data from delta lake is stored in parquet format .. while the storage format seems to be delta .. very confusing\nsome notes :\nWhat format does Delta Lake use to store data? Delta Lake uses versioned Parquet files to store your data in your cloud storage. Apart from the versions, Delta Lake also stores a transaction log to keep track of all the commits made to the table or blob store directory to provide ACID transactions.\nhttps://docs.delta.io/latest/delta-faq.html"}],"exam_id":162,"topic":"1","choices":{"C":"Parquet","B":"CSV","A":"Delta","E":"A proprietary, optimized format specific to Databricks","D":"JSON"},"question_id":118,"timestamp":"2023-10-22 04:31:00","answer_ET":"C","answer_images":[],"answer":"C","question_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/124287-exam-certified-data-engineer-associate-topic-1-question-52/","question_text":"In which of the following file formats is data from Delta Lake tables primarily stored?","answers_community":["C (90%)","10%"],"isMC":true,"answer_description":"","unix_timestamp":1697941860},{"id":"UNUNi5c0rbqur26Cy4GD","url":"https://www.examtopics.com/discussions/databricks/view/124286-exam-certified-data-engineer-associate-topic-1-question-53/","exam_id":162,"timestamp":"2023-10-22 04:25:00","question_images":[],"question_id":119,"isMC":true,"answer_description":"","question_text":"Which of the following is stored in the Databricks customer's cloud account?","discussion":[{"timestamp":"1723293300.0","comment_id":"1263468","content":"Selected Answer: D\nData stored in Delta Lake or other formats on cloud storage is managed within the customer's own cloud account, while other components like the Databricks web application and cluster management metadata are managed by Databricks itself.","upvote_count":"2","poster":"80370eb"},{"timestamp":"1714369080.0","poster":"benni_ale","comment_id":"1203827","upvote_count":"2","content":"Selected Answer: D\nData is in Data plane"},{"timestamp":"1708577760.0","content":"Answer should be B\nBecause \nWhen the customer sets up a Spark cluster, the cluster virtual machines are deployed in the data plane in the customer's cloud account.","upvote_count":"1","comment_id":"1156133","poster":"Bob123456"},{"comment_id":"1127376","upvote_count":"2","content":"Selected Answer: D\nD is correct","poster":"azure_bimonster","timestamp":"1705767300.0"},{"timestamp":"1704795900.0","poster":"bartfto","upvote_count":"2","content":"Selected Answer: D\nD. Data","comment_id":"1117391"},{"timestamp":"1697941500.0","poster":"meow_akk","comment_id":"1050118","content":"D. Data","upvote_count":"4"}],"answer_images":[],"answers_community":["D (100%)"],"choices":{"A":"Databricks web application","B":"Cluster management metadata","D":"Data","E":"Notebooks","C":"Repos"},"answer_ET":"D","unix_timestamp":1697941500,"answer":"D","topic":"1"},{"id":"PLgn53CU3deo2sk63LtA","answer_images":[],"choices":{"D":"All of these","E":"Data lakehouse","B":"Data lake","A":"None of these","C":"Data warehouse"},"isMC":true,"answer_description":"","answer_ET":"E","exam_id":162,"url":"https://www.examtopics.com/discussions/databricks/view/124127-exam-certified-data-engineer-associate-topic-1-question-54/","timestamp":"2023-10-20 16:01:00","unix_timestamp":1697810460,"question_id":120,"question_text":"Which of the following can be used to simplify and unify siloed data architectures that are specialized for specific use cases?","discussion":[{"poster":"benni_ale","upvote_count":"1","comment_id":"1203828","timestamp":"1730187480.0","content":"Selected Answer: E\nE is correct"},{"content":"Selected Answer: E\nLakehouse, so E is correct","timestamp":"1721485020.0","upvote_count":"2","poster":"azure_bimonster","comment_id":"1127377"},{"poster":"kishanu","content":"Selected Answer: E\nData Lakehouse can be used as a single source of truth for multiple specific use cases","timestamp":"1713621660.0","comment_id":"1048844","upvote_count":"4"}],"question_images":[],"answers_community":["E (100%)"],"topic":"1","answer":"E"}],"exam":{"lastUpdated":"12 Apr 2025","isMCOnly":true,"provider":"Databricks","isImplemented":true,"numberOfQuestions":169,"isBeta":false,"name":"Certified Data Engineer Associate","id":162},"currentPage":24},"__N_SSP":true}