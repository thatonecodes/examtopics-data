{"pageProps":{"questions":[{"id":"LCl3DYzTKXokVOSBYiX1","discussion":[{"poster":"aragorn_brego","timestamp":"1700587740.0","comment_id":"1076494","upvote_count":"6","content":"Selected Answer: C\nDelta Lake maintains a transaction log that records details about every change made to a table. When you execute a count operation on a Delta table, Delta Lake can use the information in the transaction log to calculate the total number of records without having to scan all the data files. This is because the transaction log includes information about the number of records in each file, allowing for an efficient aggregation of these counts to get the total number of records in the table."},{"upvote_count":"5","timestamp":"1699131780.0","content":"Answer C \nhttps://delta.io/blog/2023-04-19-faster-aggregations-metadata/#:~:text=You%20can%20get%20the%20number,a%20given%20Delta%20table%20version.","comment_id":"1062381","poster":"Syd"},{"timestamp":"1734494340.0","content":"Selected Answer: D\nAnswer D. Parquet Metadata Usage: Delta Lake does utilize Parquet file metadata for COUNT(*) operations. Parquet files store metadata, including row counts. Delta efficiently reads this metadata to get the total count without scanning the actual data within the files. This is a key optimization for performance.\n\nWhy not always scan: Scanning all data files for every COUNT(*) would be extremely inefficient, especially for large tables. This defeats the purpose of using a columnar storage format like Parquet and the optimizations built into Delta Lake and Spark.\n\nThe transaction log tracks changes to the table (adds, deletes, updates) but doesn't store pre-computed row counts. It's used for time travel, ACID properties, and other Delta features.","comment_id":"1328254","poster":"AlejandroU","upvote_count":"2","comments":[{"timestamp":"1735779420.0","poster":"arekm","comment_id":"1335327","upvote_count":"1","content":"Definitely C - see link posted by Syd"}]},{"timestamp":"1734069240.0","upvote_count":"1","content":"Selected Answer: C\n\"stats\": \"{\\\"numRecords\\\": 3, \\\"minValues\\\": {\\\"x\\\": 1}, \\\"maxValues\\\": {\\\"x\\\": 3}, \\\"nullCount\\\": {\\\"x\\\": 0}}\",\n numRecords - In Delta tx logs will give you the value","comment_id":"1326024","poster":"Sriramiyer92"},{"comment_id":"1236964","timestamp":"1719329700.0","upvote_count":"2","poster":"Ati1362","content":"Selected Answer: C\nDelta transaction log"},{"timestamp":"1702929840.0","content":"Selected Answer: C\nTransaction log provides statistics about the delta table.","upvote_count":"4","comment_id":"1100010","poster":"sodere"},{"poster":"alexvno","timestamp":"1702887900.0","comment_id":"1099533","content":"Selected Answer: C\nC - transaction logs contains info about files rows count","upvote_count":"3"},{"content":"The answer is C","upvote_count":"2","poster":"Dileepvikram","timestamp":"1699489320.0","comment_id":"1066041"},{"content":"Selected Answer: C\nThe answer should be C","poster":"PearApple","upvote_count":"2","comment_id":"1063238","timestamp":"1699215180.0"},{"timestamp":"1698154920.0","comment_id":"1052890","content":"Selected Answer: C\ntotal rows will be calculated from delta logs","poster":"sturcu","upvote_count":"3"}],"question_images":[],"timestamp":"2023-10-24 15:42:00","url":"https://www.examtopics.com/discussions/databricks/view/124566-exam-certified-data-engineer-professional-topic-1-question/","choices":{"B":"The total count of rows will be returned from cached results unless REFRESH is run","A":"The total count of rows is calculated by scanning all data files","D":"The total count of records is calculated from the parquet file metadata","E":"The total count of records is calculated from the Hive metastore","C":"The total count of records is calculated from the Delta transaction logs"},"answer":"C","question_text":"A Databricks SQL dashboard has been configured to monitor the total number of records present in a collection of Delta Lake tables using the following query pattern:\n\n\nSELECT COUNT (*) FROM table -\n\nWhich of the following describes how results are generated each time the dashboard is updated?","question_id":161,"unix_timestamp":1698154920,"answer_description":"","answers_community":["C (91%)","9%"],"isMC":true,"answer_images":[],"exam_id":163,"answer_ET":"C","topic":"1"},{"id":"Y1oYlKc8Xz4jNeFTpJl6","url":"https://www.examtopics.com/discussions/databricks/view/124117-exam-certified-data-engineer-professional-topic-1-question/","choices":{"A":"Nothing will occur until a COMMIT command is executed.","D":"An error will occur because Delta Lake prevents the deletion of production data.","B":"The table will be removed from the catalog but the data will remain in storage.","C":"The table will be removed from the catalog and the data will be deleted.","E":"Data will be marked as deleted but still recoverable with Time Travel."},"question_images":["https://img.examtopics.com/certified-data-engineer-professional/image30.png"],"unix_timestamp":1697803440,"topic":"1","exam_id":163,"timestamp":"2023-10-20 14:04:00","question_id":162,"answers_community":["C (83%)","Other"],"discussion":[{"comments":[{"poster":"db22","content":"Agree that the answer is C - the question is misleading in saying it is Delta Lake table. However, it is managed table b/c there is no USING delta clause.","comment_id":"1285265","timestamp":"1726580700.0","upvote_count":"1"}],"poster":"hal2401me","comment_id":"1167613","content":"Selected Answer: C\nAccording to the exam courses answer is C, for a managed table dropped.\nBut, as after Nov'23, UNDROP is introduced and I have test it working with UC managed tables.\nhttps://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-undrop-table.html\nHowever, I don't see any official doc says UNDROP related to 'time travel'. \nSo, be aware of the above info; in exam, watch the question carefully if it is updated.","timestamp":"1709778540.0","upvote_count":"10"},{"content":"Selected Answer: C\nUNDROP can be used, within a 7 day retention period .","poster":"kishanu","upvote_count":"1","comment_id":"1558619","timestamp":"1744041240.0"},{"poster":"Er5","comment_id":"1191364","content":"C. is only correct statement. Though the table can be UNDROP in 7 days\nhttps://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/sql-ref-syntax-ddl-undrop-table\nE. Time Travel can retrieve versioned records but not tables.\nhttps://www.databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html","upvote_count":"4","timestamp":"1712551260.0"},{"comment_id":"1160126","timestamp":"1708987380.0","upvote_count":"2","comments":[{"content":"Feel this is incorrect.\nWhy?\n\nUndrop Syntax: UNDROP TABLE { table_name | WITH ID table_id }\nSo no question if Time travel","timestamp":"1734069660.0","poster":"Sriramiyer92","comment_id":"1326025","upvote_count":"1"}],"poster":"Curious76","content":"Selected Answer: E\nI think E is better answer"},{"upvote_count":"1","comment_id":"1107227","poster":"Luv4data","timestamp":"1703713500.0","content":"E. Since the table is still recoverable from transaction logs."},{"content":"Selected Answer: C\nC : AS SELECT - Managed table\nWill remove table and data","upvote_count":"2","poster":"alexvno","timestamp":"1702888140.0","comment_id":"1099536"},{"content":"Selected Answer: C\nIn Delta Lake, when a DROP TABLE command is executed, it removes both the metadata entry for the table from the catalog and the data in storage associated with that table. Workspace administrators typically have the necessary permissions to drop tables, and unless there are additional protections or retention policies in place, the data is not recoverable through normal operations after the table is dropped.","upvote_count":"3","poster":"aragorn_brego","timestamp":"1700587920.0","comment_id":"1076498"},{"content":"I meant C is correct, not D","timestamp":"1699996860.0","poster":"60ties","upvote_count":"3","comment_id":"1070856"},{"poster":"60ties","upvote_count":"1","comment_id":"1070854","content":"Selected Answer: D\nD is most correct","timestamp":"1699996740.0"},{"upvote_count":"1","content":"Answer is C as it is a managed table","poster":"Dileepvikram","timestamp":"1699489500.0","comment_id":"1066042"},{"content":"Selected Answer: C\nit is a managed table","upvote_count":"1","timestamp":"1698743880.0","poster":"lokvamsi","comment_id":"1058627"},{"content":"Selected Answer: A\nits a as it is managed table","upvote_count":"1","timestamp":"1698743820.0","comment_id":"1058623","poster":"lokvamsi"},{"timestamp":"1698155040.0","content":"Selected Answer: C\nit is a managed table. So both table def and data will be deleted","upvote_count":"1","poster":"sturcu","comment_id":"1052892"},{"comment_id":"1048746","timestamp":"1697803440.0","poster":"jyothsna12496","content":"Selected Answer: C\nDrop will usually delete the table structure and data if its managed, hence c","upvote_count":"1"}],"answer_description":"","answer_ET":"C","question_text":"A Delta Lake table was created with the below query:\n\n//IMG//\n\n\nConsider the following query:\n\n\nDROP TABLE prod.sales_by_store -\n\nIf this statement is executed by a workspace admin, which result will occur?","answer_images":[],"isMC":true,"answer":"C"},{"id":"ZEcbn80CEC5ZhO8pfEw7","url":"https://www.examtopics.com/discussions/databricks/view/124569-exam-certified-data-engineer-professional-topic-1-question/","answer":"A","isMC":true,"answer_images":[],"question_images":[],"topic":"1","answer_description":"","question_id":163,"choices":{"E":"The DBFS root stores files in ephemeral block volumes attached to the driver, while mounted directories will always persist saved data to external storage between sessions.","C":"The DBFS root is the most secure location to store data, because mounted storage volumes must have full public read and write permissions.","A":"DBFS is a file system protocol that allows users to interact with files stored in object storage using syntax and guarantees similar to Unix file systems.","D":"Neither the DBFS root nor mounted storage can be accessed when using %sh in a Databricks notebook.","B":"By default, both the DBFS root and mounted data sources are only accessible to workspace administrators."},"timestamp":"2023-10-24 16:23:00","exam_id":163,"question_text":"Two of the most common data locations on Databricks are the DBFS root storage and external object storage mounted with dbutils.fs.mount().\n\nWhich of the following statements is correct?","answers_community":["A (100%)"],"discussion":[{"poster":"Curious76","comment_id":"1160128","timestamp":"1724705160.0","content":"Selected Answer: A\nA is correct . For E, This statement is partially incorrect. The DBFS root does use ephemeral storage, but not block volumes. Data saved there is lost when the cluster terminates unless explicitly persisted elsewhere. Mounted storage, however, can persist data between sessions depending on the underlying storage service and configuration.","upvote_count":"4"},{"timestamp":"1718734260.0","content":"Selected Answer: A\nDBFS is a layer on top of cloud storage providers.","comment_id":"1100016","poster":"sodere","upvote_count":"2"},{"content":"Selected Answer: A\nDatabricks File System (DBFS) is a layer over a cloud object storage (like AWS S3, Azure Blob Storage, or GCP Cloud Storage) that allows users to interact with data as if they were using a traditional file system. It provides familiar file system semantics and is designed to be consistent with POSIX-like file system behavior, which includes commands and actions similar to those used in Unix and Linux file systems.","comment_id":"1076503","poster":"aragorn_brego","upvote_count":"3","timestamp":"1716305640.0"},{"content":"Answer is A","comment_id":"1066043","timestamp":"1715207220.0","poster":"Dileepvikram","upvote_count":"1"},{"content":"Selected Answer: A\nit is not E.\nThe only on that would be plausible is A","timestamp":"1713969060.0","poster":"sturcu","comment_id":"1052933","upvote_count":"2"}],"answer_ET":"A","unix_timestamp":1698157380},{"id":"BUMr1XndsVXERaQfY65e","choices":{"E":"%sh executes shell code on the driver node. The code does not take advantage of the worker nodes or Databricks optimized Spark.","B":"Instead of cloning, the code should use %sh pip install so that the Python code can get executed in parallel across all nodes in a cluster.","D":"Python will always execute slower than Scala on Databricks. The run.py script should be refactored to Scala.","A":"%sh triggers a cluster restart to collect and install Git. Most of the latency is related to cluster startup time.","C":"%sh does not distribute file moving operations; the final line of code should be updated to use %fs instead."},"answers_community":["E (100%)"],"timestamp":"2023-10-24 17:25:00","topic":"1","answer_ET":"E","unix_timestamp":1698161100,"answer":"E","isMC":true,"question_images":["https://img.examtopics.com/certified-data-engineer-professional/image31.png"],"answer_images":[],"question_id":164,"discussion":[{"poster":"aragorn_brego","upvote_count":"9","comment_id":"1076504","timestamp":"1700588220.0","content":"Selected Answer: E\nWhen using %sh in a Databricks notebook, the commands are executed in a shell environment on the driver node. This means that only the resources of the driver node are used, and the execution does not leverage the distributed computing capabilities of the worker nodes in the Spark cluster. This can result in slower performance, especially for data-intensive tasks, compared to an approach that distributes the workload across all nodes in the cluster using Spark."},{"content":"Selected Answer: E\nOption E correct","poster":"robodog","comment_id":"1269668","upvote_count":"1","timestamp":"1724184960.0"},{"timestamp":"1716904140.0","poster":"Freyr","comment_id":"1220305","content":"Selected Answer: E\nOption E: Correct. The %sh magic command in Databricks runs shell commands on the driver node only. This means the operations within %sh do not leverage the distributed nature of the Databricks cluster. Consequently, the Git clone, Python script execution, and file move operations are all performed on a single node (the driver), which explains why it takes a long time to process and move 1 GB of data. This approach does not utilize the parallel processing capabilities of the worker nodes or the optimization features of Databricks Spark.\n\nOption C: Incorrect. %sh does not inherently distribute any operations, but the issue here is broader than just file moving operations. Using %fs for file operations is a best practice, but it does not resolve the inefficiency of running all commands on the driver node.","upvote_count":"2"},{"content":"E is the answer as the command is ran in the driver node and other nodes in the cluster are not used","upvote_count":"2","timestamp":"1699489860.0","poster":"Dileepvikram","comment_id":"1066046"},{"poster":"sturcu","content":"Selected Answer: E\n%sh run Bash commands on the driver node of the cluster.\nhttps://www.databricks.com/blog/2020/08/31/introducing-the-databricks-web-terminal.html","upvote_count":"3","comment_id":"1057408","timestamp":"1698648420.0"},{"upvote_count":"1","timestamp":"1698161100.0","content":"you can use mv with %sh, but the syntax is not correct , it is missing the destination operand","poster":"sturcu","comments":[{"timestamp":"1698648120.0","upvote_count":"2","content":"I just noticed there is a space between the paths, so syntax is correct","poster":"sturcu","comment_id":"1057403"}],"comment_id":"1053003"}],"answer_description":"","exam_id":163,"url":"https://www.examtopics.com/discussions/databricks/view/124571-exam-certified-data-engineer-professional-topic-1-question/","question_text":"The following code has been migrated to a Databricks notebook from a legacy workload:\n\n//IMG//\n\n\nThe code executes successfully and provides the logically correct results, however, it takes over 20 minutes to extract and load around 1 GB of data.\n\nWhich statement is a possible explanation for this behavior?"},{"id":"NgcoLknyqUSwcSBBOJKU","isMC":true,"answer_ET":"A","discussion":[{"poster":"aragorn_brego","upvote_count":"7","content":"Selected Answer: A\nDelta Lake uses statistics and data skipping to improve query performance, but these optimizations are most effective for columns with low to medium cardinality (i.e., columns with a limited set of distinct values). Free-form text fields like the review column typically have high cardinality, meaning each value in the column (each review text) is unique or nearly unique. Consequently, statistics on such columns do not significantly improve the performance of queries searching for specific keywords within the text.","timestamp":"1732210740.0","comment_id":"1076505"},{"content":"Selected Answer: A\nCollecting statistics on a column containing long values such as string or binary is an expensive operation\nhttps://docs.delta.io/latest/optimizations-oss.html","upvote_count":"1","comment_id":"1538254","poster":"bp_a_user","timestamp":"1743861480.0"},{"timestamp":"1731112560.0","upvote_count":"2","content":"answer is A","comment_id":"1066047","poster":"Dileepvikram"},{"content":"Selected Answer: A\nA is correct","poster":"mouad_attaqi","upvote_count":"2","comment_id":"1054631","timestamp":"1729949880.0"},{"timestamp":"1729784340.0","poster":"sturcu","comment_id":"1053010","content":"Selected Answer: A\nCollecting statistics on long strings is an expensive operation","upvote_count":"2"}],"unix_timestamp":1698161940,"question_id":165,"url":"https://www.examtopics.com/discussions/databricks/view/124572-exam-certified-data-engineer-professional-topic-1-question/","answer":"A","answers_community":["A (100%)"],"answer_images":[],"question_images":[],"answer_description":"","topic":"1","timestamp":"2023-10-24 17:39:00","exam_id":163,"choices":{"B":"Text data cannot be stored with Delta Lake.","A":"Delta Lake statistics are not optimized for free text fields with high cardinality.","C":"ZORDER ON review will need to be run to see performance gains.","E":"Delta Lake statistics are only collected on the first 4 columns in a table.","D":"The Delta log creates a term matrix for free text fields to support selective filtering."},"question_text":"The data science team has requested assistance in accelerating queries on free form text from user reviews. The data is currently stored in Parquet with the below schema:\n\nitem_id INT, user_id INT, review_id INT, rating FLOAT, review STRING\n\nThe review column contains the full text of the review left by the user. Specifically, the data science team is looking to identify if any of 30 key words exist in this field.\n\nA junior data engineer suggests converting this data to Delta Lake will improve query performance.\n\nWhich response to the junior data engineer s suggestion is correct?"}],"exam":{"lastUpdated":"12 Apr 2025","isMCOnly":true,"name":"Certified Data Engineer Professional","id":163,"provider":"Databricks","numberOfQuestions":200,"isBeta":false,"isImplemented":true},"currentPage":33},"__N_SSP":true}