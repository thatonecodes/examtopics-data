{"pageProps":{"questions":[{"id":"OYJFSCV0O9Cc6OvRp1Ct","unix_timestamp":1710396060,"answer_ET":"B","topic":"1","isMC":true,"exam_id":163,"question_text":"Each configuration below is identical to the extent that each cluster has 400 GB total of RAM 160 total cores and only one Executor per VM.\n\nGiven an extremely long-running job for which completion must be guaranteed, which cluster configuration will be able to guarantee completion of the job in light of one or more VM failures?","url":"https://www.examtopics.com/discussions/databricks/view/135995-exam-certified-data-engineer-professional-topic-1-question/","answer_images":[],"answer_description":"","discussion":[{"timestamp":"1738261440.0","content":"Selected Answer: B\nTotal VMs = 16\n\nResources per VM:\n25 GB RAM and 10 cores per executor.\nImpact of a VM Failure:\nLosing one VM means losing only 6.25% of the cluster’s resources.\nFault Tolerance:\nExcellent fault tolerance. The cluster can handle multiple VM failures (up to ~3 VMs) and still function effectively.\nBest Balance:\nWith smaller VMs, the job remains highly fault-tolerant while using resources efficiently.","poster":"91d511b","comment_id":"1349180","upvote_count":"1"},{"timestamp":"1726851780.0","content":"16 core provides more redundancy, fault tolerance and more parallelism. But if dataset is huge, 8VM maybe better. The question is missing some information.","poster":"shaojunni","upvote_count":"2","comment_id":"1286992"},{"upvote_count":"2","timestamp":"1720447320.0","poster":"c00ccb7","comment_id":"1244363","content":"Selected Answer: B\nThis setup ensures that the job can continue running and complete even if some VMs fail, as there are more VMs available to handle the workload"},{"upvote_count":"3","timestamp":"1716557040.0","poster":"ChayV","comment_id":"1217499","content":"Selected Answer: B\nIf VM is down, performance is degraded, so opting for vm's which has distributed memory per executor and optimal cores per executor."},{"content":"Selected Answer: B\nin my exam today, i chose B, 16VM, because the \"extremely long-run\".","timestamp":"1710396060.0","poster":"hal2401me","comment_id":"1173156","upvote_count":"4","comments":[{"comment_id":"1200045","upvote_count":"1","comments":[{"upvote_count":"1","timestamp":"1723707180.0","comments":[{"poster":"arekm","content":"So long as the data partition fits into a smaller VM. But we don't have that information. From the perspective of failures of multiple machines, the move of them the better :)","upvote_count":"1","timestamp":"1735812780.0","comment_id":"1335486"}],"comment_id":"1266289","content":"I have no link for Databricks doc. It's just a logic. The more VMs we have, the more robust our pipeline is.","poster":"practicioner"}],"content":"do you have link to databricks doc?","poster":"ThoBustos","timestamp":"1713774360.0"}]}],"question_id":16,"choices":{"E":"• Total VMs: 2\n• 200 GB per Executor\n• 80 Cores / Executor","D":"• Total VMs: 4\n• 100 GB per Executor\n• 40 Cores / Executor","A":"• Total VMs: 8\n• 50 GB per Executor\n• 20 Cores / Executor","B":"• Total VMs: 16\n• 25 GB per Executor\n• 10 Cores / Executor","C":"• Total VMs: 1\n• 400 GB per Executor\n• 160 Cores/Executor"},"answer":"B","timestamp":"2024-03-14 07:01:00","question_images":[],"answers_community":["B (100%)"]},{"id":"sW5DCS0Awk0HAQFbWz5s","question_images":[],"answers_community":["A (80%)","E (20%)"],"question_text":"A Delta Lake table in the Lakehouse named customer_churn_params is used in churn prediction by the machine learning team. The table contains information about customers derived from a number of upstream sources. Currently, the data engineering team populates this table nightly by overwriting the table with the current valid values derived from upstream data sources.\n\nImmediately after each update succeeds, the data engineering team would like to determine the difference between the new version and the previous version of the table.\n\nGiven the current implementation, which method can be used?","isMC":true,"timestamp":"2024-05-29 17:53:00","exam_id":163,"answer_ET":"A","choices":{"A":"Execute a query to calculate the difference between the new version and the previous version using Delta Lake’s built-in versioning and lime travel functionality.","B":"Parse the Delta Lake transaction log to identify all newly written data files.","C":"Parse the Spark event logs to identify those rows that were updated, inserted, or deleted.","D":"Execute DESCRIBE HISTORY customer_churn_params to obtain the full operation metrics for the update, including a log of all records that have been added or modified.","E":"Use Delta Lake’s change data feed to identify those records that have been updated, inserted, or deleted."},"unix_timestamp":1716997980,"answer":"A","question_id":17,"answer_description":"","discussion":[{"upvote_count":"1","poster":"arekm","timestamp":"1735813020.0","content":"Selected Answer: A\nD - see the discussion under Jugiboss comment.","comment_id":"1335492"},{"comment_id":"1326376","content":"Selected Answer: A\nCDF is particularly useful in Incremental loads. In our case it is overwrite. \nHence A.","timestamp":"1734160260.0","upvote_count":"2","poster":"Sriramiyer92"},{"poster":"Jugiboss","timestamp":"1729757040.0","comment_id":"1302361","comments":[{"timestamp":"1735812960.0","poster":"arekm","content":"It is the best method so long as it is enabled, and we don't know. Moreover, CDF is designed for a small portion of the data being changed, not a full overwrite.","upvote_count":"1","comment_id":"1335490"},{"timestamp":"1729757160.0","poster":"Jugiboss","content":"Nevermind, it's A","comment_id":"1302362","upvote_count":"2"}],"content":"Selected Answer: E\nhe best method to determine the difference between the new version and the previous version of the customer_churn_params table in Delta Lake is:\n\nE. Use Delta Lake’s change data feed to identify those records that have been updated, inserted, or deleted.\n\nThis approach leverages Delta Lake's built-in functionality to track changes at the record level, providing a clear view of what has changed between versions.","upvote_count":"1"},{"content":"Selected Answer: E\nChange data feed allows to check for changes between versions","poster":"cales","comments":[{"poster":"benni_ale","content":"There is no info about cdf being enabled on the table","timestamp":"1730212080.0","comment_id":"1304492","upvote_count":"1"}],"comment_id":"1296880","timestamp":"1728821160.0","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: A\nAnswer is A. The easy way to get the difference between those tables is by travel time by version","comment_id":"1270611","timestamp":"1724320920.0","poster":"robodog"},{"poster":"HelixAbdu","upvote_count":"2","timestamp":"1722073860.0","comment_id":"1256245","content":"Answer is A. There is no clue that CDF is enabled for the table"},{"content":"Selected Answer: A\nAnswer A","poster":"c00ccb7","upvote_count":"3","timestamp":"1720450560.0","comment_id":"1244424"},{"upvote_count":"2","poster":"Deb9753","comment_id":"1224722","timestamp":"1717591800.0","content":"Answer : E"}],"answer_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/141545-exam-certified-data-engineer-professional-topic-1-question/","topic":"1"},{"id":"ld7n3IalZrV7ZqbOYmBm","url":"https://www.examtopics.com/discussions/databricks/view/141546-exam-certified-data-engineer-professional-topic-1-question/","answer":"A","topic":"1","unix_timestamp":1716998100,"exam_id":163,"answers_community":["A (100%)"],"question_id":18,"answer_description":"","question_text":"A data team’s Structured Streaming job is configured to calculate running aggregates for item sales to update a downstream marketing dashboard. The marketing team has introduced a new promotion, and they would like to add a new field to track the number of times this promotion code is used for each item. A junior data engineer suggests updating the existing query as follows. Note that proposed changes are in bold.\n\nOriginal query:\n\n//IMG//\n\n\nProposed query:\n\n//IMG//\n\n\nWhich step must also be completed to put the proposed query into production?","question_images":["https://img.examtopics.com/certified-data-engineer-professional/image49.png","https://img.examtopics.com/certified-data-engineer-professional/image50.png"],"answer_images":[],"choices":{"B":"Remove .option('mergeSchema', 'true') from the streaming write","C":"Increase the shuffle partitions to account for additional aggregates","D":"Run REFRESH TABLE delta.‛/item_agg‛","A":"Specify a new checkpointLocation"},"answer_ET":"A","isMC":true,"discussion":[{"comment_id":"1224724","content":"Answer: A\nWhen updating the schema of a streaming job, specifying a new checkpoint location ensures that the streaming query starts fresh with the new schema. This avoids issues that might arise from schema mismatches between the previous state and the new schema. This is especially relevant when adding new fields because the existing state might not be compatible with the new schema.","upvote_count":"5","poster":"Deb9753","timestamp":"1717592040.0"},{"comment_id":"1342070","upvote_count":"1","poster":"SeRo42","timestamp":"1737105720.0","content":"Selected Answer: A\nAre filter conditions in count() allowed?"},{"upvote_count":"1","timestamp":"1724321100.0","poster":"robodog","content":"Selected Answer: A\nA answer","comment_id":"1270614"},{"poster":"MDWPartners","timestamp":"1716998100.0","upvote_count":"2","content":"Selected Answer: A\nThis checkpoint location preserves all of the essential information that identifies a query. Each query must have a different checkpoint location. Multiple queries should never have the same location. For more information, see the Structured Streaming Programming Guide. https://docs.databricks.com/en/structured-streaming/query-recovery.html","comment_id":"1221093"}],"timestamp":"2024-05-29 17:55:00"},{"id":"fZMUiuaH8CJZW9rLweqT","timestamp":"2024-05-29 17:57:00","isMC":true,"answers_community":["E (65%)","D (25%)","10%"],"question_text":"When using CLI or REST API to get results from jobs with multiple tasks, which statement correctly describes the response structure?","answer_images":[],"question_id":19,"answer":"E","answer_ET":"E","topic":"1","question_images":[],"answer_description":"","unix_timestamp":1716998220,"choices":{"B":"Each run of a job will have a unique job_id; all tasks within this job will have a unique task_id","C":"Each run of a job will have a unique orchestration_id; all tasks within this job will have a unique run_id","D":"Each run of a job will have a unique run_id; all tasks within this job will have a unique task_id","A":"Each run of a job will have a unique job_id; all tasks within this job will have a unique job_id","E":"Each run of a job will have a unique run_id; all tasks within this job will also have a unique run_id"},"exam_id":163,"url":"https://www.examtopics.com/discussions/databricks/view/141547-exam-certified-data-engineer-professional-topic-1-question/","discussion":[{"poster":"cales","timestamp":"1728822000.0","comment_id":"1296897","content":"Selected Answer: E\nThe correct answer is E. https://docs.databricks.com/api/workspace/jobs/getrun\nYou can visit the link and observe the response sample of the API\n{\n \"job_id\": 11223344,\n \"run_id\": 455644833,\n\njob_id is the unique id of the job. run_id is the unique id of the run\n\nThen each task will have its unique run id: \n\n\"tasks\": [\n {\n ...\n \"run_id\": 2112892,\n ...","upvote_count":"6"},{"comment_id":"1373422","upvote_count":"1","poster":"mohadjhamad","content":"Selected Answer: D\nEach job execution (run) gets a unique run_id\n\nThis run_id identifies a specific instance of a job run.\nIt allows tracking of job execution details, logs, and results.\nEach task within that job run has a unique task_id\n\nMulti-task jobs have multiple tasks, each assigned a distinct task_id.\nThe task_id helps in monitoring and retrieving individual task details.","timestamp":"1741554360.0"},{"upvote_count":"1","comment_id":"1335497","content":"Selected Answer: E\nE - but between D & E is a play on the words. Still E seems a tiny bit more explicit. The bottom line the run_id attribute of each task within the job will be different.","timestamp":"1735813380.0","poster":"arekm"},{"timestamp":"1735259940.0","content":"Selected Answer: E\nAnswer E. Job Run (run_id): Each execution of a job has a unique run_id for the entire job.\nTask Run (run_id): For jobs with multiple tasks, each task also gets its own run_id, which is distinct from the job's run_id. This run_id for tasks can be used to retrieve individual task outputs. \n\nhttps://docs.databricks.com/api/workspace/jobs/getrun\n\n\"tasks: The list of tasks performed by the run. Each task has its own run_id which you can use to call JobsGetOutput to retrieve the run results. \"","comment_id":"1332157","upvote_count":"2","poster":"AlejandroU"},{"upvote_count":"2","content":"Selected Answer: B\nB if the question is about /api/2.1/jobs/get\nD if the question /api/2.1/jobs/runs/get","timestamp":"1732967580.0","poster":"Thameur01","comment_id":"1320180"},{"upvote_count":"1","timestamp":"1732967400.0","comment_id":"1320178","content":"Selected Answer: D\nThe question should clearly specify, get jobs or get job run, there differences in response","poster":"Thameur01"},{"poster":"cf56faf","upvote_count":"1","timestamp":"1731327660.0","content":"Selected Answer: D\nMust be D.","comment_id":"1310020"},{"timestamp":"1730212500.0","content":"Selected Answer: E\nE","upvote_count":"2","comment_id":"1304496","poster":"benni_ale"},{"timestamp":"1729060860.0","poster":"Kreshu","upvote_count":"2","content":"Selected Answer: E\nCorrect answer is E","comment_id":"1298591"},{"poster":"thelio_team","upvote_count":"3","timestamp":"1727552100.0","comment_id":"1290757","content":"tested answer is E : \n'tasks': [{'run_id': *****,\n 'task_key': '######', ...."},{"poster":"MDWPartners","timestamp":"1716998220.0","comment_id":"1221096","upvote_count":"2","content":"Selected Answer: D\nSeems right"}]},{"id":"XwirdpOBHjcFO9IfAUM2","discussion":[{"poster":"MDWPartners","comment_id":"1221097","content":"Selected Answer: B\nSeems right","upvote_count":"5","timestamp":"1716998280.0"},{"upvote_count":"1","timestamp":"1736773140.0","content":"Selected Answer: B\nShallow clones of prod data can be created in dev for testing purposes","poster":"_lene_","comment_id":"1339888"},{"content":"Selected Answer: B\nThe correct answer is B. In environments where interactive code will be executed, production data should only be accessible with read permissions; creating isolated databases for each environment further reduces risks.\n\nExplanation:\nBest practices for managing production, development, and testing environments involve minimizing the risk of unintended data modifications or deletions, especially when dealing with production data. The ideal setup includes:\n\nLimiting permissions: Production data should only be accessible with read permissions in development or testing environments to prevent accidental changes.\nIsolating environments: Creating separate databases for development, testing, and production environments ensures that there are clear boundaries and that development code cannot unintentionally affect production data.","upvote_count":"1","comment_id":"1297915","poster":"Colje","timestamp":"1728959520.0"}],"answer_ET":"B","isMC":true,"answer_images":[],"unix_timestamp":1716998280,"exam_id":163,"url":"https://www.examtopics.com/discussions/databricks/view/141548-exam-certified-data-engineer-professional-topic-1-question/","answer":"B","answers_community":["B (100%)"],"answer_description":"","timestamp":"2024-05-29 17:58:00","topic":"1","choices":{"A":"All development, testing, and production code and data should exist in a single, unified workspace; creating separate environments for testing and development complicates administrative overhead.","B":"In environments where interactive code will be executed, production data should only be accessible with read permissions; creating isolated databases for each environment further reduces risks.","D":"Because Delta Lake versions all data and supports time travel, it is not possible for user error or malicious actors to permanently delete production data; as such, it is generally safe to mount production data anywhere.","C":"As long as code in the development environment declares USE dev_db at the top of each notebook, there is no possibility of inadvertently committing changes back to production data sources.","E":"Because access to production data will always be verified using passthrough credentials, it is safe to mount data to any Databricks development environment."},"question_text":"The data engineering team is configuring environments for development, testing, and production before beginning migration on a new data pipeline. The team requires extensive testing on both the code and data resulting from code execution, and the team wants to develop and test against data as similar to production data as possible.\n\nA junior data engineer suggests that production data can be mounted to the development and testing environments, allowing pre-production code to execute against production data. Because all users have admin privileges in the development environment, the junior data engineer has offered to configure permissions and mount this data for the team.\n\nWhich statement captures best practices for this situation?","question_images":[],"question_id":20}],"exam":{"isImplemented":true,"provider":"Databricks","isBeta":false,"name":"Certified Data Engineer Professional","lastUpdated":"12 Apr 2025","isMCOnly":true,"numberOfQuestions":200,"id":163},"currentPage":4},"__N_SSP":true}