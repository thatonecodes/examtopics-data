{"pageProps":{"questions":[{"id":"XdRVe6TSEpiWWeUUMaMp","timestamp":"2023-04-26 08:29:00","answer_ET":"D","exam_id":161,"answer_description":"","topic":"1","question_text":"The code block shown below contains an error. The code block is intended to return a DataFrame containing all columns from DataFrame storesDF except for column sqft and column customerSatisfaction. Identify the error.\nCode block:\nstoresDF.drop(sqft, customerSatisfaction)","choices":{"E":"The sqft and customerSatisfaction column names should be subset from the DataFrame storesDF like storesDF.\"sqft\" and storesDF.\"customerSatisfaction\".","A":"The drop() operation only works if one column name is called at a time â€“ there should be two calls in succession like storesDF.drop(\"sqft\").drop(\"customerSatisfaction\").","D":"The sqft and customerSatisfaction column names should be quoted like \"sqft\" and \"customerSatisfaction\".","C":"There is no drop() operation for storesDF.","B":"The drop() operation only works if column names are wrapped inside the col() function like storesDF.drop(col(sqft), col(customerSatisfaction))."},"answers_community":["D (100%)"],"isMC":true,"question_images":[],"unix_timestamp":1682490540,"answer":"D","question_id":96,"url":"https://www.examtopics.com/discussions/databricks/view/107535-exam-certified-associate-developer-for-apache-spark-topic-1/","discussion":[{"comment_id":"881179","timestamp":"1698301740.0","content":"Selected Answer: D\nThe error in the code block is that the column names sqft and customerSatisfaction should be quoted, like \"sqft\" and \"customerSatisfaction\", since they are strings. The correct code block should be:\n\nstoresDF.drop(\"sqft\", \"customerSatisfaction\")\n\nOption D correctly identifies this error.","upvote_count":"5","poster":"4be8126","comments":[{"timestamp":"1701976860.0","content":"The correct one is B:\nstoresDF.drop(\"sqft\").drop(\"customerSatisfaction\")\nFor D, it should be list of column name: storesDF.drop([\"sqft\", \"customerSatisfaction\"])","comments":[{"upvote_count":"1","timestamp":"1701977040.0","comment_id":"917442","content":"The correct one is D, but my explanation is correct","poster":"ZSun"}],"upvote_count":"1","comment_id":"917440","poster":"ZSun"}]},{"content":"sorry, Option D is correct","upvote_count":"1","timestamp":"1725648060.0","comment_id":"1167485","poster":"azurearch"},{"poster":"azurearch","content":"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.drop.html option A is correct, drop expects only one argument, if its more than one, you would have to use as listofcols=['col1','col2'] and drop(*listofcols)","timestamp":"1725647220.0","comment_id":"1167477","upvote_count":"1"},{"timestamp":"1706628180.0","content":"D is correct, \ndf.drop('id','firstname').show() tested code","upvote_count":"1","comment_id":"967151","poster":"zozoshanky"},{"timestamp":"1702832760.0","upvote_count":"1","comment_id":"926073","content":"Selected Answer: D\nWhen using the drop() operation in Spark DataFrame, the column names should be specified as strings and enclosed in quotes. In the given code block, the column names \"sqft\" and \"customerSatisfaction\" are not quoted, which results in a syntax error.","poster":"TmData"}],"answer_images":[]},{"id":"hpdvPRvxZNZp8qBlbA51","isMC":true,"url":"https://www.examtopics.com/discussions/databricks/view/154891-exam-certified-associate-developer-for-apache-spark-topic-1/","answer_images":[],"answer":"D","answer_ET":"D","choices":{"E":"storesDF.withColumn(\"sqftMean\", mean(col(\"sqft\")))","B":"storesDF.agg(col(\"sqft\").mean().alias(\"sqftMean\"))","C":"storesDF.agg(mean(\"sqft\").alias(\"sqftMean\"))","A":"storesDF.withColumn(mean(col(\"sqft\")).alias(\"sqftMean\"))","D":"storesDF.agg(mean(col(\"sqft\")).alias(\"sqftMean\"))"},"discussion":[{"content":"Selected Answer: D\ntested it: works","timestamp":"1737283800.0","poster":"bp_a_user","upvote_count":"1","comment_id":"1342962"}],"exam_id":161,"answers_community":["D (100%)"],"question_id":97,"topic":"1","question_text":"Which of the following code blocks returns a new DataFrame with the mean of column sqft from DataFrame storesDF in column sqftMean?","question_images":[],"timestamp":"2025-01-19 11:50:00","answer_description":"","unix_timestamp":1737283800},{"id":"xBAqYK6qyaMKYki1LhGz","url":"https://www.examtopics.com/discussions/databricks/view/154893-exam-certified-associate-developer-for-apache-spark-topic-1/","answer":"D","answer_description":"","question_id":98,"topic":"1","unix_timestamp":1737283980,"answer_images":[],"discussion":[{"poster":"bp_a_user","upvote_count":"1","comment_id":"1342965","comments":[{"timestamp":"1737283980.0","comments":[{"poster":"bp_a_user","content":"sorry, C is true (asking for incorrect answer)","timestamp":"1737284040.0","comment_id":"1342967","upvote_count":"1"}],"upvote_count":"1","poster":"bp_a_user","comment_id":"1342966","content":"df1.orderBy(\"name\",ascending=True).display()"}],"timestamp":"1737283980.0","content":"Selected Answer: D\nit is D!\nascending is the correct order and is true by default\n\nTested it\ndf1.orderBy(\"name\",ascending=False).display()"}],"timestamp":"2025-01-19 11:53:00","question_images":[],"question_text":"Which of the following code blocks fails to return a DataFrame sorted alphabetically based on column division?","choices":{"A":"storesDF.sort(asc(\"division\"))","C":"storesDF.orderBy(col(\"division\").desc())","E":"storesDF.sort(\"division\")","D":"storesDF.orderBy(\"division\")","B":"storesDF.orderBy([\"division\"], ascending = [1])"},"answer_ET":"C","exam_id":161,"isMC":true,"answers_community":["D (100%)"]},{"id":"heO1m5YRNT9aarqkXJRG","question_id":99,"answer_images":[],"unix_timestamp":1680530460,"question_text":"Which of the following describes the relationship between nodes and executors?","question_images":[],"discussion":[{"timestamp":"1696341660.0","upvote_count":"7","poster":"TC007","comment_id":"859983","content":"Selected Answer: C\nIn a Spark cluster, each node typically has multiple executors, which are responsible for executing tasks on that node. An executor is a separate process that runs on a node and is responsible for executing tasks assigned to it by the driver. Therefore, a node can have multiple executors running on it. The number of executors on a node depends on the resources available on that node and the configuration settings of the Spark application. So, option C is the correct answer."},{"content":"Selected Answer: C\nExecutors are the processing engines that run on nodes to execute tasks in a Spark application.","comment_id":"1454777","upvote_count":"1","timestamp":"1743753360.0","poster":"Saurabh_221b"},{"comment_id":"1217985","upvote_count":"3","poster":"SnData","content":"Executor is a process inside a node. Hence answer is C","timestamp":"1732516620.0"},{"comment_id":"1171561","poster":"oda_rasheed_basha","content":"A) is C. nodes are the physical machines in the Spark cluster, while executors are the worker processes running on these nodes, responsible for executing tasks and processing data as part of a Spark application.","upvote_count":"3","timestamp":"1726124940.0"},{"poster":"Anweee","upvote_count":"1","comment_id":"1139825","timestamp":"1722743580.0","content":"this is so wrong, it is option C"},{"content":"It is C, this solution is so wrong.\nExecutors are the processes running on the Nodes/Workers/Machines.","timestamp":"1722691800.0","upvote_count":"1","comment_id":"1139368","poster":"Anweee"},{"content":"Selected Answer: C\nC is correct","comment_id":"1130619","timestamp":"1721818680.0","upvote_count":"2","poster":"amirshaz"},{"poster":"mehroosali","comment_id":"1057019","content":"Selected Answer: C\nC is correct.","timestamp":"1714403940.0","upvote_count":"1"},{"poster":"GeorgiT","upvote_count":"1","comment_id":"1012011","timestamp":"1710928560.0","content":"The correct answer is C. The other (currently selected as correct )answer D is also wrong the total number of nodes is always larger than the executor nodes by 1 sins we need one of the Cluster nodes to run the Driver."},{"upvote_count":"1","timestamp":"1706976300.0","poster":"astone42","comment_id":"971174","content":"Selected Answer: C\nIt's C"},{"upvote_count":"1","poster":"Dgohel","timestamp":"1706828880.0","content":"Which one to accept as an answer community answer or a Suggested answer from exam topics? Are the examtopics answers trustworthy?","comment_id":"969419"},{"content":"C. Spark executors run on worker nodes and are responsible for executing tasks and storing intermediate data during data processing. The nodes represent the physical machines that provide computing resources to run the Spark application.","upvote_count":"1","comment_id":"967837","poster":"singh100","timestamp":"1706691540.0"},{"poster":"Sandy544","comment_id":"937373","upvote_count":"1","content":"Selected Answer: C\nAn executor is a process that is launched for a Spark application on a worker node.","timestamp":"1703822820.0"},{"poster":"TmData","content":"Selected Answer: C\nThe correct answer is C. An executor is a processing engine running on a node.\n\nExplanation: In Apache Spark, a node refers to a physical or virtual machine in a cluster that is part of the Spark cluster. Each node can have one or more executors running on it. An executor is a Spark component responsible for executing tasks and storing data in memory or on disk. It is a worker process that runs on a node and performs the actual computation and data processing tasks assigned to it by the driver program. Executors are created and managed by the cluster manager, and they are responsible for executing the tasks and managing the data partitions assigned to them.","comment_id":"926045","upvote_count":"1","timestamp":"1702831620.0"},{"poster":"evertonllins","comment_id":"871365","upvote_count":"2","content":"Selected Answer: C\nThe correct answer is C. The executor runs in a node.","timestamp":"1697411400.0"}],"answer_ET":"C","timestamp":"2023-04-03 16:01:00","exam_id":161,"answer":"C","url":"https://www.examtopics.com/discussions/databricks/view/104977-exam-certified-associate-developer-for-apache-spark-topic-1/","isMC":true,"answer_description":"","topic":"1","choices":{"E":"There are always more nodes than executors.","D":"There are always the same number of executors and nodes.","C":"An executor is a processing engine running on a node.","A":"Executors and nodes are not related.","B":"Anode is a processing engine running on an executor."},"answers_community":["C (100%)"]},{"id":"nNP3FztsZN7iuHVsHsot","isMC":true,"url":"https://www.examtopics.com/discussions/databricks/view/107537-exam-certified-associate-developer-for-apache-spark-topic-1/","answer_images":[],"answer_ET":"E","answer":"E","choices":{"A":"storesDF.filter(col(\"sqft\") <= 25000 | col(\"customerSatisfaction\") >= 30)","E":"storesDF.filter((col(\"sqft\") <= 25000) | (col(\"customerSatisfaction\") >= 30))","D":"storesDF.filter(col(sqft) <= 25000 | col(customerSatisfaction) >= 30)","C":"storesDF.filter(sqft <= 25000 or customerSatisfaction >= 30)","B":"storesDF.filter(col(\"sqft\") <= 25000 or col(\"customerSatisfaction\") >= 30)"},"discussion":[{"timestamp":"1721740620.0","upvote_count":"2","comment_id":"1253674","poster":"jds0","content":"Selected Answer: E\nAnswer: E\nI tried it with the code below, all other options raised an error:\n\n# register UDF with udf function\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import IntegerType\n\nspark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n\ndata = [\n (0, 3, 20000, \"A\"),\n (1, 1, 50000, \"A\"),\n (2, 2, 70000, \"A\"),\n (3, 5, 10000, \"B\"),\n (4, 4, 100000, \"B\"),\n ]\nstoresDF = spark.createDataFrame(data, [\"storeID\", \"customerSatisfaction\", \"sqft\", \"division\"])\n\ntry:\n storesDF.filter(col(\"sqft\") <= 25000 | col(\"customerSatisfaction\") >= 30).show()\nexcept Exception as e:\n print(e)\n\ntry:\n storesDF.filter((col(\"sqft\") <= 25000) | (col(\"customerSatisfaction\") >= 30)).show()\nexcept Exception as e:\n print(e)"},{"timestamp":"1687014540.0","poster":"TmData","upvote_count":"4","comment_id":"926082","content":"Selected Answer: E\nOption E, storesDF.filter((col(\"sqft\") <= 25000) | (col(\"customerSatisfaction\") >= 30)), is the correct option. It uses the filter() operation with the conditions (col(\"sqft\") <= 25000) | (col(\"customerSatisfaction\") >= 30) to filter the rows where the value in column sqft is less than or equal to 25,000 OR the value in column customerSatisfaction is greater than or equal to 30."},{"content":"Selected Answer: E\nE has the right syntax, logic, operator and correct number of parentheses. All of the others falter in one of these respects.","upvote_count":"2","comment_id":"898082","timestamp":"1684132020.0","poster":"SonicBoom10C9"},{"timestamp":"1682512260.0","poster":"pierre_grns","comment_id":"881610","comments":[{"poster":"pierre_grns","content":"sorry, we need 2 paranthesis indeed. So E !","comments":[{"timestamp":"1693054320.0","upvote_count":"5","comment_id":"990747","content":"Congrats man, not everyone goes back to tell they were wrong and corrects them selves. We need more people like this on this platform","poster":"evertonllins"},{"content":"Yes I agree, it's E","upvote_count":"2","comment_id":"888266","poster":"sly75","timestamp":"1683099420.0"}],"comment_id":"881615","timestamp":"1682512440.0","upvote_count":"6"}],"content":"Selected Answer: A\nShould be A. Tested it in communitity edition with 2 filters.","upvote_count":"1"},{"comment_id":"881186","poster":"4be8126","upvote_count":"2","timestamp":"1682491080.0","content":"The correct code block to return a DataFrame containing only the rows from DataFrame storesDF where the value in column sqft is less than or equal to 25,000 OR the value in column customerSatisfaction is greater than or equal to 30 is:\n\nstoresDF.filter((col(\"sqft\") <= 25000) | (col(\"customerSatisfaction\") >= 30))\nOption A uses a single pipe (|) instead of the correct syntax of two vertical bars (||) to represent \"OR\" logic, and also uses the wrong syntax for column referencing.\n\nOption B uses the correct or operator, but also uses the wrong syntax for column referencing.\n\nOption C uses the correct operator and syntax for column referencing, but does not use the col() function to reference column names.\n\nOption D uses the col() function, but also uses the wrong syntax for column referencing.\n\nOption E uses the correct syntax for both column referencing and logical operator, and correctly specifies the parentheses to ensure the proper order of operations.\n\nTherefore, the correct answer is E.\n\nstoresDF.filter((col(\"sqft\") <= 25000) | (col(\"customerSatisfaction\") >= 30))"}],"exam_id":161,"answers_community":["E (89%)","11%"],"question_id":100,"topic":"1","question_text":"Which of the following code blocks returns a DataFrame containing only the rows from DataFrame storesDF where the value in column sqft is less than or equal to 25,000 OR the value in column customerSatisfaction is greater than or equal to 30?","question_images":[],"answer_description":"","timestamp":"2023-04-26 08:38:00","unix_timestamp":1682491080}],"exam":{"lastUpdated":"12 Apr 2025","isBeta":false,"isImplemented":true,"numberOfQuestions":185,"name":"Certified Associate Developer for Apache Spark","provider":"Databricks","isMCOnly":true,"id":161},"currentPage":20},"__N_SSP":true}