{"pageProps":{"questions":[{"id":"NeWt0UFwUNmVZuSfv1NT","exam_id":162,"question_text":"A data engineer needs to apply custom logic to string column city in table stores for a specific use case. In order to apply this custom logic at scale, the data engineer wants to create a SQL user-defined function (UDF).\nWhich of the following code blocks creates this SQL UDF?","question_images":[],"discussion":[{"upvote_count":"14","comment_id":"856949","poster":"Flexron","timestamp":"1680260700.0","content":"E is wrong, the right answer is A.\nhttps://www.databricks.com/blog/2021/10/20/introducing-sql-user-defined-functions.html"},{"timestamp":"1680363540.0","comments":[{"upvote_count":"5","content":"Both A and D use RETURN CASE","comment_id":"900229","timestamp":"1684332180.0","poster":"Redwings538"}],"content":"Selected Answer: A\nThe answer E is incorrect. A user defined function is never written as CREATE UDF. The correct way is CREATE FUNCTION. So that leaves us with the choices A and D. Out of that, in D, there is no such thing as RETURN CASE so the correct answer is A.","upvote_count":"8","comment_id":"858060","poster":"XiltroX"},{"content":"A and D","upvote_count":"1","timestamp":"1725526740.0","comment_id":"1278795","poster":"Ani_Ni"},{"content":"Selected Answer: A\nA is the correct syntax for creating a UDF in Databricks.","upvote_count":"1","timestamp":"1720467780.0","comment_id":"1244541","poster":"3fbc31b"},{"content":"Selected Answer: A\nA is correct","upvote_count":"1","timestamp":"1714230600.0","comment_id":"1203175","poster":"benni_ale"},{"timestamp":"1710589980.0","content":"Selected Answer: A\nhttps://docs.databricks.com/en/udf/index.html#language-sql\n\nAnswer E is not correct when having CREATE UDF rather than CREATE FUNCTION","comment_id":"1174941","upvote_count":"1","poster":"a_51"},{"timestamp":"1707589560.0","content":"Correct Answer is A","comment_id":"1146571","upvote_count":"1","poster":"isamrat28"},{"timestamp":"1706766180.0","upvote_count":"3","comment_id":"1137306","content":"D, should be the answer. The fucntion is not returning STRING , it is applied on a string column","poster":"agAshish"},{"upvote_count":"1","poster":"SerGrey","timestamp":"1704322740.0","comment_id":"1113198","content":"Selected Answer: A\nCorrect answer is A"},{"content":"Correct answer is A. It is not E. First, you do not need to specify \"UDF\" in the syntax. You need to specify the return type and then what you want to return, usually your processed output. \nCREATE FUNCTION myUDF(udf STRING) \nRETURNS STRING \n<...udf....>","poster":"Huroye","timestamp":"1700023860.0","upvote_count":"3","comment_id":"1071094"},{"timestamp":"1697439240.0","comment_id":"1044721","poster":"VijayKula","upvote_count":"1","content":"Selected Answer: A\nCreate Function Return String"},{"upvote_count":"1","comment_id":"1028815","timestamp":"1696850580.0","content":"Selected Answer: A\nCorrect is A","poster":"VijayKula"},{"poster":"DavidRou","upvote_count":"2","content":"A is the right answer.\nThe template to use is the following:\nCREATE FUNCTION <name_function> (<function_parameter>, ..) RETURNS <return_type>\n<body>","comment_id":"1028520","timestamp":"1696828800.0"},{"comment_id":"1019629","timestamp":"1695891120.0","upvote_count":"1","content":"First need create a function and then need to register it as UDF","poster":"AtanuChat"},{"content":"Selected Answer: A\nA is correct","comment_id":"1017355","timestamp":"1695698160.0","poster":"KalavathiP","upvote_count":"1"},{"comment_id":"1016565","timestamp":"1695626160.0","upvote_count":"1","poster":"d_b47","content":"Selected Answer: A\nno UDF only FUNCTION needed."},{"content":"CORRECT ANSWER IS : A \nhttps://www.databricks.com/blog/2021/10/20/introducing-sql-user-defined-functions.html","timestamp":"1689729000.0","comment_id":"956081","poster":"akk_1289","upvote_count":"1"},{"timestamp":"1688764200.0","comment_id":"946002","upvote_count":"1","poster":"Atnafu","content":"A\n-- Create the UDF\nCREATE FUNCTION custom_logic(city STRING) RETURNS STRING\nBEGIN\n -- Custom logic goes here\n -- Example: Return the uppercase version of the city\n RETURN UPPER(city);\nEND;","comments":[{"upvote_count":"1","comment_id":"946005","content":"from datetime import datetime\ncurrent_day = datetime.now().strftime('%A')\nif current_day == 'Sunday':\n # Run the final query\n spark.sql(\"SELECT ... FROM ... WHERE ...\")\nelse:\n # Handle other scenarios or skip the final query\n pass","timestamp":"1688764560.0","poster":"Atnafu"}]},{"content":"D also works","timestamp":"1682515380.0","upvote_count":"1","comment_id":"881653","poster":"malani"},{"timestamp":"1682054280.0","upvote_count":"1","poster":"Varma_Saraswathula","content":"Ans - A\nhttps://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-sql-function.html","comment_id":"876214"},{"content":"Option A","poster":"naxacod574","upvote_count":"1","comment_id":"875877","timestamp":"1682014440.0"},{"content":"option A","timestamp":"1680583860.0","upvote_count":"1","poster":"sdas1","comment_id":"860639"},{"timestamp":"1680564120.0","content":"Selected Answer: A\nA es la correcta","upvote_count":"1","poster":"knivesz","comment_id":"860437"},{"comment_id":"859670","timestamp":"1680509580.0","content":"Selected Answer: A\noption A","upvote_count":"2","poster":"surrabhi_4"},{"timestamp":"1680453240.0","comment_id":"859070","content":"A is correct","poster":"rafahb","upvote_count":"1"}],"url":"https://www.examtopics.com/discussions/databricks/view/104636-exam-certified-data-engineer-associate-topic-1-question-17/","answers_community":["A (100%)"],"unix_timestamp":1680260700,"topic":"1","timestamp":"2023-03-31 13:05:00","choices":{"D":"","B":"","E":"","C":"","A":""},"answer_images":[],"question_id":76,"isMC":true,"answer_description":"","answer":"A","answer_ET":"A"},{"id":"ScPl49LwyQHpkOWBXrYz","answer":"C","isMC":true,"timestamp":"2024-12-16 16:07:00","answer_description":"","answer_images":[],"question_id":77,"question_text":"Which of the following commands will return the number of null values in the member_id column?","question_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/153071-exam-certified-data-engineer-associate-topic-1-question-170/","answer_ET":"C","choices":{"A":"SELECT count(member_id) FROM my_table;","D":"SELECT null(member_id) FROM my_table;","C":"SELECT count_if(member_id IS NULL) FROM my_table;","B":"SELECT count(member_id) - count_null(member_id) FROM my_table;"},"exam_id":162,"topic":"1","unix_timestamp":1734361620,"answers_community":["C (100%)"],"discussion":[{"comment_id":"1366383","content":"Selected Answer: C\ncorrect option","timestamp":"1741383000.0","poster":"Kayceetalks","upvote_count":"1"}]},{"id":"UvGX3xpKGBOL2dlCrOmb","topic":"1","timestamp":"2024-12-16 16:07:00","discussion":[{"content":"Selected Answer: B\nCorrect","upvote_count":"1","poster":"Kayceetalks","comment_id":"1366384","timestamp":"1741383060.0"}],"exam_id":162,"answer_description":"","answer":"B","unix_timestamp":1734361620,"url":"https://www.examtopics.com/discussions/databricks/view/153073-exam-certified-data-engineer-associate-topic-1-question-171/","answers_community":["B (100%)"],"question_images":[],"question_id":78,"question_text":"Which tool is used by Auto Loader to process data incrementally?","answer_ET":"B","isMC":true,"choices":{"C":"Databricks SQL","B":"Spark Structured Streaming","A":"Checkpointing","D":"Unity Catalog"},"answer_images":[]},{"id":"X8YYGM8Cvwd6e4RjL1MA","answers_community":["C (100%)"],"unix_timestamp":1733767860,"answer_ET":"C","topic":"1","question_text":"A data engineer is working with two tables. Each of these tables is displayed below in its entirety.\n\n//IMG//\n\n\nThe data engineer runs the following query to join these tables together:\n\n//IMG//\n\n\nWhich of the following will be returned by the above query?","answer_images":[],"discussion":[{"content":"Selected Answer: C\nIt's the same question as 68 and 112.","timestamp":"1741699380.0","upvote_count":"2","poster":"analyticstraining","comment_id":"1387435"}],"timestamp":"2024-12-09 19:11:00","isMC":true,"answer":"C","answer_description":"","exam_id":162,"question_images":["https://img.examtopics.com/certified-data-engineer-associate/image79.png","https://img.examtopics.com/certified-data-engineer-associate/image80.png"],"url":"https://www.examtopics.com/discussions/databricks/view/152757-exam-certified-data-engineer-associate-topic-1-question-173/","choices":{"C":"","A":"","D":"","B":""},"question_id":79},{"id":"uuK3COn08EIShoSyoZi1","question_text":"A data analyst has a series of queries in a SQL program. The data analyst wants this program to run every day. They only want the final query in the program to run on Sundays. They ask for help from the data engineering team to complete this task.\nWhich of the following approaches could be used by the data engineering team to complete this task?","question_images":[],"timestamp":"2023-04-04 01:49:00","choices":{"D":"They could automatically restrict access to the source table in the final query so that it is only accessible on Sundays.","E":"They could redesign the data model to separate the data used in the final query into a new table.","C":"They could only run the entire program on Sundays.","A":"They could submit a feature request with Databricks to add this functionality.","B":"They could wrap the queries using PySpark and use Pythonâ€™s control flow system to determine when to run the final query."},"answer_description":"","topic":"1","unix_timestamp":1680565740,"url":"https://www.examtopics.com/discussions/databricks/view/105035-exam-certified-data-engineer-associate-topic-1-question-18/","answer_images":[],"isMC":true,"question_id":80,"answer":"B","answers_community":["B (100%)"],"discussion":[{"upvote_count":"11","content":"B\n\nThe answer is B. \n\nOption A: Submitting a feature request with Databricks to add this functionality is not a feasible solution because it would require Databricks to implement new functionality.\n\nOption C: Only running the entire program on Sundays would be inconvenient for the data analyst because they would have to remember to run the program on Sundays.\n\nOption D: Automatically restricting access to the source table in the final query so that it is only accessible on Sundays would be difficult to implement and would not be a reliable solution.\n\nOption E: Redesigning the data model to separate the data used in the final query into a new table would be a major undertaking and would not be a feasible solution for this specific problem.\n\nTherefore, the only feasible solution to the problem is to wrap the queries using PySpark and use Python's control flow system to determine when to run the final query.\npython code","poster":"Atnafu","comments":[{"comment_id":"1221983","content":"Also to add for option C is:\nthe requirement says: data analysts wants to run the sql program every day, but only the final query to run on sundays. So the option c violates the requirements","upvote_count":"1","poster":"vibha2119","timestamp":"1717137900.0"}],"timestamp":"1688764500.0","comment_id":"946004"},{"content":"Selected Answer: B\nWrapping the SQL program in PySpark allows the data engineering team to leverage Pythonâ€™s control flow (e.g., if statements) to determine when the final query should execute. For example:\n\nThe PySpark program can check the current day using Python's datetime module.\nIf the current day is Sunday, it will execute all queries, including the final one.\nIf itâ€™s not Sunday, the program will skip the final query.\nThis approach ensures flexibility and allows precise control over which queries run on which days, meeting the data analyst's requirements.","timestamp":"1731953400.0","upvote_count":"2","comment_id":"1314113","poster":"806e7d2"},{"poster":"80370eb","timestamp":"1723104720.0","comment_id":"1262406","content":"Selected Answer: B\nB. They could wrap the queries using PySpark and use Pythonâ€™s control flow system to determine when to run the final query.\n\nThis approach involves using PySpark to control the execution flow based on the day of the week, allowing the final query to execute conditionally while the other queries run daily.","upvote_count":"1"},{"comment_id":"1113200","content":"Selected Answer: B\nCorrect answer is B","poster":"SerGrey","timestamp":"1704322920.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"1028817","timestamp":"1696850640.0","poster":"VijayKula","content":"Selected Answer: B\nB is correct"},{"comment_id":"1017356","timestamp":"1695698220.0","content":"Selected Answer: B\nB is correct ans","poster":"KalavathiP","upvote_count":"1"},{"content":"Selected Answer: B\nB is correct.","timestamp":"1695626220.0","poster":"d_b47","comment_id":"1016566","upvote_count":"1"},{"poster":"mehroosali","upvote_count":"1","content":"Selected Answer: B\nB is correct","timestamp":"1688737380.0","comment_id":"945732"},{"content":"Selected Answer: B\nB is correct","comment_id":"916688","poster":"[Removed]","upvote_count":"1","timestamp":"1686091500.0"},{"content":"Selected Answer: B\nB is the correct answer","timestamp":"1680704760.0","upvote_count":"1","comment_id":"862189","poster":"XiltroX"},{"timestamp":"1680601860.0","poster":"mimzzz","comment_id":"860859","content":"i think the answer is correct","upvote_count":"1"},{"poster":"knivesz","upvote_count":"1","comment_id":"860468","timestamp":"1680565740.0","content":"Selected Answer: B\nRespuesta Correcta es B"}],"answer_ET":"B","exam_id":162}],"exam":{"name":"Certified Data Engineer Associate","isMCOnly":true,"numberOfQuestions":169,"lastUpdated":"12 Apr 2025","id":162,"isImplemented":true,"provider":"Databricks","isBeta":false},"currentPage":16},"__N_SSP":true}