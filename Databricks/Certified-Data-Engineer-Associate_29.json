{"pageProps":{"questions":[{"id":"ftej0T8v21cl0vbZJlho","answer":"D","url":"https://www.examtopics.com/discussions/databricks/view/124301-exam-certified-data-engineer-associate-topic-1-question-73/","topic":"1","answers_community":["D (100%)"],"answer_description":"","discussion":[{"poster":"CommanderBigMac","comment_id":"1282225","timestamp":"1726069740.0","upvote_count":"1","content":"Selected Answer: D\nA gold table is most likely to contain aggregated data. It is also the table where cleaned up data is stored, so that is where data will be used from for a dashboard."},{"timestamp":"1701352200.0","content":"Selected Answer: D\nhttps://docs.databricks.com/en/lakehouse/medallion.html#power-analytics-with-the-gold-layer","upvote_count":"1","comment_id":"1084414","poster":"55f31c8"},{"upvote_count":"3","timestamp":"1697945100.0","comment_id":"1050172","content":"D is correct : std medallion arch","poster":"meow_akk"}],"choices":{"B":"A job that aggregates uncleaned data to create standard summary statistics","A":"A job that enriches data by parsing its timestamps into a human-readable format","C":"A job that cleans data by removing malformatted records","E":"A job that ingests raw data from a streaming source into the Lakehouse","D":"A job that queries aggregated data designed to feed into a dashboard"},"unix_timestamp":1697945100,"exam_id":162,"question_images":[],"answer_ET":"D","question_text":"Which of the following data workloads will utilize a Gold table as its source?","isMC":true,"timestamp":"2023-10-22 05:25:00","answer_images":[],"question_id":141},{"id":"QMoHFSJDErgLvwwLIXCo","unix_timestamp":1697813640,"exam_id":162,"timestamp":"2023-10-20 16:54:00","question_id":142,"isMC":true,"topic":"1","answer":"E","discussion":[{"poster":"Stemix","timestamp":"1706276040.0","content":"Selected Answer: E\nCorrect answer is E. storage location is optional. \n\"(Optional) Enter a Storage location for output data from the pipeline. The system uses a default location if you leave Storage location empty\"","comment_id":"1132545","upvote_count":"7"},{"content":"Selected Answer: E\nThe answer is E, even if technically the Pipeline name is the only mandatory field.\nHowever, if you don't provide a notebook path, the following message will be displayed:\n\"You didn't specify any source code. \"Create pipeline\" will create your pipeline along with a blank notebook, which you can edit later.\"","timestamp":"1737461880.0","upvote_count":"2","comment_id":"1344135","poster":"fits08pistils"},{"comment_id":"1315528","poster":"hakimipous","content":"Selected Answer: C\nC is correct","upvote_count":"1","timestamp":"1732140180.0"},{"timestamp":"1727447460.0","content":"D. A location of a target database for the written data\n\nWhy this is correct: When creating a Delta Live Tables (DLT) pipeline, you must specify the target database where the resulting data will be written. This ensures that the output of the pipeline is stored properly.\n\nWhy the other options are incorrect:\n\nA. A key-value pair configuration: While configurations are useful, they are not mandatory when setting up a DLT pipeline.\n\nB. The preferred DBU/hour cost: You don't specify a cost directly; the DBU is associated with the cluster used.\n\nC. A path to cloud storage location for the written data: While storage paths may be specified, the target database location is required.\n\nE. At least one notebook library: You specify the transformation logic (which could be in notebooks), but this is not a strict requirement for setting up the pipeline itself.","comment_id":"1290093","poster":"Colje","upvote_count":"4"},{"comment_id":"1263537","timestamp":"1723298460.0","poster":"80370eb","upvote_count":"1","content":"Selected Answer: E\nThis is a key requirement for creating a Delta Live Tables pipeline. You need to specify notebooks that contain the ETL logic to be executed by the pipeline."},{"upvote_count":"1","poster":"Shinigami76","comment_id":"1228292","timestamp":"1718089260.0","content":"C, just tested on databricks DLT"},{"comment_id":"1203845","timestamp":"1714370340.0","poster":"benni_ale","content":"Selected Answer: E\ntbf C is correct as well but the question is probably hinting for E","upvote_count":"1"},{"poster":"BigMF","timestamp":"1710786000.0","comments":[{"poster":"7082935","comment_id":"1273006","timestamp":"1724699880.0","content":"\"you need to select a destination for datasets published by the pipeline\". This is true if you have a notebook that is writing out a result dataset. However, nothing in this question or documentation states that a Delta Live Tables Pipeline --MUST-- contain a notebook that write dataset results.","upvote_count":"1"}],"upvote_count":"1","content":"Selected Answer: C\nPer Databaricks documentation (see below), you need to select a destination for datasets published by the pipeline, either the Hive metastore or Unity Catalog I think A is incorrect because it uses the term \"Notebook Library\" and not just \"Notebook\". \nDatabricks doc: https://docs.databricks.com/en/delta-live-tables/tutorial-pipelines.html","comment_id":"1176696"},{"comment_id":"1127418","poster":"azure_bimonster","content":"Selected Answer: E\nAs per Pipeline creating steps, choosing a Notebook is mandatory whereas specifying a location is optional. I would go with answer E","upvote_count":"1","timestamp":"1705771500.0"},{"poster":"Azure_2023","timestamp":"1705412220.0","comment_id":"1124246","content":"Selected Answer: E\nhttps://docs.databricks.com/en/delta-live-tables/tutorial-pipelines.html\n\nE. The only non-optional selection is a notebook","upvote_count":"2"},{"poster":"Garyn","timestamp":"1703987880.0","content":"Selected Answer: E\nE. At least one notebook library to be executed.\n\nExplanation:\nhttps://docs.databricks.com/en/delta-live-tables/tutorial-pipelines.html\n\nDelta Live Tables pipelines execute notebook libraries as part of their operations. These notebooks contain the logic, code, or instructions defining the data processing steps, transformations, or actions to be performed within the pipeline.\n\nSpecifying at least one notebook library to be executed is crucial when creating a new Delta Live Tables pipeline, as it defines the sequence of operations and the logic to be executed on the data within the pipeline, aligning with the documentation provided.","upvote_count":"2","comment_id":"1110201"},{"comment_id":"1100256","timestamp":"1702957740.0","poster":"saaaaaa","upvote_count":"2","content":"Selected Answer: E\nThis should be E. As per the link https://docs.databricks.com/en/delta-live-tables/tutorial-pipelines.html\n\nCreate a pipeline\n\nClick Jobs Icon Workflows in the sidebar, click the Delta Live Tables tab, and click Create Pipeline.\n\nGive the pipeline a name and click File Picker Icon to select a notebook.\n\nSelect Triggered for Pipeline Mode.\n\n(Optional) Enter a Storage location for output data from the pipeline. The system uses a default location if you leave Storage location empty.\n\n(Optional) Specify a Target schema to publish your dataset to the Hive metastore or a Catalog and a Target schema to publish your dataset to Unity Catalog. See Publish datasets.\n\n(Optional) Click Add notification to configure one or more email addresses to receive notifications for pipeline events. See Add email notifications for pipeline events.\n\nClick Create."},{"poster":"55f31c8","upvote_count":"1","comment_id":"1084461","timestamp":"1701355200.0","content":"Selected Answer: C\nhttps://docs.databricks.com/en/delta-live-tables/index.html#what-is-a-delta-live-tables-pipeline"},{"poster":"Huroye","upvote_count":"3","content":"The correct answer is E. DLT tables needs a notebook where you have to specify the processing info","comment_id":"1071980","timestamp":"1700088600.0"},{"timestamp":"1698484200.0","content":"Selected Answer: C\nstorage location is required to be specified to control the object storage location for data written by the pipeline.","poster":"kishore1980","upvote_count":"2","comment_id":"1056071"},{"poster":"meow_akk","timestamp":"1697945340.0","comment_id":"1050175","upvote_count":"3","comments":[{"content":"Answer is E\nStorage and location are optional.\nhttps://docs.databricks.com/en/delta-live-tables/tutorial-pipelines.html","comment_id":"1056574","timestamp":"1698553440.0","poster":"Syd","upvote_count":"1"}],"content":"Ans E : i think it might be E - https://docs.databricks.com/en/delta-live-tables/settings.html - this doc says that target schema and storage may be optional so it leaves us with E"},{"content":"Selected Answer: C\nA path to a cloud storage location for the written data - considering this option is talking about the source data being stored in cloud storage and being ingested to DLT using an autoloader.","upvote_count":"3","comment_id":"1048903","poster":"kishanu","timestamp":"1697813640.0"}],"question_images":[],"answers_community":["E (69%)","C (31%)"],"answer_ET":"E","answer_description":"","choices":{"D":"A location of a target database for the written data","A":"A key-value pair configuration","E":"At least one notebook library to be executed","B":"The preferred DBU/hour cost","C":"A path to cloud storage location for the written data"},"question_text":"Which of the following must be specified when creating a new Delta Live Tables pipeline?","url":"https://www.examtopics.com/discussions/databricks/view/124135-exam-certified-data-engineer-associate-topic-1-question-74/","answer_images":[]},{"id":"OCBCr8xvRlaCLC2wa5JC","discussion":[{"upvote_count":"8","poster":"meow_akk","timestamp":"1697945700.0","comment_id":"1050180","content":"Ans C is correct : \nhttps://docs.databricks.com/en/sql/load-data-streaming-table.html\nLoad data into a streaming table\nTo create a streaming table from data in cloud object storage, paste the following into the query editor, and then click Run:\n\nSQL\nCopy to clipboardCopy\n/* Load data from a volume */\nCREATE OR REFRESH STREAMING TABLE <table-name> AS\nSELECT * FROM STREAM read_files('/Volumes/<catalog>/<schema>/<volume>/<path>/<folder>')\n\n/* Load data from an external location */\nCREATE OR REFRESH STREAMING TABLE <table-name> AS\nSELECT * FROM STREAM read_files('s3://<bucket>/<path>/<folder>')"},{"comment_id":"1263541","upvote_count":"1","timestamp":"1723298700.0","content":"Selected Answer: C\nThe STREAM function is used to indicate that LIVE.customers is a streaming live table. This allows the query to process real-time streaming data.","poster":"80370eb"},{"poster":"benni_ale","timestamp":"1714370520.0","upvote_count":"1","comment_id":"1203849","content":"c is correct . about D: it can be correct but it is not given the fact it comes from pyspark ; sql supports (at least in databricks) the creation of streaming live table as well so it is not necessasarily from pyspark"},{"comment_id":"1203848","timestamp":"1714370400.0","poster":"benni_ale","content":"Selected Answer: C\nc is ok","upvote_count":"1"},{"poster":"[Removed]","timestamp":"1714219860.0","content":"Selected Answer: D\nOption E, specifying \"at least one notebook library to be executed,\" is not a requirement for setting up a Delta Live Tables pipeline. Delta Live Tables are built on top of Databricks and use notebooks to define the pipeline's logic, but the actual requirement when setting up the pipeline is typically the location where the data will be written to, like a target database or a path to cloud storage. While notebooks may contain the business logic for the transformations and actions within the pipeline, the fundamental requirement for setting up a pipeline is knowing where the data will reside after processing, hence why the location of the target database for the written data is crucial.","comments":[{"content":"Wrong question, that's for #73","comments":[{"content":"I mean question #74","upvote_count":"1","comment_id":"1217901","poster":"THC1138","timestamp":"1716596340.0"}],"poster":"THC1138","timestamp":"1716596340.0","comment_id":"1217900","upvote_count":"1"}],"comment_id":"1203100","upvote_count":"1"},{"upvote_count":"1","comment_id":"1127420","timestamp":"1705771620.0","content":"Selected Answer: C\nC is correct","poster":"azure_bimonster"},{"upvote_count":"1","timestamp":"1704273420.0","content":"Ans is A.\nCREATE STREAMING LIVE TABLE syntax is does not exist.\nIt should be CREATE LIVE TABLE AS SELECT * FROM STREAM.","comment_id":"1112624","comments":[{"comment_id":"1118430","poster":"bartfto","upvote_count":"1","timestamp":"1704881220.0","content":"LIVE references schema name\ncustomer_table references table name"}],"poster":"cxw23"}],"answer_description":"","choices":{"C":"The customers table is a streaming live table.","B":"The table being created is a live table.","D":"The customers table is a reference to a Structured Streaming query on a PySpark DataFrame.","A":"The STREAM function is not needed and will cause an error.","E":"The data in the customers table has been updated since its last run."},"question_text":"A data engineer has joined an existing project and they see the following query in the project repository:\n\nCREATE STREAMING LIVE TABLE loyal_customers AS\n\nSELECT customer_id -\nFROM STREAM(LIVE.customers)\nWHERE loyalty_level = 'high';\n\nWhich of the following describes why the STREAM function is included in the query?","answer":"C","isMC":true,"unix_timestamp":1697945700,"exam_id":162,"question_images":[],"question_id":143,"answer_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/databricks/view/124304-exam-certified-data-engineer-associate-topic-1-question-75/","answers_community":["C (75%)","D (25%)"],"answer_ET":"C","timestamp":"2023-10-22 05:35:00"},{"id":"odLBIcB5zh1f1qjn4hg6","question_id":144,"unix_timestamp":1697945760,"answers_community":["A (100%)"],"choices":{"B":"Machine learning workloads","A":"Streaming workloads","C":"Serverless workloads","D":"Batch workloads","E":"Dashboard workloads"},"question_text":"Which of the following describes the type of workloads that are always compatible with Auto Loader?","exam_id":162,"answer":"A","url":"https://www.examtopics.com/discussions/databricks/view/124305-exam-certified-data-engineer-associate-topic-1-question-76/","answer_images":[],"topic":"1","discussion":[{"upvote_count":"6","comment_id":"1050181","timestamp":"1697945760.0","poster":"meow_akk","content":"A is correct Structured streaming for autoloader"},{"comment_id":"1263542","content":"Selected Answer: A\nAuto Loader is designed to handle streaming data ingestion. It continuously processes new data as it arrives, making it well-suited for streaming workloads.","poster":"80370eb","upvote_count":"1","timestamp":"1723298760.0"},{"content":"Selected Answer: A\nA is ok","timestamp":"1714370580.0","poster":"benni_ale","comment_id":"1203851","upvote_count":"1"},{"poster":"azure_bimonster","timestamp":"1705771620.0","upvote_count":"1","comment_id":"1127421","content":"Selected Answer: A\nA is correct here"},{"timestamp":"1703081700.0","content":"Selected Answer: A\nhttps://docs.databricks.com/en/ingestion/auto-loader/unity-catalog.html#using-auto-loader-with-unity-catalog \n\nAuto Loader relies on Structured Streaming for incremental processing","comment_id":"1101606","upvote_count":"3","poster":"AndreFR"}],"question_images":[],"answer_ET":"A","timestamp":"2023-10-22 05:36:00","answer_description":"","isMC":true},{"id":"FtYkJMMCmPtaoWBSLOot","url":"https://www.examtopics.com/discussions/databricks/view/124306-exam-certified-data-engineer-associate-topic-1-question-77/","topic":"1","answer_ET":"D","question_images":[],"answer_description":"","answer":"D","isMC":true,"question_id":145,"question_text":"A data engineer and data analyst are working together on a data pipeline. The data engineer is working on the raw, bronze, and silver layers of the pipeline using Python, and the data analyst is working on the gold layer of the pipeline using SQL. The raw source of the pipeline is a streaming input. They now want to migrate their pipeline to use Delta Live Tables.\n\nWhich of the following changes will need to be made to the pipeline when migrating to Delta Live Tables?","discussion":[{"timestamp":"1718044980.0","poster":"hussamAlHunaiti","comments":[{"content":"if A and B are not there what was there, also you have to answer this question with options available. The answer is A","upvote_count":"1","comment_id":"1328323","timestamp":"1734507060.0","poster":"MultiCloudIronMan"}],"content":"Selected Answer: D\nI had the exam today and option A & B weren't exist, correct answer is D.","upvote_count":"10","comment_id":"1228049"},{"content":"Selected Answer: D\nNone is never a solution","poster":"vigaro","upvote_count":"8","timestamp":"1718889300.0","comment_id":"1233625"},{"poster":"MultiCloudIronMan","upvote_count":"3","timestamp":"1734507000.0","comment_id":"1328321","content":"Selected Answer: A\nThe correct response is A. None of these changes will need to be made. Delta Live Tables supports both Python and SQL, as well as streaming and batch sources. This means that the existing medallion-based multi-hop architecture can be maintained, and the pipeline can continue to use both Python and SQL for different layers. Therefore, no changes are necessary when migrating to Delta Live Tables."},{"poster":"806e7d2","comment_id":"1315957","timestamp":"1732214340.0","upvote_count":"2","content":"Selected Answer: A\nDelta Live Tables (DLT) is designed to support a medallion-based architecture (raw → bronze → silver → gold) and allows for a combination of Python and SQL in pipeline definitions. It also supports both batch and streaming sources."},{"content":"The correct answer is:\n\nA. None of these changes will need to be made.\n\nExplanation:\nDelta Live Tables (DLT) supports both Python and SQL:\n\nThe data engineer can continue writing transformations for the raw, bronze, and silver layers in Python.\nThe data analyst can work on the gold layer in SQL.\nMedallion-based architecture:\n\nDelta Live Tables is well-suited for the medallion architecture (raw -> bronze -> silver -> gold). It is commonly used to build reliable and maintainable data pipelines.\nStreaming sources:\n\nDelta Live Tables fully supports streaming inputs and can handle both batch and streaming sources natively.\nFlexibility in implementation:\n\nDelta Live Tables does not impose restrictions that require pipelines to be written entirely in either SQL or Python. Both languages can coexist in the same pipeline as needed.\nThus, no major changes are required for the migration to Delta Live Tables.","poster":"gul1016","timestamp":"1731964860.0","upvote_count":"1","comment_id":"1314234"},{"content":"Selected Answer: A\nA is correct","timestamp":"1731595560.0","upvote_count":"1","poster":"lj114","comment_id":"1312070"},{"timestamp":"1730812800.0","content":"Right answer is not listed here. The right answer is \"Different notbook may jused for SQL and Python\"","upvote_count":"1","poster":"ajay1709","comment_id":"1307379"},{"poster":"CommanderBigMac","content":"Selected Answer: D\nD is the answer","upvote_count":"1","comment_id":"1287629","timestamp":"1726994340.0"},{"comment_id":"1273426","timestamp":"1724762520.0","content":"A. None of these changes will need to be made.\n\nYou can continue using the medallion-based architecture, and you do not need to switch entirely to SQL or Python. Delta Live Tables will work with your existing streaming sources and support both SQL and Python.","upvote_count":"1","poster":"9d4d68a"},{"content":"Selected Answer: A\nWhen migrating to Delta Live Tables, you can continue using the medallion-based architecture, work with streaming sources, and write the pipeline in either SQL or Python. Therefore, no major changes are required for the pipeline in this scenario.","upvote_count":"2","comment_id":"1267939","timestamp":"1723959180.0","poster":"80370eb"},{"poster":"jaromarg","comment_id":"1227869","timestamp":"1718024040.0","upvote_count":"4","comments":[{"poster":"jaromarg","timestamp":"1718025780.0","upvote_count":"1","content":"Yes It must be A:\nLanguage Support: DLT allows the use of both SQL and Python, so you can integrate the existing Python and SQL code within the DLT framework.","comment_id":"1227883"}],"content":"D:\nDelta Live Tables is primarily designed to work with batch processing rather than streaming. This means that when migrating a pipeline to Delta Live Tables, any streaming sources used in the original pipeline will need to be replaced with batch sources.\n\nIn the scenario described, where the raw source of the pipeline is a streaming input, the data engineer and data analyst will need to modify their pipeline to read data from a batch source instead. This could involve changing the way data is ingested and processed to align with batch processing paradigms rather than streaming.\n\nAdditionally, Delta Live Tables enables the integration of both SQL and Python code within a pipeline, so there's no strict requirement to write the pipeline entirely in SQL or Python. Both the data engineer's Python code for the raw, bronze, and silver layers and the data analyst's SQL code for the gold layer can still be used within the Delta Live Tables environment.\n\nOverall, the key change needed when migrating to Delta Live Tables in this scenario is transitioning from a streaming input source to a batch source to align with the batch processing nature of Delta Live Tables."},{"content":"Selected Answer: A\nA is correct","poster":"benni_ale","comment_id":"1203853","upvote_count":"2","timestamp":"1714370640.0"},{"comment_id":"1198443","upvote_count":"3","timestamp":"1713515040.0","content":"Cleared the exam today . Option A and B were not available in the exam . There was a different option which was correct.","poster":"Arunava05"},{"poster":"AndreFR","upvote_count":"4","timestamp":"1703082720.0","content":"Selected Answer: A\nB - DLT support medallion architecture (see example in : https://docs.databricks.com/en/delta-live-tables/transform.html#combine-streaming-tables-and-materialized-views-in-a-single-pipeline) \nC - DLT can mix Python and SQL using multiple notebooks (according to https://docs.databricks.com/en/delta-live-tables/tutorial-python.html You cannot mix languages within a Delta Live Tables source code file. You can use multiple notebooks or files with different languages in a pipeline) \nD - DLT manage streaming sources using streaming tables (ex : https://docs.databricks.com/en/delta-live-tables/load.html#load-data-from-a-message-bus) \nE - DLT support python and sql (https://docs.databricks.com/en/delta-live-tables/tutorial-python.html and https://docs.databricks.com/en/delta-live-tables/tutorial-sql.html) \n\nCorrect answer is A by elimination","comment_id":"1101634"},{"poster":"kz_data","upvote_count":"1","content":"Selected Answer: A\nI think the answer is A","timestamp":"1701894900.0","comment_id":"1089750"},{"poster":"nedlo","comment_id":"1089166","upvote_count":"2","timestamp":"1701856440.0","content":"Selected Answer: A\nIt should be A. Medallion architecture can be used in DLT pipeline https://www.databricks.com/glossary/medallion-architecture \"Databricks provides tools like Delta Live Tables (DLT) that allow users to instantly build data pipelines with Bronze, Silver and Gold tables from just a few lines of code.\""},{"upvote_count":"3","comment_id":"1071984","poster":"Huroye","timestamp":"1700088900.0","content":"the correct answer is A. DLT needs a notebook where you specify the processing"},{"content":"Selected Answer: A\nResponse A: They have to adapt their notebook's code to be able to decalre the DLT pipeline.\nHowever, this option is not proposed in the answers so I think it might be A","poster":"mokrani","timestamp":"1699365180.0","upvote_count":"1","comment_id":"1064841"},{"content":"Answer should be A.","upvote_count":"2","poster":"hsks","timestamp":"1698392640.0","comment_id":"1055192"},{"content":"In my opinion, this should be A. Assuming they were working on the same notebook, and weren't declaring the Streaming or Live keywords during development, the would probably need to do so before adding to the DLT workflow. and that is not in the option.","timestamp":"1698161880.0","poster":"kbaba101","comment_id":"1053009","upvote_count":"2"},{"content":"i think its A ;","timestamp":"1697946060.0","poster":"meow_akk","comment_id":"1050183","upvote_count":"4"}],"answers_community":["D (51%)","A (49%)"],"choices":{"C":"The pipeline will need to be written entirely in SQL","B":"The pipeline will need to stop using the medallion-based multi-hop architecture","E":"The pipeline will need to be written entirely in Python","D":"The pipeline will need to use a batch source in place of a streaming source","A":"None of these changes will need to be made"},"exam_id":162,"unix_timestamp":1697946060,"timestamp":"2023-10-22 05:41:00","answer_images":[]}],"exam":{"lastUpdated":"12 Apr 2025","numberOfQuestions":169,"isImplemented":true,"provider":"Databricks","id":162,"isMCOnly":true,"isBeta":false,"name":"Certified Data Engineer Associate"},"currentPage":29},"__N_SSP":true}