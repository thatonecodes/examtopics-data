{"pageProps":{"questions":[{"id":"ZteZImx77lSHJs51Vigy","question_images":[],"exam_id":162,"answer_description":"","answer":"C","unix_timestamp":1680364560,"topic":"1","question_id":86,"question_text":"A data engineer is attempting to drop a Spark SQL table my_table. The data engineer wants to delete all table metadata and data.\nThey run the following command:\n\nDROP TABLE IF EXISTS my_table -\nWhile the object no longer appears when they run SHOW TABLES, the data files still exist.\nWhich of the following describes why the data files still exist and the metadata files were deleted?","timestamp":"2023-04-01 17:56:00","discussion":[{"poster":"80370eb","comment_id":"1262417","timestamp":"1723108440.0","upvote_count":"2","content":"Selected Answer: C\nC. The table was external\n\nWhen dropping an external table in Spark SQL, only the metadata is removed. The actual data files remain in their original location because they are not managed by Spark but by the external source."},{"timestamp":"1704355140.0","comment_id":"1113405","upvote_count":"1","poster":"SerGrey","content":"Selected Answer: C\nC is correct"},{"content":"THE QUESTION SHOULD BE \"Which of the following describes why the metadata files still exist and the data files were deleted?\"","poster":"hemanthgvsk","timestamp":"1697830980.0","upvote_count":"1","comment_id":"1049077"},{"content":"Selected Answer: C\nC. The table was external\n\nThe reason why the data files still exist while the metadata files were deleted is because the table was external. When a table is external in Spark SQL (or in other database systems), it means that the table metadata (such as schema information and table structure) is managed externally, and Spark SQL assumes that the data is managed and maintained outside of the system. Therefore, when you execute a DROP TABLE statement for an external table, it removes only the table metadata from the catalog, leaving the data files intact.\n\nOn the other hand, for managed tables (option E), Spark SQL manages both the metadata and the data files. When you drop a managed table, it deletes both the metadata and the associated data files, resulting in a complete removal of the table.","comment_id":"997957","timestamp":"1693775340.0","poster":"vctrhugo","upvote_count":"2"},{"upvote_count":"2","content":"Selected Answer: C\nOption C","comment_id":"860585","poster":"surrabhi_4","timestamp":"1680579060.0"},{"content":"Selected Answer: C\nC is the correct answer. For external tables, you need to go to the specific location using DESCRIBE EXTERNAL TABLE command and delete all files.","timestamp":"1680364560.0","comment_id":"858081","upvote_count":"2","poster":"XiltroX"}],"url":"https://www.examtopics.com/discussions/databricks/view/104761-exam-certified-data-engineer-associate-topic-1-question-23/","isMC":true,"answer_ET":"C","choices":{"B":"The table’s data was smaller than 10 GB","D":"The table did not have a location","E":"The table was managed","A":"The table’s data was larger than 10 GB","C":"The table was external"},"answer_images":[],"answers_community":["C (100%)"]},{"id":"5i1wMxJ7eBiNt7BTjvnD","choices":{"E":"Table","C":"View","D":"Temporary view","A":"Database","B":"Function"},"topic":"1","answer_description":"","answer":"E","url":"https://www.examtopics.com/discussions/databricks/view/104762-exam-certified-data-engineer-associate-topic-1-question-24/","isMC":true,"question_images":[],"question_text":"A data engineer wants to create a data entity from a couple of tables. The data entity must be used by other data engineers in other sessions. It also must be saved to a physical location.\nWhich of the following data entities should the data engineer create?","question_id":87,"exam_id":162,"unix_timestamp":1680364800,"answers_community":["E (80%)","C (20%)"],"timestamp":"2023-04-01 18:00:00","discussion":[{"comment_id":"894193","content":"Questions says : \n1. The data entity must be used by other data engineers in other sessions.\n2. It also must be saved to a physical location.\n\nHere View doesn't store data in physical location , from the options only table stores data in physical location \n\nSo answer should be 'E' which is Table.","timestamp":"1683737580.0","poster":"Bob123456","upvote_count":"33"},{"comment_id":"1133111","poster":"Nika12","content":"Selected Answer: E\nJust got 100% in the exam. Table was a correct answer.","timestamp":"1706339160.0","comments":[{"upvote_count":"1","timestamp":"1706553840.0","comment_id":"1135222","poster":"ADVIT","content":"Wow! Congratz!"}],"upvote_count":"9"},{"content":"Selected Answer: E\nE. Table\n\nCreating a table ensures that the data entity is saved to a physical location and can be used by other data engineers in different sessions. Tables persist data and metadata, making them suitable for long-term storage and sharing across sessions.","timestamp":"1723108740.0","upvote_count":"2","comment_id":"1262422","poster":"80370eb"},{"timestamp":"1720468740.0","content":"Selected Answer: E\nThe correct answer is \"E\". A view does not save to a physical location; only caching a SELECT statement.","upvote_count":"1","comment_id":"1244549","poster":"3fbc31b"},{"comment_id":"1203436","upvote_count":"2","timestamp":"1714289100.0","content":"Selected Answer: E\nphysical location means table","poster":"benni_ale"},{"timestamp":"1706769000.0","content":"E. as view doesnt has any location","comment_id":"1137327","poster":"agAshish","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: E\nE is correct","poster":"SerGrey","timestamp":"1704355320.0","comment_id":"1113406"},{"content":"Selected Answer: E\nE. Table\n\nUsage by Other Sessions: Tables in a database are persistent data structures that can be accessed by multiple users and sessions concurrently.\n\nSaved to a Physical Location: Tables store data physically in a structured manner on disk or in a storage system, making them suitable for long-term storage.\n\nUsage by Other Data Engineers: Other data engineers can query, access, and work with the data within the table, making it a feasible choice for shared access among multiple users or sessions.\n\nWhile other entities like views or temporary views can provide different ways to represent or filter data, a table fits the criteria best when the data engineer requires a persistent physical storage entity accessible by other sessions and users for data manipulation, retrieval, and storage.","upvote_count":"1","timestamp":"1703892960.0","poster":"Garyn","comment_id":"1109256"},{"timestamp":"1702283460.0","poster":"RafaelCFC","content":"Selected Answer: C\nI think the key to the answer is that it refers to the Data Entinty, and not to the data itself, when it mentions \"the Data Entity must be used by other Data Engineers\", and \"It must be saved to a physical location\". From this PoV, both C and E would be correct, however, creating a new table would incur in processing to a static state the relationship from \"a couple of tables\". While this make sense to many use cases, this would require either a Workflow or a DLT to make it work, which goes over the requested scope. C is the best answer for the requested scenario.","comment_id":"1093253","upvote_count":"4"},{"content":"Key point to remember during answering this question:\n \" It also must be saved to a physical location\"\n\n\nSo answer should be 'E' which is Table.","timestamp":"1701394200.0","poster":"Vikram1710","comment_id":"1084859","upvote_count":"2"},{"poster":"rbeeraka","content":"C is the right answer. View is a data entity and its definition is physically saved so other users can consume view","comment_id":"1084842","upvote_count":"1","timestamp":"1701389100.0"},{"poster":"Huroye","comment_id":"1071639","upvote_count":"1","content":"The correct answer is E because it has to be physically saved. View is in memory.","timestamp":"1700066160.0"},{"timestamp":"1699364400.0","content":"Selected Answer: E\nCorrect : E","poster":"awofalus","upvote_count":"1","comment_id":"1064826"},{"upvote_count":"2","comment_id":"997958","poster":"vctrhugo","content":"Selected Answer: E\nE. Table\n\nTo create a data entity that can be used by other data engineers in other sessions and must be saved to a physical location, you should create a table. Tables in a database are physical storage structures that hold data, and they can be accessed and shared by multiple users and sessions. By creating a table, you provide a permanent and structured storage location for the data entity that can be used across different sessions and by other users as needed.\n\nOptions like databases (A) can be used to organize tables, views (C) can provide virtual representations of data, and temporary views (D) are temporary in nature and don't save data to a physical location. Functions (B) are typically used for processing data or performing calculations, not for storing data.","timestamp":"1693775400.0"},{"content":"Selected Answer: E\nView does not have a physical location so answer has to be E","timestamp":"1693323600.0","upvote_count":"1","poster":"[Removed]","comment_id":"993224"},{"poster":"Kartz130789","comment_id":"972916","content":"Selected Answer: E\nView Doesn't physical location","upvote_count":"2","timestamp":"1691232720.0"},{"upvote_count":"2","poster":"ehsanmor18","comment_id":"951440","content":"The answer is E: \"Table\"\n\nIn the context described, creating a \"Table\" is the most suitable choice. Tables in SQL are data entities that exist independently of any session and are saved in a physical location. They can be accessed and manipulated by other data engineers in different sessions, which aligns with the requirements stated.\n\nA \"Database\" is a collection of tables, views, and other database objects. A \"Function\" is a stored procedure that performs an operation. A \"View\" is a virtual table based on the result-set of an SQL statement, but it is not stored physically. A \"Temporary view\" is a feature that allows you to store the result of a query as a view that disappears once your session with the database is closed.","timestamp":"1689331020.0"},{"poster":"keksssd","comment_id":"949868","content":"Selected Answer: E\nanswer e","upvote_count":"1","timestamp":"1689170700.0"},{"comment_id":"946074","content":"C\nA view is a virtual table that is created from a query on one or more tables. Views are stored in the database and can be used by other data engineers in other sessions.\n\nThe other options are not correct.\n\nOption A: A database is a collection of tables.\n\nOption B: A function is a named block of code that can be executed.\n\nOption D: A temporary view is a view that is only stored in memory and is not saved to a physical location.\n\nOption E: A table is a physical collection of data.","timestamp":"1688780580.0","poster":"Atnafu","upvote_count":"1"},{"content":"Selected Answer: E\nView is wrong. it should be table.","upvote_count":"1","comment_id":"945745","poster":"mehroosali","timestamp":"1688737980.0"},{"content":"[C] - A view stores the text for a query typically against one or more data sources or tables in the metastore. You can query views from any part of the Databricks product, assuming you have permission to do so.\nhttps://learn.microsoft.com/en-us/azure/databricks/lakehouse/data-objects","poster":"hrabiabw","upvote_count":"2","timestamp":"1681830540.0","comment_id":"873777"},{"poster":"4be8126","timestamp":"1680687900.0","upvote_count":"2","content":"The data engineer should create a table.\n\nA table can be used by other data engineers in other sessions, and it can be saved to a physical location. Views and temporary views can also be used by other data engineers, but they do not have a physical location to store data. A function is a piece of code that can be called repeatedly with different arguments, but it does not store data. A database is a logical container that can hold multiple tables and views.","comment_id":"861944","comments":[{"comments":[{"upvote_count":"8","content":"The correct answer is E (Table). The databricks practice exam https://files.training.databricks.com/assessments/practice-exams/PracticeExam-DataEngineerAssociate.pdf (Question 14) which you are referring to clearly states that \"e data engineer wants to avoid copying and storing physical data\" hence VIEW makes sense for that question. This Question however says \"must be saved to a physical location\", so Table makes sense. There are a lot of question in the practice that are slightly different to the ones here. It's that additional instruction that changes everything. Another example is question 35 on that practice exam.","comment_id":"874399","timestamp":"1681892460.0","poster":"SHINGX"}],"content":"This question has been covered by the official Databricks practice exam which you can download form their website. They list correct answer as C (View). They have provided a key to all the questions in that practice test.","poster":"XiltroX","upvote_count":"2","timestamp":"1680705060.0","comment_id":"862194"}]},{"timestamp":"1680579060.0","content":"Selected Answer: C\noption C","comment_id":"860586","upvote_count":"1","poster":"surrabhi_4"},{"upvote_count":"1","content":"Selected Answer: C\nC is the correct answer. Please check link below. \nhttps://learn.microsoft.com/en-us/azure/databricks/lakehouse/data-objects","timestamp":"1680364800.0","comment_id":"858084","poster":"XiltroX"}],"answer_images":[],"answer_ET":"E"},{"id":"p9b7g6fqIxbPOw3ZGgwh","choices":{"B":"Data Explorer","E":"Auto Loader","C":"Delta Lake","A":"Unity Catalog","D":"Delta Live Tables"},"answer_images":[],"unix_timestamp":1680365100,"question_images":[],"question_text":"A data engineer is maintaining a data pipeline. Upon data ingestion, the data engineer notices that the source data is starting to have a lower level of quality. The data engineer would like to automate the process of monitoring the quality level.\nWhich of the following tools can the data engineer use to solve this problem?","answer_description":"","answer_ET":"D","topic":"1","exam_id":162,"timestamp":"2023-04-01 18:05:00","discussion":[{"comments":[{"timestamp":"1686558660.0","content":"upon reading this i think you are right","upvote_count":"2","comment_id":"921252","poster":"mimzzz"}],"comment_id":"858087","upvote_count":"17","timestamp":"1680365100.0","content":"Selected Answer: D\nThe answer is incorrect. The correct answer is Delta Live Tables or (C)\nhttps://docs.databricks.com/delta-live-tables/expectations.html","poster":"XiltroX"},{"content":"Selected Answer: D\nDelta Live Tables is a declarative framework for building reliable, maintainable, and testable data processing pipelines. You define the transformations to perform on your data and Delta Live Tables manages task orchestration, cluster management, monitoring, data quality, and error handling.\n\nQuality is explicitly mentioned in the definition.","comment_id":"999903","poster":"DQCR","upvote_count":"8","timestamp":"1693947360.0"},{"upvote_count":"4","content":"Selected Answer: D\nDelta Live Tables (DLT) is designed for building and managing data pipelines with built-in support for data quality monitoring and enforcement. It allows data engineers to define expectations (data quality rules) and automatically track if the ingested data meets these expectations. If data fails the specified rules, DLT can log the errors and either reject or quarantine the data, depending on the configured behavior.","timestamp":"1731955680.0","comment_id":"1314149","poster":"806e7d2"},{"timestamp":"1723108800.0","poster":"80370eb","upvote_count":"2","comment_id":"1262423","content":"Selected Answer: D\nD. Delta Live Tables\n\nDelta Live Tables provides features for automating data quality monitoring and ensuring that the data in the pipeline meets certain quality standards. It allows you to define expectations and monitor data quality as part of the data pipeline."},{"timestamp":"1714289160.0","comment_id":"1203437","poster":"benni_ale","upvote_count":"1","content":"Selected Answer: D\ndelta live table"},{"comment_id":"1113647","poster":"SerGrey","content":"Selected Answer: D\nCorrect is D","upvote_count":"1","timestamp":"1704369960.0"},{"upvote_count":"1","comment_id":"1064830","poster":"awofalus","timestamp":"1699364700.0","content":"Selected Answer: D\nCorrect: D"},{"upvote_count":"1","timestamp":"1699364640.0","comment_id":"1064829","content":"Selected Answer: D\nD is correct","poster":"awofalus"},{"timestamp":"1693776840.0","comment_id":"997973","poster":"vctrhugo","content":"Selected Answer: D\nD. Delta Live Tables\n\nDelta Live Tables is a tool provided by Databricks that can help data engineers automate the monitoring of data quality. It is designed for managing data pipelines, monitoring data quality, and automating workflows. With Delta Live Tables, you can set up data quality checks and alerts to detect issues and anomalies in your data as it is ingested and processed in real-time. It provides a way to ensure that the data quality meets your desired standards and can trigger actions or notifications when issues are detected.\n\nWhile the other tools mentioned may have their own purposes in a data engineering environment, Delta Live Tables is specifically designed for data quality monitoring and automation within the Databricks ecosystem.","upvote_count":"3"},{"upvote_count":"1","comment_id":"946075","poster":"Atnafu","content":"D\nDelta Live Tables.\n\nDelta Live Tables is a tool that can be used to automate the process of monitoring the quality level of data in a data pipeline. Delta Live Tables provides a number of features that can be used to monitor data quality, including:\n\nData lineage: Delta Live Tables tracks the lineage of data as it flows through the data pipeline. This allows the data engineer to see where the data came from and how it has been transformed.\nData quality checks: Delta Live Tables allows the data engineer to define data quality checks that can be run on the data as it is ingested. These checks can be used to identify data that is not meeting the expected quality standards.\nAlerts: Delta Live Tables can be configured to send alerts when data quality checks fail. This allows the data engineer to be notified of potential problems with the data pipeline.","timestamp":"1688780700.0"},{"timestamp":"1683170460.0","comment_id":"889123","poster":"Majjjj","comments":[{"timestamp":"1683679380.0","upvote_count":"3","comment_id":"893514","poster":"Majjjj","content":"After reading docs and more investigation I think in the terms of managing the data quality D would be better answer"}],"upvote_count":"1","content":"Selected Answer: B\nThe data engineer can use the Data Explorer tool to monitor the quality level of the ingested data. Data Explorer is a feature of Databricks that provides data profiling and data quality metrics to monitor the health of data pipelines."},{"upvote_count":"1","comment_id":"861949","poster":"4be8126","timestamp":"1680688140.0","content":"Selected Answer: B\nB. Data Explorer can be used to monitor the quality level of data. It provides an interactive interface to analyze the data and define quality rules to identify issues. Data Explorer also offers automated validation rules that can be used to monitor data quality over time."}],"answer":"D","isMC":true,"url":"https://www.examtopics.com/discussions/databricks/view/104764-exam-certified-data-engineer-associate-topic-1-question-25/","question_id":88,"answers_community":["D (95%)","5%"]},{"id":"9EBLqDOP5oWGRtAo7Nqr","question_text":"A Delta Live Table pipeline includes two datasets defined using STREAMING LIVE TABLE. Three datasets are defined against Delta Lake table sources using LIVE TABLE.\nThe table is configured to run in Production mode using the Continuous Pipeline Mode.\nAssuming previously unprocessed data exists and all definitions are valid, what is the expected outcome after clicking Start to update the pipeline?","question_images":[],"question_id":89,"answer":"C","unix_timestamp":1680365520,"timestamp":"2023-04-01 18:12:00","answers_community":["C (100%)"],"discussion":[{"timestamp":"1703905020.0","comment_id":"1109353","upvote_count":"10","poster":"Garyn","content":"Selected Answer: C\nC. All datasets will be updated at set intervals until the pipeline is shut down. The compute resources will be deployed for the update and terminated when the pipeline is stopped.\n\nExplanation:\n\nContinuous Pipeline Mode in Production mode implies that the pipeline continuously processes incoming data updates at set intervals, ensuring the datasets are kept up-to-date as new data arrives.\nSince the pipeline is set to Continuous Pipeline Mode, it will keep running and updating the datasets until it is manually shut down.\nThe compute resources are allocated dynamically to process and update the datasets as needed, and they will be terminated when the pipeline is stopped or shut down.\nThis mode allows for real-time or near-real-time updates to the datasets from the streaming/live tables, ensuring that the data remains current and reflects the changes occurring in the data sources."},{"poster":"80370eb","timestamp":"1723109100.0","upvote_count":"1","content":"Selected Answer: C\nIn Continuous Pipeline Mode, Delta Live Tables processes data continuously and updates datasets at regular intervals. Compute resources are used to handle these updates and are terminated when the pipeline is stopped.","comment_id":"1262425"},{"comment_id":"1244550","upvote_count":"1","content":"Selected Answer: C\nCorrect answer is \"C\".","poster":"3fbc31b","timestamp":"1720468920.0"},{"timestamp":"1714289280.0","content":"Selected Answer: C\ndaje gianluca","upvote_count":"1","poster":"benni_ale","comment_id":"1203439"},{"upvote_count":"2","content":"Selected Answer: C\nCorrect is C","comment_id":"1113649","poster":"SerGrey","timestamp":"1704370080.0"},{"upvote_count":"1","content":"Selected Answer: C\nCorrect : C","comment_id":"1064838","poster":"awofalus","timestamp":"1699365000.0"},{"upvote_count":"4","timestamp":"1693776960.0","content":"Selected Answer: C\nC. All datasets will be updated at set intervals until the pipeline is shut down. The compute resources will be deployed for the update and terminated when the pipeline is stopped.\n\nIn the scenario described:\n\nThe Delta Live Table pipeline is configured in Production mode, which means it will continuously process data using the Continuous Pipeline Mode.\nThere are both STREAMING LIVE TABLE datasets and LIVE TABLE datasets defined.\nWhen you click Start to update the pipeline in Continuous Pipeline Mode:\n\nAll datasets, including both STREAMING LIVE TABLE and LIVE TABLE datasets, will be updated at set intervals.\nCompute resources will be deployed for the update, ensuring that the pipeline processes data.\nThe compute resources will be terminated when the pipeline is stopped or shut down.\nThis setup allows for continuous data processing while efficiently managing compute resources, and the pipeline can be stopped when no longer needed.","comment_id":"997975","poster":"vctrhugo"},{"poster":"Sandy_17","content":"Selected Answer: C\nAll datasets will be updated at set intervals until the pipeline is shut down. The compute resources will be deployed for the update and terminated when the pipeline is stopped.","upvote_count":"1","timestamp":"1692714480.0","comment_id":"987506"},{"poster":"say88","comment_id":"968112","upvote_count":"4","timestamp":"1690807260.0","content":"No answer is correct. Prod Continuous mode processes data at set intervals until pipe is shutdown. However, compute must be always-on and will not terminate. https://docs.databricks.com/delta-live-tables/updates.html#continuous-triggered","comments":[{"content":"You could be correct:\n\"Triggered pipelines can reduce resource consumption and expense since the cluster runs only long enough to execute the pipeline. However, new data won’t be processed until the pipeline is triggered. Continuous pipelines require an always-running cluster, which is more expensive but reduces processing latency.\"","timestamp":"1692690780.0","poster":"Inhaler_boy","comment_id":"987194","comments":[{"content":"Actually it could make A the correct answer?","poster":"Inhaler_boy","comment_id":"987196","timestamp":"1692690840.0","upvote_count":"2"}],"upvote_count":"1"}]},{"poster":"Atnafu","content":"C. \nAll datasets will be updated at set intervals until the pipeline is shut down. The compute resources will be deployed for the update and terminated when the pipeline is stopped.\n\nIn a Delta Live Table pipeline running in Continuous Pipeline Mode, when you click Start to update the pipeline, the following outcome is expected:\n\nAll datasets defined using STREAMING LIVE TABLE and LIVE TABLE against Delta Lake table sources will be updated at set intervals.\nThe compute resources will be deployed for the update process and will be active during the execution of the pipeline.\nThe compute resources will be terminated when the pipeline is stopped or shut down.\nThis mode allows for continuous and periodic updates to the datasets as new data arrives or changes in the underlying Delta Lake tables occur. The compute resources are provisioned and utilized during the update intervals to process the data and perform the necessary operations.","timestamp":"1688781360.0","comment_id":"946077","upvote_count":"2"},{"upvote_count":"3","poster":"chays","comment_id":"912061","content":"Selected Answer: C\nAnswer: C","timestamp":"1685621640.0"},{"upvote_count":"2","comment_id":"888291","poster":"Er5","timestamp":"1683102180.0","content":"Answer: C\nPipeline mode - This specifies how the pipeline will be run. Choose the mode based on latency and cost requirements. \n* Triggered pipelines run once and then shut down until the next manual or scheduled update. \n* Continuous pipelines run continuously, ingesting new data as it arrives."},{"comment_id":"873827","timestamp":"1681833600.0","poster":"hrabiabw","content":"Answer: D\nOfficial Databricks practice exam with answers - question 36","upvote_count":"3","comments":[{"poster":"SHINGX","timestamp":"1681892700.0","comments":[{"upvote_count":"3","timestamp":"1682091780.0","poster":"hrabiabw","content":"Yes, You're right. Thanks.","comment_id":"876680"}],"comment_id":"874405","content":"Correct Answer is C. That question in the practice test is using a Triggered Pipeline. This question is using a Continuous.","upvote_count":"4"}]},{"timestamp":"1680365520.0","comment_id":"858091","poster":"XiltroX","content":"E is not the right answer. The correct answer is C\nhttps://www.databricks.com/product/delta-live-tables","upvote_count":"3"}],"exam_id":162,"answer_ET":"C","answer_images":[],"topic":"1","choices":{"B":"All datasets will be updated once and the pipeline will persist without any processing. The compute resources will persist but go unused.","A":"All datasets will be updated at set intervals until the pipeline is shut down. The compute resources will persist to allow for additional testing.","E":"All datasets will be updated once and the pipeline will shut down. The compute resources will persist to allow for additional testing.","D":"All datasets will be updated once and the pipeline will shut down. The compute resources will be terminated.","C":"All datasets will be updated at set intervals until the pipeline is shut down. The compute resources will be deployed for the update and terminated when the pipeline is stopped."},"url":"https://www.examtopics.com/discussions/databricks/view/104767-exam-certified-data-engineer-associate-topic-1-question-26/","answer_description":"","isMC":true},{"id":"x9SRErNiiTJ0su46Snx4","choices":{"A":"Checkpointing and Write-ahead Logs","E":"Checkpointing and Idempotent Sinks","C":"Replayable Sources and Idempotent Sinks","D":"Write-ahead Logs and Idempotent Sinks","B":"Structured Streaming cannot record the offset range of the data being processed in each trigger."},"answer":"A","exam_id":162,"answer_images":[],"question_text":"In order for Structured Streaming to reliably track the exact progress of the processing so that it can handle any kind of failure by restarting and/or reprocessing, which of the following two approaches is used by Spark to record the offset range of the data being processed in each trigger?","question_id":90,"url":"https://www.examtopics.com/discussions/databricks/view/104770-exam-certified-data-engineer-associate-topic-1-question-27/","discussion":[{"timestamp":"1741126620.0","poster":"shanksund","upvote_count":"2","comment_id":"1365133","content":"Selected Answer: A\nIdempotent sinks is for ensuring no duplicates, that is not what the question is asking"},{"timestamp":"1739734860.0","poster":"avidlearner","upvote_count":"3","comment_id":"1357354","content":"Selected Answer: A\nThe engine uses checkpointing and write-ahead logs to record the offset range of the data being processed in each trigger. The streaming sinks are designed to be idempotent for handling reprocessing. Together, using replayable sources and idempotent sinks, Structured Streaming can ensure end-to-end exactly-once semantics under any failure.\nURL for reference"},{"content":"Selected Answer: A\nA. \n\nReally don't understand people saying E.\n\nQuestion is: \n\n\"... which of the following two approaches is used by Spark **** TO RECORD **** the offset range of the data being processed\"\n\nTO RECORD is the key here.\n\nIdempotent sinks don't record anything. It's a feature to replay operations.\n\nRECORDS of the operations are created with Checkpoints and Write Ahead Logs.","timestamp":"1739106540.0","upvote_count":"2","poster":"MrCastro","comment_id":"1353911"},{"poster":"edaf08e","upvote_count":"1","content":"Selected Answer: E\nE. Checkpointing and Idempotent Sinks","comment_id":"1349523","timestamp":"1738329360.0"},{"timestamp":"1737564600.0","poster":"SatuPatu","comment_id":"1344879","upvote_count":"1","content":"Selected Answer: E\nIf failure by restarting and/or reprocessing then choose E\nIf the worker running the task crashes then choose A"},{"comment_id":"1316696","upvote_count":"1","poster":"Poutrata","timestamp":"1732373640.0","content":"Selected Answer: E\nE is correct"},{"poster":"NzmD","timestamp":"1732097220.0","content":"Selected Answer: E\nCorrect answer is E","upvote_count":"2","comment_id":"1315170"},{"content":"Selected Answer: E\nIn Structured Streaming, Spark uses the following two mechanisms to reliably track the progress of the stream and ensure fault tolerance:\n\nCheckpointing:\n\nSpark maintains metadata about the processing state, including the offset range of the data processed in each trigger. This metadata is stored in a reliable storage system like HDFS, AWS S3, or Azure Data Lake.\nIf a failure occurs, Spark can recover and resume processing from the last recorded state in the checkpoint.\nIdempotent Sinks:\n\nIdempotent sinks ensure that output operations (e.g., writing data to storage or a database) can be re-executed without causing duplicate data or errors.\nBy combining idempotent sinks with checkpointing, Spark ensures that reprocessing data due to a failure does not compromise data integrity.","poster":"806e7d2","upvote_count":"2","timestamp":"1731956460.0","comment_id":"1314161"},{"poster":"Colje","upvote_count":"3","timestamp":"1727439600.0","comment_id":"1290018","content":"Why the correct answer is E. Checkpointing and Idempotent Sinks:\nCheckpointing: Spark Structured Streaming uses checkpointing to track the state of the data being processed. Checkpoints allow the system to restart processing from where it left off in case of failure, ensuring reliability.\nIdempotent Sinks: Idempotent sinks ensure that reprocessing the same data multiple times (in case of a failure or restart) doesn’t lead to duplicate results. The sink can handle repeated writes of the same data without issues.\nWhy A. Checkpointing and Write-ahead Logs is incorrect:\nSpark Structured Streaming does not use Write-ahead Logs (WAL) for tracking offsets or ensuring fault tolerance. While WALs are used in some systems for durability, Spark Structured Streaming relies on checkpointing and the concept of idempotent operations to ensure consistency and fault tolerance."},{"content":"The correct answer is:\nE. Checkpointing and Idempotent Sinks\nIn Structured Streaming, Spark uses checkpointing to reliably track the progress of the streaming data. Checkpointing saves the state of the streaming computation to a reliable storage system. \nIdempotent sinks ensure that even if data is reprocessed, the results remain consistent and correct, preventing duplicate data from being written.","upvote_count":"2","comment_id":"1276080","poster":"CID2024","timestamp":"1725196500.0"},{"poster":"80370eb","upvote_count":"2","content":"Selected Answer: A\nCheckpointing: Spark saves metadata, including offsets, in a checkpoint directory, allowing it to recover from failures by replaying data starting from the last checkpoint.\nWrite-ahead Logs (WAL): Spark writes information about the data being processed to a log before the data is written to the sink. This ensures that even if a failure occurs, Spark can recover and reprocess the data from the log.","timestamp":"1723109280.0","comment_id":"1262428"},{"poster":"3fbc31b","timestamp":"1720468980.0","upvote_count":"1","content":"Selected Answer: A\nA is the correct answer.","comment_id":"1244552"},{"timestamp":"1715699100.0","poster":"squidy24","comment_id":"1211510","upvote_count":"3","content":"Selected Answer: A\nThe answer is A\n\n\"Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. ... Finally, the system ensures end-to-end exactly-once fault-tolerance guarantees through checkpointing and Write-Ahead Logs.\" - Apache Spark Structured Streaming Programming Guide"},{"content":"Nice information and i hope best [url=https://keensolution.in/data-visualization-services/]Data visualization agencies in India[/url]","poster":"keensolution","upvote_count":"1","comment_id":"1207177","timestamp":"1714972140.0"},{"upvote_count":"1","timestamp":"1714474980.0","poster":"bita7","content":"The answer is Checkpointing and idempotent sinks (E)\nHow does structured streaming achieves end to end fault tolerance:\n• First, Structured Streaming uses checkpointing and write-ahead logs to record the offset range of data being processed during each trigger interval.\n• Next, the streaming sinks are designed to be _idempotent_—that is, multiple writes of the same data (as identified by the offset) do not result in duplicates being written to the sink.\nTaken together, replayable data sources and idempotent sinks allow Structured Streaming to ensure end-to-end, exactly-once semantics under any failure condition","comment_id":"1204515"},{"content":"Selected Answer: A\n1 checkpointing and write ahead logs to record the offset range of data being processed \n2 checkpointing and idempotent sinks achieve end to end fault tolerance","poster":"benni_ale","comment_id":"1203440","timestamp":"1714289520.0","upvote_count":"3"},{"upvote_count":"1","comment_id":"1113651","timestamp":"1704370200.0","content":"Selected Answer: A\nCorrect is A","poster":"SerGrey"},{"comment_id":"1001544","poster":"juadaves","upvote_count":"3","content":"The answer is Checkpointing and idempotent sinks\n\n\n\nHow does structured streaming achieves end to end fault tolerance:\n\nFirst, Structured Streaming uses checkpointing and write-ahead logs to record the offset range of data being processed during each trigger interval.\n\nNext, the streaming sinks are designed to be _idempotent_—that is, multiple writes of the same data (as identified by the offset) do not result in duplicates being written to the sink.\n\nTaken together, replayable data sources and idempotent sinks allow Structured Streaming to ensure end-to-end, exactly-once semantics under any failure condition.","timestamp":"1694089560.0"},{"content":"Selected Answer: A\nA. Checkpointing and Write-ahead Logs\n\nTo reliably track the exact progress of processing and handle failures in Spark Structured Streaming, Spark uses both checkpointing and write-ahead logs. Checkpointing allows Spark to periodically save the state of the streaming application to a reliable distributed file system, which can be used for recovery in case of failures. Write-ahead logs are used to record the offset range of data being processed, ensuring that the system can recover and reprocess data from the last known offset in the event of a failure.","poster":"vctrhugo","timestamp":"1693778040.0","upvote_count":"2","comment_id":"997982"},{"comment_id":"957058","timestamp":"1689819840.0","upvote_count":"4","poster":"akk_1289","content":"A: \nThe engine uses checkpointing and write-ahead logs to record the offset range of the data being processed in each trigger.\n-- in the link search for \"The engine uses \" youll find the answer.\nhttps://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#:~:text=The%20engine%20uses%20checkpointing%20and,being%20processed%20in%20each%20trigger."},{"content":"A. Checkpointing and Write-ahead Logs.\n\nCheckpointing is a process of periodically saving the state of the streaming computation to a durable storage system. This ensures that if the streaming computation fails, it can be restarted from the last checkpoint and resume processing from where it left off.\nWrite-ahead logs are a type of log that records all changes made to a dataset. This allows Structured Streaming to recover from failures by replaying the write-ahead logs from the last checkpoint.","upvote_count":"3","poster":"Atnafu","comment_id":"946466","timestamp":"1688819820.0"},{"poster":"mimzzz","content":"why i think both A E are correct? https://learn.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-streaming-exactly-once#:~:text=Use%20idempotent%20sinks","comments":[{"upvote_count":"6","poster":"ZSun","content":"spark handle streaming failure through:\n1. track the progress/offset(This is option A)\n2. fix failure(This is option E)\nBut the question is \"two approaches ... record the offset range\"\nTherefore, A","comment_id":"922477","timestamp":"1686683880.0"}],"upvote_count":"2","comment_id":"921268","timestamp":"1686559980.0"},{"comment_id":"912063","timestamp":"1685621760.0","poster":"chays","upvote_count":"3","content":"Selected Answer: A\nAnswer is A:\nThe engine uses checkpointing and write-ahead logs to record the offset range of the data being processed in each trigger. The streaming sinks are designed to be idempotent for handling reprocessing. Together, using replayable sources and idempotent sinks, Structured Streaming can ensure end-to-end exactly-once semantics under any failure.\nhttps://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#:~:text=The%20engine%20uses%20checkpointing%20and,being%20processed%20in%20each%20trigger."},{"comment_id":"896633","timestamp":"1683977700.0","content":"Selected Answer: A\nAnswer is A.\nFrom Spark documentation: Every streaming source is assumed to have offsets to track the read position in the stream. The engine uses checkpointing and write-ahead logs to record the offset range of the data being processed in each trigger.","poster":"prasioso","upvote_count":"3"},{"comments":[{"timestamp":"1683171360.0","upvote_count":"1","poster":"Majjjj","content":"Answer is A:\nThe engine uses checkpointing and write-ahead logs to record the offset range of the data being processed in each trigger. The streaming sinks are designed to be idempotent for handling reprocessing. Together, using replayable sources and idempotent sinks, Structured Streaming can ensure end-to-end exactly-once semantics under any failure.\nhttps://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#:~:text=The%20engine%20uses%20checkpointing%20and,being%20processed%20in%20each%20trigger.","comment_id":"889129"}],"comment_id":"889126","timestamp":"1683171060.0","poster":"Majjjj","content":"Selected Answer: E\nE. Checkpointing and Idempotent Sinks are the two approaches used by Spark to record the offset range of the data being processed in each trigger, enabling Structured Streaming to reliably track the exact progress of the processing so that it can handle any kind of failure by restarting and/or reprocessing. Checkpointing periodically checkpoints the state of the streaming query to a fault-tolerant storage system, while idempotent sinks ensure that data can be written multiple times to the sink without affecting the final result.","upvote_count":"1"},{"comment_id":"861956","comments":[{"upvote_count":"5","content":"Wrong answer. Please check official databricks documentation to confirm that the right answer is A.","comment_id":"862270","timestamp":"1680710640.0","poster":"XiltroX"}],"timestamp":"1680689040.0","poster":"4be8126","upvote_count":"1","content":"Selected Answer: E\nThe answer is E. Checkpointing and Idempotent Sinks are used by Spark to record the offset range of the data being processed in each trigger. Checkpointing helps to recover the query from the point of failure and Idempotent Sinks ensure that the output of a streaming query is consistent even in the face of failures and retries."},{"timestamp":"1680365880.0","poster":"XiltroX","content":"Selected Answer: A\nE is a partial answer. The two correct answers are A and E. Structured streaming is important because it uses these two methods to make sure there is fault tolerance and Exactly-once guarantee of data","upvote_count":"4","comment_id":"858099"}],"isMC":true,"answers_community":["A (76%)","E (24%)"],"answer_description":"","unix_timestamp":1680365880,"timestamp":"2023-04-01 18:18:00","answer_ET":"A","question_images":[],"topic":"1"}],"exam":{"numberOfQuestions":169,"isImplemented":true,"name":"Certified Data Engineer Associate","isBeta":false,"lastUpdated":"12 Apr 2025","isMCOnly":true,"id":162,"provider":"Databricks"},"currentPage":18},"__N_SSP":true}