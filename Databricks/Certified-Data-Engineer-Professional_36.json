{"pageProps":{"questions":[{"id":"lZOpomtDJfrMn9QUlWqN","url":"https://www.examtopics.com/discussions/databricks/view/124597-exam-certified-data-engineer-professional-topic-1-question/","unix_timestamp":1698212040,"answer":"E","topic":"1","isMC":true,"question_id":176,"timestamp":"2023-10-25 07:34:00","answer_images":[],"answer_ET":"E","choices":{"A":"","B":"","D":"","E":"","C":""},"discussion":[{"poster":"benni_ale","comment_id":"1323161","timestamp":"1733586240.0","upvote_count":"2","content":"Selected Answer: E\nEvolve Schema = mergeSchema option is needed ; Incrementally = checkpointing is needed; Real-Time = WriteStream with default trigger . The only option that catches all of these is E"},{"timestamp":"1723384260.0","poster":"35fd6dd","upvote_count":"2","comment_id":"1264181","content":"Selected Answer: E\nwrite is not for spark streaming"},{"timestamp":"1716944640.0","poster":"Freyr","upvote_count":"4","comment_id":"1220639","content":"Selected Answer: E\nReference: https://docs.databricks.com/en/ingestion/auto-loader/schema.html\n\nwriteStream: Ensures real-time streaming write capabilities, which is essential f\nor near real-time workloads.\ncheckpointLocation: Necessary for fault tolerance and tracking progress.\nmergeSchema: Ensures automatic schema evolution, allowing new columns to be detected and added to the target table.\n\nWhy Option 'C ' is incorrect?\nUses write instead of writeStream, which is for batch processing, making it inappropriate for real-time streaming.\n\nWhy Option 'B ' is incorrect?\nAlthough it includes checkpointLocation and mergeSchema, the addition of trigger(once=True) is not necessary in this context, and it is better suited for batch-like processing.\n\nReference: https://docs.databricks.com/en/ingestion/auto-loader/schema.html"},{"upvote_count":"2","content":"Selected Answer: E\nstreamRead & StreamWrite shares the schema using checkpoint location\nso cloudFiles.schemaLocation needs to be same for checkpointLocation so that we dont need to specify it manually \nalso mergeSchema True make sure if any new column detected , it will be added in the target table \n\nhttps://docs.databricks.com/en/ingestion/auto-loader/schema.html","comment_id":"1169467","timestamp":"1709987340.0","poster":"vikram12apr"},{"timestamp":"1709600580.0","comment_id":"1166127","upvote_count":"2","poster":"hal2401me","content":"Selected Answer: E\nhttps://notebooks.databricks.com/demos/auto-loader/01-Auto-loader-schema-evolution-Ingestion.html"},{"poster":"aragorn_brego","upvote_count":"1","timestamp":"1700597700.0","comment_id":"1076631","content":"Selected Answer: E\nThis response correctly fills in the blank to meet the specified requirements of using Databricks Auto Loader for automatic schema detection and evolution in a near real-time streaming context."},{"poster":"AzureDE2522","timestamp":"1699600680.0","upvote_count":"3","comment_id":"1067007","content":"Selected Answer: E\nPlease refer: https://docs.databricks.com/en/ingestion/auto-loader/schema.html"},{"poster":"Dileepvikram","upvote_count":"1","comment_id":"1066273","content":"It does not mention to write as stream, it mentions to write incrementally, so option C looks correct for me","timestamp":"1699521600.0"},{"poster":"mouad_attaqi","comment_id":"1054657","timestamp":"1698330240.0","content":"Selected Answer: E\nCorrect answer is E, it is a streaming write, and the default outputMode is Append (so if it's optional in this case)","upvote_count":"2"},{"upvote_count":"1","comment_id":"1053478","content":"there is a type in the statement. Is it schema or checkpoint ?\nProvided answer is not correct. It has to be a writestream, with mode append","timestamp":"1698212040.0","poster":"sturcu"}],"answers_community":["E (100%)"],"question_images":["https://img.examtopics.com/certified-data-engineer-professional/image37.png"],"exam_id":163,"answer_description":"","question_text":"In order to facilitate near real-time workloads, a data engineer is creating a helper function to leverage the schema detection and evolution functionality of Databricks Auto Loader. The desired function will automatically detect the schema of the source directly, incrementally process JSON files as they arrive in a source directory, and automatically evolve the schema of the table when new fields are detected.\n\nThe function is displayed below with a blank:\n\n//IMG//\n\n\nWhich response correctly fills in the blank to meet the specified requirements?"},{"id":"TxiZ4DALxAD91qf40gNR","url":"https://www.examtopics.com/discussions/databricks/view/124598-exam-certified-data-engineer-professional-topic-1-question/","answer_description":"","answer_ET":"C","question_images":["https://img.examtopics.com/certified-data-engineer-professional/image43.png"],"isMC":true,"answer":"C","exam_id":163,"unix_timestamp":1698212160,"choices":{"E":"An incremental job will detect if new rows have been written to the silver_customer_sales table; if new rows are detected, all aggregates will be recalculated and used to overwrite the gold_customer_lifetime_sales_summary table.","D":"An incremental job will leverage running information in the state store to update aggregate values in the gold_customer_lifetime_sales_summary table.","C":"The gold_customer_lifetime_sales_summary table will be overwritten by aggregated values calculated from all records in the silver_customer_sales table as a batch job.","B":"A batch job will update the gold_customer_lifetime_sales_summary table, replacing only those rows that have different values than the current version of the table, using customer_id as the primary key.","A":"The silver_customer_sales table will be overwritten by aggregated values calculated from all records in the gold_customer_lifetime_sales_summary table as a batch job."},"answer_images":[],"answers_community":["C (100%)"],"question_text":"The data engineering team maintains the following code:\n\n//IMG//\n\n\nAssuming that this code produces logically correct results and the data in the source table has been de-duplicated and validated, which statement describes what will occur when this code is executed?","timestamp":"2023-10-25 07:36:00","discussion":[{"upvote_count":"7","poster":"aragorn_brego","timestamp":"1716315600.0","content":"Selected Answer: C\nThe code is performing a batch aggregation operation on the \"silver_customer_sales\" table grouped by \"customer_id\". It calculates the first and last transaction dates, the average sales, the total number of distinct orders, and the lifetime value of sales for each customer. The .mode(\"overwrite\") operation specifies that the output table \"gold_customer_lifetime_sales_summary\" should be overwritten with the result of this aggregation. This means that every time this code runs, it will replace the existing \"gold_customer_lifetime_sales_summary\" table with a new version that reflects the current state of the \"silver_customer_sales\" table.","comment_id":"1076635"},{"poster":"hal2401me","timestamp":"1725491280.0","content":"Selected Answer: C\nC. there's nowhere implicating streaming.","comment_id":"1166128","upvote_count":"1"},{"content":"C is the answer","comment_id":"1066277","upvote_count":"1","poster":"Dileepvikram","timestamp":"1715239320.0"},{"content":"Selected Answer: C\nCorrect Answer is C, it is an overwrite mode","comment_id":"1054660","timestamp":"1714141560.0","poster":"mouad_attaqi","upvote_count":"3"},{"content":"Selected Answer: C\nit does overwrite, so no incremental load","upvote_count":"4","comment_id":"1053479","timestamp":"1714023360.0","poster":"sturcu"}],"topic":"1","question_id":177},{"id":"rEbnCBpqJHz4ngwrdGXB","answer":"C","url":"https://www.examtopics.com/discussions/databricks/view/124599-exam-certified-data-engineer-professional-topic-1-question/","isMC":true,"topic":"1","exam_id":163,"answer_images":[],"answers_community":["C (100%)"],"unix_timestamp":1698212280,"answer_ET":"C","question_id":178,"answer_description":"","choices":{"A":"When a database is being created, make sure that the LOCATION keyword is used.","C":"When data is saved to a table, make sure that a full file path is specified alongside the Delta format.","B":"When configuring an external data warehouse for all table storage, leverage Databricks for all ELT.","D":"When tables are created, make sure that the EXTERNAL keyword is used in the CREATE TABLE statement.","E":"When the workspace is being configured, make sure that external cloud object storage has been mounted."},"timestamp":"2023-10-25 07:38:00","discussion":[{"upvote_count":"8","poster":"sturcu","comment_id":"1053480","timestamp":"1698212280.0","content":"Non of the provided.\nIt should be: When a table is created, make sure LOCATION is provided"},{"poster":"vctrhugo","content":"Selected Answer: C\nIn Delta Lake, an external (or unmanaged) table is a table created outside of the data lake but is still accessible from the data lake. The data for external tables is stored in a location specified by the user, not in the default directory of the data lake. When you save data to an external table, you need to specify the full file path where the data will be stored. This makes the table “external” because the data itself is not managed by Delta Lake, only the metadata is. This is why specifying a full file path alongside the Delta format when saving data to a table will ensure that the table is configured as an external Delta Lake table.","timestamp":"1707184080.0","upvote_count":"5","comment_id":"1141688"},{"timestamp":"1734081900.0","comment_id":"1326069","poster":"Sriramiyer92","content":"Selected Answer: C\nFolks note:\nWhile creating a table - Use of External keyword - Non Mandatory.\nMentioning Location and providing a path - Mandatory.\nIn option C, it is not mentioned explicitly that Location keyword is used. But since the path is provided.. implies the use of Location keyword indirectly. The devil is in the details. :)","upvote_count":"3"},{"poster":"hjy","content":"'create external table' statement is using in HIVE, so C is correct.","timestamp":"1726097700.0","upvote_count":"1","comment_id":"1282365"},{"timestamp":"1705915140.0","upvote_count":"2","content":"Selected Answer: C\nC is correct.","comment_id":"1128469","poster":"jkhan2405"},{"poster":"JamesWright","content":"C is correct","comment_id":"1092035","timestamp":"1702153200.0","upvote_count":"1"},{"timestamp":"1700598660.0","poster":"aragorn_brego","content":"Selected Answer: C\nHere's why the other options may not ensure the requirement is met:\nD. Delta Lake does not use the EXTERNAL keyword in the same way as some other SQL-based systems. In Delta Lake, whether a table is external is determined by where the data files are stored, not by a keyword in the CREATE TABLE statement.\n\n%sql\nCREATE TABLE f1_demo.results_external\nUSING DELTA\nLOCATION '/mnt/formula1dl/demo/results_external'","upvote_count":"3","comment_id":"1076640"},{"timestamp":"1699521840.0","content":"possible answer is C","comment_id":"1066286","upvote_count":"1","poster":"Dileepvikram"},{"comment_id":"1064017","upvote_count":"1","timestamp":"1699285920.0","poster":"Laraujo2022","comments":[{"upvote_count":"5","timestamp":"1700003640.0","content":"Not quite. Test & see. The tables are 'managed' though database creation has 'LOCATION' keyword. C is best.","comment_id":"1070932","poster":"60ties"}],"content":"I think it should be A because when a database is created using a location all tables within this database are automatically assign as unmanaged tables."},{"comment_id":"1057418","poster":"sturcu","upvote_count":"1","content":"Selected Answer: C\nprovide path (LOCATION)","timestamp":"1698649560.0"},{"poster":"mouad_attaqi","upvote_count":"1","content":"Selected Answer: C\nC is plausible answer, as in this case we are writing the data to an external location","comment_id":"1054665","timestamp":"1698330480.0"}],"question_images":[],"question_text":"The data architect has mandated that all tables in the Lakehouse should be configured as external (also known as \"unmanaged\") Delta Lake tables.\n\nWhich approach will ensure that this requirement is met?"},{"id":"uctSwbSCqG0NFxyKqKjp","exam_id":163,"answer":"B","answers_community":["B (100%)"],"topic":"1","answer_images":[],"timestamp":"2023-09-07 12:03:00","answer_description":"","question_text":"An upstream source writes Parquet data as hourly batches to directories named with the current date. A nightly batch job runs the following code to ingest all data from the previous day as indicated by the date variable:\n//IMG//\n\nAssume that the fields customer_id and order_id serve as a composite key to uniquely identify each order.\nIf the upstream system is known to occasionally produce duplicate entries for a single order hours apart, which statement is correct?","isMC":true,"url":"https://www.examtopics.com/discussions/databricks/view/120171-exam-certified-data-engineer-professional-topic-1-question-8/","discussion":[{"upvote_count":"18","poster":"Eertyy","timestamp":"1695307440.0","comment_id":"1013185","comments":[{"upvote_count":"1","poster":"meatpoof","comment_id":"1345713","timestamp":"1737674160.0","content":"The question doesn't say orders already exists. Arekm's answer is more correct"}],"content":"B. Each write to the orders table will only contain unique records, but newly written records may have duplicates already present in the target table.\n\nExplanation:\n\nIn the provided code, the .dropDuplicates([\"customer_id\",\"order_id\"]) operation is performed on the data loaded from the Parquet files. This operation ensures that only unique records, based on the composite key of \"customer_id\" and \"order_id,\" are retained in the DataFrame before writing to the \"orders\" table.\n\nHowever, this operation does not consider duplicates that may already exist in the \"orders\" table. It only filters duplicates from the current batch of data. If there are duplicates in the \"orders\" table from previous batches, they will remain in the table.\n\nSo, newly written records will not have duplicates within the batch being written, but duplicates from previous batches may still exist in the target table."},{"upvote_count":"2","content":"Selected Answer: B\nNo duplicates in the current batch - that is obvious. The duplicates may happen since the source occasionally produces duplicates hours apart. This means that one record can be generated by the source and processed on day 1, the duplicate on day 2. Since there is no logic checking if the corresponding record exists in the target - you get the duplicates there given we use append mode.","timestamp":"1735641060.0","comment_id":"1334738","poster":"arekm"},{"comment_id":"1323897","content":"Selected Answer: B\nyeah B is the correct answer cause in the current code it will look for duplicates in the currentDF based on composite keys and not for the duplicates which are already in the target table. if we want to insert for the rows which are not there in target table then we can make use of Merge Into statement of databricks.","timestamp":"1733726220.0","upvote_count":"1","poster":"Anithec0der"},{"content":"Selected Answer: B\nAppend method does not take in consideration any key in the target table, it simply add all rows of the input table to the target table.","comment_id":"1286674","upvote_count":"1","poster":"benni_ale","timestamp":"1726811340.0"},{"comment_id":"1236309","poster":"panya","upvote_count":"1","timestamp":"1719230400.0","content":"Yes it should be B"},{"poster":"imatheushenrique","upvote_count":"1","comment_id":"1224439","timestamp":"1717547460.0","content":"B. Each write to the orders table will only contain unique records, but newly written records may have duplicates already present in the target table.\n\nUsing merge this problem would not happen"},{"timestamp":"1709976480.0","poster":"DavidRou","comment_id":"1169377","upvote_count":"1","content":"Selected Answer: B\nB is the right answer. The above code only remove duplicates from the batch that is processed, no logic is applied to already saved records."},{"upvote_count":"1","content":"Selected Answer: B\nB is correct","poster":"Jay_98_11","timestamp":"1705148460.0","comment_id":"1121604"},{"upvote_count":"1","timestamp":"1704103140.0","content":"Selected Answer: B\nAnswer B","comment_id":"1111078","poster":"5ffcd04"},{"timestamp":"1703173380.0","comment_id":"1102689","poster":"kz_data","upvote_count":"1","content":"Selected Answer: B\nB is correct"},{"comment_id":"1084634","content":"correct B","timestamp":"1701367020.0","poster":"vivekla","upvote_count":"1"},{"comment_id":"1040242","upvote_count":"1","poster":"sturcu","content":"Selected Answer: B\nCorrect","timestamp":"1697005380.0"},{"timestamp":"1696879560.0","content":"Correct. B","poster":"Starvosxant","comment_id":"1038956","upvote_count":"1"},{"content":"Selected Answer: B\nCorrect","comment_id":"1001408","upvote_count":"2","poster":"thxsgod","timestamp":"1694080980.0"}],"answer_ET":"B","question_images":["https://img.examtopics.com/certified-data-engineer-professional/image7.png"],"unix_timestamp":1694080980,"question_id":179,"choices":{"A":"Each write to the orders table will only contain unique records, and only those records without duplicates in the target table will be written.","E":"Each write to the orders table will run deduplication over the union of new and existing records, ensuring no duplicate records are present.","B":"Each write to the orders table will only contain unique records, but newly written records may have duplicates already present in the target table.","C":"Each write to the orders table will only contain unique records; if existing records with the same key are present in the target table, these records will be overwritten.","D":"Each write to the orders table will only contain unique records; if existing records with the same key are present in the target table, the operation will fail."}},{"id":"qKkhM5fANqR6MXxhBbkC","question_id":180,"discussion":[{"timestamp":"1721975100.0","upvote_count":"1","content":"Selected Answer: A\nA is the simplest one","poster":"Hadiler","comment_id":"1255507"},{"comment_id":"1141686","upvote_count":"4","timestamp":"1707183840.0","content":"Selected Answer: A\nCreating a view is a simple and efficient way to provide access to a subset of data from a table. In this case, the view can be configured to include only the fields that have been approved for the sales team. Additionally, any fields that need to be renamed to match the sales team’s naming conventions can be aliased in the view. This approach does not require the creation of additional tables or the configuration of jobs to sync data, making it a relatively straightforward solution. However, it’s important to note that views do not physically store data, so any changes to the underlying marketing table will be reflected in the view. This means that the sales team will always have access to the most up-to-date approved data.","poster":"vctrhugo"},{"upvote_count":"1","comment_id":"1131748","content":"Selected Answer: A\nA is the simplest","timestamp":"1706192820.0","poster":"spaceexplorer"},{"upvote_count":"2","content":"Selected Answer: A\nLooks like A to me","timestamp":"1703869920.0","poster":"dmov","comment_id":"1108932"}],"timestamp":"2023-12-29 18:12:00","question_images":[],"choices":{"B":"Create a new table with the required schema and use Delta Lake's DEEP CLONE functionality to sync up changes committed to one table to the corresponding table.","C":"Use a CTAS statement to create a derivative table from the marketing table; configure a production job to propagate changes.","A":"Create a view on the marketing table selecting only those fields approved for the sales team; alias the names of any fields that should be standardized to the sales naming conventions.","E":"Instruct the marketing team to download results as a CSV and email them to the sales organization.","D":"Add a parallel table write to the current production pipeline, updating a new sales table that varies as required from the marketing table."},"answer_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/129728-exam-certified-data-engineer-professional-topic-1-question/","topic":"1","question_text":"The marketing team is looking to share data in an aggregate table with the sales organization, but the field names used by the teams do not match, and a number of marketing-specific fields have not been approved for the sales org.\n\nWhich of the following solutions addresses the situation while emphasizing simplicity?","exam_id":163,"answer_ET":"A","isMC":true,"answer_description":"","answers_community":["A (100%)"],"answer":"A","unix_timestamp":1703869920}],"exam":{"isBeta":false,"provider":"Databricks","name":"Certified Data Engineer Professional","isImplemented":true,"numberOfQuestions":200,"lastUpdated":"12 Apr 2025","id":163,"isMCOnly":true},"currentPage":36},"__N_SSP":true}