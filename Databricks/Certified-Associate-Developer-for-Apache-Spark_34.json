{"pageProps":{"questions":[{"id":"hcZcc6gy6vg2Hlm32sSM","answer":"E","answers_community":["E (100%)"],"unix_timestamp":1707487080,"answer_description":"","timestamp":"2024-02-09 14:58:00","topic":"1","choices":{"A":"storesDF.collect.foreach(assessPerformance(row))","C":"storesDF.collect.apply(row => assessPerformance(row))","B":"storesDF.collect().apply(assessPerformance)","E":"storesDF.collect.foreach(row => assessPerformance(row))","D":"storesDF.collect.map(assessPerformance(row))"},"discussion":[{"upvote_count":"1","comment_id":"1342656","content":"Selected Answer: E\nSCALA question!!","timestamp":"1737226080.0","poster":"bp_a_user"},{"content":"Selected Answer: E\nShort explanation below:\n\n - collect() retrieves all the rows of the DataFrame and returns them as an array.\n - foreach() applies the specified function to each element of the array.\nSo, in this case, foreach(row => assessPerformance(row)) applies the function assessPerformance() to each row of the DataFrame storesDF.","upvote_count":"2","poster":"azure_bimonster","comment_id":"1145526","timestamp":"1723204680.0"}],"question_images":[],"question_text":"Which of the following code blocks applies the function assessPerformance() to each row of DataFrame storesDF?","exam_id":161,"isMC":true,"question_id":166,"answer_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/133451-exam-certified-associate-developer-for-apache-spark-topic-1/","answer_ET":"E"},{"id":"koPh3kiItjqmW8G8xpvH","exam_id":161,"answer_images":[],"unix_timestamp":1690914840,"url":"https://www.examtopics.com/discussions/databricks/view/117004-exam-certified-associate-developer-for-apache-spark-topic-1/","topic":"1","question_text":"The code block shown below contains an error. The code block is intended to print the schema of DataFrame storesDF. Identify the error.\n\nCode block:\n\nstoresDF.printSchema.getAs[String]","discussion":[{"upvote_count":"1","timestamp":"1723204800.0","content":"Selected Answer: D\nstoresDF.printSchema() would be enough in this case","comment_id":"1145529","poster":"azure_bimonster"},{"poster":"cookiemonster42","comment_id":"969300","timestamp":"1706819640.0","upvote_count":"1","content":"Selected Answer: D\nquestion is formulated poorly, but most possibly it's D"}],"timestamp":"2023-08-01 20:34:00","answer":"D","answer_ET":"D","answer_description":"","answers_community":["D (100%)"],"question_id":167,"choices":{"D":"The printSchema member of DataFrame is an operation prints the DataFrame – there is no need to call getAs.","B":"There is no printSchema member of DataFrame – the schema() operation should be used instead.","A":"There is no printSchema member of DataFrame – the getSchema() operation should be used instead.","C":"The entire line needs to be a string – it should be wrapped by str().","E":"There is no printSchema member of DataFrame – schema and the print() function should be used instead."},"isMC":true,"question_images":[]},{"id":"oJlyValAx9ojpW8bqhZr","question_text":"Which of the following code blocks creates and registers a SQL UDF named \"ASSESS_PERFORMANCE\" using the Scala function assessPerformance() and applies it to column customerSatisfaction in table stores?","exam_id":161,"question_images":[],"question_id":168,"discussion":[{"poster":"nadegetiedjo","comments":[{"timestamp":"1719066960.0","upvote_count":"1","content":"Option E is incorrect because it attempts to use the SQL UDF name directly in the DataFrame API, which is not supported without using expr or callUDF.","poster":"carlosmps","comment_id":"1235441"}],"timestamp":"1714026780.0","content":"column customerSatisfaction haven't quoted in A\n so Ithink that the good answer is E","comment_id":"1201785","upvote_count":"1"},{"poster":"Sowwy1","comment_id":"1189851","timestamp":"1712311860.0","upvote_count":"2","content":"A is correct"}],"answers_community":[],"isMC":true,"url":"https://www.examtopics.com/discussions/databricks/view/137959-exam-certified-associate-developer-for-apache-spark-topic-1/","answer_images":[],"answer":"A","choices":{"C":"spark.udf.register(\"ASSESS_PERFORMANCE\", assessPerformance)\nspark.sql(\"SELECT customerSatisfaction, assessPerformance(customerSatisfaction) AS result FROM stores\")","B":"spark.udf.register(\"ASSESS_PERFORMANCE\", assessPerformance)","D":"spark.udf.register(\"ASSESS_PERFORMANCE\", assessPerformance)\nstoresDF.withColumn(\"result\", assessPerformance(col(\"customerSatisfaction\")))","E":"spark.udf.register(\"ASSESS_PERFORMANCE\", assessPerformance)\nstoresDF.withColumn(\"result\", ASSESS_PERFORMANCE(col(\"customerSatisfaction\")))","A":"spark.udf.register(\"ASSESS_PERFORMANCE\", assessPerformance)\nspark.sql(\"SELECT customerSatisfaction, ASSESS_PERFORMANCE(customerSatisfaction) AS result FROM stores\")"},"unix_timestamp":1712311860,"topic":"1","timestamp":"2024-04-05 12:11:00","answer_ET":"A","answer_description":""},{"id":"xHnRVM9ZhRZaLSH43avQ","answer_ET":"E","url":"https://www.examtopics.com/discussions/databricks/view/134412-exam-certified-associate-developer-for-apache-spark-topic-1/","topic":"1","question_text":"The code block shown below should use SQL to return a new DataFrame containing column storeId and column managerName from a table created from DataFrame storesDF. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.\n\nCode block:\n\n__1__.__2__(\"stores\")\n__3__.__4__(\"SELECT storeId, managerName FROM stores\")","question_images":[],"choices":{"B":"1. spark\n2. createTable\n3. storesDF\n4. sql","D":"1. spark\n2. createOrReplaceTempView\n3. storesDF\n4. sql","A":"1. spark\n2. createOrReplaceTempView\n3. storesDF\n4. query","E":"1. storesDF\n2. createOrReplaceTempView\n3. spark\n4. sql","C":"1. storesDF\n2. createOrReplaceTempView\n3. spark\n4. query"},"timestamp":"2024-02-23 03:23:00","isMC":true,"answer_description":"","answer_images":[],"exam_id":161,"discussion":[{"timestamp":"1744069800.0","poster":"PushpakKothekar","comment_id":"1558762","content":"Selected Answer: E\nAnswer is E instead D because \"store\" is created on dataframe = storeDF.","upvote_count":"1"},{"timestamp":"1724166540.0","comment_id":"1269568","poster":"6546a53","upvote_count":"1","content":"Selected Answer: E\nIt's E"},{"upvote_count":"1","timestamp":"1717524840.0","content":"Selected Answer: E\nIt's E","comment_id":"1224256","poster":"Samir_91"},{"poster":"Sowwy1","comment_id":"1189855","upvote_count":"1","timestamp":"1712311920.0","content":"It's E"},{"comment_id":"1187986","poster":"Sowwy1","timestamp":"1712056260.0","upvote_count":"3","content":"It's E.\n\ndf = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\ndf.createTempView(\"people\")\ndf2 = spark.sql(\"SELECT * FROM people\")"},{"upvote_count":"1","comments":[{"comment_id":"1224254","timestamp":"1717524780.0","upvote_count":"1","poster":"Samir_91","content":"spark.createOrReplaceTempView(\"stores\") gives an error \nAttributeError: 'SparkSession' object has no attribute 'createOrReplaceTempView'"},{"content":"no Bro\nhttps://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.createTempView.html check this E is correct only","poster":"naman405verma","timestamp":"1708794420.0","upvote_count":"2","comment_id":"1158056"}],"poster":"tangerine141","timestamp":"1708654980.0","comment_id":"1156845","content":"Selected Answer: D\nspark.createOrReplaceTempView(\"stores\")\nstoresDF.sql(\"SELECT storeId, managerName FROM stores\")"}],"unix_timestamp":1708654980,"question_id":169,"answers_community":["E (75%)","D (25%)"],"answer":"E"},{"id":"6hYE2s7JKH8t4zJCBAyO","url":"https://www.examtopics.com/discussions/databricks/view/116814-exam-certified-associate-developer-for-apache-spark-topic-1/","answer_images":[],"timestamp":"2023-07-30 20:33:00","isMC":true,"answer_ET":"B","question_id":170,"unix_timestamp":1690741980,"question_text":"The code block shown below contains an error. The code block intended to create a single-column DataFrame from Scala List years which is made up of integers. Identify the error.\n\nCode block:\n\nspark.createDataset(years)","topic":"1","discussion":[{"content":"Selected Answer: D\nD is the correct answer because the code creates a Dataset, not a DataFrame, and needs .toDF() to convert it.\nB is incorrect because the type is inferred automatically by Spark and you do not need to specify IntegerType explicitly.","upvote_count":"1","poster":"ARUNKUMARKRISHNASAMY","comment_id":"1387232","timestamp":"1741654860.0"},{"comment_id":"1363495","timestamp":"1740827520.0","content":"Selected Answer: D\nIn Scala with Spark, createDataset() creates a Dataset rather than a DataFrame. If we want a DataFrame instead of a Dataset, we need to call toDF().","poster":"Thameur01","upvote_count":"1"},{"comment_id":"1299679","upvote_count":"1","timestamp":"1729256520.0","poster":"bublitz","content":"Selected Answer: D\nIt should be D.\nScala has a createDataset function which returns a dataset - where then toDF has to be called.\nDoc: https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Dataset.html"},{"upvote_count":"2","poster":"Dharma49","content":"Correct answer is D:\nspark.createDataset() creates a Dataset, not a DataFrame.\nTo convert a Dataset to a DataFrame, you use toDF().","timestamp":"1721752500.0","comment_id":"1253776"},{"upvote_count":"1","content":"Since this is a scala question, the correct syntax would be :\nspark.createDataset(years).toDF(\"year\")\nbut that isn't one of the options","comment_id":"1239752","poster":"deadbeef38","timestamp":"1719766080.0"},{"content":"Official Databricks tests (where answer is A)\nQuestion 44 Which of the following code blocks creates a single-column DataFrame from Scala Listyears which is made up of integers? A. spark.createDataset(years).toDF B. spark.createDataFrame(years, IntegerType) C. spark.createDataset(years) D. spark.DataFrame(years, IntegerType) E. spark.createDataFrame(years)","comments":[{"poster":"Sowwy1","timestamp":"1712813880.0","upvote_count":"1","comment_id":"1193461","content":"Hence I'll go for D."}],"upvote_count":"1","timestamp":"1712813880.0","poster":"Sowwy1","comment_id":"1193460"},{"upvote_count":"1","comment_id":"1156852","poster":"tangerine141","timestamp":"1708655520.0","content":"Selected Answer: C\nC. There is no operation createDataset – the createDataFrame operation should be used instead.\n\nThe correct method to create a DataFrame in Spark using Scala is createDataFrame, not createDataset. The correct syntax would be:\n\nscala\nCopy code\nval df = spark.createDataFrame(years.map(Tuple1.apply)).toDF(\"columnName\")\nThis assumes that years is a List of integers, and the resulting DataFrame will have a single column named \"columnName\"."},{"poster":"zozoshanky","comment_id":"967409","content":"C is the answer","timestamp":"1690741980.0","upvote_count":"3"}],"answer":"D","choices":{"C":"There is no operation createDataset – the createDataFrame operation should be used instead.","D":"The result of the above is a Dataset rather than a DataFrame – the toDF operation must be called at the end.","E":"The column name must be specified as the second argument to createDataset.","B":"The data type is not specified – the second argument to createDataset should be IntegerType.","A":"The years list should be wrapped in another list like List(years) to make clear that it is a column rather than a row."},"exam_id":161,"answers_community":["D (75%)","C (25%)"],"question_images":[],"answer_description":""}],"exam":{"isBeta":false,"provider":"Databricks","name":"Certified Associate Developer for Apache Spark","numberOfQuestions":185,"id":161,"isImplemented":true,"lastUpdated":"12 Apr 2025","isMCOnly":true},"currentPage":34},"__N_SSP":true}