{"pageProps":{"questions":[{"id":"nXpfVxJb9ucmtzFhRX3l","url":"https://www.examtopics.com/discussions/databricks/view/154490-exam-certified-data-engineer-professional-topic-1-question/","question_text":"When scheduling Structured Streaming jobs for production, which configuration automatically recovers from query failures and keeps costs low?","exam_id":163,"topic":"1","choices":{"A":"Cluster: New Job Cluster;\nRetries: Unlimited;\nMaximum Concurrent Runs: 1","B":"Cluster: New Job Cluster;\nRetries: Unlimited;\nMaximum Concurrent Runs: Unlimited","D":"Cluster: New Job Cluster;\nRetries: None;\nMaximum Concurrent Runs: 1","C":"Cluster: Existing All-Purpose Cluster;\nRetries: Unlimited;\nMaximum Concurrent Runs: 1"},"unix_timestamp":1736854260,"timestamp":"2025-01-14 12:31:00","answer_images":[],"discussion":[{"comment_id":"1341676","content":"Selected Answer: A\nThe correct answer is A: The unlimited retries will take care of query failures while the max concurrent runs = 1 will keep the costs low","upvote_count":"1","timestamp":"1737028440.0","poster":"RandomForest"}],"question_images":[],"answers_community":["A (100%)"],"answer_ET":"A","isMC":true,"answer":"A","answer_description":"","question_id":91},{"id":"J518MSBKdYmq6kts6s7u","timestamp":"2025-01-16 12:55:00","isMC":true,"answer_images":[],"answer_description":"","question_images":["https://img.examtopics.com/certified-data-engineer-professional/image76.png"],"answer_ET":"A","question_text":"A Delta Lake table was created with the below query:\n\n//IMG//\n\n\nRealizing that the original query had a typographical error, the below code was executed:\n\nALTER TABLE prod.sales_by_stor RENAME TO prod.sales_by_store\n\nWhich result will occur after running the second command?","topic":"1","unix_timestamp":1737028500,"choices":{"C":"The table name change is recorded in the Delta transaction log.","B":"All related files and metadata are dropped and recreated in a single ACID transaction.","A":"The table reference in the metastore is updated.","D":"A new Delta transaction log is created for the renamed table."},"exam_id":163,"answers_community":["C (100%)"],"discussion":[{"upvote_count":"1","timestamp":"1741345980.0","poster":"lakime","content":"Selected Answer: C\nwhile the metastore is updated, the key mechanism for tracking changes in Delta Lake is the transaction log.","comment_id":"1366230"}],"answer":"C","url":"https://www.examtopics.com/discussions/databricks/view/154677-exam-certified-data-engineer-professional-topic-1-question/","question_id":92},{"id":"bwvxveoqZoTVk1M38Neu","timestamp":"2024-12-06 19:29:00","answer_images":[],"answers_community":["B (100%)"],"answer":"B","exam_id":163,"question_text":"The data engineering team has configured a Databricks SQL query and alert to monitor the values in a Delta Lake table. The recent_sensor_recordings table contains an identifying sensor_id alongside the timestamp and temperature for the most recent 5 minutes of recordings.\n\nThe below query is used to create the alert:\n\n//IMG//\n\n\nThe query is set to refresh each minute and always completes in less than 10 seconds. The alert is set to trigger when mean (temperature) > 120. Notifications are triggered to be sent at most every 1 minute.\n\nIf this alert raises notifications for 3 consecutive minutes and then stops, which statement must be true?","question_images":["https://img.examtopics.com/certified-data-engineer-professional/image77.png"],"answer_ET":"B","topic":"1","isMC":true,"answer_description":"","discussion":[{"comment_id":"1323181","upvote_count":"1","timestamp":"1733587860.0","poster":"Thameur01","content":"Selected Answer: B\nB, because avg temp is calculated by sensor_id and not total"},{"content":"Selected Answer: B\nB is the correct answer","timestamp":"1733515380.0","poster":"Ayomidetolu_A","upvote_count":"1","comment_id":"1322902"},{"comment_id":"1322878","poster":"e904bf4","content":"Selected Answer: B\nB is correct","upvote_count":"1","timestamp":"1733509740.0"}],"unix_timestamp":1733509740,"question_id":93,"choices":{"A":"The total average temperature across all sensors exceeded 120 on three consecutive executions of the query","D":"The maximum temperature recording for at least one sensor exceeded 120 on three consecutive executions of the query","C":"The source query failed to update properly for three consecutive minutes and then restarted","B":"The average temperature recordings for at least one sensor exceeded 120 on three consecutive executions of the query"},"url":"https://www.examtopics.com/discussions/databricks/view/152629-exam-certified-data-engineer-professional-topic-1-question/"},{"id":"qx4WW5rFC8h3DpzNFxaM","answer_ET":"D","timestamp":"2024-12-05 21:42:00","answer_images":[],"question_id":94,"url":"https://www.examtopics.com/discussions/databricks/view/152574-exam-certified-data-engineer-professional-topic-1-question/","question_images":[],"exam_id":163,"answer_description":"","answers_community":["D (86%)","14%"],"isMC":true,"answer":"D","unix_timestamp":1733431320,"choices":{"A":"\"Can Manage\" privileges on the required cluster","B":"Cluster creation allowed, \"Can Restart\" privileges on the required cluster","C":"Cluster creation allowed, \"Can Attach To\" privileges on the required cluster","D":"\"Can Restart\" privileges on the required cluster"},"question_text":"The Databricks workspace administrator has configured interactive clusters for each of the data engineering groups. To control costs, clusters are set to terminate after 30 minutes of inactivity. Each user should be able to execute workloads against their assigned clusters at any time of the day.\n\nAssuming users have been added to a workspace but not granted any permissions, which of the following describes the minimal permissions a user would need to start and attach to an already configured cluster.","topic":"1","discussion":[{"poster":"UrcoIbz","timestamp":"1734651660.0","comment_id":"1329197","content":"Selected Answer: D\n'can restart' privileges as is needed start and attach a notebook.\n\n'can attach to' is not having enough privileges to start a cluster.\n\nhttps://docs.databricks.com/en/security/auth/access-control/index.html#clusters","upvote_count":"4"},{"content":"Selected Answer: C\n\"Can Attach To\" Privileges:\n\nThe \"Can Attach To\" permission is sufficient for users to run workloads on an existing cluster. This permission allows users to attach notebooks or jobs to a cluster without needing additional management permissions.\nCluster Creation Allowed:\n\nAllowing cluster creation ensures users can create clusters if needed. However, for this scenario, this isn't mandatory because clusters are already configured. The user needs only \"Can Attach To\" privileges.","poster":"Thameur01","timestamp":"1734080820.0","upvote_count":"1","comment_id":"1326066"},{"timestamp":"1733431320.0","content":"Selected Answer: D\nCan restart privilages - it was even discussed","upvote_count":"2","comment_id":"1322518","poster":"temple1305"}]},{"id":"ejZvjeE9crg30V9dsovh","question_text":"The data science team has created and logged a production model using MLflow. The following code correctly imports and applies the production model to output the predictions as a new DataFrame named preds with the schema \"customer_id LONG, predictions DOUBLE, date DATE\".\n\n//IMG//\n\n\nThe data science team would like predictions saved to a Delta Lake table with the ability to compare all predictions across time. Churn predictions will be made at most once per day.\n\nWhich code block accomplishes this task while minimizing potential compute costs?","exam_id":163,"answer_ET":"A","answers_community":["A (100%)"],"answer_images":[],"choices":{"D":"","A":"preds.write.mode(\"append\").saveAsTable(\"churn_preds\")","B":"preds.write.format(\"delta\").save(\"/preds/churn_preds\")","C":""},"question_images":["https://img.examtopics.com/certified-data-engineer-professional/image78.png"],"discussion":[{"timestamp":"1736857680.0","comment_id":"1340342","upvote_count":"1","content":"Selected Answer: A\nbatch+append","poster":"_lene_"},{"content":"Selected Answer: A\nA is the right answer.","poster":"mouthwash","upvote_count":"1","comment_id":"1337401","timestamp":"1736211720.0"},{"comment_id":"1322521","upvote_count":"3","poster":"temple1305","timestamp":"1733431620.0","content":"Selected Answer: A\nYou need:\n- Batch operation since it is at most once a day\n- Append, since you need to keep track of past predictions\n\nA is the correct answer. You don't need to specify \"format\" when you use saveAsTable."}],"answer_description":"","timestamp":"2024-12-05 21:47:00","answer":"A","unix_timestamp":1733431620,"isMC":true,"question_id":95,"topic":"1","url":"https://www.examtopics.com/discussions/databricks/view/152575-exam-certified-data-engineer-professional-topic-1-question/"}],"exam":{"provider":"Databricks","name":"Certified Data Engineer Professional","isImplemented":true,"numberOfQuestions":200,"lastUpdated":"12 Apr 2025","id":163,"isMCOnly":true,"isBeta":false},"currentPage":19},"__N_SSP":true}