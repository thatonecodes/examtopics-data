{"pageProps":{"questions":[{"id":"YF4WqeajaJpoNXeCM6g2","exam_id":163,"unix_timestamp":1717053180,"discussion":[{"timestamp":"1735267740.0","content":"Answer is D. This is same question #63.\nDelta Lake stores its data in Parquet format. Parquet files include metadata (like row counts) that can be efficiently queried.\nFor a COUNT(*) query, Delta Lake does not need to scan all the data files; instead, it reads the row count information stored in the Parquet file metadata, making the operation faster.","comment_id":"1332189","upvote_count":"1","poster":"AlejandroU"},{"poster":"vexor3","upvote_count":"1","comment_id":"1251589","content":"Selected Answer: C\nC is correct","timestamp":"1721459700.0"},{"upvote_count":"1","content":"Selected Answer: C\nC is correct","timestamp":"1718287500.0","poster":"hpkr","comment_id":"1229872"},{"timestamp":"1717593660.0","upvote_count":"1","comment_id":"1224737","content":"Selected Answer: C\nDelta Lake optimizes COUNT(*) queries by reading the row counts stored in the Delta transaction log. This eliminates the need for a full table scan, resulting in significantly faster query performance.\n\npen_spark","poster":"BrianNguyen95"},{"timestamp":"1717262460.0","comment_id":"1222813","upvote_count":"1","content":"Selected Answer: C\nCorrect Answer: C","poster":"Freyr"},{"comment_id":"1221429","poster":"MDWPartners","timestamp":"1717053180.0","upvote_count":"2","content":"Selected Answer: C\nI would've said C"}],"question_id":26,"topic":"1","answer_images":[],"choices":{"C":"The total count of records is calculated from the Delta transaction logs","D":"The total count of records is calculated from the parquet file metadata","A":"The total count of rows is calculated by scanning all data files","B":"The total count of rows will be returned from cached results unless REFRESH is run"},"isMC":true,"timestamp":"2024-05-30 09:13:00","answer_ET":"C","question_text":"A Databricks SQL dashboard has been configured to monitor the total number of records present in a collection of Delta Lake tables using the following query pattern:\n\n\nSELECT COUNT (*) FROM table -\n\nWhich of the following describes how results are generated each time the dashboard is updated?","url":"https://www.examtopics.com/discussions/databricks/view/141583-exam-certified-data-engineer-professional-topic-1-question/","question_images":[],"answer":"C","answers_community":["C (100%)"],"answer_description":""},{"id":"G8gcPs6KJSuMozW7eCCo","exam_id":163,"isMC":true,"question_id":27,"answer_description":"","answer_images":[],"unix_timestamp":1717053240,"topic":"1","question_text":"A Delta Lake table was created with the below query:\n\n//IMG//\n\n\nConsider the following query:\n\n\nDROP TABLE prod.sales_by_store -\n\nIf this statement is executed by a workspace admin, which result will occur?","question_images":["https://img.examtopics.com/certified-data-engineer-professional/image52.png"],"discussion":[{"content":"Selected Answer: C\nno location keyword, so its a managed table","upvote_count":"1","comment_id":"1270633","poster":"robodog","timestamp":"1724323080.0"},{"timestamp":"1718388480.0","poster":"Isio05","content":"Selected Answer: C\nIt's managed table, so data will be also removed","comment_id":"1230608","upvote_count":"3"},{"comment_id":"1229873","upvote_count":"2","content":"Selected Answer: C\nC is correct","timestamp":"1718287500.0","poster":"hpkr"},{"poster":"hpkr","upvote_count":"1","timestamp":"1718150580.0","comment_id":"1228724","content":"Selected Answer: C\nOption C"},{"poster":"imatheushenrique","content":"C because its a managed table","upvote_count":"1","timestamp":"1717546080.0","comment_id":"1224425"},{"content":"Selected Answer: C\nCorrect Answer: C\nNo location provided in the table. So, it is a managed table. This will result in deleting the table meta data as well as table data.","timestamp":"1717262640.0","comment_id":"1222826","poster":"Freyr","upvote_count":"1"},{"content":"Selected Answer: C\nSeems C, it's a managed table","comment_id":"1221430","upvote_count":"1","timestamp":"1717053240.0","poster":"MDWPartners"}],"answers_community":["C (100%)"],"answer":"C","answer_ET":"C","url":"https://www.examtopics.com/discussions/databricks/view/141584-exam-certified-data-engineer-professional-topic-1-question/","timestamp":"2024-05-30 09:14:00","choices":{"C":"The table will be removed from the catalog and the data will be deleted.","B":"The table will be removed from the catalog but the data will remain in storage.","A":"Data will be marked as deleted but still recoverable with Time Travel.","D":"An error will occur because Delta Lake prevents the deletion of production data."}},{"id":"9fSEQNbJIEMf5YTWX6lB","question_text":"A developer has successfully configured their credentials for Databricks Repos and cloned a remote Git repository. They do not have privileges to make changes to the main branch, which is the only branch currently visible in their workspace.\n\nWhich approach allows this user to share their code updates without the risk of overwriting the work of their teammates?","answer_description":"","isMC":true,"answer":"A","answer_images":[],"answer_ET":"A","exam_id":163,"topic":"1","question_images":[],"unix_timestamp":1719226980,"discussion":[{"poster":"RyanAck24","content":"Selected Answer: A\nA seems correct","upvote_count":"2","comment_id":"1288813","timestamp":"1727218380.0"},{"timestamp":"1719226980.0","upvote_count":"3","content":"answer B","comment_id":"1236234","poster":"Ati1362"}],"question_id":28,"choices":{"D":"Use Repos to merge all differences and make a pull request back to the remote repository.","B":"Use Repos to create a fork of the remote repository, commit all changes, and make a pull request on the source repository.","A":"Use Repos to create a new branch, commit all changes, and push changes to the remote Git repository.","C":"Use Repos to pull changes from the remote Git repository; commit and push changes to a branch that appeared as changes were pulled."},"timestamp":"2024-06-24 13:03:00","url":"https://www.examtopics.com/discussions/databricks/view/142864-exam-certified-data-engineer-professional-topic-1-question/","answers_community":["A (100%)"]},{"id":"ealYjXDtXnbxXgTn81BN","isMC":true,"unix_timestamp":1717546140,"question_text":"The security team is exploring whether or not the Databricks secrets module can be leveraged for connecting to an external database.\n\nAfter testing the code with all Python variables being defined with strings, they upload the password to the secrets module and configure the correct permissions for the currently active user. They then modify their code to the following (leaving all other variables unchanged).\n\n//IMG//\n\n\nWhich statement describes what will happen when the above code is executed?","exam_id":163,"answer_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/databricks/view/141900-exam-certified-data-engineer-professional-topic-1-question/","question_images":["https://img.examtopics.com/certified-data-engineer-professional/image53.png"],"question_id":29,"timestamp":"2024-06-05 02:09:00","answer":"A","discussion":[{"upvote_count":"3","content":"Selected Answer: A\nA is the correct answer","timestamp":"1721988060.0","comment_id":"1255629","poster":"Hadiler"},{"timestamp":"1721460120.0","poster":"vexor3","comment_id":"1251590","upvote_count":"3","content":"Selected Answer: A\nA is correct"},{"upvote_count":"1","poster":"Deb9753","timestamp":"1717614300.0","comment_id":"1224944","content":"Answer A : When using Databricks secrets, the actual value of the secret is typically protected from being displayed in plain text. Databricks automatically redacts secret values when they are printed in the notebook. So, when you use the print(password) statement, the output will not show the actual password but will instead show [REDACTED]."},{"comment_id":"1224426","upvote_count":"1","timestamp":"1717546140.0","poster":"imatheushenrique","content":"A. A. The connection to the external table will succeed; the string \"REDACTED\" will be printed."}],"answers_community":["A (100%)"],"answer_description":"","choices":{"C":"An interactive input box will appear in the notebook; if the right password is provided, the connection will succeed and the password will be printed in plain text.","A":"The connection to the external table will succeed; the string \"REDACTED\" will be printed.","B":"An interactive input box will appear in the notebook; if the right password is provided, the connection will succeed and the encoded password will be saved to DBFS.","D":"The connection to the external table will succeed; the string value of password will be printed in plain text."},"answer_ET":"A"},{"id":"ag7Vcqvi3My9j2sKwPOy","discussion":[{"content":"Selected Answer: B\nB is the correct answer","comment_id":"1255630","poster":"Hadiler","upvote_count":"1","timestamp":"1721988120.0"},{"upvote_count":"1","timestamp":"1721460180.0","content":"Selected Answer: B\nB is correct","poster":"vexor3","comment_id":"1251591"},{"comment_id":"1222809","poster":"Freyr","content":"Selected Answer: B\nCorrect Answer: B\nThis option uses select to specify columns from the DataFrame and applies the model to the specified columns (columns). The output of the model is aliased as \"predictions\", which ensures the output DataFrame will have the column names \"customer_id\" and \"predictions\" with appropriate data types assuming the model returns a double type. This syntax aligns with PySpark's DataFrame transformations and is a typical way to apply a machine learning model to specific columns in Databricks.","timestamp":"1717262400.0","upvote_count":"3"}],"exam_id":163,"answer_ET":"B","topic":"1","question_text":"The data science team has created and logged a production model using MLflow. The model accepts a list of column names and returns a new column of type DOUBLE.\n\nThe following code correctly imports the production model, loads the customers table containing the customer_id key column into a DataFrame, and defines the feature columns needed for the model.\n\n//IMG//\n\n\nWhich code block will output a DataFrame with the schema \"customer_id LONG, predictions DOUBLE\"?","answer_images":[],"question_id":30,"unix_timestamp":1717262400,"answer_description":"","url":"https://www.examtopics.com/discussions/databricks/view/141740-exam-certified-data-engineer-professional-topic-1-question/","choices":{"A":"df.map(lambda x:model(x[columns])).select(\"customer_id, predictions\")","C":"model.predict(df, columns)","D":"df.apply(model, columns).select(\"customer_id, predictions\")","B":"df.select(\"customer_id\",\nmodel(*columns).alias(\"predictions\"))"},"question_images":["https://img.examtopics.com/certified-data-engineer-professional/image54.png"],"isMC":true,"answers_community":["B (100%)"],"timestamp":"2024-06-01 19:20:00","answer":"B"}],"exam":{"isImplemented":true,"numberOfQuestions":200,"isBeta":false,"lastUpdated":"12 Apr 2025","isMCOnly":true,"id":163,"name":"Certified Data Engineer Professional","provider":"Databricks"},"currentPage":6},"__N_SSP":true}