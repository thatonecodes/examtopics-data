{"pageProps":{"questions":[{"id":"gKtL1de255kVo17CZKaY","answer_description":"","answers_community":["D (100%)"],"answer":"D","exam_id":161,"url":"https://www.examtopics.com/discussions/databricks/view/107311-exam-certified-associate-developer-for-apache-spark-topic-1/","question_id":1,"question_images":[],"timestamp":"2023-04-24 14:14:00","question_text":"Which of the following describes the Spark driver?","isMC":true,"answer_images":[],"topic":"1","discussion":[{"timestamp":"1743753360.0","content":"Selected Answer: D\nThe Spark driver runs the main method of the Spark application and coordinates the execution of tasks across the cluster.","poster":"Saurabh_221b","comment_id":"1454739","upvote_count":"1"},{"timestamp":"1734040800.0","poster":"s127","comment_id":"1325886","content":"Selected Answer: D\nspark is responsible for executing the main method of program and also collaborates with cluster manager and worker nodes as well.","upvote_count":"1"},{"content":"Selected Answer: D\nThe Spark driver is responsible for orchestrating the execution of a Spark application, managing the SparkContext, and coordinating the execution of tasks across the Spark cluster. It does not perform the execution of tasks itself but rather schedules tasks on the worker nodes. The Spark driver is not fault-tolerant in the sense that if it fails, the entire Spark application usually fails. It also does not scale horizontally; only the executors (worker nodes) do that.","upvote_count":"1","timestamp":"1723651860.0","poster":"zic00","comment_id":"1265860"},{"poster":"Raheel_te","comment_id":"1236202","upvote_count":"1","content":"ignore my previous comment.\nE is the answer as per the sample exam in Databricks site.","timestamp":"1719221520.0"},{"poster":"Raheel_te","comment_id":"1236201","upvote_count":"1","timestamp":"1719221400.0","content":"B is the answer as per the sample exam in Databricks site."},{"comment_id":"1217984","timestamp":"1716611760.0","upvote_count":"1","content":"D is the answer","poster":"SnData"},{"content":"Answer -D","upvote_count":"1","timestamp":"1713748320.0","poster":"Vikram1710","comment_id":"1199901"},{"comment_id":"1049106","content":"Please let me know what are these dumps used for? \nScala - Spark or Python Spark?","upvote_count":"1","poster":"Pankaj_Shet","timestamp":"1697834040.0"},{"upvote_count":"4","timestamp":"1690786020.0","content":"Answer: D\nReceives the user's code and breaks it into tasks for execution.\nOrchestrates the execution plan and optimizes the Spark job.\nCoordinates with cluster managers to allocate resources for tasks.\nCollects and aggregates results from distributed workers.\nMaintains the metadata and state of the Spark application during its execution.","comment_id":"967833","poster":"singh100"},{"upvote_count":"4","timestamp":"1687013040.0","content":"The correct answer is D. The Spark driver is the program space in which the Spark application's main method runs, coordinating the entire Spark application.\n\nExplanation: The Spark driver is a program that runs the main method of a Spark application and coordinates the execution of the entire application. It is responsible for defining the SparkContext, which is the entry point for any Spark functionality. The driver program is responsible for dividing the Spark application into tasks, scheduling them on the cluster, and managing the overall execution. The driver communicates with the cluster manager to allocate resources and coordinate the distribution of tasks to the worker nodes. It also maintains the overall control and monitoring of the application. Horizontal scaling, fault tolerance, and execution modes are not directly related to the Spark driver.","comment_id":"926041","poster":"TmData"},{"comment_id":"879303","poster":"4be8126","content":"Selected Answer: D\nD. The Spark driver is the program space in which the Spark application’s main method runs coordinating the Spark entire application.\n\nThe Spark driver is responsible for coordinating the execution of a Spark application, and it runs in the program space where the Spark application's main method runs. It manages the scheduling, distribution, and monitoring of tasks across the cluster, and it communicates with the cluster manager to acquire resources and allocate them to the application. The driver also maintains the state of the application and collects results. It is not responsible for performing all execution in all execution modes, nor is it fault-tolerant or horizontally scalable.","timestamp":"1682338440.0","upvote_count":"3"}],"choices":{"E":"The Spark driver is horizontally scaled to increase overall processing throughput of a Spark application.","B":"The Spare driver is fault tolerant – if it fails, it will recover the entire Spark application.","A":"The Spark driver is responsible for performing all execution in all execution modes – it is the entire Spark application.","D":"The Spark driver is the program space in which the Spark application’s main method runs coordinating the Spark entire application.","C":"The Spark driver is the coarsest level of the Spark execution hierarchy – it is synonymous with the Spark application."},"unix_timestamp":1682338440,"answer_ET":"D"},{"id":"GbVvgSfWdNSXYFCExEL2","question_id":2,"topic":"1","answer_ET":"B","answer_images":[],"exam_id":161,"url":"https://www.examtopics.com/discussions/databricks/view/108386-exam-certified-associate-developer-for-apache-spark-topic-1/","answer":"B","answer_description":"","question_text":"Which of the following DataFrame operations is classified as a wide transformation?","answers_community":["B (100%)"],"isMC":true,"choices":{"C":"DataFrame.select()","B":"DataFrame.join()","E":"DataFrame.union()","A":"DataFrame.filter()","D":"DataFrame.drop()"},"question_images":[],"timestamp":"2023-05-03 12:28:00","unix_timestamp":1683109680,"discussion":[{"poster":"4be8126","content":"Selected Answer: B\nB. DataFrame.join() is classified as a wide transformation, as it shuffles the data across the network to perform the join operation.","comment_id":"888395","upvote_count":"5","timestamp":"1683109680.0"},{"poster":"Becida","timestamp":"1727736720.0","comment_id":"1291693","content":"Selected Answer: B\nJoin takes different partitions and combines into single dataframe.","upvote_count":"1"},{"poster":"TmData","content":"Selected Answer: B\nThe DataFrame operation classified as a wide transformation is:\n\nB. DataFrame.join()\n\nExplanation: In Spark, transformations are operations on DataFrames that create a new DataFrame from an existing one. Wide transformations involve shuffling or redistributing data across partitions and typically require data movement across the network. Among the options provided, DataFrame.join() is a wide transformation because it involves combining two DataFrames based on a common key column, which often requires shuffling and redistributing the data across partitions.","timestamp":"1687013640.0","comment_id":"926057","upvote_count":"3"},{"content":"Selected Answer: B\nNone of the other options are wide transformations, they are narrow (logically, they modify the length of a dataframe). Only a join can force shuffling of data between horizontally scaled partitions.","upvote_count":"3","comment_id":"898057","poster":"SonicBoom10C9","timestamp":"1684128840.0"}]},{"id":"z6v5G7mJXnDoSqArztEB","choices":{"B":"There is no alias() operation for the approx_count_distinct() operation's output.","C":"There is no way to return an exact distinct number in Spark because the data Is distributed across partitions.","D":"The approx_count_distinct()operation is not a standalone function - it should be used as a method from a Column object.","E":"The approx_count_distinct() operation cannot determine an exact number of distinct values in a column.","A":"The approx_count_distinct() operation needs a second argument to set the rsd parameter to ensure it returns the exact number of distinct values."},"unix_timestamp":1692207120,"question_id":3,"exam_id":161,"discussion":[{"content":"Selected Answer: A\nI can write approx_count_distinct(\"value\",0)","comments":[{"content":"I correct myself:\nmin value is 3.4E-5 --> so not possible!","timestamp":"1736885400.0","upvote_count":"1","poster":"bp_a_user","comment_id":"1340523"}],"poster":"bp_a_user","timestamp":"1736885340.0","upvote_count":"1","comment_id":"1340522"},{"comment_id":"1145577","timestamp":"1723207920.0","content":"Selected Answer: E\nstoresDF.agg(countDistinct(\"division\").alias(\"divisionDistinct\")) can give an exact distinct value unlike E option","upvote_count":"1","poster":"azure_bimonster"},{"upvote_count":"1","poster":"thanab","timestamp":"1710422700.0","comment_id":"1007527","content":"E\nThe error in the code block is that the approx_count_distinct() operation cannot determine an exact number of distinct values in a column."},{"content":"Selected Answer: E\ncan not get exact distinct using apox function","comments":[{"comment_id":"1066446","upvote_count":"1","timestamp":"1715255400.0","poster":"newusername","content":"agree, should be E"}],"comment_id":"982819","poster":"Ram459","upvote_count":"3","timestamp":"1708111920.0"}],"topic":"1","url":"https://www.examtopics.com/discussions/databricks/view/118281-exam-certified-associate-developer-for-apache-spark-topic-1/","answers_community":["E (80%)","A (20%)"],"question_images":[],"timestamp":"2023-08-16 19:32:00","answer":"E","answer_description":"","answer_images":[],"question_text":"The code block shown below contains an error. The code block is intended to return the exact number of distinct values in column division in DataFrame storesDF. Identify the error.\n\nCode block:\n\nstoresDF.agg(approx_count_distinct(col(“division”)).alias(“divisionDistinct”))","isMC":true,"answer_ET":"E"},{"id":"sxtZ3Pd9X5CF5g9QF2qE","answer_ET":"C","url":"https://www.examtopics.com/discussions/databricks/view/137684-exam-certified-associate-developer-for-apache-spark-topic-1/","answer_description":"","question_images":[],"topic":"1","answer_images":[],"exam_id":161,"question_text":"Which of the following code blocks returns the number of rows in DataFrame storesDF for each distinct combination of values in column division and column storeCategory?","answers_community":[],"choices":{"C":"storesDF.groupBy(“division”, “storeCategory”).count()","B":"storesDF.groupBy(division, storeCategory).count()","A":"storesDF.groupBy(Seq(col(“division”), col(“storeCategory”))).count()","D":"storesDF.groupBy(“division”).groupBy(“StoreCategory”).count()","E":"storesDF.groupBy(Seq(“division”, “storeCategory”)).count()"},"discussion":[{"comment_id":"1187310","upvote_count":"1","content":"C. storesDF.groupBy(“division”, “storeCategory”).count()","timestamp":"1727770920.0","poster":"Sowwy1"}],"isMC":true,"unix_timestamp":1711959720,"answer":"C","timestamp":"2024-04-01 10:22:00","question_id":4},{"id":"02GCN8iFZGKQNn5YoNoX","answers_community":["E (100%)"],"timestamp":"2024-04-02 13:22:00","answer_ET":"E","answer":"E","choices":{"B":"The describe() operation does not accept a Column object as an argument outside of a sequence — the sequence Seq(col(“sqft”)) should be specified instead.","D":"The describe()operation doesn't compute summary statistics for numeric columns — the summary() operation should be used instead.","E":"The describe()operation does not accept a Column object as an argument — the column name string “sqft” should be specified instead.","A":"The column sqft should be subsetted from DataFrame storesDF prior to computing summary statistics on it alone.","C":"The describe()operation doesn’t compute summary statistics for a single column — the summary() operation should be used instead."},"question_images":[],"question_text":"The code block shown below contains an error. The code block is intended to return a collection of summary statistics for column sqft in Data Frame storesDF. Identify the error.\n\nCode block:\n\nstoresDF.describes(col(“sgft”))","answer_description":"","isMC":true,"answer_images":[],"unix_timestamp":1712056920,"discussion":[{"poster":"ARUNKUMARKRISHNASAMY","upvote_count":"1","comment_id":"1365618","timestamp":"1741216560.0","content":"Selected Answer: E\nstoresDF.describe(\"sqft\")"},{"comment_id":"1189297","content":"Selected Answer: E\nE is the right choice.","timestamp":"1728043620.0","poster":"SaiPavan10","upvote_count":"2"},{"poster":"Sowwy1","content":"I think it's E. It should be storesDF.describe(\"sqft\")","timestamp":"1727868120.0","upvote_count":"2","comment_id":"1187989"}],"exam_id":161,"question_id":5,"topic":"1","url":"https://www.examtopics.com/discussions/databricks/view/137740-exam-certified-associate-developer-for-apache-spark-topic-1/"}],"exam":{"isMCOnly":true,"isImplemented":true,"name":"Certified Associate Developer for Apache Spark","id":161,"lastUpdated":"12 Apr 2025","provider":"Databricks","isBeta":false,"numberOfQuestions":185},"currentPage":1},"__N_SSP":true}