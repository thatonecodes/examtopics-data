{"pageProps":{"questions":[{"id":"xzqOsYdEqgkaLq4TLqBI","unix_timestamp":1712059740,"question_id":21,"timestamp":"2024-04-02 14:09:00","topic":"1","exam_id":161,"url":"https://www.examtopics.com/discussions/databricks/view/137759-exam-certified-associate-developer-for-apache-spark-topic-1/","answers_community":["C (100%)"],"answer_description":"","answer":"C","question_images":[],"answer_ET":"C","answer_images":[],"choices":{"B":"1. storesDF\n2. drop\n3. sqft, customerSatisfaction","E":"1. drop\n2. storesDF\n3. col(sqft), col(customerSatisfaction)","A":"1. drop\n2. storesDF\n3. col(“sqft”), col(“customerSatisfaction”)","D":"1. storesDF\n2. drop\n3. col(sqft), col(customerSatisfaction)","C":"1. storesDF\n2. drop\n3. “sqft”, “customerSatisfaction”"},"question_text":"The code block shown below should return a DataFrame containing all columns from DataFrame storesDF except for column sqft and column customerSatisfaction. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.\n\nCode block:\n\n__1__.__2__(__3__)","isMC":true,"discussion":[{"upvote_count":"1","poster":"SaiPavan10","content":"Selected Answer: C\nC is the right choice","comment_id":"1189332","timestamp":"1728046320.0"},{"comment_id":"1188022","content":"It's C","upvote_count":"1","poster":"Sowwy1","timestamp":"1727870940.0"}]},{"id":"G4e2KnLZDuU3K5OyIaBn","answer_images":[],"answers_community":["A (100%)"],"answer_description":"","url":"https://www.examtopics.com/discussions/databricks/view/117155-exam-certified-associate-developer-for-apache-spark-topic-1/","question_text":"Which of the following describes the difference between DataFrame.repartition(n) and DataFrame.coalesce(n)?","answer_ET":"A","unix_timestamp":1691002620,"timestamp":"2023-08-02 20:57:00","answer":"A","discussion":[{"poster":"58470e1","comment_id":"1312060","content":"Selected Answer: A\nA","upvote_count":"1","timestamp":"1731595080.0"},{"poster":"SaiPavan10","upvote_count":"2","timestamp":"1712235180.0","content":"Selected Answer: A\nA is the right choice","comment_id":"1189333"},{"poster":"siva1280","upvote_count":"2","timestamp":"1711845120.0","content":"A is correct","comment_id":"1186477"},{"timestamp":"1706872140.0","comment_id":"1138467","content":"It's A","upvote_count":"1","poster":"saryu"},{"upvote_count":"4","poster":"thanab","comment_id":"1003177","timestamp":"1694261400.0","content":"A\nThe correct answer is A. DataFrame.repartition(n) will split a DataFrame into n number of new partitions with data distributed evenly. DataFrame.coalesce(n) will more quickly combine the existing partitions of a DataFrame but might result in an uneven distribution of data across the new partitions. The `repartition()` method can be used to increase or decrease the number of partitions in a DataFrame, while the `coalesce()` method is used to only decrease the number of partitions in an efficient way². The `repartition()` method does a full shuffle and creates new partitions with data that's distributed evenly. On the other hand, `coalesce()` avoids a full shuffle by allowing only the reduction of partitions."},{"upvote_count":"2","comment_id":"970477","poster":"cookiemonster42","timestamp":"1691002620.0","content":"IMO it's A:\nB - repartition is less efficient because it involves shuffling - ->false\nC - same for the B reason --> false\nD - it's because of shuffling, not because of some column --> false\nE - coalesce if more fast --> false\nE -","comments":[{"poster":"gwq1968","content":"A is correct","comment_id":"973736","timestamp":"1691318640.0","upvote_count":"1"}]}],"choices":{"B":"While the results are similar, DataFrame.repartition(n) will be more efficient than DataFrame.coalesce(n) because it can partition a Data Frame by the column.","E":"DataFrame.repartition(n) will combine the existing partitions of a DataFrame but may result in an uneven distribution of data across the new partitions.\nDataFrame.coalesce(n) will more slowly split a Data Frame into n number of new partitions with data distributed evenly.","A":"DataFrame.repartition(n) will split a DataFrame into n number of new partitions with data distributed evenly.\nDataFrame.coalesce(n) will more quickly combine the existing partitions of a DataFrame but might result in an uneven distribution of data across the new partitions.","D":"While the results are similar, DataFrame.repartition(n) will be less efficient than DataFrame.coalesce(n) because it can partition a Data Frame by the column.","C":"DataFrame.repartition(n) will split a Data Frame into any number of new partitions while minimizing shuffling.\nDataFrame.coalesce(n) will split a DataFrame onto any number of new partitions utilizing a full shuffle."},"topic":"1","question_id":22,"isMC":true,"exam_id":161,"question_images":[]},{"id":"GOrK2Pw4MP0pVznRoVzJ","answer":"D","answer_images":[],"timestamp":"2023-11-09 19:32:00","isMC":true,"question_id":23,"unix_timestamp":1699554720,"topic":"1","question_text":"Which of the following cluster configurations is most likely to experience delays due to garbage collection of a large Dataframe?\n\n//IMG//\n\n\nNote: each configuration has roughly the same compute power using 100GB of RAM and 200 cores.","discussion":[{"upvote_count":"1","timestamp":"1740833220.0","content":"Selected Answer: D\nScenario #1 would most likely experience delays due to garbage collection because it has the largest heap space per executor, leading to longer garbage collection times when managing large DataFrame","comment_id":"1363527","poster":"Thameur01"},{"poster":"deadbeef38","upvote_count":"1","comment_id":"1235928","content":"Selected Answer: D\nI think it is D- scenario 1 because the other scenarios can take advantage of parallelism.","timestamp":"1719164040.0"},{"upvote_count":"1","comment_id":"1192268","poster":"Sowwy1","content":"I think it's D - Scenario 1\n\nScenario #1 would most likely experience delays due to garbage collection because it has the largest heap space per executor, leading to longer garbage collection times when managing large DataFrames.","timestamp":"1712664360.0"},{"timestamp":"1709207820.0","upvote_count":"1","content":"The answer is Scen 6 and than answer doesn´t appear, please align the answers","comment_id":"1162504","poster":"JuanitoFM"},{"content":"Please correct the question - answers alighment \nThe scenarious do not match \nthough I would say Scen 6 is the answer","comment_id":"1066619","upvote_count":"2","poster":"newusername","timestamp":"1699554720.0"}],"answer_ET":"D","exam_id":161,"answer_description":"","choices":{"E":"Scenario #2","C":"Scenario #4","D":"Scenario #1","B":"Scenario #5","A":"More information is needed to determine an answer."},"answers_community":["D (100%)"],"question_images":["https://img.examtopics.com/certified-associate-developer-for-apache-spark/image14.png"],"url":"https://www.examtopics.com/discussions/databricks/view/125680-exam-certified-associate-developer-for-apache-spark-topic-1/"},{"id":"w2YxnueTwdZzoEpopbHj","topic":"1","timestamp":"2023-03-25 11:42:00","answers_community":["E (100%)"],"question_images":[],"answer":"E","answer_description":"","answer_ET":"E","discussion":[{"poster":"TmData","comment_id":"926062","timestamp":"1687013760.0","upvote_count":"6","content":"Selected Answer: E\nOption E is incorrect because the driver program in Spark is not reassigned to another worker node if the driver's node fails. The driver program is responsible for the coordination and control of the Spark application and runs on a separate machine, typically the client machine or cluster manager. If the driver's node fails, the Spark application as a whole may fail or need to be restarted, but the driver is not automatically reassigned to another worker node."},{"upvote_count":"2","content":"Selected Answer: E\nIn Spark, the driver node is crucial for orchestrating the execution of the Spark application. If the driver node fails, the Spark application fails. Spark does not automatically reassign the driver to another node if the driver fails. This would require the application to be restarted manually or through external high-availability mechanisms.","timestamp":"1724244780.0","comment_id":"1270123","poster":"zic00"},{"timestamp":"1721697240.0","poster":"YoSpark","comment_id":"1253361","content":"Considering the word \"any set\" looks like A is not correct either. What if all the worker nodes fail.\nA- \"Spark is designed to support the loss of any set of worker nodes.\"","upvote_count":"1"},{"timestamp":"1695054420.0","poster":"Sonu124","upvote_count":"2","content":"Option E beacuse spark doesn't assiggned driver if faild","comment_id":"1010745"},{"timestamp":"1684128660.0","comment_id":"898056","poster":"SonicBoom10C9","upvote_count":"2","content":"Selected Answer: E\nThe driver is responsible for maintaining spark context. If it fails, there is no recourse. The driver can mitigate the failure of worker nodes through limited fault tolerance mechanisms."},{"upvote_count":"2","timestamp":"1683111540.0","poster":"4be8126","comment_id":"888418","content":"Selected Answer: E\nAll of the following statements about Spark's stability are correct except for:\n\nE. Spark will reassign the driver to a worker node if the driver’s node fails.\n\nThe driver is a special process in Spark that is responsible for coordinating tasks and executing the main program. If the driver fails, the entire Spark application fails and cannot be restarted. Therefore, Spark does not reassign the driver to a worker node if the driver's node fails."},{"upvote_count":"2","comment_id":"880654","content":"The E is only valid when spark-submit is in cluster modes","timestamp":"1682441640.0","comments":[{"poster":"raghavendra516","timestamp":"1721138880.0","content":"And it also depened on Resource Manger of cluser on which spark is running.","upvote_count":"1","comment_id":"1248975"}],"poster":"Indiee"},{"timestamp":"1679740920.0","comments":[{"comment_id":"860019","content":"If the node running the driver program fails, Spark's built-in fault-tolerance mechanism can reassign the driver program to run on another node.","timestamp":"1680532380.0","upvote_count":"1","poster":"TC007"}],"comment_id":"850033","poster":"GuidoDC","upvote_count":"3","content":"Selected Answer: E\nIf the driver node fails your cluster will fail. If the worker node fails, Databricks will spawn a new worker node to replace the failed node and resumes the workload."}],"isMC":true,"exam_id":161,"answer_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/103841-exam-certified-associate-developer-for-apache-spark-topic-1/","question_id":24,"question_text":"Which of the following statements about Spark’s stability is incorrect?","unix_timestamp":1679740920,"choices":{"B":"Spark will rerun any failed tasks due to failed worker nodes.","A":"Spark is designed to support the loss of any set of worker nodes.","C":"Spark will recompute data cached on failed worker nodes.","E":"Spark will reassign the driver to a worker node if the driver’s node fails.","D":"Spark will spill data to disk if it does not fit in memory."}},{"id":"zEcfOUhtAErJWm8vP09T","answer_ET":"A","answer_description":"","question_id":25,"question_text":"Which of the following DataFrame operations is classified as a transformation?","answer":"A","url":"https://www.examtopics.com/discussions/databricks/view/137563-exam-certified-associate-developer-for-apache-spark-topic-1/","answers_community":["A (100%)"],"choices":{"A":"DataFrame.select()","D":"DataFrame.first()","E":"DataFrame.collect()","B":"DataFrame.count()","C":"DataFrame.show()"},"exam_id":161,"timestamp":"2024-03-31 01:37:00","answer_images":[],"topic":"1","isMC":true,"unix_timestamp":1711845420,"discussion":[{"upvote_count":"1","poster":"SaiPavan10","comment_id":"1189336","timestamp":"1728046620.0","content":"Selected Answer: A\nA is the right choice"},{"comment_id":"1186478","timestamp":"1727653020.0","poster":"siva1280","upvote_count":"1","content":"It's A"}],"question_images":[]}],"exam":{"isMCOnly":true,"id":161,"lastUpdated":"12 Apr 2025","name":"Certified Associate Developer for Apache Spark","numberOfQuestions":185,"isImplemented":true,"provider":"Databricks","isBeta":false},"currentPage":5},"__N_SSP":true}