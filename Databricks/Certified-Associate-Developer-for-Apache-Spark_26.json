{"pageProps":{"questions":[{"id":"T5j0yfiwEYAKziFUJk1p","answers_community":["E (67%)","B (33%)"],"answer_description":"","isMC":true,"question_images":[],"answer":"E","exam_id":161,"answer_ET":"B","timestamp":"2023-04-13 20:53:00","unix_timestamp":1681411980,"topic":"1","question_text":"The code block shown below contains an error. The code block is intended to cache DataFrame storesDF only in Spark’s memory and then return the number of rows in the cached DataFrame. Identify the error.\nCode block:\nstoresDF.cache().count()","discussion":[{"content":"E is correct. You cannot set StorageLevel Memory_only with cache(), if memory available then it keeps everything into memory else it will spill to disk. To keep everything into Memory you need to use Persist() with Storage Level Memory only.","timestamp":"1690882680.0","poster":"singh100","comment_id":"968918","upvote_count":"5"},{"timestamp":"1728928980.0","content":"A because the default behavior of cache() is MEMORY_AND_DISK, and if you want MEMORY_ONLY, you must specify it explicitly","upvote_count":"1","comment_id":"1297770","poster":"sofiess"},{"poster":"azurearch","upvote_count":"1","comment_id":"1168284","content":"E is wrong. The cache() operation can only cache DataFrames at the MEMORY_AND_DISK level (the default) \nnote the use of 'only' here, cache can also store in disk if required. \n\nB is also wrong, there is no condition to set storagelevel prior to calling cache()\n\ncorrect answer is A.","timestamp":"1709837700.0"},{"timestamp":"1698940920.0","comment_id":"1060669","poster":"juliom6","upvote_count":"1","content":"Selected Answer: E\nE is correct!\n\nfrom pyspark.sql.types import IntegerType\nfrom pyspark import StorageLevel\n\nstoresDF = spark.createDataFrame([2023, 2024], IntegerType())\nprint(storesDF.persist(StorageLevel.MEMORY_ONLY).storageLevel)"},{"content":"E\ncache() -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n Persists the :class:`DataFrame` with the default storage level (`MEMORY_AND_DISK`).\n \n .. versionadded:: 1.3.0\n \n .. versionchanged:: 3.4.0\n Supports Spark Connect.\n \n Notes\n -----\n The default storage level has changed to `MEMORY_AND_DISK` to match Scala in 2.0.","timestamp":"1697727540.0","upvote_count":"1","poster":"juadaves","comment_id":"1048047"},{"timestamp":"1688898300.0","comment_id":"947103","content":"there are two options here: B and E. Who chose B => you can't explicitly set the storage level, it's a read-only property, so the correct answer is E.","poster":"ItsAB","upvote_count":"1"},{"upvote_count":"1","poster":"Jtic","content":"Selected Answer: E\nE\n\nB. The cache() operation caches DataFrames at the MEMORY_AND_DISK level by default – the storage level must be set via storesDF.storageLevel prior to calling cache().\n\nThis option is incorrect. The storage level does not need to be set via storesDF.storageLevel prior to calling cache(). The cache() operation can be used directly on the DataFrame without explicitly setting the storage level.\n\nE. The cache() operation can only cache DataFrames at the MEMORY_AND_DISK level (the default) – persist() should be used instead.\n\nThis option is the correct answer. The error in the code block is that the cache() operation is used instead of persist(). While cache() caches DataFrames at the default MEMORY_AND_DISK level, persist() provides more flexibility by allowing different storage levels to be specified, such as MEMORY_ONLY for caching only in memory. Therefore, persist() should be used instead of cache() to achieve the desired caching behavior.","comment_id":"908220","timestamp":"1685231280.0"},{"comments":[{"timestamp":"1686069180.0","upvote_count":"3","content":"Wrong explanation. you can call cache() or persist() without set storage level, it will use default Memoery_and_disk. \nYou clearly misunderstand the question itself. storesDF.cache().count() is a workable code, but fail the requirement. This is the issue.\nThe question asked \"only in memory\", that means, if the data size is out of the memory, i do not want to store it in disk, but rather recompute. Therefore, you need to specifically set the storage level as \"MEMORY ONLY\".\nA is correct","poster":"ZSun","comment_id":"916468"}],"comment_id":"888312","poster":"4be8126","content":"Selected Answer: B\nB. The cache() operation caches DataFrames at the MEMORY_AND_DISK level by default – the storage level must be set via storesDF.storageLevel prior to calling cache().\n\nThe storage level of a DataFrame cache can be specified as an argument to the cache() operation, but if the storage level has not been specified, the default MEMORY_AND_DISK level is used. Therefore, option A is incorrect.\n\nOption C is incorrect because caching and checkpointing are different operations in Spark. Caching stores a DataFrame in memory or on disk, while checkpointing saves a DataFrame to a reliable storage system like HDFS, which is necessary for iterative computations.\n\nOption D is incorrect because DataFrames can be cached in memory or on disk using the cache() operation.\n\nOption E is incorrect because cache() is the recommended method for caching DataFrames in Spark, and it supports caching at all storage levels, including MEMORY_ONLY. The persist() operation can be used to specify a storage level, but cache() is simpler and more commonly used.","timestamp":"1683103860.0","upvote_count":"1"},{"comment_id":"869679","poster":"peekaboo15","upvote_count":"2","timestamp":"1681411980.0","content":"The answer should be E. See this post for reference https://stackoverflow.com/questions/26870537/what-is-the-difference-between-cache-and-persist","comments":[{"content":"You should use storesDF.persist(StorageLevel.MEMORY_ONLY).count()","comment_id":"1048048","upvote_count":"1","timestamp":"1697727600.0","poster":"juadaves"}]}],"question_id":126,"url":"https://www.examtopics.com/discussions/databricks/view/106137-exam-certified-associate-developer-for-apache-spark-topic-1/","choices":{"E":"The cache() operation can only cache DataFrames at the MEMORY_AND_DISK level (the default) – persist() should be used instead.","C":"The storesDF DataFrame has not been checkpointed – it must have a checkpoint in order to be cached.","B":"The cache() operation caches DataFrames at the MEMORY_AND_DISK level by default – the storage level must be set via storesDF.storageLevel prior to calling cache().","A":"The cache() operation caches DataFrames at the MEMORY_AND_DISK level by default – the storage level must be specified to MEMORY_ONLY as an argument to cache().","D":"DataFrames themselves cannot be cached – DataFrame storesDF must be cached as a table."},"answer_images":[]},{"id":"W76ObvLlgHsFQVkC0954","answer_description":"","topic":"1","answer":"D","isMC":true,"url":"https://www.examtopics.com/discussions/databricks/view/108372-exam-certified-associate-developer-for-apache-spark-topic-1/","question_images":[],"choices":{"E":"storesDF.rdd.getNumPartitions()","B":"storesDF.repartition(1)","A":"storesDF.intersect()","D":"storesDF.coalesce(1)","C":"storesDF.union()"},"answer_images":[],"question_id":127,"timestamp":"2023-05-03 10:53:00","question_text":"Which of the following operations can be used to return a new DataFrame from DataFrame storesDF without inducing a shuffle?","discussion":[{"content":"Selected Answer: D\nC is not operation","upvote_count":"1","comment_id":"1326045","timestamp":"1734074400.0","poster":"mineoolee"},{"timestamp":"1725728880.0","content":"Though Union does not cause a shuffle, you need another dataframe to do union. in this question its limited to storesDF. coalesce(1) is the correct answer, as it does not cause shuffle rather combines multiple partitions into 1, i.e. reducing partitions = no shuffle.\nexecute storedDF.coalesce(1) and check DAG","comment_id":"1168290","upvote_count":"3","poster":"azurearch"},{"content":"Answer C : union\nNarrow transformation - all transformation logic performed within one partition\nWide transformations - transformation during which is needed shuffle/exchange, distribution of data to other partitions\nUnion is narrow transaction","upvote_count":"1","timestamp":"1724419440.0","poster":"Ahlo","comment_id":"1157263"},{"timestamp":"1714918860.0","content":"Selected Answer: C\nunion is the only operation from mentioned here that won't do shuffling. And as @ZSun mentioned, do not follow any of the 4be8126 answers, they are all blindly from GPT","comment_id":"1063108","poster":"newusername","upvote_count":"2"},{"upvote_count":"1","poster":"issibra","content":"C is the correct\ncoalesce may induce a partial shuffle","timestamp":"1709236500.0","comment_id":"993375"},{"poster":"ZSun","comment_id":"917647","content":"I think this question contains error, it should not be which one without shuffle, it should be which one cause shuffle.\nunion is a narrow transformation, not causing shuffle.\ncoalesce simply combine partitions together into one, not shuffle them.\nrdd.getNumPartitions just evaluate the number of partition of a dataframe, no shuffle.\neven for repartition(1), since there is only one partition in the end, it also not causing shuffle, it simply combine all partition together.\nTherefore, it should be A, this is the only one inducing a shuffle.\nor, B C D E without inducing a shuffle","timestamp":"1701994920.0","upvote_count":"3"},{"comments":[{"content":"The problem with Union answer is that it returns an error if we run it without arg.","timestamp":"1723289640.0","upvote_count":"2","comment_id":"1146331","poster":"wlademaro"},{"comment_id":"917645","content":"This is incorrect explanation, delete it","timestamp":"1701994500.0","upvote_count":"5","poster":"ZSun"}],"content":"The Answer is C. Union rather than coalesce.\nUnion is a narrow transformation. unlike wide transformationl, narrow transformation does not require shuffle.\nCoalesce is wide transformation, combine multiple partition to smaller number of partition. Don't this process require shuffling partition together?\nif you ask ChatGPT, it will tell you what 4be8126 comment.","timestamp":"1701993300.0","poster":"ZSun","upvote_count":"2","comment_id":"917637"},{"timestamp":"1699008780.0","poster":"4be8126","content":"Selected Answer: D\nThe correct answer is D. coalesce() can be used to return a new DataFrame with a reduced number of partitions, without inducing a shuffle.\n\nA shuffle is an expensive operation that involves the redistribution of data across a cluster, so it's important to minimize its use whenever possible. In this case, repartition() and union() both involve shuffles, while intersect() returns only the common rows between two DataFrames, and rdd.getNumPartitions() returns the number of partitions in the RDD underlying the DataFrame.","comment_id":"888316","upvote_count":"3"}],"answer_ET":"D","answers_community":["D (67%)","C (33%)"],"exam_id":161,"unix_timestamp":1683103980},{"id":"f8SzuuFJoRKuCt7rWRPU","answers_community":["B (100%)"],"url":"https://www.examtopics.com/discussions/databricks/view/108373-exam-certified-associate-developer-for-apache-spark-topic-1/","answer_description":"","question_text":"The code block shown below contains an error. The code block is intended to return a new 12-partition DataFrame from the 8-partition DataFrame storesDF by inducing a shuffle. Identify the error.\nCode block:\nstoresDF.coalesce(12)","discussion":[{"timestamp":"1728652860.0","comment_id":"1040680","poster":"Raju_Bhai","content":"with version 3.4.0, \n\ndf.repartition(12).coalesce(16).rdd.getNumPartitions() returns 12. it doesn't throw error, but only doesn't increase partition either","upvote_count":"1"},{"timestamp":"1714726560.0","comment_id":"888320","poster":"4be8126","upvote_count":"3","content":"Selected Answer: B\nThe correct answer is B.\n\nThe coalesce() operation can decrease the number of partitions but cannot increase the number of partitions. It also does not induce a shuffle, and is therefore more efficient when decreasing the number of partitions.\n\nIf the goal is to increase the number of partitions, repartition() should be used instead."}],"choices":{"D":"The coalesce() operation requires a column by which to partition rather than a number of partitions – the repartition() operation should be used instead.","B":"The coalesce() operation does not induce a shuffle and cannot increase the number of partitions – the repartition() operation should be used instead.","A":"The coalesce() operation cannot guarantee the number of target partitions – the repartition() operation should be used instead.","C":"The coalesce() operation will only work if the DataFrame has been cached to memory – the repartition() operation should be used instead.","E":"The number of resulting partitions, 12, is not achievable for an 8-partition DataFrame."},"answer":"B","question_id":128,"isMC":true,"exam_id":161,"answer_ET":"B","topic":"1","question_images":[],"timestamp":"2023-05-03 10:56:00","answer_images":[],"unix_timestamp":1683104160},{"id":"s2HPpVpowl6Zzj0dV73i","timestamp":"2023-05-03 10:59:00","choices":{"E":"spark.sql.adaptive.coalescePartitions.enabled","A":"spark.sql.shuffle.partitions","D":"spark.sql.inMemoryColumnarStorage.batchSize","B":"spark.sql.autoBroadcastJoinThreshold","C":"spark.sql.adaptive.skewJoin.enabled"},"answer_images":[],"answer_ET":"E","topic":"1","question_id":129,"isMC":true,"discussion":[{"poster":"juliom6","upvote_count":"1","timestamp":"1730916780.0","content":"Selected Answer: E\nhttps://spark.apache.org/docs/latest/sql-performance-tuning.html\n\nspark.sql.adaptive.coalescePartitions.enabled: When true and spark.sql.adaptive.enabled is true, Spark will coalesce contiguous shuffle partitions according to the target size (specified by spark.sql.adaptive.advisoryPartitionSizeInBytes), to avoid too many small tasks.","comment_id":"1064142"},{"timestamp":"1714726740.0","content":"Selected Answer: E\nThe answer is E. spark.sql.adaptive.coalescePartitions.enabled is the Spark property used to configure whether DataFrame partitions that do not meet a minimum size threshold are automatically coalesced into larger partitions during a shuffle. When set to true, Spark automatically coalesces partitions that are smaller than the configured minimum size into larger partitions to optimize shuffles.","upvote_count":"3","comment_id":"888323","poster":"4be8126"}],"unix_timestamp":1683104340,"answer":"E","question_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/108374-exam-certified-associate-developer-for-apache-spark-topic-1/","exam_id":161,"answers_community":["E (100%)"],"answer_description":"","question_text":"Which of the following Spark properties is used to configure whether DataFrame partitions that do not meet a minimum size threshold are automatically coalesced into larger partitions during a shuffle?"},{"id":"cqby9eSBvpIUsfVbTlVD","answer_ET":"A","answer_description":"","question_text":"The code block shown below contains an error. The code block is intended to return a DataFrame containing a column openDateString, a string representation of Java’s SimpleDateFormat. Identify the error.\nNote that column openDate is of type integer and represents a date in the UNIX epoch format – the number of seconds since midnight on January 1st, 1970.\nAn example of Java’s SimpleDateFormat is \"Sunday, Dec 4, 2008 1:05 PM\".\nA sample of storesDF is displayed below:\n//IMG//\n\nCode block:\nstoresDF.withColumn(\"openDateString\", from_unixtime(col(\"openDate\"), \"EEE, MMM d, yyyy h:mm a\", TimestampType()))","isMC":true,"exam_id":161,"timestamp":"2023-05-28 01:56:00","answer":"A","topic":"1","answer_images":[],"unix_timestamp":1685231760,"url":"https://www.examtopics.com/discussions/databricks/view/110402-exam-certified-associate-developer-for-apache-spark-topic-1/","answers_community":["A (75%)","B (25%)"],"discussion":[{"upvote_count":"2","content":"Selected Answer: A\nA is correct:\n\nfrom pyspark.sql.functions import from_unixtime, col\n\nstoresDF = spark.createDataFrame([(0, 1100746394), (1, 1474410343)], ['storeId', 'openDate'])\nstoresDF = storesDF.withColumn(\"openDateString\", from_unixtime(col(\"openDate\"), \"EEE, MMM d, yyyy h:mm a\"))\ndisplay(storesDF)","poster":"juliom6","timestamp":"1731596640.0","comment_id":"1070530"},{"poster":"nicklasbekkevold","timestamp":"1724394480.0","upvote_count":"1","comment_id":"987993","content":"Selected Answer: A\nA is the right answer. \n\nFunction signature from the docs:\npyspark.sql.functions.from_unixtime(timestamp, format='uuuu-MM-dd HH:mm:ss')"},{"poster":"zozoshanky","comment_id":"967286","content":"A is also right.","timestamp":"1722357120.0","upvote_count":"2"},{"poster":"Jtic","comments":[{"timestamp":"1731596580.0","poster":"juliom6","comment_id":"1070527","upvote_count":"1","content":"That not make sense, the code below works perfectly:\n\nfrom pyspark.sql.functions import from_unixtime, col\n\nstoresDF = spark.createDataFrame([(0, 1100746394), (1, 1474410343)], ['storeId', 'openDate'])\nstoresDF = storesDF.withColumn('openDate', col('openDate').cast('integer'))\nstoresDF = storesDF.withColumn(\"openDateString\", from_unixtime(col(\"openDate\"), \"EEE, MMM d, yyyy h:mm a\"))\ndisplay(storesDF)"},{"timestamp":"1717690680.0","content":"This is completely nonsense about long and integer.\nlong (or bigint): It is a 64-bit signed integer data type anging from -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807. \ninteger (or int): It is a 32-bit signed integer data ranging from -2,147,483,648 to 2,147,483,647","comment_id":"916452","upvote_count":"3","poster":"ZSun"}],"upvote_count":"1","content":"Selected Answer: B\nB. The from_unixtime() operation only works if column openDate is of type long rather than integer - column openDate must first be converted.\n\nThis option is correct. The code block has an error because the from_unixtime() function expects the column openDate to be of type long, not integer. The column should be cast to long before applying the function.","comment_id":"908223","timestamp":"1716854160.0"}],"question_images":["https://img.examtopics.com/certified-associate-developer-for-apache-spark/image6.png"],"question_id":130,"choices":{"A":"The from_unixtime() operation only accepts two parameters – the TimestampTime() arguments not necessary.","D":"The from_unixtime() operation automatically places the input column in java’s SimpleDateFormat – there is no need for a second or third argument.","C":"The second argument to from_unixtime() is not correct – it should be a variant of TimestampType() rather than a string.","E":"The column openDate must first be converted to a timestamp, and then the Date() function can be used to reformat to java’s SimpleDateFormat.","B":"The from_unixtime() operation only works if column openDate is of type long rather than integer – column openDate must first be converted."}}],"exam":{"provider":"Databricks","id":161,"numberOfQuestions":185,"isMCOnly":true,"lastUpdated":"12 Apr 2025","isImplemented":true,"name":"Certified Associate Developer for Apache Spark","isBeta":false},"currentPage":26},"__N_SSP":true}