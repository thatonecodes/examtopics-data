{"pageProps":{"questions":[{"id":"4NKjPBmfrlqrakYe8vIj","answer_description":"","choices":{"A":"trigger(availableNow=True)","C":"trigger(continuous= “once”)","D":"trigger(once=True)","B":"trigger(processingTime= “once”)"},"exam_id":162,"unix_timestamp":1730015340,"timestamp":"2024-10-27 08:49:00","isMC":true,"question_images":["https://img.examtopics.com/certified-data-engineer-associate/image68.png"],"url":"https://www.examtopics.com/discussions/databricks/view/150350-exam-certified-data-engineer-associate-topic-1-question-141/","question_id":46,"answer_ET":"A","answer_images":[],"topic":"1","answers_community":["A (100%)"],"discussion":[{"comment_id":"1303510","upvote_count":"6","content":"A is correct.","timestamp":"1730015340.0","poster":"comoon"},{"comment_id":"1315559","upvote_count":"1","timestamp":"1732143960.0","content":"Selected Answer: A\nA is correct","poster":"hakimipous"},{"timestamp":"1731598260.0","poster":"lj114","comment_id":"1312105","content":"Selected Answer: A\nA is correct.\nSimilar to queries one-time micro-batch trigger, the query will process all the available data and then stop on its own. The difference is that, it will process the data in (possibly) multiple micro-batches based on the source options","upvote_count":"1"},{"comment_id":"1306269","content":"Selected Answer: A\nCorrect is A","upvote_count":"2","timestamp":"1730569980.0","poster":"rsmf"}],"question_text":"A data engineer has configured a Structured Streaming job to read from a table, manipulate the data, and then perform a streaming write into a new table.\n\nThe code block used by the data engineer is below:\n\n//IMG//\n\n\nThe data engineer only wants the query to process all of the available data in as many batches as required.\n\nWhich line of code should the data engineer use to fill in the blank?","answer":"A"},{"id":"lOCLvpRH78btFq3qylst","question_images":[],"unix_timestamp":1730545680,"choices":{"A":"The pipeline can have different notebook sources in SQL & Python","C":"The pipeline will need to use a batch source in place of a streaming source","D":"The pipeline will need to be written entirely in Python","B":"The pipeline will need to be written entirely in SQL"},"exam_id":162,"isMC":true,"discussion":[{"poster":"MultiCloudIronMan","timestamp":"1734352920.0","upvote_count":"4","comment_id":"1327350","content":"Selected Answer: A\nThe correct answer is A. The pipeline can have different notebook sources in SQL & Python. Delta Live Tables supports both SQL and Python, as well as streaming and batch sources. Therefore, the existing pipeline can continue to use both SQL and Python for different layers without needing to be rewritten entirely in one language or switching from a streaming to a batch source."},{"poster":"Worldmaster","upvote_count":"1","content":"Selected Answer: A\nImho A is correct","comment_id":"1322474","timestamp":"1733422080.0"},{"poster":"rsmf","upvote_count":"1","timestamp":"1730570100.0","content":"Selected Answer: C\nC is correct","comment_id":"1306270"},{"poster":"comoon","timestamp":"1730545680.0","comment_id":"1306151","upvote_count":"1","content":"C is correct"}],"answer":"A","answers_community":["A (83%)","C (17%)"],"answer_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/150641-exam-certified-data-engineer-associate-topic-1-question-142/","answer_description":"","timestamp":"2024-11-02 12:08:00","answer_ET":"A","topic":"1","question_text":"Data engineer and data analysts are working together on a data pipeline. The data engineer is working on the raw, bronze, and silver layers of the pipeline using Python, and the data analyst is working on the gold layer of the pipeline using SQL. The raw source of the pipeline is a streaming input. They now want to migrate their pipeline to use Delta Live Tables.\n\nWhich of the following changes will need to be made to the pipeline when migrating to Delta Live Tables?","question_id":47},{"id":"BU8j5CQ8tTibup3aX43b","question_images":[],"unix_timestamp":1733061960,"choices":{"A":"An external table where the location is pointing to specific path in external location.","D":"A managed table where the location is pointing to specific path in external location.","C":"A managed table where the catalog has managed location pointing to specific path in external location.","B":"An external table where the schema has managed location pointing to specific path in external location."},"exam_id":162,"discussion":[{"timestamp":"1734353040.0","poster":"MultiCloudIronMan","comment_id":"1327351","content":"Selected Answer: A\nThe correct answer is A. An external table where the location is pointing to specific path in external location. This allows the data engineer to specify the exact path in an external location where the Parquet bronze table will be stored.","upvote_count":"1"},{"timestamp":"1733061960.0","content":"Selected Answer: A\nA. \nIt defines an external table where the data is stored at a specific path in an external location (such as cloud storage). The path is provided when the table is created. This matches the requirement to store the Parquet data in an external location, ensuring the data is managed externally, which is the core characteristic of an external table.","upvote_count":"1","poster":"Worldmaster","comment_id":"1320584"}],"isMC":true,"answer":"A","answers_community":["A (100%)"],"answer_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/152431-exam-certified-data-engineer-associate-topic-1-question-143/","answer_description":"","timestamp":"2024-12-01 15:06:00","answer_ET":"A","topic":"1","question_text":"Identify a scenario to use an external table.\nA Data Engineer needs to create a parquet bronze table and wants to ensure that it gets stored in a specific path in an external location.\n\nWhich table can be created in this scenario?","question_id":48},{"id":"mXlJboKZlW30J7WL8q9J","exam_id":162,"question_images":[],"answer_ET":"B","answers_community":["B (71%)","D (29%)"],"answer_images":[],"isMC":true,"question_text":"Identify the impact of ON VIOLATION DROP ROW and ON VIOLATION FAIL UPDATE for a constraint violation.\n\nA data engineer has created an ETL pipeline using Delta Live table to manage their company travel reimbursement detail, they want to ensure that the if the location details has not been provided by the employee, the pipeline needs to be terminated.\n\nHow can the scenario be implemented?","timestamp":"2024-10-26 09:52:00","unix_timestamp":1729929120,"url":"https://www.examtopics.com/discussions/databricks/view/150282-exam-certified-data-engineer-associate-topic-1-question-144/","choices":{"A":"CONSTRAINT valid_location EXPECT (location = NULL)","D":"CONSTRAINT valid_location EXPECT (location != NULL) ON VIOLATION FAIL","C":"CONSTRAINT valid_location EXPECT (location != NULL) ON DROP ROW","B":"CONSTRAINT valid_location EXPECT (location != NULL) ON VIOLATION FAIL UPDATE"},"answer":"B","discussion":[{"content":"Selected Answer: B\nCorrect Answer: B\n\nFrom Databricks doc\n\nCONSTRAINT valid_count EXPECT (count > 0) ON VIOLATION FAIL UPDATE","timestamp":"1734531660.0","comment_id":"1328547","upvote_count":"1","poster":"san089"},{"timestamp":"1734353820.0","comment_id":"1327362","poster":"MultiCloudIronMan","upvote_count":"1","content":"Selected Answer: D\nThe correct answer is D. CONSTRAINT valid_location EXPECT (location != NULL) ON VIOLATION FAIL. This constraint ensures that if the location details are not provided by the employee (i.e., location is null), the pipeline will be terminated."},{"comment_id":"1322610","upvote_count":"1","content":"Selected Answer: B\nThe answer from Udemy course","timestamp":"1733457900.0","poster":"canada_2k1"},{"timestamp":"1733339700.0","content":"Selected Answer: B\nFAIL UPDATE: Immediately stop pipeline execution.\nhttps://learn.microsoft.com/en-us/azure/databricks/delta-live-tables/expectations#fail","upvote_count":"1","comment_id":"1322056","poster":"knightkkd"},{"comment_id":"1320586","upvote_count":"2","content":"Selected Answer: B\nB is correct\nhttps://docs.databricks.com/en/delta-live-tables/sql-ref.html\n\nON VIOLATION\nOptional action to take for failed rows:\nFAIL UPDATE: Immediately stop pipeline execution.\nDROP ROW: Drop the record and continue processing.","timestamp":"1733062260.0","poster":"Worldmaster"},{"comment_id":"1306271","poster":"rsmf","upvote_count":"1","content":"Selected Answer: D\nD is correct","timestamp":"1730570220.0","comments":[{"content":"There is no such a thing as ON VIOLATION FAIL. We have only 3 behaviour to trigger when a constraint is being violated, and they fall under those syntaxes : \n--> ON VIOLATION FAIL UPDATE : The pipeline fails once it detects the first violation\n--> ON VIOLATION DROP ROW : The pipeline won't fail but the failing rows will be flagged and stored in event log\n--> If you put nothing after the expectation : The rows that violates the expectation will be loaded to the sink (Since Databricks doesn't enforce classical database constraints) and the rows will be flagged in the event log.\n\nB is the best answer, if you're not convinced try to run D in a notebook and you'll get a syntax error","timestamp":"1735552680.0","poster":"CaoMengde09","upvote_count":"1","comment_id":"1334020"}]},{"timestamp":"1729929120.0","poster":"comoon","upvote_count":"2","content":"D is correct","comment_id":"1303137"}],"answer_description":"","topic":"1","question_id":49},{"id":"XPHEibf89o3PR9Uw7eQg","answer_images":[],"answer":"AE","timestamp":"2024-10-26 09:51:00","discussion":[{"content":"Selected Answer: AE\nDatabricks recommends that you assign managed storage at the catalog level for logical data isolation, with metastore-level and schema-level as options. New workspaces that are enabled for Unity Catalog automatically are created without a metastore-level managed storage location.\n\nIt means that it's a must to create a managed location for a catalog for otpimal data isolations, otherwise you'll finish up with many schemas/tables data in the same managed location which is a bad prctice from governance perspective.\n\nWorking with not isolated catalog in Unity Catalog is chaotic, i would prefer to be on my workspace rather than working in such bad governed Unity Catalog.\n\nAns : [\"A\", \"E\"]","comment_id":"1334032","poster":"CaoMengde09","timestamp":"1735553700.0","upvote_count":"2"},{"timestamp":"1734966060.0","poster":"grygi","comment_id":"1330845","content":"Selected Answer: AD\nHad this on the exam. AD was correct, I maxed this area.","upvote_count":"3"},{"timestamp":"1734354360.0","upvote_count":"2","content":"Selected Answer: AE\nThe correct answers are A. You can have more than 1 metastore within a Databricks account console but only 1 per region and E. If metastore is not associated with location, it’s mandatory to associate catalog with managed locations. These conditions are applicable for governance in Databricks Unity Catalog","comment_id":"1327373","poster":"MultiCloudIronMan"},{"poster":"sakis213","content":"Selected Answer: AD\nD. If catalog is not associated with location, it’s mandatory to associate schema with managed locations","comment_id":"1317781","upvote_count":"1","timestamp":"1732570020.0"},{"upvote_count":"2","comments":[{"upvote_count":"1","timestamp":"1732569960.0","poster":"sakis213","comment_id":"1317779","content":"Metastore must hv a managed location defined. AD is correct"}],"timestamp":"1732225140.0","poster":"806e7d2","comment_id":"1316032","content":"Selected Answer: AE\nA. You can have more than 1 metastore within a Databricks account console but only 1 per region.\nUnity Catalog allows multiple metastores within a Databricks account console. However, each region can only have one active metastore due to geographic restrictions and for managing data governance in a region-specific manner.\nE. If metastore is not associated with location, it’s mandatory to associate catalog with managed locations.\nWhen a metastore does not have a default storage location, you must configure managed locations for each catalog to ensure governance and compliance. Managed locations define where Unity Catalog manages and stores data."},{"timestamp":"1729929060.0","content":"Correct - A and D.\nA.Unity Catalog allows you to have multiple metastores within a single Databricks account, but each metastore is limited to a single region for data locality and compliance purposes.\n\nD. When a catalog does not have an associated managed location, it becomes necessary to associate schemas within the catalog with managed locations, ensuring that data is stored in a defined path.","comment_id":"1303135","upvote_count":"2","poster":"comoon"}],"answers_community":["AE (60%)","AD (40%)"],"answer_description":"","answer_ET":"AE","question_id":50,"topic":"1","unix_timestamp":1729929060,"question_images":[],"exam_id":162,"question_text":"Which two conditions are applicable for governance in Databricks Unity Catalog? (Choose two.)","choices":{"D":"If catalog is not associated with location, it’s mandatory to associate schema with managed locations","A":"You can have more than 1 metastore within a databricks account console but only 1 per region.","E":"If metastore is not associated with location, it’s mandatory to associate catalog with managed locations","B":"Both catalog and schema must have a managed location in Unity Catalog provided metastore is not associated with a location","C":"You can have multiple catalogs within metastore and 1 catalog can be associated with multiple metastore"},"url":"https://www.examtopics.com/discussions/databricks/view/150281-exam-certified-data-engineer-associate-topic-1-question-145/","isMC":true}],"exam":{"name":"Certified Data Engineer Associate","isImplemented":true,"isMCOnly":true,"lastUpdated":"12 Apr 2025","provider":"Databricks","id":162,"isBeta":false,"numberOfQuestions":169},"currentPage":10},"__N_SSP":true}