{"pageProps":{"questions":[{"id":"skDZyKXj5WSkpBVpyZKw","choices":{"E":"All of these execution/deployment modes exist","C":"Standard mode","D":"Local mode","B":"Cluster mode","A":"Client mode"},"answer_ET":"C","question_id":36,"unix_timestamp":1712667060,"answer":"C","exam_id":161,"topic":"1","timestamp":"2024-04-09 14:51:00","question_text":"Spark's execution/deployment mode determines where the driver and executors are physically located when a Spark application is run. Which of the following Spark execution/deployment modes does not exist? If they all exist, please indicate so with Response E.","url":"https://www.examtopics.com/discussions/databricks/view/138253-exam-certified-associate-developer-for-apache-spark-topic-1/","answer_images":[],"discussion":[{"poster":"Sowwy1","content":"C is correct","comment_id":"1192290","timestamp":"1728478260.0","upvote_count":"1"}],"answers_community":[],"question_images":[],"answer_description":"","isMC":true},{"id":"ybhQLJ0NQcEUiFnjzPB8","unix_timestamp":1712747520,"question_id":37,"answer":"E","answer_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/138316-exam-certified-associate-developer-for-apache-spark-topic-1/","answer_description":"","question_images":[],"question_text":"Which of the following will cause a Spark job to fail?","answers_community":[],"topic":"1","exam_id":161,"discussion":[{"poster":"Sowwy1","content":"E is correct","comment_id":"1192908","upvote_count":"1","timestamp":"1728558720.0"}],"isMC":true,"choices":{"E":"A failed driver node.","D":"A failed worker node.","A":"Never pulling any amount of data onto the driver node.","B":"Trying to cache data larger than an executor's memory.","C":"Data needing to spill from memory to disk."},"timestamp":"2024-04-10 13:12:00","answer_ET":"E"},{"id":"8l5Hzn52YCzjFPPz7p3t","answer_ET":"D","exam_id":161,"discussion":[{"timestamp":"1728558840.0","poster":"Sowwy1","comment_id":"1192909","content":"D. is correct","upvote_count":"1"}],"question_id":38,"answer_description":"","answer_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/138317-exam-certified-associate-developer-for-apache-spark-topic-1/","answers_community":[],"topic":"1","question_text":"Which of the following best describes the similarities and differences between the MEMORY_ONLY storage level and the MEMORY_AND_DISK storage level?","answer":"D","isMC":true,"unix_timestamp":1712747640,"choices":{"E":"The MEMORY_ONLY storage level will store as much data as possible in memory and will recompute any data that does not fit in memory as it’s called.\n\nThe MEMORY_AND_DISK storage level will store half of the data in memory and store half of the memory on disk. This provides quick preview and better logical plan design.","A":"The MEMORY_ONLY storage level will store as much data as possible in memory and will store any data that does on fit in memory on disk and read it as it's called.\n\nThe MEMORY_AND_DISK storage level will store as much data as possible in memory and will recompute any data that does not fit in memory as it’s called.","C":"The MEMORY_ONLY storage level will store as much data as possible in memory on two cluster nodes and will store any data that does on fit in memory on disk and read it as it's called.\n\nThe MEMORY_AND_DISK storage level will store as much data as possible in memory on two cluster nodes and will recompute any data that does not fit in memory as it's called.","B":"The MEMORY_ONLY storage level will store as much data as possible in memory on two cluster nodes and will recompute any data that does not fit in memory as it’s called.\n\nThe MEMORY_AND_DISK storage level will store as much data as possible in memory on two cluster nodes and will store any data that does on fit in memory on disk and read it as it's called.","D":"The MEMORY_ONLY storage level will store as much data as possible in memory and will recompute any data that does not fit in memory as it's called.\n\nThe MEMORY_AND_DISK storage level will store as much data as possible in memory and will store any data that does on fit in memory on disk and read it as it's called."},"timestamp":"2024-04-10 13:14:00","question_images":[]},{"id":"axy559hZjJMhIXe4gDrK","answer_ET":"B","question_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/141598-exam-certified-associate-developer-for-apache-spark-topic-1/","answer_images":[],"unix_timestamp":1717070880,"discussion":[{"timestamp":"1718177820.0","content":"Selected Answer: B\nThis property is used to configure the threshold for automatically broadcasting small tables in join operations. When the size of a DataFrame is below this threshold, it will be broadcasted to all executor nodes for efficient join operations.","comment_id":"1228885","poster":"[Removed]","upvote_count":"1"},{"comment_id":"1221567","content":"Selected Answer: B\nSorry,its B, can't edit previous answer","poster":"jtu363","upvote_count":"1","timestamp":"1717070940.0"},{"comment_id":"1221566","timestamp":"1717070880.0","content":"Selected Answer: A\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", value)","poster":"jtu363","upvote_count":"1"}],"choices":{"C":"spark.sql.shuffle.partitions","D":"spark.sql.inMemoryColumnarStorage.batchSize","A":"spark.sql.broadcastTimeout","E":"spark.sql.adaptive.localShuffleReader.enabled","B":"spark.sql.autoBroadcastJoinThreshold"},"exam_id":161,"isMC":true,"question_id":39,"question_text":"Which of the following Spark properties is used to configure whether DataFrames found to be below a certain size threshold at runtime will be automatically broadcasted?","topic":"1","timestamp":"2024-05-30 14:08:00","answers_community":["B (67%)","A (33%)"],"answer":"B","answer_description":""},{"id":"Dq43yRZzoLDVESSJN2MP","question_text":"The code block shown below contains an error. The code block is intended to return a new DataFrame from DataFrame storesDF where column storeId is of the type string. Identify the error.\n\nCode block:\n\nstoresDF.withColumn(“storeId”, cast(col(“storeId”), StringType()))","question_images":[],"choices":{"A":"Calls to withColumn() cannot create a new column of the same name on which it is operating.","B":"DataFrame columns cannot be converted to a new type inside of a call to withColumn().","C":"The call to StringType should not be followed by parentheses.","E":"The cast() operation is a method in the Column class rather than a standalone function.","D":"The column name storeId inside the col() operation should not be quoted."},"timestamp":"2024-01-11 10:35:00","answer_images":[],"unix_timestamp":1704965700,"exam_id":161,"answer_description":"","url":"https://www.examtopics.com/discussions/databricks/view/130835-exam-certified-associate-developer-for-apache-spark-topic-1/","isMC":true,"discussion":[{"timestamp":"1724923620.0","content":"Selected Answer: E\nhttps://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.Column.cast.html?highlight=cast","poster":"JuanitoFM","upvote_count":"1","comment_id":"1162483"},{"content":"Selected Answer: E\nE is the right","timestamp":"1723227420.0","comment_id":"1145764","poster":"azure_bimonster","upvote_count":"1"},{"content":"It is E, no doubt about it","comment_id":"1139747","timestamp":"1722734940.0","poster":"Anweee","upvote_count":"1"},{"comment_id":"1119555","poster":"gowthamsara","upvote_count":"1","content":"Selected Answer: E\nE should be the answer","timestamp":"1720683300.0"}],"answers_community":["E (100%)"],"answer":"E","answer_ET":"E","question_id":40,"topic":"1"}],"exam":{"id":161,"provider":"Databricks","isImplemented":true,"isBeta":false,"name":"Certified Associate Developer for Apache Spark","lastUpdated":"12 Apr 2025","numberOfQuestions":185,"isMCOnly":true},"currentPage":8},"__N_SSP":true}