{"pageProps":{"questions":[{"id":"rOsq6LS2MwxoYcQmIw38","question_images":["https://img.examtopics.com/certified-data-engineer-professional/image35.png","https://img.examtopics.com/certified-data-engineer-professional/image36.png"],"topic":"1","timestamp":"2023-12-21 14:08:00","question_id":171,"discussion":[{"poster":"f728f7f","timestamp":"1718968080.0","comment_id":"1102541","content":"This question is broken. Proposed query cannot be identified.","upvote_count":"24"},{"content":"Selected Answer: A\nBelow is the proposed query:\n\ndf.groupBy(\"item\") .agg(count(\"item\").alias(\"total_count\"), mean(\"sale_price\").alias(\"avg_price\"), count(\"promo_code = 'NEW MEMBER'\") .alias(\"new member_promo\")) writeStream .outputMode(\"complete\") .option('mergeSchema', 'true') .option(\"checkpointLocation\", \"/item_agg/ checkpoint\") .start(\"/item_agg\")\n\nAnswer A. When updating the schema of a streaming job by adding new fields (like the new_member_promo field), it’s important to use a new checkpoint location. This is because the existing checkpoint location is tied to the old schema, and adding a new field could lead to schema mismatch issues.","upvote_count":"5","comments":[{"timestamp":"1735233840.0","poster":"OnlyPraveen","comment_id":"1332022","upvote_count":"1","content":"Thank you! Also check Question #114 which has the Proposed Query image too."}],"comment_id":"1330263","poster":"AlejandroU","timestamp":"1734836760.0"},{"content":"Selected Answer: A\nSince the new field is a count (an aggregation), it is non-nullable, making the change incompatible with the existing schema. This requires a new checkpointLocation to avoid schema mismatch issues. Additionally, the \"mergeSchema=true\" option must remain enabled to allow Spark to handle the schema evolution properly. \n\nHowever, if the field were nullable and not an aggregation, it would be a backward-compatible change, allowing the checkpoint to remain unchanged, as happens with schema evolution in Kafka. In this case, the correct answer is A.","timestamp":"1734182400.0","comment_id":"1326473","upvote_count":"2","poster":"kino_1994"},{"timestamp":"1734077460.0","comment_id":"1326061","content":"Selected Answer: A\nThe given answer is correct.\nIn case of addition of new cols (or changes) the checkpoint location also needs to change.","poster":"Sriramiyer92","upvote_count":"1"}],"answer_images":[],"isMC":true,"question_text":"A data team's Structured Streaming job is configured to calculate running aggregates for item sales to update a downstream marketing dashboard. The marketing team has introduced a new promotion, and they would like to add a new field to track the number of times this promotion code is used for each item. A junior data engineer suggests updating the existing query as follows. Note that proposed changes are in bold.\n\nOriginal query:\n\n//IMG//\n\n\nProposed query:\n\n//IMG//\n\n\nProposed query:\n\n.start(“/item_agg”)\n\nWhich step must also be completed to put the proposed query into production?","url":"https://www.examtopics.com/discussions/databricks/view/129174-exam-certified-data-engineer-professional-topic-1-question/","answer_ET":"A","exam_id":163,"answer":"A","unix_timestamp":1703164080,"answer_description":"","choices":{"A":"Specify a new checkpointLocation","B":"Increase the shuffle partitions to account for additional aggregates","C":"Run REFRESH TABLE delta.'/item_agg'","E":"Remove .option(‘mergeSchema’, ‘true’) from the streaming write","D":"Register the data in the \"/item_agg\" directory to the Hive metastore"},"answers_community":["A (100%)"]},{"id":"UA1iAPk9u9ejipxfyymG","choices":{"B":"Increase the number of shuffle partitions to maximize parallelism, since the trigger interval cannot be modified without modifying the checkpoint directory.","D":"Set the trigger interval to 500 milliseconds; setting a small but non-zero trigger interval ensures that the source is not queried too frequently.","A":"Set the trigger interval to 3 seconds; the default trigger interval is consuming too many records per batch, resulting in spill to disk that can increase volume costs.","C":"Set the trigger interval to 10 minutes; each batch calls APIs in the source storage account, so decreasing trigger frequency to maximum allowable threshold should minimize this cost.","E":"Use the trigger once option and configure a Databricks job to execute the query every 10 minutes; this approach minimizes costs for both compute and storage."},"answer_ET":"C","answer":"C","question_text":"A Structured Streaming job deployed to production has been resulting in higher than expected cloud storage costs. At present, during normal execution, each microbatch of data is processed in less than 3s; at least 12 times per minute, a microbatch is processed that contains 0 records. The streaming write was configured using the default trigger settings. The production job is currently scheduled alongside many other Databricks jobs in a workspace with instance pools provisioned to reduce start-up time for jobs with batch execution.\n\nHolding all other variables constant and assuming records need to be processed in less than 10 minutes, which adjustment will meet the requirement?","timestamp":"2023-11-21 20:49:00","answers_community":["C (54%)","E (46%)"],"answer_description":"","topic":"1","unix_timestamp":1700596140,"isMC":true,"url":"https://www.examtopics.com/discussions/databricks/view/126756-exam-certified-data-engineer-professional-topic-1-question/","discussion":[{"upvote_count":"1","comment_id":"1330365","content":"Selected Answer: C\nAnswer C. Setting the trigger interval to 10 minutes (option C) directly aligns with the requirement to process records within a 10-minute window. It achieves the same reduction in processing frequency as option E but without the added complexity of job scheduling or reliance on trigger once. Using the trigger once option requires external orchestration (e.g., a scheduled Databricks job) to execute every 10 minutes. This adds operational overhead and potential delays due to job scheduling or startup times, especially in a shared workspace using instance pools.","poster":"AlejandroU","timestamp":"1734867180.0"},{"content":"Selected Answer: C\nIn my opinion, both C and E met the requirements. But the sentence says 'Holding all other variables constant'. This indicates me that E cannot be the solution, as new variables are introduced.","comment_id":"1328719","upvote_count":"2","poster":"UrcoIbz","timestamp":"1734556980.0"},{"upvote_count":"1","poster":"benni_ale","comment_id":"1321764","timestamp":"1733299860.0","content":"Selected Answer: E\nThe fact that the question mentions instance pools provisioned make me guess that we should go for trigger once option otherwise instance pools are useless."},{"timestamp":"1727530920.0","upvote_count":"2","poster":"pk07","comment_id":"1290660","content":"Selected Answer: C\nE WRONG. Using trigger once would stop the stream after one execution, not meeting the requirement of continuous processing."},{"content":"Selected Answer: E\nE is correct for two reasons:\n1) we have been using the connection pool that allows us to start our job instantly\n2) the questions are about reducing costs. Triggering one per 10 minutes allows not to use running VM (as in option C) and to keep the same SLA (due to 1) ) with lower cost for compute as well as for storage (fewer API calls which are not free )","timestamp":"1723870740.0","comment_id":"1267456","upvote_count":"1","poster":"practicioner"},{"upvote_count":"3","comment_id":"1191297","poster":"Er5","content":"required \"to be processed in less than 10 minutes\".\nC. \"set the trigger interval to 10 minutes\" means Process time + interval > 10 minutes\nE. \"trigger once\", \"execute the query every 10 minutes\"","timestamp":"1712541960.0"},{"content":"Selected Answer: E\ndefault trigger time is 0.5 seconds \nHence in a minute there are 120 triggers happens\nEach trigger consume 3 seconds to complete\nnow 120*3 = 360 seconds = 6 minutes\nHence the job is completing in 6 minutes\nNow there is buffer of 4 minutes which can be utilized in compute spin up \nbut as we are using the spot instances which will further decrease the start up time \nI think E is correct option to decrease the cost.","poster":"vikram12apr","timestamp":"1709986140.0","upvote_count":"2","comment_id":"1169449"},{"upvote_count":"3","poster":"hidelux","comments":[{"comment_id":"1267972","content":"you are right. But we need to guarantee SLA and for this reason to block VM (with autoscaling) is a good practice","upvote_count":"1","timestamp":"1723968540.0","poster":"practicioner"}],"timestamp":"1709798280.0","content":"Selected Answer: E\nThe question indicates that they are using instance pools for fast startup time. option C would block a VM permanently which is not intended. E will grab a VM, run the job, and return it to the pool to be available for other jobs mentioned in the question.","comment_id":"1167765"},{"comment_id":"1132843","content":"Selected Answer: C\nC is more effective than E as E will incur startup time for spinning new job cluster","poster":"spaceexplorer","timestamp":"1706299320.0","upvote_count":"3"},{"poster":"ranith","comment_id":"1129924","content":"The default trigger interval is 500ms, but the question says it processes batches with 0 records and the avg time to process is 3s. If the requirement is to process under 10 minutes the best option here is to trigger every 3s.","upvote_count":"1","timestamp":"1706035680.0"},{"comment_id":"1111451","upvote_count":"2","timestamp":"1704145680.0","poster":"divingbell17","content":"Selected Answer: C\nBoth C and E meet the requirement to reduce cloud storage cost. E further reduces compute cost however reducing compute cost is not a requirement in the question."},{"content":"Selected Answer: C\nFor production -> records need to be processed in less than 10 minutes. So we need to schedule each 10 minutes","upvote_count":"3","timestamp":"1702894680.0","poster":"alexvno","comment_id":"1099611"},{"timestamp":"1700596140.0","upvote_count":"4","comment_id":"1076601","comments":[{"comment_id":"1089566","poster":"Gulenur_GS","upvote_count":"1","timestamp":"1701882300.0","content":"in this case why not C? Processing trigger in 10 min ensures the same I guess.."}],"poster":"aragorn_brego","content":"Selected Answer: E\nGiven that there are frequent microbatches with 0 records being processed, it indicates that the job is polling the source too often. Using the \"trigger once\" option would allow each microbatch to process all available data and then stop. By scheduling the job to run every 10 minutes, you ensure that the system is not constantly checking for new data when there is none, thus reducing the number of read operations from the source storage and potentially reducing costs associated with those reads."}],"question_images":[],"exam_id":163,"answer_images":[],"question_id":172},{"id":"oqBpL4l1E6rNZzJqaNKl","url":"https://www.examtopics.com/discussions/databricks/view/124596-exam-certified-data-engineer-professional-topic-1-question/","answers_community":["D (100%)"],"unix_timestamp":1698211260,"answer_description":"","question_id":173,"answer":"D","answer_images":[],"question_text":"Which statement describes the correct use of pyspark.sql.functions.broadcast?","question_images":[],"exam_id":163,"timestamp":"2023-10-25 07:21:00","topic":"1","discussion":[{"upvote_count":"3","content":"Selected Answer: D\nCorrect Answer: D. It marks a DataFrame as small enough to store in memory on all executors, allowing a broadcast join.\n\nReference: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.broadcast.html","poster":"Freyr","timestamp":"1732847820.0","comment_id":"1220623"},{"poster":"aragorn_brego","comment_id":"1076602","content":"Selected Answer: D\nThe broadcast function in PySpark is used in the context of joins. When you mark a DataFrame with broadcast, Spark tries to send this DataFrame to all worker nodes so that it can be joined with another DataFrame without shuffling the larger DataFrame across the nodes. This is particularly beneficial when the DataFrame is small enough to fit into the memory of each node. It helps to optimize the join process by reducing the amount of data that needs to be shuffled across the cluster, which can be a very expensive operation in terms of computation and time.","timestamp":"1716313860.0","upvote_count":"3"},{"content":"Answer is D","poster":"Dileepvikram","upvote_count":"1","comment_id":"1066256","timestamp":"1715238240.0"},{"content":"The answer is D","timestamp":"1714948380.0","comment_id":"1063420","poster":"PearApple","upvote_count":"1"},{"poster":"hm358","comment_id":"1057120","timestamp":"1714415040.0","content":"Selected Answer: D\nhttps://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.broadcast.html","upvote_count":"2"},{"timestamp":"1714022460.0","content":"Selected Answer: D\nMarks a DataFrame as small enough for use in broadcast joins.","poster":"sturcu","upvote_count":"3","comment_id":"1053469"}],"answer_ET":"D","isMC":true,"choices":{"C":"It caches a copy of the indicated table on attached storage volumes for all active clusters within a Databricks workspace.","E":"It caches a copy of the indicated table on all nodes in the cluster for use in all future queries during the cluster lifetime.","A":"It marks a column as having low enough cardinality to properly map distinct values to available partitions, allowing a broadcast join.","D":"It marks a DataFrame as small enough to store in memory on all executors, allowing a broadcast join.","B":"It marks a column as small enough to store in memory on all executors, allowing a broadcast join."}},{"id":"VmfletzXAl3bWxsHTY1D","question_text":"A data engineer is configuring a pipeline that will potentially see late-arriving, duplicate records.\n\nIn addition to de-duplicating records within the batch, which of the following approaches allows the data engineer to deduplicate data against previously processed records as it is inserted into a Delta table?","exam_id":163,"topic":"1","unix_timestamp":1697995560,"discussion":[{"content":"Selected Answer: C\nMerge, when not match insert","timestamp":"1698211560.0","poster":"sturcu","upvote_count":"5","comment_id":"1053472"},{"upvote_count":"1","comment_id":"1338910","poster":"_lene_","content":"Selected Answer: C\nthis question was in the Databricks DE Professional exam guide","timestamp":"1736532120.0"},{"timestamp":"1732952640.0","comment_id":"1320114","poster":"hebied","upvote_count":"1","content":"Selected Answer: D\nOption C is tricky since it should be merge on on Not match condition rather than matching .. Since Option D is more suitable"},{"comment_id":"1076605","upvote_count":"4","content":"Selected Answer: C\nTo handle deduplication against previously processed records in a Delta table, the MERGE INTO command can be used to perform an upsert operation. This means that if the incoming data has a record that matches an existing record based on a unique key, the MERGE INTO operation can update the existing record (if needed) or simply ignore the duplicate. If there is no match (i.e., the record is new), then the record will be inserted","timestamp":"1700596380.0","poster":"aragorn_brego"},{"content":"Selected Answer: C\nanswer is C","upvote_count":"2","timestamp":"1700079720.0","poster":"60ties","comment_id":"1071855"},{"timestamp":"1699520760.0","upvote_count":"2","poster":"Dileepvikram","content":"Answer is C","comment_id":"1066258"},{"comment_id":"1057121","timestamp":"1698611100.0","poster":"hm358","upvote_count":"2","content":"Selected Answer: C\nmerge will be more efficient"},{"content":"C\nReference: file:///C:/Users/yuen1/Downloads/databricks-certified-data-engineer-professional-exam-guide.pdf","comment_id":"1050930","timestamp":"1697995560.0","upvote_count":"1","comments":[{"content":"you are referencing a local pdf in your computer !!!","upvote_count":"9","comment_id":"1053621","poster":"mouad_attaqi","timestamp":"1698229020.0"}],"poster":"Crocjun"}],"answer_description":"","choices":{"E":"Rely on Delta Lake schema enforcement to prevent duplicate records.","A":"Set the configuration delta.deduplicate = true.","D":"Perform a full outer join on a unique key and overwrite existing data.","B":"VACUUM the Delta table after each batch completes.","C":"Perform an insert-only merge with a matching condition on a unique key."},"question_images":[],"timestamp":"2023-10-22 19:26:00","isMC":true,"answer_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/124369-exam-certified-data-engineer-professional-topic-1-question/","answer":"C","question_id":174,"answer_ET":"C","answers_community":["C (93%)","7%"]},{"id":"dPEvuEwrLsNOIa2csn8V","unix_timestamp":1703865300,"url":"https://www.examtopics.com/discussions/databricks/view/129709-exam-certified-data-engineer-professional-topic-1-question/","answer_images":[],"question_id":175,"discussion":[{"content":"Selected Answer: A\nLooks like A to me. Does anyone think otherwise?","comment_id":"1108863","poster":"dmov","timestamp":"1719669300.0","upvote_count":"8"},{"content":"Selected Answer: A\nWhen the schema of a Delta table is updated to include new fields, these fields will only be populated for new records ingested after the schema update. The new fields will not be retroactively computed for historic records already stored in the Delta table. Therefore, the additional metadata fields (current timestamp, Kafka topic, and partition) will not exist in the historic data, limiting the scope of the diagnosis to new data ingested after the schema update.","poster":"vctrhugo","comment_id":"1141693","upvote_count":"6","timestamp":"1722901980.0"},{"poster":"RandomForest","content":"Selected Answer: A\nThe correct answer is A.","timestamp":"1736852520.0","comment_id":"1340301","upvote_count":"1"}],"question_images":[],"answers_community":["A (100%)"],"choices":{"D":"Updating the table schema will invalidate the Delta transaction log metadata.","E":"Updating the table schema requires a default value provided for each field added.","C":"New fields cannot be added to a production Delta table.","B":"Spark cannot capture the topic and partition fields from a Kafka source.","A":"New fields will not be computed for historic records."},"answer_ET":"A","exam_id":163,"topic":"1","timestamp":"2023-12-29 16:55:00","question_text":"A data pipeline uses Structured Streaming to ingest data from Apache Kafka to Delta Lake. Data is being stored in a bronze table, and includes the Kafka-generated timestamp, key, and value. Three months after the pipeline is deployed, the data engineering team has noticed some latency issues during certain times of the day.\n\nA senior data engineer updates the Delta Table's schema and ingestion logic to include the current timestamp (as recorded by Apache Spark) as well as the Kafka topic and partition. The team plans to use these additional metadata fields to diagnose the transient processing delays.\n\nWhich limitation will the team face while diagnosing this problem?","isMC":true,"answer_description":"","answer":"A"}],"exam":{"lastUpdated":"12 Apr 2025","isImplemented":true,"isMCOnly":true,"isBeta":false,"name":"Certified Data Engineer Professional","id":163,"provider":"Databricks","numberOfQuestions":200},"currentPage":35},"__N_SSP":true}