{"pageProps":{"questions":[{"id":"YCCp8V78FGdJhhI4wNc2","answer":"C","answers_community":["C (67%)","B (24%)","6%"],"question_id":46,"url":"https://www.examtopics.com/discussions/databricks/view/120174-exam-certified-data-engineer-professional-topic-1-question/","isMC":true,"answer_description":"","timestamp":"2023-09-07 12:35:00","unix_timestamp":1694082900,"discussion":[{"content":"Selected Answer: C\nMy reasoning is thus:\nThe application is based on batch processes, so A is wrong.\nOverwriting the table would destroy the Type 1 SCD behavior, so B is wrong.\nComparing versions of account_history would not be efficient, as the whole data would be scanned, so D is wrong. 'username' is not a key column, so we have no guarantee that it's unique, thus de-duplicating by it can yield wrongly grouped sets of rows, so E is not a safe bet, with the information we know.\nC is the best option.","timestamp":"1704787140.0","poster":"RafaelCFC","upvote_count":"10","comment_id":"1117279"},{"timestamp":"1710535800.0","poster":"terrku","comment_id":"1174493","upvote_count":"8","content":"Selected Answer: B\nType 1 table means the behavior is overwriting."},{"timestamp":"1740529080.0","poster":"Tedet","content":"Selected Answer: C\nThis option proposes filtering the records in the account_history table for the most recent records based on the last_updated field, which is exactly what you want to do to get the most recent value.\nThe use of a MERGE statement ensures that only the most recent records are inserted or updated in the account_current table.\nThis method avoids full overwrites of the account_current table and only updates records that have actually changed, which is efficient for large datasets.\nConclusion: This is the most efficient approach because it ensures only the most recent data is merged, and it avoids unnecessary full table rebuilds.","upvote_count":"1","comment_id":"1361626"},{"timestamp":"1729673460.0","content":"C is correct because\nB is wrong as it says filtering the max value of last updated + overwriting we will miss some valid records. \ntwo valid scenarios: 1-Filtering the max value of last updated +merging (Option-c)\n 2.use window function on last update, filter and then overwrite (no options)","comment_id":"1301966","upvote_count":"3","poster":"Ananth4Sap"},{"content":"Selected Answer: C\nA. NO. Batch job required so AutoLoader and StructuredStreaming unecessarily complex solutions.\nB. NO. A full overwrite of the table is not efficient.\nC. YES. Seems it is filterning and merging on the id by using as less data as reasonable in the merge statement, why not? \nD. NO. Difference operation is very ineffecient for this purpose \nE. NO. Username is not key","upvote_count":"3","timestamp":"1729145820.0","poster":"benni_ale","comment_id":"1299094"},{"upvote_count":"1","content":"Selected Answer: C\nC is correct","timestamp":"1722885360.0","poster":"Dhusanth","comment_id":"1261176"},{"comment_id":"1259331","poster":"faraaz132","timestamp":"1722511620.0","content":"C is correct because:\nA record might have multiple changes and we need to select the most recent change that happened on that record. For that we will use max Log in date and rank it using window function, then we filter on rank=1 and use it for UPSERT operation.","upvote_count":"1"},{"timestamp":"1717582440.0","upvote_count":"1","comment_id":"1224651","poster":"Karunakaran_R","content":"I think B ,Type 1 table must overwrite the data"},{"timestamp":"1716848760.0","comment_id":"1219870","upvote_count":"3","poster":"Freyr","content":"C is correct.\nA Type 1 table means that it performs an \"upsert\" operation without maintaining history, based on the merge condition. This means that new records are inserted, and existing records are updated. As a result, the merge process does not retain historical records.\n\nTherefore, the correct answer is C."},{"comment_id":"1145233","content":"C is correct","upvote_count":"1","timestamp":"1707453000.0","poster":"PrashantTiwari"},{"comment_id":"1136757","content":"Selected Answer: C\nanswer is C","timestamp":"1706705760.0","poster":"DAN_H","upvote_count":"2"},{"poster":"spaceexplorer","content":"Selected Answer: C\nanswer is C","timestamp":"1706116740.0","comment_id":"1130915","upvote_count":"1"},{"content":"Selected Answer: C\nCorrect answer is C","poster":"kz_data","timestamp":"1704893160.0","comment_id":"1118582","upvote_count":"1"},{"comment_id":"1114699","poster":"ATLTennis","comments":[{"upvote_count":"2","poster":"AndreFR","timestamp":"1724144160.0","content":"doesn't work because it doesn't take into account requirement on user_id","comment_id":"1269284"}],"timestamp":"1704475440.0","content":"Selected Answer: D\nD is the most optimal way to identify the changes in the last data refresh","upvote_count":"1"},{"comments":[{"comment_id":"1044818","content":"If the \"las log in\" is the column that shows the lates version of the record then answer c is correct","poster":"sturcu","upvote_count":"1","timestamp":"1697448360.0"},{"comment_id":"1084122","content":"deduplication on username does not make sense, username is not PK.","upvote_count":"3","timestamp":"1701334500.0","poster":"petrv"}],"content":"Selected Answer: E\nWe need to filter on last hours and deduplicate records, then merge.\nDo is not correct, filtering on max loggin_date makes no sense.","upvote_count":"2","poster":"sturcu","comment_id":"1040298","timestamp":"1697009040.0"},{"content":"correct answer is C","timestamp":"1695312300.0","comment_id":"1013251","poster":"Eertyy","upvote_count":"1"},{"upvote_count":"3","content":"Selected Answer: C\nCorrect","comment_id":"1001431","timestamp":"1694082900.0","poster":"thxsgod"}],"choices":{"D":"Use Delta Lake version history to get the difference between the latest version of account_history and one version prior, then write these records to account_current.","B":"Overwrite the account_current table with each batch using the results of a query against the account_history table grouping by user_id and filtering for the max value of last_updated.","E":"Filter records in account_history using the last_updated field and the most recent hour processed, making sure to deduplicate on username; write a merge statement to update or insert the most recent value for each username.","A":"Use Auto Loader to subscribe to new files in the account_history directory; configure a Structured Streaming trigger once job to batch update newly detected files into the account_current table.","C":"Filter records in account_history using the last_updated field and the most recent hour processed, as well as the max last_iogin by user_id write a merge statement to update or insert the most recent value for each user_id."},"exam_id":163,"answer_images":[],"question_images":[],"question_text":"An hourly batch job is configured to ingest data files from a cloud object storage container where each batch represent all records produced by the source system in a given hour. The batch job to process these records into the Lakehouse is sufficiently delayed to ensure no late-arriving data is missed. The user_id field represents a unique key for the data, which has the following schema: user_id BIGINT, username STRING, user_utc STRING, user_region STRING, last_login BIGINT, auto_pay BOOLEAN, last_updated BIGINT\nNew records are all ingested into a table named account_history which maintains a full record of all data in the same schema as the source. The next table in the system is named account_current and is implemented as a Type 1 table representing the most recent value for each unique user_id.\nAssuming there are millions of user accounts and tens of thousands of records processed hourly, which implementation can be used to efficiently update the described account_current table as part of each hourly batch job?","topic":"1","answer_ET":"C"},{"id":"tunrJHVCdlciidWUf8Tl","exam_id":163,"question_id":47,"answer":"C","timestamp":"2024-07-20 10:45:00","question_images":[],"answer_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/144257-exam-certified-data-engineer-professional-topic-1-question/","discussion":[{"upvote_count":"1","comment_id":"1307715","content":"Selected Answer: C\nC thanks","poster":"benni_ale","timestamp":"1730881140.0"},{"timestamp":"1728290880.0","content":"B is correct. \nC is wrong OPTIMIZE is a separate process from write.","poster":"Farid77","upvote_count":"1","comment_id":"1294146"},{"timestamp":"1721465100.0","poster":"vexor3","comments":[{"content":"Please provide your input to Questions 144,145,146,147,149 also. Thanks in advance","upvote_count":"1","comment_id":"1260867","timestamp":"1722819660.0","poster":"only_vimal"}],"content":"Selected Answer: C\nC is correct","comment_id":"1251625","upvote_count":"2"}],"question_text":"Which statement describes Delta Lake optimized writes?","isMC":true,"answer_description":"","topic":"1","unix_timestamp":1721465100,"answer_ET":"C","answers_community":["C (100%)"],"choices":{"B":"An asynchronous job runs after the write completes to detect if files could be further compacted; if yes, an OPTIMIZE job is executed toward a default of 1 GB.","C":"A shuffle occurs prior to writing to try to group similar data together resulting in fewer files instead of each executor writing multiple files based on directory partitions.","A":"Before a Jobs cluster terminates, OPTIMIZE is executed on all tables modified during the most recent job.","D":"Optimized writes use logical partitions instead of directory partitions; because partition boundaries are only represented in metadata, fewer small files are written."}},{"id":"7kWoGfFgk7PjaHW86rLe","question_id":48,"answer_images":[],"answer":"D","answer_description":"","url":"https://www.examtopics.com/discussions/databricks/view/144258-exam-certified-data-engineer-professional-topic-1-question/","question_images":[],"isMC":true,"topic":"1","unix_timestamp":1721465160,"discussion":[{"upvote_count":"2","timestamp":"1721465160.0","content":"Selected Answer: D\nD is correct","poster":"vexor3","comment_id":"1251627"}],"answer_ET":"D","exam_id":163,"choices":{"D":"Structured Streaming models new data arriving in a data stream as new rows appended to an unbounded table.","C":"Structured Streaming relies on a distributed network of nodes that hold incremental state values for cached stages.","B":"Structured Streaming is implemented as a messaging bus and is derived from Apache Kafka.","A":"Structured Streaming leverages the parallel processing of GPUs to achieve highly parallel data throughput."},"answers_community":["D (100%)"],"timestamp":"2024-07-20 10:46:00","question_text":"Which statement characterizes the general programming model used by Spark Structured Streaming?"},{"id":"kTA7ztVOtDkA131KYQAO","exam_id":163,"question_images":[],"timestamp":"2024-07-20 10:46:00","discussion":[{"comment_id":"1339925","content":"Selected Answer: A\nProperty Name Default Meaning \nspark.sql.files.maxPartitionBytes 134217728 (128 MB) The maximum number of bytes to pack into a single partition when reading files. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.","timestamp":"1736779200.0","upvote_count":"1","poster":"_lene_"},{"timestamp":"1721465160.0","upvote_count":"1","comment_id":"1251628","content":"Selected Answer: A\nA is correct","poster":"vexor3"}],"isMC":true,"question_id":49,"choices":{"D":"spark.sql.adaptive.coalescePartitions.minPartitionNum","A":"spark.sql.files.maxPartitionBytes","C":"spark.sql.adaptive.advisoryPartitionSizeInBytes","B":"spark.sql.autoBroadcastJoinThreshold"},"unix_timestamp":1721465160,"answer_description":"","answer_images":[],"answers_community":["A (100%)"],"topic":"1","answer_ET":"A","question_text":"Which configuration parameter directly affects the size of a spark-partition upon ingestion of data into Spark?","url":"https://www.examtopics.com/discussions/databricks/view/144259-exam-certified-data-engineer-professional-topic-1-question/","answer":"A"},{"id":"eetgilNb1NoyG6cEfqPC","answer_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/144260-exam-certified-data-engineer-professional-topic-1-question/","answer_ET":"D","exam_id":163,"topic":"1","answers_community":["D (100%)"],"isMC":true,"question_id":50,"question_text":"A Spark job is taking longer than expected. Using the Spark UI, a data engineer notes that the Min, Median, and Max Durations for tasks in a particular stage show the minimum and median time to complete a task as roughly the same, but the max duration for a task to be roughly 100 times as long as the minimum.\n\nWhich situation is causing increased duration of the overall job?","unix_timestamp":1721465220,"answer":"D","choices":{"A":"Task queueing resulting from improper thread pool assignment.","B":"Spill resulting from attached volume storage being too small.","D":"Skew caused by more data being assigned to a subset of spark-partitions.","C":"Network latency due to some cluster nodes being in different regions from the source data"},"timestamp":"2024-07-20 10:47:00","answer_description":"","question_images":[],"discussion":[{"content":"Selected Answer: D\nD is ok","timestamp":"1730881380.0","poster":"benni_ale","upvote_count":"1","comment_id":"1307716"},{"timestamp":"1721465220.0","poster":"vexor3","comment_id":"1251630","content":"Selected Answer: D\nD is correct","upvote_count":"1"}]}],"exam":{"name":"Certified Data Engineer Professional","provider":"Databricks","numberOfQuestions":200,"isMCOnly":true,"id":163,"lastUpdated":"12 Apr 2025","isImplemented":true,"isBeta":false},"currentPage":10},"__N_SSP":true}