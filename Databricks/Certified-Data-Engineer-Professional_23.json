{"pageProps":{"questions":[{"id":"JzgN9me8Lptpl6ISTMsD","timestamp":"2023-08-06 10:48:00","question_id":111,"answer_images":[],"topic":"1","answer_ET":"E","question_images":[],"answer_description":"","unix_timestamp":1691311680,"discussion":[{"upvote_count":"5","content":"Selected Answer: A\nDelta Lake's Auto Compaction feature is designed to improve the efficiency of data storage by reducing the number of small files in a Delta table. After data is written to a Delta table, an asynchronous job can be triggered to evaluate the file sizes. If it determines that there are a significant number of small files, it will automatically run the OPTIMIZE command, which coalesces these small files into larger ones, typically aiming for files around 1 GB in size for optimal performance.\n\nE is incorrect because the statement is similar to A but with an incorrect default file size target.","comments":[{"poster":"Kill9","comment_id":"1234698","timestamp":"1718990040.0","upvote_count":"2","content":"Table property delta.autoOptimize.autoCompact target 128 mb. For table property delta.tuneFileSizesForRewrites, tables larger than 10 TB, the target file size is 1 GB.\nhttps://learn.microsoft.com/en-us/azure/databricks/delta/tune-file-size"}],"poster":"aragorn_brego","comment_id":"1076321","timestamp":"1700574180.0"},{"comment_id":"1339891","content":"Selected Answer: E\nDelta Lake Auto Compaction is a feature that automatically detects opportunities to optimize small files. When a write operation is completed, an asynchronous job assesses whether the resulting files can be compacted into larger files (the default target size is 128 MB). If compaction is needed, the system executes an OPTIMIZE job in the background to improve file size and query performance.\n\nThis feature reduces the overhead of managing small files manually and improves storage and query efficiency. It aligns with Delta Lake's goal of simplifying and optimizing data lake performance.","timestamp":"1736773140.0","poster":"RandomForest","upvote_count":"2"},{"upvote_count":"4","poster":"mwynn","content":"Selected Answer: E\nI think it is E because they are just asking us to generally describe the feature - here's some info I gleaned from a DB Academy video:\n ○ Compact small files on write with auto-optimize (tries to achieve file size of 128 MB)\n ○ Auto-Compact launches a new job after execution of first Spark job (i.e. async), where it will try to compress files closer to 128 MB","comments":[{"content":"This is true. I just heard the same statement in Databricks Academy video. Advanced Data Engineering with Databricks/Section5/Lesson1:Designing the foundation from 4:00 into the video!","timestamp":"1739446620.0","poster":"pallazoj","comment_id":"1356082","upvote_count":"1"}],"timestamp":"1736365020.0","comment_id":"1338070"},{"comment_id":"1324338","upvote_count":"1","timestamp":"1733798340.0","poster":"Nicks_name","content":"Selected Answer: E\ntypo in databricks documentation about sync job, but default size is explicitly mentioned as 128"},{"comment_id":"1324217","poster":"carah","content":"Selected Answer: B\nTable property: delta.autoOptimize.autoCompact\n B. correct, although https://docs.databricks.com/en/delta/tune-file-size.html#auto-compaction-for-delta-lake-on-databricks \n does not mention OPTIMIZE, it is best option\n A., E. wrong, auto compaction runs synchronously\n C. wrong, it describes Table setting: delta.autoOptimize.optimizeWrite\n D. wrong, not related to file compaction","comments":[{"poster":"arekm","content":"The problem I have with B is that is says - on all tables. That depends on whether we use spark settings or table settings.\n\nHowever, I still believe the asynchronous in A and E was meant to be synchronous (it is a typo). If it was not, then you are right :)","timestamp":"1735648260.0","upvote_count":"1","comment_id":"1334793"}],"upvote_count":"3","timestamp":"1733775660.0"},{"content":"There appears to be a typo in databricks documentation","poster":"vish9","comment_id":"1306361","timestamp":"1730590320.0","upvote_count":"3"},{"upvote_count":"1","poster":"rrprofessional","timestamp":"1730386920.0","comment_id":"1305499","content":"Enable auto compaction. By default will use 128 MB as the target file size."},{"comments":[{"content":"This. Don't be fooled by the typo answers, typo is inserted for a reason. It makes the answer wrong.","upvote_count":"1","poster":"mouthwash","timestamp":"1736164920.0","comment_id":"1337110"}],"upvote_count":"3","content":"Selected Answer: B\nIf you go through this docs - then one thing is clear that it is not async job, so we have to eliminate A & C. D is wrong. It has no special job wrt the partition. Also file size 0f 128 MB is legacy config, latest one is dynamic. So we are left with B","timestamp":"1727836860.0","poster":"akashdesarda","comment_id":"1292179"},{"upvote_count":"2","timestamp":"1727439300.0","poster":"pk07","comment_id":"1290017","content":"Selected Answer: E\nhttps://docs.databricks.com/en/delta/tune-file-size.html"},{"upvote_count":"2","content":"Selected Answer: B\nAuto compaction is synchronous job.","timestamp":"1723810500.0","comment_id":"1267036","poster":"partha1022"},{"timestamp":"1721525880.0","comment_id":"1252116","poster":"Shailly","upvote_count":"4","content":"Selected Answer: B\nA and E are wrong because auto compaction is synchronous operation!\n\nI vote for B\n\nAs per documentation - \"Auto compaction occurs after a write to a table has succeeded and runs synchronously on the cluster that has performed the write. Auto compaction only compacts files that haven’t been compacted previously.\"\n\nhttps://docs.delta.io/latest/optimizations-oss.html"},{"content":"E. An asynchronous job runs after the write completes to detect if files could be further compacted; if yes, an OPTIMIZE job is executed toward a default of 128 MB.\nhttps://community.databricks.com/t5/data-engineering/what-is-the-difference-between-optimize-and-auto-optimize/td-p/21189","poster":"imatheushenrique","timestamp":"1717212240.0","upvote_count":"1","comment_id":"1222474"},{"comment_id":"1149837","timestamp":"1707886320.0","poster":"ojudz08","upvote_count":"2","content":"Selected Answer: E\nE is the answer.\nEnable the settings uses the 128 MB as the target file size\nhttps://learn.microsoft.com/en-us/azure/databricks/delta/tune-file-size"},{"upvote_count":"1","comment_id":"1136831","content":"Selected Answer: E\ndefault file size is 128MB in auto compaction","poster":"DAN_H","timestamp":"1706710560.0"},{"content":"E is correct as the default file size is 128MB in auto compaction, not 1GB as normal OPTIMIZE statement.","upvote_count":"1","timestamp":"1704895980.0","comment_id":"1118630","poster":"kz_data"},{"comment_id":"1117283","content":"Selected Answer: E\n128MB is the default.","poster":"IWantCerts","timestamp":"1704787380.0","upvote_count":"1"},{"content":"Question is more on auto compaction hence the answer is E, as default size or auto compaction is 128 mb","comment_id":"1105598","poster":"Yogi05","timestamp":"1703548260.0","upvote_count":"1"},{"content":"Selected Answer: E\nOptimize default target file size is 1Gb, however in this question we are dealing with auto compaction. Which when enabled runs optimize with 128MB file size by default.","comment_id":"1086152","upvote_count":"1","poster":"hamzaKhribi","timestamp":"1701519600.0"},{"comment_id":"1060821","timestamp":"1698951840.0","content":"Selected Answer: E\nE is correct. Auto compact tries to optimize to a file size of 128MB","poster":"BIKRAM063","upvote_count":"1"},{"comment_id":"1040341","upvote_count":"3","content":"Selected Answer: E\nE is the best feet, although databricks says that auto compaction runs runs synchronously","timestamp":"1697011380.0","poster":"sturcu"},{"timestamp":"1695322740.0","content":"correct answer is e","comment_id":"1013356","upvote_count":"1","poster":"Eertyy"},{"poster":"cotardo2077","content":"Selected Answer: E\nE fits best, but according to docs it is synchronous opeartion\n \"Auto compaction occurs after a write to a table has succeeded and runs synchronously on the cluster that has performed the write. Auto compaction only compacts files that haven’t been compacted previously.\"","timestamp":"1693896480.0","comment_id":"999180","upvote_count":"4"},{"timestamp":"1692790140.0","comment_id":"988234","poster":"taif12340","upvote_count":"3","content":"Correct answer is E:\nAuto optimize consists of 2 complementary operations:\n- Optimized writes: with this feature enabled, Databricks attempts to write out 128 MB files for each table partition.\n- Auto compaction: this will check after an individual write, if files can further be compacted. If yes, it runs an OPTIMIZE job with 128 MB file sizes (instead of the 1 GB file size used in the standard OPTIMIZE)"},{"timestamp":"1692275880.0","upvote_count":"1","content":"correct answer is A","comment_id":"983613","poster":"BrianNguyen95"},{"content":"correct answer is E, the auto-compaction runs a asynchronous job to combine small files to a default of 128 MB \nhttps://learn.microsoft.com/en-us/azure/databricks/delta/tune-file-size","comments":[],"comment_id":"973658","timestamp":"1691311680.0","upvote_count":"4","poster":"8605246"}],"question_text":"Which statement describes Delta Lake Auto Compaction?","url":"https://www.examtopics.com/discussions/databricks/view/117466-exam-certified-data-engineer-professional-topic-1-question/","choices":{"E":"An asynchronous job runs after the write completes to detect if files could be further compacted; if yes, an OPTIMIZE job is executed toward a default of 128 MB.","A":"An asynchronous job runs after the write completes to detect if files could be further compacted; if yes, an OPTIMIZE job is executed toward a default of 1 GB.","B":"Before a Jobs cluster terminates, OPTIMIZE is executed on all tables modified during the most recent job.","D":"Data is queued in a messaging bus instead of committing data directly to memory; all data is committed from the messaging bus in one batch once the job is complete.","C":"Optimized writes use logical partitions instead of directory partitions; because partition boundaries are only represented in metadata, fewer small files are written."},"isMC":true,"exam_id":163,"answers_community":["E (56%)","B (31%)","13%"],"answer":"E"},{"id":"5O19TS9IYkdPvmKA2Tq2","unix_timestamp":1733679780,"exam_id":163,"answer_ET":"C","discussion":[{"poster":"lakime","timestamp":"1741690500.0","content":"Selected Answer: C\nOption C is correct because you can diagnose performance issues related to predicate push-down by examining the Physical Plan in the Query Detail screen of the Spark UI. Predicate push-down is a feature where filtering conditions (predicates) are applied as early as possible, typically in the data scan operation, to reduce the amount of data being read. If predicate push-down is not happening, the physical plan will show that the filter operation is being applied after the data is read, leading to inefficient queries.","upvote_count":"1","comment_id":"1387352"},{"timestamp":"1733679780.0","comment_id":"1323666","content":"Selected Answer: C\nLook at the Physical Plan","upvote_count":"1","poster":"benni_ale"}],"choices":{"C":"In the Query Detail screen, by interpreting the Physical Plan","A":"In the Executor’s log file, by grepping for \"predicate push-down\"","B":"In the Stage’s Detail screen, in the Completed Stages table, by noting the size of data read from the Input column","D":"In the Delta Lake transaction log. by noting the column statistics"},"answer_description":"","isMC":true,"url":"https://www.examtopics.com/discussions/databricks/view/152713-exam-certified-data-engineer-professional-topic-1-question/","question_text":"Where in the Spark UI can one diagnose a performance problem induced by not leveraging predicate push-down?","answer":"C","question_images":[],"topic":"1","timestamp":"2024-12-08 18:43:00","answers_community":["C (100%)"],"answer_images":[],"question_id":112},{"id":"2s4qdbmVnfpoSUcYFnuw","question_text":"A data engineer needs to capture pipeline settings from an existing setting in the workspace, and use them to create and version a JSON file to create a new pipeline.\n\nWhich command should the data engineer enter in a web terminal configured with the Databricks CLI?","question_id":113,"answers_community":["C (100%)"],"url":"https://www.examtopics.com/discussions/databricks/view/152716-exam-certified-data-engineer-professional-topic-1-question/","topic":"1","question_images":[],"isMC":true,"choices":{"B":"Stop the existing pipeline; use the returned settings in a reset command","A":"Use list pipelines to get the specs for all pipelines; get the pipeline spec from the returned results; parse and use this to create a pipeline","D":"Use the clone command to create a copy of an existing pipeline; use the get JSON command to get the pipeline definition; save this to git","C":"Use the get command to capture the settings for the existing pipeline; remove the pipeline_id and rename the pipeline; use this in a create command"},"discussion":[{"comment_id":"1323672","upvote_count":"2","timestamp":"1733680260.0","poster":"benni_ale","content":"Selected Answer: C\nI say C from common logical sense, however iI have not properly tested it... I just don't see any problems with that"}],"timestamp":"2024-12-08 18:51:00","answer":"C","answer_images":[],"answer_ET":"C","answer_description":"","exam_id":163,"unix_timestamp":1733680260},{"id":"orB34JO9sP88f3dof2O1","topic":"1","choices":{"D":"/jobs/get","B":"/jobs/list","A":"/jobs/runs/list","C":"/jobs/runs/get"},"question_id":114,"answers_community":["D (100%)"],"timestamp":"2024-12-06 16:09:00","isMC":true,"exam_id":163,"answer":"D","answer_ET":"D","unix_timestamp":1733497740,"question_images":[],"answer_description":"","url":"https://www.examtopics.com/discussions/databricks/view/152622-exam-certified-data-engineer-professional-topic-1-question/","discussion":[{"content":"Selected Answer: D\nmulti-task: /jobs/get\nsingle-task: /jobs/runs/get","comments":[{"poster":"arekm","upvote_count":"3","content":"The answer is right, the explanation wrong. Both single & multi task jobs definitions are returned by /jobs/get. The /jobs/runs/get gets the executed/run jobs bases on the definitions you can get from /jobs/get.","timestamp":"1735826040.0","comment_id":"1335582"}],"upvote_count":"1","timestamp":"1733497740.0","comment_id":"1322804","poster":"temple1305"}],"answer_images":[],"question_text":"Which REST API call can be used to review the notebooks configured to run as tasks in a multi-task job?"},{"id":"QLHg2dUpagOBFEL4qtbz","discussion":[{"poster":"arekm","timestamp":"1735826220.0","comment_id":"1335585","content":"Selected Answer: B\nB - to test functions we need to import them in our unit tests. This means that storing functions in notebooks is not a good idea. You store them separately - as \"files\" - and import in notebooks the same way you import them in unit tests.","upvote_count":"1"},{"timestamp":"1733744580.0","comment_id":"1324012","upvote_count":"2","poster":"Thameur01","content":"Selected Answer: B\nDatabricks Repos is the recommended way to organize and manage code, including functions and unit tests, in a scalable and maintainable way. By defining your functions and unit tests in Files in Repos, you can:\n\nModularize Your Code:\n\nFunctions can be organized into separate Python files or modules, making them reusable and easier to test.\nUse Standard Testing Frameworks:\n\nFrameworks like pytest or unittest can be used to write and execute unit tests against these functions.\nVersion Control Integration:\n\nFiles in Repos can be version-controlled using Git, ensuring traceability and collaboration.\nProduction Data Testing:\n\nWith proper safeguards, you can design unit tests to test production-like data while maintaining modularity and separation from production pipelines."},{"upvote_count":"1","content":"Selected Answer: C\nIt is true that or Python and R notebooks, Databricks recommends storing functions and their unit tests outside of notebooks so given the question is in Python environment some could argue B being the correct solution. Nevertheless that is only an \"advice\" and Databricks more generally states that in general, it is a best practice to not run unit tests against functions that work with data in production. This is especially important for functions that add, remove, or otherwise change data. To protect your production data from being compromised by your unit tests in unexpected ways, you should run unit tests against non-production data. So I would got for option C. Alternatively I would have gone for B. https://docs.databricks.com/en/notebooks/testing.html?utm_source=chatgpt.com#write-unit-tests","timestamp":"1733680560.0","comment_id":"1323674","comments":[{"timestamp":"1735826160.0","upvote_count":"1","content":"This does not really answer the question.","comment_id":"1335584","poster":"arekm"}],"poster":"benni_ale"}],"url":"https://www.examtopics.com/discussions/databricks/view/152717-exam-certified-data-engineer-professional-topic-1-question/","answer_ET":"B","unix_timestamp":1733680560,"answer_images":[],"answer_description":"","isMC":true,"timestamp":"2024-12-08 18:56:00","answers_community":["B (75%)","C (25%)"],"question_text":"A Data Engineer wants to run unit tests using common Python testing frameworks on Python functions defined across several Databricks notebooks currently used in production.\n\nHow can the data engineer run unit tests against functions that work with data in production?","choices":{"A":"Define and import unit test functions from a separate Databricks notebook","B":"Define and unit test functions using Files in Repos","C":"Run unit tests against non-production data that closely mirrors production","D":"Define unit tests and functions within the same notebook"},"question_images":[],"exam_id":163,"answer":"B","topic":"1","question_id":115}],"exam":{"name":"Certified Data Engineer Professional","isImplemented":true,"lastUpdated":"12 Apr 2025","isMCOnly":true,"isBeta":false,"id":163,"provider":"Databricks","numberOfQuestions":200},"currentPage":23},"__N_SSP":true}