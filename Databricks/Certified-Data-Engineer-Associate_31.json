{"pageProps":{"questions":[{"id":"BJulJpSW9796OyyrOz35","discussion":[{"timestamp":"1720271460.0","content":"Selected Answer: D\nThe important point of this scenario is \"when they are submitted to a non-running SQL endpoint\". So its not about increasing the instance size or the amount of instances to improve the query performance, but its about reducing the start-up time.\nA: Not possible, serverless can't be combined with spot instance policies, see https://docs.databricks.com/en/compute/sql-warehouse/serverless.html#limitations\nB: Auto Stop is about terminating a SQL warehouse after x minutes of being idle.\nC: Increasing the cluster size provides more capacities for running queries, but doesn't reduce start-up time.\nD: Serverless reduces start-up time from minutes to seconds. Jackpot!\nE: Increasing the max bound of the SQL endpoints scaling range will help with lots of sequencial queries, which is not the case here.","comment_id":"1115262","poster":"carpa_jo","upvote_count":"18"},{"upvote_count":"1","content":"Selected Answer: D\nD is correct. Key phrase is \"submitted to a non-running SQL endpoint\". Increasing cluster size is not going to help if that's in a state like non-running.","comment_id":"1127427","poster":"azure_bimonster","timestamp":"1721490180.0"},{"timestamp":"1720599300.0","comment_id":"1118438","poster":"bartfto","content":"Selected Answer: D\n\"when they are submitted to a non-running SQL endpoint\" ANSWER D","upvote_count":"1"},{"content":"Selected Answer: C\nC. They can increase the cluster size of the SQL endpoint.\n\nExplanation:\n\nIncreasing the cluster size of the SQL endpoint can enhance query performance by providing more computational resources to execute queries. This can potentially speed up query processing by allowing more parallelism, handling larger workloads, and reducing the time taken for query execution.","upvote_count":"2","poster":"Garyn","timestamp":"1719707640.0","comment_id":"1110217"},{"content":"key word, “non-running SQL endpoint” which implies that the query is slow because the cluster needs time to get started. \n\nI suggest answer D because : \n\nA : Serverless & spot instances cannot be mixed ? \n\nB : autostop means that jobs are submitted to non-running SQL endpoints \n\nC : increasing the clustersize can compensate for slow startup time \n\nD : serverless is able to start and scale faster than non-running SQL endpoints (seconds intead of minutes) \n\nE : increasing maximum bound will help only if there are simultaneous queries \n\nhttps://docs.gcp.databricks.com/en/lakehouse-architecture/cost-optimization/best-practices.html#use-serverless-for-your-workloads","upvote_count":"4","comment_id":"1101905","timestamp":"1718905620.0","poster":"AndreFR"},{"comment_id":"1097421","content":"Selected Answer: E\nmaximum bound of the SQL endpoint's scaling range","upvote_count":"2","timestamp":"1718456040.0","poster":"olaru"},{"content":"Selected Answer: C\nD is wrong - its already Serverless (non running SQL endpoint) how would turning Serverless ON help? They also says C here https://community.databricks.com/t5/data-engineering/when-to-increase-maximum-bound-vs-when-to-increase-cluster-size/td-p/27880 . E is only true for autoscaling clusters","upvote_count":"2","poster":"nedlo","timestamp":"1717664700.0","comment_id":"1089230"},{"content":"Selected Answer: C\nhttps://community.databricks.com/t5/data-engineering/sql-query-takes-too-long-to-run/td-p/21884","timestamp":"1717291020.0","upvote_count":"2","poster":"msengupta","comment_id":"1085672"},{"upvote_count":"2","comments":[{"upvote_count":"1","content":"I mean answer C","poster":"Syd","comment_id":"1056586","timestamp":"1714359480.0"}],"poster":"Syd","timestamp":"1714359420.0","content":"Answer E:\n\nhttps://www.databricks.com/blog/2022/03/10/top-5-databricks-performance-tips.html","comment_id":"1056585"},{"timestamp":"1713757920.0","comment_id":"1050190","comments":[{"comment_id":"1056363","upvote_count":"1","content":"I don't agree. Your answer is only valid when 'sequential' is mentioned, which is not the case here.","timestamp":"1714326600.0","poster":"mike_stewart"}],"upvote_count":"1","poster":"meow_akk","content":"Ans E : you re welcome :) \nhttps://community.databricks.com/t5/data-engineering/when-to-increase-maximum-bound-vs-when-to-increase-cluster-size/td-p/27880"}],"answer_description":"","exam_id":162,"timestamp":"2023-10-22 05:52:00","topic":"1","url":"https://www.examtopics.com/discussions/databricks/view/124310-exam-certified-data-engineer-associate-topic-1-question-82/","unix_timestamp":1697946720,"question_text":"A data engineering team has noticed that their Databricks SQL queries are running too slowly when they are submitted to a non-running SQL endpoint. The data engineering team wants this issue to be resolved.\n\nWhich of the following approaches can the team use to reduce the time it takes to return results in this scenario?","choices":{"B":"They can turn on the Auto Stop feature for the SQL endpoint.","A":"They can turn on the Serverless feature for the SQL endpoint and change the Spot Instance Policy to \"Reliability Optimized.\"","C":"They can increase the cluster size of the SQL endpoint.","E":"They can increase the maximum bound of the SQL endpoint's scaling range.","D":"They can turn on the Serverless feature for the SQL endpoint."},"answer":"D","question_images":[],"isMC":true,"answer_ET":"D","answers_community":["D (71%)","C (21%)","7%"],"answer_images":[],"question_id":151},{"id":"O7IFmDadbgrXMXoKzTgF","unix_timestamp":1697946780,"timestamp":"2023-10-22 05:53:00","question_id":152,"question_text":"A data engineer has a Job that has a complex run schedule, and they want to transfer that schedule to other Jobs.\n\nRather than manually selecting each value in the scheduling form in Databricks, which of the following tools can the data engineer use to represent and submit the schedule programmatically?","isMC":true,"answer_description":"","exam_id":162,"url":"https://www.examtopics.com/discussions/databricks/view/124311-exam-certified-data-engineer-associate-topic-1-question-83/","answer":"D","discussion":[{"content":"Selected Answer: D\nDatabricks allows for programmatic representation and submission of job schedules using Cron syntax, which is a standardized format for defining schedules. This approach is particularly useful for transferring complex schedules between different jobs or automating job scheduling.\n\nCron syntax specifies schedules with fields for minutes, hours, day of the month, month, day of the week, and optionally year, making it ideal for representing complex scheduling patterns programmatically.","comment_id":"1315969","poster":"806e7d2","timestamp":"1732215780.0","upvote_count":"1"},{"poster":"80370eb","comment_id":"1264139","upvote_count":"2","timestamp":"1723379820.0","content":"Selected Answer: D\nCron syntax is a powerful way to define complex schedules programmatically. In Databricks, you can use Cron syntax to set up the scheduling of jobs, which allows for more flexibility and ease when transferring the schedule to other jobs without manually selecting each value in the scheduling form."},{"poster":"55f31c8","comment_id":"1084550","upvote_count":"2","content":"Selected Answer: D\nhttps://docs.databricks.com/en/sql/user/queries/schedule-query.html","timestamp":"1701362160.0"},{"comment_id":"1050191","upvote_count":"3","poster":"meow_akk","content":"Ans D : Cron Syntax with that you can easily copy all the syntax","timestamp":"1697946780.0"}],"answer_ET":"D","answers_community":["D (100%)"],"choices":{"E":"There is no way to represent and submit this information programmatically","A":"pyspark.sql.types.DateType","D":"Cron syntax","B":"datetime","C":"pyspark.sql.types.TimestampType"},"answer_images":[],"question_images":[],"topic":"1"},{"id":"5qyPnzCEZhkKbgaflWbN","timestamp":"2023-10-22 05:55:00","unix_timestamp":1697946900,"question_id":153,"choices":{"D":"There is no way to notify the Job owner in the case of Job failure","C":"Setting up an Alert in the Notebook","A":"Manually programming in an alert system in each cell of the Notebook","E":"MLflow Model Registry Webhooks","B":"Setting up an Alert in the Job page"},"answer_ET":"B","url":"https://www.examtopics.com/discussions/databricks/view/124312-exam-certified-data-engineer-associate-topic-1-question-84/","isMC":true,"exam_id":162,"answers_community":["B (100%)"],"discussion":[{"upvote_count":"2","content":"Selected Answer: B\nIn Databricks, you can configure job notifications directly from the Jobs page, where you can specify that an email should be sent to the Job owner or other specified individuals in the case of Job failure. This is the most straightforward and automated way to ensure notifications are sent.","poster":"80370eb","timestamp":"1723379940.0","comment_id":"1264141"},{"upvote_count":"2","content":"Selected Answer: B\nSetting up an alert in Jobs page","comment_id":"1090954","timestamp":"1702029960.0","poster":"Lavpak"},{"poster":"meow_akk","content":"Ans B : https://docs.databricks.com/en/workflows/jobs/job-notifications.html","comment_id":"1050192","timestamp":"1697946900.0","upvote_count":"3"}],"question_text":"Which of the following approaches should be used to send the Databricks Job owner an email in the case that the Job fails?","question_images":[],"answer_images":[],"answer_description":"","topic":"1","answer":"B"},{"id":"3CcvaOdjq4IVfTmGHpBr","discussion":[{"content":"Selected Answer: C\nThe manager can schedule the query to refresh every 1 day from the query’s page in Databricks SQL (Option C). Here are the steps to do this:\n\n- In the Query Editor, click Schedule > Add schedule to open a menu with schedule settings.\n- Choose when to run the query. Use the dropdown pickers to specify the frequency, period, starting time, and time zone.\n- Click Create.","comment_id":"1110226","upvote_count":"3","poster":"Garyn","timestamp":"1719708240.0"},{"upvote_count":"1","timestamp":"1718904180.0","content":"Selected Answer: C\nhas to be every 1 day to run once day. https://docs.databricks.com/en/sql/user/queries/schedule-query.html","poster":"AndreFR","comment_id":"1101887"},{"comment_id":"1089765","content":"Selected Answer: C\nCorrect Answer is C","timestamp":"1717699740.0","poster":"kz_data","upvote_count":"1"},{"timestamp":"1714305780.0","content":"Selected Answer: C\nFrom the query editor page we have option to schedule the queries","upvote_count":"2","poster":"kishore1980","comment_id":"1056165"},{"comment_id":"1050785","upvote_count":"1","content":"Ans D : think option A might not be right since we are not doing scheduling in sql end points page","poster":"meow_akk","comments":[{"upvote_count":"6","timestamp":"1713862980.0","poster":"SD5713","comment_id":"1051632","content":"it is C, Question 41 of Practice Exam Databricks"}],"timestamp":"1713798840.0"}],"question_images":[],"choices":{"C":"They can schedule the query to refresh every 1 day from the query's page in Databricks SQL.","B":"They can schedule the query to refresh every 12 hours from the SQL endpoint's page in Databricks SQL.","E":"They can schedule the query to run every 12 hours from the Jobs UI.","D":"They can schedule the query to run every 1 day from the Jobs UI.","A":"They can schedule the query to refresh every 1 day from the SQL endpoint's page in Databricks SQL."},"answer_images":[],"topic":"1","unix_timestamp":1697987640,"exam_id":162,"question_id":154,"answer":"C","timestamp":"2023-10-22 17:14:00","question_text":"An engineering manager uses a Databricks SQL query to monitor ingestion latency for each data source. The manager checks the results of the query every day, but they are manually rerunning the query each day and waiting for the results.\n\nWhich of the following approaches can the manager use to ensure the results of the query are updated each day?","isMC":true,"url":"https://www.examtopics.com/discussions/databricks/view/124344-exam-certified-data-engineer-associate-topic-1-question-85/","answer_ET":"C","answers_community":["C (100%)"],"answer_description":""},{"id":"qGqbIVp1VpGEP3MbD8Ua","answers_community":["E (100%)"],"timestamp":"2023-10-22 17:15:00","answer_images":[],"discussion":[{"poster":"CommanderBigMac","comment_id":"1284139","upvote_count":"1","timestamp":"1726406280.0","content":"Selected Answer: E\nE is correct"},{"timestamp":"1702029600.0","poster":"Lavpak","comment_id":"1090952","upvote_count":"1","content":"Selected Answer: E\nhttps://docs.databricks.com/en/workflows/jobs/conditional-tasks.html"},{"content":"Ans E : E is correct since dependency means the dependent job must complete successfully.","poster":"meow_akk","comment_id":"1050786","timestamp":"1697987700.0","upvote_count":"2"}],"question_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/databricks/view/124345-exam-certified-data-engineer-associate-topic-1-question-86/","answer_description":"","unix_timestamp":1697987700,"isMC":true,"choices":{"A":"When another task needs to be replaced by the new task","D":"When another task needs to use as little compute resources as possible","E":"When another task needs to successfully complete before the new task begins","B":"When another task needs to fail before the new task begins","C":"When another task has the same dependency libraries as the new task"},"question_id":155,"answer_ET":"E","question_text":"In which of the following scenarios should a data engineer select a Task in the Depends On field of a new Databricks Job Task?","answer":"E","exam_id":162}],"exam":{"provider":"Databricks","isBeta":false,"id":162,"name":"Certified Data Engineer Associate","isMCOnly":true,"numberOfQuestions":169,"lastUpdated":"12 Apr 2025","isImplemented":true},"currentPage":31},"__N_SSP":true}