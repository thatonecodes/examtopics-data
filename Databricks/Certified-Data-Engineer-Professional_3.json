{"pageProps":{"questions":[{"id":"G1IIs43ohieuOtpgXvZx","question_text":"Which statement describes the default execution mode for Databricks Auto Loader?","choices":{"C":"Webhooks trigger a Databricks job to run anytime new data arrives in a source directory; new data are automatically merged into target tables using rules inferred from the data.","A":"Cloud vendor-specific queue storage and notification services are configured to track newly arriving files; the target table is materialized by directly querying all valid files in the source directory.","E":"Cloud vendor-specific queue storage and notification services are configured to track newly arriving files; new files are incrementally and idempotently loaded into the target Delta Lake table.","B":"New files are identified by listing the input directory; the target table is materialized by directly querying all valid files in the source directory.","D":"New files are identified by listing the input directory; new files are incrementally and idempotently loaded into the target Delta Lake table."},"answer_description":"","topic":"1","discussion":[{"timestamp":"1707178200.0","poster":"vctrhugo","content":"Selected Answer: D\n\"Auto Loader uses directory listing mode by default. In directory listing mode, Auto Loader identifies new files by listing the input directory.\"\n\nhttps://learn.microsoft.com/en-us/azure/databricks/ingestion/auto-loader/directory-listing-mode","upvote_count":"7","comment_id":"1141632"},{"timestamp":"1735812300.0","content":"Selected Answer: D\nD - Autoloader supports:\n1. directory listing mode\n2. file notification mode\nThe first option is the default. Answer E describes the second option.","poster":"arekm","upvote_count":"2","comment_id":"1335478"},{"comment_id":"1134517","timestamp":"1706482080.0","upvote_count":"2","comments":[{"timestamp":"1725412380.0","poster":"csrazdan","upvote_count":"1","content":"Correct answer is D. However, listing the input directory is the default way of identifying new files for auto loader. Cloud Native Notification services can be used but this is not default setting for auto loader.","comment_id":"1277897"}],"content":"D definitely !\nAuto Loader is an optimized file source that overcomes all the above limitations and provides a seamless way for data teams to load the raw data at low cost and latency with minimal DevOps effort. You just need to provide a source directory path and start a streaming job. The new structured streaming source, called \"cloudFiles\", will automatically set up file notification services that subscribe file events from the input directory and process new files as they arrive, with the option of also processing existing files in that directory.","poster":"Rinscy"},{"timestamp":"1706352600.0","content":"https://docs.databricks.com/en/ingestion/auto-loader/options.html#:~:text=By%20default%2C%20Auto%20Loader%20makes,as%20true%20or%20false%20respectively.\n\nSelected answer: D","poster":"ranith","upvote_count":"1","comment_id":"1133248"},{"timestamp":"1706302920.0","content":"D is the answer. The default execution mode for Databricks Auto Loader is the Directory Listing mode","upvote_count":"1","poster":"get_certified9","comment_id":"1132887"},{"comment_id":"1131843","timestamp":"1706199300.0","poster":"spaceexplorer","upvote_count":"1","comments":[{"poster":"spaceexplorer","upvote_count":"3","comments":[{"poster":"Isio05","upvote_count":"1","timestamp":"1717860300.0","content":"Surely it's not vendor specific solution","comment_id":"1226801"}],"content":"https://www.databricks.com/blog/2020/02/24/introducing-databricks-ingest-easy-data-ingestion-into-delta-lake.html","timestamp":"1706314140.0","comment_id":"1132961"}],"content":"Selected Answer: E\nE is the answer"}],"isMC":true,"timestamp":"2024-01-25 17:15:00","url":"https://www.examtopics.com/discussions/databricks/view/132138-exam-certified-data-engineer-professional-topic-1-question/","answer_ET":"D","answer_images":[],"unix_timestamp":1706199300,"answer":"D","answers_community":["D (90%)","10%"],"question_images":[],"question_id":11,"exam_id":163},{"id":"LsqagrTu1dqqMKpyA36U","timestamp":"2024-02-06 01:03:00","answer":"E","answer_images":[],"answers_community":["E (100%)"],"question_text":"A Delta Lake table representing metadata about content posts from users has the following schema:\n\nuser_id LONG, post_text STRING, post_id STRING, longitude FLOAT, latitude FLOAT, post_time TIMESTAMP, date DATE\n\nBased on the above schema, which column is a good candidate for partitioning the Delta Table?","discussion":[{"poster":"benni_ale","comment_id":"1304490","timestamp":"1730211840.0","upvote_count":"1","content":"Selected Answer: E\nDate is usually best candidate for time series data without further specifications"},{"timestamp":"1707177780.0","poster":"vctrhugo","comment_id":"1141629","upvote_count":"4","content":"Selected Answer: E\nPartitioning a Delta Lake table on the date column is a common practice. This is because partitioning by date can significantly improve query performance when dealing with time-series data. It allows for efficient filtering of data based on time periods, which is a common requirement in many analytics workloads. Partitioning by date also helps manage the size of your partitions, as each partition will contain only the data for a specific date. This can lead to more efficient reads and writes, and can also make it easier to manage and maintain your data."}],"isMC":true,"choices":{"A":"post_time","B":"latitude","C":"post_id","E":"date","D":"user_id"},"answer_ET":"E","answer_description":"","exam_id":163,"question_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/132980-exam-certified-data-engineer-professional-topic-1-question/","unix_timestamp":1707177780,"question_id":12,"topic":"1"},{"id":"02gUaptuYlAMQae3eZRi","answer_ET":"E","timestamp":"2023-08-02 08:46:00","exam_id":163,"choices":{"E":"Because the default data retention threshold is 7 days, data files containing deleted records will be retained until the VACUUM job is run 8 days later.","B":"Because the default data retention threshold is 24 hours, data files containing deleted records will be retained until the VACUUM job is run the following day.","A":"Because the VACUUM command permanently deletes all files containing deleted records, deleted records may be accessible with time travel for around 24 hours.","C":"Because Delta Lake time travel provides full access to the entire history of a table, deleted records can always be recreated by users with full admin privileges.","D":"Because Delta Lake's delete statements have ACID guarantees, deleted records will be permanently purged from all storage systems as soon as a delete job completes."},"answer_description":"","unix_timestamp":1690958760,"answers_community":["E (61%)","A (39%)"],"question_id":13,"isMC":true,"topic":"1","discussion":[{"content":"Answer is E, default retention period is 7 days https://learn.microsoft.com/en-us/azure/databricks/delta/vacuum","comment_id":"969790","timestamp":"1690958760.0","upvote_count":"20","poster":"asmayassineg"},{"content":"Selected Answer: A\nThe answer has to be A.\nThe deletion is done on Sunday 1am and then the next day Monday 3am, VACUUM was initiated, so one can only time travel for about 24 hours.","timestamp":"1709001240.0","comment_id":"1160222","poster":"mardigras","comments":[{"comment_id":"1279271","poster":"csrazdan","comments":[{"content":"Exactly!","poster":"benni_ale","comment_id":"1297978","timestamp":"1728971700.0","upvote_count":"2"}],"upvote_count":"8","content":"The default retention threshold for time travel is 7 days. VACUUM which is executed on Monday 3 am will remove history for changes where time travel has expired for previous 7 days.","timestamp":"1725583560.0"}],"upvote_count":"12"},{"content":"Selected Answer: E\nTeam configured weekly deletion and vacuum will delete weekly data. So this is correct for me.","timestamp":"1743910140.0","comment_id":"1556603","poster":"Deep92","upvote_count":"1"},{"content":"Selected Answer: E\nRead question carefully: \"all deletions from the previous week\", \"Every Monday at 3am, a batch job executes a series of VACUUM commands\" Logically 8th day.\nAppply basics: Default retention threshold for VACUUM is 7 days beyond which on running VACUUM data will be purged.","comment_id":"1361619","poster":"Tedet","timestamp":"1740527820.0","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: E\nE is correct, because the VACUUM retention is of 168h or 7 days, after statments of deletions.","comment_id":"1350616","poster":"fabiospont","timestamp":"1738522320.0"},{"content":"Selected Answer: E\nSaturday delete puts the deleted records in the transaction log. The retention clock starts ticking. Since the default for keeping the history is 168 hours (7 days), by no means the following Monday vacuum removes since the clock did not reach 168 hours (7 days) - it is at hour 26.","timestamp":"1735642860.0","poster":"arekm","comment_id":"1334744","upvote_count":"1"},{"comment_id":"1324383","poster":"Anithec0der","upvote_count":"1","timestamp":"1733805780.0","content":"Selected Answer: E\nI was also thinking in the same way that data will be deleted immediate after the vaccum command is run but it actually logically deletes the data and not physically till the 7 day from the ask of vaccum command. so E is perfect."},{"upvote_count":"1","timestamp":"1733399640.0","content":"Selected Answer: E\nAnswer E. The retention period for time travel queries in Delta Lake is controlled by a 7-day default, not 24 hours. Hence, the statement (Option A) that deleted records may be accessible for around 24 hours is incorrect in the context of Delta Lake's default retention period.","comment_id":"1322341","poster":"AlejandroU"},{"poster":"benni_ale","comment_id":"1297980","timestamp":"1728971760.0","upvote_count":"1","content":"Selected Answer: E\nDefault retention period is 7 days so the vacuum command won't delete the files corresponding to deleted rows at Sunday 1 am but the ones of the previous week instead."},{"timestamp":"1727842920.0","poster":"tangerine141","comment_id":"1292204","content":"Selected Answer: E\nDelta Lake's default retention threshold for old data files (which allows time travel) is 7 days. This means that even after records are deleted, the files that previously contained those records are kept for 7 days before they are eligible for permanent deletion by the VACUUM command.\nThe VACUUM command is responsible for permanently deleting the old data files after the retention period. Since the job runs every Monday, this means that data deleted during the previous week will not be fully purged until after the retention period has passed (which would be 8 days after the deletion, considering the weekly processing).","upvote_count":"2"},{"poster":"akashdesarda","comment_id":"1290679","content":"Selected Answer: E\nDelete job is running as batch job for all requests made current week on Sunday & Vacuum is ran next day . Since there is no mention of change is retention period then it is 7 days. Vacuum will delete data older than 7 days, i.e. it will delete data of previous week & not current week. Current weeks data will be removed in next week’s vacuum job.","upvote_count":"1","timestamp":"1727536080.0"},{"poster":"fe3b2fc","upvote_count":"3","content":"Selected Answer: E\nFrom the documentation.\n\"The default retention threshold for data files after running VACUUM is 7 days.\" \nIt doesn't matter if VACUUM is ran the following day, the retention period on a default setup is still 7 days after they do the VACUUM on Monday.","comment_id":"1265869","timestamp":"1723653240.0"},{"comments":[{"poster":"akashdesarda","upvote_count":"2","content":"This week's vacuum will remove data of the previous week's delete command since default retention has not changed.","comment_id":"1290682","timestamp":"1727536140.0"}],"upvote_count":"3","content":"Selected Answer: A\nThey expect the deleted records for the previous week to be deleted Sunday from 1am to 2am. Then the next day(Monday) at 3am approx 24hrs later, the vacuum command is ran. This means the records from the previous week are only around for 24ish hours before they are removed with the vacuum command. They aren't waiting 8 days to run the command, there fore E is wrong.","comment_id":"1237848","timestamp":"1719456900.0","poster":"03355a2"},{"timestamp":"1717547520.0","content":"E. Because the default data retention threshold is 7 days, data files containing deleted records will be retained until the VACUUM job is run 8 days later.","comment_id":"1224442","poster":"imatheushenrique","upvote_count":"1"},{"comment_id":"1213858","poster":"coercion","content":"Selected Answer: E\nDefault retention period is 7 days so newly deleted data on Sunday will be available for next 7 days (even if vacuum was run on Monday as it will delete 7 days old data and not the data that was loaded yesterday \"Sunday\" )","timestamp":"1716132540.0","upvote_count":"1"},{"content":"Selected Answer: E\nThe default retention threshold for data files after running VACUUM is 7 days.","upvote_count":"1","comment_id":"1204722","timestamp":"1714509420.0","poster":"Tayari"},{"poster":"hedbergare","timestamp":"1712660160.0","content":"Selected Answer: E\nAnswer is E","upvote_count":"1","comment_id":"1192232"},{"poster":"juliom6","upvote_count":"2","timestamp":"1712604180.0","content":"Selected Answer: A\nSi bien la data es borrada (DELETE) el domingo, aún se puede recuperar ella mediante time traveling, sólo el día siguiente (lunes) se eliminará esta posibilidad debido a que se ejecuta el VACUUM, en consecuencia la data se podrá recuperar en ese lapso de 24 horas aprox","comment_id":"1191793"},{"upvote_count":"1","comment_id":"1145982","content":"Selected Answer: E\nif i v0: create table, v1: insert 2 reocrds, v2: insert 2 record, v3: delete 2 records, and then run the vacuum command (with default 7 day retention), the delete records will be there and you can access using SELECT * FROM delta_table VERSION AS OF 2;","timestamp":"1707553740.0","poster":"RiktRikt007"},{"timestamp":"1706115660.0","upvote_count":"1","comment_id":"1130888","content":"Selected Answer: E\nAnswer is E","poster":"spaceexplorer"},{"upvote_count":"1","content":"Selected Answer: E\nAnswer is E","poster":"kz_data","comment_id":"1118559","timestamp":"1704891600.0"},{"poster":"kz_data","upvote_count":"1","comment_id":"1118558","content":"Answer is E as the default retention period is 7 days","timestamp":"1704891540.0"},{"comment_id":"1117264","poster":"RafaelCFC","timestamp":"1704785700.0","upvote_count":"1","content":"Selected Answer: E\nCorrect according to the documentation: https://docs.databricks.com/en/sql/language-manual/delta-vacuum.html"},{"timestamp":"1701512520.0","poster":"hamzaKhribi","content":"Correct answer is E, In this question tables are with default settings and giving delta retention is 7 days the data will still be accessible for the last 7 days.","upvote_count":"1","comment_id":"1086048"},{"poster":"aragorn_brego","upvote_count":"6","content":"Selected Answer: E\nDelta Lake's time travel feature allows you to query an older snapshot of a table. By default, Delta Lake retains a 7-day history for the table to support operations like time travel. When data is deleted from a Delta table, the actual data files are not immediately removed from the storage layer; they are just marked for deletion. The VACUUM command is used to clean up these files that are no longer in the state of the table, but it will not remove any files that fall within the retention period unless it is run with an override option to reduce the retention period.\n\nThus, if the deletions are processed on Sunday and the VACUUM command is run on Monday without overriding the default retention period, the deleted records would still be accessible via time travel for approximately 8 days (until the next run of the VACUUM command after the data has aged past the 7-day retention period).","comment_id":"1075927","timestamp":"1700522820.0"},{"content":"Answer is E","upvote_count":"2","poster":"BIKRAM063","comment_id":"1060781","timestamp":"1698950040.0"},{"poster":"sturcu","upvote_count":"1","comment_id":"1040257","content":"Selected Answer: E\nVacuum by default retention is 7 days","timestamp":"1697006640.0"},{"poster":"Eertyy","comment_id":"991549","content":"e is right answer","upvote_count":"6","timestamp":"1693151220.0"}],"answer_images":[],"answer":"E","question_text":"The data engineering team has configured a job to process customer requests to be forgotten (have their data deleted). All user data that needs to be deleted is stored in Delta Lake tables using default table settings.\nThe team has decided to process all deletions from the previous week as a batch job at 1am each Sunday. The total duration of this job is less than one hour. Every Monday at 3am, a batch job executes a series of VACUUM commands on all Delta Lake tables throughout the organization.\nThe compliance officer has recently learned about Delta Lake's time travel functionality. They are concerned that this might allow continued access to deleted data.\nAssuming all delete logic is correctly implemented, which statement correctly addresses this concern?","question_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/117060-exam-certified-data-engineer-professional-topic-1-question/"},{"id":"VYbv6i0YiCrD11hUNgc3","timestamp":"2023-11-22 02:38:00","unix_timestamp":1700617080,"answer_images":[],"topic":"1","question_id":14,"answer":"A","exam_id":163,"question_text":"A large company seeks to implement a near real-time solution involving hundreds of pipelines with parallel updates of many tables with extremely high volume and high velocity data.\n\nWhich of the following solutions would you implement to achieve this requirement?","answer_ET":"A","url":"https://www.examtopics.com/discussions/databricks/view/126796-exam-certified-data-engineer-professional-topic-1-question/","discussion":[{"content":"Selected Answer: B\nHigh Concurrency clusters are depricated, so B https://learn.microsoft.com/en-us/answers/questions/1688410/are-high-concurrency-clusters-deprecated-or-rename","timestamp":"1733331060.0","comment_id":"1322023","poster":"natadatabricksadf","upvote_count":"3"},{"comment_id":"1320684","timestamp":"1733082540.0","comments":[{"comment_id":"1338077","timestamp":"1736367300.0","poster":"hesamh","upvote_count":"1","content":"doesn't it led to write small files (les than 1 GB) in each partitions?"}],"upvote_count":"3","content":"Selected Answer: B\nHigh Concurrency clusters are depricated, so B?","poster":"temple1305"},{"upvote_count":"2","poster":"shaojunni","content":"Selected Answer: A\n\"hundreds of pipelines with parallel updates of many tables\" indicates updating many tables concurrently via many pipelines. A is the best solution for that. B is the answer for updating a few large tables with few partitions.","timestamp":"1726850400.0","comment_id":"1286961"},{"comment_id":"1267588","content":"Selected Answer: B\n\"Which of the following solutions\"\nI'm sure this is a question with multichoice. A and B options are correct together.","timestamp":"1723886520.0","poster":"practicioner","upvote_count":"1"},{"poster":"BrianNguyen95","content":"Selected Answer: B\nHigh volume and high-velocity data ingestion often becomes a bottleneck due to limited write parallelism. By partitioning ingestion tables based on small time durations (e.g., hourly or even minutes), you create many smaller partitions. This allows concurrent writes to different partitions, significantly increasing the overall throughput of your data ingestion.","comment_id":"1224589","timestamp":"1717574340.0","upvote_count":"2"},{"content":"Selected Answer: A\nSince multiple pipelines are being used high concurrency cluster would give maximum resource utilization.","timestamp":"1716478680.0","upvote_count":"1","poster":"svik","comment_id":"1216696"},{"content":"A.\nB is only useful to improve performance of large tables ingestions.","comment_id":"1193397","timestamp":"1712805420.0","poster":"Er5","upvote_count":"1"},{"comment_id":"1162060","content":"Selected Answer: D\nWhy not D?","timestamp":"1709164740.0","poster":"Curious76","upvote_count":"2"},{"content":"Both options A and B could be relevant depending on the specific details of the use case. If the emphasis is on optimizing concurrent queries and overall data throughput, option A might be more appropriate. If the primary concern is parallel updates of tables with high-volume, high-velocity data, option B is a more targeted approach.","comment_id":"1141625","timestamp":"1707177600.0","upvote_count":"1","poster":"vctrhugo"},{"timestamp":"1706866920.0","content":"Selected Answer: B\nThe best way to deal with high volume and high velocity data is to use partitioning","upvote_count":"1","poster":"PrincipalJoe","comment_id":"1138390"},{"content":"Selected Answer: A\nDatabricks High Concurrency cluster","comment_id":"1119522","upvote_count":"2","poster":"bacckom","timestamp":"1704963420.0"},{"timestamp":"1701518940.0","content":"Selected Answer: A\n1) Partitioning by Time:\nPartitioning tables by a small time duration allows for efficient parallelism in data writes. Each time partition can be processed independently, enabling parallel updates to multiple partitions concurrently.\n2)Optimizing for Parallelism:\nBy partitioning the tables based on time, data can be ingested and processed in parallel, providing the ability to handle high volume and high velocity data effectively.\n\nRegarding option A, Databricks High Concurrency clusters are more focused on supporting a large number of concurrent users, which might not directly address the requirement for parallel updates of many tables with extremely high volume and high velocity data","poster":"petrv","comments":[{"timestamp":"1717847940.0","comment_id":"1226721","poster":"Isio05","upvote_count":"1","comments":[{"content":"Sorry, after going through this question once more - I'll go with B also. It will allow utilize parallelism in an efficient way.","comment_id":"1230610","upvote_count":"1","poster":"Isio05","timestamp":"1718388720.0"}],"content":"Usage of high conc. clusters can be beneficial both for mulitple users and jobs/queries running on them"},{"timestamp":"1701519000.0","content":"sorry, the selected answer should have been B","upvote_count":"1","poster":"petrv","comment_id":"1086147"}],"upvote_count":"1","comment_id":"1086146"},{"content":"Selected Answer: A\nHigh Concurrency clusters in Databricks are designed for multiple concurrent users and workloads. They provide fine-grained sharing of cluster resources and are optimized for operations such as running multiple parallel queries and updates. This would be suitable for a solution that involves many pipelines with parallel updates, especially with high volume and high velocity data.","poster":"aragorn_brego","timestamp":"1700617080.0","comment_id":"1076850","upvote_count":"4"}],"isMC":true,"answer_description":"","choices":{"C":"Configure Databricks to save all data to attached SSD volumes instead of object storage, increasing file I/O significantly.","D":"Isolate Delta Lake tables in their own storage containers to avoid API limits imposed by cloud vendors.","B":"Partition ingestion tables by a small time duration to allow for many data files to be written in parallel.","A":"Use Databricks High Concurrency clusters, which leverage optimized cloud storage connections to maximize data throughput.","E":"Store all tables in a single database to ensure that the Databricks Catalyst Metastore can load balance overall throughput."},"answers_community":["A (45%)","B (45%)","9%"],"question_images":[]},{"id":"DTp4kPnjC3Wx52BA5wHy","unix_timestamp":1700079120,"topic":"1","answer_images":[],"question_id":15,"answer_ET":"C","answers_community":["C (100%)"],"isMC":true,"exam_id":163,"discussion":[{"content":"C is correct","comment_id":"1092032","timestamp":"1733775420.0","upvote_count":"3","poster":"JamesWright"},{"content":"Selected Answer: C\nIn Databricks notebooks, you can use the %pip install command in a notebook cell to install a Python package. This will install the package on all nodes in the currently active cluster at the notebook level. It is a feature provided by Databricks to facilitate the installation of Python libraries for the notebook environment specifically.","upvote_count":"3","poster":"aragorn_brego","timestamp":"1732239540.0","comment_id":"1076851"},{"poster":"60ties","content":"Selected Answer: C\nC is correct","comment_id":"1071845","upvote_count":"3","timestamp":"1731701520.0"}],"choices":{"E":"Install libraries from PyPI using the cluster UI","A":"Run source env/bin/activate in a notebook setup script","D":"Use %sh pip install in a notebook cell","C":"Use %pip install in a notebook cell","B":"Use b in a notebook cell"},"answer":"C","question_text":"Which describes a method of installing a Python package scoped at the notebook level to all nodes in the currently active cluster?","question_images":[],"answer_description":"","timestamp":"2023-11-15 21:12:00","url":"https://www.examtopics.com/discussions/databricks/view/126313-exam-certified-data-engineer-professional-topic-1-question/"}],"exam":{"name":"Certified Data Engineer Professional","numberOfQuestions":200,"id":163,"isImplemented":true,"lastUpdated":"12 Apr 2025","provider":"Databricks","isBeta":false,"isMCOnly":true},"currentPage":3},"__N_SSP":true}