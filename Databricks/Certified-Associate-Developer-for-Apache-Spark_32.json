{"pageProps":{"questions":[{"id":"GPpZpJ4gPPzLMjKUJqvO","timestamp":"2023-07-30 20:01:00","answer":"A","question_text":"Which of the following code blocks returns a new DataFrame where column division from DataFrame storesDF has been replaced and renamed to column state and column managerName from DataFrame storesDF has been replaced and renamed to column managerFullName?","answer_ET":"A","choices":{"A":"storesDF.withColumnRenamed(\"division\", \"state\")\n.withColumnRenamed(\"managerName\", \"managerFullName\")","D":"storesDF.withColumnRenamed(Seq(\"division\", \"state\"), Seq(\"managerName\", \"managerFullName\"))","C":"storesDF.withColumn(\"state\", col(\"division\"))\n.withColumn(\"managerFullName\", col(\"managerName\"))","B":"storesDF.withColumn(\"state\", \"division\")\n.withColumn(\"managerFullName\", \"managerName\")","E":"storesDF.withColumnRenamed(\"state\", \"division\")\n.withColumnRenamed(\"managerFullName\", \"managerName\")"},"answer_images":[],"question_images":[],"topic":"1","answer_description":"","exam_id":161,"unix_timestamp":1690740060,"isMC":true,"question_id":156,"answers_community":["A (100%)"],"url":"https://www.examtopics.com/discussions/databricks/view/116805-exam-certified-associate-developer-for-apache-spark-topic-1/","discussion":[{"content":"Selected Answer: A\nA would be the right one here.","poster":"azure_bimonster","comment_id":"1145494","upvote_count":"1","timestamp":"1723202400.0"},{"timestamp":"1706644860.0","poster":"zozoshanky","comments":[{"content":"you are wrong! The answer is A","poster":"newusername","upvote_count":"1","comment_id":"1064980","timestamp":"1715090940.0"}],"upvote_count":"1","content":"B is the answer.","comment_id":"967382"}]},{"id":"AyLku9K7QhurapUqp8I0","timestamp":"2024-02-08 02:59:00","answer":"E","question_text":"Which of the following code blocks returns a new DataFrame where column sqft from DataFrame storesDF has had its missing values replaced with the value 30,000?\n\nA sample of DataFrame storesDF is below:\n\n//IMG//","choices":{"D":"storesDF.fillna(30000, col(\"sqft\"))","C":"storesDF.na.fill(30000, col(\"sqft\"))","B":"storesDF.nafill(30000, col(\"sqft\"))","A":"storesDF.na.fill(30000, Seq(\"sqft\"))","E":"storesDF.na.fill(30000, \"sqft\")"},"answer_ET":"E","answer_images":[],"question_images":["https://img.examtopics.com/certified-associate-developer-for-apache-spark/image10.png"],"topic":"1","answer_description":"","unix_timestamp":1707357540,"exam_id":161,"isMC":true,"answers_community":["E (60%)","C (20%)","A (20%)"],"question_id":157,"url":"https://www.examtopics.com/discussions/databricks/view/133328-exam-certified-associate-developer-for-apache-spark-topic-1/","discussion":[{"timestamp":"1733341140.0","poster":"Samir_91","upvote_count":"5","content":"E is answer. It's tested.\nA)AttributeError: module 'pyspark.sql.functions' has no attribute 'Seq'\nB)AttributeError: 'DataFrame' object has no attribute 'nafill'\nC)PySparkTypeError: [NOT_LIST_OR_TUPLE] Argument `subset` should be a list or tuple, got Column.\nD)PySparkTypeError: [NOT_LIST_OR_TUPLE] Argument `subset` should be a list or tuple, got Column.","comment_id":"1224232"},{"poster":"Samir_91","upvote_count":"3","comment_id":"1226894","content":"Selected Answer: E\nE is answer","timestamp":"1733692440.0"},{"upvote_count":"1","poster":"hosniadel666","comment_id":"1179080","content":"Selected Answer: A\nCheck fill function at scala API docs\nhttps://spark.apache.org/docs/3.0.0/api/scala/org/apache/spark/sql/DataFrameNaFunctions.html#fill(value:Long,cols:Array%5BString%5D):org.apache.spark.sql.DataFrame","timestamp":"1726902900.0"},{"poster":"azure_bimonster","timestamp":"1723203120.0","comments":[{"content":"it is wrong. na.fill function accepts str, list or tuple as the second argument, not column object","poster":"tmz1","upvote_count":"1","comment_id":"1349079","timestamp":"1738244580.0"}],"comment_id":"1145503","upvote_count":"1","content":"Selected Answer: C\nTo me C is likely correct, because we need to use col()\n\nC. storesDF.na.fill(30000, col(\"sqft\"))"},{"content":"typo error I THINK 1st arg is col name right ?","upvote_count":"2","timestamp":"1723075140.0","poster":"learnsh1","comment_id":"1143964"}]},{"id":"f42ps9NpopIeep1EtqKd","unix_timestamp":1693967880,"answer_description":"","choices":{"B":"DataFrame.dropDuplicates() and DataFrame.distinct()","D":"DataFrame.drop_duplicates()","C":"DataFrame.dropDuplicates()","A":"DataFrame.distinct()","E":"DataFrame.dropDuplicates(), DataFrame.distinct() and DataFrame.drop_duplicates()"},"discussion":[{"timestamp":"1724663100.0","content":"Answer E\ndrop_duplicates() is an alias for dropDuplicates() it also work in pyspark","comment_id":"1159620","upvote_count":"1","poster":"Ahlo"},{"upvote_count":"1","comment_id":"1145506","poster":"azure_bimonster","timestamp":"1723203300.0","content":"Selected Answer: E\nit asks \"most complete\" one, so E would be correct as all these three options would work in pyspark"},{"upvote_count":"1","comment_id":"1000107","content":"B\nThe most complete answer is B. DataFrame.dropDuplicates() and DataFrame.distinct(). Both DataFrame.distinct() and DataFrame.dropDuplicates() methods in PySpark can be used to return a new DataFrame with duplicate rows removed. The DataFrame.drop_duplicates() method is used in pandas, not in PySpark.","poster":"thanab","comments":[{"timestamp":"1713546780.0","comment_id":"1048142","poster":"juadaves","content":"It should be E, drop_duplicates() works in pyspark too.","upvote_count":"1"}],"timestamp":"1709699880.0"}],"topic":"1","answer_ET":"E","answer_images":[],"isMC":true,"answer":"E","answers_community":["E (100%)"],"exam_id":161,"question_images":[],"timestamp":"2023-09-06 04:38:00","question_id":158,"url":"https://www.examtopics.com/discussions/databricks/view/120030-exam-certified-associate-developer-for-apache-spark-topic-1/","question_text":"Which of the following operations can be used to return a DataFrame with no duplicate rows? Please select the most complete answer."},{"id":"r1DNi3InCpidR3ooH3az","question_images":[],"timestamp":"2024-04-02 13:05:00","question_text":"QUESTION NO: 75 -\n\nWhich of the following code blocks returns a DataFrame where column divisionDistinct is the approximate number of distinct values in column division from DataFrame storesDF?","url":"https://www.examtopics.com/discussions/databricks/view/137737-exam-certified-associate-developer-for-apache-spark-topic-1/","isMC":true,"exam_id":161,"answer_ET":"C","answer_images":[],"topic":"1","answers_community":[],"choices":{"A":"storesDF.withColumn(\"divisionDistinct\", approx_count_distinct(col(\"division\")))","B":"storesDF.agg(col(\"division\").approx_count_distinct(\"divisionDistinct\"))","C":"storesDF.agg(approx_count_distinct(col(\"division\")).alias(\"divisionDistinct\"))","D":"storesDF.withColumn(\"divisionDistinct\", col(\"division\").approx_count_distinct())","E":"storesDF.agg(col(\"division\").approx_count_distinct().alias(\"divisionDistinct\"))"},"question_id":159,"discussion":[{"upvote_count":"2","poster":"Sowwy1","comment_id":"1187982","content":"I think it's C\nhttps://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.approx_count_distinct.html","timestamp":"1727867100.0"}],"answer":"C","unix_timestamp":1712055900,"answer_description":""},{"id":"1gcLDUnGTBJaKa25JUP2","answer_images":[],"answer_ET":"A","question_id":160,"answers_community":["A (100%)"],"discussion":[{"comment_id":"968486","content":"Selected Answer: A\nA is the right one","poster":"cookiemonster42","timestamp":"1722457200.0","upvote_count":"1"}],"topic":"1","url":"https://www.examtopics.com/discussions/databricks/view/116935-exam-certified-associate-developer-for-apache-spark-topic-1/","answer_description":"","unix_timestamp":1690834800,"timestamp":"2023-07-31 22:20:00","question_images":[],"exam_id":161,"answer":"A","question_text":"The code block shown below should return a new DataFrame with the mean of column sqft from DataFrame storesDF in column sqftMean. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.\n\nCode block:\n\nstoresDF.__1__(__2__(__3__).alias(\"sqftMean\"))","isMC":true,"choices":{"B":"1. withColumn\n2. mean\n3. col(\"sqft\")","E":"1. agg\n2. mean\n3. \"sqft\"","D":"1. mean\n2. col\n3. \"sqft\"","C":"1. agg\n2. average\n3. col(\"sqft\")","A":"1. agg\n2. mean\n3. col(\"sqft\")"}}],"exam":{"name":"Certified Associate Developer for Apache Spark","isImplemented":true,"id":161,"numberOfQuestions":185,"lastUpdated":"12 Apr 2025","isBeta":false,"provider":"Databricks","isMCOnly":true},"currentPage":32},"__N_SSP":true}