{"pageProps":{"questions":[{"id":"upyTGCa5ytq15hW7rnKN","question_images":[],"choices":{"E":"Ensures code is optimized for a real-life workflow","D":"Closely simulates real world usage of your application","C":"Provides testing coverage for all code paths and branches","A":"Makes it easier to automate your test suite","B":"Pinpoints errors in the building blocks of your application"},"answer_images":[],"topic":"1","unix_timestamp":1706074260,"discussion":[{"timestamp":"1722897060.0","comment_id":"1141641","upvote_count":"4","content":"Selected Answer: D\nEnd-to-end testing is a methodology used to test whether the flow of an application is performing as designed from start to finish. The purpose of carrying out end-to-end tests is to identify system dependencies and to ensure that the right information is passed between various system components and systems. The entire application is tested in a real-world scenario such as communicating with the database, network, hardware, and other applications. Therefore, it closely simulates real-world usage of the application. Other options are benefits of different types of testing, not specifically end-to-end testing.","poster":"vctrhugo"},{"content":"Selected Answer: D\nD is correct","poster":"spaceexplorer","comment_id":"1131807","timestamp":"1721914500.0","upvote_count":"2"},{"poster":"Def21","content":"Selected Answer: B\nEnd-to-end tests use an example scenario, do not necessarily follow complex real world. Unit tests are component wise, end-to-end tests go over components.","comment_id":"1130250","upvote_count":"1","comments":[{"upvote_count":"1","timestamp":"1735811460.0","content":"B suggests an integration tests","comment_id":"1335465","poster":"arekm"}],"timestamp":"1721791860.0"}],"isMC":true,"answers_community":["D (86%)","14%"],"answer_description":"","answer_ET":"D","url":"https://www.examtopics.com/discussions/databricks/view/131948-exam-certified-data-engineer-professional-topic-1-question/","timestamp":"2024-01-24 06:31:00","question_id":6,"exam_id":163,"answer":"D","question_text":"Which statement describes a key benefit of an end-to-end test?"},{"id":"vnGqnW4JNf2fsXR481t4","answer_description":"","exam_id":163,"url":"https://www.examtopics.com/discussions/databricks/view/131955-exam-certified-data-engineer-professional-topic-1-question/","question_id":7,"question_text":"The Databricks CLI is used to trigger a run of an existing job by passing the job_id parameter. The response that the job run request has been submitted successfully includes a field run_id.\n\nWhich statement describes what the number alongside this field represents?","discussion":[{"upvote_count":"4","timestamp":"1722896880.0","comment_id":"1141640","poster":"vctrhugo","content":"Selected Answer: E\nThe number alongside the \"run_id\" field represents the globally unique identifier assigned to the newly triggered run of the job. Each run of a job in Databricks is assigned a unique run_id, allowing you to track and reference that specific execution of the job."},{"comment_id":"1130258","poster":"Def21","timestamp":"1721792160.0","content":"Selected Answer: E\nVerified from Databricks UI","upvote_count":"1"}],"unix_timestamp":1706074560,"isMC":true,"answer":"E","answer_images":[],"answer_ET":"E","question_images":[],"topic":"1","choices":{"E":"The globally unique ID of the newly triggered run.","C":"The number of times the job definition has been run in this workspace.","D":"The job_id is returned in this field.","A":"The job_id and number of times the job has been run are concatenated and returned.","B":"The total number of jobs that have been run in the workspace."},"answers_community":["E (100%)"],"timestamp":"2024-01-24 06:36:00"},{"id":"cudI4e05eqWozPIBqSdV","exam_id":163,"timestamp":"2023-11-15 17:30:00","choices":{"E":"df.apply(model, columns).select(\"customer_id, predictions\")","D":"df.select(\"customer_id\", pandas_udf(model, columns).alias(\"predictions\"))","B":"df.select(\"customer_id\", model(*columns).alias(\"predictions\"))","A":"df.map(lambda x:model(x[columns])).select(\"customer_id, predictions\")","C":"model.predict(df, columns)"},"answer_description":"","isMC":true,"question_images":["https://img.examtopics.com/certified-data-engineer-professional/image48.png"],"question_id":8,"discussion":[{"timestamp":"1716333780.0","upvote_count":"7","content":"Selected Answer: B\nThis code block applies the Spark UDF created from the MLflow model to the DataFrame df by selecting the existing customer_id column and the new column produced by the model, which is aliased to predictions. The model(*columns) part is where the UDF is applied to the columns specified in the columns list, and alias(\"predictions\") is used to name the output column of the model's predictions. This will result in a DataFrame with the desired schema: \"customer_id LONG, predictions DOUBLE\".","comment_id":"1076846","poster":"aragorn_brego"},{"comment_id":"1111560","upvote_count":"6","timestamp":"1719882120.0","poster":"divingbell17","content":"Selected Answer: B\nB is correct. It's a spark udf not pandas"},{"comment_id":"1071629","upvote_count":"2","poster":"60ties","content":"I think it is B","timestamp":"1715783400.0"}],"answer":"B","unix_timestamp":1700065800,"question_text":"The data science team has created and logged a production model using MLflow. The model accepts a list of column names and returns a new column of type DOUBLE.\n\nThe following code correctly imports the production model, loads the customers table containing the customer_id key column into a DataFrame, and defines the feature columns needed for the model.\n\n//IMG//\n\n\nWhich code block will output a DataFrame with the schema \"customer_id LONG, predictions DOUBLE\"?","answer_images":[],"answers_community":["B (100%)"],"answer_ET":"B","url":"https://www.examtopics.com/discussions/databricks/view/126298-exam-certified-data-engineer-professional-topic-1-question/","topic":"1"},{"id":"yJ0rC7YlgKCEiTIJLPLH","answer_description":"","exam_id":163,"url":"https://www.examtopics.com/discussions/databricks/view/130120-exam-certified-data-engineer-professional-topic-1-question/","question_id":9,"question_text":"A nightly batch job is configured to ingest all data files from a cloud object storage container where records are stored in a nested directory structure YYYY/MM/DD. The data for each date represents all records that were processed by the source system on that date, noting that some records may be delayed as they await moderator approval. Each entry represents a user review of a product and has the following schema:\n\nuser_id STRING, review_id BIGINT, product_id BIGINT, review_timestamp TIMESTAMP, review_text STRING\n\nThe ingestion job is configured to append all data for the previous date to a target table reviews_raw with an identical schema to the source system. The next step in the pipeline is a batch write to propagate all new records inserted into reviews_raw to a table where data is fully deduplicated, validated, and enriched.\n\nWhich solution minimizes the compute costs to propagate this batch of data?","discussion":[{"upvote_count":"5","poster":"alexvno","comment_id":"1173211","content":"Selected Answer: A\nDeduplication , so insert-only merge","timestamp":"1710400920.0"},{"upvote_count":"5","timestamp":"1704962160.0","comment_id":"1119510","poster":"bacckom","content":"Selected Answer: A\nShould we consider deduplicate? For Time travel, I don't think it can be used to duplicate the target table."},{"poster":"aarora","comment_id":"1346910","timestamp":"1737894600.0","upvote_count":"1","content":"Selected Answer: C\nTo minimize compute costs, the most efficient approach is to leverage Delta Lake’s version history to identify only the new records added since the previous ingestion and process those. Here’s why this solution works best:\n • Delta Lake versioning: Delta Lake tracks changes to the data through its transaction log. By comparing the latest version of the table with the previous version, you can identify only the records that were appended (new data for the previous date).\n • Efficient processing: By working only with the delta (new records), you avoid scanning the entire reviews_raw table, which reduces compute and storage I/O costs.\n • Accurate and optimized: This approach ensures no unnecessary reprocessing of older data while still capturing any delayed records. It works well for use cases involving deduplication and validation."},{"upvote_count":"1","comment_id":"1331328","poster":"Hienlv1","content":"Selected Answer: C\nI think C is the correct answer, use the time travel feature to get the previous version and compare it to the current version to figure out which record needs to be inserted instead of a full scan during read like option A. The goal is to minimize compute costs while propagating only new records inserted into the reviews_raw table to the next table in the pipeline.","timestamp":"1735095960.0"},{"content":"Selected Answer: A\nIn case of D. The 48 hrs point just added to confuse us folks.\nA is enough.","upvote_count":"1","timestamp":"1734154920.0","poster":"Sriramiyer92","comment_id":"1326356"},{"poster":"cales","comment_id":"1296869","content":"Selected Answer: B\n\"The next step in the pipeline is a batch write to propagate all new records inserted into reviews_raw to a table where data is fully deduplicated, validated, and enriched.\" The deduplication will be performed in the following step. Answer B should fit better with cost minimization","comments":[{"timestamp":"1734155040.0","comment_id":"1326357","upvote_count":"2","content":"Keyword - batch\nPropagated = Movement. Does not necessarily mean \"Stream\". Also with Data streaming it would become an expensive affair.","poster":"Sriramiyer92"}],"upvote_count":"1","timestamp":"1728819720.0"},{"upvote_count":"1","comment_id":"1296620","content":"Selected Answer: A\nBatch read load full table, but guarantee no duplication with merge. Trigger Once only load new data, you have to run merge to guarantee no duplication in the whole target file. But B does not indicate that.","poster":"shaojunni","timestamp":"1728763680.0"},{"comment_id":"1288337","timestamp":"1727133060.0","upvote_count":"1","poster":"RyanAck24","content":"Selected Answer: A\nA is Correct"},{"poster":"shaojunni","timestamp":"1726842120.0","comment_id":"1286910","content":"Selected Answer: B\nB is correct, trigger once is the option in structured streaming for batch style job, but much more efficient.","upvote_count":"1"},{"comment_id":"1286908","content":"B is correct, trigger once is the option in structured streaming for batch style job, but much more efficient.","poster":"shaojunni","upvote_count":"1","timestamp":"1726842060.0"},{"timestamp":"1706313720.0","content":"Selected Answer: B\nB is correct","upvote_count":"1","poster":"spaceexplorer","comment_id":"1132960"},{"poster":"ranith","upvote_count":"2","comment_id":"1132594","timestamp":"1706281080.0","content":"B should be correct when looking at cost minimalization, a batch read would scan the whole reviews_raw table, this is unnecessary as historical data is not changed. If a review is delyaed to be approved by the moderator still it is inserted as a new record. Capturing the new data is sufficient."},{"comment_id":"1111563","content":"Selected Answer: B\nB should be correct.\nhttps://www.databricks.com/blog/2017/05/22/running-streaming-jobs-day-10x-cost-savings.html","upvote_count":"4","poster":"divingbell17","comments":[{"upvote_count":"2","content":"It is a batch process.","timestamp":"1704264120.0","poster":"Istiaque","comment_id":"1112562"}],"timestamp":"1704164940.0"}],"unix_timestamp":1704164940,"isMC":true,"answer":"A","answer_images":[],"answer_ET":"A","question_images":[],"topic":"1","choices":{"A":"Perform a batch read on the reviews_raw table and perform an insert-only merge using the natural composite key user_id, review_id, product_id, review_timestamp.","E":"Reprocess all records in reviews_raw and overwrite the next table in the pipeline.","C":"Use Delta Lake version history to get the difference between the latest version of reviews_raw and one version prior, then write these records to the next table.","B":"Configure a Structured Streaming read against the reviews_raw table using the trigger once execution mode to process new records as a batch job.","D":"Filter all records in the reviews_raw table based on the review_timestamp; batch append those records produced in the last 48 hours."},"answers_community":["A (59%)","B (32%)","9%"],"timestamp":"2024-01-02 04:09:00"},{"id":"K2IQOnQrwhpeAaeJ4PC4","answer_ET":"E","answers_community":["E (100%)"],"unix_timestamp":1702973400,"answer":"E","answer_images":[],"isMC":true,"answer_description":"","question_images":[],"discussion":[{"upvote_count":"1","poster":"vctrhugo","comment_id":"1141636","timestamp":"1722896400.0","content":"Selected Answer: E\nOptimized writes improve file size as data is written and benefit subsequent reads on the table.\n\nOptimized writes are most effective for partitioned tables, as they reduce the number of small files written to each partition. Writing fewer large files is more efficient than writing many small files, but you might still see an increase in write latency because data is shuffled before being written.\n\nhttps://learn.microsoft.com/en-us/azure/databricks/delta/tune-file-size#--optimized-writes-for-delta-lake-on-azure-databricks"},{"timestamp":"1720357560.0","poster":"lexaneon","comment_id":"1115941","upvote_count":"3","content":"Selected Answer: E\nhttps://docs.databricks.com/en/delta/tune-file-size.html#optimized-writes"},{"upvote_count":"3","content":"Selected Answer: E\nOptimized writes are most effective for partitioned tables, as they reduce the number of small files written to each partition. Writing fewer large files is more efficient than writing many small files, but you might still see an increase in write latency because data is shuffled before being writte","comment_id":"1100430","timestamp":"1718777400.0","poster":"alexvno"}],"url":"https://www.examtopics.com/discussions/databricks/view/128996-exam-certified-data-engineer-professional-topic-1-question/","exam_id":163,"topic":"1","choices":{"C":"Data is queued in a messaging bus instead of committing data directly to memory; all data is committed from the messaging bus in one batch once the job is complete.","A":"Before a Jobs cluster terminates, OPTIMIZE is executed on all tables modified during the most recent job.","E":"A shuffle occurs prior to writing to try to group similar data together resulting in fewer files instead of each executor writing multiple files based on directory partitions.","B":"An asynchronous job runs after the write completes to detect if files could be further compacted; if yes, an OPTIMIZE job is executed toward a default of 1 GB.","D":"Optimized writes use logical partitions instead of directory partitions; because partition boundaries are only represented in metadata, fewer small files are written."},"question_text":"Which statement describes Delta Lake optimized writes?","timestamp":"2023-12-19 09:10:00","question_id":10}],"exam":{"isBeta":false,"lastUpdated":"12 Apr 2025","name":"Certified Data Engineer Professional","id":163,"isMCOnly":true,"numberOfQuestions":200,"isImplemented":true,"provider":"Databricks"},"currentPage":2},"__N_SSP":true}