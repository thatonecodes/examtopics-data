{"pageProps":{"questions":[{"id":"9QfgldmXkHT1ECMg3vHD","question_id":101,"timestamp":"2024-12-06 09:05:00","topic":"1","answer_description":"","question_images":["https://img.examtopics.com/certified-data-engineer-professional/image85.png"],"choices":{"D":"No; each of the streams needs to have its own checkpoint directory.","B":"Yes; both of the streams can share a single checkpoint directory.","A":"No; Delta Lake manages streaming checkpoints in the transaction log.","C":"No; only one stream can write to a Delta Lake table."},"discussion":[{"upvote_count":"1","poster":"divyapsingh","timestamp":"1733472300.0","content":"Selected Answer: D\nD is the answer. two steams can not have same checkpoint directory.","comment_id":"1322661"}],"answer":"D","answers_community":["D (100%)"],"unix_timestamp":1733472300,"url":"https://www.examtopics.com/discussions/databricks/view/152615-exam-certified-data-engineer-professional-topic-1-question/","exam_id":163,"answer_images":[],"answer_ET":"D","isMC":true,"question_text":"A data architect has designed a system in which two Structured Streaming jobs will concurrently write to a single bronze Delta table. Each job is subscribing to a different topic from an Apache Kafka source, but they will write data with the same schema. To keep the directory structure simple, a data engineer has decided to nest a checkpoint directory to be shared by both streams.\n\nThe proposed directory structure is displayed below:\n\n//IMG//\n\n\nWhich statement describes whether this checkpoint directory structure is valid for the given scenario and why?"},{"id":"jHkr33IW5HmyW3zIKIv4","question_images":[],"choices":{"C":"Webhooks trigger a Databricks job to run anytime new data arrives in a source directory; new data are automatically merged into target tables using rules inferred from the data.","D":"New files are identified by listing the input directory; new files are incrementally and idempotently loaded into the target Delta Lake table.","A":"Cloud vendor-specific queue storage and notification services are configured to track newly arriving files; new files are incrementally and idempotently loaded into the target Delta Lake table.","B":"New files are identified by listing the input directory; the target table is materialized by directly querying all valid files in the source directory."},"answers_community":["D (71%)","14%","14%"],"isMC":true,"answer":"D","unix_timestamp":1733472720,"exam_id":163,"answer_images":[],"answer_description":"","discussion":[{"upvote_count":"1","content":"Selected Answer: D\nD - it is between A & D, rest does not make sense. A describes the file notification mode, which is NOT the default.","comment_id":"1335570","poster":"arekm","timestamp":"1735824420.0"},{"comment_id":"1331042","upvote_count":"2","timestamp":"1735021380.0","poster":"OnlyPraveen","content":"Selected Answer: D\nAlso check answers to duplicate question 108."},{"poster":"carlosmps","content":"Selected Answer: D\n\"Auto Loader uses directory listing mode by default. In directory listing mode, Auto Loader identifies new files by listing the input directory.\"\n\nhttps://learn.microsoft.com/en-us/azure/databricks/ingestion/auto-loader/directory-listing-mode","upvote_count":"2","comment_id":"1324190","timestamp":"1733772360.0"},{"poster":"temple1305","upvote_count":"1","comment_id":"1323117","content":"Selected Answer: B\nCorrect answer is B. However, listing the input directory is the default way of identifying new files for auto loader. Cloud Native Notification services can be used but this is not default setting for auto loader.","timestamp":"1733578620.0"},{"comment_id":"1322662","comments":[{"content":"The question is not \"what is the better approach\", but \"what is the default approach\", hence D.","upvote_count":"1","timestamp":"1735824480.0","poster":"arekm","comment_id":"1335571"}],"timestamp":"1733472720.0","upvote_count":"1","content":"Selected Answer: A\nsolution a is better approach a it is more efficient but both vendor-specific notification service and directory file listing are used for tracking the new files.\ncheck out below link :- https://docs.databricks.com/en/ingestion/cloud-object-storage/auto-loader/file-detection-modes.html","poster":"divyapsingh"}],"url":"https://www.examtopics.com/discussions/databricks/view/152616-exam-certified-data-engineer-professional-topic-1-question/","timestamp":"2024-12-06 09:12:00","question_text":"Which statement describes the default execution mode for Databricks Auto Loader?","topic":"1","answer_ET":"D","question_id":102},{"id":"2pZj0DXg5ATNKEunCKko","url":"https://www.examtopics.com/discussions/databricks/view/117384-exam-certified-data-engineer-professional-topic-1-question-2/","answer_ET":"D","timestamp":"2023-08-05 07:38:00","answer_images":[],"topic":"1","choices":{"B":"Workspace Admin privileges, cluster creation allowed, \"Can Attach To\" privileges on the required cluster","D":"\"Can Restart\" privileges on the required cluster","A":"\"Can Manage\" privileges on the required cluster","E":"Cluster creation allowed, \"Can Restart\" privileges on the required cluster","C":"Cluster creation allowed, \"Can Attach To\" privileges on the required cluster"},"answers_community":["D (79%)","C (18%)","3%"],"exam_id":163,"answer":"D","discussion":[{"content":"Selected Answer: D\nRestart is necessary to start the cluster","comment_id":"1473635","timestamp":"1743780660.0","poster":"codebender","upvote_count":"1"},{"poster":"Ashok_Choudhary_CT","timestamp":"1743495540.0","upvote_count":"1","content":"Selected Answer: A\nThere are three types of permissions:\n1. Can Restart\n2. Can Attach To\n3. Can Manage\n\nCan manage provides both the permissions. Can restart doesn't provide \"Can Attach To\" permission, So the correct answer is \"A\"","comment_id":"1416626"},{"upvote_count":"2","content":"Selected Answer: D\n100% D \nUser has to be able to start the cluster as it shuts down after 30 min of inactivity.\nEverything is explain clearly in the table here - Compute ACLs: https://docs.databricks.com/aws/en/security/auth/access-control/#clusters","poster":"ultimomassimo","timestamp":"1743000120.0","comment_id":"1410431"},{"poster":"Tedet","content":"Selected Answer: C\nTo execute workloads against an already configured cluster in Databricks, users need the following minimum permissions:\n1. Cluster Creation Allowed: Users need the ability to create clusters, which ensures they can start a cluster if it's not already running or if the cluster has been terminated after the inactivity period.\n2. \"Can Attach To\" Privileges on the Required Cluster: This permission allows users to attach their notebooks or jobs to the existing cluster. The \"Can Attach To\" permission is the key to allowing users to interact with the cluster for running jobs or notebooks.","upvote_count":"1","comment_id":"1361474","timestamp":"1740494340.0"},{"content":"Selected Answer: C\nI should choose the C answer because the \"Can Attach To\" permission is the minimal requirement for a user to attach to an interactive cluster and execute workloads.","timestamp":"1739982660.0","upvote_count":"1","comment_id":"1358846","poster":"johnserafim"},{"comment_id":"1353250","timestamp":"1738995120.0","content":"Selected Answer: D\ncan manage all permession \nworkspace admin irrelevant\ncan attach to cannot start the cluster \ncan restart correct can do both \ncluster creation + restart (cluster is alreeady created, dont need to give permission)","upvote_count":"1","poster":"shaswat1404"},{"timestamp":"1738700340.0","upvote_count":"1","poster":"EelkeV","comment_id":"1351568","content":"Selected Answer: C\nYou do not need all the permissions, just the lowest"},{"upvote_count":"4","content":"Selected Answer: D\nhttps://docs.azure.cn/en-us/databricks/security/auth-authz/access-control/cluster-acl#cluster-permissions","poster":"Dhusanth","timestamp":"1737909660.0","comment_id":"1347006"},{"timestamp":"1736939640.0","content":"Selected Answer: D\nif added to a workspace, it has the default settings of the users in that workspace","poster":"nadegetiedjo","upvote_count":"1","comment_id":"1340845"},{"upvote_count":"2","comment_id":"1339649","timestamp":"1736709120.0","poster":"sakis213","content":"Selected Answer: C\nCan Restart only allows restarting the cluster and does not grant permission to attach workloads."},{"comment_id":"1336905","upvote_count":"1","timestamp":"1736121660.0","content":"Selected Answer: C\nOption C makes more sense since the question says also attach. Not just restart.","poster":"yeyi97"},{"timestamp":"1734075720.0","comment_id":"1326051","poster":"rockreid","content":"Selected Answer: C\nTo execute workloads, users need to be able to attach their notebooks or jobs to the cluster. The “Can Attach To” privilege specifically allows users to attach to and use the cluster, which is essential for running their workloads.","upvote_count":"1"},{"poster":"akashdesarda","timestamp":"1727523360.0","content":"Selected Answer: D\nQuestions is users need to start & use so it will be Can restrat . Can attach cannot start compute","comment_id":"1290633","upvote_count":"1"},{"content":"D is the correct answer. Focus on this line \"user would need to start and attach to an already configured cluster.\"","comment_id":"1280981","timestamp":"1725892980.0","upvote_count":"2","poster":"Robbyisok"},{"poster":"md_2000","timestamp":"1720884480.0","upvote_count":"1","comment_id":"1247367","content":"Selected Answer: D\nCOrrect"},{"content":"D is the Correct Answer","timestamp":"1719188880.0","poster":"panya","upvote_count":"1","comment_id":"1236070"},{"poster":"Freyr","timestamp":"1716847740.0","upvote_count":"1","comment_id":"1219859","content":"D is correct. Not A."},{"timestamp":"1716130020.0","upvote_count":"3","comment_id":"1213847","poster":"coercion","content":"Selected Answer: D\n\"Can restart\" permission is only required. \"Can Manage\" permission gives the ability to edit the configurations of the cluster.\nhttps://docs.databricks.com/en/security/auth-authz/access-control/cluster-acl.html"},{"upvote_count":"1","timestamp":"1716128460.0","content":"Answer should be D not A","comment_id":"1213834","poster":"coercion"},{"poster":"naveenballa2","timestamp":"1714351440.0","content":"D is correct","comment_id":"1203749","upvote_count":"1"},{"timestamp":"1713546780.0","upvote_count":"1","poster":"sandeepgoyal1984","comment_id":"1198781","content":"Correct Ans is D"},{"poster":"AziLa","comment_id":"1128054","upvote_count":"2","timestamp":"1705861980.0","content":"Correct Ans is D"},{"poster":"Jay_98_11","timestamp":"1705147680.0","upvote_count":"2","comment_id":"1121586","content":"Selected Answer: D\nD is correct"},{"upvote_count":"4","timestamp":"1703172060.0","poster":"kz_data","content":"Selected Answer: D\nD is the correct answer","comment_id":"1102663"},{"comment_id":"1075920","upvote_count":"2","poster":"aragorn_brego","content":"Selected Answer: D\nhttps://docs.databricks.com/en/security/auth-authz/access-control/cluster-acl.html","timestamp":"1700522040.0"},{"upvote_count":"1","timestamp":"1700416380.0","poster":"Eertyy","content":"D-Can restart is is minimum permission to attach and start the cluster. For more information. Read this page https://docs.databricks.com/en/security/auth-authz/access-control/cluster-acl.html","comment_id":"1074801"},{"comment_id":"1060737","poster":"BIKRAM063","content":"Option D is correct. 'Can Restart' privilege is required","timestamp":"1698947100.0","upvote_count":"1"},{"comment_id":"1052627","timestamp":"1698131820.0","poster":"sagar21692","content":"Option D is the right answer","upvote_count":"1"},{"comment_id":"1040231","poster":"sturcu","content":"Selected Answer: D\nhttps://learn.microsoft.com/en-us/azure/databricks/security/auth-authz/access-control/cluster-acl","timestamp":"1697004240.0","upvote_count":"1"},{"poster":"SRMV","upvote_count":"2","content":"D is the correct answer.\nhttps://docs.databricks.com/en/security/auth-authz/access-control/cluster-acl.html","timestamp":"1696447140.0","comment_id":"1025106"},{"upvote_count":"3","timestamp":"1696210920.0","poster":"Deepaakash","content":"Selected Answer: D\nOption D is the correct answer, refer databricks documentations","comment_id":"1022725"},{"upvote_count":"4","content":"Selected Answer: D\n\"D\" is the correct answer.\n\"A\" would be correct when it comes to editing the cluster itself.","comment_id":"983356","timestamp":"1692259500.0","poster":"Enduresoul"},{"content":"The correct answer is D, it allows a user(s) to start, restart an already existing cluster. The key phrase is \"start and attach to an *already existing cluster\"","poster":"Lungster","timestamp":"1691834400.0","comment_id":"979301","upvote_count":"2"},{"poster":"8605246","timestamp":"1691213880.0","comment_id":"972710","content":"the correct option is D; \"Can Restart\" privileges on the required cluster https://docs.databricks.com/en/security/auth-authz/access-control/cluster-acl.html","upvote_count":"4"}],"answer_description":"","question_images":[],"question_text":"The Databricks workspace administrator has configured interactive clusters for each of the data engineering groups. To control costs, clusters are set to terminate after 30 minutes of inactivity. Each user should be able to execute workloads against their assigned clusters at any time of the day.\nAssuming users have been added to a workspace but not granted any permissions, which of the following describes the minimal permissions a user would need to start and attach to an already configured cluster.","isMC":true,"question_id":103,"unix_timestamp":1691213880},{"id":"F8eT4lrxXItrnsIPGmaM","topic":"1","answer_ET":"E","answer_images":[],"answer_description":"","isMC":true,"question_text":"A data architect has designed a system in which two Structured Streaming jobs will concurrently write to a single bronze Delta table. Each job is subscribing to a different topic from an Apache Kafka source, but they will write data with the same schema. To keep the directory structure simple, a data engineer has decided to nest a checkpoint directory to be shared by both streams.\nThe proposed directory structure is displayed below:\n//IMG//\n\nWhich statement describes whether this checkpoint directory structure is valid for the given scenario and why?","choices":{"E":"No; each of the streams needs to have its own checkpoint directory.","B":"Yes; both of the streams can share a single checkpoint directory.","A":"No; Delta Lake manages streaming checkpoints in the transaction log.","C":"No; only one stream can write to a Delta Lake table.","D":"Yes; Delta Lake supports infinite concurrent writers."},"discussion":[{"comment_id":"1001484","poster":"thxsgod","upvote_count":"11","content":"Selected Answer: E\nCorrect, E.\n\nSource:\nhttps://docs.databricks.com/en/optimizations/isolation-level.html#:~:text=If%20a%20streaming%20query%20using%20the%20same%20checkpoint%20location%20is%20started%20multiple%20times%20concurrently%20and%20tries%20to%20write%20to%20the%20Delta%20table%20at%20the%20same%20time.%20You%20should%20never%20have%20two%20streaming%20queries%20use%20the%20same%20checkpoint%20location%20and%20run%20at%20the%20same%20time.","timestamp":"1694087100.0"},{"timestamp":"1728974400.0","content":"Selected Answer: E\nE is the correct","poster":"benni_ale","comment_id":"1298013","upvote_count":"1"},{"upvote_count":"2","timestamp":"1717211880.0","poster":"imatheushenrique","comment_id":"1222469","content":"E. No; each of the streams needs to have its own checkpoint directory.\nThe checkpoint directory is 1 to 1"},{"timestamp":"1715369940.0","comment_id":"1209513","upvote_count":"2","poster":"svik","content":"Selected Answer: B\nIt is not clear from the question that year_week=2020_01 and year_week=2020_02 are used by stream 1 and stream 2 respectively. If they use the common parent checkpoint directory with individual sub folders for checkpointing, that should work fine. In that case the answer should be B","comments":[{"timestamp":"1718967420.0","comment_id":"1234353","upvote_count":"1","content":"That are table partitions. They are not used to build checkpoint adress. The adress finish at /bronze","poster":"Kill9"}]},{"upvote_count":"1","timestamp":"1705170180.0","content":"Selected Answer: E\ncorrect E","comment_id":"1121948","poster":"Jay_98_11"},{"comment_id":"1118610","upvote_count":"1","content":"Selected Answer: E\nE is correct","timestamp":"1704895380.0","poster":"kz_data"},{"timestamp":"1697010600.0","comment_id":"1040330","content":"E is correct.\nIf user wants 1 checkpoint directory then he needs to unions streams before writing.","upvote_count":"2","poster":"sturcu"},{"poster":"Eertyy","timestamp":"1693393440.0","comment_id":"993993","upvote_count":"3","content":"answer is correct"}],"answers_community":["E (88%)","13%"],"answer":"E","unix_timestamp":1693393440,"exam_id":163,"question_id":104,"url":"https://www.examtopics.com/discussions/databricks/view/119397-exam-certified-data-engineer-professional-topic-1-question/","question_images":["https://img.examtopics.com/certified-data-engineer-professional/image12.png"],"timestamp":"2023-08-30 13:04:00"},{"id":"KlQ0Q7J7ndUFckkmgTp8","unix_timestamp":1736860200,"choices":{"C":"Each write to the orders table will only contain unique records; if existing records with the same key are present in the target table, these records will be overwritten.","A":"Each write to the orders table will only contain unique records, and only those records without duplicates in the target table will be written.","D":"Each write to the orders table will run deduplication over the union of new and existing records, ensuring no duplicate records are present.","B":"Each write to the orders table will only contain unique records, but newly written records may have duplicates already present in the target table."},"answers_community":["B (100%)"],"answer_description":"","url":"https://www.examtopics.com/discussions/databricks/view/154495-exam-certified-data-engineer-professional-topic-1-question/","isMC":true,"timestamp":"2025-01-14 14:10:00","answer_ET":"B","answer":"B","question_images":["https://img.examtopics.com/certified-data-engineer-professional/image86.png"],"question_id":105,"exam_id":163,"answer_images":[],"topic":"1","discussion":[{"upvote_count":"1","poster":"_lene_","timestamp":"1736860200.0","comment_id":"1340359","content":"Selected Answer: B\nNo intra-batch duplicates, can be inter-batch duplicates"}],"question_text":"An upstream source writes Parquet data as hourly batches to directories named with the current date. A nightly batch job runs the following code to ingest all data from the previous day as indicated by the date variable:\n\n//IMG//\n\n\nAssume that the fields customer_id and order_id serve as a composite key to uniquely identify each order.\n\nIf the upstream system is known to occasionally produce duplicate entries for a single order hours apart, which statement is correct?"}],"exam":{"lastUpdated":"12 Apr 2025","provider":"Databricks","isBeta":false,"isMCOnly":true,"isImplemented":true,"name":"Certified Data Engineer Professional","id":163,"numberOfQuestions":200},"currentPage":21},"__N_SSP":true}