{"pageProps":{"questions":[{"id":"2yryeWO1TPq06DOa3DtO","answers_community":["D (68%)","E (24%)","8%"],"answer":"D","question_text":"An external object storage container has been mounted to the location /mnt/finance_eda_bucket.\nThe following logic was executed to create a database for the finance team:\n//IMG//\n\nAfter the database was successfully created and permissions configured, a member of the finance team runs the following code:\n//IMG//\n\nIf all users on the finance team are members of the finance group, which statement describes how the tx_sales table will be created?","timestamp":"2023-08-24 03:57:00","unix_timestamp":1692842220,"answer_images":[],"choices":{"C":"A logical table will persist the physical plan to the Hive Metastore in the Databricks control plane.","A":"A logical table will persist the query plan to the Hive Metastore in the Databricks control plane.","B":"An external table will be created in the storage container mounted to /mnt/finance_eda_bucket.","D":"An managed table will be created in the storage container mounted to /mnt/finance_eda_bucket.","E":"A managed table will be created in the DBFS root storage container."},"answer_description":"","topic":"1","isMC":true,"question_images":["https://img.examtopics.com/certified-data-engineer-professional/image24.png","https://img.examtopics.com/certified-data-engineer-professional/image25.png"],"question_id":141,"answer_ET":"D","url":"https://www.examtopics.com/discussions/databricks/view/118940-exam-certified-data-engineer-professional-topic-1-question/","exam_id":163,"discussion":[{"comments":[{"comment_id":"999305","content":"you are right, it is managed table","poster":"cotardo2077","timestamp":"1693906500.0","upvote_count":"2"},{"poster":"CertPeople","timestamp":"1693998420.0","content":"Nope, you are talking about MANAGED LOCATION (from Unity). In the question states LOCATION (not Unity based), which is not managed\nhttps://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-schema.html","comment_id":"1000533","comments":[{"upvote_count":"9","comment_id":"1000541","content":"Effectively doing a test on one of my clusters the table is MANAGED","poster":"CertPeople","timestamp":"1693999140.0"}],"upvote_count":"2"}],"upvote_count":"11","poster":"tkg13","timestamp":"1692842220.0","comment_id":"988785","content":"Correct Answer D\n\nhttps://docs.databricks.com/en/data-governance/unity-catalog/create-schemas.html#language-SQL"},{"upvote_count":"7","comment_id":"1015370","timestamp":"1695511080.0","content":"Every unmanaged(external) table creation needs to put keyword LOCATION despite if database, that table resides, is put with LOCATION sententece. So B is incorrect. D is correct because the sentence to creates the table is a managed table.\n\nhttps://docs.databricks.com/en/lakehouse/data-objects.html","poster":"MarceloManhaes"},{"comment_id":"1336937","poster":"ASRCA","content":"Selected Answer: B\nThis is because the storage container is mounted to /mnt/finance_eda_bucket, and the code executed by the finance team member would create an external table in that location.","timestamp":"1736131080.0","upvote_count":"2"},{"comment_id":"1326466","upvote_count":"1","content":"Selected Answer: D\nAnswer D. The use of the LOCATION clause with a DBFS path (/mnt/finance_eda_bucket) suggests that Hive Metastore and DBFS location are being used. The answer is correct in the context of Hive Metastore and DBFS location, but if Unity Catalog (UC) is in use, the result would be an external table, not a managed one.","poster":"AlejandroU","timestamp":"1734180840.0"},{"poster":"benni_ale","timestamp":"1729161960.0","upvote_count":"1","comment_id":"1299156","content":"Selected Answer: D\nNo USE DATABASE statement otherwise it would have been external"},{"content":"Selected Answer: D\nD as the word LOCATION is not specified. Although the data will be stored in an external location but the table will still be a managed table.","upvote_count":"2","poster":"coercion","comment_id":"1214016","timestamp":"1716160560.0"},{"upvote_count":"2","poster":"Curious76","timestamp":"1708933860.0","comment_id":"1159465","content":"Selected Answer: E\nE is correct coz this table is managed on top of the external source file. amanaged tables are stored on DBFS."},{"content":"Selected Answer: D\nCorrect answer D.\njust did a test. As with DBR12.2, UC databases are not supported with location on dbfs, but s3/abfss. However, Hive_metastore databases are supported with location on dbfs. Then, a table created in this database IS a managed table, as verified with describe extend command.","comment_id":"1158824","poster":"hal2401me","upvote_count":"5","timestamp":"1708867740.0"},{"poster":"s_villahermosa91","upvote_count":"2","timestamp":"1706355420.0","content":"Selected Answer: E\nCorrect Answer E","comment_id":"1133290"},{"content":"Selected Answer: D\nD is the correct answer, the table created is a managed table and not external, and it will be located under the location defined in the database's creation DDL.","upvote_count":"1","comment_id":"1119874","poster":"kz_data","timestamp":"1704984660.0"},{"timestamp":"1702121760.0","upvote_count":"2","comment_id":"1091773","poster":"azurelearn2020","content":"Selected Answer: D\nIt will be a managed table created under specified database. Location keyword used for database will make sure all the managed tables are stored in database location."},{"upvote_count":"4","timestamp":"1700927940.0","content":"Selected Answer: D\nD is correct. The table will be created as managed, because no LOCATION is specified on table creation. The table will be created in the location specified with database creation","poster":"Enduresoul","comment_id":"1080143"},{"poster":"Dileepvikram","timestamp":"1699485360.0","content":"I think the answer id D","upvote_count":"1","comment_id":"1066023"},{"content":"I followed the steps to create schema and table, the answer is D","upvote_count":"2","poster":"PearApple","comment_id":"1063207","timestamp":"1699212540.0"},{"upvote_count":"1","content":"Correct answer is D. \"data for a managed table resides in the location of the database it is registered to","poster":"jerborder","timestamp":"1698682500.0","comment_id":"1058018"},{"upvote_count":"2","content":"Selected Answer: E\nA managed table will be created on DBFS.","comments":[{"upvote_count":"2","content":"The LOCATION of a database will determine the default location for data of all tables registered to that database.\nfrom the documentation","timestamp":"1713610260.0","poster":"spudteo","comment_id":"1199104"}],"poster":"sturcu","timestamp":"1697452140.0","comment_id":"1044856"}]},{"id":"hRVAEwlGbuxFzN2LyDSz","question_id":142,"question_text":"Although the Databricks Utilities Secrets module provides tools to store sensitive credentials and avoid accidentally displaying them in plain text users should still be careful with which credentials are stored here and which users have access to using these secrets.\nWhich statement describes a limitation of Databricks Secrets?","choices":{"E":"The Databricks REST API can be used to list secrets in plain text if the personal access token has proper credentials.","A":"Because the SHA256 hash is used to obfuscate stored secrets, reversing this hash will display the value in plain text.","D":"Iterating through a stored secret and printing each character will display secret contents in plain text.","C":"Secrets are stored in an administrators-only table within the Hive Metastore; database administrators have permission to query this table by default.","B":"Account administrators can see all secrets in plain text by logging on to the Databricks Accounts console."},"exam_id":163,"isMC":true,"topic":"1","discussion":[{"upvote_count":"1","comment_id":"1364182","content":"Selected Answer: D\nSecret redaction\nStoring credentials as Databricks secrets makes it easy to protect your credentials when you run notebooks and jobs. However, it is easy to accidentally print a secret to standard output buffers or display the value during variable assignment.","timestamp":"1740960960.0","poster":"Tedet"},{"timestamp":"1734251040.0","poster":"AlejandroU","content":"Selected Answer: D\nAnswer D. dbutils.secrets.get(scope=\"myScope\", key=\"myKey\") retrieves the plain text value of a secret, which is then available for use in code.\nLimitation: Once the secret is retrieved, if improperly handled (e.g., logged or iterated), its plain text value can be exposed. Option E: The REST API can list secrets in plain text if proper credentials (e.g., a personal access token) are provided. This is unrelated to dbutils.secrets.get but is a valid limitation of the overall secrets management framework in Databricks. Note that the difference between Option D or E is if it is a limitation related to Databricks Utilities Secret (dbutils.secrets), in this case option D is the correct option.","upvote_count":"1","comment_id":"1326751"},{"timestamp":"1734016080.0","content":"Selected Answer: D\nCannot be option E as it justs lists the Secret value. It does not print the content therein","poster":"Sriramiyer92","upvote_count":"1","comment_id":"1325696"},{"timestamp":"1724241420.0","content":"Selected Answer: D\nvalue = dbutils.secrets.get(scope=\"myScope\", key=\"myKey\")\n\nfor char in value:\n print(char, end=\" \")\n\nOut:\ny o u r _ v a l u e","upvote_count":"4","comment_id":"1270074","poster":"fe3b2fc"},{"content":"Selected Answer: E\nOnly through REST API or CLI you can fetch the secret if you have valid token","upvote_count":"2","timestamp":"1716161700.0","comment_id":"1214025","poster":"coercion"},{"content":"E: https://docs.databricks.com/api/azure/workspace/secrets/listsecrets \nGET /api/2.0/secrets/list won’t list secrets in plain text.\nD: if print it without iterating it in a for loop the output is kind of encrypted where it is showing [REDACTED]. But, if I do it as shown in the screenshot, I'm able to see the value of the secret key.\nhttps://community.databricks.com/t5/data-engineering/how-to-avoid-databricks-secret-scope-from-exposing-the-value-of/td-p/12254\nhttps://docs.databricks.com/en/security/secrets/redaction.html\nSecret redaction for notebook cell output applies only to literals. The secret redaction functionality does not prevent deliberate and arbitrary transformations of a secret literal.","poster":"Er5","comment_id":"1194051","upvote_count":"2","timestamp":"1712879640.0"},{"upvote_count":"2","poster":"Lucario95","timestamp":"1708798740.0","comment_id":"1158087","content":"Selected Answer: E\nBoth D and E seems correct.\nThey are poorly written thought because for D just printing the characters (not separated by spaces, newlines or something) would not work, while E if launched inside databricks workspace would not work neither."},{"content":"D is correct","timestamp":"1707467100.0","upvote_count":"2","poster":"PrashantTiwari","comment_id":"1145328"},{"content":"Selected Answer: D\nD is for sure correct (tried it several times on a Databricks environment).","comment_id":"1143095","poster":"guillesd","timestamp":"1707297600.0","upvote_count":"2","comments":[{"poster":"guillesd","comment_id":"1143098","content":"Regarding E, it can list secrets (with scopes) but I am not sure it can list secret contents.","upvote_count":"1","timestamp":"1707297660.0"}]},{"upvote_count":"3","content":"Selected Answer: D\nD is correct","poster":"DAN_H","comment_id":"1138286","timestamp":"1706861340.0"},{"timestamp":"1706296740.0","content":"Selected Answer: D\nD is correct","comment_id":"1132816","poster":"spaceexplorer","upvote_count":"2"},{"content":"Selected Answer: E\nAt least E is a correct answer.\n\nB: You can't see secrets in Admin console. Only via REST API, CLI etc.\nC: Secrets are. not stored in Hive Metastore.\nD: I am not sure if iterating through secret character by character would work?\nE: This is at least correct. Using this.","comment_id":"1130380","timestamp":"1706084220.0","upvote_count":"1","poster":"Def21"},{"comment_id":"1122534","timestamp":"1705236900.0","content":"B and E both seems to be correct: \n\nhttps://community.databricks.com/t5/data-engineering/how-to-avoid-databricks-secret-scope-from-exposing-the-value-of/td-p/12254/page/2","upvote_count":"1","poster":"ranith"},{"upvote_count":"2","timestamp":"1705173480.0","content":"Selected Answer: D\nFor sure it's D","comment_id":"1121984","poster":"Jay_98_11"},{"poster":"hkay","timestamp":"1703872020.0","content":"Answer is E: \n/api/2.0/secrets/get\n{\n \"key\": \"string\",\n \"value\": \"string\"\n}\nThe REST API can potentially expose secrets in plain text if a user with appropriate permissions (including access to both secrets/list and secrets/get) uses a personal access token.","upvote_count":"3","comment_id":"1108959"},{"comment_id":"1108695","content":"Selected Answer: D\nIterating through the secrets provides a way to see the secret's password.","timestamp":"1703855460.0","poster":"Patito","upvote_count":"2"},{"timestamp":"1700928120.0","content":"D is correct, see https://community.databricks.com/t5/data-engineering/how-to-avoid-databricks-secret-scope-from-exposing-the-value-of/td-p/12254/page/2","poster":"Enduresoul","upvote_count":"1","comment_id":"1080146","comments":[{"content":"you didn't read the entire document, they are also using the get api to print the secret.","comment_id":"1108961","upvote_count":"1","timestamp":"1703872140.0","poster":"hkay"}]},{"comment_id":"1076403","poster":"aragorn_brego","timestamp":"1700580300.0","content":"Selected Answer: E\nWhile Databricks Secrets are designed to secure sensitive information such as passwords and tokens, one limitation is that if a user's personal access token is compromised, and that token has the necessary permissions, the REST API could potentially be used to retrieve secrets. This means that the security of secrets is also dependent on the security of personal access tokens and the permissions assigned to them.","upvote_count":"3"},{"comment_id":"1062530","timestamp":"1699149000.0","content":"E is the correct answer because it describes a limitation of Databricks Secrets. Databricks Secrets is a module that provides tools to store sensitive credentials and avoid accidentally displaying them in plain text. Databricks Secrets allows creating secret scopes, which are collections of secrets that can be accessed by users or groups. Databricks Secrets also allows creating and managing secrets using the Databricks CLI or the Databricks REST API. However, a limitation of Databricks Secrets is that the Databricks REST API can be used to list secrets in plain text if the personal access token has proper credentials. Therefore, users should still be careful with which credentials are stored in Databricks Secrets and which users have access to using these secrets.","poster":"AzureDE2522","upvote_count":"2"},{"content":"Answer is D based on Udemy practice test","timestamp":"1698937560.0","comment_id":"1060637","poster":"Hannah_13","upvote_count":"2"},{"comment_id":"1050505","upvote_count":"2","content":"could be E\nreference: https://docs.databricks.com/api/workspace/secrets","timestamp":"1697974800.0","poster":"Crocjun"},{"upvote_count":"1","comment_id":"1044858","poster":"sturcu","timestamp":"1697452260.0","content":"Selected Answer: B\nB is the correct answer"},{"upvote_count":"1","poster":"TheGhost21","content":"B is the correct answer","comment_id":"1025961","timestamp":"1696536540.0"}],"question_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/122555-exam-certified-data-engineer-professional-topic-1-question/","answers_community":["D (67%)","E (30%)","4%"],"answer_ET":"D","timestamp":"2023-10-05 22:09:00","answer":"D","unix_timestamp":1696536540,"answer_images":[],"answer_description":""},{"id":"RZIZqr70zVe4WkFKJNUy","answer_description":"","choices":{"E":"It is retained for 90 days or until the run-id is re-used through custom run configuration","C":"It is retained for 60 days, during which you can export notebook run results to HTML","D":"It is retained for 60 days, after which logs are archived","B":"It is retained for 30 days, during which time you can deliver job run logs to DBFS or S3","A":"It is retained until you export or delete job run logs"},"unix_timestamp":1691330640,"question_id":143,"isMC":true,"answer":"C","exam_id":163,"answer_images":[],"timestamp":"2023-08-06 16:04:00","topic":"1","question_text":"What statement is true regarding the retention of job run history?","answers_community":["C (94%)","6%"],"url":"https://www.examtopics.com/discussions/databricks/view/117482-exam-certified-data-engineer-professional-topic-1-question/","answer_ET":"C","question_images":[],"discussion":[{"content":"B is wrong, Should be C.","poster":"stuart_gta1","timestamp":"1707419760.0","comment_id":"975873","upvote_count":"9"},{"timestamp":"1719364140.0","comment_id":"1105665","poster":"Yogi05","upvote_count":"7","content":"C is correct answer. https://docs.databricks.com/en/workflows/jobs/monitor-job-runs.html"},{"timestamp":"1740983100.0","poster":"Tedet","comment_id":"1364272","upvote_count":"1","content":"Selected Answer: C\nTo export notebook run results for a job with a single task:\n\nOn the job detail page, click the View Details link for the run in the Run column of the Completed Runs (past 60 days) table.\nClick Export to HTML.\nTo export notebook run results for a job with multiple tasks:\n\nOn the job detail page, click the View Details link for the run in the Run column of the Completed Runs (past 60 days) table.\nClick the notebook task to export.\nClick Export to HTML."},{"timestamp":"1740982980.0","comment_id":"1364271","poster":"Tedet","upvote_count":"1","content":"Selected Answer: C\nDatabricks maintains a history of your job runs for up to 60 days. If you need to preserve job runs, Databricks recommends exporting results before they expire."},{"upvote_count":"1","poster":"janeZ","content":"Selected Answer: C\nhttps://learn.microsoft.com/en-us/azure/databricks/jobs/monitor","comment_id":"1329781","timestamp":"1734747960.0"},{"timestamp":"1724587500.0","upvote_count":"2","poster":"hal2401me","comment_id":"1158847","content":"Selected Answer: C\nhttps://learn.microsoft.com/en-us/azure/databricks/workflows/jobs/monitor-job-runs\nAzure Databricks maintains a history of your job runs for up to 60 days. If you need to preserve job runs, Databricks recommends exporting results before they expire. For more information, see Export job run results."},{"comment_id":"1114980","upvote_count":"3","poster":"ATLTennis","content":"Selected Answer: C\nC is the correct answer","timestamp":"1720234440.0"},{"comment_id":"1108697","timestamp":"1719659580.0","content":"Selected Answer: C\nc is correct","upvote_count":"2","poster":"Patito"},{"comment_id":"1101870","content":"Option C is correct","poster":"SwastikaM","timestamp":"1718903280.0","upvote_count":"2"},{"comment_id":"1101341","poster":"f728f7f","upvote_count":"1","timestamp":"1718861520.0","content":"Selected Answer: D\nA secret CAN be printer character-by-character, so it's not really that secure.","comments":[{"upvote_count":"1","content":"Whoops, answer meant for previous question in the bank. Admin, please delete or move.","comment_id":"1101345","poster":"f728f7f","timestamp":"1718861640.0"}]},{"content":"Selected Answer: C\nC is correct","poster":"rok21","comment_id":"1091950","upvote_count":"3","timestamp":"1717948140.0"},{"content":"Selected Answer: C\nCorrect Answer is C","poster":"azurelearn2020","upvote_count":"1","timestamp":"1717926000.0","comment_id":"1091776"},{"content":"Selected Answer: C\nC is correct: retention is 60 days and export to html","upvote_count":"1","timestamp":"1713263580.0","poster":"sturcu","comment_id":"1044859"},{"upvote_count":"3","timestamp":"1707235440.0","poster":"8605246","comment_id":"973900","content":"this is incorrect databricks maintains a history of job runs for 60 dayshttps://docs.databricks.com/en/workflows/jobs/monitor-job-runs.html#:~:text=Databricks%20maintains%20a%20history%20of,see%20Export%20job%20run%20results."}]},{"id":"E52ePZemIT2GpDhe22GF","answer":"C","unix_timestamp":1702144380,"topic":"1","question_images":[],"timestamp":"2023-12-09 18:53:00","answer_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/128161-exam-certified-data-engineer-professional-topic-1-question/","isMC":true,"exam_id":163,"answer_ET":"C","choices":{"E":"Because User A created the jobs, their identity will be associated with both the job creation events and the job run events.","C":"Because these events are managed separately, User A will have their identity associated with the job creation events and User B will have their identity associated with the job run events.","A":"Because the REST API was used for job creation and triggering runs, a Service Principal will be automatically used to identify these events.","B":"Because User B last configured the jobs, their identity will be associated with both the job creation events and the job run events.","D":"Because the REST API was used for job creation and triggering runs, user identity will not be captured in the audit logs."},"question_text":"A data engineer, User A, has promoted a new pipeline to production by using the REST API to programmatically create several jobs. A DevOps engineer, User B, has configured an external orchestration tool to trigger job runs through the REST API. Both users authorized the REST API calls using their personal access tokens.\nWhich statement describes the contents of the workspace audit logs concerning these events?","discussion":[{"upvote_count":"9","comments":[{"content":"but the documentation says:\nThe REST API operation path, such as /api/2.0/clusters/get, to get information for the specified cluster.\nRemember User B use his token to orchestration.\nThe answer should be C","comment_id":"1323810","poster":"carlosmps","timestamp":"1733704440.0","upvote_count":"1"}],"timestamp":"1708871880.0","content":"Selected Answer: E\nhttps://docs.databricks.com/api/azure/workspace/jobs/create\nAPI/jobs/create:run_as\nobject\nWrite-only setting, available only in Create/Update/Reset and Submit calls. Specifies the user or service principal that the job runs as. If not specified, the job runs as the user who created the job.\nIn the question, it's not stated that user A creates a service principal. So runas can only be himself.","comment_id":"1158866","poster":"hal2401me"},{"timestamp":"1742939820.0","poster":"capt2101akash","comment_id":"1410177","content":"Selected Answer: C\nBoth uses their own credential for specific tasks","upvote_count":"1"},{"comment_id":"1366198","poster":"Nate_","timestamp":"1741340040.0","content":"Selected Answer: C\nUser A created the jobs via the REST API using their personal access token, so the workspace audit logs will record these job creation events with User A’s identity. Conversely, when User B triggers job runs through the REST API (again, using their own personal access token) via an external orchestration tool, those events will be logged with User B’s identity.","upvote_count":"1"},{"poster":"arekm","comment_id":"1335295","content":"Selected Answer: C\nAnswer C - the run_as property is not said to be configured, so the job will run with the permissions of the creator - user A. However, still user B will be the one that triggered the run, which is what the question is about.","upvote_count":"2","timestamp":"1735767300.0"},{"poster":"LuminaBerry","comment_id":"1330908","content":"Selected Answer: C\nC should be the correct answer.\nAlthough User A has it's user associated to the creation, and by default the run as user if omitted on the creation of the job, the question specifies the Audit Logs (Run Event Logs) associated to the run.\nI've tried it this out on a job and for a job which was created and has a run as user different from mine, if I go to the run event logs, there are logs which stated that my user trigged a \"started\" event type","upvote_count":"2","timestamp":"1734978960.0"},{"comment_id":"1329782","content":"Selected Answer: C\nbased on the standard understanding of how personal access tokens typically work, each user's actions should be logged separately with their respective identities. Therefore, \"C\" would be the standard answer unless there is a specific behavior or configuration in Databricks that causes the job run events to be attributed back to User A.","poster":"janeZ","timestamp":"1734748320.0","upvote_count":"1"},{"content":"Selected Answer: C\nAnswer C. The audit logs distinguish between actions like job creation and job execution, so User A and User B will be identified separately for these actions.","timestamp":"1734261120.0","upvote_count":"1","comment_id":"1326800","poster":"AlejandroU"},{"content":"Selected Answer: E\nI tried myself and E seems correct","upvote_count":"1","poster":"benni_ale","timestamp":"1733134440.0","comment_id":"1320887"},{"timestamp":"1732651860.0","poster":"JB90","comment_id":"1318281","content":"Selected Answer: C\nWhen you use the API to commit the jobs the creation is logged using the PAT info, the same happens when you start a run using a different PAT.","upvote_count":"1"},{"content":"Selected Answer: E\nSpecifies the user, service principal or group that the job/pipeline runs as. If not specified, the job/pipeline runs as the user who created the job/pipeline.\nEither user_name or service_principal_name should be specified. If not, an error is thrown.","upvote_count":"1","comment_id":"1316225","timestamp":"1732268940.0","poster":"benni_ale"},{"comment_id":"1309134","upvote_count":"1","timestamp":"1731170040.0","poster":"rsmf","content":"Selected Answer: C\nC is the right answer"},{"content":"Selected Answer: C\nIn Databricks, audit logs capture the identity of the user associated with each distinct event, whether it’s creating or running a job. Since User A used their personal access token to create the jobs and User B used theirs to trigger job runs, the audit logs will reflect User A's identity for job creation events and User B's identity for job run events.","upvote_count":"1","timestamp":"1729895160.0","poster":"Carkeys","comment_id":"1303026"},{"timestamp":"1723473600.0","comment_id":"1264693","poster":"quaternion","upvote_count":"2","content":"Selected Answer: E\nBy default, jobs run as the identity of the job owner. This means that the job assumes the permissions of the job owner. You can change the identity that the job is running as to a service principal. Then, the job assumes the permissions of that service principal instead of the owner.\nhttps://docs.databricks.com/en/jobs/create-run-jobs.html#run-a-job-as-a-service-principal"},{"content":"Selected Answer: E\nWhen you create a job your role is IS OWNER and RUN AS. So when you trigger a job, it will run as the RUN AS entity. And it should be user A if someone dosen't have changed it","upvote_count":"1","comment_id":"1146382","timestamp":"1707576960.0","poster":"spudteo"},{"comment_id":"1131100","timestamp":"1706127660.0","poster":"spaceexplorer","upvote_count":"3","content":"Selected Answer: C\nC is correct"},{"comment_id":"1091952","poster":"rok21","upvote_count":"3","content":"Selected Answer: C\nC is correct","timestamp":"1702144380.0"}],"answer_description":"","answers_community":["C (55%)","E (45%)"],"question_id":144},{"id":"vAWZAo7E2GgI2wVQvHsJ","isMC":true,"answers_community":["B (62%)","D (38%)"],"exam_id":163,"question_images":[],"discussion":[{"comment_id":"1143126","upvote_count":"7","poster":"guillesd","content":"Selected Answer: B\nBoth B and D are correct statements. However, D is not an adjustment (see the question), it is just an afirmation which happens to be correct. B, however, is an adjustment, and it will definitely help with profiling.","timestamp":"1707298560.0"},{"poster":"Tedet","timestamp":"1740984300.0","upvote_count":"1","content":"Selected Answer: D\nExplanation:\nUsing display() in Databricks forces a job to trigger and display the output, which can lead to an inaccurate measure of performance when benchmarking code. This is because display() triggers the job and materializes the result, which does not accurately reflect how the code will perform in production when the job is run without the display output.\n\nAdditionally, repeated execution of the same logic (with caching) may not give you meaningful performance results since the results are cached in memory and not representative of fresh computations, as they would occur in a production environment.\n\nTo get a more accurate measure of execution time, the user should focus on using appropriate job execution techniques, such as running the notebook with \"Run All\" and avoiding reliance on display() calls, which are not representative of how the pipeline would behave in production.","comment_id":"1364290"},{"comment_id":"1335300","content":"Selected Answer: B\nAnswer B, see discussion under benni_ale.","poster":"arekm","comments":[{"comment_id":"1410842","poster":"ultimomassimo","timestamp":"1743071400.0","content":"in any real life commercial project answer B is not feasible, sorry. You always use representative sample, but using same data volumes (especially when they are massive) is impractical and no one would sing off on the cost","upvote_count":"1"}],"upvote_count":"1","timestamp":"1735768020.0"},{"comment_id":"1326813","timestamp":"1734262560.0","upvote_count":"3","content":"Selected Answer: D\nAnswer D. While Option D doesn't directly provide an alternative adjustment, it points out a critical issue in the way interactive notebooks might give misleading results. It would be advisable to avoid using display() as a benchmark for performance in production-like environments.","poster":"AlejandroU"},{"poster":"carlosmps","timestamp":"1733705400.0","content":"Selected Answer: B\nWithout much thought, I would vote for option B, but since it says 'the ONLY,' it makes me hesitate. While option D only points out the issues with the data engineer's executions, it doesn’t really provide the adjustments that need to be made. On the other hand, option B at least gives you a way to simulate production behavior. I’ll vote for B, but as I said, the word 'only' makes me doubt, because it’s not the only way.","comment_id":"1323815","upvote_count":"1"},{"timestamp":"1732269600.0","upvote_count":"1","content":"Selected Answer: D\nAnswer: D.\n\nExplanation:\n\nLazy Evaluation: Spark employs lazy evaluation, meaning transformations are not executed until an action (e.g., display(), count(), collect()) is called. Using display() triggers the execution of the transformations up to that point.\n\nCaching Effects: Repeatedly executing the same cell can lead to caching, where Spark stores intermediate results. This caching can cause subsequent executions to be faster, not reflecting the true performance of the code.\n\nWhy not B: \nProduction-Sized Data and Clusters: While using production-sized data and clusters (as mentioned in option B) can provide insights into performance, it's not the only way to troubleshoot execution times. Proper testing can often be conducted on smaller datasets and clusters, especially during the development phase.","comments":[{"content":"Yep, what if your production size is 10 TB... But you have a 10GB sample. No idea what's actually right for the test, but D is correct.","comment_id":"1323785","upvote_count":"1","poster":"af4a20a","timestamp":"1733696580.0","comments":[{"poster":"arekm","timestamp":"1735767840.0","upvote_count":"1","content":"D is correct. However, it does not show direction on what to do to troubleshoot the problem, which is the first statement in the question.\n\nThe only way to troubleshoot performance problems is to start with the data & processing platform of size that is representative of production. That is why I think B is a better choice.","comment_id":"1335297"}]}],"poster":"benni_ale","comment_id":"1316229"},{"timestamp":"1723864800.0","poster":"practicioner","upvote_count":"2","content":"Selected Answer: B\nB and D are correct. The question says \"which statements\" which suggests us that this is a question with multiple choices","comment_id":"1267418"},{"comments":[{"content":"I would add to this and say that this *could* be a multi-choice question (possibly) as practicioner mentions above. But if it isn't, I would go with D as well.","poster":"RyanAck24","timestamp":"1727651220.0","upvote_count":"1","comment_id":"1291339"}],"timestamp":"1721977560.0","comment_id":"1255528","poster":"HelixAbdu","content":"Both D and B are correct. But in real life some times clients dose not accept to gave you there production data to test easily. Also it says in B it is “the only way” ans this is not true for me\n\nSo i will go with D","upvote_count":"4"},{"content":"Selected Answer: B\nThese people voting D have no reading comprehension.","upvote_count":"4","comment_id":"1172816","timestamp":"1710354660.0","poster":"ffsdfdsfdsfdsfdsf"},{"poster":"alexvno","timestamp":"1710336900.0","content":"Selected Answer: B\nClose env size volumes as possible so results make sense","upvote_count":"2","comment_id":"1172596"},{"comment_id":"1167519","timestamp":"1709761980.0","content":"Selected Answer: D\nD is correct","poster":"halleysg","upvote_count":"3"},{"content":"Selected Answer: D\nI will go with D","upvote_count":"1","poster":"Curious76","comment_id":"1160418","timestamp":"1709029260.0"},{"upvote_count":"4","content":"D is the correct answer\n\nA. Scala is the only language accurately tested using notebooks: Not true. Spark SQL and PySpark can be accurately tested in notebooks, and production performance doesn't solely depend on language choice.\nB. Production-sized data and clusters are necessary: While ideal, it's not always feasible for development. Smaller datasets and clusters can provide indicative insights.\nC. IDE and local Spark/Delta Lake: Local environments won't replicate production's scale and configuration fully.\nE. Jobs UI and Photon: True that Photon benefits scheduled jobs, but Jobs UI can track execution times regardless of Photon usage. However, Jobs UI runs might involve additional overhead compared to notebook cells.\nOption D addresses the specific limitations of using display() for performance measurement","comment_id":"1155755","poster":"agreddy","timestamp":"1708537800.0"},{"content":"Selected Answer: D\nAs B not talking about how to deal with display() function. We know that way to testing performance for the whole notebook need to avoid using display as it is way to test the code and display the data","upvote_count":"3","comments":[{"upvote_count":"1","content":"True, it is not addressing the display() function. However, D does not give any hint on how to go about the problem. On top of that display() function is an action that might help you out in investigating by triggering the actual processing. You still need the data volume that represents the inherent problem - which means that you need the production size of the data, which I think is the first step anyway. Not the last though :)","comment_id":"1335299","poster":"arekm","timestamp":"1735767960.0"}],"poster":"DAN_H","timestamp":"1706923260.0","comment_id":"1138931"},{"content":"B is correct","upvote_count":"1","comment_id":"1136357","timestamp":"1706671140.0","poster":"zzzzx"},{"timestamp":"1706296920.0","comment_id":"1132817","poster":"spaceexplorer","upvote_count":"1","content":"Selected Answer: D\nD is correct"},{"content":"Selected Answer: B\nCalling display() forces a job to trigger - doesnt make sense\ndisplay is used to display a df/table in tabular format, has nothing to do with a job trigger","comment_id":"1110832","timestamp":"1704059520.0","upvote_count":"2","poster":"divingbell17","comments":[{"poster":"guillesd","comment_id":"1143132","upvote_count":"2","content":"Actually they mean a spark job. This is true, whenever you call display, spark needs to execute the transformations up to this point to be able to collect the results.","timestamp":"1707298620.0"}]},{"poster":"ervinshang","timestamp":"1703056500.0","content":"D is correct","comment_id":"1101332","upvote_count":"1"},{"upvote_count":"1","poster":"rok21","comment_id":"1091956","timestamp":"1702144620.0","content":"Selected Answer: B\nB is correct"},{"timestamp":"1697452920.0","comments":[{"timestamp":"1697453100.0","content":"D would be the answer if it was preceded by: We should avoid calling display() too often or clear the cache before running each cell.","comments":[{"content":"but there is no caching mentioned in question","poster":"nedlo","upvote_count":"1","timestamp":"1729891380.0","comment_id":"1303020"}],"upvote_count":"2","poster":"sturcu","comment_id":"1044870"}],"upvote_count":"1","comment_id":"1044866","content":"Selected Answer: B\nYes D is a True statement. But it does not answer the question.\nThe ask is for \"which adjustments will get a more accurate measure of how code is likely to perform in production\". Answer D just describes why the chosen approach is not correct. It does not provide a solution.","poster":"sturcu"},{"comments":[{"timestamp":"1693146960.0","poster":"BrianNguyen95","upvote_count":"3","comment_id":"991507","content":"Option B one of possibility happening. Option D fully meaning"}],"upvote_count":"1","timestamp":"1692842940.0","poster":"tkg13","comment_id":"988786","content":"Is it not B?"}],"question_text":"A user new to Databricks is trying to troubleshoot long execution times for some pipeline logic they are working on. Presently, the user is executing code cell-by-cell, using display() calls to confirm code is producing the logically correct results as new transformations are added to an operation. To get a measure of average time to execute, the user is running each cell multiple times interactively.\nWhich of the following adjustments will get a more accurate measure of how code is likely to perform in production?","unix_timestamp":1692842940,"answer":"B","url":"https://www.examtopics.com/discussions/databricks/view/118941-exam-certified-data-engineer-professional-topic-1-question/","answer_description":"","timestamp":"2023-08-24 04:09:00","answer_ET":"B","answer_images":[],"choices":{"E":"The Jobs UI should be leveraged to occasionally run the notebook as a job and track execution time during incremental code development because Photon can only be enabled on clusters launched for scheduled jobs.","D":"Calling display() forces a job to trigger, while many transformations will only add to the logical query plan; because of caching, repeated execution of the same logic does not provide meaningful results.","C":"Production code development should only be done using an IDE; executing code against a local build of open source Spark and Delta Lake will provide the most accurate benchmarks for how code will perform in production.","A":"Scala is the only language that can be accurately tested using interactive notebooks; because the best performance is achieved by using Scala code compiled to JARs, all PySpark and Spark SQL logic should be refactored.","B":"The only way to meaningfully troubleshoot code execution times in development notebooks Is to use production-sized data and production-sized clusters with Run All execution."},"question_id":145,"topic":"1"}],"exam":{"id":163,"provider":"Databricks","isImplemented":true,"lastUpdated":"12 Apr 2025","numberOfQuestions":200,"isMCOnly":true,"name":"Certified Data Engineer Professional","isBeta":false},"currentPage":29},"__N_SSP":true}