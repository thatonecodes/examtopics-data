{"pageProps":{"questions":[{"id":"HUfjk6n2Z6ujDJ0dU0L8","answer":"D","timestamp":"2023-12-19 01:58:00","answer_ET":"C","answer_description":"","question_images":[],"unix_timestamp":1702947480,"choices":{"A":"A convention that deployment tools can use to wrap preprocessing logic into a Model","D":"A convention that deployment tools can use to understand the model","C":"A convention that MLflow Experiments can use to organize their Runs by project","E":"A convention that MLflow Model Registry can use to organize its Models by project","B":"A convention that MLflow Model Registry can use to version models"},"discussion":[{"upvote_count":"1","poster":"babydozz","content":"Selected Answer: D\nMLflow model flavors refer to the different framework-specific model formats that MLflow supports for logging, saving, and serving models. Each flavor represents a way to package a model along with the required metadata and environment details, allowing it to be loaded and used consistently across different environments.","timestamp":"1742922900.0","comment_id":"1410123"},{"content":"Selected Answer: D\nMLflow Model flavors are a convention that allows deployment tools to understand the structure and requirements of a model, enabling them to deploy the model efficiently across different platforms and environments. Each flavor represents a different serialization format or framework-specific representation of the model, providing flexibility in deployment.","timestamp":"1722097680.0","upvote_count":"1","poster":"hugodscarvalho","comment_id":"1133577"},{"timestamp":"1718751480.0","comment_id":"1100150","upvote_count":"1","poster":"BokNinja","content":"The correct answer is D. A convention that deployment tools can use to understand the model1.\n\nIn the MLflow ecosystem, “flavors” play a pivotal role in model management2. Essentially, a “flavor” is a designated wrapper for specific machine learning libraries2. Flavors streamline the process of saving, loading, and handling machine learning models across different frameworks2. They consider each library’s unique approach to model serialization and deserialization2. MLflow’s flavor design ensures a degree of uniformity2. For every library, its corresponding MLflow flavor defines the behavior of the loaded pyfunc for inference deployment"}],"question_id":31,"isMC":true,"question_text":"Which of the following describes the concept of MLflow Model flavors?","answers_community":["D (100%)"],"exam_id":164,"url":"https://www.examtopics.com/discussions/databricks/view/128940-exam-certified-machine-learning-professional-topic-1/","answer_images":[],"topic":"1"},{"id":"cEOiUmCCypNXEvNRnMfE","timestamp":"2023-12-19 02:00:00","answer":"E","answer_ET":"D","answer_description":"","question_images":[],"choices":{"E":"The arrival of a new model version in the MLflow Model Registry","C":"The arrival of a new feature table in the Feature Store","A":"The launch of a new cost-efficient SQL endpoint","D":"The launch of a new cost-efficient job cluster","B":"CI/CD pipelines are not needed for machine learning pipelines"},"unix_timestamp":1702947600,"discussion":[{"upvote_count":"2","content":"Selected Answer: E\nWhen a new model version is registered in the MLflow Model Registry, it often triggers automated testing to ensure that the new model version performs as expected and meets the defined criteria before deployment.","poster":"hugodscarvalho","timestamp":"1722097620.0","comment_id":"1133575"},{"poster":"BokNinja","timestamp":"1718751600.0","upvote_count":"3","content":"The correct answer is E. The arrival of a new model version in the MLflow Model Registry.\n\nIn a CI/CD process for machine learning pipelines, the arrival of a new model version in the MLflow Model Registry is a common event that triggers the execution of automated testing. This is because each new model version could potentially introduce changes that might affect the performance or functionality of the machine learning pipeline. Automated testing helps ensure that the new model version meets the necessary quality standards before it is deployed.","comment_id":"1100152"}],"question_id":32,"isMC":true,"exam_id":164,"answers_community":["E (100%)"],"question_text":"In a continuous integration, continuous deployment (CI/CD) process for machine learning pipelines, which of the following events commonly triggers the execution of automated testing?","answer_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/128941-exam-certified-machine-learning-professional-topic-1/","topic":"1"},{"id":"JZo8A1YAbpENrpDSCmMB","question_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/128944-exam-certified-machine-learning-professional-topic-1/","isMC":true,"answer_description":"","answer_images":[],"choices":{"D":"Data skipping","C":"Write as a Parquet file","A":"Z-Ordering","E":"Tuning the file size","B":"Bin-packing"},"unix_timestamp":1702947840,"timestamp":"2023-12-19 02:04:00","question_id":33,"answer":"A","discussion":[{"comment_id":"1237418","upvote_count":"1","timestamp":"1719401280.0","content":"Selected Answer: A\n\"The team has already tuned the size of the data files\" - mentioned\nSo E is OUT","poster":"Joy999"},{"comment_id":"1133574","poster":"hugodscarvalho","timestamp":"1706379900.0","content":"Selected Answer: A\nZ-Ordering is a technique used in Delta Lake to colocate similar records together based on the values of multiple columns. This optimization improves query performance by reducing the amount of data that needs to be scanned to satisfy a query, particularly when filtering on multiple columns.","upvote_count":"1"},{"comment_id":"1100159","content":"The correct answer is A. Z-Ordering.\n\nZ-Ordering is a technique used in Delta Lake to optimize the layout of data to improve query performance. It’s a multi-dimensional clustering technique that colocates related information in the same set of files. This colocation can significantly improve the speed of queries and analytics, especially when dealing with high-dimensional data. By using Z-Ordering, the team can ensure that rows meeting the query condition are located close together, thereby speeding up the query.","poster":"BokNinja","upvote_count":"2","timestamp":"1702947840.0"}],"answers_community":["A (100%)"],"answer_ET":"E","topic":"1","exam_id":164,"question_text":"A machine learning engineering team has written predictions computed in a batch job to a Delta table for querying. However, the team has noticed that the querying is running slowly. The team has already tuned the size of the data files. Upon investigating, the team has concluded that the rows meeting the query condition are sparsely located throughout each of the data files.\nBased on the scenario, which of the following optimization techniques could speed up the query by colocating similar records while considering values in multiple columns?"},{"id":"wiRARFufZaVmI7S3MQ9n","url":"https://www.examtopics.com/discussions/databricks/view/128981-exam-certified-machine-learning-professional-topic-1/","question_images":[],"isMC":true,"answer_description":"","answer_images":[],"choices":{"D":"mlflow.log_artifact(importance_path, \"feature-importance.csv\")","C":"mlflow.log_data(importance_path, \"feature-importance.csv\")","A":"","E":"None of these code blocks tan accomplish the task.","B":""},"unix_timestamp":1702961220,"timestamp":"2023-12-19 05:47:00","question_id":34,"answer":"D","discussion":[{"timestamp":"1720177800.0","content":"Selected Answer: D\nBy using the mlflow.log_artifact function, you can log the feature importance CSV file as an artifact within an existing MLflow run. Additionally, you can log the model using the appropriate MLflow flavor and optionally log the feature importance data as metrics for easier access and analysis. This approach ensures that all relevant information is logged and tracked within the same MLflow run.","poster":"64934ca","comment_id":"1242720","upvote_count":"2"},{"timestamp":"1718120280.0","comment_id":"1228509","upvote_count":"2","content":"Answer is D","poster":"sindhu_gowda"},{"comment_id":"1228149","timestamp":"1718057880.0","upvote_count":"1","poster":"c4b65b5","content":"Selected Answer: D\nmlflow does not have log_data method"},{"content":"Selected Answer: D\nAnswer is D","poster":"srikanth923","comment_id":"1204003","upvote_count":"2","timestamp":"1714392660.0"},{"content":"Selected Answer: D\nD","comment_id":"1133519","poster":"hugodscarvalho","upvote_count":"3","timestamp":"1706375700.0"},{"comment_id":"1107094","poster":"mozuca","timestamp":"1703703480.0","content":"Selected Answer: C\nAgree!","upvote_count":"2"},{"comment_id":"1106610","upvote_count":"2","timestamp":"1703659080.0","content":"Selected Answer: D\nD. mlflow.log_artifact(importance_path, \"feature-importance.csv\")","poster":"trendy01"},{"timestamp":"1703559060.0","content":"Selected Answer: D\nD. mlflow.log_artifact(importance_path, \"feature-importance.csv\")","upvote_count":"2","comment_id":"1105654","poster":"dplyr"},{"content":"D. mlflow.log_artifact(importance_path, \"feature-importance.csv\")","upvote_count":"2","comment_id":"1100294","poster":"BokNinja","timestamp":"1702961220.0"}],"answers_community":["D (86%)","14%"],"answer_ET":"D","topic":"1","exam_id":164,"question_text":"A machine learning engineer wants to log feature importance data from a CSV file at path importance_path with an MLflow run for model model.\nWhich of the following code blocks will accomplish this task inside of an existing MLflow run block?"},{"id":"ZebNxN8Uudb2uM1s47U4","exam_id":164,"answer":"E","unix_timestamp":1702947960,"question_text":"A machine learning engineer needs to deliver predictions of a machine learning model in real-time. However, the feature values needed for computing the predictions are available one week before the query time.\nWhich of the following is a benefit of using a batch serving deployment in this scenario rather than a real-time serving deployment where predictions are computed at query time?","answers_community":["E (100%)"],"timestamp":"2023-12-19 02:06:00","discussion":[{"poster":"hugodscarvalho","timestamp":"1722097380.0","upvote_count":"1","content":"Selected Answer: E\nIn a batch serving deployment, predictions are computed offline, typically in bulk, using data that is available before the query time. These predictions are stored and can be quickly queried when needed, providing faster response times compared to computing predictions in real-time, especially when the feature values needed for computing predictions are available well in advance.","comment_id":"1133573"},{"comment_id":"1100160","content":"The correct answer is E. Querying stored predictions can be faster than computing predictions in real-time.\n\nIn this scenario, since the feature values needed for computing the predictions are available one week before the query time, the predictions can be precomputed using a batch serving deployment. When the predictions are needed, they can be quickly retrieved from storage, which can be faster than computing the predictions in real-time. This approach also allows for the efficient use of resources, as the computational work can be done during off-peak times.","timestamp":"1718751960.0","upvote_count":"3","poster":"BokNinja"}],"topic":"1","url":"https://www.examtopics.com/discussions/databricks/view/128945-exam-certified-machine-learning-professional-topic-1/","question_id":35,"answer_description":"","answer_ET":"A","question_images":[],"isMC":true,"answer_images":[],"choices":{"B":"There is no advantage to using batch serving deployments over real-time serving deployments","D":"Testing is not possible in real-time serving deployments","A":"Batch serving has built-in capabilities in Databricks Machine Learning","C":"Computing predictions in real-time provides more up-to-date results","E":"Querying stored predictions can be faster than computing predictions in real-time"}}],"exam":{"isImplemented":true,"isBeta":false,"id":164,"numberOfQuestions":60,"lastUpdated":"12 Apr 2025","isMCOnly":true,"provider":"Databricks","name":"Certified Machine Learning Professional"},"currentPage":7},"__N_SSP":true}