{"pageProps":{"questions":[{"id":"QgLFh66lLFtGaJUTragV","unix_timestamp":1682339340,"discussion":[{"poster":"4be8126","comment_id":"879318","upvote_count":"7","timestamp":"1713961740.0","content":"There are two incorrect answers in the original question.\n\nOption D, \"There is no way to monitor the progress of a job,\" is incorrect. As I mentioned earlier, Spark provides various tools and interfaces for monitoring the progress of a job, including the Spark UI, which provides real-time information about the job's stages, tasks, and resource utilization. Other tools, such as the Spark History Server, can be used to view completed job information.\n\nOption E, \"Jobs are collections of tasks that are divided based on when language variables are defined,\" is also incorrect. The division of tasks in a Spark job is not based on when language variables are defined, but rather based on when actions are called."},{"timestamp":"1718635740.0","comment_id":"926048","upvote_count":"3","poster":"TmData","content":"Selected Answer: D\nThe incorrect statement is:\n\nD. There is no way to monitor the progress of a job.\n\nExplanation: Spark provides several ways to monitor the progress of a job. The Spark UI (Web UI) provides a graphical interface to monitor the progress of Spark jobs, stages, tasks, and other relevant metrics. It displays information such as job status, task completion, execution time, and resource usage. Additionally, Spark provides programmatic APIs, such as the JobProgressListener interface, which allows developers to implement custom job progress monitoring logic within their Spark applications."}],"answer_images":[],"isMC":true,"answer_description":"","topic":"1","question_text":"Which of the following statements about Spark jobs is incorrect?","question_images":[],"answers_community":["D (100%)"],"choices":{"A":"Jobs are broken down into stages.","B":"There are multiple tasks within a single job when a DataFrame has more than one partition.","D":"There is no way to monitor the progress of a job.","C":"Jobs are collections of tasks that are divided up based on when an action is called.","E":"Jobs are collections of tasks that are divided based on when language variables are defined."},"answer_ET":"D","exam_id":161,"timestamp":"2023-04-24 14:29:00","question_id":131,"answer":"D","url":"https://www.examtopics.com/discussions/databricks/view/107312-exam-certified-associate-developer-for-apache-spark-topic-1/"},{"id":"3sVwouUUFW5QTeu9gb5T","choices":{"B":"storesDF.withColumn(\"dayOfYear\", get dayofyear(col(\"openDate\")))","C":"storesDF.withColumn(\"dayOfYear\", dayofyear(col(\"openDate\")))","A":"(storesDF.withColumn(\"openTimestamp\", col(\"openDate\").cast(\"Timestamp\"))\n. withColumn(\"dayOfYear\", dayofyear(col(\"openTimestamp\"))))","D":"(storesDF.withColumn(\"openDateFormat\", col(\"openDate\").cast(\"Date\"))\n. withColumn(\"dayOfYear\", dayofyear(col(\"openDateFormat\"))))","E":"storesDF.withColumn(\"dayOfYear\", substr(col(\"openDate\"), 4, 6))"},"answer":"A","topic":"1","answer_ET":"A","timestamp":"2023-04-13 21:30:00","answers_community":["A (100%)"],"isMC":true,"question_id":132,"question_images":["https://img.examtopics.com/certified-associate-developer-for-apache-spark/image7.png"],"question_text":"Which of the following code blocks returns a DataFrame containing a column dayOfYear, an integer representation of the day of the year from column openDate from DataFrame storesDF?\nNote that column openDate is of type integer and represents a date in the UNIX epoch format – the number of seconds since midnight on January 1st, 1970.\nA sample of storesDF is displayed below:\n//IMG//","discussion":[{"comment_id":"1070553","upvote_count":"1","timestamp":"1731597780.0","poster":"juliom6","content":"Selected Answer: A\nA is correct:\n\nfrom pyspark.sql.functions import col, dayofyear\n\nstoresDF = spark.createDataFrame([(0, 1100746394), (1, 1474410343)], ['storeId', 'openDate'])\nstoresDF = (storesDF.withColumn(\"openTimestamp\", col(\"openDate\").cast(\"Timestamp\")).withColumn(\"dayOfYear\", dayofyear(col(\"openTimestamp\"))))\ndisplay(storesDF)"},{"content":"Selected Answer: A\nA is correct","timestamp":"1730970540.0","poster":"newusername","comment_id":"1064657","upvote_count":"1"},{"content":"Selected Answer: A\nstoresDF.withColumn(\"openTimestamp\", col(\"openDate\").cast(\"Timestamp\")).withColumn(\"dayOfYear\", dayofyear(col(\"openTimestamp\")))","timestamp":"1726464420.0","poster":"thanab","comment_id":"1008912","upvote_count":"2"},{"timestamp":"1722506580.0","content":"A. dayofyear function in PySpark's functions module expects the column openDate to be of type timestamp rather than long.","poster":"singh100","comment_id":"968935","upvote_count":"1"},{"upvote_count":"1","timestamp":"1714727640.0","content":"Selected Answer: A\nThe correct answer is C.\n\nOption A is correct because it casts the openDate column to a timestamp using cast(\"Timestamp\") and then uses the dayofyear function to extract the day of the year from the timestamp.\n\nOption B is incorrect because it contains syntax errors, including the \"get\" keyword, which is not necessary or valid in this context.\n\nOption C is close, but it does not cast the openDate column to a timestamp, which is necessary to use the dayofyear function.\n\nOption D is incorrect because it converts column \"openDate\" to a date format, which is unnecessary for extracting the day of the year. Additionally, the dayofyear() function can be applied directly to the \"openDate\" column.\n\nOption E is incorrect because it uses the substr() function to extract a substring from the \"openDate\" column, which does not correspond to the day of the year.","poster":"4be8126","comment_id":"888329"},{"content":"The answer should be A. Unixtime should be cast to timestamp first","comment_id":"869705","upvote_count":"2","poster":"peekaboo15","timestamp":"1713036600.0"}],"answer_images":[],"unix_timestamp":1681414200,"url":"https://www.examtopics.com/discussions/databricks/view/106140-exam-certified-associate-developer-for-apache-spark-topic-1/","exam_id":161,"answer_description":""},{"id":"cnnEkUdzTAvLb4mvf213","unix_timestamp":1685325360,"answer":"E","question_text":"The code block shown below contains an error. The code block intended to return a new DataFrame that is the result of an inner join between DataFrame storesDF and DataFrame employeesDF on column storeId. Identify the error.\nCode block:\nStoresDF.join(employeesDF, \"inner\", \"storeID\")","choices":{"B":"The key column storeID needs to be in a list like [\"storeID\"].","C":"The key column storeID needs to be specified in an expression of both DataFrame columns like storesDF.storeId == employeesDF.storeId.","A":"The key column storeID needs to be wrapped in the col() operation.","E":"The column key is the second parameter to join() and the type of join in the third parameter to join() – the second and third arguments should be switched.","D":"There is no DataFrame.join() operation – DataFrame.merge() should be used instead."},"discussion":[{"poster":"newusername","timestamp":"1730973480.0","content":"Selected Answer: E\nthere are diff methods to join dataframes, one of the: \njoinedDF = StoresDF.join(employeesDF, \"storeId\", \"inner\")","upvote_count":"1","comment_id":"1064695"},{"comment_id":"908968","content":"Selected Answer: E\nE\n\nstoresDF.join(employeesDF, \"storeID\", \"inner\")\n\nThe column key is the second parameter to join() and the type of join is in the third parameter to join() – the second and third arguments should be switched","timestamp":"1716947760.0","poster":"Jtic","upvote_count":"3"}],"question_images":[],"answer_description":"","question_id":133,"url":"https://www.examtopics.com/discussions/databricks/view/110462-exam-certified-associate-developer-for-apache-spark-topic-1/","isMC":true,"answer_ET":"E","answers_community":["E (100%)"],"exam_id":161,"timestamp":"2023-05-29 03:56:00","topic":"1","answer_images":[]},{"id":"HptuCxtQCnCEnKihDGNh","answer_images":[],"question_text":"Which of the following operations can perform an outer join on two DataFrames?","discussion":[{"upvote_count":"6","content":"Selected Answer: D\nD. result_df = df1.join(df2, on=\"id\", how=\"outer\")","poster":"cookiemonster42","timestamp":"1706217840.0","comment_id":"963061"},{"poster":"juliom6","timestamp":"1715693400.0","comment_id":"1070562","upvote_count":"2","content":"Selected Answer: D\nD is correct.\n\nThere is no exists outerJoin() operation in pyspark."},{"content":"Selected Answer: C\nThe correct answer is C - DataFrame.outerJoin(). The outer join operation can be performed by specifying the join type as \"outer\" when calling the outerJoin() function on a DataFrame. The join() function in Spark only performs an inner join, while the merge() function is not a valid function in Spark SQL. The crossJoin() function performs a Cartesian product between two DataFrames, which is not an outer join.","comment_id":"888338","poster":"4be8126","comments":[{"poster":"ZSun","timestamp":"1701895020.0","comments":[{"timestamp":"1716141840.0","upvote_count":"1","poster":"Seeker_thunder","comment_id":"1074904","content":"this guy always post wrong answers, sometime gpts as well. ignore his commnmets"}],"upvote_count":"9","comment_id":"916552","content":"There is no outerjoin, bro!\nonly dataframe.join(how='outer')"},{"poster":"65bd33e","content":"Wrong answer, check documentation","upvote_count":"1","comment_id":"1201022","timestamp":"1729723020.0"}],"timestamp":"1699011600.0","upvote_count":"1"}],"url":"https://www.examtopics.com/discussions/databricks/view/108377-exam-certified-associate-developer-for-apache-spark-topic-1/","isMC":true,"answer_ET":"D","answer_description":"","topic":"1","answer":"D","timestamp":"2023-05-03 11:40:00","exam_id":161,"question_id":134,"unix_timestamp":1683106800,"answers_community":["D (89%)","11%"],"choices":{"B":"Standalone join() function","A":"DataFrame.crossJoin()","E":"DataFrame.merge()","D":"DataFrame.join()","C":"DataFrame.outerJoin()"},"question_images":[]},{"id":"pLd6mlPIbvhzZSPxtGnY","timestamp":"2023-05-29 03:58:00","answer_ET":"B","answer_images":[],"isMC":true,"question_images":[],"discussion":[{"comment_id":"1144902","content":"Selected Answer: B\nB cannot be used as this seems ambiguous","upvote_count":"1","poster":"azure_bimonster","timestamp":"1723142880.0"},{"comment_id":"1104057","content":"Selected Answer: B\nB throws AnalysisException: [AMBIGUOUS_REFERENCE] Reference `column1` is ambiguous, could be: [`a`.`column1`, `b`.`column1`]","timestamp":"1719144360.0","poster":"Gurdel","upvote_count":"1"},{"timestamp":"1715695980.0","content":"Selected Answer: B\nAccording to the following code, only response B returns an error. The key concept here is that dataframes must be \"named\" AND \"aliased\".\n\nfrom pyspark.sql.functions import col\n\na = spark.createDataFrame([(1, 2), (3, 4)], ['column1', 'column2'])\nb = spark.createDataFrame([(1, 2), (5, 6)], ['column1', 'column2'])\n\na = a.alias('a')\nb = b.alias('b')\n\ndf = a.join(b, on = [a.column1 == b.column1, a.column2 == b.column2])\ndisplay(df)\n# df = a.join(b, on = [col(\"column1\"), col(\"column2\")])\ndf = a.join(b, on = [col(\"a.column1\") == col(\"b.column1\"), col(\"a.column2\") == col(\"b.column2\")])\ndisplay(df)\ndf = a.join(b, on = [\"column1\", \"column2\"])\ndisplay(df)","comment_id":"1070616","upvote_count":"3","poster":"juliom6"},{"comments":[{"upvote_count":"3","content":"# Sample data for DataFrame 'b'\ndataB = [Row(column1=1, column2=2), Row(column1=2, column2=5), Row(column1=3, column2=4)]\ndfB = spark.createDataFrame(dataB)\n\n# Alias DataFrames as 'a' and 'b'\na = dfA.alias(\"a\")\nb = dfB.alias(\"b\")\n\na.show()\nb.show()\n\n\n#Option A \njoinedDF_A = a.join(b, [a.column1 == b.column1, a.column2 == b.column2])\njoinedDF_A.show()\n\n\n#Option B \n#joinedDF_B = a.join(b, [col(\"column1\"), col(\"column2\")])\n#joinedDF_B.show()\n\n\n#Option C\njoinedDF_C = a.join(b, [col(\"a.column1\") == col(\"b.column1\"), col(\"a.column2\") == col(\"b.column2\")])\njoinedDF_C.show()\n\n\n#Option E\njoinedDF_E = a.join(b, [\"column1\", \"column2\"])\njoinedDF_E.show()","comment_id":"1064752","poster":"newusername","timestamp":"1715075340.0"}],"content":"Selected Answer: B\n100% B \nBelow code to test:\n\ndataA = [Row(column1=1, column2=2), Row(column1=2, column2=4), Row(column1=3, column2=6)]\ndfA = spark.createDataFrame(dataA)","upvote_count":"3","comment_id":"1064751","timestamp":"1715075340.0","poster":"newusername"},{"content":"I tried all of the options and I got 2 errors from:\n\nB\nAMBIGUOUS_REFERENCE] Reference `Category` is ambiguous, could be: [`Category`, `Category`]\n\nC:\n[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `df_1`.`Category` cannot be resolved. \n\nDid you mean one of the following? [`Category`, `Category`, `Truth`, `Truth`, `Value`].;","comments":[{"timestamp":"1714309680.0","poster":"Ahmadkt","comment_id":"1056218","upvote_count":"1","content":"it's B, it seems you didn't do the alias \na = df1.alias(\"a\")\nb = df2.alias(\"b\")"}],"timestamp":"1713467460.0","poster":"juadaves","comment_id":"1047164","upvote_count":"2"},{"comment_id":"1021264","upvote_count":"1","content":"from pyspark.sql.functions import col\ndf2.alias('a').join(df3.alias('b'),\n [col(\"a.name\") == col(\"b.name\"), col(\"a.name\") == col(\"b.name\")],\n 'full_outer').select(df2['name'],'height','age').show()\n It worked. so every answer is correct.","timestamp":"1711782660.0","poster":"Singh_Sumit"},{"timestamp":"1706986500.0","content":"Selected Answer: C\nshould be C as in col() we specify only a column name as a string, not a dataframe","comment_id":"971325","upvote_count":"3","poster":"cookiemonster42"},{"comment_id":"908969","upvote_count":"2","poster":"Jtic","timestamp":"1701230280.0","content":"Selected Answer: A\nA. on = [a.column1 == b.column1, a.column2 == b.column2]\nThis option is valid and can be used to perform an inner join on two key columns. It specifies the key columns using the syntax a.column1 == b.column1 and a.column2 == b.column2.","comments":[{"poster":"ZSun","timestamp":"1701882300.0","content":"I think the question \"which one cannot be used to perform inner join\", is confusing,\nBecause only A works, the rest of answer is incorrect.\nThe question should be \"which one can be used\"","upvote_count":"2","comment_id":"916390"}]}],"question_text":"Which of the following pairs of arguments cannot be used in DataFrame.join() to perform an inner join on two DataFrames, named and aliased with \"a\" and \"b\" respectively, to specify two key columns?","exam_id":161,"answer_description":"","choices":{"C":"on = [col(\"a.column1\") == col(\"b.column1\"), col(\"a.column2\") == col(\"b.column2\")]","B":"on = [col(\"column1\"), col(\"column2\")]","A":"on = [a.column1 == b.column1, a.column2 == b.column2]","D":"All of these options can be used to perform an inner join with two key columns.","E":"on = [\"column1\", \"column2\"]"},"unix_timestamp":1685325480,"url":"https://www.examtopics.com/discussions/databricks/view/110463-exam-certified-associate-developer-for-apache-spark-topic-1/","topic":"1","answer":"B","answers_community":["B (62%)","C (23%)","A (15%)"],"question_id":135}],"exam":{"isBeta":false,"provider":"Databricks","name":"Certified Associate Developer for Apache Spark","id":161,"isMCOnly":true,"lastUpdated":"12 Apr 2025","isImplemented":true,"numberOfQuestions":185},"currentPage":27},"__N_SSP":true}