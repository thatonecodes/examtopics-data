{"pageProps":{"questions":[{"id":"8ly8s8V6CLa7IbV5CAYi","topic":"1","answer_description":"","isMC":true,"exam_id":162,"question_text":"A data engineer needs to create a table in Databricks using data from a CSV file at location /path/to/csv.\n\nThey run the following command:\n\n//IMG//\n\n\nWhich of the following lines of code fills in the above blank to successfully complete the task?","discussion":[{"content":"Selected Answer: B\ncorrect answer: B\nexplanation: To create a table in Databricks using data from a CSV file, the correct syntax after specifying the table name and schema (if applicable) would be to use the USING CSV clause to define the format of the source data. This clause tells Databricks that the data source format is CSV. The command would typically look","comment_id":"1170507","upvote_count":"2","poster":"fifirifi","timestamp":"1725983640.0"},{"timestamp":"1724287080.0","content":"I have a question \n\nWhy can option using delta","comment_id":"1156035","poster":"Bob123456","upvote_count":"1"},{"poster":"kz_data","comment_id":"1089742","content":"Selected Answer: B\nB is correct","upvote_count":"1","timestamp":"1717697940.0"},{"content":"Selected Answer: B\nhttps://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table-using.html#parameters","upvote_count":"3","poster":"55f31c8","timestamp":"1716981780.0","comment_id":"1083495"},{"content":"Ans B : Using csv is correct. that is the correct syntax","comment_id":"1050168","poster":"meow_akk","upvote_count":"3","timestamp":"1713755640.0"},{"poster":"kishanu","comment_id":"1048892","content":"Selected Answer: B\nUSING CSV","upvote_count":"2","timestamp":"1713624300.0"}],"timestamp":"2023-10-20 16:45:00","question_images":["https://img.examtopics.com/certified-data-engineer-associate/image38.png"],"question_id":136,"answer_ET":"B","answer":"B","choices":{"C":"FROM CSV","D":"USING DELTA","E":"FROM \"path/to/csv\"","B":"USING CSV","A":"None of these lines of code are needed to successfully complete the task"},"url":"https://www.examtopics.com/discussions/databricks/view/124133-exam-certified-data-engineer-associate-topic-1-question-69/","unix_timestamp":1697813100,"answers_community":["B (100%)"],"answer_images":[]},{"id":"S9PIxGSsSLWrOOBQqUfT","topic":"1","answer_description":"","isMC":true,"exam_id":162,"question_text":"A data engineer has realized that they made a mistake when making a daily update to a table. They need to use Delta time travel to restore the table to a version that is 3 days old. However, when the data engineer attempts to time travel to the older version, they are unable to restore the data because the data files have been deleted.\nWhich of the following explains why the data files are no longer present?","discussion":[{"poster":"Feroz_Raza","content":"Selected Answer: A\nThere is no DELETE HISTORY command in Databricks \nVACCUM command can remove history and we can also specify the retention period with VACCUM Command. Default Retention period is 7 days.\nTo allow changing the default retention period you can rum the following command \n\nALTER TABLE your_table SET TBLPROPERTIES ('delta.retentionDurationCheck.enabled' = 'true');","upvote_count":"12","timestamp":"1700129220.0","comment_id":"1072323"},{"poster":"devbila","timestamp":"1743151680.0","comment_id":"1411230","upvote_count":"1","content":"Selected Answer: A\nThe response is A"},{"timestamp":"1727257320.0","comment_id":"1288950","poster":"mateo_vansweevelt","upvote_count":"1","content":"Selected Answer: A\nIts vacuum"},{"content":"Selected Answer: A\nA. The VACUUM command was run on the table\n\nThe VACUUM command in Delta Lake is used to clean up and remove unnecessary data files that are no longer needed for time travel or query purposes. When you run VACUUM with certain retention settings, it can delete older data files, which might include versions of data that are older than the specified retention period. If the data engineer is unable to restore the table to a version that is 3 days old because the data files have been deleted, it's likely because the VACUUM command was run on the table, removing the older data files as part of data cleanup.","timestamp":"1727170200.0","poster":"vctrhugo","comment_id":"997867","upvote_count":"3"},{"poster":"9d4d68a","comment_id":"1272797","upvote_count":"1","timestamp":"1724683500.0","content":"Selected Answer: A\nA. The VACUUM command was run on the table\n\nThe VACUUM command in Delta Lake is used to clean up old data files that are no longer needed, which could include files that are older than a certain retention period. If the data engineer is unable to restore data to a version that is 3 days old, it is likely because the VACUUM command has deleted the old data files beyond the retention period."},{"poster":"80370eb","upvote_count":"1","timestamp":"1723102920.0","content":"Selected Answer: A\nvacuum command is used to remove the history of the table.","comment_id":"1262388"},{"comment_id":"1249372","poster":"ranjan24","upvote_count":"1","timestamp":"1721190300.0","content":"A is the correct answer"},{"poster":"3fbc31b","timestamp":"1720467480.0","content":"Selected Answer: A\nThere is no DELETE HISTORY command anywhere in Databricks. The VACUUM command removes files older than the value that is set. The default value is 7 days.","comment_id":"1244536","upvote_count":"1"},{"timestamp":"1718525700.0","poster":"potaryxkug","upvote_count":"1","content":"A is the good answer","comment_id":"1231261"},{"timestamp":"1717966140.0","content":"The answer is A!","poster":"mascarenhaslucas","comment_id":"1227527","upvote_count":"1"},{"comment_id":"1182680","poster":"bettermakeme","upvote_count":"1","timestamp":"1711387560.0","content":"Answer is A. Just finished exam-got 100% [Databricks Associate Exam Practice Exams] All questions came from \nDatabricks Certified Data Engineer Associate\nhttps://www.udemy.com/share/10aEFa3@9M_uT6vrKbnl68tOK96kfy-YWitjwzLTlVCrzPs-0hGUu8fyX8V4Tn_x_y65bwLm/"},{"poster":"Itmma","content":"Selected Answer: A\nA is correct","timestamp":"1710840900.0","upvote_count":"1","comment_id":"1177170"},{"upvote_count":"1","content":"Selected Answer: A\nA i correct","timestamp":"1703432340.0","comment_id":"1104703","poster":"SerGrey"},{"timestamp":"1700230800.0","content":"I agree with the first post. A is the correct answer. There is no such thing as a Delete History Command","poster":"Huroye","comment_id":"1073382","upvote_count":"2"},{"timestamp":"1699359000.0","content":"Selected Answer: A\nright answer is A","comment_id":"1064769","poster":"awofalus","upvote_count":"1"},{"comment_id":"1057375","poster":"vivekrrr","timestamp":"1698645360.0","content":"i think B is the answer, plz let me know if not correct","upvote_count":"1"},{"timestamp":"1698645300.0","poster":"vivekrrr","upvote_count":"1","comment_id":"1057373","content":"but vaccum allows to vaccum anything that's older than 7 days right"},{"upvote_count":"1","poster":"VijayKula","comment_id":"1028763","content":"Selected Answer: A\nAnswer is A Vaccum","timestamp":"1696848600.0"},{"comment_id":"1022510","timestamp":"1696179120.0","content":"Reading Material: https://learn.microsoft.com/en-us/azure/databricks/delta/vacuum#example-syntax-for-vacuum","upvote_count":"1","poster":"Sriramiyer92"},{"poster":"KalavathiP","comment_id":"1017342","content":"Selected Answer: A\nA is correct","timestamp":"1695697860.0","upvote_count":"2"},{"timestamp":"1692814320.0","content":"A is Correct!\nDoes DELETE HISTORY command exist?","upvote_count":"2","poster":"cpalmier","comment_id":"988569"},{"poster":"Atnafu","content":"A\nWhen the data engineer attempted to time travel to an older version of the table, the data files were no longer present because the VACUUM command was run on the table. The VACUUM command in Delta Lake is used to clean up files that are no longer necessary for the current version of the table. It permanently removes older versions of data files and transaction log files that are no longer needed for queries or time travel.\nBy running the VACUUM command, the data engineer inadvertently deleted the data files of the version they were trying to restore, making it impossible to access that specific version of the table through Delta time travel.\nVACUUM [db_name.]table_name [RETAIN num_hrs] [DRY RUN]","comments":[{"content":"VACUUM my_database.my_table RETAIN 30","timestamp":"1688853420.0","upvote_count":"1","comment_id":"946761","comments":[{"content":"If not specified, the default retention period of 7 days is used.","timestamp":"1688853480.0","upvote_count":"2","comment_id":"946762","poster":"Atnafu"}],"poster":"Atnafu"}],"timestamp":"1688853420.0","upvote_count":"1","comment_id":"946760"},{"upvote_count":"4","poster":"Majjjj","content":"Selected Answer: A\nThe most likely reason why the data files are no longer present when the data engineer attempts to time travel to an older version of a Delta table is that the VACUUM command was run on the table. The VACUUM command removes files that are no longer in use by the Delta table, including files that are required for time travel. Therefore, if the VACUUM command is run on a Delta table, it can make it impossible to use time travel to recover older versions of the table.","timestamp":"1683157080.0","comment_id":"889038"},{"timestamp":"1682052300.0","content":"Agreed A is answer","upvote_count":"1","poster":"Varma_Saraswathula","comment_id":"876194"},{"content":"Vaccum","timestamp":"1682013420.0","poster":"naxacod574","comment_id":"875845","upvote_count":"1"},{"content":"Selected Answer: A\nVACUUM command is the only way to explicitly remove the history information from a delta table. There is no command like DELETE HISTORY exists in Databricks","poster":"Data_4ever","comment_id":"863851","timestamp":"1680870180.0","upvote_count":"3"},{"upvote_count":"1","content":"Selected Answer: A\nVacuum not Delete","comment_id":"861138","timestamp":"1680617220.0","poster":"upliftinghut"},{"poster":"sdas1","content":"option A","timestamp":"1680582720.0","upvote_count":"1","comment_id":"860617"},{"comment_id":"860261","content":"Selected Answer: A\nVACUUM elimina los archivos pasados, dejando solo los ultimos, disponibles","timestamp":"1680548760.0","poster":"knivesz","upvote_count":"1"},{"timestamp":"1680505860.0","comment_id":"859620","content":"Selected Answer: A\noption A","poster":"surrabhi_4","upvote_count":"1"},{"upvote_count":"3","comment_id":"857962","timestamp":"1680356460.0","content":"C is the wrong answer. Correct answer should be A. Using VACUUM command will not make it possible to TIME TRAVEL (depends on the conditions on the VACUUM command). \nhttps://docs.databricks.com/sql/language-manual/delta-vacuum.html","poster":"XiltroX"},{"comment_id":"854102","content":"wrong answer. there is no delete history command\nhttps://docs.databricks.com/delta/history.html","timestamp":"1680073380.0","poster":"h79","upvote_count":"2"},{"content":"wrong answer, vaccum command removes the history","poster":"azurearch","timestamp":"1679904540.0","upvote_count":"4","comment_id":"851909"}],"question_images":[],"timestamp":"2023-03-27 10:09:00","question_id":137,"answer":"A","answer_ET":"A","choices":{"E":"The HISTORY command was run on the table","A":"The VACUUM command was run on the table","B":"The TIME TRAVEL command was run on the table","D":"The OPTIMIZE command was nun on the table","C":"The DELETE HISTORY command was run on the table"},"url":"https://www.examtopics.com/discussions/databricks/view/104051-exam-certified-data-engineer-associate-topic-1-question-7/","unix_timestamp":1679904540,"answers_community":["A (100%)"],"answer_images":[]},{"id":"CesFh273ckQ2F8rdN7u6","answers_community":["B (100%)"],"topic":"1","answer_images":[],"answer_description":"","question_images":["https://img.examtopics.com/certified-data-engineer-associate/image39.png"],"choices":{"B":"trigger(availableNow=True)","E":"trigger(continuous=\"once\")","C":"trigger(parallelBatch=True)","A":"processingTime(1)","D":"trigger(processingTime=\"once\")"},"exam_id":162,"timestamp":"2023-10-22 05:16:00","answer":"B","question_id":138,"isMC":true,"answer_ET":"B","url":"https://www.examtopics.com/discussions/databricks/view/124299-exam-certified-data-engineer-associate-topic-1-question-70/","question_text":"A data engineer has configured a Structured Streaming job to read from a table, manipulate the data, and then perform a streaming write into a new table.\n\nThe code block used by the data engineer is below:\n\n//IMG//\n\n\nIf the data engineer only wants the query to process all of the available data in as many batches as required, which of the following lines of code should the data engineer use to fill in the blank?","unix_timestamp":1697944560,"discussion":[{"content":"Selected Answer: B\nb is ok","poster":"benni_ale","timestamp":"1714370100.0","upvote_count":"1","comment_id":"1203843"},{"timestamp":"1710093480.0","poster":"fifirifi","comment_id":"1170512","content":"Selected Answer: B\ncorrect answer: B\nexplanation: In Structured Streaming, if a data engineer wants to process all the available data in as many batches as required without any explicit trigger interval, they can use the option trigger(availableNow=True). This feature, availableNow, is used to specify that the query should process all the data that is available at the moment and not wait for more data to arrive.","upvote_count":"4"},{"content":"Selected Answer: B\nit’s the only answer with a correct syntax","comment_id":"1101417","timestamp":"1703066100.0","upvote_count":"1","poster":"AndreFR"},{"comment_id":"1083536","upvote_count":"2","content":"Selected Answer: B\nhttps://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.trigger.html","timestamp":"1701266220.0","poster":"55f31c8"},{"comment_id":"1052752","upvote_count":"4","content":"B\navailableNowbool, optional\nif set to True, set a trigger that processes all available data in multiple batches then terminates the query. Only one trigger can be set.","timestamp":"1698145860.0","poster":"kbaba101"},{"comment_id":"1050963","timestamp":"1697998080.0","content":"sorry Ans is B : https://stackoverflow.com/questions/71061809/trigger-availablenow-for-delta-source-streaming-queries-in-pyspark-databricks\n\nfor batch we use available now","upvote_count":"4","poster":"meow_akk"},{"upvote_count":"1","timestamp":"1697944560.0","content":"Correct Ans is D : \n%python\n\nspark.readStream.format(\"delta\").load(\"<delta_table_path>\")\n.writeStream\n.format(\"delta\")\n.trigger(processingTime='5 seconds') #Added line of code that defines .trigger processing time.\n.outputMode(\"append\")\n.option(\"checkpointLocation\",\"<checkpoint_path>\")\n.options(**writeConfig)\n.start()\n\nhttps://kb.databricks.com/streaming/optimize-streaming-transactions-with-trigger","poster":"meow_akk","comments":[{"content":"Nopes ! Use trigger(availableNow=True)","timestamp":"1721911080.0","upvote_count":"1","comment_id":"1254995","poster":"Souvik_79"}],"comment_id":"1050169"}]},{"id":"P9uirovIcYtBaOoaKUhD","timestamp":"2023-10-22 05:19:00","question_id":139,"answers_community":["B (100%)"],"choices":{"E":"Auto Loader cannot infer the schema of ingested data","C":"Auto Loader only works with string data","D":"All of the fields had at least one null value","B":"JSON data is a text-based format","A":"There was a type mismatch between the specific schema and the inferred schema"},"answer":"B","isMC":true,"discussion":[{"timestamp":"1718870820.0","upvote_count":"2","poster":"AndreFR","comment_id":"1101423","content":"Selected Answer: B\nhttps://docs.databricks.com/en/ingestion/auto-loader/schema.html#how-does-auto-loader-schema-inference-work \n\nBy default, Auto Loader schema inference seeks to avoid schema evolution issues due to type mismatches. For formats that don’t encode data types (JSON and CSV), Auto Loader infers all columns as strings (including nested fields in JSON files)."},{"upvote_count":"2","timestamp":"1718262480.0","comment_id":"1095309","poster":"nedlo","content":"Selected Answer: B\nIts B \"By default, Auto Loader schema inference seeks to avoid schema evolution issues due to type mismatches. For formats that don’t encode data types (JSON and CSV), Auto Loader infers all columns as strings (including nested fields in JSON files). For formats with typed schema (Parquet and Avro), Auto Loader samples a subset of files and merges the schemas of individual files. This behavior is summarized in the following table:\" https://docs.databricks.com/en/ingestion/auto-loader/schema.html"},{"comment_id":"1083553","content":"Selected Answer: B\nhttps://docs.databricks.com/en/ingestion/auto-loader/schema.html#how-does-auto-loader-schema-inference-work","timestamp":"1716984900.0","poster":"55f31c8","upvote_count":"2"},{"content":"The correct answer is: B. JSON data is a text-based format\n\nJSON data is a text-based format that uses strings to represent all values. When Auto Loader infers the schema of JSON data, it assumes that all values are strings. This is because Auto Loader cannot determine the type of a value based on its string representation.\n\nhttps://docs.databricks.com/en/ingestion/auto-loader/schema.html\n\nFor example, the following JSON string represents a value that is logically a boolean:\n\nJSON\n\"true\"\nUse code with caution. Learn more\nHowever, Auto Loader would infer that the type of this value is string. This is because Auto Loader cannot determine that the value is a boolean based on its string representation.\n\nIn order to get Auto Loader to infer the correct types for columns, the data engineer can provide type inference or schema hints. Type inference hints can be used to specify the types of specific columns. Schema hints can be used to provide the entire schema of the data.\n\nTherefore, the correct answer is B. JSON data is a text-based format.","comment_id":"1050170","poster":"meow_akk","timestamp":"1713755940.0","upvote_count":"2"}],"answer_images":[],"exam_id":162,"question_images":[],"unix_timestamp":1697944740,"topic":"1","url":"https://www.examtopics.com/discussions/databricks/view/124300-exam-certified-data-engineer-associate-topic-1-question-71/","answer_description":"","question_text":"A data engineer has developed a data pipeline to ingest data from a JSON source using Auto Loader, but the engineer has not provided any type inference or schema hints in their pipeline. Upon reviewing the data, the data engineer has noticed that all of the columns in the target table are of the string type despite some of the fields only including float or boolean values.\n\nWhich of the following describes why Auto Loader inferred all of the columns to be of the string type?","answer_ET":"B"},{"id":"d1ZWko5b9hIrr1TAQDDz","discussion":[{"timestamp":"1697945040.0","upvote_count":"11","content":"Ans E : Development and production modes\nYou can optimize pipeline execution by switching between development and production modes. Use the Delta Live Tables Environment Toggle Icon buttons in the Pipelines UI to switch between these two modes. By default, pipelines run in development mode.\n\nWhen you run your pipeline in development mode, the Delta Live Tables system does the following:\n\nReuses a cluster to avoid the overhead of restarts. By default, clusters run for two hours when development mode is enabled. You can change this with the pipelines.clusterShutdown.delay setting in the Configure your compute settings.\n\nDisables pipeline retries so you can immediately detect and fix errors.\n\nIn production mode, the Delta Live Tables system does the following:\n\nRestarts the cluster for specific recoverable errors, including memory leaks and stale credentials.\n\nRetries execution in the event of specific errors, for example, a failure to start a cluster.\n\nhttps://docs.databricks.com/en/delta-live-tables/updates.html#optimize-execution","comment_id":"1050171","poster":"meow_akk"},{"poster":"1017857","comment_id":"1356925","content":"Selected Answer: B\ntesting why testing","upvote_count":"1","timestamp":"1739635680.0"},{"content":"Selected Answer: D\nAccording to the document downloaded from the official website, the correct answer is letter D.\nPracticeExam-DataEngineerAssociate.pdf","comments":[{"comment_id":"1354997","upvote_count":"1","poster":"JuarezNJunior","timestamp":"1739277240.0","content":"Hello everyone. I made a mistake. The practical exam pdf is about \"Triggered Pipeline Mode\" against \"Continuous Pipeline Mode.\" in this question. The correct answer is E."}],"poster":"JuarezNJunior","timestamp":"1738960260.0","comment_id":"1353128","upvote_count":"1"},{"timestamp":"1736050680.0","content":"Selected Answer: E\nContinuous and Development Mode. Hence E","poster":"AnirbanRC","comment_id":"1336624","upvote_count":"1"},{"content":"Selected Answer: B\nThe correct answer is B. All datasets will be updated at set intervals until the pipeline is shut down. The compute resources will persist until the pipeline is shut down. In Continuous Pipeline Mode, the pipeline continuously processes data at set intervals, and the compute resources remain active until the pipeline is manually shut down.","comment_id":"1327282","poster":"MultiCloudIronMan","timestamp":"1734345840.0","upvote_count":"1"},{"comment_id":"1315945","timestamp":"1732212780.0","upvote_count":"1","poster":"806e7d2","content":"Selected Answer: B\nThe pipeline runs in Continuous Pipeline Mode, so datasets will be updated at set intervals.\nSince the pipeline is in Development Mode, the compute resources will persist until manually shut down."},{"upvote_count":"1","content":"E is correct !\n\nOption B: \"All datasets will be updated at set intervals until the pipeline is shut down. The compute resources will persist until the pipeline is shut down.\"\n\nThis option correctly reflects that the pipeline continues running, updating datasets at intervals, and only stops when manually shut down. Compute resources persist throughout this process.\nOption E: \"All datasets will be updated at set intervals until the pipeline is shut down. The compute resources will persist to allow for additional testing.\"\n\nWhile this is very similar, it adds the phrase \"to allow for additional testing,\" which might imply that the resources are persisting just for testing purposes. This can be misleading because the primary reason for resource persistence in Continuous mode is to keep the pipeline active and processing data, not solely for testing.","comment_id":"1267020","poster":"7a22144","timestamp":"1723808280.0"},{"upvote_count":"1","poster":"3fbc31b","timestamp":"1720473720.0","content":"Selected Answer: E\nThe answer is E. The compute resources will persist even after the pipeline is shut down.","comment_id":"1244568"},{"comment_id":"1203844","upvote_count":"1","content":"Selected Answer: E\ne as teh cluster actually persits differently from b","poster":"benni_ale","timestamp":"1714370220.0"},{"content":"Selected Answer: E\nE. All datasets will be updated at set intervals until the pipeline is shut down. The compute resources will persist to allow for additional testing.\n\nExplanation:\n\nIn Development mode, Delta Live Tables persistently updates datasets at set intervals. The pipeline continuously processes incoming data until manually stopped or shut down.\n\nCompute resources, including the cluster used for processing, persist without automatic restarts or retries (as it is the behavior in Development mode). This persistence allows for ongoing processing of data, enabling additional testing or continued data processing until the pipeline is manually shut down.\n\nTherefore, option E accurately captures the behavior expected in Development mode, emphasizing the continuous update of datasets and the persistence of compute resources until the pipeline is manually terminated.","poster":"Garyn","upvote_count":"2","comment_id":"1110192","timestamp":"1703986920.0"},{"poster":"kz_data","timestamp":"1701894420.0","comment_id":"1089743","content":"Selected Answer: E\nE seems the correct answer","upvote_count":"2"},{"poster":"nedlo","comment_id":"1089109","upvote_count":"2","comments":[{"timestamp":"1703078580.0","poster":"AndreFR","comments":[{"poster":"AndreFR","timestamp":"1703078580.0","comment_id":"1101544","content":"So correct answer is E","upvote_count":"1"}],"comment_id":"1101543","upvote_count":"1","content":"because \"The table is configured to run in Development mode\" when tables are set in dev mode, \"The compute resources will persist to allow for additional testing.\""}],"timestamp":"1701852840.0","content":"Selected Answer: B\nWhy E? It persists with same functionality as was before, not for \"additional testing\"?"},{"content":"Selected Answer: E\nhttps://docs.databricks.com/en/delta-live-tables/updates.html#continuous-vs-triggered-pipeline-execution\n\nhttps://docs.databricks.com/en/delta-live-tables/testing.html#use-development-mode-to-run-pipeline-updates","poster":"55f31c8","comment_id":"1084405","timestamp":"1701351600.0","upvote_count":"2"},{"comment_id":"1053328","content":"Selected Answer: E\nE is correct","timestamp":"1698193680.0","upvote_count":"2","poster":"anandpsg101"},{"poster":"SD5713","upvote_count":"2","timestamp":"1697882760.0","comment_id":"1049399","content":"Selected Answer: E\nE. All datasets will be updated at set intervals until the pipeline is shut down. The compute resources will persist to allow for additional testing."}],"unix_timestamp":1697882760,"question_images":[],"exam_id":162,"answer":"E","answers_community":["E (68%)","B (26%)","5%"],"answer_images":[],"question_text":"A Delta Live Table pipeline includes two datasets defined using STREAMING LIVE TABLE. Three datasets are defined against Delta Lake table sources using LIVE TABLE.\n\nThe table is configured to run in Development mode using the Continuous Pipeline Mode.\n\nAssuming previously unprocessed data exists and all definitions are valid, what is the expected outcome after clicking Start to update the pipeline?","answer_description":"","url":"https://www.examtopics.com/discussions/databricks/view/124177-exam-certified-data-engineer-associate-topic-1-question-72/","question_id":140,"isMC":true,"choices":{"B":"All datasets will be updated at set intervals until the pipeline is shut down. The compute resources will persist until the pipeline is shut down.","A":"All datasets will be updated once and the pipeline will shut down. The compute resources will be terminated.","C":"All datasets will be updated once and the pipeline will persist without any processing. The compute resources will persist but go unused.","D":"All datasets will be updated once and the pipeline will shut down. The compute resources will persist to allow for additional testing.","E":"All datasets will be updated at set intervals until the pipeline is shut down. The compute resources will persist to allow for additional testing."},"topic":"1","answer_ET":"E","timestamp":"2023-10-21 12:06:00"}],"exam":{"isMCOnly":true,"numberOfQuestions":169,"isBeta":false,"name":"Certified Data Engineer Associate","id":162,"isImplemented":true,"lastUpdated":"12 Apr 2025","provider":"Databricks"},"currentPage":28},"__N_SSP":true}