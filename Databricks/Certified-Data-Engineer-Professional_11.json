{"pageProps":{"questions":[{"id":"aSgHpnxemCqtjyiUnj1X","answer_ET":"B","exam_id":163,"isMC":true,"choices":{"D":"• Total VMs: 4\n• 100 GB per Executor\n• 40 Cores / Executor","C":"• Total VMs: 1\n• 400 GB per Executor\n• 160 Cores/Executor","A":"• Total VMs: 8\n• 50 GB per Executor\n• 20 Cores / Executor","B":"• Total VMs: 16\n• 25 GB per Executor\n• 10 Cores / Executor"},"discussion":[{"poster":"m79590530","timestamp":"1729434960.0","upvote_count":"1","content":"Selected Answer: B\nDistributing work across more Workers/Executors will have better guarantee in case of 1 or more of them fail","comment_id":"1300514"}],"answer":"B","topic":"1","timestamp":"2024-10-20 16:36:00","answers_community":["B (100%)"],"url":"https://www.examtopics.com/discussions/databricks/view/149859-exam-certified-data-engineer-professional-topic-1-question/","question_id":51,"unix_timestamp":1729434960,"question_text":"Each configuration below is identical to the extent that each cluster has 400 GB total of RAM, 160 total cores and only one Executor per VM.\n\nGiven an extremely long-running job for which completion must be guaranteed, which cluster configuration will be able to guarantee completion of the job in light of one or more VM failures?","answer_description":"","question_images":[],"answer_images":[]},{"id":"s9bsbaoJT28uFRn5yZaC","url":"https://www.examtopics.com/discussions/databricks/view/146974-exam-certified-data-engineer-professional-topic-1-question/","exam_id":163,"timestamp":"2024-09-05 03:58:00","question_images":["https://img.examtopics.com/certified-data-engineer-professional/image63.png"],"answer_images":[],"isMC":true,"answer":"A","question_text":"A task orchestrator has been configured to run two hourly tasks. First, an outside system writes Parquet data to a directory mounted at /mnt/raw_orders/. After this data is written, a Databricks job containing the following code is executed:\n\n//IMG//\n\n\nAssume that the fields customer_id and order_id serve as a composite key to uniquely identify each order, and that the time field indicates when the record was queued in the source system.\n\nIf the upstream system is known to occasionally enqueue duplicate entries for a single order hours apart, which statement is correct?","question_id":52,"answer_ET":"A","unix_timestamp":1725501480,"choices":{"C":"The orders table will contain only the most recent 2 hours of records and no duplicates will be present.","A":"Duplicate records enqueued more than 2 hours apart may be retained and the orders table may contain duplicate records with the same customer_id and order_id.","B":"All records will be held in the state store for 2 hours before being deduplicated and committed to the orders table.","D":"The orders table will not contain duplicates, but records arriving more than 2 hours late will be ignored and missing from the table."},"discussion":[{"content":"Selected Answer: A\nA - two orders [customer_id, order_id] might be emitted with time column value that is 2 or more hours apart. So the watermark will not drop the new record, since it will have a new time value. From the composite key perspective, it is going to be a duplicate.","upvote_count":"1","timestamp":"1735818780.0","comment_id":"1335541","poster":"arekm"},{"content":"Selected Answer: A\ndropDuplicates only deletes the duplicates of the processed batch. If we have records with same key in different batches, we will have duplicates in the final table. \n\nIn addition, withWatermark, when is not used in a window, gets the MAX(eventTime) and uses the threshold to define the time range. As the time represent when the data has been queued in the source system, we can get records where the time we get older than 2 hours.\n\npyspark.sql.DataFrame.dropDuplicates — PySpark 3.5.3 documentation\npyspark.sql.DataFrame.withWatermark — PySpark 3.5.3 documentation","comments":[{"upvote_count":"1","comment_id":"1388257","content":"Without any windowing or event-time-based aggregation, the watermark won't be used in the query.","timestamp":"1741861440.0","poster":"73109a1"}],"poster":"UrcoIbz","comment_id":"1329104","timestamp":"1734633180.0","upvote_count":"2"},{"content":"Selected Answer: D\nThe orders arriving 2 hours or later will be dropped. There is a chance that they can be processed, but still deduplication will happen.","timestamp":"1731259500.0","poster":"vish9","comment_id":"1309552","upvote_count":"2"},{"upvote_count":"1","timestamp":"1731086880.0","poster":"smashit","content":"There might be chance that same record for example A1,O1 comes in Batch B1 also comes in B2. we need to implement merge logic inside our target table or perform insert-only merge.","comment_id":"1308874"},{"comment_id":"1301444","poster":"Jugiboss","content":"Selected Answer: A\nWatermark thresholds guarantee that records arriving within the specified threshold are processed according to the semantics of the defined query. Late-arriving records arriving outside the specified threshold might still be processed using query metrics, but this is not guaranteed.","upvote_count":"1","timestamp":"1729579920.0"},{"timestamp":"1729435320.0","content":"Selected Answer: D\nThe default write mode is 'append'. Duplicate will be resolved for each 2 hr window and .withWatermark() will drop/ignore the records delayed more than 2 hours apart.","upvote_count":"1","poster":"m79590530","comment_id":"1300518"},{"upvote_count":"1","content":"Selected Answer: A\nThe default write mode is append. Duplicate will be resolved for only 2 hr window but may still exist because of previous execution.","timestamp":"1725501480.0","poster":"csrazdan","comment_id":"1278582"}],"topic":"1","answers_community":["A (63%)","D (38%)"],"answer_description":""},{"id":"utBTqWfkLzDH8lmppTWE","timestamp":"2024-10-20 16:54:00","answer_description":"","question_id":53,"url":"https://www.examtopics.com/discussions/databricks/view/149865-exam-certified-data-engineer-professional-topic-1-question/","discussion":[{"poster":"benni_ale","upvote_count":"1","timestamp":"1730882400.0","content":"Selected Answer: C\nC seems logical","comment_id":"1307726"},{"poster":"m79590530","timestamp":"1729436040.0","upvote_count":"2","comment_id":"1300526","content":"Selected Answer: C\nFrom all the provided options Answer C is the only meaningful and possible one. Also MERGE INTO ... WHEN NOT MATCHED INSERT *; is a standard solution for adding/appending non-existing records (by key) to the target table withOUT duplicating."}],"question_images":[],"question_text":"A data engineer is configuring a pipeline that will potentially see late-arriving, duplicate records.\n\nIn addition to de-duplicating records within the batch, which of the following approaches allows the data engineer to deduplicate data against previously processed records as it is inserted into a Delta table?","answer_ET":"C","topic":"1","choices":{"B":"VACUUM the Delta table after each batch completes.","D":"Perform a full outer join on a unique key and overwrite existing data.","A":"Rely on Delta Lake schema enforcement to prevent duplicate records.","C":"Perform an insert-only merge with a matching condition on a unique key."},"unix_timestamp":1729436040,"isMC":true,"exam_id":163,"answers_community":["C (100%)"],"answer":"C","answer_images":[]},{"id":"uBxn0eXd0Fzo1PEELyp0","url":"https://www.examtopics.com/discussions/databricks/view/147917-exam-certified-data-engineer-professional-topic-1-question/","isMC":true,"answer":"B","question_images":["https://img.examtopics.com/certified-data-engineer-professional/image64.png"],"unix_timestamp":1726919580,"topic":"1","choices":{"C":"Each time the job is executed, only those records that have been inserted or updated since the last execution will be appended to the target table, giving the desired result.","A":"Each time the job is executed, newly updated records will be merged into the target table, overwriting previous values with the same primary keys.","D":"Each time the job is executed, the differences between the original and current versions are calculated; this may result in duplicate entries for some records.","B":"Each time the job is executed, the entire available history of inserted or updated records will be appended to the target table, resulting in many duplicate entries."},"timestamp":"2024-09-21 13:53:00","answer_description":"","answer_ET":"B","exam_id":163,"answer_images":[],"answers_community":["B (100%)"],"question_id":54,"discussion":[{"content":"Selected Answer: B\nB seems ok","comment_id":"1307729","timestamp":"1730882460.0","poster":"benni_ale","upvote_count":"1"},{"upvote_count":"1","timestamp":"1729436520.0","content":"Selected Answer: B\nSince the code is using version 0 for the CDF-enabled table every time it is executed all the historical changes being insert of update for the table will be 'append'-ed to the target table since this is the write command option provided.","comment_id":"1300535","poster":"m79590530"},{"content":"B. This bad effect (many duplicates) happens because the code reads from the starting version 0, appending all changes since the beginning.","timestamp":"1726919580.0","upvote_count":"2","comment_id":"1287307","poster":"Adrifersilva"}],"question_text":"A junior data engineer seeks to leverage Delta Lake's Change Data Feed functionality to create a Type 1 table representing all of the values that have ever been valid for all rows in a bronze table created with the property delta.enableChangeDataFeed = true. They plan to execute the following code as a daily job:\n\n//IMG//\n\n\nWhich statement describes the execution and results of running the above query multiple times?"},{"id":"ir9F7dlyrvVitDkdTPFw","answer_ET":"B","exam_id":163,"isMC":true,"choices":{"C":"Set the pipelines.reset.allowed property to false on bpm_stats","B":"Set the skipChangeCommits flag to true on raw_iot","D":"Set the skipChangeCommits flag to true on bpm_stats","A":"Set the pipelines.reset.allowed property to false on raw_iot"},"discussion":[{"upvote_count":"1","comment_id":"1559185","poster":"teowis","content":"Selected Answer: A\nThe purpuse of it is to take into account changees made manually in the raw_iot table and propagate them to the downstream tables.","timestamp":"1744188840.0"},{"poster":"Sriramiyer92","upvote_count":"2","timestamp":"1735809960.0","content":"Selected Answer: B\nAnswer is B.\n\nIgnore updates and deletes - https://docs.databricks.com/en/structured-streaming/delta-lake.html#ignore-updates-and-deletes\n\nStructured Streaming does not handle input that is not an append and throws an exception if any modifications occur on the table being used as a source. There are two main strategies for dealing with changes that cannot be automatically propagated downstream:\n\nYou can delete the output and checkpoint and restart the stream from the beginning.\n\nYou can set either of these two options:\n\nignoreDeletes: ignore transactions that delete data at partition boundaries.\n\nskipChangeCommits: ignore transactions that delete or modify existing records. skipChangeCommits subsumes ignoreDeletes.","comment_id":"1335438"},{"comment_id":"1300549","poster":"m79590530","content":"Selected Answer: A\nSetting pipelines.reset.allowed property to false on raw_iot prevents its full refresh/reset being done from its source, meaning it preserves all other modifications done for it additionally outside of its source streaming process. If we set skipChangeCommits for it or the target table transactions that delete or modify records on the table are ignored","upvote_count":"2","timestamp":"1729438980.0"},{"poster":"pk07","comment_id":"1289441","upvote_count":"4","content":"Selected Answer: B\nB. Set the skipChangeCommits flag to true on raw_iot\n\nLet's break down the requirements and explain why this is the best solution:\n\nRetain manually deleted or updated records in raw_iot: The skipChangeCommits flag, when set to true, tells Delta Live Tables (DLT) to ignore any manual changes (updates or deletes) made to the table outside of the pipeline. This means that even if records are manually deleted or updated in the raw_iot table, these changes won't be reflected in the table when the pipeline runs again.\nRecompute downstream bpm_stats table: By default, DLT will recompute downstream tables when their upstream dependencies change. Since bpm_stats is based on raw_iot, it will naturally be recomputed when the pipeline updates, without any special configuration.\nWhy the other options are not correct:\n\nA. Setting pipelines.reset.allowed to false on raw_iot would prevent the table from being reset, but it wouldn't address the requirement to retain manually deleted or updated records.","timestamp":"1727354280.0"},{"timestamp":"1720552980.0","content":"answer A\nThis property, when set to false, ensures that the table will not be reset during pipeline updates, thus preserving manually deleted or updated records. This is crucial for the raw_iot table to retain the manual modifications.","upvote_count":"3","comment_id":"1245096","poster":"nikoliko"},{"comment_id":"1244965","content":"Selected Answer: A\nSet the pipelines.reset.allowed property to false on raw_iot","timestamp":"1720537020.0","poster":"c00ccb7","upvote_count":"3"},{"upvote_count":"1","timestamp":"1717628820.0","comment_id":"1225051","content":"Answer: B","poster":"Deb9753"}],"answer":"B","topic":"1","timestamp":"2024-05-29 19:42:00","answers_community":["B (54%)","A (46%)"],"url":"https://www.examtopics.com/discussions/databricks/view/141556-exam-certified-data-engineer-professional-topic-1-question/","unix_timestamp":1717004520,"question_id":55,"question_text":"A DLT pipeline includes the following streaming tables:\n\n• raw_iot ingests raw device measurement data from a heart rate tracking device.\n• bpm_stats incrementally computes user statistics based on BPM measurements from raw_iot.\n\nHow can the data engineer configure this pipeline to be able to retain manually deleted or updated records in the raw_iot table, while recomputing the downstream table bpm_stats table when a pipeline update is run?","answer_description":"","question_images":[],"answer_images":[]}],"exam":{"isBeta":false,"isImplemented":true,"id":163,"name":"Certified Data Engineer Professional","provider":"Databricks","lastUpdated":"12 Apr 2025","isMCOnly":true,"numberOfQuestions":200},"currentPage":11},"__N_SSP":true}