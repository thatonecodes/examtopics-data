{"pageProps":{"questions":[{"id":"B83eq77Qwt9mYEDpt8VL","timestamp":"2024-02-09 15:15:00","topic":"1","question_id":171,"answer":"C","answer_ET":"C","isMC":true,"exam_id":161,"url":"https://www.examtopics.com/discussions/databricks/view/133453-exam-certified-associate-developer-for-apache-spark-topic-1/","choices":{"A":"storesDF.repartition(4, \"sqft\")","B":"storesDF.repartition()","D":"storesDF.repartition(4)","E":"storesDF.coalesce","C":"storesDF.coalesce(4)"},"question_images":[],"answer_description":"","discussion":[{"poster":"azure_bimonster","comment_id":"1145539","upvote_count":"1","content":"Selected Answer: C\nC is the right one here.\nUnlike repartition(), coalesce() reduces the number of partitions without shuffling the data. By specifying the number of partitions (4), it ensures that the resulting DataFrame has 4 partitions without inducing a shuffle.","timestamp":"1723205700.0"}],"unix_timestamp":1707488100,"answer_images":[],"answers_community":["C (100%)"],"question_text":"Which of the following code blocks will always return a new 4-partition DataFrame from the 8-partition DataFrame storesDF without inducing a shuffle?"},{"id":"PIIXfCo1mAnTtkSDAKvP","answers_community":[],"question_id":172,"topic":"1","answer_images":[],"exam_id":161,"choices":{"B":"1. storesDF\n2. coalesce\n3. 4, \"storeId\"","E":"1. storesDF\n2. repartition\n3. Nothing","A":"1. storesDF\n2. coalesce\n3. 4","C":"1. storesDF\n2. repartition\n3. \"storeId\"","D":"1. storesDF\n2. repartition\n3. 12"},"answer_description":"","answer_ET":"D","unix_timestamp":1712312100,"question_text":"The code block shown below should return a new 12-partition DataFrame from DataFrame storesDF. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.\n\nCode block:\n\n__1__.__2__(__3__)","url":"https://www.examtopics.com/discussions/databricks/view/137960-exam-certified-associate-developer-for-apache-spark-topic-1/","question_images":[],"isMC":true,"discussion":[{"timestamp":"1728123300.0","comment_id":"1189857","poster":"Sowwy1","upvote_count":"1","content":"Should be D"}],"answer":"D","timestamp":"2024-04-05 12:15:00"},{"id":"H8Tamv9vi7eCrO7cCryt","question_text":"The code block shown below contains an error. The code block is intended to adjust the number of partitions used in wide transformations like join() to 32. Identify the error.\n\nCode block:\n\nspark.conf.set(\"spark.default.parallelism\", \"32\")","answer_description":"","question_images":[],"timestamp":"2023-08-15 17:37:00","exam_id":161,"choices":{"A":"spark.default.parallelism is not the right Spark configuration parameter – spark.sql.shuffle.partitions should be used instead.","D":"Spark configuration parameters are not set with spark.conf.set().","E":"The second argument should not be the string version of \"32\" – it should be the integer 32.","B":"There is no way to adjust the number of partitions used in wide transformations – it defaults to the number of total CPUs in the cluster.","C":"Spark configuration parameters cannot be set in runtime."},"topic":"1","question_id":173,"unix_timestamp":1692113820,"url":"https://www.examtopics.com/discussions/databricks/view/118188-exam-certified-associate-developer-for-apache-spark-topic-1/","answer_images":[],"answer_ET":"A","answers_community":["A (100%)"],"isMC":true,"discussion":[{"content":"A ist richtig","poster":"Anweee","upvote_count":"1","comment_id":"1139706","timestamp":"1722727200.0"},{"content":"Selected Answer: A\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"32\")","upvote_count":"3","poster":"newusername","comment_id":"1066231","timestamp":"1715234160.0"},{"content":"Selected Answer: A\nshould be spark.sql.shuffle.partitions for joins","poster":"Ram459","timestamp":"1708018620.0","comment_id":"981813","upvote_count":"4"}],"answer":"A"},{"id":"YAFsnAH1Zz2HkxG526OV","choices":{"E":"There is no dayofyear() operation – the day of year number must be extracted using substring utilities.","D":"The dayofyear() operation is not applicable in a withColumn() call – the newColumn() operation must be used instead.","C":"The dayofyear() operation cannot extract the day of year from a column of type integer – column openDate must first be converted to type Date.","A":"The dayofyear() operation cannot extract the day of year from a column of type integer – column openDate must first be converted to type Timestamp.","B":"The dayofyear() operation takes a quoted column name rather than a Column object as its first argument – the first argument should be \"openDate\"."},"unix_timestamp":1695564360,"exam_id":161,"answer_description":"","answers_community":["A (100%)"],"url":"https://www.examtopics.com/discussions/databricks/view/121318-exam-certified-associate-developer-for-apache-spark-topic-1/","discussion":[{"comment_id":"1156859","upvote_count":"1","timestamp":"1724373960.0","content":"Selected Answer: A\nA is correct","poster":"tangerine141"},{"upvote_count":"1","content":"Selected Answer: A\nA is correct","timestamp":"1715236320.0","poster":"newusername","comment_id":"1066241"},{"poster":"kaisa1234","upvote_count":"1","comment_id":"1015866","comments":[{"timestamp":"1715236260.0","comment_id":"1066240","content":"the answer there is wrong. I posted the correct one.","upvote_count":"1","poster":"newusername"}],"timestamp":"1711296360.0","content":"But question 50 is the same question and states that the code block with an error is the correct"}],"answer":"A","topic":"1","isMC":true,"question_images":["https://img.examtopics.com/certified-associate-developer-for-apache-spark/image11.png"],"timestamp":"2023-09-24 16:06:00","answer_images":[],"answer_ET":"A","question_text":"The code block shown below contains an error. The code block intended to return a DataFrame containing a column dayOfYear, an integer representation of the day of the year from column openDate from DataFrame storesDF. Identify the error.\n\nNote that column openDate is of type integer and represents a date in the UNIX epoch format – the number of seconds since midnight on January 1st, 1970.\n\nA sample of storesDF is displayed below:\n\n//IMG//\n\n\nCode block:\n\nstoresDF.withColumn(\"dayOfYear\", dayofyear(col(\"openDate\")))","question_id":174},{"id":"7wPjOVeVb8pQVjv4m6TD","timestamp":"2023-05-15 07:40:00","question_images":[],"discussion":[{"content":"Selected Answer: C\nThe DataFrame operation classified as an action is:\n\nC. DataFrame.take()\n\nExplanation: In Spark, actions are operations that trigger the execution of transformations on a DataFrame and return results or side effects. Actions are evaluated eagerly, meaning they initiate the execution of the computation plan built by transformations. Among the options provided, DataFrame.take() is an action because it returns an array with the first n elements from the DataFrame as an array. It triggers the execution of any pending transformations and collects the resulting data.","timestamp":"1687013580.0","upvote_count":"5","poster":"TmData","comment_id":"926056"},{"content":"Selected Answer: C\nIt returns first n rows of an array","comment_id":"1291694","poster":"Becida","upvote_count":"1","timestamp":"1727736720.0"},{"comment_id":"898062","content":"Selected Answer: C\nDataFrame.take(num: int) → List[pyspark.sql.types.Row]\nAll the other functions return a dataframe again, which is defined as a transformation. An action returns a result of a computation, which take() does.","timestamp":"1684129200.0","poster":"SonicBoom10C9","upvote_count":"3"}],"topic":"1","question_text":"Which of the following DataFrame operations is classified as an action?","unix_timestamp":1684129200,"url":"https://www.examtopics.com/discussions/databricks/view/109255-exam-certified-associate-developer-for-apache-spark-topic-1/","answer":"C","question_id":175,"answers_community":["C (100%)"],"isMC":true,"answer_ET":"C","exam_id":161,"answer_description":"","choices":{"C":"DataFrame.take()","D":"DataFrame.join()","A":"DataFrame.drop()","B":"DataFrame.coalesce()","E":"DataFrame.filter()"},"answer_images":[]}],"exam":{"isBeta":false,"isImplemented":true,"id":161,"name":"Certified Associate Developer for Apache Spark","lastUpdated":"12 Apr 2025","provider":"Databricks","isMCOnly":true,"numberOfQuestions":185},"currentPage":35},"__N_SSP":true}