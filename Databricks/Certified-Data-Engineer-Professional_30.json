{"pageProps":{"questions":[{"id":"xaEQtKPQWV8nDENSCbXq","question_text":"A junior developer complains that the code in their notebook isn't producing the correct results in the development environment. A shared screenshot reveals that while they're using a notebook versioned with Databricks Repos, they're using a personal branch that contains old logic. The desired branch named dev-2.3.9 is not available from the branch selection dropdown.\nWhich approach will allow this developer to review the current logic for this notebook?","answer":"B","answer_description":"","answer_images":[],"question_id":146,"choices":{"E":"Use Repos to merge the current branch and the dev-2.3.9 branch, then make a pull request to sync with the remote repository","B":"Use Repos to pull changes from the remote Git repository and select the dev-2.3.9 branch.","C":"Use Repos to checkout the dev-2.3.9 branch and auto-resolve conflicts with the current branch","D":"Merge all changes back to the main branch in the remote Git repository and clone the repo again","A":"Use Repos to make a pull request use the Databricks REST API to update the current branch to dev-2.3.9"},"question_images":[],"timestamp":"2023-10-16 06:51:00","discussion":[{"upvote_count":"1","comment_id":"1506718","content":"Selected Answer: B\nFirst step is to pull from the latest commits","timestamp":"1743820680.0","poster":"codebender"},{"comments":[{"content":"The first step is to perform a pull. This fetches all the changes from the remote branch and only after that will you see the dev branch.","poster":"NBurman","comment_id":"1310142","timestamp":"1731335160.0","upvote_count":"4"}],"content":"Selected Answer: B\nI would also say B but could anyone explain how to pick that branch if it is not available from dropdown?","poster":"benni_ale","upvote_count":"1","timestamp":"1728364680.0","comment_id":"1294569"},{"poster":"benni_ale","upvote_count":"1","content":"Selected Answer: B\nI would say B","timestamp":"1726810620.0","comment_id":"1286663"},{"upvote_count":"2","content":"B. Use Repos to pull changes from the remote Git repository and select the dev-2.3.9 branch.","timestamp":"1717547340.0","comment_id":"1224436","poster":"imatheushenrique"},{"comment_id":"1128060","poster":"AziLa","timestamp":"1705862760.0","content":"correct ans is B","upvote_count":"1"},{"timestamp":"1705147980.0","content":"Selected Answer: B\nvote for B also","poster":"Jay_98_11","comment_id":"1121592","upvote_count":"2"},{"upvote_count":"1","content":"Selected Answer: B\nB is correct","timestamp":"1697431860.0","comment_id":"1044666","poster":"sturcu"}],"isMC":true,"unix_timestamp":1697431860,"url":"https://www.examtopics.com/discussions/databricks/view/123733-exam-certified-data-engineer-professional-topic-1-question-5/","answer_ET":"B","exam_id":163,"answers_community":["B (100%)"],"topic":"1"},{"id":"20N1c7BqnikzdlqTkbxP","answers_community":["E (48%)","D (33%)","A (19%)"],"timestamp":"2023-08-27 16:39:00","discussion":[{"comment_id":"991509","timestamp":"1693147140.0","poster":"BrianNguyen95","comments":[{"timestamp":"1724245140.0","upvote_count":"3","content":"A bottleneck occurs when resources are over utilized not underutilized, so that explanation doesn't make too much sense. CPU utilization would be at 100% and you wouldn't see spike in I/O if the driver was the issue. Conversely if the I/O was spiked and CPU utilization was at 25% , then network could be the issue. D is the only logical answer in this case.","comments":[{"timestamp":"1730194320.0","upvote_count":"2","content":"i like this more","comment_id":"1304375","poster":"benni_ale"}],"poster":"fe3b2fc","comment_id":"1270128"},{"upvote_count":"2","timestamp":"1707299220.0","comment_id":"1143158","content":"Overall CPU utilization can be misleading. The 25% utilization could be caused by the workload not requiring more than that rather than everything being executed in the driver node.","poster":"guillesd"}],"upvote_count":"19","content":"Option E: In a Spark cluster, the driver node is responsible for managing the execution of the Spark application, including scheduling tasks, managing the execution plan, and interacting with the cluster manager. If the overall cluster CPU utilization is low (e.g., around 25%), it may indicate that the driver node is not utilizing the available resources effectively and might be a bottleneck."},{"timestamp":"1740984780.0","upvote_count":"2","poster":"Tedet","content":"Selected Answer: A\nWhen you see the \"Five Minute Load Average\" remain consistent or flat, it could indicate that the driver is under heavy load and is struggling to keep up with the workload. In the case of a Spark cluster, if the driver is handling too much work, it can become a bottleneck and prevent the overall job from progressing efficiently.","comment_id":"1364294"},{"timestamp":"1735309860.0","poster":"srinivasa","comment_id":"1332442","content":"Selected Answer: A\nConsistent/Flat Five Minute Load Average: If the load average on the driver node remains consistent and does not fluctuate, it suggests that the driver is under constant, significant load. This could be a sign that the driver is performing a lot of work, potentially leading to a bottleneck.","upvote_count":"3"},{"timestamp":"1734262800.0","upvote_count":"2","content":"Selected Answer: E\nAnswer E. A low CPU usage could indicate that the driver isn't working as efficiently as expected, which can lead to underutilization of the cluster and slower processing times.","comment_id":"1326817","poster":"AlejandroU"},{"content":"Selected Answer: E\nOnly when the driver does all or most the work will the overall cluster CPU util be this low since the driver cpu is 25% of the overall cluster CPU amount","timestamp":"1732652160.0","upvote_count":"1","poster":"JB90","comment_id":"1318289"},{"timestamp":"1729941720.0","content":"Selected Answer: E\nbottleneck means data skew means one of the nodes is doing majority of work while other is idle, so E is correct","comment_id":"1303250","upvote_count":"2","poster":"nedlo"},{"poster":"m79590530","comment_id":"1299700","upvote_count":"1","timestamp":"1729259220.0","content":"Selected Answer: E\nD also means that Driver never send big data chunks to the Worker nodes but as it is not mentioned to be 0 then it has a constant flow of data going in & out between the Driver node and the Worker nodes. Therefore it is not a measure of Driver bottleneck. However Answer E means one of the 4 cluster nodes is always working at 100% which can not be other than the Driver node as it is always working and coordinating work across Executors."},{"upvote_count":"2","poster":"fe3b2fc","content":"Selected Answer: D\nExecutors talk between each other and between nodes, if the code/driver is working as intended you would see a spike in I/O while transferring data. If the code/driver was the issue you would see a spike in CPU usage and little network traffic between nodes. The correct answer is D.","timestamp":"1724245320.0","comment_id":"1270130"},{"timestamp":"1717999980.0","content":"Selected Answer: E\nE is correct","upvote_count":"1","poster":"lophonos","comment_id":"1227690"},{"content":"Selected Answer: D\nIf there's no IO between driver and executor nodes then the executor nodes are not working","upvote_count":"1","poster":"guillesd","timestamp":"1707299160.0","comment_id":"1143155"},{"content":"Selected Answer: D\nD seems to be right","upvote_count":"2","poster":"Patito","comment_id":"1108701","timestamp":"1703855760.0"},{"content":"Selected Answer: E\nE is correct","poster":"rok21","timestamp":"1702144920.0","upvote_count":"1","comment_id":"1091957"},{"timestamp":"1702122720.0","comments":[{"comment_id":"1130382","content":"Not correct. 25% could (in theory) mean driver is using 100% CPU","timestamp":"1706084460.0","poster":"Def21","upvote_count":"1"}],"comment_id":"1091782","content":"Selected Answer: E\n25% indicates Cluster CPU under-utilized","poster":"azurelearn2020","upvote_count":"2"},{"upvote_count":"3","content":"Selected Answer: E\nIf the overall cluster CPU utilization is around 25%, it means that only one out of the four nodes (driver + 3 executors) is using its full CPU capacity, while the other three nodes are idle or underutilized","comment_id":"1052869","timestamp":"1698153360.0","poster":"sturcu"},{"content":"Selected Answer: D\nIf the overall cluster CPU utilization is around 25%, it means that only one out of the four nodes (driver + 3 executors) is using its full CPU capacity, while the other three nodes are idle or underutilized","comments":[],"upvote_count":"4","comment_id":"1044878","timestamp":"1697453340.0","poster":"sturcu"}],"answer_description":"","question_id":147,"answer":"E","choices":{"D":"Network I/O never spikes","C":"Total Disk Space remains constant","A":"The five Minute Load Average remains consistent/flat","B":"Bytes Received never exceeds 80 million bytes per second","E":"Overall cluster CPU utilization is around 25%"},"exam_id":163,"url":"https://www.examtopics.com/discussions/databricks/view/119184-exam-certified-data-engineer-professional-topic-1-question/","isMC":true,"unix_timestamp":1693147140,"topic":"1","answer_ET":"E","answer_images":[],"question_text":"A production cluster has 3 executor nodes and uses the same virtual machine type for the driver and executor.\nWhen evaluating the Ganglia Metrics for this cluster, which indicator would signal a bottleneck caused by code executing on the driver?","question_images":[]},{"id":"rruD0fJCk76pvMu50jx7","unix_timestamp":1707297360,"question_id":148,"choices":{"E":"In the Query Detail screen, by interpreting the Physical Plan","D":"In the Delta Lake transaction log. by noting the column statistics","A":"In the Executor’s log file, by grepping for \"predicate push-down\"","C":"In the Storage Detail screen, by noting which RDDs are not stored on disk","B":"In the Stage’s Detail screen, in the Completed Stages table, by noting the size of data read from the Input column"},"answer_images":[],"timestamp":"2024-02-07 10:16:00","answers_community":["E (83%)","B (17%)"],"answer_ET":"E","exam_id":163,"answer":"E","url":"https://www.examtopics.com/discussions/databricks/view/133247-exam-certified-data-engineer-professional-topic-1-question/","topic":"1","question_text":"Where in the Spark UI can one diagnose a performance problem induced by not leveraging predicate push-down?","isMC":true,"answer_description":"","discussion":[{"upvote_count":"1","poster":"Tedet","timestamp":"1740985500.0","content":"Selected Answer: E\nPredicate push-down is an optimization where conditions (such as filters) are pushed as close to the data source as possible (often to the database or file system level), reducing the amount of data read and processed. If predicate push-down isn't being leveraged, it can result in reading unnecessary data, leading to performance degradation.\nExecute a query --> Click View and go to Spark UI --> Navigate to SQL/DataFrame tab in SparkUI --> Click on any stage --> Navigate to details to find Physical Plan","comment_id":"1364299"},{"poster":"shaswat1404","content":"Selected Answer: B\nwhen predicated pushdown is working properly, the amount of data read should be much lower because the data source is able to filter out the rows at read time based on the query predicates. if predicate pushdown is not levaraged, stages might read a much larger volume of data than necessary, which can be observed in the input column in the stage detail screen\ntherefore B is the correct option \nnot A : executor logs might contain some information, but they are niot the most direct way to assess predicate push-down performance\nnot C : used to check RDD caching and persistence, not predicate push-down\nnot D : it holds meta data and statistics but is not viewed via the spark UI for diagnosing query performance\nnot E : while physical plan in the query detail screen might filter push-down, interpreting it requires more expertise, and the metric on the input data size(option B) is more straight forward indicator.","comment_id":"1353918","upvote_count":"1","timestamp":"1739107680.0"},{"poster":"benni_ale","upvote_count":"1","comment_id":"1306886","content":"Selected Answer: E\nE","timestamp":"1730716020.0"},{"comment_id":"1293856","upvote_count":"2","poster":"dd1192d","content":"Selected Answer: E\nE is correct : https://docs.datastax.com/en/dse/6.9/spark/predicate-push-down.html","timestamp":"1728219900.0"},{"comment_id":"1143091","upvote_count":"1","timestamp":"1707297360.0","poster":"P1314","content":"Selected Answer: E\nQuery plan. Correct is E"}],"question_images":[]},{"id":"OMDtthj31Uvaf0fRLYnX","choices":{"E":"There is a syntax error because the heartrate column is not correctly identified as a column.","D":"There is a type error because a DataFrame object cannot be multiplied.","B":"There is no column in the table named heartrateheartrateheartrate","C":"There is a type error because a column object cannot be multiplied.","A":"The code executed was PySpark but was executed in a Scala notebook."},"discussion":[{"content":"Selected Answer: B\nIt's B, there is no column with that name","poster":"CertPeople","timestamp":"1710233220.0","comment_id":"1005470","upvote_count":"8"},{"poster":"rok21","upvote_count":"5","comment_id":"1091960","content":"Selected Answer: E\nE is correct","timestamp":"1717949340.0"},{"poster":"guillesd","upvote_count":"2","content":"Selected Answer: B\nIt's B. Regarding E, a syntax error would mean that the query is not valid due to a wrongfully written SQL statement. However, this is not the case. The column just does not exist.","comment_id":"1143185","timestamp":"1723017660.0"},{"poster":"Jay_98_11","upvote_count":"1","comment_id":"1121989","timestamp":"1720891500.0","content":"Selected Answer: B\nhttps://sparkbyexamples.com/spark/spark-cannot-resolve-given-input-columns/"},{"content":"the answer is E, because \ndf.select(3*df['heartrate']).show() perfectly returns","comment_id":"1088715","comments":[{"upvote_count":"1","content":"3*\"heartrate\" is triple of string \"heartrate\" ,isn't value of heartrate multiplied by 3.","timestamp":"1721540520.0","comment_id":"1127695","poster":"chokthewa"}],"upvote_count":"2","timestamp":"1717605120.0","poster":"Gulenur_GS"},{"timestamp":"1717594560.0","poster":"Gulenur","comment_id":"1088603","content":"Answer is E\ndf.select(3*df['heartrate']) returns perfect result without error","upvote_count":"2"},{"upvote_count":"2","content":"Selected Answer: B\nAnswer B","poster":"npc0001","timestamp":"1715241780.0","comment_id":"1066322"},{"content":"Answer is B","upvote_count":"2","timestamp":"1715205300.0","comment_id":"1066034","poster":"Dileepvikram"},{"timestamp":"1713264840.0","upvote_count":"2","content":"Selected Answer: B\nNo such column found","poster":"sturcu","comment_id":"1044884"}],"unix_timestamp":1694501220,"exam_id":163,"timestamp":"2023-09-12 08:47:00","topic":"1","answer_images":[],"question_text":"Review the following error traceback:\n//IMG//\n\nWhich statement describes the error being raised?","answers_community":["B (75%)","E (25%)"],"question_id":149,"question_images":["https://img.examtopics.com/certified-data-engineer-professional/image26.png"],"answer":"B","isMC":true,"answer_description":"","url":"https://www.examtopics.com/discussions/databricks/view/120580-exam-certified-data-engineer-professional-topic-1-question/","answer_ET":"B"},{"id":"84GCbbeykbwleCQ4zbKb","url":"https://www.examtopics.com/discussions/databricks/view/128879-exam-certified-data-engineer-professional-topic-1-question/","timestamp":"2023-12-18 08:29:00","answer_images":[],"isMC":true,"question_id":150,"answer":"D","answers_community":["D (100%)"],"answer_description":"","topic":"1","question_text":"Which distribution does Databricks support for installing custom Python code packages?","choices":{"D":"jars","B":"CRANC. npm","A":"sbt","C":"Wheels"},"exam_id":163,"discussion":[{"timestamp":"1729231560.0","content":"Selected Answer: D\nI think D is correct","upvote_count":"1","comment_id":"1299558","poster":"benni_ale"},{"content":"Selected Answer: D\nhttps://learn.microsoft.com/en-us/azure/databricks/workflows/jobs/how-to/use-python-wheels-in-workflows","upvote_count":"4","timestamp":"1708948740.0","comment_id":"1159649","poster":"hal2401me"},{"comment_id":"1099944","timestamp":"1702923540.0","upvote_count":"1","content":"Selected Answer: D\nhttps://learn.microsoft.com/en-us/azure/databricks/workflows/jobs/how-to/use-python-wheels-in-workflows","poster":"sodere"},{"timestamp":"1702884540.0","content":"Selected Answer: D\nWheels should be ok","poster":"alexvno","comment_id":"1099478","upvote_count":"2"}],"answer_ET":"D","unix_timestamp":1702884540,"question_images":[]}],"exam":{"provider":"Databricks","isImplemented":true,"isBeta":false,"id":163,"lastUpdated":"12 Apr 2025","numberOfQuestions":200,"isMCOnly":true,"name":"Certified Data Engineer Professional"},"currentPage":30},"__N_SSP":true}