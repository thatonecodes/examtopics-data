{"pageProps":{"questions":[{"id":"vIQeNL4iJPBCNG5xpA5x","answers_community":[],"discussion":[{"content":"The correct answer is:\n\nE. The wrong SQL function is used to compute column result — it should be ASSESS_PERFORMANCE instead of assessPerformance.\n\nExplanation:\nWhen registering a UDF with spark.udf.register(), the name provided (e.g., \"ASSESS_PERFORMANCE\") is the name used in SQL queries.\nIn the SQL statement, ASSESS_PERFORMANCE should be used to invoke the registered UDF, not the original Python function name assessPerformance.","upvote_count":"1","timestamp":"1737785220.0","poster":"Souvik_79","comment_id":"1346348"},{"timestamp":"1728576120.0","comment_id":"1193142","poster":"Sowwy1","upvote_count":"1","content":"E. The wrong SQL function is used to compute column result — it should be ASSESS_PERFORMANCE instead of assessPerformance."}],"answer":"E","answer_description":"","choices":{"C":"The customerSatisfaction column cannot be called twice inside the SQL statement.","B":"The order of the arguments to spark.udf.register() should be reversed.","D":"Registered UDFs cannot be applied inside of a SQL statement.","E":"The wrong SQL function is used to compute column result — it should be ASSESS_PERFORMANCE instead of assessPerformance.","A":"There is no sql() operation — the DataFrame API must be used to apply the UDF assessPerformance()."},"answer_ET":"E","question_text":"The code block shown below contains an error. The code block is intended to create and register a SQL UDF named \"ASSESS_PERFORMANCE\" using the Python function assessPerformance() and apply it to column customerSatistfaction in table stores. Identify the error.\n\nCode block:\n\nspark.udf.register(\"ASSESS_PERFORMANCE\", assessPerformance)\nspark.sql(\"SELECT customerSatisfaction, assessPerformance(customerSatisfaction) AS result FROM stores\")","question_images":[],"exam_id":161,"answer_images":[],"timestamp":"2024-04-10 18:02:00","url":"https://www.examtopics.com/discussions/databricks/view/138367-exam-certified-associate-developer-for-apache-spark-topic-1/","question_id":81,"topic":"1","isMC":true,"unix_timestamp":1712764920},{"id":"OcaICLslRfr2uJ2s4gN0","choices":{"C":"storesDF.cache().count()","B":"storesDF.persist().count()","E":"storesDF.persist(\"MEMORY_ONLY\").count()","A":"storesDF.cache(StorageLevel.MEMORY_ONLY).count()","D":"storesDF.persist(StorageLevel.MEMORY_ONLY).count()"},"answer_description":"","unix_timestamp":1712764980,"answer_ET":"D","timestamp":"2024-04-10 18:03:00","answers_community":["D (100%)"],"discussion":[{"comment_id":"1342952","poster":"bp_a_user","upvote_count":"1","timestamp":"1737282360.0","content":"Selected Answer: D\ntrue\nExample from spec\ndf.spark.persist(pyspark.StorageLevel.MEMORY_ONLY)"},{"timestamp":"1728576180.0","comment_id":"1193143","poster":"Sowwy1","upvote_count":"1","content":"D. storesDF.persist(StorageLevel.MEMORY_ONLY).count()"}],"exam_id":161,"question_id":82,"topic":"1","isMC":true,"question_text":"Which of the following code blocks attempts to cache the partitions of DataFrame storesDF only in Spark’s memory?","answer_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/138368-exam-certified-associate-developer-for-apache-spark-topic-1/","answer":"D","question_images":[]},{"id":"p2kKQzxUfuObSe0rrnUt","answer_ET":"C","timestamp":"2024-04-10 18:03:00","discussion":[{"content":"The correct answer is:\n\nC. storesDF.repartition()\n\nExplanation:\nrepartition(): This operation induces a shuffle and creates a new DataFrame with the specified number of partitions. It is used when you want to increase or decrease the number of partitions in a DataFrame, and it always triggers a shuffle to evenly distribute data across the partitions.","timestamp":"1737785280.0","comment_id":"1346349","poster":"Souvik_79","upvote_count":"1"},{"timestamp":"1728576180.0","content":"C. storesDF.repartition()","comment_id":"1193145","upvote_count":"1","poster":"Sowwy1"}],"topic":"1","answer":"C","exam_id":161,"url":"https://www.examtopics.com/discussions/databricks/view/138370-exam-certified-associate-developer-for-apache-spark-topic-1/","question_id":83,"answer_images":[],"isMC":true,"question_images":[],"choices":{"A":"storesDF.coalesce()","D":"storesDF.union()","C":"storesDF.repartition()","E":"storesDF.intersect()","B":"storesDF.rdd.getNumPartitions()"},"answers_community":[],"answer_description":"","question_text":"Which of the following operations will always return a new DataFrame with updated partitions from DataFrame storesDF by inducing a shuffle?","unix_timestamp":1712764980},{"id":"WbNJ74E7i8axvvBEHgzc","discussion":[{"comment_id":"1342953","content":"Selected Answer: D\nDi is correct\nThe spec says, that the parameter of month requires a target \"date/timestamp column to work on\".","timestamp":"1737282480.0","upvote_count":"1","poster":"bp_a_user"},{"timestamp":"1728576240.0","poster":"Sowwy1","upvote_count":"1","comment_id":"1193146","content":"D. (storesDF.withColumn(\"openTimestamp\", col(\"openDate\").cast(\"Timestamp\"))\n.withColumn(\"month\", month(col(\"openTimestamp\"))))"}],"topic":"1","answer":"D","unix_timestamp":1712765040,"answer_images":[],"question_id":84,"answer_ET":"D","question_images":["https://img.examtopics.com/certified-associate-developer-for-apache-spark/image20.png"],"url":"https://www.examtopics.com/discussions/databricks/view/138371-exam-certified-associate-developer-for-apache-spark-topic-1/","choices":{"C":"(storesDF.withColumn(\"openDateFormat\", col(\"openDate\").cast(\"Date\"))\n.withColumn(\"month\", month(col(\"openDateFormat\"))))","B":"storesDF.withColumn(\"month\", substr(col(\"openDate\"), 4, 2))","A":"storesDF.withColumn(\"month\", getMonth(col(\"openDate\")))","E":"storesDF.withColumn(\"month\", month(col(\"openDate\")))","D":"(storesDF.withColumn(\"openTimestamp\", col(\"openDate\").cast(\"Timestamp\"))\n.withColumn(\"month\", month(col(\"openTimestamp\"))))"},"exam_id":161,"isMC":true,"timestamp":"2024-04-10 18:04:00","question_text":"Which of the following code blocks returns a DataFrame containing a column month, an integer representation of the month from column openDate from DataFrame storesDF?\n\nNote that column openDate is of type integer and represents a date in the UNIX epoch format — the number of seconds since midnight on January 1 st, 1970.\n\nA sample of storesDF is displayed below:\n\n//IMG//","answer_description":"","answers_community":["D (100%)"]},{"id":"CVpZ5Obe4MU6mGEumauX","exam_id":161,"isMC":true,"answers_community":["E (100%)"],"answer_description":"","discussion":[{"poster":"Souvik_79","content":"Selected Answer: E\nThe correct answer is:\n\nE. 1. spark 2. read 3. load 4. filePath\n\nExplanation:\nspark.read: This creates a DataFrameReader object for reading data.\nload(): This method is used to load data from a file, and when the file is in Parquet format, it can automatically infer the format without explicitly specifying it.","upvote_count":"1","timestamp":"1737785340.0","comment_id":"1346352"},{"timestamp":"1720790700.0","content":"Selected Answer: E\nIt's E. read is a property of spark, not a method, so that makes the choice C or E. In C the parameter source is being assigned \"parquet\". Parquet is the format. So C is wrong. E is correct, because the default format is parquet.","upvote_count":"1","poster":"5effea7","comment_id":"1246774"},{"poster":"Sowwy1","upvote_count":"2","timestamp":"1712765040.0","content":"E. 1. spark\n2. read\n3. load\n4. filePath","comment_id":"1193147"},{"upvote_count":"2","content":"Selected Answer: E\nTo me E is most likely correct\n\nSyntax: spark.read.load(filePath)","timestamp":"1707519660.0","poster":"azure_bimonster","comment_id":"1145846"}],"url":"https://www.examtopics.com/discussions/databricks/view/133473-exam-certified-associate-developer-for-apache-spark-topic-1/","answer_images":[],"answer":"E","timestamp":"2024-02-10 00:01:00","question_images":[],"question_text":"The code block shown below should read a parquet at the file path filePath into a DataFrame. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.\n\nCode block:\n__1__.__2__.__3__(__4__)","answer_ET":"E","unix_timestamp":1707519660,"question_id":85,"choices":{"C":"1. spark\n2. read\n3. load\n4. filePath, source = \"parquet\"","A":"1.spark\n2. read()\n3. parquet\n4. filePath","B":"1. spark\n2. read()\n3. load\n4. filePath","D":"1. storesDF\n2. read()\n3. load\n4. filePath","E":"1. spark\n2. read\n3. load\n4. filePath"},"topic":"1"}],"exam":{"numberOfQuestions":185,"isMCOnly":true,"lastUpdated":"12 Apr 2025","name":"Certified Associate Developer for Apache Spark","isImplemented":true,"id":161,"isBeta":false,"provider":"Databricks"},"currentPage":17},"__N_SSP":true}