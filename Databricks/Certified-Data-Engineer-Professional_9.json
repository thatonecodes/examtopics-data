{"pageProps":{"questions":[{"id":"ntXzzV0x7LobKSuTWfDf","question_id":41,"isMC":true,"unix_timestamp":1717054200,"discussion":[{"content":"Selected Answer: D\nCorrect Answer: D\nCorrect because this option correctly describes the behavior of SQL views in Databricks. The view's query is executed against the current state of the data in the source tables at the moment the query begins. This means that any changes to the data that are committed while the query is running will not be reflected in the results of the query currently executing.","comment_id":"1222889","poster":"Freyr","timestamp":"1717273500.0","upvote_count":"5"},{"poster":"benni_ale","comment_id":"1307257","upvote_count":"2","timestamp":"1730795880.0","content":"Selected Answer: D\nits D u scums"},{"comment_id":"1226817","timestamp":"1717862640.0","upvote_count":"1","poster":"Isio05","content":"Selected Answer: D\nAlso voting for D, such view results are recalculated each time when called"},{"poster":"MDWPartners","comment_id":"1221438","timestamp":"1717054200.0","content":"Selected Answer: B\nIt should be B","upvote_count":"1"}],"answers_community":["D (89%)","11%"],"answer_description":"","answer_ET":"D","answer_images":[],"topic":"1","choices":{"D":"All logic will execute at query time and return the result of joining the valid versions of the source tables at the time the query began.","B":"Results will be computed and cached when the view is defined; these cached results will incrementally update as new records are inserted into source tables.","A":"All logic will execute when the view is defined and store the result of joining tables to the DBFS; this stored data will be returned when the view is queried.","C":"All logic will execute at query time and return the result of joining the valid versions of the source tables at the time the query finishes."},"timestamp":"2024-05-30 09:30:00","answer":"D","question_images":["https://img.examtopics.com/certified-data-engineer-professional/image61.png"],"exam_id":163,"url":"https://www.examtopics.com/discussions/databricks/view/141585-exam-certified-data-engineer-professional-topic-1-question/","question_text":"A view is registered with the following code:\n\n//IMG//\n\n\nBoth users and orders are Delta Lake tables.\n\nWhich statement describes the results of querying recent_orders?"},{"id":"sdTAJVROJYfRIR3gv1nL","url":"https://www.examtopics.com/discussions/databricks/view/141745-exam-certified-data-engineer-professional-topic-1-question/","choices":{"D":"Ingest the data, execute the narrow transformations, repartition to 2,048 partitions (1TB* 1024*1024/512), and then write to parquet.","C":"Set spark.sql.adaptive.advisoryPartitionSizeInBytes to 512 MB bytes, ingest the data, execute the narrow transformations, coalesce to 2,048 partitions (1TB*1024*1024/512), and then write to parquet.","A":"Set spark.sql.files.maxPartitionBytes to 512 MB, ingest the data, execute the narrow transformations, and then write to parquet.","B":"Set spark.sql.shuffle.partitions to 2,048 partitions (1TB*1024*1024/512), ingest the data, execute the narrow transformations, optimize the data by sorting it (which automatically repartitions the data), and then write to parquet."},"topic":"1","answer":"A","answer_images":[],"answer_description":"","isMC":true,"answer_ET":"A","answers_community":["A (52%)","D (38%)","10%"],"question_text":"A data ingestion task requires a one-TB JSON dataset to be written out to Parquet with a target part-file size of 512 MB. Because Parquet is being used instead of Delta Lake, built-in file-sizing features such as Auto-Optimize & Auto-Compaction cannot be used.\n\nWhich strategy will yield the best performance without shuffling data?","discussion":[{"comment_id":"1341591","poster":"RandomForest","upvote_count":"1","timestamp":"1737019200.0","content":"Selected Answer: D\nCorrect answer is D:\nWhy Not Other Options?:\nA. Set spark.sql.files.maxPartitionBytes: This configuration controls how many bytes Spark reads per input partition during a file scan, not the output file size. It does not help in controlling Parquet file sizes during writing.\nB. Set spark.sql.shuffle.partitions and sort data: While sorting data can optimize performance in some cases, it introduces unnecessary overhead for this scenario. Additionally, spark.sql.shuffle.partitions controls the number of shuffle partitions, not directly the output partitioning of the data.\nC. Use spark.sql.adaptive.advisoryPartitionSizeInBytes: Adaptive Query Execution (AQE) optimizes queries at runtime, but this configuration does not directly control Parquet file sizes. It dynamically adjusts partition sizes for shuffle stages, not for the write output."},{"upvote_count":"1","comment_id":"1339918","poster":"_lene_","timestamp":"1736778000.0","content":"Selected Answer: A\narekm explanation"},{"poster":"arekm","upvote_count":"3","timestamp":"1735817940.0","comment_id":"1335531","content":"Selected Answer: A\nDefinitely A - no repartitioning and subsequent shuffle (which the question is asking about). The parameter defines how many bytes per partition to read, tasks will read in those chunks, since only narrow operations performed (per definition - no shuffle), we just write what we read. The target files size is 512MBs and we did not shuffle."},{"upvote_count":"1","poster":"temple1305","content":"Selected Answer: C\nI think, \"execute the narrow transformations, coalesce to\" is key words here - because coalesce is not cause shuffling.","timestamp":"1733149200.0","comment_id":"1320978"},{"content":"Selected Answer: D\nIt's D, because A primarily affects the reading of the data","timestamp":"1731331080.0","upvote_count":"2","poster":"cf56faf","comment_id":"1310073"},{"timestamp":"1729759680.0","upvote_count":"2","poster":"Jugiboss","content":"Selected Answer: A\nA does not shuffle while D shuffles","comment_id":"1302374"},{"comment_id":"1300477","content":"Selected Answer: A\nAnswer A as narrow transformations like union, filter and map do not cause shuffle across partitions.","poster":"m79590530","timestamp":"1729430940.0","upvote_count":"2"},{"timestamp":"1728961440.0","upvote_count":"1","comment_id":"1297926","comments":[{"poster":"arekm","timestamp":"1735818000.0","comment_id":"1335532","content":"All true, but not a correct answer. We are looking for a solution without shuffle/repartition/coalesce.","upvote_count":"1"}],"poster":"Colje","content":"Selected Answer: D\nThe correct answer is D. Ingest the data, execute the narrow transformations, repartition to 2,048 partitions (1TB * 1024 * 1024 / 512), and then write to Parquet.\n\nExplanation:\nIn this case, the goal is to write a 1 TB dataset to Parquet with a target file size of 512 MB without incurring the overhead of data shuffling. To achieve optimal performance, we must balance the number of partitions to match the file size requirements while avoiding expensive shuffle operations.\n\nNarrow transformations: These transformations (such as map, filter) donâ€™t require shuffling the data, which keeps the operation efficient.\nRepartition to 2,048 partitions: Given that the desired part-file size is 512 MB and the total dataset size is 1 TB, repartitioning the dataset into 2,048 partitions ensures that each partition will be approximately 512 MB in size, which matches the target file size. This avoids shuffle operations and allows for an efficient write."},{"content":"Selected Answer: D\nNot A because spark.sql.files.maxPartitionBytes primarily affects the reading of data, not the writing. It determines the maximum size of a partition when reading files, not when writing them.","upvote_count":"2","timestamp":"1727268960.0","comment_id":"1288996","poster":"pk07"},{"comment_id":"1288267","poster":"shaojunni","timestamp":"1727114700.0","upvote_count":"1","content":"Selected Answer: C\nA, D will not prevent shuffling data. C using coalesce to reduce shuffling data."},{"comment_id":"1237812","upvote_count":"3","timestamp":"1719449340.0","content":"Selected Answer: A\nbest performance without shuffling data","poster":"03355a2"},{"content":"Selected Answer: D\noption D","poster":"hpkr","timestamp":"1718207160.0","comment_id":"1229282","upvote_count":"1"},{"poster":"Freyr","timestamp":"1717273920.0","upvote_count":"1","comment_id":"1222892","content":"Selected Answer: D\nCorrect Answer D: Repartition to 2,048 partitions and write to Parquet\n\nThis option directly controls the number of output files by repartitioning the data into 2,048 partitions, assuming that 1TB/512MB per file roughly translates to 2,048 files. Repartitioning the data involves shuffling, but it's a deliberate shuffle designed to achieve a specific partitioning beneficial for writing. After repartitioning, the data is written to Parquet files, each expected to be approximately 512 MB if the data is uniformly distributed across partitions."}],"exam_id":163,"unix_timestamp":1717273920,"question_id":42,"question_images":[],"timestamp":"2024-06-01 22:32:00"},{"id":"Wrj7BXqh47XQHk2pI9Lh","answer_description":"","exam_id":163,"question_text":"Which statement regarding stream-static joins and static Delta tables is correct?","choices":{"B":"Each microbatch of a stream-static join will use the most recent version of the static Delta table as of the job's initialization.","D":"Stream-static joins cannot use static Delta tables because of consistency issues.","C":"The checkpoint directory will be used to track state information for the unique keys present in the join.","A":"The checkpoint directory will be used to track updates to the static Delta table."},"question_id":43,"isMC":true,"url":"https://www.examtopics.com/discussions/databricks/view/141554-exam-certified-data-engineer-professional-topic-1-question/","discussion":[{"timestamp":"1735818360.0","upvote_count":"2","comment_id":"1335534","poster":"arekm","content":"Selected Answer: B\nAll answers are wrong:\nA - checkpoint directory to track changes to Delta table?\nB - microbatch uses the state of the table at the time the query is executed, not at initialization\nC - unique keys? - stream-static joins are not stateful, so we are only looking at the current batch of records\nD - you can totally have stream-static joins, see: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#support-matrix-for-joins-in-streaming-queries\n\nI believe they made a typo in the B, that seems to be the only logical explanation."},{"poster":"benni_ale","timestamp":"1730880420.0","content":"Selected Answer: A\nIf you look at question 18 you find that the correct solution should be Each microbatch of a stream-static join will use the most recent version of the static Delta table as of each microbatch. This is not listed here meaning that B could not be correct leading to A being the only possible solution.... The wrong part about B is that the latest version of the static delta table is returned at each micro-batch rather than as of job initialisation...","comment_id":"1307707","upvote_count":"1"},{"poster":"MDWPartners","content":"Selected Answer: B\nWhen Databricks processes a micro-batch of data in a stream-static join, the latest valid version of data from the static Delta table joins with the records present in the current micro-batch. Because the join is stateless, you do not need to configure watermarking and can process results with low latency. The data in the static Delta table used in the join should be slowly-changing.\nhttps://docs.databricks.com/en/transform/join.html#stream-static","timestamp":"1717003440.0","upvote_count":"3","comment_id":"1221151"}],"unix_timestamp":1717003440,"topic":"1","answer_ET":"B","timestamp":"2024-05-29 19:24:00","question_images":[],"answer_images":[],"answers_community":["B (83%)","A (17%)"],"answer":"B"},{"id":"a6wMp9vjIQJJsNXzy9WN","question_text":"A junior data engineer has been asked to develop a streaming data pipeline with a grouped aggregation using DataFrame df. The pipeline needs to calculate the average humidity and average temperature for each non-overlapping five-minute interval. Events are recorded once per minute per device.\n\nStreaming DataFrame df has the following schema:\n\n\"device_id INT, event_time TIMESTAMP, temp FLOAT, humidity FLOAT\"\n\nCode block:\n\n//IMG//\n\n\nWhich line of code correctly fills in the blank within the code block to complete this task?","answer_images":[],"exam_id":163,"answers_community":["B (100%)"],"topic":"1","answer":"B","question_id":44,"url":"https://www.examtopics.com/discussions/databricks/view/149854-exam-certified-data-engineer-professional-topic-1-question/","discussion":[{"timestamp":"1737019440.0","content":"Selected Answer: B\nCorrect answer is B: use the window-function","poster":"RandomForest","upvote_count":"1","comment_id":"1341594"},{"content":"Selected Answer: B\nThis is the standard syntax to do non-overlapping time interval Window-ed grouping by the time field in a dataset in Structured Streaming. .withWatermatk() function defines the staging buffers after which delayed records will be dropped/ignored.","poster":"m79590530","upvote_count":"2","comment_id":"1300497","timestamp":"1729433100.0"}],"unix_timestamp":1729433100,"question_images":["https://img.examtopics.com/certified-data-engineer-professional/image62.png"],"timestamp":"2024-10-20 16:05:00","choices":{"C":"\"event_time\"","B":"window(\"event_time\", \"5 minutes\").alias(\"time\")","D":"lag(\"event_time\", \"10 minutes\").alias(\"time\")","A":"to_interval(\"event_time\", \"5 minutes\").alias(\"time\")"},"isMC":true,"answer_description":"","answer_ET":"B"},{"id":"l7nbfkY1EoyT51MvKbJj","isMC":true,"answer_description":"","discussion":[{"content":"Selected Answer: C\nC, \nA - incorrect explanation\nB - trigger once is not correct option here\nD - 500 miliseconds is already used, it's default trigger interval","comment_id":"1230601","timestamp":"1718388060.0","poster":"Isio05","upvote_count":"4"},{"poster":"hpkr","timestamp":"1718207400.0","comment_id":"1229289","content":"Selected Answer: C\nOption C","upvote_count":"1"}],"timestamp":"2024-06-12 17:50:00","exam_id":163,"topic":"1","choices":{"D":"Set the trigger interval to 500 milliseconds; setting a small but non-zero trigger interval ensures that the source is not queried too frequently.","B":"Use the trigger once option and configure a Databricks job to execute the query every 10 minutes; this approach minimizes costs for both compute and storage.","C":"Set the trigger interval to 10 minutes; each batch calls APIs in the source storage account, so decreasing trigger frequency to maximum allowable threshold should minimize this cost.","A":"Set the trigger interval to 3 seconds; the default trigger interval is consuming too many records per batch, resulting in spill to disk that can increase volume costs."},"question_text":"A Structured Streaming job deployed to production has been resulting in higher than expected cloud storage costs. At present, during normal execution, each microbatch of data is processed in less than 3s; at least 12 times per minute, a microbatch is processed that contains 0 records. The streaming write was configured using the default trigger settings. The production job is currently scheduled alongside many other Databricks jobs in a workspace with instance pools provisioned to reduce start-up time for jobs with batch execution.\n\nHolding all other variables constant and assuming records need to be processed in less than 10 minutes, which adjustment will meet the requirement?","answer":"C","url":"https://www.examtopics.com/discussions/databricks/view/142392-exam-certified-data-engineer-professional-topic-1-question/","question_images":[],"answer_images":[],"answers_community":["C (100%)"],"question_id":45,"unix_timestamp":1718207400,"answer_ET":"C"}],"exam":{"isBeta":false,"id":163,"isMCOnly":true,"numberOfQuestions":200,"provider":"Databricks","isImplemented":true,"lastUpdated":"12 Apr 2025","name":"Certified Data Engineer Professional"},"currentPage":9},"__N_SSP":true}