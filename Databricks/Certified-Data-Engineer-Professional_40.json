{"pageProps":{"questions":[{"id":"EdGJq4N8UhPRHxSUeFbX","answer_description":"","answer":"A","answers_community":["A (54%)","E (46%)"],"question_text":"A task orchestrator has been configured to run two hourly tasks. First, an outside system writes Parquet data to a directory mounted at /mnt/raw_orders/. After this data is written, a Databricks job containing the following code is executed:\n\n//IMG//\n\n\nAssume that the fields customer_id and order_id serve as a composite key to uniquely identify each order, and that the time field indicates when the record was queued in the source system.\n\nIf the upstream system is known to occasionally enqueue duplicate entries for a single order hours apart, which statement is correct?","choices":{"C":"The orders table will contain only the most recent 2 hours of records and no duplicates will be present.","E":"The orders table will not contain duplicates, but records arriving more than 2 hours late will be ignored and missing from the table.","A":"Duplicate records enqueued more than 2 hours apart may be retained and the orders table may contain duplicate records with the same customer_id and order_id.","D":"Duplicate records arriving more than 2 hours apart will be dropped, but duplicates that arrive in the same batch may both be written to the orders table.","B":"All records will be held in the state store for 2 hours before being deduplicated and committed to the orders table."},"question_id":196,"url":"https://www.examtopics.com/discussions/databricks/view/128995-exam-certified-data-engineer-professional-topic-1-question/","exam_id":163,"answer_images":[],"topic":"1","timestamp":"2023-12-19 08:28:00","answer_ET":"A","question_images":["https://img.examtopics.com/certified-data-engineer-professional/image45.png"],"unix_timestamp":1702970880,"discussion":[{"timestamp":"1702970880.0","upvote_count":"8","content":"Selected Answer: A\nOnly A seems logical","comment_id":"1100408","poster":"alexvno"},{"timestamp":"1717847580.0","content":"Selected Answer: A\nIt's A, rows are deduplicated only in 2hrs window, therefore final table may eventually contain duplicates","upvote_count":"5","comment_id":"1226718","poster":"Isio05"},{"comment_id":"1441690","content":"Selected Answer: E\n- The code snippet uses **`withWatermark(\"time\", \"2 hours\")`**, which means that Spark will only process records with a timestamp (`time` field) that falls within a 2-hour window from the most recent watermark. Any records arriving **more than 2 hours late** will be ignored and won't be written to the `orders` table.\n- The **`dropDuplicates([\"customer_id\", \"order_id\"])`** ensures that duplicate records (based on the composite key of `customer_id` and `order_id`) are removed **within the 2-hour window**. This guarantees no duplicates will be present in the `orders` table for data falling within the watermark range.","upvote_count":"1","poster":"a85becd","timestamp":"1743727440.0"},{"upvote_count":"2","comment_id":"1411620","content":"Selected Answer: E\nE. The orders table will not contain duplicates, but records arriving more than 2 hours late will be ignored and missing from the table.\nThis matches Spark’s actual behavior with watermark + dropDuplicates: the stateful deduplication ensures no duplicate (customer_id, order_id) rows are ever written to the Delta table, and the 2-hour watermark means any record that comes in more than 2 hours behind the stream’s current event-time will be dropped (i.e. not processed/output)","timestamp":"1743227880.0","poster":"EZZALDIN"},{"timestamp":"1736930340.0","content":"Selected Answer: A\nA is correct. The watermark ensures that data arriving up to 2h late will be deduplicated in the incoming batch.","upvote_count":"1","poster":"RandomForest","comment_id":"1340742"},{"upvote_count":"1","content":"Selected Answer: A\nA - since the same [customer_id, order_id] record can be emitted twice with the \"time\" value set apart by more than 2 hours. The target table will then contain duplicates (duplicates from the perspective of the composite key).","poster":"arekm","timestamp":"1735810500.0","comment_id":"1335452"},{"comment_id":"1315290","poster":"benni_ale","timestamp":"1732111500.0","upvote_count":"1","content":"Per me è la cipolla"},{"poster":"cf56faf","upvote_count":"2","timestamp":"1731322680.0","comment_id":"1309965","content":"Selected Answer: E\nSeems that E should be the correct answer. \nAs time is the time it was queued in the *source_system*. \n\nAnd withWatermark ignores records that have a \"time\" more than 2 hours old."},{"comment_id":"1303304","upvote_count":"2","poster":"Ananth4Sap","content":"When watermarking is set to 2 hours, the system will wait for up to 2 hours for late data to arrive. Any data that arrives within this 2-hour window will be considered for processing and deduplication. However, data that arrives later than 2 hours after the event time will be considered too late and will be discarded. This ensures that the state store does not grow indefinitely, but it also means that any records arriving more than 2 hours late will not be included in the orders table.","timestamp":"1729953240.0"},{"comment_id":"1299924","upvote_count":"2","timestamp":"1729322520.0","comments":[{"upvote_count":"2","comment_id":"1335449","timestamp":"1735810380.0","content":"Deduplication happens each hour (when the job is run) and contains records from the last 2 hours based on the time column.\n\nHowever, the source system might generate [customer_id, order_id] combination with a newer time value set apart more than 2 hours from the original record. It will be a new record, however from the key perspective a duplicate. Since deduplication happens in 2h windows, the new record will be appended (default for structured streaming) to the target table.","poster":"arekm"}],"poster":"m79590530","content":"Selected Answer: E\nEvery Stream micro-batch is executed on all of the new data that arrived after the last run 2 hours ago by the .trigger(once=True) option. Deduplication is done for it based on the combined key fields values but all records older than 2 hours based on the 'time' field will be ignored thanks to the .withWatermark() option/function. So target table will have deduplicated data withOUT the late records arriving more than 2 hours later based on the 2 hours watermark buffer set for the readStream."},{"poster":"shaojunni","comment_id":"1296611","upvote_count":"2","content":"Selected Answer: E\nData arrive outside of watermark will be dropped.","timestamp":"1728760560.0"},{"content":"Selected Answer: E\nWatermark(\"time\", \"2 hours\") --> does'nt let records arriving more than 2 hours late to be written\ndropDuplicates --> removes duplicate records from the records that are read","timestamp":"1723534920.0","poster":"quaternion","upvote_count":"3","comment_id":"1265048"},{"poster":"QuangTrinh","content":"Selected Answer: E\nShould be E.\n\nWatermarking (withWatermark(\"time\", \"2 hours\")): This sets a 2-hour watermark on the time column. The watermark specifies the event time threshold for data completeness, meaning that data older than 2 hours will be considered late and may be dropped.\nDeduplication (dropDuplicates([\"customer_id\", \"order_id\"])): This operation removes duplicates based on the composite key (customer_id and order_id). However, it only works within the window defined by the watermark.","upvote_count":"1","comments":[{"timestamp":"1735810440.0","comment_id":"1335450","upvote_count":"2","content":"Deduplication happens each hour (when the job is run) and contains records from the last 2 hours based on the time column. However, the source system might generate [customer_id, order_id] combination with a newer time value set apart more than 2 hours from the original record. It will be a new record, however from the key perspective a duplicate. Since deduplication happens in 2h windows, the new record will be appended (default for structured streaming) to the target table.","poster":"arekm"}],"comment_id":"1225115","timestamp":"1717641420.0"}],"isMC":true},{"id":"AmykmnppUMNnrpjPxD8b","discussion":[{"content":"Selected Answer: D\nIn Databricks Delta Lake, transactions are ACID compliant at the table level, meaning that transactions apply to a single table. However, Delta Lake does not enforce foreign key constraints across tables. Therefore, the data engineer needs to be aware that Databricks does not automatically enforce referential integrity between tables through foreign key constraints, and it becomes the responsibility of the data engineer to manage these relationships appropriately.","timestamp":"1722898020.0","comment_id":"1141650","upvote_count":"6","poster":"vctrhugo"},{"content":"Selected Answer: D\nPrimary and foreign keys are informational only and are not enforced.","comment_id":"1100412","upvote_count":"2","poster":"alexvno","timestamp":"1718775120.0"},{"comment_id":"1071440","content":"Selected Answer: D\nD makes more sense. \nSince there are no database-level transactions, locks, or guarantees, and since primary key & foreign key constraints are informational only, there is no guarantee of enforced relations (the start schema) in place will remain in place after migration. This means B cannot be right.","timestamp":"1715769660.0","poster":"60ties","upvote_count":"1"}],"timestamp":"2023-11-15 13:41:00","question_id":197,"exam_id":163,"topic":"1","answer_description":"","unix_timestamp":1700052060,"answer":"D","choices":{"A":"Databricks only allows foreign key constraints on hashed identifiers, which avoid collisions in highly-parallel writes.","E":"Foreign keys must reference a primary key field; multi-table inserts must leverage Delta Lake’s upsert functionality.","B":"Databricks supports Spark SQL and JDBC; all logic can be directly migrated from the source system without refactoring.","C":"Committing to multiple tables simultaneously requires taking out multiple table locks and can lead to a state of deadlock.","D":"All Delta Lake transactions are ACID compliant against a single table, and Databricks does not enforce foreign key constraints."},"answer_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/126259-exam-certified-data-engineer-professional-topic-1-question/","question_images":[],"isMC":true,"answer_ET":"D","question_text":"A junior data engineer is migrating a workload from a relational database system to the Databricks Lakehouse. The source system uses a star schema, leveraging foreign key constraints and multi-table inserts to validate records on write.\n\nWhich consideration will impact the decisions made by the engineer while migrating this workload?","answers_community":["D (100%)"]},{"id":"iVvTjn6OSiwkTMSyZHVu","answer_description":"","unix_timestamp":1700052240,"choices":{"D":"Delta Lake time travel does not scale well in cost or latency to provide a long-term versioning solution.","E":"Delta Lake only supports Type 0 tables; once records are inserted to a Delta Lake table, they cannot be modified.","A":"Data corruption can occur if a query fails in a partially completed state because Type 2 tables require setting multiple fields in a single update.","C":"Delta Lake time travel cannot be used to query previous versions of these tables because Type 1 changes modify data files in place.","B":"Shallow clones can be combined with Type 1 tables to accelerate historic queries for long-term versioning."},"answers_community":["D (100%)"],"answer_images":[],"answer":"D","isMC":true,"question_images":[],"exam_id":163,"topic":"1","discussion":[{"comment_id":"1339543","upvote_count":"1","timestamp":"1736692140.0","poster":"_lene_","content":"Selected Answer: D\nVacuuming restricts time travel capability"},{"upvote_count":"3","timestamp":"1734111840.0","poster":"Sriramiyer92","comment_id":"1326203","content":"Selected Answer: D\nAlso performing a Vaccum Op on Data will not suite well with the requirement in the long run."},{"poster":"vish9","timestamp":"1731202500.0","comment_id":"1309265","upvote_count":"1","content":"Selected Answer: D\nD makes sense"},{"upvote_count":"1","comment_id":"1295212","timestamp":"1728489660.0","poster":"KB_Ai_Champ","content":"its C time travel cant be performed on type 1"},{"comment_id":"1141649","upvote_count":"4","poster":"vctrhugo","content":"Selected Answer: D\nDelta Lake’s time travel feature allows you to access previous versions of the data, which can be useful for auditing purposes. However, if you’re planning to use time travel as a long-term versioning solution, it’s important to know that it may not scale well in terms of cost or latency. This is because every time you perform a write operation, a new version of the data is created, which can consume significant storage over time. Additionally, querying older versions of the data may require scanning through many files, which can increase query latency.","timestamp":"1707180360.0"},{"timestamp":"1700052240.0","content":"Selected Answer: D\nD makes more sense","upvote_count":"4","poster":"60ties","comment_id":"1071443"}],"url":"https://www.examtopics.com/discussions/databricks/view/126260-exam-certified-data-engineer-professional-topic-1-question/","answer_ET":"D","question_id":198,"timestamp":"2023-11-15 13:44:00","question_text":"A data architect has heard about Delta Lake’s built-in versioning and time travel capabilities. For auditing purposes, they have a requirement to maintain a full record of all valid street addresses as they appear in the customers table.\n\nThe architect is interested in implementing a Type 1 table, overwriting existing records with new values and relying on Delta Lake time travel to support long-term auditing. A data engineer on the project feels that a Type 2 table will provide better performance and scalability.\n\nWhich piece of information is critical to this decision?"},{"id":"OLEAOQTPoyUFbuesNCTS","answer":"A","topic":"1","answer_images":[],"question_images":["https://img.examtopics.com/certified-data-engineer-professional/image46.png"],"discussion":[{"upvote_count":"8","comment_id":"1113061","content":"Selected Answer: A\nDefinitely A. It's greater than or equal to","timestamp":"1704309660.0","poster":"dmov"},{"timestamp":"1738261020.0","upvote_count":"1","poster":"91d511b","comment_id":"1349175","content":"Selected Answer: E\n>=18 means that the analyst that is NOT in the auditing gorup will only see recorders greater than or equal to 18."},{"content":"Selected Answer: A\nA - at first I thought that was a silly answer since 17.5 is still greater than 17, but not really what the condition says. However, if you look at the schema - the age is an integer - you can only have ..., 17, 18, 19, ...","poster":"arekm","timestamp":"1735810920.0","comment_id":"1335457","upvote_count":"1"},{"content":"Selected Answer: A\nSimple - Play of Words in option A and E. Read the option carefully.","timestamp":"1734112260.0","upvote_count":"1","poster":"Sriramiyer92","comment_id":"1326207"},{"comment_id":"1245707","timestamp":"1720639500.0","upvote_count":"1","content":"Selected Answer: A\nBecause greater than 18 doesnt include 18\n\n All columns will be displayed normally for those records that have an age greater than 18; records not meeting this condition will be omitted.","poster":"c00ccb7"},{"content":"Option E","comment_id":"1167555","comments":[{"timestamp":"1716955020.0","content":"Incorrect because the condition specified is age >= 18, not age > 18. So, the answer is A.","comment_id":"1220681","poster":"Freyr","upvote_count":"2"}],"poster":"Tamele001","upvote_count":"1","timestamp":"1709768040.0"},{"content":"Selected Answer: A\nA is correct. \"greater than 17\" is the equivalent to \"equal or greater than 18\"","poster":"divingbell17","comment_id":"1111505","upvote_count":"4","timestamp":"1704153240.0"},{"content":"Selected Answer: E\n18 not 17.","timestamp":"1702966920.0","poster":"sodere","comments":[{"content":"A is right.","timestamp":"1702966980.0","comment_id":"1100372","upvote_count":"3","poster":"sodere"},{"content":">= 18 (greather than 17)","timestamp":"1707180240.0","poster":"vctrhugo","comment_id":"1141646","upvote_count":"3"}],"upvote_count":"1","comment_id":"1100368"}],"timestamp":"2023-12-19 07:22:00","question_text":"A table named user_ltv is being used to create a view that will be used by data analysts on various teams. Users in the workspace are configured into groups, which are used for setting up data access using ACLs.\n\nThe user_ltv table has the following schema:\n\nemail STRING, age INT, ltv INT\n\nThe following view definition is executed:\n\n//IMG//\n\n\nAn analyst who is not a member of the auditing group executes the following query:\n\nSELECT * FROM user_ltv_no_minors\n\nWhich statement describes the results returned by this query?","question_id":199,"url":"https://www.examtopics.com/discussions/databricks/view/128986-exam-certified-data-engineer-professional-topic-1-question/","unix_timestamp":1702966920,"answer_description":"","exam_id":163,"answers_community":["A (88%)","12%"],"choices":{"E":"All columns will be displayed normally for those records that have an age greater than 18; records not meeting this condition will be omitted.","A":"All columns will be displayed normally for those records that have an age greater than 17; records not meeting this condition will be omitted.","C":"All values for the age column will be returned as null values, all other columns will be returned with the values in user_ltv.","D":"All records from all columns will be displayed with the values in user_ltv.","B":"All age values less than 18 will be returned as null values, all other columns will be returned with the values in user_ltv."},"answer_ET":"A","isMC":true},{"id":"qBCT91l3ozzToP7ZwHHN","answer_description":"","isMC":true,"exam_id":163,"question_id":200,"question_text":"The data governance team is reviewing code used for deleting records for compliance with GDPR. The following logic has been implemented to propagate delete requests from the user_lookup table to the user_aggregates table.\n\n//IMG//\n\n\nAssuming that user_id is a unique identifying key and that all users that have requested deletion have been removed from the user_lookup table, which statement describes whether successfully executing the above logic guarantees that the records to be deleted from the user_aggregates table are no longer accessible and why?","answer_ET":"B","unix_timestamp":1700054640,"url":"https://www.examtopics.com/discussions/databricks/view/126268-exam-certified-data-engineer-professional-topic-1-question/","answers_community":["B (100%)"],"choices":{"C":"Yes; the change data feed uses foreign keys to ensure delete consistency throughout the Lakehouse.","D":"Yes; Delta Lake ACID guarantees provide assurance that the DELETE command succeeded fully and permanently purged these records.","B":"No; files containing deleted records may still be accessible with time travel until a VACUUM command is used to remove invalidated data files.","E":"No; the change data feed only tracks inserts and updates, not deleted records.","A":"No; the Delta Lake DELETE command only provides ACID guarantees when combined with the MERGE INTO command."},"discussion":[{"poster":"_lene_","content":"Selected Answer: B\nvacuum prevents delta travel","comment_id":"1339565","upvote_count":"1","timestamp":"1736693940.0"},{"comment_id":"1255590","timestamp":"1721982300.0","poster":"Hadiler","content":"Selected Answer: B\nB records will be available in time travel until VACUUM will be executed","upvote_count":"2"},{"comment_id":"1100421","timestamp":"1702971900.0","content":"Selected Answer: B\nDelta travel","poster":"alexvno","upvote_count":"2"},{"content":"Selected Answer: B\nB is best. \nVACUUM command is needed to completely remove logs of the deleted files.","upvote_count":"2","poster":"60ties","comment_id":"1071471","timestamp":"1700054640.0"}],"timestamp":"2023-11-15 14:24:00","question_images":["https://img.examtopics.com/certified-data-engineer-professional/image47.png"],"answer":"B","topic":"1","answer_images":[]}],"exam":{"isBeta":false,"isMCOnly":true,"lastUpdated":"12 Apr 2025","id":163,"provider":"Databricks","numberOfQuestions":200,"isImplemented":true,"name":"Certified Data Engineer Professional"},"currentPage":40},"__N_SSP":true}