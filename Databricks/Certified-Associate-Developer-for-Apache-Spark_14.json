{"pageProps":{"questions":[{"id":"m2kCUwS9qivuhPJOn40z","answers_community":[],"answer_ET":"B","timestamp":"2024-04-10 16:09:00","topic":"1","discussion":[{"content":"B. assessPerformanceUDF = udf(assessPerformance, IntegerType()) storesDF.withColumn(\"result\", assessPerformanceUDF(col(\"customerSatisfaction\")))","comment_id":"1193051","timestamp":"1728569340.0","upvote_count":"1","poster":"Sowwy1"}],"answer_description":"","exam_id":161,"question_id":66,"answer_images":[],"isMC":true,"answer":"B","question_text":"Which of the following code blocks creates a Python UDF assessPerformanceUDF() using the integer-returning Python function assessPerformance() and applies it to Column customerSatisfaction in DataFrame storesDF?","choices":{"D":"assessPerformanceUDF = udf(assessPerformance) storesDF.withColumn(\"result\", assessPerformanceUDF(col(\"customerSatisfaction\")))","E":"assessPerformanceUDF = udf(assessPerformance, IntegerType()) storesDF.withColumn(\"result\", assessPerformance(col(\"customerSatisfaction\")))","A":"assessPerformanceUDF = udf(assessPerformance, IntegerType) storesDF.withColumn(\"result\", assessPerformanceUDF(col(\"customerSatisfaction\")))","B":"assessPerformanceUDF = udf(assessPerformance, IntegerType()) storesDF.withColumn(\"result\", assessPerformanceUDF(col(\"customerSatisfaction\")))","C":"assessPerformanceUDF - udf(assessPerformance) storesDF.withColumn(\"result\", assessPerformance(col(“customerSatisfaction\")))"},"unix_timestamp":1712758140,"question_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/138353-exam-certified-associate-developer-for-apache-spark-topic-1/"},{"id":"8MX1i45LieT3L1AgMiFB","url":"https://www.examtopics.com/discussions/databricks/view/108391-exam-certified-associate-developer-for-apache-spark-topic-1/","answer_images":[],"question_id":67,"discussion":[{"upvote_count":"6","comment_id":"888435","poster":"4be8126","content":"Selected Answer: A\nThe answer is A. The repartition operation can be used to increase or decrease the number of partitions in a DataFrame. In this case, the number of partitions is being increased from 8 to 12, so we can use the repartition operation with a partition count of 12: df.repartition(12).\n\nOption B, df.cache(), is used to cache a DataFrame in memory for faster access, but it does not change the number of partitions.\n\nOption C, df.partitionBy(1.5), is not a valid operation for partitioning a DataFrame.\n\nOption D, df.coalesce(12), can be used to reduce the number of partitions in a DataFrame, but it cannot be used to increase the number of partitions beyond the current number.\n\nOption E, df.partitionBy(12), is used to partition a DataFrame by a specific column or set of columns, but it does not change the number of partitions.","timestamp":"1714734900.0"},{"content":"Selected Answer: A\nnice explanation @4be8126","upvote_count":"1","comment_id":"1063879","poster":"NuclearGandhi","timestamp":"1730900100.0"},{"content":"Selected Answer: A\nThe operation that can be used to create a new DataFrame with 12 partitions from an original DataFrame df that has 8 partitions is:\n\nD. df.coalesce(12)\n\nExplanation:\n\nThe coalesce() operation in Spark is used to decrease the number of partitions in a DataFrame, and it can be used to create a new DataFrame with a specific number of partitions. In this case, calling df.coalesce(12) on the original DataFrame df with 8 partitions will create a new DataFrame with 12 partitions.","comment_id":"926070","timestamp":"1718636580.0","poster":"TmData","upvote_count":"1"},{"timestamp":"1715753400.0","upvote_count":"1","poster":"SonicBoom10C9","content":"Selected Answer: A\nComprehensive explanation by 4be8126, only using this comment to vote A.","comment_id":"898075"}],"exam_id":161,"timestamp":"2023-05-03 13:15:00","answer_description":"","question_text":"Which of the following operations can be used to create a new DataFrame that has 12 partitions from an original DataFrame df that has 8 partitions?","question_images":[],"unix_timestamp":1683112500,"isMC":true,"answer_ET":"A","answer":"A","choices":{"A":"df.repartition(12)","C":"df.partitionBy(1.5)","D":"df.coalesce(12)","E":"df.partitionBy(12)","B":"df.cache()"},"answers_community":["A (100%)"],"topic":"1"},{"id":"oHPUGxYuXR4dHJXGuIrX","question_images":[],"question_text":"The code block shown below contains an error. The code block is intended to create a single-column DataFrame from Python list years which is made up of integers. Identify the error.\n\nCode block:\n\nspark.createDataFrame(years, IntegerType)","choices":{"B":"The years list should be wrapped in another list like [years] to make clear that it is a column rather than a row.","D":"The IntegerType call must be followed by parentheses.","E":"The IntegerType call should not be present — Spark can tell that list years is full of integers.","A":"The column name must be specified.","C":"There is no createDataFrame operation in spark."},"answer_description":"","url":"https://www.examtopics.com/discussions/databricks/view/132693-exam-certified-associate-developer-for-apache-spark-topic-1/","timestamp":"2024-02-02 13:31:00","exam_id":161,"unix_timestamp":1706877060,"isMC":true,"question_id":68,"answer_ET":"D","topic":"1","answer_images":[],"answer":"D","answers_community":["D (100%)"],"discussion":[{"upvote_count":"1","timestamp":"1737281700.0","content":"Selected Answer: D\ntested it\nspark.createDataFrame(years, IntegerType()).display() makes the job","comment_id":"1342949","poster":"bp_a_user"},{"upvote_count":"1","content":"Selected Answer: D\nThe answer is D","comment_id":"1246829","poster":"5effea7","timestamp":"1720795320.0"},{"comment_id":"1145833","timestamp":"1707517920.0","content":"Selected Answer: D\nD is right","upvote_count":"4","poster":"azure_bimonster"},{"comment_id":"1138516","upvote_count":"2","content":"Answer should be D","timestamp":"1706877060.0","poster":"saryu"}]},{"id":"jdVqitfonyrU1PvINTfm","answer_ET":"A","discussion":[{"comment_id":"1193053","upvote_count":"1","timestamp":"1728569400.0","content":"A. 1. withColumn\n2. from_unixtime\n3. col(\"openDate\")\n4. “EEEE, MMM d, yyyy h:mm a\"","poster":"Sowwy1"}],"exam_id":161,"answers_community":[],"answer_description":"","answer":"A","unix_timestamp":1712758200,"question_id":69,"topic":"1","answer_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/138354-exam-certified-associate-developer-for-apache-spark-topic-1/","timestamp":"2024-04-10 16:10:00","question_images":["https://img.examtopics.com/certified-associate-developer-for-apache-spark/image20.png"],"question_text":"The code block shown below should return a DataFrame containing a column openDateString, a string representation of Java’s SimpleDateFormat. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.\n\nNote that column openDate is of type integer and represents a date in the UNIX epoch format — the number of seconds since midnight on January 1st, 1970.\n\nAn example of Java’s SimpleDateFormat is \"Sunday, Dec 4, 2008 1:05 pm\".\n\nA sample of storesDF is displayed below:\n\n//IMG//\n\n\nCode block:\n\nstoresDF.__1__(\"openDateString\", __2__(__3__, __4__))","isMC":true,"choices":{"D":"1. withColumn\n2. from_unixtlme\n3. col(\"openDate\")\n4. SimpleDateFormat","B":"1. withColumn\n2. date_format\n3. col(\"openDate\")\n4. \"EEEE, mmm d, yyyy h:mm a\"","E":"1. withColumn\n2. from_unixtime\n3. col(\"openDate\")\n4. \"dw, MMM d, yyyy h:mm a\"","A":"1. withColumn\n2. from_unixtime\n3. col(\"openDate\")\n4. “EEEE, MMM d, yyyy h:mm a\"","C":"1. newColumn\n2. from_unixtinie\n3. \"openDate\"\n4. \"EEEE, MMM d, yyyy h:mm a\""}},{"id":"YSb7kRXTuyLfKxja5Ey8","answer":"A","isMC":true,"answer_description":"","unix_timestamp":1712758200,"answers_community":[],"choices":{"C":"DataFrame.merge()","D":"DataFrame.leftJoin()","B":"DataFrame.crossJoin()","A":"DataFrame.join()","E":"Standalone join() function"},"question_images":[],"question_id":70,"answer_images":[],"timestamp":"2024-04-10 16:10:00","url":"https://www.examtopics.com/discussions/databricks/view/138355-exam-certified-associate-developer-for-apache-spark-topic-1/","topic":"1","discussion":[{"timestamp":"1728569400.0","comment_id":"1193055","poster":"Sowwy1","upvote_count":"1","content":"A. DataFrame.join()"}],"answer_ET":"A","question_text":"Which of the following operations can be used to perform a left join on two DataFrames?","exam_id":161}],"exam":{"lastUpdated":"12 Apr 2025","isBeta":false,"isMCOnly":true,"provider":"Databricks","isImplemented":true,"id":161,"numberOfQuestions":185,"name":"Certified Associate Developer for Apache Spark"},"currentPage":14},"__N_SSP":true}