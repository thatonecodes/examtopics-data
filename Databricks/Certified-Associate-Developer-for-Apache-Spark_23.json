{"pageProps":{"questions":[{"id":"mjH8KM7ct1lnMmGOncMr","isMC":true,"answer_images":[],"topic":"1","timestamp":"2023-04-03 17:15:00","url":"https://www.examtopics.com/discussions/databricks/view/104988-exam-certified-associate-developer-for-apache-spark-topic-1/","choices":{"E":"storesDF.agg(approx_count_distinct(col(\"division\"), 0.05).alias(\"divisionDistinct\"))","A":"storesDF.agg(approx_count_distinct(col(\"division\")).alias(\"divisionDistinct\"))","D":"storesDF.agg(approx_count_distinct(col(\"division\"), 0.0).alias(\"divisionDistinct\"))","B":"storesDF.agg(approx_count_distinct(col(\"division\"), 0.01).alias(\"divisionDistinct\"))","C":"storesDF.agg(approx_count_distinct(col(\"division\"), 0.15).alias(\"divisionDistinct\"))"},"question_images":[],"answer_ET":"C","answers_community":["C (69%)","B (31%)"],"exam_id":161,"answer_description":"","question_id":111,"unix_timestamp":1680534900,"discussion":[{"comment_id":"881280","upvote_count":"5","comments":[{"comment_id":"1227357","content":"But your answer contradicts the question, they only ask you for the fastest way, while the error value is closer to zero, then it will take more time and resources. 0.15>0.01, that means that option C will be faster, it will have more errors, but it will be the fastest.","poster":"carlosmps","timestamp":"1717941600.0","upvote_count":"3"},{"comment_id":"916481","timestamp":"1686070320.0","comments":[{"comment_id":"1065969","content":"I noticed the same thing with this ID - bro has confidence, I have to triple make sure because he keeps answering wrong thus creating doubts in my head.","timestamp":"1699478760.0","poster":"outwalker","upvote_count":"1"}],"upvote_count":"19","poster":"ZSun","content":"I see you reply in a lot of question, barely correct.\nbro, you need to stop comment wrong information here.\nThis question only ask for efficiency, no need to balance between accuracy and efficiency. \nStop posting ChatGPT answer here"}],"poster":"4be8126","timestamp":"1682495820.0","content":"Selected Answer: B\nTo quickly return an approximation for the number of distinct values in column division in DataFrame storesDF, the most efficient code block to use would be:\n\nB. storesDF.agg(approx_count_distinct(col(\"division\"), 0.01).alias(\"divisionDistinct\"))\n\nUsing the approx_count_distinct() function allows for an approximate count of the distinct values in the column without scanning the entire DataFrame. The second parameter passed to the function is the maximum estimation error allowed, which in this case is set to 0.01. This is a trade-off between the accuracy of the estimate and the computational cost. Option C may still be efficient but with a larger estimation error of 0.15. Option A and D are not correct as they do not specify the estimation error, which means that the function would use the default value of 0.05. Option E specifies an estimation error of 0.05, but a smaller error of 0.01 is a better choice for a more accurate estimate with less computational cost."},{"content":"Selected Answer: C\nC is correct because it will provide the fastest approximate count with a standard deviation of 0.15","upvote_count":"3","timestamp":"1724070120.0","comment_id":"1268639","poster":"oussa_ama"},{"timestamp":"1703948940.0","upvote_count":"1","content":"B. storesDF.agg(approx_count_distinct(col(\"division\"), 0.01).alias(\"divisionDistinct\"))\n\nExplanation:\n\napprox_count_distinct(col(\"division\"), 0.01): This uses the approx_count_distinct function to approximate the number of distinct values in the \"division\" column with a relative error of 1%. The smaller the relative error, the more accurate the approximation, but it may require more resources.\n.alias(\"divisionDistinct\"): This renames the result column to \"divisionDistinct\" for better readability.\nSo, the correct answer is:\n\nB. storesDF.agg(approx_count_distinct(col(\"division\"), 0.01).alias(\"divisionDistinct\"))","comments":[{"timestamp":"1718949300.0","comment_id":"1234205","upvote_count":"1","content":"C is the correct answer.\nC. storesDF.agg(approx_count_distinct(col(\"division\"), 0.15).alias(\"divisionDistinct\"))\n\nThis option uses the largest rsd value (0.15), which means it prioritizes speed over accuracy. the smaller the rsd, the more accurate the result, but the longer it might take to compute. Conversely, a larger rsd value provides a faster result with less accuracy.","poster":"smd_"},{"comment_id":"1227354","poster":"carlosmps","content":"But your answer contradicts the question, they only ask you for the fastest way, while the error value is closer to zero, then it will take more time and resources. 0.15>0.01, that means that option C will be faster, it will have more errors, but it will be the fastest.","timestamp":"1717941540.0","upvote_count":"1"}],"poster":"zozoshanky","comment_id":"1109800"},{"timestamp":"1693962960.0","upvote_count":"4","comment_id":"1000055","poster":"thanab","content":"C\nThe code block that will most quickly return an approximation for the number of distinct values in column `division` in DataFrame `storesDF` is **C**, `storesDF.agg(approx_count_distinct(col(\"division\"), 0.15).alias(\"divisionDistinct\"))`. The `approx_count_distinct` function can be used to quickly estimate the number of distinct values in a column by using a probabilistic data structure. The second parameter of the `approx_count_distinct` function specifies the maximum estimation error allowed, with a smaller value resulting in a more accurate but slower estimation. In this case, an error of 0.15 is specified, which will result in a faster but less accurate estimation than the other options."},{"comment_id":"971117","poster":"cookiemonster42","content":"Selected Answer: C\nC - the less accurate the calculation, the faster it is","timestamp":"1691067060.0","upvote_count":"3"},{"content":"A. https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.approx_count_distinct.html","poster":"singh100","comment_id":"968821","upvote_count":"3","timestamp":"1690874340.0"},{"upvote_count":"3","content":"Selected Answer: C\nWhile not an option I would use, the question says most quickly (relatively), and this will be the fastest. Note that a 15% error is too high.","timestamp":"1684152420.0","poster":"SonicBoom10C9","comment_id":"898252"},{"poster":"TC007","comment_id":"860067","upvote_count":"2","content":"Selected Answer: C\nThe higher the relative error parameter, the less accurate and faster. The lower the relative error parameter, the more accurate and slower.","timestamp":"1680534900.0"}],"answer":"C","question_text":"Which of the following code blocks will most quickly return an approximation for the number of distinct values in column division in DataFrame storesDF?"},{"id":"nn1YZWyQ9kna0h2yz1ut","isMC":true,"answer_images":[],"topic":"1","timestamp":"2023-04-26 10:02:00","url":"https://www.examtopics.com/discussions/databricks/view/107546-exam-certified-associate-developer-for-apache-spark-topic-1/","choices":{"E":"The only way to compute a mean of a column is with the mean() method from a DataFrame.","D":"The agg() operation is not appropriate here – the withColumn() operation should be used instead.","B":"The argument to the mean() operation should not be quoted.","C":"The mean() operation is not a standalone function – it’s a method of the Column object.","A":"The argument to the mean() operation should be a Column abject rather than a string column name."},"question_images":[],"answer_ET":"A","answers_community":["A (47%)","E (37%)","D (16%)"],"exam_id":161,"answer_description":"","question_id":112,"unix_timestamp":1682496120,"discussion":[{"poster":"4be8126","comment_id":"881286","timestamp":"1682496120.0","content":"Selected Answer: E\nThe code block shown is correct and should return a new DataFrame with the mean of column sqft from DataFrame storesDF in column sqftMean. Therefore, the answer is E - none of the options identify a valid error in the code block.\n\nHere's an explanation for each option:\n\nA. The argument to the mean() operation can be either a Column object or a string column name, so there is no error in using a string column name in this case.\n\nE. This option is incorrect because the code block shown is a valid way to compute the mean of a column using PySpark. Another way to compute the mean of a column is with the mean() method from a DataFrame, but that doesn't mean the code block shown is invalid.","upvote_count":"7","comments":[{"timestamp":"1699885200.0","comment_id":"1069374","content":"wrong! A","poster":"newusername","upvote_count":"3"}]},{"poster":"sofiess","comment_id":"1297539","timestamp":"1728908940.0","upvote_count":"2","content":"The mean() function expects a Column object as an argument, which can be created using col(\"sqft\"). Simply passing the column name as a string will result in an error."},{"upvote_count":"1","comment_id":"1296569","content":"The correct answer is A. The argument to the mean() operation should be a Column object rather than a string column name.\n\nIn Spark DataFrames, the mean() function takes a Column object as its argument, not a string column name. To create a Column object from a string column name, you can use the col() function.","poster":"DanYanez","timestamp":"1728745620.0"},{"comment_id":"1171541","content":"The error in the code is A. The argument to the mean() operation should be a Column object rather than a string column name.\nIn the provided code block, \"sqft\" is passed as a string column name to the mean() function. However, the correct approach is to use a Column object. This can be achieved by referencing the column using the storesDF DataFrame and the col() function. Here's the corrected code:\nstoresDF.agg(mean(col(\"sqft\")).alias(\"sqftMean\"))","upvote_count":"2","poster":"ajayrtk","timestamp":"1710231720.0"},{"comment_id":"1168367","content":"from pyspark.sql.functions import col, mean\n\nstudents =[\n{'rollno':'001','name':'sravan','sqft':23, 'height':5.79,'weight':67,'address':'guntur'},\n{'rollno':'002','name':'ojaswi','sqft':16, 'height':3.79,'weight':34,'address':'hyd'}]\nstoresDF = spark.createDataFrame( students)\nstoresDF.agg(mean('sqft').alias('sqftMean')).show()\n\nthis works as well! not sure which one is wrong then","upvote_count":"3","timestamp":"1709846580.0","poster":"azurearch"},{"content":"Selected Answer: A\nA is most like correct here","timestamp":"1707416400.0","upvote_count":"2","poster":"azure_bimonster","comment_id":"1144798"},{"timestamp":"1703164920.0","comment_id":"1102552","content":"Selected Answer: A\nA) should be the one considering databricks practice pdf. mean() function should take col object as input.","poster":"Saurabh_prep","upvote_count":"1"},{"upvote_count":"1","content":"it appears that there might be some flexibility in how the mean function can be used with either a string column name or a col() function. However, the most accurate and recommended approach is to use the col() function to create a Column object explicitly.\n\nWith this in mind, the best choice is:\n\nA. The argument to the mean() operation should be a Column object rather than a string column name. The mean function takes a Column object as an argument, not a string column name. To fix the error, the code block should be rewritten as storesDF.agg(mean(col(\"sqft\")).alias(\"sqftMean\")), where the col function is used to create a Column object from the string column name \"sqft\".\n\nWhile there might be situations where using a string column name works, following the standard practice of creating a Column object with col() ensures compatibility and clarity in code.","poster":"outwalker","timestamp":"1699479480.0","comment_id":"1065978"},{"upvote_count":"3","timestamp":"1698805080.0","comment_id":"1059315","poster":"juliom6","content":"Selected Answer: A\nCorrect answer is A:\n\nfrom pyspark.sql.functions import col, mean\n\nstudents =[\n{'rollno':'001','name':'sravan','sqft':23, 'height':5.79,'weight':67,'address':'guntur'},\n{'rollno':'002','name':'ojaswi','sqft':16, 'height':3.79,'weight':34,'address':'hyd'}]\nstoresDF = spark.createDataFrame( students)\nstoresDF.agg(mean(col('sqft')).alias('sqftMean')).show()"},{"timestamp":"1697725140.0","upvote_count":"1","comment_id":"1048001","poster":"juadaves","content":"D\nwithColumn() for new calculated column."},{"comments":[{"content":"storesDF.agg(mean(\"Value\").alias(\"sqftMean\")).show() it works","comment_id":"1048000","timestamp":"1697725020.0","poster":"juadaves","upvote_count":"1"}],"comment_id":"1000059","upvote_count":"2","poster":"thanab","timestamp":"1693963200.0","content":"A.\nA\nThe error in the code block is **A**, the argument to the `mean` operation should be a Column object rather than a string column name. The `mean` function takes a Column object as an argument, not a string column name. To fix the error, the code block should be rewritten as `storesDF.agg(mean(col(\"sqft\")).alias(\"sqftMean\"))`, where the `col` function is used to create a Column object from the string column name `\"sqft\"`.\n\nHere is the correct code\nstoresDF.agg(mean(col(\"sqft\")).alias(\"sqftMean\"))"},{"comment_id":"980157","upvote_count":"1","content":"The correct answer is:\n\nB. The argument to the mean() operation should not be quoted.\n\nIn the context of Apache Spark, the mean function takes a column name as its argument. Therefore, you would write it without quotes. The corrected code line would look something like this:","timestamp":"1691946480.0","poster":"halouanne"},{"comment_id":"977773","timestamp":"1691678220.0","poster":"cookiemonster42","upvote_count":"3","content":"Selected Answer: A\nThere's a similar question in the official Databricks samples and the right answer there is: \nCode block:\nstoresDF.__1__(__2__(__3__).alias(\"sqftMean\"))\nA.\n1. agg\n2. mean\n3. col(\"sqft\")\n\nIf we stick to this logic, the answer is A."},{"content":"df.agg(mean(\"amountpaid\").alias(\"amountpaid\")).show()\ndf.agg(mean(col(\"amountpaid\")).alias(\"sqftMean\")).show(). Both produces the result","comment_id":"967237","poster":"zozoshanky","timestamp":"1690730460.0","upvote_count":"1"},{"content":"Selected Answer: D\nagg is not required here.","poster":"Mohitsain","upvote_count":"3","timestamp":"1687513680.0","comment_id":"931434"}],"answer":"A","question_text":"The code block shown below contains an error. The code block is intended to return a new DataFrame with the mean of column sqft from DataFrame storesDF in column sqftMean. Identify the error.\nCode block:\nstoresDF.agg(mean(\"sqft\").alias(\"sqftMean\"))"},{"id":"zHZltqWjYBHKH4Gj0TTi","url":"https://www.examtopics.com/discussions/databricks/view/125645-exam-certified-associate-developer-for-apache-spark-topic-1/","question_images":[],"isMC":true,"answer_images":[],"unix_timestamp":1699479720,"topic":"1","exam_id":161,"timestamp":"2023-11-08 22:42:00","answers_community":["D (100%)"],"choices":{"D":"DataFrame.count()","A":"DataFrame.numberOfRows()","C":"DataFrame.sum()","E":"DataFrame.countDistinct()","B":"DataFrame.n()"},"answer_ET":"D","answer_description":"","question_text":"Which of the following operations can be used to return the number of rows in a DataFrame?","discussion":[{"upvote_count":"1","timestamp":"1721716740.0","content":"Selected Answer: D\nOption D is correct: DataFrame.count()","comment_id":"1253451","poster":"jds0"},{"timestamp":"1699479720.0","comment_id":"1065981","poster":"outwalker","upvote_count":"2","content":"The operation that can be used to return the number of rows in a DataFrame is:\n\nD. DataFrame.count()\n\nThe count() method in Spark DataFrame returns the number of rows in the DataFrame, and it is the standard way to determine the row count. Options A, B, C, and E are not valid methods for counting the number of rows in a DataFrame."}],"answer":"D","question_id":113},{"id":"lvj4cRAytxwrHEw1h1Id","question_images":[],"exam_id":161,"answer_description":"","choices":{"B":"DataFrame.cubed()","C":"DataFrame.group()","E":"DataFrame.grouping_id()","A":"DataFrame.GroupBy()","D":"DataFrame.groupBy()"},"unix_timestamp":1699479780,"question_text":"Which of the following operations returns a GroupedData object?","answers_community":["D (100%)"],"discussion":[{"content":"Selected Answer: D\n.groupBy() is correct one","comment_id":"1144805","poster":"azure_bimonster","upvote_count":"2","timestamp":"1723134720.0"},{"poster":"outwalker","content":"D. DataFrame.groupBy()\n\nThe groupBy() method is used to group the DataFrame based on one or more columns, and it returns a GroupedData object, which can then be used to perform various aggregation operations on the grouped data. Options A, B, C, and E do not return a GroupedData object.","comment_id":"1065982","timestamp":"1715197380.0","upvote_count":"2"}],"timestamp":"2023-11-08 22:43:00","answer":"D","question_id":114,"isMC":true,"url":"https://www.examtopics.com/discussions/databricks/view/125646-exam-certified-associate-developer-for-apache-spark-topic-1/","topic":"1","answer_ET":"D","answer_images":[]},{"id":"qZSiAy4pAoY5zaAOKCAG","answers_community":["E (83%)","B (17%)"],"discussion":[{"content":"Selected Answer: E\nE is the right option.\n\nSee code below with Spark 3.5.1\n# Summary statistics of a DataFrame \nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nfrom pyspark.errors import PySparkTypeError\n\nspark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n\ndata = [\n (0, 43161),\n (1, 51200),\n (2, None),\n (3, 78367),\n (4, None),\n ]\nstoresDF = spark.createDataFrame(data, [\"storeID\", \"sqft\"])\n \ntry:\n storesDF.summary(\"mean\").show()\nexcept Exception as e:\n print(e)\n\ntry:\n storesDF.describe(all = True).show()\nexcept Exception as e:\n print(e)\n\ntry:\n storesDF.describe(\"all\").show()\nexcept Exception as e:\n print(e)\n\ntry:\n storesDF.summary(\"all\").show()\nexcept Exception as e:\n print(e)\n\ntry:\n storesDF.describe().show()\nexcept Exception as e:\n print(e)","poster":"jds0","comment_id":"1253453","upvote_count":"2","timestamp":"1721717100.0"},{"timestamp":"1712689320.0","upvote_count":"1","content":"Selected Answer: E\nE is the correct one","poster":"dbdantas","comment_id":"1192456"},{"comment_id":"1144812","poster":"azure_bimonster","upvote_count":"1","timestamp":"1707417360.0","content":"Selected Answer: E\nE would be correct here"},{"poster":"mahmoud_salah30","upvote_count":"2","content":"tested e is the right answer","comment_id":"1110431","timestamp":"1704017220.0"},{"comment_id":"982508","upvote_count":"1","content":"E is the correct answer","timestamp":"1692186900.0","poster":"souha_axa"},{"comment_id":"971152","upvote_count":"1","content":"Selected Answer: E\ncheck the documentation, mates. both methods receive names of columns as arguments, so E is correct!","poster":"cookiemonster42","timestamp":"1691069700.0"},{"poster":"zozoshanky","content":"E is correct, it's giving the output.","comment_id":"967238","timestamp":"1690730640.0","upvote_count":"2"},{"comments":[{"content":"checked it, it gave me the right result, so E is the one","timestamp":"1691069820.0","comment_id":"971155","poster":"cookiemonster42","upvote_count":"3"}],"poster":"zozoshanky","upvote_count":"1","content":"B is correct. On running the last option it gives error.\n\nTypeError: describe() got an unexpected keyword argument 'all'","timestamp":"1690068300.0","comment_id":"959903"},{"upvote_count":"1","comments":[{"poster":"ZSun","upvote_count":"6","comment_id":"916383","comments":[{"timestamp":"1688129040.0","poster":"8605246","comment_id":"939085","content":"describe() is correct","upvote_count":"5"}],"content":"Did you really try this in pyspark, or look up the document?\nTypeError: describe() got an unexpected keyword argument 'all'","timestamp":"1686063000.0"},{"poster":"Deuterium","timestamp":"1688735220.0","comment_id":"945699","upvote_count":"1","comments":[{"timestamp":"1691069760.0","comment_id":"971154","content":"even chat gpt says E is the correct one :)","upvote_count":"3","poster":"cookiemonster42"}],"content":"Is you answer from Chat GPT ?"},{"content":"TypeError Traceback (most recent call last)\n<ipython-input-34-5077330dead7> in <cell line: 1>()\n----> 1 storesDF.describe(all = True)\n\nTypeError: DataFrame.describe() got an unexpected keyword argument 'all'","poster":"juadaves","upvote_count":"1","comment_id":"1048008","timestamp":"1697725500.0"}],"comment_id":"881470","timestamp":"1682506080.0","poster":"4be8126","content":"Selected Answer: B\nThe answer is B.\n\nExplanation: The describe() method in DataFrame returns a DataFrame with summary statistics for all numeric columns in the input DataFrame. By default, only the count, mean, standard deviation, minimum, and maximum values are returned, but additional statistics can be specified with the percentiles parameter. Setting the all parameter to True will include non-numeric columns in the output as well. Therefore, option B is the correct answer.\n\nOption A is not correct, as the summary() method only returns summary statistics for the specified column(s) and is not a valid option for returning summary statistics for all columns in the DataFrame.\n\nOption C is not correct, as the describe() method does not have an \"all\" option.\n\nOption D is also not correct, as the summary() method only returns summary statistics for the specified column(s) and does not have an \"all\" option.\n\nOption E is not incorrect, but it does not specify whether to include non-numeric columns in the output. Therefore, option B is a better answer."}],"url":"https://www.examtopics.com/discussions/databricks/view/107565-exam-certified-associate-developer-for-apache-spark-topic-1/","answer_ET":"E","topic":"1","answer_images":[],"question_text":"Which of the following code blocks returns a collection of summary statistics for all columns in\nDataFrame storesDF?","isMC":true,"exam_id":161,"question_id":115,"timestamp":"2023-04-26 12:48:00","question_images":[],"choices":{"E":"storesDF.describe()","B":"storesDF.describe(all = True)","D":"storesDF.summary(\"all\")","A":"storesDF.summary(\"mean\")","C":"storesDF.describe(\"all\")"},"unix_timestamp":1682506080,"answer":"E","answer_description":""}],"exam":{"provider":"Databricks","isMCOnly":true,"id":161,"numberOfQuestions":185,"isBeta":false,"name":"Certified Associate Developer for Apache Spark","isImplemented":true,"lastUpdated":"12 Apr 2025"},"currentPage":23},"__N_SSP":true}