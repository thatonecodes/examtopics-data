{"pageProps":{"questions":[{"id":"eXSbr0mUty1itWZG2bH0","discussion":[{"timestamp":"1693778220.0","comment_id":"997985","poster":"vctrhugo","upvote_count":"6","content":"Selected Answer: A\nA. Gold tables are more likely to contain aggregations than Silver tables.\n\nIn some data processing pipelines, especially those following a typical \"Bronze-Silver-Gold\" data lakehouse architecture, Silver tables are often considered a more refined version of the raw or Bronze data. Silver tables may include data cleansing, schema enforcement, and some initial transformations.\n\nGold tables, on the other hand, typically represent a stage where data is further enriched, aggregated, and processed to provide valuable insights for analytical purposes. This could indeed involve more aggregations compared to Silver tables."},{"upvote_count":"1","timestamp":"1731956580.0","content":"Selected Answer: A\nIn the medallion architecture commonly used in Delta Lake and Databricks, the relationship between Gold and Silver tables is as follows:\n\nBronze Tables: Raw, unprocessed data directly ingested from the source.\nSilver Tables: Cleaned and enriched data, often with transformations applied for a more refined view.\nGold Tables: Highly refined data, typically containing business-level aggregations, metrics, and summaries that are ready for analytics or reporting.","poster":"806e7d2","comment_id":"1314163"},{"upvote_count":"1","comment_id":"1280088","content":"Selected Answer: A\nGold data is often refined and aggregated.","poster":"6aa83ae","timestamp":"1725728760.0"},{"timestamp":"1725196800.0","comment_id":"1276082","upvote_count":"1","poster":"CID2024","content":"A. Gold tables are more likely to contain aggregations than Silver tables.\nIn the Delta Lake architecture, Silver tables typically contain cleaned and enriched data that has been transformed from raw data (Bronze tables). Gold tables, on the other hand, are often used for business-level aggregates, reporting, and analytics. They are built on top of Silver tables and provide a more refined and aggregated view of the data, making them more likely to contain aggregations."},{"upvote_count":"1","content":"Selected Answer: A\nIn the typical data pipeline architecture, Gold tables are often the final layer and contain aggregated, high-value insights that are ready for reporting and analysis. Silver tables usually contain more detailed and refined data that is processed from the raw or Bronze tables but may not yet be aggregated.","poster":"80370eb","timestamp":"1723109340.0","comment_id":"1262429"},{"content":"A\n\nThis gold data is often highly refined and aggregated, containing data that powers analytics, machine learning, and production applications. While all tables in the lakehouse should serve an important purpose, gold tables represent data that has been transformed into knowledge, rather than just information.\n\nAnalysts largely rely on gold tables for their core responsibilities, and data shared with a customer would rarely be stored outside this level.","comment_id":"1253899","poster":"Bujji1234","timestamp":"1721759280.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"1246608","timestamp":"1720774740.0","content":"Correct answer is B!","poster":"rukerrc"},{"comment_id":"1244554","poster":"3fbc31b","upvote_count":"2","content":"Selected Answer: B\nCorrect answer is B. I saw this from the Databricks practice test. It's a little blurry, but this is the correct answer.","comments":[{"content":"No, it's not true I checked it in the databricks exam and the answer is A","timestamp":"1722968280.0","upvote_count":"1","comment_id":"1261790","poster":"as3099"}],"timestamp":"1720469040.0"},{"upvote_count":"1","timestamp":"1717754760.0","content":"In some data processing pipelines, particularly those following a \"Bronze-Silver-Gold\" data lakehouse architecture, Silver tables are indeed considered a more refined version of raw or Bronze data. Gold tables, which represent the final stage of data processing, typically contain highly refined, aggregated, and ready-to-consume data.\nTherefore, it's common for Gold tables to contain aggregations, as they often represent the final, summarized, and aggregated view of the data. On the other hand, Silver tables may contain partially aggregated or cleansed data but are not typically the final destination for aggregated data.\n\"Gold tables are more likely to contain aggregations than Silver tables\" is accurate, making option A a valid choice.","poster":"jaromarg","comment_id":"1226040"},{"comment_id":"1221969","timestamp":"1717135860.0","poster":"Dusica","upvote_count":"1","content":"A; row data = bronze data > silver data > golden data\nC is so opposite and wrong"},{"timestamp":"1716215100.0","content":"Raw Data > Bronze Data > Silver Data > Golden Data","poster":"carlosmps","upvote_count":"1","comment_id":"1214402"},{"poster":"benni_ale","content":"Selected Answer: A\ncorrect is A","comment_id":"1203442","upvote_count":"1","timestamp":"1714289580.0"},{"poster":"SerGrey","upvote_count":"1","content":"Selected Answer: A\nCorrect is A","comment_id":"1113652","timestamp":"1704370260.0"},{"poster":"awofalus","content":"Selected Answer: A\nCorrect: A","comment_id":"1064865","upvote_count":"1","timestamp":"1699366560.0"},{"comment_id":"978342","upvote_count":"2","content":"To me it seems A and E is equally correct. Truthfull is not very defined in the question. But Gold layer typically have more rules and transformations in order to be consumed by business and reports. So It could be intepreted as more \"truthfull\". Or am I wrong here?","timestamp":"1691735040.0","poster":"Inhaler_boy"},{"content":"B\n2 Type of Tables in Delta Lake data lake architecture\nGold tables are the most refined and valuable tables in the data lake, while Silver tables are less refined and less valuable.\nGold tables are typically used for downstream analysis and reporting, while Silver tables are typically used for data exploration and experimentation.\n\nGold tables typically contain the most refined, high-quality, and valuable data in an organization's data architecture. They often represent the final output or result of data processing pipelines, where data has undergone extensive cleansing, transformation, and aggregation. Gold tables are typically used for critical business analysis, reporting, and decision-making processes.\n\nOption A: Gold tables are not necessarily more likely to contain aggregations than Silver tables.\nOption C: Gold tables are more likely to contain a more refined view of data than Silver tables.\nOption D: Gold tables are not necessarily more likely to contain more data than Silver tables.","comment_id":"946467","timestamp":"1688820360.0","poster":"Atnafu","upvote_count":"1","comments":[{"upvote_count":"2","poster":"Inhaler_boy","timestamp":"1691734920.0","content":"The data itself should be the same. However the Transformations are not. Gold Layer, as I understand it, is more probable to have more transformations as its ready for reports and business consumptions. So A?\n\"The Gold layer is for reporting and uses more de-normalized and read-optimized data models with fewer joins. The final layer of data transformations and data quality rules are applied here.\"\nhttps://www.databricks.com/glossary/medallion-architecture","comment_id":"978337"}]},{"comment_id":"861959","comments":[{"timestamp":"1680705360.0","content":"Dude you are providing all the wrong answers and giving baseless explanations without any link to a documentation or something. Please stop misleading people.","poster":"XiltroX","upvote_count":"6","comment_id":"862196"}],"timestamp":"1680689220.0","poster":"4be8126","content":"Selected Answer: B\nThe correct answer is B. Gold tables are typically considered to be the most valuable and trusted data assets in an organization. They represent the final, refined view of the data after all cleaning, transformations, and enrichments have been performed. Silver tables are the intermediate tables that feed into the Gold tables, and are typically used to perform data cleansing, filtering, and enrichment before the data is promoted to Gold.","upvote_count":"1"},{"poster":"rafahb","content":"Selected Answer: A\nA os correct","comment_id":"861133","upvote_count":"2","timestamp":"1680617100.0"},{"poster":"surrabhi_4","content":"Selected Answer: A\nOption A","comment_id":"859698","upvote_count":"4","timestamp":"1680511980.0"},{"timestamp":"1680366060.0","upvote_count":"4","poster":"XiltroX","comment_id":"858105","content":"Selected Answer: A\nTHE ANSWER C IS INCORRECT! Silver tables usually contain data that is commonly a little more refined than Bronze tables. Meaning they contain data that is likely cleaned and contains no duplicates. Gold tables usually contain aggregate or \"corrected\" data."}],"answer_ET":"A","question_id":91,"choices":{"C":"Gold tables are more likely to contain a less refined view of data than Silver tables.","B":"Gold tables are more likely to contain valuable data than Silver tables.","E":"Gold tables are more likely to contain truthful data than Silver tables.","A":"Gold tables are more likely to contain aggregations than Silver tables.","D":"Gold tables are more likely to contain more data than Silver tables."},"topic":"1","answer_description":"","url":"https://www.examtopics.com/discussions/databricks/view/104772-exam-certified-data-engineer-associate-topic-1-question-28/","isMC":true,"unix_timestamp":1680366060,"question_text":"Which of the following describes the relationship between Gold tables and Silver tables?","exam_id":162,"answer_images":[],"answer":"A","answers_community":["A (88%)","12%"],"question_images":[],"timestamp":"2023-04-01 18:21:00"},{"id":"quFCk94vGp9eM47FRkbI","answers_community":["E (100%)"],"answer_ET":"E","choices":{"D":"Bronze tables contain a less refined view of data than raw data.","A":"Bronze tables contain less data than raw data files.","E":"Bronze tables contain raw data with a schema applied.","C":"Bronze tables contain aggregates while raw data is unaggregated.","B":"Bronze tables contain more truthful data than raw data."},"isMC":true,"question_images":[],"unix_timestamp":1680368400,"timestamp":"2023-04-01 19:00:00","topic":"1","answer_images":[],"discussion":[{"content":"Selected Answer: E\nBronze tables are basically raw ingested data, often with schema borrowed from the original data source or table. Correct answer is E.","upvote_count":"14","poster":"XiltroX","timestamp":"1680368400.0","comment_id":"858146"},{"poster":"806e7d2","comment_id":"1314190","content":"Selected Answer: E\nIn the medallion architecture, Bronze tables are the first stage in the data pipeline and directly represent raw data ingested into the system. The raw data is stored in its original form but typically has a schema applied to make it queryable and usable within a structured data processing framework like Delta Lake.\n\nWhy E is correct:\nBronze tables contain the raw data as-is but with a defined schema to enable easier downstream processing and integration.\nThis schema provides structure to the otherwise unstructured or semi-structured raw data.","upvote_count":"1","timestamp":"1731959160.0"},{"content":"Selected Answer: E\nCorrect is E","upvote_count":"1","poster":"joaosanti","timestamp":"1727715240.0","comment_id":"1291626"},{"poster":"benni_ale","comment_id":"1203443","timestamp":"1714289700.0","upvote_count":"2","content":"Selected Answer: E\nstill i am not sure about the schema as i thought that correct types are usually defined in silver while in bronze are all strings"},{"content":"Selected Answer: E\nCorrect is E","poster":"SerGrey","upvote_count":"4","timestamp":"1704370320.0","comment_id":"1113654"},{"poster":"awofalus","upvote_count":"1","content":"Selected Answer: E\nE is correct","comment_id":"1064869","timestamp":"1699366740.0"},{"comment_id":"1058848","timestamp":"1698760980.0","poster":"DavidRou","content":"Selected Answer: E\nE is the right answer. Bronze data are simply a more structured (in terms of schema) version of raw data to be found in the \"landing area\".","upvote_count":"3"},{"poster":"vctrhugo","comment_id":"997986","content":"Selected Answer: E\nE. Bronze tables contain raw data with a schema applied.\n\nIn a typical data processing pipeline following a \"Bronze-Silver-Gold\" data lakehouse architecture, Bronze tables are the initial stage where raw data is ingested and transformed into a structured format with a schema applied. The schema provides structure and meaning to the raw data, making it more usable and accessible for downstream processing.\n\nTherefore, Bronze tables contain the raw data but in a structured and schema-enforced format, which makes them distinct from the unprocessed, unstructured raw data files.","timestamp":"1693778340.0","upvote_count":"2"},{"content":"Ans : E\n\nThe Bronze layer is where we land all the data from external source systems. The table structures in this layer correspond to the source system table structures \"as-is,\" along with any additional metadata columns that capture the load date/time, process ID, etc. The focus in this layer is quick Change Data Capture and the ability to provide an historical archive of source (cold storage), data lineage, auditability, reprocessing if needed without rereading the data from the source system.\nhttps://www.databricks.com/glossary/medallion-architecture#:~:text=Bronze%20layer%20%28raw%20data%29","upvote_count":"3","timestamp":"1689963060.0","poster":"akk_1289","comment_id":"958794"},{"timestamp":"1689962940.0","upvote_count":"1","poster":"akk_1289","comment_id":"958791","content":"Ans: E\nhttps://www.databricks.com/glossary/medallion-architecture#:~:text=Bronze%20layer%20%28raw%20data%29"},{"content":"E\nBronze tables are the foundation of the Delta Lake data lake architecture. They are created from raw data files and contain a schema that describes the data. This makes it easy to query and analyze the data in Bronze tables.\n\nRaw data files, on the other hand, do not have a schema applied. This means that it can be difficult to query and analyze the data in raw data files.\n\nOption A: Bronze tables typically contain more data than raw data files, because they include the schema.\n\nOption B: There is no indication that Bronze tables contain more truthful data than raw data.\n\nOption C: Bronze tables can contain aggregates, but they do not have to.\n\nOption D: Bronze tables typically contain a more refined view of data than raw data, because they include the schema.","timestamp":"1688820540.0","comment_id":"946471","poster":"Atnafu","comments":[{"comments":[{"poster":"Atnafu","upvote_count":"1","timestamp":"1688820900.0","content":"never mind :)","comment_id":"946481"}],"timestamp":"1688820840.0","content":"Sorry this is meant to be on question #30","upvote_count":"1","poster":"Atnafu","comment_id":"946479"}],"upvote_count":"2"},{"comment_id":"861139","poster":"rafahb","upvote_count":"2","timestamp":"1680617220.0","content":"Selected Answer: E\nE option"},{"upvote_count":"3","timestamp":"1680511980.0","comment_id":"859699","poster":"surrabhi_4","content":"Selected Answer: E\nOption E"}],"answer_description":"","question_id":92,"answer":"E","exam_id":162,"question_text":"Which of the following describes the relationship between Bronze tables and raw data?","url":"https://www.examtopics.com/discussions/databricks/view/104780-exam-certified-data-engineer-associate-topic-1-question-29/"},{"id":"JCWgsp4MCn7dK07gf27M","topic":"1","exam_id":162,"answer_images":[],"timestamp":"2023-03-27 10:09:00","answer":"C","question_images":[],"choices":{"C":"Databricks web application","E":"Driver node","D":"Databricks Filesystem","A":"Worker node","B":"JDBC data source"},"isMC":true,"answers_community":["C (100%)"],"answer_ET":"C","question_text":"Which of the following is hosted completely in the control plane of the classic Databricks architecture?","answer_description":"","unix_timestamp":1679904540,"question_id":93,"discussion":[{"timestamp":"1727170020.0","poster":"vctrhugo","comment_id":"997858","upvote_count":"15","content":"Selected Answer: C\nC. Databricks web application\n\nIn the classic Databricks architecture, the control plane includes components like the Databricks web application, the Databricks REST API, and the Databricks Workspace. These components are responsible for managing and controlling the Databricks environment, including cluster provisioning, notebook management, access control, and job scheduling.\n\nThe other options, such as worker nodes, JDBC data sources, Databricks Filesystem (DBFS), and driver nodes, are typically part of the data plane or the execution environment, which is separate from the control plane. Worker nodes are responsible for executing tasks and computations, JDBC data sources are used to connect to external databases, DBFS is a distributed file system for data storage, and driver nodes are responsible for coordinating the execution of Spark jobs."},{"poster":"h79","comment_id":"854101","content":"I disagree with this answer. I think its the databricks web app that is always in the control plane","upvote_count":"15","comments":[{"poster":"XiltroX","upvote_count":"2","content":"Agreed. Its Web app for sure","timestamp":"1680355680.0","comment_id":"857949"},{"comment_id":"861274","timestamp":"1680624540.0","content":"I think I meant to say that its option C. Not sure why I said that I agree with the answer in Examtopics. Option C for sure.","upvote_count":"1","poster":"XiltroX"}],"timestamp":"1680073320.0"},{"poster":"Tedet","upvote_count":"1","content":"Selected Answer: C\nRefer to architecture of Lakehouse","comment_id":"1339005","timestamp":"1736553000.0"},{"timestamp":"1731594420.0","poster":"806e7d2","upvote_count":"2","comment_id":"1312056","content":"Selected Answer: C\nIn the classic Databricks architecture, the Databricks web application (which includes the Databricks user interface, job scheduling, and management components) is hosted entirely in the control plane. This control plane is managed by Databricks and contains the user-facing services and APIs that allow users to interact with the Databricks environment.\n\nHere’s why the other options don’t fit:\n\nA. Worker node and E. Driver node: Both the driver and worker nodes are part of the data plane, where actual data processing occurs.\nB. JDBC data source: This is external to Databricks, typically hosted wherever the source database resides, and does not exist within the control plane.\nD. Databricks Filesystem (DBFS): While DBFS metadata might be managed by the control plane, the actual data is stored in the data plane (often within the customer’s cloud account, such as in AWS S3 or Azure Data Lake Storage)."},{"timestamp":"1727170020.0","content":"Selected Answer: C\nThe control plane in the classic Databricks architecture is responsible for managing the Databricks workspace, user and group management, and cluster management, among other things. The Databricks web application is a part of the control plane that enables users to interact with the workspace, create and manage clusters, and work with notebooks, jobs, and data. Worker nodes and driver nodes are part of the data plane, which is responsible for executing data processing tasks. JDBC data sources and the Databricks Filesystem are services that are used by both the control plane and the data plane.","comment_id":"889031","upvote_count":"3","poster":"Majjjj"},{"timestamp":"1724861100.0","comment_id":"1274179","poster":"afzalmp40","upvote_count":"1","content":"Selected Answer: C\nC is correct"},{"content":"Selected Answer: C\nC is correct","comment_id":"1271032","timestamp":"1724383620.0","poster":"Teja_50","upvote_count":"1"},{"timestamp":"1723102620.0","comment_id":"1262384","upvote_count":"1","poster":"80370eb","content":"Selected Answer: C\nits \"C\" - Control plane has Databricks web application"},{"comment_id":"1249369","upvote_count":"1","timestamp":"1721190000.0","poster":"ranjan24","content":"Its C. \nother options, such as worker nodes, JDBC data sources, Databricks Filesystem (DBFS), and driver nodes, are typically part of the data plane or the execution environment, which is separate from the control plane."},{"timestamp":"1717965720.0","upvote_count":"1","comment_id":"1227522","poster":"mascarenhaslucas","content":"Selected Answer: C\nThe answer is C! Accordinglu with the Databricks documentation, a cluster consists of one driver node and zero or more worker nodes, by default the driver node uses the same instance type as the worker node."},{"upvote_count":"1","comment_id":"1200789","timestamp":"1713883440.0","content":"Who decide the correct answer on this website ? CertiIQ says C ; ITExams says E.... For me it's C","poster":"pierrickaosis"},{"comment_id":"1188033","poster":"benni_ale","upvote_count":"1","content":"Selected Answer: C\nNodes are on the Data Plane. I think the Web App is the only one in the Control Pane.","timestamp":"1712061060.0"},{"content":"Selected Answer: C\nC is correct","poster":"Itmma","timestamp":"1710839400.0","comment_id":"1177153","upvote_count":"1"},{"comment_id":"1166586","content":"Answer is C: https://docs.databricks.com/en/_images/databricks-architecture-aws.png","timestamp":"1709653680.0","poster":"kirshoff","upvote_count":"1"},{"timestamp":"1706763780.0","poster":"agAshish","comments":[{"poster":"Isio05","upvote_count":"1","comment_id":"1202794","content":"Cluster nodes (both driver and worker) are located on customer cloud account. So E is no the correct answer here.","timestamp":"1714160040.0"}],"comment_id":"1137284","content":"E.Driver Node , is the correct answer.\nIn the classic Databricks architecture, the control plane includes components responsible for managing and coordinating the execution of tasks. The driver node is part of the control plane, and it handles the coordination and execution of the overall Spark application.","upvote_count":"1"},{"poster":"poundmanluffy","content":"Selected Answer: C\nWebapplication always resides in Control Plane","timestamp":"1703579100.0","upvote_count":"2","comment_id":"1105826"},{"upvote_count":"1","poster":"SerGrey","timestamp":"1703431980.0","comment_id":"1104696","content":"Selected Answer: C\nC is correct"},{"upvote_count":"1","content":"Selected Answer: C\nCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC","poster":"CHHIPA","comment_id":"1086064","timestamp":"1701513480.0"},{"comment_id":"1081285","upvote_count":"1","content":"Selected Answer: C\nC is correct","poster":"Ajinkyavsawant7","timestamp":"1701071040.0"},{"upvote_count":"1","content":"It is the web UI. C","timestamp":"1700021700.0","poster":"Huroye","comment_id":"1071065"},{"comment_id":"1022306","timestamp":"1696168440.0","upvote_count":"2","content":"Reading material: https://learn.microsoft.com/en-us/azure/databricks/getting-started/overview","poster":"Sriramiyer92"},{"timestamp":"1695697620.0","comment_id":"1017338","poster":"KalavathiP","content":"Selected Answer: C\nCorrect ans C","upvote_count":"1"},{"comment_id":"1016523","timestamp":"1695624000.0","content":"Selected Answer: C\nweb application","upvote_count":"1","poster":"d_b47"},{"timestamp":"1693201920.0","upvote_count":"1","poster":"Lipon23","comment_id":"991828","content":"Selected Answer: C\nDatabricks Web App for sure"},{"comment_id":"966937","timestamp":"1690700700.0","poster":"Gajen100","content":"Selected Answer: C\nDatabricks web application","upvote_count":"1"},{"content":"Can someone help me with access to all the questions?","upvote_count":"2","poster":"Jannat_13","timestamp":"1689663840.0","comment_id":"955078"},{"poster":"rickwolfe86","upvote_count":"3","timestamp":"1682994240.0","content":"Selected Answer: C\nOption C","comment_id":"886972"},{"poster":"rafahb","timestamp":"1681794060.0","content":"Selected Answer: C\nOption C","comment_id":"873288","upvote_count":"2"},{"timestamp":"1681225260.0","poster":"SireeJ","comment_id":"867429","upvote_count":"1","content":"Option: C"},{"timestamp":"1680869700.0","poster":"Data_4ever","content":"Selected Answer: C\nDatabricks web application is hosted in Control plane","comment_id":"863834","upvote_count":"1"},{"comment_id":"859600","upvote_count":"2","timestamp":"1680503940.0","poster":"surrabhi_4","content":"Selected Answer: C\nOption C"},{"upvote_count":"3","poster":"azurearch","content":"https://learn.microsoft.com/en-us/azure/databricks/getting-started/overview\ndatabricks webapp is in control plane. driver nodes are in data plane","comment_id":"851907","timestamp":"1679904540.0"}],"url":"https://www.examtopics.com/discussions/databricks/view/104049-exam-certified-data-engineer-associate-topic-1-question-3/"},{"id":"wlXDUQeT2XxwyiowLnS5","question_images":[],"unix_timestamp":1680368460,"answer_description":"","topic":"1","url":"https://www.examtopics.com/discussions/databricks/view/104781-exam-certified-data-engineer-associate-topic-1-question-30/","answers_community":["B (88%)","12%"],"discussion":[{"poster":"XiltroX","upvote_count":"10","content":"Selected Answer: B\nB is the correct answer. Checkpointing is a method that is part of structured streaming.","timestamp":"1680368460.0","comment_id":"858148"},{"content":"Selected Answer: B\nSpark Structured Streaming is for incremental loading. Checkpointing is for failover.","timestamp":"1737567000.0","upvote_count":"2","comment_id":"1344888","poster":"SatuPatu"},{"comment_id":"1325502","poster":"res3","content":"Selected Answer: B\nDatabricks uses Apache Spark Structured Streaming to back numerous product associated with ingestion workloads, including:\n- Auto Loader\n- COPY INTO\n- Delta Live Tables pipelines\n- Materialized views and streaming tables in Databricks SQL\n\nSource: https://docs.databricks.com/en/ingestion/streaming.html","timestamp":"1733995200.0","upvote_count":"3"},{"comment_id":"1321190","timestamp":"1733193180.0","poster":"heystatgal","content":"Selected Answer: A\nB. Spark Structured Streaming:\nSpark Structured Streaming is a key underlying technology for Auto Loader to process streaming data. However, checkpointing is the specific mechanism that allows Auto Loader to track incremental progress. While Structured Streaming is essential for real-time data processing, checkpointing is the mechanism used to track what data has been processed.","upvote_count":"3"},{"upvote_count":"1","poster":"80370eb","timestamp":"1723173120.0","comment_id":"1262732","content":"Selected Answer: B\nB. Spark Structured Streaming\n\nAuto Loader uses Spark Structured Streaming to incrementally and efficiently process new data as it arrives, enabling scalable and reliable data ingestion in Databricks."},{"poster":"RBKasemodel","comment_id":"1124897","upvote_count":"2","timestamp":"1705489140.0","content":"The answer should be A. \nAuto Loader is used by Structured Streaming to process data incrementaly, not the other way around."},{"upvote_count":"1","content":"Selected Answer: B\nCorrect is B","timestamp":"1704370380.0","comment_id":"1113656","poster":"SerGrey"},{"comment_id":"1064870","upvote_count":"1","content":"Selected Answer: B\nB is correct","poster":"awofalus","timestamp":"1699366800.0"},{"upvote_count":"1","content":"Selected Answer: B\nB is orrect","timestamp":"1696923900.0","poster":"anandpsg101","comment_id":"1039270"},{"comment_id":"997987","upvote_count":"2","timestamp":"1693778520.0","poster":"vctrhugo","content":"Selected Answer: B\nB. Spark Structured Streaming\n\nThe Auto Loader process in Databricks is typically used in conjunction with Spark Structured Streaming to process data incrementally. Spark Structured Streaming is a real-time data processing framework that allows you to process data streams incrementally as new data arrives. The Auto Loader is a feature in Databricks that works with Structured Streaming to automatically detect and process new data files as they are added to a specified data source location. It allows for incremental data processing without the need for manual intervention."},{"timestamp":"1689963660.0","comment_id":"958799","poster":"akk_1289","upvote_count":"2","content":"ans:A\nHow does Auto Loader track ingestion progress?\nAs files are discovered, their metadata is persisted in a scalable key-value store (RocksDB) in the checkpoint location of your Auto Loader pipeline. This key-value store ensures that data is processed exactly once.\n\nIn case of failures, Auto Loader can resume from where it left off by information stored in the checkpoint location and continue to provide exactly-once guarantees when writing data into Delta Lake. You don’t need to maintain or manage any state yourself to achieve fault tolerance or exactly-once semantics.\nhttps://docs.databricks.com/ingestion/auto-loader/index.html"},{"poster":"akk_1289","timestamp":"1689963360.0","content":"ans:B\nHow does Auto Loader track ingestion progress?\nAs files are discovered, their metadata is persisted in a scalable key-value store (RocksDB) in the checkpoint location of your Auto Loader pipeline. This key-value store ensures that data is processed exactly once.\n\nIn case of failures, Auto Loader can resume from where it left off by information stored in the checkpoint location and continue to provide exactly-once guarantees when writing data into Delta Lake. You don’t need to maintain or manage any state yourself to achieve fault tolerance or exactly-once semantics.\nhttps://docs.databricks.com/ingestion/auto-loader/index.html","upvote_count":"1","comment_id":"958796"},{"content":"B\nAuto Loader uses Spark Structured Streaming to process data incrementally. Spark Structured Streaming is a streaming engine that can be used to process data as it arrives. This makes it ideal for processing data that is being generated in real time.\n\nOption A: Checkpointing is a technique used to ensure that data is not lost in case of a failure. It is not used to process data incrementally.\n\nOption C: Data Explorer is a data exploration tool that can be used to explore data. It is not used to process data incrementally.\n\nOption D: Unity Catalog is a metadata management tool that can be used to store and manage metadata about data assets. It is not used to process data incrementally.\n\nOption E: Databricks SQL is a SQL engine that can be used to query data. It is not used to process data incrementally.","comment_id":"946478","timestamp":"1688820780.0","poster":"Atnafu","upvote_count":"3"},{"poster":"surrabhi_4","upvote_count":"2","comment_id":"859702","timestamp":"1680512100.0","content":"Selected Answer: B\nOption B"}],"timestamp":"2023-04-01 19:01:00","question_id":94,"answer_ET":"B","exam_id":162,"isMC":true,"question_text":"Which of the following tools is used by Auto Loader process data incrementally?","answer":"B","choices":{"D":"Unity Catalog","B":"Spark Structured Streaming","A":"Checkpointing","E":"Databricks SQL","C":"Data Explorer"},"answer_images":[]},{"id":"Y508IVfMmb6eNStkjkXi","answer_images":[],"answer":"D","topic":"1","question_id":95,"answer_ET":"D","answer_description":"","question_images":["https://img.examtopics.com/certified-data-engineer-associate/image9.png"],"unix_timestamp":1680437640,"discussion":[{"upvote_count":"5","timestamp":"1680437640.0","poster":"XiltroX","content":"D is the correct answer","comment_id":"858823"},{"content":"Selected Answer: D\nThe correct line of code to fill in the blank to execute a micro-batch to process data every 5 seconds is:\n\nD. trigger(processingTime=\"5 seconds\")\n\nOption A (\"trigger(\"5 seconds\")\") would not work because it does not specify that the trigger should be a processing time trigger, which is necessary to trigger a micro-batch processing at regular intervals.\n\nOption B (\"trigger()\") would not work because it would use the default trigger, which is not a processing time trigger.\n\nOption C (\"trigger(once=\"5 seconds\")\") would not work because it would only trigger the query once, not at regular intervals.\n\nOption E (\"trigger(continuous=\"5 seconds\")\") would not work because it would trigger the query to run continuously, without any pauses in between, which is not what the data engineer wants.","upvote_count":"5","timestamp":"1680690120.0","comment_id":"861969","poster":"4be8126"},{"comment_id":"1291855","upvote_count":"1","timestamp":"1727768520.0","poster":"Raghu_Dasara","content":"D is correct answer\nProcessingTime\nhttps://learn.microsoft.com/en-us/azure/databricks/structured-streaming/triggers\nContinues Processing :"},{"upvote_count":"1","content":"Selected Answer: D\ncorrect syntax is D","poster":"benni_ale","timestamp":"1714289820.0","comment_id":"1203445"},{"comment_id":"1064874","poster":"awofalus","timestamp":"1699367100.0","content":"Selected Answer: D\nCorrect: D","upvote_count":"1"},{"poster":"vctrhugo","content":"Selected Answer: D\n# ProcessingTime trigger with two-seconds micro-batch interval\ndf.writeStream \\\n .format(\"console\") \\\n .trigger(processingTime='2 seconds') \\\n .start()\n\nhttps://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#triggers","upvote_count":"2","timestamp":"1693778640.0","comment_id":"997991"},{"poster":"AndreFR","upvote_count":"1","comment_id":"985343","content":"Selected Answer: D\nhttps://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#triggers","timestamp":"1692466200.0"},{"comment_id":"946484","comments":[{"upvote_count":"3","comment_id":"997992","timestamp":"1693778700.0","poster":"vctrhugo","content":"This is Scala example. Exam should be 100% on Python."}],"timestamp":"1688821080.0","content":"D\nval query = sourceTable\n .writeStream\n .format(\"delta\")\n .outputMode(\"append\")\n .trigger(Trigger.ProcessingTime(\"5 seconds\"))\n .start(destinationTable)","poster":"Atnafu","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: D\nD os correct","comment_id":"861140","timestamp":"1680617280.0","poster":"rafahb"},{"upvote_count":"3","poster":"surrabhi_4","content":"Selected Answer: D\nOption D","comment_id":"859703","timestamp":"1680512100.0"}],"timestamp":"2023-04-02 14:14:00","exam_id":162,"question_text":"A data engineer has configured a Structured Streaming job to read from a table, manipulate the data, and then perform a streaming write into a new table.\nThe cade block used by the data engineer is below:\n//IMG//\n\nIf the data engineer only wants the query to execute a micro-batch to process data every 5 seconds, which of the following lines of code should the data engineer use to fill in the blank?","choices":{"A":"trigger(\"5 seconds\")","B":"trigger()","E":"trigger(continuous=\"5 seconds\")","D":"trigger(processingTime=\"5 seconds\")","C":"trigger(once=\"5 seconds\")"},"answers_community":["D (100%)"],"isMC":true,"url":"https://www.examtopics.com/discussions/databricks/view/104854-exam-certified-data-engineer-associate-topic-1-question-31/"}],"exam":{"isImplemented":true,"isBeta":false,"lastUpdated":"12 Apr 2025","provider":"Databricks","isMCOnly":true,"name":"Certified Data Engineer Associate","numberOfQuestions":169,"id":162},"currentPage":19},"__N_SSP":true}