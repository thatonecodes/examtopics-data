{"pageProps":{"questions":[{"id":"GfquT9tS5pHnsMWJJfNf","question_text":"Of the following situations, in which will it be most advantageous to store DataFrame df at the MEMORY_AND_DISK storage level rather than the MEMORY_ONLY storage level?","answer_images":[],"choices":{"C":"When it’s faster to recompute all the data in DataFrame df that cannot fit into memory based on its logical plan rather than read it from disk.","D":"When it’s faster to read all the computed data in DataFrame df that cannot fit into memory from disk rather than recompute it based on its logical plan.","E":"The storage level MENORY_ONLY will always be more advantageous because it’s faster to read data from memory than it is to read data from disk.","A":"When all of the computed data in DataFrame df can fit into memory.","B":"When the memory is full and it’s faster to recompute all the data in DataFrame df rather than read it from disk."},"exam_id":161,"unix_timestamp":1680123540,"answer_description":"","topic":"1","answer_ET":"D","url":"https://www.examtopics.com/discussions/databricks/view/104430-exam-certified-associate-developer-for-apache-spark-topic-1/","answers_community":["D (100%)"],"timestamp":"2023-03-29 22:59:00","question_images":[],"isMC":true,"discussion":[{"content":"D. When it’s faster to read all the computed data in DataFrame df that cannot fit into memory from disk rather than recompute it based on its logical plan.","upvote_count":"9","timestamp":"1711753140.0","comment_id":"854959","poster":"sousouka"},{"comment_id":"917431","poster":"ZSun","timestamp":"1717780080.0","upvote_count":"7","content":"All other explanation is either wrong or misleading. To understand the question, you need to understand the difference between Memory_only and Memory_and_Disk\n1. Memory_and_Disk, which is the default mode for cache ro persist. That means, if the data size is larger than the memory, it will store the extra data in disk. next time when we n eed to read data, we will read data firstly from memory, and then read from disk.\n2. Memory_Only means, if the data size is larger than memory, it will not store the extra data. next time we read data, we will read from memory first and then recompute the extra data which cannot store in memory.\nPS. Mr. 4be8126 is wrong about raising error when out of memory.\nTherefore, the difference/balance between Memory_only and memory_and_disk lay in how they handle the extra data out of memory. which is option D, if it is faster to read data from disk is faster than recompute it, then memory_and_disk."},{"timestamp":"1730816940.0","content":"Selected Answer: D\nD is correct","comment_id":"1062977","upvote_count":"1","poster":"newusername"},{"poster":"astone42","timestamp":"1723200960.0","content":"Selected Answer: D\nD is correct","upvote_count":"1","comment_id":"976520"},{"upvote_count":"1","comment_id":"968670","poster":"singh100","timestamp":"1722480720.0","content":"D. It is faster to read the computed data from disk instead of recomputing it based on its logical plan when the recomputation is costly and time-consuming."},{"content":"Selected Answer: D\nIf it's faster to read from memory and can fit in, then there is no reason to use Memory_and_disk, Memory_only is sufficient. Also, if it's faster to compute than read from disk, that's what you would do. The only options is when it's too big to fit in memory and too expensive to recompute, so reading from disk (or rather caching from disk into memory on the fly) is faster.","poster":"SonicBoom10C9","upvote_count":"1","timestamp":"1715753160.0","comment_id":"898072"},{"content":"Selected Answer: D\nThe most advantageous situation to store a DataFrame at the MEMORY_AND_DISK storage level instead of the MEMORY_ONLY storage level is option D - when it’s faster to read all the computed data in DataFrame df that cannot fit into memory from disk rather than recompute it based on its logical plan.\n\nThis is because the MEMORY_ONLY storage level only stores data in memory, which can result in an out-of-memory error if the data exceeds the available memory. On the other hand, the MEMORY_AND_DISK storage level will spill data to disk if there is not enough memory available, allowing more data to be processed without errors.\n\nIn situations where the computed data can fit entirely into memory, it is best to use the MEMORY_ONLY storage level as it will be faster than reading from disk. However, when there is not enough memory to store all the computed data, it may be necessary to use the MEMORY_AND_DISK storage level.","timestamp":"1714734660.0","poster":"4be8126","comment_id":"888432","upvote_count":"1"},{"comment_id":"888254","upvote_count":"1","content":"Yes but what about the link with the question ? I would say B too :)","timestamp":"1714721160.0","poster":"sly75"},{"upvote_count":"2","content":"Answer is D. This is the whole idea behind caching","poster":"Indiee","timestamp":"1714064280.0","comment_id":"880659"}],"answer":"D","question_id":46},{"id":"6XimH82jsa6ZmsMdIq7Q","discussion":[{"content":"E is correct","timestamp":"1727656260.0","upvote_count":"1","poster":"siva1280","comment_id":"1186498"}],"answer_images":[],"answer":"E","answers_community":[],"topic":"1","question_text":"Which of the following code blocks extracts the value for column sqft from the first row of DataFrame storesDF?","question_images":[],"choices":{"E":"storesDF.first().sqft","C":"storesDF.collect(l)[0][\"sqft\"]","D":"storesDF.first.sqft","A":"storesDF.first()[col(\"sqft\")]","B":"storesDF[0][\"sqft\"]"},"timestamp":"2024-03-31 02:31:00","answer_ET":"E","url":"https://www.examtopics.com/discussions/databricks/view/137566-exam-certified-associate-developer-for-apache-spark-topic-1/","exam_id":161,"question_id":47,"unix_timestamp":1711848660,"isMC":true,"answer_description":""},{"id":"XuD9uVWRt1erV8FYEubX","timestamp":"2024-04-10 13:21:00","exam_id":161,"unix_timestamp":1712748060,"discussion":[{"upvote_count":"1","poster":"Sowwy1","timestamp":"1728559260.0","comment_id":"1192915","content":"D is correct"}],"answer_images":[],"question_id":48,"answer_ET":"D","choices":{"D":"storesDF.createOrReplaceTempView(\"stores\")\nspark.sql(\"SELECT storeId, managerName FROM stores\")","A":"storesDF.createOrReplaceTempView()\nspark.sql(\"SELECT storeId, managerName FROM stores\")","B":"storesDF.query(”SELECT storeid, managerName from stores\")","E":"storesDF.createOrReplaceTempView(\"stores\")\nstoresDF.query(\"SELECT storeId, managerName FROM stores\")","C":"spark.createOrReplaceTempView(\"storesDF\")\nstoresDF.sql(\"SELECT storeId, managerName from stores\")"},"answer":"D","isMC":true,"question_text":"Which of the following code blocks uses SQL to return a new DataFrame containing column storeId and column managerName from a table created from DataFrame storesDF?","answers_community":[],"answer_description":"","question_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/138319-exam-certified-associate-developer-for-apache-spark-topic-1/","topic":"1"},{"id":"lRrEcrT4Ei7VDMwzpEYS","question_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/138320-exam-certified-associate-developer-for-apache-spark-topic-1/","question_text":"The code block shown below should adjust the number of partitions used in wide transformations like join() to 32. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.\n\nCode block:\n\n__1__(__2__, __3__)","topic":"1","answer_ET":"E","answer_description":"","answers_community":[],"choices":{"C":"1. spark.conf.text\n2. \"spark.default.parallelism\"\n3. \"32\"","A":"1. spark.conf.get\n2. \"spark.sql.shuffle.partitions\"\n3. \"32\"","D":"1. spark.conf.set\n2. \"spark.default.parallelism\"\n3. \"32\"","B":"1. spark.conf.set\n2. \"spark.default.parallelism\"\n3. 32","E":"1. spark.conf.set\n2. \"spark.sql.shuffle.partitions\"\n3. \"32\""},"discussion":[{"poster":"maddyj111","upvote_count":"2","content":"in Option E there is \"32\" is in string format is it acceptable? I think it should be numeric 32","comment_id":"1267214","timestamp":"1723819260.0"},{"poster":"cd6a625","content":"A and E same","comment_id":"1243841","upvote_count":"1","timestamp":"1720352520.0","comments":[{"poster":"cd6a625","upvote_count":"1","comment_id":"1243842","timestamp":"1720352520.0","content":"nevermind"}]},{"timestamp":"1712748120.0","upvote_count":"1","comment_id":"1192916","poster":"Sowwy1","content":"E. 1. spark.conf.set\n2. \"spark.sql.shuffle.partitions\"\n3. \"32\""}],"question_id":49,"answer_images":[],"unix_timestamp":1712748120,"timestamp":"2024-04-10 13:22:00","exam_id":161,"isMC":true,"answer":"E"},{"id":"PNocmXxTzUUY69jO5gfW","answer_ET":"A","isMC":true,"answer_description":"","answers_community":["A (60%)","B (40%)"],"question_text":"Which of the following code blocks returns a DataFrame containing a column openDateString, a string representation of Java’s SimpleDateFormat?\n\nNote that column openDate is of type integer and represents a date in the UNIX epoch format — the number of seconds since midnight on January 1st, 1970.\n\nAn example of Java's SimpleDateFormat is \"Sunday, Dec 4, 2008 1:05 pm\".\n\nA sample of storesDF is displayed below:\n\n//IMG//","question_images":["https://img.examtopics.com/certified-associate-developer-for-apache-spark/image16.png"],"timestamp":"2024-02-09 23:05:00","unix_timestamp":1707516300,"discussion":[{"upvote_count":"1","poster":"oussa_ama","content":"Selected Answer: A\npyspark.sql.functions.from_unixtime(timestamp: ColumnOrName, format: str = 'yyyy-MM-dd HH:mm:ss') → pyspark.sql.column.Column[source]\nConverts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the given format.","comment_id":"1270068","timestamp":"1724241000.0"},{"content":"Selected Answer: A\nThe correct code block that returns a DataFrame containing a column openDateString, which is a string representation of Java's SimpleDateFormat, is:\n\nA. storesDF.withColumn(\"openDatestring\", from_unixtime(col(\"openDate\"), \"EEEE, MMM d, yyyy h:mm a\"))\n\nThis option correctly converts the openDate column from UNIX epoch format to a human-readable string format using the from_unixtime function in PySpark, which is equivalent to Java's SimpleDateFormat. The format \"EEEE, MMM d, yyyy h:mm a\" aligns with the example provided: \"Sunday, Dec 4, 2008 1:05 pm\".","comment_id":"1266968","timestamp":"1723803960.0","poster":"65bd33e","upvote_count":"1"},{"upvote_count":"1","poster":"deadbeef38","timestamp":"1719170700.0","comment_id":"1235966","content":"Selected Answer: A\nnew column should be a String, existing column is Int"},{"timestamp":"1712748240.0","comment_id":"1192918","content":"A. storesDF.withColumn(\"openDatestring\", from unixtime(col(\"openDate“), “EEEE, MMM d, yyyy h:mm a\"))","poster":"Sowwy1","upvote_count":"1"},{"timestamp":"1710969540.0","poster":"c3d91ee","comment_id":"1178720","upvote_count":"2","content":"A is the right answer \n\n# Assuming you have a SparkSession created (replace with your setup if needed)\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import from_unixtime\n\n# Sample data (replace with your actual DataFrame)\ndata = [(\"store1\", 1668518400), (\"store2\", 1668432000)] # Sample UNIX epoch timestamps\ndf = spark.createDataFrame(data, [\"storeName\", \"openDate\"])\n\n# Convert openDate to formatted string\nformatted_df = df.withColumn(\n \"openDateString\", from_unixtime(col(\"openDate\"), \"EEEE, MMM d, yyyy h:mm a\")\n)\n\n# Print the resulting DataFrame\nformatted_df.show(truncate=False)\n\n# Stop the SparkSession (optional)\n#spark.stop()"},{"timestamp":"1707516300.0","upvote_count":"2","comment_id":"1145825","content":"Selected Answer: B\nB is likely tru, because A option has some typo issue and the underscore in from keyword is missing before unixtime, and the return type is not specified.","poster":"azure_bimonster"}],"url":"https://www.examtopics.com/discussions/databricks/view/133472-exam-certified-associate-developer-for-apache-spark-topic-1/","topic":"1","question_id":50,"exam_id":161,"answer_images":[],"choices":{"B":"storesDF.withColumn(\"openDateString\", from_unixtime(col(\"openDate“), \"EEEE, MMM d, yyyy h:mm a\", TimestampType()))","C":"storesDF.withColumn(\"openDateString\", date(col(\"openDate\"), \"EEEE, MMM d, yyyy h:mm a\"))","D":"storesDF.newColumn(col(\"openDateString\"), from_unixtime(\"openDate\", \"EEEE, MMM d, yyyy h:mm a\"))","A":"storesDF.withColumn(\"openDatestring\", from unixtime(col(\"openDate“), “EEEE, MMM d, yyyy h:mm a\"))","E":"storesDF.withColumn(\"openDateString\", date(col(\"openDate“), \"EEEE, MMM d, yyyy h:mm a\", TimestampType))"},"answer":"A"}],"exam":{"isMCOnly":true,"lastUpdated":"12 Apr 2025","isImplemented":true,"provider":"Databricks","id":161,"isBeta":false,"numberOfQuestions":185,"name":"Certified Associate Developer for Apache Spark"},"currentPage":10},"__N_SSP":true}