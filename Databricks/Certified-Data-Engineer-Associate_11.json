{"pageProps":{"questions":[{"id":"WGaFObuXnaOgyZiQDkyF","discussion":[{"content":"Selected Answer: B\nTo read a view, the permissions required depend on the compute type, Databricks Runtime version, and access mode:\n\n For all compute resources, you must have SELECT on the view itself, USE CATALOG on its parent catalog, and USE SCHEMA on its parent schema. This applies to all compute types that support Unity Catalog, including SQL warehouses, clusters in shared access mode, and clusters in single user access mode on Databricks Runtime 15.4 and above.\n\n For clusters on Databricks Runtime 15.3 and below that use single user access mode, you must also have SELECT on all tables and views that are referenced by the view, in addition to USE CATALOG on their parent catalogs and USE SCHEMA on their parent schemas.","poster":"san089","upvote_count":"1","comment_id":"1328541","timestamp":"1734530400.0"},{"timestamp":"1734183900.0","upvote_count":"1","poster":"Rinscy","comment_id":"1326480","content":"Selected Answer: B\nB and key here is “Shared Cluster”. On a single cluster with a runtime prior to 15.4 it will need the permissions on view and tables.\nWith Shared Access Cluster, only on the view."},{"upvote_count":"2","comment_id":"1318611","timestamp":"1732708080.0","poster":"Pirate_boid","content":"Selected Answer: B\nFor a view one does not need the permissions on the underlying table"},{"timestamp":"1732192200.0","upvote_count":"2","comment_id":"1315801","poster":"Medkalys","content":"Selected Answer: A\nIn Databricks Unity Catalog, permissions are hierarchical and must cover all data objects involved. If a user needs to query a view, the following conditions apply:\n\nSELECT permission on the VIEW: Allows the user to query the view itself.\nSELECT permission on the underlying TABLE(s): Views depend on the underlying tables or data sources. The user must also have SELECT permissions on these tables to access the data exposed by the view."},{"poster":"SajadAhm","content":"B is correct. in databricks partner platform, it shows privileges on a view and says:\nas you see, no one has access to this table, but we could give access to the view without giving access to the underlying table. this is one of the main advantages of views.","timestamp":"1731753240.0","upvote_count":"3","comment_id":"1313035"}],"answer_images":[],"answer":"B","url":"https://www.examtopics.com/discussions/databricks/view/151417-exam-certified-data-engineer-associate-topic-1-question-146/","question_images":[],"isMC":true,"exam_id":162,"answer_ET":"B","answer_description":"","timestamp":"2024-11-16 11:34:00","topic":"1","question_id":51,"choices":{"D":"Needs ALL PRIVILEGES at the SCHEMA level","C":"Needs ALL PRIVILEGES on the VIEW","B":"Needs SELECT permission only on the VIEW","A":"Needs SELECT permission on the VIEW and the underlying TABLE."},"question_text":"A data engineer needs to access the view created by the sales team, using a shared cluster. The data engineer has been provided usage permissions on the catalog and schema. In order to access the view created by sales team.\n\nWhat are the minimum permissions the data engineer would require in addition?","unix_timestamp":1731753240,"answers_community":["B (67%)","A (33%)"]},{"id":"tyYSr1qhAOFTqEmvLaab","topic":"1","url":"https://www.examtopics.com/discussions/databricks/view/153042-exam-certified-data-engineer-associate-topic-1-question-147/","isMC":true,"discussion":[{"timestamp":"1734354660.0","comment_id":"1327380","content":"Selected Answer: C\nThe correct answer is C. Scheduled Workflows can reduce resource consumption and expense since the cluster runs only long enough to execute the pipeline. This method ensures that the cluster is only active for the duration of the workflow execution, minimizing resource usage and costs.","poster":"MultiCloudIronMan","upvote_count":"2"}],"unix_timestamp":1734354660,"answer_description":"","answers_community":["C (100%)"],"question_id":52,"exam_id":162,"answer":"C","choices":{"C":"Scheduled Workflows can reduce resource consumption and expense since the cluster runs only long enough to execute the pipeline.","A":"Scheduled Workflows require an always-running cluster, which is more expensive but reduces processing latency.","D":"Scheduled Workflows run continuously until manually stopped.","B":"Scheduled Workflows process data as it arrives at configured sources."},"timestamp":"2024-12-16 14:11:00","question_images":[],"answer_images":[],"question_text":"Which method should a Data Engineer apply to ensure Workflows are being triggered on schedule?","answer_ET":"C"},{"id":"VQpX3T8qMmmaCb2DwIzv","discussion":[{"comment_id":"1334036","comments":[{"comment_id":"1409777","poster":"Billybob0604","timestamp":"1742844960.0","upvote_count":"1","content":"No , SELECT * FROM students@v4 is the same as running SELECT * FROM students VERSION AS OF 4"},{"content":"Typing error : *SELECT * FROM students@v4 VERSION AS OF 4","comment_id":"1334037","timestamp":"1735554300.0","poster":"CaoMengde09","upvote_count":"1"}],"timestamp":"1735554300.0","upvote_count":"2","poster":"CaoMengde09","content":"Selected Answer: AB\nSELECT * FROM people10m VERSION AS OF 123;\nIs identical to \nSELECT * FROM people10m@v123;\n\nso . SELECT * FROM students@v4 is the same as running . SELECT * FROM students@v4 VERSION AS OF 5\n\n[\"A\", \"B\"]"},{"timestamp":"1734783300.0","content":"Selected Answer: AB\nhttps://docs.databricks.com/en/delta/history.html#delta-time-travel-syntax","poster":"DipeshGandhi131","comment_id":"1329999","upvote_count":"2"},{"content":"Selected Answer: BD\nOption A (SELECT * FROM students@v4) is not correct because the syntax students@v4 is not valid in SQL for querying a specific version of a Delta table. The correct syntax to query a specific version of a Delta table is to use the VERSION AS OF or TIMESTAMP AS OF clauses.\n\nTherefore, the correct options are:\n\nB. SELECT * FROM students TIMESTAMP AS OF ‘2024-04-22T 14:32:47.000+00:00’\n\nD. SELECT * FROM students VERSION AS OF 5","poster":"MultiCloudIronMan","timestamp":"1734354960.0","comment_id":"1327384","upvote_count":"1"},{"poster":"Worldmaster","comment_id":"1320589","timestamp":"1733062860.0","upvote_count":"1","content":"Selected Answer: AB\nAB correct\nhttps://docs.databricks.com/en/delta/history.html"}],"url":"https://www.examtopics.com/discussions/databricks/view/152432-exam-certified-data-engineer-associate-topic-1-question-148/","isMC":true,"answer":"AB","question_images":["https://img.examtopics.com/certified-data-engineer-associate/image69.png"],"topic":"1","unix_timestamp":1733062860,"timestamp":"2024-12-01 15:21:00","question_id":53,"question_text":"The Delta transaction log for the ‘students’ tables is shown using the ‘DESCRIBE HISTORY students’ command. A Data Engineer needs to query the table as it existed before the UPDATE operation listed in the log.\n\nWhich command should the Data Engineer use to achieve this? (Choose two.)\n\n//IMG//","answer_ET":"AB","exam_id":162,"answer_description":"","answers_community":["AB (83%)","BD (17%)"],"answer_images":[],"choices":{"A":"SELECT * FROM students@v4","B":"SELECT * FROM students TIMESTAMP AS OF ‘2024-04-22T 14:32:47.000+00:00’","D":"SELECT * FROM students VERSION AS OF 5","C":"SELECT * FROM students FROM HISTORY VERSION AS OF 3","E":"SELECT * FROM students TIMESTAMP AS OF ‘2024-04-22T 14:32:58.000+00:00’"}},{"id":"Me4u56mwCkH5izMPyEgE","exam_id":162,"timestamp":"2024-12-16 14:18:00","choices":{"B":"They can schedule the query to refresh every 12 hours from the SQL endpoint's page in Databricks SQL.","D":"They can schedule the query to run every 12 hours from the Jobs UI.","C":"They can schedule the query to refresh every 1 day from the query's page in Databricks SQL.","A":"They can schedule the query to refresh every 1 day from the SQL endpoint's page in Databricks SQL."},"answers_community":["C (100%)"],"question_text":"An engineering manager uses a Databricks SQL query to monitor ingestion latency for each data source. The manager checks the results of the query every day, but they are manually rerunning the query each day and waiting for the results.\n\nWhich of the following approaches can the manager use to ensure the results of the query are updated each day?","answer_description":"","answer":"C","topic":"1","url":"https://www.examtopics.com/discussions/databricks/view/153043-exam-certified-data-engineer-associate-topic-1-question-149/","answer_images":[],"unix_timestamp":1734355080,"question_id":54,"discussion":[{"upvote_count":"2","timestamp":"1734355080.0","poster":"MultiCloudIronMan","comment_id":"1327386","content":"Selected Answer: C\nThe correct answer is C. They can schedule the query to refresh every 1 day from the query's page in Databricks SQL. This approach ensures that the query results are automatically updated each day without the need for manual intervention."}],"isMC":true,"answer_ET":"C","question_images":[]},{"id":"ILDQ79FBUW7OY6aWyErm","answers_community":["D (96%)","4%"],"timestamp":"2023-03-31 19:03:00","question_id":55,"url":"https://www.examtopics.com/discussions/databricks/view/104649-exam-certified-data-engineer-associate-topic-1-question-15/","answer_ET":"D","topic":"1","answer_images":[],"isMC":true,"answer_description":"","question_images":[],"exam_id":162,"unix_timestamp":1680282180,"choices":{"B":"An ability to work with data within certain partitions and windows","E":"An ability to work with an array of tables for procedural automation","D":"An ability to work with complex, nested data ingested from JSON files","C":"An ability to work with time-related data in specified intervals","A":"An ability to work with data in a variety of types at once"},"question_text":"Which of the following benefits is provided by the array functions from Spark SQL?","discussion":[{"comment_id":"945996","upvote_count":"11","timestamp":"1688763720.0","content":"Array functions in Spark SQL allow you to work with complex, nested data ingested from JSON files. These functions can be used to extract data from nested structures, manipulate data within nested structures, and aggregate data within nested structures.\n\nThe other options are not benefits provided by the array functions from Spark SQL.\n\nOption A: Array functions do not allow you to work with data in a variety of types at once.\nOption B: Array functions do not allow you to work with data within certain partitions and windows.\nOption C: Array functions do not allow you to work with time-related data in specified intervals.\nOption E: Array functions do not allow you to work with an array of tables for procedural automation.\nTherefore, the only benefit provided by the array functions from Spark SQL is the ability to work with complex, nested data ingested from JSON files.","poster":"Atnafu"},{"poster":"danishanis","upvote_count":"1","timestamp":"1735674060.0","comment_id":"1335020","content":"Selected Answer: D\nD is correct because: Array functions in Spark SQL are particularly useful for working with complex, nested data structures, such as those commonly found in JSON files. These functions allow you to manipulate and query arrays and nested data within your DataFrame, making it easier with Hierarchical data. \n\nOption A is not specific to array functions. Spark SQL provides the ability to work with various array functions.\nOption B is an ability related to window functions and partitioning in Spark SQL, not specifically to array functions. Window functions allow you to perform operations across a set of table rows that are somehow related to the current row\nOption C is an ability related to time functions and interval operations in Spark SQL and not specific to array functions.\nOption E is an ability not specific to array functions as Spark SQL does not provide direct support for working with an array of tables for procedural automation through array functions."},{"poster":"806e7d2","comment_id":"1314109","upvote_count":"2","content":"Selected Answer: D\nSpark SQL array functions are particularly useful for working with complex and nested data structures, such as arrays, which are often found in semi-structured data formats like JSON. These functions allow users to manipulate and process array data directly, making it easier to handle nested structures without needing to flatten them upfront.","timestamp":"1731952980.0"},{"poster":"80370eb","content":"Selected Answer: D\nD. An ability to work with complex, nested data ingested from JSON files\n\nArray functions in Spark SQL allow you to work with complex and nested data structures, such as those found in JSON files, enabling operations on arrays and nested elements.","comment_id":"1262401","upvote_count":"2","timestamp":"1723104300.0"},{"comment_id":"1249379","content":"D is the correct one","poster":"ranjan24","timestamp":"1721191200.0","upvote_count":"1"},{"timestamp":"1720691160.0","content":"The correct Answer is D","poster":"ranjan24","upvote_count":"1","comment_id":"1246043"},{"timestamp":"1720467720.0","content":"Selected Answer: D\nThe correct answer is D.","comment_id":"1244540","upvote_count":"1","poster":"3fbc31b"},{"content":"Selected Answer: D\nD is the right answeer","upvote_count":"1","comment_id":"1213172","timestamp":"1716015840.0","poster":"BharaniRaj"},{"timestamp":"1712210580.0","upvote_count":"1","comment_id":"1189114","poster":"benni_ale","content":"Selected Answer: D\ni thought sql arrays are usually seen in json files read"},{"poster":"Itmma","upvote_count":"1","timestamp":"1710841980.0","content":"Selected Answer: E\nE is correct","comment_id":"1177196"},{"poster":"SerGrey","upvote_count":"1","comment_id":"1113195","timestamp":"1704322440.0","content":"Correct answer is D"},{"timestamp":"1703883480.0","comment_id":"1109094","upvote_count":"4","poster":"Garyn","content":"Selected Answer: D\nD. An ability to work with complex, nested data ingested from JSON files\n\nArray functions in Spark SQL enable users to work efficiently with arrays and complex, nested data structures that are often ingested from JSON files or other nested data formats. These functions allow manipulation, querying, and extraction of elements from arrays and nested structures within the dataset, facilitating operations on complex data types within Spark SQL."},{"poster":"Huroye","comment_id":"1071089","content":"Correct answer is D. Array provides complex nesting of data and it is easy to query. That's why we use arrays for definding data domains.","upvote_count":"1","timestamp":"1700023380.0"},{"content":"Selected Answer: D\nD is correct","upvote_count":"1","poster":"awofalus","comment_id":"1064800","timestamp":"1699362120.0"},{"comment_id":"1028779","timestamp":"1696849500.0","poster":"VijayKula","upvote_count":"1","content":"Selected Answer: D\nD is the correct answer"},{"upvote_count":"1","poster":"chris_mach","content":"Selected Answer: D\narray functions allow you to work with JSON data","timestamp":"1695966840.0","comment_id":"1020506"},{"comment_id":"1017353","content":"Selected Answer: D\nD is right ans","upvote_count":"1","poster":"KalavathiP","timestamp":"1695698100.0"},{"upvote_count":"3","poster":"vctrhugo","comment_id":"997919","timestamp":"1693770240.0","content":"Selected Answer: D\nD. An ability to work with complex, nested data ingested from JSON files\n\nArray functions in Spark SQL are primarily used for working with arrays and complex, nested data structures, such as those often encountered when ingesting JSON files. These functions allow you to manipulate and query nested arrays and structures within your data, making it easier to extract and work with specific elements or values within complex data formats.\n\nWhile some of the other options (such as option A for working with different data types) are features of Spark SQL or SQL in general, array functions specifically excel at handling complex, nested data structures like those found in JSON files."},{"upvote_count":"2","poster":"prasioso","content":"Selected Answer: D\nCorrect answer is D. Spark SQL Array functions allow us to work with nested datasets in JSON files","comment_id":"897238","timestamp":"1684035060.0"},{"timestamp":"1682054160.0","upvote_count":"1","content":"Option D","poster":"Varma_Saraswathula","comment_id":"876212"},{"content":"Option D","comment_id":"875870","upvote_count":"1","poster":"naxacod574","timestamp":"1682014260.0"},{"timestamp":"1680583800.0","comment_id":"860636","content":"option D","poster":"sdas1","upvote_count":"1"},{"poster":"surrabhi_4","content":"Selected Answer: D\noption D","upvote_count":"1","timestamp":"1680509460.0","comment_id":"859669"},{"poster":"knivesz","upvote_count":"1","content":"Selected Answer: D\nCorrect answer is D","comment_id":"858876","timestamp":"1680442380.0"},{"poster":"XiltroX","upvote_count":"2","timestamp":"1680358020.0","content":"Selected Answer: D\nCorrect answer is D. Arrays are nested datasets in JSON files","comment_id":"857997"},{"content":"it should be D","comments":[],"comment_id":"857308","poster":"sguzel","upvote_count":"1","timestamp":"1680282180.0"}],"answer":"D"}],"exam":{"provider":"Databricks","numberOfQuestions":169,"id":162,"isImplemented":true,"isMCOnly":true,"name":"Certified Data Engineer Associate","isBeta":false,"lastUpdated":"12 Apr 2025"},"currentPage":11},"__N_SSP":true}