{"pageProps":{"questions":[{"id":"rx06hqLpixtxCFW96LI0","question_id":136,"topic":"1","answer_images":[],"unix_timestamp":1683107280,"choices":{"D":"The broadcast() operation will only perform a broadcast join if the Spark property spark.sql.autoBroadcastJoinThreshold is manually set.","A":"The larger DataFrame employeesDF is being broadcasted rather than the smaller DataFrame storesDF.","C":"The entire line of code should be wrapped in broadcast() rather than just DataFrame employeesDF.","E":"Only one of the DataFrames is being broadcasted rather than both of the DataFrames.","B":"There is never a need to call the broadcast() operation in Apache Spark 3."},"discussion":[{"poster":"juliom6","content":"Selected Answer: A\nA si correct:\n\n# https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.broadcast.html\n\nfrom pyspark.sql import types\nfrom pyspark.sql.functions import broadcast\n\ndf = spark.createDataFrame([1, 2, 3, 3, 4], types.IntegerType())\ndf_small = spark.range(3)\ndf.join(broadcast(df_small), df.value == df_small.id).show()","comment_id":"1070640","upvote_count":"1","timestamp":"1731601680.0"},{"comment_id":"888349","poster":"4be8126","upvote_count":"2","timestamp":"1714729680.0","content":"Selected Answer: A\nThe answer is A.\n\nThe logical error in the code block is that the larger DataFrame, employeesDF, is being broadcasted instead of the smaller DataFrame, storesDF. This defeats the purpose of a broadcast join, which is to optimize performance by broadcasting the smaller DataFrame to all the worker nodes, avoiding the need to shuffle data over the network.\n\nTo perform a broadcast join efficiently, the smaller DataFrame should be broadcasted, which in this case is storesDF. The corrected code should be:\n\nbroadcast(storesDF).join(employeesDF, \"storeId\")"}],"answer":"A","answers_community":["A (100%)"],"answer_description":"","exam_id":161,"question_images":[],"question_text":"The below code block contains a logical error resulting in inefficiency. The code block is intended to efficiently perform a broadcast join of DataFrame storesDF and the much larger DataFrame employeesDF using key column storeId. Identify the logical error.\nCode block:\nstoresDF.join(broadcast(employeesDF), \"storeId\")","url":"https://www.examtopics.com/discussions/databricks/view/108380-exam-certified-associate-developer-for-apache-spark-topic-1/","isMC":true,"timestamp":"2023-05-03 11:48:00","answer_ET":"A"},{"id":"wmKWJes1gZ4gPPKO7RLJ","exam_id":161,"answer_ET":"C","url":"https://www.examtopics.com/discussions/databricks/view/105657-exam-certified-associate-developer-for-apache-spark-topic-1/","question_id":137,"question_images":[],"choices":{"C":"A cross join is not implemented by the DataFrame.join()operation – the DataFrame.crossJoin()operation should be used instead.","E":"A cross join is not implemented by the DataFrame.join() operations – the standalone join() operation should be used instead.","D":"There is no key column specified – the key column \"storeId\" should be the second argument.","B":"There is no direct cross join in Spark, but it can be implemented by performing an outer join on all columns of both DataFrames.","A":"A cross join is not implemented by the DataFrame.join() operations – the standalone CrossJoin() operation should be used instead."},"discussion":[{"poster":"mineoolee","upvote_count":"1","comment_id":"1327223","timestamp":"1734338280.0","content":"Selected Answer: D\nit is wokring\ndata = [\n (0, 2, 1100746394),\n (1, 2, 1474410343)\n]\n\ndf = spark.createDataFrame(\n data, \n ['storeId','a', 'openDate']\n)\n\n\n_data = [\n ('a', 2, 4444444444),\n ('c', 2, None),\n ('b', None, 2222222222)\n]\n\n_df = spark.createDataFrame(\n _data, \n ['storeId','a', 'openDate']\n)\n\ndf.join(_df, 'a', \"cross\").show()","comments":[{"poster":"mineoolee","comments":[{"upvote_count":"1","timestamp":"1736428020.0","poster":"Kalipe","content":"it's wrong, it doesn't work or you obviously haven't try it","comment_id":"1338347"}],"comment_id":"1327224","timestamp":"1734338340.0","content":"also, df.join(_df, '\"cross\").show() is working","upvote_count":"1"}]},{"upvote_count":"2","content":"Selected Answer: C\nCross Join in PySpark: A cross join (also known as a Cartesian product) returns the Cartesian product of the two DataFrames, meaning every row from the first DataFrame is paired with every row from the second DataFrame. In PySpark, the crossJoin() method is used specifically for this type of join.","comment_id":"1268799","poster":"oussa_ama","timestamp":"1724080740.0"},{"content":"Selected Answer: C\nThe correct identification of the error is:\n\nC. A cross join is not implemented by the DataFrame.join() operation – the DataFrame.crossJoin() operation should be used instead.\n\nExplanation:\n\nIn Spark, to perform a cross join between two DataFrames, you should use the crossJoin() method, not the join() method with the \"cross\" argument.","upvote_count":"1","poster":"65bd33e","comment_id":"1265456","timestamp":"1723596900.0"},{"upvote_count":"2","poster":"Ahlo","content":"Correct answer C \nfrom pyspark.sql import Row\ndf = spark.createDataFrame(\n [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\ndf2 = spark.createDataFrame(\n [Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\ndf.crossJoin(df2.select(\"height\")).select(\"age\", \"name\", \"height\").show()\n\nhttps://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.crossJoin.html","comment_id":"1159588","timestamp":"1708942740.0"},{"poster":"azure_bimonster","timestamp":"1707426360.0","comment_id":"1144910","content":"Selected Answer: D\nD is the answer here as key is missing. As per syntax, key is needed.","upvote_count":"2"},{"upvote_count":"3","content":"Selected Answer: C\nC is correct.\n\n# https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.crossJoin.html\n\na = spark.createDataFrame([(1, 2), (3, 4)], ['column1', 'column2'])\nb = spark.createDataFrame([(5, 6), (7, 8)], ['column3', 'column4'])\n\ndf = a.crossJoin(b)\ndisplay(df)","timestamp":"1699979760.0","poster":"juliom6","comment_id":"1070649"},{"comment_id":"1064776","upvote_count":"4","comments":[{"content":"Totally agree. The stament in answer C \"A cross join is not implemented by the DataFrame.join()operation\" is incorrect. It is implemented and I have tested it. Results below:\n products_df = spark.table('products')\norders_df = spark.table('orders')\nprint(products_df.count()) -> 200\nprint(orders_df.count()) -> 2140\ncross_joined_df = products_df.join(orders_df, None, \"cross\")\nprint(cross_joined_df.count()) -> 428000","upvote_count":"1","comment_id":"1347324","poster":"tmz1","timestamp":"1737968760.0"}],"poster":"newusername","timestamp":"1699360200.0","content":"Selected Answer: D\nI know it looks confusing to have key column for cross join, but it ijoin method syntaxis: \nhttps://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.join.html\n\nsee example below : \ndataA = [Row(column1=1, column2=2), Row(column1=2, column2=4), Row(column1=3, column2=6)]\ndfA = spark.createDataFrame(dataA)\n\n# Sample data for DataFrame 'b'\ndataB = [Row(column1=1, column2=2), Row(column1=2, column2=5), Row(column1=3, column2=4)]\ndfB = spark.createDataFrame(dataB)\n\njoinedDF = dfA.join(dfB, on=None, how=\"cross\")\njoinedDF.show()\n\nit is possible to do Cross join this way as well DataFrame.crossJoin() but answer C states that df.join () doesn't do cross, which is wrong."},{"poster":"4be8126","upvote_count":"2","comment_id":"888358","timestamp":"1683107520.0","content":"Selected Answer: C\nC. A cross join is not implemented by the DataFrame.join()operation – the DataFrame.crossJoin()operation should be used instead."},{"comment_id":"869716","timestamp":"1681414980.0","comments":[{"comment_id":"888356","poster":"4be8126","timestamp":"1683107460.0","content":"No, the issue is not that the key column is missing. In a cross join, there is no key column to join on. The correct answer is C: a cross join is not implemented by the DataFrame.join() operation – the DataFrame.crossJoin() operation should be used instead.","upvote_count":"1"}],"poster":"peekaboo15","content":"cross join doesn't need a key. Answer is C","upvote_count":"2"},{"timestamp":"1681044660.0","comments":[{"comment_id":"916539","upvote_count":"2","content":"completely wrong.\njoin(other, on=None, how=None)\nJoins with another DataFrame, using the given join expression.\n[source]\nParameters:\nother – Right side of the join\non – a string for the join column name, a list of column names, a join expression (Column), or a list of Columns. If on is a string or a list of strings indicating the name of the join column(s), the column(s) must exist on both sides, and this performs an equi-join.\nhow – str, default inner. Must be one of: inner, cross, outer, full, fullouter, full_outer, left, leftouter, left_outer, right, rightouter, right_outer, semi, leftsemi, left_semi, anti, leftanti and left_anti.","comments":[{"comments":[{"comment_id":"1064774","upvote_count":"1","content":"ZSun is as always right. 4be8126 - it is not a problem to use gpt, but check its answers. Otherwise do not post it anywhere.","poster":"newusername","timestamp":"1699359900.0"}],"timestamp":"1686075900.0","content":"you can specify cross in dataframe.join( how = 'cross')\nthe reason why this code block doesn't work, because the second parameter is on. You need to specify the key column and then use how = 'cross'.\notherwise, the function will regard 'cross' for 'on' instead of 'how'","comment_id":"916540","upvote_count":"2","poster":"ZSun"}],"timestamp":"1686075780.0","poster":"ZSun"}],"content":"Key is missing. Answer is D.","comment_id":"865502","poster":"ronfun","upvote_count":"4"}],"isMC":true,"topic":"1","unix_timestamp":1681044660,"timestamp":"2023-04-09 14:51:00","answers_community":["C (53%)","D (47%)"],"answer":"C","question_text":"The code block shown below contains an error. The code block is intended to return a new DataFrame that is the result of a cross join between DataFrame storesDF and DataFrame employeesDF. Identify the error.\nCode block:\nstoresDF.join(employeesDF, \"cross\")","answer_images":[],"answer_description":""},{"id":"uABLbrnCeLwLqQ99OmpD","question_images":[],"answer_images":[],"unix_timestamp":1683107640,"topic":"1","choices":{"D":"The unionByName() operation is a standalone operation rather than a method of DataFrame – it should have both DataFrames as arguments.","E":"There are no column positions specified – the desired column positions should be the second argument.","B":"There are no key columns specified – similar column names should be the second argument.","C":"The DataFrame.unionByName() operation does not union DataFrames based on column position – it uses column name instead.","A":"There is no DataFrame.unionByName() operation – the concat() operation should be used instead with both DataFrames as arguments."},"answers_community":["C (100%)"],"answer_description":"","timestamp":"2023-05-03 11:54:00","exam_id":161,"discussion":[{"upvote_count":"1","content":"Selected Answer: C\nC is correct according to documentation:\nhttps://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.unionByName.html\n\n\"The difference between this function and union() is that this function resolves columns by name (not by position)\"","timestamp":"1731602580.0","comment_id":"1070656","poster":"juliom6"},{"content":"Selected Answer: C\nC is correct - https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.unionByName.html","upvote_count":"1","poster":"newusername","timestamp":"1730983140.0","comment_id":"1064784"},{"upvote_count":"3","poster":"4be8126","timestamp":"1714730040.0","content":"Selected Answer: C\nThe error in the code block is:\n\nC. The DataFrame.unionByName() operation does not union DataFrames based on column position – it uses column name instead.\n\nThe unionByName() operation performs a position-wise union based on column names, not based on column positions. Therefore, the error in the code block is that the intended operation should be union(), which performs a position-wise union regardless of column names.\n\nThe correct code block to perform a position-wise union between DataFrame storesDF and DataFrame acquiredStoresDF would be:\n\nstoresDF.union(acquiredStoresDF)","comment_id":"888364"}],"isMC":true,"question_text":"The code block shown below contains an error. The code block is intended to return a new DataFrame that is the result of a position-wise union between DataFrame storesDF and DataFrame acquiredStoresDF. Identify the error.\nCode block:\nstoresDF.unionByName(acquiredStoresDF)","question_id":138,"url":"https://www.examtopics.com/discussions/databricks/view/108382-exam-certified-associate-developer-for-apache-spark-topic-1/","answer_ET":"C","answer":"C"},{"id":"NoGLMsLw5EWt6Tks3SBB","unix_timestamp":1683107760,"answer_ET":"B","answers_community":["B (100%)"],"timestamp":"2023-05-03 11:56:00","question_images":[],"answer":"B","question_id":139,"url":"https://www.examtopics.com/discussions/databricks/view/108384-exam-certified-associate-developer-for-apache-spark-topic-1/","discussion":[{"upvote_count":"1","poster":"juliom6","content":"Selected Answer: B\nB is correct according documentation:\n\nhttps://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.json.html","comment_id":"1070665","timestamp":"1731603060.0"},{"timestamp":"1714730160.0","content":"Selected Answer: B\nThe correct answer is B.\n\nExplanation:\n\nThe write method is used to write a DataFrame to a file system in various formats.\nThe json method specifies that the output format should be JSON.\nThe filePath argument specifies the location to write the output file.\nOption A is incorrect because option requires a key-value pair (e.g., option(\"key\", \"value\")).\n\nOption C is incorrect because path is not a valid option for write.\n\nOption D is incorrect because write method requires a format argument to specify the output format.\n\nOption E is a valid option, but the parentheses after write are unnecessary.","comment_id":"888366","upvote_count":"1","poster":"4be8126"}],"isMC":true,"answer_description":"","topic":"1","choices":{"A":"storesDF.write.option(\"json\").path(filePath)","E":"storesDF.write().json(filePath)","C":"storesDF.write.path(filePath)","B":"storesDF.write.json(filePath)","D":"storesDF.write(filePath)"},"question_text":"Which of the following code blocks writes DataFrame storesDF to file path filePath as JSON?","exam_id":161,"answer_images":[]},{"id":"3Cq2sDLwTyollAboq29O","timestamp":"2023-11-07 14:39:00","topic":"1","discussion":[{"content":"Selected Answer: C\nC is correct:\n\nhttps://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.parquet.html","timestamp":"1731608040.0","comment_id":"1070723","upvote_count":"1","poster":"juliom6"},{"poster":"newusername","content":"Selected Answer: C\nCorrect","timestamp":"1730986740.0","comment_id":"1064825","upvote_count":"1"}],"question_images":[],"exam_id":161,"answer_description":"","url":"https://www.examtopics.com/discussions/databricks/view/125577-exam-certified-associate-developer-for-apache-spark-topic-1/","question_text":"In what order should the below lines of code be run in order to write DataFrame storesDF to file path filePath as parquet and partition by values in column division?\nLines of code:\n1. .write() \\\n2. .partitionBy(\"division\") \\\n3. .parquet(filePath)\n4. .storesDF \\\n5. .repartition(\"division\")\n6. .write \\\n7. .path(filePath, \"parquet\")","answers_community":["C (100%)"],"isMC":true,"unix_timestamp":1699364340,"choices":{"B":"4, 1, 5, 7","D":"4, 1, 5, 3","E":"4, 6, 2, 7","A":"4, 1, 2, 3","C":"4, 6, 2, 3"},"answer_ET":"C","question_id":140,"answer":"C","answer_images":[]}],"exam":{"isMCOnly":true,"name":"Certified Associate Developer for Apache Spark","isImplemented":true,"lastUpdated":"12 Apr 2025","provider":"Databricks","isBeta":false,"id":161,"numberOfQuestions":185},"currentPage":28},"__N_SSP":true}