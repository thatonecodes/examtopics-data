{"pageProps":{"questions":[{"id":"S9crePGbIfGIhbaKyCBi","answer_description":"","url":"https://www.examtopics.com/discussions/databricks/view/132441-exam-certified-data-engineer-professional-topic-1-question/","answer_ET":"A","question_text":"A Delta Lake table was created with the below query:\n//IMG//\n\nRealizing that the original query had a typographical error, the below code was executed:\nALTER TABLE prod.sales_by_stor RENAME TO prod.sales_by_store\nWhich result will occur after running the second command?","isMC":true,"choices":{"B":"The table name change is recorded in the Delta transaction log.","C":"All related files and metadata are dropped and recreated in a single ACID transaction.","E":"A new Delta transaction log Is created for the renamed table.","A":"The table reference in the metastore is updated and no data is changed.","D":"The table reference in the metastore is updated and all data files are moved."},"answer_images":[],"unix_timestamp":1706613960,"exam_id":163,"question_images":["https://img.examtopics.com/certified-data-engineer-professional/image27.png"],"answers_community":["A (100%)"],"topic":"1","timestamp":"2024-01-30 12:26:00","discussion":[{"upvote_count":"8","content":"Selected Answer: A\ndid a test. No data is changed. dir & filename not changed. the rename is not recorded in transition log neither.","comment_id":"1167199","timestamp":"1725623940.0","poster":"hal2401me"},{"timestamp":"1725572280.0","comments":[{"comment_id":"1335320","poster":"arekm","content":"Transaction log captures operations on the data (including adding columns, renaming them). This operation is a change of the name of the external table - just a change in the metastore.","timestamp":"1735777020.0","upvote_count":"1"}],"upvote_count":"2","poster":"Tamele001","content":"B is the correct answer. When you alter a table name in Delta Lake, the change is logged in the transaction log that Delta Lake uses to maintain a versioned history of all changes to the table. This is how Delta Lake maintains ACID properties and ensures a consistent view of the data. The transaction log is key to supporting features like time travel, auditing, and rollbacks in Delta Lake. The metadata and the actual data remain intact, and the reference to the table in the metastore is updated to reflect the new name.","comment_id":"1166820"},{"content":"Selected Answer: A\nA is Correct","timestamp":"1722331560.0","poster":"adenis","comment_id":"1135723","upvote_count":"4"}],"answer":"A","question_id":156},{"id":"AEGUCcKZpEIGyDwGTOGW","answer_ET":"E","answer_description":"","question_id":157,"question_images":["https://img.examtopics.com/certified-data-engineer-professional/image2.png"],"exam_id":163,"topic":"1","discussion":[{"poster":"benni_ale","comment_id":"1294571","content":"Selected Answer: E\nShave like a bomber","timestamp":"1728364860.0","upvote_count":"1"},{"upvote_count":"4","comment_id":"1290646","timestamp":"1727525280.0","poster":"akashdesarda","content":"Selected Answer: E\nWhatever we read using dbutls.secret module is always printed as '[REDACTED]', but when consumed in code, underlying vales are passed."},{"comment_id":"1224437","upvote_count":"1","poster":"imatheushenrique","content":"E. The connection to the external table will succeed; the string \"REDACTED\" will be printed.","timestamp":"1717547400.0"},{"timestamp":"1707393000.0","comment_id":"1144395","poster":"PrashantTiwari","upvote_count":"1","content":"E is correct"},{"timestamp":"1705862880.0","poster":"AziLa","upvote_count":"1","comment_id":"1128062","content":"correct ans is E"},{"upvote_count":"2","timestamp":"1705148040.0","content":"Selected Answer: E\nE is correct","poster":"Jay_98_11","comment_id":"1121593"},{"content":"Selected Answer: E\nE is correct","poster":"ATLTennis","upvote_count":"2","comment_id":"1114981","timestamp":"1704517260.0"},{"timestamp":"1703172840.0","poster":"kz_data","content":"Selected Answer: E\nCorrect answer E","comment_id":"1102676","upvote_count":"2"},{"timestamp":"1697004840.0","poster":"sturcu","comment_id":"1040236","content":"Selected Answer: E\nCorrect: https://docs.databricks.com/en/external-data/jdbc.html","upvote_count":"4"},{"content":"https://learn.microsoft.com/en-us/azure/databricks/security/secrets/redaction\n\nOption E - which is selected answer seems correct.","upvote_count":"3","timestamp":"1692000900.0","comment_id":"980601","poster":"Brian9"},{"poster":"8605246","comment_id":"973574","timestamp":"1691303280.0","upvote_count":"1","content":"This option is correct, although the password won't be printed out, the connection will still succeed."}],"answers_community":["E (100%)"],"url":"https://www.examtopics.com/discussions/databricks/view/117453-exam-certified-data-engineer-professional-topic-1-question-6/","timestamp":"2023-08-06 08:28:00","answer":"E","isMC":true,"unix_timestamp":1691303280,"answer_images":[],"question_text":"The security team is exploring whether or not the Databricks secrets module can be leveraged for connecting to an external database.\nAfter testing the code with all Python variables being defined with strings, they upload the password to the secrets module and configure the correct permissions for the currently active user. They then modify their code to the following (leaving all other variables unchanged).\n//IMG//\n\nWhich statement describes what will happen when the above code is executed?","choices":{"B":"An interactive input box will appear in the notebook; if the right password is provided, the connection will succeed and the encoded password will be saved to DBFS.","C":"An interactive input box will appear in the notebook; if the right password is provided, the connection will succeed and the password will be printed in plain text.","D":"The connection to the external table will succeed; the string value of password will be printed in plain text.","A":"The connection to the external table will fail; the string \"REDACTED\" will be printed.","E":"The connection to the external table will succeed; the string \"REDACTED\" will be printed."}},{"id":"UCAiGnwsC7FtrRe2PPSq","answers_community":["C (49%)","A (43%)","9%"],"question_images":["https://img.examtopics.com/certified-data-engineer-professional/image28.png"],"exam_id":163,"answer_images":[],"question_id":158,"timestamp":"2023-09-10 21:44:00","topic":"1","isMC":true,"choices":{"A":"Implement the appropriate aggregate logic as a batch read against the daily_store_sales table and overwrite the store_sales_summary table with each Update.","B":"Implement the appropriate aggregate logic as a batch read against the daily_store_sales table and append new rows nightly to the store_sales_summary table.","D":"Implement the appropriate aggregate logic as a Structured Streaming read against the daily_store_sales table and use upsert logic to update results in the store_sales_summary table.","C":"Implement the appropriate aggregate logic as a batch read against the daily_store_sales table and use upsert logic to update results in the store_sales_summary table.","E":"Use Structured Streaming to subscribe to the change data feed for daily_store_sales and apply changes to the aggregates in the store_sales_summary table with each update."},"url":"https://www.examtopics.com/discussions/databricks/view/120443-exam-certified-data-engineer-professional-topic-1-question/","question_text":"The data engineering team maintains a table of aggregate statistics through batch nightly updates. This includes total sales for the previous day alongside totals and averages for a variety of time periods including the 7 previous days, year-to-date, and quarter-to-date. This table is named store_saies_summary and the schema is as follows:\n//IMG//\n\nThe table daily_store_sales contains all the information needed to update store_sales_summary. The schema for this table is: store_id INT, sales_date DATE, total_sales FLOAT\nIf daily_store_sales is implemented as a Type 1 table and the total_sales column might be adjusted after manual data auditing, which approach is the safest to generate accurate reports in the store_sales_summary table?","unix_timestamp":1694375040,"answer_ET":"C","answer":"C","discussion":[{"timestamp":"1694375040.0","content":"The answer should be A. it is the safest to generate accurate report","comment_id":"1004270","poster":"hammer_1234_h","comments":[{"comment_id":"1360978","poster":"djohn_prasad","timestamp":"1740392280.0","upvote_count":"1","content":"But what if each batch did not have historical data but only updates? Then doing an overwrite will risk removing historical data in the target table so Option C is much preferable"},{"upvote_count":"1","comment_id":"1099519","content":"Incorrect BATCH processing and OVERWRITE will give partial results","poster":"alexvno","timestamp":"1702887180.0"},{"upvote_count":"3","timestamp":"1706076360.0","content":"This is confusing: \"overwrite the store_sales_summary table with each Update.\" sounds like it is only doing updates, not inserting new possible stories.","poster":"Def21","comment_id":"1130283"}],"upvote_count":"12"},{"upvote_count":"1","timestamp":"1743072960.0","content":"people that claim type 1 scd is not maintained by upsert need to make some more reading before posting anything here...","comment_id":"1410848","poster":"ultimomassimo"},{"upvote_count":"1","timestamp":"1741351500.0","content":"Selected Answer: C\nIt makes no sense to recalculate all aggregations for all historical data and then overwrite. It would lead to a non scalable solution because all data should be recalculated to not \"delete\" rows in target table. Option C lets us filter by date ranges or even partitions to merge only target periods and get the same results in a much more performant, safer and cheaper way.","comment_id":"1366256","poster":"Jamuro"},{"content":"Selected Answer: C\nThe answer is C. Option A is correct in ensuring accuracy, as it recalculates the entire store_sales_summary table based on the full historical data in daily_store_sales. However, it is computationally expensive and may not scale well.\nOption C (upsert logic) could be a better choice in most real-world scenarios, as it focuses only on the records that have changed, reducing computational costs and minimizing disruption for downstream systems.","poster":"AlejandroU","timestamp":"1734490560.0","upvote_count":"1","comments":[{"timestamp":"1735777620.0","comment_id":"1335321","poster":"arekm","content":"How do you know which records have changed? I think A is the safest answer.","comments":[{"timestamp":"1740392400.0","comment_id":"1360980","content":"Option C tells us based on upsert logic where matching records (matching key assumed as storeid and sales date) are checked for differences in sales values and the value from batch is taken over that of the target table. C is computational much easier then","upvote_count":"1","poster":"djohn_prasad"}],"upvote_count":"1"}],"comment_id":"1328241"},{"upvote_count":"3","timestamp":"1734068100.0","comments":[{"poster":"ultimomassimo","comment_id":"1410849","content":"you dont know what is scd type 1 , do you? you dont understand the difference between overwriting a column value and overwriting the whole table, do you?","upvote_count":"1","timestamp":"1743073080.0"}],"poster":"Sriramiyer92","comment_id":"1326022","content":"Selected Answer: A\nA it is\nSCD Type 1, so clearly Append and Upsert logic should not be used."},{"timestamp":"1733413500.0","poster":"benni_ale","upvote_count":"1","content":"Selected Answer: A\nA as the table does not require history","comment_id":"1322419"},{"poster":"shaojunni","upvote_count":"1","content":"Selected Answer: A\ndaily_store_sales is type1 table, no history is maintained. You have to treat every record as new record and do aggregation for every store. Overwrite is much efficient than upsert.","comment_id":"1296208","timestamp":"1728671760.0"},{"timestamp":"1719329580.0","comment_id":"1236963","content":"Selected Answer: C\nI will go with c. upsert","upvote_count":"3","poster":"Ati1362"},{"upvote_count":"4","timestamp":"1716983460.0","comment_id":"1220888","comments":[{"timestamp":"1723736580.0","poster":"fe3b2fc","comment_id":"1266536","content":"Incorrect. The daily store sales table contains all of the history needed to update the table. The summary table holds no historical records. Seeing as this is a nightly job, any manual changes made to daily store sales will be captured. A is the correct answer.","upvote_count":"2"}],"poster":"MDWPartners","content":"Selected Answer: C\nA is not correct because the table is daily. If you overwrite you delete all history. You need to insert/update to keep history."},{"content":"Selected Answer: A\nNot sure if that's right but I would go for A. What do you think?\n\nType1: Data is overwritten\nType 2: History is maintained, new data is inserted as new rows\nType 3: Stores two versions per record: a previous and a current value\n\nA. batch + overwrite -> Match Type 1 requirements. YES\nB: batch + append new rows -> Would be for type 2. NO\nC. Batch + Upsert -> Data is not being overwritten (which is required for Type 1). NO\nD. ReadStream + Upsert -> Data is not being overwritten (which is required for Type 1). NO\nE. Change Data Feed to update -> Problem is manual edits + not overwriting (required for type 1). No\n\nI have doubts around \"which approach is the safest\". Maybe because due to some manual changes it is hard to track changes or do upsert, so to make sure that the stats are right \n overwriting is safer.","upvote_count":"4","comment_id":"1195899","timestamp":"1713169020.0","poster":"ThoBustos"},{"timestamp":"1709981160.0","content":"Selected Answer: C\nNot A because overwriting will only provide a daily based data not the history of it.\nNot B because it will not fix the issue of incorrect sales amount\nAs these data are fit for natch processing so neither D or E.\nC will only upsert the changes while making sure we are updating the records based on sales_date & store_id","comment_id":"1169411","upvote_count":"2","poster":"vikram12apr"},{"comment_id":"1136213","upvote_count":"1","content":"E definitely because it say that the total_sales column may be change by manual auditing so not via a job, so streaming with CDF is the only option here !","poster":"Rinscy","timestamp":"1706652540.0"},{"timestamp":"1706636940.0","upvote_count":"3","content":"Selected Answer: A\nI would go with Option A. \nBecause it has manual auditing hence values can change. Uses type 1 hence replace original data","poster":"Somesh512","comment_id":"1135999"},{"content":"Selected Answer: E\nIt should be E, as structure streaming has built-in fault-tolerance feature.","timestamp":"1706297940.0","poster":"spaceexplorer","comment_id":"1132830","upvote_count":"1"},{"timestamp":"1706216700.0","poster":"Rinscy","upvote_count":"2","content":"It said type 1 so A is the correct answer !","comment_id":"1132061"},{"comment_id":"1110895","upvote_count":"2","poster":"divingbell17","content":"The question is unclear whether the aggregated table needs to support a rolling history. Note the aggregated table does not have a date column to distinguish which date the summary is generated for so one could assume the table is maintained only for the current snapshot.\n\nAssuming the above - A would be the safest option as all stores and aggregates would need to be refreshed nightly","timestamp":"1704067620.0"},{"comments":[{"poster":"Def21","upvote_count":"1","timestamp":"1706076180.0","comment_id":"1130279","content":"\"Safest\" probably includes having Delta table. And history is maintained anyway."}],"comment_id":"1108032","timestamp":"1703787420.0","content":"Selected Answer: A\nA is correct because it's a static table that is written nightly through a batch job. The summary table does not maintain history and so an upsert results in having extra, unecessary records. Overwrite it nightly with updated aggregates for the required time period.","poster":"dmov","upvote_count":"3"},{"comment_id":"1107219","poster":"Luv4data","upvote_count":"3","timestamp":"1703712240.0","content":"The answer is A. Note that the target table has columns which stores quarter to date,previous day sates etc, which will result in daily updates, i.e. large volume of records will be updated, hence better to overwirte than to update large volume of records."},{"comment_id":"1099520","content":"Selected Answer: C\nBatch processing so you need to update and insert - C","upvote_count":"3","poster":"alexvno","timestamp":"1702887240.0"},{"upvote_count":"3","comments":[{"timestamp":"1701852240.0","poster":"Gulenur_GS","content":"You are absolutely right!","comment_id":"1089092","upvote_count":"1"}],"poster":"Enduresoul","timestamp":"1701016920.0","comment_id":"1080817","content":"Selected Answer: C\nAnswer C is correct. \nAnswer E would do the job too, but the table schema and the question indicates, that there will be only one update daily needed. Therefore a structured streaming job is way too expensive to archive the outcome."},{"comment_id":"1062321","upvote_count":"1","poster":"Syd","timestamp":"1699126560.0","content":"Correct answer A\nType 1 data is overwritten \nhttps://streamsets.com/blog/slowly-changing-dimensions-vs-change-data-capture/#:~:text=In%20Type%201%2C%20any%20new,change%20to%20maintain%20a%20history."},{"comments":[{"timestamp":"1699181520.0","upvote_count":"1","content":"manual data auditing, implies we do not know when a change is made, hence we do not know when to schedule the \"batch update\" for the aggregated table","comment_id":"1062777","poster":"sturcu"}],"upvote_count":"2","comment_id":"1044902","timestamp":"1697455140.0","poster":"sturcu","content":"Selected Answer: E\nI would say that it is E. If daily_store_sales table is implemented as a Type 1 table, this means that values are overwritten, and we do not keep the history. So we would need to create a streaming from CDF and apply those changes into the aggregated table."}],"answer_description":""},{"id":"tmkarXIgodNeHCWFJwEv","timestamp":"2023-11-03 09:07:00","unix_timestamp":1698998820,"topic":"1","question_id":159,"answer":"E","exam_id":163,"isMC":true,"answer_ET":"E","question_text":"A member of the data engineering team has submitted a short notebook that they wish to schedule as part of a larger data pipeline. Assume that the commands provided below produce the logically correct results when run as presented.\n\n//IMG//\n\n\nWhich command should be removed from the notebook before scheduling it as a job?","discussion":[{"timestamp":"1701370620.0","comment_id":"1084674","upvote_count":"9","poster":"petrv","content":"Selected Answer: E\nWhen scheduling a Databricks notebook as a job, it's generally recommended to remove or modify commands that involve displaying output, such as using the display() function. Displaying data using display() is an interactive feature designed for exploration and visualization within the notebook interface and may not work well in a production job context.\n\nThe finalDF.explain() command, which provides the execution plan of the DataFrame transformations and actions, is often useful for debugging and optimizing queries. While it doesn't display interactive visualizations like display(), it can still be informative for understanding how Spark is executing the operations on your DataFrame."},{"content":"Selected Answer: D\nCmd 5 (finalDF.explain()) is used for debugging and understanding the logical and physical plans of a DataFrame. It provides insights into how Spark plans to execute the query but does not produce output that is necessary for the scheduled job. Including this command in a scheduled job is unnecessary and could clutter the job logs without adding value to the final output.","comments":[{"comment_id":"1335322","upvote_count":"1","poster":"arekm","content":"display() is more costly operation than finalDF.explain(). The DataFrame might contain millions of rows that you would be trying to print out each time the notebook is run.","timestamp":"1735777980.0"}],"timestamp":"1729902360.0","poster":"Carkeys","comment_id":"1303045","upvote_count":"1"},{"timestamp":"1729248180.0","content":"Selected Answer: E\nif i was multiple solutions than i would have gone for .explain method and print schema as well as they do not contribute in any sort of ETL operation but as a rule of thumb display should always be omitted first so -> E","upvote_count":"1","poster":"benni_ale","comment_id":"1299625"},{"timestamp":"1723643340.0","comment_id":"1265803","poster":"71dfab9","upvote_count":"1","content":"Selected Answer: E\nI agree with petrv and KhoaLe, but I will add that not displaying the finalDF would be wise as it could display and log PII data and that to me is why I choose E. Like hal2401 said, commands 2, 5 & 6 can be removed as they don't manipulate the data."},{"comment_id":"1159782","content":"Selected Answer: E\nperhaps it's a multi-choice question in exam. I'll select E and D. if single choice then E.","timestamp":"1708954260.0","upvote_count":"1","poster":"hal2401me"},{"upvote_count":"2","poster":"KhoaLe","content":"Selected Answer: E\nLooking through at all steps, Cmd 2,5,6 can be eliminated without impacting to the whole process.\nHowever, in terms of duration cost, Cmd 2 and 5 does not impact much as they only show the current results of logical query plan. In contrast, display() in Cmd6 is actually a transformation, which will take much time to run.","timestamp":"1707354900.0","comment_id":"1143938"},{"content":"Selected Answer: E\nNo display()","poster":"alexvno","upvote_count":"3","timestamp":"1702887540.0","comment_id":"1099521"},{"timestamp":"1699994520.0","poster":"60ties","comments":[{"content":"in order to display a dataframe you also need to calculate it. So display also acts as an action.","upvote_count":"1","poster":"ofed","timestamp":"1700119800.0","comment_id":"1072250"}],"upvote_count":"1","comment_id":"1070832","content":"Selected Answer: D\nNo actions on production scripts. D is best"},{"timestamp":"1698998820.0","upvote_count":"2","poster":"Karen1232123","comment_id":"1061164","content":"Why not D?"}],"answers_community":["E (89%)","11%"],"question_images":["https://img.examtopics.com/certified-data-engineer-professional/image29.png"],"choices":{"E":"Cmd 6","B":"Cmd 3","C":"Cmd 4","A":"Cmd 2","D":"Cmd 5"},"answer_images":[],"answer_description":"","url":"https://www.examtopics.com/discussions/databricks/view/125249-exam-certified-data-engineer-professional-topic-1-question/"},{"id":"HLH8a1LIrgeotGnTdQ8N","isMC":true,"url":"https://www.examtopics.com/discussions/databricks/view/126360-exam-certified-data-engineer-professional-topic-1-question/","discussion":[{"poster":"divingbell17","comments":[{"content":"Always a job cluster.","poster":"arekm","timestamp":"1735778100.0","comment_id":"1335323","upvote_count":"1"}],"timestamp":"1704072060.0","content":"Selected Answer: B\nB is correct I think.\nWith option C, the cluster remains on 24/7 with trigger = 60 mins which is more costly\n\nIf there is an option with structure streaming with trigger = availablenow, and job scheduled per hour, that would be even more efficient.\nhttps://www.databricks.com/blog/2017/05/22/running-streaming-jobs-day-10x-cost-savings.html","upvote_count":"10","comment_id":"1110920"},{"poster":"robodog","comments":[{"poster":"robodog","timestamp":"1724184720.0","upvote_count":"2","comment_id":"1269667","content":"B answer i mean"}],"comment_id":"1269666","upvote_count":"1","content":"Selected Answer: C\nC. The lowest cost is obtained by using job cluster","timestamp":"1724184720.0"},{"poster":"Curious76","upvote_count":"2","comment_id":"1160110","timestamp":"1708984980.0","content":"Selected Answer: C\nDatabricks recommends using Structured Streaming with trigger AvailableNow for incremental workloads that do not have low latency requirements."},{"poster":"spaceexplorer","content":"Selected Answer: B\nB is correct","comment_id":"1131149","upvote_count":"4","timestamp":"1706132700.0"},{"timestamp":"1702887720.0","content":"Selected Answer: B\nB : Job cluster is cheap , hourly = 60 minutes","poster":"alexvno","comment_id":"1099529","upvote_count":"4"},{"comment_id":"1076493","upvote_count":"2","timestamp":"1700587620.0","content":"Selected Answer: B\nScheduling a job to execute the pipeline on an hourly basis aligns with the requirement for data to be updated every hour. Using a job cluster (which is brought up for the job and torn down upon completion) rather than a dedicated interactive cluster will usually be more cost-effective. This is because you are only paying for the compute resources when the job is running, which is 10 minutes out of every hour, rather than paying for an interactive cluster that would be up and running (and incurring costs) continuously.","poster":"aragorn_brego"},{"poster":"ofed","upvote_count":"1","timestamp":"1700138280.0","content":"It's either B or D. I think B, because we want the lowest cost.","comment_id":"1072406"}],"choices":{"A":"Manually trigger a job anytime the business reporting team refreshes their dashboards","E":"Configure a job that executes every time new data lands in a given directory","D":"Schedule a job to execute the pipeline once an hour on a dedicated interactive cluster","C":"Schedule a Structured Streaming job with a trigger interval of 60 minutes","B":"Schedule a job to execute the pipeline once an hour on a new job cluster"},"answer_description":"","question_images":[],"question_id":160,"answers_community":["B (87%)","13%"],"unix_timestamp":1700138280,"exam_id":163,"answer_images":[],"topic":"1","answer":"B","question_text":"The business reporting team requires that data for their dashboards be updated every hour. The total processing time for the pipeline that extracts transforms, and loads the data for their pipeline runs in 10 minutes.\n\nAssuming normal operating conditions, which configuration will meet their service-level agreement requirements with the lowest cost?","timestamp":"2023-11-16 13:38:00","answer_ET":"B"}],"exam":{"lastUpdated":"12 Apr 2025","isMCOnly":true,"isBeta":false,"isImplemented":true,"provider":"Databricks","name":"Certified Data Engineer Professional","id":163,"numberOfQuestions":200},"currentPage":32},"__N_SSP":true}