{"pageProps":{"questions":[{"id":"jThfRKaCLSBlQbdYPGtA","answers_community":["D (90%)","5%"],"unix_timestamp":1697450640,"isMC":true,"question_text":"A Delta Lake table representing metadata about content posts from users has the following schema: user_id LONG, post_text STRING, post_id STRING, longitude FLOAT, latitude FLOAT, post_time TIMESTAMP, date DATE\nThis table is partitioned by the date column. A query is run with the following filter: longitude < 20 & longitude > -20\nWhich statement describes how data will be filtered?","answer":"D","topic":"1","question_images":[],"timestamp":"2023-10-16 12:04:00","answer_ET":"D","url":"https://www.examtopics.com/discussions/databricks/view/123743-exam-certified-data-engineer-professional-topic-1-question/","answer_images":[],"question_id":131,"exam_id":163,"discussion":[{"poster":"Enduresoul","timestamp":"1700927280.0","content":"Selected Answer: D\nD is correct. A partition can include multiple files. And the statistics are collected for each file.","comment_id":"1080129","upvote_count":"12"},{"timestamp":"1734166320.0","upvote_count":"1","poster":"AlejandroU","comment_id":"1326395","content":"Selected Answer: B\nAnswer B. Single Comparison Filter (e.g., latitude > 66.3): File skipping is highly efficient because Delta can use min/max statistics to directly eliminate files that don't meet the condition.\nRange Filters (e.g., longitude < 20 AND longitude > -20): File skipping is still possible but less efficient, because Delta has to evaluate whether any records in the file might meet the condition, even if the min and max values of the column in the file overlap with the filter range.\nSo in summary, file skipping works best with single comparisons like latitude > 66.3 but is less effective with range filters like longitude < 20 AND longitude > -20."},{"upvote_count":"1","comment_id":"1325633","content":"Selected Answer: D\nDo not get confused between option c and d. Given answer is correct.","timestamp":"1734010860.0","poster":"Sriramiyer92"},{"timestamp":"1732948200.0","upvote_count":"1","poster":"hebied","content":"Selected Answer: D\nD is more suitable","comment_id":"1320092"},{"comment_id":"1270078","upvote_count":"2","poster":"AndreFR","timestamp":"1724241840.0","content":"Selected Answer: D\nMin and max values of each parquet file are stored in Delta Logs\nDelta data skipping automatically collects the stats (min, max, etc.) for the first 32 columns for each underlying Parquet file when you write data into a Delta table. Databricks takes advantage of this information (minimum and maximum values) at query time to skip unnecessary files in order to speed up the queries.\nhttps://www.databricks.com/discover/pages/optimize-data-workloads-guide#delta-data"},{"timestamp":"1706451900.0","content":"Correct Ans is D","comment_id":"1134149","poster":"AziLa","upvote_count":"2"},{"comments":[{"content":"I reread the question and thing that I made a mistake, in option C there is information about row-level statistics, but, I guess, statistics in Delta Log it is more less about columns. So, now D looks fine for me.","poster":"Quadronoid","upvote_count":"4","timestamp":"1698739500.0","comment_id":"1058563"}],"timestamp":"1698651540.0","poster":"Quadronoid","upvote_count":"1","comment_id":"1057435","content":"Selected Answer: C\nI guess C option is right since transaction log contains information about max/min values of first 32 columns, it can be used in order to filter files."},{"comment_id":"1044837","content":"Selected Answer: D\nD is Correct","poster":"sturcu","upvote_count":"3","timestamp":"1697450640.0"}],"answer_description":"","choices":{"D":"Statistics in the Delta Log will be used to identify data files that might include records in the filtered range.","A":"Statistics in the Delta Log will be used to identify partitions that might Include files in the filtered range.","E":"The Delta Engine will scan the parquet file footers to identify each row that meets the filter criteria.","B":"No file skipping will occur because the optimizer does not know the relationship between the partition column and the longitude.","C":"The Delta Engine will use row-level statistics in the transaction log to identify the flies that meet the filter criteria."}},{"id":"3agNAMhRPP8lTPWrJ4YV","answer_images":[],"unix_timestamp":1697935620,"discussion":[{"content":"Selected Answer: C\nC is the correct answer.","comment_id":"1339914","poster":"RandomForest","upvote_count":"1","timestamp":"1736777460.0"},{"poster":"imatheushenrique","comment_id":"1223854","upvote_count":"2","content":"(C)\nThe decision is about where the Databricks workspace used by the contractors should be deployed. The contractors are based in India, while all the company's data is stored in regional cloud storage in the United States. When choosing a region for deploying a Databricks workspace, one of the important factors to consider is the proximity to the data sources and sinks. Cross-region reads and writes can incur significant costs and latency due to network bandwidth and data transfer fees. Therefore, whenever possible, compute should be deployed in the same region the data is stored to optimize performance and reduce costs","timestamp":"1733282160.0"},{"content":"Selected Answer: C\nC is the answer.","poster":"spaceexplorer","upvote_count":"3","comment_id":"1131058","timestamp":"1721842620.0"},{"content":"Selected Answer: C\nAn important part of data governance is usage cost, and, as a general data engineering practice, egress costs related to moving data between regions is always an important consideration. Having the workspaces located in a different region than the contractors will incur to them in very little nuisance, while greatly saving in this sense.","comment_id":"1114431","poster":"RafaelCFC","timestamp":"1720167840.0","upvote_count":"2"},{"comments":[{"comment_id":"1214014","content":"These pipelines will create clusters (machines) which will reside in a different region than the data and that will cause latency issues. So C should be the correct option.","poster":"coercion","upvote_count":"2","timestamp":"1732064400.0"}],"timestamp":"1719658440.0","content":"Selected Answer: B\nFrom where data engineering team developes pipelines is independent of where the data objects reside in the cloud storage.","comment_id":"1108682","upvote_count":"1","poster":"Patito"},{"content":"C is correct.","upvote_count":"2","poster":"chokthewa","timestamp":"1713746820.0","comment_id":"1050001"}],"answer_description":"","question_text":"A small company based in the United States has recently contracted a consulting firm in India to implement several new data engineering pipelines to power artificial intelligence applications. All the company's data is stored in regional cloud storage in the United States.\nThe workspace administrator at the company is uncertain about where the Databricks workspace used by the contractors should be deployed.\nAssuming that all data governance considerations are accounted for, which statement accurately informs this decision?","url":"https://www.examtopics.com/discussions/databricks/view/124285-exam-certified-data-engineer-professional-topic-1-question/","exam_id":163,"answers_community":["C (86%)","14%"],"question_images":[],"topic":"1","choices":{"B":"Databricks workspaces do not rely on any regional infrastructure; as such, the decision should be made based upon what is most convenient for the workspace administrator.","E":"Databricks notebooks send all executable code from the userâ€™s browser to virtual machines over the open internet; whenever possible, choosing a workspace region near the end users is the most secure.","C":"Cross-region reads and writes can incur significant costs and latency; whenever possible, compute should be deployed in the same region the data is stored.","D":"Databricks leverages user workstations as the driver during interactive development; as such, users should always use a workspace deployed in a region they are physically near.","A":"Databricks runs HDFS on cloud volume storage; as such, cloud virtual machines must be deployed in the region where the data is stored."},"answer":"C","isMC":true,"question_id":132,"timestamp":"2023-10-22 02:47:00","answer_ET":"C"},{"id":"hD3f5kmERd4wSPoJFg5r","answer_description":"","isMC":true,"discussion":[{"comment_id":"973896","upvote_count":"13","content":"incorrect the correct option is C, with constraints, if added to an existing table the existing data in the table must be consistent with the constraint otherwise it fails\nhttps://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-alter-table.html#add-constraint","poster":"8605246","timestamp":"1691330160.0"},{"timestamp":"1724244120.0","content":"Selected Answer: C\n-- CREATE TABLE \ncreate table test_constraint (t1 varchar(2), n1 int);\n\n-- ADD VALUE \ninsert into test_constraint values ('v3', 3);\n\n-- ADD CONSTAINT VIOLATED BY CURRENT DATA \n-- should throw error : 1 row in spark_catalog.default.test_constaint violate the new CHECK constraint (n1 < 3)\n\nalter table test_constraint add constraint valid_n1 check (n1 < 3);\n\n-- ADD CONSTAINT NOT VIOLATED BY CURRENT DATA (no error)\nalter table test_constraint add constraint valid_n1 check (n1 < 100);","poster":"AndreFR","comment_id":"1270114","upvote_count":"1"},{"timestamp":"1722689040.0","content":"Selected Answer: C\nC is correct.","poster":"faraaz132","comment_id":"1260290","upvote_count":"1"},{"comment_id":"1145313","content":"C is correct","timestamp":"1707465060.0","upvote_count":"1","poster":"PrashantTiwari"},{"comment_id":"1138097","poster":"DAN_H","content":"correct ans is C","upvote_count":"1","timestamp":"1706848020.0"},{"timestamp":"1706452860.0","upvote_count":"1","poster":"AziLa","comment_id":"1134174","content":"correct ans is C"},{"comment_id":"1121973","content":"Selected Answer: C\ncorrect","poster":"Jay_98_11","timestamp":"1705172040.0","upvote_count":"1"},{"content":"Selected Answer: C\nC is the correct answer","upvote_count":"1","comment_id":"1118784","poster":"kz_data","timestamp":"1704902280.0"},{"timestamp":"1703854620.0","comment_id":"1108685","upvote_count":"1","content":"Selected Answer: C\nC is correct","poster":"Patito"},{"timestamp":"1701600540.0","content":"Selected Answer: C\nC is correct","poster":"hamzaKhribi","upvote_count":"1","comment_id":"1086758"},{"poster":"Enduresoul","upvote_count":"1","timestamp":"1700927460.0","content":"Selected Answer: C\nC is correct","comment_id":"1080135"},{"content":"Selected Answer: C\nWhen adding a CHECK constraint to an existing table, the operation will fail if there are any rows in the table that do not meet the constraint. Before a CHECK constraint can be added, the data already in the table must be validated to ensure that it complies with the constraint conditions. If any existing records violate the new constraints, they must be corrected or removed before the ALTER TABLE command can be successfully executed.","upvote_count":"2","poster":"aragorn_brego","timestamp":"1700578140.0","comment_id":"1076384"},{"timestamp":"1698953340.0","poster":"BIKRAM063","content":"Selected Answer: C\nCorrect option C : existing data violated check constraint condition","comment_id":"1060831","upvote_count":"1"},{"upvote_count":"1","timestamp":"1698648600.0","comment_id":"1057411","content":"Selected Answer: C\nRight answer is C","poster":"Quadronoid"},{"timestamp":"1697450820.0","content":"Selected Answer: C\nC - table already has data","upvote_count":"1","comment_id":"1044838","poster":"sturcu"},{"upvote_count":"1","poster":"MarceloManhaes","timestamp":"1695507600.0","comment_id":"1015359","content":"Yes the correct is option C"}],"unix_timestamp":1691330160,"url":"https://www.examtopics.com/discussions/databricks/view/117481-exam-certified-data-engineer-professional-topic-1-question/","question_id":133,"answer_ET":"C","question_images":["https://img.examtopics.com/certified-data-engineer-professional/image19.png"],"answer":"C","question_text":"The downstream consumers of a Delta Lake table have been complaining about data quality issues impacting performance in their applications. Specifically, they have complained that invalid latitude and longitude values in the activity_details table have been breaking their ability to use other geolocation processes.\nA junior engineer has written the following code to add CHECK constraints to the Delta Lake table:\n//IMG//\n\nA senior engineer has confirmed the above logic is correct and the valid ranges for latitude and longitude are provided, but the code fails when executed.\nWhich statement explains the cause of this failure?","answer_images":[],"topic":"1","answers_community":["C (100%)"],"choices":{"E":"The current table schema does not contain the field valid_coordinates; schema evolution will need to be enabled before altering the table to add a constraint.","B":"The activity_details table already exists; CHECK constraints can only be added during initial table creation.","C":"The activity_details table already contains records that violate the constraints; all existing data must pass CHECK constraints in order to add them to an existing table.","D":"The activity_details table already contains records; CHECK constraints can only be added prior to inserting values into a table.","A":"Because another team uses this table to support a frequently running application, two-phase locking is preventing the operation from committing."},"exam_id":163,"timestamp":"2023-08-06 15:56:00"},{"id":"YISFJT5AhX4JVUoFdbJR","question_id":134,"answer":"B","isMC":true,"question_text":"Which of the following is true of Delta Lake and the Lakehouse?","answer_images":[],"answers_community":["B (89%)","11%"],"timestamp":"2023-10-23 06:41:00","discussion":[{"upvote_count":"1","timestamp":"1737052740.0","comment_id":"1341787","poster":"SRV_33","content":"Selected Answer: B\nComplete statement is correct only in this option"},{"content":"B is correct","poster":"PrashantTiwari","comment_id":"1145316","upvote_count":"1","timestamp":"1707465180.0"},{"upvote_count":"2","poster":"guillesd","content":"Selected Answer: B\nB is correct","comment_id":"1141084","timestamp":"1707136620.0"},{"poster":"spaceexplorer","comment_id":"1131068","timestamp":"1706125260.0","upvote_count":"1","content":"Selected Answer: B\nB is correct"},{"comment_id":"1113779","poster":"Crocjun","timestamp":"1704379920.0","upvote_count":"1","content":"Can anyone explain why D is not correct?","comments":[{"poster":"decisiontree","comment_id":"1335884","content":"Foreign key constraints have nothing to do with the duplicate values.","upvote_count":"1","timestamp":"1735877760.0"},{"upvote_count":"2","poster":"cryptoflam","timestamp":"1704448740.0","comment_id":"1114411","content":"Because Primary & Foreign Key information is not enforced. \n\"Primary and foreign keys are informational only and are not enforced\" from:\nhttps://docs.databricks.com/en/tables/constraints.html#declare-primary-key-and-foreign-key-relationships"}]},{"comments":[{"poster":"AndreFR","content":"https://www.databricks.com/discover/pages/optimize-data-workloads-guide#delta-data\n\nDelta data skipping automatically collects the stats (min, max, etc.) for the first 32 columns for each underlying Parquet file when you write data into a Delta table. Databricks takes advantage of this information (minimum and maximum values) at query time to skip unnecessary files in order to speed up the queries.","upvote_count":"1","timestamp":"1724244780.0","comment_id":"1270124"}],"comment_id":"1108687","content":"Selected Answer: B\nB is correct since statistics are collected for the first 32 columns and stored in the transaction log.","timestamp":"1703854740.0","poster":"Patito","upvote_count":"3"},{"comment_id":"1106522","upvote_count":"1","timestamp":"1703645400.0","content":"Selected Answer: B\nB is correctï¼Œ \nC is error, con't have new cache in view","poster":"ervinshang"},{"content":"Selected Answer: C\nC is correct","comment_id":"1101333","timestamp":"1703056500.0","upvote_count":"1","poster":"f728f7f"},{"timestamp":"1698036060.0","poster":"chokthewa","upvote_count":"1","comment_id":"1051446","content":"B is correct.\nhttps://docs.delta.io/2.0.0/table-properties.html"}],"answer_ET":"B","topic":"1","answer_description":"","choices":{"B":"Delta Lake automatically collects statistics on the first 32 columns of each table which are leveraged in data skipping based on query filters.","C":"Views in the Lakehouse maintain a valid cache of the most recent versions of source tables at all times.","D":"Primary and foreign key constraints can be leveraged to ensure duplicate values are never entered into a dimension table.","E":"Z-order can only be applied to numeric values stored in Delta Lake tables.","A":"Because Parquet compresses data row by row. strings will only be compressed when a character is repeated multiple times."},"unix_timestamp":1698036060,"question_images":[],"exam_id":163,"url":"https://www.examtopics.com/discussions/databricks/view/124424-exam-certified-data-engineer-professional-topic-1-question/"},{"id":"2O2ggTfzmxaKnrTXl0Nm","url":"https://www.examtopics.com/discussions/databricks/view/121876-exam-certified-data-engineer-professional-topic-1-question-4/","topic":"1","answer_images":[],"isMC":true,"question_images":["https://img.examtopics.com/certified-data-engineer-professional/image1.png"],"question_text":"The data engineering team has configured a Databricks SQL query and alert to monitor the values in a Delta Lake table. The recent_sensor_recordings table contains an identifying sensor_id alongside the timestamp and temperature for the most recent 5 minutes of recordings.\nThe below query is used to create the alert:\n//IMG//\n\nThe query is set to refresh each minute and always completes in less than 10 seconds. The alert is set to trigger when mean (temperature) > 120. Notifications are triggered to be sent at most every 1 minute.\nIf this alert raises notifications for 3 consecutive minutes and then stops, which statement must be true?","exam_id":163,"unix_timestamp":1696074180,"answer_description":"","discussion":[{"timestamp":"1738701600.0","upvote_count":"2","comment_id":"1351579","content":"Selected Answer: E\nBecause the mean is calculated for each sensor, and on that the alert is raised. It happened for three times, unknown for which sensor. Could be any","poster":"EelkeV"},{"upvote_count":"2","poster":"AndreFR","timestamp":"1724050080.0","content":"A excluded because there is a group by clause\nB & C excluded table needs to be updated to mean value to change \nD excluded, because alert is set on average not max temperature\nCorrect answer is E by elimination","comment_id":"1268444"},{"poster":"panya","content":"Correct","timestamp":"1719189420.0","upvote_count":"1","comment_id":"1236071"},{"content":"E. The average temperature recordings for at least one sensor exceeded 120 on three consecutive executions of the query","timestamp":"1717547340.0","poster":"imatheushenrique","comment_id":"1224435","upvote_count":"1"},{"timestamp":"1705147800.0","upvote_count":"1","poster":"Jay_98_11","comment_id":"1121589","content":"Selected Answer: E\nE is correct"},{"upvote_count":"2","content":"Selected Answer: E\ncorrect","poster":"sturcu","comment_id":"1040233","timestamp":"1697004480.0"},{"timestamp":"1696074180.0","poster":"saikot","content":"The correct answer is E\nhttps://www.myexamcollection.com/databricks-certified-professional-data-engineer-databricks-certified-professional-data-engineer-exam-question-answers.htm","upvote_count":"1","comment_id":"1021421"}],"timestamp":"2023-09-30 13:43:00","answer":"E","question_id":135,"choices":{"E":"The average temperature recordings for at least one sensor exceeded 120 on three consecutive executions of the query","A":"The total average temperature across all sensors exceeded 120 on three consecutive executions of the query","B":"The recent_sensor_recordings table was unresponsive for three consecutive runs of the query","D":"The maximum temperature recording for at least one sensor exceeded 120 on three consecutive executions of the query","C":"The source query failed to update properly for three consecutive minutes and then restarted"},"answer_ET":"E","answers_community":["E (100%)"]}],"exam":{"numberOfQuestions":200,"isBeta":false,"lastUpdated":"12 Apr 2025","isMCOnly":true,"id":163,"name":"Certified Data Engineer Professional","isImplemented":true,"provider":"Databricks"},"currentPage":27},"__N_SSP":true}