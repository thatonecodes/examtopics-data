{"pageProps":{"questions":[{"id":"M9TBKjpsMuwBYBlFAOqc","choices":{"C":"import sys\ndate = sys.argv[1]","A":"date = spark.conf.get(\"date\")","E":"dbutils.widgets.text(\"date\", \"null\")\ndate = dbutils.widgets.get(\"date\")","B":"input_dict = input()\ndate= input_dict[\"date\"]","D":"date = dbutils.notebooks.getParam(\"date\")"},"answer_ET":"E","answer_images":[],"answer_description":"","exam_id":163,"unix_timestamp":1690966980,"isMC":true,"question_images":[],"discussion":[{"timestamp":"1727249340.0","comment_id":"1173064","poster":"hal2401me","upvote_count":"10","content":"Selected Answer: E\ndbutils.widget.\nJust passed the exam with score >80%.\nexamtopics covers about 90% of questions. there were 5 questions I didn't see here in examtopics.\nBut friends, you need to look at the discussions, and do test yourself.\nmany answers provided here, even most voted answer, does NOT exists anymore in the exam - not the question, but the answer.\nWish you all good luck, friends!"},{"timestamp":"1742999760.0","upvote_count":"1","poster":"ultimomassimo","comment_id":"1410425","content":"Selected Answer: E\nE 100%\nThere is no such thing as dbutils.notebooks.getParam, so no idea why some halfwits suggest D..."},{"timestamp":"1740486180.0","content":"Selected Answer: E\ndbutils.widgets is where we store all the param and use .get to fetch the params","comment_id":"1361428","upvote_count":"1","poster":"shoaibmohammed3"},{"timestamp":"1739981460.0","comment_id":"1358833","upvote_count":"1","content":"Selected Answer: D\nD is correct!\n\nThe question states that the upstream system passes the date as a parameter to the Databricks Jobs API. In Databricks, when a parameter is passed to a notebook via the Jobs API, it can be retrieved using the dbutils.notebooks.getParam() method.\n\nOption D directly retrieves the parameter value using this method, which is the correct approach for this scenario.","poster":"johnserafim"},{"upvote_count":"1","poster":"EelkeV","content":"Selected Answer: E\nIt is the way to fill in a parameter in a notebook","timestamp":"1738700340.0","comment_id":"1351567"},{"comment_id":"1335691","upvote_count":"2","poster":"HairyTorso","content":"Selected Answer: E\nAround question #130 they just repeat themselves. So it's not 226 but around 130... Shame","timestamp":"1735841400.0"},{"content":"Selected Answer: E\nJobs API allows to sending parameters via jobs parameter. This parameter must have the same notebook params. Eventually, it can be read using dbutils.widgets.get","poster":"akashdesarda","comment_id":"1290627","timestamp":"1727522100.0","upvote_count":"1"},{"poster":"HorskKos","upvote_count":"4","timestamp":"1723533000.0","comment_id":"1265036","content":"E is correct because:\nA - gets configuration of a spark session\nB - gets a value from a manual input - non relevant for the job run\nC - sys.argv - gets parameters, which were used to run a Pyrhon script from CMD - completely non-related\nD - haven't found this function on the web at all, assume that it doesn't exist\nTherefore E is correct, though it's a bad practice to type a date as a parameter, it's better to get it with datetime library and then use it in the code"},{"content":"Answer is E.\nEven though the value is passed from a upstream system, you can create parameters using widgets inside notebook and use the value as an input from the databricks jobs API.","poster":"Shailly","comment_id":"1247581","timestamp":"1720922400.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"1226795","timestamp":"1717859040.0","content":"Selected Answer: E\nWidgets are used to create parameters in notebook that can be then utilized by e.g. jobs","poster":"Isio05"},{"timestamp":"1717208400.0","poster":"imatheushenrique","comment_id":"1222432","upvote_count":"1","content":"E.\nE. dbutils.widgets.text(\"date\", \"null\")\ndate = dbutils.widgets.get(\"date\")"},{"timestamp":"1713537240.0","content":"correct ans is E","upvote_count":"1","poster":"AziLa","comment_id":"1198680"},{"upvote_count":"1","poster":"Sosicha","comment_id":"1195398","content":"Are you reading the question? It asks about an upstream system that has been configured to pass the date for a given batch of data to the Databricks Jobs API as a parameter. Upstream system usually don't use widgets. Widgets they are made for humans. Only C and D are correct but D is better so D.","timestamp":"1713083760.0"},{"content":"Selected Answer: E\nvote for E\ndbutils.widget","upvote_count":"1","timestamp":"1708947420.0","poster":"hal2401me","comment_id":"1159633"},{"upvote_count":"1","timestamp":"1705861680.0","poster":"AziLa","content":"Correct Ans is E","comment_id":"1128049"},{"comment_id":"1121585","content":"Selected Answer: E\nE is correct","timestamp":"1705147620.0","upvote_count":"2","poster":"Jay_98_11"},{"comment_id":"1113402","timestamp":"1704354840.0","poster":"RafaelCFC","content":"Selected Answer: E\nIn https://docs.databricks.com/en/notebooks/notebook-workflows.html#dbutilsnotebook-api the \"run Example\" is an equivalent use-case as E.","upvote_count":"2"},{"poster":"kz_data","content":"Selected Answer: E\nE is correct","upvote_count":"2","timestamp":"1703172000.0","comment_id":"1102660"},{"upvote_count":"1","poster":"chokthewa","timestamp":"1696658760.0","comment_id":"1027115","content":"I think D is correct answer, refer to https://docs.databricks.com/en/notebooks/notebook-workflows.html#dbutilsnotebook-api"},{"comments":[{"timestamp":"1694853900.0","comment_id":"1009024","content":"Did you take exam? Are these questions valid?","upvote_count":"3","poster":"unk1102"}],"poster":"BrianNguyen95","timestamp":"1692051600.0","comment_id":"981132","upvote_count":"3","content":"E is correct answer"},{"poster":"lokvamsi","content":"Selected Answer: E\nCorrect. Ans: E","upvote_count":"1","timestamp":"1691667660.0","comment_id":"977584"},{"timestamp":"1690966980.0","content":"Correct","upvote_count":"2","poster":"Happy_Prince","comment_id":"969962"}],"topic":"1","timestamp":"2023-08-02 11:03:00","url":"https://www.examtopics.com/discussions/databricks/view/117083-exam-certified-data-engineer-professional-topic-1-question-1/","question_id":1,"answer":"E","answers_community":["E (96%)","4%"],"question_text":"An upstream system has been configured to pass the date for a given batch of data to the Databricks Jobs API as a parameter. The notebook to be scheduled will use this parameter to load data with the following code: df = spark.read.format(\"parquet\").load(f\"/mnt/source/(date)\")\nWhich code block should be used to create the date Python variable used in the above code block?"},{"id":"sk2J5xKeNzrykM27zMEC","unix_timestamp":1691003940,"isMC":true,"choices":{"B":"The Parquet file footers are scanned for min and max statistics for the latitude column","A":"All records are cached to an operational database and then the filter is applied","D":"The Delta log is scanned for min and max statistics for the latitude column","C":"All records are cached to attached storage and then the filter is applied","E":"The Hive metastore is scanned for min and max statistics for the latitude column"},"answer_ET":"D","timestamp":"2023-08-02 21:19:00","answer_images":[],"exam_id":163,"url":"https://www.examtopics.com/discussions/databricks/view/117162-exam-certified-data-engineer-professional-topic-1-question/","topic":"1","answer":"D","answers_community":["D (90%)","10%"],"discussion":[{"upvote_count":"22","content":"Answer D:\n\nIn the Transaction log, Delta Lake captures statistics for each data file of the table. These statistics indicate per file:\n - Total number of records\n - Minimum value in each column of the first 32 columns of the table\n - Maximum value in each column of the first 32 columns of the table\n - Null value counts for in each column of the first 32 columns of the table\n\nWhen a query with a selective filter is executed against the table, the query optimizer uses these statistics to generate the query result. it leverages them to identify data files that may contain records matching the conditional filter.\nFor the SELECT query in the question, The transaction log is scanned for min and max statistics for the price column","poster":"taif12340","timestamp":"1692787800.0","comment_id":"988204"},{"upvote_count":"2","content":"Selected Answer: B\nB is correct!\n\nDelta Lake stores min/max statistics for each column in the Parquet file footers. The engine scans these footers to determine if a file contains any data that satisfies the latitude > 66.3 condition. If the minimum latitude in a file is greater than 66.3, the file is loaded. If the maximum latitude is less than or equal to 66.3, the file is skipped.","poster":"johnserafim","timestamp":"1741206960.0","comment_id":"1365581"},{"timestamp":"1727533920.0","upvote_count":"3","comment_id":"1290673","poster":"akashdesarda","content":"Selected Answer: D\nAbove mentioned points are correct. If the table was just parquet table then parquet file footer have been used. But since this is Delta table, then delta log is used to scan & skip files. It uses stats written in in transaction log."},{"comment_id":"1268752","upvote_count":"2","timestamp":"1724075760.0","content":"Answer D : \n\nDelta data skipping automatically collects the stats (min, max, etc.) for the first 32 columns for each underlying Parquet file when you write data into a Delta table. Databricks takes advantage of this information (minimum and maximum values) at query time to skip unnecessary files in order to speed up the queries.\n\nhttps://www.databricks.com/discover/pages/optimize-data-workloads-guide#delta-data","poster":"AndreFR"},{"content":"Selected Answer: D\nDelta table stores file statistics in transaction log","comment_id":"1267459","poster":"saravanan289","timestamp":"1723870860.0","upvote_count":"2"},{"timestamp":"1719424380.0","upvote_count":"2","poster":"03355a2","comment_id":"1237632","content":"Selected Answer: D\nNo explanation needed, this is where the information is stored."},{"content":"D. The Delta log is scanned for min and max statistics for the latitude column","poster":"imatheushenrique","comment_id":"1224441","upvote_count":"1","timestamp":"1717547520.0"},{"upvote_count":"1","timestamp":"1716132060.0","poster":"coercion","comment_id":"1213855","content":"Selected Answer: D\nDelta log collects statistics like min value, max value, no of records, no of files for each transaction that happens on the table for the first 32 columns (default value)"},{"poster":"Tayari","upvote_count":"1","comment_id":"1204721","timestamp":"1714509120.0","content":"Selected Answer: D\nD is the answer"},{"upvote_count":"1","timestamp":"1711468980.0","poster":"arik90","comment_id":"1183436","content":"Selected Answer: D\nBased on Docu is D I don't know why here is showing B"},{"content":"Selected Answer: D\nDelta log first","upvote_count":"1","comment_id":"1172271","timestamp":"1710310140.0","poster":"alexvno"},{"content":"Selected Answer: D\nStatistics on first 32 columns of a table are computed and written in the Delta Log by default.","poster":"DavidRou","comment_id":"1170375","upvote_count":"1","timestamp":"1710080400.0"},{"timestamp":"1709205120.0","content":"Selected Answer: D\nD is the right answer","comment_id":"1162470","poster":"vikram12apr","upvote_count":"1"},{"upvote_count":"1","poster":"Curious76","timestamp":"1709068440.0","content":"Selected Answer: D\nD is the answer","comment_id":"1161000"},{"timestamp":"1708117380.0","content":"Selected Answer: D\nD is correct one","poster":"kkravets","comment_id":"1152224","upvote_count":"1"},{"upvote_count":"2","poster":"RiktRikt007","comment_id":"1145974","content":"I checked the delta log, and it dose store stat, stats\":\"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":1,\\\"name\\\":\\\"one\\\",\\\"age\\\":11},\\\"maxValues\\\":{\\\"id\\\":1,\\\"name\\\":\\\"one\\\",\\\"age\\\":11},\\\"nullCount\\\":{\\\"id\\\":0,\\\"name\\\":0,\\\"age\\\":0}}\"","timestamp":"1707552180.0"},{"poster":"AziLa","content":"correct ans is D","upvote_count":"1","timestamp":"1705864500.0","comment_id":"1128075"},{"comment_id":"1121654","upvote_count":"1","timestamp":"1705149780.0","poster":"Jay_98_11","content":"Selected Answer: D\nD for sure"},{"content":"Selected Answer: D\nI think the correct answer is D","poster":"kz_data","comment_id":"1118554","timestamp":"1704891360.0","upvote_count":"1"},{"comment_id":"1116057","content":"_delta_log contains the max and min of each column for the first 30 odd columns in a table for each partition. Also there is nothing called parquet file footers. Correct answer is D.","upvote_count":"1","timestamp":"1704650940.0","poster":"ranith"},{"timestamp":"1704097260.0","poster":"lexaneon","content":"D\nhttps://www.databricks.com/discover/pages/optimize-data-workloads-guide#:~:text=Delta%20data%20skipping%20automatically%20collects,to%20speed%20up%20the%20queries.","comment_id":"1111031","upvote_count":"1"},{"poster":"chowchowchow","comments":[{"content":"bro ChatGPT 3.5 was last updated in 2021. Please don't rely on ChatGPT for any databricks queries.","comment_id":"1108665","upvote_count":"1","timestamp":"1703852580.0","poster":"hkay"}],"content":"chatgpt vote b as well","timestamp":"1703126100.0","upvote_count":"1","comment_id":"1102119"},{"upvote_count":"1","poster":"hamzaKhribi","content":"For me D is correct, as statistics are collected in the delta loge of the first 32 columns","comment_id":"1086046","timestamp":"1701512340.0"},{"content":"D is correct, Transaction log will be scanned","upvote_count":"1","timestamp":"1698949920.0","comment_id":"1060780","poster":"BIKRAM063"},{"upvote_count":"1","content":"Selected Answer: D\nD is the correct answer","poster":"jms309","comment_id":"1043874","timestamp":"1697347380.0"},{"poster":"Eertyy","comment_id":"991545","upvote_count":"3","timestamp":"1693150920.0","content":"D is correct answer"},{"upvote_count":"3","comment_id":"970495","timestamp":"1691003940.0","content":"D is correct Answer","poster":"tusharl"}],"question_text":"A Delta table of weather records is partitioned by date and has the below schema: date DATE, device_id INT, temp FLOAT, latitude FLOAT, longitude FLOAT\nTo find all the records from within the Arctic Circle, you execute a query with the below filter: latitude > 66.3\nWhich statement describes how the Delta engine identifies which files to load?","question_images":[],"question_id":2,"answer_description":""},{"id":"7wcwoe7HFa64NKQXGWGy","exam_id":163,"topic":"1","discussion":[{"upvote_count":"4","content":"Selected Answer: C\nIn Databricks, secret scopes are used to manage and organize secrets. By setting \"Read\" permissions on a secret scope containing the credentials, you allow the team to access the necessary credentials without granting unnecessary privileges. This approach ensures that the teams have the minimum necessary access to the credentials required for connecting to the external database. \"Manage\" permissions would provide more access than needed for just using the credentials.\n\nOption A and D suggest setting permissions on individual secret keys, which might work, but using a secret scope for organizational purposes is a cleaner and more scalable solution.","timestamp":"1722897480.0","comments":[{"timestamp":"1735811220.0","poster":"arekm","content":"Answer C.\n\nYou cannot manage permissions on secretes, only on secret scopes: https://docs.databricks.com/en/security/auth/access-control/index.html#secret-acls","upvote_count":"1","comment_id":"1335460"}],"poster":"vctrhugo","comment_id":"1141644"},{"content":"Selected Answer: C\nAccess is at scope level and not key level","upvote_count":"2","comment_id":"1136731","timestamp":"1722420360.0","poster":"Somesh512"},{"timestamp":"1717313940.0","comment_id":"1086019","poster":"petrv","content":"Selected Answer: C\nIn summary, while technically feasible, setting \"Read\" permissions on a secret key might not be the most efficient or scalable solution when dealing with multiple teams and their corresponding credentials. Using secret scopes provides a more organized and maintainable approach for managing secrets in Databricks.","upvote_count":"1"},{"poster":"Enduresoul","upvote_count":"3","comment_id":"1080934","timestamp":"1716739380.0","content":"Selected Answer: C\nAnswer C is correct:\nhttps://docs.databricks.com/en/security/auth-authz/access-control/secret-acl.html#secret-access-control\n\"Access control for secrets is managed at the secret scope level\""}],"choices":{"B":"\"Read\" permissions should be set on a secret key mapped to those credentials that will be used by a given team.","D":"\"Manage\" permissions should be set on a secret scope containing only those credentials that will be used by a given team.\nNo additional configuration is necessary as long as all users are configured as administrators in the workspace where secrets have been added.","A":"\"Manage\" permissions should be set on a secret key mapped to those credentials that will be used by a given team.","C":"\"Read\" permissions should be set on a secret scope containing only those credentials that will be used by a given team."},"timestamp":"2023-11-26 19:03:00","question_images":[],"question_text":"The data engineering team has been tasked with configuring connections to an external database that does not have a supported native connector with Databricks. The external database already has data security configured by group membership. These groups map directly to user groups already created in Databricks that represent various teams within the company.\n\nA new login credential has been created for each group in the external database. The Databricks Utilities Secrets module will be used to make these credentials available to Databricks users.\n\nAssuming that all the credentials are configured correctly on the external database and group membership is properly configured on Databricks, which statement describes how teams can be granted the minimum necessary access to using these credentials?","isMC":true,"answer_description":"","answers_community":["C (100%)"],"question_id":3,"answer":"C","answer_ET":"C","answer_images":[],"unix_timestamp":1701021780,"url":"https://www.examtopics.com/discussions/databricks/view/127269-exam-certified-data-engineer-professional-topic-1-question/"},{"id":"VeABWHbIxbNNJR0z2ajA","discussion":[{"comment_id":"1141643","timestamp":"1707179640.0","content":"Selected Answer: C\nC. Size on Disk is > 0\n\nWhen using Spark's MEMORY_ONLY storage level, the ideal scenario is that the data is fully cached in memory, and the Size on Disk should be 0 (indicating that the data is not spilled to disk). If the Size on Disk is greater than 0, it suggests that some data has been spilled to disk, which can lead to degraded performance as reading from disk is slower than reading from memory.","upvote_count":"7","poster":"vctrhugo"},{"timestamp":"1733663100.0","comment_id":"1323543","content":"Selected Answer: C\nI think is C","poster":"benni_ale","upvote_count":"1"},{"timestamp":"1717860480.0","content":"Selected Answer: C\nIn this case any data on disk means that cache is not performing optimally","poster":"Isio05","upvote_count":"2","comment_id":"1226802"}],"answer_images":[],"topic":"1","question_images":[],"answer":"C","answer_ET":"C","exam_id":163,"answers_community":["C (100%)"],"question_text":"Which indicators would you look for in the Spark UI’s Storage tab to signal that a cached table is not performing optimally? Assume you are using Spark’s MEMORY_ONLY storage level.","url":"https://www.examtopics.com/discussions/databricks/view/132982-exam-certified-data-engineer-professional-topic-1-question/","isMC":true,"choices":{"C":"Size on Disk is > 0","B":"The RDD Block Name includes the “*” annotation signaling a failure to cache","E":"On Heap Memory Usage is within 75% of Off Heap Memory Usage","A":"Size on Disk is < Size in Memory","D":"The number of Cached Partitions > the number of Spark Partitions"},"unix_timestamp":1707179640,"question_id":4,"timestamp":"2024-02-06 01:34:00","answer_description":""},{"id":"CNRCo07OuYSxwtCFTb2l","topic":"1","choices":{"C":"# Databricks notebook source","D":"-- Databricks notebook source","A":"%python","E":"# MAGIC %python","B":"// Databricks notebook source"},"answer":"C","question_id":5,"exam_id":163,"question_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/126297-exam-certified-data-engineer-professional-topic-1-question/","unix_timestamp":1700065140,"answer_ET":"C","answer_description":"","question_text":"What is the first line of a Databricks Python notebook when viewed in a text editor?","discussion":[{"content":"Selected Answer: C\nPython: # Databricks notebook source\nSQL: -- Databricks notebook source\nScala: // Databricks notebook source\nR: # Databricks notebook source","poster":"bacckom","comment_id":"1119503","timestamp":"1704961200.0","upvote_count":"8"},{"comment_id":"1237014","upvote_count":"2","poster":"Ati1362","timestamp":"1719337260.0","content":"Selected Answer: C\nc is the answer"},{"upvote_count":"2","content":"Selected Answer: C\nhttps://docs.databricks.com/en/notebooks/notebook-export-import.html#import-a-file-and-convert-it-to-a-notebook","timestamp":"1704164160.0","poster":"divingbell17","comment_id":"1111556"},{"content":"Selected Answer: C\nThis is the correct line that you would find at the top of a Databricks notebook when viewed in a text editor, especially for Python notebooks. The # symbol is used for comments in Python, and the comment # Databricks notebook source is used by Databricks to indicate the start of the notebook's source code in the plain text file.\n\nThese lines are comments in the respective languages (Scala uses // and SQL uses -- for single-line comments) and indicate the beginning of the Databricks notebook content in the text file.","timestamp":"1700615880.0","poster":"aragorn_brego","upvote_count":"2","comment_id":"1076842"},{"timestamp":"1700449920.0","upvote_count":"1","poster":"AWSMaster69","comment_id":"1075100","content":"Selected Answer: C\nThe Answer is C, Just downloaded a notebook from Databricks and viewed it in a text editor."},{"comment_id":"1071617","timestamp":"1700065200.0","upvote_count":"1","content":"Selected Answer: C\nAnswer is C","poster":"60ties"},{"timestamp":"1700065140.0","upvote_count":"1","poster":"60ties","comment_id":"1071615","content":"// Databricks notebook source - Scala\n # Databricks notebook source - Python\n -- Databricks notebook source - SQL\nAnswer is C"}],"answer_images":[],"isMC":true,"timestamp":"2023-11-15 17:19:00","answers_community":["C (100%)"]}],"exam":{"isBeta":false,"provider":"Databricks","id":163,"numberOfQuestions":200,"isMCOnly":true,"lastUpdated":"12 Apr 2025","name":"Certified Data Engineer Professional","isImplemented":true},"currentPage":1},"__N_SSP":true}