{"pageProps":{"questions":[{"id":"2sSQkWXpqNSNTAz8BINK","discussion":[{"comment_id":"1388139","timestamp":"1741829220.0","upvote_count":"1","content":"Selected Answer: D\nExplanation\nIn Apache Spark, operations are classified as transformations and actions:\n\nTransformations (Lazy Evaluation) → They define a logical plan but do not trigger execution.\nActions (Trigger Evaluation) → They cause Spark to execute the transformations and return a result.\nAnalysis of Each Option\nA. DataFrame.collect() → Action → Brings all data to the driver, triggering execution.\nB. DataFrame.count() → Action → Computes the number of rows, triggering execution.\nC. DataFrame.first() → Action → Returns the first row, triggering execution.\nD. DataFrame.join() → Transformation → Defines a new DataFrame but does not trigger execution.\nE. DataFrame.take() → Action → Returns a limited number of rows, triggering execution.","poster":"ARUNKUMARKRISHNASAMY"},{"comment_id":"1139737","timestamp":"1722733020.0","upvote_count":"1","content":"Join is a tranformation so it will not throw results, while others are actions, so Join() is correct","poster":"Anweee"}],"answer_description":"","question_text":"Which of the following operations will fail to trigger evaluation?","isMC":true,"answer_images":[],"timestamp":"2024-02-04 03:57:00","answer":"D","answer_ET":"D","question_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/132760-exam-certified-associate-developer-for-apache-spark-topic-1/","question_id":26,"topic":"1","answers_community":["D (100%)"],"unix_timestamp":1707015420,"choices":{"A":"DataFrame.collect()","C":"DataFrame.first()","B":"DataFrame.count()","E":"DataFrame.take()","D":"DataFrame.join()"},"exam_id":161},{"id":"5jAL47RwtOh7XFYGbDEv","answers_community":["D (100%)"],"timestamp":"2024-04-04 15:07:00","answer_ET":"D","exam_id":161,"isMC":true,"answer_description":"","discussion":[{"timestamp":"1737799860.0","upvote_count":"2","poster":"Souvik_79","content":"Selected Answer: D\nIsn't the question incorrect? It should be :\n__1__.__2__.__3__(__4__).format(\"json\").__5__(__6__)\nCode : spark.read.schema(schema).format(\"json\").load(filePath)","comment_id":"1346403"},{"timestamp":"1735852560.0","upvote_count":"1","poster":"nilave","comment_id":"1335759","content":"Selected Answer: D\nquestion is wrong , should be csv"},{"poster":"PV1995","comment_id":"1324671","content":"Selected Answer: D\nD has the right format with everything needed. E is wrong","upvote_count":"1","timestamp":"1733849580.0"},{"comment_id":"1302395","content":"Selected Answer: D\nThe correct answer is D.","upvote_count":"1","timestamp":"1729764120.0","poster":"Nina993"},{"timestamp":"1714123980.0","comments":[{"upvote_count":"3","comment_id":"1203735","poster":"65bd33e","timestamp":"1714348200.0","content":"I agree with you correct is D"}],"upvote_count":"4","comment_id":"1202514","content":"I think that we have an error in the question, the format is CSV so the good answer is D","poster":"nadegetiedjo"},{"content":"The question itself is bogus.","poster":"SaiPavan10","upvote_count":"4","timestamp":"1712236020.0","comment_id":"1189345"}],"answer_images":[],"question_text":"The code block shown below should read a JSON at the file path filePath into a DataFrame with the specified schema schema. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.\n\nCode block:\n\n__1__.__2__.__3__(__4__).format(\"csv\").__5__(__6__)","question_images":[],"question_id":27,"answer":"D","choices":{"A":"1. spark\n2. read()\n3. schema\n4. schema\n5. json\n6. filePath","E":"1. spark\n2. read\n3. format\n4. \"json\"\n5. load\n6. filePath","B":"1. spark\n2. read()\n3. json\n4. filePath\n5. format\n6. schema","D":"1. spark\n2. read\n3. schema\n4. schema\n5. load\n6. filePath","C":"1. spark\n2. read()\n3. schema\n4. schema\n5. load\n6. filePath"},"url":"https://www.examtopics.com/discussions/databricks/view/137900-exam-certified-associate-developer-for-apache-spark-topic-1/","topic":"1","unix_timestamp":1712236020},{"id":"oaemgXe3BlzSx0vN95Ec","isMC":true,"question_id":28,"choices":{"A":"storesDF.withColumn(“customerSatisfactionAbs”, abs(col(“customerSatisfaction”)))","B":"storesDF.withColumnRenamed(“customerSatisfactionAbs”, abs(col(“customerSatisfaction”)))","E":"storesDF.withColumn(“customerSatisfactionAbs”, abs(“customerSatisfaction”))","C":"storesDF.withColumn(col(“customerSatisfactionAbs”, abs(col(“customerSatisfaction”)))","D":"storesDF.withColumn(“customerSatisfactionAbs”, abs(col(customerSatisfaction)))"},"answer_images":[],"answer":"A","answers_community":[],"question_images":[],"question_text":"Which of the following code blocks returns a new DataFrame with a new column customerSatisfactionAbs that is the absolute value of column customerSatisfaction in DataFrame storesDF? Note that column customerSatisfactionAbs is not in the original DataFrame storesDF.","exam_id":161,"answer_ET":"A","answer_description":"","discussion":[{"poster":"Sowwy1","upvote_count":"1","timestamp":"1728475860.0","content":"A is correct","comment_id":"1192270"},{"poster":"cookiemonster42","timestamp":"1706912520.0","comment_id":"970532","upvote_count":"4","content":"it works for A and E:\npyspark.sql.functions.abs(col: ColumnOrName) → pyspark.sql.column.Column[source]\nComputes the absolute value.\n\nNew in version 1.3.0.\n\nChanged in version 3.4.0: Supports Spark Connect.\n\nParameters\ncolColumn or str\ntarget column to compute on."}],"unix_timestamp":1691007720,"timestamp":"2023-08-02 22:22:00","topic":"1","url":"https://www.examtopics.com/discussions/databricks/view/117165-exam-certified-associate-developer-for-apache-spark-topic-1/"},{"id":"QIpDGvQyfxnMtTW1czoZ","isMC":true,"exam_id":161,"answers_community":[],"url":"https://www.examtopics.com/discussions/databricks/view/117166-exam-certified-associate-developer-for-apache-spark-topic-1/","unix_timestamp":1691007900,"topic":"1","discussion":[{"comment_id":"1192272","content":"It's D","timestamp":"1728476040.0","upvote_count":"1","poster":"Sowwy1"},{"comment_id":"990947","poster":"martcerv","upvote_count":"2","content":"I believe D is the correct one according to documentation from Databricks [1]:\n\"The driver process runs your main() function, sits on a node in the cluster, and is responsible for three things: maintaining information about the Spark Application; responding to a user’s program or input; and analyzing, distributing, and scheduling work across the executors (defined momentarily).\"\n\nAddittionaly:\n\"The cluster manager controls physical machines and allocates resources to Spark Applications.\"\n\nBased on the above we could say that cluster manager is charge assign resources (CPU, Memory, etc) to the VMs used. Keep in mind that this is based on the definition from Databricks other definitions may include what was mentioned by cookiemonster42.\n\n[1] https://www.databricks.com/glossary/what-are-spark-applications","timestamp":"1708972560.0"},{"poster":"cookiemonster42","content":"Should be B - \nD - Spark driver is not directly responsible for scheduling the execution of data by various worker nodes in cluster mode. It submits tasks to the cluster manager (e.g., YARN, Mesos, or Kubernetes), and the cluster manager handles the scheduling of tasks on worker nodes.","timestamp":"1706912700.0","comment_id":"970533","upvote_count":"1"}],"timestamp":"2023-08-02 22:25:00","answer":"D","answer_description":"","answer_ET":"D","question_images":[],"question_id":29,"answer_images":[],"choices":{"D":"Spark driver is responsible for scheduling the execution of data by various worker nodes in cluster mode.","B":"Spark driver is the most coarse level of the Spark execution hierarchy.","E":"Spark driver is only compatible with its included cluster manager.","A":"Spark driver is horizontally scaled to increase overall processing throughput.","C":"Spark driver is fault tolerant — if it fails, it will recover the entire Spark application."},"question_text":"Which of the following statements about the Spark driver is true?"},{"id":"k5KwxylVDXPP4dN55yLv","answer":"B","answers_community":["B (100%)"],"isMC":true,"question_id":30,"choices":{"C":"1. write\n2. partitionBy\n3. col(“division”)\n4. parquet\n5. filePath","D":"1. write()\n2. partitionBy\n3. col(“division”)\n4. parquet\n5. filePath","B":"1. write\n2. partitionBy\n3. “division”\n4. parquet\n5. filePath","E":"1. write\n2. repartition\n3. “division”\n4. path\n5. filePath, mode = “parquet”","A":"1. write\n2. partitionBy\n3. “division”\n4. path\n5. filePath, node = parquet"},"timestamp":"2024-04-04 15:19:00","unix_timestamp":1712236740,"exam_id":161,"answer_images":[],"topic":"1","answer_description":"","question_text":"The code block shown below should write DataFrame storesDF to file path filePath as parquet and partition by values in column division. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.\n\nCode block:\n\nstoresDF.__1__.__2__(__3__).__4__(__5__)","answer_ET":"B","url":"https://www.examtopics.com/discussions/databricks/view/137901-exam-certified-associate-developer-for-apache-spark-topic-1/","question_images":[],"discussion":[{"poster":"SaiPavan10","timestamp":"1728047940.0","content":"Selected Answer: B\nB is the right choice","comment_id":"1189355","upvote_count":"1"}]}],"exam":{"isImplemented":true,"isMCOnly":true,"numberOfQuestions":185,"isBeta":false,"id":161,"provider":"Databricks","lastUpdated":"12 Apr 2025","name":"Certified Associate Developer for Apache Spark"},"currentPage":6},"__N_SSP":true}