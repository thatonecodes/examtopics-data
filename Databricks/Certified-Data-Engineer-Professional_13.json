{"pageProps":{"questions":[{"id":"Z0l0xrNhqM3foGy9Q1vJ","question_text":"The data engineering team is configuring environments for development, testing, and production before beginning migration on a new data pipeline. The team requires extensive testing on both the code and data resulting from code execution, and the team wants to develop and test against data as similar to production data as possible.\n\nA junior data engineer suggests that production data can be mounted to the development and testing environments, allowing pre-production code to execute against production data. Because all users have admin privileges in the development environment, the junior data engineer has offered to configure permissions and mount this data for the team.\n\nWhich statement captures best practices for this situation?","choices":{"D":"Because Delta Lake versions all data and supports time travel, it is not possible for user error or malicious actors to permanently delete production data; as such, it is generally safe to mount production data anywhere.","C":"Because access to production data will always be verified using passthrough credentials, it is safe to mount data to any Databricks development environment.","B":"In environments where interactive code will be executed, production data should only be accessible with read permissions; creating isolated databases for each environment further reduces risks.","A":"All development, testing, and production code and data should exist in a single, unified workspace; creating separate environments for testing and development complicates administrative overhead."},"answer_ET":"B","answers_community":["B (100%)"],"timestamp":"2024-10-20 18:27:00","question_id":61,"url":"https://www.examtopics.com/discussions/databricks/view/149874-exam-certified-data-engineer-professional-topic-1-question/","answer_description":"","exam_id":163,"answer":"B","answer_images":[],"isMC":true,"topic":"1","question_images":[],"unix_timestamp":1729441620,"discussion":[{"poster":"m79590530","timestamp":"1729441620.0","upvote_count":"2","comment_id":"1300569","content":"Selected Answer: B\nProduction data should be maximum secured against intentional and unintentional modifications by developers or workspace/UC admins. So setting it up with read only access and in different catalog or schema/database per environment is best approach."}]},{"id":"7Y7X4WIWBYlfDDNFaRXV","answer":"C","question_images":[],"answers_community":["C (100%)"],"topic":"1","answer_ET":"C","isMC":true,"answer_images":[],"timestamp":"2024-10-20 18:33:00","exam_id":163,"question_text":"The data architect has mandated that all tables in the Lakehouse should be configured as external Delta Lake tables.\n\nWhich approach will ensure that this requirement is met?","url":"https://www.examtopics.com/discussions/databricks/view/149876-exam-certified-data-engineer-professional-topic-1-question/","question_id":62,"choices":{"C":"Whenever a table is being created, make sure that the LOCATION keyword is used.","B":"When the workspace is being configured, make sure that external cloud object storage has been mounted.","A":"Whenever a database is being created, make sure that the LOCATION keyword is used.","D":"When tables are created, make sure that the UNMANAGED keyword is used in the CREATE TABLE statement."},"answer_description":"","unix_timestamp":1729441980,"discussion":[{"comment_id":"1307436","upvote_count":"1","content":"Selected Answer: C\nLocation keyword in CTAS statment in only way to create External tables","poster":"benni_ale","timestamp":"1730821140.0"},{"poster":"m79590530","comments":[{"comment_id":"1307437","poster":"benni_ale","upvote_count":"1","timestamp":"1730821200.0","content":"U are a dragon"}],"upvote_count":"2","content":"Selected Answer: C\nCREATE-ing a TABLE with LOCATION key word makes it EXTERNAL TABLE. By CREATE-ing the database/schema with the LOCATION key word we can have specific locations for the schemas/databases containing MANAGED tables when these tables inside these schemas/databases are CREATE-d withOUT the LOCATION key word.\nThis approach allows for configuring MANAGED TABLES at specific locations by fully leveraging Databricks Lakehouse automatic optimizations and performance tunning for them.","comment_id":"1300572","timestamp":"1729441980.0"}]},{"id":"sKVH8zmOupf3cNL9gwvU","topic":"1","answer_description":"","choices":{"A":"Create a view on the marketing table selecting only those fields approved for the sales team; alias the names of any fields that should be standardized to the sales naming conventions.","B":"Create a new table with the required schema and use Delta Lake's DEEP CLONE functionality to sync up changes committed to one table to the corresponding table.","C":"Use a CTAS statement to create a derivative table from the marketing table; configure a production job to propagate changes.","D":"Add a parallel table write to the current production pipeline, updating a new sales table that varies as required from the marketing table."},"question_id":63,"isMC":true,"exam_id":163,"timestamp":"2024-09-09 22:25:00","discussion":[{"comment_id":"1400014","content":"Selected Answer: A\nYup, no overhead, easy","upvote_count":"1","timestamp":"1742284380.0","poster":"lakime"},{"upvote_count":"3","content":"Selected Answer: A\nCreating a view is the simplest and most effective solution","poster":"db22","comment_id":"1281234","timestamp":"1725913500.0"}],"answer":"A","answer_images":[],"answers_community":["A (100%)"],"question_images":[],"unix_timestamp":1725913500,"url":"https://www.examtopics.com/discussions/databricks/view/147232-exam-certified-data-engineer-professional-topic-1-question/","question_text":"The marketing team is looking to share data in an aggregate table with the sales organization, but the field names used by the teams do not match, and a number of marketing-specific fields have not been approved for the sales org.\n\nWhich of the following solutions addresses the situation while emphasizing simplicity?","answer_ET":"A"},{"id":"Tnq33F79NW6VTm2d2P1v","topic":"1","timestamp":"2024-07-20 11:39:00","choices":{"A":"Statistics in the Delta Log will be used to identify partitions that might Include files in the filtered range.","B":"No file skipping will occur because the optimizer does not know the relationship between the partition column and the longitude.","C":"The Delta Engine will scan the parquet file footers to identify each row that meets the filter criteria.","D":"Statistics in the Delta Log will be used to identify data files that might include records in the filtered range."},"answers_community":["D (100%)"],"answer_ET":"D","exam_id":163,"url":"https://www.examtopics.com/discussions/databricks/view/144262-exam-certified-data-engineer-professional-topic-1-question/","answer_description":"","unix_timestamp":1721468340,"question_images":[],"question_id":64,"answer":"D","discussion":[{"upvote_count":"1","comment_id":"1366219","poster":"lakime","timestamp":"1741344540.0","content":"Selected Answer: D\nYup - data skipping D)"},{"upvote_count":"2","comment_id":"1251666","poster":"vexor3","timestamp":"1721468340.0","content":"Selected Answer: D\nD is correct"}],"answer_images":[],"question_text":"A Delta Lake table representing metadata about content posts from users has the following schema:\n\nuser_id LONG, post_text STRING, post_id STRING, longitude FLOAT, latitude FLOAT, post_time TIMESTAMP, date DATE\n\nThis table is partitioned by the date column. A query is run with the following filter:\n\nlongitude < 20 & longitude > -20\n\nWhich statement describes how data will be filtered?","isMC":true},{"id":"JQ6JtZ32bfDu2NzFgGO0","timestamp":"2024-06-12 18:31:00","answer":"C","isMC":true,"answers_community":["C (100%)"],"question_text":"A small company based in the United States has recently contracted a consulting firm in India to implement several new data engineering pipelines to power artificial intelligence applications. All the company's data is stored in regional cloud storage in the United States.\n\nThe workspace administrator at the company is uncertain about where the Databricks workspace used by the contractors should be deployed.\n\nAssuming that all data governance considerations are accounted for, which statement accurately informs this decision?","url":"https://www.examtopics.com/discussions/databricks/view/142396-exam-certified-data-engineer-professional-topic-1-question/","answer_description":"","topic":"1","choices":{"C":"Cross-region reads and writes can incur significant costs and latency; whenever possible, compute should be deployed in the same region the data is stored.","A":"Databricks runs HDFS on cloud volume storage; as such, cloud virtual machines must be deployed in the region where the data is stored.","D":"Databricks notebooks send all executable code from the userâ€™s browser to virtual machines over the open internet; whenever possible, choosing a workspace region near the end users is the most secure.","B":"Databricks workspaces do not rely on any regional infrastructure; as such, the decision should be made based upon what is most convenient for the workspace administrator."},"answer_images":[],"exam_id":163,"answer_ET":"C","question_images":[],"discussion":[{"comment_id":"1301566","content":"Selected Answer: C\nC is correct","timestamp":"1729599120.0","poster":"benni_ale","upvote_count":"1"},{"timestamp":"1718209860.0","poster":"hpkr","content":"Selected Answer: C\noption C","upvote_count":"2","comment_id":"1229314"}],"question_id":65,"unix_timestamp":1718209860}],"exam":{"isBeta":false,"lastUpdated":"12 Apr 2025","name":"Certified Data Engineer Professional","isImplemented":true,"id":163,"isMCOnly":true,"provider":"Databricks","numberOfQuestions":200},"currentPage":13},"__N_SSP":true}