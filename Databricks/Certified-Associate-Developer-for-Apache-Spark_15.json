{"pageProps":{"questions":[{"id":"ABo6KuAWCmBkQ5V3jTI2","question_images":[],"timestamp":"2024-04-10 17:49:00","isMC":true,"topic":"1","question_text":"The code block shown below should return a new DataFrame that is the result of an inner join between DataFrame storesDF and DataFrame employeesDF on column storeId and column employeeId. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.\n\nCode block:\n\nstoresDF.join(employeesDF, [__1__ == __2__, __3__ == __4__])","answer_description":"","choices":{"E":"1. storesDF.storeId\n2. employeesDF.storeId\n3. storesDF.employeeId\n4. employeesDF.employeeId","C":"1. storeId\n2. storeId\n3. employeeId\n4. employeeId","D":"1. col(\"storeId\")\n2. col(\"employeeId\")\n3. col(\"employeeId\")\n4. col(''storeId\")","B":"1. col(\"storeId\")\n2.col(\"storeId\")\n3.col(\"employeeId\")\n4. col(\"employeeId\")","A":"1. storesDF.storeId\n2. storesDF.employeeId\n3. employeesDF.storeId\n4. employeesDF.employeeId"},"answer":"E","answer_images":[],"question_id":71,"url":"https://www.examtopics.com/discussions/databricks/view/138360-exam-certified-associate-developer-for-apache-spark-topic-1/","answers_community":[],"exam_id":161,"unix_timestamp":1712764140,"discussion":[{"content":"E. 1. storesDF.storeId\n2. employeesDF.storeId\n3. storesDF.employeeId\n4. employeesDF.employeeId","upvote_count":"1","timestamp":"1728575340.0","poster":"Sowwy1","comment_id":"1193129"}],"answer_ET":"E"},{"id":"tKpwhlbNSbUO46UYyVz0","exam_id":161,"question_id":72,"question_text":"The code block shown below should return a new DataFrame that is the result of a position-wise union between DataFrame storesDF and DataFrame acquiredStoresDF. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.\n\nCode block:\n\n__1__.__2__(__3__)","discussion":[{"poster":"Souvik_79","comment_id":"1346343","timestamp":"1737784860.0","upvote_count":"1","content":"D.\nunionByName() is used to perform a position-wise union between two DataFrames. This method ensures that the union is based on the column names, so columns are matched by name rather than by position."},{"upvote_count":"1","timestamp":"1728575340.0","comment_id":"1193127","poster":"Sowwy1","content":"C. 1. storesDF\n2. union\n3.acquiredStoresDF"}],"topic":"1","unix_timestamp":1712764140,"answer_images":[],"answer_description":"","answer":"C","url":"https://www.examtopics.com/discussions/databricks/view/138359-exam-certified-associate-developer-for-apache-spark-topic-1/","choices":{"A":"1. DataFrame\n2. union\n3. storesDF, acquiredStoresDF","C":"1. storesDF\n2. union\n3.acquiredStoresDF","D":"1. storesDF\n2. unionByName\n3. acquiredStoresDF","E":"1. DataFrame\n2. unionAll\n3. storesDF, acquiredStoresDF","B":"1. DataFrame\n2. concat\n3. storesDF, acqulredStoresDF"},"question_images":[],"answers_community":[],"timestamp":"2024-04-10 17:49:00","answer_ET":"C","isMC":true},{"id":"ryEeNfFTz4O9NigpH8mn","discussion":[{"timestamp":"1737784920.0","upvote_count":"1","poster":"Souvik_79","content":"B. storesDF.write.mode(\"overwrite\").text(filePath)\n\nExplanation:\n\nwrite.mode(\"overwrite\") sets the mode for writing the DataFrame, with overwrite indicating that any existing data at the target location should be overwritten.\n.text(filePath) writes the DataFrame to the specified file path in text file format.","comment_id":"1346344"},{"comment_id":"1193131","content":"B. storesDF.write.mode(\"overwrite\").text(filePath)","upvote_count":"1","timestamp":"1728575460.0","poster":"Sowwy1"}],"answer_images":[],"answer_ET":"B","url":"https://www.examtopics.com/discussions/databricks/view/138361-exam-certified-associate-developer-for-apache-spark-topic-1/","question_text":"Which of the following code blocks writes DataFrame storesDF to file path filePath as text files overwriting any existing files in that location?","answer":"B","timestamp":"2024-04-10 17:51:00","answers_community":[],"answer_description":"","question_images":[],"question_id":73,"unix_timestamp":1712764260,"exam_id":161,"topic":"1","choices":{"B":"storesDF.write.mode(\"overwrite\").text(filePath)","C":"storesDF.write.mode(\"overwrite\").path(filePath)","E":"storesDF.write().mode(\"overwrite\").text(filePath)","D":"storesDF.write.option(\"text\", \"overwrite\").path(filePath)","A":"storesDF.write(filePath, mode = \"overwrite\", source = \"text\")"},"isMC":true},{"id":"bcrnv2cdaC7o6kRu1l0G","topic":"1","answers_community":[],"url":"https://www.examtopics.com/discussions/databricks/view/138362-exam-certified-associate-developer-for-apache-spark-topic-1/","question_id":74,"question_text":"The code block shown below contains an error. The code block is intended to read JSON at the file path filePath into a DataFrame with the specified schema schema. Identify the error.\n\nCode block:\n\nspark.read.schema(\"schema\").format(\"json\").load(filePath)","answer_description":"","choices":{"C":"The spark.read operation should be followed by parentheses in order to return a DataFrameReader object.","B":"There is no load() operation for DataFrameReader — it should be replaced with the json() operation.","E":"The schema operation from read takes a column rather than a string — the argument should be col(\"schema\").","D":"There is no read property of spark — spark should be replaced with DataFrame.","A":"The schema operation from read takes a schema object rather than a string — the argument should be schema."},"answer_ET":"A","timestamp":"2024-04-10 17:51:00","answer_images":[],"discussion":[{"poster":"Sowwy1","content":"A. The schema operation from read takes a schema object rather than a string — the argument should be schema.","timestamp":"1728575460.0","upvote_count":"1","comment_id":"1193132"}],"answer":"A","question_images":[],"unix_timestamp":1712764260,"isMC":true,"exam_id":161},{"id":"24t8O0hSJFJZBap0Horf","isMC":true,"topic":"1","question_images":[],"exam_id":161,"url":"https://www.examtopics.com/discussions/databricks/view/138363-exam-certified-associate-developer-for-apache-spark-topic-1/","answer":"E","question_id":75,"answers_community":[],"answer_ET":"E","answer_images":[],"answer_description":"","choices":{"E":"Executors are processing engine instances for performing data computations which run on a worker node.","A":"Executors are the communication pathways from the driver node to the worker nodes.","C":"Executors always have a one-to-one relationship with worker nodes.","D":"Executors are synonymous with worker nodes.","B":"Executors are the most granular level of execution in the Spark execution hierarchy."},"timestamp":"2024-04-10 17:57:00","unix_timestamp":1712764620,"discussion":[{"poster":"Sowwy1","upvote_count":"2","content":"E. Executors are processing engine instances for performing data computations which run on a worker node.","comment_id":"1193133","timestamp":"1728575820.0"}],"question_text":"Which of the following describes executors?"}],"exam":{"provider":"Databricks","isMCOnly":true,"isBeta":false,"name":"Certified Associate Developer for Apache Spark","isImplemented":true,"id":161,"lastUpdated":"12 Apr 2025","numberOfQuestions":185},"currentPage":15},"__N_SSP":true}