{"pageProps":{"questions":[{"id":"aRwr8ByLVNMYRW3M0lQj","exam_id":163,"topic":"1","discussion":[{"poster":"RafaelCFC","timestamp":"1720163100.0","comment_id":"1114380","upvote_count":"14","content":"Selected Answer: D\nA is wrong, because Tungsten is a project around improving Spark's efficiency on memory and CPU usage;\nB is wrong because Parquet does not support file editing, it only supports overwrite and create operations by itself;\nC is wrong because completely automating schema declaration for tables will incur in reduced previsibility for data types and data quality;\nE is false because unlucky sampling can yield bad inferences by Spark;"},{"poster":"hal2401me","timestamp":"1726288920.0","comment_id":"1173179","upvote_count":"7","content":"from my exam today, both C & D are no longer available, so they can't be correct.\nE & A are available. E states \"always accurate\" so I hesitate to choose it.\nThere is a new option stating like \"delta lake indexes first 32column in delta log for Z order and optimization\"(not sure I remember exactly, it looks statementfully correct). and I chosed this \"new\" option. Because, this should impact the schema decision by putting high-usage field in the first 32 columns."},{"content":"Selected Answer: D\nExplanation: Databricks can infer schema when reading data, but automatic schema inference doesn't always guarantee the accuracy of data types. For complex or highly-nested structures, schema inference might not always align with the actual data quality or the needs of downstream applications, and manual type definition ensures that the schema is more consistent and predictable. While automatic inference is useful for quick analysis or exploratory work, manual schema definition provides better data quality assurance in production workloads, especially when dealing with large, complex data structures.\nConclusion: This statement correctly emphasizes the importance of manual schema declaration to ensure data quality enforcement and consistency, especially when dealing with complex structures. Best option.","poster":"Tedet","upvote_count":"1","timestamp":"1740812760.0","comment_id":"1363435"},{"upvote_count":"1","content":"Selected Answer: D\nOnly answer that makes sense","comment_id":"1141038","poster":"guillesd","timestamp":"1722851340.0"},{"content":"Correct Ans is D","timestamp":"1722165960.0","comment_id":"1134100","poster":"AziLa","upvote_count":"1"},{"comment_id":"1044827","upvote_count":"2","timestamp":"1713261360.0","poster":"sturcu","content":"Selected Answer: D\ncorrect"},{"content":"D is correct. \nwe can use `schema hint` to enforce the schema information that we know and expect on an inferred schema.","comment_id":"1007012","poster":"hammer_1234_h","upvote_count":"2","timestamp":"1710378420.0"}],"choices":{"D":"Because Databricks will infer schema using types that allow all observed data to be processed, setting types manually provides greater assurance of data quality enforcement.","A":"The Tungsten encoding used by Databricks is optimized for storing string data; newly-added native support for querying JSON strings means that string types are always most efficient.","E":"Schema inference and evolution on Databricks ensure that inferred types will always accurately match the data types used by downstream systems.","B":"Because Delta Lake uses Parquet for data storage, data types can be easily evolved by just modifying file footer information in place.","C":"Human labor in writing code is the largest cost associated with data engineering workloads; as such, automating table declaration logic should be a priority in all migration workloads."},"isMC":true,"question_id":126,"url":"https://www.examtopics.com/discussions/databricks/view/120702-exam-certified-data-engineer-professional-topic-1-question/","answer_description":"","question_text":"A junior data engineer is working to implement logic for a Lakehouse table named silver_device_recordings. The source data contains 100 unique fields in a highly nested JSON structure.\nThe silver_device_recordings table will be used downstream to power several production monitoring dashboards and a production model. At present, 45 of the 100 fields are being used in at least one of these applications.\nThe data engineer is trying to determine the best approach for dealing with schema declaration given the highly-nested structure of the data and the numerous fields.\nWhich of the following accurately presents information about Delta Lake and Databricks that may impact their decision-making process?","unix_timestamp":1694646420,"timestamp":"2023-09-14 01:07:00","answer_images":[],"answers_community":["D (100%)"],"answer_ET":"D","answer":"D","question_images":[]},{"id":"XkEQx9YFLGPipAGsgml0","answers_community":["B (100%)"],"question_text":"The data engineering team maintains the following code:\n//IMG//\n\nAssuming that this code produces logically correct results and the data in the source tables has been de-duplicated and validated, which statement describes what will occur when this code is executed?","question_id":127,"question_images":["https://img.examtopics.com/certified-data-engineer-professional/image18.png"],"url":"https://www.examtopics.com/discussions/databricks/view/123741-exam-certified-data-engineer-professional-topic-1-question/","exam_id":163,"topic":"1","answer_images":[],"choices":{"D":"An incremental job will detect if new rows have been written to any of the source tables; if new rows are detected, all results will be recalculated and used to overwrite the enriched_itemized_orders_by_account table.","C":"An incremental job will leverage information in the state store to identify unjoined rows in the source tables and write these rows to the enriched_iteinized_orders_by_account table.","A":"A batch job will update the enriched_itemized_orders_by_account table, replacing only those rows that have different values than the current version of the table, using accountID as the primary key.","B":"The enriched_itemized_orders_by_account table will be overwritten using the current valid version of data in each of the three tables referenced in the join logic.","E":"No computation will occur until enriched_itemized_orders_by_account is queried; upon query materialization, results will be calculated using the current valid version of data in each of the three tables referenced in the join logic."},"discussion":[{"poster":"arekm","comment_id":"1334822","content":"Selected Answer: B\nB - it is a batch overwrite, which means: whatever was there is gone.","upvote_count":"1","timestamp":"1735651800.0"},{"content":"Selected Answer: B\ni agree. Cannot be E because write itself is action","upvote_count":"1","comment_id":"1302995","timestamp":"1729884600.0","poster":"nedlo"},{"content":"Selected Answer: B\nB because code has : .mode(“Overwrite”)","poster":"AndreFR","timestamp":"1724236800.0","upvote_count":"1","comment_id":"1270025"},{"timestamp":"1717462200.0","upvote_count":"1","comment_id":"1223843","poster":"imatheushenrique","content":"B is correct"},{"poster":"AziLa","comment_id":"1134104","upvote_count":"2","timestamp":"1706448600.0","content":"Correct Ans is B"},{"comment_id":"1121967","timestamp":"1705171380.0","poster":"Jay_98_11","content":"Selected Answer: B\ncorrect","upvote_count":"2"},{"comment_id":"1044830","content":"Selected Answer: B\nB is correct","timestamp":"1697450220.0","upvote_count":"3","poster":"sturcu"}],"timestamp":"2023-10-16 11:57:00","unix_timestamp":1697450220,"answer_ET":"B","answer":"B","isMC":true,"answer_description":""},{"id":"rWc51GIkEqmmtq58Q2kw","choices":{"A":"Isolating tables in separate databases based on data quality tiers allows for easy permissions management through database ACLs and allows physical separation of default storage locations for managed tables.","C":"Storing all production tables in a single database provides a unified view of all data assets available throughout the Lakehouse, simplifying discoverability by granting all users view privileges on this database.","E":"Because all tables must live in the same storage containers used for the database they're created in, organizations should be prepared to create between dozens and thousands of databases depending on their data isolation requirements.","D":"Working in the default Databricks database provides the greatest security when working with managed tables, as these will be created in the DBFS root.","B":"Because databases on Databricks are merely a logical construct, choices around database organization do not impact security or discoverability in the Lakehouse."},"answer_ET":"A","question_text":"The data engineering team is migrating an enterprise system with thousands of tables and views into the Lakehouse. They plan to implement the target architecture using a series of bronze, silver, and gold tables. Bronze tables will almost exclusively be used by production data engineering workloads, while silver tables will be used to support both data engineering and machine learning workloads. Gold tables will largely serve business intelligence and reporting purposes. While personal identifying information (PII) exists in all tiers of data, pseudonymization and anonymization rules are in place for all data at the silver and gold levels.\nThe organization is interested in reducing security concerns while maximizing the ability to collaborate across diverse teams.\nWhich statement exemplifies best practices for implementing this system?","question_images":[],"isMC":true,"answer_description":"","answer_images":[],"discussion":[{"comment_id":"1334824","upvote_count":"2","poster":"arekm","content":"Selected Answer: A\nA - most logical\nB - it is a logical construct, but under default settings tables are stored where the database is so there is a security component to it\nC - never a good idea to store everything in one db since db allows to group tables with similar area of interest and allows to manage permissions (like groups in Entra and assigning permissions to groups)\nD - not default database does not mean we cannot use managed tables and you can specify your location still; I do not think that storing anything on DBFS is a good idea - even Databricks suggests to use workspaces for your code, not to mention the data.\nE - thousand databases - nonsense; you can specify the location of individual tables.","comments":[{"comment_id":"1334825","upvote_count":"1","poster":"arekm","content":"Correction to D explanation - apparently the default location is bfs","timestamp":"1735652580.0"}],"timestamp":"1735652460.0"},{"poster":"strayda","content":"Selected Answer: A\nThe most logical answer is A","comment_id":"1231912","timestamp":"1718623860.0","upvote_count":"1"},{"poster":"imatheushenrique","timestamp":"1717462260.0","upvote_count":"2","comment_id":"1223844","content":"A is correct"},{"upvote_count":"1","comment_id":"1149885","timestamp":"1707893100.0","poster":"ojudz08","content":"Selected Answer: A\nanswer is A"},{"content":"Correct Ans is A","poster":"AziLa","timestamp":"1706451540.0","upvote_count":"2","comment_id":"1134147"},{"timestamp":"1700926920.0","upvote_count":"2","comment_id":"1080126","poster":"Enduresoul","content":"Selected Answer: A\nA is correct"}],"unix_timestamp":1697868960,"answers_community":["A (100%)"],"url":"https://www.examtopics.com/discussions/databricks/view/124155-exam-certified-data-engineer-professional-topic-1-question/","question_id":128,"timestamp":"2023-10-21 08:16:00","exam_id":163,"answer":"A","topic":"1"},{"id":"5e6xud5YDaQ5cXy0TSTv","timestamp":"2023-10-21 08:17:00","answers_community":["C (90%)","10%"],"choices":{"B":"When configuring an external data warehouse for all table storage, leverage Databricks for all ELT.","D":"When tables are created, make sure that the EXTERNAL keyword is used in the CREATE TABLE statement.","E":"When the workspace is being configured, make sure that external cloud object storage has been mounted.","A":"Whenever a database is being created, make sure that the LOCATION keyword is used","C":"Whenever a table is being created, make sure that the LOCATION keyword is used."},"url":"https://www.examtopics.com/discussions/databricks/view/124156-exam-certified-data-engineer-professional-topic-1-question/","question_text":"The data architect has mandated that all tables in the Lakehouse should be configured as external Delta Lake tables.\nWhich approach will ensure that this requirement is met?","unix_timestamp":1697869020,"isMC":true,"topic":"1","question_images":[],"question_id":129,"answer_ET":"C","discussion":[{"poster":"Sriramiyer92","timestamp":"1734009360.0","comment_id":"1325620","content":"Selected Answer: C\nNote: External keyword is not mandatory. \nLocation is mandatory the presence implies, that the table is external","upvote_count":"2"},{"upvote_count":"3","poster":"carah","content":"Selected Answer: C\nA. is not correct:\nhaving schema with LOCATION\nCREATE SCHEMA my_schema\nLOCATION 's3://<bucket-path>/my_schema';\n\nTable Location Scenarios:\n\n Table Without LOCATION:\n\nCREATE TABLE my_schema.my_table (id INT);\n\nThe table will be stored in the default warehouse directory (e.g., dbfs:/user/hive/warehouse/), not the schema's LOCATION.\n\nTable With Explicit LOCATION: If you want the table to be stored under the schema's LOCATION, you need to specify the location explicitly:\n\n CREATE TABLE my_schema.my_table (id INT)\n LOCATION 's3://<bucket-path>/my_schema/my_table/';\n\nSo, if you want all tables under the schema to use the schema’s LOCATION, explicitly specify the LOCATION for each table during creation.","comment_id":"1324742","timestamp":"1733862840.0"},{"poster":"y2kal","timestamp":"1731042780.0","upvote_count":"1","comment_id":"1308645","content":"It should be A, as the question states \"all tables\". Once an external DB is created, then all the tables in that would be by default be external."},{"upvote_count":"1","comment_id":"1306496","content":"Selected Answer: A\nA is correct. If a database is created using location keyword then by default all the tables created in it will use that location. They folows <provided location>/_unity_catalog/tables/<uuid>","timestamp":"1730634180.0","poster":"akashdesarda"},{"upvote_count":"2","comment_id":"1154661","timestamp":"1708428780.0","poster":"leopedroso1","content":"C is the correct answer. According to the documentation only the LOCATION is needed to make a table external. Moreover, we can also assume the keyword EXTERNAL is optional in the SQL statement.\n\nhttps://docs.databricks.com/en/sql/language-manual/sql-ref-external-tables.html"},{"upvote_count":"3","timestamp":"1707601140.0","poster":"CY","comment_id":"1146837","content":"'A' seems more appropriate.\nAll the tables in Delta lake house should be marked as external.. which can be achieved using location keyword at database level instead of each table level."},{"upvote_count":"2","poster":"Yogi05","comment_id":"1105614","timestamp":"1703552640.0","comments":[{"comment_id":"1106752","timestamp":"1703677560.0","content":"my bad. D is having EXTERNAL keyword, got confused. C is correct answer","poster":"Yogi05","upvote_count":"2"}],"content":"Why not D? i know both C and D are same, but D is more precise"},{"comment_id":"1072451","timestamp":"1700142480.0","content":"If you set a location in a database level, all tables under this database are automatically external table, in my opinion is A is correct.","upvote_count":"1","poster":"Laraujo2022","comments":[{"content":"According to what I've found in Databricks forums: \"Database location and Table location are independent\". So it looks like specifying location at DB level is not sufficient as tables will be still created as managed ones.","timestamp":"1717837440.0","poster":"Isio05","comment_id":"1226652","upvote_count":"3"}]},{"content":"Selected Answer: C\nC is correct. Location keyword should be in create script of the table","timestamp":"1698649440.0","poster":"Quadronoid","upvote_count":"4","comment_id":"1057414"},{"content":"C is correct, the key word to be used is Location, the keyword external is optional","upvote_count":"3","timestamp":"1698151320.0","comment_id":"1052836","poster":"mouad_attaqi"},{"comments":[{"comment_id":"1131952","poster":"mht3336","timestamp":"1706207880.0","upvote_count":"1","comments":[{"timestamp":"1717146480.0","poster":"Dusica","comment_id":"1222035","upvote_count":"1","content":"and microsoft synapse"}],"content":"there is no EXTERNAL key word in databricks, however it is there for other systems like Oracle, Hive, Cassandra etc."}],"poster":"chokthewa","comment_id":"1049282","timestamp":"1697869020.0","content":"The correct is D","upvote_count":"2"}],"answer":"C","answer_images":[],"exam_id":163,"answer_description":""},{"id":"k4Edq9ImnquKaHGz0RZN","discussion":[{"content":"Selected Answer: B\nB makes way more sense, the number of tables managed do not increase since the old table won't be used anymore, then the view on top of this table is not another table to manage, just maintains the \"original API\" of the table to avoid breaking changes in downstream applications","timestamp":"1707135720.0","comment_id":"1141070","poster":"guillesd","upvote_count":"7"},{"poster":"alexvno","comments":[{"upvote_count":"1","comment_id":"1359145","timestamp":"1740037860.0","poster":"EZZALDIN","content":"Replaces the current table with a view, but still creates a separate table for the customer app, this doesn’t cleanly separate the two schemas."},{"content":"But option B does not increase the number of tables to maintain; in fact, it replaces the source table. The question states that it should not increase, and from that perspective, the NUMBER of tables does not increase. It only replaces the source and creates a view.","timestamp":"1733702460.0","comment_id":"1323803","upvote_count":"1","poster":"carlosmps"}],"timestamp":"1710334320.0","content":"Selected Answer: D\nCreate view. Can't be B as -> without increasing the number of tables that need to be managed","upvote_count":"7","comment_id":"1172555"},{"comment_id":"1359147","content":"Selected Answer: B\nit recommends creating a view that maps the new table back to the original schema. This view lets other teams continue using the table as they always have, with no changes to their queries.","poster":"EZZALDIN","upvote_count":"1","timestamp":"1740037920.0"},{"upvote_count":"1","comment_id":"1339913","timestamp":"1736777160.0","poster":"RandomForest","content":"Selected Answer: B\nThis approach achieves the following key goals:\n\n1. Minimizes Disruption: By creating a view that mirrors the original schema, existing workloads that depend on the current schema remain uninterrupted.\nOther teams can continue their queries without needing to adjust their logic for the schema change.\n2. Meets New Requirements: The new table accommodates the changes required by the customer-facing application, ensuring that the application's updated requirements are fulfilled.\n3. Avoids Table Duplication: Instead of maintaining multiple tables for the same dataset, this approach uses a combination of a new table and a view, reducing the overall management burden.\n4Flexibility for Future Changes: Views can be adjusted as needed, providing a layer of abstraction. Future schema updates can be handled similarly without directly impacting dependent systems."},{"poster":"HairyTorso","content":"Selected Answer: B\nCreate view -> number of tables stay the same. Option D has overhead","comment_id":"1335222","upvote_count":"1","timestamp":"1735747440.0"},{"comment_id":"1334828","poster":"arekm","upvote_count":"1","timestamp":"1735653000.0","content":"Selected Answer: B\nB - but I was wondering between B & D.\n\nI do not like D since you replace the table with a view (query costs + you need to change the currently working workflow). Additionally, you create a table that does similar thing to the view - why?"},{"poster":"Nicks_name","upvote_count":"1","timestamp":"1732467900.0","content":"E. not D because, by converting the aggregate table into a view, might introduce performance overheads as every access now potentially involves running complex query logic to reconstruct the desired dataset on-the-fly. This might not be ideal for performance-sensitive applications like business intelligence dashboards.","comment_id":"1317128"},{"comment_id":"1312853","content":"Selected Answer: B\nOption D will increase the Compute cost significantly as all the downstream teams will run the view which has logic for Aggregate table.\nOption B make more sense with less impact to storage and compute cost which is the original ask for the data engineering team in the question.","timestamp":"1731714240.0","upvote_count":"2","poster":"vish007"},{"upvote_count":"1","timestamp":"1731316740.0","poster":"benni_ale","content":"Selected Answer: D\nI am not sure whether B or D... I believe B increases the number of managed Tables as it states that a CREATE TABLE statement is run before a CREATE VIEW ... the fact that the CREATE VIEW will replace the current table is not really specified... still one could argue that it would be dumb not do it but at this point i would say that D is more precise","comment_id":"1309919"},{"poster":"b.b.da.costa","content":"The problem with this question is if the order of the sentence matters.\n\nB: Create a table then create a view. Teams are interrupted after the creation of the table.\nD: Create a view then create a table. Teams are not interrupted because they are consuming the view first.","upvote_count":"1","comments":[{"comment_id":"1312851","timestamp":"1731713940.0","content":"Option D will increase the Compute cost significantly as all the downstream teams will run the view which has logic for Aggregate table. \nOption B make more sense with less impact to storage and compute cost which is the original ask for the data engineering team in the question.","upvote_count":"1","poster":"vish007"},{"upvote_count":"1","poster":"benni_ale","content":"Also B does it really not increase the number of written tables? It states that a CREATE TABLE is run and CREATE VIEW is run... Nothing really points to the fact that the view will repalce the table... Indeed I would opt for D","timestamp":"1731316620.0","comment_id":"1309915"}],"comment_id":"1308870","timestamp":"1731086040.0"},{"timestamp":"1730833800.0","comment_id":"1307530","content":"Selected Answer: B\nB is Correct. It does not create additional tables. The view mimics the old schema so not to interrupt downstream consumers. It ensures the aggregates are persisted to save on compute. \n\nD is incorrect mostly due to the aggregates being baked into the view which is not optimal as each time downstream users query the view the joins and aggregates have to be recomputed.","upvote_count":"1","poster":"kimberlyvsmith"},{"comment_id":"1295720","timestamp":"1728586440.0","content":"Selected Answer: D\nD will not increate the number of table. It will create a new table and replace the aggregation table with a view. B will create a new table, a new view match old table name and schema, aggregation table still there.","upvote_count":"1","poster":"shaojunni"},{"timestamp":"1726291560.0","comment_id":"1283497","upvote_count":"2","comments":[{"upvote_count":"2","comment_id":"1283499","poster":"KB_Ai_Champ","timestamp":"1726291920.0","content":"Reasons :\nNo Increase in Managed Tables: By replacing the current table with a view, you maintain the same number of managed tables.\nBackward Compatibility: The view can mimic the original table’s schema, ensuring that existing queries and applications continue to function without modification.\nDedicated Table for New Requirements: The new table can be tailored to meet the specific needs of the customer-facing application without affecting other users."}],"content":"option D is correct \ndocs : https://docs.databricks.com/en/delta/update-schema.html\nalso they specifically says that they dont want to increase managed tables!","poster":"KB_Ai_Champ"},{"content":"Selected Answer: B\nB is correct, no new tables, and minimally interrupting other teams in the organization\nA & E excluded, because they interrupt other teams in the organisation, usually answer that require user communication are wrong answers.\nC excluded, because it’s used for table creation, not after creation\nD excluded because it increases the number of tables","upvote_count":"1","poster":"AndreFR","timestamp":"1724241240.0","comment_id":"1270071"},{"timestamp":"1724237280.0","comment_id":"1270030","upvote_count":"2","content":"Selected Answer: A\nB,C and D all state creating a new table, therefore increasing the number of tables to manage. This is exactly what the question says to avoid.\n\n\"minimally interrupting other teams in the organization without increasing the number of tables that need to be managed\"\n\nAnswer A is the only one that makes sense and is pretty standard operation procedure for databases. E is wrong because you would never update a column comment to inform users of anything.","poster":"fe3b2fc"},{"comment_id":"1260288","content":"Selected Answer: B\nB is correct.\nWhy not D: Because it will create interruption when you replace the current table with a view and question says minimal interruption","timestamp":"1722688680.0","upvote_count":"2","poster":"faraaz132"},{"upvote_count":"2","timestamp":"1721114940.0","content":"Selected Answer: B\nI would go for B.\n\nWith option B you will run the aggregations once and store in in a table, then present these aggregations in the old schema in a view.\n\nWith D the aggregations will be done twice, for the old schema view and for the new table.","poster":"pravieee","comment_id":"1248749"},{"timestamp":"1714112760.0","content":"Selected Answer: B\nto me it's b because by creating a new table + the view that will substitute the previous table we still have 1 table. It seems to be the most efficient way to solve this. Not 100% sure though","upvote_count":"1","poster":"ThoBustos","comment_id":"1202409"},{"poster":"hal2401me","timestamp":"1710398700.0","comment_id":"1173180","content":"Selected Answer: D\nin my exam today I chose D.","upvote_count":"2"},{"timestamp":"1704871080.0","upvote_count":"2","poster":"IWantCerts","comment_id":"1118257","content":"Selected Answer: B\nI think it's B. D replaces original table definition with a view, which will run up compute costs for queries using the table."},{"content":"Selected Answer: D\nD. B has new table and view created.","comment_id":"1100122","poster":"aksand13","timestamp":"1702943880.0","upvote_count":"4"},{"poster":"Quadronoid","timestamp":"1698651360.0","comment_id":"1057432","content":"Selected Answer: B\nB is definitely the best option","upvote_count":"1"},{"timestamp":"1697932620.0","comment_id":"1049968","upvote_count":"2","poster":"chokthewa","content":"B is suitable for fact , don't interrupt the end-user , just managed by technical term. The technical team will create view refer field mapping ."},{"upvote_count":"1","timestamp":"1697450520.0","poster":"sturcu","comment_id":"1044835","content":"Selected Answer: B\nB is correct."},{"content":"Answer is D","comment_id":"1025261","timestamp":"1696475880.0","upvote_count":"1","poster":"TheGhost21"}],"answer":"B","choices":{"B":"Configure a new table with all the requisite fields and new names and use this as the source for the customer-facing application; create a view that maintains the original data schema and table name by aliasing select fields from the new table.","C":"Create a new table with the required schema and new fields and use Delta Lake's deep clone functionality to sync up changes committed to one table to the corresponding table.","A":"Send all users notice that the schema for the table will be changing; include in the communication the logic necessary to revert the new table schema to match historic queries.","E":"Add a table comment warning all users that the table schema and field names will be changing on a given date; overwrite the table in place to the specifications of the customer-facing application.","D":"Replace the current table definition with a logical view defined with the query logic currently writing the aggregate table; create a new table to power the customer-facing application."},"timestamp":"2023-10-05 05:18:00","isMC":true,"answers_community":["B (59%)","D (37%)","5%"],"question_text":"To reduce storage and compute costs, the data engineering team has been tasked with curating a series of aggregate tables leveraged by business intelligence dashboards, customer-facing applications, production machine learning models, and ad hoc analytical queries.\nThe data engineering team has been made aware of new requirements from a customer-facing application, which is the only downstream workload they manage entirely. As a result, an aggregate table used by numerous teams across the organization will need to have a number of fields renamed, and additional fields will also be added.\nWhich of the solutions addresses the situation while minimally interrupting other teams in the organization without increasing the number of tables that need to be managed?","question_id":130,"answer_description":"","answer_ET":"B","url":"https://www.examtopics.com/discussions/databricks/view/122481-exam-certified-data-engineer-professional-topic-1-question/","unix_timestamp":1696475880,"exam_id":163,"question_images":[],"topic":"1","answer_images":[]}],"exam":{"isBeta":false,"provider":"Databricks","id":163,"name":"Certified Data Engineer Professional","lastUpdated":"12 Apr 2025","isMCOnly":true,"isImplemented":true,"numberOfQuestions":200},"currentPage":26},"__N_SSP":true}