{"pageProps":{"questions":[{"id":"K8DWLXBn7vvwMZSuEz8N","answers_community":["E (86%)","14%"],"topic":"1","answer_ET":"E","exam_id":162,"discussion":[{"timestamp":"1697942160.0","comment_id":"1050141","upvote_count":"6","poster":"meow_akk","content":"E is correct you dont need to specify Delta as its the default storage format for tables."},{"poster":"Stemix","timestamp":"1706272380.0","content":"A and E have both correct syntax, but the question mentioned \"regardless of whether a table already exists with this name\". Hence the correct answer is E","comment_id":"1132510","upvote_count":"5"},{"content":"Selected Answer: E\nE is correct because you dont need to specify Delta as its the default storage format for tables and the question states \"regardless of whether the table already exists\". As a result A is wrong as it will NOT recreate the table if it exists.","poster":"RandomForest","comment_id":"1301418","timestamp":"1729574400.0","upvote_count":"1"},{"comment_id":"1273390","timestamp":"1724757060.0","content":"The closest correct option could be C but it shouldn't have with column clause\nTo create an empty Delta table with the specified schema in SQL, you can use the CREATE TABLE statement with the USING DELTA clause. The IF NOT EXISTS option ensures that the table is created only if it does not already exist. Here’s the SQL code block that accomplishes this:\n\n\nCREATE TABLE IF NOT EXISTS your_table_name (\n employeeld STRING,\n startDate DATE,\n avgRating DOUBLE\n) USING DELTA;","upvote_count":"1","poster":"9d4d68a"},{"poster":"80370eb","timestamp":"1723293600.0","comments":[{"content":"Disagree. The question states that it wants to create an empty table \"regardless of whether the table already exists\". This means that your chosen answer A will NOT recreate the table if it exists.","comment_id":"1273644","timestamp":"1724783580.0","upvote_count":"2","poster":"7082935"}],"upvote_count":"1","comment_id":"1263475","content":"Selected Answer: A\nA is the correct answer. create table if not exists will check if the table already exists in database. if not it will create the new table."},{"timestamp":"1714369140.0","comment_id":"1203829","upvote_count":"1","content":"Selected Answer: E\nE is correct","poster":"benni_ale"},{"poster":"azure_bimonster","upvote_count":"2","comment_id":"1127384","timestamp":"1705767780.0","content":"Selected Answer: E\nE is correct option"},{"content":"Selected Answer: E\nE. correct","comment_id":"1117397","poster":"bartfto","timestamp":"1704796080.0","upvote_count":"2"}],"isMC":true,"question_text":"A data architect has determined that a table of the following format is necessary:\n\n//IMG//\n\n\nWhich of the following code blocks uses SQL DDL commands to create an empty Delta table in the above format regardless of whether a table already exists with this name?","question_images":["https://img.examtopics.com/certified-data-engineer-associate/image15.png"],"answer_description":"","answer_images":[],"choices":{"B":"","A":"","D":"","C":"","E":""},"url":"https://www.examtopics.com/discussions/databricks/view/124291-exam-certified-data-engineer-associate-topic-1-question-55/","answer":"E","timestamp":"2023-10-22 04:36:00","question_id":121,"unix_timestamp":1697942160},{"id":"K4c2qewORCPg6klLcuze","unix_timestamp":1697942160,"topic":"1","discussion":[{"comment_id":"1263480","poster":"80370eb","upvote_count":"1","timestamp":"1723293960.0","content":"Selected Answer: D\nwe can use magic comment in notebook to indicate the cell to be run in specific language %SQL."},{"poster":"azure_bimonster","timestamp":"1705767840.0","content":"Selected Answer: D\nMagic command % can be used to switch the language, so D is correct","upvote_count":"1","comment_id":"1127386"},{"upvote_count":"1","comment_id":"1117398","content":"Selected Answer: D\nD. Correct. Use %sql magic in first line.","poster":"bartfto","timestamp":"1704796140.0"},{"content":"Selected Answer: D\nUse magic command %sql","upvote_count":"3","poster":"Lavpak","comment_id":"1078561","timestamp":"1700755980.0"},{"comment_id":"1057563","upvote_count":"3","poster":"MFEST","timestamp":"1698662760.0","content":"correct answer D"}],"question_images":[],"timestamp":"2023-10-22 04:36:00","isMC":true,"choices":{"D":"They can add %sql to the first line of the cell","B":"They can attach the cell to a SQL endpoint rather than a Databricks cluster","E":"They can change the default language of the notebook to SQL","C":"They can simply write SQL syntax in the cell","A":"It is not possible to use SQL in a Python notebook"},"answer_description":"","answer":"D","question_text":"A data engineer has a Python notebook in Databricks, but they need to use SQL to accomplish a specific task within a cell. They still want all of the other cells to use Python without making any changes to those cells.\n\nWhich of the following describes how the data engineer can use SQL within a cell of their Python notebook?","question_id":122,"url":"https://www.examtopics.com/discussions/databricks/view/124290-exam-certified-data-engineer-associate-topic-1-question-56/","answers_community":["D (100%)"],"answer_images":[],"exam_id":162,"answer_ET":"D"},{"id":"OmuUP1e2bQReHd3lMgId","answer_description":"","discussion":[{"upvote_count":"1","comment_id":"1263485","poster":"80370eb","timestamp":"1723294140.0","content":"Selected Answer: B\nThe PIVOT operation is used to rotate data from rows to columns, which effectively converts a table from a long format (where each row represents a single observation or measurement) to a wide format (where each row represents a single entity with multiple observations or measurements as columns)."},{"upvote_count":"1","comment_id":"1203830","poster":"benni_ale","timestamp":"1714369200.0","content":"Selected Answer: B\nB is correct"},{"content":"Selected Answer: B\nAnswer is B","poster":"azure_bimonster","comment_id":"1127390","timestamp":"1705767960.0","upvote_count":"1"},{"comment_id":"1101148","timestamp":"1703039880.0","poster":"AndreFR","content":"Selected Answer: B\nhttps://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-qry-select-pivot.html \n\n“Pivot” transforms the rows of the table_reference by rotating unique values of a specified column list into separate columns. \n\nSYNTAX : \n\ntable_reference PIVOT ( { aggregate_expression [ [ AS ] agg_column_alias ] } [, ...] \n FOR column_list IN ( expression_list ) ) \n \ncolumn_list \n { column_name | \n ( column_name [, ...] ) } \n \nexpression_list \n { expression [ AS ] [ column_alias ] | \n { ( expression [, ...] ) [ AS ] [ column_alias] } [, ...] ) }","upvote_count":"2"},{"content":"Selected Answer: B\nPIVOT is correct.","comment_id":"1052939","upvote_count":"2","timestamp":"1698158280.0","poster":"athu07"},{"poster":"meow_akk","upvote_count":"3","comment_id":"1050139","timestamp":"1697942100.0","content":"PIVOT is correct."}],"question_images":[],"question_id":123,"unix_timestamp":1697942100,"timestamp":"2023-10-22 04:35:00","answer_images":[],"isMC":true,"question_text":"Which of the following SQL keywords can be used to convert a table from a long format to a wide format?","answer_ET":"B","choices":{"C":"SUM","D":"CONVERT","E":"WHERE","A":"TRANSFORM","B":"PIVOT"},"answers_community":["B (100%)"],"exam_id":162,"answer":"B","url":"https://www.examtopics.com/discussions/databricks/view/124289-exam-certified-data-engineer-associate-topic-1-question-57/","topic":"1"},{"id":"JJoRkcfWTPzHbGJYAyL7","discussion":[{"upvote_count":"1","comment_id":"1247403","timestamp":"1720892040.0","content":"Vote for D\nParquet files are a columnar storage file format that allows for efficient data compression and encoding schemes, enabling optimization and faster query performance compared to CSV files. This format supports efficient reading and writing of large datasets, making it a preferred choice for big data applications.","poster":"1a44567"},{"timestamp":"1716806760.0","comment_id":"1219456","content":"Selected Answer: C\nThe keywords are \"CREATE TABLE AS SELECT \"","poster":"MDWPartners","upvote_count":"1"},{"content":"Selected Answer: C\nC is correct","poster":"benni_ale","timestamp":"1714369320.0","comment_id":"1203832","upvote_count":"1"},{"comment_id":"1132190","poster":"UGOTCOOKIES","upvote_count":"2","content":"Selected Answer: C\nCREATE TABLE AS SELECT adopts the schema details from the source. Parquet files have a defined schema.","timestamp":"1706230920.0"},{"upvote_count":"1","timestamp":"1704796260.0","comment_id":"1117399","poster":"bartfto","content":"Selected Answer: C\nC. Paruqet has well defined schema unline csv"},{"upvote_count":"1","comment_id":"1109975","timestamp":"1703969880.0","poster":"Garyn","content":"Selected Answer: C\nC. Parquet files have a well-defined schema.\n\nExplanation:\n\nParquet files inherently store metadata about the schema within the files themselves, allowing for a well-defined schema. This schema information includes data types, column names, and other structural information. When creating an external table from Parquet, this schema is retained, providing a structured and well-defined format for the data. It ensures consistency and enables more efficient processing, query optimization, and compatibility across various systems or tools that work with the Parquet format.\nThis structured schema within Parquet files offers advantages in terms of data integrity, ease of data processing, and compatibility, making it a beneficial choice over CSV, which lacks inherent schema information and might need additional handling or inference of schema during data ingestion."},{"poster":"AndreFR","comment_id":"1101159","timestamp":"1703041260.0","content":"Selected Answer: B\nThe key word here is : CREATE TABLE AS SELECT\n\nnot A : partitioning is not relevant in a create table as statement because the data will be created in a delta table \nnot C : Parquet schema is not well defined and there can be parquet files with multiple schema in a folder\nnot D : Parquet are already optimized and are not relevant in a create table as statement because the data will be created in a delta table \nnot E : both CSV & Parquet will become delta tables in a create table as statement\nB : correct answer by elimination","upvote_count":"1"},{"upvote_count":"1","poster":"nedlo","comment_id":"1094895","content":"Selected Answer: D\nI disagree i think its D. Schema can be inferred from CSV as well, but CSV cannot provide same optimizations as Parquet","timestamp":"1702410600.0"},{"poster":"FastEddie","comment_id":"1058405","upvote_count":"4","timestamp":"1698717660.0","content":"Selected Answer: C\nCTAS - CTAS automatically infer schema information from query\nresults and do not support manual schema declaration.This means\nthat CTAS statements are useful for external data ingestion from\nsources with well-defined schema, such as Parquet files and\ntables.CTAS statements also do not support specifying additional\nfile options."},{"content":"Selected Answer: C\nC is the correct option","comment_id":"1057667","timestamp":"1698669720.0","upvote_count":"2","poster":"kishore1980"},{"poster":"anandpsg101","comment_id":"1053321","content":"Selected Answer: C\nc is correct","timestamp":"1698192060.0","upvote_count":"2"},{"content":"Ans : C \nhttps://www.databricks.com/glossary/what-is-parquet#:~:text=Columnar%20storage%20like%20Apache%20Parquet,compared%20to%20row%2Doriented%20databases.\n\nColumnar storage like Apache Parquet is designed to bring efficiency compared to row-based files like CSV. When querying, columnar storage you can skip over the non-relevant data very quickly. As a result, aggregation queries are less time-consuming compared to row-oriented databases.","poster":"meow_akk","upvote_count":"4","timestamp":"1697942400.0","comment_id":"1050150"},{"poster":"kbaba101","comment_id":"1049562","content":"C.\nit supports well-defined schema, such as Parquet files and tables and do not support specifying additional file options such as Delimeter if you were to use CSV","upvote_count":"4","timestamp":"1697898660.0"}],"answers_community":["C (88%)","6%"],"isMC":true,"choices":{"A":"Parquet files can be partitioned","E":"Parquet files will become Delta tables","C":"Parquet files have a well-defined schema","D":"Parquet files have the ability to be optimized","B":"CREATE TABLE AS SELECT statements cannot be used on files"},"question_text":"Which of the following describes a benefit of creating an external table from Parquet rather than CSV when using a CREATE TABLE AS SELECT statement?","exam_id":162,"topic":"1","answer_images":[],"question_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/124209-exam-certified-data-engineer-associate-topic-1-question-58/","answer_description":"","question_id":124,"answer":"C","unix_timestamp":1697898660,"answer_ET":"C","timestamp":"2023-10-21 16:31:00"},{"id":"uS5NDHw9HyR980QFix1C","question_text":"A data engineer wants to create a relational object by pulling data from two tables. The relational object does not need to be used by other data engineers in other sessions. In order to save on storage costs, the data engineer wants to avoid copying and storing physical data.\n\nWhich of the following relational objects should the data engineer create?","answer_images":[],"choices":{"E":"Delta Table","D":"Temporary view","A":"Spark SQL Table","C":"Database","B":"View"},"answer_description":"","topic":"1","question_id":125,"exam_id":162,"timestamp":"2023-10-22 04:47:00","answers_community":["D (100%)"],"answer_ET":"D","unix_timestamp":1697942820,"isMC":true,"url":"https://www.examtopics.com/discussions/databricks/view/124293-exam-certified-data-engineer-associate-topic-1-question-59/","discussion":[{"upvote_count":"8","content":"D is correct.\n﻿﻿Temp view : session based\n Create temp view view_name as query\n All these are termed as session ended:\n Opening a new notebook\n Detaching and reattaching a cluster\n Installing a python package\nRestarting a cluster","comment_id":"1050152","timestamp":"1697942820.0","poster":"meow_akk"},{"upvote_count":"1","timestamp":"1723294740.0","poster":"80370eb","content":"Selected Answer: D\nA temporary view is created in memory and does not persist beyond the session. It does not require physical storage, making it ideal for avoiding storage costs.","comment_id":"1263495"},{"poster":"Garyn","timestamp":"1703970300.0","upvote_count":"2","content":"Selected Answer: D\nD. Temporary view\n\nExplanation:\n\nTemporary View: A temporary view in database systems like Apache Spark provides a temporary and ephemeral representation of data based on an SQL query's result set. It exists for the duration of a Spark session and is not persisted to storage. Similar to a regular view, a temporary view allows the data engineer to define a logical schema by pulling and combining data from multiple tables using SQL queries, but it does not store any physical data on disk. Temporary views are suitable when there's no need for long-term storage of the combined data and are helpful for immediate analysis or processing within the current session without incurring storage costs.","comment_id":"1109989"},{"upvote_count":"1","timestamp":"1703041740.0","comment_id":"1101164","content":"Selected Answer: D\nhttps://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html \n\nShould be a view or temporary view to avoid copying and storing data. \n\nDoes not need to be used by other data engineers in other sessions. So this view should be temporary. TEMPORARY views are visible only to the session that created them and are dropped when the session ends.","poster":"AndreFR"},{"poster":"Huroye","timestamp":"1700087040.0","content":"Answer is D. key phrase is \"...does not need to be used by other data engineers in other sessions...\"","upvote_count":"2","comment_id":"1071964"},{"upvote_count":"1","content":"Selected Answer: D\nD is right option","poster":"kishore1980","comment_id":"1057676","timestamp":"1698670020.0"}],"answer":"D","question_images":[]}],"exam":{"isBeta":false,"id":162,"isMCOnly":true,"provider":"Databricks","lastUpdated":"12 Apr 2025","isImplemented":true,"name":"Certified Data Engineer Associate","numberOfQuestions":169},"currentPage":25},"__N_SSP":true}