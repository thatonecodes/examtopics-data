{"pageProps":{"questions":[{"id":"4LLWPsJoV1fKhFEWcnpe","unix_timestamp":1699183740,"choices":{"C":"Network I/O never spikes","E":"CPU Utilization is around 75%","A":"The five Minute Load Average remains consistent/flat","B":"Bytes Received never exceeds 80 million bytes per second","D":"Total Disk Space remains constant"},"question_id":186,"isMC":true,"question_images":[],"answer_ET":"E","timestamp":"2023-11-05 12:29:00","discussion":[{"poster":"sturcu","content":"Selected Answer: E\nI would look at max CPU utilization and max Memory usage.\nHaving 75% CPU usage would signify we have a proper utilization of cpu resources","comment_id":"1062817","upvote_count":"7","timestamp":"1714901340.0"},{"upvote_count":"2","comment_id":"1141671","poster":"vctrhugo","content":"Selected Answer: E\nProper utilization of VM resources, especially in a distributed computing environment like Spark, often involves efficient usage of CPU resources. A CPU utilization around 75% indicates that the CPU is being utilized without being fully saturated, allowing room for additional processing without causing excessive contention.","timestamp":"1722899880.0"},{"upvote_count":"1","poster":"alexvno","comment_id":"1100376","timestamp":"1718771400.0","content":"Selected Answer: E\n75% good"},{"poster":"aragorn_brego","comment_id":"1076694","content":"Selected Answer: E\nAn average CPU utilization around 75% is a good indicator of proper utilization of the VM's resources in a distributed computing environment. It suggests that the CPUs are being actively used for computation without being maxed out, which could indicate a bottleneck. It leaves some headroom to handle additional load without causing excessive queuing or delays.","upvote_count":"3","timestamp":"1716318660.0"}],"answer":"E","exam_id":163,"answers_community":["E (100%)"],"answer_images":[],"question_text":"When evaluating the Ganglia Metrics for a given cluster with 3 executor nodes, which indicator would signal proper utilization of the VM's resources?","topic":"1","url":"https://www.examtopics.com/discussions/databricks/view/125408-exam-certified-data-engineer-professional-topic-1-question/","answer_description":""},{"id":"lahZOckwB9uht3pLJ1xF","choices":{"C":"pyspsark.ml.feature","A":"Regex","E":"C++","B":"Julia","D":"Scala Datasets"},"answer_images":[],"question_id":187,"exam_id":163,"answers_community":["A (89%)","11%"],"topic":"1","unix_timestamp":1698215580,"answer":"A","answer_ET":"A","answer_description":"","question_text":"Which of the following technologies can be used to identify key areas of text when parsing Spark Driver log4j output?","discussion":[{"comment_id":"1141668","timestamp":"1722899520.0","upvote_count":"3","poster":"vctrhugo","content":"Selected Answer: A\nIt allows us to define patterns that match the structure of the log entries and capture relevant data."},{"content":"Selected Answer: A\nRegular expressions (regex) can be used to identify and extract patterns from text data, which makes them very useful for parsing log files like the Spark Driver's log4j output. By defining specific regex patterns, you can search for error messages, timestamps, specific log levels, or any other text that follows a particular format within the log files.","comment_id":"1076699","poster":"aragorn_brego","upvote_count":"4","timestamp":"1716318780.0"},{"poster":"sturcu","content":"Selected Answer: A\nRegex to extract text","comment_id":"1057429","upvote_count":"3","timestamp":"1714454640.0"},{"content":"Selected Answer: A\nregex is for string identification","timestamp":"1714416360.0","poster":"hm358","upvote_count":"2","comment_id":"1057124"},{"upvote_count":"4","poster":"mouad_attaqi","comment_id":"1056075","content":"Selected Answer: A\nUsing regex, we can identify key ans values areas","timestamp":"1714295760.0"},{"poster":"sturcu","upvote_count":"1","timestamp":"1714026780.0","content":"Why C++, why not python or Java? Plus there are tools om parsing the log4j output like Chainsaw and xmlstarlet.","comment_id":"1053521"}],"question_images":[],"isMC":true,"timestamp":"2023-10-25 08:33:00","url":"https://www.examtopics.com/discussions/databricks/view/124605-exam-certified-data-engineer-professional-topic-1-question/"},{"id":"60cj5Jh2N1zHOhH7R4gy","question_text":"You are testing a collection of mathematical functions, one of which calculates the area under a curve as described by another function.\n\nassert(myIntegrate(lambda x: x*x, 0, 3) [0] == 9)\n\nWhich kind of test would the above line exemplify?","isMC":true,"answer":"A","url":"https://www.examtopics.com/discussions/databricks/view/130110-exam-certified-data-engineer-professional-topic-1-question/","exam_id":163,"discussion":[{"content":"Selected Answer: A\nAnswer is A, unit test","comment_id":"1207354","timestamp":"1715002500.0","upvote_count":"3","poster":"Nickff"},{"poster":"barnac1es","content":"Selected Answer: C\nI think it should be Functional Test","comment_id":"1180359","timestamp":"1711145520.0","upvote_count":"3","comments":[{"content":"There are 3 testing types:\nUnit testing\nIntegration testing\nAnd end to end testing","comment_id":"1255694","upvote_count":"5","poster":"HelixAbdu","timestamp":"1721994180.0"}]},{"poster":"vctrhugo","upvote_count":"3","timestamp":"1707181860.0","comment_id":"1141667","content":"Selected Answer: A\nA. Unit"},{"poster":"divingbell17","content":"Selected Answer: A\nA is correct","comment_id":"1111480","timestamp":"1704149280.0","upvote_count":"3"}],"topic":"1","answer_images":[],"answer_description":"","question_id":188,"answers_community":["A (75%)","C (25%)"],"answer_ET":"A","timestamp":"2024-01-01 23:48:00","choices":{"B":"Manual","A":"Unit","C":"Functional","D":"Integration","E":"End-to-end"},"question_images":[],"unix_timestamp":1704149280},{"id":"vMuyBdca92FBzYgQVb5D","answer_images":[],"exam_id":163,"answer":"D","discussion":[{"comment_id":"1056076","content":"Selected Answer: D\nD is correct, taks B and C will definitely be skipped, since Task A is notebook, the ACID logic is at cell level, some logic might be executed before failing cell","timestamp":"1730114280.0","poster":"mouad_attaqi","upvote_count":"6"},{"upvote_count":"4","timestamp":"1732223820.0","comment_id":"1076705","content":"Selected Answer: D\nIn Databricks job execution, if a task that other tasks depend on fails, the dependent tasks will not be executed. Since Tasks B and C depend on the successful completion of Task A, they will be skipped if Task A fails. However, if Task A performs any operations that commit changes before the failure occurs (such as writing to a Delta table), those changes remain and are not automatically rolled back unless the logic within Task A specifically includes rollback mechanisms for partial failures.","poster":"aragorn_brego"},{"comment_id":"1066315","poster":"Dileepvikram","upvote_count":"3","content":"D is the answer","timestamp":"1731146040.0"},{"timestamp":"1729838100.0","poster":"sturcu","comment_id":"1053525","content":"Selected Answer: D\nSome ops in task A may have fished before fail","upvote_count":"3"}],"timestamp":"2023-10-25 08:35:00","answers_community":["D (100%)"],"unix_timestamp":1698215700,"question_id":189,"topic":"1","answer_description":"","question_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/124606-exam-certified-data-engineer-professional-topic-1-question/","question_text":"A Databricks job has been configured with 3 tasks, each of which is a Databricks notebook. Task A does not depend on other tasks. Tasks B and C run in parallel, with each having a serial dependency on Task A.\n\nIf task A fails during a scheduled run, which statement describes the results of this run?","isMC":true,"choices":{"C":"Unless all tasks complete successfully, no changes will be committed to the Lakehouse; because task A failed, all commits will be rolled back automatically.","A":"Because all tasks are managed as a dependency graph, no changes will be committed to the Lakehouse until all tasks have successfully been completed.","B":"Tasks B and C will attempt to run as configured; any changes made in task A will be rolled back due to task failure.","D":"Tasks B and C will be skipped; some logic expressed in task A may have been committed before task failure.","E":"Tasks B and C will be skipped; task A will not commit any changes because of stage failure."},"answer_ET":"D"},{"id":"yCkzkxWy84EEsEVpEYQv","isMC":true,"choices":{"B":"Cmd 1 will succeed. Cmd 2 will search all accessible databases for a table or view named countries_af: if this entity exists, Cmd 2 will succeed.","A":"Both commands will succeed. Executing show tables will show that countries_af and sales_af have been registered as views.","C":"Cmd 1 will succeed and Cmd 2 will fail. countries_af will be a Python variable representing a PySpark DataFrame.","D":"Both commands will fail. No new variables, tables, or views will be created.","E":"Cmd 1 will succeed and Cmd 2 will fail. countries_af will be a Python variable containing a list of strings."},"question_images":["https://img.examtopics.com/certified-data-engineer-professional/image8.png"],"topic":"1","question_text":"A junior member of the data engineering team is exploring the language interoperability of Databricks notebooks. The intended outcome of the below code is to register a view of all sales that occurred in countries on the continent of Africa that appear in the geo_lookup table.\nBefore executing the code, running SHOW TABLES on the current database indicates the database contains only two tables: geo_lookup and sales.\n//IMG//\n\nWhich statement correctly describes the outcome of executing these command cells in order in an interactive notebook?","answer":"E","answer_images":[],"question_id":190,"discussion":[{"content":"Selected Answer: E\nCmd 1 is a PySpark command that collects the list of countries from the 'geo_lookup' table where the continent is Africa ('AF'). This command will execute successfully, resulting in countries_af being a list of country names (strings) in Python's local memory.\n\nCmd 2 is an SQL command intended to create a view named 'sales_af' from the 'sales' table, filtered by the cities in the countries_af list. However, this will fail because the countries_af variable exists in the Python environment and is not recognized in the SQL context. SQL does not have access to Python variables directly; they are two separate execution contexts within a Databricks notebook. There is no table or view named countries_af that SQL can reference; it is merely a Python list variable.\n\nThe other options are incorrect because they either assume cross-contextual operation between Python and SQL within a Databricks notebook (which is not possible in the way described in the commands), or they do not correctly interpret the outcome of running the commands.","comments":[{"timestamp":"1734196740.0","comment_id":"1326549","upvote_count":"1","poster":"freely","content":"I mean without sepecifying the catalog and the schema in a unity catalog context ? this will only succeed if the table is in the defaut catalog and schema"}],"comment_id":"1075925","poster":"aragorn_brego","timestamp":"1700522460.0","upvote_count":"11"},{"poster":"benni_ale","upvote_count":"1","content":"Selected Answer: E\nE , the collect method outputs strings so the python variable bill be a list of string which should not be called as a spark table as in cmd 2","comment_id":"1292755","timestamp":"1727951520.0"},{"timestamp":"1717547520.0","poster":"imatheushenrique","content":"E. Cmd 1 will succeed and Cmd 2 will fail. countries_af will be a Python variable containing a list of strings.","comment_id":"1224440","upvote_count":"1"},{"timestamp":"1712599200.0","comments":[{"comment_id":"1268704","content":"%python\nprint(countries_af)\ntype(countries_af)","upvote_count":"1","timestamp":"1724073780.0","poster":"AndreFR"}],"comment_id":"1191739","content":"Selected Answer: E\nE is correct.\n\n%sql\ncreate table geo_lookup (continent varchar(2), country varchar(15));\ninsert into geo_lookup (continent, country) values\n('AF','Nigeria'),\n('AF','Kenya');\ncreate table sales (city varchar(15), continent varchar(2));\ninsert into sales (city, continent) values\n('Nigeria','AF'),\n('Kenya','AF');\n\n%python\ncountries_af = [x[0] for x in spark.table('geo_lookup').filter(\"continent='AF'\").select('country').collect()]\n\n%sql\ncreate view sales_af as\nselect *\nfrom sales\nwhere city in countries_af\nand continent = \"AF\";\n\nParseException: [PARSE_SYNTAX_ERROR] Syntax error at or near 'in'.(line 4, pos 11)\n\ni.e. countries_af is a python list of strings and can't be used inside a sql statement","poster":"juliom6","upvote_count":"3"},{"upvote_count":"1","poster":"leopedroso1","content":"By simulating this code in databricks we can see an error being thrown in the SQL statement\n\nParseException: \n[PARSE_SYNTAX_ERROR] Syntax error at or near 'IN'.(line 1, pos 38)\n\n== SQL ==\nSELECT * FROM backup.sales WHERE CITY IN countries_af AND CONTINENT = \"AF\"","comment_id":"1150223","timestamp":"1707915360.0"},{"comments":[{"content":"how could it succed if all people tested sql parse syntax error?","poster":"benni_ale","comment_id":"1286677","upvote_count":"1","timestamp":"1726812060.0"}],"upvote_count":"1","content":"Selected Answer: B\nB shows the actual flow of spark sql, where E shows the question context, i mean from databricks point of view E never looked, it's true that question state that database has no other tables, so ?? that mean databricks will not check for that particular table ? it will right ? i also confused by \"database has no other database statement\" and E and B both are right, but again B state \"if countries table exists then command 2 will run\" here \"if\" used, but question want to describe the language interoperability, so most of us selected E","poster":"RiktRikt007","comment_id":"1145943","timestamp":"1707544980.0"},{"timestamp":"1707393960.0","comment_id":"1144403","content":"E is correct","upvote_count":"2","poster":"PrashantTiwari"},{"content":"Selected Answer: E\nvote for E","timestamp":"1705149720.0","upvote_count":"2","comment_id":"1121649","poster":"Jay_98_11"},{"timestamp":"1704891240.0","comment_id":"1118553","content":"Selected Answer: E\nE is correct answer","upvote_count":"1","poster":"kz_data"},{"upvote_count":"1","comment_id":"1062069","timestamp":"1699098900.0","content":"Selected Answer: B\nhttps://docs.databricks.com/en/notebooks/notebooks-code.html#mix-languages\nVariables defined in one language (and hence in the REPL for that language) are not available in the REPL of another language","poster":"ismoshkov","comments":[{"timestamp":"1701066480.0","comment_id":"1081248","content":"It is mentioned there exists only 2 objects in database. so B is not an option","upvote_count":"1","poster":"Naveenkm"},{"upvote_count":"2","timestamp":"1699138800.0","comment_id":"1062457","content":"even if it exists, a table or a view won't work in cmd 2","poster":"Karen1232123"}]},{"timestamp":"1697005620.0","comment_id":"1040245","upvote_count":"1","poster":"sturcu","content":"Selected Answer: E\ncorrect"},{"poster":"lucasasterio","comment_id":"1000703","upvote_count":"2","timestamp":"1694009400.0","content":"Selected Answer: E\ncorrect"},{"content":"E is right nswer","timestamp":"1693150500.0","upvote_count":"2","comment_id":"991538","poster":"Eertyy"}],"answer_description":"","unix_timestamp":1693150500,"timestamp":"2023-08-27 17:35:00","answer_ET":"E","url":"https://www.examtopics.com/discussions/databricks/view/119186-exam-certified-data-engineer-professional-topic-1-question-9/","exam_id":163,"answers_community":["E (91%)","9%"]}],"exam":{"lastUpdated":"12 Apr 2025","isImplemented":true,"isMCOnly":true,"name":"Certified Data Engineer Professional","numberOfQuestions":200,"isBeta":false,"id":163,"provider":"Databricks"},"currentPage":38},"__N_SSP":true}