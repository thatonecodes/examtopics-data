{"pageProps":{"questions":[{"id":"H15MG7w2KSjqnl96FurD","answers_community":[],"topic":"1","question_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/146531-exam-certified-data-engineer-associate-topic-1-question-117/","isMC":true,"unix_timestamp":1724750460,"answer_ET":"D","answer_images":[],"answer":"D","question_text":"A data engineer has three tables in a Delta Live Tables (DLT) pipeline. They have configured the pipeline to drop invalid records at each table. They notice that some data is being dropped due to quality concerns at some point in the DLT pipeline. They would like to determine at which table in their pipeline the data is being dropped.\n\nWhich approach can the data engineer take to identify the table that is dropping the records?","question_id":21,"exam_id":162,"discussion":[{"timestamp":"1724750460.0","comment_id":"1273332","upvote_count":"5","poster":"9d4d68a","content":"Repeated, Correct"}],"timestamp":"2024-08-27 11:21:00","answer_description":"","choices":{"D":"They can navigate to the DLT pipeline page, click on each table, and view the data quality statistics.","A":"They can set up separate expectations for each table when developing their DLT pipeline.","B":"They can navigate to the DLT pipeline page, click on the “Error” button, and review the present errors.","C":"They can set up DLT to notify them via email when records are dropped."}},{"id":"dj77z4lwOSYcRjblcFPJ","discussion":[{"timestamp":"1742494020.0","poster":"Lili97","comment_id":"1401251","upvote_count":"2","content":"Selected Answer: D\nHello, is it usual to have duplicated questions? What is the point of paying if some questions are repeated?"},{"comment_id":"1330847","content":"Selected Answer: A\nA is correct.\nI had this on the exam, from my results it seems so. \nI chose D and didn't max this area and I was sure of all other answers.","poster":"grygi","timestamp":"1734966120.0","upvote_count":"1"},{"poster":"MultiCloudIronMan","comment_id":"1327314","content":"Selected Answer: D\nThe correct answer is D. Checkpointing and Idempotent Sinks. In Structured Streaming, Spark uses checkpointing to reliably track the progress of the data being processed. Checkpointing saves the state of the streaming query, including the offset ranges of the data processed in each trigger. Idempotent sinks ensure that even if the same data is processed multiple times due to a failure and restart, the results remain consistent and correct.","timestamp":"1734350580.0","upvote_count":"1"},{"comment_id":"1313097","timestamp":"1731770160.0","poster":"NzmD","upvote_count":"1","comments":[{"timestamp":"1735545540.0","content":"Repeated and false. It’s D","poster":"CaoMengde09","upvote_count":"1","comment_id":"1333971"}],"content":"Selected Answer: A\nRepeated!"},{"upvote_count":"2","poster":"9d4d68a","comment_id":"1273345","timestamp":"1724753100.0","content":"Repeated, Correct\n\nThe correct answer is A. Checkpointing and Write-ahead Logs. Checkpointing records the progress of streaming queries, while write-ahead logs (WALs) capture the data before it is processed, allowing Spark to recover and process data reliably in case of failures."}],"topic":"1","question_text":"What is used by Spark to record the offset range of the data being processed in each trigger in order for Structured Streaming to reliably track the exact progress of the processing so that it can handle any kind of failure by restarting and/or reprocessing?","timestamp":"2024-08-27 12:05:00","answer_images":[],"question_id":22,"exam_id":162,"unix_timestamp":1724753100,"choices":{"C":"Write-ahead Logs and Idempotent Sinks","D":"Checkpointing and Idempotent Sinks","B":"Replayable Sources and Idempotent Sinks","A":"Checkpointing and Write-ahead Logs"},"isMC":true,"answer_ET":"A","question_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/146534-exam-certified-data-engineer-associate-topic-1-question-118/","answers_community":["D (60%)","A (40%)"],"answer_description":"","answer":"D"},{"id":"R75OKQYdzOoHvmDwhR7b","isMC":true,"answer_description":"","question_images":[],"choices":{"A":"Gold tables are more likely to contain aggregations than Silver tables.","B":"Gold tables are more likely to contain valuable data than Silver tables.","D":"Gold tables are more likely to contain truthful data than Silver tables.","C":"Gold tables are more likely to contain a less refined view of data than Silver tables."},"answers_community":["A (100%)"],"discussion":[{"content":"Selected Answer: A\nGold is final stage to feed analytics platforms","timestamp":"1734350640.0","upvote_count":"1","comment_id":"1327316","poster":"MultiCloudIronMan"},{"comment_id":"1273343","upvote_count":"2","content":"Repeated, Correct","timestamp":"1724752920.0","poster":"9d4d68a"}],"answer_ET":"A","exam_id":162,"answer_images":[],"unix_timestamp":1724752920,"question_text":"What describes the relationship between Gold tables and Silver tables?","question_id":23,"url":"https://www.examtopics.com/discussions/databricks/view/146532-exam-certified-data-engineer-associate-topic-1-question-119/","answer":"A","timestamp":"2024-08-27 12:02:00","topic":"1"},{"id":"9Gm25ynrYzW2IyOJxUlu","timestamp":"2023-04-01 16:03:00","answer_ET":"E","choices":{"B":"There is no way to share data between PySpark and SQL.","D":"spark.table(\"sales\")","C":"spark.sql(\"sales\")D. spark.delta.table(\"sales\")","A":"SELECT * FROM sales"},"exam_id":162,"question_images":[],"answer":"E","discussion":[{"content":"E\nThe spark.table() function in PySpark allows you to access tables registered in the catalog, including Delta tables. By specifying the table name (\"sales\"), the data engineering team can read the Delta table and perform various operations on it using PySpark.\n\nOption A, SELECT * FROM sales, is a SQL syntax and cannot be directly used in PySpark.\n\nOption B, \"There is no way to share data between PySpark and SQL,\" is incorrect. PySpark provides the capability to interact with data using both SQL and DataFrame/DataSet APIs.\n\nOption C, spark.sql(\"sales\"), is a valid command to execute SQL queries on registered tables in PySpark. However, in this case, the \"sales\" argument alone is not a valid SQL query.\n\nOption D, spark.delta.table(\"sales\"), is a specific method provided by Delta Lake to access Delta tables directly. While it can be used to access the \"sales\" table, it is not the most common approach in PySpark.","comment_id":"946766","poster":"Atnafu","timestamp":"1688853900.0","upvote_count":"14"},{"content":"Selected Answer: E\nThis answer is pure python and is a simple solution for the Question.","comment_id":"1344724","poster":"dhohigh","upvote_count":"1","timestamp":"1737546120.0"},{"comment_id":"1272813","poster":"9d4d68a","upvote_count":"1","timestamp":"1724684880.0","content":"To access the Delta table sales using PySpark, the data engineering team can use the following command:\n\nE. spark.table(\"sales\")\n\nThis command allows them to load the table into a PySpark DataFrame, which they can then use for their tests and data processing in Python.\nNo, the command spark.delta.table(\"table name\") does not exist in PySpark. To access a Delta table, you should use:\n\nspark.table(\"table name\")\n\nOr, if you need to use Delta-specific functionality, you would typically use Delta's APIs or spark.read.format(\"delta\").table(\"table name\") to read the table into a DataFrame."},{"timestamp":"1723103520.0","upvote_count":"1","content":"Selected Answer: E\nE. spark.table(\"sales\")\n\nThis command allows the team to access the table using PySpark, enabling them to implement their tests in Python.","comment_id":"1262396","poster":"80370eb"},{"content":"spark.table() . E is the correct one","upvote_count":"1","comment_id":"1252366","timestamp":"1721555700.0","poster":"souldiv"},{"comment_id":"1203171","poster":"benni_ale","timestamp":"1714230300.0","upvote_count":"1","content":"Selected Answer: E\nE is correct"},{"content":"Selected Answer: E\ne is correct","upvote_count":"2","comment_id":"1189113","timestamp":"1712209980.0","poster":"benni_ale"},{"poster":"Itmma","comment_id":"1177189","upvote_count":"1","content":"Selected Answer: E\nE is correct","timestamp":"1710841620.0"},{"upvote_count":"1","content":"Selected Answer: E\nCorrect answer is E","poster":"SerGrey","comment_id":"1113192","timestamp":"1704322140.0"},{"poster":"Garyn","comment_id":"1109089","upvote_count":"4","timestamp":"1703883000.0","content":"Selected Answer: E\nE. spark.table(\"sales\")\n\nThe spark.table() function in PySpark allows access to a registered table within the SparkSession. In this case, \"sales\" is the name of the Delta table created by the data analyst, and the spark.table() function enables access to this table for performing data engineering tests using Python (PySpark)."},{"content":"C is correct Answer","timestamp":"1703594580.0","poster":"csd","comment_id":"1106007","upvote_count":"1"},{"comment_id":"1064788","content":"Selected Answer: E\nCorrect is E","poster":"awofalus","upvote_count":"1","timestamp":"1699361160.0"},{"poster":"KalavathiP","timestamp":"1695697980.0","comment_id":"1017350","content":"Selected Answer: E\nE is correct","upvote_count":"1"},{"timestamp":"1695625860.0","content":"Selected Answer: E\ndelta is default.","upvote_count":"1","comment_id":"1016561","poster":"d_b47"},{"poster":"ThomasReps","comment_id":"921433","content":"Selected Answer: E\nIt's E. As stated by others, the default format is delta\n\nIf you try to run D, you get an error, that there are no \"delta\"-command for spark: \"AttributeError: 'SparkSession' object has no attribute 'delta'\". If you want to explicit tell it should be delta, then you need an \".option(format='delta')\" insted.","timestamp":"1686571380.0","upvote_count":"2"},{"poster":"Dwarakkrishna","timestamp":"1685847540.0","content":"You access data in Delta tables by the table name or the table path, as shown in the following examples:\npeople_df = spark.read.table(table_name)\n\ndisplay(people_df)","upvote_count":"1","comment_id":"914071"},{"timestamp":"1683888900.0","upvote_count":"1","comment_id":"895846","content":"I believe the answer is E as in databricks the default tables are delta tables hence spark.table should be enough. Have not seen a spark.delta.table function before.","poster":"prasioso"},{"timestamp":"1683618300.0","poster":"Tickxit","content":"Selected Answer: E\nE: spark.table or spark.read.table","comment_id":"892904","upvote_count":"2"},{"comments":[{"timestamp":"1699477320.0","comment_id":"1065956","poster":"qium","upvote_count":"1","content":"default type type is \"delta\"."}],"timestamp":"1683189000.0","upvote_count":"1","comment_id":"889298","content":"Correct Answer is D spark.delta.table(\"sales\") And the reason that its asking for delta table not normal table if its for normal table then it should be spark.table(\"sales\")","poster":"softthinkers"},{"comment_id":"889065","upvote_count":"1","poster":"Majjjj","timestamp":"1683160200.0","content":"The correct answer is D. \n\nThe data engineering team can access the Delta table sales in PySpark by using the spark.delta.table command. This command is used to create a DataFrame based on a Delta table. Therefore, the correct command is spark.delta.table(\"sales\")."},{"upvote_count":"1","content":"Option E -\nhttps://spark.apache.org/docs/3.2.1/api/python/reference/api/pyspark.sql.SparkSession.table.html","comment_id":"876207","timestamp":"1682053020.0","poster":"Varma_Saraswathula"},{"comment_id":"875861","timestamp":"1682013960.0","content":"Option E","upvote_count":"1","poster":"naxacod574"},{"upvote_count":"2","poster":"azurearch","timestamp":"1681287780.0","content":"option E","comment_id":"868052"},{"comment_id":"867435","timestamp":"1681225860.0","poster":"SireeJ","upvote_count":"1","content":"Option: D"},{"comment_id":"860627","upvote_count":"2","content":"Option E","timestamp":"1680583320.0","poster":"sdas1"},{"timestamp":"1680563340.0","content":"Selected Answer: E\nCreamos una tabla: create or replace table delta_su (id INT , nombre STRING)\nInsertamos la tabla y posteriomente obtenemos los valores registrados con : spark.table(\"delta_su\").show()","upvote_count":"3","poster":"knivesz","comment_id":"860422"},{"content":"E is correct, spark.table(\"sales\")","comment_id":"860276","timestamp":"1680549240.0","upvote_count":"4","poster":"Retko"},{"comments":[{"timestamp":"1680563400.0","poster":"knivesz","content":"Asi obtenemos el contenido de la tabla delta, spark.table(\"tabladelta\").show(), la C es para consulta SQL por ejemplo: spark.sql(\"SELECT * FROM sales\").show()","upvote_count":"1","comment_id":"860424"}],"comment_id":"857990","upvote_count":"1","timestamp":"1680357780.0","content":"Selected Answer: C\nCorrect answer is C","poster":"XiltroX"}],"answers_community":["E (95%)","5%"],"topic":"1","isMC":true,"answer_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/104748-exam-certified-data-engineer-associate-topic-1-question-12/","unix_timestamp":1680357780,"question_text":"A data analyst has created a Delta table sales that is used by the entire data analysis team. They want help from the data engineering team to implement a series of tests to ensure the data is clean. However, the data engineering team uses Python for its tests rather than SQL.\nWhich of the following commands could the data engineering team use to access sales in PySpark?","question_id":24,"answer_description":""},{"id":"JxCrj6gQstqtDXOGPCxg","timestamp":"2024-08-27 12:05:00","answers_community":["B (100%)"],"choices":{"A":"CREATE STREAMING LIVE TABLE should be used when the subsequent step in the DLT pipeline is static.","B":"CREATE STREAMING LIVE TABLE should be used when data needs to be processed incrementally.","C":"CREATE STREAMING LIVE TABLE should be used when data needs to be processed through complicated aggregations.","D":"CREATE STREAMING LIVE TABLE should be used when the previous step in the DLT pipeline is static."},"discussion":[{"timestamp":"1734350760.0","content":"Selected Answer: B\nStreaming data from source to destination","comment_id":"1327318","poster":"MultiCloudIronMan","upvote_count":"1"},{"content":"Repeated, Correct","timestamp":"1724753100.0","comment_id":"1273344","upvote_count":"2","poster":"9d4d68a"}],"answer_description":"","answer_images":[],"unix_timestamp":1724753100,"exam_id":162,"question_id":25,"isMC":true,"question_images":[],"question_text":"What describes when to use the CREATE STREAMING LIVE TABLE (formerly CREATE INCREMENTAL LIVE TABLE) syntax over the CREATE LIVE TABLE syntax when creating Delta Live Tables (DLT) tables using SQL?","url":"https://www.examtopics.com/discussions/databricks/view/146533-exam-certified-data-engineer-associate-topic-1-question-120/","answer":"B","topic":"1","answer_ET":"B"}],"exam":{"id":162,"lastUpdated":"12 Apr 2025","numberOfQuestions":169,"name":"Certified Data Engineer Associate","isMCOnly":true,"provider":"Databricks","isBeta":false,"isImplemented":true},"currentPage":5},"__N_SSP":true}