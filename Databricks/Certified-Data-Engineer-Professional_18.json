{"pageProps":{"questions":[{"id":"EGDkICKOhbEAEYwJx5Gd","question_text":"Incorporating unit tests into a PySpark application requires upfront attention to the design of your jobs, or a potentially significant refactoring of existing code.\n\nWhich benefit offsets this additional effort?","isMC":true,"unix_timestamp":1717191600,"answer":"C","question_id":86,"timestamp":"2024-05-31 23:40:00","exam_id":163,"choices":{"B":"Validates a complete use case of your application","C":"Troubleshooting is easier since all steps are isolated and tested individually","A":"Improves the quality of your data","D":"Ensures that all steps interact correctly to achieve the desired end result"},"answer_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/141700-exam-certified-data-engineer-professional-topic-1-question/","discussion":[{"comment_id":"1300631","content":"Selected Answer: C\nC is testing each unit of the solution separately. It doesn't necessarily validate the data quality as mentioned in A.\nB is more for Business Case scenario testing like end-to-end testing for real life, real data execution.\nD is more related to Integration testing.","poster":"m79590530","upvote_count":"1","timestamp":"1729446780.0"},{"content":"C. Troubleshooting is easier since all steps are isolated and tested individually\nThe unit tests will ensuree that specific functions and transformations will work as intended.","poster":"imatheushenrique","comment_id":"1222347","timestamp":"1717191600.0","upvote_count":"1"}],"answer_ET":"C","answers_community":["C (100%)"],"question_images":[],"answer_description":"","topic":"1"},{"id":"BVVA949iMldGOz9Fi96I","isMC":true,"discussion":[{"poster":"m79590530","content":"Selected Answer: D\nInteractions and cooperation between the solution and/or interfacing systems/data consumers components, subsystems and interfaces is exactly Integration testing.","timestamp":"1729447020.0","upvote_count":"1","comment_id":"1300634"},{"poster":"imatheushenrique","comment_id":"1222346","content":"D. It validates interactions between subsystems of your application.\nAn integration test is used for different softwares validation components, subsystems, or applications that has dependencies.","timestamp":"1717191480.0","upvote_count":"2"}],"unix_timestamp":1717191480,"exam_id":163,"question_id":87,"topic":"1","question_text":"What describes integration testing?","answers_community":["D (100%)"],"answer_ET":"D","url":"https://www.examtopics.com/discussions/databricks/view/141699-exam-certified-data-engineer-professional-topic-1-question/","answer_description":"","timestamp":"2024-05-31 23:38:00","question_images":[],"choices":{"B":"It validates behavior of individual elements of an application,","C":"It requires an automated testing framework.","A":"It validates an application use case.","D":"It validates interactions between subsystems of your application."},"answer_images":[],"answer":"D"},{"id":"tTFbNVzpPoerY5nqMHkt","discussion":[{"poster":"m79590530","comment_id":"1300638","timestamp":"1729447200.0","upvote_count":"1","content":"Selected Answer: B\nEvery job run creates and assigns a globally unique run ID to the job RUN as well as globally unique run ID's for the Tasks RUN's inside the Job."}],"choices":{"B":"The globally unique ID of the newly triggered run.","A":"The job_id and number of times the job has been run are concatenated and returned.","D":"The job_id is returned in this field.","C":"The number of times the job definition has been run in this workspace."},"answer_description":"","answer_ET":"B","answer":"B","unix_timestamp":1717191360,"isMC":true,"topic":"1","url":"https://www.examtopics.com/discussions/databricks/view/141698-exam-certified-data-engineer-professional-topic-1-question/","question_images":[],"question_id":88,"answers_community":["B (100%)"],"timestamp":"2024-05-31 23:36:00","exam_id":163,"question_text":"The Databricks CLI is used to trigger a run of an existing job by passing the job_id parameter. The response that the job run request has been submitted successfully includes a field run_id.\n\nWhich statement describes what the number alongside this field represents?","answer_images":[]},{"id":"m0v8KDZYlM6jiSgNgAA8","timestamp":"2024-05-31 23:34:00","answer":"A","question_images":[],"answer_description":"","topic":"1","discussion":[{"poster":"m79590530","timestamp":"1729447620.0","content":"Selected Answer: A\nEach Notebook or Task consists of multiple commands and actions performed by them. Each action may be on the data in the Delta Lake where ACID transactions take place and fully rollback certain data manipulations if some of them fail but the Notebooks/Tasks in the Job themselves will not completely fail or rollback. Therefore Answer A correctly describes the result considering the dependencies configures between the Notebooks/Tasks as described in the question.","upvote_count":"1","comment_id":"1300641"},{"comment_id":"1222341","poster":"imatheushenrique","upvote_count":"1","content":"A: A. All logic expressed in the notebook associated with tasks A and B will have been successfully completed; some operations in task C may have completed successfully.\n\nBecause this type of orchestration indicates a Fan-Out.","timestamp":"1717191240.0"}],"answer_ET":"A","question_text":"A Databricks job has been configured with three tasks, each of which is a Databricks notebook. Task A does not depend on other tasks. Tasks B and C run in parallel, with each having a serial dependency on task A.\n\nWhat will be the resulting state if tasks A and B complete successfully but task C fails during a scheduled run?","isMC":true,"answers_community":["A (100%)"],"choices":{"D":"All logic expressed in the notebook associated with tasks A and B will have been successfully completed; any changes made in task C will be rolled back due to task failure.","B":"Unless all tasks complete successfully, no changes will be committed to the Lakehouse; because task C failed, all commits will be rolled back automatically.","C":"Because all tasks are managed as a dependency graph, no changes will be committed to the Lakehouse until all tasks have successfully been completed.","A":"All logic expressed in the notebook associated with tasks A and B will have been successfully completed; some operations in task C may have completed successfully."},"answer_images":[],"question_id":89,"unix_timestamp":1717191240,"exam_id":163,"url":"https://www.examtopics.com/discussions/databricks/view/141697-exam-certified-data-engineer-professional-topic-1-question/"},{"id":"21EnX3xmZksEeqkBmYJt","answer":"A","answer_images":[],"answer_ET":"A","question_id":90,"isMC":true,"answer_description":"","topic":"1","question_images":[],"question_text":"Which statement regarding stream-static joins and static Delta tables is correct?","discussion":[{"poster":"Eertyy","timestamp":"1695314460.0","upvote_count":"12","comments":[{"content":"The explanation suggests the author would like the stream-static join to work in this way. However, it works as it does - see the first sentence in here: https://learn.microsoft.com/en-us/azure/databricks/transform/join#stream-static","timestamp":"1735646700.0","poster":"arekm","comment_id":"1334770","upvote_count":"3"},{"poster":"hamzaKhribi","upvote_count":"13","content":"Answer is A, When Azure Databricks processes a micro-batch of data in a stream-static join, the latest valid version of data from the static Delta table joins with the records present in the current micro-batch\nfrom https://learn.microsoft.com/en-us/azure/databricks/structured-streaming/delta-lake","comment_id":"1086136","timestamp":"1701517380.0"}],"content":"B is the right answer as Option B is more typical for stream-static joins, as it provides a consistent static DataFrame snapshot for the entire job's duration. Option A might be suitable in specialized cases where you need real-time updates of the static DataFrame for each microbatch.","comment_id":"1013280"},{"content":"correct answer is A","upvote_count":"6","poster":"BrianNguyen95","comment_id":"983582","timestamp":"1692273900.0"},{"content":"Selected Answer: A\nA is correct, see: https://learn.microsoft.com/en-us/azure/databricks/transform/join#stream-static","upvote_count":"3","poster":"arekm","timestamp":"1735646580.0","comment_id":"1334769"},{"timestamp":"1733986920.0","content":"Selected Answer: B\nA stream-static join joins the latest valid version of a Delta table (the static data) to a data stream using a stateless join.\n\nWhen Databricks processes a micro-batch of data in a stream-static join, the latest valid version of data from the static Delta table joins with the records present in the current micro-batch. Because the join is stateless, you do not need to configure watermarking and can process results with low latency. The data in the static Delta table used in the join should be slowly-changing.","upvote_count":"1","poster":"Sriramiyer92","comment_id":"1325453"},{"comment_id":"1325446","poster":"Sriramiyer92","upvote_count":"1","content":"Selected Answer: A\nhttps://docs.databricks.com/en/transform/join.html#stream-static","timestamp":"1733986440.0"},{"poster":"akashdesarda","timestamp":"1727671920.0","content":"Selected Answer: A\nThis is straight from docs, \"A stream-static join joins the latest valid version of a Delta table (the static data) to a data stream using a stateless join.\n\nWhen Azure Databricks processes a micro-batch of data in a stream-static join, the latest valid version of data from the static Delta table joins with the records present in the current micro-batch. Because the join is stateless, you do not need to configure watermarking and can process results with low latency. The data in the static Delta table used in the join should be slowly-changing.\"\nhttps://learn.microsoft.com/en-us/azure/databricks/transform/join#stream-static","upvote_count":"1","comment_id":"1291403"},{"poster":"kz_data","upvote_count":"1","timestamp":"1704895200.0","comment_id":"1118607","content":"Selected Answer: A\ncorrect answer is A"},{"upvote_count":"1","timestamp":"1701517440.0","comment_id":"1086137","content":"Selected Answer: A\nCorrect Answer A","poster":"hamzaKhribi"},{"comment_id":"1040322","content":"Selected Answer: A\nA is correct.\nWhen Databricks processes a micro-batch of data in a stream-static join, the latest valid version of data from the static","poster":"sturcu","upvote_count":"1","timestamp":"1697010300.0"},{"timestamp":"1696145400.0","comment_id":"1022082","poster":"sagar21692","content":"Correct answer is A. https://docs.databricks.com/en/structured-streaming/delta-lake.html","upvote_count":"1"}],"choices":{"A":"Each microbatch of a stream-static join will use the most recent version of the static Delta table as of each microbatch.","D":"Stream-static joins cannot use static Delta tables because of consistency issues.","B":"Each microbatch of a stream-static join will use the most recent version of the static Delta table as of the job's initialization.","C":"The checkpoint directory will be used to track state information for the unique keys present in the join.","E":"The checkpoint directory will be used to track updates to the static Delta table."},"url":"https://www.examtopics.com/discussions/databricks/view/118335-exam-certified-data-engineer-professional-topic-1-question/","answers_community":["A (89%)","11%"],"timestamp":"2023-08-17 14:05:00","exam_id":163,"unix_timestamp":1692273900}],"exam":{"lastUpdated":"12 Apr 2025","isMCOnly":true,"name":"Certified Data Engineer Professional","numberOfQuestions":200,"isBeta":false,"id":163,"provider":"Databricks","isImplemented":true},"currentPage":18},"__N_SSP":true}