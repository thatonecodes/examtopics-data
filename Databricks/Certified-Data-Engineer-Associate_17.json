{"pageProps":{"questions":[{"id":"reXOQqFKJTSZp4JyxxnC","isMC":true,"topic":"1","unix_timestamp":1680566580,"exam_id":162,"question_id":81,"question_text":"A data engineer runs a statement every day to copy the previous day’s sales into the table transactions. Each day’s sales are in their own file in the location \"/transactions/raw\".\nToday, the data engineer runs the following command to complete this task:\n//IMG//\n\nAfter running the command today, the data engineer notices that the number of records in table transactions has not changed.\nWhich of the following describes why the statement might not have copied any new records into the table?","answer_description":"","answer":"C","answer_ET":"C","question_images":["https://img.examtopics.com/certified-data-engineer-associate/image7.png"],"discussion":[{"upvote_count":"5","comment_id":"1133109","content":"Selected Answer: C\nJust got 100% on the test. C was correct.","timestamp":"1706339040.0","poster":"Nika12"},{"upvote_count":"5","content":"Selected Answer: E\nE is the correct answer, because immediately after using copy into you might query the cashed version of the table.","poster":"ezeik","timestamp":"1695557520.0","comment_id":"1015737"},{"upvote_count":"1","timestamp":"1731954360.0","poster":"806e7d2","comment_id":"1314128","content":"Selected Answer: C\nIn Databricks, the COPY INTO command is designed to prevent duplicate data ingestion. When files are copied into a table, Databricks keeps track of the files that have already been processed using a file log. If a file has already been copied, subsequent runs of the COPY INTO command will skip that file to avoid duplication."},{"content":"Selected Answer: C\nCorrect answer is C","poster":"SerGrey","timestamp":"1704323100.0","comment_id":"1113201","upvote_count":"2"},{"timestamp":"1703884260.0","comment_id":"1109108","content":"Selected Answer: C\nC. The previous day’s file has already been copied into the table.\n\nThe COPY INTO statement is generally used to copy data from files or a location into a table. If the data engineer runs this statement daily to copy the previous day’s sales into the \"transactions\" table and the number of records hasn't changed after today's execution, it's possible that the data from today's file might not have differed from the data already present in the table.\n\nIf the files in the \"/transactions/raw\" location are expected to contain distinct data for each day and the number of records in the table remains the same, it implies that the data engineer might have already copied today's data previously, or today's data was identical to the data already present in the table.\n\nOptions A, B, D, and E don't accurately explain why the statement might not have copied new records into the table based on the provided scenario.","upvote_count":"3","poster":"Garyn"},{"poster":"awofalus","content":"Selected Answer: C\nC is correct","comment_id":"1064817","timestamp":"1699363500.0","upvote_count":"2"},{"timestamp":"1697367780.0","upvote_count":"1","poster":"kishanu","content":"If the table \"transaction\" is an external table, then option E, if its internal C should suffice.","comment_id":"1044074"},{"comment_id":"1028537","upvote_count":"1","content":"Selected Answer: C\nCOPY INTO statement does skip already copied rows.","poster":"DavidRou","timestamp":"1696831560.0"},{"poster":"KalavathiP","upvote_count":"1","content":"Selected Answer: C\nC is correct ans","timestamp":"1695698280.0","comment_id":"1017357"},{"poster":"AndreFR","upvote_count":"1","comment_id":"984288","timestamp":"1692346980.0","content":"Selected Answer: C\nhttps://docs.databricks.com/en/ingestion/copy-into/index.html \n\nThe COPY INTO SQL command lets you load data from a file location into a Delta table. This is a re-triable and idempotent operation; files in the source location that have already been loaded are skipped.\n\nif there are no new records, the only consistent choice is C no new files were loaded because already loaded files were skipped."},{"content":"C\nThe COPY INTO statement copies the data from the specified files into the target table. If the previous day's file has already been copied into the table, then the COPY INTO statement will not copy any new records into the table.","comment_id":"946014","upvote_count":"1","poster":"Atnafu","timestamp":"1688765280.0"},{"timestamp":"1685338560.0","content":"Selected Answer: C\nCOPY INTO\nLoads data from a file location into a Delta table. This is a retriable and idempotent operation—files in the source location that have already been loaded are skipped.","poster":"junction","upvote_count":"1","comment_id":"909061"},{"poster":"testdb","comments":[{"upvote_count":"2","timestamp":"1684885380.0","content":"The correct answer is letter C. \nThe use of specific files names with keyword \"FILES\" is optional as the syntax of COPY INTO declares:\n [ FILES = ( file_name [, ...] ) | PATTERN = glob_pattern ]\nWhen keyword FILES is not used in the statement all files of the directory is used once (because this operation is idempotent).","poster":"[Removed]","comment_id":"905337"}],"timestamp":"1684380180.0","content":"Selected Answer: B\nAnswer: B\nFILES = ('f1.json', 'f2.json', 'f3.json', 'f4.json', 'f5.json')\nhttps://docs.databricks.com/ingestion/copy-into/examples.html","upvote_count":"1","comment_id":"900715"},{"upvote_count":"1","comment_id":"876221","content":"C-\n\nhttps://docs.databricks.com/ingestion/copy-into/tutorial-notebook.html\nBecause this action is idempotent, you can run it multiple times but data will only be loaded once.","poster":"Varma_Saraswathula","timestamp":"1682056080.0"},{"upvote_count":"3","poster":"XiltroX","comment_id":"861297","timestamp":"1680626100.0","content":"Selected Answer: C\nOption C is the correct answer."},{"upvote_count":"1","timestamp":"1680602160.0","content":"i am not sure whether C is the correct answer, but A is definitely not right","poster":"mimzzz","comment_id":"860864"},{"timestamp":"1680584160.0","poster":"sdas1","upvote_count":"1","content":"option C","comment_id":"860647"},{"upvote_count":"2","comment_id":"860483","poster":"knivesz","content":"Selected Answer: C\nRespuesta C, por descarte, A) No es necesario B) No se coloca FILES D) PARQUET si es soportado E) No es necesario refrescar la vista, ya que se esta copiando un archivo","timestamp":"1680566580.0"}],"answers_community":["C (79%)","E (18%)","4%"],"url":"https://www.examtopics.com/discussions/databricks/view/105038-exam-certified-data-engineer-associate-topic-1-question-19/","choices":{"D":"The PARQUET file format does not support COPY INTO.","A":"The format of the files to be copied were not included with the FORMAT_OPTIONS keyword.","B":"The names of the files to be copied were not included with the FILES keyword.","C":"The previous day’s file has already been copied into the table.","E":"The COPY INTO statement requires the table to be refreshed to view the copied rows."},"answer_images":[],"timestamp":"2023-04-04 02:03:00"},{"id":"Grxjpjl4QfSviyfjQukJ","question_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/104728-exam-certified-data-engineer-associate-topic-1-question-2/","timestamp":"2023-04-01 15:23:00","choices":{"B":"An automated report needs to be made reproducible.","C":"An automated report needs to be tested to identify errors.","E":"An automated report needs to be runnable by all stakeholders.","D":"An automated report needs to be version-controlled across multiple collaborators.","A":"An automated report needs to be refreshed as quickly as possible."},"question_text":"Which of the following describes a scenario in which a data team will want to utilize cluster pools?","discussion":[{"comment_id":"863832","content":"Selected Answer: A\nUsing cluster pools reduces the cluster startup time. So in this case, the reports can be refreshed quickly and not having to wait long for the cluster to start","timestamp":"1680869640.0","upvote_count":"19","poster":"Data_4ever"},{"upvote_count":"6","content":"Selected Answer: A\nCluster pools are used in Databricks to reduce the time needed to create and scale clusters by maintaining a set of pre-configured, ready-to-use instances. When an automated report needs to be refreshed quickly, cluster pools help by minimizing cluster startup time, allowing the report generation process to start almost immediately. This is especially beneficial in scenarios where low latency is required to ensure data is updated in near real-time.\n\nThe other options (B, C, D, and E) do not directly benefit from the use of cluster pools, as they involve aspects like reproducibility, testing, version control, and stakeholder access, which are not specifically addressed by the primary function of cluster pools.","comment_id":"1312055","poster":"806e7d2","timestamp":"1731594300.0"},{"comment_id":"1558893","upvote_count":"1","poster":"Manohar77","content":"Selected Answer: A\nanswer A","timestamp":"1744110000.0"},{"upvote_count":"1","comment_id":"1305485","timestamp":"1730385840.0","content":"Selected Answer: A\nThe correct answer is: A. An automated report needs to be refreshed as quickly as possible.\nYou can minimize instance acquisition time by creating a pool for each instance type and Databricks runtime your organization commonly uses. For example, if most data engineering clusters use instance type A, data science clusters use instance type B, and analytics clusters use instance type C, create a pool with each instance type.","poster":"Gusberg"},{"timestamp":"1727169960.0","content":"Selected Answer: A\nCluster pools in Databricks are used to ensure that a set of pre-warmed clusters is readily available to run workloads. This means that when a job is submitted, it can be executed more quickly because there is no need to wait for a cluster to spin up. Therefore, if a data team needs to refresh an automated report as quickly as possible, they will want to utilize cluster pools to ensure that the job can be executed as quickly as possible.","comment_id":"889028","poster":"Majjjj","upvote_count":"4"},{"poster":"vctrhugo","timestamp":"1727169960.0","comment_id":"997856","content":"Selected Answer: A\nA. An automated report needs to be refreshed as quickly as possible.\n\nCluster pools are typically used in distributed computing environments, such as cloud-based data platforms like Databricks. They allow you to pre-allocate a set of compute resources (a cluster) for specific tasks or workloads. In this case, if an automated report needs to be refreshed as quickly as possible, you can allocate a cluster pool with sufficient resources to ensure fast data processing and report generation. This helps ensure that the report is generated with minimal latency and can be delivered to stakeholders in a timely manner. Cluster pools allow you to optimize resource allocation for high-demand, time-sensitive tasks like real-time report generation.","upvote_count":"3"},{"timestamp":"1727169900.0","poster":"9d4d68a","upvote_count":"1","content":"In Databricks, cluster pools are used to manage and optimize the allocation of cluster resources. They help ensure that clusters are efficiently provisioned and reused, which can reduce startup times and improve cost management.\n\nGiven the options:\n\nA. An automated report needs to be refreshed as quickly as possible. B. An automated report needs to be made reproducible. C. An automated report needs to be tested to identify errors. D. An automated report needs to be version-controlled across multiple collaborators. E. An automated report needs to be runnable by all stakeholders.\n\nThe most appropriate answer is:\n\nA. An automated report needs to be refreshed as quickly as possible.\n\nCluster pools are designed to minimize the time it takes to start up clusters by keeping a pool of pre-warmed instances available. This is particularly useful for scenarios where quick access to computing resources is crucial, such as in the case of refreshing automated reports quickly.","comment_id":"1272771"},{"timestamp":"1723102560.0","content":"Selected Answer: A\nwe can reduce the start-up time of cluster using cluster pools.","upvote_count":"1","poster":"80370eb","comment_id":"1262383"},{"poster":"mascarenhaslucas","upvote_count":"1","content":"Selected Answer: A\nI believe it's A!","comment_id":"1227518","timestamp":"1717965480.0"},{"poster":"poo_san","upvote_count":"1","timestamp":"1716378000.0","comment_id":"1215699","content":"Selected Answer: A\nA is the correct answer as cluster pools are used to speed up the cluster startup time"},{"comment_id":"1199399","timestamp":"1713659280.0","poster":"M15","content":"Considering the recommendation to create pools based on workloads and to pre-populate pools to ensure instances are available when clusters need them, the most suitable option would be:\n\nE. An automated report needs to be runnable by all stakeholders.\n\nThis aligns with the concept of pre-populating pools to ensure that instances are readily available when needed, enabling the automated report to be executed promptly whenever stakeholders require it without waiting for instance acquisition.","upvote_count":"2"},{"comment_id":"1188031","content":"Selected Answer: A\nA : I think cluster pools are used mainly to accellerate cluster start up by using vms somehow.","upvote_count":"1","timestamp":"1712060880.0","poster":"benni_ale"},{"timestamp":"1710839340.0","upvote_count":"1","poster":"Itmma","content":"Selected Answer: A\nA is correct","comment_id":"1177151"},{"comment_id":"1168517","poster":"Huepig","timestamp":"1709866020.0","upvote_count":"1","content":"Selected Answer: A\nhttps://www.databricks.com/blog/2019/11/11/databricks-pools-speed-up-data-pipelines.html"},{"comment_id":"1137288","content":"E is correct for sure. For data team , their tasks is not just to refresh a report. They equally want to share the cluster for running their queries. Please read at below:\nhttps://docs.databricks.com/en/compute/pool-best-practices.html#create-pools-based-on-workloads","timestamp":"1706764380.0","poster":"agAshish","upvote_count":"1"},{"comment_id":"1104689","content":"Selected Answer: A\nA is correct","poster":"SerGrey","upvote_count":"1","timestamp":"1703431680.0"},{"comment_id":"1081283","content":"Selected Answer: A\nA is correct","upvote_count":"1","timestamp":"1701071040.0","poster":"Ajinkyavsawant7"},{"content":"Selected Answer: A\nA is correct","comment_id":"1043125","poster":"anandpsg101","upvote_count":"2","timestamp":"1697258400.0"},{"comment_id":"1017337","content":"Selected Answer: A\nCluster pools are allows us to reduce the start time Ans A","upvote_count":"1","timestamp":"1695697620.0","poster":"KalavathiP"},{"content":"Selected Answer: A\n.Cluster pools allow us to reserve VM's ahead of time, which means that its start-up time will be faster.","upvote_count":"1","timestamp":"1695623880.0","comment_id":"1016521","poster":"d_b47"},{"comment_id":"1015472","content":"Option: A is correct.","poster":"len","timestamp":"1695527340.0","upvote_count":"1"},{"comment_id":"1002195","upvote_count":"1","poster":"alexitogs","content":"Selected Answer: A\nCluster pools allow us to reserve VM's ahead of time, which means that its start-up time will be faster.","timestamp":"1694154420.0"},{"upvote_count":"1","poster":"Gajen100","timestamp":"1690700460.0","content":"Selected Answer: A\nAn automated report needs to be refreshed as quickly as possible.","comment_id":"966935"},{"comment_id":"945724","upvote_count":"1","poster":"mehroosali","content":"Selected Answer: A\nA is correct","timestamp":"1688736420.0"},{"upvote_count":"1","poster":"rafahb","content":"Selected Answer: A\nOption A","comment_id":"873287","timestamp":"1681794000.0"},{"poster":"SireeJ","timestamp":"1681225200.0","comment_id":"867428","upvote_count":"2","content":"Option: A"},{"comment_id":"860606","timestamp":"1680581880.0","poster":"sdas1","content":"option A","upvote_count":"2"},{"comments":[{"content":"You can attach a cluster to a pool of idle instances for the driver and worker nodes to speed up cluster startup time. Instances from the pools are used to form the cluster.","upvote_count":"2","poster":"sdas1","comment_id":"860609","timestamp":"1680582000.0"}],"poster":"surrabhi_4","upvote_count":"1","timestamp":"1680503820.0","comment_id":"859598","content":"Selected Answer: D\noption D"},{"timestamp":"1680355380.0","poster":"XiltroX","upvote_count":"1","content":"I believe 'D' should be the right answer. version control is one of the strong features of Delta Lake","comment_id":"857944","comments":[{"upvote_count":"1","comment_id":"857947","poster":"XiltroX","comments":[{"comment_id":"861972","poster":"Oleskie","timestamp":"1680690240.0","upvote_count":"1","content":"Even according to this article, the right option is 'A'. Quote: 'You can attach a cluster to a pool of idle instances for the driver and worker nodes to speed up cluster startup time'","comments":[{"timestamp":"1680720960.0","upvote_count":"1","comment_id":"862400","poster":"XiltroX","content":"Yes I admit I was mistaken. The right option is A. Thanks for pointing it out."}]}],"content":"Sorry forgot to add resource\nhttps://hevodata.com/learn/databricks-clusters/","timestamp":"1680355620.0"}]}],"answer":"A","exam_id":162,"question_id":82,"answer_description":"","unix_timestamp":1680355380,"isMC":true,"answer_images":[],"answers_community":["A (98%)","2%"],"answer_ET":"A","topic":"1"},{"id":"Lun3yC4gVvMguLqFpsCY","timestamp":"2023-04-02 15:12:00","answer_images":[],"answer_ET":"A","choices":{"D":"sqlite","B":"autoloader","E":"org.apache.spark.sql.sqlite","C":"DELTA","A":"org.apache.spark.sql.jdbc"},"answers_community":["A (100%)"],"isMC":true,"exam_id":162,"question_text":"A data engineer needs to create a table in Databricks using data from their organization’s existing SQLite database.\nThey run the following command:\n//IMG//\n\nWhich of the following lines of code fills in the above blank to successfully complete the task?","question_images":["https://img.examtopics.com/certified-data-engineer-associate/image8.png"],"question_id":83,"discussion":[{"timestamp":"1680453120.0","content":"A is correct","comment_id":"859065","upvote_count":"8","poster":"rafahb"},{"timestamp":"1724781720.0","upvote_count":"3","poster":"7082935","comment_id":"1273629","content":"Selected Answer: A\nNobody mentioned this, but the big hint in this question is the url, which has a \"jdbc:\" url prefix. Hence, a JDBC type driver is required here."},{"upvote_count":"1","poster":"ranjan24","timestamp":"1721192340.0","content":"A is correct","comment_id":"1249384"},{"content":"Selected Answer: A\nA is correct","poster":"benni_ale","upvote_count":"1","comment_id":"1203180","timestamp":"1714231320.0"},{"upvote_count":"1","poster":"SerGrey","timestamp":"1704323160.0","content":"Correct answer is A","comment_id":"1113202"},{"content":"I think the correct answer is A. All that is missing the the jdbc drive. org.apache.spark.sql.jdbc","upvote_count":"2","poster":"Huroye","comment_id":"1071095","timestamp":"1700024160.0"},{"poster":"chris_mach","timestamp":"1695967440.0","content":"Selected Answer: A\nA is correct","upvote_count":"1","comment_id":"1020515"},{"poster":"KalavathiP","upvote_count":"1","timestamp":"1695698460.0","comment_id":"1017359","content":"Selected Answer: A\nA is correct"},{"comments":[{"content":"I correct myself https://docs.yugabyte.com/preview/integrations/apache-spark/spark-sql/","timestamp":"1687258020.0","upvote_count":"5","poster":"juliom6","comment_id":"928360"}],"comment_id":"928343","timestamp":"1687256880.0","upvote_count":"1","poster":"juliom6","content":"Selected Answer: A\nmust be \"USING JDBC\", there is no such thing as \"USING org.apache.spark.sql.jdbc\". https://docs.databricks.com/external-data/jdbc.html#language-sql"},{"content":"Selected Answer: A\nTo specify the JDBC driver and other options, the using clause should be followed by the fully qualified name of the JDBC data source, which is org.apache.spark.sql.jdbc.","poster":"Majjjj","comment_id":"889091","timestamp":"1683164640.0","upvote_count":"2"},{"content":"Answer A -\nCREATE TABLE new_employees_table\n USING JDBC\nOPTIONS (\n url \"<jdbc_url>\",\n dbtable \"<table_name>\",\n user '<username>',\n password '<password>'\n) AS\nSELECT * FROM employees_table_vw","timestamp":"1682056500.0","comment_id":"876226","upvote_count":"1","poster":"Varma_Saraswathula"},{"content":"JDBC - Option A","comment_id":"875886","timestamp":"1682014740.0","upvote_count":"1","poster":"naxacod574"},{"upvote_count":"2","comment_id":"861299","timestamp":"1680626220.0","content":"Selected Answer: A\nOption A is correct answer","poster":"XiltroX"},{"poster":"sdas1","upvote_count":"2","content":"option A","timestamp":"1680584160.0","comment_id":"860648"},{"poster":"surrabhi_4","timestamp":"1680578880.0","upvote_count":"1","comment_id":"860583","content":"Selected Answer: A\noption A"},{"timestamp":"1680566880.0","content":"Selected Answer: A\nEs JDBC osea la A, pregunta con truco para confundir","poster":"knivesz","upvote_count":"1","comment_id":"860490"},{"poster":"knivesz","content":"es JDBC","upvote_count":"3","comment_id":"858860","timestamp":"1680441120.0"}],"topic":"1","url":"https://www.examtopics.com/discussions/databricks/view/104862-exam-certified-data-engineer-associate-topic-1-question-20/","answer_description":"","unix_timestamp":1680441120,"answer":"A"},{"id":"usQT2u1IBpTGLIzGOQdO","question_images":[],"exam_id":162,"answer_description":"","topic":"1","answer_images":[],"unix_timestamp":1680364440,"answer_ET":"B","url":"https://www.examtopics.com/discussions/databricks/view/104760-exam-certified-data-engineer-associate-topic-1-question-21/","isMC":true,"question_id":84,"answer":"B","question_text":"A data engineering team has two tables. The first table march_transactions is a collection of all retail transactions in the month of March. The second table april_transactions is a collection of all retail transactions in the month of April. There are no duplicate records between the tables.\nWhich of the following commands should be run to create a new table all_transactions that contains all records from march_transactions and april_transactions without duplicate records?","discussion":[{"content":"Selected Answer: B\nB. CREATE TABLE all_transactions AS\nSELECT * FROM march_transactions\nUNION SELECT * FROM april_transactions;","timestamp":"1723108020.0","poster":"80370eb","upvote_count":"1","comment_id":"1262414"},{"content":"Selected Answer: B\nB is correct","poster":"SerGrey","comment_id":"1113401","timestamp":"1704354780.0","upvote_count":"2"},{"timestamp":"1699363680.0","content":"Selected Answer: B\nCorrect: B","upvote_count":"1","poster":"awofalus","comment_id":"1064820"},{"poster":"ezeik","timestamp":"1695019740.0","content":"UNION [ALL | DISTINCT]\n\nReturns the result of subquery1 plus the rows of subquery2`.\n\nIf ALL is specified duplicate rows are preserved.\n\nIf DISTINCT is specified the result does not contain any duplicate rows. This is the default.\nhttps://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-qry-select-setops.html#examples","upvote_count":"4","comment_id":"1010334"},{"content":"Selected Answer: B\nB. CREATE TABLE all_transactions AS\nSELECT * FROM march_transactions\nUNION SELECT * FROM april_transactions;\n\nTo create a new table all_transactions that contains all records from march_transactions and april_transactions without duplicate records, you should use the UNION operator, as shown in option B. This operator combines the result sets of the two tables while automatically removing duplicate records.","poster":"vctrhugo","comment_id":"997955","timestamp":"1693775100.0","upvote_count":"1"},{"timestamp":"1688779980.0","comment_id":"946072","content":"B\nCREATE TABLE all_transactions AS\nSELECT * FROM march_transactions\nUNION\nSELECT * FROM april_transactions;","upvote_count":"1","poster":"Atnafu"},{"comment_id":"896715","upvote_count":"1","poster":"prasioso","timestamp":"1683985500.0","content":"Selected Answer: B\nAnswer is B."},{"poster":"surrabhi_4","timestamp":"1680579000.0","comment_id":"860584","upvote_count":"1","content":"Selected Answer: B\noption B"},{"content":"Selected Answer: B\nAnswer is correct","poster":"XiltroX","upvote_count":"2","comment_id":"858079","timestamp":"1680364440.0"}],"answers_community":["B (100%)"],"timestamp":"2023-04-01 17:54:00","choices":{"A":"CREATE TABLE all_transactions AS\nSELECT * FROM march_transactions\nINNER JOIN SELECT * FROM april_transactions;","B":"CREATE TABLE all_transactions AS\nSELECT * FROM march_transactions\nUNION SELECT * FROM april_transactions;","D":"CREATE TABLE all_transactions AS\nSELECT * FROM march_transactions\nINTERSECT SELECT * from april_transactions;","C":"CREATE TABLE all_transactions AS\nSELECT * FROM march_transactions\nOUTER JOIN SELECT * FROM april_transactions;","E":"CREATE TABLE all_transactions AS\nSELECT * FROM march_transactions\nMERGE SELECT * FROM april_transactions;"}},{"id":"6CN3dYdBb7shQXDE7g6N","timestamp":"2023-04-05 11:41:00","answer_images":[],"choices":{"C":"if day_of_week == 1 and review_period == \"True\":","B":"if day_of_week = 1 and review_period = \"True\":","D":"if day_of_week == 1 and review_period:","A":"if day_of_week = 1 and review_period:","E":"if day_of_week = 1 & review_period: = \"True\":"},"answer_ET":"D","answers_community":["D (95%)","5%"],"isMC":true,"exam_id":162,"question_text":"A data engineer only wants to execute the final block of a Python program if the Python variable day_of_week is equal to 1 and the Python variable review_period is True.\nWhich of the following control flow statements should the data engineer use to begin this conditionally executed code block?","question_images":[],"question_id":85,"discussion":[{"timestamp":"1680687660.0","content":"The correct control flow statement to begin the conditionally executed code block would be D. if day_of_week == 1 and review_period:.\n\nThis statement will check if the variable day_of_week is equal to 1 and if the variable review_period evaluates to a truthy value. The use of the double equal sign (==) in the comparison of day_of_week is important, as a single equal sign (=) would be used to assign a value to the variable instead of checking its value. The use of a single ampersand (&) instead of the keyword and is not valid syntax in Python. The use of quotes around True in options B and C will result in a string comparison, which will not evaluate to True even if the value of review_period is True.","poster":"4be8126","comment_id":"861940","upvote_count":"19"},{"comment_id":"1355752","poster":"avidlearner","upvote_count":"2","timestamp":"1739389440.0","content":"Selected Answer: D\nC and D look similar but if you look closer, C has 'True' as string which is not equals to Boolean True, Hence D"},{"upvote_count":"1","content":"Selected Answer: D\nIn Python, the control flow statement to check conditions involves the following syntax:\n\n== for equality comparison: To compare if day_of_week is equal to 1, we use ==.\nBoolean evaluation: The variable review_period is already a Boolean (True/False). There's no need to compare it to a string like \"True\"; instead, it can be directly evaluated in the condition.\nand for logical conjunction: The and operator ensures both conditions must be true for the block to execute.","comment_id":"1314131","timestamp":"1731954540.0","poster":"806e7d2"},{"upvote_count":"1","comment_id":"1244547","timestamp":"1720468680.0","content":"Selected Answer: D\nYou need the == to use the \"equals\" operation. A single \"=\" is an assignment operation.","poster":"3fbc31b"},{"comment_id":"1165423","content":"Selected Answer: D\nC fits if you're looking for a string == 'True', in this case you are using a boolean so D","poster":"Mircuz","upvote_count":"3","timestamp":"1709539980.0"},{"comment_id":"1113404","poster":"SerGrey","content":"Selected Answer: D\nD is correct","upvote_count":"1","timestamp":"1704355020.0"},{"content":"Selected Answer: D\nD. if day_of_week == 1 and review_period:\n\n- In Python, the equality comparison operator is ==, not =. == is used to check if two values are equal.\n- The logical operator \"and\" is used to combine two conditions, ensuring that both conditions (day_of_week == 1 and review_period) are true for the subsequent code block to execute.\n- day_of_week == 1 checks if the variable day_of_week is equal to the integer value 1.\n- review_period is already assumed to be a Boolean variable since it is stated to be True (without quotes) in the question. Therefore, it should not be compared to a string \"True\".\n\nTherefore, option D correctly represents the condition for executing the final block of the Python program based on the given conditions.","upvote_count":"1","comment_id":"1109253","timestamp":"1703892540.0","poster":"Garyn"},{"comment_id":"1064823","poster":"awofalus","content":"Selected Answer: D\nD is correct","upvote_count":"1","timestamp":"1699364040.0"},{"timestamp":"1697441820.0","comment_id":"1044742","content":"Selected Answer: D\nreview_period == \"true\" is different from review_period == true","poster":"VijayKula","upvote_count":"1"},{"comment_id":"997956","upvote_count":"1","content":"Selected Answer: D\nD. if day_of_week == 1 and review_period:\n\nThe correct control flow statement to begin the conditionally executed code block is option D. In Python, the == operator is used for equality comparison, and and is used for logical \"and\" operations. So, this statement checks if day_of_week is equal to 1 and review_period is True (a boolean value), which is the correct way to express the conditions you mentioned.","timestamp":"1693775160.0","poster":"vctrhugo"},{"upvote_count":"1","comment_id":"993221","timestamp":"1693323540.0","poster":"[Removed]","content":"Selected Answer: D\nAnswer is D"},{"timestamp":"1688780100.0","poster":"Atnafu","comment_id":"946073","upvote_count":"1","content":"D\nif day_of_week == 1 and review_period:"},{"comment_id":"896622","timestamp":"1683976800.0","poster":"prasioso","upvote_count":"2","content":"Selected Answer: D\nin python value comparison is done by double equal signs (==). in case of boolean values that are TRUE these may be omitted. Quotes around True would result in string comparison and here we are comparing to a bool value."},{"timestamp":"1683737220.0","upvote_count":"1","comment_id":"894188","poster":"Bob123456","content":"Answer is 'D'\n\nday_of_week=1\nreview_period = True\n\n1)\nif day_of_week == 1 and review_period:\n print(\"yes\")\n\noutput:\nAbove code block's output is yes\n \n2)\nif day_of_week == 1 and review_period == \"True\":\n print(\"yes\")\n \noutput:\nThere is no output for above code block"},{"content":"Selected Answer: D\nThe data engineer should use option D: if day_of_week == 1 and review_period:. This statement checks if the variable day_of_week is equal to 1 and if the variable review_period is True. It uses the double equal sign (==) to compare the values of the variables, and does not use quotes around the keyword True, which is a boolean value.","comment_id":"889119","upvote_count":"1","poster":"Majjjj","timestamp":"1683170040.0"},{"comment_id":"866526","poster":"surrabhi_4","timestamp":"1681150740.0","content":"Selected Answer: D\noption D","upvote_count":"2"},{"poster":"XiltroX","comment_id":"862191","timestamp":"1680704940.0","content":"Selected Answer: C\nI believe the right answer is C","comments":[{"comment_id":"928372","content":"It's not C. Conditional check of \"True\" is treated as a string and not Boolean. Hence D is the right answer","upvote_count":"4","timestamp":"1687258740.0","poster":"lgkofficialwork"}],"upvote_count":"1"}],"topic":"1","answer_description":"","url":"https://www.examtopics.com/discussions/databricks/view/105262-exam-certified-data-engineer-associate-topic-1-question-22/","unix_timestamp":1680687660,"answer":"D"}],"exam":{"isMCOnly":true,"provider":"Databricks","name":"Certified Data Engineer Associate","id":162,"lastUpdated":"12 Apr 2025","numberOfQuestions":169,"isBeta":false,"isImplemented":true},"currentPage":17},"__N_SSP":true}