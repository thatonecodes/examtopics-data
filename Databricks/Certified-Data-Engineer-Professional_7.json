{"pageProps":{"questions":[{"id":"juMfFfxwOBnubvdwstdC","answer_ET":"D","exam_id":163,"discussion":[{"poster":"_lene_","content":"Selected Answer: D\nD is correct","upvote_count":"1","comment_id":"1339901","timestamp":"1736774340.0"},{"poster":"nedlo","upvote_count":"1","timestamp":"1730028720.0","content":"i wonder if not B? I guess name of view can be the same as name of Python variable?","comment_id":"1303561"},{"comment_id":"1255633","comments":[{"timestamp":"1735817040.0","content":"Still D, but collect() actually returns a DataFrame. It is however used in a comprehension that extracts first column of each row and gets the string.","upvote_count":"1","comment_id":"1335524","poster":"arekm"}],"poster":"Hadiler","upvote_count":"4","timestamp":"1721988240.0","content":"Selected Answer: D\nD is correct. It will not be DataFrame since collect() will change it to list of strings"},{"upvote_count":"1","content":"Selected Answer: D\nD is correct","poster":"vexor3","comment_id":"1251593","timestamp":"1721460240.0"},{"upvote_count":"1","timestamp":"1718150760.0","content":"Selected Answer: D\nD is correct","comment_id":"1228728","poster":"hpkr"}],"isMC":true,"answer":"D","answer_images":[],"question_images":["https://img.examtopics.com/certified-data-engineer-professional/image55.png"],"question_id":31,"timestamp":"2024-06-12 02:06:00","question_text":"A junior member of the data engineering team is exploring the language interoperability of Databricks notebooks. The intended outcome of the below code is to register a view of all sales that occurred in countries on the continent of Africa that appear in the geo_lookup table.\n\nBefore executing the code, running SHOW TABLES on the current database indicates the database contains only two tables: geo_lookup and sales.\n\n//IMG//\n\n\nWhat will be the outcome of executing these command cells m order m an interactive notebook?","choices":{"A":"Both commands will succeed. Executing SHOW TABLES will show that countries_af and sales_af have been registered as views.","C":"Cmd 1 will succeed and Cmd 2 will fail. countries_af will be a Python variable representing a PySpark DataFrame.","B":"Cmd 1 will succeed. Cmd 2 will search all accessible databases for a table or view named countries_af: if this entity exists, Cmd 2 will succeed.","D":"Cmd 1 will succeed and Cmd 2 will fail. countries_af will be a Python variable containing a list of strings."},"answers_community":["D (100%)"],"unix_timestamp":1718150760,"answer_description":"","topic":"1","url":"https://www.examtopics.com/discussions/databricks/view/142366-exam-certified-data-engineer-professional-topic-1-question/"},{"id":"PNZRk9m5O3zXLJTKH9qr","discussion":[{"upvote_count":"1","content":"Selected Answer: A\nDelta Lake optimizations are not well suited for long TIMESTAMP or STRING fields and can not provide good indexing, data skipping or statistics logging for them.","poster":"m79590530","comment_id":"1300340","timestamp":"1729411740.0"}],"timestamp":"2024-10-20 10:09:00","answer_description":"","question_id":32,"answers_community":["A (100%)"],"answer_images":[],"isMC":true,"answer_ET":"A","question_text":"The data science team has requested assistance in accelerating queries on free-form text from user reviews. The data is currently stored in Parquet with the below schema:\n\nitem_id INT, user_id INT, review_id INT, rating FLOAT, review STRING\n\nThe review column contains the full text of the review left by the user. Specifically, the data science team is looking to identify if any of 30 key words exist in this field.\n\nA junior data engineer suggests converting this data to Delta Lake will improve query performance.\n\nWhich response to the junior data engineerâ€™s suggestion is correct?","topic":"1","choices":{"D":"The Delta log creates a term matrix for free text fields to support selective filtering.","B":"Delta Lake statistics are only collected on the first 4 columns in a table.","C":"ZORDER ON review will need to be run to see performance gains.","A":"Delta Lake statistics are not optimized for free text fields with high cardinality."},"answer":"A","exam_id":163,"question_images":[],"unix_timestamp":1729411740,"url":"https://www.examtopics.com/discussions/databricks/view/149833-exam-certified-data-engineer-professional-topic-1-question/"},{"id":"LNwuzZxpzCf9WMaRHmww","url":"https://www.examtopics.com/discussions/databricks/view/142967-exam-certified-data-engineer-professional-topic-1-question/","timestamp":"2024-06-27 07:23:00","topic":"1","choices":{"C":"Because the default data retention threshold is 7 days, data files containing deleted records will be retained until the VACUUM job is run 8 days later.","A":"Because the VACUUM command permanently deletes all files containing deleted records, deleted records may be accessible with time travel for around 24 hours.","B":"Because the default data retention threshold is 24 hours, data files containing deleted records will be retained until the VACUUM job is run the following day.","D":"Because Delta Lake's delete statements have ACID guarantees, deleted records will be permanently purged from all storage systems as soon as a delete job completes."},"exam_id":163,"isMC":true,"unix_timestamp":1719465780,"discussion":[{"content":"Selected Answer: C\nIs C since by default Vacuum retains files no more referenced in the current table version for 7 days. https://docs.databricks.com/en/delta/history.html#configure-data-retention-for-time-travel-queries","upvote_count":"1","poster":"cales","timestamp":"1728830760.0","comment_id":"1296960"},{"upvote_count":"2","poster":"Hadiler","comment_id":"1255636","timestamp":"1721988360.0","content":"Selected Answer: C\nC is the correct answer"},{"content":"Selected Answer: C\nC is correct","upvote_count":"3","comment_id":"1252396","timestamp":"1721560320.0","poster":"vexor3"},{"timestamp":"1719465780.0","content":"Selected Answer: A\nSince the team is expecting last week's data to be deleted on Sunday at 1am to 2am. The data will be available for approx 24hrs until the vacuum command is run on Monday at 3am.","poster":"03355a2","upvote_count":"1","comments":[{"poster":"cales","comment_id":"1296959","upvote_count":"1","content":"No! By default Vacuum does not remove rows deleted whithin the last 7 days. To do it you should modify the property delta.deletedFileRetentionDuration\nhttps://docs.databricks.com/en/delta/history.html#configure-data-retention-for-time-travel-queries","timestamp":"1728830640.0"}],"comment_id":"1237887"}],"answer_images":[],"question_text":"The data engineering team has configured a job to process customer requests to be forgotten (have their data deleted). All user data that needs to be deleted is stored in Delta Lake tables using default table settings.\n\nThe team has decided to process all deletions from the previous week as a batch job at 1am each Sunday. The total duration of this job is less than one hour. Every Monday at 3am, a batch job executes a series of VACUUM commands on all Delta Lake tables throughout the organization.\n\nThe compliance officer has recently learned about Delta Lake's time travel functionality. They are concerned that this might allow continued access to deleted data.\n\nAssuming all delete logic is correctly implemented, which statement correctly addresses this concern?","answer_ET":"C","question_id":33,"answer":"C","question_images":[],"answer_description":"","answers_community":["C (86%)","14%"]},{"id":"0bVHT1yruwiMyPgpZaSP","topic":"1","answer_images":[],"discussion":[{"poster":"vexor3","content":"Selected Answer: B\nB is correct","timestamp":"1721460660.0","comment_id":"1251596","upvote_count":"1"}],"unix_timestamp":1721460660,"isMC":true,"choices":{"C":"workspace","B":"fs","A":"configure","D":"libraries"},"question_id":34,"url":"https://www.examtopics.com/discussions/databricks/view/144256-exam-certified-data-engineer-professional-topic-1-question/","exam_id":163,"answer":"B","question_images":[],"timestamp":"2024-07-20 09:31:00","question_text":"Assuming that the Databricks CLI has been installed and configured correctly, which Databricks CLI command can be used to upload a custom Python Wheel to object storage mounted with the DBFS for use with a production job?","answers_community":["B (100%)"],"answer_description":"","answer_ET":"B"},{"id":"QE6kFMBDuqVYIfky05Zb","discussion":[{"content":"The answer given is correct","upvote_count":"10","poster":"Eertyy","timestamp":"1693152360.0","comment_id":"991557","comments":[{"poster":"fabiospont","comment_id":"1350619","timestamp":"1738522740.0","content":"Option D","upvote_count":"1"},{"content":"I want to correct my response.It seems the right answer Option D, it leverages Delta Lake's built-in capabilities for handling CDC data. It is designed to efficiently capture, process, and propagate changes, making it a more robust and scalable solution, particularly for large-scale data scenarios with frequent updates and auditing requirements.","timestamp":"1695310680.0","comment_id":"1013230","comments":[{"content":"The question names 2 requirements to keep the data\n- archival with all records\n- querying with only the currently valid values\nCDF is not designed as a permanent storage for archival purposes. It keeps the data to propagate it to downstream applications / workloads. CDF is also purged with the vacuum command, so this would make a very unreliable archival.\n\nMedallion architecture that Databricks promotes seems to be a clear winner.","upvote_count":"1","poster":"arekm","comment_id":"1334748","timestamp":"1735643580.0"},{"timestamp":"1735660020.0","poster":"Sriramiyer92","content":"Actually you were correct in the first go. If you are dealing with small amount of data to changes, CDC is it way to go. But not in this case. (Keywords: For auditing purposes, the data governance team wishes to maintain a full record of all values that have ever been valid in the source system.) Answer is E in that case.","comment_id":"1334869","upvote_count":"1"},{"comment_id":"1038962","poster":"Starvosxant","upvote_count":"5","content":"Databricks is NOT able to process CDC alone. It needs a intermediare Tool to make it on an object storage and then ingest it.\nSo how can be D?","timestamp":"1696880220.0"},{"content":"the D states: process CDC data from an external system. so this delta CDF.","poster":"sturcu","comment_id":"1040291","timestamp":"1697008500.0","upvote_count":"1"}],"poster":"Eertyy","upvote_count":"3"}]},{"content":"E.\nThis is the correct answer because it meets the requirements of maintaining a full record of all values that have ever been valid in the source system and recreating the current table state with only the most recent value for each record. The code ingests all log information into a bronze table, which preserves the raw CDC data as it is. Then, it uses merge into to perform an upsert operation on a silver table, which means it will insert new records or update or delete existing records based on the change type and the pk_id columns. This way, the silver table will always reflect the current state of the source table, while the bronze table will keep the history of all changes.","comment_id":"1222457","upvote_count":"5","timestamp":"1717210680.0","poster":"imatheushenrique"},{"poster":"Tedet","content":"Selected Answer: E\nBronze Table (Raw Ingest): You start by ingesting all the change data capture (CDC) records into a bronze table. \nSilver Table (Processed State): The silver table represents the most recent state of the data. You would use the MERGE INTO command to process the changes from the bronze table and update the silver table accordingly. \nAudit Trail: Since you're ingesting all the data into the bronze table, you maintain a full history of changes that have occurred over time, which satisfies the auditing requirement.","upvote_count":"2","timestamp":"1740528360.0","comment_id":"1361624"},{"comment_id":"1337965","timestamp":"1736345100.0","upvote_count":"1","content":"Selected Answer: E\nIf getting External CDC Data (Kafka, etc) no need for CDF! Just ingest to Bronze with (pipelines.reset.allowed = false)","poster":"mwynn"},{"comment_id":"1334751","timestamp":"1735643700.0","upvote_count":"1","poster":"arekm","content":"On top of that - CDC with CDF is not automatic. You still need SQL or Python to read the changes and put them somewhere."},{"upvote_count":"1","poster":"benni_ale","timestamp":"1731579300.0","comments":[{"timestamp":"1731579780.0","content":"i meant can't be D","poster":"benni_ale","comment_id":"1311832","upvote_count":"1"}],"content":"Selected Answer: E\nYou can only read the change data feed for enabled tables. You must explicitly enable the change data feed option using one of the following methods: TBLPROPERTIES (delta.enableChangeDataFeed = true) . this means it is a delta feature or in other words it is a feature supported by delta tables. the data to process in the question is external so it is not a delta table => can't be B... Hopefully I am correct.","comment_id":"1311825"},{"timestamp":"1728901740.0","upvote_count":"1","content":"Selected Answer: E\nE . databricks cdc is not set to process external cdc. if u have external cdc u could send to bronze for auditing purposes and use bronze to get silver where u have only valid records","comment_id":"1297457","poster":"benni_ale"},{"comment_id":"1285468","poster":"databrick_work","upvote_count":"1","timestamp":"1726609920.0","content":"E is correct"},{"comment_id":"1130898","upvote_count":"2","timestamp":"1706116080.0","poster":"spaceexplorer","content":"Selected Answer: E\nThe answer is E"},{"content":"Selected Answer: E\nComplimenting kz_data's response, be aware that the data that is being consumed is not a Databrick's CDC data feed object, but rather, CDC coming from somewhere else, that is, just regular data. So, indeed, it can't be processed without another tool.","timestamp":"1704959700.0","poster":"RafaelCFC","comment_id":"1119489","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: E\nAnswer E is correct, as the CDC captured from the external database may contain duplicates for the same pk_id (key) due to multiple updates within the processed hour, we need to take the most recent update for the pk_id, and then MERGE into a silver table.","poster":"kz_data","timestamp":"1704892860.0","comment_id":"1118573"},{"poster":"a560fe1","comment_id":"1117617","upvote_count":"2","timestamp":"1704815460.0","content":"CDF captures changes only from a Delta table and is only forward-looking once enabled. The CDC logs are writing to object storage. So you would need to ingestion those and merge into downstream tables, hence the answer is E"},{"upvote_count":"3","comments":[{"timestamp":"1706116260.0","content":"This article shows exactly why D is not right. Since \"CDF captures changes only from a Delta table and is only forward-looking once enabled.\"","poster":"spaceexplorer","upvote_count":"4","comment_id":"1130902"}],"comment_id":"1086072","timestamp":"1701514140.0","content":"Selected Answer: D\nFor me the answer is D, the question states that CDC logs are emitted on an external storage meaning it can be ingested into the bronze layer on a table with CDF enabled. In this case we let databricks handle the complexity of following changes and only worry about data quality. meaning with CDF enabled databricks will already work the audit data for us with the table_changes of the pre-image and post-image and also give us the last updated value for our use case.\nhere is a similar example: https://www.databricks.com/blog/2021/06/09/how-to-simplify-cdc-with-delta-lakes-change-data-feed.html","poster":"hamzaKhribi"}],"timestamp":"2023-08-27 18:06:00","answer_description":"","question_id":35,"answers_community":["E (82%)","D (18%)"],"isMC":true,"answer_images":[],"answer_ET":"E","question_text":"An upstream system is emitting change data capture (CDC) logs that are being written to a cloud object storage directory. Each record in the log indicates the change type (insert, update, or delete) and the values for each field after the change. The source table has a primary key identified by the field pk_id.\nFor auditing purposes, the data governance team wishes to maintain a full record of all values that have ever been valid in the source system. For analytical purposes, only the most recent value for each record needs to be recorded. The Databricks job to ingest these records occurs once per hour, but each individual record may have changed multiple times over the course of an hour.\nWhich solution meets these requirements?","topic":"1","choices":{"C":"Iterate through an ordered set of changes to the table, applying each in turn; rely on Delta Lake's versioning ability to create an audit log.","D":"Use Delta Lake's change data feed to automatically process CDC data from an external system, propagating all changes to all dependent tables in the Lakehouse.","E":"Ingest all log information into a bronze table; use MERGE INTO to insert, update, or delete the most recent entry for each pk_id into a silver table to recreate the current table state.","B":"Use MERGE INTO to insert, update, or delete the most recent entry for each pk_id into a bronze table, then propagate all changes throughout the system.","A":"Create a separate history table for each pk_id resolve the current state of the table by running a union all filtering the history tables for the most recent state."},"answer":"E","exam_id":163,"question_images":[],"unix_timestamp":1693152360,"url":"https://www.examtopics.com/discussions/databricks/view/119187-exam-certified-data-engineer-professional-topic-1-question/"}],"exam":{"lastUpdated":"12 Apr 2025","numberOfQuestions":200,"isMCOnly":true,"name":"Certified Data Engineer Professional","isImplemented":true,"isBeta":false,"id":163,"provider":"Databricks"},"currentPage":7},"__N_SSP":true}