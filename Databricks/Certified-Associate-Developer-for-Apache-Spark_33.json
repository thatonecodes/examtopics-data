{"pageProps":{"questions":[{"id":"SSLmZ6UXlrYHBCzXRM20","topic":"1","unix_timestamp":1707486360,"answer":"E","isMC":true,"choices":{"D":"storesDF.groupBy().count(\"division\")","B":"storesDF.agg(groupBy(\"division\").count())","A":"storesDF.groupBy(\"division\").agg(count())","C":"storesDF.groupby.count(\"division\")","E":"storesDF.groupBy(\"division\").count()"},"question_id":161,"answer_images":[],"timestamp":"2024-02-09 14:46:00","question_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/133450-exam-certified-associate-developer-for-apache-spark-topic-1/","answer_ET":"E","discussion":[{"comment_id":"1145514","content":"Selected Answer: E\nE is the right answer here","poster":"azure_bimonster","upvote_count":"2","timestamp":"1723203960.0"}],"question_text":"Which of the following code blocks returns the number of rows in DataFrame storesDF for each unique value in column division?","exam_id":161,"answer_description":"","answers_community":["E (100%)"]},{"id":"0655ZDfnsQqNtElqYaRr","topic":"1","unix_timestamp":1690741200,"answer":"A","isMC":true,"choices":{"E":"storesDF.sort(desc(\"division\"))","B":"storesDF.orderBy(desc(\"division\"))","D":"storesDF.orderBy(\"division\", ascending - true)","A":"storesDF.sort(\"division\")","C":"storesDF.orderBy(col(\"division\").desc())"},"question_id":162,"answer_images":[],"timestamp":"2023-07-30 20:20:00","question_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/116808-exam-certified-associate-developer-for-apache-spark-topic-1/","answer_ET":"A","discussion":[{"timestamp":"1731691800.0","upvote_count":"1","comment_id":"1312737","poster":"58470e1","content":"Selected Answer: A\nCannot be D...ascending - true syntax is incorrect. if the optional ascending parameter is supplied, it should be ascending = True...False for descending. \n\nBy Default, sort method is ascending...so it's A"},{"comment_id":"1170913","content":"Option D works as well","timestamp":"1710148860.0","upvote_count":"2","comments":[{"timestamp":"1724426340.0","content":"no. Only A. D would work if it was \"ascending = True\". \"ascending = true\" is not a valid statement.","upvote_count":"2","comment_id":"1271338","poster":"gaco"},{"upvote_count":"1","comment_id":"1235439","timestamp":"1719066840.0","content":"i think so, both works :s","poster":"carlosmps"}],"poster":"arturffsi"},{"poster":"azure_bimonster","timestamp":"1707486480.0","comment_id":"1145517","upvote_count":"1","content":"Selected Answer: A\nA is good one here"},{"timestamp":"1693978380.0","poster":"thanab","comment_id":"1000192","upvote_count":"2","content":"A\n\nhttps://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.orderBy.html#pyspark.sql.DataFrame.orderBy"},{"comment_id":"969259","content":"Selected Answer: A\nit's A, checked in databricks","upvote_count":"1","poster":"cookiemonster42","timestamp":"1690911720.0"},{"poster":"zozoshanky","content":"C is correct","comments":[{"content":"we need ascending, not descending, mate","comment_id":"969260","upvote_count":"3","timestamp":"1690911780.0","poster":"cookiemonster42"}],"upvote_count":"1","comment_id":"967392","timestamp":"1690741200.0"}],"question_text":"Which of the following code blocks returns a DataFrame sorted alphabetically based on column division?","exam_id":161,"answer_description":"","answers_community":["A (100%)"]},{"id":"zN8RawuywAULAgAuB9l3","answer":"B","topic":"1","answer_images":[],"question_text":"Which of the following code blocks returns a 10 percent sample of rows from DataFrame storesDF with replacement?","answer_ET":"B","url":"https://www.examtopics.com/discussions/databricks/view/117001-exam-certified-associate-developer-for-apache-spark-topic-1/","isMC":true,"exam_id":161,"timestamp":"2023-08-01 19:53:00","answers_community":[],"discussion":[{"timestamp":"1722534780.0","comment_id":"969266","poster":"cookiemonster42","content":"correct, an easy one","upvote_count":"1"}],"question_id":163,"answer_description":"","question_images":[],"choices":{"D":"storesDF.sampleBy(fraction = 0.1)","C":"storesDF.sample(true, fraction = 0.15)","B":"storesDF.sample(true, fraction = 0.1)","E":"storesDF.sample(false, fraction = 0.1)","A":"storesDF.sample(true)"},"unix_timestamp":1690912380},{"id":"Wysto8Ywlnpt2u8Sxvo2","answer_description":"","isMC":true,"question_id":164,"answer_images":[],"answer":"B","url":"https://www.examtopics.com/discussions/databricks/view/107438-exam-certified-associate-developer-for-apache-spark-topic-1/","discussion":[{"timestamp":"1718635920.0","upvote_count":"4","content":"Selected Answer: B\nThe most complete description of lazy evaluation is:\n\nB. A process is lazily evaluated if its execution does not start until it is put into action by some type of trigger.\n\nExplanation: Lazy evaluation is a programming language feature that delays the evaluation of expressions or computations until the result is actually needed or requested. In a lazily evaluated system, expressions are not immediately executed or evaluated when they are defined or assigned. Instead, the evaluation is deferred until the value is needed by another part of the program or when an action is triggered.","comment_id":"926053","poster":"TmData"},{"content":"B. A process is lazily evaluated if its execution does not start until it is put into action by some type of trigger.\n\nLazy evaluation is a programming paradigm that defers the evaluation of an expression until its value is needed. In other words, lazy evaluation delays the computation of a value until the value is actually required by the program. This is in contrast to eager evaluation, where expressions are evaluated as soon as they are bound to a variable.\n\nIn Spark, lazy evaluation is used extensively to optimize the execution of complex data processing pipelines. When a user creates a series of operations to transform a dataset, Spark does not immediately execute those operations. Instead, it builds a logical plan that represents the operations as a set of transformations on the original dataset. The actual execution of the plan is deferred until the user requests the result by triggering an action, such as writing the result to disk or displaying it on the screen. By using lazy evaluation, Spark can optimize the execution plan and avoid unnecessary computations, resulting in faster processing times and more efficient use of cluster resources.","poster":"4be8126","comment_id":"880375","upvote_count":"2","timestamp":"1714048620.0"}],"answers_community":["B (100%)"],"topic":"1","question_images":[],"question_text":"Which of the following is the most complete description of lazy evaluation?","answer_ET":"B","choices":{"D":"A process is lazily evaluated if its execution does not start until it reaches a specified date and time","C":"A process is lazily evaluated if its execution does not start until it is forced to display a result to the user","A":"None of these options describe lazy evaluation","E":"A process is lazily evaluated if its execution does not start until it is finished compiling","B":"A process is lazily evaluated if its execution does not start until it is put into action by some type of trigger"},"timestamp":"2023-04-25 14:37:00","exam_id":161,"unix_timestamp":1682426220},{"id":"JbEtO43LGah8a92kRVxO","question_id":165,"answer":"C","question_text":"Which of the following code blocks returns the first 3 rows of DataFrame storesDF?","topic":"1","url":"https://www.examtopics.com/discussions/databricks/view/113029-exam-certified-associate-developer-for-apache-spark-topic-1/","isMC":true,"unix_timestamp":1687472640,"answer_description":"","timestamp":"2023-06-23 00:24:00","choices":{"E":"storesDF.collect(3)","B":"storesDF.n(3)","D":"storesDF.head(3)","C":"storesDF.take(3)","A":"storesDF.top_n(3)"},"discussion":[{"timestamp":"1723204560.0","content":"odd question indeed. \nhead() and take() methods perform a similar operation of retrieving rows from the beginning of the DataFrame. However, head() may be slightly slower than take() due to additional checks for boundary conditions. I can't see the difference in terms of retrieving 1st three rows.","poster":"azure_bimonster","comment_id":"1145523","upvote_count":"2"},{"content":"weird question, taking into account that head essentially calls take on the DataFrame.","upvote_count":"1","comment_id":"1004207","poster":"newusername","timestamp":"1710099960.0"},{"poster":"cookiemonster42","content":"C, D and even E are correct. There's no requirement to return a dataframe in the question","timestamp":"1706819040.0","comment_id":"969295","upvote_count":"2"},{"upvote_count":"3","content":"Correct answer is D. head(n) returns the first n rows and take(n) returns first n rows as a list","comment_id":"931092","timestamp":"1703291040.0","poster":"ryanmu","comments":[{"timestamp":"1706646420.0","upvote_count":"2","content":"Both head and take gives list as an output , am confused need to do more investigation","comment_id":"967401","poster":"zozoshanky"}]}],"answer_images":[],"answer_ET":"C","answers_community":[],"question_images":[],"exam_id":161}],"exam":{"isImplemented":true,"name":"Certified Associate Developer for Apache Spark","isBeta":false,"lastUpdated":"12 Apr 2025","isMCOnly":true,"numberOfQuestions":185,"id":161,"provider":"Databricks"},"currentPage":33},"__N_SSP":true}