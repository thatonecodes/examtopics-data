{"pageProps":{"questions":[{"id":"tijwbBP9swcrqlEmf6jB","answer_ET":"D","timestamp":"2023-04-25 19:01:00","discussion":[{"upvote_count":"1","timestamp":"1739593440.0","comment_id":"1356713","poster":"te1","content":"Selected Answer: B\nDataset B is smaller Dataset. It will be brodcasted to all worker nodes. So No shuffle for Dataset B."},{"upvote_count":"2","poster":"monibun","comment_id":"1295332","content":"Should be B: During the join, the intention of the shuffle would be to bring the same keys from both dataframes in same partition. Now, this would ideally require both of them to be shuffled. however, if smaller one is broadcasted, that would mean we have sent the entire smaller dataframe in each partition whereas the bigger one would still undergo a shuffle to get its similar keys in each partition. hence, the re-shuffle of just the smaller one is avoided.","timestamp":"1728517680.0"},{"timestamp":"1723540020.0","comment_id":"1265066","poster":"65bd33e","content":"Selected Answer: D\nThe correct answer is:\n\nD. DataFrame B should be broadcasted because it is smaller and will eliminate the need for the shuffling of DataFrame A.\n\nExplanation:\n\nIn a broadcast join, the smaller DataFrame (in this case, DataFrame B, which is 1 GB) is broadcasted to all worker nodes. This allows the larger DataFrame (DataFrame A, which is 128 GB) to be joined without shuffling its data across the cluster, which would be computationally expensive.\nBroadcasting the smaller DataFrame reduces the amount of data that needs to be shuffled, improving the efficiency of the join operation.","upvote_count":"1"},{"poster":"atulrao","upvote_count":"3","timestamp":"1720672320.0","content":"The correct answer is:\nB. DataFrame B should be broadcasted because it is smaller and will eliminate the need for the shuffling of itself.\n\nExplanation:\n\nIn Spark, a broadcast join is a specific type of join where one DataFrame is sent to every node in the cluster to avoid the costly network shuffle that can occur with large datasets in regular joins.\nGenerally, the smaller DataFrame should be broadcasted to optimize performance. This is because broadcasting a smaller DataFrame requires less network bandwidth and memory usage across the cluster.\nBroadcasting DataFrame B (the smaller DataFrame at 1 GB) means that each node will have a local copy of DataFrame B, allowing them to perform the join operation locally with their respective partitions of DataFrame A without needing to shuffle DataFrame B across the network.\nThis approach significantly reduces the amount of data that needs to be shuffled (since only DataFrame A is partitioned across the nodes), thereby improving the performance of the join operation.","comment_id":"1245879"},{"timestamp":"1709756100.0","poster":"azurearch","comment_id":"1167469","upvote_count":"1","content":"Correct answer is B. D is wrong. Being the larger dataset Dataframe A (128 GB) will get shuffled being the larger dataset. Dataframe A (1 GB) (if hint is specified in join), will be broadcasted hence it would not get shuffled."},{"poster":"Ahlo","timestamp":"1708618620.0","comment_id":"1156520","content":"answer D - With broadcast join, Spark broadcast the smaller DataFrame to all executors and the executor keeps this DataFrame in memory and the larger DataFrame is split and distributed across all executors so that Spark can perform a join without shuffling any data from the larger DataFrame as the data required for join colocated on every executor.","upvote_count":"1"},{"content":"Selected Answer: B\nIt should really be B.","poster":"mehroosali","upvote_count":"3","timestamp":"1699372500.0","comment_id":"1064968"},{"timestamp":"1693956240.0","upvote_count":"1","comment_id":"999993","poster":"thanab","content":"The correct answer is B. DataFrame B should be broadcasted because it is smaller and will eliminate the need for the shuffling of itself. A broadcast join is an optimization technique in the Spark SQL engine that is used to join two DataFrames. This technique is ideal for joining a large DataFrame with a smaller one. With broadcast join, Spark broadcasts the smaller DataFrame to all executors and the executor keeps this DataFrame in memory. The larger DataFrame is split and distributed across all executors so that Spark can perform a join without shuffling any data from the larger DataFrame as the data required for join colocated on every executor."},{"timestamp":"1693956180.0","comment_id":"999992","upvote_count":"2","content":"Option D is incorrect because it states that DataFrame B should be broadcasted because it is smaller and will eliminate the need for the shuffling of DataFrame A. However, broadcasting DataFrame B will not eliminate the need for shuffling DataFrame A. Instead, broadcasting DataFrame B will eliminate the need for shuffling itself. In a broadcast join, the smaller DataFrame is broadcasted to all executors and kept in memory. The larger DataFrame is split and distributed across all executors so that Spark can perform a join without shuffling any data from the larger DataFrame as the data required for join colocated on every executor.","poster":"thanab"},{"comment_id":"978461","content":"Selected Answer: D\nhttps://sparkbyexamples.com/spark/broadcast-join-in-spark/\n\nSpark Broadcast Join is an important part of the Spark SQL execution engine, With broadcast join, Spark broadcast the smaller DataFrame to all executors and the executor keeps this DataFrame in memory and the larger DataFrame is split and distributed across all executors so that Spark can perform a join without shuffling any data from the larger DataFrame as the data required for join colocated on every executor.","upvote_count":"3","timestamp":"1691743080.0","poster":"eendee"},{"comment_id":"947354","content":"D should be correct. Broadcast join happens on smaller DataFrame to prevent the shuffling of larger DataFrame.","upvote_count":"2","timestamp":"1688914560.0","poster":"Diws"},{"comment_id":"926069","upvote_count":"3","content":"Selected Answer: D\nOption A is incorrect because not both DataFrames can be broadcasted. Only one of the DataFrames should be broadcasted to minimize shuffling.\n\nOption B is correct because DataFrame B is smaller and broadcasting it will eliminate the shuffling of DataFrame B, improving the join operation's efficiency.\n\nOption C is incorrect because DataFrame A is larger and shuffling DataFrame B is not a concern in this scenario.\n\nOption E is incorrect because DataFrame A is larger, and broadcasting it would not eliminate the shuffling of itself. The larger DataFrame typically undergoes shuffling in a broadcast join.\n\nTherefore, the correct option is D.","timestamp":"1687014120.0","poster":"TmData"},{"content":"Selected Answer: B\nB is correct.","comment_id":"917437","upvote_count":"2","timestamp":"1686158100.0","poster":"ZSun"},{"poster":"4be8126","content":"Selected Answer: D\nThe correct answer is D. DataFrame B should be broadcasted because it is smaller and will eliminate the need for the shuffling of DataFrame A.\n\nA broadcast join is a technique where the smaller DataFrame is broadcast to all the worker nodes in the cluster, so that it can be joined with the larger DataFrame without requiring any shuffling of the larger DataFrame. This is generally more efficient than a shuffle join, which requires data to be shuffled across the network.\n\nIn this scenario, DataFrame B is much smaller than DataFrame A, so it is more efficient to broadcast DataFrame B to all worker nodes in the cluster. This will eliminate the need for shuffling of DataFrame A, making the join more efficient.","timestamp":"1683112380.0","upvote_count":"1","comment_id":"888434"},{"timestamp":"1682442060.0","upvote_count":"2","poster":"Indiee","comment_id":"880664","content":"All the ANS are incorrect. The DAG will perform a sort merge join instead of BCJ. The size of a DF needed to be 10MB max for broadcast else it will cause a network overload."}],"answers_community":["D (57%)","B (43%)"],"question_id":56,"isMC":true,"url":"https://www.examtopics.com/discussions/databricks/view/107464-exam-certified-associate-developer-for-apache-spark-topic-1/","unix_timestamp":1682442060,"question_text":"A Spark application has a 128 GB DataFrame A and a 1 GB DataFrame B. If a broadcast join were to be performed on these two DataFrames, which of the following describes which DataFrame should be broadcasted and why?","exam_id":161,"question_images":[],"choices":{"E":"DataFrame A should be broadcasted because it is smaller and will eliminate the need for the shuffling of itself.","C":"DataFrame A should be broadcasted because it is larger and will eliminate the need for the shuffling of DataFrame B.","B":"DataFrame B should be broadcasted because it is smaller and will eliminate the need for the shuffling of itself.","A":"Either DataFrame can be broadcasted. Their results will be identical in result and efficiency.","D":"DataFrame B should be broadcasted because it is smaller and will eliminate the need for the shuffling of DataFrame A."},"answer_description":"","answer":"D","answer_images":[],"topic":"1"},{"id":"pCOhQnrV5G1yFZjmqhLT","answer_images":[],"answer_ET":"B","isMC":true,"topic":"1","discussion":[{"content":"Selected Answer: B\nfilter() and distinct()","poster":"Thameur01","timestamp":"1741172880.0","upvote_count":"2","comment_id":"1365365"},{"poster":"58470e1","upvote_count":"1","content":"Selected Answer: B\nB. filter() is a narrow transformation along with select, cast, union whereas distinct, groupBy, sort, join are wide and induces shuffles/exchanges","timestamp":"1731599220.0","comment_id":"1312129"},{"comment_id":"1192983","timestamp":"1712752800.0","upvote_count":"1","content":"B. DataFrame.fliter()","poster":"Sowwy1"}],"question_text":"Which of the following operations is least likely to result in a shuffle?","answer_description":"","timestamp":"2024-04-10 14:40:00","unix_timestamp":1712752800,"exam_id":161,"url":"https://www.examtopics.com/discussions/databricks/view/138341-exam-certified-associate-developer-for-apache-spark-topic-1/","question_images":[],"choices":{"B":"DataFrame.fliter()","D":"DataFrame.distinct()","A":"DataFrame.join()","C":"DataFrame.orderBy()","E":"DataFrame.intersect()"},"answers_community":["B (100%)"],"answer":"B","question_id":57},{"id":"mrjArDKNtslMZhNV9VO8","answer_images":[],"answer_ET":"D","isMC":true,"topic":"1","discussion":[{"upvote_count":"2","timestamp":"1725412200.0","poster":"EmmanuelRams","content":"Selected Answer: E\nLarge cluster will keep large DF objects live and GC will take more time to collect those","comment_id":"1277896"},{"content":"I think it's Scemario 6","poster":"Sowwy1","upvote_count":"3","comment_id":"1192985","timestamp":"1712752980.0"}],"answer_description":"","timestamp":"2024-04-10 14:43:00","question_text":"Which of the following cluster configurations is least likely to experience delays due to garbage collection of a large DataFrame?\n\n//IMG//\n\n\nNote: each configuration has roughly the same compute power using 100GB of RAM and 200 cores.","unix_timestamp":1712752980,"exam_id":161,"question_images":["https://img.examtopics.com/certified-associate-developer-for-apache-spark/image17.png"],"url":"https://www.examtopics.com/discussions/databricks/view/138342-exam-certified-associate-developer-for-apache-spark-topic-1/","choices":{"A":"Scenario #4","D":"More information is needed to determine an answer.","C":"Scenario #5","B":"Scenario #1","E":"Scenario #6"},"answers_community":["E (100%)"],"answer":"E","question_id":58},{"id":"28hjt6zBKbLJRVzEEJYU","choices":{"D":"1. newColumn\n2. \"productCategory\"\n3. explode\n4. col\n5. \"productCategories\"","B":"1. withColumn\n2. \"productCategory\"\n3. split\n4. col\n5. \"productCategories\"","E":"1. withColumn\n2. \"productCategories\"\n3. explode\n4. col\n5. \"productCategories\"","A":"1. newColumn\n2. \"productCategories\"\n3. col\n4. split\n5. \"productCategories\"","C":"1. withColumn\n2. \"productCategory\"\n3. explode\n4. col\n5. \"productCategories\""},"discussion":[{"poster":"oussa_ama","timestamp":"1724241780.0","comment_id":"1270077","content":"Selected Answer: E\nproductCategories","upvote_count":"1"},{"timestamp":"1720798740.0","content":"Selected Answer: E\nThe answer is E. If you feel like getting in an argument with the question on the proper use of plural field names, then pick C.","upvote_count":"1","poster":"5effea7","comment_id":"1246857"},{"upvote_count":"2","timestamp":"1719172380.0","content":"Selected Answer: C\nnew column should be \"productCategory\" singular","comments":[{"timestamp":"1719172500.0","comment_id":"1235974","poster":"deadbeef38","upvote_count":"1","content":"ok, that wasn't in the spec, but it should have been. I guess E is ok then."}],"comment_id":"1235973","poster":"deadbeef38"},{"timestamp":"1712753040.0","content":"E. 1. withColumn\n2. \"productCategories\"\n3. explode\n4. col\n5. \"productCategories\"","upvote_count":"1","comment_id":"1192989","poster":"Sowwy1"}],"answer":"C","answers_community":["C (50%)","E (50%)"],"isMC":true,"exam_id":161,"answer_description":"","answer_images":[],"question_text":"The code block shown below should return a new DataFrame where column productСategories only has one word per row, resulting in a DataFrame with many more rows than DataFrame storesDF. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.\n\nA sample of storesDF is displayed below:\n\n//IMG//\n\n\nCode block:\n\nstoresDF.__1__(__2__, __3__(__4__(__5__)))","question_id":59,"topic":"1","timestamp":"2024-04-10 14:44:00","url":"https://www.examtopics.com/discussions/databricks/view/138343-exam-certified-associate-developer-for-apache-spark-topic-1/","question_images":["https://img.examtopics.com/certified-associate-developer-for-apache-spark/image18.png"],"answer_ET":"E","unix_timestamp":1712753040},{"id":"e3IjTSzIWYtUH7oEIv9V","question_id":60,"answer_description":"","question_images":["https://img.examtopics.com/certified-associate-developer-for-apache-spark/image19.png"],"question_text":"Which of the following code blocks returns a new DataFrame with column storeReview where the pattern \"End\" has been removed from the end of column storeReview in DataFrame storesDF?\n\nA sample DataFrame storesDF is below:\n\n//IMG//","answers_community":["B (100%)"],"answer_images":[],"topic":"1","isMC":true,"discussion":[{"upvote_count":"1","content":"the official documentation says\npyspark.sql.functions.regexp_replace(str, pattern, replacement)\nReplace all substrings of the specified string value that match regexp with rep.\nso I think D is the only correct option","comment_id":"1297489","timestamp":"1728904740.0","poster":"max_manfred"},{"content":"Selected Answer: B\nB & D work","timestamp":"1716450480.0","comment_id":"1216361","poster":"jtu363","upvote_count":"2"},{"timestamp":"1712174400.0","content":"Selected Answer: B\nB is the right choice","comment_id":"1188880","upvote_count":"2","poster":"SaiPavan10"},{"poster":"azure_bimonster","comment_id":"1145829","timestamp":"1707517320.0","upvote_count":"1","content":"Selected Answer: B\nB is right one it seems, col should be used"},{"upvote_count":"3","poster":"saryu","content":"B also works right?","comment_id":"1138505","timestamp":"1706875740.0"}],"timestamp":"2024-02-02 13:09:00","unix_timestamp":1706875740,"exam_id":161,"answer_ET":"B","answer":"B","choices":{"B":"storesDF.withColumn(\"storeReview\", regexp_replace(col(\"storeReview\"), \" End$\", \"\"))","A":"storesDF.withColumn(\"storeReview\", col(\"storeReview\").regexp_replace(\" End$\", \"\"))","C":"storesDF.withColumn(\"storeReview”, regexp_replace(col(\"storeReview\"), \" End$\"))","D":"storesDF.withColumn(\"storeReview\", regexp_replace(\"storeReview\", \" End$\", \"\"))","E":"storesDF.withColumn(\"storeReview\", regexp_extract(col(\"storeReview\"), \" End$\", \"\"))"},"url":"https://www.examtopics.com/discussions/databricks/view/132690-exam-certified-associate-developer-for-apache-spark-topic-1/"}],"exam":{"name":"Certified Associate Developer for Apache Spark","lastUpdated":"12 Apr 2025","provider":"Databricks","isImplemented":true,"isMCOnly":true,"numberOfQuestions":185,"id":161,"isBeta":false},"currentPage":12},"__N_SSP":true}