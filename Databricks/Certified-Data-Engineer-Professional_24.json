{"pageProps":{"questions":[{"id":"DQGxvIYeVozLgJQu8WDe","discussion":[{"poster":"Thameur01","timestamp":"1733995500.0","upvote_count":"2","comment_id":"1325505","content":"Selected Answer: C\nhere is a correct implementation:\ndef create_table(t):\n @dlt.table(name=f\"{t}_dataset\")\n def table_definition():\n return spark.read.table(t)\n\ntables = [\"t1\", \"t2\", \"t3\"]\nfor t in tables:\n create_table(t)"},{"poster":"benni_ale","content":"Selected Answer: C\nProblem seems to be the fact that the new_table function has no parameter and the t variable won't be recognized as a variable.. However i have not tested it as DLT is not available in free membership. :(","upvote_count":"2","timestamp":"1733695320.0","comment_id":"1323778"}],"answer_ET":"C","question_text":"A data engineer wants to refactor the following DLT code, which includes multiple table definitions with very similar code.\n\n//IMG//\n\n\nIn an attempt to programmatically create these tables using a parameterized table definition, the data engineer writes the following code.\n\n//IMG//\n\n\nThe pipeline runs an update with this refactored code, but generates a different DAG showing incorrect configuration values for these tables.\n\nHow can the data engineer fix this?","topic":"1","answer_description":"","question_id":116,"answer_images":[],"exam_id":163,"timestamp":"2024-12-08 23:02:00","choices":{"C":"Move the table definition into a separate function, and make calls to this function using different input parameters inside the for loop.","A":"Wrap the for loop inside another table definition, using generalized names and properties to replace with those from the inner table definition.","D":"Load the configuration values for these tables from a separate file, located at a path provided by a pipeline parameter.","B":"Convert the list of configuration values to a dictionary of table settings, using table names as keys."},"url":"https://www.examtopics.com/discussions/databricks/view/152742-exam-certified-data-engineer-professional-topic-1-question/","unix_timestamp":1733695320,"isMC":true,"answer":"C","answers_community":["C (100%)"],"question_images":["https://img.examtopics.com/certified-data-engineer-professional/image95.png","https://img.examtopics.com/certified-data-engineer-professional/image96.png"]},{"id":"ncAplwMCDiQwucWNcTcd","answer_description":"","question_id":117,"answer_images":[],"exam_id":163,"answers_community":["D (100%)"],"isMC":true,"discussion":[{"comment_id":"1334796","timestamp":"1735648500.0","poster":"arekm","upvote_count":"1","content":"Selected Answer: D\nD - see explanation of 8605246"},{"timestamp":"1733030640.0","content":"D. Structured Streaming models new data arriving in a data stream as new rows appended to an unbounded table.","upvote_count":"2","poster":"imatheushenrique","comment_id":"1222475"},{"content":"Selected Answer: D\nYes. answer is D","timestamp":"1724807880.0","upvote_count":"2","comment_id":"1161219","poster":"mardigras"},{"timestamp":"1720888080.0","comment_id":"1121953","upvote_count":"1","poster":"Jay_98_11","content":"Selected Answer: D\nvote for D"},{"poster":"sturcu","comment_id":"1040345","timestamp":"1712822760.0","upvote_count":"2","content":"Selected Answer: D\nCorrect.\nStructured streaming needs to be considered as a table with append"},{"upvote_count":"4","timestamp":"1707216540.0","poster":"8605246","content":"correct; The key idea in Structured Streaming is to treat a live data stream as a table that is being continuously appended. This leads to a new stream processing model that is very similar to a batch processing model. You will express your streaming computation as standard batch-like query as on a static table, and Spark runs it as an incremental query on the unbounded input table. Let’s understand this model in more detail.\nhttps://spark.apache.org/docs/latest/structured-streaming-programming-guide.html","comment_id":"973661"}],"topic":"1","question_images":[],"url":"https://www.examtopics.com/discussions/databricks/view/117468-exam-certified-data-engineer-professional-topic-1-question/","choices":{"B":"Structured Streaming is implemented as a messaging bus and is derived from Apache Kafka.","E":"Structured Streaming relies on a distributed network of nodes that hold incremental state values for cached stages.","A":"Structured Streaming leverages the parallel processing of GPUs to achieve highly parallel data throughput.","D":"Structured Streaming models new data arriving in a data stream as new rows appended to an unbounded table.","C":"Structured Streaming uses specialized hardware and I/O streams to achieve sub-second latency for data transfer."},"answer":"D","question_text":"Which statement characterizes the general programming model used by Spark Structured Streaming?","answer_ET":"D","unix_timestamp":1691311740,"timestamp":"2023-08-06 10:49:00"},{"id":"aaeirAs5Cm4derdxGkLE","answer_description":"","question_id":118,"answer_images":[],"exam_id":163,"answers_community":["A (100%)"],"isMC":true,"discussion":[{"content":"correct; The maximum number of bytes to pack into a single partition when reading files. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.\nhttps://spark.apache.org/docs/latest/sql-performance-tuning.html","timestamp":"1707216600.0","comment_id":"973662","upvote_count":"5","poster":"8605246"},{"upvote_count":"3","comment_id":"1121954","timestamp":"1720888140.0","poster":"Jay_98_11","content":"Selected Answer: A\ncorrect"},{"upvote_count":"1","content":"Selected Answer: A\nfrom the provided list, this fits best.\nIn reality partition size/number can be influenced my many settings","comment_id":"1040372","poster":"sturcu","timestamp":"1712824200.0"}],"question_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/databricks/view/117469-exam-certified-data-engineer-professional-topic-1-question/","answer":"A","choices":{"E":"spark.sql.adaptive.advisoryPartitionSizeInBytes","A":"spark.sql.files.maxPartitionBytes","C":"spark.sql.files.openCostInBytes","B":"spark.sql.autoBroadcastJoinThreshold","D":"spark.sql.adaptive.coalescePartitions.minPartitionNum"},"question_text":"Which configuration parameter directly affects the size of a spark-partition upon ingestion of data into Spark?","answer_ET":"A","unix_timestamp":1691311800,"timestamp":"2023-08-06 10:50:00"},{"id":"fAKTHx7trU7vYFzbUQXB","url":"https://www.examtopics.com/discussions/databricks/view/121108-exam-certified-data-engineer-professional-topic-1-question/","question_id":119,"answer_images":[],"choices":{"C":"Network latency due to some cluster nodes being in different regions from the source data","E":"Credential validation errors while pulling data from an external system.","B":"Spill resulting from attached volume storage being too small.","A":"Task queueing resulting from improper thread pool assignment.","D":"Skew caused by more data being assigned to a subset of spark-partitions."},"unix_timestamp":1695323700,"timestamp":"2023-09-21 21:15:00","question_images":[],"topic":"1","answer_description":"","question_text":"A Spark job is taking longer than expected. Using the Spark UI, a data engineer notes that the Min, Median, and Max Durations for tasks in a particular stage show the minimum and median time to complete a task as roughly the same, but the max duration for a task to be roughly 100 times as long as the minimum.\nWhich situation is causing increased duration of the overall job?","answer_ET":"D","answers_community":["D (100%)"],"exam_id":163,"answer":"D","isMC":true,"discussion":[{"content":"Selected Answer: D\nD - other answers don't make sense. In particular C - all nodes of the cluster must be in the same region (at least on AWS and Azure; GCP - I don't know, but they have networks spanning regions, so maybe it is possible).","comment_id":"1334801","timestamp":"1735648980.0","poster":"arekm","upvote_count":"2"},{"comment_id":"1319131","poster":"benni_ale","upvote_count":"1","timestamp":"1732784820.0","content":"Selected Answer: D\nD is correct"},{"comment_id":"1269674","poster":"AndreFR","content":"A excluded because task queueing does not increase the duration of a task\nB excluded, spill is writing to storage when a memory is insufficient (not storage insufficient)\nC excluded, region cannot have a 100 times impact on duration\nE excluded, no errors mentioned in question","timestamp":"1724186040.0","upvote_count":"1"},{"timestamp":"1717212420.0","comment_id":"1222478","content":"D. Skew caused by more data being assigned to a subset of spark-partitions.","upvote_count":"1","poster":"imatheushenrique"},{"poster":"vikram12apr","content":"Selected Answer: D\nbecause a particular executors are executing majority of data while rest are processing very less. The total execution time depends upon the slowest executors.\nAnswer is D.","timestamp":"1709879580.0","upvote_count":"3","comment_id":"1168595"},{"upvote_count":"1","poster":"Jay_98_11","timestamp":"1705170660.0","comment_id":"1121955","content":"Selected Answer: D\ncorrect"},{"comment_id":"1118666","timestamp":"1704896640.0","poster":"kz_data","content":"Selected Answer: D\nI think D is correct","upvote_count":"1"},{"comment_id":"1040373","poster":"sturcu","content":"Selected Answer: D\nD is correct","timestamp":"1697013060.0","upvote_count":"1"},{"poster":"Eertyy","content":"D is the correct answer","timestamp":"1695323700.0","comment_id":"1013363","upvote_count":"3"}]},{"id":"bUUaAnJWxDHlnnXRLqNh","question_text":"Each configuration below is identical to the extent that each cluster has 400 GB total of RAM, 160 total cores and only one Executor per VM.\nGiven a job with at least one wide transformation, which of the following cluster configurations will result in maximum performance?","unix_timestamp":1690972440,"timestamp":"2023-08-02 12:34:00","question_images":[],"answer_images":[],"answer_description":"","choices":{"D":"• Total VMs: 4\n• 100 GB per Executor\n• 40 Cores/Executor","E":"• Total VMs:2\n• 200 GB per Executor\n• 80 Cores / Executor","B":"• Total VMs: 8\n• 50 GB per Executor\n• 20 Cores / Executor","C":"• Total VMs: 16\n• 25 GB per Executor\n• 10 Cores/Executor","A":"• Total VMs; 1\n• 400 GB per Executor\n• 160 Cores / Executor"},"isMC":true,"topic":"1","answer":"B","question_id":120,"answers_community":["B (44%)","A (29%)","D (15%)","12%"],"url":"https://www.examtopics.com/discussions/databricks/view/117097-exam-certified-data-engineer-professional-topic-1-question/","discussion":[{"poster":"robson90","comments":[{"timestamp":"1699089480.0","upvote_count":"3","poster":"dp_learner","content":"source : https://docs.databricks.com/en/clusters/cluster-config-best-practices.html","comment_id":"1061977"}],"content":"Option A, question is about maximum performance. Wide transformation will result in often expensive shuffle. With one executor this problem will be resolved. https://docs.databricks.com/en/clusters/cluster-config-best-practices.html#complex-batch-etl","upvote_count":"43","comment_id":"988395","timestamp":"1692800820.0"},{"comment_id":"1422587","poster":"Ashok_Choudhary_CT","content":"Selected Answer: C\nHow Option (C) Excels?\n✅ More Executors (16 vs. 8 in Option B) → Faster parallel execution.\n✅ Fewer Cores per Executor (10 vs. 20 in Option B) → Prevents CPU contention and scheduling delays.\n✅ Better Memory Management (25GB vs. 50GB in Option B) → Reduces GC overhead.\n\nFinal Verdict\nOption (C) is the \"Best\" configuration for handling a job with wide transformations.","timestamp":"1743592680.0","upvote_count":"1"},{"poster":"capt2101akash","content":"Selected Answer: A\nThe question talks about higher performance for one large wide transformation. This needs fewer large VMs/Executor. Therefore, one needs to choose the largest possible option.","upvote_count":"1","timestamp":"1742936100.0","comment_id":"1410164"},{"content":"Selected Answer: C\noverly large executors are bad due to large Garbage Collection (GC) overhead and inefficient parallelism \noption C provides the best balance of parallelism, memory utilization and performance efficiency","upvote_count":"1","timestamp":"1739031900.0","comment_id":"1353500","poster":"shaswat1404"},{"timestamp":"1738628400.0","poster":"fabiospont","upvote_count":"1","comment_id":"1351152","content":"Selected Answer: A\nA is correct only one VM per Job."},{"timestamp":"1735804740.0","upvote_count":"2","poster":"hassan_1","comment_id":"1335397","content":"Selected Answer: B\nas the question states 1 executer per VM and the recommendation is not to use single worker in production so the answer should be B"},{"poster":"HairyTorso","timestamp":"1735741440.0","content":"Selected Answer: B\nNumber of workers\nChoosing the right number of workers requires some trials and iterations to figure out the compute and memory needs of a Spark job. Here are some guidelines to help you start:\n\nNever choose a single worker for a production job, as it will be the single point for failure\nStart with 2-4 workers for small workloads (for example, a job with no wide transformations like joins and aggregations)\nStart with 8-10 workers for medium to big workloads that involve wide transformations like joins and aggregations, then scale up if necessary\n\nhttps://www.databricks.com/discover/pages/optimize-data-workloads-guide#number-workers","upvote_count":"4","comment_id":"1335203"},{"content":"Selected Answer: A\nMaximum performance - A guarantees no shuffles between nodes in the cluster. Only processes on one VM.","comment_id":"1334804","timestamp":"1735649580.0","poster":"arekm","upvote_count":"1"},{"comments":[{"comment_id":"1334803","content":"The question is about maximum performance.","poster":"arekm","timestamp":"1735649460.0","upvote_count":"1"}],"upvote_count":"1","content":"Selected Answer: B\nAnswer B offers a good balance with 8 executors, providing a decent amount of memory and cores per executor, allowing for significant parallel processing.\nOption C increases the number of executors further but at the cost of reduced memory and cores per executor, which might not be as effective for wide transformations.","timestamp":"1733922060.0","comment_id":"1325025","poster":"AlejandroU"},{"comment_id":"1324187","timestamp":"1733772120.0","upvote_count":"2","poster":"janeZ","content":"Selected Answer: C\nfor wide transformations, leveraging multiple executors typically results in better performance, resource utilization, and fault tolerance."},{"upvote_count":"2","poster":"Shakmak","comment_id":"1320335","content":"Selected Answer: B\nB is a correct Answer based on \nhttps://www.databricks.com/discover/pages/optimize-data-workloads-guide#all-purpose","timestamp":"1732990680.0"},{"comments":[{"content":"Exactly, Also how can one node will resolve shuffle issue","comment_id":"1319514","poster":"Snakode","timestamp":"1732846800.0","upvote_count":"1","comments":[{"content":"VM != node","timestamp":"1733798880.0","comment_id":"1324340","poster":"Nicks_name","upvote_count":"1"}]}],"timestamp":"1731325560.0","upvote_count":"3","content":"Selected Answer: B\nBesides that A & E do not provide enough parallelism & fault tolerance, I can't explain why, but the correct answer is B. I got the same question during the exam and got 100% at tooling with answer B. (B is the answer provided by other sites similar to examtopics)\n\nChoosing between B, C & D is tricky !","poster":"AndreFR","comment_id":"1309988"},{"upvote_count":"3","comments":[{"timestamp":"1730983320.0","comment_id":"1308357","poster":"benni_ale","content":"https://www.databricks.com/discover/pages/optimize-data-workloads-guide","upvote_count":"1"}],"timestamp":"1730825400.0","poster":"kimberlyvsmith","comment_id":"1307463","content":"Selected Answer: B\nB\n\n\"Number of workers\nChoosing the right number of workers requires some trials and iterations to figure out the compute and memory needs of a Spark job. Here are some guidelines to help you start:\n\nNever choose a single worker for a production job, as it will be the single point for failure\nStart with 2-4 workers for small workloads (for example, a job with no wide transformations like joins and aggregations)\nStart with 8-10 workers for medium to big workloads that involve wide transformations like joins and aggregations, then scale up if necessary\""},{"comment_id":"1184214","content":"Selected Answer: A\nWide transformation falls under complex etl which means Option A is correct in the documentation didn't mention to do otherwise in this scenario.","poster":"arik90","timestamp":"1711555620.0","upvote_count":"1"},{"comment_id":"1145269","upvote_count":"1","poster":"PrashantTiwari","timestamp":"1707460020.0","content":"A is correct"},{"timestamp":"1705834080.0","upvote_count":"3","content":"Selected Answer: A\nOption A:\nhttps://docs.databricks.com/en/clusters/cluster-config-best-practices.html#complex-batch-etl","poster":"vikrampatel5","comment_id":"1127760"},{"content":"Selected Answer: A\nrobson90's response explains it perfectly and has documentation to support it.","poster":"RafaelCFC","comment_id":"1116490","timestamp":"1704703320.0","upvote_count":"1"},{"poster":"ofed","content":"Option A","timestamp":"1699378920.0","comment_id":"1065040","upvote_count":"2"},{"comment_id":"1062114","timestamp":"1699103880.0","upvote_count":"2","poster":"ismoshkov","content":"Selected Answer: A\nOur goal is top performance.\nVertical scaling is more performant rather that horizontal. Especially we know that we need cross VM exchange. Option A."},{"upvote_count":"1","content":"response A. \nas of \nComplex batch ETL\n\n\" More complex ETL jobs, such as processing that requires unions and joins across multiple tables, will probably work best when you can minimize the amount of data shuffled. Since reducing the number of workers in a cluster will help minimize shuffles, you should consider a smaller cluster like cluster A in the following diagram over a larger cluster like cluster D. \"","comments":[],"timestamp":"1699089420.0","poster":"dp_learner","comment_id":"1061975"},{"timestamp":"1695348000.0","upvote_count":"4","poster":"Santitoxic","content":"Selected Answer: D\nConsidering the need for both memory and parallelism, option D seems to offer the best balance between resources and parallel processing. It provides a reasonable amount of memory and cores per Executor while maintaining a sufficient level of parallelism with 4 Executors. This configuration is likely to result in maximum performance for a job with at least one wide transformation.","comment_id":"1013509"},{"upvote_count":"3","content":"Sorry Response C = 16VM for maximing Wide Transformation","timestamp":"1694935380.0","comment_id":"1009649","poster":"mwyopme"},{"upvote_count":"1","content":"Key message is : Given a job with at least one wide transformation\nPerformance, should max the number of concurrent VM, Selecting response B. 160/10 = 16 VM","timestamp":"1694935320.0","comment_id":"1009648","poster":"mwyopme"},{"timestamp":"1692796740.0","comment_id":"988351","upvote_count":"1","content":"Selected Answer: D\nConsidering the need for both memory and parallelism, option D seems to offer the best balance between resources and parallel processing. It provides a reasonable amount of memory and cores per Executor while maintaining a sufficient level of parallelism with 4 Executors. This configuration is likely to result in maximum performance for a job with at least one wide transformation.","poster":"taif12340"},{"timestamp":"1692530220.0","upvote_count":"1","comment_id":"985731","poster":"BrianNguyen95","content":"correct answer is E: Option E provides a substantial amount of memory and cores per executor, allowing the job to handle wide transformations efficiently. However, performance can also be influenced by factors like the nature of your specific workload, data distribution, and overall cluster utilization. It's a good practice to conduct benchmarking and performance testing with various configurations to determine the optimal setup for your specific use case."},{"content":"C. More VMs helps to distribute the workload across the cluster, which results in better fault tolerance and increase the chances of job completion.","upvote_count":"2","poster":"stuart_gta1","timestamp":"1691515560.0","comment_id":"975880"},{"comment_id":"970043","comments":[{"upvote_count":"1","comment_id":"973668","content":"would it be fault-tolerant?","timestamp":"1691312100.0","poster":"8605246"}],"content":"answer should be E. if at least one transformation is wide, so 1 executor of 200GB can do the job, rest of tasks can be carried out on the other node","upvote_count":"1","poster":"asmayassineg","timestamp":"1690972440.0"}],"answer_ET":"B","exam_id":163}],"exam":{"numberOfQuestions":200,"isMCOnly":true,"isBeta":false,"provider":"Databricks","lastUpdated":"12 Apr 2025","isImplemented":true,"name":"Certified Data Engineer Professional","id":163},"currentPage":24},"__N_SSP":true}