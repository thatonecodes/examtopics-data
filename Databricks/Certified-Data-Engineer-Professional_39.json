{"pageProps":{"questions":[{"id":"slSue2y8VE49lfg0pryf","choices":{"A":"The Databricks REST API can be used to modify the Spark configuration properties for an interactive cluster without interrupting jobs currently running on the cluster.","C":"Spark configuration properties can only be set for an interactive cluster by creating a global init script.","E":"When the same Spark configuration property is set for an interactive cluster and a notebook attached to that cluster, the notebook setting will always be ignored.","D":"Spark configuration properties set for an interactive cluster with the Clusters UI will impact all notebooks attached to that cluster.","B":"Spark configurations set within a notebook will affect all SparkSessions attached to the same interactive cluster."},"question_id":191,"answers_community":["D (90%)","5%"],"answer_ET":"D","answer":"D","url":"https://www.examtopics.com/discussions/databricks/view/127597-exam-certified-data-engineer-professional-topic-1-question/","question_images":[],"isMC":true,"unix_timestamp":1701465060,"answer_description":"","exam_id":163,"discussion":[{"comment_id":"1087767","upvote_count":"8","poster":"hamzaKhribi","content":"Selected Answer: D\nI tried it myself, setting a spark conf on the cluster ui, will impact all notebooks attached to that cluster, for example i set the number of shuffle partitions to 4, and in every notebook when i inspect the number of partitions i find 4.","timestamp":"1701707280.0"},{"upvote_count":"2","poster":"Sriramiyer92","comment_id":"1335237","content":"Selected Answer: D\nD is correct. \nPoint to Note:\nPrecedence - (Spark configurations set programmatically in a notebook or script) > (Cluster-wide configurations set in the Clusters UI)","timestamp":"1735752120.0"},{"content":"Selected Answer: D\nBoth via API or UI will trigger a restart of the cluster so A is wrong. D is correct given that \"impact\" does not mean the cluster can't be restarted.","poster":"benni_ale","comment_id":"1321796","timestamp":"1733305620.0","upvote_count":"1"},{"poster":"Curious76","upvote_count":"1","timestamp":"1709003460.0","content":"Selected Answer: B\nA. Incorrect: Modifying configurations through the Databricks REST API while jobs are running can lead to unexpected behavior or disruption. It's generally not recommended.\n C. Incorrect: While global init scripts can be used, it's not the only way. Configurations can also be set within notebooks.\n D. Incorrect: Configurations set through the Clusters UI apply to the entire cluster, but they might not necessarily override configurations set within notebooks attached to the cluster.\n E. Incorrect: Notebook configurations can take precedence over cluster-level configurations for the same property, offering finer-grained control at the notebook level.","comment_id":"1160230"},{"content":"Selected Answer: D\nThese settings are applied at the cluster level and affect all SparkSessions on the cluster.","poster":"vctrhugo","timestamp":"1707181560.0","comment_id":"1141664","upvote_count":"3"},{"poster":"alexvno","comment_id":"1100388","timestamp":"1702968600.0","upvote_count":"4","content":"Selected Answer: D\nA wrong, cluster will restart -> D"},{"poster":"petrv","upvote_count":"1","content":"Selected Answer: A\nIn Databricks, you can use the Databricks REST API to modify Spark configuration properties for an interactive cluster without interrupting currently running jobs. This allows you to dynamically adjust Spark configurations to optimize performance or meet specific requirements without the need to restart the cluster.","timestamp":"1701465060.0","comments":[{"poster":"petrv","upvote_count":"2","content":"If you update the configuration of a cluster using the Databricks REST API or the Clusters UI while the cluster is in a RUNNING state, the cluster will be restarted to apply the new configuration. However, Databricks typically handles this situation in a way that minimizes disruption to running jobs.","comment_id":"1085501","timestamp":"1701465840.0"}],"comment_id":"1085492"}],"answer_images":[],"topic":"1","timestamp":"2023-12-01 22:11:00","question_text":"Which statement regarding Spark configuration on the Databricks platform is true?"},{"id":"tV7c3Y99bBZympTgKibZ","exam_id":163,"choices":{"C":"Use Repos to pull changes from the remote Git repository; commit and push changes to a branch that appeared as changes were pulled.","A":"Use Repos to checkout all changes and send the git diff log to the team.","E":"Use Repos to create a new branch, commit all changes, and push changes to the remote Git repository.","D":"Use Repos to merge all differences and make a pull request back to the remote repository.","B":"Use Repos to create a fork of the remote repository, commit all changes, and make a pull request on the source repository."},"answer_images":[],"isMC":true,"answer_description":"","topic":"1","question_images":[],"question_id":192,"question_text":"A developer has successfully configured their credentials for Databricks Repos and cloned a remote Git repository. They do not have privileges to make changes to the main branch, which is the only branch currently visible in their workspace.\n\nWhich approach allows this user to share their code updates without the risk of overwriting the work of their teammates?","answer":"E","timestamp":"2024-01-01 15:49:00","url":"https://www.examtopics.com/discussions/databricks/view/130091-exam-certified-data-engineer-professional-topic-1-question/","answers_community":["E (63%)","B (37%)"],"unix_timestamp":1704120540,"answer_ET":"E","discussion":[{"timestamp":"1734957300.0","upvote_count":"1","content":"Selected Answer: E\nAnswer E. Creating a new branch within the repository is the most effective and recommended approach for developers who lack write access to the main branch in Databricks Repos. While forking is a valid strategy in some situations, it's generally less common and potentially more complex within the context of Databricks Repos, especially when the primary goal is to contribute to the existing shared repository.","comment_id":"1330810","poster":"AlejandroU"},{"poster":"benni_ale","comment_id":"1301422","upvote_count":"1","timestamp":"1729575240.0","content":"Selected Answer: E\nI changed my mind i think is E"},{"poster":"benni_ale","upvote_count":"1","timestamp":"1729253700.0","content":"Selected Answer: B\nExplanation:\n\nThe developer does not have privileges to make changes to the main branch of the remote repository, and it's the only branch visible in their workspace. To share their code updates without risking overwriting their teammates' work, the best approach is to:\n\nCreate a personal copy (fork) of the remote repository. This forked repository will be under the developer's own account or workspace, allowing full control over it.\nMake Changes in the Fork:\n\nCommit all code updates to the forked repository. Since the developer has full privileges on their fork, they can create branches, commit changes, and manage the repository as needed.\n\nOption E: Creating a new branch and pushing changes to the remote repository requires write access to the repository, which the developer does not have.\n\nBy forking the repository, the developer avoids any permission issues and ensures that their work does not interfere with the main codebase until it is reviewed and approved by the team.","comments":[{"content":"My bad i think E is correct","comment_id":"1301423","timestamp":"1729575480.0","poster":"benni_ale","upvote_count":"1"}],"comment_id":"1299655"},{"comments":[{"upvote_count":"3","content":"Sorry, my mistake, just tested B is a do-able way. Fork from github can create pull request against the original repository and contribute back. B seems to be a better answer.","comment_id":"1167747","poster":"hal2401me","timestamp":"1709796600.0"}],"poster":"hal2401me","timestamp":"1709622240.0","content":"Selected Answer: E\nE is the regular collaboration approach.\nB makes a fork so breaks away from the collaborating teamates. There's no way they can make a pull request on the source repository after making change to a fork.","comment_id":"1166272","upvote_count":"4"},{"comments":[{"poster":"carlosmps","upvote_count":"3","timestamp":"1722122640.0","content":"The argument provided suggests using a traditional Git workflow that involves forking a repository on a Git hosting service like GitHub, making changes, and then creating a pull request. This is a valid and widely used approach in standard Git workflows. However, it is essential to distinguish between the capabilities and features provided directly by Databricks Repos and the general Git practices that can be applied outside of Databricks.","comments":[{"upvote_count":"1","content":"Exactly my thought as well!","poster":"Sriramiyer92","timestamp":"1735752420.0","comment_id":"1335238"}],"comment_id":"1256509"}],"comment_id":"1160498","poster":"Curious76","timestamp":"1709032800.0","content":"Selected Answer: B\nDatabricks Repos itself does not currently support creating forks directly within the platform. However, you can achieve a similar workflow using the following steps:\n\n Use the git clone command in a terminal or IDE to create a local copy of the remote repository. This effectively creates a local fork.\n Make your changes in the local copy.\n Use git push to push your changes to a new remote repository you create on a Git hosting service like GitHub.\n Create a pull request from your new remote repository to the original repository on Databricks Repos.\n\nThis approach allows you to make changes to your own copy of the code, collaborate with others through code reviews, and propose your changes for integration into the main branch without directly modifying it","upvote_count":"2"},{"timestamp":"1707181380.0","poster":"vctrhugo","content":"Selected Answer: E\nThis is a common workflow in collaborative development environments. The developer can create a new branch in their local repository, make changes, and then push the branch to the remote repository. This way, they can share their updates without modifying the main branch directly. After pushing the changes, they can create a pull request on the remote repository, allowing their teammates to review the changes before merging them into the main branch. This process ensures that the main branch remains stable and that all changes are reviewed and approved before they’re incorporated. It also prevents any accidental overwrites of teammates’ work.","upvote_count":"3","comment_id":"1141661"},{"timestamp":"1706866560.0","poster":"PrincipalJoe","content":"Databricks Repos cannot be used to fork a repository","comment_id":"1138381","upvote_count":"3"},{"upvote_count":"3","comment_id":"1137369","timestamp":"1706773260.0","poster":"adenis","content":"Selected Answer: E\nE is correct"},{"comment_id":"1136242","content":"E is correct\nif you create a fork you create another repository so not B","timestamp":"1706656560.0","poster":"Rinscy","upvote_count":"2"},{"poster":"Crocjun","comment_id":"1134620","content":"Selected Answer: B\nB is correct","upvote_count":"2","timestamp":"1706497440.0"},{"content":"Selected Answer: B\nB is correct","comment_id":"1131768","timestamp":"1706194320.0","upvote_count":"2","poster":"spaceexplorer"},{"poster":"dmov","content":"Isn't this B?","upvote_count":"1","comment_id":"1111275","timestamp":"1704120540.0"}]},{"id":"7tBseGd9qKWWrnsSJpYV","answer":"D","discussion":[{"timestamp":"1702969380.0","poster":"alexvno","comment_id":"1100398","content":"Selected Answer: D\nShallow clone: only duplicates the metadata of the table being cloned; the data files of the table itself are not copied. These clones are cheaper to create but are not self-contained and depend on the source from which they were cloned as the source of data. If the files in the source that the clone depends on are removed, for example with VACUUM, a shallow clone may become unusable. Therefore, shallow clones are typically used for short-lived use cases such as testing and experimentation.","upvote_count":"8"},{"timestamp":"1729077120.0","upvote_count":"2","poster":"benni_ale","content":"I was not sure whether B or D but somehow I think that running VACUUM comand does not invalidate SHALLOW CLONEs . I mean its just that the data referenced by the clone is no longer present. It can still happen that a SHALLOW CLONE is working even after a VACUUM command run on the cloned table (origin) . So B is not completely correct.","comment_id":"1298677"},{"timestamp":"1707181200.0","poster":"vctrhugo","content":"Selected Answer: D\nIn Delta Lake, the VACUUM command deletes data files that are no longer referenced by a Delta table and are older than the retention threshold. When a table is cloned using SHALLOW CLONE, the clone references the same data files as the original table but creates a new transaction log. If VACUUM is run on the original table, it can delete data files that are still being referenced by the cloned table’s metadata, causing the cloned table to stop working. This is because the VACUUM command doesn’t know about the cloned table’s references to the data files. Therefore, it’s important to be cautious when running VACUUM on tables that have clones.","upvote_count":"3","comment_id":"1141658"},{"poster":"spaceexplorer","content":"Selected Answer: D\nD is correct","upvote_count":"1","comment_id":"1131771","timestamp":"1706194500.0"},{"upvote_count":"2","comment_id":"1075152","content":"Selected Answer: D\nPlease refer: \nhttps://docs.databricks.com/en/delta/clone.html#what-are-the-semantics-of-delta-clone-operations","timestamp":"1700455440.0","poster":"AzureDE2522"},{"timestamp":"1700035500.0","upvote_count":"2","poster":"60ties","comment_id":"1071206","content":"Selected Answer: B\nB is best"}],"topic":"1","answer_description":"","question_id":193,"url":"https://www.examtopics.com/discussions/databricks/view/126210-exam-certified-data-engineer-professional-topic-1-question/","isMC":true,"answer_images":[],"choices":{"C":"Tables created with SHALLOW CLONE are automatically deleted after their default retention threshold of 7 days.","A":"Because Type 1 changes overwrite existing records, Delta Lake cannot guarantee data consistency for cloned tables.","B":"Running VACUUM automatically invalidates any shallow clones of a table; DEEP CLONE should always be used when a cloned table will be repeatedly queried.","E":"The data files compacted by VACUUM are not tracked by the cloned metadata; running REFRESH on the cloned table will pull in recent changes.","D":"The metadata created by the CLONE operation is referencing data files that were purged as invalid by the VACUUM command."},"answers_community":["D (88%)","13%"],"unix_timestamp":1700035500,"answer_ET":"D","question_images":[],"timestamp":"2023-11-15 09:05:00","question_text":"In order to prevent accidental commits to production data, a senior data engineer has instituted a policy that all development work will reference clones of Delta Lake tables. After testing both DEEP and SHALLOW CLONE, development tables are created using SHALLOW CLONE.\n\nA few weeks after initial table creation, the cloned versions of several tables implemented as Type 1 Slowly Changing Dimension (SCD) stop working. The transaction logs for the source tables show that VACUUM was run the day before.\n\nWhich statement describes why the cloned tables are no longer working?","exam_id":163},{"id":"kUZpLaiA7Htn8FA1jRtV","question_images":[],"answer":"B","question_text":"You are performing a join operation to combine values from a static userLookup table with a streaming DataFrame streamingDF.\n\nWhich code block attempts to perform an invalid stream-static join?","answer_description":"","timestamp":"2023-11-26 18:50:00","unix_timestamp":1701021000,"question_id":194,"answer_ET":"B","answer_images":[],"discussion":[{"timestamp":"1716738600.0","poster":"Enduresoul","upvote_count":"11","content":"Selected Answer: B\nAnswer B is correct:\nhttps://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#support-matrix-for-joins-in-streaming-queries\n\nWhen we take a look in the supported join matrix between static and stream inputs, we can identify, that Stream-Static + outer is not supported.\nAnswer E is wrong, because the Static-Stream + right join is supported.","comment_id":"1080923"},{"upvote_count":"1","comment_id":"1339443","timestamp":"1736679480.0","content":"Selected Answer: B\naccording to the Support matrix for joins in streaming queries","poster":"_lene_"},{"content":"Selected Answer: B\nAnswer B. We can directly discard options C and D since the streaming DataFrame (streamingDF) must be the left table in the join and the join type must be inner join or left outer join. Thus, the most directly invalid code block due to an unsupported join type is B.","upvote_count":"1","timestamp":"1735019820.0","poster":"AlejandroU","comment_id":"1331033"},{"content":"B.\nWe match all the records from a static DataFrame on the left with a stream DataFrame on the right. If records do not match from the static DF (Left) to stream DF (Right), then the system cannot return null since the data changes on stream DF (Right), and we cannot guarantee if we will get matching records. That is why full_outer join is not supported.","upvote_count":"2","timestamp":"1733026740.0","comment_id":"1222431","poster":"imatheushenrique"},{"upvote_count":"4","comment_id":"1173210","timestamp":"1726290960.0","poster":"hal2401me","content":"Selected Answer: E\nin my exam today, BCD are removed. i chose E, because I recall that stream-static right join are less supported."},{"content":"Selected Answer: B\nb is correct","comment_id":"1160250","timestamp":"1724726220.0","upvote_count":"1","poster":"Curious76"},{"content":"Selected Answer: B\nSpecifically, outer joins are not supported with a static DataFrame on the right and a streaming DataFrame on the left. This is because it’s not possible to guarantee all necessary rows will be available in the streaming DataFrame for every micro-batch.","comment_id":"1141655","upvote_count":"1","timestamp":"1722898500.0","poster":"vctrhugo"},{"comment_id":"1120773","timestamp":"1720781040.0","poster":"kz_data","comments":[{"upvote_count":"1","comment_id":"1120775","content":"Sorry I missread the question.","timestamp":"1720781220.0","poster":"kz_data"}],"upvote_count":"1","content":"Selected Answer: D\nI think the correct answer is D."},{"content":"Selected Answer: B\nbelieve B is correct as provided below","comment_id":"1114416","upvote_count":"2","poster":"lexaneon","timestamp":"1720166700.0"}],"isMC":true,"answers_community":["B (77%)","E (18%)","5%"],"exam_id":163,"url":"https://www.examtopics.com/discussions/databricks/view/127268-exam-certified-data-engineer-professional-topic-1-question/","topic":"1","choices":{"D":"streamingDF.join(userLookup, [\"userid\"], how=\"inner\")","B":"streamingDF.join(userLookup, [\"user_id\"], how=\"outer\")","E":"userLookup.join(streamingDF, [\"user_id\"], how=\"right\")","C":"streamingDF.join(userLookup, [\"user_id”], how=\"left\")","A":"userLookup.join(streamingDF, [\"userid\"], how=\"inner\")"}},{"id":"659pbR3NuntI8jfmEI8H","question_images":[],"answer":"B","question_text":"Spill occurs as a result of executing various wide transformations. However, diagnosing spill requires one to proactively look for key indicators.\n\nWhere in the Spark UI are two of the primary indicators that a partition is spilling to disk?","timestamp":"2023-11-15 12:45:00","answer_description":"","unix_timestamp":1700048700,"question_id":195,"answer_ET":"B","answer_images":[],"discussion":[{"comment_id":"1071383","timestamp":"1700048700.0","poster":"60ties","content":"Selected Answer: B\nB is correct","upvote_count":"7"},{"comment_id":"1141651","timestamp":"1707180660.0","upvote_count":"5","content":"Selected Answer: B\nIn the Spark UI, the Stage’s detail screen provides key metrics about each stage of a job, including the amount of data that has been spilled to disk. If you see a high number in the “Spill (Memory)” or “Spill (Disk)” columns, it’s an indication that a partition is spilling to disk.\n\nThe Executor’s log files can also provide valuable information about spill. If a task is spilling a lot of data, you’ll see messages in the logs like “Spilling UnsafeExternalSorter to disk” or “Task memory spill”. These messages indicate that the task ran out of memory and had to spill data to disk.","poster":"vctrhugo"},{"content":"Selected Answer: B\nb","comment_id":"1294050","timestamp":"1728281820.0","poster":"dd1192d","upvote_count":"1"},{"timestamp":"1703044920.0","content":"Selected Answer: E\nE is correct","upvote_count":"1","comments":[{"poster":"jin1991","comment_id":"1101196","timestamp":"1703044980.0","upvote_count":"2","content":"My bad, looking again, B is correct."}],"poster":"jin1991","comment_id":"1101194"}],"isMC":true,"exam_id":163,"answers_community":["B (93%)","7%"],"url":"https://www.examtopics.com/discussions/databricks/view/126236-exam-certified-data-engineer-professional-topic-1-question/","topic":"1","choices":{"E":"Stage’s detail screen and Query’s detail screen","B":"Stage’s detail screen and Executor’s log files","D":"Executor’s detail screen and Executor’s log files","C":"Driver’s and Executor’s log files","A":"Query’s detail screen and Job’s detail screen"}}],"exam":{"id":163,"name":"Certified Data Engineer Professional","isBeta":false,"provider":"Databricks","isMCOnly":true,"lastUpdated":"12 Apr 2025","isImplemented":true,"numberOfQuestions":200},"currentPage":39},"__N_SSP":true}