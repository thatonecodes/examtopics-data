{"pageProps":{"questions":[{"id":"nmymvfixWWpG3NUnMGjP","unix_timestamp":1617561000,"choices":{"D":"Work with the build team to automate a JSON-based manifest to the repository when changes occur in the repository. Create a subscribed content library for each vSphere environment. Configure the content library to download content automatically.","C":"Work with the build team to automate a JSON-based manifest to the repository when changes occur in the repository. Create a subscribed content library for each vSphere environment. Configure the content library to download content when needed.","A":"Work with the build team to create a local content library for each vSphere environment. Import the OVF images when new image are published to the repository.","B":"Create a local content library for the primary vSphere environment in each data center. Create a subscribed content library for each additional vSphere environment in each data center. Configure the content library to download content automatically."},"isMC":true,"answer":"D","answers_community":["D (62%)","B (31%)","8%"],"question_text":"An architect is designing a series of new vSphere environments for an organization. The environments will be deployed in their US-East and US-West region data centers. Each data center may have one or more dedicated vSphere environments. Only the vSphere environments within a data center will be configured with\nEnhanced Linked Mode. The Chief Technology Officer (CTO) has authorized the use of VMware vRealize Automation Cloud for automation. The build team creates standardized virtual machine images for various operating systems in Open Virtualization Format (OVF) and publishes the latest version on an as-needed basis to an internal HTTPS-accessible repository.\nThe architect must design a content library topology that meets the following requirements:\n✑ A localized content library must be available in each data center.\nEach content library must be updated when an image is updated and released by the build team.\n//IMG//\n\n✑ The cloud automation platform must be able to consume the latest approved content library images.\n✑ It must leverage the existing build team processes.\nWhat should the architect recommend to meet the requirements?","exam_id":253,"discussion":[{"poster":"nemisis95","upvote_count":"11","comments":[{"timestamp":"1650897240.0","comment_id":"467501","comments":[{"poster":"puff91","timestamp":"1656582360.0","upvote_count":"2","comment_id":"513311","content":"Just click on synchronize. It only works with OVF\nhttps://docs.vmware.com/en/VMware-vSphere/6.7/com.vmware.vsphere.vm_admin.doc/GUID-C431394B-5D06-48B5-999B-181DB0E8BAEF.html"}],"upvote_count":"3","content":"✑ Each content library must be updated when an image is updated and released by the build team. how can this requirement be satisfied","poster":"Anzee"}],"timestamp":"1641300120.0","comment_id":"398225","content":"I've re-read it - it's B"},{"poster":"FR_Wolfman","comment_id":"1128683","content":"Selected Answer: C\nFor me, C would be the best option.\nWe must leverage the existing build team processes : so we should use the existing internal HTTPS-accessible repository, instead of creating a local/published repository on which they will have to put the OVFs --> That eliminates A & B.\nC & D could fit the need : with a JSON-based manifest in the internal repository, you can create subscribed libraries linked to it.\nWe will go to answer C, as it is stated that we need the versions on an as-needed basis.","upvote_count":"1","timestamp":"1721648460.0"},{"timestamp":"1720112700.0","poster":"Justang197031","content":"B is correct. Why D is wrong? Because \"It must leverage the existing build team processes.\" and D requires and change to the existing process. Also a Subscribed library cannot be created without a local-published library to subscribe to.","comment_id":"1113998","upvote_count":"1"},{"poster":"MohamedZohair","content":"The answer is D","timestamp":"1697324160.0","comment_id":"870516","upvote_count":"1"},{"poster":"diegof1","content":"Answer is D\n\nThe requirements say that It must leverage the existing build team processes, and they publish the latest version on an as-needed basis to an internal HTTPS-accessible repository so a subscription from 3rd party Content Library is the solution.\n\nhttps://williamlam.com/2015/06/creating-your-own-3rd-party-content-library-for-vsphere-6-0-vcloud-director-5-x.html","upvote_count":"2","timestamp":"1690490280.0","comment_id":"790046"},{"poster":"Alchot","upvote_count":"2","content":"Selected Answer: D\nhttps://blogs.vmware.com/cloud/2020/05/13/use-content-libraries-vrealize-automation-8-vrealize-automation-cloud/","comment_id":"789021","timestamp":"1690390200.0"},{"timestamp":"1686047940.0","content":"Selected Answer: D\nI`ll go with \"D\", as it must leverage from existing build team processes.","upvote_count":"1","comment_id":"736812","poster":"purulence"},{"poster":"NayanajithS","upvote_count":"1","content":"Selected Answer: D\nBuild team publish to internal repository only... This need to be synchronized to other site. Therefore, suggest creating subscribed content library for each environment.\n\nAfter the synchronization finishes, the item content and metadata are downloaded to the storage of the subscribed library. On the Templates tab for the subscribed library, the value for the item in the Stored Content Locally column changes to Yes.","timestamp":"1677563340.0","comment_id":"653791"},{"poster":"Bobob55","content":"Selected Answer: B\nCorrect answer","upvote_count":"2","timestamp":"1676460420.0","comment_id":"647093"},{"comment_id":"644023","poster":"Bobob55","timestamp":"1675852140.0","upvote_count":"1","content":"Selected Answer: B\nB as local for each site and automatically bringing down content as needed"},{"timestamp":"1675501080.0","comment_id":"642204","content":"Selected Answer: B\nB, in this case vRA Cloud can interact with local CL, one for each DC. And other sites have subscribed version of local ones.","upvote_count":"1","poster":"PSE_IT"},{"upvote_count":"1","timestamp":"1669248660.0","comment_id":"606333","poster":"c11","content":"A\nI did a good reading. There is one factor that everyone is missing.\nvRA cloud image mapping can only see images from the \"local content library\". Images from the subcriber library are not visible to vRA cloud.\nREF 1: https://docs.vmware.com/en/vRealize-Automation/8.7/Using-and-Managing-Cloud-Assembly/GUID-9CBAA91A-FAAD-4409-AFFC-ACC1810E4FA5.html\n\nREF-2 - https://vtam.nl/2022/02/10/using-vcenter-content-library-with-vrealize-automation-8/\n\nSomeone, please correct me on this."},{"comment_id":"564654","upvote_count":"1","poster":"krimie","content":"I think It's B since we can create a Two local Content Library in each datacenter , then create a subscribed content Library with the internal HTTPS-accessible repository and finally Create a new subscription from the local content and select the existing Subscriber library we've created","timestamp":"1662790080.0"},{"timestamp":"1656327420.0","content":"I think it's B because it's asked about local CL in the design.","poster":"Mezze","comment_id":"510305","upvote_count":"3"},{"upvote_count":"4","timestamp":"1655303220.0","comment_id":"502338","poster":"cr2001","content":"Selected Answer: D\nSee this example:https://www.brianjgraf.com/3rd-party-content-library-synology-s3/\n3rd party local library with JSON manifest, and subscribed via HTTPS vsphere library. \nSo i think it`s D","comments":[{"poster":"IronMtn","content":"That website is gone. Try https://web.archive.org/web/20210418054332/https://www.brianjgraf.com/3rd-party-content-library-synology-s3/ instead.","timestamp":"1706785440.0","comment_id":"968905","upvote_count":"1"},{"upvote_count":"2","content":"That definitelly works but I doubt that vmware would include in exam something based on blog article and soho grade storage. Or is this actually documented somewhere else?","comment_id":"577501","poster":"mnq59986","timestamp":"1664448060.0"}]},{"timestamp":"1653554700.0","comment_id":"487264","poster":"ugurgazi","content":"B or D, I am also still stuck. localized versus local.. come on vmware","upvote_count":"2"},{"timestamp":"1652956200.0","upvote_count":"1","comment_id":"481630","poster":"cloud29","content":"Which answer is correct and why ?\nis it B or D?"},{"upvote_count":"2","comment_id":"423954","content":"Could it not be C? I read somewhere that the OVF templates can't be synced to a subscribed library automatically. It has to be manually copied from published to subscribed library.","poster":"luckybme","timestamp":"1644717180.0"},{"comment_id":"408289","upvote_count":"1","content":"I think B","timestamp":"1642408620.0","poster":"hansel"},{"upvote_count":"2","comment_id":"375770","content":"The answer is D for the following reasons\n\nThe build team creates standardized virtual machine images for various operating systems in Open Virtualization Format (OVF) and publishes the latest version on an as-needed basis to an internal HTTPS-accessible repository.\n\nA localized content library must be available in each data center.\n\nEach content library must be updated when an image is updated and released by the build team.\n\nIt must leverage the existing build team processes.","timestamp":"1638783780.0","poster":"nemisis95"},{"upvote_count":"3","content":"Is it B Correct because of the local content requirement\nany insight here??","comment_id":"349747","comments":[{"poster":"nemisis95","content":"Yes you are right","timestamp":"1642059480.0","upvote_count":"1","comment_id":"405174"}],"timestamp":"1636061820.0","poster":"Moda"},{"comment_id":"328168","timestamp":"1633372200.0","poster":"amgice","upvote_count":"4","comments":[{"content":"B is correct because the requirement of 2 local on each site...","timestamp":"1633422420.0","comment_id":"328494","upvote_count":"2","comments":[{"timestamp":"1648188180.0","comment_id":"451206","upvote_count":"1","content":"Not \"local\" but \"localized\"","poster":"zonder"}],"poster":"amgice"}],"content":"D -- It must leverage the existing build team processes\nYou can subscribed a library to a thidparty https repository\n\nhttps://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.vm_admin.doc/GUID-9DE2BD8F-E499-4F1E-956B-67212DE593C6.html"}],"question_images":["https://www.examtopics.com/assets/media/exam-media/04348/0001800002.png"],"answer_description":"","topic":"1","answer_images":[],"question_id":16,"timestamp":"2021-04-04 20:30:00","answer_ET":"D","url":"https://www.examtopics.com/discussions/vmware/view/49077-exam-3v0-2121-topic-1-question-23-discussion/"},{"id":"ZhTUv5VNsGoNHtSVuaqC","question_id":17,"timestamp":"2021-04-04 20:44:00","discussion":[{"comment_id":"440710","content":"Changing to D - vVOLs \n\nvVOLs accomplish each one of the requirements, but specially this one: \"Virtual machine aware back up will be leveraged\"\n\nHere references:\nReplication -> https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.storage.doc/GUID-6346A936-5084-4F38-ACB5-B5EC70AB8269.html\n\nhttps://dyertribe.co.uk/2020/05/04/vvol-replication-with-nimble/\n\nhttps://support.purestorage.com/Solutions/VMware_Platform_Guide/User_Guides_for_VMware_Solutions/Virtual_Volumes_User_Guide/vVols_Deep_Dive%3A_Array_Based_Replication_with_vVols\n\nAware backup -> https://documentation.commvault.com/commvault/v11/article?p=14307.htm\n\nhttps://www.nakivo.com/blog/application-aware-vm-backup/\n\n\"The impact on the storage layer should not impact the performance of the compute layer.\" VSAN will have an important impact of compute layer.\n\n\"Operational management overhead should be minimized\" >> FC Additional management overhead (e.g. switch zoning)","comments":[{"upvote_count":"2","timestamp":"1651830240.0","poster":"nefini","comment_id":"473437","content":"I think your right. vSAN would fit but the fact that storage must not impact compute layer makes it a worse awnser than vvols."}],"poster":"primanturin","timestamp":"1646637000.0","upvote_count":"11"},{"poster":"FR_Wolfman","content":"Selected Answer: D\nAnswer D.\nTo have different replication attributes at the application level (or VM level), the only options to do this are vSAN and vVols.\nAs we need the storage layer to not impact the compute layer, we can eliminate answer B (vSAN).","timestamp":"1721648640.0","upvote_count":"1","comment_id":"1128686"},{"content":"Selected Answer: D\nChanging to D \n\n✑ Asynchronous replication is required between two sites.\nOffloaded to the array\n\n✑ The impact on the storage layer should not impact the performance of the compute layer.\nArray has dedicated compute power\n\n✑ Each application tier will require different replication attributes.\n\n✑ Virtual machine live migration across compute and storage must be supported.\nSupported in vvols\n✑ Virtual machine aware back up will be leveraged.\nSelected solution must support vvols\n✑ Operational management overhead should be minimized.\nvvols\n✑ Operational automation should be supported.\nvvols","upvote_count":"2","timestamp":"1692689280.0","poster":"Alchot","comment_id":"817668"},{"content":"Selected Answer: A\nArray capability ✑ Asynchronous replication is required between two sites. \nISCSi configuration can be part of host profile ✑ Operational management overhead should be minimized. \nArray capability ✑ Each application tier will require different replication attributes.\n Supported ✑ Virtual machine live migration across compute and storage must be supported. Supported ✑ Virtual machine aware back up will be leveraged. Host profiles, PoweCLI covers this ✑ Operational automation should be supported.","poster":"Alchot","timestamp":"1690392060.0","comment_id":"789044","upvote_count":"1"},{"comment_id":"737749","upvote_count":"1","timestamp":"1686128160.0","content":"Selected Answer: D\nfirst thought was B or D, but the requirements include \"Operational automation\" \n\nMust be D:\nhttps://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/products/virtualvolumes/vmw-vsphere-virtual-volumes-vvolvs-solution-overview.pdf","poster":"unofficial_official"},{"comment_id":"602356","timestamp":"1668570180.0","content":"Selected Answer: D\nI think it's D. vVOL.","poster":"c11","upvote_count":"2"},{"timestamp":"1647484560.0","poster":"Helpinghanditexams","content":"Correct Answer : A","upvote_count":"1","comments":[{"poster":"ugurgazi","comment_id":"487269","upvote_count":"1","timestamp":"1653555180.0","content":"Because?"}],"comment_id":"446235"},{"upvote_count":"2","timestamp":"1645680540.0","poster":"lifeflakes","comment_id":"430432","content":"It should be D, based on the following requirement - Each application tier will require different replication attributes. vVol provides this. Also vSAN would be ruled out, because the impact of performance on Storage layer should not have impact on Compute layer."},{"comment_id":"425981","upvote_count":"2","content":"I read this: https://docs.vmware.com/en/vSphere-Replication/8.4/com.vmware.vsphere.replication-admin.doc/GUID-1FF815EB-80DC-401B-AD0E-0898255DE624.html and vSAN with vsphere replication has some limitations.\n\nI think to work with vsan is better think in a stretched cluster, but for this solution, i think A is better option.","poster":"JLF_VMW","timestamp":"1645046040.0"},{"poster":"primanturin","comment_id":"407678","comments":[{"content":"My initial thoughts were A as FC arrays with replication meet all requirements and better addresses \"The impact on the storage layer should not impact the performance of the compute layer.\" but after re-reading \"Each application tier will require different replication attributes.\" perhaps B with vSAN is a better choice.\n\nAlthough could argue that A better achieves \"Operational management overhead should be minimized\" - replication is achieved on the arrays there is no need to manage vSphere replication...are we talking about management overhead being minimized for storage admins or vSphere admins?","comments":[{"timestamp":"1642596660.0","upvote_count":"1","comments":[{"comment_id":"415130","timestamp":"1643264880.0","comments":[{"upvote_count":"2","comment_id":"481649","timestamp":"1652957220.0","poster":"cloud29","content":"The impact on the storage layer should not impact the performance of the compute layer\n\nvSAN can eat around 25-30% of your CPU so B is wrong."}],"poster":"hansel","content":"VMware vRealize Orchestrator Plug-In for vSphere Replication is able to meet the requirement of \"Operational automation should be supported.\"\n\nhttps://docs.vmware.com/en/vSphere-Replication/8.4/com.vmware.vsphere.replication-vro.plugin.doc/GUID-AF14DCEB-8153-4D31-B238-33E4819B678C.html\n\nB seems to be the best fit","upvote_count":"2"}],"poster":"primanturin","comment_id":"409460","content":"\"...designing storage for a new vSphere environment..\"\n\nI think they are asking about \"management overhead being minimized for storage admins\". I this case vSAN better than FC"}],"poster":"hansel","comment_id":"408285","upvote_count":"2","timestamp":"1642407960.0"}],"upvote_count":"2","content":"I am in doubt between A and B\n\n\"The impact on the storage layer should not impact the performance of the compute layer.\"\nWhy vSAN is better than FC in this case? This is actually a reason why to take FC over vSAN. For example, anytime a host goes down for any reason such as maintenance mode, etc, we will loss compute performances on our vSAN cluster.\n\n\"Operational management overhead should be minimized\" >> FC Additional management overhead (e.g. switch zoning) - In this case, vSAN is clearly better.\n\nBoth answers cover the rest of the requirements.\n\nSo... why do I believe the answer is B? Because it is a VMware exam and in case of doubt the right answer will be a full VMware solution. And probably because vSphere replication is a better way to keep everything working and running from both sides and with a more reduced downtime.\n\nDon't know, what do you think?","timestamp":"1642325760.0"},{"timestamp":"1638784620.0","comment_id":"375782","upvote_count":"4","content":"I suppose B","poster":"nemisis95"},{"timestamp":"1633573860.0","poster":"andy33","comment_id":"330013","upvote_count":"2","content":"I think A"},{"timestamp":"1633532820.0","poster":"rajeshrub","comment_id":"329732","upvote_count":"3","content":"It shall be B. A and C are ruled out. D can't be because vvol requires a certified storage array for replication. A, C ,D will have impact on compute performance due to some challenge on storage."},{"comment_id":"328547","upvote_count":"1","content":"Not sure...","timestamp":"1633428000.0","poster":"estornudo"},{"upvote_count":"2","comment_id":"328176","timestamp":"1633373040.0","content":"Shoul be B","poster":"amgice"}],"unix_timestamp":1617561840,"question_text":"An architect is designing storage for a new vSphere environment to meet the following requirements:\n✑ Asynchronous replication is required between two sites.\n✑ The impact on the storage layer should not impact the performance of the compute layer.\n✑ Each application tier will require different replication attributes.\n✑ Virtual machine live migration across compute and storage must be supported.\n✑ Virtual machine aware back up will be leveraged.\n✑ Operational management overhead should be minimized.\n✑ Operational automation should be supported.\nWhich storage design recommendations would meet the requirements?","answer":"D","answer_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/vmware/view/49080-exam-3v0-2121-topic-1-question-24-discussion/","choices":{"D":"Two new storage arrays will be deployed, one at either site. vSphere Volumes (vVOLs) will be used to provide policy-based management for each application tier. Data replication will be offloaded to the new arrays.","C":"Two new ISCSI storage arrays will be deployed, one at either site. Each application tier will be initially provisioned a new LUN. Data replication will be offloaded to the new arrays.","B":"Two new vSphere clusters enabled with vSAN will be deployed, one at either site. vSAN will be used to provide policy-based management for each application tier. vSphere Replication will be used to replicate the virtual machine data in an asynchronous configuration.","A":"Two new Fibre Channel storage arrays will be deployed, one at either site. Each application tier will be initially provisioned a new LUN. Data replication will be offloaded to the new arrays."},"answer_description":"","answers_community":["D (86%)","14%"],"isMC":true,"answer_ET":"D","question_images":[],"exam_id":253},{"id":"JbRvlhhYXzro5yGiGXeg","question_images":[],"topic":"1","isMC":true,"question_text":"An organization's data scientists are executing a plan to use machine learning (ML). They must have access to graphical processing unit (GPU) capabilities to execute their computational models when needed. The solutions architect needs to design a solution to ensure that GPUs can be shared by multiple virtual machines.\nWhich two solutions should the architect recommend to meet these requirements? (Choose two.)","choices":{"D":"vSGA","A":"NVIDIA vGPU","E":"vSphere Bitfusion","B":"AMD MxGPU","C":"vSphere DirectPath I/O"},"answer_ET":"AE","discussion":[{"upvote_count":"1","content":"Selected Answer: AE\nA and E is correct","timestamp":"1730024940.0","comment_id":"882395","poster":"chevreuil00"},{"comments":[{"comment_id":"703075","poster":"marlin008","upvote_count":"4","timestamp":"1713966780.0","content":"A&E Correct."}],"comment_id":"702198","timestamp":"1713878280.0","poster":"taku94","content":"For Grid vGPU, I think A is correct.\nAre D and E correct?","upvote_count":"1"},{"poster":"marlin008","content":"A&D\nonly two technology:\nhttps://blogs.vmware.com/performance/2020/01/vmware-vsga-for-content-rich-vdi.html","comments":[{"poster":"marlin008","comments":[{"content":"the two Key Point, \n1. it's for machine learning\n2. it's can be shared by VMs.","upvote_count":"1","timestamp":"1713966960.0","comment_id":"703080","poster":"marlin008"}],"upvote_count":"3","timestamp":"1713966720.0","comment_id":"703074","content":"\"ensure that GPUs can be shared by multiple virtual machines\" Change My Answer \nA&E"}],"timestamp":"1713794640.0","upvote_count":"2","comment_id":"701568"}],"answer_description":"Reference:\nhttps://blogs.vmware.com/apps/files/2019/08/5521-VMW-GPU-MACHINE-LEARNING-GUIDE-USLET-WEB-20190812.pdf","url":"https://www.examtopics.com/discussions/vmware/view/86202-exam-3v0-2121-topic-1-question-25-discussion/","timestamp":"2022-10-22 16:04:00","unix_timestamp":1666447440,"exam_id":253,"answer":"AE","answer_images":["https://www.examtopics.com/assets/media/exam-media/04348/0002100001.png"],"answers_community":["AE (100%)"],"question_id":18},{"id":"TAEQFLUoTlW3pU1ERcQv","answer_ET":"A","answer_description":"","choices":{"D":"Run both resource profiles on the same cluster with host hardware that has fast CPU, large amounts of memory, and the fastest storage platform.","A":"Separate the two resource profiles into two clusters. The Tier 1 cluster will have fast storage while the Tier 2 cluster will not.","C":"Separate the two resource profiles into two clusters. The Tier 2 cluster will have faster CPU and more memory while the Tier 1 cluster will have slower CPU and less memory but more disk space.","B":"Run both resource profiles on the same cluster with the same host hardware platform."},"question_id":19,"answers_community":["A (63%)","B (38%)"],"answer_images":[],"answer":"A","url":"https://www.examtopics.com/discussions/vmware/view/49084-exam-3v0-2121-topic-1-question-26-discussion/","discussion":[{"comment_id":"383903","poster":"nemisis95","content":"Changing my answer to B.\n\nBoth resource profiles DO NOT require high CPU or memory, only a different level of disk IOPS. We can use vSAN here to apply a storage policy to the the different tier VM's","upvote_count":"16","comments":[{"poster":"Aletzziss","timestamp":"1671839280.0","content":"good observation! that's the only difference","comment_id":"389097","upvote_count":"6"},{"timestamp":"1720509900.0","comment_id":"770195","content":"Ok but no one talks about usage of VSAN here. Technically both Tiers could run on same cluster in an average IT company. But if we focus what is desired in ideal world, separating tiers to different clusters would be the best approach in my opinion. So that`s why I`m still with \"A\".","upvote_count":"3","poster":"purulence"}],"timestamp":"1671258360.0"},{"upvote_count":"15","comment_id":"476177","timestamp":"1683798180.0","poster":"Mariosoftnet","content":"Answer is A.\n\nChecked in the training manual, chapter 10 Infrastructure Manageability:\n\nWhen mention about tiers... always talks about DIFFERENT CLUSTERS. Vmware suggest this in order to calculate spare hosts, cpu contention, disk performance, etc.\nIn this case B and D should be disregarded, you should not mix Tier 1 and 2 in the same cluster. \nA option meets requirements: faster storage","comments":[{"upvote_count":"1","comment_id":"522796","timestamp":"1689240360.0","poster":"Aolivera","content":"I agree. Just checked the same doc and I agree with your comment"}]},{"comment_id":"916036","upvote_count":"1","timestamp":"1733479680.0","poster":"safodz","content":"Selected Answer: B\nStorage array has storage tiering function that allow classification of of iops workload"},{"content":"The same storage can handle high IOPS and low IOPS so it is not question to have to sperate storage for each workload","upvote_count":"1","timestamp":"1733314860.0","poster":"safodz","comment_id":"914359"},{"content":"The answer is A","timestamp":"1728946680.0","upvote_count":"2","comment_id":"870517","poster":"MohamedZohair"},{"upvote_count":"2","poster":"Alchot","comment_id":"789108","content":"Selected Answer: B\nIOPS performance profile for each app tier are created in the storage and vSphere storage policies.\nThere is no requirement to say sharing the host hardware or array will affect storage performance.\nTherefore without more NFR option B.","timestamp":"1722019980.0"},{"content":"Selected Answer: A\nB = Same profile and same hardware. Did not mention about storage. \nC = More disk space but doesnt talk about IOPS. So requirement is not met.\nD = Fast CPU / Memory is not required for both Tier of VMs. Hence it is a expensive solution. \n\nHence answer is A. It fulfills the requirement and follows the best design practice of isolating VM Tiers.","upvote_count":"1","poster":"balabharath","comment_id":"763775","timestamp":"1719922620.0"},{"timestamp":"1717672500.0","poster":"purulence","upvote_count":"1","comment_id":"736831","content":"Selected Answer: A\n\"A\" looks like the best approach if there`s no budget constraints. But in a real life scenario, we all know that, \"B\" is what is being applied in most companies. I`m not sure from which perspective we should evaluate this question. I hope this question doesn`t appear in the exam as there`re two correct answers in my understanding. Please provide more concrete info on this one if you have."},{"comment_id":"704573","poster":"VCIX_Chris","content":"Selected Answer: A\nUsually you'd have one cluster and a storage device with differently performing storage tiers. \n\nI suppose it's A for this specific question. I also suppose that 75% of the test takers will choose the wrong answer here.","upvote_count":"2","timestamp":"1714127640.0"},{"upvote_count":"1","content":"Selected Answer: A\nAgree with Mariosoftnet.\nDoubting between A and B, so following best practices as there is no cost contraints.","comment_id":"528307","poster":"spam","timestamp":"1689839760.0"},{"comment_id":"422685","poster":"JLF_VMW","timestamp":"1676036340.0","upvote_count":"5","content":"I think the key to the answer is the word \"profile\", you can have different profiles in the same hardware. So, the answer is B."},{"timestamp":"1675937880.0","poster":"ertin74","upvote_count":"1","comment_id":"421997","content":"B Fast CPU and large memory aren't required."},{"timestamp":"1675580160.0","content":"My answer is D, \nRegardless of fast CPU and Memory. The requirement is the fastest storage, which is only mentioned in D.","comment_id":"420058","upvote_count":"1","poster":"cloudsinair"},{"comment_id":"403123","upvote_count":"1","timestamp":"1673333520.0","content":"We DON'T need host hardware that has fast CPU, large amounts of memory. That's the key difference between B and D.","comments":[{"upvote_count":"1","comment_id":"418256","poster":"cloudsinair","content":"D seems to be correct, as the some VMs are disk intensive while others are not. \nso we need fast disk storage with resource profiles.","timestamp":"1675262220.0"}],"poster":"nemisis95"},{"poster":"rajeshrub","upvote_count":"4","timestamp":"1665071040.0","content":"Might be D. Just create two resource pools and assign shares within pool.","comment_id":"329771","comments":[{"upvote_count":"1","timestamp":"1670321340.0","poster":"nemisis95","comment_id":"375792","content":"Agree with this."}]},{"comment_id":"328183","content":"i think is A","upvote_count":"3","poster":"amgice","timestamp":"1664909580.0"}],"question_images":[],"question_text":"An architect is designing a solution for an environment with two types of resource profiles that must be virtualized. The first type consists of Tier 1 virtual machines that are disk I/O intensive, but do NOT require high CPU or memory. The second type consists of Tier 2 virtual machines that require a lower CPU and memory allocation and have minimal disk I/O.\nWhich design recommendation should the architect make for distributing the resource profiles?","unix_timestamp":1617562380,"timestamp":"2021-04-04 20:53:00","topic":"1","isMC":true,"exam_id":253},{"id":"Gf0TGbmACfa1Ob2Ud9iQ","answers_community":["AE (80%)","AD (20%)"],"url":"https://www.examtopics.com/discussions/vmware/view/86267-exam-3v0-2121-topic-1-question-27-discussion/","answer_ET":"AE","exam_id":253,"timestamp":"2022-10-23 15:18:00","topic":"1","question_images":[],"unix_timestamp":1666531080,"answer":"AE","question_id":20,"isMC":true,"discussion":[{"poster":"Major88","upvote_count":"6","comment_id":"702197","comments":[{"upvote_count":"1","content":"This answer is VERY wrong. The KB refers to the general supportability of WSFC in VMware environments, which includes the possibility to configure it by using RDMs. However, the question states clearly that no FC Storage is available, therefore no RDM can ever be used. In other words, C is so wrong.\n\nvSAN, on the other hand, has supportability for WSFC both by sharing vmdk natively or by using iSCSI service. \n\nRight answer is A, E.","poster":"LFC1280","timestamp":"1709199120.0","comment_id":"994891"}],"timestamp":"1682255880.0","content":"A,C \nhttps://kb.vmware.com/s/article/79616\nNO SMB supported\nNo vSAN iSCSI Target\nNo NFS4"},{"content":"Selected Answer: AE\nA >> vSAN native fully supports Microsoft cluster techniques with shared disks on 6.7 U3 or higher version  https://kb.vmware.com/s/article/54461 \nB >> NFS does not support RDM, or clustered disks \nC >> Neither vSAN, NFS or SMB support RDM.\nD >> SMB 2.1 is not supported for sharing disks. WSFC can use only SMB 3.0 file shared, or attached shared storage  https://learn.microsoft.com/en-us/windows-server/failover-clustering/clustering-requirements\nE >> You cannot have a RDM for WSFC, but you can use the vSAN iSCSI Target Service to provide an iSCSI volume to the virtual machin guest OS : https://kb.vmware.com/s/article/54461","upvote_count":"1","poster":"FR_Wolfman","comment_id":"1128712","timestamp":"1721650140.0"},{"timestamp":"1704564780.0","content":"Selected Answer: AE\nWindows Server Failover Clusters (WSFC) using the vSAN iSCSI target service, fully transparent failover of LUNs is now possible with the iSCSI service for vSAN when used in conjunction with WSFC. This feature is incredibly powerful as it can protect against scenarios in which the host that is serving a LUN’s I/O fails. This failure might occur for any reason: power, hardware failure or link loss. In these scenarios, the I/O path will now transparently failover to another host with no impact to the application running in the WFSC","comment_id":"944821","poster":"andr3","upvote_count":"2"},{"content":"there is no RDM in the VSAN nor the Externals storage array (It support sonly NFS /SMB)","timestamp":"1701694080.0","upvote_count":"3","comment_id":"914414","poster":"safodz"},{"content":"Selected Answer: AE\nanswers are A and E \nconfirmed with KB https://kb.vmware.com/s/article/54461","timestamp":"1699823100.0","comment_id":"896191","upvote_count":"1","poster":"Alfil"},{"upvote_count":"1","content":"The answer is AE","comment_id":"870518","poster":"MohamedZohair","timestamp":"1697324340.0"},{"upvote_count":"1","timestamp":"1692690720.0","poster":"Alchot","comment_id":"817690","content":"Selected Answer: AD\nhttps://kb.vmware.com/s/article/79616\nvSAN (vSphere 7.0)\nClustered VMDKs\n vSAN\n\nVMware fully supports a configuration of WSFC using in-guest iSCSI initiators or in-guest SMB (Server Message Block) protocol, provided that all other configuration meets the documented and supported WSFC configuration. Using this configuration in VMware virtual machines is similar to using it in physical environments."},{"timestamp":"1690496220.0","upvote_count":"1","content":"A, C \n\nOnly these two answers make sense.\n\nStorage protocols:\n\n-> FC, FCoE and Native iSCSI are fully supported with pRDMs and vVols.\n-> Clustered VMDKs are supported with FC only.\n-> NFS is not a supported storage protocol to access a clustered disk resource for WSFC. NFS backend VMDKs can be used as non-shared disks (system disk, backup, etc.) without limitations.\n-> vSAN supports natively clustered VMDK in vSphere 7.x\n-> Virtual Volumes – supported. Check with your storage vendor if the implementation of vVol includes support for WSFC on VMware vSphere.\n\nhttps://kb.vmware.com/s/article/79616","comments":[{"upvote_count":"1","content":"Nether vSAN or NFS supports RDM disks. On vSAN not even on the iSCSI service.\nhttps://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.vsan-planning.doc/GUID-E9984893-DF64-49EC-B0DB-44F8C271BCFE.html","comment_id":"790685","timestamp":"1690549260.0","poster":"JailBreak"}],"poster":"diegof1","comment_id":"790104"},{"content":"Answer is A,E\nhttps://kb.vmware.com/s/article/54461","upvote_count":"3","timestamp":"1687929660.0","comment_id":"759490","poster":"Shrin"},{"poster":"unofficial_official","upvote_count":"1","content":"agreed with Major88.\n\nOnly Clustered VMDKs, vVols and RDMs allowed for Microsoft Windows Server Failover Cluster (WSFC) - https://kb.vmware.com/s/article/79616","comment_id":"728512","timestamp":"1685205060.0"}],"answer_images":[],"choices":{"B":"Use NFS 4.1 shares for quorum and shared disk","E":"Run WSFC on vSAN iSCSI Target Service","A":"Use vSAN native support for WSFC","D":"Use the SMB 2.1 protocol for sharing disks","C":"Use raw device mapping (RDM)"},"answer_description":"","question_text":"Application owners require support of a Microsoft Windows Server Failover Cluster (WSFC).\nTheir current environment consists of the following components:\n✑ vSphere 7.0 and vSAN 7.0\n✑ External array supporting NFS 3.0/4.1, Server Message Block (SMB) 2.1\n✑ 10 GbE storage connectivity for all devices\nThe solution architect is tasked with coming up with a solution to meet this requirement while utilizing their existing investments.\nWhich two recommendations could the architect make? (Choose two.)"}],"exam":{"isImplemented":true,"isMCOnly":false,"lastUpdated":"12 Apr 2025","numberOfQuestions":90,"name":"3V0-21.21","isBeta":false,"id":253,"provider":"Vmware"},"currentPage":4},"__N_SSP":true}