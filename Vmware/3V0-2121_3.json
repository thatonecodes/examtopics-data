{"pageProps":{"questions":[{"id":"OjQ6TQ39qaT0txil12zS","timestamp":"2021-04-03 16:56:00","answer_description":"","topic":"1","discussion":[{"upvote_count":"15","poster":"Aletzziss","timestamp":"1640297220.0","content":"B&C \nE is a recommendation but is an assumption to think they have budget\n\"Use jumbo frames for best vMotion performance\"\nhttps://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.vcenterhost.doc/GUID-7DAD15D4-7F41-4913-9F16-567289E22977.html","comment_id":"389076"},{"poster":"estornudo","upvote_count":"14","comments":[{"upvote_count":"4","content":"10GBE is not a requirement for vMotion.\n\nvMotion network has at least 250 Mbps of dedicated bandwidth per concurrent vMotion session.\nhttps://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.vcenterhost.doc/GUID-3B41119A-1276-404B-8BFB-A32409052449.html\n\nWe use C, to create an additional vSwitch with dedicated 1GB/ps network","poster":"nemisis95","comments":[{"upvote_count":"1","timestamp":"1682507880.0","comment_id":"704614","content":"Why an additional Switch for multi-nic-vmotion? An additional vmkernel would be sufficient.","poster":"VCIX_Chris"},{"upvote_count":"1","timestamp":"1665805260.0","poster":"EW3772","comment_id":"586144","content":"Refer to your link:\nYou MUST ENSURE that the vMotion network has at least 250 Mbps of dedicated bandwidth per concurrent vMotion session. Greater bandwidth lets migrations complete more quickly. \n\nit is you must provide vMotion network at least 250Mbps, not vMotion network has at least 250Mbps."},{"comment_id":"471710","upvote_count":"2","content":"but using 10gbe increases the concurrent migration to 8 from 4 which is also the client requirement - ''they want to maximize the number of concurrent virtual machine migrations''","timestamp":"1651501080.0","poster":"shav007"}],"timestamp":"1638781560.0","comment_id":"375718"}],"content":"I'd say B and E","timestamp":"1633272960.0","comment_id":"327456"},{"content":"Selected Answer: BE\nBE are the right answer.\nA >> Changing the MTU from 1500 to 1600 won't help in anything\nB >> Switch to jumbo frames (MTU 9000) is one of the recommendations to improve vMotion performances (it works with vSAN too)\nC >> Creating a new standard switch and moving the vMotion to it won't help in anything\nD >> Aggregating the uplinks won't help, the vMotion parallelization with stays stuck at 4 VMs\nE >> Switching to a 10Gb network will push the limit from 4 VMs to 8 VMs moved in parallel.","upvote_count":"1","timestamp":"1721646240.0","poster":"FR_Wolfman","comment_id":"1128642"},{"poster":"guille804","comment_id":"958789","content":"For me B&E.\nYa que C dice: Crear un adicional standar switch y la teoria dice que debe ser al mismo vss o vsd.\n\nvMotion con varias NIC:\nPuede configurar varias NIC para vMotion si agrega dos o más NIC al conmutador distribuido o estándar que se requiera. Para obtener información detallada, consulte el artículo de la base de conocimientos KB 2007467.","upvote_count":"1","timestamp":"1705867500.0"},{"content":"Selected Answer: BE\nso they are asking you for two recommendations: \n\n\"Which two recommendations should the architect make...\" \n\nrecommendation is always to use 10GB net over 1GB \n\nand Jumbo frames for vMotion\n\nANS: \nB\nE","poster":"andr3","comment_id":"946940","timestamp":"1704792000.0","upvote_count":"2"},{"timestamp":"1701792060.0","comment_id":"915480","upvote_count":"1","poster":"safodz","content":"Selected Answer: BC\nB & C of course ; E is out of subject her the host are configured with onle 01Gbe network"},{"poster":"Alchot","timestamp":"1690132620.0","upvote_count":"2","content":"Selected Answer: BE\nThere is no constrain saying customer does not have budget or must use exisiting hardware.\nAssumption are being made that should be cleared by asking the questions.\n\nTherefore 10GbE network and Jumbo frames can be recommended.\nDuring design workshop customer can confirm if network fabric speed, free ports and budget for cables, NICs.\n\nIf these then become constrains. LACP and Jumbo Frames can be recommended.\nNIOC should be also part of this question. IN MY OPINION","comment_id":"785751"},{"poster":"hicall","content":"Selected Answer: BE\nB and E","upvote_count":"1","timestamp":"1682853480.0","comment_id":"708429"},{"poster":"Akhi69","comment_id":"682823","content":"B&E\nC is talking about creating a new vSS which means that we cannot use multi-nic vmotion because the second switch will not see the nic used by the first vSS ? Am i wrong ?","upvote_count":"2","timestamp":"1680105660.0"},{"poster":"Bobob55","upvote_count":"1","content":"B and E. as that increases number of concurrent vmotions","timestamp":"1675851240.0","comment_id":"644017"},{"comment_id":"620870","content":"B&C, Best option on my opinion. We do not have info in exhibit about 10 GBE NICs.","upvote_count":"2","poster":"OlekPL","timestamp":"1671794700.0"},{"timestamp":"1666979520.0","content":"LACP negotiating mode of the LAG has one active and one passive nic as by default. \nHere is the need for recommendations to overcome the challenge, not to have idela config, so, having jumbo is the first recommended move and second, dedicate one other pnic to use vmotion capabilities, seems to B and C.","poster":"mvojka1990","upvote_count":"2","comment_id":"593975"},{"comment_id":"576705","timestamp":"1664352360.0","poster":"Megalodon","upvote_count":"2","content":"Selected Answer: BE\nQuestion states to maximize number of concurrent migrations, so 10 Gbs is best. C can also increase vmotion performance because of dedicated nic but no so much."},{"timestamp":"1658295540.0","comments":[{"upvote_count":"1","timestamp":"1658295600.0","content":"10gb ethernet uses a default MTU of 9000","poster":"spam","comments":[{"upvote_count":"2","timestamp":"1676570820.0","comment_id":"647753","content":"No. it depends on the vmkernel config","poster":"tuvituvi"}],"comment_id":"528227"}],"comment_id":"528226","upvote_count":"1","content":"Selected Answer: CE\nvMotion Best Practices\nThe following are best practices when using vMotion: \n• Consider using a 10GbE vMotion network. Using a 10GbE network in place of a 1GbE network for vMotion will result in significant improvements in vMotion performance. When using very large virtual machines (for example, 64GB or more), consider using multiple 10GbE network adaptors for vMotion to further improve vMotion performance.\n\nhttps://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/techpaper/vmware-vmotion-performance-vsphere5.pdf","poster":"spam"},{"content":"B & E. 10Gb maximizes the amount of bandwidth, as well as increases the number of potential simultaneous vmotions.","poster":"Garbage","upvote_count":"1","timestamp":"1655943000.0","comment_id":"507496"},{"comment_id":"499439","content":"C is not a valid option since it will be simply a replacement for current PNIC, be careful with the proposed solution which is a new standard switch . I would recommend Jumbo frame and aggregation","upvote_count":"1","poster":"anisss21","timestamp":"1654949100.0"},{"content":"Is there any way to scale the maximum number of migrations of virtual machines other than adding 10Gb NICs? Also, there is no condition on whether or not it can be used for the additional budget. Aren't the best answers that satisfy the conditions C,E?","comment_id":"487053","upvote_count":"1","timestamp":"1653526140.0","poster":"eric_jung"},{"comment_id":"486064","content":"vSphere vMotion:\nRecommended networking best practices:\n\n Use one dedicated GigE adapter for vMotion. Consider using a 10GbE vMotion network. Using a 10GbE network in place of a 1GbE network for vMotion will result in significant improvements in vMotion performance. When using very large virtual machines (for example, 64GB or more), consider using multiple 10GbE network adaptors for vMotion to further improve vMotion performance.","timestamp":"1653399540.0","poster":"Aung","upvote_count":"1"},{"content":"does jumbo frame maximize the number of concurrent virtual machine migrations? 10gbe increase the concurrent migration to 8 instead of 4","poster":"slooky","upvote_count":"1","comment_id":"465739","timestamp":"1650552300.0"},{"poster":"Helpinghanditexams","timestamp":"1646375280.0","comment_id":"438862","content":"Correct Answer : B & C","upvote_count":"1"},{"timestamp":"1642410000.0","upvote_count":"1","content":"Cant say that E is an appropriate recommendation to make based on the information provided. Would require more input and info from the customer before recommending this (although its a nice to have)\n\nGoing with B&C to maximize the number of concurrent VM migrations and reduce the time it takes to place the hosts into maintenance mode.","comment_id":"408301","poster":"hansel"},{"timestamp":"1642240440.0","upvote_count":"2","content":"For all who said \"Use 10 GbE NICs instead of 1 GbE\", consider the first point - \"Each ESXi host has approximately 200 virtual machines.\"\n\nWe have no idea how many ESXi hosts the customer has, if they have a budget to purchase 10GB NICS, the budget and resources to perform the work...BIG JOB!","comment_id":"406899","comments":[{"content":"Not only that, if you choose a 10gb nic, maybe you need to change the network switches.","timestamp":"1657441860.0","upvote_count":"1","poster":"jdonoso","comment_id":"520788"}],"poster":"nemisis95"},{"content":"B & C. Jumbo frames and additional switch for vMotion(pnic3).\nE is good but its additional cost and they need to procure them. For existing infra; B&C","comment_id":"373120","timestamp":"1638498660.0","poster":"Achary","upvote_count":"3","comments":[{"upvote_count":"1","comment_id":"375720","content":"Yes, E is nice, but vMotion network only needs at least 250 Mbps of dedicated bandwidth per concurrent vMotion session. As you said, C works (dedicated vSwitch)","timestamp":"1638781620.0","poster":"nemisis95"}]},{"comment_id":"364361","content":"B & C\n\nhttps://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.vcenterhost.doc/GUID-7DAD15D4-7F41-4913-9F16-567289E22977.html\n\nB. Configure the network to use MTU for the VMotion VMKernel to 9,000 bytes\nUse jumbo frames for best vMotion performance.\n\nC. Create an additional standard switch with pNIC3 to use for vMotion\nTo distribute and allocate more bandwidth to vMotion traffic across several physical NICs, use multiple-NIC vMotion.","upvote_count":"5","poster":"nemisis95","timestamp":"1637668740.0"},{"content":"It's C and E, because the architect should recommend a solution. So, for reduce time for vMotion operations Multiple-NIC vMotion is an option and to increase the concurrent vMotions operations you need a 10Gb NIC, this increase to 8 concurrent operations instead the 4 operations you have with 1 Gb NIC.","comment_id":"355556","poster":"aledevenanzi","upvote_count":"3","timestamp":"1636733760.0"},{"comments":[{"timestamp":"1656312840.0","comments":[{"upvote_count":"1","comment_id":"821654","poster":"Mo_Mustafa","content":"That's right. NIOC available only on Dist. Switch","timestamp":"1692974460.0"}],"comment_id":"510164","upvote_count":"3","content":"I think for D we may need a distributed switch while the diagram says its a standard switch.","poster":"harofe2121"}],"timestamp":"1635263760.0","comment_id":"343387","upvote_count":"7","content":"I think B and C\nA >> Doesn't use Jumbo frames, may have poor performance\nB >> Use Jumbo frames for best performance\nC >> Multiple-NIC vMotion\nD >> Will need the use of NIOC to limit the bandwidth for the difference network\nE >> Can be, but if we refer to the exhibit there is no 10 GbE NICs","poster":"CloudRRA"},{"upvote_count":"1","comments":[{"comment_id":"375713","comments":[{"upvote_count":"2","comment_id":"432239","poster":"moustahy","timestamp":"1645883400.0","content":"yes, B & C"}],"poster":"nemisis95","upvote_count":"1","timestamp":"1638781380.0","content":"I think you meant \"B & C\". A doesn't use Jumbo Frames."}],"content":"I would say A&C","timestamp":"1634021580.0","poster":"moustahy","comment_id":"333745"}],"question_text":"Refer to the exhibit.\nDuring a requirements gathering workshop, the customer shares the following about their existing ESXi host virtual networking infrastructure:\n//IMG//\n\nThe customer confirms that:\n✑ Each ESXi host has approximately 200 virtual machines.\n✑ They want to maximize the number of concurrent virtual machine migrations.\n✑ When placing a host in maintenance mode, it takes a long time to evacuate the virtual machines.\nWhich two recommendations should the architect make in order to help the customer overcome their challenge? (Choose two.)","url":"https://www.examtopics.com/discussions/vmware/view/48951-exam-3v0-2121-topic-1-question-19-discussion/","unix_timestamp":1617461760,"isMC":true,"choices":{"C":"Create an additional standard switch with pNIC3 to use for vMotion","A":"Configure the network to use MTU for the VMotion VMKernel to 1,600 bytes","E":"Use 10 GbE NICs instead of 1 GbE","B":"Configure the network to use MTU for the VMotion VMKernel to 9,000 bytes","D":"Use the 3 pNICs and bundle them in a link aggregation group (LAG) configuration"},"answer_ET":"BE","exam_id":253,"question_images":["https://www.examtopics.com/assets/media/exam-media/04348/0001600001.png"],"answers_community":["BE (80%)","10%","10%"],"answer_images":[],"answer":"BE","question_id":11},{"id":"oIemPVxmkzwnSwg9gdFx","answers_community":["B (73%)","A (27%)"],"question_text":"An architect will be taking over control of a former Linux server fleet and repurposing the hardware into a new vSphere cluster. The current environment is already connected to the network but the hosts do not have any local disks. Since the fleet hardware is uniform, the architect can use a single ESXi image. All hosts within the cluster have the same CPU and memory capacity.\nWhich ESXi deployment method should the architect use?","exam_id":253,"topic":"1","isMC":true,"question_images":[],"answer":"B","answer_images":[],"url":"https://www.examtopics.com/discussions/vmware/view/80276-exam-3v0-2121-topic-1-question-2-discussion/","question_id":12,"answer_ET":"B","unix_timestamp":1662367680,"answer_description":"","choices":{"D":"Stateful vSphere Auto Deploy","A":"Stateless cached vSphere Auto Deploy","B":"Stateless vSphere Auto Deploy","C":"Manual install of each ESXi host with an image from USB"},"timestamp":"2022-09-05 10:48:00","discussion":[{"poster":"FR_Wolfman","comment_id":"1128393","content":"Selected Answer: B\nThe host has no local disk, which is necessary for all other options.","timestamp":"1721626320.0","upvote_count":"1"},{"content":"Selected Answer: B\nnow I am thinking its B \n\nI\"n stateless casched\" the initial boot image and configuration are fetched from the Auto Deploy server and then cached on the ESXi host's local disk. \n\n\"Stateless, no local storage is used,\" the host configuration is abstracted into a Host Profile and applied when the host connects to the appropriate vCenter/Cluster","upvote_count":"2","timestamp":"1704665160.0","comment_id":"945973","poster":"andr3"},{"comments":[{"comment_id":"907560","upvote_count":"1","poster":"Wynning123","content":"A is correct,\nTwo deployment modes are available. \nStateless Caching and Stateful installs, this is a trick question.","timestamp":"1701033120.0"}],"comment_id":"907559","upvote_count":"1","timestamp":"1701032820.0","content":"B Unlike stateless ESXi hosts, stateless caching requires a dedicated boot device to be assigned to the host.","poster":"Wynning123"},{"timestamp":"1697323380.0","comment_id":"870508","upvote_count":"1","poster":"MohamedZohair","content":"The answer is B"},{"timestamp":"1695465060.0","upvote_count":"1","comment_id":"848189","content":"D, can be the correct answer?\nStateful vSphere Auto Deploy allows the architect to create a reference host with the necessary ESXi image and configuration, and then deploy this image and configuration to multiple hosts in the cluster. Since the hardware is uniform, a single ESXi image can be used for all hosts, and the stateful deployment method allows for quick and easy deployment without the need for manual installation on each host.","poster":"SoporteISS"},{"poster":"diegof1","content":"The Correct Answer is A\n\nLike @Alchot mentioned, Only Stateless caching or Stateful installs are possible options for AutoDeploy\n\n\"You can use the System Cache Configuration host profile to provision hosts with vSphere Auto Deploy stateless caching and stateful installs.\"\n\nTaken from: https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.esxi.install.doc/GUID-C6FBA22C-048A-455F-9D45-58834445ACA5.html","upvote_count":"1","timestamp":"1690311240.0","comment_id":"788130"},{"poster":"diegof1","upvote_count":"1","comment_id":"788129","content":"Selected Answer: B\n\nLike @Alchot mentioned, Only Stateless caching or Stateful installs are possible options for AutoDeploy\n\n\"You can use the System Cache Configuration host profile to provision hosts with vSphere Auto Deploy stateless caching and stateful installs.\"\n\nTaken from: https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.esxi.install.doc/GUID-C6FBA22C-048A-455F-9D45-58834445ACA5.html","timestamp":"1690311180.0"},{"comment_id":"777188","upvote_count":"2","content":"Selected Answer: A\nThe options needs corrections.\nOnly Stateless caching or Stateful installs are possible options for AutoDeploy.\n\nFrom VMware ESXi Installation and Setup page 111\n\nStateless caching\nBy default, Auto Deploy does not store ESXi configuration or state on the host disk. Instead, an image profile defines the image that the host is provisioned with, and other host attributes are managed through host profiles. A host that uses Auto Deploy for stateless caching still needs to connect to the Auto Deploy server and the vCenter Server.\n\nStateful installs\nYou can provision a host with Auto Deploy and set up the host to store the image to disk. On subsequent boots, the host boots from disk.","timestamp":"1689458160.0","poster":"Alchot"},{"upvote_count":"1","timestamp":"1688985600.0","comment_id":"771411","poster":"purulence","content":"Selected Answer: B\nI`m closer to \"stateless\" but I think this question doesn`t have a very meaningful answer as both \"stateless\" and \"cached stateless\" modes are possible.\n\nAuto Deploy modes\nHaving completed the Auto Deploy installation procedure, let's walk-through the different modes you can use to configure vSphere Auto Deploy. There are three possible installation types you can use:\n\nStateless: The ESXi image is not technically installed but it is loaded directly into the host's memory as it boots.\n\nStateless caching: The image is cached on the local disk, remote disk, or USB. If the Auto Deploy server is not available, the host boots from the local cache.\n\nStateful: The image is cached on the local disk, remote disk, or USB. As compared to stateless caching, the boot order is inverted; the host boots first from local disk then from the network.\n\nhttps://www.oreilly.com/library/view/mastering-vmware-vsphere/9781787286016/28754c90-98d5-4f29-9606-1f1c2e3e1b11.xhtml"},{"comment_id":"742567","content":"Selected Answer: A\nA sounds right. The image cache can be on a remote disk\n\n\nHosts provisioned with vSphere Auto Deploy cache the image (stateless caching)\nSet up and apply a host profile for stateless caching. You can cache the image on a local disk, a remote disk, or a USB drive. Continue provisioning this host with vSphere Auto Deploy. If the vSphere Auto Deploy server becomes unavailable, for example because hundreds of hosts attempt to access it simultaneously, the host boots from the cache. The host attempts to reach the vSphere Auto Deploy server after the boot operation to complete configuration.","upvote_count":"1","poster":"Alchot","timestamp":"1686552720.0"},{"content":"Selected Answer: B\nBoth Stateless caching and Stateful Auto Deploy options store ESXi configuration or state on the host disk. Since no local disk available, Stateless Auto Deploy is the best option...","upvote_count":"4","poster":"NayanajithS","comment_id":"725230","timestamp":"1684848720.0"},{"upvote_count":"1","content":"Definately B, as no disks: https://www.oreilly.com/library/view/mastering-vmware-vsphere/9781787286016/28754c90-98d5-4f29-9606-1f1c2e3e1b11.xhtml","poster":"BarkingAardvark","comment_id":"706474","timestamp":"1682686800.0"},{"upvote_count":"1","timestamp":"1682502960.0","comment_id":"704555","content":"stateless is always with caching on Local Disk or USB Disk. i think the answer is C","poster":"Sirouszad"},{"upvote_count":"1","poster":"Testyboy15","comment_id":"688469","content":"Answer is wrong. Should be B. No local disk","timestamp":"1680856920.0"},{"upvote_count":"2","poster":"VCIX","comment_id":"659949","timestamp":"1678013280.0","content":"B as no local disks"}]},{"id":"Nlezm7W7MdyXdXSHMHrJ","isMC":true,"url":"https://www.examtopics.com/discussions/vmware/view/48953-exam-3v0-2121-topic-1-question-20-discussion/","answer":"CD","timestamp":"2021-04-03 17:11:00","answer_description":"","question_images":[],"exam_id":253,"topic":"1","unix_timestamp":1617462660,"choices":{"B":"The ESXi hosts will leverage NFS 3","E":"The ESXi hosts will leverage NFS 4.1","C":"The ESXi hosts will leverage Fibre Channel over Ethernet (FCoE)","A":"The ESXi hosts will leverage Fibre Channel (FC)","D":"The ESXi hosts will leverage iSCSI"},"question_id":13,"discussion":[{"timestamp":"1633273860.0","comment_id":"327460","poster":"estornudo","content":"I'd say B and D: NFS 3 and iSCSI\n\nFC is not IP\n\nNFS4.1 does not support I/O control\nhttps://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.storage.doc/GUID-8A929FE4-1207-4CC5-A086-7016D73C328F.html\n\nStarting from vSphere 7.0, VMware no longer supports software FCoE in production environments.\nhttps://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.storage.doc/GUID-6B49866F-7005-4099-84AC-4FB2A1A91F64.html","upvote_count":"25"},{"comment_id":"460614","timestamp":"1649682720.0","upvote_count":"14","content":"Must be C and D.\nFC is not IP\nNFS does not support RDMs (direct access to LUNs)\nOnly Software FCoE is not support anymore. Not FCoE in general","poster":"Tachinsky","comments":[{"comments":[{"upvote_count":"2","timestamp":"1682953380.0","comment_id":"709303","content":"\"existing storage array that supports both block and file storage\"\nIt's the existing array who supports block and file.\nThere is no prerequisite concerning the solution on that point.","poster":"Geoquake"}],"poster":"Tachinsky","comment_id":"475388","timestamp":"1652178240.0","content":"I need to correct my given answer:\nFollowing the requirement: the solution must support block and file storage \nThe answer must be B NFS 3 for file and D iSCSI for Block.","upvote_count":"5"}]},{"timestamp":"1721647380.0","comment_id":"1128670","poster":"FR_Wolfman","content":"Selected Answer: CD\nAnswers are C & D, which are the only possible solutions for using RDM on an IPv4 network\nA >> FC does not use the IPv4 network infrastructure, it needs a SAN\nB >> No RDM on NFS 3\nC >> FCoE almost the same things as FC, but through IPv4 network (and the proper hardware of course)\nD >> Of course, you can use RDM on iSCSI (Demo here : https://www.starwindsoftware.com/blog/vmware/rdm-disks-vmware-vsphere-vms-create/)\nE >> No RDM on NFS 4.1.","upvote_count":"1"},{"comment_id":"1126787","timestamp":"1721396820.0","poster":"HIC3540","content":"RDM is not supported by NFS, and that knocks both versions out. FC violates Req 5. That leaves us with 2 answers verified here: KB / Table 2. \nhttps://kb.vmware.com/s/article/79616","upvote_count":"2","comments":[{"comment_id":"1127615","upvote_count":"1","timestamp":"1721527800.0","content":"I agree C & D are the correct answers","poster":"Coop99"}]},{"content":"The answer is BD\nNFS 3 and iSCSI","timestamp":"1717976160.0","upvote_count":"1","poster":"hamadakota","comment_id":"1092230"},{"content":"Selected Answer: BD\nFCoE not supported","upvote_count":"3","comment_id":"915482","poster":"safodz","timestamp":"1701792240.0"},{"comment_id":"870515","upvote_count":"2","poster":"MohamedZohair","timestamp":"1697324100.0","content":"The answer is BD"},{"timestamp":"1693122240.0","poster":"Alchot","upvote_count":"1","comment_id":"823496","content":"Selected Answer: CD\nThe storage supports file and block but does not say file storage type must be used.\nISCSI and FCOE meets all FR"},{"upvote_count":"4","poster":"diegof1","timestamp":"1690759200.0","comment_id":"793523","content":"Selected Answer: BD\nA. FC does not use IP\n\nC. Starting from vSphere 7.0, VMware no longer supports software FCoE in production environments.\n\nhttps://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.storage.doc/GUID-6B49866F-7005-4099-84AC-4FB2A1A91F64.html\n\n\nE. NFS 4.1 does not support Storage IO Control\n\nhttps://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.storage.doc/GUID-8A929FE4-1207-4CC5-A086-7016D73C328F.html"},{"timestamp":"1690759140.0","poster":"diegof1","upvote_count":"1","comment_id":"793522","content":"Selected Answer: BD\nA. FC does not use IP\n\nC. Starting from vSphere 7.0, VMware no longer supports software FCoE in production environments.\n\nhttps://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.storage.doc/GUID-6B49866F-7005-4099-84AC-4FB2A1A91F64.html\n\n\nE. NFS 4.1 does not support Storage IO Control\n\nhttps://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.storage.doc/GUID-8A929FE4-1207-4CC5-A086-7016D73C328F.html"},{"upvote_count":"1","content":"The answer is always NFS and iSCSI. There are many Storage systems that support both, so only one system will be needed here. \n\nThe question is NFS v3 or NFS v4.1, I will vote E for v4.1","timestamp":"1689403860.0","comment_id":"776291","poster":"JailBreak"},{"content":"Selected Answer: CD\nI`m convinced with the explanations regarding RDM is not supported by NFS. ISCSI, FCOE","poster":"purulence","upvote_count":"1","timestamp":"1688885700.0","comment_id":"770173"},{"upvote_count":"1","comment_id":"743682","content":"Selected Answer: CD\nC & D, no other support RDM and use existing IPv4","poster":"MasterHow","timestamp":"1686632280.0"},{"upvote_count":"4","comment_id":"704608","comments":[{"poster":"JailBreak","timestamp":"1689403620.0","comment_id":"776286","upvote_count":"1","content":"Sorry but \"End of Availability and End of Support for Fibre Channel over Ethernet (FCoE) on Intel Network Controllers from vSphere 6.5 and later\" \n\nAnd where is the File Level solution?"}],"poster":"VCIX_Chris","content":"Selected Answer: CD\nC and D obviously. \n\n\"The solution must support the ability to connect virtual machines directly to LUNs.\" - this rules out non-block storage. \n\n\"The solution should use existing IPv4 based network infrastructure.\" - this rules out FC.\n\nLeft are iSCSI and FCOE, both is IPv4-based block storage.","timestamp":"1682507520.0"},{"poster":"Bobob55","timestamp":"1676035500.0","content":"Selected Answer: BD\nBlock and file so nfs and iscsi","upvote_count":"1","comment_id":"644934"},{"poster":"c11","content":"Selected Answer: CD\nC,D for sure\n\nFC is not over IP \nNFS doesn't support RDM\nFCoE is FC over Ethernet (IP network)\niSCSI is over IP network.","timestamp":"1669243380.0","upvote_count":"5","comment_id":"606308"},{"content":"Selected Answer: BD\nA: FC is not IP\nC: Since vSphere 7 no support for FCoE in production\nE: NFS 4.1 does not support SIOC","comment_id":"576713","timestamp":"1664353380.0","poster":"Megalodon","upvote_count":"4"},{"poster":"komodo08","timestamp":"1657025940.0","upvote_count":"9","comment_id":"517585","content":"Selected Answer: CD\nThe most important REQs are:\n- The ability to use LUNs as RDM. It excludes NFS.\n- IPv4 must be used. This excludes FC\nSo FCoE and iSCSI are the best answer.\n\nCheck Table 2 on\nhttps://kb.vmware.com/s/article/79616"},{"comment_id":"510293","poster":"Mezze","timestamp":"1656326580.0","content":"B&D in my opinion","upvote_count":"2"},{"poster":"Aolivera","comment_id":"479722","comments":[{"upvote_count":"2","content":"sorry i seems it is not supported on nfs 4.1, the other link should indicate the difference, https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.storage.doc/GUID-8A929FE4-1207-4CC5-A086-7016D73C328F.html","poster":"Aolivera","comment_id":"479725","timestamp":"1652739420.0"}],"content":"why NFS 3 over FS 4.1? there is no reqs for storage IO control for version 4.1 as per https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.resmgmt.doc/GUID-37CC0E44-7BC7-479C-81DC-FFFC21C1C4E3.html","timestamp":"1652739240.0","upvote_count":"1"},{"comment_id":"451201","poster":"zonder","timestamp":"1648187340.0","upvote_count":"2","content":"The solution must support the ability to connect virtual machines directly to LUNs.\nWhich means RDMs.\nNFS doen't support RDMs"},{"poster":"nemisis95","comments":[{"poster":"vmware_official","upvote_count":"2","timestamp":"1655455200.0","content":"This time you're wrong! RDM does not support NFS.","comments":[{"timestamp":"1665736680.0","upvote_count":"1","content":"you're missing this: ✑ The solution must use investments in existing storage array that supports both block and file storage.\n\nSo I'd say iSCSi and NFS","comment_id":"585623","poster":"Bojack0","comments":[{"timestamp":"1684068780.0","content":"Yes the Array supports both file and block but there are no requirements to use the file functionaility. By using iSCSI or FCoE it is using the existing investment (just not all of it)","comment_id":"718046","poster":"Testyboy15","upvote_count":"1"}]}],"comment_id":"503580"}],"upvote_count":"2","content":"B (NFS 3) & D (iSCSI)","timestamp":"1637670180.0","comment_id":"364374"},{"timestamp":"1633331520.0","poster":"rajeshrub","comment_id":"327832","content":"Shall be B & D.","upvote_count":"1"}],"answers_community":["CD (63%)","BD (37%)"],"answer_images":[],"question_text":"An architect is designing a VMware solution for a customer based on the following information:\n✑ The solution must use investments in existing storage array that supports both block and file storage.\n✑ The solution must support the ability to migrate workloads between hosts within a cluster.\n✑ The solution must support resource management priorities.\n✑ The solution must support the ability to connect virtual machines directly to LUNs.\n✑ The solution should use existing IPv4 based network infrastructure.\n✑ There is no budget for additional physical hardware.\nWhich two design decisions could the architect make to meet these requirements? (Choose two.)","answer_ET":"CD"},{"id":"5XNLzYsC5qM62p3Hp58N","question_text":"A Cloud Service Provider wants to introduce backup as a service for a customer's vSphere-based virtual machines.\nThe following information is noted:\n✑ They have a single four-port (2 ֳ— 10 GbE and 2 ֳ— 1 GbE) NIC per ESXi host\n✑ All top-of-rack (ToR) switches are 10 GbE and fully populated\n✑ The backup traffic must not impact existing services\nWhich two recommendations should the architect make to help the customer incorporate the service? (Choose two.)","timestamp":"2021-04-05 11:55:00","exam_id":253,"discussion":[{"comments":[{"timestamp":"1656327000.0","content":"Best choice A&D.","poster":"Mezze","comment_id":"510299","upvote_count":"3"}],"content":"OK, changing my answer AGAIN after reading\n\nA & D - Best to use\n\nhttps://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.networking.doc/GUID-84207BEA-986E-443E-94B0-B1EC66C56598.html\n\nAssign priority tags to traffic, such as VoIP and streaming video, that has higher networking requirements for bandwidth, low latency, and so on. You can mark the traffic with a CoS tag in Layer 2 of the network protocol stack or with a DSCP tag in Layer 3.","poster":"nemisis95","timestamp":"1641794160.0","upvote_count":"17","comment_id":"403095"},{"comment_id":"437063","timestamp":"1646138940.0","upvote_count":"5","content":"AD\n\n- Assign priority tags to traffic, such as VoIP and streaming video, that has higher networking requirements for bandwidth, low latency, and so on. You can mark the traffic with a CoS tag in Layer 2 of the network protocol stack or with a DSCP tag in Layer 3.\n\n- Priority tagging is a mechanism to mark traffic that has higher QoS demands. In this way, the network can recognize different classes of traffic. The network devices can handle the traffic from each class according to its priority and requirements.\n\n- You can also re-tag traffic to either raise or lower the importance of the flow. By using a low QoS tag, you can restrict data tagged in a guest operating system.\n\nhttps://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.networking.doc/GUID-84207BEA-986E-443E-94B0-B1EC66C56598.html","poster":"primanturin"},{"content":"Selected Answer: AD\nB >> Adding a new NIC in hosts won’t help, as the TOR switches are full\nC >> The TOR switches are 10 Gbps. Replacing the NICs with 25Gbps adapters won’t help.\nE >> Same problem as in B : no more place on the TOR switches","poster":"FR_Wolfman","upvote_count":"2","timestamp":"1721647500.0","comment_id":"1128674"},{"upvote_count":"1","timestamp":"1690489380.0","content":"AD\n\nA. Enable and tag traffic on the backup distributed port group (is the way to optimize the network infrastructure)\nB. Add a new two-port 10 GbE NIC per ESXi host (there is no free ports in the TOR to connect them)\nC. Replace the existing NIC with a two-port 25 GbE NIC per ESXi host (Port in the TOR are 10 GbE)\nD. Match the Class of Service (CoS) and Differentiated Services Code Point (DSCP) values to the physical network\nE. Create a new virtual switch using the 1 GbE uplinks (There is no free ports in TOR to connect it)\n\nSo A and D are the only answers that make sense.","comment_id":"790037","poster":"diegof1"},{"upvote_count":"2","poster":"Alchot","content":"Selected Answer: AD\nOnly AD makes sense","timestamp":"1690389540.0","comment_id":"789005"},{"upvote_count":"4","timestamp":"1662704100.0","content":"Selected Answer: AD\nB- false = TOR is fully populated\nC- false= we cannot change a 4 port 10GB with a 2 port 25Gb this is will impact network config\nE- false","poster":"krimie","comment_id":"563853"},{"upvote_count":"3","comment_id":"448664","content":"This is about elimination of answers..\nB is not because we cant add ports. We dont have available ports on the ToR switches\nC. same\nE. same. We dont have 1GbE switch at all","timestamp":"1647846960.0","poster":"bolekomp"},{"upvote_count":"4","timestamp":"1642409460.0","poster":"hansel","comment_id":"408295","content":"Question is poorly worded.\n\"All top-of-rack (ToR) switches are 10 GbE and fully populated\" so are we assuming the 2 x 1GbE NICs mentioned are connected to these via backwards compatibility with 10G copper? Or that they can't be used at all(constraint)?\n\nAs this is ambiguous im ruling out E and going with A&D"},{"poster":"nemisis95","upvote_count":"1","timestamp":"1641793320.0","content":"I think it's probably D&E\n\nD. Match the Class of Service (CoS) and Differentiated Services Code Point (DSCP) values to the physical network\nClass of Service (CoS) or Quality of Service (QoS) is a way to manage multiple traffic profiles over a network by giving certain types of traffic priority over others. For example you can give Voice traffic priority over email or http traffic. CoS is offered by service providers normally within an MPLS (Multi Protocol Label Switching) offering.\n\nDifferentiated Services Code Point (DSCP) is a means of classifying and managing network traffic and of providing quality of service (QoS) in modern Layer 3 IP networks. It uses the 6-bit Differentiated Services (DS) field in the IP header for the purpose of packet classification.\n\nE. Create a new virtual switch using the 1 GbE uplinks\nDedicate the 2x1GB NICS on the new vSwitch","comment_id":"403088"},{"content":"A&E.\n\nA. Enable and tag traffic on the backup distributed port group\nYES\n\nB. Add a new two-port 10 GbE NIC per ESXi host\nThey already have hardware.\n\nC. Replace the existing NIC with a two-port 25 GbE NIC per ESXi host\nThey already have hardware.\n\nD. Match the Class of Service (CoS) and Differentiated Services Code Point (DSCP) values to the physical network\nNO\n\nE. Create a new virtual switch using the 1 GbE uplinks\nYES","timestamp":"1637751780.0","comment_id":"365446","poster":"nemisis95","upvote_count":"3"},{"comments":[{"poster":"moustahy","comment_id":"466459","upvote_count":"1","content":"Change to A,D","timestamp":"1650698520.0"}],"comment_id":"355824","poster":"moustahy","upvote_count":"1","timestamp":"1636764480.0","content":"I think A & E"},{"upvote_count":"4","timestamp":"1633483440.0","comment_id":"329179","content":"I would say A and D??","poster":"andy33"},{"content":"I don't get it. The ToR are fully populated","comments":[{"comment_id":"383888","timestamp":"1639720200.0","poster":"nemisis95","upvote_count":"1","content":"Yes so we should assume the 2 x 10 GbE and 2 x 1 GbE NIC's are plugged into those TOR's already. Hence we can use the 1GbE NIC's on an independent new Virtual Switch."}],"upvote_count":"1","timestamp":"1633427700.0","poster":"estornudo","comment_id":"328543"}],"topic":"1","answer_ET":"AD","question_id":14,"answer_description":"","isMC":true,"unix_timestamp":1617616500,"answer_images":[],"answers_community":["AD (100%)"],"url":"https://www.examtopics.com/discussions/vmware/view/49164-exam-3v0-2121-topic-1-question-21-discussion/","answer":"AD","question_images":[],"choices":{"E":"Create a new virtual switch using the 1 GbE uplinks","C":"Replace the existing NIC with a two-port 25 GbE NIC per ESXi host","B":"Add a new two-port 10 GbE NIC per ESXi host","D":"Match the Class of Service (CoS) and Differentiated Services Code Point (DSCP) values to the physical network","A":"Enable and tag traffic on the backup distributed port group"}},{"id":"NY2XeFWZ8DNmP8CsF5oB","question_images":[],"unix_timestamp":1617463140,"answer_description":"","isMC":true,"answer_images":[],"discussion":[{"comment_id":"343393","upvote_count":"27","content":"A for sure!\n2 Hosts to hold the running VM at full capacity\n2 As resiliency N+2\n2 In maintenance mode","timestamp":"1635265140.0","poster":"CloudRRA"},{"content":"Isn't it A= 2 ?\n2 with VMs \n+\n 2 In maintenance mode \n+ \n2 as resiliency (N+2)","comment_id":"327462","upvote_count":"12","timestamp":"1633274340.0","poster":"estornudo"},{"poster":"FR_Wolfman","content":"Selected Answer: A\nAnswer is A of course.","timestamp":"1721647620.0","comment_id":"1128675","upvote_count":"1"},{"timestamp":"1714887720.0","upvote_count":"1","content":"Selected Answer: C\nIT is C. It states N+2 resiliency as a requirement: 2 hosts for the running load + 2 more. There will remain two hosts: from which one can be put in maintenance mode. while the other host remains available for a failover scenario (because the condition is to have N+2 all the time).","comments":[{"content":"Why can't the customer put both remaining hosts in maintenance mode? The requirement is N+2. They need 2 hosts to run the workload, 2 for failover, that leaves 2 hosts with spare capacity. Imagine customer puts both hosts in maintenance mode, and loses the two production hosts it still leaves 2 hosts to run all the workload. Hence A is the right answer.","comment_id":"1127616","timestamp":"1721528100.0","upvote_count":"1","poster":"Coop99"}],"comment_id":"1062707","poster":"Ady84"},{"comment_id":"753391","upvote_count":"2","poster":"mazenkur","content":"A. 6 - 2 = 4\nN+2=4\nN=4-2\nN=2\neasy math","timestamp":"1687437660.0"},{"poster":"bpexam","content":"Selected Answer: A\nA is correct","upvote_count":"5","comment_id":"634155","timestamp":"1674242640.0"},{"timestamp":"1664792700.0","comment_id":"580208","content":"Selected Answer: A\nA is correct","upvote_count":"4","poster":"leoclyen"},{"comment_id":"563860","comments":[{"timestamp":"1664290980.0","comment_id":"576270","upvote_count":"2","content":"Correcting my answer to A= 2 hosts","poster":"krimie"}],"timestamp":"1662704640.0","content":"Selected Answer: D\nwell guys this is a tricky question, I think when he said \"at the same time\" it matter here,\nwe've got : 2Host for VM load, when will put 2 host in the same Time in MM this n+1==> which Mean for n+2 it will Left None","upvote_count":"2","poster":"krimie"},{"content":"Not sure why all answered are incorrect in this dump.","comments":[{"poster":"JailBreak","upvote_count":"1","content":"To make discussions about each question and answer. That is my view.","timestamp":"1690547580.0","comment_id":"790663"}],"poster":"vinny_Md","upvote_count":"5","timestamp":"1653536280.0","comment_id":"487093"},{"poster":"treadw","timestamp":"1652709900.0","comment_id":"479479","upvote_count":"1","content":"Do they mean the 2 hosts are at full capacity with no room, or do they mean they still provide HA (n+1) ? If the latter, then it would be 3 hosts - 1 to run the VMs and 2 for the N+2, which leaves 3 hosts able to put in MM."},{"timestamp":"1637752800.0","comment_id":"365458","poster":"nemisis95","content":"Answer is A (2)","upvote_count":"5"},{"timestamp":"1633331820.0","upvote_count":"7","comment_id":"327836","content":"Shall be A","poster":"rajeshrub"}],"question_text":"A customer has six hosts available in a cluster. When running at full capacity, all virtual machines can be run on two hosts.\nHow many hosts can the customer place into maintenance mode at the same time while still providing N+2 resiliency to the cluster?","question_id":15,"url":"https://www.examtopics.com/discussions/vmware/view/48954-exam-3v0-2121-topic-1-question-22-discussion/","topic":"1","exam_id":253,"answer":"A","answers_community":["A (77%)","D (15%)","8%"],"timestamp":"2021-04-03 17:19:00","answer_ET":"A","choices":{"C":"One","A":"Two","D":"None","B":"Three"}}],"exam":{"lastUpdated":"12 Apr 2025","name":"3V0-21.21","isBeta":false,"isMCOnly":false,"provider":"Vmware","isImplemented":true,"id":253,"numberOfQuestions":90},"currentPage":3},"__N_SSP":true}