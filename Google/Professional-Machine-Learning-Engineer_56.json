{"pageProps":{"questions":[{"id":"18XvodOn4b7g6yhuIS5B","topic":"1","answer":"A","timestamp":"2022-12-11 13:18:00","answers_community":["A (60%)","C (36%)","3%"],"answer_images":[],"answer_ET":"A","exam_id":13,"unix_timestamp":1670761080,"choices":{"A":"Use then TFX ModelValidator tools to specify performance metrics for production readiness.","B":"Use k-fold cross-validation as a validation strategy to ensure that your model is ready for production.","D":"Use the entire dataset and treat the area under the receiver operating characteristics curve (AUC ROC) as the main metric.","C":"Use the last relevant week of data as a validation set to ensure that your model is performing accurately on current data."},"question_images":[],"question_id":276,"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/91004-exam-professional-machine-learning-engineer-topic-1-question/","discussion":[{"poster":"John_Pongthorn","timestamp":"1690290060.0","comment_id":"787766","upvote_count":"14","content":"https://www.tensorflow.org/tfx/guide/evaluator"},{"comment_id":"748749","content":"Selected Answer: C\nit's seem C for me\nB is wrong cuz \"Many machine learning techniques don’t work well here due to the sequential nature and temporal correlation of time series. For example, k-fold cross validation can cause data leakage; models need to be retrained to generate new forecasts\" \n- https://cloud.google.com/learn/what-is-time-series","poster":"hiromi","upvote_count":"9","timestamp":"1687077780.0"},{"poster":"PhilipKoku","timestamp":"1733566800.0","content":"Selected Answer: A\nA) TFX ModelValidator is designed to handle the exact needs described in the scenario: training on all data, validating on specific subsets, and ensuring production readiness with comprehensive performance metrics.\nThis makes it the most streamlined and reliable method compared to other options, which either lack specificity in production readiness (B), are too narrow in scope (C), or risk overfitting and inadequate validation (D).","upvote_count":"3","comment_id":"1225983"},{"poster":"gscharly","comment_id":"1199010","content":"Selected Answer: A\nEvaluator TFX lets you evaluate the performance on different subsets of data https://www.tensorflow.org/tfx/guide/evaluator","timestamp":"1729411860.0","upvote_count":"3"},{"upvote_count":"3","poster":"pinimichele01","timestamp":"1728930360.0","comment_id":"1195644","content":"Selected Answer: A\nThe Evaluator TFX pipeline component performs deep analysis on the training results for your models, to help you understand how your model performs on subsets of your data."},{"upvote_count":"4","timestamp":"1725543960.0","comment_id":"1166584","content":"Selected Answer: A\nI prefer A to C because 1 week of data may be insufficient to generalize the model and could lead to overfitting on the validation subset.","poster":"edoo"},{"poster":"pmle_nintendo","comment_id":"1161728","upvote_count":"1","content":"Selected Answer: C\noption C provides a streamlined and reliable approach that focuses on evaluating the model's performance on the most relevant and recent data, which is essential for predicting out-of-stock events in a dynamic retail setting.","timestamp":"1724850720.0"},{"timestamp":"1715834040.0","poster":"Mickey321","content":"Selected Answer: A\nEither A or C but C is only last week which is not specific data sets","upvote_count":"2","comment_id":"1072207"},{"poster":"AdiML","timestamp":"1711115460.0","upvote_count":"1","content":"Answer should be C, we are dealing with dynamic data and the \"last\" data is more relevant to have an idea about the future performance","comment_id":"1014061"},{"comment_id":"1011633","upvote_count":"1","timestamp":"1710881580.0","content":"Selected Answer: C\nOption C, because it allows you to track your model's performance on the most *recent* data, which is the most relevant data for predicting stockout risk. Given that the preferences are dynamic, the most important thing is that the model WORKS correctly with the newest data","poster":"joaquinmenendez"},{"poster":"[Removed]","comments":[{"upvote_count":"2","poster":"tavva_prudhvi","content":"The ModelValidator TFX Pipeline Component (Deprecated)","timestamp":"1706993340.0","comment_id":"971395"}],"content":"Selected Answer: A\nThe answer is A. Performance on specific subsets of data before pushing to production == TFX ModelValidator with custom performance metrics for production readiness.\n\nC is wrong because performance in the last relevant week of data != performance on specific subsets of data.","upvote_count":"2","comment_id":"960301","timestamp":"1706010240.0"},{"poster":"atlas_lyon","content":"Selected Answer: A\nI will go for A. I don't think the aim of the question is to test if the candidates know whether or not a component is deprecated . Note that ModelValidator has been fused with Evaluator. So we can imagine, the question would have been updated in recent exams. Evaluator enables testing on specific subsets with the metrics we want, then indicates to Pusher component to push the new model to production if \"model is good enough\". This would make the pipeline quite streamlined (https://www.tensorflow.org/tfx/guide/evaluator)\n\nB: wrong: using historical data, one should watch data leakage\nC: wrong: We want to track performance on specific subsets of data (not necessarily the last week) maybe to do some targeting/segmentation ? who knows. \nD: wrong because we want to track performance on specific subsets of data not the entire dataset","upvote_count":"3","comments":[{"comments":[{"timestamp":"1714080540.0","comment_id":"1054019","poster":"MultipleWorkerMirroredStrategy","content":"TFXModelValidator is deprecated, but its behaviour can be replicated using the Evaluator object - which is the point he tried to make. See the docs here: https://www.tensorflow.org/tfx/guide/modelval","upvote_count":"1"}],"upvote_count":"1","poster":"tavva_prudhvi","content":"Bro, thats not TFXModelValidator its Evaluator, are both the same?","timestamp":"1706017920.0","comment_id":"960411"}],"timestamp":"1705678920.0","comment_id":"956691"},{"comment_id":"945757","upvote_count":"1","timestamp":"1704643680.0","content":"Selected Answer: C\nWent with C","poster":"Liting"},{"timestamp":"1702037520.0","upvote_count":"1","content":"Selected Answer: C\nI think that it should be C for the following key point\n\", but track your performance on specific subsets of data before pushing to production\"\nSo the ask is which subset of data you should use.","poster":"Voyager2","comment_id":"918103"},{"timestamp":"1701484320.0","comment_id":"912428","upvote_count":"1","poster":"julliet","content":"Could someone explain why A is better option than C? C is correct one in terms of evaluation overall, no doubt. But do we choose TFX because it understands we are dealing with time series? Or is it the \"specific subset\" in the Q that makes us thinking we have already chosen the data of last period and just need to push it into the TFX?"},{"timestamp":"1700479560.0","comment_id":"902489","upvote_count":"1","content":"Selected Answer: C\nA is deprecated.. so C","poster":"aw_49"},{"content":"Selected Answer: A\nWent with A","timestamp":"1699514160.0","upvote_count":"3","comment_id":"892760","poster":"M25"},{"upvote_count":"1","poster":"SergioRubiano","timestamp":"1699026840.0","content":"Selected Answer: A\nTFX ModelValidator","comment_id":"888582"},{"timestamp":"1698307500.0","upvote_count":"1","content":"Selected Answer: C\nI go for C . A is deprecated","poster":"lucaluca1982","comment_id":"881291"},{"timestamp":"1696679160.0","content":"Selected Answer: C\nC is the right answer","upvote_count":"1","comment_id":"863776","poster":"Tomriddle690"},{"poster":"dfdrin","content":"Selected Answer: A\nIt's A. C is wrong since the model isn't being trained on all available data and you aren't tracking performance on specific subsets","upvote_count":"3","comment_id":"858733","timestamp":"1696241700.0"},{"comment_id":"853358","timestamp":"1695912660.0","content":"Selected Answer: A\nA. Use the TFX ModelValidator tools to specify performance metrics for production readiness.\n\nTensorFlow Extended (TFX) ModelValidator is a useful tool for evaluating your model's performance on specific subsets of data before pushing it to production. It enables you to set specific performance metrics that the model should meet to be considered production-ready. ModelValidator can help ensure that your model is performing well on the different subsets of data that matter to your business, addressing the highly dynamic nature of customer behavior in the footwear retail context.\n\nOption C, using the last relevant week of data as a validation set, may not be sufficient to validate the model's performance on various subsets of data.","poster":"alejandroverger","upvote_count":"4"},{"comment_id":"845087","upvote_count":"1","timestamp":"1695221520.0","poster":"tavva_prudhvi","content":"https://www.tensorflow.org/tfx/guide/modelval ModelValidator is deprecated, so we can rule out A. & from the options below everyone is mentioning about \"evaluator\"??. I reckon, the answer is C."},{"upvote_count":"1","poster":"tavva_prudhvi","comment_id":"845075","content":"Google's best practices for machine learning recommend using holdout data (a validation set) that is representative of the data that the model will encounter in production. In the case of a global footwear retailer trying to predict when an item will be out of stock, using k-fold cross-validation as a validation strategy may not be the best approach, as it involves training the model multiple times on different subsets of the data, which may not be representative of the data that the model will encounter in production. Instead, using the last relevant week of data as a validation set is recommended, as it is more likely to be representative of the data that the model will encounter in production. This approach allows for tracking the model's performance on current data before pushing it to production, which can help ensure that the model is accurate and robust.","timestamp":"1695220980.0","comments":[{"upvote_count":"1","poster":"tavva_prudhvi","content":"I am confused now, Option C only tests the model on a single recent subset of data, which might not capture the full spectrum of customer behavior and various factors influencing footwear demand. Then, B could be right...? What do you say?","comment_id":"960408","timestamp":"1706017800.0"}]},{"comment_id":"811693","upvote_count":"1","comments":[{"timestamp":"1695221100.0","upvote_count":"1","comment_id":"845080","poster":"tavva_prudhvi","content":"Hey, John its a classical case of demand forecasting, where in this case we try to predict the demand of footwear stock."}],"timestamp":"1692257580.0","content":"Selected Answer: B\nAt first glace and after that a while I thought it over \nIt is basic machine-learning concept,\nIt has nothing to do with Times Series Model, It is just regression.\n\"Track your performance on specific subsets of data\" ==> it is cross-validation\n\nMore specifically, AutomML use this thing on vertext ai\nlet take a closer look at this link https://cloud.google.com/vertex-ai/docs/tabular-data/tabular-workflows/e2e-automl","poster":"John_Pongthorn"},{"content":"Selected Answer: A\nThe answer is A because talks about a specific subset of data, not necessarily the most recent","timestamp":"1691419080.0","comment_id":"801136","poster":"FDS1993","upvote_count":"2"},{"timestamp":"1691136420.0","content":"Selected Answer: A\nhttps://www.tensorflow.org/tfx/guide/evaluator\nUnderstand performances based on subset of data","poster":"[Removed]","upvote_count":"2","comment_id":"797802","comments":[{"timestamp":"1695221280.0","upvote_count":"1","poster":"tavva_prudhvi","comment_id":"845082","content":"https://www.tensorflow.org/tfx/guide/modelval...this is what mentioned in the A, right?"}]},{"timestamp":"1688062680.0","poster":"koakande","comment_id":"761450","content":"Selected Answer: C\nK fold is not suitable for time series task as it leads to data leakage rather time partitioning is more suitable","upvote_count":"1"},{"timestamp":"1686617880.0","content":"Selected Answer: C\nC. it is a time series like problem and need to be careful of leakage issue","comment_id":"743538","upvote_count":"4","poster":"YangG"},{"comment_id":"741844","content":"It should be A, but it also should be TFX Evaluator.","timestamp":"1686490800.0","poster":"ares81","comments":[{"timestamp":"1686663060.0","upvote_count":"1","comment_id":"744205","content":"Thinking more about this question, I think that k-fold cross validation in TSA is a better fit, so B.","poster":"ares81","comments":[{"comment_id":"748902","upvote_count":"1","content":"but the question has been mentioned: track on specific subsets of data \nthen TFX Evaluator should be better","timestamp":"1687087320.0","poster":"mymy9418"}]}],"upvote_count":"3"},{"comment_id":"741670","poster":"LearnSodas","timestamp":"1686478680.0","content":"Selected Answer: B\nI will go with B","upvote_count":"1"}],"isMC":true,"question_text":"You work for a global footwear retailer and need to predict when an item will be out of stock based on historical inventory data Customer behavior is highly dynamic since footwear demand is influenced by many different factors. You want to serve models that are trained on all available data, but track your performance on specific subsets of data before pushing to production. What is the most streamlined and reliable way to perform this validation?"},{"id":"bnVlJhjk8xBxlwu0BBB4","answer_images":[],"unix_timestamp":1670749500,"question_images":[],"url":"https://www.examtopics.com/discussions/google/view/90994-exam-professional-machine-learning-engineer-topic-1-question/","isMC":true,"choices":{"C":"Use base64 to encode your data before using it for prediction.","A":"Use batch prediction mode instead of online mode.","B":"Send the request again with a smaller batch of instances.","D":"Apply for a quota increase for the number of prediction requests."},"answer":"B","answer_ET":"B","exam_id":13,"question_text":"You have deployed a model on Vertex AI for real-time inference. During an online prediction request, you get an “Out of Memory” error. What should you do?","answer_description":"","question_id":277,"answers_community":["B (97%)","3%"],"timestamp":"2022-12-11 10:05:00","discussion":[{"comment_id":"748756","poster":"hiromi","timestamp":"1687078440.0","content":"Selected Answer: B\nB is the answer\n429 - Out of Memory\nhttps://cloud.google.com/ai-platform/training/docs/troubleshooting","upvote_count":"23","comments":[{"poster":"tavva_prudhvi","content":"Upvote this comment, its the right answer!","timestamp":"1695221700.0","upvote_count":"4","comment_id":"845089"}]},{"timestamp":"1733568660.0","content":"Selected Answer: B\nB) Use smaller set of tokens","upvote_count":"1","poster":"PhilipKoku","comment_id":"1225991"},{"upvote_count":"1","comment_id":"1161729","poster":"pmle_nintendo","content":"Selected Answer: B\nBy reducing the batch size of instances sent for prediction, you decrease the memory footprint of each request, potentially alleviating the out-of-memory issue. However, be mindful that excessively reducing the batch size might impact the efficiency of your prediction process.","timestamp":"1724850840.0"},{"timestamp":"1699514160.0","poster":"M25","content":"Selected Answer: B\nWent with B","comment_id":"892761","upvote_count":"1"},{"upvote_count":"2","poster":"tavva_prudhvi","comment_id":"845090","timestamp":"1695221820.0","content":"B. Send the request again with a smaller batch of instances.\n\nIf you are getting an \"Out of Memory\" error during an online prediction request, it suggests that the amount of data you are sending in each request is too large and is exceeding the available memory. To resolve this issue, you can try sending the request again with a smaller batch of instances. This reduces the amount of data being sent in each request and helps avoid the out-of-memory error. If the problem persists, you can also try increasing the machine type or the number of instances to provide more resources for the prediction service."},{"content":"Selected Answer: C\nThis question is about prediction not training - and specifically it's about _online_ prediction (aka realtime serving).\n\nAll the answers are about batch workloads apart from C.","poster":"BenMS","comment_id":"824616","timestamp":"1693210620.0","comments":[{"content":"Okay, option D is also about online serving, but the error message indicates a problem for individual predictions, which will not be fixed by increasing the number of predictions per second.","upvote_count":"1","comment_id":"824625","poster":"BenMS","timestamp":"1693210860.0","comments":[{"content":"@BenMS this feels like a trick question.... makes on to zone to the word batch. https://cloud.google.com/ai-platform/training/docs/troubleshooting .... states then when an error occurs with an online prediction request, you usually get an HTTP status code back from the service. These are some commonly encountered codes and their meaning in the context of online prediction:\n\n429 - Out of Memory\nThe processing node ran out of memory while running your model. There is no way to increase the memory allocated to prediction nodes at this time. You can try these things to get your model to run:\n\nReduce your model size by:\n1. Using less precise variables.\n2. Quantizing your continuous data.\n3. Reducing the size of other input features (using smaller vocab sizes, for example).\n4. Send the request again with a smaller batch of instances.","comment_id":"856232","upvote_count":"2","poster":"Antmal","timestamp":"1696105920.0"}]}],"upvote_count":"1"},{"poster":"koakande","content":"Selected Answer: B\nhttps://cloud.google.com/ai-platform/training/docs/troubleshooting","upvote_count":"2","comment_id":"761455","timestamp":"1688062980.0"},{"content":"The correct answer is B.","poster":"ares81","upvote_count":"1","timestamp":"1686490680.0","comment_id":"741841"},{"timestamp":"1686478920.0","comment_id":"741673","upvote_count":"1","poster":"LearnSodas","content":"Selected Answer: B\nanswer B as reported here: https://cloud.google.com/ai-platform/training/docs/troubleshooting"},{"timestamp":"1686467100.0","poster":"Sivaram06","content":"Selected Answer: B\nhttps://cloud.google.com/ai-platform/training/docs/troubleshooting#http_status_codes","comment_id":"741556","upvote_count":"1"}],"topic":"1"},{"id":"pEj16vyLRBg10J729ZAR","question_images":[],"topic":"1","answers_community":["B (92%)","8%"],"answer":"B","isMC":true,"question_id":278,"question_text":"You work at a subscription-based company. You have trained an ensemble of trees and neural networks to predict customer churn, which is the likelihood that customers will not renew their yearly subscription. The average prediction is a 15% churn rate, but for a particular customer the model predicts that they are 70% likely to churn. The customer has a product usage history of 30%, is located in New York City, and became a customer in 1997. You need to explain the difference between the actual prediction, a 70% churn rate, and the average prediction. You want to use Vertex Explainable AI. What should you do?","answer_images":[],"choices":{"D":"Measure the effect of each feature as the weight of the feature multiplied by the feature value.","A":"Train local surrogate models to explain individual predictions.","B":"Configure sampled Shapley explanations on Vertex Explainable AI.","C":"Configure integrated gradients explanations on Vertex Explainable AI."},"exam_id":13,"unix_timestamp":1670773320,"answer_ET":"B","discussion":[{"comment_id":"1225993","content":"Selected Answer: B\nB) Shapley","timestamp":"1733568900.0","upvote_count":"2","poster":"PhilipKoku"},{"timestamp":"1724851260.0","upvote_count":"3","poster":"pmle_nintendo","content":"Selected Answer: B\nSampled Shapley explanations offer a more sophisticated and model-agnostic method for understanding feature importance and contributions to predictions.","comment_id":"1161739"},{"timestamp":"1715075400.0","content":"Selected Answer: B\nI agree, it seems like B","comment_id":"1064753","poster":"adavid213","upvote_count":"1"},{"comment_id":"933659","upvote_count":"2","timestamp":"1703519580.0","poster":"NickHapton","content":"B\nrefer: \nhttps://cloud.google.com/vertex-ai/docs/explainable-ai/overview#compare-methods"},{"timestamp":"1699514160.0","poster":"M25","content":"Selected Answer: B\nWent with B","upvote_count":"2","comment_id":"892762"},{"comment_id":"892661","upvote_count":"2","content":"Selected Answer: B\nAssigns credit for the outcome to each feature, and considers different permutations of the features. This method provides a sampling approximation of exact Shapley values.\nshampled shapely recommended Model Type: Non-differentiable models, such as ensembles of trees and neural networks.\nhttps://cloud.google.com/ai-platform/prediction/docs/ai-explanations/overview","poster":"CloudKida","timestamp":"1699511580.0"},{"upvote_count":"2","poster":"enghabeth","comment_id":"802755","timestamp":"1691542200.0","content":"Selected Answer: B\nSampled Shapley works well for these models, which are meta-ensembles of trees and neural networks.\nhttps://cloud.google.com/vertex-ai/docs/explainable-ai/overview#sampled-shapley"},{"comments":[{"poster":"John_Pongthorn","timestamp":"1690354740.0","comment_id":"788557","upvote_count":"3","content":"https://cloud.google.com/vertex-ai/docs/explainable-ai/overview#compare-methods"}],"upvote_count":"2","comment_id":"788556","timestamp":"1690354680.0","poster":"John_Pongthorn","content":"Selected Answer: B\nB is optimal for tabular data Tree or DNN\n\nC integrated gradients explanations on Vertex Explainable AI.\nIt is used for image."},{"comment_id":"765714","upvote_count":"1","timestamp":"1688471580.0","content":"Selected Answer: B\nIt should be B.","poster":"ares81"},{"upvote_count":"2","comment_id":"760005","timestamp":"1687958100.0","content":"Selected Answer: B\nhttps://cloud.google.com/vertex-ai/docs/explainable-ai/overview#sampled-shapley","poster":"emma_aic"},{"poster":"egdiaa","timestamp":"1687579440.0","upvote_count":"1","comment_id":"754714","content":"B - For sure as per GCP Docs here: https://cloud.google.com/vertex-ai/docs/explainable-ai/overview"},{"content":"Selected Answer: B\nB\n- https://christophm.github.io/interpretable-ml-book/shapley.html\n- https://cloud.google.com/vertex-ai/docs/explainable-ai/overview","comment_id":"748765","timestamp":"1687079160.0","poster":"hiromi","upvote_count":"2"},{"comment_id":"744133","upvote_count":"3","poster":"JeanEl","content":"Selected Answer: B\nAgree with B : individual instance prediction + ensemble of trees and neural networks (recommended model types for Sampled Shapley : \"Non-differentiable models, such as ensembles of trees and neural networks \" ). Check out the link below :\nhttps://cloud.google.com/vertex-ai/docs/explainable-ai/overview","timestamp":"1686659520.0"},{"upvote_count":"2","content":"Selected Answer: C\nit is about a individual instance prediction. I think use integrated gradient method","timestamp":"1686620340.0","poster":"YangG","comment_id":"743568"},{"poster":"ares81","upvote_count":"1","comment_id":"741848","timestamp":"1686490920.0","content":"It seems D."}],"timestamp":"2022-12-11 16:42:00","url":"https://www.examtopics.com/discussions/google/view/91029-exam-professional-machine-learning-engineer-topic-1-question/","answer_description":""},{"id":"e0oCe79HuVPSctc7qQU1","question_images":[],"answer_images":[],"unix_timestamp":1670773500,"answer":"B","answer_description":"","question_id":279,"url":"https://www.examtopics.com/discussions/google/view/91030-exam-professional-machine-learning-engineer-topic-1-question/","answers_community":["B (73%)","D (18%)","9%"],"discussion":[{"timestamp":"1737134520.0","poster":"desertlotus1211","upvote_count":"2","comment_id":"1342264","content":"Selected Answer: C\nYou are working with time series data yet used random cross-validation, and you immediately achieved an extremely high AUC (99%) with little effort. This is a red flag for data leakage—meaning information from the future (or directly from the target) is leaking into the training process\n\nC is better answer"},{"timestamp":"1713719400.0","upvote_count":"3","comment_id":"1199779","poster":"pinimichele01","content":"Selected Answer: B\nrandom cross-validation \ntime series data\n\n-> B"},{"comments":[{"poster":"pinimichele01","upvote_count":"1","comment_id":"1199034","timestamp":"1713604200.0","content":"can you explain me why?"}],"timestamp":"1713099180.0","upvote_count":"2","comment_id":"1195513","content":"Selected Answer: B\nB with nested cross validation.","poster":"gscharly"},{"upvote_count":"3","comment_id":"1162283","content":"Selected Answer: B\n\"99% on training data\" -> Data leakage\n\"random cross-validation\" -> Not suitable for time series, use \"nested cross-validation\"","poster":"Werner123","timestamp":"1709189880.0"},{"upvote_count":"1","content":"Selected Answer: D\nOptions B and C (Address data leakage by applying nested cross-validation during model training; Address data leakage by removing features highly correlated with the target value) are less relevant in this scenario because the primary concern appears to be overfitting rather than data leakage. Data leakage typically involves inadvertent inclusion of information from the test set in the training process, which may lead to overly optimistic performance metrics. However, there is no indication that data leakage is the cause of the high AUC ROC value in this case.","poster":"pmle_nintendo","timestamp":"1709136060.0","comments":[{"timestamp":"1731403500.0","upvote_count":"1","poster":"503b759","comment_id":"1310502","content":"Data leakage is occuring owing to the use of k-fold cross val, because of the time series nature of the data."}],"comment_id":"1161775"},{"poster":"pico","comment_id":"1070487","content":"Selected Answer: D\nOptions A and B also address overfitting, but they involve different strategies. Option A suggests using a less complex algorithm and k-fold cross-validation. While this can be effective, it might be premature to change the algorithm without first exploring hyperparameter tuning. Option B suggests addressing data leakage, which is a different issue and may not be the primary cause of overfitting in this scenario.","timestamp":"1699972560.0","upvote_count":"3"},{"content":"Selected Answer: B\nB with nested cross validation.","poster":"humancomputation","timestamp":"1695895440.0","comment_id":"1019703","upvote_count":"1"},{"timestamp":"1683609420.0","upvote_count":"2","poster":"M25","content":"Selected Answer: B\nWent with B","comment_id":"892764"},{"content":"Selected Answer: B\nNested cross-validation to reduce data leakage - same as a previous question.","comment_id":"824645","upvote_count":"1","poster":"BenMS","timestamp":"1677580440.0"},{"comment_id":"823748","upvote_count":"1","content":"Selected Answer: B\nIt`s B","poster":"Alexarr6","timestamp":"1677505620.0"},{"timestamp":"1671361800.0","comment_id":"748772","content":"Selected Answer: B\nB (same question 48)\n- https://towardsdatascience.com/time-series-nested-cross-validation-76adba623eb9","poster":"hiromi","upvote_count":"3"},{"content":"To say overfitting, I should have results on testing data, so it's data leakage. Common sense excludes C, so it's B.","comment_id":"741853","upvote_count":"1","timestamp":"1670773500.0","poster":"ares81"}],"choices":{"A":"Address the model overfitting by using a less complex algorithm and use k-fold cross-validation.","D":"Address the model overfitting by tuning the hyperparameters to reduce the AUC ROC value.","B":"Address data leakage by applying nested cross-validation during model training.","C":"Address data leakage by removing features highly correlated with the target value."},"isMC":true,"question_text":"You are working on a classification problem with time series data. After conducting just a few experiments using random cross-validation, you achieved an Area Under the Receiver Operating Characteristic Curve (AUC ROC) value of 99% on the training data. You haven’t explored using any sophisticated algorithms or spent any time on hyperparameter tuning. What should your next step be to identify and fix the problem?","answer_ET":"B","timestamp":"2022-12-11 16:45:00","exam_id":13,"topic":"1"},{"id":"is63vtiypVkO7g3rOxFS","question_images":[],"isMC":true,"answer_images":[],"timestamp":"2022-12-11 16:54:00","discussion":[{"timestamp":"1687080060.0","content":"Selected Answer: A\nA should work with less effort\n- https://cloud.google.com/bigquery-ml/docs/making-predictions-with-imported-tensorflow-models#api\n- https://towardsdatascience.com/how-to-do-batch-predictions-of-tensorflow-models-directly-in-bigquery-ffa843ebdba6","upvote_count":"11","poster":"hiromi","comment_id":"748780"},{"timestamp":"1737134640.0","content":"Selected Answer: D\nBigQuery ML does not support importing arbitrary custom TensorFlow models for direct inference","comment_id":"1342265","poster":"desertlotus1211","upvote_count":"2"},{"comment_id":"1226480","timestamp":"1733628360.0","content":"Selected Answer: D\nBigQuery ML might not support custom TensorFlow DNN models directly.","poster":"livewalk","upvote_count":"2"},{"timestamp":"1725290760.0","poster":"etienne0","content":"Selected Answer: C\nWent with C","upvote_count":"1","comment_id":"1164240"},{"timestamp":"1720524360.0","comment_id":"1117511","content":"Simplest doesn't mean it is the most effecient/optimal. If I follow the Best practices offered by Google for Serving / Inference Pipeline I would go with Vertex AI predictions. Read More for correct details : https://cloud.google.com/architecture/ml-on-gcp-best-practices#machine-learning-development","comments":[],"upvote_count":"2","poster":"pawan94"},{"timestamp":"1699514220.0","comment_id":"892765","poster":"M25","content":"Selected Answer: A\nWent with A","upvote_count":"2"},{"comment_id":"853075","timestamp":"1695894780.0","content":"Selected Answer: A\nhttps://cloud.google.com/bigquery-ml/docs/making-predictions-with-imported-tensorflow-models","poster":"JamesDoe","upvote_count":"2"},{"comment_id":"802763","poster":"enghabeth","upvote_count":"2","content":"Selected Answer: A\nfor this:\nhttps://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-inference-overview\nPredict the label, either a numerical value for regression tasks or a categorical value for classification tasks on DNN regresion","timestamp":"1691542740.0"},{"timestamp":"1686498660.0","comment_id":"741946","poster":"ares81","content":"ml.predict: https://cloud.google.com/bigquery-ml/docs/making-predictions-with-imported-tensorflow-models#api --> A","upvote_count":"1"},{"timestamp":"1686491640.0","poster":"LearnSodas","upvote_count":"1","comment_id":"741864","content":"Selected Answer: A\nAnswer A as the simplest"}],"question_id":280,"url":"https://www.examtopics.com/discussions/google/view/91031-exam-professional-machine-learning-engineer-topic-1-question/","unix_timestamp":1670774040,"answer_ET":"A","question_text":"You need to execute a batch prediction on 100 million records in a BigQuery table with a custom TensorFlow DNN regressor model, and then store the predicted results in a BigQuery table. You want to minimize the effort required to build this inference pipeline. What should you do?","answer_description":"","answers_community":["A (78%)","D (17%)","4%"],"exam_id":13,"choices":{"D":"Load the TensorFlow SavedModel in a Dataflow pipeline. Use the BigQuery I/O connector with a custom function to perform the inference within the pipeline, and write the results to BigQuery.","A":"Import the TensorFlow model with BigQuery ML, and run the ml.predict function.","C":"Create a Dataflow pipeline to convert the data in BigQuery to TFRecords. Run a batch inference on Vertex AI Prediction, and write the results to BigQuery.","B":"Use the TensorFlow BigQuery reader to load the data, and use the BigQuery API to write the results to BigQuery."},"topic":"1","answer":"A"}],"exam":{"name":"Professional Machine Learning Engineer","isMCOnly":true,"isBeta":false,"id":13,"isImplemented":true,"lastUpdated":"11 Apr 2025","provider":"Google","numberOfQuestions":304},"currentPage":56},"__N_SSP":true}