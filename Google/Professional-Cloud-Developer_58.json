{"pageProps":{"questions":[{"id":"lswsewq5b4h1ITbOPskZ","discussion":[{"poster":"omermahgoub","comment_id":"768348","content":"Answer is A:\nTo view the logs sent to stdout for all replicas in a Deployment in multiple clusters, the correct command to use would be kubectl logs [PARAM].\n\nThe gcloud logging read command reads log entries from the specified logs. It does not allow you to view the logs of specific replicas in a Deployment across multiple clusters. kubectl logs allows you to view the logs of a specific Pod or Deployment across multiple clusters. You can specify the Deployment name and the relevant parameters to view the logs of all replicas in the Deployment.\n\nFor example, the following command would allow you to view the logs of all replicas in a Deployment named \"my-deployment\" in all clusters:\n\nkubectl logs -l app=my-deployment --all-containers","timestamp":"1688710200.0","upvote_count":"6","comments":[{"comment_id":"768349","timestamp":"1688710200.0","upvote_count":"1","poster":"omermahgoub","content":"gcloud logging read [PARAM], can be used to read log entries from Stackdriver Logging, but it is not specifically designed for viewing the logs of Pods in a Kubernetes cluster. Additionally, gcloud logging read does not have a way to filter the log entries based on the Pod or Deployment, so it would not be possible to use it to view the logs for all of the replicas in a Deployment across multiple clusters"}]},{"comment_id":"649213","content":"Selected Answer: B\nB is correct","upvote_count":"5","poster":"tomato123","timestamp":"1676879160.0"},{"upvote_count":"1","comment_id":"1168476","poster":"santoshchauhan","timestamp":"1725751500.0","content":"Selected Answer: B\nB. gcloud logging read [PARAM]: Google Cloud's operations suite (formerly known as Stackdriver) aggregates logs from all the pods across all the GKE clusters. By using gcloud logging read, you can query these logs with specific parameters (like the name of the Deployment, container, or other filters) to view the combined logs from all replicas across all clusters. This command provides a centralized way to access logs at scale."},{"content":"Selected Answer: B\nB is correct","timestamp":"1710908520.0","poster":"__rajan__","comment_id":"1011852","upvote_count":"1"},{"content":"choose B. gcloud logging also can be used for quering pod log\nhttps://stackoverflow.com/questions/62007471/how-to-view-container-logs-via-stackdriver-on-gke","timestamp":"1692516420.0","upvote_count":"2","poster":"edward_zhang","comment_id":"815048"},{"timestamp":"1665457920.0","content":"B\nhttps://cloud.google.com/sdk/gcloud/reference/logging/read","comment_id":"583999","upvote_count":"2","poster":"yogi_508"},{"poster":"ParagSanyashiv","comment_id":"519469","timestamp":"1657272780.0","upvote_count":"1","content":"Selected Answer: B\nCorrect answer is B"},{"upvote_count":"2","comment_id":"479721","poster":"PrissMedrano","timestamp":"1652738700.0","content":"Selected Answer: B\nCorrect answer is B"},{"upvote_count":"2","content":"Correct answer is B\nhttps://cloud.google.com/logging/docs/reference/tools/gcloud-logging#examples_2","comment_id":"471394","poster":"Valant","timestamp":"1651428420.0"},{"poster":"therealsohail","timestamp":"1648798680.0","content":"A and B","comment_id":"455392","upvote_count":"1"},{"timestamp":"1642505580.0","poster":"[Removed]","content":"B) with parameters : resource.type=(\"k8s_container\" OR \"container\" OR \"k8s_cluster\" OR \"gke_cluster\" OR \"gke_nodepool\" OR \"k8s_node\")","comment_id":"408837","upvote_count":"1"},{"poster":"celia20200410","upvote_count":"3","comment_id":"408790","content":"B: gcloud logging read\n\nUsing the \"gcloud logging read\" command, select the appropriate cluster, node, pod, and container logs.\nhttps://cloud.google.com/stackdriver/docs/solutions/gke/using-logs#accessing_your_logs\n\nHowever if you use \"kubectl logs\" to see logs on CLI, logs won’t be seen readable. It prints each line as a JSON object. \nhttps://medium.com/google-cloud/display-gke-logs-in-a-text-format-with-kubectl-db0169be0282","timestamp":"1642499100.0"},{"poster":"syu31svc","comments":[{"upvote_count":"3","timestamp":"1643608500.0","comment_id":"417690","poster":"syu31svc","content":"Changing to B\n\nhttps://cloud.google.com/blog/products/management-tools/using-logging-your-apps-running-kubernetes-engine:\n\"gcloud command line tool – Using the gcloud logging read command, select the appropriate cluster, node, pod and container logs.\""}],"timestamp":"1641259500.0","upvote_count":"1","comment_id":"397875","content":"https://kubernetes.io/docs/reference/kubectl/cheatsheet/\n\nI would take A"},{"content":"B for me.\ngcloud logging read","upvote_count":"3","comment_id":"394657","poster":"kernel1973","timestamp":"1640872140.0"},{"timestamp":"1629356580.0","upvote_count":"3","content":"Option: B. (Link1: https://cloud.google.com/blog/products/management-tools/finding-your-gke-logs, Link2: https://cloud.google.com/sdk/gcloud/reference/logging/read)","poster":"StelSen","comment_id":"294074"},{"comment_id":"216218","poster":"saurabh1805","timestamp":"1620586380.0","comments":[{"comment_id":"361972","content":"doesn't this view the logs of a single cluster?","upvote_count":"2","comments":[{"timestamp":"1640871960.0","poster":"kernel1973","comment_id":"394654","content":"You need to change the cluster connection's in order to view the logs for a specific deployments.","upvote_count":"1"}],"poster":"mastodilu","timestamp":"1637405040.0"}],"content":"A seems to be correct answer.","upvote_count":"4"}],"url":"https://www.examtopics.com/discussions/google/view/36607-exam-professional-cloud-developer-topic-1-question-51/","topic":"1","unix_timestamp":1604955180,"isMC":true,"answer_ET":"B","choices":{"D":"gcloud compute ssh [PARAM] ג€\"-command= ג€sudo journalctlג€","B":"gcloud logging read [PARAM]","A":"kubectl logs [PARAM]","C":"kubectl exec ג€\"it [PARAM] journalctl"},"answer_images":[],"question_text":"Your application is running in multiple Google Kubernetes Engine clusters. It is managed by a Deployment in each cluster. The Deployment has created multiple replicas of your Pod in each cluster. You want to view the logs sent to stdout for all of the replicas in your Deployment in all clusters.\nWhich command should you use?","question_id":286,"exam_id":7,"question_images":[],"timestamp":"2020-11-09 21:53:00","answer":"B","answer_description":"","answers_community":["B (100%)"]},{"id":"W6uMLCnhPxWIYSRqWfu8","choices":{"B":"Create a build trigger on a Git tag pattern. Use a Git tag convention for new releases.","A":"Manually trigger the build for new releases.","D":"Commit your source code to a second Cloud Source Repositories repository with a second Cloud Build trigger. Use this repository for new releases only.","C":"Create a build trigger on a Git branch name pattern. Use a Git branch naming convention for new releases."},"question_id":287,"answer":"B","answer_images":[],"timestamp":"2020-11-09 21:55:00","answer_ET":"B","discussion":[{"content":"B is correct answer","upvote_count":"19","timestamp":"1620586500.0","comment_id":"216219","poster":"saurabh1805"},{"timestamp":"1725751920.0","content":"Selected Answer: B\nB. Create a build trigger on a Git tag pattern. Use a Git tag convention for new releases.\n\nThis approach is effective for managing releases in an automated yet controlled manner. By creating a Cloud Build trigger that activates based on a specific Git tag pattern, you can automate the build and deployment process for new releases. Git tags are often used to mark release points in the repository, so this aligns well with common development practices.\n\nWhen a commit is tagged in the repository with a specific pattern (e.g., \"v1.0\", \"release-*\"), Cloud Build can automatically trigger a build and potentially a deployment process. This allows for a more deliberate release process compared to triggering on every commit to the master branch.","upvote_count":"1","comment_id":"1168483","poster":"santoshchauhan"},{"comment_id":"1011854","upvote_count":"1","content":"Selected Answer: B\nB is correct.","timestamp":"1710908640.0","poster":"__rajan__"},{"content":"Selected Answer: B\nB is correct","comment_id":"649214","timestamp":"1676879160.0","poster":"tomato123","upvote_count":"3"},{"upvote_count":"4","poster":"nehaxlpb","timestamp":"1674891240.0","content":"Selected Answer: B\nI don't know why people are selecting C , the qus says commit to master . C literally does not make sense how commit to a feature branch can trigger a master build.","comment_id":"638485"},{"poster":"nqthien041292","upvote_count":"2","comment_id":"592749","content":"Selected Answer: B\nVote B","timestamp":"1666833300.0"},{"content":"I think C correct answer","poster":"Khratata","timestamp":"1650824400.0","comment_id":"467080","upvote_count":"1"},{"content":"C) is not correct because the question says the commits are made in master branch. \nB) is good answer. When you want a release create a tag release-* , in Cloud Build use this pattern for tag.","comment_id":"408841","poster":"[Removed]","timestamp":"1642505880.0","upvote_count":"4"},{"comments":[{"upvote_count":"1","timestamp":"1651276440.0","content":"specific commits made to the master branch --> Tag would work here, team can continue pushing further changes in master branch, tag will point to specific version, so I feel correct answer is B","poster":"gcp0omkar","comment_id":"470448"}],"comment_id":"397880","poster":"syu31svc","upvote_count":"2","timestamp":"1641260400.0","content":"https://cloud.google.com/source-repositories/docs/integrating-with-cloud-build:\n\"you can set a trigger to start a build on commits that are made to a particular branch, or on commits that contain a particular tag\"\n\nI would take C since the qn states that the commit is made to a branch"},{"upvote_count":"1","poster":"pythonrocks","timestamp":"1634009040.0","content":"C. https://cloud.google.com/build/docs/automating-builds/create-manage-triggers has either branch pattern or tag pattern","comment_id":"333661"},{"comment_id":"300720","poster":"maxdanny","content":"Also B is correct, but in the questions is specificied \"branch name\" , not tag","timestamp":"1630138380.0","comments":[{"timestamp":"1647810600.0","comment_id":"448405","content":"\"Your application is built on every commit to the master branch. You want to release SPECIFIC commits made to the master branch in an automated method.\"\n\nhttps://cloud.google.com/source-repositories/docs/integrating-with-cloud-build#create_a_build_trigger\n\nBoth branch name and tag are valid triggers, but the question suggests we want to target specific commits to our master branch, therefore B, using a tag, seems more legit in this case.","poster":"boof","upvote_count":"10"},{"poster":"lollo1234","comment_id":"386091","content":"\"You want to release specific commits made to the master branch in an automated method\"\n\nLooks like they're committing to the master branch, hence a tag would make more sense than a branch name.","timestamp":"1639998060.0","upvote_count":"4"}],"upvote_count":"4"}],"exam_id":7,"answer_description":"","topic":"1","url":"https://www.examtopics.com/discussions/google/view/36608-exam-professional-cloud-developer-topic-1-question-52/","answers_community":["B (100%)"],"unix_timestamp":1604955300,"question_images":[],"isMC":true,"question_text":"You are using Cloud Build to create a new Docker image on each source code commit to a Cloud Source Repositories repository. Your application is built on every commit to the master branch. You want to release specific commits made to the master branch in an automated method.\nWhat should you do?"},{"id":"q3kkCRNMjc23qvUgsjGx","url":"https://www.examtopics.com/discussions/google/view/36720-exam-professional-cloud-developer-topic-1-question-53/","answer":"B","question_images":["https://www.examtopics.com/assets/media/exam-media/04137/0004000001.png"],"answer_images":[],"question_id":288,"answers_community":["B (90%)","10%"],"discussion":[{"upvote_count":"21","poster":"salgabri","content":"correct answer is B\nhttps://cloud.google.com/bigtable/docs/schema-design","comments":[{"upvote_count":"3","timestamp":"1625355840.0","content":"From the link:\n\"Include a timestamp as part of your row key if you often need to retrieve data based on the time when it was recorded.\n\nFor example, your application might need to record performance-related data, such as CPU and memory usage, once per second for a large number of machines. Your row key for this data could combine an identifier for the machine with a timestamp for the data (for example, machine_4223421#1425330757685). Keep in mind that row keys are sorted lexicographically.\"","poster":"syu31svc","comment_id":"397881"}],"timestamp":"1605047640.0","comment_id":"216970"},{"upvote_count":"1","content":"Selected Answer: C\nBy using Event_timestamp_Account_id as the row key, you can efficiently query for events within a specific time range for a given account.\n\nWhile Option B would allow you to query events for a specific account, it might not be as efficient for time-based queries.","comment_id":"1322220","timestamp":"1733373000.0","poster":"PinkeshExampTopics"},{"timestamp":"1709862360.0","poster":"santoshchauhan","content":"Selected Answer: B\nDesigning an appropriate row key for Cloud Bigtable requires considering the access patterns and ensuring that the read and write operations are spread evenly across the key space to avoid hotspots.\n\nB. Set Account_id_Event_timestamp as a key.\n\nThis option is likely the best choice because:\n\nCombining Account_id with Event_timestamp in the row key would allow you to maintain a good level of data distribution while preserving the ability to query efficiently by Account_id and sort by Event_timestamp within each account. This aligns well with Bigtable’s strengths in handling large, scalable, and sparse datasets.\n\nBy leading with Account_id, you group all events for a single account close together in the key space, which can be efficient for reads that are interested in the activity of a specific account.","upvote_count":"1","comment_id":"1168493"},{"comment_id":"1011858","timestamp":"1695177120.0","poster":"__rajan__","upvote_count":"1","content":"Selected Answer: B\nI would go with B."},{"timestamp":"1673079300.0","comment_id":"768350","poster":"omermahgoub","upvote_count":"1","content":"B. Set Account_id_Event_timestamp as a key.\n\nThe primary key in the MySQL table is a composite key consisting of Account_id and Event_timestamp, so it would make sense to use both of these values as the row key in Cloud Bigtable. This allows for efficient querying and sorting by both Account_id and Event_timestamp.","comments":[{"poster":"omermahgoub","comment_id":"768353","upvote_count":"1","timestamp":"1673079300.0","content":"A would not be a good choice because the row key would not include the Event_timestamp, which is part of the primary key in the MySQL table. Option"}]},{"timestamp":"1660974420.0","content":"Selected Answer: B\nB is correct","comment_id":"649215","poster":"tomato123","upvote_count":"3"},{"comment_id":"636683","timestamp":"1658749920.0","content":"Selected Answer: B\nhttps://cloud.google.com/bigtable/docs/schema-design#row-keys\n\nIt's B because :\n\"Row keys that start with a timestamp. This pattern causes sequential writes to be pushed onto a single node, creating a hotspot. If you put a timestamp in a row key, precede it with a high-cardinality value like a user ID to avoid hotspotting.\"","upvote_count":"3","poster":"maxdanny"},{"poster":"saumabhaM","content":"Selected Answer: B\nInclude a timestamp as part of your row key and avoid having timestamp at the start of the key","comment_id":"588520","timestamp":"1650444060.0","upvote_count":"1"},{"poster":"whigy","timestamp":"1605263940.0","comments":[{"timestamp":"1621508400.0","upvote_count":"4","poster":"mastodilu","content":"https://cloud.google.com/bigtable/docs/schema-design#row-keys\navoid row keys that starts with a timestamp.\nAlso using a key such as userID_timestamp allows bigtable to query related rows in a range rather than parsing the entire database.","comment_id":"362075"},{"poster":"donchick","timestamp":"1608571080.0","comments":[{"upvote_count":"8","timestamp":"1609593600.0","poster":"fraloca","comment_id":"257478","content":"\"Row keys that start with a timestamp. This will cause sequential writes to be pushed onto a single node, creating a hotspot. If you put a timestamp in a row key, you need to precede it with a high-cardinality value like a user ID to avoid hotspotting.\""}],"content":"https://cloud.google.com/bigtable/docs/schema-design#timestamps - avoid placing a timestamp at the start of the row key. I vote for B.","comment_id":"249516","upvote_count":"6"}],"comment_id":"218397","upvote_count":"2","content":"Should be C. Account id as the first part of the key has no benifit for search"}],"topic":"1","exam_id":7,"choices":{"A":"Set Account_id as a key.","C":"Set Event_timestamp_Account_id as a key.","D":"Set Event_timestamp as a key.","B":"Set Account_id_Event_timestamp as a key."},"question_text":"You are designing a schema for a table that will be moved from MySQL to Cloud Bigtable. The MySQL table is as follows:\n//IMG//\n\nHow should you design a row key for Cloud Bigtable for this table?","answer_ET":"B","unix_timestamp":1605047640,"timestamp":"2020-11-10 23:34:00","answer_description":"","isMC":true},{"id":"UGRMCHwu9BI2dQVDFERV","question_id":289,"unix_timestamp":1604956140,"discussion":[{"content":"Option-B is correct. https://cloud.google.com/monitoring/api/metrics_agent#agent-memory (By default Memory metrics is not collected). To double confirm. Just goto Console->Operations->Monitoring->Dashboards->VM Instances->Memory Tab (Assume you have VM running already). You will see a info message saying that No agents detected. Monitoring agents collect memory metrics, disk metrics, and more. Learn more about agents and how to manage them across multiple VMs.","upvote_count":"17","comments":[{"poster":"Ram02","comment_id":"441914","timestamp":"1631184840.0","content":"Correct, see following link for more detail\nhttps://stackoverflow.com/questions/43991246/google-cloud-platform-how-to-monitor-memory-usage-of-vm-instances","upvote_count":"3"}],"comment_id":"294091","timestamp":"1613727300.0","poster":"StelSen"},{"content":"For me B si the correct answer as you can not read memory usage directly from stackdriver without the monitoring agent","upvote_count":"8","poster":"dxxdd7","comment_id":"276244","timestamp":"1611600000.0"},{"upvote_count":"2","poster":"thewalker","timestamp":"1721211840.0","comments":[{"content":"Let's look at why the other options are less suitable:\n\nA. Install the Stackdriver Client Library: The Stackdriver Client Library is used for programmatically interacting with Stackdriver from your application code. While it can be used to collect and send metrics, it's not the primary tool for viewing memory usage.\nB. Install the Stackdriver Monitoring Agent: The Stackdriver Monitoring Agent is responsible for collecting metrics from your Compute Engine instances. It's a necessary component for monitoring, but it doesn't provide a user interface for viewing metrics.\nD. Use the Google Cloud Platform Console: The Google Cloud Platform Console provides a general overview of your resources, but it doesn't offer the same level of detail and flexibility as the Stackdriver Metrics Explorer for viewing memory usage.","comment_id":"1249527","poster":"thewalker","timestamp":"1721211900.0","upvote_count":"1"}],"comment_id":"1249526","content":"Selected Answer: C\nThe best answer here is C. Use the Stackdriver Metrics Explorer.\n\nHere's why:\n\nStackdriver Metrics Explorer: The Stackdriver Metrics Explorer is a powerful tool for visualizing and analyzing metrics collected from your Google Cloud resources. It allows you to view various metrics, including memory usage, over time. You can filter and group metrics based on different criteria, making it easy to identify trends and potential issues."},{"comment_id":"1168495","content":"Selected Answer: B\nB. Install the Stackdriver Monitoring Agent.\n\nThe Stackdriver Monitoring Agent allows you to collect more system-level and third-party application metrics than what is provided by default with Google Cloud's operations suite. By installing the agent on your Compute Engine instances, you can collect detailed memory usage metrics, which can then be viewed in the Google Cloud Console or through the Metrics Explorer in Google Cloud's operations suite (formerly Stackdriver).","poster":"santoshchauhan","timestamp":"1709862960.0","upvote_count":"1"},{"content":"Selected Answer: B\nI would go with B.","poster":"__rajan__","comment_id":"1011859","upvote_count":"1","timestamp":"1695177240.0"},{"timestamp":"1660974420.0","comment_id":"649216","content":"Selected Answer: B\nB is correct","upvote_count":"3","poster":"tomato123"},{"timestamp":"1651068660.0","poster":"nqthien041292","comment_id":"593227","upvote_count":"1","content":"Selected Answer: B\nVote B"},{"content":"https://cloud.google.com/monitoring/agent\n\nB is correct","comment_id":"397885","poster":"syu31svc","upvote_count":"3","timestamp":"1625356380.0"},{"upvote_count":"1","timestamp":"1614817500.0","content":"Option: D","poster":"dwbi_shrikant","comment_id":"302941"},{"comment_id":"294364","content":"I think option B and C are correct answer - you need to install stack driver manager on VM to push logs into stack driver and then you can use stack driver metrics to view the metrics.","timestamp":"1613748540.0","upvote_count":"3","poster":"elan1302"},{"comment_id":"216220","timestamp":"1604956140.0","poster":"saurabh1805","upvote_count":"5","content":"C is correct answer."}],"answers_community":["B (75%)","C (25%)"],"topic":"1","answer":"B","choices":{"B":"Install the Stackdriver Monitoring Agent.","A":"Install the Stackdriver Client Library.","D":"Use the Google Cloud Platform Console.","C":"Use the Stackdriver Metrics Explorer."},"question_text":"You want to view the memory usage of your application deployed on Compute Engine.\nWhat should you do?","question_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/google/view/36609-exam-professional-cloud-developer-topic-1-question-54/","answer_ET":"B","answer_images":[],"timestamp":"2020-11-09 22:09:00","exam_id":7,"answer_description":""},{"id":"QUleNMCLv7dtHWP6BS3H","answer":"D","answer_images":[],"question_text":"You have an analytics application that runs hundreds of queries on BigQuery every few minutes using BigQuery API. You want to find out how much time these queries take to execute.\nWhat should you do?","isMC":true,"unix_timestamp":1605102720,"exam_id":7,"question_id":290,"topic":"1","answer_description":"","question_images":[],"choices":{"D":"Use Stackdriver Monitoring to plot query execution times.","B":"Use Stackdriver Trace to plot API execution time.","C":"Use Stackdriver Trace to plot query execution time.","A":"Use Stackdriver Monitoring to plot slot usage."},"url":"https://www.examtopics.com/discussions/google/view/36767-exam-professional-cloud-developer-topic-1-question-55/","timestamp":"2020-11-11 14:52:00","answer_ET":"D","answers_community":["D (60%)","C (40%)"],"discussion":[{"poster":"saurabh1805","comments":[{"upvote_count":"8","comment_id":"252575","content":"D is correct answer: https://cloud.google.com/bigquery/docs/monitoring","comments":[{"comment_id":"397888","content":"\"Use Cloud Monitoring to view BigQuery metrics and create charts and alerts\"","upvote_count":"2","timestamp":"1625356680.0","poster":"syu31svc"},{"timestamp":"1613727540.0","upvote_count":"2","poster":"StelSen","comment_id":"294096","content":"Use this link and locate BigQuery: https://cloud.google.com/monitoring/api/metrics_gcp#gcp-bigquery"}],"poster":"fraloca","timestamp":"1608978660.0"}],"content":"You dont need to enable trace for this, Best and correct option is D","comment_id":"217301","timestamp":"1605102720.0","upvote_count":"7"},{"timestamp":"1733373660.0","upvote_count":"1","poster":"PinkeshExampTopics","comment_id":"1322224","content":"Selected Answer: C\nStackdriver Trace provides detailed traces of individual requests, including the time spent in each function and database query.\n\nHowever, Stackdriver Monitoring doesn't provide detailed information about individual query execution times."},{"upvote_count":"1","poster":"santoshchauhan","timestamp":"1709863260.0","comment_id":"1168496","content":"Selected Answer: D\nD. Use Stackdriver Monitoring to plot query execution times.\n\nStackdriver Monitoring (now part of Google Cloud's operations suite) provides capabilities to monitor BigQuery and create custom dashboards to visualize various metrics, including query execution times. You can track how long your queries take to run by plotting the query_execution_times metric in a custom dashboard."},{"poster":"mohammeddigital","content":"Selected Answer: D\nD is correct","timestamp":"1704008100.0","upvote_count":"1","comment_id":"1110354"},{"comment_id":"1011860","content":"Selected Answer: C\nOption C is best suited here.","poster":"__rajan__","timestamp":"1695177480.0","upvote_count":"1"},{"timestamp":"1692878460.0","poster":"maxdanny","comment_id":"989160","content":"Selected Answer: C\nhttps://www.exam-answer.com/the-best-way-to-measure-query-execution-time-in-bigquery","upvote_count":"1"},{"content":"Selected Answer: C\nThe correct answer is C. Use Stackdriver Trace to plot query execution time. Stackdriver Trace is a distributed tracing system that allows you to profile and debug your application's performance. It allows you to trace requests across multiple services, and it provides a detailed breakdown of where time is being spent within your application. Since you want to find out how much time your queries take to execute, using Stackdriver Trace to plot query execution time would be the most appropriate approach.","timestamp":"1680003180.0","comment_id":"853179","upvote_count":"1","poster":"Teraflow"},{"upvote_count":"1","content":"Selected Answer: D\nD is correct\nhttps://cloud.google.com/bigquery/docs/monitoring","timestamp":"1673277720.0","comment_id":"770566","poster":"telp"},{"poster":"tomato123","content":"Selected Answer: D\nD is correct","comment_id":"649218","upvote_count":"3","timestamp":"1660974420.0"}]}],"exam":{"provider":"Google","name":"Professional Cloud Developer","numberOfQuestions":338,"id":7,"isImplemented":true,"isMCOnly":false,"lastUpdated":"11 Apr 2025","isBeta":false},"currentPage":58},"__N_SSP":true}