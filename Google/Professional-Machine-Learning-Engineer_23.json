{"pageProps":{"questions":[{"id":"d7t7r7pdc83AvPWqBQIu","timestamp":"2024-01-13 19:26:00","isMC":true,"choices":{"A":"1. Create a Vertex AI managed dataset.\n2. Use a Vertex AI training pipeline to train your model.\n3. Generate batch predictions in Vertex AI.","C":"1. Upload your dataset to BigQuery.\n2. Use a Vertex AI custom training job to train your model.\n3. Generate predictions by using Vertex Al SDK custom prediction routines.","B":"1. Use a Vertex AI Pipelines custom training job component to tram your model.\n2. Generate predictions by using a Vertex AI Pipelines model batch predict component.","D":"1. Use Vertex AI Experiments to train your model.\n2. Register your model in Vertex AI Model Registry.\n3. Generate batch predictions in Vertex AI."},"url":"https://www.examtopics.com/discussions/google/view/131119-exam-professional-machine-learning-engineer-topic-1-question/","answer_images":[],"discussion":[{"content":"Selected Answer: D\nMy Answer: D\n\nAccording with: https://cloud.google.com/vertex-ai/docs/experiments/intro-vertex-ai-experiments\n\n“Vertex AI Experiments is a tool that helps you track and analyze different model architectures, hyperparameters, and training environments, letting you track the steps, inputs, and outputs of an experiment run. Vertex AI Experiments can also evaluate how your model performed in aggregate, against test datasets, and during the training run. You can then use this information to select the best model for your particular use case.”.\n\nConsidering that both options A and B could demonstrate some form of lineage, I believe option D is the most suitable. The text explicitly states \"show lineage for your model and predictions,\" which aligns perfectly with the functionality provided by Vertex AI Experiments.","comment_id":"1149740","upvote_count":"8","poster":"guilhermebutzke","timestamp":"1707874080.0"},{"timestamp":"1709901540.0","poster":"edoo","content":"Selected Answer: B\nVertex AI Pipelines are suited to do artifact lineage\nhttps://cloud.google.com/vertex-ai/docs/pipelines/lineage\nExperiments can do it also, but their main goal is to \"track and analyze different model architectures, hyperparameters, and training environments\"","upvote_count":"6","comment_id":"1168782"},{"upvote_count":"1","comment_id":"1399027","timestamp":"1742068440.0","poster":"bc3f222","content":"Selected Answer: D\nVertex AI Experiments helps track all your training runs, including:\nDataset version\nHyperparameters\nModel metrics\nCode version\nThis enables full lineage and traceability from data → training → model artifact."},{"comment_id":"1331976","timestamp":"1735227480.0","content":"Selected Answer: B\nAnswer is B\nD is wrong as there is only one model not models, experiments is used for multiple runs of a model/multiple models, also lineage is tracked using a pipeline","upvote_count":"1","poster":"Ankit267"},{"upvote_count":"1","poster":"rajshiv","timestamp":"1733193120.0","comment_id":"1321188","content":"Selected Answer: B\nVertex AI Pipelines will track the Model lineage while the batch prediction component in Vertex AI Pipelines will provide lineage tracking because each prediction is part of the pipeline and is connected to the corresponding training process."},{"upvote_count":"1","comment_id":"1318389","timestamp":"1732673940.0","poster":"AB_C","content":"Selected Answer: B\nVertex AI Pipeline for lineage tracking"},{"upvote_count":"1","timestamp":"1728673500.0","comment_id":"1296218","poster":"Foxy2021","content":"My answer is B."},{"upvote_count":"1","comment_id":"1282457","timestamp":"1726117200.0","content":"It's a bit ambiguously worded this question. Model lineage involves knowledge of the data it was trained on, so that should be A. That being said, I think the question is implying D from it's wording, experiment tracking. I went for A, but suspect it's wrong.","poster":"baimus"},{"content":"Selected Answer: D\nOption A/B doesn't mention anything about lineage. C is definitely wrong as there is no need to upload the dataset to Bigquery. \n\nOnly correct answer is D","comment_id":"1207928","upvote_count":"2","poster":"SahandJ","timestamp":"1715095620.0"},{"content":"Selected Answer: B\nrunning your custom model in production -> need pipeline\n-> B","timestamp":"1714132080.0","upvote_count":"1","comment_id":"1202570","poster":"pinimichele01"},{"comment_id":"1201484","poster":"cruise93","content":"Selected Answer: D\nAgree with guilhermebutzke","timestamp":"1713973200.0","upvote_count":"2"},{"comments":[{"poster":"pinimichele01","timestamp":"1712498100.0","content":"lineage of the model i think, not for data, so it's B","upvote_count":"1","comment_id":"1190992"}],"content":"Selected Answer: A\nA because to track lineage you need a managed dataset and vertex ai pipelines","timestamp":"1712147640.0","upvote_count":"1","comment_id":"1188660","poster":"Shark0"},{"comment_id":"1171082","poster":"Yan_X","timestamp":"1710168900.0","content":"Selected Answer: A\nA\nD cannot provide lineage for the source of your data. \nHas to be A to go with Vertex AI managed dataset.","upvote_count":"1"},{"timestamp":"1707141060.0","poster":"sonicclasps","content":"Selected Answer: A\nManaged data set to help track lineage \n https://cloud.google.com/vertex-ai/docs/training/using-managed-datasets","upvote_count":"1","comment_id":"1141144"},{"upvote_count":"5","content":"Selected Answer: B\nB) REF https://cloud.google.com/vertex-ai/docs/pipelines/lineage\n\nTrack the lineage of pipeline artifacts \n\nWhen you run a pipeline using Vertex AI Pipelines, the artifacts and parameters of your pipeline run are stored using Vertex ML Metadata. Vertex ML Metadata makes it easier to analyze the lineage of your pipeline's artifacts, by saving you the difficulty of keeping track of your pipeline's metadata.\n\nAn artifact's lineage includes all the factors that contributed to its creation, as well as artifacts and metadata that are derived from this artifact. For example, a model's lineage could include the following:\n\nThe training, test, and evaluation data used to create the model.\nThe hyperparameters used during model training.\nMetadata recorded from the training and evaluation process, such as the model's accuracy.\nArtifacts that descend from this model, such as the results of batch predictions.","timestamp":"1706794080.0","poster":"ddogg","comment_id":"1137645"},{"content":"Selected Answer: D\nD. Sample on how to keep track of experiments lineage -> https://cloud.google.com/vertex-ai/docs/experiments/user-journey/uj-model-training","upvote_count":"1","timestamp":"1705327080.0","comment_id":"1123422","poster":"b1a8fae"},{"content":"Selected Answer: B\nVertex AI Pipelines provides ability to track the lineage for your model and predictions","timestamp":"1705273080.0","comment_id":"1122940","poster":"BlehMaks","upvote_count":"1"},{"poster":"36bdc1e","content":"D\n“track the lineage of pipeline artifacts”. Vertex AI Experiments2 is a service that allows you to track and compare the results of your model training runs. Vertex AI Experiments automatically logs metadata such as hyperparameters, metrics, and artifacts for each training run.","upvote_count":"1","comment_id":"1121951","timestamp":"1705170360.0"}],"question_id":111,"answer":"B","question_images":[],"answers_community":["B (48%)","D (42%)","9%"],"question_text":"You are developing a process for training and running your custom model in production. You need to be able to show lineage for your model and predictions. What should you do?","unix_timestamp":1705170360,"topic":"1","answer_ET":"B","answer_description":"","exam_id":13},{"id":"Ee6sNcqnc5D6IODpbmgT","question_id":112,"url":"https://www.examtopics.com/discussions/google/view/55686-exam-professional-machine-learning-engineer-topic-1-question/","answer":"C","choices":{"B":"1. Build a tree-based classification model that predicts whether the shuttle should pick up passengers at each shuttle station. 2. Dispatch an available shuttle and provide the map with the required stops based on the prediction.","C":"1. Define the optimal route as the shortest route that passes by all shuttle stations with confirmed attendance at the given time under capacity constraints. 2. Dispatch an appropriately sized shuttle and indicate the required stops on the map.","A":"1. Build a tree-based regression model that predicts how many passengers will be picked up at each shuttle station. 2. Dispatch an appropriately sized shuttle and provide the map with the required stops based on the prediction.","D":"1. Build a reinforcement learning model with tree-based classification models that predict the presence of passengers at shuttle stops as agents and a reward function around a distance-based metric. 2. Dispatch an appropriately sized shuttle and provide the map with the required stops based on the simulated outcome."},"exam_id":13,"question_text":"Your organization wants to make its internal shuttle service route more efficient. The shuttles currently stop at all pick-up points across the city every 30 minutes between 7 am and 10 am. The development team has already built an application on Google Kubernetes Engine that requires users to confirm their presence and shuttle station one day in advance. What approach should you take?","answer_description":"","answer_images":[],"answer_ET":"C","isMC":true,"unix_timestamp":1624202220,"answers_community":["C (100%)"],"question_images":[],"discussion":[{"timestamp":"1624202220.0","comments":[{"timestamp":"1726853340.0","upvote_count":"15","poster":"sensev","content":"I agree with this, because it mentioned that they now \"require users to confirm their presence\". I think this is an example of when a classical routing algorithm is a better fit compare to ML-approach.","comment_id":"412454"}],"comment_id":"386354","content":"C: for all confirmed.","upvote_count":"24","poster":"nissili"},{"poster":"JPA210","comment_id":"1416610","content":"Selected Answer: C\nI agree with what is being said, that this use case is not for ML.","upvote_count":"1","timestamp":"1743493020.0"},{"poster":"VishnuCh","content":"Selected Answer: C\nThis is a route optimization problem not an machine learning problem.","upvote_count":"1","comment_id":"1387799","timestamp":"1741774440.0"},{"upvote_count":"2","timestamp":"1739253240.0","poster":"jimmygrand","comment_id":"1354856","content":"Selected Answer: C\nAnswer is C. This is a case where machine learning would be terrible, as it would not be 100% accurate and some passengers would not get picked up. A simple algorith works better here, and the question confirms customers will be indicating when they are at the stop so no ML required."},{"upvote_count":"3","content":"Answer is C. This is a case where machine learning would be terrible, as it would not be 100% accurate and some passengers would not get picked up. A simple algorith works better here, and the question confirms customers will be indicating when they are at the stop so no ML required.","timestamp":"1726853400.0","comment_id":"568286","poster":"baimus"},{"timestamp":"1717591260.0","comment_id":"1224714","upvote_count":"1","poster":"PhilipKoku","content":"Selected Answer: C\nC is the option that covers the scenario."},{"content":"Selected Answer: C\nC - Since we have the attendance list in advance. Tree-based classification, regression and reinforced learning sounds useless in this case.","upvote_count":"3","timestamp":"1701428040.0","poster":"fragkris","comment_id":"1085156"},{"poster":"Sum_Sum","comment_id":"1070393","content":"Selected Answer: C\nyou do not need to predict how many people will be at each station as the requirement mentions they have to register a day in advance","upvote_count":"1","timestamp":"1699967580.0"},{"upvote_count":"1","content":"Selected Answer: C\nWent with C","timestamp":"1683607920.0","comment_id":"892675","poster":"M25"},{"comment_id":"860651","content":"I think it should be C. I can easily eliminate D, this is not a case for reinforcement learning. Moreover, it seems like a Route Optimization rather than finding out best sized shuttle as mentioned in A or whether the shuttle should stop at a point as per point B.","upvote_count":"1","timestamp":"1680584220.0","poster":"n_shanthi"},{"comment_id":"839124","content":"Selected Answer: C\nThis is a route optimization problem","timestamp":"1678818300.0","upvote_count":"1","poster":"asava"},{"comment_id":"725152","timestamp":"1669210260.0","upvote_count":"3","content":"Selected Answer: C\nNo need to predict the presences since they are already confirmed, best thing we can do is optimize the route","poster":"EFIGO"},{"comment_id":"708325","poster":"abhi0706","content":"C. route more efficient is an optimization model","upvote_count":"1","timestamp":"1667210220.0"},{"upvote_count":"1","poster":"GCP72","comment_id":"647128","timestamp":"1660560960.0","content":"Selected Answer: C\nC is looks correct for me"},{"comment_id":"643703","upvote_count":"1","content":"Confirmed C","timestamp":"1659871440.0","poster":"Dr_Ethan"},{"timestamp":"1659036840.0","content":"Selected Answer: C\nC. route more efficient is an optimization model","poster":"enghabeth","comment_id":"638853","upvote_count":"2"},{"timestamp":"1652125680.0","upvote_count":"1","content":"Selected Answer: C\nAnswer is C","comment_id":"599266","poster":"David_ml"},{"poster":"David_ml","comment_id":"584987","timestamp":"1649822700.0","upvote_count":"1","content":"Selected Answer: C\ncorrect answer is C"},{"upvote_count":"1","poster":"giaZ","comment_id":"558878","timestamp":"1646158140.0","content":"C. Why would you want to predict anything here? The info on how many passengers will be and at which stations are already given by passengers themselves."},{"comment_id":"523807","timestamp":"1642202460.0","poster":"sid515","upvote_count":"1","content":"Selected Answer: C\nIt needs to be C. No use of ML here"},{"content":"C is CORRECT","poster":"VinodSangare","upvote_count":"1","timestamp":"1640748660.0","comment_id":"511752"},{"content":"I agree with C.","poster":"alphard","upvote_count":"2","comment_id":"495028","timestamp":"1638785400.0"},{"upvote_count":"2","content":"C is correct.","timestamp":"1637951640.0","poster":"JobQ","comment_id":"487558"},{"timestamp":"1634555820.0","content":"C because is following the first rule of Google's best practices for Machine Learning: https://developers.google.com/machine-learning/guides/rules-of-ml#rule_1_don%E2%80%99t_be_afraid_to_launch_a_product_without_machine_learning\n\nRule #1 Don't be afraid to launch a product without machine learning\n\nAnd this product doesn't need any ML models to work.","comment_id":"464076","poster":"mousseUwU","upvote_count":"4"}],"topic":"1","timestamp":"2021-06-20 17:17:00"},{"id":"qSbrjupWOGv7WF1wZ0vX","timestamp":"2021-06-02 22:24:00","answer_ET":"B","answer_description":"","discussion":[{"timestamp":"1624350000.0","comment_id":"387726","poster":"maartenalexander","upvote_count":"32","content":"B. Changing the scale tier does not impact performance–only speeds up training time. Epochs, Batch size, and learning rate all are hyperparameters that might impact model accuracy."},{"timestamp":"1740856020.0","comment_id":"1363668","poster":"bc3f222","content":"Selected Answer: B\nB is still correct as now scale-tier will be replaced by the exact machine config instead","upvote_count":"1"},{"upvote_count":"2","timestamp":"1729628940.0","content":"B is correct however this parameter looks like it is being deprecated.","comment_id":"1301716","poster":"DaleR"},{"comment_id":"1300593","poster":"desertlotus1211","timestamp":"1729443960.0","upvote_count":"2","content":"he scale-tier parameter in AI Platform determines the computing resources (e.g., CPU, GPU, or TPU) that are allocated for your training job. By increasing the scale-tier from basic to a more powerful tier (e.g., standard, premium, or custom), you can allocate more resources (like GPUs or TPUs) for your job. This will significantly reduce training time, especially for LSTM-based models that benefit from parallel processing on GPUs or TPUs.","comments":[{"timestamp":"1729443960.0","upvote_count":"1","content":"Answer B","poster":"desertlotus1211","comment_id":"1300595"}]},{"upvote_count":"1","timestamp":"1688713320.0","comment_id":"945425","poster":"SamuelTsch","content":"Selected Answer: B\nA, C, D could impact the accuracy. But B not."},{"timestamp":"1683608340.0","comment_id":"892696","poster":"M25","content":"Selected Answer: B\nWent with B","upvote_count":"1"},{"poster":"enghabeth","content":"Selected Answer: B\nA is incorrect, less training iteration will affect model performance.\n\nB is correct, cost is not a concern as it is not mentioned in the question, the scale tier can be upgraded to significantly minimize the training time.\n\nC is incorrect, wouldn’t affect training time, but would affect model performance.\n\nD is incorrect, the model might converge faster with higher learning rate, but this would affect the training routine and might cause exploding gradients.","timestamp":"1675730100.0","comment_id":"800396","upvote_count":"2"},{"comment_id":"772622","upvote_count":"1","poster":"ares81","content":"Selected Answer: B\nIt's B!","timestamp":"1673447040.0"},{"content":"Selected Answer: B\nA, C, D are all about hyperparameters that might impact model accuracy, while B is just about computing speed; so upgrading the scale tier will make the model faster with no chance of reducing accuracy.","comment_id":"725676","poster":"EFIGO","timestamp":"1669280160.0","upvote_count":"2"},{"upvote_count":"1","timestamp":"1660567680.0","comment_id":"647195","content":"Selected Answer: B\nCorrect answer is \"B\"","poster":"GCP72"},{"comment_id":"615482","content":"Selected Answer: B\n- using options elimination all options except B can harm the accuracy","poster":"Mohamed_Mossad","upvote_count":"3","timestamp":"1655064300.0"},{"upvote_count":"2","timestamp":"1649296320.0","content":"Selected Answer: B\nB for sure.","comment_id":"582109","poster":"morgan62"},{"content":"Selected Answer: B\nMight be hrlpfull https://cloud.google.com/ai-platform/training/docs/machine-types#scale_tiers\nGoogle may optimize the configuration of the scale tiers for different jobs over time, based on customer feedback and the availability of cloud resources. Each scale tier is defined in terms of its suitability for certain types of jobs. Generally, the more advanced the tier, the more machines are allocated to the cluster, and the more powerful the specifications of each virtual machine. As you increase the complexity of the scale tier, the hourly cost of training jobs, measured in training units, also increases. See the pricing page to calculate the cost of your job.","timestamp":"1645558500.0","comment_id":"553961","upvote_count":"1","poster":"igor_nov1"},{"timestamp":"1639323600.0","upvote_count":"3","comment_id":"500103","content":"A,C and D all point to hyper parameter tuning which is not the objective in the question.\n\nAs others have said - B is only way to improve the time to training the model.","poster":"ashii007"},{"upvote_count":"1","content":"Selected Answer: B\nexamtopics , Can we attach releveant docs why C ?","poster":"santy79","timestamp":"1637422200.0","comment_id":"482661"},{"poster":"mousseUwU","timestamp":"1634644740.0","comment_id":"464634","content":"Correct is B, scale-tier is the definition of what GPU will be used: https://cloud.google.com/ai-platform/training/docs/using-gpus","upvote_count":"3"},{"content":"Should be B.\nQuestion didn't say anything about cost, so while B would increase cost with more computation time, it would save real-world time.","upvote_count":"3","comment_id":"444285","timestamp":"1631586000.0","poster":"Y2Data"},{"upvote_count":"3","comment_id":"441644","content":"Go with B, all the other options could affect the accuracy.","poster":"Danny2021","timestamp":"1631134740.0"}],"answer_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/google/view/54305-exam-professional-machine-learning-engineer-topic-1-question/","question_text":"You are training an LSTM-based model on AI Platform to summarize text using the following job submission script: gcloud ai-platform jobs submit training $JOB_NAME \\\n--package-path $TRAINER_PACKAGE_PATH \\\n--module-name $MAIN_TRAINER_MODULE \\\n--job-dir $JOB_DIR \\\n--region $REGION \\\n--scale-tier basic \\\n-- \\\n--epochs 20 \\\n--batch_size=32 \\\n--learning_rate=0.001 \\\nYou want to ensure that training time is minimized without significantly compromising the accuracy of your model. What should you do?","choices":{"D":"Modify the 'learning rate' parameter.","A":"Modify the 'epochs' parameter.","B":"Modify the 'scale-tier' parameter.","C":"Modify the 'batch size' parameter."},"isMC":true,"question_id":113,"question_images":[],"answer":"B","answers_community":["B (100%)"],"exam_id":13,"unix_timestamp":1622665440},{"id":"xd51H3Y4IuFDjt6szKW8","timestamp":"2024-01-13 04:18:00","discussion":[{"comments":[{"upvote_count":"2","poster":"fitri001","comment_id":"1197856","timestamp":"1713435900.0","content":"A. Vision API - parseText: While the Vision API can extract text from PDFs, it wouldn't necessarily target the specific comments section without a custom parser.\nB. Natural Language API - analyzeEntitySentiment: This feature focuses on sentiment analysis for named entities within the text. It might not be ideal for overall satisfaction extraction from general customer comments."}],"poster":"fitri001","comment_id":"1197855","timestamp":"1713435840.0","upvote_count":"5","content":"Selected Answer: C\nDocument AI custom extractor: Since the layout of the feedback forms is consistent, training a custom extractor in Document AI allows for efficient and accurate extraction of the specific comments section. This ensures the Natural Language API receives the relevant text for sentiment analysis.\nNatural Language API - analyzeSentiment: This functionality within the Natural Language API is specifically designed to analyze sentiment in a piece of text. It provides an overall sentiment score that can be mapped to a satisfaction score (e.g., high positive sentiment translates to high satisfaction)."},{"content":"Selected Answer: C\nDocument AI best fit for this use case.","poster":"Kalai_1","timestamp":"1735401240.0","upvote_count":"1","comment_id":"1333037"},{"content":"Selected Answer: A\n\"quickly\" is the differentiator between A & C","timestamp":"1735032000.0","poster":"Ankit267","upvote_count":"1","comment_id":"1331062"},{"content":"Selected Answer: C\nDocumentAI is perfect for the case. Since the question says: \"overall satisfaction\", then entity is not needed.","upvote_count":"1","timestamp":"1733847600.0","poster":"Pau1234","comment_id":"1324653"},{"comment_id":"1321431","timestamp":"1733240280.0","upvote_count":"1","poster":"lunalongo","content":"Selected Answer: A\nIn summary, option A offers the optimal balance of speed, accuracy, and simplicity for this specific task. Using the pre-trained APIs is faster and requires less expertise than training a custom model. The analyzeSentiment function directly addresses the need for an overall satisfaction score.\n\nWhy not D? If speed is the absolute priority and the layout is truly consistent, the Vision API's speed might outweigh the potential for slightly improved accuracy from a custom extractor."},{"content":"My vote is a. It is simple and do the job.","comment_id":"1296220","poster":"Foxy2021","upvote_count":"1","timestamp":"1728673740.0"},{"poster":"AzureDP900","timestamp":"1719015360.0","upvote_count":"1","content":"C is right\nDocument AI custom extractor: Allows you to train a custom model to extract relevant information (in this case, customer comments) from the PDF files.\nNatural Language API analyzeSentiment feature: Analyzes the sentiment of the extracted text to predict an overall satisfaction score.","comment_id":"1235046"},{"timestamp":"1717706940.0","comment_id":"1225775","upvote_count":"2","content":"Selected Answer: A\nC & D are overkill\n\nWe don't care about entities sentiment -> B is out \n\nLeft with A and https://cloud.google.com/natural-language/docs/reference/rest/v1/documents/analyzeSentiment","poster":"bobjr"},{"comment_id":"1194917","upvote_count":"2","content":"Selected Answer: A\nquickly predict an overall satisfaction -> a","comments":[{"poster":"pinimichele01","upvote_count":"1","timestamp":"1714131960.0","comment_id":"1202569","content":"no sorrt, it's C, you need doc AI"}],"poster":"pinimichele01","timestamp":"1713007800.0"},{"poster":"edoo","content":"Selected Answer: A\nI go with A, because \"you need quickly predict\", no time for fine-tunning.","upvote_count":"3","comment_id":"1168788","timestamp":"1709902080.0"},{"content":"Selected Answer: C\nMy answer: Letter C\n\nDocument AI is a suitable tool for cases where there are patterns of forms or documentation. Additionally, it is possible to directly read PDF files. In the Natural Language API, the analyzeSentiment function can determine the overall sentiment, as the text asks, \"You need to quickly predict an overall satisfaction.\" The analyzeEntitySentiment function provides a score for each entity or word found.\nhttps://cloud.google.com/natural-language/docs/basics","upvote_count":"2","comment_id":"1149781","poster":"guilhermebutzke","timestamp":"1707877200.0"},{"timestamp":"1706794500.0","upvote_count":"1","comment_id":"1137654","poster":"ddogg","content":"Selected Answer: C\nDocument AI custom extractor: This allows you to tailor the text extraction specifically to the layout and format of your customer feedback forms, ensuring accurate capture of the comments section.\n\nNatural Language API analyzeSentiment: This feature analyzes the extracted text and provides an overall sentiment score, which can be used to gauge customer satisfaction."},{"comment_id":"1121267","content":"Selected Answer: C\nPrecision in text extraction: Document AI is specifically designed for extracting text from structured documents like forms, ensuring accurate extraction of comments, even with varying handwriting styles.\nCustom model for form layout: Training a custom extractor tailored to the hotel's feedback form layout further enhances accuracy and targets the relevant comments section effectively.\nSentiment analysis: Natural Language API's analyzeSentiment feature analyzes overall sentiment in a text block, aligning with the goal of deriving overall satisfaction scores.","poster":"pikachu007","upvote_count":"2","timestamp":"1705115880.0"}],"answer_ET":"C","answer_images":[],"unix_timestamp":1705115880,"exam_id":13,"url":"https://www.examtopics.com/discussions/google/view/131016-exam-professional-machine-learning-engineer-topic-1-question/","question_text":"You work for a hotel and have a dataset that contains customers’ written comments scanned from paper-based customer feedback forms, which are stored as PDF files. Every form has the same layout. You need to quickly predict an overall satisfaction score from the customer comments on each form. How should you accomplish this task?","answers_community":["C (57%)","A (43%)"],"choices":{"C":"Uptrain a Document AI custom extractor to parse the text in the comments section of each PDF file. Use the Natural Language API analyzeSentiment feature to infer overall satisfaction scores.","B":"Use the Vision API to parse the text from each PDF file. Use the Natural Language API analyzeEntitySentiment feature to infer overall satisfaction scores.","A":"Use the Vision API to parse the text from each PDF file. Use the Natural Language API analyzeSentiment feature to infer overall satisfaction scores.","D":"Uptrain a Document AI custom extractor to parse the text in the comments section of each PDF file. Use the Natural Language API analyzeEntitySentiment feature to infer overall satisfaction scores."},"answer_description":"","answer":"C","topic":"1","isMC":true,"question_id":114,"question_images":[]},{"id":"tU2G4Iizv4XG25LmboZo","url":"https://www.examtopics.com/discussions/google/view/131017-exam-professional-machine-learning-engineer-topic-1-question/","answer_description":"","answers_community":["A (52%)","C (29%)","B (19%)"],"answer":"A","choices":{"B":"Add the {\"kubeflow.v1.caching\": True} parameter to the set of params provided to your PipelineJob.","C":"Move the first step of your pipeline to a separate step, and provide a cached path to Cloud Storage as an input to the main pipeline.","D":"Change the name of the pipeline to f\"my-awesome-pipeline-{dt}\".","A":"Change the components’ YAML filenames to export.yaml, preprocess,yaml, f \"train-\n{dt}.yaml\", f\"calibrate-{dt).vaml\"."},"discussion":[{"content":"Selected Answer: A\nMy Answer: A\n\nFrom what I understood, it's about optimizing the process of adjusting code while utilizing previously processed results from the pipeline. Kubeflow inherently caches these steps, eliminating the need to explicitly store results in a designated path.\n\nHowever, the original filenames include a timestamp (**`-dt`**), suggesting that by removing this timestamp, the pipeline steps might not rerun as expected.\n\nOption C could be an approach, but it would require more effort to implement (since Kubeflow handles it automatically). Additionally, the beginning of the option only mentions moving the first step, which is the export, and doesn't say anything about preprocessing (which could be one of the more expensive steps).\n\nSo, considering all of these factors, I think A is the best choice.\"","timestamp":"1707880320.0","comment_id":"1149793","poster":"guilhermebutzke","upvote_count":"6"},{"content":"Selected Answer: B\n**Use Option B** (`{\"kubeflow.v1.caching\": True}`) to enable caching in your Vertex AI pipeline. This is the most efficient and cost-effective way to avoid redundant executions of expensive steps like data export and preprocessing.","comment_id":"1387466","timestamp":"1741702560.0","poster":"HaroonRaizada01","upvote_count":"1"},{"content":"Selected Answer: B\nAdding caching to your pipeline by setting the parameter {\"kubeflow.v1.caching\": True} is the most efficient and effective approach to reduce model development costs, particularly for steps like data export and preprocessing, which are often time-consuming and costly to repeat during multiple iterations. This will help you avoid unnecessary re-computation and save on resource usage.","comment_id":"1339417","poster":"Sivaram06","upvote_count":"1","timestamp":"1736666640.0"},{"upvote_count":"2","content":"Selected Answer: C\nOption A is a superficial change with no significant impact on cost optimization. Option C is the correct approach for effectively leveraging caching to reduce costs.\n\nC strategically uses the caching mechanism by separating the expensive preprocessing steps and storing their outputs in Cloud Storage, thus reducing costs by reusing the preprocessed data across multiple pipeline runs.\n\nChanging filenames could affect caching only if the caching mechanism relies on exact filename matching, which is unlikely. Besides, Kubeflow and Vertex AI Pipelines do not automatically handle caching of intermediate results; it is not inherent to the pipeline steps themselves; it's a feature that needs to be explicitly managed and leveraged.","poster":"lunalongo","comment_id":"1321441","timestamp":"1733241900.0"},{"content":"Selected Answer: A\nA. The dynamic filename is causing kubeflow to be unable to cache the export and preprocess steps, causing the problems mentioned in the question.","upvote_count":"1","poster":"f084277","comment_id":"1312345","timestamp":"1731628680.0"},{"comment_id":"1296227","upvote_count":"1","poster":"Foxy2021","timestamp":"1728674520.0","content":"I select C:\nBy leveraging a Dataproc cluster, you can maintain compatibility with your existing PySpark jobs, minimize management overhead, and create a scalable proof of concept quickly and efficiently."},{"timestamp":"1728674280.0","content":"I select B.\nA:\n\nChanging the YAML filenames does not affect caching behavior or cost reduction. The pipeline's efficiency and cost effectiveness are primarily governed by how it handles inputs and outputs rather than the filenames of the components.\nC:\n\nMoving the first step to a separate pipeline may help with organization but doesn’t directly address the cost incurred by repeated data exports and preprocessing. Also, simply providing a cached path does not guarantee that the preprocessing step itself won’t be executed multiple times.\nD:\n\nChanging the name of the pipeline to include a timestamp or other identifier does not influence caching or resource usage. It merely alters the identification of the pipeline runs without any impact on the efficiency of the operations being performed.","poster":"Foxy2021","upvote_count":"1","comment_id":"1296223"},{"comment_id":"1197340","content":"Selected Answer: A\nsee guilhermebutzke","upvote_count":"1","poster":"gscharly","timestamp":"1713371760.0"},{"poster":"pinimichele01","upvote_count":"1","comment_id":"1196517","content":"Selected Answer: A\nsee guilhermebutzke","timestamp":"1713262860.0"},{"content":"Selected Answer: C\nC\nCaching should be enabled for all steps, e.g., export, preprocessing and training.","timestamp":"1710165540.0","upvote_count":"1","comment_id":"1171056","poster":"Yan_X"},{"comment_id":"1124612","content":"Selected Answer: C\nNot A - Changing file names does not help with reducing costs\nNot B - you cannot directly use kubeflow.v1.caching on a pipeline that uses the KubeFlow v2 API. Version Incompatibility: The kubeflow.v1.caching module is specifically designed for KubeFlow Pipelines v1, and its structure and functionality are not directly compatible with KubeFlow Pipelines v2.\nso best option here is C","timestamp":"1705451820.0","poster":"shadz10","upvote_count":"2"},{"poster":"b1a8fae","upvote_count":"1","content":"Selected Answer: C\nI considered B but a search of \"kubeflow.v1.caching\" on Google only produces 1 result, which is this very question on this very website. Thus, I rule it out as non-existent (please share a resource if there is any that proves it exists) and opt for C.","timestamp":"1705329240.0","comment_id":"1123441"},{"timestamp":"1705324680.0","poster":"BlehMaks","comments":[{"poster":"BlehMaks","timestamp":"1705324860.0","content":"3)i'm not sure but may be it does matter that KubeFlow v2 API and kubeflow.v1.caching have different versions (v1 and v2)","upvote_count":"1","comment_id":"1123396"}],"content":"Selected Answer: A\ni think it's A. \n1)if we want to use the same results several times we shouldn't rename them. so we need to delete {dt} from the first two components names.\n2)we already have this option enable_caching = True, why do we need kubeflow.v1.caching then?\n3)i'm not sure but may be it does metter","comment_id":"1123393","upvote_count":"2"},{"timestamp":"1705116060.0","upvote_count":"2","poster":"pikachu007","content":"Selected Answer: B\nEnables caching: Setting this parameter instructs Vertex AI Pipelines to cache the outputs of pipeline steps that have successfully completed. This means that if a step's inputs haven't changed, its execution can be skipped, reusing the cached output instead.\nTargets costly steps: The prompt highlights that data export and preprocessing steps are particularly expensive. Caching these steps can significantly reduce costs during model iterations.","comment_id":"1121269"}],"timestamp":"2024-01-13 04:21:00","answer_images":[],"unix_timestamp":1705116060,"exam_id":13,"question_id":115,"topic":"1","question_text":"You developed a Vertex AI pipeline that trains a classification model on data stored in a large BigQuery table. The pipeline has four steps, where each step is created by a Python function that uses the KubeFlow v2 API. The components have the following names:\n\n//IMG//\n\n\nYou launch your Vertex AI pipeline as the following:\n\n//IMG//\n\n\nYou perform many model iterations by adjusting the code and parameters of the training step. You observe high costs associated with the development, particularly the data export and preprocessing steps. You need to reduce model development costs. What should you do?","question_images":["https://img.examtopics.com/professional-machine-learning-engineer/image3.png","https://img.examtopics.com/professional-machine-learning-engineer/image4.png"],"isMC":true,"answer_ET":"A"}],"exam":{"provider":"Google","name":"Professional Machine Learning Engineer","id":13,"lastUpdated":"11 Apr 2025","isMCOnly":true,"numberOfQuestions":304,"isBeta":false,"isImplemented":true},"currentPage":23},"__N_SSP":true}