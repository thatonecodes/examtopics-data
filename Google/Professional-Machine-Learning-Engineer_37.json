{"pageProps":{"questions":[{"id":"Jjr5uPoCMiHJqBvmA47d","answers_community":["B (83%)","A (17%)"],"question_images":[],"answer_ET":"B","answer":"B","exam_id":13,"timestamp":"2024-01-13 15:45:00","url":"https://www.examtopics.com/discussions/google/view/131098-exam-professional-machine-learning-engineer-topic-1-question/","question_text":"You are developing a recommendation engine for an online clothing store. The historical customer transaction data is stored in BigQuery and Cloud Storage. You need to perform exploratory data analysis (EDA), preprocessing and model training. You plan to rerun these EDA, preprocessing, and training steps as you experiment with different types of algorithms. You want to minimize the cost and development effort of running these steps as you experiment. How should you configure the environment?","answer_description":"","discussion":[{"upvote_count":"6","comment_id":"1128681","poster":"b1a8fae","timestamp":"1705930680.0","content":"Selected Answer: B\n\"Managed notebooks are usually a good choice if you want to use a notebook for data exploration, analysis, modeling, or as part of an end-to-end data science workflow.\n\nManaged notebooks instances let you perform workflow-oriented tasks without leaving the JupyterLab interface. They also have many integrations and features for implementing your data science workflow.\"\n\nvs. \n\n\"User-managed notebooks can be a good choice for users who require extensive customization or who need a lot of control over their environment.\"\n\nSeems more like the former -> B"},{"comment_id":"1242818","poster":"AzureDP900","content":"B is right because this option allows you to minimize cost and development effort by using a managed notebook in Vertex AI Workbench, which integrates well with BigQuery and Cloud Storage. You can browse and query your data directly within the JupyterLab interface without having to create a separate BigQuery client or use the bq command-line tool.","timestamp":"1720188300.0","upvote_count":"2"},{"upvote_count":"1","comment_id":"1199580","poster":"pinimichele01","content":"Selected Answer: B\nsee b1a8fae","timestamp":"1713692400.0"},{"timestamp":"1713631140.0","upvote_count":"1","poster":"gscharly","content":"Selected Answer: A\nagree with guilhermebutzke. Also, this option is easier to reuse in multiple experiments","comment_id":"1199264"},{"timestamp":"1708124100.0","upvote_count":"1","content":"Selected Answer: A\nMy Answer: A\n\nA: Default VM instance is the best to minimize the cost, and the command %%bigquery magic is the most easy way to get data from BQ. \n\nB: Not necessary JupyerLab interface to run code. The %%bigquerv magic commands is sufficient to get data and run easily queries. \n\nC: Dataproc Hub seems overkill and it is more expensive than a default VM instance. \n\nC: spark-bigquery-connector unnecessary to get tables in the notebook. better use %%bigquery.","comment_id":"1152273","poster":"guilhermebutzke"},{"timestamp":"1705986000.0","content":"Selected Answer: B\nhttps://cloud.google.com/bigquery/docs/visualize-jupyter","poster":"daidai75","comment_id":"1129206","upvote_count":"1"},{"poster":"shadz10","timestamp":"1705546980.0","upvote_count":"1","comment_id":"1125515","content":"Selected Answer: B\nhttps://cloud.google.com/vertex-ai/docs/workbench/notebook-solution#:~:text=For%20users%20who%20have%20specific,user%2Dmanaged%20notebooks%20instance's%20VM."},{"upvote_count":"1","comment_id":"1121756","timestamp":"1705157100.0","poster":"pikachu007","content":"Selected Answer: B\nOption A: User-managed notebooks require VM instance management, adding cost and complexity. %%bigquery magic commands are still needed.\nOption C: Dataproc Hub adds unnecessary cost and complexity for simple BigQuery interactions.\nOption D: Spark-bigquery-connector adds complexity and overhead compared to the native BigQuery integration in managed notebooks."}],"answer_images":[],"topic":"1","isMC":true,"choices":{"D":"Create a Vertex AI Workbench managed notebook on a Dataproc cluster, and use the spark-bigquery-connector to access the tables.","B":"Create a Vertex AI Workbench managed notebook to browse and query the tables directly from the JupyterLab interface.","C":"Create a Vertex AI Workbench user-managed notebook on a Dataproc Hub, and use the %%bigquery magic commands in Jupyter to query the tables.","A":"Create a Vertex AI Workbench user-managed notebook using the default VM instance, and use the %%bigquerv magic commands in Jupyter to query the tables."},"question_id":181,"unix_timestamp":1705157100},{"id":"QFEaRB5JkDScdFsCF4tL","url":"https://www.examtopics.com/discussions/google/view/131099-exam-professional-machine-learning-engineer-topic-1-question/","isMC":true,"unix_timestamp":1705157220,"answer_description":"","answers_community":["B (58%)","A (33%)","8%"],"question_text":"You recently deployed a model to a Vertex AI endpoint and set up online serving in Vertex AI Feature Store. You have configured a daily batch ingestion job to update your featurestore. During the batch ingestion jobs, you discover that CPU utilization is high in your featurestore’s online serving nodes and that feature retrieval latency is high. You need to improve online serving performance during the daily batch ingestion. What should you do?","choices":{"C":"Enable autoscaling for the prediction nodes of your DeployedModel in the Vertex AI endpoint","B":"Enable autoscaling of the online serving nodes in your featurestore","A":"Schedule an increase in the number of online serving nodes in your featurestore prior to the batch ingestion jobs","D":"Increase the worker_count in the ImportFeatureValues request of your batch ingestion job"},"answer_ET":"B","answer_images":[],"question_id":182,"exam_id":13,"timestamp":"2024-01-13 15:47:00","discussion":[{"upvote_count":"1","timestamp":"1735521300.0","content":"Selected Answer: B\nAnswer is B, coz how do you predict there will be a problem before the batch ingestion job?\n\nSeems preemptive and may be unnecessary.\n\nB aligns more coz error has happened and now you enabling autoscaling, so that in the future it will autoscale.","comment_id":"1333835","poster":"mouthwash"},{"upvote_count":"1","content":"Selected Answer: A\nAgree with bobjr","comment_id":"1265515","poster":"tardigradum","timestamp":"1723613820.0"},{"upvote_count":"1","poster":"Prakzz","timestamp":"1719975900.0","comment_id":"1241129","content":"Selected Answer: A\nhttps://cloud.google.com/vertex-ai/docs/featurestore/managing-featurestores\nSpecifically mentioned here that --> If CPU utilization is consistently high, consider increasing the number of online serving nodes for your featurestore."},{"upvote_count":"2","content":"Selected Answer: A\nGemini + Perplexity ai + ChatGPT votes A\n\nBecause : B. Enable Autoscaling: While autoscaling can be useful, it might not react quickly enough to sudden spikes in traffic during batch ingestion. Scheduling the increase ensures that the resources are available when needed.","poster":"bobjr","timestamp":"1717529760.0","comment_id":"1224322"},{"timestamp":"1713979020.0","comments":[{"comment_id":"1202643","content":"\"CPU utilization is high in your featurestore’s online serving nodes\"","upvote_count":"1","timestamp":"1714140120.0","poster":"pinimichele01"}],"content":"Selected Answer: D\nThis question is valid for the Legacy feature store.\nhttps://cloud.google.com/vertex-ai/docs/featurestore/ingesting-batch#import_job_performance","poster":"cruise93","comment_id":"1201536","upvote_count":"1"},{"content":"Selected Answer: B\nhttps://cloud.google.com/vertex-ai/docs/featurestore/managing-featurestores?&_gl=1*sswg5e*_ga*NDE2OTc3OTAzLjE3MDU4OTQ5OTE.*_ga_WH2QY8WWF5*MTcwNTkzNDM0NS40LjAuMTcwNTkzNDM0NS4wLjAuMA..&_ga=2.242492743.-416977903.1705894991#online_serving_nodes","poster":"daidai75","upvote_count":"2","timestamp":"1705990320.0","comment_id":"1129223"},{"timestamp":"1705931280.0","upvote_count":"2","poster":"b1a8fae","comment_id":"1128688","content":"Selected Answer: B\nVertex AI Feature Store provides two options for online serving: Bigtable and optimized online serving. Both options support autoscaling, which means that the number of online serving nodes can automatically adjust to the traffic demand. By enabling autoscaling, you can improve the online serving performance and reduce the feature retrieval latency during the daily batch ingestion. Autoscaling also helps you optimize the cost and resource utilization of your featurestore."},{"comment_id":"1121757","comments":[{"upvote_count":"1","poster":"iieva","comments":[{"content":"Yes passed","comment_id":"1125656","poster":"pikachu007","timestamp":"1705570080.0","upvote_count":"5"}],"content":"Hey Pikachu, \ndid you pass the exam or are you preparing? I am as well preparing and I have noticed that in many question you chose the same answer I would chose, but which is not the indicated answer of my Udemy Course Exam Preparation. \nThanks and Best","timestamp":"1705405860.0","comment_id":"1124162"},{"content":"Hi @pikachu007 May I ask when did you pass the exam? was it after they updated the new questions? I need to take the exam ASAP, just want to make sure the new questions are valid?","timestamp":"1706246580.0","upvote_count":"1","comment_id":"1132276","poster":"Sunny_M"}],"upvote_count":"2","timestamp":"1705157220.0","poster":"pikachu007","content":"Selected Answer: B\nOption A: Manually scheduling node increases requires prior knowledge of batch ingestion times and might not be as responsive to unexpected workload spikes.\nOption C: Autoscaling prediction nodes in the Vertex AI endpoint might help with model prediction latency but doesn't directly address feature retrieval latency from the featurestore.\nOption D: Increasing worker_count in the batch ingestion job could speed up ingestion but might further strain online serving nodes, potentially worsening latency."}],"topic":"1","question_images":[],"answer":"B"},{"id":"13TelGHcOutHHgcYDP7i","answer_ET":"C","question_text":"You are developing a custom TensorFlow classification model based on tabular data. Your raw data is stored in BigQuery. contains hundreds of millions of rows, and includes both categorical and numerical features. You need to use a MaxMin scaler on some numerical features, and apply a one-hot encoding to some categorical features such as SKU names. Your model will be trained over multiple epochs. You want to minimize the effort and cost of your solution. What should you do?","timestamp":"2024-01-13 16:02:00","answer_description":"","answer_images":[],"exam_id":13,"question_id":183,"isMC":true,"answer":"C","answers_community":["C (47%)","D (28%)","B (25%)"],"choices":{"C":"1. Use TFX components with Dataflow to encode the text features and scale the numerical features.\n2. Export results to Cloud Storage as TFRecords.\n3. Feed the data into Vertex AI Training.","A":"1. Write a SQL query to create a separate lookup table to scale the numerical features.\n2. Deploy a TensorFlow-based model from Hugging Face to BigQuery to encode the text features.\n3. Feed the resulting BigQuery view into Vertex AI Training.","D":"1. Write a SQL query to create a separate lookup table to scale the numerical features.\n2. Perform the one-hot text encoding in BigQuery.\n3. Feed the resulting BigQuery view into Vertex AI Training.","B":"1. Use BigQuery to scale the numerical features.\n2. Feed the features into Vertex AI Training.\n3. Allow TensorFlow to perform the one-hot text encoding."},"url":"https://www.examtopics.com/discussions/google/view/131100-exam-professional-machine-learning-engineer-topic-1-question/","unix_timestamp":1705158120,"question_images":[],"topic":"1","discussion":[{"content":"Selected Answer: C\n\"Full-pass stateful transformations aren't suitable for implementation in BigQuery. If you use BigQuery for full-pass transformations, you need auxiliary tables to store quantities needed by stateful transformations, such as means and variances to scale numerical features. Further, implementation of full-pass transformations using SQL on BigQuery creates increased complexity in the SQL scripts, and creates intricate dependency between training and the scoring SQL scripts.\"\nhttps://www.tensorflow.org/tfx/guide/tft_bestpractices#where_to_do_preprocessing","comment_id":"1203156","comments":[{"content":"only requirement is minmax scaling, not mean or variance.\nD, why need extra component like Cloud Storage, Vertex AI when it could be done easily in BQ where already the raw data is stored","comment_id":"1333030","timestamp":"1735400160.0","upvote_count":"1","poster":"Ankit267"},{"comment_id":"1241136","timestamp":"1719976740.0","upvote_count":"1","poster":"Prakzz","content":"Isn't Dataflow includes a lot of effort as the question asking to minimize the effort here?"}],"timestamp":"1714228440.0","upvote_count":"7","poster":"b2aaace"},{"content":"Selected Answer: B\nWith multiple epochs, the data is passed through the model multiple times. If you pre-encoded the categorical features (as in options C and D), you would be storing and repeatedly reading a much larger dataset (due to the one-hot encoding). This significantly increases storage costs and I/O overhead. By performing the one-hot encoding within TensorFlow during training (as in option B), the encoding happens on-the-fly for each batch of data during each epoch.","comment_id":"1334703","poster":"thescientist","timestamp":"1735634640.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"1333027","content":"Selected Answer: D\nB & D as top 2 choices, C is including Dataflow unnecessarily.\nD as \"minimize the effort and cost of your solution\", still some room for B but I selected D","timestamp":"1735399920.0","poster":"Ankit267"},{"poster":"pipefaxaf","content":"Selected Answer: D\nOption D minimizes effort and cost by using BigQuery to handle both the scaling and one-hot encoding. BigQuery is efficient for these types of preprocessing tasks, especially when dealing with large datasets.\nBy preparing the data in BigQuery, you avoid the need to export data to other services or use additional resources for preprocessing, such as Dataflow.\nThis approach provides a streamlined workflow by creating a preprocessed view in BigQuery, which can then be directly fed into Vertex AI Training without extra transformation steps. This helps optimize cost and simplicity while handling large tabular data effectively.","comment_id":"1307523","comments":[{"content":"Agree with pipefaxal. Minimize effort is key here.","timestamp":"1732737000.0","comment_id":"1318867","poster":"DaleR","upvote_count":"1"}],"timestamp":"1730832660.0","upvote_count":"3"},{"timestamp":"1728771300.0","upvote_count":"2","comment_id":"1296651","content":"Selected Answer: C\nmultiple epochs --> need to persist data after preprocessing","poster":"YangG"},{"upvote_count":"2","comment_id":"1284261","content":"Selected Answer: D\nOption D since it says minimize effort and cost following that adding something rather than BQ will increase complexity.","timestamp":"1726428120.0","poster":"wences"},{"content":"Option C uses TFX (TensorFlow Extended) components with Dataflow, which is a great way to perform complex data preprocessing tasks like one-hot encoding and scaling.\nThis approach allows you to process your data in a scalable and efficient manner, using Cloud Storage as the output location.\nBy exporting the results as TFRecords, you can easily feed this preprocessed data into Vertex AI Training for model development.","upvote_count":"1","timestamp":"1720188960.0","poster":"AzureDP900","comment_id":"1242825"},{"content":"Selected Answer: C\nagree with TFX components with Dataflow","timestamp":"1719914880.0","upvote_count":"1","poster":"dija123","comment_id":"1240670"},{"content":"Selected Answer: D\nGPT says D, Gemini says B, Perplexity says C....\n\nI say D : stay in one tool, BQ, which is cheap and natively scalable.\n\nB has a risk of out of memory error.","timestamp":"1717530180.0","upvote_count":"4","comment_id":"1224323","poster":"bobjr"},{"content":"Selected Answer: B\nBigQuery for Preprocessing:\nBigQuery is a serverless data warehouse optimized for large datasets.expand_more It can handle scaling numerical features using built-in functions like SCALE or QUANTILE_SCALE, reducing the need for complex custom logic or separate lookup tables.\nTensorFlow for One-Hot Encoding:\nTensorFlow excels at in-memory processing. One-hot encoding of categorical features, especially text features like SKU names, can be efficiently performed within your TensorFlow model during training. This avoids unnecessary data movement or transformations in BigQuery.\nVertex AI Training:\nBy feeding the preprocessed data (scaled numerical features) directly into Vertex AI Training, you leverage its managed infrastructure for training your custom TensorFlow model.","timestamp":"1714209120.0","comments":[{"poster":"fitri001","upvote_count":"1","timestamp":"1714209120.0","content":"Option A: Creates unnecessary complexity and data movement. BigQuery is better suited for scaling numerical features, and TensorFlow is efficient for one-hot encoding.\nOption C: TFX is a powerful framework for complex pipelines, but for a simpler scenario like this, it might be an overkill. Additionally, exporting data as TFRecords adds an extra step, potentially increasing cost and complexity.\nOption D: One-hot encoding in BigQuery might be cumbersome for textual features like SKU names.\n\npen_spark\nexclamation It can be computationally expensive and result in data explosion. TensorFlow handles this efficiently within the model.","comment_id":"1203037"}],"upvote_count":"1","comment_id":"1203036","poster":"fitri001"},{"comment_id":"1201538","timestamp":"1713979200.0","content":"Selected Answer: C\nAgree with b1a8fae","poster":"cruise93","upvote_count":"1"},{"timestamp":"1713631380.0","content":"Selected Answer: C\nagree with daidai75","comment_id":"1199269","poster":"gscharly","comments":[{"poster":"pinimichele01","comment_id":"1202644","timestamp":"1714140240.0","upvote_count":"1","content":"Option B is not suitable for the big volume of data processing?????\nBQ is not suitable for big volume??..\n\nfor me is B"}],"upvote_count":"2"},{"timestamp":"1708304160.0","upvote_count":"4","poster":"guilhermebutzke","content":"Selected Answer: B\nMy Answer: B\n\n1. Use BigQuery to scale the numerical features.: Simpler and cheaper then use TFX components with Dataflow to scale the numerical features\n2. Feed the features into Vertex AI Training.\n3. Allow TensorFlow to perform the one-hot text encoding: TensorFlow handles the one-hot text encoding better than BQ.","comment_id":"1153644"},{"content":"Selected Answer: C\nkey messages: \"contains hundreds of millions of rows, and includes both categorical and numerical features. You need to use a MaxMin scaler on some numerical features, and apply a one-hot encoding to some categorical features such as SKU names\". \nOption B is not suitable for the big volume of data processing. Option C is better.","timestamp":"1705990800.0","poster":"daidai75","comment_id":"1129228","upvote_count":"2"},{"upvote_count":"2","content":"Selected Answer: C\nInclined to choose C over B. By using TFX components with Dataflow, you can perform feature engineering on large-scale tabular data in a distributed and efficient way. You can use the Transform component to apply the MaxMin scaler and the one-hot encoding to the numerical and categorical features, respectively. You can also use the ExampleGen component to read data from BigQuery and the Trainer component to train your TensorFlow model.","poster":"b1a8fae","timestamp":"1705931820.0","comment_id":"1128700"},{"poster":"pikachu007","upvote_count":"3","comment_id":"1121772","content":"Selected Answer: B\nOption A: Involves creating a separate lookup table and deploying a Hugging Face model in BigQuery, increasing complexity and cost.\nOption C: While TFX offers robust preprocessing capabilities, it adds overhead for this use case and requires knowledge of Dataflow.\nOption D: Performing one-hot encoding in BigQuery can be less efficient than TensorFlow's optimized implementation.","timestamp":"1705158120.0"}]},{"id":"uSvdaVvBh45DfevlBwr5","answer_ET":"D","question_text":"You work for a retail company. You have been tasked with building a model to determine the probability of churn for each customer. You need the predictions to be interpretable so the results can be used to develop marketing campaigns that target at-risk customers. What should you do?","answer_images":[],"answer_description":"","timestamp":"2024-01-13 16:06:00","exam_id":13,"question_id":184,"isMC":true,"answer":"D","answers_community":["D (66%)","B (34%)"],"choices":{"D":"Build a random forest classification model in a Vertex AI Workbench notebook instance. Configure the model to generate feature importances after the model is trained.","B":"Build an AutoML tabular regression model. Configure the model to generate explanations when it makes predictions.","A":"Build a random forest regression model in a Vertex AI Workbench notebook instance. Configure the model to generate feature importances after the model is trained.","C":"Build a custom TensorFlow neural network by using Vertex AI custom training. Configure the model to generate explanations when it makes predictions."},"url":"https://www.examtopics.com/discussions/google/view/131101-exam-professional-machine-learning-engineer-topic-1-question/","unix_timestamp":1705158360,"question_images":[],"topic":"1","discussion":[{"upvote_count":"1","poster":"GCP_ML","timestamp":"1743495360.0","content":"Selected Answer: D\nChurn Prediction, therefore classification model not regression. Using softmax or sigmoid you can you can predict probabilities with random forest classification","comment_id":"1416624"},{"timestamp":"1735756680.0","poster":"VinnyD","upvote_count":"2","comment_id":"1335260","content":"Selected Answer: B\nModel has been asked to provide Probability and this Regression. AutoML models do provide explainability"},{"timestamp":"1735400400.0","content":"Selected Answer: D\nChurn Prediction, therefore classification model not regression","comment_id":"1333034","poster":"Ankit267","upvote_count":"2"},{"poster":"Omi_04040","upvote_count":"2","content":"Selected Answer: D\nRecommends constructing a random forest classification model within a Vertex AI Workbench notebook instance and configuring it to generate feature importances post-training, aligns perfectly with the requirement as it correctly identifies the task as a classification problem and offers high interpretability through feature importances, making it the best choice for targeting interventions in a retail context to reduce customer churn.","timestamp":"1733815740.0","comment_id":"1324421"},{"comment_id":"1296649","content":"Selected Answer: B\nProbability --> regression model","poster":"YangG","timestamp":"1728771300.0","upvote_count":"1"},{"timestamp":"1726411020.0","poster":"wences","upvote_count":"2","content":"Selected Answer: D\nChurn probability is required; linear regression will give the label, and classification will provide the likelihood as requested.","comment_id":"1284171"},{"upvote_count":"3","content":"Selected Answer: D\nWe can't use AutoML due to the lack of explicability. AutoML is a black box, and we can't know which model is GCP using under the hood:\n\nWhether is true that you can use the feature importance tool when using AutoML, GCP doesn't publicly disclose the specific models used internally for each type of problem (classification, regression, etc.). AutoML employs a wide range of algorithms, from linear models and decision trees to more complex neural networks. Consequently, the lack of explicability lead us to discard any AutoML option.\n\nRegarding the classification/regression discussion, as Roulle says \"Churn problems are cases of classification. We don't predict the label, but the probability of belonging to a given class (churn or not). We then set a threshold to indicate the probability at which we can affirm that the person will or will not unsubscribe.\"","comment_id":"1264169","poster":"tardigradum","timestamp":"1723382160.0"},{"comment_id":"1241329","upvote_count":"3","content":"Selected Answer: D\nChurn problems are cases of classification. We don't predict the label, but the probability of belonging to a given class (churn or not). We then set a threshold to indicate the probability at which we can affirm that the person will or will not unsubscribe.\nWe can eliminate all responses that mention regression (A & B).\n\nA random forest is therefore less complex to interpret than a neural network.\n\nSo I'm pretty sure it's D","poster":"Roulle","timestamp":"1720002420.0"},{"comment_id":"1198453","content":"agree with Yan_X. This is a classification problem, so regression should not be used (rule out A&B). Neural networks don't have explainable features by default, and Random Forest provides global explanations...","upvote_count":"1","comments":[{"poster":"pinimichele01","timestamp":"1713692640.0","content":"probability of churn for each customer......","upvote_count":"1","comment_id":"1199581"}],"poster":"gscharly","timestamp":"1713515520.0"},{"timestamp":"1713232740.0","poster":"fitri001","content":"Selected Answer: B\nSince interpretability is key for your churn prediction model to inform marketing campaigns, \n--> Choose an interpretable model:\nLogistic Regression: This is a classic choice for interpretability. It provides coefficients for each feature, indicating how a unit increase in that feature impacts the probability of churn. Easy to understand and implement, it's a good starting point.\nDecision Trees with Rule Extraction: Decision trees are inherently interpretable, with each branch representing a decision rule. By extracting these rules, you can understand the specific factors leading to churn (e.g., \"Customers with low tenure and high number of support tickets are more likely to churn\").","comment_id":"1196293","upvote_count":"2"},{"timestamp":"1712640420.0","comment_id":"1191999","poster":"pinimichele01","content":"Selected Answer: B\nthe probability of churn for each customer -> regression -> B","upvote_count":"1"},{"content":"I don't know which one is correct...\nAs D is 'after the model is trained', so not for each prediction.\nAnd B 'AutoML tabular regression model' is regression, but for not classification problem...","timestamp":"1712231640.0","comment_id":"1189285","poster":"Yan_X","upvote_count":"2"},{"content":"Selected Answer: B\nMy Answer: B\n\n“the probability of churn for each customer”: the probability is a number. So regression problem. (A,B, C)\n\n“predictions to be interpretable”: explainable in predict not in the model (B,C)\n\nChoosing between “Build an AutoML tabular regression model” and “Build a custom TensorFlow neural network by using Vertex AI custom training”, I think B could be the most relevant for the problem. However I also think that others no enough information in the text to choose between the two.","timestamp":"1708124760.0","poster":"guilhermebutzke","upvote_count":"4","comment_id":"1152279"},{"comment_id":"1136728","poster":"sonicclasps","content":"Selected Answer: B\nthe question asks for explainability for predictions, answer D does not provide that. \nAlthough not the ideal solution, B is the only answer that suits the requirements, because churn can also be expressed as a probability.","comments":[{"comment_id":"1148860","upvote_count":"1","poster":"tavva_prudhvi","timestamp":"1707792180.0","content":"But, in Option B is says \"AutoML Regression\" if the problem statement is about classification!"}],"timestamp":"1706701980.0","upvote_count":"1"},{"timestamp":"1705991220.0","upvote_count":"3","content":"Selected Answer: D\nThe answer is D.\n1.Churn prediction is a classification problem: We want to categorize customers as either churning or not churning, not predict a continuous value like revenue. Therefore, a classification model is needed.\n2.Random forest models are interpretable: Feature importances provide insights into which features contribute most to the model's predictions, making them a good choice for understanding why customers churn. This interpretability is crucial for developing targeted marketing campaigns.\n3.Vertex AI Workbench is a suitable platform: It provides notebook instances for building and training models, making it a good choice for this task.","poster":"daidai75","comment_id":"1129235"},{"timestamp":"1705547760.0","upvote_count":"2","poster":"shadz10","content":"Selected Answer: D\nhttps://cloud.google.com/bigquery/docs/xai-overview","comment_id":"1125518"},{"poster":"pikachu007","timestamp":"1705158360.0","upvote_count":"3","comment_id":"1121775","content":"Selected Answer: D\nOption A: Regression, not classification, is used for random forest model, which is not appropriate for predicting probabilities.\nOption B: While AutoML tabular can generate model explanations, random forests inherently provide more granular insights into feature importance.\nOption C: Neural networks can be less interpretable than tree-based models, and generating explanations for them often requires additional techniques and libraries."}]},{"id":"lwOIGGBn72egW1CMvzzW","answer_description":"","timestamp":"2024-01-13 16:08:00","answer_ET":"A","discussion":[{"poster":"Wuthuong1234","content":"Selected Answer: C\nThe Entity detection in the NLP API will be sufficient to identify ingredients and cookware-related words. It is much easier than training your own model in AutoML.\nKeep in mind that training on your own dataset could introduce some bias. Imagine your training data might cover many French or western recipes, but suddenly you get lots of Thai recipes in production. Your AutoML model would struggle to correctly identify ingredients that are not so common in western cooking such as lemongrass, kecap manis, kaffir or galangal.","comment_id":"1364875","upvote_count":"1","timestamp":"1741088100.0"},{"poster":"andrea_c_","timestamp":"1734308220.0","content":"Selected Answer: C\nWith A you must label a dataset. Since the entities that need to be recognized are pretty common this effort is not justified. \nMoreover, as specified in https://cloud.google.com/vertex-ai/docs/text-data/entity-extraction/prepare-data, \"You must supply at least 1, and no more than 100, unique labels to annotate entities that you want to extract.\" So, it looks like the dataset has a limit of 100 entities, which I do not think is enough for this use case.","upvote_count":"1","comment_id":"1327111"},{"poster":"Omi_04040","timestamp":"1733815260.0","comment_id":"1324419","content":"Selected Answer: A\nThis option involves creating a dataset specifically for entity extraction and training an AutoML model to identify ingredients and cookware. By labeling a minimum of 200 instances for each entity, it ensures a sufficient amount of data for training. Using a holdout dataset for assessment helps evaluate the model's performance. Overall, this approach seems appropriate for the task at hand.\n\nReference:\nhttps://cloud.google.com/vertex-ai/docs/text-data/entity-extraction/prepare-data","upvote_count":"1"},{"timestamp":"1720189740.0","upvote_count":"1","content":"By choosing option A, you can leverage the power of machine learning to efficiently extract ingredients and cookware from recipes in a scalable manner.\noption C uses the Entity Analysis method of the Natural Language API, which might be a viable option if you had access to the API's pre-trained models. However, since you're working with Vertex AI, creating a dataset for entity extraction is a better choice.","comment_id":"1242827","poster":"AzureDP900"},{"poster":"fitri001","comment_id":"1196295","upvote_count":"2","content":"Selected Answer: A\nFor extracting ingredients and cookware from recipe text files, creating a text dataset on Vertex AI for entity extraction with a custom NER model is the better approach. While it requires more upfront effort for data labeling and training, it offers superior accuracy and control over the types of entities extracted.\n\nHowever, if you need a quick and easy solution to get started, the Natural Language API's Entity Analysis can be a temporary option. Be aware that the accuracy might be lower, and you might need to post-process the results to filter out irrelevant entities.","timestamp":"1713233640.0"},{"comment_id":"1194691","timestamp":"1712989620.0","upvote_count":"1","poster":"omermahgoub","content":"Selected Answer: C\nNatural Language API offers a pre-built solution for entity analysis which eliminates the need for custom model training and labeling large datasets, saving time and resources.\n\nVertex AI AutoML can aslo be used for entity extraction but it requires data labeling and training, which can be time-consuming for a vast number of potential ingredients and cookware."},{"upvote_count":"2","comment_id":"1152290","poster":"guilhermebutzke","content":"Selected Answer: A\nMy Answer: A\n\n A: is the most suitable approach for this task because we need to identify and extract specific named entities (\"ingredient\" and \"cookware\") from the text, not classify the entire recipe into predefined categories.\n\nB: This approach would require classifying each recipe based on all possible ingredients and cookware, leading to a vast number of classes and potential performance issues.\n\nC: This pre-built solution might not be as customizable or scalable as training a specific model for this task.\n\nD: This is impractical and unnecessary as the number of potential ingredients and cookware is vast.","timestamp":"1708127340.0"},{"poster":"daidai75","upvote_count":"2","content":"I prefer to A.\nOption C is not the best, because The NLP API is designed to identify general entities within text. While it's effective for broad categories, it may not be as precise for specialized domains like cooking ingredients and cookware, which require a more tailored approach.","comment_id":"1129239","timestamp":"1705991760.0"},{"comment_id":"1128731","upvote_count":"4","content":"Selected Answer: A\nA.\n\"... you might create an entity extraction model to identify specialized terminology in legal documents or patents.\"\n\nI prefer this over C, which might classify carrot as vegetable, chicken as meat... custom entity extraction allows you to specify what entities you wish to extract from the text.","timestamp":"1705933020.0","comments":[{"poster":"b1a8fae","comment_id":"1128732","content":"https://cloud.google.com/vertex-ai/docs/text-data/entity-extraction/prepare-data","upvote_count":"3","timestamp":"1705933020.0"}],"poster":"b1a8fae"},{"comment_id":"1125520","poster":"shadz10","comments":[{"content":"No, A is right as it may not be as effective for this specific task unless the ingredients and cookware are already well-represented within the types of entities the API is trained to recognize. This approach might require less initial setup but could be less accurate for specialized domains like recipes.","timestamp":"1707792480.0","upvote_count":"1","poster":"tavva_prudhvi","comment_id":"1148862"}],"timestamp":"1705548180.0","upvote_count":"1","content":"Selected Answer: C\nReconsidering my answer and going with C \nOption A involves using AutoML entity extraction, which could be a valid approach. However, for extracting entities like ingredients and cookware, Google Cloud's pre-trained Natural Language API might be a more straightforward solution."},{"poster":"shadz10","upvote_count":"2","comment_id":"1123265","timestamp":"1705316940.0","content":"Selected Answer: A\nA is the correct option here"},{"comment_id":"1121778","upvote_count":"1","timestamp":"1705158480.0","poster":"pikachu007","content":"Selected Answer: C\nOption B: Multi-label text classification is less suitable for identifying specific entities within text and would require labeling entire recipes with multiple classes, increasing complexity and reducing model specificity.\nOption C: Natural Language API's Entity Analysis might not be as accurate for this specialized domain as a model trained on custom recipe data.\nOption D: Creating separate entities for each ingredient and cookware type would significantly increase labeling effort and potentially hinder model generalization.","comments":[{"comment_id":"1122156","content":"do you mean Option A?","poster":"kalle_balle","upvote_count":"3","timestamp":"1705192860.0"}]}],"answer_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/google/view/131102-exam-professional-machine-learning-engineer-topic-1-question/","question_text":"You work for a company that is developing an application to help users with meal planning. You want to use machine learning to scan a corpus of recipes and extract each ingredient (e.g., carrot, rice, pasta) and each kitchen cookware (e.g., bowl, pot, spoon) mentioned. Each recipe is saved in an unstructured text file. What should you do?","isMC":true,"choices":{"A":"Create a text dataset on Vertex AI for entity extraction Create two entities called “ingredient” and “cookware”, and label at least 200 examples of each entity. Train an AutoML entity extraction model to extract occurrences of these entity types. Evaluate performance on a holdout dataset.","C":"Use the Entity Analysis method of the Natural Language API to extract the ingredients and cookware from each recipe. Evaluate the model's performance on a prelabeled dataset.","D":"Create a text dataset on Vertex AI for entity extraction. Create as many entities as there are different ingredients and cookware. Train an AutoML entity extraction model to extract those entities. Evaluate the model’s performance on a holdout dataset.","B":"Create a multi-label text classification dataset on Vertex AI. Create a test dataset, and label each recipe that corresponds to its ingredients and cookware. Train a multi-class classification model. Evaluate the model’s performance on a holdout dataset."},"answers_community":["A (69%)","C (31%)"],"answer":"A","question_id":185,"exam_id":13,"question_images":[],"unix_timestamp":1705158480}],"exam":{"isImplemented":true,"isBeta":false,"isMCOnly":true,"provider":"Google","id":13,"name":"Professional Machine Learning Engineer","numberOfQuestions":304,"lastUpdated":"11 Apr 2025"},"currentPage":37},"__N_SSP":true}