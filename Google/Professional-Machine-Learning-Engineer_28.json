{"pageProps":{"questions":[{"id":"dUPUygUq76zXhfhQkqQY","answer_description":"","answers_community":["C (88%)","13%"],"choices":{"A":"1. Create a new service account and grant it the Notebook Viewer role\n2. Grant the Service Account User role to each team member on the service account\n3. Grant the Vertex AI User role to each team member\n4. Provision a Vertex AI Workbench user-managed notebook instance that uses the new service account","B":"1. Grant the Vertex AI User role to the default Compute Engine service account\n2. Grant the Service Account User role to each team member on the default Compute Engine service account\n3. Provision a Vertex AI Workbench user-managed notebook instance that uses the default Compute Engine service account.","D":"1. Grant the Vertex AI User role to the primary team member\n2. Grant the Notebook Viewer role to the other team members\n3. Provision a Vertex AI Workbench user-managed notebook instance that uses the primary user’s account","C":"1. Create a new service account and grant it the Vertex AI User role\n2. Grant the Service Account User role to each team member on the service account\n3. Grant the Notebook Viewer role to each team member.\n4. Provision a Vertex AI Workbench user-managed notebook instance that uses the new service account"},"question_id":136,"answer_ET":"C","timestamp":"2024-01-13 06:18:00","topic":"1","unix_timestamp":1705123080,"question_images":[],"question_text":"You are collaborating on a model prototype with your team. You need to create a Vertex AI Workbench environment for the members of your team and also limit access to other employees in your project. What should you do?","discussion":[{"timestamp":"1729229820.0","poster":"fitri001","upvote_count":"1","comments":[{"upvote_count":"1","poster":"fitri001","timestamp":"1729229880.0","comment_id":"1197721","content":"A. Notebook Viewer with Service Account User: Granting the Notebook User role on the service account would allow team members to modify notebooks, potentially exceeding your intended access limitations.\nB. Default Service Account: Granting access on the default Compute Engine service account is not recommended for security reasons. It's a shared resource and could grant unintended access.\nD. Primary User Access: Granting access through a single user account creates a security risk and is not scalable for managing team member permissions."}],"content":"Selected Answer: C\n1. Create a new service account and grant it the Vertex AI User role: This dedicated service account will control access to the Vertex AI Workbench environment.\n\n2. Grant the Service Account User role to each team member on the service account: This grants your team members the ability to use the service account to access the Workbench environment.\n\n3. Grant the Notebook Viewer role to each team member: While they can't modify notebooks, this role allows team members to view and run existing notebooks within the Workbench environment.\n\n4. Provision a Vertex AI Workbench user-managed notebook instance that uses the new service account: By associating the instance with the service account, you ensure only authorized team members (through the service account) can access the environment.","comment_id":"1197720"},{"content":"Selected Answer: C\nMy Answer: C\n\nThis approach ensures that each team member has access to the necessary resources while limiting access to other employees not involved in the project. In A, the Notebook Viewer role is just to see, which is not sufficient for accessing Vertex AI resources. In B, This option grants permissions to the default Compute Engine service account, which may not be ideal for managing access to Vertex AI resources specifically. In D, This approach does not provide uniform access control for all team members and may lead to inconsistencies in resource management.","timestamp":"1723666800.0","poster":"guilhermebutzke","upvote_count":"2","comment_id":"1150580"},{"comment_id":"1141034","content":"Selected Answer: C\nWhy not A?\n\nMainly because of the fact that we're only giving the role \"Notebook Viewer\" to the SA, which is not sufficient.","comments":[],"upvote_count":"1","timestamp":"1722851160.0","poster":"mindriddler"},{"poster":"b1a8fae","upvote_count":"1","timestamp":"1721114520.0","content":"Selected Answer: A\nA and C really sound like the same. Only going for A because I understand it gives the lowest level of permission role when creating the project (that is, all members in the Compute Engine Project); and subsequently, grants User role ONLY to the team members. https://cloud.google.com/iam/docs/overview#resource","comment_id":"1124049","comments":[{"poster":"tavva_prudhvi","timestamp":"1722960180.0","upvote_count":"1","comment_id":"1142432","content":"Creating a new service account with the Notebook Viewer role would not provide sufficient permissions for managing the Vertex AI Workbench environment, right?"}]},{"content":"Selected Answer: C\nDedicated Service Account: Creating a separate service account ensures isolation and control over access to Vertex AI resources.\nVertex AI User Role: Granting this role to the service account provides it with necessary permissions to interact with Vertex AI services.\nService Account User Role: Assigning this role to team members allows them to impersonate the service account, enabling them to use its permissions.\nNotebook Viewer Role: This role grants team members access to the notebook instance, but not direct Vertex AI resource management.\nUser-Managed Notebook Instance: This type of instance uses a specific service account, ensuring access control is aligned with the designated service account's permissions.","upvote_count":"3","comment_id":"1121318","timestamp":"1720840680.0","poster":"pikachu007"}],"answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/131040-exam-professional-machine-learning-engineer-topic-1-question/","answer":"C","isMC":true,"exam_id":13},{"id":"cLofE9rbc3lSUH2Ci6lQ","url":"https://www.examtopics.com/discussions/google/view/131041-exam-professional-machine-learning-engineer-topic-1-question/","question_id":137,"unix_timestamp":1705123260,"answer":"C","discussion":[{"content":"Selected Answer: C\nC. \"AutoML Entity Extraction for Healthcare allows you to create a custom entity extraction model trained using your own annotated medical text and using your own categories.\" https://cloud.google.com/healthcare-api/docs/concepts/nlp#choosing_between_the_and","comment_id":"1124053","timestamp":"1705397340.0","poster":"b1a8fae","upvote_count":"9","comments":[{"poster":"sonicclasps","content":"textbook use case as described in the link provided","comment_id":"1136526","timestamp":"1706689140.0","upvote_count":"1"},{"timestamp":"1706422440.0","poster":"daidai75","comment_id":"1133872","content":"Full Agreed","upvote_count":"1"}]},{"upvote_count":"1","content":"Selected Answer: C\nC is the best option because AutoML Entity Extraction provides the best balance of ease of use, speed, and effectiveness for building a custom medical entity extraction model with your specific labeled data.\n*A is not proper for custom labels, B requires deep expertise and is time consuming. D is too complex, requires deep expertise and extensive code.","poster":"lunalongo","comment_id":"1320689","timestamp":"1733084280.0"},{"comment_id":"1239335","poster":"VinaoSilva","timestamp":"1719673080.0","upvote_count":"1","content":"Selected Answer: C\n\"unstructured textual data with custom labels \" = AutoML Entity Extraction"},{"comment_id":"1197743","upvote_count":"2","poster":"fitri001","comments":[{"comment_id":"1197745","upvote_count":"1","content":"A. Healthcare Natural Language API: While this API can extract medical entities like diseases or medications, it might not support the level of customization you need for your specific medical phrases with custom labels.\nB. BERT-based Model with Fine-tuning: Fine-tuning a BERT model can be effective, but it requires significant expertise in machine learning and natural language processing. AutoML Entity Extraction provides a more accessible and potentially faster approach for your use case.\nD. TensorFlow for Custom Model: Building a custom model with TensorFlow offers maximum control, but it requires a high level of expertise and can be time-consuming, especially for a team that might not specialize in NLP.","timestamp":"1713422160.0","poster":"fitri001"}],"timestamp":"1713422100.0","content":"Selected Answer: C\nPre-built Functionality: It's a pre-built and managed service within Vertex AI that streamlines the process of building custom entity extraction models. This can save you time and resources compared to building a model from scratch using TensorFlow (option D).\nCustomizable Labels: AutoML Entity Extraction allows you to define your custom labels for medical phrases, which aligns well with your specific needs.\nUnstructured Text Support: It's designed to handle unstructured text data like your medical records.\nFaster Experimentation: Compared to a custom BERT-based model (option B), AutoML Entity Extraction often allows for faster experimentation as it automates many hyperparameter tuning aspects."},{"upvote_count":"2","content":"Selected Answer: B\nMy answer: B \n\nLooking for “developing state-of-the-art algorithms for various use cases” in the question, I think the best approach is BERT-based model. AutoML Entity Extraction could be a approach for a quickstart, and Healthcare Natural Language API might not have your custom labels built-in, limiting its effectiveness. Tensorflow model can be time-consuming and require significant expertise\n\nhttps://cloud.google.com/healthcare-api/docs/concepts/nlp#choosing_between_the_and","timestamp":"1707950700.0","comment_id":"1150591","poster":"guilhermebutzke"},{"upvote_count":"3","timestamp":"1705840980.0","comment_id":"1127803","poster":"Dagogi96","content":"Selected Answer: A\nA.- \"The Healthcare Natural Language API parses unstructured medical text such as medical records or insurance claims. It then generates a structured data representation of the medical knowledge entities stored in these data sources for downstream analysis and automatio\""},{"upvote_count":"2","poster":"pikachu007","timestamp":"1705123260.0","content":"Selected Answer: B\nA. Healthcare Natural Language API: While convenient, it lacks the customization capabilities for fine-tuning with custom labels, potentially limiting accuracy for your specific needs.\nC. AutoML Entity Extraction: It's generally well-suited for common entity types, but its pre-defined label set might not accommodate the full range of medical entities and relationships you need to extract.\nD. TensorFlow Custom Model: Building a model from scratch requires significant expertise, time, and resources, often less efficient than leveraging the power of pre-trained BERT models.","comment_id":"1121319"}],"exam_id":13,"answer_ET":"C","isMC":true,"topic":"1","question_images":[],"answer_description":"","timestamp":"2024-01-13 06:21:00","question_text":"You work at a leading healthcare firm developing state-of-the-art algorithms for various use cases. You have unstructured textual data with custom labels. You need to extract and classify various medical phrases with these labels. What should you do?","answers_community":["C (65%)","B (20%)","A (15%)"],"choices":{"A":"Use the Healthcare Natural Language API to extract medical entities","C":"Use AutoML Entity Extraction to train a medical entity extraction model","D":"Use TensorFlow to build a custom medical entity extraction model","B":"Use a BERT-based model to fine-tune a medical entity extraction model"},"answer_images":[]},{"id":"bYoj4Cg1yGvar5FImqUR","isMC":true,"choices":{"C":"Separate the training dataset into two tables based on demographic and behavioral features. Deploy the models to two separate endpoints, and submit two Vertex AI Model Monitoring jobs.","D":"Separate the training dataset into two tables based on demographic and behavioral features. Deploy both models to the same endpoint, and submit a Vertex AI Model Monitoring job with a monitoring-config-from-file parameter that accounts for the model IDs and training datasets.","B":"Keep the training dataset as is. Deploy both models to the same endpoint and submit a Vertex AI Model Monitoring job with a monitoring-config-from-file parameter that accounts for the model IDs and feature selections.","A":"Keep the training dataset as is. Deploy the models to two separate endpoints, and submit two Vertex AI Model Monitoring jobs with appropriately selected feature-thresholds parameters."},"url":"https://www.examtopics.com/discussions/google/view/131046-exam-professional-machine-learning-engineer-topic-1-question/","answers_community":["B (68%)","D (32%)"],"answer_ET":"B","answer_images":[],"question_images":[],"exam_id":13,"discussion":[{"content":"Selected Answer: B\nMy answer: B\n\nIf you're using Vertex AI Model Monitoring for skew detection and your data is stored in BigQuery, it's not strictly necessary to separate the data into two tables. Vertex AI Model Monitoring can indeed analyze each feature individually to detect skew. So, isn't necessary to separate data. \n\nThen, the `monitoring-config-from-file` parameter lets you specify unique configurations for each model, including ID and training data information. This ensures targeted monitoring and analysis and a unique monitoring job.","comment_id":"1150600","timestamp":"1707951780.0","poster":"guilhermebutzke","upvote_count":"7"},{"poster":"Dirtie_Sinkie","content":"Selected Answer: D\nMy vote is D, have to separate the training dataset","timestamp":"1727410320.0","upvote_count":"1","comment_id":"1289807"},{"comment_id":"1260839","timestamp":"1722812880.0","poster":"bfdf9c8","content":"Selected Answer: D\nThe question mentions skew, yo need to configure the model monitoring with this in mind, so the better option is to separate in two diferent tables to user skew detection","upvote_count":"1"},{"comment_id":"1197750","comments":[{"upvote_count":"2","timestamp":"1713422760.0","poster":"fitri001","comment_id":"1197751","content":"A. Separate Endpoints and Monitoring Jobs: This approach requires managing two endpoints and monitoring jobs, increasing complexity.\nC. Separate Training Data and Separate Endpoints: While it separates training data, it requires managing separate endpoints and monitoring jobs, similar to option A. Additionally, splitting the data might be unnecessary for monitoring purposes in this scenario.\nD. Separate Training Data (Optional) and Single Endpoint: Splitting the data (optional) adds complexity, and while you can use a single endpoint, defining configurations for each model within the monitoring job is more efficient using the monitoring-config-from-file parameter (option B)."}],"upvote_count":"3","poster":"fitri001","timestamp":"1713422760.0","content":"Selected Answer: B\nReduced Management Effort: You only need to deploy and monitor a single endpoint, minimizing complexity compared to managing two separate endpoints and monitoring jobs (Option A and C).\nEfficient Data Usage: Maintaining the original training dataset simplifies data management and avoids the need to split it into separate tables (Option C and D).\nGranular Monitoring: The monitoring-config-from-file parameter allows you to specify configurations for each model within the same monitoring job. You can define the model ID and the features to monitor for potential skew or drift for each model independently."},{"upvote_count":"1","timestamp":"1712570340.0","poster":"pinimichele01","comments":[{"comment_id":"1225326","content":"For training-skew detection, you require the training dataset. Hence, by splitting the original dataset into the two features, it would make management easier later on.\n\nCorrect me if I'm wrong, but you would have to update the monitoring job when you retrain the model to keep the monitoring job updated as well. Hence splitting it makes sense. Agreed that same endpoint would be easier to manage as opposed to two. \n\nAs a result, my answer is D.","poster":"SausageMuffins","upvote_count":"1","timestamp":"1717666260.0"}],"comment_id":"1191492","content":"Selected Answer: B\nI don't understand why it is necessary to separate dataset when there is Vertex AI Monitoring"},{"upvote_count":"1","timestamp":"1711726380.0","poster":"shuvs","comments":[{"poster":"pinimichele01","content":"why? i don't understand sorry","upvote_count":"1","timestamp":"1712570400.0","comment_id":"1191493"}],"comment_id":"1185547","content":"Selected Answer: D\nNot B, as training on separate datasets is recommended."},{"timestamp":"1710057240.0","comment_id":"1170129","content":"Selected Answer: D\nD\n\nSeparate data to 2 tables to make sure both models are trained with most relevant data.","upvote_count":"1","poster":"Yan_X"},{"timestamp":"1705397940.0","comments":[{"upvote_count":"2","poster":"vaibavi","content":"Why not B?","timestamp":"1707525720.0","comment_id":"1145874"}],"upvote_count":"1","comment_id":"1124061","poster":"b1a8fae","content":"Selected Answer: D\nD.\nYou need to split the training dataset for each respective model. Furthermore, you only need to control for 2 differences between models in monitoring-config-from-file: model ID, and training set. Feature selection should be the same in both models."},{"poster":"shadz10","comment_id":"1123060","content":"Selected Answer: D\nD - makes more sense two models to be trained seperately and more accuarately also submits a Vertex Al Model Monitoring job with a monitoring-config-from parameter which would enable the skew detecttion to work for each model","timestamp":"1705293960.0","upvote_count":"2"},{"content":"Selected Answer: B\nA. Separate Endpoints: This approach involves more management overhead and potentially complicates monitoring configurations.\nC. Separate Datasets: Splitting the dataset into two tables is unnecessary for model monitoring and could introduce data management complexities.\nD. Separate Datasets, Same Endpoint: While feasible, this option lacks the flexibility of granular feature control provided by monitoring-config-from-file.","upvote_count":"4","poster":"pikachu007","comment_id":"1121334","timestamp":"1705126500.0"}],"question_text":"You developed a custom model by using Vertex AI to predict your application's user churn rate. You are using Vertex AI Model Monitoring for skew detection. The training data stored in BigQuery contains two sets of features - demographic and behavioral. You later discover that two separate models trained on each set perform better than the original model. You need to configure a new model monitoring pipeline that splits traffic among the two models. You want to use the same prediction-sampling-rate and monitoring-frequency for each model. You also want to minimize management effort. What should you do?","question_id":138,"answer_description":"","topic":"1","unix_timestamp":1705126500,"answer":"B","timestamp":"2024-01-13 07:15:00"},{"id":"mJPF8Q5xhZ5yGzLGSu4a","answers_community":["D (100%)"],"question_text":"You work for a pharmaceutical company based in Canada. Your team developed a BigQuery ML model to predict the number of flu infections for the next month in Canada. Weather data is published weekly, and flu infection statistics are published monthly. You need to configure a model retraining policy that minimizes cost. What should you do?","choices":{"A":"Download the weather and flu data each week. Configure Cloud Scheduler to execute a Vertex AI pipeline to retrain the model weekly.","D":"Download the weather data each week, and download the flu data each month. Deploy the model to a Vertex AI endpoint with feature drift monitoring, and retrain the model if a monitoring alert is detected.","C":"Download the weather and flu data each week. Configure Cloud Scheduler to execute a Vertex AI pipeline to retrain the model every month.","B":"Download the weather and flu data each month. Configure Cloud Scheduler to execute a Vertex AI pipeline to retrain the model monthly."},"isMC":true,"timestamp":"2024-01-13 07:19:00","topic":"1","url":"https://www.examtopics.com/discussions/google/view/131047-exam-professional-machine-learning-engineer-topic-1-question/","question_id":139,"answer_description":"","question_images":[],"answer_ET":"D","unix_timestamp":1705126740,"exam_id":13,"answer_images":[],"answer":"D","discussion":[{"timestamp":"1713423000.0","content":"Selected Answer: D\nWeather Data Update: Downloading weather data weekly captures the latest trends potentially influencing flu infections.\nFlu Data Update: Downloading flu statistics monthly aligns with the data publication schedule and avoids unnecessary processing for data that might not have changed.\nFeature Drift Monitoring: Vertex AI endpoint monitoring helps identify significant changes in the weather data distribution (feature drift) over time.\nRetrain Based on Alerts: Retraining the model is triggered only when feature drift is detected, ensuring the model stays relevant without unnecessary retraining cycles.","comment_id":"1197754","poster":"fitri001","comments":[{"comments":[{"comment_id":"1324911","content":"Why does a model that does batch prediction need to be deployed to an endpoint, the right ans seems to be B","timestamp":"1733903400.0","poster":"Omi_04040","upvote_count":"1"}],"comment_id":"1197755","content":"A. Weekly Retraining: Retraining the model every week incurs processing costs even if the flu data (target variable) hasn't changed, potentially leading to wasted resources.\nB. Monthly Retraining: While cheaper than option A, it might miss capturing the impact of recent weather changes on flu infections.\nC. Weekly Data Download, Monthly Retraining: This approach downloads weather data more frequently than necessary and still incurs retraining costs even if feature drift hasn't occurred.","timestamp":"1713423000.0","poster":"fitri001","upvote_count":"1"}],"upvote_count":"5"},{"comment_id":"1191495","timestamp":"1712570700.0","upvote_count":"1","poster":"pinimichele01","content":"Selected Answer: D\nminimize cost"},{"upvote_count":"1","comment_id":"1153615","timestamp":"1708296360.0","content":"Selected Answer: D\nMy Answer: D\n\nEven though the model predicts values for the next month, it is necessary to consume weekly data because the model's output could change based on new weekly data. Therefore, it is necessary to download data weekly and monthly. Furthermore, it is not necessary to retrain the model if the feature distribution remains unchanged.","poster":"guilhermebutzke"},{"timestamp":"1705405380.0","comment_id":"1124157","poster":"b1a8fae","upvote_count":"4","content":"Selected Answer: D\nD. This way, cost is minimized by only retraining when feature drift takes place."},{"poster":"pikachu007","comment_id":"1121336","upvote_count":"2","content":"Selected Answer: D\nSelective Retraining: Retraining occurs only when necessary, triggered by feature drift alerts, reducing cloud resource usage and associated costs.\nEfficient Data Utilization: Weather data is downloaded weekly to capture potential changes, but model retraining waits for monthly flu data, ensuring model relevance without excessive updates.\nEarly Drift Detection: Vertex AI's feature drift monitoring proactively identifies model performance degradation, prompting timely retraining to maintain accuracy.","timestamp":"1705126740.0"}]},{"id":"dqDJzXwOMmZcqQ7Ofpyh","answer":"C","url":"https://www.examtopics.com/discussions/google/view/131048-exam-professional-machine-learning-engineer-topic-1-question/","choices":{"C":"Store parameters in Vertex ML Metadata, store the models’ source code in GitHub, and store the models’ binaries in Cloud Storage.","B":"Store parameters in Cloud SQL, store the models’ source code in GitHub, and store the models’ binaries in Cloud Storage.","A":"Store parameters in Cloud SQL, and store the models’ source code and binaries in GitHub.","D":"Store parameters in Vertex ML Metadata and store the models’ source code and binaries in GitHub."},"answer_ET":"C","timestamp":"2024-01-13 07:24:00","answer_images":[],"discussion":[{"timestamp":"1729234380.0","comment_id":"1197756","comments":[{"timestamp":"1729234440.0","content":"A. Cloud SQL for Parameters: While Cloud SQL is a relational database service, Vertex ML Metadata offers a dedicated solution for ML metadata management, including parameters, providing better integration and functionality within the MLOps context.\nD. Vertex ML Metadata for Source Code and Binaries: Vertex ML Metadata is primarily focused on ML pipeline metadata and experiment tracking. Cloud Storage is a more appropriate service for storing large binary files like model artifacts.","upvote_count":"1","comment_id":"1197757","poster":"fitri001"}],"upvote_count":"3","content":"Selected Answer: C\nVertex ML Metadata: This service is specifically designed to store and track metadata for ML pipelines, including parameters. It provides a centralized location to manage and query pipeline execution details, making it ideal for dozens of pipelines.\nCloud Storage: This is a scalable and cost-effective storage solution for model binaries. It integrates well with Vertex AI and other cloud services.\nGitHub: While not a Google Cloud service, it's a popular version control system well-suited for storing and managing your models' source code, particularly for collaboration among team members.","poster":"fitri001"},{"upvote_count":"1","comment_id":"1191497","poster":"pinimichele01","content":"Selected Answer: C\nshadz10","timestamp":"1728382080.0"},{"timestamp":"1721246400.0","upvote_count":"2","poster":"shadz10","comment_id":"1125372","content":"Selected Answer: C\nhttps://cloud.google.com/architecture/architecture-for-mlops-using-tfx-kubeflow-pipelines-and-cloud-build"},{"timestamp":"1720844640.0","upvote_count":"2","poster":"pikachu007","content":"Selected Answer: C\nA. Cloud SQL and GitHub: Cloud SQL isn't designed for ML metadata management, potentially leading to challenges in tracking experiment details and lineage.\nB. Cloud SQL, GitHub, and Cloud Storage: While viable, this approach misses the benefits of Vertex ML Metadata for organized ML artifact management.\nD. Vertex ML Metadata and GitHub: Storing model binaries in GitHub can be inefficient for large files and might incur higher storage costs.","comment_id":"1121340"}],"isMC":true,"answer_description":"","question_text":"You are building a MLOps platform to automate your company’s ML experiments and model retraining. You need to organize the artifacts for dozens of pipelines. How should you store the pipelines’ artifacts?","topic":"1","question_images":[],"answers_community":["C (100%)"],"exam_id":13,"unix_timestamp":1705127040,"question_id":140}],"exam":{"id":13,"lastUpdated":"11 Apr 2025","isImplemented":true,"numberOfQuestions":304,"provider":"Google","isBeta":false,"name":"Professional Machine Learning Engineer","isMCOnly":true},"currentPage":28},"__N_SSP":true}