{"pageProps":{"questions":[{"id":"tCaeRCbG3AuNAKJRB0Zp","answer_description":"","question_images":[],"question_text":"You have written unit tests for a Kubeflow Pipeline that require custom libraries. You want to automate the execution of unit tests with each new push to your development branch in Cloud Source Repositories. What should you do?","answers_community":["B (100%)"],"answer_images":[],"topic":"1","discussion":[{"content":"B. GCP recommends to use Cloud Build when building KubeFlow Pipelines. It's possible to run unit tests in Cloud Build. And, the others seems overly complex/unnecessary","timestamp":"1624349640.0","poster":"maartenalexander","comment_id":"387723","upvote_count":"18"},{"upvote_count":"9","poster":"mousseUwU","comments":[{"comment_id":"464582","upvote_count":"2","content":"The image explains a lot","timestamp":"1634635020.0","poster":"mousseUwU"}],"content":"B makes sense because of this: https://cloud.google.com/architecture/architecture-for-mlops-using-tfx-kubeflow-pipelines-and-cloud-build#cicd_architecture","timestamp":"1634634900.0","comment_id":"464581"},{"comment_id":"1363666","timestamp":"1740855840.0","upvote_count":"1","content":"Selected Answer: B\nGCP recommends to use Cloud Build when building KubeFlow Pipelines","poster":"bc3f222"},{"poster":"bludw","content":"Selected Answer: B\nB: No need of any Pub/Sub stuff","comment_id":"1237893","upvote_count":"1","timestamp":"1719467160.0"},{"poster":"PhilipKoku","upvote_count":"1","content":"Selected Answer: B\nB. Cloud Build.","timestamp":"1717674840.0","comment_id":"1225458"},{"timestamp":"1699972380.0","comment_id":"1070484","poster":"Sum_Sum","content":"Selected Answer: B\nB is the only sensible answer as its a feature of CloudBuild \neverything else is the delusions of a madmen","upvote_count":"2"},{"poster":"SamuelTsch","timestamp":"1688712900.0","upvote_count":"1","comment_id":"945417","content":"Selected Answer: B\nA, C, D need addiontal maunal tasks.\nB is correct."},{"upvote_count":"1","comment_id":"901201","content":"Selected Answer: B\nCloud Build is the best choice but the other answers are feasible.","timestamp":"1684413180.0","poster":"Scipione_"},{"upvote_count":"1","comment_id":"892695","content":"Selected Answer: B\nWent with B","poster":"M25","timestamp":"1683608340.0"},{"content":"Selected Answer: B\nBecause it is the most automatic of the options","comment_id":"800388","upvote_count":"1","timestamp":"1675729740.0","poster":"enghabeth"},{"content":"Selected Answer: B\nans: B","poster":"wish0035","comment_id":"746528","upvote_count":"1","timestamp":"1671139860.0"},{"timestamp":"1669279980.0","poster":"EFIGO","comment_id":"725674","content":"Selected Answer: B\nB is the Google-recommended best practice.","upvote_count":"1"},{"upvote_count":"1","timestamp":"1660567680.0","comment_id":"647194","poster":"GCP72","content":"Correct answer is \"B\""},{"content":"Selected Answer: B\nB it is.","timestamp":"1649295960.0","comment_id":"582106","poster":"morgan62","upvote_count":"2"},{"timestamp":"1631134500.0","upvote_count":"5","content":"Easy one, B, Cloud Build is the tool for CI/CD.","comment_id":"441643","poster":"Danny2021"}],"exam_id":13,"question_id":101,"answer":"B","answer_ET":"B","unix_timestamp":1624349640,"timestamp":"2021-06-22 10:14:00","url":"https://www.examtopics.com/discussions/google/view/55816-exam-professional-machine-learning-engineer-topic-1-question/","isMC":true,"choices":{"B":"Using Cloud Build, set an automated trigger to execute the unit tests when changes are pushed to your development branch.","C":"Set up a Cloud Logging sink to a Pub/Sub topic that captures interactions with Cloud Source Repositories. Configure a Pub/Sub trigger for Cloud Run, and execute the unit tests on Cloud Run.","D":"Set up a Cloud Logging sink to a Pub/Sub topic that captures interactions with Cloud Source Repositories. Execute the unit tests using a Cloud Function that is triggered when messages are sent to the Pub/Sub topic.","A":"Write a script that sequentially performs the push to your development branch and executes the unit tests on Cloud Run."}},{"id":"j611RAk4Q9tfbcQdQukV","timestamp":"2024-01-13 03:44:00","answers_community":["C (100%)"],"choices":{"B":"Replace the monitoring job with a custom SQL script to calculate statistics on the features and predictions in BigQuery","D":"Increase the monitor_interval parameter in the ScheduleConfig of the monitoring job","C":"Decrease the sample_rate parameter in the RandomSampleConfig of the monitoring job","A":"Replace the monitoring job with a DataFlow pipeline that uses TensorFlow Data Validation (TFDV)"},"url":"https://www.examtopics.com/discussions/google/view/131006-exam-professional-machine-learning-engineer-topic-1-question/","question_id":102,"answer_description":"","answer_ET":"C","question_images":[],"answer":"C","discussion":[{"upvote_count":"1","poster":"LaxmanTiwari","timestamp":"1719733800.0","content":"fitri001 2 months, 1 week ago\nA. DataFlow pipeline with TFDV: While DataFlow pipelines with TFDV can be used for data validation, they require additional development and management overhead compared to simply adjusting the Vertex AI Model Monitoring job configuration.\nB. Custom SQL script: Custom SQL scripts might not be as efficient or maintainable as the built-in Vertex AI Model Monitoring features. Additionally, it would require manually calculating drift metrics, which can be error-prone.\nD. Increase monitor_interval: Increasing the monitoring interval reduces the frequency of monitoring checks, potentially delaying drift detection. This is not ideal if data drifts frequently.","comment_id":"1239565"},{"upvote_count":"3","comments":[{"comment_id":"1199509","timestamp":"1713684720.0","poster":"fitri001","content":"A. DataFlow pipeline with TFDV: While DataFlow pipelines with TFDV can be used for data validation, they require additional development and management overhead compared to simply adjusting the Vertex AI Model Monitoring job configuration.\nB. Custom SQL script: Custom SQL scripts might not be as efficient or maintainable as the built-in Vertex AI Model Monitoring features. Additionally, it would require manually calculating drift metrics, which can be error-prone.\nD. Increase monitor_interval: Increasing the monitoring interval reduces the frequency of monitoring checks, potentially delaying drift detection. This is not ideal if data drifts frequently.","upvote_count":"2"}],"comment_id":"1199508","poster":"fitri001","timestamp":"1713684660.0","content":"Selected Answer: C\nReduced Monitoring Overhead: By decreasing the sample_rate, you instruct Vertex AI Model Monitoring to analyze a smaller percentage of incoming requests. This directly reduces the billing cost associated with monitoring.\nFast Drift Detection: A well-chosen sampling rate can still provide enough data to capture significant data drift. Monitoring a smaller sample shouldn't significantly impact your ability to detect drift if it's happening rapidly."},{"poster":"Carlose2108","comment_id":"1160492","timestamp":"1709032440.0","upvote_count":"1","content":"Selected Answer: C\nI went with C."},{"upvote_count":"2","comment_id":"1137447","content":"Selected Answer: C\nC as the sample size will be relative to the traffic and also reduce costs.","poster":"ddogg","timestamp":"1706779320.0"},{"content":"Selected Answer: C\nC. https://cloud.google.com/vertex-ai/docs/model-monitoring/overview#considerations","upvote_count":"1","comment_id":"1123312","timestamp":"1705319880.0","poster":"b1a8fae"},{"poster":"pikachu007","comment_id":"1121240","upvote_count":"2","content":"Selected Answer: C\nThe answer is C, simplest and does not affect the time it takes to detect the drift","timestamp":"1705113840.0"}],"unix_timestamp":1705113840,"exam_id":13,"isMC":true,"answer_images":[],"topic":"1","question_text":"You recently deployed a model to a Vertex AI endpoint. Your data drifts frequently, so you have enabled request-response logging and created a Vertex AI Model Monitoring job. You have observed that your model is receiving higher traffic than expected. You need to reduce the model monitoring cost while continuing to quickly detect drift. What should you do?"},{"id":"1ZFsGFdttiJf3jJd1d0D","timestamp":"2024-01-13 03:47:00","question_images":[],"url":"https://www.examtopics.com/discussions/google/view/131007-exam-professional-machine-learning-engineer-topic-1-question/","question_id":103,"answers_community":["B (100%)"],"unix_timestamp":1705114020,"answer_description":"","answer":"B","question_text":"You work for a retail company. You have created a Vertex AI forecast model that produces monthly item sales predictions. You want to quickly create a report that will help to explain how the model calculates the predictions. You have one month of recent actual sales data that was not included in the training dataset. How should you generate data for your report?","discussion":[{"content":"Selected Answer: B\nFeature Attribution: By enabling feature attributions in the batch prediction job, you gain insights into how each feature in the actual sales data contributes to the model's predictions. This information is crucial for explaining the model's reasoning to non-technical audiences.\nDirect Model Insights: Analyzing the feature attributions allows you to demonstrate how the model uses historical trends, seasonality, and other factors (represented by features) to predict future sales.","comments":[{"upvote_count":"2","content":"A. Prediction vs. Actuals: While comparing predictions to actuals can be informative, it doesn't directly explain how the model arrives at those predictions.\nC. Counterfactual Examples: Counterfactuals can be useful for understanding model behavior, but creating them requires additional effort and might not be necessary for explaining the basic prediction process.\nD. Training a New Model: Training another model is time-consuming and unnecessary. Feature attributions provide valuable insights without needing a separate model.","timestamp":"1729496640.0","poster":"fitri001","comment_id":"1199518"}],"comment_id":"1199517","upvote_count":"2","poster":"fitri001","timestamp":"1729496580.0"},{"upvote_count":"1","poster":"MultiCloudIronMan","content":"Selected Answer: B\nB is the best answer but am unsure why the report has to be compared with actual sales","timestamp":"1728052740.0","comment_id":"1189409"},{"content":"Selected Answer: B\nB) Will actually give you the information needed with Feature Attributions. E.g. The importance of each feature influencing the predictions sales items.","poster":"ddogg","upvote_count":"3","timestamp":"1722497280.0","comment_id":"1137451"},{"content":"Selected Answer: B\nFeature attributions explicitly measure how much each input feature contributed to each prediction, providing the most relevant insights for understanding model behavior.","timestamp":"1720831620.0","comment_id":"1121245","upvote_count":"2","poster":"pikachu007"}],"topic":"1","answer_images":[],"isMC":true,"choices":{"C":"Generate counterfactual examples by using the actual sales data. Create a batch prediction job using the actual sales data and the counterfactual examples. Compare the results in the report.","B":"Create a batch prediction job by using the actual sales data, and configure the job settings to generate feature attributions. Compare the results in the report.","A":"Create a batch prediction job by using the actual sales data. Compare the predictions to the actuals in the report.","D":"Train another model by using the same training dataset as the original, and exclude some columns. Using the actual sales data create one batch prediction job by using the new model and another one with the original model. Compare the two sets of predictions in the report."},"answer_ET":"B","exam_id":13},{"id":"Crq0OQ8SnzLrFIv0UmVW","discussion":[{"content":"Selected Answer: D\nData-driven Retraining: Monitoring for feature drift identifies significant changes in the underlying data distribution used to train the model. Retraining based on drift detection ensures the model stays relevant to evolving data patterns, prioritizing model accuracy.\nReduced Cost: Triggering retraining only when drift is detected avoids unnecessary training runs, minimizing costs associated with Vertex AI training jobs.","upvote_count":"3","timestamp":"1729496820.0","comments":[{"content":"A. New Data Availability: While new data is important, it might not always necessitate retraining, especially if the new data aligns with existing patterns.\nB. Predetermined Frequency: Fixed scheduling can lead to either under-training (data evolves faster than the schedule) or over-training (drift happens slower than the schedule), potentially wasting resources.\nC. Anomaly Detection: Anomalies might not directly indicate feature drift, and retraining based solely on anomalies could introduce noise into the model.","comment_id":"1199521","poster":"fitri001","timestamp":"1729496880.0","upvote_count":"1"}],"comment_id":"1199520","poster":"fitri001"},{"comment_id":"1137455","content":"Selected Answer: D\nD) Makes the most sense and scales","poster":"ddogg","timestamp":"1722497580.0","upvote_count":"2"},{"poster":"b1a8fae","timestamp":"1721038500.0","comment_id":"1123325","content":"Selected Answer: D\nKeep the model up to date -> monitoring drift (distribution of production data doesnt change wildly). Only rerun training when necessary.","upvote_count":"1"},{"upvote_count":"1","poster":"pikachu007","timestamp":"1720831740.0","comment_id":"1121247","content":"Selected Answer: D\nIt proactively triggers retraining when feature drift is detected, ensuring the model adapts to changing data patterns and maintains accuracy."},{"timestamp":"1720694160.0","upvote_count":"1","poster":"winston9","comment_id":"1119720","content":"Selected Answer: D\nfeature drifting detecting to trigger retraining"}],"unix_timestamp":1704976560,"url":"https://www.examtopics.com/discussions/google/view/130850-exam-professional-machine-learning-engineer-topic-1-question/","answers_community":["D (100%)"],"topic":"1","question_images":[],"answer_description":"","exam_id":13,"answer_ET":"D","answer":"D","answer_images":[],"isMC":true,"question_id":104,"choices":{"A":"Configure Pub/Sub to call the Cloud Function when a sufficient amount of new data becomes available","D":"Enable model monitoring on the Vertex AI endpoint. Configure Pub/Sub to call the Cloud Function when feature drift is detected","B":"Configure a Cloud Scheduler job that calls the Cloud Function at a predetermined frequency that fits your team’s budget","C":"Enable model monitoring on the Vertex AI endpoint. Configure Pub/Sub to call the Cloud Function when anomalies are detected"},"timestamp":"2024-01-11 13:36:00","question_text":"Your team has a model deployed to a Vertex AI endpoint. You have created a Vertex AI pipeline that automates the model training process and is triggered by a Cloud Function. You need to prioritize keeping the model up-to-date, but also minimize retraining costs. How should you configure retraining?"},{"id":"N7uQXS6QVsOJMYpCePJ2","unix_timestamp":1705114440,"url":"https://www.examtopics.com/discussions/google/view/131009-exam-professional-machine-learning-engineer-topic-1-question/","answer_ET":"B","choices":{"A":"1. Upload the audio files to Cloud Storage\n2. Call the speech:longrunningrecognize API endpoint to generate transcriptions\n3. Call the predict method of an AutoML sentiment analysis model to analyze the transcriptions.","C":"1. Iterate over your local files in Python\n2. Use the Speech-to-Text Python library to create a speech.RecognitionAudio object, and set the content to the audio file data\n3. Call the speech:recognize API endpoint to generate transcriptions\n4. Call the predict method of an AutoML sentiment analysis model to analyze the transcriptions.","D":"1. Iterate over your local files in Python\n2. Use the Speech-to-Text Python Library to create a speech.RecognitionAudio object and set the content to the audio file data\n3. Call the speech:longrunningrecognize API endpoint to generate transcriptions.\n4. Call the Natural Language API by using the analyzeSentiment method","B":"1. Upload the audio files to Cloud Storage.\n2. Call the speech:longrunningrecognize API endpoint to generate transcriptions\n3. Create a Cloud Function that calls the Natural Language API by using the analyzeSentiment method"},"timestamp":"2024-01-13 03:54:00","topic":"1","answer_description":"","answer_images":[],"question_images":[],"isMC":true,"answers_community":["B (68%)","A (32%)"],"discussion":[{"content":"Selected Answer: B\nMy answer: B\n\nAccording to https://cloud.google.com/vertex-ai/docs/text-data/sentiment-analysis/prepare-data, AutoML sentiment analysis requires a minimum of 10 labeled training documents per sentiment category, with a maximum of 100,000 total training documents. This means you need to ensure you have an adequate amount of labeled data to train a reliable model. Therefore, option B is more suitable since the API will return the sentiment and there is no mention of a customized problem that justifies the use of AutoML.","poster":"guilhermebutzke","timestamp":"1707684060.0","upvote_count":"8","comment_id":"1147659"},{"content":"Selected Answer: A\nIt is definitely A and not B. Natural Language API can be used for sentiment analysis but it will require an additional Cloud Function to handle the sentiment analysis, which adds complexity and overhead. Since AutoML models are built specifically for sentiment analysis tasks, using AutoML directly is more efficient.","upvote_count":"2","poster":"rajshiv","comment_id":"1321182","timestamp":"1733192160.0"},{"content":"Selected Answer: B\nAgree with guilhermebutzke","poster":"pinimichele01","comment_id":"1202562","timestamp":"1714131540.0","upvote_count":"1"},{"content":"Selected Answer: B\nScalability: Uploading audio files to Cloud Storage provides a scalable and reliable storage solution for your large dataset.\nAsynchronous Processing: The speech:longrunningrecognize API enables asynchronous transcription, allowing your code to proceed without waiting for each file to finish processing. This improves overall throughput.\nManaged Service: Cloud Functions are serverless functions that automatically scale to handle the workload. You don't need to manage servers or infrastructure.\nNatural Language API Integration: The Cloud Function can directly call the Natural Language API's analyzeSentiment method for sentiment analysis, streamlining the workflow.","comments":[{"content":"C & D: Local Processing: Iterating over local files and using the Speech-to-Text Python library might be suitable for small datasets. However, for a large number of audio files, local processing becomes slow and inefficient, especially for long audio files (5 minutes).\nC: Speech-to-Text API Limitation: The speech:recognize API is designed for short audio snippets (less than a minute) and might not be suitable for 5-minute audio files.","poster":"fitri001","comment_id":"1199524","upvote_count":"1","timestamp":"1713685860.0"}],"timestamp":"1713685860.0","poster":"fitri001","upvote_count":"2","comment_id":"1199523"},{"upvote_count":"1","timestamp":"1713330420.0","comment_id":"1196992","content":"Selected Answer: B\nAgree with guilhermebutzke","poster":"gscharly"},{"upvote_count":"2","content":"Selected Answer: A\nA)\nEfficiency: Option A leverages the optimized and scalable infrastructure of Google Cloud Platform (GCP). Using the speech:longrunningrecognize API allows you to transcribe large audio files efficiently without overwhelming your local machine or network.\n\nCost-effectiveness: Paying for processing in Cloud Storage can be more cost-effective than performing it locally, especially for large datasets.\n\nEase of use: The Cloud Storage and Speech-to-Text APIs are well-documented and provide readily available libraries for easy integration.\n\nScalability: This approach scales easily as your dataset grows, as GCP can handle large workloads efficiently.","poster":"ddogg","timestamp":"1706780520.0","comment_id":"1137468"},{"comment_id":"1124604","timestamp":"1705451040.0","content":"Selected Answer: A\nRe-considering as question states large dataset going with option A","upvote_count":"1","poster":"shadz10"},{"poster":"shadz10","upvote_count":"1","timestamp":"1705402140.0","content":"Selected Answer: B\nI’m going with b and agree with BlehMaks - For your convenience, the Natural Language API can perform sentiment analysis directly on a file located in Cloud Storage, without the need to send the contents of the file in the body of your request. \nGoogles best practices try api first then auto ml then custom training. \nhttps://cloud.google.com/natural-language/docs/analyzing-sentiment","comment_id":"1124111"},{"upvote_count":"1","comment_id":"1123336","poster":"b1a8fae","content":"Selected Answer: A\nA.\nIt must be longrunningrecognize -> no C.\nNo point speaking about Python files -> no D.\nFinal question being: NL analyzeSentiment or AutoML sentiment? I feel due to large dataset VertexAI AutoML is the way to go (can scale to large volumes of data)","timestamp":"1705322040.0","comments":[{"upvote_count":"1","comment_id":"1123338","timestamp":"1705322220.0","content":"More info: what natural language is right for you? https://cloud.google.com/natural-language?hl=en","poster":"b1a8fae"}]},{"content":"Selected Answer: B\nVertex AI AutoML is overkill as the build-in NL API provides sentiment analysis.","comment_id":"1122832","upvote_count":"4","timestamp":"1705265580.0","poster":"BlehMaks"},{"content":"B\nBecause don't need to train model just use google api transcride and sentiment analysis","timestamp":"1705168680.0","upvote_count":"2","poster":"36bdc1e","comment_id":"1121933"},{"upvote_count":"2","timestamp":"1705114440.0","poster":"pikachu007","comment_id":"1121252","content":"Selected Answer: A\nEfficient audio processing: speech:longrunningrecognize is specifically designed for handling large audio files, offering asynchronous processing and optimized performance.\nScalability: Cloud Storage and Vertex AI AutoML scale seamlessly to handle large volumes of data and model inferences.\nCost-effectiveness: Separating transcription and sentiment analysis allows for potential cost optimization by using different pricing models for each service."}],"question_id":105,"exam_id":13,"question_text":"Your company stores a large number of audio files of phone calls made to your customer call center in an on-premises database. Each audio file is in wav format and is approximately 5 minutes long. You need to analyze these audio files for customer sentiment. You plan to use the Speech-to-Text API You want to use the most efficient approach. What should you do?","answer":"B"}],"exam":{"isImplemented":true,"lastUpdated":"11 Apr 2025","name":"Professional Machine Learning Engineer","isMCOnly":true,"isBeta":false,"numberOfQuestions":304,"id":13,"provider":"Google"},"currentPage":21},"__N_SSP":true}