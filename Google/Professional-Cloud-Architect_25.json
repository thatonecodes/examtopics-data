{"pageProps":{"questions":[{"id":"sIBkQdSEg4HRx2EaDlTV","exam_id":4,"topic":"1","answer_description":"","question_id":121,"answer":"A","question_text":"Your company has decided to build a backup replica of their on-premises user authentication PostgreSQL database on Google Cloud Platform. The database is 4\nTB, and large updates are frequent. Replication requires private address space communication.\nWhich networking approach should you use?","timestamp":"2020-01-11 20:54:00","unix_timestamp":1578772440,"question_images":[],"answers_community":["A (71%)","B (29%)"],"answer_images":[],"isMC":true,"choices":{"B":"Google Cloud VPN connected to the data center network","D":"A Google Compute Engine instance with a VPN server installed connected to the data center network","C":"A NAT and TLS translation gateway installed on-premises","A":"Google Cloud Dedicated Interconnect"},"discussion":[{"upvote_count":"29","timestamp":"1578772440.0","content":"A is the one","poster":"AWS56","comment_id":"37830","comments":[{"content":"A is ok","poster":"tartar","timestamp":"1596688860.0","comment_id":"151692","upvote_count":"8"},{"timestamp":"1614877500.0","content":"A, direct connect is private. VPN not enough for 4 TB with huge frequent changes.","poster":"nitinz","upvote_count":"3","comment_id":"303489"}]},{"poster":"amxexam","comment_id":"431179","upvote_count":"18","timestamp":"1629871200.0","content":"Let's go with option elimination\nA. Google Cloud Dedicated Interconnect\n>> Secured, fast connection, hence the choice. This will allow private connection from GCP to the data centre with a fast connection. Cost is not mentioned in the requirement to eliminate this option.\nB. Google Cloud VPN connected to the data centre network\n>> We have to think about data flowing on the internet and the requirement talks about private connect. Also not sure how well you connect VPN with Data Center until you use the hybrid option. https://cloud.google.com/network-connectivity/docs/vpn/concepts/overview hence eliminate\nC. A NAT and TLS translation gateway installed on-premises\n>>This is a VM option to reach outside won't for this requirement hence eliminate\nD. A Google Compute Engine instance with a VPN server installed connected to the data centre network\n>>This is a slow option hence eliminate\n\nHence A"},{"content":"Selected Answer: A\nGoogle Cloud Dedicated Interconnect provides the necessary performance, reliability, and private connectivity required for your database replication scenario. Therefore, Option A is the best choice.","poster":"gaufchamp","comment_id":"1418587","timestamp":"1743527820.0","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: A\nA - Private Connection to VPC network, and transfer speed of 10 GBPS makes it obvious choice to maintain fast read replicas","timestamp":"1737807720.0","poster":"alihabib","comment_id":"1346433"},{"timestamp":"1735398720.0","content":"Selected Answer: A\nKey clue is \"Replication requires private address space communication\" . Only Google Cloud Dedicated Interconnect has private address. VPN is public, encrypted and slower !!! Another clue is \"Large updates are frequent\" means you need faster Dedicated interconnect, as VPN will be slower.","upvote_count":"3","comment_id":"1333017","poster":"ramjisriram"},{"poster":"Ekramy_Elnaggar","timestamp":"1731320760.0","upvote_count":"6","comment_id":"1309948","content":"Selected Answer: A\n1. High Bandwidth and Reliability: Dedicated Interconnect provides a direct physical connection between your on-premises network and Google Cloud, offering high bandwidth and low latency. This is essential for replicating a 4TB database with frequent large updates.\n\n2. Private Address Space: Dedicated Interconnect allows you to extend your private IP address space to Google Cloud, ensuring secure and private communication for database replication.\n\n3. Security: Dedicated Interconnect offers a more secure connection compared to VPN, as traffic doesn't traverse the public internet.\n\nNote: Question didn't mention anything about costs, so guys please stop overthinking and focus on the question key words."},{"timestamp":"1716662580.0","comments":[{"timestamp":"1717603920.0","poster":"tocsa","upvote_count":"1","content":"A simple VPN may not provide enough bandwidth for replication if the DB is busy. We know that the auth DB is 4TB, I'd say this must be a quite big company, possibly they can offer an interconnect? But it surely is expensive","comment_id":"1224823"}],"content":"Selected Answer: B\nOption B\nInterconnect is incredibly expensive and the usecase do not justify it.\nProperly configured, a VPN provides similar features. If the question included an inusally high SLA, I will go with Interconnect. If not, VPN is a great option.","comment_id":"1218525","upvote_count":"5","poster":"Robert0"},{"upvote_count":"3","comment_id":"1204380","timestamp":"1714460580.0","content":"Selected Answer: B\nI'll go for VPN.\nFirst, the database is only for authentication and updates will be on this part, small portion of data needs to be replicated between on-premise and cloud. so no need for high bandwidth. the first migration will needs bandwidth but not toom much (5T) can be migrated using VPN.\nVPN permits to use private networking and it's secure.\nVPN not expensive as direct connect.\nAs architect you should also evaluate the cost over the requirement, at the end you need convice business with solution. Paying 5K will kick you out the project for such small requirement.","comments":[{"poster":"hpf97","upvote_count":"1","comment_id":"1347970","content":"I do not agree, it is said that the 4TB database is for user authentication database; so it cannot be part of it.","timestamp":"1738076100.0"}],"poster":"sidiosidi"},{"comment_id":"1167583","content":"If you tried to sell me on Interconnect when all I needed was a VPN (meets bandwidth req, private address space, encryption of traffic possible), I would reach out to AWS for a quote...","upvote_count":"1","timestamp":"1709773800.0","poster":"Jen3"},{"poster":"lisabisa","comment_id":"1017254","upvote_count":"3","timestamp":"1695685080.0","content":"GoogleVPN throughput is 3Gbps. It supports private IP connection and cheaper than DIrect Connection.\n\nDirect connect supports 8 * 10Gbps or 2*100Gbps. But too expensive for this"},{"upvote_count":"3","content":"Selected Answer: A\nConnect to private space with high-speed bandwidth will go to A.","poster":"eka_nostra","timestamp":"1690418400.0","comment_id":"964255"},{"upvote_count":"1","comment_id":"896231","poster":"mrhege","content":"B: Dedicated Interconnect would be a major overkill here and a quite expensive one as well. Requirements mention private _address space_, not private connection. Data over VPN is just as secure. Also there is no mention that a Google PoP would be available.\nhttps://cloud.google.com/network-connectivity/docs/how-to/choose-product","timestamp":"1683922920.0"},{"timestamp":"1669438980.0","poster":"mohideenks","content":"Selected Answer: A\nA is the correct answer","upvote_count":"1","comment_id":"727269"},{"timestamp":"1666463040.0","poster":"Mahmoud_E","content":"Selected Answer: A\nA is great but expensive for just a database DR but what can we do about that","upvote_count":"1","comment_id":"701714"},{"poster":"zr79","upvote_count":"1","content":"VPN is not private, it is public but encrypted. Also, VPN is not suitable for large updates that happen frequently","timestamp":"1665977460.0","comment_id":"696776"},{"timestamp":"1665952260.0","comment_id":"696538","poster":"AzureDP900","content":"without any second thought A is right","upvote_count":"1"},{"content":"Selected Answer: A\nA. Google Cloud Dedicated Interconnect - large updates and better security, however may not be the most cost effective choice","comment_id":"693760","poster":"minmin2020","upvote_count":"1","timestamp":"1665651960.0"},{"poster":"[Removed]","upvote_count":"1","content":"Selected Answer: A\nA is the one","comment_id":"601441","timestamp":"1652507160.0"},{"content":"Selected Answer: A\nDirect connect.","comment_id":"589048","timestamp":"1650508380.0","poster":"Nirca","upvote_count":"1"},{"comments":[{"upvote_count":"2","comment_id":"601162","timestamp":"1652445120.0","poster":"AmitAr","content":"Key points of quesiton - \n1) Huge data and \n2) on-premises user authentication PostgreSQL - which means security - vpn uses public internet .. so B is not option.\nA - should be correct answer"}],"poster":"hibi6x","upvote_count":"5","content":"Challenge me but this is answer B. I have 4TB DB, frequent update would be what ? 50% daily change means 2TB daily means ~25Mbps. With VPN I can easily achieved that. It is typical ingress to cloud free ....It would be madness to pay 5k montly only for Directo Connect...","timestamp":"1639682580.0","comment_id":"503143"},{"comment_id":"493588","upvote_count":"2","content":"Go for A","timestamp":"1638606900.0","poster":"haroldbenites"},{"content":"A is the correct answer.","poster":"vincy2202","timestamp":"1637564940.0","comment_id":"483926","upvote_count":"2"},{"comment_id":"470065","poster":"dlpkmr98","upvote_count":"2","timestamp":"1635566880.0","content":"always go with best practices --Google Cloud Dedicated Interconnect"},{"poster":"aviratna","content":"A is correct. \n- Its a DB replication of 4 TB which will be continuous so Dedicated Interconnect is cost effective when data volume and traffic is high.\n- Any other option will be costly because of ingress & egress high volume traffic\n- Dedicated Interconnect it can also communicate based on private IP range\n- replicating sensitive data like users data over public internet using Cloud VPN is not good option from security perspective. \n- Dedicated Interconnect traffic will not go over internet","comment_id":"391263","upvote_count":"2","timestamp":"1624713000.0"},{"content":"A is expected to be chosen here but this s authentication data and with option A there is no traffic encryption happening. VPN supports encryption, but has low throughput.","upvote_count":"2","poster":"gatul28","comment_id":"365448","timestamp":"1621847160.0"},{"poster":"victory108","comment_id":"360259","content":"A. Google Cloud Dedicated Interconnect","upvote_count":"1","timestamp":"1621326420.0"},{"content":"A is correct","comment_id":"353982","timestamp":"1620667980.0","upvote_count":"1","poster":"un"},{"content":"Answer is A","comment_id":"324702","upvote_count":"1","timestamp":"1617164340.0","poster":"Ausias18"},{"content":"https://cloud.google.com/network-connectivity/docs/how-to/choose-product\nA is the answer. Because the document clearly says that the Cloud VPN is not for high performance blablabla.\nIn real life, I may just go with Cloud VPN.","comment_id":"275501","timestamp":"1611522780.0","upvote_count":"4","poster":"bnlcnd"},{"timestamp":"1609894800.0","upvote_count":"3","comment_id":"260648","content":"They clearly want you to answer A. I'm vaguely curious who would have a 4TB \"user authentication database\" (that's 500KB of authentication data for every person on Earth).with frequent large updates.","poster":"mwilbert"},{"poster":"certmonkey","upvote_count":"1","timestamp":"1605262920.0","comment_id":"218387","content":"user authentication data needs to be encrypted. Interconnect will not encrypt, VPN will. The on prem database in the serving master and the one in cloud is just a back up so speed and availability has low importance. I will go with B."},{"upvote_count":"1","content":"A - Dedicated Interconnect","poster":"AshokC","timestamp":"1600108080.0","comment_id":"179487"},{"timestamp":"1598701680.0","comment_id":"169215","content":"The question does not ask for a cost effective solution . It says\"large updates are frequent\". Looks like they want us to answer A(Dedicated interconnect).","poster":"parag27","upvote_count":"1"},{"comment_id":"157087","content":"B for sure cuz A is very high price and they shouldn't pay that just for DB replica","upvote_count":"1","poster":"ezat","timestamp":"1597303620.0"},{"comment_id":"147453","upvote_count":"4","content":"Won't B be cheaper and efficient? I'd assume 4TB transfer will happen once and the rest are incremental. Interconnect seems an overkill for merely a Db backup","timestamp":"1596117660.0","poster":"[Removed]"},{"comment_id":"141220","upvote_count":"3","content":"4TBs over VPN (3Gbps) will take 3-4 hours, having in mind that we will not always send 4TB updates, B is possible answer. It depends what exactly is meant by \"updates are frequent\".","timestamp":"1595430840.0","poster":"deaglee"},{"poster":"mlantonis","comment_id":"119390","upvote_count":"2","content":"I agree with A.","timestamp":"1593084780.0"},{"content":"A is the correct answer","upvote_count":"4","timestamp":"1592631660.0","comment_id":"114451","poster":"Tushant"},{"content":"I prefer B. Cheaper and good enough. I don't care about egress traffic being cheaper because there is very little because data changes on-prem and gets copied to the Cloud (ingress).","poster":"Musk","timestamp":"1592253180.0","comment_id":"111086","upvote_count":"3","comments":[{"content":"How will you manage private address space? - A will provide this","poster":"Cloudy_Apple_Juice","timestamp":"1601839680.0","comment_id":"193162","comments":[{"content":"VPN also provide private address space? isn't it?","comments":[{"content":"Nope, VPN traffic goes through the public Internet.","poster":"rickyticotaco","comment_id":"234312","comments":[{"timestamp":"1607035980.0","upvote_count":"1","content":"My bad, with VPN uses private addressing but is not optimal for large data transfers","poster":"rickyticotaco","comment_id":"234439"}],"timestamp":"1607026320.0","upvote_count":"2"}],"comment_id":"225819","upvote_count":"2","poster":"oraldevel","timestamp":"1606135800.0"}],"upvote_count":"1"}]},{"timestamp":"1591768980.0","comment_id":"106514","upvote_count":"2","content":"A, for sure.\nDedicated Interconnect","poster":"gfhbox0083"},{"timestamp":"1591105800.0","comment_id":"100877","content":"A is the correct answer","upvote_count":"1","poster":"Nirms"},{"poster":"Ziegler","content":"A is the correct answer","upvote_count":"2","comment_id":"98357","timestamp":"1590767820.0"},{"content":"Final Decision to go with Option A","poster":"AD2AD4","timestamp":"1590737460.0","comment_id":"98091","upvote_count":"2"},{"timestamp":"1589678940.0","content":"Agree A. Dedicated interconnect and does not use public internet","poster":"laksg","upvote_count":"2","comment_id":"90248"},{"timestamp":"1589318100.0","poster":"gcp_aws","comment_id":"87963","upvote_count":"2","content":"Agree with A"},{"upvote_count":"2","poster":"2g","comment_id":"44723","timestamp":"1580390580.0","content":"answer: A"}],"url":"https://www.examtopics.com/discussions/google/view/11781-exam-professional-cloud-architect-topic-1-question-27/","answer_ET":"A"},{"id":"S4PlNkIJs8Tvl6VqQciO","choices":{"A":"Create custom Google Stackdriver alerts and send them to the auditor","D":"Enable Google Cloud Storage (GCS) log export to audit logs into a GCS bucket and delegate access to the bucket","C":"Use cloud functions to transfer log entries to Google Cloud SQL and use ACLs and views to limit an auditor's view","B":"Enable Logging export to Google BigQuery and use ACLs and views to scope the data shared with the auditor"},"answers_community":["B (69%)","D (31%)"],"timestamp":"2019-10-21 14:10:00","unix_timestamp":1571659800,"discussion":[{"poster":"ghitesh","comment_id":"38816","comments":[{"content":"b) seems correct","timestamp":"1579447500.0","comment_id":"40633","upvote_count":"3","poster":"rockstar9622"},{"upvote_count":"5","comment_id":"70651","poster":"anton_royce","timestamp":"1585895760.0","content":"I agree. Answer B"},{"content":"The article references either gcs or bq. I think this q is referring to gcs","timestamp":"1629966960.0","upvote_count":"1","comment_id":"432083","poster":"MikeB19"},{"comment_id":"483786","poster":"TheCloudBoy77","timestamp":"1637544840.0","content":"B makes more sense after reading it. thx","upvote_count":"4"}],"content":"B. https://cloud.google.com/iam/docs/roles-audit-logging#scenario_external_auditors","timestamp":"1578992160.0","upvote_count":"98"},{"content":"Think B is better. Export to Bigquery and restrict access to queries with ACLs to auditors","timestamp":"1571659800.0","comments":[{"upvote_count":"4","poster":"trainor","comment_id":"236433","timestamp":"1607259180.0","content":"I think D is better. B implies too much data manipulation to make it suitable for an audit."},{"comments":[{"poster":"tartar","comments":[{"comments":[{"comment_id":"403410","content":"B is correct","timestamp":"1625928900.0","poster":"RKS_2021","upvote_count":"1"}],"timestamp":"1611023220.0","comment_id":"270875","content":"don't change your view, D was right :)","upvote_count":"4","poster":"alii"}],"timestamp":"1597393320.0","comment_id":"157933","content":"Sorry, changed my view. B is the recommended practice","upvote_count":"14"}],"upvote_count":"7","comment_id":"151700","content":"D is ok.","poster":"tartar","timestamp":"1596689280.0"},{"upvote_count":"3","comment_id":"303613","comments":[{"content":"Please check the keywords in question -- \"streamline and expedite\" -- Bigquery is suitable not storage bucket. so it should be (B)","comment_id":"601607","upvote_count":"3","poster":"AmitAr","timestamp":"1652535780.0"}],"content":"D, rest all options are no good.","timestamp":"1614895680.0","poster":"nitinz"},{"poster":"passnow","timestamp":"1576615200.0","content":"I thought same as well. I would go with B","upvote_count":"5","comment_id":"30461"}],"comment_id":"16415","poster":"jcmoranp","upvote_count":"38"},{"poster":"hpf97","content":"Selected Answer: B\nA : not good as sending security data outside, even for audit, is not compliant\nC : cloud SQL is not designed for audits\nD : Would be great as the linly one to mention audit logs but only the log from GCS, and not IAM.\nB : remaining answer but it would be prefered audit log","timestamp":"1738077180.0","comment_id":"1347983","upvote_count":"1"},{"timestamp":"1735399140.0","poster":"ramjisriram","content":"Selected Answer: B\nKey clue from the question is \"You want to streamline and expedite the analysis\". How can you expect streamline and analysis capabilities from Cloud Storage? Rather BigQuery has the analytics and streamline capabilities inbuilt in the tool. User access can be controlled in both, so the correct answer with key clues is to Enable Logging export to Google BigQuery (this will store all the logs streamlined) and use ACLs (these are filters, so according to the audior) and views (these are read only, there is no obfuscation or tokenization or sensistive data to be handled from the logs !!!!) to scope the data shared with the auditor.","upvote_count":"3","comment_id":"1333021"},{"upvote_count":"1","poster":"Ishu_awsguy","content":"Selected Answer: B\nThe power word here is ,\nStreamline analysis and audit , hence BQ is the answer.\nIf it would have been only audit , then D would have been beteer","timestamp":"1735379880.0","comment_id":"1332900"},{"comment_id":"1309950","upvote_count":"4","poster":"Ekramy_Elnaggar","content":"Selected Answer: B\n1. Comprehensive Audit Trail: Cloud Logging automatically captures audit logs for all Cloud IAM activity. Exporting these logs to BigQuery provides a centralized and comprehensive audit trail for analysis.\n\n2. Powerful Analysis: BigQuery's analytical capabilities allow auditors to efficiently query and analyze IAM policy changes over the 12-month period. They can filter, aggregate, and generate reports to identify any anomalies or security concerns.\n\n3. Granular Access Control: BigQuery's Access Control Lists (ACLs) and views enable you to precisely control which data the auditors can access. This ensures that they only see the information relevant to their audit without exposing sensitive data.\n\nNote: While exporting logs to Cloud Storage is possible, it's less efficient for analysis compared to BigQuery.","timestamp":"1731321180.0"},{"timestamp":"1729760340.0","poster":"nareshthumma","content":"Answer B","comment_id":"1302377","upvote_count":"1"},{"poster":"maxdanny","content":"Selected Answer: B\nOption B is the best approach. Enable Logging export to Google BigQuery and use ACLs and views to scope the data shared with the auditor. This method provides robust querying capabilities, ensures that historical IAM policy changes can be analyzed effectively, and allows you to control access securely.","comment_id":"1277490","timestamp":"1725359760.0","upvote_count":"1"},{"poster":"joecloud12","comment_id":"1261644","upvote_count":"1","content":"Selected Answer: B\nb is correct because it is easier to implement compared to D","timestamp":"1722944280.0"},{"upvote_count":"4","poster":"H_S","timestamp":"1720986660.0","comment_id":"1247950","content":"Selected Answer: D\nREAD THIS, ACL is not available in BIG QUERY , thereforeD. Enable Google Cloud Storage (GCS) log export to audit logs into a GCS bucket and delegate access to the bucket"},{"content":"ACLs would provide year-round access to the data which is more privileges than necessary. Logs will need to be retained for a full year because hypothetically, January logs could be looked at in December. Cloud Storage offers signed URLs, and less expensive storage options.","comment_id":"1167589","poster":"Jen3","timestamp":"1709774340.0","upvote_count":"1"},{"comment_id":"1156861","content":"Both B and D are ok.\n\nUsing cloud storage requires additional setup for auditors, pulling data to BQ.\nUsing BQ would satisfy \"streamline and expedite the analysis and audit process\"","upvote_count":"1","poster":"lisabisa","timestamp":"1708656420.0"},{"content":"Selected Answer: B\nBased on google documentation B is the correct answer.\nhttps://cloud.google.com/iam/docs/job-functions/auditing#scenario_external_auditors\nDashboard is available in BigQuery to review historic logs and in case anamoly is found elevated access is provided. Access is revoked after audit activities are done.","poster":"Teckexam","upvote_count":"4","timestamp":"1705776360.0","comment_id":"1127448"},{"content":"D - Correct\nB - his option requires additional work to set up the ACLs and views to limit an auditor's view of the data. This could be time-consuming and complex to implement. Furthermore, BigQuery may not be the ideal tool for auditors who are only interested in reviewing Cloud IAM policy changes.","poster":"kip21","upvote_count":"2","comment_id":"1122628","timestamp":"1705246200.0"},{"content":"Selected Answer: B\nThat‘s the only logical one also Bard is confirming this one","poster":"CloudDom","upvote_count":"1","comment_id":"1075291","timestamp":"1700472960.0"},{"upvote_count":"3","timestamp":"1699712520.0","poster":"thewalker","comment_id":"1067890","content":"D\nI will not go with B, as the requirement is once for 12 months. Push the data in Coldline for 12 months and retrieve it during audit is enough. Save costs.","comments":[{"poster":"thewalker","comment_id":"1067891","content":"Coldline / Archive","timestamp":"1699712580.0","upvote_count":"2"},{"poster":"hogtrough","content":"Streamline and expedite analysis is the goal. Costs are never brought up.","comment_id":"1115994","upvote_count":"2","timestamp":"1704643560.0"}]},{"upvote_count":"2","content":"Selected Answer: B\nReading from Cloud Storage raw audit logs (without filtering applied) is everything but streamlined. Imagine the auditor fetching all audit logs, then write some script to analyze them...","timestamp":"1698134340.0","comment_id":"1052647","poster":"krisek"}],"answer_description":"","question_text":"Auditors visit your teams every 12 months and ask to review all the Google Cloud Identity and Access Management (Cloud IAM) policy changes in the previous 12 months. You want to streamline and expedite the analysis and audit process.\nWhat should you do?","answer":"B","answer_ET":"B","answer_images":[],"question_id":122,"question_images":[],"exam_id":4,"topic":"1","url":"https://www.examtopics.com/discussions/google/view/6884-exam-professional-cloud-architect-topic-1-question-28/","isMC":true},{"id":"fBSXiwYfL42KCTfxA1bT","answer_ET":"C","discussion":[{"timestamp":"1572008040.0","upvote_count":"34","comment_id":"17376","content":"Google Secret Management was designed explicitly for this purpose.","poster":"Eroc","comments":[{"timestamp":"1596689340.0","upvote_count":"7","comment_id":"151701","poster":"tartar","content":"C is ok"},{"upvote_count":"6","comment_id":"303614","poster":"nitinz","content":"C, microservices = GKE = Kubernetes = secrets.","timestamp":"1614895740.0"}]},{"comment_id":"24667","timestamp":"1574791980.0","poster":"shandy","upvote_count":"14","content":"C is the answer, since key management systems generate, use, rotate, encrypt, and destroy cryptographic keys and manage permissions to those keys.\n\nA is incorrect because storing credentials in source code and source control is discoverable, in plain text, by anyone with access to the source code. This also introduces the requirement to update code and do a deployment each time the credentials are rotated. B is not correct because consistently populating environment variables would require the credentials to be available, in plain text, when the session is started. D is incorrect because instead of managing access to the config file and updating manually as keys are rotated, it would be better to leverage a key management system. Additionally, there is increased risk if the config file contains the credentials in plain text."},{"poster":"Ekramy_Elnaggar","content":"Selected Answer: C\n1. Centralized and Secure Storage: Secret management systems like HashiCorp Vault, AWS Secrets Manager, or Google Cloud Secret Manager provide a centralized and secure location to store sensitive credentials. This ensures that database credentials are not scattered across multiple microservices or configuration files.\n2. Access Control: Secret management systems offer fine-grained access control, allowing you to restrict access to secrets based on roles and permissions. This ensures that only authorized microservices and users can access the database credentials.\n3. Rotation and Auditing: These systems often provide features for automatic secret rotation and auditing, which helps improve security and compliance.\n4. Integration: Secret management systems can integrate with your deployment pipelines and orchestration tools, making it easier to manage secrets throughout the application lifecycle.","timestamp":"1731321540.0","upvote_count":"3","comment_id":"1309956"},{"upvote_count":"1","comment_id":"1128958","poster":"ashishdwi007","content":"Selected Answer: C\nOther options are not best practices.","timestamp":"1705950840.0"},{"poster":"Teckexam","upvote_count":"1","content":"Selected Answer: C\nNeed to use key management system for this usecase since other options are not secure.","timestamp":"1705776420.0","comment_id":"1127451"},{"comment_id":"863783","poster":"hiromi","timestamp":"1680868200.0","content":"Selected Answer: C\nC is correct","upvote_count":"1"},{"upvote_count":"4","timestamp":"1671605400.0","content":"C. In a secret management system\n\nIt is important to store the credentials for your database back-end securely in order to protect them from unauthorized access. One way to do this is by using a secret management system, such as Google Cloud's Secret Manager. Secret Manager is a secure and convenient storage system for API keys, passwords, and other sensitive data that is designed to protect against unauthorized access. By storing the credentials in Secret Manager, you can ensure that they are kept secure and can be easily accessed by your microservices as needed.\n\nStoring the credentials in the source code, an environment variable, or a config file that has restricted access through ACLs may not provide the same level of security as a dedicated secret management system. It is important to ensure that your credentials are stored in a secure and controlled manner to protect against unauthorized access.","poster":"omermahgoub","comment_id":"751948"},{"content":"Selected Answer: C\nC is correct; If credential then always use secret manager.","upvote_count":"1","poster":"AniketD","comment_id":"721953","timestamp":"1668855780.0"},{"upvote_count":"1","content":"Selected Answer: C\nC is ok","poster":"megumin","comment_id":"711644","timestamp":"1667640960.0"},{"timestamp":"1665977940.0","upvote_count":"1","poster":"zr79","content":"secret manager is the answer","comment_id":"696781"},{"comment_id":"696543","timestamp":"1665952500.0","upvote_count":"1","content":"C is right","poster":"AzureDP900"},{"upvote_count":"1","timestamp":"1663066920.0","poster":"Kubernetes","content":"answer is C","comment_id":"667905"},{"timestamp":"1646309700.0","poster":"[Removed]","content":"Selected Answer: C\nUse Google Secret Manager","comment_id":"560036","upvote_count":"1"},{"content":"Selected Answer: C\nC is correct","poster":"rogerlovato","comment_id":"526002","upvote_count":"1","timestamp":"1642444500.0"},{"upvote_count":"1","timestamp":"1638607620.0","poster":"haroldbenites","content":"Go for C","comment_id":"493592"},{"timestamp":"1638019800.0","content":"C is the right answer","poster":"vincy2202","comment_id":"488165","upvote_count":"1"},{"timestamp":"1633771740.0","content":"Google Practice exam question with option C : In a key management system\nC is correct because key management systems generate, use, rotate, encrypt, and destroy cryptographic keys and manage permissions to those keys.\nhttps://cloud.google.com/kms/\nFor this question, refer to the Mountkirk Games case study.","poster":"unnikrisb","comment_id":"459574","upvote_count":"1"},{"upvote_count":"1","comment_id":"459570","poster":"unnikrisb","content":"Google Practice exam question with option C : In a key management system\nHere also C is correct because key management systems generate, use, rotate, encrypt, and destroy cryptographic keys and manage permissions to those keys.\nhttps://cloud.google.com/kms/\nFor this question, refer to the Mountkirk Games case study.","timestamp":"1633771020.0"},{"upvote_count":"1","comment_id":"459568","content":"Again part of practice tests ( option was key management instead of secret management system )\nC is correct because key management systems generate, use, rotate, encrypt, and destroy cryptographic keys and manage permissions to those keys.\nhttps://cloud.google.com/kms/\nFor this question, refer to the Mountkirk Games case study.","poster":"unnikrisb","timestamp":"1633770840.0"},{"content":"C. In a secret management system","timestamp":"1621401000.0","comment_id":"361019","poster":"victory108","upvote_count":"2"},{"content":"Answer is C","timestamp":"1617164580.0","comment_id":"324705","poster":"Ausias18","upvote_count":"1"},{"comment_id":"324054","timestamp":"1617087120.0","upvote_count":"1","poster":"lynx256","content":"C is ok"},{"comments":[{"timestamp":"1616496960.0","comment_id":"317999","upvote_count":"1","poster":"DickDastardly","content":"Agreed, the lack of capitalization implies some bespoke \"secret\" method hidden from the masses"}],"poster":"JackIsMyName","content":"I think the wording is confusing for this question. The right answer is suggesting using an obscure handling process and not Google Secret Manager. Which are two different things.","upvote_count":"2","timestamp":"1615382580.0","comment_id":"307189"},{"comment_id":"226062","upvote_count":"1","poster":"practicioner","content":"C - this in the list of sample question from google","timestamp":"1606160040.0"},{"upvote_count":"1","comment_id":"187041","timestamp":"1601043540.0","poster":"kimberjdaw","content":"I'm glad to see everyone selecting C. Anyone who says B has no idea how easy it is for developers to hide code to read the environment variables and secretly send them somewhere. This happens all the time and it's why C exists!"},{"upvote_count":"1","content":"C - Secret Management for key management","timestamp":"1600110840.0","comment_id":"179509","poster":"AshokC"},{"comment_id":"126724","content":"No brainer C is right answer","timestamp":"1593941400.0","poster":"Gobblegobble","upvote_count":"1"},{"upvote_count":"1","poster":"mlantonis","content":"C is correct","comment_id":"117168","timestamp":"1592896740.0"},{"comment_id":"114528","content":"C is the correct answer","upvote_count":"1","timestamp":"1592635620.0","poster":"Tushant"},{"upvote_count":"1","comment_id":"106521","poster":"gfhbox0083","timestamp":"1591770000.0","content":"C, for sure\nGoogle Secret Management"},{"timestamp":"1591106580.0","comment_id":"100888","poster":"Nirms","content":"C is the correct answer","upvote_count":"1"},{"timestamp":"1590778440.0","comment_id":"98432","upvote_count":"1","content":"C is the correct answer","poster":"Ziegler"},{"comment_id":"97310","poster":"AD2AD4","upvote_count":"1","content":"Final Decision to go with Option C and it will be a Key Management System.","timestamp":"1590647640.0"},{"content":"C is the answer","timestamp":"1589321040.0","comment_id":"87973","poster":"gcp_aws","upvote_count":"2"},{"comment_id":"44726","poster":"2g","upvote_count":"2","content":"answer: C","timestamp":"1580390700.0"}],"question_text":"You are designing a large distributed application with 30 microservices. Each of your distributed microservices needs to connect to a database back-end. You want to store the credentials securely.\nWhere should you store the credentials?","answer_description":"","exam_id":4,"topic":"1","answer":"C","unix_timestamp":1572008040,"url":"https://www.examtopics.com/discussions/google/view/7200-exam-professional-cloud-architect-topic-1-question-29/","timestamp":"2019-10-25 14:54:00","answer_images":[],"choices":{"C":"In a secret management system","B":"In an environment variable","D":"In a config file that has restricted access through ACLs","A":"In the source code"},"answers_community":["C (100%)"],"isMC":true,"question_images":[],"question_id":123},{"id":"adRrDWt0WG1tDsZTr32f","timestamp":"2021-06-03 09:59:00","answer":"CDE","exam_id":4,"question_id":124,"unix_timestamp":1622707140,"question_text":"The operations manager asks you for a list of recommended practices that she should consider when migrating a J2EE application to the cloud.\nWhich three practices should you recommend? (Choose three.)","url":"https://www.examtopics.com/discussions/google/view/54378-exam-professional-cloud-architect-topic-1-question-3/","isMC":true,"answer_ET":"CDE","question_images":[],"answer_description":"","discussion":[{"content":"This is talking about the APPLICATION not the infrastructure, therefore I believe we should focus on the APP-side of things:\n1. port the app to app engine for content delivery\n2. add monitoring for troubleshooting\n3. use a CI/CD workflow for continuous delivery w/testing for a stable application\n\nso, for me: A, C and E should be the answers","poster":"NapoleonBorntoparty","upvote_count":"68","comment_id":"391421","timestamp":"1726852080.0","comments":[{"poster":"segkhachat","content":"the person who asking you recommendation is operation manager, it can be related to infrastructure","upvote_count":"5","comment_id":"840735","timestamp":"1678957800.0"}]},{"timestamp":"1629781440.0","comment_id":"430457","content":"Let's go with option elimination\nA. Port the application code to run on Google App Engine\n>> PaaS serverless managed service, so all my infra provisioning is taken care by GCP.\nB. Integrate Cloud Dataflow into the application to capture real-time metrics\n>> Good to have \nC. Instrument the application with a monitoring tool like Stackdriver Debugger\n>> Is a must for debugging issues and monitoring application logs this is now GCP Cloud monitoring and logging.\nD. Select an automation framework to reliably provision the cloud infrastructure\n>> App Engine is a PaaS so the infrastructure is taken care of by App Engine, I would select this if I have not selected A, hence will eliminate this option for now\nE. Deploy a continuous integration tool with automated testing in a staging environment\n>> Good to have \nF. Migrate from MySQL to a managed NoSQL database like Google Cloud Datastore or Bigtable\n>> There is no requirement for DB enhancement hence will elimination this option\n\nA and C are must-have \nB and E are Good to have, but E has more importance than Big\n\nHence will go with ACE","poster":"amxexam","upvote_count":"14"},{"content":"Selected Answer: ACE\nThere is no requirement stated for B and F, so we left A, C, D and E\nA is important because the question focuses on an app.\nC is definitely needed when it comes to application development\nsince D will be covered by Google App Engine in A, E is more useful if we are talking about best practice.\n\nso, A,C,E","upvote_count":"1","poster":"examxjh","timestamp":"1743324420.0","comment_id":"1411972"},{"content":"Selected Answer: ADE\nC is not the good answer because Stack Debugger as been abandonned since 2023.\nSo it's ADE for the others reasons ...","timestamp":"1742462040.0","poster":"Syruis","upvote_count":"1","comment_id":"1400974"},{"content":"Selected Answer: CDE\nCDE for me. \nC, because it’s a recommended practice to monitor the application after migration. Stackdriver Debugger (now part of the Cloud Operations suite) is useful to understand the application’s behavior without interrupting it. \nD, because using tools like Terraform, Deployment Manager, or similar is a key best practice when migrating to the cloud.\nE, because automating testing and integration is a core part of any modern cloud migration strategy.","timestamp":"1742207580.0","upvote_count":"1","poster":"apb98","comment_id":"1399605"},{"comment_id":"1364772","timestamp":"1741069680.0","upvote_count":"1","poster":"RTATAVAR","content":"Selected Answer: CDE\nMy answer is C,D,E because choose the options which does not mention specifically about a particular service"},{"upvote_count":"1","poster":"PatnalaJayaram","content":"Selected Answer: ACD\nI feel acd are correct","comment_id":"1362801","timestamp":"1740709620.0"},{"poster":"hpf97","content":"Selected Answer: ACE\nA : GCP managed infrastructure to make JEE application work, easier than on a VM with server appli to configure\nC : Mandatory in order to operate the application (maybe not the debugger, but the monitoring)\nE : CI/CD for better application quality / security; so the operator would appreciate","timestamp":"1738064160.0","upvote_count":"1","comment_id":"1347865"},{"poster":"LeoSantos121212121212121","content":"Selected Answer: ACE\nI chose these answers based on what would make the application easier to get deployed to gcp.","comment_id":"1342596","upvote_count":"1","timestamp":"1737218760.0"},{"timestamp":"1735498440.0","upvote_count":"1","content":"Selected Answer: CDE\nI will go for CDE.","poster":"JonathanSJ","comment_id":"1333673"},{"timestamp":"1735106340.0","upvote_count":"2","poster":"Ishu_awsguy","content":"Selected Answer: ACD\nI would say A C D . \nE has no relevance to the question , creating a CI CD automation testing pipeline that too in staging , yes is a good thing but it is not relavant I feel. \nSimilary doing Data analysis is also irrelevant . \nhence B & E go out for the same reason .","comment_id":"1331380"},{"upvote_count":"1","timestamp":"1733023920.0","poster":"motimoti","content":"Selected Answer: ADE\nI think C is wrong now because it was already deprecated.\nhttps://cloud.google.com/stackdriver/docs/deprecations/debugger-deprecation","comment_id":"1320438"},{"timestamp":"1731815940.0","content":"Would recommending Cloud debugger be the right option? As it is no longer available","upvote_count":"1","comment_id":"1313379","poster":"KV_2001"},{"comment_id":"1309560","content":"Selected Answer: CDE\nAnswer is : C, D, E.\n\nC. Instrument the application with a monitoring tool like Stackdriver Debugger: Visibility is key in the cloud. Tools like Stackdriver Debugger (now called Cloud Debugger) allow you to inspect the state of your application in real-time without stopping or slowing it down. \n\nD. Select an automation framework to reliably provision the cloud infrastructure: Manual configuration is error-prone and doesn't scale. Infrastructure-as-code tools like Terraform or Deployment Manager let you define your infrastructure in code, making it repeatable, version-controlled, and easier to manage. \n\nE. Deploy a continuous integration tool with automated testing in a staging environment: A robust CI/CD pipeline is essential for rapid and reliable deployments. Automated testing in a staging environment that mirrors production helps catch issues early, ensuring a smoother transition and reducing the risk of production outages.","upvote_count":"3","poster":"Ekramy_Elnaggar","comments":[{"upvote_count":"1","poster":"Prabhuanandan","content":"agreed. other options are less critical in this specific context to the operation manager.","comment_id":"1330895","timestamp":"1734975720.0"}],"timestamp":"1731261240.0"},{"comment_id":"1302201","content":"agree CDE","upvote_count":"1","poster":"nareshthumma","timestamp":"1729715040.0"},{"timestamp":"1726907640.0","comment_id":"508852","upvote_count":"3","poster":"ehgm","content":"I chose ACE, but ADE make sense.\n\nA. Port the application code to run on Google App Engine.\nOk. It's a good practice use managed services when possible, we shouldn't worry about infrastructure.\n\nB. Integrate Cloud Dataflow into the application to capture real-time metrics.\nNo Ok. It's just a J2EE application, the question says nothin about a batch or stream pipeline or real-time in insight.\n\nC. Instrument the application with a monitoring tool like Stackdriver Debugger.\nNo Ok. App Engine already have natively logging and monitoring, we only have to enable debugger to fix some problem.\n\nD. Select an automation framework to reliably provision the cloud infrastructure.\nOk. It's a good practice use IaC (infrastructure as code).\n\nE. Deploy a continuous integration tool with automated testing in a staging environment.\nOk. It's a good practice use CI/CD and tests.\n\nF. Migrate from MySQL to a managed NoSQL database like Google Cloud Datastore or Bigtable.\nNo Ok. The question says nothin about Database."},{"upvote_count":"2","timestamp":"1726907640.0","comment_id":"511828","content":"Absolutely different：\nB No need to use DataFlow \nF No need to use NOSQL.We should use CloudSQL.\nAbsolutely Correct:A、E\nA First Step.\nI'm at a loss:C,D,E\nC It is microservices app best practice.App Engine is microservices app.\nAndIt is also written on this page.(Configuring your App with app.yaml)\nD This is Correct, but App Engine does it automatically.\nE Automatically test is a Java best practice.","poster":"OrangeTiger"}],"topic":"1","answer_images":[],"choices":{"B":"Integrate Cloud Dataflow into the application to capture real-time metrics","C":"Instrument the application with a monitoring tool like Stackdriver Debugger","D":"Select an automation framework to reliably provision the cloud infrastructure","F":"Migrate from MySQL to a managed NoSQL database like Google Cloud Datastore or Bigtable","A":"Port the application code to run on Google App Engine","E":"Deploy a continuous integration tool with automated testing in a staging environment"},"answers_community":["CDE (46%)","ADE (28%)","ACE (23%)","2%"]},{"id":"PB7MYzhaC57ErI3q3HE3","url":"https://www.examtopics.com/discussions/google/view/54125-exam-professional-cloud-architect-topic-1-question-30/","choices":{"A":"Cloud Deployment Manager uses Python","E":"Cloud Deployment Manager can be used to permanently delete cloud resources","F":"Cloud Deployment Manager only supports automation of Google Cloud resources","C":"Cloud Deployment Manager is unfamiliar to the company's engineers","D":"Cloud Deployment Manager requires a Google APIs service account to run","B":"Cloud Deployment Manager APIs could be deprecated in the future"},"timestamp":"2021-06-01 20:44:00","answer_description":"","answer":"CF","discussion":[{"comments":[{"poster":"Gregwaw","timestamp":"1695662640.0","upvote_count":"5","content":"F is not a risk, it is a limitation of solution. Risk is something that is not known for sure and is manageable (risk can be mitigated, avoided). You cannot manage the limitation of solution. You can use it with this limitation or not and you know it in advance.","comments":[{"comment_id":"1105779","poster":"Terryhsieh","upvote_count":"3","content":"Advocating to adopt Google Cloud Deployment Manager will become a risk if the lead engineer or other business need ask for use other cloud platform.","timestamp":"1703573760.0"}],"comment_id":"1017053"},{"upvote_count":"14","timestamp":"1627248300.0","content":"Yup, E + F. In GCP documentation it states as a warning note that deletion made through Deployment Manager scripts cannot be undone, if devs are not well trained a human errors can impact Business","comment_id":"414246","poster":"poseidon24"}],"timestamp":"1625406660.0","comment_id":"398382","poster":"victory108","content":"E. Cloud Deployment Manager can be used to permanently delete cloud resources\nF. Cloud Deployment Manager only supports automation of Google Cloud resources","upvote_count":"80"},{"comments":[{"comment_id":"482350","comments":[{"comment_id":"696546","timestamp":"1665952620.0","poster":"AzureDP900","content":"yes, C and F right","upvote_count":"3"},{"poster":"[Removed]","content":"C - Makes sense, because company engineer may take longer to develop, so more cost and more 'time-to-market'\n\nReg F:\nCan I pls ask how does business care whether you are Google Cloud Resources or legacy data center tools, as long as it servs business requirement? \n\nSo I'm leaning towards E, as the engineers are still in the process of learning CDM and may accidently delete VMs bringing down the entire application.","comments":[{"comment_id":"858565","upvote_count":"2","poster":"[Removed]","content":"Forgot to mention, once determined as \"risks\", the mitigation actions below can be followed:\nC: Train the existing resources, Hire an experienced personnel\nE: Peer Reviews, QA, thorough testing etc.","timestamp":"1680416160.0"}],"upvote_count":"5","timestamp":"1680415440.0","comment_id":"858561"}],"content":"I think this is right. the key of the question is \"business risks\".","upvote_count":"3","poster":"ssepiro","timestamp":"1637397840.0"}],"poster":"AK2020","timestamp":"1623743700.0","comment_id":"382435","upvote_count":"42","content":"C and F- make sense to me"},{"comment_id":"1399494","timestamp":"1742174580.0","upvote_count":"1","poster":"ktan13","content":"Selected Answer: BC\nThese are business risk"},{"poster":"izekc","content":"Selected Answer: CE\nCE looks more possible and relavent to risk","timestamp":"1742091780.0","upvote_count":"2","comment_id":"1399115"},{"comment_id":"1347987","content":"Selected Answer: CE\nSo the aim solution should turn on \"Business risk\"\nA : no technical requirement\nB : as any COTS\nC : yes, so setting up the system can take time, and time is money, so business risk\nD : technical requirement, as for other IaC tool\nE : if the system is definitevely deleted, money is lost and bad reputation\nF : that's a fact, a limitation\nFuthermore when there is risk actions could be taken to mitigate them \nC : train people, hire experience ones\nE : code reviews, CI/CD...\nSo C&E is the good couple","poster":"hpf97","upvote_count":"2","timestamp":"1738077780.0"},{"poster":"user263263","content":"Selected Answer: BE\nDefinition of risk: \"the possibility of something bad happening\"\n\nA. is a fact\nB. Cloud Deployment Manager APIs could be deprecated in the future - part of it was already deprecated, see https://cloud.google.com/deployment-manager/docs/deprecations/composite-types, Deprecation is always a business risk for cloud services. There is some probability and if it happens, it will cost time and money to resolve.\nC. if it's true, it's a fact\nD. that's false, see example deployment without service account https://cloud.google.com/deployment-manager/docs/manage-cloud-resources-deployment\nE. Cloud Deployment Manager can be used to permanently delete cloud resources - there's a probability (hint: \"can\") that this causes some bad things to happen\nF. maybe that was a fact when the question was written, using 3rd party APIs is in beta now https://cloud.google.com/deployment-manager/docs/configuration/type-providers/process-adding-api","upvote_count":"2","timestamp":"1737729600.0","comment_id":"1346160"},{"comment_id":"1345695","upvote_count":"2","timestamp":"1737671220.0","poster":"LeoSantos121212121212121","content":"Selected Answer: BC\nThese are the business risks"},{"upvote_count":"1","poster":"JonathanSJ","timestamp":"1735507500.0","content":"Selected Answer: CF\nI will go for CF","comment_id":"1333737"},{"poster":"kino_1994","timestamp":"1735442400.0","upvote_count":"1","content":"Selected Answer: BF\nMy answer is BF and aligns with similar arguments as those presented by @Choopaower\n\nB: This option can be deprecated, as the custom tool has been used for a long time.\nF: Cloud Deployment Manager only supports automation for Google Cloud resources, not custom resources as often happens in On-Premise environments.\n\nC is not a business risk. It is a technical risk. Their engineers may not know it yet, but they can learn.","comment_id":"1333297"},{"poster":"ramjisriram","timestamp":"1735399620.0","upvote_count":"2","content":"Selected Answer: BC\nB and C are the business risks, rest of the options are technical limitations. \n\nB. Cloud Deployment Manager APIs could be deprecated in the future \nC. Cloud Deployment Manager is unfamiliar to the company's engineers\n\nThe other options might present technical challenges or considerations but are not necessarily business risks. For example:\n\nA is more about the programming language used.\n\nD is about the service account requirement.\n\nE is a capability that, if misused, could cause issues but is not inherently a business risk.\n\nF limits scope but isn't a direct business risk.","comment_id":"1333025"},{"upvote_count":"2","poster":"drinkwater","content":"the right answers are C & E\nC: because the engineers are dealing with legacy technologies \nE: Cloud Deployment Manager has the ability to delete resources","timestamp":"1732373040.0","comment_id":"1316695"},{"content":"Selected Answer: CF\nI guess everyone agree on C, the debate is between B and F.\n\nBefore I start, I need to stress on the word \"Business\" not \"Technical\" risks, we need to take this in our minds.\n\nB: Cloud Deployment Manager APIs could be deprecated in the future >> this is not a risk at all as the APIs cannot be changed without an announcement and also there is a backward compatibility to prevent such issues, this is something that is tackled a decade ago in all cloud providers.\n\nF: Cloud Deployment Manager only supports automation of Google Cloud >> Indeed this is a business risk, as the question didn't mention explicitly that the customer is using only GCP, they left it vague by saying \"new cloud environment\", so we have to assume that it is multi-cloud strategy including On-premise BTW ( Hybrid-Cloud as well ), so we need a tool that can handle deployments on all of those targets.\n\nBased on all of that, I recommend ( C & F )","comment_id":"1309961","poster":"Ekramy_Elnaggar","upvote_count":"4","timestamp":"1731322440.0"},{"poster":"beagle_Masato","content":"Selected Answer: CF\nC and F right","upvote_count":"1","comment_id":"1306250","timestamp":"1730566260.0"},{"content":"BF are correct","comment_id":"1295570","poster":"Shasha1","timestamp":"1728561540.0","upvote_count":"1"},{"content":"Selected Answer: EF\nE and F","comment_id":"1289026","poster":"Leo212003","upvote_count":"1","timestamp":"1727271660.0"},{"timestamp":"1725360000.0","poster":"maxdanny","upvote_count":"4","comment_id":"1277494","content":"Selected Answer: BC\nB. Cloud Deployment Manager APIs could be deprecated in the future: There's always a risk that APIs and tools can be deprecated or replaced with new versions. This could impact the long-term stability of your deployment process if the APIs used by Cloud Deployment Manager are deprecated and require migration to new APIs.\n\nC. Cloud Deployment Manager is unfamiliar to the company's engineers: If the company's engineers are not familiar with Cloud Deployment Manager, there could be a learning curve and potential delays during the migration process. Training and adaptation time could affect productivity and introduce risks associated with potential mistakes or inefficiencies during the transition."},{"content":"B. Cloud deployment manager is being deprecated.\nF. Cloud Deployment Manager only supports automation of Google Cloud resources\n\nThe question is about business risks - it's not about technical risks \nA C D E options are technical aspects of the Deployment manager","upvote_count":"2","timestamp":"1723715520.0","comment_id":"1266342","poster":"Armne96X"}],"isMC":true,"exam_id":4,"topic":"1","answer_ET":"CF","question_text":"A lead engineer wrote a custom tool that deploys virtual machines in the legacy data center. He wants to migrate the custom tool to the new cloud environment.\nYou want to advocate for the adoption of Google Cloud Deployment Manager.\nWhat are two business risks of migrating to Cloud Deployment Manager? (Choose two.)","answer_images":[],"question_id":125,"answers_community":["CF (41%)","CE (20%)","EF (16%)","Other"],"unix_timestamp":1622573040,"question_images":[]}],"exam":{"id":4,"provider":"Google","name":"Professional Cloud Architect","lastUpdated":"11 Apr 2025","isImplemented":true,"isMCOnly":false,"isBeta":false,"numberOfQuestions":279},"currentPage":25},"__N_SSP":true}