{"pageProps":{"questions":[{"id":"XOe8qBc7Y8lapwOEf5jZ","answers_community":["B (71%)","D (25%)","4%"],"url":"https://www.examtopics.com/discussions/google/view/6896-exam-professional-cloud-architect-topic-1-question-81/","answer_images":[],"choices":{"A":"Provision preemptible VMs to reduce cost. Discontinue use of all GCP services and APIs that are not HIPAA-compliant.","D":"Provision standard VMs to the same region to reduce cost. Disable and then discontinue use of all GCP services and APIs that are not HIPAA-compliant.","C":"Provision standard VMs in the same region to reduce cost. Discontinue use of all GCP services and APIs that are not HIPAA-compliant.","B":"Provision preemptible VMs to reduce cost. Disable and then discontinue use of all GCP services and APIs that are not HIPAA-compliant."},"question_text":"Your company operates nationally and plans to use GCP for multiple batch workloads, including some that are not time-critical. You also need to use GCP services that are HIPAA-certified and manage service costs.\nHow should you design to meet Google best practices?","unix_timestamp":1571668260,"answer":"B","topic":"1","exam_id":4,"question_images":[],"timestamp":"2019-10-21 16:31:00","discussion":[{"content":"Disabling and then discontinuing allows you to see the effects of not using the APIs, so you can gauge (check) alternatives. So that leaves B and D as viable answers. The question says only some are not time-critical which implies others are... this means preemptible VMs are good because they will secure a spot for scaling when needed. So I'm also going to choose B.","comment_id":"17793","poster":"Eroc","comments":[{"upvote_count":"11","comments":[{"upvote_count":"6","timestamp":"1617367560.0","poster":"army234","comments":[{"content":"agree otherwise answer goes to non-preemtible VM's","comment_id":"696374","poster":"AzureDP900","upvote_count":"2","timestamp":"1665937500.0"},{"poster":"Sur_Nikki","timestamp":"1683535800.0","upvote_count":"2","content":"Ver well said...\"In an exam it's important to not being in individual assumptions and focus on the information in question. Key word here is \"not time-critical\"\"","comment_id":"891938"}],"comment_id":"326617","content":"No mention of others in the question. In an exam it's important to not being in individual assumptions and focus on the information in question. Key word here is \"not time-critical\""},{"timestamp":"1599981480.0","poster":"Darahaas","comment_id":"178597","upvote_count":"3","content":"And the others are not spoken about. By taking the question just by the context that it sets, preemptible is what I choose. So it's B according to me."}],"comment_id":"148980","content":"If others are time-critical, preemtible does not fit. Answer is D.","poster":"Musk","timestamp":"1596355920.0"},{"upvote_count":"1","poster":"Sur_Nikki","content":"Correctly explained","timestamp":"1683882360.0","comment_id":"895780"},{"content":"correct","comment_id":"696373","upvote_count":"1","poster":"AzureDP900","timestamp":"1665937440.0"},{"comment_id":"1147535","poster":"netizens","upvote_count":"1","content":"Why you decided to emphasize on key word \"not time-critical\" but not\" \"operates nationally\"?","timestamp":"1707671880.0"}],"upvote_count":"43","timestamp":"1572194520.0"},{"comments":[{"comment_id":"150476","content":"I dont think it means use premtible vms for everything. It says to use preemtible vms to reduce cost","upvote_count":"10","timestamp":"1596549360.0","poster":"[Removed]"}],"timestamp":"1594731420.0","upvote_count":"20","poster":"Karna","content":"They say that some (not all) of the Batch workloads are not time critical which implies that there are time critical Batch workloads for which Preemptible VMs are not appropriate, so going with D as the answer","comment_id":"134847"},{"poster":"3a7557a","content":"Selected Answer: D\n. Provision standard VMs to the same region to reduce cost. Disable and then discontinue use of all GCP services and APIs that are not HIPAA-compliant. \n\nyou can't use all premeptible, because only some are not time senstive, rest are time critical.. cash... D.\n\nkeyword google best practice... if you have any time senstive u have to have some standard VMI.","comment_id":"1353612","timestamp":"1739047260.0","upvote_count":"1"},{"content":"Selected Answer: B\nI will go for B.","timestamp":"1735529460.0","comment_id":"1333893","upvote_count":"1","poster":"JonathanSJ"},{"upvote_count":"1","poster":"desertlotus1211","comment_id":"1322161","timestamp":"1733360700.0","content":"Selected Answer: A\nThe answer is A. Though some workloads are critical, the question is really asking about saving costs on the non-critical workloads. it's already understood that some workloads will incur a cost. So 'how' do we save in general?? Use preemptible VMs."},{"comment_id":"1302186","timestamp":"1729712580.0","poster":"nareshthumma","content":"Answer is B","upvote_count":"1"},{"poster":"Ponchi14","comment_id":"1242067","upvote_count":"1","timestamp":"1720096080.0","content":"D is the correct answer and for the following regions:\n1. Deploying a read replica and then manually failing over by stopping the current cloud SQL instance will only impact the authentication layer, which is what we're looking to test.\n2. Stopping all VM's won't have an impact on the authentication layer testing becasuse Cloud SQL is a PaaS service, you cannot just turn off the VM's"},{"content":"What is the difference between A and B other than adding the obvious that you'd disable them if you discontinue using them. This is the most obnoxiously confusing question I've ever read and someone else pointed out that *some* are not time critical which implies others are time critical.","comment_id":"1231972","poster":"Sephethus","upvote_count":"1","timestamp":"1718635020.0"},{"content":"Selected Answer: D\nIf the question would have said: \"The batch are not time-critical\", then Option B with preentible VMs.\n\nBUT, it clearly points that only SOME are not time-critical\". Option D is the only one that satisfies all conditions","poster":"JaimeMS","timestamp":"1717350240.0","upvote_count":"1","comment_id":"1223232"},{"timestamp":"1700461740.0","comment_id":"1075193","content":"Selected Answer: D\nD\nAs per the requirement, some of the batches are not time critical - which means some are critical. Choosing preemptible VMs may mean time critical batches may be affected. Cost effective solutions should not come at cost of requirements. \nDisabling and then discontinuing allows you to see the effects of not using the APIs, so you can gauge (check) alternatives.","poster":"thewalker","upvote_count":"1"},{"comment_id":"1042629","timestamp":"1697197920.0","poster":"Arun_m_123","upvote_count":"2","content":"Selected Answer: B\nWhen it is \"some\" batches are non-critical - when we want to focus on price reduction and if it is batch apps, then we can definitely choose \"Pre-emptible VMs\". Also non-compliant APIs needs to be disabled for sure (APIs can be enabled if there is a need).\n\nPutting altogether, B is the right answer"},{"comment_id":"973412","content":"Selected Answer: D\nI think the answer should be D. some of the batches are not time critical which means some are. Choosing premtible VMs may mean time critical batches may be affected in some cases. Even though the solution needs to be cost effective, it should not come at cost of requirements. Hence D","upvote_count":"3","poster":"Net50","comments":[{"content":"Premptible VMs is better for batch workloads which are critical and not critical since it takes bit longer to complete the load when some VMs are preempted . It greatly reduces cost with using preempted vms for batch workloads. \n\nB is the better answer","timestamp":"1712772420.0","comment_id":"1193203","poster":"Diwz","upvote_count":"1"}],"timestamp":"1691275620.0"},{"upvote_count":"1","timestamp":"1683535740.0","content":"B looks good to me","poster":"Sur_Nikki","comment_id":"891935"},{"poster":"8d31d36","upvote_count":"1","comment_id":"810069","timestamp":"1676504280.0","content":"To design a GCP solution that meets Google's best practices for operating nationally with multiple batch workloads, including some that are not time-critical, and using HIPAA-compliant services while managing service costs, you should provision standard VMs in the same region to reduce cost, and use GCP services that are HIPAA-compliant as needed. Therefore, the correct option is C.\n\nPreemptible VMs can provide cost savings, but they are not recommended for workloads that are not time-critical, as they may be interrupted at any time. Provisioning standard VMs in the same region will provide better performance and stability, and can still be cost-effective by using features such as sustained-use discounts and committed use discounts."},{"timestamp":"1668012960.0","comment_id":"714744","content":"Selected Answer: B\nB is ok","upvote_count":"1","poster":"megumin"},{"upvote_count":"4","poster":"minmin2020","comment_id":"694757","timestamp":"1665752940.0","content":"Selected Answer: B\nAssumption here is that cost is more important that the time critical batches, therefore use preemptible instances. Disable and discontinue is a better option as it gives you opportunity to see the impact before blasting any APIs or services that are not certified."},{"comment_id":"685509","content":"ans is B - HAHAHA","poster":"Prashant2022","timestamp":"1664799120.0","upvote_count":"2"},{"content":"Selected Answer: B\nhttps://cloud.google.com/security/compliance/hipaa#unique_features","upvote_count":"3","timestamp":"1659610260.0","poster":"DrishaS4","comment_id":"642334"},{"timestamp":"1658574180.0","comment_id":"635539","poster":"backhand","upvote_count":"1","content":"vote B\nit's obviously, keywords \"multiple batch workloads\" and \"not time-critical\", preemptible vm first choice."},{"upvote_count":"1","poster":"amonzo","content":"I think answer is D cos for using preemtible instances, you need to guarantee batch jobs are able to continue without losing data or causing an issue when vm restarts. There is no that guarantee in the question.","timestamp":"1655915460.0","comment_id":"620511"},{"content":"Selected Answer: B\nIt took me a while to finally decided the answer should be B, for the following reasons :\n- in the question it said \"multiple batch workloads\" it doesn't matter if it's critical or not it's still patching , then we need to pick Preemptable VM \n\n- from GCP documentation ( https://cloud.google.com/security/compliance/hipaa#unique_features ) , they explicitly talked about Preemabtible VM covered by the HIPAA , and this question want t make sure that we know this info.","comment_id":"587643","poster":"kimharsh","timestamp":"1650283980.0","upvote_count":"2"},{"comment_id":"587200","content":"Selected Answer: B\nYou can also benefit from multi-regional service redundancy as well as the ability to use Preemptible VMs to reduce costs.\nhttps://cloud.google.com/security/compliance/hipaa/","poster":"JohnPi","timestamp":"1650196440.0","upvote_count":"1"},{"poster":"Rajasa","timestamp":"1640578680.0","content":"Selected Answer: B\ngo with B","upvote_count":"1","comment_id":"510008"},{"upvote_count":"2","timestamp":"1638867060.0","comment_id":"495789","content":"Go for B.\nhttps://cloud.google.com/compute/docs/instances/preemptible\nIf your apps are fault-tolerant and can withstand possible instance preemptions, then preemptible instances can reduce your Compute Engine costs significantly. For example, batch processing jobs can run on preemptible instances. If some of those instances stop during processing, the job slows but does not completely stop. Preemptible instances complete your batch processing tasks without placing additional workload on your existing instances and without requiring you to pay full price for additional normal instances.","poster":"haroldbenites","comments":[{"content":"But in case of time critical jobs, as in this case, increasing duration time doesn't match a great part of the requirements because it is clearly say that only \"some\" batches are non time-critical, not all. So D appears more suitable.","timestamp":"1702981320.0","comment_id":"1100520","poster":"JoeJoe","upvote_count":"1"},{"content":"But in case of time critical jobs, as in this case, increasing duration time doesn't match a great part of the requirements because it is clearly said that only \"some\" batches are non time-critical, not all. So D appears more suitable.","upvote_count":"1","poster":"JoeJoe","timestamp":"1702981380.0","comment_id":"1100521"}]},{"timestamp":"1638090780.0","upvote_count":"1","comment_id":"489012","poster":"vincy2202","content":"Selected Answer: B\nB is the correct answer"},{"poster":"joe2211","upvote_count":"1","comment_id":"486338","content":"Selected Answer: B\nvote B","timestamp":"1637799060.0"},{"comment_id":"469817","content":"B – Provisioning preemptible VMs to reduce costs. Disable and discontinue use all GCP services and APIs that are not HIPPA-compliant.\nA - has neat differences from B. It says just “discontinue”, though key word here is “disable”. See this quote from GCP HIPPA page:\n\nEssential best practices:\n• 1) Execute a Google Cloud BAA. You can request a BAA directly from your account manager.\n• 2) Disable or otherwise ensure that you do not use Google Cloud Products that are not explicitly covered by the BAA (see Covered Products) when working with PHI.\n\nAnd this page explains that you need to Enable Cloud APIs for your project. You can disable any APIs for your","comments":[{"comment_id":"469818","poster":"MaxNRG","timestamp":"1635520140.0","upvote_count":"1","content":"https://cloud.google.com/apis/docs/getting-started?hl=en&visit_id=636991287264824416-979825397&rd=1"}],"poster":"MaxNRG","upvote_count":"2","timestamp":"1635520140.0"},{"upvote_count":"2","poster":"thakursumeet89","content":"Why to disable APIs since by default API's are disabled. So A?","comment_id":"469220","timestamp":"1635421200.0"},{"timestamp":"1621423800.0","comment_id":"361315","poster":"victory108","upvote_count":"4","content":"B. Provisioning preemptible VMs to reduce cost. Disable and then discontinue use of all GCP and APIs that are not HIPAA-compliant."},{"content":"B is correct","timestamp":"1621086900.0","poster":"un","comment_id":"357911","upvote_count":"1"},{"timestamp":"1618928940.0","upvote_count":"2","content":"https://cloud.google.com/security/compliance/hipaa/#unique_features\nGCP’s security practices allow us to have a HIPAA BAA covering GCP’s entire infrastructure, not a set aside portion of our cloud. As a result, you are not restricted to a specific region which has scalability, operational and architectural benefits. You can also benefit from multi-regional service redundancy as well as the ability to use Preemptible VMs to reduce costs.\nHIPPA doesn't need region restriction. Answer is B","comments":[{"poster":"JoeJoe","upvote_count":"1","timestamp":"1702981620.0","content":"\"you can\" but you don't have to! In our case you must not use preemptible VMs because you have time-critical jobs.\nIMO D is the right choise.","comment_id":"1100522"}],"poster":"jasim21","comment_id":"339635"},{"poster":"Ausias18","timestamp":"1617257160.0","upvote_count":"1","content":"Answer is B","comment_id":"325559"},{"comment_id":"321792","comments":[{"upvote_count":"4","poster":"lynx256","comments":[{"poster":"LisX","upvote_count":"1","comment_id":"442423","content":"B is correct. Data coped cross region will be billed. So low cost really means preemptible and process in the region close to the source.","timestamp":"1631269080.0"}],"comment_id":"325701","content":"Wait a moment... About B or D....\nSo far everyone (including me) wrote about \"time-critical\" and \"not time-critical\".\nBut how about \"Your company operates NATIONALLY [...]\" and \"D.Provision standard VMs to the SAME REGION to reduce cost\" ?\nSo D restricts us to ONE REGION only whereas B restricts us to PREEMPTIBLE VMs only. \nAbout HIPAA. I'm not sure, CMIIW, but I thing HIPAA regulations concern USA only. So our company probably uses GCP resources in many regions in the USA, not only one.\nSo maybe B is more relevant because it does NOT talk about \"region\".\nWhat do you think ?","timestamp":"1617271740.0"}],"upvote_count":"2","poster":"lynx256","content":"B or D ?\n\"multiple batch workloads, including SOME that are not time-critical\" - this means MOST of them are TIME CRITICAL.\nAlso we don't know how long particular batches run and if they can be repreated in case of break. \nI'll go with D.","timestamp":"1616844720.0"},{"comment_id":"270992","poster":"ybe_gcp_cert","upvote_count":"3","content":"B or D. Preemptible or standard.\nThe question is using \"some batches are not time-critical\" to mislead. \nEven if some are still time critical, they are still batches and, in real life most batch use cases \n will run nightly and last less than 24h. The question doesn't give details on batch's use case.\n\nPreemptible will do the job.\nI would go with B.","timestamp":"1611042960.0"},{"poster":"AshokC","comment_id":"180453","content":"Answer: B","timestamp":"1600274280.0","upvote_count":"4"},{"poster":"wiqi","comment_id":"163214","timestamp":"1598049420.0","content":"I will go with B\nIt makes sense to ensure the usage is disabled and then terminate it.","upvote_count":"3"},{"content":"D) is the right answer\nAs cypt0 stated, the documentation is clear about disabling be prefered (you cannot ensure anything just 'discountinuing', not to talk when you you face an audit process):\nhttps://cloud.google.com/security/compliance/hipaa/\nThe other part (cost) there are 2 options: preemtive VMs or using a common region (I guess to prevent egress costs). preemtive VMs are not an option, because some of the workload is time-critical. It would be an option if it was \"a mix of \", but not one-size-fits-all","poster":"jespinosar","upvote_count":"4","comment_id":"160620","timestamp":"1597731420.0"},{"upvote_count":"1","poster":"mlantonis","comment_id":"117460","timestamp":"1592915880.0","content":"It says \"not time-critical and manage service costs\", so we need to use Preemptible VMs.\nSo C and D are incorrect.\n\nI would choose B."},{"poster":"Tushant","timestamp":"1592648040.0","comment_id":"114689","upvote_count":"2","content":"B is the answer"},{"content":"B is correct answer.\nAs per google doc, https://cloud.google.com/security/compliance/hipaa/\n\nEssential best practices:\n\nExecute a Google Cloud BAA. You can request a BAA directly from your account manager.\nDisable or otherwise ensure that you do not use Google Cloud Products that are not explicitly covered by the BAA (see Covered Products) when working with PHI.","timestamp":"1592479620.0","poster":"kban","comment_id":"113120","upvote_count":"4"},{"timestamp":"1591292040.0","comment_id":"102529","poster":"Ziegler","content":"B is the correct answer.\n What to what out for in this question are:\n - multiple batch workloads, \n - some that are not time-critical. \n - You also need to use GCP services that are HIPAA-certified \n - manage service costs","upvote_count":"2"},{"content":"B, for sure.\nDisable and then discontinue","timestamp":"1591074720.0","comment_id":"100542","upvote_count":"3","poster":"gfhbox0083","comments":[{"timestamp":"1591273680.0","poster":"gfhbox0083","comment_id":"102365","upvote_count":"1","content":"... And preemptible VMs, for not time-critical workloads"}]},{"content":"Final Decision to go with Option B","upvote_count":"4","timestamp":"1590665700.0","comment_id":"97483","poster":"AD2AD4"},{"poster":"gcp_aws","comment_id":"95659","content":"B is the answer","upvote_count":"2","timestamp":"1590439740.0"},{"timestamp":"1585620780.0","content":"preemptible VMs last only 24 hours. so, I think the answer D","comments":[{"content":"Guys, I agree with most of you and I go with B as this multiple batch workloads, including some that are not time-critical. https://cloud.google.com/preemptible-vms","timestamp":"1585955940.0","comment_id":"70871","comments":[{"comment_id":"148979","upvote_count":"1","poster":"Musk","timestamp":"1596355860.0","content":"What about the time critical batches. Preemtible means they can be interrupted. I go with D."}],"poster":"anton_royce","upvote_count":"2"}],"upvote_count":"1","comment_id":"69698","poster":"anton_royce"},{"upvote_count":"1","timestamp":"1582406460.0","comment_id":"53917","poster":"Smart","content":"I think it is important to disable APIs. In case, you haven't discontinued its usage there is no chance to accidentally use them and be out of compliance."},{"poster":"jobs","timestamp":"1574818920.0","comment_id":"24724","upvote_count":"4","content":"agree B"},{"poster":"KouShikyou","upvote_count":"5","content":"I think discontinuing implies changing the application.\nI would select B.","timestamp":"1573954500.0","comment_id":"22086"},{"timestamp":"1571911560.0","poster":"crypt0","comment_id":"17146","content":"If I am reading this correct, here they are saying you have to disable the APIs:\n\nDisable or otherwise ensure that you do not use Google Cloud Products that are not explicitly covered by the BAA (see Covered Products) when working with PHI.\n\nhttps://cloud.google.com/security/compliance/hipaa/","upvote_count":"8"},{"poster":"KouShikyou","timestamp":"1571831280.0","content":"Totally don't have idea about this one.\nAny reference document?","upvote_count":"1","comment_id":"16940"},{"upvote_count":"1","comment_id":"16447","content":"Answer B implies disable APIs? Think is enough not using it, isn't it? In this case correct answer is A","comments":[{"content":"disabling will guarantee that APIs are not used","timestamp":"1592938740.0","poster":"motty","upvote_count":"2","comment_id":"117760","comments":[{"timestamp":"1596759660.0","poster":"tartar","content":"B is ok","comment_id":"152228","upvote_count":"2"}]},{"upvote_count":"2","comment_id":"210542","content":"B is correct","timestamp":"1604243100.0","poster":"kumarp6"},{"upvote_count":"1","comment_id":"303751","content":"B os correct anwer","timestamp":"1614905760.0","poster":"nitinz"}],"poster":"jcmoranp","timestamp":"1571668260.0"}],"answer_description":"","answer_ET":"B","question_id":181,"isMC":true},{"id":"2udmniuD2bjZ4SNoDazB","timestamp":"2019-10-17 12:42:00","question_id":182,"isMC":true,"answer":"C","url":"https://www.examtopics.com/discussions/google/view/6709-exam-professional-cloud-architect-topic-1-question-82/","exam_id":4,"choices":{"A":"Engage with a security company to run web scrapers that look your for users' authentication data om malicious websites and notify you if any is found.","C":"Schedule a disaster simulation exercise during which you can shut off all VMs in a zone to see how your application behaves.","D":"Configure a read replica for your Cloud SQL instance in a different zone than the master, and then manually trigger a failover while monitoring KPIs for our REST API.","B":"Deploy intrusion detection software to your virtual machines to detect and log unauthorized access."},"answer_images":[],"answers_community":["C (64%)","D (36%)"],"unix_timestamp":1571308920,"answer_description":"","question_images":[],"topic":"1","answer_ET":"C","discussion":[{"timestamp":"1575371760.0","comments":[{"comments":[{"timestamp":"1639785720.0","comment_id":"503945","poster":"vartiklis","content":"You're not testing *authentication*, you're testing *the resilience of the authentication layer*. \"A resilient app is one that continues to function despite failures of system components\" (https://cloud.google.com/architecture/scalable-and-resilient-apps#resilience_designing_to_withstand_failures) - such as shutting down all VMs in a zone.","upvote_count":"19"},{"comments":[{"comment_id":"989176","content":"yes chaos testing is industry standard","poster":"heretolearnazure","timestamp":"1692879420.0","upvote_count":"2"}],"content":"Agree, Chaos testing is to shutdown random instances.","timestamp":"1654138020.0","comment_id":"610417","upvote_count":"4","poster":"elaineshi"}],"timestamp":"1590953700.0","upvote_count":"6","comment_id":"99560","content":"Shutting off all VMs in a zone is not good approach for testing of authentication","poster":"Jack_in_Large"}],"content":"As per google documentation(https://cloud.google.com/solutions/scalable-and-resilient-apps) answer is C.\n\nC: A well-designed application should scale seamlessly as demand increases and decreases, and be resilient enough to withstand the loss of one or more compute resources.\nResilience: designed to withstand the unexpected\nA highly-available, or resilient, application is one that continues to function despite expected or unexpected failures of components in the system. If a single instance fails or an entire zone experiences a problem, a resilient application remains fault tolerant—continuing to function and repairing itself automatically if necessary. Because stateful information isn’t stored on any single instance, the loss of an instance—or even an entire zone—should not impact the application’s performance.","comment_id":"26227","upvote_count":"63","poster":"Kri_2525"},{"comments":[{"poster":"Darahaas","timestamp":"1599981660.0","upvote_count":"4","content":"Resilience testing of the \"Authentication Layer\", not the \"Application\". So the answer is B.","comment_id":"178599"}],"poster":"KouShikyou","timestamp":"1573955460.0","upvote_count":"16","content":"Since the question is asking to do a resilience testing, I prefer C.","comment_id":"22089"},{"poster":"david_tay","comment_id":"1360067","upvote_count":"1","timestamp":"1740213900.0","content":"Selected Answer: D\nanswer is D, check in Gemini to understand why."},{"timestamp":"1735529760.0","poster":"JonathanSJ","comment_id":"1333895","content":"Selected Answer: C\nI will go for C.","upvote_count":"1"},{"upvote_count":"2","timestamp":"1732037940.0","content":"Selected Answer: C\nD. is not correct as this tests the resilience of the database (Cloud SQL) but not necessarily the authentication layer. The authentication layer might have separate components or dependencies that need to be tested under failure conditions.","poster":"Ekramy_Elnaggar","comment_id":"1314822"},{"upvote_count":"1","content":"Agree with C","poster":"nareshthumma","comment_id":"1302187","timestamp":"1729712700.0"},{"upvote_count":"2","content":"Selected Answer: D\nOption C, which involves scheduling a disaster simulation exercise to shut off all VMs in a zone, is indeed a strong choice for resilience testing. This approach helps you understand how your application behaves under failure conditions and ensures that your system can handle unexpected disruptions.\n\nHowever, Option D is also highly relevant. Configuring a read replica for your Cloud SQL instance in a different zone and manually triggering a failover while monitoring KPIs for your REST API directly tests the resilience of your database layer. This can provide valuable insights into the failover process and the impact on your application’s performance and availability.\n\nBoth options have their merits, but if the primary goal is to test the resilience of the authentication layer specifically, Option D might be more targeted and effective.","poster":"wooyourdaddy","comment_id":"1291165","timestamp":"1727618220.0"},{"upvote_count":"2","content":"Selected Answer: C\nC is correct. It is not D because you are not designing system, your goal is testing existed system","comment_id":"1212550","timestamp":"1715884740.0","poster":"hitmax87"},{"upvote_count":"3","content":"Selected Answer: C\nChaos testing","poster":"666Amitava666","comment_id":"1201618","timestamp":"1713989280.0"},{"content":"I choose Answer C\nhttps://cloud.google.com/sql/docs/mysql/replication \nThis URL states \"Read replicas are read-only; you cannot write to them. The read replica processes queries, read requests, and analytics traffic, thus reducing the load on the primary instance.\" \n\"Note: Read replicas do not provide failover capability. To provide failover capability for an instance, see Configuring an instance for high availability.\"\n\"As a best practice, put read replicas in a different zone than the primary instance when you use HA on your primary instance. This practice ensures that read replicas continue to operate when the zone that contains the primary instance has an outage. See the Overview of high availability for more information.\"","comment_id":"1189571","timestamp":"1712273400.0","poster":"activist","upvote_count":"3"},{"comment_id":"1183975","poster":"santoshchauhan","timestamp":"1711532880.0","upvote_count":"1","content":"Selected Answer: D\nTesting Database Resilience: By setting up a read replica in a different zone and triggering a manual failover, you simulate a failure of the primary database. This allows you to assess how well your authentication layer and the overall application cope with the loss of the primary database.\n\nMonitoring Performance and Availability: During the failover, monitoring key performance indicators (KPIs) for your REST API will give you insights into how the application's performance and availability are impacted. This helps in identifying potential bottlenecks and areas for improvement in your resilience strategy.\n\nEnsuring Data Continuity: A read replica ensures data continuity and minimizes downtime, which is critical for an authentication system. The replica will take over as the primary database during the failover, ensuring that the authentication service remains functional."},{"comment_id":"1177234","upvote_count":"1","poster":"Rehamss","timestamp":"1710846120.0","content":"Selected Answer: D\nD is okay"},{"poster":"Teckexam","content":"Selected Answer: C\nAuthentication layer resiliency can be covered as part of overall application resiliency testing. Option C is asking to use read replica which is not useful in case of testing resiliency in case of failure","upvote_count":"2","comment_id":"1151607","timestamp":"1708038060.0"},{"comment_id":"1147517","timestamp":"1707670500.0","poster":"practice_sample","upvote_count":"3","content":"Selected Answer: C\nRead replicas do not provide failover capability. \nhttps://cloud.google.com/sql/docs/mysql/replication#read-replicas"},{"content":"Selected Answer: C\nIt is c","poster":"didek1986","comment_id":"1128063","upvote_count":"2","timestamp":"1705863000.0"},{"timestamp":"1703570280.0","poster":"Tamim321","comment_id":"1105736","upvote_count":"6","content":"Selected Answer: C\nRead replica do not provide failover capability \nhttps://cloud.google.com/sql/docs/mysql/replication#:~:text=Note%3A%20Read%20replicas%20do%20not,HA%20on%20your%20primary%20instance."},{"poster":"Roro_Brother","upvote_count":"2","content":"Selected Answer: C\nC is the good choice","comment_id":"1096515","timestamp":"1702561860.0"},{"timestamp":"1699027860.0","upvote_count":"6","content":"I choose C.\nI don't say D because the REST API read and WRITE in the database, if you create a READ replica in Cloud SQL, the REST API will not have the possibility of write in the database. The answer D doesn't mention anything about promote the read replica to master.","comments":[{"poster":"parthkulkarni998","timestamp":"1702705800.0","comment_id":"1097937","content":"Exactly. Because in GCP a read replica cant be auto upgraded to become a master in case of failover. So basically the database will allow only READ operations and not WRITE operations. Basically leaving it non-functional","upvote_count":"1"}],"comment_id":"1061579","poster":"juliansierra"},{"poster":"RuibinC","timestamp":"1697487840.0","comment_id":"1045247","content":"Selected Answer: C\nC is good","upvote_count":"2"},{"poster":"Prakzz","timestamp":"1696484580.0","comment_id":"1025300","upvote_count":"3","content":"Selected Answer: C\nD can be correct if it's a Failover replica but it's a read replica and not have anything to do with resiliency. It's C"},{"comments":[{"content":"I think answer should be C, because Read Replicas are there for improving scalability and not availability. So whenever, a node/zone goes down, a read replica wont auto transform to a master. For that you need a failover replica by enabling HA configuration in VMs.","poster":"parthkulkarni998","timestamp":"1706348820.0","comment_id":"1133215","upvote_count":"1"}],"upvote_count":"3","content":"Selected Answer: D\nOption A is focused on external threat intelligence and is more suited to security testing rather than resilience testing.\n\nOption B is related to security monitoring, and while important, does not directly address resilience testing requirements.\n\nOption C simulates a zone failure scenario. This could provide insights into how the application behaves in a failure scenario, making it a valid resilience testing method. However, it does not specifically address the interaction with Cloud SQL.\n\nOption D directly addresses resilience testing involving a Cloud SQL instance by creating a read replica in a different zone and simulating a failover. This will allow you to assess the impact on your REST API and verify whether the authentication layer remains functional and available, even when the primary Cloud SQL instance is inaccessible.","timestamp":"1695817740.0","comment_id":"1018791","poster":"marcohol"},{"content":"its quite tricky here\nresilient = chaos test. \nso with C, we have shut down all VMs in one zone to see how its it working on another zone( note that this is regional ) is my chose","poster":"ductrinh","timestamp":"1695800940.0","comment_id":"1018582","upvote_count":"2"},{"timestamp":"1695588540.0","content":"The application was deployed to a managed Instance Group, it must have been configured with a load balancer, health check and AutoHealing. Turning off VMs would instantly bring up another instance as soon as the health checks fail, and traffic is automatically redirected to these instances. Therefore, the Answer is D.","upvote_count":"2","poster":"piiizu","comment_id":"1016174"},{"upvote_count":"1","poster":"didek1986","timestamp":"1693843980.0","content":"Selected Answer: D\nd is correc","comment_id":"998680"},{"content":"Selected Answer: D\nThis will test the ability of your authentication layer to continue to function even if one of the zones fails. By configuring a read replica in a different zone, you can ensure that there is always a copy of your data available. And, by manually triggering a failover, you can test how your application behaves when the primary instance is unavailable.","upvote_count":"3","timestamp":"1693705200.0","comment_id":"997233","poster":"ChauHuuPhat"},{"upvote_count":"1","poster":"Sur_Nikki","comment_id":"891946","timestamp":"1683536160.0","content":"Resilience testing of their authentication layer means the testing of availability of service/application even when many of the instances fail in a particular location. That’s why. Disaster type of scenario is better where all VM instances becomes unavailable in a particular zone"},{"timestamp":"1683180000.0","upvote_count":"2","poster":"salim_","comment_id":"889187","content":"Selected Answer: C\nC as it's definition of resilience testing. D is not correct at all as when disaster occurs you don't have time to \"Configure a read replica for your Cloud SQL instance in a different zone than the master\""},{"content":"Selected Answer: D\nD is right","poster":"Hisayuki","timestamp":"1682140080.0","comment_id":"877032","upvote_count":"2"},{"upvote_count":"5","timestamp":"1681952100.0","comment_id":"875193","poster":"JC0926","content":"Selected Answer: D\nD. Configure a read replica for your Cloud SQL instance in a different zone than the master, and then manually trigger a failover while monitoring KPIs for our REST API.\n\nThis option is the most suitable for resilience testing of the authentication layer. By configuring a read replica in a different zone than the master, you add redundancy to your system. Manually triggering a failover while monitoring KPIs for your REST API helps you observe how the system behaves during a failure and ensures that the authentication layer remains available and performs as expected during an outage or disaster."},{"upvote_count":"2","timestamp":"1679918100.0","content":"Selected Answer: C\nvote for C","comment_id":"852072","poster":"taer"},{"upvote_count":"2","comment_id":"848418","poster":"feholen210","timestamp":"1679587680.0","content":"Selected Answer: C\nC is correct."},{"content":"Selected Answer: C\nShould be C","upvote_count":"2","comment_id":"848395","poster":"Hawik","timestamp":"1679586240.0"},{"poster":"Deb2293","content":"Selected Answer: D\nD is right","comment_id":"844128","timestamp":"1679253660.0","upvote_count":"1"},{"poster":"zeekerblade","comment_id":"841523","content":"Selected Answer: C\nIn the question, \n//This consists of a regional managed instance group serving a public REST API that reads from and writes to a Cloud SQL instance.\nIt means it consist of MIG, but does not imply it consists of Cloud SQL as well.\n\nSome argues that MIG is regional and don't need to be tested is definitely incorrect.\nthere are many configurations may differs the behaviors and even cloud provider stated does not mean that we do not need to test it out.","timestamp":"1679019480.0","upvote_count":"2"},{"comment_id":"841478","timestamp":"1679015580.0","upvote_count":"1","poster":"JC0926","content":"Selected Answer: D\nOption D is a valid method of testing the resilience of the authentication layer. Configuring a read replica in a different zone allows for redundancy and failover capabilities in the event of a disaster. By manually triggering a failover and monitoring the KPIs for the REST API, the resilience of the authentication layer can be tested under realistic conditions."},{"timestamp":"1678778220.0","content":"Selected Answer: D\nD => instance group is alreay regional the point of failure is the cloud sql so need to test resilience","upvote_count":"1","comment_id":"838615","poster":"telp"},{"upvote_count":"3","poster":"Deb2293","comment_id":"829519","timestamp":"1677978900.0","comments":[{"poster":"Deb2293","content":"Option C is incorrect as it relies on using the bq command line tool to list jobs and job information, which may not be the most efficient or reliable way to get the information required. Additionally, it may require more manual effort to compile the data for each user.","comment_id":"829522","timestamp":"1677979260.0","upvote_count":"1"}],"content":"Selected Answer: D\nOption D is right\nOption C is also incorrect because shutting off all VMs in a zone is not a realistic scenario for testing the resilience of the authentication layer, as it does not simulate a specific failure mode. Additionally, it may have unintended consequences on other parts of the application that rely on the same VMs."},{"upvote_count":"6","timestamp":"1671697380.0","comment_id":"753086","poster":"omermahgoub","content":"The correct answer is D. Configure a read replica for your Cloud SQL instance in a different zone than the master, and then manually trigger a failover while monitoring KPIs for our REST API.\n\nResilience testing is a process of evaluating the ability of a system to recover from failures or disruptions. In this case, the customer has an authentication layer consisting of a regional managed instance group serving a public REST API that reads from and writes to a Cloud SQL instance.\n\nTo perform resilience testing, one option would be to configure a read replica for the Cloud SQL instance in a different zone than the master. This would allow you to test the system's ability to recover from a failure or disruption in one zone. Then, you could manually trigger a failover while monitoring key performance indicators (KPIs) for the REST API, such as response times and error rates. This will allow you to see how the system behaves during a failover and identify any potential issues that need to be addressed.","comments":[{"timestamp":"1683882600.0","poster":"Sur_Nikki","comment_id":"895782","upvote_count":"1","content":"Sorry, but I don't agree with your answer. I would like to move with C"},{"upvote_count":"4","timestamp":"1671697440.0","content":"Option A, engaging with a security company to run web scrapers looking for users' authentication data on malicious websites, is not related to resilience testing. \n\nOption B, deploying intrusion detection software to your virtual machines to detect and log unauthorized access, is a good practice for improving the security of your system, but it is not directly related to resilience testing. \n\nOption C, scheduling a disaster simulation exercise during which you can shut off all VMs in a zone to see how your application behaves, is a valid method of resilience testing, but it does not involve testing the system's ability to recover from a failure or disruption in one zone.","poster":"omermahgoub","comment_id":"753087"}]},{"poster":"fiercedog","content":"Selected Answer: D\nAnswer is D.\n\nApp layer is covered by regional managed instance group.\nDatabase is the point of failure that needs to be addressed.","comment_id":"739652","timestamp":"1670548980.0","upvote_count":"4"},{"comment_id":"714746","upvote_count":"2","timestamp":"1668013140.0","poster":"megumin","content":"Selected Answer: C\nC is ok"},{"upvote_count":"1","comment_id":"696372","timestamp":"1665937320.0","content":"C is right - It correct way to test.","poster":"AzureDP900"},{"upvote_count":"2","timestamp":"1665754020.0","poster":"minmin2020","comment_id":"694765","content":"Selected Answer: C\nC is the only option closer to testing resilience of the authentication layer\nA and B I cannot see how resilience of the authentication layer is tested \nD will only test SQL"},{"content":"Selected Answer: C\nSo, C and D to consider.\nResiliency definition: a highly-available, or resilient app is one that continues to function despite expected or unexpected failures of components in the system.","upvote_count":"3","timestamp":"1659610440.0","poster":"DrishaS4","comment_id":"642336"},{"poster":"xinsong001","content":"The answer is C: \nRegarding the resilience test, they don't mean to test MIG autoscaling, which has been tested by Google. What they want to test is whether or not the request for authentication will be lost if the VM node suddenly crashed when it's sending API request","comment_id":"615818","upvote_count":"3","timestamp":"1655129160.0"},{"poster":"haroldbenites","upvote_count":"3","comment_id":"495793","content":"Go for C.","timestamp":"1638867360.0"},{"content":"Answer is C","timestamp":"1638092040.0","upvote_count":"2","poster":"vincy2202","comment_id":"489019"},{"comment_id":"487555","upvote_count":"4","poster":"mudot","content":"Selected Answer: C\nA & B - not resiliency testing\nD - tests only for SQL \n\nits C","timestamp":"1637951160.0"},{"poster":"joe2211","content":"Selected Answer: C\nvote C","timestamp":"1637799240.0","comment_id":"486339","upvote_count":"2"},{"upvote_count":"4","timestamp":"1635520380.0","comments":[{"upvote_count":"3","content":"HA Configuration of Cloud SQL – shows Failover example with Master and Read + Failover replica: https://cloud.google.com/sql/docs/mysql/high-availability\nRead also about Building scalable and Resilient Apps: https://cloud.google.com/architecture/scalable-and-resilient-apps","poster":"MaxNRG","timestamp":"1635520380.0","comment_id":"469823"}],"comment_id":"469822","poster":"MaxNRG","content":"C – schedule a disaster simulation exercise during which you can shut off all VMs in a zone to see how your app behaves.\n\nA, B – don’t test resilience at all, just security / authentication attacks. But, Q asks different – check if authentication works if some resources are down. So, C and D to consider.\nResiliency definition: a highly-available, or resilient app is one that continues to function despite expected or unexpected failures of components in the system.\nD – says about Read Replica and trigger a failover. But, Read replicas neither provide High-Availability nor offer it; a master instance cannot failover to a read replica, and read replicas are unable to fail over in any way during outage.\nC – focuses on both aspects: resilience and authentication layer testing by shutting down FrontEnd VMs in the zone. Cloud SQL testing is OOO, since this is back-end / data layer."},{"poster":"PeppaPig","timestamp":"1627918380.0","comment_id":"418806","content":"I would go with D. Obviously a single DB instance would become a single point of failure. You can prompt a cross-region read replica to the primary in the case of DR.\nhttps://cloud.google.com/sql/docs/mysql/replication/cross-region-replicas","upvote_count":"5"},{"upvote_count":"5","content":"Answer: C\nSupport: Read replicas do not provide failover capability. So option D is out.","timestamp":"1626944280.0","poster":"Unfaithful","comment_id":"411485"},{"comments":[{"timestamp":"1626429240.0","comment_id":"407771","poster":"SSV","upvote_count":"3","content":"could you please post those question here itself. I am unable to access #152"}],"timestamp":"1626315300.0","upvote_count":"2","comment_id":"406671","content":"hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152","poster":"kopper2019"},{"content":"D is correct. \nRegional MIG by default ensures resilience of VMs on application layer\nWhat you really need to test is the Database layer as there is only a single DB instance that's obviously became single point of failure.\nManual failover means you promote a replica as a standalone database, it is a recommended practice by GCP in disaster recovery.\nRef: https://cloud.google.com/sql/docs/mysql/replication/cross-region-replicas#promote-a-replica","upvote_count":"9","comment_id":"405571","comments":[{"timestamp":"1636453620.0","poster":"robotgeek","comment_id":"474733","content":"agree, in a MIG shutting down VMs serves no purpose, am I right?","upvote_count":"1"},{"comment_id":"458190","content":"This is the correct answer + explanation","poster":"pnearn","timestamp":"1633517460.0","upvote_count":"1"}],"poster":"PeppaPig","timestamp":"1626194280.0"},{"upvote_count":"1","content":"C is correct","timestamp":"1622436720.0","poster":"NYKC","comment_id":"370643"},{"timestamp":"1621423620.0","poster":"victory108","comment_id":"361312","content":"C. Schedule a disaster simulation exercise during which you can shut off all VMs in a zone to see how your application behaves.","upvote_count":"4"},{"poster":"un","upvote_count":"1","timestamp":"1621087140.0","content":"i would go with C","comment_id":"357917"},{"upvote_count":"1","comment_id":"325560","poster":"Ausias18","timestamp":"1617257400.0","content":"answer is C"},{"upvote_count":"2","comment_id":"321805","content":"IMO - C is ok.\nD is about resilience architecture/solution. But we want to TEST their authentication layer for RESILIANCY.\nA and B are about testing of their authentication layer base functionality, not it's RESILIENCY.","timestamp":"1616845740.0","poster":"lynx256"},{"timestamp":"1616144040.0","content":"Resilience testing is \"Resilience testing is a type of software testing that observes how applications act under stress. It's meant to ensure the product's ability to perform in chaotic conditions without a loss of core functions or data; it ensures a quick recovery after unforeseen, uncontrollable events\" \n\nI guess it must be C","upvote_count":"1","comment_id":"314688","poster":"AndreUanKenobi"},{"comment_id":"279007","content":"This is about \"resilience testing\". only C means something of \"resilience testing\".\nB - it sound to me that you just deploy something and wait. Not cover the \"test\" part.","upvote_count":"1","timestamp":"1611892320.0","poster":"bnlcnd"},{"timestamp":"1609982340.0","content":"Answer should be C. Authentication Layer is just an application nothing to do with what is being asked in this question. D is wrong as Cloud SQL should have a failover replica not a read replica for resiliency. Google recommends and its quite logical to have Failover replica in different zone and read replica in same zone. So D is not the option. B is completely incorrect as we are not testing authenticity of the application but the resiliency.","comment_id":"261389","poster":"Arimaverick","upvote_count":"3"},{"comment_id":"259813","timestamp":"1609805220.0","poster":"mwilbert","upvote_count":"1","content":"C is correct. You aren't testing the authentication itself, you are testing the resilience of the authentication by shutting down a zone and seeing if it still works."},{"comments":[{"upvote_count":"1","poster":"Rothmansua","content":"And how is that related to authentication?","timestamp":"1611871140.0","comment_id":"278856"}],"comment_id":"249696","timestamp":"1608586080.0","poster":"VPK2222","upvote_count":"2","content":"Answer is C \n\nhttps://cloud.google.com/solutions/scalable-and-resilient-apps#test_your_resilience"},{"poster":"ybe_gcp_cert","comment_id":"238096","timestamp":"1607420040.0","upvote_count":"4","content":"A and B focus on security, intrusion test and scraper scan doesn't test the resiliency which is a broader domain.\n\nWhich leave C and D.\nD might do the job but, the problem is changing the configuration by adding a replica. You are supposed to design your application and test it's resilience as is, not add new components. The question doesn't ask about what can you do to improve or implement a DR. it's asking to DO resilience test.\n\nI would go with C."},{"upvote_count":"1","poster":"Chulbul_Pandey","timestamp":"1606900140.0","content":"C is good","comment_id":"232785"},{"upvote_count":"1","content":"I tend to lean towards C:\n1) testing authentication layer is just a smoke screen, the key is to test \"resilience\" of a layer which happened to be authentication layer\n2) I also think added read replica is changing infrastructure and not testing existing setup","poster":"SKSKSK","comment_id":"216330","timestamp":"1604975820.0"},{"poster":"pepYash","comment_id":"216282","content":"This question is quite ambiguous. Resilience talks about the behavior in stress. In this case the authentication layer. None of the options seem to justify the resilient testing.\nB is the only option that makes a little sense.","upvote_count":"2","timestamp":"1604967540.0"},{"comment_id":"205361","poster":"N1_arch","timestamp":"1603580760.0","content":"Answer is C - mention of authentication application in the question is a red-herring. Question is about resiliency testing of a component.","upvote_count":"1"},{"comment_id":"192803","comments":[{"comment_id":"210938","upvote_count":"1","content":"Yes but resilence\nits the same that were talk about any other layer\nI think its C or D","poster":"francisco_guerra","timestamp":"1604286120.0"}],"poster":"akhadar2001","upvote_count":"1","timestamp":"1601802960.0","content":"Question is about authentication layer so answer is B."},{"timestamp":"1600275480.0","upvote_count":"1","comment_id":"180462","poster":"AshokC","content":"C is meaningful"},{"upvote_count":"2","timestamp":"1599923160.0","poster":"richardxyz","comment_id":"178262","content":"C is the answer; D is not correct, a primary instance cannot failover to a read replica and read replicas are unable to failover in any way during an outage."},{"timestamp":"1598801340.0","content":"Answer is C for me.\nOur customer wants to do RESILIENCE testing.\nA & B are security assessment and D is changing architecture (you are testing another configuration, not your production configuration)","poster":"Ale1973","upvote_count":"1","comment_id":"170140"},{"timestamp":"1598652900.0","comment_id":"168799","upvote_count":"1","poster":"Kabiliravi","content":"What is the definition of resiliency tests???\nAnswer is C.\nhttps://cloud.google.com/solutions/scalable-and-resilient-apps"},{"timestamp":"1598108280.0","comment_id":"163707","poster":"wiqi","content":"D is correct, because the ask is about resiliency. Cloud SQL is hosting the authentication DB, hence we have to look at Cloud SQL resiliency. So having a read replica in different zone and triggering the failover will do.","upvote_count":"3"},{"poster":"nezih","upvote_count":"5","content":"this is about testing resilience not improving. So deploying IDS won't give any idea how resilient our system is. Answer is C","timestamp":"1593357660.0","comment_id":"121977"},{"upvote_count":"3","comment_id":"117762","poster":"motty","timestamp":"1592939280.0","content":"A is incorrect. It has nothing to do with resilience\nB is incorrect. it is related to intrusion detection\nC is incorrect. Usage of MIG will ensure that that load will be swithced to proper zone.\nD is correct as it is testing component which resilience is not covered by MIG but by\nCloudSQL funtionality (replica) and this needs to be tested"},{"content":"C, for sure.\nWhen talking about Resilience.","upvote_count":"5","timestamp":"1592057040.0","poster":"gfhbox0083","comment_id":"109509"},{"timestamp":"1591850160.0","content":"The word \"resiliency\" refers to\n===============\nTo test resiliency, you should verify how the end-to-end workload performs under intermittent failure conditions.\n===============\n\nUnder this:-\nHA - VMs in different zones to prevent app going down when one zone goes down\nDR - Entire app infra moves to another region with minimum downtime\n\nTesting resiliency assumes a degree of HA or DR or both. \nC is the most proper answer in this case, as it will test resiliency (of everything including authentication) by simulating a downtime (shutting down all VMs in one zone).","upvote_count":"7","comment_id":"107408","poster":"Paul_DSouza"},{"content":"C is the correct answer\nThe question is assuming that DR setting is already existing and they need to test how the application response if the database (authentication layer fails).","poster":"Ziegler","timestamp":"1591292340.0","upvote_count":"3","comment_id":"102534"},{"timestamp":"1590945900.0","poster":"nrajesh","upvote_count":"5","content":"C is the answer. D is misleading. Read replica does not provide failover capability. If it was \n failover replica, we would have a tie between C and D https://cloud.google.com/sql/docs/mysql/replication/create-replica -\"Note: Read replicas do not provide failover capability. To provide failover capability for an instance, see Configuring an Instance for High Availability.\"","comment_id":"99460"},{"content":"Final Decision to go with Option D","timestamp":"1590665880.0","comment_id":"97484","upvote_count":"1","poster":"AD2AD4","comments":[{"comment_id":"98118","poster":"AD2AD4","timestamp":"1590741420.0","comments":[{"poster":"kaush","content":"Read Replicas do not provide failover capability Read replicas do not provide failover capability. To provide failover capability for an instance, see Configuring an Instance for High Availability.\nhttps://cloud.google.com/sql/docs/mysql/replication/create-replica","comment_id":"118175","upvote_count":"2","timestamp":"1592982240.0","comments":[{"poster":"kaush","content":"Also C doesnot not mention shutdown all VMs in MIG ,merely shutdown in a zone, hence this really test resiliency hence correct answer is C","upvote_count":"2","timestamp":"1592982480.0","comment_id":"118181"}]}],"upvote_count":"2","content":"Final Decision Updated to go with Option C"}]},{"content":"I will go with C... The option D also looks good, but it is talking about read replica for cloudsql ,not failover .. and it is not talking about promoting the read replica as primary instance.","timestamp":"1590441360.0","upvote_count":"1","poster":"gcp_aws","comment_id":"95672"},{"poster":"Sat_G","content":"D - the question says resilience testing of authentication layer. Its using REST API with Cloud SQL ... Cloud SQL could very easily be the layer that does authentication hence having a read replicate and manually trigger a fail over proves the objective. C is an overkill to turn off all your VMs to test authentication resiliency.","timestamp":"1590331440.0","upvote_count":"4","comment_id":"94967"},{"upvote_count":"2","comment_id":"92716","poster":"amralieg","content":"Should be C. A & B are not valid, because they don't want to test the auth itself, but rather its resilience, D is wrong because read replica is not a failover replica","timestamp":"1589975220.0"},{"upvote_count":"4","timestamp":"1589627220.0","poster":"GunjGupta","comment_id":"89885","content":"There are 2 key points in Question-- \nResiliency testing and authentication layer. \nThe way I read the question, the Authentication layer consists of MIG with Cloud SQL. Hence we need to propose a solution for creating chaos on this (MIG with Cloud SQL). A and B do not seems to reflect resiliency aspects. I will go with C"},{"poster":"Jack_in_Large","content":"B is right. C is wrong, as the objective is \"do resilience testing of their authentication layer\" instead of HA or DR testing.","comment_id":"89623","upvote_count":"1","timestamp":"1589570400.0"},{"timestamp":"1588630260.0","upvote_count":"1","poster":"Zarmi","comment_id":"83837","content":"Answer: C"},{"comment_id":"75107","upvote_count":"2","content":"A and B are not tests in the first place. That is Monitoring.\nD isn't bad but it only tests HA of Cloud SQL without including a regional instance groups.\n\nC is better. It deserves resilience testing.","poster":"toyochik","timestamp":"1586996700.0"},{"content":"Answer is D. Test the resiliency by manually failing over the DB and see if REST API continues to serve.","poster":"SMS","comments":[{"upvote_count":"4","content":"D should not be the answer, since \"This consists of a regional managed instance group serving a public REST API that reads from and writes to a Cloud SQL instance\" ; D is creating a read-only instance.","poster":"ronald89","comment_id":"110770","timestamp":"1592221620.0"}],"comment_id":"67126","timestamp":"1584919380.0","upvote_count":"4"},{"content":"It should be B as it is related to authentication layer.","poster":"Javed","upvote_count":"2","comment_id":"62370","timestamp":"1583923020.0"},{"upvote_count":"2","comment_id":"42952","content":"I thin C , since the question is asking to do a resilience testing.","poster":"natpilot","timestamp":"1580053560.0"},{"timestamp":"1578133260.0","content":"So... again very confusing question. They highlighted first of Authentication layer resilience testing. but.... they continue saying..... THIS CONSIST of regional MIGs and Cloud SQL. When I hear on Resilience testing..... I would look for Is service available when some components down. That leaves me with option C and D. and out of both... C would be one I would go with.","poster":"MeasService","comment_id":"35193","upvote_count":"4"},{"timestamp":"1575162480.0","poster":"kalschi","content":"I think the question states the _authentication_layer_ consists of regional MIG and Cloud SQL. so to _resilience_test_ the _authentication_layer_ , it's really to test the MIG and Cloud SQL. So prefer C.\nAlso, intrusion detection software is not TESTING the layer, it's more like a detection and prevention process.","upvote_count":"4","comment_id":"25556"},{"poster":"AWS56","comment_id":"24586","upvote_count":"1","timestamp":"1574774460.0","content":"Agree B"},{"timestamp":"1572195600.0","upvote_count":"6","poster":"Eroc","comment_id":"17802","content":"I'm with @jcmoranp and @crypt0 on this one... shutting down the VMs wouldn't test only the authentication layer... same for replication of database (D).. answer B"},{"poster":"jcmoranp","upvote_count":"2","content":"resilience testing of AUTHENTICATION LAYER, so its A o B. A only tracks for compromised users, B is better because tests with a hacking software. It's B","comment_id":"17512","timestamp":"1572078000.0"},{"upvote_count":"5","poster":"KouShikyou","timestamp":"1571835420.0","comment_id":"16963","content":"How about D?"},{"upvote_count":"4","content":"I am also not sure here, but they say: resilience testing of their _authentication layer_. How does shutting down all vms from one zone test the authentication layer?","timestamp":"1571639820.0","comment_id":"16358","poster":"crypt0"},{"poster":"MeasService","upvote_count":"4","comments":[{"comments":[{"poster":"tartar","comment_id":"152231","upvote_count":"7","content":"C is ok","timestamp":"1596760380.0"}],"timestamp":"1584920100.0","content":"Ignore above. Answer is C","upvote_count":"8","poster":"SMS","comment_id":"67128"},{"upvote_count":"2","comment_id":"210544","timestamp":"1604243220.0","poster":"kumarp6","content":"C it is"},{"poster":"joshuaquek","timestamp":"1610192040.0","comment_id":"263174","content":"Its B, because penetration testing programs test for both resiliency and authentication. \n\n\nResiliency because it causes a service to go down if it penetrates, infects and takes the service down.\n\nAuthentication because it tests if it can penetrate the resources, to begin with.","upvote_count":"1"},{"comment_id":"303762","timestamp":"1614906060.0","upvote_count":"1","poster":"nitinz","content":"C is correct. read between lines and eliminate : -\n\nYour customer wants to do resilience testing of their authentication layer. This consists of a regional managed instance group serving a public REST API that reads from and writes to a Cloud SQL instance.\nWhat should you do?\nA. Engage with a security company to run web scrapers that look your users' authentication data om malicious websites and notify you if any if found. **BULL SHIT**\nB. Deploy intrusion detection software to your virtual machines to detect and log unauthorized access. **NOT RELEVANT**\nC. Schedule a disaster simulation exercise during which you can shut off all VMs in a zone to see how your application behaves. **NOTHING ELSE TO PICK, SO BE IT**\nD. Configure a read replica for your Cloud SQL instance in a different zone than the master, and then manually trigger a failover while monitoring KPIs for our REST API. **READ REPLCA CAN NOT BE WRITTEN TO**"}],"timestamp":"1571308920.0","comment_id":"15765","content":"Is it really B? why not C ? It is taking about resilience testing. shutting down all vms from one zone would be ideal testing."}],"question_text":"Your customer wants to do resilience testing of their authentication layer. This consists of a regional managed instance group serving a public REST API that reads from and writes to a Cloud SQL instance.\nWhat should you do?"},{"id":"PvEgFnCDM4AdluDS92s7","question_images":[],"answers_community":["B (52%)","D (43%)","3%"],"topic":"1","discussion":[{"content":"D- reasons:\n1.-Cloud Audit Logs maintains audit logs for admin activity, data access and system events. BIGQUERY is automatically send to cloud audit log functionality.\n2.- In the filter you can filter relevant BigQuery Audit messages, you can express filters as part of the export\n\nhttps://cloud.google.com/logging/docs/audit\nhttps://cloud.google.com/bigquery/docs/reference/auditlogs#ids\nhttps://cloud.google.com/bigquery/docs/reference/auditlogs#auditdata_examples","comment_id":"73728","comments":[{"upvote_count":"9","content":"D is the right as you can get the monthly view of the query usage across all the users and projects for auditing purpose. C does need appropriate permission to see the detail level data. Monthly view is tough to get directly from the bq ls or bq show commands.","comment_id":"143637","timestamp":"1595705640.0","poster":"GooglecloudArchitect"},{"poster":"heretolearnazure","comment_id":"989179","upvote_count":"1","timestamp":"1692879540.0","content":"Answer is D"}],"upvote_count":"50","poster":"Googler2","timestamp":"1586705280.0"},{"timestamp":"1588631400.0","content":"Answer is D:\nhttps://cloud.google.com/bigquery/docs/reference/auditlogs#example_query_cost_breakdown_by_identity","poster":"Zarmi","upvote_count":"27","comments":[{"comment_id":"507961","upvote_count":"2","timestamp":"1640272200.0","content":"Nailed it","poster":"BobbyFlash"},{"timestamp":"1667824320.0","content":"No mention about exporting to bq","upvote_count":"1","poster":"ErenYeager","comment_id":"713026"}],"comment_id":"83842"},{"content":"Selected Answer: D\nI agree B could be the solution, but the best option is D.\n\nThis is the correct and scalable approach:\n\nCloud Audit Logs capture who ran what, including queries.\n\nYou can filter on methodName = \"jobservice.jobcompleted\" and analyze logs in Logs Explorer or export to BigQuery for querying.\n\nSupports organization-wide, project-wide, and per-user visibility.","upvote_count":"1","comment_id":"1559157","poster":"francisco94","timestamp":"1744179480.0"},{"poster":"Mikeliz","content":"Selected Answer: B\nB is the better answer. You get more logs relating to BigQuery jobs from BigQuery than you get from Cloud Audit Logs","comment_id":"1363784","upvote_count":"1","timestamp":"1740890880.0"},{"upvote_count":"1","timestamp":"1740705720.0","comment_id":"1362789","poster":"david_tay","content":"Selected Answer: B\nB is correct. Go search in Gemini \"what are the steps to execute a query on the JOBS table in Bigquery to see how many queries each user ran in the last month.\" to see how easy are the steps."},{"content":"Selected Answer: D\nD is the right answer","comment_id":"1361567","timestamp":"1740513240.0","poster":"PetarMarinkovic","upvote_count":"1"},{"comment_id":"1360079","upvote_count":"2","timestamp":"1740215700.0","poster":"david_tay","content":"Selected Answer: B\nanswer is B, fastest and efficient method. Question said that they just need to know \"how many queries each user ran in the last month\" which B can do in a short time."},{"comment_id":"1340044","upvote_count":"3","timestamp":"1736798580.0","content":"Selected Answer: B\nBigQuery's INFORMATION_SCHEMA: BigQuery provides metadata about datasets, tables, and jobs through the INFORMATION_SCHEMA. The JOBS_BY_USER view within this schema is specifically designed to give you information about jobs run by each user. You can easily query this view to get the number of queries run by each user in the last month.","poster":"1P5811"},{"upvote_count":"2","comment_id":"1333898","poster":"JonathanSJ","content":"Selected Answer: B\nI will go for B because it is more efficient and easy.","timestamp":"1735530060.0"},{"upvote_count":"1","content":"Selected Answer: D\n\"Audit logs versus INFORMATION_SCHEMA views\nAudit logs help you answer the question \"Who did what, where, and when?\" within your Google Cloud resources. Audit logs are the definitive source of information for system activity by user and access patterns and should be your primary source for audit or security questions.\"\n\nhttps://cloud.google.com/bigquery/docs/introduction-audit-workloads","timestamp":"1732564380.0","poster":"alpay","comment_id":"1317734"},{"upvote_count":"2","timestamp":"1729712940.0","comment_id":"1302188","poster":"nareshthumma","content":"Answer is B\nIn the BigQuery interface, execute a query on the JOBS table to get the required \ninformation.\nExplanation:\nJOBS Table:BigQuery automatically logs job information, including queries, in a special table called JOBS.\nBy querying this table, you can retrieve details about each job, including the user who ran it, the query text, and the timestamp.\n\nWhy the Other Options Are Less Suitable:\n\nConnect Google Data Studio to BigQuery: While this can visualize data, you still need to execute a query to pull the data first. This option is not directly querying for the information you need.\nUse ‘bq show’ and ‘bq ls’: These commands provide metadata about jobs but do not efficiently retrieve the count of queries per user, especially for a large number of jobs over a month.\nUse Cloud Audit Logging: This approach could work but would be more complex and less efficient for simply counting queries. The JOBS table is specifically designed for this purpose, making it easier to extract the necessary data."},{"comment_id":"1255851","poster":"awsgcparch","timestamp":"1722020640.0","content":"Selected Answer: B\nUsing the INFORMATION_SCHEMA.JOBS_BY_USER table within BigQuery is the most efficient and straightforward method to get the required audit information about the number of queries each user ran in the last month. Therefore, option B is the best choice.. D.While Cloud Audit Logs can provide detailed logs of activities, querying them directly for this purpose is less efficient than using the JOBS table in BigQuery. Additionally, setting up and querying audit logs involves more steps and may require exporting logs to BigQuery for complex queries.","upvote_count":"6"},{"comment_id":"1255850","upvote_count":"5","timestamp":"1722020460.0","content":"Selected Answer: B\nWhy B is the Best Answer:\n\nDirect Access to Job Metadata: BigQuery maintains metadata about jobs (including query jobs) in the INFORMATION_SCHEMA views, specifically in the INFORMATION_SCHEMA.JOBS table.\nDetailed Information: This table contains information about all jobs, including who ran them, when they were run, and the type of job. This makes it easy to filter and count queries by user.\nQuerying JOBS Table: You can write a SQL query to count the number of queries executed by each user over the specified period.","poster":"awsgcparch"},{"content":"Selected Answer: B\nQuerying the INFORMATION_SCHEMA.JOBS_BY_USER view in BigQuery is the most efficient and straightforward way to obtain the number of queries each user ran in the last month. This method leverages built-in BigQuery capabilities designed specifically for auditing and monitoring query jobs.\nCloud Audit Logs provide detailed logging information but are more complex to query for specific metrics like the number of queries run by each user. BigQuery’s INFORMATION_SCHEMA.JOBS_BY_USER is designed for this purpose and is easier to use for querying job data.","poster":"eff12c1","upvote_count":"5","timestamp":"1717565100.0","comment_id":"1224529"},{"comment_id":"1223234","content":"Selected Answer: D\nAudit logs, Option D","timestamp":"1717350720.0","poster":"JaimeMS","upvote_count":"1"},{"timestamp":"1707556800.0","poster":"AhmedSami","content":"Selected Answer: C\nreason:\nhttps://cloud.google.com/logging/docs/audit#data-access\n\nData Access audit logs—except for BigQuery Data Access audit logs—are disabled by default because audit logs can be quite large. If you want Data Access audit logs to be written for Google Cloud services other than BigQuery, you must explicitly enable them","upvote_count":"1","comment_id":"1146005"},{"poster":"SSS987","upvote_count":"2","timestamp":"1705410840.0","content":"I finally decide to go with Option D over B because we or the auditor might not have access to the metadata. In fact, in our project, not all of us had access to query this view.\n\n\"To get the permission that you need to query the INFORMATION_SCHEMA.JOBS view, ask your administrator to grant you the BigQuery Resource Viewer\"\nhttps://cloud.google.com/bigquery/docs/information-schema-jobs#required_role.\n\n(And not because of the wordings \"Table\" instead of \"view\" - don't think an architect exam will try to assess your memory of whether it is a table or a view or your understanding of the difference between a table and a view).","comment_id":"1124226"}],"timestamp":"2019-11-17 03:01:00","choices":{"C":"Use 'bq show' to list all jobs. Per job, use 'bq ls' to list job information and get the required information.","D":"Use Cloud Audit Logging to view Cloud Audit Logs, and create a filter on the query operation to get the required information.","A":"Connect Google Data Studio to BigQuery. Create a dimension for the users and a metric for the amount of queries per user.","B":"In the BigQuery interface, execute a query on the JOBS table to get the required information."},"unix_timestamp":1573956060,"answer_ET":"B","answer_description":"","answer_images":[],"answer":"B","exam_id":4,"isMC":true,"question_id":183,"question_text":"Your BigQuery project has several users. For audit purposes, you need to see how many queries each user ran in the last month. What should you do?","url":"https://www.examtopics.com/discussions/google/view/8378-exam-professional-cloud-architect-topic-1-question-83/"},{"id":"BeYXt8KUjnodKVse0ueL","exam_id":4,"question_images":[],"answer_images":[],"question_text":"You want to automate the creation of a managed instance group. The VMs have many OS package dependencies. You want to minimize the startup time for new\nVMs in the instance group.\nWhat should you do?","answer_ET":"B","question_id":184,"topic":"1","unix_timestamp":1571640660,"answers_community":["B (100%)"],"isMC":true,"timestamp":"2019-10-21 08:51:00","url":"https://www.examtopics.com/discussions/google/view/6873-exam-professional-cloud-architect-topic-1-question-84/","answer_description":"","answer":"B","discussion":[{"timestamp":"1618987860.0","poster":"crypt0","comments":[{"poster":"kumarp6","comment_id":"210550","content":"B is the answer,","upvote_count":"6","timestamp":"1651410960.0"},{"poster":"Jos","content":"It is.","comment_id":"41039","upvote_count":"10","timestamp":"1626791340.0","comments":[{"poster":"tartar","content":"B is ok","timestamp":"1644205680.0","upvote_count":"11","comment_id":"152254"}]},{"timestamp":"1662332640.0","upvote_count":"4","comment_id":"303765","poster":"nitinz","content":"It is B"}],"comment_id":"16362","content":"Why is it not answer B?","upvote_count":"42"},{"upvote_count":"21","content":"B- minimal start time means a pre-baked golden image","timestamp":"1621001400.0","poster":"JoeShmoe","comment_id":"21572"},{"poster":"omermahgoub","comments":[{"poster":"omermahgoub","timestamp":"1719038100.0","upvote_count":"5","comment_id":"753093","content":"Option A, using Terraform to create the managed instance group and a startup script to install the OS package dependencies, would not minimize the startup time for new VMs in the instance group. \n\nOption C, using Puppet to create the managed instance group and install the OS package dependencies, would not minimize the startup time for new VMs in the instance group. \n\nOption D, using Deployment Manager to create the managed instance group and Ansible to install the OS package dependencies, would not minimize the startup time for new VMs in the instance group."}],"comment_id":"753092","content":"The correct answer is B. Create a custom VM image with all OS package dependencies. Use Deployment Manager to create the managed instance group with the VM image.\n\nManaged instance groups are a way to manage a group of Compute Engine instances as a single entity. If you want to automate the creation of a managed instance group, you can use tools such as Terraform, Deployment Manager, or Puppet to automate the process.\n\nTo minimize the startup time for new VMs in the instance group, you should create a custom VM image with all of the OS package dependencies pre-installed. This will allow you to create new VMs from the custom image, which will significantly reduce the startup time compared to installing the dependencies on each VM individually. You can then use Deployment Manager to create the managed instance group with the custom VM image.","upvote_count":"12","timestamp":"1719038040.0"},{"poster":"megumin","comment_id":"715001","content":"Selected Answer: B\nB is ok","timestamp":"1715318880.0","upvote_count":"1"},{"poster":"AzureDP900","upvote_count":"1","content":"B. Create a custom VM image with all OS package dependencies. Use Deployment Manager to create the managed instance group with the VM image.","comment_id":"696358","timestamp":"1713283140.0"},{"upvote_count":"1","timestamp":"1713101880.0","poster":"minmin2020","comment_id":"694772","content":"Selected Answer: B\nB will reduce the startup time"},{"content":"Selected Answer: B\nB- minimal start time means a pre-baked golden image","timestamp":"1707051540.0","upvote_count":"4","comment_id":"642339","poster":"DrishaS4"},{"content":"As someone who works on Terraform. It may not be Googles best practice, even though it's built in just need to be initialized. But it is the easiest way to build and restructure infrastructure with a simple line of code change and a quick shell command to apply terraform. I mean B would work. But it doesn't include the start-up script for the OS dependencies to be loaded. ?>?>? Any feedback?","comment_id":"633032","comments":[{"upvote_count":"2","timestamp":"1705770660.0","comment_id":"634110","content":"Start up scripts aren't need here as you're making a custom OS image with all OS package dependencies. Question is not asking for the easiest way, it's asking how to minimize VM startup times. Not having to run the startup scripts because it's baked into the image is how I understand and interpret this, therefore B.","poster":"Ric350"}],"poster":"BigSteveO","timestamp":"1705590240.0","upvote_count":"1"},{"comment_id":"627583","upvote_count":"5","content":"06/30/2022 Exam","timestamp":"1704489240.0","poster":"mv2000"},{"upvote_count":"1","comment_id":"526947","timestamp":"1689700020.0","content":"Selected Answer: B\nB is correct","poster":"rogerlovato"},{"upvote_count":"1","timestamp":"1686121440.0","poster":"haroldbenites","comment_id":"495800","content":"Go for B"},{"timestamp":"1685778960.0","poster":"Godlike","upvote_count":"2","comment_id":"493055","content":"yes B is right"},{"poster":"vincy2202","upvote_count":"2","content":"B is the right answer","timestamp":"1685259420.0","comment_id":"489021"},{"poster":"exam_war","upvote_count":"1","content":"go with B. D: it involves so many other third software to configure/manage which makes build more complicated.","comment_id":"473124","timestamp":"1683294660.0"},{"content":"B – create a custom VM instance image with all OS dependencies. Use Deployment Manager to create a MIG with the VM image.\nRead more about Public and Custom VM Images: https://cloud.google.com/compute/docs/images\nCustom images are available in your project only, they don’t add cost to your VM instances, incur image storage cost (0.085$ GB/month)\nD – could be also an alternative (if to consider requirement to install dependencies in start up script). But, last sentence stresses on “minimize VM’s start up time”. So, B is fastest solution. Also, what is a point to use Ansible if you can complete same task via startup script of Deployment Manager. Ansible won’t make this faster, but just will add 3rd party dependency.","timestamp":"1682781600.0","comment_id":"469829","upvote_count":"3","poster":"MaxNRG"},{"content":"B. Create a custom VM image with all OS package dependencies. Use Deployment Manager to create the managed instance group with the VM image.","timestamp":"1668866160.0","poster":"victory108","upvote_count":"2","comment_id":"361336"},{"poster":"un","upvote_count":"1","timestamp":"1668592200.0","comment_id":"358403","content":"B is correct"},{"timestamp":"1666179780.0","comment_id":"338817","poster":"pentium2000","content":"B is best answer","upvote_count":"1"},{"poster":"Ausias18","upvote_count":"1","timestamp":"1664604840.0","comment_id":"325563","content":"Answer is B"},{"content":"The answer is B","upvote_count":"1","comment_id":"295034","poster":"BobBui","timestamp":"1660993980.0"},{"poster":"Rothmansua","timestamp":"1659039540.0","content":"We want to minimize VM startup time. And we don't care whether it is serving anything meaningful.\nSo the quickest way to start is to start empty image. Let whatever tools spend time copying anything inside after VMs are already running.\nIs that the reasoning for D?","upvote_count":"2","comment_id":"278861"},{"poster":"joshuaquek","timestamp":"1657359780.0","comment_id":"263178","upvote_count":"2","content":"B is the answer, pre-install all dependencies to reduce deployment timing."},{"content":"Should be B","upvote_count":"1","comment_id":"252168","poster":"Prakzz","timestamp":"1656167100.0"},{"poster":"RajeevVij","upvote_count":"1","timestamp":"1654275360.0","comment_id":"234258","content":"B is the answer"},{"content":"B is ok","comment_id":"232788","timestamp":"1654153920.0","poster":"Chulbul_Pandey","upvote_count":"1"},{"content":"VM Image will contain all the dependencies and you can create an Instance template from this VM image to create MIG. I will go with option B.","poster":"AdityaGupta","upvote_count":"2","comment_id":"206537","timestamp":"1650994200.0"},{"upvote_count":"1","timestamp":"1649157060.0","comment_id":"193525","poster":"homer_simpson","content":"For me the answer is B"},{"timestamp":"1647457500.0","content":"B is more meaningful","upvote_count":"1","poster":"AshokC","comment_id":"180471"},{"upvote_count":"1","content":"B is correct","timestamp":"1646012160.0","poster":"Kabiliravi","comment_id":"168836"},{"poster":"wiqi","content":"B is correct","upvote_count":"1","timestamp":"1645723080.0","comment_id":"165309"},{"poster":"wiqi","upvote_count":"1","content":"B is correct.","comment_id":"164446","timestamp":"1645632540.0"},{"content":"I will go with B.","comment_id":"155913","timestamp":"1644626760.0","upvote_count":"1","poster":"haidertanveer0808"},{"poster":"mlantonis","timestamp":"1640271540.0","comment_id":"117483","content":"B is a better option","upvote_count":"1"},{"comment_id":"109358","upvote_count":"1","timestamp":"1639399260.0","poster":"syu31svc","content":"B for sure; having the pre-installed software will definitely reduce time for availability"},{"timestamp":"1639381860.0","poster":"gfhbox0083","upvote_count":"1","comment_id":"109189","content":"B, for sure.\nUsing custom VM image"},{"content":"B is the correct answer for me as this is a complete automation after the initial template is created","poster":"Ziegler","timestamp":"1638648540.0","comment_id":"102550","upvote_count":"2"},{"poster":"misho","content":"For the answer is B.\n\n\"Custom images are more deterministic and start more quickly than instances with startup scripts. However, startup scripts are more flexible and let you update the apps and settings in your instances more easily.\"\nhttps://cloud.google.com/compute/docs/instance-templates/create-instance-templates#using_custom_or_public_images_in_your_instance_templates","upvote_count":"6","timestamp":"1638266700.0","comment_id":"98712"},{"comment_id":"99156","upvote_count":"1","poster":"arunsvoice","content":"Answer is B,\nImage has all the packages, so minimal start up time.","timestamp":"1638263880.0"},{"upvote_count":"1","content":"Final Decision to go with Option D","comment_id":"97500","poster":"AD2AD4","comments":[{"comment_id":"104179","poster":"asure","content":"-> B is right","timestamp":"1638836280.0","upvote_count":"1"},{"poster":"Gobblegobble","comment_id":"129825","timestamp":"1641659220.0","upvote_count":"2","content":"Final Decision to go with option B"}],"timestamp":"1638107520.0"},{"poster":"jayaen","timestamp":"1638103860.0","comment_id":"97446","content":"Answer B is correct , since it satisfies VM startup time requirement","upvote_count":"1"},{"poster":"Jack_in_Large","comment_id":"89577","timestamp":"1636999980.0","upvote_count":"1","content":"You have to choose from B or sB"},{"upvote_count":"6","content":"B - the only way to minimize the startup time for VMs is to pre-bake the image with the dependencies.","poster":"mawsman","timestamp":"1629330000.0","comment_id":"52414"},{"timestamp":"1629011820.0","content":"A MIG is created from an instance template, not a VM? image. (I know about DISK images)\nI would discard B.","poster":"VASI","comment_id":"50801","upvote_count":"5","comments":[{"comment_id":"52417","content":"An instance template still needs to be created from an image doesn't it?","poster":"mawsman","upvote_count":"5","timestamp":"1629330060.0"}]},{"content":"It's B, Custom image will minimize startup time.","timestamp":"1628875140.0","poster":"ADVIT","upvote_count":"2","comment_id":"50187"},{"upvote_count":"2","poster":"natpilot","comment_id":"42955","comments":[{"upvote_count":"2","poster":"alilog","content":"Correct answer is B.\nIt's a managed instance group, so VMs are the same and have the same OS and package requirements. The fastest is to create a custom image. Answer is B without hesitation.","timestamp":"1636973220.0","comment_id":"89386"}],"content":"D is better; you could use same instance template for base installation and use ansible to do different configuration in base of instance tipology.","timestamp":"1627307580.0"},{"upvote_count":"3","content":"B is ans","comment_id":"40441","timestamp":"1626628920.0","poster":"sri007"},{"timestamp":"1625388300.0","upvote_count":"3","poster":"MeasService","comment_id":"35197","comments":[{"comments":[{"upvote_count":"2","poster":"Alekshar","timestamp":"1661512380.0","content":"Why would Google suggest to use another product when GCP already provides all you need ? \nCaring about how to move outside GCP is not really a question for *Google* Cloud Platform architects","comment_id":"299750"}],"poster":"[Removed]","upvote_count":"1","timestamp":"1643995200.0","content":"You can create custom images using Packer. I'm surprised why that was omitted","comment_id":"150517"},{"poster":"kimberjdaw","comment_id":"190565","content":"The point is to pre-install everything in the image. B.","timestamp":"1648671060.0","upvote_count":"1"}],"content":"I find it one more typical Google type question. The question says\" You want to AUTOMATE creation of MIG and install OS Package dependancies\". Yes ofcourse with minimal startup time. Answer B does not go well with automate as you are creating fix image manually. Answer D goes perfect wtih Automate but with little compromise on startup time. D is the right answer for me."},{"comment_id":"17805","timestamp":"1619537400.0","content":"Yeah installing after instance start-up would increase start-up time... the question said they want to minimize start-up","upvote_count":"1","poster":"Eroc"},{"timestamp":"1619179200.0","comment_id":"16945","content":"My answer is B as well.","upvote_count":"9","poster":"KouShikyou"},{"poster":"jcmoranp","timestamp":"1619015760.0","comment_id":"16449","upvote_count":"12","content":"think B is the answer"}],"choices":{"A":"Use Terraform to create the managed instance group and a startup script to install the OS package dependencies.","D":"Use Deployment Manager to create the managed instance group and Ansible to install the OS package dependencies.","B":"Create a custom VM image with all OS package dependencies. Use Deployment Manager to create the managed instance group with the VM image.","C":"Use Puppet to create the managed instance group and install the OS package dependencies."}},{"id":"ivMk5IpACOQqgKDwdPJb","answer_description":"","discussion":[{"timestamp":"1575166860.0","upvote_count":"61","comments":[{"timestamp":"1681824060.0","comments":[{"comments":[{"poster":"RKS_2021","content":"A is wrong","timestamp":"1696005660.0","upvote_count":"1","comment_id":"1020945"}],"poster":"jits1984","timestamp":"1693512120.0","comment_id":"995461","upvote_count":"3","content":"incorrect, should be A, BigQuery Job User \n(roles/bigquery.jobUser)\n\nProvides permissions to run jobs, including queries, within the project."}],"poster":"jits1984","comment_id":"873657","content":"Should be C.\n\nhttps://cloud.google.com/bigquery/docs/access-control#bigquery.dataViewer\n\nData viewer role can be applied to a Table and a View. \n\nJobUser can be applied only at a Project level not at a Dataset level\n\nhttps://cloud.google.com/bigquery/docs/access-control#bigquery.jobUser","upvote_count":"11"}],"comment_id":"25570","content":"It should be A. The question requires that user from each country can only view a specific data set, so BQ dataViewer cannot be assigned at project level. Only A could limit the user to query and view the data that they are supposed to be allowed to.","poster":"Sebatian"},{"comments":[{"comments":[{"timestamp":"1634386020.0","comment_id":"463045","upvote_count":"9","content":"https://cloud.google.com/bigquery/docs/access-control#bigquery.dataViewer\n\"When applied to a dataset..\" you can apply dataViewer role to a specific dataset.","poster":"BrunoTostes"}],"poster":"Jack_in_Large","timestamp":"1589559720.0","comment_id":"89580","upvote_count":"31","content":"Option C grant read permission to all datasets globally, which violated the request \"You want analysts from each country\nto be able to see and query only the data for their respective countries\"\n\nSo the correct answer is A."}],"poster":"wk","content":"Should be C\nhttps://cloud.google.com/bigquery/docs/access-control#bigquery.dataViewer\n\nWhen applied to a dataset, dataViewer provides permissions to:\n\nRead the dataset's metadata and to list tables in the dataset.\nRead data and metadata from the dataset's tables.\nWhen applied at the project or organization level, this role can also enumerate all datasets in the project. Additional roles, however, are necessary to allow the running of jobs.","timestamp":"1571537940.0","comment_id":"16147","upvote_count":"32"},{"timestamp":"1740909060.0","upvote_count":"1","comment_id":"1363886","content":"Selected Answer: C\nAnswer should be C. As the lowest possible level to grant jobUser role is project, all the analyst will get access to all the datasets in the project. Whereas the lowest possible level to dataViewer is Table/View, hence the required restriction can be setup by this approach only.","poster":"cloud_rider"},{"timestamp":"1739821740.0","content":"Selected Answer: A\nAll provided options seem to be bad practice. If you grant a group of job user on the project level (The lowest level allowed), you essentially grant them the ability to query all counties datasets. However, you can allow data viewer at the resource level which is what makes the most sense. But specifically for this question, Job user on the project level is less bad than Data viewer on the project level since both let you run queries but data viewer also allows you to view the dataset. Hence , A","comment_id":"1357957","upvote_count":"2","poster":"d83229d"},{"content":"Selected Answer: A\nin answr 'A' theis sentence \"Share the appropriate dataset with view access with each respective analyst country-group\" is very cryptic. I assume this means assign \"data viewer\" role for the respective analyst groups for the country specific datasets","poster":"venkee","upvote_count":"1","comment_id":"1348910","timestamp":"1738212840.0"},{"timestamp":"1732504920.0","poster":"Cloud_Architect_05","comment_id":"1317303","content":"Should be A. DataViewer: \"When applied at the project or organization level, this role can also enumerate all datasets in the project. Additional roles, however, are necessary to allow the running of jobs.\"\n\nhttps://cloud.google.com/bigquery/docs/access-control","upvote_count":"1"},{"poster":"nareshthumma","timestamp":"1729713300.0","upvote_count":"2","content":"C.\n\nExplanation:\nEach country should have its own group to manage access efficiently. This allows you to easily add or remove analysts from their respective groups.\nBy adding analysts to their specific country groups, you can manage permissions in a way that aligns with their data access needs.\nThis group will include all country groups. It simplifies the management of roles for all analysts collectively.\nThe dataViewer role provides permission to view datasets and tables. This role allows analysts to read data without the ability to modify it, which is appropriate for your use case.\nGranting view access to the respective datasets for each country group ensures that analysts can only access data relevant to their country. This is crucial for maintaining data privacy and compliance.\n\nWhy Other Options Are Less Suitable:\nUsing BigQuery jobUser Role: The BigQuery jobUser role allows users to run jobs (like queries) but does not inherently grant access to view datasets or tables. This option would not effectively limit visibility to data by country.","comment_id":"1302190"},{"comment_id":"1176496","content":"Selected Answer: C\nIt is C.\nQuestion says analyst should be able to see and query only the data for their respective countries. BigQueryDta viewer permission will allow only to read and query the table/view data","upvote_count":"3","timestamp":"1710766800.0","poster":"Diwz","comments":[{"upvote_count":"2","content":"You cant query with dataviewer. user with the roles/bigquery.dataViewer role has read-only access to datasets and tables but does not inherently have the permissions to run queries (which are considered jobs in BigQuery). The dataViewer role allows users to view dataset metadata and table contents but does not include the ability to create or execute jobs.The dataViewer role alone does not allow users to run queries. Analysts need the ability to run queries, which requires the jobUser role.","timestamp":"1722021420.0","comment_id":"1255861","poster":"awsgcparch"}]},{"timestamp":"1707027600.0","poster":"OrangeTiger","comment_id":"1139842","content":"Selected Answer: A\nGo with a.","upvote_count":"1"},{"content":"Selected Answer: C\nC is right, even if DataViwer is granted on Project level but Dataset is shared with view access to only the country group.","timestamp":"1705782780.0","poster":"islamfouda","comment_id":"1127493","upvote_count":"2"},{"poster":"JohnDohertyDoe","timestamp":"1704990840.0","comment_id":"1119972","upvote_count":"7","content":"Selected Answer: A\nA is the correct answer. Tested the two scenarios, with `jobUser` permissions it does not allow the user to see a dataset. Whereas with `dataViewer` it has permissions for all the datasets. Note the difference is in the initial permission across the project and not per dataset."},{"upvote_count":"1","poster":"bandegg","comment_id":"1113107","timestamp":"1704314220.0","content":"Selected Answer: A\nIt's A because in order to query, on needs the jobUser role. dataViewer doesn't grant the ability to actually query the datasets one has been given access to.\n\nhttps://cloud.google.com/bigquery/docs/running-queries#required_permissions"},{"content":"I'm siding with C on this one.\njobUser role has the bigquery.jobs.create permission, which allow it to load data into BQ, which analyst shouldn't do.\nData Viewer has no permissions to add or edit data (It can create a snapshot of the data, extract it or replicate it at most)","poster":"e5019c6","upvote_count":"1","comment_id":"1108978","timestamp":"1703873160.0"},{"poster":"whoosh007","comment_id":"1106559","timestamp":"1703650080.0","content":"Selected Answer: C\nBigQuery Data Viewer \n(roles/bigquery.dataViewer)\nWhen applied to a table or view, this role provides permissions to:\nRead data and metadata from the table or view.\nThis role cannot be applied to individual models or routines.\nWhen applied to a dataset, this role provides permissions to:\nRead the dataset's metadata and list tables in the dataset.\nRead data and metadata from the dataset's tables.\nWhen applied at the project or organization level, this role can also enumerate all datasets in the project. Additional roles, however, are necessary to allow the running of jobs.\nLowest-level resources where you can grant this role:\nTable and view\n\nBigQuery Job User \n(roles/bigquery.jobUser)\nProvides permissions to run jobs, including queries, within the project.\nLowest-level resources where you can grant this role:\nProject\n\nAnalyst must query data --> BigQuery Data Viewer","upvote_count":"3"},{"poster":"steghe","comment_id":"1043156","content":"Selected Answer: A\nA: JobUser to execute queries in general. Data viewer for viewing the country dataset.","timestamp":"1697264220.0","upvote_count":"1"},{"comment_id":"1027004","poster":"TopTalk","upvote_count":"2","content":"Selected Answer: C\nLowest-level resources where you can grant this role: \ndataViewer: Table, View\njobUser: Project\n\nYou don't want to grant access to the entire project, only the dataset which is divided per country. Definitely C. \nhttps://cloud.google.com/bigquery/docs/access-control#bigquery.dataViewer","timestamp":"1696639500.0","comments":[{"poster":"don_v","upvote_count":"1","content":"Correct answer is A.\n\nNote this: \"Share the appropriate dataset with *view access* with each respective analyst country-group\".\n\n\"view access\" is the key.","comment_id":"1123728","timestamp":"1705356240.0"}]},{"poster":"AdityaGupta","content":"Selected Answer: A\nA. Create a group per country. Add analysts to their respective country-groups. Create a single group 'all_analysts', and add all country-groups as members. Grant the 'all_analysts' group the IAM role of BigQuery jobUser. Share the appropriate dataset with view access with each respective analyst country-group.\n\nAs all analysts need to execute query, they need JobUser role.\nThey should be restricted to view all datasets (not tables) of respective country.","upvote_count":"1","comment_id":"1026271","timestamp":"1696570800.0"}],"isMC":true,"choices":{"A":"Create a group per country. Add analysts to their respective country-groups. Create a single group 'all_analysts', and add all country-groups as members. Grant the 'all_analysts' group the IAM role of BigQuery jobUser. Share the appropriate dataset with view access with each respective analyst country-group.","D":"Create a group per country. Add analysts to their respective country-groups. Create a single group 'all_analysts', and add all country-groups as members. Grant the 'all_analysts' group the IAM role of BigQuery dataViewer. Share the appropriate table with view access with each respective analyst country-group.","C":"Create a group per country. Add analysts to their respective country-groups. Create a single group 'all_analysts', and add all country-groups as members. Grant the 'all_analysts' group the IAM role of BigQuery dataViewer. Share the appropriate dataset with view access with each respective analyst country- group.","B":"Create a group per country. Add analysts to their respective country-groups. Create a single group 'all_analysts', and add all country-groups as members. Grant the 'all_analysts' group the IAM role of BigQuery jobUser. Share the appropriate tables with view access with each respective analyst country-group."},"question_id":185,"exam_id":4,"answers_community":["A (65%)","C (35%)"],"answer_images":[],"timestamp":"2019-10-11 13:12:00","answer":"A","question_text":"Your company captures all web traffic data in Google Analytics 360 and stores it in BigQuery. Each country has its own dataset. Each dataset has multiple tables.\nYou want analysts from each country to be able to see and query only the data for their respective countries.\nHow should you configure the access rights?","url":"https://www.examtopics.com/discussions/google/view/6457-exam-professional-cloud-architect-topic-1-question-85/","answer_ET":"A","unix_timestamp":1570792320,"question_images":[],"topic":"1"}],"exam":{"provider":"Google","isBeta":false,"isMCOnly":false,"name":"Professional Cloud Architect","lastUpdated":"11 Apr 2025","numberOfQuestions":279,"isImplemented":true,"id":4},"currentPage":37},"__N_SSP":true}