{"pageProps":{"questions":[{"id":"qqneAi2lDLitb0wIzw5u","question_text":"You have deployed an application on Anthos clusters (formerly Anthos GKE). According to the SRE practices at your company, you need to be alerted if request latency is above a certain threshold for a specified amount of time. What should you do?","url":"https://www.examtopics.com/discussions/google/view/60624-exam-professional-cloud-architect-topic-1-question-153/","question_images":[],"answer_ET":"A","question_id":61,"timestamp":"2021-08-25 14:47:00","topic":"1","choices":{"C":"Use Cloud Profiler to follow up the request latency. Create a custom metric in Cloud Monitoring based on the results of Cloud Profiler, and create an Alerting policy in case this metric exceeds the threshold.","D":"Configure Anthos Config Management on your cluster, and create a yaml file that defines the SLO and alerting policy you want to deploy in your cluster.","A":"Install Anthos Service Mesh on your cluster. Use the Google Cloud Console to define a Service Level Objective (SLO), and create an alerting policy based on this SLO.","B":"Enable the Cloud Trace API on your project, and use Cloud Monitoring Alerts to send an alert based on the Cloud Trace metrics."},"exam_id":4,"answers_community":["A (74%)","B (26%)"],"answer":"A","discussion":[{"poster":"vladik820","content":"A is ok. \nhttps://cloud.google.com/service-mesh/docs/observability/slo-overview","comment_id":"431465","timestamp":"1661431620.0","upvote_count":"25"},{"comment_id":"1336357","timestamp":"1735991940.0","upvote_count":"1","poster":"plumbig11","content":"Selected Answer: B\nCloud trace isn't appropriated to SLO. In this case Anthos Service mesh is the better option."},{"poster":"Ouss_123","timestamp":"1735419540.0","upvote_count":"1","content":"Selected Answer: A\nCloud Service Mesh displays a Latency graph on the Metrics page for each of your services. The Latency graph shows you the latency over time, which can help you determine a latency threshold or upper bound for a service.\n\nhttps://cloud.google.com/service-mesh/docs/observability/slo-overview","comment_id":"1333158"},{"upvote_count":"4","comment_id":"1055001","poster":"cchiaramelli","timestamp":"1729995840.0","content":"Selected Answer: B\n\"Google Cloud Console to define a Service Level Objective (SLO)\" seems odd, B doesn't seem wrong"},{"poster":"tamj123","comment_id":"1047381","content":"Answer A looks correct","upvote_count":"1","timestamp":"1729297380.0"},{"upvote_count":"4","timestamp":"1704632640.0","poster":"examch","comment_id":"768567","content":"Selected Answer: A\nCloud Monitoring can trigger an alert when a Service is on track to violate an SLO. You can create an alerting policy based on the rate of consumption of your error budget. All alerts on error budgets have the same basic condition: a specified percentage of the error budget for the compliance period is consumed in a lookback period, which is a time period, such as the previous 60 minutes. When you create the alerting policy, Anthos Service Mesh automatically sets most of the conditions for the alert based on the settings in the SLO. You specify the lookback period and the consumption percentage.\n\nhttps://cloud.google.com/service-mesh/docs/observability/alert-policy-slo"},{"content":"Selected Answer: A\nA is ok","upvote_count":"2","timestamp":"1700143620.0","poster":"megumin","comment_id":"719717"},{"timestamp":"1694168640.0","upvote_count":"2","poster":"Jay_Krish","comment_id":"663484","content":"Selected Answer: A\nA seems correct"},{"timestamp":"1692512280.0","comment_id":"649305","poster":"RitwickKumar","upvote_count":"2","content":"Selected Answer: A\nhttps://cloud.google.com/service-mesh/docs/observability/alert-policy-slo"},{"content":"Use the Google Cloud Console to define a Service Level Objective (SLO)\nWAAAAT ?\nHow Console help you to define SLO?","timestamp":"1691322360.0","poster":"igor_nov1","upvote_count":"1","comment_id":"643361"},{"timestamp":"1691238240.0","comment_id":"642964","content":"Specific Purpose of Cloud Trace API is to get info regarding Latency.\nWould go with B.","upvote_count":"2","poster":"AMohanty"},{"content":"A is right","poster":"AzureDP900","timestamp":"1688402760.0","comment_id":"626661","upvote_count":"1"},{"comments":[{"poster":"ryzior","content":"I think A is about monitoring and alerting without any further investigation, while Trace is for finding the root cause/detective purposes, when you look into a call and track this call step by step through each endpoint, the call is going through.","comment_id":"613139","upvote_count":"3","timestamp":"1686210540.0"},{"timestamp":"1686130320.0","comment_id":"612660","comments":[{"content":"yep, you can","timestamp":"1710571440.0","upvote_count":"2","comment_id":"840597","poster":"Shawnn"}],"upvote_count":"2","poster":"kimharsh","content":"Can you create an Alert when you use Cloud Trace?"}],"timestamp":"1680445080.0","content":"Why not B....\nCloud Trace is a distributed tracing system that collects latency data from the applications and displays it in near real-time. It allows you to follow a sample request through your distributed system, observe the network calls and profile your system end to end.\nNote that Cloud Trace is disabled by default.\nThe Anthos Service Mesh pages provide a link to the traces in the Cloud Trace page in the Cloud Console.\nhttps://cloud.google.com/service-mesh/docs/observability/accessing-traces \nIn Anthos clusters you need to install Anthos service mesh? From this link you need to install it only on GKE and on-premises platforms\nhttps://cloud.google.com/service-mesh/docs/observability/accessing-traces","upvote_count":"3","poster":"sivre","comment_id":"579882"},{"timestamp":"1676149260.0","poster":"[Removed]","upvote_count":"2","content":"I got same question on my exam.","comment_id":"545524"},{"comment_id":"544351","timestamp":"1676011440.0","poster":"haroldbenites","upvote_count":"1","content":"Go for A"},{"comment_id":"527720","upvote_count":"4","poster":"technodev","timestamp":"1674145500.0","content":"Got this question in my exam, answered A"},{"upvote_count":"2","poster":"vincy2202","timestamp":"1671071460.0","comment_id":"501805","content":"Selected Answer: A\nA is the correct answer"},{"content":"Selected Answer: A\nVote A","timestamp":"1669796940.0","comment_id":"490523","poster":"nqthien041292","upvote_count":"1"},{"poster":"zt00","upvote_count":"1","comment_id":"454892","timestamp":"1664538780.0","content":"A, https://cloud.google.com/service-mesh/docs/observability/alert-policy-slo"},{"poster":"diaga2","upvote_count":"1","comment_id":"441525","content":"Yes, A is fine!","timestamp":"1662651660.0"},{"timestamp":"1661696760.0","content":"A. Install Anthos Service Mesh on your cluster. Use the Google Cloud Console to define a Service Level Objective (SLO), and create an alerting policy based on this SLO.","poster":"victory108","upvote_count":"2","comment_id":"433956"}],"isMC":true,"unix_timestamp":1629895620,"answer_description":"","answer_images":[]},{"id":"8XfFMg8pZCGqBzObROso","isMC":true,"timestamp":"2021-08-25 14:58:00","unix_timestamp":1629896280,"url":"https://www.examtopics.com/discussions/google/view/60627-exam-professional-cloud-architect-topic-1-question-154/","answers_community":["C (68%)","B (16%)","A (16%)"],"question_id":62,"topic":"1","discussion":[{"comment_id":"431475","upvote_count":"36","comments":[{"poster":"rishab86","upvote_count":"3","comment_id":"460064","content":"After going through the link I feel its C","timestamp":"1649595240.0","comments":[{"timestamp":"1650816600.0","upvote_count":"6","content":"Mee too. \nCDN does not make sense","comments":[{"content":"Indeed. We don't know if the API is authenticated, reveals private data, static or not.","poster":"bandegg","timestamp":"1719752580.0","comment_id":"1110667","upvote_count":"1"}],"poster":"mikesp","comment_id":"467034"}]}],"content":"C is ok .\nhttps://cloud.google.com/blog/products/gcp/how-to-deploy-geographically-distributed-services-on-kubernetes-engine-with-kubemci","poster":"vladik820","timestamp":"1645801080.0"},{"content":"I'm not sure about C. kubemci is deprecated and is not part anymore of cloud sdk in favor of ingress for anthos. I'll go with A","timestamp":"1646903820.0","comments":[{"poster":"MikeB19","comment_id":"443842","comments":[{"poster":"Linus11","upvote_count":"2","comment_id":"459285","content":"It is hee -- https://github.com/GoogleCloudPlatform/k8s-multicluster-ingress","timestamp":"1649430900.0"}],"content":"I think either a or c is correct. I chose c base on the article ref in the chat. Do u have supporting article ref kubemci is deprecated? I also found some chatter about kubemci being deprecated but couldnâ€™t find anything offical","timestamp":"1647168120.0","upvote_count":"1"},{"comments":[{"comments":[{"content":"Nope, service is L4 Network/Internal load balancer","upvote_count":"1","timestamp":"1732521720.0","poster":"huuthanhdlv","comment_id":"1218037"}],"poster":"cotam","comment_id":"467375","timestamp":"1650878520.0","content":"That's actually not true. Service of type: LoadBalancer, is a service from \"K8s\" point of view, which creates L7 HTTP(S) Load Balancer.","upvote_count":"5"},{"comment_id":"1199031","poster":"dija123","content":"Agree with you","upvote_count":"1","timestamp":"1729415040.0"}],"timestamp":"1648067940.0","content":"Problem with A is that a service load bancer is not l7 https. The question is outdated, the answer will have been C. Now it would be Anthos multi cluster ingress -https://cloud.google.com/kubernetes-engine/docs/concepts/multi-cluster-ingress","comment_id":"450468","poster":"Rzla","upvote_count":"14"}],"upvote_count":"11","poster":"Lk9876","comment_id":"442353"},{"timestamp":"1735729020.0","upvote_count":"1","content":"Selected Answer: C\nC is correct .. https://cloud.google.com/kubernetes-engine/docs/concepts/multi-cluster-ingress","comment_id":"1335167","poster":"gcloud007"},{"upvote_count":"2","comment_id":"1209622","content":"Selected Answer: B\nTo reduce latency for users in Asia while maintaining high availability and scalability, the most appropriate option would be:\n\nB. Use a global HTTP(s) load balancer with Cloud CDN enabled.","poster":"svkds","timestamp":"1731313800.0"},{"content":"Selected Answer: B\nB is answer","upvote_count":"2","poster":"Diwz","comment_id":"1187727","timestamp":"1727826900.0"},{"content":"Selected Answer: C\nquestion is old but it should be c. however currently should be multi cluster ingress\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/multi-cluster-ingress","comment_id":"1134150","timestamp":"1722169500.0","poster":"Pime13","upvote_count":"4"},{"content":"really powefull \nkubernetes-engine-with-kubemci","poster":"AzFarid","comment_id":"1104673","timestamp":"1719233640.0","upvote_count":"1"},{"content":"Well it should be C, but it is deprecated in favor of ingress for Anthos as can be read here https://github.com/GoogleCloudPlatform/k8s-multicluster-ingress","poster":"theBestStudent","upvote_count":"2","comment_id":"1075059","timestamp":"1716161460.0"},{"upvote_count":"1","content":"go for C","comment_id":"1047385","timestamp":"1713486240.0","poster":"tamj123"},{"upvote_count":"2","content":"Selected Answer: C\nB is funny","poster":"RaviRS","timestamp":"1709817300.0","comment_id":"1001456"},{"timestamp":"1704798300.0","comment_id":"947043","content":"If it is an API performing scientific calculations then its customer base is a very specific targeted group. It should not be considered as a mass market app that is used by lots of people all over the world. Considering the business purpose of the API, option B would be more than sufficient to serve the need of customers anywhere in the world.","upvote_count":"1","poster":"Vignesh_Krishnamurthi"},{"content":"IDK if this question will be in the exam bc the answer should be C.\nbut kubemci has now been deprecated in favor of Ingress for Anthos. \nIngress for Anthos is the recommended way to deploy multi-cluster ingress.","timestamp":"1704788340.0","poster":"kapara","upvote_count":"3","comment_id":"946909"},{"timestamp":"1702302060.0","upvote_count":"1","poster":"BiddlyBdoyng","comment_id":"920621","content":"The problem with B is the question very much infers we are dealing with dynamic content & not static."},{"content":"It's A or C but I think A might be better.\nA is a simpler solution. Cloud DNS allows you to add multiple targets and part of its decision making is the latency. https://cloud.google.com/dns/docs/zones/zones-overview\nTough but I think A because it's only one API being exposed. Ingress comes into its own when exposing many services.","timestamp":"1702300740.0","upvote_count":"1","poster":"BiddlyBdoyng","comment_id":"920604"},{"comment_id":"814500","upvote_count":"4","content":"kubemci - deprecated\nhttps://github.com/GoogleCloudPlatform/k8s-multicluster-ingress","timestamp":"1692469380.0","poster":"r1ck"},{"comments":[{"comment_id":"757139","poster":"omermahgoub","timestamp":"1687754940.0","content":"Option A, \"Create a second GKE cluster in asia-southeast1, and expose both APIs using a Service of type LoadBalancer. Add the public IPs to the Cloud DNS zone,\" would not be a good choice because it would not provide a single global IP address for users to access the API, which would increase latency and complexity.\n\nOption B, \"Use a global HTTP(s) load balancer with Cloud CDN enabled,\" would not be a good choice because it would not allow you to serve the API from a closer location for users in Asia.\n\nOption D, \"Increase the memory and CPU allocated to the application in the cluster,\" would not be a good choice because it would not address the issue of latency for users in Asia accessing the API.","upvote_count":"2"}],"comment_id":"757138","timestamp":"1687754940.0","poster":"omermahgoub","upvote_count":"2","content":"A good option for reducing latency for users in Asia accessing the web API would be to create a second GKE cluster in asia-southeast1 and use kubemci to create a global HTTP(s) load balancer.\n\nOption C, \"Create a second GKE cluster in asia-southeast1, and use kubemci to create a global HTTP(s) load balancer,\" would be the correct choice for this scenario.\n\nBy creating a second GKE cluster in asia-southeast1, you can reduce latency for users in Asia by serving the API from a closer location. You can then use kubemci, a command-line tool that simplifies the process of creating a global HTTP(s) load balancer, to expose the APIs from both clusters through a single global IP address. This allows users to access the API with low latency, regardless of their location."},{"comment_id":"735796","timestamp":"1685949900.0","poster":"ale_brd_111","content":"Selected Answer: C\nAnswer is C but kubemci is deprecated, now you have to go with:\nMulti Cluster Ingress is a cloud-hosted controller for Google Kubernetes Engine (GKE) clusters. It's a Google-hosted service that supports deploying shared load balancing resources across clusters and across regions. To deploy Multi Cluster Ingress across multiple clusters, complete Setting up Multi Cluster Ingress then see Deploying Ingress across multiple clusters.\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/multi-cluster-ingress","upvote_count":"5"},{"poster":"Mahmoud_E","content":"Selected Answer: C\nC is correct, however, this question is an old question and need to be updated to use the ingress for global HTTPS LB","upvote_count":"2","comment_id":"702355","timestamp":"1682270220.0"},{"timestamp":"1681983060.0","content":"I understand that if a GKE cluster houses an API in US, Another cluster will house another API in Asia. So 2 APIs (load balanced or whatever) right?","comment_id":"699749","upvote_count":"1","poster":"melono"},{"content":"Kubemci is now deprecated for ingress for Anthos\nhttps://github.com/GoogleCloudPlatform/k8s-multicluster-ingress","timestamp":"1680966480.0","comment_id":"689462","upvote_count":"1","comments":[{"comments":[{"poster":"melono","timestamp":"1681982940.0","content":"It will be changed or not be in the exam bro, that is what will happen","comment_id":"699745","upvote_count":"1"}],"comment_id":"693145","upvote_count":"1","content":"We all know that Kubenci is deprecated. But it wasnt when this question was created. The truly concern is what happened when a question is outdated? Will it change its response?","timestamp":"1681308000.0","poster":"Rolazo"}],"poster":"AMEJack"},{"upvote_count":"5","content":"Selected Answer: A\nThis is an old question and answers are not updated .... Given that Google's best practice is to use \"Multi-Cluster Ingress\". I would go with Option A . kubemci is not recommended for multi-cluster solution .","poster":"muneebarshad","comment_id":"669344","timestamp":"1678836840.0"},{"content":"Selected Answer: B\nB, two reasons: 1. Kubemci is deprecated. 2. Considering the cost of having additional GKE.\nGKE is costlier than CDN.","poster":"shekarcfc","timestamp":"1678257360.0","upvote_count":"1","comment_id":"663104"},{"poster":"desertlotus1211","upvote_count":"1","content":"https://cloud.google.com/blog/products/gcp/how-to-deploy-geographically-distributed-services-on-kubernetes-engine-with-kubemci\n\nAnswer C","comment_id":"645631","timestamp":"1676162760.0"},{"timestamp":"1675351860.0","comment_id":"641285","poster":"abdelilahfa","upvote_count":"1","content":"Selected Answer: C\nC is the correct answer based on \nhttps://cloud.google.com/blog/products/gcp/how-to-deploy-geographically-distributed-services-on-kubernetes-engine-with-kubemci"},{"poster":"AzureDP900","comment_id":"626664","timestamp":"1672771800.0","content":"C is fine after reading the Google docs!","upvote_count":"2"},{"comment_id":"596186","timestamp":"1667422260.0","content":"ANSWER : A kubemci is deprecated","poster":"xfall12","upvote_count":"2"},{"timestamp":"1666423800.0","comment_id":"589800","upvote_count":"1","content":"Selected Answer: C\nC is ok. \nCluster is regional and you need a Global LB to balance","poster":"mark_af"},{"poster":"learner311","timestamp":"1665763980.0","comment_id":"585890","upvote_count":"2","content":"C. you need another cluster because 1. distance 2. increased load (number of users). load balance between them with a single endpoint. although the command is \"deprecated\" it's probably updated to MultiClusterIngress by now."},{"content":"I would think the fact that it's stateless means that CDN would not help (because there's no state to cache), in which case I would select the kubemci option.","comment_id":"581932","upvote_count":"2","timestamp":"1665075120.0","poster":"cloudmon"},{"timestamp":"1662470040.0","upvote_count":"1","comment_id":"562087","content":"Answer C cause kubmci is made for that porpouse.","poster":"Joanale"},{"comments":[{"timestamp":"1664554800.0","content":"Because question says \"scientific calculations\". CDN is bhasically static files cache.","poster":"snwbr","comment_id":"578403","comments":[{"content":"Best Answer for not using CDN","comment_id":"603145","upvote_count":"2","poster":"AmitAr","timestamp":"1668753300.0"}],"upvote_count":"2"}],"content":"I don't get it , Why not B?\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/ingress-features#cloud_cdn","comment_id":"555281","timestamp":"1661339520.0","poster":"kimharsh","upvote_count":"1"},{"upvote_count":"2","timestamp":"1660108440.0","content":"Go for B.\nThe CDN is Better option.\nadditionally, kubemci is deprecated.\nhttps://github.com/GoogleCloudPlatform/k8s-multicluster-ingress","comment_id":"544362","poster":"haroldbenites"},{"comment_id":"511824","upvote_count":"1","poster":"cherry23","timestamp":"1656473880.0","content":"Selected Answer: C\nC is better choice for a new region"},{"timestamp":"1655293320.0","upvote_count":"1","content":"seems C to me but kubemci should be deprecated or i'm wrong?","comment_id":"502195","poster":"pakilodi"},{"upvote_count":"1","timestamp":"1655254620.0","content":"Selected Answer: C\nC is the correct answer\nhttps://cloud.google.com/blog/products/gcp/how-to-deploy-geographically-distributed-services-on-kubernetes-engine-with-kubemci","comment_id":"501817","poster":"vincy2202"},{"poster":"shindeswap","comment_id":"499582","upvote_count":"2","timestamp":"1654963020.0","content":"C\nThe App does Scientific Calculations. Will the calculations be same for all users since its a Stateless Web API?"},{"content":"Stateless Web API as mentioned in the question can indicate possiblility for data caching (unlike website images etc). The company has already rolled out API already in Asia but latency needs to be addressed. \nOption A - Funny assumption. there is no mention of two api roll outs (\"expose both APIs using a Service of type LoadBalancer\")\nOption B - Global Http(s) load balancer is specific for this kind of global rollouts. Cloud CDN inherent feature of caching thereby reducing the latency can be used.\nOption C - Http(s) load balancer creation alone won't address the latency. Creation of second GKE Cluster might be a solution but would address specific to users in Asia. As an archtiect i believe there should be a holistic approach of the solution and US region also needs to be considered. This answer incresases technical complexity.\nOption D - Increase in memory and CPU does not alone solve latency issue when requests arrive from Internet !","upvote_count":"3","comment_id":"493092","poster":"otts","timestamp":"1654247100.0"},{"timestamp":"1654128960.0","content":"Selected Answer: C\nB is too simplistic and assumes the calculations are cacheable. would be the right answer is true.","poster":"cdcollector","comment_id":"492093","upvote_count":"2"},{"content":"Selected Answer: C\nVote C","poster":"nqthien041292","comment_id":"490527","upvote_count":"1","timestamp":"1653892740.0"},{"upvote_count":"2","content":"The answer is C.\nhttps://cloud.google.com/blog/products/gcp/how-to-deploy-geographically-distributed-services-on-kubernetes-engine-with-kubemci","timestamp":"1650877620.0","comment_id":"467370","poster":"Agrossi74"},{"comment_id":"461514","content":"\"API that performs scientific calculations\"=> it seems that CDN is not suitable for write workload","timestamp":"1649849820.0","upvote_count":"3","poster":"JasonChuang","comments":[{"comment_id":"466971","content":"The question does not indicate a write workload. \"Stateless\" makes me think they want the CDN solution.","poster":"trismegistus","upvote_count":"1","timestamp":"1650805620.0"}]},{"comment_id":"459282","content":"Answer B.\n\nUsing Cloud CDN\n\nCloud CDN works with external HTTP(S) Load Balancing to deliver content to your users. The external HTTP(S) load balancer provides the frontend IP addresses and ports that receive requests and the backends (or origins) that respond to the requests.\n\nCloud CDN content can be sourced from various types of backends:\n\n Instance groups\n Zonal network endpoint groups (NEGs)\n Serverless NEGs: One or more App Engine, Cloud Run, or Cloud Functions services\n Internet NEGs for external backends\n Buckets in Cloud Storage\nhttps://cloud.google.com/cdn/docs/using-cdn","poster":"Linus11","timestamp":"1649430600.0","upvote_count":"4"},{"comment_id":"451078","upvote_count":"1","content":"Why is it not b. https://cloud.google.com/kubernetes-engine/docs/concepts/multi-cluster-ingress?","poster":"Nik22","timestamp":"1648158900.0","comments":[{"upvote_count":"1","timestamp":"1657471740.0","comment_id":"521048","content":"How is your link related? It's not about Cloud CDN","poster":"lxgywil"}]},{"upvote_count":"2","content":"C is the answer ! there should not be any latency for Asia users is the key","timestamp":"1646141760.0","comment_id":"437097","poster":"papputaufik"},{"timestamp":"1646065380.0","comment_id":"433955","poster":"victory108","content":"C. Create a second GKE cluster in asia-southeast1, and use kubemci to create a global HTTP(s) load balancer.","upvote_count":"3"}],"answer":"C","choices":{"A":"Create a second GKE cluster in asia-southeast1, and expose both APIs using a Service of type LoadBalancer. Add the public IPs to the Cloud DNS zone.","C":"Create a second GKE cluster in asia-southeast1, and use kubemci to create a global HTTP(s) load balancer.","B":"Use a global HTTP(s) load balancer with Cloud CDN enabled.","D":"Increase the memory and CPU allocated to the application in the cluster."},"exam_id":4,"question_text":"Your company has a stateless web API that performs scientific calculations. The web API runs on a single Google Kubernetes Engine (GKE) cluster. The cluster is currently deployed in us-central1. Your company has expanded to offer your API to customers in Asia. You want to reduce the latency for users in Asia.\nWhat should you do?","answer_images":[],"answer_ET":"C","question_images":[],"answer_description":""},{"id":"Kt1n3AOufnYPwXMhVH2z","choices":{"B":"Create an App Engine flexible environment, and deploy the third-party application using a Dockerfile and a custom runtime. Set CPU and memory options similar to your application's current on-premises virtual machine in the app.yaml file.","A":"Create an instance template with the smallest available machine type, and use an image of the third-party application taken from a current on-premises virtual machine. Create a managed instance group that uses average CPU utilization to autoscale the number of instances in the group. Modify the average CPU utilization threshold to optimize the number of instances running.","D":"Create a Compute Engine instance with CPU and memory options similar to your application's current on-premises virtual machine. Install the Cloud Monitoring agent, and deploy the third-party application. Run a load test with normal traffic levels on the application, and follow the Rightsizing Recommendations in the Cloud Console.","C":"Create multiple Compute Engine instances with varying CPU and memory options. Install the Cloud Monitoring agent, and deploy the third-party application on each of them. Run a load test with high traffic levels on the application, and use the results to determine the optimal settings."},"answer_ET":"D","answers_community":["D (69%)","A (22%)","9%"],"answer_images":[],"isMC":true,"question_images":[],"topic":"1","unix_timestamp":1629804180,"exam_id":4,"url":"https://www.examtopics.com/discussions/google/view/60494-exam-professional-cloud-architect-topic-1-question-155/","discussion":[{"poster":"pr2web","comment_id":"441715","comments":[{"poster":"melono","content":"The point:\n2. Cost-based recommendations: Recommends Compute Engine instances based on:\n The current CPU and RAM configuration of the on-premises VM.","comment_id":"699763","upvote_count":"1","timestamp":"1681983840.0"}],"upvote_count":"55","content":"Answer is D. \n\nhttps://cloud.google.com/migrate/compute-engine/docs/4.9/concepts/planning-a-migration/cloud-instance-rightsizing?hl=en\n\n\"Rightsizing provides two types of recommendations:\n\n1. Performance-based recommendations: Recommends Compute Engine instances based on the CPU and RAM currently allocated to the on-premises VM. This recommendation is the default.\n\n2. Cost-based recommendations: Recommends Compute Engine instances based on:\n- The current CPU and RAM configuration of the on-premises VM.\n- The average usage of this VM during a given period. To use this option, you must activate rightsizing monitoring with vSphere for this group of VMs and allow time for Migrate for Compute Engine to analyze usage.","timestamp":"1646797560.0"},{"poster":"cloudmon","comments":[{"content":"Another (less obvious) reason for choosing D: I've noticed a pattern in these exams that the cloud provider wants to advertise and promote anything that they consider to be a cool feature of their platform. In this case, they are promoting their recommendation engine. If there's even an option that sounds like it's advertising a relevant managed service from the cloud provider, then that's usually one to consider.","comment_id":"583438","comments":[{"upvote_count":"1","content":"I also find the following wording in option A to be a bit iffy: \"an image of the third-party application taken from a current on-premises virtual machine\". That seems a bit vague in terms of what the image format would be.","comment_id":"583440","poster":"cloudmon","timestamp":"1665338880.0"}],"poster":"cloudmon","timestamp":"1665338760.0","upvote_count":"4"}],"upvote_count":"9","content":"Selected Answer: D\nIt's definitely D. See the reference at the following link that says \"The recommendation algorithm is suited to workloads that follow weekly patterns\", which matches the part of the questions that says \"consistent usage pattern over multiple weeks\":\nhttps://cloud.google.com/compute/docs/instances/apply-machine-type-recommendations-for-instances\n\nOption A also has two problems;\n1. It only focuses on CPU, but the question says \"CPU and memory\"\n2. The question does not mention anything about horizontal scalability","timestamp":"1665338460.0","comment_id":"583436"},{"poster":"e5019c6","comments":[{"content":"Also note that Migrate to Virtual Machines v4.11 (Which the info is from) is no longer the latest version. V5 is already out and, strangely, lacks any article about rightsizing recommendations...","upvote_count":"2","poster":"e5019c6","comment_id":"1107462","timestamp":"1719546480.0"}],"content":"Selected Answer: D\nI choose A at first, because I thought that the Rightsizing Recommendations took various days to offer the estimate stats. But according to this article:\nhttps://cloud.google.com/migrate/compute-engine/docs/4.11/concepts/planning-a-migration/cloud-instance-rightsizing\nWhile it needs a week to give a proper estimate, it can give an estimate with less time too (But the accuracy decreases)\n\"For better recommendations, Migrate for Compute Engine recommends monitoring the migrated workloads for at least seven consecutive days (or one typical business week). Migrate for Compute Engine warns you when the monitoring period is insufficient for an adequate recommendation.\n\nEven if the monitoring period is insufficient, Migrate for Compute Engine still offers a cost-optimized recommendation based on the data available.\"","comment_id":"1107460","timestamp":"1719546480.0","upvote_count":"3"},{"timestamp":"1713486540.0","poster":"tamj123","content":"D make sense.","comment_id":"1047392","upvote_count":"1"},{"content":"Selected Answer: D\nD is correct","poster":"RaviRS","upvote_count":"1","comment_id":"1001459","timestamp":"1709817420.0"},{"comment_id":"839690","timestamp":"1694759640.0","content":"Selected Answer: D\nOption D would be the best option to optimize resource usage for the lowest cost when migrating third-party applications from optimized on-premises virtual machines to Google Cloud.","poster":"WinSxS","upvote_count":"2"},{"content":"Selected Answer: D\nAnswer is D. Similar than third-party convince me...","poster":"AugustoKras011111","upvote_count":"1","comment_id":"831127","timestamp":"1694017320.0"},{"poster":"beehive","content":"why most of the answers selected by host is INCORRECT? Is it intentional to misguide the folks?","upvote_count":"7","comment_id":"766103","timestamp":"1688498880.0"},{"comment_id":"756023","upvote_count":"1","poster":"thamaster","content":"Selected Answer: D\ni choose D as it's best practice create an instance with similar configuration as on premise and check metrics","timestamp":"1687721820.0"},{"content":"Selected Answer: D\nD is the right one because of Rightsizing option from GCP","comment_id":"748461","timestamp":"1687033380.0","upvote_count":"1","poster":"shefalia"},{"poster":"surajkrishnamurthy","content":"Selected Answer: D\nD is the correct answer","comment_id":"745788","timestamp":"1686805920.0","upvote_count":"1"},{"poster":"megumin","comment_id":"719730","timestamp":"1684239780.0","content":"Selected Answer: D\nD is ok","upvote_count":"1"},{"poster":"Mahmoud_E","comment_id":"699402","upvote_count":"1","content":"Selected Answer: D\nI agree with D is the most accurate","timestamp":"1681946280.0"},{"timestamp":"1681580760.0","upvote_count":"1","poster":"AzureDP900","comment_id":"695570","content":"D is correct"},{"poster":"SerGCP","upvote_count":"3","comment_id":"681994","content":"Selected Answer: D\nA, application may not support horizontal scaling and may not run in instances whith small cpu\nB, dockerize third-party applications is not a requirement....Complex and costly\nC, too expensive\nD, simple and works","timestamp":"1680025860.0"},{"timestamp":"1678257600.0","poster":"shekarcfc","upvote_count":"4","content":"Selected Answer: A\nA, the benefit of moving to cloud is scaling based on load, start with min infra and scale-up based on usage.","comment_id":"663106"},{"comment_id":"604928","upvote_count":"3","poster":"amxexam","content":"Selected Answer: D\nA - you cannot expect application to behavior similar in 2 different envior met without a test.\nB - App Engine is costly\nC- Varing cpu and memory cannot be doone.\nD- correa.","timestamp":"1669047900.0"},{"timestamp":"1663408380.0","upvote_count":"5","comment_id":"569686","poster":"Meyucho","content":"Selected Answer: A\nThe D is not correct. The MIG image was made before the third party software was installed so any new VM on the group will start without this software. Answer is A"},{"comment_id":"544366","upvote_count":"1","timestamp":"1660108800.0","poster":"haroldbenites","content":"Go for D"},{"upvote_count":"3","timestamp":"1660108200.0","poster":"anjuagrawal","content":"Selected Answer: A\nThe options can be A , B or D. B is not true because is says CPU and Memory options are not clear. Hence, configuration of app.ymp in AppEngine Flexible will not be viable. Option D is not correct because it says create a virtual machine whereas there are many VMs and also scaling would not be possible. So, A is the correct answer.","comment_id":"544361"},{"upvote_count":"2","poster":"Supss","comment_id":"532313","timestamp":"1658764560.0","content":"Selected Answer: B\nkey here is 'lowest cost' = App Engin.\nso the answer is B"},{"comment_id":"527722","timestamp":"1658240700.0","poster":"technodev","comments":[{"poster":"[Removed]","upvote_count":"1","timestamp":"1659634800.0","content":"What was your result?","comment_id":"540610"}],"content":"Got this question in my exam, answered D","upvote_count":"3"},{"poster":"OrangeTiger","content":"Selected Answer: D\nA is not worong way.\nBut the question said clearly 'optimized on-premises virtual machines' and 'The applications have a consistent usage pattern across multiple weeks.'.\nSo I choose D.","upvote_count":"2","comment_id":"519994","timestamp":"1657343520.0"},{"upvote_count":"5","content":"A no make sense. The lowest machine on GCP is e2-micro (Micro machine type with 0.25 vCPU and 1 GB of memory, backed by a shared physical core). The application may not even start with this configuration. Horizontal scaling doesn't resolve it.","comment_id":"512829","comments":[{"upvote_count":"1","content":"very true. image start up an in-memory db on e2-micro :-D \nGCE would not have any chance to scale up before the db terminates itself.","comment_id":"587408","poster":"meokey","timestamp":"1666046340.0"}],"timestamp":"1656536280.0","poster":"ehgm"},{"comment_id":"506326","timestamp":"1655826960.0","upvote_count":"1","poster":"ABO_Doma","content":"Selected Answer: A\nyou could use a predefined machine type like e2-highmem-4/n2-highmem-4/n2d-highmem-4 etc. if you need 4 VCPUs and 32GB memory, there's no guarantee that it performs similar to the existing VM in the data centre. The networking fabric is different, the disk I/O is different, and the CPUs are different too. We don't know the exact specifications of the data centre CPUs to draw a parallel to the processors offered by GCP. As you can see, the performance can vary a lot depending on the frequency. (remember shelling out additional 500$ for upgrading CPU from 2.6GHz to 2.8GHz when buying your a laptop?). You may realize that you need more vCPUs after migrating to Google Cloud or maybe less, but until you migrate and test it out, there is no way to say which is the best machine type. So the recommendation should be to start small, increase the instance size as needed until the performance is of an acceptable standard, and that is your machine type."},{"comment_id":"505765","poster":"AmitMittal","upvote_count":"3","content":"Selected Answer: B\nb is right. For CONSISTENT Traffic, APP flexible should be used.","timestamp":"1655767860.0"},{"content":"Selected Answer: D\nD is the answer, you start with matching spec, monitoring agent gathers metrics and right sizing recommendations help to size them correctly. It is the complete solution. \nOption A talks about instance groups to get the right sizing for a single application. That means you end up with multiple instance groups for all the 3rd party applications.","timestamp":"1655384340.0","comments":[{"timestamp":"1676421720.0","poster":"LuisFdz","upvote_count":"1","content":"D, is correct","comment_id":"646939"}],"comment_id":"502996","poster":"pradhyumna","upvote_count":"2"},{"poster":"vincy2202","content":"Selected Answer: D\nD is the correct answer\nhttps://cloud.google.com/migrate/compute-engine/docs/4.9/concepts/planning-a-migration/cloud-instance-rightsizing?hl=en","comment_id":"501819","timestamp":"1655255280.0","upvote_count":"2"},{"poster":"vchrist","content":"Selected Answer: D\nD\nhttps://cloud.google.com/blog/products/compute/5-best-practices-compute-engine-cost-optimization","comment_id":"490283","timestamp":"1653862320.0","upvote_count":"3"},{"content":"Selected Answer: D\nVote D","timestamp":"1653709140.0","poster":"pakilodi","upvote_count":"2","comment_id":"488855"},{"upvote_count":"1","comment_id":"488046","content":"Selected Answer: D\nproblem with A is you are running multiple instances to get the sizing correct. \nWith D - you are sizing the instance properly","timestamp":"1653640140.0","poster":"mudot"},{"timestamp":"1653545220.0","comment_id":"487164","upvote_count":"1","content":"Selected Answer: D\nvote D","poster":"joe2211"},{"poster":"sriandy","comment_id":"472299","content":"Answer is D. Not sure if the application is optimized to run on the cloud with the same infrastructure as on-premise. Option A may work but it's not guaranteed that the resources are optimized.","timestamp":"1651606860.0","upvote_count":"1"},{"content":"For all people that consider A as an option:\nA suggest to put the application with instance group and scale the number of instances based on CPU utilization. However - what if the application is stateful, how would you take care of keeping a synchronized 'state' in the instance group? What if application is not optimized for horizontal scalling at all?\n\nWhat if application itself needs more cpu/ram to run even without the workload? Then scalling the IG to 10, or 100 machines, doesn't change the fact that each of the instances will be over-loaded.\n\nA simply does_not_make_any_sense.\nit's D, reason already given in other comments.","poster":"cotam","upvote_count":"1","comment_id":"467381","timestamp":"1650879180.0"},{"comments":[{"timestamp":"1655626200.0","upvote_count":"1","comment_id":"504776","poster":"[Removed]","content":"Got this one. Q was with Resizing recommendation appended. Selected D."}],"content":"A or D are the nearest answers. Reading the right-sizing article, D is more convincing answer here.","upvote_count":"1","comment_id":"466627","poster":"[Removed]","timestamp":"1650725940.0"},{"poster":"ACE_ASPIRE","comment_id":"450966","timestamp":"1648143840.0","upvote_count":"2","content":"it should be A"},{"upvote_count":"3","content":"I will go for D. Considering the statement \"You want to optimize resource usage for the lowest cost\"","poster":"diaga2","comments":[{"timestamp":"1649594700.0","poster":"rottzy","content":"still you'd rather go with resource allocation similar to on-prem for Cloud??","upvote_count":"1","comment_id":"460058","comments":[{"content":"D sounds realistic though, would you rather start with the base minimum config & expect optimum resource usage?\nwith D, its easier to convince clients to go for existing resource & inform them of 'cost cutting provided by cloud' after resizing : D","upvote_count":"2","comment_id":"460066","timestamp":"1649595300.0","poster":"rottzy"}]}],"comment_id":"441534","timestamp":"1646763000.0"},{"poster":"Manh","content":"on-premises virtual machines to Google Cloud -> GCE. So D is correct","upvote_count":"2","timestamp":"1646641740.0","comment_id":"440743"},{"comment_id":"440740","timestamp":"1646641560.0","upvote_count":"2","poster":"Manh","content":"B is working"},{"poster":"victory108","content":"D. Create a Compute Engine instance with CPU and memory options similar to your application's current on-premises virtual machine. Install the Cloud Monitoring agent, and deploy the third-party application. Run a load test with normal traffic levels on the application, and follow the Rightsizing Recommendations in the Cloud Console.","timestamp":"1646065320.0","comment_id":"433952","upvote_count":"1"},{"content":"D is ok. \nhttps://cloud.google.com/migrate/compute-engine/docs/4.9/concepts/planning-a-migration/cloud-instance-rightsizing?hl=en","timestamp":"1645800720.0","poster":"vladik820","comment_id":"431470","upvote_count":"4"},{"timestamp":"1645768620.0","upvote_count":"2","poster":"fahad01hbti","content":"B, because of Applications that receive consistent traffic, experience regular traffic fluctuations, or meet the parameters for scaling up and down gradually.","comment_id":"431117"},{"comment_id":"430701","poster":"SweetieS","upvote_count":"3","timestamp":"1645708980.0","content":"D is OK"}],"timestamp":"2021-08-24 13:23:00","answer_description":"","answer":"D","question_text":"You are migrating third-party applications from optimized on-premises virtual machines to Google Cloud. You are unsure about the optimum CPU and memory options. The applications have a consistent usage pattern across multiple weeks. You want to optimize resource usage for the lowest cost. What should you do?","question_id":63},{"id":"0mrUxbPp40JOOhQeEONr","question_text":"Your company has a Google Cloud project that uses BigQuery for data warehousing. They have a VPN tunnel between the on-premises environment and Google\nCloud that is configured with Cloud VPN. The security team wants to avoid data exfiltration by malicious insiders, compromised code, and accidental oversharing.\nWhat should they do?","timestamp":"2021-08-23 20:13:00","url":"https://www.examtopics.com/discussions/google/view/60416-exam-professional-cloud-architect-topic-1-question-156/","answer_description":"","answer_images":[],"isMC":true,"answer":"C","discussion":[{"timestamp":"1653592380.0","upvote_count":"72","content":"Without the discussion this site would be useless, many thanks to all that participate. Majority of answers are wrong...","comments":[{"timestamp":"1701627360.0","poster":"VarunGo","comments":[{"poster":"Murtuza","comment_id":"1023931","upvote_count":"12","timestamp":"1712147700.0","content":"Then you are definitely bound to fail :-)"}],"upvote_count":"3","comment_id":"913792","content":"you can used chatGPT now"}],"comment_id":"487640","poster":"Craigenator"},{"content":"C is the recommended one https://cloud.google.com/vpc-service-controls/docs/overview","upvote_count":"31","comment_id":"441540","poster":"diaga2","timestamp":"1646763720.0"},{"upvote_count":"1","timestamp":"1735992120.0","comment_id":"1336362","poster":"plumbig11","content":"Selected Answer: C\nThey have a VPN tunnel between the on-premises environment and Google\nCloud that is configured with Cloud VPN. Si the better option is VPC service controls. https://cloud.google.com/vpc-service-controls/docs/overview"},{"timestamp":"1718401860.0","comment_id":"1096911","upvote_count":"2","content":"Correct answer is C.\nSecurity benefits of VPC Service Controls\nAccess from unauthorized networks using stolen credentials\nData exfiltration by malicious insiders or compromised code\nhttps://cloud.google.com/vpc-service-controls/docs/overview#benefits","poster":"squishy_fishy"},{"upvote_count":"2","timestamp":"1715434620.0","comment_id":"1067932","content":"Selected Answer: C\nVPC Service Controls is required to stop data exfiltration. Hence C","poster":"thewalker"},{"comment_id":"1047897","content":"C, VPC Service controls is need for the solution","timestamp":"1713529920.0","poster":"tamj123","upvote_count":"1"},{"content":"Selected Answer: C\nC is correct","comment_id":"821829","timestamp":"1692989760.0","poster":"Mrinalini19","upvote_count":"1"},{"timestamp":"1688776980.0","comment_id":"769053","content":"Selected Answer: C\nC is the correct answer,\n\nTo secure data from exfiltration by malicious insiders, compromised code or accidental oversharing, we use VPC Service controls\n\nhttps://cloud.google.com/vpc-service-controls/docs/overview\n\nFor private access options, connect to services in VPC networks we use private service endpoints or VPC network peering.\n\nhttps://cloud.google.com/vpc/docs/private-access-options#connect-services","poster":"examch","upvote_count":"2"},{"timestamp":"1686806160.0","upvote_count":"2","comment_id":"745794","content":"Selected Answer: C\nC is the correct answer","poster":"surajkrishnamurthy"},{"timestamp":"1684241220.0","upvote_count":"2","content":"Selected Answer: C\nC is ok","comment_id":"719753","poster":"megumin"},{"poster":"Mahmoud_E","comment_id":"699404","timestamp":"1681946520.0","upvote_count":"1","content":"Selected Answer: C\nC is the right answer"},{"upvote_count":"1","timestamp":"1681580880.0","content":"I will go with C","poster":"AzureDP900","comment_id":"695571"},{"comment_id":"590652","content":"Selected Answer: C\nGoing by definition- VPC Service Controls improves your ability to mitigate the risk of data exfiltration from Google Cloud services such as Cloud Storage and BigQuery. \n\nhence C is correct","timestamp":"1666531740.0","upvote_count":"7","poster":"nkit"},{"upvote_count":"2","timestamp":"1666061160.0","comment_id":"587448","poster":"dangcpped","content":"Selected Answer: C\nC is the recommended \nhttps://cloud.google.com/vpc-service-controls/docs/overview"},{"upvote_count":"2","timestamp":"1661608200.0","content":"I don't get it , C is correct because of the \"VPC service Control\", But Privet Google access is not for on On-premises, A is for On-premises = https://cloud.google.com/vpc/docs/private-access-options","poster":"kimharsh","comment_id":"557419"},{"upvote_count":"1","timestamp":"1657344360.0","comment_id":"520000","content":"Selected Answer: C\nI agree C.\nThe link that wroted in Reveral Solution means C.","poster":"OrangeTiger"},{"upvote_count":"2","content":"Selected Answer: C\nC is the correct answer\nhttps://cloud.google.com/vpc-service-controls/docs/overview","timestamp":"1655255580.0","comment_id":"501821","poster":"vincy2202"},{"upvote_count":"2","content":"Selected Answer: C\nhttps://cloud.google.com/vpc-service-controls/docs/overview","comment_id":"493286","timestamp":"1654270860.0","poster":"sapsant"},{"upvote_count":"2","timestamp":"1653709260.0","content":"Selected Answer: C\nVote C","comment_id":"488857","poster":"pakilodi"},{"timestamp":"1653545280.0","upvote_count":"2","comment_id":"487165","content":"Selected Answer: C\nvote C","poster":"joe2211"},{"comment_id":"474367","content":"VPC Service Control and Private Google Access","poster":"dmc123","upvote_count":"2","timestamp":"1652016780.0"},{"poster":"sudsap","timestamp":"1649983560.0","comment_id":"462323","upvote_count":"8","content":"C. VPC Service Controls provides an extra layer of security defense for Google Cloud services that is independent of Identity and Access Management (IAM). You can configure private communication to Google Cloud resources from VPC networks that span hybrid environments with Private Google Access on-premises extensions."},{"content":"C - exfilteration?? its VPC service control\nits a new feature released in Google cloud","comment_id":"460069","timestamp":"1649595420.0","upvote_count":"6","poster":"rottzy"},{"poster":"[Removed]","comment_id":"453970","timestamp":"1648544040.0","content":"Answer should be C\nAlso stated at GCP site: https://cloud.google.com/vpc-service-controls/docs/overview\nData exfiltration by malicious insiders or compromised code: VPC Service Controls complements network egress controls by preventing clients within those networks from accessing the resources of Google-managed services outside the perimeter.","upvote_count":"4"},{"content":"Wow, there are soooo many wrong answers here. NOt sure this place helps?!","comments":[{"timestamp":"1646637540.0","content":"1) Ignore the default answer\n2) Go through the discussions\n3) Validate the most probable answer against google docs and your own studies.","comment_id":"440717","upvote_count":"17","poster":"JustJack21"}],"poster":"JustADudeTakingATest","timestamp":"1646088180.0","comment_id":"435799","upvote_count":"2"},{"upvote_count":"4","content":"C. Configure VPC Service Controls and configure Private Google Access.","timestamp":"1646067240.0","comment_id":"433971","poster":"victory108"},{"content":"C is OK","timestamp":"1645709040.0","upvote_count":"4","comment_id":"430703","poster":"SweetieS"},{"content":"IMO its C: VPC Service Control","comment_id":"430643","poster":"Bhardwajriddhi","upvote_count":"4","timestamp":"1645702140.0"},{"poster":"pr2web","comment_id":"430239","upvote_count":"8","timestamp":"1645647180.0","content":"Why isn't this C. Configure VPC Service Controls and configure Private Google Access.? \n\nVPC Service Controls restrict the necessary access as well as private google restricts the need for an external IP altogether."}],"answer_ET":"C","question_images":[],"topic":"1","choices":{"D":"Configure Private Google Access.","C":"Configure VPC Service Controls and configure Private Google Access.","B":"Perform the following tasks: 1. Create a service account. 2. Give the BigQuery JobUser role and Storage Reader role to the service account. 3. Remove all other IAM access from the project.","A":"Configure Private Google Access for on-premises only."},"unix_timestamp":1629742380,"question_id":64,"answers_community":["C (100%)"],"exam_id":4},{"id":"txx5kHFwFsp5GubtGybU","topic":"1","question_images":[],"answer_images":[],"timestamp":"2021-08-24 13:26:00","isMC":true,"answer_ET":"D","answers_community":["D (76%)","C (24%)"],"unix_timestamp":1629804360,"answer":"D","question_id":65,"url":"https://www.examtopics.com/discussions/google/view/60495-exam-professional-cloud-architect-topic-1-question-157/","choices":{"A":"Add the node group name as a network tag when creating Compute Engine instances in order to host each workload on the correct node group.","C":"Use node affinity labels based on the node group name when creating Compute Engine instances in order to host each workload on the correct node group.","D":"Use node affinity labels based on the node name when creating Compute Engine instances in order to host each workload on the correct node.","B":"Add the node name as a network tag when creating Compute Engine instances in order to host each workload on the correct node."},"exam_id":4,"answer_description":"","discussion":[{"comments":[{"timestamp":"1718890140.0","upvote_count":"1","poster":"Sephethus","content":"Except that sole tenant nodes can also be grouped, and wouldn't it be a best practice to design for scaling?","comment_id":"1233637"}],"upvote_count":"61","timestamp":"1631152920.0","poster":"pr2web","comment_id":"441719","content":"Answer is D. \n\nY'all not reading the fine details. The question is about aligning EACH client to their dedicated nodes (D), not to a node group (C). \n\nhttps://cloud.google.com/compute/docs/nodes/sole-tenant-nodes#default_affinity_labels\n\nThe above reference clearly articulates the default affinity label for node group and node name. Unless we're thinking about growing each client to their own dedicated node groups (not in the current requirement), then the answer is not C, rather D. \n\nCompute Engine assigns two default affinity labels to each node:\n\nA label for the node group name:\nKey: compute.googleapis.com/node-group-name\nValue: Name of the node group.\nA label for the node name:\nKey: compute.googleapis.com/node-name\nValue: Name of the individual node."},{"poster":"Binoz","timestamp":"1630395240.0","comments":[{"upvote_count":"7","comment_id":"441340","content":"Thatâ€™s what i thought too","timestamp":"1631091300.0","poster":"MikeB19"}],"upvote_count":"18","comment_id":"436102","content":"D. Afinity should be set at node level, not node-group as every client has its own node in the group"},{"poster":"plumbig11","upvote_count":"1","timestamp":"1735992240.0","content":"Selected Answer: D\naffinity label only for dedicated nodes, not a node group.\nhttps://cloud.google.com/compute/docs/nodes/sole-tenant-nodes#default_affinity_labels","comment_id":"1336364"},{"timestamp":"1734648000.0","poster":"exam400","comment_id":"1329190","upvote_count":"1","content":"Selected Answer: D\nyou cannot use node affinity labels based on the node group name."},{"comment_id":"1284166","upvote_count":"2","comments":[{"content":"Sure, but the affinity must be set at the individual node level, not the node group level. Thus D is correct.","poster":"Positron75","timestamp":"1729504860.0","comment_id":"1300900","upvote_count":"1"}],"content":"Selected Answer: C\nQuestion already says node-group created \"You created a sole-tenant node group and added a node for each client\"","timestamp":"1726410360.0","poster":"192dcc7"},{"content":"Selected Answer: C\nKV Affinity Label > Node template > Node Group > CE\nQuestion already said Node group is created. Then why tag it to node ?","upvote_count":"1","poster":"192dcc7","comment_id":"1283303","timestamp":"1726250700.0"},{"upvote_count":"1","comment_id":"1258661","poster":"lucaluca1982","content":"Selected Answer: C\nUsing node affinity labels based on the node group name when creating Compute Engine instances is the appropriate method to ensure workloads are hosted on the correct node group. This approach aligns with Google Cloud's recommended practices for provisioning sole-tenant VMs and provides the required isolation for client workloads.","timestamp":"1722409440.0"},{"poster":"thewalker","upvote_count":"2","comment_id":"1082253","timestamp":"1701156120.0","content":"Selected Answer: D\nD\nAs per the documentation: https://cloud.google.com/compute/docs/nodes/provisioning-sole-tenant-vms#provision_a_sole-tenant_vm"},{"timestamp":"1697719080.0","comment_id":"1047905","poster":"tamj123","upvote_count":"1","content":"D, the question ask â€œYou created a sole-tenant node group and added a node for each client.â€ï¼Œso node affinity labels based on the node name is need it."},{"content":"I had this question recently (end of jan 2023) and went with answer D. After doing some investigation, that seems to be the right answer to me.","comments":[{"timestamp":"1684944780.0","content":"Preparing for exam and gone through the concept make sense the answer is D","upvote_count":"1","comment_id":"906031","poster":"LaxmanTiwari"}],"upvote_count":"3","comment_id":"793852","poster":"Andras2k","timestamp":"1675154040.0"},{"content":"Answer is D.\nRef: you can't specify node affinity labels on a node group.>> https://cloud.google.com/compute/docs/nodes/sole-tenant-nodes#node_templates","upvote_count":"1","comment_id":"768081","poster":"beehive","timestamp":"1673039040.0"},{"content":"Selected Answer: D\nD is the correct answer","upvote_count":"1","comment_id":"745799","timestamp":"1671088740.0","poster":"surajkrishnamurthy"},{"poster":"megumin","upvote_count":"1","timestamp":"1668611100.0","content":"Selected Answer: D\nD is ok","comment_id":"719771"},{"poster":"Mahmoud_E","upvote_count":"1","content":"Selected Answer: D\nD is the correct answer, VMs must be associated to a specific node within the node-group, so you must use the node name label to provision the VM.","timestamp":"1666223580.0","comment_id":"699415"},{"poster":"AzureDP900","timestamp":"1665856320.0","comment_id":"695576","content":"D is right, Node is right choice instead of node group","upvote_count":"1"},{"timestamp":"1658304960.0","upvote_count":"4","comment_id":"633940","content":"D : https://cloud.google.com/compute/docs/nodes/sole-tenant-nodes\nNode affinity labels are key-value pairs assigned to nodes, and are inherited from a node template. Affinity labels let you:\n\nControl how individual VM instances are assigned to nodes.\nControl how VM instances created from a template, such as those created by a managed instance group, are assigned to nodes.\nGroup sensitive VM instances on specific nodes or node groups, separate from other VMs.","poster":"deenee"},{"timestamp":"1648719060.0","comment_id":"578768","upvote_count":"3","content":"I go with C as I believe single-tenant node group meant for only one client","poster":"slars2k"},{"comment_id":"572587","poster":"Skr6266","content":"Answer is D since it is clearly documented as \nWhen you create a VM, you request sole-tenancy by specifying node affinity or anti-affinity, referencing one or more node affinity labels. You specify custom node affinity labels when you create a node template, and Compute Engine automatically includes some default affinity labels on each node. By specifying affinity when you create a VM, you can schedule VMs together on a specific node or nodes in a node group. By specifying anti-affinity when you create a VM, you can ensure that certain VMs are not scheduled together on the same node or nodes in a node group.\n\nNode affinity labels are key-value pairs assigned to nodes, and are inherited from a node template. Affinity labels let you:\n\nControl how individual VM instances are assigned to nodes.\nControl how VM instances created from a template, such as those created by a managed instance group, are assigned to nodes.\nGroup sensitive VM instances on specific nodes or node groups, separate from other VMs.","timestamp":"1647905580.0","upvote_count":"1"},{"timestamp":"1644960300.0","comment_id":"548068","upvote_count":"1","poster":"azureaspirant","content":"2/15/21"},{"comment_id":"520004","upvote_count":"1","timestamp":"1641714240.0","poster":"OrangeTiger","content":"They must 'Use node affinity labels based on the node name'.\nBecause 'added a node for each client'.\nSo I chose D.\nTy guys."},{"comment_id":"505768","content":"D is abs right","poster":"AmitMittal","timestamp":"1640050500.0","upvote_count":"1"},{"content":"Selected Answer: D\nhttps://cloud.google.com/compute/docs/nodes/sole-tenant-nodes#default_affinity_labels","poster":"SamGCP","comment_id":"503170","timestamp":"1639685460.0","upvote_count":"2"},{"comment_id":"501826","timestamp":"1639538460.0","poster":"vincy2202","upvote_count":"1","content":"Selected Answer: D\nD is the correct answer."},{"poster":"[Removed]","upvote_count":"1","comment_id":"493317","content":"Selected Answer: D\nD is right. \nThere is very thin difference node group vs node label. Marked C is wrong.","timestamp":"1638557820.0"},{"comment_id":"487173","poster":"joe2211","timestamp":"1637914740.0","upvote_count":"2","content":"Selected Answer: D\nvote D"},{"poster":"faceits","timestamp":"1637230200.0","content":"Answer is C\"\nWhy? ---> Description: \n\"You created a sole-tenant node group and added a node for each client\", it means that:\n\nUse node affinity labels based on the node group name when creating Compute Engine instances in order to host each workload on the correct node group.\n\nIf you choose the option \"D\" , it is talking based on node name: \"Use node affinity labels based on the node name\", however, you did not create the labels based on node name, you have created the labels based on node group name.","comment_id":"480583","comments":[{"content":"You create only 1 node group and add nodes in this group, so adding labels based on node group will not work","upvote_count":"2","poster":"Bert_77","comment_id":"497707","timestamp":"1639052520.0"}],"upvote_count":"3"},{"timestamp":"1635665460.0","comment_id":"470548","upvote_count":"3","poster":"MaxNRG","content":"D, workloads from different clients must also be separated!"},{"timestamp":"1634212200.0","poster":"[Removed]","upvote_count":"8","comment_id":"461995","content":"I can 100% say the answer is D. I finally went and just created one in my GCP console, and The goal is to segment the traffic for each client to a single node. There is only 1 node group, so separating it by node groups is not the right direction. I created a Sole Tenant Template and then created a Sole Tenant Node Group of 2 nodes. All nodes get the same affinity labels as assigned by the template. \n\nThen I jumped over to the VM screen, and near the bottom is the Sole-Tenancy section. It is a key:operator:value style. and I used the UI and selected a specific node and this is what it generated: \n\ncompute.googleapis.com/node-name:IN:node-group-1-gv42\n\nThus assigning the VM to a specific Node. So the answer is D."},{"timestamp":"1631377140.0","comment_id":"443053","upvote_count":"4","content":"D is the correct answer","poster":"ACE_ASPIRE"},{"timestamp":"1631296440.0","content":"You created a sole-tenant node group and added a node for each client; one node for each client hence it option D i.e affinity at node level and not node group.","comment_id":"442636","poster":"Sarguna","upvote_count":"3"},{"timestamp":"1630997700.0","comment_id":"440757","poster":"Manh","content":"it's C .\nAfter creating a node group based on a previously created node template, you can provision individual VMs on a sole-tenant node group.\nhttps://cloud.google.com/compute/docs/nodes/provisioning-sole-tenant-vms#provision_a_sole-tenant_vm","upvote_count":"3"},{"poster":"victory108","comment_id":"433970","content":"C. Use node affinity labels based on the node group name when creating Compute Engine instances in order to host each workload on the correct node group.","timestamp":"1630162320.0","upvote_count":"2"},{"comment_id":"431456","poster":"vladik820","upvote_count":"2","content":"C. Use node affinity labels based on the node group name when creating Compute Engine instances in order to host each workload on the correct node group","timestamp":"1629895260.0","comments":[{"upvote_count":"1","timestamp":"1631536680.0","content":"Sorry. D is ok. https://cloud.google.com/compute/docs/nodes/sole-tenant-nodes#default_affinity_labels","poster":"vladik820","comment_id":"443992"}]},{"comment_id":"431118","timestamp":"1629863940.0","poster":"fahad01hbti","upvote_count":"2","content":"C is ok."},{"comment_id":"430706","poster":"SweetieS","timestamp":"1629804360.0","content":"D is OK","comments":[{"timestamp":"1640617860.0","content":"D is the correct answer","comment_id":"510412","upvote_count":"1","poster":"hkere"}],"upvote_count":"4"}],"question_text":"You are working at an institution that processes medical data. You are migrating several workloads onto Google Cloud. Company policies require all workloads to run on physically separated hardware, and workloads from different clients must also be separated. You created a sole-tenant node group and added a node for each client. You need to deploy the workloads on these dedicated hosts. What should you do?"}],"exam":{"isImplemented":true,"id":4,"isBeta":false,"lastUpdated":"11 Apr 2025","isMCOnly":false,"numberOfQuestions":279,"name":"Professional Cloud Architect","provider":"Google"},"currentPage":13},"__N_SSP":true}