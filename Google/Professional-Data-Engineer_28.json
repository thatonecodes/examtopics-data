{"pageProps":{"questions":[{"id":"XZUq0jMtXuS3b9Lpzz1Q","isMC":true,"topic":"1","unix_timestamp":1703925960,"answer_description":"","timestamp":"2023-12-30 09:46:00","answer":"B","question_id":136,"answers_community":["B (100%)"],"answer_images":[],"discussion":[{"upvote_count":"8","timestamp":"1720086600.0","poster":"raaad","comment_id":"1113636","content":"Selected Answer: B\n- Datastream is a serverless change data capture and replication service, which can be used to replicate data changes from MySQL to BigQuery. \n- Using Cloud Interconnect provides a private, secure connection between your on-premises environment and Google Cloud ==> This method ensures that data doesn't go through the public internet and is a recommended approach for secure, large-scale data migrations. \n- Setting up private connectivity with Datastream allows for secure and direct data transfer."},{"content":"Selected Answer: B\nhttps://cloud.google.com/datastream/docs/network-connectivity-options","comment_id":"1213430","upvote_count":"2","timestamp":"1731961740.0","poster":"josech"},{"upvote_count":"1","poster":"datapassionate","comment_id":"1123288","content":"Selected Answer: B\nDatastream is a seamless replication from relational databases directly to BigQuery. The source database can be hosted on-premises, on Google Cloud services such as Cloud SQL or Bare Metal Solution for Oracle, or anywhere else on any cloud.\nhttps://cloud.google.com/datastream-for-bigquery#benefits","timestamp":"1721035980.0","comments":[{"upvote_count":"1","poster":"datapassionate","comment_id":"1123291","content":"It is required that the data ingestion into BigQuery is done securely and does not go through the public internet. It can be done by Interconnect.","timestamp":"1721036100.0"}]},{"poster":"e70ea9e","timestamp":"1719729960.0","comment_id":"1109547","content":"Selected Answer: B\nSecure Private Connection:\n\nCloud Interconnect establishes a direct, private connection between your on-premises network and Google Cloud, bypassing the public internet and ensuring data confidentiality.\nDatastream Integration:\n\nDatastream seamlessly replicates data from your MySQL database to BigQuery, handling the complexities of data transfer and synchronization.","upvote_count":"2"}],"url":"https://www.examtopics.com/discussions/google/view/129867-exam-professional-data-engineer-topic-1-question-220/","question_images":[],"answer_ET":"B","choices":{"D":"Use Datastream to replicate data from your on-premises MySQL database to BigQuery. Gather Datastream public IP addresses of the Google Cloud region that will be used to set up the stream. Add those IP addresses to the firewall allowlist of your on-premises data center. Use IP Allowlisting as the connectivity method and Server-only as the encryption type when setting up the connection profile in Datastream.","C":"Use Datastream to replicate data from your on-premises MySQL database to BigQuery. Use Forward-SSH tunnel as the connectivity method to establish a secure tunnel between Datastream and your on-premises MySQL database through a tunnel server in your on-premises data center. Use None as the encryption type when setting up the connection profile in Datastream.","B":"Use Datastream to replicate data from your on-premises MySQL database to BigQuery. Set up Cloud Interconnect between your on-premises data center and Google Cloud. Use Private connectivity as the connectivity method and allocate an IP address range within your VPC network to the Datastream connectivity configuration. Use Server-only as the encryption type when setting up the connection profile in Datastream.","A":"Update your existing on-premises ETL tool to write to BigQuery by using the BigQuery Open Database Connectivity (ODBC) driver. Set up the proxy parameter in the simba.googlebigqueryodbc.ini file to point to your data centerâ€™s NAT gateway."},"exam_id":11,"question_text":"You are migrating your on-premises data warehouse to BigQuery. One of the upstream data sources resides on a MySQL. database that runs in your on-premises data center with no public IP addresses. You want to ensure that the data ingestion into BigQuery is done securely and does not go through the public internet. What should you do?"},{"id":"6AbDsi8VFanFqYnd9bIM","choices":{"D":"Use the BigQuery Omni functionality and BigLake tables to query files in Azure and AWS.","B":"Create a Dataflow pipeline to ingest files from Azure and AWS to BigQuery.","C":"Load files from AWS and Azure to Cloud Storage with Cloud Shell gsutil rsync arguments.","A":"Use BigQuery Data Transfer Service to load files from Azure and AWS into BigQuery."},"answer":"D","url":"https://www.examtopics.com/discussions/google/view/129868-exam-professional-data-engineer-topic-1-question-221/","question_text":"You store and analyze your relational data in BigQuery on Google Cloud with all data that resides in US regions. You also have a variety of object stores across Microsoft Azure and Amazon Web Services (AWS), also in US regions. You want to query all your data in BigQuery daily with as little movement of data as possible. What should you do?","answers_community":["D (100%)"],"timestamp":"2023-12-30 09:47:00","answer_ET":"D","exam_id":11,"unix_timestamp":1703926020,"isMC":true,"question_images":[],"topic":"1","answer_description":"","question_id":137,"answer_images":[],"discussion":[{"timestamp":"1720087140.0","content":"Selected Answer: D\n- BigQuery Omni allows us to analyze data stored across Google Cloud, AWS, and Azure directly from BigQuery without having to move or copy the data. \n- It extends BigQuery's data analysis capabilities to other clouds, enabling cross-cloud analytics.","upvote_count":"7","comment_id":"1113641","poster":"raaad"},{"content":"Selected Answer: D\nDirect Querying:\n\nBigQuery Omni allows you to query data in Azure and AWS object stores directly without physically moving it to BigQuery, reducing data transfer costs and delays.\nBigLake Tables:\n\nProvide a unified view of both BigQuery tables and external object storage files, enabling seamless querying across multi-cloud data.","timestamp":"1719730020.0","poster":"e70ea9e","comment_id":"1109549","upvote_count":"6"},{"timestamp":"1731961920.0","comment_id":"1213432","upvote_count":"2","content":"https://cloud.google.com/blog/products/data-analytics/introducing-bigquery-omni\nhttps://cloud.google.com/bigquery/docs/omni-introduction","poster":"josech"},{"comment_id":"1159914","poster":"Ramon98","content":"Option A, B, and C all involve moving data, which is described as something that shouldn't happen.","timestamp":"1724681220.0","upvote_count":"2"},{"content":"Selected Answer: D\nBigQuery Omni","timestamp":"1723889040.0","upvote_count":"2","comment_id":"1152522","poster":"JyoGCP"}]},{"id":"ZTqUA5ejIhgUSBzCyTpz","question_images":[],"answers_community":["D (100%)"],"exam_id":11,"answer_images":[],"question_id":138,"answer_description":"","answer_ET":"D","discussion":[{"content":"Selected Answer: D\n- Dataprep is a serverless, no-code data preparation tool that allows users to visually explore, cleanse, and prepare data for analysis. \n- It's designed for business analysts, data scientists, and others who want to work with data without writing code. \n- Dataprep can directly access and transform data in Cloud Storage, making it a suitable choice for a team that prefers a low-code, user-friendly solution.","comment_id":"1113642","poster":"raaad","timestamp":"1720087320.0","upvote_count":"13"},{"poster":"JyoGCP","upvote_count":"1","comment_id":"1152530","content":"Selected Answer: D\nDataprep","timestamp":"1723889580.0"},{"timestamp":"1721191380.0","content":"Selected Answer: D\nGoes without say","upvote_count":"1","comment_id":"1124751","poster":"JimmyBK"},{"content":"Selected Answer: D\nOption D - Low code and efficient way to explore and prep data","poster":"Matt_108","upvote_count":"1","comment_id":"1121517","timestamp":"1720859940.0"},{"upvote_count":"1","content":"why you message wrong answers\ncorrect is C","comments":[{"comment_id":"1121518","content":"The \"Reveal Answer\" button contains 90% of the time an incorrect answer. You should always check the community and the discussion during studying :)","poster":"Matt_108","upvote_count":"2","timestamp":"1720860000.0"}],"timestamp":"1720354140.0","poster":"Alex3551","comment_id":"1115891"},{"content":"Selected Answer: D\nLow-Code Interface:\n\nOffers a visual, drag-and-drop interface that empowers users with varying technical skills to cleanse and explore data without extensive coding, aligning with the low-code requirement.\nData Cleaning and Validation:\n\nProvides built-in tools for data profiling, cleaning, transformation, and validation, ensuring data quality and accuracy before model training.\nDirect Cloud Storage Access:\n\nConnects directly to Cloud Storage, allowing users to work with data in place without additional data movement or storage costs, optimizing efficiency.","comment_id":"1109550","timestamp":"1719730080.0","poster":"e70ea9e","upvote_count":"4"}],"timestamp":"2023-12-30 09:48:00","isMC":true,"choices":{"B":"Create an external table in BigQuery and use SQL to transform the data as necessary. Provide the data science team access to the external tables to explore the raw data.","A":"Provide the data science team access to Dataflow to create a pipeline to prepare and validate the raw data and load data into BigQuery for data exploration.","C":"Load the data into BigQuery and use SQL to transform the data as necessary. Provide the data science team access to staging tables to explore the raw data.","D":"Provide the data science team access to Dataprep to prepare, validate, and explore the data within Cloud Storage."},"topic":"1","unix_timestamp":1703926080,"url":"https://www.examtopics.com/discussions/google/view/129869-exam-professional-data-engineer-topic-1-question-222/","question_text":"You have a variety of files in Cloud Storage that your data science team wants to use in their models. Currently, users do not have a method to explore, cleanse, and validate the data in Cloud Storage. You are looking for a low code solution that can be used by your data science team to quickly cleanse and explore data within Cloud Storage. What should you do?","answer":"D"},{"id":"5RltioiaaySM4wL33YpM","topic":"1","exam_id":11,"answer_images":[],"answer_description":"","answer":"C","question_id":139,"choices":{"A":"Build BigQuery user-defined functions (UDFs).","B":"Create Dataplex data quality tasks.","D":"Write a Spark-based stored procedure.","C":"Build Dataform assertions into your code."},"isMC":true,"question_text":"You are building an ELT solution in BigQuery by using Dataform. You need to perform uniqueness and null value checks on your final tables. What should you do to efficiently integrate these checks into your pipeline?","unix_timestamp":1703926140,"url":"https://www.examtopics.com/discussions/google/view/129870-exam-professional-data-engineer-topic-1-question-223/","answers_community":["C (100%)"],"answer_ET":"C","timestamp":"2023-12-30 09:49:00","question_images":[],"discussion":[{"content":"Selected Answer: C\n- Dataform provides a feature called \"assertions,\" which are essentially SQL-based tests that you can define to verify the quality of your data. \n- Assertions in Dataform are a built-in way to perform data quality checks, including checking for uniqueness and null values in your tables.","timestamp":"1720087440.0","upvote_count":"7","comment_id":"1113645","poster":"raaad"},{"poster":"JyoGCP","timestamp":"1723889640.0","upvote_count":"2","comment_id":"1152532","content":"Selected Answer: C\nhttps://docs.dataform.co/guides/assertions"},{"poster":"tibuenoc","timestamp":"1721292180.0","content":"Selected Answer: C\nhttps://cloud.google.com/dataform/docs/assertions","comment_id":"1125744","upvote_count":"4"},{"comment_id":"1115900","poster":"Alex3551","upvote_count":"1","timestamp":"1720354500.0","content":"Selected Answer: C\nAgree with C"},{"upvote_count":"1","timestamp":"1720354440.0","comment_id":"1115899","content":"agree with C","poster":"Alex3551"},{"content":"Selected Answer: C\nNative Integration:\n\nDataform assertions are designed specifically for data quality checks within Dataform pipelines, ensuring seamless integration and compatibility.\nThey leverage Dataform's execution model and configuration, aligning with the existing workflow.\nDeclarative Syntax:\n\nAssertions are defined using a simple, declarative syntax within Dataform code, making them easy to write and understand, even for users with less SQL expertise.","comment_id":"1109551","timestamp":"1719730140.0","upvote_count":"3","poster":"e70ea9e"}]},{"id":"mVGjIBwo6LGoNwx43bsA","topic":"1","answer_images":[],"exam_id":11,"question_images":[],"timestamp":"2023-12-30 09:50:00","answer_ET":"G","choices":{"C":"Messages in your Dataflow job are processed in less than 30 seconds, but your job cannot keep up with the backlog in the Pub/Sub subscription. Optimize your job or increase the number of workers to fix this.","D":"The web server is not pushing messages fast enough to Pub/Sub. Work with the web server team to fix this.","B":"Messages in your Dataflow job are taking more than 30 seconds to process. Optimize your job or increase the number of workers to fix this.","A":"The advertising department is causing delays when consuming the messages. Work with the advertising department to fix this."},"question_text":"A web server sends click events to a Pub/Sub topic as messages. The web server includes an eventTimestamp attribute in the messages, which is the time when the click occurred. You have a Dataflow streaming job that reads from this Pub/Sub topic through a subscription, applies some transformations, and writes the result to another Pub/Sub topic for use by the advertising department. The advertising department needs to receive each message within 30 seconds of the corresponding click occurrence, but they report receiving the messages late. Your Dataflow job's system lag is about 5 seconds, and the data freshness is about 40 seconds. Inspecting a few messages show no more than 1 second lag between their eventTimestamp and publishTime. What is the problem and what should you do?","answers_community":["G (92%)","8%"],"question_id":140,"discussion":[{"upvote_count":"11","timestamp":"1703926200.0","comment_id":"1109552","content":"Selected Answer: G\nSystem Lag vs. Data Freshness: System lag is low (5 seconds), indicating that individual messages are processed quickly. However, data freshness is high (40 seconds), suggesting a backlog in the pipeline.\nNot Advertising's Fault: The issue is upstream of their consumption, as they're already receiving delayed messages.\nNot Web Server's Fault: The lag between eventTimestamp and publishTime is minimal (1 second), meaning the server is publishing messages promptly.","poster":"e70ea9e"},{"content":"Selected Answer: G\n- It suggest a backlog problem. \n- It indicates that while individual messages might be processed quickly once they're handled, the job overall cannot keep up with the rate of incoming messages, causing a delay in processing the backlog.","poster":"raaad","upvote_count":"5","timestamp":"1704381120.0","comment_id":"1113796","comments":[{"poster":"datapassionate","content":"Why not B than?","upvote_count":"2","comment_id":"1123304","comments":[{"upvote_count":"2","comment_id":"1148637","content":"I guess that's because it says in the text that \"Your Dataflow job's system lag is about 5 seconds\".","poster":"RenePetersen","timestamp":"1707771060.0"}],"timestamp":"1705319460.0"}]},{"content":"Selected Answer: G\nAnswer is C lol","timestamp":"1742664180.0","comment_id":"1401985","poster":"desertlotus1211","upvote_count":"1"},{"poster":"4a8ffd7","content":"Selected Answer: B\nI don't know why you guys got the processing time is less than 30 sec. But I would consider the processing time with 40(freshness) - 5(system lag) = 35 sec. Even minus the publish time of Pub/sub which is less than 1 sec. The processing time still larger than 30 sec. I believe inspecting a few messages show no more than 1 sec lag is about pub/sub processing time. Not inspecting a few messages for dataflow. So I would choose B.","comment_id":"1280679","timestamp":"1725847020.0","upvote_count":"2"},{"timestamp":"1708172100.0","poster":"JyoGCP","upvote_count":"2","comment_id":"1152534","content":"Selected Answer: G\nOption C"},{"upvote_count":"4","comment_id":"1121520","timestamp":"1705142760.0","content":"Selected Answer: G\nOption C - low system lag (which identifies fast processing) but high data freshness (which identifies that the messages sit in the backlog a lot)","poster":"Matt_108"},{"timestamp":"1704637020.0","poster":"Alex3551","upvote_count":"1","comment_id":"1115901","content":"Selected Answer: G\nagree correct is C"}],"isMC":true,"url":"https://www.examtopics.com/discussions/google/view/129871-exam-professional-data-engineer-topic-1-question-224/","answer_description":"","unix_timestamp":1703926200,"answer":"G"}],"exam":{"isMCOnly":true,"isBeta":false,"lastUpdated":"11 Apr 2025","isImplemented":true,"provider":"Google","name":"Professional Data Engineer","id":11,"numberOfQuestions":319},"currentPage":28},"__N_SSP":true}