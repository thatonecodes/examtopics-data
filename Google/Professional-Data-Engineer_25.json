{"pageProps":{"questions":[{"id":"lfj23VgBuXbLOGN85u4T","answer_images":[],"question_id":121,"question_text":"You are collecting IoT sensor data from millions of devices across the world and storing the data in BigQuery. Your access pattern is based on recent data, filtered by location_id and device_version with the following query:\n\n//IMG//\n\n\nYou want to optimize your queries for cost and performance. How should you structure your data?","answers_community":["B (100%)"],"choices":{"A":"Partition table data by create_date, location_id, and device_version.","D":"Cluster table data by create_date, partition by location_id, and device_version.","B":"Partition table data by create_date, cluster table data by location_id, and device_version.","C":"Cluster table data by create_date, location_id, and device_version."},"exam_id":11,"question_images":["https://img.examtopics.com/professional-data-engineer/image1.png"],"timestamp":"2023-12-30 09:29:00","answer":"B","isMC":true,"unix_timestamp":1703924940,"answer_ET":"B","url":"https://www.examtopics.com/discussions/google/view/129854-exam-professional-data-engineer-topic-1-question-207/","discussion":[{"upvote_count":"1","poster":"JyoGCP","content":"Selected Answer: B\nB. Partition table data by create_date, cluster table data by location_id, and device_version.","comment_id":"1151090","timestamp":"1723728780.0"},{"comment_id":"1123189","content":"Selected Answer: B\nB. Partition table data by create_date, cluster table data by location_id, and device_version.","upvote_count":"1","timestamp":"1721025900.0","poster":"datapassionate"},{"content":"Selected Answer: B\nB: Partitioning makes date-related querying efficient, clustering will keep relevant data close together and optimize the performance of filters for the cluster columns","poster":"Matt_108","upvote_count":"2","timestamp":"1720851900.0","comment_id":"1121414"},{"upvote_count":"3","timestamp":"1720339860.0","poster":"MaxNRG","comment_id":"1115707","content":"Selected Answer: B\n1. Partitioning the data by create_date will allow BigQuery to prune partitions that are not relevant to the query by date.\n2. Clustering the data by location_id and device_version within each partition will keep related data close together and optimize the performance of filters on those columns. \nThis provides both the pruning benefits of partitioning and locality benefits of clustering for filters on multiple columns.\nThe query provided indicates that the access pattern is primarily based on the most recent data (within the last 7 days), filtered by location_id and device_version. Given this pattern, you would want to optimize your table structure in such a way that queries scanning through the data will process the least amount of data possible to reduce costs and improve performance."},{"content":"Selected Answer: B\nOnly correct answer is B, you can only partition by one field, and you can only cluster on partitioned tables","timestamp":"1720335480.0","upvote_count":"1","comment_id":"1115658","poster":"Smakyel79"},{"upvote_count":"2","content":"Selected Answer: B\nAnswer is B:\n- Partitioning the table by create_date allows us to efficiently query data based on time, which is common in access patterns that prioritize recent data. \n- Clustering the table by location_id and device_version further organizes the data within each partition, making queries filtered by these columns more efficient and cost-effective.","poster":"raaad","timestamp":"1719938340.0","comment_id":"1112145"},{"content":"Selected Answer: B\nThe best answer is B. Partition table data by create_date, cluster table data by location_id, and device_version.\n\nHere's a breakdown of why this structure is optimal:\n\nPartitioning by create_date:\n\nAligns with query pattern: Filters for recent data based on create_date, so partitioning by this column allows BigQuery to quickly narrow down the data to scan, reducing query costs and improving performance.\nManages data growth: Partitioning effectively segments data by date, making it easier to manage large datasets and optimize storage costs.\nClustering by location_id and device_version:\n\nEnhances filtering: Frequently filtering by location_id and device_version, clustering physically co-locates related data within partitions, further reducing scan time and improving performance.","upvote_count":"2","poster":"e70ea9e","timestamp":"1719728940.0","comment_id":"1109525"}],"answer_description":"","topic":"1"},{"id":"NIrhbBopu31i5Gu49KhC","exam_id":11,"answer":"D","isMC":true,"answer_images":[],"question_images":["https://img.examtopics.com/professional-data-engineer/image2.png"],"unix_timestamp":1703925000,"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/129855-exam-professional-data-engineer-topic-1-question-208/","answers_community":["D (82%)","C (18%)"],"timestamp":"2023-12-30 09:30:00","question_text":"A live TV show asks viewers to cast votes using their mobile phones. The event generates a large volume of data during a 3-minute period. You are in charge of the \"Voting infrastructure\" and must ensure that the platform can handle the load and that all votes are processed. You must display partial results while voting is open. After voting closes, you need to count the votes exactly once while optimizing cost. What should you do?\n\n//IMG//","question_id":122,"choices":{"D":"Write votes to a Pub/Sub topic and load into both Bigtable and BigQuery via a Dataflow pipeline. Query Bigtable for real-time results and BigQuery for later analysis. Shut down the Bigtable instance when voting concludes.","A":"Create a Memorystore instance with a high availability (HA) configuration.","C":"Write votes to a Pub/Sub topic and have Cloud Functions subscribe to it and write votes to BigQuery.","B":"Create a Cloud SQL for PostgreSQL database with high availability (HA) configuration and multiple read replicas."},"answer_ET":"D","discussion":[{"poster":"MaxNRG","timestamp":"1720340220.0","upvote_count":"7","content":"Selected Answer: D\nSince cost optimization and minimal latency are key requirements, option D is likely the best choice to meet all the needs:\n\nThe key reasons option D works well:\n\nUsing Pub/Sub to ingest votes provides scalable, reliable transport.\n\nLoading into Bigtable and BigQuery provides both:\n\nLow latency reads from Bigtable for real-time results.\nCost effective storage in BigQuery for longer term analysis.\nShutting down Bigtable after voting concludes reduces costs.\n\nBigQuery remains available for cost-optimized storage and analysis.\n\nSo you are correct that option D combines the best of real-time performance for queries using Bigtable, with cost-optimized storage in BigQuery.\n\nThe only additional consideration may be if 3 minutes of Bigtable usage still incurs higher charges than ingesting directly into BigQuery. But for minimizing latency while optimizing cost, option D is likely the right architectural choice given the requirements.","comment_id":"1115710"},{"timestamp":"1735504140.0","poster":"f74ca0c","content":"Selected Answer: C\nModern Capabilities: BigQueryâ€™s advancements make it suitable for both real-time and historical querying. \nCost Efficiency: No need to spin up and shut down a Bigtable instance.\nSimplified Workflow: Real-time and post-event data are stored in the same system, reducing the need to synchronize or transfer data between systems.","upvote_count":"2","comment_id":"1333708"},{"timestamp":"1723728840.0","upvote_count":"1","content":"Selected Answer: D\nD. Write votes to a Pub/Sub topic and load into both Bigtable and BigQuery via a Dataflow pipeline. Query Bigtable for real-time results and BigQuery for later analysis. Shut down the Bigtable instance when voting concludes.","poster":"JyoGCP","comment_id":"1151093"},{"upvote_count":"1","timestamp":"1720851960.0","content":"Selected Answer: D\nD, i do agree with everything MaxNRG said.","poster":"Matt_108","comment_id":"1121416"},{"poster":"Smakyel79","content":"Selected Answer: C\nPub/Sub for sure, and Cloud Functions + BigQuery Streaming seems a good solution. Won't use BigTable as need at least 100GB of data (don't thing a voting system could arrive to that amount of data) and needs to \"heat\" to work right for >10 minutes... and would be $$$ over C solution","upvote_count":"1","timestamp":"1720335960.0","comment_id":"1115662"},{"timestamp":"1719939240.0","upvote_count":"2","comment_id":"1112159","content":"Selected Answer: D\nAnswer is D:\n- Google Cloud Pub/Sub can manage the high-volume data ingestion. \n- Google Cloud Dataflow can efficiently process and route data to both Bigtable and BigQuery.\n- Bigtable is excellent for handling high-throughput writes and reads, making it suitable for real-time vote tallying. \n- BigQuery is ideal for exact vote counting and deeper analysis once voting concludes.","poster":"raaad"},{"timestamp":"1719729000.0","comment_id":"1109529","poster":"e70ea9e","upvote_count":"3","content":"Selected Answer: D\nHandling High-Volume Data Ingestion:\n\nPub/Sub: Decouples vote collection from processing, ensuring scalability and resilience under high load.\nDataflow: Efficiently ingests and processes large data streams, scaling as needed.\nReal-Time Results with Exactly-Once Processing:\n\nBigtable: Optimized for low-latency, high-throughput reads and writes, ideal for real-time partial results.\nExactly-Once Semantics: Dataflow guarantees each vote is processed only once, ensuring accurate counts.\nCost Optimization:\n\nTemporary Bigtable Instance: Running Bigtable only during voting minimizes costs.\nBigQuery Storage: Cost-effective for long-term storage and analysis."}],"topic":"1"},{"id":"jOblvw6Rlszyv2NHUm0N","question_id":123,"question_text":"A shipping company has live package-tracking data that is sent to an Apache Kafka stream in real time. This is then loaded into BigQuery. Analysts in your company want to query the tracking data in BigQuery to analyze geospatial trends in the lifecycle of a package. The table was originally created with ingest-date partitioning. Over time, the query processing time has increased. You need to copy all the data to a new clustered table. What should you do?","unix_timestamp":1703925060,"url":"https://www.examtopics.com/discussions/google/view/129856-exam-professional-data-engineer-topic-1-question-209/","question_images":[],"answer_description":"","answers_community":["B (100%)"],"timestamp":"2023-12-30 09:31:00","answer_ET":"B","answer":"B","answer_images":[],"topic":"1","discussion":[{"timestamp":"1734028500.0","poster":"apoio.certificacoes.closer","comment_id":"1325800","upvote_count":"1","content":"Selected Answer: B\nYou don't need to recreate a table to cluster it, contrary to partitioning, where you have to create a new table with the old data (migration)\n\n> If you alter an existing non-clustered table to be clustered, the existing data is not automatically clustered. Only new data that's stored using the clustered columns is subject to automatic reclustering. \nhttps://cloud.google.com/bigquery/docs/clustered-tables#limitations"},{"content":"Selected Answer: B\nB. Implement clustering in BigQuery on the package-tracking ID column.","timestamp":"1723729020.0","comment_id":"1151099","upvote_count":"1","poster":"JyoGCP"},{"content":"Selected Answer: B\nB. Implement clustering in BigQuery on the package-tracking ID column.","poster":"datapassionate","upvote_count":"1","timestamp":"1721026500.0","comment_id":"1123196"},{"content":"Selected Answer: B\nDefinitely B","timestamp":"1720852080.0","poster":"Matt_108","comment_id":"1121417","upvote_count":"1"},{"timestamp":"1720341000.0","comment_id":"1115719","poster":"MaxNRG","content":"Selected Answer: B\nThis looks like Question #166\n\nOption B, implementing clustering in BigQuery on the package-tracking ID column, seems the most appropriate. It directly addresses the query slowdown issue by reorganizing the data in a way that aligns with the analysts' query patterns, leading to more efficient and faster query execution.","upvote_count":"3"},{"upvote_count":"3","poster":"raaad","content":"Selected Answer: B\nAnswer is B","comment_id":"1112160","timestamp":"1719939720.0"},{"comment_id":"1109531","content":"Selected Answer: B\nQuery Focus: Analysts are interested in geospatial trends within individual package lifecycles. Clustering by package-tracking ID physically co-locates related data, significantly improving query performance for these analyses.\n\nAddressing Slow Queries: Clustering addresses the query slowdown issue by optimizing data organization for the specific query patterns.\n\nPartitioning vs. Clustering:\n\nPartitioning: Divides data into segments based on a column's values, primarily for managing large datasets and optimizing query costs.\nClustering: Organizes data within partitions for faster querying based on specific columns.","poster":"e70ea9e","upvote_count":"4","timestamp":"1719729060.0"}],"choices":{"C":"Implement clustering in BigQuery on the ingest date column.","D":"Tier older data onto Cloud Storage files and create a BigQuery table using Cloud Storage as an external data source.","B":"Implement clustering in BigQuery on the package-tracking ID column.","A":"Re-create the table using data partitioning on the package delivery date."},"exam_id":11,"isMC":true},{"id":"o4mCZVUMrGEZhXPcvp7d","answer":"A","url":"https://www.examtopics.com/discussions/google/view/16929-exam-professional-data-engineer-topic-1-question-21/","answer_description":"","question_id":124,"answer_images":[],"answer_ET":"A","choices":{"D":"Maintain a database table to store the hash value and other metadata for each data entry.","B":"Compute the hash value of each data entry, and compare it with all historical data.","A":"Assign global unique identifiers (GUID) to each data entry.","C":"Store each data entry as the primary key in a separate database and apply an index."},"question_text":"Your company uses a proprietary system to send inventory data every 6 hours to a data ingestion service in the cloud. Transmitted data includes a payload of several fields and the timestamp of the transmission. If there are any concerns about a transmission, the system re-transmits the data. How should you deduplicate the data most efficiency?","question_images":[],"topic":"1","isMC":true,"unix_timestamp":1584544380,"timestamp":"2020-03-18 16:13:00","exam_id":11,"discussion":[{"poster":"dg63","comment_id":"126113","comments":[{"content":"If you add a unique ID aren't you by definition not getting a duplicate record. Honestly I hate all these answers.","upvote_count":"4","poster":"emmylou","comment_id":"1024081","comments":[{"poster":"billalltf","comment_id":"1212409","timestamp":"1715861040.0","content":"You can add a function or condition that verifies if the global unique id already exists or just do a deduplication later","upvote_count":"1"}],"timestamp":"1696351500.0"},{"upvote_count":"13","comments":[{"upvote_count":"12","comment_id":"392857","content":"A - In D, same message with different timestamp will have different hash, though the message content is the same.","comments":[{"content":"agreed, the key here is \"payload of several fields and the timestamp\"","timestamp":"1642700880.0","comment_id":"528632","comments":[{"content":"\"payload of several fields and the timestamp of the transmission\"","comment_id":"528633","upvote_count":"2","timestamp":"1642700940.0","comments":[{"poster":"BigDataBB","comment_id":"537740","content":"Hi Max, I also think that the hash value would be worng because the timestamp is part of payload and is not written that the hash value is generated without the ts; but it also not written if GUID is linked or not with sending. Often this is a point where the answer is vague. Because don't specify if the GUID is related to the data or to the send.","upvote_count":"1","timestamp":"1643706180.0"}],"poster":"MaxNRG"}],"upvote_count":"2","poster":"MaxNRG"},{"timestamp":"1626393420.0","poster":"omakin","content":"Strong Answer is A - in another question on the gcp sample questions: the correct answer to that particular question was \"You are building a new real-time data warehouse for your company and will use BigQuery streaming inserts. There is no guarantee that data will only be sent in once but you do have a unique ID for each row of data and an event timestamp. You want to ensure that duplicates are not included while interactively querying data. Which query type should you use?\"\nThis means you need a \"uniqueid\" and timestamps to properly dedupe a data.","comment_id":"407482","upvote_count":"8","comments":[{"content":"U need a uniqueid but in this scenario, there is none. So u have to calculate by hashing w/ some of the fields in the dataset. \n\nA is assigning guid in processing side will not solve the issue. Cause u will assign diff. ids...","poster":"Tanzu","timestamp":"1642928700.0","upvote_count":"1","comments":[{"comment_id":"786609","timestamp":"1674570960.0","poster":"cetanx","upvote_count":"5","content":"Answer - D\nKey statement is \"Transmitted data includes a payload of several fields and the timestamp of the transmission.\" \n\nSo the timestamp is appended to message while sending, in other words that field is subject to change if message is retransmitted. However, adding a GUID doesn't help much because if message is transmitted twice you will have different GUID for both messages but they will be the same/duplicate data.\n\nYou can simply calculate a hash based on not all data but from a select of columns (with the payload of several fields AND definitely by excluding the timestamp). By doing so, you can assure a different hash for each message."}],"comment_id":"530397"}]}],"poster":"ralf_cc","timestamp":"1624879260.0"},{"upvote_count":"5","comment_id":"502054","poster":"MarcoDipa","content":"Answer is D. Using Hash values we can remove duplicate values from a database. Hash values will be same for duplicate data and thus can be easily rejected. Obviously you won't check hash for timestmp.\nD is better thatn B because maintaning a different table will reduce cost for hash computation for all historical data","comments":[{"upvote_count":"1","comment_id":"955216","poster":"Mathew106","content":"Why can't it be A, where the GUID is a hash value? Why do we need to store the hash with the metadata in a separate database to do the deduplication?","timestamp":"1689675180.0"}],"timestamp":"1639564500.0"}],"timestamp":"1603155960.0","comment_id":"202898","content":"If the goal is to ensure at least ONE of each pair of entries is inserted into the db, then how is assigning a GUID to each entry resolving the duplicates? Keep in mind if the 1st entry fails, then hopefully the 2nd (duplicate) is successful.","poster":"retax"}],"timestamp":"1593861840.0","content":"The best answer is \"A\". \nAnswer \"D\" is not as efficient or error-proof due to two reasons\n1. You need to calculate hash at sender as well as at receiver end to do the comparison. Waste of computing power.\n2. Even if we discount the computing power, we should note that the system is sending inventory information. Two messages sent at different can denote same inventory level (and thus have same hash). Adding sender time stamp to hash will defeat the purpose of using hash as now retried messages will have different timestamp and a different hash. \nif timestamp is used as message creation timestamp than that can also be used as a UUID.","upvote_count":"67"},{"poster":"[Removed]","comments":[{"timestamp":"1651040220.0","poster":"stefanop","comment_id":"592895","upvote_count":"2","content":"Hash values for same data will be the same, but in this case data contains also the timestamp","comments":[{"poster":"DGames","timestamp":"1670886780.0","comment_id":"743404","content":"While calculating Hash value we exclude the timestamp.","upvote_count":"1"}]}],"upvote_count":"24","comment_id":"68527","content":"Answer: D\nDescription: Using Hash values we can remove duplicate values from a database. Hashvalues will be same for duplicate data and thus can be easily rejected.","timestamp":"1585292820.0"},{"upvote_count":"1","timestamp":"1744287540.0","content":"Selected Answer: D\nA is incorrect. how can you find duplicates if you assign a unique id to every record? The answer is D.","comment_id":"1559574","poster":"fassil"},{"comment_id":"1426961","upvote_count":"1","content":"Selected Answer: D\nThe most efficient way to deduplicate your inventory data would be:\nD. Maintain a database table to store the hash value and other metadata for each data entry.\nThis approach is optimal because:\n\nIt creates a lightweight reference table that stores just the hash values and essential metadata (like timestamps) rather than the full payload data\nHash values can be quickly compared to identify duplicates without expensive full-data comparisons\nThe metadata can help with auditing and troubleshooting transmission issues\nThis solution scales well as your data volume grows\n\nOption A (using GUIDs) doesn't address the retransmission scenario well, as new GUIDs might be generated each time. Option B requires comparing against all historical data, which becomes increasingly inefficient over time. Option C creates unnecessary storage overhead by using entire data entries as primary keys when only a hash value is needed for comparison.","poster":"Mo5454545454","timestamp":"1743672840.0"},{"comment_id":"1398865","upvote_count":"2","poster":"Parandhaman_Margan","timestamp":"1742047740.0","content":"Selected Answer: D\nDeduplicate data with retransmissions. Use a database table with hash"},{"content":"Selected Answer: A\nmost obvious answer","poster":"Abizi","upvote_count":"1","comment_id":"1364868","timestamp":"1741086540.0"},{"timestamp":"1735128360.0","comment_id":"1331533","poster":"Rav761","content":"Selected Answer: D\nOption D: Maintain a database table to store the hash value and other metadata for each data entry.\n\nThis approach is efficient and scalable. By storing a computed hash value (as a compact representation of the data) along with metadata, deduplication can be performed by comparing new entries with the stored hashes. This minimizes storage requirements and improves lookup efficiency.","upvote_count":"1"},{"timestamp":"1718588220.0","comment_id":"1231624","content":"1. My original vote was 'B'. I chose it over 'D' because option 'D' does not explicitly say anything about how that table will be used for deduplication. In hindsight, explicit usage of table should not be given much weightage so after review and seeing other comments, I thought of 'D' as the correct answer.\n\n2. Now looking more clearly at option 'D' (and 'B' also), it's a little ambiguous of what keys will be used to create the hash. So, if you use the payload PLUS the timestamp, the hash is of no use. This is a little confusing\n\n3. Finally, although I never thought this is the right option, 'A' seems to be the correct option. The GUID is created at Data entry, NOT at the transmission stage. So, the GUID should be representative of the payload only and NOT the timestamp which will make it unique per payload, not per transmission of the same payload. So, in the end, I feel like 'A' is the correct choice.","upvote_count":"1","poster":"vbrege"},{"upvote_count":"1","timestamp":"1703144880.0","poster":"TVH_Data_Engineer","comment_id":"1102240","content":"Selected Answer: D\nTo deduplicate the data most efficiently, especially in a cloud environment where the data is sent periodically and re-transmissions can occur, the recommended approach would be:\n\nD. Maintain a database table to store the hash value and other metadata for each data entry.\n\nThis approach allows you to quickly check if an incoming data entry is a duplicate by comparing hash values, which is much faster than comparing all fields of a data entry. The metadata, which includes the timestamp and possibly other relevant information, can help resolve any ambiguities that may arise if the hash function ever produces collisions."},{"upvote_count":"3","comment_id":"1075610","content":"B. Compute the hash value of each data entry, and compare it with all historical data.\n\nExplanation:\n\nEfficiency: Hashing is a fast and efficient operation, and comparing hash values is generally quicker than comparing the entire payload or using other methods.\n\nSpace Efficiency: Storing hash values requires less storage space compared to storing entire payloads or using global unique identifiers (GUIDs).\n\nDeduplication: By computing the hash value of each data entry and comparing it with historical data, you can easily identify duplicate transmissions. If the hash value matches an existing one, it indicates that the payload is the same.","poster":"JustQ","timestamp":"1700498460.0"},{"poster":"steghe","comment_id":"1066201","content":"I though the answer was A 'cos it's more efficient. But I read the answer with more attention: GUID is given \"at each data entry\". It's not said that GUID was given from publisher. If GUID is given in data entry (subscriber), two equal messages can have different GUID.\n D is not complete 'cos it's not so precise about hash field that are used.\nI'm in doubt on this answer :-(","upvote_count":"2","timestamp":"1699513620.0","comments":[{"comment_id":"1176979","content":"Data entry means record, it is not an action. that means that each record will have a unique id. so assuming our sink will not accept duplicates based on a key, the GUID will work.","timestamp":"1710822240.0","upvote_count":"1","poster":"Lestrang"}]},{"content":"Selected Answer: A\nAnswer : A\n\"D\" is not as efficient or error-proof due to two reasons\n1. You need to calculate hash at sender as well as at receiver end to do the comparison. Waste of computing power.\n2. Even if we discount the computing power, we should note that the system is sending inventory information. Two messages sent at different can denote same inventory level (and thus have same hash). Adding sender time stamp to hash will defeat the purpose of using hash as now retried messages will have different timestamp and a different hash.\nif timestamp is used as message creation timestamp than that can also be used as a UUID.","comment_id":"1064146","timestamp":"1699294740.0","upvote_count":"1","poster":"rocky48"},{"upvote_count":"1","poster":"rtcpost","timestamp":"1697975820.0","content":"Selected Answer: D\nD. Maintain a database table to store the hash value and other metadata for each data entry.\n\nStoring a database table with hash values and metadata is an efficient way to deduplicate data. When new data is transmitted, you can calculate the hash of the payload and check whether it already exists in the database. This approach allows for efficient duplicate detection without the need to compare the new data with all historical data. It's a common and scalable technique used to ensure data consistency and avoid processing the same data multiple times.\n\nOptions A (assigning GUIDs to each data entry) and C (storing each data entry as the primary key) can work, but they might be less efficient than using hash values when dealing with a large volume of data. Option B (computing the hash value of each data entry and comparing it with all historical data) can be computationally expensive and slow, especially if there's a significant amount of historical data to compare against. Storing hash values in a table allows for fast and efficient deduplication.","comment_id":"1050523"},{"timestamp":"1691215740.0","comment_id":"972728","poster":"alihabib","upvote_count":"2","content":"Why not D ? Generate a Hash for payload entry and maintain the value as metadata. Do the validation check on Dataflow..... A GUID will generate 2 different entries for same payload entry, it will not tackle duplication check"},{"poster":"Hungry_guy","content":"Answer is B - although the time stamp is diff for each transmission - the hash value is computed for the payload, not for the timestamp - which is just an added field for transmission. So, has val remains the same for all transmissions of the same data - which is what we can use for comparision.\n\nSo, much more efficient to just directly compare the hash values with the historical data - to check and remove duplicates - instead of again wasting space storing stuff - in option D","comment_id":"972299","timestamp":"1691164260.0","upvote_count":"3"},{"timestamp":"1690376100.0","poster":"Mark_86","comment_id":"963735","upvote_count":"2","content":"Selected Answer: D\nThis question is formulated very badly.\nFrom the way that A is formulated, you would not deduplicate but rather the duplicates would have the same GUID.\nThen we have D, which is storing the information (assuming the hash is created without the timestamp). B is doing it right away. D only alludes to the actual deduplication. But it would be more efficient."},{"timestamp":"1682953200.0","poster":"boca_2022","upvote_count":"2","comments":[{"comment_id":"990353","upvote_count":"2","timestamp":"1692990660.0","poster":"FP77","content":"A is incorrect. how can you find duplicates if you assign a unique id to every record? The answer is either B or D. I first selected B, but reading through the answers D may be better."}],"content":"Selected Answer: A\nA is best choice. D doesn't make sense.","comment_id":"886410"},{"upvote_count":"1","comment_id":"882010","timestamp":"1682541420.0","content":"Selected Answer: D\nyou cannot deduplicate data adding a random guid, with guid row is distinct than others","poster":"Melampos"},{"timestamp":"1679079960.0","poster":"juliobs","comment_id":"842226","upvote_count":"4","content":"Hard question.\nIt's a *proprietary* system. Who guarantees we can even add a GUID?\nBut if you can, it's definitely more efficient than calculating hashes (ignoring timestamp)."},{"content":"Selected Answer: A\nAs Dg63 wrote.","comment_id":"825692","upvote_count":"2","poster":"tibuenoc","timestamp":"1677663360.0"},{"poster":"AshokPalle","content":"Just asked Chatgpt, it gave me option D","timestamp":"1677172080.0","upvote_count":"1","comment_id":"819453"},{"poster":"musumusu","content":"Answer B:\nOption A: GUIDs can deduplicate the data but is expensive and good for multiple data processing. \nOption B: Using hash function to authenticate the unique rows, this function can be applied directly in bigquery. \nOption D, is complex and more expensive. \n\n``\n`CREATE TEMP FUNCTION hashValue(input STRING) AS (\n CAST(FARM_FINGERPRINT(input) AS STRING)\n);\n``","comment_id":"819154","upvote_count":"1","timestamp":"1677154200.0"},{"comment_id":"814208","content":"Selected Answer: A\nA is prefereed way to generate unique identifier compared to hashing/indexing.","timestamp":"1676819460.0","upvote_count":"2","poster":"techtitan"},{"timestamp":"1676280300.0","content":"Best Answer = B\nanswer A, GUID will help in keeping all values unique but will not remove duplicates for transaction purpose. \nanswer D is not correct because maintaining external database is a good approach but it will not be efficient when there are large transactions that needs to be processed quickly and additional cost for database. I dont wanna pay for it. \nAnswer B is correct because: you just need to pass a check cloud function because i assume data has to be processed in real time, before loading the data to bigquery. \n```import haslib \n def compute_hash(data):\n hash_value = hashlib.sha256(data.encode('utf-8'))\n # detecting duplicates and ensure data integrity before storing in BQ\n return hash_value.hexdigest()\n```","comment_id":"807236","upvote_count":"3","poster":"hellobot"},{"comment_id":"802363","content":"D. Maintaining a database table to store the hash value and other metadata for each data entry would be the most efficient approach to deduplicate the data. This would allow you to compare the hash value of each new data entry with the values stored in the database, to quickly determine if it is a duplicate or not. Option A (Assigning GUID to each data entry) would be too time-consuming, Option B (computing the hash value and comparing it with all historical data) would not scale well with large amounts of data, and Option C (storing each data entry as the primary key in a separate database and applying an index) would not allow for efficient comparisons of the data.","timestamp":"1675879740.0","upvote_count":"1","poster":"samdhimal"},{"comment_id":"787573","content":"Selected Answer: B\nI would go for B. Seems to be most intuitive. If you want to know whether the input is already in the database, you need to compare it with the current content of the database. How ? by using hash","timestamp":"1674647820.0","upvote_count":"3","comments":[{"comment_id":"858856","poster":"SonicBoom10C9","upvote_count":"2","content":"It would have a different hash, as the timestamp would be different.","timestamp":"1680440640.0"}],"poster":"PolyMoe"},{"upvote_count":"4","content":"Selected Answer: A\nThe simplest option is A. And simple is is an advantage.","comment_id":"771649","poster":"Nirca","timestamp":"1673372760.0"},{"upvote_count":"1","poster":"captainbu","timestamp":"1673257020.0","content":"Answer D","comment_id":"770209"},{"timestamp":"1673094120.0","content":"Selected Answer: A\nI would vote on A, assuming that the re-transmission will submit the same GUID.\nD can be confused and can cause duplication or incorrect data on two scenarios:\n1. If the hash includes transmission timestamp, every data transmission will have different hash --> data re-transmission will have different hash --> duplicate\n2. If the hash data doesn't include transmission timestamp --> the inventory data on different time can have the same hash --> no data with the same value can be inserted to the DB!","comment_id":"768535","upvote_count":"5","poster":"korntewin"},{"timestamp":"1672244160.0","poster":"Jackalski","upvote_count":"2","comment_id":"760070","content":"Selected Answer: D\ngoal is to avoid duplicates in case resending the same information.\nAssuming using key-value store to handle duplicated values correctly - would failed on insert etc ( however not shared details here in this question)\ntricky question is what are definition of GUID versus hash.\ncalculated hash on data sent first time and resent again can be the same assuming timestamp of datapoints inslide payload are not changed. timestamp of transmission is another story and I dont assume it is consider as value here (assumption to resend).\nGUID usually means to use UUID function which is random - no option to validate uniqness \nif GUID is build on specific way - than it would require complex logic ...\nHash gives easy way to calculate it on full payload received\nQuestion was about most effective ...\n\nso I vote on D"},{"timestamp":"1672209540.0","upvote_count":"1","comment_id":"759450","poster":"PrashantGupta1616","content":"Selected Answer: D\nAnswer is D."},{"content":"Selected Answer: A\nInitially misinterpreted the answers.. Assumed all of them to be very different approaches.\n\nBut essentially the question is a comparison of different ways to generate unique identifiers for entries. Ways to generate unique identifiers:\n1. Hashing\n2. indexing\n3. GUID\n\nThere are storage and compute considerations that we have to make while choosing these methods.\n\nGUID does not require calculation again compared to hashing or indexing. However storage of GUID is heavy compared to the two.\n\nSince we are only concerned of efficiency, GUID is the fastest.\n\nWe also do not need to store as we can implement a deduplication when our window closes. We don't need to store the unique values in a database in this case, as aggregations will be performed after the windowing period ends.","timestamp":"1671544680.0","upvote_count":"3","poster":"jkhong","comment_id":"750963"},{"comment_id":"745907","timestamp":"1671098700.0","poster":"martarroyo","content":"Selected Answer: A\nAnswer \"D\" is not as efficient or error-proof due to two reasons\n1. You need to calculate hash at sender as well as at receiver end to do the comparison. Waste of computing power.\n2. Even if we discount the computing power, we should note that the system is sending inventory information. Two messages sent at different can denote same inventory level (and thus have same hash). Adding sender time stamp to hash will defeat the purpose of using hash as now retried messages will have different timestamp and a different hash.\nif timestamp is used as message creation timestamp than that can also be used as a UUID.","upvote_count":"2"},{"poster":"DGames","content":"Selected Answer: D\nAnswer is D- Calculating Hash value (exclude timestamp) which help to find duplicate data already processed and eliminate.","timestamp":"1670886900.0","comment_id":"743406","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: A\nGuid +timestamp","timestamp":"1669715820.0","comment_id":"730196","poster":"hauhau"},{"poster":"assU2","content":"Selected Answer: A\nI think this phrase \"company uses a proprietary system\" is the cause of such confusion. \nSo, everything is about - can you change the output of this proprietary system/pubsub input or not. If yes then answer A. And I think proprietary means that yes you can.","upvote_count":"2","timestamp":"1669386180.0","comment_id":"726839"},{"poster":"gudiking","content":"Selected Answer: A\nA - as dg63 explained","timestamp":"1668942360.0","upvote_count":"2","comment_id":"722556"},{"comment_id":"720965","poster":"ovokpus","timestamp":"1668728760.0","upvote_count":"2","content":"Selected Answer: D\nDifferent GUIDs will be assigned to duplicate entries as each entry is assigned a GUID. Hash values with timestamps will always be the same for duplicate values and that can easily be deduped with a query"},{"content":"Selected Answer: A\nThe questions asks most efficient way. We can simply assign unique keys to dedupe.","poster":"moonbro","upvote_count":"2","timestamp":"1668132240.0","comment_id":"715692"},{"content":"A is the appropriate answer to deduplicate the messages","upvote_count":"1","timestamp":"1666301100.0","poster":"kennyloo","comment_id":"700309"},{"timestamp":"1662792420.0","content":"Selected Answer: A\nMy answer is A. \nInventory data can often be naturally duplicate. Assigning a unique GUID at sender's end is ensuring that we can track a unique record reliably at the recieving end and if there are issues which causes same field to be sent twice, we can easily dedup using the GUID with lesser hassle.","poster":"TOXICcharlie","comment_id":"665164","upvote_count":"3"},{"upvote_count":"1","comment_id":"663654","timestamp":"1662643560.0","poster":"HarshKothari21","content":"Selected Answer: D\nI would pick D as well."},{"poster":"ducc","timestamp":"1661723100.0","content":"Selected Answer: D\nIt should be D.\nMaintain a database is the only way to deduplicate two messages with same metadata.","comment_id":"653170","upvote_count":"1"},{"comment_id":"650605","poster":"ducc","upvote_count":"1","timestamp":"1661233740.0","content":"Selected Answer: D\nD is correct\nYou must maintain a database, or how can you deduplicate the two like this\nID: 100, metadata1: duplicate, metadata2: duplicate2\nID: 101, metadata1: duplicate, metadata2: duplicate2"},{"comment_id":"648364","timestamp":"1660813740.0","poster":"yellow28","upvote_count":"2","content":"Your publisher when publishing messages to Pub/Sub can generate duplicates due to at-least-once publishing guarantees. Such duplicates are referred to as â€˜publisher generated duplicatesâ€™. \n\nPub/Sub automatically assigns a unique message_id to each message successfully published to a topic. Each message is considered successfully published by the publisher when Pub/Sub returns an acknowledgement to the publisher. Within a topic all messages have a unique message_id and no two messages have the same message_id. If success of the publish is not observed for some reason (network delays, interruptions etc) the same message payload may be retried by the publisher. If retries happen, we may end up with duplicate messages with different message_id in Pub/Sub. For Pub/Sub these are unique messages as they have different message_id.\n\nhttps://cloud.google.com/blog/products/data-analytics/handling-duplicate-data-in-streaming-pipeline-using-pubsub-dataflow"},{"timestamp":"1658683920.0","content":"Answer Might be D: '\n- This link explain it https://www.dbta.com/Editorial/Think-About-It/The-ABCs-of-Data-Deduplication-Demystifying-the-Different-Methods-108274.aspx#:~:text=The%20deduplication%20process%20begins%20by,is%20unique%20or%20a%20duplicate","poster":"duytran_d","upvote_count":"1","comment_id":"636162"},{"content":"Selected Answer: D\nBest answer is D. \nFor those saying the unique timestamp will make hash unique, that is incorrect. Creating a hash-key is a post-processing activity in sink, and you will only apply it on payload attributes, excluding timestamps. \nBetween B and D, D is more optimal. This is because in B, you are redoing the hashing of all historical data every time you compare new transactions. Whereas in D, you are storing already created hash values in a table, which is re-usable.","comment_id":"628248","timestamp":"1657178640.0","upvote_count":"1","poster":"NM1212"},{"content":"D should be correct not A. Reason is the GUID would be different for retransmitted data and will not solve the problem of duplication. While if you store the hash key for records with out transmission time you will able to identify the duplicate records before each data entry.","poster":"thapliyal","upvote_count":"2","comment_id":"622389","timestamp":"1656226440.0"},{"poster":"willymac2","upvote_count":"1","timestamp":"1654753680.0","content":"For me the best answer is A.\nIf we discard B and C, it let only D.\nAnd D seems wrong... First, we cannot include the timestamp in the hash as it is a transmission timestamp which will change for each re-transmission.\nBut if we hash only the data, we would remove from our dataset all the \"normal\" doubles.\nE.g. if the data contained two fields which were a product ID and quantity of items sold, we may often sell item X 1 time. Using the hash would remove all these \"normal\" doubles.","comment_id":"613641"},{"timestamp":"1653634140.0","content":"The correct Answer is B\nAs we can compare the hash keys and dedup the data","comment_id":"607951","upvote_count":"1","poster":"Yad_datatonic"},{"timestamp":"1653341520.0","upvote_count":"3","poster":"[Removed]","comment_id":"606325","content":"Selected Answer: A\nhttps://cloud.google.com/blog/products/data-analytics/handling-duplicate-data-in-streaming-pipeline-using-pubsub-dataflow"},{"upvote_count":"1","content":"Selected Answer: B\nB. Compute the hash value of each data entry, and compare it with all historical data.\nCorrect Answer - hash function of all data without transmit time will identify duplicate data in already stored data. You do not need a new table.","poster":"mn_1311","timestamp":"1651810980.0","comment_id":"597546"},{"comment_id":"594118","content":"Selected Answer: B\nA. Assign global unique identifiers (GUID) to each data entry. \n This approach do not help identify duplicate data.\nB. Compute the hash value of each data entry, and compare it with all historical data.\n Correct Answer - hash function of all data without transmit time will identify duplicate data in already stored data.\nC. Store each data entry as the primary key in a separate database and apply an index.\n This option is irrelevant and not logical at all. \nD. Maintain a database table to store the hash value and other metadata for each data entry. \n no need to store hash value specially it do not talk about comparing existing data.","poster":"Vip777","timestamp":"1651191540.0","upvote_count":"2"},{"poster":"i_b1","timestamp":"1651023780.0","comment_id":"592756","content":"Selected Answer: D\nIt's a proprietary solution. Cannot modify input","upvote_count":"3"},{"upvote_count":"2","timestamp":"1650453120.0","content":"Selected Answer: D\nD is the most efficient way to avoid data duplication based on the content.\n\nA->UUID generates a virtual unique identifier for the message, but doesn't imply that the content of the message is not duplicated.","poster":"alecuba16","comment_id":"588580"},{"upvote_count":"3","timestamp":"1649104200.0","comment_id":"580909","content":"It's D. A doesn't recognize duplicates (it just assign an ID to each row).","poster":"devric"},{"upvote_count":"1","content":"Selected Answer: D\nI agree it should be D","comment_id":"580206","timestamp":"1648981440.0","poster":"GCPCloudArchitectUser"},{"comment_id":"572642","poster":"i_b1","upvote_count":"1","timestamp":"1647914580.0","content":"Selected Answer: D\n\"A\" requires a modification of proprietory system.\nB is inefficient.\nD it is"},{"content":"Selected Answer: D\ncoz it's correct FFS ^_^","upvote_count":"1","comment_id":"538479","timestamp":"1643792280.0","poster":"ionescuandrei"},{"poster":"Nico1310","content":"Selected Answer: B\nhash values as a PK","timestamp":"1642451100.0","comment_id":"526084","upvote_count":"2"},{"comment_id":"516514","poster":"medeis_jar","timestamp":"1641295200.0","content":"Selected Answer: A\nTransmitted data includes fields and timestamp of transmission.\nSo, hash value changes with re-transmission ==> Option B & D are wrong.","upvote_count":"7","comments":[{"timestamp":"1642929120.0","poster":"Tanzu","content":"U dont have to put timestamp in your hash algorithm.","comments":[{"upvote_count":"3","timestamp":"1647399780.0","poster":"JK007","content":"But this question is not saying how you are calculating the hash algo. It says hash of \"data entry\" which includes other fields and timestamp field. So taking hash of the entire data entry the hash value will be different as timestamp will be different each time.","comment_id":"568743"}],"comment_id":"530401","upvote_count":"1"},{"comment_id":"529719","upvote_count":"1","poster":"exnaniantwort","timestamp":"1642843080.0","comments":[{"poster":"Tanzu","upvote_count":"2","content":"Hashing is inevitable. So only b or d is appropriate. \nThe main diff. b/w them is persistence. At least 6+6 hours, u need to persist the hashes in either storage or a table.\n\nThe is no persistence in b. So d is more ok then ..","comment_id":"530406","timestamp":"1642929420.0"}],"content":"The clearest explanation why D is wrong."}]},{"timestamp":"1640207040.0","comment_id":"507382","content":"D is the correct answer. A would require a modification of the proprietary application sending the data, which is beyond what you can do.","upvote_count":"1","poster":"qmhao99"},{"comments":[{"timestamp":"1649505540.0","poster":"abhineet1313","comment_id":"583292","content":"B requires comparing with all historical data, that is not efficient for each record.","upvote_count":"1"}],"timestamp":"1640024280.0","poster":"kishanu","content":"I feel it should be B, \nAssume: we are sending a message \"Hello\", a GUID gets assigned(11002321). Now, if this message is undelivered and try to retransmit, the new message will have a different GUID and it would be impossible to recreate the scenario to transmit the data again. Hence, the HASH value comes into the picture which can be compared with the rest of the historical records and the same data can be re-transmitted.","comment_id":"505616","upvote_count":"1"},{"content":"Its B.\nA - I feel its incorrect, as the question talks about re-transmiting the data. If you assing a unique ID to the data, how we would identify tf the data is duplicated.\nB - As the hash value of a similar entry would be same, this can be used to re-transmit the data\nC- Way far then the question.\nD - Close, but why use a database and create overhead.","poster":"kishanu","timestamp":"1639900560.0","comments":[{"timestamp":"1649505600.0","upvote_count":"1","content":"B requires comparing with all historical data, that is not efficient for each record.","poster":"abhineet1313","comment_id":"583293"}],"comment_id":"504709","upvote_count":"3"},{"comment_id":"501364","timestamp":"1639486800.0","poster":"aldno","upvote_count":"3","content":"The question hints that the transmission timestamp is set anew with every re-transmission. This makes A the only viable answer. B, C and D all rely on the \"first\" transmission timestamp being constant for every data point."},{"timestamp":"1636556760.0","upvote_count":"2","content":"Pub/Sub assigns a unique `message_id` to each message, which can be used to detect duplicate messages received by the subscriber. This will not, however, allow you to detect duplicates resulting from multiple publish requests on the same data. Detecting those will require a unique message identifier to be provided by the publisher. See Pub/Sub I/O for further discussion.\nhttps://cloud.google.com/pubsub/docs/faq#duplicates","comment_id":"475585","poster":"MaxNRG"},{"timestamp":"1635453000.0","poster":"SonuKhan1","upvote_count":"1","comment_id":"469474","content":"GUID & hashing isn't same ?"},{"poster":"anji007","comment_id":"462674","upvote_count":"2","content":"Ans: A\n\nTransmitted data includes fields and timestamp of transmission.\nSo, hash value changes with re-transmission ==> Option B & D proves wrong.","timestamp":"1634311800.0"},{"upvote_count":"1","content":"vote for A. D is also correct but A seems to be more efficient way.\nA: store UUID(only UUID!)\nD: store hash and metadata (it seems little bit redundant)","timestamp":"1631325300.0","comment_id":"442772","poster":"kubosuke"},{"content":"A is the correct answer","comment_id":"426363","upvote_count":"1","poster":"sandipk91","timestamp":"1629207600.0"},{"content":"A - In D, same message with different timestamp will have different hash, though the message content is the same.","poster":"ralf_cc","upvote_count":"3","timestamp":"1624879200.0","comment_id":"392856"},{"timestamp":"1624707480.0","comments":[{"timestamp":"1624707660.0","comment_id":"391176","comments":[{"content":"you should not attach a GUID to each message to support the scenario.","poster":"sumanshu","timestamp":"1625740080.0","comment_id":"401778","upvote_count":"1"}],"content":"I don't think - A is correct answer here - because A says - Assign global unique identifiers (GUID) to each data entry. (i.e. in source itself we need assign Unique ID to each data) - Who will assign this ? (if data source is cloud storage).\n\nThough, if it;s PUB-SUB (then it automatically assign message ID - but no where in question its mention we need to use PUB-SUB) - it's only mention ingestion service ie. DATAFLOW and DATAPROC","poster":"sumanshu","upvote_count":"3"}],"poster":"sumanshu","comment_id":"391169","upvote_count":"2","content":"Vote for 'D'\n\nData ingestion service in cloud are - Dataflow and Dataproc.\n\nand data generally get inerted from Source (Cloud storage) to BigQuery \n\nData ingestion: Dataflow or Dataproc can be used to transform and load data from Cloud Storage into Bigtable..\n\nSo, we can maintain a HASH in a Table to prevent the duplicates.\n\nCreates a hash suitable for identifying duplicate rows, useful when inserting duplicate rows."},{"timestamp":"1624218540.0","content":"Answer: A\nit is a proprietary application, insert a GUID with a record is easy. in cloud whenever see fame GUID, will discard new transmitted entry. so simple and elegant. I don't see why it wouldn't work. others are all too complicated and unnecessary.","upvote_count":"4","comments":[{"upvote_count":"1","content":"you should not attach a GUID to each message to support the scenario.","timestamp":"1625740140.0","poster":"sumanshu","comment_id":"401779"},{"comment_id":"588808","timestamp":"1650466740.0","poster":"msaqib934","upvote_count":"2","content":"guid always assign new id to each entry, so how can you identify duplicate record if entry get new at each entry ?"}],"comment_id":"386557","poster":"moonlightbeamer"},{"comment_id":"284992","timestamp":"1612631880.0","upvote_count":"2","poster":"naga","content":"Correct D"},{"poster":"Roxy2","comment_id":"282030","upvote_count":"5","content":"D is incorrect , data reord include transmission timestamp field and incase of retransmission there would be different timestamp and thus different HASH thus you can't compare based on has untill you remove timestamp field - which is not given n question","timestamp":"1612283220.0"},{"timestamp":"1612217940.0","poster":"tejeev","upvote_count":"2","comment_id":"281465","content":"D is the answer"},{"content":"Many suggested D as answer.. to dedulicate the data why should I use costly database ..\nDatabase is our assumption..\n\nI think B.. has th record and compare it with history data by hashing them","comment_id":"274381","timestamp":"1611386700.0","upvote_count":"2","poster":"[Removed]"},{"poster":"muraric","comment_id":"256517","upvote_count":"2","timestamp":"1609453260.0","content":"D\nI didn't see pub/sub mentioned anywhere in the question. Are we assuming the same? Data ingestion can happen through GCS also with the scheduled scripts"},{"upvote_count":"2","content":"Do we really need to use pubsub for injesting data every 6 hours?","poster":"abunasar786","comment_id":"252748","timestamp":"1608999180.0"},{"upvote_count":"6","comments":[{"timestamp":"1609631760.0","poster":"Alexej_123","comment_id":"258021","upvote_count":"3","content":"The answer A is Assign global unique identifiers (GUID) to each data entry. My question who should assign GUID. In the description it is pretty cear, that client will not assign and transfer guid to the cloud. If the client will not assign and transfer guid, how guid assignment will hellp to deduplicate? So I think A is wrong, and correct is D."}],"timestamp":"1605424980.0","poster":"Radhika7983","comment_id":"219555","content":"According to me the answer is A. the question is about a company sending inventory data every 6 hours to data ingestion service in cloud. Cloud data flow can run both in batch and streaming and hence data will be ingested using pub sub and will be processed by cloud pub sub in batch mode. \n\nOne thing to learn here is Pub/Sub assigns a unique `message_id` to each message, which can be used to detect duplicate messages received by the subscriber. So de duplication will be handle only if pub sub tries to send the same message. \nIn this case publisher is transmitting the message again and hence publishes need to provide a unique message identifier. \n\nThe other options talk about data base. As the question is specific to data ingestion and not data storage, I am going with A. However, even in cloud spanner, we can generate UUID for each message. Generating unique identifier is the most efficient way."},{"poster":"Alasmindas","comment_id":"219258","content":"Not sure between A and D. \nAssuming it \"proprietary system\" is Cloud Pub/Sub and sends streaming data and this is google questions with google managed services to be chosen, definitely Option A is the correct answer. However, as the question mentions \"Proprietary systems which sends inventory data every 6 hours (is every 6 hours a streaming data ???) - then the correct answer is Option D.","timestamp":"1605376260.0","upvote_count":"1"},{"poster":"Govind10","comment_id":"211895","content":"D. Even using Pub/Sub unless there is a non duplicate ID, we will not be able to assign a GUID.","timestamp":"1604404260.0","upvote_count":"2"},{"content":"Answer A\nMessage deduplication is required for exactly-once message processing. Dataflow deduplicates messages with respect to the Pub/Sub message ID. As a result, all processing logic can assume that the messages are already unique with respect to the Pub/Sub message ID. The efficient, incremental aggregation mechanism to accomplish this is abstracted in the PubsubIO API.\n\nIf PubsubIO is configured to use custom message IDs, Dataflow deduplicates messages by maintaining a list of all the custom IDs it has seen in the last 10 minutes. If a new message's ID is in this list, the message is assumed to be a duplicate and discarded.","timestamp":"1602226500.0","upvote_count":"3","comment_id":"196558","poster":"vickutk"},{"upvote_count":"2","poster":"saydawg","comment_id":"194400","timestamp":"1601999100.0","content":"Answer is D but should have given a full descrption of the solution (store hash values in database and compute the hash value of new entry the compare it with historical hash values, if hash value already exist, remove new entry). At this point, answer B is partially true because if values are not stored then we assume that historical hash values are computed every time we have new entries (so that's why it is not completely true)"},{"upvote_count":"2","timestamp":"1593330180.0","comment_id":"121711","content":"A is more effiecient that D.\nFor D, imagine if you are sending millions of record to the ingest pipeline then you need to maintain and query a separate database with millions of record.","poster":"yasoy"},{"upvote_count":"4","content":"Answer D","comment_id":"86700","poster":"arnabbis4u","timestamp":"1589136960.0"},{"poster":"[Removed]","upvote_count":"3","comments":[{"comment_id":"65679","content":"Assuming data collected in pub/sub, it is A.\nQuestion doesn't mention where we store the data, best practices for data duplicates recommend the use of HASH. So appropriate is D - using the HASH","poster":"[Removed]","upvote_count":"9","timestamp":"1584544740.0"},{"content":"Very unlikely pub/sub since data is send every 6 hours and not in a stream.\nD is better choice","comment_id":"76113","timestamp":"1587224700.0","poster":"vindahake","upvote_count":"5"},{"timestamp":"1594699620.0","poster":"Rajuuu","content":"This will not, however, allow you to detect duplicates resulting from multiple publish requests on the same data.","comment_id":"134549","upvote_count":"2"}],"content":"Answer: A\nhttps://cloud.google.com/pubsub/docs/faq#duplicates","comment_id":"65676","timestamp":"1584544380.0"}],"answers_community":["A (54%)","D (35%)","10%"]},{"id":"I3ZAOnMfgHiT2GH588ke","url":"https://www.examtopics.com/discussions/google/view/129857-exam-professional-data-engineer-topic-1-question-210/","answer_description":"","discussion":[{"timestamp":"1735504680.0","poster":"f74ca0c","content":"Selected Answer: D\nCreate a Dataplex lake that acts as the domain for your data mesh.\nAdd zones to your lake that represents individual teams within each domain and provide managed data contracts.\nAttach assets that map to data stored in Cloud Storage.\nhttps://cloud.google.com/transfer-appliance/docs/4.0/overview?_gl=1*8pbq1*_up*MQ..&gclid=CjwKCAiAg8S7BhATEiwAO2-R6gOYtlc2FJa7zE4lhz3-2f00x9F3hwgul9lYjfJs2cAprxOIeXq_NhoCw-8QAvD_BwE&gclsrc=aw.ds","upvote_count":"1","comment_id":"1333715"},{"upvote_count":"1","content":"Selected Answer: D\njust like MaxNRG said","timestamp":"1730147940.0","comment_id":"1304176","poster":"SamuelTsch"},{"comment_id":"1151149","poster":"JyoGCP","upvote_count":"1","timestamp":"1708017720.0","content":"Selected Answer: D\nAnswer D"},{"upvote_count":"1","content":"Selected Answer: D\nD. 1. Create a Dataplex virtual lake for each data product, and create multiple zones for landing, raw, and curated data.\n2. Provide the data engineering teams with full access to the virtual lake assigned to their data product.\n\nLake: A logical construct representing a data domain or business unit. For example, to organize data based on group usage, you can set up a lake for each department (for example, Retail, Sales, Finance).\nZone: A subdomain within a lake, which is useful to categorize data by the following:\nStage: For example, landing, raw, curated data analytics, and curated data science.","comments":[{"upvote_count":"1","content":"https://cloud.google.com/dataplex/docs/introduction","timestamp":"1705309080.0","poster":"datapassionate","comment_id":"1123202"}],"comment_id":"1123200","poster":"datapassionate","timestamp":"1705309080.0"},{"comment_id":"1121418","content":"Selected Answer: D\nD: 1 virtual lake per Data Product (which stands for domain basically), zones to split data by \"status\". Each Data Eng team can access their own data exclusively and in a data mesh compliant way","poster":"Matt_108","upvote_count":"1","timestamp":"1705134720.0"},{"comment_id":"1116050","upvote_count":"4","poster":"MaxNRG","timestamp":"1704650340.0","comments":[{"upvote_count":"3","poster":"MaxNRG","timestamp":"1704650400.0","comment_id":"1116051","content":"So the steps would be:\n1. Create a Dataplex virtual lake for each data product.\n2. Within each lake, create separate zones for landing, raw, and curated data.\n3. Provide each data engineering team with access only to the zones they need within their assigned virtual lake.\n4. Configure cross-lake sharing on the curated data zones to share curated data products between teams.\nThis provides isolation and access control between teams for raw data while enabling easy sharing of curated data products.\nhttps://cloud.google.com/dataplex/docs/introduction#a_domain-centric_data_mesh"}],"content":"Selected Answer: D\nThe best approach is to create a Dataplex virtual lake for each data product, with multiple zones for landing, raw, and curated data. Then provide the data engineering teams with access only to the zones they need within the virtual lake assigned to their product.\n\nTo enable teams to easily share curated data products, you should use cross-lake sharing in Dataplex. This allows curated zones to be shared across virtual lakes while maintaining data isolation for other zones."},{"upvote_count":"2","content":"I believe the answer is B, but there is a misspelling in the answer, should say \"create multiple zones\"","comment_id":"1115666","timestamp":"1704618600.0","poster":"Smakyel79"},{"poster":"Helinia","timestamp":"1704590760.0","content":"Selected Answer: D\nEach lake should be created per data product since data product sounds like a domain in this question. \n\nSince we have landing, raw, curated data, we should create different zones. \n\n\"Zones are of two types: raw and curated.\n\nRaw zone: Contains data that is in its raw format and not subject to strict type-checking.\n\nCurated zone: Contains data that is cleaned, formatted, and ready for analytics. The data is columnar, Hive-partitioned, and stored in Parquet, Avro, Orc files, or BigQuery tables. Data undergoes type-checking- for example, to prohibit the use of CSV files because they don't perform as well for SQL access.\"\n\nRef: https://cloud.google.com/dataplex/docs/introduction#terminology","upvote_count":"1","comment_id":"1115540"},{"upvote_count":"4","content":"why not B?","timestamp":"1704473880.0","comment_id":"1114674","poster":"Jordan18"},{"timestamp":"1704444540.0","comment_id":"1114368","comments":[{"content":"Because it's the best practice is separated zones. One zone for landing, raw and curated. \n\nThe answer B - has this part that excluded it \"create a single zone to contain landing\"\n\nThe correct awser is D","timestamp":"1705483860.0","upvote_count":"2","comment_id":"1124852","poster":"tibuenoc"}],"content":"Why not B?","poster":"Sofiia98","upvote_count":"3"},{"comment_id":"1112405","upvote_count":"2","timestamp":"1704245880.0","poster":"Ed_Kim","content":"Selected Answer: D\nThe answer is D"},{"content":"Selected Answer: C\nVirtual Lake per Data Product: Each virtual lake acts as a self-contained domain for a specific data product, aligning with the data mesh principle of decentralized ownership and responsibility.\nTeam Autonomy: Teams have full control over their virtual lake, enabling independent development, management, and sharing of their data products.","poster":"e70ea9e","comment_id":"1109533","upvote_count":"2","timestamp":"1703925180.0"}],"topic":"1","choices":{"A":"1. Create a single Dataplex virtual lake and create a single zone to contain landing, raw, and curated data.\n2. Provide each data engineering team access to the virtual lake.","D":"1. Create a Dataplex virtual lake for each data product, and create multiple zones for landing, raw, and curated data.\n2. Provide the data engineering teams with full access to the virtual lake assigned to their data product.","B":"1. Create a single Dataplex virtual lake and create a single zone to contain landing, raw, and curated data.\n2. Build separate assets for each data product within the zone.\n3. Assign permissions to the data engineering teams at the zone level.","C":"1. Create a Dataplex virtual lake for each data product, and create a single zone to contain landing, raw, and curated data.\n2. Provide the data engineering teams with full access to the virtual lake assigned to their data product."},"answer_ET":"D","isMC":true,"question_images":[],"question_id":125,"unix_timestamp":1703925180,"exam_id":11,"answers_community":["D (86%)","14%"],"timestamp":"2023-12-30 09:33:00","answer_images":[],"answer":"D","question_text":"You are designing a data mesh on Google Cloud with multiple distinct data engineering teams building data products. The typical data curation design pattern consists of landing files in Cloud Storage, transforming raw data in Cloud Storage and BigQuery datasets, and storing the final curated data product in BigQuery datasets. You need to configure Dataplex to ensure that each team can access only the assets needed to build their data products. You also need to ensure that teams can easily share the curated data product. What should you do?"}],"exam":{"id":11,"provider":"Google","isImplemented":true,"isBeta":false,"numberOfQuestions":319,"name":"Professional Data Engineer","isMCOnly":true,"lastUpdated":"11 Apr 2025"},"currentPage":25},"__N_SSP":true}