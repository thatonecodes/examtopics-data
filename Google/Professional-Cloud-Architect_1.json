{"pageProps":{"questions":[{"id":"gBrqAqgw26m2Yvyff7eL","answer_description":"","question_images":[],"question_text":"Your company has decided to make a major revision of their API in order to create better experiences for their developers. They need to keep the old version of the API available and deployable, while allowing new customers and testers to try out the new API. They want to keep the same SSL and DNS records in place to serve both APIs.\nWhat should they do?","unix_timestamp":1571849820,"isMC":true,"exam_id":4,"discussion":[{"content":"D is the answer because HTTP(S) load balancer can direct traffic reaching a single IP to different backends based on the incoming URL. A is not correct because configuring a new load balancer would require a new or different SSL and DNS records which conflicts with the requirements to keep the same SSL and DNS records. B is not correct because it goes against the requirements. The company wants to keep the old API available while new customers and testers try the new API. C is not correct because it is not a requirement to decommission the implementation behind the old API. Moreover, it introduces unnecessary risk in case bugs or incompatibilities are discovered in the new API.","poster":"shandy","comments":[{"content":"Answer: D. Use separate backend pools for each API path behind the load balancer\nThis is the correct option because:\nlearns-google.blogspot.com\nIt maintains a single entry point with existing SSL/DNS\nAllows path-based routing to direct traffic appropriately\nRequires no client reconfiguration\nProvides clean separation between versions\nFollows common API versioning patterns","comment_id":"1398379","poster":"metoxig","upvote_count":"1","timestamp":"1742022480.0"},{"comment_id":"696472","content":"D is right","timestamp":"1665948840.0","poster":"AzureDP900","upvote_count":"3"}],"comment_id":"24651","upvote_count":"107","timestamp":"1726851840.0"},{"content":"agreed, The answer is D","upvote_count":"20","timestamp":"1574174280.0","poster":"AWS56","comment_id":"22719"},{"poster":"Paxtons_Aunders","content":"Selected Answer: D\nI need to perform the Google Architect Renew. There's a Google release of one Architect certification called \"Professional Cloud Architect Renewal.\" I would like to know if the Google Architect certification available on this site covers these renewal questions. Thanks in advance.\n\nhttps://docs.google.com/document/d/1VV6vkkjShXDgPLSG6V_7-0dweLmZTUnYiTSxo6C5ERY/edit?tab=t.0","timestamp":"1742544420.0","comment_id":"1401472","upvote_count":"1"},{"comment_id":"1366142","timestamp":"1741321380.0","upvote_count":"1","poster":"WakandaF","content":"Selected Answer: D\nI need to perform the Google Architect Renew. There's a Google release of one Architect certification called \"Professional Cloud Architect Renewal.\" I would like to know if the Google Architect certification available on this site covers these renewal questions. Thanks in advance.\n\nhttps://services.google.com/fh/files/misc/pca_renewal_exam_overview_and_faqs.pdf\n\nCQ"},{"timestamp":"1741003980.0","poster":"j_vish","upvote_count":"1","comment_id":"1364387","content":"Selected Answer: D\nD is correct"},{"timestamp":"1739463000.0","poster":"biplabid","content":"Selected Answer: D\nAnswer is D.\nUsing a HTTPS load balancer for different backend makes sense as we dont want to disturb the old API setup and also want to test new API","comment_id":"1356195","upvote_count":"1"},{"upvote_count":"1","comment_id":"1355569","timestamp":"1739357820.0","poster":"Test88888","content":"Selected Answer: D\nCan anybody tell if these here question are still in the current exam?"},{"content":"Selected Answer: D\nD is right!","upvote_count":"1","poster":"XiaobinJiang","comment_id":"1332341","timestamp":"1735293780.0"},{"upvote_count":"1","timestamp":"1733722260.0","poster":"FI22","content":"Selected Answer: D\nD is good for keep the same records.","comment_id":"1323878"},{"content":"Selected Answer: D\nUsing separate backend pools for each API version behind a single load balancer fulfills all requirements:\n\nSingle SSL and DNS Endpoint:\n\nBoth API versions share the same SSL certificate and DNS records, so users and developers donâ€™t need to reconfigure their endpoints.\nThe load balancer handles routing transparently.\nSeparation of APIs:\n\nDifferent backend pools allow for clear segregation between the old and new API versions.\nEach API can be deployed, updated, and scaled independently.\nTraffic Management:\n\nThe load balancer can route traffic based on request paths (e.g., /v1 for the old API and /v2 for the new API).\nThis allows smooth coexistence of the APIs while developers gradually adopt the new version.","comment_id":"1319710","timestamp":"1732884000.0","poster":"Ghorbel","upvote_count":"1"},{"timestamp":"1732157400.0","upvote_count":"1","poster":"ddatta","comment_id":"1315619","content":"Selected Answer: D\nYou have to split the traffic"},{"content":"Answer: D. Use separate backend pools for each API path behind the load balancer\nThis is the correct option because:\n\nIt maintains a single entry point with existing SSL/DNS\nAllows path-based routing to direct traffic appropriately\nRequires no client reconfiguration\nProvides clean separation between versions\nFollows common API versioning patterns","timestamp":"1732072380.0","comment_id":"1315019","upvote_count":"1","poster":"devenderpraksh"},{"comment_id":"1309556","poster":"Ekramy_Elnaggar","content":"The best answer here is D. Use separate backend pools for each API path behind the load balancer.\n\nHere's why:\n\n* Maintaining the same DNS and SSL records: This requirement implies that both API versions need to be accessible through the same domain and endpoint.\n* Separate backend pools: By using separate backend pools for each API version, the load balancer can direct traffic to the appropriate servers based on the request path. This allows for seamless co-existence of both versions without any conflict.\n* Flexibility and Control: This approach provides flexibility in managing traffic, allowing you to gradually shift users to the new API or run A/B testing.","upvote_count":"1","timestamp":"1731260460.0"},{"content":"You have a Compute Engine managed instance group that adds and removes Compute Engine instances from the group in response to the load on your application.The instances have a shutdown script that removes REDIS database entries associated with the instance.You see that many database entries have not been removed, and you suspect that the shutdown script is the problem.You need to ensure that the commands in the shutdown script are run reliably every time an instance is shut down. You create a Cloud Function to remove the database entries.What should you do next?\nA.\nModify the shutdown script to wait for 30 seconds and then publish a message to a Pub/Sub queue.\nB.\nModify the shutdown script to wait for 30 seconds before triggering the Cloud Function.\nC.\nSet up a Cloud Monitoring sink that triggers the Cloud Function after an instance removal log message arrives in Cloud logging\nD.\nDo not use the Cloud Function. Modify the shutdown script to restart if it has not completed in 30 seconds. \n\nwhat is your answer?","poster":"nhatpham","timestamp":"1726907340.0","upvote_count":"2","comments":[{"poster":"pupi08","upvote_count":"1","timestamp":"1648728480.0","comment_id":"578837","content":"c it's correct"}],"comment_id":"566848"},{"poster":"RGTest","content":"A. VPC peering","comment_id":"612514","upvote_count":"2","timestamp":"1654559280.0"},{"comment_id":"610654","content":"Must be A","poster":"davidbilla","upvote_count":"1","timestamp":"1654182060.0"},{"timestamp":"1660557780.0","poster":"MQQNB","content":"C.\nUse the Cloud Storage Signed URL feature to generate a POST URL","upvote_count":"1","comment_id":"647106"},{"poster":"snome","timestamp":"1668266340.0","comment_id":"716776","upvote_count":"1","content":"A, The Cross Origin Resource Sharing (CORS) spec was developed by the World Wide Web Consortium (W3C) to remove the limit of Same Origin Policy. Cloud Storage supports this specification by allowing you to configure your buckets to support CORS. \n https://cloud.google.com/storage/docs/cross-origin\n\nThen for authentication I'll go with signed URL feature, it gives more security"},{"comment_id":"580069","content":"Option D seems to be the right one..\ncloud.google.com/network-connectivity/docs/vpn/concepts/topologies#to_peer_vpn_gateways","poster":"Skr6266","timestamp":"1648940760.0","upvote_count":"2"},{"poster":"MQQNB","timestamp":"1660556280.0","comment_id":"647095","content":"I think it should be A. refer to the documentation, to have HA 99.99%, it needs at least 2 tunnels. And one HA VPN with one peer VPN could have 2 tunnels. In Google Cloud, the REDUNDANCY_TYPE for this configuration takes the value SINGLE_IP_INTERNALLY_REDUNDANT.\nD. not cost optimized\nB. didn't mention 2 tunnels\nC. not simple","upvote_count":"1"},{"comment_id":"1276683","timestamp":"1726907280.0","poster":"maxdanny","upvote_count":"3","content":"Selected Answer: D\nD because This approach allows you to keep the same SSL and DNS records while directing traffic based on the API path. The load balancer can be configured to route requests to different backend pools depending on whether the request is for the old or new API version. This ensures that both versions are accessible under the same domain, providing a seamless transition for both old and new users."},{"upvote_count":"1","poster":"RickMorais","comment_id":"1241442","timestamp":"1720014360.0","content":"Selected Answer: D\nD is right"},{"content":"Selected Answer: D\nD is the correct","poster":"SaurabhL","comment_id":"1209203","upvote_count":"1","timestamp":"1715317560.0"},{"poster":"juanlopezcervero","comment_id":"1199822","upvote_count":"1","content":"D is the answer","timestamp":"1713731640.0"},{"upvote_count":"2","poster":"eloyus","timestamp":"1706080560.0","comment_id":"1130330","content":"Selected Answer: D\nD is ok"},{"timestamp":"1704802800.0","poster":"hzaoui","comment_id":"1117460","content":"Selected Answer: D\nD answer is correct","upvote_count":"1"},{"comment_id":"1105894","timestamp":"1703586780.0","poster":"SWMok","upvote_count":"1","content":"Agreed D is the answer"},{"content":"Selected Answer: D\nD is the right answer.","timestamp":"1702234560.0","upvote_count":"1","poster":"Haigk","comment_id":"1092768"},{"content":"Selected Answer: D\nObvious.... split the feed","upvote_count":"1","timestamp":"1695016260.0","poster":"ChinaSailor","comment_id":"1010307"},{"content":"The correct answer is D as visually explained in the HTTPS Load Balancer architecture infographic (figure 5.5) at page 225 in the \"GCP Professional Cloud Network Engineer Certification Companion\" book.","comment_id":"988269","upvote_count":"1","timestamp":"1692792060.0","poster":"dar10"},{"content":"Selected Answer: D\nD is correct","upvote_count":"1","poster":"RaviRS","comment_id":"981405","timestamp":"1692086520.0"},{"poster":"vyomkeshbakshi","timestamp":"1689077760.0","content":"Selected Answer: D\nD is right","upvote_count":"1","comment_id":"948944"},{"content":"Selected Answer: D\nThe correct answer is D:","comment_id":"948902","timestamp":"1689072840.0","poster":"anirban7172","upvote_count":"1"},{"timestamp":"1689072720.0","content":"Selected Answer: D\nThe correct answer is D: \n\nExplanation: \nOption D, using separate backend pools for each API path behind the load balancer, allows traffic to be routed to the appropriate backend based on the API path. This ensures that the old and new APIs can coexist and be independently deployable while sharing the same SSL and DNS records. It provides isolation and simplifies the management of the two versions.\nTherefore, option D is the recommended approach to achieve the desired outcome of keeping the old API available and deployable while allowing new customers and testers to try out the new API.","comment_id":"948900","upvote_count":"5","poster":"anirban7172"},{"poster":"b_max","upvote_count":"1","content":"Selected Answer: D\nDebe ser la D","comment_id":"838908","timestamp":"1678802280.0"},{"timestamp":"1678534380.0","poster":"DB3118","upvote_count":"1","content":"Selected Answer: D\nD is most suitable","comment_id":"835897"},{"poster":"simonab23","upvote_count":"3","comment_id":"789696","timestamp":"1674830520.0","content":"Selected Answer: D\nThe question states that the company needs to keep the old version of the API available and deployable, while allowing new customers and testers to try out the new API. They want to keep the same SSL and DNS records in place to serve both APIs.\nUsing a separate backend pool for each API path behind the load balancer will allow the company to keep the old version of the API available and deployable, while allowing new customers and testers to try out the new API. This is because the load balancer will be able to route traffic to the appropriate backend pool based on the path."},{"timestamp":"1672474440.0","upvote_count":"1","comment_id":"762547","poster":"AShrujit","content":"D for me"},{"poster":"Jaldhi24","timestamp":"1671898500.0","upvote_count":"1","content":"Selected Answer: D\nD is right","comment_id":"754995"},{"content":"D. Use separate backend pools for each API path behind the load balancer\n\nTo keep the old version of the API available and deployable while allowing new customers and testers to try out the new API, while also keeping the same SSL and DNS records in place, your company can use separate backend pools for each API path behind the load balancer. This will allow them to route traffic to the appropriate backend pool based on the path, allowing them to serve both the old and new versions of the API using the same load balancer and DNS records.\n\nOther options, such as configuring a new load balancer for the new version of the API or reconfiguring old clients to use a new endpoint for the new API, may not allow them to keep the same SSL and DNS records in place. Forwarding traffic to the new API based on the path may not provide a reliable way to separate the two APIs.","comment_id":"751965","upvote_count":"2","poster":"omermahgoub","timestamp":"1671607200.0"},{"upvote_count":"1","comment_id":"751229","content":"Selected Answer: D\nD is the answer","poster":"Ahmed1984_","timestamp":"1671554820.0"},{"comment_id":"741390","poster":"GCP_Student1","timestamp":"1670729460.0","upvote_count":"1","content":"D is correct answer,"},{"comment_id":"721825","content":"Selected Answer: D\nD is correct answer","upvote_count":"1","poster":"AniketD","timestamp":"1668841740.0"},{"comment_id":"708274","upvote_count":"1","content":"Selected Answer: D\nD is the correct one","poster":"ckorbet","timestamp":"1667203620.0"},{"content":"Vote D","timestamp":"1665675600.0","poster":"jecoji8660","upvote_count":"1","comment_id":"694055"},{"content":"Selected Answer: D\nD. Use separate backend pools for each API path behind the load balancer","upvote_count":"1","timestamp":"1665576960.0","comment_id":"693062","poster":"minmin2020"},{"comment_id":"660165","poster":"gee1979","upvote_count":"1","timestamp":"1662380340.0","content":"D... while allowing new customers and testers to try out the new API. They want to keep the same SSL and DNS records in place to serve both APIs.\n\nD. Use separate backend pools for each API path behind the load balancer."},{"comment_id":"635385","content":"vote D\nkey point is :\nThey want to keep the same SSL and DNS records in place to serve both APIs.","timestamp":"1658535360.0","poster":"backhand","upvote_count":"2"},{"comment_id":"623128","content":"Selected Answer: D\nanswer D","timestamp":"1656315660.0","poster":"nicoueron","upvote_count":"2"},{"timestamp":"1644103260.0","poster":"AJRD","comment_id":"541335","content":"Selected Answer: D\nD is the correct answer.","upvote_count":"1"},{"timestamp":"1643719440.0","content":"Selected Answer: D\nD is the answer because HTTP(S) load balancer can direct traffic reaching a single IP to different backends based on the incoming URL.","poster":"anjuagrawal","upvote_count":"1","comment_id":"537846"},{"comment_id":"537244","timestamp":"1643647380.0","upvote_count":"1","content":"Answer is D","poster":"sasithra"},{"timestamp":"1642141320.0","upvote_count":"1","content":"Selected Answer: D\nVote D","comment_id":"523370","poster":"JohnDoeSSS"},{"comment_id":"517034","content":"Selected Answer: D\nBecause they want to have the same SSL and DNS","timestamp":"1641336780.0","poster":"Moss2011","upvote_count":"1"},{"timestamp":"1641178140.0","upvote_count":"1","content":"Selected Answer: D\nvote D","comment_id":"515366","poster":"cuongnd"},{"upvote_count":"2","timestamp":"1640597460.0","poster":"PhuocT","comment_id":"510187","content":"Selected Answer: D\nVote D"},{"comment_id":"493986","content":"ans is D","timestamp":"1638656220.0","upvote_count":"1","poster":"cloud_enthusiast_in"},{"content":"Go for D.","poster":"haroldbenites","timestamp":"1638545940.0","comment_id":"493203","upvote_count":"2"},{"content":"Selected Answer: D\nVote D","poster":"joe2211","comment_id":"490590","timestamp":"1638268680.0","upvote_count":"1"},{"upvote_count":"1","poster":"nqthien041292","comment_id":"482589","timestamp":"1637417220.0","content":"Selected Answer: D\nVote D"},{"content":"Selected Answer: D\nThe answer is D","upvote_count":"2","poster":"GuilhermeAlteia","comment_id":"479611","timestamp":"1637093460.0"},{"content":"Option D is the correct choice.","timestamp":"1636189320.0","comment_id":"473342","upvote_count":"2","poster":"vincy2202"},{"timestamp":"1635982500.0","comment_id":"472325","poster":"exam_war","content":"go with D","upvote_count":"1"},{"poster":"fwfw","content":"DDD can configure existed Load Balancer","comment_id":"463021","upvote_count":"1","timestamp":"1634382480.0"},{"content":"I go with option D","comment_id":"457426","poster":"mum_lalitha0508","upvote_count":"1","timestamp":"1633398420.0"},{"poster":"aviratna","comment_id":"390914","content":"D is correct which can be combined with cloud native load balancer like traefik for Cananry, A/B testing to redirect % traffic to old & new API for testing","timestamp":"1624681320.0","upvote_count":"1"},{"upvote_count":"1","poster":"Papafel","comment_id":"388405","content":"Yes, I go for D also.","timestamp":"1624409640.0"},{"upvote_count":"1","timestamp":"1624138920.0","comment_id":"385801","content":"D for sure","poster":"hoanghhvnu"},{"poster":"victory108","content":"D. Use separate backend pools for each API path behind the load balancer","upvote_count":"2","timestamp":"1621316880.0","comment_id":"360105"},{"comment_id":"359920","content":"D is the answer because HTTP(S) load balancer can direct traffic reaching a single IP to different backends based on the incoming URL.\n\nFor all other options there will not be same SSL and DNS configration for both working APIs.","upvote_count":"2","timestamp":"1621298400.0","poster":"Amber25"},{"comment_id":"351208","content":"D is correct","poster":"un","timestamp":"1620319080.0","upvote_count":"1"},{"content":"Answers is D","poster":"Ausias18","comment_id":"323953","timestamp":"1617078300.0","upvote_count":"1"},{"content":"\"D\" best suites the purpose.","poster":"Joyjit_Deb","comment_id":"289990","upvote_count":"2","timestamp":"1613274540.0"},{"timestamp":"1610827500.0","upvote_count":"1","content":"D should be the answer","comment_id":"269076","poster":"VenV"},{"poster":"whitley030390","comment_id":"189100","upvote_count":"1","content":"D sounds right here","timestamp":"1601302200.0"},{"poster":"AshokC","timestamp":"1601166960.0","content":"Answer D","upvote_count":"1","comment_id":"188038"},{"poster":"gkdinesh","timestamp":"1599318300.0","upvote_count":"1","content":"D is correct","comment_id":"174030"},{"comment_id":"136191","content":"Agree with D","upvote_count":"1","poster":"zzaric","timestamp":"1594876140.0"},{"timestamp":"1594289160.0","comment_id":"130476","upvote_count":"1","content":"Agree with Answer : D.","poster":"RM07"},{"poster":"saurabh1805","upvote_count":"1","comment_id":"129738","timestamp":"1594211280.0","content":"D is correct answer for this"},{"poster":"mlantonis","upvote_count":"1","comment_id":"116626","timestamp":"1592845380.0","content":"D for sure"},{"comment_id":"106486","upvote_count":"1","timestamp":"1591765260.0","content":"D, for sure","poster":"gfhbox0083"},{"timestamp":"1591100100.0","content":"D is the correct answer","poster":"Nirms","upvote_count":"2","comment_id":"100781"},{"poster":"Ziegler","content":"D is the correct answer","upvote_count":"1","timestamp":"1590760380.0","comment_id":"98254"},{"upvote_count":"1","content":"Answer D","comment_id":"94580","poster":"Javed","timestamp":"1590269640.0"},{"timestamp":"1589668860.0","comment_id":"90183","upvote_count":"1","content":"D is the right answer","poster":"laksg"},{"content":"I will go with D.\nA does not seem right because the requirement is to use the same SSL and DNS entries for both APIs.\nB does not seem right because it would mean changing old client code.\nC does not seem right because it would mean modifying the old API.\nD makes the most sense because a path-based decision would require SSL and DNS changes and wouldn't require changes to old clients and the old API code.","timestamp":"1589057520.0","poster":"clouddude","comment_id":"86225","upvote_count":"1"},{"content":"D is the correct answer","upvote_count":"2","timestamp":"1588537140.0","poster":"gcp_aws","comment_id":"83236"},{"comment_id":"44681","content":"answer: D","timestamp":"1580388660.0","upvote_count":"2","poster":"2g"},{"comment_id":"16998","poster":"Eroc","timestamp":"1571849820.0","comments":[{"upvote_count":"3","comments":[{"content":"DNS has nothing to do with path-based forwarding. With path-based forwarding, the hostname (which DNS resolution is based on) does not change.","timestamp":"1613741760.0","comment_id":"294270","upvote_count":"1","poster":"bogd"}],"comment_id":"151024","content":"D is ok\nC is incorrect. LB can't forward any traffic based on path as DNS is not changing.","poster":"tartar","timestamp":"1596617760.0"}],"content":"Options \"A\", \"B\", and \"C\" would change the availability of the old API. Preventing traffic from flowing through it, option \"D\" mentions load balancer which in Google's marketing context implies traffic to both APIs.","upvote_count":"3"}],"answer_ET":"D","question_id":1,"topic":"1","answer_images":[],"answer":"D","url":"https://www.examtopics.com/discussions/google/view/7083-exam-professional-cloud-architect-topic-1-question-1/","timestamp":"2019-10-23 18:57:00","choices":{"A":"Configure a new load balancer for the new version of the API","C":"Have the old API forward traffic to the new API based on the path","D":"Use separate backend pools for each API path behind the load balancer","B":"Reconfigure old clients to use a new endpoint for the new API"},"answers_community":["D (100%)"]},{"id":"4KmnvZRBEFmPoVqTrydy","url":"https://www.examtopics.com/discussions/google/view/7041-exam-professional-cloud-architect-topic-1-question-10/","topic":"1","question_id":2,"choices":{"B":"Run your script on a new virtual machine with the BigQuery access scope enabled","C":"Create a new service account with BigQuery access and execute your script with that user","A":"Install the latest BigQuery API client library for Python","D":"Install the bq component for gcloud with the command gcloud components install bq."},"question_text":"You write a Python script to connect to Google BigQuery from a Google Compute Engine virtual machine. The script is printing errors that it cannot connect to\nBigQuery.\nWhat should you do to fix the script?","answer":"C","answer_description":"","discussion":[{"comments":[{"timestamp":"1633773600.0","content":"Access scopes are the legacy method of specifying permissions for your instance. read from > https://cloud.google.com/compute/docs/access/service-accounts . So , I would go with C","poster":"rishab86","comment_id":"459586","upvote_count":"11"},{"comment_id":"297170","content":"agreed to comment here . C seems like a good option","poster":"Vika","timestamp":"1614059520.0","upvote_count":"4"},{"comment_id":"649791","timestamp":"1661094600.0","content":"agree\naccess scope is enabled by default\nhttps://cloud.google.com/bigquery/docs/authorization#authenticate_with_oauth_20\n\n If you use the BigQuery client libraries, you do not need this information, as this is done for you automatically.","upvote_count":"2","poster":"MQQNB"},{"upvote_count":"4","comment_id":"109428","content":"Might be an old version","timestamp":"1592050380.0","poster":"Musk"}],"timestamp":"1726852500.0","comment_id":"25560","content":"A - If client library was not installed, the python scripts won't run - since the question states the script reports \"cannot connect\" - the client library must have been installed. so it's B or C.\n\nB - https://cloud.google.com/bigquery/docs/authorization an access scope is how your client application retrieve access_token with access permission in OAuth when you want to access services via API call - in this case, it is possible that the python script use an API call instead of library, if this is true, then access scope is required. client library requires no access scope (as it does not go through OAuth)\n\nC - service account is Google Cloud's best practice\nSo prefer C.","upvote_count":"99","poster":"kalschi"},{"upvote_count":"13","timestamp":"1571810400.0","comments":[{"timestamp":"1606564020.0","upvote_count":"6","comment_id":"229588","content":"Configure the Python API to use a service account with relevant BigQuery access enabled. is the right answer.\n\nIt is likely that this service account this script is running under does not have the permissions to connect to BigQuery and that could be causing issues. You can prevent these by using a service account that has the necessary roles to access BigQuery.\n\nRef: https://cloud.google.com/bigquery/docs/reference/libraries#cloud-console\n\nA service account is a special kind of account used by an application or a virtual machine (VM) instance, not a person.\n\nRef: https://cloud.google.com/iam/docs/service-accounts","poster":"techalik"},{"comments":[{"comments":[{"comment_id":"158217","upvote_count":"36","poster":"cloudguy1","content":"Stop confusing people, B) doesn't make any sense. Why would you use or create a whole new VM just because of a permission issue? If anything, just stop the instance and edit the scope of the default Compute Service Account and grant it the role through IAM. C) is the most appropriate answer since you can only set scopes of the default Compute Service Account, if you're using any other, there's no scope option - its access is dictated strictly by IAM in such scenario. So C) is the answer: Stop the VM, change the Service Account with the appropriate permissions and done. B) would still need to have permission the set through IAM & Admin, the scope isn't enough with the default Compute Service Account.","timestamp":"1597423140.0","comments":[{"timestamp":"1605345960.0","content":"cloud guy1, relax. tartar is the hero for google cloud and if you read his answer, he explains the service account user's role granting on this one as that is the best practice","poster":"certificatores","upvote_count":"4","comment_id":"219026"}]}],"timestamp":"1597391640.0","upvote_count":"15","poster":"tartar","comment_id":"157911","content":"Sorry, B is ok. You can create service account, add user to service account, and grant the user role as Service Account User. You still need to enable BigQuery scope to make the Python script running the instance to access BigQuery."}],"timestamp":"1596621300.0","upvote_count":"11","comment_id":"151074","poster":"tartar","content":"C is ok"},{"comments":[{"timestamp":"1672067220.0","poster":"[Removed]","upvote_count":"2","comment_id":"757572","content":"Create a new service account with BigQuery access and execute your script with that user: If you want to run the script on an existing virtual machine, you can create a new service account with the necessary permissions to access BigQuery and then execute the script using that service account. This will allow the script to connect to BigQuery and access the data it needs."},{"timestamp":"1614840480.0","comment_id":"303179","upvote_count":"2","content":"I stand corrected, B you need to have scope. It is union of Scope + Service Account. If scope is not there, you are screwed anyways.","poster":"nitinz"}],"comment_id":"303170","content":"C, no brainer. You need SA for using API period. Thats where your start your troubleshooting.","poster":"nitinz","timestamp":"1614840180.0","upvote_count":"6"}],"poster":"KouShikyou","comment_id":"16868","content":"Why not B? It looks better for me."},{"upvote_count":"1","timestamp":"1739687220.0","comment_id":"1357162","content":"Selected Answer: B\nGoogle Compute Engine instances have access scopes that control which Google Cloud services they can interact with.","poster":"Jamee"},{"comment_id":"1347891","poster":"hpf97","timestamp":"1738067520.0","upvote_count":"1","content":"Selected Answer: C\nA & D does not deal about connection\nC : is mandatory for security reason\nB : could be true but here is always a \"default\" scope. And the error message is about connection not access. And it is for OAuth2 which we do not know if the python script uses it."},{"timestamp":"1736021280.0","poster":"ryaryarya","comment_id":"1336521","upvote_count":"1","content":"Selected Answer: B\nThis is obviously a really old question. The best answer is B. Here's why:\n\n- There was a time when you couldn't update access scopes on a VM and had to create a new one if the scopes needed to change (which was painful).\n- Without access scopes enabled, Google will block access to those APIs, so a new service account isn't enough, even if IAM was a problem.\n- The question just says that the script 'cannot connect' to BQ - it doesn't mention permissions."},{"upvote_count":"1","timestamp":"1735214760.0","content":"Selected Answer: B\nActually the correct answer is B, reason is , you need to define the scope first, service accounts comes later.","poster":"gcloud007","comment_id":"1331901"},{"upvote_count":"1","content":"Selected Answer: C\nA and C are correct, but we eliminated A because they mentioned \"cannot connect\" which means the script can run which means the client library was already installed, so final answer is only \"C\"\n\n\"C\" was chosen because in order to access BigQuery, the script needs to authenticate and be authorized. The recommended way to do this for applications running on Compute Engine is to use a service account. Create a service account with the appropriate permissions (e.g., \"BigQuery Data Editor\") to access your BigQuery data. When running the script, make sure it uses the service account credentials to authenticate. This can be done by setting the GOOGLE_APPLICATION_CREDENTIALS environment variable to the path of the service account key file.","poster":"Ekramy_Elnaggar","comment_id":"1309597","timestamp":"1731267480.0"},{"timestamp":"1723094760.0","poster":"Hungdv","comment_id":"1262316","upvote_count":"1","content":"Choose C"},{"timestamp":"1717861920.0","poster":"kingfighers","comment_id":"1226809","content":"I suppose all of them are correct, but we should choose the least effort, B is correct..","comments":[{"poster":"kingfighers","upvote_count":"1","comment_id":"1226810","content":"run script on a new vm, not create a new vm..","timestamp":"1717861980.0"}],"upvote_count":"1"},{"poster":"a2le","timestamp":"1717822740.0","comment_id":"1226533","upvote_count":"2","content":"Selected Answer: C\nTricky question.\nHowever, as you can read in gcloud compute instances create documentation:\n--scopes=[SCOPE,â€¦]\nIf not provided, the instance will be assigned the default scopes, described below. However, if neither --scopes nor --no-scopes are specified and the project has no default service account, then the instance will be created with no scopes. Note that the level of access that a service account has is determined by a combination of access scopes and IAM roles so you must configure both access scopes and IAM roles for the service account to work properly.\n\nSo, probably, B is the right one, as for the \"new vm\", I guess that this is because you don't want to stop the current one before having the working one ready..."},{"timestamp":"1716576360.0","poster":"Robert0","upvote_count":"1","content":"Selected Answer: C\nC - service account is Google Cloud's best practice","comment_id":"1217726"},{"poster":"researched_answer_boi","content":"Selected Answer: C\nYou don't need to create a new VM to have different access scopes:\nhttps://cloud.google.com/compute/docs/access/service-accounts#accesscopesiam\nThis weakens answer B.\nWhen a user-managed service account is attached to the instance, the access scope defaults to cloud-platform:\nhttps://cloud.google.com/compute/docs/access/service-accounts#scopes_best_practice\nSee Step 6 in: https://cloud.google.com/compute/docs/instances/change-service-account#changeserviceaccountandscopes\nThese facts leave C as the valid answer.","upvote_count":"2","comment_id":"1201522","timestamp":"1713977280.0"},{"content":"Selected Answer: C\nC. Create a new service account with BigQuery access and execute your script with that user.\nService accounts are used for server-to-server interactions, such as those between a virtual machine and BigQuery. You would need to create a service account that has the necessary permissions to access BigQuery, then download the service account key in JSON format. Once you have the key, you can set an environment variable (GOOGLE_APPLICATION_CREDENTIALS) to the path of the JSON key file before running your script, which will authenticate your requests to BigQuery.","comment_id":"1169388","poster":"santoshchauhan","comments":[{"comment_id":"1171807","timestamp":"1710258840.0","content":"better than creating and downloading a service account key would be to impersonate the service account","upvote_count":"1","poster":"Powerboy"}],"upvote_count":"3","timestamp":"1709978100.0"},{"upvote_count":"1","timestamp":"1707080040.0","comment_id":"1140524","poster":"tosinogunfile","content":"The answer is C\n\nhttps://cloud.google.com/bigquery/docs/authentication\nFor most services, you must attach the service account when you create the resource that will run your code; you cannot add or replace the service account later. Compute Engine is an exceptionâ€”it lets you attach a service account to a VM instance at any time."},{"content":"Selected Answer: C\nConnecting to BigQuery from a script requires proper authorization. Service accounts provide a secure way to grant access without sharing user credentials.","poster":"hzaoui","upvote_count":"1","timestamp":"1704987600.0","comment_id":"1119912"},{"upvote_count":"1","poster":"public_figure","timestamp":"1703803140.0","comment_id":"1108139","content":"It should be B,\nScript cannot be run by user and user cannot be assigned with Service account, SA can be assigned to a VM"},{"comment_id":"1067286","upvote_count":"1","timestamp":"1699626360.0","poster":"thewalker","content":"C\nBest practice is that SA with least privilege from a CE should access BQ."}],"answer_ET":"C","timestamp":"2019-10-23 08:00:00","question_images":[],"exam_id":4,"answer_images":[],"isMC":true,"unix_timestamp":1571810400,"answers_community":["C (80%)","B (19%)","2%"]},{"id":"qq28KGb8RNFl4vc33AK0","question_id":3,"answer_images":[],"unix_timestamp":1572080220,"url":"https://www.examtopics.com/discussions/google/view/7233-exam-professional-cloud-architect-topic-1-question-100/","choices":{"D":"Using the Cron service provided by GKE, publish messages to a Cloud Pub/Sub topic. Subscribe to that topic using a message-processing utility service running on Compute Engine instances.","C":"Using the Cron service provided by Google Kubernetes Engine (GKE), publish messages directly to a message-processing utility service running on Compute Engine instances.","B":"Using the Cron service provided by App Engine, publish messages to a Cloud Pub/Sub topic. Subscribe to that topic using a message-processing utility service running on Compute Engine instances.","A":"Using the Cron service provided by App Engine, publish messages directly to a message-processing utility service running on Compute Engine instances."},"answer_description":"","answers_community":["B (97%)","3%"],"topic":"1","exam_id":4,"timestamp":"2019-10-26 10:57:00","answer":"B","question_text":"You need to ensure reliability for your application and operations by supporting reliable task scheduling for compute on GCP. Leveraging Google best practices, what should you do?","answer_ET":"B","isMC":true,"discussion":[{"comment_id":"21580","timestamp":"1589467620.0","content":"Answer is B","upvote_count":"32","poster":"JoeShmoe"},{"poster":"Smart","comments":[{"poster":"fraloca","content":"https://cloud.google.com/solutions/reliable-task-scheduling-compute-engine#schedule-compute-engine","comment_id":"269747","upvote_count":"4","timestamp":"1626541320.0"}],"timestamp":"1598376720.0","content":"B is correct. More appropriately: https://cloud.google.com/solutions/reliable-task-scheduling-compute-engine","comment_id":"55141","upvote_count":"30"},{"poster":"xaqanik","comment_id":"1138446","timestamp":"1722588780.0","content":"Selected Answer: B\nYou can create Cron job using HTTP endpoint, Pub/Sub and App engine.","upvote_count":"1"},{"timestamp":"1716024120.0","content":"Selected Answer: B\nAnswer is B, but this question is outdated, Today the best practices for cron is Cloud Scheduler: fully managed enterprise-grade cron job scheduler\nhttps://cloud.google.com/scheduler/?gad_source=1&gclsrc=ds&gclsrc=ds","poster":"odacir","comments":[{"timestamp":"1733173020.0","poster":"JaimeMS","content":"Thanks... I was a little confused by this options","upvote_count":"2","comment_id":"1223252"}],"upvote_count":"20","comment_id":"1073949"},{"timestamp":"1712820300.0","content":"This seems to be an old question, despite B could be the more correct answer, it is not exactly a good one. 'Using the Cron service provided by App Engine', the cron service is provided by Cloud Scheduler, not App Engine. App Engine HTTP endpoint can be a target for the cron task.","poster":"JPA210","upvote_count":"9","comment_id":"1040299"},{"poster":"salim_","upvote_count":"3","timestamp":"1699093380.0","comment_id":"889292","content":"Selected Answer: B\nhttps://cloud.google.com/blog/products/gcp/reliable-task-scheduling-on-google-compute-engine"},{"comment_id":"837345","timestamp":"1694540640.0","poster":"rr4444","content":"Something feels missing/broken about this question\n\nEven before comments in discussion that correctly mentioned Cloud Scheduler, which is not mentioned in the question","upvote_count":"6","comments":[{"content":"This is because cloud scheduler is a newly released service which is a replacement to cloud app engine cron service.","upvote_count":"1","poster":"parthkulkarni998","timestamp":"1718517480.0","comment_id":"1097998"}]},{"upvote_count":"2","content":"\"By using Cloud Scheduler for scheduling and Pub/Sub for distributed messaging, you can build an application to reliably schedule tasks across a fleet of Compute Engine instances.\" https://cloud.google.com/architecture/reliable-task-scheduling-compute-engine\n\nAnswer is B. (Note: It was down to B or D but containerization was not mentioned)","comment_id":"779446","poster":"dataqueen_3110","timestamp":"1689634500.0"},{"comment_id":"762972","content":"Answer is B.\nCloud Scheduler provides a fully managed, enterprise-grade service that lets you schedule events. After you have scheduled a job, Cloud Scheduler will call the configured event handlers, which can be App Engine services, HTTP endpoints, or Pub/Sub subscriptions.\n\nTo run tasks on your Compute Engine instance in response to Cloud Scheduler events, you need to relay the events to those instances. One way to do this is by calling an HTTP endpoint that runs on your Compute Engine instances. Another option is to pass messages from Cloud Scheduler to your Compute Engine instances using Pub/Sub.","poster":"beehive","timestamp":"1688156700.0","upvote_count":"8"},{"poster":"habros","comment_id":"730710","upvote_count":"2","timestamp":"1685376840.0","content":"Selected Answer: B\nA and C are outâ€¦ messages are to be sent to pub sub and processed using a client. D is overkill for this purpose"},{"upvote_count":"1","poster":"megumin","comment_id":"715083","content":"Selected Answer: B\nB is ok","timestamp":"1683703080.0"},{"upvote_count":"1","timestamp":"1681589400.0","comment_id":"695695","poster":"AzureDP900","content":"B is right"},{"poster":"Nirca","content":"Selected Answer: B\nAns is B https://cloud.google.com/architecture/reliable-task-scheduling-compute-engine","comment_id":"671245","timestamp":"1679034780.0","upvote_count":"1"},{"comment_id":"663320","upvote_count":"1","comments":[{"timestamp":"1678896960.0","comments":[{"timestamp":"1681727340.0","comment_id":"697305","poster":"zr79","upvote_count":"2","content":"This is the new way to run schedule"}],"comment_id":"670003","poster":"zellck","upvote_count":"3","content":"the link points to use Cloud Scheduler, and not Cron service provided by App Engine."}],"content":"Ans is B https://cloud.google.com/architecture/reliable-task-scheduling-compute-engine\n\nrefer the examples with diagram","timestamp":"1678268340.0","poster":"Sbgani"},{"upvote_count":"5","timestamp":"1677665760.0","comment_id":"655977","poster":"FAD04","content":"I got this question in exam 01/09/2022"},{"content":"Selected Answer: D\nThis solution can be implemented using both A and D\n1) With App Engine - https://cloud.google.com/appengine/docs/flexible/nodejs/scheduling-jobs-with-cron-yaml\n2) With GKE - https://cloud.google.com/kubernetes-engine/docs/how-to/cronjobs\n\nThey ask for best practices and it's well known that GKE (aka containers) is the best practice for building modern infra solution. \n\nYet another confusing PCA question on the card. Honestly, think the quality of the questions can be mightily improved.","comments":[{"comment_id":"700789","upvote_count":"1","timestamp":"1682076300.0","content":"GKE is too expensive if all you are after is cron scheduling.","poster":"BiddlyBdoyng"},{"upvote_count":"2","comment_id":"652750","comments":[{"upvote_count":"1","content":"You right, but D is overkill. \nSo B if the best practices for this task.","timestamp":"1706351280.0","comment_id":"964498","poster":"kapara"}],"poster":"pp0709","content":"Sorry, can be implemented using both B and D","timestamp":"1677546060.0"},{"poster":"medi01","timestamp":"1697955180.0","upvote_count":"1","comment_id":"877065","content":"A is a bad solution as \"send message directly to the utility\" is not really reliable, you'd want pub/sub in between."}],"poster":"pp0709","upvote_count":"1","timestamp":"1677546000.0","comment_id":"652749"},{"timestamp":"1677389520.0","upvote_count":"2","content":"B says Appengine.\nBut Cloud Scheduler is itself a managed service.\nTo schedule jobs via AppEngine, the cron.yaml has to be used.\nIt can be done similarly via GKE as well.\nThis question is confusing","poster":"6721sora","comment_id":"652021"},{"comment_id":"646505","comments":[{"comment_id":"892670","timestamp":"1699512180.0","upvote_count":"1","poster":"Sur_Nikki","content":"So, did u passed? If yes then congratulations and please let us know the answer of this question"},{"comment_id":"724176","upvote_count":"1","content":"and? speak your thoughts","poster":"ale_brd_111","timestamp":"1684736880.0"}],"content":"I got this question in exam.","upvote_count":"6","poster":"ACE_ASPIRE","timestamp":"1676346300.0"},{"poster":"DrishaS4","content":"Selected Answer: B\nhttps://cloud.google.com/solutions/reliable-task-scheduling-compute-engine","timestamp":"1675567620.0","upvote_count":"1","comment_id":"642671"},{"content":"B is correct , there is no need of GKE for this scenario.","poster":"AzureDP900","upvote_count":"1","timestamp":"1672692120.0","comment_id":"626274"},{"timestamp":"1657371120.0","comments":[{"upvote_count":"2","poster":"pddddd","timestamp":"1657371180.0","comment_id":"520260","content":"That said, App Engine has a cron job capabilities, so out of the options, indeed B makes sense, although App Engine is still an overkill comparing to Cloud Scheduler..."},{"poster":"SHalgatti","timestamp":"1660087620.0","upvote_count":"1","content":"To use Cloud Scheduler App Engine must be enable at project level, else you can't use Cloud Scheduler","comment_id":"544227"}],"comment_id":"520259","poster":"pddddd","content":"none seem correct.\nYou do not need App Engine - Cloud Scheduler can call Pub/Sub directly...\nGKE seems like a massive overkill for job scheduling. Nonetheless, B seems the only choice that makes sense.","upvote_count":"4"},{"comment_id":"497412","upvote_count":"1","timestamp":"1654752840.0","content":"Go for B","poster":"haroldbenites"},{"comment_id":"491417","content":"B is the correct answer\nhttps://cloud.google.com/architecture/reliable-task-scheduling-compute-engine","timestamp":"1654065120.0","poster":"vincy2202","upvote_count":"2"},{"poster":"nqthien041292","content":"Selected Answer: B\nVote B","comment_id":"487822","timestamp":"1653613620.0","upvote_count":"1"},{"timestamp":"1646649720.0","comment_id":"440810","content":"B\n\nCloud Scheduler provides a fully managed, enterprise-grade service that lets you schedule events. After you have scheduled a job, Cloud Scheduler will call the configured event handlers, which can be App Engine services, HTTP endpoints, or Pub/Sub subscriptions.","upvote_count":"2","poster":"Bhagesh"},{"content":"GKE also provide cronjob\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/cronjobs","timestamp":"1643091480.0","comment_id":"413600","upvote_count":"1","poster":"Hemendra007"},{"timestamp":"1637415540.0","content":"B. Using the Cron service provided by App Engine, publish messages to a Cloud Pub/Sub topic. Subscribe to that topic using a message-processing utility service running on Compute Engine instances.","poster":"victory108","comment_id":"362096","upvote_count":"4"},{"timestamp":"1637072400.0","upvote_count":"1","content":"B is correct\nhttps://cloud.google.com/solutions/reliable-task-scheduling-compute-engine","poster":"un","comment_id":"358697"},{"upvote_count":"1","content":"Answer is B","poster":"Ausias18","comment_id":"325600","timestamp":"1633071780.0"},{"poster":"singlanikhil","content":"It should be B as per below link\nhttps://cloud.google.com/solutions/reliable-task-scheduling-compute-engine","comment_id":"322574","timestamp":"1632827940.0","upvote_count":"1"},{"comment_id":"320612","timestamp":"1632603960.0","content":"A is probably correct because the sender has to ensure that the receiver has got the message and it's not hanging in the Pub/Sub. If the subscriber is not up and attached to the topic then it's possible that the subscriber may miss it. So, it's not a reliable conversation in that terms.","poster":"AD3","upvote_count":"1"},{"poster":"bnlcnd","comment_id":"280254","content":"Both B & D work. Let's assume the Cron Service in D means CronJob.\nBut D requires more infra cost while B is simpler and easier.\n- B.","timestamp":"1627684620.0","upvote_count":"3"},{"content":"I am debating between A and B. The only concern I have in choosing B that Cloud Pub/Sub can send duplicate messages since it will send the message \"at least once\". However, the direct messaging option in A can also fail.","timestamp":"1626095160.0","comment_id":"265582","upvote_count":"1","poster":"HKim"},{"content":"B, but of course now you should use Cloud Scheduler.","poster":"mwilbert","timestamp":"1625440320.0","upvote_count":"2","comment_id":"259841"},{"content":"Answer is B friends","poster":"okixavi","upvote_count":"2","timestamp":"1623856020.0","comment_id":"245794"},{"content":"B is the correct answer , App engine with pubsub is worked best for scheduling the tasks","upvote_count":"1","comment_id":"209675","poster":"gcparchitect007","timestamp":"1619744940.0"},{"upvote_count":"1","comment_id":"195018","timestamp":"1617784800.0","poster":"homer_simpson","content":"A is correct also B"},{"upvote_count":"3","content":"B - https://cloud.google.com/solutions/reliable-task-scheduling-compute-engine","timestamp":"1615948800.0","comment_id":"180625","poster":"AshokC"},{"upvote_count":"1","poster":"Narendra1891","content":"B is the correct answer.","comment_id":"174452","timestamp":"1615030560.0"},{"timestamp":"1614202920.0","upvote_count":"1","content":"I would suggest to go with B","poster":"wiqi","comment_id":"165508"},{"content":"I don't think it's any of these. It should be Cloud Scheduler: https://cloud.google.com/solutions/reliable-task-scheduling-compute-engine","poster":"ry9280087","comment_id":"152218","timestamp":"1612662780.0","upvote_count":"3"},{"upvote_count":"2","comment_id":"110992","poster":"desertlotus1211","content":"Answer B:\nhttps://cloud.google.com/scheduler/docs/quickstart","timestamp":"1608061500.0"},{"comment_id":"108563","timestamp":"1607773980.0","upvote_count":"3","poster":"Rajuuu","content":"Answer is B.\nhttps://cloud.google.com/solutions/reliable-task-scheduling-compute-engine"},{"content":"Final Decision to go with Option B \nhttps://cloud.google.com/solutions/reliable-task-scheduling-compute-engine","timestamp":"1606575480.0","poster":"AD2AD4","upvote_count":"6","comment_id":"97559"},{"comment_id":"90979","timestamp":"1605678300.0","content":"B is the answer\n\nExact explanation is here https://cloud.google.com/solutions/reliable-task-scheduling-compute-engine","upvote_count":"4","poster":"sri30"},{"content":"ignore and compute engine reference when GKE is involved.","upvote_count":"1","comment_id":"86722","timestamp":"1605044880.0","poster":"asure"},{"timestamp":"1603564560.0","comment_id":"79234","poster":"serg3d","upvote_count":"1","content":"No mention about k8s. B is better than A"},{"poster":"Javed","content":"B seems more correct","upvote_count":"1","timestamp":"1599900120.0","comment_id":"62955"},{"content":"Answer - B","comment_id":"38119","timestamp":"1594557780.0","upvote_count":"6","poster":"AWS56"},{"content":"B is correct, as we'll need to use pub-sub","comment_id":"24914","timestamp":"1590614040.0","upvote_count":"4","poster":"jobs"},{"content":"It is A or B, https://cloud.google.com/appengine/docs/flexible/nodejs/scheduling-jobs-with-cron-yaml \n\nThere is no mention of Cron Service for Kubernetes in the Google Documentation","comment_id":"17969","comments":[{"upvote_count":"5","comment_id":"104897","content":"CronJob is an object type creatable in Kubernetes, but that would require a cluster, too much overhead for a simple thing. B is best.","timestamp":"1607389440.0","poster":"rehma017"},{"upvote_count":"2","poster":"JJu","timestamp":"1591534740.0","content":"Perhaps cronjob is cron of kubernetes version.\nthis question don't mention cronjob, only cron.","comment_id":"27628"}],"timestamp":"1588081200.0","poster":"Eroc","upvote_count":"5"},{"comments":[{"upvote_count":"1","timestamp":"1630800960.0","comment_id":"303839","content":"it is B","poster":"nitinz"},{"poster":"kumarp6","comment_id":"210600","timestamp":"1619879340.0","upvote_count":"3","content":"B is correct"}],"comment_id":"17518","content":"Think D is better because:\n- Perhaps you can't have all your apps in APP Engine \n- If you send directly messages to GCE and there is any availabity problem, message is lost. Otherwise message is in Pub/Sub, that has better availabity and performance than GCE VMs running messaging software","timestamp":"1587891420.0","poster":"jcmoranp","upvote_count":"1"}],"question_images":[]},{"id":"s1xlIXkxJMW0NN71TZPD","choices":{"C":"Lease a Transfer Appliance, upload archived files to it, and send it to Google to transfer archived data to Cloud Storage. Establish one Cloud VPN Tunnel to VPC networks over the public internet, and compress and upload files daily using the gsutil ×’â‚¬\"m option.","B":"Lease a Transfer Appliance, upload archived files to it, and send it to Google to transfer archived data to Cloud Storage. Establish a connection with Google using a Dedicated Interconnect or Direct Peering connection and use it to upload files daily.","D":"Lease a Transfer Appliance, upload archived files to it, and send it to Google to transfer archived data to Cloud Storage. Establish a Cloud VPN Tunnel to VPC networks over the public internet, and compress and upload files daily.","A":"Compress and upload both archived files and files uploaded daily using the gsutil ×’â‚¬\"m option."},"answer_description":"","answer":"B","answers_community":["B (90%)","5%"],"timestamp":"2019-10-09 06:42:00","question_images":[],"isMC":true,"answer_ET":"B","question_id":4,"url":"https://www.examtopics.com/discussions/google/view/6306-exam-professional-cloud-architect-topic-1-question-101/","exam_id":4,"question_text":"Your company is building a new architecture to support its data-centric business focus. You are responsible for setting up the network. Your company's mobile and web-facing applications will be deployed on-premises, and all data analysis will be conducted in GCP. The plan is to process and load 7 years of archived .csv files totaling 900 TB of data and then continue loading 10 TB of data daily. You currently have an existing 100-MB internet connection.\nWhat actions will meet your company's needs?","answer_images":[],"discussion":[{"comments":[{"timestamp":"1706870760.0","upvote_count":"1","content":"Also with option A you need months to download archive files(900TB)","poster":"xaqanik","comment_id":"1138439"},{"poster":"kumarp6","upvote_count":"5","content":"B is correct","comments":[{"timestamp":"1608299580.0","content":"ok but dedicated connection is available from 10 GBPS right where as in question it says internet connection is 100 MB, to me D is correct.","poster":"Jay_82","comments":[{"timestamp":"1654226040.0","poster":"9xnine","content":"Dedicated Interconnect will be a new connection and will not run over the existing internet connection. With dedicated interconnect the existing ISP becomes irrelevant. If you were trying to use VPN the existing internet connection would be relevant. Answer is B.","upvote_count":"6","comment_id":"610880"}],"upvote_count":"5","comment_id":"247353"}],"comment_id":"210602","timestamp":"1604248260.0"},{"comment_id":"303840","upvote_count":"7","timestamp":"1614910680.0","content":"it is B","poster":"nitinz"},{"upvote_count":"2","comment_id":"362382","timestamp":"1621531200.0","poster":"malequardos","comments":[{"timestamp":"1656087240.0","comment_id":"621760","poster":"NG123","upvote_count":"1","content":"True. B is the most apt answer with just this extra bit \"direct peering\" raising some confusion."}],"content":"Direct peering is meant only to connect to G Suite Services. Its reference may invalidate the whole answer."},{"poster":"tartar","comment_id":"152437","timestamp":"1596789780.0","upvote_count":"10","content":"B is ok"},{"content":"ok but dedicated connection is available from 10 GBPS right where as in question it says internet connection is 100 MB, to me D is correct.","upvote_count":"2","comments":[{"poster":"Alekshar","comment_id":"301887","timestamp":"1614677880.0","upvote_count":"4","content":"The 100MB is the internet connection Dedicated Interconnect means physically connecting the on-prem server and google with a new network cable, I do hope the internal company network is not limited to only 100MB"},{"timestamp":"1706870640.0","content":"Dedication interconnect doesn't connect with your internet. It is separate connection That is physical connection.","poster":"xaqanik","comment_id":"1138434","upvote_count":"1"}],"poster":"Jay_82","comment_id":"247351","timestamp":"1608299520.0"}],"content":"With option A, daily data would take 27 hours.\nMy answer is B.\nHow do you think?","upvote_count":"53","comment_id":"14367","timestamp":"1570596120.0","poster":"KouShikyou"},{"upvote_count":"21","comment_id":"16155","poster":"wk","content":"Agree B. 100Mbps connections for 10TB data transfer is takes too long\n\nhttps://cloud.google.com/solutions/transferring-big-data-sets-to-gcp#close","comments":[{"content":"not 100Mbps. 100MB","comment_id":"26491","poster":"JJu","upvote_count":"3","timestamp":"1575438540.0","comments":[{"upvote_count":"5","poster":"misho","comment_id":"100271","timestamp":"1591038420.0","content":"even with 100MB internet it's slow. It's 800 Mbps and transfer for 10 TB will take 2 days"},{"upvote_count":"5","content":"There is no such thing as a \"100MB\" internet connection :) . That must be a speed (per second), and I would guess that the \"B\" is just a typo (it is highly atypical to measure bandwidth in Bps).","comment_id":"296778","timestamp":"1614012000.0","poster":"bogd"}]}],"timestamp":"1571542260.0"},{"comment_id":"1363947","timestamp":"1740920040.0","upvote_count":"1","content":"Selected Answer: B\nOption B is the correct answer, but the term \"or Direct Peering connection\" is probably a typo. Direct peering is GCP to GCP service NOT an on-prem to GCP service.","poster":"halifax"},{"comment_id":"1334811","upvote_count":"1","timestamp":"1735650600.0","content":"Selected Answer: B\nLease a Transfer Appliance, upload archived files to it, and send it to Google to transfer archived data to Cloud Storage. (In this case is the more appropriated solution)\nEstablish a connection with Google using a Dedicated Interconnect or Direct Peering connection and use it to upload files daily. (Because of the size Dedicated interconnect or peering performs better and offer a better latency)","poster":"plumbig11"},{"timestamp":"1720711380.0","comment_id":"1246202","content":"Selected Answer: B\nThere is no option than B.","poster":"RickMorais","upvote_count":"1"},{"timestamp":"1716658320.0","upvote_count":"1","poster":"ManojNegi","comment_id":"1218471","content":"Selected Answer: B\nB is correct"},{"upvote_count":"1","timestamp":"1706213520.0","poster":"Devx198912233","comment_id":"1132026","content":"Selected Answer: B\nB would be correct as pubsub b service might redeliver messages. When you receive messages in order and the Pub/Sub service redelivers a message with an ordering key, Pub/Sub maintains order by also redelivering the subsequent messages with the same ordering key. The Pub/Sub service redelivers these messages in the order that it originally received them."},{"upvote_count":"1","timestamp":"1704603360.0","poster":"gun123","comment_id":"1115599","content":"Selected Answer: D\nB has an option of direct peering too which is not a recommended practise"},{"timestamp":"1701787980.0","comment_id":"1088577","upvote_count":"1","content":"Selected Answer: B\nB makes sense","poster":"BisoWafik"},{"upvote_count":"1","timestamp":"1700306880.0","comment_id":"1073953","poster":"odacir","content":"Selected Answer: B\nB. https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#time"},{"upvote_count":"1","comments":[{"poster":"odacir","upvote_count":"1","comment_id":"1073952","timestamp":"1700306820.0","content":"It's B, transferring over 100mbps 10TB daily is not possible, because could take up to 12 days.. You need a better connection."}],"timestamp":"1697441880.0","content":"Selected Answer: C\nI don't think the company needs \"dedicated intereconnect\" - because it is clearly said that the company wants to do \"data-centric\" business. Dedicated interconnect is more for having private-access to google cloud. \n\nC seems like a correct option to me","comment_id":"1044744","poster":"Arun_m_123"},{"timestamp":"1677511320.0","content":"Selected Answer: B\nAnswer B => Dedicated interconnect will provide a private network with 10gbs. The internet limited to 100 mb is not possible to use cloud VPN ( it will use public internet so be limited for the daily)","upvote_count":"2","poster":"telp","comment_id":"823883"},{"comment_id":"762518","content":"B is correct.","timestamp":"1672468920.0","upvote_count":"1","poster":"sunny2421"},{"comment_id":"730714","timestamp":"1669745760.0","content":"Selected Answer: B\nB. Since it is a new network just sign up for a dedicated lineâ€¦","upvote_count":"1","poster":"habros"},{"poster":"megumin","timestamp":"1668072780.0","content":"Selected Answer: B\nB is ok","comment_id":"715091","upvote_count":"1"},{"comment_id":"705207","timestamp":"1666843140.0","poster":"Balaji_Sakthi","upvote_count":"1","content":"its option B. i think"},{"timestamp":"1666056660.0","content":"you can not use gsutil to load 10TB daily >>>and then continue loading 10 TB of data daily<<< it will take longer than 24hrs to upload using gsutil","comment_id":"697783","upvote_count":"2","poster":"zr79"},{"comment_id":"695699","poster":"AzureDP900","timestamp":"1665864660.0","upvote_count":"2","content":"B is the best, VPN doesn't scale very well for huge data"},{"poster":"Nirca","content":"Selected Answer: B\nB is ok","timestamp":"1663389900.0","upvote_count":"1","comment_id":"671254"},{"content":"Selected Answer: B\nB is the answer.\n\nTo support daily 10TB upload, you need Dedicated Interconnect who has at least 10Gbps.","timestamp":"1663252200.0","upvote_count":"3","poster":"zellck","comment_id":"670013"},{"comment_id":"652752","poster":"pp0709","content":"Selected Answer: B\nKey is \"Daily transfer of 10 TB\"- Any data transfer that requires more than 3 Gigs in bandwidth, VPN is out of equation.","timestamp":"1661641440.0","comments":[{"content":"3Gps not 3Gigs bandwidth","timestamp":"1666056480.0","comment_id":"697781","upvote_count":"1","poster":"zr79"}],"upvote_count":"1"},{"upvote_count":"2","timestamp":"1659475080.0","content":"Answer is B. The question asks what action would the company need to take to support the requirement...","comment_id":"641460","poster":"desertlotus1211"},{"upvote_count":"1","comment_id":"626276","content":"I would go with B , VPN is not viable option to transfer that much date with 100mbps.","timestamp":"1656787680.0","poster":"AzureDP900"},{"content":"My answer is B.","upvote_count":"1","poster":"chelovalpo","comment_id":"609992","timestamp":"1654054800.0"},{"poster":"cpi_web","comments":[{"poster":"9xnine","upvote_count":"2","timestamp":"1654226100.0","content":"Dedicated Interconnect will be a new connection and will not run over the existing internet connection. With dedicated interconnect the existing ISP becomes irrelevant. If you were trying to use VPN the existing internet connection would be relevant. Answer is B.","comment_id":"610882"},{"poster":"michalsosn","comment_id":"976955","upvote_count":"1","timestamp":"1691606880.0","content":"I agree that in real life compressing a csv would probably reduce it's size by 100-1000x, making transfer over VPN connection actually feasible.\n\nBut test makers probably want B. Just as we don't know if direct interconnect is possible, we also don't know how effective compression is."}],"content":"Hmm. Not easy. Everybody tends to B. But I think that can't be correct for the simple reason...\n\nA is obviously not an option because of the initial amount of data. It could be that through compression the amount can be extremely reduced (I saw examples down to 5% of the original size). But the assumption at that point is, that the entropy is high => that means the compression is not necessarily the most important factor.\n\nB: The answer doesn't say anything about the reach-ability of the interconnect / peering (meaning IXP's). So one has to assume that they CAN'T be reached. That means that only C/D can be correct.\n\nI would tend to C because Google obviously would always recommend their own tools (gsutil). Then the hope would be that the compression and the bandwidth of the VPN is enough to get accomplish the job successfully.","upvote_count":"3","timestamp":"1652509500.0","comment_id":"601450"},{"upvote_count":"1","timestamp":"1649326440.0","content":"go for B cause \"900 TB of data and then continue loading 10 TB of data daily. You currently have an existing 100-MB internet connection.\"","poster":"gaojun","comment_id":"582355"},{"timestamp":"1642682400.0","poster":"Ombromanto69","comment_id":"528397","upvote_count":"1","content":"Selected Answer: B\nIt's B. Look at https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#transfer-options"},{"timestamp":"1641378060.0","poster":"OrangeTiger","comment_id":"517358","upvote_count":"1","content":"I think C is correct."},{"content":"gsutil is not a choice for large migration. Leasing Transfer Appliance for one off migation is an option. Use GCS for archieving. Now coming to connectivity: Cloud VPN is only good for low bandwidth work. Google recommends to use Interconnect over Peering. However, here both are under option B. So answer is B","timestamp":"1640784240.0","poster":"cloud_enthusiast_in","upvote_count":"1","comment_id":"512198"},{"comment_id":"497426","content":"Go for B.\n\"100-MB internet connection\" is for download. In this case, we are going to send data to the Cloud.","comments":[{"poster":"haroldbenites","comment_id":"497447","upvote_count":"1","timestamp":"1639038420.0","content":"Consider: \nThe gsutil tool is the standard tool for small- to medium-sized transfers (less than 1 TB)"}],"timestamp":"1639037100.0","poster":"haroldbenites","upvote_count":"1"},{"upvote_count":"1","comment_id":"491465","poster":"vincy2202","content":"Selected Answer: B\nB is the correct answer","timestamp":"1638349800.0"},{"timestamp":"1637979360.0","poster":"nqthien041292","content":"Selected Answer: B\nVote B","upvote_count":"1","comment_id":"487774"},{"upvote_count":"1","content":"B is the right answer","poster":"exam_war","timestamp":"1636250940.0","comment_id":"473702"},{"timestamp":"1634572560.0","comment_id":"464201","poster":"MaxNRG","upvote_count":"2","content":"B\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#close\n10 TB / 100 Mbps = 12 days"},{"comment_id":"378889","timestamp":"1623314700.0","upvote_count":"1","poster":"areza","content":"B is ok"},{"poster":"victory108","comment_id":"361865","upvote_count":"3","timestamp":"1621491120.0","content":"B. Lease a Transfer Appliance, upload archived files to it, and send it, and send it to Google to transfer archived data to Cloud Storage. Establish a connection with Google using a Dedicated Interconnect or Direct Peering connection and use it to upload files daily."},{"content":"B is correct","poster":"un","timestamp":"1621167780.0","upvote_count":"1","comment_id":"358701"},{"comment_id":"335297","content":"Answer is B.\n\nBreakdown plus solution:\n1- Process and load 7 years of archived files totalling 900TB (Cloud Storage/Archive storage class + Transfer Appliance to ship data)\n\n2- Continue loading 10TB data daily (Dedicated Interconnect - 10-200GBps line, typically you will compress files before uploading).","poster":"JohnWick2020","timestamp":"1618388400.0","upvote_count":"4"},{"poster":"Ausias18","comment_id":"325754","timestamp":"1617277920.0","content":"Answer is B","upvote_count":"1"},{"poster":"AD3","comment_id":"320621","timestamp":"1616715300.0","content":"B probably is correct. MB is notation for Mega Bytes. The internet connections are available up to 2000Mbps and more widely maximum rate available is 1000Mbps. Here 100 MB which means 800Mbps (considering 8 bits per byte) which is a realistic internet speed the company has currently. So 10TB=10000000MB/100MBsec=100000 sec/60sec=1666.66667min/60min=27.77777778hr. Which is more than 24 hours. Unrealistic on the internet with no connection drops!!!","upvote_count":"2"},{"comments":[{"poster":"Alekshar","timestamp":"1614678000.0","comment_id":"301889","content":"10GB is not the required bandwith but the technology that has to be supported by the physical network of the company. \nAs Dedicated Interconnect implies to create a new physical link between the company and Google, the bandwith for this connection will be faster than the current internet connection","upvote_count":"5"}],"poster":"GCB","content":"It cannot be B, as Dedicated interconnect/Direct Peering needs 10Gbps-80Gbps bandwidth. question clearly mentions available bandwidth is 100Mbps. Its between C and D.","timestamp":"1614385680.0","comment_id":"300010","upvote_count":"1"},{"upvote_count":"2","content":"answer is B","comment_id":"295092","timestamp":"1613830440.0","poster":"BobBui"},{"content":"Its B to answer 1st part of requirement and Dedicated interconnect supports from 50 MBPS to 50 GBPS . Refer below snippet from documentation \n Dedicated Interconnect \nFeatures \nA direct connection to Google.\nTraffic flows directly between networks, not through the public internet.\n10-Gbps or 100-Gbps circuits with flexible VLAN attachment capacities from 50 Mbps to 50 Gbps.\nReference: https://cloud.google.com/network-connectivity/docs/how-to/choose-product#:~:text=If%20you%20have%20high%20bandwidth,options%20starting%20at%2050%20Mbps","timestamp":"1613373420.0","comment_id":"290774","upvote_count":"1","poster":"guid1984"},{"comments":[{"timestamp":"1612054680.0","comment_id":"280259","upvote_count":"3","content":"daily 10T csv, can be easily compressed to 1T daily. with 100MB speed, 10K sec is enough to send the file. 1 day = 86400sec. Even with overhead, 100MB is enough for 1T file transfer.","poster":"bnlcnd"}],"poster":"bnlcnd","upvote_count":"3","timestamp":"1612054500.0","comment_id":"280256","content":"The company has only 100 MBps internet connection. so, interconnect will not help them. Cloud VPN is upda to 375MBps. It works well.\nC vs D: C has a typo says \"compares\" instead of \"compress\". \"gsutil cp -m\" can compress and upload the files. But that compare wording failed this option. So, only answer is :\nD"},{"content":"Answer is B guys","timestamp":"1608138540.0","comment_id":"245797","poster":"okixavi","upvote_count":"1"},{"upvote_count":"1","timestamp":"1604113860.0","content":"B is the correct answer, self explanatory.","poster":"gcparchitect007","comment_id":"209676"},{"content":"I will go with answer B, It will take 3 years to transfer 900 TB of data over 100 MBps line. https://cloud.google.com/solutions/migration-to-google-cloud-transferring-your-large-datasets","upvote_count":"1","timestamp":"1603893840.0","poster":"AdityaGupta","comment_id":"207821"},{"timestamp":"1602243600.0","content":"The catch here is that when you notice on-premises to GCP requirements means most definitely B is right. Transfer Applaince for transferring old 900TB and dedicated interconnect for daily transfer.","upvote_count":"2","poster":"LoganIsh","comment_id":"196698"},{"comment_id":"195075","upvote_count":"1","timestamp":"1602063900.0","content":"for me correct answer is B","poster":"homer_simpson"},{"upvote_count":"1","content":"B is right. Transfer Applaince for transferring old 900TB and dedicated interconnect for daily transfer.","timestamp":"1600303620.0","comment_id":"180627","poster":"AshokC"},{"comment_id":"178739","upvote_count":"3","content":"Answer is B; although the per tunnel capacity of Clould VPN is 3Gbps, but Internet bandwidth is only 100Mbps, which can only upload around 800GB data","timestamp":"1600003860.0","poster":"richardxyz"},{"poster":"Narendra1891","timestamp":"1599385320.0","upvote_count":"1","content":"B is correct.","comment_id":"174458"},{"timestamp":"1598911140.0","comment_id":"171053","upvote_count":"2","comments":[{"timestamp":"1598912940.0","poster":"haidertanveer0808","upvote_count":"2","comment_id":"171063","content":"10 TB over 100 MBps will take more than a day so B is best answer ."}],"content":"Best answer is C as Google Transfer Appliance is a physical appliance which is shipped to on Prem network , data copied and shipped back to Google for huge volume of data and used only for one Time Load . For daily load of data gsutil is preferred\n.","poster":"haidertanveer0808"},{"upvote_count":"2","content":"B is correct, as you need to transfer 10TB daily too.","comment_id":"165510","poster":"wiqi","timestamp":"1598298240.0"},{"upvote_count":"2","poster":"syu31svc","content":"The sheer amount of volume of data involved would mean that you have to transfer at very high speeds. So the answer is B for sure.","comment_id":"109872","timestamp":"1592110500.0"},{"comment_id":"106720","upvote_count":"1","timestamp":"1591786320.0","content":"B, for sure.","poster":"gfhbox0083"},{"poster":"Ziegler","content":"B is my own correct answer\nhttps://cloud.google.com/interconnect/docs/how-to/direct-peering","timestamp":"1591379940.0","upvote_count":"5","comment_id":"103324"},{"content":"Final Decision to go with Option B","upvote_count":"5","poster":"AD2AD4","comment_id":"97561","timestamp":"1590670860.0"},{"upvote_count":"2","timestamp":"1589140320.0","comment_id":"86725","content":"B, Interconnect","poster":"asure"},{"poster":"anton_royce","timestamp":"1585683780.0","content":"Answer C. 900 TB data - Transfer Appliance use for data over 100TB, public internet is available, so it should be VPN","comments":[{"upvote_count":"3","content":"10TB transfer can take more than 10 days over VPN connection 100 MB. My answer is B","timestamp":"1587754080.0","comment_id":"79236","comments":[{"upvote_count":"1","comments":[{"poster":"droidmasta","content":"3gbps per tunnel the capacity is way more than 100mbps, even more than 100mb/s. Cloud interconnect would be overkill IMO","upvote_count":"2","timestamp":"1598226960.0","comment_id":"164749"}],"timestamp":"1591611780.0","comment_id":"105155","poster":"rbrto","content":"VPN have limited bandwidth: 3Gbps, it is the worst choice"}],"poster":"serg3d"}],"upvote_count":"1","comment_id":"69938"},{"poster":"rickywck","upvote_count":"3","comment_id":"60355","timestamp":"1583596020.0","content":"Agree B"},{"content":"Agree B","timestamp":"1578840360.0","comment_id":"38122","poster":"AWS56","upvote_count":"4"},{"timestamp":"1571391600.0","comment_id":"15874","poster":"MeasService","content":".csv files can be compressed well. but even though, it wont be drastic reduction to 900TB data. So ideal hear is answer B. It also considers 10TB of daily data to be transferred.","upvote_count":"3"},{"timestamp":"1571147580.0","content":"It's depends on the compression. You are compressing before, you don't know how much it will occupy. But I'would choose B answer too. But I'm not sure","poster":"chiar","comment_id":"15433","upvote_count":"7"}],"topic":"1","unix_timestamp":1570596120},{"id":"CG3Q5NFxu6n9dGbfTjST","timestamp":"2019-10-18 11:45:00","question_text":"You are developing a globally scaled frontend for a legacy streaming backend data API. This API expects events in strict chronological order with no repeat data for proper processing.\nWhich products should you deploy to ensure guaranteed-once FIFO (first-in, first-out) delivery of data?","question_id":5,"discussion":[{"comments":[{"poster":"Clouddude123","comment_id":"1341316","upvote_count":"2","timestamp":"1736993580.0","content":"\"globally scaled frontend\" means Dataflow is required. If the clients were in 1 region only pushing messages, then Pub/Sub alone would be the solution. Since there are global clients, this requires Dataflow."},{"poster":"TiagoM","content":"Now Pub/Sub guarantees message order. Until the exam does not change I would pick B.","upvote_count":"10","comments":[{"comments":[{"timestamp":"1665443820.0","content":"I believe Pub/Sub now also supports exactly once delivery (in preview):\nhttps://cloud.google.com/pubsub/docs/exactly-once-delivery","upvote_count":"9","comment_id":"691561","poster":"emirhosseini"}],"upvote_count":"14","content":"Answer is B. The question is talking about guaranteed-once FIFO delivery of data. Although Pub/sub provides data in order (FIFO) but it does 'at-least' once delivery of data. So, we need Dataflow for deduplication of data.","timestamp":"1632976440.0","poster":"jask","comment_id":"454687"},{"poster":"melono","timestamp":"1665859860.0","content":"https://cloud.google.com/pubsub/docs/exactly-once-delivery\nreference","comment_id":"695620","upvote_count":"1"},{"comment_id":"695617","upvote_count":"9","comments":[{"timestamp":"1675005840.0","content":"I believe that only the frontend is scaled globally. The backend API is the one that requires ordered delivery of the messages and guaranteed-once delivery of data. Currently, Pub/Sub supports ordered delivery within the same region (https://cloud.google.com/pubsub/docs/ordering#receiving_messages_in_order) and exactly-once delivery within the same region (https://cloud.google.com/pubsub/docs/exactly-once-delivery#exactly-once_delivery_guarantees).\nThe right answer could be A, Pub/Sub alone.","upvote_count":"3","poster":"CosminCiuc","comment_id":"791767"}],"timestamp":"1665859800.0","poster":"melono","content":"Pub/Sub supports exactly-once delivery, within a cloud region.\nThe question states Â¨globalÂ¨, so needs Dataflow"}],"comment_id":"342226","timestamp":"1619298540.0"},{"timestamp":"1647186720.0","upvote_count":"1","comment_id":"566981","content":"the correct is B https://cloud.google.com/pubsub/docs/stream-messages-dataflow","poster":"zanfo"}],"upvote_count":"69","comment_id":"34574","timestamp":"1577976540.0","content":"I believe the answer is B. \"Pub/Sub doesn't provide guarantees about the order of message delivery. Strict message ordering can be achieved with buffering, often using Dataflow.\" https://cloud.google.com/solutions/data-lifecycle-cloud-platform","poster":"exampanic"},{"poster":"xhova","timestamp":"1588553040.0","content":"B is the answer. CloudSQL is only for storage, to get the messages in order you need timestamp processed in dataflow to arrange them before putting it in any storage volume. The system described is not querying a db it is expecting a stream of messages only dataflow can correct the order. ACID has no value here because the db is not being queried. You'll not find any documentation on pub/sub order being corrected with a db. See notes below on pub/sub and dataflow using timestamps and windows to ensure order\n\nhttps://cloud.google.com/pubsub/docs/pubsub-dataflow","upvote_count":"28","comment_id":"83324"},{"poster":"neokyle","timestamp":"1738618020.0","content":"Selected Answer: B\nAnswer is still B (Pub/Sub + Dataflow), because https://cloud.google.com/pubsub/docs/exactly-once-delivery#regional_considerations \nmentions \"The exactly-once delivery guarantee only applies when subscribers connect to the service in the same region. If your subscriber application is spread across multiple regions, it can lead to duplicate message delivery, even when exactly-once delivery is enabled.\"\n(so the exactly-once delivery only works regionally, but the question says architect for global)\nThe question's constraint of a globally scaled frontend, means you'll have clients in multiple regions.","comment_id":"1351114","upvote_count":"1"},{"upvote_count":"1","poster":"plumbig11","content":"Selected Answer: B\nCloud Pub/Sub to Dataflow because the data is streaming.","comment_id":"1334812","timestamp":"1735650600.0"},{"upvote_count":"2","poster":"selected","content":"Selected Answer: B\nhttps://cloud.google.com/pubsub/docs/exactly-once-delivery#regional_considerations","timestamp":"1731437280.0","comment_id":"1310840"},{"upvote_count":"4","comment_id":"1255891","content":"Selected Answer: A\nGoogle Cloud Pub/Sub now supports message ordering, which ensures that messages with the same ordering key are delivered in the exact order they were published. This feature addresses the requirement for strict chronological order without the need for additional services.\n\nKey Features of Cloud Pub/Sub with Message Ordering:\nMessage Ordering: By using ordering keys, Pub/Sub can guarantee that messages are delivered in the order they are published.\nExactly-once Delivery: Pub/Sub supports at-least-once delivery and can be configured to handle duplicate messages.\nScalability and Reliability: Pub/Sub is a fully managed service that scales automatically and ensures high availability.","poster":"awsgcparch","timestamp":"1722024300.0"},{"upvote_count":"6","poster":"19040e5","content":"Selected Answer: A\nA. Cloud Pub/Sub alone\n\nCloud Pub/Sub Ordered Delivery: Cloud Pub/Sub natively supports ordered delivery when using the same ordering key. This guarantees that messages with the same key are delivered to subscribers in the order they were published, preventing out-of-order events.\nExactly-once Delivery: Cloud Pub/Sub also offers exactly-once delivery within a region, ensuring that each message is delivered to a subscriber only once.","comment_id":"1214332","timestamp":"1716208440.0"},{"comment_id":"1212626","content":"Selected Answer: B\nPub/Sub provide ordered delivery but doesn't ensure deduplication. Cloud SQL is regional resource and it cant perform custom logic. I go with B","timestamp":"1715904060.0","poster":"hitmax87","upvote_count":"1"},{"comment_id":"1165098","content":"Selected Answer: A\nA is correct. While Cloud Dataflow can be used for data processing, it doesn't guarantee FIFO order on its own. Additionally, introducing another processing layer adds complexity and might not be necessary for this specific requirement.","upvote_count":"2","poster":"mesodan","timestamp":"1709500800.0"},{"comment_id":"1149760","timestamp":"1707875400.0","poster":"yas_cloud","content":"Selected Answer: B\nDont see a need for SQL here as the answer suggests. Option B is more appropriate","upvote_count":"1"},{"upvote_count":"2","comment_id":"1132028","timestamp":"1706213640.0","poster":"Devx198912233","content":"Selected Answer: B\nB would be correct as pubsub b service might redeliver messages. When you receive messages in order and the Pub/Sub service redelivers a message with an ordering key, Pub/Sub maintains order by also redelivering the subsequent messages with the same ordering key. The Pub/Sub service redelivers these messages in the order that it originally received them."},{"poster":"Pime13","timestamp":"1705843560.0","comment_id":"1127828","content":"Selected Answer: B\nb, even if pubsub now have exactly-once-delivery this is within a region. question is for a global app.\nhttps://cloud.google.com/pubsub/docs/exactly-once-delivery#exactly-once_delivery_2","upvote_count":"4"},{"upvote_count":"1","timestamp":"1698026580.0","poster":"theBestStudent","content":"Answer is B.\nKey words: globally scaled and ensure delivery only once: Pubsub + Dataflow.\n\nIf it were only one region, it would be fine to say just pubsub, but it is globally scaled.","comment_id":"1051347"},{"poster":"Frusci","comment_id":"1002452","timestamp":"1694176080.0","upvote_count":"2","content":"Selected Answer: B\nB, you need dataflow to deduplicate. Pub/Sub does \"at-least\" once delivery."},{"content":"Selected Answer: A\nIt should be A","upvote_count":"1","comment_id":"980382","poster":"AL_everyday","timestamp":"1691975940.0"},{"content":"Selected Answer: A\nhttps://cloud.google.com/pubsub/docs/exactly-once-delivery","poster":"jlambdan","timestamp":"1686596220.0","upvote_count":"2","comment_id":"921665"},{"content":"Answer: B","poster":"AmarReddy","timestamp":"1686136800.0","upvote_count":"1","comment_id":"917147"},{"poster":"TheCloudGuruu","timestamp":"1684169100.0","upvote_count":"1","content":"Selected Answer: B\nPub/Sub and Dataflow","comment_id":"898518"},{"upvote_count":"1","timestamp":"1682152800.0","comment_id":"877153","poster":"Hisayuki","content":"Selected Answer: B\nPub/Sub is an at-least-once service. So you need to deduplicate messages with DataFlow as a data pipeline."},{"timestamp":"1681286220.0","content":"Selected Answer: B\nB\nCloud Pub/Sub to Cloud Dataflow: Cloud Dataflow is a managed data processing service that can be used to process and transform streaming data. By using Cloud Dataflow with Cloud Pub/Sub, you can implement custom processing logic to ensure data is delivered in strict chronological order with no repeats. This combination allows you to achieve guaranteed-once FIFO delivery.","comment_id":"868041","poster":"JC0926","upvote_count":"3"},{"comments":[{"timestamp":"1687910820.0","poster":"enado","upvote_count":"3","comment_id":"935916","content":"The Answer should be B simply because the question specifically stated that the solution will be globally scaled. \n\nthis same link states:\n\"If messages have the same ordering key and are in the same REGION, you can enable message ordering and receive the messages in the order that the Pub/Sub service receives them.\"\n\nhttps://cloud.google.com/pubsub/docs/ordering#receiving_messages_in_order"}],"comment_id":"853522","poster":"jlambdan","content":"Selected Answer: A\nhttps://cloud.google.com/pubsub/docs/ordering","timestamp":"1680027360.0","upvote_count":"1"},{"upvote_count":"2","poster":"8d31d36","timestamp":"1676511360.0","comment_id":"810166","content":"Selected Answer: B\ncloud sql does not gurantee FIFO, but cloud data flow does on real time streams and historical batch data"},{"upvote_count":"2","poster":"zerg0","timestamp":"1676242320.0","comment_id":"806875","content":"Selected Answer: A\nPub/Sub covers the order and the exact once delivery."},{"timestamp":"1672756020.0","upvote_count":"7","comment_id":"764739","poster":"jay9114","comments":[{"upvote_count":"5","content":"the messages need to be from a single region for pub/sub ordering to work.","timestamp":"1676643960.0","poster":"honmet","comment_id":"811974"}],"content":"Selected Answer: A\nI choose A. As of today, PubSub can single-handedly deal with ensuring the messages are delivery once and in order. \n\nOnly once deliver - https://cloud.google.com/pubsub/docs/exactly-once-delivery\nOrdering of messages - https://cloud.google.com/pubsub/docs/ordering#receiving_messages_in_order"},{"poster":"omermahgoub","timestamp":"1671706680.0","content":"To ensure guaranteed-once FIFO (first-in, first-out) delivery of data from a globally scaled frontend to a legacy streaming backend data API, you can use Cloud Pub/Sub in combination with Cloud Dataflow.\n\nCloud Pub/Sub is a fully-managed messaging service that enables you to send and receive messages between independent systems. It provides a range of features to ensure the delivery of messages, including at-least-once delivery, ordering guarantees, and message deduplication.\n\nCloud Dataflow is a fully-managed, cloud-native data processing service that allows you to build and deploy data processing pipelines that can process unbounded streams of data. It can be used to process data from Cloud Pub/Sub in a streaming fashion, ensuring that messages are processed in strict chronological order with no repeat data.\n\nTogether, Cloud Pub/Sub and Cloud Dataflow can provide a reliable, scalable solution for delivering data from a globally scaled frontend to a legacy streaming backend data API, ensuring guaranteed-once FIFO delivery of the data.","upvote_count":"8","comment_id":"753199"},{"upvote_count":"1","poster":"[Removed]","content":"With Cloud Pub/Sub FIFO all transactions will be sequenced in the same order as transactions are received and Cloud Dataflow will ensure exactly once processing.","comment_id":"731151","timestamp":"1669788060.0"},{"timestamp":"1669679880.0","upvote_count":"6","content":"Selected Answer: A\nOnly once deliver - https://cloud.google.com/pubsub/docs/exactly-once-delivery\nOrdering of messages - https://cloud.google.com/pubsub/docs/ordering#receiving_messages_in_order\n\nboth supported by Cloud Pubsub","poster":"SureshbabuK","comment_id":"729732"},{"upvote_count":"1","comment_id":"719182","content":"B is correct","poster":"Tutun_Aus","timestamp":"1668554460.0"},{"timestamp":"1665864720.0","upvote_count":"1","poster":"AzureDP900","content":"B is right","comment_id":"695700"},{"upvote_count":"2","comment_id":"671841","timestamp":"1663455240.0","content":"Selected Answer: B\nDataflow deduplicates messages with respect to the Pub/Sub message ID","poster":"satyagade"},{"upvote_count":"2","comment_id":"670014","timestamp":"1663252440.0","content":"Selected Answer: B\nB is the answer.\n\nhttps://cloud.google.com/blog/products/data-analytics/handling-duplicate-data-in-streaming-pipeline-using-pubsub-dataflow","poster":"zellck"},{"comment_id":"655748","upvote_count":"2","timestamp":"1662003660.0","poster":"anilcruise","content":"Selected Answer: A\non pub as Create subscriptions with exactly-once delivery\nYou can create a subscription with exactly-once delivery using the Google Cloud console, the Google Cloud CLI, client library, or the Pub/Sub API.","comments":[{"comment_id":"662160","poster":"kiappy81","timestamp":"1662535500.0","upvote_count":"2","content":"you are right regarding pub/sub order but question says that this fronted is global and so it means that you cannot rely on pub/sub alone. \nInfact if messages have the same ordering key and are in the same region, you can enable message ordering and receive the messages in the order that the Pub/Sub service receives them. https://cloud.google.com/pubsub/docs/ordering"}]},{"upvote_count":"3","content":"Selected Answer: B\nCloud Dataflow helps in exactly once processing.\nhttps://cloud.google.com/blog/products/data-analytics/after-lambda-exactly-once-processing-in-google-cloud-dataflow-part-1","timestamp":"1660896180.0","poster":"RitwickKumar","comment_id":"648805"},{"content":"Selected Answer: B\nAns B is still valid in 2022\nPub/Sub now provides an argument --enable-message-ordering but dataflow is still needed to ensure the \"no repeat data\" element.","poster":"Mikado211","comment_id":"647483","timestamp":"1660630320.0","upvote_count":"2"},{"content":"B is the only right answer. Pub/Sub doesn't provide guarantees about the order of message delivery. Message ordering can be achieved using Dataflow.","upvote_count":"2","timestamp":"1656788160.0","comment_id":"626278","poster":"AzureDP900"},{"comment_id":"621764","timestamp":"1656087480.0","upvote_count":"1","content":"Selected Answer: B\nCloud Pub/Sub to Cloud Dataflow is the correct answer keeping in mind the FIFO anf no-repeat keywords.","poster":"NG123"},{"content":"Selected Answer: B\nData Flow is crucial in FIFO","poster":"MarcExams","upvote_count":"1","comment_id":"617763","timestamp":"1655476260.0"},{"comment_id":"609989","poster":"chelovalpo","comments":[{"comment_id":"609990","upvote_count":"1","timestamp":"1654054620.0","poster":"chelovalpo","content":"By ordering key"}],"content":"Selected Answer: A\nPub/sub guarentee ordering.","timestamp":"1654054560.0","upvote_count":"2"},{"timestamp":"1654052820.0","content":"The answer B is n't. Because it requires a database like Bigquery, CloudSQL or Other. Other hand, The answer D is probably, but Cloud Pub/Sub has a new feature that is ORDERING...please check this link https://cloud.google.com/pubsub/docs/ordering\n\nI use that feature 1,5 years ago.","upvote_count":"2","comment_id":"609982","poster":"chelovalpo"},{"poster":"xiaofeng_0226","timestamp":"1653910140.0","comment_id":"609155","upvote_count":"1","content":"Selected Answer: B\nchoose B"},{"comment_id":"576254","upvote_count":"1","content":"Bâ€¦\nhttps://cloud.google.com/blog/products/data-analytics/handling-duplicate-data-in-streaming-pipeline-using-pubsub-dataflow#:~:text=Your%20publisher%20when%20publishing%20messages,successfully%20published%20to%20a%20topic.","poster":"SAMBIT","timestamp":"1648392060.0"},{"timestamp":"1646482080.0","content":"Selected Answer: B\nB it is","poster":"ss909098","comment_id":"561411","upvote_count":"1"},{"upvote_count":"4","content":"Selected Answer: B\nIf messages have the same ordering key and are in the same region, you can enable message ordering and receive the messages in the order that the Pub/Sub service receives them.\nThis is for a Globally scaled App -- So Pub Sub Message Ordering alone cannot be used -- B it is","timestamp":"1643277900.0","poster":"KevPinto","comment_id":"533650"},{"content":"Selected Answer: B\nThis B","poster":"AGHPE","comment_id":"517490","upvote_count":"1","timestamp":"1641391140.0"},{"timestamp":"1639215840.0","upvote_count":"2","poster":"duhhh","comment_id":"499285","content":"Selected Answer: B\nVote B"},{"content":"Go for B","timestamp":"1639039080.0","comment_id":"497453","upvote_count":"1","poster":"haroldbenites"},{"upvote_count":"2","timestamp":"1638801120.0","poster":"pnvijay","content":"Selected Answer: B\nVote B","comment_id":"495203"},{"poster":"pakilodi","content":"Selected Answer: B\nVote B","comment_id":"494403","upvote_count":"2","timestamp":"1638713340.0"},{"poster":"vincy2202","upvote_count":"1","timestamp":"1638358020.0","content":"B seems to be the correct answer.","comment_id":"491581"},{"upvote_count":"2","poster":"nqthien041292","timestamp":"1637982480.0","content":"Selected Answer: B\nVote B","comment_id":"487823"},{"content":"Selected Answer: B\nB is right answer","timestamp":"1637781000.0","poster":"iobluedot","comment_id":"486197","upvote_count":"1"},{"timestamp":"1637625540.0","content":"Selected Answer: B\nB is right answer","upvote_count":"2","comment_id":"484630","poster":"[Removed]"},{"content":"Answer is B. \"Pub/Sub doesn't provide guarantees about the order of message delivery. Strict message ordering can be achieved with buffering, often using Dataflow.\"","comment_id":"469431","poster":"rexo","timestamp":"1635447120.0","upvote_count":"4"},{"poster":"rottzy","upvote_count":"2","content":"streaming = dataflow","comment_id":"460658","timestamp":"1633963920.0"},{"comments":[{"poster":"Lorenzot92","comments":[{"content":"You need a Database OLTP or OLAP for that link Bigquery or other","poster":"chelovalpo","upvote_count":"1","timestamp":"1654052940.0","comment_id":"609985"}],"comment_id":"455287","timestamp":"1633064280.0","upvote_count":"2","content":"I'm not agree.\nPub/syb have introduced the ordering message, but not guarantee that these message are processed in order.\nMoreover, if you confirm to receive one or multiple message in a middle of chain, pub/sub will resend all chain of message in order.\nSo question require FIFO, that can be satisfied by pub/sub, but to avoid duplication on streaming data, I think Datafllow is required for deduplication.\nFor me remain B also on Q3 2021.\nSome links:\nhttps://cloud.google.com/pubsub/docs/ordering\nhttps://cloud.google.com/dataflow/docs/concepts/streaming-with-cloud-pubsub#efficient_deduplication"}],"comment_id":"440950","upvote_count":"10","content":"Pub/Sub has recently introduced Message ordering. So answer would be A if responded to in third quarter of 2021. Prior to that you'd have to use a persistent store like Cloud SQL to manage the ordering aspect.","poster":"pr2web","timestamp":"1631022720.0"},{"timestamp":"1626064080.0","comment_id":"404381","content":"I go for B too","upvote_count":"1","poster":"Papafel"},{"poster":"areza","upvote_count":"1","comment_id":"378890","content":"B is ok","timestamp":"1623314760.0"},{"poster":"victory108","upvote_count":"5","content":"B. Cloud Pub/Sub to Cloud DataFlow","comment_id":"361863","timestamp":"1621491000.0"},{"comment_id":"358713","content":"B is correct","poster":"un","timestamp":"1621168920.0","upvote_count":"1"},{"poster":"getzsagar","timestamp":"1617871200.0","upvote_count":"3","comment_id":"331005","content":"Answer B - Data Flow Features - Fully managed data processing service\n\nAutomated provisioning and management of processing resources\n\nHorizontal autoscaling of worker resources to maximize resource utilization\n\nOSS community-driven innovation with Apache Beam SDK\n\nReliable and consistent exactly-once processing"},{"upvote_count":"1","poster":"Ausias18","content":"Answer is B","timestamp":"1617277980.0","comment_id":"325755"},{"comment_id":"290777","upvote_count":"5","content":"Correct Answer is B : Cloud Pub/Sub and DataFlow. Refer below from google documentation: \n\"https://cloud.google.com/dataflow/docs/concepts/streaming-with-cloud-pubsub\"\nCloud DataFlow solves duplication issue of Cloud Pub/Sub \n\"Efficient deduplication\nMessage deduplication is required for exactly-once message processing. Dataflow deduplicates messages with respect to the Pub/Sub message ID. As a result, all processing logic can assume that the messages are already unique with respect to the Pub/Sub message ID. The efficient, incremental aggregation mechanism to accomplish this is abstracted in the PubsubIO API.\n\nIf PubsubIO is configured to use custom message IDs, Dataflow deduplicates messages by maintaining a list of all the custom IDs it has seen in the last 10 minutes. If a new message's ID is in this list, the message is assumed to be a duplicate and discarded.\"","poster":"guid1984","timestamp":"1613374320.0"},{"timestamp":"1613147160.0","poster":"Mitra123","comments":[{"upvote_count":"1","content":"Pub/Sub + Dataflow do both. B is ok.","comment_id":"324935","timestamp":"1617183300.0","poster":"lynx256"}],"comment_id":"289005","content":"Guys, there are two requirements: 1. Ordering and 2. Strict non duplication of data. Pub sub can offer ordering but non duplication,- which can only be achieved by a database with a primary or an unique key. D is the correct answer.","upvote_count":"4"},{"timestamp":"1612057260.0","content":"This question is not worth to look at. it's very vauge. not anything related to real life. I think the guy who made this question is trying to let you choose B.\nYou data need to end up somewhere. dataflow is still a step in the middle. cloud SQL could be the end state. but, there are many missing pieces in between.","poster":"bnlcnd","comment_id":"280280","upvote_count":"2"},{"timestamp":"1611213840.0","comment_id":"272641","poster":"ahmedemad3","content":"Ans: B\nStreaming with Dataflow and PUB/SUB for ordering messaging","upvote_count":"1"},{"comment_id":"259801","content":"I recon is B.\nDataflow compliments Pub/Sub's scalable, at-least-once delivery model with message deduplication and exactly-once, in-order processing if you use windows and buffering.\nhttps://cloud.google.com/dataflow/docs/concepts/streaming-with-cloud-pubsub","upvote_count":"3","timestamp":"1609802940.0","poster":"sealvarezmx"},{"comment_id":"252186","content":"It should be B","upvote_count":"1","timestamp":"1608916260.0","poster":"Prakzz"},{"comment_id":"250431","content":"The messages are to be \"processed\", not stored, so C&D are out. Between A & B, Pub/Sub allows you to enable message ordering, that should help with the FIFO requirement, but if messages are redelivered by PubSub, we need an interface in between to handle that case - and that would be DataFlow, for that reason, I have to go with B.","timestamp":"1608670860.0","poster":"lxqus","upvote_count":"3"},{"comment_id":"247248","upvote_count":"1","timestamp":"1608286080.0","poster":"Reki6","content":"It is B"},{"content":"Answer is B:\nOrdering of messages is provided by Pub/Sub:\nhttps://cloud.google.com/pubsub/docs/faq#order\nDeduplication is done using Dataflow:\nhttps://cloud.google.com/dataflow/docs/concepts/streaming-with-cloud-pubsub#integration-features","timestamp":"1606590900.0","comment_id":"229821","poster":"OSNG","upvote_count":"2"},{"poster":"TaherShaker","comment_id":"221857","upvote_count":"1","timestamp":"1605702600.0","content":"The answer is B:\nhttps://stackoverflow.com/questions/53823366/google-pubsub-and-duplicated-messages-from-the-topic"},{"poster":"Chulbul_Pandey","upvote_count":"1","timestamp":"1605417360.0","comment_id":"219505","content":"B, as Dataflow supports \"exactly\" once processing."},{"upvote_count":"2","content":"B. Answer here \"Dataflow compliments Pub/Sub's scalable, at-least-once delivery model with message deduplication and exactly-once, in-order processing if you use windows and buffering.\" https://cloud.google.com/dataflow/docs/concepts/streaming-with-cloud-pubsub","poster":"ccie_pgh","timestamp":"1605231480.0","comment_id":"218215"},{"content":"Guys, how you can ensure order by using Cloud SQL??? Any suggestions?\nIt's Dataflow, definitely. This best choice for creating time windows and ordering messages within it.","timestamp":"1604737680.0","poster":"practicioner","comment_id":"214478","upvote_count":"2"},{"content":"Its B this is the exact answer \nPub/Sub doesn't provide guarantees about the order of message delivery. Strict message ordering can be achieved with buffering, often using Dataflow\n\nhttps://cloud.google.com/solutions/data-lifecycle-cloud-platform","upvote_count":"2","comment_id":"212473","poster":"francisco_guerra","timestamp":"1604465640.0"},{"upvote_count":"3","comment_id":"209677","content":"B is the correct answer, pub-sub, and cloud dataflow","timestamp":"1604113980.0","poster":"gcparchitect007"},{"timestamp":"1602502260.0","upvote_count":"6","poster":"andzuk","comment_id":"198361","content":"Answer B:\n\n\"Dataflow compliments Pub/Sub's scalable, at-least-once delivery model with message deduplication and exactly-once, in-order processing if you use windows and buffering. \"\n\nhttps://cloud.google.com/dataflow/docs/concepts/streaming-with-cloud-pubsub#efficient_deduplication"},{"content":"correct answer is D..","timestamp":"1601803440.0","upvote_count":"2","comment_id":"192805","poster":"akhadar2001"},{"timestamp":"1601716860.0","comment_id":"192132","poster":"LalBisota","content":"B is the correct answer. \nCloud Pub/sub also provide ordering of messaging at a cost of latency. see this: https://cloud.google.com/pubsub/docs/ordering#console\nSo A could be the answer but the problem is that it can redeilver the messages so need to send only a copy of the message so B is the answer.","upvote_count":"2"},{"comment_id":"188343","poster":"shajusn","timestamp":"1601209380.0","upvote_count":"1","content":"Pub/Sub doesn't provide guarantees about the order of message delivery. Strict message ordering can be achieved with buffering, often using Dataflow.(https://cloud.google.com/solutions/data-lifecycle-cloud-platform)"},{"timestamp":"1600303980.0","comment_id":"180628","content":"B is correct. Pub/Sub and Dataflow","poster":"AshokC","upvote_count":"1"},{"upvote_count":"3","poster":"varuneshwar","content":"B - If this question was added before August 2020. \nA - if its post, as with the new ordering in place we can use simple pub/sub to manage to order.","comments":[{"content":"Only 'order' is newly introduced. Repeat of data is still possible. PubSub delivers data ATLEAST once. It may send data multiple times in few cases. I'll go with B.","upvote_count":"1","comment_id":"186388","poster":"JyoGCP","timestamp":"1600968420.0"}],"timestamp":"1599559500.0","comment_id":"175800"},{"comment_id":"171479","content":"why not simple A?\nif a publisher sends two messages with the same ordering key, the Pub/Sub service delivers the oldest message first.","upvote_count":"4","timestamp":"1598982120.0","poster":"navi86"},{"poster":"Kabiliravi","comment_id":"168861","upvote_count":"3","timestamp":"1598661960.0","content":"Cloud SQL has no guarantee \nB is correct"},{"upvote_count":"1","timestamp":"1598298480.0","comment_id":"165513","content":"B makes sense.","poster":"wiqi"},{"content":"It's B. The requirement is -- \"API expects events in strict chronological order with no repeat data\" and Cloud Dataflow does that. https://cloud.google.com/dataflow/docs/concepts/streaming-with-cloud-pubsub","upvote_count":"2","poster":"GMarqz","comment_id":"164259","timestamp":"1598170380.0"},{"poster":"Krishna2401","upvote_count":"2","timestamp":"1598025300.0","content":"answer is B: Dataflow pipelines is Google Cloud Pub/Sub. Cloud Pub/Sub is a non-deterministic source: multiple subscribers can pull from a Cloud Pub/Sub topic but which subscribers receive a given message is unpredictable. If processing fails, Cloud Pub/Sub will redeliver messages but the messages might be delivered to different workers than those that processed them originally and in a different order. This non-deterministic behavior means that Cloud Dataflow needs assistance for detecting duplicates because there's no way for the service to deterministically assign record ids that will be stable upon retry. (Weâ€™ll dive into a more detailed use case involving Cloud Pub/Sub later.)","comment_id":"163061"},{"comment_id":"156851","poster":"viiqdozal","timestamp":"1597280400.0","upvote_count":"2","content":"B is the correct answer - DataFlow helps with the correct ordering of the data and processing of duplicates before sending anywhere else"},{"comment_id":"147460","upvote_count":"1","poster":"dipenich","content":"I think the answer is B, CloudSQL is no legacy backend. It has to go through DataFlow for Streaming in chronoogical Order","timestamp":"1596118260.0"},{"content":"I agree D.\nhttps://cloud.google.com/pubsub/docs/ordering#how_should_i_handle_order","upvote_count":"2","poster":"mlantonis","comment_id":"117569","timestamp":"1592924100.0"},{"comment_id":"110994","upvote_count":"3","poster":"desertlotus1211","content":"Answer D:\nhttps://cloud.google.com/pubsub/docs/ordering\n\n'If order of processing is important, then subscribers would need to coordinate through some ACID storage system such as Cloud Firestore or Cloud SQL.'","timestamp":"1592243580.0"},{"comment_id":"99516","timestamp":"1590950400.0","content":"B. Dataflow can buffer and align (use Dataflow's Windowing and Triggers features to aggregate data into Windows based on event-time. )\nD - Not able to follow how CloudSQL will solve this for streaming API","poster":"nrajesh","upvote_count":"3"},{"poster":"AD2AD4","comment_id":"97568","timestamp":"1590671520.0","upvote_count":"6","content":"Final Decision to go with Option B as Dataflow supports late arrival of data and PubSub- Dataflow is a good streaming pattern. \nRefer - https://cloud.google.com/blog/products/gcp/after-lambda-exactly-once-processing-in-cloud-dataflow-part-3-sources-and-sinks"},{"content":"I agree with D","upvote_count":"3","comment_id":"97002","poster":"RajGoog","timestamp":"1590608880.0"},{"timestamp":"1590333360.0","poster":"Ziegler","content":"Why is the answer not B because the question says \" legacy streaming backend data API\" and not database. Data streaming in GCP is dataflow. I strongly go with B. CloudSQL is for transactional database which was not mentioned in the question.","upvote_count":"5","comment_id":"94979"},{"comment_id":"84060","poster":"Ashutosh007","timestamp":"1588675140.0","content":"D should be correct\n\nhttps://cloud.google.com/pubsub/docs/ordering","upvote_count":"2"},{"timestamp":"1588646880.0","content":"Dataflow\nUnified stream and batch data processing that's serverless, fast, and cost-effective.\n\nFor Cloud SQL you need additional App.\nDataflow you can use directly with Pub/Sub https://cloud.google.com/pubsub/docs/pubsub-dataflow.\n\nAnswer B.","poster":"Zarmi","comment_id":"83910","upvote_count":"1"},{"content":"Answer D appears to be the correct option according to the provided. I have also copied the following from the page to outline answer D:\n\nIn this case, even if messages come through the subscription in order, there are no guarantees of the order in which the messages will be processed by your subscribers. If order of processing is important, then subscribers would need to coordinate through some ACID storage system such as Cloud Firestore or Cloud SQL.","comment_id":"52682","poster":"jonclem","timestamp":"1582130580.0","upvote_count":"3"},{"comment_id":"50306","comments":[{"poster":"YuriP","comment_id":"68115","timestamp":"1585137120.0","upvote_count":"2","content":"Then you need an app for insertions in to SQL. Dataflow is, basically, this missing app. And then, you don't need SQL, you can just stream out further."},{"content":"SQL is only for legacy apps still using SQL; for everything else, use something modern like PUB/SUB with DATAFLOW.","comment_id":"191218","timestamp":"1601586540.0","poster":"kimberjdaw","upvote_count":"1"}],"content":"It's D.\nPub/Sub does not guarantee in-order, or FIFO (first-in-first-out) delivery.\n If order of processing is important, then subscribers would need to coordinate through some ACID storage system such as Cloud Firestore or Cloud SQL.\n\nhttps://cloud.google.com/pubsub/docs/ordering#do_i_really_have_order","upvote_count":"6","poster":"ADVIT","timestamp":"1581633240.0"},{"poster":"evanadarsh","comment_id":"49834","content":"Agree with D","timestamp":"1581569940.0","upvote_count":"2"},{"poster":"coffeecupz","timestamp":"1581272400.0","upvote_count":"2","content":"Answer is (D)\nIf you decide to implement some form of message ordering with Pub/Sub, see Cloud Firestore and Cloud SQL to learn more about how to implement the strategies described in this document.\nhttps://cloud.google.com/pubsub/docs/ordering#how_should_i_handle_order","comment_id":"48409","comments":[{"comment_id":"689041","content":"where it mention cloud sql/firestore helps in ordering? it says pub/sub itself manage ordering","upvote_count":"1","timestamp":"1665205920.0","poster":"Rajeev26"}]},{"comment_id":"43004","timestamp":"1580063340.0","poster":"natpilot","upvote_count":"2","content":"I think B, for legacy streaming and for proper processing."},{"comment_id":"38124","timestamp":"1578840540.0","content":"D is the answer","upvote_count":"3","poster":"AWS56"},{"timestamp":"1571391900.0","poster":"MeasService","upvote_count":"9","content":"In this case, even if messages come through the subscription in order, there are no guarantees of the order in which the messages will be processed by your subscribers. If order of processing is important, then subscribers would need to coordinate through some ACID storage system such as Cloud Datastore or Cloud SQL.\n\nso answer D sounds good","comment_id":"15875","comments":[{"upvote_count":"5","comment_id":"41285","content":"B, to process pub/sub msgs (stream) in batch.","timestamp":"1579605660.0","poster":"Jos"},{"content":"B is correct","timestamp":"1604248320.0","poster":"kumarp6","upvote_count":"3","comment_id":"210603"},{"upvote_count":"2","poster":"nitinz","timestamp":"1614910860.0","comment_id":"303841","content":"Correct answer is B. Adding to SQL, well then you need to delete the message too. dataflow is enough. Unless you want to kill a fly with hand grenade."}]}],"isMC":true,"answer_ET":"B","question_images":[],"choices":{"D":"Cloud Pub/Sub to Cloud SQL","A":"Cloud Pub/Sub alone","B":"Cloud Pub/Sub to Cloud Dataflow","C":"Cloud Pub/Sub to Stackdriver"},"topic":"1","answers_community":["B (59%)","A (41%)"],"exam_id":4,"unix_timestamp":1571391900,"url":"https://www.examtopics.com/discussions/google/view/6747-exam-professional-cloud-architect-topic-1-question-102/","answer_description":"","answer":"B","answer_images":[]}],"exam":{"isBeta":false,"numberOfQuestions":279,"isImplemented":true,"id":4,"lastUpdated":"11 Apr 2025","isMCOnly":false,"name":"Professional Cloud Architect","provider":"Google"},"currentPage":1},"__N_SSP":true}