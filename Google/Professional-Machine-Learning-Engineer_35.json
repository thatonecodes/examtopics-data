{"pageProps":{"questions":[{"id":"61m79JZCgCKO5kOR8MAy","answer_description":"","isMC":true,"topic":"1","url":"https://www.examtopics.com/discussions/google/view/131088-exam-professional-machine-learning-engineer-topic-1-question/","exam_id":13,"question_id":171,"timestamp":"2024-01-13 15:09:00","unix_timestamp":1705154940,"answer_images":[],"answers_community":["B (58%)","D (25%)","A (17%)"],"answer":"B","discussion":[{"content":"Option B because there's no mention of \"flexibility\". Easy access to viz tools with Looker","poster":"pertoise","comment_id":"1157790","upvote_count":"6","timestamp":"1708769580.0"},{"comment_id":"1541160","content":"Selected Answer: D\nOption D provides the best balance of simplicity and meeting all requirements. It allows for visualization and EDA in Vertex AI Workbench and uses the simplest modeling approach (BQML via SQL commands) executed directly from the notebook environment using IPython magics, creating a cohesive and straightforward workflow.","poster":"NithishReddyNY","upvote_count":"1","timestamp":"1743865920.0"},{"upvote_count":"1","comment_id":"1365104","content":"Selected Answer: B\nThe correct answer is probably B. AutoML does the feature engineering for you, so it requires the least amount of effort. For all other options, you have to do the exploration and feature engineering yourself, which will take a lot of time.\nIn a real world scenario you'd expect to do a bit of EDA to identify and deal with missing values and to check the data quality though... even if you go for the B option.","timestamp":"1741119420.0","poster":"Wuthuong1234"},{"timestamp":"1726650480.0","upvote_count":"2","poster":"Dirtie_Sinkie","comment_id":"1285657","content":"Selected Answer: D\nGoing for D"},{"timestamp":"1725594180.0","poster":"andymetzen","upvote_count":"2","comment_id":"1279315","content":"Option D is the answer given by an official Google trainer."},{"upvote_count":"1","content":"Simple training and integration with visualization tools = BQ","poster":"tardigradum","timestamp":"1723611960.0","comment_id":"1265499"},{"timestamp":"1719726180.0","upvote_count":"2","poster":"LaxmanTiwari","comment_id":"1239534","content":"Selected Answer: B\nAs requested :\" simplest approach\", the option B is the best choice."},{"poster":"rcapj","content":"D \nVertex AI Workbench notebook: Provides an environment for data analysis, model building, and visualization tools all in one place.\nIPython magics: Allows seamless interaction with BigQuery for data exploration and feature creation directly within the notebook.\nCREATE MODEL statement: Enables model creation within the notebook environment, simplifying the workflow.\nML.EVALUATE and ML.PREDICT statements: Facilitate model validation directly within the notebook for assessing performance.","timestamp":"1718953380.0","upvote_count":"4","comment_id":"1234234"},{"poster":"omermahgoub","content":"B. Use Bigquery ML Features to create, evaluate and predict","timestamp":"1712995200.0","upvote_count":"3","comment_id":"1194780"},{"upvote_count":"2","content":"Selected Answer: B\nAs requested :\" simplest approach\", the option B is the best choice.","poster":"daidai75","comment_id":"1129284","timestamp":"1705996080.0"},{"poster":"b1a8fae","upvote_count":"1","timestamp":"1705911720.0","content":"Selected Answer: B\nForgot to vote.","comment_id":"1128432"},{"upvote_count":"2","poster":"b1a8fae","comment_id":"1128431","timestamp":"1705911660.0","content":"Simplest approach that allows visualization is option B."},{"upvote_count":"1","timestamp":"1705741740.0","comment_id":"1127157","poster":"winston9","content":"Selected Answer: B\nall the other options create a new BQ table, I don't think it's needed."},{"upvote_count":"2","timestamp":"1705154940.0","poster":"pikachu007","content":"Selected Answer: A\nOption B: While AutoML simplifies model selection and training, it lacks the flexibility and visualization capabilities of Vertex AI Workbench.\nOption C: Manually saving features as CSV files and importing them back into BigQuery involves unnecessary data movement and complexity.\nOption D: Completing all steps within the notebook is possible but requires more coding and might not be as intuitive for those less familiar with BigQuery ML syntax.","comment_id":"1121709"}],"choices":{"B":"Run the CREATE MODEL statement from the BigQuery console to create an AutoML model. Validate the results by using the ML.EVALUATE and ML.PREDICT statements.","A":"Create a Vertex AI Workbench notebook to perform exploratory data analysis. Use IPython magics to create a new BigQuery table with input features. Use the BigQuery console to run the CREATE MODEL statement. Validate the results by using the ML.EVALUATE and ML.PREDICT statements.","C":"Create a Vertex AI Workbench notebook to perform exploratory data analysis and create input features. Save the features as a CSV file in Cloud Storage. Import the CSV file as a new BigQuery table. Use the BigQuery console to run the CREATE MODEL statement. Validate the results by using the ML.EVALUATE and ML.PREDICT statements.","D":"Create a Vertex AI Workbench notebook to perform exploratory data analysis. Use IPython magics to create a new BigQuery table with input features, create the model, and validate the results by using the CREATE MODEL, ML.EVALUATE, and ML.PREDICT statements."},"question_images":[],"question_text":"You work for a company that sells corporate electronic products to thousands of businesses worldwide. Your company stores historical customer data in BigQuery. You need to build a model that predicts customer lifetime value over the next three years. You want to use the simplest approach to build the model and you want to have access to visualization tools. What should you do?","answer_ET":"B"},{"id":"ouhjsNGQ5uSf2apC4pau","answer_description":"","question_id":172,"timestamp":"2024-01-08 17:32:00","exam_id":13,"answers_community":["B (100%)"],"discussion":[{"comment_id":"1153638","timestamp":"1724020740.0","poster":"guilhermebutzke","content":"Selected Answer: B\nMy Answer: B\n\n Vertex AI Feature Store because of these: “must retrieve the features with low latency” ,“retrieve historical data at a specific point in time”, and “ store the features with minimal effort”","upvote_count":"5"},{"timestamp":"1722970620.0","comment_id":"1142590","poster":"CHARLIE2108","content":"Selected Answer: B\nI agree with dadai75","upvote_count":"1"},{"timestamp":"1721713380.0","content":"Selected Answer: B\nAs required: \"minimal effort\" and \"load latency\", the Option B is the best choice.","poster":"daidai75","comment_id":"1129279","upvote_count":"1"},{"poster":"b1a8fae","comment_id":"1128434","timestamp":"1721629500.0","upvote_count":"1","content":"Selected Answer: B\nVertex AI Feature Store is optimized for ultra-low latency serving"},{"poster":"winston9","timestamp":"1720449180.0","upvote_count":"1","comment_id":"1116781","content":"Selected Answer: B\nFeature store allows point in time retrieval"},{"comment_id":"1116780","upvote_count":"1","content":"This is B","poster":"winston9","timestamp":"1720449120.0"}],"answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/130595-exam-professional-machine-learning-engineer-topic-1-question/","answer":"B","topic":"1","unix_timestamp":1704731520,"question_images":[],"choices":{"C":"Store features as a Vertex AI dataset, and use those features to train the models hosted in Vertex AI endpoints.","B":"Store features in Vertex AI Feature Store.","D":"Store features in BigQuery timestamp partitioned tables, and use the BigQuery Storage Read API to serve the features.","A":"Store features in Bigtable as key/value data."},"question_text":"You work for a delivery company. You need to design a system that stores and manages features such as parcels delivered and truck locations over time. The system must retrieve the features with low latency and feed those features into a model for online prediction. The data science team will retrieve historical data at a specific point in time for model training. You want to store the features with minimal effort. What should you do?","isMC":true,"answer_ET":"B"},{"id":"Crmw86cwn2amaP8BI8pA","isMC":true,"answer_description":"","question_images":[],"topic":"1","choices":{"D":"Install the NLTK library from a Jupyter cell by using the !pip install nltk --user command.","B":"Write a custom Dataflow job that uses NLTK to tokenize your text and saves the output to Cloud Storage.","A":"Install the NLTK library from a terminal by using the pip install nltk command.","C":"Create a new Vertex AI Workbench notebook with a custom image that includes the NLTK library."},"answer":"D","timestamp":"2024-01-13 15:22:00","url":"https://www.examtopics.com/discussions/google/view/131091-exam-professional-machine-learning-engineer-topic-1-question/","unix_timestamp":1705155720,"answer_ET":"D","answer_images":[],"question_id":173,"question_text":"You are working on a prototype of a text classification model in a managed Vertex AI Workbench notebook. You want to quickly experiment with tokenizing text by using a Natural Language Toolkit (NLTK) library. How should you add the library to your Jupyter kernel?","exam_id":13,"discussion":[{"timestamp":"1705155720.0","poster":"pikachu007","upvote_count":"7","content":"Selected Answer: D\nDirect Installation: It installs the library directly within the notebook environment, making it immediately available for use.\nSimplicity: It requires a single command in a Jupyter cell, eliminating the need for external tools or configuration.\nUser-Specific Installation: The --user flag ensures the library is installed in your user space, avoiding conflicts with system-wide packages.","comment_id":"1121723"},{"timestamp":"1722828660.0","poster":"forport","comment_id":"1260906","upvote_count":"1","content":"Selected Answer: D\nRight command : !pip install nltk --user"},{"timestamp":"1713394560.0","poster":"fitri001","upvote_count":"1","comment_id":"1197567","comments":[{"upvote_count":"1","content":"A. Terminal Installation: While possible if allowed, it requires switching contexts outside the notebook and might not be permitted in managed environments.\nB. Dataflow Job: A Dataflow job is an overkill for simple library usage within a notebook. It's designed for large-scale data processing pipelines.\nC. Custom Image: Creating a custom image with NLTK requires additional development effort and can be time-consuming for quick experimentation.","comment_id":"1197569","poster":"fitri001","timestamp":"1713394620.0"}],"content":"Selected Answer: D\nEfficiency: It allows installation directly within your notebook cell, minimizing setup time compared to creating a custom image or using an external terminal.\nUser-Level Installation: Using --user ensures the library is installed within your user environment, avoiding conflicts with system-wide installations or impacting other users."},{"poster":"tavva_prudhvi","content":"Selected Answer: D\nThis command installs the NLTK library directly from within your Jupyter notebook, allowing you to quickly proceed with your text tokenization experiments without needing to manage Docker images or set up external data processing jobs. The `--user` flag ensures that the library is installed in the user's space, avoiding potential conflicts with system-wide packages.","timestamp":"1707566040.0","comment_id":"1146296","upvote_count":"3"}],"answers_community":["D (100%)"]},{"id":"Ld9kmLCPrVESVoS6xo1T","discussion":[{"timestamp":"1706389680.0","comment_id":"1133657","poster":"BlehMaks","upvote_count":"10","content":"Selected Answer: C\nThe DataflowPythonJobOp operator lets you create a Vertex AI Pipelines component that prepares data by submitting a Python-based Apache Beam job to Dataflow for execution.\nhttps://cloud.google.com/vertex-ai/docs/pipelines/dataflow-component#dataflowpythonjobop\nUsing we can specify an output location for Vertex AI to store predictions results\nhttps://cloud.google.com/vertex-ai/docs/pipelines/batchprediction-component\nA - is incorrect since we dont need an endpoint for batch predictions\nB - creating a new Dataflow pipeline is redundant"},{"upvote_count":"1","poster":"lunalongo","comment_id":"1316867","timestamp":"1732409340.0","content":"Selected Answer: C\nC is the best option because it uses: \n1) Vertex AI Pipelines for orchestrating the flow (managed and scalable).\n2) DataflowPythonJobOp for prep and ModelBatchPredictOp for batch predictions on Vertex AI.\n\n*A deploys the model to a Vertex AI endpoint, inefficient for batch jobs!\n*B uses a single Dataflow pipeline, which needs custom Vertex AI and BQ integration.\n*D uses BigQuery, a datawarehouse, for model deployment and prediction."},{"content":"Selected Answer: B\nUploading predictions directly to BigQuery from the Dataflow pipeline integrates seamlessly with your data storage.","upvote_count":"1","timestamp":"1722699600.0","comment_id":"1260361","poster":"AK2020"},{"upvote_count":"1","timestamp":"1720184220.0","content":"B is right because\n1)You've already trained a classification model using TensorFlow, so you need to productionize it by deploying it to a Vertex AI endpoint.\n2)To automate the prediction process on a weekly schedule, you can create a Dataflow pipeline that reuses your existing data processing logic. This pipeline will send requests to the deployed model for inference and then upload the predicted results to BigQuery.","poster":"AzureDP900","comment_id":"1242783"},{"upvote_count":"1","timestamp":"1720069020.0","content":"Selected Answer: B\nOnly option B talks about loading the data to BigQuery","comment_id":"1241827","poster":"Prakzz"},{"poster":"rcapj","comment_id":"1234237","upvote_count":"2","content":"B Vertex AI Deployment: Vertex AI provides a managed environment for deploying machine learning models. It simplifies the process and ensures scalability.\nDataflow Pipeline Reuse: Reusing the existing Dataflow pipeline for data processing leverages your existing code and avoids redundant logic.\nModel Endpoint Predictions: Sending requests to the deployed model endpoint allows for efficient prediction generation.\nBigQuery Upload: Uploading predictions directly to BigQuery from the Dataflow pipeline integrates seamlessly with your data storage.","timestamp":"1718953800.0"},{"poster":"gscharly","timestamp":"1713506220.0","content":"Selected Answer: C\nNo need to deploy to endpoint as we need batch predictions. ModelBatchPredictOp can upload data to BQ. Dataflow pipeline logic can be implemented in DataflowPythonJobOp","upvote_count":"4","comment_id":"1198348"},{"content":"Selected Answer: B\nTFRecords is a specific file format designed by TensorFlow for storing data in a way that's efficient for the machine learning framework. Here are some key points about TFRecords:","poster":"fitri001","upvote_count":"1","comment_id":"1196890","timestamp":"1713312000.0"},{"upvote_count":"2","comment_id":"1196888","timestamp":"1713311820.0","content":"Selected Answer: B\nOption A: Vertex AI Pipelines' ModelBatchPredictOp is designed for batch prediction within pipelines, not for serving models through an endpoint.\nOption C: Importing the model directly into BigQuery is not feasible for TensorFlow models.\nOption D: Vertex AI Pipelines' BigqueryPredictModelJobOp assumes the model is already trained and hosted in BigQuery ML, which isn't the case here.","poster":"fitri001","comments":[{"upvote_count":"3","poster":"pinimichele01","timestamp":"1713377040.0","content":"Importing the model directly into BigQuery is not feasible for TensorFlow models. -> not true","comment_id":"1197380"}]},{"timestamp":"1713072300.0","comment_id":"1195285","content":"Selected Answer: C\nModelBatchPredictOp -> upload automatically on BQ\nNo need for endpoint \n\n--> C","poster":"pinimichele01","upvote_count":"2"},{"poster":"pinimichele01","comment_id":"1191774","upvote_count":"1","content":"Selected Answer: C\nagree with BlehMaks","timestamp":"1712602440.0"},{"comment_id":"1162893","poster":"pertoise","timestamp":"1709230620.0","content":"Answer is C. No need for an endpoint here : Simply specify the BigQuery table URI in the ModelBatchPredictOp parameter and you're done automatically uploading to BigQuery","upvote_count":"3"},{"poster":"guilhermebutzke","comment_id":"1152195","content":"Selected Answer: B\nMy Answer: B\n\nThe most complete answer, and reuse a created pipeline. Don’t make sense to use DataflowPythonJobOp when you have already created a dataflow pipeline that does the same.","timestamp":"1708110720.0","upvote_count":"2"},{"comment_id":"1146298","content":"Selected Answer: B\nNot A, C as they does not explicitly mention how the predictions will be uploaded to BigQuery.","timestamp":"1707566520.0","upvote_count":"1","poster":"tavva_prudhvi"},{"poster":"daidai75","upvote_count":"1","comment_id":"1129271","content":"Selected Answer: B\nThe answer is B, optional A and B doesn't mention how to import prediction result to BigQuery.","timestamp":"1705995180.0"},{"timestamp":"1705155960.0","poster":"pikachu007","upvote_count":"1","content":"Selected Answer: B\nOption A: Vertex AI Pipelines are excellent for orchestrating ML workflows but might not be as efficient as Dataflow for large-scale data processing, especially with existing Dataflow logic.\nOption C: While Vertex AI Pipelines can handle model loading and prediction, Dataflow is better suited for large-scale data processing and BigQuery integration.\nOption D: BigQuery ML is primarily for in-database model training and prediction, not ideal for external models or large-scale data processing.","comment_id":"1121727"}],"timestamp":"2024-01-13 15:26:00","unix_timestamp":1705155960,"question_images":[],"answer_description":"","question_id":174,"answer_images":[],"answers_community":["C (64%)","B (36%)"],"exam_id":13,"url":"https://www.examtopics.com/discussions/google/view/131092-exam-professional-machine-learning-engineer-topic-1-question/","question_text":"You have recently used TensorFlow to train a classification model on tabular data. You have created a Dataflow pipeline that can transform several terabytes of data into training or prediction datasets consisting of TFRecords. You now need to productionize the model, and you want the predictions to be automatically uploaded to a BigQuery table on a weekly schedule. What should you do?","answer_ET":"C","answer":"C","choices":{"A":"Import the model into Vertex AI and deploy it to a Vertex AI endpoint. On Vertex AI Pipelines, create a pipeline that uses the DataflowPythonJobOp and the ModelBacthPredictOp components.","B":"Import the model into Vertex AI and deploy it to a Vertex AI endpoint. Create a Dataflow pipeline that reuses the data processing logic sends requests to the endpoint, and then uploads predictions to a BigQuery table.","D":"Import the model into BigQuery. Implement the data processing logic in a SQL query. On Vertex AI Pipelines create a pipeline that uses the BigquervQueryJobOp and the BigqueryPredictModelJobOp components.","C":"Import the model into Vertex AI. On Vertex AI Pipelines, create a pipeline that uses the\nDataflowPvthonJobOp and the ModelBatchPredictOp components."},"isMC":true,"topic":"1"},{"id":"DPx02XxpziVWIVvQL0AC","discussion":[{"timestamp":"1713312420.0","content":"Selected Answer: C\nOption A: Manually adding compute nodes after an alert might lead to delays and potential outages during peak traffic.\nOption B: Upgrading to 32 vCPUs upfront might be an overkill if the current machine type with 8 vCPUs can handle the typical daily traffic. Vertical scaling (more vCPUs) might be suitable only if the model can benefit from additional CPU power.\nOption D: Using a GPU is unlikely to benefit a recipe recommendation model, which likely doesn't involve intensive graphical processing. Additionally, monitoring GPU usage wouldn't be relevant.","comment_id":"1196892","poster":"fitri001","upvote_count":"6"},{"content":"Selected Answer: C\nC) Option C is the best because:\n1) It leverages the built-in autoscaling capabilities of Vertex AI. 2) It's the most efficient/cost-effective solution for fluctuating traffic. 2) Manually scaling (options A and B) is reactive and inefficient\n3) A GPU is unnecessary, there is no intensive graphical processing","upvote_count":"1","poster":"lunalongo","comment_id":"1316805","timestamp":"1732396860.0"},{"comment_id":"1242784","upvote_count":"1","content":"C is right because\n1)Since you've already optimized your model's deployment on a single machine with 8 vCPUs, it makes sense to maintain the same machine type to avoid any potential performance issues.\n2)Enabling autoscaling based on vCPU usage will allow your endpoint to automatically add more machines as needed to handle the increased traffic during the holiday season. This approach is more efficient and cost-effective than scaling up individual machines or adding new machines manually.\n3)Monitoring CPU usage with a job and alerting when thresholds are exceeded allows you to detect potential issues before they impact performance.","poster":"AzureDP900","timestamp":"1720184580.0"},{"upvote_count":"1","poster":"omermahgoub","content":"Selected Answer: C\nC: Use Autoscaling Based on vCPU Usage","comment_id":"1194782","timestamp":"1712995320.0"},{"timestamp":"1712711580.0","content":"Selected Answer: C\nAutoscaling based on vCPU usage aligns well with the workload.","comment_id":"1192596","poster":"emsherff","upvote_count":"1"},{"content":"Option A is manual intervention \nOption B is overprovisioning preemptively, which is an overkill ( autoscaling should be preferred) \nOption D - Unless the recipe recommendation model uses GPU-accelerated computations (e.g., some deep learning models), adding a GPU won't be beneficial and will increase costs.\nI would go with C - Autoscaling based on vCPU usage which aligns well with the workload.","comment_id":"1192584","timestamp":"1712710140.0","poster":"emsherff","upvote_count":"2"},{"poster":"daidai75","timestamp":"1705991940.0","upvote_count":"1","comment_id":"1129244","content":"Selected Answer: C\nOption B can only support exact 4x times traffic, but the requirement is four times \"more\", so B is not the best at least for me."},{"content":"Selected Answer: C\nI would go for C as it enables autoscaling when exceeding a determined CPU usage threshold.","upvote_count":"1","poster":"b1a8fae","timestamp":"1705914000.0","comment_id":"1128455"},{"comment_id":"1121730","content":"Selected Answer: C\nCost Optimization: It starts with the current machine type, avoiding unnecessary upfront costs, and scales only when needed.\nAutoscaling: It automatically adjusts compute resources based on vCPU usage, ensuring the endpoint can handle traffic spikes without manual intervention.\nMonitoring and Alerting: It provides visibility into resource usage and triggers alerts for potential issues, enabling proactive actions.\nInvestigation: It encourages investigation of alerts to identify any underlying problems beyond expected traffic growth, ensuring overall system health.","poster":"pikachu007","timestamp":"1705156080.0","upvote_count":"1"},{"content":"Selected Answer: B\nVoting for B as it's the only option to autoscale even though the cost will go up. All other options include manual intervention.","timestamp":"1704838020.0","comments":[{"timestamp":"1705913940.0","poster":"b1a8fae","comment_id":"1128453","content":"Wouldn't scaling up the vCPUs after receiving the alert also be manual? It comes across as such to me at least.","upvote_count":"1"}],"poster":"kalle_balle","upvote_count":"1","comment_id":"1117881"}],"exam_id":13,"answer_images":[],"question_id":175,"answers_community":["C (92%)","8%"],"url":"https://www.examtopics.com/discussions/google/view/130733-exam-professional-machine-learning-engineer-topic-1-question/","topic":"1","answer_ET":"C","choices":{"D":"1. Change the machine type on the endpoint to have a GPU. Configure the endpoint to enable autoscaling based on the GPU usage.\n2. Set up a monitoring job and an alert for GPU usage.\n3. If you receive an alert, investigate the cause.","C":"1. Maintain the same machine type on the endpoint Configure the endpoint to enable autoscaling based on vCPU usage.\n2. Set up a monitoring job and an alert for CPU usage.\n3. If you receive an alert, investigate the cause.","A":"1. Maintain the same machine type on the endpoint.\n2. Set up a monitoring job and an alert for CPU usage.\n3. If you receive an alert, add a compute node to the endpoint.","B":"1. Change the machine type on the endpoint to have 32 vCPUs.\n2. Set up a monitoring job and an alert for CPU usage.\n3. If you receive an alert, scale the vCPUs further as needed."},"unix_timestamp":1704838020,"question_text":"You work for an online grocery store. You recently developed a custom ML model that recommends a recipe when a user arrives at the website. You chose the machine type on the Vertex AI endpoint to optimize costs by using the queries per second (QPS) that the model can serve, and you deployed it on a single machine with 8 vCPUs and no accelerators.\n\nA holiday season is approaching and you anticipate four times more traffic during this time than the typical daily traffic. You need to ensure that the model can scale efficiently to the increased demand. What should you do?","timestamp":"2024-01-09 23:07:00","isMC":true,"question_images":[],"answer":"C","answer_description":""}],"exam":{"name":"Professional Machine Learning Engineer","isMCOnly":true,"numberOfQuestions":304,"isBeta":false,"provider":"Google","id":13,"isImplemented":true,"lastUpdated":"11 Apr 2025"},"currentPage":35},"__N_SSP":true}