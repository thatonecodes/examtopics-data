{"pageProps":{"questions":[{"id":"hqjJFtlIJMKtAwJAf27x","discussion":[{"poster":"TotoroChina","timestamp":"1641024900.0","comment_id":"395596","upvote_count":"52","comments":[{"upvote_count":"4","content":"B. Agree, clearly it’s B. Focus on keyword “zone”","poster":"AmitRBS","timestamp":"1669520100.0","comment_id":"607853"}],"content":"Answer is B, it only request zonal resiliency.\nRegional persistent disk is a storage option that provides synchronous replication of data between two zones in a region. Regional persistent disks can be a good building block to use when you implement HA services in Compute Engine.\n\nhttps://cloud.google.com/compute/docs/disks/high-availability-regional-persistent-disk"},{"content":"Answer is B","poster":"Ssoumya","timestamp":"1641211500.0","comment_id":"397481","upvote_count":"14"},{"comment_id":"1336137","content":"Selected Answer: B\nto be quickly as possible just change the zone in the same region configuring a; Compute Engine instance;","upvote_count":"1","timestamp":"1735933140.0","poster":"plumbig11"},{"poster":"oscarcampos","timestamp":"1729876920.0","content":"why D ?","upvote_count":"1","comment_id":"1202177"},{"comment_id":"1164997","poster":"mesodan","content":"Selected Answer: B\nIt is B. As for D: Spinning up the application in another region might be too geographically distant, leading to higher latency and potential issues.","upvote_count":"1","timestamp":"1725380640.0"},{"upvote_count":"1","content":"Selected Answer: B\nA regional persistent disk is designed to provide synchronous replication of data between two zones in the same region, ensuring that data remains available even if one zone is affected by an outage. By using an instance template along with a regional disk, you can quickly create new instances in an available zone during a zonal outage and attach the regional persistent disk to continue operations with the latest application data.","timestamp":"1721656140.0","comment_id":"1128851","poster":"hzaoui"},{"upvote_count":"1","content":"Can someone explain why not C - snapshot?","comments":[{"poster":"xaqanik","upvote_count":"2","timestamp":"1722429600.0","content":"B - uses instance template which is ready for deploy. Option C requires manual configuration and this may take more time . But we need as quickly as possible .","comment_id":"1136863"}],"comment_id":"1124887","poster":"SSS987","timestamp":"1721205000.0"},{"content":"Selected Answer: D\nOption D suggests using the same approach as Option B but restoring the application in another region instead of the same region. This approach provides high availability and disaster recovery across regions, making it suitable for applications that require high RTOs and minimal data loss.","comments":[{"poster":"convers39","content":"can you use the regional persistent disk in a different region?","upvote_count":"3","comment_id":"1112779","timestamp":"1720005420.0","comments":[{"comment_id":"1124679","timestamp":"1721184060.0","upvote_count":"1","poster":"don_v","content":"apparently, nope."}]}],"timestamp":"1715661300.0","comment_id":"1070071","upvote_count":"1","poster":"thewalker"},{"comment_id":"1041621","poster":"JPA210","upvote_count":"2","content":"Of course it is answer B. I would like to understand who chooses the right answers in examtopics! Choosing here option D is completely wrong. This takes people less instructed to be mistaken.","timestamp":"1712914620.0"},{"timestamp":"1712397360.0","comment_id":"1026411","upvote_count":"1","poster":"AdityaGupta","content":"Selected Answer: B\nB. Configure the Compute Engine instances with an instance template for the application, and use a regional persistent disk for the application data. Whenever a zonal outage occurs, use the instance template to spin up the application in another zone in the same region. Use the regional persistent disk for the application data. Most Voted\n\nWhy to change the region as mentioned in Option D, when the ask is different zone only."},{"content":"Selected Answer: B\nAnswer is B, it only request zonal resiliency","timestamp":"1710061620.0","comment_id":"1003759","upvote_count":"1","poster":"duzapo"},{"timestamp":"1709127300.0","content":"It says restore in a zonal. Hence answer is B.","comment_id":"992103","poster":"heretolearnazure","upvote_count":"1"},{"comment_id":"849921","timestamp":"1695622800.0","content":"B. In my opinion, I cannot use a regional disk in a different region!!! So, it can only be another zone in the same region. Therefore D must be wrong!","poster":"mifrah","upvote_count":"2"},{"timestamp":"1690859340.0","content":"Answer B is Correct - since it talks about spin up application in different zone but same region. \n\nWhereas,D is incoorect , since its talking about spin up application in different region which is not our requirement.","upvote_count":"1","comment_id":"794858","poster":"SambuSoni"},{"poster":"CosminCiuc","upvote_count":"2","content":"If it is a regional persistent disk created in region A for example. If I start the compute engine instance in the region B, how am I going to use a regional disk from region A (another region)? I do not think it is possible. So answer D should be excluded. \nI do believe that the correct answer is B.","comment_id":"791928","timestamp":"1690648260.0"},{"poster":"windsor_43","upvote_count":"5","content":"The Answer is B\n\nJust had my exam today with a pass, this question was in the exam. Dated 31/12/22\nThanks to this site it was by far my most valuable","timestamp":"1688143560.0","comment_id":"762853"},{"content":"14/12/22 Exam,but IM failed :(","comments":[{"comments":[{"upvote_count":"1","content":"How was it?","timestamp":"1699526220.0","comment_id":"892927","poster":"Sur_Nikki"}],"content":"Tomorrow 12/27/22 is my exam :)","timestamp":"1687784580.0","comment_id":"757571","upvote_count":"2","poster":"Praveen_G"}],"timestamp":"1686748020.0","comment_id":"745197","upvote_count":"1","poster":"rascalbrick"},{"content":"Selected Answer: B\nB is ok","poster":"megumin","upvote_count":"1","timestamp":"1683894360.0","comment_id":"716742"},{"comment_id":"697324","poster":"minmin2020","upvote_count":"1","timestamp":"1681728480.0","content":"Selected Answer: B\nB. Configure the Compute Engine instances with an instance template for the application, and use a regional persistent disk for the application data. Whenever a zonal outage occurs, use the instance template to spin up the application in another zone in the same region. Use the regional persistent disk for the application data."},{"timestamp":"1681059000.0","upvote_count":"1","content":"I think this has to be B not D. One the question talks about a zonal failure not a region failure, two the answers both talk about regional storage not multi-region.","poster":"kmreeves","comment_id":"690358"},{"timestamp":"1680105660.0","poster":"VinayNune","content":"Selected Answer: B\nAnswer is B","upvote_count":"2","comment_id":"682822"},{"content":"Answer is B. No need to move to another region. This will most probably increase the latency of certain customers.","comment_id":"682821","poster":"VinayNune","upvote_count":"2","timestamp":"1680105600.0"},{"timestamp":"1676767680.0","content":"Selected Answer: B\nIts B, the regional PD is available across the zones.","comment_id":"648642","upvote_count":"1","poster":"cloudinit"},{"comment_id":"646521","content":"I got this question in exam.","timestamp":"1676347800.0","poster":"ACE_ASPIRE","upvote_count":"2"},{"content":"Selected Answer: B\nClearly B - Every region will have different zones, if any issue with one zone , then can replicate/fail over to another zone in the same region. Instead of creating the instance in new region.","upvote_count":"1","timestamp":"1676045400.0","comment_id":"645023","poster":"Bahubali1988"},{"content":"06/30/2022 Exam","poster":"mv2000","upvote_count":"6","timestamp":"1673035020.0","comment_id":"628042"},{"upvote_count":"1","comment_id":"626314","poster":"AzureDP900","content":"B is right, D is close enough with region so it is not correct.","timestamp":"1672700220.0","comments":[{"poster":"AzureDP900","upvote_count":"1","content":"I agree with other folks, there are no second thought and is only required for Zone.","comment_id":"626316","timestamp":"1672700340.0"}]},{"content":"Selected Answer: B\nAnother zone, not another region. B is the correct Answer","poster":"ZLT","comment_id":"623876","timestamp":"1672227060.0","upvote_count":"1"},{"comment_id":"622374","timestamp":"1672042080.0","poster":"GPK","upvote_count":"3","content":"B, Agree..exam 25/06/22"},{"poster":"sarath","upvote_count":"1","timestamp":"1669849440.0","content":"Selected Answer: B\nQuestion expects the environment to be created in another zone, and not another region, so B is correct","comment_id":"609904"},{"comment_id":"598169","timestamp":"1667840700.0","content":"Got this q. Ans B.","upvote_count":"1","poster":"tannV"},{"comment_id":"596971","content":"Selected Answer: B\nAlready mentioned but\nFrom: https://cloud.google.com/compute/docs/disks/high-availability-regional-persistent-disk\n1st sentence, 2nd paragraph. \"Regional persistent disk is a storage option that provides synchronous replication of data between two zones in a region.\"\n\nBetween zones, not regions.","timestamp":"1667589900.0","poster":"ridyr","upvote_count":"3"},{"comment_id":"582443","content":"Go for B, \"the application should be restored in another zone as quickly as possible\"","upvote_count":"1","poster":"gaojun","timestamp":"1665146520.0"},{"timestamp":"1662397260.0","content":"Selected Answer: B\nCorrect answer is B","upvote_count":"1","poster":"ss909098","comment_id":"561582"},{"comments":[{"upvote_count":"1","comment_id":"561586","timestamp":"1662397380.0","poster":"ss909098","content":"That's not true. An instance template is a global resource that is not bound to a zone or a region."}],"upvote_count":"1","comment_id":"549010","content":"Also instance template has regional scope only. If it has to be used outside of the region, you would have to create other versions of the instance template per region","timestamp":"1660687020.0","poster":"Sunilkg"},{"upvote_count":"1","timestamp":"1660590960.0","comment_id":"548047","content":"2/15/21 exam","poster":"azureaspirant"},{"poster":"[Removed]","comment_id":"545499","upvote_count":"2","content":"I got similar question on my exam. Answered B.","timestamp":"1660243320.0"},{"poster":"technodev","upvote_count":"2","content":"Got this question in my exam, answered B","comment_id":"527709","timestamp":"1658240400.0"},{"comment_id":"518718","timestamp":"1657153980.0","poster":"OrangeTiger","upvote_count":"1","comments":[{"poster":"pddddd","content":"regional disk scope is... wait for it... regional!","comment_id":"520826","timestamp":"1657447800.0","upvote_count":"2"}],"content":"Whay D?\nIs there a reason .need to use a different region?"},{"comment_id":"511662","upvote_count":"1","timestamp":"1656454200.0","poster":"BattleSlim","content":"Selected Answer: B\nB is correct, no need to bother with another region. Just switching to another zone should do it as mentioned in the question."},{"comment_id":"508094","content":"Selected Answer: B\nAs quickly as possible so performance is important.","upvote_count":"1","poster":"mardon","timestamp":"1656002820.0"},{"content":"Selected Answer: B\nRegional persistent disks provide synchronous replication of data between two zones in a region.","upvote_count":"1","poster":"ABO_Doma","comment_id":"504822","timestamp":"1655633700.0"},{"comment_id":"503060","timestamp":"1655389440.0","poster":"SamGCP","content":"Selected Answer: B\nRegional persistent disk is not available in another region so option D doesn't work.","upvote_count":"3"},{"upvote_count":"2","comment_id":"499224","timestamp":"1654930980.0","content":"Selected Answer: B\nB) It is a regional disk, so it needs to be used by Compute Engine instances in a different zone in the same region.","poster":"PhilipKoku"},{"upvote_count":"1","timestamp":"1654765560.0","comment_id":"497651","content":"Go for B. \nZonal","poster":"haroldbenites"},{"upvote_count":"1","timestamp":"1654748340.0","poster":"Bert_77","comment_id":"497379","content":"The answer should be B. There is no need to spin up a new instance in another region, just another zone should be fine.Application data should be located on a regional persistent disk : https://cloud.google.com/compute/docs/disks/high-availability-regional-persistent-disk"},{"comment_id":"496621","timestamp":"1654668060.0","content":"B is right. Another zone spin-up in same region for Zonal Outage handling.","poster":"[Removed]","upvote_count":"1"},{"comment_id":"492929","timestamp":"1654220340.0","content":"Selected Answer: B\nB is the correct answer\nhttps://cloud.google.com/compute/docs/disks/high-availability-regional-persistent-disk","poster":"vincy2202","upvote_count":"2"},{"upvote_count":"1","content":"Selected Answer: B\nonly zonal resiliency is required","poster":"mudot","comment_id":"487608","timestamp":"1653587640.0"},{"timestamp":"1653476340.0","content":"Selected Answer: B\nvote B","comment_id":"486661","upvote_count":"3","poster":"joe2211"},{"content":"Clearly B. Zonal outage and not regional. So, it can't be D.","timestamp":"1653237480.0","upvote_count":"1","comment_id":"484469","poster":"DMC1163"},{"upvote_count":"1","poster":"Anilc","timestamp":"1653125100.0","comment_id":"483200","content":"Selected Answer: B\nQuestion is Asking to bring up on another zone is same region not in another region."},{"comment_id":"483032","timestamp":"1653105060.0","upvote_count":"1","poster":"pakilodi","content":"Selected Answer: B\nAnswer is B: it is requested only zonal resiliency"},{"poster":"pakilodi","upvote_count":"1","comment_id":"482569","timestamp":"1653046680.0","content":"Selected Answer: B\nAnswer Is B. It Is requested only zonal resiliency not regional"},{"timestamp":"1651924740.0","poster":"exam_war","content":"The answer is B","upvote_count":"1","comment_id":"473925"},{"content":"B - another \"zone\"\nnot region","upvote_count":"2","comment_id":"454008","poster":"rottzy","timestamp":"1648547040.0"},{"poster":"AnilKr","upvote_count":"1","content":"B is correct, zonal is ask","timestamp":"1648536240.0","comment_id":"453899"},{"content":"Ans -B, The benefit of regional persistent disks is that in the event of a zonal outage, where your virtual machine (VM) instance might become unavailable, you can usually force attach a regional persistent disk to a VM instance in a secondary zone in the same region.","timestamp":"1644990780.0","comment_id":"425576","upvote_count":"4","poster":"AnilKr"},{"timestamp":"1644824760.0","comment_id":"424637","comments":[{"poster":"rishab86","upvote_count":"1","content":"as per link https://cloud.google.com/compute/docs/disks/high-availability-regional-persistent-disk it should be B","timestamp":"1647672300.0","comment_id":"447397"}],"poster":"Sandipcst","upvote_count":"3","content":"Answer would be C- Create a snapshot schedule for the disk containing the application data. Whenever a zonal outage occurs, use the latest snapshot to restore the disk in another zone within the same region. \n\nInstance template cannot give you guarantee to have same application data. So, B cannot be an answer."},{"comment_id":"411406","poster":"AshwathD","upvote_count":"3","content":"must be B","timestamp":"1642839540.0"},{"upvote_count":"4","timestamp":"1641889740.0","poster":"victory108","comment_id":"403726","content":"B. Configure the Compute Engine instances with an instance template for the application and use a regional persistent disk for the application data. Whenever a zonal outage occurs, use the instance template to spin up the application in another zone in the same region. Use the regional persistent disk for the application data."},{"comment_id":"403225","poster":"milan74","upvote_count":"2","content":"Answer is B: question states to restore in different zone, not region as in answer D.","timestamp":"1641812040.0"},{"upvote_count":"2","timestamp":"1641670380.0","poster":"VishalB","content":"Ans B \no The benefit of regional persistent disks is that in the event of a zonal outage, where your virtual machine (VM) instance might become unavailable, you can usually force attach a regional persistent disk to a VM instance in a secondary zone in the same region. To perform this task, you must either start another VM instance in the same zone as the regional persistent disk that you are force attaching or maintain a hot standby VM instance in that zone. A hot standby is a running VM instance that is identical to the one you are using. The two instances have the same data.","comment_id":"402110"},{"timestamp":"1641585600.0","comment_id":"401161","upvote_count":"3","poster":"MamthaSJ","content":"Answer is B"},{"timestamp":"1641031020.0","comment_id":"395673","upvote_count":"3","poster":"DuncanK53","content":"Should this be B?"},{"poster":"Enzian","comment_id":"395604","content":"Should be B. The question calls for mitigation of zonal outages but D suggests creating a new instance in a different region (\"use the instance template to spin up the application in another region.\"). This is hard to do with regional disks.","timestamp":"1641026340.0","upvote_count":"3"},{"comment_id":"394884","content":"IMHO d) is wrong - it must be b).\nThe requirement is that in case of a zonal outage the instance has to be created in a different zone, not in a different region.","upvote_count":"5","poster":"XDevX","timestamp":"1640885760.0"}],"answers_community":["B (97%)","3%"],"question_id":21,"question_images":[],"unix_timestamp":1625067360,"answer":"B","answer_description":"","isMC":true,"url":"https://www.examtopics.com/discussions/google/view/56403-exam-professional-cloud-architect-topic-1-question-117/","answer_images":[],"choices":{"B":"Configure the Compute Engine instances with an instance template for the application, and use a regional persistent disk for the application data. Whenever a zonal outage occurs, use the instance template to spin up the application in another zone in the same region. Use the regional persistent disk for the application data.","D":"Configure the Compute Engine instances with an instance template for the application, and use a regional persistent disk for the application data. Whenever a zonal outage occurs, use the instance template to spin up the application in another region. Use the regional persistent disk for the application data.","A":"Create a snapshot schedule for the disk containing the application data. Whenever a zonal outage occurs, use the latest snapshot to restore the disk in the same zone.","C":"Create a snapshot schedule for the disk containing the application data. Whenever a zonal outage occurs, use the latest snapshot to restore the disk in another zone within the same region."},"topic":"1","question_text":"Your company is designing its application landscape on Compute Engine. Whenever a zonal outage occurs, the application should be restored in another zone as quickly as possible with the latest application data. You need to design the solution to meet this requirement. What should you do?","exam_id":4,"answer_ET":"B","timestamp":"2021-06-30 17:36:00"},{"id":"Etwt5sa9BuYxSM0N3LRy","answer":"B","topic":"1","answers_community":["B (43%)","A (42%)","C (15%)"],"choices":{"D":"Create a Cloud VPN connection from the new VPC to the data center, and apply a firewall rule that blocks the overlapping IP space.","A":"Create a Cloud VPN connection from the new VPC to the data center, create a Cloud Router, and apply new IP addresses so there is no overlapping IP space.","B":"Create a Cloud VPN connection from the new VPC to the data center, and create a Cloud NAT instance to perform NAT on the overlapping IP space.","C":"Create a Cloud VPN connection from the new VPC to the data center, create a Cloud Router, and apply a custom route advertisement to block the overlapping IP space."},"answer_ET":"B","url":"https://www.examtopics.com/discussions/google/view/56680-exam-professional-cloud-architect-topic-1-question-118/","question_images":[],"question_id":22,"unix_timestamp":1625121660,"timestamp":"2021-07-01 08:41:00","exam_id":4,"answer_description":"","isMC":true,"answer_images":[],"discussion":[{"upvote_count":"42","content":"Correct Answer: A\n- IP Should not overlap so applying new IP address is the solution","timestamp":"1627658040.0","poster":"VishalB","comments":[{"upvote_count":"7","comment_id":"567484","poster":"zanfo","content":"A is not correct. \"What should you do to enable connectivity and make sure that there are no routing conflicts when connectivity is established?\" if you apply VPN con BGP, the actual IP address will be propagated to on prem environment with overlapping RFC1918 as result. B is correct with custom route","timestamp":"1647242880.0"}],"comment_id":"417500"},{"upvote_count":"36","timestamp":"1625121660.0","poster":"TotoroChina","content":"Answer is C.\nhttps://cloud.google.com/network-connectivity/docs/router/how-to/advertising-custom-ip","comments":[{"upvote_count":"5","poster":"meh009","timestamp":"1634110740.0","comment_id":"461421","content":"The Q states to establish connectivity. This would merely prevent that. Ans is A"},{"timestamp":"1705466880.0","poster":"don_v","upvote_count":"2","content":"I would also agree with C.\n\nStill, this part is confusing: \"C. Create a Cloud VPN connection from the new VPC to the data center, create a Cloud Router, and apply a custom route advertisement to *block* the overlapping IP space.\"\n\nTo *block*? Not to block. just to alias with advertised IP addresses.","comment_id":"1124682"},{"comment_id":"405074","content":"ANS is B\nhttps://cloud.google.com/architecture/best-practices-vpc-design","comments":[{"timestamp":"1696257960.0","content":"It will be a NAT Router instance, which will route the traffic. I have practically applied the configuration.","poster":"RKS_2021","comment_id":"1023178","upvote_count":"3"},{"comments":[{"comment_id":"1197979","content":"You can use private or hybrid NAT\nhttps://cloud.google.com/nat/docs/overview#private-nat","upvote_count":"3","poster":"dija123","timestamp":"1713450720.0"},{"comment_id":"464128","content":"Thanks for the clarification, just one question, without a solution like NAT or reip, the service on the devices with overlapping IP subnet will be unavailable for on-premise devices, not sure if the question also about this","timestamp":"1634561580.0","upvote_count":"1","poster":"Bill831231"},{"poster":"imgcp","upvote_count":"4","timestamp":"1627470300.0","comments":[{"poster":"JohnnyBG","upvote_count":"1","timestamp":"1656490200.0","comment_id":"624506","content":"Be careful because CloudNAT for private IPs will be GA soon"}],"comment_id":"416159","content":"*you can't use Cloud NAT to translate from one private IP to another private ip to avoid overlapping ip range issue."}],"comment_id":"416157","content":"B is NOT correct. Cloud NAT is specifically used for translating the IP address of the outbound packets destined to the Internet. But this question is about using VPN communication between two private IP address spaces (RFC1918). Cloud NAT cannot achieve the purpose here, you can't use Cloud NAT to translate from one private IP to another private ip. I would vote for C.","upvote_count":"13","timestamp":"1627470240.0","poster":"imgcp"},{"comment_id":"518122","timestamp":"1641462120.0","upvote_count":"16","poster":"elenamatay","content":"You can't use Cloud NAT according to this documentation: https://cloud.google.com/nat/docs/troubleshooting#overlapping-ip-addresses\n\n\"Can I use Cloud NAT to connect a VPC network to another network to work around overlapping IP addresses? No, Cloud NAT cannot apply to any custom route whose next hop is not the default internet gateway. For example, Cloud NAT cannot apply to traffic sent to a next hop Cloud VPN tunnel, even if the destination is a publicly routable IP address.\""}],"poster":"RKS_2021","upvote_count":"8","timestamp":"1626143820.0"}],"comment_id":"395608"},{"comment_id":"1365411","content":"Selected Answer: B\nhttps://cloud.google.com/nat/docs/overview#private-nat\n\nAssume that your Google Cloud resources in a VPC network need to communicate with destinations in a VPC, on-premises, or other cloud provider network that is owned by a different business unit. However, the destination network contains subnets whose IP addresses overlap with the IP addresses of your VPC network. In this scenario, you create a Cloud NAT gateway for Private NAT that translates traffic between the subnets in your VPC network to the non-overlapping subnets of the other network.","upvote_count":"2","timestamp":"1741182540.0","poster":"Zek"},{"poster":"Peto12","timestamp":"1734978600.0","upvote_count":"1","comment_id":"1330905","content":"Selected Answer: B\nWith A you need to apply new IP addresses, with B you can use private NAT."},{"poster":"andreacola","content":"Selected Answer: B\nAssume that the resources in your VPC network need to communicate with the resources in a VPC network or an on-premises or other cloud provider network that is owned by a different business unit. However, that network contains subnets whose IP addresses overlap with the IP addresses of your VPC network. In this scenario, you create a Private NAT gateway that translates traffic between the subnets in your VPC network to the non-overlapping subnets of the other network.","upvote_count":"5","timestamp":"1730674560.0","comment_id":"1306675"},{"comment_id":"1293927","poster":"Abhinavchawlac2d","content":"Correct Option:\nB. Create a Cloud VPN connection from the new VPC to the data center, and create a Cloud NAT instance to perform NAT on the overlapping IP space.\n\nThis option effectively allows you to connect the two environments while addressing the overlapping IP space issue through NAT, ensuring that the VMs can communicate without conflicts.","upvote_count":"2","timestamp":"1728240960.0"},{"timestamp":"1727799480.0","poster":"3fd692e","comment_id":"1292025","content":"Selected Answer: B\nThere is a Private NAT you can use and is specifically designed to resolve overlapping private IP issues: https://medium.com/niveus-solutions/private-cloud-nat-and-why-we-need-it-on-gcp-f6ad0c96facb#:~:text=Private%20Cloud%20NAT%20with%20NCC,helps%20connect%20onprem%20to%20gcp.","upvote_count":"5"},{"poster":"lucaluca1982","timestamp":"1722406860.0","upvote_count":"1","comment_id":"1258609","content":"Selected Answer: B\nGiven that you are not going out to the internet and you need to use a Cloud Router for your VPC, you need to ensure that there is no overlap in the IP ranges between your data center and the newly acquired company's VPC. The best approach to manage this without renumbering the entire network is to use Network Address Translation (NAT) to handle the overlapping IP addresses."},{"content":"Selected Answer: B\nwould go for B","comment_id":"1242025","upvote_count":"2","timestamp":"1720092540.0","poster":"nhatne"},{"timestamp":"1718726220.0","upvote_count":"1","content":"The answer is B. Cloud VPN and Cloud NAT help you get around this problem easily without all the work of creating a new subnet and reassigning IPs to everything.\n\nCloud NAT: Network Address Translation (NAT) allows you to translate IP addresses in your VPC to a different IP range, avoiding conflicts with overlapping IP ranges in your data center. This ensures that traffic can flow between the environments without routing conflicts.\nCloud VPN: Establishing a Cloud VPN connection provides secure connectivity between the new VPC and your data center. By combining this with Cloud NAT, you can effectively manage and resolve the IP address overlap.","comments":[{"timestamp":"1718726340.0","comment_id":"1232502","poster":"Sephethus","content":"Cloud NAT does not directly resolve IP address conflicts due to overlapping ranges. Cloud NAT is typically used for instances without external IP addresses to access the internet while preserving their internal IPs for internal communications.","upvote_count":"1"}],"comment_id":"1232501","poster":"Sephethus"},{"upvote_count":"4","comment_id":"1224633","poster":"eff12c1","content":"Selected Answer: B\nUsing Cloud NAT to translate overlapping IP addresses is the most effective solution to ensure seamless connectivity between the new company's VPC and your company's data center without routing conflicts. This approach avoids the complexity of reconfiguring IP addresses and ensures that both networks can communicate effectively. https://cloud.google.com/nat/docs/overview#private-nat","comments":[{"upvote_count":"1","content":"It is not NAT, we are not going out to internet. We need cloud router","timestamp":"1717853760.0","comments":[{"upvote_count":"1","poster":"VegasDegenerate","timestamp":"1735689180.0","content":"NAT doesn’t necessarily need to be used only for internet communication","comment_id":"1335070"}],"poster":"ccpmad","comment_id":"1226743"}],"timestamp":"1717579680.0"},{"comment_id":"1222531","upvote_count":"1","timestamp":"1717223880.0","poster":"sandyrao","content":"Selected Answer: B\nAns is B"},{"poster":"pico","comment_id":"1209806","upvote_count":"2","timestamp":"1715434680.0","content":"Selected Answer: B\nhttps://cloud.google.com/nat/docs/overview#private-nat\n\nAssume that the resources in your VPC network need to communicate with the resources in a VPC network or an on-premises or other cloud provider network that is owned by a different business entity. However, the VPC network of that business entity contains subnets whose IP addresses overlap with the IP addresses of your VPC network. In this scenario, you create a Private NAT gateway that routes traffic between the subnets in your VPC network to the non-overlapping subnets of that business entity."},{"comment_id":"1176817","content":"Selected Answer: B\nI was absolutely sure that B was obviously wrong until I found that \nhttps://cloud.google.com/nat/docs/overview#private-nat\nSo it seems like the answer is B...","upvote_count":"8","timestamp":"1710795780.0","poster":"Polosaty","comments":[{"content":"B. THIS should be the accepted answer, the link you provide is 100% certain. It's a Private Hybrid NAT:\n\" ...private-to-private translations... traffic between VPC networks and on-premises networks...\"\n\"...IP addresses overlap with the IP addresses of your VPC network. In this scenario, you create a Private NAT gateway...\"\nB, 100%","comment_id":"1207541","upvote_count":"2","timestamp":"1715022780.0","poster":"JaimeMS"}]},{"poster":"shashii82","upvote_count":"1","content":"The challenge with Option A is that changing IP addresses can be complex and might impact existing applications, configurations, and dependencies within the new company's VPC. It might introduce additional complexity and potential risks during the integration process.\n\nOption C, on the other hand, allows you to maintain the existing IP addressing in the new company's VPC while selectively blocking the overlapping IP space during the routing process. This can be a more flexible and less disruptive approach, especially in scenarios where readdressing is not practical.\n\nIn summary, both options might have their use cases, but Option C provides a solution that doesn't require changing IP addresses and can help avoid potential disruptions caused by such changes.","comment_id":"1170299","timestamp":"1710072000.0"},{"timestamp":"1706861160.0","content":"Selected Answer: A\nwith C option we would not able to connect to VM with those overlapping IP.\nwe need to add a middle VPC between them. it will be more complicated.\nwe have not choice here except reassigning IP adresses so i choose option A","poster":"bargou","upvote_count":"1","comment_id":"1138283"},{"timestamp":"1702503060.0","content":"I think now the answer should change since Private NAT is publicly available: https://cloud.google.com/nat/docs/private-nat","poster":"stefanop","comment_id":"1095836","upvote_count":"7"},{"poster":"Jconnor","upvote_count":"1","comment_id":"1087832","timestamp":"1701713820.0","content":"Apply new IP addresses? You do not apply new IP, you replace them. Either poorly written or deceiving. To enable connectivity and avoid routing conflicts, C is perfect. Long term of course we need to replace IP, but not to enable connectivity. C."},{"upvote_count":"1","poster":"yilexar","comment_id":"1056875","content":"All answers are incorrect. Overall, it is a NAT question, but cloud NAT can't nat private IP space. No idea how route can solve the overlapping issue. \nThere is a third party NAT option: https://www.linkedin.com/pulse/resolving-overlapping-ip-issue-when-connecting-tofrom-bayu-wibowo","timestamp":"1698589560.0"},{"poster":"BigfootPanda","upvote_count":"5","content":"Selected Answer: A\nCould not be B, as Cloud NAT only apply on route targeting default gateway.\nCould not be C : if you block route advertisement, then you will have no route to your datacenter, and you will be unable to connect your datacenter\nCould not be D : blocking using firewall the overlapping IP space will not provide connectivity to these ressource\n\nSo answer could only be A : user should update its IP space so it does not overlap","timestamp":"1689748620.0","comment_id":"956326"},{"upvote_count":"3","poster":"salim_","timestamp":"1683728700.0","content":"Selected Answer: C\nI believe answer is C: \nhttps://cloud.google.com/network-connectivity/docs/router/how-to/advertising-subnets","comment_id":"894081"},{"poster":"JC0926","content":"Selected Answer: A\nA. Create a Cloud VPN connection from the new VPC to the data center, create a Cloud Router, and apply new IP addresses so there is no overlapping IP space.\n\nIn a situation where RFC 1918 IP ranges overlap between the new company's VPC and your data center IP space, it is important to reconfigure the IP addresses to avoid any conflicts. To enable connectivity, first create a Cloud VPN connection between the new VPC and the data center. Then, set up a Cloud Router to manage routing between the environments. Finally, apply new IP addresses to the new company's VPC to ensure there is no overlapping IP space with your data center. This will prevent routing conflicts when connectivity is established.","comment_id":"868928","timestamp":"1681344660.0","upvote_count":"5"},{"content":"Selected Answer: A\nCreate a Cloud VPN connection from the new VPC to the data center, create a Cloud Router, and apply new IP addresses so there is no overlapping IP space.When there is overlapping IP space between two networks that need to be connected, it is necessary to re-address one of the networks to eliminate the conflict.","upvote_count":"1","poster":"Deb2293","timestamp":"1678323660.0","comment_id":"833493"},{"content":"Your company has a project in Google Cloud with three Virtual Private Clouds (VPCs). There is a Compute\nD283ABFBEDB32CDCE3B3406B9C29DB2F\nEngine instance on each VPC. Network subnets do not overlap and must remain separated. The network configuration is shown below.","timestamp":"1674707880.0","comment_id":"788390","upvote_count":"1","poster":"i_am_robot"},{"timestamp":"1671986580.0","upvote_count":"1","poster":"thamaster","comment_id":"755826","content":"D is out because firewall is for blocking traffic, C is out router does not prevent overlapping IP, B nat does not like that, answer is A"},{"content":"Selected Answer: C\nApplying new IP addresses is the second step as it is time intensive, needs a lot of investigation and error-prone. Solution is to establish connection first (with C) and as a next step re-configure IP addresses.","comments":[{"timestamp":"1735689420.0","comment_id":"1335071","content":"But C alone isn’t the answer..","poster":"VegasDegenerate","upvote_count":"1"}],"comment_id":"740062","timestamp":"1670587260.0","upvote_count":"5","poster":"markus_de"},{"poster":"jaxclain","comments":[{"content":"Don’t see anything there. Please read this\nAssume that the resources in your VPC network need to communicate with the resources in a VPC network or an on-premises or other cloud provider network that is owned by a different business unit. However, that network contains subnets whose IP addresses overlap with the IP addresses of your VPC network. In this scenario, you create a Private NAT gateway that translates traffic between the subnets in your VPC network to the non-overlapping subnets of the other network.\nhttps://cloud.google.com/nat/docs/overview","upvote_count":"1","timestamp":"1735689540.0","comment_id":"1335072","poster":"VegasDegenerate"}],"comment_id":"729890","timestamp":"1669693800.0","upvote_count":"2","content":"Selected Answer: A\nThe correct answer is A, not sure why some here are confused with C... Cloud NAT won't serve this purpose, you can read the documentation here: https://cloud.google.com/nat/docs/troubleshooting#overlapping-ip-addresses"},{"timestamp":"1669545720.0","comment_id":"728134","content":"Answer is A","poster":"moustaoui","upvote_count":"1"},{"poster":"megumin","content":"Selected Answer: A\nA is ok","comment_id":"717802","timestamp":"1668414300.0","upvote_count":"1"},{"comment_id":"697345","timestamp":"1666005000.0","content":"Selected Answer: A\nBy definition, In cases where you have a VPC subnet and an on-premises route advertisement with overlapping IP ranges, Google Cloud directs egress traffic depending on their IP ranges.\nWith custom route advertisements, you choose which routes Cloud Router advertises to your on-premises router through the Border Gateway Protocol (BGP).\nTherefore:\nA - is a permanent solution to the problem which will require time, resources, testing and funding should you decide to change the ip addresses\nC - is an interim solution that will help to integrate the new company quickly, with view to change the overlaping range at some point in the future (if needed)\n\nThe question does not mention any information on the time, cost or complexity, therefore I will go for A","upvote_count":"7","poster":"minmin2020"},{"upvote_count":"1","poster":"enter_co","comment_id":"696155","content":"Selected Answer: C\nThe correct answer is A, because:\n- we want to establish communication\n- we want to solve the overlapping ranges problem. \nTherefore, \n- The correct answer is C https://cloud.google.com/network-connectivity/docs/router/how-to/advertising-removing-routes#bgp-session\n- A is not correct, because applying new addresses only extends the list of advertised addresses\n- B won't stop cloud router advertisements\n- D firewall rules won't change the list of advertised networks, it only treats the symptom, not the cause","timestamp":"1665913620.0"},{"poster":"AzureDP900","content":"A is right","comment_id":"672590","timestamp":"1663519380.0","upvote_count":"1"},{"upvote_count":"3","poster":"ShadowLord","content":"Selected Answer: C\nOptions C\n\"Your company has just acquired another company\" . Changing IP is not solution.\nSince solution are working independent, the Cloud Router with blocking IP is solution from traffic from each side wrongly traversing to other side ...","comment_id":"649110","timestamp":"1660949880.0"},{"upvote_count":"4","poster":"MQQ","content":"it can't be B: https://cloud.google.com/nat/docs/troubleshooting#overlapping-ip-addresses","comment_id":"636853","timestamp":"1658769840.0"},{"timestamp":"1657824480.0","upvote_count":"1","comment_id":"631502","content":"Selected Answer: B\nI'd go with B","poster":"imarri876"},{"timestamp":"1656704820.0","poster":"qbitgcp","upvote_count":"1","comment_id":"625860","content":"Ans A, https://aws.amazon.com/blogs/networking-and-content-delivery/connecting-networks-with-overlapping-ip-ranges/ >>This is always the first suggestion we make to customers. It won’t work in the service provider scenario above. However, if there’s an opportunity to renumber the networks, then it’s the best option"},{"upvote_count":"3","comment_id":"595004","timestamp":"1651322760.0","poster":"Venket","content":"Ans is C: Custom mode VPC networks better integrate into existing IP address management schemes. Because all auto mode networks use the same set of internal IP ranges, auto mode IP ranges might overlap when connected with your on-premises corporate networks"},{"comment_id":"589075","timestamp":"1650513780.0","poster":"wilwong","upvote_count":"5","content":"Selected Answer: A\nThat's basic network knowlage"},{"content":"Selected Answer: B\ncreate a Cloud NAT instance to perform NAT on the overlapping IP space","upvote_count":"2","comment_id":"587507","timestamp":"1650264780.0","poster":"JohnPi"},{"poster":"[Removed]","content":"Selected Answer: A\nI got similar question on my exam. Answered A.","upvote_count":"3","comment_id":"545502","timestamp":"1644612180.0"},{"content":"Answer is C:\nOtherwise, routing conflicts will occur. Assumption is that changing IP address space on either side will need planning and not provide immediate hybrid connectivity so I will not go with A.","upvote_count":"1","comment_id":"544262","timestamp":"1644458880.0","poster":"mshry"},{"poster":"DoVale","upvote_count":"2","content":"C is correctbecause, With custom route advertisements, you choose whichroutes Cloud Router advertises to your on-premises router through the BorderGateway Protocol (BGP). You can block the overlapping IPs by applyingcustom route advertisements.","comment_id":"525715","timestamp":"1642418460.0"},{"comment_id":"518125","upvote_count":"1","content":"It's A according to this documentation: https://cloud.google.com/network-connectivity/docs/router/concepts/overview#overlap\nLearned custom dynamic routes\nWhen a Cloud Router receives multiple next hops for the same destination prefix, Google Cloud uses route metrics and, in some cases, AS path length to create custom dynamic routes in your VPC network. The following sections describe that process:\n(...)\n- Overlapping IP ranges. In cases where you have a VPC subnet and an on-premises route advertisement with overlapping IP ranges, Google Cloud directs egress traffic depending on their IP ranges.","poster":"elenamatay","timestamp":"1641462300.0"},{"poster":"haroldbenites","content":"Go for A","timestamp":"1639048140.0","comment_id":"497658","upvote_count":"1"},{"timestamp":"1638504480.0","content":"Selected Answer: A\nA seems to be the correct answer","comment_id":"492946","upvote_count":"1","poster":"vincy2202"},{"poster":"joe2211","timestamp":"1637846340.0","content":"Selected Answer: A\nvote A","comment_id":"486675","upvote_count":"2"},{"comment_id":"485088","content":"Option C is not be correct as it is blocking the IP space","poster":"dmc123","upvote_count":"1","timestamp":"1637677320.0"},{"upvote_count":"1","timestamp":"1635621960.0","comment_id":"470377","content":"A snd B both will solve it. A is more accurate and good choice for no overlapping of IPs. \nB is not preferable here. \nC is also incorrect. Its an extra configuration","poster":"[Removed]"},{"content":"Vote for A\n\nWhy not C?\nFirst, custom advertisements should not be used for “blocking”, should use firewall rules for that purpose\nhttps://cloud.google.com/network-connectivity/docs/router/how-to/advertising-removing-routes\n“To secure your VPC network, use firewall rules to block traffic from reaching unadvertised subnets. Don't rely on hidden routes. Users can still reach unadvertised virtual machine (VM) instances through static routes.”\n\nSecond, if \"block\", connection can be established for those unblocked IPs, but on-prem and GCP VPC cannot communicate effectively","upvote_count":"5","timestamp":"1632959760.0","poster":"gingerbeer","comment_id":"454606"},{"comment_id":"454010","upvote_count":"2","timestamp":"1632908820.0","poster":"rottzy","content":"A looks okay,\nwhy bother sticking to same IP's - can't new ones be created?!"},{"upvote_count":"1","timestamp":"1629519360.0","poster":"PeppaPig","content":"Since the primary IP range for the subnet can be expanded, but not replaced or shrunk, after the subnet has been created. You must create new subnets to apply new IP ranges.\nhttps://cloud.google.com/vpc/docs/using-vpc#subnet-rules\n\nTechnically I would say \"A\" is ok. You can create a new VPC and apply a new IP range.\nBut in practice, it means you would have to consider the migration of existing resources in the old VPC to the new VPC, This is not going to be an easy task, and most likely there will be downtime during the migration.\nFor example, VPC migration is going to be a \"cold\" one, the VM must be stopped before it can be migrated.\nIf downtime is not an issue, then answer is \"A\", otherwise \"B\" looks like the only option","comment_id":"428489"},{"content":"no doubt A","comment_id":"415898","timestamp":"1627447140.0","upvote_count":"2","poster":"hello_aws"},{"upvote_count":"8","poster":"lazybeanbag","comment_id":"411429","content":"I think A is more suitable.\n\nReason:\nTo connect two networks together we need (1) either VPN or interconnect and (2) peering. When there is peering, you cannot have conflicting IP addresses.\nYou can use either Cloud VPN or Cloud Interconnect to securely connect your on-premises network to your VPC network.\n(https://cloud.google.com/vpc/docs/vpc-peering#transit-network)\nAt the time of peering, Google Cloud checks to see if there are any subnet IP ranges that overlap subnet IP ranges in the other network. If there is any overlap, peering is not established. \n(https://cloud.google.com/vpc/docs/vpc-peering#considerations)\nNAT is used to translate private to public IP and vice versa, however because we are connecting 2 networks together, they become private IPs. So it is not applicable.","timestamp":"1626938100.0"},{"poster":"victory108","content":"C. Create a Cloud VPN connection from the new VPC to the data center, create a Cloud Router, and apply a custom route advertisement to block the overlapping IP space.","comments":[{"content":"It is B.","timestamp":"1626143880.0","upvote_count":"1","poster":"RKS_2021","comment_id":"405075"}],"comment_id":"403978","upvote_count":"2","timestamp":"1626007440.0"},{"timestamp":"1625919720.0","content":"B\nCloud NAT can be configured to provide NAT to the internet for packets sent from the following:\nAn alias IP range assigned to the VM's network interface.\n\nThen,\nAlias IP addresses can be announced by Cloud Router to an on-premises network connected via VPN or Interconnect.\n\nref\nhttps://cloud.google.com/nat/docs/overview\nhttps://cloud.google.com/vpc/docs/alias-ip","comments":[{"upvote_count":"1","poster":"taoj","content":"sorry. I am also confused about the GCP service.\nbut out of GCP. As a normal solution Configure NAT to Enable Communication Between Overlapping Networks is definitely right.\nhttps://www.cisco.com/c/en/us/support/docs/ip/network-address-translation-nat/200726-Configure-NAT-to-Enable-Communication-Be.html","comment_id":"403355","timestamp":"1625922480.0"},{"comment_id":"427757","content":"Can I use Cloud NAT to connect a VPC network to another network to work around overlapping IP addresses?\nNo, Cloud NAT cannot apply to any custom route whose next hop is not the default internet gateway. For example, Cloud NAT cannot apply to traffic sent to a next hop Cloud VPN tunnel, even if the destination is a publicly routable IP address.","poster":"ezioauditore123","upvote_count":"2","timestamp":"1629412980.0"}],"poster":"taoj","upvote_count":"2","comment_id":"403329"},{"content":"I think A would only be possible. The requirements is to establish connectivity with no routing conflicts. Answer B, Cloud NAT is used for outbound internet connections for VM's that do not have an external IP. Answer C, is about blocking the overlapping IP space so that will not enable connectivity as per requirement. Answer D doesn't make any sense. So the only option I see is to change IP addresses on GCP/on prem side for the systems concerned that have overlapping IP","poster":"milan74","comment_id":"403242","upvote_count":"2","timestamp":"1625909160.0"},{"upvote_count":"1","content":"there is not no info about overlapping, everywhere says no overlap, this question is not good\n\nDoc says that primary and secondary segments not overlap that's why I don't get this Q\n\nwhy some say B?\nCloud NAT (network address translation) lets certain resources without external IP addresses create outbound connections to the internet.","comment_id":"401449","timestamp":"1625710020.0","poster":"kopper2019"},{"poster":"MamthaSJ","comment_id":"401165","timestamp":"1625680920.0","content":"Answer is B","comments":[{"timestamp":"1634396100.0","content":"There si not such thing as cloud nat instance","comment_id":"463141","upvote_count":"1","poster":"kopper2019"}],"upvote_count":"5"},{"timestamp":"1625561760.0","upvote_count":"4","poster":"JeffClarke111","comment_id":"399786","content":"Should be B"},{"content":"Why not A?","timestamp":"1625552280.0","comment_id":"399695","poster":"kbouwmee","upvote_count":"2"},{"timestamp":"1625509920.0","comment_id":"399379","upvote_count":"4","poster":"shaw2021","content":"should be B:\nhttps://cloud.google.com/architecture/best-practices-vpc-design","comments":[{"comment_id":"508291","timestamp":"1640316780.0","content":"According to this FAQ's section, NAT Service (refered to Google Cloud Services) the use of NAT in this context, is not possible: https://cloud.google.com/nat/docs/troubleshooting#overlapping-ip-addresses","poster":"BobbyFlash","upvote_count":"1"}]},{"timestamp":"1625143800.0","content":"I do not know if the plan of the integration really is to block the ressources. With NAT you could make them accessible, so that would be B","comment_id":"395965","poster":"Irbis","upvote_count":"4"}],"question_text":"Your company has just acquired another company, and you have been asked to integrate their existing Google Cloud environment into your company's data center. Upon investigation, you discover that some of the RFC 1918 IP ranges being used in the new company's Virtual Private Cloud (VPC) overlap with your data center IP space. What should you do to enable connectivity and make sure that there are no routing conflicts when connectivity is established?"},{"id":"DzNkubt0WdZkHCrK1Jbb","exam_id":4,"answer":"B","question_id":23,"answers_community":["B (60%)","A (40%)"],"timestamp":"2021-07-01 08:47:00","discussion":[{"upvote_count":"68","timestamp":"1625122020.0","poster":"TotoroChina","comment_id":"395614","comments":[{"comment_id":"461754","upvote_count":"4","comments":[{"comment_id":"726000","poster":"ale_brd_111","comments":[{"comment_id":"1302939","content":"Question literally says \"You want to minimize costs and infrastructure management effort.\"","timestamp":"1729875360.0","poster":"Amrx","upvote_count":"2"},{"upvote_count":"6","content":"OMG, you again?\n\n zetalexg says:\nIt's dissapointing that you waste your time writting on this topic instead of paying attention at the questions.","timestamp":"1680453480.0","comment_id":"859079","poster":"grejao"}],"upvote_count":"2","timestamp":"1669303260.0","content":"I think it's A.\nThe question does not mention anything about minimize the costs, all the questions in GCP exams that require minimize the costs as requirement literally mention that in the question.\nAlso in order to minimize the costs you need to build jobs that are fault tolerant, as workers instances are preemptible. This also requires some kind of Dev investment of work. So if not mentioned in the question fault tolerant and minimize costs then is not required/needed.\n\nDoc states below:\nOnly use preemptible nodes for jobs that are fault-tolerant or that are low enough priority that occasional job failure won't disrupt your business."}],"poster":"J19G","content":"Agree, the migration guide also recommends to think about preemptible worker nodes: https://cloud.google.com/architecture/hadoop/hadoop-gcp-migration-jobs#using_preemptible_worker_nodes","timestamp":"1634169420.0"},{"timestamp":"1625148000.0","content":"Hi TotoroChina, \nI had the same thought when I first read the question - the problem I see is, in real business I think you would try to mix preemtible instances and on-demand instances... Here you have to choose between only preemtible instances and on-demand instances... Preemptible instances have some downsides - so we would need more details and ideally a mixed approach. That's why both answers might be correcy, a) and b)...\nDo you see that different?\nThanks!\nCheers,\nD.","poster":"XDevX","comment_id":"396029","comments":[{"upvote_count":"5","timestamp":"1625710200.0","comments":[{"content":"B requires to create new instances at least every 24h.","poster":"HenkH","upvote_count":"2","timestamp":"1668108840.0","comment_id":"715510"}],"content":"but you need to reduce management overhead so B\nif you create a cluster manually and create and maintain GCE is not the way to go","poster":"kopper2019","comment_id":"401450"}],"upvote_count":"5"},{"upvote_count":"8","timestamp":"1667149320.0","poster":"Sukon_Desknot","content":"\"without modifying the underlying infrastructure\" is the watch word. Most likely did not utilize preemptible on-premises","comment_id":"707928"},{"timestamp":"1674123720.0","upvote_count":"3","comment_id":"781009","poster":"Yogi42","content":"A cost-savings consideration: Using preemptible VMs does not always save costs since preemptions can cause longer job execution with resulting higher job costs. This is mentioned in above link So I think Ans should be A"},{"timestamp":"1670180760.0","poster":"jasenmornin","content":"The question is very short and literally says \"You want to minimize costs\" lol","comment_id":"735382","upvote_count":"14"},{"timestamp":"1670235780.0","poster":"xprtz1","upvote_count":"8","content":"learn to read before apply to the exam","comment_id":"735843"},{"poster":"zetalexg","comment_id":"742976","timestamp":"1670858280.0","upvote_count":"30","content":"It's dissapointing that you waste your time writting on this topic instead of paying attention at the questions."},{"poster":"charline","comment_id":"726159","upvote_count":"2","timestamp":"1669320660.0","content":"in the question \"You want to minimize costs...\""}],"content":"Should be B, you want to minimize costs.\nhttps://cloud.google.com/dataproc/docs/concepts/compute/secondary-vms#preemptible_and_non-preemptible_secondary_workers"},{"comments":[{"content":"agreed","poster":"Manh","comment_id":"440207","upvote_count":"2","timestamp":"1630918620.0"}],"poster":"firecloud","comment_id":"416229","upvote_count":"35","timestamp":"1627479780.0","content":"It's A, the primary workers can only be standard, where secondary workers can be preemtible.------In addition to using standard Compute Engine VMs as Dataproc workers (called \"primary\" workers), Dataproc clusters can use \"secondary\" workers.\nThere are two types of secondary workers: preemptible and non-preemptible. All secondary workers in your cluster must be of the same type, either preemptible or non-preemptible. The default is preemptible."},{"timestamp":"1740358380.0","upvote_count":"2","content":"Selected Answer: B\nit mentioned cost saving, hence B is fine as Hadoop is usually used for batch processing so a bit of downtime is tolerable.","poster":"david_tay","comment_id":"1360797"},{"content":"Selected Answer: A\nA. Create a Dataproc cluster using standard worker instances. \n\nyou can use premtible instnace however should be less then 50% totaak wroker instances... could impact staiblirty...\n\ncons transient task failures.\n\nfollow best practice:\n\nGeneral Recommendations:\nStart Conservatively: Begin with a smaller percentage of preemptible workers (e.g., less than 30% of the total worker nodes). Monitor your jobs closely to see how they perform and adjust as needed.\nDon't Exceed 50%: Google generally recommends keeping the number of preemptible workers below 50% of the total worker nodes in your cluster. 1 This helps maintain stability and reduces the risk of significant job disruptions.","poster":"3a7557a","comment_id":"1354146","upvote_count":"1","timestamp":"1739138160.0"},{"comment_id":"1336138","timestamp":"1735933260.0","poster":"plumbig11","content":"Selected Answer: B\nDataproc, as they want to save costs, preemptible is the best option, for sure.","upvote_count":"2"},{"timestamp":"1727692560.0","upvote_count":"1","content":"Probably A\n- Primary workers must be standard\n- Preemptible doesn't always save cost\n- Infrastructure on prem doesn't have spot machines\n- You cannot choose spot/preemptible when creating the cluster, only when provisioning secondary nodes, which are actually preemptible by default.\n- They do not store data, only do data processing","poster":"Lestrang","comment_id":"1291525"},{"comment_id":"1290550","content":"Selected Answer: B\nB: being a multiple-choice question, we need to focus on explicit keywords here. \nManagement effort => Managed service -> Dataproc.\nCost-optimization => Preemptible. \n\nFor ones who say \"but that also requires fault toleration\": well, there is no explicit keyword in the question says \"we have critical jobs\" or \"out data scientists team has not takes into account toleration\". So we must not assume that's needed.","timestamp":"1727506440.0","poster":"pcamaster","upvote_count":"2"},{"poster":"afsarkhan","timestamp":"1720899720.0","comment_id":"1247457","upvote_count":"1","content":"Selected Answer: A\nI will go with A , reason preemptible instances are unpredictable and there is no mention of work criticallity. So my answer is A against B"},{"content":"Selected Answer: A\nA - only secondary workers can be preemptible and \"Using preemptible VMs does not always save costs since preemptions can cause longer job execution with resulting higher job costs\" according to: https://cloud.google.com/dataproc/docs/concepts/compute/secondary-vms#preemptible_and_non-preemptible_secondary_workers","comment_id":"1201260","poster":"Gino17m","timestamp":"1713954780.0","upvote_count":"1"},{"poster":"dija123","upvote_count":"2","comment_id":"1197982","content":"Selected Answer: B\nAgree with B","timestamp":"1713451560.0"},{"content":"Answer is B. \nThe secondary worker type instance for default Dataproc cluster is preemptible VMs.\nhttps://cloud.google.com/dataproc/docs/concepts/compute/secondary-vms","timestamp":"1711565340.0","poster":"Diwz","upvote_count":"1","comment_id":"1184294"},{"content":"Dataproc: Dataproc is a fully managed Apache Spark and Hadoop service on Google Cloud Platform. It allows you to run clusters without the need to manually deploy and manage Hadoop clusters on Compute Engine.\n\nPreemptible Worker Instances: Preemptible instances are short-lived, cost-effective virtual machine instances that are suitable for fault-tolerant and batch processing workloads. Since Hadoop jobs can often tolerate interruptions, using preemptible instances can significantly reduce costs.\n\nOption B leverages the benefits of Dataproc for managing Hadoop clusters without the need for manual deployment and takes advantage of preemptible instances to minimize costs. This aligns well with the goal of minimizing both costs and infrastructure management efforts.","comment_id":"1170304","timestamp":"1710072540.0","poster":"shashii82","upvote_count":"1"},{"poster":"VidhyaBupesh","timestamp":"1709022420.0","comment_id":"1160336","upvote_count":"1","content":"Using preemptible VMs does not always save costs since preemptions can cause longer job execution with resulting higher job costs"},{"timestamp":"1708258740.0","poster":"Amrita2012","comment_id":"1153254","content":"Selected Answer: A\nUsing standard Compute Engine VMs as Dataproc workers (called \"primary\" workers), Preemptible can be only used for secondary workers hence A is valid answer","upvote_count":"1"},{"content":"Selected Answer: B\nminimize costs -> preemtipble","poster":"Pime13","timestamp":"1706859540.0","comment_id":"1138264","upvote_count":"3"},{"poster":"d0094d6","upvote_count":"3","timestamp":"1706854080.0","comment_id":"1138167","content":"Selected Answer: B\nYou want to minimize costs and infrastructure management effort > B"},{"content":"Selected Answer: A\nIt is A","poster":"didek1986","timestamp":"1705606440.0","comment_id":"1126139","upvote_count":"1"},{"timestamp":"1704873480.0","poster":"Romio2023","content":"Selected Answer: B\nAnswer should be B, because minizing the costs is wanted.","comment_id":"1118317","upvote_count":"1"},{"content":"A is the correct response. Per documentation \"You can gain low-cost processing power for your jobs by adding preemptible worker nodes to your cluster. These nodes use preemptible virtual machines.\" The focus of the question is to reduce cost, hence preempttible VM works best","comments":[{"comment_id":"1115245","timestamp":"1704552360.0","upvote_count":"1","content":"B not A","poster":"discuss24"}],"poster":"discuss24","timestamp":"1704552120.0","comment_id":"1115239","upvote_count":"1"},{"poster":"odacir","comment_id":"1074084","content":"Selected Answer: B\nB. \n- migrate Hadoop jobs -> dataproc\n- saving money -> preemptible(spot)","upvote_count":"1","timestamp":"1700319120.0"},{"upvote_count":"2","content":"Selected Answer: B\nThe Answer should be Spot VMs (This has been changed in actual Exam) Spot VM's have no expiration.","timestamp":"1698052260.0","poster":"CyanideX","comment_id":"1051639"},{"upvote_count":"1","content":"Selected Answer: B\nI think it's B because it literaly says \"minimize costs\" for a Job-like workload","timestamp":"1697985360.0","comment_id":"1050757","poster":"cchiaramelli"},{"poster":"Murtuza","content":"In the real exam wording has change from \" preemptible worker instances \" to SPOT instances","upvote_count":"3","timestamp":"1697459880.0","comment_id":"1044948"},{"upvote_count":"1","comment_id":"1012249","content":"Selected Answer: B\nIt is B","poster":"didek1986","timestamp":"1695209580.0"},{"comment_id":"1006020","poster":"smlabonia","upvote_count":"1","timestamp":"1694548260.0","content":"Selected Answer: B\nWe should use preemtible vms in order to minimize costs."},{"poster":"Murtuza","timestamp":"1694342820.0","upvote_count":"1","comment_id":"1003912","content":"Preemptible worker instances are a cost-effective option for running Hadoop jobs in Dataproc. These instances are the same as regular Compute Engine instances, but with the caveat that Google may reclaim them at any time with a 30-second warning. Because preemptible instances are priced at a significant discount compared to regular instances, they can help you save money on your Hadoop jobs.\n\nHere are some additional reasons why choosing option B is the best choice:\n\nOption A: Creating a Dataproc cluster using standard worker instances is a viable option, but it would likely be more expensive than using preemptible instances. Standard instances are priced at a premium compared to preemptible instances, which would increase the cost of running your Hadoop jobs."},{"timestamp":"1694322180.0","comments":[{"upvote_count":"1","content":"Thank you for your explanation. It clears things up for me.\nHere is a link to the Dataproc documentation explaining that:\nhttps://cloud.google.com/dataproc/docs/concepts/compute/secondary-vms","comment_id":"1107246","poster":"e5019c6","timestamp":"1703716620.0"}],"upvote_count":"3","poster":"daidaidai","comment_id":"1003702","content":"Selected Answer: B\nYes, when using Google Cloud Dataproc with preemptible VMs (Virtual Machines) as worker nodes, if a preemptible VM is reclaimed or experiences an issue, Dataproc will still continue to operate.\n\nDataproc is designed with the transient nature of preemptible VMs in mind. If a preemptible VM gets reclaimed, Dataproc will attempt to migrate tasks that were running on that VM to other available nodes (either standard or preemptible) to continue the work.\n\nHowever, the main risk with preemptible VMs is that they can be reclaimed by Google Cloud at any moment. Hence, they are best suited for short-term, non-critical workloads that can tolerate disruptions. To ensure higher stability and reliability, the master node and the default worker nodes of a Dataproc cluster utilize standard (non-preemptible) VMs. Only additional worker nodes can be configured as preemptible VMs.\n\nIn short, to answer your question, Dataproc will continue to operate even when a preemptible VM faces disruptions. However, for ensuring higher stability and continuity, it's recommended to keep critical tasks on standard VMs."},{"content":"Selected Answer: A\nminimized costs + no change in underlying structure = can't be preemptible so A ?","poster":"PKookNN","upvote_count":"1","timestamp":"1693387980.0","comment_id":"993906"},{"content":"Selected Answer: A\nIMHO: answer A, preemtible can only be added as secondary workers, these won't even store data, they are just here to scale compute at lowest cost. B suggests you can create a cluster based only on preemtible workers which is not possible.","timestamp":"1692088500.0","upvote_count":"1","poster":"rusll","comment_id":"981425"},{"comments":[{"poster":"grubb","timestamp":"1697330100.0","upvote_count":"1","comment_id":"1043787","content":"Winning comment right there. Thank you for the link to the documentation"}],"poster":"wooloo","timestamp":"1690456440.0","upvote_count":"2","content":"https://cloud.google.com/architecture/hadoop/hadoop-gcp-migration-jobs#using_preemptible_worker_nodes\n\nIt should combine regular and pre-emtion nodes, so both A and B are correct.\nBut if there are only preemtion nodes, the migration task may fail (\" the more preemptible nodes you use relative to standard nodes, the higher the chances are that the job won't have enough nodes to complete the task\")","comment_id":"964636"},{"comment_id":"956330","content":"Selected Answer: A\nVoting A as preemptible workload should only be used with preemptive task.","poster":"BigfootPanda","upvote_count":"2","timestamp":"1689748680.0"},{"timestamp":"1684003080.0","comment_id":"897008","poster":"mateuszma","upvote_count":"3","content":"Selected Answer: A\nVoting A - preemptible/spot are nice but not in this case. Such setup would require additional changes within ETL pipelines to support unexpected brakes."},{"upvote_count":"1","content":"Selected Answer: B\nB\nminimize costs","comment_id":"872446","poster":"JC0926","timestamp":"1681716840.0"},{"comments":[{"upvote_count":"1","timestamp":"1704552300.0","comment_id":"1115242","poster":"discuss24","content":"Leveraging a cluster of preemptible VMs would resolve the issue. However, focus on what the question id asking. The question is focus on reducing cost, which in this case, preemptible VMs would be the best option"}],"content":"Selected Answer: A\nPreemptible instances may be terminated at any time, which could disrupt your Hadoop jobs.","poster":"taer","timestamp":"1680251580.0","upvote_count":"3","comment_id":"856795"},{"comment_id":"856737","poster":"musumusu","content":"Answer B:\n2 reasons: Save Cost and Data Science Team job, which means the job will not run all the time like client side global application or real time processing.So, job will be used mainly for training models etc.","upvote_count":"1","timestamp":"1680248580.0"},{"upvote_count":"1","content":"I am voting for A: You want to minimize costs and infrastructure management effort.\nBecause I do not have any information about the duration of the jobs, I would start with Pre-emptible VMs. Cost reduction of up to 70%, if I remember correctly.","comment_id":"849922","poster":"mifrah","timestamp":"1679732580.0"},{"upvote_count":"1","timestamp":"1678430340.0","poster":"segkhachat","content":"Selected Answer: A\nShould be A, not all processes are compatible with preemptive instances. There is no mentioning about fault tolerancity.","comment_id":"834689"},{"timestamp":"1678323780.0","poster":"Deb2293","content":"Selected Answer: B\nto minimize costs and infrastructure management effort, it should be B","upvote_count":"1","comment_id":"833494"},{"timestamp":"1677513300.0","poster":"telp","content":"Selected Answer: B\nReduce cost = preemptible\nNo management = no VM\nso answer is B","upvote_count":"2","comment_id":"823915"},{"poster":"razabpn","content":"Selected Answer: B\nB: You want to minimize costs and infrastructure management efforts.\nThe preemptible worker provides 1. lower price 2—reduction in infra management, unlike VMs which you need to manage.","upvote_count":"3","comment_id":"811772","timestamp":"1676631360.0"},{"content":"Selected Answer: B\nShould be B, Questions require to minimize costs.","poster":"WFCheong","upvote_count":"1","comment_id":"773126","timestamp":"1673498040.0"},{"content":"Selected Answer: B\nB is the correct answer, Use ephemeral cluster to reduce cost, use small persistent storage only if necessary.\n\nhttps://cloud.google.com/architecture/hadoop#moving_to_an_ephemeral_model","poster":"examch","upvote_count":"1","comment_id":"766275","timestamp":"1672898340.0"},{"timestamp":"1672512420.0","poster":"windsor_43","upvote_count":"4","content":"The Answer is B\n\nJust had my exam today with a pass, this question was in the exam. Dated 31/12/22\nThanks to this site it was by far my most valuable","comment_id":"762855"},{"poster":"richlee0423","upvote_count":"1","timestamp":"1672266720.0","content":"Selected Answer: B\nGoogle Dataproc supports different instance types, or preemptible virtual machines, so you can leverage the ideal server sizes that fit your needs.","comment_id":"760392"},{"timestamp":"1671986880.0","poster":"thamaster","upvote_count":"1","comment_id":"755830","content":"Selected Answer: B\nI'll chose B but I'm not sure because you can't use only pre emptible you need at least one instance."},{"poster":"gonlafer","comment_id":"744305","upvote_count":"1","timestamp":"1670952180.0","content":"Selected Answer: B\nShould be B, you want to minimize costs."},{"comment_id":"743448","timestamp":"1670891700.0","content":"Selected Answer: A\nI vote A, Standard is the best option to meet this condition \"without modifying the underlying infrastructure\". I am aware of minimize cost, however, preemtible would not meet the first condition.","poster":"1209apl","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: B\nas \"You want to minimize costs\" so b is ok","timestamp":"1669320720.0","comment_id":"726161","poster":"charline"},{"poster":"megumin","timestamp":"1668414900.0","content":"Selected Answer: A\nA is ok. We need standard because we can't modifying the underlying infrastructure","upvote_count":"1","comment_id":"717809"},{"content":"Google Cloud tutorial: https://youtu.be/iyqtKV0fnFc?t=143\nso apparentry some standard nodes are required. Anyone keen to confirm right option?","upvote_count":"1","timestamp":"1665914880.0","poster":"melono","comment_id":"696163"},{"poster":"Rajeev26","content":"Selected Answer: B\nshould be B as need to minimize cost","upvote_count":"2","timestamp":"1665216480.0","comment_id":"689125","comments":[]},{"timestamp":"1664467560.0","comment_id":"682826","poster":"VinayNune","upvote_count":"1","content":"Selected Answer: A\nShould be A think on the lines of reliability. The question does not mention about the nature of jobs. Given reliability and cost, i would go for reliability.\nhttps://cloud.google.com/architecture/hadoop/hadoop-gcp-migration-jobs\n\"Consider the inherent unreliability of preemptible nodes before choosing to use them. Dataproc attempts to smoothly handle preemption, but jobs might fail if they lose too many nodes. Only use preemptible nodes for jobs that are fault-tolerant or that are low enough priority that occasional job failure won't disrupt your business.\""},{"poster":"abirroy","content":"Selected Answer: A\nI think A is a better option. Preemptible is better for secondary nodes.","upvote_count":"1","timestamp":"1663618260.0","comment_id":"673607"},{"poster":"AzureDP900","upvote_count":"1","timestamp":"1663519500.0","comment_id":"672594","content":"B is right"},{"content":"Selected Answer: B\nB is the answer.\n\nhttps://cloud.google.com/architecture/hadoop/hadoop-gcp-migration-jobs#using_preemptible_worker_nodes\nYou can gain low-cost processing power for your jobs by adding preemptible worker nodes to your cluster. These nodes use preemptible virtual machines.","timestamp":"1663469400.0","poster":"zellck","comment_id":"671942","upvote_count":"1"},{"upvote_count":"1","comment_id":"671300","timestamp":"1663395360.0","content":"Selected Answer: A\nA is the most accurate one.","poster":"Nirca"},{"upvote_count":"1","content":"Selected Answer: A\nDataproc cluster nodes are ephemeral and therefore are short-lived. Preemptible instances are only supported in the secondary worker, which is not an answer choice.","timestamp":"1660863660.0","poster":"cloudinit","comment_id":"648649"},{"comment_id":"647325","poster":"bchmni","upvote_count":"2","content":"Selected Answer: A\nYou cannot add just a preemptible woker (a secondary worker) without having a primary worker. Cloud data proc automatically adds a primary worker if not specified during cluster creation.","timestamp":"1660587840.0"},{"content":"Selected Answer: B\nSo guys, first of C and D is eliminated since they want to minimize management overhead. Dataproc is fully managed.\n\nNow between A and B as per link, preemptible VMs help reduce costs - https://cloud.google.com/dataproc/pricing","poster":"Thornadoo","timestamp":"1658710980.0","upvote_count":"2","comment_id":"636298"},{"upvote_count":"2","comment_id":"633304","timestamp":"1658197980.0","content":"Selected Answer: A\nYes, you can lower costs by adding preemptible VMs to your clusters but they can't be the main/only VMs running (not at least with some modification)","poster":"marmar11111"},{"comment_id":"632234","timestamp":"1657984800.0","poster":"DAYAGOWDA","upvote_count":"3","content":"The question say without modifying the underlaying infrastructure.Consider the inherent unreliability of preemptible nodes before choosing to use them. Dataproc attempts to smoothly handle preemption, but jobs might fail if they lose too many nodes. Only use preemptible nodes for jobs that are fault-tolerant or that are low enough priority that occasional job failure won't disrupt your business."},{"content":"Selected Answer: A\nHadoop job are generally long running ones, so preempible instances won't be a good choice.","timestamp":"1657321680.0","comment_id":"628923","poster":"ritikamittal","upvote_count":"3"},{"timestamp":"1657130280.0","comment_id":"628044","comments":[{"content":"How did the exam compare to these 262 questions??","poster":"Bill76","upvote_count":"2","timestamp":"1658272860.0","comment_id":"633776"}],"poster":"mv2000","content":"06/30/2022 Exam","upvote_count":"2"},{"poster":"Superr","upvote_count":"2","content":"Selected Answer: A\nA is good","comment_id":"609865","timestamp":"1654020960.0"},{"comments":[{"timestamp":"1664320680.0","poster":"jay9114","comment_id":"681297","upvote_count":"2","content":"I like your logic. Thank you for sharing."}],"poster":"ridyr","upvote_count":"2","timestamp":"1651678800.0","content":"Selected Answer: B\nThe dump that I bought had A, but it has been worthless as far as answers or accuracy. Not certain that A covers lowering cost. I would suggest only one of the Dataproc answers as manually deploying Hadoop does nothing for the 2nd part of the desired results. \"minimize..infrastructure management effort\"..or so cloud products proclaim. \n\nI see mixed results on other dump sites between the 2 Dataproc answers. \nBeing that the question wants two desired results and not certain that standard worker instances achieves the first part(minimize costs), I am going with Preemptible. \n\nPreemptible instances lower cost, but some standard worker nodes(dataproc will require worker instances to process the data..that's fact as I see it) would still be required and have to work out the ratio between them as I read it. So to achieve the results your going to have to add standard worker instances(They'll be a requirement whether or not you use preemptible instances), but preemptible definitely gets you into the minimizing cost part of question. So to me preemptible elaborates more on the solution to minimize costs.","comment_id":"596916"},{"content":"Selected Answer: A\nyou cant have worker nodes completely only with preemptible instances.Only the secondary worker nodes is possible to add Preemtibe or non-preemtible. As the answer B states completely with preemtible instances which is impossible.","poster":"googlearch","upvote_count":"3","timestamp":"1646591040.0","comment_id":"562206"},{"content":"Selected Answer: A\nreduce costs with minimal changes. - so go with option A","poster":"neversaynever","timestamp":"1642940580.0","comment_id":"530511","upvote_count":"1"},{"timestamp":"1642699620.0","poster":"Q_Review","content":"Selected Answer: B\n\"You can gain low-cost processing power for your jobs by adding preemptible worker nodes to your cluster. These nodes use preemptible virtual machines.\n\nConsider the inherent unreliability of preemptible nodes before choosing to use them. Dataproc attempts to smoothly handle preemption, but jobs might fail if they lose too many nodes. Only use preemptible nodes for jobs that are fault-tolerant or that are low enough priority that occasional job failure won't disrupt your business.\"","comment_id":"528618","upvote_count":"2"},{"poster":"pddddd","content":"minimum change of infrastructure - using Preemptible instance is a big change. We do not know whether jobs support interruption...","upvote_count":"6","comment_id":"520834","timestamp":"1641817380.0"},{"upvote_count":"1","content":"Correct Answer is B","timestamp":"1640789460.0","comment_id":"512284","poster":"GauravLahoti"},{"comment_id":"511550","timestamp":"1640722680.0","upvote_count":"2","poster":"ehgm","content":"We don't know if the Jobs are able to handle with preemptable VMs. Just using a Dataproc cluster we will reduce cost, when the job is not running (idle) we don't pay for it. \nDataproc is a managed service!"},{"upvote_count":"1","timestamp":"1640285340.0","comment_id":"508095","poster":"mardon","content":"Selected Answer: B\nB since question says minimum cost"},{"timestamp":"1639699320.0","poster":"Mount09","upvote_count":"1","comment_id":"503257","content":"Selected Answer: B\nPreemptible- reducing costs . Was not specified if workload needed to remain. Vote B."},{"timestamp":"1639048200.0","upvote_count":"1","comment_id":"497661","poster":"haroldbenites","content":"Go for B"},{"upvote_count":"1","poster":"[Removed]","content":"Selected Answer: B\nShould be B. Due to cost factor.","comment_id":"492623","timestamp":"1638458520.0"},{"timestamp":"1638457740.0","upvote_count":"1","comment_id":"492612","poster":"pakilodi","content":"Selected Answer: B\nB) minimize costs"},{"poster":"joe2211","upvote_count":"3","timestamp":"1637846820.0","content":"Selected Answer: B\nvote B","comment_id":"486683"},{"content":"Only the secondary worker has these two types - either preemptible or non-preemptible instance","upvote_count":"1","timestamp":"1637677560.0","comment_id":"485089","poster":"dmc123"},{"content":"Answer is B (Reduce cost). Create a cluster with primary-worker shuffle for Spark and HCFS shuffle for MapReduce:\n\n\ngcloud dataproc clusters create cluster-name \\\n --region=region \\\n --properties=dataproc:efm.spark.shuffle=primary-worker \\\n --properties=dataproc:efm.mapreduce.shuffle=hcfs \\\n --image-version=1.4 \\\n --worker-machine-type=n1-highmem-8 \\\n --num-workers=25 \\\n --num-worker-local-ssds=2 \\\n --secondary-worker-type=preemptible \\\n --secondary-worker-boot-disk-size=500GB \\\n --num-secondary-workers=25","comment_id":"482548","upvote_count":"2","timestamp":"1637413200.0","poster":"ravisar"},{"comment_id":"482338","content":"Answer A, I will go with A as we don't know if excisting workload can handle shut down","upvote_count":"1","timestamp":"1637395800.0","poster":"nehaxlpb"},{"timestamp":"1633590300.0","upvote_count":"1","poster":"anki_4AA","content":"by default dataproc adds two primary worker instances,even if we don't specify one,with two primary and a pool of secondary workers we can minimize costs","comment_id":"458567"},{"timestamp":"1632908940.0","content":"A - what if hadoop jobs run for a longer time?","poster":"rottzy","upvote_count":"1","comment_id":"454013"},{"timestamp":"1632848280.0","poster":"AK2020","upvote_count":"4","content":"Should be B as Hadoop is background Job.\n\nIf your apps are fault-tolerant and can withstand possible instance preemptions, then preemptible instances can reduce your Compute Engine costs significantly. For example, batch processing jobs can run on preemptible instances. If some of those instances stop during processing, the job slows but does not completely stop. Preemptible instances complete your batch processing tasks without placing additional workload on your existing instances and without requiring you to pay full price for additional normal instances.","comment_id":"453531"},{"timestamp":"1632154020.0","poster":"jask","upvote_count":"1","content":"Should be A. In addition to using standard Compute Engine VMs as Dataproc workers (called \"primary\" workers), Dataproc clusters can use \"secondary\" workers. \nhttps://cloud.google.com/dataproc/docs/concepts/compute/secondary-vms#preemptible_and_non-preemptible_secondary_workers","comment_id":"448334"},{"content":"Should be B but not 100% preempible VM. The answers aren't well defined. I mean: in a cluster you can use a % of preemptible VM to lower costs, but Google recommends: \"For best results, the number of preemptible workers in your cluster should be less than 50% of the total number of all workers (primary plus all secondary workers) in your cluster.\"","poster":"Besss","comment_id":"446928","upvote_count":"3","timestamp":"1631942820.0"},{"comment_id":"427537","content":"I agree with A after reading this link. Seems like preempt machines do not work standalone. They will only work in conjunction with standard machines. \nhttps://cloud.google.com/architecture/hadoop/hadoop-gcp-migration-jobs\n\nIf you decide to use preemptible worker nodes, consider the ratio of regular nodes to preemptible nodes. There is no universal formula to get the best results, but in general, the more preemptible nodes you use relative to standard nodes, the higher the chances are that the job won't have enough nodes to complete the task. You can determine the best ratio of preemptible to regular nodes for a job by experimenting with different ratios and analyzing the results","upvote_count":"2","poster":"MikeB19","timestamp":"1629384960.0"},{"upvote_count":"2","comments":[{"content":"Agreed. Both A and B incorrect in that sense.","comment_id":"472963","timestamp":"1636101420.0","poster":"tsiddique","upvote_count":"1"}],"poster":"rnv64","comment_id":"424686","content":"questions says need to migrate \"without changing underlying infrastructure\".\nto me implies on-prem cluster needs to be created as-is in cloud compute service.\nmigrating to dataproc means change in underlying infrastructure because it's PaaS.","timestamp":"1628928240.0"},{"poster":"AnilKr","upvote_count":"1","content":"Ans - B, you may decide to use preemptible instances to lower per-hour compute costs for non-critical data processing or to create very large clusters at a lower total cost.\n\nhttps://cloud.google.com/dataproc/docs/concepts/compute/secondary-vms#preemptible_and_non-preemptible_secondary_workers","comment_id":"423643","timestamp":"1628774820.0"},{"comment_id":"403960","upvote_count":"5","content":"B. Create a Dataproc cluster using preemptible worker instances.","timestamp":"1626006180.0","poster":"victory108"},{"timestamp":"1625681040.0","upvote_count":"6","comment_id":"401166","content":"Answer is B","poster":"MamthaSJ"},{"comment_id":"399787","upvote_count":"6","poster":"JeffClarke111","content":"I would go for B due to \"minimize costs\"","timestamp":"1625561940.0"}],"unix_timestamp":1625122020,"choices":{"B":"Create a Dataproc cluster using preemptible worker instances.","D":"Manually deploy a Hadoop cluster on Compute Engine using preemptible instances.","A":"Create a Dataproc cluster using standard worker instances.","C":"Manually deploy a Hadoop cluster on Compute Engine using standard instances."},"isMC":true,"answer_images":[],"answer_ET":"B","question_images":[],"question_text":"You need to migrate Hadoop jobs for your company's Data Science team without modifying the underlying infrastructure. You want to minimize costs and infrastructure management effort. What should you do?","answer_description":"","topic":"1","url":"https://www.examtopics.com/discussions/google/view/56684-exam-professional-cloud-architect-topic-1-question-119/"},{"id":"JcdomvvAShHAz8OUtsI8","url":"https://www.examtopics.com/discussions/google/view/7134-exam-professional-cloud-architect-topic-1-question-12/","isMC":true,"answer_ET":"B","answer_images":[],"timestamp":"2019-10-24 15:17:00","choices":{"D":"Google Compute Engine with Google BigQuery","A":"Google Cloud Dataproc","C":"Google Container Engine with Bigtable","B":"Google Cloud Dataflow"},"question_images":[],"answers_community":["B (100%)"],"exam_id":4,"answer":"B","question_id":24,"discussion":[{"comment_id":"17188","poster":"Eroc","content":"All four options can accomplish what the question asks, in regards to batching and streaming processes. \"A\" is for Apache Spark and Hadoop, a juggernaut in speed of data processing. \"B\" is Google's best attempt at TIBCO, Ab Initio, and other processing technology, built explicity for visualizing batch operations and streams without through various labeled circuit boards. \"C\" and \"D\" are used within \"A\" and \"B\" and would require more work and higher risk. I'd guess Google wants you to select \"B\"","upvote_count":"37","timestamp":"1726852680.0"},{"comment_id":"44699","upvote_count":"6","content":"answer: B","poster":"2g","timestamp":"1580389680.0"},{"upvote_count":"1","content":"Selected Answer: B\nDataFlow is a managed service which processes stream and batch data. \nSo it prefectly fits the need","timestamp":"1738069980.0","poster":"hpf97","comment_id":"1347910"},{"upvote_count":"4","content":"Selected Answer: B\n1. Unified Batch and Stream Processing: Dataflow is a fully managed service designed for both batch and stream data processing. This makes it ideal for your company's needs, as they require both hourly batch jobs and live stream processing.\n2. No Existing Code: Dataflow provides a unified programming model and SDKs (Java, Python) for building data pipelines, which is beneficial since your company doesn't have existing code and needs to develop new solutions.\n3. Serverless and Scalable: Dataflow is serverless, meaning you don't need to manage infrastructure. It automatically scales resources based on the workload, ensuring efficient processing of both batch and stream data.\n4. Cost-Effective: Dataflow's autoscaling and pay-per-use model optimize costs by only utilizing resources when needed.","timestamp":"1731270180.0","comment_id":"1309611","poster":"Ekramy_Elnaggar"},{"poster":"Singapore123","comment_id":"1288547","content":"Selected Answer: B\nB. Google Cloud Dataflow\n\nExplanation:\nUnified Processing:\n\nGoogle Cloud Dataflow is designed to handle both batch and stream processing in a unified manner. This means you can process data as it arrives (stream processing) and also perform scheduled batch jobs efficiently.\nServerless and Scalable:\n\nDataflow is serverless, which means you don’t have to worry about managing the underlying infrastructure. It automatically scales to handle varying workloads, making it ideal for optimizing operations based on live data streams and scheduled jobs.\nIntegration with Other Google Cloud Services:\n\nDataflow integrates well with other Google Cloud services, such as Google Cloud Storage, BigQuery, and Pub/Sub. This makes it easier to build a comprehensive data pipeline that can analyze data streams effectively.\nFlexible SDKs:\n\nDataflow supports popular programming languages like Java and Python, allowing your team to write custom processing logic as needed.","timestamp":"1727175240.0","upvote_count":"3"},{"poster":"Hungdv","timestamp":"1723096380.0","content":"Choose B","comment_id":"1262332","upvote_count":"1"},{"comment_id":"1119962","content":"Selected Answer: B\nB is correct","upvote_count":"1","poster":"hzaoui","timestamp":"1704990060.0"},{"poster":"devakram","comment_id":"1102049","upvote_count":"1","content":"chatGPT answers:\nB. Google Cloud Dataflow\nGoogle Cloud Dataflow is a fully managed service for stream and batch data processing. It is built on Apache Beam and provides a unified programming model, making it an ideal choice for scenarios where both batch and stream data processing are required. Dataflow simplifies the complexities of data parallel processing, allowing for easy development and maintenance of data processing pipelines. It integrates well with other Google Cloud services, like BigQuery for analytics and Cloud Storage for storing data, providing a comprehensive solution for real-time and batch data processing needs.","timestamp":"1703116260.0"},{"poster":"BiddlyBdoyng","upvote_count":"5","content":"The word analysis throws me off. Wonder if the question is just written incorrectly here? I'd say Dataflow is a key tool to enable the processing of the data to be able to do the analysis but feels like the final analysis should be in a database.","comment_id":"930747","timestamp":"1687448340.0"},{"comment_id":"847537","timestamp":"1679520420.0","upvote_count":"2","content":"Selected Answer: B\nB is the answer","poster":"alekonko"},{"comment_id":"819686","poster":"Deb2293","upvote_count":"2","timestamp":"1677182760.0","content":"Selected Answer: B\nA is a managed Hadoop and Spark service. C and D are mostly for petabyte kinds of data. So remains B (suitable for ETL jobs)"},{"timestamp":"1671524160.0","comment_id":"750642","poster":"omermahgoub","content":"To analyze a data stream and optimize operations, your company could consider using Google Cloud Dataflow, which is a fully-managed, cloud-native data processing service that can handle both batch and stream processing.\n\nGoogle Cloud Dataflow is designed to handle large volumes of data and can scale up or down automatically to meet the needs of the workload. It provides a number of pre-built connectors and integrations that make it easy to ingest data from a variety of sources, and it offers a range of processing options, including batch processing and stream processing, that can be used to analyze the data in real-time.\n\nOption A: Google Cloud Dataproc, option C: Google Container Engine with Bigtable, and option D: Google Compute Engine with Google BigQuery, while potentially useful for certain types of data processing, would not necessarily be well-suited to handle both batch and stream processing in the way that Google Cloud Dataflow can","upvote_count":"3"},{"upvote_count":"2","content":"answer is D for me the question is which tool for analyse data. Dataflow does not analyse data","comment_id":"747292","poster":"thamaster","timestamp":"1671201540.0"},{"content":"Selected Answer: B\nok for B","timestamp":"1667637780.0","poster":"megumin","upvote_count":"1","comment_id":"711623"},{"upvote_count":"1","comment_id":"701667","poster":"Mahmoud_E","content":"Selected Answer: B\nB is the right answer","timestamp":"1666457640.0"},{"timestamp":"1665950640.0","content":"B is correct","comment_id":"696506","poster":"AzureDP900","upvote_count":"1"},{"comment_id":"693620","content":"Selected Answer: B\nB. Google Cloud Dataflow","poster":"minmin2020","timestamp":"1665636900.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"674158","poster":"holerina","content":"correct is B use data flow for stream and batch process","timestamp":"1663677900.0"},{"poster":"Jay_Krish","timestamp":"1662271920.0","upvote_count":"2","content":"Selected Answer: B\nAnswer seems to be B in most other websites as well.\n\nhttps://cloud.google.com/solutions/authenticating-corporate-users-in-a-hybrid-environment","comment_id":"658998"},{"timestamp":"1650433920.0","comment_id":"588455","content":"Selected Answer: B\nData flow is Air flow open source project. Is an ETL tool. Exactly for copying source data to destination; online or batch.","poster":"Nirca","upvote_count":"1"},{"comment_id":"588454","content":"Selected Answer: B\nData flow is Air flow open source project. Is an ETL tool. Exactly for copying source data to destination; online or batch.","timestamp":"1650433860.0","upvote_count":"1","poster":"Nirca"},{"comment_id":"532189","content":"Selected Answer: B\nGoogle wants you to select \"B\" ---- +10000","poster":"AWS56","timestamp":"1643121000.0","upvote_count":"3"},{"comment_id":"518755","content":"The questions is asking the solution for analysing and not for processing. DataFlow is to process batch and stream data but analysing for batch and stream is done with BigQuery. I would go with D","timestamp":"1641528420.0","upvote_count":"3","poster":"anjuagrawal"},{"comment_id":"508415","content":"B is the correct answer","timestamp":"1640339340.0","poster":"vincy2202","upvote_count":"3"},{"content":"Reading the key words it is undoubted choice to mark Ans as B. But here they ased about 'analyze' and not 'process'. From that point D is answer but not sure how a compute engine will work. Also, may be the question setter has not differentiated between analyze and compute. I would say, an improper and not a serious question","timestamp":"1640104440.0","poster":"cloud_enthusiast_in","comment_id":"506271","upvote_count":"1"},{"timestamp":"1638548700.0","comment_id":"493247","poster":"haroldbenites","content":"Go for B","upvote_count":"2"},{"content":"Selected Answer: B\nDataflow for Batch and Streaming Data Processing","upvote_count":"1","comment_id":"482127","poster":"joe2211","timestamp":"1637359980.0"},{"comment_id":"430638","timestamp":"1629796620.0","upvote_count":"3","content":"I will go against the flow....\nThe question clearly states they want to analyze their data stream to optimize operations that don't have any code. \nLet's go with option elimination\nA. Google Cloud Dataproc\n>> Data proc is a service to run big data analytics like sparks or presto, but is not ready to use like big query.\nB. Google Cloud Dataflow\n>> With data flow you will be able to process streams of data. The requirement is to analyze not process.\nC. Google Container Engine with Bigtable\n>> Provides storage and process but clients will have to put their own solution.\nD. Google Compute Engine with Google BigQuery\n>> BigQuery will provide analytics options. Not sure what compute engine will help.","poster":"amxexam"},{"upvote_count":"1","poster":"Papafel","timestamp":"1625026740.0","comment_id":"394425","content":"B will be the correct answer: Dataflow is for processing both the Batch and Stream. Cloud Dataflow is a fully-managed service for transforming and enriching data in stream (real time) and batch (historical) modes with equal reliability and expressiveness -- no more complex workarounds or compromises needed.\nReferences: https://cloud.google.com/dataflow/"},{"timestamp":"1624694580.0","comment_id":"391023","upvote_count":"2","poster":"aviratna","content":"B. Dataflow is right option as user doesnt have their own code. Other options are applicable if user has own code which they wants to use for analysis."},{"poster":"victory108","timestamp":"1621320480.0","upvote_count":"1","comment_id":"360169","content":"B. Google Cloud Dataflow"},{"upvote_count":"1","timestamp":"1620661560.0","poster":"un","comment_id":"353878","content":"B is correct answer"},{"upvote_count":"1","timestamp":"1620647160.0","comment_id":"353645","poster":"un","content":"B is correct"},{"content":"Go with the FLOW - DataFlow","upvote_count":"3","timestamp":"1618903140.0","poster":"ronVenom","comment_id":"339384"},{"timestamp":"1618903080.0","content":"Go with the FLOW - DataFlow","upvote_count":"1","poster":"ronVenom","comment_id":"339383"},{"upvote_count":"3","timestamp":"1618225020.0","poster":"JohnWick2020","content":"My Answer - B.\n\nKeynotes from question:\n1 - Want to analyze data stream to optimize operations.\n2 - No existing code for analysis and want to explore (greenfield).\n3 - Options - mix of batch and stream processing, hourly jobs, live processing.\n\nExplanation:\nA - Ok but DataProc is more hands-on/ devops-focused. Some config overhead setting up Apache/Haddop clusters.\nB - Correct; DataFlow is more hand-off / serverless and supports a variety of processing patterns. Dev worry less of infra setups and can focus more on their day jobs.\nC&D - incorrect; can rule out quickly these. Not viable options for batch or stream processing.","comment_id":"333873"},{"timestamp":"1617080160.0","content":"Answer is B","poster":"Ausias18","upvote_count":"1","comment_id":"323977"},{"poster":"Joyjit_Deb","upvote_count":"1","content":"\"B\" - Dataflow seems to be the correct approach.","comment_id":"290558","timestamp":"1613344260.0"},{"upvote_count":"1","poster":"bnlcnd","comment_id":"274276","timestamp":"1611375360.0","content":"B:\nhttps://stackoverflow.com/questions/46436794/what-is-the-difference-between-google-cloud-dataflow-and-google-cloud-dataproc\n\n- Cloud Dataproc provides you with a Hadoop cluster, on GCP, and access to Hadoop-ecosystem tools (e.g. Apache Pig, Hive, and Spark); this has strong appeal if you are already familiar with Hadoop tools and have Hadoop jobs\n- Cloud Dataflow provides you with a place to run Apache Beam based jobs, on GCP, and you do not need to address common aspects of running jobs on a cluster (e.g. Balancing work, or Scaling the number of workers for a job; by default, this is automatically managed for you, and applies to both batch and streaming) -- this can be very time consuming on other systems"},{"timestamp":"1609318320.0","content":"Everyone says B. Ok.\nBut what about the requirement \"wants to analyze their data\"?\nDataflow is an ETL for batch and streaming processing, but how to analyse your data? and where do you store data taht you want to analyze?\nSo I choose A, the simplest service to work on batch and streaming processing and for data analysis.","upvote_count":"2","poster":"gianberg","comment_id":"255462"},{"upvote_count":"1","content":"B is correct","comment_id":"190420","timestamp":"1601478120.0","poster":"Kiranazure"},{"timestamp":"1600094640.0","comment_id":"179354","upvote_count":"1","poster":"AshokC","content":"B - Dataflow can process both Batch and Streaming data"},{"poster":"[Removed]","comment_id":"156834","upvote_count":"1","timestamp":"1597278780.0","content":"I'd think B too. But what about the part which say they don't have the code? i assume they have some exe or bin file to run the process? if so wouldn't compute engine be the only option to begin with?"},{"timestamp":"1596985020.0","upvote_count":"2","poster":"Musk","content":"DataFlow takes you the data to the engine that will analyze it, but is not a tool to analyze by itself. I'd go with D.","comments":[{"content":"Agree. I also go with D.","timestamp":"1641527640.0","comment_id":"518752","upvote_count":"2","poster":"anjuagrawal"}],"comment_id":"153752"},{"content":"My answer : B","timestamp":"1594613460.0","upvote_count":"2","poster":"cookiethecat","comment_id":"133613"},{"poster":"mlantonis","timestamp":"1592889180.0","comment_id":"117042","content":"batch and stream processing so you pick Dataflow. B is the correct.","upvote_count":"3"},{"upvote_count":"2","timestamp":"1591908840.0","content":"B for sure","poster":"Pupina","comment_id":"108086"},{"upvote_count":"2","poster":"gfhbox0083","comment_id":"106496","content":"B, for sure.\nGoogle Cloud Dataflow","timestamp":"1591767180.0"},{"upvote_count":"2","poster":"Nirms","comment_id":"100838","timestamp":"1591103820.0","content":"B is the correct answer"},{"content":"B is correct","poster":"Ziegler","comment_id":"98306","upvote_count":"2","timestamp":"1590765060.0"},{"comment_id":"98084","poster":"AD2AD4","content":"Final Decision to go with Option B","timestamp":"1590736680.0","upvote_count":"2"},{"poster":"clouddude","content":"I'll go with B.\nA does not seem reasonable as Dataproc is for Hadoop workloads.\nB seems reasonable because Dataflow can handle the ingestion of both batch and stream flows.\nC does not seem reasonable because it doesn't address the streaming requirement.\nD does not seem reasonable because it doesn't address the streaming requirement.","comment_id":"86281","timestamp":"1589069760.0","upvote_count":"3"},{"comment_id":"74423","timestamp":"1586855400.0","poster":"PRC","upvote_count":"2","content":"Dataflow is the right choice for batch and real time data processing. \"B\""},{"poster":"desertlotus1211","comment_id":"67335","upvote_count":"4","timestamp":"1584987420.0","content":"Answer is B:\nhttps://cloud.google.com/dataflow\nFast, unified stream and batch data processing\nDataflow is a fully managed streaming analytics service that minimizes latency, processing time, and cost through autoscaling and batch processing. With its serverless approach to resource provisioning and management, you have access to virtually limitless capacity to solve your biggest data processing challenges, while paying only for what you use."}],"answer_description":"","question_text":"Your company has successfully migrated to the cloud and wants to analyze their data stream to optimize operations. They do not have any existing code for this analysis, so they are exploring all their options. These options include a mix of batch and stream processing, as they are running some hourly jobs and live- processing some data as it comes in.\nWhich technology should they use for this?","unix_timestamp":1571923020,"topic":"1"},{"id":"WHMFdTJJm687kMlVDTzr","answer_images":[],"timestamp":"2021-06-30 17:57:00","answer":"B","answer_ET":"B","question_text":"Your company has a project in Google Cloud with three Virtual Private Clouds (VPCs). There is a Compute Engine instance on each VPC. Network subnets do not overlap and must remain separated. The network configuration is shown below.\n//IMG//\n\nInstance #1 is an exception and must communicate directly with both Instance #2 and Instance #3 via internal IPs. How should you accomplish this?","discussion":[{"poster":"XDevX","comment_id":"394913","content":"According to my understanding the requirement is that only VM1 shall be able to communicate with VM2 and VM3, but not VM2 with VM3.\nWe can exclude d) as d) would enable VM2 to communicate with VM3 as well - my assumption is, that if the quizzer wanted that d) is the correct answer, he would make just 2 peerings - 1x between VM1 and VM2 and 1x between VM1 and VM3 repectively the VPCs.\nWe can exclude c) as well - there is no connection between VPC1 and VPC3.\nIMHO a) will not work.\nSo the only correct answer seems to be b) - what I don't understand is why we have to update the firewall rules as IMHO the default firewall rules enable such communication (maybe some restrictive rules are implemented - not enough details in the question to clarify that part). Please correct me if I am wrong.","timestamp":"1625068620.0","comments":[{"timestamp":"1626939900.0","comment_id":"411443","content":"I think it is because the instances are in separate VPCs.\n\n\"Google Cloud Virtual Private Cloud (VPC) networks are by default isolated private networking domains. Networks have a global scope and contain regional subnets. VM instances within a VPC network can communicate among themselves using internal IP addresses as long as firewall rules permit. However, NO INTERNAL IP ADDRESS COMMUNICATION IS ALLOWED BETWEEN networks, unless you set up mechanisms such as VPC Network Peering or Cloud VPN.\"\n\nThe instructions for setting up multiple interfaces tells you to check your firewall rules as as the firewall rules of the VPC apply to the network interface that it is attached to.\n\nhttps://cloud.google.com/vpc/docs/multiple-interfaces-concepts#firewall_rules_and_multiple_network_interfaces","poster":"lazybeanbag","upvote_count":"8","comments":[{"comment_id":"649389","timestamp":"1660993200.0","poster":"Ishu_awsguy","upvote_count":"4","comments":[{"timestamp":"1696915020.0","comment_id":"1039170","upvote_count":"1","poster":"b6f53d8","content":"you can not add additional network interface to existing VM's"}],"content":"The answer is \"B\". The following link has this - \"Use multiple network interfaces when an individual instance needs access to more than one VPC network, but you don't want to connect both networks directly.\" https://cloud.google.com/vpc/docs/multiple-interfaces-concepts"}]},{"poster":"JeffClarke111","comment_id":"399797","timestamp":"1625563020.0","upvote_count":"2","content":"Correct, maybe fw on the VM"},{"comments":[{"comment_id":"758449","timestamp":"1672143120.0","upvote_count":"6","poster":"sameer2803","content":"this link says VM can have multiple NICs and attached to different VPCs.\nhttps://cloud.google.com/vpc/docs/create-use-multiple-interfaces\nso B is the answer"}],"content":"B will not work.\nVM instances within a VPC network can communicate among themselves using internal IP addresses as long as firewall rules permit. However, no internal IP address communication is allowed between networks, unless you set up mechanisms such as VPC Network Peering or Cloud VPN.","poster":"Pankaj_007","timestamp":"1668939960.0","upvote_count":"1","comment_id":"722528"}],"upvote_count":"25"},{"content":"Answer is B","upvote_count":"11","poster":"MamthaSJ","comments":[{"content":"Instances are exist. You can not add or remove additional NICs to a VM","upvote_count":"6","comment_id":"599194","poster":"coutcin","timestamp":"1652116440.0"}],"timestamp":"1625716200.0","comment_id":"401495"},{"upvote_count":"2","timestamp":"1735933320.0","poster":"plumbig11","comment_id":"1336139","content":"Selected Answer: B\nAdd two additional NICs to Instance"},{"comment_id":"1255906","upvote_count":"2","poster":"awsgcparch","content":"Selected Answer: B\nDirect Connectivity:\n\nAdding multiple NICs to Instance #1 allows it to be part of multiple VPCs directly. This configuration enables direct communication with Instance #2 and Instance #3 via internal IPs without requiring additional routing configurations.\nSimplicity:\n\nThis approach is straightforward and avoids the complexity of setting up VPC peering or VPN tunnels. It ensures that only Instance #1 has access to both VPC #2 and VPC #3, maintaining the separation of the other VPCs.","timestamp":"1722029280.0"},{"upvote_count":"1","timestamp":"1720899840.0","content":"Selected Answer: D\nVPC peering will allow access to instance 2 & 3 from 1 with internal IP, with necessary firewall rules added.","comment_id":"1247458","poster":"afsarkhan"},{"timestamp":"1713451680.0","poster":"dija123","upvote_count":"2","content":"Selected Answer: B\nB for sure","comment_id":"1197983"},{"comment_id":"1170308","content":"Option B allows you to add additional NICs to Instance #1, each connected to a different VPC, facilitating direct communication between Instance #1 and the other instances while maintaining separate subnets.","upvote_count":"1","poster":"shashii82","timestamp":"1710073020.0"},{"comment_id":"1111809","poster":"kshlgpt","content":"B is wrong. NIC can only be configured while creating the instance. Here the instance is already created. \nC is correct answer. \n\nRefer limitation in this link:\nhttps://cloud.google.com/vpc/docs/create-use-multiple-interfaces","timestamp":"1704194760.0","upvote_count":"2"},{"comment_id":"1026419","content":"Selected Answer: B\nRouter, VPN and VPC Peering for all 3 network is not required.\n\nOnly option B solves the given scenario.","poster":"AdityaGupta","timestamp":"1696587240.0","upvote_count":"2"},{"comment_id":"981447","poster":"rusll","upvote_count":"3","content":"All answers are incorrect: subnets do not overlap and must remain separated. => can't choose A or C or D. \nWhich leaves us with A: you can't attach nics to a compute engine instance after creation : see: https://cloud.google.com/vpc/docs/create-use-multiple-interfaces","timestamp":"1692089700.0"},{"timestamp":"1681562460.0","content":"Is D the correct, peering with adeguate forewall rule for only communication of Instance 1 with Instance 2 and 3","poster":"natpilot","comment_id":"870915","upvote_count":"1"},{"comment_id":"849925","poster":"mifrah","content":"I vote for B:\nVPC peering does not support \"cascading\". Peer VPC 1 with VPC 2, and VPC 2 with VPC 3 does not allow traffic from VPC 1 to VPC 3.","timestamp":"1679732820.0","upvote_count":"1"},{"upvote_count":"1","poster":"razabpn","timestamp":"1676632260.0","content":"Selected Answer: B\nB: NIC usecase when an individual instance needs access to more than one VPC network, but you don't want to connect both networks directly\nhttps://cloud.google.com/vpc/docs/multiple-interfaces-concepts","comment_id":"811792"},{"upvote_count":"2","comment_id":"766282","content":"Selected Answer: B\nB is the correct answer,\n\nConnect the VPC1 instance to VPC2 instance with NIC1 and Connect VPC1 instance to VPC3 instance with NIC2. And update firewall rules to enable traffic between them.\n\nhttps://cloud.google.com/vpc/docs/multiple-interfaces-concepts#firewall_rules_and_multiple_network_interfaces","poster":"examch","timestamp":"1672899120.0"},{"content":"Selected Answer: B\nbest practice is to add NIC to first instance","upvote_count":"1","timestamp":"1671987000.0","comment_id":"755831","poster":"thamaster"},{"timestamp":"1670355660.0","upvote_count":"1","comment_id":"737169","content":"Only solution is peering. N1 peering to n3 and n3 to n1 makes all network peered. So answer should be D","poster":"ANKITMANDLA"},{"upvote_count":"1","content":"B would be incorrect --> As without VPC peering or VPN it will not come into Play.\nD --> This is good as once VPN is established from 1 --> 2 and from 2 --> 3 ... data can flow from 1 to 3 via 2 ...","comment_id":"722523","timestamp":"1668939660.0","comments":[{"content":"I mean C should be correct ..","poster":"Pankaj_007","timestamp":"1668939720.0","upvote_count":"1","comment_id":"722525"}],"poster":"Pankaj_007"},{"upvote_count":"2","comment_id":"717817","content":"Selected Answer: B\nB is ok. C&D are wrong because they connect 1 to 2 and 2 to 3 , not 1 to3. 2 and 3 must be unreachable","poster":"megumin","timestamp":"1668415320.0"},{"upvote_count":"3","comments":[{"comment_id":"770258","upvote_count":"1","timestamp":"1673261580.0","content":"As stated here, You should check firewalls rules because it will be applied to the NIC. \ncheck this https://cloud.google.com/vpc/docs/multiple-interfaces-concepts#firewall_rules_and_multiple_network_interfaces","poster":"n_nana"},{"content":"B will not work. \naccroding to https://cloud.google.com/vpc/docs/create-use-multiple-interfaces , You can only configure a network interface when you create an instance. You cannot add or remove network interfaces from an existing VM.","comment_id":"924143","timestamp":"1686828960.0","upvote_count":"1","poster":"Flight1976"}],"comment_id":"710355","timestamp":"1667462820.0","poster":"vgiuseppe77","content":"Selected Answer: C\n- A seems incomplete and not compliant with \"network subnets must remain separated\" requirement.\n- B makes no sense, because if you add NICs with IPs of other subnet, the firewall rules are not needed. Therefore, typically this type of configuration is considered a safety hole.\n- D is incorrect because peering is not transitive.\n- So, remains only C."},{"upvote_count":"1","content":"Selected Answer: B\nA. Cloud Router will do half the job, it will allow all traffic\nB. Looks like an option - not the best but it answers the question\nC. HA VPN gateway in each VPC network (assuming in the same region only), this will do half the job as it needs a lot more to be configured \nD. Only directly peered networks can communicate. Transitive peering is not supported.","poster":"minmin2020","comment_id":"697362","timestamp":"1666006140.0"},{"upvote_count":"1","comment_id":"672596","content":"B seems to be the best choice","poster":"AzureDP900","timestamp":"1663519980.0"},{"upvote_count":"1","comment_id":"648651","timestamp":"1660863840.0","poster":"cloudinit","content":"Selected Answer: B\nThe answer is B, which alone enables communication from Instance 1 to Instance 2 and 3."},{"timestamp":"1654574760.0","poster":"kiranp29587","content":"Every instance in a VPC network has a default network interface. You can create additional network interfaces attached to your VMs, but each interface must attach to a different VPC network. Multiple network interfaces let you create configurations in which an instance connects directly to several VPC networks. Answer B","upvote_count":"2","comment_id":"612567"},{"upvote_count":"1","comment_id":"612275","content":"Selected Answer: B\nD, I think, is wrong, it connects 1 and 2 then 2 and 3 instead 1 and 3.","poster":"ryzior","timestamp":"1654507200.0"},{"comments":[{"comments":[{"comment_id":"654209","poster":"parmand","content":"Agreed. This is a terrible question. The correct answer is peering 1 with 2 and 1 with 3, but that isn't an option here. I doubt this is a real question on the exam.","upvote_count":"4","timestamp":"1661877060.0"}],"comment_id":"610996","poster":"marleybu","content":"Peering would be good but transitive peering is not support with GCP (https://cloud.google.com/vpc/docs/vpc-peering#restrictions) \nIf we peer 1 with 2 and 2 with 3, VPC 1 can't communicate with VPC 3","upvote_count":"1","timestamp":"1654246380.0"}],"content":"Selected Answer: D\nvpc peering is way to go","comment_id":"603351","upvote_count":"1","poster":"amxexam","timestamp":"1652886540.0"},{"content":"Selected Answer: B\nB is the answer","upvote_count":"1","timestamp":"1650514740.0","comment_id":"589080","poster":"wilwong"},{"upvote_count":"2","poster":"Meyuchito","timestamp":"1649403540.0","content":"Selected Answer: C\nA. Create a cloud router to advertise subnet #2 and subnet #3 to subnet #1: WRONG. It will allow all triffic and they are different VPCs\n\nB. Add two additional NICs to Instance #1 with the following configuration: ג€¢ NIC1 ג—‹ VPC: VPC #2 ג—‹ SUBNETWORK: subnet #2 ג€¢ NIC2 ג—‹ VPC: VPC #3 ג—‹ SUBNETWORK: subnet #3 Update firewall rules to enable traffic between instances. WORNG because this can be done only if there where shared VPCs\n\nC. Create two VPN tunnels via CloudVPN: ג€¢ 1 between VPC #1 and VPC #2. ג€¢ 1 between VPC #2 and VPC #3. Update firewall rules to enable traffic between the instances. CORRECT!!!!\n\nD. Peer all three VPCs: ג€¢ Peer VPC #1 with VPC #2. ג€¢ Peer VPC #2 with VPC #3. Update firewall rules to enable traffic between the instances. WRONG!!!!! Peereing is not transitive so in this scenario INSTANCE#1 can't reach INSTANCE#3","comments":[{"comment_id":"587865","poster":"S_marquez","content":"you do not want Subnet 2 to reach subnet 3, so C is incorrect","upvote_count":"3","timestamp":"1650314220.0"}],"comment_id":"582743"},{"poster":"gcmrjbr","timestamp":"1646486700.0","content":"The problem whit B is that you can only add NICs when you create a VM and the option says ADD...","comment_id":"561436","upvote_count":"2"},{"content":"Selected Answer: B\nThe answer is \"B\". \nThe following link has this - \"Use multiple network interfaces when an individual instance needs access to more than one VPC network, but you don't want to connect both networks directly.\"\nhttps://cloud.google.com/vpc/docs/multiple-interfaces-concepts","upvote_count":"6","comment_id":"524234","poster":"nymets","timestamp":"1642258080.0"},{"comments":[{"poster":"S_marquez","comment_id":"587864","timestamp":"1650314100.0","content":"But you do not want 2, to reach 3","upvote_count":"1"},{"poster":"zanhsieh","comment_id":"635846","timestamp":"1658627700.0","content":"Also for CloudVPN, we have to set up an external ip address for each VPC and the traffic has to flow through there. This violates what question asked \"via internal IPs\".","upvote_count":"1"}],"upvote_count":"1","content":"Why not C? VPNs permit transitive communication, as explained in this table: https://cloud.google.com/architecture/best-practices-vpc-design#choose-method , so AFAIK connecting VPC#1 to VPC#2 and VPC#2 to VPC#3, you should be able to reach #3 from #1. Please correct me if wrong.","timestamp":"1641465180.0","comment_id":"518148","poster":"elenamatay"},{"content":"Selected Answer: D\nD more sense to me, Peering then limit the access via firewall, with B, multi NIC only available when re-create the instances.","comment_id":"515566","timestamp":"1641202740.0","poster":"PhuocT","upvote_count":"2"},{"poster":"SamGCP","content":"C & D don't work since transitive communication is not possible ie VM1 wont be able to communicate with VM3 since there is no direct connection between them","comment_id":"503064","upvote_count":"1","timestamp":"1639672140.0"},{"content":"Selected Answer: D\nVPC networks are isolated by definition. You can not make connection, routed or not, between any two VM which are not in the same VPC. Peering is a requirement, than you can add nics/routes/firewall rules to regulate flows.","timestamp":"1639300740.0","comments":[{"upvote_count":"1","timestamp":"1649075040.0","poster":"lxs","comment_id":"580708","content":"If you were right, shared VPCs could not work. You can attach multiple NIC from different VPCs in case all are in the same region."}],"poster":"mgm7","comment_id":"499903","upvote_count":"1"},{"poster":"vincy2202","timestamp":"1638607980.0","upvote_count":"1","comment_id":"493597","content":"Selected Answer: B\nB seems to be the correct answer"},{"timestamp":"1637847000.0","poster":"joe2211","upvote_count":"3","comment_id":"486690","content":"Selected Answer: B\nvote B"},{"comments":[{"comment_id":"476185","upvote_count":"1","poster":"robotgeek","content":"cloud router is cloud to on-premises, google it pal","timestamp":"1636632420.0"}],"comment_id":"473459","timestamp":"1636203720.0","poster":"StanPeng","content":"Why a is wrong?","upvote_count":"1"},{"comment_id":"448603","content":"Answer should be B. https://cloud.google.com/vpc/docs/create-use-multiple-interfaces#i_am_not_able_to_connect_to_secondary_interfaces_internal_ip","poster":"jask","upvote_count":"3","timestamp":"1632194640.0"},{"upvote_count":"2","comment_id":"440021","content":"VPC peering is the way to go when dealing with multi VPC.\nhttps://cloud.google.com/vpc/docs/vpc-peering\nThe firewall rule will give the permission to communicate.\nSo D.","comments":[{"timestamp":"1631430300.0","poster":"apclb","content":"It's B). Yes Peering VPCs would work, would it not for that answer D says to Peer VPC#2 with VPC#3 which is not the requirement. So Answer D is wrong if you do it in the way the answer is written.","upvote_count":"2","comment_id":"443325"}],"timestamp":"1630888860.0","poster":"amxexam"},{"poster":"JustADudeTakingATest","content":"B seems to be the best choice, but its worded terribly because you CANT add NICs to a created VM, youd have to create a whole new VM","timestamp":"1630519620.0","comment_id":"437393","upvote_count":"2"},{"poster":"PeppaPig","upvote_count":"1","comment_id":"428493","content":"A is definitely wrong, and since VPC does not support transitive peering, you must set up direct peering between VPC1 and VPC3, that rules out C&D.\nIt leaves B the only correct answer","timestamp":"1629520200.0"},{"content":"B is correct.\n\nAs per GCP documentation: \"By default, every instance in a VPC network has a single network interface. Use these instructions to create additional network interfaces. Each interface is attached to a different VPC network, giving that instance access to different VPC networks in Google Cloud. You cannot attach multiple network interfaces to the same VPC network.\"\n\nRefer to: https://cloud.google.com/vpc/docs/create-use-multiple-interfaces\n\nThe VPN and VPC Peering could work for other scenarios but the requirement is to keep the connection only between 1-2, and 1-3, not a transitive connection (1-2-3).","poster":"poseidon24","timestamp":"1627501980.0","upvote_count":"5","comment_id":"416449"},{"upvote_count":"3","timestamp":"1627130340.0","comment_id":"413184","poster":"victory108","content":"B. Add two additional NICs to Instance #1 with the following configuration: ג€¢ NIC1 ג—‹ VPC: VPC #2 ג—‹ SUBNETWORK: subnet #2 ג€¢ NIC2 ג—‹ VPC: VPC #3 ג—‹ SUBNETWORK: subnet #3 Update firewall rules to enable traffic between instances."},{"timestamp":"1625710980.0","comments":[{"content":"so it B","poster":"kopper2019","comment_id":"401457","upvote_count":"2","timestamp":"1625710980.0"},{"timestamp":"1630488180.0","comment_id":"437007","upvote_count":"1","poster":"sandipk91","content":"\" as long as VMs and VPC are in the same region \" - I guess you mean subnets here as VPC is a global service unlike Azure VNET"}],"content":"I just tested I was thinking in Azure mode where a VM cannot have NIC in two vNets but I just tested and google allows that so as long as VMs and VPC are in the same region this works","poster":"kopper2019","upvote_count":"1","comment_id":"401456"},{"upvote_count":"1","comment_id":"401452","poster":"kopper2019","content":"this Q is not correct I mean all this to answer B, well no way or C or D are incorrect and of those 2 is correct","timestamp":"1625710560.0"}],"answer_description":"","topic":"1","isMC":true,"url":"https://www.examtopics.com/discussions/google/view/56416-exam-professional-cloud-architect-topic-1-question-120/","unix_timestamp":1625068620,"answers_community":["B (74%)","13%","13%"],"choices":{"A":"Create a cloud router to advertise subnet #2 and subnet #3 to subnet #1.","B":"Add two additional NICs to Instance #1 with the following configuration: ג€¢ NIC1 ג—‹ VPC: VPC #2 ג—‹ SUBNETWORK: subnet #2 ג€¢ NIC2 ג—‹ VPC: VPC #3 ג—‹ SUBNETWORK: subnet #3 Update firewall rules to enable traffic between instances.","C":"Create two VPN tunnels via CloudVPN: ג€¢ 1 between VPC #1 and VPC #2. ג€¢ 1 between VPC #2 and VPC #3. Update firewall rules to enable traffic between the instances.","D":"Peer all three VPCs: ג€¢ Peer VPC #1 with VPC #2. ג€¢ Peer VPC #2 with VPC #3. Update firewall rules to enable traffic between the instances."},"question_id":25,"exam_id":4,"question_images":["https://www.examtopics.com/assets/media/exam-media/04339/0013300001.jpg"]}],"exam":{"isBeta":false,"numberOfQuestions":279,"isImplemented":true,"lastUpdated":"11 Apr 2025","isMCOnly":false,"provider":"Google","id":4,"name":"Professional Cloud Architect"},"currentPage":5},"__N_SSP":true}