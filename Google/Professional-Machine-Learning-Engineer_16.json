{"pageProps":{"questions":[{"id":"WhBhcrQPeLIuzsVrB5Cq","url":"https://www.examtopics.com/discussions/google/view/130575-exam-professional-machine-learning-engineer-topic-1-question/","question_images":[],"answer_images":[],"answers_community":["B (47%)","C (33%)","A (20%)"],"answer":"B","question_id":76,"timestamp":"2024-01-08 15:31:00","topic":"1","unix_timestamp":1704724260,"exam_id":13,"question_text":"You are working with a dataset that contains customer transactions. You need to build an ML model to predict customer purchase behavior. You plan to develop the model in BigQuery ML, and export it to Cloud Storage for online prediction. You notice that the input data contains a few categorical features, including product category and payment method. You want to deploy the model as quickly as possible. What should you do?","choices":{"C":"Use the CREATE MODEL statement and select the categorical and non-categorical features.","A":"Use the TRANSFORM clause with the ML.ONE_HOT_ENCODER function on the categorical features at model creation and select the categorical and non-categorical features.","D":"Use the ML.MULTI_HOT_ENCODER function on the categorical features, and select the encoded categorical features and non-categorical features as inputs to create your model.","B":"Use the ML.ONE_HOT_ENCODER function on the categorical features and select the encoded categorical features and non-categorical features as inputs to create your model."},"answer_description":"","discussion":[{"content":"Selected Answer: B\nML.ONE_HOT_ENCODER transforms the categorical features into one-hot encoded values. You then select these encoded categorical features along with the non-categorical features to create your model. This is the most common approach for handling categorical features in BigQuery ML for fast deployment.","upvote_count":"1","timestamp":"1738835880.0","comment_id":"1352307","poster":"vini123"},{"comment_id":"1338516","upvote_count":"1","content":"Selected Answer: C\nUsing the CREATE MODEL statement with the categorical and non-categorical features directly (Option C) is the simplest, fastest, and most effective way to build and deploy your model in BigQuery ML","timestamp":"1736457000.0","poster":"potomeek"},{"content":"Selected Answer: C\nThe create_model statement automatically one-hot encodes categorical features. https://cloud.google.com/bigquery/docs/auto-preprocessing\n\nThis may not be the best solution in terms of transparency, but the question asked for the \"fastest\" solution","upvote_count":"1","comment_id":"1333205","timestamp":"1735428840.0","poster":"0e6b9e2"},{"upvote_count":"3","poster":"phani49","comment_id":"1330579","content":"Selected Answer: C\nBigQuery ML automatically handles categorical features. When you use the CREATE MODEL statement, it recognizes categorical columns and applies appropriate encoding (e.g., one-hot encoding or embeddings) under the hood.","timestamp":"1734901560.0"},{"poster":"YushiSato","timestamp":"1723350720.0","content":"Selected Answer: A\nTRANSFORM is used to transform the input for both learning and inference.\nONE_HOT_ENCODER can also be used within TRANSFORM.\nThe other options require conversion on the input in prediction.\nA is correct.","comment_id":"1263746","upvote_count":"1","comments":[{"content":"Sorry, BlehMaks is correct.\nIn this case, we don't use TRANSFORM, we need to do the conversion in the forecast as well.","timestamp":"1723351080.0","upvote_count":"1","poster":"YushiSato","comment_id":"1263748"}]},{"timestamp":"1717693680.0","poster":"bobjr","comment_id":"1225652","content":"Selected Answer: A\nCREATE OR REPLACE MODEL `project.dataset.model_name`\nOPTIONS(model_type='logistic_reg') AS\nSELECT\n *,\n TRANSFORM(\n product_category,\n payment_method\n USING\n ML.ONE_HOT_ENCODER(product_category) AS encoded_product_category,\n ML.ONE_HOT_ENCODER(payment_method) AS encoded_payment_method\n )\nFROM\n `project.dataset.table_name`;","upvote_count":"2"},{"comment_id":"1120838","poster":"BlehMaks","upvote_count":"4","content":"Selected Answer: B\nWhen the TRANSFORM clause is present, only output columns from the TRANSFORM clause are used in training. Any results from query_statement that don't appear in the TRANSFORM clause are ignored. https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create#transform\nso if you want TRANSFORM then use TRANSFORM for both categorical and non-categorical features","timestamp":"1705069800.0"},{"poster":"pikachu007","upvote_count":"1","comment_id":"1118348","content":"Selected Answer: B\nGiven the goal of quickly deploying the model for predicting customer purchase behavior while handling categorical features, option B - \"Use the ML.ONE_HOT_ENCODER function on the categorical features and select the encoded categorical features and non-categorical features as inputs to create your model\" seems to be the most appropriate. This approach directly handles the encoding of categorical features using one-hot encoding and selects the necessary features for model creation, ensuring efficient utilization of categorical data in the BigQuery ML model.","timestamp":"1704874020.0"},{"timestamp":"1704724260.0","comment_id":"1116677","upvote_count":"1","poster":"b1a8fae","comments":[{"comment_id":"1116679","upvote_count":"1","poster":"b1a8fae","content":"Also I understand it cannot be A because it says \"take the categorical features\" as opposed to the more specific \"take the encoded categorical features\" in B","timestamp":"1704724380.0"}],"content":"Selected Answer: B\nOnly B and D make sense. Between the two, after reading the use case of multi-hot encoding (https://cloud.google.com/bigquery/docs/auto-preprocessing#feature-transform), I would tend towards B, since one-hot encoding is preferred over in case of using non-numerical, non-array features (product category and payment methods are often respresented as such); multi-hot encoding is preferred in case of non-numerical, array features, which is not the case here."}],"isMC":true,"answer_ET":"B"},{"id":"Zi94l0npc2F9TO9oATJb","answers_community":["C (60%)","B (33%)","7%"],"answer_ET":"C","choices":{"A":"Use Vertex AI Pipelines with the Kubeflow Pipelines SDK to create a pipeline that reads the images from Cloud Storage and trains the model.","B":"Use Vertex AI Pipelines with TensorFlow Extended (TFX) to create a pipeline that reads the images from Cloud Storage and trains the model.","D":"Convert the image dataset to a tabular format using Dataflow Load the data into BigQuery and use BigQuery ML to train the model.","C":"Import the labeled images as a managed dataset in Vertex AI and use AutoML to train the model."},"topic":"1","answer":"C","timestamp":"2024-01-08 15:59:00","question_images":[],"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/130580-exam-professional-machine-learning-engineer-topic-1-question/","question_text":"You need to develop an image classification model by using a large dataset that contains labeled images in a Cloud Storage bucket. What should you do?","unix_timestamp":1704725940,"isMC":true,"discussion":[{"poster":"NamitSehgal","content":"Selected Answer: C\nleverage Vertex AI's AutoML capabilities to automatically build a high-quality image classification model","upvote_count":"1","comment_id":"1358095","timestamp":"1739846820.0"},{"upvote_count":"1","comment_id":"1339680","content":"Selected Answer: B\nManaged dataset has a size limitation of 100GB . Question states \" a large dataset \" . Unmanged dataset has not size limitation . Assuming large here implies > 100GB , it should eliminate answer C","poster":"sekhrivijay","timestamp":"1736717880.0"},{"upvote_count":"2","timestamp":"1731559380.0","content":"Selected Answer: C\nYou're just trying to TRAIN A MODEL, not set up a whole pipeline. Answer is clearly C","poster":"f084277","comment_id":"1311731"},{"comment_id":"1234857","poster":"AzureDP900","upvote_count":"1","content":"B is right in my opinion, while both options C and B involve importing labeled images into Vertex AI, using AutoML for image classification might not be the most suitable choice. TFX is a more specialized tool that provides a robust pipeline framework specifically designed for image classification tasks, making it a better fit for this particular use case.","timestamp":"1718998980.0"},{"comments":[{"poster":"pinimichele01","timestamp":"1714128120.0","comment_id":"1202540","content":"no need to use a pipeline, automl is ok","upvote_count":"1"}],"upvote_count":"1","timestamp":"1712491020.0","poster":"pinimichele01","comment_id":"1190922","content":"Selected Answer: C\nhttps://cloud.google.com/vertex-ai/docs/tutorials/image-classification-automl/dataset"},{"upvote_count":"4","timestamp":"1707255240.0","comment_id":"1142621","content":"Selected Answer: B\nMy answer: B \n\nTensorFlow Extended (TFX) and Kubeflow provide capabilities for building machine learning pipelines that can handle data stored in Google Cloud Storage (GCS). However, when it comes to ease of use specifically for working with data in GCS, TFX may have a slight edge over Kubeflow for \n1- Integration with GCS- TensorFlow: TFX is tightly integrated with TensorFlow that has built-in support for GCS and provides convenient APIs for reading data directly from GCS buckets\n2 - Abstraction of Data Handling TFX provides higher-level abstractions and components specifically designed for common machine learning tasks, including data preprocessing, model training, and model evaluation","comments":[{"upvote_count":"2","timestamp":"1714148340.0","poster":"pinimichele01","content":"Which SDK use?\n• If you use TensorFlow in an ML workflow that processes terabytes of structured data or text data -> TFX\n• For other use-cases -> KFP","comment_id":"1202694"}],"poster":"guilhermebutzke"},{"timestamp":"1705131480.0","poster":"winston9","content":"Selected Answer: C\nIt's C","upvote_count":"3","comment_id":"1121382"},{"comment_id":"1120869","upvote_count":"1","timestamp":"1705072080.0","comments":[{"upvote_count":"1","content":"95 is a similar question but it does not offer Vertex AI AutoML as an option. which I think it's the right answer here consider the little amount of info provided in the question","poster":"winston9","comment_id":"1125081","timestamp":"1705504380.0"}],"poster":"BlehMaks","content":"Selected Answer: A\n95th is the similar question. https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline#sdk"},{"poster":"b1a8fae","content":"Selected Answer: C\nVery vaguely put. I choose C over B just because it sounds like a simpler approach, but both should theoretically work.","comment_id":"1116701","upvote_count":"2","timestamp":"1704725940.0"}],"exam_id":13,"answer_images":[],"question_id":77},{"id":"e0ii9sdIXVswM6Fpkrwm","answer_ET":"B","timestamp":"2024-01-08 16:13:00","question_id":78,"isMC":true,"answer":"B","exam_id":13,"answer_images":[],"question_images":[],"topic":"1","question_text":"You are developing a model to detect fraudulent credit card transactions. You need to prioritize detection, because missing even one fraudulent transaction could severely impact the credit card holder. You used AutoML to tram a model on users' profile information and credit card transaction data After training the initial model, you notice that the model is failing to detect many fraudulent transactions. How should you adjust the training parameters in AutoML to improve model performance? (Choose two.)","url":"https://www.examtopics.com/discussions/google/view/130583-exam-professional-machine-learning-engineer-topic-1-question/","choices":{"A":"Increase the score threshold","D":"Add more negative examples to the training set","B":"Decrease the score threshold.","C":"Add more positive examples to the training set","E":"Reduce the maximum number of node hours for training"},"discussion":[{"comment_id":"1358096","upvote_count":"1","timestamp":"1739847060.0","content":"Selected Answer: B\nB. Decrease the score threshold and C. Add more positive examples to the training set.","poster":"NamitSehgal"},{"content":"Selected Answer: B\nB&C\n\nIf we want to increase the detection rate of fraudulent transactions, we can lower the classification threshold. By doing so, the model becomes less strict and classifies more transactions as potentially fraudulent. This implies including a higher number of false positives in our results.\n\nTo improve the performance, we can also add more fradulent transactions examples to the dataset (fraudulent transactions are the positivies, in this case)","comment_id":"1265100","poster":"tardigradum","upvote_count":"3","timestamp":"1723544220.0"},{"upvote_count":"2","timestamp":"1713694560.0","poster":"fitri001","comment_id":"1199592","comments":[{"comment_id":"1265097","content":"Positive is fraudulent in this case, so B & C","timestamp":"1723543920.0","poster":"tardigradum","upvote_count":"2"},{"upvote_count":"3","content":"positive is fraudulent.. aka minority class","poster":"pinimichele01","timestamp":"1713876120.0","comment_id":"1200734"},{"poster":"fitri001","timestamp":"1713694680.0","upvote_count":"1","content":"A. Increase the score threshold: This would make the model more conservative and less likely to flag fraudulent transactions, potentially missing actual fraud.\nC. Add more positive examples (legitimate transactions): While having a balanced dataset is important, in this case, prioritizing fraud detection suggests focusing on improving the model's ability to identify fraudulent transactions (negative examples) rather than adding more legitimate ones.\nE. Reduce the maximum number of node hours for training: Reducing training time might limit the model's ability to learn complex patterns, potentially hindering its performance.","comment_id":"1199594"}],"content":"Selected Answer: B\nB & D\nD. Add more negative examples to the training set: Fraudulent transactions are typically a minority compared to legitimate transactions. By increasing the number of negative examples (fraudulent transactions) in your training data, you provide AutoML with more information about the patterns of fraudulent activity. This can help the model better distinguish between legitimate and fraudulent transactions.\nB. Decrease the score threshold: The score threshold determines the level of suspicion assigned to a transaction by the model. A lower threshold means the model flags more transactions as suspicious, potentially catching more fraudulent activities. However, this might also lead to an increase in false positives (flagging legitimate transactions). You'll need to find a balance between fraud detection and acceptable false positive rates based on your business needs."},{"poster":"shadz10","content":"B&C - Fraudulent transactions are often rare events, so the model might not have enough exposure to learn their patterns effectively.","upvote_count":"2","comment_id":"1122826","timestamp":"1705265280.0"},{"poster":"36bdc1e","timestamp":"1705160880.0","upvote_count":"2","content":"B & C\nThey are the options","comment_id":"1121817"},{"comment_id":"1120879","upvote_count":"3","timestamp":"1705072980.0","poster":"BlehMaks","content":"Selected Answer: C\nBC\nB. More suspicious transactions are marked as fraudulent\nC. Usually real fraudulent transactions are rare in datasets so we need to add more examples to make our model focus more on them"},{"upvote_count":"2","poster":"pikachu007","timestamp":"1704874620.0","content":"Selected Answer: B\nB & D\n\nB. Decrease the score threshold: This adjustment could make the model more sensitive, potentially reducing the chance of missing fraudulent transactions, but might increase false positives.\n\nD. Add more negative examples to the training set: Providing more examples of non-fraudulent transactions could help the model better distinguish between legitimate and fraudulent transactions, improving its overall performance.","comments":[{"comment_id":"1203447","poster":"tavva_prudhvi","upvote_count":"1","timestamp":"1714290360.0","content":"Option D's approach could be beneficial in a scenario where the model is overfitting to the fraudulent (positive) cases due to an imbalance in the training data favoring fraudulent examples. But, as per the question \"model is failing to detect many fraudulent transactions\""}],"comment_id":"1118361"},{"upvote_count":"2","comment_id":"1116726","content":"Selected Answer: C\nRegarding the 2nd choice (did not notice), I would choose C: adding more positive examples to the training set. It did not sound like a change of parameter to me, but apparently AutoML allows parametrization of data split: https://cloud.google.com/vertex-ai/docs/general/ml-use. I am not entirely convinced but it seems more likely than any other option (reducing max number of hours per node for training can only affect performance negatively I reckon?)","poster":"b1a8fae","timestamp":"1704727380.0"},{"poster":"b1a8fae","timestamp":"1704726780.0","comment_id":"1116713","content":"Selected Answer: B\nB. Decreasing the score threshold will cause the model to make more positive predictions and potentially decrease the number of false negatives (non detected fraudulent transactions)","upvote_count":"2"}],"unix_timestamp":1704726780,"answers_community":["B (67%)","C (33%)"],"answer_description":""},{"id":"kc4sYWcv5pi3ZKGljwR9","url":"https://www.examtopics.com/discussions/google/view/55437-exam-professional-machine-learning-engineer-topic-1-question/","question_images":[],"answer_images":[],"answers_community":["D (63%)","B (33%)","4%"],"answer":"D","question_id":79,"timestamp":"2021-06-16 13:58:00","topic":"1","unix_timestamp":1623844680,"exam_id":13,"question_text":"You are building a real-time prediction engine that streams files which may contain Personally Identifiable Information (PII) to Google Cloud. You want to use the\nCloud Data Loss Prevention (DLP) API to scan the files. How should you ensure that the PII is not accessible by unauthorized individuals?","choices":{"D":"Create three buckets of data: Quarantine, Sensitive, and Non-sensitive. Write all data to the Quarantine bucket. Periodically conduct a bulk scan of that bucket using the DLP API, and move the data to either the Sensitive or Non-Sensitive bucket.","B":"Stream all files to Google Cloud, and write batches of the data to BigQuery. While the data is being written to BigQuery, conduct a bulk scan of the data using the DLP API.","A":"Stream all files to Google Cloud, and then write the data to BigQuery. Periodically conduct a bulk scan of the table using the DLP API.","C":"Create two buckets of data: Sensitive and Non-sensitive. Write all data to the Non-sensitive bucket. Periodically conduct a bulk scan of that bucket using the DLP API, and move the sensitive data to the Sensitive bucket."},"answer_description":"","discussion":[{"timestamp":"1623844680.0","comment_id":"383351","comments":[{"comment_id":"437404","content":"All PII should be Sensitive data, that's why I think the answer is A.","poster":"Swagluke","timestamp":"1630520820.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"631155","poster":"u_phoria","timestamp":"1657772220.0","content":"Option D, as documented in that link (a fully automated process, using Cloud Functions - rather than a \"periodic\" scan as worded in the question), would be my choice.\n\nIt's easier than B, which would work for a real-time scenario - but would require loads more custom work to implement (things like batching, segmentation, triggering).\n\nA and C are 'reactive' / periodic, and so not appropriate for the given scenario."}],"upvote_count":"26","content":"Should be D\nhttps://cloud.google.com/architecture/automating-classification-of-data-uploaded-to-cloud-storage#building_the_quarantine_and_classification_pipeline","poster":"chohan"},{"poster":"maartenalexander","upvote_count":"5","timestamp":"1624348500.0","comment_id":"387708","content":"D; others pose risks"},{"timestamp":"1733629620.0","poster":"joqu","content":"Selected Answer: B\nTask: \"You are building a real-time prediction engine\".\nSolution D: \" Periodically conduct a bulk scan\" is NOT REAL TIME. It is the recommended architecture, but in order to satisfy the task, the cloud function would need to be triggered by each new data landing in GCP - which is totally doable and would be the right solution, but this is NOT WHAT THE ANSWER IS SAYING. Therefore B is the next best option (although not the recommended architecture design)","upvote_count":"1","comment_id":"1323347"},{"upvote_count":"1","poster":"Choisus","timestamp":"1729714440.0","content":"Selected Answer: B\nwhy not b, it requires real-time right?","comment_id":"1302199"},{"upvote_count":"1","poster":"PhilipKoku","comment_id":"1225174","content":"Selected Answer: D\nD) The best choice for this scenario would be D. Create three buckets of data: Quarantine, Sensitive, and Non-sensitive. Write all data to the Quarantine bucket. Periodically conduct a bulk scan of that bucket using the DLP API, and move the data to either the Sensitive or Non-Sensitive bucket.","timestamp":"1717654260.0"},{"upvote_count":"2","comment_id":"1087445","content":"Selected Answer: D\nD - Quarantine bucket is the google reccomended approach","timestamp":"1701679680.0","poster":"fragkris"},{"content":"Selected Answer: D\nOption B does not provide a clear separation between sensitive and non-sensitive data before it is written to BigQuery, which means that PII might be exposed during the process. \n\nBut, in D offers a better level of security by writing all the data to a Quarantine bucket first. This way, the DLP API can scan and categorize the data into Sensitive or Non-sensitive buckets before it is further processed or stored. This ensures that PII is not accessible by unauthorized individuals, as the sensitive data is identified and separated from the non-sensitive data before any further actions are taken.","upvote_count":"1","poster":"tavva_prudhvi","timestamp":"1699041060.0","comment_id":"1061714"},{"upvote_count":"1","poster":"harithacML","content":"Selected Answer: D\nreal-time prediction engine, that streams files to Google Cloud. PII is not accessible by unauthorized individuals.\nD","comment_id":"947111","timestamp":"1688898660.0"},{"comment_id":"945696","upvote_count":"1","poster":"Liting","content":"Selected Answer: D\nD should be the correct answer","timestamp":"1688735160.0"},{"poster":"M25","upvote_count":"2","timestamp":"1683608280.0","content":"Selected Answer: D\nWent with D","comment_id":"892693"},{"timestamp":"1681990320.0","comment_id":"875524","poster":"lucaluca1982","content":"Selected Answer: B\nB is real time","upvote_count":"1"},{"content":"Selected Answer: D\nIt's D","poster":"dfdrin","upvote_count":"1","comment_id":"857692","timestamp":"1680334800.0"},{"comment_id":"800330","poster":"enghabeth","upvote_count":"3","comments":[{"comment_id":"832045","content":"Never mentioned periodically in the question, if I'm not wrong?","upvote_count":"1","timestamp":"1678201800.0","poster":"tavva_prudhvi"}],"content":"Selected Answer: B\nA, D, C they do not apply to a realtime case, all three say that the scan is applied periodically\nThen it's B","timestamp":"1675721280.0"},{"timestamp":"1673465760.0","comment_id":"772843","poster":"guilhermebutzke","upvote_count":"1","content":"Selected Answer: B\nI think that is the correct because of the \"real time\" application."},{"comment_id":"725657","poster":"EFIGO","timestamp":"1669278000.0","upvote_count":"1","content":"Selected Answer: D\nD is the right answer: you can temporarily store the sensitive data in a Quarantine bucket with restricted access, then move the data to the relative buckets once the PII have been protected."},{"comment_id":"647192","upvote_count":"1","content":"Selected Answer: D\nCorrect answer is \"D\"","poster":"GCP72","timestamp":"1660567620.0"},{"timestamp":"1653375600.0","comment_id":"606523","poster":"dasouna","upvote_count":"1","content":"Answer is D : Question says that there MAY be sensitive data, so not all data is sensitive. This is why we need 3 buckets : Quarantine as a landing bucket, sensitive for sensitive data after DLP scan, non-sensitive for non-sensitive after DLP scan.\nhttps://cloud.google.com/architecture/automating-classification-of-data-uploaded-to-cloud-storage"},{"timestamp":"1648638060.0","poster":"atuls287","comment_id":"578194","upvote_count":"2","content":"Selected Answer: B\nReason being \"Real Time' DLP scanning. Option A would scan all the data again and again. For others - Buckets etc is overkill and offline process."},{"comment_id":"573733","poster":"lukacs16","timestamp":"1648049640.0","content":"But what about the real-time element, how would that work with the quarantine?","upvote_count":"1"},{"comments":[{"upvote_count":"1","content":"Why not B then? Ans A says \"periodically\". Shouldn't you scan as the data comes in, for real-time?","timestamp":"1648137960.0","poster":"giaZ","comment_id":"574511"}],"poster":"pml2021","content":"Selected Answer: A\nAltough both A and D are correct when scanning for the data using DLP, however here the question is streaming data and the best option in this particular case would be A. \nCheck this use case Using Cloud DLP with BigQuery\nhttps://cloud.google.com/dlp/docs/dlp-bigquery\nAlso the other use case involving the DLP using Quarantine bucket is by uploading the files and not streaming. https://cloud.google.com/architecture/automating-classification-of-data-uploaded-to-cloud-storage#building_the_quarantine_and_classification_pipeline","comment_id":"568721","upvote_count":"1","timestamp":"1647392400.0"},{"poster":"amanshin","timestamp":"1644831120.0","comment_id":"547013","content":"D without thinking twice. Google practice is to put the data in quarantine first.","upvote_count":"2"},{"content":"B as writing to google cloud (streaming) to bigquery, scan along the way.","poster":"NamitSehgal","comment_id":"516215","upvote_count":"1","timestamp":"1641267900.0"},{"comment_id":"503084","poster":"entrpn","timestamp":"1639675140.0","comments":[{"upvote_count":"1","content":"I meant B, not A.","timestamp":"1639675200.0","poster":"entrpn","comment_id":"503085"}],"content":"I think is A because in a real world scenario, sensitive data should be in a completely isolated environment. A real life pattern is to have a regulated environment push to a non-regulated and apply DLP at that instance so that someone can't accidentally misconfigure a bucket and leak the sensitive data.","upvote_count":"1"},{"timestamp":"1638877440.0","comment_id":"495929","poster":"alphard","upvote_count":"1","content":"My answer is A. \n\nIt is a DLP use case in BigQuery."},{"content":"Selected Answer: D\nShould be D","timestamp":"1638138180.0","upvote_count":"3","poster":"Sneg42","comment_id":"489444"},{"poster":"santy79","content":"Selected Answer: D\nShould be D","timestamp":"1637417100.0","upvote_count":"3","comment_id":"482588"},{"comment_id":"464290","upvote_count":"4","timestamp":"1634586780.0","poster":"mousseUwU","content":"I would say D because if the data is considered non sensitive there is no reason to recheck if it's sensitive so you move to a non sensitive bucket. Data that is sensitive will be directed to the Sensitive bucket that has limited access just like the Quarantine bucket that should have limited access policy."},{"content":"Should be D.","poster":"Y2Data","timestamp":"1631585160.0","comment_id":"444282","upvote_count":"3","comments":[{"poster":"mouhannad","upvote_count":"1","comment_id":"452006","content":"why not C?","timestamp":"1632687240.0","comments":[{"poster":"george_ognyanov","upvote_count":"1","timestamp":"1633433400.0","comment_id":"457657","content":"I think since you write all the data to non-sensitive bucket this implies anyone can have access to it. The periodical part means after certain time you do the DLP check, meaning in between sensitive data wrongly being written to non-sensitive bucket and running the DLP check there is a security vulnerability. I think D is correct if we presume the quarantine bucket is restricted from the get go"}]}]},{"upvote_count":"4","poster":"Danny2021","timestamp":"1631133840.0","content":"You need to ensure that the PII is not accessible by unauthorized individuals. Only D allows to control the access to PII.","comment_id":"441639"}],"isMC":true,"answer_ET":"D"},{"id":"nQ4dDn6PSb0PZMrb5nv6","answer":"B","topic":"1","url":"https://www.examtopics.com/discussions/google/view/130587-exam-professional-machine-learning-engineer-topic-1-question/","question_text":"You need to deploy a scikit-leam classification model to production. The model must be able to serve requests 24/7, and you expect millions of requests per second to the production application from 8 am to 7 pm. You need to minimize the cost of deployment. What should you do?","exam_id":13,"choices":{"D":"Deploy an online Vertex AI prediction endpoint with one GPU per replica. Set the max replica count to 100","A":"Deploy an online Vertex AI prediction endpoint. Set the max replica count to 1","B":"Deploy an online Vertex AI prediction endpoint. Set the max replica count to 100","C":"Deploy an online Vertex AI prediction endpoint with one GPU per replica. Set the max replica count to 1"},"answer_ET":"B","answers_community":["B (100%)"],"discussion":[{"upvote_count":"5","comment_id":"1118363","timestamp":"1704874680.0","poster":"pikachu007","content":"Selected Answer: B\nB. Deploy an online Vertex AI prediction endpoint. Set the max replica count to 100:\nThis option provides a higher number of replicas (100) to handle the expected high volume of requests during peak hours. While it might result in increased costs, it provides the necessary scalability to manage the incoming traffic efficiently. During non-peak hours, you can consider scaling down the replicas to reduce costs, as Vertex AI allows dynamic scaling based on demand."},{"upvote_count":"1","timestamp":"1718999880.0","poster":"AzureDP900","content":"Option A (Deploying an online Vertex AI prediction endpoint. Set the max replica count to 1) is still a good choice for minimizing costs. By setting the max replica count to 1, you are allowing Vertex AI to scale up or down based on load, which means that during off-peak hours, you won't be paying for unnecessary instances.","comment_id":"1234881"},{"upvote_count":"1","comment_id":"1194885","timestamp":"1713004860.0","content":"Selected Answer: B\nsee pikachu007","poster":"pinimichele01"},{"content":"B\nwe don't need GPU for scikit-learn","comment_id":"1121818","upvote_count":"2","poster":"36bdc1e","timestamp":"1705160940.0"},{"comment_id":"1120888","content":"Selected Answer: B\nscikit-learn doesn't support GPU\nhttps://scikit-learn.org/stable/faq.html#will-you-add-gpu-support","timestamp":"1705073760.0","upvote_count":"2","poster":"BlehMaks"},{"content":"B.\nscikit-learn -> no need for GPU\nmax number of replicas -> 1 is too little if we are serving online predictions at such a massive scale (millions per second)","poster":"b1a8fae","comment_id":"1116731","upvote_count":"1","timestamp":"1704727980.0"}],"unix_timestamp":1704727980,"question_id":80,"timestamp":"2024-01-08 16:33:00","isMC":true,"answer_images":[],"question_images":[],"answer_description":""}],"exam":{"isBeta":false,"provider":"Google","isMCOnly":true,"isImplemented":true,"numberOfQuestions":304,"name":"Professional Machine Learning Engineer","lastUpdated":"11 Apr 2025","id":13},"currentPage":16},"__N_SSP":true}