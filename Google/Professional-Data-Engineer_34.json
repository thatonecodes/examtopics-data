{"pageProps":{"questions":[{"id":"prnIYspx9outIjcpTnRU","topic":"1","question_images":["https://img.examtopics.com/professional-data-engineer/image8.png"],"answer_description":"","discussion":[{"content":"Selected Answer: A\n- The table structure shows that the vCPU data is stored in a nested field within the components column. \n- Using the UNNEST operator to flatten the nested field and apply the filter.","timestamp":"1720128300.0","comment_id":"1114122","poster":"raaad","upvote_count":"6"},{"timestamp":"1726856580.0","content":"Selected Answer: A\noption A","poster":"hanoverquay","upvote_count":"1","comment_id":"1178673"},{"poster":"JyoGCP","comment_id":"1154470","timestamp":"1724118900.0","upvote_count":"1","content":"Selected Answer: A\nOption A - UNNEST"},{"upvote_count":"4","content":"Selected Answer: A\nA seems to be the correct answer because of the table structure and the UNNEST operator.\nHowever, i don’t understand why wouldn’t we chose a materialized view","timestamp":"1720984380.0","comment_id":"1122856","poster":"Krauser59"},{"upvote_count":"4","timestamp":"1720871160.0","poster":"Matt_108","content":"Selected Answer: A\nOption A - The regular reporting doesn't justify a materialized view, since the frequency of access is not so high; a simple view would do the trick. Moreover, the vcpu data is in a nested field and requires Unnest.","comment_id":"1121695"},{"timestamp":"1720006740.0","poster":"scaenruy","comment_id":"1112792","content":"Selected Answer: A\nA. Create a view with a filter to drop rows with fewer than 8 vCPU, and use the UNNEST operator.","upvote_count":"2"}],"url":"https://www.examtopics.com/discussions/google/view/130191-exam-professional-data-engineer-topic-1-question-248/","answers_community":["A (100%)"],"timestamp":"2024-01-03 14:39:00","answer_ET":"A","answer":"A","choices":{"C":"Create a view with a filter to drop rows with fewer than 8 vCPU, and use the WITH common table expression.","A":"Create a view with a filter to drop rows with fewer than 8 vCPU, and use the UNNEST operator.","D":"Use Dataflow to batch process and write the result to another BigQuery table.","B":"Create a materialized view with a filter to drop rows with fewer than 8 vCPU, and use the WITH common table expression."},"unix_timestamp":1704289140,"question_id":166,"answer_images":[],"isMC":true,"exam_id":11,"question_text":"dataset.inventory_vm sample records:\n\n//IMG//\n\n\nYou have an inventory of VM data stored in the BigQuery table. You want to prepare the data for regular reporting in the most cost-effective way. You need to exclude VM rows with fewer than 8 vCPU in your report. What should you do?"},{"id":"gGCg6LmqMC6QfMG78u1j","topic":"1","question_images":[],"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/130353-exam-professional-data-engineer-topic-1-question-249/","discussion":[{"comment_id":"1115763","upvote_count":"10","poster":"Smakyel79","content":"Selected Answer: A\nhttps://cloud.google.com/storage/docs/autoclass","timestamp":"1704627000.0"},{"timestamp":"1704411360.0","poster":"raaad","upvote_count":"8","comment_id":"1114126","content":"Selected Answer: A\n- Autoclass automatically moves objects between storage classes without impacting performance or availability, nor incurring retrieval costs. \n- It continuously optimizes storage costs based on access patterns without the need to set specific lifecycle management policies."},{"poster":"SamuelTsch","timestamp":"1730314200.0","content":"Selected Answer: A\nFrom the documentation https://cloud.google.com/storage/docs/autoclass","comment_id":"1305114","upvote_count":"1"},{"comment_id":"1248391","poster":"hussain.sain","upvote_count":"1","timestamp":"1721051100.0","content":"Selected Answer: B\nthe question clearly specifies there should not be any retrieval charges. so enabling autoclass is not recommended because we have to pay one time fees while retrieving the data. and usually soft delete is enable.","comments":[{"upvote_count":"2","content":"A one-time pay isn't considered a retrieval charge. A is correct","poster":"nadavw","comment_id":"1272592","timestamp":"1724663580.0"}]},{"upvote_count":"3","content":"Selected Answer: B\nWhy B is the best choice:\n\nCost Optimization: This option leverages Cloud Storage's different storage classes to significantly reduce costs for storing older data. Nearline, coldline, and archive storage classes are progressively cheaper than the standard storage class, with trade-offs in availability and retrieval times.\nMeets Requirements:\nOld data deletion: You can manually delete old data whenever needed, fulfilling the first requirement.\nNo predefined access pattern: The policy automatically transitions data to cheaper storage classes based on age, regardless of access patterns.\nInstant availability: Nearline storage provides immediate access to data, meeting the third requirement.\nNo retrieval charges: While there are retrieval charges for coldline and archive storage, nearline storage has no retrieval fees, satisfying the fourth requirement.","comment_id":"1191275","timestamp":"1712535720.0","poster":"CGS22"},{"content":"Selected Answer: A\nFor sure A, read the documentation","upvote_count":"4","comment_id":"1117331","timestamp":"1704791940.0","poster":"Sofiia98"},{"upvote_count":"2","content":"Selected Answer: A\nautoclass is the correct way to handle all business cases","comment_id":"1117123","timestamp":"1704758580.0","poster":"GCP001"},{"timestamp":"1704542520.0","upvote_count":"3","comment_id":"1115155","content":"Selected Answer: B\nCreate an Object Lifecycle Management policy to modify the storage class for data older than 30 days to nearline, 90 days to coldline, and 365 days to archive storage class. Delete old data as needed.","poster":"therealsohail"}],"timestamp":"2024-01-05 00:36:00","answers_community":["A (78%)","B (22%)"],"answer_ET":"A","answer":"A","choices":{"A":"Create the bucket with the Autoclass storage class feature.","C":"Create an Object Lifecycle Management policy to modify the storage class for data older than 30 days to coldline, 90 days to nearline, and 365 days to archive storage class. Delete old data as needed.","D":"Create an Object Lifecycle Management policy to modify the storage class for data older than 30 days to nearline, 45 days to coldline, and 60 days to archive storage class. Delete old data as needed.","B":"Create an Object Lifecycle Management policy to modify the storage class for data older than 30 days to nearline, 90 days to coldline, and 365 days to archive storage class. Delete old data as needed."},"unix_timestamp":1704411360,"question_id":167,"answer_images":[],"isMC":true,"exam_id":11,"question_text":"Your team is building a data lake platform on Google Cloud. As a part of the data foundation design, you are planning to store all the raw data in Cloud Storage. You are expecting to ingest approximately 25 GB of data a day and your billing department is worried about the increasing cost of storing old data. The current business requirements are:\n\n• The old data can be deleted anytime.\n• There is no predefined access pattern of the old data.\n• The old data should be available instantly when accessed.\n• There should not be any charges for data retrieval.\n\nWhat should you do to optimize for cost?"},{"id":"9vlsKxKXqiAU8JRZvjgm","choices":{"C":"In the Stackdriver logging admin interface, enable a log sink export to Google Cloud Pub/Sub, and subscribe to the topic from your monitoring tool.","A":"Make a call to the Stackdriver API to list all logs, and apply an advanced filter.","B":"In the Stackdriver logging admin interface, and enable a log sink export to BigQuery.","D":"Using the Stackdriver API, create a project sink with advanced log filter to export to Pub/Sub, and subscribe to the topic from your monitoring tool."},"unix_timestamp":1583950560,"answer":"D","question_text":"You want to use Google Stackdriver Logging to monitor Google BigQuery usage. You need an instant notification to be sent to your monitoring tool when new data is appended to a certain table using an insert job, but you do not want to receive notifications for other tables. What should you do?","answer_images":[],"timestamp":"2020-03-11 19:16:00","question_id":168,"question_images":[],"url":"https://www.examtopics.com/discussions/google/view/16286-exam-professional-data-engineer-topic-1-question-25/","discussion":[{"content":"I would choose D.\nA and B are wrong since don't notify anything to the monitoring tool.\nC has no filter on what will be notified. We want only some tables.","timestamp":"1599840960.0","comment_id":"62604","poster":"jvg637","upvote_count":"49"},{"upvote_count":"15","poster":"MaxNRG","comment_id":"475602","timestamp":"1652188860.0","content":"D as the key requirement is to have notification on a particular table. It can be achieved using advanced log filter to filter only the table logs and create a project sink to Cloud Pub/Sub for notification.\nRefer GCP documentation - Advanced Logs Filters: https://cloud.google.com/logging/docs/view/advanced-queries\nA is wrong as advanced filter will help in filtering. However, there is no notification sends.\nB is wrong as it would send all the logs and BigQuery does not provide notifications.\nC is wrong as it would send all the logs."},{"poster":"suwalsageen12","content":"D is the correct answer because: \n- we need to advance filtering to filter the logs for the specific table\n- we need to use monitoring tool for notification.","timestamp":"1731764940.0","upvote_count":"1","comment_id":"1212403"},{"content":"Selected Answer: D\nGood point by MaxNRG about reducing the number of logs sending to Pub/Sub","comment_id":"1076348","upvote_count":"2","poster":"axantroff","timestamp":"1716293340.0"},{"timestamp":"1714462920.0","comment_id":"1058647","content":"Theorically Pub/Sub could filters log to forward the right ones to the correct topic. https://cloud.google.com/pubsub/docs/subscription-message-filter\nSo C could be accepted, but It's better if filtering is performed earlier, so in this case D is more performing","upvote_count":"1","poster":"ruben82"},{"poster":"rtcpost","comment_id":"1050531","content":"Selected Answer: D\nD. Using the Stackdriver API, create a project sink with an advanced log filter to export to Pub/Sub, and subscribe to the topic from your monitoring tool.\n\nThis approach allows you to set up a custom log sink with an advanced filter that targets the specific table and then export the log entries to Google Cloud Pub/Sub. Your monitoring tool can subscribe to the Pub/Sub topic, providing you with instant notifications when relevant events occur without being inundated with notifications from other tables.\n\nOptions A and B do not offer the same level of customization and specificity in targeting notifications for a particular table.\n\nOption C is almost correct but doesn't mention the use of an advanced log filter in the sink configuration, which is typically needed to filter the logs to a specific table effectively. Using the Stackdriver API for more advanced configuration is often necessary for fine-grained control over log filtering.","timestamp":"1713787440.0","upvote_count":"2"},{"content":"Selected Answer: D\nD makes sense.","comment_id":"1008760","timestamp":"1710554400.0","upvote_count":"1","poster":"suku2"},{"content":"D should be the answer","upvote_count":"1","poster":"GCP_PDE_AG","comment_id":"975414","timestamp":"1707393000.0"},{"content":"Selected Answer: D\nA and B mention nothing about notifications and C would push all data. It's D.","comment_id":"961252","timestamp":"1706087220.0","upvote_count":"1","poster":"Mathew106"},{"content":"Selected Answer: D\nD makes sense","comment_id":"835673","timestamp":"1694401740.0","upvote_count":"1","poster":"bha11111"},{"timestamp":"1687963620.0","comment_id":"760106","poster":"Jackalski","content":"Selected Answer: D\n\"advanced log filter\" is the key word here, all other options push all data ...","upvote_count":"2"},{"content":"Selected Answer: D\nD is the best choice","timestamp":"1683738780.0","upvote_count":"1","comment_id":"715495","poster":"Jasar"},{"timestamp":"1666265040.0","upvote_count":"4","comment_id":"588587","poster":"alecuba16","content":"Selected Answer: D\nUsing the Stackdriver API, create a project sink with advanced log filter to export to Pub/Sub, and subscribe to the topic from your monitoring tool."},{"content":"Selected Answer: D\nD. Option B doesn't make sense","timestamp":"1664916360.0","comment_id":"580917","poster":"devric","upvote_count":"2"},{"timestamp":"1658586240.0","poster":"samdhimal","upvote_count":"3","comment_id":"530672","content":"correct answer -> Using the Stackdriver API, create a project sink with advanced log filter to export to Pub/Sub, and subscribe to the topic from your monitoring tool.\n\nOption C is also most likely right answer but it doesn't have the filter. We don't want all the tables. We only want one. So the correct answer is D.\n\nLogging sink - Using a Logging sink, you can direct specific log entries to your business logic. In this example, you can use Cloud Audit logs for Compute Engine which use the resource type gce_firewall_rule to filter for the logs of interest. You can also add an event type GCE_OPERATION_DONE to the filter to capture only the completed log events. Here is the Logging filter used to identify the logs. You can try out the query in the Logs Viewer.\n\nPub/Sub topic – In Pub/Sub, you can create a topic to which to direct the log sink and use the Pub/Sub message to trigger a cloud function. \n\nReference: https://cloud.google.com/blog/products/management-tools/automate-your-response-to-a-cloud-logging-event"},{"comment_id":"521159","poster":"santoshindia","timestamp":"1657482900.0","upvote_count":"3","content":"Selected Answer: D\nexplained by MaxNRG"},{"content":"Selected Answer: D\nas explained by MaxNRG","poster":"medeis_jar","timestamp":"1656927240.0","upvote_count":"3","comment_id":"516525"},{"timestamp":"1656416760.0","poster":"rebootshen","comment_id":"511161","upvote_count":"2","content":"C is simple, the sink has filter option, \"In the Build inclusion filter field, enter a filter expression that matches the log entries you want to include\""},{"timestamp":"1653553080.0","comment_id":"487248","content":"Selected Answer: D\nas wrote MaxNRG","poster":"StefanoG","upvote_count":"3"},{"upvote_count":"3","content":"Ans: D\nNotification to the monitoring can be sent via Pub/Sub and it is one off use case of Pub/Sub. To deliver log/ notification to Pub/Sub need a sink followed by a filter to look only for append data to BigQuery.\nSo \nA is out as no Sink.\nB is not correct as it is saying sink export to BigQuery.\nC is not talking about filtering after setting up sink.","timestamp":"1650042600.0","poster":"anji007","comment_id":"462719"},{"poster":"fire558787","upvote_count":"5","content":"It's definitely D. From: https://cloud.google.com/logging/docs/reference/v2/rest/v2/projects.sinks \"a sink used to export log entries to one of the following destinations in any project: a Cloud Storage bucket, a BigQuery dataset, a Pub/Sub topic or a Cloud Logging log bucket. A logs filter controls which log entries are exported\".\nB is wrong because why would you export to BigQuery again if you want to be notified by your monitoring tool?","timestamp":"1645030980.0","comment_id":"425861"},{"upvote_count":"2","comment_id":"397047","content":"I would choose D\nReference : https://cloud.google.com/logging/docs/reference/v2/rest/v2/projects.sinks\nOnly Key word that was sure initially is \"Stackdriver API\" (and I found the link for it too https://cloud.google.com/monitoring/api/v3)","timestamp":"1641147900.0","poster":"raf2121"},{"comment_id":"391277","poster":"sumanshu","content":"Vote for 'D' - because we need to put filter for particular table, also we need monitoring tool","upvote_count":"2","timestamp":"1640532540.0"},{"poster":"jasper_430","content":"I would choose D since A and B don't make any notification and C doesn't make a filter of the result.","comment_id":"354349","upvote_count":"1","timestamp":"1636616880.0"},{"poster":"Ette","content":"I would choose C: you can set a filter 'protoPayload.metadata.jobInsertion.job.jobConfig.queryConfig.destinationTable=...'","comments":[{"content":"i agree, in the link it reads: \"Depending on the log sink's configuration, which includes filters and a destination\" so you should be able to config the filter when sinking to destination - Pub/Sub. it would be silly it can't do simple things like this. whileas \"project sink\" in Answer D, what is that?","comment_id":"386604","timestamp":"1640047620.0","poster":"moonlightbeamer","upvote_count":"1"}],"timestamp":"1636187340.0","comment_id":"350772","upvote_count":"4"},{"timestamp":"1631150340.0","upvote_count":"2","content":"D:\nhttps://cloud.google.com/logging/docs/routing/overview","comment_id":"306114","poster":"daghayeghi"},{"comment_id":"285001","poster":"naga","content":"Correct D","timestamp":"1628264520.0","upvote_count":"2"},{"poster":"Radhika7983","timestamp":"1621133940.0","comment_id":"220058","upvote_count":"5","content":"The correct answer is D. please check the link https://cloud.google.com/blog/products/management-tools/automate-your-response-to-a-cloud-logging-event\n\nUsing a Logging sink, you can build an event-driven system to detect and respond to log events in real time. Cloud Logging can help you to build this event-driven architecture through its integration with Cloud Pub/Sub and a serverless computing service such as Cloud Functions or Cloud Run.\n\nHowever as we need to create an alert only for certain table we will need to go with advanced log queries to filter only required log."},{"comment_id":"219830","upvote_count":"3","poster":"xfoursea","content":"The solution answers are a bit out of date, check out new solution in similar scenario: https://www.linkedin.com/pulse/bring-us-red-alert-how-implement-stackdriver-alerting-phil-goerdt/\n\nI would pick D","timestamp":"1621091760.0"},{"content":"you can use log router for this: https://cloud.google.com/logging/docs/routing/overview.\nso i think it should be C. I have no idea why it would be B.","poster":"aleedrew","upvote_count":"2","timestamp":"1617795780.0","comment_id":"195159"},{"upvote_count":"6","content":"Answer is D.\nhttps://cloud.google.com/logging/docs/export \nAll logs, including audit logs, platform logs, and user logs, are sent to the Cloud Logging API where they pass through the Logs Router. The Logs Router checks each log entry against existing rules to determine which log entries to ingest (store), which log entries to include in exports, and which log entries to discard. For more details, see Logs Router overview.\n\nExporting involves writing a filter that selects the log entries you want to export, and choosing a destination in Cloud Storage, BigQuery, Pub/Sub, or Cloud Logging. The filter and destination are held in an object called a sink. Sinks can be created in Google Cloud projects, organizations, folders, and billing accounts.","comment_id":"192618","timestamp":"1617499860.0","poster":"Cloud_Enthusiast"},{"poster":"yurstev","upvote_count":"2","comment_id":"182717","timestamp":"1616226060.0","content":"D. \nhttps://cloud.google.com/logging/docs/export"},{"comment_id":"171735","timestamp":"1614669000.0","upvote_count":"2","poster":"KennneK","content":"D\nSD logging filters out the \"insert job\" on specific dataset/table, P/S does the broadcasting and finally monitoring tool get message from the topic."},{"comment_id":"68533","content":"Answer: D\nDescription: Looks best out of the rest","poster":"[Removed]","upvote_count":"6","timestamp":"1601184660.0"},{"upvote_count":"6","comment_id":"65941","timestamp":"1600501260.0","content":"Should choose D.","poster":"[Removed]"},{"timestamp":"1600304040.0","comment_id":"64983","upvote_count":"12","content":"I also prefer D. While C should also works, but the subscriber needs to do filtering on its own and not sure the monitoring tool can do that.","poster":"rickywck"}],"topic":"1","answer_description":"","exam_id":11,"answers_community":["D (100%)"],"isMC":true,"answer_ET":"D"},{"id":"oLi2DvpIHZLckvPSmqKl","isMC":true,"topic":"1","choices":{"B":"1. Create a pipeline to de-identify the email field by using recordTransformations in Cloud DLP with format-preserving encryption with FFX as the de-identification transformation type.\n2. Load the booking and user profile data into a BigQuery table.","D":"1. Load the CSV files from Cloud Storage into a BigQuery table, and enable dynamic data masking.\n2. Create a policy tag with the default masking value as the data masking rule.\n3. Assign the policy to the email field in both tables.\n4. Assign the Identity and Access Management bigquerydatapolicy.maskedReader role for the BigQuery tables to the analysts","A":"1. Create a pipeline to de-identify the email field by using recordTransformations in Cloud Data Loss Prevention (Cloud DLP) with masking as the de-identification transformations type.\n2. Load the booking and user profile data into a BigQuery table.","C":"1. Load the CSV files from Cloud Storage into a BigQuery table, and enable dynamic data masking.\n2. Create a policy tag with the email mask as the data masking rule.\n3. Assign the policy to the email field in both tables. A\n4. Assign the Identity and Access Management bigquerydatapolicy.maskedReader role for the BigQuery tables to the analysts."},"question_images":[],"question_id":169,"answer_description":"","answer":"B","url":"https://www.examtopics.com/discussions/google/view/130198-exam-professional-data-engineer-topic-1-question-250/","unix_timestamp":1704292500,"answer_ET":"B","answer_images":[],"timestamp":"2024-01-03 15:35:00","exam_id":11,"answers_community":["B (85%)","Other"],"discussion":[{"timestamp":"1706179140.0","comment_id":"1131572","upvote_count":"15","poster":"lipa31","content":"Selected Answer: B\nFormat-preserving encryption (FPE) with FFX in Cloud DLP is a strong choice for de-identifying PII like email addresses. FPE maintains the format of the data and ensures that the same input results in the same encrypted output consistently. This means the email fields in both datasets can be encrypted to the same value, allowing for accurate joins in BigQuery while keeping the actual email addresses hidden."},{"comment_id":"1115773","timestamp":"1704627360.0","upvote_count":"5","content":"As it states \"You need to de-identify the email field in both the datasets before loading them into BigQuery for analysts\" data masking should not be an option as the data would stored unmasked in BigQuery?","poster":"Smakyel79"},{"upvote_count":"4","poster":"Anudeep58","timestamp":"1718354220.0","comment_id":"1230310","content":"Selected Answer: B\nOption A:\nMasking: Simple masking might not preserve the uniqueness and joinability of the email field, making it difficult to perform accurate joins between datasets.\nOption C and D:\nDynamic Data Masking: These options involve masking the email field dynamically within BigQuery, which does not address the requirement to de-identify data before loading into BigQuery. Additionally, dynamic masking does not prevent access to the actual email data before it is loaded into BigQuery, potentially exposing PII during the data ingestion process."},{"comment_id":"1212428","poster":"chrissamharris","upvote_count":"2","content":"Selected Answer: B\nformat-preserving encryption with FFX is required as the analysts want to perform JOINs","timestamp":"1715865600.0"},{"content":"Selected Answer: B\nOption B\nhttps://cloud.google.com/sensitive-data-protection/docs/pseudonymization","upvote_count":"3","comment_id":"1154472","timestamp":"1708401420.0","poster":"JyoGCP"},{"upvote_count":"4","poster":"ML6","content":"Selected Answer: B\nA) masking = replace with a surrogate character like # or * = output not unique, so cannot apply joins\nC and D) question specifies to de-identify BEFORE loading into BQ, whereas these options perform dynamic masking IN BigQuery.\n\nTherefore, only valid option is B.","timestamp":"1708168320.0","comment_id":"1152501"},{"comment_id":"1121698","timestamp":"1705154160.0","upvote_count":"1","content":"Selected Answer: C\nOption C. The need is to just mask the data to Analyst, without modifying the underlying data. Moreover, it's stored on 2 separate tables and the analysts need to be able to perform joins based on the masked data. Dynamic masking is the right module and the right masking rule is email mask (https://cloud.google.com/bigquery/docs/column-data-masking-intro#masking_options) which guarantees the join capabilities join","poster":"Matt_108"},{"comment_id":"1119508","poster":"task_7","content":"Selected Answer: B\nA wouldn't preserve the email format\nC&D maskedReader roles still grant access to the underlying values.\nthe only option is B","timestamp":"1704961620.0","upvote_count":"5","comments":[{"poster":"alfguemat","upvote_count":"1","timestamp":"1705042080.0","comment_id":"1120498","comments":[{"poster":"dduenas","upvote_count":"1","comment_id":"1141634","content":"masking only replace by specific characters, doing the field not unique and not ready for joins.","timestamp":"1707178500.0"}],"content":"I dont't know why preserve email format is necessary to perform the join. A could be valid."}]},{"upvote_count":"1","comment_id":"1117342","timestamp":"1704792720.0","content":"Selected Answer: C\nI will go for C, because there is a separate type of masking for emails, so whe to use the dafault? https://cloud.google.com/bigquery/docs/column-data-masking-intro#masking_options","poster":"Sofiia98"},{"timestamp":"1704759060.0","comments":[{"content":"should be correct if they want to access tables and it's not valid for datasets","poster":"tibuenoc","upvote_count":"1","timestamp":"1707316620.0","comment_id":"1143407"}],"content":"Selected Answer: C\ndata masking with BQ is correct with email masking rule.\nRef - https://cloud.google.com/bigquery/docs/column-data-masking-intro","comment_id":"1117126","upvote_count":"1","poster":"GCP001"},{"timestamp":"1704569520.0","upvote_count":"2","poster":"Jordan18","comment_id":"1115398","content":"why not B?"},{"comment_id":"1114136","content":"Selected Answer: C\n- The reason option C works well is that dynamic data masking in BigQuery allows the underlying data to remain unaltered (thus preserving the ability to join on this field), while also preventing analysts from viewing the actual PII. \n- The analysts can query and join the data as needed for their analysis, but when they access the data, the email field will be masked according to the policy tag, and they will only see the masked version.","poster":"raaad","upvote_count":"2","timestamp":"1704413040.0"},{"timestamp":"1704292500.0","upvote_count":"1","content":"Selected Answer: D\nD. 1. Load the CSV files from Cloud Storage into a BigQuery table, and enable dynamic data masking.\n2. Create a policy tag with the default masking value as the data masking rule.\n3. Assign the policy to the email field in both tables.\n4. Assign the Identity and Access Management bigquerydatapolicy.maskedReader role for the BigQuery tables to the analysts","comment_id":"1112854","poster":"scaenruy"}],"question_text":"Your company's data platform ingests CSV file dumps of booking and user profile data from upstream sources into Cloud Storage. The data analyst team wants to join these datasets on the email field available in both the datasets to perform analysis. However, personally identifiable information (PII) should not be accessible to the analysts. You need to de-identify the email field in both the datasets before loading them into BigQuery for analysts. What should you do?"},{"id":"ryKe7k5blBXe5kKWekfN","unix_timestamp":1704294960,"question_id":170,"topic":"1","timestamp":"2024-01-03 16:16:00","url":"https://www.examtopics.com/discussions/google/view/130202-exam-professional-data-engineer-topic-1-question-251/","question_images":[],"question_text":"You have important legal hold documents in a Cloud Storage bucket. You need to ensure that these documents are not deleted or modified. What should you do?","isMC":true,"answer_ET":"A","answer_description":"","answers_community":["A (100%)"],"answer_images":[],"answer":"A","exam_id":11,"discussion":[{"poster":"raaad","content":"Selected Answer: A\n- Setting a retention policy on a Cloud Storage bucket prevents objects from being deleted for the duration of the retention period. \n- Locking the policy makes it immutable, meaning that the retention period cannot be reduced or removed, thus ensuring that the documents cannot be deleted or overwritten until the retention period expires.","upvote_count":"6","comment_id":"1114138","comments":[{"upvote_count":"2","content":"Agree. https://cloud.google.com/storage/docs/bucket-lock#overview","timestamp":"1721214600.0","poster":"AllenChen123","comment_id":"1124983"}],"timestamp":"1720131120.0"},{"timestamp":"1724119620.0","comment_id":"1154479","upvote_count":"2","poster":"JyoGCP","content":"Selected Answer: A\nOption A"},{"content":"Selected Answer: A\nOption A - set retention policy to prevent deletion, lock it to make it immutable (not subject to edits)","timestamp":"1720872240.0","poster":"Matt_108","upvote_count":"2","comment_id":"1121704"},{"timestamp":"1720012560.0","upvote_count":"1","content":"Selected Answer: A\nA. Set a retention policy. Lock the retention policy.","comment_id":"1112882","poster":"scaenruy"}],"choices":{"D":"Enable the Object Versioning feature. Create a copy in a bucket in a different region.","A":"Set a retention policy. Lock the retention policy.","C":"Enable the Object Versioning feature. Add a lifecycle rule.","B":"Set a retention policy. Set the default storage class to Archive for long-term digital preservation."}}],"exam":{"isBeta":false,"provider":"Google","isMCOnly":true,"numberOfQuestions":319,"isImplemented":true,"id":11,"lastUpdated":"11 Apr 2025","name":"Professional Data Engineer"},"currentPage":34},"__N_SSP":true}