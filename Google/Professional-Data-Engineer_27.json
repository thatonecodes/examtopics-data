{"pageProps":{"questions":[{"id":"2Q5ncaKRP92FeeGKqFsb","unix_timestamp":1703925660,"answer_description":"","answer":"A","question_id":131,"question_images":[],"discussion":[{"timestamp":"1724112780.0","poster":"aoifneofi_ef","content":"Selected Answer: D\nIt is a tie between A and D. \nOption A will definitely provide necessary connectivity but is less secure as access is enabled to \"all instances\". Which i feel is unnecessary considering industry best practices.\n\nOption D provides the necessary connectivity but brings in the unnecessary overhead of managing an extra VM and introduces a bit of extra complexity.\n\nSince the question emphasises on data not going through public internet(which is satisfied in both options), i would prioritise security over simplicity and choose option D in this case.","upvote_count":"6","comment_id":"1268972"},{"timestamp":"1734226860.0","upvote_count":"5","content":"Selected Answer: A\nA looks to be the best out of the 4, D is complicated involving Compute Engine which is unnecessary making it cumbersome to address the problem","poster":"clouditis","comment_id":"1326651"},{"upvote_count":"1","comment_id":"1345058","timestamp":"1737600240.0","poster":"71083a7","content":"Selected Answer: D\n\"all instances\" freaks me out"},{"comment_id":"1324451","timestamp":"1733820480.0","poster":"julydev82","content":"Selected Answer: A\nvpc create a subnet beetwen sql Cloud and the dataflow.\nhttps://cloud.google.com/sql/docs/mysql/private-ip#multiple_vpc_connectivity","upvote_count":"2"},{"upvote_count":"1","content":"Selected Answer: A\nThe answer is A. While D might work, it adds unnecessary complexity. Setting up a proxy is an extra layer of infrastructure that isnâ€™t required","timestamp":"1726349400.0","comment_id":"1283781","poster":"fadlkhafdofpew"},{"poster":"Saaaurabh","timestamp":"1723854060.0","upvote_count":"1","content":"Selected Answer: A\nIf properly implemented with the right routing and firewall rules, Option A can be the correct and most straightforward solution, as it leverages VPC Peering to maintain internal traffic without going through the public internet.","comment_id":"1267377"},{"content":"A is correct","comment_id":"1263553","upvote_count":"1","poster":"meh_33","timestamp":"1723300980.0"},{"timestamp":"1720727280.0","content":"still confused between A and D","poster":"kk1211","upvote_count":"1","comment_id":"1246293"},{"upvote_count":"2","content":"Selected Answer: A\nA is correct","poster":"Lenifia","timestamp":"1720223100.0","comment_id":"1243085"},{"content":"Selected Answer: A\nno proxy needed","upvote_count":"1","comment_id":"1240731","timestamp":"1719919860.0","poster":"kajitsu"},{"comment_id":"1226605","upvote_count":"1","timestamp":"1717830840.0","poster":"Lestrang","content":"Selected Answer: A\nPeople referencing \"VPC Network Peering does not provide transitive routing. For example, if VPC networks net-a and net-b are connected using VPC Network Peering, and VPC networks net-a and net-c are also connected using VPC Network Peering, VPC Network Peering does not provide connectivity between net-b and net-c.\"\n\nthe question states that cloud sql is running in project B. \nWhich means the instance is already part of the VPC in project B, so with Network Peering workers from A can definitely access data in B. No proxy is needed."},{"timestamp":"1716534060.0","comments":[{"content":"Now I see why, I put this on ChatGPT and it thinks the right answer is D. I'm pretty sure that's a hallucination.","timestamp":"1716534840.0","poster":"fabiogoma","upvote_count":"2","comment_id":"1217270"}],"poster":"fabiogoma","content":"Selected Answer: A\nWhy so many people are voting for D? There's no need for a proxy, the peering is enough to allow network traffic between subnets.","comment_id":"1217262","upvote_count":"2"},{"timestamp":"1716284280.0","content":"Proxy? no, it is not necessary..\n\nA","upvote_count":"2","poster":"ccpmad","comment_id":"1214840"},{"comment_id":"1213420","poster":"josech","upvote_count":"2","timestamp":"1716055920.0","content":"Selected Answer: D\nhttps://cloud.google.com/sql/docs/mysql/connect-multiple-vpcs"},{"content":"Selected Answer: A\nA - The requirement for a proxy is un-necessary: \n https://cloud.google.com/sql/docs/mysql/private-ip#multiple_vpc_connectivity","comment_id":"1160007","poster":"chrissamharris","timestamp":"1708974300.0","upvote_count":"4"},{"poster":"ML6","upvote_count":"2","timestamp":"1708350600.0","comment_id":"1153955","content":"Option D. Source: https://cloud.google.com/sql/docs/mysql/private-ip#multiple_vpc_connectivity"},{"content":"Selected Answer: D\nOption D","comment_id":"1152526","timestamp":"1708171620.0","upvote_count":"1","poster":"JyoGCP"},{"upvote_count":"2","poster":"lipa31","timestamp":"1706205840.0","comment_id":"1131915","content":"Selected Answer: D\nthe reason : Cloud SQL supports private IP addresses through private service access. When you create a Cloud SQL instance, Cloud SQL creates the instance within its own virtual private cloud (VPC), called the Cloud SQL VPC. Enabling private IP requires setting up a peering connection between the Cloud SQL VPC and your VPC network."},{"poster":"saschak94","upvote_count":"1","comment_id":"1124083","timestamp":"1705400220.0","content":"Selected Answer: D\nUsing VPC Network Peering, Cloud SQL implements private service access internally, which allows internal IP addresses to connect across two VPC networks regardless of whether they belong to the same project or organization. \n\nHowever, since VPC Network Peering isn't transitive, it only broadcasts routes between the two VPCs that are directly peered. If you have an additional VPC, it won't be able to access your Cloud SQL resources using the connection set up with your original VPC."},{"timestamp":"1705314180.0","upvote_count":"1","poster":"datapassionate","content":"Selected Answer: D\nD. Set up VPC Network Peering between Project A and Project B. Create a Compute Engine instance without external IP address in Project B on the peered subnet to serve as a proxy server to the Cloud SQL database.","comment_id":"1123244"},{"timestamp":"1705140540.0","content":"Selected Answer: D\nOption D is the most aligned to best practices for me","comment_id":"1121485","upvote_count":"1","poster":"Matt_108"},{"comment_id":"1118400","content":"Selected Answer: D\nOption D is the correct answer. The reason is you cannot access cloud sql or alloydb instances from a peered vpc connection as they will be hosted in service project not in Project B. The VPC Peering doesn't give transitive routing so accessing cloud sql directly is not possible without a proxy vm. https://cloud.google.com/vpc/docs/vpc-peering#spec-general","upvote_count":"3","timestamp":"1704878280.0","poster":"BIGQUERY_ALT_ALT"},{"comments":[{"upvote_count":"2","comment_id":"1160337","timestamp":"1709022420.0","content":"I think you're incorrect. VPC Peering does not traverse the public internet. \nhttps://cloud.google.com/vpc/docs/using-vpc-peering","poster":"chrissamharris"}],"comment_id":"1116137","content":"Selected Answer: D\nD is the correct solution.\n\nTo allow the Dataflow workers in Project A to connect to the private Cloud SQL instance in Project B, you need to set up VPC Network Peering between the two projects.\n\nThen create a Compute Engine instance without external IP in Project B on the peered subnet. This instance can serve as a proxy server to connect to the private Cloud SQL instance.\n\nThe Dataflow workers can connect through the peered network to the proxy instance, which then connects to Cloud SQL. This allows accessing the private Cloud SQL instance without going over the public internet.\n\nOption A would allow access but still goes over the public internet.\n\nOption B and C would not work since the Cloud SQL instance does not have a public IP address.\n\nSo D is the right approach to resolve the connection issue while keeping the data private.","timestamp":"1704657300.0","upvote_count":"4","poster":"MaxNRG"},{"comments":[{"upvote_count":"1","poster":"Anudeep58","timestamp":"1716547860.0","content":"The Cloud SQL. instance is running in Project B and does not have a public IP address.\nCorrect would be D. Any thoughts ?","comment_id":"1217386"}],"comment_id":"1113219","timestamp":"1704324900.0","content":"Selected Answer: A\nVPC Network Peering allows for the connection of two VPC networks so that they can communicate internally as if they were part of the same network.","poster":"raaad","upvote_count":"2"},{"upvote_count":"2","comment_id":"1109542","content":"Selected Answer: A\nSecure Private Communication:\n\nEstablishes a direct, private connection between the VPCs, eliminating exposure to the public internet.\nEnsures data confidentiality and integrity.","poster":"e70ea9e","timestamp":"1703925660.0"}],"exam_id":11,"answer_ET":"A","url":"https://www.examtopics.com/discussions/google/view/129863-exam-professional-data-engineer-topic-1-question-216/","isMC":true,"answers_community":["A (51%)","D (49%)"],"choices":{"A":"Set up VPC Network Peering between Project A and Project B. Add a firewall rule to allow the peered subnet range to access all instances on the network.","B":"Turn off the external IP addresses on the Dataflow worker. Enable Cloud NAT in Project A.","C":"Add the external IP addresses of the Dataflow worker as authorized networks in the Cloud SQL instance.","D":"Set up VPC Network Peering between Project A and Project B. Create a Compute Engine instance without external IP address in Project B on the peered subnet to serve as a proxy server to the Cloud SQL database."},"timestamp":"2023-12-30 09:41:00","answer_images":[],"question_text":"You are developing an Apache Beam pipeline to extract data from a Cloud SQL instance by using JdbcIO. You have two projects running in Google Cloud. The pipeline will be deployed and executed on Dataflow in Project A. The Cloud SQL. instance is running in Project B and does not have a public IP address. After deploying the pipeline, you noticed that the pipeline failed to extract data from the Cloud SQL instance due to connection failure. You verified that VPC Service Controls and shared VPC are not in use in these projects. You want to resolve this error while ensuring that the data does not go through the public internet. What should you do?","topic":"1"},{"id":"u5UBjL58pYSJ8Qq2q7Br","url":"https://www.examtopics.com/discussions/google/view/129864-exam-professional-data-engineer-topic-1-question-217/","unix_timestamp":1703925720,"topic":"1","discussion":[{"poster":"qq589539483084gfrgrgfr","comment_id":"1116435","timestamp":"1704693120.0","content":"Option B & E","upvote_count":"7"},{"comment_id":"1123270","content":"Selected Answer: E\nB& E\n https://cloud.google.com/bigquery/docs/column-level-security-intro","poster":"datapassionate","timestamp":"1705317240.0","upvote_count":"5"},{"comment_id":"1243086","upvote_count":"2","comments":[{"comment_id":"1243087","poster":"Lenifia","timestamp":"1720223280.0","content":"Explanation of why other options are incorrect:\n\nB. Ensure that the data analytics team members do not have the Data Catalog Fine-Grained Reader role for the policy tags: This role relates to viewing data in Data Catalog based on policy tags, not directly controlling access to BigQuery data.\n\nD. Remove the bigquery.dataViewer role from the data analytics team on the authorized datasets: Removing this role would block all access to the dataset, which is too restrictive if they still need access to non-sensitive columns.\n\nE. Enforce access control in the policy tag taxonomy: While policy tags are used to enforce access controls, simply enforcing controls in the taxonomy does not directly address the issue of sensitive data access in BigQuery.","upvote_count":"2"}],"timestamp":"1720223280.0","poster":"Lenifia","content":"Selected Answer: A\nthe correct options are:\n\nA. Create two separate authorized datasets; one for the data analytics team and another for the consumer support team.\nC. Replace the authorized dataset with an authorized view. Use row-level security and apply filter_expression to limit data access."},{"upvote_count":"3","content":"Selected Answer: E\nB & E\nB - It will ensure they don't have access to secure columns\nE- It will allow to enforce column level security\nRef - https://cloud.google.com/bigquery/docs/column-level-security-intro","poster":"GCP001","timestamp":"1705242360.0","comment_id":"1122596"},{"content":"Selected Answer: B\nOption B& E to me","comment_id":"1121500","poster":"Matt_108","timestamp":"1705141200.0","upvote_count":"2"},{"comments":[{"upvote_count":"3","comment_id":"1121499","content":"Max I feel like it's more B&E. \nI do agree on the revoking Data Catalog Fine-grained reader role to avoid the data analytics team to read policy tags metadata, but if the tags are setup as stated, it's just missing the enforcement of the policy tags themselves.\nCreating 2 auth dataset is not efficient on big datasets and Data catalog+ policy tags are built to manage these situations. Don't you agree?","timestamp":"1705141140.0","poster":"Matt_108"}],"timestamp":"1704694200.0","content":"Selected Answer: A\nA & B\nThe current setup is not effective because the data analytics team still has access to the sensitive columns despite using an authorized dataset and policy tags. This indicates that the policy tags are not being enforced properly, and the data analytics team members are able to view the tags and gain access to the sensitive data.\nSeparating the data into two distinct authorized datasets is a better approach because it isolates the sensitive data from the non-sensitive data. This prevents the data analytics team from accessing the sensitive columns directly, even if they have access to the authorized dataset for general customer data.\nAdditionally, revoking the Data Catalog Fine-Grained Reader role from the data analytics team members ensures that they cannot view or modify the policy tags. This limits their ability to bypass the access control imposed by the authorized dataset and policy tags.","comment_id":"1116447","poster":"MaxNRG","upvote_count":"3"},{"upvote_count":"1","content":"And the second answer? One is option B and the other is option D maybe?","poster":"imiu","timestamp":"1704373500.0","comment_id":"1113695"},{"upvote_count":"3","poster":"raaad","content":"Selected Answer: B\n- The Data Catalog Fine-Grained Reader role allows users to read metadata that is restricted by policy tags. \n- If members of the data analytics team have this role, they might bypass the restrictions set by policy tags. \n- Ensuring they do not have this role will help enforce the restrictions intended by the policy tags.","comment_id":"1113225","timestamp":"1704325200.0"},{"comment_id":"1109543","upvote_count":"2","timestamp":"1703925720.0","poster":"e70ea9e","content":"Selected Answer: B\nPrevents data analytics team members from viewing sensitive data, even if it's tagged.\nRestricts access to policy tags themselves, ensuring confidentiality of sensitive information."}],"answer":"E","isMC":true,"timestamp":"2023-12-30 09:42:00","answer_description":"","answer_images":[],"choices":{"A":"Create two separate authorized datasets; one for the data analytics team and another for the consumer support team.","D":"Remove the bigquery.dataViewer role from the data analytics team on the authorized datasets.","E":"Enforce access control in the policy tag taxonomy.","C":"Replace the authorized dataset with an authorized view. Use row-level security and apply filter_expression to limit data access.","B":"Ensure that the data analytics team members do not have the Data Catalog Fine-Grained Reader role for the policy tags."},"question_images":[],"exam_id":11,"question_id":132,"answers_community":["E (40%)","B (35%)","A (25%)"],"answer_ET":"E","question_text":"You have a BigQuery table that contains customer data, including sensitive information such as names and addresses. You need to share the customer data with your data analytics and consumer support teams securely. The data analytics team needs to access the data of all the customers, but must not be able to access the sensitive data. The consumer support team needs access to all data columns, but must not be able to access customers that no longer have active contracts. You enforced these requirements by using an authorized dataset and policy tags. After implementing these steps, the data analytics team reports that they still have access to the sensitive columns. You need to ensure that the data analytics team does not have access to restricted data. What should you do? (Choose two.)"},{"id":"oAlg43zkxDh00nRGtVel","discussion":[{"upvote_count":"5","timestamp":"1704325800.0","comment_id":"1113228","poster":"raaad","content":"Selected Answer: C\nAfter promoting the read replica in Region2 to be the new primary instance, creating additional read replicas from it can help distribute the read load and maintain or increase the database's total capacity."},{"poster":"skhaire","comment_id":"1365592","timestamp":"1741208460.0","upvote_count":"1","content":"Selected Answer: C\nCorrected answer: C\n\nIf the primary instance (db-a-0) becomes unavailable, you can promote the replica in region B to become the primary. To again have additional replicas in regions A and C, delete the old instances (the former primary instance in A, and the replica in C), and create new read replicas from the new primary instance in B."},{"comment_id":"1365591","poster":"skhaire","timestamp":"1741208040.0","upvote_count":"1","content":"Selected Answer: D\nQuestion is flawed but the closest answer would be D since C will result in 2 read replicas on Region 3 (original one and new now) \n Option C- Create two new read replicas from the new primary instance - contradicts with the requirements - You need to ensure that your application has the same database capacity available 'before you switch over the connections.' \nOption D- Create a new read replica in Region1, promote the new read replica to be the primary instance - contradicts with the requirement - 'requires that you perform disaster\nrecovery by promoting a read replica in Region2.' How does this affect the answer choices?"},{"content":"Selected Answer: C\nhttps://cloud.google.com/sql/docs/mysql/replication#cross-region-read-replicas","upvote_count":"3","poster":"josech","timestamp":"1716056520.0","comment_id":"1213428","comments":[{"timestamp":"1724600460.0","content":"requires 2 new read replicas as the read replica that wasn't promoted isn't capable to be a replica any more as the primary isa gone","upvote_count":"1","poster":"nadavw","comment_id":"1272196"}]},{"upvote_count":"4","comment_id":"1191195","comments":[{"content":"Who said that i can use a 4Â° region? If have constraint that i can't go out from that 3 regions?\nBy My opinion will be a right solution if the new replica will in another zona of the region 1 or 3.\nmay be the best solution is the case D","poster":"BigDataBB","upvote_count":"1","timestamp":"1712898180.0","comment_id":"1194151","comments":[{"poster":"BigDataBB","comment_id":"1194153","content":"https://cloud.google.com/sql/docs/postgres/replication/cross-region-replicas?hl=en","timestamp":"1712898660.0","upvote_count":"1"}]}],"content":"Selected Answer: C\nThe best option here is C. Create two new read replicas from the new primary instance, one in Region3 and one in a new region.\n\nHere's the breakdown:\n\nCapacity Restoration: Promoting the Region2 replica makes it the new primary. You need to replicate from this new primary to maintain redundancy and capacity. Creating two replicas (Region3, new region) accomplishes this.\nGeographic Distribution: Distributing replicas across regions ensures availability if another regional event occurs.\nSpeed: Creating new replicas from the promoted primary is likely faster than promoting another existing replica.","poster":"CGS22","timestamp":"1712522640.0"},{"content":"Selected Answer: C\nOption C","upvote_count":"1","timestamp":"1708171680.0","comment_id":"1152527","poster":"JyoGCP"},{"upvote_count":"2","timestamp":"1703925780.0","poster":"e70ea9e","content":"Selected Answer: C\nImmediate Failover:\n\nPromoting the read replica in Region2 quickly restores database operations in a different region, aligning with disaster recovery goals.\nCapacity Restoration:\n\nCreates two new read replicas from the promoted primary instance (formerly the read replica in Region2).\nThis replaces the lost capacity in Region1 and adds a read replica in a new region for further redundancy.","comment_id":"1109544"}],"unix_timestamp":1703925780,"question_id":133,"answer":"C","exam_id":11,"answer_images":[],"isMC":true,"answer_ET":"C","timestamp":"2023-12-30 09:43:00","choices":{"D":"Create a new read replica in Region1, promote the new read replica to be the primary instance, and enable zonal high availability.","A":"Enable zonal high availability on the primary instance. Create a new read replica in a new region.","B":"Create a cascading read replica from the existing read replica in Region3.","C":"Create two new read replicas from the new primary instance, one in Region3 and one in a new region."},"question_images":[],"url":"https://www.examtopics.com/discussions/google/view/129865-exam-professional-data-engineer-topic-1-question-218/","answer_description":"","answers_community":["C (94%)","6%"],"topic":"1","question_text":"You have a Cloud SQL for PostgreSQL instance in Regionâ€™ with one read replica in Region2 and another read replica in Region3. An unexpected event in Regionâ€™ requires that you perform disaster recovery by promoting a read replica in Region2. You need to ensure that your application has the same database capacity available before you switch over the connections. What should you do?"},{"id":"yDL8ZGpcpOFGNHYLxO4o","answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/129866-exam-professional-data-engineer-topic-1-question-219/","choices":{"A":"Assign a function with notification logic to the on_retry_callback parameter for the operator responsible for the task at risk.","C":"Assign a function with notification logic to the on_failure_callback parameter tor the operator responsible for the task at risk.","B":"Configure a Cloud Monitoring alert on the sla_missed metric associated with the task at risk to trigger a notification.","D":"Assign a function with notification logic to the sla_miss_callback parameter for the operator responsible for the task at risk."},"unix_timestamp":1703925840,"discussion":[{"poster":"jonty4gcp","content":"What is Task is long-running and in between stuck?","upvote_count":"1","comment_id":"1266801","timestamp":"1723778700.0"},{"comment_id":"1217491","poster":"Anudeep58","timestamp":"1716556740.0","upvote_count":"2","content":"Selected Answer: C\nhttps://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/callbacks.html"},{"content":"Selected Answer: C\non_failure_callback","poster":"JyoGCP","upvote_count":"2","comment_id":"1158021","timestamp":"1708789980.0"},{"comment_id":"1123276","upvote_count":"3","poster":"datapassionate","timestamp":"1705317600.0","content":"Selected Answer: C\non_failure_callback is invoked when the task fails\nhttps://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/callbacks.html"},{"timestamp":"1705141620.0","comment_id":"1121508","upvote_count":"1","content":"Selected Answer: C\nOption C to me","poster":"Matt_108"},{"content":"Selected Answer: C\n- The on_failure_callback is a function that gets called when a task fails. \n- Assigning a function with notification logic to this parameter is a direct way to handle task failures. \n- When the task fails, this function can trigger a notification, making it an appropriate solution for the need to be alerted on task failures.","upvote_count":"4","timestamp":"1704368820.0","comment_id":"1113634","poster":"raaad"},{"content":"Selected Answer: C\nDirect Trigger:\n\nThe on_failure_callback parameter is specifically designed to invoke a function when a task fails, ensuring immediate notification.\nCustomizable Logic:\n\nYou can tailor the notification function to send emails, create alerts, or integrate with other notification systems, providing flexibility.","poster":"e70ea9e","timestamp":"1703925840.0","comment_id":"1109546","upvote_count":"4"}],"topic":"1","isMC":true,"question_id":134,"exam_id":11,"answer_ET":"C","answers_community":["C (100%)"],"timestamp":"2023-12-30 09:44:00","answer":"C","question_images":[],"answer_description":"","question_text":"You orchestrate ETL pipelines by using Cloud Composer. One of the tasks in the Apache Airflow directed acyclic graph (DAG) relies on a third-party service. You want to be notified when the task does not succeed. What should you do?"},{"id":"K8qufgfVk6cDJ87CmwKJ","choices":{"B":"Grant the user access to Google Cloud Shell.","D":"Deploy Google Cloud Datalab to a virtual machine (VM) on Google Compute Engine.","A":"Run a local version of Jupiter on the laptop.","C":"Host a visualization tool on a VM on Google Compute Engine."},"timestamp":"2020-03-15 03:36:00","question_text":"Your company has hired a new data scientist who wants to perform complicated analyses across very large datasets stored in Google Cloud Storage and in a\nCassandra cluster on Google Compute Engine. The scientist primarily wants to create labelled data sets for machine learning projects, along with some visualization tasks. She reports that her laptop is not powerful enough to perform her tasks and it is slowing her down. You want to help her perform her tasks.\nWhat should you do?","question_images":[],"answer_description":"","answer_images":[],"answers_community":["D (98%)","2%"],"answer":"D","url":"https://www.examtopics.com/discussions/google/view/16624-exam-professional-data-engineer-topic-1-question-22/","discussion":[{"content":"Answer should be D.","timestamp":"1600130160.0","comment_id":"64120","upvote_count":"48","poster":"Rajokkiyam"},{"timestamp":"1601183340.0","content":"Answer: D \nDescription: Datalab provides Jupyter for this kind of work","upvote_count":"14","poster":"[Removed]","comment_id":"68528"},{"poster":"Abizi","timestamp":"1741086780.0","content":"Selected Answer: D\nD is the right answer","comment_id":"1364870","upvote_count":"1"},{"content":"Selected Answer: D\nGoogle Cloud Datalab is a powerful interactive tool for data exploration, analysis, and machine learning.","timestamp":"1732423560.0","upvote_count":"1","poster":"VictorBa","comment_id":"1217191"},{"timestamp":"1731132480.0","comment_id":"1208682","upvote_count":"4","poster":"trashbox","content":"Selected Answer: D\nMy answer is Google Cloud Datalab, but since that service has already been discontinued, I question whether a problem like this would actually be asked on the actual exam."},{"content":"Selected Answer: D\nD sounds good for me","poster":"GCanteiro","timestamp":"1721210700.0","upvote_count":"1","comment_id":"1124928"},{"poster":"TVH_Data_Engineer","comments":[{"content":"You mistakenly answered the question above haha","poster":"simpa17","upvote_count":"1","comment_id":"1356827","timestamp":"1739620800.0"}],"upvote_count":"1","comment_id":"1096332","timestamp":"1718353320.0","content":"Selected Answer: A\nHash Value for Deduplication: By computing a hash value for each data entry, you create a unique identifier based on the content of the data. This allows you to efficiently identify duplicates, as entries with identical content will have the same hash value.\n\nStoring Hash Value and Metadata: Maintaining a database table that includes the hash value and other relevant metadata (like the timestamp of transmission) allows for quick lookups and comparisons. This way, when new data is received, you can check if an entry with the same hash value already exists.\n Assign global unique identifiers (GUID) to each data entry: While GUIDs are unique, they do not inherently identify duplicate content. Two transmissions of the same data would have different GUIDs."},{"timestamp":"1716292260.0","comment_id":"1076326","upvote_count":"1","poster":"axantroff","content":"Selected Answer: D\nD sounds good for me"},{"timestamp":"1715111460.0","poster":"RT_G","comment_id":"1065187","content":"Selected Answer: D\nAgree with D","upvote_count":"1"},{"poster":"rtcpost","content":"Selected Answer: D\nD. Deploy Google Cloud Datalab to a virtual machine (VM) on Google Compute Engine.\n\nGoogle Cloud Datalab is a powerful interactive tool for data exploration, analysis, and machine learning. By deploying it to a VM on Google Compute Engine, you can provide her with a robust and scalable environment where she can work with large datasets, create labeled datasets, and perform data analyses efficiently.\n\nOption A (running a local version of Jupyter on her laptop) might not be sufficient for very large datasets, and her laptop's limited power could still be a bottleneck.\n\nOption B (granting access to Google Cloud Shell) is useful for running command-line tools but may not provide the interactive and visual capabilities she needs.\n\nOption C (hosting a visualization tool on a VM on Google Compute Engine) is beneficial for visualization tasks but does not cover the full spectrum of data analysis and machine learning tasks that Google Cloud Datalab offers.","timestamp":"1713787080.0","comment_id":"1050525","upvote_count":"3"},{"comment_id":"999556","poster":"gudguy1a","upvote_count":"1","content":"Selected Answer: D\nD - as it is a FULL set up, not a shell that is needed...","timestamp":"1709652720.0"},{"content":"Nowadays it should be similar to D, deploy a Vertex workbench","timestamp":"1709182440.0","comment_id":"994675","upvote_count":"2","poster":"sergiomujica"},{"timestamp":"1708245900.0","content":"As per Options , Correct Answer should be D. ie Datalab\nHowever Datalab is no longer used in GCP (Deprecated in Sep2022), It is Vertex AI or Deep Learning VM Images","comment_id":"984230","upvote_count":"1","poster":"yash12"},{"poster":"HeoMaTo","content":"Selected Answer: D\nI think.\nAnswer is D","timestamp":"1708237140.0","upvote_count":"1","comment_id":"984170"},{"content":"Datalab is deprecated. This question should appear in the exam.","poster":"Acocado","upvote_count":"2","timestamp":"1705992960.0","comment_id":"960075","comments":[{"content":"typo- should NOT appear in the exam","upvote_count":"6","comment_id":"960076","comments":[{"upvote_count":"1","timestamp":"1714412640.0","content":"Good point - https://cloud.google.com/datalab/deprecation-notice. Google recommends using Vertex AI Workbench instead","poster":"axantroff","comment_id":"1057110"}],"timestamp":"1705992960.0","poster":"Acocado"}]},{"timestamp":"1702129440.0","content":"Selected Answer: D\nShould be D, because Cloud shell alone does not provide access to what they need.\nNowadays is Vertex AI, but still, correct answer is D","comment_id":"919251","poster":"dgteixeira","upvote_count":"3"},{"timestamp":"1701302520.0","comment_id":"909750","content":"Selected Answer: D\nGoogle Cloud Datalab is now Vertex AI. So, letter D make more sense.","upvote_count":"3","poster":"Maurilio_Cardoso"},{"comment_id":"886417","timestamp":"1698858360.0","upvote_count":"1","content":"Selected Answer: D\nB doesn't make sense at all","poster":"boca_2022"},{"comment_id":"875147","poster":"abi01a","timestamp":"1697757360.0","upvote_count":"2","content":"Do you ever get clearly wrong answer like this one ever reversed? I dont understand how D the most voted option at 100% not flagged as the correct answer."},{"content":"Selected Answer: D\nD of course","timestamp":"1694971140.0","poster":"juliobs","comment_id":"842228","upvote_count":"1"},{"timestamp":"1694401380.0","upvote_count":"1","content":"Selected Answer: D\nD is correct","comment_id":"835671","poster":"bha11111"},{"upvote_count":"1","content":"Selected Answer: D\nB is not relevant at all","comment_id":"787699","poster":"PolyMoe","timestamp":"1690285860.0"},{"timestamp":"1688496840.0","poster":"AzureDP900","upvote_count":"2","content":"D \nCloud Datalab is **packaged as a container and run in a VM (Virtual Machine) instance.**\nVM creation**, running the container in that VM, and establishing a connection from your browser to the Cloud Datalab container, which allows you to open existing Cloud Datalab notebooks and create new notebooks**. Read through the introductory notebooks in the `/docs/intro` directory to get a sense of how a notebook is organized and executed.\n\nCloud Datalab uses **notebooks instead of the text files containing code. Notebooks bring together code, documentation written as markdown, and the results of code executionâ€”whether as text, image or, HTML/JavaScript**.\n\nReference:\nhttps://cloud.google.com/datalab/docs/quickstarts","comment_id":"766079"},{"timestamp":"1686605040.0","poster":"DGames","upvote_count":"2","content":"Selected Answer: D\nAnswer - D, Its clear mention Data Scientist have task of visualization. that possible in Datalab","comment_id":"743413"},{"poster":"Atnafu","content":"D\nDatalab before it get deprecated now Vertex AI\nhttps://cloud.google.com/datalab/docs#:~:text=Datalab%20is%20deprecated,Google%20Charts%20APIs","upvote_count":"7","comment_id":"726163","timestamp":"1684951980.0"},{"comment_id":"719785","content":"Selected Answer: D\nThe scientist primarily wants to create labelled data sets for machine learning projects, along with some visualization tasks.\n\nDefinitely not the Shell. \nAnd its Vertex AI Workbench now.","poster":"assU2","upvote_count":"6","timestamp":"1684243380.0"},{"upvote_count":"3","poster":"gudiking","comment_id":"719621","timestamp":"1684233420.0","content":"Selected Answer: D\nD because of visualization tasks, they can't be done in the shell, right?"},{"timestamp":"1684050180.0","upvote_count":"1","content":"How in the hell isn't this D?","comment_id":"717860","poster":"wubston"},{"timestamp":"1676910840.0","content":"Answer: D","poster":"crisimenjivar","upvote_count":"1","comment_id":"649464"},{"comment_id":"646459","timestamp":"1676336880.0","poster":"Megmang","content":"Selected Answer: D\nDatalab is the solution for this","upvote_count":"1"},{"poster":"nexus1_","timestamp":"1670872140.0","upvote_count":"1","content":"Selected Answer: D\nAnswer is D","comment_id":"615437"},{"timestamp":"1666264440.0","upvote_count":"2","poster":"alecuba16","content":"Selected Answer: D\nCloud Datalab -> AI Notebooks -> Vertex AI Workbench","comment_id":"588582"},{"upvote_count":"1","timestamp":"1662113700.0","poster":"MichaelOcto","content":"Selected Answer: D\nDatalab was made for this","comment_id":"559391"},{"poster":"davidqianwen","upvote_count":"1","content":"Selected Answer: D\nAnswer should be D.","timestamp":"1658576100.0","comment_id":"530558"},{"poster":"samdhimal","content":"correct answer -> Deploy Google Cloud Datalab to a virtual machine (VM) on Google Compute Engine.\n\nGoogle says:\nUse Cloud Datalab to easily explore, visualize, analyze, and transform data using familiar languages, such as Python and SQL, interactively.\nReference: https://cloud.google.com/datalab/docs\n***This is exactly the tool needed in this scenario***\nBut what about her laptop slowing her down?\nWell, Cloud Datalab is packaged as a container and run in a VM (Virtual Machine) instance.\nReference: https://cloud.google.com/datalab/docs/concepts/key-concepts","upvote_count":"5","timestamp":"1658528760.0","comment_id":"530166"},{"content":"Selected Answer: D\nBut there is a new product that evolved from Cloud Datalab\nCloud Datalab -> AI Notebooks -> Vertex AI Workbench","upvote_count":"5","timestamp":"1656926880.0","comment_id":"516522","poster":"medeis_jar"},{"timestamp":"1656514500.0","content":"Selected Answer: D\nAnswer should be D.","poster":"Jlozano","upvote_count":"1","comment_id":"512458"},{"timestamp":"1655430420.0","upvote_count":"3","comment_id":"503336","poster":"hendrixlives","content":"Selected Answer: D\nD: what is being described from the requirements is literally a Notebook (either the old Datalab, or the new AI Notebooks, or Dataproc HUB/Extended/Notebooks or a Jupyter notebook in GKE Hub Extended...\n\n...hey Google, how many notebooks options do we really need?"},{"timestamp":"1654321200.0","poster":"BigQuery","upvote_count":"3","comment_id":"493562","content":"Selected Answer: D\nD is the Correct answer But, a thing to note here is there are two products offer the ipynb functionality. one is DataLab another is AI Notebooks. \n\nAI Notebooks is correct answer. Google Cloud AI Platform Notebooks is effectively the upgraded version of Google Cloud Datalab and gives you benefits like being able to use the notebook directly in your browser without having to setup an ssh tunnel first (which datalab forces you to do).\n\nLink - https://stackoverflow.com/questions/58031608/what-is-the-difference-between-google-cloud-datalab-and-google-cloud-ai-platform#:~:text=Google%20Cloud%20AI%20Platform%20Notebooks%20is%20effectively%20the%20upgraded%20version,datalab%20forces%20you%20to%20do)."},{"poster":"MaxNRG","timestamp":"1652188200.0","comment_id":"475590","comments":[{"upvote_count":"1","comment_id":"475592","content":"Cloud Datalab is built on Jupyter (formerly IPython), which boasts a thriving ecosystem of modules and a robust knowledge base. Cloud Datalab enables analysis of your data on Google BigQuery, Cloud Machine Learning Engine, Google Compute Engine, and Google Cloud Storage using Python, SQL, and JavaScript (for BigQuery user-defined functions).\nWhether you're analyzing megabytes or terabytes, Cloud Datalab has you covered. Query terabytes of data in BigQuery, run local analysis on sampled data and run training jobs on terabytes of data in Cloud Machine Learning Engine seamlessly.\nUse Cloud Datalab to gain insight from your data. Interactively explore, transform, analyze, and visualize your data using BigQuery, Cloud Storage and Python.\nGo from data to deployed machine-learning (ML) models ready for prediction. Explore data, build, evaluate and optimize Machine Learning models using TensorFlow or Cloud Machine Learning Engine.\nOptions A, B & C do not provide all the abilities.","poster":"MaxNRG","timestamp":"1652188200.0"}],"content":"D as Cloud Datalab provides a powerful interactive, scalable tool on Google Cloud with the ability to analyze, visualize data.\nhttps://cloud.google.com/datalab/\nCloud Datalab is a powerful interactive tool created to explore, analyze, transform and visualize data and build machine learning models on Google Cloud Platform. It runs on Google Compute Engine and connects to multiple cloud sendees easily so you can focus on your data science tasks.","upvote_count":"2"},{"comment_id":"462682","poster":"anji007","content":"Ans: D\nAt first even I thought 'B' could be correct answer after looking at the documentation I concluded that Datalab is best option. \n\nThis quickstart page shows creating Datalab instance and connecting to it via web browser which solves major issue here, physical machine capacity. This way can shift load on physical machine (Laptop) on to Cloud.\nhttps://cloud.google.com/datalab/docs/quickstart\n\nAlso, from google docs:\nCloud Datalab is an interactive data analysis and machine learning environment designed for Google Cloud Platform. You can use it to explore, analyze, transform, and visualize your data interactively and to build machine learning models from your data.\n\nAdditionally check this link for usage scenarios.\nhttps://cloud.google.com/datalab/docs/concepts/key-concepts#cloud_datalab_usage_scenarios\n\nFinally, its Data scientist who is working on it so can not expect them doing much on technical side (at least on Cloud side).","timestamp":"1650037560.0","upvote_count":"1"},{"content":"The answer really should be Cloud AI Notebooks","timestamp":"1646659740.0","poster":"michaelkhan3","comment_id":"440879","upvote_count":"3"},{"timestamp":"1645648500.0","poster":"asksathvik","content":"## Cloud Datalab and notebooks\n\nCloud Datalab is **packaged as a container and run in a VM (Virtual Machine) instance.** The [quickstart](https://cloud.google.com/datalab/docs/quickstarts) explains VM creation**, running the container in that VM, and establishing a connection from your browser to the Cloud Datalab container, which allows you to open existing Cloud Datalab notebooks and create new notebooks**. Read through the introductory notebooks in the `/docs/intro` directory to get a sense of how a notebook is organized and executed.\n\nCloud Datalab uses **notebooks instead of the text files containing code. Notebooks bring together code, documentation written as markdown, and the results of code executionâ€”whether as text, image or, HTML/JavaScript**.","upvote_count":"1","comment_id":"430261"},{"timestamp":"1645112460.0","content":"Should be D","upvote_count":"1","poster":"sandipk91","comment_id":"426364"},{"content":"It says about computing power - so it can't be Cloud Shell which has limited compute power.","timestamp":"1645030500.0","upvote_count":"1","comment_id":"425856","poster":"fire558787"},{"poster":"CuchoLeo","content":"is D: Page 200 Ch8 \"Users of Cloud Datalab create a Compute engine instance \"\"datalab create --machine-type n1-highmem my-datalab-instance-1\" and she needs to visualice the data.","upvote_count":"1","timestamp":"1644694500.0","comment_id":"423782"},{"timestamp":"1640526540.0","poster":"sumanshu","content":"Vote for 'D\"","comment_id":"391189","upvote_count":"1"},{"timestamp":"1640037360.0","comment_id":"386560","poster":"moonlightbeamer","upvote_count":"1","content":"Answer: D\nwhat makes you think cloud shell VM (choice B) is more powerful than her laptop?"},{"content":"The correct answer to this question is B. Here is why:-\nOption A : Not correct, running a local version of Jupiter will not help her to work with the existing Casandra DB hosted in a VM in Google Cloud\nOption B : This is the correct answer, you can access google cloud SHELL either by installing local SDK's or you can access through Web. Go to the console page, login with your credential and access cloud shell. Cloud shell will allow you to create a Data Studio for visualization or any other machine learning tools (Cloud AI API) - all through command lines\nOption C- Not correct, does not specify which visualization tool - its too generic\nOption D - Not correct, Google cloud Data Lab is a managed service, you dont need a VM (Compute instance) to load Data Studio","comment_id":"306089","poster":"daghayeghi","comments":[{"upvote_count":"1","content":"datalab create [--machine-type MACHINE_TYPE]","comment_id":"433440","timestamp":"1646018520.0","poster":"manocha_01887"},{"poster":"dem2021","upvote_count":"10","content":"Datalab runs as a container inside a VM !! \nhttps://cloud.google.com/datalab/docs/concepts/key-concepts","comment_id":"355574","timestamp":"1636735560.0"},{"timestamp":"1646737500.0","upvote_count":"2","comment_id":"441344","poster":"StefanoG","content":"Finally a clear answer!"},{"comment_id":"475589","content":"Data Lab is not a managed service!\nhttps://cloud.google.com/datalab/docs/how-to/machine-type","timestamp":"1652188140.0","upvote_count":"2","poster":"MaxNRG"}],"upvote_count":"7","timestamp":"1631146500.0"},{"upvote_count":"1","timestamp":"1628263380.0","poster":"naga","content":"correct D","comment_id":"284995"},{"content":"it says visualisation, the answer should be D.","upvote_count":"1","comment_id":"229087","poster":"ceak","timestamp":"1622119560.0"},{"comment_id":"219800","upvote_count":"1","poster":"xfoursea","timestamp":"1621088520.0","content":"Now you can instantiate a Jupyter notebook on AI Platform. I would pick D which is the closest."},{"upvote_count":"5","poster":"Alasmindas","timestamp":"1619957880.0","comment_id":"211282","content":"The correct answer to this question is B. Here is why:-\nOption A : Not correct, running a local version of Jupiter will not help her to work with the existing Casandra DB hosted in a VM in Google Cloud \nOption B : This is the correct answer, you can access google cloud SHELL either by installing local SDK's or you can access through Web. Go to the console page, login with your credential and access cloud shell. Cloud shell will allow you to create a Data Studio for visualization or any other machine learning tools (Cloud AI API) - all through command lines \nOption C- Not correct, does not specify which visualization tool - its too generic\nOption D - Not correct, Google cloud Data Lab is a managed service, you dont need a VM (Compute instance) to load Data Studio","comments":[{"content":"What Alasmindas is stating about cloud Data Lab is in contradiction to what GCP states here: https://cloud.google.com/datalab/docs/concepts/key-concepts","upvote_count":"2","poster":"kdiab","timestamp":"1628507820.0","comment_id":"286827"}]},{"upvote_count":"6","poster":"Alasmindas","content":"The correct answer to this question is B. Here is why:-\nOption A : Not correct, running a local version of Jupiter will not help her to work with the existing Casandra DB hosted in a VM in Google Cloud \nOption B : This is the correct answer, you can access google cloud SHELL either by installing local SDK's or you can access through Web. Go to the console page, login with your credential and access cloud shell. Cloud shell will allow you to create a Data Studio for visualization or any other machine learning tools (Cloud AI API) - all through command lines \nOption C- Not correct, does not specify which visualization tool - its too generic\nOption D - Not correct, Google cloud Data Lab is a managed service, you dont need a VM (Compute instance) to load Data Studio","timestamp":"1619957700.0","comment_id":"211280"},{"content":"Sorry the Answer should be D , using Datalab she can analyze and Visualize the data as well","timestamp":"1617770400.0","poster":"Tanmoyk","upvote_count":"1","comment_id":"194848"},{"content":"Answer should be D. Cloud shell is provided for normal admin work and has limited capacity. Having a dedicated machine with the right tool (Datalab) should serve the purpose.","poster":"SteelWarrior","timestamp":"1615979700.0","comment_id":"180797","upvote_count":"2"},{"content":"Problem statement.\nãƒ» She reports that her laptop is not powerful enough to perform her tasks and it is slowing her down.\nãƒ» You want to help her perform her tasks.\n------------------------------------------------------------------------https://console.cloud.google.com/marketplace/details/click-to-deploy-images/cassandra?filter=category%3Adatabase&id=25ca0967-cd8e-419e-b554-fe32e87f04be&walkthrough_tutorial_id=java_gae_quickstart\n------------------------------------------------------------------------\nBuild and run your \"Hello, world!\" App\n\nYou will learn how to run your app using Cloud Shell, right in your browser. At the end, you'll deploy your app to the web using the App Engine Maven plugin.\n------------------------------------------------------------------------\nTherefore, B is the correct answer.","poster":"kino2020","timestamp":"1615526460.0","comments":[{"timestamp":"1615527120.0","comment_id":"178007","upvote_count":"1","poster":"kino2020","content":"Cassandra DB has already been built.\nShe states that she is doing it on her laptop.\nHer request is to work as a data scientist on a laptop.\nThere was a cassandra page on the Google Plat Form, so when I looked at it, it said, \"Use Cloud Shell.\"\nIf there is no description of laptop and data scientist, it will be D, but if the above points are improvement requests, it will be B."}],"comment_id":"178003","upvote_count":"5"},{"comment_id":"172262","poster":"AniruddhaSinha","content":"Seems D is the correct ans, but many people are saying B. Can someone share some proofs against B please ?","timestamp":"1614730680.0","upvote_count":"1"},{"poster":"Tanmoyk","content":"B seems most suitable answer having Cloud shell access the user can perform the required tasks","upvote_count":"2","timestamp":"1614493620.0","comment_id":"170536"},{"content":"B is correct answer here as in order to deploy dataloab you need cloud shell access","timestamp":"1613592480.0","poster":"saurabh1805","upvote_count":"3","comment_id":"160261"},{"timestamp":"1610026320.0","comment_id":"128865","content":"B - using datalab command a instance could be created\nNot D - Catch is the word \"Deploy Datalab to VM\" that's not required datalab creates and use VM machine as intance details provided","upvote_count":"6","poster":"saurabhsingh92"},{"comment_id":"126116","poster":"dg63","comments":[{"upvote_count":"1","timestamp":"1625264160.0","poster":"Alexej_123","comment_id":"258026","content":"I don't think that B is correct. You can not create VM just because you have access to cloud shell. Additional permissions will be required, which are not automatically assigned through B."}],"content":"\"B\" is correct. You just need to use command \"datalab create my-datalab\" to create a datalab instance.","upvote_count":"6","timestamp":"1609767120.0"},{"upvote_count":"1","comment_id":"125949","timestamp":"1609741800.0","content":"Answer is D.","poster":"Rajuuu"},{"poster":"AJKumar","timestamp":"1608721380.0","comment_id":"117257","content":"Question asks for creation of labeled data and some visualization - Datalab offers both.","upvote_count":"1"},{"poster":"arnabbis4u","upvote_count":"2","comment_id":"86702","timestamp":"1605041880.0","content":"Answer D"},{"comment_id":"65682","poster":"[Removed]","timestamp":"1600436220.0","upvote_count":"7","content":"Answer: D"},{"comment_id":"65075","timestamp":"1600320540.0","upvote_count":"5","poster":"Snobid","content":"How does B help answer the question?"},{"poster":"rickywck","comment_id":"64981","upvote_count":"7","timestamp":"1600303500.0","content":"Obviously the answer is D"}],"isMC":true,"exam_id":11,"topic":"1","unix_timestamp":1584239760,"answer_ET":"D","question_id":135}],"exam":{"provider":"Google","isBeta":false,"isMCOnly":true,"lastUpdated":"11 Apr 2025","isImplemented":true,"numberOfQuestions":319,"name":"Professional Data Engineer","id":11},"currentPage":27},"__N_SSP":true}