{"pageProps":{"questions":[{"id":"2cw4DgOSmFnlw9QZ82Lt","isMC":true,"answer_ET":"B","exam_id":11,"topic":"1","question_images":[],"answers_community":["B (76%)","14%","10%"],"timestamp":"2024-01-03 13:55:00","answer_images":[],"answer":"B","choices":{"A":"Increase the slot capacity of the project with baseline as 0 and maximum reservation size as 3000.","C":"Increase the slot capacity of the project with baseline as 2000 and maximum reservation size as 3000.","D":"Update SQL pipelines and ad-hoc queries to run as interactive query jobs.","B":"Update SQL pipelines to run as a batch query, and run ad-hoc queries as interactive query jobs."},"discussion":[{"timestamp":"1704400620.0","poster":"raaad","content":"Selected Answer: B\n- BigQuery allows you to specify job priority as either BATCH or INTERACTIVE. \n- Batch queries are queued and then started when idle resources are available, making them suitable for non-time-sensitive workloads. \n- Running ad-hoc queries as interactive ensures they have prompt access to resources.","comment_id":"1114054","upvote_count":"9"},{"poster":"LP_PDE","timestamp":"1737917100.0","upvote_count":"1","content":"Selected Answer: B\nBy updating your SQL pipelines to run as batch queries you can reduce concurrency, avoid quota errors, and ensure that your analysts have the resources they need for their interactive queries.","comment_id":"1347069"},{"upvote_count":"2","content":"Selected Answer: B\nThis question has nothing to do with increasing slots, it is just confusing and misleading, therefore A and C do not make sense. \nD (All interactive queries): Running all queries as interactive would prioritize speed over cost-efficiency and might not be necessary for your non-time-sensitive SQL pipelines.","timestamp":"1730405520.0","comment_id":"1305579","poster":"ToiToi"},{"upvote_count":"3","content":"Selected Answer: C\nYou already have a 2000 slots consumption and sudden peaks, so you should use a baseline of 2000 slots and a maximum of 3000 to tackle the peak concurrent activity.\nhttps://cloud.google.com/bigquery/docs/slots-autoscaling-intro","poster":"josech","timestamp":"1716071820.0","comment_id":"1213500"},{"comment_id":"1191250","content":"Selected Answer: A\nWhy A is the best choice:\n\nAddresses Concurrency: Increasing the maximum reservation size to 3000 slots directly addresses the concurrency issue by providing more capacity for simultaneous queries. Since the current peak usage is 1500 queries, this increase ensures sufficient headroom.\nCost Optimization: Setting the baseline to 0 means you only pay for the slots actually used, avoiding unnecessary costs for idle capacity. This is ideal for non-time-sensitive workloads where flexibility is more important than guaranteed instant availability.","poster":"CGS22","upvote_count":"2","timestamp":"1712532060.0"},{"upvote_count":"2","timestamp":"1708396440.0","comments":[],"poster":"JyoGCP","content":"Selected Answer: B\nOption B","comment_id":"1154434"},{"comment_id":"1112760","content":"Selected Answer: B\nB. \nUpdate SQL pipelines to run as a batch query, and run ad-hoc queries as interactive query jobs.","poster":"scaenruy","timestamp":"1704286500.0","upvote_count":"2"}],"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/130182-exam-professional-data-engineer-topic-1-question-239/","question_text":"The data analyst team at your company uses BigQuery for ad-hoc queries and scheduled SQL pipelines in a Google Cloud project with a slot reservation of 2000 slots. However, with the recent introduction of hundreds of new non time-sensitive SQL pipelines, the team is encountering frequent quota errors. You examine the logs and notice that approximately 1500 queries are being triggered concurrently during peak time. You need to resolve the concurrency issue. What should you do?","unix_timestamp":1704286500,"question_id":156},{"id":"nklfYjK4FrMKrzdAP7C9","unix_timestamp":1583950380,"timestamp":"2020-03-11 19:13:00","exam_id":11,"url":"https://www.examtopics.com/discussions/google/view/16285-exam-professional-data-engineer-topic-1-question-24/","question_text":"You have spent a few days loading data from comma-separated values (CSV) files into the Google BigQuery table CLICK_STREAM. The column DT stores the epoch time of click events. For convenience, you chose a simple schema where every field is treated as the STRING type. Now, you want to compute web session durations of users who visit your site, and you want to change its data type to the TIMESTAMP. You want to minimize the migration effort without making future queries computationally expensive. What should you do?","answer_description":"","answer_ET":"E","topic":"1","choices":{"D":"Add two columns to the table CLICK STREAM: TS of the TIMESTAMP type and IS_NEW of the BOOLEAN type. Reload all data in append mode. For each appended row, set the value of IS_NEW to true. For future queries, reference the column TS instead of the column DT, with the WHERE clause ensuring that the value of IS_NEW must be true.","A":"Delete the table CLICK_STREAM, and then re-create it such that the column DT is of the TIMESTAMP type. Reload the data.","C":"Create a view CLICK_STREAM_V, where strings from the column DT are cast into TIMESTAMP values. Reference the view CLICK_STREAM_V instead of the table CLICK_STREAM from now on.","E":"Construct a query to return every row of the table CLICK_STREAM, while using the built-in function to cast strings from the column DT into TIMESTAMP values. Run the query into a destination table NEW_CLICK_STREAM, in which the column TS is the TIMESTAMP type. Reference the table NEW_CLICK_STREAM instead of the table CLICK_STREAM from now on. In the future, new data is loaded into the table NEW_CLICK_STREAM.","B":"Add a column TS of the TIMESTAMP type to the table CLICK_STREAM, and populate the numeric values from the column TS for each row. Reference the column TS instead of the column DT from now on."},"question_id":157,"answer":"E","answers_community":["E (56%)","C (41%)","2%"],"isMC":true,"answer_images":[],"question_images":[],"discussion":[{"comments":[{"timestamp":"1671118980.0","upvote_count":"5","content":"Also D doesn't make sense since we're filtering IS_NEW to true to only consider future data, which disregards our previously loaded data","comment_id":"746264","poster":"jkhong"},{"content":"\"You want to minimize the migration effort without making future queries computationally expensive.\" Nothing about storage price.","comment_id":"720354","poster":"assU2","timestamp":"1668676680.0","upvote_count":"4"}],"content":"\"E\" looks better. For D, the database will be double in size (which increases the storage price) and the user has to spend some more days reloading all the data.","timestamp":"1583950380.0","upvote_count":"31","poster":"jvg637","comment_id":"62602"},{"timestamp":"1584610440.0","content":"E - more simple and reasonable. Also recommended if not concerned about cost but simplicity. \nhttps://cloud.google.com/bigquery/docs/manually-changing-schemas#changing_a_columns_data_type","poster":"[Removed]","comment_id":"65938","upvote_count":"22","comments":[{"poster":"Tanzu","content":"Due to the hard limitations of bq, Not E is the simple answer by the way!","upvote_count":"1","timestamp":"1642937640.0","comment_id":"530477"}]},{"comment_id":"1398866","poster":"Parandhaman_Margan","upvote_count":"1","timestamp":"1742047860.0","content":"Selected Answer: C\nChanging DT to TIMESTAMP. Create a view casting the string. Answer C."},{"poster":"Abizi","timestamp":"1741089300.0","upvote_count":"1","content":"Selected Answer: E\nI was hesitating between C and E, but E seems to be the good one","comment_id":"1364882"},{"upvote_count":"1","poster":"LP_PDE","content":"Selected Answer: C\nBoth options C (creating a view) and E (creating a new table) avoid reloading the original data. However, C is the better choice for minimizing effort and maintaining performance. Views don't store any data themselves. They simply act as a layer on top of the existing table. This means you avoid the cost of storing duplicate data, which can be significant for large tables.","comment_id":"1346631","timestamp":"1737834960.0"},{"comment_id":"1262397","upvote_count":"3","poster":"Nittin","content":"Selected Answer: C\nCreate a view no data migration easy to do but computational efficient queries not sure (?)","timestamp":"1723103640.0"},{"content":"E. It recreates the table one time and everything is fixed. Next time you load, load to the new table, you can delete the previous one.\nDefinitely not C. The question says I have to minimize future query effort, which literally means \"don't create a view that converts from STR to TIMESTAMP for every row.\"","poster":"mark1223jkh","upvote_count":"1","timestamp":"1715920080.0","comment_id":"1212693"},{"content":"Selected Answer: E\nOption E is correct.\nThe question is asking to consider the Query cost for future.\nThis is a one time job to fix the Timestamp column. no views were created.","poster":"suwalsageen12","comment_id":"1212407","upvote_count":"1","timestamp":"1715860920.0"},{"comment_id":"1200194","upvote_count":"1","timestamp":"1713789840.0","poster":"teka112233","content":"Selected Answer: E\nWhy Option E is the best choice:\n\nIt modifies the schema with minimal data movement.\nThe original table remains untouched for potential future needs.\nFuture data loads can directly go to the new table with the desired schema.\nQueries referencing the new table (NEW_CLICK_STREAM) will benefit from the optimized data type for timestamp operations."},{"comment_id":"1171550","upvote_count":"1","content":"Selected Answer: C\nminimizing effort is key.","poster":"GYORK","timestamp":"1710232740.0"},{"timestamp":"1702549500.0","content":"Selected Answer: C\nA view in Google BigQuery is a virtual table defined by a SQL query. By creating a view that casts the DT column as a TIMESTAMP, you can transform the data format without altering the underlying data in the CLICK_STREAM table. This means you don't have to reload any data, thereby minimizing migration effort.","comments":[{"upvote_count":"1","comment_id":"1324708","poster":"apoio.certificacoes.closer","content":"Depends. If you create a materialized view, that tracks. If it's not a materialized view, the underlying query will run every time there's a query against the view.","timestamp":"1733856540.0"}],"comment_id":"1096342","upvote_count":"4","poster":"TVH_Data_Engineer"},{"content":"Selected Answer: E\nGood point about the logical views and the desire to reduce costs. I would vote for E","upvote_count":"1","timestamp":"1700575380.0","comment_id":"1076342","poster":"axantroff"},{"timestamp":"1698180180.0","comment_id":"1053158","comments":[{"content":"C doesn't say materialized view, there's a difference with a regular view so it'll be slower and more expensive on every call to that view.","upvote_count":"1","timestamp":"1699552800.0","poster":"brokeasspanda","comment_id":"1066588"}],"content":"The best way to minimize the migration effort without making future queries computationally expensive is to create a view and reference it instead of the table. This is because views are materialized when they are queried, so they do not incur any additional overhead.\nSo the answer is (C).","upvote_count":"1","poster":"mk_choudhary"},{"upvote_count":"2","content":"Selected Answer: E\nOption \"E\"\nIt avoids the need to delete and recreate the entire CLICK_STREAM table, which is time-consuming and requires reloading all data.\n\nIt allows you to use a simple query to cast the existing DT column as TIMESTAMP values and store the results in a new table, NEW_CLICK_STREAM.\n\nYou can gradually migrate to the new data format, and your future queries will be able to utilize the TIMESTAMP data type for more efficient processing.","timestamp":"1697976120.0","poster":"rtcpost","comment_id":"1050528"},{"comment_id":"994692","timestamp":"1693450680.0","poster":"sergiomujica","content":"Option D duplicates, not a good solution","upvote_count":"1"},{"upvote_count":"1","timestamp":"1690690260.0","comment_id":"966838","poster":"NeoNitin","content":"E. E. You can use a special command to change the time on the old cards to the better type \"TIMESTAMP\" and create a new box called \"NEW_CLICK_STREAM.\" From now on, you'll look at the new box whenever you want to know the time. It's like having a new and better box to keep things tidy and organized.\n\nSo, the best way to change the time on the little cards to the better type \"TIMESTAMP\" is option E. It's like using magic to create a new box and making sure everything is still easy to find and work with. It's a clever way to keep track of time and make your website even better!"},{"content":"Selected Answer: E\nthey asked to \"change its data type\"","timestamp":"1686729900.0","comment_id":"922874","poster":"tal_","upvote_count":"1"},{"content":"Selected Answer: E\nit's E. computationally less expensive than running a view every time","poster":"momosoundz","upvote_count":"1","timestamp":"1683470760.0","comment_id":"891480"},{"upvote_count":"1","poster":"boca_2022","content":"Selected Answer: E\nE is best option","comment_id":"886428","timestamp":"1682954100.0"},{"comment_id":"814216","upvote_count":"5","poster":"techtitan","content":"Selected Answer: E\nits C vs E. E is better because C will try to do a cast operation everytime query is run making it computationally expensive.","timestamp":"1676820060.0"},{"comment_id":"760098","comments":[{"content":"That was my thought initially but note “making future queries not computationally expensive “ so creating a view you will need to always parse the value because of that I would go with E","timestamp":"1683922440.0","upvote_count":"2","comment_id":"896225","poster":"Kiroo"},{"comment_id":"1041968","timestamp":"1697131680.0","upvote_count":"2","poster":"Simhamed2015","content":"BigQuery's views are logical views, not materialized views. Because views are not materialized, the query that defines the view is run each time the view is queried. Queries are billed according to the total amount of data in all table fields referenced directly or indirectly by the top-level query. For more information, see query pricing.\n\nB would be very expensive, since the we will be charged each time the view is queried."}],"upvote_count":"4","content":"Selected Answer: C\noption E - includes already effort for C (building query with cast ) however adding steps on rebuilding the table with magic word \"NEW\" ( how often you have NEW tables on your side - have you wonder when NEW becomes OLD ? not to mentioned that it kept previous table as well)\nA - not consider to \"reload\" which took few days .\nD - not consider to \"reload\" which took few days ... and have duplicates\nB - almost good one but why kept having two columns with same data\n\nquestion was about minimal effort on migration - no ideal answer exists on this list (no cleanup etc)\nso I vote on C","poster":"Jackalski","timestamp":"1672245780.0"},{"comment_id":"743753","upvote_count":"1","poster":"Nirca","timestamp":"1670920560.0","content":"Selected Answer: E\nWe are dealing here with \"comma-separated values (CSV)\" not \"application\" that sensitive to SQL fix. E is the best way to implement stand-alone use case like this. For applications that are sensitive for DDL & DML - another method might be useful"},{"timestamp":"1670888940.0","poster":"DGames","content":"Selected Answer: D\nD- Make more sense because then E because we are create new table again need to over head to update everywhere table name in application","upvote_count":"1","comment_id":"743425"},{"comment_id":"736842","poster":"odacir","timestamp":"1670332920.0","content":"Selected Answer: E\nIf I do this in my work, E is my answer 100% Otherwise is to complicate.","upvote_count":"1"},{"timestamp":"1667315400.0","comments":[{"comment_id":"711429","timestamp":"1667603100.0","content":"We are interested in query performance (\"without making future queries computationally expensive\") so a common view (not materialized) will have the computational cost of performing the cast every time it is queried.","poster":"NicolasN","upvote_count":"6"}],"comment_id":"709236","poster":"beowulf_kat","upvote_count":"3","content":"Selected Answer: C\nC - as there are no structural changes to the original table, and no increase in storage/compute costs are there. whenever analysis is required, the view with casted datatypes will suffice."},{"comment_id":"625047","upvote_count":"4","timestamp":"1656567720.0","poster":"KundanK973","content":"Selected Answer: E\nE is the best way to implement it.","comments":[{"comment_id":"709243","upvote_count":"1","comments":[{"upvote_count":"1","content":"Storage cost isn't a factor for this question. And even if it was, since \"In the future, new data is loaded into the table NEW_CLICK_STREAM\" the table CLICK_STREAM could be deleted. The DROP TABLE statement is free.","poster":"NicolasN","comment_id":"711432","timestamp":"1667603520.0"}],"timestamp":"1667315640.0","poster":"beowulf_kat","content":"The problem with E is, the new table will have to be re-created each time the original table is updated. It also increases storage costs as it is basically a replica with changed data types. I think C makes more sense hence."}]},{"timestamp":"1656227520.0","content":"D should be the correct answer. E is not because it is mentioned in the question that we have do less migration effort and D also supports SCD TYPE 2.","comment_id":"622392","poster":"thapliyal","upvote_count":"1"},{"content":"none of these, i would use cloud dataprep which is perfect for the task.","comment_id":"603287","timestamp":"1652877480.0","poster":"JimJam1","upvote_count":"3"},{"upvote_count":"4","content":"E : Export and load data into a new table\nYou can also change a column's data type by exporting your table data to Cloud Storage, and then loading the data into a new table with a schema definition that specifies the correct data type for the column. You can also use the load job to overwrite the existing table.\n\nAdvantages\nYou are not charged for the export job or the load job. Currently, BigQuery load and export jobs are free.\nIf you use the load job to overwrite the original table, you incur storage costs for one table instead of two, but you lose the original data.\nDisadvantages\nIf you load the data into a new table, you incur storage costs for the original table and the new table (unless you delete the old one).\nYou incur costs for storing the exported data in Cloud Storage.","comment_id":"584141","timestamp":"1649672100.0","poster":"richardchan66"},{"comment_id":"580915","poster":"devric","timestamp":"1649104980.0","content":"E. D adds processing reading each row.","upvote_count":"1"},{"comment_id":"530510","upvote_count":"6","content":"A does the job. Exporting from bq to gcs is free but storage costs. Sending from computer to bq, again, is ok but it is still hard to eliminate. A few days ... sounds like that way is not possible some how way.\n\nB, adding a column w/o deleting the old one increases storage and computation costs further ops. Not valid\n\nC, creating a view (means source table has to remain) is increasing storage and computation costs, too.\n\nD, adding 2 column is non sense.\n\nSo E is much more sense. More complicated but less computational cost. The tradeoff in bq :)","comments":[{"comment_id":"646415","content":"uhh.. for C, \"creating a view is increasing storage\"? Isn't view just a query and results are not persisted on any disk? i was guessing C to be the right answer.. can anyone help, what am i missing?","comments":[{"upvote_count":"1","comment_id":"712758","content":"C won't increase storage, but C would be computationally expensive, as casting would have to be performed at runtime, each time the view is accessed.","poster":"vetaal","timestamp":"1667787420.0"}],"upvote_count":"5","poster":"AlRaf","timestamp":"1660410060.0"}],"poster":"Tanzu","timestamp":"1642940340.0"},{"poster":"samdhimal","content":"I dont think any of the answer here are correct. Because google says that if you want to Change a column's data type then there are two options: \n1)Using a SQL query: If needed simplicity and ease of use. Also if you are less concerned about costs.\n2)Recreating the table: If More concerned about costs, and you are less concerned about simplicity and ease of use.\n\nWe aren't concern about current cost so we go a head and go with (1) Using a SQL query. This option basically creates a new table with type casted datatype for cols you want as well as there is the other original table. Yes we dont have to currently worry about cost but we do have to worry for future use. If we have two existing tables its costs more money then just having one so we have a conflict here.","timestamp":"1642899720.0","upvote_count":"1","comment_id":"530178","comments":[{"comment_id":"530179","upvote_count":"2","content":"Closest to correct answer is E:\nConstruct a query to return every row of the table CLICK_STREAM, while using the built-in function to cast strings from the column DT into TIMESTAMP values. Run the query into a destination table NEW_CLICK_STREAM, in which the column TS is the TIMESTAMP type. Reference the table NEW_CLICK_STREAM instead of the table CLICK_STREAM from now on. In the future, new data is loaded into the table NEW_CLICK_STREAM.\n\nWith this option there isn't any mention about deleting old table so that you don't have to pay for that table's storage anymore.... Not sure...\n\nReference: https://cloud.google.com/bigquery/docs/manually-changing-schemas#changing_a_columns_data_type","poster":"samdhimal","timestamp":"1642899720.0"}]},{"upvote_count":"1","poster":"Austinaws","content":"E. Column type can not be changed and column needs to casting and loaded into new table using either sql query or import/export features","timestamp":"1642883100.0","comment_id":"530066"},{"poster":"Nico1310","comment_id":"526092","content":"Selected Answer: E\nAnswer is E","upvote_count":"1","timestamp":"1642451580.0"},{"content":"Selected Answer: E\n\"You want to minimize the migration effort without making future queries computationally expensive\". The future queries will be less expensive with the new Table.","upvote_count":"1","comment_id":"524177","timestamp":"1642252320.0","poster":"deep_ROOT"},{"timestamp":"1642103280.0","comment_id":"523047","content":"Selected Answer: E\n@jvg637","upvote_count":"1","poster":"sraakesh95"},{"timestamp":"1636557540.0","upvote_count":"4","poster":"MaxNRG","comment_id":"475601","content":"E as the column type cannot be changed and the column needs to casting loaded into a new table using either SQL Query or import/export.\nRefer GCP documentation - BigQuery Changing Schema: https://cloud.google.com/bigquery/docs/manually-changing-schemas#changing_a_columns_data_type\nA is wrong as with this approach all the data would be lost and needs to be reloaded\nB is wrong as numeric values cannot be used directly and would need casting.\nC is wrong as view is not materialized views, so the future queries would always be taxed as the casting would be done always."},{"timestamp":"1634315400.0","comment_id":"462701","upvote_count":"2","poster":"anji007","content":"Ans: E\nFor me B or E are matching for this problem, but B is talking about numeric values not about casting to TIMESTAMP type.\nA & D: has one thing in common reloading the data, which may require some efforts as the question starts with that loading took some time.\nC: creating view doesn't solve the problem of computational cost."},{"content":"So the goals are \n1. minimize the migration effort - Not re-creating the table and using query for ease of use not for the cost - Answer A is out.\n2. Minimize future query cost : Modify the existing table with the Timestamp datatype not using view over and over, so Answer C is out.\nGoogle recommendation for changing column type is use query for simplicity and ease of use. So the answer is E.","poster":"squishy_fishy","upvote_count":"2","comment_id":"456159","timestamp":"1633192680.0"},{"poster":"maddy5835","upvote_count":"1","timestamp":"1631431020.0","comment_id":"443331","content":"Option B can be ignored because of the option didn't mention using cast function.\nSo from Option D and E, Since we have to reduce the migration effort, as per the bigQuery best practice \"Recreating the table: choose this option if you are more concerned about costs, and you are less concerned about simplicity and ease of use\". So option E can be ignored and D comes out to be the suggested option"},{"timestamp":"1629125940.0","content":"B and E seems OK but there is a \"catch\": I believe you reference a Field in general more often in a Query than you reference a Table. Hence it might be easier to just change references to Tables in your queries rather than to Fields, as you would likely do it fewer times than if you did Fields...","comment_id":"425860","poster":"fire558787","upvote_count":"1"},{"content":"'B' and 'E\" both looks okay.\nbut in 'E\" everything is mention like - CAST function, Future Load, So I will go with 'E'","upvote_count":"2","comment_id":"391237","poster":"sumanshu","timestamp":"1624710300.0"},{"upvote_count":"3","poster":"amarkan","timestamp":"1610371620.0","comment_id":"264749","content":"out of all option E Looks correct to me"},{"timestamp":"1610367000.0","content":"I would go for Option.E\nReason:\n1. Question says, I want to change data type to the TIMESTAMP. So there is change required\n2. To minimize migration effort without making future queries computationally expensive: No view required as I will run this query again and again. In order to convert string to Timestamp we need to use CAST function for sure. Simple copy of numeric value won't work. https://cloud.google.com/bigquery/docs/reference/standard-sql/conversion_rules\nOption-A: I don't want to redo this and spent few days again\nOption-B: Copying numeric value won't work.\nOption-C: View is expensive to run again and again.\nOption-D: To me this seems onetime data load which took 3 days. So, the IS_NEW seems give us reduntant data as we are adding appended mode.\nOption-E: Technically it will work, although it will add more space (new table) ,cost. But no choice, because Option-B didn't mention Cast function. Else I would have gone for \"B\"","comment_id":"264672","poster":"StelSen","upvote_count":"6"},{"content":"Adding a column is now possible, making both B and E viable options. If the table is not used anywhere yet, I'd go for E, otherwise for B. The suggestion solution, D, seems to date from the time where BQ was append-only...","timestamp":"1607015280.0","upvote_count":"3","comment_id":"234186","poster":"IKGx1iGetOWGSjAQDD2x3"},{"upvote_count":"2","poster":"aviassu","comments":[{"upvote_count":"1","comment_id":"456163","content":"I think migration cost is referring to loading the data into the BigQuery not moving data within BigQuery.","poster":"squishy_fishy","timestamp":"1633192920.0"}],"comment_id":"219292","content":"Though option D is listed, the catch in option D is 'Reload all data' which increases migration cost and time. Option E is correct and recommended to create new table and use CAST operation to get the TIMESTAMP value and delete old table later.","timestamp":"1605380640.0"},{"poster":"DeepakKhattar","timestamp":"1604668320.0","content":"E\nOption D can not be right choice becuase requirement categorically mentions \"you want to minimize the migration effort without making future queries computationally expensive. \"\nEvery time we run query , we need to read extra column IS_NEW, that means query is getting expenstive","comment_id":"214083","upvote_count":"2"},{"upvote_count":"3","comment_id":"203420","timestamp":"1603232340.0","poster":"Pupina","content":"I agree that D is true but also A. Why not A? Is another possibility that is also true.","comments":[{"timestamp":"1633192860.0","upvote_count":"1","content":"The concern is the time not the cost, only concern about the cost for the future query cost. So that is why A is out.","comment_id":"456161","poster":"squishy_fishy"}]},{"content":"Answer should be E. Creating a new table from existing table in BigQuery with new transformed column will be simple and will not involve and migration effort. Also future query performance will improve.","upvote_count":"3","timestamp":"1600334400.0","poster":"SteelWarrior","comment_id":"180798"},{"content":"E\nrecommended google practice","poster":"FARR","comment_id":"149721","timestamp":"1596454560.0","upvote_count":"2"},{"content":"Answer D\nOption E would be computationally expensive hence not appropriate\nhttps://cloud.google.com/bigquery/docs/best-practices-performance-compute","upvote_count":"3","timestamp":"1595593200.0","comment_id":"142717","poster":"priyam"},{"content":"Question asks to minimize migration efforts + make future queries computationally not expensive. So the more data we try to reload again, it results in increased migration efforts; if a view is used, it results in more using computational resources.\nPossibly B, as it talks about reloading only one column TS. But confusing thing is the data type it talks about is numeric, which is wrong.\nI may end up using B","comment_id":"133268","comments":[{"upvote_count":"2","poster":"[Removed]","timestamp":"1611387240.0","comment_id":"274386","content":"Absolutely you mirrored what I was thinking..the best answer is B except they given numeric.."}],"upvote_count":"2","poster":"tprashanth","timestamp":"1594589880.0"},{"comment_id":"131574","upvote_count":"4","timestamp":"1594395660.0","content":"The question doesn't ask us to change table schema. also, as it has been mentioned that loading took few days,it will again take those many days to transform and load the data in new table with timestamp schema. using a view saves that effort.So, option c","poster":"Devx198912233"},{"upvote_count":"3","content":"Why not option C? we can simply use a view instead of migrating the data into another table","comments":[{"poster":"atnafu2020","comments":[{"poster":"StefanoG","upvote_count":"1","timestamp":"1631099820.0","comment_id":"441410","content":"And with partitioned tables i can always use a view\nhttps://cloud.google.com/bigquery/docs/querying-partitioned-tables#querying_partitioned_tables_2"},{"comment_id":"441405","poster":"StefanoG","upvote_count":"1","timestamp":"1631099460.0","content":"Hi, I don't understand your POV, with C You create a view on a single table, with same data, where you simply create a cast to convert string to TS.\nWe not have in this scenario a partioned table where the view \"create\" an union of all partition, so i don't see these cost increase."}],"upvote_count":"4","comment_id":"137892","timestamp":"1595075460.0","content":"C is not the answer. BigQuery's views are logical views, not materialized views. Because views are not materialized, the query that defines the view is run each time the view is queried. Queries are billed according to the total amount of data in all table fields referenced directly or indirectly by the top-level query. For more information, see query pricing."}],"comment_id":"131507","timestamp":"1594390740.0","poster":"Devx198912233"},{"comment_id":"86704","poster":"arnabbis4u","content":"Answer E","upvote_count":"4","timestamp":"1589137320.0"},{"content":"https://cloud.google.com/bigquery/docs/manually-changing-schemas#changing_a_columns_data_type, E","timestamp":"1586415540.0","comment_id":"72570","poster":"Nidie","upvote_count":"5"},{"comment_id":"68532","poster":"[Removed]","upvote_count":"11","timestamp":"1585293480.0","content":"Answer: E\nDescription: It’s better to create a new table and delete old one when we are changing the datatype is permanent. View is not suitable because every time the query will run and additional charges will be applied"}]},{"id":"ZRL0Sg61FkGJwiI0ZeEK","unix_timestamp":1704287100,"timestamp":"2024-01-03 14:05:00","answer_ET":"A","isMC":true,"url":"https://www.examtopics.com/discussions/google/view/130183-exam-professional-data-engineer-topic-1-question-240/","question_images":[],"question_id":158,"discussion":[{"upvote_count":"9","timestamp":"1704401400.0","poster":"raaad","comment_id":"1114065","content":"Selected Answer: A\n- dataplex.dataOwner: Grants full control over data assets, including reading, writing, managing, and granting access to others.\n- dataplex.dataReader: Allows users to read data but not modify it.","comments":[{"timestamp":"1705205580.0","comment_id":"1122250","content":"Yes, https://cloud.google.com/dataplex/docs/lake-security#data-roles\nDataplex maps its roles to the data roles for each underlying storage resource (Cloud Storage, BigQuery).\n^ simplify the permissions.","upvote_count":"4","poster":"AllenChen123"}]},{"poster":"josech","comment_id":"1213505","content":"Selected Answer: C\nThe quetion is for BigQuery AND Cloud Storage for a Data Lake, so you should assign IAM permissions for both of them. C is correct.","upvote_count":"1","comments":[{"poster":"[Removed]","upvote_count":"2","comment_id":"1249592","timestamp":"1721219880.0","content":"Dataplex roles are mapped to roles for the underlying resources, like BQ and GCS. So A and C are functionally (almost) equivalent, but A is simpler (2 roles rather than 4). See https://cloud.google.com/dataplex/docs/lake-security#data-roles"}],"timestamp":"1716072420.0"},{"upvote_count":"1","content":"Selected Answer: A\nOption A","comment_id":"1154435","poster":"JyoGCP","timestamp":"1708396440.0"},{"comment_id":"1122595","content":"Selected Answer: A\nA correct answer","timestamp":"1705242360.0","poster":"qq589539483084gfrgrgfr","upvote_count":"3"},{"content":"Selected Answer: A\nOption A clearly correct","comment_id":"1121676","timestamp":"1705151820.0","upvote_count":"2","poster":"Matt_108"},{"content":"Selected Answer: A\nA. \n1. Grant the dataplex.dataOwner role to the data engineer group on the customer data lake.\n2. Grant the dataplex.dataReader role to the analytic user group on the customer curated zone.","poster":"scaenruy","comment_id":"1112770","timestamp":"1704287100.0","upvote_count":"1"}],"exam_id":11,"question_text":"You are designing a data mesh on Google Cloud by using Dataplex to manage data in BigQuery and Cloud Storage. You want to simplify data asset permissions. You are creating a customer virtual lake with two user groups:\n\n• Data engineers, which require full data lake access\n• Analytic users, which require access to curated data\n\nYou need to assign access rights to these two groups. What should you do?","answers_community":["A (94%)","6%"],"topic":"1","choices":{"D":"1. Grant the bigquery.dataViewer role on BigQuery datasets and the storage.objectViewer role on Cloud Storage buckets to data engineers.\n2. Grant the bigquery.dataOwner role on BigQuery datasets and the storage.objectEditor role on Cloud Storage buckets to analytic users.","B":"1. Grant the dataplex.dataReader role to the data engineer group on the customer data lake.\n2. Grant the dataplex.dataOwner to the analytic user group on the customer curated zone.","A":"1. Grant the dataplex.dataOwner role to the data engineer group on the customer data lake.\n2. Grant the dataplex.dataReader role to the analytic user group on the customer curated zone.","C":"1. Grant the bigquery.dataOwner role on BigQuery datasets and the storage.objectCreator role on Cloud Storage buckets to data engineers.\n2. Grant the bigquery.dataViewer role on BigQuery datasets and the storage.objectViewer role on Cloud Storage buckets to analytic users."},"answer_images":[],"answer":"A","answer_description":""},{"id":"XGZD09zCLAV9HUQeVyZW","choices":{"C":"Adopt a dual-region Cloud Storage bucket, and enable turbo replication in your architecture.","A":"Adopt multi-regional Cloud Storage buckets in your architecture.","D":"Adopt two regional Cloud Storage buckets, and create a daily task to copy from one bucket to the other.","B":"Adopt two regional Cloud Storage buckets, and update your application to write the output on both buckets."},"answer":"C","url":"https://www.examtopics.com/discussions/google/view/130184-exam-professional-data-engineer-topic-1-question-241/","answer_images":[],"question_images":[],"unix_timestamp":1704287160,"answer_ET":"C","exam_id":11,"answers_community":["C (81%)","A (19%)"],"topic":"1","isMC":true,"answer_description":"","question_id":159,"timestamp":"2024-01-03 14:06:00","discussion":[{"content":"Selected Answer: C\n- Dual-region buckets are a specific type of storage that automatically replicates data between two geographically distinct regions. \n- Turbo replication is an enhanced feature that provides faster replication between the two regions, thus minimizing RPO. \n- This option ensures that your data is resilient to regional failures and is replicated quickly, meeting the needs for low RPO and no impact on application performance.","comment_id":"1114072","upvote_count":"12","timestamp":"1704402480.0","poster":"raaad"},{"upvote_count":"5","comment_id":"1191255","content":"Selected Answer: A\nA. Adopt multi-regional Cloud Storage buckets in your architecture.\n\nWhy A is the best choice:\n\nAutomatic Cross-Region Replication: Multi-regional buckets automatically replicate data across multiple geographically separated regions within a selected multi-region location (e.g., us). This ensures data redundancy and availability even if one region experiences an outage.\nMinimal RPO: Data written to a multi-regional bucket is synchronously replicated to at least two regions. This means that in the event of a regional failure, the RPO is essentially zero, as the data is already available in other regions.\nNo Application Changes: Applications can continue reading and writing data to the multi-regional bucket without any modifications, as the cross-region replication is handled transparently by Cloud Storage","timestamp":"1712533140.0","comments":[{"poster":"mdell","content":"Minimal yet, but not minimized as stated in the question. That's why C is correct.\nTurbo replication provides faster redundancy across regions for data in your dual-region buckets, which reduces the risk of data loss exposure and helps support uninterrupted service following a regional outage. When enabled, turbo replication is designed to replicate 100% of newly written objects to the two regions that constitute a dual-region within the recovery point objective of 15 minutes, regardless of object size.\n\nNote that even for default replication, most objects finish replication within minutes.\n\nhttps://cloud.google.com/storage/docs/availability-durability#turbo-replication","timestamp":"1734533040.0","comment_id":"1328562","upvote_count":"1"}],"poster":"CGS22"},{"upvote_count":"1","comment_id":"1316715","poster":"petulda","comments":[{"comment_id":"1316718","content":"Sorry, it is about minimizing RPO, where Turbo replication is a factor..","poster":"petulda","upvote_count":"1","timestamp":"1732377480.0"}],"content":"Why not A\nhttps://cloud.google.com/storage/docs/locations\nmulti regional location has cross region redundancy","timestamp":"1732376940.0"},{"upvote_count":"2","comment_id":"1179552","content":"Selected Answer: C\nvote c","timestamp":"1711049400.0","poster":"hanoverquay"},{"comment_id":"1173316","upvote_count":"3","timestamp":"1710413280.0","poster":"ricardovazz","content":"Selected Answer: C\nhttps://cloud.google.com/storage/docs/availability-durability#turbo-replication\n\n\"Default replication in Cloud Storage is designed to provide redundancy across regions for 99.9% of newly written objects within a target of one hour and 100% of newly written objects within a target of 12 hours\"\n\n\"When enabled, turbo replication is designed to replicate 100% of newly written objects to both regions that constitute the dual-region within the recovery point objective of 15 minutes, regardless of object size.\"\n\nThus, since they want to minimize RPO, should use turbo replication"},{"poster":"JyoGCP","content":"Selected Answer: C\nOption C","comment_id":"1154436","upvote_count":"2","timestamp":"1708396500.0"},{"upvote_count":"5","poster":"Matt_108","comment_id":"1121677","content":"Selected Answer: C\nOption C: https://cloud.google.com/storage/docs/dual-regions + https://cloud.google.com/storage/docs/managing-turbo-replication","timestamp":"1705152180.0"},{"poster":"therealsohail","comment_id":"1115150","content":"Selected Answer: C\nTurbo replication provides faster redundancy across regions for data in your dual-region buckets, which reduces the risk of data loss exposure and helps support uninterrupted service following a regional outage.","timestamp":"1704541740.0","upvote_count":"5"},{"upvote_count":"2","comments":[{"comment_id":"1124073","upvote_count":"3","comments":[{"content":"But multi-region is completely transparent for the application if one fails. it would need to fail all EU or US regions. I dont undertand why multi-region would have impact on that","upvote_count":"1","comment_id":"1157453","poster":"ashdam","timestamp":"1708718520.0"},{"content":"Whereas with multi-region \" it can also introduce unpredictable latency into the response time and higher network egress charges for cloud workloads when multi-region data is read from remote regions\"\nhttps://cloud.google.com/blog/products/storage-data-transfer/choose-between-regional-dual-region-and-multi-region-cloud-storage","comments":[{"content":"There is no requirment on latency, just RPO which it would be 0 since multi-region.","timestamp":"1708719060.0","upvote_count":"2","comment_id":"1157466","poster":"ashdam"}],"upvote_count":"4","poster":"datapassionate","comment_id":"1124075","timestamp":"1705399440.0"}],"poster":"datapassionate","content":"It wont be a correct answer. Correct is C. It is required \"no impact on applications that use the stored data\"","timestamp":"1705399440.0"}],"poster":"scaenruy","content":"Selected Answer: A\nA. Adopt multi-regional Cloud Storage buckets in your architecture.","comment_id":"1112771","timestamp":"1704287160.0"}],"question_text":"You are designing the architecture of your application to store data in Cloud Storage. Your application consists of pipelines that read data from a Cloud Storage bucket that contains raw data, and write the data to a second bucket after processing. You want to design an architecture with Cloud Storage resources that are capable of being resilient if a Google Cloud regional failure occurs. You want to minimize the recovery point objective (RPO) if a failure occurs, with no impact on applications that use the stored data. What should you do?"},{"id":"ABJBNTzkoaAxCrhjOWGI","answer":"D","question_text":"You have designed an Apache Beam processing pipeline that reads from a Pub/Sub topic. The topic has a message retention duration of one day, and writes to a Cloud Storage bucket. You need to select a bucket location and processing strategy to prevent data loss in case of a regional outage with an RPO of 15 minutes. What should you do?","question_id":160,"answer_images":[],"answer_ET":"D","choices":{"D":"1. Use a dual-region Cloud Storage bucket with turbo replication enabled.\n2. Monitor Dataflow metrics with Cloud Monitoring to determine when an outage occurs.\n3. Seek the subscription back in time by 60 minutes to recover the acknowledged messages.\n4. Start the Dataflow job in a secondary region.","B":"1. Use a multi-regional Cloud Storage bucket.\n2. Monitor Dataflow metrics with Cloud Monitoring to determine when an outage occurs.\n3. Seek the subscription back in time by 60 minutes to recover the acknowledged messages.\n4. Start the Dataflow job in a secondary region.","A":"1. Use a dual-region Cloud Storage bucket.\n2. Monitor Dataflow metrics with Cloud Monitoring to determine when an outage occurs.\n3. Seek the subscription back in time by 15 minutes to recover the acknowledged messages.\n4. Start the Dataflow job in a secondary region.","C":"1. Use a regional Cloud Storage bucket.\n2. Monitor Dataflow metrics with Cloud Monitoring to determine when an outage occurs.\n3. Seek the subscription back in time by one day to recover the acknowledged messages.\n4. Start the Dataflow job in a secondary region and write in a bucket in the same region."},"timestamp":"2024-01-03 14:15:00","isMC":true,"answer_description":"","discussion":[{"upvote_count":"8","poster":"datapassionate","comment_id":"1124081","comments":[{"upvote_count":"1","timestamp":"1724437260.0","content":"Why multi-region is not correct. There is no downtime in case a region goes down.","poster":"ashdam","comment_id":"1157471"}],"timestamp":"1721117760.0","content":"Selected Answer: D\nD. 1. Use a dual-region Cloud Storage bucket with turbo replication enabled.\n2. Monitor Dataflow metrics with Cloud Monitoring to determine when an outage occurs.\n3. Seek the subscription back in time by 60 minutes to recover the acknowledged messages.\n4. Start the Dataflow job in a secondary region.\n\nRPO of 15 minutes is guaranteed when turbo replication is used\nhttps://cloud.google.com/storage/docs/availability-durability"},{"content":"Selected Answer: D\nOption D is correct.\n\nNot A, because dual-region bucket WITHOUT turbo replication takes atleast 1 hour to sync data between regions. SLA for 100% data sync is 12 hours as per google.","timestamp":"1724114460.0","upvote_count":"5","comment_id":"1154441","poster":"JyoGCP"},{"timestamp":"1737919620.0","content":"Selected Answer: D\nCould be A or D. The choice between a 15-minute seek and a 60-minute seek depends on your specific requirements and priorities. If a very low RPO is critical, a 60-minute seek might be necessary to ensure data completeness.If minimizing cost and processing time is more important, a 15-minute seek might be sufficient, especially if you're confident in the reliability of Turbo Replication.","poster":"LP_PDE","upvote_count":"1","comment_id":"1347076"},{"content":"Selected Answer: A\nAn RPO of 15 minutes seemingly suggests using Turbo Replication. But here's the thing - why would you want to seek the subscription back in time by 60 minutes and run the Dataflow job? Thus, if turbo replication is enabled, steps 3 & 4 are completely redundant and unnecessary. Which is why option A is correct. This was a tricky one!","comment_id":"1331924","timestamp":"1735219800.0","poster":"m_a_p_s","upvote_count":"1"},{"content":"Selected Answer: A\nI don't like answer D. If we have turbo replication can ensure that change within 15min can be replicated, why do we still need to seek the subscription back in time by 60min?","timestamp":"1734720660.0","poster":"shangning007","comment_id":"1329614","upvote_count":"1"},{"poster":"SVGoogle89","comment_id":"1210267","upvote_count":"1","timestamp":"1731432780.0","content":"D\nhttps://cloud.google.com/storage/docs/availability-durability#cross-region-redundancy"},{"content":"Selected Answer: D\nhttps://cloud.google.com/storage/docs/availability-durability#turbo-replication says : \"When enabled, turbo replication is designed to replicate 100% of newly written objects to both regions that constitute the dual-region within the recovery point objective of 15 minutes, regardless of object size.\"\nso seems D to me","timestamp":"1721898780.0","poster":"lipa31","comment_id":"1131592","upvote_count":"4"},{"content":"Selected Answer: A\n- Low RPO: Dual-region buckets offer synchronous replication, ensuring data is immediately available in both regions, aligning with the 15-minute RPO.\n- Turbo Replication: enabling turbo replication can further reduce replication latency to near-real-time for even stricter RPO requirements.\n- Resilient Data Storage: Dual-region buckets ensure data availability even during regional outages, protecting processed data.\n- Fast Recovery: Reprocessing from the last 15 minutes of acknowledged messages minimizes data loss and downtime.","comments":[{"comment_id":"1126379","poster":"qq589539483084gfrgrgfr","timestamp":"1721351220.0","content":"why not D then, if turbo replication improves RPO??","upvote_count":"2"}],"poster":"raaad","upvote_count":"1","timestamp":"1720120380.0","comment_id":"1114073"},{"comment_id":"1112778","content":"Selected Answer: A\nA. \n1. Use a dual-region Cloud Storage bucket.\n2. Monitor Dataflow metrics with Cloud Monitoring to determine when an outage occurs.\n3. Seek the subscription back in time by 15 minutes to recover the acknowledged messages.\n4. Start the Dataflow job in a secondary region.","poster":"scaenruy","timestamp":"1720005300.0","upvote_count":"1"}],"exam_id":11,"topic":"1","unix_timestamp":1704287700,"question_images":[],"answers_community":["D (82%)","A (18%)"],"url":"https://www.examtopics.com/discussions/google/view/130186-exam-professional-data-engineer-topic-1-question-242/"}],"exam":{"name":"Professional Data Engineer","lastUpdated":"11 Apr 2025","isImplemented":true,"numberOfQuestions":319,"provider":"Google","id":11,"isMCOnly":true,"isBeta":false},"currentPage":32},"__N_SSP":true}