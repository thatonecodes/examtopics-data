{"pageProps":{"questions":[{"id":"vxFpMiS1OBMcukhmWOa3","choices":{"B":"Cloud Dataflow connected to the Kafka cluster to scale the processing of incoming messages.","C":"An IoT gateway connected to Cloud Pub/Sub, with Cloud Dataflow to read and process the messages from Cloud Pub/Sub.","A":"Edge TPUs as sensor devices for storing and transmitting the messages.","D":"A Kafka cluster virtualized on Compute Engine in us-east with Cloud Load Balancing to connect to the devices around the world."},"answers_community":["C (100%)"],"unix_timestamp":1584872520,"timestamp":"2020-03-22 11:22:00","answer":"C","question_id":26,"discussion":[{"poster":"[Removed]","comment_id":"66900","upvote_count":"21","timestamp":"1632298920.0","content":"Should be C"},{"poster":"[Removed]","comment_id":"68817","upvote_count":"19","content":"Answer: C\nDescription: Pubsub is global and dataflow can scale workers","timestamp":"1632816960.0"},{"upvote_count":"1","timestamp":"1732892160.0","poster":"ga8our","content":"Can anyone pls explain what's wrong with D, the load balancing solution?","comment_id":"909410"},{"content":"Answer C:\nWhat is wrong with D, nothing, Cloud load balancing can shift traffic for high volume and low internet in one region. It cost avg. 0.01-0.25 $ per GB, or if volume is too high. 0.05 $ per Hour http request. This might be the answer if your exam for network engineer.","comment_id":"820795","upvote_count":"2","timestamp":"1724514480.0","poster":"musumusu"},{"comment_id":"811704","poster":"musumusu","timestamp":"1723880700.0","content":"Answer C, but it will not solve bad internet connection, make sure 100mbps speed of internet is at sensor side.","upvote_count":"1"},{"comments":[{"upvote_count":"1","poster":"AzureDP900","timestamp":"1719708720.0","comment_id":"762447","content":"Agree with your explanation"}],"timestamp":"1717574700.0","upvote_count":"10","comment_id":"735829","content":"Selected Answer: C\nC is the answer.\n\nhttps://cloud.google.com/architecture/iot-overview#cloud-pubsub\nPub/Sub can act like a shock absorber and rate leveller for both incoming data streams and application architecture changes. Many devices have limited ability to store and retry sending telemetry data. Pub/Sub scales to handle data spikes that can occur when swarms of devices respond to events in the physical world, and buffers these spikes to help isolate them from applications monitoring the data.","poster":"zellck"},{"content":"\"single on-premises Kafka cluster in a data center in the us-east region\"\nis it on-prem or in a datacenter in us-east ?","poster":"MisuLava","comment_id":"708655","upvote_count":"1","timestamp":"1714494540.0"},{"poster":"JamesKarianis","comment_id":"633686","timestamp":"1705691760.0","upvote_count":"1","content":"Selected Answer: C\nAnswer is C"},{"poster":"Prasanna_kumar","upvote_count":"1","comment_id":"553021","timestamp":"1692628980.0","content":"Answer is option C"},{"content":"Answer c\nkafka cluster in on-premise for streaming msgs\npub/sub for streaming msgs in cloud","poster":"ivanhsiav","timestamp":"1673768400.0","upvote_count":"4","comment_id":"406803"},{"upvote_count":"4","content":"Vote for C","comment_id":"397439","timestamp":"1672743540.0","poster":"sumanshu"},{"poster":"Allan222","upvote_count":"4","content":"Should be C","timestamp":"1661380740.0","comment_id":"298639"},{"comment_id":"293413","upvote_count":"5","timestamp":"1660823820.0","poster":"daghayeghi","content":"C is correct:\nthe main trick come from A, and response is that TPU only use when we have a deployed machine learning model that we don't have now."},{"upvote_count":"4","timestamp":"1660599120.0","poster":"ArunSingh1028","comment_id":"291347","content":"Answer - C"},{"timestamp":"1652175300.0","comment_id":"216558","upvote_count":"5","content":"Easy Question : ANswer is Option C. \nAlterative to Kafka in google cloud native service is Pub/Sub and Dataflow punched with Pub/Sub is the google recommended option","poster":"Alasmindas"},{"content":"C\nthe issue is with a single Kafka cluster is the need to scale automatically with Dataflow","timestamp":"1645582200.0","comment_id":"163998","upvote_count":"4","poster":"atnafu2020"},{"content":"C is correct","poster":"haroldbenites","timestamp":"1645474260.0","upvote_count":"4","comment_id":"163113"},{"upvote_count":"5","poster":"Rajuuu","comment_id":"131891","content":"Answer is C. Pub/Sub is the messaging tool for Global.","timestamp":"1641882720.0"},{"content":"Answer C - Cloud Native = Pub/Sub + DataFlow","poster":"Rajokkiyam","upvote_count":"8","comment_id":"69728","timestamp":"1632978780.0"}],"answer_ET":"C","url":"https://www.examtopics.com/discussions/google/view/17240-exam-professional-data-engineer-topic-1-question-121/","answer_description":"","exam_id":11,"isMC":true,"topic":"1","question_images":[],"answer_images":[],"question_text":"You currently have a single on-premises Kafka cluster in a data center in the us-east region that is responsible for ingesting messages from IoT devices globally.\nBecause large parts of globe have poor internet connectivity, messages sometimes batch at the edge, come in all at once, and cause a spike in load on your\nKafka cluster. This is becoming difficult to manage and prohibitively expensive. What is the Google-recommended cloud native architecture for this scenario?"},{"id":"Mv4K2tM8RUxTyS0OHMkB","question_text":"You decided to use Cloud Datastore to ingest vehicle telemetry data in real time. You want to build a storage system that will account for the long-term data growth, while keeping the costs low. You also want to create snapshots of the data periodically, so that you can make a point-in-time (PIT) recovery, or clone a copy of the data for Cloud Datastore in a different environment. You want to archive these snapshots for a long time. Which two methods can accomplish this?\n(Choose two.)","discussion":[{"upvote_count":"38","comment_id":"74230","content":"A,B\nhttps://cloud.google.com/datastore/docs/export-import-entities","poster":"Ganshank","timestamp":"1602618600.0","comments":[{"content":"\"while keeping the costs\"\n\nshould be A,D","upvote_count":"6","timestamp":"1635569580.0","poster":"salsabilsf","comments":[{"timestamp":"1644253200.0","upvote_count":"9","comment_id":"421258","poster":"MrCastro","comments":[{"content":"If you use B , not D , how can we do \"point in time\" recovery? is it possible?\nPoint in time recovery needs export along with timestamp, so that we can recover for a particular timestamp.","upvote_count":"4","timestamp":"1648778700.0","comment_id":"455245","poster":"hellofrnds"}],"content":"Big query streaming inserts ARE NOT cheap"}],"comment_id":"345885"}]},{"comments":[{"upvote_count":"6","timestamp":"1652086980.0","poster":"aparna4387","comment_id":"474749","content":"https://cloud.google.com/datastore/docs/export-import-entities#import-into-bigquery\nData exported without specifying an entity filter cannot be loaded into BigQuery. This is not mentioned explicitly. Safe to assume there is no filter on the exports. So options are AB"},{"comment_id":"762448","content":"A, B is perfect","poster":"AzureDP900","upvote_count":"1","timestamp":"1688086500.0"},{"timestamp":"1664264820.0","upvote_count":"2","comment_id":"576056","poster":"tavva_prudhvi","content":"As you've mentioned in B, does the environment meant to be a project or a resource? As, we can clone a copy of the data in a datastore even in another project!? Then, it's B. \n\nAlso, in point C they didn't mention any entity filter hence we eliminate C how can you support your own statement with a different answer?"},{"comments":[{"content":"you wanna say invalid?","poster":"Chelseajcole","comment_id":"455055","upvote_count":"1","timestamp":"1648661400.0"}],"timestamp":"1644003000.0","poster":"Yiouk","content":"C is valid because of table snapshots. Else standard time travel is valid only for 7 days\nhttps://cloud.google.com/bigquery/docs/table-snapshots-intro#table_snapshots\nhttps://cloud.google.com/bigquery/docs/time-travel#limitation","comment_id":"419857","upvote_count":"1"}],"comment_id":"167150","content":"AC\nhttps://cloud.google.com/datastore/docs/export-import-entities\nC: To import only a subset of entities or to import data into BigQuery, you must specify an entity filter in your export.\nB: Not correct since you want to store in a different environment than Datastore. Tho this statment is true: Data exported from one Datastore mode database can be imported into another Datastore mode database, even one in another project.\nA is correct\nBilling and pricing for managed exports and imports in Datastore\nOutput files stored in Cloud Storage count towards your Cloud Storage data storage costs.\nSteps to Export all the entities\n1. Go to the Datastore Entities Export page in the Google Cloud Console.\n2. Go to the Datastore Export page\n2. Set the Namespace field to All Namespaces, and set the Kind field to All Kinds.\n3. Below Destination, enter the name of your \"Cloud Storage bucket\".\n4. Click Export.","upvote_count":"23","timestamp":"1614386460.0","poster":"atnafu2020"},{"content":"Selected Answer: AB\nhttps://cloud.google.com/datastore/docs/export-import-entities","comment_id":"1189539","poster":"CGS22","timestamp":"1728075420.0","upvote_count":"1"},{"poster":"kskssk","upvote_count":"5","content":"AB chatgpt\nA. Use managed export, and store the data in a Cloud Storage bucket using Nearline or Coldline class:\n\nManaged export is a feature provided by Cloud Datastore to export your data.\nStoring the data in a Cloud Storage bucket, especially using Nearline or Coldline storage classes, helps keep storage costs low while allowing you to retain the snapshots for a long time.\nB. Use managed export, and then import to Cloud Datastore in a separate project under a unique namespace reserved for that export:\n\nThis method allows you to create snapshots by exporting data from Cloud Datastore (using managed export) and then importing it into a separate project under a unique namespace.\nBy importing into a separate project, you can keep a copy of the data in a different environment, which is useful for point-in-time recovery or creating clones of the data.","timestamp":"1709400540.0","comment_id":"996975"},{"timestamp":"1685785740.0","poster":"zellck","upvote_count":"3","content":"Selected Answer: AB\nAB is the answer.","comment_id":"734409"},{"comment_id":"733189","poster":"NicolasN","content":"Selected Answer: AB\nA rather complicated question, of a kind I wish I won't face in the exam. My opinion:\n‚úÖ [A] A valid and cost-effective solution satisfying the requirement for PIT recovery\n‚úÖ [B] A valid solution but far from ideal for archiving. It satisfies the requirement part \"you can ‚Ä¶ clone a copy of the data for Cloud Datastore in a different environment\" (an objection to the word \"namespace\", I think it should be just \"name\")","upvote_count":"11","timestamp":"1685653740.0","comments":[{"comment_id":"733191","timestamp":"1685653740.0","poster":"NicolasN","upvote_count":"11","content":"‚ùå[C] There is the limitation \"Data exported without specifying an entity filter cannot be loaded into BigQuery\". The entity filter for this case should contain all the kinds of entities but there is another limitation of \"100 entity filter combinations\". We have no knowledge of the kinds or the namespaces of the entities.\nSources:\nüîó https://cloud.google.com/datastore/docs/export-import-entities#import-into-bigquery\nüîó https://cloud.google.com/datastore/docs/export-import-entities#exporting_specific_kinds_or_namespaces\n‚ùå [D] seems a detailed candidate solution but it violates the limitation \"You cannot append Datastore export data to an existing table.\"\nüîó https://cloud.google.com/bigquery/docs/loading-data-cloud-datastore#appending_to_or_overwriting_a_table_with_cloud_datastore_data\n‚ùå [E] Cloud Source Repositories are for source code and not a suitable storage for this case."}]},{"content":"Selected Answer: AB\nhttps://cloud.google.com/datastore/docs/export-import-entities","timestamp":"1680187800.0","comment_id":"683682","upvote_count":"1","poster":"John_Pongthorn"},{"poster":"John_Pongthorn","content":"Selected Answer: AB\nhttps://cloud.google.com/datastore/docs/export-import-entities","upvote_count":"1","comment_id":"668040","timestamp":"1678720140.0"},{"timestamp":"1678702200.0","comment_id":"667773","upvote_count":"1","poster":"John_Pongthorn","content":"Selected Answer: AB\nThe answer is nothing to do with bigquery , so you can skip what mention to bigquery.\n\nA B is the final answer"},{"comment_id":"634033","content":"A,B\n\nFor those who say using BQ as archival, How can we achieve that while datastore are NO-SQL whereas BQ are SQL , will that work? also BQ are not created for achieving purposes.","timestamp":"1674224820.0","poster":"DataEngineer_WideOps","upvote_count":"2"},{"poster":"AmirN","upvote_count":"1","timestamp":"1671391920.0","content":"Option B is 36 times more expensive than C","comment_id":"618360"},{"timestamp":"1657963260.0","comment_id":"524882","poster":"Nico1310","content":"Selected Answer: AB\nAB. for sure streaming to BQ its quite expensive!","upvote_count":"2"},{"upvote_count":"3","poster":"MaxNRG","timestamp":"1657353360.0","comments":[{"upvote_count":"2","timestamp":"1666428780.0","poster":"tavva_prudhvi","content":"D is wrong, BQ Streaming inserts costs are high!","comment_id":"589848","comments":[{"upvote_count":"1","content":"Agreed, AB\nhttps://cloud.google.com/datastore/docs/export-import-entities","timestamp":"1718628900.0","poster":"MaxNRG","comment_id":"1099011"}]}],"comment_id":"520080","content":"Selected Answer: AD\nA - Cloud Storage (long-term data + costs low)\nD - BigQuery (timestamp for point-in-time (PIT) recovery)"},{"upvote_count":"2","poster":"medeis_jar","content":"Selected Answer: AB\nOption A; Cheap storage and it is a supported method https://cloud.google.com/datastore/docs/export-import-entities\nOption B; Rationale - \"Data exported from one Datastore mode database can be imported into another Datastore mode database, even one in another project.\" <https://cloud.google.com/datastore/docs/export-import-entities>","comment_id":"519480","timestamp":"1657273800.0"},{"poster":"squishy_fishy","upvote_count":"1","timestamp":"1650207660.0","content":"Answer is A, B. \nhttps://cloud.google.com/datastore/docs/export-import-entities#exporting_specific_kinds_or_namespaces","comment_id":"463583"},{"comment_id":"459031","poster":"sergio6","upvote_count":"4","content":"A, D\nA: Option for storage system that will account for the long-term data growth\nD: Option for snapshots, PIT recovery, copy of the data for Cloud Datastore in a different environment and, above all, archive snapshots for a long time \nB: not a good solution for archiving snapshots for a long time\nC: to import data into BigQuery, you must specify an entity filter \nE: Cloud Source Repositories is for code\nOne note: E --> would be my second choice if there was Cloud Storage instead of Source Repositories (typo?)","timestamp":"1649395620.0"},{"timestamp":"1647543360.0","comments":[{"timestamp":"1648661160.0","content":"https://cloud.google.com/datastore/docs/export-import-entities#import-into-bigquery\nImporting into BigQuery\nTo import data from a managed export into BigQuery, see Loading Datastore export service data.\n\nData exported without specifying an entity filter cannot be loaded into BigQuery. If you want to import data into BigQuery, your export request must include one or more kind names in the entity filter.\n\nYou have to specify an entity fliter before you can load from datastore to BQ. It didn't mention that at all. So C is incorrect","comment_id":"455052","poster":"Chelseajcole","upvote_count":"3"}],"upvote_count":"1","comment_id":"446713","poster":"Chelseajcole","content":"Vote A B . What‚Äôs the purpose load into bigquery?"},{"upvote_count":"7","timestamp":"1645194840.0","comment_id":"426845","poster":"fire558787","content":"A for sure. Then I was undecided between B and C; B has high costs and C has low costs (storage is more expensive in Datastore). However the question says that you want data to be used for Datastore. There is no native way to export data from BigQuery to Datastore, hence the only two options that allow data to be restored to Datastore are A and B."},{"content":"Answer should be A, B\nHere the requirement is yo have PIT recovery, by periodicaaly copy data to a differenent datastore , recovery can be faster.","timestamp":"1643692860.0","comment_id":"418094","poster":"DeepakS227","upvote_count":"2"},{"content":"AD - D seems complicated but the question talks about real time ingestion etc. and if you are presented with the same problem in real life, you will go with option D.","poster":"ralf_cc","upvote_count":"4","timestamp":"1641215460.0","comment_id":"397513"},{"timestamp":"1641208860.0","content":"Vote for A & B\n\nC - does not look good. because You cannot append Datastore export data to an existing table with a defined schema., which means we need to create multiple tables for all export, which is expensive\n\nhttps://cloud.google.com/bigquery/docs/loading-data-cloud-datastore","comment_id":"397452","poster":"sumanshu","upvote_count":"3"},{"content":"Correct Answer: A & C\nUnderlying storage for BigQuery is cloud storage and it supports storage classes like nearline/coldline. As we need to keep the cost low and archive these snapshots for long time A & C are the best answers.","poster":"Jeysolomon","upvote_count":"1","timestamp":"1640218800.0","comment_id":"388328"},{"comments":[{"poster":"Jay3244","content":"Yes its present in the above document. Options AB","comment_id":"303511","timestamp":"1630771200.0","upvote_count":"3"}],"content":"Option A; Cheap storage and it is a supported meathod https://cloud.google.com/datastore/docs/export-import-entities\nOption B; Rationale - \"Data exported from one Datastore mode database can be imported into another Datastore mode database, even one in another project.\" <https://cloud.google.com/datastore/docs/export-import-entities>","upvote_count":"5","comment_id":"252241","poster":"sashank_1","timestamp":"1624640520.0"},{"comment_id":"231280","content":"AC because B its no posible\n\"clone a copy of the data for Cloud Datastore in a different environment.\"","poster":"federicohi","timestamp":"1622391060.0","upvote_count":"4"},{"content":"Option A is correct , remaining all options are very vague to me ... none of them exactly fits to the question based on my knowledge, however I will go with - IMPORT and EXPORT - so option B","comment_id":"216567","timestamp":"1620639960.0","poster":"Alasmindas","upvote_count":"2"},{"poster":"Tanmoyk","upvote_count":"2","content":"In my Opinion A and B are the most suitable answer","comment_id":"195110","timestamp":"1617791520.0"},{"upvote_count":"3","comment_id":"184283","poster":"girgu","timestamp":"1616410140.0","content":"AC\nNote: With the managed export and import service, you can recover from accidental deletion of data and export data for offline processing. You can export all entities or just specific kinds of entities. Likewise, you can import all data from an export or only specific kinds. Big Query only storage charges and when required export CSV and import in Datastore and apply filter on which entities to import. E >> Source Repo not data, B>> High Cost, D>> BQ Directly reads Datastore dumps so not required the complex process."},{"comment_id":"163120","poster":"haroldbenites","upvote_count":"3","content":"A, C .\nC is better than B because Bigquery can be used to sotrage data and if the data is not qeried more than 30days this data becomes to state nearline and the storage cost decreases.","timestamp":"1613939100.0"},{"content":"Its A & B","timestamp":"1611240840.0","poster":"Javed","upvote_count":"3","comment_id":"140311"},{"timestamp":"1610592060.0","content":"Answer A & B","poster":"Rajokkiyam","comment_id":"134468","upvote_count":"3"},{"poster":"dambilwa","timestamp":"1609341240.0","content":"Options [A&B] - are the most appropriate options.. ruling out C&D as Datastore & BigQuery store different structures of data.. E is ruled out as Cloud Source repositories is not an option here","comment_id":"123524","upvote_count":"3"},{"comment_id":"116068","content":"A for sure--as question demands long term storage. Out of B/C/D/E, E-eliminated as cloud source repositories are not required. out of B/C/D--D makes most sense. Answer A&D.","comments":[{"poster":"tavva_prudhvi","comment_id":"576050","content":"Streaming inserts are more expensive, hence not D. it has to be AB/AC.","upvote_count":"1","timestamp":"1664264040.0"}],"poster":"AJKumar","upvote_count":"1","timestamp":"1608624000.0"},{"comment_id":"90814","timestamp":"1605648000.0","comments":[{"upvote_count":"2","poster":"Rajuuu","content":"A and B is correct..Export and import in a new project under Datastore.","timestamp":"1610871360.0","comment_id":"136936"}],"upvote_count":"5","poster":"arnabbis4u","content":"A and B"},{"comment_id":"70293","poster":"Rajokkiyam","timestamp":"1601599380.0","content":"Answer A & C","upvote_count":"3"},{"comments":[{"content":"Selected - AE","timestamp":"1600834920.0","upvote_count":"3","poster":"[Removed]","comment_id":"67194","comments":[{"comment_id":"397445","timestamp":"1641208200.0","upvote_count":"2","content":"E looks wrong - Cloud Source Repositories. (where we store CODE, not data)","poster":"sumanshu"}]}],"upvote_count":"1","poster":"[Removed]","content":"Confirm A\nOther one may be out of B,D,E","comment_id":"66903","timestamp":"1600763400.0"},{"timestamp":"1600336140.0","poster":"rickywck","upvote_count":"4","comment_id":"65169","content":"No way E is correct since Cloud Source Repositories is for storing programs, not data. However, even for A, other answers seem strange ..."}],"url":"https://www.examtopics.com/discussions/google/view/16858-exam-professional-data-engineer-topic-1-question-122/","choices":{"B":"Use managed export, and then import to Cloud Datastore in a separate project under a unique namespace reserved for that export.","E":"Write an application that uses Cloud Datastore client libraries to read all the entities. Format the exported data into a JSON file. Apply compression before storing the data in Cloud Source Repositories.","C":"Use managed export, and then import the data into a BigQuery table created just for that export, and delete temporary export files.","D":"Write an application that uses Cloud Datastore client libraries to read all the entities. Treat each entity as a BigQuery table row via BigQuery streaming insert. Assign an export timestamp for each export, and attach it as an extra column for each row. Make sure that the BigQuery table is partitioned using the export timestamp column.","A":"Use managed export, and store the data in a Cloud Storage bucket using Nearline or Coldline class."},"answers_community":["AB (88%)","12%"],"question_id":27,"answer_images":[],"topic":"1","exam_id":11,"unix_timestamp":1584445740,"answer":"AB","answer_description":"","question_images":[],"answer_ET":"AB","timestamp":"2020-03-17 12:49:00","isMC":true},{"id":"MnmKr325jZRFGObU7K6T","answers_community":["AD (82%)","Other"],"exam_id":11,"question_images":[],"answer_images":[],"answer_ET":"AD","question_text":"You need to create a data pipeline that copies time-series transaction data so that it can be queried from within BigQuery by your data science team for analysis.\nEvery hour, thousands of transactions are updated with a new status. The size of the initial dataset is 1.5 PB, and it will grow by 3 TB per day. The data is heavily structured, and your data science team will build machine learning models based on this data. You want to maximize performance and usability for your data science team. Which two strategies should you adopt? (Choose two.)","timestamp":"2020-03-20 04:21:00","answer_description":"","unix_timestamp":1584674460,"isMC":true,"choices":{"D":"Develop a data pipeline where status updates are appended to BigQuery instead of updated.","E":"Copy a daily snapshot of transaction data to Cloud Storage and store it as an Avro file. Use BigQuery's support for external data sources to query.","A":"Denormalize the data as must as possible.","B":"Preserve the structure of the data as much as possible.","C":"Use BigQuery UPDATE to further reduce the size of the dataset."},"question_id":28,"discussion":[{"upvote_count":"40","content":"I think AD is the answer. E will not improve performance.","comment_id":"66177","poster":"rickywck","timestamp":"1632100860.0"},{"content":"Answer: A, D\nDescription: Denormalization will help in performance by reducing query time, update are not good with bigquery","comment_id":"68820","timestamp":"1632818100.0","poster":"[Removed]","upvote_count":"20","comments":[{"comment_id":"402320","upvote_count":"3","timestamp":"1673232060.0","content":"My guess is append has better performance than update.","poster":"awssp12345"}]},{"poster":"midgoo","comment_id":"847792","comments":[{"poster":"vaga1","upvote_count":"1","content":"Denormalization is just a best practice when using BQ.","comment_id":"917292","timestamp":"1733588460.0"},{"timestamp":"1733475660.0","upvote_count":"5","comment_id":"915989","content":"Shouting data-science teams are not part of question, this is more about what is exam correct, not what it the best for your own situation","poster":"WillemHendr"}],"upvote_count":"3","timestamp":"1727058360.0","content":"Selected Answer: BD\nIf we denormalize the data, the Data Science team will shout at us. Preserving it is the way to go"},{"upvote_count":"7","content":"Selected Answer: AD\nA and D:\nA- Improve performance\nD- Is better for DS have all the history and not the last update...","comment_id":"738811","timestamp":"1717831200.0","poster":"odacir"},{"timestamp":"1717407960.0","content":"Selected Answer: AD\nAD is the answer.\n\nhttps://cloud.google.com/bigquery/docs/best-practices-performance-nested\nBest practice: Use nested and repeated fields to denormalize data storage and increase query performance.\n\nDenormalization is a common strategy for increasing read performance for relational datasets that were previously normalized. The recommended way to denormalize data in BigQuery is to use nested and repeated fields. It's best to use this strategy when the relationships are hierarchical and frequently queried together, such as in parent-child relationships.","comment_id":"734405","upvote_count":"4","comments":[{"content":"A, C is correct I agree","upvote_count":"1","poster":"AzureDP900","timestamp":"1719709080.0","comment_id":"762449"}],"poster":"zellck"},{"comment_id":"710980","poster":"NicolasN","timestamp":"1714802220.0","content":"The criteria for selecting a strategy are the performance and usability for the data science team. This team performs the analysis by querying stored data. So we don't care for performance related with data ingestion. According to this point of view:\nA: YES - undisputedly favours query performance\nB: YES - Keeping the structure unchanged promotes usability (the team won't need to update queries or ML models)\nC: Questionable - Updating the status of a row instead of appending newer versions is keeping the size smaller. But does this affect significantly the analysis performance? Even if it does, creating materialized views to keep the most recent status per row eliminates it\nD: NO - has nothing to do with DS team's tasks, affects ingestion performance\nE: NO - demotes usability","comments":[{"content":"For B there is no mention that the current data structure is being used (...data science team WILL build machine learning models based on this data.) ... We're developing a new data model to be used by them in the future","comment_id":"747130","poster":"jkhong","timestamp":"1718531820.0","upvote_count":"1"},{"timestamp":"1714802460.0","upvote_count":"1","poster":"NicolasN","content":"(mistakenly voted AC instead of AB)","comment_id":"710982"}],"upvote_count":"2"},{"comments":[{"upvote_count":"5","content":"Is not about the quota. You should avoid using UPDATE because it makes a big scan of the table, and is not efficient or high performant. Usually prefer appends and merges instead, and using the optimized schema approach of Big Query that denormalizes the table to avoid joins and leverages nested and repeated fields.","timestamp":"1713281280.0","comment_id":"696335","poster":"devaid"}],"upvote_count":"1","content":"Selected Answer: AC\nThe DML quota limit is removed since 2020, I think C is better than D now.","comment_id":"674844","timestamp":"1711008960.0","poster":"DerickTW"},{"upvote_count":"4","comment_id":"520081","poster":"MaxNRG","timestamp":"1688889600.0","content":"Selected Answer: AD\nA: Denormalization increases query speed for tables with billions of rows because BigQuery's performance degrades when doing JOINs on large tables, but with a denormalized data structure, you don't have to use JOINs, since all of the data has been combined into one table. \nDenormalization also makes queries simpler because you do not have to use JOIN clauses. \nhttps://cloud.google.com/solutions/bigquery-data-warehouse#denormalizing_data\nD: BigQuery append"},{"timestamp":"1688810340.0","poster":"medeis_jar","upvote_count":"3","content":"Selected Answer: AD\nrequirements are -> performance and usability.\n\nDenormalization will help in performance by reducing query time, update is not good with big query.\n\nAnd append has better performance than Update.","comment_id":"519485"},{"upvote_count":"1","poster":"doninakula","comment_id":"477936","content":"I think AD. E is not valid because it use external table which is not good for performance","timestamp":"1684031160.0"},{"poster":"sumanshu","comment_id":"397474","timestamp":"1672746960.0","upvote_count":"5","content":"A - correct (denormlization will help)\nB - data already heavily structured (no use and no impact)\nC - more than 1500 Updates not possible \nD - Not sure..(because appending will increase size and cost)\nE - Does not look good (increase cost..also we are storing for all days....again for query we need to issue mutiple query for all days....)\n\nSo, A & D (left out of 5)"},{"content":"Correct Answer: AE\nA ‚Äì Denormalisation helps improve performance.\nB, C - Not helping to address the problem. \nD ‚Äì Append will increase the db size and cost involved for storage and also for large number of records to scan for queries by data science team which is costlier.\nE - Addresses the problem of maximising the usability of the data science team and the data. They can anayse the data exported to cloud storage instead of reading from bigquery which is expensive and impact performance considerably.","upvote_count":"3","comment_id":"388338","poster":"Jeysolomon","timestamp":"1671756240.0","comments":[{"content":"It didn't mention cost is a concern","poster":"Chelseajcole","timestamp":"1681695480.0","comment_id":"463343","upvote_count":"1"},{"comment_id":"454223","content":"E is wrong, you've been asked to use bigquery and reading files from storage in bq is significantly more time consuming","upvote_count":"2","timestamp":"1680100440.0","poster":"retep007"}]},{"comment_id":"308968","upvote_count":"4","content":"A, D:\nUsing BigQuery as an OLTP store is considered an anti-pattern. Because OLTP stores have a high volume of updates and deletes, they are a mismatch for the data warehouse use case. To decide which storage option best fits your use case, review the Cloud storage products table.\nBigQuery is built for scale and can scale out as the size of the warehouse grows, so there is no need to delete older data. By keeping the entire history, you can deliver more insight on your business. If the storage cost is a concern, you can take advantage of BigQuery's long term storage pricing by archiving older data and using it for special analysis when the need arises. If you still have good reasons for dropping older data, you can use BigQuery's native support for date-partitioned tables and partition expiration. In other words, BigQuery can automatically delete older data.\nhttps://cloud.google.com/solutions/bigquery-data-warehouse#handling_change","timestamp":"1662989340.0","poster":"daghayeghi"},{"comment_id":"304956","timestamp":"1662523200.0","content":"should be AC.. \"Every hour, thousands of transactions are updated with a new status\" if we append how we will handle the new status change..","poster":"Hithesh","upvote_count":"2","comments":[{"upvote_count":"1","comment_id":"397472","comments":[{"content":"DML without limits now in BQ (below blog says March 2020, Not sure whether these questions were prepared before or after March 2020) \n\nhttps://cloud.google.com/blog/products/data-analytics/dml-without-limits-now-in-bigquery","timestamp":"1674581640.0","comments":[{"timestamp":"1674828660.0","content":"There is no more hard limit, but UPDATES are queued:\n\"BigQuery runs up to 2 of them concurrently, after which up to 20 are queued as PENDING. When a previously running job finishes, the next pending job is dequeued and run. Currently, queued mutating DML statements share a per-table queue with maximum length 20. Additional statements past the maximum queue length for each table fail.\"\n\nWith thousands of updates per hour, this doesn't seem feasible. I would assume the question is marked as outdated anyway or the answers are update in the actual exam.","comment_id":"415445","upvote_count":"4","poster":"hdmi_switch"}],"upvote_count":"1","poster":"raf2121","comment_id":"413265"}],"poster":"sumanshu","timestamp":"1672746660.0","content":"C not possible, maximum 1500 updates possible in a day"}]},{"comments":[{"comment_id":"294843","poster":"karthik89","timestamp":"1660976100.0","content":"you can update bigquery 1500 times in a day","upvote_count":"3"},{"content":"A, D:\nit was my mistake, we should decrease update as Bigquery is not design for update.\n\nhttps://cloud.google.com/solutions/bigquery-data-warehouse#handling_change","comment_id":"308972","poster":"daghayeghi","timestamp":"1662989820.0","upvote_count":"3"}],"comment_id":"293598","content":"AC:\nthe problem is exactly about Updating and preserving size of database as much as possible, then denormalization and using UPDATE function from DML will address the issue. they don't want to update faster. then A & C is correct.\nhttps://cloud.google.com/solutions/bigquery-data-warehouse","poster":"daghayeghi","upvote_count":"1","timestamp":"1660837800.0"},{"content":"A,D Since the requirements are both performance and usability.","poster":"Nams_139","comment_id":"227444","upvote_count":"5","timestamp":"1653469680.0"},{"content":"i tink may be ita AC becuase appending its worst to increase dataset size. THe question seems to put like a problem the size of dataset and performance to datascience so inserting more rows decrease performace for them.","poster":"federicohi","comment_id":"223024","timestamp":"1652979780.0","upvote_count":"3"},{"timestamp":"1652206140.0","upvote_count":"4","content":"AD looks like a good fit","comment_id":"216923","poster":"Ram459"},{"poster":"DeepakKhattar","upvote_count":"2","timestamp":"1648603320.0","comment_id":"189987","content":"Problem statement is - How to handle thousands of status updates. A, B, C does not address problem. \"Which two strategies should you adopt?\" D & E addresses the problem by avoiding UPDATE since its expensive & slow."},{"comment_id":"185150","upvote_count":"2","timestamp":"1648034340.0","poster":"SteelWarrior","content":"Should be B & D. The question says the data is heavily structured and doesn't mentioned about the normalization of the data. The machine learning models will require structured data."},{"timestamp":"1645922760.0","upvote_count":"3","comment_id":"167154","content":"AD\ncorrect Answer","poster":"atnafu2020"},{"poster":"haroldbenites","content":"A, D is correct","timestamp":"1645475280.0","comment_id":"163121","upvote_count":"3"},{"upvote_count":"3","timestamp":"1641442380.0","content":"AD yes","poster":"stuagano","comment_id":"127368"},{"timestamp":"1640402700.0","upvote_count":"4","poster":"dambilwa","content":"Options A & D are most appropriate","comment_id":"119014"},{"poster":"arnabbis4u","upvote_count":"5","content":"Yes AD","comment_id":"80034","timestamp":"1635287880.0"},{"poster":"Rajokkiyam","content":"Confirm AD","upvote_count":"4","comment_id":"69731","timestamp":"1632979200.0"},{"comment_id":"66914","upvote_count":"7","content":"Should be AD","poster":"[Removed]","timestamp":"1632301080.0"}],"answer":"AD","url":"https://www.examtopics.com/discussions/google/view/17023-exam-professional-data-engineer-topic-1-question-123/","topic":"1"},{"id":"WUZnA5aYAy0OqdTsM6wB","question_id":29,"question_text":"You are designing a cloud-native historical data processing system to meet the following conditions:\n‚úë The data being analyzed is in CSV, Avro, and PDF formats and will be accessed by multiple analysis tools including Dataproc, BigQuery, and Compute\nEngine.\n‚úë A batch pipeline moves daily data.\n‚úë Performance is not a factor in the solution.\n‚úë The solution design should maximize availability.\nHow should you design data storage for this solution?","question_images":[],"url":"https://www.examtopics.com/discussions/google/view/81264-exam-professional-data-engineer-topic-1-question-124/","answer":"D","answer_ET":"D","unix_timestamp":1662660420,"isMC":true,"choices":{"C":"Store the data in a regional Cloud Storage bucket. Access the bucket directly using Dataproc, BigQuery, and Compute Engine.","D":"Store the data in a multi-regional Cloud Storage bucket. Access the data directly using Dataproc, BigQuery, and Compute Engine.","B":"Store the data in BigQuery. Access the data using the BigQuery Connector on Dataproc and Compute Engine.","A":"Create a Dataproc cluster with high availability. Store the data in HDFS, and perform analysis as needed."},"timestamp":"2022-09-08 20:07:00","topic":"1","discussion":[{"upvote_count":"7","comment_id":"697377","content":"Selected Answer: D\nProblem: How to store data?\nConsiderations: High availability, performance not an issue\n\nA ‚Üí avoid HDFS\nC ‚Üí multi-regional > regional in terms of availability\n\nB could be the answer but we‚Äôre dealing with PDF documents, we need blob storage (cloud storage). If we only have csv or Avro, this may be the answer","timestamp":"1666007820.0","poster":"jkhong"},{"timestamp":"1726935180.0","poster":"serch_engine","upvote_count":"1","content":"Selected Answer: D\nD is the answer","comment_id":"1287401"},{"upvote_count":"1","comment_id":"762450","poster":"AzureDP900","timestamp":"1672455600.0","content":"D is right"},{"content":"Selected Answer: D\nvote for D","poster":"dconesoko","timestamp":"1672331760.0","upvote_count":"2","comment_id":"761263"},{"poster":"zellck","comment_id":"734396","upvote_count":"2","timestamp":"1670067360.0","content":"Selected Answer: D\nD is the answer."},{"content":"Selected Answer: D\nD of course","poster":"devaid","timestamp":"1665933840.0","upvote_count":"2","comment_id":"696331"},{"poster":"kenanars","content":"Selected Answer: D\nD is the correct answer","timestamp":"1662660420.0","upvote_count":"1","comment_id":"663892"}],"exam_id":11,"answer_images":[],"answer_description":"","answers_community":["D (100%)"]},{"id":"9TwaMrDOq8M9wLisP23S","discussion":[{"upvote_count":"34","timestamp":"1617168240.0","poster":"Rajokkiyam","comment_id":"69733","content":"Answer C."},{"content":"A and B can be eliminated right away as they do not talk about providing for other cloud providers. between C and D. The question says nothing about warm or cold data-rather that data should be made available for other providers--C--can fulfill this condition. Answer C.","timestamp":"1624339620.0","comments":[{"upvote_count":"1","content":"Agree with C","timestamp":"1703991780.0","poster":"AzureDP900","comment_id":"762452"}],"comment_id":"116043","poster":"AJKumar","upvote_count":"24"},{"upvote_count":"1","comments":[{"timestamp":"1731038700.0","poster":"spicebits","content":"If you export data from BQ to GCS then you will have two copies and you will be in the same architecture as answer C.","comment_id":"1065309","upvote_count":"4"}],"timestamp":"1726590120.0","content":"For me A. I can use export from BQ to Cloud Storage. There is no need to store two copies of data.","poster":"zbyszek1","comment_id":"1009942"},{"comment_id":"964391","timestamp":"1722059280.0","poster":"vamgcp","upvote_count":"2","content":"Selected Answer: B\nIt can be C or D , but I will go with C as storing the full dataset in BigQuery and a compressed copy of the data in Cloud Storage is a good way to balance performance and cost."},{"timestamp":"1717221060.0","content":"Selected Answer: C\nBest answer is C, although BQ can query gzipped files stored on GCS directly.\nMaybe this double storage makes it a bit more highly available.","comment_id":"911770","poster":"forepick","upvote_count":"2"},{"timestamp":"1715032320.0","comment_id":"891038","content":"Selected Answer: D\nD is much more accurate.","poster":"izekc","upvote_count":"1"},{"poster":"jkhong","comment_id":"747855","timestamp":"1702799820.0","upvote_count":"1","content":"Selected Answer: C\nD ‚Üí does not guarantee 100% queryable or accessible/available"},{"poster":"zellck","content":"Selected Answer: C\nC is the answer.","timestamp":"1701603300.0","upvote_count":"1","comment_id":"734395"},{"poster":"Smaks","upvote_count":"1","comments":[{"comment_id":"630331","upvote_count":"10","content":"ignore this comment, please","timestamp":"1689143400.0","poster":"Smaks"}],"content":"You can read streaming data from Pub/Sub, and you can write streaming data to Pub/Sub or BigQuery. \nThus Cloud Storage is not a proper sink for streaming pipeline.\nI vote for B, since it is possible to convert unstructured data and store in BQ","timestamp":"1689143100.0","comment_id":"630328"},{"upvote_count":"1","timestamp":"1677673560.0","content":"BQ can reach files at google storage as external table. so my answer is D. (If data was smaller than this, I would choose C)","poster":"Aslkdup","comment_id":"558726"},{"poster":"Bhawantha","comment_id":"525945","timestamp":"1673973480.0","content":"Selected Answer: C\nboth requirements are full filled.","upvote_count":"2"},{"comment_id":"520087","timestamp":"1673258880.0","content":"Selected Answer: D\nD: BigQuery + Cloud Storage","upvote_count":"1","comments":[],"poster":"MaxNRG"},{"poster":"medeis_jar","timestamp":"1673179680.0","comment_id":"519497","content":"Selected Answer: C\n\"You must be able to perform data warehouse-style analytics on the data in Google Cloud and expose the dataset as files for batch analysis tools in other cloud providers?\"\nAnalytics -> BQ\nExposing -> GCS","upvote_count":"7"},{"timestamp":"1669443300.0","comment_id":"487109","upvote_count":"2","content":"Correct: C","poster":"JG123"},{"content":"vote for C","timestamp":"1659702840.0","comment_id":"420277","upvote_count":"3","poster":"xiaofeng_0226"},{"comment_id":"397488","timestamp":"1656843540.0","poster":"sumanshu","content":"Vote for 'C'\n\nA - Only Half requirement fulfil, expose as a file not getting fulfiled\nB - Not a warehouse\nC. Both requirements fulfiled...Bigquery and GCS\nD. Both requirement fulfiled...but what if other cloud provider wants to analysis on rest 80% of the data. - \n\nSo out of 4 options, C looks okay","upvote_count":"8"},{"comment_id":"301561","poster":"gcper","timestamp":"1646162340.0","upvote_count":"3","content":"C\n\nBigQuery for analytics processing and Cloud Storage for exposing the data as files"},{"timestamp":"1645217640.0","upvote_count":"4","comments":[{"timestamp":"1651295100.0","comment_id":"345893","upvote_count":"2","poster":"salsabilsf","content":"the question says :\n\"expose the dataset as files\" wich means Cloud Storage"}],"poster":"daghayeghi","content":"answer A:\nwith BigQuery Omni it is now possible to read data from other cloud providers without transferring the data to GCP and thereby by saving on egress charges.\nhttps://cloud.google.com/blog/products/data-analytics/introducing-bigquery-omni","comment_id":"293710"},{"content":"C is the right answer","timestamp":"1636575480.0","upvote_count":"3","comment_id":"216929","poster":"Ram459"},{"timestamp":"1636473480.0","upvote_count":"3","content":"Answer is D. However with BigQuery Omni it is now possible to read data from other cloud providers without transferring the data to GCP and thereby by saving on egress charges.\n\nhttps://cloud.google.com/blog/products/data-analytics/introducing-bigquery-omni","comments":[{"upvote_count":"1","comment_id":"222963","poster":"snamburi3","content":"But, Omni does not mention storage for batch analysis. the data will be still be available through Bigquery (I am no sure about it)....","timestamp":"1637342760.0"}],"poster":"Cloud_Enthusiast","comment_id":"216065"},{"upvote_count":"2","content":"Choice is between C and D, C - makes ticks both the boxes.\n1. Perform data warehouse-style analytics (you need historical data, data can be stored on files in 20/80 ratio but other cloud providers will not have access to 20% data, if we have 5 year worth data then 20% means other cloud providers will be missing out 1 year data)\n2. expose the dataset as files for batch analysis tools in other cloud providers. (A and B does not meet this requirement)","timestamp":"1632963300.0","comment_id":"189978","poster":"DeepakKhattar"},{"comment_id":"134327","poster":"tprashanth","comments":[{"content":"I think coz it says we have to expose the dataset as files for batch analysis tools in other cloud providers. So we need a copy as files too","comment_id":"149153","poster":"Prakzz","upvote_count":"4","timestamp":"1627910340.0"},{"timestamp":"1631942880.0","comment_id":"181397","poster":"Diqtator","content":"And BQ can't handle PDF","upvote_count":"2"},{"timestamp":"1629571140.0","upvote_count":"2","comment_id":"163128","poster":"haroldbenites","content":"The cost in GCS is cheaper and it make available the data at any time. Don't need to create processes to download data from bigquery and then delete it, that could incur in maintenance costs or monitoring."}],"timestamp":"1626210900.0","upvote_count":"3","content":"I'm inclined to go for A.\nThe difference between A and C is 'C' has additional compressed data on GCS. Why should we store this additional data on GCS when whole set is available on BQ?"},{"comment_id":"127886","poster":"Rajuuu","content":"C is the right answer.\nD is very specific as pointed above.","timestamp":"1625581560.0","upvote_count":"4"},{"poster":"dambilwa","comment_id":"119011","content":"Option [C] is most appropriate. Option [D] is too specific - i.e. ratio of hot & cold data which is nowhere mentioned in the question. Hence option [D] can be eliminated","upvote_count":"6","timestamp":"1624584120.0"},{"poster":"arnabbis4u","comment_id":"93011","timestamp":"1621548660.0","upvote_count":"5","content":"Answer C"},{"timestamp":"1616411220.0","comments":[{"content":"can you provide the details where I can read about D.","upvote_count":"3","poster":"Radhika7983","timestamp":"1639214580.0","comments":[{"timestamp":"1657396500.0","content":"D is wrong","comment_id":"402930","upvote_count":"1","poster":"sumanshu"}],"comment_id":"240781"}],"comment_id":"66916","upvote_count":"1","content":"Should be D","poster":"[Removed]"}],"answer_ET":"C","answer_description":"","choices":{"C":"Store the full dataset in BigQuery, and store a compressed copy of the data in a Cloud Storage bucket.","D":"Store the warm data as files in Cloud Storage, and store the active data in BigQuery. Keep this ratio as 80% warm and 20% active.","B":"Store and process the entire dataset in Bigtable.","A":"Store and process the entire dataset in BigQuery."},"isMC":true,"answer":"C","question_text":"You have a petabyte of analytics data and need to design a storage and processing platform for it. You must be able to perform data warehouse-style analytics on the data in Google Cloud and expose the dataset as files for batch analysis tools in other cloud providers. What should you do?","topic":"1","url":"https://www.examtopics.com/discussions/google/view/17243-exam-professional-data-engineer-topic-1-question-125/","question_images":[],"answer_images":[],"answers_community":["C (76%)","12%","12%"],"exam_id":11,"question_id":30,"unix_timestamp":1584875220,"timestamp":"2020-03-22 12:07:00"}],"exam":{"numberOfQuestions":319,"isImplemented":true,"lastUpdated":"11 Apr 2025","id":11,"provider":"Google","name":"Professional Data Engineer","isBeta":false,"isMCOnly":true},"currentPage":6},"__N_SSP":true}