{"pageProps":{"questions":[{"id":"nR4J0DUn73LE7LickYCZ","exam_id":13,"answers_community":["D (95%)","5%"],"isMC":true,"answer_ET":"D","question_text":"Your team is building an application for a global bank that will be used by millions of customers. You built a forecasting model that predicts customers' account balances 3 days in the future. Your team will use the results in a new feature that will notify users when their account balance is likely to drop below $25. How should you serve your predictions?","timestamp":"2021-06-09 12:23:00","topic":"1","answer_images":[],"question_images":[],"question_id":241,"url":"https://www.examtopics.com/discussions/google/view/54973-exam-professional-machine-learning-engineer-topic-1-question/","answer":"D","unix_timestamp":1623234180,"discussion":[{"poster":"salsabilsf","comments":[{"content":"Yes, create a topic is overkill but not a NOTIFICATION SYSTEM. it's totally normal.\nSeriously, the step two involves \"REGISTER EACH USER ....\", how is this better than create a topic????\n\nshould be A and it's so obvious!","comments":[{"upvote_count":"3","timestamp":"1647967380.0","poster":"q4exam","comment_id":"449579","content":"I think A is straight forward answer but in real life, customer also consider cost, so practically, app engine will be picked in this case..... because of the large user base"}],"comment_id":"446195","timestamp":"1647476940.0","poster":"Y2Data","upvote_count":"3"}],"content":"Should be D !\n creating a Pub/Sub topic for each user is overkill","comment_id":"378176","timestamp":"1639052580.0","upvote_count":"20"},{"timestamp":"1660963380.0","comment_id":"551528","content":"Selected Answer: D\nD is correct. Firebase is designed for exactly this sort of scenario. Also, it would not be possible to create millions of pubsub topics due to GCP quotas\nhttps://cloud.google.com/pubsub/quotas#quotas\nhttps://firebase.google.com/docs/cloud-messaging","poster":"SlipperySlope","upvote_count":"9"},{"content":"Selected Answer: D\nD) Firebase","timestamp":"1733503680.0","upvote_count":"1","comment_id":"1225579","poster":"PhilipKoku"},{"content":"Selected Answer: D\nD is correct. Firebase is used for applications.","poster":"fragkris","upvote_count":"1","timestamp":"1717569720.0","comment_id":"1088295"},{"comment_id":"950748","upvote_count":"1","timestamp":"1705163340.0","content":"Selected Answer: A\nsimple answer , use tools most mentioned during training . , cloud functions","poster":"harithacML","comments":[{"content":"Pub/Sub has a limit of 10,000 topics only and can't be increased https://cloud.google.com/pubsub/quotas#resource_limits.","poster":"Kowalski","comment_id":"994735","timestamp":"1709187600.0","upvote_count":"3"}]},{"timestamp":"1699513560.0","comment_id":"892721","poster":"M25","content":"Selected Answer: D\nWent with D","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: D\n\"Create a Pub/Sub topic for each user\" xD","poster":"SergioRubiano","comment_id":"853189","timestamp":"1695901200.0"},{"comment_id":"629215","content":"Selected Answer: D\n\"Create a Pub/Sub topic for each user\" this is crazy , we can not imagine a system with millions of pub/sub topics , so A,B wrong\nC also wrong","poster":"Mohamed_Mossad","timestamp":"1673282880.0","upvote_count":"3"},{"content":"Selected Answer: D\nD- is more automated compared to A. A is overkill","upvote_count":"1","timestamp":"1665757560.0","poster":"mmona19","comment_id":"585828"},{"poster":"Vidyasagar","upvote_count":"3","comment_id":"526880","timestamp":"1658159640.0","content":"Selected Answer: D\nI think, D is the best answer"},{"timestamp":"1657997400.0","content":"Project limit is 10,000 topics, you could have multiple projects but that does not scale well. so D.\nhttps://cloud.google.com/pubsub/quotas#resource_limits","upvote_count":"4","poster":"fdmenendez","comment_id":"525256"},{"comment_id":"516264","upvote_count":"3","timestamp":"1656908700.0","poster":"NamitSehgal","content":"D looks more relevant\nNotification messages: Simply display a message content, which is handled by the FCM SDK. Data Messages: Display a message with some set interactions"},{"poster":"Danny2021","timestamp":"1651955160.0","content":"A doesn't work. There is a quota limit on the number of pub/sub topics you can create, also one Cloud function cannot subscribe to millions of topics. A doesn't scale at all.","comment_id":"474055","upvote_count":"3"},{"poster":"Danny2021","content":"Answer is D. FCM is designed for this type of notification sent to mobile and desktop apps.","comment_id":"474053","upvote_count":"4","timestamp":"1651954800.0"}],"answer_description":"","choices":{"D":"1. Build a notification system on Firebase. 2. Register each user with a user ID on the Firebase Cloud Messaging server, which sends a notification when your model predicts that a user's account balance will drop below the $25 threshold.","A":"1. Create a Pub/Sub topic for each user. 2. Deploy a Cloud Function that sends a notification when your model predicts that a user's account balance will drop below the $25 threshold.","C":"1. Build a notification system on Firebase. 2. Register each user with a user ID on the Firebase Cloud Messaging server, which sends a notification when the average of all account balance predictions drops below the $25 threshold.","B":"1. Create a Pub/Sub topic for each user. 2. Deploy an application on the App Engine standard environment that sends a notification when your model predicts that a user's account balance will drop below the $25 threshold."}},{"id":"ElOICwgAYGMV9WENnAL1","topic":"1","unix_timestamp":1623331860,"timestamp":"2021-06-10 15:31:00","answer_ET":"A","answers_community":["A (92%)","8%"],"isMC":true,"answer":"A","answer_images":[],"question_images":[],"answer_description":"","exam_id":13,"question_id":242,"question_text":"You work for an advertising company and want to understand the effectiveness of your company's latest advertising campaign. You have streamed 500 MB of campaign data into BigQuery. You want to query the table, and then manipulate the results of that query with a pandas dataframe in an AI Platform notebook.\nWhat should you do?","choices":{"D":"From a bash cell in your AI Platform notebook, use the bq extract command to export the table as a CSV file to Cloud Storage, and then use gsutil cp to copy the data into the notebook. Use pandas.read_csv to ingest the file as a pandas dataframe.","B":"Export your table as a CSV file from BigQuery to Google Drive, and use the Google Drive API to ingest the file into your notebook instance.","A":"Use AI Platform Notebooks' BigQuery cell magic to query the data, and ingest the results as a pandas dataframe.","C":"Download your table from BigQuery as a local CSV file, and upload it to your AI Platform notebook instance. Use pandas.read_csv to ingest he file as a pandas dataframe."},"discussion":[{"upvote_count":"27","content":"A: no \"CSV\" found in provided link https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas","comment_id":"379051","poster":"zosoabi","timestamp":"1639150260.0"},{"comments":[{"content":"Dude, I laughed so hard","timestamp":"1720083540.0","comment_id":"1113602","poster":"sharth","upvote_count":"3"}],"content":"Selected Answer: A\nA is the google recommended answer. And what you should use\nC is what the intern does ...","timestamp":"1715795940.0","comment_id":"1071835","poster":"Sum_Sum","upvote_count":"6"},{"timestamp":"1733503800.0","upvote_count":"1","comment_id":"1225581","poster":"PhilipKoku","content":"Selected Answer: A\nA) Magic command"},{"comment_id":"892723","upvote_count":"2","timestamp":"1699513560.0","content":"Selected Answer: A\nWent with A","poster":"M25"},{"comment_id":"853191","upvote_count":"1","content":"Selected Answer: A\nA, Using the command %%bigquery df","poster":"SergioRubiano","timestamp":"1695901560.0"},{"upvote_count":"2","content":"Why not D? using BQ notebook magic would be ok for a single time use. but usually a DS would reload the data multiple time, and every time you need to stream 500mb data to the notebook instance from BQ. Isn't it cheaper to store the data as a csv in a bucket?","comment_id":"808926","poster":"Dunnoth","timestamp":"1692047040.0"},{"poster":"John_Pongthorn","comment_id":"776142","timestamp":"1689385320.0","content":"Selected Answer: A\n%%bigquery df\nSELECT name, SUM(number) as count\nFROM `bigquery-public-data.usa_names.usa_1910_current`\nGROUP BY name\nORDER BY count DESC\nLIMIT 3\n\nprint(df.head())","upvote_count":"4"},{"upvote_count":"2","comment_id":"741063","poster":"hiromi","timestamp":"1686403020.0","content":"Selected Answer: A\nA\nhttps://cloud.google.com/bigquery/docs/visualize-jupyter"},{"timestamp":"1671691560.0","upvote_count":"2","content":"Answer : A . Refer to this link for details: https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas\nFirst 2 points talks about querying the data. \nDownload query results to a pandas DataFrame by using the BigQuery Storage API from the IPython magics for BigQuery in a Jupyter notebook.\nDownload query results to a pandas DataFrame by using the BigQuery client library for Python.\nDownload BigQuery table data to a pandas DataFrame by using the BigQuery client library for Python.\nDownload BigQuery table data to a pandas DataFrame by using the BigQuery Storage API client library for Python.","poster":"Sachin2360","comment_id":"620163"},{"comment_id":"614989","poster":"Mohamed_Mossad","timestamp":"1670774700.0","upvote_count":"2","content":"Selected Answer: A\nhttps://googleapis.dev/python/bigquery/latest/magics.html#ipython-magics-for-bigquery"},{"content":"Selected Answer: A\nthis is the simplest and most straightforward way read BQ data into Pandas dataframe.","timestamp":"1666959960.0","comment_id":"593808","upvote_count":"3","poster":"NickNtaken"},{"comments":[{"comment_id":"746570","content":"\"A and C are valid, but C is more difficult than A. they don't ask to be easier so I will go with the more difficult\". WHAAAT?\nGoogle best practices are always: easier > harder. Even they encourage you to skip ML if you don't need ML.","poster":"wish0035","timestamp":"1686861600.0","upvote_count":"2"}],"timestamp":"1665758160.0","content":"Selected Answer: C\nboth A and C is technically correct. C has more manual step and A has less. The question does not ask which requires least effort. so C is clear answer","comment_id":"585839","poster":"mmona19","upvote_count":"1"},{"comment_id":"551529","timestamp":"1660963560.0","content":"Selected Answer: C\nC is the correct answer due to the size of the data. It wouldn't be possible to download it all into an in memory data frame.","poster":"SlipperySlope","upvote_count":"1","comments":[{"upvote_count":"2","content":"500mb of data into a pandas dataframe generally isn't a problem, far from it.","poster":"u_phoria","timestamp":"1672039920.0","comment_id":"622364"}]},{"poster":"ggorzki","timestamp":"1658242560.0","upvote_count":"1","comment_id":"527754","content":"Selected Answer: A\nIPython magics for BigQuery\nhttps://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas"},{"timestamp":"1656912060.0","poster":"NamitSehgal","upvote_count":"1","content":"I agree with A","comment_id":"516298"},{"comment_id":"446196","timestamp":"1647477000.0","upvote_count":"2","content":"Just load it \n\nhttps://googleapis.dev/python/bigquery/latest/magics.html","poster":"Y2Data"}],"url":"https://www.examtopics.com/discussions/google/view/55062-exam-professional-machine-learning-engineer-topic-1-question/"},{"id":"Z9BI25uPiGARlr1GNvnS","question_text":"You are an ML engineer at a global car manufacture. You need to build an ML model to predict car sales in different cities around the world. Which features or feature crosses should you use to train city-specific relationships between car type and number of sales?","discussion":[{"poster":"Paul_Dirac","upvote_count":"22","timestamp":"1627723320.0","content":"C\nhttps://developers.google.com/machine-learning/crash-course/feature-crosses/check-your-understanding","comment_id":"417790"},{"poster":"ebinv2","timestamp":"1626059700.0","upvote_count":"8","comment_id":"404342","content":"C should be the answer"},{"comment_id":"1301611","upvote_count":"1","poster":"desertlotus1211","comments":[{"timestamp":"1729606200.0","poster":"desertlotus1211","comment_id":"1301616","upvote_count":"1","content":"Would C be moe complex than D?"}],"content":"Why not Answer D?","timestamp":"1729606080.0"},{"comments":[],"upvote_count":"2","content":"While i acknowledge the answer is C, It seems wrong to elementwise combine binned lat/lon, as it means there are at least 2 places with the same number in the world, probably more. Not only but by multiplying the binned a values it implies they are ordinal, but they are not ordinal in the same direction, so the relationship on price will be lost (a good example is northern countries tend to be richer, but the east/west relationship isn't defined)","poster":"baimus","comment_id":"1280257","timestamp":"1725780660.0"},{"content":"Selected Answer: C\nC) one feature","comment_id":"1225583","poster":"PhilipKoku","upvote_count":"1","timestamp":"1717685580.0"},{"comments":[{"poster":"rigori","upvote_count":"1","comment_id":"1242125","content":"creating this cross feature is madness from explainability standpoint","timestamp":"1720101660.0"}],"poster":"Sum_Sum","timestamp":"1700078460.0","comment_id":"1071838","upvote_count":"2","content":"C - everything else is madness"},{"timestamp":"1683608760.0","comment_id":"892724","content":"Selected Answer: C\nWent with C","poster":"M25","upvote_count":"1"},{"content":"Selected Answer: C\nhttps://developers.google.com/machine-learning/crash-course/feature-crosses/video-lecture","comment_id":"614993","poster":"Mohamed_Mossad","timestamp":"1654957980.0","upvote_count":"5"},{"poster":"A4M","timestamp":"1643175120.0","content":"C - Answer\nwhen doing feature cross the features need to be binned","upvote_count":"4","comment_id":"532640"},{"timestamp":"1641792660.0","comment_id":"520667","upvote_count":"3","poster":"MK_Ahsan","content":"Selected Answer: C\nhttps://developers.google.com/machine-learning/crash-course/feature-crosses/check-your-understanding\nAnswer C: It needs a feature cross to obtain one feature."},{"timestamp":"1641280920.0","comment_id":"516301","poster":"NamitSehgal","content":"I got with C","upvote_count":"3"},{"poster":"ramen_lover","timestamp":"1636272960.0","upvote_count":"1","content":"\"element-wise product\" sounds like we are not using a feature cross but artificially creating a new column whose values is the \"element-wise product\" of other column values...; i.e., (1, 2, 3) => 1 * 2 * 3 = 6.\nI am not a native English speaker; thus, I might misunderstand the sentence.","comment_id":"473805"},{"upvote_count":"4","poster":"ralf_cc","timestamp":"1625813700.0","comment_id":"402470","comments":[{"poster":"jk73","comment_id":"448047","content":"Cannot be D, Despite Binning is a good idea because it enables the model to learn nonlinear relationships within a single feature; separate latitude and longitude in different feature crosses is not a good one, this separation will prevent the model from learning city-specific sales. A city is the conjunction of latitude and longitude.\n\nIn that order of Ideas Crossing binned latitude with binned longitude enables the model to learn city-specific effects of car type.\n\nI will go for C,\nhttps://developers.google.com/machine-learning/crash-course/feature-crosses/check-your-understanding","timestamp":"1632123600.0","upvote_count":"13","comments":[{"poster":"george_ognyanov","timestamp":"1633785900.0","comment_id":"459643","content":"Damn that was a good explanation. Thank you for writing it out.","upvote_count":"2"}]}],"content":"D - https://developers.google.com/machine-learning/crash-course/feature-crosses/video-lecture"}],"answer":"C","choices":{"B":"One feature obtained as an element-wise product between latitude, longitude, and car type.","C":"One feature obtained as an element-wise product between binned latitude, binned longitude, and one-hot encoded car type.","D":"Two feature crosses as an element-wise product: the first between binned latitude and one-hot encoded car type, and the second between binned longitude and one-hot encoded car type.","A":"Thee individual features: binned latitude, binned longitude, and one-hot encoded car type."},"question_images":[],"unix_timestamp":1625813700,"answer_ET":"C","url":"https://www.examtopics.com/discussions/google/view/57513-exam-professional-machine-learning-engineer-topic-1-question/","exam_id":13,"question_id":243,"answers_community":["C (100%)"],"topic":"1","answer_images":[],"answer_description":"","isMC":true,"timestamp":"2021-07-09 08:55:00"},{"id":"TklJgNWQz3btas7KvU9W","answer":"B","choices":{"A":"Use the AI Platform Training built-in algorithms to create a custom model.","C":"Use the Cloud Natural Language API to extract custom entities for classification.","D":"Build a custom model to identify the product keywords from the transcribed calls, and then run the keywords through a classification algorithm.","B":"Use AutoMlL Natural Language to extract custom entities for classification."},"isMC":true,"timestamp":"2021-06-18 15:03:00","unix_timestamp":1624021380,"answers_community":["B (60%)","C (40%)"],"url":"https://www.examtopics.com/discussions/google/view/55569-exam-professional-machine-learning-engineer-topic-1-question/","question_id":244,"question_images":[],"topic":"1","exam_id":13,"answer_description":"","answer_ET":"B","answer_images":[],"discussion":[{"comment_id":"384841","content":"Should be B\n-> minimize data preprocessing and development time","comments":[{"poster":"sensev","timestamp":"1627656960.0","comment_id":"417490","content":"Agree its B. A and D is incorrect since it requires more development time. C is also incorrect since the product is company specific and might not be well recognized by Cloud Natural Language API.","upvote_count":"6"},{"timestamp":"1630157760.0","poster":"neohanju","comment_id":"433920","content":"I thought the answer is B too. However, after carefully reading the question and answers again, B produces entities for classification only, not a classification result.\nSo, A and D are only candidates and A is better.","upvote_count":"2"}],"poster":"chohan","upvote_count":"23","timestamp":"1624021380.0"},{"poster":"baimus","upvote_count":"5","comment_id":"569800","comments":[{"poster":"giaZ","content":"If anything, C is wrong because it tells you something that is not true: extract custom entities with Natural Language API it's not possible. That is something you can do only with AutoML. Look at this comparison table: https://cloud.google.com/natural-language#section-6\nThat's how they subtly point you at answer B.","upvote_count":"9","timestamp":"1648479240.0","comment_id":"576918"}],"content":"I'm leaning towards C over B here. The question is underlining that minimal development time is required, and C is even less than B. If the information is really domain specific, then you'd need B, but it's not clear what products the company sells, so we don't have enough info to say it's too domain specific for C.","timestamp":"1647528780.0"},{"content":"Selected Answer: C\nI think it's C.\nNatural Language API supports entity extraction, is pre-trained and comes off as a SaaS that you just pay per use.\nConsidering you have already used STT on your data, I see no reason to use AutoML in this case.","timestamp":"1734604560.0","comment_id":"1328940","upvote_count":"1","poster":"theseawillclaim"},{"content":"Selected Answer: C\nCloud NLP API no require custom training","upvote_count":"1","timestamp":"1723784160.0","poster":"misya","comment_id":"1266819"},{"poster":"PhilipKoku","content":"Selected Answer: C\nC) Cloud NLP API","timestamp":"1717685760.0","comment_id":"1225585","upvote_count":"2"},{"upvote_count":"3","comment_id":"1152543","content":"I'm voting C here!","timestamp":"1708174860.0","poster":"21c17b3"},{"upvote_count":"2","timestamp":"1707789480.0","poster":"ralf_cc","content":"AutoML only has classification and regression","comment_id":"1148832"},{"comment_id":"1001650","content":"Selected Answer: C\nKey Differences:\n\nApproach: Option B (AutoML Natural Language) involves using an AutoML service to train a custom NLP model, while Option C (Cloud Natural Language API) relies on a pre-built NLP API.\n\nControl and Customization: Option B gives you more control and customization over the training process, as you train a model specific to your needs. Option C offers less control but is quicker to set up since it uses a pre-built API.\n\nComplexity: Option B might require more technical expertise to set up and configure the AutoML model, while Option C is more straightforward and user-friendly.\n\nIn summary, both options allow you to extract custom entities for classification, but Option B (AutoML) involves more manual involvement in training a custom model, while Option C (Cloud Natural Language API) provides a simpler, pre-built solution","upvote_count":"1","poster":"pico","timestamp":"1694097120.0"},{"content":"Selected Answer: B\nWent with B","timestamp":"1683608820.0","comment_id":"892725","upvote_count":"2","poster":"M25"},{"poster":"lucaluca1982","timestamp":"1681994040.0","comment_id":"875584","upvote_count":"1","comments":[{"content":"you have to classify company products, which are custom classes","poster":"julliet","upvote_count":"1","timestamp":"1684972440.0","comment_id":"906205","comments":[{"comment_id":"1265367","content":"It seems to me that if there is a product name that needs to be learned in AutoML Natural Language, there is a possibility that it cannot be transcribed into text by the Speech-to-Text API in the first place.","upvote_count":"1","timestamp":"1723583100.0","poster":"YushiSato"},{"content":"you can still use Option C (Cloud Natural Language API) even when the solution needs to classify incoming calls by company-specific products rather than general products. The Cloud Natural Language API can be customized to handle company-specific entities and classifications effectively.","upvote_count":"2","comment_id":"1001654","poster":"pico","timestamp":"1694097240.0"}]}],"content":"Selected Answer: C\nwhy not C?"},{"timestamp":"1677576300.0","content":"Selected Answer: B\nAutoML is appropriate to classify incoming calls by product (Custom) to be routed to the correct support team.\n\nCloud Natural Language API is for general case (not particular business)","comment_id":"824585","upvote_count":"1","poster":"John_Pongthorn"},{"comment_id":"615006","upvote_count":"2","content":"Selected Answer: B\n\"minimize data preprocessing and development time\" answer will be limited to B,C\nwill choose C as Natural Language API does not handle custom operation","poster":"Mohamed_Mossad","timestamp":"1654960560.0"},{"upvote_count":"4","poster":"mmona19","content":"B- automl custom classification and entity is going to help with minimum effort.","comment_id":"585843","timestamp":"1649947200.0"},{"comment_id":"527760","poster":"ggorzki","upvote_count":"4","content":"Selected Answer: B\nAutoML Natural Language - custom entities, with least development time","timestamp":"1642611960.0"},{"comments":[{"timestamp":"1652123880.0","content":"no. if you need custom entities you don't use APIs","upvote_count":"1","comment_id":"599242","poster":"David_ml"}],"upvote_count":"2","timestamp":"1641281820.0","comment_id":"516311","poster":"NamitSehgal","content":"Should be B\nBasic classification, entity extraction, and sentiment analysis are available through the Cloud Natural Language API. AutoML Natural Language enables you to define custom classification categories, entities, and sentiment scores that are relevant to your application."}],"question_text":"You work for a large technology company that wants to modernize their contact center. You have been asked to develop a solution to classify incoming calls by product so that requests can be more quickly routed to the correct support team. You have already transcribed the calls using the Speech-to-Text API. You want to minimize data preprocessing and development time. How should you build the model?"},{"id":"MaU87VhThD0sl2BSH2yq","answer":"C","choices":{"B":"Load the data into Cloud Bigtable, and read the data from Bigtable.","C":"Convert the CSV files into shards of TFRecords, and store the data in Cloud Storage.","D":"Convert the CSV files into shards of TFRecords, and store the data in the Hadoop Distributed File System (HDFS).","A":"Load the data into BigQuery, and read the data from BigQuery."},"isMC":true,"timestamp":"2021-07-03 14:15:00","unix_timestamp":1625314500,"answers_community":["C (68%)","A (27%)","5%"],"url":"https://www.examtopics.com/discussions/google/view/57011-exam-professional-machine-learning-engineer-topic-1-question/","question_id":245,"question_images":[],"topic":"1","exam_id":13,"answer_description":"","answer_ET":"C","answer_images":[],"discussion":[{"upvote_count":"26","timestamp":"1641718920.0","poster":"ralf_cc","content":"C - not enough info in the question, but C is the \"most correct\" one","comment_id":"402476"},{"poster":"theseawillclaim","timestamp":"1734604620.0","upvote_count":"1","content":"Selected Answer: C\nIt's C. \nBigTable would usually help with heavy I/O ops, but is not suited for (semi)structured data by design.","comment_id":"1328941"},{"timestamp":"1733511420.0","poster":"PhilipKoku","upvote_count":"2","comment_id":"1225645","content":"Selected Answer: C\nC) The most suitable option for improving input/output execution performance in this scenario is C. Convert the CSV files into shards of TFRecords and store the data in Cloud Storage. This approach leverages the efficiency of TFRecords and the scalability of Cloud Storage, aligning with TensorFlow best practices."},{"content":"Selected Answer: C\nC is the google reccomended approach.","upvote_count":"1","timestamp":"1717571160.0","comment_id":"1088309","poster":"fragkris"},{"timestamp":"1715796480.0","upvote_count":"1","poster":"Sum_Sum","content":"C is the correct one as BQ will not help you with performance","comment_id":"1071840"},{"upvote_count":"2","content":"Selected Answer: C\nC https://datascience.stackexchange.com/questions/16318/what-is-the-benefit-of-splitting-tfrecord-file-into-shards#:~:text=Splitting%20TFRecord%20files%20into%20shards,them%20through%20a%20training%20process.","poster":"peetTech","timestamp":"1712084580.0","comment_id":"1023397"},{"content":"bard: The correct answer is:\n\nC. Convert the CSV files into shards of TFRecords, and store the data in Cloud Storage.\nTFRecords is a TensorFlow-specific binary format that is optimized for performance. Converting the CSV files into TFRecords will improve the input/output execution performance. Sharding the TFRecords will allow the data to be read in parallel, which will further improve performance.\n\nThe other options are not as likely to improve performance.\n\nLoading the data into BigQuery or Cloud Bigtable will add an additional layer of abstraction, which can slow down performance.\nStoring the TFRecords in HDFS is not likely to improve performance, as HDFS is not optimized for TensorFlow.","comment_id":"1010082","upvote_count":"1","poster":"ftl","timestamp":"1710718740.0"},{"comment_id":"975896","content":"Using BigQuery or Bigtable may not be the most efficient option for input/output operations with TensorFlow. Storing the data in HDFS may be an option, but Cloud Storage is generally a more scalable and cost-effective solution.","upvote_count":"1","timestamp":"1707421560.0","poster":"tavva_prudhvi"},{"content":"While Bigtable can offer high-performance I/O capabilities, it is important to note that it is primarily designed for structured data storage and real-time access patterns. In this scenario, the focus is on optimizing input/output execution performance, and using TFRecords in Cloud Storage aligns well with that goal.","poster":"PST21","comment_id":"919031","timestamp":"1702113600.0","upvote_count":"1"},{"comments":[{"timestamp":"1705911000.0","upvote_count":"3","comment_id":"959253","poster":"tavva_prudhvi","content":"BigQuery is designed for running large-scale analytical queries, not for serving input pipelines for machine learning models like TensorFlow. BigQuery's strength is in its ability to handle complex queries over vast amounts of data, but it may not provide the optimal performance for the specific task of feeding data into a TensorFlow model.\n\nOn the other hand, converting the CSV files into shards of TFRecords and storing them in Cloud Storage (Option C) will provide better performance because TFRecords is a format designed specifically for TensorFlow. It allows for efficient storage and retrieval of data, making it a more suitable choice for improving the input/output execution performance. Additionally, Cloud Storage provides high throughput and low-latency data access, which is beneficial for training large-scale TensorFlow models."}],"upvote_count":"2","content":"Selected Answer: A\nA. Load the data into BigQuery, and read the data from BigQuery.\nhttps://cloud.google.com/blog/products/ai-machine-learning/tensorflow-enterprise-makes-accessing-data-on-google-cloud-faster-and-easier\nPrecisely on this link provided in other comments it whos that the best shot with tfrecords is: 18752 Records per second. In the same report it shows that bigquery is morethan 40000 recors per second","comment_id":"915160","poster":"Voyager2","timestamp":"1701768060.0"},{"upvote_count":"2","comment_id":"892727","content":"Selected Answer: C\nWent with C","timestamp":"1699513620.0","poster":"M25"},{"comment_id":"817515","content":"Selected Answer: C\nCloud Bigtable is typically used to process unstructured data, such as time-series data, logs, or other types of data that do not conform to a fixed schema. However, Cloud Bigtable can also be used to store structured data if necessary, such as in the case of a key-value store or a database that does not require complex relational queries.","timestamp":"1692677040.0","upvote_count":"1","poster":"shankalman717"},{"content":"Selected Answer: C\nOption C, converting the CSV files into shards of TFRecords and storing the data in Cloud Storage, is the most appropriate solution for improving input/output execution performance in this scenario","upvote_count":"1","poster":"shankalman717","timestamp":"1692675660.0","comment_id":"817496"},{"upvote_count":"4","timestamp":"1688646600.0","comments":[{"poster":"ShePiDai","comment_id":"900160","upvote_count":"1","content":"agree. BigQuery and Cloud Storage have effectively identical storage performance, where BigQuery is optimised for structured dataset and GCS for unstructured.","timestamp":"1700233080.0"}],"comment_id":"767794","content":"Selected Answer: A\nhttps://cloud.google.com/architecture/ml-on-gcp-best-practices#store-tabular-data-in-bigquery\nBigQuery for structured data, cloud storage for unstructed data","poster":"behzadsw"},{"poster":"Mohamed_Mossad","comment_id":"614107","timestamp":"1670611560.0","comments":[{"poster":"hoai_nam_1512","upvote_count":"2","content":"HDFS will require more resources\n100 bil record is processed fine with Cloud Storage object","timestamp":"1676907300.0","comment_id":"649454"}],"content":"Selected Answer: D\n\"100 billion records stored in several CSV files\" that means we deal with distributed big data problem , so HDFS is very suitable , Will choose D","upvote_count":"1"},{"timestamp":"1668050340.0","upvote_count":"4","poster":"David_ml","content":"Answer is C. TFRecords in cloud storage for big data is the recommended practice by Google for training TF models.","comment_id":"599347"},{"timestamp":"1664094900.0","poster":"giaZ","comment_id":"574941","upvote_count":"3","content":"Selected Answer: C\nGoogle best practices: Use Cloud Storage buckets and directories to group the shards of data (either sharded TFRecord files if using Tensorflow, or Avro if using any other framework). Aim for files of at least 100Mb, and 100 - 10000 shards."},{"upvote_count":"2","content":"It's C, although I do note that: \"A very common case of this practise is to store TF Records in a Hadoop File System or on bucket — based public cloud solutions like Google Cloud Storage.\", but they haven't specified a hadoop cluster is available and performanec from cloud storage will be better.","timestamp":"1663420020.0","comment_id":"569817","poster":"baimus"},{"timestamp":"1658163780.0","poster":"Vidyasagar","upvote_count":"2","content":"Selected Answer: C\nC is the best","comment_id":"526942"},{"timestamp":"1657971840.0","comment_id":"524972","content":"A structured data set cannot be stored into cloud BigTable. So B is ruled out.\nOption A seems to be a redundant operation.\nOption C seems to be the best fit as Cloud Storage is almost equivalent to HDFS in terms of performance!","poster":"DHEEPAK","upvote_count":"2"},{"content":"Looking at this link, my mind seems to be changing from bigqeury to bigtable\nhttps://github.com/tensorflow/tensorflow/issues/20068","poster":"NamitSehgal","comment_id":"516317","timestamp":"1656913920.0","upvote_count":"1"},{"content":"A Bigquery \nbig table for specific schema types , monotonous sensor dat not CSV types \nTfrecird read multiple binary files on GCS","timestamp":"1656734700.0","poster":"NamitSehgal","comment_id":"514850","upvote_count":"2"},{"timestamp":"1652231700.0","content":"I am confused about B or C. But I've searched something. Not sure if it's right but at least some experiences from others: \n1. \"We decided to start training a machine learning model in TensorFlow using the data in Bigtable. Normally, TensorFlow models rely on data in GCS. But one of the perks of Bigtable is its extremely fast read speed, which means you can train a model without having to move data out of Bigtable. This is especially helpful if you’re using an online model, which is constantly updating and learning.\"( https://medium.com/google-cloud/breathing-easy-with-bigtable-b58eb302cc1a)\n2. saeta's answer: https://github.com/tensorflow/tensorflow/issues/20068","comment_id":"475950","upvote_count":"1","poster":"vivid_cucumber"},{"upvote_count":"1","poster":"gcp2021go","timestamp":"1651200420.0","comment_id":"469562","content":"keyword of this question: tensorflow model, structure data, 100billion records in several CSV, improve input/output performance. I can see the confusion between ABC, however, since it's Tensorflow model, a good mental model will be using TFrecord, as it is the most scalable solution for TF model"},{"poster":"pddddd","content":"https://cloud.google.com/architecture/data-preprocessing-for-ml-with-tf-transform-pt2","timestamp":"1648365960.0","upvote_count":"1","comment_id":"452226"},{"comment_id":"446199","content":"https://cloud.google.com/blog/products/ai-machine-learning/tensorflow-enterprise-makes-accessing-data-on-google-cloud-faster-and-easier\n\nTFRecords on GCS","upvote_count":"2","timestamp":"1647477540.0","poster":"Y2Data"},{"timestamp":"1646795520.0","content":"C. Sharded TFRecords (TFRecord is stored as binary using Protocol Buffer) allow the shards to read / write in parallel, thus improve input/output IOs dramatically.","upvote_count":"2","poster":"Danny2021","comment_id":"441705"},{"comment_id":"438952","content":"I'm not sure that Bigtable is the correct option as it's a NoSQL DB which is truely efficient when you have to query data by row keys\n\nAs the data is structured and the model will need to query billions of data, I suppose Bigquery would be a good fit. I'm going with A","timestamp":"1646383740.0","upvote_count":"2","poster":"dxxdd7"},{"content":"Cloud Storage is for unstructured data set, so it's B","poster":"Jijiji","upvote_count":"2","timestamp":"1645841280.0","comment_id":"431820","comments":[{"content":"I don't think it matters, once you have converted you \"structured\" CSV into TFRecords shards. \nI'd still go with C.","poster":"giaZ","comment_id":"563392","upvote_count":"1","timestamp":"1662647580.0"}]},{"comments":[{"content":"Agree, BigTable is ms level","comment_id":"449587","timestamp":"1647968220.0","upvote_count":"1","poster":"q4exam"}],"comment_id":"424205","timestamp":"1644753600.0","upvote_count":"2","poster":"ori5225","content":"Shoud be B\nFor structured data, Bigtable or Bigquery is suitable\nBigtable is faster than Bigquery so B is the answer"},{"poster":"nunzio144","timestamp":"1642243080.0","comment_id":"406925","content":"Bigtable is faster than bigquery ... answer b ?","upvote_count":"2"},{"comments":[],"timestamp":"1641219300.0","content":"Shouldn't it be with Bigquery? therefore answer A?","upvote_count":"2","comment_id":"397559","poster":"burnout"}],"question_text":"You are training a TensorFlow model on a structured dataset with 100 billion records stored in several CSV files. You need to improve the input/output execution performance. What should you do?"}],"exam":{"isImplemented":true,"id":13,"lastUpdated":"11 Apr 2025","name":"Professional Machine Learning Engineer","isMCOnly":true,"numberOfQuestions":304,"provider":"Google","isBeta":false},"currentPage":49},"__N_SSP":true}