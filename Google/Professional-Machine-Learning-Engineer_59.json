{"pageProps":{"questions":[{"id":"wlkUPpTlWEd8L8IEdHeb","question_images":[],"answers_community":["A (72%)","C (28%)"],"choices":{"B":"Read the data from BigQuery using Dataproc, and run several models using SparkML.","A":"Use BigQuery ML to run several regression models, and analyze their performance.","C":"Use Vertex AI Workbench user-managed notebooks with scikit-learn code for a variety of ML algorithms and performance metrics.","D":"Train a custom TensorFlow model with Vertex AI, reading the data from BigQuery featuring a variety of ML algorithms."},"answer":"A","isMC":true,"question_text":"You have been given a dataset with sales predictions based on your company’s marketing activities. The data is structured and stored in BigQuery, and has been carefully managed by a team of data analysts. You need to prepare a report providing insights into the predictive capabilities of the data. You were asked to run several ML models with different levels of sophistication, including simple models and multilayered neural networks. You only have a few hours to gather the results of your experiments. Which Google Cloud tools should you use to complete this task in the most efficient and self-serviced way?","discussion":[{"comment_id":"1360406","timestamp":"1740284700.0","content":"Selected Answer: A\nnot enough time, data already in big query therefore BQML","upvote_count":"1","poster":"bc3f222"},{"poster":"Werner123","comment_id":"1162344","upvote_count":"4","content":"Selected Answer: A\nYou only have a few hours. The dataset is in BQ. The dataset is carefully managed. BQML it is.","timestamp":"1724913060.0"},{"poster":"ludovikush","timestamp":"1724833920.0","comment_id":"1161520","upvote_count":"1","content":"Selected Answer: C\nI agree with pico answer"},{"poster":"iieva","timestamp":"1721550660.0","content":"Selected Answer: A\nAll deep neural networks are multilayered neural networks, but not all multilayered neural networks are necessarily deep. The term \"deep\" is used to emphasize the depth of the network in the context of having many hidden layers, which has been shown to be effective for learning hierarchical representations of complex patterns in data.\n\nHence BQ allows creation of DNNs (https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-dnn-models) it should be A.","upvote_count":"3","comment_id":"1127751"},{"upvote_count":"3","poster":"pico","content":"Selected Answer: C\nVertex AI Workbench provides user-managed notebooks that allow you to run Python code using libraries like scikit-learn, TensorFlow, and more.\nYou can easily connect to your BigQuery dataset from within the notebook, extract the data, and perform data preprocessing.\nYou can then experiment with different ML algorithms available in scikit-learn and track performance metrics.\nIt provides flexibility, control, and the ability to run various models quickly.","timestamp":"1710325320.0","comment_id":"1006363","comments":[{"timestamp":"1710325440.0","comment_id":"1006365","content":"Not A. \nBigQuery ML is convenient for quick model training and predictions within BigQuery itself, but it has limitations in terms of the variety of ML algorithms and customization options it offers.\nIt may not be the best choice for running more sophisticated ML models or extensive experiments.\n\nand It only said regression model","poster":"pico","upvote_count":"2"}]},{"upvote_count":"1","content":"Selected Answer: C\nI think multilayered neural networks need to be trained externally from BQ ML as stated here:\nhttps://cloud.google.com/bigquery/docs/bqml-introduction","timestamp":"1707209340.0","comments":[{"comment_id":"974353","upvote_count":"1","timestamp":"1707282000.0","poster":"MTTTT","content":"nvm you can import DNN in BQ"}],"poster":"MTTTT","comment_id":"973587"},{"timestamp":"1704671760.0","content":"Selected Answer: A\nAccording to the question, you don't have enough time. B, C, D need much more time to set up the service, or write the code. Also the data is already in BigQuery. BQML should be the fastest way. Besides, BQML supports xgboost, NN models as well.","poster":"SamuelTsch","comment_id":"946019","upvote_count":"2"},{"timestamp":"1703719680.0","poster":"Jarek7","comment_id":"935851","upvote_count":"2","content":"Selected Answer: C\nThe question says that \"You were asked to run several ML models with different levels of sophistication, including simple models and multilayered neural networks\" BQ ML doesn't allow this. BQ ML provides only simple regression/categorization models. It is not about training these \"sophisticated models\" but only run them, so you can easly do it within few hours with notebooks."},{"timestamp":"1699514400.0","upvote_count":"2","poster":"M25","content":"Selected Answer: A\nWent with A","comment_id":"892776"},{"upvote_count":"1","timestamp":"1698642720.0","poster":"lucaluca1982","comments":[{"poster":"tavva_prudhvi","timestamp":"1706123100.0","upvote_count":"1","content":"However, given the limited time constraint of a few hours and the fact that the data is already stored in BigQuery, option A is more efficient.\n\nBigQuery ML allows you to quickly create and evaluate ML models directly within BigQuery, without the need to move the data or set up a separate environment. This makes it faster and more convenient for running several regression models and analyzing their performance within the given time frame.","comment_id":"961850"}],"comment_id":"884843","content":"Selected Answer: C\nC allows to execute more complex tests"},{"comment_id":"815999","poster":"FherRO","upvote_count":"2","content":"Selected Answer: A\nB,C,D requires coding. You only have some hours, A is the fastest.","timestamp":"1692565920.0"},{"content":"Selected Answer: A\nI vote for A","comment_id":"749704","poster":"hiromi","timestamp":"1687161960.0","upvote_count":"3"},{"content":"Selected Answer: A\nIt's A.","comment_id":"745041","timestamp":"1686737880.0","poster":"ares81","upvote_count":"2"},{"comment_id":"742014","content":"Selected Answer: A\nI will go with A, since it's the fastest way to do it. Custom training in Vertex AI requires time and writing scikit-learn models in notebooks too","upvote_count":"2","poster":"LearnSodas","timestamp":"1686503220.0"}],"question_id":291,"url":"https://www.examtopics.com/discussions/google/view/91065-exam-professional-machine-learning-engineer-topic-1-question/","exam_id":13,"answer_images":[],"answer_description":"","answer_ET":"A","timestamp":"2022-12-11 20:07:00","topic":"1","unix_timestamp":1670785620},{"id":"lZwRQg8qWkQZz1TQa8wU","question_text":"You are an ML engineer at a bank. You have developed a binary classification model using AutoML Tables to predict whether a customer will make loan payments on time. The output is used to approve or reject loan requests. One customer’s loan request has been rejected by your model, and the bank’s risks department is asking you to provide the reasons that contributed to the model’s decision. What should you do?","answers_community":["A (93%)","7%"],"exam_id":13,"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/91131-exam-professional-machine-learning-engineer-topic-1-question/","discussion":[{"poster":"shankalman717","timestamp":"1724405580.0","content":"Selected Answer: A\nTo access local feature importance in AutoML Tables, you can use the \"Explain\" feature, which shows the contribution of each feature to the prediction for a specific example. This will help you identify the most important features that contributed to the loan request being rejected.\n\nOption B, using the correlation with target values in the data summary page, may not provide the most accurate explanation as it looks at the overall correlation between the features and target variable, rather than the contribution of each feature to a specific prediction.\n\nOption C, using the feature importance percentages in the model evaluation page, may not provide a sufficient explanation for the specific prediction, as it shows the importance of each feature across all predictions, rather than for a specific prediction.\n\nOption D, varying features independently to identify the threshold per feature that changes the classification, is not recommended as it can be time-consuming and does not provide a clear explanation for why the loan request was rejected","upvote_count":"11","comment_id":"819117"},{"upvote_count":"1","timestamp":"1731136860.0","comment_id":"892778","content":"Selected Answer: A\nWent with A","poster":"M25"},{"upvote_count":"4","poster":"JamesDoe","timestamp":"1727520300.0","content":"Selected Answer: A\nLocal, not global since they asked about one specific prediction.\nCheck out that section on this blog: https://cloud.google.com/blog/products/ai-machine-learning/explaining-model-predictions-structured-data/\nCool stuff!","comment_id":"853119"},{"upvote_count":"2","timestamp":"1727097060.0","poster":"tavva_prudhvi","comment_id":"848353","content":"Local feature importance can provide insight into the specific features that contributed to the model's decision for a particular instance. This information can be used to explain the model's decision to the bank's risks department and potentially identify any issues or biases in the model. Option B is not applicable as the loan request has already been rejected by the model, so there are no target values to correlate with. Option C may provide some insights, but local feature importance will provide more specific information for this particular instance. Option D involves changing the features, which may not be feasible or ethical in this case."},{"upvote_count":"1","content":"C seems more apt & exhaustive to explain for bank's purpose; it uses various Feature Attribution methods.\nA explains how much each feature added to or subtracted from the result as compared with the baseline prediction score; indicative, but less optimal for the purpose at hand","poster":"Yajnas_arpohc","comment_id":"841922","timestamp":"1726564620.0"},{"content":"Selected Answer: A\nit's think is more easy to explain with feature importance","poster":"enghabeth","timestamp":"1723167480.0","upvote_count":"2","comment_id":"802800"},{"content":"Selected Answer: C\nAutoML Tables tells you how much each feature impacts this model. It is shown in the Feature importance graph. The values are provided as a percentage for each feature: the higher the percentage, the more strongly that feature impacted model training. C.","timestamp":"1720083120.0","upvote_count":"1","poster":"ares81","comment_id":"765509"},{"poster":"hiromi","upvote_count":"2","timestamp":"1718784780.0","comment_id":"749710","content":"Selected Answer: A\nA\nhttps://cloud.google.com/automl-tables/docs/explain#local"},{"content":"Selected Answer: A\nAgree with A. \n\"Local feature importance gives you visibility into how the individual features in a specific prediction request affected the resulting prediction.\nEach local feature importance value shows only how much the feature affected the prediction for that row. To understand the overall behavior of the model, use model feature importance.\"\nhttps://cloud.google.com/automl-tables/docs/explain#local","timestamp":"1718622360.0","poster":"mil_spyro","upvote_count":"4","comment_id":"748062"},{"comment_id":"745054","comments":[{"content":"Can you tell the feature importance for a specific prediction?","poster":"tavva_prudhvi","upvote_count":"2","comment_id":"848352","timestamp":"1727097000.0"}],"timestamp":"1718360880.0","upvote_count":"1","poster":"ares81","content":"Selected Answer: C\n\"Feature importance: AutoML Tables tells you how much each feature impacts this model. It is shown in the Feature importance graph. The values are provided as a percentage for each feature: the higher the percentage, the more strongly that feature impacted model training.\" The correct answer is C."},{"timestamp":"1718242260.0","content":"Selected Answer: A\nShould be A. it is specific to this example. so use local feature importance","poster":"YangG","upvote_count":"2","comment_id":"743559"},{"poster":"ares81","content":"It seems C, to me.","upvote_count":"1","comment_id":"742534","timestamp":"1718171880.0"}],"unix_timestamp":1670831880,"timestamp":"2022-12-12 08:58:00","question_images":[],"choices":{"B":"Use the correlation with target values in the data summary page.","D":"Vary features independently to identify the threshold per feature that changes the classification.","A":"Use local feature importance from the predictions.","C":"Use the feature importance percentages in the model evaluation page."},"answer":"A","answer_ET":"A","isMC":true,"answer_images":[],"topic":"1","question_id":292},{"id":"p2LtSdXjH4JGGBtUWfWB","answer":"C","url":"https://www.examtopics.com/discussions/google/view/91459-exam-professional-machine-learning-engineer-topic-1-question/","unix_timestamp":1670946420,"question_images":[],"choices":{"D":"Use the What-If tool in Google Cloud to determine how your model will perform when individual features are excluded. Rank the feature importance in order of those that caused the most significant performance drop when removed from the model.","C":"Use the AI Explanations feature on AI Platform. Submit each prediction request with the ‘explain’ keyword to retrieve feature attributions using the sampled Shapley method.","B":"Stream prediction results to BigQuery. Use BigQuery’s CORR(X1, X2) function to calculate the Pearson correlation coefficient between each feature and the target variable.","A":"Use AI Platform notebooks to perform a Lasso regression analysis on your model, which will eliminate features that do not provide a strong signal."},"exam_id":13,"question_id":293,"isMC":true,"answer_images":[],"timestamp":"2022-12-13 16:47:00","answer_ET":"C","answers_community":["C (95%)","5%"],"answer_description":"","topic":"1","discussion":[{"comment_id":"1150653","content":"Selected Answer: C\nVertex AI Explanations went with C","poster":"SubbuJV","timestamp":"1723674960.0","upvote_count":"5"},{"content":"Selected Answer: C\nFeature Attributions for Individual Predictions\nDirect and Efficient","comment_id":"1358075","timestamp":"1739842380.0","poster":"NamitSehgal","upvote_count":"1"},{"timestamp":"1699514460.0","poster":"M25","upvote_count":"2","comment_id":"892780","content":"Selected Answer: C\nWent with C"},{"timestamp":"1699443180.0","upvote_count":"1","poster":"CloudKida","content":"Selected Answer: C\nhttps://cloud.google.com/ai-platform/prediction/docs/ai-explanations/overview\nAI Explanations helps you understand your model's outputs for classification and regression tasks. Whenever you request a prediction on AI Platform, AI Explanations tells you how much each feature in the data contributed to the predicted result.","comment_id":"891971"},{"content":"Key words in question \"for each prediction served\" - that make its C\nD is more of a broader analysis activity","comment_id":"841842","poster":"Yajnas_arpohc","upvote_count":"3","timestamp":"1694935320.0"},{"timestamp":"1690458660.0","upvote_count":"1","content":"Selected Answer: C\nYou have to use a flagship native service as much as possible.","poster":"John_Pongthorn","comment_id":"789649"},{"comments":[{"poster":"mil_spyro","content":"This is from the doc you provided:\n\"Feature attribution is supported for all types of models (both AutoML and custom-trained), frameworks (TensorFlow, scikit, XGBoost), and modalities (images, text, tabular, video).\"\nhttps://cloud.google.com/vertex-ai/docs/explainable-ai/overview#supported_model_types_2","comment_id":"750067","upvote_count":"3","timestamp":"1687187220.0","comments":[{"timestamp":"1687550220.0","content":"Sorry, i tink C is the answer. Tks","upvote_count":"2","comment_id":"754565","poster":"hiromi"},{"comment_id":"754156","upvote_count":"1","poster":"hiromi","content":"Sorry, I mean Shapley method doesn't support TensorFlow Models\nSee https://cloud.google.com/vertex-ai/docs/explainable-ai/overview#compare-methods","timestamp":"1687513080.0"}]}],"comment_id":"749722","timestamp":"1687163400.0","upvote_count":"1","poster":"hiromi","content":"Selected Answer: D\nI vote for D\n- https://www.tensorflow.org/tensorboard/what_if_tool\n- https://pair-code.github.io/what-if-tool/\n- https://medium.com/red-buffer/tensorflows-what-if-tool-c52914ea215c\nC is wrong cuz AI Explanation dosen't work for TensorFlow models (https://cloud.google.com/vertex-ai/docs/explainable-ai/overview)"},{"content":"Selected Answer: C\nAI Explanations provides feature attributions using the sampled Shapley method, which can help you understand how much each feature contributes to a model's prediction.","poster":"mil_spyro","comment_id":"748057","upvote_count":"4","timestamp":"1686999540.0"},{"poster":"ares81","timestamp":"1686738660.0","upvote_count":"2","comment_id":"745058","content":"Selected Answer: C\nAI Explanations helps you understand your model's outputs for classification and regression tasks. Whenever you request a prediction on AI Platform, AI Explanations tells you how much each feature in the data contributed to the predicted result.\" It's C!"},{"poster":"JeanEl","upvote_count":"2","timestamp":"1686664020.0","content":"Selected Answer: C\nAgree with C","comment_id":"744228"}],"question_text":"You work for a magazine distributor and need to build a model that predicts which customers will renew their subscriptions for the upcoming year. Using your company’s historical data as your training set, you created a TensorFlow model and deployed it to AI Platform. You need to determine which customer attribute has the most predictive power for each prediction served by the model. What should you do?"},{"id":"6beLrcA4y9STNKlZT7ud","isMC":true,"answer_images":[],"question_id":294,"answers_community":["C (100%)"],"choices":{"B":"Use the gcloud command-line tool to submit training jobs on AI Platform when you update your code.","D":"Create an automated workflow in Cloud Composer that runs daily and looks for changes in code in Cloud Storage using a sensor.","C":"Use Cloud Build linked with Cloud Source Repositories to trigger retraining when new code is pushed to the repository.","A":"Use Cloud Functions to identify changes to your code in Cloud Storage and trigger a retraining job."},"answer":"C","unix_timestamp":1624037280,"timestamp":"2021-06-18 19:28:00","exam_id":13,"answer_description":"","answer_ET":"C","discussion":[{"timestamp":"1626608040.0","comments":[{"comment_id":"441843","upvote_count":"3","content":"I think B might be make sense if they have compute concern, there might be many version change but not all that you want to trigger compute","timestamp":"1631175720.0","poster":"q4exam"}],"content":"ANS:C\n\nCI/CD for Kubeflow pipelines.\n\nAt the heart of this architecture is Cloud Build, infrastructure. Cloud Build can import source from Cloud Source Repositories, GitHub, or Bitbucket, and then execute a build to your specifications, and produce artifacts such as Docker containers or Python tar files.","comment_id":"408894","upvote_count":"28","poster":"celia20200410"},{"comment_id":"384960","timestamp":"1624037280.0","poster":"chohan","content":"Should be C\nhttps://cloud.google.com/architecture/architecture-for-mlops-using-tfx-kubeflow-pipelines-and-cloud-build#cicd_architecture","upvote_count":"10"},{"content":"Selected Answer: C\nReq :frequently rerun training + minimise computation costs + 0 manual intervention + version control for your code\nA. Use Cloud Functions to identify changes to your code in Cloud Storage and trigger a retraining job. : No version control\nB. Use the gcloud command-line tool to submit training jobs on AI Platform when you update your code. : Needs manual intervention to gcloud cli code submission\nC. Use Cloud Build linked with Cloud Source Repositories to trigger retraining when new code is pushed to the repository. Yes, connects to github like Vcontrols, automated=0 manual intervention + can initiate upon code changes + cost(not sure compared to other options)\nD. Create an automated workflow in Cloud Composer that runs daily and looks for changes in code in Cloud Storage using a sensor. : Sensor?? too much . also none of req meets.","upvote_count":"1","timestamp":"1727240580.0","comment_id":"946845","poster":"harithacML"},{"content":"Selected Answer: C\nC) It is the only answer with version control.","timestamp":"1717648320.0","comment_id":"1225138","poster":"PhilipKoku","upvote_count":"1"},{"upvote_count":"1","comment_id":"1124272","content":"I mean C is indeed the most logical, but i do not see anything relevant to cost concern. Anyone has any explanation?","timestamp":"1705414200.0","poster":"HaiMinhNguyen"},{"content":"Selected Answer: C\nWent with C","timestamp":"1683608100.0","comment_id":"892682","upvote_count":"1","poster":"M25"},{"upvote_count":"1","content":"Selected Answer: C\nC follows a best practice, B is a manual step","comment_id":"855775","timestamp":"1680181140.0","poster":"fredcaram"},{"content":"Selected Answer: C\nC is the correct answer, it's the Google recommended approach;\nChecking for changes in code without using Cloud Source Repository is a bad choice, so no A and B;\nCloud Composer is an overkill, so no D.","upvote_count":"1","poster":"EFIGO","timestamp":"1669213800.0","comment_id":"725194"},{"poster":"abhi0706","upvote_count":"1","content":"Answer is C","timestamp":"1667325660.0","comment_id":"709350"},{"poster":"GCP72","timestamp":"1660566360.0","comment_id":"647176","content":"Selected Answer: C\nCorrect answer is \"C\"","upvote_count":"2"},{"upvote_count":"1","poster":"Mohamed_Mossad","comment_id":"615323","content":"C is the best answer because \"having version control for your code\"","timestamp":"1655038440.0"},{"upvote_count":"3","comment_id":"557786","timestamp":"1646015400.0","content":"Selected Answer: C\nCommunity vote","poster":"caohieu04"},{"upvote_count":"1","content":"C cloudbuild","poster":"NamitSehgal","comment_id":"518697","timestamp":"1641519960.0"},{"comment_id":"500068","upvote_count":"1","timestamp":"1639320480.0","content":"B is definitely wrong because it will require manual intervention.Question specifically states the objective of minimal manual intervention. C is the way to go.","poster":"ashii007"},{"poster":"alphard","timestamp":"1638787260.0","upvote_count":"1","comment_id":"495053","content":"My answer is C.\n\nCi/CD/CT is executed in Cloud Build."},{"content":"C is correct","upvote_count":"1","comment_id":"464113","timestamp":"1634559600.0","poster":"mousseUwU"},{"timestamp":"1631338680.0","upvote_count":"2","comment_id":"442832","content":"C\n\nCloud Build + Source Repository triggers for CI/CD","poster":"gcper"}],"question_text":"You are developing ML models with AI Platform for image segmentation on CT scans. You frequently update your model architectures based on the newest available research papers, and have to rerun training on the same dataset to benchmark their performance. You want to minimize computation costs and manual intervention while having version control for your code. What should you do?","question_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/google/view/55580-exam-professional-machine-learning-engineer-topic-1-question/"},{"id":"6WkcwuT4T7rqZhbBn9Fo","answer_ET":"A","unix_timestamp":1670838360,"url":"https://www.examtopics.com/discussions/google/view/91150-exam-professional-machine-learning-engineer-topic-1-question/","exam_id":13,"isMC":true,"answers_community":["A (52%)","C (36%)","12%"],"answer_images":[],"answer":"A","answer_description":"","question_images":[],"question_id":295,"timestamp":"2022-12-12 10:46:00","discussion":[{"poster":"tavva_prudhvi","comment_id":"848361","content":"Selected Answer: A\nIn this scenario, the dataset is highly imbalanced, where most of the examples do not have the company's logo. Therefore, accuracy could be misleading as the model can have high accuracy by simply predicting that all images do not have the logo. F1 score is a good metric to consider in such cases, as it takes both precision and recall into account. However, since the dataset is highly skewed, we should weigh recall more than precision to ensure that the model is correctly identifying the images that do have the logo. Therefore, F-score where recall is weighed more than precision is the best metric to evaluate the performance of the model in this scenario. Option B (RMSE) is not applicable to this classification problem, and option D (F-score where precision is weighed more than recall) is not suitable for highly skewed datasets.","timestamp":"1679584560.0","upvote_count":"12"},{"timestamp":"1687906020.0","poster":"Jarek7","upvote_count":"5","comment_id":"935889","content":"Selected Answer: C\nYeah, I know - everyone is voting A... To be honest I still don't understand why are you more affraid of these few FNs than FPs. In my opinion they are exactly same evil. Every documantation says that F1 is great on skewed data. You should use weighted F1 when you know what is worse for you FNs or FPs. In this case we have no any hints on it, so I would stay with ordinary F1."},{"timestamp":"1735446540.0","content":"Selected Answer: C\nDepending on the +ve class, among A & D, more likely A\nthough most important thing is penalty for misclassifying which s missing therefore going with F1, the safest choice","upvote_count":"1","poster":"Ankit267","comment_id":"1333318"},{"content":"Selected Answer: C\nF1-score: Harmonic mean of precision and recall. It ensures that both fasle positive and false negatives are considerd.\nF1- focusing on recall may be useful if missing a logo is more costly than incorrectly identifying one.","poster":"jkkim_jt","comment_id":"1302189","timestamp":"1729713240.0","upvote_count":"2"},{"poster":"gscharly","upvote_count":"3","content":"Selected Answer: C\nI'd go with C. We don't know which option (less FP or less FN) is most important for business with the provided information, so we should seek a balance.","comment_id":"1199024","timestamp":"1713602520.0"},{"comment_id":"1163665","poster":"etienne0","timestamp":"1709316600.0","content":"Selected Answer: D\nI think it's D.","upvote_count":"1"},{"upvote_count":"3","comment_id":"1129138","content":"Selected Answer: D\nI think it could be D, but the question does not provide enough information for this.\n\nI have this feeling: If 4% have the logo, we are looking just for these ones, right? So, the 'quality of TP,' that's it, the precision, could be more interesting because we want a model that we can rely on. So, when this model Predict a image with logo, we`ll be more certain about it. \n\nIf we use recall, for example, a model with 99% recall has more chance of getting the logo, but we won't have quality in this. This model could suggest a lot of images without logo. It is better to use any ML than this...","timestamp":"1705971180.0","poster":"guilhermebutzke"},{"upvote_count":"3","poster":"pico","comment_id":"1070556","content":"Selected Answer: C\nboth option A (F-score with higher weight on recall) and option C (F1 score) could be suitable depending on the specific priorities and requirements of your classification problem. If missing a company's logo is considered more problematic than having false alarms, then option A might be preferred. The F1 score (option C) is a balanced measure that considers both precision and recall, which is generally a good choice in imbalanced datasets.\n\nUltimately, the choice between option A and option C depends on the specific goals and constraints of your application.","timestamp":"1699975440.0"},{"timestamp":"1699806180.0","content":"Selected Answer: C\nThe question not have clear preference for recall or precision hence going with C","poster":"Mickey321","upvote_count":"4","comment_id":"1068659"},{"comment_id":"918138","poster":"Voyager2","content":"Selected Answer: A\nA. F-score where recall is weighed more than precision\nEven a model which always says that don't have the logo will have a good precision because is the most common. What we need is improve recall.","timestamp":"1686220800.0","upvote_count":"2"},{"poster":"M25","upvote_count":"2","content":"Selected Answer: A\nWent with A","comment_id":"892781","timestamp":"1683609660.0"},{"comment_id":"820849","poster":"guilhermebutzke","upvote_count":"3","timestamp":"1677265260.0","content":"Selected Answer: A\nI think is A. The positive Class is the minority. So, it's more important to correctly detect logos in all images that have logo (recall) than correctly detect logos in images classified with logos (precision)."},{"timestamp":"1675914300.0","upvote_count":"3","comment_id":"802804","poster":"enghabeth","content":"Selected Answer: A\nI think is D becouse u try detect TP then it's more important recall than precision"},{"comment_id":"772629","timestamp":"1673447340.0","poster":"ares81","upvote_count":"1","content":"Selected Answer: A\nAnswer A is my choice."},{"upvote_count":"1","poster":"Abhijat","timestamp":"1672362840.0","content":"A is correct","comment_id":"761589"},{"comment_id":"759977","poster":"Dataspire","content":"Selected Answer: A\nless logo images. Recall should be weighted more","timestamp":"1672239540.0","upvote_count":"3"},{"timestamp":"1672133880.0","upvote_count":"4","comment_id":"758322","content":"I think A. \nIf D were the answer, the threshold would be set higher to increase PRECISION, but the low percentage of positives (4%) would allow RECALL to be extremely low. If the percentage of positives is low, greater weight should be given to RECALL.\nhttps://medium.com/@douglaspsteen/beyond-the-f-1-score-a-look-at-the-f-beta-score-3743ac2ef6e3","poster":"kn29"},{"poster":"egdiaa","timestamp":"1671868020.0","comment_id":"754757","upvote_count":"5","content":"Answer C: F1-Score is the best for imbalanced Data like this case: https://stephenallwright.com/imbalanced-data-metric/"},{"timestamp":"1671834000.0","upvote_count":"2","comment_id":"754577","poster":"hiromi","content":"Selected Answer: D\nD (not sure)"},{"comment_id":"742623","upvote_count":"1","content":"There are way less logo images. Recall seems the right way to go. A.","poster":"ares81","timestamp":"1670838360.0"}],"question_text":"You are working on a binary classification ML algorithm that detects whether an image of a classified scanned document contains a company’s logo. In the dataset, 96% of examples don’t have the logo, so the dataset is very skewed. Which metrics would give you the most confidence in your model?","choices":{"C":"F1 score","B":"RMSE","A":"F-score where recall is weighed more than precision","D":"F-score where precision is weighed more than recall"},"topic":"1"}],"exam":{"provider":"Google","isBeta":false,"isImplemented":true,"id":13,"name":"Professional Machine Learning Engineer","numberOfQuestions":304,"lastUpdated":"11 Apr 2025","isMCOnly":true},"currentPage":59},"__N_SSP":true}