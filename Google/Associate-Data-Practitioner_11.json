{"pageProps":{"questions":[{"id":"skDrfGLk6c7cN1bMrkit","answers_community":["A (67%)","D (33%)"],"answer_description":"","discussion":[{"comment_id":"1365683","content":"Selected Answer: A\nThe best option is A. Use AEAD functions and delete keys when employees leave the organization. Option A is best because AEAD encryption makes data values unreadable by design when keys are deleted, directly meeting the requirement. Option B (Dynamic masking) is incorrect because masking only hides data, not making it unreadable at the storage level. Option C (CMEK) is less ideal than AEAD for this specific need, as it's broader encryption and potentially more disruptive. Option D (Column-level access control) is incorrect because it restricts access, but data remains readable for authorized users, not inherently unreadable. Therefore, Option A is the most precise solution for rendering data values unreadable upon employee departure.","upvote_count":"1","poster":"n2183712847","timestamp":"1741230600.0"},{"timestamp":"1740701280.0","content":"Selected Answer: D\nYou don't delete data record keys (row, tags, columns, KMS, etc..) when an employee leaves the company as that will make the data unusable for everyone who remains. That means that A and C are plain wrong. Using policy tags and removing viewer access to data tagged as sensitive is the easiest answer.","upvote_count":"2","comment_id":"1362778","poster":"rich_maverick"},{"content":"Selected Answer: A\nWith AEAD, we can target specific fields and ensure that only sensitive data is affected when an employee leaves, which is more efficient than locking down the entire dataset using CMEK.","poster":"SaquibHerman","timestamp":"1739984760.0","upvote_count":"2","comment_id":"1358863"},{"poster":"a_vi","timestamp":"1738043700.0","upvote_count":"1","content":"Selected Answer: A\nAEAD looks the best option here.","comment_id":"1347745"}],"answer":"A","question_text":"Your organization stores highly personal data in BigQuery and needs to comply with strict data privacy regulations. You need to ensure that sensitive data values are rendered unreadable whenever an employee leaves the organization. What should you do?","unix_timestamp":1738043700,"timestamp":"2025-01-28 06:55:00","exam_id":2,"choices":{"B":"Use dynamic data masking and revoke viewer permissions when employees leave the organization.","C":"Use customer-managed encryption keys (CMEK) and delete keys when employees leave the organization.","D":"Use column-level access controls with policy tags and revoke viewer permissions when employees leave the organization.","A":"Use AEAD functions and delete keys when employees leave the organization."},"isMC":true,"url":"https://www.examtopics.com/discussions/google/view/155530-exam-associate-data-practitioner-topic-1-question-55/","question_id":51,"question_images":[],"answer_images":[],"answer_ET":"A","topic":"1"},{"id":"FQCT95yZ4LhkaVLymxoG","question_images":[],"url":"https://www.examtopics.com/discussions/google/view/157627-exam-associate-data-practitioner-topic-1-question-56/","discussion":[{"upvote_count":"1","timestamp":"1741230900.0","poster":"n2183712847","content":"Selected Answer: C\nThe best option is C. Evaluate data drift. Option C is best because data drift specifically refers to changes in data distribution over time, directly indicating if the current serving data differs from the historical data the model was trained on, thus signaling a need for retraining. Option A (Compare models) is incorrect because comparing models doesn't directly assess changes in serving data. Option B (Evaluate skewness) is incorrect because skewness is a static data property, not a comparison of data over time. Option D (Compare confusion matrix) is incorrect because it evaluates model performance change, an indirect consequence of data drift, not the drift itself. Therefore, Option C is the most direct method to determine if data changes necessitate model retraining.","comment_id":"1365687"}],"answer_description":"","question_id":52,"answer_ET":"C","answer":"C","isMC":true,"timestamp":"2025-03-06 04:15:00","choices":{"B":"Evaluate the data skewness.","C":"Evaluate data drift.","A":"Compare the two different models.","D":"Compare the confusion matrix."},"unix_timestamp":1741230900,"exam_id":2,"question_text":"You used BigQuery ML to build a customer purchase propensity model six months ago. You want to compare the current serving data with the historical serving data to determine whether you need to retrain the model. What should you do?","topic":"1","answers_community":["C (100%)"],"answer_images":[]},{"id":"5smzTFDWsfvX6HE9Cplp","topic":"1","discussion":[{"upvote_count":"1","poster":"n2183712847","comment_id":"1365741","timestamp":"1741236480.0","content":"Selected Answer: C\nThe best option is C. Single Explore. Option C is best: Single Explore is Google's recommended way for dashboards with related metrics, offering unified filtering. Option A (Multiple Explores) is incorrect: Fragments user experience. Option B (Materialized Views) is incorrect: BigQuery optimization, not Looker dashboard design. Option D (Custom Visualization) is incorrect: Less efficient than built-in Explore features. Therefore, Option C, Single Explore, is the Google-recommended, most efficient approach."}],"question_id":53,"url":"https://www.examtopics.com/discussions/google/view/157642-exam-associate-data-practitioner-topic-1-question-57/","answer_images":[],"answer_ET":"C","answers_community":["C (100%)"],"answer_description":"","choices":{"D":"Use Looker's custom visualization capabilities to create a single visualization that displays all the sales metrics with filtering and drill-down functionality.","C":"Create a single Explore with all sales metrics. Build the dashboard using this Explore.","B":"Use BigQuery to create multiple materialized views, each focusing on a specific sales metric. Build the dashboard using these views.","A":"Create multiple Explores, each focusing on each sales metric. Link the Explores together in a dashboard using drill-down functionality."},"exam_id":2,"unix_timestamp":1741236480,"timestamp":"2025-03-06 05:48:00","isMC":true,"question_text":"Your company uses Looker to visualize and analyze sales data. You need to create a dashboard that displays sales metrics, such as sales by region, product category, and time period. Each metric relies on its own set of attributes distributed across several tables. You need to provide users the ability to filter the data by specific sales representatives and view individual transactions. You want to follow the Google-recommended approach. What should you do?","question_images":[],"answer":"C"},{"id":"56jdt0RDJc2uNlyFVNlU","topic":"1","unix_timestamp":1739985360,"answers_community":["B (100%)"],"choices":{"A":"Load the data into BigQuery using Dataproc. Use Apache Spark to translate the reviews by invoking the Cloud Translation API. Set BigQuery as the sink.","B":"Use a Dataflow templates pipeline to translate the reviews using the Cloud Translation API. Set BigQuery as the sink.","C":"Load the data into BigQuery using a Cloud Run function. Use the BigQuery ML create model statement to train a translation model. Use the model to translate the product reviews within BigQuery.","D":"Load the data into BigQuery using a Cloud Run function. Create a BigQuery remote function that invokes the Cloud Translation API. Use a scheduled query to translate new reviews."},"question_text":"Your companyâ€™s ecommerce website collects product reviews from customers. The reviews are loaded as CSV files daily to a Cloud Storage bucket. The reviews are in multiple languages and need to be translated to Spanish. You need to configure a pipeline that is serverless, efficient, and requires minimal maintenance. What should you do?","exam_id":2,"url":"https://www.examtopics.com/discussions/google/view/156808-exam-associate-data-practitioner-topic-1-question-58/","question_id":54,"answer_ET":"B","isMC":true,"discussion":[{"timestamp":"1741236420.0","comment_id":"1365739","poster":"n2183712847","upvote_count":"1","content":"Selected Answer: B\nThe best option is B. Dataflow template with Cloud Translation API. Option B is best because Dataflow templates are serverless, managed, and efficient for data pipelines like translation. Option A (Dataproc/Spark) is incorrect because Dataproc is not serverless and adds maintenance. Option C (Cloud Run/BigQuery ML) is incorrect because training a BigQuery ML model for translation is overly complex for this. Option D (Cloud Run/Remote Function) is incorrect because it adds unnecessary complexity with remote functions and scheduled queries. Therefore, Option B, Dataflow template, is the most streamlined and best-fit serverless solution."},{"timestamp":"1739985360.0","content":"Selected Answer: B\nDataflow is a fully managed","poster":"SaquibHerman","upvote_count":"3","comment_id":"1358865"}],"timestamp":"2025-02-19 18:16:00","answer":"B","answer_description":"","answer_images":[],"question_images":[]},{"id":"xI82bXBPB1qqdlM2P7XG","exam_id":2,"question_text":"You have a Dataproc cluster that performs batch processing on data stored in Cloud Storage. You need to schedule a daily Spark job to generate a report that will be emailed to stakeholders. You need a fully-managed solution that is easy to implement and minimizes complexity. What should you do?","answers_community":["A (50%)","B (50%)"],"choices":{"D":"Use Cloud Scheduler to trigger the Spark job, and use Cloud Run functions to email the report.","B":"Use Dataproc workflow templates to define and schedule the Spark job, and to email the report.","C":"Use Cloud Run functions to trigger the Spark job and email the report.","A":"Use Cloud Composer to orchestrate the Spark job and email the report."},"question_images":[],"unix_timestamp":1739985780,"topic":"1","answer_description":"","answer":"A","timestamp":"2025-02-19 18:23:00","url":"https://www.examtopics.com/discussions/google/view/156811-exam-associate-data-practitioner-topic-1-question-59/","answer_ET":"B","question_id":55,"isMC":true,"discussion":[{"upvote_count":"1","poster":"n2183712847","content":"Selected Answer: B\ndataproc worker template will work the best.","comment_id":"1366607","timestamp":"1741444560.0"},{"poster":"n2183712847","timestamp":"1741236360.0","comment_id":"1365738","upvote_count":"2","content":"Selected Answer: B\nThe best option is B. Dataproc workflow templates. Option B is best because workflow templates are built into Dataproc for simple Spark job scheduling. Option A (Composer) is incorrect because Composer is too complex for this simple task. Option C (Cloud Run trigger) is incorrect because Cloud Run is not for Dataproc job management. Option D (Cloud Scheduler + Cloud Run) is incorrect because it's more complex than workflow templates. Therefore, Option B, workflow templates, is the easiest and most direct solution."},{"comment_id":"1358872","content":"Selected Answer: A\nDataproc Workflow Templates are excellent for defining and scheduling Spark jobs, but they do not natively support emailing reports.","poster":"SaquibHerman","upvote_count":"3","timestamp":"1739985780.0"}],"answer_images":[]}],"exam":{"name":"Associate Data Practitioner","provider":"Google","id":2,"lastUpdated":"11 Apr 2025","numberOfQuestions":72,"isImplemented":true,"isBeta":false,"isMCOnly":true},"currentPage":11},"__N_SSP":true}