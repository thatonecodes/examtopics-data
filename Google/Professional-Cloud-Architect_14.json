{"pageProps":{"questions":[{"id":"EFXmGIS4ZTeGI4cwJc2G","choices":{"A":"Google Compute Engine unmanaged instance groups and Network Load Balancer","C":"Google Cloud Dataproc to run Apache Hadoop jobs to process each test","D":"Google App Engine with Google StackDriver for logging","B":"Google Compute Engine managed instance groups with auto-scaling"},"answer":"B","timestamp":"2019-11-16 15:07:00","discussion":[{"poster":"AWS56","content":"B, https://cloud.google.com/compute/docs/autoscaler/","comment_id":"21964","upvote_count":"24","timestamp":"1589630820.0","comments":[{"comment_id":"150957","upvote_count":"7","timestamp":"1612517280.0","poster":"tartar","content":"B is ok"}]},{"comment_id":"1336366","poster":"plumbig11","upvote_count":"1","timestamp":"1735992360.0","content":"Selected Answer: B\nC++ test in on prem== compute engine,\nin this case we need to scale if necessary so managed."},{"content":"Selected Answer: B\nChanging the tests as little as possible rules out C & D.\nTest takes several hours and you need to improve perfromace. Autocaling with MIG will do it\nUnmanaged group cannot autosacle. Load balancer will not improve perfromance","comment_id":"799704","upvote_count":"3","timestamp":"1691317200.0","poster":"RVivek"},{"timestamp":"1682953140.0","content":"Why not A? the custom APP may be not supporto autoscaling....","upvote_count":"2","comment_id":"709300","comments":[{"poster":"e5019c6","timestamp":"1719548340.0","content":"I second the question. The App might not support horizontal scaling.\nBut I also admit that no other answer is valid. \nA: The Load Balancer offers no benefit.\nC: Hadoop doesn't process C++\nD: App Engine is for web apps.","upvote_count":"1","comment_id":"1107487"},{"upvote_count":"1","comment_id":"799706","poster":"RVivek","timestamp":"1691317260.0","content":"Changing the tests as little as possible rules out C & D.\nTest takes several hours and you need to improve perfromace. Autocaling with MIG will do it\nUnmanaged group cannot autosacle. Load balancer will not improve perfromance"}],"poster":"SerGCP"},{"poster":"Mahmoud_E","timestamp":"1681948500.0","content":"Selected Answer: B\nB is the right answer","upvote_count":"1","comment_id":"699417"},{"content":"B is right","timestamp":"1681581240.0","comment_id":"695577","upvote_count":"2","poster":"AzureDP900"},{"upvote_count":"1","timestamp":"1658580900.0","comment_id":"530604","content":"Selected Answer: B\nchoose b","poster":"Pime13"},{"comment_id":"501829","content":"Selected Answer: B\nB is the correct answer","upvote_count":"1","timestamp":"1655256480.0","poster":"vincy2202"},{"poster":"joe2211","comment_id":"487178","upvote_count":"3","content":"Selected Answer: B\nvote B","timestamp":"1653546240.0"},{"comment_id":"427865","timestamp":"1645338840.0","comments":[{"comment_id":"445693","content":"D is the correct answer \nhttps://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#pubsub","upvote_count":"2","poster":"PleeO","timestamp":"1647419820.0"},{"poster":"PeppaPig","timestamp":"1645511940.0","comment_id":"429033","upvote_count":"4","content":"D is the answer"}],"content":"New Question \nYour company has a Kubernetes application that pulls messages from Pub/Sub and stores them in Filestore. Because the application is simple, it was deployed as a single pod. The infrastructure team has analyzed Pub/Sub metrics and discovered that the application cannot process the messages in real time. Most of them wait for minutes before being processed. You need to scale the elaboration process that is 1/0-intensive. What should you do? \n\nA. Usekubectl autoscale deployment APP_NAME --max 6 --min 2 --cpu-percent 50 to \nconfigure Kubernetes autoscaling deployment. \nB. Configure a Kubemetes autoscaling deployment based on the \nsubscription/push_request_latencies metric. \nC. Use the --enable-autoscaling flag when you create the Kubernetes cluster. \nD. Configure a Kubernetes autoscaling deployment based on the subscription/num_undelivered_messages metric.","poster":"Cloudguy123","upvote_count":"2"},{"timestamp":"1645337340.0","poster":"Cloudguy123","comment_id":"427856","comments":[{"timestamp":"1649596260.0","poster":"rottzy","comment_id":"460072","content":"why are you repeating questions from this series as comments?","upvote_count":"3"},{"poster":"fahad01hbti","upvote_count":"2","comment_id":"430759","timestamp":"1645714080.0","content":"it is B\nhttps://cloud.google.com/storage/docs/encryption/using-customer-managed-keys"}],"content":"New Question\nYour organization has stored sensitive data in a Cloud Storage bucket. For regulatory reasons, your company must be able to rotate the encryption key used to encrypt the data in the bucket. The data will be processed in Dataproc. You want to follow Google-recommended practices for security What should you do? \n\nA. Create a key with Cloud Key Management Service (KMS) Encrypt the data using the encrypt method of Cloud KMS. \nB. Create a key with Cloud Key Management Service (KMS). Set the encryption key on the bucket to the Cloud KMS key. \nC. Generate a GPG key pair. Encrypt the data using the GPG key. Upload the encrypted data to the bucket. \nD. Generate an AES-256 encryption key. Encrypt the data in the bucket using the customer-supplied encryption keys feature.\n\nAnswer Please","upvote_count":"2"},{"upvote_count":"2","comment_id":"414974","content":"Are we getting questions from 1-100 in the exam?","comments":[{"comment_id":"473156","content":"Yeah I have the same question in mind.","poster":"bishalsainju","upvote_count":"2","timestamp":"1651766700.0"}],"timestamp":"1643247900.0","poster":"GCP_New"},{"upvote_count":"3","content":"Google Compute Engine and with MIG for auto-scaling","timestamp":"1642605360.0","poster":"AnilKr","comment_id":"409622"},{"upvote_count":"2","comment_id":"400338","content":"Agree with Option B.Google Compute Managed instance groups with auto-scaling","poster":"bala786","timestamp":"1641517740.0"},{"comment_id":"374577","timestamp":"1638657000.0","poster":"[Removed]","content":"agree with B","upvote_count":"3"},{"content":"B. Google Compute Engine managed instance groups with auto-scaling","timestamp":"1637220120.0","poster":"victory108","upvote_count":"2","comment_id":"360073"},{"comment_id":"348781","timestamp":"1635962280.0","content":"B is correct","poster":"un","upvote_count":"2"},{"comment_id":"338556","content":"B should be the one","timestamp":"1634608740.0","poster":"sidbet","upvote_count":"2"},{"comment_id":"323503","timestamp":"1632922980.0","content":"B is ok","poster":"lynx256","upvote_count":"2"},{"comment_id":"323104","content":"The answer is B","timestamp":"1632890160.0","poster":"Ausias18","upvote_count":"2"},{"comment_id":"322878","content":"Compute Engine for Linux and C++, autoscaling to improve times","poster":"salgabri","timestamp":"1632857520.0","upvote_count":"3"},{"timestamp":"1631203860.0","upvote_count":"2","comments":[{"comment_id":"315227","content":"Hi Rathul,\n\nHave you cleared the Cloud architect exam? \nDid you get questions from this site??","poster":"padma29","timestamp":"1632086100.0","upvote_count":"2"}],"comment_id":"306597","poster":"Rathul","content":"Agree with Option B"},{"content":"Correct Answer is B\nIt can be done with Autoscaling. Autoscaling is only possible in case of Managed Instance Groups.","timestamp":"1627201740.0","comment_id":"275875","upvote_count":"3","poster":"gautam23"},{"poster":"willan","content":"should be B","upvote_count":"1","comment_id":"267184","timestamp":"1626274920.0"},{"upvote_count":"1","poster":"iptorrent786","comment_id":"254293","timestamp":"1624899360.0","content":"not relevant to this question but this exam:\nIs there any difference in questions between PTR000088 vs PTR0000178?"},{"poster":"CYL","timestamp":"1620599340.0","comment_id":"216288","upvote_count":"1","content":"Agree with B. Use autoscaling to provide ability to run more instances to reduce testing time."},{"poster":"emybreth","comment_id":"202783","timestamp":"1618860900.0","upvote_count":"1","content":"Selected B as final for me, passed exam"},{"content":"A since the custom app might it able to auto scale","timestamp":"1618018560.0","comment_id":"197069","poster":"Kplow","upvote_count":"1"},{"comment_id":"192958","poster":"dxxdd7","timestamp":"1617546180.0","upvote_count":"1","content":"B is good"},{"timestamp":"1617370800.0","content":"B is the right answer.","comment_id":"191633","upvote_count":"1","poster":"Vijen012"},{"content":"I also agree with B","upvote_count":"1","comment_id":"189038","poster":"whitley030390","timestamp":"1616936760.0"},{"upvote_count":"1","content":"Answer: B","comment_id":"186326","poster":"AshokC","timestamp":"1616610660.0"},{"upvote_count":"1","content":"B is right","comment_id":"182232","poster":"RaviprasadD","timestamp":"1616159940.0"},{"comment_id":"154261","timestamp":"1612950840.0","poster":"RaviprasadD","upvote_count":"1","content":"B IS RIGHT"},{"upvote_count":"1","comment_id":"150624","poster":"Ani26","timestamp":"1612467060.0","content":"Agree with B"},{"comment_id":"122503","content":"\"The Test Suite takes several hours to run on a limited amount of on-premises servers\"\nThe autoscaler can be configured to scale on a wide range of server resource parameters. Therefore, Identify the constrained resource parameter and autoscale based on that. So the answer is B.","timestamp":"1609231680.0","poster":"OnomeOkuma","comments":[{"timestamp":"1631015160.0","poster":"devDhrey","upvote_count":"1","content":"agree with you","comment_id":"305133"}],"upvote_count":"2"},{"comment_id":"116571","content":"Correct is B\nit says \"to reduce the amount of time it takes to fully test a change to the system, while changing the tests as little as possible.\", so you should go with MIG and autoscaling enabled.","timestamp":"1608660900.0","poster":"mlantonis","upvote_count":"3"},{"content":"B, for sure","timestamp":"1607582580.0","poster":"gfhbox0083","upvote_count":"1","comment_id":"106465"},{"upvote_count":"1","content":"D is not correct because apps cannot take several hours to finish. I imagine there is a time limit.","comment_id":"104842","timestamp":"1607381280.0","poster":"Musk"},{"upvote_count":"1","comment_id":"100790","content":"B is the correct answer","timestamp":"1606918740.0","poster":"Nirms"},{"upvote_count":"1","poster":"Ziegler","comment_id":"98076","timestamp":"1606639500.0","content":"B is the correct Answer"},{"timestamp":"1606172940.0","poster":"Javed","upvote_count":"1","comment_id":"94563","content":"Answer B"},{"timestamp":"1604949120.0","poster":"clouddude","upvote_count":"3","comment_id":"86130","content":"I am going with B because of two things: first, the reference to the limited number of servers being a problem and, second, there is no reference to networking being a part of the current testing."},{"poster":"rbrto","comment_id":"83862","upvote_count":"2","content":"agree with B","timestamp":"1604540340.0"},{"poster":"gcp_cert_2020","content":"Should be B","comment_id":"82079","upvote_count":"2","timestamp":"1604224020.0"},{"poster":"service_at","comment_id":"81039","content":"There is typo: \"B:\" => \"A:\"\n\nIncorrect Answers:\nB: There is no mention of incoming IP data traffic for the custom C++ applications.","upvote_count":"4","timestamp":"1603926420.0"},{"timestamp":"1602511860.0","poster":"PRC","content":"B it should be","upvote_count":"1","comment_id":"73689"},{"timestamp":"1598324280.0","poster":"Rathish","content":"Answer: Google Compute Engine Managed Instance Group with Autoscaling","comment_id":"54834","upvote_count":"1"},{"timestamp":"1596105300.0","upvote_count":"2","poster":"2g","comment_id":"44663","content":"answer: B"},{"content":"Agree with B","comment_id":"32728","upvote_count":"2","timestamp":"1593151620.0","poster":"MyPractice"}],"question_id":66,"question_text":"Your company's test suite is a custom C++ application that runs tests throughout each day on Linux virtual machines. The full test suite takes several hours to complete, running on a limited number of on-premises servers reserved for testing. Your company wants to move the testing infrastructure to the cloud, to reduce the amount of time it takes to fully test a change to the system, while changing the tests as little as possible.\nWhich cloud infrastructure should you recommend?","answer_ET":"B","url":"https://www.examtopics.com/discussions/google/view/8340-exam-professional-cloud-architect-topic-1-question-158/","unix_timestamp":1573913220,"answers_community":["B (100%)"],"question_images":[],"exam_id":4,"answer_description":"","topic":"1","answer_images":[],"isMC":true},{"id":"d3oruU6CK5faxBWhItoc","answer_images":[],"unix_timestamp":1573913280,"choices":{"C":"Meet with the cloud operations team and the engineer to discuss load balancer options","B":"Review the encryption requirements for websocket connections with the security team","A":"Help the engineer to convert his websocket code to use HTTP streaming","D":"Help the engineer redesign the application to use a distributed user session service that does not rely on websockets and HTTP sessions."},"answers_community":["C (75%)","D (25%)"],"discussion":[{"content":"I agree with C","timestamp":"1589630880.0","upvote_count":"16","comments":[{"comments":[{"poster":"[Removed]","comment_id":"462778","timestamp":"1650049920.0","upvote_count":"4","content":"The key line from the link above: \n\nSession affinity for WebSockets works the same as for any other request. For information, see Session affinity."}],"comment_id":"150959","poster":"tartar","upvote_count":"8","timestamp":"1612517400.0","content":"C is ok"},{"comment_id":"264010","poster":"fraloca","upvote_count":"6","timestamp":"1625919480.0","content":"https://cloud.google.com/load-balancing/docs/https#websocket_support"}],"comment_id":"21965","poster":"AWS56"},{"upvote_count":"7","content":"IMO C is ok.\nBeside the reasons mentioned above regarding why A, B and D are wrong, there are also:\nA and D are wrong because are abot changing the app - whereas in the task \"You want to help him ensure his application will run properly on GCP\" (not REDESIGN/CHANGE).\nB is wrong because you don't have to \"Review the encryption requirements for websocket connections with the security team\"...","timestamp":"1632462240.0","comments":[{"upvote_count":"1","poster":"ashrafh","timestamp":"1684556280.0","comment_id":"722427","content":"thanks"}],"poster":"lynx256","comment_id":"318892"},{"upvote_count":"1","timestamp":"1736228220.0","content":"Selected Answer: D\nThe answer is D.\nAs an architect, it is our job to help design scalable and resilient systems. The developer has presented you with a poor design if session data is tied to one server. If that server goes down, you lose everyone's session. And it doesn't scale. No good developer is building apps nowadays that require sticky sessions.","comment_id":"1337451","poster":"ryaryarya"},{"poster":"plumbig11","comment_id":"1336369","upvote_count":"1","content":"Selected Answer: C\nMeet with the cloud operations team and the engineer to discuss load balancer options","timestamp":"1735992420.0"},{"poster":"e5019c6","comment_id":"1107498","upvote_count":"3","timestamp":"1719549060.0","content":"Selected Answer: D\nI think that, since the app is in design stage, it's totally valid to change its design to adapt to work better in the cloud. Websockets and HTTP session, while supported, are not the optimal choice for apps in the cloud.\nSome user said that the question asks for help ensure his app runs on GCP, so we shouldn't change it. But I don't think that's the case. As architects we should oversee any design that developers and engineers are introducing to the organization's architecture."},{"timestamp":"1688780700.0","content":"Selected Answer: C\nC is the correct answer,\nGoogle Cloud HTTP(S)-based load balancers have native support for the WebSocket protocol when you use HTTP or HTTPS as the protocol to the backend. The load balancer does not need any configuration to proxy WebSocket connections.\n\nhttps://cloud.google.com/load-balancing/docs/https#websocket_support","poster":"examch","upvote_count":"3","comment_id":"769063"},{"content":"The answer is D. C. is not the best answer because it does not address the issue of websockets and HTTP sessions not being distributed across the web servers. While load balancer options may be relevant to the overall operation of the application, they do not address the specific issue of ensuring that the websockets and HTTP sessions are properly distributed. A better solution would be to help the engineer redesign the application to use a distributed user session service that does not rely on websockets and HTTP sessions, as this would address the issue of session distribution. Alternatively, the engineer could consider converting their websocket code to use HTTP streaming, which could potentially help with session distribution.","poster":"NodummyIQ","comment_id":"763358","upvote_count":"1","comments":[{"upvote_count":"1","poster":"e5019c6","comment_id":"1107494","timestamp":"1719548760.0","content":"Totally agree. But you didn't vote to change the C hegemony :P"}],"timestamp":"1688243640.0"},{"poster":"AzureDP900","timestamp":"1681581420.0","comment_id":"695580","upvote_count":"2","content":"C is fine."},{"comment_id":"646673","upvote_count":"2","content":"Selected Answer: C\nI agree with C","poster":"ijazahmad722","timestamp":"1676374320.0"},{"timestamp":"1655257080.0","poster":"vincy2202","comment_id":"501835","upvote_count":"2","content":"Selected Answer: C\nC is the correct answer"},{"upvote_count":"4","content":"Selected Answer: C\nvote C","poster":"joe2211","timestamp":"1653546300.0","comment_id":"487179"},{"poster":"Cloudguy123","comments":[{"comments":[{"timestamp":"1645915260.0","upvote_count":"5","comment_id":"432562","poster":"nickojul","content":"A is ok"}],"timestamp":"1645468680.0","upvote_count":"2","poster":"Cloudguy123","comment_id":"428831","content":"A) Configure a trigger in Cloud Build for new source changes. Invoke Cloud Build to build container images for each microservice, and tag them using the code commit hash. Push the images to the Container Registry. \n\nB)Configure a trigger in Cloud Build for new source changes. The trigger invokes build jobs and build container images for the microservices. Tag the images with a version number, and push them to Cloud Storage. \n\nC) Create a Scheduler job to check the repo every minute. For any new change, invoke Cloud Build to build container images for the microservices. Tag the images using the current timestamp, and push them to the Container Registry. \n\nD) Configure a trigger in Cloud Build for new source changes. Invoke Cloud Build to build one container image, and tag the image with the label 'latest' Push the image to the Container Registry"}],"comment_id":"428829","timestamp":"1645468620.0","content":"New Case Study Question- TerramEarth\n\nFor this question, refer to the TerramEarth case study. \n\nYou are building a microservice-based application for TerramEarth. \nThe application is based on Docker containers. You want to follow Google-recommended practices to build the application continuously and store the build artifacts. What should you do?","upvote_count":"2"},{"timestamp":"1643078520.0","upvote_count":"1","poster":"DreamerK","comment_id":"413531","content":"Why D is wrong is the wording \"doesn't rely on\". This means the application needs to use other protocols instead of http or websocket. This is not realistic and requires too much application refactoring. Actually a distributed session service is possible with http or websocket as long as the session information is stored in shared storage such as nosql database or redis that can be accessed by all web servers. In this sense, D is wrong answer.","comments":[{"comment_id":"1107506","timestamp":"1719549240.0","poster":"e5019c6","upvote_count":"1","content":"There is no refactor since the app is only in design stage. Read the question carefully:\n''A lead software engineer tells you that his NEW APPLICATION DESIGN...''"}]},{"poster":"AnilKr","timestamp":"1642605780.0","comment_id":"409627","upvote_count":"1","content":"C is fine. Global HTTP(S) load Balancer supports webSockets."},{"content":"hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152","poster":"kopper2019","comment_id":"406665","comments":[{"poster":"samsonakala","comment_id":"1526877","timestamp":"1743845100.0","upvote_count":"1","content":"You mean the 21 new questions are from number 152?"}],"timestamp":"1642220100.0","upvote_count":"3"},{"poster":"giovy_82","timestamp":"1642146660.0","comments":[{"content":"from where you got the \"new\" word? ha ha","timestamp":"1684556160.0","upvote_count":"1","comment_id":"722425","poster":"ashrafh","comments":[{"comment_id":"1107509","poster":"e5019c6","timestamp":"1719549420.0","upvote_count":"1","content":"From the question, ninth word from the start.\nAlso, ''design'' is an important word. It means the app hasn't been built yet. Else it would have said ''his new application'' only."}]}],"upvote_count":"1","comment_id":"405975","content":"I also agree with C. the answer D could be ok but the question says \"his new application design \" so it means that the app has just been developed and deployed so there's no convenience to redesign it from scratch to avoid use of sessions and websocket."},{"timestamp":"1637220060.0","comment_id":"360071","poster":"victory108","upvote_count":"1","content":"C. Meet with the cloud operations team and the engineer to discuss load balancer options"},{"comment_id":"348784","poster":"un","timestamp":"1635962340.0","content":"C is correct","upvote_count":"1"},{"content":"Answer is C","timestamp":"1632890340.0","comment_id":"323105","poster":"Ausias18","upvote_count":"1"},{"comments":[{"comment_id":"315237","upvote_count":"2","timestamp":"1632088020.0","content":"Hello Rathul,\n\nHave you cleared the Cloud Architect Exam? Was this site useful??","poster":"padma29"}],"comment_id":"306598","timestamp":"1631203860.0","upvote_count":"2","poster":"Rathul","content":"Agree with option C"},{"content":"Discuss Load Balancer Options. Global HTTP(S) load Balancer supports webSockets\nhttps://cloud.google.com/load-balancing/docs/https#websocket_support\nCorrect Answer : C","poster":"gautam23","upvote_count":"3","timestamp":"1627201920.0","comment_id":"275879"},{"comment_id":"267190","content":"C is correct","poster":"willan","timestamp":"1626275100.0","upvote_count":"1"},{"comment_id":"258513","upvote_count":"3","poster":"bolu","content":"If you are moving to cloud,it is appropriate to focus on distributed model (as any architect would recommend the same point to avoid down time when 1 server goes down.) so the most appropriate answer for this question would be D instead C as pilantra said (\"HTTP sessions that are not distributed across the web servers.\")","timestamp":"1625312760.0"},{"upvote_count":"2","comment_id":"255862","timestamp":"1625075760.0","content":"\"HTTP sessions that are not distributed across the web servers.\" Because of this, the first thing you have to do is to distribute the HTTP session, so it does not live inside a single server. If the HTTP session lives in a single server, in case that server dies, the users will lose their session data (shopping carts will be empty, users will be logged out, etc.). So, having a load balancing will not help, as the session is not distributed. Therefore, D is the correct answer.","poster":"pilantra"},{"content":"Agree with C","comment_id":"232583","timestamp":"1622603700.0","upvote_count":"1","poster":"Chulbul_Pandey"},{"content":"Answer: C","comment_id":"187999","timestamp":"1616807700.0","upvote_count":"1","poster":"AshokC"},{"content":"C : The HTTP(S) load balancer has native support for the WebSocket protocol.","comment_id":"150625","timestamp":"1612467120.0","poster":"Ani26","upvote_count":"1"},{"timestamp":"1609232340.0","upvote_count":"3","comment_id":"122510","content":"C\n\nBecause the requirement is to make sure the application runs as-is on Google Cloud Platform, without making any changes to it.","poster":"OnomeOkuma"},{"poster":"mlantonis","timestamp":"1608660960.0","upvote_count":"2","comment_id":"116573","content":"I agree with C. The HTTP(S) load balancer has native support for the WebSocket protocol."},{"comment_id":"106466","content":"C, for sure","poster":"gfhbox0083","timestamp":"1607582580.0","upvote_count":"2"},{"upvote_count":"2","timestamp":"1606918800.0","poster":"Nirms","comment_id":"100791","content":"C is the correct answer"},{"upvote_count":"2","timestamp":"1606645860.0","poster":"Ziegler","comment_id":"98113","content":"C is the correct answer"},{"comment_id":"95097","content":"got this question in exam.Answer is C","timestamp":"1606259160.0","upvote_count":"2","poster":"laksg"},{"poster":"Javed","timestamp":"1606173000.0","comment_id":"94565","upvote_count":"2","content":"Answer C"},{"timestamp":"1604949300.0","comment_id":"86131","upvote_count":"3","poster":"clouddude","content":"Going with C. Encryption hasn't come up at all so I am assuming it's not needed so there is no need to talk to the security team. Talking to the operations team seems to make sense because the goal is to make the current environment work in the cloud, not to drastically change it. Adding a load balancer is a relatively simple thing to do."},{"timestamp":"1604224080.0","comment_id":"82080","upvote_count":"2","poster":"gcp_cert_2020","content":"I think C"},{"content":"answer: C","timestamp":"1596105300.0","comment_id":"44664","poster":"2g","upvote_count":"2"},{"content":"I agree with C. HTTP(S) load balancer support websocket.","comment_id":"25852","poster":"JJu","timestamp":"1591055220.0","upvote_count":"3"}],"topic":"1","question_text":"A lead software engineer tells you that his new application design uses websockets and HTTP sessions that are not distributed across the web servers. You want to help him ensure his application will run properly on Google Cloud Platform.\nWhat should you do?","answer":"C","answer_ET":"C","exam_id":4,"question_id":67,"isMC":true,"answer_description":"","timestamp":"2019-11-16 15:08:00","url":"https://www.examtopics.com/discussions/google/view/8341-exam-professional-cloud-architect-topic-1-question-159/","question_images":[]},{"id":"60iZc165VahyzpChxnSm","choices":{"B":"Google Cloud Bigtable","A":"Google Cloud SQL","C":"Google Cloud Storage","D":"Google Cloud Datastore"},"question_id":68,"isMC":true,"answers_community":["B (85%)","C (15%)"],"unix_timestamp":1589141880,"discussion":[{"upvote_count":"12","content":"B. Google Cloud Bigtable","timestamp":"1625404860.0","poster":"victory108","comment_id":"398361"},{"comment_id":"332916","poster":"jeff001","timestamp":"1618106220.0","content":"B, Bigtable due to the IoT like requirements","upvote_count":"10"},{"comment_id":"1310741","timestamp":"1731430020.0","poster":"fff2e69","content":"Selected Answer: B\nGoogle Cloud Bigtable is well-suited for handling high-throughput, low-latency workloads like clickstream data. It is optimized for analytics on time-series and event data at scale, and it supports high write rates, with the capacity to handle thousands of writes per second. This makes it ideal for storing large volumes of clickstream data with bursts, ensuring data is available for analysis by data science and user experience teams.","upvote_count":"3"},{"upvote_count":"4","content":"Selected Answer: B\n1. High throughput for clickstream data: Bigtable is a NoSQL database designed for high write throughput, making it ideal for handling the continuous stream of click data with bursts up to 8,500 clicks per second.\n2. Scalability: Bigtable is highly scalable, allowing you to handle increasing data volumes as your website portfolio grows.\n3. Low latency: Bigtable provides low latency data access, which is important for real-time analysis and reporting on clickstream data.\n4. Integration with BigQuery: Bigtable integrates well with BigQuery, enabling your data science team to perform complex analysis and generate insights from the clickstream data.","poster":"Ekramy_Elnaggar","timestamp":"1731271500.0","comment_id":"1309623"},{"comment_id":"1232885","upvote_count":"3","poster":"upliftinghut","content":"Selected Answer: C\nBig table if it needs real time but here the need for analysis is not urgent => Google cloud storage.","timestamp":"1718806620.0"},{"content":"Selected Answer: B\nThe right answer is BigTable for a such large volume of data.","poster":"pakilodi","comment_id":"1084975","upvote_count":"1","timestamp":"1701410520.0"},{"comment_id":"1065011","upvote_count":"1","poster":"Nora9","timestamp":"1699376040.0","content":"Selected Answer: B\nB. Google cloud Bigtable is most suitable as it is for analysis and streamed data."},{"upvote_count":"1","comment_id":"981690","timestamp":"1692104700.0","content":"Selected Answer: B\nNote: DataStore is now rename as FireStore.","poster":"MartinFish"},{"comment_id":"863753","content":"Selected Answer: B\nB is correct","timestamp":"1680865740.0","upvote_count":"1","poster":"hiromi"},{"timestamp":"1679521200.0","poster":"alekonko","upvote_count":"3","comment_id":"847544","content":"Selected Answer: B\nBigtable is a high-perf NoSQL db service that handle large volumes of structured data with low latency"},{"timestamp":"1674843120.0","content":"Selected Answer: B\nThe Google Cloud Bigtable goes together with the BigQuery. The question itself gives away a bit.","upvote_count":"1","comment_id":"789880","poster":"zerg0"},{"upvote_count":"4","content":"For storing click-data that is streamed in at a rate of 6,000 clicks per minute, with bursts of up to 8,500 clicks per second, and that needs to be stored for future analysis by your data science and user experience teams, you should consider using a scalable, high-performance, and low-latency NoSQL database such as Google Cloud Bigtable, option B.\n\nGoogle Cloud Bigtable is a fully managed, high-performance NoSQL database service that is designed to handle large volumes of structured data with low latency. It is well-suited for storing high-velocity data streams and can scale to handle millions of reads and writes per second.\n\nOption A: Google Cloud SQL, option C: Google Cloud Storage, and option D: Google Cloud Datastore, would not be suitable for this use case, as they are not designed to handle high-velocity data streams at this scale.","timestamp":"1671525120.0","poster":"omermahgoub","comment_id":"750667"},{"upvote_count":"2","poster":"i_am_robot","comment_id":"747817","timestamp":"1671259020.0","content":"B. Google Cloud Bigtable\n\nGoogle Cloud Bigtable is a scalable, high-performance NoSQL database that is well-suited for storing large amounts of data with low latency. It is designed for high-throughput workloads such as streaming data, and is able to handle bursts of up to millions of reads and writes per second.\n\nGiven the high volume of click data that needs to be stored and the requirement for low latency, Google Cloud Bigtable would be a good choice for storing the data. It is able to handle the high rate of incoming data and provide fast access to the data for analysis by the data science and user experience teams.\n\nGoogle Cloud SQL is a fully-managed relational database service, and may not be the best choice for storing high-volume streaming data. Google Cloud Storage is an object storage service, and may not provide the necessary performance for storing and querying large amounts of data in real-time. Google Cloud Datastore is a NoSQL document database, and while it may be suitable for storing large amounts of data, it may not provide the necessary performance for handling high volumes of streaming data."},{"comment_id":"736515","poster":"Melampos","upvote_count":"1","content":"Selected Answer: B\nhttps://cloud.google.com/bigtable#section-9","timestamp":"1670300640.0"},{"upvote_count":"1","comment_id":"727298","content":"B. Google Cloud Bigtable","poster":"Bry_040706","timestamp":"1669442640.0"},{"upvote_count":"1","comment_id":"721878","timestamp":"1668849300.0","poster":"AniketD","content":"Selected Answer: B\nCloud Bigtable has all the features to fulfill the requirements mentioned in the question"},{"upvote_count":"1","comment_id":"701683","poster":"Mahmoud_E","timestamp":"1666459440.0","content":"D is right answer"},{"timestamp":"1665974940.0","content":"Bigtable for a high volume of writes. 8500 clicks per sec","comment_id":"696755","upvote_count":"3","poster":"zr79"},{"poster":"minmin2020","timestamp":"1665644760.0","comment_id":"693679","content":"Selected Answer: B\nB. Google Cloud Bigtable","upvote_count":"1"},{"poster":"holerina","timestamp":"1663678260.0","comment_id":"674169","content":"for analystics choose big table","upvote_count":"2"},{"poster":"chickennuggets","timestamp":"1660352760.0","upvote_count":"1","content":"They don't call it Bigtable for no reason - high throughput and low latency. GCS not enough performance. SQL & datastore not good fit","comment_id":"646073"},{"comments":[{"comment_id":"696513","upvote_count":"1","timestamp":"1665951060.0","poster":"AzureDP900","content":"B is right"}],"comment_id":"644425","content":"B. Google Cloud Bigtable because it supports :\na) the low latency + high throughput workloads are required in this use-case to support 8500 clicks events per second. \nb) the OLAP use case to integrate and analyse this data with various ML and data-science services.","timestamp":"1660037520.0","poster":"AashishAggarwal2000","upvote_count":"3"},{"poster":"sgofficial","content":"I would go with Bigtable, because here question is stressing on huge volume of data writes and for analysis pupose....bigtable supports heavy reads/writes within seconds","comment_id":"640019","upvote_count":"1","timestamp":"1659259380.0"},{"content":"The question says for future analysis. CBT is a very expensive storage option to keep the data for future analysis. Cloud storage would be a great place to keep data, and it can then be loaded to BQ via cloud dataflow ETL, saving a lot of money","comment_id":"623148","upvote_count":"1","comments":[{"timestamp":"1662644340.0","poster":"kuboraam","content":"They didnt say anything about cost savings in the question, so BigTable is the clear solution. Also they mention bursts of around 8500 clicks per second. Cloud Storage supports a max of only about 1000 writes per second. https://cloud.google.com/storage/docs/request-rate","comment_id":"663668","upvote_count":"3"}],"timestamp":"1656317700.0","poster":"ZLT"},{"poster":"Dhiraj03","timestamp":"1655460180.0","content":"Bigtable","comment_id":"617658","upvote_count":"1"},{"content":"Selected Answer: B\nGoogle Cloud Bigtable is the right option.","comment_id":"588476","timestamp":"1650438780.0","poster":"Nirca","upvote_count":"1"},{"poster":"pakochiu","comment_id":"587475","upvote_count":"1","content":"Selected Answer: B\nGoogle Cloud Bigtable for heaily read write","timestamp":"1650258240.0"},{"comment_id":"551506","timestamp":"1645327620.0","poster":"belly265","upvote_count":"2","content":"Ans B -Google cloud big table can only handle this"},{"content":"For me is B.","upvote_count":"1","timestamp":"1643114460.0","comment_id":"532119","poster":"llanerox"},{"upvote_count":"3","content":"Selected Answer: B\nFor the amount of clicks per second the only storage that supports that is BigTable","poster":"Moss2011","comment_id":"517055","timestamp":"1641339480.0"},{"timestamp":"1640768460.0","content":"Selected Answer: C\nThe more I look up, C is correct.\nStore in Cloud Starage for future analysis.\nLoad it into BigQuely when analysis is needed.\nThat way i can reduce costs.\nThe question does not mention real-time analysis.","poster":"OrangeTiger","upvote_count":"1","comment_id":"511988","comments":[{"poster":"RitwickKumar","comment_id":"647954","timestamp":"1660719120.0","upvote_count":"1","content":"Objects in GCS are immutable, how are you planning to store the data: Individual event file or batched file or a single file using post processing?\nGCS would never fit the bill for streaming data."}]},{"timestamp":"1640766360.0","upvote_count":"1","content":"First, i choose C Cloud strage.\nI confused by a word 'future analysis'.\nLow data keep on CloudStrage until i need it.\n\nStore large amounts of data for analysis, B is best options.\nBut can I stream data directly to BigTable?\nDon't you need CloudDataflow or CloudFunction in between?","comment_id":"511958","poster":"OrangeTiger"},{"poster":"StelSen","comment_id":"507485","upvote_count":"3","timestamp":"1640224440.0","content":"Answer is B. This links provides nice explanation. https://stackoverflow.com/questions/53489250/price-aside-why-ever-choose-google-cloud-bigtable-over-google-cloud-datastore"},{"comment_id":"493260","content":"Go for C.\nAnalytics event - Cold Path:\nhttps://cloud.google.com/architecture/optimized-large-scale-analytics-ingestion","upvote_count":"2","timestamp":"1638550740.0","poster":"haroldbenites"},{"content":"Answer is B","poster":"vincy2202","upvote_count":"2","timestamp":"1638015300.0","comment_id":"488108"},{"timestamp":"1634752620.0","poster":"Q_Review","comment_id":"465317","content":"The question references \"Analytics\" a large quantity of data. It's not a trick question; Bigtable is the answer. B.","upvote_count":"2"},{"timestamp":"1623308760.0","comments":[{"comment_id":"458783","timestamp":"1633616940.0","content":"What are you talking about? Cloud storage is being used as a data lake there, and is being queried by BigQuery. Click-streams are not SQL, therefore this diagram is not applicable at all. You need a non-SQL solution. Hence B.","poster":"penelop","upvote_count":"3"}],"poster":"rsamant","comment_id":"378834","content":"it should be C for real time streaming also . IO can be throttled using pubsub or dataflow while writing data justifying bigtable cost for later analysis is not justified . look @ below diagram from GCS Page\nhttps://cloud.google.com/storage#section-6","upvote_count":"3"},{"poster":"guid1984","upvote_count":"3","content":"Answer is C, if you read below two articles then its clear \nhttps://cloud.google.com/solutions/build-a-data-lake-on-gcp\nhttps://cloud.google.com/solutions/architecture/optimized-large-scale-analytics-ingestion","timestamp":"1613474820.0","comment_id":"291711","comments":[{"comments":[{"poster":"Jambalaja","comment_id":"331237","content":"B is right. C is wrong, because Cloud Storage has IO write limits of 1000 writes / second. See source: https://cloud.google.com/storage/docs/request-rate#auto-scaling","upvote_count":"9","timestamp":"1617889620.0"}],"upvote_count":"4","timestamp":"1617384720.0","poster":"army234","comment_id":"326863","content":"Nope, B is correct."}]},{"timestamp":"1608032820.0","poster":"doumx","upvote_count":"4","content":"B for sure cause : NoSQL and hight volume transaction","comment_id":"244515"},{"content":"D - Web traffic, same as game state","comment_id":"191055","upvote_count":"1","timestamp":"1601564640.0","poster":"RomiAwasthy"},{"content":"Tricky question. Usually google recommends Bigtable for IoT. But none of the requirements rule out Datastore as far as I could find. \nFor safety I would go with Bigtable, as bursts of 8500 writes per second have the potencial to disrupt operations in Datastore. If someone can find documentation on this it would be much appreciated.","comment_id":"164409","upvote_count":"3","timestamp":"1598187900.0","poster":"Pipoca"},{"comment_id":"107536","poster":"gfhbox0083","content":"B, for sure.\nBigTable for IOT\nhttps://cloud.google.com/solutions/iot-overview","upvote_count":"2","timestamp":"1591863060.0"},{"upvote_count":"2","timestamp":"1591482660.0","content":"B is rght","poster":"Ziegler","comment_id":"104183"},{"timestamp":"1590722700.0","content":"Final Decision to go with Option B","comment_id":"98008","poster":"AD2AD4","upvote_count":"2"},{"poster":"Mont","comment_id":"96209","upvote_count":"2","timestamp":"1590507660.0","content":"Data is in IoT Nature and Big Table is the right choice. Cloud SQL is mainly for OLTP (Transactional, CRUD) not for taking and storing streaming data. It does not have the scalability and elasticity to absorb this amount of data in real-time.\nSO the answer is without any doubt Big Table \nB."},{"content":"since its stored for future analysis, bigtable is not required. it says data science teams would use it hence would go for cloud storage as bigquery can read from cloud storage","timestamp":"1590439620.0","upvote_count":"1","poster":"dotdotc","comment_id":"95658"},{"comment_id":"93707","timestamp":"1590118920.0","poster":"SanderA","content":"The data is stored for later analysis only, not for live reporting, So why go with BigTable? I would say go for Cloud Storage. \nC","comments":[{"upvote_count":"1","content":"B is ok reason-\nGCS is ideally for Object storage purpose although it has pretty good scalability. It's not suitable for IoT kind of spiky streaming data. Its buckets initially support roughly 1000 writes per second and then scale as needed. As the request rate for a given bucket grows, Cloud Storage automatically increases the IO capacity for that bucket by distributing the request load across multiple servers. Especially considering the click stream rate of 6,000 clicks per minute, with bursts of up to 8,500 clicks per second, the way GCS handle and absorb this kind high and low data stream by scale up and down make it not suitable for this task.","timestamp":"1606823220.0","comment_id":"231887","poster":"bjuneja"}],"upvote_count":"4"},{"upvote_count":"2","comment_id":"86734","comments":[{"comment_id":"304030","timestamp":"1614923400.0","poster":"nitinz","upvote_count":"1","content":"Ans is B. high rate = bigtable"}],"poster":"Shabje","content":"Since this is required only for future analysis, won’t we prefer to land it on the less expensive Cloud Datastore, instead of BigTable whose cost is not justified here.","timestamp":"1589141880.0"}],"answer_images":[],"question_images":[],"timestamp":"2020-05-10 22:18:00","question_text":"You have been asked to select the storage system for the click-data of your company's large portfolio of websites. This data is streamed in from a custom website analytics package at a typical rate of 6,000 clicks per minute. With bursts of up to 8,500 clicks per second. It must have been stored for future analysis by your data science and user experience teams.\nWhich storage infrastructure should you choose?","url":"https://www.examtopics.com/discussions/google/view/20238-exam-professional-cloud-architect-topic-1-question-16/","answer_description":"","exam_id":4,"answer_ET":"B","answer":"B","topic":"1"},{"id":"WFHQok3dCBfKQHx1MAjj","isMC":true,"exam_id":4,"choices":{"B":"ג€¢ Batch every 10,000 events with a single manifest file for metadata ג€¢ Compress event files and manifest file into a single archive file ג€¢ Name files using serverName ג€\" EventSequence ג€¢ Create a new bucket if bucket is older than 1 day and save the single archive file to the new bucket. Otherwise, save the single archive file to existing bucket.","A":"ג€¢ Append metadata to file body ג€¢ Compress individual files ג€¢ Name files with serverName ג€\" Timestamp ג€¢ Create a new bucket if bucket is older than 1 hour and save individual files to the new bucket. Otherwise, save files to existing bucket.","D":"ג€¢ Append metadata to file body ג€¢ Compress individual files ג€¢ Name files with a random prefix pattern ג€¢ Save files to one bucket","C":"ג€¢ Compress individual files ג€¢ Name files with serverName ג€\" EventSequence ג€¢ Save files to one bucket ג€¢ Set custom metadata headers for each object after saving"},"question_images":[],"timestamp":"2021-06-03 08:55:00","topic":"1","answer_description":"","answer_images":[],"answer":"D","answers_community":["D (79%)","B (21%)"],"unix_timestamp":1622703300,"question_id":69,"question_text":"The application reliability team at your company this added a debug feature to their backend service to send all server events to Google Cloud Storage for eventual analysis. The event records are at least 50 KB and at most 15 MB and are expected to peak at 3,000 events per second. You want to minimize data loss.\nWhich process should you implement?","discussion":[{"timestamp":"1622820300.0","poster":"rishab86","content":"answer is definitely D\nhttps://cloud.google.com/storage/docs/request-rate#naming-convention\n \"A longer randomized prefix provides more effective auto-scaling when ramping to very high read and write rates. For example, a 1-character prefix using a random hex value provides effective auto-scaling from the initial 5000/1000 reads/writes per second up to roughly 80000/16000 reads/writes per second, because the prefix has 16 potential values. If your use case does not need higher rates than this, a 1-character randomized prefix is just as effective at ramping up request rates as a 2-character or longer randomized prefix.\"\n Example: \nmy-bucket/2fa764-2016-05-10-12-00-00/file1 \nmy-bucket/5ca42c-2016-05-10-12-00-00/file2 \nmy-bucket/6e9b84-2016-05-10-12-00-01/file3","upvote_count":"38","comment_id":"374457"},{"comments":[{"comment_id":"420786","upvote_count":"2","poster":"kravenn","content":"answer C","timestamp":"1628251140.0"},{"upvote_count":"1","content":"is it C?","poster":"juccjucc","comments":[{"upvote_count":"4","comment_id":"397083","comments":[{"content":"The correct answer is A","poster":"Papafel","upvote_count":"1","comments":[{"timestamp":"1637139840.0","upvote_count":"2","poster":"matmuh","comment_id":"479888","content":"Absulatly C"}],"timestamp":"1626325380.0","comment_id":"406770"},{"poster":"squishy_fishy","comment_id":"1096408","timestamp":"1702553880.0","content":"The correct answer is C based on the URL you shared. gcloud recommender recommendations list \\\n --project=PROJECT_ID \\\n --location=ZONE \\\n --recommender=google.compute.instance.IdleResourceRecommender \\\n --format=yaml","upvote_count":"1"}],"content":"this is not 100% accurate. you should investigate if you doubt if is incorrect\nhttps://cloud.google.com/compute/docs/instances/viewing-and-applying-idle-vm-recommendations","poster":"cloudstd","timestamp":"1625247540.0"}],"timestamp":"1625142180.0","comment_id":"395932"},{"poster":"KS1911","comment_id":"407635","content":"I have my exam scheduled after 3 days. Would there be more questions coming on ExamTopics?","timestamp":"1626416100.0","upvote_count":"3"},{"comment_id":"396155","content":"answer: C","timestamp":"1625155500.0","upvote_count":"8","poster":"cloudstd"}],"poster":"kopper2019","upvote_count":"6","timestamp":"1625019360.0","comment_id":"394386","content":"- New Q, 06/2021\nHelicopter Racing League Testlet 1\nCompany overview\nQUESTION 6\nFor this question, refer to the Helicopter Racing League (HRL) case study. A recent finance audit of cloud infrastructure noted an exceptionally high number of Compute Engine instances are allocated to do video encoding and transcoding. You suspect that these Virtual Machines are zombie machines that were not deleted after their workloads completed. You need to quickly get a list of which VM instances are idle. What should you do?\nA. Log into each Compute Engine instance and collect disk, CPU, memory, and network usage statistics for analysis.\nB. Use the gcloud compute instances list to list the virtual machine instances that have the idle: true label set.\nC. Use the gcloud recommender command to list the idle virtual machine instances.\nD. From the Google Console, identify which Compute Engine instances in the managed instance groups are no longer responding to health check probes."},{"upvote_count":"1","poster":"plumbig11","comment_id":"1336370","content":"Selected Answer: D\nD. ג€¢ Append metadata to file body ג€¢ Compress individual files ג€¢ Name files with a random prefix pattern ג€¢ Save files to one bucket","timestamp":"1735992480.0"},{"poster":"Sephethus","comment_id":"1233685","upvote_count":"1","timestamp":"1718894340.0","content":"This question is messed up. The formatting, the discussion, everything. I have no idea what to choose here. Chat GPT thinks the answer is C but most think it is D and there's not much difference between the two answers."},{"timestamp":"1702554480.0","upvote_count":"2","poster":"squishy_fishy","comment_id":"1096419","content":"The question is how to reduce the data loss, the answer should be something like separation of duty, data lost prevention, but answer D is for reducing latency retrieving data. I'm baffled by this question."},{"upvote_count":"2","timestamp":"1696703880.0","content":"I agree with D, but then, using a random prefix wouldn't it make more difficult the file retrieve?","comment_id":"1027589","poster":"marcohol"},{"content":"Selected Answer: B\nWhy not option B??","poster":"ptsironis","comment_id":"908681","upvote_count":"3","timestamp":"1685283360.0"},{"upvote_count":"1","timestamp":"1674714840.0","comment_id":"788439","content":"I was thinking correct answer was A, because we should have some kind of bucket rotation in order to avoid hiting the max size of a bucket.\nHowever it seems there is no size limit for a GCP cloud bucket, so I will have to agree with community and stick to answer D.","poster":"nunopires2001"},{"content":"Selected Answer: D\nD is the correct answer\nhttps://cloud.google.com/storage/docs/request-rate#naming-convention","comment_id":"699419","poster":"Mahmoud_E","upvote_count":"3","timestamp":"1666223940.0"},{"poster":"Pime13","upvote_count":"2","comment_id":"530606","content":"D: https://cloud.google.com/storage/docs/request-rate#naming-convention","timestamp":"1642949820.0"},{"content":"Selected Answer: D\nD is the correct answer","upvote_count":"2","timestamp":"1639363620.0","poster":"vincy2202","comment_id":"500340"},{"comment_id":"487720","timestamp":"1637971800.0","content":"Selected Answer: D\nvote D","upvote_count":"5","poster":"joe2211"},{"content":"Request admin to intervene and delete the hijacking of the question by kopper2019","upvote_count":"4","poster":"amxexam","comments":[{"comment_id":"446014","content":"Use the material for study dude! Hello? Anyone home?","timestamp":"1631803500.0","poster":"Examster1","upvote_count":"6"},{"timestamp":"1637956140.0","comment_id":"487603","content":"it looks like this website does not have any admin","poster":"Arad","upvote_count":"1"}],"timestamp":"1631418240.0","comment_id":"443259"},{"timestamp":"1625019300.0","poster":"kopper2019","comments":[{"timestamp":"1625155560.0","comment_id":"396157","content":"answer: C","comments":[{"poster":"Papafel","content":"Yes answer is C","upvote_count":"2","timestamp":"1626325500.0","comment_id":"406771"}],"poster":"cloudstd","upvote_count":"2"},{"content":"is it C?\nall these questions are from the new exam? why they are here in the comments and not as questions in the list?","poster":"juccjucc","timestamp":"1625142420.0","upvote_count":"2","comment_id":"395940","comments":[{"comment_id":"397231","upvote_count":"4","poster":"kopper2019","content":"because exam was not updated so I added the Qs but they added this new Qs as normal now we have 218 Qs","timestamp":"1625275440.0","comments":[{"comment_id":"455228","upvote_count":"1","content":"Hey Kopper, when would you provide the new set of questions ?","poster":"Roncy","timestamp":"1633052100.0"}]}]},{"timestamp":"1628251440.0","comment_id":"420787","content":"answer: C","upvote_count":"1","poster":"kravenn"}],"upvote_count":"3","comment_id":"394385","content":"- New Q, 06/2021\nHelicopter Racing League Testlet 1\nCompany overview\nQUESTION 5\nFor this question, refer to the Helicopter Racing League (HRL) case study. HRL is looking for a cost- effective approach for storing their race data such as telemetry. They want to keep all historical records, train models using only the previous season's data, and plan for data growth in terms of volume and information collected. You need to propose a data solution. Considering HRL business requirements and the goals expressed by CEO S. Hawke, what should you do?\nA. Use Firestore for its scalable and flexible document-based database. Use collections to aggregate race data by season and event.\nB. Use Cloud Spanner for its scalability and ability to version schemas with zero downtime. Split race data using season as a primary key.\nC. Use BigQuery for its scalability and ability to add columns to a schema. Partition race data based on season.\nD. Use Cloud SQL for its ability to automatically manage storage increases and compatibility with MySQL. Use separate database instances for each season."},{"timestamp":"1718894280.0","comment_id":"1233684","content":"what does this have to do with the cloud storage question?","poster":"Sephethus","upvote_count":"1"},{"upvote_count":"4","comment_id":"396158","timestamp":"1625155620.0","content":"answer: A","poster":"cloudstd"},{"comments":[{"upvote_count":"1","poster":"Papafel","content":"Yes answer is A","timestamp":"1626325560.0","comment_id":"406772"}],"upvote_count":"2","poster":"juccjucc","content":"is it A?","timestamp":"1625142480.0","comment_id":"395942"},{"poster":"Amrit123","content":"C, is the right answer. The scheduler would run without a trigger even though the release has not been done. If you read (application as soon as it is released ), the time is not certain. So, the answer is C. Check out the last 30 questions, would give a better idea as there is a separate discussion","timestamp":"1634783040.0","comment_id":"465479","upvote_count":"2"},{"timestamp":"1625370000.0","content":"answer : A","upvote_count":"5","comment_id":"397975","comments":[{"comment_id":"490301","content":"why A? Does Cloud Storage make sense ?","timestamp":"1638233160.0","upvote_count":"1","poster":"vchrist"},{"content":"in option A what is the use of Cloud storage bucket? In my opinion answer is C.","timestamp":"1632333000.0","poster":"jask","comment_id":"449659","upvote_count":"3"},{"content":"Answer is A","timestamp":"1626325620.0","poster":"Papafel","comment_id":"406774","upvote_count":"2"}],"poster":"esc"},{"comment_id":"582043","upvote_count":"1","timestamp":"1649278740.0","comments":[{"timestamp":"1686590100.0","content":"It's probably C due to pub sub on Cloud Deploy rather than source repos\nhttps://cloud.google.com/deploy/docs/subscribe-deploy-notifications","comment_id":"921603","poster":"BiddlyBdoyng","upvote_count":"1"}],"poster":"cloudmon","content":"I would go with C\nhttps://cloud.google.com/source-repositories/docs/code-change-notification"},{"content":"- New Q, 06/2021\nHelicopter Racing League Testlet 1\nCompany overview\n\nQUESTION 2\nFor this question, refer to the Helicopter Racing League (HRL) case study. Recently HRL started a new regional racing league in Cape Town, South Africa. In an effort to give customers in Cape Town a better user experience, HRL has partnered with the Content Delivery Network provider, Fastly. HRL needs to allow traffic coming from all of the Fastly IP address ranges into their Virtual Private Cloud network (VPC network). You are a member of the HRL security team and you need to configure the update that will allow only the Fastly IP address ranges through the External HTTP(S) load balancer. Which command should you use?","comment_id":"394379","upvote_count":"1","comments":[{"content":"A. gcloud compute security-policies rules update 1000 \\\n--security-policy from-fastly \\\n--src-ip-ranges * \\\n--action “allow”\nB. gcloud compute firewall rules update sourceiplist-fastly \\\n--priority 100 \\\n--allow tcp:443\nC. gcloud compute firewall rules update hir-policy \\\n--priority 100 \\\n--target-tags=sourceiplist-fastly \\\n--allow tcp:443\nD. gcloud compute security-policies rules update 1000 \\\n--security-policy hir-policy \\\n--expression “evaluatePreconfiguredExpr(‘sourceiplist-fastly’)” \\\n--action “allow”","comments":[{"timestamp":"1625155860.0","content":"answer: D","comments":[{"content":"Answer is A","comment_id":"406776","timestamp":"1626325680.0","upvote_count":"2","comments":[{"timestamp":"1637259660.0","upvote_count":"1","comment_id":"480938","poster":"matmuh","content":"A is incorrect : To match all IPs specify *\nhttps://cloud.google.com/sdk/gcloud/reference/compute/security-policies/rules/update"}],"poster":"Papafel"}],"comment_id":"396163","poster":"cloudstd","upvote_count":"6"},{"poster":"kravenn","comment_id":"420789","upvote_count":"4","timestamp":"1628251500.0","content":"answer D"},{"content":"both A and D have correct syntax, but src-ip-ranges cannot be \"*\", correct is D","upvote_count":"5","timestamp":"1629098040.0","comments":[{"content":"I agree","poster":"cloudmon","comment_id":"582051","upvote_count":"1","timestamp":"1649280060.0"}],"poster":"xavi1","comment_id":"425644"},{"timestamp":"1627009680.0","upvote_count":"1","content":"@kooper2019 what is the answer for this?","comment_id":"412018","poster":"sai953"},{"comment_id":"845987","poster":"GrandAM","content":"The gcloud compute firewall rules update command can be used to update the firewall rules for a specific VPC network.\n\nOption A, \"gcloud compute security-policies rules update,\" is not applicable in this scenario as it's used to define and manage security policies in a centralized manner.\n\nOption B, \"gcloud compute firewall rules update sourceiplist-fastly,\" allows traffic from all TCP ports and it does not restrict traffic to specific IP address ranges.\n\nOption D, \"gcloud compute security-policies rules update,\" is also not applicable in this scenario as it's used to define and manage security policies in a centralized manner.\n\nTherefore, Option C, \"gcloud compute firewall rules update hir-policy --priority 100 --target-tags=sourceiplist-fastly --allow tcp:443,\" would be the right command to allow only the Fastly IP address ranges through the External HTTP(S) load balancer.","upvote_count":"1","timestamp":"1679404680.0"}],"upvote_count":"1","timestamp":"1625019180.0","poster":"kopper2019","comment_id":"394380"}],"timestamp":"1625019180.0","poster":"kopper2019"},{"comments":[{"upvote_count":"1","timestamp":"1625019000.0","poster":"kopper2019","comment_id":"394373","comments":[{"timestamp":"1625019060.0","poster":"kopper2019","comments":[{"comments":[{"poster":"kopper2019","comment_id":"394377","upvote_count":"2","content":"QUESTION 1\nFor this question, refer to the Helicopter Racing League (HRL) case study. Your team is in charge of creating a payment card data vault for card numbers used to bill tens of thousands of viewers, merchandise consumers, and season ticket holders. You need to implement a custom card tokenization service that meets the following requirements:\n• It must provide low latency at minimal cost.\n• It must be able to identify duplicate credit cards and must not store plaintext card numbers.\n• It should support annual key rotation.\n\nWhich storage approach should you adopt for your tokenization service?\n\nA. Store the card data in Secret Manager after running a query to identify duplicates.\nB. Encrypt the card data with a deterministic algorithm stored in Firestore using Datastore mode.\nC. Encrypt the card data with a deterministic algorithm and shard it across multiple Memorystore instances.\nD. Use column-level encryption to store the data in Cloud SQL.","timestamp":"1625019060.0","comments":[{"poster":"SPNBLUE","comment_id":"419268","content":"Why D ?","upvote_count":"1","timestamp":"1628002740.0"},{"poster":"cloudstd","comment_id":"396164","upvote_count":"4","content":"answer: D","timestamp":"1625155920.0"},{"upvote_count":"1","timestamp":"1632036960.0","poster":"cugena","comment_id":"447468","content":"This tutorial shows how to set up an access-controlled credit and debit card tokenization service on Cloud Functions. To set up the service, the article uses these Google Cloud services: Identity and Access Management (IAM), Cloud Key Management Service (KMS), and Firestore in Datastore mode."},{"content":"Answer : B","upvote_count":"6","timestamp":"1628320320.0","poster":"tatai1987","comment_id":"421097"},{"poster":"Papafel","content":"Answer is D","upvote_count":"3","comment_id":"406777","timestamp":"1626325740.0"},{"poster":"pabloinigo","upvote_count":"2","comment_id":"455062","content":"Answer is B\n\nhttps://cloud.google.com/architecture/tokenizing-sensitive-cardholder-data-for-pci-dss\n\nTo set up the service, the article uses these Google Cloud services: Identity and Access Management (IAM), Cloud Key Management Service (KMS), and Firestore in Datastore mode.","timestamp":"1633023900.0"},{"timestamp":"1626614520.0","poster":"dlzhang","comment_id":"408962","content":"Can you explain why D is the choice?","upvote_count":"3"}]}],"poster":"kopper2019","comment_id":"394376","upvote_count":"1","content":"Executive statement\n\nOur CEO, S. Hawke, wants to bring high-adrenaline racing to fans all around the world. We listen to our fans, and they want enhanced video streams that include predictions of events within the race (e.g., overtaking). Our current platform allows us to predict race outcomes but lacks the facility to support real- time predictions during races and the capacity to process season-long results.","timestamp":"1625019060.0"}],"comment_id":"394375","content":"Business requirements\nHRL’s owners want to expand their predictive capabilities and reduce latency for their viewers in emerging markets. Their requirements are:\n\nSupport ability to expose the predictive models to partners. Increase predictive capabilities during and before races:\n○ Race results\n○ Mechanical failures\n○ Crowd sentiment\nIncrease telemetry and create additional insights. Measure fan engagement with new predictions. Enhance global availability and quality of the broadcasts. Increase the number of concurrent viewers.\nMinimize operational complexity. Ensure compliance with regulations.\nCreate a merchandising revenue stream.\n\nTechnical requirements\nMaintain or increase prediction throughput and accuracy. Reduce viewer latency.\nIncrease transcoding performance.\nCreate real-time analytics of viewer consumption patterns and engagement. Create a data mart to enable processing of large volumes of race data.","upvote_count":"1"}],"content":"Existing technical environment\n\nHRL is a public cloud-first company; the core of their mission-critical applications runs on their current public cloud provider. Video recording and editing is performed at the race tracks, and the content is encoded and transcoded, where needed, in the cloud. Enterprise-grade connectivity and local compute is provided by truck-mounted mobile data centers. Their race prediction services are hosted exclusively on their existing public cloud provider. Their existing technical environment is as follows:\n\n- Existing content is stored in an object storage service on their existing public cloud provider. \nVideo encoding and transcoding is performed on VMs created for each job.\nRace predictions are performed using TensorFlow running on VMs in the current public cloud provider."}],"timestamp":"1625018940.0","content":"- New Q, 06/2021 \nHelicopter Racing League Testlet 1\nCompany overview\n\nHelicopter Racing League (HRL) is a global sports league for competitive helicopter racing. Each year HRL holds the world championship and several regional league competitions where teams compete to earn a spot in the world championship. HRL offers a paid service to stream the races all over the world with live telemetry and predictions throughout each race.\n\nSolution concept\n\nHRL wants to migrate their existing service to a new platform to expand their use of managed AI and ML services to facilitate race predictions. Additionally, as new fans engage with the sport, particularly in emerging regions, they want to move the serving of their content, both real-time and recorded, closer to their users.","upvote_count":"1","comment_id":"394372","poster":"kopper2019"},{"comments":[{"content":"yes ans A","comment_id":"446482","poster":"abhtri","timestamp":"1631868600.0","upvote_count":"1"}],"content":"- New Q, 06/2021 Mountkirk Games, C Testlet 1 Company overview\nQUESTION 7\nYour development team has created a mobile game app. You want to test the new mobile app on Android and iOS devices with a variety of configurations. You need to ensure that testing is efficient and cost- effective. What should you do?\n\nA. Upload your mobile app to the Firebase Test Lab, and test the mobile app on Android and iOS devices.\nB. Create Android and iOS VMs on Google Cloud, install the mobile app on the VMs, and test the mobile app.\nC. Create Android and iOS containers on Google Kubernetes Engine (GKE), install the mobile app on the containers, and test the mobile app.\nD. Upload your mobile app with different configurations to Firebase Hosting and test each configuration.","poster":"kopper2019","comment_id":"394371","timestamp":"1625018880.0","upvote_count":"1"},{"content":"answer: A","comment_id":"396166","upvote_count":"2","comments":[{"timestamp":"1626350520.0","poster":"kamilC","comments":[{"comment_id":"414401","poster":"vimal1206","content":"Yes, the answer is D","comments":[{"comment_id":"415861","poster":"siri7676","content":"why D?","upvote_count":"1","timestamp":"1627442040.0"}],"upvote_count":"1","timestamp":"1627273980.0"},{"poster":"rottzy","comment_id":"460114","content":"if workload on GKE - answer is A","upvote_count":"2","timestamp":"1633877640.0"}],"content":"why not D?","comment_id":"407046","upvote_count":"2"}],"timestamp":"1625155980.0","poster":"cloudstd"},{"poster":"sai953","content":"@kopper2019 have you mentioned answers for all of these questions anywhere? Can you please help with the correct answers?","timestamp":"1627011780.0","comment_id":"412092","upvote_count":"1"},{"timestamp":"1629098820.0","poster":"xavi1","content":"A, check: https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity\nAlternatives to Workload Identity\nThere are two alternative methods to access Cloud APIs from GKE. With the release of Workload Identity we no longer recommend these approaches because of the compromises they require.","comment_id":"425650","upvote_count":"1"},{"poster":"kravenn","content":"answer A, https://cloud.google.com/blog/products/containers-kubernetes/introducing-workload-identity-better-authentication-for-your-gke-applications","upvote_count":"2","timestamp":"1628252280.0","comment_id":"420795"},{"poster":"hello_aws","timestamp":"1627880820.0","comment_id":"418518","content":"https://www.examtopics.com/discussions/google/view/56645-exam-professional-cloud-architect-topic-5-question-6/","upvote_count":"3"},{"poster":"vchrist","upvote_count":"1","content":"C - Mountkirk Games Study Case, need low latency","comment_id":"490316","timestamp":"1638234960.0"},{"poster":"kbouwmee","timestamp":"1625213760.0","upvote_count":"9","content":"answer: C, because these impact the users perspective","comment_id":"396703"},{"poster":"trismegistus","timestamp":"1635105840.0","comment_id":"467116","content":"C and D seem equally viable to me. Server up time and request latency would both directly impact the user. I think I'll go with C ultimately because GKE, being a cluster, should be able to survive the loss of any single server.","upvote_count":"1"},{"timestamp":"1625018700.0","poster":"kopper2019","comment_id":"394363","upvote_count":"1","content":"- New Q, 06/2021\nMountkirk Games, C Testlet 1\nCompany overview\n\nMountkirk Games makes online, session-based, multiplayer games for mobile platforms. They have recently started expanding to other platforms after successfully migrating their on-premises environments to Google Cloud.\n\nTheir most recent endeavor is to create a retro-style first-person shooter (FPS) game that allows hundreds of simultaneous players to join a geo-specific digital arena from multiple platforms and locations. A real- time digital banner will display a global leaderboard of all the top players across every active arena.\n\nSolution concept\n\nMountkirk Games is building a new multiplayer game that they expect to be very popular. They plan to deploy the game’s backend on Google Kubernetes Engine so they can scale rapidly and use Google’s global load balancer to route players to the closest regional game arenas. In order to keep the global leader board in sync, they plan to use a multi-region Spanner cluster.","comments":[{"content":"Existing technical environment\n\nThe existing environment was recently migrated to Google Cloud, and five games came across using lift- and-shift virtual machine migrations, with a few minor exceptions. Each new game exists in an isolated Google Cloud project nested below a folder that maintains most of the permissions and network policies. Legacy games with low traffic have been consolidated into a single project. There are also separate environments for development and testing.\n\nBusiness requirements\n\nSupport multiple gaming platforms. Support multiple regions.\nSupport rapid iteration of game features. Minimize latency.\nOptimize for dynamic scaling.\nUse managed services and pooled resources. Minimize costs.\n\nTechnical requirements\n\nDynamically scale based on game activity.\nPublish scoring data on a near real-time global leaderboard. Store game activity logs in structured files for future analysis.\nUse GPU processing to render graphics server-side for multi-platform support. Support eventual migration of legacy games to this new platform.","poster":"kopper2019","comments":[{"upvote_count":"1","comment_id":"394365","poster":"kopper2019","timestamp":"1625018760.0","comments":[{"content":"answer: B","timestamp":"1625156040.0","comment_id":"396169","poster":"cloudstd","upvote_count":"6"},{"comment_id":"394366","content":"QUESTION 2\nYou are implementing Firestore for Mountkirk Games. Mountkirk Games wants to give a new game programmatic access to a legacy game's Firestore database. Access should be as restricted as possible. What should you do?\n\nA. Create a service account (SA) in the legacy game’s Google Cloud project, add a second SA in the new game’s IAM page, and then give the Organization Admin role to both SAs.\nB. Create a service account (SA) in the legacy game’s Google Cloud project, give the SA the Organization Admin role, and then give it the Firebase Admin role in both projects.\nC. Create a service account (SA) in the legacy game’s Google Cloud project, add this SA in the new game’s IAM page, and then give it the Firebase Admin role in both projects.\nD. Create a service account (SA) in the legacy game’s Google Cloud project, give it the Firebase Admin role, and then migrate the new game to the legacy game’s project.","timestamp":"1625018760.0","upvote_count":"2","poster":"kopper2019","comments":[{"poster":"afsarkhan","comment_id":"1247538","timestamp":"1720911360.0","content":"answer: C","upvote_count":"1"},{"timestamp":"1628149080.0","content":"Why migrate the new game to the legacy game's project? \nCould be C?","upvote_count":"9","comment_id":"420127","poster":"kravenn"},{"poster":"kopper2019","timestamp":"1625018760.0","content":"QUESTION 3\nMountkirk Games wants to limit the physical location of resources to their operating Google Cloud regions. What should you do?\n\nA. Configure an organizational policy which constrains where resources can be deployed.\nB. Configure IAM conditions to limit what resources can be configured.\nC. Configure the quotas for resources in the regions not being used to 0.\nD. Configure a custom alert in Cloud Monitoring so you can disable resources as they are created in other regions.","comments":[{"timestamp":"1625156100.0","poster":"cloudstd","upvote_count":"5","content":"answer: A","comment_id":"396172"},{"upvote_count":"3","comment_id":"420797","content":"answer A","poster":"kravenn","timestamp":"1628252640.0"},{"timestamp":"1628252700.0","poster":"kravenn","upvote_count":"4","content":"QUESTION 4 -> answer B","comment_id":"420798"},{"upvote_count":"2","comment_id":"444216","poster":"LisX","timestamp":"1631574900.0","content":"Question 4 -> D. Ingress imply KKE. Kubemci has been replaced by ingress controller."},{"timestamp":"1629099360.0","upvote_count":"1","poster":"xavi1","content":"correct is c?","comment_id":"425659"},{"poster":"chouse","content":"A, https://cloud.google.com/resource-manager/docs/organization-policy/org-policy-constraints","upvote_count":"3","comment_id":"438638","timestamp":"1630689960.0"},{"comment_id":"394368","upvote_count":"2","content":"QUESTION 4\nYou need to implement a network ingress for a new game that meets the defined business and technical requirements. Mountkirk Games wants each regional game instance to be located in multiple Google Cloud regions. What should you do?\n\nA. Configure a global load balancer connected to a managed instance group running Compute Engine instances.\nB. Configure kubemci with a global load balancer and Google Kubernetes Engine.\nC. Configure a global load balancer with Google Kubernetes Engine.\nD. Configure Ingress for Anthos with a global load balancer and Google Kubernetes Engine.","timestamp":"1625018820.0","poster":"kopper2019"}],"upvote_count":"1","comment_id":"394367"},{"timestamp":"1630155480.0","comment_id":"433891","upvote_count":"3","poster":"prassuu","content":"C is okay.https://www.examtopics.com/discussions/google/view/56978-exam-professional-cloud-architect-topic-7-question-2/"},{"timestamp":"1678667880.0","upvote_count":"2","comment_id":"837489","content":"(C)\nTo give a new game programmatic access to a legacy game's Firestore database, access should be restricted as possible. To accomplish this, you should create a service account (SA) in the legacy game’s Google Cloud project and add this SA in the new game’s IAM page. Then, you should give the SA the Firebase Admin role in both projects. This will allow the new game to have access to the Firestore database but only with the specific permissions granted to the SA. This approach restricts access to only the necessary resources and actions, providing the necessary level of security for the implementation.","poster":"Deb2293"}]}],"content":"Our last game was the first time we used Google Cloud, and it was a tremendous success. We were able to analyze player behavior and game telemetry in ways that we never could before. This success allowed us to bet on a full migration to the cloud and to start building all-new games using cloud-native design principles. Our new game is our most ambitious to date and will open up doors for us to support more gaming platforms beyond mobile. Latency is our top priority, although cost management is the next most important challenge. As with our first cloud-based game, we have grown to expect the cloud to enable advanced analytics capabilities so we can rapidly iterate on our deployments of bug fixes and new functionality.\n\nQUESTION 1\nYou need to optimize batch file transfers into Cloud Storage for Mountkirk Games’ new Google Cloud solution. The batch files contain game statistics that need to be staged in Cloud Storage and be processed by an extract transform load (ETL) tool. What should you do?\n\nA. Use gsutil to batch move files in sequence.\n \nB. Use gsutil to batch copy the files in parallel.\nC. Use gsutil to extract the files as the first part of ETL.\nD. Use gsutil to load the files as the last part of ETL."}],"timestamp":"1625018700.0","upvote_count":"1","comment_id":"394364"}]},{"comments":[{"poster":"cloudstd","content":"answer: A","comment_id":"396173","upvote_count":"10","timestamp":"1625156220.0"},{"timestamp":"1638235800.0","comment_id":"490324","upvote_count":"1","content":"B -> You can test a little auto scale using curl.","poster":"vchrist"},{"timestamp":"1631878680.0","comment_id":"446569","upvote_count":"1","poster":"zbyszekz","content":"in my opinion B,"},{"comment_id":"446491","poster":"abhtri","timestamp":"1631869200.0","content":"B check successful launch if everything works","upvote_count":"1"},{"content":"answer : C","upvote_count":"2","timestamp":"1629810180.0","poster":"fahad01hbti","comment_id":"430768"},{"content":"A must be the answer. We are tasked with testing the current deployment. B - you can't simulate 10s of Ks of users with curl. C - this would be extremely expensive and isn't a test. D - This does not simulate the expected load without which the latency information is useless.","poster":"trismegistus","upvote_count":"3","timestamp":"1635108720.0","comment_id":"467128"},{"poster":"Deb2293","comment_id":"837492","upvote_count":"2","timestamp":"1678668120.0","content":"A is the answer"}],"comment_id":"394361","upvote_count":"3","content":"QUESTION 152 - New Q, 06/2021\nYour team is developing a web application that will be deployed on Google Kubernetes Engine (GKE). Your CTO expects a successful launch and you need to ensure your application can handle the expected load of tens of thousands of users. You want to test the current deployment to ensure the latency of your application stays below a certain threshold. What should you do?\n\nA. Use a load testing tool to simulate the expected number of concurrent users and total requests to your application, and inspect the results.\nB. Enable autoscaling on the GKE cluster and enable horizontal pod autoscaling on your application deployments. Send curl requests to your application, and validate if the auto scaling works.\n C. Replicate the application over multiple GKE clusters in every Google Cloud region. Configure a global HTTP(S) load balancer to expose the different clusters over a single global IP address.\nD. Use Cloud Debugger in the development environment to understand the latency between the different microservices.","poster":"kopper2019","timestamp":"1625018640.0"},{"upvote_count":"1","timestamp":"1625018580.0","poster":"kopper2019","comment_id":"394360","comments":[{"upvote_count":"3","content":"answer: C","timestamp":"1625156280.0","poster":"cloudstd","comment_id":"396175"},{"poster":"kravenn","comment_id":"420799","upvote_count":"2","content":"definitely C","timestamp":"1628252760.0"},{"poster":"Manh","comment_id":"441972","upvote_count":"3","content":"answer: C","timestamp":"1631190180.0"},{"timestamp":"1629420360.0","comment_id":"427800","content":"Why not A?","comments":[{"poster":"otts","upvote_count":"1","comment_id":"493484","content":"re-architect involves analysis, design, development, test, uat and roll-out. while this is stated as a need that is agreed upon leadership often wants to know reliability. option A does not address reliability, option c has that with A/B testing, scaling the solution.","timestamp":"1638589140.0"}],"poster":"sgin","upvote_count":"2"}],"content":"QUESTION 151 - New Q, 06/2021\nYour company has developed a monolithic, 3-tier application to allow external users to upload and share files. The solution cannot be easily enhanced and lacks reliability. The development team would like to re- architect the application to adopt microservices and a fully managed service approach, but they need to convince their leadership that the effort is worthwhile. Which advantage(s) should they highlight to leadership?\n\nA. The new approach will be significantly less costly, make it easier to manage the underlying infrastructure, and automatically manage the CI/CD pipelines.\nB. The monolithic solution can be converted to a container with Docker. The generated container can then be deployed into a Kubernetes cluster.\nC. The new approach will make it easier to decouple infrastructure from application, develop and release new features, manage the underlying infrastructure, manage CI/CD pipelines and perform A/B testing, and scale the solution if necessary.\nD. The process can be automated with Migrate for Compute Engine."},{"comments":[{"upvote_count":"3","timestamp":"1629099900.0","poster":"xavi1","comment_id":"425660","content":"agree: https://cloud.google.com/apis/design/versioning"}],"upvote_count":"7","timestamp":"1625156400.0","poster":"cloudstd","comment_id":"396177","content":"answer: C"},{"poster":"cloudstd","comments":[{"timestamp":"1632251100.0","upvote_count":"2","content":"Answer D. AppEngine is a regional resource, you can only have one instance per project and you cant use serverless network endpoint groups cross project so no may to make it globally available.","poster":"Rzla","comments":[{"content":"Not D. because GKE option needs creation of Infra and does not remove the focus of just building the code. A is the answer","poster":"otts","timestamp":"1638589560.0","upvote_count":"2","comment_id":"493487"}],"comment_id":"449121"}],"comment_id":"396179","timestamp":"1625156460.0","upvote_count":"9","content":"answer: A"},{"upvote_count":"1","content":"https://www.examtopics.com/discussions/google/view/56754-exam-professional-cloud-architect-topic-1-question-136/","poster":"Deb2293","timestamp":"1678668360.0","comment_id":"837493"},{"timestamp":"1628252820.0","comment_id":"420800","comments":[{"content":"Its B. Question states during testing. Istio fault injection can simulate failure so you can observe how the wider application behaves if an element hosted by particular service / pods fails.","timestamp":"1632378000.0","upvote_count":"4","poster":"Rzla","comment_id":"449945"},{"timestamp":"1631781000.0","content":"nope, istio fault injection is a correct answer","comment_id":"445759","upvote_count":"3","poster":"PleeO"}],"poster":"kravenn","content":"voted for D at the exam but not totally sure","upvote_count":"2"},{"upvote_count":"1","comment_id":"406779","timestamp":"1626325980.0","poster":"Papafel","comments":[{"upvote_count":"8","comment_id":"433898","timestamp":"1630155960.0","poster":"prassuu","content":"B is correct. Use Istio’s fault injection on the particular microservice whose faulty behavior you want to simulate."}],"content":"Answer is D"},{"upvote_count":"1","poster":"tifo16","timestamp":"1640373840.0","content":"it's B. as mentioned in https://istio.io/latest/docs/concepts/traffic-management/#fault-injection\nYou can inject two types of faults, both configured using a virtual service:\nDelays: Delays are timing failures. They mimic increased network latency or an overloaded upstream service.\nAborts: Aborts are crash failures. They mimic failures in upstream services. Aborts usually manifest in the form of HTTP error codes or TCP connection failures.","comment_id":"508775"},{"timestamp":"1625018520.0","content":"QUESTION 147 - New Q, 06/2021\nYour company is using Google Cloud. You have two folders under the Organization: Finance and Shopping. The members of the development team are in a Google Group. The development team group has been assigned the Project Owner role on the Organization. You want to prevent the development team from creating resources in projects in the Finance folder. What should you do?\n\nA. Assign the development team group the Project Viewer role on the Finance folder, and assign the development team group the Project Owner role on the Shopping folder.\nB. Assign the development team group only the Project Viewer role on the Finance folder.\nC. Assign the development team group the Project Owner role on the Shopping folder, and remove the development team group Project Owner role from the Organization.\nD. Assign the development team group only the Project Owner role on the Shopping folder.","upvote_count":"1","comment_id":"394356","comments":[{"poster":"cloudstd","comments":[{"poster":"Rzla","timestamp":"1632378240.0","comment_id":"449949","content":"Agreed C. Its the only one which removes the permission from org level which is required.","upvote_count":"1"}],"timestamp":"1625158440.0","upvote_count":"6","comment_id":"396202","content":"ans: C"}],"poster":"kopper2019"},{"content":"answer D, bastion host is a best practice in this scenario","comment_id":"420801","timestamp":"1628252880.0","comments":[{"timestamp":"1632378360.0","upvote_count":"4","poster":"Rzla","comment_id":"449953","content":"Its C. No connection from on-prem to GCP so you cannot SSH direct to a bastion without a public IP. You need to use IAP."}],"poster":"kravenn","upvote_count":"2"},{"comments":[{"timestamp":"1629100680.0","upvote_count":"2","poster":"xavi1","comment_id":"425663","content":"when creating the bastion, you will need a public ip"},{"content":"definitely C: https://cloud.google.com/iap/docs/using-tcp-forwarding","timestamp":"1631781300.0","poster":"PleeO","upvote_count":"3","comment_id":"445764"}],"timestamp":"1627276980.0","content":"Answer: C","upvote_count":"9","poster":"vimal1206","comment_id":"414417"},{"content":"QUESTION 145 - New Q, 06/2021\nYour company sends all Google Cloud logs to Cloud Logging. Your security team wants to monitor the logs. You want to ensure that the security team can react quickly if an anomaly such as an unwanted firewall change or server breach is detected. You want to follow Google-recommended practices. What should you do?\n\nA. Schedule a cron job with Cloud Scheduler. The scheduled job queries the logs every minute for the relevant events.\nB. Export logs to BigQuery, and trigger a query in BigQuery to process the log data for the relevant events.\nC. Export logs to a Pub/Sub topic, and trigger Cloud Function with the relevant log events.\nD. Export logs to a Cloud Storage bucket, and trigger Cloud Run with the relevant log events.","comment_id":"394353","poster":"kopper2019","timestamp":"1625018460.0","comments":[{"timestamp":"1632378480.0","content":"Answer C - https://cloud.google.com/logging/docs/export/using_exported_logs#pubsub-overview\nPub sub can trigger a function, can also sink to 3rd party SIEM etc","upvote_count":"2","comment_id":"449955","comments":[{"timestamp":"1640376360.0","comment_id":"508837","content":"based on your link, export logs through pubsub is recommended when we want to integrate these logs with third party apps.\ni m for B","upvote_count":"1","poster":"tifo16"}],"poster":"Rzla"}],"upvote_count":"1"},{"content":"Answer D","comment_id":"420803","poster":"kravenn","timestamp":"1628252940.0","upvote_count":"1"},{"timestamp":"1632380100.0","content":"D works. You can use a load balancer with URL maps to target storage buckets and functions. Functions scale on demand and firestore is good for userdata.","comment_id":"449979","upvote_count":"1","poster":"Rzla"},{"timestamp":"1626326520.0","content":"Answer is D","comment_id":"406787","upvote_count":"1","poster":"Papafel"},{"content":"Not D. Cloud functions require ESPv2 container to Cloud Run for APIs (https://cloud.google.com/endpoints/docs/openapi/get-started-cloud-functions). hence C is the most suitable answer.","comment_id":"493489","upvote_count":"1","poster":"otts","timestamp":"1638589860.0"},{"poster":"kopper2019","content":"QUESTION 143 - New Q, 06/2021\nYour company has a networking team and a development team. The development team runs applications on Compute Engine instances that contain sensitive data. The development team requires administrative permissions for Compute Engine. Your company requires all network resources to be managed by the networking team. The development team does not want the networking team to have access to the sensitive data on the instances. What should you do?\n\nA. 1. Create a project with a standalone VPC and assign the Network Admin role to the networking team.\n2. Create a second project with a standalone VPC and assign the Compute Admin role to the development team.\n3. Use Cloud VPN to join the two VPCs.\nB. 1. Create a project with a standalone Virtual Private Cloud (VPC), assign the Network Admin role to the networking team, and assign the Compute Admin role to the development team.","comments":[{"upvote_count":"4","comment_id":"420804","poster":"kravenn","timestamp":"1628253000.0","content":"answer C"},{"content":"answer C","comment_id":"441981","poster":"Manh","upvote_count":"2","timestamp":"1631191260.0"}],"upvote_count":"1","timestamp":"1625018400.0","comment_id":"394350"},{"poster":"kravenn","content":"Answer A","upvote_count":"5","timestamp":"1628253060.0","comment_id":"420805"},{"timestamp":"1625018340.0","poster":"kopper2019","comments":[{"content":"B - Low cost-> Cloud run, real time -> Bigtable","comment_id":"440600","upvote_count":"6","poster":"LisX","timestamp":"1630970160.0"},{"comment_id":"449998","upvote_count":"2","poster":"Rzla","timestamp":"1632380940.0","content":"Answer B. Cloud run can scale in line with required request rate and then scale to 0. Big Table is also used where high throughput is required and can do real time analytics."},{"comment_id":"406794","content":"IMO the answer is D","timestamp":"1626326940.0","poster":"Papafel","upvote_count":"2"},{"upvote_count":"1","timestamp":"1628256180.0","comment_id":"420830","content":"answer A,\nkeep cost low due to Cloud Run and BigQuery, also 500.000 is exactly the sum of rows per second streamed to all tables for a given project within a multi-region for BigQuery","poster":"kravenn","comments":[{"comment_id":"441983","poster":"Manh","content":"store -> mean write . big querry is for reading. so Bigtable is correct","timestamp":"1631191440.0","upvote_count":"2"},{"content":"100000 - limit for BQ, answer is big table","comment_id":"460168","timestamp":"1633886640.0","upvote_count":"1","poster":"rottzy"}]},{"comment_id":"420822","upvote_count":"1","poster":"kravenn","timestamp":"1628255460.0","content":"answer A, \nkeep cost low due to cloud run and bigquery, also 500.000 is the sum of rows per second streamed to all tables for a given project within a multi-region"}],"upvote_count":"1","comment_id":"394348","content":"QUESTION 141 - New Q, 06/2021\nYou are implementing the infrastructure for a web service on Google Cloud. The web service needs to\n \nreceive and store the data from 500,000 requests per second. The data will be queried later in real time, based on exact matches of a known set of attributes. There will be periods where the web service will not receive any requests. The business wants to keep costs low. Which web service platform and database should you use for the application?\n\nA. Cloud Run and BigQuery\nB. Cloud Run and Cloud Bigtable\nC. A Compute Engine autoscaling managed instance group and BigQuery\nD. A Compute Engine autoscaling managed instance group and Cloud Bigtable"},{"upvote_count":"6","comment_id":"396206","poster":"cloudstd","content":"ans; D","timestamp":"1625158800.0"},{"upvote_count":"1","timestamp":"1678669320.0","poster":"Deb2293","content":"Easiest and quickest option is D","comment_id":"837507"},{"timestamp":"1625018280.0","comment_id":"394345","poster":"kopper2019","upvote_count":"2","comments":[{"poster":"kravenn","content":"Answer C","comment_id":"420832","upvote_count":"6","timestamp":"1628256240.0"},{"poster":"Neo_ACE","timestamp":"1637172360.0","upvote_count":"1","content":"I choose D because, C says \"builds the container and stores it in Container Registry\".\nCloud build can build container images not containers.\nhttps://cloud.google.com/container-registry/docs/overview\n\nSo D seems more appropriate","comment_id":"480198"},{"timestamp":"1628937540.0","content":"D looks more appropriate, it ensures successful tested code proceed ahead and deployed to dev cluster.","poster":"Ajay20","comment_id":"424745","comments":[{"content":"I think you meant C as D isn't doing any build time tests.","upvote_count":"2","comment_id":"450016","poster":"Rzla","timestamp":"1632382080.0"}],"upvote_count":"3"}],"content":"QUESTION 139 - New Q, 06/2021\nYour team will start developing a new application using microservices architecture on Kubernetes Engine. As part of the development lifecycle, any code change that has been pushed to the remote develop branch on your GitHub repository should be built and tested automatically. When the build and test are successful, the relevant microservice will be deployed automatically in the development environment. You want to ensure that all code deployed in the development environment follows this process. What should you do?\n\nA. Have each developer install a pre-commit hook on their workstation that tests the code and builds the container when committing on the development branch. After a successful commit, have the developer deploy the newly built container image on the development cluster.\nB. Install a post-commit hook on the remote git repository that tests the code and builds the container when code is pushed to the development branch. After a successful commit, have the developer deploy the newly built container image on the development cluster."},{"poster":"kopper2019","timestamp":"1625018220.0","upvote_count":"1","comments":[],"content":"QUESTION 138 - New Q, 06/2021\nYou are working at a financial institution that stores mortgage loan approval documents on Cloud Storage. Any change to these approval documents must be uploaded as a separate approval file, so you want to ensure that these documents cannot be deleted or overwritten for the next 5 years. What should you do?\n\nA. Create a retention policy on the bucket for the duration of 5 years. Create a lock on the retention policy.\nB. Create the bucket with uniform bucket-level access, and grant a service account the role of Object Writer. Use the service account to upload new files.\nC. Use a customer-managed key for the encryption of the bucket. Rotate the key after 5 years.\n \nD. Create the bucket with fine-grained access control, and grant a service account the role of Object Writer. Use the service account to upload new files.","comment_id":"394344"},{"upvote_count":"4","comments":[{"poster":"tifo16","upvote_count":"2","comment_id":"508874","timestamp":"1640382180.0","content":"https://cloud.google.com/service-mesh/docs/observability/explore-dashboard"}],"comment_id":"420834","poster":"kravenn","timestamp":"1628256300.0","content":"Answer A"},{"timestamp":"1625018160.0","comments":[{"upvote_count":"2","comments":[{"timestamp":"1626844800.0","upvote_count":"2","content":"C is the correct answer.","comment_id":"410652","poster":"Papafel"}],"timestamp":"1625533800.0","poster":"Argst","content":"C as gcsfuse is open source and filestore is fully managed","comment_id":"399566"}],"upvote_count":"2","content":"QUESTION 136 - New Q, 06/2021\nYou need to deploy a stateful workload on Google Cloud. The workload can scale horizontally, but each instance needs to read and write to the same POSIX filesystem. At high load, the stateful workload needs to support up to 100 MB/s of writes. What should you do?\n\nA. Use a persistent disk for each instance.\nB. Use a regional persistent disk for each instance.\nC. Create a Cloud Filestore instance and mount it in each instance.\nD. Create a Cloud Storage bucket and mount it in each instance using gcsfuse.","comment_id":"394340","poster":"kopper2019"},{"content":"Not sure on this one. A seems incomplete as an alert when error occurred would be useful as not been able to replicate. However with C I dont see what value is Prometheus adding and you dont just simply deploy it like you enable cloud logging!","upvote_count":"1","comment_id":"450041","timestamp":"1632384480.0","poster":"Rzla"},{"timestamp":"1628256540.0","comment_id":"420839","upvote_count":"4","content":"answer A","poster":"kravenn"},{"timestamp":"1630735500.0","content":"can also be C, prometheus is a real option.","poster":"xavi1","upvote_count":"2","comment_id":"438922","comments":[{"poster":"LisX","content":"C. Not replicated is the key","comment_id":"440605","upvote_count":"1","timestamp":"1630971960.0"},{"comment_id":"460174","timestamp":"1633888260.0","poster":"rottzy","content":"https://cloud.google.com/stackdriver/docs/solutions/gke/observing\nstart using GKE dashboard - promethous ~ open source dashboard for k8s","upvote_count":"1"}]},{"upvote_count":"2","content":"QUESTION 134 - New Q, 06/2021\nYou need to deploy an application on Google Cloud that must run on a Debian Linux environment. The application requires extensive configuration in order to operate correctly. You want to ensure that you can install Debian distribution updates with minimal manual intervention whenever they become available. What should you do?\n\nA. Create a Compute Engine instance template using the most recent Debian image. Create an instance from this template, and install and configure the application as part of the startup script. Repeat this process whenever a new Google-managed Debian image becomes available.\nB. Create a Debian-based Compute Engine instance, install and configure the application, and use OS patch management to install available updates.","comments":[{"content":"C. Create an instance with the latest available Debian image. Connect to the instance via SSH, and install and configure the application on the instance. Repeat this process whenever a new Google-managed Debian image becomes available.\nD. Create a Docker container with Debian as the base image. Install and configure the application as part of the Docker image creation process. Host the container on Google Kubernetes Engine and restart the container whenever a new update is available.","timestamp":"1625018100.0","upvote_count":"1","comments":[{"comment_id":"407795","timestamp":"1626432120.0","upvote_count":"5","poster":"kamilC","content":"B https://cloud.google.com/compute/docs/os-patch-management"}],"poster":"kopper2019","comment_id":"394337"}],"timestamp":"1625018100.0","poster":"kopper2019","comment_id":"394336"},{"poster":"kopper2019","upvote_count":"1","timestamp":"1625017980.0","comments":[{"timestamp":"1628256600.0","poster":"kravenn","content":"answer B","upvote_count":"3","comment_id":"420840"},{"comments":[{"content":"https://www.examtopics.com/discussions/google/view/56416-exam-professional-cloud-architect-topic-1-question-120/","comment_id":"838408","poster":"Deb2293","timestamp":"1678749540.0","upvote_count":"1"}],"comment_id":"450042","content":"Answer B. Its not D as peering is not transitive so VPC 1 would have to peer with 2 & 3 directly.","timestamp":"1632384780.0","upvote_count":"1","poster":"Rzla"}],"comment_id":"394334","content":"QUESTION 133 - New Q, 06/2021\nYour company has a project in Google Cloud with three Virtual Private Clouds (VPCs). There is a Compute Engine instance on each VPC. Network subnets do not overlap and must remain separated. The network configuration is shown below.\nPicture includes 3 VPC and 1 subnet in each VPC there are not interconnected\nInstance #1 is an exception and must communicate directly with both Instance #2 and Instance #3 via internal IPs. How should you accomplish this?\n\nA. Create a cloud router to advertise subnet #2 and subnet #3 to subnet #1.\nB. Add two additional NICs to Instance #1 with the following configuration:\n- NIC1\nVPC: VPC #2\nSUBNETWORK: subnet #2\n- NIC2\nVPC: VPC #3\nSUBNETWORK: subnet #3\nUpdate firewall rules to enable traffic between instances.\nC. Create two VPN tunnels via CloudVPN:\n- 1 between VPC #1 and VPC #2.\n- 1 between VPC #2 and VPC #3.\nUpdate firewall rules to enable traffic between the instances.\nD. Peer all three VPCs:\n- Peer VPC #1 with VPC #2.\n- Peer VPC #2 with VPC #3.\nUpdate firewall rules to enable traffic between the instances."},{"timestamp":"1625017800.0","comments":[{"poster":"Phil_z","upvote_count":"2","comment_id":"421094","comments":[{"comments":[{"poster":"xavi1","timestamp":"1630735680.0","comment_id":"438924","upvote_count":"1","content":"correcting myself, i won't use preemtible for \"standalone\" jobs/workloads... there is no guarantee they they will work.","comments":[{"comment_id":"460178","timestamp":"1633888860.0","content":"its a cluster - something is always available","upvote_count":"1","poster":"rottzy"}]}],"comment_id":"425688","timestamp":"1629103560.0","content":"IMO B, you want to minimize both cost and management","poster":"xavi1","upvote_count":"2"}],"content":"Answer : A","timestamp":"1628317260.0"},{"upvote_count":"2","comments":[{"comments":[{"upvote_count":"1","poster":"Medofree","comment_id":"768091","timestamp":"1673040180.0","content":"Only secondary workers can be preemptible as of 01/06/2023 https://cloud.google.com/sdk/gcloud/reference/dataproc/clusters/create"}],"comment_id":"460176","timestamp":"1633888800.0","content":"why secondary workers mentioned here?","upvote_count":"1","poster":"rottzy"}],"comment_id":"450095","content":"Answer B - By default secondary workers are pre-emptible, can be used for processing.\nhttps://cloud.google.com/dataproc/docs/concepts/compute/secondary-vms#preemptible_and_non-preemptible_secondary_workers","timestamp":"1632387420.0","poster":"Rzla"}],"content":"QUESTION 132 - New Q, 06/2021\nYou need to migrate Hadoop jobs for your company’s Data Science team without modifying the underlying infrastructure. You want to minimize costs and infrastructure management effort. What should you do?\n\nA. Create a Dataproc cluster using standard worker instances.\nB. Create a Dataproc cluster using preemptible worker instances.\nC. Manually deploy a Hadoop cluster on Compute Engine using standard instances.\nD. Manually deploy a Hadoop cluster on Compute Engine using preemptible instances.","comment_id":"394330","poster":"kopper2019","upvote_count":"2"},{"content":"Answer A.","timestamp":"1632387660.0","poster":"Rzla","upvote_count":"2","comment_id":"450099"},{"content":"answer is C","poster":"fahad01hbti","upvote_count":"2","timestamp":"1629865680.0","comment_id":"431131"},{"content":"QUESTION 130 - New Q, 06/2021\nYour company is designing its application landscape on Compute Engine. Whenever a zonal outage occurs, the application should be restored in another zone as quickly as possible with the latest application data. You need to design the solution to meet this requirement. What should you do?\n\nA. Create a snapshot schedule for the disk containing the application data. Whenever a zonal outage occurs, use the latest snapshot to restore the disk in the same zone.\nB. Configure the Compute Engine instances with an instance template for the application, and use a regional persistent disk for the application data. Whenever a zonal outage occurs, use the instance template to spin up the application in another zone in the same region. Use the regional persistent disk for the application data.","comment_id":"394327","comments":[{"timestamp":"1625017740.0","upvote_count":"1","poster":"kopper2019","comments":[{"poster":"kravenn","upvote_count":"4","comment_id":"420844","content":"answer B","timestamp":"1628256660.0"}],"content":"C. Create a snapshot schedule for the disk containing the application data. Whenever a zonal outage occurs, use the latest snapshot to restore the disk in another zone within the same region.\nD. Configure the Compute Engine instances with an instance template for the application, and use a regional persistent disk for the application data. Whenever a zonal outage occurs, use the instance template to spin up the application in another region. Use the regional persistent disk for the application data,","comment_id":"394328"}],"timestamp":"1625017740.0","upvote_count":"1","poster":"kopper2019"},{"content":"QUESTION 129 - New Q, 06/2021\nYou have developed a non-critical update to your application that is running in a managed instance group,\n \nand have created a new instance template with the update that you want to release. To prevent any possible impact to the application, you don't want to update any running instances. You want any new instances that are created by the managed instance group to contain the new update. What should you do?\n\nA. Start a new rolling restart operation.\nB. Start a new rolling replace operation.\nC. Start a new rolling update. Select the Proactive update mode.\nD. Start a new rolling update. Select the Opportunistic update mode.","comment_id":"394326","poster":"kopper2019","timestamp":"1625017680.0","upvote_count":"1","comments":[{"content":"answer: B","upvote_count":"1","poster":"cloudstd","timestamp":"1625157660.0","comments":[{"comment_id":"397212","comments":[{"upvote_count":"1","content":"Yes i will go for D","comment_id":"410655","poster":"Papafel","timestamp":"1626844920.0"}],"upvote_count":"5","content":"Answer is D","poster":"kopper2019","timestamp":"1625274660.0"},{"timestamp":"1629104100.0","poster":"xavi1","content":"IMO D, it says \"to prevent any possible impact\"","upvote_count":"2","comment_id":"425696"}],"comment_id":"396193"}]},{"poster":"kravenn","timestamp":"1628256720.0","upvote_count":"1","comment_id":"420847","content":"answer A"},{"poster":"prassuu","upvote_count":"1","comment_id":"433909","timestamp":"1630156860.0","content":"B is okay"},{"content":"B, https://cloud.google.com/vpc/docs/using-firewall-rules-logging","poster":"xavi1","upvote_count":"5","timestamp":"1629104400.0","comment_id":"425699"},{"content":"QUESTION 126 - New Q, 06/2021\nYour organization has decided to restrict the use of external IP addresses on instances to only approved instances. You want to enforce this requirement across all of your Virtual Private Clouds (VPCs). What should you do?\n \nA. Remove the default route on all VPCs. Move all approved instances into a new subnet that has a default route to an internet gateway.\nB. Create a new VPC in custom mode. Create a new subnet for the approved instances, and set a default route to the internet gateway on this new subnet.\nC. Implement a Cloud NAT solution to remove the need for external IP addresses entirely.\nD. Set an Organization Policy with a constraint on constraints/compute.vmExternalIpAccess. List the approved instances in the allowedValues list.","comments":[{"poster":"cloudstd","comment_id":"396189","upvote_count":"5","timestamp":"1625157480.0","content":"answer: D"}],"poster":"kopper2019","timestamp":"1625017620.0","upvote_count":"1","comment_id":"394321"},{"upvote_count":"1","timestamp":"1624930920.0","comment_id":"393403","content":"Q125 - Ans C \nExternal replica promotion migration\nIn the migration strategy of external replica promotion, you create an external database replica and synchronize the existing data to that replica. This can happen with minimal downtime to the existing database.\nWhen you have a replica database, the two databases have different roles that are referred to in this document as primary and replica.\nAfter the data is synchronized, you promote the replica to be the primary in order to move the management layer with minimal impact to database uptime.\nIn Cloud SQL, an easy way to accomplish the external replica promotion is to use the automated migration workflow. This process automates many of the steps that are needed for this type of migration.","poster":"muhasinem"},{"comments":[{"upvote_count":"3","comment_id":"391618","poster":"kopper2019","comments":[{"timestamp":"1624931100.0","content":"Q125 - Ans C\nExternal replica promotion migration\nIn the migration strategy of external replica promotion, you create an external database replica and synchronize the existing data to that replica. This can happen with minimal downtime to the existing database.\nWhen you have a replica database, the two databases have different roles that are referred to in this document as primary and replica.\nAfter the data is synchronized, you promote the replica to be the primary in order to move the management layer with minimal impact to database uptime.\nIn Cloud SQL, an easy way to accomplish the external replica promotion is to use the automated migration workflow. This process automates many of the steps that are needed for this type of migration.","upvote_count":"5","poster":"muhasinem","comment_id":"393406"}],"timestamp":"1624750080.0","content":"C. 1. Set up Cloud VPN to provide private network connectivity between the Compute Engine application and the on-premises MySQL server.\n2. Stop the on-premises application.\n3. Start the Compute Engine application, configured to read and write to the on-premises MySQL server.\n4. Create the replication configuration in Cloud SQL.\n5. Configure the source database server to accept connections from the Cloud SQL replica.\n6. Finalize the Cloud SQL replica configuration.\n7. When replication has been completed, stop the Compute Engine application.\n8. Promote the Cloud SQL replica to a standalone instance.\n9. Restart the Compute Engine application, configured to read and write to the Cloud SQL standalone instance.\nD. 1. Stop the on-premises application.\n2. Create a mysqldump of the on-premises MySQL server.\n3. Upload the dump to a Cloud Storage bucket.\n4. Import the dump into Cloud SQL.\n5. Start the application on Compute Engine."}],"upvote_count":"1","content":"QUESTION 125 - New Q, 06/2021\nYou are moving an application that uses MySQL from on-premises to Google Cloud. The application will run on Compute Engine and will use Cloud SQL. You want to cut over to the Compute Engine deployment of the application with minimal downtime and no data loss to your customers. You want to migrate the application with minimal modification. You also need to determine the cutover strategy. What should you do?\n\nA. 1. Set up Cloud VPN to provide private network connectivity between the Compute Engine application and the on-premises MySQL server.\n2. Stop the on-premises application.\n3. Create a mysqldump of the on-premises MySQL server.\n4. Upload the dump to a Cloud Storage bucket.\n5. Import the dump into Cloud SQL.\n6. Modify the source code of the application to write queries to both databases and read from its local database.\n7. Start the Compute Engine application.\n8. Stop the on-premises application.\nB. 1. Set up Cloud SQL proxy and MySQL proxy.\n2. Create a mysqldump of the on-premises MySQL server.\n3. Upload the dump to a Cloud Storage bucket.\n4. Import the dump into Cloud SQL.\n5. Stop the on-premises application.\n6. Start the Compute Engine application.","timestamp":"1624750080.0","poster":"kopper2019","comment_id":"391617"},{"comment_id":"391616","poster":"kopper2019","upvote_count":"2","content":"QUESTION 124 - New Q, 06/2021\nYour company is running its application workloads on Compute Engine. The applications have been deployed in production, acceptance, and development environments. The production environment is business-critical and is used 24/7, while the acceptance and development environments are only critical during office hours. Your CFO has asked you to optimize these environments to achieve cost savings during idle times. What should you do?\n\nA. Create a shell script that uses the gcloud command to change the machine type of the development and acceptance instances to a smaller machine type outside of office hours. Schedule the shell script on one of the production instances to automate the task.\nB. Use Cloud Scheduler to trigger a Cloud Function that will stop the development and acceptance environments after office hours and start them just before office hours.\nC. Deploy the development and acceptance applications on a managed instance group and enable\n \nautoscaling.\nD. Use regular Compute Engine instances for the production environment, and use preemptible VMs for the acceptance and development environments.","timestamp":"1624750020.0","comments":[{"timestamp":"1625015820.0","content":"Ans ) B , assuming VM doesn't need to be up after office hours .","comments":[{"timestamp":"1632392400.0","comment_id":"450139","poster":"gigibit","comments":[{"comment_id":"460190","poster":"rottzy","timestamp":"1633892880.0","upvote_count":"1","content":"better than D - its not a cluster!"}],"content":"Its theorical B; but this solution would not be accetable in reality :)","upvote_count":"1"}],"upvote_count":"6","poster":"muhasinem","comment_id":"394311"}]},{"timestamp":"1624749960.0","content":"QUESTION 123 - New Q, 06/2021\n\nYour company has announced that they will be outsourcing operations functions. You want to allow developers to easily stage new versions of a cloud-based application in the production environment and allow the outsourced operations team to autonomously promote staged versions to production. You want to minimize the operational overhead of the solution. Which Google Cloud product should you migrate to?\n\nA. App Engine\nB. GKE On-Prem\nC. Compute Engine\nD. Google Kubernetes Engine","comment_id":"391615","comments":[{"timestamp":"1624932060.0","content":"A. App Engine","comment_id":"393411","poster":"muhasinem","upvote_count":"4"}],"upvote_count":"1","poster":"kopper2019"},{"timestamp":"1632390180.0","poster":"Rzla","content":"Answer B - This is a great use case for GDPR right to be forgotten.","upvote_count":"1","comment_id":"450118"},{"poster":"Manh","upvote_count":"1","content":"answer B","comment_id":"442007","timestamp":"1631194500.0"},{"content":"Ans) B","comment_id":"394362","upvote_count":"4","poster":"muhasinem","comments":[{"upvote_count":"3","poster":"xavi1","timestamp":"1629107160.0","content":"correct, you only need to delete \"such\" information, not \"all\".","comment_id":"425720","comments":[{"timestamp":"1642522020.0","content":"where does option B talk about deletion, it just says querying such information","comment_id":"526799","upvote_count":"1","poster":"Wonka"}]}],"timestamp":"1625018640.0"},{"timestamp":"1628256840.0","content":"answer A","comments":[{"comment_id":"450143","timestamp":"1632392640.0","poster":"gigibit","content":"Why A? it should be B, you don't have to delete everything, just \"such\" information","upvote_count":"2"}],"poster":"kravenn","upvote_count":"4","comment_id":"420849"},{"poster":"gigibit","comment_id":"450154","timestamp":"1632393720.0","content":"C and D. Binary Logging & Automated Backups","upvote_count":"1"},{"upvote_count":"5","content":"Ans) C and D\nCloud SQL. If you use Cloud SQL, the fully managed Google Cloud MySQL database, you should enable automated backups and binary logging for your Cloud SQL instances. This allows you to perform a point-in-time recovery, which restores your database from a backup and recovers it to a fresh Cloud SQL instance","timestamp":"1624977000.0","poster":"muhasinem","comment_id":"393864"},{"poster":"kopper2019","comments":[{"upvote_count":"3","content":"answer D","comment_id":"420850","timestamp":"1628256900.0","comments":[{"poster":"asdfzxcv123","upvote_count":"1","timestamp":"1629005760.0","content":"test test","comment_id":"425061"}],"poster":"kravenn"},{"upvote_count":"6","comment_id":"393880","content":"Ans ) A","timestamp":"1624977540.0","poster":"muhasinem"},{"content":"answer A","comment_id":"442008","timestamp":"1631194560.0","poster":"Manh","upvote_count":"3"}],"upvote_count":"2","comment_id":"391612","content":"QUESTION 120 New Q, 06/2021\nYou are monitoring Google Kubernetes Engine (GKE) clusters in a Cloud Monitoring workspace. As a Site Reliability Engineer (SRE), you need to triage incidents quickly. What should you do?\n\nA. Navigate the predefined dashboards in the Cloud Monitoring workspace, and then add metrics and create alert policies.\nB. Navigate the predefined dashboards in the Cloud Monitoring workspace, create custom metrics, and install alerting software on a Compute Engine instance.\nC. Write a shell script that gathers metrics from GKE nodes, publish these metrics to a Pub/Sub topic, export the data to BigQuery, and make a Data Studio dashboard.\nD. Create a custom dashboard in the Cloud Monitoring workspace for each incident, and then add metrics and create alert policies.","timestamp":"1624749900.0"},{"content":"A also for me","upvote_count":"1","comment_id":"450157","timestamp":"1632393840.0","poster":"gigibit"},{"upvote_count":"7","timestamp":"1624977660.0","content":"Ans ) A","comment_id":"393883","poster":"muhasinem"},{"content":"New Q, 06/2021 - QUESTION 118\nYour company has an application running on multiple Compute Engine instances. You need to ensure that the application can communicate with an on-premises service that requires high throughput via internal IPs, while minimizing latency. What should you do?\n\nA. Use OpenVPN to configure a VPN tunnel between the on-premises environment and Google Cloud.\nB. Configure a direct peering connection between the on-premises environment and Google Cloud.\nC. Use Cloud VPN to configure a VPN tunnel between the on-premises environment and Google Cloud.\nD. Configure a Cloud Dedicated Interconnect connection between the on-premises environment and Google Cloud.","comments":[{"timestamp":"1624977780.0","content":"Ans ) D , Reason : high throughput via internal IPs","upvote_count":"8","poster":"muhasinem","comment_id":"393886"}],"upvote_count":"1","timestamp":"1624749840.0","poster":"kopper2019","comment_id":"391609"},{"comments":[{"content":"answer D","upvote_count":"2","timestamp":"1628256960.0","poster":"kravenn","comment_id":"420852"},{"upvote_count":"1","content":"Unmanaged instance group, and network load balancer. i would go for D","comment_id":"450158","poster":"gigibit","timestamp":"1632394020.0"},{"comment_id":"393908","poster":"muhasinem","timestamp":"1624978620.0","content":"Ans ) D , unmanaged instance group as application does not support horizontal scaling and network load balancer as no mention of http traffic .","upvote_count":"6"}],"upvote_count":"1","comment_id":"391606","poster":"kopper2019","content":"New Q, 06/2021 - QUESTION 117\nYou need to deploy an application to Google Cloud. The application receives traffic via TCP and reads and writes data to the filesystem. The application does not support horizontal scaling. The application process requires full control over the data on the file system because concurrent access causes corruption. The business is willing to accept a downtime when an incident occurs, but the application must be available 24/7 to support their business operations. You need to design the architecture of this application on Google Cloud. What should you do?","timestamp":"1624749780.0"},{"comment_id":"391605","poster":"kopper2019","timestamp":"1624749720.0","upvote_count":"1","content":"New Q, 06/2021\nQUESTION 116\nYour company is planning to perform a lift and shift migration of their Linux RHEL 6.5+ virtual machines. The virtual machines are running in an on-premises VMware environment. You want to migrate them to Compute Engine following Google-recommended practices. What should you do?\n\nA. 1. Define a migration plan based on the list of the applications and their dependencies.\n2. Migrate all virtual machines into Compute Engine individually with Migrate for Compute Engine.\nB. 1. Perform an assessment of virtual machines running in the current VMware environment.\n2. Create images of all disks. Import disks on Compute Engine.\n3. Create standard virtual machines where the boot disks are the ones you have imported.\nC. 1. Perform an assessment of virtual machines running in the current VMware environment.\n2. Define a migration plan, prepare a Migrate for Compute Engine migration RunBook, and execute the migration.\nD. 1. Perform an assessment of virtual machines running in the current VMware environment.\n2. Install a third-party agent on all selected virtual machines.\n3. Migrate all virtual machines into Compute Engine.","comments":[{"content":"Answer C.\nAccess, plan, deploy, optimize.\nhttps://cloud.google.com/architecture/migration-to-google-cloud-optimizing-your-environment","upvote_count":"1","comment_id":"450160","poster":"gigibit","timestamp":"1632394140.0"},{"comment_id":"394262","timestamp":"1625009460.0","poster":"muhasinem","upvote_count":"5","content":"Ans ) C , \nMigrate for Compute Engine organizes groups of VMs into Waves. After understanding the dependencies of your applications, create runbooks that contain groups of VMs and begin your migration!\nhttps://cloud.google.com/migrate/compute-engine/docs/4.5/how-to/migrate-on-premises-to-gcp/overview"}]},{"content":"Seems answer is c as all objects can be stored in single bucket.","comments":[{"timestamp":"1622704140.0","comment_id":"373332","comments":[{"timestamp":"1625687520.0","poster":"fzz","upvote_count":"5","content":"Your answer of D makes a lot of sense, and thanks for the pointer! It's sad that your comment is buried at the end of the thread where most of the comments are irrelevant to this question.","comment_id":"401212"}],"upvote_count":"8","poster":"rishab86","content":"although after read https://medium.com/@duhroach/google-cloud-storage-sequential-file-names-f1ba1205c6a8 it seems answer is D."}],"poster":"rishab86","comment_id":"373320","upvote_count":"4","timestamp":"1622703300.0"}],"answer_ET":"D","url":"https://www.examtopics.com/discussions/google/view/54369-exam-professional-cloud-architect-topic-1-question-160/"},{"id":"5l4dyzDUxhTIa0ldKNM1","answer_images":[],"answers_community":["C (100%)"],"isMC":true,"exam_id":4,"question_images":[],"answer_description":"","answer":"C","discussion":[{"content":"I am going to go with C. Answer A doesn't seem to fit because the matter of when a VM was created.\nAnswer B focuses on Data Access logs which doesn't seem to fit since the matter of creating a network firewall rule\nis an Admin activity, not a data access activity.\nD focuses on who logged in which is good to know but doesn't answer the question of how the network was created.\nC focuses on logging, the selection of network events, and the Create/Insert entry.","upvote_count":"18","poster":"clouddude","comment_id":"86138","timestamp":"1636486380.0"},{"poster":"Eroc","comments":[{"comment_id":"150977","poster":"tartar","upvote_count":"14","content":"C is ok","timestamp":"1644054480.0"}],"upvote_count":"17","content":"When you search for Create Insert, it displays a JSON code string that contains the creators e-mail","timestamp":"1619108760.0","comment_id":"16763"},{"poster":"AugustoKras011111","content":"Selected Answer: C\nC is ok to me!","upvote_count":"1","timestamp":"1724858820.0","comment_id":"825086"},{"poster":"NodummyIQ","timestamp":"1719866820.0","comments":[{"poster":"n_nana","timestamp":"1720769700.0","comment_id":"773312","upvote_count":"3","content":"Question is asking about network origin creation not VM creation. that's why is C"}],"comment_id":"763366","upvote_count":"1","content":"Option C is incorrect because the GCE Network logs are not the correct place to search for the creation of a VM instance. The correct place to search for this information is the Activity page, as specified in option B."},{"poster":"megumin","upvote_count":"1","content":"Selected Answer: C\nC is ok","timestamp":"1715326740.0","comment_id":"715101"},{"upvote_count":"1","comment_id":"699423","timestamp":"1713571320.0","poster":"Mahmoud_E","content":"Selected Answer: C\nC is the right answer"},{"comment_id":"695583","poster":"AzureDP900","content":"C is right","upvote_count":"2","timestamp":"1713204180.0"},{"timestamp":"1696614600.0","comment_id":"581955","content":"Sorry to gripe again, but why on Earth would anybody need to remember this from the top of their mind. You will never be in a situation in which you need to remember this without looking at the available options in the console (or simply Googling it, lol).","poster":"cloudmon","upvote_count":"12"},{"comment_id":"501838","poster":"vincy2202","timestamp":"1686793440.0","content":"Selected Answer: C\nC is the correct answer","upvote_count":"1"},{"comment_id":"497532","upvote_count":"1","timestamp":"1686295860.0","poster":"Bobch","content":"Selected Answer: C\nVote C"},{"comment_id":"487185","upvote_count":"2","timestamp":"1685083080.0","poster":"joe2211","content":"Selected Answer: C\nvote C"},{"timestamp":"1675895280.0","poster":"muneebarshad","content":"In Logs Explorer , Filter \"resource.type=\"gce_firewall_rule\" and Query insert Create\n\nYou would see below and email address\n \"methodName\": \"v1.compute.firewalls.insert\",\n \"authorizationInfo\": [\n {\n \"permission\": \"compute.firewalls.create\",","comment_id":"421822","upvote_count":"2"},{"poster":"bala786","upvote_count":"2","timestamp":"1673054700.0","content":"Option C is correct, because logging section is the correct choice to get this details","comment_id":"400348"},{"poster":"victory108","upvote_count":"2","timestamp":"1668755940.0","content":"C - In the Logging section of the console, specify GCE Network as the logging section. Search for the Create Insert entry","comment_id":"360067"},{"poster":"un","content":"C is correct","upvote_count":"2","timestamp":"1667584260.0","comment_id":"349572"},{"comment_id":"323107","upvote_count":"2","poster":"Ausias18","content":"Answer is C","timestamp":"1664426640.0"},{"content":"Agree..C","timestamp":"1657811700.0","upvote_count":"3","comment_id":"267202","poster":"willan"},{"comment_id":"216291","upvote_count":"2","poster":"CYL","timestamp":"1652135580.0","content":"C. Correct place to search on network configuration."},{"comment_id":"202799","upvote_count":"3","timestamp":"1650398160.0","poster":"emybreth","content":"Selected C as final for me, passed exam"},{"upvote_count":"2","timestamp":"1649772780.0","content":"GO with c","poster":"Aru23","comment_id":"198463"},{"comment_id":"189034","poster":"whitley030390","content":"C makes the most sense to me","upvote_count":"2","timestamp":"1648472400.0"},{"timestamp":"1648341240.0","upvote_count":"2","poster":"AshokC","content":"C is right","comment_id":"188005"},{"comment_id":"116586","upvote_count":"2","poster":"mlantonis","timestamp":"1640197620.0","content":"I agree C is the correct"},{"content":"C is the correct answer","poster":"Nirms","comment_id":"100794","upvote_count":"3","timestamp":"1638454800.0"},{"timestamp":"1638181980.0","content":"C is the correct answer","poster":"Ziegler","comment_id":"98115","upvote_count":"3"},{"upvote_count":"2","timestamp":"1637709240.0","content":"Answer C","poster":"Javed","comment_id":"94568"},{"content":"Google Cloud Logging provides 3 audit logs for each project, folder and organization:\n1. Admin activity 2. Data Access 3. System Event audit Log\nAdmin Activity: Create/Modify resources & metadata - create VM/VPC etc - Always On, Cannot disable, No Charge.\nData Access: Read the config and metadata & user driven meta data - Disabled by default, Enable - incur additional cost to project.\nSystem Event: Generated by Google Systems and not user driven - Always on , cannot disable, no charge.","upvote_count":"4","timestamp":"1629877380.0","poster":"Rathish","comment_id":"54917"},{"content":"answer: C","poster":"2g","comment_id":"44666","upvote_count":"1","timestamp":"1627641360.0"},{"timestamp":"1622591640.0","comment_id":"25855","upvote_count":"1","poster":"JJu","content":"Agree C"},{"content":"Agree C","comment_id":"21969","poster":"AWS56","upvote_count":"2","timestamp":"1621167600.0"}],"url":"https://www.examtopics.com/discussions/google/view/7016-exam-professional-cloud-architect-topic-1-question-161/","unix_timestamp":1571761560,"question_id":70,"topic":"1","timestamp":"2019-10-22 18:26:00","choices":{"D":"Connect to the GCE instance using project SSH keys. Identify previous logins in system logs, and match these with the project owners list","C":"In the Logging section of the console, specify GCE Network as the logging section. Search for the Create Insert entry","B":"Navigate to the Activity page in the Home section. Set category to Data Access and search for Create VM entry","A":"Search for Create VM entry in the Stackdriver alerting console"},"question_text":"A recent audit revealed that a new network was created in your GCP project. In this network, a GCE instance has an SSH port open to the world. You want to discover this network's origin.\nWhat should you do?","answer_ET":"C"}],"exam":{"name":"Professional Cloud Architect","isMCOnly":false,"id":4,"numberOfQuestions":279,"isBeta":false,"lastUpdated":"11 Apr 2025","isImplemented":true,"provider":"Google"},"currentPage":14},"__N_SSP":true}