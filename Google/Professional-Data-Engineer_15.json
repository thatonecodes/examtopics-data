{"pageProps":{"questions":[{"id":"RG63NGynecUoHhfXkcUs","timestamp":"2020-03-18 02:11:00","exam_id":11,"answer_description":"","unix_timestamp":1584493860,"url":"https://www.examtopics.com/discussions/google/view/16902-exam-professional-data-engineer-topic-1-question-162/","question_text":"You want to archive data in Cloud Storage. Because some data is very sensitive, you want to use the `Trust No One` (TNO) approach to encrypt your data to prevent the cloud provider staff from decrypting your data. What should you do?","choices":{"A":"Use gcloud kms keys create to create a symmetric key. Then use gcloud kms encrypt to encrypt each archival file with the key and unique additional authenticated data (AAD). Use gsutil cp to upload each encrypted file to the Cloud Storage bucket, and keep the AAD outside of Google Cloud.","B":"Use gcloud kms keys create to create a symmetric key. Then use gcloud kms encrypt to encrypt each archival file with the key. Use gsutil cp to upload each encrypted file to the Cloud Storage bucket. Manually destroy the key previously used for encryption, and rotate the key once.","C":"Specify customer-supplied encryption key (CSEK) in the .boto configuration file. Use gsutil cp to upload each archival file to the Cloud Storage bucket. Save the CSEK in Cloud Memorystore as permanent storage of the secret.","D":"Specify customer-supplied encryption key (CSEK) in the .boto configuration file. Use gsutil cp to upload each archival file to the Cloud Storage bucket. Save the CSEK in a different project that only the security team can access."},"answers_community":["A (60%)","D (40%)"],"isMC":true,"question_images":[],"question_id":71,"topic":"1","discussion":[{"timestamp":"1585785000.0","upvote_count":"44","poster":"dhs227","comments":[{"poster":"[Removed]","comment_id":"518278","timestamp":"1641478380.0","upvote_count":"3","content":"The trust no one design philosophy requires that the keys for encryption should always be, and stay, in the hands of the user that applies them. This implies that no external party can access the encrypted data (assumed that the encryption is strong enough).\nhttps://en.wikipedia.org/wiki/Trust_no_one_(Internet_security)"},{"upvote_count":"4","poster":"mikey007","content":"AAD is bound to the encrypted data, because you cannot decrypt the ciphertext unless you know the AAD, but it is not stored as part of the ciphertext. AAD also does not increase the cryptographic strength of the ciphertext. Instead it is an additional check by Cloud KMS to authenticate a decryption request.","timestamp":"1598964120.0","comment_id":"171351"}],"content":"The correct answer must be D\nA and B can be eliminated immediately since kms generated keys are considered potentially accessible by CSP. \nC is incorrect because memory store is essentially a cache service. \n\nAdditional authenticated data (AAD) acts as a \"salt\", it is not a cipher.","comment_id":"70284"},{"content":"Answer: A\nDescription: AAD is used to decrypt the data so better to keep it outside GCP for safety","timestamp":"1585414440.0","upvote_count":"15","comment_id":"68924","poster":"[Removed]"},{"comment_id":"1559564","timestamp":"1744286460.0","upvote_count":"1","content":"Selected Answer: D\nThis is the correct option because:\n\nCustomer-supplied encryption keys (CSEKs) provide client-side encryption where you fully control the keys.\nBy specifying the CSEK in the .boto configuration file, the data is encrypted before it reaches Google's servers.\nStoring the keys in a different project with restricted access ensures proper separation.\nThis approach keeps the encryption keys entirely under your control, following the TNO principle.","poster":"aaaaaaaasdasdasfs"},{"poster":"Anudeep58","timestamp":"1718366460.0","content":"Selected Answer: A\nKeep AAD Outside of Google Cloud:\n\nKeeping the AAD outside of Google Cloud ensures that Google cannot access the additional context required to decrypt the files, thus implementing the TNO approach.\n\nOption C:\nCustomer-Supplied Encryption Key (CSEK) in .boto File:\nStoring the CSEK in Cloud Memorystore or any cloud service introduces a risk where the key could be potentially accessed by cloud provider staff.\nOption D:\nCustomer-Supplied Encryption Key (CSEK) in a Different Project:\nWhile storing the CSEK in a different project adds some security, it still leaves the keys within the Google Cloud environment, which does not fully meet the TNO approach.","upvote_count":"2","comment_id":"1230469"},{"content":"I just cannot understand this question. If you can't trust the provider, in this case Google, then how can you use the KMS approach. In my mind you have to generate the key locally and upload but I'm clearly wrong and don't get why.","poster":"emmylou","upvote_count":"1","comment_id":"1076470","timestamp":"1700585700.0"},{"timestamp":"1696000800.0","content":"Selected Answer: D\nIMO must be (D) : to reach TNO goal keys must be customer supplied.","upvote_count":"3","poster":"shanwford","comment_id":"1020900"},{"comment_id":"1016261","content":"Selected Answer: D\nCustomer-Supplied Encryption Key (CSEK): CSEK allows you to provide your encryption keys, ensuring that the cloud provider staff does not have access to the keys and cannot decrypt your data.\n\nSeparate Project for Key Management: Saving the CSEK in a different project that only the security team can access adds an additional layer of security. It isolates the encryption keys from the project where the data is stored, ensuring that even within the same cloud provider, only authorized personnel can access the keys.\n\nUse of .boto Configuration: Specifying the CSEK in the .boto configuration file ensures that it is applied consistently when interacting with Cloud Storage through tools like gsutil. This way, every archival file is encrypted using your keys.\n\nOptions A and B involve using Google Cloud Key Management Service (KMS) to manage keys, which does not align with the TNO approach because cloud provider staff could potentially access the keys stored in Google Cloud KMS.","poster":"barnac1es","timestamp":"1695603360.0","upvote_count":"2"},{"content":"Selected Answer: A\nThe answer is A\nThe question tells us that \"prevent the cloud provider staff from decrypting\", so we cannot keep anything that helps decrypt on GCP, not even in a different project. so the answer cannot be D.","timestamp":"1694457780.0","comment_id":"1005103","poster":"[Removed]","upvote_count":"4"},{"poster":"NewDE2023","upvote_count":"4","content":"Selected Answer: D\nCSEKs are used when an organization needs complete control over key management.","comment_id":"972371","timestamp":"1691172540.0"},{"upvote_count":"2","poster":"tavva_prudhvi","timestamp":"1690388820.0","comment_id":"963965","content":"Option A is not the best choice for the \"Trust No One\" (TNO) approach because it involves using Google Cloud's Key Management Service (KMS) to create and manage encryption keys. This means that the cloud provider will have access to the keys, which could potentially enable their staff to decrypt the data."},{"content":"Selected Answer: A\nD may work, but 'Trust No One' = do not trust GCP too. So D cannot be the answer.","poster":"midgoo","timestamp":"1678702440.0","upvote_count":"3","comment_id":"837828"},{"poster":"musumusu","content":"answer A: KMS + AAD is more secure than CSEK","timestamp":"1676744940.0","comment_id":"813385","upvote_count":"2"},{"poster":"zellck","comments":[{"poster":"AzureDP900","content":"Agree with A","timestamp":"1672512060.0","upvote_count":"2","comment_id":"762840"}],"upvote_count":"8","comment_id":"731528","timestamp":"1669815300.0","content":"Selected Answer: A\nA is the answer.\n\nhttps://cloud.google.com/kms/docs/additional-authenticated-data\nAdditional authenticated data (AAD) is any string that you pass to Cloud Key Management Service as part of an encrypt or decrypt request. AAD is used as an integrity check and can help protect your data from a confused deputy attack. The AAD string must be no larger than 64 KiB.\n\nCloud KMS will not decrypt ciphertext unless the same AAD value is used for both encryption and decryption.\n\nAAD is bound to the encrypted data, because you cannot decrypt the ciphertext unless you know the AAD, but it is not stored as part of the ciphertext. AAD also does not increase the cryptographic strength of the ciphertext. Instead it is an additional check by Cloud KMS to authenticate a decryption request."},{"poster":"Jay_Krish","upvote_count":"4","comment_id":"722583","timestamp":"1668945480.0","content":"Selected Answer: D\nCSEK with only security team having access seems to be right approach. Not sure how A can be better."},{"upvote_count":"2","timestamp":"1667582400.0","poster":"cloudmon","comment_id":"711274","content":"Selected Answer: A\nIt’s A, because you cannot decrypt the ciphertext unless you know the AAD (https://cloud.google.com/kms/docs/additional-authenticated-data)"},{"upvote_count":"1","comment_id":"686473","poster":"devaid","content":"Selected Answer: A\nAnswer: A","timestamp":"1664924820.0"},{"content":"D it is","timestamp":"1663816980.0","poster":"clouditis","comment_id":"675703","upvote_count":"1"},{"poster":"DataEngineer_WideOps","content":"Selected Answer: A\nC can not be the answer since memorystore cant be used to save CSEK key.\nhttps://cloud.google.com/memorystore/docs/redis/cmek#when_does_memorystore_interact_with_cmek_keys\n\nA is the Answer.","upvote_count":"2","comment_id":"634434","timestamp":"1658387640.0"},{"upvote_count":"2","comment_id":"545374","timestamp":"1644590340.0","poster":"BigDataBB","content":"Selected Answer: A\nA because: \"keep the AAD outside of Google Cloud\""},{"comment_id":"520154","upvote_count":"2","poster":"medeis_jar","timestamp":"1641727680.0","content":"Selected Answer: D\nAs it is Google best practice."},{"content":"I don't see the correct answer.\nD is a good practice; however, we have to use the \"Trust No One\" (TNO) approach to encrypt your data to prevent the cloud provider staff from decrypting your data.\nIf you upload the key in the cloud, the cloud provider staff eventually decrypt the file.\nWe have to keep the key anywhere else.\nA prevents the cloud provider staff from decrypting the file but I don't think it's a good practice; use CMEK and keep your key on your side is a better practice.","poster":"ramen_lover","upvote_count":"4","comment_id":"504757","timestamp":"1639907040.0"},{"content":"Correct: D","upvote_count":"2","poster":"JG123","timestamp":"1637817180.0","comment_id":"486451"},{"poster":"squishy_fishy","timestamp":"1633733820.0","comment_id":"459415","content":"The answer is A. In answer D, the security team can access the encryption key stored in the project, the requirement is trust no one.","upvote_count":"5"},{"timestamp":"1609736040.0","content":"here Symmetric key can never be the answer, because same key used for both encryption and decryption, which will be tough to hide because we have to share the information for data encryptiion. so A and B are eliminated. since in D , we are storing key in separate project , so it seems to be much secured way","comment_id":"259131","comments":[{"comment_id":"555650","timestamp":"1645746420.0","poster":"Ina34fs","upvote_count":"2","content":"CSP can access the other project..."}],"poster":"apnu","upvote_count":"2"},{"comment_id":"232181","poster":"federicohi","upvote_count":"2","timestamp":"1606850400.0","content":"People D its no possible because fo this \"your data to prevent the cloud provider staff from decrypting your data.\" Any project in cloud give access to cloud staff"},{"upvote_count":"7","comment_id":"186259","content":"Should be A. As the question says Trust No One, this is the only approach where the additional authenticated data (AAD) is not available in GCP without which the files can not be decrypted.","timestamp":"1600961400.0","poster":"SteelWarrior"},{"content":"My Answer is D , seems most suitable","poster":"Tanmoyk","timestamp":"1600574580.0","upvote_count":"1","comment_id":"182641"},{"timestamp":"1596848760.0","content":"D is the best option as its the only case option where the key is the most secure","poster":"clouditis","comment_id":"152793","upvote_count":"2"},{"timestamp":"1594644660.0","comment_id":"133956","comments":[{"comments":[{"content":"\" Instead it(AAD) is an additional check by Cloud KMS to authenticate a decryption request.\" https://cloud.google.com/kms/docs/additional-authenticated-data","poster":"xfoursea","timestamp":"1605486120.0","upvote_count":"2","comment_id":"219976"}],"upvote_count":"1","comment_id":"219973","timestamp":"1605485880.0","content":"D should be the answer. Trust no one suggests using your own key. Using AAD seems just an extra checking step, but does not stop those having your key to decrypt the data.\nhttps://cloud.google.com/storage/docs/encryption/customer-supplied-keys","poster":"xfoursea"}],"content":"Ans is D as CSEK are more secured and stored at Customer end and \nOne example of using AAD is when your application serves as a wrap/unwrap proxy with a single key and an unbounded number of clients, with each client in distinct security boundaries. For example, the application could be a diary application that allows users to maintain a private diary. When a user needs to view a private diary entry, the application can use the unique user name as the AAD in the unwrap (decrypt) request to explicitly authenticate the user. In this scenario you can use a single key to serve multiple (unbounded) users. A primary benefit is you don't need to maintain state for individual users.","upvote_count":"8","poster":"VishalB"},{"poster":"dg63","timestamp":"1594214400.0","upvote_count":"5","comment_id":"129779","content":"\"A\" is best choice as AAD string, not saved in cloud, is not accessible to the cloud provider.\nOption \"D\" leaves the key stored in cloud in another project. Cloud provider staff can potentially access that key."},{"poster":"Callumr","upvote_count":"2","timestamp":"1592747220.0","comment_id":"115611","content":"its D - Use CSEK - but don't store them in memory store"},{"poster":"Rajokkiyam","upvote_count":"2","comment_id":"70619","timestamp":"1585888260.0","content":"Answer D"},{"poster":"[Removed]","content":"Answer : A","comment_id":"66803","comments":[{"upvote_count":"4","content":"D .. too looks a good option. Confused","poster":"[Removed]","comment_id":"66825","timestamp":"1584858120.0"}],"timestamp":"1584853980.0","upvote_count":"2"},{"comment_id":"65971","upvote_count":"3","timestamp":"1584618600.0","poster":"rickywck","content":"A should be the answer:\n\nhttps://cloud.google.com/kms/docs/additional-authenticated-data"},{"poster":"rickywck","content":"For B, there will then be no way to restore those archived data ... I think D is the better option among these choices.","timestamp":"1584493860.0","comment_id":"65410","upvote_count":"3"}],"answer":"A","answer_ET":"A","answer_images":[]},{"id":"l1oZa2gZMGmQ97h4wfMZ","topic":"1","timestamp":"2022-09-02 18:03:00","exam_id":11,"answer":"A","question_text":"You have data pipelines running on BigQuery, Dataflow, and Dataproc. You need to perform health checks and monitor their behavior, and then notify the team managing the pipelines if they fail. You also need to be able to work across multiple projects. Your preference is to use managed products or features of the platform. What should you do?","answer_description":"","url":"https://www.examtopics.com/discussions/google/view/79472-exam-professional-data-engineer-topic-1-question-163/","isMC":true,"choices":{"D":"Develop an App Engine application to consume logs using GCP API calls, and send emails if you find a failure in the logs","A":"Export the information to Cloud Monitoring, and set up an Alerting policy","C":"Export the logs to BigQuery, and set up App Engine to read that information and send emails if you find a failure in the logs","B":"Run a Virtual Machine in Compute Engine with Airflow, and export the information to Cloud Monitoring"},"unix_timestamp":1662134580,"question_images":[],"answers_community":["A (100%)"],"answer_ET":"A","answer_images":[],"discussion":[{"timestamp":"1662885180.0","comment_id":"665954","poster":"John_Pongthorn","upvote_count":"5","content":"Selected Answer: A\nA . Your preference is to use managed products or features of the platform"},{"upvote_count":"1","content":"Selected Answer: A\nA","timestamp":"1730103480.0","poster":"SamuelTsch","comment_id":"1303878"},{"content":"Selected Answer: A\nCloud Monitoring (formerly known as Stackdriver) is a fully managed monitoring service provided by GCP, which can collect metrics, logs, and other telemetry data from various GCP services, including BigQuery, Dataflow, and Dataproc.\n\nAlerting Policies: Cloud Monitoring allows you to define alerting policies based on specific conditions or thresholds, such as pipeline failures, latency spikes, or other custom metrics. When these conditions are met, Cloud Monitoring can trigger notifications (e.g., emails) to alert the team managing the pipelines.\n\nCross-Project Monitoring: Cloud Monitoring supports monitoring resources across multiple GCP projects, making it suitable for your requirement to monitor pipelines in multiple projects.\n\nManaged Solution: Cloud Monitoring is a managed service, reducing the operational overhead compared to running your own virtual machine instances or building custom solutions.","timestamp":"1695603480.0","poster":"barnac1es","comment_id":"1016262","upvote_count":"2"},{"upvote_count":"1","content":"Selected Answer: A\nuse managed products","comment_id":"1002818","timestamp":"1694223780.0","poster":"sergiomujica"},{"timestamp":"1677848580.0","comment_id":"827980","content":"Selected Answer: A\nShould be A","upvote_count":"2","poster":"whorillo"},{"poster":"pluiedust","timestamp":"1662541200.0","content":"Selected Answer: A\nShould be A","upvote_count":"1","comment_id":"662270"},{"upvote_count":"2","poster":"AWSandeep","content":"Selected Answer: A\nA. Export the information to Cloud Monitoring, and set up an Alerting policy","comment_id":"657595","timestamp":"1662135660.0"},{"content":"Selected Answer: A\nShould be A","comment_id":"657583","timestamp":"1662134580.0","upvote_count":"1","poster":"PhuocT"}],"question_id":72},{"id":"ZJoCE64KT2ALCytet1VP","exam_id":11,"url":"https://www.examtopics.com/discussions/google/view/79478-exam-professional-data-engineer-topic-1-question-164/","choices":{"C":"Use TensorFlow to create a categorical variable with a vocabulary list. Create the vocabulary file and upload that as part of your model to BigQuery ML.","A":"Create a new view with BigQuery that does not include a column with city information.","B":"Use SQL in BigQuery to transform the state column using a one-hot encoding method, and make each city a column with binary values.","D":"Use Cloud Data Fusion to assign each city to a region that is labeled as 1, 2, 3, 4, or 5, and then use that number to represent the city in the model."},"isMC":true,"question_id":73,"answer":"B","answer_description":"","discussion":[{"comment_id":"799284","poster":"cajica","comments":[{"content":"I think it should say citiy instead of state... it is a typooi in the transcription of the question","poster":"sergiomujica","upvote_count":"3","comment_id":"1002825","timestamp":"1694223960.0"},{"poster":"knith66","upvote_count":"1","comment_id":"964308","content":"you are right, OHE is mentioned for state in option B, but in option B it is also mentioned to use binary conversion for the city column. an additional method can be used which is applicable for the conversion.","timestamp":"1690430100.0"},{"upvote_count":"1","comment_id":"912901","content":"But also for D, assigning each city to a numbered region could lose important information, as cities within the same region might have different characteristics affecting customer purchasing behavior (from Chat GPT).","timestamp":"1685718120.0","poster":"cetanx"}],"content":"Selected Answer: D\nIf we're rigorous, as we should because it's a professional exam, I think option B is incorrect because it's one-hot-encoding the \"state\" column, if the answer was \"city\" column, then I'd go for B. As this is not the case and I do not accept an spelling error like this in an official question, I would go for D.","upvote_count":"10","timestamp":"1675643640.0"},{"timestamp":"1703010720.0","comments":[{"comment_id":"1100897","timestamp":"1703010720.0","comments":[{"comment_id":"1100911","content":"https://cloud.google.com/bigquery/docs/auto-preprocessing#one_hot_encoding","timestamp":"1703012040.0","poster":"MaxNRG","upvote_count":"2"}],"poster":"MaxNRG","upvote_count":"3","content":"A removes the city information completely, losing a key predictive component.\n\nC requires additional coding and infrastructure with TensorFlow and vocabulary files outside of what BigQuery already provides.\n\nD transforms the distinct city values into numeric regions, losing granularity of the city data.\n\nBy using SQL within BigQuery to one-hot encode cities into multiple yes/no columns, the city data is maintained and formatted appropriately for the BigQuery ML linear regression model with minimal additional coding. This aligns with the requirements stated in the question."}],"poster":"MaxNRG","upvote_count":"6","content":"Selected Answer: B\nOne-hot encoding is a common technique used to handle categorical data in machine learning. This approach will transform the city name variable into a series of binary columns, one for each city. Each row will have a \"1\" in the column corresponding to the city it represents and \"0\" in all other city columns. This method is effective for linear regression models as it enables the model to use city data as a series of numeric, binary variables. BigQuery supports SQL operations that can easily implement one-hot encoding, thus minimizing the amount of coding required and efficiently preparing the data for the model.","comment_id":"1100895"},{"poster":"clouditis","content":"Selected Answer: D\nD it is, Question clearly says least amount of coding!","comment_id":"1326979","timestamp":"1734282540.0","upvote_count":"1"},{"content":"Selected Answer: B\nThis is B. It's easier to one hot in bigquery than to do it in datafusion and then import the values back into bigquery.","timestamp":"1727805120.0","poster":"baimus","comment_id":"1292063","upvote_count":"1"},{"timestamp":"1695604560.0","poster":"barnac1es","comment_id":"1016273","content":"Selected Answer: B\nOne-Hot Encoding: One-hot encoding is a common technique for handling categorical variables like city names in machine learning models. It transforms categorical data into a binary matrix, where each city becomes a separate column with binary values (0 or 1) indicating the presence or absence of that city.\n\nLeast Amount of Coding: One-hot encoding in BigQuery is straightforward and can be accomplished with SQL. You can use SQL expressions to pivot the city names into separate columns and assign binary values based on the city's presence in the original data.\n\nPredictive Power: One-hot encoding retains the predictive power of city information while making it suitable for linear regression models, which require numerical input.","upvote_count":"4"},{"comment_id":"964306","content":"Selected Answer: B\nOne hot encoding for state and binary values for each city will allow me to choose the B option.","poster":"knith66","timestamp":"1690429920.0","upvote_count":"3"},{"poster":"tavva_prudhvi","comment_id":"963932","content":"I guess Option D loses the granularity of the city-level information, as multiple cities will be grouped into the same region and represented by the same number. This can result in a loss of important predictive information for your linear regression model.\n\nOn the other hand, if we use one-hot encoding to create binary columns for each city. This method preserves the city-level information, allowing the model to capture the unique effects of each city on the likelihood of purchasing your company's products. Additionally, it can be done directly in BigQuery using SQL, which requires less coding and is more efficient.","timestamp":"1690386960.0","upvote_count":"2"},{"content":"Selected Answer: B\nOne-hot encoding is a common technique used to represent categorical variables as binary columns. In this case, you can transform the city variable into multiple binary columns, with each column representing a specific city. This allows you to maintain the predictive city information while organizing the data in columns suitable for training and serving the linear regression model.\n\nBy using SQL in BigQuery, you can perform the necessary transformations to implement one-hot encoding.","upvote_count":"4","timestamp":"1687500420.0","comment_id":"931257","poster":"blathul"},{"content":"Selected Answer: B\n- A is wrong since it drops the city which is a key predictor. \n- C is wrong since we want to keep it simple, and not use Tensorflow here.\n- D is wrong since there is no specific reason to use Data Fusion, and also this encoding here is ordinal, which doesn't make sense for something non-quantitative such as cities - we want one-hot coding instead.\n\nTherefore, B must be the correct answer.","poster":"KC_go_reply","comment_id":"930408","timestamp":"1687431600.0","upvote_count":"3","comments":[{"poster":"ckanaar","comment_id":"1012286","upvote_count":"1","content":"It could be argued that a specific reason to use Data Fusion is the minimal coding requirement.","timestamp":"1695212160.0"}]},{"comment_id":"924658","content":"Selected Answer: D\nCloud Datafusion: least amount of coding","comments":[{"upvote_count":"1","comment_id":"964310","poster":"knith66","timestamp":"1690430340.0","content":"OHE is better that datafusion considering least amount coding"},{"content":"While it's true that Cloud Data Fusion can simplify data integration tasks with a visual interface, it might not be the best choice in this specific scenario as using Cloud Data Fusion to assign each city to a region might result in a loss of important predictive information due to the grouping of cities","upvote_count":"1","comment_id":"963935","timestamp":"1690387080.0","poster":"tavva_prudhvi"}],"upvote_count":"4","poster":"leandrors","timestamp":"1686872400.0"},{"comment_id":"895937","timestamp":"1683897060.0","poster":"vaga1","upvote_count":"1","content":"Selected Answer: B\nA doesn't include the city column. \nC is not low code.\nD is not a one hot encoding, but an ordinal one on the city column.\n\nB applies a one hot encoding on the state column and a binary encoding on the city column, which works for me."},{"timestamp":"1683104280.0","poster":"mialll","upvote_count":"2","comment_id":"888322","content":"Selected Answer: B\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-auto-preprocessing#one_hot_encoding"},{"comment_id":"847019","upvote_count":"4","content":"Selected Answer: D\nD uses the least amount of coding... even if the model is not good.\nB encodes the \"state\", not the \"city\".","poster":"juliobs","timestamp":"1679486220.0"},{"content":"Selected Answer: B\nManually bigquery ml does preprocessing for you however if one wants to do a manual processing one can use the ML.ONE_HOT_ENCODER function. It just acts as an analytical funciton.","comment_id":"766780","upvote_count":"2","poster":"dconesoko","timestamp":"1672933020.0"},{"comment_id":"731509","poster":"zellck","content":"Selected Answer: B\nB is the answer.\n\nhttps://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-auto-preprocessing#one_hot_encoding\nOne-hot encoding maps each category that a feature has to its own binary feature where 0 represents the absence of the feature and 1 represents the presence (known as a dummy variable) creating N new feature columns where N is the number of unique categories for the feature across the training table.","upvote_count":"3","timestamp":"1669814580.0"},{"upvote_count":"4","timestamp":"1669226220.0","poster":"ovokpus","comment_id":"725313","content":"Selected Answer: B\nThe Cloud Data Fusion method will add unecessary weights to categories with higher value labels, which will skew the model. The best practice for encoding nominal categorical data is to one-hot-encode them into binary values. That is conveniently done in BigQuery:\n\nhttps://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-auto-preprocessing#one_hot_encoding"},{"poster":"Atnafu","upvote_count":"1","timestamp":"1668452220.0","comments":[{"timestamp":"1672933140.0","poster":"dconesoko","content":"Does it come with an out of the box one hot encoding template ?","comment_id":"766784","upvote_count":"1"}],"content":"D\nCloud Data Fusion is a fully managed, code-free data integration service that helps users efficiently build and manage ETL/ELT data pipelines.","comment_id":"718194"},{"content":"Selected Answer: D\nI have the same feeling with @cloudmon so I compromised to answer [D].\nIn more detail, here is my reasoning:\n\nThe requirement \"maintaining the predictable variables\" (a.k.a. city) makes:\n[A] obiously invalid\n[B] invalid, since it broadens the prediction to be state-dependent (all cities in particular state will be treated as the same variable). Additionally, one-hot encoding is not suitable for linear regression problems, dummy encoding (drop one) is better.\n\nAnswer [C] doesn't satisfy the \"least amount of coding\" directive. Other than that (as far I understood by searcing the keyword tf.feature_column.categorical_column_with_vocabulary_list) the TensorFlow vocabulary list is another form of one-hot encoding.\n\nSo it remains [D] which offers a visual interface but uses ordinal (or label) encoding which is far from ideal for regression problems.","upvote_count":"4","comment_id":"713170","timestamp":"1667839320.0","poster":"NicolasN"},{"comment_id":"711291","timestamp":"1667583720.0","comments":[{"timestamp":"1667583840.0","poster":"cloudmon","content":"D does have the ordinality problem, but at least it’s actually something that’s possible/relevant to do.","comment_id":"711293","upvote_count":"1"}],"upvote_count":"4","content":"Selected Answer: D\nThis is one of those examples in which none of the answers are actually the right way to do it, but D is the only one that may make some sense.\nB is wrong because it talks about transforming the STATE column. The actual correct way would be to transform/one-hot encode the CITY column (thus creating binary columns representing the city).","poster":"cloudmon"},{"content":"Selected Answer: B\nkey is in 'binary value'. It says low code not 'no code'. B is right one.","upvote_count":"3","timestamp":"1662840420.0","comment_id":"665656","poster":"Remi2021"},{"timestamp":"1662541620.0","poster":"pluiedust","upvote_count":"2","content":"Selected Answer: B\nI was also majoring in Econometrics and I think it is B","comment_id":"662278"},{"timestamp":"1662487560.0","upvote_count":"2","poster":"damaldon","comment_id":"661547","content":"Ans. B\nhttps://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-auto-preprocessing"},{"content":"Selected Answer: B\nComing for an econometric background i can assure you that for linear regression you can't encode a non ordinal feature as a sequence of number (awnser D).\n\nThe most appropriate answer is B, this is how to proceed with non-ordinal categorical features in a linear regression context.","poster":"YorelNation","comments":[{"timestamp":"1662447240.0","content":"And i discovered that BQ ML does the one-hot encoding for you:\n\nhttps://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-auto-preprocessing","upvote_count":"2","comment_id":"660904","poster":"YorelNation"}],"upvote_count":"2","timestamp":"1662446580.0","comment_id":"660889"},{"upvote_count":"3","content":"Selected Answer: D\nD. Use Cloud Data Fusion to assign each city to a region that is labeled as 1, 2, 3, 4, or 5, and then use that number to represent the city in the model.","timestamp":"1662324720.0","poster":"AWSandeep","comment_id":"659537"},{"upvote_count":"1","timestamp":"1662307980.0","poster":"AmirNaik204","content":"D. The Key is \"linear regression model\". Linear regression model is used if you are predicting something numeric. So option is to write a no code transform that assign each city a number and use the number in the model. Predict the number and translate back to its corresponding city.","comment_id":"659381"},{"comment_id":"657597","content":"Online points to D but not sure why B is incorrect.","upvote_count":"2","poster":"AWSandeep","timestamp":"1662135900.0","comments":[{"timestamp":"1662181020.0","upvote_count":"1","content":"It is least amount of coding, so D is correct.\nThank you for other explanations too.","poster":"ducc","comment_id":"658080"}]}],"answer_ET":"B","answer_images":[],"answers_community":["B (57%)","D (43%)"],"question_images":[],"unix_timestamp":1662135900,"timestamp":"2022-09-02 18:25:00","topic":"1","question_text":"You are working on a linear regression model on BigQuery ML to predict a customer's likelihood of purchasing your company's products. Your model uses a city name variable as a key predictive component. In order to train and serve the model, your data must be organized in columns. You want to prepare your data using the least amount of coding while maintaining the predictable variables. What should you do?"},{"id":"2zaals3OTWW3RxHCaA4m","choices":{"A":"Store transaction data in Cloud Spanner. Enable stale reads to reduce latency.","B":"Store transaction in Cloud Spanner. Use locking read-write transactions.","C":"Store transaction data in BigQuery. Disabled the query cache to ensure consistency.","D":"Store transaction data in Cloud SQL. Use a federated query BigQuery for analysis."},"isMC":true,"answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/79627-exam-professional-data-engineer-topic-1-question-165/","question_text":"You work for a large bank that operates in locations throughout North America. You are setting up a data storage system that will handle bank account transactions. You require ACID compliance and the ability to access data with SQL. Which solution is appropriate?","question_id":74,"discussion":[{"poster":"devaid","upvote_count":"12","timestamp":"1680651060.0","content":"Selected Answer: B\nI'd say B as the documentation primarily says ACID compliance for Spanner, not Cloud SQL.\nhttps://cloud.google.com/blog/topics/developers-practitioners/your-google-cloud-database-options-explained\nAlso, spanner supports read-write transactions for use cases, as handling bank transactions:\nhttps://cloud.google.com/spanner/docs/transactions#read-write_transactions","comments":[{"poster":"AzureDP900","upvote_count":"1","comment_id":"762849","timestamp":"1688143440.0","content":"B is right"},{"comment_id":"723175","content":"I wonder if you understood the meaning of ACID. This is an inherent property of any relational DB. Cloud SQL is fully ACID complaint","poster":"Jay_Krish","timestamp":"1684639200.0","upvote_count":"13"}],"comment_id":"686487"},{"timestamp":"1695377700.0","comments":[{"content":"Read replicas are enough to make Cloud SQL work as a multi-region service. That's not the point. The point is that the answer introduces the use of Bigquery when it's not needed for the use case. That's why B is right.","timestamp":"1708115040.0","poster":"FP77","upvote_count":"3","comment_id":"982866"}],"comment_id":"847033","upvote_count":"11","content":"Selected Answer: B\n\"locations throughout North America\" implies multi-region (northamerica-northeast1, us-central1, us-south1, us-west4, us-east5, etc.)\nCloud SQL can only do read replicas in other regions.","poster":"juliobs"},{"comment_id":"1100915","timestamp":"1718816340.0","content":"Selected Answer: B\nB. Store transaction in Cloud Spanner. Use locking read-write transactions.\n\nSince the banking transaction system requires ACID compliance and SQL access to the data, Cloud Spanner is the most appropriate solution. Unlike Cloud SQL, Cloud Spanner natively provides ACID transactions and horizontal scalability.\n\nEnabling stale reads in Spanner (option A) would reduce data consistency, violating the ACID compliance requirement of banking transactions.\n\nBigQuery (option C) does not natively support ACID transactions or SQL writes which are necessary for a banking transactions system.\n\nCloud SQL (option D) provides ACID compliance but does not scale horizontally like Cloud Spanner can to handle large transaction volumes.\n\nBy using Cloud Spanner and specifically locking read-write transactions, ACID compliance is ensured while providing fast, horizontally scalable SQL processing of banking transactions.","poster":"MaxNRG","upvote_count":"4"},{"poster":"Aman47","timestamp":"1718348760.0","content":"Selected Answer: B\nSpanner is an enterprise level resource which Banks require, Cloud SQL is limited to 30TB of storage. And Banking transactions should be read write locked.","upvote_count":"1","comment_id":"1096259"},{"poster":"barnac1es","content":"Selected Answer: B\nACID Compliance: Cloud Spanner is a globally distributed, strongly consistent database service that offers ACID compliance, making it a suitable choice for handling bank account transactions where data consistency and integrity are crucial.\n\nSQL Access: Cloud Spanner supports SQL queries, which align with your requirement to access data with SQL. You can use standard SQL to interact with the data stored in Cloud Spanner.\n\nLocking Read-Write Transactions: Cloud Spanner allows you to perform locking read-write transactions, ensuring that transactions are executed in a serializable and consistent manner. This is essential for financial transactions to prevent conflicts and maintain data integrity.","comment_id":"1016275","upvote_count":"3","timestamp":"1711336740.0"},{"poster":"NeoNitin","upvote_count":"1","comment_id":"966880","content":"B. Store transaction data in Cloud Spanner. Use locking read-write transactions.\n\nHere's why:\n\nACID Compliance: ACID stands for Atomicity, Consistency, Isolation, and Durability. Cloud Spanner is a fully managed, globally distributed database that provides strong consistency and ACID compliance. This ensures that bank account transactions are processed reliably and accurately, avoiding issues like data corruption or incomplete transactions.\n\nAbility to access data with SQL: Cloud Spanner supports SQL, which allows you to perform standard SQL queries on the data. This means that you can use familiar SQL commands to access, retrieve, and manipulate transaction data easily.","timestamp":"1706599620.0"},{"upvote_count":"1","poster":"Adswerve","content":"Selected Answer: D\nI initially selected B. However, it might be D.\n\nhttps://cloud.google.com/blog/topics/developers-practitioners/your-google-cloud-database-options-explained\nCloud Spanner: Cloud Spanner is an enterprise-grade, globally-distributed, and strongly-consistent database that offers up to 99.999% availability, built specifically to combine the benefits of relational database structure with non-relational horizontal scale. It is a unique database that combines ACID transactions, SQL queries, and relational structure with the scalability that you typically associate with non-relational or NoSQL databases. As a result, Spanner is best used for applications such as gaming, payment solutions, global financial ledgers, retail banking and inventory management that require ability to scale limitlessly with strong-consistency and high-availability.","comment_id":"872177","timestamp":"1697490060.0"},{"timestamp":"1692905880.0","content":"Answer B: \nlocking read-write = for data accuracy \nstate read = for speed up or latency","comment_id":"820972","poster":"musumusu","upvote_count":"2"},{"upvote_count":"2","poster":"musumusu","timestamp":"1692376980.0","comment_id":"813394","content":"Answer B: Spanner\nIt's incomplete question, what do you assume by large bank, until we are not sure about size and scale. Region is north america, that can be managed by cloud sql. but\ni am going for spanner, as its large bank and transaction data."},{"content":"This is definitely a tricky question because both B and D are \"appropriate\" as the question suggests, of course we can make assumptions with the \"large bank\" sentence but there are other questions here where making assumptions is not accepted by the community so I wonder when can we make assumptions and when we can't. I think the real problem here is the ambiguous question. This is one of the few questions where the community accept that both (B and D) answers are appropriate but some comments (and I agree) argue the BEST approach is B. I really think some questions can be written in a better and non-ambiguous way, it's just about thinking a little bit more and not conforming when a poor spelling.","upvote_count":"5","comment_id":"799296","timestamp":"1691276040.0","poster":"cajica"},{"timestamp":"1686982560.0","content":"Selected Answer: B\nThe question is hinting a requirement for global consistency, i.e. being available for NA region, which does not just include US but also Mexico, Argentina etc.\n\nLarge bank = priority over consistency over read-write","poster":"jkhong","comments":[{"content":"Good catch, definetely Spanner in that case.","poster":"ckanaar","upvote_count":"1","timestamp":"1710944760.0","comment_id":"1012303"},{"content":"Argentina is South America...","timestamp":"1690413780.0","upvote_count":"5","comment_id":"789242","poster":"desertlotus1211"}],"comment_id":"747867","upvote_count":"6"},{"comment_id":"737688","poster":"NicolasN","timestamp":"1686123900.0","content":"Selected Answer: B\nFinally, it's [B]. \nThere is no measurable requirement that rules out [D] (Cloud SQL) and this fact made me to select it as a preferrable answer.\nBut since we are talking about a large bank (which normally implies massive reads/writes per sec.) and nobody has posed any cost limitation, in a real case I would definitely prefer the advantages of Spanner.","upvote_count":"1"},{"poster":"zellck","comment_id":"731488","content":"Selected Answer: B\nB is the answer.\n\nhttps://cloud.google.com/spanner/docs/transactions\nSpanner supports these transaction modes:\n- Locking read-write. This type of transaction is the only transaction type that supports writing data into Spanner. These transactions rely on pessimistic locking and, if necessary, two-phase commit. Locking read-write transactions may abort, requiring the application to retry.","timestamp":"1685444400.0","upvote_count":"3"},{"timestamp":"1683607200.0","content":"Selected Answer: D\n[A] No - Stale reads not accepted for bank account transactions. \"A stale read is read at a timestamp in the past. If your application is latency sensitive but tolerant of stale data, then stale reads can provide performance benefits.\"\n[B] Yes - Fulfills all requirements\n[C] No - BigQuery is ACID-compliant, but it is too much to use it for such a case (mainly a CRUD app)\n[D] Yes+ - Fulfills all requirements. The BigQuery part may seem redundant, but it states a true fact that doesn't violate the \"access data with SQL\" requirement.\n\nSo, when SQL Cloud and SQL Spanner fit both, there is no reason to prefer the second.\nAnd the question doesn't mention any obvious fact for which should we prefer the expensive SQL Spanner:\n- We don't know if we have to deal with a big amount of data and thousands of writes per second. \n- We don't know the database size.\n- There is no need for multi-regional writes that would exclude SQL Cloud as an alternative. Is it a coincidence that the question limits the problem to the single region of North America?","comments":[{"content":"Also, correct me if I am wrong, Bigquery cannot query Cloud SQL directly, only when Cloud SQL is exported into GCS, then BQ can connect to GCS using federated queries.","upvote_count":"1","comment_id":"830849","timestamp":"1694000640.0","poster":"SuperVee"},{"comments":[{"poster":"NicolasN","comment_id":"738807","upvote_count":"2","timestamp":"1686208620.0","content":"You are correct of course. The final sentence was totally inaccurate."}],"upvote_count":"3","poster":"zellck","comment_id":"737952","timestamp":"1686140700.0","content":"North America has many regions, and the requirement is throughout North America, so Cloud Spanner will be more suitable to support multi-regions."},{"timestamp":"1686123840.0","poster":"NicolasN","upvote_count":"1","comment_id":"737687","content":"I changed my mind to [B] since I underestimated the given of a \"large bank\" where the cost difference for a single region Spanner wouldn't matter."}],"poster":"NicolasN","upvote_count":"2","comment_id":"714338"},{"upvote_count":"1","timestamp":"1683403980.0","poster":"cloudmon","content":"Selected Answer: B\nI'd go for B.\nThe only other somewhat valid option is D, but there's no requirement for analytics in the question.","comment_id":"712673"},{"comment_id":"701581","upvote_count":"1","poster":"mattab1627","content":"Surely its B, transactional data at a large US based bank would surely be massive in size and probably too much for CloudSQL? There is also no mention of a requirement for analytics","timestamp":"1682173800.0"},{"poster":"MounicaN","comment_id":"664747","content":"why not spanner?","upvote_count":"2","timestamp":"1678383660.0"},{"poster":"pluiedust","upvote_count":"1","content":"Selected Answer: D\nD is correct","timestamp":"1678187340.0","comment_id":"662282"},{"comments":[{"poster":"Mcloudgirl","content":"+ CloudSQL is cheaper than Spanner","timestamp":"1683640140.0","comment_id":"714703","upvote_count":"2"},{"timestamp":"1686140760.0","content":"North America has many regions, and the requirement is throughout North America, so Cloud Spanner will be more suitable to support multi-regions.","upvote_count":"2","poster":"zellck","comment_id":"737954"}],"poster":"YorelNation","comment_id":"660908","timestamp":"1678093080.0","upvote_count":"2","content":"Selected Answer: D\nD no indication for multi region availability or very big volume of data. so no spanner."},{"timestamp":"1678010820.0","upvote_count":"1","comment_id":"659904","content":"Why not B? There is no requirement for analysis and hence bq.\nhttps://cloud.google.com/spanner/docs/transactions#read-write_transactions","poster":"nwk"},{"comments":[{"upvote_count":"2","comment_id":"693428","poster":"adarifian","timestamp":"1681339680.0","content":"there's no requirement for analysis, though"}],"comment_id":"659540","content":"Selected Answer: D\nD. Store transaction data in Cloud SQL. Use a federated query BigQuery for analysis.\n\nCloud SQL or Cloud Spanner is required for ACID-based transactions in real-time. BigQuery cannot be used as table & row-level locking is required. BigQuery federated queries can directly access operational databased for ad-hoc analysis.","poster":"AWSandeep","timestamp":"1677970500.0","upvote_count":"2"},{"comment_id":"658084","timestamp":"1677826980.0","poster":"ducc","upvote_count":"1","content":"Selected Answer: D\nD is correct\nBecause ACID = (atomicity, consistency, isolation, and durability)\n\nSorry for other false explanation"},{"timestamp":"1677826860.0","upvote_count":"1","poster":"ducc","comment_id":"658083","content":"C. Store transaction data in BigQuery. Disabled the query cache to ensure consistency.\nIt is storage system, not a processing system, so Bigquery is the choice\nHere is my short-cut for choosing:\nStorage system (or even \"store transaction\") -> Big Query\nProcess, consistency -> SQL (if global/ scale horizontally -> Spanner)"},{"timestamp":"1677806340.0","comment_id":"657896","upvote_count":"1","content":"Selected Answer: C\nC is correct","poster":"ducc"}],"unix_timestamp":1662160740,"question_images":[],"answer_description":"","answers_community":["B (81%)","Other"],"answer_ET":"B","answer":"B","timestamp":"2022-09-03 01:19:00","topic":"1","exam_id":11},{"id":"roXs89IN0ytPd8xA4Use","answer_ET":"B","isMC":true,"exam_id":11,"answers_community":["B (73%)","D (27%)"],"topic":"1","question_text":"A shipping company has live package-tracking data that is sent to an Apache Kafka stream in real time. This is then loaded into BigQuery. Analysts in your company want to query the tracking data in BigQuery to analyze geospatial trends in the lifecycle of a package. The table was originally created with ingest-date partitioning. Over time, the query processing time has increased. You need to implement a change that would improve query performance in BigQuery. What should you do?","answer_description":"","choices":{"C":"Tier older data onto Cloud Storage files and create a BigQuery table using Cloud Storage as an external data source.","D":"Re-create the table using data partitioning on the package delivery date.","B":"Implement clustering in BigQuery on the package-tracking ID column.","A":"Implement clustering in BigQuery on the ingest date column."},"question_images":[],"unix_timestamp":1662324600,"timestamp":"2022-09-04 22:50:00","answer_images":[],"discussion":[{"comments":[{"poster":"AzureDP900","comment_id":"762854","content":"Yes it is B. Implement clustering in BigQuery on the package-tracking ID column.","timestamp":"1688143620.0","upvote_count":"1"}],"upvote_count":"10","comment_id":"731482","poster":"zellck","timestamp":"1685444100.0","content":"Selected Answer: B\nB is the answer.\n\nhttps://cloud.google.com/bigquery/docs/clustered-tables\nClustered tables in BigQuery are tables that have a user-defined column sort order using clustered columns. Clustered tables can improve query performance and reduce query costs.\n\nIn BigQuery, a clustered column is a user-defined table property that sorts storage blocks based on the values in the clustered columns. The storage blocks are adaptively sized based on the size of the table. A clustered table maintains the sort properties in the context of each operation that modifies it. Queries that filter or aggregate by the clustered columns only scan the relevant blocks based on the clustered columns instead of the entire table or table partition."},{"content":"Selected Answer: B\nThough I almost fell for D, but the delivery date information is only available on the event(s) that happen after the delivery, but not on the ones before where it will be NULL I guess. The only other option that can make some sense is B, though high cardinality is not recommended for clustering.","comment_id":"1224577","upvote_count":"2","timestamp":"1733389920.0","poster":"AlizCert"},{"upvote_count":"1","poster":"MaxNRG","comments":[{"poster":"MaxNRG","content":"Clustering can improve the performance of certain types of queries such as queries that use filter clauses and queries that aggregate data. When data is written to a clustered table by a query job or a load job, BigQuery sorts the data using the values in the clustering columns. These values are used to organize the data into multiple blocks in BigQuery storage. When you submit a query containing a clause that filters data based on the clustering columns, BigQuery uses the sorted blocks to eliminate scans of unnecessary data.\nCurrently, BigQuery allows clustering over a partitioned table. Use clustering over a partitioned table when:\n- Your data is already partitioned on a date, timestamp, or integer column.\n- You commonly use filters or aggregation against particular columns in your queries.\nTable clustering is possible for tables partitioned by:\n- ingestion time\n- date/timestamp\n- integer range","upvote_count":"1","comments":[{"comments":[{"timestamp":"1718816760.0","poster":"MaxNRG","comment_id":"1100922","upvote_count":"1","content":"Although more metadata must be maintained, by ensuring that data is partitioned globally, BigQuery can more accurately estimate the bytes processed by a query before you run it. This cost calculation provides an upper bound on the final cost of the query.\nIn a clustered table, BigQuery automatically sorts the data based on the values in the clustering columns and organizes them in optimally sized storage blocks. You can achieve more finely grained sorting by creating a table that is clustered and partitioned. A clustered table maintains the sort properties in the context of each operation that modifies it. As a result, BigQuery may not be able to accurately estimate the bytes processed by the query or the query costs. When blocks of data are eliminated during query execution, BigQuery provides a best effort reduction of the query costs."}],"upvote_count":"1","timestamp":"1718816760.0","comment_id":"1100921","content":"In a table partitioned by a date or timestamp column, each partition contains a single day of data. When the data is stored, BigQuery ensures that all the data in a block belongs to a single partition. A partitioned table maintains these properties across all operations that modify it: query jobs, Data Manipulation Language (DML) statements, Data Definition Language (DDL) statements, load jobs, and copy jobs. This requires BigQuery to maintain more metadata than a non-partitioned table. As the number of partitions increases, the amount of metadata overhead increases.","poster":"MaxNRG"}],"timestamp":"1718816700.0","comment_id":"1100920"}],"content":"Selected Answer: B\nB as Clustering the data on the package Id can greatly improve the performance.\nRefer GCP documentation - BigQuery Clustered Table:https://cloud.google.com/bigquery/docs/clustered-tables","comment_id":"1100919","timestamp":"1718816700.0"},{"comment_id":"1096272","timestamp":"1718350380.0","upvote_count":"2","content":"Selected Answer: B\nPackage Tracking mostly contains, geospatial prefixes, Like HK0011, US0022, etc, this can help in clustering.","poster":"Aman47"},{"comment_id":"1024660","content":"Selected Answer: D\nD is the correct answer\n\nrequirements: analyze geospatial trends in the lifecycle of a package\n\ncuz the data of the lifecycle of the package would span across ingest-date-based partition table, it would degrade the performance.\n\nhence, re-partitoning by package delivery date, which is the package initially delivered, would improve the performance when querying such table.","upvote_count":"4","poster":"kcl10","timestamp":"1712227800.0"},{"upvote_count":"3","comment_id":"922264","content":"Selected Answer: D\nI vote D\nQueries to analyze the package lifecycle will cross partitions when using ingest date. Changing this to delivery date will allow a query to full a package's full lifecycle in a single partition.","timestamp":"1702481820.0","poster":"sdi_studiers"},{"content":"Selected Answer: B\nB. https://cloud.google.com/bigquery/docs/clustered-tables","comment_id":"711300","timestamp":"1683216180.0","upvote_count":"1","poster":"cloudmon"},{"poster":"John_Pongthorn","content":"D is not correct becsuse This Is problem Is The Real Time so ingested date is the same as delivery date.","timestamp":"1678526700.0","comment_id":"665907","upvote_count":"3"},{"timestamp":"1678373640.0","comments":[{"upvote_count":"1","comment_id":"685426","timestamp":"1680518520.0","poster":"John_Pongthorn","content":"There are several rows that represent movement of life cycle of 1 package-tracking ID \npackage delivery date = ingestion date , i suppose"},{"content":"The table has already been partitioned","timestamp":"1686906900.0","comment_id":"747091","poster":"jkhong","upvote_count":"2"}],"comment_id":"664582","upvote_count":"1","poster":"kenanars","content":"why not D ?"},{"comment_id":"662289","upvote_count":"2","timestamp":"1678187640.0","poster":"pluiedust","content":"Selected Answer: B\nB;\nAs the table has already created with ingest-date partitioning."},{"timestamp":"1677970200.0","upvote_count":"1","poster":"AWSandeep","content":"Selected Answer: B\nB. Implement clustering in BigQuery on the package-tracking ID column.","comment_id":"659535"}],"url":"https://www.examtopics.com/discussions/google/view/80145-exam-professional-data-engineer-topic-1-question-166/","question_id":75,"answer":"B"}],"exam":{"numberOfQuestions":319,"isMCOnly":true,"provider":"Google","lastUpdated":"11 Apr 2025","isBeta":false,"isImplemented":true,"name":"Professional Data Engineer","id":11},"currentPage":15},"__N_SSP":true}