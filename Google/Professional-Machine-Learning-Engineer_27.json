{"pageProps":{"questions":[{"id":"dGHjPhgKUayiGquhBTZ1","answer_images":[],"answer_description":"","answer":"D","discussion":[{"poster":"VinaoSilva","content":"Selected Answer: D\n\"Text dataset -> TextDatasetCreateOp\nControl over parameters -> CustomTrainingJobOp\"","comment_id":"1239330","timestamp":"1719672540.0","upvote_count":"2"},{"timestamp":"1713417600.0","comment_id":"1197708","poster":"fitri001","comments":[{"timestamp":"1713417600.0","content":"why not others?\nA. TabularDatasetCreateOp and EndpointCreateOp:\nTabularDatasetCreateOp is designed for tabular data, not raw text.\nEndpointCreateOp creates an endpoint, but you need a model upload step before deployment (handled by ModelDeployOp).\nB. AutoMLTextTrainingOp: While AutoML offers convenience, it removes control over hyperparameter tuning, which you require.\nC. TabularDatasetCreateOp and AutoMLTextTrainingOp: Similar to option A, TabularDatasetCreateOp is not ideal for text data, and AutoML removes hyperparameter control.","comment_id":"1197709","poster":"fitri001","upvote_count":"2"}],"content":"Selected Answer: D\nTextDatasetCreateOp: This component is specifically designed to handle text-based data like product reviews. It reads and prepares the text data for training the model.\nCustomTrainingJobOp: Since you want control over hyperparameter tuning, a custom training job is the most suitable option. This component allows you to define your training script using a framework like TensorFlow and configure hyperparameters for optimization.\nModelDeployOp: After training, this component uploads the trained model to the Vertex AI Model Registry and deploys it to a Vertex AI Endpoint for serving predictions.","upvote_count":"2"},{"timestamp":"1712565540.0","upvote_count":"1","poster":"pinimichele01","content":"Selected Answer: D\nD fits perfect","comment_id":"1191474"},{"poster":"vaibavi","comment_id":"1144278","content":"Selected Answer: D\nD AutoML uses a predefined set of hyperparameter values for each algorithm used in model training. We can not have a control over hyperparameter","timestamp":"1707380880.0","upvote_count":"1"},{"timestamp":"1705394640.0","poster":"b1a8fae","comment_id":"1124020","upvote_count":"2","content":"Selected Answer: D\nText dataset -> TextDatasetCreateOp\nControl over parameters -> CustomTrainingJobOp"},{"comment_id":"1121308","poster":"pikachu007","content":"Selected Answer: D\nTextDatasetCreateOp: This component is specifically designed to create datasets from text-based data, essential for handling product reviews.\nCustomTrainingJobOp: This component provides full control over the training process, allowing you to specify model architecture, hyperparameter tuning strategies, and other training parameters, aligning with the requirement for control over model tuning.\nModelDeployOp: This component streamlines model deployment to a Vertex AI endpoint for real-time or batch inference, enabling the trained model to serve predictions.","upvote_count":"1","timestamp":"1705122240.0"}],"unix_timestamp":1705122240,"question_text":"You are creating a model training pipeline to predict sentiment scores from text-based product reviews. You want to have control over how the model parameters are tuned, and you will deploy the model to an endpoint after it has been trained. You will use Vertex AI Pipelines to run the pipeline. You need to decide which Google Cloud pipeline components to use. What components should you choose?","exam_id":13,"isMC":true,"topic":"1","answers_community":["D (100%)"],"choices":{"C":"TabularDatasetCreateOp. AutoMLTextTrainingOp, and ModelDeployOp","A":"TabularDatasetCreateOp, CustomTrainingJobOp, and EndpointCreateOp","B":"TextDatasetCreateOp, AutoMLTextTrainingOp, and EndpointCreateOp","D":"TextDatasetCreateOp, CustomTrainingJobOp, and ModelDeployOp"},"question_id":131,"answer_ET":"D","url":"https://www.examtopics.com/discussions/google/view/131036-exam-professional-machine-learning-engineer-topic-1-question/","timestamp":"2024-01-13 06:04:00","question_images":[]},{"id":"s46xpz1XvcQHbqauFEtp","unix_timestamp":1705122360,"answer_ET":"B","answer":"B","topic":"1","discussion":[{"upvote_count":"1","poster":"forport","timestamp":"1722139020.0","content":"Selected Answer: B\nB. According to Gemini-Advanced.","comment_id":"1256579"},{"comment_id":"1197710","upvote_count":"2","timestamp":"1713417840.0","poster":"fitri001","comments":[{"timestamp":"1713417840.0","upvote_count":"1","content":"C. Cloud Function for Code Changes: While Cloud Functions can be used for CI pipelines, manually configuring a function for every code change might become cumbersome and less scalable compared to a dedicated CI/CD service like Cloud Build with built-in triggering functionalities.\nD. Cloud Function for New Branches: Triggering on new branch creation alone wouldn't retrain models on existing branches where your team actively works. You'd need an additional trigger for existing branches (e.g., push to branch) to achieve automatic retraining.","poster":"fitri001","comment_id":"1197711"}],"content":"Selected Answer: B\nContinuous Integration: CI pipelines aim for frequent integration of code changes. Triggering the build pipeline upon every push to a branch (including the main branch) ensures your models retrain whenever the code relevant to them is modified.\nFocus on Relevant Changes: Compared to option A (\"Pull Request\"), triggering on pushes allows retraining even for direct pushes to the main branch, not just pull request merges. This can be crucial for catching critical code changes that might bypass pull requests."},{"comment_id":"1191475","timestamp":"1712565660.0","poster":"pinimichele01","content":"Selected Answer: B\nFor ANY modifications, “Push to a branch” is the best choice in Cloud Build trigger.","upvote_count":"1"},{"content":"Selected Answer: B\nMy Answer B:\n\nFor ANY modifications, “Push to a branch” is the best choice in Cloud Build trigger. However, when it comes to ML model training, retraining models on every push might be resource-intensive, especially if the training process is computationally expensive. So, I think triggering the CI pipeline on a pull request allows for changes to be tested before merging into the main branch. would be a better choice …","upvote_count":"1","timestamp":"1707937740.0","comment_id":"1150426","poster":"guilhermebutzke"},{"timestamp":"1705394940.0","upvote_count":"1","poster":"b1a8fae","content":"Selected Answer: B\nB. Any code change on the Cloud repo is done by pushing to a branch.","comment_id":"1124024"},{"comment_id":"1121311","upvote_count":"4","content":"Selected Answer: B\nCloud Build Integration: Cloud Build is Google Cloud's fully managed CI/CD platform, designed to automate builds and deployments, making it ideal for this task.\nTrigger on Code Pushes: Setting the trigger event to \"Push to a branch\" ensures that the pipeline automatically activates whenever new code is pushed to any branch of the repository, aligning with the goal of retraining models on code modifications.","timestamp":"1705122360.0","poster":"pikachu007"}],"timestamp":"2024-01-13 06:06:00","exam_id":13,"question_id":132,"question_images":[],"answer_images":[],"question_text":"Your team frequently creates new ML models and runs experiments. Your team pushes code to a single repository hosted on Cloud Source Repositories. You want to create a continuous integration pipeline that automatically retrains the models whenever there is any modification of the code. What should be your first step to set up the CI pipeline?","isMC":true,"answer_description":"","answers_community":["B (100%)"],"url":"https://www.examtopics.com/discussions/google/view/131037-exam-professional-machine-learning-engineer-topic-1-question/","choices":{"C":"Configure a Cloud Function that builds the repository each time there is a code change","B":"Configure a Cloud Build trigger with the event set as \"Push to a branch\"","A":"Configure a Cloud Build trigger with the event set as \"Pull Request\"","D":"Configure a Cloud Function that builds the repository each time a new branch is created"}},{"id":"8GkLgMPdiYTa8VijT3Gb","unix_timestamp":1705122480,"answer_ET":"D","answer":"D","discussion":[{"content":"Selected Answer: D\nD. \nThe idea behind this question is getting autoscaling to handle well the fluctuating input of requests. Changing the machine (A) is not related to autoscaling, and you might not be using the full potential of the machine during the whole time, bur rather only during instances of peak traffic. You need to lower the autoscaling threshold (the target utilization metric mentioned in the options is CPU, so we will go with this) so you make use of more resources whenever too many memory-intensive requests are happening. \n\nhttps://cloud.google.com/compute/docs/autoscaler/scaling-cpu#scaling_based_on_cpu_utilization\nhttps://cloud.google.com/compute/docs/autoscaler#autoscaling_policy","upvote_count":"11","comments":[{"content":"Addition: although memory-intensive is not directly related to CPU, for me the key is \"the model does not autoscale as expected\". To me this is addressing directly the settings of autoscaling, which won't change by changing the machine.","upvote_count":"2","comment_id":"1124039","timestamp":"1705396200.0","poster":"b1a8fae"}],"comment_id":"1124034","poster":"b1a8fae","timestamp":"1705395600.0"},{"upvote_count":"6","comment_id":"1121313","content":"Selected Answer: A\nB. Decreasing Workers: This might reduce memory usage per machine but could also decrease overall throughput, potentially impacting performance.\nC. Increasing CPU Utilization Target: This wouldn't directly address the memory bottleneck and could trigger unnecessary scaling based on CPU usage, not memory requirements.\nD. Decreasing CPU Utilization Target: This could lead to premature scaling, potentially increasing costs without addressing the root cause.","timestamp":"1705122480.0","poster":"pikachu007"},{"upvote_count":"1","content":"Selected Answer: D\n\"use autoscale\" = deacrease cpu utilization target","timestamp":"1719672600.0","poster":"VinaoSilva","comment_id":"1239331"},{"timestamp":"1713418080.0","upvote_count":"2","comment_id":"1197714","content":"Selected Answer: D\nD. Decrease the CPU utilization target: This is the most suitable approach. By lowering the CPU utilization target, the endpoint will scale up at a lower CPU usage level. This increases the likelihood of scaling up when the memory-intensive preprocessing tasks cause a rise in CPU utilization, even though memory is the root cause.","poster":"fitri001","comments":[{"content":"A. Use a machine type with more memory: While this might seem logical, autoscaling in Vertex AI endpoints relies on CPU utilization as the metric, not directly on memory usage. Even with more memory, the endpoint might not scale up if CPU utilization remains below the threshold.\nB. Decrease the number of workers per machine (Not applicable to Vertex AI Endpoints): This option might be relevant for some serving frameworks, but Vertex AI Endpoints don't typically use a worker concept. Scaling down workers wouldn't directly address the memory bottleneck.\nC. Increase the CPU utilization target: This would instruct the endpoint to scale up only when CPU usage reaches a higher threshold. Since the issue is memory usage, increasing the CPU target wouldn't trigger scaling when memory is the limiting factor.","timestamp":"1713418140.0","comment_id":"1197715","upvote_count":"1","poster":"fitri001"}]},{"upvote_count":"3","poster":"guilhermebutzke","timestamp":"1707938820.0","content":"Selected Answer: D\nOption D, \"Decrease the CPU utilization target in the autoscaling configurations,\" could be a valid approach to address the issue of autoscaling and anticipate spikes in traffic. By lowering the threshold, the autoscaling system would initiate scaling actions at a lower CPU utilization level, allowing for a more proactive response to increasing demands.","comment_id":"1150445"}],"topic":"1","timestamp":"2024-01-13 06:08:00","exam_id":13,"question_id":133,"question_images":[],"answer_images":[],"question_text":"You have built a custom model that performs several memory-intensive preprocessing tasks before it makes a prediction. You deployed the model to a Vertex AI endpoint, and validated that results were received in a reasonable amount of time. After routing user traffic to the endpoint, you discover that the endpoint does not autoscale as expected when receiving multiple requests. What should you do?","isMC":true,"answer_description":"","answers_community":["D (74%)","A (26%)"],"url":"https://www.examtopics.com/discussions/google/view/131038-exam-professional-machine-learning-engineer-topic-1-question/","choices":{"D":"Decrease the CPU utilization target in the autoscaling configurations","C":"Increase the CPU utilization target in the autoscaling configurations.","A":"Use a machine type with more memory","B":"Decrease the number of workers per machine"}},{"id":"ST0rgXUMJhMaid2uV31P","question_id":134,"answer_images":[],"discussion":[{"comment_id":"1150491","timestamp":"1707943320.0","content":"Selected Answer: D\nMy answer: D \n\nThis Google Documentation explains “Instead of deploying the model to an endpoint, you can use the RunInference API to serve machine learning models in your Apache Beam pipeline. This approach has several advantages, including flexibility and portability.”\nhttps://cloud.google.com/blog/products/ai-machine-learning/streaming-prediction-with-dataflow-and-vertex\n\nThis documentation uses RunInference and WatchFilePattern to “to automatically update the ML model without stopping the Apache Beam”.\nhttps://cloud.google.com/dataflow/docs/notebooks/automatic_model_refresh\n\nSo, thinking in “minimize prediction latency”, its suggested use RunInfenrece, while “effort required to update the model” the **WatchFilePattern is the best approach.** I think D is the best option","poster":"guilhermebutzke","upvote_count":"6"},{"timestamp":"1734983280.0","upvote_count":"1","poster":"phani49","comment_id":"1330929","content":"Selected Answer: D\nExposing the model as a Vertex AI endpoint and using Dataflow with a custom DoFn provides the optimal solution for real-time predictions with minimal latency.\n\nhttps://cloud.google.com/blog/products/ai-machine-learning/streaming-prediction-with-dataflow-and-vertex"},{"comment_id":"1321386","content":"Selected Answer: A\nA is the best option because:\n- Minimizes Latency: Loading the model into the Cloud Function's memory eliminates the overhead of loading the model from storage for each prediction request. This significantly reduces latency, crucial for near real-time recommendations. The function is triggered directly by Pub/Sub messages, further streamlining the process.\n- Simplified Model Updates: Updating the model involves simply deploying a new version of the Cloud Function with the updated model. This is a much simpler process than managing pipelines or endpoints.\n\nD is the most voted so far, but... The complexity of managing the Dataflow pipeline and the potential latency introduced by the pipeline outweigh the benefits of automatic model updates using WatchFilePattern in this context. Therefore, option A (Cloud Function) remains the most efficient solution.","upvote_count":"3","poster":"lunalongo","timestamp":"1733232840.0"},{"content":"Selected Answer: C\nC) Expose the model as Vertex AI End Point","poster":"PhilipKoku","comment_id":"1228020","timestamp":"1718041200.0","upvote_count":"1"},{"content":"Selected Answer: D\nagree with guilhermebutzke","comment_id":"1196679","poster":"pinimichele01","upvote_count":"1","timestamp":"1713283740.0"},{"timestamp":"1710062400.0","upvote_count":"1","poster":"Yan_X","content":"Selected Answer: A\nA for me.","comment_id":"1170177"},{"content":"Selected Answer: D\nAutomatic Model Updates: WatchFilePattern automatically detects model changes in Cloud Storage, leading to seamless updates without managing endpoint deployments.","poster":"ddogg","upvote_count":"3","timestamp":"1707136380.0","comment_id":"1141078"},{"comments":[{"poster":"CHARLIE2108","timestamp":"1707225720.0","upvote_count":"2","comment_id":"1142164","content":"Cloud Functions offer low latency but it might not scale well."}],"poster":"pikachu007","content":"Selected Answer: A\nLow Latency:\n\nServerless Execution: Cloud Functions start up almost instantly, reducing prediction latency compared to alternatives that require longer setup or deployment times.\nIn-Memory Model: Loading the model into memory eliminates disk I/O overhead, further contributing to rapid predictions.","upvote_count":"2","comment_id":"1121317","timestamp":"1705122960.0"}],"timestamp":"2024-01-13 06:16:00","topic":"1","url":"https://www.examtopics.com/discussions/google/view/131039-exam-professional-machine-learning-engineer-topic-1-question/","answers_community":["D (61%)","A (33%)","6%"],"answer":"D","choices":{"C":"Expose the model as a Vertex AI endpoint. Write a custom DoFn in a Dataflow job that calls the endpoint for prediction.","D":"Use the RunInference API with WatchFilePattern in a Dataflow job that wraps around the model and serves predictions.","B":"Create a pipeline in Vertex AI Pipelines that performs preprocessing, prediction, and postprocessing. Configure the pipeline to be triggered by a Cloud Function when messages are sent to Pub/Sub.","A":"Write a Cloud Function that loads the model into memory for prediction. Configure the function to be triggered when messages are sent to Pub/Sub."},"question_text":"Your company manages an ecommerce website. You developed an ML model that recommends additional products to users in near real time based on items currently in the user’s cart. The workflow will include the following processes:\n\n1. The website will send a Pub/Sub message with the relevant data and then receive a message with the prediction from Pub/Sub\n2. Predictions will be stored in BigQuery\n3. The model will be stored in a Cloud Storage bucket and will be updated frequently\n\nYou want to minimize prediction latency and the effort required to update the model. How should you reconfigure the architecture?","question_images":[],"exam_id":13,"isMC":true,"unix_timestamp":1705122960,"answer_description":"","answer_ET":"D"},{"id":"NY3x9mnwpHFa3sLBOsbh","choices":{"C":"data = json.dumps({ג€signature_nameג€: ג€serving_defaultג€, ג€instancesג€ [['a', 'b', 'c'], ['d', 'e', 'f']]})","B":"data = json.dumps({ג€signature_nameג€: ג€serving_defaultג€, ג€instancesג€ [['a', 'b', 'c', 'd', 'e', 'f']]})","A":"data = json.dumps({ג€signature_nameג€: ג€seving_defaultג€, ג€instancesג€ [['ab', 'bc', 'cd']]})","D":"data = json.dumps({ג€signature_nameג€: ג€serving_defaultג€, ג€instancesג€ [['a', 'b'], ['c', 'd'], ['e', 'f']]})"},"answer_ET":"D","answer":"D","question_text":"You trained a text classification model. You have the following SignatureDefs:\n//IMG//\n\nYou started a TensorFlow-serving component server and tried to send an HTTP request to get a prediction using: headers = {\"content-type\": \"application/json\"} json_response = requests.post('http: //localhost:8501/v1/models/text_model:predict', data=data, headers=headers)\nWhat is the correct way to write the predict request?","isMC":true,"answer_description":"","answer_images":[],"question_images":["https://www.examtopics.com/assets/media/exam-media/03841/0001300001.png"],"discussion":[{"comment_id":"373042","timestamp":"1622666280.0","upvote_count":"29","content":"Options:\n\nA. data = json.dumps({“signature_name”: “seving_default”, “instances” [[‘ab’, ‘bc’, ‘cd’]]})\nB. data = json.dumps({“signature_name”: “serving_default”, “instances” [[‘a’, ‘b’, ‘c’, ‘d’, ‘e’, ‘f’]]})\nC. data = json.dumps({“signature_name”: “serving_default”, “instances” [[‘a’, ‘b’, ‘c’], [‘d’, ‘e’, ‘f’]]})\nD. data = json.dumps({“signature_name”: “serving_default”, “instances” [[‘a’, ‘b’], [‘c’, ‘d’], [‘e’, ‘f’]]})","poster":"[Removed]"},{"timestamp":"1624351020.0","poster":"maartenalexander","comment_id":"387740","content":"Most likely D. A negative number in the shape enables auto expand (https://stackoverflow.com/questions/37956197/what-is-the-negative-index-in-shape-arrays-used-for-tensorflow). \n\nThen the first number -1 out of the shape (-1, 2) speaks the number of 1 dimensional arrays within the tensor (and it can autoexpand) while the second numer (2) sets the number of elements in the inner array at 2. Hence D.","upvote_count":"23"},{"content":"Selected Answer: D\nthe shape (-1, 2) indicates that the data can have any number of rows (denoted by -1), but must have exactly 2 columns. In machine learning, especially in frameworks like TensorFlow or Keras, the -1 acts as a placeholder for dynamic batch sizes, meaning the model can process inputs with any number of samples (rows), but each sample must have exactly 2 features (columns).","comment_id":"1300175","poster":"jkkim_jt","timestamp":"1729371900.0","upvote_count":"4"},{"upvote_count":"1","comment_id":"1225482","content":"Selected Answer: D\nD) Any rows, 2 columns.","timestamp":"1717677060.0","poster":"PhilipKoku"},{"upvote_count":"2","poster":"M25","content":"Selected Answer: D\nWent with D","timestamp":"1683608400.0","comment_id":"892698"},{"content":"Selected Answer: D\nans: D","comment_id":"746537","upvote_count":"1","timestamp":"1671140400.0","poster":"wish0035"},{"timestamp":"1669282440.0","upvote_count":"1","poster":"EFIGO","content":"Selected Answer: D\nHaving \"shape=[-1,2]\", the input can have as many rows as we want, but each row needs to be of 2 elements. The only option satisfying this requirement is D.","comment_id":"725699"},{"content":"Selected Answer: D\nCorrect answer is \"D\"","comment_id":"647197","poster":"GCP72","timestamp":"1660567800.0","upvote_count":"1"},{"comment_id":"615496","content":"Selected Answer: D\nwill vote for D , as the data shape in instances matches the shape in signature def","timestamp":"1655067540.0","upvote_count":"1","poster":"Mohamed_Mossad"},{"comment_id":"574038","poster":"pml2021","timestamp":"1648090980.0","content":"Selected Answer: D\nshape is (-1,2) indicating any no of rows, 2 columns only.","upvote_count":"2"},{"timestamp":"1634646180.0","comment_id":"464645","poster":"mousseUwU","upvote_count":"3","content":"D is correct if shape(-1,2) means 2 columns for each row","comments":[{"poster":"mousseUwU","comment_id":"464646","upvote_count":"1","timestamp":"1634646240.0","content":"Link to explanation: https://stackoverflow.com/questions/37956197/what-is-the-negative-index-in-shape-arrays-used-for-tensorflow"}]},{"comment_id":"441652","content":"D: (-1, 2) represents a vector with any number of rows but only 2 columns.","upvote_count":"5","poster":"Danny2021","timestamp":"1631135580.0"},{"timestamp":"1623235920.0","comment_id":"378195","poster":"inder0007","content":"Correct answer is D, the shapes otherwise don't matter","upvote_count":"4"}],"timestamp":"2021-06-02 22:38:00","answers_community":["D (100%)"],"exam_id":13,"topic":"1","question_id":135,"url":"https://www.examtopics.com/discussions/google/view/54308-exam-professional-machine-learning-engineer-topic-1-question/","unix_timestamp":1622666280}],"exam":{"id":13,"isImplemented":true,"isMCOnly":true,"lastUpdated":"11 Apr 2025","provider":"Google","name":"Professional Machine Learning Engineer","numberOfQuestions":304,"isBeta":false},"currentPage":27},"__N_SSP":true}