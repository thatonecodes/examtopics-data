{"pageProps":{"questions":[{"id":"A5MmSFMNw79KN1XFCmQN","choices":{"A":"Store each comment in a subcollection of the article.","D":"Store each comment in a document, and add the comment's key to an array property on the user profile.","B":"Add each comment to an array property on the article.","C":"Store each comment in a document, and add the comment's key to an array property on the article."},"isMC":true,"answer_description":"","unix_timestamp":1642266900,"timestamp":"2022-01-15 18:15:00","exam_id":7,"url":"https://www.examtopics.com/discussions/google/view/70075-exam-professional-cloud-developer-topic-1-question-108/","question_id":11,"answers_community":["A (53%)","D (32%)","C (16%)"],"discussion":[{"comment_id":"529386","timestamp":"1642800240.0","poster":"p4","comments":[{"timestamp":"1646087340.0","upvote_count":"1","content":"the subcollection has a limit of 1Mb of data, so for unknown number of comments is not valid, the answer is D","poster":"juancambb","comment_id":"558394","comments":[{"comment_id":"597716","content":"That's wrong, the 1MB limit it's on the document inside the subcollection (not on the subcollection itself. \n\n\"Document size - Cloud Firestore is optimized for small documents and enforces a 1MB size limit on documents. If your array can expand arbitrarily, it is better to use a subcollection, which has better scaling performance.\"\n\nCheck out also the example in the subcollections documentation, showing a rooms-messages hierarchy example.\nhttps://firebase.google.com/docs/firestore/data-model#subcollections","poster":"[Removed]","upvote_count":"6","timestamp":"1651837320.0"}]}],"upvote_count":"21","content":"Selected: A\nFirestore has a “hierarchical structure”: collection contains documents, document can contain (sub)collections\nD does not make sense bc why do you want to link comments to the user profile instead of the article?\nhttps://stackoverflow.com/questions/48634227/limitation-to-number-of-documents-under-one-collection-in-firebase-firestore\n“There is no documented limit to the number of documents that can be stored in a Cloud Firestore collection. The system is designed to scale to huge data sets.”"},{"upvote_count":"9","poster":"htakami","content":"Why D and not C? For my understanding, we need to keep a relation between the articles and their comments. I don't see how the user profile could come in handy... but please let me know if I misunderstood something. For me, Ans C makes more sense.","timestamp":"1648327080.0","comment_id":"575775"},{"content":"Selected Answer: A\nAnswer A. Comments in article subcollection:\n\nStandard Firestore pattern\nScales infinitely\nEasy to query per article\nClean data organization\nNo size limits concerns","timestamp":"1734890280.0","poster":"dneves","upvote_count":"1","comment_id":"1330501"},{"comments":[{"content":"Let's look at why the other options are less ideal:\n\nB. Add each comment to an array property on the article: This approach is limited by the maximum document size in Firestore (1 MB). You'll run into issues if you have a large number of comments for a single article.\nC. Store each comment in a document and add the comment's key to an array property on the article: This approach is less efficient for querying comments related to a specific article. You'd need to perform multiple queries to retrieve all comments.\nD. Store each comment in a document and add the comment's key to an array property on the user profile: This approach is not ideal for retrieving comments related to a specific article. You'd need to query the user profile for each comment, which is inefficient.","timestamp":"1721367720.0","upvote_count":"1","comment_id":"1250874","poster":"thewalker"}],"upvote_count":"1","poster":"thewalker","timestamp":"1721367720.0","content":"Selected Answer: A\nThe best answer here is A. Store each comment in a subcollection of the article.\n\nHere's why:\n\nScalability: Storing comments as subcollections within the article document allows you to handle an unlimited number of comments per article. Firestore scales well with large numbers of subcollections.\nQuerying: You can easily query for comments related to a specific article using the collectionGroup() query. This allows you to retrieve all comments for an article without needing to know the specific comment IDs.\nPerformance: Firestore's subcollection structure optimizes for reading and writing comments related to a specific article.","comment_id":"1250873"},{"comment_id":"1247202","timestamp":"1720861920.0","content":"Selected Answer: A\nFirestore has a hierarchical structure","upvote_count":"1","poster":"d_ella2001"},{"upvote_count":"1","poster":"alpha_canary","comment_id":"1196952","timestamp":"1713322980.0","content":"Selected Answer: A\nIt can't be D because:\nStoring each comment in a document and adding the comment's key to an array property on the user profile wouldn't efficiently link comments to articles."},{"comment_id":"1103740","upvote_count":"1","content":"Selected Answer: D\nAnswer is D in my opinion for \"display user-submitted comments\" and \"unknown number of comments and articles\"","timestamp":"1703282280.0","poster":"Kadhem"},{"timestamp":"1700594040.0","content":"Selected Answer: A\nAnswer is A","upvote_count":"3","comment_id":"1076574","poster":"Aeglas"},{"content":"Selected Answer: A\nAs per previous comments also appointed, there is not such a limitation on size for a Subcollection, and it does not make sense to store the relation with User profile like answer D","comment_id":"1075876","timestamp":"1700518140.0","poster":"Aeglas","upvote_count":"3"},{"poster":"braska","comment_id":"1073629","content":"Selected Answer: A\nOption A is the recommended approach for structuring data in Firestore to support an unknown number of comments and articles. Firestore is a NoSQL document-oriented database, and using subcollections provides a flexible and scalable way to organize related data","upvote_count":"2","timestamp":"1700254020.0"},{"poster":"__rajan__","comment_id":"1013163","content":"Selected Answer: C\nI would go with C.","upvote_count":"1","timestamp":"1695306120.0"},{"content":"Selected Answer: A\nthe correct answer is A. reference: https://firebase.google.com/docs/firestore/data-model","upvote_count":"2","comment_id":"939613","timestamp":"1688188680.0","poster":"sota_hi_there"},{"timestamp":"1686423540.0","comment_id":"920243","content":"Answer is D","upvote_count":"1","poster":"gc_exam2022"},{"poster":"efrenpq","timestamp":"1684341600.0","comment_id":"900370","content":"Selected Answer: A\nFirestore has a \"hierarchical structure\"","upvote_count":"1"},{"upvote_count":"1","comment_id":"795140","content":"Selected Answer: C\nIt is recommended to add the comment document IDs to an array property on the corresponding article document, rather than on a user profile. This approach allows you to easily retrieve all comments for a specific article by querying the comments collection using the article ID and then filtering the results based on the IDs in the article's comments array.\n\nStoring the comment IDs in the article document also avoids the need to make multiple read operations to retrieve the comments for a given article, which can be slow and increase latency.\n\nFor example, you could create an array property named \"comments\" in the article document and add the comment document IDs to this array every time a user submits a new comment for the article. This allows you to efficiently retrieve all comments for a given article by querying the comments collection and filtering based on the IDs in the article's \"comments\" array.","timestamp":"1675257060.0","poster":"mrvergara"},{"upvote_count":"2","comments":[{"poster":"felipeschossler","timestamp":"1682410860.0","upvote_count":"2","comment_id":"880127","content":"Exactly this, thanks for sharing the docs here, I change my answer because of you!"}],"content":"Selected Answer: A\nClose to this example: https://firebase.google.com/docs/firestore/data-model#subcollections","timestamp":"1674112260.0","poster":"chunker","comment_id":"780832"},{"timestamp":"1673415840.0","comment_id":"772073","comments":[{"content":"Storing each comment in a document, and adding the comment's key to an array property on the user profile (Option D) would make it more difficult to fetch all the comments associated with a specific article. Additionally, it would also make it more difficult to query the comments and articles since you would have to go through the user profile to find the comment's key and then use that to find the comment's information. The subcollection approach allows for better organization and querying of the data, making it a better choice in this scenario.","upvote_count":"2","comment_id":"772074","timestamp":"1673415840.0","comments":[{"comment_id":"772076","timestamp":"1673415900.0","comments":[{"upvote_count":"1","comment_id":"772077","content":"Another reason for not choosing C is that, it might also pose an issue for data consistency as comments can change over time and updating the comment document would not automatically update the array property on the article, creating inconsistencies in the data.","poster":"omermahgoub","timestamp":"1673415900.0"}],"upvote_count":"1","content":"C. Store each comment in a document, and add the comment's key to an array property on the article would work, but it may not be the best solution for this use case. While it would allow you to query for all the comments associated with an article by finding the document with the article and reading the array property of its key.\n\nHowever, this approach would make it more difficult to scale the data as the number of comments grows, because it would require you to retrieve all the comment keys in the array of the article and then perform additional queries to retrieve the actual comment information one by one. This could slow down the application as the number of comments increase, and make it more difficult to handle high-load situations.","poster":"omermahgoub"}],"poster":"omermahgoub"}],"upvote_count":"3","content":"Answer is A. Store each comment in a subcollection of the article, because it allows for easy scaling and querying of the comments as the data increases. With this approach, you can easily fetch the comments associated with an article by querying the subcollection of that article, instead of querying all the comments in a single collection. It also allows you to query specific comments and articles easily, since you have the reference to the specific article they are associated with.","poster":"omermahgoub"},{"upvote_count":"1","poster":"TNT87","timestamp":"1672217520.0","content":"https://firebase.google.com/docs/firestore/best-practices#high_read_write_and_delete_rates_to_a_narrow_document_range","comment_id":"759597"},{"comment_id":"752106","timestamp":"1671616260.0","upvote_count":"1","content":"Selected Answer: D\nD is the answer.","poster":"zellck"},{"upvote_count":"2","timestamp":"1660975560.0","comment_id":"649278","poster":"tomato123","content":"Selected Answer: D\nD is correct I think"},{"timestamp":"1660651080.0","comment_id":"647632","content":"Selected Answer: A\nThe unknown number of articles and comments can exceeds the 1 mb limit for document... I vote A","poster":"alex8081","upvote_count":"3"},{"content":"Selected Answer: C\nI'm not sure why everyone is in the favor of option D here. \nLogically thinking, I feel that storing the comment's key to an array property of the article would make more sense. If think of the application's flow then you would realise that the comments will always be associated with an article. Of course, for a single comment, we would have the user id as a field so that it can also find out which user has posted that comment.","upvote_count":"4","poster":"akshaychavan7","timestamp":"1659759120.0","comment_id":"643183"},{"timestamp":"1653760740.0","content":"Selected Answer: D\nAgree with D.\nAn use case of the application is to display user submitted comments.\nWhat happen if you don't choose D? You'll need to iterate through all articles and each comment inside the article and filter for user ID, this will be complex for an unknown number of articles and comments.","poster":"[Removed]","upvote_count":"3","comment_id":"608499"},{"content":"Selected Answer: D\nP4 : re: D does not make sense bc why do you want to link comments to the user profile instead of the article?\n\nAgreed use case here is we need to track user activity and using D option yiu can connect each comment back to article","poster":"GCPCloudArchitectUser","upvote_count":"5","timestamp":"1645891680.0","comment_id":"556765"},{"upvote_count":"2","content":"Agree with option D as questions mentions unknown number of articles and comments","poster":"Blueocean","timestamp":"1642266900.0","comment_id":"524311"}],"answer_images":[],"question_text":"You are developing an application that will allow users to read and post comments on news articles. You want to configure your application to store and display user-submitted comments using Firestore. How should you design the schema to support an unknown number of comments and articles?","answer":"A","question_images":[],"topic":"1","answer_ET":"A"},{"id":"XSgjL29t6iYmhtdvLcDL","topic":"1","question_id":12,"choices":{"C":"Use Shared VPC networks","B":"Use VPC Network Peering","D":"Use Private Google Access","A":"Use Carrier Peering"},"unix_timestamp":1641748200,"timestamp":"2022-01-09 18:10:00","answer_images":[],"question_text":"You recently developed an application. You need to call the Cloud Storage API from a Compute\nEngine instance that doesn't have a public IP address. What should you do?","question_images":[],"url":"https://www.examtopics.com/discussions/google/view/69770-exam-professional-cloud-developer-topic-1-question-109/","answer_ET":"D","answer":"D","isMC":true,"answer_description":"","exam_id":7,"answers_community":["D (100%)"],"discussion":[{"content":"Selected Answer: D\nPrivate Google Access allows your Compute Engine instances to access Google Cloud APIs and services without requiring a public IP address. It enables outbound connectivity to Google APIs and services using internal IP addresses.","upvote_count":"3","comment_id":"1012132","poster":"__rajan__","timestamp":"1726823400.0"},{"upvote_count":"4","poster":"telp","comment_id":"774507","timestamp":"1705151160.0","content":"Selected Answer: D\nA is not correct because Carrier Peering enables you to access Google applications, such as Google Workspace, by using a service provider to obtain enterprise-grade network services that connect your infrastructure to Google.\nB is not correct because VPC Network Peering enables you to peer VPC networks so that workloads in different VPC networks can communicate in a private RFC 1918 space. Traffic stays within Google's network and doesn't traverse the public internet.\nC is not correct because Shared VPC allows an organization to connect resources from multiple projects to a common VPC network so that they can communicate with each other securely and efficiently using internal IPs from that network.\nD is correct because Private Google Access is an option available for each subnetwork. When it is enabled, instances in the subnetwork can communicate with public Google API endpoints even if the instances don't have external IP addresses."},{"content":"D. Use Private Google Access\n\nPrivate Google Access is a feature that enables access to Google Cloud APIs and services for instances that don't have a public IP address. With this feature, you can allow your Compute Engine instances in a VPC network to access Google services over the private IP addresses, without the need for a NAT gateway or VPN.\n\nThis feature is especially useful when you want to access Google APIs and services from an instance that doesn't have internet access or a public IP address. In this case, you can enable Private Google Access on the VPC network that your Compute Engine instances belong to, and they will be able to call the Cloud Storage API using the private IP address.\n\nTo enable Private Google Access, you can use the gcloud command-line tool, the Cloud Console, or the REST API. This feature is also available for other services like BigQuery and Cloud SQL as well, to access them from instances without a public IP address","poster":"omermahgoub","upvote_count":"1","timestamp":"1704952020.0","comment_id":"772079"},{"timestamp":"1703151540.0","poster":"zellck","comment_id":"752084","upvote_count":"1","content":"Selected Answer: D\nD is the answer.\n\nhttps://cloud.google.com/vpc/docs/private-google-access\nVM instances that only have internal IP addresses (no external IP addresses) can use Private Google Access. They can reach the external IP addresses of Google APIs and services. The source IP address of the packet can be the primary internal IP address of the network interface or an address in an alias IP range that is assigned to the interface. If you disable Private Google Access, the VM instances can no longer reach Google APIs and services; they can only send traffic within the VPC network."},{"upvote_count":"2","comment_id":"649279","content":"Selected Answer: D\nD is correct","timestamp":"1692511620.0","poster":"tomato123"},{"content":"Selected Answer: D\nYup, it's D.","upvote_count":"1","poster":"akshaychavan7","comment_id":"643185","timestamp":"1691295240.0"},{"content":"Selected Answer: D\nI vote for D as well","comment_id":"556811","timestamp":"1677430980.0","upvote_count":"4","poster":"GCPCloudArchitectUser"},{"comment_id":"520353","timestamp":"1673284200.0","poster":"scaenruy","upvote_count":"4","comments":[{"content":"Yes agree should be Option D","timestamp":"1673803020.0","comment_id":"524312","upvote_count":"1","poster":"Blueocean"}],"content":"I vote D\nhttps://cloud.google.com/vpc/docs/private-google-access"}]},{"id":"JcNULDlH79nJByr2LTBU","exam_id":7,"choices":{"C":"Use signed URLs to upload files.","B":"Use CPanel to upload files.","A":"Use FTP to upload files.","D":"Change the API to be a multipart file upload API."},"question_images":[],"isMC":true,"answer":"C","unix_timestamp":1604700420,"answer_description":"","question_text":"You need to migrate an internal file upload API with an enforced 500-MB file size limit to App Engine.\nWhat should you do?","answer_images":[],"question_id":13,"answers_community":["C (50%)","D (50%)"],"timestamp":"2020-11-06 23:07:00","url":"https://www.examtopics.com/discussions/google/view/36292-exam-professional-cloud-developer-topic-1-question-11/","answer_ET":"C","discussion":[{"timestamp":"1732764240.0","comment_id":"1318996","upvote_count":"1","content":"Selected Answer: C\nVia 'Signed URLs' to upload the file directly to Google Cloud Storage without 32MB limit of App Engine.","poster":"wuhaosheng"},{"upvote_count":"1","timestamp":"1720850940.0","comment_id":"1247130","poster":"d_ella2001","content":"Selected Answer: C\nC is correct: Signed URLs allow you to generate URLs with limited-time access to upload directly to Google Cloud Storage. This method bypasses App Engine's file upload limitations and allows clients to upload large files directly to Cloud Storage."},{"content":"Selected Answer: C\nD. Change the API to be a multipart file upload API: While multipart uploads can be helpful for large files, they don't address the core issue of App Engine's size limitations. The uploads would still need to go through App Engine, potentially exceeding the limits.","timestamp":"1717682160.0","upvote_count":"1","comment_id":"1225559","poster":"pico"},{"content":"Selected Answer: C\nC. Use signed URLs to upload files: Signed URLs are a secure way to give time-limited read or write access to a specific Google Cloud Storage object, without needing Google account credentials. You can create a signed URL that allows an object to be accessed with the specified restrictions such as HTTP method (PUT for uploads) and an expiration time. This method would allow your API users to upload files directly to Google Cloud Storage, which can handle large files efficiently. Your App Engine application can then process or reference these files as needed.","upvote_count":"1","comment_id":"1168106","poster":"santoshchauhan","timestamp":"1709822940.0"},{"poster":"manikanthk","content":"Selected Answer: C\nhttps://stackoverflow.com/questions/45812595/google-cloud-storage-signed-urls-how-to-specify-a-maximum-file-size","upvote_count":"1","timestamp":"1709209140.0","comment_id":"1162522"},{"poster":"theseawillclaim","timestamp":"1707669420.0","upvote_count":"3","comment_id":"1147506","content":"Selected Answer: D\nWhile C is a very good option if you want to people to upload files, it does not solve the problem represented by the size."},{"comment_id":"1011386","timestamp":"1695130980.0","upvote_count":"1","poster":"__rajan__","content":"Selected Answer: D\nBy changing the API to support multipart file uploads, you can maintain the functionality of your existing API while adapting it to the App Engine environment."},{"upvote_count":"2","comment_id":"988950","timestamp":"1692861720.0","poster":"maxdanny","content":"The correct answer is C because signed url permits to upload a big file in multipart-mode"},{"content":"Selected Answer: D\nIt should use multipart to upload big size files","upvote_count":"2","timestamp":"1687822200.0","poster":"DonWang","comment_id":"934865"},{"content":"How is C correct ? Isn't it used to give temporary access to objects in buckets ?","upvote_count":"1","comment_id":"854208","timestamp":"1680081660.0","poster":"Ayushman_koul23"},{"poster":"tomato123","timestamp":"1660973340.0","content":"Selected Answer: C\nC is correct","comment_id":"649168","upvote_count":"1"},{"poster":"wilwong","upvote_count":"1","content":"C is the best choice","timestamp":"1626851940.0","comment_id":"410718"},{"upvote_count":"2","poster":"syu31svc","comment_id":"386003","timestamp":"1624170840.0","content":"https://cloud.google.com/appengine/docs/standard/php/googlestorage/user_upload:\n\"Note that you must start uploading to this URL within 10 minutes of its creation. Also, you cannot change the URL in any way - it is signed and the signature is checked before your upload begins\"\n\nC is the answer"},{"comments":[{"comment_id":"361400","poster":"mastodilu","upvote_count":"2","timestamp":"1621429740.0","content":"true\nhttps://stackoverflow.com/a/18882565/8681600"}],"poster":"saurabh1805","content":"C is correct answer","upvote_count":"2","comment_id":"214310","timestamp":"1604700420.0"}],"topic":"1"},{"id":"Xtd7oKUY5JXsVBrD0blJ","question_id":14,"isMC":true,"topic":"1","unix_timestamp":1641748560,"answer":"D","answers_community":["D (100%)"],"url":"https://www.examtopics.com/discussions/google/view/69771-exam-professional-cloud-developer-topic-1-question-110/","answer_images":[],"choices":{"A":"Create a new feature branch, and ask the build team to rebuild the image.","B":"Shut down the deployed virtual machine, export the disk, and then mount the disk locally to access the boot logs.","C":"Install Packer locally, build the Compute Engine image locally, and then run it in your personal Google Cloud project.","D":"Check Compute Engine OS logs using the serial port, and check the Cloud Logging logs to confirm access to the serial port."},"exam_id":7,"question_text":"You are a developer working with the CI/CD team to troubleshoot a new feature that your team introduced. The CI/CD team used HashiCorp Packer to create a new Compute Engine image from your development branch. The image was successfully built, but is not booting up. You need to investigate the issue with the CI/\nCD team. What should you do?","answer_ET":"D","answer_description":"","question_images":[],"timestamp":"2022-01-09 18:16:00","discussion":[{"content":"I vote D\nhttps://cloud.google.com/compute/docs/troubleshooting/troubleshooting-using-serial-console","upvote_count":"9","poster":"scaenruy","timestamp":"1641748560.0","comments":[{"comment_id":"524314","timestamp":"1642267200.0","upvote_count":"4","poster":"Blueocean","content":"Agree with Option D"}],"comment_id":"520359"},{"poster":"thewalker","upvote_count":"1","comments":[{"timestamp":"1721368140.0","content":"Let's look at why the other options are less ideal:\n\nA. Create a new feature branch and ask the build team to rebuild the image: This is a time-consuming and potentially unnecessary step. It's better to investigate the issue with the existing image first.\nB. Shut down the deployed virtual machine, export the disk, and then mount the disk locally to access the boot logs: This is a complex and potentially disruptive process. It's better to leverage the serial port logs for initial troubleshooting.\nC. Install Packer locally, build the Compute Engine image locally, and then run it in your personal Google Cloud project: This approach might help isolate the issue, but it doesn't address the root cause of the problem in the CI/CD pipeline.","comment_id":"1250877","upvote_count":"1","poster":"thewalker"}],"timestamp":"1721368080.0","comment_id":"1250876","content":"Selected Answer: D\nThe best approach here is D. Check Compute Engine OS logs using the serial port and check the Cloud Logging logs to confirm access to the serial port.\n\nHere's why:\n\nSerial Port Logs: Compute Engine instances have a serial port that captures boot messages and other system-level information. This is often the most valuable source of information when troubleshooting boot failures.\nCloud Logging: If you've enabled serial port logging in your project, the boot messages will be captured and sent to Cloud Logging. This provides a centralized location for reviewing the logs."},{"content":"Selected Answer: D\nD is the answer - the other are too long.","comment_id":"956645","poster":"jnas","upvote_count":"1","timestamp":"1689771360.0"},{"upvote_count":"2","comment_id":"772083","poster":"omermahgoub","timestamp":"1673416140.0","content":"Selected Answer: D\nAnswer is D\n\nIf the Compute Engine image is not booting up, one of the first steps to troubleshoot the issue would be to check the OS logs to see what might be causing the problem. Compute Engine provides access to the serial console logs of a virtual machine, which can be accessed through the Cloud Console or the gcloud command-line tool. This will allow you to see the output of the virtual machine's boot process and identify any errors or issues that might be preventing it from starting up.\n\nAdditionally, you should also check the Cloud Logging logs to confirm that you have access to the serial port. It may be possible that the firewall rules or IAM permissions are blocking access to the serial port and causing the image not to boot. So, you should check the logs for any errors related to access or firewall rules.\n\nBy checking the OS logs and the Cloud Logging logs, you and the CI/CD team can get a better understanding of what might be causing the issue and take steps to fix it."},{"content":"Selected Answer: D\nD is the answer.\n\nhttps://cloud.google.com/compute/docs/troubleshooting/vm-startup#identify_the_reason_why_the_boot_disk_isnt_booting\n\nIdentify the reason why the boot disk isn't booting\n- Examine your virtual machine instance's serial port output.\nAn instance's BIOS, bootloader, and kernel prints their debug messages into the instance's serial port output, providing valuable information about any errors or issues that the instance experienced. If you enable serial port output logging to Cloud Logging, you can access this information even when your instance is not running.","timestamp":"1671545820.0","comment_id":"750987","upvote_count":"1","poster":"zellck"},{"upvote_count":"1","poster":"ash_meharun","content":"In option D, what does it mean by \"confirm access to the serial port\"?\nIf I need to see the boot logs, then how the checking the access to serial port gonna help?","timestamp":"1670240400.0","comment_id":"735888"},{"comment_id":"723352","content":"https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-using-serial-console#connecting_to_a_serial_console_with_a_login_prompt\nAnswer D","upvote_count":"1","poster":"TNT87","timestamp":"1669023720.0"},{"timestamp":"1660975620.0","poster":"tomato123","comment_id":"649280","content":"Selected Answer: D\nD is correct","upvote_count":"3"},{"timestamp":"1641988980.0","poster":"ParagSanyashiv","upvote_count":"3","content":"Selected Answer: D\nD is more suitable","comments":[{"content":"This is interesting to learn that if a compute engine isn’t bootable and you can connect still","poster":"GCPCloudArchitectUser","upvote_count":"4","comment_id":"556941","timestamp":"1645903560.0"}],"comment_id":"522120"}]},{"id":"mafmABkE9VASKrD2Ut2w","discussion":[{"content":"\"the backend services are scaled by managed instance groups in multiple regions\", the Internal load balancer is a regional service, so It's A in my opinion.\n\n- \"An internal HTTP(S) load balancer routes internal traffic to the service running on the VM. Traffic Director works with Cloud Load Balancing to provide a managed ingress experience. You set up an external or internal load balancer, and then configure that load balancer to send traffic to your microservices.\"","upvote_count":"9","timestamp":"1651838400.0","poster":"[Removed]","comment_id":"597718"},{"upvote_count":"2","timestamp":"1721368680.0","comments":[{"poster":"thewalker","comment_id":"1250884","upvote_count":"2","content":"Let's look at why the other options are less ideal:\n\nB. Service Directory: Service Directory is primarily used for service discovery, allowing applications to find and connect to services within a cluster or across multiple clusters. While it can be used for routing, it's not as robust as Traffic Director for advanced traffic management.\nC. Anthos Service Mesh: Anthos Service Mesh is a powerful service mesh solution that provides features like traffic management, security, and observability. However, it's typically used for microservices running in Kubernetes environments, not for stand-alone Docker containers in Compute Engine.\nD. Internal HTTP(S) Load Balancing: Internal Load Balancing is designed for routing traffic within a VPC network. While it can handle basic routing, it doesn't offer the advanced features and flexibility of Traffic Director for dynamic routing based on HTTP headers.","timestamp":"1721368740.0"}],"content":"Selected Answer: A\nTraffic Director: Traffic Director is a Google Cloud service designed for advanced traffic management across multiple regions and zones. It allows you to route traffic based on various criteria, including HTTP headers. This makes it ideal for loosely coupled applications where you want to dynamically choose backend services based on request attributes.\n\nHow Traffic Director Works:\nRouting Rules: You define routing rules in Traffic Director that specify how traffic should be directed based on HTTP headers, path patterns, or other criteria.\nBackend Services: You associate backend services with your routing rules. These backend services can be managed instance groups, network endpoint groups (NEGs), or other supported backends.\nTraffic Distribution: When a request arrives, Traffic Director evaluates the routing rules and directs the traffic to the appropriate backend service based on the matching criteria.","comment_id":"1250883","poster":"thewalker"},{"upvote_count":"1","comment_id":"1196955","poster":"alpha_canary","timestamp":"1713323340.0","content":"Selected Answer: A\nhttps://cloud.google.com/traffic-director/docs/overview#traffic_management:~:text=Advanced%20traffic%20management%2C%20including%20routing%20and%20request%20manipulation%20(based%20on%20hostname%2C%20path%2C%20headers%2C%20cookies%2C%20and%20more)%2C%20enables%20you%20to%20determine%20how%20traffic%20flows%20between%20your%20services"},{"content":"Selected Answer: C\nAnthos Service Mesh is a service mesh solution that can be used to invoke distinct service implementations based on the value of an HTTP header in the request. It provides a platform-agnostic way to connect, manage, and secure microservices running on Google Cloud or other environments","poster":"braska","comment_id":"1073631","upvote_count":"1","timestamp":"1700254320.0"},{"upvote_count":"1","content":"Selected Answer: A\nTraffic Director is a Google Cloud feature that provides a global traffic management control plane for service mesh architectures. It allows you to configure and manage traffic routing across multiple services and environments.","comment_id":"1012146","timestamp":"1695201300.0","poster":"__rajan__"},{"timestamp":"1691228760.0","comment_id":"972857","upvote_count":"2","poster":"purushi","content":"I go for A: Traffic director is used to direct the traffic to services running in the different regions or within a region based on the HTTP header values."},{"comment_id":"956648","poster":"jnas","upvote_count":"1","content":"Selected Answer: A\nA as its the only one that let's you router based on headers","timestamp":"1689771540.0"},{"comment_id":"876649","poster":"closer89","content":"Selected Answer: A\nhttps://cloud.google.com/traffic-director/docs/overview#traffic_management\nAdvanced traffic management, including routing and request manipulation (based on hostname, path, headers, cookies, and more), enables you to determine how traffic flows between your services. You can also apply actions like retries, redirects, and weight-based traffic splitting for canary deployments. Advanced patterns like fault injection, traffic mirroring, and outlier detection enable DevOps use cases that improve your resiliency.","timestamp":"1682088720.0","upvote_count":"1"},{"timestamp":"1677505140.0","comment_id":"823740","content":"Selected Answer: A\nA: https://cloud.google.com/traffic-director/docs/set-up-gce-vms","poster":"Pime13","upvote_count":"1"},{"timestamp":"1673416320.0","upvote_count":"1","comments":[{"comment_id":"772085","content":"It also works with MIGs to allow you to manage the scaling of the backend services in different regions, and the combination of Traffic Director and MIG allow you to provide service availability in multiple regions.","timestamp":"1673416320.0","poster":"omermahgoub","upvote_count":"1"},{"comment_id":"876652","poster":"closer89","timestamp":"1682088840.0","upvote_count":"2","content":"do not post chatgpt answers!"}],"comment_id":"772084","content":"Selected Answer: A\nTraffic Director provides global traffic management for service meshes and hybrid deployments. It allows you to configure routing rules based on the values of HTTP headers, so you can direct traffic to different service implementations based on the value of an HTTP header found in the request. With Traffic Director, you can route traffic to different services running in different regions, and it also supports automatic failover, so you can ensure high availability for your backend services.\n\nIn your case, you can configure Traffic Director to inspect the value of an HTTP header in the request, and then route the traffic to the appropriate service implementation running in different regions. This allows your application to invoke the backend services in a loosely coupled way, and ensures that the backend services can scale independently of the calling application.","poster":"omermahgoub"},{"content":"Selected Answer: D\nD is the answer.\n\nhttps://cloud.google.com/load-balancing/docs/l7-internal/traffic-management\nInternal HTTP(S) Load Balancing supports advanced traffic management functionality that enables you to use the following features:\n- Traffic steering. Intelligently route traffic based on HTTP(S) parameters (for example, host, path, headers, and other request parameters).","poster":"zellck","comment_id":"750980","timestamp":"1671545640.0","upvote_count":"2"},{"content":"Selected Answer: A\nI think A is correct","poster":"tomato123","upvote_count":"2","timestamp":"1660975680.0","comment_id":"649282"},{"timestamp":"1659762000.0","upvote_count":"3","comments":[{"poster":"akshaychavan7","timestamp":"1659762060.0","comment_id":"643197","upvote_count":"1","content":"I went through the link shared by Blueocean, and also the Google documentation for Traffic Director, and I feel the correct answer should be option A.\nHere's my take on this -\nHTTP(s) load balancer does allow the option of Traffic Steering which identifies the header values and based on that it directs the traffic. But, here it is important to note that the traffic is directed to the specific Compute Engine instance and not to the service endpoint!\nOn the other hand, Traffic Director also allows Traffic Steering which directs the traffic based on header values to the specific service endpoint, which is what is needed in the above scenario. It also supports the point of loosely coupled services.\nSources - \nhttps://cloud.google.com/traffic-director\nhttps://cloud.google.com/traffic-director/docs/features"},{"comment_id":"643198","content":"Here's what the documentation says, 'Traffic Director supports advanced request routing features like traffic splitting, enabling use cases like canarying, url rewrites/redirects, fault injection, traffic mirroring, and advanced routing capabilities based on various header values, including cookies. Traffic Director also supports many advanced traffic policies with the inclusion of many load-balancing schemes, circuit breaking, and backend outlier detections.' \n\nAlso, in the above question, there's a statement specifying the need for the application to be loosely coupled. To support this point, I found out one line on the Traffic Director's documentation which goes like this, 'This separation of application logic from networking logic lets you improve your development velocity, increase service availability, and introduce modern DevOps practices to your organization.'","timestamp":"1659762120.0","upvote_count":"1","poster":"akshaychavan7"}],"content":"Selected Answer: A\nI believe, it should be A.","comment_id":"643195","poster":"akshaychavan7"},{"comment_id":"524317","timestamp":"1642267440.0","poster":"Blueocean","upvote_count":"2","content":"Agree with Option D \nhttps://cloud.google.com/load-balancing/docs/l7-internal/traffic-management"},{"timestamp":"1641715200.0","content":"Agree with D","poster":"ParagSanyashiv","upvote_count":"3","comment_id":"520012"}],"unix_timestamp":1641715200,"isMC":true,"answers_community":["A (81%)","Other"],"url":"https://www.examtopics.com/discussions/google/view/69722-exam-professional-cloud-developer-topic-1-question-111/","answer_images":[],"answer_ET":"A","timestamp":"2022-01-09 09:00:00","choices":{"B":"Service Directory","D":"Internal HTTP(S) Load Balancing","C":"Anthos Service Mesh","A":"Traffic Director"},"question_text":"You manage an application that runs in a Compute Engine instance. You also have multiple backend services executing in stand-alone Docker containers running in Compute Engine instances. The Compute Engine instances supporting the backend services are scaled by managed instance groups in multiple regions. You want your calling application to be loosely coupled. You need to be able to invoke distinct service implementations that are chosen based on the value of an HTTP header found in the request. Which Google Cloud feature should you use to invoke the backend services?","question_id":15,"answer_description":"","topic":"1","question_images":[],"exam_id":7,"answer":"A"}],"exam":{"name":"Professional Cloud Developer","provider":"Google","lastUpdated":"11 Apr 2025","numberOfQuestions":338,"isImplemented":true,"isBeta":false,"id":7,"isMCOnly":false},"currentPage":3},"__N_SSP":true}