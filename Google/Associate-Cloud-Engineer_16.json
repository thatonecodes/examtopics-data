{"pageProps":{"questions":[{"id":"kTDFqdamya5sQz697N2A","question_text":"Your company developed a mobile game that is deployed on Google Cloud. Gamers are connecting to the game with their personal phones over the Internet. The game sends UDP packets to update the servers about the gamers' actions while they are playing in multiplayer mode. Your game backend can scale over multiple virtual machines (VMs), and you want to expose the VMs over a single IP address. What should you do?","choices":{"C":"Configure an External HTTP(s) load balancer in front of the application servers.","A":"Configure an SSL Proxy load balancer in front of the application servers.","B":"Configure an Internal UDP load balancer in front of the application servers.","D":"Configure an External Network load balancer in front of the application servers."},"answer_ET":"D","isMC":true,"question_id":76,"timestamp":"2021-03-11 16:41:00","unix_timestamp":1615477260,"answer":"D","answer_images":[],"exam_id":1,"discussion":[{"timestamp":"1634501580.0","comment_id":"337787","poster":"kopper2019","content":"Answer is D, cell phones are sending UDP packets and the only that can receive that type of traffic is a External Network TCP/UDP\nhttps://cloud.google.com/load-balancing/docs/network","comments":[{"comment_id":"425985","timestamp":"1645048020.0","content":"Google Cloud HTTP(S) Load Balancing is a global, proxy-based Layer 7 load balancer that enables you to run and scale your services worldwide behind a single external IP address. External HTTP(S) Load Balancing distributes HTTP and HTTPS traffic to backends hosted on Compute Engine and Google Kubernetes Engine (GKE).\n\nhttps://cloud.google.com/load-balancing/docs/https","comments":[{"content":"All the load balancer products in GCP give you a single IP address for the backend servers you registered to it.\n\nAlso, External HTTP(s) load balancer only support the port that used by HTTP which is the port 80 and HTTPS which is the port 443.\n\nAnd Google Cloud external TCP/UDP Network Load Balancing is referred to as \"Network Load Balancing\" which supports UDP packets.\n\n- https://cloud.google.com/load-balancing/docs/load-balancing-overview#about\n- https://cloud.google.com/load-balancing/docs/network\n- https://cloud.google.com/load-balancing/docs/https","poster":"ryumada","comment_id":"643216","upvote_count":"3","timestamp":"1675671360.0"},{"comment_id":"630691","upvote_count":"3","timestamp":"1673565540.0","content":"what you are trying to say ? What is your answer ? A B C D ?","poster":"patashish"}],"upvote_count":"2","poster":"ashrafh"}],"upvote_count":"32"},{"upvote_count":"14","comments":[{"poster":"BobbyFlash","timestamp":"1650922320.0","upvote_count":"4","comment_id":"467693","content":"Following the diagram, there's no doubt about D. We have external clients connecting to our gaming service on google cloud that works using UDP traffic that results in using External Network Load Balancing. I feel that it's simple as it is. I also go with D."}],"comment_id":"379378","content":"Answer is D. there are so many confusion here, from B,C or D. For myself im eliminating all options except B,D due to the traffic type. which leaves me with B or D. Then next the traffic source either external or internal which in this case is an external traffic from the internet, therefore my final answer is D. \n\nhttps://cloud.google.com/load-balancing/docs/choosing-load-balancer","timestamp":"1639193100.0","poster":"JH86"},{"timestamp":"1719877860.0","comment_id":"1111545","content":"Selected Answer: D\n1. UDP Traffic Support:\n• An external Network Load Balancer in Google Cloud supports both TCP and UDP traffic. Since your game uses UDP packets for multiplayer interactions, the Network Load Balancer is appropriate for handling this type of traffic.\n2. Single IP for Multiple VMs:\n• Network Load Balancers allow you to use a single, anycast IP address that can distribute incoming traffic across multiple VMs in your backend. This aligns with your requirement to expose the backend servers through a single IP address.","upvote_count":"4","poster":"Cynthia2023"},{"comment_id":"999606","content":"Selected Answer: D\nfor udp external load balancer, D is the correct answer","timestamp":"1709656440.0","upvote_count":"2","poster":"Captain1212"},{"timestamp":"1702301760.0","comment_id":"920619","poster":"CVGCP","upvote_count":"5","content":"By elimination\nA: SSL proxy LB is for TCP traffic not for UDP, eliminated\nB: External LB is required, Eliminated\nC: Http LB works at layer 7, here protocol is UDP, eliminated\nD: Correct answer"},{"comment_id":"901512","upvote_count":"1","timestamp":"1700351280.0","poster":"kumar262639","content":"\"Correct Answer\" says A and community vote says D(100%) \nwhich one is correct?"},{"timestamp":"1695845880.0","comment_id":"852451","upvote_count":"1","content":"Going with D","poster":"PPP_D"},{"comment_id":"811834","timestamp":"1692266460.0","content":"Selected Answer: D\nAns is D","poster":"Andoameda9","upvote_count":"1"},{"poster":"fragment137","upvote_count":"2","comment_id":"733038","content":"The question tricked me. I saw UDP and immediately thought it was B.\n\nThe correct answer is D, as the LB needs to be External, and SSL\\HTTPS are not the right load balancers for this application.","timestamp":"1685642760.0"},{"upvote_count":"1","content":"Selected Answer: D\nExternal Network LB used for UDP","poster":"abirroy","comment_id":"639344","timestamp":"1675021260.0"},{"comment_id":"621578","content":"D seems correct..","upvote_count":"1","timestamp":"1671885240.0","poster":"AzureDP900"},{"content":"Selected Answer: D\nI'm dead sure, it's D!","poster":"akshaychavan7","timestamp":"1669301100.0","comment_id":"606711","upvote_count":"1"},{"content":"Selected Answer: D\nhttps://cloud.google.com/load-balancing/docs/choosing-load-balancer#lb-decision-tree","comment_id":"575021","poster":"somenick","timestamp":"1664103540.0","upvote_count":"2"},{"comment_id":"568611","poster":"[Removed]","upvote_count":"1","timestamp":"1663262460.0","content":"Selected Answer: D\nD - Check https://cloud.google.com/load-balancing/images/choose-lb.svg"},{"timestamp":"1658692680.0","content":"Selected Answer: D\nD, because: \nhttps://cloud.google.com/load-balancing/docs/network#:~:text=Google%20Cloud%20external-,TCP/UDP,-Network%20Load%20Balancing","comment_id":"531605","upvote_count":"2","poster":"Raz0r"},{"poster":"exam_war","comment_id":"521243","timestamp":"1657501200.0","comments":[{"poster":"S00999","upvote_count":"1","comment_id":"617696","content":"It is not specified whether the app protocol is HTTP(S) or not, only that it is UDP paquets. Internet is not limited to the http protocol.\nAnswer D","timestamp":"1671285720.0"}],"content":"Answer is C. Players need to access through internet by HTTP(S) load balancing","upvote_count":"2"},{"content":"D. Configure an External Network load balancer in front of the application servers.\n\"VM over single (external) IP address ->>> getting UDP packets through External LB\n\"","comment_id":"518825","timestamp":"1657172940.0","upvote_count":"3","poster":"[Removed]"},{"content":"The key statement we need to watch out for ; \"And you want to expose the Vms over a single IP address'' . \nGoogle Cloud external TCP/UDP Network Load Balancing (after this referred to as Network Load Balancing) is a regional, \"pass-through\" load balancer. A network load balancer \"distributes external traffic among virtual machine (VM) instances\" in the same region.\nYou can configure a network load balancer for TCP, UDP, ESP, and ICMP traffic. \n\nhttps://cloud.google.com/load-balancing/docs/network","poster":"Eben01","comment_id":"508937","timestamp":"1656118500.0","upvote_count":"3"},{"poster":"dttncl","timestamp":"1649773320.0","upvote_count":"2","comment_id":"461105","content":"My answer here is A for the following reasons. Feel free to comment if I missed something.\n\n1. For a mobile game to reach its maximum audience, it must be available globally. SSL proxy LB supports global load balancing service with the Premium Tier.\n2. It is intended for non-HTTP(S) traffic. For HTTP(S) traffic, GCP recommends using HTTP(S) Load Balancing.\n3. Preserves the original source IP addresses of incoming connections to the load balancer\n4. SSL proxy LB allows you to expose a single IP address\n\nhttps://cloud.google.com/network-tiers/docs/overview#configuring_standard_tier_for_https_lb_and_tcpssl_proxy\nhttps://cloud.google.com/load-balancing/docs/ssl"},{"upvote_count":"2","content":"I was also thinking about option D. Whoever, my problem with option D is that the scenario mentions the following: \n\"Your game backend can scale over multiple virtual machines (VMs)\"\nAnd unless I am reading the documentation wrong, Layer 4 Network LB does not support autoscaling.\nhttps://cloud.google.com/load-balancing/docs/features#autoscaling_and_autohealing","timestamp":"1647697680.0","poster":"pas77","comment_id":"447561"},{"poster":"procloud","upvote_count":"2","content":"Answer is D ,https://cloud.google.com/load-balancing/docs/choosing-load-balancer\u0003\nLoad balancer type Traffic type \nExternal HTTP(S) HTTP or HTTPS \u0003\nExternal TCP/UDP Network\n(also referred to as Network Load Balancing) TCP, UDP, ESP, or ICMP (Preview)","timestamp":"1646935740.0","comment_id":"442577"},{"upvote_count":"4","poster":"associatecloudexamuser","comment_id":"405476","timestamp":"1642089540.0","content":"Oh. Missed \"Internal\" term. So it should be External Load Balancer. \nAnswer is D."},{"comment_id":"360785","upvote_count":"1","content":"Answer is D, Reason- UDP and preserve client ID","poster":"curious_kitty9","timestamp":"1637269560.0"},{"upvote_count":"3","content":"Ans: B check in https://cloud.google.com/load-balancing/docs/internal/multiple-forwarding-rules-same-ip","comment_id":"341965","timestamp":"1635070440.0","poster":"jahnu"},{"upvote_count":"4","content":"D is the correct answer. External Network Load Balancing does provide a single IP. Also it allows UDP traffic too","comment_id":"333381","poster":"mj98","timestamp":"1633965180.0"},{"content":"Now, as we have UDP packets it's either External Network TCP/UDP Load balancing or Internal TCP/UDP Load balancing. But, at the end, it also mentions to \"expose the VMs over a single IP address\" then i have seen the official documentation which clearly mentions the point. \nhttps://cloud.google.com/load-balancing/docs/https","upvote_count":"3","comment_id":"325929","timestamp":"1633098300.0","poster":"tavva_prudhvi"},{"content":"answer is D","timestamp":"1632629280.0","upvote_count":"3","comment_id":"320837","poster":"yuvarajmrec"},{"upvote_count":"4","comments":[{"timestamp":"1633098540.0","poster":"tavva_prudhvi","comment_id":"325933","upvote_count":"2","comments":[{"comments":[{"content":"Here, https://cloud.google.com/load-balancing/images/choose-lb.svg specifies Internet-to-GCP traffic, which means incoming UDP traffic. Also, it's possible to expose the VMs with a single external IP address: https://cloud.google.com/load-balancing/docs/features#ip_addresses.","timestamp":"1638334860.0","poster":"jcols","upvote_count":"3","comment_id":"371519"}],"upvote_count":"3","poster":"victory108","timestamp":"1633587840.0","comment_id":"330093","content":"No, requirement is to expose the VMs over a single IP address. UDP packets just sent to update the servers, it's not asked to serve UDP traffic."}],"content":"Yeah, but what about the UDP packets mentioned here, will you choose D then?"}],"timestamp":"1631886540.0","poster":"victory108","content":"C - Configure an External HTTP(s) load balancer in front of the application servers.\nNetwork Load balancer is regional and not global.","comment_id":"313413"},{"content":"C. Configure an External HTTP(s) load balancer in front of the application servers.","upvote_count":"3","timestamp":"1631643900.0","comments":[{"content":"Read some theory! Or try to google!\nHere is the link\nhttps://cloud.google.com/load-balancing/docs/choosing-load-balancer\nUDP traffic is possible with 2 options:\n- External Network TCP/UDP Load balancing \n- Internal TCP/UDP Load balancing\nFor LoadBalancer to be available outside of GCP we will need External Network TCP/UDP LoadBalancer.\n\"D\" is correct answer!","upvote_count":"6","comment_id":"311312","poster":"TAvenger","comments":[{"comment_id":"311673","poster":"GCP_Student1","timestamp":"1631724360.0","content":"It will make life easier if we understand the question correctly.\n1. I think the load balancer behavior would not change for he type of external traffic received, be it TCP/UDP etc. It will distribute it to the appropriate recipient at the backend. \n2. This question asks only one thing. (To expose the VMs on a SINGLE IP Address). This is not doable with External Network TCP/UDP Load Balancer.","upvote_count":"3","comments":[{"upvote_count":"4","poster":"pca2b","content":"you want to expose the VMs over a single IP address. What should you do?\n...just looking at backends, they will need an internal TCP/UDP LB, to provide a single IP for multiple VMs.\n\nGame users will use an external NLB also, but that is not the point of the question.\n\nso I vote for B","comments":[{"timestamp":"1633098660.0","upvote_count":"1","content":"Sorry, can you mention the supporting articles stating the same?","comments":[{"timestamp":"1633282380.0","content":"the game backends need to be consolidated behind 1x IP address. messages are coming to backend in UDP, so a UDP internal LB is needed. i.e. B. Please follow the flowchart: https://cloud.google.com/load-balancing/images/choose-lb.svg\n\nNot to confuse with several discussions here about external LB. there may be a need for that too...but the question is asking specifically about the backends behind ONE IP address. That has to be TCP/UDP internal LB, imho.","upvote_count":"1","poster":"pca2b","comment_id":"327517"},{"poster":"pca2b","timestamp":"1633282620.0","content":"the game has a frontend interface, and a backend interface.\nwe're just being asked about the backend VMs behind a LB IP.\n\nhope that helps. Pls upvote if you agree :)","upvote_count":"5","comment_id":"327520"}],"poster":"tavva_prudhvi","comment_id":"325936"}],"comment_id":"322954","timestamp":"1632866340.0"}]}],"timestamp":"1631694000.0"}],"comment_id":"310851","poster":"GCP_Student1"},{"content":"Correct is D - External Network load balancer.https://cloud.google.com/load-balancing/docs/network","comment_id":"310422","timestamp":"1631604780.0","comments":[{"upvote_count":"5","content":"The main part of the question is in quotes\n \"you want to expose the VMs over a SINGLE IP address.\"\nThis is possible only with HTTPs load balancing. The HTTPs Load Balancer terminate all incoming traffic at load balancer point and establish new connection to VMs in the backend. This way the outside world can only see a SINGLE IP address of the load balancer.","comment_id":"310858","comments":[{"comment_id":"320834","timestamp":"1632628740.0","upvote_count":"1","content":"HTTP loadbalancer does not terminate incoming traffic at loadbalancer. it just balances the traffic among backend resources","poster":"yuvarajmrec"},{"comments":[{"comment_id":"311675","upvote_count":"3","poster":"GCP_Student1","content":"Follow this link for more info on this;\n\nhttps://cloud.google.com/load-balancing/docs/network","timestamp":"1631724480.0"}],"upvote_count":"1","comment_id":"311313","content":"The answer is \"D\" !!!\nRead some theory! Or try to google!\nHere is the link. Check what types of LoadBalancers support UDP\nhttps://cloud.google.com/load-balancing/docs/choosing-load-balancer","poster":"TAvenger","timestamp":"1631694060.0"},{"content":"I thought External Network load balancer can also have a single static external IP address? https://cloud.google.com/load-balancing/docs/network/setting-up-network-backend-service#testing_the_load_balancer","poster":"JieHeng","upvote_count":"1","timestamp":"1640533500.0","comment_id":"391288"}],"timestamp":"1631644560.0","poster":"GCP_Student1"}],"poster":"iri_gcp","upvote_count":"3"},{"poster":"alexin2","upvote_count":"3","timestamp":"1631603040.0","comment_id":"310410","content":"Answer is D as it states it that gamers need to send \"UDP packets\" - Therefore external network loadbanlancer. \nSee: https://cloud.google.com/load-balancing/docs/choosing-load-balancer","comments":[{"upvote_count":"4","timestamp":"1631645400.0","comments":[{"upvote_count":"4","content":"External Network TCP/UDP LoadBalancer will provide you single IP address. What's the problem?","timestamp":"1631732220.0","poster":"TAvenger","comment_id":"311763"}],"comment_id":"310870","poster":"GCP_Student1","content":"How about this sentence in question ?\n\"you want to expose the VMs over a SINGLE IP address.\""}]},{"comment_id":"310327","content":"Answer is D. because UPD Packet","upvote_count":"4","timestamp":"1631594100.0","poster":"pdbeta"},{"content":"answer is d\nFor UDP traffic, use:\nNetwork Load Balancing\nInternal TCP/UDP Load Balancing","comment_id":"310300","poster":"raman23125","upvote_count":"6","timestamp":"1631591580.0"},{"poster":"Jamaal_a","timestamp":"1631367660.0","comments":[{"timestamp":"1671286560.0","upvote_count":"1","comment_id":"617701","content":"Doesn't support UDP packets.","poster":"S00999"}],"upvote_count":"3","comment_id":"308007","content":"Answer is C - Google Cloud HTTP(S) Load Balancing is a global, proxy-based Layer 7 load balancer that enables you to run and scale your services worldwide behind a single external IP address."}],"question_images":[],"answers_community":["D (100%)"],"url":"https://www.examtopics.com/discussions/google/view/46427-exam-associate-cloud-engineer-topic-1-question-167/","answer_description":"","topic":"1"},{"id":"ZF69A9pRSI0S2LxOYent","discussion":[{"comment_id":"310834","comments":[{"comment_id":"322956","content":"Pub/Sub will be good for all future files in in-prem data-storage.\n\nwe want to sync all + new, so a local on-prem server running a cron job (not GCE CronJob) to run gsutil to transfer files to Cloud Storage would work.\n\nI vote for C","upvote_count":"6","comments":[{"poster":"yvinisiupacuando","comment_id":"348386","timestamp":"1635932400.0","upvote_count":"6","content":"Sorry you are wrong, the question clearly indicates \"The hospital wants an automated process to upload ANY NEW medical images to Cloud Storage.\" It does not mention the need to upload the original stock of images, only the new ones. Then I think the right answer must be A, as you said \"Pub/sub will be good for all future files in prem data-storage\" which is exactly what the questions is pointing to.","comments":[{"comment_id":"683911","content":"In option C we are using a cron job, not dragging and dropping the images.","poster":"Priyanka109","timestamp":"1680312000.0","upvote_count":"1"},{"timestamp":"1643410920.0","upvote_count":"3","comment_id":"416476","poster":"gcpengineer","content":"ans is C"}]}],"timestamp":"1632866580.0","poster":"pca2b"},{"poster":"dunhill","comment_id":"311556","timestamp":"1631712600.0","upvote_count":"1","content":"I am not sure but the question also mentions that \"wants to use Cloud Storage for archival storage of these images\". It can create an application that sends all medical images to storage and no need via PubSub?"}],"timestamp":"1631642880.0","upvote_count":"26","content":"From the question the key point is \"upload ANY NEW medical images to Cloud Storage\". So we are not interested in old images. That's why we need some trigger that will upload images. I think option \"A\" with PubSub is the best","poster":"TAvenger"},{"poster":"GCP_Student1","comments":[{"content":"Where does it say that the on-premises images are already digitized? and even if they are, where does it say that we also keep the old images?\nI think the correct answer is \"A\"","timestamp":"1676537640.0","comment_id":"647501","upvote_count":"1","comments":[{"content":"Tell yo yourself how the images would end up in the pubsub first of all. Also usually the process is in the other way around for pubsub notifications: Once an object lands in GCS the pubsub is notified of it. \n\nOption A makes totally nonsense. Check the flow again. \n\nFrom the options the only one that \"makes more sense\" is Option C","upvote_count":"7","timestamp":"1678586160.0","poster":"theBestStudent","comment_id":"666513"}],"poster":"cserra"}],"timestamp":"1631645940.0","content":"C. Create a script that uses the gsutil command line interface to synchronize the on-premises storage with Cloud Storage. Schedule the script as a cron job.","comment_id":"310884","upvote_count":"23"},{"poster":"kamee15","upvote_count":"2","timestamp":"1735908720.0","comment_id":"1336014","content":"Selected Answer: C\nThe best option is:\n\n\"Create a script that uses the gsutil command line interface to synchronize the on-premises storage with Cloud Storage. Schedule the script as a cron job.\"\n\nThis solution is straightforward and efficient for automating the upload of new medical images. The gsutil tool is designed for interacting with Cloud Storage and can synchronize files effectively. Scheduling the script as a cron job ensures that any new images are automatically uploaded at regular intervals without manual intervention. Using Pub/Sub is more complex and better suited for event-driven architectures rather than bulk data transfer from on-premises storage."},{"upvote_count":"8","content":"Selected Answer: C\nA. Pub/Sub Topic with Cloud Storage Trigger: While Cloud Pub/Sub is great for event-driven architectures, it's not directly applicable for file synchronization scenarios. It would also require substantial modification to the existing infrastructure to send images to Pub/Sub.\n\nB. Dataflow Job: Dataflow is a powerful service for stream and batch data processing, but using it solely for file synchronization is overkill. It also requires more setup and maintenance compared to a simple gsutil script.\n\nD. Manual Upload in Cloud Console: This is not feasible for an automated process, as it requires manual intervention and isn’t practical for a large number of files.","poster":"Cynthia2023","comment_id":"1111548","timestamp":"1719878340.0"},{"timestamp":"1710745080.0","content":"Selected Answer: A\nI am also for A. Any new data should be send to Cloud Storage. Yes you need to create an application. To send data to pubsub. But for possible migration to cloud you can use the existing setup","poster":"jkim1708","upvote_count":"1","comment_id":"1010263"},{"upvote_count":"2","poster":"Captain1212","timestamp":"1709656680.0","content":"Selected Answer: C\nC is the right answer as they want the automated process to upload any new medical image","comment_id":"999610"},{"poster":"respawn","comment_id":"994201","content":"Selected Answer: C\nC is correct. \nA will not work because pub/sub is meant for service to service communication only: \nhttps://cloud.google.com/pubsub/docs/overview#compare_service-to-service_and_service-to-client_communication\nYes C option will sync any new images from onprem to cloud.","timestamp":"1709223180.0","upvote_count":"2"},{"poster":"shreykul","content":"Selected Answer: A\nNew images can use Pub/Sub","timestamp":"1706014800.0","upvote_count":"1","comment_id":"960362"},{"upvote_count":"8","comments":[{"content":"I agree. The requirement is for both history images and future images. So I go with Option \"C\".","upvote_count":"2","timestamp":"1704481860.0","comment_id":"943979","poster":"Naree"}],"poster":"Charumathi","comment_id":"690024","content":"Selected Answer: C\nC is the correct answer.\nKeyword, they require cloud storage for archival and the want to automate the process to upload new medical image to cloud storage, hence we go for gsutil to copy on-prem images to cloud storage and automate the process via cron job. whereas Pub/Sub listens to the changes in the Cloud Storage bucket and triggers the pub/sub topic, which is not required.","timestamp":"1681032060.0"},{"comment_id":"639641","poster":"zolthar_z","upvote_count":"3","content":"Selected Answer: C\nThe Hospital wants Cloud storage for archival of old images and also sync the new images, for this logic the answer is C","timestamp":"1675082100.0"},{"comment_id":"621586","upvote_count":"1","poster":"AzureDP900","content":"C is correct","timestamp":"1671885660.0"},{"poster":"jblima","comment_id":"594371","content":"Selected Answer: C\nC is correct.","upvote_count":"1","timestamp":"1667037480.0"},{"poster":"wael_tn","comment_id":"591093","timestamp":"1666619100.0","upvote_count":"4","content":"Selected Answer: C\nDiscarding (A) because \"Cloud Storage trigger\". So for option A the triggering event should be making a change in Cloud Storage, while in the real use case, the triggering should be adding a new medical image to the \"on-premises data room\"","comments":[{"comment_id":"666514","upvote_count":"1","timestamp":"1678586280.0","poster":"theBestStudent","content":"Correct, and on top of that, how are they suppose to connect the on premises to the cloud. Nothing is mentioned. But either way, option A does not make sense at all (as you already explained too). Correct option: C"}]},{"poster":"somenick","timestamp":"1664111220.0","upvote_count":"3","content":"Selected Answer: C\nI would go with C\n\nNot A, don’t think you can send image files to Pub/Sub. Technically you can do so by converting image to some binary text, but then we don’t know the size of the image and there is a limitation on message size. Not recommended.\n\nNot B – there is only this template “Datastore to Cloud Storage Text”, as the name implies it is for text, https://cloud.google.com/dataflow/docs/guides/templates/provided-batch#datastore-to-cloud-storage-text, and it reads from datastore which is definitely not where the medical images are stored, from the question “… stores its medical images in an on-premises data room”.\n\nNot D – it’s not automated","comment_id":"575103"},{"comment_id":"545233","poster":"lilapause","content":"Selected Answer: C\nPub/Sub could make sense if you ignore the size limit. But the way it is described with the storage trigger would just not be working.","timestamp":"1660205700.0","upvote_count":"1"},{"poster":"exam_war","comment_id":"522622","content":"A is correct. The key is automation whenever there is a new image, it needs to upload to cloud storage. Only pub/sub can make the automation work.","timestamp":"1657678020.0","upvote_count":"1"},{"comment_id":"518821","content":"I think A(once the hospital receive the new images Cloud pub/sub will act on it) and C(creating a script with rsync command https://stackoverflow.com/questions/37662416/how-to-sync-a-local-folder-with-a-folder-in-a-google-cloud-platform-bucket) must be the Correct options","poster":"[Removed]","upvote_count":"1","timestamp":"1657172760.0"},{"timestamp":"1655710380.0","content":"\" any new medical images\" so \"A\" using Pub\\Sub","upvote_count":"1","poster":"RealEL40","comment_id":"505306"},{"upvote_count":"2","comment_id":"497936","content":"I'll go with A for he following reason:\nYes, the question says that they use onprem to store images and they want those images in gcloud storage from now on, BUT the solution for automation is requested for uploading new images only: \"The hospital wants an automated process to upload any new medical images to Cloud Storage\"\nUsing sync (A) for new images implies that you will continue to use your onprem and keep synchronizing it fover... Sync just once for the old images, new images go directly to gcloud via pubsub, and eventually get rid of the onprem.","poster":"wh1t4k3r","comments":[{"content":"Correcting: \"Using sync (C) for new images implies that you will continue to use your onprem and keep synchronizing it fover... Sync just once for the old images, new images go directly to gcloud via pubsub, and eventually get rid of the onprem.\"","comment_id":"497937","upvote_count":"1","timestamp":"1654788840.0","poster":"wh1t4k3r"}],"timestamp":"1654788780.0"},{"comment_id":"461384","content":"'C' is correct. Question says 'use Cloud Storage for archival'. Pub/Sub is for real time processing and in addition, 'Cloud Storage trigger' does not make sense.","timestamp":"1649828520.0","poster":"ankatsu2010","upvote_count":"3"},{"poster":"jackdbd","comment_id":"414498","timestamp":"1643190900.0","comments":[{"content":"I think the question is: why to use pubsub when I can do it better using direct gcs upload with mutlipart option etc. and this approach is covered in C :)","comment_id":"564851","upvote_count":"2","timestamp":"1662813660.0","poster":"ryzior"}],"upvote_count":"11","content":"I would go with C.\n\nHere is my rationale for excluding option A.\nPub/Sub accepts a maximum of 1,000 messages in a batch, and the size of a batch can not exceed 10 megabytes. A single CT scan can be 35 MB, so unless we chunk it up in many smaller pieces and publish them as smaller messages, using a Pub/Sub topic is not a viable alternative."},{"poster":"associatecloudexamuser","comment_id":"405481","timestamp":"1642090140.0","content":"Answer is C. gsutil rsync <source_location> <destination_location>. This can sync content with Google cloud storage locations","upvote_count":"3"},{"comment_id":"391302","poster":"JieHeng","timestamp":"1640535000.0","content":"I would go with C\n\nNot A, don’t think you can send image files to Pub/Sub. Technically you can do so by converting image to some binary text, but then we don’t know the size of the image and there is a limitation on message size. Not recommended.\n\nNot B – there is only this template “Datastore to Cloud Storage Text”, as the name implies it is for text, https://cloud.google.com/dataflow/docs/guides/templates/provided-batch#datastore-to-cloud-storage-text, and it reads from datastore which is definitely not where the medical images are stored, from the question “… stores its medical images in an on-premises data room”.\n\nNot D – it’s not automated","upvote_count":"2"},{"upvote_count":"1","content":"I am really confuse the question states to upload new images to cloud why do we need the new images to be uploaded first in on premises datastore and then syn it using gsutil","timestamp":"1637663940.0","comment_id":"364306","poster":"Umesh09","comments":[{"timestamp":"1671288000.0","poster":"S00999","content":"Maybe because the existing on-prem application does it like that, today; and there are no plans to redevelop it.","upvote_count":"1","comment_id":"617708"}]},{"upvote_count":"2","timestamp":"1637422320.0","poster":"arsh1916","content":"C 100%","comment_id":"362235"},{"timestamp":"1633869720.0","comment_id":"332537","upvote_count":"6","poster":"[Removed]","content":"https://stackoverflow.com/questions/37662416/how-to-sync-a-local-folder-with-a-folder-in-a-google-cloud-platform-bucket\nAnswer is C\ngsutil rsync + cron job"},{"poster":"victory108","timestamp":"1633585680.0","upvote_count":"3","content":"Need to upload new images to Cloud Storage. Option A doesn't say anything about that.\nC is correct - Create a script that uses the gsutil command line interface to synchronize the on-premises storage with Cloud Storage. Schedule the script as a cron job.","comment_id":"330074"},{"poster":"GoCloud","timestamp":"1632531840.0","comment_id":"319732","upvote_count":"2","content":"Cron jobs only support Http, pub/sub and app engine http target types. \n\nThe answer is B."},{"upvote_count":"2","comments":[{"timestamp":"1634501940.0","poster":"kopper2019","comment_id":"337789","upvote_count":"1","content":"you install it in a Windows o linux machine for example and a cron job to run let's ray every 30 min \nhttps://cloud.google.com/storage/docs/gsutil/commands/cp"},{"upvote_count":"2","timestamp":"1632326880.0","poster":"ace_student","content":"Dataflow is stream and batch data processing. In this scenario we are not processing any data, we are just uploading images to GCS. I would go for option C","comment_id":"317413"}],"content":"Just don't know how to use gsutil command to sync on premises storage and how they connect to each other (On premises storage and cloud storage.) B may be possible however need to verify.","timestamp":"1631829540.0","poster":"GCP_user","comment_id":"312825"},{"poster":"raman23125","comment_id":"310302","content":"answer C\ngsutil rsync + cron job","upvote_count":"1","timestamp":"1631591640.0"}],"answer_ET":"C","exam_id":1,"answer_description":"","question_images":[],"answer_images":[],"unix_timestamp":1615701240,"answer":"C","choices":{"A":"Create a Pub/Sub topic, and enable a Cloud Storage trigger for the Pub/Sub topic. Create an application that sends all medical images to the Pub/Sub topic.","C":"Create a script that uses the gsutil command line interface to synchronize the on-premises storage with Cloud Storage. Schedule the script as a cron job.","B":"Deploy a Dataflow job from the batch template, ג€Datastore to Cloud Storage.ג€ Schedule the batch job on the desired interval.","D":"In the Cloud Console, go to Cloud Storage. Upload the relevant images to the appropriate bucket."},"question_text":"You are working for a hospital that stores its medical images in an on-premises data room. The hospital wants to use Cloud Storage for archival storage of these images. The hospital wants an automated process to upload any new medical images to Cloud Storage. You need to design and implement a solution. What should you do?","topic":"1","answers_community":["C (94%)","6%"],"isMC":true,"timestamp":"2021-03-14 06:54:00","question_id":77,"url":"https://www.examtopics.com/discussions/google/view/47007-exam-associate-cloud-engineer-topic-1-question-168/"},{"id":"48I2yDPTEZ8AlttABCx0","answer_ET":"A","answer":"A","question_id":78,"choices":{"D":"Use the export logs API to provide the Admin Activity Audit Logs in the format they want.","C":"Assign the appropriate permissions, and then use Cloud Monitoring to review metrics.","A":"Turn on Data Access Logs for the buckets they want to audit, and then build a query in the log viewer that filters on Cloud Storage.","B":"Assign the appropriate permissions, and then create a Data Studio report on Admin Activity Audit Logs."},"answer_images":[],"exam_id":1,"question_text":"Your auditor wants to view your organization's use of data in Google Cloud. The auditor is most interested in auditing who accessed data in Cloud Storage buckets. You need to help the auditor access the data they need. What should you do?","answer_description":"","answers_community":["A (100%)"],"timestamp":"2021-03-14 10:52:00","isMC":true,"discussion":[{"poster":"iri_gcp","upvote_count":"41","comment_id":"310437","timestamp":"1615715520.0","content":"It should be A. \nData access log are not enabled by default due to the fact that it incurs costs. \nSo you need to enable it first. \nAnd then you can filter it in the log viewer"},{"upvote_count":"11","content":"A. Turn on Data Access Logs for the buckets they want to audit, and then build a query in the log viewer that filters on Cloud Storage.","timestamp":"1615759920.0","poster":"GCP_Student1","comment_id":"310965"},{"comment_id":"1288894","upvote_count":"1","content":"Once again wrong answer. It should be option A","timestamp":"1727242140.0","poster":"Enamfrancis"},{"comment_id":"1174485","upvote_count":"1","poster":"pzacariasf7","content":"Selected Answer: A\nA. Turn on Data Access Logs for the buckets they want to audit, and then build a query in the log viewer that filters on Cloud Storage.","timestamp":"1710534780.0"},{"comment_id":"1000391","upvote_count":"1","content":"IF Data Access Logs had ALREADY been enabled, then option B would be a good answer\nReason - (1) best practice for cloud auditing - enable Admin Activity audit logs, then set IAM permissions\n(ref: https://cloud.google.com/logging/docs/audit/best-practices)\nand (2) Create a Data Studio (now renamed to Looker) report on Admin Activity Audit Logs\n(ref: https://cloud.google.com/looker/docs/looker-core-audit-logging)\nBut you cannot assume from the question that Data Access Logs are enabled (NB: they are NOT by default)","timestamp":"1693988760.0","poster":"NoCrapEva"},{"timestamp":"1693924860.0","upvote_count":"1","content":"Selected Answer: A\nA is the right answer as first we need to turn on the data access logs","poster":"Captain1212","comment_id":"999612"},{"comment_id":"715979","upvote_count":"1","poster":"anolive","content":"I have doubts about the answer A, the auditor wants to see the audit logs, and in this answer it is not explicit if he will be allowed to see it.","timestamp":"1668164520.0"},{"comment_id":"690033","upvote_count":"2","timestamp":"1665308040.0","content":"Selected Answer: A\nA is the correct answer,\nSince the auditor wants to know who accessed the cloud storage data, we need data acces logs for cloud storage.\n\nTypes of audit logs\nCloud Audit Logs provides the following audit logs for each Cloud project, folder, and organization:\n\nAdmin Activity audit logs\nData Access audit logs\nSystem Event audit logs\nPolicy Denied audit logs\n\n***Data Access audit logs contain API calls that read the configuration or metadata of resources, as well as user-driven API calls that create, modify, or read user-provided resource data.\n\nhttps://cloud.google.com/logging/docs/audit#types","poster":"Charumathi"},{"upvote_count":"1","poster":"AzureDP900","content":"A is right","comment_id":"621589","timestamp":"1656067380.0"},{"timestamp":"1655784660.0","poster":"Jman007","comment_id":"619595","content":"Selected Answer: A\nquestion says auditor is most interested in who accessED data in Cloud Storage. im not sure how auditoring is done for those who answered A but this means they want the logs for past users who accessed the data from a sepecified time. Turning on the feature now is kind of too late. poorly written question and answers. No point in an auditor coming in and giving the company all the exact questions they are going to ask and come back and ask them in a few months time. A seems like the better choices though","upvote_count":"6"},{"timestamp":"1653396780.0","poster":"akshaychavan7","comment_id":"606716","upvote_count":"2","content":"If it's A then how will we assign the permission for the auditor to view the logs?\nI had chosen option A on the first place, but later changed it considering that the auditor won't have the access to view the logs."},{"poster":"peugeotdude","upvote_count":"1","content":"Selected Answer: A\nBased on how I read the question- \nWe want Data Access log, not Admin Activity Audit Logs.","comment_id":"596439","timestamp":"1651601820.0"},{"content":"Selected Answer: A\nData access log are not enabled by default due to the fact that it incurs costs.\nSo you need to enable it first.\nAnd then you can filter it in the log viewer","poster":"somenick","timestamp":"1648220940.0","upvote_count":"1","comment_id":"575107"},{"poster":"[Removed]","timestamp":"1647370560.0","content":"https://cloud.google.com/logging/docs/audit#data-access\n\nCloud Storage: When Cloud Storage usage logs are enabled, Cloud Storage writes usage data to the Cloud Storage bucket, which generates Data Access audit logs for the bucket. The generated Data Access audit log has its caller identity redacted.","upvote_count":"1","comment_id":"568602"},{"comment_id":"553368","content":"Selected Answer: A\nThe majority vote here is A, despite some confusion around the wording of the question. I tend to agree because it's the solution that most closely reflects the requirements of the question (buckets, cloud storage).","poster":"DaveNZ","timestamp":"1645493340.0","upvote_count":"1"},{"timestamp":"1639072260.0","upvote_count":"3","comments":[{"content":"Actually, there is a different service named User Logs that permits to focus on a single bucket.\nRefer to google page:\nhttps://cloud.google.com/storage/docs/access-logs\nUsage logs provide information for all of the requests made on a specified bucket","poster":"MarcoDipa","upvote_count":"1","timestamp":"1639418940.0","comment_id":"500800"},{"comment_id":"557921","content":"The question just says \"buckets\" and hints that the audit should cover all org data, so I don't think there is any need to overanalyse, you are correct in choosing A","poster":"obeythefist","upvote_count":"1","timestamp":"1646032500.0"}],"comment_id":"497946","poster":"wh1t4k3r","content":"A. I could not find a way to enable audit logs in specific buckets, only on the whole storage level:\nhttps://cloud.google.com/logging/docs/audit/services\n\nB. Admin activity audit logs cover admin actions, such as metada or config changes:\nhttps://cloud.google.com/logging/docs/audit#admin-activity\n\nC. Cloud monitoring is not for auditing: https://cloud.google.com/monitoring\n\nD. Again, Admin Activity Audit Logs should not be used to audit data access, specially from bukets.\n\nMy conclusion: all these answers are wrong. My assumption: A is badly written. Specific buckets were not to be mentioned. I Vote A, but i think this Q&A is messed up. Maybe a correction? or deletion."},{"poster":"ericyev","comment_id":"496208","content":"I choose D. reason is here: Cloud Audit Logs generates the following audit logs for operations in Cloud Storage:\n\nAdmin Activity logs: Entries for operations that modify the configuration or metadata of a project, bucket, or object.\n\nData Access logs: Entries for operations that modify objects or read a project, bucket, or object. There are several sub-types of data access logs:\n\nADMIN_READ: Entries for operations that read the configuration or metadata of a project, bucket, or object.\n\nDATA_READ: Entries for operations that read an object.\n\nDATA_WRITE: Entries for operations that create or modify an object.","timestamp":"1638897780.0","upvote_count":"4"},{"poster":"kimharsh","timestamp":"1638454860.0","upvote_count":"1","content":"Also A because it's the only one that mention DATA ACCESS LOGS, which is the one that Logs objects access , t\n\nAdmin Activity logs: Entries for operations that modify the configuration or metadata of a project, bucket, or object.\n\nData Access logs: Entries for operations that modify objects or read a project, bucket, or object. There are several sub-types of data access logs:\n\nADMIN_READ: Entries for operations that read the configuration or metadata of a project, bucket, or object.\n\nDATA_READ: Entries for operations that read an object.\n\nDATA_WRITE: Entries for operations that create or modify an object.\n\n\nhttps://cloud.google.com/storage/docs/audit-logging","comment_id":"492580"},{"upvote_count":"1","timestamp":"1634274480.0","poster":"jackwillis","content":"Question is about user activities log not about Data Access log.","comment_id":"462422"},{"poster":"ankatsu2010","comment_id":"461403","timestamp":"1634108160.0","content":"Looks like there is no correct answer in this question. Because you can't turn on Data Access Logs for the specific buckets independently. What should you do???","upvote_count":"1"},{"comments":[{"content":"from the link, it's the Data Access logs that contain \"Entries for operations that read the configuration or metadata of a project, bucket, or object.\" and \"Entries for operations that read an object.\"\nAdmin Activity logs do not have these.","upvote_count":"4","timestamp":"1624717200.0","comment_id":"391313","poster":"JieHeng"}],"timestamp":"1622601960.0","content":"based on the https://cloud.google.com/storage/docs/audit-logging link, Answer is D.","poster":"learnazureportal","upvote_count":"4","comment_id":"372319"},{"comment_id":"362236","upvote_count":"1","timestamp":"1621517520.0","content":"A is correct option","poster":"arsh1916"},{"timestamp":"1619259720.0","content":"Ans: A Turn on Data Access Logs for the buckets they want to audit, and then build a query in the log viewer that filters on Cloud Storage.","comment_id":"341971","poster":"jahnu","upvote_count":"2"},{"timestamp":"1618154100.0","content":"A is the correct option","comment_id":"333382","upvote_count":"1","poster":"mj98"},{"content":"Excuse me Should be D","poster":"relaxm","comments":[{"upvote_count":"2","comment_id":"325794","timestamp":"1617279540.0","poster":"tavva_prudhvi","content":"Initially, you mentioned it B, I don't know why and now you say it's D again. it would be better if you mention few supporting articles w.r.t the option you choose. Then, we can discuss it."},{"timestamp":"1618062660.0","poster":"mj98","upvote_count":"1","content":"Wrong. It's A","comment_id":"332584"},{"comment_id":"395582","content":"you are excused, but still answer is A","timestamp":"1625118600.0","poster":"GCPACE2020","upvote_count":"6"}],"timestamp":"1616990700.0","upvote_count":"2","comment_id":"323093"},{"poster":"relaxm","content":"In my opinion B","timestamp":"1616990640.0","comment_id":"323091","upvote_count":"2"},{"poster":"GCP_user","upvote_count":"1","content":"Agree with option: A","timestamp":"1615939800.0","comment_id":"312834"},{"timestamp":"1615752840.0","upvote_count":"1","poster":"TAvenger","content":"\"A\"\nThis is the only option that allows to read Data access logs","comment_id":"310841"},{"upvote_count":"1","content":"I'd go for A","comment_id":"310497","timestamp":"1615722540.0","poster":"JDoutthere"}],"url":"https://www.examtopics.com/discussions/google/view/47038-exam-associate-cloud-engineer-topic-1-question-169/","question_images":[],"unix_timestamp":1615715520,"topic":"1"},{"id":"UMWaASZMebjMuCuGr14f","question_text":"You are analyzing Google Cloud Platform service costs from three separate projects. You want to use this information to create service cost estimates by service type, daily and monthly, for the next six months using standard query syntax. What should you do?","url":"https://www.examtopics.com/discussions/google/view/15898-exam-associate-cloud-engineer-topic-1-question-17-discussion/","discussion":[{"comments":[{"comment_id":"639820","upvote_count":"16","content":"Cloud billing data can only be exported to a JSON local file and to Bigquery. So, using Cloud Storage to export cloud billing data is not possible to do.\n\nhttps://cloud.google.com/billing/docs/how-to/export-data-bigquery","poster":"ryumada","timestamp":"1675120860.0"}],"upvote_count":"110","poster":"mohdafiuddin","comment_id":"253637","timestamp":"1624822260.0","content":"Solving this by first eliminating the options that don't suit us. By breaking down the question into the key requirements-\n\n1. Analyzing Google Cloud Platform service costs from three separate projects. \n2. Using standard query syntax. -> (Relational data and SQL)\n\nA. 'Cloud Storage bucket'........'Cloud Bigtable'. Not feasible, mainly because cloud BigTable is not good for Structured Data (or Relational Data on which we can run SQL queries as per the question's requirements). BigTable is better suited for Semi Structured data and NoSQL data.\nB. 'Cloud Storage bucket'.....'Google Sheets'. Not Feasible because there is no use of SQL in this option, which is one of the requirements.\nC. Local file, external tools... this is automatically eliminated because the operation we need is simple, and there has to be a GCP native solution for this. We shouldn't need to rely on going out of the cloud for such a simple thing.\nD. 'BigQuery'.....'SQL queries' -> This is the right answer."},{"upvote_count":"18","timestamp":"1599615780.0","comments":[{"content":"the key is standard query syntax","poster":"yurstev","comment_id":"251304","upvote_count":"4","timestamp":"1624480380.0"}],"content":"Agreed, BigQuery","poster":"cesar7816","comment_id":"60922"},{"poster":"Raghav2650","comment_id":"1400512","upvote_count":"1","timestamp":"1742386500.0","content":"Selected Answer: D\nYou need a scalable and efficient way to query cost data across multiple projects.\nBigQuery is the best choice because it supports large-scale data analysis using SQL.\n\nBigQuery allows you to run time-based SQL queries to analyze trends and forecast future costs.\nou can write queries using window functions (e.g., DATE_TRUNC(), INTERVAL) to break costs down by day or month."},{"comment_id":"1136994","poster":"SAMBIT","upvote_count":"1","timestamp":"1722440340.0","content":"Try it here: https://lookerstudio.google.com/reporting/0B7GT7ZlyzUmCZHFhNDlKVENHYmc/page/tLtE"},{"upvote_count":"2","comment_id":"1063315","content":"The correct answer is D","timestamp":"1714938180.0","poster":"BAofBK"},{"upvote_count":"1","poster":"Evan7557","content":"D is the Answer","comment_id":"1040420","timestamp":"1712826900.0"},{"comment_id":"1016956","content":"Selected Answer: D\nThe correct answer is D. \n\nBigQuery is a fully-managed, petabyte-scale analytics data warehouse that enables businesses to analyze all their data very quickly. It is also a good choice for analyzing cost data because it can handle large amounts of data and can perform complex queries quickly.\n\nTime window-based SQL queries allow you to analyze data over a specific time period. For example, you could write a query to calculate the total cost of a particular service for each day of the month.\n\nHere is an example of a BigQuery query that you could use to calculate the total cost of a particular service for each day of the month:\nsql\nSELECT\n cost,\n DATE(usage_start_time) AS date\nFROM\n `[PROJECT_ID].billing.dataset`\nWHERE\n service_id = 'YOUR_SERVICE_ID'\nGROUP BY\n date\nORDER BY\n date\nThis query will return a table with two columns: `cost` and `date`. The `cost` column will contain the total cost of the service for each day of the month. The `date` column will contain the date for each row.","timestamp":"1711389360.0","poster":"YourCloudGuru","upvote_count":"2"},{"content":"yes D, is the correct answer, because we only use bigquery for semi or structured data","comment_id":"996188","timestamp":"1709314620.0","poster":"Captain1212","upvote_count":"1"},{"comment_id":"859856","content":"Selected Answer: D\nIn GCP, Always use Big Query to export Billing. And Big Query is best for Analyzing","timestamp":"1696332840.0","poster":"Ashish_Tayal","upvote_count":"3"},{"comment_id":"846958","poster":"Partha117","content":"Selected Answer: D\nBig Query will allow sql analysis","timestamp":"1695372900.0","upvote_count":"2"},{"poster":"Buruguduystunstugudunstuy","comment_id":"761511","upvote_count":"5","timestamp":"1688068140.0","content":"Selected Answer: D\nThe correct answer is Option D. Exporting the bill to a BigQuery dataset allows you to use SQL queries to analyze the data and create service cost estimates by service type, daily and monthly, for the next six months. This is an efficient and effective way to analyze the data, especially if you are familiar with SQL syntax. \n\nOption A, importing the bill into Cloud Bigtable, may be more complex and may not offer the same level of flexibility as using SQL queries in BigQuery. \n\nOption B, importing the bill into Google Sheets, may be more suitable for simple analysis, but may not be as efficient for more complex analysis. \n\nOption C, exporting the transactions to a local file and using a desktop tool, may not be as efficient or effective as using a cloud-based solution like BigQuery.\n\nhttps://cloud.google.com/billing/docs/how-to/export-data-bigquery\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/"},{"comment_id":"712227","content":"Selected Answer: D\nAgreed, BigQuery","upvote_count":"1","timestamp":"1683358440.0","poster":"Kopy"},{"upvote_count":"1","comment_id":"702010","poster":"leogor","timestamp":"1682238060.0","content":"D, using standard query syntax so use BQ"},{"upvote_count":"1","comment_id":"633918","timestamp":"1674206880.0","poster":"Haarish","content":"Agreed, BigQuery \nAns: D"},{"comment_id":"625953","timestamp":"1672644660.0","poster":"RanjithK","content":"Answer is D","upvote_count":"1"},{"poster":"AzureDP900","timestamp":"1671749340.0","upvote_count":"1","comment_id":"620625","content":"D is the right answer."},{"content":"I agreed, BigQuery Answer D is correct","timestamp":"1670891100.0","upvote_count":"1","poster":"denkyira","comment_id":"615518"},{"content":"answer is Option D","timestamp":"1669216680.0","comment_id":"606107","poster":"jagan_cloud","upvote_count":"1"},{"poster":"haroldbenites","content":"Go for D","timestamp":"1669117200.0","upvote_count":"1","comment_id":"605323"},{"content":"Agreed , answer is D","poster":"LaxmanTiwari","upvote_count":"1","comment_id":"602191","timestamp":"1668536940.0"},{"comment_id":"481457","timestamp":"1652938740.0","content":"D. Export your bill to a BigQuery dataset, and then write time window-based SQL queries for analysis.","poster":"shawnkkk","upvote_count":"2"},{"comment_id":"481373","timestamp":"1652932500.0","poster":"vishnukumartr","content":"D. Export your bill to a BigQuery dataset, and then write time window-based SQL queries for analysis.","upvote_count":"1"},{"poster":"Jaira1256","timestamp":"1652903040.0","comment_id":"481081","content":"Selected Answer: D\nD is correct","upvote_count":"3"},{"comment_id":"426155","content":"agree D is the right choice.","poster":"Chotebhaisahab","timestamp":"1645088340.0","upvote_count":"1"},{"timestamp":"1643473200.0","comment_id":"416860","upvote_count":"3","content":"D is correct : \"...we recommend that you enable Cloud Billing data export to BigQuery at the same time that you create a Cloud Billing account. \"\nhttps://cloud.google.com/billing/docs/how-to/export-data-bigquery\nhttps://medium.com/google-cloud/analyzing-google-cloud-billing-data-with-big-query-30bae1c2aae4","poster":"YAS007"},{"timestamp":"1637831340.0","poster":"HogwartsTrue","comment_id":"366196","content":"Ans:D BigQuery is for Analytical Purpose","upvote_count":"1"},{"timestamp":"1636784220.0","content":"D as its talking about analysis, which can be performed by BigQuery.","comment_id":"355984","poster":"Finger41","upvote_count":"2"},{"content":"D is correct","upvote_count":"1","comment_id":"354249","timestamp":"1636603260.0","poster":"mcaromit"},{"content":"\"D\" is the right answer. Talking about analysis and SQL queries it clearly points to D.","comment_id":"348633","timestamp":"1635951780.0","poster":"yvinisiupacuando","upvote_count":"1"},{"timestamp":"1632492660.0","poster":"[Removed]","upvote_count":"1","content":"D is correct. Export your bill to a BigQuery dataset, and then write time window-based SQL queries for analysis.","comment_id":"319338"},{"comment_id":"305794","upvote_count":"1","content":"D.Export your bill to a BigQuery dataset, and then write time window-based SQL queries for analysis","timestamp":"1631108160.0","poster":"EABDAJA"},{"upvote_count":"1","comment_id":"302586","timestamp":"1630661460.0","poster":"Hi2ALL","content":"The given answer is acceptable"},{"timestamp":"1629408300.0","comment_id":"294670","upvote_count":"2","poster":"GCP_Student1","content":"D. Export your bill to a BigQuery dataset, and then write time window-based SQL queries for analysis."},{"timestamp":"1628956140.0","content":"D is the correct answer","poster":"iPablo26","comment_id":"290423","upvote_count":"1"},{"content":"D , Agreed, BigQuery","timestamp":"1627251900.0","poster":"INASR","upvote_count":"1","comment_id":"276441"},{"upvote_count":"1","timestamp":"1626292740.0","comment_id":"267457","content":"D is correct.","poster":"nherrerab"},{"upvote_count":"1","content":"D BigQuery","timestamp":"1619787120.0","poster":"hems4all","comment_id":"209358"},{"timestamp":"1617786540.0","poster":"glam","content":"D. Export your bill to a BigQuery dataset, and then write time window-based SQL queries for analysis.","upvote_count":"2","comment_id":"195046"},{"poster":"prasanu","comment_id":"194254","timestamp":"1617707820.0","content":"Agreed D","upvote_count":"1"},{"content":"D.\nBigQuery is the only option that allows SQL.","upvote_count":"3","poster":"JJ_ME","comment_id":"193795","timestamp":"1617659460.0"},{"comment_id":"119804","timestamp":"1608933360.0","content":"Ans is D\n\nBigQuery","poster":"professor","upvote_count":"2"},{"upvote_count":"3","poster":"Bharathy","timestamp":"1605836880.0","comment_id":"92446","content":"Bills can be exported to Storage/BigQuery for analysis. To support standard SQL queries, BigQuery is the option ! Hence Option D is correct..."},{"content":"Correct answer is D as BigQuery provides an ideal storage option to store and query in standard SQL dialect.BigQuery, Google's serverless, highly scalable enterprise data warehouse.","poster":"YashBindlish","timestamp":"1604467440.0","comment_id":"83376","upvote_count":"4"},{"content":"D is correct","poster":"Agents89","timestamp":"1603246800.0","upvote_count":"2","comment_id":"77238"},{"poster":"karol_wu","timestamp":"1600162080.0","upvote_count":"2","comment_id":"64228","content":"agreed"}],"exam_id":1,"answer_ET":"D","question_images":[],"answer":"D","choices":{"B":"Export your bill to a Cloud Storage bucket, and then import into Google Sheets for analysis.","A":"Export your bill to a Cloud Storage bucket, and then import into Cloud Bigtable for analysis.","D":"Export your bill to a BigQuery dataset, and then write time window-based SQL queries for analysis.","C":"Export your transactions to a local file, and perform analysis with a desktop tool."},"topic":"1","answer_images":[],"unix_timestamp":1583725380,"timestamp":"2020-03-09 04:43:00","answer_description":"","isMC":true,"answers_community":["D (100%)"],"question_id":79},{"id":"jpwFAB1AsmWQjjwIl4Ij","question_images":[],"answers_community":["B (80%)","A (20%)"],"answer":"B","choices":{"B":"Use the command gcloud auth activate-service-account and point it to the private key.","A":"Use the command gcloud auth login and point it to the private key.","C":"Place the private key file in the installation directory of the Cloud SDK and rename it to ג€credentials.jsonג€.","D":"Place the private key file in your home directory and rename it to ג€GOOGLE_APPLICATION_CREDENTIALSג€."},"answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/46456-exam-associate-cloud-engineer-topic-1-question-170/","timestamp":"2021-03-11 17:35:00","discussion":[{"comment_id":"310990","content":"B. Use the command gcloud auth activate-service-account and point it to the private key.\n\nAuthorizing with a service account\ngcloud auth activate-service-account authorizes access using a service account. As with gcloud init and gcloud auth login, this command saves the service account credentials to the local system on successful completion and sets the specified account as the active account in your Cloud SDK configuration.\n\nhttps://cloud.google.com/sdk/docs/authorizing#authorizing_with_a_service_account","timestamp":"1615762980.0","poster":"GCP_Student1","upvote_count":"44"},{"poster":"TAvenger","upvote_count":"20","comment_id":"310848","comments":[{"timestamp":"1676727180.0","poster":"eBooKz","comment_id":"813008","content":"See below information suggesting that service account can be used to authorize with the command \"gcloud auth login\". Not sure if this is a recent update:\n\n\"The gcloud auth login command authorizes access by using workload identity federation, which provides access to external workloads, or by using a service account key.\"\n\n\"To activate your service account, run gcloud auth login with the --cred-file flag:\n\n\ngcloud auth login --cred-file=CONFIGURATION_OR_KEY_FILE\nReplace CONFIGURATION_OR_KEY_FILE with the path to one of the following:\n\nA credential configuration file for workload identity federation\nA service account key file\"\n\nhttps://cloud.google.com/sdk/docs/authorizing#authorize_with_a_service_account","comments":[{"comment_id":"1000805","upvote_count":"1","timestamp":"1694017620.0","poster":"itsimranmalik","content":"As per google - gcloud auth activate-service-account serves the same function as gcloud auth login but uses a service account rather than Google user credentials.\n\nRef: https://cloud.google.com/sdk/gcloud/reference/auth/activate-service-account"}],"upvote_count":"1"}],"timestamp":"1615753320.0","content":"B.\ngcloud auth activate-service-account --help\n\nNAME)\n gcloud auth activate-service-account - authorize access to Google Cloud\n Platform with a service account\n\nSYNOPSIS\n gcloud auth activate-service-account [ACCOUNT] --key-file=KEY_FILE\n [--password-file=PASSWORD_FILE | --prompt-for-password]\n [GCLOUD_WIDE_FLAG ...]\n\nDESCRIPTION\n To allow gcloud (and other tools in Cloud SDK) to use service account\n credentials to make requests, use this command to import these credentials\n from a file that contains a private authorization key, and activate them\n for use in gcloud. gcloud auth activate-service-account serves the same\n function as gcloud auth login but uses a service account rather than Google\n user credentials."},{"content":"Selected Answer: A\nA works \nhttps://cloud.google.com/sdk/docs/authorizing#auth-login\nCheck this sub menu: \"Authorize a service account using a service account key\"","comments":[{"comment_id":"1303704","upvote_count":"2","timestamp":"1730053800.0","content":"Correction, B also works,\nBut for the question at hand, B seems to be more relatable. Question seems to be confusing.","poster":"yomi95"}],"timestamp":"1730053620.0","poster":"yomi95","upvote_count":"1","comment_id":"1303702"},{"poster":"denno22","comments":[{"content":"Now, I see that while A works, B is a better answer.","upvote_count":"1","comment_id":"1295491","poster":"denno22","timestamp":"1728549600.0"}],"content":"Selected Answer: A\nAuthorize with a service account\nThe gcloud auth login command can authorize access with a service account by using a credential file stored on your local file system. This credential can be a user credential with permission to impersonate the service account, a credential configuration file for workload identity federation, or a service account key.\n\nhttps://cloud.google.com/sdk/docs/authorizing#auth-login","timestamp":"1727876520.0","upvote_count":"1","comment_id":"1292433"},{"timestamp":"1707411180.0","poster":"blackBeard33","upvote_count":"2","content":"Selected Answer: B\nThe Answer is A. The command to use service account for authentication is precisely gcloud auth activate-service-account where you can point out to the key file using the flag --key-file.\n\nhttps://cloud.google.com/sdk/gcloud/reference/auth/activate-service-account","comment_id":"1144734"},{"upvote_count":"1","content":"Selected Answer: B\n• The command syntax is gcloud auth activate-service-account --key-file=PATH_TO_KEY_FILE, where PATH_TO_KEY_FILE is the path to the JSON file containing the service account's private key.","timestamp":"1704161160.0","poster":"Cynthia2023","comment_id":"1111550"},{"comment_id":"1000934","upvote_count":"2","timestamp":"1694028780.0","content":"Selected Answer: B\nhttps://cloud.google.com/sdk/gcloud/reference/auth/activate-service-account","poster":"scanner2"},{"timestamp":"1683780420.0","poster":"N_A","upvote_count":"3","comment_id":"894574","content":"D. This method is for application default credentials. See: https://cloud.google.com/docs/authentication/application-default-credentials\nA. This method is to obtain credentials for a user account.\nC. This does nothing. Useless.\nB. Is the correct answer. See: https://cloud.google.com/sdk/gcloud/reference/auth/activate-service-account"},{"timestamp":"1680603420.0","poster":"Abhi00754","content":"Selected Answer: B\nhttps://cloud.google.com/sdk/gcloud/reference/auth/activate-service-account \nB","comment_id":"860876","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: B\nUse the command gcloud auth activate-service-account and point it to the private key","timestamp":"1659747540.0","comment_id":"643142","poster":"abirroy"},{"comment_id":"625923","content":"ANswer is B\nTo activate your service account, run gcloud auth activate-service-account:\ngcloud auth activate-service-account [ACCOUNT] --key-file=[KEY_FILE]","timestamp":"1656734940.0","poster":"skrjha20","upvote_count":"1"},{"poster":"AzureDP900","content":"I will go with B","comment_id":"621590","upvote_count":"1","timestamp":"1656067500.0"},{"poster":"AzureDP900","comment_id":"615960","content":"B is right Please refer \nhttps://cloud.google.com/storage/docs/authentication","upvote_count":"1","timestamp":"1655156340.0"},{"content":"Selected Answer: B\nAns: B","timestamp":"1647850380.0","poster":"Rukman","upvote_count":"1","comment_id":"572027"},{"upvote_count":"5","poster":"KunK","comment_id":"451395","content":"B, really straightforward","timestamp":"1632568380.0"},{"comment_id":"308071","poster":"ravixkumar","upvote_count":"11","timestamp":"1615480500.0","content":"Ans : B\ngcloud auth activate-service-account --key-file=/test-service-account.json"}],"answer_ET":"B","isMC":true,"exam_id":1,"question_text":"You received a JSON file that contained a private key of a Service Account in order to get access to several resources in a Google Cloud project. You downloaded and installed the Cloud SDK and want to use this private key for authentication and authorization when performing gcloud commands. What should you do?","answer_description":"","topic":"1","question_id":80,"unix_timestamp":1615480500}],"exam":{"name":"Associate Cloud Engineer","isMCOnly":true,"numberOfQuestions":285,"id":1,"isImplemented":true,"provider":"Google","lastUpdated":"11 Apr 2025","isBeta":false},"currentPage":16},"__N_SSP":true}