{"pageProps":{"questions":[{"id":"WXkdsaRZuulPQcpAUVhg","answer_ET":"A","answer":"A","answers_community":["A (89%)","7%"],"question_text":"A production database virtual machine on Google Compute Engine has an ext4-formatted persistent disk for data files. The database is about to run out of storage space.\nHow can you remediate the problem with the least amount of downtime?","answer_images":[],"exam_id":4,"isMC":true,"topic":"1","url":"https://www.examtopics.com/discussions/google/view/7142-exam-professional-cloud-architect-topic-1-question-14/","timestamp":"2019-10-24 16:43:00","discussion":[{"upvote_count":"28","comment_id":"23696","poster":"TosO","content":"A is the correct answer because the question says \"with minimum downtime\"","timestamp":"1574450520.0"},{"comment_id":"30448","upvote_count":"14","poster":"passnow","timestamp":"1576613640.0","content":"least amount of downtime? is the sugar word. You miss that you miss all. Everything there is correct but I believe its only A that fits that requirement","comments":[{"poster":"raj117","upvote_count":"1","timestamp":"1617792180.0","content":"but in option A, nowhere it is mentioned to shut down the VM.","comments":[{"poster":"monkeym","timestamp":"1626354120.0","content":"No need to reboot.","comment_id":"407103","upvote_count":"1"}],"comment_id":"330259"}]},{"comment_id":"1309620","upvote_count":"3","timestamp":"1731270960.0","poster":"Ekramy_Elnaggar","content":"Selected Answer: A\n1. Online Resizing: Google Cloud Platform allows you to increase the size of a persistent disk while it's attached to a running VM. This means you don't need to shut down the database server.\n2. resize2fs: This Linux command extends the file system to utilize the newly added space on the disk. It can be run while the file system is mounted, minimizing downtime."},{"timestamp":"1730358840.0","comment_id":"1305309","poster":"SerGCP","content":"Selected Answer: C\nA) can not work becouse you must extend partition and after than you can extend filesystem \nC) ok becouse you can use fdisk in linux to extend partition and after than you can resize ext4 filesystem","upvote_count":"1"},{"poster":"Nora9","content":"Selected Answer: A\nA is the right answer.You should resize the disk, take a snapshot, then resize the filesystem and partitions (eg.) ext4, xfs etc.","upvote_count":"1","comment_id":"1064947","timestamp":"1699371420.0"},{"poster":"AdityaGupta","content":"Selected Answer: A\nUnlike Azure, in google you can dynamically resize the persistent disk while VM is running. This narrows down the option to A or C. Since the question says \"ext4-formatted persistent disk\", we need to choose correct command (resize2fs or fdisk ) for Linux for resizing ext4 file format disk. To resize an ext4 file system in Linux, you can use the resize2fs command. FDISK to manipulate partition tables in Linux.","upvote_count":"8","comment_id":"1024414","timestamp":"1696391880.0"},{"upvote_count":"4","comment_id":"1000141","content":"Selected Answer: A\nAccording to the right url, A is the right answer. https://cloud.google.com/compute/docs/disks/resize-persistent-disk","poster":"blackhawk86","timestamp":"1693972080.0"},{"upvote_count":"1","timestamp":"1693971960.0","poster":"blackhawk86","content":"The right URL for the oficial document is, https://cloud.google.com/compute/docs/disks/resize-persistent-disk","comment_id":"1000139"},{"comment_id":"987565","timestamp":"1692718860.0","content":"A is no brainer","poster":"heretolearnazure","upvote_count":"1"},{"comment_id":"979286","content":"Selected Answer: E\nE is the correct because is true you need minimum downtime but in Production a backup is a must.","poster":"jalberto","timestamp":"1691832780.0","upvote_count":"2"},{"content":"E because you are in Production, and you need a backup","comment_id":"979284","upvote_count":"1","poster":"jalberto","timestamp":"1691832720.0"},{"poster":"alekonko","upvote_count":"3","timestamp":"1679520660.0","content":"Selected Answer: A\nA is correct, resize disk don't required reboot or downtime\nhttps://cloud.google.com/compute/docs/disks/resize-persistent-disk","comment_id":"847539"},{"poster":"omermahgoub","timestamp":"1671524700.0","content":"A: Increasing the size of the persistent disk in the Cloud Platform Console and using the resize2fs command in Linux.\n\nIncreasing the size of the persistent disk can be done without requiring the virtual machine to be shut down, and the resize2fs command can be used to resize the ext4 filesystem on the disk to take advantage of the additional space. This will allow you to add more storage space to the virtual machine without disrupting the database service.","comment_id":"750656","upvote_count":"1"},{"content":"Selected Answer: A\nA is ok","timestamp":"1668073320.0","comment_id":"715105","upvote_count":"2","poster":"megumin"},{"content":"Selected Answer: A\nA https://cloud.google.com/compute/docs/disks/resize-persistent-disk?_ga=2.233866652.-3622898.1631303718","upvote_count":"1","comment_id":"701679","timestamp":"1666458720.0","poster":"Mahmoud_E"},{"upvote_count":"1","timestamp":"1665638580.0","comment_id":"693630","poster":"minmin2020","content":"Selected Answer: A\nA. In the Cloud Platform Console, increase the size of the persistent disk and use the resize2fs command in Linux.","comments":[{"content":"yes, A is right","poster":"AzureDP900","comment_id":"696509","timestamp":"1665950820.0","upvote_count":"1"}]},{"poster":"holerina","content":"A resize the disk standard command","comment_id":"674165","upvote_count":"1","timestamp":"1663678080.0"},{"poster":"abirroy","content":"Selected Answer: A\nIn the Cloud Platform Console, increase the size of the persistent disk and use the resize2fs command in Linux.","timestamp":"1663200000.0","comment_id":"669396","upvote_count":"1"},{"upvote_count":"1","content":"A is correct","timestamp":"1661338620.0","poster":"Kubernetes","comment_id":"651193"},{"comment_id":"640015","poster":"sgofficial","content":"A is correct answer\nhttps://cloud.google.com/compute/docs/disks/resize-persistent-disk?_ga=2.233866652.-3622898.1631303718","upvote_count":"1","timestamp":"1659258480.0"},{"content":"The resize2fs command is used to enlarge or shrink an ext2/3/4 file system on a device. You can enlarge a mounted file system, but you must unmount the file system before you can shrink it. You can specify the desired size of the file system in order to either enlarge or shrink it.","timestamp":"1659007380.0","poster":"raaj_p","upvote_count":"4","comment_id":"638663"},{"poster":"belly265","timestamp":"1645327500.0","comment_id":"551504","content":"A is the correct answer because you can just resize it without any downtime","upvote_count":"3"},{"poster":"PhuocT","timestamp":"1640694720.0","content":"Selected Answer: A\nVote A","comment_id":"511086","upvote_count":"1"},{"poster":"vincy2202","content":"A is the correct answer","comment_id":"508417","timestamp":"1640339460.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1626832740.0","comment_id":"410604","content":"A is correct","poster":"rm_2495"},{"poster":"victory108","comment_id":"360238","timestamp":"1621325520.0","upvote_count":"1","content":"A. In the Cloud Platform Console, increase the size of the persistent disk and use the resize2fs command in Linux."},{"comment_id":"353884","upvote_count":"9","poster":"un","timestamp":"1620662340.0","content":"A is correct\nhttps://cloud.google.com/compute/docs/disks/working-with-persistent-disks#resize_pd"},{"timestamp":"1617080460.0","content":"Answer is A","upvote_count":"1","poster":"Ausias18","comment_id":"323979"},{"content":"A is ok.","timestamp":"1617078180.0","poster":"lynx256","comment_id":"323951","upvote_count":"1"},{"timestamp":"1611433200.0","content":"A is correct.\nhttps://cloud.google.com/compute/docs/disks/add-persistent-disk#resize_partitions","upvote_count":"1","comment_id":"274846","poster":"bnlcnd"},{"poster":"Arimaverick","upvote_count":"1","comments":[{"poster":"raj117","timestamp":"1617792240.0","upvote_count":"2","comments":[{"poster":"Jambalaja","upvote_count":"2","content":"No, if the disk is a persistant disk you dont need to shutdown the VM. You can change the size of a persistant disk dynamically. However, in this case, since the persistant disk is ext4 formatted you need to use the resize2fs command.","comment_id":"336026","timestamp":"1618469760.0"}],"content":"to increase disk from Google Console, do we need to shut down the VM?","comment_id":"330262"}],"comment_id":"260976","timestamp":"1609933920.0","content":"Yes A is correct as the google documentations says \"You can resize disks at any time, whether or not the disk is attached to a running instance.\n\nResizing a disk doesn't delete or modify disk data, but as a best practice, snapshot your disk before you make any changes......If you are using ext4, use the resize2fs command. If you grew a partition on your disk, specify the partition. \""},{"comment_id":"260741","upvote_count":"1","timestamp":"1609905240.0","poster":"dlzhang","content":"A is the right one."},{"content":"Answer is A:","poster":"nimso","comment_id":"206500","timestamp":"1603737360.0","upvote_count":"1"},{"upvote_count":"1","content":"I would go with A","timestamp":"1600940400.0","poster":"asheesh0574","comment_id":"186028"},{"comments":[{"content":"https://cloud.google.com/compute/docs/disks/working-with-persistent-disks#resize_pd This is the correct link, thank you for sharing where I found the above one.","comment_id":"330266","timestamp":"1617792480.0","poster":"raj117","upvote_count":"2"}],"timestamp":"1600095420.0","comment_id":"179357","poster":"AshokC","upvote_count":"1","content":"A -- https://cloud.google.com/compute/docs/disks/add-persistent-disk"},{"comment_id":"130592","timestamp":"1594297980.0","poster":"saurabh1805","content":"A only fit with requirement in question that is minimum downtime.","upvote_count":"2"},{"comment_id":"117065","poster":"mlantonis","timestamp":"1592890260.0","content":"A for sure, because the answer comes form Google itself.","upvote_count":"2"},{"comment_id":"108101","poster":"Pupina","timestamp":"1591910520.0","content":"A for sure","upvote_count":"2"},{"timestamp":"1591767360.0","upvote_count":"1","comment_id":"106499","poster":"gfhbox0083","content":"A, for sure."},{"content":"A is the correct answer","poster":"Nirms","upvote_count":"1","comment_id":"100842","timestamp":"1591103940.0"},{"timestamp":"1590645240.0","poster":"AD2AD4","upvote_count":"3","comments":[{"content":"A is the correct answer","comment_id":"98308","timestamp":"1590765240.0","poster":"Ziegler","upvote_count":"2"}],"comment_id":"97290","content":"Final Decision to go with Option A"},{"timestamp":"1590055800.0","comment_id":"93314","content":"A is correct.\nResources:\nhttps://cloud.google.com/compute/docs/disks/add-persistent-disk\nhttps://medium.com/google-cloud/resize-your-persist-disk-on-google-cloud-on-the-fly-b3491277b718","upvote_count":"3","poster":"misho"},{"comment_id":"86285","timestamp":"1589070360.0","upvote_count":"4","comments":[{"upvote_count":"1","timestamp":"1651056180.0","poster":"MQQ","comment_id":"593073","content":"fdisk is also a linux command"}],"poster":"clouddude","content":"I'll go with A.\nA seems reasonable because persistent disk space can be increased live and then Linux can be told to recognize the larger space.\nB does not seem reasonable because the system must be shut down.\nC does not seem reasonable because FDISK is a windows command.\nD does not seem reasonable because database services typically do not have the ability to move data.\nE does not seem reasonable because, even if this worked, it doesn't tell the OS how to recognize the additional space."},{"poster":"gcp_aws","upvote_count":"2","timestamp":"1588794540.0","content":"A is the correct answer","comment_id":"84704"},{"timestamp":"1584987900.0","upvote_count":"3","comment_id":"67336","poster":"desertlotus1211","content":"Answer is A:\n\nhttps://medium.com/google-cloud/resize-your-persist-disk-on-google-cloud-on-the-fly-b3491277b718"},{"timestamp":"1580389860.0","comment_id":"44702","upvote_count":"3","content":"answer: A","poster":"2g"},{"comment_id":"17205","content":"\"A\" The resize2fs documention explicity states that it can resize the ext4 file system, it is also recommended within Google's documentation. All \"B\", \"C\", \"D\" and \"E\" would work but \"B\" and \"E\" require downtown so they are not correct. \"C\" and \"D\" are both correct but \"D\" better describes \"C\". When choosing between between \"A\" and \"D\", \"D\" is more work than \"A\" but better organizes the data than \"A\" so \"D\" is correct.","timestamp":"1571928180.0","poster":"Eroc","comments":[{"comment_id":"151615","content":"A is ok","upvote_count":"4","poster":"tartar","timestamp":"1596679920.0"},{"upvote_count":"1","poster":"nitinz","timestamp":"1614868980.0","content":"A is the correct option.","comment_id":"303393"}],"upvote_count":"2"}],"answer_description":"","question_id":46,"question_images":[],"unix_timestamp":1571928180,"choices":{"D":"In the Cloud Platform Console, create a new persistent disk attached to the virtual machine, format and mount it, and configure the database service to move the files to the new disk","B":"Shut down the virtual machine, use the Cloud Platform Console to increase the persistent disk size, then restart the virtual machine","E":"In the Cloud Platform Console, create a snapshot of the persistent disk restore the snapshot to a new larger disk, unmount the old disk, mount the new disk and restart the database service","C":"In the Cloud Platform Console, increase the size of the persistent disk and verify the new space is ready to use with the fdisk command in Linux","A":"In the Cloud Platform Console, increase the size of the persistent disk and use the resize2fs command in Linux."}},{"id":"uHTJToYpHWRyJCM08oKP","timestamp":"2021-08-23 15:13:00","question_images":[],"exam_id":4,"question_id":47,"isMC":true,"discussion":[{"comments":[{"poster":"rishab86","upvote_count":"2","content":"D is correct !","timestamp":"1632240600.0","comment_id":"449003"},{"poster":"PATILDXB","content":"The provided link is not relevant to kubernetes, but pertains to cloud pub/sub....the num_undelivered_messages metric is not available for kubernetes autoscaling...C is correct","timestamp":"1669478700.0","comment_id":"727646","upvote_count":"2","comments":[{"timestamp":"1716613860.0","upvote_count":"1","comment_id":"1218000","content":"Kubernetes allow autoscaling based on external metrics","poster":"huuthanhdlv"}]},{"upvote_count":"4","comment_id":"786154","poster":"GopeshSahu","timestamp":"1674534240.0","content":"https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#pubsub"}],"timestamp":"1630781160.0","comment_id":"439319","upvote_count":"40","poster":"Rzla","content":"Answer is D. num_undelivered_messages metric can indicate if subscribers are keeping up with message submissions. \n https://cloud.google.com/pubsub/docs/monitoring#monitoring_the_backlog"},{"poster":"aut0pil0t","upvote_count":"23","content":"Selected Answer: D\nDirect answer - D\n\nhttps://cloud.google.com/kubernetes-engine/docs/samples/container-pubsub-horizontal-pod-autoscaler\n\napiVersion: autoscaling/v2beta2\nkind: HorizontalPodAutoscaler\nmetadata:\n name: pubsub\nspec:\n minReplicas: 1\n maxReplicas: 5\n metrics:\n - external:\n metric:\n name: pubsub.googleapis.com|subscription|num_undelivered_messages\n selector:\n matchLabels:\n resource.labels.subscription_id: echo-read\n target:\n type: AverageValue\n averageValue: 2\n type: External\n scaleTargetRef:\n apiVersion: apps/v1\n kind: Deployment\n name: pubsub","timestamp":"1662179640.0","comment_id":"658067"},{"content":"Selected Answer: D\nConfigure a Kubernetes autoscaling deployment based on the subscription/num_undelivered_messages metric","poster":"plumbig11","timestamp":"1735968060.0","upvote_count":"1","comment_id":"1336267"},{"comment_id":"1255920","timestamp":"1722038820.0","content":"Selected Answer: D\nSubscription Metric: Scaling based on the subscription/num_undelivered_messages metric directly ties the scaling behavior to the number of unprocessed messages in Pub/Sub. This ensures that your application scales out when there are more messages to process and scales in when the queue is short.\nRelevant Metric: This metric is relevant for an I/O-intensive application that processes messages from Pub/Sub, ensuring that the scaling is directly responsive to the message processing demand.","poster":"awsgcparch","upvote_count":"1"},{"timestamp":"1709564340.0","upvote_count":"1","comment_id":"1165733","content":"Selected Answer: D\nD is the correct answer: subscription/num_undelivered_messages directly indicates the number of messages waiting to be processed, making it a perfect indicator of the workload on the application.","poster":"mesodan"},{"comment_id":"1019608","content":"https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#pubsub\nhere . so this is D","upvote_count":"1","poster":"ductrinh","timestamp":"1695889140.0"},{"timestamp":"1694082540.0","content":"Option C (--enable-autoscaling flag for the entire cluster): Enabling autoscaling at the cluster level doesn't provide fine-grained control over scaling individual pods or deployments based on specific workload metrics.","upvote_count":"1","comment_id":"1001425","poster":"RaviRS"},{"comment_id":"996187","upvote_count":"2","poster":"Zahasan","content":"Can someone tell that these answers are right like people voted D but answer is C.","timestamp":"1693582560.0"},{"comment_id":"935915","timestamp":"1687910760.0","upvote_count":"2","poster":"sampon279","content":"Answer D"},{"timestamp":"1686397920.0","poster":"BiddlyBdoyng","comment_id":"920002","content":"B & C refer to using auto scaler with custom metrics but\n\n\"Cluster autoscaler makes these scaling decisions based on the resource requests (rather than actual resource utilization) of Pods running on that node pool's nodes. \"\n\nA makes no sense as it defines a min and max that we don't know\n\nD seems like part of the solution so D.","upvote_count":"1"},{"timestamp":"1681578360.0","upvote_count":"1","comment_id":"871144","content":"is D - https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#pubsub","poster":"natpilot"},{"timestamp":"1673518920.0","comments":[{"content":"The problems says PULL request and B is related to PUSH request. I do not think it is related.","poster":"gcppandit","timestamp":"1675198980.0","upvote_count":"4","comment_id":"794612"}],"comment_id":"773350","content":"Selected Answer: B\nI vot for B. Configure a Kubernetes autoscaling deployment based on the subscription/push_request_latencies metric. as the metric should be based on latency instead of num_undelivered_messages metric in D.","poster":"WFCheong","upvote_count":"2"},{"comment_id":"744995","upvote_count":"1","poster":"surajkrishnamurthy","content":"Selected Answer: D\nD is the correct answer","timestamp":"1671016020.0"},{"comment_id":"734450","content":"Selected Answer: D\nD is the correct one","poster":"ale_brd_111","timestamp":"1670071380.0","upvote_count":"1"},{"content":"Selected Answer: D\nD is ok","timestamp":"1668594180.0","comment_id":"719540","poster":"megumin","upvote_count":"1"},{"comment_id":"708978","poster":"newuser111","content":"Selected Answer: D\nD\n\nhttps://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#pubsub","upvote_count":"2","timestamp":"1667293020.0"},{"poster":"bossdellacert","comment_id":"651155","upvote_count":"2","timestamp":"1661330460.0","content":"Selected Answer: D\nThis seems relevant \nhttps://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#pubsub\neven if it uses Deployment + HorizontalPodAutoscaler which is not mentioned in the context of the question/answer"},{"timestamp":"1660648020.0","upvote_count":"2","comment_id":"647612","poster":"tycho","content":"I think it wrong for application to be in pod it should be a deployment, and deployment would scale"},{"comment_id":"626623","content":"Option D is most suitable for given requirement.","timestamp":"1656863340.0","comments":[{"comment_id":"626624","upvote_count":"1","poster":"AzureDP900","content":"https://cloud.google.com/pubsub/docs/monitoring#monitoring_subscriptions_with_filters","timestamp":"1656863340.0"}],"upvote_count":"2","poster":"AzureDP900"},{"upvote_count":"2","comment_id":"616700","timestamp":"1655290860.0","poster":"DP_GCP","content":"D is correct, please check below code:\ngcloud compute instance-groups managed set-autoscaling \\\n our-instance-group \\\n --zone=us-central1-a \\\n --max-num-replicas=100 \\\n --min-num-replicas=0 \\\n --update-stackdriver-metric=pubsub.googleapis.com/subscription/num_undelivered_messages \\\n --stackdriver-metric-filter=\"resource.type = pubsub_subscription AND resource.labels.subscription_id = our-subscription\" \\\n --stackdriver-metric-single-instance-assignment=15"},{"timestamp":"1652200200.0","poster":"ridyr","content":"Selected Answer: D\nI'm going with D, but not certain\nQuestion says; app pulls(subscription) messages. B is push metric?","upvote_count":"3","comment_id":"599696"},{"upvote_count":"3","poster":"wilwong","timestamp":"1650592380.0","comment_id":"589676","content":"Selected Answer: D\nIt's for io, not cpu, so A is wrong, not for push B is wrong, C can't fix the problem, Just D is correct."},{"comment_id":"521603","content":"D is ok.","upvote_count":"1","timestamp":"1641913920.0","poster":"user1324567"},{"timestamp":"1641842460.0","content":"https://cloud.google.com/kubernetes-engine/docs/samples/container-pubsub-horizontal-pod-autoscaler","upvote_count":"5","poster":"pddddd","comment_id":"521071"},{"upvote_count":"1","comment_id":"516237","content":"D is correct.\nB is a push_request_latencies metric how does that helps when the app \"pulls\" messages from Pub/Sub? The issue is It is not pulling fast enough from the pub/sub.","poster":"timotei","timestamp":"1641273120.0"},{"timestamp":"1640792820.0","upvote_count":"4","comment_id":"512352","poster":"ehgm","content":"Selected Answer: D\nC is wrong: Create a Kubernetes cluster with --enable-autoscaling flag don't scale the Pod. We need a Deployment."},{"timestamp":"1639814220.0","comment_id":"504081","poster":"JaSza80","upvote_count":"5","content":"Selected Answer: B\nQuestion is about long processing time, not undelivered messages, hence I vote for B : push_request_latencies\n\nsubscription/push_request_latencies:\nThis metric helps you understand your push endpoint's response latency distribution. Because of the limit on the number of outstanding messages, endpoint latency affects subscription throughput. If it takes 100 milliseconds to process each message, your throughput limit is likely to be 10 messages per second.\n\nhttps://cloud.google.com/pubsub/docs/monitoring#monitoring_exp"},{"poster":"rajadhav","comment_id":"501831","comments":[{"poster":"ehgm","content":"The question says: \"it was deployed as a single pod\". You cant scale a Pod using deployment syntax command line.","timestamp":"1640792700.0","upvote_count":"1","comment_id":"512345"}],"upvote_count":"2","content":"I am surprise why not option A?\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/scaling-apps","timestamp":"1639539060.0"},{"timestamp":"1639502100.0","poster":"PhilipKoku","upvote_count":"1","content":"Selected Answer: D\nD) Is the right answer https://cloud.google.com/pubsub/docs/monitoring#monitoring_the_backlog","comment_id":"501546"},{"poster":"vincy2202","upvote_count":"1","content":"D is the correct answer\nhttps://cloud.google.com/pubsub/docs/monitoring#monitoring_the_backlog","timestamp":"1638688860.0","comment_id":"494130"},{"comment_id":"492614","poster":"[Removed]","content":"Selected Answer: D\nMarked C is wrong.","timestamp":"1638457980.0","upvote_count":"1"},{"upvote_count":"3","content":"Selected Answer: D\nvote D","comment_id":"488201","timestamp":"1638023580.0","poster":"pakilodi"},{"comment_id":"482605","upvote_count":"1","timestamp":"1637418000.0","comments":[{"content":"but the answer mentions autoscale of deployment not cluster","upvote_count":"1","timestamp":"1638260820.0","poster":"Nimbus2021","comment_id":"490520"}],"content":"D - pub/sub num_undelivered_messages metric can be used to autoscale cluster HPA","poster":"TheCloudBoy77"},{"timestamp":"1633967400.0","content":"D\nhttps://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#pubsub_7","upvote_count":"4","comment_id":"460693","poster":"rottzy"},{"timestamp":"1632958260.0","poster":"rottzy","upvote_count":"1","content":"why not A ?","comment_id":"454552"},{"comment_id":"437377","poster":"sandipk91","upvote_count":"2","timestamp":"1630518420.0","content":"I think option A is the correct answer as Horizontal Pod Autoscaling is needed here to increase the application capacity"},{"upvote_count":"2","poster":"KPUSA","comment_id":"433954","timestamp":"1630160580.0","content":"https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics?hl=en#step4"},{"poster":"babuu2021","comment_id":"433100","content":"D is okay","timestamp":"1630069740.0","upvote_count":"7"},{"content":"Can anyone explain the answer. This is so confusing.","poster":"Nik22","comment_id":"432634","timestamp":"1630023540.0","upvote_count":"1"},{"comment_id":"431969","timestamp":"1629956160.0","content":"D is ok","comments":[{"comment_id":"442924","upvote_count":"1","content":"Can it be done?","comments":[{"content":"If it can be done will go with it.","upvote_count":"1","poster":"amxexam","comments":[{"comment_id":"443978","upvote_count":"1","content":"Why not? https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics?hl=en#step3 https://cloud.google.com/pubsub/docs/monitoring#monitoring_the_backlog","poster":"vladik820","timestamp":"1631535540.0"}],"comment_id":"442925","timestamp":"1631354640.0"}],"timestamp":"1631354580.0","poster":"amxexam"}],"poster":"vladik820","upvote_count":"2"},{"content":"https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#pubsub\nB is answer","poster":"meh_33","timestamp":"1629914700.0","comment_id":"431682","upvote_count":"3"},{"timestamp":"1629774300.0","comments":[{"comments":[{"comment_id":"438532","content":"What is correct answer","poster":"[Removed]","timestamp":"1630674480.0","upvote_count":"1"}],"poster":"VishalB","upvote_count":"1","content":"The application is already deployed with single pod, so this option is eliminated","comment_id":"435129","timestamp":"1630302720.0"}],"upvote_count":"1","content":"C is OK","poster":"SweetieS","comment_id":"430422"},{"timestamp":"1629724380.0","poster":"AWS26","content":"Ans is \nD","upvote_count":"3","comment_id":"429997","comments":[{"upvote_count":"6","comment_id":"435128","timestamp":"1630302660.0","content":"in above case their is latency, messaging are getting processed 1 min later so metrics \nname: pubsub.googleapis.com|subscription|num_undelivered_messages would not be usefull","poster":"VishalB"}]}],"answer_images":[],"answer":"D","choices":{"D":"Configure a Kubernetes autoscaling deployment based on the subscription/num_undelivered_messages metric.","C":"Use the --enable-autoscaling flag when you create the Kubernetes cluster.","B":"Configure a Kubernetes autoscaling deployment based on the subscription/push_request_latencies metric.","A":"Use kubectl autoscale deployment APP_NAME --max 6 --min 2 --cpu-percent 50 to configure Kubernetes autoscaling deployment."},"unix_timestamp":1629724380,"answer_description":"","answers_community":["D (87%)","13%"],"url":"https://www.examtopics.com/discussions/google/view/60396-exam-professional-cloud-architect-topic-1-question-140/","answer_ET":"D","topic":"1","question_text":"Your company has a Kubernetes application that pulls messages from Pub/Sub and stores them in Filestore. Because the application is simple, it was deployed as a single pod. The infrastructure team has analyzed Pub/Sub metrics and discovered that the application cannot process the messages in real time. Most of them wait for minutes before being processed. You need to scale the elaboration process that is I/O-intensive. What should you do?"},{"id":"3KKoI7PTyrpt4RVfCE9D","answers_community":["C (100%)"],"timestamp":"2021-08-26 10:08:00","answer_ET":"C","answer":"C","question_images":[],"question_text":"Your company is developing a web-based application. You need to make sure that production deployments are linked to source code commits and are fully auditable. What should you do?","isMC":true,"url":"https://www.examtopics.com/discussions/google/view/60698-exam-professional-cloud-architect-topic-1-question-141/","answer_description":"","exam_id":4,"answer_images":[],"question_id":48,"topic":"1","discussion":[{"upvote_count":"35","comment_id":"432061","poster":"djosani","comments":[{"content":"@Kopper2019- what do you think about ans C?","comment_id":"504452","upvote_count":"2","timestamp":"1639859580.0","poster":"Urban_Life"}],"timestamp":"1629965280.0","content":"Developer shouldn't tag or comment every commit with some specific data, like timestamps or something else. There might be an app version, but it's not mentioned. I'd go with C as it's an automated, error-less approach that answers the question."},{"content":"C. Make the container tag match the source code commit hash.","poster":"victory108","comments":[{"comment_id":"442931","timestamp":"1631355180.0","poster":"amxexam","upvote_count":"2","comments":[{"upvote_count":"10","timestamp":"1638114600.0","poster":"ynoot","comment_id":"489261","content":"if you got the commit hash from the container you can check the corresponding commit in the git repository. So the change, that was made and deployed into your environment can be audited."}],"content":"Not sure how the container tag match with the commit will help to audit, can someone explain?"}],"timestamp":"1630138620.0","comment_id":"433691","upvote_count":"16"},{"timestamp":"1718811840.0","comment_id":"1232934","poster":"Sephethus","upvote_count":"2","content":"Selected Answer: C\nLinking Deployments to Commits: By tagging the container image with the source code commit hash, you create a direct link between the deployed container and the specific state of the source code. This provides a clear and auditable trail from the deployed application back to the exact source code that was used to build it.\n\nAuditability: Using the commit hash as the container tag ensures that each deployment can be traced back to a unique and immutable source code commit. This makes it easy to audit deployments and verify which version of the code is running in production."},{"upvote_count":"1","content":"Selected Answer: C\nCan't fathom A. This is what ChatGPT says about A - I agree to this.\nOption A (tagging with date and time): Using date and time as tags may not be precise enough to identify the exact code version associated with a deployment, especially if multiple commits occurred within the same time window.","timestamp":"1694082660.0","comment_id":"1001427","poster":"RaviRS"},{"poster":"BiddlyBdoyng","content":"Really C should say image?\nWe have to seperate systems: source code repo & container repo.\nHow do we link the two together? C is the only attempt at solving the problem.","comment_id":"920006","upvote_count":"1","timestamp":"1686398100.0"},{"comment_id":"773352","poster":"WFCheong","upvote_count":"1","content":"Selected Answer: C\nAgreed with C instead of A with them.","timestamp":"1673519160.0"},{"timestamp":"1671016140.0","upvote_count":"1","content":"Selected Answer: C\nC is the correct answer","poster":"surajkrishnamurthy","comment_id":"744997"},{"poster":"KumarSelvaraj","upvote_count":"1","content":"Answer is C","comment_id":"725118","timestamp":"1669207260.0"},{"upvote_count":"2","poster":"megumin","content":"Selected Answer: C\nC is ok","comment_id":"719542","timestamp":"1668594240.0"},{"content":"Selected Answer: C\nC is correct \"By design, the Git commit hash is immutable and references a specific version of your software.\" as per https://cloud.google.com/architecture/best-practices-for-building-containers#tagging_using_the_git_commit_hash","poster":"Mahmoud_E","timestamp":"1666204800.0","upvote_count":"4","comment_id":"699270"},{"comment_id":"671953","content":"Selected Answer: C\nC is the answer.\n\nhttps://cloud.google.com/architecture/best-practices-for-building-containers#tagging_using_the_git_commit_hash\nYou can use this commit hash as a version number for your software, but also as a tag for the Docker image built from this specific version of your software. Doing so makes Docker images traceable: because in this case the image tag is immutable, you instantly know which specific version of your software is running inside a given container.","poster":"zellck","timestamp":"1663470840.0","upvote_count":"5"},{"poster":"AzureDP900","timestamp":"1656863700.0","upvote_count":"3","comment_id":"626626","content":"Every Git commit with timestamp A doesn't make since. C is right"},{"comment_id":"613795","poster":"munnysh","upvote_count":"4","content":"Selected Answer: C\nNo manual intervention is preferred in automatic deployments. Only automating the container tag to match the commit hash will be fully auditable with the help of the scm.","timestamp":"1654769280.0"},{"comment_id":"596973","upvote_count":"7","timestamp":"1651685580.0","poster":"ridyr","content":"Selected Answer: C\nFrom: https://cloud.google.com/architecture/best-practices-for-building-containers\nUnder: Tagging using the Git commit hash (bottom of page almost)\n\n\"In this case, a common way of handling version numbers is to use the Git commit SHA-1 hash (or a short version of it) as the version number. By design, the Git commit hash is immutable and references a specific version of your software.\n\nYou can use this commit hash as a version number for your software, but also as a tag for the Docker image built from this specific version of your software. Doing so makes Docker images traceable: because in this case the image tag is immutable, you instantly know which specific version of your software is running inside a given container.\""},{"upvote_count":"4","content":"It's got to be A. Option C talks about containers whereas there is no mention of containers in the question.","poster":"SCVinod","timestamp":"1646205660.0","comment_id":"559230"},{"timestamp":"1644612600.0","poster":"[Removed]","content":"Selected Answer: C\nI got similar question on my exam. Answered C.","upvote_count":"5","comment_id":"545511"},{"timestamp":"1642857900.0","comment_id":"529874","poster":"Narinder","upvote_count":"5","content":"I think answer is A.\n\nIn Git, tag is used to mark release points (v1.0, v2.0 and so on). You can tag the release based on the time stamp and using git show <tag-name> command, you can see the commit detailed history. \n\nReference: https://git-scm.com/book/en/v2/Git-Basics-Tagging\n\nC could be the correct answer for the case if you are going with container based solution which is not mentioned anywhere in the question."},{"timestamp":"1642705320.0","poster":"ks100","content":"Selected Answer: C\nshould be C.\nIf A is correct, can the site provide some reference to the reason?","comment_id":"528679","upvote_count":"1"},{"upvote_count":"6","content":"Selected Answer: C\nHumans are unreliable","timestamp":"1642016280.0","comment_id":"522412","poster":"tmnd91"},{"content":"A is useless because commits have date and time already. Go for C for best practice.","comment_id":"521604","poster":"user1324567","upvote_count":"2","timestamp":"1641914100.0"},{"poster":"lxgywil","timestamp":"1641823140.0","content":"C is ok","upvote_count":"1","comment_id":"520885"},{"content":"Option D is correct.","comment_id":"501834","upvote_count":"1","comments":[{"poster":"lxgywil","content":"You gotta be trolling.","timestamp":"1641823080.0","comment_id":"520883","upvote_count":"1"}],"poster":"rajadhav","timestamp":"1639539420.0"},{"poster":"anjuagrawal","timestamp":"1639023000.0","content":"think C: Make the container tag match the source code commit hash. tagging is a good way to match production deployments to source code. We can even tag stage or production depending on if we want the code deployment to only stage. So ,C","upvote_count":"1","comment_id":"497307"},{"content":"An anyone give any reference for this?","comment_id":"497227","timestamp":"1639004340.0","upvote_count":"1","poster":"Nalo1"},{"comment_id":"494138","poster":"vincy2202","timestamp":"1638689700.0","content":"C is the correct answer","upvote_count":"1"},{"content":"Selected Answer: C\nC is the answer","timestamp":"1638673740.0","poster":"Minm","upvote_count":"1","comment_id":"494054"},{"upvote_count":"1","comment_id":"492772","content":"Selected Answer: C\nVote C. A commit is already provided with the timestamp","poster":"pakilodi","timestamp":"1638475500.0"},{"upvote_count":"1","comment_id":"491994","content":"Selected Answer: C\nCommits already have date time","poster":"cdcollector","timestamp":"1638400200.0"},{"upvote_count":"2","poster":"nqthien041292","content":"Selected Answer: C\nVote C","comment_id":"490384","timestamp":"1638242820.0"},{"timestamp":"1637725020.0","content":"Only hash is unique","upvote_count":"1","poster":"dmc123","comment_id":"485581"},{"poster":"xaliq","timestamp":"1636098720.0","content":"https://cloud.google.com/source-repositories/docs/commit-details-overview","upvote_count":"1","comment_id":"472948"},{"poster":"[Removed]","content":"C makes more sense here for auditing based on code commit hash.","timestamp":"1635347220.0","upvote_count":"2","comment_id":"468689"},{"content":"C is correct.","timestamp":"1633143360.0","poster":"AnilKr","comment_id":"455804","upvote_count":"2"},{"comment_id":"454837","poster":"gingerbeer","upvote_count":"3","timestamp":"1632996120.0","content":"Vote for C\nIn this way, you can track production deployment -> container -> source code commit\n\nA does not make sense as tagging date and time does not build any linkage with production deployment\nB incorrect as it should be \"tagging\" rather than \"commenting\"\nD incorrect as \"latest\" itself can iterate as well"},{"content":"A is no sense. Each commit in a source code is correlated by commit ID, timestamp and so on.\nPer exclusion, C is ok","upvote_count":"1","timestamp":"1632988680.0","comment_id":"454788","poster":"Lorenzot92"},{"poster":"ale183","upvote_count":"5","comment_id":"454433","timestamp":"1632943680.0","content":"Those that passed exam , can you confirm if any of old questions (1-102) came up on exam ? As well any old case study questions ?"},{"poster":"JasonL_GCP","comment_id":"453700","timestamp":"1632871620.0","upvote_count":"2","content":"A is fine, for easy auditable using date and time instead of the git generated commit hash."},{"poster":"meh_33","timestamp":"1629972480.0","upvote_count":"4","content":"C is ok","comment_id":"432153"}],"unix_timestamp":1629965280,"choices":{"B":"Make sure a developer is adding a comment to the commit that links to the deployment.","D":"Make sure the developer is tagging the commits with latest.","A":"Make sure a developer is tagging the code commit with the date and time of commit.","C":"Make the container tag match the source code commit hash."}},{"id":"Sc2duIiCxhfALhNwtjAH","isMC":true,"answer_images":[],"discussion":[{"comment_id":"431467","upvote_count":"21","poster":"vladik820","timestamp":"1645800540.0","content":"B is ok"},{"comments":[{"comment_id":"447412","content":"Source code is written in specific versions of the supported programming languages:\nPython 2.7, Python 3.7, Python 3.8, Python 3.9\nJava 8, Java 11\nNode.js 10, Node.js 12, Node.js 14, Node.js 16 (preview)\nPHP 5.5, PHP 7.2, PHP 7.3, and PHP 7.4\nRuby 2.5, Ruby 2.6, and Ruby 2.7\nGo 1.11, Go 1.12, Go 1.13, Go 1.14, Go 1.15, and Go 1.16 (preview)","poster":"cugena","timestamp":"1647675120.0","upvote_count":"7"},{"comment_id":"447413","poster":"cugena","upvote_count":"4","timestamp":"1647675180.0","content":"Intended to run for free or at very low cost, where you pay only for what you need and when you need it. For example, your application can scale to 0 instances when there is no traffic.\n\nExperiences sudden and extreme spikes of traffic which require immediate scaling."}],"comment_id":"430424","poster":"SweetieS","timestamp":"1645679220.0","content":"B is ok.\nhttps://cloud.google.com/appengine/docs/the-appengine-environments","upvote_count":"12"},{"content":"Both B and D are okay, however, the need for App engine Flexible environment is not required unless you want to run docker containers, have more control over the instance used and so on, hence in this case B works well. \nhttps://cloud.google.com/appengine/docs/the-appengine-environments","comment_id":"1172224","timestamp":"1726193100.0","upvote_count":"3","poster":"nairj"},{"timestamp":"1716882180.0","comment_id":"1082362","content":"Selected Answer: A\nA\nGKE is much reliable compared to the other options provided here.","upvote_count":"1","poster":"thewalker"},{"timestamp":"1710861660.0","upvote_count":"1","content":"Selected Answer: B\nB is correct. It supports Go 1.12, and can handle sudden spikes.\nhttps://cloud.google.com/appengine/docs/the-appengine-environments","poster":"rakp","comment_id":"1011346"},{"poster":"gotcertified","timestamp":"1704331200.0","comment_id":"942248","content":"Can someone explain why we cannot use AppEngine Flexible environment ?","comments":[{"poster":"anjanc","comment_id":"1097041","content":"Bcs They want to minimize operational overhead for this application","timestamp":"1718419500.0","upvote_count":"1"},{"timestamp":"1723526280.0","poster":"edoo","content":"I guess it can't scale down to 0.","comment_id":"1148948","upvote_count":"1"}],"upvote_count":"1"},{"upvote_count":"1","poster":"sampon279","comment_id":"935918","content":"Selected Answer: B\nApp engine standard provides go env.","timestamp":"1703729340.0"},{"poster":"AugustoKras011111","comment_id":"828200","upvote_count":"1","content":"Selected Answer: B\nApp Engine Std. Can run this Go version and Scales to 0.","timestamp":"1693755960.0"},{"upvote_count":"1","content":"Selected Answer: B\nStandard AppEngine Environment supports Go 1.2. The AppEngine can be low cost if no or low traffic. It has free quotas.","poster":"zerg0","comment_id":"806938","timestamp":"1691878320.0"},{"content":"Selected Answer: B\nAppEngine scales well, only dev effort. No infrastructure. go is supported in the standard distribution.","comment_id":"794846","poster":"zerg0","upvote_count":"1","timestamp":"1690854120.0"},{"comment_id":"763310","upvote_count":"1","poster":"NodummyIQ","timestamp":"1688230680.0","content":"The answer is A. B option is not correct. It is not recommended to use App Engine Standard environment for an HTTP(S) API with a very unpredictable workload because App Engine Standard environment has certain limitations and constraints that may not be suitable for an API with an unpredictable workload. For example, App Engine Standard environment has a maximum request timeout of 60 seconds, which may not be sufficient for an API with a very unpredictable workload."},{"comment_id":"719545","content":"Selected Answer: B\nB is ok","timestamp":"1684225620.0","poster":"megumin","upvote_count":"1"},{"content":"B is correct ..https://cloud.google.com/appengine/docs/the-appengine-environments\n\nExperiences sudden and extreme spikes of traffic which require immediate scaling.","comment_id":"626630","poster":"AzureDP900","timestamp":"1672768800.0","upvote_count":"2"},{"timestamp":"1670587860.0","content":"Selected Answer: B\nhttps://cloud.google.com/appengine/docs/the-appengine-environments App engine standard environment support go 1.13 and also handles the unpredictable load.","poster":"munnysh","comment_id":"613803","upvote_count":"3"},{"content":"B. Unpredictable traffic & low overhead.","timestamp":"1660997160.0","comment_id":"551836","poster":"TitaniumBurger","upvote_count":"2"},{"timestamp":"1657647540.0","comment_id":"522413","poster":"tmnd91","upvote_count":"6","content":"Selected Answer: B\nApp Engine standard has autoscaling out of the box, supports Go 1.12 and can scale down to 0 to save money"},{"timestamp":"1657454460.0","comment_id":"520888","poster":"lxgywil","content":"B is ok.","upvote_count":"1"},{"timestamp":"1656169680.0","comment_id":"509195","poster":"PhuocT","upvote_count":"2","content":"Selected Answer: B\nB is the right answer"},{"content":"Selected Answer: B\nAppEngine Standard supports Go language now. Fully-managed service - So no operational overhead and pay-only-for-what-you-use model.","upvote_count":"2","comment_id":"505746","timestamp":"1655763180.0","poster":"phantomsg"},{"poster":"rajadhav","upvote_count":"2","content":"B is correct answer.","comment_id":"501837","timestamp":"1655257320.0"},{"upvote_count":"2","comment_id":"497634","timestamp":"1654764660.0","poster":"Bert_77","content":"Selected Answer: B\nB will be the best option. App engine standard supports Go 1.12, can scale quickly during peaks and even scale to 0 when not used, no management overhead"},{"content":"Vote B. Go is supported by App Engine Std. Also reduces operational overhead with PaaS","poster":"anjuagrawal","upvote_count":"1","timestamp":"1654741080.0","comment_id":"497311"},{"comment_id":"494135","poster":"vincy2202","upvote_count":"1","content":"Selected Answer: B\nB is the correct answer.","timestamp":"1654406940.0"},{"comment_id":"492354","timestamp":"1654156620.0","poster":"daveya","content":"B -because of operational overhead and GO is supported on App Engine","upvote_count":"2"},{"upvote_count":"1","timestamp":"1654117980.0","comment_id":"491997","poster":"cdcollector","content":"Selected Answer: B\nAES supports Go 1.12 , AEF is also good but this answer combined with custom runtime which is not needed"},{"comment_id":"483411","poster":"sam1972","content":"B sounds good","upvote_count":"1","timestamp":"1653141240.0"},{"timestamp":"1653107700.0","comment_id":"483053","content":"Selected Answer: B\nanswer is b","upvote_count":"3","poster":"pakilodi"},{"poster":"TheCloudBoy77","upvote_count":"2","content":"B - App engine standard Unpredictable workload, less management overhead, supports Go.","comment_id":"482618","timestamp":"1653049860.0"},{"content":"B is correct, GAE is recommended for sudden spike and specific version of language","timestamp":"1648868280.0","poster":"AnilKr","upvote_count":"3","comment_id":"455805"},{"timestamp":"1648597260.0","comment_id":"454563","upvote_count":"3","content":"unpredictable workload - its App engine std, supports Go","poster":"rottzy"},{"timestamp":"1646043360.0","comment_id":"433688","content":"B. Develop the application for App Engine standard environment.","poster":"victory108","upvote_count":"3"},{"timestamp":"1646029740.0","content":"B is ok","upvote_count":"3","comment_id":"433517","poster":"serious_user"},{"upvote_count":"5","poster":"meh_33","content":"B is ok","comment_id":"432154","timestamp":"1645877340.0"},{"comments":[{"upvote_count":"1","timestamp":"1648510260.0","comment_id":"453701","content":"Agreed, b","poster":"JasonL_GCP"}],"comment_id":"430497","poster":"murugane","timestamp":"1645690260.0","content":"I think it should be B","upvote_count":"7"}],"exam_id":4,"timestamp":"2021-08-24 05:07:00","unix_timestamp":1629774420,"topic":"1","question_images":[],"answers_community":["B (96%)","4%"],"url":"https://www.examtopics.com/discussions/google/view/60437-exam-professional-cloud-architect-topic-1-question-142/","answer":"B","answer_description":"","question_text":"An application development team has come to you for advice. They are planning to write and deploy an HTTP(S) API using Go 1.12. The API will have a very unpredictable workload and must remain reliable during peaks in traffic. They want to minimize operational overhead for this application. Which approach should you recommend?","question_id":49,"choices":{"B":"Develop the application for App Engine standard environment.","D":"Develop the application for App Engine flexible environment, using a custom runtime.","A":"Develop the application with containers, and deploy to Google Kubernetes Engine.","C":"Use a Managed Instance Group when deploying to Compute Engine."},"answer_ET":"B"},{"id":"aVIiR2ugWHxX3tAz5sJl","answers_community":["D (100%)"],"exam_id":4,"discussion":[{"poster":"vladik820","timestamp":"1645861380.0","comment_id":"431975","content":"D is ok\nThe data needs to be stored as it is retrieved. This would mean that any processing should be done after it is stored.","upvote_count":"28"},{"poster":"MaxNRG","comment_id":"466428","timestamp":"1650689640.0","content":"D, store RAW unstructured data as-is in Cloud Storage, and then define how to process it.\nClassical Data Lake ELT (Extract -> Load -> Transform )","upvote_count":"7"},{"upvote_count":"1","timestamp":"1735968240.0","poster":"plumbig11","comment_id":"1336269","content":"Selected Answer: D\nUnstructured data, cloud storage;"},{"upvote_count":"1","timestamp":"1729921320.0","comment_id":"1202400","poster":"Gino17m","content":"Selected Answer: D\nD\nUnstructured data - GCS\nData stored axactly as it was retrieved - store before processing"},{"comment_id":"1097045","timestamp":"1718420040.0","poster":"anjanc","content":"Key word is \"The data must be stored exactly as it was retrieved for reprocessing purposes in case the data structure is incompatible with the current processing pipelines.\" and hence D","upvote_count":"1"},{"content":"D. It aligns with an example in the Cloud Architecture Framework\nhttps://cloud.google.com/architecture/big-data-analytics/analytics-lakehouse","poster":"devnul","timestamp":"1708638000.0","comment_id":"987721","upvote_count":"2"},{"comment_id":"835352","comments":[{"poster":"jlambdan","upvote_count":"3","comment_id":"858741","timestamp":"1696242420.0","content":"here gcs is the lake. Not a copy.\nThe data warehouse will be what comes out of the pipelines."}],"content":"What is the point of data being in a lake and then being dumped into GCS without processing. What purpose is served with GCS being a copy of lake?","poster":"BeCalm","upvote_count":"1","timestamp":"1694367300.0"},{"poster":"megumin","comment_id":"719548","timestamp":"1684225860.0","content":"Selected Answer: D\nD is ok","upvote_count":"2"},{"content":"D is ok","upvote_count":"2","timestamp":"1683049260.0","comment_id":"710022","poster":"jmblancof"},{"timestamp":"1679072280.0","upvote_count":"2","comment_id":"671612","content":"Selected Answer: D\nD is ok\nThe data needs to be stored as it is retrieved. This would mean that any processing should be done after it is stored in GCS.","poster":"Nirca"},{"upvote_count":"1","timestamp":"1672769100.0","poster":"AzureDP900","content":"storing and retrieving data in cloud storage solve the purpose of this use case. D is perfect answer.","comment_id":"626632"},{"comments":[{"content":"Both BigTable and DataStore are NoSQL Databases, qns mentioned that data structure may change anytime","comment_id":"635176","poster":"wykofc","upvote_count":"2","timestamp":"1674392400.0"}],"poster":"snwbr","comment_id":"578415","timestamp":"1664556060.0","upvote_count":"1","content":"Although... wouldn't be Bigtable or Datastore better than GCS?"},{"content":"Selected Answer: D\nI got similar question on my exam. Answered D.","poster":"[Removed]","comment_id":"545513","upvote_count":"4","timestamp":"1660243800.0"},{"poster":"technodev","timestamp":"1658240640.0","content":"Got this question in my exam, answered D","upvote_count":"4","comment_id":"527717"},{"upvote_count":"2","poster":"lxgywil","timestamp":"1657455060.0","comment_id":"520895","content":"D is ok"},{"content":"D is the correct answer","upvote_count":"1","timestamp":"1654417620.0","poster":"vincy2202","comment_id":"494226"},{"timestamp":"1653655140.0","poster":"pakilodi","comment_id":"488207","upvote_count":"1","content":"Selected Answer: D\nD is correct"},{"poster":"TheCloudBoy77","upvote_count":"2","timestamp":"1653050160.0","comment_id":"482624","content":"D - Data must be stored as it is before and after so use Cloud storage and then build pipelines as needed."},{"upvote_count":"2","comment_id":"465604","timestamp":"1650524280.0","poster":"danielfootc","content":"I would select D as well."},{"upvote_count":"2","comment_id":"455806","poster":"AnilKr","timestamp":"1648868340.0","content":"D is correct."},{"poster":"amxexam","upvote_count":"4","content":"\"After the data is stored in Google Cloud, it will be processed in several data pipelines to build a recommendation engine\" \n\nSo first store then process in the pipeline.\n\nSo we need to store first then process it.\nWill eliminate A and C.\n\nThe second point big data table needs a fixed schema to work so it won't work.\nWill eliminate B\n\nHence D","comment_id":"442957","timestamp":"1647005520.0"},{"upvote_count":"2","content":"D. Store the data in a Cloud Storage bucket. Design the processing pipelines to retrieve the data from the bucket. (keyword : unstructured)","timestamp":"1646043300.0","poster":"victory108","comments":[{"comments":[{"content":"C first processes the data before storing it. Original (unprocessed) data should remain available, so C is not a good solution.","comment_id":"497639","poster":"Bert_77","timestamp":"1654764900.0","upvote_count":"1"}],"poster":"VishalB","upvote_count":"1","content":"why not C ? it is doing the same thing storing data in cloud storage","timestamp":"1646035980.0","comment_id":"435144"}],"comment_id":"433686"},{"comment_id":"433537","upvote_count":"2","content":"Your company is designing its data lake on Google Cloud and wants to develop different ingestion pipelines (pipelines 1) to collect unstructured data from different sources.\nAfter the data is stored in Google Cloud, it will be processed in several data pipelines (pipelines 2) to build a recommendation engine for end users on the website. The structure of the data retrieved(retrieve 1) from the source systems can change at any time(can be completed by pipelines 1). The data must be stored exactly as it was retrieved (retrieve 2) for reprocessing purposes in case the data structure is incompatible with the current processing pipelines(pipelines 2). You need to design an architecture to support the use case after you retrieve the data. What should you do? \nIngestion data must go through pipelines 1: changing the structure then save to Cloud Storage first. So my answer is \"C\"","poster":"GCP_daity","comments":[{"comment_id":"528605","upvote_count":"1","poster":"Shaileshss","content":"Data must be stored as is without processing. So C would not be right choice.","timestamp":"1658329560.0"}],"timestamp":"1646030820.0"},{"content":"D is ok","poster":"meh_33","upvote_count":"2","comment_id":"432159","timestamp":"1645877580.0"}],"topic":"1","answer_images":[],"question_text":"Your company is designing its data lake on Google Cloud and wants to develop different ingestion pipelines to collect unstructured data from different sources.\nAfter the data is stored in Google Cloud, it will be processed in several data pipelines to build a recommendation engine for end users on the website. The structure of the data retrieved from the source systems can change at any time. The data must be stored exactly as it was retrieved for reprocessing purposes in case the data structure is incompatible with the current processing pipelines. You need to design an architecture to support the use case after you retrieve the data. What should you do?","timestamp":"2021-08-26 07:43:00","question_id":50,"answer_ET":"D","question_images":[],"unix_timestamp":1629956580,"url":"https://www.examtopics.com/discussions/google/view/60682-exam-professional-cloud-architect-topic-1-question-143/","isMC":true,"answer_description":"","answer":"D","choices":{"A":"Send the data through the processing pipeline, and then store the processed data in a BigQuery table for reprocessing.","C":"Send the data through the processing pipeline, and then store the processed data in a Cloud Storage bucket for reprocessing.","D":"Store the data in a Cloud Storage bucket. Design the processing pipelines to retrieve the data from the bucket.","B":"Store the data in a BigQuery table. Design the processing pipelines to retrieve the data from the table."}}],"exam":{"numberOfQuestions":279,"provider":"Google","isMCOnly":false,"lastUpdated":"11 Apr 2025","isBeta":false,"id":4,"name":"Professional Cloud Architect","isImplemented":true},"currentPage":10},"__N_SSP":true}