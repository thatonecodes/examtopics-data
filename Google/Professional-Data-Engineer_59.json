{"pageProps":{"questions":[{"id":"SrIu6R3IWv5uwaCkahTq","answer_images":[],"topic":"1","isMC":true,"url":"https://www.examtopics.com/discussions/google/view/17112-exam-professional-data-engineer-topic-1-question-73/","unix_timestamp":1584805920,"answer_description":"","question_images":[],"timestamp":"2020-03-21 16:52:00","answer":"C","discussion":[{"content":"Answer: C\nDescription: Spanner allows transaction tables to scale horizontally and secondary indexes for range queries","upvote_count":"31","comment_id":"68736","poster":"[Removed]","timestamp":"1616896260.0"},{"content":"Correct: C","poster":"[Removed]","upvote_count":"9","comment_id":"66582","timestamp":"1616341920.0"},{"comment_id":"870169","content":"Selected Answer: C\nCorrect: C","timestamp":"1713097680.0","poster":"nhanhoangle","upvote_count":"1"},{"poster":"PolyMoe","content":"Selected Answer: C\nCloud Spanner is a fully-managed, horizontally scalable relational database service that supports transactions and allows you to optimize data for range queries on non-key columns. By using Cloud Spanner for storage, you can ensure that your database can scale horizontally to meet the needs of your application.\nTo optimize data for range queries on non-key columns, you can add secondary indexes, this will allow you to perform range scans on non-key columns, which can improve the performance of queries that filter on non-key columns.","comment_id":"789093","upvote_count":"2","timestamp":"1706301000.0"},{"content":"C. Use Cloud Spanner for storage. Add secondary indexes to support query patterns.\n\nCloud Spanner is a fully-managed, horizontally scalable relational database service that supports transactions and allows you to optimize data for range queries on non-key columns. By using Cloud Spanner for storage, you can ensure that your database can scale horizontally to meet the needs of your application.\nTo optimize data for range queries on non-key columns, you can add secondary indexes, this will allow you to perform range scans on non-key columns, which can improve the performance of queries that filter on non-key columns.","comments":[{"comment_id":"785840","timestamp":"1706044140.0","poster":"samdhimal","comments":[{"poster":"Mathew106","timestamp":"1721722560.0","comment_id":"960225","content":"Cloud SQL does support replicas to increase availability. Why is that not considered horizontal scaling?","upvote_count":"2"},{"comment_id":"785841","content":"- Option B, Using Cloud SQL for storage and using Cloud Dataflow to transform data to support query patterns, may not be the best option as Cloud SQL is a relational database service that does not support horizontal scaling and may not be able to handle the large amount of data and the number of queries required by your application. Additionally, Cloud Dataflow is a data processing service and not a storage service, so it may not be the best fit for this use case.\n\n- Option D, Using Cloud Spanner for storage and using Cloud Dataflow to transform data to support query patterns, is not necessary as Cloud Spanner provides the ability to optimize data for range queries on non-key columns by adding secondary indexes. Cloud Spanner also supports transactional consistency, which is a feature that allows you to perform multiple operations that must be performed together in a single transaction. Additionally, Cloud Dataflow is a data processing service and not a storage service, so it may not be the best fit for this use case.","upvote_count":"1","timestamp":"1706044140.0","poster":"samdhimal"}],"content":"- Option A, Using Cloud SQL for storage and adding secondary indexes to support query patterns, may not be the best option as Cloud SQL is a relational database service that does not support horizontal scaling and may not be able to handle the large amount of data and the number of queries required by your application.","upvote_count":"1"}],"comment_id":"785839","upvote_count":"3","timestamp":"1706044140.0","poster":"samdhimal"},{"content":"Selected Answer: C\nC is the answer.\n\nhttps://cloud.google.com/architecture/autoscaling-cloud-spanner\nWhen you create a Cloud Spanner instance, you choose the number of compute capacity nodes or processing units to serve your data. However, if the workload of an instance changes, Cloud Spanner doesn't automatically adjust the size of the instance. This document introduces the Autoscaler tool for Cloud Spanner (Autoscaler), an open source tool that you can use as a companion tool to Cloud Spanner. This tool lets you automatically increase or reduce the number of nodes or processing units in one or more Spanner instances based on how their capacity is being used.\n\nhttps://cloud.google.com/spanner/docs/secondary-indexes\nYou can also create secondary indexes for other columns. Adding a secondary index on a column makes it more efficient to look up data in that column.","upvote_count":"1","timestamp":"1701869760.0","comment_id":"736852","poster":"zellck"},{"upvote_count":"1","content":"Selected Answer: C\nAs sumanshu said","poster":"sedado77","comment_id":"668443","timestamp":"1694643300.0"},{"upvote_count":"1","content":"Answer: C","comment_id":"464881","timestamp":"1666230420.0","poster":"tsoetan001"},{"comments":[{"content":"A is not correct because Cloud SQL does not natively scale horizontally.\nB is not correct because Cloud SQL does not natively scale horizontally.\nC is correct because Cloud Spanner scales horizontally, and you can create secondary indexes for the range queries that are required.\nD is not correct because Dataflow is a data pipelining tool to move and transform data, but the use case is centered around querying.","upvote_count":"8","comment_id":"402248","timestamp":"1657315980.0","poster":"sumanshu"}],"comment_id":"393747","poster":"sumanshu","timestamp":"1656503940.0","content":"Vote for C","upvote_count":"4"},{"content":"Answer: C\nhttps://cloud.google.com/spanner/docs/secondary-indexes","upvote_count":"2","timestamp":"1648073460.0","comment_id":"318533","poster":"timolo"},{"content":"Correct: C","timestamp":"1640112120.0","poster":"Nileshk611","upvote_count":"3","comment_id":"249578"},{"comment_id":"219765","content":"Correct answers is C","timestamp":"1636989960.0","poster":"arghya13","upvote_count":"2"}],"answers_community":["C (100%)"],"exam_id":11,"answer_ET":"C","choices":{"B":"Use Cloud SQL for storage. Use Cloud Dataflow to transform data to support query patterns.","C":"Use Cloud Spanner for storage. Add secondary indexes to support query patterns.","A":"Use Cloud SQL for storage. Add secondary indexes to support query patterns.","D":"Use Cloud Spanner for storage. Use Cloud Dataflow to transform data to support query patterns."},"question_text":"You are designing storage for two relational tables that are part of a 10-TB database on Google Cloud. You want to support transactions that scale horizontally.\nYou also want to optimize data for range queries on non-key columns. What should you do?","question_id":291},{"id":"yaI6XH4A7x63cDuWWCAs","answer":"A","url":"https://www.examtopics.com/discussions/google/view/79320-exam-professional-data-engineer-topic-1-question-74/","question_text":"Your financial services company is moving to cloud technology and wants to store 50 TB of financial time-series data in the cloud. This data is updated frequently and new data will be streaming in all the time. Your company also wants to move their existing Apache Hadoop jobs to the cloud to get insights into this data.\nWhich product should they use to store the data?","choices":{"C":"Google Cloud Storage","B":"Google BigQuery","A":"Cloud Bigtable","D":"Google Cloud Datastore"},"answers_community":["A (82%)","C (18%)"],"answer_ET":"A","isMC":true,"answer_description":"","answer_images":[],"question_images":[],"topic":"1","unix_timestamp":1662104520,"discussion":[{"timestamp":"1670333340.0","comments":[{"timestamp":"1722793860.0","poster":"opt_sub","content":"Gmail is migrated to Spanner now!","upvote_count":"1","comment_id":"1260756"},{"comment_id":"748309","content":"Hbase concept here us beautiful","poster":"Atnafu","upvote_count":"2","timestamp":"1671303480.0"}],"upvote_count":"12","comment_id":"736850","poster":"zellck","content":"Selected Answer: A\nA is the answer.\n\nhttps://cloud.google.com/dataproc/docs/concepts/connectors/cloud-bigtable\nBigtable is Google's NoSQL Big Data database service. It's the same database that powers many core Google services, including Search, Analytics, Maps, and Gmail. Bigtable is designed to handle massive workloads at consistent low latency and high throughput, so it's a great choice for both operational and analytical applications, including IoT, user analytics, and financial data analysis.\n\nBigtable is an excellent option for any Apache Spark or Hadoop uses that require Apache HBase. Bigtable supports the Apache HBase 1.0+ APIs and offers a Bigtable HBase client in Maven, so it is easy to use Bigtable with Dataproc."},{"content":"Every time you hear financial, time series, fast reads and write data, Any of that combinations, think Big Table first.\nSo A.","timestamp":"1707281880.0","poster":"philli1011","comment_id":"1142955","upvote_count":"4"},{"upvote_count":"2","comment_id":"960231","content":"Selected Answer: A\nAt first I thought that GCS was the answer but the question does mention that the data is updated frequently. Thereby, it has to be BigTable since we talk about a large amount of data, a streaming application and many individual updates. Storing the data in BigQuery and having to make individual updates doesn't make sense, and neither does running Apache jobs.\n\nIf the requirement for updates was not there I would not see any issue with GCS. GCS could serve as a replacement to HDFS and run Hadoop jobs from Dataproc.","timestamp":"1690100760.0","poster":"Mathew106"},{"upvote_count":"1","comment_id":"927168","content":"Selected Answer: A\nThis scenario screams for BigTable.\n\nIt's not B) BigQuery or C) Cloud Storage because both aren't supposed to contain data that is updated frequently. Then, we have to decide between A) BigTable and D) Datastore.\n\nIt is A) BigTable because\n- it is the most suited for real-time / high-frequency updates\n- it is similar to HBase, which is commonly used in Hadoop ecosystem stacks to store streaming / time-series data.","timestamp":"1687152480.0","poster":"KC_go_reply"},{"content":"Selected Answer: A\nMany here also selected Cloud Storage. But the way I see it BigTable is specifically for low latency, high throughput, mission critical streaming data (financial data is one of them). Also the mentioning of Hadoop that points to HBase functionality if BigTable clarifies the choice more.","upvote_count":"1","comment_id":"906478","timestamp":"1684999680.0","poster":"AmmarFasih"},{"timestamp":"1682925960.0","comment_id":"885962","content":"Selected Answer: A\nBigTable - a No-SQL database but does not support SQL Querying\nApache HBase - Based on Google's BigTable on top of HDFS and you can migrate Hadoop Apps to Cloud BigTable with the HBase API","upvote_count":"2","poster":"Hisayuki"},{"upvote_count":"1","timestamp":"1681433040.0","content":"Selected Answer: A\nA. time series data","comment_id":"869863","poster":"izekc"},{"comment_id":"824363","poster":"midgoo","timestamp":"1677555840.0","upvote_count":"1","content":"Selected Answer: A\nPlease note that there is Connector for Bigtable for Hadoop\nhttps://cloud.google.com/dataproc/docs/concepts/connectors/cloud-bigtable"},{"content":"Why not Biquery?\n\nGoogle BigQuery would be the best option for storing and analyzing large amounts of financial time-series data that is frequently updated and streamed in real-time. It is a fully managed, cloud-native data warehouse that allows you to analyze large datasets using SQL-like queries, and it can handle streaming data as well as batch data. Additionally, it can easily integrate with Apache Hadoop to allow your company to run their existing Hadoop jobs in the cloud and gain insights into the data.","upvote_count":"1","poster":"samdhimal","comments":[{"upvote_count":"2","content":"A. Google Bigtable is a fully managed, NoSQL, wide-column database that is designed for large scale, low-latency workloads. It is well suited for use cases such as real-time analytics, IoT, and gaming, but it may not be the best fit for storing and analyzing large amounts of financial time-series data that is frequently updated and streamed in real-time. It lacks built-in support for SQL-like queries, which is a standard way of analyzing data in Data Warehousing and Business Intelligence. It is more focused on handling high-performance low-latency workloads, while BigQuery is focused on providing an easy and cost-effective way to analyze large amounts of data using SQL-like queries. Additionally, Bigtable doesn't provide built-in support for running Apache Hadoop jobs, and it would require additional work to integrate it with Hadoop and set it up for data warehousing and Business Intelligence use cases.","poster":"samdhimal","timestamp":"1674523320.0","comment_id":"786032","comments":[{"timestamp":"1674523380.0","poster":"samdhimal","upvote_count":"2","comments":[{"upvote_count":"1","content":"Can someone clarify why Bigtable and Not Bigquery? Super Confused.","timestamp":"1674523380.0","comments":[{"upvote_count":"1","content":"Yes, it is possible to analyze data in Bigtable. Bigtable is a distributed NoSQL database that is designed to handle large volumes of structured data with high read and write throughput. While Bigtable itself does not provide analysis tools, it is often used in combination with other tools and technologies to perform analysis on the stored data.","comment_id":"880089","poster":"Oleksandr0501","timestamp":"1682408760.0"},{"content":"part2, gpt:\nOne common approach is to use Google Cloud's Bigtable with other services such as Google Cloud Dataflow, Apache Spark, or Hadoop to perform analysis tasks. These tools can be used to extract data from Bigtable, transform it, and perform analysis tasks such as aggregations, filtering, and machine learning algorithms.\n\nIn addition, Bigtable also provides support for secondary indexes and filtering, which can be used to efficiently query and analyze data. The secondary indexes allow you to index specific columns of your data, which makes it easier to search and analyze the data. Filtering can be used to retrieve only the relevant rows of data, reducing the amount of data that needs to be processed and analyzed.\n\nOverall, while Bigtable is not a complete analytics solution on its own, it can be used as a powerful storage backend for analytical workloads, and can be integrated with other tools and technologies to provide a complete analytics solution.","timestamp":"1682408820.0","comment_id":"880090","poster":"Oleksandr0501","upvote_count":"1"}],"comment_id":"786034","poster":"samdhimal"}],"comment_id":"786033","content":"C. Google Cloud Storage is an object storage service that allows you to store and retrieve large amounts of unstructured data, such as video, audio, images and other files. It is not a data warehouse and does not provide built-in support for SQL-like queries, which is a standard way of analyzing data in Data Warehousing and Business Intelligence. It would not be suitable for storing and analyzing large amounts of financial time-series data that is frequently updated and streamed in real-time.\n\nD. Google Cloud Datastore is a fully-managed, NoSQL document database that allows you to store, retrieve, and query data. It is not a data warehouse and does not provide built-in support for SQL-like queries, which is a standard way of analyzing data in Data Warehousing and Business Intelligence. It would not be suitable for storing and analyzing large amounts of financial time-series data that is frequently updated and streamed in real-time."}]}],"comment_id":"786031","timestamp":"1674523320.0"},{"timestamp":"1673981100.0","poster":"desertlotus1211","upvote_count":"2","comment_id":"779219","content":"https://cloud.google.com/bigtable/docs/schema-design-time-series"},{"upvote_count":"3","poster":"Yazar97","content":"Time series data = Bigtable... So it's A","comment_id":"723438","timestamp":"1669031640.0"},{"comment_id":"722947","poster":"Jay_Krish","timestamp":"1668973620.0","content":"Selected Answer: A\nOption A seems right","upvote_count":"1"},{"content":"Selected Answer: A\nBig Table has a HBase compliant API and is transactional unlike GCS.","timestamp":"1668966600.0","comment_id":"722844","poster":"drunk_goat82","upvote_count":"1"},{"timestamp":"1668688260.0","poster":"solar_maker","comment_id":"720453","upvote_count":"1","content":"Selected Answer: A\nBigTable can take in data from dataproc, spark and hadoop\nhttps://cloud.google.com/dataproc/docs/concepts/connectors/cloud-bigtable#using_with"},{"upvote_count":"3","content":"Selected Answer: C\nIt must be C because of the existing Hadoop jobs","timestamp":"1667758080.0","comment_id":"712546","comments":[{"timestamp":"1667944560.0","comment_id":"714135","upvote_count":"6","content":"On 2nd thought, it’s Bigtable: https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-bigtable","poster":"cloudmon"}],"poster":"cloudmon"},{"timestamp":"1664919780.0","upvote_count":"2","content":"Selected Answer: C\nI think it is C","poster":"pluiedust","comment_id":"686447"},{"poster":"maia01","upvote_count":"2","comment_id":"681829","timestamp":"1664377320.0","content":"Selected Answer: C\nUse Datarproc with Cloud Storage in combo with HDFS\nhttps://cloud.google.com/dataproc/docs/concepts/dataproc-hdfs","comments":[{"comment_id":"945684","timestamp":"1688734320.0","upvote_count":"1","content":"Answer is A: Hadoop doesn't mean Dataproc + HDFS. This scenario is about time series that is a use-case for BigTable. Coincidentally BigTable is the best solution for migration of HBase...","poster":"euro202"}]},{"upvote_count":"4","content":"Selected Answer: A\nA. Cloud Bigtable","poster":"AWSandeep","comment_id":"658398","timestamp":"1662205920.0"},{"timestamp":"1662104520.0","content":"Selected Answer: A\nA but not sure about the existing hadoop jobs","upvote_count":"4","comment_id":"657131","poster":"YorelNation"}],"question_id":292,"timestamp":"2022-09-02 09:42:00","exam_id":11},{"id":"CECmTnetd548qCVlyH2M","answer":"A","url":"https://www.examtopics.com/discussions/google/view/79767-exam-professional-data-engineer-topic-1-question-75/","choices":{"C":"Create and share a new dataset and table that contains the aggregate results.","A":"Create and share an authorized view that provides the aggregate results.","B":"Create and share a new dataset and view that provides the aggregate results.","D":"Create dataViewer Identity and Access Management (IAM) roles on the dataset to enable sharing."},"question_text":"An organization maintains a Google BigQuery dataset that contains tables with user-level data. They want to expose aggregates of this data to other Google\nCloud projects, while still controlling access to the user-level data. Additionally, they need to minimize their overall storage cost and ensure the analysis cost for other projects is assigned to those projects. What should they do?","answers_community":["A (56%)","B (44%)"],"answer_ET":"A","answer_description":"","isMC":true,"answer_images":[],"question_images":[],"topic":"1","unix_timestamp":1662205860,"discussion":[{"poster":"midgoo","content":"Selected Answer: A\nA is the answer. Don't be confused by the documentation saying \"Authorized views should be created in a different dataset\". It is a best practice but not a technical requirement. And we don't create a new dataset for each authorized view. If you are not clear on this, try in the system, don't just read the documentation without understanding.\nB is wrong when saying we must SHARE Dataset. Although creating a dataset and view in it will not incur extra cost, but sharing dataset is something we always try not to do.\nAt for the project that run the query it the project to be billed, that is standard behaviour. View only give access to data, whoever run the view will need pay for the query cost","comment_id":"826441","comments":[{"upvote_count":"1","timestamp":"1709581200.0","content":"https://cloud.google.com/bigquery/docs/authorized-views#:~:text=An%20authorized%20view%20and%20authorized,users%20are%20able%20to%20query.","comment_id":"829329","poster":"DAYAGOWDA"},{"poster":"Yiouk","upvote_count":"3","timestamp":"1720900140.0","content":"Have to consider where the billing goes to:\nhttps://stackoverflow.com/questions/52201034/bigquery-authorized-view-cost-billing-account\nhence anwer is B","comments":[{"comment_id":"960245","timestamp":"1721723760.0","upvote_count":"4","poster":"Mathew106","content":"Did you even read the answer in the SO link you shared?\nPart of the answer is below:\n\"\"\"After a deeper investigation and some test scenarios, I have confirmed that the billing charges related to the query jobs are applied to the Billing account associated to the project that executes the query; however, the view owner keeps getting the charges related to the storage of the source data.\"\"\"\n\nSoo, if you create an authorized view, the users from the other project that has access to the view will get billed for the querying.\n\nThe only reason to pick up B over A is that it's the recommended approach to store views in a different dataset than the base data."}],"comment_id":"950959"}],"timestamp":"1709348460.0","upvote_count":"17"},{"upvote_count":"13","poster":"[Removed]","content":"Selected Answer: B\nThe link on authorized views (https://cloud.google.com/bigquery/docs/share-access-views) explicitly states \"Authorized views should be created in a different dataset from the source data. That way, data owners can give users access to the authorized view without simultaneously granting access to the underlying data.\" therefore B is the correct answer because we are to create a new dataset and view within that dataset.","comment_id":"704059","timestamp":"1698254700.0"},{"poster":"desertlotus1211","comment_id":"1398840","timestamp":"1742044260.0","content":"Selected Answer: A\nWhen other projects query the authorized view, the query costs are billed to the project that runs the query","upvote_count":"1"},{"comments":[],"poster":"loki82","content":"Selected Answer: B\nI think both A and B are totally valid answers, making this a fairly stupid question. But as a DBA, B would be easier to implement, easier to manage, easier to audit. So even if it's the wrong answer, it's still the right solution, so I can't help but choose B.","timestamp":"1737288360.0","comment_id":"1342981","upvote_count":"1"},{"content":"Selected Answer: B\nB is the correct Ans \n\nYou are correct in noting that creating a view within an existing dataset could potentially expose other tables within that dataset if the dataset-level permissions are not carefully managed. To ensure that only the aggregate results are shared and to avoid inadvertently exposing other tables, it is indeed a good practice to create a new dataset specifically for the view.\n\n\nRevised Answer:\n\nB. Create and share a new dataset and view that provides the aggregate results.","comment_id":"1332280","timestamp":"1735284720.0","poster":"inamm","upvote_count":"1"},{"poster":"rocky48","comment_id":"1082193","content":"Selected Answer: A\nA. Create and share an authorized view that provides the aggregate results.\n\nAn authorized view is a BigQuery feature that allows you to share only a specific subset of data from a table, while still keeping the original data private. This way, the organization can expose only the aggregate data to other projects, while still controlling access to the user-level data. By using an authorized view, the organization can minimize their overall storage cost as the aggregate data takes up less storage space than the original data. Additionally, by using authorized view, the analysis cost for other projects is assigned to those projects.","upvote_count":"2","timestamp":"1732772700.0"},{"poster":"odiez3","content":"I thing that is B because for security yo need to create a new data set when share a view, apart when yo grant access the top level is a data set if you share a view in same dataset that you have your tables, that access can see all tables inside dataset.","upvote_count":"1","comment_id":"960622","timestamp":"1721751720.0"},{"content":"Selected Answer: B\n\"Authorized views should be created in a different dataset from the source data. That way, data owners can give users access to the authorized view without simultaneously granting access to the underlying data.\"\nhttps://cloud.google.com/bigquery/docs/share-access-views?hl=en#console_5","poster":"baht","timestamp":"1718716140.0","comment_id":"926708","upvote_count":"2"},{"content":"Selected Answer: A\nB is wrong by itself. why do you need to create a view, if you have already created an aggregated dataset?","poster":"[Removed]","timestamp":"1713104160.0","comment_id":"870247","upvote_count":"1"},{"comment_id":"854356","poster":"MrMone","upvote_count":"3","timestamp":"1711720860.0","content":"Selected Answer: A\n\"they need to minimize their overall storage cost\". Also, you are sharing the aggregate's results, not the underlying table"},{"content":"Selected Answer: A\nminimize cost so view","upvote_count":"1","poster":"bha11111","comment_id":"835716","timestamp":"1710141660.0"},{"content":"Selected Answer: A\nA should be the answer, as we need to separate costs according to projects. As in the following SO question (and the attached google resources), the 'project that runs the queries is the project that gets billed.' \nSo we can generate a view and give it's access to the other project to run the analysis\nhttps://stackoverflow.com/questions/52201034/bigquery-authorized-view-cost-billing-account","poster":"Paritosh07","timestamp":"1709136180.0","upvote_count":"3","comment_id":"825001"},{"upvote_count":"2","poster":"musumusu","comment_id":"808532","timestamp":"1707925320.0","content":"I will go with A, as i wanna save cost, dont need to create separate dataset for permanent storage."},{"comment_id":"786042","poster":"samdhimal","timestamp":"1706059800.0","content":"A. Create and share an authorized view that provides the aggregate results.\n\nAn authorized view is a BigQuery feature that allows you to share only a specific subset of data from a table, while still keeping the original data private. This way, the organization can expose only the aggregate data to other projects, while still controlling access to the user-level data. By using an authorized view, the organization can minimize their overall storage cost as the aggregate data takes up less storage space than the original data. Additionally, by using authorized view, the analysis cost for other projects is assigned to those projects.","comments":[{"upvote_count":"2","timestamp":"1706059860.0","content":"B. Creating and sharing a new dataset and view that provides the aggregate results is also a correct option but not as optimal as authorized view as it creates a copy of the data and increases the storage costs.\nC. Creating and sharing a new dataset and table that contains the aggregate results is also a correct option but not as optimal as authorized view as it creates a copy of the data and increases the storage costs.\nD. Creating dataViewer Identity and Access Management (IAM) roles on the dataset to enable sharing is not the best option as it would give access to the user-level data, not just the aggregate data.","comment_id":"786043","poster":"samdhimal"}],"upvote_count":"2"},{"timestamp":"1706033580.0","comment_id":"785697","content":"Selected Answer: B\nEnsure the analysis cost for other projects is assigned to those projects indicates B is the correct answer.","upvote_count":"2","poster":"Rupendra06"},{"upvote_count":"2","timestamp":"1705746120.0","poster":"GCPpro","content":"B is the correct answer","comment_id":"782104"},{"comment_id":"758589","content":"Selected Answer: A\nI would say 1 too","upvote_count":"1","poster":"Kyr0","timestamp":"1703685660.0"},{"comment_id":"750616","upvote_count":"2","poster":"slade_wilson","content":"Selected Answer: B\nB is the correct approach.","timestamp":"1703058360.0"},{"comment_id":"745275","content":"I think its B, as projects need to be charged separately","poster":"wan2three","timestamp":"1702571640.0","upvote_count":"1"},{"timestamp":"1701869160.0","content":"Selected Answer: B\nB is the answer.\n\nhttps://cloud.google.com/bigquery/docs/share-access-views#create_a_dataset_where_you_can_store_your_view\nAfter creating your source dataset, you create a new, separate dataset to store the authorized view that you share with your data analysts. In a later step, you grant the authorized view access to the data in the source dataset. Your data analysts then have access to the authorized view, but not direct access to the source data.\n\nAuthorized views should be created in a different dataset from the source data. That way, data owners can give users access to the authorized view without simultaneously granting access to the underlying data. The source data dataset and authorized view dataset must be in the same regional location.","upvote_count":"5","poster":"zellck","comment_id":"736846","comments":[{"content":"But the wording of option B says create and share a new dataset, do you also need to share dataset apart from authorized view access? In option A, isn't is implicit that authorized view is created on a new dataset and hence option A. B also doesn't mention about Authorized keyword so you may interpret it as normal view which doesn't make sense?","poster":"Wonka87","upvote_count":"1","comment_id":"760777","timestamp":"1703838480.0"}]},{"upvote_count":"5","comments":[{"poster":"Wonka87","content":"But the wording of option B says create and share a new dataset, do you also need to share dataset apart from authorized view access? In option A, isn't is implicit that authorized view is created on a new dataset and hence option A. B also doesn't mention about Authorized keyword so you may interpret it as normal view which doesn't make sense?","comment_id":"760780","timestamp":"1703838540.0","upvote_count":"1"}],"content":"That's ambiguous. While A is correct, B is the recommended approach:\n\"Authorized views should be created in a different dataset from the source data. That way, data owners can give users access to the authorized view without simultaneously granting access to the underlying data. The source data dataset and authorized view dataset must be in the same regional location.\"\n\nBit it doesn't say \"authorised view\" in B.","timestamp":"1701350220.0","poster":"Gudwin","comment_id":"731499"},{"comment_id":"726991","timestamp":"1700935020.0","upvote_count":"3","content":"Selected Answer: A\nAs they need to minimize their overall storage cost and ensure the analysis cost for other projects is assigned to those projects.\n\noptaion A is most feasible solution as it just create authorized view with aggregated data so storage cost can me minimize also this won't compromise the access.","poster":"dish11dish"},{"comment_id":"720463","content":"Selected Answer: B\nB, as Joey_chicity explains","timestamp":"1700226240.0","upvote_count":"1","poster":"gudiking"},{"poster":"sedado77","comment_id":"668444","content":"Selected Answer: A\nA is correct","timestamp":"1694643420.0","upvote_count":"3"},{"comment_id":"658397","upvote_count":"1","timestamp":"1693741860.0","poster":"AWSandeep","content":"Selected Answer: A\nA. Create and share an authorized view that provides the aggregate results."}],"question_id":293,"exam_id":11,"timestamp":"2022-09-03 13:51:00"},{"id":"lPx1np40BeUZuONKQmsT","answers_community":["B (61%)","D (37%)","2%"],"discussion":[{"upvote_count":"50","comment_id":"315800","comments":[{"content":"I have no idea why so many upvotes on this answer:\n1) archived doesn't mean immutable and cloud storage is not immutable too.\n2) auditable means viewable for authorized personel - and in this case not changes need to be monitored but any access.\n3) with option D it is easy to go around logging - you can add another access to the bucket read the data remove the access and no one will ever know that you accessed the data.\n4) option D is much more difficult - you need to application on AppEngine to log the data and provide access for users.\n5) option D doesn;t explain where and how it stores the audit data - it could be accessed and modified from some side app/service.","comment_id":"888751","timestamp":"1699036860.0","upvote_count":"11","poster":"Jarek7"}],"timestamp":"1632155220.0","poster":"Mitra123","content":"Keywords here are\n1. \"Archived\": Immutable and hence, BQ and Cloud SQL are ruled out\n2. \"Auditable\": Means track any changes done. \nOnly D can provide the audibility piece!\nI will go with D"},{"content":"Answer: B\nDescription: Bigquery is used to analyse access logs, data access logs capture the details of the user that accessed the data","timestamp":"1601258220.0","poster":"[Removed]","upvote_count":"23","comment_id":"68738","comments":[{"poster":"sraakesh95","content":"There is no option for archiving with BQ","timestamp":"1657924920.0","upvote_count":"1","comment_id":"524528","comments":[{"poster":"tavva_prudhvi","upvote_count":"5","comments":[{"content":"The question is about where to store the _data_ for which the logs will be generated. \n\nThe bit you quoted is about the _logs_ that will be generated when accesssing data. The “archived correctly” implies that proper retention policies will be set up if you choose GCS.","comment_id":"631923","timestamp":"1673820900.0","poster":"vartiklis","upvote_count":"3"}],"comment_id":"586811","content":"You dont need to archive the expiring logs, you have to archive the un-archived data here! See the question, it says \"Assuming that all expiring logs will be archived correctly\", which means they are already stored somewhere like in GCS!!! Hence, better to store the remaining un-archived data in BQ.","timestamp":"1665930180.0"}]},{"poster":"awssp12345","content":"The question has no mention of ANALYZE.. BQ is not correct. I would go with D.","timestamp":"1641394440.0","upvote_count":"12","comment_id":"399184"}]},{"poster":"philli1011","upvote_count":"1","timestamp":"1723000500.0","content":"In recent GCP, we have cloud audit.","comment_id":"1142965"},{"upvote_count":"1","poster":"Nandababy","timestamp":"1717327260.0","comment_id":"1086201","content":"Option B is valid only when analytics to be performed over logs, which is not mentioned anywhere"},{"comment_id":"1084836","poster":"rocky48","content":"Selected Answer: B\nFor maintaining an auditable record of access to certain types of data, especially when government regulations are in place, the most suitable option would be:\n\nB. In a BigQuery dataset that is viewable only by authorized personnel, with the Data Access log used to provide the auditability.\n\nStoring the data in a BigQuery dataset with restricted access ensures control over who can view the data, and utilizing Data Access logs provides a comprehensive audit trail for compliance purposes. This option aligns well with the need for maintaining an auditable record as mandated by government regulations.","timestamp":"1717191480.0","upvote_count":"3"},{"timestamp":"1699037580.0","comments":[{"timestamp":"1700162820.0","content":"That was my thought, either B or D could work but D it’s a little bit odd create an app to do something that could be achieved natively gcp","poster":"Kiroo","upvote_count":"3","comment_id":"899427"},{"poster":"phidelics","comment_id":"915534","timestamp":"1701797640.0","content":"I was about to say the same thing. Why go through that stress?","upvote_count":"2"}],"upvote_count":"9","comment_id":"888760","poster":"Jarek7","content":"Selected Answer: B\nIf you are going for option D, why do you eliminate option B? The only REAL difference is that for opption D you need to develop an app for storing log data and providing bucket link and in option B you have it all done BETTER by GCP. You might also pay a bit more for BQ storage, but the question never mentions about cost optimization.\nBTW in the D option the bucket is accessible only by AppEngine service, so what will the user do with the provided link? he has no access anyway... And if he even has the access to this link what stops him form using the same link many times? How the AppEngine get and store the information what specific data he accessed and how?"},{"upvote_count":"2","content":"Selected Answer: D\nD amongus","comment_id":"875677","timestamp":"1697810520.0","poster":"Rodrigo4N"},{"poster":"juliobs","content":"Selected Answer: B\nThey want to know where you can store **data** in a way that every access is logged in an auditable way.\n\nBoth BQ and GCS have audit logs, except that in alternative D you're circumventing it by creating your own logs. I doubt Google would recommend that.\n\nBy types of data you can understand \"financial type\", \"marketing type\", etc.","timestamp":"1695488760.0","upvote_count":"3","comment_id":"848578"},{"upvote_count":"4","poster":"midgoo","comment_id":"826470","timestamp":"1693618020.0","content":"Selected Answer: D\nI was thinking it should be A. However, 'data' in this question is too vague. It does not say anywhere that the data could fit in BigQuery tables. It could be unstructure data such as videos or images\nOption D seems to involve more setup but it is the only viable option for this scenario. Note that GCS do have Cloud Audit logs. That should be the best option. Maybe this question was asked when Cloud Audit log is not yet available for GCS."},{"timestamp":"1690622640.0","comment_id":"791581","poster":"aleixfc96","content":"It is so clear that is B lol","upvote_count":"1"},{"poster":"NamitSehgal","content":"B bigquery for a record set store","upvote_count":"1","timestamp":"1690594260.0","comment_id":"791361"},{"upvote_count":"2","content":"Selected Answer: B\nB. In a BigQuery dataset that is viewable only by authorized personnel, with the Data Access log used to provide the auditability.\n\nBigQuery provides built-in logging of all data access, including the user's identity, the specific query run and the time of the query. This log can be used to provide an auditable record of access to the data. Additionally, BigQuery allows you to control access to the dataset using Identity and Access Management (IAM) roles, so you can ensure that only authorized personnel can view the dataset.","poster":"PolyMoe","timestamp":"1690441920.0","comment_id":"789456"},{"poster":"samdhimal","content":"B. In a BigQuery dataset that is viewable only by authorized personnel, with the Data Access log used to provide the auditability.\n\nBigQuery provides built-in logging of all data access, including the user's identity, the specific query run and the time of the query. This log can be used to provide an auditable record of access to the data. Additionally, BigQuery allows you to control access to the dataset using Identity and Access Management (IAM) roles, so you can ensure that only authorized personnel can view the dataset.","comments":[{"content":"gpt: You are correct that option A does not provide an auditable record of access to the data, as it only addresses data security through encryption. Option C provides auditability through Cloud SQL Admin activity logs, but it may not be the best option as it requires additional setup and management.\n\nOption D is a feasible solution, but as you mentioned, it requires additional setup and maintenance of the AppEngine service. It also may not provide a comprehensive audit log of all data access.\n\nOption B, storing the data in a BigQuery dataset that is viewable only by authorized personnel and using the Data Access log to provide auditability, is the most appropriate option as it provides built-in logging of all data access and allows you to control access to the dataset using IAM roles. Therefore, it provides both data security and auditable access to the data. /// ok let it be B","comment_id":"880113","poster":"Oleksandr0501","upvote_count":"2","comments":[{"timestamp":"1698404880.0","content":"OR MAYBE D....","comments":[{"upvote_count":"1","content":"!!! confused. Give 69% confidence to B, as user Jarek7 explained","comment_id":"894198","timestamp":"1699643100.0","poster":"Oleksandr0501"}],"upvote_count":"1","comment_id":"882564","poster":"Oleksandr0501"}],"timestamp":"1698221280.0"},{"timestamp":"1690159920.0","upvote_count":"2","comment_id":"786095","content":"A. Encrypted on Cloud Storage with user-supplied encryption keys. A separate decryption key will be given to each authorized user. is a good option for data security but it does not provide an auditable record of access to the data.\n\nC. In Cloud SQL, with separate database user names to each user. The Cloud SQL Admin activity logs will be used to provide the auditability. is also a good option for data security but it does not provide an auditable record of access to the data.\n\nD. In a bucket on Cloud Storage that is accessible only by an AppEngine service that collects user information and logs the access before providing a link to the bucket. is also a good option but it requires additional setup and maintenance of the AppEngine service, and it may not provide an auditable record of access to the data.","poster":"samdhimal"}],"upvote_count":"3","timestamp":"1690159920.0","comment_id":"786094"},{"upvote_count":"1","timestamp":"1689841920.0","poster":"GCPpro","comment_id":"782112","content":"D is the correct answer"},{"content":"Selected Answer: D\nKeys\nTYPES of data --> Cloud Storage not BQ \nArchival --> Cloud Storage\nAccess --> No decryption keys to all users","upvote_count":"1","timestamp":"1689115560.0","comment_id":"773001","poster":"RoshanAshraf"},{"content":"Selected Answer: D\nI will go with D","upvote_count":"1","poster":"PrashantGupta1616","comment_id":"759377","timestamp":"1687921920.0"},{"timestamp":"1686789840.0","comment_id":"745647","poster":"DGames","upvote_count":"1","content":"Selected Answer: D\nKeyword, Archiver , certain type of data, auditable, GCS is better option . Durability 11 time 9 to store log immutable for long time."},{"poster":"zellck","upvote_count":"2","content":"Selected Answer: B\nB is the answer.","comment_id":"736843","comments":[{"content":"are you assuming that data is in BQ compatible format?","timestamp":"1687923720.0","comment_id":"759401","poster":"Wonka87","upvote_count":"2"}],"timestamp":"1686050580.0"},{"poster":"cloudmon","timestamp":"1683389700.0","upvote_count":"1","content":"Selected Answer: A\nJust store it in GCS. Cloud Audit logging is enforced, and will provide the required auditability logs. D is overkill.","comment_id":"712549","comments":[{"poster":"z83j","timestamp":"1683737580.0","upvote_count":"2","content":"In what world is user-supplied encryiption keys and seperate decryption key to each user account for audibility records? It's also just bad practice to give everyone a personal key and hope they don't get stolen somewhere down the road from any of the users.","comment_id":"715482"},{"comment_id":"712564","timestamp":"1683390900.0","poster":"cloudmon","upvote_count":"1","content":"Reference: https://cloud.google.com/storage/docs/audit-logging#available-logs"}]},{"comment_id":"676668","content":"Its B, question is asking where will the data set reside","upvote_count":"2","timestamp":"1679540100.0","poster":"clouditis"},{"timestamp":"1678535520.0","content":"Selected Answer: B\n\"record of ACCESS to certain types of data.\" It's not the data, it's the access of it, so no image or video data is stored, I go with B","comment_id":"665994","upvote_count":"2","poster":"crismo04"},{"poster":"Azlijaffar","upvote_count":"2","timestamp":"1678210800.0","comments":[{"comment_id":"699026","upvote_count":"1","poster":"Azlijaffar","timestamp":"1681907520.0","content":"\"where should you store DATA that is subject to that mandate?\". Where to store the DATA, not the record or the logs. The DATA that could be of CERTAIN TYPES. At least that's how I understand this tricky question. Will go with D."}],"comment_id":"662653","content":"Certain types of data is subjected to this mandate. Data can be audio files etc. You need to keep these files on Cloud Storage and not BigQuery. \nD says the data is stored in a bucket in GCS (correct) and users that access these data will be provided with a link to the storage bucket of that data after their user information is logged for accessing said data.\nShould be D."},{"comment_id":"650785","poster":"ducc","timestamp":"1677159600.0","content":"Selected Answer: D\nThere are audio, image,...","upvote_count":"2"},{"timestamp":"1672227960.0","poster":"alecuba16","comments":[{"comment_id":"653668","upvote_count":"1","content":"Record from government, there might be Audio, Photo,...","timestamp":"1677543180.0","poster":"ducc"}],"content":"Selected Answer: B\n-Create a dataset with the permissions\n-Create a federated table with GCS as source (the logs are already in GCS)\n-Set audit on","comment_id":"623881","upvote_count":"1"},{"timestamp":"1670531940.0","poster":"AmirN","content":"I would gravitate towards BigQuery first, and it seems more simple than maintaining an app engine service indefinitely which may cost a lot.\nBUT, the question states access to 'certain types of data'. How do you know that the data isn't audio files and images? Cloud Storage will cover all data types but BQ cannot.","upvote_count":"1","comment_id":"613461"},{"content":"B. \nhttps://cloud.google.com/bigquery/docs/reference/auditlogs#data_access_data_access","timestamp":"1668240600.0","comment_id":"600481","poster":"sw52099","upvote_count":"1"},{"upvote_count":"1","poster":"Nikhil_Devadiga","timestamp":"1667784660.0","comment_id":"597915","content":"Selected Answer: D\nAchieving in BQ is harder than Storage"},{"content":"Selected Answer: B\nThe question is \"where do you store data\", not logs. You store data in BQ. Data access logs provide auditability. Expiring logs are \"archived correctly\", it is not a concern in this question.","timestamp":"1666899360.0","comment_id":"593370","poster":"i_b1","upvote_count":"3"},{"content":"Selected Answer: D\nlow coast !","timestamp":"1666790340.0","comment_id":"592457","upvote_count":"1","poster":"ryless"},{"timestamp":"1666454040.0","comment_id":"590097","poster":"NR22","upvote_count":"1","content":"Selected Answer: D\nNo active analysis required so BQ would be overkill"},{"comment_id":"586332","poster":"Didine_22","upvote_count":"1","content":"Selected Answer: D\nD\nThe data must be \"archived\"","timestamp":"1665835740.0"},{"timestamp":"1665124920.0","comments":[{"timestamp":"1687923660.0","comment_id":"759398","poster":"Wonka87","upvote_count":"1","content":"but can you store all types of data on BQ? Why GCS is preferred usually to store any kind of data?"}],"content":"Guys, here they are not talking about the archived logs, they have already stored in GCS as stated in the question which says, \"Assuming that all expiring logs will be archived correctly\", what do you mean by that? it means, they are expecting the expiring logs will be archived correctly (like in GCS!) then what about the data that they must alter based on the govt regulations? Then, B looks good here.","poster":"tavva_prudhvi","upvote_count":"1","comment_id":"582226"},{"timestamp":"1665063540.0","content":"Selected Answer: D\n\"Archived\"","poster":"MATHICHOU","comment_id":"581846","upvote_count":"1"},{"comment_id":"576629","content":"Selected Answer: D\nWe often hear that customers have large amounts of bulk data that needs to be retained due to regulatory compliance, as well as for data protection. Use the Archive class in conjunction with Bucket Lock to ensure that objects will be retained without modification for a period of time that you can choose. This is helpful to meet legal retention requirements in many industries, especially in healthcare and financial services.\nhttps://cloud.google.com/blog/products/storage-data-transfer/archive-storage-class-for-coldest-data-now-available","upvote_count":"1","comments":[{"content":"Audit logging is definitely possible on GCS\nhttps://cloud.google.com/storage/docs/audit-logging","timestamp":"1664342760.0","comment_id":"576631","upvote_count":"3","poster":"lichstwo"}],"poster":"lichstwo","timestamp":"1664342580.0"},{"timestamp":"1662559080.0","poster":"Arkon88","comment_id":"562749","upvote_count":"1","content":"Selected Answer: D\nD. In a bucket on Cloud Storage that is accessible only by an AppEngine service that collects user information and logs the access before providing a link to the bucket.\n\n1. \"Archived\": Immutable and hence, BQ and Cloud SQL are ruled out\n2. \"Auditable\": Means track any changes done.\n3. Big query for togs is too expensive plus nobody asks to provide analitycs"},{"timestamp":"1662483120.0","upvote_count":"1","content":"Selected Answer: D\nMake sense for archiving purpose","comment_id":"562218","poster":"Jatinmaya"},{"content":"Selected Answer: D\nGCS meets archiving requirement with less cost and security","poster":"rbeeraka","timestamp":"1658599320.0","upvote_count":"1","comment_id":"530784"},{"upvote_count":"9","poster":"Jambalaja","content":"Selected Answer: B\nThe questions says: \n1. You need to \"maintain an auditable record of access to certain types of data\" => you need to be able to access and audit that data\n2. \"Assuming that all expiring logs will be archived correctly\" => means that all expired logs are \"already\" being archived correctly (e.g. in Google Cloud Storage). \n3. \"where should you store data that is subject to that mandate?\" => where do you store the data **that you can audit to answer the question regarding data access logs (who accessed what, this is the questions you need to answer when regulatory department asks, so you need a suitable storage to answer this question)**\n\nTherefore the answer is B. You store the un-archived data in BigQuery and use BigQuery for audit purpose to answer questions regarding data access in the past. The old data that is no longer subject for data access questions is archived already, like described in the question.","comment_id":"528278","timestamp":"1658301300.0"},{"upvote_count":"1","poster":"medeis_jar","content":"Selected Answer: B\nhttps://cloud.google.com/bigquery/docs/reference/auditlogs/","timestamp":"1657032720.0","comment_id":"517667"},{"comment_id":"512706","comments":[{"poster":"BigDataBB","timestamp":"1659601620.0","upvote_count":"2","content":"This is the smoking gun ;)","comment_id":"540358"}],"upvote_count":"3","poster":"kuik","content":"I will go with B.\nhttps://cloud.google.com/architecture/exporting-stackdriver-logging-for-security-and-access-analytics","timestamp":"1656528600.0"},{"content":"Selected Answer: B\nWith A we won’t have audit, C doesnt make sense, neither does D.\nWith B, only allowed users will have access and it also access log enables auditability.\nhttps://cloud.google.com/bigquery/docs/reference/auditlogs/","timestamp":"1655958600.0","comment_id":"507633","poster":"MaxNRG","upvote_count":"1"},{"comment_id":"489862","content":"Selected Answer: D\nThe answer is D: https://cloud.google.com/architecture/exporting-stackdriver-logging-for-compliance-requirements","upvote_count":"1","timestamp":"1653819780.0","poster":"StefanoG"},{"content":"B: My guess is to store data in Big Query and to use the already existing Audit logging function.","comment_id":"477353","timestamp":"1652424360.0","poster":"Thierry_1","upvote_count":"1"},{"poster":"JayZeeLee","upvote_count":"3","content":"D. \nI see a lot of Bs here. BigQuery won't work in this case because it's better used to analyses. This question mentions no analysis. It's asking to \"store\". So choosing B would be having all these logs sitting in BigQuery not being used. If that's the case, Cloud Storage is a better fit, recommended by Google.","comments":[{"comment_id":"582234","upvote_count":"1","poster":"tavva_prudhvi","content":"And what do you want to store? read the question properly, it's not asking to store the archived logs, its asking us to store the un-archived data!","timestamp":"1665125400.0"}],"timestamp":"1651788960.0","comment_id":"473257"},{"timestamp":"1650288600.0","poster":"squishy_fishy","comment_id":"464146","upvote_count":"1","content":"The answer is D.\nBigquery is not the best option here to store the auditable record of access to certain types of data, because to comply with the government regulations, businesses have to keep records of for up to 7 years. Cloud Storage is cheaper for file storages and can be used as external federated source and queried through big query."},{"upvote_count":"3","poster":"ManojT","timestamp":"1649474040.0","content":"Answer B: \"auditable record of access to certain types of data\" only BQ can provide access to auditable records for certain type of access as it would be required to query the DB for such kind of questions.","comment_id":"459466"},{"poster":"nguyenmoon","upvote_count":"5","comment_id":"445522","timestamp":"1647393660.0","content":"BQ is for analysis and it's expensive if just using for storing audit logs. I would go with D"},{"content":"option B makes the most sense as we need to keep auditable record of each personal accessing the data also Bigquery provides as cheap as archive storage tier charges for archive storage","comment_id":"421755","upvote_count":"1","comments":[{"poster":"triipinbee","comment_id":"432549","upvote_count":"5","content":"BQ is s tool not for archival, it's for analysis. Cloud storage the correct place to archive.","timestamp":"1645912260.0"}],"timestamp":"1644348600.0","poster":"sandipk91"},{"timestamp":"1640828160.0","comments":[{"comments":[{"poster":"sumanshu","upvote_count":"3","timestamp":"1641685800.0","comments":[{"timestamp":"1641685800.0","upvote_count":"3","content":"Logs need to be Archive, So we need to select Cloud Storage Bucket (option) - So D","comment_id":"402258","poster":"sumanshu","comments":[{"upvote_count":"2","content":"but in 'D' what about access to logs ? any application can access the logs via Service account","comment_id":"402259","comments":[{"content":"B is okay than - https://cloud.google.com/logging/docs/audit/best-practices#export-best-practices","poster":"sumanshu","upvote_count":"3","timestamp":"1641686040.0","comment_id":"402260"}],"timestamp":"1641685920.0","poster":"sumanshu"}]}],"content":"Yes, Should be D","comment_id":"402257"}],"poster":"awssp12345","content":"Why not D?","comment_id":"399207","timestamp":"1641396600.0","upvote_count":"3"}],"content":"Ideally logs needs to be stored in Cloud Storage in Archive Class.\nBut here, in option 'A', user supplied encryption keys, and then seperate decryption keys for each user - does not make any sense ...\n\nThus looks, Option 'B' is more relevant as compare to A","poster":"sumanshu","comment_id":"394264","upvote_count":"1"},{"content":"answer B:\nWhen you export log entries, they are written to a destination or sink, which\ncould be Cloud Storage, BigQuery, or Cloud Pub/Sub, then only A and B is correct. and it says \"auditable\" it means queryable dataset or SQL compatible database, then Bigquery is correct.\nhttps://cloud.google.com/logging/docs/export/aggregated_sinks#creating_the_export_destination\nhttps://cloud.google.com/logging/docs/audit/best-practices#export-best-practices","comment_id":"307983","timestamp":"1631366460.0","comments":[{"poster":"squishy_fishy","content":"It can't be B, because \"certain types of data\" could be logs in the stackdriver, not data in BigQuery.","comment_id":"456839","upvote_count":"1","timestamp":"1649035560.0"}],"poster":"daghayeghi","upvote_count":"5"},{"upvote_count":"6","content":"B since data needs to be auditable and google recommends keeping long term data in bigquery since it is charged at nearline cloud storage if the partition in a table is not accessed for 90 days.\nhttps://cloud.google.com/bigquery/docs/best-practices-storage#take_advantage_of_long-term_storage","timestamp":"1626648540.0","comment_id":"270843","poster":"lammingtons"},{"comment_id":"175197","content":"B: To help you meet these security and analytics requirements, Cloud Logging can capture two types of audit logs: Admin Activity logs and Data Access logs.\nhttps://cloud.google.com/solutions/exporting-stackdriver-logging-for-security-and-access-analytics","poster":"oku","upvote_count":"4","timestamp":"1615125960.0"},{"upvote_count":"4","comment_id":"172135","poster":"sh2020","timestamp":"1614707460.0","content":"Why is D not correct?"},{"timestamp":"1613785080.0","content":"B is correct. \nData Access audit logs are off by default. When you enable new Google Cloud services, evaluate whether or not to enable Data Access audit logs for that new service. Only BigQuery has Data Access audit logs enabled by default.\nhttps://cloud.google.com/logging/docs/audit/best-practices#export-best-practices\n\nNon-chargeable logs\nThe following logs are free and don't count towards the logs allotment limit:\n\nCloud Audit Logging logs that are enabled by default. This includes all Admin Activity audit logs and System Event audit logs.\nhttps://cloud.google.com/stackdriver/pricing#logging-costs","upvote_count":"7","comment_id":"161858","poster":"haroldbenites"},{"content":"B is correct answer, Read below link for more clarification.\nhttps://cloud.google.com/logging/docs/audit/best-practices#export-best-practices","upvote_count":"7","comment_id":"160721","poster":"saurabh1805","timestamp":"1613643300.0"},{"poster":"evangelist","timestamp":"1612330920.0","upvote_count":"5","content":"A is incorrect because whenever it says \"auditable\" it means querable dataset in an SQL compatible database not a simple file object","comment_id":"149496"},{"comment_id":"119057","comments":[{"timestamp":"1612343280.0","poster":"MadHolm","comment_id":"149585","content":"I believe it's because it's a structured log data, hence BigQuery is a better option. The logs has to be aduitable, so if you stored them in GCS you'd still need another service to view them, that would increase the cost and could lead to possible security violations.","upvote_count":"5"}],"poster":"dambilwa","content":"I wonder why Option [A] is incorrect","timestamp":"1608873120.0","upvote_count":"3"},{"timestamp":"1600700460.0","comment_id":"66599","upvote_count":"6","poster":"[Removed]","content":"Answer: B"}],"choices":{"A":"Encrypted on Cloud Storage with user-supplied encryption keys. A separate decryption key will be given to each authorized user.","D":"In a bucket on Cloud Storage that is accessible only by an AppEngine service that collects user information and logs the access before providing a link to the bucket.","B":"In a BigQuery dataset that is viewable only by authorized personnel, with the Data Access log used to provide the auditability.","C":"In Cloud SQL, with separate database user names to each user. The Cloud SQL Admin activity logs will be used to provide the auditability."},"answer_description":"","unix_timestamp":1584810060,"answer_images":[],"answer_ET":"B","answer":"B","question_images":[],"question_id":294,"isMC":true,"topic":"1","timestamp":"2020-03-21 18:01:00","exam_id":11,"url":"https://www.examtopics.com/discussions/google/view/17114-exam-professional-data-engineer-topic-1-question-76/","question_text":"Government regulations in your industry mandate that you have to maintain an auditable record of access to certain types of data. Assuming that all expiring logs will be archived correctly, where should you store data that is subject to that mandate?"},{"id":"zITMRCb9EQxOEWzoGmZZ","answer_description":"","choices":{"C":"Increase the number of input features to your model.","B":"Subsample your training dataset.","A":"Subsample your test dataset.","D":"Increase the number of layers in your neural network."},"answer":"B","discussion":[{"poster":"mantwosmart","upvote_count":"9","timestamp":"1715349480.0","content":"Answer: B. Subsample your training dataset.\n\nSubsampling your training dataset can help increase the training speed of your neural network model. By reducing the size of your training dataset, you can speed up the process of updating the weights in your neural network. This can help you quickly test and iterate your model to improve its accuracy.\n\nSubsampling your test dataset, on the other hand, can lead to inaccurate evaluation of your model's performance and may result in overfitting. It is important to evaluate your model's performance on a representative test dataset to ensure that it can generalize to new data.\n\nIncreasing the number of input features or layers in your neural network can also improve its performance, but this may not necessarily increase the training speed. In fact, adding more layers or features can increase the complexity of your model and make it take longer to train. It is important to balance the model's complexity with its performance and training time.","comment_id":"894048"},{"content":"Selected Answer: B\nB is correct","timestamp":"1722715080.0","poster":"crazycosmos","upvote_count":"3","comment_id":"971433"},{"poster":"Vipul1600","content":"B should be correct. Increasing the layers can also decrease the training time but may introduce vanishing gradient hence D may not be correct","upvote_count":"1","comment_id":"969714","timestamp":"1722576000.0"},{"timestamp":"1714418640.0","upvote_count":"1","poster":"email2nn","comment_id":"884637","content":"answer is B"},{"poster":"juliobs","content":"Selected Answer: B\nReduce training time and probably accuracy too.","timestamp":"1711221300.0","upvote_count":"2","comment_id":"848594"},{"timestamp":"1708644120.0","comment_id":"818536","poster":"MingSer","upvote_count":"1","content":"Selected Answer: B\nall other are wrong"},{"poster":"PolyMoe","upvote_count":"1","content":"Selected Answer: B\nof course !","timestamp":"1706347560.0","comment_id":"789466"},{"content":"B. Subsampling your training dataset can decrease the amount of data the model needs to process and can speed up training time. However, it can lead to decrease in the model's accuracy.\n\nAlthough it shouldn't matter since we are not even in testing phase yet and we aren't looking for accuracy.","upvote_count":"2","timestamp":"1706065020.0","poster":"samdhimal","comment_id":"786103"},{"upvote_count":"2","poster":"GCPpro","content":"B is the answer as we are bothered about speed not the accuracy.","comment_id":"782114","timestamp":"1705746840.0"},{"poster":"ler_mp","upvote_count":"1","content":"Selected Answer: B\nThe answer is B. Building a more complex model by increasing the number of layer will not reduce the training time.","timestamp":"1704777000.0","comment_id":"770040"},{"timestamp":"1703059740.0","comment_id":"750635","content":"Selected Answer: B\nBy SubSampling the training data, you will reduce the training time. \n\nIn case of D, if you increase the number of layers, then the model's accuracy will be increased. But it will not reduce the time required to train the model.","upvote_count":"4","poster":"slade_wilson"},{"upvote_count":"1","comment_id":"745650","timestamp":"1702608600.0","poster":"DGames","content":"Selected Answer: D\nIncrease speed of the help to train quicker.. option B is sub sample that also help but it drop accurately of model . So I think Option D is good to go.","comments":[{"upvote_count":"2","content":"That makes speed of training model lower absolutely. because not only throughput of inference but back-propagation calculation would be increase so, D should be not a answer. there is only answer in those options is B. while it makes dropping performance","timestamp":"1709258100.0","poster":"jin0","comment_id":"825467"}]},{"content":"Selected Answer: B\nB is the answer.","upvote_count":"1","poster":"zellck","comment_id":"736837","timestamp":"1701868740.0"},{"content":"Selected Answer: B\nIt is B. D would improve the accuracy, not speed.","timestamp":"1696640160.0","poster":"pluiedust","comment_id":"688181","upvote_count":"4"},{"timestamp":"1696117740.0","upvote_count":"1","poster":"Chavoz","content":"It's B. D Would be for increase performance","comment_id":"683890"},{"upvote_count":"1","content":"if you Increase the number of layers, you increase the training time, right?","poster":"crismo04","timestamp":"1694426220.0","comment_id":"665998"},{"poster":"HarshKothari21","upvote_count":"1","content":"Both B and D seems correct.","comment_id":"663792","timestamp":"1694190900.0","comments":[{"upvote_count":"3","poster":"jkhong","timestamp":"1702284660.0","content":"Increasing D will increase training time","comment_id":"741551"}]},{"content":"Selected Answer: B\nOnly valid awnser","timestamp":"1693645260.0","comment_id":"657206","poster":"YorelNation","upvote_count":"2"}],"url":"https://www.examtopics.com/discussions/google/view/79343-exam-professional-data-engineer-topic-1-question-77/","topic":"1","timestamp":"2022-09-02 11:01:00","question_id":295,"question_images":[],"answers_community":["B (95%)","5%"],"exam_id":11,"unix_timestamp":1662109260,"answer_ET":"B","isMC":true,"question_text":"Your neural network model is taking days to train. You want to increase the training speed. What can you do?","answer_images":[]}],"exam":{"isImplemented":true,"isBeta":false,"isMCOnly":true,"lastUpdated":"11 Apr 2025","name":"Professional Data Engineer","provider":"Google","numberOfQuestions":319,"id":11},"currentPage":59},"__N_SSP":true}