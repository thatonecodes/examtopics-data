{"pageProps":{"questions":[{"id":"y3DH4ZZSPCJRWVfKRZkx","topic":"1","answer":"A","discussion":[{"comment_id":"68740","timestamp":"1585368060.0","content":"Answer: A\nDescription: Pig is scripting language which can be used for checkpointing and splitting pipelines","upvote_count":"23","poster":"[Removed]"},{"content":"Should be A","poster":"[Removed]","timestamp":"1584810660.0","comment_id":"66606","upvote_count":"15"},{"poster":"Parandhaman_Margan","upvote_count":"1","timestamp":"1742051160.0","content":"Selected Answer: C\nMapReduce in Java (C) allows more control for checkpointing and splitting.","comment_id":"1398902"},{"upvote_count":"1","content":"Selected Answer: C\nWriting your pipeline in Java using MapReduce allows you to implement these custom controls and fine-tune the execution, ensuring robust and manageable ETL processes on your Hadoop cluster","comment_id":"1398844","poster":"desertlotus1211","timestamp":"1742045040.0"},{"poster":"grshankar9","timestamp":"1737204240.0","comment_id":"1342508","upvote_count":"1","content":"Selected Answer: A\nPig Latin supports both splitting pipelines and checkpointing, allowing users to create complex data processing workflows with the ability to restart from specific points in the pipeline if necessary."},{"upvote_count":"1","content":"Selected Answer: A\nI would go to A. \nC, D are similar. So both are excluded. B, Hive is actually a data warehouse system. I don't use Apache Pig. But, BCD are wrong. Then A should be correct.","poster":"SamuelTsch","comment_id":"1302103","timestamp":"1729698660.0"},{"content":"Selected Answer: A\nA as others have said","poster":"AnonymousPanda","timestamp":"1692603480.0","comment_id":"986292","upvote_count":"1"},{"poster":"Oleksandr0501","upvote_count":"2","comment_id":"880124","content":"Selected Answer: C\nComment content is too short","timestamp":"1682410560.0"},{"upvote_count":"2","comment_id":"848636","timestamp":"1679602800.0","content":"Selected Answer: A\nPigLatin is the correct answer, however... the last release was 6 years ago and has lots of bugs.","poster":"juliobs"},{"upvote_count":"7","poster":"musumusu","content":"This answer depends which language you are comfortable with. \nHadoop is your framework, where mapReduce is your Native programming model in JAVA, which is designed to scale, parallel processing, restart pipeline from any checkpoint etc. , So if you are comfortable with JAVA, you can customize your checkpoint at lowlevel in better way. otherwise, choose PIG which is another programming concept run over JAVA but then you need to learn this also, if not choose python as it can be deployed with hadoop because hadoop has been making updates for python clients regularly. \nOption C: is the best one.","timestamp":"1676455320.0","comment_id":"809336"},{"comment_id":"788942","upvote_count":"4","content":"C. Java using MapReduce or D. Python using MapReduce\n\nApache Hadoop is a distributed computing framework that allows you to process large datasets using the MapReduce programming model. There are several options for writing ETL pipelines to run on a Hadoop cluster, but the most common are using Java or Python with the MapReduce programming model.","comments":[{"upvote_count":"3","content":"A. PigLatin using Pig is a high-level data flow language that is used to create ETL pipelines. Pig is built on top of Hadoop, and it allows you to write scripts in PigLatin, a SQL-like language that is used to process data in Hadoop. Pig is a simpler option than MapReduce but it lacks some capabilities like the control over low-level data manipulation operations.\n\nB. HiveQL using Hive is a SQL-like language for querying and managing large datasets stored in Hadoop's distributed file system. Hive is built on top of Hadoop and it provides an SQL-like interface for querying data stored in Hadoop. Hive is more suitable for querying and managing large datasets stored in Hadoop than for ETL pipelines.\n\nBoth Java and Python using MapReduce provide low-level control over data manipulation operations, and they allow you to write custom mapper and reducer functions that can be used to process data in a Hadoop cluster. The choice between Java and Python will depend on the development team's expertise and preference.","comment_id":"788943","comments":[{"comment_id":"905768","poster":"cetanx","timestamp":"1684926000.0","upvote_count":"2","content":"It has to be C \nbecause while Pig can be used to simplify the writing of complex data transformation tasks and can store intermediate results, it doesn't provide the detailed control over checkpointing and pipeline splitting in the way that is typically implied by those terms.\n\nalso, while one can write MapReduce jobs in languages other than Java (like Python) using Hadoop Streaming or other similar APIs, it may not be as efficient or as seamless as using Java due to the JVM-native nature of Hadoop."}],"timestamp":"1674751980.0","poster":"samdhimal"}],"poster":"samdhimal","timestamp":"1674751980.0"},{"timestamp":"1662693840.0","comment_id":"664182","content":"Selected Answer: A\nDescription: Pig is scripting language which can be used for checkpointing and splitting pipelines","upvote_count":"1","poster":"Koushik25sep"},{"comment_id":"548470","poster":"BigDataBB","upvote_count":"1","content":"Why not D?","timestamp":"1645005240.0"},{"comment_id":"530786","content":"Selected Answer: A\nPigLatin supports checkpoints","upvote_count":"1","poster":"rbeeraka","timestamp":"1642968780.0"},{"upvote_count":"1","comment_id":"530777","poster":"davidqianwen","timestamp":"1642967340.0","content":"Selected Answer: A\nAnswer: A"},{"poster":"maddy5835","comment_id":"462740","timestamp":"1634320500.0","upvote_count":"3","content":"Pig is just a scripting language, how pig can be used in creation of pipelines, should be answer from c & D"},{"upvote_count":"1","poster":"sumanshu","timestamp":"1625011020.0","content":"Vote for A","comment_id":"394272"},{"comment_id":"292728","timestamp":"1613582460.0","poster":"kdiab","content":"Found this slideset that puts in favor answer A (pig) :\nhttps://poloclub.github.io/cx4242-2019fall-campus/slides/17-CSE6242-612-ScalingUp-hive.pdf","upvote_count":"2"},{"poster":"IsaB","comments":[{"upvote_count":"1","comment_id":"205242","poster":"Pupina","content":"Did you take the exam? I am ready to do it this month","timestamp":"1603559220.0"},{"poster":"MaxNRG","content":"seems like a very old question :)\nnot sure it's actual","timestamp":"1640241960.0","upvote_count":"2","comment_id":"507639"}],"comment_id":"180366","timestamp":"1600263240.0","content":"Is this really a question that could appear in Google Cloud Professional Data Engineer Exam? What does it have to do with Google Cloud? I would use DataProc no?","upvote_count":"10"},{"content":"A is correct","timestamp":"1597880820.0","poster":"haroldbenites","comment_id":"161863","upvote_count":"1"},{"timestamp":"1597739100.0","content":"A should be correct answer here.","comment_id":"160727","upvote_count":"2","poster":"saurabh1805"},{"upvote_count":"8","comment_id":"128213","content":"A, C and D - all are valid answers. You can do checkpointing, splitting pipelines and merging pipelines with all three options.","poster":"dg63","timestamp":"1594064820.0"},{"content":"A\nhttps://books.google.no/books?id=uL59BAAAQBAJ&pg=PA263&lpg=PA263&dq=pig+checkpoint+hadoop&source=bl&ots=8mqgffDBTQ&sig=ACfU3U0NDfWpLKsQcdtjwIor3f85_m8zJA&hl=en&sa=X&ved=2ahUKEwihhq61maTqAhXSwqYKHS7JDXkQ6AEwAHoECAkQAQ#v=onepage&q=pig%20checkpoint%20hadoop&f=false","upvote_count":"2","timestamp":"1593330240.0","comment_id":"121712","poster":"norwayping"},{"upvote_count":"3","comment_id":"120214","timestamp":"1593150240.0","poster":"dambilwa","content":"Option [A] - Pig Latin - Checkpoint & splits:\nhttps://books.google.co.in/books?id=daGiDQAAQBAJ&pg=PT128&lpg=PT128&dq=pig+latin+splits+checkpoint&source=bl&ots=zTvVN_7Sh-&sig=ACfU3U1_Aug8U4ZfU7VUl9WG0kf78weHPQ&hl=en&sa=X&ved=2ahUKEwjGq_nz_Z7qAhU4zjgGHd-NCrQQ6AEwAHoECAcQAQ"}],"question_images":[],"unix_timestamp":1584810660,"isMC":true,"question_id":296,"timestamp":"2020-03-21 18:11:00","answer_ET":"A","exam_id":11,"answer_images":[],"answers_community":["A (67%)","C (33%)"],"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/17115-exam-professional-data-engineer-topic-1-question-78/","question_text":"You are responsible for writing your company's ETL pipelines to run on an Apache Hadoop cluster. The pipeline will require some checkpointing and splitting pipelines. Which method should you use to write the pipelines?","choices":{"A":"PigLatin using Pig","C":"Java using MapReduce","B":"HiveQL using Hive","D":"Python using MapReduce"}},{"id":"uvWi7OT9HHrOHe6YPmPw","question_id":297,"answer_description":"","answer_ET":"C","timestamp":"2020-03-21 18:17:00","answer_images":[],"isMC":true,"answer":"C","unix_timestamp":1584811020,"discussion":[{"poster":"[Removed]","timestamp":"1600701420.0","comment_id":"66609","upvote_count":"16","content":"correct: C"},{"upvote_count":"15","poster":"[Removed]","content":"Answer: C\nDescription : Speed of data transfer depends on Bandwidth \nFew things in computing highlight the hardware limitations of networks as transferring large amounts of data. Typically you can transfer 1 GB in eight seconds over a 1 Gbps network. If you scale that up to a huge dataset (for example, 100 TB), the transfer time is 12 days. Transferring huge datasets can test the limits of your infrastructure and potentially cause problems for your business.","comment_id":"68741","timestamp":"1601258640.0"},{"content":"Selected Answer: C\nWe are talking about transfer speed. Network transfer speed does not increase with CPU, but with bandwidth. Since there is no other extra information about what the issue, we have to assume that they imply network transfer speed.","comment_id":"960294","timestamp":"1706009280.0","poster":"Mathew106","upvote_count":"1"},{"upvote_count":"3","content":"Selected Answer: C\nTo be honest this question is incomplete, I would go increasing the bandwidth, but first I would analyze why it’s taking long time maybe I’m uploading many files so I could compress and agregate then and upload just one, maybe the target cpu is overloaded at the time of the upload, maybe the target disk reaching the max iops,","timestamp":"1700163360.0","comment_id":"899444","poster":"Kiroo"},{"content":"Selected Answer: C\nEven if transfer server is deployed on the slowest machine available in GCP there is no way it is bottleneck for simple data transfer without any data processing.","poster":"Jarek7","timestamp":"1699038780.0","upvote_count":"1","comment_id":"888775"},{"timestamp":"1698405300.0","comments":[{"comment_id":"888776","timestamp":"1699038840.0","content":"Please stop using GPT as knowledge source. v3.5 is usually wrong even in simple cases. v4 is much better, but it is not designed to be knowledge source. Looking at the answer you must have used v3.5. The question says nothing about cost-effectivness. The issue is data transfer. No any data processing is done on the data while it is transferred. Simple transfer doesn't need much processing power - the real bottleneck even on slowest machines available on GCP must be data transfer - it is obvious.\nBTW for me GPT3.5 said it is C.","poster":"Jarek7","upvote_count":"4","comments":[{"comment_id":"889932","poster":"Oleksandr0501","content":"it should be C, for real, bcz nothing said about cost restrictions in the question. And the user \"snamburi3 \" found docs.","upvote_count":"1","timestamp":"1699185540.0"},{"timestamp":"1699185240.0","poster":"Oleksandr0501","comment_id":"889929","upvote_count":"1","content":"yea, i know it can make mistakes. Thank you.\nThat`s why i always mark \"GPT\" at the start of my answer."}]}],"poster":"Oleksandr0501","content":"Selected Answer: A\nGPT: Option A, increasing the CPU size on the data transfer server, could potentially increase the transfer speeds if the bottleneck in the data transfer process is the processing power of the server. By increasing the CPU size, the server may be able to process data more quickly, leading to faster transfers. \nOption C, increasing the network bandwidth from the datacenter to GCP, could potentially improve the transfer speeds, but it may not be feasible or cost-effective depending on the current infrastructure and network limitations.","upvote_count":"1","comment_id":"882567"},{"content":"Selected Answer: A\nit's refer to data transfer server slow here. not transfer data to cloud slow. \n100% A","comment_id":"879763","upvote_count":"1","timestamp":"1698189240.0","poster":"izekc"},{"upvote_count":"1","comment_id":"829038","poster":"jonathanthezombieboy","content":"Selected Answer: C\nAnswer is C","timestamp":"1693830420.0"},{"comment_id":"825475","timestamp":"1693526880.0","poster":"jin0","upvote_count":"1","content":"This question makes people confused only. there is no refer to network or size of data or something could be referred. the answer could be A or C"},{"timestamp":"1692983940.0","comment_id":"821770","poster":"[Removed]","upvote_count":"1","content":"Answer is C"},{"poster":"mahdiaqim","timestamp":"1692613860.0","content":"Selected Answer: A\nVery confusing question. I selected A because I assume increasing the CPU size on the cloud server is easier to change, as a data engineer, than the bandwidth.","upvote_count":"1","comment_id":"816553"},{"comment_id":"789753","content":"C. Increase your network bandwidth from your datacenter to GCP.\nThis will likely have the most impact on transfer speeds as it addresses the bottleneck in the transfer between your data center and GCP. Increasing the CPU size or the size of the Google Persistent Disk on the server may help with processing the data once it has been transferred, but will not address the bottleneck in the transfer itself. Increasing the network bandwidth from Compute Engine to Cloud Storage would also help with processing the data once it has been transferred but will not address the bottleneck in the transfer itself as well.","timestamp":"1690466220.0","poster":"samdhimal","upvote_count":"3"},{"comment_id":"736814","poster":"zellck","timestamp":"1686048060.0","content":"Selected Answer: C\nC is the answer.","upvote_count":"1"},{"timestamp":"1681281060.0","comment_id":"692767","upvote_count":"1","poster":"Nirca","content":"A bit unprofessional question, having performance issues should be addressed by analyzing and looking for saturation in the system and understanding \"wait-events\". Only than adding more resources."},{"comment_id":"624740","timestamp":"1672334700.0","poster":"rr4444","content":"\"The data are imported to Cloud Storage from your data center through parallel uploads to a data transfer server running on GCP. \"\n\nThis makes zero sense. Is it to GCS or GCE? Question had to make up its mind. Nonsense, literally.","upvote_count":"3"},{"upvote_count":"1","content":"Vote for C. Mostly because other options seems useless","poster":"Thierry_1","timestamp":"1652425140.0","comment_id":"477357"},{"comment_id":"394277","poster":"sumanshu","timestamp":"1640829840.0","content":"Vote for 'C'\n\n You want to maximize transfer speeds.","upvote_count":"3"},{"upvote_count":"2","poster":"Tolberic","comment_id":"287763","timestamp":"1628610660.0","content":"https://cloud.google.com/compute/docs/machine-types#n1_machine_types"},{"timestamp":"1621345860.0","comment_id":"222029","content":"\"The data are imported to Cloud Storage from your data center through parallel uploads to a data transfer server running on GCP.\" \nThis is confusing, as the upload is through a data transfer server on GCP - not directly to Storage. In this case, maybe A?","upvote_count":"1","comments":[{"comment_id":"222031","timestamp":"1621346100.0","poster":"snamburi3","content":"I am going with C as I found a doc: https://cloud.google.com/solutions/migration-to-google-cloud-transferring-your-large-datasets#increasing_network_bandwidth. still a confusing question...","upvote_count":"4"}],"poster":"snamburi3"},{"content":"I guess option A because, increasing processing power for parallel uploads is the first thing to try. And if it doesn't work, then go to bandwidth issues.","timestamp":"1616518080.0","poster":"SureshKotla","upvote_count":"1","comment_id":"185414"},{"upvote_count":"3","poster":"bbozz_","content":"Answer: CA","comment_id":"138627","timestamp":"1611062700.0"},{"timestamp":"1608277740.0","content":"B. Increasing the disk size will increase the write performance and will enable better parallel throughput - this is an easy first thing to try","poster":"Callumr","upvote_count":"3","comment_id":"112947","comments":[{"upvote_count":"2","comments":[{"poster":"atnafu2020","upvote_count":"1","content":"increasing disk or resizing it will have downtime","comment_id":"163255","timestamp":"1613961240.0"}],"comment_id":"162767","content":"I don't think so, parallel throughput depends upon processor rather than the disk size","poster":"ayush955","timestamp":"1613898660.0"}]}],"question_images":[],"answers_community":["C (70%)","A (30%)"],"url":"https://www.examtopics.com/discussions/google/view/17118-exam-professional-data-engineer-topic-1-question-79/","topic":"1","exam_id":11,"question_text":"Your company maintains a hybrid deployment with GCP, where analytics are performed on your anonymized customer data. The data are imported to Cloud\nStorage from your data center through parallel uploads to a data transfer server running on GCP. Management informs you that the daily transfers take too long and have asked you to fix the problem. You want to maximize transfer speeds. Which action should you take?","choices":{"B":"Increase the size of the Google Persistent Disk on your server.","D":"Increase your network bandwidth from Compute Engine to Cloud Storage.","A":"Increase the CPU size on your server.","C":"Increase your network bandwidth from your datacenter to GCP."}},{"id":"YyJfmYc8fHaBVk24L59q","answer":"D","choices":{"A":"Include ORDER BY DESK on timestamp column and LIMIT to 1.","B":"Use GROUP BY on the unique ID column and timestamp column and SUM on the values.","C":"Use the LAG window function with PARTITION by unique ID along with WHERE LAG IS NOT NULL.","D":"Use the ROW_NUMBER window function with PARTITION by unique ID along with WHERE row equals 1."},"answers_community":["D (100%)"],"unix_timestamp":1584258240,"answer_description":"","timestamp":"2020-03-15 08:44:00","question_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/google/view/16641-exam-professional-data-engineer-topic-1-question-8/","answer_images":[],"discussion":[{"content":"I personally don't think any answer is correct, \n\nD is the closest one but it's missing a \"ORDER BY timestamp DESC\" to ensure to get only the latest record based in the timestamp","upvote_count":"12","poster":"Ender_H","comment_id":"681066","timestamp":"1664303460.0","comments":[{"poster":"ndimu","comment_id":"1318837","content":"the idea is you can have multiple events occurring at the same time so the only way to distinct is id","upvote_count":"1","timestamp":"1732732200.0"},{"poster":"Davijde13","upvote_count":"6","comment_id":"776764","content":"The question mention only duplicated data and nothing about taking only the latest ones. Therefore I assume there is no need to always take the latest, we should ensure we take only one record for each ID.","timestamp":"1673798160.0"}]},{"upvote_count":"9","content":"D:\nhttps://cloud.google.com/bigquery/streaming-data-into-bigquery#manually_removing_duplicates","poster":"daghayeghi","timestamp":"1615169760.0","comment_id":"305460"},{"comment_id":"1399890","timestamp":"1742253600.0","content":"Selected Answer: D\nD is closest, as there will always be at least 1 row for each ID. Would have rather used SELECT DISTINCT.","poster":"willyunger","upvote_count":"1"},{"content":"Selected Answer: D\nD ensures data is partitioned by the unique id and only one record is picked thereby ensuring results are de-duplicated","poster":"RT_G","upvote_count":"1","comment_id":"1065070","timestamp":"1727154600.0"},{"comment_id":"1050470","poster":"rtcpost","upvote_count":"2","timestamp":"1727154600.0","content":"Selected Answer: D\nThis approach will assign a row number to each row within a unique ID partition, and by selecting only rows with a row number of 1, you will ensure that duplicates are excluded in your query results. It allows you to filter out redundant rows while retaining the latest or earliest records based on your timestamp column.\n\nOptions A, B, and C do not address the issue of duplicates effectively or interactively as they do not explicitly remove duplicates based on the unique ID and event timestamp."},{"content":"Correct answer is D. Group by column us used to check for the duplicates where you can have the count(*) for each of the unique id column. If the count is greater than 1, we will know duplicate exists.The easiest way to remove duplicates while streaming inserts is to use row_number. Use GROUP BY on the unique ID column and timestamp column and SUM on the values will not remove duplicates.\nI also executed LAG function and LAG function will return NULL on unique id when no previous records with same unique id exist. Hence LAG is also not an option here.","poster":"Radhika7983","comment_id":"208073","upvote_count":"8","timestamp":"1727154600.0"},{"poster":"MaxNRG","upvote_count":"6","content":"D is correct because it will just pick out a single row for each set of duplicates.\nA is not correct because this will just return one row.\nB is not correct because this doesn’t get you the latest value, but will get you a sum of the same event over time which doesn’t make too much sense if you have duplicates.\nC is not correct because if you have events that are not duplicated, it will be excluded.","timestamp":"1727154540.0","comment_id":"474007"},{"comment_id":"819313","poster":"Zosby","upvote_count":"1","timestamp":"1677163800.0","content":"Correct D"},{"poster":"Morock","timestamp":"1676599920.0","content":"Selected Answer: D\nRow number gives the unique number ranking based on target column.","upvote_count":"3","comment_id":"811314"},{"poster":"odacir","content":"Selected Answer: D\nIt's the only valid option, try it your self with examples in QB.","upvote_count":"1","comment_id":"740928","timestamp":"1670672040.0"},{"poster":"Mamta072","comment_id":"618613","timestamp":"1655631780.0","content":"Ans is D as Row number is the clause to fetch unique record from duplicate","upvote_count":"1"},{"poster":"Arkon88","comment_id":"559280","upvote_count":"1","timestamp":"1646210640.0","content":"Answer: D"},{"comments":[{"comments":[{"comment_id":"784810","upvote_count":"2","timestamp":"1674432000.0","poster":"samdhimal","content":"Option A is not recommended because it will only return the first row based on the timestamp column, it doesn't consider the unique ID, so you could have multiple rows with the same timestamp, and you will get one of them arbitrarily.\n\nOption B is not recommended because it's used for aggregation, it doesn't return the first row for each unique ID based on the timestamp column.\n\nOption C is not recommended because it's used for comparing rows, it doesn't return the first row for each unique ID based on the timestamp column."}],"upvote_count":"4","timestamp":"1727154540.0","content":"When you are using BigQuery streaming inserts, there is no guarantee that data will only be sent once. However, you can use the ROW_NUMBER window function to ensure that duplicates are not included while interactively querying data. By using a PARTITION BY clause on the unique ID column, you can assign a unique number to each row within a result set, based on the order specified in the timestamp column. Then, a WHERE clause can be used to select only the row with the number 1. This will return the first row for each unique ID based on the timestamp column, which will ensure that duplicates are not included in your query results.","poster":"samdhimal","comment_id":"784809"}],"content":"correct answer -> Use the ROW_NUMBER window function with PARTITION by unique ID along with WHERE row equals 1.\n\nYou can use the ROW_NUMBER() to turn non-unique rows into unique rows and then delete the duplicate rows.\n\nReference:\nhttps://www.mysqltutorial.org/mysql-window-functions/mysql-row_number-function/","upvote_count":"3","timestamp":"1642867080.0","poster":"samdhimal","comment_id":"529954"},{"comment_id":"485659","content":"Sorry, but IMHO no response is correct, because, in addition to making the ID field unique, it occurs consider the record with most recent timestamp","upvote_count":"1","timestamp":"1637735400.0","poster":"nofaruccio"},{"timestamp":"1634215320.0","comment_id":"462024","upvote_count":"1","content":"Ans: D","poster":"anji007"},{"content":"Correct : D","comment_id":"319796","poster":"lbhhoya82","upvote_count":"1","timestamp":"1616649540.0"},{"poster":"sid091","content":"D is correct","timestamp":"1614520380.0","upvote_count":"3","comment_id":"300812"},{"upvote_count":"2","comment_id":"285703","content":"Correct answer is D.","timestamp":"1612721820.0","poster":"senthil836"},{"upvote_count":"2","content":"Correct D","comment_id":"284923","poster":"naga","timestamp":"1612626360.0"},{"poster":"saurabh1805","upvote_count":"2","comments":[{"timestamp":"1642165080.0","content":"This question is from Sample Questions of Google Certification website, they gave answer as D","comment_id":"523531","upvote_count":"1","poster":"deep_ROOT"},{"comments":[{"poster":"piotrpiskorski","comment_id":"718751","upvote_count":"1","timestamp":"1668519240.0","content":"you can use QUALIFY for that"}],"timestamp":"1646047980.0","poster":"Kowsikp","comment_id":"558011","content":"In your link only: \nRefer step 3 so its D.\nTo remove duplicates, run the following query. Specify a destination table, allow large results, and disable result flattening.\n\n#standardSQL\nSELECT\n * EXCEPT(row_number)\nFROM (\n SELECT\n *,\n ROW_NUMBER()\n OVER (PARTITION BY ID_COLUMN) row_number\n FROM\n `TABLE_NAME`)\nWHERE\n row_number = 1","upvote_count":"1"}],"comment_id":"160185","content":"B is closed to correct answer. \n\nhttps://cloud.google.com/bigquery/streaming-data-into-bigquery#manually_removing_duplicates","timestamp":"1597682160.0"},{"poster":"PRABHUKKARTHI","timestamp":"1597655040.0","comment_id":"159797","content":"Option D: This is the best approach which follows the expected output to be.","upvote_count":"2"},{"content":"https://cloud.google.com/bigquery/streaming-data-into-bigquery#manually_removing_duplicates","comment_id":"135448","poster":"wyextay","upvote_count":"5","timestamp":"1594790460.0"},{"upvote_count":"3","comment_id":"72554","content":"https://cloud.google.com/bigquery/docs/reference/standard-sql/analytic-function-concepts","timestamp":"1586411760.0","poster":"Nidie"}],"exam_id":11,"isMC":true,"question_id":298,"answer_ET":"D","question_text":"You are building new real-time data warehouse for your company and will use Google BigQuery streaming inserts. There is no guarantee that data will only be sent in once but you do have a unique ID for each row of data and an event timestamp. You want to ensure that duplicates are not included while interactively querying data. Which query type should you use?"},{"id":"DH8tU5XpntoIuOsIMIk0","url":"https://www.examtopics.com/discussions/google/view/17119-exam-professional-data-engineer-topic-1-question-80/","answers_community":["C (100%)"],"question_text":"MJTelco Case Study -\n\nCompany Overview -\nMJTelco is a startup that plans to build networks in rapidly growing, underserved markets around the world. The company has patents for innovative optical communications hardware. Based on these patents, they can create many reliable, high-speed backbone links with inexpensive hardware.\n\nCompany Background -\nFounded by experienced telecom executives, MJTelco uses technologies originally developed to overcome communications challenges in space. Fundamental to their operation, they need to create a distributed data infrastructure that drives real-time analysis and incorporates machine learning to continuously optimize their topologies. Because their hardware is inexpensive, they plan to overdeploy the network allowing them to account for the impact of dynamic regional politics on location availability and cost.\nTheir management and operations teams are situated all around the globe creating many-to-many relationship between data consumers and provides in their system. After careful consideration, they decided public cloud is the perfect environment to support their needs.\n\nSolution Concept -\nMJTelco is running a successful proof-of-concept (PoC) project in its labs. They have two primary needs:\n✑ Scale and harden their PoC to support significantly more data flows generated when they ramp to more than 50,000 installations.\n✑ Refine their machine-learning cycles to verify and improve the dynamic models they use to control topology definition.\nMJTelco will also use three separate operating environments `\" development/test, staging, and production `\" to meet the needs of running experiments, deploying new features, and serving production customers.\n\nBusiness Requirements -\n✑ Scale up their production environment with minimal cost, instantiating resources when and where needed in an unpredictable, distributed telecom user community.\n✑ Ensure security of their proprietary data to protect their leading-edge machine learning and analysis.\n✑ Provide reliable and timely access to data for analysis from distributed research workers\n✑ Maintain isolated environments that support rapid iteration of their machine-learning models without affecting their customers.\n\nTechnical Requirements -\nEnsure secure and efficient transport and storage of telemetry data\nRapidly scale instances to support between 10,000 and 100,000 data providers with multiple flows each.\nAllow analysis and presentation against data tables tracking up to 2 years of data storing approximately 100m records/day\nSupport rapid iteration of monitoring infrastructure focused on awareness of data pipeline problems both in telemetry flows and in production learning cycles.\n\nCEO Statement -\nOur business model relies on our patents, analytics and dynamic machine learning. Our inexpensive hardware is organized to be highly reliable, which gives us cost advantages. We need to quickly stabilize our large distributed data pipelines to meet our reliability and capacity commitments.\n\nCTO Statement -\nOur public cloud services must operate as advertised. We need resources that scale and keep our data secure. We also need environments in which our data scientists can carefully study and quickly adapt our models. Because we rely on automation to process our data, we also need our development and test environments to work as we iterate.\n\nCFO Statement -\nThe project is too large for us to maintain the hardware and software required for the data and analysis. Also, we cannot afford to staff an operations team to monitor so many data feeds, so we will rely on automation and infrastructure. Google Cloud's machine learning will allow our quantitative researchers to work on our high-value problems instead of problems with our data pipelines.\nMJTelco is building a custom interface to share data. They have these requirements:\n1. They need to do aggregations over their petabyte-scale datasets.\n2. They need to scan specific time range rows with a very fast response time (milliseconds).\nWhich combination of Google Cloud Platform products should you recommend?","topic":"1","choices":{"B":"Cloud Bigtable and Cloud SQL","C":"BigQuery and Cloud Bigtable","D":"BigQuery and Cloud Storage","A":"Cloud Datastore and Cloud Bigtable"},"answer_description":"","question_id":299,"isMC":true,"answer_images":[],"answer_ET":"C","discussion":[{"poster":"[Removed]","timestamp":"1584811140.0","comment_id":"66610","content":"correct: C","upvote_count":"19"},{"comment_id":"147824","upvote_count":"9","poster":"atnafu2020","content":"C\nBigquery and Big table =PB storage capacity \nBigtable=to read scan rows Big query select row to read","timestamp":"1596172620.0"},{"content":"Why not A? Can someone please explain","poster":"09878d5","timestamp":"1730837580.0","upvote_count":"1","comment_id":"1307549"},{"poster":"baht","timestamp":"1687107780.0","comment_id":"926813","content":"Selected Answer: C\nResponse C => Bigquery and bigtable","upvote_count":"1"},{"content":"Why not A? If we're already using Bigtable, what's the use of another, slower analytic solution, like BigQuery? Wouldn't Datastore be more useful to store our data than BigQuery?","timestamp":"1682876700.0","upvote_count":"5","comment_id":"885484","poster":"ga8our"},{"timestamp":"1674338100.0","upvote_count":"1","content":"Selected Answer: C\nbigquery and bigtable","poster":"dconesoko","comment_id":"783769"},{"poster":"sandipk91","upvote_count":"1","content":"C is correct, no doubt","timestamp":"1628445660.0","comment_id":"421772"},{"poster":"sumanshu","timestamp":"1625011920.0","content":"Vote for C","upvote_count":"2","comment_id":"394281"},{"comment_id":"308105","poster":"daghayeghi","upvote_count":"6","content":"C:\nThey need to do aggregations over their petabyte-scale datasets: Bigquery\nThey need to scan specific time range rows with a very fast response time (milliseconds): Bigtable","timestamp":"1615482300.0"},{"comments":[{"upvote_count":"5","comment_id":"249451","content":"With GCS you can only scan the rows from BigQuery using External federated Datasources, with that millisecond latency is not possible. Also \"scan specific time range rows with a very fast response time\" is a natural fit use case for Cloud Bigtable.","timestamp":"1608565140.0","poster":"Gcpyspark"}],"content":"Why not D? Biqquery and GCS.\nAlso Big Table is no sql, where as BQ is SQL","upvote_count":"1","poster":"Insane7","comment_id":"209563","timestamp":"1604089560.0"},{"comment_id":"161865","timestamp":"1597880940.0","poster":"haroldbenites","content":"C is correct.","upvote_count":"4"}],"exam_id":11,"question_images":[],"timestamp":"2020-03-21 18:19:00","unix_timestamp":1584811140,"answer":"C"},{"id":"dmeswA30bgpS1dXBgb0E","topic":"1","exam_id":11,"url":"https://www.examtopics.com/discussions/google/view/17264-exam-professional-data-engineer-topic-1-question-81/","answer_images":[],"choices":{"B":"Look through the current data and compose a small set of generalized charts and tables bound to criteria filters that allow value selection.","D":"Load the data into relational database tables, write a Google App Engine application that queries all rows, summarizes the data across each criteria, and then renders results using the Google Charts and visualization API.","C":"Export the data to a spreadsheet, compose a series of charts and tables, one for each possible combination of criteria, and spread them across multiple tabs.","A":"Look through the current data and compose a series of charts and tables, one for each possible combination of criteria."},"question_id":300,"question_text":"MJTelco Case Study -\n\nCompany Overview -\nMJTelco is a startup that plans to build networks in rapidly growing, underserved markets around the world. The company has patents for innovative optical communications hardware. Based on these patents, they can create many reliable, high-speed backbone links with inexpensive hardware.\n\nCompany Background -\nFounded by experienced telecom executives, MJTelco uses technologies originally developed to overcome communications challenges in space. Fundamental to their operation, they need to create a distributed data infrastructure that drives real-time analysis and incorporates machine learning to continuously optimize their topologies. Because their hardware is inexpensive, they plan to overdeploy the network allowing them to account for the impact of dynamic regional politics on location availability and cost.\nTheir management and operations teams are situated all around the globe creating many-to-many relationship between data consumers and provides in their system. After careful consideration, they decided public cloud is the perfect environment to support their needs.\n\nSolution Concept -\nMJTelco is running a successful proof-of-concept (PoC) project in its labs. They have two primary needs:\n✑ Scale and harden their PoC to support significantly more data flows generated when they ramp to more than 50,000 installations.\nRefine their machine-learning cycles to verify and improve the dynamic models they use to control topology definition.\n//IMG//\n\nMJTelco will also use three separate operating environments `\" development/test, staging, and production `\" to meet the needs of running experiments, deploying new features, and serving production customers.\n\nBusiness Requirements -\n✑ Scale up their production environment with minimal cost, instantiating resources when and where needed in an unpredictable, distributed telecom user community.\n✑ Ensure security of their proprietary data to protect their leading-edge machine learning and analysis.\n✑ Provide reliable and timely access to data for analysis from distributed research workers\n✑ Maintain isolated environments that support rapid iteration of their machine-learning models without affecting their customers.\n\nTechnical Requirements -\nEnsure secure and efficient transport and storage of telemetry data\nRapidly scale instances to support between 10,000 and 100,000 data providers with multiple flows each.\nAllow analysis and presentation against data tables tracking up to 2 years of data storing approximately 100m records/day\nSupport rapid iteration of monitoring infrastructure focused on awareness of data pipeline problems both in telemetry flows and in production learning cycles.\n\nCEO Statement -\nOur business model relies on our patents, analytics and dynamic machine learning. Our inexpensive hardware is organized to be highly reliable, which gives us cost advantages. We need to quickly stabilize our large distributed data pipelines to meet our reliability and capacity commitments.\n\nCTO Statement -\nOur public cloud services must operate as advertised. We need resources that scale and keep our data secure. We also need environments in which our data scientists can carefully study and quickly adapt our models. Because we rely on automation to process our data, we also need our development and test environments to work as we iterate.\n\nCFO Statement -\nThe project is too large for us to maintain the hardware and software required for the data and analysis. Also, we cannot afford to staff an operations team to monitor so many data feeds, so we will rely on automation and infrastructure. Google Cloud's machine learning will allow our quantitative researchers to work on our high-value problems instead of problems with our data pipelines.\nYou need to compose visualization for operations teams with the following requirements:\n✑ Telemetry must include data from all 50,000 installations for the most recent 6 weeks (sampling once every minute)\n✑ The report must not be more than 3 hours delayed from live data.\n✑ The actionable report should only show suboptimal links.\n✑ Most suboptimal links should be sorted to the top.\nSuboptimal links can be grouped and filtered by regional geography.\n//IMG//\n\n✑ User response time to load the report must be <5 seconds.\nYou create a data source to store the last 6 weeks of data, and create visualizations that allow viewers to see multiple date ranges, distinct geographic regions, and unique installation types. You always show the latest data without any changes to your visualizations. You want to avoid creating and updating new visualizations each month. What should you do?","question_images":["https://www.examtopics.com/assets/media/exam-media/04341/0006100002.png","https://www.examtopics.com/assets/media/exam-media/04341/0006200009.png"],"answers_community":["B (52%)","D (48%)"],"discussion":[{"comment_id":"67032","upvote_count":"32","timestamp":"1584897540.0","poster":"[Removed]","content":"Should be B"},{"upvote_count":"7","poster":"Jarek7","comments":[{"content":"As per old question - must be. I heard, that the exam will mostly have questions rather from 100 to 205 than form 1 to 100. And smb told me, that the other w/s gave questions, that happened more often in exam, in comparison to questions given here","upvote_count":"3","timestamp":"1683281520.0","comment_id":"889944","poster":"Oleksandr0501"}],"content":"Selected Answer: D\nFirst I thought B, as D seems too complex with writing app for AppEngine. But B is too simple - just look through the data doesnt seem right.\nIt must be very old question. Today you would load the data to BQ, optionally you can use Dataprep for simple data cleaning or a Dataflow job for more complex data processing, and finally use Looker to create tables and charts.","timestamp":"1683135960.0","comment_id":"888806"},{"content":"Selected Answer: B\nFilters allow dynamic interaction: Instead of static charts, filters enable users to select date ranges, regions, and installation types without requiring frequent updates.","comment_id":"1400874","upvote_count":"1","poster":"oussama7","timestamp":"1742432160.0"},{"timestamp":"1742051400.0","comment_id":"1398906","upvote_count":"1","poster":"Parandhaman_Margan","content":"Selected Answer: B\nDynamic Filtering → Instead of creating a fixed set of charts for every combination, filters allow users to explore data interactively without manual updates.\nScalability → Creating a small number of general charts with filters reduces maintenance effort and dashboard complexity."},{"comment_id":"1341100","upvote_count":"1","poster":"Augustax","content":"Selected Answer: B\nData engineer especially the Front-End developer will pick B.","timestamp":"1736955240.0"},{"comment_id":"1319203","upvote_count":"2","content":"Selected Answer: B\nD is not the right answer as Chart and Viz API is deprecated now (https://en.wikipedia.org/wiki/Google_Chart_API#:~:text=The%20Google%20Chart%20API%20is,charts%20from%20user%2Dsupplied%20data.)\n\nB is the most logical answer as it talks about creation of general chart wtih filter for value selection (as asked in the requirement),","comments":[{"timestamp":"1737205440.0","poster":"grshankar9","content":"Google Chart API is deprecated but there is a 'Google Charts' API now. And Visualization is part of it","comment_id":"1342516","upvote_count":"1"}],"timestamp":"1732794960.0","poster":"cloud_rider"},{"timestamp":"1696493940.0","comment_id":"1025392","content":"Selected Answer: B\nbound to criteria filters that allow value selection. - Simple and Smart.","poster":"Nirca","upvote_count":"3"},{"comment_id":"789500","timestamp":"1674814380.0","poster":"PolyMoe","upvote_count":"1","content":"Selected Answer: D\nD.\neverything is fixed except data that is updated regularly in order to keep the last 6 weeks. Then, the pipeline does not change ==> obtaining (same) charts and viz on regularly updated data"},{"content":"Selected Answer: B\nB \nBut can someone explain the question and selection clearly?","poster":"hauhau","timestamp":"1669901700.0","upvote_count":"4","comment_id":"732657"},{"upvote_count":"5","content":"Selected Answer: B\nIt's B. All the other choices are unreasonable.","comment_id":"712570","timestamp":"1667760300.0","poster":"cloudmon"},{"poster":"edwardlin421","timestamp":"1667697480.0","comment_id":"712112","upvote_count":"1","content":"ACD-Design for each possible combination of criteria, so if your team has new requirements, you must design new charts.\nSo, answer shoud be B."},{"timestamp":"1661410800.0","comments":[{"poster":"wan2three","timestamp":"1671866400.0","content":"D you might need to load data from source to table for each month. It stated the source will keep last 6 weeks data, but not in D","upvote_count":"1","comment_id":"754752"}],"content":"Selected Answer: D\nthe key is \" You want to avoid creating and updating new visualizations each month.\"\nonly D work for that phrase","comment_id":"651680","upvote_count":"2","poster":"ducc"},{"poster":"KundanK973","comment_id":"625720","timestamp":"1656672900.0","content":"must be D","upvote_count":"1"},{"content":"Selected Answer: D\nThe answer is B","comment_id":"624966","poster":"ealpuche","upvote_count":"2","timestamp":"1656550140.0"},{"poster":"rr4444","comment_id":"624743","upvote_count":"3","timestamp":"1656516420.0","content":"This Q feels very disconnected from GCP products....."},{"comment_id":"600486","timestamp":"1652336520.0","upvote_count":"4","content":"Selected Answer: D\nVote D.\nSince B just uses \"current data\", which means if new data enters, you need to re-run those charts again.","poster":"sw52099","comments":[{"poster":"wan2three","upvote_count":"1","content":"But q says the data sources only have latest 6 weeks data, so current data means latest?","timestamp":"1671866280.0","comment_id":"754750"}]},{"upvote_count":"1","timestamp":"1645247220.0","content":"B is optimal to avoid creating and updating new visualizations each month","comment_id":"550667","poster":"RRK2021"},{"upvote_count":"4","poster":"ManojT","content":"Answer D: Data in SQL so querying becomes easier on any pattern. create mutiple charts, graphs to fulfill your requirements.","timestamp":"1633749840.0","comment_id":"459470"},{"timestamp":"1628445960.0","content":"Yes it's B","upvote_count":"2","poster":"sandipk91","comment_id":"421775"},{"poster":"sumanshu","timestamp":"1625087640.0","upvote_count":"1","comment_id":"395224","content":"vote for B"},{"content":"Yes agreed. B seems to be the only reasonable choice here.","timestamp":"1603652220.0","comment_id":"205803","poster":"reima990","upvote_count":"4"}],"answer_ET":"B","unix_timestamp":1584897540,"answer_description":"","answer":"B","isMC":true,"timestamp":"2020-03-22 18:19:00"}],"exam":{"isMCOnly":true,"id":11,"numberOfQuestions":319,"lastUpdated":"11 Apr 2025","provider":"Google","isBeta":false,"name":"Professional Data Engineer","isImplemented":true},"currentPage":60},"__N_SSP":true}