{"pageProps":{"questions":[{"id":"eAguiUBmNunVJrmakjl2","unix_timestamp":1605087480,"question_images":[],"question_id":191,"answer":"B","answer_description":"","question_text":"You need to define an address plan for a future new GKE cluster in your VPC. This will be a VPC native cluster, and the default Pod IP range allocation will be used. You must pre-provision all the needed VPC subnets and their respective IP address ranges before cluster creation. The cluster will initially have a single node, but it will be scaled to a maximum of three nodes if necessary. You want to allocate the minimum number of Pod IP addresses.\nWhich subnet mask should you use for the Pod IP address range?","answer_images":[],"exam_id":8,"topic":"1","choices":{"A":"/21","B":"/22","C":"/23","D":"/25"},"answer_ET":"B","url":"https://www.examtopics.com/discussions/google/view/36752-exam-professional-cloud-network-engineer-topic-1-question-65/","timestamp":"2020-11-11 10:38:00","discussion":[{"comment_id":"261053","upvote_count":"34","content":"I think it's B.\n\n\"This will be a VPC native cluster, and the *default* Pod IP range allocation will be used.\"\n\nFrom https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr#overview\n\"With the *default* maximum of 110 Pods per node, Kubernetes assigns a /24 CIDR block (256 addresses) to each of the nodes.\"\n\nThat is, /24 for one node.\nWe have 3 nodes, so we need /22.","poster":"groovygorilla","timestamp":"1609941000.0"},{"comments":[{"timestamp":"1632228000.0","comment_id":"448895","content":"In GKE the maximum number of pods per node is hard limited to 110. So in this question we have to extimate 330 pods. Anyway, GKE has an ultraconservative policy on IP addresses number, so for every pod 2 Ip addresses are reserved (even if only one is actually assigned).\n\n\nSo we have 330 pods , we double this number 330*2 = 660 and we get the minimum number of ip addresses we need. So 512 aren't enough and we go with 1024. To reserve 1024 ip addresses (2^10) we need to use a /22 subnet","upvote_count":"21","comments":[{"upvote_count":"3","timestamp":"1669594440.0","poster":"AzureDP900","comment_id":"728683","content":"Thank you for detailed explanation, I agree with you! B is right"}],"poster":"lollo883"},{"timestamp":"1708464180.0","content":"Wrong!","poster":"desertlotus1211","comment_id":"1155032","upvote_count":"1"}],"timestamp":"1605087480.0","upvote_count":"14","comment_id":"217181","poster":"lukedj87","content":"TL;DR: correct answer /22\n\nMax nodes will be three.\nEach node can have a max of 254 pods.\n254 * 3 -> 762 pods\nBoth a /25 and /23 wouldn't be enough --> those would respectively account for 128 and 512 pods\n/21 would be too large -> that would be enough for 2048 pods\n/22 is the right one, accounting for 1024 PODs"},{"content":"Selected Answer: B\n/21 = 2048\n/22 = 1024\n/23 = 512\n/25=128\n\nmax pod per node = 110 (double this as Google Best practice) = 220 IPs per pod needed\nWe have 3 pods x 3 = 660 Ips needed \nclosest but not too much is 1024 = /22","timestamp":"1735782600.0","upvote_count":"1","comment_id":"1335334","poster":"ian_gcpca"},{"timestamp":"1730603880.0","upvote_count":"1","poster":"mohan999","content":"I think it should be /21, considering each node can have 256 pods and the address block always contains at least twice as many addresses as the maximum number of Pods per node as per documentation.\n256(max pods per node)*3 nodes=768 pods\n768(total max pods)*2=1536(twice the total pods) addresses\nAnd /21 range satisfies this.","comment_id":"1306421","comments":[{"content":"If a default 110 pods per node is considered, then /22 should be enough","upvote_count":"1","poster":"mohan999","comment_id":"1306423","timestamp":"1730603940.0"}]},{"poster":"desertlotus1211","content":"/23 for a total of 510 IPs and /24 for a total of 254 IPs ...sorry about that","timestamp":"1708464360.0","comment_id":"1155037","upvote_count":"1"},{"content":"It’s B","comment_id":"1092975","poster":"Gurminderjit","upvote_count":"1","timestamp":"1702254180.0"},{"poster":"crg63","comment_id":"1020214","content":"Selected Answer: B\n/22 allows 4 nodes, since each node needs /24 allocated for Pods (110 pods per node)","upvote_count":"3","timestamp":"1695932220.0"},{"content":"Answer is B see the table attached for 4 nodes and 330 pods( 440 is max size )","comment_id":"885085","poster":"PranavP96","comments":[{"poster":"desertlotus1211","comments":[{"poster":"desertlotus1211","comment_id":"900602","content":"Where is the table :)","upvote_count":"1","timestamp":"1684369320.0"}],"upvote_count":"2","timestamp":"1684369260.0","comment_id":"900601","content":"there is no table :)"},{"content":"this is not the answer.\nthe answer is:\nmax pods in node is 110, double it in 3 is 330.\nthe best practice is always double so 660 --> its btw 512-1024 so the answer is /22.","comment_id":"982211","poster":"kapara","upvote_count":"1","comments":[{"poster":"desertlotus1211","content":"why? where is the BP for this?","comment_id":"1155034","timestamp":"1708464240.0","upvote_count":"1"}],"timestamp":"1692163860.0"}],"upvote_count":"1","timestamp":"1682849040.0"},{"comment_id":"855624","poster":"Komal697","timestamp":"1680172620.0","upvote_count":"1","content":"Selected Answer: D\nOption D (/25) is the correct answer because it allows for the minimum number of Pod IP addresses while still providing enough IP addresses for the maximum of three nodes in the cluster. A /25 subnet mask provides 128 IP addresses, which is enough for a single node cluster (with one IP address used for the node) and for a three node cluster (with one IP address used for each node).\nOption A (/21) provides more IP addresses than necessary and could result in IP address wastage. Option B (/22) and Option C (/23) also provide more IP addresses than necessary for a single node cluster and may lead to IP address wastage.\nSo, the best option is to use a /25 subnet mask to allocate the minimum number of Pod IP addresses while still providing enough IP addresses for the maximum of three nodes in the cluster."},{"timestamp":"1666375740.0","comment_id":"701097","content":"The corre tra answer is B.\n/24 is the default CIDR block assigned to each worker node, with maximum 110 PODs for node. For 3 node, we need 3 x /24 = /22","poster":"MMEB","upvote_count":"1"},{"comment_id":"516683","content":"Answer is : B","upvote_count":"1","poster":"kumarp6","timestamp":"1641304920.0"},{"timestamp":"1641224520.0","upvote_count":"1","content":"it's B\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/alias-ips#defaults_limits","comment_id":"515880","poster":"kumarp6"},{"upvote_count":"3","comment_id":"482491","content":"Selected Answer: B\nB\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/alias-ips#defaults_limits","timestamp":"1637409840.0","poster":"JesusMariaJose"},{"comment_id":"458390","timestamp":"1633542720.0","content":"https://cloud.google.com/kubernetes-engine/docs/concepts/alias-ips#cluster_sizing_secondary_range_pods","poster":"Morgan91","upvote_count":"2"},{"comment_id":"452098","content":"Correction to my below reply - Each node will have max of 2^8 =256 , so for 3 nodes it will be 254*3 = 762. If you chose /23 then 2^(32-23) = 2^9 = 512 which is less than 762 so incorrect option .if you do the same thing for /22 you get 2048 which is more than 762 hence option B /22 is correct","timestamp":"1632702840.0","poster":"vamgcp","upvote_count":"1"},{"upvote_count":"1","timestamp":"1632678660.0","comment_id":"451965","poster":"vamgcp","content":"Each node will have max of 2^8 =254 , so for 3 nodes it will be 254*3 = 762. If you chose /23 then 2^(32-23) = 2^9 = 512 which is less than 762 so incorrect option .if you do same thing for /212 you get 2048 which is more than 762 hence option B /21 is correct"},{"upvote_count":"1","poster":"PeppaPig","timestamp":"1630854840.0","content":"B is the answer.\nWhen the \"default\" Pod IP range allocation is used, GKE assigns /24 CIDR block for pods on each node, which means the minimum secondary IP range of the subnet is /24, and that allows for max 1 node in your cluster. \nTo expand to 3 Nodes, /22 range is required.","comments":[{"poster":"PeppaPig","comment_id":"439752","content":"Calculate the maximum number of nodes, N, that the subnet's secondary IP address range for Pods can support:\nN = 2(M - S) where:\nM is the size of the netmask of each node's alias IP address range for Pods, calculated in the first step\nS is the size of the subnet mask of the subnet's secondary IP address range\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/alias-ips#cluster_sizing_secondary_range_pods","timestamp":"1630854900.0","upvote_count":"1"}],"comment_id":"439751"},{"poster":"PiotrKam","comment_id":"384340","upvote_count":"3","timestamp":"1623948240.0","content":"It's C - /23\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/alias-ips#cluster_sizing"},{"comment_id":"318327","upvote_count":"3","content":"C is correct","timestamp":"1616522040.0","poster":"Vidyasagar"},{"timestamp":"1614162240.0","content":"I think B is the right answer. although best practice suits the answer C I will go with B because it says \"it will scaled to a maximum of three nodes if necessary\" so there's a chance you will not be using all of the ip address and if t scales to a maximum you can still accomodate it with the minimum number of ip addresses","poster":"LaXuS","upvote_count":"3","comments":[{"upvote_count":"1","timestamp":"1614162300.0","comment_id":"298101","content":"oops i switched the B and C. C is the right choice","poster":"LaXuS"}],"comment_id":"298098"},{"timestamp":"1613297280.0","poster":"porsak","comments":[{"comment_id":"339958","content":"you can have 3 nodes","upvote_count":"1","timestamp":"1618962180.0","poster":"[Removed]"}],"content":"I think it's a tricky question. You can't have just three nodes, just 2 or 4. Default Pod IP range will be used so /24. For 4 nodes with 110 pods for each node u have to allocate /22 IPs. Also if you use their equation (24-22 = 2, 2^2 = 4 nodes). 1024 addresses.\nSee: https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/alias-ips#defaults_limits","comment_id":"290166","upvote_count":"1"},{"comment_id":"281561","poster":"chetz12","content":"I would go with /23 as it says minimum IP addresses to fulfill the total of 110*3 = 330 which is > /24 (256) and less than the next best option which is /23 (512)","upvote_count":"1","timestamp":"1612232940.0","comments":[{"timestamp":"1612233120.0","upvote_count":"1","poster":"chetz12","content":"Correct is C /22 . Forgot the fun part around the best practice and it's mentioned in one of the thread here.","comment_id":"281565"}]},{"upvote_count":"3","timestamp":"1609773840.0","comment_id":"259476","content":"\"D\" is correct.\nWe want to allocate the minimum number of Pod IP addresses so we use /25 for Pod IP adressess.\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr#overview","poster":"ydanno"},{"upvote_count":"2","timestamp":"1607952480.0","poster":"glk","content":"D should the answer as it says \"You want to allocate the minimum number of Pod IP addresses\"","comment_id":"243607"},{"timestamp":"1607869740.0","comment_id":"242648","upvote_count":"1","content":"In accordance to this https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr both /23 and /22 mask are possible solutions. Since we are looking for the minimum number of POD's IP addresses I would go with mask /23.","poster":"gless"},{"content":"Ans - B","timestamp":"1605897180.0","upvote_count":"2","comment_id":"223887","poster":"[Removed]"},{"comment_id":"218880","upvote_count":"3","timestamp":"1605320760.0","content":"Max pods per node is = 110. So, that correspond to /24 ip allocation per node. And in this case you need to provide 3 nodes for the future. If you assign /22 ip range for pods, (24-22=2^2=4) you can will scale to 4 nodes. Correct answer is /22","poster":"mikelabs"},{"comment_id":"217658","content":"I think should be C.\n1 Node can have max 110 pods though /24 subnet has 254 IP addresses.\n110 * 3 = 330 pods and this is IP address range for /23.","comments":[{"upvote_count":"3","comment_id":"218573","content":"be careful, because that's the max number of PODs per node you can have, but as per G best practice you should always -at least- double the IPs reserved for them (for PODs rescheduling).\nThe default allocation indeed keeps in mind 110 pods per node and 254 IPs for them.\nThat's why I've used 254.","timestamp":"1605280440.0","poster":"lukedj87","comments":[{"upvote_count":"1","comments":[{"poster":"ThisisJohn","timestamp":"1636971540.0","upvote_count":"1","comment_id":"478597","content":"Sorry, please disregard my previous comment as it focuses on nodes, not on pods"}],"comment_id":"478593","timestamp":"1636971060.0","poster":"ThisisJohn","content":"I would go with C as per the formula and example below, according to which we need to provide addressing for 330 pods, and a /23 is enough for that.\n\n\"EXAMPLE - If you plan to create a 900-node cluster, the primary IP address range of the cluster's subnet must be at least a /22 (2^(32-22) = 2^10 = 1,024 addresses)\" https://cloud.google.com/kubernetes-engine/docs/concepts/alias-ips#cluster_sizing"},{"timestamp":"1636971900.0","upvote_count":"1","content":"Agree with lukedj87. Following the below example, answer should be B, as GKE allocates a /24 subnet for pods per node.\n\n\"EXAMPLE - For a 900-node cluster supporting up to 110 Pods per node, you need 900 × 256 = 230,400 IP addresses for Pods.\" Considering 3 nodes instead of 900 and doing the math, a /22 is required https://cloud.google.com/kubernetes-engine/docs/concepts/alias-ips#cluster_sizing","poster":"ThisisJohn","comment_id":"478602"}]}],"poster":"hjson821109","timestamp":"1605152220.0","upvote_count":"1"}],"isMC":true,"answers_community":["B (88%)","13%"]},{"id":"2VyZ1JJOfHoa1Nfkxi5I","unix_timestamp":1604433240,"question_text":"You have created a firewall with rules that only allow traffic over HTTP, HTTPS, and SSH ports. While testing, you specifically try to reach the server over multiple ports and protocols; however, you do not see any denied connections in the firewall logs. You want to resolve the issue.\nWhat should you do?","timestamp":"2020-11-03 20:54:00","answers_community":["D (100%)"],"answer_description":"","isMC":true,"question_images":[],"url":"https://www.examtopics.com/discussions/google/view/35949-exam-professional-cloud-network-engineer-topic-1-question-66/","answer_ET":"D","topic":"1","discussion":[{"comments":[{"content":"Agree!","comment_id":"217183","poster":"lukedj87","upvote_count":"1","timestamp":"1605087660.0"},{"upvote_count":"1","timestamp":"1669594500.0","content":"Yes. D. Create an explicit Deny Any rule and enable logging on the new rule.","poster":"AzureDP900","comment_id":"728685"}],"upvote_count":"25","comment_id":"212210","content":"Correct Answer is (D):\nFirewall Rules Logging has the following specifications:\n\nYou can only enable Firewall Rules Logging for rules in a Virtual Private Cloud (VPC) network. Legacy networks are not supported.\nFirewall Rules Logging only records TCP and UDP connections. Although you can create a firewall rule applicable to other protocols, you cannot log their connections.\nYou cannot enable Firewall Rules Logging for the implied deny ingress and implied allow egress rules.\nLog entries are written from the perspective of virtual machine (VM) instances. Log entries are only created if a firewall rule has logging enabled and if the rule applies to traffic sent to or from the VM. Entries are created according to the connection logging limits on a best effort basis.\nThe number of connections that can be logged in a given interval is based on the machine type.\nChanges to firewall rules can be viewed in VPC audit logs.\n\n\nhttps://cloud.google.com/vpc/docs/firewall-rules-logging#specifications","timestamp":"1604433240.0","poster":"ESP_SAP"},{"comment_id":"1246189","poster":"nkastanas","timestamp":"1720710480.0","content":"Selected Answer: D\nit is D","upvote_count":"1"},{"comment_id":"1123476","poster":"dragos_dragos62000","content":"Selected Answer: D\nAnswer D","timestamp":"1705331340.0","upvote_count":"1"},{"upvote_count":"1","poster":"Gurminderjit","content":"D is the answer","timestamp":"1702254240.0","comment_id":"1092976"},{"comment_id":"775568","timestamp":"1673708940.0","poster":"pk349","content":"• D. Create an explicit ******* Deny Any rule and enable logging on the new rule.","upvote_count":"1"},{"poster":"small1_small2","comment_id":"650437","upvote_count":"2","content":"Selected Answer: D\nCorrect Answer is (D): Explicit deny rule is required to see the logs\nhttps://cloud.google.com/vpc/docs/firewall-rules-logging#specifications","timestamp":"1661203620.0"},{"poster":"kumarp6","content":"Answer is : D","upvote_count":"2","timestamp":"1641305700.0","comment_id":"516695"},{"upvote_count":"2","timestamp":"1641224640.0","poster":"kumarp6","comment_id":"515881","content":"Answer is D"},{"timestamp":"1616555640.0","poster":"Vidyasagar","content":"D is correct","comment_id":"318718","upvote_count":"3"},{"content":"Ans - D","poster":"[Removed]","timestamp":"1605897540.0","comment_id":"223890","upvote_count":"3"}],"answer":"D","exam_id":8,"question_id":192,"answer_images":[],"choices":{"D":"Create an explicit Deny Any rule and enable logging on the new rule.","A":"Enable logging on the default Deny Any Firewall Rule.","C":"Create a logging sink forwarding all firewall logs with no filters.","B":"Enable logging on the VM Instances that receive traffic."}},{"id":"5tZhqlrc2t7vfkcg2XHm","url":"https://www.examtopics.com/discussions/google/view/36754-exam-professional-cloud-network-engineer-topic-1-question-67/","question_text":"In your company, two departments with separate GCP projects (code-dev and data-dev) in the same organization need to allow full cross-communication between all of their virtual machines in GCP. Each department has one VPC in its project and wants full control over their network. Neither department intends to recreate its existing computing resources. You want to implement a solution that minimizes cost.\nWhich two steps should you take? (Choose two.)","question_id":193,"answer":"BD","answers_community":["BD (80%)","BE (20%)"],"discussion":[{"content":"Answer is B & D.\nB: Minimizes cost and quickly.\nD: You need to create firewall rules to allow traffic between subnets over each VPC.","comment_id":"218883","timestamp":"1636857480.0","upvote_count":"26","comments":[{"poster":"PurplePanda","upvote_count":"2","content":"I don't think D will work. Firewall rules only apply to a particular project, not across projects. \n\"Virtual Private Cloud (VPC) firewall rules apply to a given project and network. If you want to apply firewall rules to multiple VPC networks in an organization, see Hierarchical firewall policies overiew. \"\nhttps://cloud.google.com/vpc/docs/firewalls","comment_id":"643694","timestamp":"1691406240.0","comments":[{"timestamp":"1692446820.0","comments":[{"comment_id":"830327","poster":"subhala","upvote_count":"1","timestamp":"1709672700.0","content":"there will be routes across VPCs."}],"poster":"amoyano","comment_id":"648951","content":"PurplePanda, it's true, rules applies for one project, but you can configure the firewall rules of the other project and it's solved. D alternative doesn't say that you'd work only over one firewall.","upvote_count":"1"}]},{"poster":"zbyszekz","comments":[{"comment_id":"900609","upvote_count":"1","poster":"desertlotus1211","content":"You're adding additional cost and overhead","timestamp":"1715991900.0"}],"comment_id":"761421","upvote_count":"2","timestamp":"1703878860.0","content":"We don't know about IP range in each VPC, so VPN is better to avoid IP conflict."}],"poster":"mikelabs"},{"content":"B and D 100%\n-First of all, we only have 2 separate VPCs in 2 different projects each where each project resides in the same organization. This set-up already yells that we need NW peering! \n-In addition, to be able to use a Shared VPC we need to delete existing service project resources and recreate them in the shared VPC subnet, which is something the question statement does not want, so Shared VPC is automatically eliminated\n-Lastly, with nw peering, the subnet routes of both VPCs are automatically shared, but we still need to create firewall rules to allow incoming requests for both ends.\nHence B and D","upvote_count":"16","comment_id":"358604","timestamp":"1652699040.0","poster":"seddy","comments":[{"comment_id":"728687","upvote_count":"1","poster":"AzureDP900","timestamp":"1701130500.0","content":"Agreed."},{"upvote_count":"2","poster":"gcpengineer","comment_id":"977996","content":"Nope, each department want full control of ntw so shared vpc is ruled out","timestamp":"1723314900.0"}]},{"poster":"Gurminderjit","content":"B and D","comment_id":"1092977","upvote_count":"1","timestamp":"1733876700.0"},{"comment_id":"1042754","content":"Selected Answer: BD\nB & D are the best bet","upvote_count":"1","poster":"bus_karan19","timestamp":"1728831120.0"},{"comment_id":"985965","upvote_count":"1","content":"Selected Answer: BE\nThis question is confusing - It clearly states in the document (https://cloud.google.com/vpc/docs/vpc-peering):\n\nYou can't disable the subnet route exchange or select which subnet routes are exchanged. After peering is established, all resources within subnet IP addresses are accessible across directly peered networks. VPC Network Peering doesn't provide granular route controls to filter out which subnet CIDR ranges are reachable across peered networks. You must use firewall rules to filter traffic if that's required.\n\nThis is why to me D doesn't make sense as they want unfettered access between their subnets for the 2 projects (FW rules only required for granular access).","comments":[{"poster":"gcpengineer","comment_id":"996818","upvote_count":"1","content":"without fw rules they wont be able toa ccess","timestamp":"1725273840.0"}],"poster":"Thornadoo","timestamp":"1724176920.0"},{"poster":"rglearn","upvote_count":"1","timestamp":"1722165960.0","content":"Selected Answer: BD\nAlong with VPC-Peering we will need firewall rules also in place to allow unrestricted communication across two VPCs.","comment_id":"965526"},{"comment_id":"894597","poster":"adfghn","content":"Key point is STEP SHOULD BE FOLLOWED\n1st VPC peering\n2nd Allow firewall policy b/w 2 VPCs\n====\nCloud VPN is one option but it will increase the cost.","timestamp":"1715405640.0","comments":[{"content":"what's your answer?","poster":"desertlotus1211","comment_id":"945069","timestamp":"1720308720.0","upvote_count":"1"}],"upvote_count":"1"},{"timestamp":"1705245000.0","poster":"pk349","comment_id":"775571","content":"B D are correct","upvote_count":"1"},{"content":"Selected Answer: BD\nDDDDDDD&&&&&&&&BBBBBBBBBB","timestamp":"1696409100.0","poster":"Mr_MIXER007","upvote_count":"2","comment_id":"686043"},{"content":"Answer is : B and D","upvote_count":"2","poster":"kumarp6","comment_id":"516698","timestamp":"1672841820.0"},{"comment_id":"515882","content":"Answer is B & D.","poster":"kumarp6","upvote_count":"1","timestamp":"1672760940.0"},{"upvote_count":"6","poster":"VivekMishraV","content":"it B and D\nhttps://cloud.google.com/vpc/docs/vpc-peering#firewall\nWhen you connect networks using VPC Network Peering, firewall rules are not exchanged between them. To allow ingress traffic from VM instances in a peer network, you must create ingress allow firewall rules. By default, ingress traffic to VMs is blocked by the implied deny ingress rule.\n\nIf you need to restrict access to VMs such that only other VMs in your VPC network have access, ensure that the sources for your ingress allow firewall rules only identify VMs in your VPC network, not ones from peer networks. For example, you can specify source IP ranges for just the subnets in your VPC network.\n\nTo restrict access to an internal TCP/UDP load balancer, create ingress firewall rules that apply to the load balancer's backend VMs.","timestamp":"1651918860.0","comment_id":"351847"},{"comment_id":"337565","content":"Has to be A and B.\nD would not work as VPCs are in different projects, allowing all traffic would expose resources on it externally, you can't allow the subnet private ranges as it would reach the VPC with an external IP through Internet and not the source subnet private IP ranges.","poster":"Plinci","upvote_count":"1","timestamp":"1650197700.0","comments":[{"upvote_count":"2","poster":"buldas","timestamp":"1650265800.0","content":"VPN or Peereing, A and B doesn't make any sense.","comment_id":"337998"}]},{"upvote_count":"4","content":"B and D","timestamp":"1648059720.0","poster":"Vidyasagar","comment_id":"318361"},{"timestamp":"1640485440.0","upvote_count":"1","comment_id":"252399","poster":"subhala","content":"How about A and B?"},{"timestamp":"1639433460.0","upvote_count":"2","content":"B and D,","comment_id":"243029","poster":"cesar7816"},{"timestamp":"1637433720.0","upvote_count":"3","comment_id":"223892","content":"Ans - BD","poster":"[Removed]"},{"comment_id":"217186","content":"I'm sure about B, not that sure about D...but it's the only other option -by exclusion- that makes sense to me","poster":"lukedj87","timestamp":"1636623900.0","upvote_count":"3"}],"exam_id":8,"answer_images":[],"timestamp":"2020-11-11 10:45:00","question_images":[],"unix_timestamp":1605087900,"choices":{"E":"Create a route in the code-dev project to the destination prefixes in project data-dev and use nexthop as the default gateway, and vice versa.","C":"Enable Shared VPC in one project (e. g., code-dev), and make the second project (e. g., data-dev) a service project.","A":"Connect both projects using Cloud VPN.","B":"Connect the VPCs in project code-dev and data-dev using VPC Network Peering.","D":"Enable firewall rules to allow all ingress traffic from all subnets of project code-dev to all instances in project data-dev, and vice versa."},"isMC":true,"topic":"1","answer_ET":"BD","answer_description":""},{"id":"OttojnbqBeL8JiDJuJS9","url":"https://www.examtopics.com/discussions/google/view/35951-exam-professional-cloud-network-engineer-topic-1-question-68/","question_id":194,"answer":"D","answers_community":["D (83%)","A (17%)"],"answer_ET":"D","discussion":[{"content":"Correct Answer is (D):\n\nCreating GKE private clusters with network proxies for controller access\nWhen you create a GKE private cluster with a private cluster controller endpoint, the cluster's controller node is inaccessible from the public internet, but it needs to be accessible for administration.\nBy default, clusters can access the controller through its private endpoint, and authorized networks can be defined within the VPC network.\n\nTo access the controller from on-premises or another VPC network, however, requires additional steps. This is because the VPC network that hosts the controller is owned by Google and cannot be accessed from resources connected through another VPC network peering connection, Cloud VPN or Cloud Interconnect.\n\nhttps://cloud.google.com/solutions/creating-kubernetes-engine-private-clusters-with-net-proxies","timestamp":"1635969720.0","upvote_count":"22","comments":[{"comment_id":"728689","content":"Agree with D","upvote_count":"1","poster":"AzureDP900","timestamp":"1701130620.0"},{"poster":"JohnnyBG","comments":[{"timestamp":"1659945660.0","comment_id":"421525","content":"scratch that .. the peering between Google's VPC is done via a privare endpoint .. D is OK I guess","upvote_count":"1","poster":"JohnnyBG"}],"upvote_count":"4","timestamp":"1659945480.0","content":"All that document is saying is that you need to export your route to Google's VPC where the master is. Private endpoint is not required .. I would go with C on this one.","comment_id":"421523"},{"comment_id":"217193","upvote_count":"1","timestamp":"1636624380.0","content":"Agree with D","poster":"lukedj87"}],"poster":"ESP_SAP","comment_id":"212214"},{"content":"Selected Answer: D\nD is the best bet as we need enable private end point","timestamp":"1728831360.0","comment_id":"1042756","upvote_count":"1","poster":"bus_karan19"},{"content":"Selected Answer: A\ncreate private cluster. A is ans","upvote_count":"1","timestamp":"1725281880.0","comment_id":"996900","poster":"gcpengineer"},{"timestamp":"1713068580.0","poster":"aparna20","comment_id":"869960","upvote_count":"2","content":"Selected Answer: D\nAgree with D"},{"poster":"pk349","comments":[{"poster":"exambott","content":"https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters","timestamp":"1706213700.0","upvote_count":"1","comment_id":"788106"}],"content":"• D. Create a VPC-native GKE cluster using user-managed IP ranges. Enable privateEndpoint ******* on the cluster master. Set the pod and service ranges as /24. Set up a network proxy to access the master. Enable master authorized networks.","timestamp":"1705245060.0","upvote_count":"1","comment_id":"775572"},{"comments":[{"poster":"Thornadoo","comment_id":"991180","content":"Wrong - DDDDDDDDDDD","upvote_count":"1","timestamp":"1724728380.0"}],"poster":"Mr_MIXER007","upvote_count":"2","content":"Selected Answer: D\nAns - D","comment_id":"686045","timestamp":"1696409220.0"},{"comment_id":"516701","poster":"kumarp6","content":"Answer is : D","upvote_count":"2","timestamp":"1672842000.0"},{"comment_id":"515888","upvote_count":"1","timestamp":"1672761180.0","content":"Answer is : D","poster":"kumarp6"},{"timestamp":"1648091520.0","content":"D is correct","upvote_count":"1","comment_id":"318717","poster":"Vidyasagar"},{"upvote_count":"1","comment_id":"223893","content":"Ans - D","poster":"[Removed]","timestamp":"1637433780.0"}],"timestamp":"2020-11-03 21:02:00","unix_timestamp":1604433720,"exam_id":8,"question_images":[],"isMC":true,"choices":{"A":"\"¢ Create a private cluster that uses VPC advanced routes. \"¢ Set the pod and service ranges as /24. \"¢ Set up a network proxy to access the master.","D":"\"¢ Create a VPC-native GKE cluster using user-managed IP ranges. \"¢ Enable privateEndpoint on the cluster master. \"¢ Set the pod and service ranges as /24. \"¢ Set up a network proxy to access the master. \"¢ Enable master authorized networks.","C":"\"¢ Create a VPC-native GKE cluster using user-managed IP ranges. \"¢ Enable a GKE cluster network policy, set the pod and service ranges as /24. \"¢ Set up a network proxy to access the master. \"¢ Enable master authorized networks.","B":"\"¢ Create a VPC-native GKE cluster using GKE-managed IP ranges. \"¢ Set the pod IP range as /21 and service IP range as /24. \"¢ Set up a network proxy to access the master."},"topic":"1","question_text":"You need to create a GKE cluster in an existing VPC that is accessible from on-premises. You must meet the following requirements:\n✑ IP ranges for pods and services must be as small as possible.\n✑ The nodes and the master must not be reachable from the internet.\n✑ You must be able to use kubectl commands from on-premises subnets to manage the cluster.\nHow should you create the GKE cluster?","answer_images":[],"answer_description":""},{"id":"CA3pa8WYhXAalF6cwHZD","answer_description":"","answer_ET":"AC","unix_timestamp":1604988720,"answer":"AC","question_images":[],"timestamp":"2020-11-10 07:12:00","discussion":[{"upvote_count":"16","poster":"densnoigaskogen","comment_id":"383122","timestamp":"1639640520.0","content":"A and C.\nUnless you use target pool-based Network LB, then it's required to use legacy health check, otherwise, legacy health check is not recommended to be used for HTTP(S) LB.\nref: https://cloud.google.com/load-balancing/docs/health-check-concepts"},{"poster":"seddy","upvote_count":"9","timestamp":"1637078100.0","comments":[{"upvote_count":"1","comment_id":"728691","content":"A, C is correct","poster":"AzureDP900","timestamp":"1685225940.0"}],"content":"A and C for sure!\nLink: https://cloud.google.com/load-balancing/docs/health-checks\n-Important lines from the link that lead me to say the answer is A and C:\n\n''Google Cloud allows you to create or select a health check when you complete the load balancer's backend configuration in the Cloud Console.\" - A\n\n\"You can create a health check using the Cloud Console, the gcloud command-line tool, or the REST APIs.\" - C\n\nPeace :)","comment_id":"358780"},{"poster":"dev62","timestamp":"1723125240.0","comment_id":"1144667","content":"Selected Answer: AC\nAC looks good","upvote_count":"1"},{"comment_id":"1042758","poster":"bus_karan19","content":"Selected Answer: AC\nA & C best bet","upvote_count":"1","timestamp":"1713020340.0"},{"timestamp":"1708064580.0","upvote_count":"1","poster":"GCBC","content":"A & C are correct","comment_id":"982167"},{"upvote_count":"1","poster":"BrunoRangel","timestamp":"1699730880.0","comment_id":"895258","content":"Answer A & C\ngcloud compute health-checks create and after create the backend config of thr loadbalancer"},{"timestamp":"1689340320.0","comment_id":"775573","poster":"pk349","upvote_count":"1","content":"Answer is AC. Most load balancers do not use legacy health checks, with exception of target pool-based Network load balancer requires legacy health checks. Definitely, you won’t find health check option in VPC network section"},{"upvote_count":"2","comment_id":"686046","poster":"Mr_MIXER007","timestamp":"1680598200.0","content":"Selected Answer: AC\nAAAAAAAAA&&&&&&&&&&CCCCCCCCCCCC"},{"timestamp":"1677564720.0","poster":"Jasonwcc","comment_id":"653274","upvote_count":"1","content":"Answer is AC.\nMost load balancers do not use legacy health checks, with exception of target pool-based Network load balancer requires legacy health checks.\nDefinitely, you won’t find health check option in VPC network section"},{"content":"Answer is : A and C","comment_id":"516705","poster":"kumarp6","timestamp":"1656937380.0","upvote_count":"1"},{"content":"Answer is A&C","upvote_count":"1","poster":"kumarp6","comment_id":"515892","timestamp":"1656856620.0"},{"timestamp":"1656601200.0","comment_id":"513537","content":"This one is tricky!\nAnswers are A&C... \nThough you can create a legacy health check for HTTP(S) load balancing via gcloud tools -however it's ONLY for Target Pools, not instance groups. look here:\nhttps://cloud.google.com/load-balancing/docs/health-check-concepts\n\nAnd also - the question is not referring to an HTTP(S) Load BalancER...it's referring to HTTP(S) load BalancING.\n\nAlso when they are referring to 'new' health check - they imply to the NEW instance group. so selecting an existing HC is feasible since it's 'new' to the instance group... ;)\n\nThoughts?","upvote_count":"2","poster":"desertlotus1211"},{"timestamp":"1647518220.0","content":"Answer is A&C\nLegacy health check is only applicable and required when you use target pool based NLB.\nFor almost all other load balancer types, you MUST use regular, non-legacy health checks where the protocol matches the load balancer's backend service protocol.\nhttps://cloud.google.com/load-balancing/docs/health-check-concepts#category_and_protocol","upvote_count":"2","comment_id":"446522","poster":"PeppaPig"},{"upvote_count":"2","poster":"[Removed]","timestamp":"1634776860.0","content":"I support AD, legacy can not use console. https://cloud.google.com/load-balancing/docs/health-checks#console_4","comment_id":"339983"},{"timestamp":"1634459820.0","poster":"CloudTrip","comment_id":"337436","upvote_count":"1","content":"Answer is A,C as \"Traffic Director and most load balancers use non-legacy health checks, but target pool-based Network Load Balancing requires that you use legacy health check\". This question asks about https load balancer so definitely C makes a better choice than E."},{"timestamp":"1632410760.0","comment_id":"318287","poster":"Vidyasagar","upvote_count":"1","content":"A and C"},{"upvote_count":"1","poster":"mamh","comment_id":"309446","timestamp":"1631502960.0","content":"Although the Cloud Console's health checks page lists and allows you to edit both health checks and legacy health checks, you cannot create a new legacy health check from the Cloud Console's health checks page.\nAD\n\nTo create a legacy health check, use the Cloud Console's network load balancer page or use this section's gcloud or API instructions.\n\nhttps://cloud.google.com/load-balancing/docs/health-checks#legacy-health-checks"},{"poster":"eeghai7thioyaiR4","content":"Why not B: health checks are in the \"instance groups\" section, not the \"vpc network\" one\nWhy not C: you want to create a new check, so selecting an existing one is not an option\nWhy not E: you cannot create legacy health check using the GCP console\n\nSo at the end:\n- can you create a check via gcloud: yes\n- can you create a legacy check via gcloud: yes too\n\nBoth the \"current\" and the \"legacy\" health checks can be used for http load balancing\n\n-> A and D","timestamp":"1630500780.0","upvote_count":"2","comment_id":"301411"},{"upvote_count":"3","comment_id":"244101","timestamp":"1623703380.0","content":"I think its AC: Although the Cloud Console's health checks page lists and allows you to edit both health checks and legacy health checks, you cannot create a new legacy health check from the Cloud Console's health checks page.\n\nTo create a legacy health check, use the Cloud Console's network load balancer page or use this section's gcloud or API instructions.\nhttps://cloud.google.com/load-balancing/docs/health-checks#create-legacy-health-checks","poster":"glk"},{"comment_id":"223894","poster":"[Removed]","timestamp":"1621529160.0","upvote_count":"1","content":"Ans - AC"},{"timestamp":"1620730560.0","upvote_count":"1","content":"It's A and C","comment_id":"217251","poster":"lukedj87"},{"poster":"marekmatula2020","comment_id":"216393","upvote_count":"4","timestamp":"1620619920.0","content":"The question is about the HTTP(S) load balancer so the A and C are correct."}],"exam_id":8,"url":"https://www.examtopics.com/discussions/google/view/36634-exam-professional-cloud-network-engineer-topic-1-question-69/","choices":{"C":"Create a new health check, or select an existing one, when you complete the load balancer's backend configuration in the GCP Console.","D":"Create a new legacy health check using the gcloud command line tool.","A":"Create a new health check using the gcloud command line tool.","E":"Create a new legacy health check using the Health checks section in the GCP Console.","B":"Create a new health check using the VPC Network section in the GCP Console."},"question_id":195,"isMC":true,"question_text":"You are creating an instance group and need to create a new health check for HTTP(s) load balancing.\nWhich two methods can you use to accomplish this? (Choose two.)","answers_community":["AC (100%)"],"answer_images":[],"topic":"1"}],"exam":{"numberOfQuestions":228,"provider":"Google","id":8,"lastUpdated":"11 Apr 2025","isMCOnly":true,"isBeta":false,"isImplemented":true,"name":"Professional Cloud Network Engineer"},"currentPage":39},"__N_SSP":true}