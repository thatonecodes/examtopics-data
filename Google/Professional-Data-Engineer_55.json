{"pageProps":{"questions":[{"id":"MTj2wpQpepZRBLY8MGtO","question_images":[],"answer_description":"","answers_community":["CD (50%)","CE (17%)","DE (17%)","Other"],"answer_images":[],"discussion":[{"poster":"jvg637","upvote_count":"54","comment_id":"64341","content":"C = A standard SQL query cannot reference a view defined using legacy SQL syntax.\nD = For the ODBC drivers is needed a service account which will get a standard Bigquery role.","timestamp":"1584285240.0"},{"poster":"[Removed]","content":"Answer: CD","comment_id":"66454","upvote_count":"10","timestamp":"1584783480.0"},{"content":"Selected Answer: AD\nODBC requires standard SQL. **A** creates a new view using standard SQL, and **D** sets up a service account for authentication. Options A and D are necessary.","comment_id":"1398892","upvote_count":"1","poster":"Parandhaman_Margan","timestamp":"1742049960.0"},{"content":"Selected Answer: AD\nTo ensure applications can connect to BigQuery via an ODBC connection, take these two actions: \nA. Create a new view over events using standard SQL to replace the legacy SQL view, ensuring compatibility with ODBC, and D. Create a service account for the ODBC connection to authenticate and access the data. These steps ensure the applications can query the last 14 days of data efficiently and securely. Avoid unnecessary changes like creating new tables or custom IAM roles.","timestamp":"1737627480.0","comment_id":"1345291","upvote_count":"1","poster":"Yad_datatonic"},{"content":"Selected Answer: AD\nA. Legacy SQL views are not compatible with ODBC connections, which require standard SQL. Creating a new view in standard SQL ensures compatibility for the applications connecting via ODBC.\nD. ODBC connections to BigQuery require authentication, typically via a service account with the appropriate permissions. Setting up a service account ensures secure and reliable access.","timestamp":"1732639080.0","poster":"Smakyel79","comment_id":"1318186","upvote_count":"1"},{"upvote_count":"1","content":"I think question should be rewrite slightly like which 3 actions should you take rather than 2 ..\nThen answer would be A,D and E..No ambiguity then","timestamp":"1704612960.0","comment_id":"1115637","poster":"Vullibabu"},{"poster":"task_7","content":"Selected Answer: BD\nODBC connections require standard SQL, not legacy SQL.\nService account for the ODBC connection","upvote_count":"1","timestamp":"1704432540.0","comment_id":"1114283"},{"poster":"Bahubali1988","content":"This dump is full of wrong answers - not sure which one to go for.","upvote_count":"2","timestamp":"1696222200.0","comment_id":"1022809"},{"upvote_count":"1","comment_id":"973235","timestamp":"1691255940.0","poster":"alihabib","content":"CD..... C because, ODBC drivers dont support switch b/w legacy SQL & google SQL, hence better to create a new view from recent partitioned table & D as Google best practice for role binding"},{"poster":"baht","comment_id":"926036","content":"the answer is C & D","upvote_count":"1","timestamp":"1687012860.0"},{"timestamp":"1676374200.0","upvote_count":"2","poster":"musumusu","content":"answer: A & D\nConfusion here: Legacy SQL vs Standard, BQ supports legacy SQL but ODBC or Most RDBMS connection doesn't support Legacy SQL, so in this case we need to create a new view on existing view or replace the existing one by changing syntax. \nFor ODBC, you just need a service account to authenticate as its external service connection. Option E is not necessary.","comments":[{"upvote_count":"1","timestamp":"1677191700.0","poster":"musumusu","comment_id":"819843","content":"Go for B, create a new view from the table, If you modify the syntex in option A, its also mean you created a new view on table :P"}],"comment_id":"808311"},{"comment_id":"788767","timestamp":"1674740580.0","content":"Selected Answer: DE\nD. Create a service account for the ODBC connection to use for authentication. This service account will be used to authenticate the ODBC connection, and will be granted specific permissions to access the BigQuery resources.\nE. Create a Cloud IAM role for the ODBC connection and shared events. This role will be used to grant permissions to the service account created in step D, and will allow the applications to access the events view in BigQuery.\nCreating a new view over events using standard SQL may also be beneficial to improve performance and compatibility with the applications, but is not required for the ODBC connection to work.","poster":"PolyMoe","upvote_count":"4"},{"comment_id":"781269","poster":"samdhimal","content":"INFO:\n- The majority of the data analyzed is placed in a time-partitioned table named events_partitioned. \n- To reduce the cost of queries, your organization created a view called events, which queries only the last 14 days of data. \n- The view is described in legacy SQL. \nQUESTION:\nNext month, existing applications will be connecting to BigQuery to read the events data via an ODBC connection. You need to ensure the applications can connect. Which two actions should you take? (Choose two.)\n\n-> First and foremost we need to understand the information. So our actual data is stored in events_partitioned table. The organization is currently using view called events to reduce the cost. \n-> Since the view called events only has last 14 days of data we cannot use that view.\n-> We also cannot use that view because standard SQL is not used to describe the view. In order to connectt ODBC we need a view described by standard SQL.","upvote_count":"3","timestamp":"1674143700.0","comments":[{"upvote_count":"1","poster":"samdhimal","timestamp":"1674180060.0","comment_id":"781777","content":"A. Create a new view over events using standard SQL\n-> Wrong, events view contains only last 14 days of data and also it uses Legacy SQL.\n\nB. Create a new partitioned table using a standard SQL query\n-> Partitioned Table is not helpful in this situation.Hence, I am ruling it out.\n\nC. Create a new view over events_partitioned using standard SQL\n-> Correct this is exactly what we need. \n1.We need to create a new view over events_partitioned.\n2. We need to use Standard SQL.\nThis is a valid option.\n\nD. Create a service account for the ODBC connection to use for authentication.\n- Correct answer because we are required to authenticate before ODBC connection.\n\nE. Create a Google Cloud Identity and Access Management (Cloud IAM) role for the ODBC connection and shared ג€eventsג€\n- This option is of no use in this scenario"}]},{"poster":"GCPpro","comment_id":"780855","upvote_count":"1","timestamp":"1674113880.0","content":"CE is the correct answer"},{"timestamp":"1661538480.0","comment_id":"652317","content":"Selected Answer: CD\nneeded a service account for ODBC drivers\nstandard SQL vs legacy SQL.","upvote_count":"2","poster":"MisuLava"},{"timestamp":"1658246280.0","comment_id":"633664","upvote_count":"4","poster":"Smaks","comments":[{"content":"typo - D; E","timestamp":"1658246340.0","comment_id":"633666","upvote_count":"4","poster":"Smaks"}],"content":"Selected Answer: CE\n1. Create Services account from IAM & Admin\n2. Add Services account permission Roles as \"BigQuery Admin\" or any custom Role.\nOther options are not related ' to ensure the applications can connect'"},{"timestamp":"1646410980.0","upvote_count":"1","content":"Selected Answer: CD\nAs stated by jvg637\n\nC = A standard SQL query cannot reference a view defined using legacy SQL syntax.\nD = For the ODBC drivers is needed a service account which will get a standard Bigquery role.","comment_id":"560895","poster":"Arkon88"},{"content":"Selected Answer: CD\nAs stated by jvg637\n\nC = A standard SQL query cannot reference a view defined using legacy SQL syntax.\nD = For the ODBC drivers is needed a service account which will get a standard Bigquery role.","comment_id":"516747","timestamp":"1641308280.0","poster":"medeis_jar","upvote_count":"3"},{"comment_id":"504395","content":"Selected Answer: CD\nA standard SQL query cannot reference a view defined using legacy SQL syntax. In order to connect through ODBC connection, we need to use standard SQL. So, we need to create a new view over events_partitioned table using standard SQL which is C. Need service account to connect through ODBC which is option D. Check the links below.\nI am not sure about A whether we can create a view over another view which was built using legacy SQL\nhttps://cloud.google.com/bigquery/docs/views\nhttps://cloud.google.com/community/tutorials/bigquery-from-excel\nhttps://www.simba.com/products/BigQuery/doc/ODBC_InstallGuide/mac/content/odbc/bq/configuring/authenticating/serviceaccount.htm","poster":"MaxNRG","comments":[{"timestamp":"1639853160.0","content":"It has to be standard because of this:\nGoogle has collaborated with Magnitude Simba to provide ODBC and JDBC drivers that leverage the power of BigQuery’s –>standard SQL.\nOn what should we build the view, on the events_partitioned, just like the view you had before but in standard SQL.\nno sense in creating a new partitioned table as B says.\nTo let it access the data you should access with a service account.\nYou can configure the driver to authenticate the connection with a Google service account. When you authenticate your connection this way, the driver handles authentication on behalf of the service account, so that an individual user account is not directly involved and no user input is required.\nSo I think is C and D.","comment_id":"504396","poster":"MaxNRG","upvote_count":"2"}],"timestamp":"1639853160.0","upvote_count":"6"},{"timestamp":"1637924940.0","comment_id":"487286","upvote_count":"8","poster":"JG123","content":"Why there are so many wrong answers? Examtopics.com are you enjoying paid subscription by giving random answers from people?\nAns: C,D"},{"timestamp":"1634328360.0","content":"Ans: C & D.\n\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/migrating-from-legacy-sql#logical_views","upvote_count":"2","comment_id":"462789","poster":"anji007"},{"timestamp":"1615600800.0","upvote_count":"3","comment_id":"309339","poster":"BhupiSG","content":"Correct: CD"},{"poster":"Tanmoyk","content":"Should be C,D","comment_id":"175025","timestamp":"1599462840.0","upvote_count":"3"},{"poster":"haroldbenites","timestamp":"1597795980.0","content":"C y D is correct","upvote_count":"3","comment_id":"161166"},{"poster":"dambilwa","timestamp":"1593056880.0","content":"Options [C&D] are most appropriate","upvote_count":"3","comment_id":"119073"},{"upvote_count":"5","comment_id":"76558","timestamp":"1587330120.0","content":"This is very confusing. \nGoogle documentation mentions that Simba ODBC driver leverages Standard SQL (https://cloud.google.com/bigquery/providers/simba-drivers), whereas Simba documentation mentions it supports both Legacy SQL and Standard SQL (https://www.simba.com/products/BigQuery/doc/ODBC_InstallGuide/win/content/odbc/bq/windows/advanced.htm).\nAlso, the Simba ODBC supports authentication using Service Account and User Account.\nDepending on how you interpret the question, any 2 amongst C,D,E might be a good answer","poster":"Ganshank"},{"content":"Answer: C, D\nDescription: Cannot create standard sql view on top of legacy sql, service account to connect cloud","poster":"[Removed]","upvote_count":"6","comment_id":"68707","timestamp":"1585362780.0"}],"answer_ET":"CD","topic":"1","answer":"CD","question_text":"Your organization has been collecting and analyzing data in Google BigQuery for 6 months. The majority of the data analyzed is placed in a time-partitioned table named events_partitioned. To reduce the cost of queries, your organization created a view called events, which queries only the last 14 days of data. The view is described in legacy SQL. Next month, existing applications will be connecting to BigQuery to read the events data via an ODBC connection. You need to ensure the applications can connect. Which two actions should you take? (Choose two.)","url":"https://www.examtopics.com/discussions/google/view/16669-exam-professional-data-engineer-topic-1-question-55/","timestamp":"2020-03-15 16:14:00","unix_timestamp":1584285240,"exam_id":11,"isMC":true,"choices":{"E":"Create a Google Cloud Identity and Access Management (Cloud IAM) role for the ODBC connection and shared ג€eventsג€","B":"Create a new partitioned table using a standard SQL query","D":"Create a service account for the ODBC connection to use for authentication","C":"Create a new view over events_partitioned using standard SQL","A":"Create a new view over events using standard SQL"},"question_id":271},{"id":"P2dSdaE0GgswBFyJYuLY","answer_ET":"A","answer_images":[],"question_text":"You have enabled the free integration between Firebase Analytics and Google BigQuery. Firebase now automatically creates a new table daily in BigQuery in the format app_events_YYYYMMDD. You want to query all of the tables for the past 30 days in legacy SQL. What should you do?","question_id":272,"timestamp":"2022-09-19 20:29:00","answer":"A","discussion":[{"content":"A. is correct according to this link:\nhttps://cloud.google.com/bigquery/docs/reference/legacy-sql","timestamp":"1663612140.0","upvote_count":"9","poster":"damaldon","comment_id":"673550"},{"timestamp":"1727013420.0","upvote_count":"1","comment_id":"1287748","content":"Selected Answer: A\nhttps://cloud.google.com/bigquery/docs/reference/legacy-sql#table-date-range","poster":"baimus"},{"comments":[{"timestamp":"1715946960.0","poster":"mark1223jkh","comment_id":"1212873","upvote_count":"2","content":"We actually have, look at the documentation, \n\nhttps://cloud.google.com/bigquery/docs/reference/legacy-sql"}],"comment_id":"1156788","content":"Selected Answer: B\nWe don’t have TABLE DATE RANGE function in legacy SQL. Answer should be B","upvote_count":"1","timestamp":"1708643280.0","poster":"Preetmehta1234"},{"comment_id":"1043671","upvote_count":"1","timestamp":"1697310120.0","content":"Selected Answer: A\nThe recommended action is to use the TABLE_DATE_RANGE function (option A). This function allows you to specify a range of dates to query across multiple tables.","poster":"AjoseO"},{"content":"Selected Answer: A\nThe TABLE_DATE_RANGE function in BigQuery is a table wildcard function that can be used to query a range of daily tables. It takes two arguments: a table prefix and a date range. The table prefix is the beginning of the table names, and the date range is the start and end dates of the tables to be queried.\n\nThe TABLE_DATE_RANGE function will expand to cover all tables in the dataset that match the table prefix and are within the date range. For example, if you have a dataset that contains daily tables named my_table_20230804, my_table_20230805, and my_table_20230806, you could use the TABLE_DATE_RANGE function to query all of the tables in the dataset between August 4, 2023 and August 6, 2023 as follows:\nSELECT *\nFROM TABLE_DATE_RANGE('my_table_', '2023-08-04', '2023-08-06');","upvote_count":"3","poster":"Nirca","comment_id":"1023637","timestamp":"1696312320.0"},{"upvote_count":"2","poster":"samdhimal","comments":[{"content":"Example:\nSELECT *\nFROM TABLE_DATE_RANGE(app_events_, TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY), CURRENT_TIMESTAMP())","upvote_count":"2","poster":"samdhimal","timestamp":"1674436320.0","comment_id":"784837"}],"comment_id":"781780","content":"A Is correct.\n\nTABLE_DATE_RANGE() : Queries multiple daily tables that span a date range.","timestamp":"1674180240.0"},{"upvote_count":"1","timestamp":"1671192840.0","poster":"DipT","comment_id":"747154","content":"Selected Answer: A\nhttps://cloud.google.com/bigquery/docs/reference/legacy-sql\nTABLE_DATE_RANGE() Queries multiple daily tables that span a date range."},{"upvote_count":"1","content":"A. is Correct...\nfrom...https://cloud.google.com/blog/products/management-tools/using-bigquery-and-firebase-analytics-to-understand-your-mobile-app\nSELECT\n user_dim.app_info.app_platform as appPlatform,\n user_dim.device_info.device_category as deviceType,\n COUNT(user_dim.device_info.device_category) AS device_type_count FROM\nTABLE_DATE_RANGE([firebase-analytics-sample-data:android_dataset.app_events_], DATE_ADD('2016-06-07', -7, 'DAY'), CURRENT_TIMESTAMP()),\nTABLE_DATE_RANGE([firebase-analytics-sample-data:ios_dataset.app_events_], DATE_ADD('2016-06-07', -7, 'DAY'), CURRENT_TIMESTAMP())\nGROUP BY\n 1,2\nORDER BY\n device_type_count DESC","poster":"skp57","timestamp":"1668108360.0","comment_id":"715504"}],"answers_community":["A (86%)","14%"],"topic":"1","choices":{"D":"Use SELECT IF.(date >= YYYY-MM-DD AND date <= YYYY-MM-DD","B":"Use the WHERE_PARTITIONTIME pseudo column","A":"Use the TABLE_DATE_RANGE function","C":"Use WHERE date BETWEEN YYYY-MM-DD AND YYYY-MM-DD"},"unix_timestamp":1663612140,"exam_id":11,"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/82857-exam-professional-data-engineer-topic-1-question-56/","question_images":[],"isMC":true},{"id":"UoYQmKvhUgoh43PkXixC","answer_description":"","question_text":"Your company is currently setting up data pipelines for their campaign. For all the Google Cloud Pub/Sub streaming data, one of the important business requirements is to be able to periodically identify the inputs and their timings during their campaign. Engineers have decided to use windowing and transformation in Google Cloud Dataflow for this purpose. However, when testing this feature, they find that the Cloud Dataflow job fails for the all streaming insert. What is the most likely cause of this problem?","exam_id":11,"isMC":true,"answers_community":["D (58%)","A (32%)","5%"],"discussion":[{"upvote_count":"67","content":"Answer: D\nDescription: Caution: Beam’s default windowing behavior is to assign all elements of a PCollection to a single, global window and discard late data, even for unbounded PCollections. Before you use a grouping transform such as GroupByKey on an unbounded PCollection, you must do at least one of the following:\n—->>>>>>Set a non-global windowing function. See Setting your PCollection’s windowing function.\nSet a non-default trigger. This allows the global window to emit results under other conditions, since the default windowing behavior (waiting for all data to arrive) will never occur.\n—->>>>If you don’t set a non-global windowing function or a non-default trigger for your unbounded PCollection and subsequently use a grouping transform such as GroupByKey or Combine, your pipeline will generate an error upon construction and your job will fail.\nSo it looks like D","comments":[{"poster":"samdhimal","timestamp":"1674441540.0","comments":[{"timestamp":"1690030860.0","content":"You are missing that the global window is the default window that we typically use for batch processing. The global window by default waits until all data is available before processing it so if you want to use it with streaming you need to set some custom trigger so that we don't wait indefinitely to wait until we aggregate. All in all it doesn't sound right. \n\nhttps://www.youtube.com/watch?v=oJ-LueBvOcM\nhttps://www.youtube.com/watch?v=MuFA6CSti6M","upvote_count":"4","poster":"Mathew106","comment_id":"959526"}],"content":"Why not C?\nBecause I think that the most likely cause of the problem is C. They have not applied a global windowing function, which causes the job to fail when the pipeline is created.\n\nIn Dataflow, windowing is used to divide the input data into smaller time intervals, called windows. Without a windowing function, all the data may be treated as part of the same window and the pipeline may not be able to process the data correctly. In this specific scenario, the engineers are trying to use windowing and transformation in Google Cloud Dataflow to periodically identify the inputs and their timings during the campaign, so it's likely that they need to use a windowing function to divide the data into smaller time intervals in order to process it correctly. Not applying a windowing function, or applying the wrong one can cause the job to fail.\n\nSomeone Clarify? Am I missing an important point?","comment_id":"784881","upvote_count":"1"}],"poster":"[Removed]","comment_id":"68712","timestamp":"1585363320.0"},{"upvote_count":"15","comment_id":"64363","content":"Global windowing is the default behavior, so I don't think C is right.\nAn error can occur if a non-global window or a non-default trigger is not set.\nI would say D.\n(https://beam.apache.org/documentation/programming-guide/#windowing)","poster":"jvg637","timestamp":"1584288300.0"},{"comment_id":"1398894","poster":"Parandhaman_Margan","upvote_count":"1","content":"Selected Answer: A\nGoogle Cloud Dataflow requires event timestamps when using windowing in streaming mode.\nBy default, Pub/Sub messages do not have timestamps; they need to be assigned manually using withTimestampFn()","timestamp":"1742050200.0"},{"comment_id":"1345294","poster":"Yad_datatonic","content":"Selected Answer: B\nThe job fails because triggers are not set to handle late-arriving data, causing the pipeline to mishandle or drop delayed records.","timestamp":"1737628680.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1735170180.0","poster":"Rav761","content":"Selected Answer: A\nA. They have not assigned the timestamp, which causes the job to fail\n\nAnalysis: Cloud Dataflow relies on timestamps to perform windowing operations. Without proper event-time timestamps, windowing cannot be applied correctly, and the job may fail or behave unpredictably. This is a common issue when processing streaming data from Google Cloud Pub/Sub, as timestamps must be explicitly assigned if not already embedded in the data.\nThis is the most likely cause.","comment_id":"1331730"},{"content":"Selected Answer: A\nThis option is very likely, as without timestamps assigned to streaming data, the system cannot properly process time windows. Timestamps are crucial for the correct time handling in Dataflow pipelines","poster":"Erg_de","timestamp":"1730689380.0","upvote_count":"2","comment_id":"1306749"},{"upvote_count":"6","content":"The most likely cause of this problem is A. They have not assigned the timestamp, which causes the job to fail.\n\nHere's why:\n\nImportance of Timestamps in Windowing: Windowing in Dataflow relies on timestamps to group elements into windows. If timestamps are not explicitly assigned or extracted from the data, Dataflow cannot determine which elements belong to which windows, leading to failures in the job.\nLet's look at the other options:\n\nB. They have not set the triggers to accommodate the data coming in late: While triggers are important for managing late data, not setting them would not cause the job to fail for all streaming inserts. It might affect the accuracy of the results, but the job would still run.\nC & D. Global vs. Non-global Windowing: The choice between global and non-global windowing depends on the specific requirements of the analysis. While incorrect windowing choices can lead to unexpected results, they would not typically cause the job to fail completely.","comment_id":"1214365","poster":"39405bb","timestamp":"1716213000.0"},{"poster":"philli1011","content":"D\nYou have to apply a non-global windowing function because the global windowing function is a default windowing function for every pub/sub stream or batch data.","upvote_count":"1","comment_id":"1136760","timestamp":"1706706300.0"},{"comment_id":"1017059","content":"option B: They have not set the triggers to accommodate the data coming in late, which causes the job to fail.\n\nIn a streaming data processing pipeline, it's common to encounter data that arrives late, meaning it arrives after the event time has passed for the associated window. If you don't handle late data appropriately by setting triggers, it can cause issues in your pipeline, including job failures.","timestamp":"1695662820.0","poster":"MikkelRev","upvote_count":"2"},{"upvote_count":"2","poster":"Oleksandr0501","comments":[{"comment_id":"880059","content":"without a correct timestamp, the pipeline still run fine with the default timestamp. The result maybe incorrect but the job will not fail.","comments":[{"timestamp":"1682591280.0","upvote_count":"2","comment_id":"882514","content":"okay, so D maybe","poster":"Oleksandr0501"},{"upvote_count":"1","comment_id":"882521","poster":"Oleksandr0501","timestamp":"1682591700.0","content":"gpt pt2: For example, if your use case requires you to calculate a running average of values over a fixed time interval, you would likely use a non-global windowing function with a fixed time interval. On the other hand, if you need to perform a computation on the entire stream of data at once, a global windowing function might be more appropriate.\n\nSo, the choice of windowing function should be based on the specific requirements of the data processing task at hand, and it may or may not be important to apply a non-global windowing function when the pipeline is created.\n------ \nquestion says that we need to identify streaming input, time, so non-global needed, let it be d..."}],"poster":"muhusman","upvote_count":"4","timestamp":"1682406060.0"},{"timestamp":"1700252100.0","comment_id":"1073610","upvote_count":"1","poster":"emmylou","content":"Which is the moment I decided that AI was nothing to fear"}],"content":"Selected Answer: A\ngpt: The most likely cause of the problem is A, that they have not assigned the timestamp.\nIn streaming data processing, timestamps are essential for proper windowing and triggering of data. Without timestamps, the system cannot correctly determine which window a particular piece of data belongs to, or when it is safe to trigger processing of a window. If the engineers did not assign timestamps to the data, the Cloud Dataflow job would not be able to process the data correctly, and it would fail.\n\nOption B, not setting triggers to accommodate late data, is also an important consideration for streaming data processing. However, it is less likely to cause the job to fail outright than missing timestamps.\n\nOption C, not applying a global windowing function, and Option D, not applying a non-global windowing function, are also important considerations for windowing in Cloud Dataflow. However, neither of these would cause the job to fail when the pipeline is created. Instead, they would affect the performance and correctness of the data processing.","timestamp":"1682395080.0","comment_id":"879912"},{"timestamp":"1679209260.0","comment_id":"843506","poster":"lucaluca1982","content":"what about A? This can cause the job to fail","upvote_count":"2"},{"upvote_count":"1","timestamp":"1677486720.0","poster":"midgoo","content":"Selected Answer: D\nA: note that without a correct timestamp, the pipeline still run fine with the default timestamp. The result maybe incorrect but the job will not fail.\nD: For unbound collection, this will fail if any aggregation function is done.","comment_id":"823400"},{"upvote_count":"2","poster":"musumusu","content":"Answer: A\nAll Streaming Insert failed, because there is no TimeStamp added, otherwise there is already a DEFAULT global windowing function and can execute without assigning any windowing function. \nI mean first there should be Timestamp in the data, then according to our aggregation outcome either its full time (global) or batch/chunks time aggregation(non global) will be performed.","comment_id":"808334","timestamp":"1676375700.0"},{"timestamp":"1671192900.0","poster":"DipT","comment_id":"747156","upvote_count":"1","content":"Selected Answer: D\nhttps://beam.apache.org/documentation/programming-guide/#windowing\nBeam’s default windowing behavior is to assign all elements of a PCollection to a single, global window and discard late data, even for unbounded PCollections. Before you use a grouping transform such as GroupByKey on an unbounded PCollection, you must do at least one of the following:\n\nSet a non-global windowing function. See Setting your PCollection’s windowing function.\nSet a non-default trigger. This allows the global window to emit results under other conditions, since the default windowing behavior (waiting for all data to arrive) will never occur."},{"upvote_count":"1","comment_id":"669980","content":"Selected Answer: D\nAnswer is D","poster":"Ray0506","timestamp":"1663249320.0"},{"upvote_count":"2","poster":"TOXICcharlie","content":"Selected Answer: D\nCorrect answer is D. C does not make sense because for unbounded source like Pub/Sub, the global functions are applied by default. The reason for failure would be they are using specific aggregations that require non-global window functions, e.g. tumbling or hopping windows.","timestamp":"1662803700.0","comment_id":"665287"},{"timestamp":"1654700640.0","upvote_count":"1","comment_id":"613350","content":"Selected Answer: C\nC is the answer.\nhttps://beam.apache.org/documentation/programming-guide/#windowing-bounded-collections\n\n8.2.4. The single global window\nBy default, all data in a PCollection is assigned to the single global window, and late data is discarded. If your data set is of a fixed size, you can use the global window default for your PCollection (not our case because we are streaming).\nYou can use the single global window if you are working with an unbounded data set (e.g. from a streaming data source) but use caution when applying aggregating transforms such as GroupByKey and Combine. The single global window with a default trigger generally requires the entire data set to be available before processing, which is not possible with continuously updating data. To perform aggregations on an unbounded PCollection that uses global windowing, you should specify a non-default trigger for that PCollection.","poster":"FrankT2L"},{"poster":"gingercat","upvote_count":"3","content":"Why A won't cause an error?","timestamp":"1649869320.0","comment_id":"585311"},{"upvote_count":"3","content":"Selected Answer: D\nAnswer: D\nDescription: Caution: Beam’s default windowing behavior is to assign all elements of a PCollection to a single, global window and discard late data, even for unbounded PCollections. Before you use a grouping transform such as GroupByKey on an unbounded PCollection, you must do at least one of the following:\n—->>>>>>Set a non-global windowing function. See Setting your PCollection’s windowing function.\nSet a non-default trigger. This allows the global window to emit results under other conditions, since the default windowing behavior (waiting for all data to arrive) will never occur.\n—->>>>If you don’t set a non-global windowing function or a non-default trigger for your unbounded PCollection and subsequently use a grouping transform such as GroupByKey or Combine, your pipeline will generate an error upon construction and your job will fail.\nSo it looks like D","timestamp":"1648711920.0","poster":"910","comment_id":"578732"},{"upvote_count":"3","comment_id":"516750","poster":"medeis_jar","content":"Selected Answer: D\nD.\nhttps://beam.apache.org/documentation/programming-guide/#windowing","timestamp":"1641308580.0"},{"comment_id":"487529","timestamp":"1637948460.0","upvote_count":"4","content":"Why there are so many wrong answers? Examtopics.com are you enjoying paid subscription by giving random answers from people?\nAns: D","poster":"JG123"},{"comment_id":"456602","content":"From Beam documentation:\n\"If you do apply GroupByKey or CoGroupByKey to a group of unbounded PCollections without setting either a non-global windowing strategy, a trigger strategy, or both for each collection, Beam generates an IllegalStateException error at pipeline construction time.\"\n\n...as it says one of windowing or triggering or both are required. But question itself says windowing and transformations are already. So may be this particular pipeline requires both but triggering strategy is missing. So I will go with Option B.","poster":"anji007","timestamp":"1633267800.0","upvote_count":"3"},{"comment_id":"186131","upvote_count":"5","timestamp":"1600948740.0","poster":"zxing233","content":"Why not A? D can still work if you add a trigger"},{"content":"Correct Answer: D\nExplanation:-This option is correct as with unbounded (Streaming) Pub/Sub collection\nyou need to apply the non-global windowing function.","comment_id":"143905","timestamp":"1595749860.0","poster":"VishalB","upvote_count":"9"},{"comments":[{"timestamp":"1597836720.0","upvote_count":"3","content":"Right. D is correct","comment_id":"161460","poster":"haroldbenites"}],"timestamp":"1587232740.0","content":"D, global is default and for batch not streaming.","upvote_count":"7","poster":"itche_scratche","comment_id":"76140"},{"comment_id":"66460","content":"Should be D","timestamp":"1584784860.0","upvote_count":"5","poster":"[Removed]"}],"topic":"1","unix_timestamp":1584288300,"answer":"D","timestamp":"2020-03-15 17:05:00","answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/16673-exam-professional-data-engineer-topic-1-question-57/","question_images":[],"answer_ET":"D","choices":{"D":"They have not applied a non-global windowing function, which causes the job to fail when the pipeline is created","B":"They have not set the triggers to accommodate the data coming in late, which causes the job to fail","A":"They have not assigned the timestamp, which causes the job to fail","C":"They have not applied a global windowing function, which causes the job to fail when the pipeline is created"},"question_id":273},{"id":"fufE0jJGH6jXHVc102TF","discussion":[{"poster":"SteelWarrior","comment_id":"184289","comments":[{"timestamp":"1628004060.0","comment_id":"419280","content":"B. different MR jobs execute in series, adding 1 more job makes sense in this case.","poster":"Yiouk","upvote_count":"7"}],"upvote_count":"58","timestamp":"1600765860.0","content":"Should go with B. Two reasons, it is a cleaner approach with single job to handle the calibration before the data is used in the pipeline. Second, doing this step in later stages can be complex and maintenance of those jobs in the future will become challenging."},{"timestamp":"1585363920.0","content":"Answer: A\nDescription: My take on this is for sensor calibration you just need to update the transform function, rather than creating a whole new mapreduce job and storing/passing the values to next job","poster":"[Removed]","comment_id":"68716","comments":[{"comments":[{"comment_id":"1212886","upvote_count":"1","poster":"mark1223jkh","timestamp":"1715949060.0","content":"Why all jobs, change only the first job for calibration, right?"}],"comment_id":"368336","upvote_count":"11","poster":"Jphix","content":"It's B. A would involving changing every single job (notice it said jobS, plural, not a single job). If that is computationally intensive, which it is, you're repeating a computationally intense process needlessly several times. SteelWarrior and YuriP are right on this one.","timestamp":"1622155080.0"}],"upvote_count":"20"},{"timestamp":"1720172280.0","content":"Selected Answer: A\nI'll choose A. WHY? cause the process already takes DAYS and adding another step will increase the time more","poster":"Marwan95","comment_id":"1242670","upvote_count":"1"},{"timestamp":"1677567180.0","content":"What kinds of sensor calibrations exists? I don't understand how computation in pipeline would be expense due to calibration being omitted..?","poster":"jin0","comment_id":"824474","upvote_count":"1"},{"comment_id":"784886","comments":[{"comment_id":"784888","content":"Option A is not ideal, as it would be time-consuming to modify all the transformMapReduce jobs to apply sensor calibration before doing anything else, and there is a risk of introducing bugs or errors.\nOption C is not ideal, as it would rely on users to apply sensor calibration themselves, which would be inefficient and could introduce inconsistencies in the data.\nOption D is not ideal, as it would require a lot of simulation and testing to develop an algorithm that can predict the variance of data output accurately and it may not be as accurate as calibrating the sensor directly.","poster":"samdhimal","timestamp":"1674441900.0","upvote_count":"1"}],"timestamp":"1674441900.0","poster":"samdhimal","content":"B. Introduce a new MapReduce job to apply sensor calibration to raw data, and ensure all other MapReduce jobs are chained after this.\n\nThis approach would ensure that sensor calibration is systematically carried out every time the ETL process runs, as the new MapReduce job would be responsible for calibrating the sensors before the data is processed by the other steps. This would ensure that all data is calibrated before being analyzed, thus avoiding the omission of the sensor calibration step in the future.\nIt also allows you to chain all other MapReduce jobs after this one, so that the calibrated data is used in all the downstream jobs.","upvote_count":"1"},{"poster":"DipT","content":"Selected Answer: B\nIt is much cleaner approach","comment_id":"747157","upvote_count":"1","timestamp":"1671192960.0"},{"timestamp":"1671053160.0","poster":"DGames","upvote_count":"1","comment_id":"745463","content":"Selected Answer: B\nBest approach is calibration will be separate job because if we need to tune the calibration later also it would be to maintain without worries about all other jobs."},{"content":"Selected Answer: B\nShould be B. My reason, this is like an Anti corruption layer, and that's a good practice, \nC- , if you modify your transformMapReduce will be harder to test and debug, so it's a bad practice.\nC the idea de introduce manual operation is an anti patron and has a lot of problems\nD It's overkilling, a don't have sense in this scenario.","timestamp":"1670402940.0","upvote_count":"1","poster":"odacir","comment_id":"737609"},{"upvote_count":"3","poster":"ZIMARAKI","content":"Selected Answer: B\nSteelWarrior explanation is correct :)","timestamp":"1642365120.0","comment_id":"525247"},{"poster":"lord_ryder","comment_id":"524329","content":"Selected Answer: B\nSteelWarrior explanation is correct","timestamp":"1642269660.0","upvote_count":"1"},{"timestamp":"1641308760.0","comment_id":"516754","upvote_count":"1","content":"Selected Answer: B\nSteelWarrior explanation is correct","poster":"medeis_jar"},{"timestamp":"1639799700.0","comment_id":"504003","upvote_count":"1","content":"Selected Answer: B\nSteelWarrior's answer is correct","poster":"hendrixlives"},{"comment_id":"462794","upvote_count":"1","timestamp":"1634329260.0","poster":"anji007","content":"Ans: B\nAdding a new job in the beginning of chain makes more sense than updating existing chain of jobs."},{"poster":"sumanshu","content":"Vote for 'B' (introduce new job) over 'A', (instead of modifying existing job)","timestamp":"1624885800.0","comment_id":"392953","upvote_count":"5"},{"content":"Should be B. It's a Data Quality step which has to go right after Raw Ingest. Otherwise you repeat the same step unknown (see \"job_s_\" in A) number of times, possibly for no reason, therefore extending ETL time.","upvote_count":"5","poster":"YuriP","comment_id":"149623","timestamp":"1596443040.0"},{"upvote_count":"4","content":"It's between A or B.\nShould choose A","timestamp":"1584786660.0","comment_id":"66461","poster":"[Removed]"}],"question_id":274,"unix_timestamp":1584786660,"url":"https://www.examtopics.com/discussions/google/view/17090-exam-professional-data-engineer-topic-1-question-58/","isMC":true,"answers_community":["B (90%)","10%"],"answer":"B","answer_images":[],"answer_ET":"B","choices":{"C":"Add sensor calibration data to the output of the ETL process, and document that all users need to apply sensor calibration themselves.","B":"Introduce a new MapReduce job to apply sensor calibration to raw data, and ensure all other MapReduce jobs are chained after this.","D":"Develop an algorithm through simulation to predict variance of data output from the last MapReduce job based on calibration factors, and apply the correction to all data.","A":"Modify the transformMapReduce jobs to apply sensor calibration before they do anything else."},"answer_description":"","question_images":[],"exam_id":11,"question_text":"You architect a system to analyze seismic data. Your extract, transform, and load (ETL) process runs as a series of MapReduce jobs on an Apache Hadoop cluster. The ETL process takes days to process a data set because some steps are computationally expensive. Then you discover that a sensor calibration step has been omitted. How should you change your ETL process to carry out sensor calibration systematically in the future?","timestamp":"2020-03-21 11:31:00","topic":"1"},{"id":"zzYuupwq5UTqaNgslqRR","unix_timestamp":1662564900,"answer_ET":"B","question_id":275,"topic":"1","answers_community":["B (69%)","A (29%)","2%"],"choices":{"D":"Cloud Datastore","A":"BigQuery","B":"Cloud SQL","C":"Cloud BigTable"},"question_images":[],"answer_images":[],"answer_description":"","isMC":true,"exam_id":11,"url":"https://www.examtopics.com/discussions/google/view/80950-exam-professional-data-engineer-topic-1-question-59/","discussion":[{"upvote_count":"11","poster":"PolyMoe","content":"Selected Answer: B\nB. Cloud SQL would be the most appropriate choice for the online retailer in this scenario. Cloud SQL is a fully-managed relational database service that allows for easy management and analysis of data using SQL. It is well-suited for applications built on Google App Engine and can handle the transactional workload of an e-commerce application, as well as the analytical workload of a BI tool.","comment_id":"788849","timestamp":"1674746400.0"},{"upvote_count":"5","poster":"Aaronn14","comments":[{"content":"Yeah my thinking was the same, but actually cloud SQL is fine to connect BI tools to, which is specified in this question.","timestamp":"1727015640.0","upvote_count":"1","poster":"baimus","comment_id":"1287781"}],"timestamp":"1678201560.0","comment_id":"832040","content":"A. \"They want to use only a single database for this purpose\" is a key requirement. You can use BigQuery for transactions, though it is not efficient. You can not use CloudSQL for analytics. So it is probably BQ."},{"poster":"Mathew106","upvote_count":"3","timestamp":"1690031460.0","comment_id":"959535","content":"Selected Answer: B\nCloud SQL seems to fit the best. It supports transactions and can be used to run queries and do analytics.\n\nBigQuery is good for the analysis part but it's not good for managing transactions. If the question needed a database just to store the data for analysis it would be ok. But if we want to update single transactions or add them row by row, then it's not good. BigQuery is not made to support an application. It's a DW.\n\nBigTable is can not carry transactions over multiple rows and is better for large scale analytics jobs. Also we should pick it for use-cases with high throughput/low latency requirements. Seems redundant."},{"content":"Selected Answer: A\nBig Query because of analysis","poster":"Siddhesh05","timestamp":"1681668840.0","comment_id":"872022","upvote_count":"5"},{"comments":[{"poster":"baimus","comment_id":"1287780","upvote_count":"2","timestamp":"1727015580.0","content":"I can't really see that. Bigtable is only ever the right choice for noSql at vast scale."}],"comment_id":"867685","poster":"izekc","content":"Selected Answer: C\nShould be bigtable","upvote_count":"1","timestamp":"1681248540.0"},{"content":"Selected Answer: A\nI think BigQuery makes sense here. It works for transactions too.","comments":[{"upvote_count":"2","poster":"juliobs","comments":[{"poster":"Fotofilico","upvote_count":"2","comments":[{"comment_id":"1322351","content":"but, how to analyze 'combined data from multiple datasets' in cloud sql?","timestamp":"1733401260.0","upvote_count":"1","poster":"certs4pk"}],"content":"I'm an official trainer from Google and I can say that my best two options for this scenario would be Cloud SQL and BigQuery in that order. \nAlso we can consider datastore since we're using it with a web app, but it's another topic.","comment_id":"1052934","timestamp":"1698157980.0"}],"timestamp":"1680251700.0","comment_id":"856800","content":"I just did a session with an official trainer from Google that said BigTable is better."}],"poster":"juliobs","comment_id":"843123","timestamp":"1679169360.0","upvote_count":"4"},{"content":"Transactional Data need to written first by application before it could be analysed so cloudsql.","timestamp":"1677830820.0","upvote_count":"2","comment_id":"827752","poster":"ninjatech"},{"poster":"samdhimal","comments":[{"comment_id":"824486","poster":"jin0","upvote_count":"1","content":"Bigquery is a OLAP. So it could be not a answer I think.","timestamp":"1677567780.0"},{"upvote_count":"2","poster":"samdhimal","timestamp":"1674442320.0","comment_id":"784892","content":"Cloud SQL and Cloud Datastore are also good options for certain use cases, but they may not be as well-suited for this specific scenario where the retailer needs to manage and analyze large amounts of data from multiple datasets using a BI tool."}],"timestamp":"1674442260.0","content":"Both BigQuery and Cloud Bigtable are valid options for this use case, but BigQuery is better suited for this specific scenario where the retailer needs to manage and analyze large amounts of data from multiple datasets using a BI tool.\n\nBigQuery is a fully-managed, cloud-native data warehouse that enables super-fast SQL queries using the processing power of Google's infrastructure. It can handle large, complex datasets and is well-suited for both transactional and analytical workloads. It can also handle data from multiple datasets and can be integrated with other Google Cloud services, such as Dataflow, Dataproc and Looker for BI analysis.\n\nWhile Cloud Bigtable is also a good option for this use case as it is a highly scalable and performant NoSQL database that is well-suited for handling large amounts of data and high-write loads. It is not as good as BigQuery for analytical workloads and it may not be as well-suited for this specific scenario where the retailer needs to manage and analyze large amounts of data from multiple datasets using a BI tool.","comment_id":"784891","upvote_count":"2"},{"poster":"desertlotus1211","timestamp":"1673637660.0","upvote_count":"3","content":"The Community is choosing Answer B - Cloud SQL, as per the question.\nHowever when they explain - they're speaking about BQ[????] \n\nSo is it BigQuery or Cloud SQL?","comment_id":"774795"},{"timestamp":"1671193200.0","poster":"DipT","upvote_count":"1","comment_id":"747163","content":"Selected Answer: B\nhttps://cloud.google.com/bigquery/docs/partitioned-tables"},{"upvote_count":"4","comment_id":"747158","timestamp":"1671193020.0","poster":"DipT","content":"Selected Answer: B\nIt needs support for transaction so cloud sql is the choice of database and with Bigquery we can still analyze cloud sql data via federated queries https://cloud.google.com/bigquery/docs/reference/legacy-sql"},{"content":"Selected Answer: B\nMost important part of question is transaction (RDBMS) strong ACID property database. Second part analysis of data, yes possible using any BI tool its possible with RDBMS db.","poster":"DGames","timestamp":"1671053760.0","comment_id":"745469","upvote_count":"1"},{"poster":"zellck","upvote_count":"4","comment_id":"737839","content":"Selected Answer: B\nB is the answer.\n\nhttps://cloud.google.com/bigquery/docs/cloud-sql-federated-queries\nBigQuery Cloud SQL federation enables BigQuery to query data residing in Cloud SQL in real time, without copying or moving data. Query federation supports both MySQL (2nd generation) and PostgreSQL instances in Cloud SQL.","timestamp":"1670415780.0","comments":[{"content":"Agreed. Two catches here: transactional, and BI tool. Although BigQuery nowadays can handle everything, if we specifically deal with questions highlighting transactional data, I believe to differenticate services, we should choose what they primarily mean to be .","poster":"sjtesla","upvote_count":"2","comment_id":"871678","timestamp":"1681643160.0"}]},{"poster":"odacir","upvote_count":"2","timestamp":"1670403300.0","content":"Selected Answer: B\nC and D are not able to work with BI directly, so discard.\nA: It's the best option for BI for awful for transactions\nB: it's the best option for transaction, and works for BI, so this must be the answer","comment_id":"737618"},{"timestamp":"1669447800.0","upvote_count":"1","content":"Selected Answer: B\nBigQuery for Analytics and BI","comment_id":"727366","poster":"Leeeeee"},{"upvote_count":"2","timestamp":"1667573580.0","content":"Selected Answer: B\nCloud Sql is Used to store Transactional Data and supports Sql Transactions. Where as Big Query is used for Analytics.","poster":"Leelas","comment_id":"711226"},{"upvote_count":"2","content":"Cloud SQL supports transactions as well as analysis through a BI tool. Firestore/Datastore does not support SQL syntax typically needed to do analysis done by a BI tool. BigQuery is not suitable for transactional use case. BigTable does not support SQL.\nIt's A.","timestamp":"1667341740.0","poster":"Zion0722","comment_id":"709451"},{"content":"Selected Answer: B\nit is obvious.","timestamp":"1667165640.0","poster":"MisuLava","upvote_count":"1","comments":[{"timestamp":"1667165640.0","upvote_count":"1","comment_id":"708054","poster":"MisuLava","content":"I meant to choose A, BugQuery :)"}],"comment_id":"708053"},{"comment_id":"685010","timestamp":"1664735520.0","upvote_count":"2","content":"Selected Answer: B\nI'd say B. Big query is not suitable for transactional use case, and Cloud SQL supports transactions as well as analysis through a BI tool.","comments":[{"content":"Changed my mind. It's A. Even if they mention transactional, BI it's Big Query. Also, they don't mention it must be relational, in that case would be Cloud SQL. If the main focus is Business Inteligence, Big Query.","poster":"Chavoz","timestamp":"1664735640.0","comment_id":"685012","upvote_count":"1"}],"poster":"Chavoz"},{"poster":"DiegoGonL","content":"Selected Answer: A\nBigQuery for analysing","comment_id":"669702","upvote_count":"2","timestamp":"1663231860.0"},{"content":"Selected Answer: A\nBigQuery for anything Analytics","poster":"sedado77","timestamp":"1663104540.0","upvote_count":"1","comment_id":"668427"},{"comment_id":"666836","timestamp":"1662980040.0","content":"Selected Answer: A\nBigquery","poster":"badrisrinivas9","upvote_count":"1"},{"poster":"TOXICcharlie","content":"Selected Answer: A\nI feel BigQuery would make more sense here because data needs to be aggregated and there would be analytical workloads, which would benefit from bigquery's caching.","timestamp":"1662804120.0","upvote_count":"1","comment_id":"665294"},{"poster":"Remi2021","comments":[{"poster":"Remi2021","timestamp":"1663009200.0","comment_id":"667384","content":"Sorry meant A","upvote_count":"1"}],"content":"Selected Answer: B\nBigquery for analysing (BI Tool)","upvote_count":"2","timestamp":"1662564900.0","comment_id":"662648"}],"answer":"B","question_text":"An online retailer has built their current application on Google App Engine. A new initiative at the company mandates that they extend their application to allow their customers to transact directly via the application. They need to manage their shopping transactions and analyze combined data from multiple datasets using a business intelligence (BI) tool. They want to use only a single database for this purpose. Which Google Cloud database should they choose?","timestamp":"2022-09-07 17:35:00"}],"exam":{"provider":"Google","lastUpdated":"11 Apr 2025","id":11,"numberOfQuestions":319,"isImplemented":true,"isMCOnly":true,"isBeta":false,"name":"Professional Data Engineer"},"currentPage":55},"__N_SSP":true}