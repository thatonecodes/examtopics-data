{"pageProps":{"questions":[{"id":"mZAefb4OTJQS4pcNenCN","timestamp":"2025-02-27 18:44:00","question_images":[],"url":"https://www.examtopics.com/discussions/google/view/157238-exam-associate-data-practitioner-topic-1-question-14/","discussion":[{"upvote_count":"2","content":"Selected Answer: A\nThe best option is A. Create a materialized view in BigQuery that uses the SUM( ) function and the DATE_SUB( ) function. Materialized views provide the efficiency of pre-computation, automatic updates, and are directly usable in Looker Studio, making them the ideal choice for creating an automated and performant monthly inventory report. Options B, C, and D are either manual, technically inaccurate in their description, or less efficient than leveraging the power of materialized views for this reporting task.","poster":"n2183712847","comment_id":"1362663","timestamp":"1740678240.0"}],"isMC":true,"choices":{"C":"Create a BigQuery table that uses the SUM( ) function and the _PARTITIONDATE filter.","D":"Create a BigQuery table that uses the SUM( ) function and the DATE_DIFF( ) function.","B":"Create a saved query in the BigQuery console that uses the SUM( ) function and the DATE_SUB( ) function. Re-run the saved query every month, and save the results to a BigQuery table.","A":"Create a materialized view in BigQuery that uses the SUM( ) function and the DATE_SUB( ) function."},"answer_ET":"A","question_text":"Your team wants to create a monthly report to analyze inventory data that is updated daily. You need to aggregate the inventory counts by using only the most recent month of data, and save the results to be used in a Looker Studio dashboard. What should you do?","answer_images":[],"topic":"1","answer_description":"","unix_timestamp":1740678240,"exam_id":2,"answer":"A","answers_community":["A (100%)"],"question_id":6},{"id":"HL1TfCBpcC9Ftx1eLA2Y","isMC":true,"url":"https://www.examtopics.com/discussions/google/view/157237-exam-associate-data-practitioner-topic-1-question-15/","answer_description":"","question_images":[],"question_text":"You have a BigQuery dataset containing sales data. This data is actively queried for the first 6 months. After that, the data is not queried but needs to be retained for 3 years for compliance reasons. You need to implement a data management strategy that meets access and compliance requirements, while keeping cost and administrative overhead to a minimum. What should you do?","topic":"1","choices":{"B":"Partition a BigQuery table by month. After 6 months, export the data to Coldline storage. Implement a lifecycle policy to delete the data from Cloud Storage after 3 years.","A":"Use BigQuery long-term storage for the entire dataset. Set up a Cloud Run function to delete the data from BigQuery after 3 years.","C":"Set up a scheduled query to export the data to Cloud Storage after 6 months. Write a stored procedure to delete the data from BigQuery after 3 years.","D":"Store all data in a single BigQuery table without partitioning or lifecycle policies."},"exam_id":2,"answers_community":["B (100%)"],"answer_ET":"B","discussion":[{"poster":"n2183712847","content":"Selected Answer: B\nThe best option is B. Partition a BigQuery table by month. After 6 months, export the data to Coldline storage. Implement a lifecycle policy to delete the data from Cloud Storage after 3 years. Option B provides the optimal balance of meeting the 3-year data retention requirement, achieving excellent cost optimization by tiering data to Coldline, and maintaining reasonable administrative overhead. Option C is more complex and less clear on overall retention and cost due to data duplication. Option D is the simplest administratively but provides the worst cost optimization. Option A is not cost-optimized and has moderate overhead. Therefore, Option B is the most practical and well-rounded solution for this data management scenario.","upvote_count":"2","comment_id":"1362662","timestamp":"1740678120.0"}],"unix_timestamp":1740678120,"question_id":7,"answer":"B","timestamp":"2025-02-27 18:42:00","answer_images":[]},{"id":"F4J1DgiWlHyPpuv7bt69","exam_id":2,"answer_ET":"A","answer_images":[],"question_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/google/view/157236-exam-associate-data-practitioner-topic-1-question-16/","unix_timestamp":1740677940,"answers_community":["A (100%)"],"question_text":"You have created a LookML model and dashboard that shows daily sales metrics for five regional managers to use. You want to ensure that the regional managers can only see sales metrics specific to their region. You need an easy-to-implement solution. What should you do?","choices":{"B":"Create five different Explores with the sql_always_filter Explore filter applied on the region_name dimension. Set each region_name value to the corresponding region for each manager.","A":"Create a sales_region user attribute, and assign each manager’s region as the value of their user attribute. Add an access_filter Explore filter on the region_name dimension by using the sales_region user attribute.","C":"Create separate Looker dashboards for each regional manager. Set the default dashboard filter to the corresponding region for each manager.","D":"Create separate Looker instances for each regional manager. Copy the LookML model and dashboard to each instance. Provision viewer access to the corresponding manager."},"timestamp":"2025-02-27 18:39:00","answer_description":"","answer":"A","discussion":[{"poster":"n2183712847","content":"Selected Answer: A\nThe best and easiest-to-implement solution is A. Create a sales_region user attribute, and assign each manager’s region as the value of their user attribute. Add an access_filter Explore filter on the region_name dimension by using the sales_region user attribute. This option is optimal because it directly leverages Looker's built-in features for row-level security (Access Filters) and user personalization (User Attributes), making it easy to implement, scalable, maintainable, and secure. Option B (Separate Explores) is inefficient and less maintainable. Option C (Separate Dashboards) does not provide data security and is easily bypassed. Option D (Separate Looker Instances) is extreme overkill, costly, and impractical. Therefore, Option A is the clear and correct choice for an easy-to-implement and secure solution.","upvote_count":"1","timestamp":"1740677940.0","comment_id":"1362661"}],"topic":"1","question_id":8},{"id":"ZKZWbQw03CzUzbmlBzXz","answer_ET":"C","answer_images":[],"discussion":[{"content":"Selected Answer: C\n\"remove all malicious SQL injections before storing the data in BigQuery\"\n\nIf you are preforming transformation before, it is ETL.\nIf you are doing after, it is ELT.\nIf you are doing before and after, ETLT","upvote_count":"2","comment_id":"1362658","poster":"n2183712847","timestamp":"1740677580.0"}],"question_images":[],"isMC":true,"question_id":9,"question_text":"You need to design a data pipeline that ingests data from CSV, Avro, and Parquet files into Cloud Storage. The data includes raw user input. You need to remove all malicious SQL injections before storing the data in BigQuery. Which data manipulation methodology should you choose?","unix_timestamp":1740677580,"timestamp":"2025-02-27 18:33:00","answer":"C","choices":{"B":"ELT","C":"ETL","D":"ETLT","A":"EL"},"answer_description":"","answers_community":["C (100%)"],"exam_id":2,"topic":"1","url":"https://www.examtopics.com/discussions/google/view/157235-exam-associate-data-practitioner-topic-1-question-17/"},{"id":"cT2xbsYpFkKl6mLqLjoF","url":"https://www.examtopics.com/discussions/google/view/156773-exam-associate-data-practitioner-topic-1-question-18/","question_text":"You are working with a large dataset of customer reviews stored in Cloud Storage. The dataset contains several inconsistencies, such as missing values, incorrect data types, and duplicate entries. You need to clean the data to ensure that it is accurate and consistent before using it for analysis. What should you do?","unix_timestamp":1739907840,"topic":"1","answer_description":"","answers_community":["A (67%)","B (33%)"],"answer_ET":"B","question_images":[],"discussion":[{"comment_id":"1362657","upvote_count":"1","poster":"n2183712847","timestamp":"1740677520.0","content":"Selected Answer: B\nThe best option is B. Use BigQuery to batch load the data into BigQuery and use SQL for cleaning and analysis. Loading directly into BigQuery and using SQL provides the optimal balance of efficiency and simplicity for cleaning large datasets before analysis by leveraging BigQuery's scalable processing for both loading and transformation. Option A (Cloud Composer + PythonOperator) adds unnecessary complexity of workflow orchestration and external processing before loading, reducing efficiency. Option C (Storage Transfer Service + Cloud Run) overcomplicates the process with extra data movement and event-driven functions, making it less direct for data cleaning. Option D (Cloud Run functions) is less efficient for large-scale data cleaning compared to BigQuery SQL's parallel processing and adds complexity before data is in BigQuery for analysis. Therefore, loading into BigQuery and using SQL is the most efficient and straightforward approach for cleaning data before analysis in this scenario."},{"comment_id":"1358425","content":"Selected Answer: A\nPythonOperator allows leveraging Python libraries (e.g., Pandas, PySpark) to perform robust data cleaning tasks:\n\nHandle missing values (e.g., imputation, filtering).\n\nFix incorrect data types (e.g., string-to-date conversions).\n\nRemove duplicates (e.g., using deduplication logic).","timestamp":"1739907840.0","poster":"SaquibHerman","upvote_count":"2"}],"choices":{"D":"Use Cloud Run functions to clean the data and load it into BigQuery. Use SQL for analysis.","C":"Use Storage Transfer Service to move the data to a different Cloud Storage bucket. Use event triggers to invoke Cloud Run functions to load the data into BigQuery. Use SQL for analysis.","A":"Use the PythonOperator in Cloud Composer to clean the data and load it into BigQuery. Use SQL for analysis.","B":"Use BigQuery to batch load the data into BigQuery. Use SQL for cleaning and analysis."},"answer":"A","isMC":true,"exam_id":2,"answer_images":[],"question_id":10,"timestamp":"2025-02-18 20:44:00"}],"exam":{"isMCOnly":true,"lastUpdated":"11 Apr 2025","name":"Associate Data Practitioner","id":2,"isBeta":false,"isImplemented":true,"numberOfQuestions":72,"provider":"Google"},"currentPage":2},"__N_SSP":true}