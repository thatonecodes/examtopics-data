{"pageProps":{"questions":[{"id":"2QjogQmPprH1jsUMcbKe","question_id":156,"isMC":true,"unix_timestamp":1675236720,"answer_images":[],"answers_community":["C (65%)","B (29%)","6%"],"choices":{"D":"1. Implement a rolling update pattern by replacing the Pods gradually with the new release version.\n2. Validate the application's performance for the new subset of users during the rollout, and roll back if an issue arises.","C":"1. Install the Anthos Service Mesh on your GKE cluster.\n2. Create two Deployments on the GKE cluster, and label them with different version names.\n3. Implement an Istio routing rule to send a small percentage of traffic to the Deployment that references the new version of the application.","A":"1. Configure your CI/CD pipeline to update the Deployment manifest file by replacing the container version with the latest version.\n2. Recreate the Pods in your cluster by applying the Deployment manifest file.\n3. Validate the application's performance by comparing its functionality with the previous release version, and roll back if an issue arises.","B":"1. Create a second namespace on GKE for the new release version.\n2. Create a Deployment configuration for the second namespace with the desired number of Pods.\n3. Deploy new container versions in the second namespace.\n4. Update the Ingress configuration to route traffic to the namespace with the new container versions."},"answer_ET":"C","question_images":[],"timestamp":"2023-02-01 08:32:00","answer_description":"","answer":"C","topic":"1","question_text":"You need to deploy an internet-facing microservices application to Google Kubernetes Engine (GKE). You want to validate new features using the A/B testing method. You have the following requirements for deploying new container image releases:\n• There is no downtime when new container images are deployed.\n• New production releases are tested and verified using a subset of production users.\n\nWhat should you do?","url":"https://www.examtopics.com/discussions/google/view/97516-exam-professional-cloud-developer-topic-1-question-239/","discussion":[{"timestamp":"1709105100.0","poster":"Pime13","content":"Selected Answer: C\nhttps://cloud.google.com/architecture/implementing-deployment-and-testing-strategies-on-gke#perform_an_ab_test \n\ni would say C: \nTo try this pattern, you perform the following steps:\n\nDeploy the current version of the application (app:current) on the GKE cluster.\nDeploy a new version of the application (app:new) alongside the current version.\nUse Istio to route incoming requests that have the username test in the request's cookie to app:new. All other requests are routed to app:current.","comment_id":"824511","upvote_count":"6"},{"poster":"__rajan__","upvote_count":"1","timestamp":"1727185800.0","content":"Selected Answer: C\nC is correct.","comment_id":"1015848"},{"comment_id":"975667","upvote_count":"1","content":"Selected Answer: C\nC looks good since \"send a small percentage of traffic to the Deployment that references the new version of the application\" for A/B testing.\nD is close but not perfect for the said requirements.","poster":"purushi","timestamp":"1723121040.0"},{"comment_id":"920259","timestamp":"1718048880.0","upvote_count":"2","poster":"zanhsieh","content":"Selected Answer: C\nC. The keywords, \"A/B testing\", \"verified using a subset of production users\", mean we need canary deployment. \nA: No. In-place deployment.\nB: No. This is Blue/Green deployment, but Ingress config (=manifest) does not have way to specify subset of traffic routing to different namespace.\nC: Yes. \nD: No, there's no mechanism on Ingress / Services manifests that can specify a subset of users, plus this is rolling update (=in-place deployment)"},{"comment_id":"888611","timestamp":"1714746540.0","content":"Selected Answer: C\nI couldn't find the wrong point in Option C.\nAnd it's cool way.\nI think in option B, some accidents possibly occur in the case the communication occurred between some microservices includeing new container.","poster":"NewComer200","upvote_count":"1"},{"content":"Selected Answer: B\nActually according to this link , its B\nhttps://cloud.google.com/kubernetes-engine/docs/tutorials/hello-app#deploying_a_new_version_of_the_sample_app","comment_id":"799155","poster":"TNT87","upvote_count":"1","timestamp":"1707165300.0"},{"content":"Selected Answer: B\nThis approach allows you to deploy new container images without downtime, as the traffic is only being redirected to the new namespace once the Deployment is ready. This also allows you to test and verify the new production release using a subset of production users by routing only a portion of the traffic to the new namespace.","comment_id":"797803","comments":[{"upvote_count":"2","content":"Option D, which implements a rolling update pattern, can result in some downtime as Pods are gradually replaced with the new release version. While this approach can minimize the impact of any issues with the new release, it does not meet the requirement of \"no downtime when new container images are deployed.\" Option D would be a suitable approach for situations where downtime is acceptable and can be managed, but it does not meet the requirements specified in this scenario.","comment_id":"797804","poster":"mrvergara","timestamp":"1707041280.0"}],"timestamp":"1707041280.0","poster":"mrvergara","upvote_count":"4"},{"poster":"TNT87","upvote_count":"1","content":"Selected Answer: D\nhttps://auth0.com/blog/deployment-strategies-in-kubernetes/\n Rolling updates are ideal because they allow you to deploy an application slowly with minimal overhead, minimal performance impact, and minimal","comment_id":"794896","timestamp":"1706772720.0"}],"exam_id":7},{"id":"4AO1VV0VJUWzEI7r7J7s","question_images":[],"exam_id":7,"answers_community":["A (100%)"],"question_id":157,"answer_description":"","choices":{"B":"Create a separate project for the user to run jobs.","A":"Ask the user to run the jobs as batch jobs.","C":"Add the user as a job.user role in the existing project.","D":"Allow the user to run jobs when important workloads are not running."},"isMC":true,"unix_timestamp":1594866840,"topic":"1","answer_images":[],"answer":"A","answer_ET":"A","url":"https://www.examtopics.com/discussions/google/view/25854-exam-professional-cloud-developer-topic-1-question-24/","discussion":[{"poster":"syu31svc","timestamp":"1624678560.0","content":"Option A makes the most sense\n\nB is wrong since it will incur more costs which is not what the qn wants\nC is definitely out as creating roles is not what the qn is asking for\nD is wrong as it would not minimise effort","comment_id":"390890","upvote_count":"11"},{"upvote_count":"5","poster":"mlyu","comment_id":"136129","timestamp":"1594866840.0","comments":[{"upvote_count":"1","comment_id":"361440","poster":"mastodilu","timestamp":"1621433220.0","content":"this seems like the perfect scenario for batch jobs"}],"content":"Answer is A\nhttps://cloud.google.com/bigquery/docs/running-queries#batch"},{"timestamp":"1733318160.0","content":"Selected Answer: A\nAs read from google cloud docs \"Batch query jobs. With these jobs, BigQuery queues each batch query on your behalf and then starts the query when idle resources are available, usually within a few minutes.\"","comment_id":"1321892","poster":"forallthings","upvote_count":"1"},{"content":"Selected Answer: A\nA. Ask the user to run the jobs as batch jobs.\n\nRunning BigQuery jobs as batch jobs is a good solution when there is no concern about how long it takes to complete these jobs. Batch jobs are executed when BigQuery has available resources, which ensures that they do not interfere with high-priority workloads. This is also a cost-effective solution since it does not require additional resources or the overhead of managing a separate project. BigQuery automatically prioritizes interactive jobs over batch jobs, so important workloads are less likely to be interrupted.","comment_id":"1168201","upvote_count":"1","poster":"santoshchauhan","timestamp":"1709831340.0"},{"poster":"__rajan__","timestamp":"1695099060.0","upvote_count":"1","content":"Selected Answer: A\nBatch jobs in BigQuery are not subject to the usual quota limits and do not count towards your concurrent rate limit, which makes them suitable for running large queries and reducing costs. They are executed when system resources become available, so there might be a delay, but since the user isn’t concerned about the time it takes to run these jobs, this would be a suitable solution","comment_id":"1011061"},{"content":"Selected Answer: A\nCorrect Answer:A","upvote_count":"1","timestamp":"1684691580.0","comment_id":"903438","poster":"gc_exam2022"},{"upvote_count":"2","poster":"sbonessi","content":"Selected Answer: A\nDefinitly the correct answer is A","timestamp":"1683956820.0","comment_id":"896423"},{"content":"Option A is the correct answer. By running the jobs as batch jobs, the user can specify a priority level for their jobs, allowing them to be run when system resources are available. This minimizes the impact on important workloads and allows the user to run their jobs without interrupting other users. Additionally, batch jobs are generally less expensive to run than interactive queries, so this option would also minimize cost to the company. Option B is not a good solution because it would involve creating a separate project for the user to run their jobs, which would add unnecessary complexity and effort. Option C is not a good solution because the job.user role does not provide any additional permissions beyond those of the bigquery.user role, which the user likely already has. Option D is not a good solution because it would require manual intervention to determine when important workloads are not running, which would be difficult to manage and could lead to delays in running the user's jobs.","timestamp":"1673159520.0","comment_id":"769123","upvote_count":"1","poster":"omermahgoub"},{"comment_id":"731807","poster":"jcataluna","content":"Selected Answer: A\nA is correct","timestamp":"1669829460.0","upvote_count":"1"},{"comment_id":"649185","timestamp":"1660973760.0","upvote_count":"2","content":"Selected Answer: A\nA is correct","poster":"tomato123"},{"comment_id":"527152","timestamp":"1642557540.0","poster":"herocc","content":"A is right","upvote_count":"1"},{"comment_id":"519423","content":"Selected Answer: A\nA is more suitable answer here","upvote_count":"4","poster":"ParagSanyashiv","timestamp":"1641634920.0"},{"upvote_count":"3","comment_id":"215449","poster":"saurabh1805","content":"A is best answer","timestamp":"1604860860.0"}],"question_text":"Your company has a BigQuery data mart that provides analytics information to hundreds of employees. One user of wants to run jobs without interrupting important workloads. This user isn't concerned about the time it takes to run these jobs. You want to fulfill this request while minimizing cost to the company and the effort required on your part.\nWhat should you do?","timestamp":"2020-07-16 04:34:00"},{"id":"Adk9bPMuN4rdViqMVA7A","answer":"D","answer_ET":"D","timestamp":"2023-02-01 08:07:00","answer_images":[],"question_id":158,"choices":{"B":"Create a new namespace for each environment in the existing cluster, and define resource quotas.","D":"Create a new namespace for each team in the existing cluster, and define resource quotas.","C":"Create a new GKE cluster for each team.","A":"Create new role-based access controls (RBAC) for each team in the existing cluster, and define resource quotas."},"unix_timestamp":1675235220,"question_images":[],"question_text":"Your team manages a large Google Kubernetes Engine (GKE) cluster. Several application teams currently use the same namespace to develop microservices for the cluster. Your organization plans to onboard additional teams to create microservices. You need to configure multiple environments while ensuring the security and optimal performance of each team’s work. You want to minimize cost and follow Google-recommended best practices. What should you do?","answer_description":"","discussion":[{"upvote_count":"2","timestamp":"1720799520.0","poster":"d_ella2001","comment_id":"1246869","content":"Selected Answer: D\ncorrect answer D"},{"timestamp":"1719638580.0","comment_id":"1239047","poster":"rglearn","upvote_count":"2","content":"Selected Answer: D\nkey word- \"optimal performance of each team’s work\""},{"comment_id":"1191937","poster":"alpha_canary","content":"Selected Answer: D\nD: Creating a new namespace for each team within the existing cluster and defining resource quotas is a good way to provide isolation, manage resources, and maintain security without incurring the cost of additional clusters.\n\nRejected:\nA: While RBAC can help manage access control, it doesn't provide the same level of resource isolation and management as using namespaces.\nB: Creating a namespace for each environment doesn't account for multiple teams working in the same environment.\nC: Creating a new GKE cluster for each team could lead to higher costs and complexity. It's more efficient to use namespaces within a single cluster for team isolation.","upvote_count":"2","timestamp":"1712634840.0"},{"poster":"edoo","timestamp":"1707901200.0","content":"Selected Answer: D\nI'd like to say A, but namespacing is too important to be left aside.\nI say D.","upvote_count":"3","comment_id":"1149961"},{"comment_id":"1015874","timestamp":"1695564840.0","upvote_count":"2","content":"Selected Answer: D\nI will go with D.","poster":"__rajan__"},{"upvote_count":"1","poster":"kapara","content":"Selected Answer: B\nB is correct","comment_id":"997515","timestamp":"1693735620.0"},{"comment_id":"975673","poster":"purushi","timestamp":"1691499120.0","content":"Selected Answer: A\nI go with A. Because of Security, low cost and Google-recommended best practices. I hope there is no need to create additional namespaces since several application teams are already use the same namespace to develop microservices for the cluster.","upvote_count":"1"},{"timestamp":"1686742740.0","comment_id":"923098","comments":[{"comment_id":"997514","timestamp":"1693735560.0","poster":"kapara","content":"This is the correct answer as its the only one which addresses the question: \"You need to configure multiple environments\"","upvote_count":"1"}],"upvote_count":"4","poster":"phil_thain","content":"Selected Answer: B\nOption B is the only one which addresses the part of the question that says 'You need to configure multiple environments'"},{"upvote_count":"1","content":"Selected Answer: A\nI worried A or D.\nI judged these teams are creating a microservice for each function on a learge same application by the explain of \"to develop microservices for the cluster\" .\nIf it's true, you don't need to separate using namespace.\nI think the thing you should protect is resources, for example the spanner for develop environment, the spanner for release environment and forbidden other team's the spanner access.\nIn the case I think like that, I think this Q's answer is A.","comment_id":"888670","timestamp":"1683126900.0","poster":"NewComer200"},{"content":"Selected Answer: A\nsecurity","upvote_count":"1","timestamp":"1682356860.0","poster":"closer89","comment_id":"879561"},{"comment_id":"849664","content":"Selected Answer: D\nfor each team, hence need namespaces and quota","poster":"guruguru","upvote_count":"2","timestamp":"1679698920.0","comments":[{"timestamp":"1683128760.0","poster":"NewComer200","comment_id":"888707","upvote_count":"1","content":"You could give the Role to user or user group."}]},{"timestamp":"1676830920.0","poster":"Pime13","content":"Selected Answer: A\nTo configure more granular access to Kubernetes resources at the cluster level or within Kubernetes namespaces, you use Role-Based Access Control (RBAC). RBAC allows you to create detailed policies that define which operations and resources you allow users and service accounts to access. With RBAC, you can control access for Google Accounts, Google Cloud service accounts, and Kubernetes service accounts. T","upvote_count":"2","comment_id":"814369"},{"content":"Selected Answer: A\nhttps://cloud.google.com/kubernetes-engine/docs/best-practices/rbac","comment_id":"794893","poster":"TNT87","upvote_count":"1","timestamp":"1675235220.0"}],"url":"https://www.examtopics.com/discussions/google/view/97515-exam-professional-cloud-developer-topic-1-question-240/","topic":"1","isMC":true,"answers_community":["D (54%)","A (25%)","B (21%)"],"exam_id":7},{"id":"t1QuPWzGiDOAQ2XkkIQx","question_id":159,"discussion":[{"timestamp":"1728446580.0","poster":"alpha_canary","content":"Selected Answer: B\nhttps://cloud.google.com/sql/docs/mysql/connect-run#private-ip","comment_id":"1191938","upvote_count":"1"},{"upvote_count":"1","timestamp":"1711296960.0","comment_id":"1015875","poster":"__rajan__","content":"Selected Answer: B\nB is correct."},{"timestamp":"1691225340.0","content":"Selected Answer: B\nIt should be B, EI faced this exact challenge in one of my projects","comment_id":"798752","poster":"Maddyricky","upvote_count":"3"},{"poster":"mrvergara","upvote_count":"1","comment_id":"797812","comments":[{"comment_id":"797814","content":"Option A, configuring a Cloud SQL connection, is not possible because Cloud Run does not support direct connections to Cloud SQL instances.\n\nOption C, using the Cloud SQL Java connector, is a valid way to connect to a Cloud SQL instance but does not provide the secure and scalable VPC connectivity that is recommended by Google.\n\nOption D, connecting to an instance of the Cloud SQL Auth proxy, is a valid way to connect to a Cloud SQL instance, but it requires additional setup and maintenance, and may not be the most secure or scalable option, especially for large-scale deployments.","upvote_count":"1","timestamp":"1691136900.0","poster":"mrvergara"}],"timestamp":"1691136900.0","content":"Selected Answer: B\nOption B, using a Serverless VPC Access connector, is the recommended best practice for accessing a Cloud SQL instance from Cloud Run because it provides a secure and scalable way to connect to your internal resources.\n\nWith this option, you can connect your Cloud Run service to your internal VPC network, allowing it to access resources such as Cloud SQL instances that have internal IP addresses. This eliminates the need for a public IP address or a public network connection to your database, which can increase security and regulatory compliance."},{"timestamp":"1690866000.0","content":"Selected Answer: C\nhttps://cloud.google.com/sql/docs/mysql/connect-connectors#setup-and-usage\n If your application is written in Java you can skip this step, since you do this in the Java Cloud SQL Connector","upvote_count":"1","comment_id":"794891","poster":"TNT87","comments":[{"comment_id":"802352","timestamp":"1691510460.0","upvote_count":"2","poster":"mrvergara","comments":[{"timestamp":"1691510460.0","comment_id":"802353","comments":[{"comment_id":"821416","content":"According doc tnt87 sent - SQL connectors can't provide a network path to a Cloud SQL instance if one is not already present.","poster":"mathieu89","upvote_count":"1","timestamp":"1692955140.0"}],"content":"Option C, \"Configure your application to use the Cloud SQL Java connector,\" is a valid option, but it is not recommended by Google as the best practice. The Cloud SQL Java connector is designed to work with external IP addresses, and using it with an internal IP address can result in increased latency and potential security vulnerabilities.\n\nUsing a Serverless VPC Access connector to connect to the internal IP address, as suggested by option B, provides a more secure and performant solution. This method allows you to access the internal IP address of your Cloud SQL instance from a private network, bypassing the public internet, and avoiding exposure to security threats.","poster":"mrvergara","upvote_count":"3"}],"content":"https://cloud.google.com/sql/docs/mysql/connect-run#vpc-access\n\nIn this documentation, Google recommends using a Serverless VPC Access connector to connect to the internal IP address of a Cloud SQL instance, which is a secure and scalable way to access resources in a VPC network."}]}],"exam_id":7,"isMC":true,"choices":{"B":"Configure your Cloud Run service to use a Serverless VPC Access connector.","C":"Configure your application to use the Cloud SQL Java connector.","A":"Configure your Cloud Run service with a Cloud SQL connection.","D":"Configure your application to connect to an instance of the Cloud SQL Auth proxy."},"answer_description":"","answer_images":[],"answer":"B","answers_community":["B (86%)","14%"],"topic":"1","timestamp":"2023-02-01 08:00:00","question_text":"You have deployed a Java application to Cloud Run. Your application requires access to a database hosted on Cloud SQL. Due to regulatory requirements, your connection to the Cloud SQL instance must use its internal IP address. How should you configure the connectivity while following Google-recommended best practices?","answer_ET":"B","question_images":[],"unix_timestamp":1675234800,"url":"https://www.examtopics.com/discussions/google/view/97514-exam-professional-cloud-developer-topic-1-question-241/"},{"id":"i16nGyr4N8nthKJhpTNB","choices":{"B":"You attempted the read operation without the base64-encoded SHA256 hash of the encryption key.","A":"You attempted the read operation on the object with the customer's base64-encoded key.","D":"You attempted the read operation on the object with the base64-encoded SHA256 hash of the customer's key.","C":"You entered the same encryption algorithm specified by the customer when attempting the read operation."},"exam_id":7,"topic":"1","question_text":"Your application stores customers’ content in a Cloud Storage bucket, with each object being encrypted with the customer's encryption key. The key for each object in Cloud Storage is entered into your application by the customer. You discover that your application is receiving an HTTP 4xx error when reading the object from Cloud Storage. What is a possible cause of this error?","question_images":[],"question_id":160,"timestamp":"2023-02-01 07:47:00","answer_ET":"B","discussion":[{"upvote_count":"6","comment_id":"823830","content":"Selected Answer: B\nAccording to the documentation the SHA256 is needed in the REST API -> B\nhttps://cloud.google.com/storage/docs/encryption/using-customer-supplied-keys#rest-csek-download-object","timestamp":"1677509460.0","poster":"molntamas"},{"content":"Selected Answer: A\nBase64-encoded SHA256 hash vs. Raw Encryption Key:\n\nThe Google Cloud Storage documentation you linked mentions two approaches for customer-managed encryption keys:\n\nBase64-encoded SHA256 hash: This is primarily used for verification purposes and access control. not for reading \nOption A: Correct. Using the base64-encoded encryption key instead of the raw key bytes for reading will likely lead to a 4xx error.\nOption B: Incorrect. You don't directly use the base64-encoded SHA256 hash for reading the object, but it might be required for authentication purposes.\nOption C: Incorrect. Entering the correct encryption algorithm shouldn't lead to a 4xx error if everything else is configured correctly.\nOption D: Incorrect. Similar to option B, using the base64-encoded SHA256 hash for reading the object is not the correct approach.","timestamp":"1725780240.0","poster":"mrgarfield","comment_id":"1280243","upvote_count":"1"},{"timestamp":"1703536680.0","content":"Selected Answer: B\nas some guys said, in the link https://cloud.google.com/storage/docs/encryption/customer-supplied-keys#response we understand why B is correct","poster":"Kadhem","upvote_count":"1","comment_id":"1105527"},{"upvote_count":"1","content":"Selected Answer: B\nB is correct.","poster":"mohammeddigital","timestamp":"1703431260.0","comment_id":"1104684"},{"content":"Selected Answer: B\nB is correct.","upvote_count":"1","timestamp":"1695565140.0","comment_id":"1015878","poster":"__rajan__"},{"upvote_count":"2","poster":"purushi","content":"Selected Answer: B\n4xx is for Bad request, resource forbidden, not found and many more.\nIf we want to read the object of Cloud storage bucket programmatically, then we need to pass the same customer key that was used for encrypting the object. \n\nThe request we need to send with Base64Encode ( SHA256 Hash (customer-key ) )\nThe key set for object is SHA256 Hash (customer-key ) and while reading the Base64decode of the key will happen and comparing the Hash of the keys. If Hash are equal, then read access is permitted.","timestamp":"1691500200.0","comment_id":"975706"},{"poster":"Pime13","timestamp":"1677746640.0","comments":[{"upvote_count":"1","content":"what was the answer? did you pass?","comment_id":"916213","poster":"markware","comments":[{"timestamp":"1686052860.0","upvote_count":"1","comment_id":"916214","poster":"markware","content":"I think its A"}],"timestamp":"1686052860.0"}],"upvote_count":"2","content":"took my exam yesterday (01-03-2023) and this question was there","comment_id":"826672"},{"upvote_count":"3","comment_id":"800414","poster":"anukulk","timestamp":"1675731960.0","content":"https://cloud.google.com/storage/docs/encryption/customer-supplied-keys"},{"timestamp":"1675506540.0","comment_id":"797827","comments":[{"comments":[{"comment_id":"822481","content":"link do not exists :/","timestamp":"1677419820.0","poster":"Pime13","upvote_count":"1"}],"upvote_count":"1","poster":"mrvergara","content":"The Google Cloud Storage documentation explains how to access objects in a bucket, including the use of an encryption key. The encryption key must be base64-encoded, and it is recommended to use the base64-encoded SHA256 hash of the encryption key for secure access to the objects.\n\nHere's the link to the Google Cloud Storage documentation: https://cloud.google.com/storage/docs/access-control/using-encryption-keys#using-base64-encoded-sha256-hashes-to-authenticate","comment_id":"802361","timestamp":"1675879680.0"}],"content":"Selected Answer: D\nOption D is a possible cause of an HTTP 4xx error when reading an object from Cloud Storage because it is incorrect to use the base64-encoded SHA256 hash of the customer's encryption key to read an encrypted object. To read an encrypted object, you need to use the original encryption key, not its hash. The HTTP 4xx error could be a result of an incorrect or unsupported key format, or a key mismatch. On the other hand, using the base64-encoded key (Option A), the encryption algorithm (Option C), or the base64-encoded SHA256 hash of the encryption key (Option B) without the original encryption key would not allow the object to be decrypted and read.","upvote_count":"2","poster":"mrvergara"},{"content":"Selected Answer: B\nAnswer B, made a mistsake","upvote_count":"2","poster":"TNT87","comment_id":"795078","timestamp":"1675253340.0"},{"comment_id":"794886","timestamp":"1675234020.0","upvote_count":"4","poster":"TNT87","comments":[{"timestamp":"1675253280.0","content":"typo , its B not C","upvote_count":"1","comment_id":"795077","poster":"TNT87"}],"content":"Selected Answer: C\nYou receive an HTTP 400 error in the following cases:\n\n1.You upload an object using a customer-supplied encryption key, and you attempt to perform another operation on the object (other than requesting or updating most metadata or deleting the object) without providing the key.\n2.You upload an object using a customer-supplied encryption key, and you attempt to perform another operation on the object with an incorrect key.\n3.You upload an object without providing a customer-supplied encryption key, and you attempt to perform another operation on the object with a customer-supplied encryption key.\n4.You specify an encryption algorithm, key, or SHA256 hash that is not valid.\nPoint number 2 has the answer \nhttps://cloud.google.com/storage/docs/encryption/customer-supplied-keys#response"}],"answer":"B","answer_images":[],"unix_timestamp":1675234020,"answers_community":["B (65%)","C (20%)","Other"],"isMC":true,"url":"https://www.examtopics.com/discussions/google/view/97513-exam-professional-cloud-developer-topic-1-question-242/","answer_description":""}],"exam":{"lastUpdated":"11 Apr 2025","isImplemented":true,"provider":"Google","id":7,"isBeta":false,"name":"Professional Cloud Developer","numberOfQuestions":338,"isMCOnly":false},"currentPage":32},"__N_SSP":true}