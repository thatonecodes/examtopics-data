{"pageProps":{"questions":[{"id":"bpnM1AxmbMMtKXatLbYh","question_images":[],"unix_timestamp":1597280100,"isMC":true,"url":"https://www.examtopics.com/discussions/google/view/28353-exam-associate-cloud-engineer-topic-1-question-153/","answers_community":["B (100%)"],"exam_id":1,"choices":{"D":"Use Dataflow as a batch job, and configure the bucket as a data source.","C":"Use Google Kubernetes Engine and configure a CronJob to trigger the application using Pub/Sub.","A":"Use App Engine and configure Cloud Scheduler to trigger the application using Pub/Sub.","B":"Use Cloud Functions and configure the bucket as a trigger resource."},"topic":"1","answer_ET":"B","question_id":61,"answer_images":[],"answer":"B","discussion":[{"comment_id":"163834","timestamp":"1614027000.0","content":"Correct Answer is (B):\n\nGoogle Cloud Storage Triggers\nCloud Functions can respond to change notifications emerging from Google Cloud Storage. These notifications can be configured to trigger in response to various events inside a bucket—object creation, deletion, archiving and metadata updates.\n\nNote: Cloud Functions can only be triggered by Cloud Storage buckets in the same Google Cloud Platform project.\nEvent types\nCloud Storage events used by Cloud Functions are based on Cloud Pub/Sub Notifications for Google Cloud Storage and can be configured in a similar way.\n\nSupported trigger type values are:\n\ngoogle.storage.object.finalize\n\ngoogle.storage.object.delete\n\ngoogle.storage.object.archive\n\ngoogle.storage.object.metadataUpdate\n\nObject Finalize\nTrigger type value: google.storage.object.finalize\n\nThis event is sent when a new object is created (or an existing object is overwritten, and a new generation of that object is created) in the bucket.\n\nhttps://cloud.google.com/functions/docs/calling/storage#event_types","poster":"ESP_SAP","upvote_count":"42"},{"comment_id":"156844","poster":"francisco_guerra","comments":[{"poster":"SSPC","timestamp":"1613301840.0","content":"Sure B? Please you could share the link with the Google documentation","comment_id":"157980","upvote_count":"1","comments":[{"upvote_count":"3","content":"https://cloud.google.com/functions/docs/calling/storage","poster":"Ale1973","comment_id":"176017","timestamp":"1615228560.0"}]}],"upvote_count":"19","content":"The answer is B","timestamp":"1613184900.0"},{"upvote_count":"2","comment_id":"1111465","poster":"Cynthia2023","content":"Selected Answer: B\nGoogle Cloud Functions supports several types of triggers, allowing you to run your functions in response to various events in the Google Cloud environment or via HTTP requests. \n1. HTTP Triggers:\n• HTTP triggers allow your Cloud Function to be invoked via standard HTTP requests. These are useful for building APIs, webhooks, and other services that are accessible over the internet or within your internal network.\n2. Cloud Pub/Sub Triggers:\n• Cloud Functions can be triggered by messages published to Cloud Pub/Sub topics. This is useful for asynchronous event-driven architectures and integrating with systems that publish events to Pub/Sub.","timestamp":"1719864840.0","comments":[{"content":"3. Cloud Storage Triggers:\n• Functions can respond to changes in Google Cloud Storage, such as creating, deleting, or updating objects. This is helpful for processing uploaded files, data backups, and more.\n4. Firestore Triggers:\n• These triggers allow functions to execute in response to changes in Google Cloud Firestore data, including document creation, updates, and deletions. They are useful for syncing Firestore data with other data stores, or for handling real-time data updates.\n5. Firebase Realtime Database Triggers:\n• Cloud Functions can be triggered by changes in Firebase Realtime Database. This is similar to Firestore triggers but specific to Firebase's Realtime Database service.","timestamp":"1719864900.0","comment_id":"1111466","poster":"Cynthia2023","comments":[{"content":"6. Firebase Authentication Triggers:\n• Functions can react to Firebase Authentication events, such as user creation, deletion, or attribute updates. These triggers are useful for custom user management workflows and integration with external systems.\n7. Google Analytics for Firebase Triggers:\n• These triggers enable functions to respond to Analytics events collected by Firebase, useful for custom event processing and integrations.\n8. Scheduled (Cron) Triggers:\n• Cloud Scheduler can be used to trigger functions on a time-based schedule (cron). This is ideal for running batch jobs, regular clean-ups, or other scheduled tasks.","poster":"Cynthia2023","timestamp":"1719864900.0","comment_id":"1111467","upvote_count":"1"}],"upvote_count":"1"}]},{"comment_id":"1000521","poster":"scanner2","timestamp":"1709729880.0","upvote_count":"2","content":"Selected Answer: B\nUse \"Object finalized\" event of the Cloud Storage bucket as trigger for the Cloud Functions.\nhttps://cloud.google.com/functions/docs/calling/storage"},{"poster":"Captain1212","upvote_count":"1","content":"Selected Answer: B\nB seems the correct option, as we can use the cloud functions as per our requirement for the cloud storage bucket..","comment_id":"999065","timestamp":"1709622540.0"},{"upvote_count":"1","comment_id":"648774","content":"Selected Answer: B\nThe answer is B","timestamp":"1676796600.0","poster":"Angel_99"},{"content":"B is correct, it is required on demand when upload happens","poster":"AzureDP900","timestamp":"1671882000.0","comment_id":"621541","upvote_count":"1"},{"poster":"arvsrv","content":"Selected Answer: B\nThe answer is B","comment_id":"530736","upvote_count":"2","timestamp":"1658592960.0"},{"timestamp":"1657337040.0","comment_id":"519922","poster":"Surat","content":"I vote for B","upvote_count":"2"},{"upvote_count":"4","comment_id":"482396","content":"Selected Answer: B\nVote For B","timestamp":"1653034020.0","poster":"alaahakim"},{"comment_id":"451946","poster":"vamgcp","content":"Correct Answer is B - Use Cloud Functions and configure the bucket as a trigger resource.","upvote_count":"2","timestamp":"1648319640.0"},{"poster":"Zimcruza","content":"Question asks:\nYou want to deploy this code snippet. What should you do?\nTo me, none of the answers is relevant to DEPLOYMENT - they all are about how you get the trigger to run ...","upvote_count":"1","timestamp":"1639820160.0","comment_id":"384647"},{"timestamp":"1632084300.0","upvote_count":"2","poster":"GoCloud","comment_id":"315206","content":"vote for B"},{"timestamp":"1632049860.0","poster":"marialix87","content":"I think is B","upvote_count":"3","comment_id":"314893"},{"content":"B. Use Cloud Functions and configure the bucket as a trigger resource.","comment_id":"310682","upvote_count":"3","poster":"GCP_Student1","timestamp":"1631630820.0"},{"poster":"lutoa","comment_id":"289211","content":"Answer is B, app engine is for applications, in this case it’s just a code snippet which cloud functions is more suitable","timestamp":"1628801340.0","upvote_count":"2"},{"comment_id":"283409","content":"B - Use Cloud Functions and configure the bucket as a trigger resource.","timestamp":"1628071140.0","upvote_count":"3","poster":"victory108"},{"comment_id":"226693","content":"Most of you suggest B.\n\nsomeone chosed A - I will choose B","timestamp":"1621855980.0","poster":"Bhagirathi","upvote_count":"1"},{"content":"• B. Use Cloud Functions and configure the bucket as a trigger resource.","upvote_count":"2","timestamp":"1621681260.0","comment_id":"224966","poster":"swatititame"},{"poster":"ritzheg","timestamp":"1620209040.0","content":"B.\nhttps://cloud.google.com/functions/docs/tutorials/storage#objectives","upvote_count":"2","comment_id":"213394"},{"upvote_count":"1","timestamp":"1618003620.0","comment_id":"196995","comments":[{"poster":"obeythefist","comment_id":"557898","upvote_count":"1","timestamp":"1661660940.0","content":"How are you going to get App Engine to run a code snippet? App engine is for containers."},{"upvote_count":"1","comment_id":"462298","timestamp":"1649980200.0","poster":"aamirahal","content":"I go with A and this is why https://cloud.google.com/scheduler/docs/tut-pub-sub"}],"poster":"JJ_ME","content":"I think A.\nYou can send notifications to any Pub/Sub topic in any project for which you have sufficient permissions.\nhttps://cloud.google.com/storage/docs/pubsub-notifications#overview\n\nCloud Functions - Note that your bucket must reside in the same project as Cloud Functions\nhttps://cloud.google.com/storage/docs/pubsub-notifications#other_notification_options\n\nWithout knowing if the Storage bucket is in the same project as Cloud Functions, one must use Pub/Sub as the preferred option."},{"timestamp":"1617682380.0","content":"B for me","poster":"RockAJ","comment_id":"193933","upvote_count":"2"},{"poster":"technicalguru","content":"Agreed with B","upvote_count":"1","comment_id":"163571","timestamp":"1614001620.0"},{"content":"Yes B is correct","timestamp":"1613580300.0","poster":"SSPC","upvote_count":"2","comment_id":"160114"}],"answer_description":"","question_text":"You have created a code snippet that should be triggered whenever a new file is uploaded to a Cloud Storage bucket. You want to deploy this code snippet. What should you do?","timestamp":"2020-08-13 02:55:00"},{"id":"abLlkft7l7qVSE06Ychg","question_images":[],"topic":"1","unix_timestamp":1597156440,"answer":"B","answer_description":"","answer_images":[],"choices":{"A":"Set up a policy that uses Nearline storage for 30 days and then moves to Archive storage for three years.","C":"Set up a policy that uses Nearline storage for 30 days, then moves the Coldline for one year, and then moves to Archive storage for two years.","B":"Set up a policy that uses Standard storage for 30 days and then moves to Archive storage for three years.","D":"Set up a policy that uses Standard storage for 30 days, then moves to Coldline for one year, and then moves to Archive storage for two years."},"timestamp":"2020-08-11 16:34:00","question_text":"You have been asked to set up Object Lifecycle Management for objects stored in storage buckets. The objects are written once and accessed frequently for 30 days. After 30 days, the objects are not read again unless there is a special need. The objects should be kept for three years, and you need to minimize cost.\nWhat should you do?","answers_community":["B (100%)"],"question_id":62,"exam_id":1,"isMC":true,"answer_ET":"B","url":"https://www.examtopics.com/discussions/google/view/28130-exam-associate-cloud-engineer-topic-1-question-154/","discussion":[{"content":"Correct Answer is (B):\n \nThe key to understand the requirement is : \"The objects are written once and accessed frequently for 30 days\" \nStandard Storage\nStandard Storage is best for data that is frequently accessed (\"hot\" data) and/or stored for only brief periods of time.\n\nArchive Storage\nArchive Storage is the lowest-cost, highly durable storage service for data archiving, online backup, and disaster recovery. Unlike the \"coldest\" storage services offered by other Cloud providers, your data is available within milliseconds, not hours or days. Archive Storage is the best choice for data that you plan to access less than once a year.\n\nhttps://cloud.google.com/storage/docs/storage-classes#standard","poster":"ESP_SAP","comments":[{"timestamp":"1615638240.0","comments":[{"comment_id":"201136","poster":"gcper","timestamp":"1618589280.0","upvote_count":"5","content":"It doesn't minimize the costs. Check the costs of coldline vs archival"}],"content":"What if we chose option D to minimize the cost as asked in the question? What do you think?","upvote_count":"1","comment_id":"178651","poster":"naveedpk00"}],"timestamp":"1614028020.0","upvote_count":"54","comment_id":"163848"},{"upvote_count":"15","timestamp":"1613061240.0","poster":"SSPC","comments":[{"comment_id":"156640","comments":[{"timestamp":"1613157120.0","comment_id":"156641","content":"Sorry you are right accessed frequently for 30 days, its B","upvote_count":"6","poster":"pepepy"}],"content":"The object should be kept for three years, and you need to minimize cost, after 30 days it will be moved to archive, ans A","timestamp":"1613157060.0","upvote_count":"1","poster":"pepepy"}],"content":"I think the correct one is B. Because Nearline has a 30-day minimum storage duration.\nhttps://cloud.google.com/storage/docs/storage-classes","comment_id":"155563"},{"content":"Selected Answer: B\nThe objects are written once and accessed frequently for 30 days. Then rarely accessed.","comment_id":"1128314","upvote_count":"1","timestamp":"1721612940.0","poster":"Tanidanindo"},{"timestamp":"1715490120.0","comment_id":"1068336","upvote_count":"1","poster":"Jin1206t","content":"Selected Answer: B\nB is the correct answer"},{"content":"Selected Answer: B\nKey terms frequently accessed for 30 days -> Standard storage class.\nNot accessed unless special need for 3 years -> Archive storage class.","poster":"scanner2","comment_id":"1000523","upvote_count":"2","timestamp":"1709730060.0"},{"comment_id":"999067","content":"Selected Answer: B\nB is the correct answer, as the data for first 30 days in accessed frequently so for it we can use the standard , and after it to minimize the cost we can use the archive storage for 3 years","upvote_count":"1","poster":"Captain1212","timestamp":"1709622780.0"},{"content":"Answer is B, we cannot select A because data is accedesed frequently and nearline only allows access once per month (you can access more incurring in aditional cost but being not a cost optimized selection)","poster":"ashtonez","comment_id":"834866","upvote_count":"1","timestamp":"1694335800.0"},{"content":"Answer is A: there is a retrieval fee for data access from nearline. Please check https://cloud.google.com/storage/docs/storage-classes. So Standard storage is the cheaper option","comment_id":"774042","poster":"thaliath","upvote_count":"1","timestamp":"1689209160.0"},{"upvote_count":"1","timestamp":"1681011180.0","content":"Selected Answer: B\nB is the correct Answer,\nFrequently accessed data 'Hot Data' should be stored in Standard Storage for 30 days, \nThen this can be moved to Archive after 30 days for period of three years which is accessed only when a special need arises, to reduce cost.","comment_id":"689895","poster":"Charumathi"},{"comments":[{"poster":"taiyi078","upvote_count":"1","timestamp":"1672225620.0","comment_id":"623865","content":"Nearline storage is a low-cost, highly durable storage service for storing infrequently accessed data. Nearline storage is a better choice than Standard storage in scenarios where slightly lower availability, a 30-day minimum storage duration, and costs for data access are acceptable trade-offs for lowered at-rest storage costs."}],"timestamp":"1672225560.0","comment_id":"623864","content":"https://cloud.google.com/storage/docs/storage-classes#nearline\n\nNearline storage is ideal for data you plan to read or modify on average once per month or less. For example, if you want to continuously add files to Cloud Storage and plan to access those files once a month for analysis, Nearline storage is a great choice.\n\nNearline storage is also appropriate for data backup, long-tail multimedia content, and data archiving. Note, however, that for data accessed less frequently than once a quarter, Coldline storage or Archive storage are more cost-effective, as they offer lower storage costs.","upvote_count":"2","poster":"taiyi078"},{"timestamp":"1671882180.0","upvote_count":"1","comment_id":"621542","poster":"AzureDP900","comments":[{"timestamp":"1672002240.0","comments":[{"content":"Is B right or D?","timestamp":"1698574200.0","upvote_count":"1","comment_id":"884152","poster":"Shweta2jun"}],"poster":"AzureDP900","upvote_count":"2","comment_id":"622243","content":"I am changing it to D. Set up a policy that uses Standard storage for 30 days, then moves to Coldline for one year, and then moves to Archive storage for two years."}],"content":"B is right"},{"comment_id":"565994","poster":"pluiedust","content":"Selected Answer: B\nB for sure","upvote_count":"1","timestamp":"1662965760.0"},{"timestamp":"1658624280.0","upvote_count":"4","poster":"SleepyHitman","comment_id":"531016","content":"Selected Answer: B\nThe answer is: B\n\nStandard storage description:\nhttps://cloud.google.com/storage/docs/storage-classes#:~:text=Standard%20Storage%20is%20best%20for%20data%20that%20is%20frequently%20accessed%20(%22hot%22%20data)%20and/or%20stored%20for%20only%20brief%20periods%20of%20time.\n \nNearline storage imp description:\nhttps://cloud.google.com/storage/docs/storage-classes#:~:text=storage%20service%20for-,storing%20infrequently%20accessed%20data.,-Nearline%20Storage%20is\n \nPricing for frequent access:\nhttps://cloud.google.com/storage/pricing#:~:text=Free%20operations-,Standard%20Storage,Free,-Coldline%20Storage"},{"timestamp":"1657338060.0","comment_id":"519932","content":"Standard – Frequently access and short period\nNearline - Low cost, highly durable for infrequent data access, lower availability\nColdline –Very low cost, highly durable for infrequently accessed, 90 days minimum storage \nArchive - Lowest cost, highly durable for archiving, backup and DR, lower availability \nI will also go for B","poster":"Surat","upvote_count":"4"},{"content":"B. Set up a policy that uses Standard storage for 30 days and then moves to Archive storage for three years. \n _ clearly mentioned data access frequently for 30days and then object not read again until special need ( archive suitable).","comment_id":"507185","poster":"fazalmf","timestamp":"1655902560.0","upvote_count":"1"},{"timestamp":"1655887920.0","poster":"Naren080914","upvote_count":"1","content":"Selected Answer: B\nIf you access frequently in Nearline, it will cost you more. There is no retrieval cost for Standard. So for frequently accessed object, go with standard. Also in Standard, there is no minimum period to store the object, so don't get confused by 30 days. Ans is B.","comment_id":"506983"},{"upvote_count":"1","poster":"alaahakim","timestamp":"1653034080.0","content":"Vote For B","comment_id":"482397"},{"timestamp":"1639559160.0","comment_id":"382400","comments":[{"content":"Typo, answer should e B. Description remains the same.","comment_id":"382401","poster":"AD_0525","timestamp":"1639559220.0","upvote_count":"2"}],"poster":"AD_0525","content":"Answer should be D. object will be frequently access for first 30 days. cost of data access from standard storage is less than Nearline. Then after 30 days the objects can be moved to archive.","upvote_count":"2"},{"poster":"jahnu","comment_id":"341916","timestamp":"1635066240.0","upvote_count":"1","content":"My Answer is B. why because Nearline Storage is a low-cost, highly durable storage service for storing infrequently accessed data. so we use standard storage for frequent access."},{"content":"B, Standard since they frequently access the for 30 days and after that rarely accessed and you need to reduce cost so Archive","comment_id":"337744","poster":"kopper2019","upvote_count":"2","timestamp":"1634495460.0"},{"poster":"pondai","content":"B is my answer after 30 day no read again ,so put in archive is the way to min cost","timestamp":"1631850720.0","upvote_count":"1","comment_id":"312963"},{"poster":"Devgela","comment_id":"311956","timestamp":"1631754420.0","content":"B for sure","upvote_count":"1"},{"upvote_count":"1","content":"I will go with B","timestamp":"1630993860.0","poster":"neerajgoyal","comment_id":"304993"},{"content":"B. Set up a policy that uses Standard storage for 30 days and then moves to Archive storage for three years.","upvote_count":"2","poster":"GCP_Student1","timestamp":"1630181100.0","comment_id":"301007"},{"content":"I think B is the correct. https://cloud.google.com/storage/docs/storage-classes\nStandard Storage is best for data that is frequently accessed (\"hot\" data) and/or stored for only brief periods of time.\nNearline Storage is ideal for data you plan to read or modify on average once per month or less.\nArchive Storage is the lowest-cost, highly durable storage service for data archiving","poster":"JackGlemins","timestamp":"1629770220.0","comment_id":"297927","upvote_count":"1"},{"content":"bhai B hy","comment_id":"240060","upvote_count":"3","timestamp":"1623317880.0","poster":"Faraz2"},{"comment_id":"224974","content":"B. Set up a policy that uses Standard storage for 30 days and then moves to Archive storage for three years.","timestamp":"1621682220.0","upvote_count":"1","poster":"swatititame"},{"upvote_count":"1","content":"The correct answer is D. The cost would be really high after 3years when you decide to move to archive.","timestamp":"1618247760.0","comment_id":"198592","poster":"adeyemi5700"},{"poster":"JJ_ME","content":"I think A.\nThe question is about minimizing costs.\n\nThe objects are written ONCE and accessed frequently for 30 days.\nNearline has a minimum storage duration of 30 days, so it can be used in this scenario. Nearline storage costs are half the price of Standard. Multiply that by 30 days and that is quite a bit of savings.\nCosts for accessing objects in Standard and Nearline are the same. \nWriting the object ONCE in Nearline will higher than in Standard.\nOverall, the pricing is better for Nearline to Standard.\n\nhttps://cloud.google.com/storage/docs/storage-classes#available_storage_classes\nhttps://cloud.google.com/storage/pricing","comment_id":"197010","timestamp":"1618005360.0","upvote_count":"1","comments":[{"poster":"Rom0817","comment_id":"384707","upvote_count":"1","timestamp":"1639825680.0","content":"take note of \"accessed frequently for 30 days\" so standard storage to archive. B"}]},{"comment_id":"193935","timestamp":"1617682500.0","content":"B for me","poster":"RockAJ","upvote_count":"1"},{"upvote_count":"2","poster":"muk5658","comment_id":"186624","content":"Undoubtedly its B , as it clear asks we need to use the file frequently for 30 days which means standard and after 30 days we dont need it. So archive class is the best.","timestamp":"1616637780.0"},{"comments":[{"upvote_count":"2","poster":"[Removed]","comment_id":"183487","content":"Take back my previous comment.\n\nRead frequently for 30 days so standard storage. After that not accessed just to archive so archival storage\n\nAnswers B","timestamp":"1616315820.0"}],"upvote_count":"2","timestamp":"1615577640.0","content":"Its D, \n\n1) standard storage as it is frequently accessed for 30 days . So we can have option B and D\n2) It can be accessed after that so if it is required that if kept in nearline storage the breakage in retention changes are less compared to Coldline or Archival. So D","poster":"[Removed]","comment_id":"178346"},{"timestamp":"1615509420.0","content":"Can someone explain what Archive storage is? \n\nI taught the only storage classes where Standard(Reginal and multi-regional), Nearline and cold line","poster":"rakshak","comment_id":"177898","comments":[{"content":"seems that google changed it, now STANDARD = regional and multiregional, NEARLINE = the same (for 1 access per mont at most), COLDLINE changed (now is for 1 access per quarter at most ) and ARCHIVE STORAGE is like the former COLDLINE ( 1 access per year at most)\nhttps://cloud.google.com/storage/docs/storage-classes#standard","upvote_count":"1","comment_id":"178870","timestamp":"1615665840.0","poster":"xtian2900"}],"upvote_count":"1"},{"upvote_count":"1","content":"\"A\" is my answer.\nNearline Storage has minimum retention period of 30 days but the objects stored are only for backup or data access once a month. Standard storage should be used for frequently accessed data.","comment_id":"164131","comments":[{"content":"Sorry..i mean \"B\" is my answer","poster":"arpitmehra0","upvote_count":"4","comment_id":"164133","timestamp":"1614064500.0"}],"poster":"arpitmehra0","timestamp":"1614064380.0"},{"content":"It says \"accessed frequently\", so should be Standard instead of nearline storage. In Nearline you pay per transaction so it could be even more expensive than Standard in this scenario.\n\n\"Standard Storage is best for data that is frequently accessed (\"hot\" data) and/or stored for only brief periods of time.\"\n\nI go with A","timestamp":"1614009300.0","upvote_count":"1","comment_id":"163665","poster":"juliandm"},{"comment_id":"156662","poster":"MohammedGhouse","timestamp":"1613158560.0","upvote_count":"7","content":"\"B\" is the answer, objects are accessed frequently for 30-day"}]},{"id":"oktZrIwrWfv1lAJkvVUw","unix_timestamp":1597280220,"exam_id":1,"answer_ET":"D","question_images":[],"url":"https://www.examtopics.com/discussions/google/view/28354-exam-associate-cloud-engineer-topic-1-question-155/","discussion":[{"comment_id":"163852","content":"Correct Answer is (D):\n\nLogged information\nWithin Cloud Audit Logs, there are two types of logs:\n\nAdmin Activity logs: Entries for operations that modify the configuration or metadata of a project, bucket, or object.\n\nData Access logs: Entries for operations that modify objects or read a project, bucket, or object. There are several sub-types of data access logs:\n\nADMIN_READ: Entries for operations that read the configuration or metadata of a project, bucket, or object.\n\nDATA_READ: Entries for operations that read an object.\n\nDATA_WRITE: Entries for operations that create or modify an object.\n\nhttps://cloud.google.com/storage/docs/audit-logs#types","upvote_count":"32","poster":"ESP_SAP","timestamp":"1614028620.0"},{"comment_id":"156846","timestamp":"1613185020.0","upvote_count":"19","content":"D is the correct one","poster":"francisco_guerra","comments":[{"timestamp":"1613300400.0","upvote_count":"6","comment_id":"157964","poster":"SSPC","content":"Yes D is the correct"}]},{"upvote_count":"2","timestamp":"1724867880.0","comment_id":"1161962","content":"D is the best answer:\n- Data Access audit logs are specifically designed to track Google Cloud API operations related to data, including reads from Cloud Storage buckets.\n- These logs include details about the user or service account making the request, the time, and the specific data resource accessed.\n- Having this audit trail is essential for demonstrating adherence to regulations around sensitive data handling.\nWhy Others Aren't as Ideal:\nA: Identity-Aware Proxy (IAP): IAP focuses on controlling access to web apps behind firewalls but doesn't inherently log all data read operations.\nB: Data Loss Prevention (DLP): DLP is excellent for identifying sensitive data within your bucket but doesn't provide a continuous audit log of every access.\nC: Restricting Access: While limiting access is a security best practice, it doesn't address the legal requirement to log every read operation.","poster":"PiperMe"},{"timestamp":"1709730180.0","poster":"scanner2","upvote_count":"2","comment_id":"1000525","content":"Selected Answer: D\nEnable Data access audit logs for Cloud storage bucket\nhttps://cloud.google.com/storage/docs/audit-logging"},{"timestamp":"1709622840.0","upvote_count":"1","comment_id":"999068","poster":"Captain1212","content":"Selected Answer: D\nD is the correct answer"},{"timestamp":"1685124120.0","comment_id":"727793","upvote_count":"1","poster":"calm_fox","content":"Selected Answer: D\nOnly logical option"},{"comment_id":"621544","poster":"AzureDP900","content":"D is right for this use case","timestamp":"1671882300.0","upvote_count":"1"},{"comment_id":"602182","content":"D is correct as Data Access logs pertaining to Cloud Storage operations are not recorded by default. You have to enable them ...\nhttps://cloud.google.com/storage/docs/audit-logging","timestamp":"1668536100.0","upvote_count":"2","poster":"Akash7"},{"comment_id":"591129","poster":"wael_tn","upvote_count":"1","content":"Selected Answer: D\nI think it's D","timestamp":"1666624260.0"},{"content":"I also vote for D","poster":"Surat","timestamp":"1657338180.0","comment_id":"519937","upvote_count":"2"},{"comment_id":"433085","upvote_count":"2","content":"D is the correct Answer","poster":"Vinoth9289","timestamp":"1645973460.0"},{"comments":[{"upvote_count":"3","comment_id":"416054","poster":"YAS007","timestamp":"1643363040.0","content":"the question doesn't ask you to manage or understand sensitive data :\n\" you need to be able to record all requests that read any of the stored data\""}],"poster":"WakandaF","timestamp":"1637688000.0","comment_id":"364685","content":"seems that B is the right!\n\nCloud Data Loss Prevention (DLP) helps you to understand and manage such sensitive data. It provides fast, scalable classification and redaction for sensitive data elements. Using the Data Loss Prevention API and Cloud Functions, you can automatically scan this data before it is uploaded to the shared storage bucket.","upvote_count":"1"},{"comment_id":"309620","content":"D - Enable Data Access audit logs for the Cloud Storage API.","poster":"victory108","upvote_count":"1","timestamp":"1631521200.0"},{"poster":"EABDAJA","timestamp":"1631178180.0","upvote_count":"1","comment_id":"306329","content":"D is correct"},{"content":"D. Enable Data Access audit logs for the Cloud Storage API.","timestamp":"1630181160.0","comment_id":"301008","upvote_count":"2","poster":"GCP_Student1"},{"comment_id":"224976","timestamp":"1621682280.0","poster":"swatititame","upvote_count":"1","content":"• D. Enable Data Access audit logs for the Cloud Storage API."},{"content":"Ans is D","timestamp":"1617682680.0","comment_id":"193938","poster":"RockAJ","upvote_count":"2"},{"comment_id":"158363","upvote_count":"2","content":"agree with D","poster":"pepepy","timestamp":"1613352180.0"}],"topic":"1","question_text":"You are storing sensitive information in a Cloud Storage bucket. For legal reasons, you need to be able to record all requests that read any of the stored data. You want to make sure you comply with these requirements. What should you do?","answer_images":[],"answer":"D","answer_description":"","isMC":true,"question_id":63,"timestamp":"2020-08-13 02:57:00","answers_community":["D (100%)"],"choices":{"A":"Enable the Identity Aware Proxy API on the project.","C":"Allow only a single Service Account access to read the data.","B":"Scan the bucket using the Data Loss Prevention API.","D":"Enable Data Access audit logs for the Cloud Storage API."}},{"id":"9ut7MoTWS0XhcfY07Zgr","choices":{"B":"Create a separate billing account per sandbox project and enable BigQuery billing exports. Create a Data Studio dashboard to plot the spending per billing account.","D":"Create a single billing account for all sandbox projects and enable BigQuery billing exports. Create a Data Studio dashboard to plot the spending per project.","A":"Create a single budget for all projects and configure budget alerts on this budget.","C":"Create a budget per project and configure budget alerts on all of these budgets."},"unix_timestamp":1597307160,"answer_ET":"C","discussion":[{"content":"Correct Answer is (C):\n\nSet budgets and budget alerts\nOverview\nAvoid surprises on your bill by creating Cloud Billing budgets to monitor all of your Google Cloud charges in one place. A budget enables you to track your actual Google Cloud spend against your planned spend. After you've set a budget amount, you set budget alert threshold rules that are used to trigger email notifications. Budget alert emails help you stay informed about how your spend is tracking against your budget.\n\n2. Set budget scope\nSet the budget Scope and then click Next.\n\nIn the Projects field, select one or more projects that you want to apply the budget alert to. To apply the budget alert to all the projects in the Cloud Billing account, choose Select all.\n\nhttps://cloud.google.com/billing/docs/how-to/budgets#budget-scop","timestamp":"1598124240.0","upvote_count":"49","comment_id":"163855","comments":[{"comment_id":"543907","upvote_count":"3","content":"You're the only answer I take seriously \"Thumbs up\"","poster":"dang1986","timestamp":"1644423240.0"},{"comment_id":"642381","upvote_count":"1","timestamp":"1659614820.0","comments":[{"content":"It will be a combined budget that's why it's C","poster":"Priyanka109","comment_id":"683530","timestamp":"1664538960.0","upvote_count":"2"}],"content":"wait a minute, why not A ?\nAs you said that \n\" In the Projects field, select one or more projects that you want to apply the budget alert to. To apply the budget alert to all the projects in the Cloud Billing account, choose Select all. \"\nAs per this I should be able to create single budget for all the projects and should be able to set alert on that, why create separate budget for all 10 projects ?","poster":"bobthebuilder55110"}],"poster":"ESP_SAP"},{"timestamp":"1597858080.0","comment_id":"161709","poster":"Hjameel","content":"I think C is the best answer.","upvote_count":"10"},{"comment_id":"1295459","poster":"denno22","timestamp":"1728544560.0","content":"Selected Answer: C\nC","upvote_count":"1"},{"poster":"yehia2221","upvote_count":"1","content":"Agree, anwser C, as there is no a common billing account mentioned in the question, we need to create a budget by project.","comment_id":"1255595","timestamp":"1721983020.0"},{"upvote_count":"2","timestamp":"1704148260.0","comment_id":"1111471","content":"Selected Answer: C\nThe scope of a budget in GCP can be defined at different levels:\n1. Project-Level Budget:\n2. Billing Account-Level Budget:\n3. Specific Services or Labels:\n• GCP allows you to create budgets for specific services (like Compute Engine, Cloud Storage, etc.) or resources labeled with specific labels within a project or billing account. This level of granularity is useful for tracking costs associated with particular services or resource categories.\n4. Credits and Other Filters:\n• When setting up a budget, you can include or exclude certain types of costs, such as credits, discounts, or taxes, depending on your monitoring needs.","poster":"Cynthia2023"},{"poster":"Captain1212","comment_id":"999070","upvote_count":"2","content":"Selected Answer: C\nC is the correct answer, because question demands that which project goes over the 500 per month, to check that you need to create the budget per project","timestamp":"1693891020.0"},{"timestamp":"1668364860.0","content":"Selected Answer: C\nCorrect Answer is (C):","poster":"Kopy","comment_id":"717458","upvote_count":"1"},{"comment_id":"648799","upvote_count":"2","poster":"Angel_99","content":"Selected Answer: C\nKey is anyone goes above $500 means it requires project level","timestamp":"1660895160.0"},{"poster":"AzureDP900","upvote_count":"1","comment_id":"621552","content":"Key is anyone goes above $500 means it requires project level so C is right","timestamp":"1656064800.0"},{"poster":"wael_tn","content":"Selected Answer: C\nClearly C is the answer","comment_id":"591127","timestamp":"1650812880.0","upvote_count":"1"},{"upvote_count":"5","poster":"pondai","content":"Does anyone knows Data Studio can be alert to email?If it can't I'll pick C","comment_id":"312972","timestamp":"1615961340.0"},{"timestamp":"1614560160.0","poster":"GCP_Student1","comment_id":"301048","content":"C. Create a budget per project and configure budget alerts on all of these budgets.","upvote_count":"4"},{"comment_id":"282693","content":"I believe is A because of this...\nProjects: In the Projects field, select one or more projects that you want to apply the budget alert to. To apply the budget alert to all of the projects in the Cloud Billing account, choose Select all.\n\n Some costs are not related to a project, such as the costs of subscriptions or Support costs.\n In the budget's project scope, in the list of projects you can filter on, [Charges not specific to a project] is not an option you can select.\n If you choose Select all, then the costs in all projects, including Charges not specific to a project, are included in the budget and cost trend chart cost calculations.\n If you select one or more projects - but not all projects - then the Charges not specific to a project are not included in the budget and cost trend chart cost calculations.\n You can view your costs that are not related to a project in the billing reports. Using the projects filter in the reports page, you can select and view [Charges not specific to a project].\n\nURL: https://cloud.google.com/billing/docs/how-to/budgets","upvote_count":"3","timestamp":"1612354200.0","poster":"andregrjp","comments":[{"timestamp":"1638957360.0","content":"but how will you know who crossed the limit. what if the summation of their usage exceeds 500? the corresponding alert would be a false alarm","comment_id":"496708","poster":"Ridhanya","upvote_count":"5"}]},{"timestamp":"1606225260.0","poster":"Bhagirathi","comment_id":"226699","upvote_count":"1","content":"which one correct?"},{"content":"• C. Create a budget per project and configure budget alerts on all of these budgets.","upvote_count":"1","poster":"swatititame","timestamp":"1606051140.0","comment_id":"224978"},{"comment_id":"178878","content":"is C, not A\nwith A, i guess if you create a single budget for all projects, together they can easily beat the $500 mark and you need to know if \"one deveoper' did it.\nso one budget per project is the solution.","timestamp":"1600021440.0","poster":"xtian2900","upvote_count":"4"},{"timestamp":"1599932400.0","comment_id":"178347","upvote_count":"3","content":"Yes C is correct, I would have gone with B and D as billing export is the crucial element for billing, but both the option don't talk about notifying about spending.\nAlso, you don't want to combine the billing for all as each one can spend up to 500 so it will be better if they are individual so A is out.","poster":"[Removed]"},{"content":"You want to be notified if any of the developers are spending above $500 per month on their sandbox environment, so Answer is C","timestamp":"1599170640.0","upvote_count":"2","comment_id":"172942","poster":"pepepy"},{"content":"C is correct","timestamp":"1597844820.0","upvote_count":"4","poster":"tothecloud","comment_id":"161537"},{"comment_id":"160111","content":"Do you think \"C\" is correct?","upvote_count":"2","timestamp":"1597674900.0","poster":"SSPC"},{"upvote_count":"1","timestamp":"1597331040.0","comments":[{"timestamp":"1599583320.0","poster":"Ale1973","content":"Yes, when you are creating a budget, the budget can be scoped to focus on a specific set of resources (Projects, Products or Labels)","comment_id":"176021","upvote_count":"3"}],"content":"A budget per project?","comment_id":"157439","poster":"SSPC"}],"question_text":"You are the team lead of a group of 10 developers. You provided each developer with an individual Google Cloud Project that they can use as their personal sandbox to experiment with different Google Cloud solutions. You want to be notified if any of the developers are spending above $500 per month on their sandbox environment. What should you do?","question_id":64,"timestamp":"2020-08-13 10:26:00","answer_description":"","topic":"1","answers_community":["C (100%)"],"answer":"C","isMC":true,"exam_id":1,"question_images":[],"url":"https://www.examtopics.com/discussions/google/view/28436-exam-associate-cloud-engineer-topic-1-question-156/","answer_images":[]},{"id":"u0iM4qNrgqqEfBY2lxZw","question_id":65,"timestamp":"2020-08-12 19:40:00","choices":{"A":"Disable the flag ג€Delete boot disk when instance is deleted.ג€","D":"Enable Preemptibility on the instance.","C":"Disable Automatic restart on the instance.","B":"Enable delete protection on the instance."},"isMC":true,"answer_images":[],"answer_ET":"B","answer_description":"","topic":"1","question_images":[],"question_text":"You are deploying a production application on Compute Engine. You want to prevent anyone from accidentally destroying the instance by clicking the wrong button. What should you do?","unix_timestamp":1597254000,"url":"https://www.examtopics.com/discussions/google/view/28327-exam-associate-cloud-engineer-topic-1-question-157/","discussion":[{"content":"Correct Answer is (B):\n\nPreventing Accidental VM Deletion\nThis document describes how to protect specific VM instances from deletion by setting the deletionProtection property on an Instance resource. To learn more about VM instances, read the Instances documentation.\n\nAs part of your workload, there might be certain VM instances that are critical to running your application or services, such as an instance running a SQL server, a server used as a license manager, and so on. These VM instances might need to stay running indefinitely so you need a way to protect these VMs from being deleted.\n\nBy setting the deletionProtection flag, a VM instance can be protected from accidental deletion. If a user attempts to delete a VM instance for which you have set the deletionProtection flag, the request fails. Only a user that has been granted a role with compute.instances.create permission can reset the flag to allow the resource to be deleted.\n\nhttps://cloud.google.com/compute/docs/instances/preventing-accidental-vm-deletion","comment_id":"163860","poster":"ESP_SAP","upvote_count":"45","timestamp":"1629660420.0","comments":[{"content":"Mr.ESP_SAP, your answers are on the spot and I look forward to your notes on all the questions first.. Appreciate your effort and support for this cloud community.. :)","timestamp":"1720198020.0","poster":"Naree","comment_id":"943962","upvote_count":"6"}]},{"upvote_count":"11","content":"\"B\" is the answer","comment_id":"156667","poster":"MohammedGhouse","timestamp":"1628790000.0"},{"comment_id":"1079233","upvote_count":"3","timestamp":"1732448940.0","content":"Selected Answer: B\nB. Enable delete protection on the instance.","comments":[{"timestamp":"1732449000.0","comment_id":"1079234","upvote_count":"1","content":"Enabling delete protection helps safeguard your instances from accidental deletion. This means that even if someone attempts to delete the instance through the console or API, they will receive an error, preventing accidental deletion. It acts as an additional layer of protection to avoid critical mistakes.","poster":"mufuuuu"}],"poster":"mufuuuu"},{"content":"Selected Answer: B\nhttps://cloud.google.com/compute/docs/instances/preventing-accidental-vm-deletion","poster":"scanner2","upvote_count":"1","comment_id":"1000563","timestamp":"1725623640.0"},{"comment_id":"999075","content":"Selected Answer: B\nB is the correct answer , as it helps to prevent critical instance to get deleted","timestamp":"1725513600.0","upvote_count":"1","poster":"Captain1212"},{"upvote_count":"1","comment_id":"621554","content":"This is straight forward question, enable delete protection. B is right","timestamp":"1687600920.0","poster":"AzureDP900"},{"comment_id":"608644","upvote_count":"2","timestamp":"1685339880.0","poster":"Himadhar1997","content":"Selected Answer: B\nPreventing Accidental VM Deletion\nThis document describes how to protect specific VM instances from deletion by setting the deletionProtection property on an Instance resource. To learn more about VM instances, read the Instances documentation."},{"content":"B seems right option","upvote_count":"3","timestamp":"1673243400.0","comment_id":"519942","poster":"Surat"},{"timestamp":"1672966200.0","content":"B - on VM Enable delete protection","upvote_count":"2","poster":"kped21","comment_id":"517886"},{"timestamp":"1670917560.0","poster":"jaffarali","upvote_count":"3","comment_id":"500439","content":"Selected Answer: B\nAnswer is B. there is an Option in VM instance while creating"},{"timestamp":"1660104060.0","poster":"Sreedharveluru","comment_id":"422440","content":"Option A would not prevent , It can be used only after the damage is done. Hence B","upvote_count":"1"},{"timestamp":"1649276040.0","content":"B should be the answer.","comment_id":"329902","poster":"NARWAL","upvote_count":"1"},{"comment_id":"293034","upvote_count":"2","timestamp":"1645152840.0","content":"B. Enable delete protection on the instance.","poster":"GCP_Student1"},{"comment_id":"291924","poster":"Lomy","upvote_count":"1","timestamp":"1645029120.0","content":"B\nThe ans is B"},{"content":"B for me","poster":"RockAJ","comment_id":"193940","timestamp":"1633494060.0","upvote_count":"2"},{"comment_id":"186627","upvote_count":"3","content":"Correct Answer is 'B'","poster":"muk5658","timestamp":"1632528300.0"},{"comment_id":"157084","upvote_count":"5","content":"B is correct answer. https://cloud.google.com/compute/docs/instances/preventing-accidental-vm-deletion","poster":"SSPC","timestamp":"1628839500.0"},{"upvote_count":"3","content":"I think is B","poster":"francisco_guerra","comment_id":"156848","timestamp":"1628816280.0"}],"exam_id":1,"answers_community":["B (100%)"],"answer":"B"}],"exam":{"isMCOnly":true,"id":1,"provider":"Google","name":"Associate Cloud Engineer","lastUpdated":"11 Apr 2025","isImplemented":true,"numberOfQuestions":285,"isBeta":false},"currentPage":13},"__N_SSP":true}