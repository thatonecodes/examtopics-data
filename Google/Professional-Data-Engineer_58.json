{"pageProps":{"questions":[{"id":"dis814pFdYBNGf865aDc","question_id":286,"answer":"A","question_images":[],"timestamp":"2020-03-10 13:10:00","answer_images":[],"isMC":true,"answers_community":["A (48%)","C (46%)","7%"],"answer_ET":"A","discussion":[{"upvote_count":"74","content":"Correct Answer: A\n\nDestination is GCS and having multi-regional so A is the best option available.\n\nEven since BigQuery Data Transfer Service initially supports Google application sources like Google Ads, Campaign Manager, Google Ad Manager and YouTube but it does not support destination anything other than bq data set","poster":"VishalB","comments":[{"content":"I differ, destination is not mentioned as GCS, it only says Google Cloud which can mean BigQuery too. And ANSI SQL also points towards this direction.","timestamp":"1732791120.0","poster":"cloud_rider","upvote_count":"1","comment_id":"1319178"},{"content":"What about ANSI SQL?","timestamp":"1609549140.0","poster":"henryCho","comment_id":"257176","comments":[{"content":"I guess they are assuming that you will just query the data in Cloud Storage from BQ. The question specifically is, \"How should you set up the log data transfer into Google Cloud?\", not \"How should you set up the querying.\" ANSI SQL is a distraction!","upvote_count":"8","poster":"Jphix","timestamp":"1622158380.0","comment_id":"368364"},{"content":"use external table for it","comment_id":"610197","timestamp":"1654091340.0","poster":"nadavw","upvote_count":"2"}],"upvote_count":"7"},{"comment_id":"438974","poster":"tainangao","content":"Currently, you cannot use the BigQuery Data Transfer Service to transfer data out of BigQuery.\n\nhttps://cloud.google.com/bigquery-transfer/docs/introduction","comments":[{"comment_id":"915461","upvote_count":"4","poster":"phidelics","content":"You can use BQ Data transfer Service for Youtube channels mow","timestamp":"1685970600.0"},{"content":"but I think now you can use BigQuery Data Transfer Service for youtube channels and many other","comment_id":"905950","poster":"AmmarFasih","upvote_count":"3","timestamp":"1684936500.0"}],"timestamp":"1630741260.0","upvote_count":"10"},{"comment_id":"431129","content":"Kindly re-read the question,the question says Google Cloud not Cloud storage...once you master that you will understand the whole question and be able to pick the right answer which is C","timestamp":"1629865440.0","comments":[{"upvote_count":"4","content":"logs like stuff goes better on buckets","timestamp":"1631957460.0","poster":"yoshik","comment_id":"447016"},{"timestamp":"1630005600.0","poster":"triipinbee","comments":[{"content":"Gottem!","upvote_count":"2","poster":"Rodrigo4N","comment_id":"874165","timestamp":"1681858200.0"},{"content":"good one :)","poster":"HarshKothari21","upvote_count":"3","timestamp":"1662653760.0","comment_id":"663782"}],"upvote_count":"57","content":"all the option clearly says \"storage bucket\", once you master that, you'll realize the correct option is A","comment_id":"432541"}],"upvote_count":"16","poster":"asksathvik"}],"timestamp":"1595751540.0","comment_id":"143919"},{"upvote_count":"21","content":"None of the answers make any sense. \nBigQuery Transfer Service is for moving data from various sources (S3, Youtube etc) into BigQuery, not Google Cloud Storage.\nFurther, how are we supposed to use SQL to query data if it is stored in Cloud Storage?","poster":"Ganshank","comment_id":"76632","timestamp":"1587344460.0","comments":[{"comment_id":"123531","upvote_count":"1","timestamp":"1593523740.0","comments":[{"comments":[{"comment_id":"446435","upvote_count":"3","content":"The best option would be to use \"BigQuery Transfer Service\" to upload data to BQ. But BQ is not present as a destination, so the only working option is Multi Regional GCS","poster":"StefanoG","timestamp":"1631864400.0"}],"timestamp":"1593523980.0","upvote_count":"5","content":"Option [A] is the least worse option... for world wide teams to perform ANSI SQL Queries, it would be easier to create a ext. table or load from Multi AZ bucket... BQ Data Transfer service is used to push data in BQ, hence ruling out Option C & D","comment_id":"123533","poster":"dambilwa"}],"content":"Agreed! - All Options look wrong","poster":"dambilwa"},{"content":"Kindly re-read the question,the question says Google Cloud not Cloud storage...once you master that you will understand the whole question and be able to pick the right answer which is C","poster":"TNT87","comments":[{"upvote_count":"3","timestamp":"1601713560.0","content":"https://cloud.google.com/bigquery-transfer/docs/youtube-channel-transfer\nthis link will help to cement the answer.","comment_id":"192111","poster":"TNT87"}],"comment_id":"192110","upvote_count":"9","timestamp":"1601713500.0"}]},{"content":"Selected Answer: C\nBigQuery Data Transfer Service automates data loading into BigQuery from various sources, including Google Cloud Storage.BigQuery is specifically designed for performing ANSI SQL and other types of analysis on large datasets. Cloud Storage can be used as a staging area, especially a multi-region bucket if the teams are worldwide","timestamp":"1743022740.0","poster":"abhaya2608","comment_id":"1410566","upvote_count":"1"},{"timestamp":"1742422680.0","comment_id":"1400764","poster":"oussama7","content":"Selected Answer: C\nBigQuery Data Transfer Service (C) is the best choice because it natively integrates with YouTube, automates data transfers, and enables seamless analysis in BigQuery.","upvote_count":"1"},{"poster":"cloud_rider","upvote_count":"1","comment_id":"1319177","timestamp":"1732791060.0","content":"Selected Answer: C\nC is the correct answer as the requirement is to transfer data to Google Cloud, which means Big Query and not GCS."},{"comment_id":"1301717","content":"Selected Answer: A\nFirst thing log data is usually semi-strucutred or unstrucutred data, then GCS is more suitable in this situation. Besides, you can use external table or BigLake to run BQ query directly running on GCS.","poster":"SamuelTsch","timestamp":"1729629360.0","upvote_count":"1"},{"poster":"baimus","timestamp":"1727079660.0","upvote_count":"1","content":"This is A, because it's \"offsite backup files\". You can transfer direct from Youtube to BigQuery: https://cloud.google.com/bigquery/docs/youtube-channel-transfer, but this isn't that. It's direct from some backup files to Cloud Storage, all answers mandate that fact, and all that remains is \"should this be Storage Transfer, or BigQuery\" - clearly this is storage transfer, and all the youtube/ANSI sql stuff is just distraction.","comment_id":"1288031"},{"poster":"iooj","timestamp":"1722534720.0","comment_id":"1259499","content":"Selected Answer: A\nE. Use BigQuery Data Transfer Service to transfer the offsite backup files to BigQuery as a final destination.","upvote_count":"1"},{"content":"Using BQ data transfer we can only load data into the Bigquery storage, which means a table inside the bq dataset. \n\nWhat is Storage Transfer Service? \nStorage Transfer Service automates the transfer of data to, from, and between object and file storage systems, including Google Cloud Storage, Amazon S3, Azure Storage, on-premises data, and more. It can be used to transfer large amounts of data quickly and reliably, without the need to write any code.\n\nAs per the above lines from the Google's documentation on Storage transfer service, we can go with option A. \n\nFor additional info, have a look at the below link. \nhttps://cloud.google.com/storage-transfer/docs/overview?_gl=1*jaku2h*_ga*MjA5Mzc4OTM0LjE2ODQ3MzA5NzQ.*_ga_WH2QY8WWF5*MTcxNTA1OTM0OC4xODMuMS4xNzE1MDU5NTY3LjAuMC4w&_ga=2.6452401.-209378934.1684730974&_gac=1.162721358.1713078448.CjwKCAjw_e2wBhAEEiwAyFFFo4Da6-2MNQqNJuzAGSyJmCXdaPpXXiqaI0lkZYHlcln0IBbtWSJjLBoCp_4QAvD_BwE","timestamp":"1715052960.0","upvote_count":"1","poster":"ABKR1300","comment_id":"1207699"},{"timestamp":"1713253260.0","comment_id":"1196439","poster":"zevexWM","content":"Selected Answer: A\nCorrect answer: A\nSolution should cater a worldwide solution which makes B and D invalid.\nYou don't use BigQuery Data transfer to move data to a bucket. So C is also invalid.","upvote_count":"2"},{"timestamp":"1710418500.0","poster":"AshishDhamu","upvote_count":"2","content":"Selected Answer: A\nBigQuery Data Transfer Service can only transfer to BigQuery not in Cloud Storage. So Ans A is correct.","comment_id":"1173382"},{"timestamp":"1709463480.0","comment_id":"1164712","content":"Selected Answer: A\nNo sense to use BQ data transfer serice to store the data to a storage bucket ... It is obviously A","poster":"demoro86","upvote_count":"2"},{"content":"Selected Answer: C\nTo transfer YouTube channel data to Google Cloud for analysis, you can use the BigQuery Data Transfer Service to transfer the offsite backup files to a Cloud Storage Multi-Regional storage bucket as a final destination 1. This service allows you to automatically schedule and manage recurring load jobs for YouTube Channel reports 1. The BigQuery Data Transfer Service for YouTube Channel reports supports the following reporting options: Channel Reports (automatically loaded into BigQuery) 1. When you transfer data from a YouTube Channel into BigQuery, the data is loaded into BigQuery tables that are partitioned by date 1.","poster":"rocky48","upvote_count":"2","comment_id":"1075177","timestamp":"1700460240.0"},{"poster":"ziyunxiao","upvote_count":"1","timestamp":"1697412900.0","comment_id":"1044560","content":"The correct answer is C. https://cloud.google.com/bigquery/docs/dts-introduction"},{"poster":"exnaniantwort","timestamp":"1695338280.0","upvote_count":"2","content":"not c\nThe BigQuery Data Transfer Service automates data movement into [[[[ BigQuery ]]]] on a scheduled, managed basis. Your analytics team can lay the foundation for a BigQuery data warehouse without writing a single line of code.\n...\nAfter you configure a data transfer, the BigQuery Data Transfer Service automatically loads data into [[[[[ BigQuery ]]]]] on a regular basis.","comment_id":"1013438"},{"content":"Correct answer is A.\nAt first, it's between A and C because of the term multi-regional. However, since you cannot use the BigQuery Data Transfer Service to transfer data out of BigQuery, A is the correct answer.\n\nSource: https://cloud.google.com/bigquery/docs/dts-introduction","timestamp":"1694502780.0","upvote_count":"3","comment_id":"1005496","poster":"bmquijano7"},{"poster":"Mathew106","content":"Selected Answer: A\nWhat VishalB said is correct.","timestamp":"1690038360.0","upvote_count":"1","comment_id":"959643"},{"comment_id":"882541","timestamp":"1682592780.0","content":"GPT: Option A: Using Storage Transfer Service to transfer offsite backup files to a Cloud Storage Multi-Regional storage bucket is not the best option for this use case since this service is designed to transfer data from on-premises or other cloud providers to Google Cloud, not for data transfer from YouTube channels.\nOption B: Using Storage Transfer Service to transfer offsite backup files to a Cloud Storage Regional bucket as a final destination is also not the best option since this service is designed for data transfer from on-premises or other cloud providers to Google Cloud, not for data transfer from YouTube channels.","comments":[{"timestamp":"1682592840.0","comment_id":"882542","poster":"Oleksandr0501","content":"Option C: BigQuery Data Transfer Service is specifically designed for ingesting data into BigQuery from various sources, including YouTube channels, and supports automated data transfers on a schedule. The service can also transfer data to Cloud Storage buckets, which makes it an ideal choice for this use case. The Multi-Regional storage bucket would ensure high availability and low latency for access to the data globally.","upvote_count":"1","comments":[{"content":"Option D: Using BigQuery Data Transfer Service to transfer data to a Cloud Storage Regional storage bucket is not the best option for this use case since it may not be the most efficient or cost-effective solution for global teams that need to access the data. A multi-regional storage bucket would be more suitable for this use case.\nTherefore, option C is the best option for setting up the log data transfer from YouTube channels to Google Cloud for analysis.","poster":"Oleksandr0501","timestamp":"1682592840.0","comment_id":"882543","upvote_count":"2"}]}],"poster":"Oleksandr0501","upvote_count":"1"},{"poster":"Oleksandr0501","timestamp":"1682402940.0","comments":[{"content":"also chatgpt: Option C suggests using BigQuery Data Transfer Service to transfer the offsite backup files to a Cloud Storage Multi-Regional storage bucket as a final destination. This option is more appropriate as it allows you to easily transfer the data from YouTube channels to a Cloud Storage Multi-Regional bucket, which can then be accessed and analyzed using ANSI SQL and other types of analysis in BigQuery. ....Sh*t","poster":"Oleksandr0501","upvote_count":"1","comment_id":"880024","timestamp":"1682403360.0"}],"comment_id":"880017","content":"Option C suggests using BigQuery Data Transfer Service to transfer data to a Cloud Storage Multi-Regional storage bucket. While this is a valid approach to transfer data to a storage bucket, using a multi-regional bucket is not necessary for this use case and can incur additional cost.","upvote_count":"1"},{"upvote_count":"3","poster":"pankajk0811","content":"Answer is A\n\nAs Bigquery data transfer service cannot support loading data from offsite locations.\n\nhttps://cloud.google.com/bigquery/docs/dts-introduction","timestamp":"1681621800.0","comment_id":"871475"},{"timestamp":"1681432320.0","content":"Selected Answer: C\nI think it is talking about this one. \nThe BigQuery Data Transfer Service for YouTube allows you to automatically schedule and manage recurring load jobs for YouTube Channel reports.\nSince then, it should be \"C\"\nhttps://cloud.google.com/bigquery/docs/youtube-channel-transfer","upvote_count":"4","poster":"izekc","comment_id":"869857"},{"content":"Selected Answer: C\nShould be C","timestamp":"1681432200.0","upvote_count":"1","comment_id":"869856","poster":"izekc"},{"timestamp":"1679174580.0","comment_id":"843177","upvote_count":"1","content":"Selected Answer: C\nI vote for C.\nhttps://cloud.google.com/bigquery-transfer/docs/youtube-channel-transfer\n\nThe question never mentioned GCS as some are commenting here.","poster":"juliobs"},{"upvote_count":"1","poster":"jin0","comment_id":"824790","content":"why most of questions is not a clear..","timestamp":"1677588120.0"},{"upvote_count":"1","comment_id":"791436","timestamp":"1674973620.0","poster":"ayush_1995","content":"Selected Answer: A\nA over C as why to involve bq service if its not confirmed if its using bq or not"},{"content":"So the issue I am facing currently is my answer seems to be the first one for this questions. I can be wrong. Please someone reply with some document if I am missing something.\n\nCorrect Answer:\nOption D: Use BigQuery Data Transfer Service to transfer the offsite backup files to a Cloud Storage Regional storage bucket as a final destination.","upvote_count":"3","comments":[{"content":"This option offers several advantages for your use case:\n\nBigQuery Data Transfer Service allows for automatic, scheduled data transfer from YouTube to a Cloud Storage bucket, and then into BigQuery for analysis. This ensures that your marketing teams have access to up-to-date data for ANSI SQL and other types of analysis.\n\nUsing a Regional storage bucket ensures low-latency data access for your worldwide marketing teams. Since regional storage bucket is located in close proximity to the teams, this improves the performance of the analysis.\n\nBy using BigQuery for analysis and Cloud Storage for data storage, you can take advantage of the scalability and reliability of Google Cloud's infrastructure.","comment_id":"784741","comments":[{"content":"Option A and B are incorrect because Storage Transfer Service is used for moving large amounts of data from online or on-premises storage to a Cloud Storage bucket, but it doesn't automatically load data into BigQuery, so it wouldn't be the best choice for performing ANSI SQL and other types of analysis on YouTube channel data.\n\nOption C is incorrect because it's using BigQuery Data Transfer Service but it's transferring data to a Multi-Regional storage bucket, but Multi-Regional storage bucket is less ideal for low-latency data access for worldwide marketing teams.","timestamp":"1674426420.0","poster":"samdhimal","comment_id":"784742","upvote_count":"2"}],"timestamp":"1674426420.0","poster":"samdhimal","upvote_count":"3"}],"comment_id":"784740","poster":"samdhimal","timestamp":"1674426360.0"},{"comment_id":"778396","content":"Big Query is only multi-region in US and EU... question ask for worldwide...\nHence Answer is: A","poster":"desertlotus1211","timestamp":"1673913300.0","upvote_count":"1"},{"comment_id":"772982","timestamp":"1673481600.0","poster":"RoshanAshraf","content":"Selected Answer: A\nAns - A\nKeys - Multi region, TRANSFER FROM \"LOCAL FILES\" TO GCP. \nBigquery transfer is from Youtube App to BQ.\nBut the tricky part is Analysis for the log files and SQL which is BQ. \nAnswers are little confusing","upvote_count":"1"},{"poster":"lool","upvote_count":"3","comment_id":"769627","timestamp":"1673195700.0","content":"Selected Answer: A\nBQ can't transfer from on-prem infrastructure yet, and it can only transfer to BQ and not a storage bucket. The main purpose of the question is to transfer logs into a bucket, and not on how to analyse it.\nhttps://cloud.google.com/bigquery/docs/dts-introduction"},{"content":"Selected Answer: C\nCorrect answer:C","comment_id":"766479","timestamp":"1672911840.0","poster":"Mkumar43","upvote_count":"2"},{"upvote_count":"1","timestamp":"1672511100.0","poster":"juliansierra","content":"The possible sources in Storage Transfer Service are: Cloud Storage, S3, Azure Blob, HTTP and HTTPs URLs and File System. How do you transfer the data from YouTube channels using these possible sources in Storage Transfer Service? For me there is not an exactly correct answer. The best answer could be use BigQuery Data Transfer Service (that accept YouTube Channels data how source) and storage these data in BigQuery datasets and tables, not in Cloud Storage.","comment_id":"762816"},{"content":"Correct Answer : A\n\nThe BigQuery Data Transfer Service automates data movement into BigQuery on a scheduled, managed basis. Your analytics team can lay the foundation for a BigQuery data warehouse without writing a single line of code.\nAfter you configure a data transfer, the BigQuery Data Transfer Service automatically loads data into BigQuery on a regular basis. You can also initiate data backfills to recover from any outages or gaps. Currently, you cannot use the BigQuery Data Transfer Service to transfer data out of BigQuery.","timestamp":"1671513960.0","upvote_count":"1","poster":"slade_wilson","comment_id":"750534"},{"upvote_count":"1","poster":"lukas_xls","content":"Selected Answer: C\nIt says 'Google Cloud', not 'Google Cloud Storage'!","comment_id":"741955","timestamp":"1670781480.0"},{"upvote_count":"1","comment_id":"736868","timestamp":"1670334480.0","poster":"zellck","content":"Selected Answer: A\nA is the answer."},{"comment_id":"721787","timestamp":"1668836520.0","poster":"Jay_Krish","content":"I'm not sure of the answer but from the comments here and logically looking it seems to be A for me. All listed option clearly says \"storage bucket\" as the final destination, so it is better to use a Storage Transfer service instead of BigQuery transfer. Don't see the benefit of C over A. ANSI SQL is just to divert attention I feel and as many have commented here you could have a table created to read data from the storage.","upvote_count":"1"},{"poster":"Lynix","timestamp":"1666282200.0","content":"Selected Answer: B\nhttps://cloud.google.com/bigquery/docs/dts-introduction\nCurrently, you cannot use the BigQuery Data Transfer Service to transfer data out of BigQuery.","comment_id":"700103","upvote_count":"2"},{"content":"Selected Answer: C\n\"to perform ANSI SQL and other types of analysis\" so it must be C and Bigquery, not GCS object store","comment_id":"624721","poster":"rr4444","timestamp":"1656514080.0","upvote_count":"2"},{"timestamp":"1654651200.0","comment_id":"613005","upvote_count":"1","content":"C is the answer","poster":"GCP_data_engineer"},{"upvote_count":"1","timestamp":"1652330460.0","poster":"sw52099","content":"According to the docs, storage transfer service is for on-premise, another cloud storage bucket or data source from other cloud platform.\nhttps://cloud.google.com/storage-transfer/docs/overview#what-is-storage-transfer \n\nYoutube data is one of the data source of Bigquery Data Transfer service, from https://cloud.google.com/bigquery-transfer/docs/introduction#supported_data_sources","comment_id":"600449"},{"poster":"gabrysave","comment_id":"598178","upvote_count":"1","content":"Correct answer is A: \nworld-wide team --> multi-regional , so options are A or C.\nbig query data transfer is used to transfer data into bigquery so C is not correct (https://cloud.google.com/bigquery-transfer/docs/introduction).\nYou can connect bigquery to google cloud storage to create a table and use SQL.","timestamp":"1651937220.0"},{"content":"Selected Answer: C\nC. Bigquery data transfer to transfer data from youtube to BQ. Multiregional for worlwide clients.https://www.examtopics.com/exams/google/professional-data-engineer/view/14/#","poster":"devric","comment_id":"583455","timestamp":"1649532300.0","upvote_count":"3"},{"poster":"tavva_prudhvi","upvote_count":"1","comment_id":"582198","content":"BigQuery Transfer Service\n- Import data to BigQuery from other Google advertising SaaS applications\n- Google AdWords\n- DoubleClick\n- YouTube reports\n\nDid this ask to import data into BQ? Hence, A.","timestamp":"1649311740.0"},{"comment_id":"562615","timestamp":"1646656740.0","comments":[{"poster":"Arkon88","comment_id":"562621","upvote_count":"1","timestamp":"1646657100.0","content":"changed my mind :\nCorrect Answer: A\n\nDestination is GCS and having multi-regional so A is the best option available.\n\nEven since BigQuery Data Transfer Service initially supports Google application sources like Google Ads, Campaign Manager, Google Ad Manager and YouTube but it does not support destination anything other than bq data set"}],"content":"Selected Answer: C\nC\n\"https://cloud.google.com/bigquery-transfer/docs/youtube-channel-transfer\nhttps://cloud.google.com/bigquery-transfer/docs/introduction\"","poster":"Arkon88","upvote_count":"1"},{"upvote_count":"2","poster":"Prashant2022","content":"correct ans is C - why becuz\n1. need for analysis hence BQ\n2. from YT etc hence BQTS\n3. into Cloud\n\nGCS is not the ans, we dont have to use GCS here..","timestamp":"1646156700.0","comment_id":"558865"},{"timestamp":"1644954480.0","comments":[{"poster":"Deepakd","timestamp":"1648881540.0","content":"If you are suggesting load the data to big query using bq data transfer service , take it to gcp and create federated source , it is an overkill , increases cost and wastes resources without any benefit.","comment_id":"579743","upvote_count":"1"}],"comment_id":"547983","upvote_count":"1","poster":"sraakesh95","content":"Selected Answer: C\nIn my opinion, BigQuery has the capability to retrieve \"YouTube channel reports\" directly from a YouTube streaming channel (https://cloud.google.com/bigquery-transfer/docs/introduction)\nUpon transfer it can be exported to Cloud Storage using a simple file export or any service that google provides to have a federated datasource for BigQuery.\nThis data can again be accessed by BigQuery using the BiqQuery Transfer Service and analyzed using ANSI-SQL\nFor the second part \"worldwide\" makes it \"multi-regional\""},{"poster":"kped21","upvote_count":"1","timestamp":"1644518580.0","content":"A - Multiregion (worldwide)","comment_id":"544796"},{"comment_id":"524380","comments":[{"upvote_count":"2","comment_id":"530436","timestamp":"1642932840.0","content":"\"world-wide marketing teams to perform ANSI SQL\" -> multi-region","poster":"MaxNRG"}],"timestamp":"1642274040.0","upvote_count":"1","poster":"sraakesh95","content":"Selected Answer: B\nThere is no clear explanation of why a multi-region bucket would be necessary. There is no mention about availability or speed of data transfer"},{"timestamp":"1641545700.0","poster":"moumou","content":"i think it C:\nCurrently, the BigQuery Data Transfer Service supports loading data from the following data sources:\n\nGoogle Software as a Service (SaaS) apps\nCampaign Manager\nCloud Storage\nGoogle Ad Manager\nGoogle Ads\nGoogle Merchant Center (beta)\nGoogle Play\nSearch Ads 360 (beta)\nYouTube Channel reports\nYouTube Content Owner reports\nExternal cloud storage providers\nAmazon S3\nData warehouses\nTeradata\nAmazon Redshift\nIn addition, several third-party transfers are available in the Google Cloud Marketplace.\nSupported regions\nLike BigQuery, the BigQuery Data Transfer Service is a multi-regional resource, with many additional single regions available.","comments":[{"poster":"kapara","upvote_count":"1","comment_id":"633511","timestamp":"1658229480.0","content":"BigQuery Data Transfer Service is used to import data from the list you mentioned and not to transfer to."}],"upvote_count":"1","comment_id":"518852"},{"content":"Selected Answer: A\nhttps://cloud.google.com/architecture/streaming-avro-records-into-bigquery-using-dataflow","poster":"medeis_jar","comment_id":"517622","timestamp":"1641396420.0","upvote_count":"2"},{"content":"Selected Answer: A\nBQ DT cannot transfer data OUT only IN, so CS DT needs to be used with a multiregional bucket.","upvote_count":"1","comment_id":"517612","timestamp":"1641396060.0","poster":"medeis_jar"},{"comment_id":"506285","upvote_count":"4","poster":"MaxNRG","content":"Selected Answer: A\nBigQuery Data Transfer Service can only transfer data into BigQuery. https://cloud.google.com/bigquery-transfer/docs/transfer-service-overview\n> Currently, you cannot use the BigQuery Data Transfer Service to transfer data out of BigQuery.\nThe data has to be analysed “worldwide”, says the brief.\nTo have perfomant queries, storage should be worldwide https://cloud.google.com/bigquery/external-data-sources?hl=en#data-locations\n> When you query data in an external data source such as Cloud Storage, the data you’re querying must be in the same location as your BigQuery dataset.\nSo, A","timestamp":"1640105820.0"},{"poster":"kishanu","upvote_count":"1","content":"Its A. Using Storage Transfer service to a muti regional bucket\nOther Transfer Services:\nTransfer Appliance for moving offline data, large data sets, or data from a source with limited bandwidth\n\nBigQuery Data Transfer Service to move data from SaaS applications to BigQuery.\n\nTransfer service for on-premises data to move data from your on-premises machines to Cloud Storage.","timestamp":"1639936620.0","comment_id":"505014"},{"poster":"aldno","upvote_count":"2","timestamp":"1639508880.0","comment_id":"501628","content":"I'd say: B\nIn federated tables, you should \"co-locate\" dataset and GCS bucket. BUT not put the bucket in a multi-region: https://cloud.google.com/bigquery/docs/locations?hl=en#data-locations\nC & D are straight incorrect. BigQuery Data Transfer service moves data into BigQuery.\nAll answers are sub-optimal at best, though."},{"comment_id":"466162","comments":[{"poster":"Chelseajcole","comment_id":"466165","timestamp":"1634909580.0","content":"The question obviously is testing if you know what is BigQuery transfer service and what it used for","upvote_count":"1"},{"comment_id":"582201","poster":"tavva_prudhvi","upvote_count":"1","content":"BQTS is importing data to BQ, did the question ask that? Yes, BQTS supports Youtube ADS, but read the question properly!","timestamp":"1649311860.0"}],"content":"The answer is C. BigQuery transfer support loading data from the following data sources:\nhttps://cloud.google.com/bigquery-transfer/docs/introduction#supported_data_sources\nGoogle Software as a Service (SaaS) apps\nCampaign Manager\nCloud Storage\nGoogle Ad Manager\nGoogle Ads\nGoogle Merchant Center (beta)\nGoogle Play\nSearch Ads 360 (beta)\nYouTube Channel reports\nYouTube Content Owner reports","poster":"Chelseajcole","timestamp":"1634909520.0","upvote_count":"5"},{"poster":"nguyenmoon","upvote_count":"1","comment_id":"445020","timestamp":"1631684760.0","content":"Answer should be C\nSince \"perform ANSI SQL\" is BigQuery, Cloud Storage is not ANSI SQL, and world-wide marketing teams mean need to have Multiple regions"},{"poster":"sandipk91","timestamp":"1628438400.0","content":"Correct option should be A, as the customer is looking for global service (i.e multiregional GCS bucket) also in bigquery transfer service you can't configure GCS as a final destination hence option C & D is wrong.","upvote_count":"1","comment_id":"421720"},{"content":"A. Use Storage Transfer Service to transfer the offsite backup files to a Cloud Storage Multi-Regional storage bucket as a final destination. - CORRECT (multi-regional, Correct transfer service for coud storage).\n\nB. Use Storage Transfer Service to transfer the offsite backup files to a Cloud Storage Regional bucket as a final destination. - WRONG (regional mention)\nC. Use BigQuery Data Transfer Service to transfer the offsite backup files to a Cloud Storage Multi-Regional storage bucket as a final destination. (WRONG - we cannot transfer to cloud storage using BigQuery transfer service)\nD. Use BigQuery Data Transfer Service to transfer the offsite backup files to a Cloud Storage Regional storage bucket as a final destination - WRONG ( regional)","comments":[{"upvote_count":"3","timestamp":"1625443980.0","content":"The question mentions Google Cloud not GCS. The correct answers is C.","comment_id":"398737","poster":"awssp12345","comments":[{"poster":"gaco","comment_id":"402148","upvote_count":"1","content":"You are correct that the question doesn't mention GCS. But answer C says that final destination is a bucket, that is, GCS. So best answer is A. And from there the customer can load to Big Query.","timestamp":"1625769300.0"},{"upvote_count":"1","poster":"sumanshu","timestamp":"1625777100.0","content":"The BigQuery Data Transfer Service automates data movement into BigQuery on a scheduled, managed basis. (i.e. into BigQuery...Does it mean first you will send to BigQuery and then to MultiRegional Bucket??)","comment_id":"402226"},{"upvote_count":"1","timestamp":"1625777280.0","comment_id":"402231","content":"https://cloud.google.com/bigquery-transfer/docs/introduction","poster":"sumanshu"}]}],"timestamp":"1624922940.0","upvote_count":"3","comment_id":"393317","poster":"sumanshu"},{"timestamp":"1624055040.0","comment_id":"385074","content":"Answer : C\nhttps://cloud.google.com/bigquery-transfer/docs/introduction\nCurrently, the BigQuery Data Transfer Service supports loading data from \nYouTube Channel reports\nYouTube Content Owner reports\nAs ANSI-SQL is needed and global teams involved, it is best to use Big Query with multi-region bucket.","upvote_count":"4","poster":"Jeysolomon"},{"upvote_count":"1","comment_id":"355886","content":"I think the correct answer should be C, since ANSI SQL is required and YouTube Channel reports can be imported to BigQuery through BigQuery Data Transfer: https://cloud.google.com/bigquery-transfer/docs/introduction.","poster":"jasper_430","timestamp":"1620869280.0"},{"content":"Answer is A. \nit is a worldwide team so B and D are ruled out, since we need multi-regional storage\nC is out because big query transfer service is for loading data INTO big query, not to other services.\nhttps://cloud.google.com/bigquery-transfer/docs/introduction\nthat leaves A as the correct option","upvote_count":"1","timestamp":"1617093960.0","poster":"SageDO","comment_id":"324122"},{"poster":"gopinath_k","timestamp":"1615558800.0","upvote_count":"1","content":"I think B may be the right answer because they wanted to design a solutions for transfering the files [based on the options] not direct data from YouTube. Could not justify regional or multi regional as they were supposed to be on the Bigquery which serves for SQL Part.","comment_id":"308904"},{"upvote_count":"1","comment_id":"307834","content":"A:\nWhy not C & D: BigQuery Data Transfer Service: the target is BigQuery dataset not cloud storage.\nhttps://cloud.google.com/bigquery-transfer/docs/youtube-channel-transfer\nwhy not B: because the question said that provide service for all of the world and we need multi-regional.\nhttps://cloud.netapp.com/blog/cloud-storage-transfer-service-for-google-cloud\nwhy A: since you can use federated sources in Bigquery to insert data from GCS in multi-regional mode.","poster":"daghayeghi","timestamp":"1615464360.0"},{"timestamp":"1613271480.0","poster":"karthik89","content":"the suggested answers and the answers in the comments are always out of sync. where is the issue?","comment_id":"289951","upvote_count":"2"},{"comment_id":"284137","poster":"PratikG","timestamp":"1612527060.0","upvote_count":"2","content":"Correct ans is A reason - BigQuery Data Transfer Service automatically loads data into BigQuery on a regular basis. You can also initiate data backfills to recover from any outages or gaps. Currently, you cannot use the BigQuery Data Transfer Service to transfer data out of BigQuery"},{"comment_id":"275661","poster":"[Removed]","timestamp":"1611546000.0","content":"Ans is A.\nAll the 4 target is cloud storage bucket. Bigquery transfer service only do import to bigquery. So c&d eeliminated","upvote_count":"2"},{"content":"A is answer, we can create federated tabled on top of GCS to run queries","poster":"apnu","upvote_count":"2","timestamp":"1610119500.0","comment_id":"262664"},{"content":"best and easiest way to get the data is through BQ Transfer service and store in GCS Multi-Regional and can query data through BQ. \nhttps://cloud.google.com/bigquery/external-data-cloud-storage?hl=en\nI will go with C.","poster":"bobby8521","timestamp":"1609701240.0","comment_id":"258796","upvote_count":"2"},{"timestamp":"1603555680.0","poster":"Pupina","upvote_count":"3","content":"A: world wide marketing team.","comment_id":"205191"},{"content":"A should be the answer.\n\nBigQuery Data Transfer Service doesn't take files, just youtubedata. That rules out C&D.","upvote_count":"3","comment_id":"185478","timestamp":"1600878060.0","poster":"Diqtator"},{"comment_id":"161824","upvote_count":"2","poster":"haroldbenites","timestamp":"1597876860.0","content":"Correct is C. Bigqeury Data Trasnfer Service is used to connect with others services like YpuTube, Google Ads, etc. Is nulti-regional because marketing team is in all the world."},{"poster":"tprashanth","upvote_count":"1","content":"C.\nIn BQ, external table can be created for the data on GCS. This way it can be queries from BQ.","comment_id":"134010","timestamp":"1594651080.0"},{"comment_id":"131884","poster":"Rajuuu","timestamp":"1594440720.0","content":"Answer C:\nReason :- High Availability- Multiregion bucket. BigQuery Transfer service to Google Cloud.","upvote_count":"1"},{"content":"A: world wide marketing team. \nUse a multi-region when you want to serve content to data consumers that are outside of the Google network and distributed across large geographic areas, or when you want the higher availability that comes with being geo-redundant.\nhttps://cloud.google.com/storage/docs/locations\nC,D are wrong. destionatin must be Bigquery","upvote_count":"4","timestamp":"1593326760.0","poster":"norwayping","comment_id":"121679"},{"comment_id":"121597","upvote_count":"2","timestamp":"1593314760.0","content":"C..Big Query Transfer service to transfer from YouTube channel. And multi region for worldwide marketing team","poster":"PRC"},{"content":"Question talks abt ANSI SQL and other types of analysis on up-to-date YouTube channels log data. So has to be btw C & D. No mention of costs, so picking C as more expensive but larger in scope considering youtube as global app. Answer C.","poster":"AJKumar","timestamp":"1592981100.0","comment_id":"118162","upvote_count":"1"},{"poster":"ch3n6","comment_id":"115365","upvote_count":"2","timestamp":"1592726760.0","content":"A; \nWhy not C & D: BigQuery Data Transfer Service: the target is BigQuery dataset not cloud storage. https://cloud.google.com/bigquery-transfer/docs/youtube-channel-transfer why not B: multi-regional is better: https://cloud.netapp.com/blog/cloud-storage-transfer-service-for-google-cloud"},{"upvote_count":"5","comment_id":"91684","content":"Nobody would go with A?","poster":"aadaisme","timestamp":"1589845680.0"},{"upvote_count":"2","timestamp":"1588503480.0","content":"Option B\nC D are ruled out as it is meant for Saas apps. To move from Saas to BQ, not CS. \nbetween A, B - i would go wth B. \n https://cloud.google.com/bigquery/external-data-sources?hl=en#data-locations","comment_id":"83005","poster":"psu"},{"comment_id":"68733","content":"Answer: C\nDescription: ANSI SQl with Bigquery data transfer service is best","timestamp":"1585366500.0","comments":[{"comments":[{"content":"for SQL - Data lake\nhttps://www.linkedin.com/pulse/sql-behind-cloud-data-lakes-alex-skorokhod/","poster":"sumanshu","upvote_count":"1","comment_id":"393314","timestamp":"1624922820.0"}],"poster":"sumanshu","comment_id":"393312","upvote_count":"1","content":"Big Query transfer service to transfer data to cloud storage ?? - Not possible at the moment\n\nhttps://cloud.google.com/bigquery-transfer/docs/introduction\nCurrently, you cannot use the BigQuery Data Transfer Service to transfer data out of BigQuery.","timestamp":"1624922640.0"}],"upvote_count":"3","poster":"[Removed]"},{"content":"correct: C","timestamp":"1584804900.0","upvote_count":"1","poster":"[Removed]","comment_id":"66571"},{"content":"Using BigQuery Transfer Service makes sense but why the final destination is Cloud storage bucket? A & B are also wrong since STS does not support on-premise data.","poster":"rickywck","upvote_count":"6","comment_id":"65054","comments":[{"content":"https://cloud.netapp.com/blog/cloud-storage-transfer-service-for-google-cloud \nIt does support !","poster":"psu","comment_id":"82990","timestamp":"1588502100.0","upvote_count":"3"}],"timestamp":"1584427800.0"},{"comment_id":"63493","content":"Answer should be C","upvote_count":"5","poster":"madhu1171","comments":[{"comments":[{"timestamp":"1625444280.0","content":"Google cloud != GCS","poster":"awssp12345","upvote_count":"2","comment_id":"398739"}],"content":"option 'C' said - Use BigQuery Data Transfer Service to transfer the offsite backup files to a Cloud Storage .\n\nBUT\nYou cannot use the BigQuery Data Transfer Service to transfer data out of BigQuery.","upvote_count":"1","timestamp":"1624922520.0","comment_id":"393311","poster":"sumanshu"}],"timestamp":"1584104700.0"},{"timestamp":"1583842200.0","comment_id":"61650","poster":"cleroy","upvote_count":"6","content":"see : https://cloud.google.com/bigquery-transfer/docs/youtube-channel-transfer"}],"topic":"1","url":"https://www.examtopics.com/discussions/google/view/16072-exam-professional-data-engineer-topic-1-question-69/","unix_timestamp":1583842200,"answer_description":"","exam_id":11,"question_text":"Your infrastructure includes a set of YouTube channels. You have been tasked with creating a process for sending the YouTube channel data to Google Cloud for analysis. You want to design a solution that allows your world-wide marketing teams to perform ANSI SQL and other types of analysis on up-to-date YouTube channels log data. How should you set up the log data transfer into Google Cloud?","choices":{"C":"Use BigQuery Data Transfer Service to transfer the offsite backup files to a Cloud Storage Multi-Regional storage bucket as a final destination.","A":"Use Storage Transfer Service to transfer the offsite backup files to a Cloud Storage Multi-Regional storage bucket as a final destination.","D":"Use BigQuery Data Transfer Service to transfer the offsite backup files to a Cloud Storage Regional storage bucket as a final destination.","B":"Use Storage Transfer Service to transfer the offsite backup files to a Cloud Storage Regional bucket as a final destination."}},{"id":"S1NJDIZ5YPjsAJiS1MJt","question_id":287,"isMC":true,"unix_timestamp":1584258180,"topic":"1","question_text":"You are creating a model to predict housing prices. Due to budget constraints, you must run it on a single resource-constrained virtual machine. Which learning algorithm should you use?","question_images":[],"choices":{"B":"Logistic classification","D":"Feedforward neural network","A":"Linear regression","C":"Recurrent neural network"},"answer":"A","answer_description":"","timestamp":"2020-03-15 08:43:00","answers_community":["A (100%)"],"answer_ET":"A","discussion":[{"comments":[{"timestamp":"1609814220.0","comments":[{"upvote_count":"25","comment_id":"282827","timestamp":"1612369860.0","content":"Neural Networks(Feed Forward or Recurrent) require resource intensive machines(i.e GPU's) whereas Linear regression can be done on ordinary CPU's","poster":"muzammilnxs"}],"upvote_count":"7","poster":"Anirkent","content":"Liner Regression is correct but this is one aspect of the question, how does it relates to resource constrained machines? or that could be just a distraction?","comment_id":"259880"}],"comment_id":"207480","timestamp":"1603855680.0","upvote_count":"58","content":"Correct answer is A. A tip here to decide when a liner regression should be used or logistics regression needs to be used. If you are forecasting that is the values in the column that you are predicting is numeric, it is always liner regression. If you are classifying, that is buy or no buy, yes or no, you will be using logistics regression.","poster":"Radhika7983"},{"content":"Correct A","timestamp":"1584258180.0","comment_id":"64162","poster":"[Removed]","upvote_count":"8"},{"content":"Selected Answer: A\nlinear regression to predict numeric with least cost. for classifying with options use logistics.","upvote_count":"1","poster":"willyunger","comment_id":"1399887","timestamp":"1742253240.0"},{"poster":"SamuelTsch","timestamp":"1729526760.0","content":"Selected Answer: A\nthe keyword here is running it on a single resource-constrained virtual machine. linear regression is a simple and efficient algorithm that is well-suited for predicting continuous values.","comment_id":"1301135","upvote_count":"2"},{"upvote_count":"3","comment_id":"1050469","content":"Selected Answer: A\nLinear regression is a simple and resource-efficient algorithm for predicting continuous values like housing prices. It's computationally lightweight and well-suited for single machines with limited resources. It doesn't require the extensive computational power or specialized hardware that more complex algorithms like neural networks (options C and D) might need.\n\nOption B (Logistic classification) is used for binary classification tasks, not for predicting continuous values like housing prices, so it's not the right choice in this context.","poster":"rtcpost","timestamp":"1727154480.0"},{"timestamp":"1708390200.0","upvote_count":"1","comment_id":"1154388","poster":"AshishDhamu","content":"Selected Answer: A\nLinear regression is used for continous distribution."},{"poster":"Fazan456","upvote_count":"1","comment_id":"1116623","timestamp":"1704719280.0","content":"Selected Answer: A\nHere, due to budget constraints, we're utilizing a single resource-constrained virtual machine, operating in a minimal resource environment. Linear regression emerges as the appropriate algorithm. It's a lightweight predictive model that suits our resource limitations"},{"comment_id":"1065068","content":"Selected Answer: A\nLinear regression will be used since the prediction requires forecasting prices involving numeric values and is computationally less resource intensive","upvote_count":"1","timestamp":"1699380900.0","poster":"RT_G"},{"content":"Selected Answer: A\nCorrect answer is A","upvote_count":"1","timestamp":"1698981600.0","poster":"rocky48","comment_id":"1061051"},{"timestamp":"1684838880.0","content":"Selected Answer: A\nCorrect Answer is A. Since linear regression is used to predict a numeric value. While logistic regression is used to classify among the binary scenario.\nFurther option C and D are advance ML options and not cost and resource effective for the current situation.","comment_id":"904824","upvote_count":"1","poster":"AmmarFasih"},{"upvote_count":"2","poster":"Zosby","timestamp":"1677224400.0","content":"predict housing prices = linear regresssion","comment_id":"820218"},{"upvote_count":"1","poster":"JJJJim","timestamp":"1676508480.0","comment_id":"810092","content":"Selected Answer: A\nmust be A.\nThough C can do it, linear regression is the better practice."},{"content":"Selected Answer: A\nMust be A","comment_id":"741509","timestamp":"1670745480.0","poster":"lukas_xls","upvote_count":"1"},{"content":"A for sure. B is for classification. Neural nets can accomplish the task but they take WAY too many resources","upvote_count":"2","poster":"rowan_","comment_id":"647411","timestamp":"1660616340.0"},{"comments":[{"content":"Linear regression is a simple and computationally efficient algorithm that can be used to predict a continuous target variable based on one or more input variables. It is particularly well-suited for resource-constrained environments, as it requires minimal computational resources and can be run on a single virtual machine.\nLinear regression is a good fit for this problem as it is a supervised learning algorithm that can be used for regression problems, and it's not computationally expensive.\n\nOption B is not recommended as Logistic classification is a supervised learning algorithm that is used for classification problems, not regression problems.\n\nOption C and D are not recommended as Recurrent Neural Network (RNN) and Feedforward Neural Network (FNN) are computationally expensive and may require significant computational resources and memory to run on a single virtual machine.","timestamp":"1674431820.0","comment_id":"784804","upvote_count":"2","poster":"samdhimal"}],"upvote_count":"4","content":"correct answer -> Linear Regression\n\nLinear regression is a statistical method that allows to summarize and study relationships between two continuous (quantitative) variables: One variable, denoted X, is regarded as the independent variable. The other variable denoted y is regarded as the dependent variable. Linear regression uses one independent variable X to explain or predict the outcome of the dependent variable y.\n\nWhenever you are told to predict some future value of a process which is currently running, you can go with a regression algorithm.","poster":"samdhimal","timestamp":"1642863600.0","comment_id":"529925"},{"poster":"MaxNRG","timestamp":"1636309740.0","content":"A as Supervised learning using Regression can help build a model to predict house prices.\nOption B is wrong as Classification would not help to solve the problem.\nOptions C & D are wrong as they would need more resources.","upvote_count":"3","comment_id":"474006"},{"upvote_count":"1","content":"Ans: A","timestamp":"1634215260.0","comment_id":"462019","poster":"anji007"},{"timestamp":"1630669620.0","comment_id":"438490","upvote_count":"2","content":"Ok the right answer is A, but the question is why? Then:\n- B not because we are make forecasting and not classifying\n- C and D not because this solution need more nodes, then more VM.\nRight?","poster":"StefanoG"},{"comment_id":"400785","content":"Vote for A","timestamp":"1625654820.0","poster":"sumanshu","upvote_count":"1"},{"poster":"lbhhoya82","upvote_count":"1","comment_id":"319793","content":"Correct : A","timestamp":"1616649360.0"},{"upvote_count":"1","comment_id":"305456","poster":"daghayeghi","timestamp":"1615169520.0","content":"A:\nWhen a model is used to predict a continuous value, it is known as a regression model."},{"poster":"davidbilla","upvote_count":"2","timestamp":"1614852000.0","comment_id":"303269","content":"A. Linear Regression"},{"upvote_count":"2","timestamp":"1614520260.0","comment_id":"300811","content":"Easy One. Linear Regression . Option A","poster":"sid091"},{"poster":"senthil836","timestamp":"1612721820.0","content":"Correct Answer is A","comment_id":"285702","upvote_count":"2"},{"timestamp":"1612626180.0","upvote_count":"2","content":"Correct A","comment_id":"284922","poster":"naga"},{"upvote_count":"4","poster":"saurabh1805","timestamp":"1597679160.0","content":"Correct Answer is A, refer below link for more information.","comment_id":"160146"},{"timestamp":"1597654920.0","upvote_count":"3","content":"Option A: Because Linear Regression is applicable for the continuous value output as the Price for the House will be a continuous values.","comment_id":"159792","poster":"PRABHUKKARTHI"}],"exam_id":11,"url":"https://www.examtopics.com/discussions/google/view/16640-exam-professional-data-engineer-topic-1-question-7/","answer_images":[]},{"id":"GpEASVP8Qua21xxGVljX","answer_ET":"B","answer_images":[],"question_id":288,"question_text":"You are designing storage for very large text files for a data pipeline on Google Cloud. You want to support ANSI SQL queries. You also want to support compression and parallel load from the input locations using Google recommended practices. What should you do?","timestamp":"2020-03-15 06:11:00","answer":"B","discussion":[{"content":"B.\nThe question is focused on designing storage for very large files, with support for compression, ANSI SQL queries, and parallel loading from the input locations. This can be met using GCS for storage and Bigquery permanent tables with external data source in GCS.","comment_id":"73310","upvote_count":"59","poster":"Ganshank","timestamp":"1586606880.0","comments":[{"comment_id":"146057","comments":[{"comments":[{"upvote_count":"4","poster":"atnafu2020","content":"Since its best practice, i go by with B not A","timestamp":"1597979820.0","comment_id":"162644"},{"content":"They want to store the files if you try with bq I think you will need to strike the word compression.","poster":"gopinath_k","timestamp":"1615558980.0","upvote_count":"2","comment_id":"308909"}],"upvote_count":"11","content":"A seems correct for me","timestamp":"1595964900.0","comment_id":"146058","poster":"atnafu2020"},{"upvote_count":"5","comment_id":"748114","timestamp":"1671287040.0","content":"The question focuses on \"designing storage\", rather than designing a data warehouse.","poster":"jkhong"}],"poster":"atnafu2020","upvote_count":"10","timestamp":"1595964900.0","content":"why GCS as external since Bigquery can be used as storage as well?"}]},{"comment_id":"66574","poster":"[Removed]","comments":[{"timestamp":"1655273040.0","content":"Not A : Importing data into BigQuery may take more time compared to creating external tables on data. Additional storage costs by BigQuery is another issue which can be more expensive than Google Storage.","poster":"tavva_prudhvi","upvote_count":"7","comment_id":"616556"}],"upvote_count":"15","timestamp":"1584805080.0","content":"Should be A"},{"content":"Selected Answer: A\nAvro is a Google-recommended format for BigQuery because it supports schema evolution, efficient compression, and parallel processing. Using Cloud Dataflow ensures scalable transformation, and storing the data in BigQuery allows for optimized ANSI SQL queries.","upvote_count":"1","poster":"oussama7","timestamp":"1742423100.0","comment_id":"1400769"},{"timestamp":"1742050860.0","poster":"Parandhaman_Margan","upvote_count":"1","comment_id":"1398899","content":"Selected Answer: A\nStoring large text files with SQL support**\n\nAvro in BigQuery (A) supports compression and efficient queries."},{"content":"Selected Answer: B\nFor me it would make more sense to store the data directly in BQ, but A does not make sense, because why compress to Avro if you dont store the avro files and directly saves the data to BQ?\nYou are not using the compression","timestamp":"1741078860.0","upvote_count":"1","comment_id":"1364820","poster":"dcruzado"},{"poster":"imarri876","comment_id":"1357082","content":"Selected Answer: A\nBigQuery now has physical storage, which makes storage cost fairly cheap on BigQuery with compression. I would go with A.","upvote_count":"1","timestamp":"1739661240.0"},{"comment_id":"1325210","upvote_count":"1","content":"Selected Answer: B\nGros fichier + sql = GCS + Bigquery","timestamp":"1733947380.0","poster":"deineiveu"},{"timestamp":"1724836080.0","poster":"Nittin","upvote_count":"1","content":"Selected Answer: B\ncopy to gcs and use external tble in bq","comment_id":"1273931"},{"upvote_count":"2","comment_id":"1255232","comments":[{"content":"While option B can work, it introduces additional complexity by linking Cloud Storage with BigQuery. Directly storing data in BigQuery is more efficient for querying purposes.\n\nThere are no requirements about cost, So simple is better","poster":"carmltekai","comment_id":"1255233","upvote_count":"1","timestamp":"1721938500.0"}],"content":"Selected Answer: A\nShould be A.\n\nCheck this link for the advantage of load Avro data to BigQuery https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro#advantages_of_avro\n\n\"\"\"The Avro binary format:\n* Is faster to load. The data can be read in parallel, even if the data blocks are compressed.\n* Doesn't require typing or serialization.\n* Is easier to parse because there are no encoding issues found in other formats such as ASCII.\nWhen you load Avro files into BigQuery, the table schema is automatically retrieved from the self-describing source data.\"\"\"","timestamp":"1721938380.0","poster":"carmltekai"},{"upvote_count":"2","comment_id":"1185264","timestamp":"1711694520.0","poster":"SK1594","content":"B makes sense"},{"content":"Selected Answer: B\n1. Store Avro files in GCS\n2. Query them in BigQuery (federated tables)","comment_id":"1098715","upvote_count":"3","timestamp":"1702798920.0","poster":"MaxNRG"},{"upvote_count":"6","poster":"forepick","comment_id":"911135","timestamp":"1685531820.0","content":"Selected Answer: B\nAnswer is B.\nThe requirements are:\n- storage for compressed text files\n- parallel loads to SQL tool\n\nAVRO is a compressed format for text files, which makes it possible to load chunks of a very large file in parallel to BigQuery.\n\ngzip files are seamless in GCS though, but cannot load in parallel to BQ."},{"comments":[{"upvote_count":"1","comment_id":"784747","timestamp":"1674426900.0","poster":"samdhimal","content":"Option B is similar to option A but it's using a permanent linked table between Cloud Storage and BigQuery, this approach is not recommended as it's not efficient and could lead to data duplication, and it doesn't take advantage of the parallel processing capabilities of Cloud Dataflow.\n\nOption C and D are incorrect because they don't take advantage of the parallel processing capabilities of Cloud Dataflow, and they don't use Avro format for compression which is more efficient and recommended by Google. Storing the data in Cloud Bigtable also doesn't support ANSI SQL queries which is a requirement for this use case."}],"poster":"samdhimal","comment_id":"784746","timestamp":"1674426900.0","upvote_count":"3","content":"Correct Answer:\nA. Transform text files to compressed Avro using Cloud Dataflow. Use BigQuery for storage and query.\n\nThis option offers several advantages:\n\n- Transforming the text files to compressed Avro using Cloud Dataflow allows for parallel processing of the input data, improving the efficiency of the pipeline.\n\n- Compressing the data in Avro format further reduces the storage space required and improves data transfer performance.\n\n- Storing the data in BigQuery supports ANSI SQL queries and allows for easy querying of the data.\n\n- BigQuery is a fully-managed data warehousing solution, it's scalable and can handle large datasets and concurrent queries, so it's suitable for large text files."},{"upvote_count":"3","content":"Selected Answer: B\nDesigning storage solution, not data warehousing -> So Cloud Storage.\n\nSupport compression -> just use Avro\nParallel load -> refers to upload from input locations, NOT download.\n\nLoad in parallel using -m flag for gsutil cp\n\nhttps://cloud.google.com/storage/docs/uploads-downloads#console","timestamp":"1671287520.0","poster":"jkhong","comment_id":"748123"},{"timestamp":"1670759220.0","poster":"odacir","upvote_count":"3","comment_id":"741639","content":"Selected Answer: B\nC and D are discarted.\nA and B are possible.\nA is the best for query, but … the sentence says: ou also want to support compression and parallel load from the input locations using Google recommended practices.\nBigQuery only support parallel load from storage, storage support parallel load from CLI. So the only option is B."},{"comment_id":"736863","upvote_count":"1","timestamp":"1670334240.0","poster":"zellck","content":"Selected Answer: B\nB is the answer."},{"timestamp":"1670133360.0","upvote_count":"1","comment_id":"734866","poster":"nkit","content":"Selected Answer: B\n\"Very large files\" and \"long term storage\" are two key phrases- both of which indicate to pick cloud storage as option. Hence B is correct."},{"timestamp":"1669979040.0","poster":"NicolasN","comments":[{"comment_id":"869861","timestamp":"1681432500.0","poster":"izekc","content":"No, it is not\nhttps://github.com/GoogleCloudPlatform/bigquery-ingest-avro-dataflow-sample","upvote_count":"1"},{"poster":"ffggrre","timestamp":"1698160320.0","comment_id":"1052992","content":"Compressed AVRO files are supported by BQ","upvote_count":"1"}],"comment_id":"733663","upvote_count":"3","content":"Selected Answer: B\nAll the comments argue about [A] and [B] as a storage destination. But there is a limitation on loading compressed Avro files into BigQuery that cuts the Gordian knot:\n❗ \"... Compressed Avro files are not supported, but compressed data blocks are ...\"\nFrom: https://cloud.google.com/bigquery/docs/batch-loading-data#loading_compressed_and_uncompressed_data"},{"poster":"cloudmon","content":"Selected Answer: A\nhttps://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro\n\nAdvantages of Avro:\nAvro is the preferred format for loading data into BigQuery. Loading Avro files has the following advantages over CSV and JSON (newline delimited):\n\n The Avro binary format:\n Is faster to load. The data can be read in parallel, even if the data blocks are compressed.\n Doesn't require typing or serialization.\n Is easier to parse because there are no encoding issues found in other formats such as ASCII.\n When you load Avro files into BigQuery, the table schema is automatically retrieved from the self-describing source data.","timestamp":"1667756580.0","upvote_count":"2","comment_id":"712517"},{"poster":"Lui1979","timestamp":"1652041500.0","comments":[{"timestamp":"1667756940.0","upvote_count":"2","content":"Your comment supports A more than B","comment_id":"712527","poster":"cloudmon"}],"upvote_count":"5","comment_id":"598712","content":"Selected Answer: B\nhttps://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro\nThe Avro binary format:\nIs faster to load. The data can be read in parallel, even if the data blocks are compressed"},{"comment_id":"586322","poster":"Didine_22","upvote_count":"1","content":"Selected Answer: B\nB\nBecause they are talking about the parallel loading from input locations.","timestamp":"1650022560.0"},{"timestamp":"1649187120.0","upvote_count":"2","comment_id":"581435","comments":[{"poster":"devric","comment_id":"581436","timestamp":"1649187180.0","content":"Sory I mean A not B :-)","upvote_count":"1"}],"poster":"devric","content":"Selected Answer: B\nB. The objetive is to follow the best practices."},{"comments":[{"upvote_count":"1","comment_id":"712526","poster":"cloudmon","content":"So does A","timestamp":"1667756820.0"}],"content":"Selected Answer: B\nB fits all the requirements","poster":"danieltttt","upvote_count":"1","comment_id":"572305","timestamp":"1647872400.0"},{"upvote_count":"2","comments":[{"content":"GCP and BigQuery storage costs are the same ~20$ per 1TB per month","comment_id":"730263","poster":"sfsdeniso","upvote_count":"1","timestamp":"1669718940.0"}],"comment_id":"562700","poster":"Arkon88","content":"Selected Answer: B\nB. Transform text files to compressed Avro using Cloud Dataflow. Use Cloud Storage and BigQuery permanent linked tables for query.\n1) Avro covers parallelism and compression\n2) A and B are correct, but B is the best answer\nThe advantages of creating external tables are that they are fast to create so you skip the part of importing data and no additional monthly billing storage costs are accrued to your account since you only get charged for the data that is stored in the data lake, which is comparatively cheaper than storing it in BigQuery\nhttps://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro","timestamp":"1646665320.0"},{"comment_id":"559072","timestamp":"1646178480.0","content":"B is better than A as there is no need for Analytics","upvote_count":"1","poster":"rbeeraka","comments":[{"poster":"cloudmon","content":"They want ANSI SQL queries. In any case, both A and B support analytics.","comment_id":"712531","upvote_count":"1","timestamp":"1667757120.0"}]},{"timestamp":"1644668280.0","upvote_count":"3","content":"A and B are correct, but B is the best answer\n\nThe advantages of creating external tables are that they are fast to create so you skip the part of importing data and no additional monthly billing storage costs are accrued to your account since you only get charged for the data that is stored in the data lake, which is comparatively cheaper than storing it in BigQuery\n\nA : Importing data into BigQuery may take more time compared to creating external tables on data. Additional storage costs by BigQuery is another issue which can be more expensive than Google Storage","comment_id":"545818","poster":"VishalBule","comments":[{"content":"Yeah, I agree with you.","timestamp":"1647751920.0","comment_id":"571395","upvote_count":"2","poster":"Mr_Stark"},{"poster":"cloudmon","comment_id":"712536","timestamp":"1667757600.0","content":"Who cares if it takes more time? There are no load time requirements in the question. Putting it in GCS just for that reason is unnecessary in this case. Store it in BQ, query it in BQ. (Answer A)","upvote_count":"1"}]},{"upvote_count":"2","timestamp":"1644502740.0","content":"Selected Answer: A\nA, as per medeis_jar","comment_id":"544611","poster":"bury"},{"poster":"Richag23","content":"What is the best answer?","upvote_count":"1","comment_id":"539191","timestamp":"1643835420.0"},{"poster":"sraakesh95","content":"Selected Answer: B\nNaturally Avro based on the scenario. Also large text files, thereby, eliminating A","timestamp":"1642274160.0","upvote_count":"1","comment_id":"524382"},{"upvote_count":"3","comment_id":"523549","poster":"ssepiro","content":"B is correct. even while using Dataflow for Avro. It must goes through cloud storage. Also. parallel loading only works on top of cloud storage. Please guess space between Dataflow and BQ.","timestamp":"1642167720.0"},{"comment_id":"518456","poster":"medeis_jar","upvote_count":"2","timestamp":"1641492720.0","content":"Selected Answer: A\nhttps://cloud.google.com/bigquery/docs/loading-data"},{"upvote_count":"2","comments":[],"timestamp":"1640107080.0","content":"Selected Answer: A\nIf you use gzip compression BigQuery cannot read the data in parallel….C and D are out since you want to paralelize.\nBigQuery supports standard ANSI SQL 2011, but also BigQuery is also capable of making SQL queries against external data sources, such as log files in Cloud Storage, transactional records in Cloud Bigtable, and many other kinds of data outside BigQuery.\nI would say A since we don’t have a data lake strategy and there is no advantage in having data in cloud storage.\nhttps://cloud.google.com/bigquery/docs/loading-data","poster":"MaxNRG","comment_id":"506305"},{"comment_id":"504415","content":"A is the correct answer\n\nHere the latest Google Update:\nhttps://cloud.google.com/architecture/streaming-avro-records-into-bigquery-using-dataflow","timestamp":"1639854660.0","upvote_count":"3","comments":[{"comment_id":"582203","timestamp":"1649312040.0","comments":[{"timestamp":"1667756760.0","upvote_count":"1","content":"Yes but A is better","poster":"cloudmon","comment_id":"712524"}],"poster":"tavva_prudhvi","upvote_count":"1","content":"According to your link, B satisfies the condition?"}],"poster":"[Removed]"},{"content":"Why there are so many wrong answers? Examtopics.com are you enjoying paid subscription by giving random answers from people?\nAns: B","comments":[{"timestamp":"1667757720.0","comment_id":"712537","upvote_count":"1","content":"What is your reasoning for selecting B?","poster":"cloudmon"}],"poster":"JG123","upvote_count":"2","comment_id":"487547","timestamp":"1637950560.0"},{"content":"If you use gzip compression, BigQuery cannot read the data in parallel. Loading compressed CSV data into BigQuery is slower than loading uncompressed data.","upvote_count":"1","poster":"Abhi16820","timestamp":"1636597620.0","comment_id":"475931"},{"comment_id":"472089","timestamp":"1635946500.0","comments":[{"poster":"Abhi16820","comment_id":"475930","upvote_count":"3","timestamp":"1636597320.0","content":"storing in bigquery costs same as storing in cloud storage as nearline, so storage costs in bigquery is not at all the reason to consider."}],"content":"A and B are correct, but B is the best answer \nThe advantages of creating external tables are that they are fast to create so you skip the part of importing data and no additional monthly billing storage costs are accrued to your account since you only get charged for the data that is stored in the data lake, which is comparatively cheaper than storing it in BigQuery\n\nA : Importing data into BigQuery may take more time compared to creating external tables on data. Additional storage costs by BigQuery is another issue which can be more expensive than Google Storage.","poster":"Track22","upvote_count":"3"},{"content":"B makes sense as \nStep 1: compress the file and store in the GCS using Data flow and remove the uncompressed file which helps in saving on storage space. \nStep 2: Use BQ external table on the GCS compressed file.","poster":"u_t_s","comment_id":"458728","upvote_count":"2","timestamp":"1633608600.0"},{"timestamp":"1632937140.0","poster":"Chelseajcole","upvote_count":"6","comments":[{"content":"Yes you can store Avro in BQ, and that is actually recommended by Google: https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro#advantages_of_avro","poster":"cloudmon","timestamp":"1667757780.0","upvote_count":"1","comment_id":"712542"}],"content":"Should be A since there is no advantage using cloud storage vs Bigquery from a costing perspective. Other than that, only concern is are we able to load Avro directly to BigQuery, I think the answer is yes.\n\nStreaming Avro records into BigQuery using Dataflow\nhttps://cloud.google.com/architecture/streaming-avro-records-into-bigquery-using-dataflow","comment_id":"454362"},{"content":"A seem correct to me. Google recommend avro format","upvote_count":"2","poster":"nguyenmoon","timestamp":"1631685240.0","comment_id":"445024"},{"comments":[{"upvote_count":"1","comment_id":"538594","comments":[{"comment_id":"712544","timestamp":"1667757900.0","content":"Incorrect, see here for correct answer (Answer A): https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro#advantages_of_avro","upvote_count":"1","poster":"cloudmon"}],"poster":"BigDataBB","content":"I think that is the best answer: \"Compressed Avro files are not supported, but compressed data blocks are\" The question talks about compressed avro files, so A is not applicalble!!","timestamp":"1643798760.0"},{"timestamp":"1667757840.0","comment_id":"712543","content":"Incorrect, see here for correct answer (Answer A): https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro#advantages_of_avro","poster":"cloudmon","upvote_count":"1"}],"comment_id":"428027","content":"Answer is B not A :\nCompressed Avro files are not supported, but compressed data blocks are.\nhttps://cloud.google.com/bigquery/docs/batch-loading-data","timestamp":"1629455040.0","upvote_count":"3","poster":"EricM22"},{"comment_id":"398743","comments":[{"content":"B - question calls for large files. Google recommended practices is to store large files in GCS. Also permanent BQ tablets do not materialize the data. \n\"To query an external data source using a permanent table, you create a table in a BigQuery dataset that is linked to your external data source. The data is not stored in the BigQuery table.\"\n https://cloud.google.com/bigquery/external-data-cloud-storage#:~:text=To%20query%20an%20external%20data%20source%20using%20a%20permanent%20table%2C%20you%20create%20a%20table%20in%20a%20BigQuery%20dataset%20that%20is%20linked%20to%20your%20external%20data%20source.%20The%20data%20is%20not%20stored%20in%20the%20BigQuery%20table.","timestamp":"1627031220.0","comment_id":"412407","upvote_count":"2","poster":"omakin"}],"poster":"awssp12345","timestamp":"1625445060.0","content":"B is fishy because it states use GCS + BQ permanent tables which is repetition of same data right?","upvote_count":"1"},{"poster":"sumanshu","timestamp":"1624923120.0","comment_id":"393322","content":"Vote for A","comments":[{"comments":[{"timestamp":"1624958640.0","content":"I guess, option 'B' - as per above Link - Both Cloud Storage and Big Query used","comment_id":"393650","comments":[{"timestamp":"1627895580.0","poster":"GCP_Guru","upvote_count":"1","comment_id":"418621","content":"A or B then?"}],"upvote_count":"2","poster":"sumanshu"}],"timestamp":"1624923240.0","upvote_count":"1","poster":"sumanshu","comment_id":"393324","content":"https://cloud.google.com/architecture/streaming-avro-records-into-bigquery-using-dataflow"}],"upvote_count":"1"},{"poster":"daghayeghi","timestamp":"1613502600.0","content":"A:\nGCP suggest AVRO for file compression :\nhttps://towardsdatascience.com/load-files-faster-into-bigquery-94355c4c086a","comment_id":"292036","upvote_count":"5"},{"content":"D is the correct answer, as google recomment gzip for compression. so A and B are rules out. As biguqery is required only to query data , so I think it si better to store data in GCS, so thta you can delete or reload the file in bigquery whenever you want and you have the past data already there in GCS","comment_id":"263306","timestamp":"1610204580.0","poster":"apnu","upvote_count":"4"},{"content":"the correct answer is B. You cannot store avro files directly in BQ. You need to store in a cloud storage, so you can query the file from BQ.","comment_id":"230974","upvote_count":"4","timestamp":"1606728180.0","poster":"ceak"},{"content":"Compressed blocks are supported so A is also valid.","comments":[{"poster":"BigDataBB","upvote_count":"1","comment_id":"538595","content":"\"Compressed Avro files are not supported, but compressed data blocks are\" The question talks about compressed avro files","timestamp":"1643798820.0","comments":[{"upvote_count":"1","poster":"BigDataBB","timestamp":"1643798820.0","content":"https://cloud.google.com/bigquery/docs/batch-loading-data#loading_compressed_and_uncompressed_data","comment_id":"538596"}]}],"comment_id":"207202","poster":"Surjit24","upvote_count":"4","timestamp":"1603819560.0"},{"upvote_count":"2","poster":"Pupina","timestamp":"1603555920.0","content":"Should be A Avro is compressed format and dataflow for parallel pipeline and bigquery for storage","comment_id":"205197"},{"upvote_count":"1","poster":"Darlee","comments":[{"poster":"tikna","content":"please refer documentation avro is allowed","timestamp":"1610303400.0","comment_id":"264195","upvote_count":"1"}],"comment_id":"194288","content":"I will go for B, \nwhy not A: yes Avro is the preferred format for parallel loading to bq, but bigquery cannot be used to store Avro files","timestamp":"1601986080.0"},{"comment_id":"186243","comments":[{"comment_id":"188844","timestamp":"1601274060.0","upvote_count":"4","content":"Avro is the preferred format for loading data into BigQuery. Loading Avro files has the following advantages over CSV and JSON","poster":"mk951995"}],"content":"Should be B. You cannot store the avro-file itself in BQ","poster":"abhi0706","timestamp":"1600960260.0","upvote_count":"1"},{"content":"Should be B. You cannot store the avro-file itself in BQ. Hence you run SQLqueries on the file in CS from BQ","poster":"Diqtator","comment_id":"180834","upvote_count":"1","timestamp":"1600341360.0","comments":[{"upvote_count":"2","timestamp":"1604902560.0","content":"You can load avro files in bigquery. see you the link\nhttps://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro\nhttps://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro","comment_id":"215709","poster":"JuanCar"}]},{"content":"B\nUnless you are directly loading data from your local machine, before loading the data into BigQuery you have to upload data to GCS. To move data to GCS you have multiple options:\n\nGsutil is a command line tool which can be used to upload data to GCS from different servers.\nIf your data is present in any online data sources like AWS S3 you can use Storage Transfer Service from Google cloud. This service has options to schedule transfer jobs.","poster":"atnafu2020","timestamp":"1597979640.0","upvote_count":"1","comment_id":"162643"},{"content":"Answer is A..Cloud Storage is not needed .","poster":"Rajuuu","comment_id":"136900","timestamp":"1594960680.0","upvote_count":"2"},{"upvote_count":"3","content":"Bigquery supports both compression and parallel loads. Answer A.","poster":"AJKumar","comment_id":"132036","timestamp":"1594466220.0"},{"timestamp":"1594059360.0","content":"Best option is \"A\". Cloud storage as an intermediate doesn't add any value\nhttps://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro","comment_id":"128128","upvote_count":"2","poster":"dg63"},{"poster":"cleroy","comment_id":"123535","content":"Should be A, not B because : https://cloud.google.com/bigquery/docs/best-practices-storage","timestamp":"1593524160.0","upvote_count":"4"},{"poster":"norwayping","timestamp":"1593327240.0","comment_id":"121686","upvote_count":"3","content":"A: The Avro binary format is the preferred format for loading both compressed and uncompressed data. Avro data is faster to load because the data can be read in parallel, even when the data blocks are compressed.","comments":[{"poster":"haroldbenites","upvote_count":"5","content":"Compressed Avro files are not supported, but compressed data blocks are. BigQuery supports the DEFLATE and Snappy codecs for compressed data blocks in Avro files.\nhttps://cloud.google.com/bigquery/docs/loading-data","comment_id":"161832","timestamp":"1597877520.0"}]},{"upvote_count":"3","timestamp":"1593264780.0","comment_id":"121233","content":"Should be A","poster":"dass"},{"poster":"dambilwa","upvote_count":"3","comment_id":"119063","timestamp":"1593055980.0","content":"Option [A] is correct - https://cloud.google.com/bigquery/docs/loading-data\nAvro supports parallel reads. \nOption [B] - There's no need to use Cloud Storage & link Avro in BigQuery\n\nOption C & D are ruled out"},{"timestamp":"1590087180.0","upvote_count":"4","poster":"arnabbis4u","content":"Answer A","comment_id":"93565"},{"comment_id":"68734","poster":"[Removed]","content":"Answer: A\nDescription: Avro is compressed format and dataflow for parallel pipeline and bigquery for storage","timestamp":"1585366680.0","upvote_count":"5"},{"upvote_count":"4","comment_id":"65057","content":"I will pick A. Option C has a limitation - maximum file size is 4GB.","poster":"rickywck","timestamp":"1584428460.0"},{"content":"ANSI SQL.. how come BigTable?","comment_id":"64141","poster":"Rajokkiyam","upvote_count":"4","timestamp":"1584249060.0"}],"answers_community":["B (73%)","A (27%)"],"topic":"1","unix_timestamp":1584249060,"choices":{"D":"Compress text files to gzip using the Grid Computing Tools. Use Cloud Storage, and then import into Cloud Bigtable for query.","B":"Transform text files to compressed Avro using Cloud Dataflow. Use Cloud Storage and BigQuery permanent linked tables for query.","C":"Compress text files to gzip using the Grid Computing Tools. Use BigQuery for storage and query.","A":"Transform text files to compressed Avro using Cloud Dataflow. Use BigQuery for storage and query."},"answer_description":"","exam_id":11,"url":"https://www.examtopics.com/discussions/google/view/16631-exam-professional-data-engineer-topic-1-question-70/","question_images":[],"isMC":true},{"id":"MMk2GFYxuDKtHKpmXyHE","answer":"A","timestamp":"2020-03-21 16:45:00","isMC":true,"question_images":[],"answers_community":["A (100%)"],"answer_images":[],"answer_description":"","unix_timestamp":1584805500,"topic":"1","discussion":[{"comments":[{"timestamp":"1704402720.0","upvote_count":"1","poster":"AzureDP900","content":"https://cloud.google.com/natural-language/docs/analyzing-entities\nhttps://cloud.google.com/natural-language/docs/analyzing-sentiment","comment_id":"766098"}],"poster":"VishalB","upvote_count":"36","timestamp":"1626622620.0","content":"Correct Answer : A\n\nEntity analysis -> Identify entities within documents receipts, invoices, and contracts and label them by types such as date, person, contact information, organization, location, events, products, and media.\n\nSentiment analysis -> Understand the overall opinion, feeling, or attitude sentiment expressed in a block of text.\n-- Avoid Custom models","comment_id":"137968"},{"timestamp":"1616341500.0","upvote_count":"12","content":"should be A","comment_id":"66578","poster":"[Removed]"},{"comment_id":"1366581","content":"Selected Answer: A\nCall the Cloud Natural Language API from your application. Process the generated Entities Analysis as labels.\n\nThe Cloud Natural Language API is a pre-trained machine learning model that can be used for natural language processing tasks such as entity recognition, sentiment analysis, and syntax analysis. The API can be called from your application using a simple API call, and it can generate entities analysis that can be used as labels for the user's blog posts. This would be the quickest and easiest option for your team since it would not require any machine learning expertise or additional developer resources to build and train a model. Additionally, it will give you accurate and up-to-date results as the API is constantly updated by Google.","upvote_count":"1","timestamp":"1741441560.0","poster":"Ratikl"},{"upvote_count":"3","content":"For the first time, the answer in exam topics matches community vote :-).","comment_id":"1021787","timestamp":"1727719380.0","poster":"Az900Exam2021"},{"upvote_count":"2","poster":"AmmarFasih","timestamp":"1716616200.0","comment_id":"906417","content":"Selected Answer: A\nOf course the answer is A. Since the problem already states that you don't have time, resources or expertise. So the best solution in the case is to utilize the available API. Also since we need to extract the labels and not the sentiment of the text, we'll go for option A and not B"},{"comment_id":"785802","poster":"samdhimal","content":"A. Call the Cloud Natural Language API from your application. Process the generated Entities Analysis as labels.\n\nThe Cloud Natural Language API is a pre-trained machine learning model that can be used for natural language processing tasks such as entity recognition, sentiment analysis, and syntax analysis. The API can be called from your application using a simple API call, and it can generate entities analysis that can be used as labels for the user's blog posts. This would be the quickest and easiest option for your team since it would not require any machine learning expertise or additional developer resources to build and train a model. Additionally, it will give you accurate and up-to-date results as the API is constantly updated by Google.","upvote_count":"1","timestamp":"1706041440.0"},{"comment_id":"766096","content":"Answer is A \nCall the Cloud Natural Language API from your application. Process the generated Entity Analysis as labels.\n\nEntity analysis -> Identify entities within documents receipts, invoices, and contracts and label them by types such as date, person, contact information, organization, location, events, products, and media.\n\nSentiment analysis -> Understand the overall opinion, feeling, or attitude sentiment expressed in a block of text.","poster":"AzureDP900","timestamp":"1704402540.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1701870180.0","content":"Selected Answer: A\nA is the answer.\n\nhttps://cloud.google.com/natural-language/docs/analyzing-entities\nEntity Analysis inspects the given text for known entities (proper nouns such as public figures, landmarks, etc.), and returns information about those entities.","poster":"zellck","comment_id":"736861"},{"poster":"NicolasN","upvote_count":"2","content":"Apparently, there is unanimity on answer [A]\nWhat if there was another available answer in an actual exam?\n\nE. Call the Cloud Natural Language API from your application. Process the generated Content Classification as labels\n\nWhat would you choose, A or E?\n\nMy opinion is that Content Classification is more suitable for detecting subject.","timestamp":"1699430400.0","comment_id":"713583"},{"poster":"Remi2021","comment_id":"632243","content":"A is the right one . Doc says:\nEntity analysis inspects the given text for known entities (Proper nouns such as public figures, landmarks, and so on. Common nouns such as restaurant, stadium, and so on.) and returns information about those entities. Entity analysis is performed with the analyzeEntities method.","timestamp":"1689521580.0","upvote_count":"1"},{"content":"Selected Answer: A\nVote for A","upvote_count":"1","comment_id":"599512","timestamp":"1683713340.0","poster":"waterh2oeau"},{"poster":"bury","upvote_count":"1","comment_id":"544613","content":"Selected Answer: A\na is correct","timestamp":"1676039160.0"},{"upvote_count":"1","poster":"JayZeeLee","timestamp":"1667690760.0","content":"A. \nCD don't work as it requires Machine Learning experience. \nB - Sentiment Analysis is to analyze attitude, opinion, etc. So A.","comment_id":"473249"},{"comment_id":"393701","timestamp":"1656499860.0","content":"Vote for A","upvote_count":"3","poster":"sumanshu"},{"timestamp":"1629414060.0","upvote_count":"4","content":"A is correct","comment_id":"161835","poster":"haroldbenites"},{"timestamp":"1616895840.0","poster":"[Removed]","upvote_count":"5","comment_id":"68735","content":"Answer: A\nDescription: As time is less, use cloud NLP and entity is used to label general subjects, sentiment label for sentiment analysis"}],"choices":{"B":"Call the Cloud Natural Language API from your application. Process the generated Sentiment Analysis as labels.","A":"Call the Cloud Natural Language API from your application. Process the generated Entity Analysis as labels.","C":"Build and train a text classification model using TensorFlow. Deploy the model using Cloud Machine Learning Engine. Call the model from your application and process the results as labels.","D":"Build and train a text classification model using TensorFlow. Deploy the model using a Kubernetes Engine cluster. Call the model from your application and process the results as labels."},"answer_ET":"A","question_id":289,"question_text":"You are developing an application on Google Cloud that will automatically generate subject labels for users' blog posts. You are under competitive pressure to add this feature quickly, and you have no additional developer resources. No one on your team has experience with machine learning. What should you do?","exam_id":11,"url":"https://www.examtopics.com/discussions/google/view/17111-exam-professional-data-engineer-topic-1-question-71/"},{"id":"mtF0FQBAcOlk0Iz83IO8","unix_timestamp":1584428760,"question_images":[],"answers_community":["C (100%)"],"topic":"1","url":"https://www.examtopics.com/discussions/google/view/16832-exam-professional-data-engineer-topic-1-question-72/","question_text":"You are designing storage for 20 TB of text files as part of deploying a data pipeline on Google Cloud. Your input data is in CSV format. You want to minimize the cost of querying aggregate values for multiple users who will query the data in Cloud Storage with multiple engines. Which storage service and schema design should you use?","answer_description":"","answer":"C","answer_images":[],"discussion":[{"poster":"daghayeghi","comment_id":"307908","upvote_count":"54","content":"answer C:\nBigQuery can access data in external sources, known as federated sources. Instead of first\nloading data into BigQuery, you can create a reference to an external source. External\nsources can be Cloud Bigtable, Cloud Storage, and Google Drive.\nWhen accessing external data, you can create either permanent or temporary external\ntables. Permanent tables are those that are created in a dataset and linked to an external\nsource. Dataset-level access controls can be applied to these tables. When you are using a\ntemporary table, a table is created in a special dataset and will be available for approxi-\nmately 24 hours. Temporary tables are useful for one-time operations, such as loading data\ninto a data warehouse.\n\"Dan Sullivan\" Book","timestamp":"1647007320.0"},{"poster":"[Removed]","upvote_count":"30","content":"Should be C","timestamp":"1616341680.0","comment_id":"66580"},{"timestamp":"1730491020.0","upvote_count":"1","poster":"emmylou","content":"On so many of these questions, how do you actually know if you're correct. I said C but the correct answer was A. Honestly, it's driving me crazy.","comment_id":"1060032"},{"timestamp":"1721722260.0","upvote_count":"1","comment_id":"960222","poster":"Mathew106","content":"Selected Answer: C\nFor the ones saying BigTable is cheaper, BigTable in eu-north1 costs $0.748/hour per node. So if you were to run the node 24/7 you would pay more than 500$ per month. Querying 1TB of data in BigQuery is 7.5$. With smart querying and good database design you can minimize the bytes processed by BQ. So even though BigTable does not directly charge for querying, it charges for running the cluster and the overall price does not make sense. And as far as I know, it's not possible to spin up and shut down BigTable automatically. \n\nAlso, since the table is an external table to BigQuery, we incur no cost for storing that data in BigQuery and paying 300$ per month for storage."},{"poster":"samdhimal","upvote_count":"2","timestamp":"1706043900.0","comment_id":"785832","content":"C. Use Cloud Storage for storage. Link as permanent tables in BigQuery for query.\n\nCloud Storage is a highly durable and cost-effective object storage service that can be used to store large amounts of text files. By storing the input data in CSV format in Cloud Storage, you can minimize costs while still being able to query the data using BigQuery.\n\nBigQuery is a fully-managed, highly-scalable data warehouse that allows you to perform fast SQL-like queries on large datasets. By linking the Cloud Storage data as permanent tables in BigQuery, you can enable multiple users to query the data using multiple engines without the need for additional compute resources. This approach would be the most cost-effective for querying aggregate values for multiple users, as BigQuery charges based on the amount of data scanned per query, so the more data you store in BigQuery the less you pay per query.","comments":[{"comment_id":"785833","upvote_count":"1","content":"Option D, using Cloud Storage for storage and linking as temporary tables in BigQuery for query, would not be the best choice because temporary tables only exist for the duration of a user session or query and you would need to create and delete them each time a user queries the data, which would add additional cost and complexity to the process.\n\nOption A, Using Cloud Bigtable for storage, and installing the HBase shell on a Compute Engine instance to query the data, is not a cost-effective solution as Cloud Bigtable is a managed NoSQL database service which is more expensive than storing in Cloud Storage and querying in BigQuery.\n\nOption B, Using Cloud Bigtable for storage, and linking as permanent tables in BigQuery for query, is not a cost-effective solution as Cloud Bigtable is a managed NoSQL database service which is more expensive than storing in Cloud Storage and querying in BigQuery.","timestamp":"1706043900.0","poster":"samdhimal"}]},{"comment_id":"772990","poster":"RoshanAshraf","content":"Selected Answer: C\nCSV files - Cloud Storage\nBigQuery - Aggregate, multiple users\nPermanent table - multiple users\nExternal Tables is Easy to implement, cost effective","timestamp":"1705019220.0","upvote_count":"3"},{"timestamp":"1702332720.0","content":"The 'correct' answers on this platform are ridiculous","comment_id":"742146","upvote_count":"7","poster":"rivua"},{"comment_id":"736857","content":"Selected Answer: C\nC is the answer.\n\nhttps://cloud.google.com/bigquery/docs/external-data-cloud-storage#create_a_permanent_external_table","timestamp":"1701870000.0","upvote_count":"2","poster":"zellck"},{"content":"Answer is C Use Cloud Storage for storage. Link as permanent tables in BigQuery for query.\n\nBigQuery can access data in external sources, known as federated sources. Instead of first loading data into BigQuery, you can create a reference to an external source. External sources can be Cloud Bigtable, Cloud Storage, and Google Drive.\n\nWhen accessing external data, you can create either permanent or temporary external tables. Permanent tables are those that are created in a dataset and linked to an external source. Dataset-level access controls can be applied to these tables. When you are using a temporary table, a table is created in a special dataset and will be available for approxi- mately 24 hours. Temporary tables are useful for one-time operations, such as loading data into a data warehouse","poster":"VishalBule","comment_id":"545843","upvote_count":"1","timestamp":"1676206260.0"},{"content":"Selected Answer: C\nBigtable is expensive. So Cloud Storage for storing and BigQuery with permanent table for linking and querying.","poster":"medeis_jar","upvote_count":"3","comment_id":"517628","timestamp":"1672932660.0"},{"content":"Selected Answer: C\nNot A or B\nBig table is expensive, que initial data is in csv format, besides, if others are going to query data with multiple engines… GCS is the storage. Between c and D is all about permanent or temorary.\nPermanent table is a table that is created in a dataset and is linked to your external data source. Because the table is permanent, you can use dataset-level access controls to share the table with others who also have access to the underlying external data source, and you can query the table at any time.\nWhen you use a temporary table, you do not create a table in one of your BigQuery datasets. Because the table is not permanently stored in a dataset, it cannot be shared with others. Querying an external data source using a temporary table is useful for one-time, ad-hoc queries over external data, or for extract, transform, and load (ETL) processes.\nI think is C.","timestamp":"1671730260.0","upvote_count":"5","comment_id":"507267","poster":"MaxNRG","comments":[{"poster":"MaxNRG","upvote_count":"3","timestamp":"1671730260.0","comment_id":"507268","content":"https://cloud.google.com/blog/products/gcp/accessing-external-federated-data-sources-with-bigquerys-data-access-layer.\nPermanent table—You create a table in a BigQuery dataset that is linked to your external data source. This allows you to use BigQuery dataset-level IAM roles to share the table with others who may have access to the underlying external data source. Use permanent tables when you need to share the table with others.\nTemporary table—You submit a command that includes a query and creates a non-permanent table linked to the external data source. With this approach you do not create a table in one of your BigQuery datasets, so make sure to give consideration towards sharing the query or table. Consider using a temporary table for one-time, ad-hoc queries, or for one time extract, transform, or load (ETL) workflows"}]},{"upvote_count":"2","poster":"maurodipa","timestamp":"1669952280.0","comment_id":"492118","content":"Answer is A. While C seems the most reasonable answer there are 2 points to notics: a) load jobs are limited to 15 TB across all input files in BigQuery (https://cloud.google.com/bigquery/quotas); b) It is requested to minimize the cost of querying and queries in BigTable are free, while queries in BigQuery are charged per byte (https://cloud.google.com/bigquery/pricing)"},{"content":"https://cloud.google.com/bigquery/external-data-bigtable#:~:text=shared%20with%20others.-,Querying%20an%20external%20data%20source%20using%20a%20temporary%20table%20is%20useful%20for%20one%2Dtime%2C%20ad%2Dhoc%20queries%20over%20external%20data%2C%20or%20for%20extract%2C%20transform%2C%20and%20load%20(ETL)%20processes.,-Querying%20Cloud%20Bigtable","upvote_count":"1","poster":"Abhi16820","timestamp":"1669279200.0","comment_id":"485776"},{"timestamp":"1666230360.0","content":"C is the answer.","upvote_count":"1","comment_id":"464880","poster":"tsoetan001"},{"poster":"Ysance_AGS","timestamp":"1664365980.0","content":"A is correct since the question asks \"You want to minimize the cost of querying aggregate values\" => Big Table is free when querying data.","comment_id":"453345","upvote_count":"3"},{"upvote_count":"1","timestamp":"1663278540.0","content":"Vote for C","comment_id":"445480","poster":"nguyenmoon"},{"poster":"gcp_learner","comments":[{"upvote_count":"1","timestamp":"1661542680.0","content":"Storage cost of data for BQ is the same as standard cloud storage, actually less for long term storage as it automatically moves to nearline storage.\n\nhttps://cloud.google.com/bigquery/pricing#storage\nhttps://cloud.google.com/storage#section-10","comment_id":"432545","poster":"triipinbee"},{"timestamp":"1659543060.0","comment_id":"419295","poster":"Yiouk","content":"https://cloud.google.com/bigquery/docs/writing-results#temporary_and_permanent_tables","upvote_count":"1"}],"upvote_count":"4","comment_id":"399601","content":"Interesting options. For me, A & B ruled out because BigTable doesn’t fit this use case, leaves us with C & D. C will incur additional cost of storing data in GCS & BigQuery because it mentions linking. \n\nSo I would go with D ie store the data in GCS and create external tables in BigQuery.","timestamp":"1657073040.0"},{"comments":[{"upvote_count":"1","poster":"sumanshu","comment_id":"402242","timestamp":"1657315800.0","comments":[{"comment_id":"402246","content":"There are no storage costs for temporary tables, but if you write query results to a permanent table, you are charged for storing the data.","timestamp":"1657315920.0","comments":[{"poster":"triipinbee","content":"even if it's a temp table, the storage cost for active storage in cloud bucket and the standard storage in BQ is the same. So no point creating temp tables.","upvote_count":"1","timestamp":"1661542500.0","comment_id":"432544"}],"poster":"sumanshu","upvote_count":"1"}],"content":"Vote for 'D' \n\nBigQuery uses temporary tables to cache query results.\nhttps://cloud.google.com/bigquery/docs/writing-results"}],"content":"Vote for C","upvote_count":"2","poster":"sumanshu","timestamp":"1656503820.0","comment_id":"393744"},{"comment_id":"393735","poster":"sumanshu","timestamp":"1656502800.0","upvote_count":"1","content":"Vote for 'C'","comments":[]},{"comment_id":"388387","timestamp":"1655942880.0","upvote_count":"2","poster":"DeepakKhattar","content":"A - The main requirement is \"You want to minimize the cost of querying\". Only in BigTable there is no cost querying. Plus there is no caching for external datasource. When querying an external data source from BigQuery, you are charged for the number of bytes read by the query. https://cloud.google.com/bigquery/pricing"},{"timestamp":"1638821580.0","comments":[{"comment_id":"270813","timestamp":"1642552080.0","content":"Just because Cloud storage works for unstructured data doesn't mean it can't work for structured data. Google lists cloud storage as a data lake option.\nhttps://cloud.google.com/solutions/build-a-data-lake-on-gcp#processing_and_analytics\nI would say the answer is C.\nTemporary tables can't be shared and doesn't have access controls so its not D.\nBigQuery is specifically designed for multiple speedy queries so I doubt its A.","upvote_count":"4","poster":"lammingtons"}],"upvote_count":"1","poster":"mila32905","comment_id":"236777","content":"B, file is structured so Cloud Storage (unstructured data) is not an option and you can query BigTable from BigQuery - https://cloud.google.com/bigquery/external-data-bigtable"},{"poster":"snamburi3","upvote_count":"2","comment_id":"221902","timestamp":"1637241720.0","content":"maybe C. \nhttps://stackoverflow.com/questions/54356127/use-case-of-using-big-query-or-big-table-for-querying-aggregate-values"},{"upvote_count":"3","content":"C is correct","comment_id":"161836","timestamp":"1629414180.0","poster":"haroldbenites"},{"poster":"gvaf4","content":"I would say A. \nShoud not use bigquery due to Maximum size per load job — 15 TB across all input files\nhttps://cloud.google.com/bigquery/quotas","comment_id":"118370","timestamp":"1624534920.0","upvote_count":"3","comments":[{"poster":"tprashanth","content":"The limit is for loading data into BQ, not to query. So I would go with C.","comment_id":"134025","upvote_count":"5","timestamp":"1626188040.0"}]},{"comment_id":"66728","upvote_count":"3","content":"A and C ruled out because Users would like to run Aggregate query on the data.\nTo Minimize the cost, its best to create the table as permanent table that can make use of caching feature. External(Federated) table add's additional cost and performance issue while running analytics query. So C.","poster":"Rajokkiyam","timestamp":"1616380140.0","comments":[{"upvote_count":"10","timestamp":"1616380200.0","comment_id":"66729","poster":"Rajokkiyam","content":"A and B ruled out because Users would like to run Aggregate query on the data.\nTo Minimize the cost, its best to create the table as permanent table that can make use of caching feature. External(Federated) table add's additional cost and performance issue while running analytics query. So C."}]},{"poster":"jvg637","timestamp":"1616355720.0","content":"How about \"to minimize the cost of querying\"? . May be A is OK","upvote_count":"2","comment_id":"66657"},{"content":"Why not C?","upvote_count":"11","poster":"rickywck","comment_id":"65061","timestamp":"1615964760.0"}],"exam_id":11,"answer_ET":"C","choices":{"D":"Use Cloud Storage for storage. Link as temporary tables in BigQuery for query.","C":"Use Cloud Storage for storage. Link as permanent tables in BigQuery for query.","A":"Use Cloud Bigtable for storage. Install the HBase shell on a Compute Engine instance to query the Cloud Bigtable data.","B":"Use Cloud Bigtable for storage. Link as permanent tables in BigQuery for query."},"timestamp":"2020-03-17 08:06:00","isMC":true,"question_id":290}],"exam":{"provider":"Google","isImplemented":true,"numberOfQuestions":319,"id":11,"isMCOnly":true,"lastUpdated":"11 Apr 2025","isBeta":false,"name":"Professional Data Engineer"},"currentPage":58},"__N_SSP":true}