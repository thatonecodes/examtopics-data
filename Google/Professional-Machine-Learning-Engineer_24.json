{"pageProps":{"questions":[{"id":"qQOz3SYBgkTdiken8sRW","answer_ET":"C","unix_timestamp":1705116180,"choices":{"D":"Create a Vertex AI Workbench notebook with instance type n2-standard-4.","B":"Create a Google Kubernetes Engine cluster with a basic node pool configuration, install Java, Scala, and Apache Spark dependencies on it.","C":"Create a Standard (1 master, 3 workers) Dataproc cluster, and run a Vertex AI Workbench notebook instance on it.","A":"Create a n2-standard-4 VM instance and install Java, Scala, and Apache Spark dependencies on it."},"question_id":116,"answer":"C","discussion":[{"upvote_count":"1","content":"Selected Answer: C\nC is the right answer because it ensures:\nCost-effectiveness: Dataproc is managed and you only pay for the compute time used, which is cost-effective for a POC. A standard cluster is enough for the task.\n\nEase of use: Dataproc simplifies the process of setting up and managing a Spark cluster\n\nMinimal effort: a Dataproc cluster + a Vertex AI Workbench instance is a straightforward process through the console or command-line tools, minimizing setup time and effort compared to manually configuring VMs or Kubernetes clusters.\n\n*A and B include manual installation steps; D creates a notebook environment but it's not enough to run a PySpark job.","poster":"lunalongo","comment_id":"1321446","timestamp":"1733242800.0"},{"poster":"DaleR","comment_id":"1316819","content":"D. Just ran a pilot on Workbench","timestamp":"1732399440.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"1312347","content":"Selected Answer: D\nD. \"minimal cost and effort\". There's only one answer.","timestamp":"1731628860.0","poster":"f084277"},{"upvote_count":"1","timestamp":"1726118700.0","comment_id":"1282474","poster":"baimus","content":"Selected Answer: C\nC and D are both valid, as people point out you can technically have Spark preinstalled on D. But this is for a proof of concept for the real design. The concept is not proved by using a notebook, as it's not best practice. Therefore C makes more sense, and is still low effort as it's managed."},{"poster":"AK2020","timestamp":"1722634920.0","comment_id":"1260054","upvote_count":"1","content":"Selected Answer: C\nC is the answer"},{"content":"Selected Answer: C\nI'm following option C. Please take a look the concept of 'Dataproc documentation' (ref: https://cloud.google.com/dataproc/docs)\n\nWith option D: doesn't provide a solution for managing and scaling the Spark environment, which is necessary for running PySpark workloads.","poster":"TanTran04","timestamp":"1720397880.0","upvote_count":"2","comment_id":"1244036"},{"timestamp":"1713491820.0","comment_id":"1198269","content":"Selected Answer: D\nVertex AI Workbench notebook: This option provides a pre-configured environment with popular data science libraries like PySpark already installed. It allows you to focus on migrating your PySpark code with minimal changes.\nn2-standard-4 instance type: This is a general-purpose machine type suitable for various data science tasks. It offers a good balance between cost and performance for initial exploration.","upvote_count":"1","comments":[{"poster":"Jason_Cloud_at","timestamp":"1724979120.0","upvote_count":"1","comment_id":"1274745","content":"Option D doesnt provide Pyspark out of the box, you have to manually install it wherelse in C dataproc is managed spark and hadoop services which supports running pyspark services right away."},{"timestamp":"1713616440.0","content":"https://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#overview \nwhy not c?","poster":"pinimichele01","upvote_count":"1","comment_id":"1199141"},{"comment_id":"1198271","timestamp":"1713491820.0","content":"A. Create a n2-standard-4 VM instance: This option requires manually installing Java, Scala, and Spark dependencies, which is time-consuming and prone to errors. It also involves managing the VM instance lifecycle, increasing complexity.\nB. Create a Google Kubernetes Engine cluster: Setting up and managing a Kubernetes cluster for a single job is overkill for a proof of concept. It adds unnecessary complexity and cost.\nC. Create a Standard Dataproc cluster: While Dataproc is a managed Spark environment on GCP, setting up a full cluster (master and workers) might be more resource-intensive than needed for a single job, especially for a proof of concept.","poster":"fitri001","upvote_count":"1"}],"poster":"fitri001"},{"poster":"gscharly","upvote_count":"2","content":"Selected Answer: D\nwent with D: https://cloud.google.com/vertex-ai/docs/workbench/instances/create-dataproc-enabled","comment_id":"1197345","comments":[{"comment_id":"1199142","content":"https://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#overview","timestamp":"1713616500.0","poster":"pinimichele01","upvote_count":"1"}],"timestamp":"1713372060.0"},{"content":"Selected Answer: C\nWhen you want to move your Apache Spark workloads from an on-premises environment to Google Cloud, we recommend using Dataproc to run Apache Spark/Apache Hadoop clusters.\n\nhttps://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#overview","poster":"pinimichele01","upvote_count":"1","comment_id":"1191393","timestamp":"1712553900.0"},{"content":"Selected Answer: D\nD\nCan use Notebook pre-installed libraries and tools, including PySpark.","comment_id":"1171068","poster":"Yan_X","upvote_count":"2","timestamp":"1710167160.0"},{"upvote_count":"1","comment_id":"1164166","content":"Selected Answer: D\nMy bad, I mean is Option D.","poster":"Carlose2108","timestamp":"1709391300.0"},{"upvote_count":"2","timestamp":"1709391240.0","content":"Selected Answer: C\nI went with C.\nFor Proof Of Concept and requires minimal cost and effort. Furthermore, Vertex AI Workbench notebooks come pre-configured with PySpark.","poster":"Carlose2108","comment_id":"1164165"},{"content":"Selected Answer: C\nMy answer: C\nC: This option leverages Google Cloud's Dataproc service, which is designed for running Apache Spark and other big data processing frameworks. By creating a Standard Dataproc cluster, you can easily scale resources as needed for your workload. \n\nA. n2-standard-4 VM: This requires manual setup and ongoing maintenance, increasing cost and effort.\n\nB. GKE cluster: While offering containerization benefits, it necessitates managing containers and Spark configurations, adding complexity.\n\nD. With Vertex AI Workbench, your team can develop, train, and deploy machine learning models using popular frameworks like TensorFlow, PyTorch, and scikit-learn. However, while Vertex AI Workbench supports PySpark, it may not be the optimal choice for migrating existing PySpark workloads, as it's primarily focused on machine learning tasks.","comment_id":"1149806","comments":[{"poster":"Carlose2108","timestamp":"1708991520.0","comment_id":"1160150","upvote_count":"2","content":"You're right but I have a doubt about in a part of Option D\n\"You need to build a proof of concept to migrate one data science job to Google Cloud\""}],"upvote_count":"4","timestamp":"1707881160.0","poster":"guilhermebutzke"},{"timestamp":"1706860380.0","upvote_count":"2","content":"Selected Answer: C\nAgree with BlehMaks https://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#overview Dataproc cluster seems more suitable","comment_id":"1138278","poster":"ddogg"},{"content":"Selected Answer: D\nhttps://cloud.google.com/vertex-ai-notebooks?hl=en\nData Data Lake and Spark in one place\n\nWhether you use TensorFlow, PyTorch, or Spark, you can run any engine from Vertex AI Workbench. \n\nD is correct","poster":"shadz10","comment_id":"1125275","timestamp":"1705522020.0","upvote_count":"1"},{"content":"Selected Answer: C\nhttps://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#overview","poster":"BlehMaks","upvote_count":"2","comment_id":"1123417","timestamp":"1705326780.0"},{"comment_id":"1121270","poster":"pikachu007","content":"Selected Answer: D\nMinimal setup: Vertex AI Workbench notebooks come pre-configured with PySpark and other data science tools, eliminating the need for manual installation and setup.\nCost-effectiveness: Vertex AI Workbench offers managed notebooks with pay-as-you-go pricing, making it a cost-efficient option for proof-of-concept testing.\nEase of use: Data scientists can directly run PySpark code in the notebook without managing infrastructure, streamlining the migration process.\nScalability: Vertex AI Workbench can easily scale to handle larger workloads or multiple users if the proof-of-concept is successful.","upvote_count":"1","timestamp":"1705116180.0"}],"exam_id":13,"timestamp":"2024-01-13 04:23:00","answers_community":["C (64%)","D (36%)"],"answer_images":[],"question_text":"You work for a startup that has multiple data science workloads. Your compute infrastructure is currently on-premises, and the data science workloads are native to PySpark. Your team plans to migrate their data science workloads to Google Cloud. You need to build a proof of concept to migrate one data science job to Google Cloud. You want to propose a migration process that requires minimal cost and effort. What should you do first?","topic":"1","isMC":true,"question_images":[],"url":"https://www.examtopics.com/discussions/google/view/131018-exam-professional-machine-learning-engineer-topic-1-question/","answer_description":""},{"id":"4EQYDrTQibf0H04d48Rs","choices":{"D":"Vertex AI Pipelines, Vertex AI Feature Store, and Vertex AI TensorBoard","C":"Vertex ML Metadata, Vertex AI Experiments, and Vertex AI TensorBoard","B":"Vertex AI Pipelines, Vertex AI Experiments, and Vertex AI Vizier","A":"Vertex ML Metadata, Vertex AI Feature Store, and Vertex AI Vizier"},"unix_timestamp":1705116660,"answers_community":["C (100%)"],"topic":"1","url":"https://www.examtopics.com/discussions/google/view/131019-exam-professional-machine-learning-engineer-topic-1-question/","answer_description":"","question_text":"You work for a bank. You have been asked to develop an ML model that will support loan application decisions. You need to determine which Vertex AI services to include in the workflow. You want to track the model’s training parameters and the metrics per training epoch. You plan to compare the performance of each version of the model to determine the best model based on your chosen metrics. Which Vertex AI services should you use?","question_images":[],"answer_images":[],"timestamp":"2024-01-13 04:31:00","answer":"C","answer_ET":"C","question_id":117,"discussion":[{"comment_id":"1238335","upvote_count":"2","content":"Selected Answer: C\nAgree with C","timestamp":"1719510960.0","poster":"dija123"},{"upvote_count":"1","comment_id":"1191394","content":"Selected Answer: C\nagree with pikachu007","timestamp":"1712554020.0","poster":"pinimichele01"},{"upvote_count":"2","comment_id":"1174215","comments":[{"poster":"info_appsatori","upvote_count":"2","content":"I guess because Vizier is a tool that helps to tune hyperparameters, and in a contrary Tensorboard is a tool to explore experiments.","timestamp":"1723377420.0","comment_id":"1264097"}],"timestamp":"1710499860.0","poster":"VipinSingla","content":"Why not B ?"},{"timestamp":"1708991280.0","comment_id":"1160148","content":"Selected Answer: C\nI went C","poster":"Carlose2108","upvote_count":"1"},{"poster":"winston9","timestamp":"1705137480.0","content":"Selected Answer: C\nuse Tensorboard to track the model’s training parameters and the metrics per training epoch.","comment_id":"1121444","upvote_count":"3"},{"timestamp":"1705116660.0","poster":"pikachu007","content":"Selected Answer: C\nVertex ML Metadata:\nTracks model training parameters, hyperparameters, metrics, and lineage information.\nStores metadata in a central repository for easy access and comparison.\nIntegrates seamlessly with Vertex AI Experiments and TensorBoard.\nVertex AI Experiments:\nOrganizes and manages model training runs as experiments.\nVisualizes experiment results, including metrics and parameter comparisons.\nFacilitates tracking of the best performing model versions.\nVertex AI TensorBoard:\nProvides detailed visualizations of training metrics and model performance.\nEnables analysis of model behavior at each training epoch.\nIntegrates with Vertex AI Experiments for seamless access to experiment data.","upvote_count":"3","comment_id":"1121275"}],"isMC":true,"exam_id":13},{"id":"wkosk6iPaya47PC6ynvq","question_text":"You work for an auto insurance company. You are preparing a proof-of-concept ML application that uses images of damaged vehicles to infer damaged parts. Your team has assembled a set of annotated images from damage claim documents in the company’s database. The annotations associated with each image consist of a bounding box for each identified damaged part and the part name. You have been given a sufficient budget to train models on Google Cloud. You need to quickly create an initial model. What should you do?","answer_ET":"B","isMC":true,"discussion":[{"poster":"louisaok","content":"Selected Answer: B\n>>\" You have been given a sufficient budget to train models on Google Cloud\"\nit is rare to see a company give enough money to run a mission-critical project.","comment_id":"1313775","timestamp":"1731889500.0","upvote_count":"1"},{"comment_id":"1296233","timestamp":"1728675180.0","upvote_count":"1","content":"My vote is B","poster":"Foxy2021"},{"content":"Selected Answer: B\nquickly create an initial model = automl","timestamp":"1719670980.0","comment_id":"1239325","upvote_count":"2","poster":"VinaoSilva"},{"timestamp":"1712554080.0","comment_id":"1191395","poster":"pinimichele01","content":"Selected Answer: B\nwent with B","upvote_count":"2"},{"content":"Selected Answer: B\nBy doing B we are doing D. I suppose B in more specific about the model and thus \"more\" correct?\nThoughts?","timestamp":"1709920080.0","upvote_count":"2","poster":"edoo","comment_id":"1169008"},{"upvote_count":"1","content":"Selected Answer: B\nB makes the most sense, data is already labelled and a pretrained model may not fit for this specific case","poster":"ddogg","comment_id":"1138345","timestamp":"1706864880.0"},{"upvote_count":"4","timestamp":"1705116840.0","poster":"pikachu007","comment_id":"1121276","content":"Selected Answer: B\nSpeed: AutoML excels in creating high-quality models with minimal code and setup, significantly accelerating model development.\nEase of use: It provides a user-friendly interface and automates many aspects of model training, making it accessible even for those without extensive ML expertise.\nAutomatic optimization: AutoML automatically handles hyperparameter tuning, feature engineering, and architecture selection, reducing manual effort and expertise required.\nCustom object detection: It supports custom object detection tasks, directly addressing the need to identify damaged parts in images."}],"choices":{"B":"Train an object detection model in AutoML by using the annotated image data.","D":"Train an object detection model in Vertex AI custom training by using the annotated image data.","A":"Download a pre-trained object detection model from TensorFlow Hub. Fine-tune the model in Vertex AI Workbench by using the annotated image data.","C":"Create a pipeline in Vertex AI Pipelines and configure the AutoMLTrainingJobRunOp component to train a custom object detection model by using the annotated image data."},"answers_community":["B (100%)"],"answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/131020-exam-professional-machine-learning-engineer-topic-1-question/","unix_timestamp":1705116840,"answer_description":"","topic":"1","question_images":[],"exam_id":13,"question_id":118,"timestamp":"2024-01-13 04:34:00","answer":"B"},{"id":"4oc9SmNcCqhS3huWoxJr","answer_ET":"A","url":"https://www.examtopics.com/discussions/google/view/131024-exam-professional-machine-learning-engineer-topic-1-question/","isMC":true,"answers_community":["A (100%)"],"question_images":[],"answer":"A","discussion":[{"timestamp":"1729304160.0","upvote_count":"3","comment_id":"1198277","comments":[{"poster":"fitri001","comment_id":"1198278","content":"B. CMEK and decryption: While CMEKs provide strong encryption, decrypting PII data during exploration exposes sensitive information. This increases the risk of accidental leaks or unauthorized access.\nC. VM with VPC Service Controls: This approach can add complexity and doesn't directly address PII privacy concerns during analysis.\nD. Google-managed encryption and decryption: Similar to option B, decrypting PII data for exploration weakens privacy.","timestamp":"1729304220.0","upvote_count":"1"}],"content":"Selected Answer: A\nCloud DLP API: This service redacts or replaces sensitive information in your data before processing. It allows data exploration and analysis without exposing PII directly.\nPrivacy Preservation: De-identification ensures sensitive information is not revealed during analysis, protecting patient privacy.","poster":"fitri001"},{"content":"Selected Answer: A\nhttps://cloud.google.com/dlp/docs/inspect-sensitive-text-de-identify","poster":"pinimichele01","comment_id":"1191397","timestamp":"1728365520.0","upvote_count":"1"},{"poster":"edoo","timestamp":"1725810540.0","upvote_count":"1","content":"Selected Answer: A\nA is obvious.","comment_id":"1169009"},{"comment_id":"1123449","timestamp":"1721047320.0","upvote_count":"1","poster":"b1a8fae","content":"Selected Answer: A\nA. https://cloud.google.com/dlp/docs/inspect-sensitive-text-de-identify"},{"content":"Selected Answer: A\nMinimizes exposure of sensitive data: De-identification replaces or removes sensitive information, reducing the risk of accidental exposure or unauthorized access during analysis.\nPreserves data utility: DLP can de-identify data while maintaining its usefulness for exploration and preprocessing, ensuring meaningful analysis without compromising privacy.\nFlexibility in de-identification: You can choose appropriate de-identification techniques (e.g., masking, pseudonymization, generalization) based on specific privacy requirements and analysis needs.","upvote_count":"2","comment_id":"1121280","timestamp":"1720834500.0","poster":"pikachu007"}],"choices":{"B":"Use customer-managed encryption keys (CMEK) to encrypt the PII data at rest, and decrypt the PII data during data exploration and preprocessing.","C":"Use a VM inside a VPC Service Controls security perimeter to perform data exploration and preprocessing.","D":"Use Google-managed encryption keys to encrypt the PII data at rest, and decrypt the PII data during data exploration and preprocessing.","A":"Use the Cloud Data Loss Prevention (DLP) API to de-identify the PII before performing data exploration and preprocessing."},"unix_timestamp":1705116900,"question_text":"You are analyzing customer data for a healthcare organization that is stored in Cloud Storage. The data contains personally identifiable information (PII). You need to perform data exploration and preprocessing while ensuring the security and privacy of sensitive fields. What should you do?","answer_description":"","topic":"1","answer_images":[],"timestamp":"2024-01-13 04:35:00","exam_id":13,"question_id":119},{"id":"StLzxXD3vdEUaZgjMto8","question_id":120,"unix_timestamp":1705117380,"exam_id":13,"choices":{"B":"Use scikit-learn to build a tree-based model, and use partial dependence plots (PDP) to explain the model output.","D":"Use TensorFlow to create a deep learning-based model, and use the sampled Shapley method to explain the model output.","C":"Use TensorFlow to create a deep learning-based model, and use Integrated Gradients to explain the model output.","A":"Use scikit-learn to build a tree-based model, and use SHAP values to explain the model output."},"topic":"1","timestamp":"2024-01-13 04:43:00","answer_ET":"C","answer_images":[],"answer":"C","isMC":true,"question_images":[],"answer_description":"","answers_community":["C (100%)"],"discussion":[{"timestamp":"1733348220.0","upvote_count":"1","comment_id":"1322101","content":"Selected Answer: C\nThe question is about image/pixels. So the integrated Gradients is better. Shapley is for input features.","poster":"FireAtMe"},{"comment_id":"1238580","timestamp":"1719555480.0","upvote_count":"2","content":"Selected Answer: C\nUse Integrated Gradients to explain the model output","poster":"dija123"},{"upvote_count":"2","timestamp":"1712555280.0","poster":"pinimichele01","comment_id":"1191405","content":"Selected Answer: C\nhttps://cloud.google.com/vertex-ai/docs/explainable-ai/overview"},{"timestamp":"1712332380.0","comment_id":"1189993","content":"Selected Answer: C\nGiven the scenario of using high definition images as inputs for predictive maintenance on bridges, and the need to explain the model output to stakeholders, the most appropriate choice would be:\n\nC. Use TensorFlow to create a deep learning-based model, and use Integrated Gradients to explain the model output.\n\nIntegrated Gradients is a method used to explain the predictions of deep learning models by attributing the contribution of each pixel in the input image to the final prediction. This would provide insights into which parts of the bridge images are most influential in the model's decision-making process, helping stakeholders understand why a particular prediction was made and allowing them to take appropriate action.","upvote_count":"3","poster":"Shark0"},{"timestamp":"1705331820.0","comments":[{"timestamp":"1712555220.0","content":"https://cloud.google.com/vertex-ai/docs/explainable-ai/overview\n\nthis is right, your is deprecated!","upvote_count":"1","comment_id":"1191404","poster":"pinimichele01"}],"content":"Selected Answer: C\nhttps://cloud.google.com/ai-platform/prediction/docs/ai-explanations/overview#compare-methods","upvote_count":"2","comment_id":"1123481","poster":"BlehMaks"},{"content":"Selected Answer: C\nHandling image input: Deep learning models excel in processing complex visual data like high-definition images, making them ideal for extracting relevant features from bridge images for defect detection.\nExplainability with Integrated Gradients: Integrated Gradients is a powerful technique specifically designed to explain the predictions of deep learning models. It attributes model output to specific input features, providing insights into how the model makes decisions.\nVisualization: Integrated Gradients can generate visual explanations, such as heatmaps, that highlight image regions most influential to predictions, aiding in understanding and trust for stakeholders.","upvote_count":"1","timestamp":"1705117380.0","comment_id":"1121283","poster":"pikachu007"}],"url":"https://www.examtopics.com/discussions/google/view/131026-exam-professional-machine-learning-engineer-topic-1-question/","question_text":"You are building a predictive maintenance model to preemptively detect part defects in bridges. You plan to use high definition images of the bridges as model inputs. You need to explain the output of the model to the relevant stakeholders so they can take appropriate action. How should you build the model?"}],"exam":{"isMCOnly":true,"numberOfQuestions":304,"isImplemented":true,"lastUpdated":"11 Apr 2025","name":"Professional Machine Learning Engineer","id":13,"isBeta":false,"provider":"Google"},"currentPage":24},"__N_SSP":true}