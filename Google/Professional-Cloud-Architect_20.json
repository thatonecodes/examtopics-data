{"pageProps":{"questions":[{"id":"gkmD9shhcIABhQflbWoR","choices":{"D":"Configure an uptime alert in Cloud Monitoring.","A":"Configure liveness and readiness probes in the Pod specification.","B":"Configure health checks on the managed instance group.","C":"Create a Scheduled Task to check whether the application is available."},"discussion":[{"comment_id":"659179","poster":"jabrrJ68w02ond1","comments":[{"content":"more explanation in the below link..https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-setting-up-health-checks-with-readiness-and-liveness-probes","upvote_count":"9","poster":"khadar","comment_id":"664792","timestamp":"1662740400.0"}],"content":"Selected Answer: A\nA: Configuring the right liveness and readiness probes prevents outages when rolling out a new ReplicaSet of a Deployment, because Pods are only getting traffic when they are considered ready.\nB: With GKE, you do not deal with MIGs.\nC: Does not use GKE tools and is therefore not the best option.\nD: Does alert you but does not prevent the outage.","upvote_count":"21","timestamp":"1662288360.0"},{"content":"B. We are talking about GKE, not Compute Engine instances\nC. The task will not check in real-time\nD. Uptime alerts do not apply to GKE pods","comment_id":"1298691","upvote_count":"1","timestamp":"1729078020.0","poster":"MarcoPellegrino"},{"comment_id":"1193106","poster":"a53fd2c","upvote_count":"2","timestamp":"1712762640.0","content":"There is not such a thing as Managed compute instances in GKE\nhttps://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-setting-up-health-checks-with-readiness-and-liveness-probes"},{"upvote_count":"1","content":"Selected Answer: D\nRight answer is D. Liveness and readiness probes (option A) are essential for overall application health but might not directly detect misconfigurations during deployments. They focus on ensuring pods are healthy and responsive, not necessarily catching configuration issues.","poster":"mesodan","comments":[{"comment_id":"1227399","poster":"tlopsm","timestamp":"1717947000.0","content":"uptime and monitoring will not stop outages in application, howver you will be informed on time to respond to the issue.\n\nA. Configuring liveness and readiness probe in each pod will stop starting pods from receiving traffic before they are declared ready and available. hence before taking down a working pod.","upvote_count":"1"}],"comment_id":"1165082","timestamp":"1709499360.0"},{"upvote_count":"2","poster":"massacare","content":"Selected Answer: A\nWho answered aside from A never read/implementes kubernetes best practices. Link https://cloud.google.com/blog/products/containers-kubernetes/best-practices-for-creating-a-highly-available-gke-cluster","timestamp":"1691639580.0","comment_id":"977252"},{"poster":"red_panda","timestamp":"1687237080.0","content":"Selected Answer: A\nA without any doubts","upvote_count":"2","comment_id":"928159"},{"content":"D. Configure an uptime alert in Cloud Monitoring.\nConfiguring an uptime alert in Cloud Monitoring will notify the team when the application becomes unavailable. This will help in detecting outages before they occur and mitigate the risks of releasing new versions with misconfigurations.\nWhile configuring liveness and readiness probes in the Pod specification and configuring health checks on the managed instance group are important for ensuring that the application is running, they do not prevent outages caused by misconfigurations with production parameters.\nCreating a Scheduled Task to check whether the application is available is also useful, but it is not preventive in nature. By the time a scheduled task detects an outage, the damage may have already been done.","poster":"mimicha1","upvote_count":"2","comment_id":"926120","timestamp":"1687019340.0","comments":[{"upvote_count":"1","poster":"mimicha1","content":"why not A ? \n* Configuring liveness and readiness probes in the Pod specification is important to detect when a container in a Pod becomes unresponsive or starts experiencing problems. However, it does not directly prevent outages caused by misconfigurations with parameters that are only used in production.\n\nLiveness and readiness probes can help to detect issues with the application, but they do not provide information about the health of the underlying infrastructure. Misconfigurations with parameters that are only used in production can cause problems with the infrastructure itself, which may not be detected by liveness and readiness probes.\n\nIn summary, while configuring liveness and readiness probes is important, it should be done in addition to other preventive measures such as configuring an uptime alert in Cloud Monitoring to ensure timely detection of outages and reduce their impact on the application.","comment_id":"926122","timestamp":"1687019400.0"}]},{"comment_id":"834398","content":"Selected Answer: A\nConfigure liveness and readiness probes in the Pod specification.\n\nThis will help to prevent outages by ensuring that only healthy Pods are serving traffic. The liveness probe will check that the Pod is running and responding to requests. The readiness probe will check that the Pod is ready to serve traffic, such as by checking that the application is installed and configured.","poster":"CGS22","timestamp":"1678395120.0","upvote_count":"4"},{"content":"Liveness and readiness probes are used to determine the health of a Pod. Liveness probes are used to determine whether a Pod is running, and readiness probes are used to determine whether a Pod is able to receive traffic.\n\nBy configuring liveness and readiness probes in the Pod specification, you can help to prevent outages when releasing new versions of the application via a rolling deployment. If a Pod fails a liveness or readiness probe, it will be restarted, which can help to prevent issues caused by misconfigured parameters or other problems.\nThe correct answer is A: Configure liveness and readiness probes in the Pod specification.","timestamp":"1672060320.0","comment_id":"757445","comments":[{"comment_id":"757446","timestamp":"1672060320.0","upvote_count":"2","content":"Option B: Configuring health checks on the managed instance group is not relevant in this scenario, as the application is running in a GKE cluster, not on a managed instance group.\n\nOption C: Creating a Scheduled Task to check whether the application is available may help to detect outages, but it will not prevent them from occurring. To prevent outages, you should focus on identifying and addressing the root cause of the problem.\n\nOption D: Configuring an uptime alert in Cloud Monitoring may help to detect outages, but it will not prevent them from occurring. To prevent outages, you should focus on identifying and addressing the root cause of the problem.","poster":"omermahgoub"}],"upvote_count":"3","poster":"omermahgoub"},{"timestamp":"1671190320.0","upvote_count":"1","content":"Selected Answer: A\nA Is the Correct Answer","comment_id":"747107","poster":"surajkrishnamurthy"},{"upvote_count":"1","timestamp":"1669023180.0","content":"Selected Answer: A\nA is ok","comment_id":"723347","poster":"megumin"},{"upvote_count":"1","content":"A is best answer","comment_id":"695661","poster":"AzureDP900","timestamp":"1665862440.0"},{"comment_id":"669098","content":"Selected Answer: A\nA is the answer.\n\nKubernetes Health Checks with Readiness and Liveness Probes\nhttps://www.youtube.com/watch?v=mxEvAPQRwhw","timestamp":"1663164660.0","poster":"zellck","upvote_count":"2"},{"poster":"rhage_56","comment_id":"659071","timestamp":"1662279720.0","upvote_count":"1","content":"Selected Answer: A\nB is out since MIGs relate to compute engine. D and C are both not preventive measures."},{"poster":"spET_1024","upvote_count":"2","timestamp":"1662198120.0","comment_id":"658290","content":"Option A is correct. Since it is regarding GKE and the application deployed in GKE cluster. Therefore, managed instance group does not have anything to do.\nSo, right answer is:\nA. Configure liveness and readiness probes in the Pod specification."},{"content":"Selected Answer: A\nA. \nThere are no MIGs in GKE. Only thing that makes sense is to have good readiness probes","poster":"aut0pil0t","timestamp":"1662188280.0","comment_id":"658147","upvote_count":"4"}],"exam_id":4,"question_images":[],"answers_community":["A (97%)","3%"],"unix_timestamp":1662188280,"answer_description":"","topic":"1","question_id":96,"answer_images":[],"question_text":"Your company has an application running as a Deployment in a Google Kubernetes Engine (GKE) cluster. When releasing new versions of the application via a rolling deployment, the team has been causing outages. The root cause of the outages is misconfigurations with parameters that are only used in production. You want to put preventive measures for this in the platform to prevent outages. What should you do?","url":"https://www.examtopics.com/discussions/google/view/79702-exam-professional-cloud-architect-topic-1-question-185/","answer":"A","isMC":true,"answer_ET":"A","timestamp":"2022-09-03 08:58:00"},{"id":"CNKSIWyqDZo6YdCQ9kGu","answer_images":[],"exam_id":4,"discussion":[{"timestamp":"1662288660.0","content":"Selected Answer: C\nA: Is not necessary because you can have multiple node pools with different configurations.\nB: Optimizes resource usage of CPU/memory in your existing node pool but does not necessarily improve cost - still an option that should be considered.\nC: This looks really good. Autoscaling workloads and the node pools makes your whole infrastructure more elastic and gives you the option to rely on the same node pool.\nD: This might not be a good option for every type of workload. Batch and stateless workloads can often handle this quite well, but stateful workloads are not well-suited for operation on preemptible VMs.\n\nSince only one answer is accepted, I'll choose C.","poster":"jabrrJ68w02ond1","comment_id":"659181","upvote_count":"15"},{"content":"Selected Answer: C\nC is the correct answer as it doesn't involve major changes to the current Kubernetes configuration","poster":"ramzez4815","timestamp":"1662434100.0","upvote_count":"8","comment_id":"660748"},{"poster":"frank_tsai_tech","comment_id":"1409852","content":"Selected Answer: C\nThe correct answer is C. Configure a HorizontalPodAutoscaler for all stateless workloads and for all compatible stateful workloads. Configure the cluster to use node auto scaling.\n\nExplanation\nBy implementing horizontal pod autoscaling, your cluster will adjust the number of pod replicas based on current demand. In tandem with node autoscaling, the GKE cluster can automatically add or remove nodes based on overall utilization. This means that during periods of low workload, the cluster can scale down—reducing the number of nodes and therefore lowering costs—while still maintaining the necessary capacity to handle peak loads without compromising availability. This approach optimizes resource usage across the cluster, making it cost effective for mixed workloads.","timestamp":"1742864340.0","upvote_count":"1"},{"poster":"MarcoPellegrino","timestamp":"1729077240.0","content":"It uses Google features","upvote_count":"1","comment_id":"1298678"},{"poster":"Gino17m","comment_id":"1204423","upvote_count":"1","content":"Selected Answer: C\nVote for C. In B limits could compromise availability.","timestamp":"1714466460.0"},{"timestamp":"1677678420.0","upvote_count":"1","comment_id":"825904","content":"Selected Answer: C\nAnswer C. Use HorizontalPodAutoscaler.","poster":"AugustoKras011111"},{"upvote_count":"1","content":"Selected Answer: C\nHorizontalPodAutoscaler is the way","timestamp":"1675393920.0","poster":"zerg0","comment_id":"796675"},{"content":"Selected Answer: C\nc is good","upvote_count":"1","comment_id":"776881","poster":"tdotcat","timestamp":"1673804400.0"},{"timestamp":"1672061820.0","comment_id":"757478","comments":[{"content":"A: Creating a second GKE cluster for the batch workloads only and allocating the 200 original nodes across both clusters would not necessarily help to reduce costs, as the total number of nodes in the clusters would remain the same.\n\nB: Configuring CPU and memory limits on the namespaces in the cluster and configuring all Pods to have CPU and memory limits may help to reduce costs, but it is not sufficient on its own. You should also use HPA and node auto scaling to ensure that the cluster is properly sized based on the actual resource usage.\n\nD: Changing the node pool to use preemptible VMs may help to reduce costs, but it is not sufficient on its own. Preemptible VMs can be terminated at any time, which may not be suitable for all workloads. You should also use HPA and node auto scaling to ensure that the cluster is properly sized based on the actual resource usage.","poster":"omermahgoub","upvote_count":"3","comment_id":"757479","timestamp":"1672061820.0"}],"upvote_count":"5","poster":"omermahgoub","content":"The correct answer is C: Configure a HorizontalPodAutoscaler for all stateless workloads and for all compatible stateful workloads. Configure the cluster to use node auto scaling.\n\nOne way to reduce the cost of a Google Kubernetes Engine (GKE) cluster without compromising availability is to use horizontal pod autoscalers (HPA) and node auto scaling.\n\nHPA allows you to automatically scale the number of Pods in a deployment based on the resource usage of the Pods. By configuring HPA for stateless workloads and for compatible stateful workloads, you can ensure that the number of Pods is automatically adjusted based on the actual resource usage, which can help to reduce costs.\n\nNode auto scaling allows you to automatically add or remove nodes from the node pool based on the resource usage of the cluster. By configuring node auto scaling, you can ensure that the cluster has the minimum number of nodes needed to meet the resource requirements of the workloads, which can also help to reduce costs."},{"content":"Selected Answer: C\nC is the correct answer as it doesn't involve major changes to the current Kubernetes configuration","upvote_count":"1","poster":"ale_brd_111","timestamp":"1671547800.0","comment_id":"751040"},{"comment_id":"723348","content":"Selected Answer: C\nC is ok","upvote_count":"1","timestamp":"1669023420.0","poster":"megumin"},{"comment_id":"695663","content":"C is correct","upvote_count":"1","timestamp":"1665862620.0","poster":"AzureDP900"},{"timestamp":"1662413280.0","comment_id":"660563","upvote_count":"1","poster":"rorz","content":"Selected Answer: C\nC is correc"},{"timestamp":"1662405360.0","content":"Selected Answer: C\nConfigure a HorizontalPodAutoscaler for all stateless workloads and for all compatible stateful workloads. Configure the cluster to use node auto scaling","poster":"aswani","comment_id":"660487","upvote_count":"2"},{"comment_id":"658300","upvote_count":"3","timestamp":"1662198600.0","content":"Option C is correct. Since, the company does not want to compromise availability of the application so, HPA is suitable option for autoscaling pods. Keeping the cost optimization in mind, nodes of the GKE cluster also needs to be autoscaled. Therefore, the correct option is, \nC. Configure a HorizontalPodAutoscaler for all stateless workloads and for all compatible stateful workloads. Configure the cluster to use node auto scaling.","poster":"spET_1024"}],"answer":"C","answers_community":["C (100%)"],"question_text":"Your company uses Google Kubernetes Engine (GKE) as a platform for all workloads. Your company has a single large GKE cluster that contains batch, stateful, and stateless workloads. The GKE cluster is configured with a single node pool with 200 nodes. Your company needs to reduce the cost of this cluster but does not want to compromise availability. What should you do?","choices":{"B":"Configure CPU and memory limits on the namespaces in the cluster. Configure all Pods to have a CPU and memory limits.","C":"Configure a HorizontalPodAutoscaler for all stateless workloads and for all compatible stateful workloads. Configure the cluster to use node auto scaling.","A":"Create a second GKE cluster for the batch workloads only. Allocate the 200 original nodes across both clusters.","D":"Change the node pool to use preemptible VMs."},"topic":"1","answer_description":"","timestamp":"2022-09-03 11:50:00","question_id":97,"unix_timestamp":1662198600,"answer_ET":"C","url":"https://www.examtopics.com/discussions/google/view/79736-exam-professional-cloud-architect-topic-1-question-186/","question_images":[],"isMC":true},{"id":"H5wJpCMc5cWMAGOQa47e","choices":{"D":"1. Activate billing export into BigQuery. 2. Perform a BigQuery query on the billing table to extract the information you need.","C":"1. Create a Cloud Logging sink to export BigQuery data access logs to Cloud Storage. 2. Develop a Dataflow pipeline to compute the cost of queries split by users.","B":"1. Create a Cloud Logging sink to export BigQuery data access logs to BigQuery. 2. Perform a BigQuery query on the generated table to extract the information you need.","A":"1. In the BigQuery dataset that contains all the tables to be queried, add a label for each user that can launch a query. 2. Open the Billing page of the project. 3. Select Reports. 4. Select BigQuery as the product and filter by the user you want to check."},"question_images":[],"url":"https://www.examtopics.com/discussions/google/view/80112-exam-professional-cloud-architect-topic-1-question-187/","topic":"1","discussion":[{"poster":"kuboraam","timestamp":"1662537720.0","content":"Selected Answer: B\nI choose B because of \"real-time\". Otherwise, D seems to be the most relevant and flexible.","comment_id":"662213","upvote_count":"14","comments":[{"comment_id":"797464","upvote_count":"9","poster":"VSMu","timestamp":"1675464780.0","content":"D also can be continuous https://cloud.google.com/billing/docs/how-to/export-data-bigquery#setup. I think D is the right answer."}]},{"content":"Selected Answer: B\nB is the correct answer https://cloud.google.com/blog/products/data-analytics/taking-a-practical-approach-to-bigquery-cost-monitoring\nA is incorrect as there is not billing page for a project, its billing account that handles all org billing.","upvote_count":"10","poster":"Mahmoud_E","comment_id":"699889","comments":[{"upvote_count":"1","timestamp":"1684263660.0","poster":"jlambdan","comment_id":"899508","content":"\"details about the query that was executed, like the SQL code, the job ID and, most important, the user who executed the query and the amount of data that was processed. With that information, you can compute the total cost of the query using a simple multiplication equation: cost per TB processed * numbers of TB processed\" means it will be an estimation, not the real numbers."},{"timestamp":"1692920400.0","content":"That blog page says it shows DAILY results, not real time...","upvote_count":"1","poster":"[Removed]","comment_id":"989550"}],"timestamp":"1666267860.0"},{"upvote_count":"1","poster":"gaufchamp","timestamp":"1743764220.0","content":"The correct answer is:\n\nD.\n1. Activate billing export into BigQuery.\n2. Perform a BigQuery query on the billing table to extract the information you need.\n\n✅ Why this is correct:\nBilling export to BigQuery allows you to analyze cloud usage and costs in detail.\n\nYou can break down costs by service (like BigQuery), project, labels, or even user (if billing data is detailed enough).\n\nThis method is real-time (or near real-time) and doesn’t require complex pipelines.","comment_id":"1463475"},{"comment_id":"1411840","timestamp":"1743278640.0","upvote_count":"1","poster":"OnoPa","content":"Selected Answer: D\nB does not provide cost!!!!"},{"poster":"frank_tsai_tech","comment_id":"1409851","upvote_count":"1","content":"Selected Answer: B\nThe correct answer is B. 1. Create a Cloud Logging sink to export BigQuery data access logs to BigQuery. 2. Perform a BigQuery query on the generated table to extract the information you need.\n\nExplanation\nBigQuery automatically logs data access events (which include query execution details) to Cloud Logging. By exporting these logs via a logging sink to BigQuery, you can write SQL queries to analyze:\n - The cost of each query.\n - Which users are executing the most costly queries.\n\nThis approach provides near–real-time insights into query performance and cost, aligning with the pay-per-use nature of BigQuery and allowing you to monitor and optimize expenses effectively.","timestamp":"1742863800.0"},{"comments":[{"comment_id":"1321163","content":"Read again... the question relates to BQ queries cost. The link states:\n\nUse the Detailed usage export to analyze costs at the resource level, and identify specific resources that might be driving up costs. The detailed export includes resource-level information for the following products:\n\nCompute Engine\nGoogle Kubernetes Engine (GKE)\nCloud Run functions\nCloud Run.... \nBig Query is not listed","upvote_count":"1","timestamp":"1733189820.0","poster":"desertlotus1211","comments":[{"timestamp":"1733190540.0","poster":"desertlotus1211","comments":[{"poster":"desertlotus1211","upvote_count":"1","comment_id":"1321169","timestamp":"1733190540.0","content":"Answer is D."}],"comment_id":"1321168","content":"please ignore my comment James. I miss read the answers and the usage of the export to BQ ;)","upvote_count":"1"}]}],"comment_id":"1273573","poster":"JamesKarianis","content":"Selected Answer: D\nGoogle recommends to export cloud billing data to bigquery to control cost in real-time: https://cloud.google.com/billing/docs/how-to/export-data-bigquery","timestamp":"1724775660.0","upvote_count":"1"},{"timestamp":"1712766840.0","comment_id":"1193163","upvote_count":"1","poster":"dija123","content":"Selected Answer: B\nB is correct!"},{"timestamp":"1709586420.0","upvote_count":"1","comment_id":"1166009","poster":"mesodan","content":"Selected Answer: B\nB is the correct option. Why not A: While the Billing page offers reports with user-level cost breakdowns, it doesn't provide real-time information or detailed query data. Why not D: Billing export can provide cost data in BigQuery, but it doesn't capture details about individual queries or users, making it insufficient for the specific needs of identifying costly queries and high-spending users."},{"content":"Selected Answer: B\nb -> real time","upvote_count":"1","timestamp":"1707040140.0","poster":"Pime13","comment_id":"1139962"},{"content":"Selected Answer: B\nI tend to agree with the GPT4 summary:\nIn summary, \n\nOption B is more focused on analyzing specific BigQuery usage patterns and costs down to the level of individual queries and users. It's better for real-time analysis of query activities. \n\nOption D, on the other hand, provides a broader overview of all costs associated with the Google Cloud project, which is beneficial for general cost management but less so for in-depth analysis of specific BigQuery queries and user activities. \n\nFor the specific need to discover the most costly queries and which users are responsible, Option B is more targeted and appropriate","poster":"ammonia_free","upvote_count":"1","comment_id":"1130578","timestamp":"1706098740.0"},{"timestamp":"1704793320.0","comments":[{"content":"My congrats! Did the exam results show you that you answered correctly to this particular question?","upvote_count":"2","comment_id":"1130569","poster":"ammonia_free","timestamp":"1706098440.0"}],"comment_id":"1117354","poster":"91d8ca7","upvote_count":"1","content":"I took PCA exam today and I passed PCA exam. In my test, I choose D on this question."},{"timestamp":"1701137460.0","upvote_count":"1","poster":"theBestStudent","comment_id":"1082073","content":"Selected Answer: B\nIt can't be better explained https://cloud.google.com/blog/products/data-analytics/taking-a-practical-approach-to-bigquery-cost-monitoring"},{"timestamp":"1695968940.0","poster":"someone2011","content":"B: because of\" https://cloud.google.com/blog/products/data-analytics/taking-a-practical-approach-to-bigquery-cost-monitoring\nAnd because https://cloud.google.com/billing/docs/how-to/export-data-bigquery#example-queries. is not mentioning anything about query per user.\nB","upvote_count":"2","comment_id":"1020536"},{"poster":"dsyouness","timestamp":"1695737400.0","comment_id":"1017845","comments":[{"poster":"someone2011","timestamp":"1695969360.0","content":"Yes it can:\nhttps://cloud.google.com/blog/products/data-analytics/taking-a-practical-approach-to-bigquery-cost-monitoring","comment_id":"1020539","upvote_count":"2"}],"upvote_count":"2","content":"Selected Answer: D\nB don't provide the cost"},{"poster":"daidaidai","timestamp":"1693704720.0","upvote_count":"2","content":"The answer is D. 1. Activate billing export into BigQuery. 2. Perform a BigQuery query on the billing table to extract the information you need.\n\nExplanation:\n\nA. This option is not correct because adding a label for each user in the BigQuery dataset will not allow you to monitor the cost of queries or find out which users spend the most.\n\nB. This option is not correct because BigQuery data access logs do not include billing information or query costs.\n\nC. This option is not correct because BigQuery data access logs stored in Cloud Storage do not include billing information or query costs, and developing a Dataflow pipeline to compute the cost of queries would be unnecessarily complex.\n\nD. This is the correct option because activating billing export into BigQuery will allow you to query the billing data in real-time to discover the most costly queries and which users spend the most.","comment_id":"997230"},{"upvote_count":"2","content":"Selected Answer: D\nD is the easiest","poster":"didek1986","timestamp":"1691230800.0","comment_id":"972896"},{"content":"Selected Answer: D\n1. Activate billing export into BigQuery. \n2. Perform a BigQuery query on the billing table to extract the information you need.","comment_id":"954455","poster":"gary_cooper","timestamp":"1689615480.0","upvote_count":"1"},{"timestamp":"1688654460.0","content":"both B and D do not meet the exact requirements while B does not give the cost D does not provide user level details. Answer A seems to better suited though complicated","upvote_count":"1","comment_id":"944759","poster":"Umesh09"},{"content":"Selected Answer: D\nFor me, most simple and also available in real time is D","comment_id":"928167","timestamp":"1687237980.0","poster":"red_panda","upvote_count":"1"},{"upvote_count":"3","poster":"JC0926","comment_id":"862724","content":"Selected Answer: B\nOption D might seem like a reasonable choice, but it doesn't provide real-time monitoring of queries, which is the requirement mentioned in the question.\n\nActivating billing export to BigQuery provides detailed billing information for your Google Cloud project. However, this method doesn't provide real-time insights into query costs and user expenditures, as billing data is typically updated once per day.\n\nOn the other hand, option B allows you to monitor queries in real-time by exporting BigQuery data access logs directly to another BigQuery table, enabling you to analyze the most costly queries and user expenses as they happen.","timestamp":"1680762240.0"},{"comment_id":"849767","upvote_count":"1","poster":"JC0926","content":"Selected Answer: D\nD\nExporting BigQuery data access logs to BigQuery and performing a query on the generated table would not provide information about the cost of BigQuery queries.","timestamp":"1679713080.0"},{"poster":"msmamrs","comment_id":"847142","content":"Selected Answer: D\nB won't give costs","timestamp":"1679494920.0","upvote_count":"2"},{"timestamp":"1678738200.0","upvote_count":"2","poster":"rr4444","comment_id":"838253","content":"Selected Answer: D\nD, cos B won't give costs"},{"content":"Selected Answer: B\nB seems backed by google's blog","upvote_count":"1","poster":"tdotcat","comment_id":"776896","timestamp":"1673805120.0"},{"timestamp":"1672062840.0","content":"The correct answer is D: 1. Activate billing export into BigQuery. 2. Perform a BigQuery query on the billing table to extract the information you need.\n\nTo monitor queries in real time and discover the most costly queries and which users spend the most in BigQuery, you can activate billing export into BigQuery. This will allow you to access detailed billing information for your Google Cloud project in real time, including information about the cost of BigQuery queries.\n\nOnce you have activated billing export into BigQuery, you can perform a BigQuery query on the billing table to extract the information you need. This will allow you to analyze the data and identify the most costly queries and the users who are spending the most.","comments":[{"timestamp":"1672063020.0","content":"1. Activate billing export into BigQuery: This will allow you to access a table in BigQuery that contains information about the cost of queries run in your project. To enable billing export, follow these steps:\n- Go to the Billing page of your Google Cloud project.\n- In the top left and select \"billing Export.\"\n- Follow the prompts to enable billing export and specify the BigQuery dataset and table where you want the data to be exported.\n- Perform a BigQuery query on the billing table: Once you have enabled billing export and the data has started flowing into the specified BigQuery table, you can run a query on the table to extract the information you need. For example, you could use a query like the following to get the total cost of queries split by user:\n```\nSELECT user, SUM(cost) as total_cost\nFROM `your-project.your-dataset.billing_export`\nGROUP BY user\nORDER BY total_cost DESC\n```","poster":"omermahgoub","comment_id":"757503","comments":[{"comment_id":"757505","timestamp":"1672063020.0","poster":"omermahgoub","upvote_count":"3","content":"This query will give you a list of users and the total cost of the queries they have run, sorted by the highest cost first. You can then use this information to identify the most costly queries and users and take steps to optimize them if necessary.\n\nOption A: Adding labels to the BigQuery dataset and filtering the Reports page in the project's Billing page would not provide real-time information about BigQuery query costs.\n\nOption B: Exporting BigQuery data access logs to BigQuery and performing a query on the generated table would not provide information about the cost of BigQuery queries.\n\nOption C: Exporting BigQuery data access logs to Cloud Storage and developing a Dataflow pipeline to compute the cost of queries split by users would not provide real-time information about BigQuery query costs."}],"upvote_count":"5"}],"comment_id":"757498","upvote_count":"6","poster":"omermahgoub"},{"upvote_count":"2","content":"Selected Answer: B\nB is ok","comment_id":"723350","timestamp":"1669023600.0","poster":"megumin"},{"upvote_count":"1","poster":"AzureDP900","content":"I will go with B","comment_id":"695665","timestamp":"1665862800.0"},{"poster":"ShadowLord","upvote_count":"3","timestamp":"1663633800.0","comment_id":"673717","content":"Selected Answer: A\nIt also need details on which user spend more time .. Seems like answer is A considering both aspects\nAs this need both query as well time spent by users"},{"upvote_count":"5","timestamp":"1662996060.0","poster":"zellck","comment_id":"667152","content":"Selected Answer: B\nB is my answer.\n\nhttps://cloud.google.com/blog/products/data-analytics/taking-a-practical-approach-to-bigquery-cost-monitoring","comments":[{"comment_id":"673716","comments":[{"comment_id":"673718","content":"After more detailed consideration .. B seems alright","upvote_count":"1","timestamp":"1663633980.0","poster":"ShadowLord"}],"timestamp":"1663633740.0","upvote_count":"2","content":"It also need details on which user spend more time .. Seems like answer is A considering both aspects","poster":"ShadowLord"}]},{"upvote_count":"2","content":"Selected Answer: B\nB because D is billing export which, I'm guessing, is not likely to contain user level data...A seems too far fetched and complicated even if technically feasible.","poster":"6721sora","comment_id":"664778","timestamp":"1662739620.0"},{"timestamp":"1662401940.0","upvote_count":"4","content":"Selected Answer: B\nI would choose B, since 1. need to get result in real time. 2 . The result need to be categorized by specific queries, and users. So only log can achieve that.","poster":"[Removed]","comment_id":"660437"},{"timestamp":"1662311460.0","content":"I think it is D","comment_id":"659415","upvote_count":"4","comments":[{"content":"https://cloud.google.com/billing/docs/how-to/export-data-bigquery","comment_id":"660934","comments":[{"timestamp":"1662740160.0","content":"more explanation for this question in the below link..https://helpcenter.itopia.com/en/articles/1306325-enable-billing-export-to-bigquery","comment_id":"664789","poster":"khadar","upvote_count":"2"}],"poster":"ilcasta73","upvote_count":"3","timestamp":"1662448680.0"}],"poster":"ilcasta73"}],"exam_id":4,"answer_images":[],"question_text":"Your company has a Google Cloud project that uses BigQuery for data warehousing on a pay-per-use basis. You want to monitor queries in real time to discover the most costly queries and which users spend the most. What should you do?","answer_ET":"B","isMC":true,"timestamp":"2022-09-04 19:11:00","unix_timestamp":1662311460,"question_id":98,"answers_community":["B (75%)","D (21%)","5%"],"answer_description":"","answer":"B"},{"id":"3bM5bmfITEb6Z1VleeKp","choices":{"D":"1. Create an additional instance in vpc-a. 2. Create an additional instance in vpc-b. 3. Install OpenVPN in newly created instances. 4. Configure a VPN tunnel between vpc-a and vpc-b with the help of OpenVPN.","B":"Set up a VPN between vpc-a and vpc-b using Cloud VPN.","C":"Configure IAP TCP forwarding on the instance in vpc-b, and then launch the following gcloud command from one of the instances in vpc-a gcloud: gcloud compute start-iap-tunnel INSTANCE_NAME_IN_VPC_8 22 \\ --local-host-port=localhost:22","A":"Set up a network peering between vpc-a and vpc-b."},"question_id":99,"exam_id":4,"topic":"1","question_text":"Your company and one of its partners each have a Google Cloud project in separate organizations. Your company's project (prj-a) runs in Virtual Private Cloud\n(vpc-a). The partner's project (prj-b) runs in vpc-b. There are two instances running on vpc-a and one instance running on vpc-b. Subnets defined in both VPCs are not overlapping. You need to ensure that all instances communicate with each other via internal IPs, minimizing latency and maximizing throughput. What should you do?","answer_description":"","answer":"A","answer_ET":"A","discussion":[{"comment_id":"667143","upvote_count":"13","timestamp":"1678641240.0","poster":"zellck","content":"Selected Answer: A\ndefinitely A.\n\nhttps://cloud.google.com/vpc/docs/vpc-peering\nGoogle Cloud VPC Network Peering allows internal IP address connectivity across two Virtual Private Cloud (VPC) networks regardless of whether they belong to the same project or the same organization."},{"timestamp":"1730286900.0","content":"Selected Answer: A\nVPC peering: https://cloud.google.com/vpc/docs/vpc-peering\nPeered VPC networks can be in the same project, different projects of the same organization, or different projects of different organizations.\n\nIPv4 subnet routes in peered VPC networks can't overlap","poster":"Gino17m","upvote_count":"3","comment_id":"1204444"},{"content":"Selected Answer: A\nA is ok!","upvote_count":"1","comment_id":"1127205","timestamp":"1721465040.0","poster":"OrangeTiger"},{"content":"Selected Answer: A\nSince it's mentioned that the subnets do not overlap, A is the best way to go. \nIf the subnets overlapped, you would go with B. \nhttps://cloud.google.com/vpc/docs/vpc-peering#interaction-subnet-subnet","upvote_count":"2","poster":"[Removed]","comment_id":"1109628","timestamp":"1719738360.0"},{"timestamp":"1684653300.0","poster":"megumin","upvote_count":"2","comment_id":"723331","content":"Selected Answer: A\nA is ok"},{"timestamp":"1684424340.0","upvote_count":"2","comment_id":"721460","poster":"jake_edman","content":"Selected Answer: A\nClearly A as the IPs do not overlap"},{"comment_id":"699901","content":"Selected Answer: A\nA is the correct answer as per https://cloud.google.com/vpc/docs/vpc-peering","upvote_count":"2","poster":"Mahmoud_E","timestamp":"1681993380.0"},{"content":"A is right answer","poster":"AzureDP900","upvote_count":"2","comment_id":"695667","timestamp":"1681587660.0"},{"timestamp":"1678385340.0","upvote_count":"2","comment_id":"664782","content":"Selected Answer: A\nClearly A","poster":"6721sora"},{"poster":"rorz","timestamp":"1678059900.0","upvote_count":"1","comment_id":"660566","content":"Selected Answer: A\nA - VPC peering should be good"},{"content":"Selected Answer: A\nIt should be A","comment_id":"660492","upvote_count":"1","poster":"aswani","timestamp":"1678051260.0"},{"poster":"kiappy81","timestamp":"1678021920.0","upvote_count":"1","content":"Selected Answer: A\nnetwork peering is fine","comment_id":"660086"},{"timestamp":"1677957060.0","upvote_count":"1","poster":"ilcasta73","comment_id":"659416","content":"It should be A"},{"timestamp":"1677926100.0","content":"Selected Answer: A\npeering is better as both orgs are in GCP","comment_id":"659089","poster":"rhage_56","upvote_count":"1"}],"answer_images":[],"timestamp":"2022-09-04 10:35:00","question_images":[],"answers_community":["A (100%)"],"url":"https://www.examtopics.com/discussions/google/view/80000-exam-professional-cloud-architect-topic-1-question-188/","isMC":true,"unix_timestamp":1662280500},{"id":"uoUHm3q1svTAKQsmLQTU","answer_images":[],"unix_timestamp":1662376320,"discussion":[{"comment_id":"836609","timestamp":"1726091460.0","poster":"namesgeo","upvote_count":"8","content":"Object Versioning is a feature that allows you to store multiple versions of an object in Cloud Storage. Hence, answer should be B"},{"poster":"megumin","content":"Selected Answer: B\nB is ok","comment_id":"723340","timestamp":"1716276240.0","upvote_count":"1"},{"content":"Most definitely B","poster":"samsonakala","comment_id":"707421","timestamp":"1714418400.0","upvote_count":"1"},{"timestamp":"1713616080.0","comment_id":"699906","content":"Selected Answer: B\nB is the correct answer as per https://cloud.google.com/storage/docs/object-versioning","upvote_count":"1","poster":"Mahmoud_E"},{"upvote_count":"2","comment_id":"695670","poster":"AzureDP900","content":"B - Object versioning","timestamp":"1713210180.0"},{"comment_id":"667389","upvote_count":"4","content":"Selected Answer: B\nObject versioning, super important to be able to rollback in case of any deletion.","timestamp":"1710277500.0","poster":"alexandercamachop"},{"timestamp":"1709644320.0","comments":[{"content":"I too got this question in 10-09-22 exam with similar option and result is pass","timestamp":"1710060240.0","comment_id":"665161","upvote_count":"7","poster":"khadar"}],"comment_id":"660089","poster":"kiappy81","content":"Selected Answer: B\nhttps://cloud.google.com/storage/docs/object-versioning","upvote_count":"3"}],"answer_ET":"B","question_id":100,"topic":"1","answer_description":"","isMC":true,"question_images":[],"choices":{"C":"Object change notification","A":"Bucket Lock","B":"Object Versioning","D":"Object Lifecycle Management"},"exam_id":4,"url":"https://www.examtopics.com/discussions/google/view/80304-exam-professional-cloud-architect-topic-1-question-189/","answers_community":["B (100%)"],"timestamp":"2022-09-05 13:12:00","answer":"B","question_text":"You want to store critical business information in Cloud Storage buckets. The information is regularly changed, but previous versions need to be referenced on a regular basis. You want to ensure that there is a record of all changes to any information in these buckets. You want to ensure that accidental edits or deletions can be easily rolled back. Which feature should you enable?"}],"exam":{"id":4,"isImplemented":true,"lastUpdated":"11 Apr 2025","isMCOnly":false,"provider":"Google","isBeta":false,"numberOfQuestions":279,"name":"Professional Cloud Architect"},"currentPage":20},"__N_SSP":true}