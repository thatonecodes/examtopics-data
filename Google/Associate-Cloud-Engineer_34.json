{"pageProps":{"questions":[{"id":"0oSWVtMUnJaQZPpOqT2f","answer":"A","topic":"1","answers_community":["A (63%)","B (38%)"],"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/129835-exam-associate-cloud-engineer-topic-1-question-248/","answer_images":[],"choices":{"C":"Create a retention policy on the storage bucket of 30 days, and lock the bucket by using a retention policy lock.","A":"Create an object lifecycle on the storage bucket to change the storage class to Archive Storage for objects with an age over 30 days.","D":"Enable object versioning on the storage bucket and add lifecycle rules to expire non-current versions after 30 days.","B":"Create a cron job in Cloud Scheduler to call a Cloud Functions instance every day to delete files older than 30 days."},"unix_timestamp":1703915520,"answer_ET":"A","exam_id":1,"discussion":[{"timestamp":"1742951400.0","upvote_count":"1","poster":"Phat","content":"Selected Answer: A\nIt seems we need to select ones with cheapest cost instead of deleting them.","comment_id":"1410217"},{"poster":"kapara","upvote_count":"2","content":"Selected Answer: A\nThe question is badly worded - because they didn't tell us what they intend to do with the files afterwards. \nI choose A anyway, because with lifecycle management you can delete files after 30 days, and using other services in this situation is really unnecessary..","comment_id":"1339126","timestamp":"1736593620.0"},{"timestamp":"1727959680.0","upvote_count":"2","content":"Selected Answer: B\nIt is cheaper to delete the files as there is no requirement to keep them.","comment_id":"1292789","poster":"denno22"},{"timestamp":"1727763000.0","upvote_count":"3","content":"Selected Answer: A\nThis is the same logic as Microsoft:\nwhen you have 2 options: one needs to pay and the other is free, choose the one with fee.\nThat is the right answer.","comment_id":"1291822","poster":"louisaok"},{"poster":"master9","upvote_count":"3","timestamp":"1726187580.0","content":"Selected Answer: A\nCloud Storage lifecycle management, you can automatically transition objects between storage classes based on certain conditions, such as age. Since your application only requires access to files created in the last 30 days, you can set a lifecycle rule to move files that are older than 30 days to Archive Storage, which offers the lowest storage costs but is designed for infrequent access.","comment_id":"1282904"},{"timestamp":"1725517620.0","upvote_count":"2","comment_id":"1278730","content":"Selected Answer: A\nThis option utilizes Cloud Storage's built-in object lifecycle management feature, which can automatically transition files older than 30 days to Archive Storage, thereby saving storage costs without requiring manual management. In comparison, option B is feasible but more complex and does not align with best practices.","poster":"klayhung"},{"comment_id":"1271011","content":"Selected Answer: B\nB is correct","poster":"caminosdk","upvote_count":"2","timestamp":"1724379060.0"},{"upvote_count":"1","comment_id":"1267904","content":"A\nA is Correct because it suggests changing the storage class to Archive Storage for objects with an age of over 30 days through a lifecycle rule on the storage bucket. This is a cost-effective solution because Google Cloud Storage offers different storage classes with varying costs. The \"Archive Storage\" class is designed for infrequently accessed data and comes at a lower cost compared to the standard storage class. Using a lifecycle rule to transition objects older than 30 days to the Archive Storage class helps save costs by utilizing a more cost-efficient storage class for older data.","poster":"rajeevpt","timestamp":"1723945500.0"},{"content":"Selected Answer: A\nAnother classic annoyingly vague question, but I would have to go with A because 'normally' you would keep files for longer than 30 days. If it is ok to delete, then B","poster":"flummoxed_individual","comment_id":"1253729","upvote_count":"1","timestamp":"1721746260.0"},{"timestamp":"1721328240.0","poster":"unprogram","upvote_count":"4","comment_id":"1250625","content":"I would say A even though it doesn't mention that the files are still needed. As keeping archived version/ backups is best practise in IT generally. If the question explicitly mentioned that backups were taken using some other method or that old versions were no longer required then B would be the correct answer."},{"content":"Selected Answer: B\nAt question not to talk about any analytics or critical data for audit and so on. You need only save cost, so B is answer.","poster":"BuenaCloudDE","timestamp":"1721326380.0","upvote_count":"1","comment_id":"1250617"},{"comment_id":"1250615","poster":"BuenaCloudDE","upvote_count":"2","timestamp":"1721326080.0","content":"Ridiculous question."},{"poster":"BuenaCloudDE","timestamp":"1721253960.0","upvote_count":"1","comment_id":"1250010","content":"Selected Answer: B\nB- Deleting the files means you no longer have to pay for storing them"},{"content":"Selected Answer: B\nThere is no requirement listed to keep the files. Deletion is the best option.","poster":"Alscoran","upvote_count":"1","comment_id":"1189939","timestamp":"1712323680.0"},{"poster":"ezzap90","content":"Selected Answer: B\nB or D are correct in my opinion as A does not delete unused files (archive storage is not as cheap as deleting). C only sets a retention policy (https://cloud.google.com/storage/docs/bucket-lock) which means you can only delete files over 30 days but it does not enable automatic deletion of old files. Object Versioning preserves deleted objects as versioned, noncurrent objects that remain accessible in your bucket until explicitly removed (https://cloud.google.com/storage/docs/object-versioning).","timestamp":"1711448340.0","upvote_count":"2","comment_id":"1183199"},{"poster":"TanTran04","content":"Selected Answer: A\nBest choice is A","timestamp":"1711165800.0","upvote_count":"1","comment_id":"1180519"},{"content":"Selected Answer: A\nYour application stores files on Cloud Storage by using the Standard Storage class. The application only requires access to files created in the last 30 days. You want to automatically save costs on files that are no longer accessed by the application. What should you do?\n\nA. Create an object lifecycle on the storage bucket to change the storage class to Archive Storage for objects with an age over 30 days.\nB. Create a cron job in Cloud Scheduler to call a Cloud Functions instance every day to delete files older than 30 days.\nC. Create a retention policy on the storage bucket of 30 days, and lock the bucket by using a retention policy lock.\nD. Enable object versioning on the storage bucket and add lifecycle rules to expire non-current versions after 30 days.\n\nA provides a simple, automated, and cost-effective solution for your scenario.","comments":[{"comment_id":"1213825","poster":"ccpmad","content":"A is not cost-effective solution, so no longer accessed files persist instead of eliminate them","upvote_count":"3","timestamp":"1716127140.0"}],"poster":"mshamia","comment_id":"1178102","upvote_count":"1","timestamp":"1710927300.0"},{"upvote_count":"4","content":"Shouldn't we delete files that are over 30 days old because they are unnecessary?","comments":[{"comments":[{"comment_id":"1165418","poster":"kuracpalac","timestamp":"1709539860.0","upvote_count":"2","content":"I don't know the answer TBH, but the Q doesn't say anything about retaining files. It does ask to lower cost as much as possible, so deleting them would be cheaper, right?"}],"poster":"kaustubh19","timestamp":"1707389040.0","comment_id":"1144362","content":"Options B and C involve deleting files, which may not be suitable if you need to retain the files for compliance or historical purposes.","upvote_count":"2"}],"poster":"sinh","comment_id":"1123236","timestamp":"1705313400.0"},{"upvote_count":"1","timestamp":"1705255680.0","content":"Option A, use of object lifecycle","comment_id":"1122745","comments":[{"poster":"ccpmad","comment_id":"1213824","upvote_count":"2","content":"save costs on files that are no longer accessed by the application; DELETION","timestamp":"1716127080.0"}],"poster":"JB28"},{"timestamp":"1704454320.0","poster":"Gocool28","upvote_count":"1","content":"A thaan da correct","comment_id":"1114454"},{"poster":"KelvinToo","timestamp":"1704045240.0","content":"Selected Answer: A\nChatGPT says option A.\ncreating an object lifecycle policy to transition objects older than 30 days to the Archive Storage class is the recommended solution for automatically managing costs and storage of files in Cloud Storage.","comment_id":"1110764","upvote_count":"1"},{"poster":"shiowbah","comment_id":"1109445","content":"A. Create an object lifecycle on the storage bucket to change the storage class to Archive Storage for objects with an age over 30 days.","upvote_count":"1","timestamp":"1703915520.0"}],"question_id":166,"isMC":true,"timestamp":"2023-12-30 06:52:00","question_images":[],"question_text":"Your application stores files on Cloud Storage by using the Standard Storage class. The application only requires access to files created in the last 30 days. You want to automatically save costs on files that are no longer accessed by the application. What should you do?"},{"id":"XqXiBaz4xPK6OXwGhe52","answer_ET":"B","answer_images":[],"topic":"1","answers_community":["B (88%)","12%"],"answer":"B","unix_timestamp":1704045120,"question_id":167,"exam_id":1,"question_images":[],"url":"https://www.examtopics.com/discussions/google/view/130018-exam-associate-cloud-engineer-topic-1-question-249/","isMC":true,"discussion":[{"poster":"Urbanvzla","upvote_count":"8","comment_id":"1119081","content":"Selected Answer: B\nHorizontal Pod Autoscaler (HPA): It automatically scales the number of pods in a deployment, replica set, or stateful set based on observed CPU utilization (or, with custom metrics support, on some other application-provided metrics). This helps maintain availability by ensuring that your application has the necessary number of pods to handle the workload.\n\nVertical Pod Autoscaler (VPA): It automatically adjusts the CPU and memory reservations for your pods to help \"right size\" your applications. This is particularly useful when you're unsure of the resource requirements. VPA makes recommendations for the appropriate CPU and memory settings based on usage patterns, which can be very effective for cost optimization.\n\nThis combination ensures that your workload is both horizontally scalable (to handle changes in demand) and vertically optimized (to use resources efficiently), following Google-recommended practices for Kubernetes workloads.","timestamp":"1720641480.0"},{"poster":"PiperMe","upvote_count":"2","timestamp":"1725401520.0","content":"Selected Answer: B\nI believe B is the best choice: HPA ensures availability by scaling the number of pods based on metrics (like CPU utilization). The VPA analyzes resource utilization and provides recommendations for CPU and memory requests and limits. This is key for right-sizing your pods for optimal cost efficiency.","comment_id":"1165173"},{"upvote_count":"1","comment_id":"1115708","comments":[{"content":"VPA is not recommended as per Google requirements, so that answer must be wrong.","upvote_count":"1","timestamp":"1725431100.0","comment_id":"1165446","poster":"kuracpalac"}],"content":"Selected Answer: D\nD is the correct answer","timestamp":"1720339920.0","poster":"Lakshvenkat"},{"upvote_count":"3","poster":"Cynthia2023","comments":[{"upvote_count":"1","poster":"Cynthia2023","timestamp":"1719957360.0","content":"A. Cluster Autoscaler for Suggestions: While the Cluster Autoscaler is useful for scaling the number of nodes in the cluster, it doesn’t provide recommendations on the CPU and memory requirements for individual pods.\n\nC. VPA for Availability, Cluster Autoscaler for Suggestions: VPA can automatically adjust pod sizes, but using it for ensuring availability might lead to frequent and potentially disruptive pod restarts. The Cluster Autoscaler is again more about node-level scaling rather than providing pod resource recommendations.\n\nD. VPA for Availability, HPA for Suggestions: This configuration isn't ideal as VPA's primary function isn't about maintaining high availability but rather about optimizing resource allocation. HPA, on the other hand, is specifically designed for scaling the number of pods based on load, which is directly related to availability.","comment_id":"1112361"}],"timestamp":"1719957300.0","content":"Selected Answer: B\nHorizontal Pod Autoscaler (HPA): HPA automatically adjusts the number of pods in a deployment based on observed CPU utilization (or other select metrics). This is crucial for maintaining the availability of your workload, especially if the workload experiences varying levels of traffic or load. HPA ensures that there are enough pods to handle the load, scaling out (adding more pods) when demand is high and scaling in (removing pods) when demand is low.\n\n\nVertical Pod Autoscaler (VPA) Recommendations: VPA automatically adjusts the CPU and memory reservations for pods in a deployment. It can operate in a mode where it only provides recommendations (without automatically applying them), which is useful for understanding the resource needs of your workload. VPA recommendations can guide you in setting appropriate CPU and memory limits based on the observed usage of your workload.","comment_id":"1112360"},{"upvote_count":"2","content":"Selected Answer: B\nAns is B\nB. Configure the Horizontal Pod Autoscaler for availability, and configure the Vertical Pod Autoscaler recommendations for suggestions.\n\nThis approach allows you to manage the number of pods based on the workload (HPA) and get optimal CPU and memory settings for each pod (VPA), which is in line with Google-recommended practices for managing Kubernetes workloads with uncertain resource requirements. This combination ensures that your workload can function consistently in varying situations by automatically adjusting both the quantity of pods and the resources each pod is allocated.","timestamp":"1719870240.0","poster":"kaby1987","comment_id":"1111501"},{"comment_id":"1110763","poster":"KelvinToo","timestamp":"1719762720.0","comments":[{"comment_id":"1165171","poster":"PiperMe","timestamp":"1725401220.0","content":"Chat strikes again. Option B provides a more tailored and Google-recommended approach given the uncertainty about the workload's resource needs. It prioritizes establishing an efficient baseline with VPA before relying on HPA for scaling.","upvote_count":"1"}],"upvote_count":"1","content":"Selected Answer: D\nChatGPT says option D,\nBy configuring VPA for resource recommendations based on actual usage patterns and HPA for scaling pod instances based on demand, you can ensure that your workload is both cost-effective and capable of adapting to varying resource requirements, all while following Google-recommended practices for Kubernetes workloads."}],"choices":{"D":"Configure the Vertical Pod Autoscaler recommendations for availability, and configure the Horizontal Pod Autoscaler for suggestions.","A":"Configure the Horizontal Pod Autoscaler for availability, and configure the cluster autoscaler for suggestions.","B":"Configure the Horizontal Pod Autoscaler for availability, and configure the Vertical Pod Autoscaler recommendations for suggestions.","C":"Configure the Vertical Pod Autoscaler recommendations for availability, and configure the Cluster autoscaler for suggestions."},"question_text":"Your manager asks you to deploy a workload to a Kubernetes cluster. You are not sure of the workload's resource requirements or how the requirements might vary depending on usage patterns, external dependencies, or other factors. You need a solution that makes cost-effective recommendations regarding CPU and memory requirements, and allows the workload to function consistently in any situation. You want to follow Google-recommended practices. What should you do?","timestamp":"2023-12-31 18:52:00","answer_description":""},{"id":"VwO6F9vZ069OhSVjy5CQ","choices":{"D":"Add the auditor user accounts to two new custom IAM roles.","C":"Add the auditor user accounts to the 'logging.viewer' and 'bigQuery.dataViewer' predefined IAM roles.","B":"Add the auditors group to two new custom IAM roles.","A":"Add the auditors group to the 'logging.viewer' and 'bigQuery.dataViewer' predefined IAM roles."},"question_images":[],"discussion":[{"comments":[{"comment_id":"126178","upvote_count":"5","timestamp":"1593870900.0","content":"You assume Auditors Group = External Auditors only. Auditors Group may contain both Internal and External Auditors.","poster":"droogie","comments":[{"upvote_count":"14","content":"The question literally says - External Auditors","poster":"robor97","comment_id":"232215","timestamp":"1606853220.0"},{"poster":"adeice","content":"I can create External group and Internal group Auditors","comment_id":"318984","timestamp":"1616577960.0","upvote_count":"2"}]}],"comment_id":"64442","timestamp":"1584300000.0","upvote_count":"83","poster":"coldpar","content":"Correct is A.\nAs per google best practices it is recommended to use predefined roles and create groups to control access to multiple users with same responsibility"},{"upvote_count":"47","timestamp":"1595865540.0","comment_id":"145115","content":"Correct answer is A as per:\nhttps://cloud.google.com/iam/docs/job-functions/auditing#scenario_external_auditors","comments":[{"upvote_count":"1","timestamp":"1697565180.0","comment_id":"1046250","content":"very useful\n\nThe organization creates a Google group for these external auditors and adds the current auditor to the group. This group is monitored and is typically granted access to the dashboard application","poster":"ArtistS"}],"poster":"JavierCorrea"},{"upvote_count":"1","comment_id":"1363844","content":"Selected Answer: A\nCorrect is A.","timestamp":"1740896880.0","poster":"tabnaz"},{"comment_id":"1354383","content":"Selected Answer: A\nBased on best practices, A group should be created, and both auditors should be added and predefined 'logging.viewer' and 'bigQuery.dataViewer roles will be granted.","upvote_count":"1","poster":"Cloudmoh","timestamp":"1739180940.0"},{"comment_id":"1287852","content":"Selected Answer: A\nI will go A","upvote_count":"1","timestamp":"1727022960.0","poster":"Enamfrancis"},{"content":"Selected Answer: A\nThe organization creates a Google group for these external auditors and adds the current auditor to the group. This group is monitored and is typically granted access to the dashboard application.\nhttps://cloud.google.com/iam/docs/job-functions/auditing#scenario_external_auditors","upvote_count":"1","timestamp":"1722388380.0","poster":"Seleth","comment_id":"1258477"},{"comment_id":"1254325","upvote_count":"1","content":"Correct Answer is A. you should add a role to the group of users instead of adding particular users in IAM","poster":"garg.vnay","timestamp":"1721817240.0"},{"comment_id":"1227848","upvote_count":"1","content":"Selected Answer: A\ncorrect answer is A","poster":"andreiboaghe95","timestamp":"1718021640.0"},{"content":"Selected Answer: A\nauditors group","upvote_count":"1","poster":"sinh","comment_id":"1124131","timestamp":"1705403760.0"},{"poster":"sinh","comment_id":"1104497","upvote_count":"2","timestamp":"1703405820.0","content":"https://cloud.google.com/iam/docs/job-functions/auditing?hl=ja#scenario_external_auditors"},{"timestamp":"1700655660.0","content":"Selected Answer: A\nA\nCreate a group with the auditors, grant 'logging.viewer' and 'bigQuery.dataViewer roles to the group on a table / view with the required data.","comment_id":"1077361","upvote_count":"2","poster":"thewalker"},{"content":"Selected Answer: A\nAS per Google best practices the roles should be assigned to a group & not to individual users","upvote_count":"2","poster":"gsmasad","timestamp":"1698830580.0","comment_id":"1059493"},{"poster":"ArtistS","content":"A is correct. 1st you should know this is a exam. Google recommend xxx means you should choose group first.","timestamp":"1697565120.0","comment_id":"1046248","upvote_count":"4"},{"content":"Selected Answer: A\nThe correct answer is A. \n\nThis option follows Google-recommended practices, because it allows you to grant auditors access to view audit logs without granting them access to other resources in your project.\n\nThe other options are not as good:\n\n* Option B is not as good, because it requires you to create two new custom IAM roles. This can be complex and time-consuming.\n* Option C is not as good, because it grants auditors access to all audit logs in your project, including audit logs for resources that they do not need access to.\n* Option D is not as good, because it grants auditors access to all data in your BigQuery datasets, including data that they do not need access to.","upvote_count":"5","timestamp":"1695838980.0","poster":"YourCloudGuru","comment_id":"1019147"},{"poster":"Captain1212","comment_id":"996210","content":"Selected Answer: A\nGoogle Recommended Practice A is the correct Answer add the users in the group then grant them the access","upvote_count":"3","timestamp":"1693583700.0"},{"poster":"sthapit","content":"C\nOption A, which suggests adding the auditors group to predefined roles, might not be as appropriate as using individual auditor user accounts. It's generally a best practice to assign permissions to specific users rather than groups, as it provides better granularity and control over access.","comment_id":"976731","timestamp":"1691590800.0","upvote_count":"1"},{"timestamp":"1689850920.0","content":"Selected Answer: A\nCorrect answer is A","comment_id":"957426","upvote_count":"2","poster":"ExamsFR"}],"topic":"1","url":"https://www.examtopics.com/discussions/google/view/16694-exam-associate-cloud-engineer-topic-1-question-25-discussion/","exam_id":1,"isMC":true,"answer_ET":"A","answer_description":"","answer":"A","answers_community":["A (78%)","B (16%)","6%"],"unix_timestamp":1584300000,"timestamp":"2020-03-15 20:20:00","question_text":"You need to configure IAM access audit logging in BigQuery for external auditors. You want to follow Google-recommended practices. What should you do?","question_id":168,"answer_images":[]},{"id":"3dsHBEDUajxkm5HkjUMz","answer_description":"","topic":"1","question_text":"You need to migrate invoice documents stored on-premises to Cloud Storage. The documents have the following storage requirements:\n\n• Documents must be kept for five years.\n• Up to five revisions of the same invoice document must be stored, to allow for corrections.\n• Documents older than 365 days should be moved to lower cost storage tiers.\n\nYou want to follow Google-recommended practices to minimize your operational and development costs. What should you do?","exam_id":1,"answer":"D","discussion":[{"content":"Selected Answer: D\n- Object Versioning: Enabling object versioning on the Cloud Storage bucket allows you to store up to five revisions of the same invoice document. This satisfies the requirement for keeping multiple versions of each document for corrections.\n\n- Lifecycle Conditions: Google Cloud Storage allows you to define lifecycle conditions for objects within a bucket. These conditions can automatically change the storage class of objects when they meet certain criteria, such as age. After 365 days, you can automatically move documents to lower-cost storage classes like Nearline, Coldline, or Archive, which reduces storage costs while still retaining the data.\n\n- Version Management and Deletion: The lifecycle rules can also be configured to manage the number of object versions retained and to delete old versions or objects, ensuring compliance with the five-year retention requirement.","timestamp":"1719958200.0","upvote_count":"7","comments":[{"upvote_count":"5","comment_id":"1112370","content":"Why B is not correct:\nLifecycle rules in Google Cloud Storage can be used to manage the deletion of old versions of objects. However, it's important to note that lifecycle rules alone do not set the number of versions to keep; they can only delete versions based on age or other criteria.\nLifecycle rules in Google Cloud Storage do not have a direct setting to limit the number of object versions (like keeping only the last five versions). Object versioning in Google Cloud Storage keeps all versions of an object until they are explicitly deleted (either manually or through lifecycle rules).","poster":"Cynthia2023","timestamp":"1719958260.0"}],"comment_id":"1112369","poster":"Cynthia2023"},{"timestamp":"1727056560.0","upvote_count":"1","poster":"TanTran04","comment_id":"1180524","content":"Selected Answer: D\nFollow D"},{"timestamp":"1725579960.0","comments":[{"upvote_count":"5","timestamp":"1725748860.0","comment_id":"1168465","poster":"sukouto","content":"Upon further reading, it seems Retention Policies and Object Versioning are mutually exclusive, meaning B cannot cover the second requirement. https://cloud.google.com/storage/docs/object-versioning\n\nIt's not explicitly stated, but it is implied that Object Versioning can prevent total deletion (i.e. deleting a live version of an object moves it to a non-current version).\n\nI guess the answer will have to be D"}],"comment_id":"1166861","content":"Selected Answer: B\nI believe the answer is actually B, and D won't cut it.\n\nD does not address the need \"Documents must be kept for five years.\" A retention policy is required, otherwise someone can just delete a document.\n\nSomeone else suggested that you can't set object versioning with life cycle rules, but that's not quite true. https://cloud.google.com/storage/docs/lifecycle\n\nYou can, but it does require object versioning to be enabled... So none of these answers are ideal, but I think the omission of setting a retention policy explicitly misses the first requirement stated.","upvote_count":"2","poster":"sukouto"},{"content":"The correct answer is **D. Enable object versioning on the bucket, use lifecycle conditions to change the storage class of the objects, set the number of versions, and delete old files**.\n\nHere's why:\n- **Object versioning** allows you to keep up to five revisions of the same invoice document.\n- **Lifecycle conditions** can be used to automatically change the storage class of objects older than 365 days to a lower-cost storage tier.\n- You can also set the number of versions to keep and automatically delete old files, which helps to manage storage costs effectively. \n\nThis approach aligns with Google-recommended practices and helps to minimize operational and development costs.","upvote_count":"4","poster":"JB28","comment_id":"1122748","timestamp":"1720973700.0"},{"comments":[{"comment_id":"1353510","upvote_count":"1","poster":"1826c27","timestamp":"1739032500.0","content":"why not B mr Obvious? \"Documents must be kept for five years.\" - how does D cover this requirement?"}],"timestamp":"1720173120.0","content":"Obvious answer is D","upvote_count":"1","poster":"Gocool28","comment_id":"1114464"},{"poster":"KelvinToo","comments":[{"timestamp":"1739032440.0","upvote_count":"1","poster":"1826c27","content":"Per my grandma, Option A aligns with Google-recommended practices for managing objects in Cloud Storage, including versioning, lifecycle management, and cost optimization, making it the best choice for the given scenario.","comment_id":"1353508"}],"content":"Selected Answer: D\nPer ChatGPT, Option D aligns with Google-recommended practices for managing objects in Cloud Storage, including versioning, lifecycle management, and cost optimization, making it the best choice for the given scenario.","upvote_count":"1","comment_id":"1110755","timestamp":"1719762120.0"},{"comment_id":"1109446","content":"D. Enable object versioning on the bucket, use lifecycle conditions to change the storage class of the objects, set the number of versions, and delete old files.","timestamp":"1719719580.0","upvote_count":"1","poster":"shiowbah"}],"timestamp":"2023-12-30 06:53:00","choices":{"D":"Enable object versioning on the bucket, use lifecycle conditions to change the storage class of the objects, set the number of versions, and delete old files.","B":"Enable retention policies on the bucket, use lifecycle rules to change the storage classes of the objects, set the number of versions, and delete old files.","A":"Enable retention policies on the bucket, and use Cloud Scheduler to invoke a Cloud Function to move or delete your documents based on their metadata.","C":"Enable object versioning on the bucket, and use Cloud Scheduler to invoke a Cloud Functions instance to move or delete your documents based on their metadata."},"question_images":[],"question_id":169,"answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/129836-exam-associate-cloud-engineer-topic-1-question-250/","answers_community":["D (82%)","B (18%)"],"isMC":true,"answer_ET":"D","unix_timestamp":1703915580},{"id":"96Qqlr5qox15ZMYdwDkx","url":"https://www.examtopics.com/discussions/google/view/129837-exam-associate-cloud-engineer-topic-1-question-251/","unix_timestamp":1703915580,"answers_community":["D (92%)","8%"],"question_text":"You installed the Google Cloud CLI on your workstation and set the proxy configuration. However, you are worried that your proxy credentials will be recorded in the gcloud CLI logs. You want to prevent your proxy credential from being logged. What should you do?","answer_ET":"D","discussion":[{"content":"Answer is D. See Google Docs: https://cloud.google.com/sdk/docs/proxy-settings\n\nAlternatively, to avoid having the proxy credentials recorded in any logs (such as shell history or gcloud CLI logs) or in the gcloud CLI configuration file, you can set the properties using environment variables.\n\nexport CLOUDSDK_PROXY_USERNAME [USERNAME]\n\nexport CLOUDSDK_PROXY_PASSWORD [PASSWORD]","comment_id":"1161717","upvote_count":"5","poster":"ChemaKSado","timestamp":"1709132400.0"},{"poster":"denno22","timestamp":"1728034140.0","comment_id":"1293084","upvote_count":"3","comments":[{"content":"export CLOUDSDK_PROXY_USERNAME [USERNAME]\nexport CLOUDSDK_PROXY_PASSWORD [PASSWORD]","comment_id":"1295931","poster":"denno22","timestamp":"1728636540.0","upvote_count":"2"}],"content":"Selected Answer: D\nAlternatively, to avoid having the proxy credentials recorded in any logs (such as shell history or gcloud CLI logs) or in the gcloud CLI configuration file, you can set the properties using environment variables\n\nhttps://cloud.google.com/sdk/docs/proxy-settings"},{"upvote_count":"1","timestamp":"1709511900.0","content":"Selected Answer: D\nOption D is the best answer.","comment_id":"1165181","poster":"PiperMe"},{"timestamp":"1708186860.0","upvote_count":"1","content":"Selected Answer: D\nTo avoid shell CLI history, logs or conf. file. Google recommends to set it on env. variables related with no storing of those values. Option D.","comment_id":"1152682","poster":"leoalvarezh"},{"upvote_count":"1","content":"Selected Answer: D\nhttps://cloud.google.com/sdk/docs/proxy-settings","poster":"sinh","comment_id":"1113619","timestamp":"1704367140.0"},{"poster":"Cynthia2023","comment_id":"1112373","comments":[{"comment_id":"1112374","upvote_count":"2","content":"A. Using gcloud config set for Username and Password: This approach directly enters the credentials into the gcloud CLI configuration, which could potentially be logged or exposed in the configuration file.\nB. Encoding Credentials and Custom CA Certs File: This option suggests a method that isn't directly related to setting proxy credentials. The core/custom_ca_certs_file is used for specifying a custom CA (Certificate Authority) certificate file, not for proxy credentials.\nC. Using the Configuration File: Modifying the configuration file to include proxy credentials might expose them in plain text within the file, which could be a security risk. It's generally safer to use environment variables for this purpose.","poster":"Cynthia2023","timestamp":"1704241020.0"}],"timestamp":"1704241020.0","upvote_count":"4","content":"Selected Answer: D\n- Using Environment Variables: By setting the proxy credentials as environment variables (CLOUDSDK_PROXY_USERNAME and CLOUDSDK_PROXY_PASSWORD), you avoid having to enter them directly into the CLI tool where they might be logged. Environment variables are a common way to securely pass sensitive information like credentials.\n\n- No Logging of Credentials: The gcloud CLI typically does not log environment variables, so your credentials should be safe from being recorded in the CLI logs.\n\n- Ease of Use: Setting environment variables is straightforward and does not require modifying configuration files or encoding credentials."},{"upvote_count":"1","content":"Selected Answer: D\nAns is D","comment_id":"1111499","timestamp":"1704152580.0","poster":"kaby1987"},{"timestamp":"1704044460.0","comment_id":"1110753","content":"Selected Answer: C\nPer ChatGPT, Option C is the most appropriate choice for securely providing proxy credentials to the gcloud CLI tool without risking exposure in logs or other outputs.","comments":[{"comment_id":"1165179","upvote_count":"1","content":"I think this test may have rocked you due to ChatGPT. Storing credentials in the config file increases the risk of exposure if the file is compromised. It's the same problem as A. The answer is D.","poster":"PiperMe","timestamp":"1709511840.0"}],"poster":"KelvinToo","upvote_count":"1"},{"content":"D. Set the CLOUDSDK_PROXY_USERNAME and CLOUDSDK_PROXY_PASSWORD properties by using environment variables in your command line tool.","poster":"shiowbah","timestamp":"1703915580.0","comment_id":"1109447","upvote_count":"1"}],"answer":"D","answer_description":"","timestamp":"2023-12-30 06:53:00","question_id":170,"exam_id":1,"question_images":[],"choices":{"C":"Provide values for CLOUDSDK_PROXY_USERNAME and CLOUDSDK_PROXY_PASSWORD in the gcloud CLI tool configuration file.","D":"Set the CLOUDSDK_PROXY_USERNAME and CLOUDSDK_PROXY_PASSWORD properties by using environment variables in your command line tool.","B":"Encode username and password in sha256 encoding, and save in to a text file. Use filename as a value in the gcloud config set core/custom_ca_certs_file command.","A":"Configure username and password by using gcloud config set proxy/username and gcloud config set proxy/password commands."},"isMC":true,"topic":"1","answer_images":[]}],"exam":{"provider":"Google","id":1,"isBeta":false,"lastUpdated":"11 Apr 2025","isImplemented":true,"isMCOnly":true,"name":"Associate Cloud Engineer","numberOfQuestions":285},"currentPage":34},"__N_SSP":true}