{"pageProps":{"questions":[{"id":"hcjXVuggbI4gyl3NSDZ2","answer_description":"","discussion":[{"comment_id":"839022","content":"D.\nAnytime you see words like “develop” or “manually” be suspicious given this is cloud and everything is supposed to be automated and point-and-click easy. Eliminate A. Federated queries are SQL queries initiated FROM BigQuery to Cloud Spanner or Cloud SQL databases. So B doesn’t make sense. The Database Migration Service does not support BigQuery as a destination database engine. Eliminate C. That leaves D. From Google’s documentation, “Datastream is a serverless and easy-to-use Change Data Capture (CDC) and replication service that allows you to synchronize data across heterogeneous databases, storage systems, and applications reliably and with minimal latency. Datastream supports change data streaming from Oracle and MySQL databases to Google Cloud Storage (GCS). The service offers streamlined integration with Dataflow templates to power up to date materialized views in BigQuery for analytics, replicate their databases into Cloud SQL or Cloud Spanner for database synchronization, or leverage the event stream directly from GCS to realize event-driven architectures.”","upvote_count":"12","poster":"dynamic_dba","timestamp":"1694700780.0"},{"content":"Selected Answer: D\nAgree with D","poster":"dija123","timestamp":"1732790640.0","comment_id":"1220082","upvote_count":"1"},{"upvote_count":"1","timestamp":"1714900020.0","comment_id":"1062796","content":"Selected Answer: D\nD because we need replication: \nAs a data analyst, you can query data in Cloud SQL from BigQuery using federated queries (...) Alternatively, to replicate data into BigQuery, you can also use Cloud Data Fusion or Datastream.\nDatastream is a serverless and easy-to-use change data capture (CDC) and replication service that lets you synchronize data reliably, and with minimal latency.\n\nDatastream provides seamless replication of data from operational databases into BigQuery.\n\nreference: https://cloud.google.com/bigquery/docs/cloud-sql-federated-queries https://cloud.google.com/datastream/docs/overview","poster":"Pime13"},{"comment_id":"981694","upvote_count":"2","poster":"pico","content":"You can not connect Datastream directly to Cloud SQL with an internal IP without using A compute instance where a SQL Proxy is deployed to bridge the traffic between Datastream and Cloud SQL. Because connecting to Cloud SQL from Datastream is not possible\n\nhttps://github.com/rocketechgroup/mysql-to-bq-datastream","timestamp":"1708009800.0"},{"upvote_count":"1","timestamp":"1706802360.0","content":"B\n\nAs a data analyst, you can query data in Cloud SQL from BigQuery using federated queries.\n\nBigQuery Cloud SQL federation enables BigQuery to query data residing in Cloud SQL in real time, without copying or moving data. Query federation supports both MySQL (2nd generation) and PostgreSQL instances in Cloud SQL.\n\nAlternatively, to replicate data into BigQuery, you can also use Cloud Data Fusion or Datastream. For more about using Cloud Data Fusion, see Replicating data from MySQL to BigQuery.\n\nhttps://cloud.google.com/bigquery/docs/cloud-sql-federated-queries","poster":"pico","comment_id":"969117"},{"timestamp":"1687715160.0","content":"Selected Answer: D\nI'll go for D","poster":"chelbsik","comment_id":"755964","upvote_count":"2"},{"upvote_count":"3","poster":"pk349","timestamp":"1687639380.0","comments":[{"content":"https://cloud.google.com/datastream-for-bigquery","timestamp":"1688147220.0","poster":"sp57","comment_id":"762910","comments":[{"upvote_count":"1","content":"Linked article confirms datastream + dataflow is a \"thing\". Provides additional customization vs just datastream.","poster":"sp57","timestamp":"1688147400.0","comment_id":"762912"}],"upvote_count":"1"}],"comment_id":"755220","content":"D: Use Datastream to capture *** changes, and use Dataflow to write those changes to BigQuery.\nDataflow is a fully managed streaming analytics service that minimizes latency, processing time, and cost through autoscaling and batch processing. Dataflow is a managed service for executing a wide variety of data processing patterns. The documentation on this site shows you how to deploy your batch and streaming data processing pipelines using Dataflow, including directions for using service features."}],"unix_timestamp":1671921780,"answer_images":[],"question_text":"Your company is using Cloud SQL for MySQL with an internal (private) IP address and wants to replicate some tables into BigQuery in near-real time for analytics and machine learning. You need to ensure that replication is fast and reliable and uses Google-managed services. What should you do?","timestamp":"2022-12-24 23:43:00","exam_id":5,"question_images":[],"isMC":true,"answer":"D","answer_ET":"D","topic":"1","choices":{"B":"Use Cloud SQL federated queries.","C":"Use Database Migration Service to replicate tables into BigQuery.","D":"Use Datastream to capture changes, and use Dataflow to write those changes to BigQuery.","A":"Develop a custom data replication service to send data into BigQuery."},"question_id":116,"answers_community":["D (100%)"],"url":"https://www.examtopics.com/discussions/google/view/92718-exam-professional-cloud-database-engineer-topic-1-question/"},{"id":"lAHBpKdYKXpmdmiNrnL3","exam_id":5,"url":"https://www.examtopics.com/discussions/google/view/92717-exam-professional-cloud-database-engineer-topic-1-question/","isMC":true,"timestamp":"2022-12-24 23:42:00","answer_ET":"A","answer":"A","choices":{"C":"Use Pub/Sub to synchronize the changes from the application to Cloud Spanner.","B":"Use Memorystore for Memcached.","A":"Use Firestore and ensure that the PersistenceEnabled option is set to true.","D":"Use Table.read with the exactStaleness option to perform a read of rows in Cloud Spanner."},"answers_community":["A (100%)"],"unix_timestamp":1671921720,"answer_description":"","question_images":[],"answer_images":[],"topic":"1","question_text":"You are designing a physician portal app in Node.js. This application will be used in hospitals and clinics that might have intermittent internet connectivity. If a connectivity failure occurs, the app should be able to query the cached data. You need to ensure that the application has scalability, strong consistency, and multi-region replication. What should you do?","question_id":117,"discussion":[{"timestamp":"1687639320.0","comment_id":"755218","upvote_count":"6","content":"A: Use Firestore and ensure that the PersistenceEnabled ***** option is set to true.\nCloud Firestore is a NoSQL document database that lets you easily store, sync, and query data for your mobile and web apps - at global scale.\nTo use offline persistence, you don't need to make any changes to the code that you use to access Cloud Firestore data. With offline persistence enabled, the Cloud Firestore client library automatically manages online and offline data access and synchronizes local data when the device is back online.","poster":"pk349"},{"content":"Selected Answer: A\nIntermittment connectivity -> Firestore","timestamp":"1687715220.0","comment_id":"755966","upvote_count":"5","poster":"chelbsik"},{"comment_id":"1347096","poster":"887ad17","upvote_count":"1","content":"Selected Answer: A\nFirestore meets all the requirements for your physician portal app:\n\nOffline Access: With the PersistenceEnabled option, Firestore caches data locally, allowing the app to function during intermittent internet connectivity.\nScalability: Firestore is designed to scale automatically with your application's needs.\nStrong Consistency: Firestore provides strong consistency for read and write operations.\nMulti-Region Replication: Firestore supports multi-region replication, ensuring high availability and resilience.","timestamp":"1737922980.0"},{"poster":"gcp_k","content":"The answer is: C since the use case is for Strong consistency and multi-regional replication.","upvote_count":"1","timestamp":"1731182340.0","comment_id":"1209026"},{"comment_id":"1062798","poster":"Pime13","upvote_count":"1","timestamp":"1714900140.0","content":"Selected Answer: A\nA:\n\nFirestore supports offline data persistence. This feature caches a copy of the Firestore data that your app is actively using, so your app can access the data when the device is offline. You can write, read, listen to, and query the cached data. When the device comes back online, Firestore synchronizes any local changes made by your app to the Firestore backend.\nhttps://cloud.google.com/firestore/docs/manage-data/enable-offline"},{"content":"correct answer is C! Firestore may not provide the same level of strong consistency and multi-region replication as Cloud Spanner. :)","comment_id":"1005099","timestamp":"1710189420.0","upvote_count":"1","poster":"learnazureportal"},{"comment_id":"839035","upvote_count":"4","content":"A.\nAn app with intermitted internet access meaning it does not HAVE to sync with the live data source must mean Firestore in Datastore mode. The only option that mentions Firestore is A.\nhttps://firebase.google.com/docs/firestore/manage-data/enable-offline","poster":"dynamic_dba","timestamp":"1694701500.0"}]},{"id":"jnbvmlRvZCPxzMjNSKv1","answer_description":"","question_images":[],"answers_community":["A (93%)","7%"],"answer_ET":"A","topic":"1","unix_timestamp":1671921660,"question_id":118,"url":"https://www.examtopics.com/discussions/google/view/92716-exam-professional-cloud-database-engineer-topic-1-question/","answer_images":[],"discussion":[{"timestamp":"1687715460.0","comments":[{"timestamp":"1691343240.0","poster":"FunkyB","comment_id":"800186","upvote_count":"1","content":"Correct Answer: A\n\"Deny maintenance period. A block of days in which Cloud SQL does not schedule maintenance. Deny maintenance periods can be up to 90 days long. \"\n\nhttps://cloud.google.com/sql/docs/mysql/maintenance"}],"poster":"chelbsik","content":"Selected Answer: A\nA loogs good. B is impossible because you can't hold maintenance for more than a 90 days period","comment_id":"755967","upvote_count":"10"},{"timestamp":"1694701860.0","poster":"dynamic_dba","comment_id":"839037","upvote_count":"5","content":"A.\nThis is a maintenance deny question, so eliminate C and D. Maintenance deny periods can be set that last up to 90 days. That eliminates B since the period mentioned would be 107 days. B is also eliminated because the maintenance windows are an hour. You can't set a maintenance window in Cloud SQL to be 5 hours. A is the answer."},{"timestamp":"1730287680.0","upvote_count":"1","poster":"Pime13","comment_id":"1204452","content":"Selected Answer: A\nhttps://cloud.google.com/sql/docs/mysql/maintenance\nYou can have a deny maintenance period even if you don't have maintenance windows configured for your instance. Deny maintenance periods can span from 1 to 90 days."},{"comment_id":"1070313","timestamp":"1715679720.0","upvote_count":"1","content":"Selected Answer: D\nOption D allows you to create a Cloud Scheduler job to start maintenance at 12 AM on Sundays and pause the Cloud Scheduler job between November 1 and January 15. This will allow you to perform routine maintenance on Sundays, when traffic is slow, and avoid maintenance during the holiday shopping season, when traffic is highest. This option is the most complete and secure, as it allows you to ensure that your production system is available 24/7 during the holidays.","poster":"[Removed]"},{"poster":"Pime13","upvote_count":"2","timestamp":"1714900320.0","content":"Selected Answer: A\nA:\n\nhttps://cloud.google.com/sql/docs/mysql/maintenance\nhttps://cloud.google.com/sql/docs/mysql/maintenance#management","comment_id":"1062799"},{"content":"correct answer is D. Create a Cloud Scheduler job to start maintenance at 12 AM on Sundays. Pause the Cloud Scheduler job between November 1 and January 15.","poster":"learnazureportal","timestamp":"1710189600.0","upvote_count":"1","comment_id":"1005100"},{"upvote_count":"2","comment_id":"755216","content":"A: Define a maintenance window on Sundays between 12 AM and *****1 AM, and deny maintenance periods between November 1 and ***** January 15.\nCloud SQL offers you the ability to configure maintenance updates through a set of maintenance settings.\nYou can configure maintenance to be scheduled at times when brief downtime causes the lowest impact to your applications. For each Cloud SQL instance, you can configure the following:\n• Maintenance window. The day of the week and the hour in which Cloud SQL schedules maintenance. Maintenance windows last for ***** one hour. Learn how to configure a maintenance window.","poster":"pk349","timestamp":"1687639260.0"}],"isMC":true,"answer":"A","question_text":"You manage a production MySQL database running on Cloud SQL at a retail company. You perform routine maintenance on Sunday at midnight when traffic is slow, but you want to skip routine maintenance during the year-end holiday shopping season. You need to ensure that your production system is available 24/7 during the holidays. What should you do?","exam_id":5,"timestamp":"2022-12-24 23:41:00","choices":{"D":"Create a Cloud Scheduler job to start maintenance at 12 AM on Sundays. Pause the Cloud Scheduler job between November 1 and January 15.","A":"Define a maintenance window on Sundays between 12 AM and 1 AM, and deny maintenance periods between November 1 and January 15.","C":"Build a Cloud Composer job to start a maintenance window on Sundays between 12 AM and 1AM, and deny maintenance periods between November 1 and January 15.","B":"Define a maintenance window on Sundays between 12 AM and 5 AM, and deny maintenance periods between November 1 and February 15."}},{"id":"e6KC8OnZ37Q4AgaU2Vt9","answer":"AC","question_text":"You want to migrate an on-premises 100 TB Microsoft SQL Server database to Google Cloud over a 1 Gbps network link. You have 48 hours allowed downtime to migrate this database. What should you do? (Choose two.)","topic":"1","answer_ET":"AC","isMC":true,"question_id":119,"discussion":[{"comment_id":"755214","content":"AUse a change data capture ***** (CDC) migration strategy.\n\n BMove the physical database servers from on-premises to Google Cloud.\n\n CKeep the network bandwidth at 1 Gbps, and then perform an offline data migration.\n1 Gbps = 1*24*60*60/8 = 10,800 GB = 10 TB per 24 hrs So, 20 TB per 48 hrs\n DIncrease the network bandwidth to 2 Gbps, and then perform an offline data migration.\n So, 40 TB per 48 hrs\n EIncrease the network bandwidth to 10 ***** Gbps, and then perform an offline data migration.\n So, 200 TB per 48 hrs\nAnswer: A E","upvote_count":"6","timestamp":"1671921540.0","comments":[{"timestamp":"1695200040.0","upvote_count":"1","poster":"ticky","comment_id":"1012097","content":"what is 8 in formal?","comments":[{"upvote_count":"1","timestamp":"1713524280.0","comment_id":"1198547","content":"it's number of bits in 1 byte","poster":"Alex0707"}]}],"poster":"pk349"},{"comment_id":"954357","content":"Selected Answer: AC\nA, C\nThe question doesn't tell you that you have to move the database within 48 hours from right now - it says you're allow 48 hours of downtime. i.e. You can schedule it. C, D, and E all require an offline transfer, so increasing bandwidth wouldn't help. 1 Gbps should be enough to handle CDC updates within 48 hours after the initial offline transfer is complete, and it minimizes cost.","poster":"DBAgain","timestamp":"1689610260.0","upvote_count":"6"},{"comment_id":"1334195","upvote_count":"1","content":"Selected Answer: AE\nA--> CDC makes sense. The only other alternative is E to cover 200TB in 48hrs.","poster":"Ral17","timestamp":"1735570500.0"},{"upvote_count":"1","timestamp":"1727483580.0","comment_id":"1290448","content":"Selected Answer: AE\nAt 1 Gbps, transferring 100 TB of data could take much longer than 48 hours, so this is not a viable option.","poster":"kitechen"},{"poster":"omermahgoub","content":"Selected Answer: AC\nA and C","timestamp":"1713859440.0","upvote_count":"2","comment_id":"1200548"},{"upvote_count":"2","content":"Selected Answer: AE\n1 minutes = 60 seconds\n1 hour = 60 minutes = 60 * 60 = 3_600 seconds\n1 day = 24 hours = 24 * 3_600 = 86_400 seconds\n48 hours = 2 days = 86_400 * 2 = 172_800 seconds\n\nNetwork link 1 Gbps, 48 hours transfer 172_800 Gb = 172_800/8 GB = 21_600 GB < 100 TB\n\nNetwork link 2 Gbps, 48 hours transfer 43_200 GB < 100 TB\n\nNetwork link 10 Gbps, 48 hours transfer 210_600 GB = 205.66 TB > 100 TB.","comment_id":"1177331","poster":"james2033","timestamp":"1710853320.0"},{"comment_id":"1129509","upvote_count":"2","timestamp":"1706013240.0","poster":"PKookNN","content":"Selected Answer: AC\nI think the question tricks you into thinking that you need more bandwidth. For offline, you don't need bandwidth. So answers is A and C."},{"content":"Selected Answer: AE\nOption A & E is correct:\nThe questions and the option tell us two scenarios:\nFirst Scenario: Option A: CDC\nSetup CDC and replication. Within 48 hours this can be completed using 1 GB network bandwidth. Database is required restart to change configuration and the rest is online during replication \nSecond scenario: offline migration (offline database migration)\nDatabase need to be shutdown and take a full backup and transfer it to GCP.\nAt this points we can eliminate option C and D because option C transfer 100 TB over 1 GB network bandwidth required 12 days (above allowed downtime 48 hours) and option D transfer 100 TB over network bandwidth 4,74 days (113,78 hours) which is above allowed downtime 48 hours","poster":"whoosh","upvote_count":"2","comment_id":"1101104","timestamp":"1703034660.0"},{"comment_id":"1062800","timestamp":"1699182840.0","content":"Selected Answer: AC\nA and C for me","poster":"Pime13","upvote_count":"5"},{"poster":"learnazureportal","comment_id":"1005109","upvote_count":"1","content":"Correct answer is A & D.","timestamp":"1694458200.0"},{"poster":"dynamic_dba","upvote_count":"5","comment_id":"839050","content":"A, E.\nAccording to Google’s data transfer chart, 100 TB across a 1 Gbps link would take 12 days. I don’t think you can physically move your own equipment into a Google DC. Eliminate B. Increasing the bandwidth to 2 Gbps and doing an offline migration anyway wouldn’t help. The Google Transfer Appliance comes in 7 TB, 40 TB and 300 TB sizes. However, the turnaround time (Google ships the appliance to you, you load it with data, you ship it back, Google unloads it to a Cloud Storage bucket) is approximately 3 weeks. Eliminate C and D. That leaves A and E. A 10 Gbps link would transfer 100 TB in 30 hours. That leaves 18 hours for a CDC solution to sync the data.\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#online_versus_offline_transfer","timestamp":"1678812660.0","comments":[{"comment_id":"954355","poster":"DBAgain","timestamp":"1689610140.0","content":"E doesn't allow for an online transfer - It's still offline. Doesn't make sense to me to add bandwidth if you're not going to use it.","upvote_count":"2"}]},{"content":"Selected Answer: AE\nIdeally, you can transfer 1 GB in eight seconds over a 1 Gbps network. \nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets","comments":[{"comment_id":"832952","timestamp":"1678279560.0","poster":"Nirca","upvote_count":"1","content":"Can A+E go together?"}],"timestamp":"1677712140.0","poster":"PrtkKA","upvote_count":"4","comment_id":"826323"},{"comment_id":"755985","timestamp":"1671998940.0","poster":"chelbsik","upvote_count":"2","content":"Selected Answer: AE\nI confirm pk349 calculations"}],"answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/92715-exam-professional-cloud-database-engineer-topic-1-question/","choices":{"D":"Increase the network bandwidth to 2 Gbps, and then perform an offline data migration.","C":"Keep the network bandwidth at 1 Gbps, and then perform an offline data migration.","A":"Use a change data capture (CDC) migration strategy.","B":"Move the physical database servers from on-premises to Google Cloud.","E":"Increase the network bandwidth to 10 Gbps, and then perform an offline data migration."},"unix_timestamp":1671921540,"timestamp":"2022-12-24 23:39:00","answer_description":"","question_images":[],"exam_id":5,"answers_community":["AC (56%)","AE (44%)"]},{"id":"hcak3HbEcli2RHro93oi","answers_community":["B (100%)"],"answer":"B","question_text":"You need to provision several hundred Cloud SQL for MySQL instances for multiple project teams over a one-week period. You must ensure that all instances adhere to company standards such as instance naming conventions, database flags, and tags. What should you do?","answer_ET":"B","exam_id":5,"answer_images":[],"choices":{"D":"Create clones from a template Cloud SQL instance.","A":"Automate instance creation by writing a Dataflow job.","C":"Create the instances using the Google Cloud Console UI.","B":"Automate instance creation by setting up Terraform scripts."},"question_id":120,"topic":"1","timestamp":"2022-12-24 23:37:00","url":"https://www.examtopics.com/discussions/google/view/92714-exam-professional-cloud-database-engineer-topic-1-question/","answer_description":"","isMC":true,"discussion":[{"comment_id":"1062803","content":"Selected Answer: B\nB use terraform","poster":"Pime13","upvote_count":"1","timestamp":"1730805300.0"},{"timestamp":"1710435300.0","poster":"dynamic_dba","content":"B.\nThe scale of the problem suggests automation. Eliminate C and D. Dataflow concerns data, not instances. The only answer which makes sense is to use Terraform.","comment_id":"839052","upvote_count":"2"},{"content":"Selected Answer: B\nB will work","poster":"Ayush9596","upvote_count":"2","timestamp":"1708616220.0","comment_id":"817984"},{"upvote_count":"3","poster":"chelbsik","content":"Selected Answer: B\nUse Terraform, it will allow you to template same resource","comment_id":"755986","timestamp":"1703535000.0"},{"poster":"pk349","upvote_count":"1","timestamp":"1703457420.0","content":"B: Automate instance creation by setting up ***** Terraform scripts.","comment_id":"755212"}],"question_images":[],"unix_timestamp":1671921420}],"exam":{"provider":"Google","isBeta":false,"name":"Professional Cloud Database Engineer","numberOfQuestions":132,"lastUpdated":"11 Apr 2025","isImplemented":true,"isMCOnly":true,"id":5},"currentPage":24},"__N_SSP":true}