{"pageProps":{"questions":[{"id":"utssHRxqAL4rgcc3aSBd","isMC":true,"answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/6462-exam-professional-cloud-architect-topic-1-question-90/","answer_ET":"B","answer":"B","answers_community":["B (100%)"],"question_images":[],"answer_description":"","exam_id":4,"unix_timestamp":1570795320,"timestamp":"2019-10-11 14:02:00","topic":"1","question_id":191,"choices":{"C":"Deploy the update in a new VPC, and use Google's global HTTP load balancing to split traffic between the update and current applications.","A":"Deploy the update using the Instance Group Updater to create a partial rollout, which allows for canary testing.","D":"Deploy the update as a new App Engine application, and use Google's global HTTP load balancing to split traffic between the new and current applications.","B":"Deploy the update as a new version in the App Engine application, and split traffic between the new and current versions."},"discussion":[{"content":"I think B is correct. Because GAE supports service version control and A/B test.\nIs my understanding correct?","poster":"KouShikyou","comment_id":"14753","timestamp":"1618142520.0","comments":[{"comment_id":"210560","poster":"kumarp6","timestamp":"1651411560.0","content":"Yes, B is correct","upvote_count":"5"},{"upvote_count":"4","poster":"nitinz","comment_id":"303788","timestamp":"1662333300.0","content":"Only B works."}],"upvote_count":"61"},{"upvote_count":"15","content":"Only one App Engine application can be created per Project.\nSo it's B.","poster":"ADVIT","comment_id":"50292","timestamp":"1628885100.0"},{"comment_id":"753137","timestamp":"1719041700.0","upvote_count":"11","comments":[{"content":"Other options, such as deploying the update in a new VPC or as a new App Engine application, are not recommended for testing updates with production traffic, as they can be more complex and may require additional steps to set up.","timestamp":"1719041700.0","upvote_count":"3","comment_id":"753139","poster":"omermahgoub"}],"content":"B. Deploy the update as a new version in the App Engine application, and split traffic between the new and current versions.\n\nTo test an update to an App Engine application with production traffic before replacing the current version, you can deploy the update as a new version in the App Engine application and split traffic between the new and current versions. This is known as a \"blue-green\" deployment, and it allows you to test the new version with a portion of production traffic while the current version is still serving the remainder of traffic.\n\nTo split traffic between the new and current versions, you can use the App Engine traffic splitting feature. This feature allows you to specify the percentage of traffic that should be sent to each version, and it can be used to gradually ramp up traffic to the new version over time. This allows you to test the new version with a small portion of traffic initially, and gradually increase the traffic as you become more confident in the update.","poster":"omermahgoub"},{"content":"Answer B : You can use traffic splitting to specify a percentage distribution of traffic across two or more of the versions within a service. Splitting traffic allows you to conduct A/B testing between your versions and provides control over the pace when rolling out features.\nTraffic splitting is applied to URLs that do not explicitly target a version. For example, the following URLs split traffic because they target all the available versions within the specified service:\nhttps://cloud.google.com/appengine/docs/standard/splitting-traffic","upvote_count":"3","timestamp":"1717692660.0","comment_id":"737140","poster":"TonytheTiger"},{"content":"Selected Answer: B\nB is ok","upvote_count":"2","timestamp":"1715321820.0","comment_id":"715034","poster":"megumin"},{"content":"B is right , Option D is just to confuse you.\nDeploy the update as a new version in the App Engine application, and split traffic between the new and current versions.","poster":"AzureDP900","timestamp":"1713281280.0","upvote_count":"2","comment_id":"696334"},{"comment_id":"642350","poster":"DrishaS4","upvote_count":"2","timestamp":"1707052800.0","content":"Selected Answer: B\nVersioning is supported in App Engine."},{"poster":"ghadxx","content":"Selected Answer: B\nVersioning is supported in App Engine.","comment_id":"542953","upvote_count":"2","timestamp":"1691478900.0"},{"comments":[{"timestamp":"1716293160.0","content":"No mate, only one app engine per project can be deployed, you can have multiple version on the same app tho. D is to confuse you. B is the only feasible answer in here.","upvote_count":"4","poster":"ale_brd_111","comment_id":"723581"}],"comment_id":"495879","content":"Go for D,\nThe option B don´t say with wich service will split the traffic.\nThe option D gives more datail and makes sense.","poster":"haroldbenites","upvote_count":"1","timestamp":"1686126420.0"},{"upvote_count":"1","comment_id":"489309","timestamp":"1685288100.0","content":"B is the correct answer","poster":"vincy2202"},{"timestamp":"1683622320.0","content":"A is not because \"Instance Group Updater \" is only for Computer Engine MIG","comment_id":"474742","upvote_count":"2","poster":"robotgeek"},{"timestamp":"1682785380.0","upvote_count":"6","poster":"MaxNRG","content":"B – Deploy the update as a new version in AppEngine app, and split traffic between the new and current versions.\nTraffic Splitting is feature of AppEngine for A/B testing. \nhttps://cloud.google.com/appengine/docs/standard/python/splitting-traffic","comment_id":"469866"},{"content":"B is correct. App Engine supports versioning.","comment_id":"466825","poster":"[Removed]","upvote_count":"1","timestamp":"1682312640.0"},{"timestamp":"1681026180.0","content":"B is correct... Canary Testing -> Traffic Splitting","poster":"unnikrisb","upvote_count":"1","comment_id":"459540"},{"content":"B. Deploy the update as a new version in the App Engine application, and split traffic between the new and current versions","comment_id":"361353","upvote_count":"3","poster":"victory108","timestamp":"1668867360.0"},{"poster":"un","content":"B is the answer","comment_id":"358617","timestamp":"1668604500.0","upvote_count":"1"},{"upvote_count":"3","poster":"getzsagar","timestamp":"1665206220.0","comment_id":"330908","content":"Answer - B \nConfigure how much traffic the version that you just deployed should receive.\n\nBy default, the initial version that you deploy to your App Engine application is automatically configured to receive 100% of traffic. However, all subsequent versions that you deploy to that same App Engine application must be manually configured, otherwise they receive no traffic.\n\nFor details about how to configure traffic for your versions, see Migrating and Splitting Traffic.\nhttps://cloud.google.com/appengine/docs/admin-api/migrating-splitting-traffic"},{"timestamp":"1664527920.0","content":"B is ok","poster":"lynx256","comment_id":"324885","upvote_count":"1"},{"content":"as per this B\nhttps://cloud.google.com/appengine/docs/standard/python/splitting-traffic","poster":"VenV","upvote_count":"1","timestamp":"1662902040.0","comment_id":"307975"},{"timestamp":"1659167520.0","content":"I go with B","upvote_count":"1","poster":"BobBui","comment_id":"279855"},{"poster":"bnlcnd","timestamp":"1659144360.0","comment_id":"279719","content":"B is working as people suggested.\nBut why D is not? It actually can work in a surgical way. The newly introduced global LB has to take the hostname of the existing one. Yes, we can make the change to allow that. But we have to change it back when we fully rollout the new version since the LB is not required any more. With this complication, B is best answer.","upvote_count":"1"},{"content":"ANS :B\nSee the link: https://cloud.google.com/appengine/docs/admin-api/deploying-apps","poster":"ahmedemad3","comment_id":"271875","upvote_count":"1","timestamp":"1658305920.0"},{"timestamp":"1656475080.0","comment_id":"254578","upvote_count":"1","content":"D is the correct answer ... you need to be able to split traffic between 2 app instances, and that would require spinning up a new instance","poster":"varushar","comments":[{"content":"problme with D it uses a Load Balancer, an extra component.","comment_id":"271703","timestamp":"1658276640.0","upvote_count":"1","poster":"jcamilodo"}]},{"upvote_count":"1","comment_id":"235465","poster":"0x24141","content":"B, https://cloud.google.com/appengine/docs/admin-api/migrating-splitting-traffic","timestamp":"1654405620.0"},{"comment_id":"232918","upvote_count":"1","poster":"Chulbul_Pandey","timestamp":"1654163040.0","content":"B is the way"},{"poster":"Bharathy","comment_id":"200090","content":"B is correct, as GAE provides support for Traffic Splitting ( By IP, Cookie and Other ) options which can be used to test the Production version with limited no of users.","timestamp":"1649968200.0","upvote_count":"3"},{"content":"D. I think it cannot deploy the update as a new verison in the same app engine application(option b), while it should deploy as a new application (taking the new version as a new application).","upvote_count":"1","poster":"passtest100","comment_id":"181396","timestamp":"1647588420.0"},{"timestamp":"1647465840.0","content":"Answer: B","upvote_count":"1","comment_id":"180548","poster":"AshokC"},{"content":"My answer is D. You are updating an App Engine application and you want to test the update with production traffic before replacing the current application version.\nA. Deploy the update using the Instance Group Updater: BUT YOU ARE UPDATING A APP ENGINE, NOT A INSTANCE GROUP.\nB. This option sounds like Canary Deployment, and requeriment is not progressive updating, is only test new version using production traffic \nC. This option is a mixed solution btween canary and I don't know exactly what.\nD. This option raises an Blue Green deployment that is exactly the requeriment in my opinion","comments":[{"comment_id":"299757","timestamp":"1661513400.0","upvote_count":"4","poster":"Alekshar","content":"Option D is indeed blue/green deployment, which means you will have to switch all your users to the new version at once (which is not the question's requirement)\n\nWhereas option B allows you to ensure everything works with a small part of the production traffic before moving all the users."}],"upvote_count":"1","comment_id":"170170","poster":"Ale1973","timestamp":"1646072100.0"},{"poster":"takataka","content":"Rolling Update is default in app engine. A/B is achieved through splitting in App engine.\nCanary Release is achieved through splitting in App Engine. \nAnswer: B \nNo doubt.","comment_id":"169770","timestamp":"1646030400.0","upvote_count":"2"},{"upvote_count":"1","poster":"wiqi","content":"B is correct.","comment_id":"165321","timestamp":"1645723680.0"},{"poster":"GooglecloudArchitect","timestamp":"1643335140.0","upvote_count":"3","content":"It should be B. Traffic splitting between multiple revision of the same app is possible.\nYou cannot deploy multiple \"apps\" in the same project. ... Your App Engine app can have many services and each service can have its own language/runtime and even be on different App Engine environments.","comment_id":"145334"},{"poster":"motty","upvote_count":"2","timestamp":"1640296620.0","comment_id":"117810","content":"traffic splitter is the normal way to go. B"},{"upvote_count":"2","comment_id":"117501","poster":"mlantonis","timestamp":"1640272920.0","content":"It is definitely B"},{"upvote_count":"2","timestamp":"1640250480.0","content":"B is correct- https://cloud.google.com/appengine/docs/admin-api/deploying-apps","poster":"akhilesh_pundir","comment_id":"117162"},{"upvote_count":"2","poster":"Tushant","content":"correct ans is B","comment_id":"109196","timestamp":"1639382820.0"},{"upvote_count":"2","content":"B for sure.\nOnly one App Engine application / per Project.","comment_id":"106691","poster":"gfhbox0083","timestamp":"1639138800.0"},{"timestamp":"1638652920.0","comment_id":"102582","content":"B is the correct answer","upvote_count":"3","poster":"Ziegler"},{"upvote_count":"3","content":"Answer should be B","poster":"jayaen","timestamp":"1638166260.0","comment_id":"98020"},{"upvote_count":"3","comment_id":"97518","timestamp":"1638108480.0","content":"Final Decision to go with Option B","poster":"AD2AD4"},{"content":"I go with B","poster":"anton_royce","upvote_count":"6","timestamp":"1633307040.0","comment_id":"70883"},{"poster":"sri007","content":"B is correct","timestamp":"1626629100.0","upvote_count":"9","comment_id":"40443"},{"comment_id":"31788","upvote_count":"3","content":"Can't understand why D is marked as a correct answer.","poster":"Arttu","timestamp":"1624369320.0"},{"upvote_count":"5","poster":"KouShikyou","timestamp":"1618317180.0","content":"https://cloud.google.com/appengine/docs/standard/python/splitting-traffic","comment_id":"14988"}],"question_text":"You have an App Engine application that needs to be updated. You want to test the update with production traffic before replacing the current application version.\nWhat should you do?"},{"id":"yffsx2Ix00SfYYBEMZjf","topic":"1","answer_ET":"A","exam_id":4,"isMC":true,"url":"https://www.examtopics.com/discussions/google/view/6817-exam-professional-cloud-architect-topic-1-question-91/","question_id":192,"discussion":[{"content":"Should be A, there is no implied deny egress but only implied allow egress\n\nhttps://cloud.google.com/vpc/docs/firewalls#default_firewall_rules\n\nEvery VPC network has two implied firewall rules. These rules exist, but are not shown in the Cloud Console:\n\nThe implied allow egress rule: An egress rule whose action is allow, destination is 0.0.0.0/0, and priority is the lowest possible (65535) lets any instance send traffic to any destination, except for traffic blocked by GCP. Outbound access may be restricted by a higher priority firewall rule. Internet access is allowed if no other firewall rules deny outbound traffic and if the instance has an external IP address or uses a NAT instance. Refer to Internet access requirements for more details.\n\nThe implied deny ingress rule: An ingress rule whose action is deny, source is 0.0.0.0/0, and priority is the lowest possible (65535) protects all instances by blocking incoming traffic to them. Incoming access may be allowed by a higher priority rule. Note that the default network includes some additional rules that override this one, allowing certain types of incoming traffic.","poster":"wk","comments":[{"poster":"nitinz","upvote_count":"2","comment_id":"303789","comments":[{"upvote_count":"1","poster":"zr79","content":"thank you","timestamp":"1681725240.0","comment_id":"697255"}],"timestamp":"1630797420.0","content":"It is A, rest all do not make sense. If you think of any other option then go back and read about firewalls. Seriously you are not ready for this exam."},{"upvote_count":"2","content":"B is correct...","poster":"kumarp6","comment_id":"210563","timestamp":"1619875620.0"},{"timestamp":"1621090380.0","upvote_count":"13","poster":"p4","content":"from a book:\n\"Firewall rules control network traffic by blocking or allowing traffic into (ingress) or out of (egress) a network. Two implied firewall rules are defined with VPCs: one blocks all incoming traffic, and the other allows all outgoing traffic. You can change this behavior\nVirtual Private Clouds 115\n116 Chapter 6 ■ Designing Networks\nby defining firewall rules with higher priority. Firewall rules have a priority specified by an integer from 0 to 65535, with 0 being the highest priority and 65535 being the lowest.\"\n\nso this confirms A","comment_id":"219817","comments":[{"content":"Good summary. To the point!","poster":"SSS987","timestamp":"1721185560.0","upvote_count":"1","comment_id":"1124690"}]}],"timestamp":"1587351120.0","upvote_count":"93","comment_id":"16152"},{"upvote_count":"10","timestamp":"1587403680.0","poster":"MeasService","content":"Agree Correct is A. There is no implied deny egress only deny ingress rule","comments":[{"poster":"MyPractice","upvote_count":"3","timestamp":"1593514500.0","comment_id":"33989","content":"Agree with A . only Implied allow egress rule (or) Implied deny ingress rule. \nThere is No \"Implied deny egress rule\" which rules out C & D"}],"comment_id":"16279"},{"content":"B. Create an egress rule with priority 100 to deny all traffic for all instances. Create another egress rule with priority 1000 to allow the Active Directory traffic for all instances.\n\nThis option creates a deny all rule with a lower priority and an allow rule with a higher priority. \n\nThis option will work as intended, as the Active Directory traffic will be allowed and all other outbound traffic will be blocked.","timestamp":"1706612640.0","poster":"ManishKS","upvote_count":"2","comment_id":"966979"},{"poster":"Emmarof","comment_id":"838516","timestamp":"1694653260.0","content":"The answer to this question is A.\n\nExplanation:\nTo enforce the requirement that all Compute Engine instances in your VPC should be able to connect to an Active Directory server on specific ports while blocking any other traffic emerging from instances, the following two egress rules should be created:\n\nCreate an egress rule with priority 1000 to deny all traffic for all instances.\nCreate another egress rule with priority 100 to allow the Active Directory traffic for all instances.\nIn this configuration, the rule that allows the AD traffic has a lower priority number than the rule that denies all other traffic. Therefore, this rule should be evaluated first.","upvote_count":"2"},{"poster":"Deb2293","content":"Selected Answer: A\nIt should be A.\nIt cannot be D as The Implied allow egress rule, with its action of “allow”, allows all traffic out to the 0.0. 0.0/0 destination, which basically means everywhere. The priority of the implied allow egress rule is the lowest possible, 65535. The implied deny ingress rule, with an action of “deny”, blocks all incoming connections.","timestamp":"1693879620.0","comment_id":"829615","upvote_count":"1"},{"upvote_count":"1","poster":"8d31d36","comment_id":"810087","timestamp":"1692139320.0","content":"The correct answer is B.\n\nTo enforce that all Compute Engine instances in a VPC can connect to an Active Directory server on specific ports while blocking any other traffic, you should create an egress rule with a high priority (lower numerical value) to deny all traffic from all instances, and another egress rule with a lower priority (higher numerical value) to allow traffic to the Active Directory server on the specific ports.\n\nOption B creates the necessary egress rules in the correct order: a deny-all rule with a high priority (100), followed by an allow rule for the Active Directory traffic with a lower priority (1000). This way, traffic to the Active Directory server is allowed, but all other traffic is denied."},{"timestamp":"1683699660.0","poster":"megumin","upvote_count":"1","content":"Selected Answer: A\nA is ok","comment_id":"715039"},{"poster":"AzureDP900","comment_id":"696333","timestamp":"1681658820.0","content":"It is pretty straight forward question, It this case priority low should be allow and high priority rules deny all requests. A is right","upvote_count":"2"},{"content":"Selected Answer: A\nhttps://cloud.google.com/vpc/docs/firewalls#priority_order_for_firewall_rules","timestamp":"1675516980.0","upvote_count":"1","comment_id":"642351","poster":"DrishaS4"},{"comment_id":"627956","upvote_count":"6","comments":[{"poster":"moiradavis","timestamp":"1673975160.0","upvote_count":"1","comment_id":"632592","content":"Oh, really? I got this question on my exam 2 years ago, I did not expect to repeat this kind of questions in the current exam."}],"content":"06/30/2022 Exam question.","timestamp":"1673026140.0","poster":"mv2000"},{"comment_id":"558260","upvote_count":"1","content":"OT: why is there no way to mark questions for review/repeat later on?","poster":"Baumster","timestamp":"1661706120.0"},{"timestamp":"1654627800.0","upvote_count":"1","poster":"haroldbenites","content":"Go for A.\nWhile the priority is higher, the egress rule is more restricted.\nWhile the priority is higher, the ingress rule is more free.","comment_id":"496326"},{"comment_id":"489316","content":"Selected Answer: A\nA is correct answer","poster":"vincy2202","timestamp":"1653752940.0","upvote_count":"1"},{"timestamp":"1653673020.0","comment_id":"488466","content":"Selected Answer: A\nto understand rules priority:\nhttps://cloud.google.com/vpc/docs/firewalls#priority_order_for_firewall_rules","poster":"vchrist","upvote_count":"1"},{"poster":"nqthien041292","timestamp":"1653610560.0","upvote_count":"1","comment_id":"487775","content":"Selected Answer: A\nVote A"},{"timestamp":"1651295160.0","poster":"MaxNRG","upvote_count":"2","comment_id":"470076","content":"A – create an egress rule with priority 1000 to deny all traffic for all instances. Create another egress rule with priority 100 to allow the Active Directory traffic for all instances.\nDefault Firewall rules (aka implied rules) are following:\n1) Egress traffic is allowed to all IP/ports.\n2) Ingress traffic is disabled completely.\nBoth these rules have lowest priority (65535) and cannot be removed.\nhttps://cloud.google.com/vpc/docs/firewalls#default_firewall_rules"},{"upvote_count":"2","poster":"victory108","timestamp":"1637331300.0","content":"A. Create an egress rule with priority 1000 to deny all traffic for all instances. Create another egress rule with priority 100 to allow the Active Directory traffic for all instances.","comment_id":"361352"},{"comment_id":"358630","upvote_count":"1","timestamp":"1637069400.0","content":"A is correct","poster":"un"},{"upvote_count":"2","poster":"sidhappy","timestamp":"1634865120.0","comment_id":"340717","content":"By default all outgoing traffics are allowed in firewall, creating a deny rule with less priority and another one for allowing outgoing traffic with higher priority will work here, the higher priority one(AD rule) will override the deny rule."},{"upvote_count":"1","poster":"Ausias18","timestamp":"1633070940.0","content":"Answer is A","comment_id":"325582"},{"content":"A- \negress traffic is allowed by default \nwith 1000 deny FW rule you can block communication \nwith 100 to allow AD traffic","upvote_count":"3","comment_id":"322019","poster":"gu9singg","timestamp":"1632757380.0"},{"poster":"AD3","content":"'D' is correct. But for clear documentation purpose 'A' is good. There is implied Deny. I worked on it. The Deny log says no matching rule found.","timestamp":"1632531420.0","upvote_count":"2","comment_id":"319729"},{"upvote_count":"2","content":"100 priority to allow AD traffic from all instance . 1000 to block all traffic for instance. so ans is A","timestamp":"1629038160.0","poster":"CloudGenious","comment_id":"291135"},{"comment_id":"232919","timestamp":"1622627100.0","upvote_count":"3","content":"A is the ans","poster":"Chulbul_Pandey"},{"content":"Priority default value for implicit rules is 65535.\n1000 instead is inserted automatically by GCP if left blank the priority field in rule creation.\nImplicit deny egress rule doesn't exist.\nA is correct.","upvote_count":"1","timestamp":"1620043320.0","poster":"occupatissimo","comment_id":"211979"},{"timestamp":"1618432500.0","content":"A is correct, as there is no implied egress rule. Setting the priority 100 ( lower ) i.e lower value for priority means higher precedence , which will override the default priority 1000 to deny traffic for Active directory .\nPriority can be set between 0 to 65535 . Lower the priority, higher the precedence.","comment_id":"200094","upvote_count":"3","poster":"Bharathy"},{"comment_id":"196675","poster":"LoganIsh","upvote_count":"2","content":"as per udemy exam the answer was d had mentioned, now its quite confusing it..","timestamp":"1617966480.0"},{"comment_id":"180555","upvote_count":"1","timestamp":"1615930260.0","poster":"AshokC","content":"Answer: A"},{"timestamp":"1614187860.0","poster":"wiqi","upvote_count":"2","content":"A is correct.","comment_id":"165323"},{"comment_id":"117504","timestamp":"1608737100.0","poster":"mlantonis","upvote_count":"1","content":"A is correct"},{"comment_id":"106693","content":"A, for sure.","timestamp":"1607602860.0","poster":"gfhbox0083","upvote_count":"1"},{"comment_id":"102589","timestamp":"1607117280.0","content":"A is correct for me","upvote_count":"2","poster":"Ziegler"},{"timestamp":"1606630500.0","poster":"jayaen","upvote_count":"1","comment_id":"98021","content":"A is correct"},{"content":"Final Decision to go with Option A. \nhttps://cloud.google.com/vpc/docs/firewalls\nImplied rules are allow all egress and deny all ingress. NO deny egress.","upvote_count":"3","poster":"AD2AD4","timestamp":"1606572660.0","comment_id":"97519"},{"poster":"GunjGupta","timestamp":"1605607920.0","comment_id":"90422","upvote_count":"1","content":"The answer should be A with obvious reasoning mentioned in \nhttps://cloud.google.com/vpc/docs/firewalls#default_firewall_rules.\n Please see carefully ingress/egress wording"},{"comment_id":"89496","upvote_count":"1","content":"A is right. D is wrong as no implied deny egress.","timestamp":"1605450300.0","poster":"Jack_in_Large"},{"upvote_count":"1","content":"A is better","comment_id":"51074","poster":"sssz","timestamp":"1597537920.0"},{"comment_id":"40445","upvote_count":"1","comments":[{"poster":"massasun","timestamp":"1729521060.0","upvote_count":"1","content":"D. Create an egress rule with priority 100 to allow the Active Directory traffic. Rely on the implied deny egress rule with priority 1000 to block all traffic for all instances.\n\nHere's why this is the best solution:\n\nImplied Deny Egress Rule: VPC Service Controls by default enforce an implicit deny egress rule with priority 1000, blocking all outbound traffic by default.\nSpecific Allow Rule: Creating an egress rule with priority 100 to allow the specific Active Directory traffic overrides the default deny rule, permitting only that traffic.","comment_id":"1199702"}],"content":"A is correct","timestamp":"1595093220.0","poster":"sri007"}],"answer_description":"","answers_community":["A (100%)"],"question_text":"All Compute Engine instances in your VPC should be able to connect to an Active Directory server on specific ports. Any other traffic emerging from your instances is not allowed. You want to enforce this using VPC firewall rules.\nHow should you configure the firewall rules?","question_images":[],"answer_images":[],"unix_timestamp":1571539920,"answer":"A","timestamp":"2019-10-20 04:52:00","choices":{"A":"Create an egress rule with priority 1000 to deny all traffic for all instances. Create another egress rule with priority 100 to allow the Active Directory traffic for all instances.","C":"Create an egress rule with priority 1000 to allow the Active Directory traffic. Rely on the implied deny egress rule with priority 100 to block all traffic for all instances.","B":"Create an egress rule with priority 100 to deny all traffic for all instances. Create another egress rule with priority 1000 to allow the Active Directory traffic for all instances.","D":"Create an egress rule with priority 100 to allow the Active Directory traffic. Rely on the implied deny egress rule with priority 1000 to block all traffic for all instances."}},{"id":"bgVG78ukyMU7BJV838de","timestamp":"2021-12-28 10:11:00","answers_community":["D (100%)"],"answer_ET":"D","unix_timestamp":1640682660,"question_text":"Your customer runs a web service used by e-commerce sites to offer product recommendations to users. The company has begun experimenting with a machine learning model on Google Cloud Platform to improve the quality of results.\nWhat should the customer do to improve their model's results over time?","isMC":true,"discussion":[{"content":"Selected Answer: D\nModel performance is generally based on the volume of its training data input. The more the data, the better the model.","upvote_count":"20","poster":"ghadxx","comments":[{"content":"I agree with you, D is right","comment_id":"696332","poster":"AzureDP900","upvote_count":"1","timestamp":"1713281040.0"},{"upvote_count":"1","poster":"Sur_Nikki","content":"Yes, correctly said..This is actually a question for Data Engineer role","timestamp":"1731074520.0","comment_id":"892124"}],"comment_id":"542959","timestamp":"1691479320.0"},{"content":"Selected Answer: D\nA,B,C is defining about the performance of ML but not the result....only the training data will give good ML result/predictions","comment_id":"640233","timestamp":"1706735640.0","upvote_count":"6","poster":"sgofficial"},{"upvote_count":"1","timestamp":"1725502380.0","comment_id":"829617","poster":"Deb2293","content":"Selected Answer: D\nBest answer is D. Other 3 makes no sense"},{"timestamp":"1723969860.0","poster":"PST21","content":"Need to improve the model results and not performance .. hence D","upvote_count":"1","comment_id":"812857"},{"upvote_count":"1","content":"Selected Answer: D\nAnswer is D","poster":"surajkrishnamurthy","timestamp":"1718087100.0","comment_id":"741531"},{"upvote_count":"3","content":"Selected Answer: D\nModel performance is generally based on the volume of its training data input. The more the data, the better the model.","timestamp":"1707053100.0","poster":"DrishaS4","comment_id":"642353"},{"poster":"sivre","comment_id":"578540","upvote_count":"2","comments":[{"poster":"kimharsh","upvote_count":"2","content":"what we will do with metrics , it won't improve our Machine learning model , D is the closest answer , also it didn't say export it said Save , which could be manually moving the data to BQ","comment_id":"588799","timestamp":"1697811780.0"}],"content":"The following insights and recommendations can be exported (to bigquery):\nIAM recommender\nVM machine type recommender\nManaged instance group machine type recommender \nIdle PD recommender\nIdle VM recommender\nCloud SQL overprovisioned instance recommender \nCloud SQL idle instance recommender\nUnattended project recommender\nCloud Run Service Identity recommender\nhttps://cloud.google.com/recommender/docs/bq-export/export-recommendations-to-bq\n\nNone of this is correlated with Machine Learning, how can be D? looks more A the answer","timestamp":"1696106520.0"},{"upvote_count":"3","poster":"Pime13","content":"Selected Answer: D\ni vote D","timestamp":"1690646040.0","comment_id":"535630"},{"timestamp":"1688245980.0","comment_id":"514736","poster":"victory108","upvote_count":"2","content":"D. Save a history of recommendations and results of the recommendations in BigQuery, to be used as training data."},{"timestamp":"1688124120.0","comment_id":"513391","poster":"LoveT","upvote_count":"4","content":"\"training data\" is the key in option \"D\" and that's the answer"},{"content":"Selected Answer: D\nD seems to be the correct answer","timestamp":"1687936260.0","comment_id":"510927","poster":"vincy2202","upvote_count":"1"}],"answer_description":"","answer":"D","question_images":[],"answer_images":[],"question_id":193,"topic":"1","url":"https://www.examtopics.com/discussions/google/view/68718-exam-professional-cloud-architect-topic-1-question-92/","choices":{"A":"Export Cloud Machine Learning Engine performance metrics from Stackdriver to BigQuery, to be used to analyze the efficiency of the model.","C":"Monitor Compute Engine announcements for availability of newer CPU architectures, and deploy the model to them as soon as they are available for additional performance.","B":"Build a roadmap to move the machine learning model training from Cloud GPUs to Cloud TPUs, which offer better results.","D":"Save a history of recommendations and results of the recommendations in BigQuery, to be used as training data."},"exam_id":4},{"id":"0ueC39gRswSgh5gtGfv0","question_text":"A development team at your company has created a dockerized HTTPS web application. You need to deploy the application on Google Kubernetes Engine (GKE) and make sure that the application scales automatically.\nHow should you deploy to GKE?","answer":"A","exam_id":4,"question_id":194,"answer_description":"","answer_images":[],"isMC":true,"question_images":[],"url":"https://www.examtopics.com/discussions/google/view/7123-exam-professional-cloud-architect-topic-1-question-93/","answer_ET":"A","choices":{"C":"Enable autoscaling on the Compute Engine instance group. Use an Ingress resource to load-balance the HTTPS traffic.","A":"Use the Horizontal Pod Autoscaler and enable cluster autoscaling. Use an Ingress resource to load-balance the HTTPS traffic.","B":"Use the Horizontal Pod Autoscaler and enable cluster autoscaling on the Kubernetes cluster. Use a Service resource of type LoadBalancer to load-balance the HTTPS traffic.","D":"Enable autoscaling on the Compute Engine instance group. Use a Service resource of type LoadBalancer to load-balance the HTTPS traffic."},"discussion":[{"comment_id":"17153","timestamp":"1571915460.0","comments":[{"comment_id":"231241","upvote_count":"2","poster":"techalik","content":"I think A is OK:","timestamp":"1606755180.0"},{"poster":"nitinz","content":"It is A, K8s best way to LB is Ingress.","timestamp":"1614907140.0","upvote_count":"5","comment_id":"303795"},{"comment_id":"55103","timestamp":"1582653960.0","comments":[{"timestamp":"1582654560.0","comment_id":"55105","comments":[{"upvote_count":"9","content":"B is ok.\nhttps://cloud.google.com/kubernetes-engine/docs/tutorials/hello-app","poster":"tartar","comment_id":"152420","comments":[{"timestamp":"1602740340.0","poster":"GopiSivanathan","upvote_count":"8","comment_id":"200288","content":"service resource does a NLB using IP address, however, Ingress does HTTP(S) Load balancer. A should be an answer."}],"timestamp":"1596787740.0"}],"poster":"Smart","content":"My bad, as stated by other, Service doesn't support L7 load balancing. Hence, need to setup ingress resource. Correct answer is A.","upvote_count":"46"}],"poster":"Smart","upvote_count":"42","content":"\"Ingress is a Kubernetes resource that encapsulates a collection of rules and configuration for routing external HTTP(S) traffic to internal services.\n\nOn GKE, Ingress is implemented using Cloud Load Balancing. When you create an Ingress in your cluster, GKE creates an HTTP(S) load balancer and configures it to route traffic to your application.\"\n\nAre you exposing multiple services through single IP address? Hence, do you need routing your traffic?\n\nCorrect answer is B."}],"content":"Why not using Ingress? (A)","upvote_count":"30","poster":"crypt0"},{"timestamp":"1572079440.0","comment_id":"17515","content":"Name is service resource, it's B:\n\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/service?hl=es-419","upvote_count":"13","poster":"jcmoranp"},{"content":"Selected Answer: A\nA is the correct answer. Let me explain why:\nFor HTTPS traffic handling:\n\nAnswer A uses an Ingress resource which is the better choice because:\n\nIngress operates at L7 (application layer)\nCan handle SSL/TLS termination\nCan manage certificates\nMore cost-effective (shares load balancer across multiple services)\nProvides URL-based routing\n\n\nAnswer B uses a Service type LoadBalancer which is less optimal because:\n\nCreates a separate L4 load balancer for each service (more expensive)\nCannot terminate SSL/TLS\nCannot do path-based routing\nJust forwards TCP traffic on port 443\n\n\n\nBoth answers correctly mention:\n\nHorizontal Pod Autoscaler (for pod scaling)\nCluster autoscaling (for node scaling)\n\nBut the key differentiator is Ingress vs LoadBalancer, where Ingress is the better choice for HTTPS web applications due to its L7 capabilities and cost-effectiveness.","upvote_count":"2","comment_id":"1360100","timestamp":"1740221820.0","poster":"liorkale"},{"timestamp":"1739515800.0","content":"Selected Answer: B\n\"team has created a dockerized HTTPS web application\" - My understanding is that the HTTPS certificate is baked in the image and we can't do TLS termination. The app no longer operates on L7, but on L4 at port 443. Thus Ingress can't be used (because it can't understand encrypted traffic) and LoadBalancer service is the way to go.","upvote_count":"1","poster":"igrek","comment_id":"1356383"},{"comment_id":"1336985","poster":"ryaryarya","upvote_count":"1","content":"Selected Answer: B\nIt states \"A development team at your company has created a dockerized HTTPS web application\". The web application already supports HTTPS based on that statement, so we don't need HTTPS termination and can use a LoadBalancer service. Who writes these questions, anyway?","timestamp":"1736144580.0"},{"content":"Answer A","poster":"nareshthumma","timestamp":"1729714320.0","comment_id":"1302198","upvote_count":"1"},{"comment_id":"1241387","upvote_count":"2","poster":"mstaicu","content":"Selected Answer: A\nA and B both create under the hood a Service of type LoadBalancer with external IP address. However, when it comes to http(s) traffic an ingress is the way to go because of ssl termination and for the routing options.","timestamp":"1720007460.0"},{"content":"Selected Answer: A\nC & D is clearly incorrect.\n\nB is incorrect because of this:\n\"service of type LoadBalancer to load-balance the HTTPS traffic.\" \nGKE Service Load Balancer is L4 Network or Internal Load Balancer, does not support HTTPS traffic.\n\nThus only A is correct.","comment_id":"1217452","timestamp":"1716554640.0","poster":"huuthanhdlv","upvote_count":"4"},{"content":"Selected Answer: A\nThe clue is HTTPS traffic. You need L7 stack. It can be achieved only through ingress controller.","upvote_count":"3","timestamp":"1715887500.0","comment_id":"1212561","poster":"hitmax87"},{"poster":"nanasenishino","upvote_count":"1","comment_id":"1210069","content":"B\n\nA. Ingress resource: While Ingress can be used for external load balancing, it often requires additional configuration for HTTPS termination (offloading SSL from your application containers). Additionally, LoadBalancer services typically offer a simpler setup for basic external load balancing without HTTPS termination concerns.\nC & D. Compute Engine Instance Group Autoscaling: GKE manages its own nodes separate from Compute Engine instances. Autoscaling on a Compute Engine instance group wouldn't manage the Kubernetes pods or nodes effectively in this scenario.","timestamp":"1715480220.0"},{"upvote_count":"3","timestamp":"1706872140.0","comment_id":"1138466","poster":"Pime13","content":"Selected Answer: A\nservice loadBalancer: https://cloud.google.com/kubernetes-engine/docs/concepts/service-load-balancer\nThis page provides a general overview of how Google Kubernetes Engine (GKE) creates and manages Google Cloud load balancers when you apply a Kubernetes LoadBalancer Services manifest. It describes the different types of load balancers and how settings like the externalTrafficPolicy and GKE subsetting for L4 internal load balancers determine how the load balancers are configured. -> l4 tcp/udp not https\nIngress: https://cloud.google.com/kubernetes-engine/docs/concepts/ingress This page provides a general overview of what Ingress for external Application Load Balancers is and how it works. Google Kubernetes Engine (GKE) provides a built-in and managed Ingress controller called GKE Ingress. This controller implements Ingress resources as Google Cloud load balancers for HTTP(S) workloads in GKE. -S http(s)"},{"comment_id":"1117656","content":"Selected Answer: B\nB is correct","timestamp":"1704816900.0","upvote_count":"1","poster":"gun123"},{"comment_id":"1113118","timestamp":"1704315300.0","poster":"bandegg","upvote_count":"2","content":"Selected Answer: A\nI'm assuming B is the suggested answer because a the question doesn't state that the application should be available externally. Services allow exposing resources internally and to load balancers.\n\nHowever, it should be A, as the assumption would be a an external web application.\n\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/service"},{"timestamp":"1702006260.0","comment_id":"1090745","poster":"MahAli","upvote_count":"2","content":"Selected Answer: B\nMost if the labs in Google boost skills discuss how to expose the deployment using a load balancer."},{"poster":"AwsSuperTrooper","upvote_count":"2","content":"Selected Answer: A\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/ingress\n\"This page provides a general overview of what Ingress for external Application Load Balancers is and how it works. Google Kubernetes Engine (GKE) provides a built-in and managed Ingress controller called GKE Ingress. This controller implements Ingress resources as Google Cloud load balancers for HTTP(S) workloads in GKE.\"","comment_id":"1079778","timestamp":"1700893680.0"},{"poster":"thewalker","comment_id":"1066370","upvote_count":"1","content":"https://cloud.google.com/kubernetes-engine/docs/concepts/ingress\nAs there is no mention about the type of the traffic, Internal or external - Going with A - Ingress.","timestamp":"1699528440.0"},{"upvote_count":"1","poster":"Arun_m_123","timestamp":"1697208780.0","content":"Selected Answer: B\nOption-C and D are straightforwardly wrong\n\nBetween A and B : B is the correct answer, because it makes use of loadbalancing the ingress in K8S native style. That is the reason why cluster scaling is also done. \n\nThis is how it should \nExternal Load Balancing Ingress --> K8S Service of type LoadBalancer --> pods that can autoscale\n\nDirectly allowing external loadbalcing ingress to autoscaled Pod, doesn't makes sense to use GKE","comment_id":"1042755"}],"unix_timestamp":1571915460,"timestamp":"2019-10-24 13:11:00","topic":"1","answers_community":["A (70%)","B (30%)"]},{"id":"wZ6xKvipEwXPA7GvMJv9","answer":"B","answer_ET":"B","question_images":[],"isMC":true,"question_id":195,"discussion":[{"timestamp":"1668868140.0","content":"B. Create an HTTPS load balancer with URL maps.","upvote_count":"14","comment_id":"361373","poster":"victory108"},{"poster":"betiy","comment_id":"32973","content":"URL paths supported only in HTTP(S) Load balancing \nhttps://cloud.google.com/load-balancing/docs/ssl/#FAQ","timestamp":"1624781940.0","upvote_count":"10"},{"timestamp":"1735532460.0","upvote_count":"1","poster":"JonathanSJ","comment_id":"1333913","content":"Selected Answer: B\nI will go for B."},{"upvote_count":"6","poster":"examch","comment_id":"762468","timestamp":"1719712920.0","content":"Selected Answer: B\nB is the correct Answer,\n\nGoogle Cloud HTTP(S) load balancers and Traffic Director use a Google Cloud configuration resource called a URL map to route HTTP(S) requests to backend services or backend buckets.\n\nFor example, with an external HTTP(S) load balancer, you can use a single URL map to route requests to different destinations based on the rules configured in the URL map:\n\nRequests for https://example.com/video go to one backend service.\nRequests for https://example.com/audio go to a different backend service.\nRequests for https://example.com/images go to a Cloud Storage backend bucket.\nRequests for any other host and path combination go to a default backend service.\nURL maps are used with the following Google Cloud products:\n\nExternal HTTP(S) Load Balancing (global, regional, and classic modes)\nInternal HTTP(S) Load Balancing\nTraffic Director\n\nhttps://cloud.google.com/load-balancing/docs/url-map-concepts"},{"comments":[{"timestamp":"1719042840.0","comment_id":"753164","upvote_count":"3","poster":"omermahgoub","content":"Option A, creating a cross-region load balancer with URL Maps, is also a valid solution, but it is not specifically designed for end-to-end in-transit encryption.\n\nOption C, creating appropriate instance groups and instances and configuring SSL proxy load balancing, is not a complete solution for global load balancing. SSL proxy load balancing is a feature that enables you to terminate SSL/TLS connections at the load balancer and establish a new SSL/TLS connection between the load balancer and the back-end service. It is not a global load balancing solution in and of itself.\n\nOption D, creating a global forwarding rule and configuring SSL proxy load balancing, is not a complete solution for global load balancing based on the URL path being requested. A global forwarding rule is a type of load balancing configuration that directs traffic to a specific back-end service based on the IP address and port of the incoming request. It does not allow for routing based on the URL path.\n\n\n\n\nRegenerate"}],"timestamp":"1719042840.0","content":"B. Create an HTTPS load balancer with URL Maps.\n\nAn HTTPS load balancer is a type of load balancer that can distribute incoming HTTPS traffic to one or more back-end services, such as Compute Engine instances or Google Kubernetes Engine clusters. It can also provide SSL/TLS termination, enabling you to use your own SSL/TLS certificates and keys.\n\nYou can use URL Maps to configure the HTTPS load balancer to route traffic based on the URL path being requested. This allows you to set up different URL paths to be served by different back-end services, providing a high level of flexibility in your load balancing configuration.","upvote_count":"1","poster":"omermahgoub","comment_id":"753163"},{"poster":"TonytheTiger","content":"Answer B: URL maps used with global external HTTP(S) load balancers and regional external HTTP(S) load balancer support several advanced traffic management features such as header-based traffic steering, weight-based traffic splitting, and request mirroring.\n\nhttps://cloud.google.com/load-balancing/docs/https#url-maps","upvote_count":"3","timestamp":"1717693980.0","comment_id":"737153"},{"content":"Selected Answer: B\nB is ok","upvote_count":"1","poster":"megumin","timestamp":"1715324340.0","comment_id":"715066"},{"poster":"Sbgani","content":"Selected Answer: B\nUrlMaps are used to route requests to a backend service based on rules that you define for the host and path of an incoming URL.","comment_id":"663091","timestamp":"1709879040.0","upvote_count":"5"},{"content":"https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/compute_url_map ANS B","timestamp":"1709878980.0","comment_id":"663090","upvote_count":"2","poster":"Sbgani"},{"content":"Selected Answer: B\nhttps://cloud.google.com/load-balancing/docs/https/url-map","timestamp":"1707053460.0","poster":"DrishaS4","upvote_count":"4","comments":[{"poster":"AzureDP900","timestamp":"1713280680.0","upvote_count":"1","comment_id":"696328","content":"thank you for pointing the link"}],"comment_id":"642358"},{"comment_id":"496758","poster":"haroldbenites","upvote_count":"2","content":"Go for B","timestamp":"1686215340.0"},{"upvote_count":"2","content":"Selected Answer: B\nB is correct answer","timestamp":"1685405040.0","poster":"vincy2202","comment_id":"490339"},{"content":"Selected Answer: B\nVote B","timestamp":"1685147940.0","comment_id":"487797","poster":"nqthien041292","upvote_count":"1"},{"timestamp":"1668606900.0","comment_id":"358662","upvote_count":"1","content":"B is correct","poster":"un"},{"content":"there are interl https load balancers they are regional https://cloud.google.com/load-balancing/docs/l7-internal","poster":"ccmcwolf","timestamp":"1665828180.0","comment_id":"336140","upvote_count":"1"},{"upvote_count":"2","comment_id":"325590","poster":"Ausias18","timestamp":"1664607300.0","content":"Answer is B"},{"content":"confused with A vs B. A has the word \"cross region\" but finally find out HTTP/S Load Balancing is naturally global.\n- B","comment_id":"280180","timestamp":"1659207540.0","upvote_count":"5","poster":"bnlcnd"},{"timestamp":"1654548960.0","upvote_count":"2","poster":"doumx","content":"B easy","comment_id":"236864"},{"content":"B is correct","upvote_count":"2","poster":"awadheshk","timestamp":"1649394840.0","comment_id":"195752"},{"timestamp":"1647482940.0","content":"B is correct","poster":"AshokC","comment_id":"180614","upvote_count":"2"},{"comment_id":"145584","upvote_count":"4","content":"there is not such thing called cross-region load balancer\nso B is correct","poster":"AmazonAu","timestamp":"1643367480.0"},{"comment_id":"117561","timestamp":"1640277720.0","upvote_count":"1","poster":"mlantonis","content":"I agree with B"},{"poster":"Tushant","comment_id":"114764","timestamp":"1640010000.0","content":"B is correct answer","upvote_count":"2"},{"poster":"gfhbox0083","upvote_count":"3","content":"B, for sure.\nURL paths supported in HTTP(S) Load balancing","timestamp":"1639139520.0","comment_id":"106703"},{"comments":[{"content":"B - Cross Region LoadBalancing comes by default with premium tier.","timestamp":"1638924900.0","poster":"rehma017","comment_id":"104895","upvote_count":"3"}],"comment_id":"98026","upvote_count":"2","timestamp":"1638168000.0","content":"answer should be A \ncross region load balancing required to support global reach \nhttps://cloud.google.com/load-balancing/docs/https","poster":"jayaen"},{"timestamp":"1638109320.0","comment_id":"97530","poster":"AD2AD4","upvote_count":"3","content":"Final Decision to go with Option B"},{"content":"B is correct","timestamp":"1631435220.0","upvote_count":"3","comment_id":"62951","poster":"Javed"},{"timestamp":"1623586980.0","comment_id":"29355","poster":"Ramrao14","comments":[{"comments":[{"comment_id":"152422","upvote_count":"4","content":"B is ok","timestamp":"1644228720.0","poster":"tartar"}],"upvote_count":"7","poster":"MyPractice","content":"Yes B is correct - anything to do with Path based leads to HTTPs LB","timestamp":"1625190060.0","comment_id":"34408"},{"upvote_count":"2","comment_id":"210566","poster":"kumarp6","content":"Yes, B is correct","timestamp":"1651411920.0"},{"poster":"nitinz","content":"No brainer it is B","timestamp":"1662333600.0","comment_id":"303796","upvote_count":"1"}],"content":"Is B really the correct answer ?","upvote_count":"2"}],"exam_id":4,"topic":"1","unix_timestamp":1576246980,"choices":{"A":"Create a cross-region load balancer with URL Maps.","C":"Create appropriate instance groups and instances. Configure SSL proxy load balancing.","B":"Create an HTTPS load balancer with URL Maps.","D":"Create a global forwarding rule. Configure SSL proxy load balancing."},"answer_description":"","question_text":"You need to design a solution for global load balancing based on the URL path being requested. You need to ensure operations reliability and end-to-end in- transit encryption based on Google best practices.\nWhat should you do?","answers_community":["B (100%)"],"answer_images":[],"timestamp":"2019-12-13 15:23:00","url":"https://www.examtopics.com/discussions/google/view/10289-exam-professional-cloud-architect-topic-1-question-94/"}],"exam":{"provider":"Google","id":4,"lastUpdated":"11 Apr 2025","numberOfQuestions":279,"isMCOnly":false,"name":"Professional Cloud Architect","isBeta":false,"isImplemented":true},"currentPage":39},"__N_SSP":true}