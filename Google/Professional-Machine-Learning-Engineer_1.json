{"pageProps":{"questions":[{"id":"JU7X00JZyM8GJeGd3lsO","answer_ET":"A","question_id":1,"answer":"A","question_text":"You are building an ML model to detect anomalies in real-time sensor data. You will use Pub/Sub to handle incoming requests. You want to store the results for analytics and visualization. How should you configure the pipeline?","discussion":[{"upvote_count":"24","poster":"esuaaaa","content":"Definitely A. Dataflow is must.","comment_id":"373940","timestamp":"1622760900.0"},{"poster":"inder0007","upvote_count":"12","comment_id":"373009","timestamp":"1622663100.0","content":"Even if I follow the link, it should be dataflow, AI-Platform and Bigquery.\nReal answer should be A"},{"timestamp":"1743251280.0","upvote_count":"1","comment_id":"1411705","content":"Selected Answer: A\nDataflow required and BQ at the end","poster":"yia20082000"},{"upvote_count":"1","content":"Selected Answer: A\nA. Dataflow is a must.\nhttps://docs.google.com/document/d/1VV6vkkjShXDgPLSG6V_7-0dweLmZTUnYiTSxo6C5ERY/edit?tab=t.0","timestamp":"1742544780.0","poster":"Paxtons_Aunders","comment_id":"1401480"},{"upvote_count":"1","content":"Selected Answer: A\nI think it s A","comment_id":"1361421","poster":"AWBY_sback","timestamp":"1740484380.0"},{"poster":"ki_123","timestamp":"1733397600.0","upvote_count":"2","comment_id":"1322331","content":"Selected Answer: A\nBigQuery for the analytics and visualization."},{"poster":"jkkim_jt","content":"Selected Answer: A\nChatGPT Prompt: Using Google Pub/Sub Define ML Pipeline for Anomaly Detection\n```mermaid\ngraph TD;\nA[Data Source] -->|Pub/Sub Topic| B[Pub/Sub]\nB --> C[Dataflow for Preprocessing]\nC --> D[ML Model Inference (AI Platform)]\nD -->|Prediction| E[BigQuery / Cloud Storage]\nD -->|Alert| F[Cloud Functions]\n```","upvote_count":"1","comment_id":"1300002","timestamp":"1729338660.0"},{"content":"ChatGPT Prompt: Using Google Pub/Sub Define ML Pipeline for Anomaly Detection\n```mermaid\ngraph TD;\n A[Data Source] -->|Pub/Sub Topic| B[Pub/Sub]\n B --> C[Dataflow for Preprocessing]\n C --> D[ML Model Inference (AI Platform)]\n D -->|Prediction| E[BigQuery / Cloud Storage]\n D -->|Alert| F[Cloud Functions]\n```","timestamp":"1729338420.0","poster":"jkkim_jt","upvote_count":"1","comment_id":"1300000"},{"comment_id":"1260520","poster":"nktyagi","content":"Selected Answer: A\nTo preprocess data you will use Dataflow, and then you can use the Vertex AI platform for training and serving. Since it's a recommendation use case, Cloud BigQuery is the recommended NoSQL store to manage this use case storage at scale and reduce latency.","timestamp":"1726853220.0","upvote_count":"2"},{"poster":"LeumaS_NoswaY","timestamp":"1726591080.0","upvote_count":"1","content":"PubSub -> Dataflow -> AI Platform -> BiqQuery","comment_id":"1285364"},{"comment_id":"1244328","timestamp":"1720441620.0","upvote_count":"1","poster":"Yorko","content":"Selected Answer: A\nBigQuery for analytics 100%"},{"timestamp":"1717591080.0","comment_id":"1224713","content":"Selected Answer: A\nBig Query is ideal for analytics","upvote_count":"1","poster":"PhilipKoku"},{"upvote_count":"1","comment_id":"1202049","timestamp":"1714054200.0","poster":"RPS007","content":"Selected Answer: A\nVerified Answer"},{"poster":"Shreeti_Saha","content":"Option A","upvote_count":"1","comment_id":"1189672","timestamp":"1712295660.0"},{"content":"Selected Answer: A\nA - Dataflow is the only correct option for this case.","timestamp":"1701427920.0","comment_id":"1085155","upvote_count":"1","poster":"fragkris"},{"content":"AutoML is useful for labeled data. So either A or D. Dataflow is must for pipeline so A is correct","upvote_count":"1","timestamp":"1698885000.0","poster":"RangasamyArran","comment_id":"1060146"},{"upvote_count":"1","content":"Selected Answer: A\nA. Definitely it's the correct answer","comment_id":"1017475","timestamp":"1695711000.0","poster":"LMDY"},{"poster":"deepakno","timestamp":"1693751700.0","comment_id":"997690","upvote_count":"1","content":"This use case similar to anomaly detection also points to A only. \nhttps://cloud.google.com/blog/products/data-analytics/anomaly-detection-using-streaming-analytics-and-ai"},{"poster":"sachinbhavar","content":"The correct answer is D. 1 = BigQuery, 2 = AI Platform, 3 = Cloud Storage.","upvote_count":"1","comment_id":"975111","timestamp":"1691460900.0"},{"poster":"Remi2021","timestamp":"1689333420.0","comment_id":"951468","content":"right answer i A. Dataflow is a must.","upvote_count":"1"},{"poster":"Puneet2022","timestamp":"1682738820.0","upvote_count":"1","comment_id":"884008","content":"Selected Answer: A\nDataflow is required"},{"comment_id":"884007","upvote_count":"1","timestamp":"1682738760.0","poster":"Puneet2022","content":"A, data flow is required"},{"upvote_count":"1","timestamp":"1678307340.0","comment_id":"833355","poster":"encasfer","content":"Selected Answer: A\nDefinitely A."},{"poster":"f828ba8","comment_id":"823555","upvote_count":"1","timestamp":"1677494460.0","content":"Selected Answer: A\nDefinitely A."},{"timestamp":"1676920800.0","poster":"FherRO","content":"Selected Answer: A\nDataflow - data transformation\nVertex AI - model creation\nBig query - analytics and visualization","upvote_count":"1","comment_id":"815720"},{"content":"Selected Answer: A\nDataflow for the pipeline, BigQuery for storing and visualization and Platform AI (now Vertex) to build the model","poster":"EFIGO","timestamp":"1669210140.0","upvote_count":"1","comment_id":"725151"},{"timestamp":"1667209680.0","comment_id":"708320","upvote_count":"1","content":"Definitely A. Dataflow is must for pusub","poster":"abhi0706"},{"content":"Selected Answer: A\ndefinitely A","comment_id":"656269","timestamp":"1662037800.0","poster":"paggio25","upvote_count":"1"},{"comment_id":"650631","timestamp":"1661237520.0","poster":"florianB192833","upvote_count":"1","content":"Selected Answer: A\nFor sure"},{"timestamp":"1660524060.0","poster":"GCP72","upvote_count":"1","content":"Selected Answer: A\n1 = Dataflow, 2 = AI Platform, 3 = BigQuery ..A is correct ans","comment_id":"646963"},{"content":"Selected Answer: A\nIt's for visualization and analytics, the final storage must be BigQuery","upvote_count":"1","poster":"alejo_1053","comment_id":"641274","timestamp":"1659445080.0"},{"timestamp":"1659036780.0","content":"Selected Answer: A\nA. , because DataFlow should definitely be at the beginning and BigQuery at the end","poster":"enghabeth","upvote_count":"1","comment_id":"638852"},{"comments":[{"timestamp":"1675238040.0","comment_id":"794908","content":"I think you can in some situations: https://cloud.google.com/pubsub/docs/bigquery","poster":"Rastri","upvote_count":"1"}],"timestamp":"1655021040.0","upvote_count":"2","content":"Selected Answer: A\nby options eliminations , answer will between A,D\nin D you cant save streaming data from pubsub to bigquery without using Dataflow\nso A is the answer","poster":"Mohamed_Mossad","comment_id":"615202"},{"comment_id":"599269","poster":"David_ml","content":"Selected Answer: A\nAnswer is A","timestamp":"1652125800.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1649822640.0","content":"Selected Answer: A\nAnswer is A","poster":"David_ml","comment_id":"584986"},{"content":"Answer is A. It's a shame these can't be corrected, when there are so many clear mistakes.","comment_id":"568283","upvote_count":"1","timestamp":"1647339480.0","poster":"baimus"},{"upvote_count":"1","poster":"NamitSehgal","content":"Selected Answer: A\nBigquery needs to be at the end of the pipeline to store and analyze the results.","comment_id":"514780","timestamp":"1641090720.0"},{"poster":"VinodSangare","timestamp":"1640748600.0","content":"Selected Answer: A\nA is CORRECT","upvote_count":"1","comment_id":"511748"},{"timestamp":"1640485860.0","content":"A for sure","upvote_count":"1","poster":"NickHapton","comment_id":"509377"},{"content":"=New Question1=\nDuring batch training of a neural network, you notice that there is an oscillation in the loss. How should you adjust your model to ensure that it converges?\n\nA. Increase the size of the training batch\nB. Decrease the size of the training batch\nC. Increase the learning rate hyperparameter\nD. Decrease the learning rate hyperparameter","comments":[{"content":"I agree with D. The oscillation indicates the learning rate is too big and it's missing the target so it bounces back and forth.","timestamp":"1648472160.0","comment_id":"576864","upvote_count":"3","poster":"lukacs16"},{"upvote_count":"1","comment_id":"601845","timestamp":"1652574900.0","poster":"szl0144","content":"answer is d"},{"upvote_count":"1","comment_id":"542781","timestamp":"1644280740.0","content":"Should be C increasing will make it fail more, batch increase/decrease will not make loss function to perform better, https://developers.google.com/machine-learning/crash-course/introduction-to-neural-networks/playground-exercises","poster":"wences"},{"content":"Answer?","comment_id":"507159","timestamp":"1640183400.0","poster":"MisterHairy","upvote_count":"1"}],"upvote_count":"2","timestamp":"1640182920.0","poster":"MisterHairy","comment_id":"507148"},{"comment_id":"500057","timestamp":"1639319160.0","upvote_count":"6","content":"B. 1 = DataProc, 2 = AutoML, 3 = Cloud Bigtable - wrong because bigtable is not for analytics and dataflow is preferred over dataproc.\nC. 1 = BigQuery, 2 = AutoML, 3 = Cloud Functions - wrong because cloud function is not a data store product\nD. 1 = BigQuery, 2 = AI Platform, 3 = Cloud Storage - wrong because cloud storage is not best way to store data for analytics","poster":"ashii007"},{"content":"C is definitely wrong. You cannot use cloud function to store data for analytics and cloud storage is not an optimized option (which rules out B).","upvote_count":"1","timestamp":"1639319040.0","comment_id":"500056","poster":"ashii007"},{"poster":"alphard","upvote_count":"1","comment_id":"495018","timestamp":"1638784380.0","content":"Option A looks better than others because Dataflow for PubSub streaming and BigQuery for data analytics and visualization."},{"comment_id":"475934","content":"You need to train a regression model based on a dataset containing 50,000 records that is stored in BigQuery. The data includes a total of 20 categorical and numerical features with a target variable that can include negative values. You need to minimize effort and training time while maximizing model performance. What approach should you take to train this regression model?\nA. Create a custom TensorFlow DNN model.\nB. Use BQML XGBoost regression to train the model\nC. Use AutoML Tables to train the model without early stopping.\nD. Use AutoML Tables to train the model with RMSLE as the optimization objective","poster":"RahulSingh","upvote_count":"1","timestamp":"1636597740.0","comments":[{"upvote_count":"2","content":"Answer - B\nQuestion indicates data in BQ, minimise effort and training time hence makes sense to use something provided in BQ ML","comment_id":"528150","timestamp":"1642654800.0","poster":"A4M"},{"poster":"giaZ","timestamp":"1647357780.0","upvote_count":"3","comment_id":"568485","content":"I think it's B, by elimination. When they tell you \"minimize effort\" you immediately think AutoML, but here they make you rule that possibility out by specifying on purpose \"without early stopping\" which goes against \"minimize training time\" ) and \"with RMSLE...\" (which goes against \"can include negative values\"). So you're are left with B as the best option, even though with less than 100,000 records you should usually go for pre-built models."}]},{"poster":"ramen_lover","timestamp":"1636006980.0","upvote_count":"1","comment_id":"472409","content":"Definitely A.\nhttps://cloud.google.com/architecture/detecting-anomalies-in-financial-transactions?hl=en"},{"poster":"mousseUwU","upvote_count":"3","content":"A (Correct)\ndoc: https://cloud.google.com/architecture/building-anomaly-detection-dataflow-bigqueryml-dlp","comment_id":"464070","timestamp":"1634555520.0"},{"content":"Totally agree with everyone else, it should be A. Dataflow and the giveaway is real-time / streaming data.","poster":"george_ognyanov","upvote_count":"2","timestamp":"1633427520.0","comment_id":"457612"},{"timestamp":"1631857020.0","content":"Should be A","comment_id":"446356","upvote_count":"1","poster":"SelvaPalani"}],"answer_description":"","isMC":true,"choices":{"B":"1 = DataProc, 2 = AutoML, 3 = Cloud Bigtable","C":"1 = BigQuery, 2 = AutoML, 3 = Cloud Functions","A":"1 = Dataflow, 2 = AI Platform, 3 = BigQuery","D":"1 = BigQuery, 2 = AI Platform, 3 = Cloud Storage"},"answer_images":[],"timestamp":"2021-06-02 21:45:00","question_images":[],"topic":"1","exam_id":13,"url":"https://www.examtopics.com/discussions/google/view/54296-exam-professional-machine-learning-engineer-topic-1-question/","answers_community":["A (100%)"],"unix_timestamp":1622663100},{"id":"5vL5OhNiaCNTV2ta0JLO","answer":"C","url":"https://www.examtopics.com/discussions/google/view/54830-exam-professional-machine-learning-engineer-topic-1-question/","question_id":2,"isMC":true,"question_images":[],"question_text":"Your team needs to build a model that predicts whether images contain a driver's license, passport, or credit card. The data engineering team already built the pipeline and generated a dataset composed of 10,000 images with driver's licenses, 1,000 images with passports, and 1,000 images with credit cards. You now have to train a model with the following label map: [`˜drivers_license', `˜passport', `˜credit_card']. Which loss function should you use?","choices":{"B":"Binary cross-entropy","D":"Sparse categorical cross-entropy","C":"Categorical cross-entropy","A":"Categorical hinge"},"answers_community":["C (50%)","D (50%)"],"discussion":[{"comments":[{"upvote_count":"9","comments":[{"upvote_count":"8","comment_id":"463507","content":"Definitely C - the target variable label formulated in the question requires a categorical cross entropy loss function i.e. 3 columns 'drivers_license' , 'passport', 'credit_card' that can take values 1, 0. Meanwhile sparse categorical cross entropy would require the labels to be integer encoded in a single vector, for example, 'drivers_license' = 1, 'passport' = 2, 'credit_card' = 3.","poster":"GogoG","comments":[{"content":"wrong - its; [0,1,2]","timestamp":"1734455160.0","upvote_count":"1","poster":"desertlotus1211","comment_id":"1328053"},{"timestamp":"1732020840.0","comment_id":"1314635","content":"Using 1, 2 and 3 the model might interpret one value to be more important than the other which is not the case","upvote_count":"1","poster":"dorinas"},{"comment_id":"946519","comments":[{"content":"No, I'm sorry, I wrote it before checking - You were right. We use sparse categorical cross entropy when we have just an index (integer) as a label. The only difference is that it decodes the integer into one hot representation that suites to out DNN output.","comment_id":"946549","timestamp":"1688825640.0","upvote_count":"1","poster":"Jarek7"}],"timestamp":"1688823120.0","content":"Actually it is exactly the opposite. Your label map has 3 options which are mutually exclusive. A document cannot be both - a driver license and a passport. There is a SPARSE vector as output - only one of the categorical outputs is valid for a one example.","poster":"Jarek7","upvote_count":"1"}],"timestamp":"1634467080.0"}],"comment_id":"395392","content":"Use sparse categorical crossentropy when your classes are mutually exclusive (e.g. when each sample belongs exactly to one class) and categorical crossentropy when one sample can have multiple classes or labels are soft probabilities (like [0.5, 0.3, 0.2]).","timestamp":"1625105220.0","poster":"gcp2021go"}],"comment_id":"389831","content":"Answer is C","poster":"ransev","upvote_count":"21","timestamp":"1624560420.0"},{"upvote_count":"10","poster":"gcp2021go","comments":[{"comment_id":"423279","poster":"ori5225","timestamp":"1628681760.0","content":"Use sparse categorical crossentropy when your classes are mutually exclusive (e.g. when each sample belongs exactly to one class) and categorical crossentropy when one sample can have multiple classes or labels are soft probabilities (like [0.5, 0.3, 0.2]).","upvote_count":"3"},{"comment_id":"576740","upvote_count":"2","timestamp":"1648460400.0","poster":"giaZ","content":"Literally from the link you posted: \n\"A possible cause of frustration when using cross-entropy with classification problems with a large number of labels is the one hot encoding process. [...] This can mean that the target element of each training example may require a one hot encoded vector with tens or hundreds of thousands of zero values, requiring significant memory. Sparse cross-entropy addresses this by performing the same cross-entropy calculation of error, without requiring that the target variable be one hot encoded prior to training\".\nHere we have 3 categories...No problem doing one-hot encoding. Answer: C"}],"comment_id":"376896","content":"answer is D \nhttps://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/","timestamp":"1623082260.0"},{"timestamp":"1743809220.0","upvote_count":"1","content":"Selected Answer: C\nIt should be categorical cross entropy","poster":"shahriar096","comment_id":"1496199"},{"content":"Selected Answer: D\nSince the problem is a multi-class classification task (choosing between drivers_license, passport, and credit_card), you need a loss function designed for multi-class classification. Sparse Categorical Cross-Entropy is the best choice because:\n\nThe labels are integer-encoded (not one-hot encoded).\nIt is computationally more efficient than Categorical Cross-Entropy when dealing with class indices instead of one-hot vectors.\n\nWhy not the others?\nA (Categorical Hinge): Used for multi-class classification with hinge loss, typically for SVMs, not neural networks.\nB (Binary Cross-Entropy): Used for binary classification (two classes), while this problem has three classes.\nC (Categorical Cross-Entropy): Works for multi-class classification but requires one-hot encoded labels, whereas the dataset likely uses integer labels.","poster":"ddeveloperr","comment_id":"1351111","timestamp":"1738617720.0","upvote_count":"1"},{"comment_id":"1350859","upvote_count":"1","timestamp":"1738585740.0","content":"Selected Answer: C\nCategorical cross-entropy is suitable for multi-class classification problems where each instance belongs to one and only one class. Since you have multiple classes (driver's license, passport, credit card), this loss function is appropriate.","poster":"vishalzade29"},{"comment_id":"1349718","upvote_count":"1","timestamp":"1738370940.0","poster":"arjun2025","content":"Selected Answer: D\nIn case of multiclass classification problems, we use sparse categorical cross‐entropy."},{"content":"Selected Answer: C\nBecause you have a multi-class classification problem with mutually exclusive classes and a label map, categorical cross-entropy is the most suitable and commonly used loss function.","poster":"strafer","timestamp":"1737983700.0","upvote_count":"1","comment_id":"1347407"},{"timestamp":"1737334560.0","upvote_count":"1","comment_id":"1343269","content":"Selected Answer: C\nThe answer is C. No need to overthink it as sparse categorical cross entropy is used for sparse matrix which is not the case.","poster":"moammary"},{"comment_id":"1335111","poster":"kongae","timestamp":"1735708920.0","content":"Selected Answer: C\nAnswer will be D if the label values are integer but it is string, I will go for C","upvote_count":"1"},{"comment_id":"1328052","poster":"desertlotus1211","timestamp":"1734455100.0","upvote_count":"1","content":"Selected Answer: D\nThe label map [driver's_license, passport, credit_card] naturally maps to 0, 1, 2 as per machine learning standards. Which is used in Sparse categorical cross-entropy"},{"timestamp":"1733537520.0","poster":"rajshiv","upvote_count":"1","content":"Selected Answer: C\nIt is C. D will be appropriate only if the labels are integers which is not true in this case.","comment_id":"1322974"},{"upvote_count":"1","comment_id":"1314046","poster":"joqu","timestamp":"1731942720.0","content":"Selected Answer: D\nThe question clearly says \"You now have to train a model with the following LABEL MAP\". Label map is not one-hot encoding."},{"content":"Selected Answer: D\nㅇ Categorial Cross-Entropy for the multiple classification with one-hot-encoding labels\nㅇ Sparse Categorical Cross-Entropy for the multiple classification with index labels","comment_id":"1300019","timestamp":"1729344180.0","upvote_count":"1","poster":"jkkim_jt"},{"upvote_count":"1","content":"Selected Answer: D\nC needs the target to be One hot encoded already. Since it is not, the answer is D","comment_id":"1241185","poster":"Prakzz","timestamp":"1719986580.0"},{"poster":"PhilipKoku","upvote_count":"1","timestamp":"1717648800.0","content":"Selected Answer: C\nC) Multi-Class Classification (Three or More Classes):\nSince you have three classes, you should use a multi-class loss function.\nThe most common choice for multi-class image classification is categorical cross-entropy2.\nCategorical cross-entropy is designed for scenarios where each input belongs to exactly one class (i.e., mutually exclusive classes).\nTherefore, the correct answer is C. Categorical cross-entropy. It’s well-suited for multi-class classification tasks like this one.\nReferences:\nHow to Choose Loss Functions When Training Deep Learning Neural Networks (https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/)\nStack Exchange: How to know which loss function is suitable for image classification? (https://datascience.stackexchange.com/questions/58138/how-to-know-which-loss-function-is-suitable-for-image-classification)","comment_id":"1225140"},{"poster":"gscharly","content":"Selected Answer: C\nI'd go with C. Categorical cross entropy is used when classes are mutually exclusive. If the number of classes was very high, then we could use sparse categorical cross entropy.","upvote_count":"1","comment_id":"1199486","timestamp":"1713681300.0"},{"comments":[{"poster":"pinimichele01","timestamp":"1713718740.0","upvote_count":"2","content":"A. Categorical hinge : Mainly for SVM soft margins\nB. Binary cross-entropy : for 2 class only\nC. Categorical cross-entropy: Multi class but not necessarily Mutually exclusive\nD. Sparse categorical cross-entropy : Multi class + Mutually exclusive only , saves memory too","comment_id":"1199774"},{"comment_id":"1199776","content":"https://www.tensorflow.org/api_docs/python/tf/keras/losses/categorical_crossentropy\nhttps://www.tensorflow.org/api_docs/python/tf/keras/metrics/sparse_categorical_crossentropy","timestamp":"1713718920.0","poster":"pinimichele01","upvote_count":"1"}],"comment_id":"1194365","content":"Selected Answer: D\nUse sparse categorical crossentropy when your classes are mutually exclusive (e.g. when each sample belongs exactly to one class) and categorical crossentropy when one sample can have multiple classes or labels are soft probabilities (like [0.5, 0.3, 0.2]).","timestamp":"1712929020.0","poster":"pinimichele01","upvote_count":"3"},{"content":"Selected Answer: C\nC\nD is for integer value instead of one-hot encoded vectors, in our question, it is 'drivers_license', 'passport', 'credit_card' one-hot.","timestamp":"1712213760.0","comment_id":"1189143","upvote_count":"1","poster":"Yan_X"},{"timestamp":"1709241300.0","poster":"Paulus89","upvote_count":"1","comment_id":"1163026","content":"Selected Answer: C\nIt depends on how the labels are encoded. If onehot use CCE. If its a single integer representing the class use SCCE (Source: same as in the official (wrong) answer)\nFrom the question it's not clear how the labels are encoded. But for just 3 classes there is no doubt it's better to go with one-hot encoding. Memory restrictions or a huge number of classes might point to SCCE"},{"content":"Selected Answer: D\nYou now HAVE TO to train a model with the following label map: [`˜drivers_license', `˜passport', `˜credit_card'].","timestamp":"1706083860.0","upvote_count":"2","comment_id":"1130375","poster":"Zwi3b3l"},{"poster":"Sum_Sum","upvote_count":"1","comment_id":"1070435","timestamp":"1699970460.0","content":"Selected Answer: C\nIf you are wondering between C & D - think about what \"sparse\" means\nIt is used when dealing with hundreds of categories"},{"upvote_count":"1","timestamp":"1698595980.0","comment_id":"1056954","content":"Selected Answer: D\nmutually exclusive classes","poster":"Sahana_98"},{"comment_id":"1039157","poster":"syedsajjad","timestamp":"1696913460.0","content":"In this case, we have a multi-class classification problem with three classes: driver's license, passport, and credit card. Therefore, we should use the categorical cross-entropy loss function to train our model.\n\nSparse categorical cross-entropy is used for multi-class classification problems where the labels are represented in a sparse matrix format. This is not the case in this problem.","upvote_count":"2"},{"poster":"lalala_meow","comment_id":"1015360","upvote_count":"1","content":"Selected Answer: C\nOnly 3 categories of values being either T or F. They don't really need to be integer encoded, which differs sparse cross-entropy from categorical.","timestamp":"1695508080.0"},{"poster":"Dan137","comment_id":"997050","timestamp":"1693677600.0","comments":[{"timestamp":"1693677660.0","poster":"Dan137","content":"categorical_crossentropy (cce) produces a one-hot array containing the probable match for each category,\n\nsparse_categorical_crossentropy (scce) produces a category index of the most likely matching category.","comment_id":"997051","upvote_count":"1"}],"upvote_count":"1","content":"Selected Answer: D\nhttps://fmorenovr.medium.com/sparse-categorical-cross-entropy-vs-categorical-cross-entropy-ea01d0392d28"},{"content":"The correct answer is: C. Categorical cross-entropy.\n\nyou are dealing with a multi-class classification problem where each image can belong to one of three classes: \"driver's license,\" \"passport,\" or \"credit card.\" Categorical cross-entropy is the appropriate loss function for multi-class classification tasks. It measures the dissimilarity between the predicted class probabilities and the true class labels. It's designed to penalize larger errors in predicted probabilities and help the model converge towards more accurate predictions.","timestamp":"1691853960.0","poster":"Venish","upvote_count":"1","comment_id":"979433"},{"content":"Req : Multi class + mutually exclusive labels\nA. Categorical hinge : Mainly for SVM soft margins\nB. Binary cross-entropy : for 2 class only\nC. Categorical cross-entropy: Multi class but not necessarily Mutually exclusive\nD. Sparse categorical cross-entropy : Multi class + Mutually exclusive only , saves memory too","timestamp":"1688877720.0","comment_id":"946881","upvote_count":"1","poster":"harithacML"},{"timestamp":"1687529940.0","poster":"momosoundz","comment_id":"931695","upvote_count":"1","content":"Selected Answer: C\nit's C"},{"content":"Answer is C.\nsparse means your label is mutually exclusive, but in this case, an image can consist of driver licence and credit card and etc","comment_id":"928634","upvote_count":"2","poster":"NickHapton","timestamp":"1687281420.0"},{"upvote_count":"1","content":"Selected Answer: C\nCategorical cross entropy as model is trained with [1,0,0]/[0,1,0]/[0,0,1] kind of labels as given in the question","poster":"ashu381","timestamp":"1686408540.0","comment_id":"920136"},{"content":"Selected Answer: C\nC. Categorical cross-entropy \nAs you have only three categories the recomendation will be use one-hot encoding whci means categorical. If you use integers (sparse) then there could be \"preferences\" of the model based on the integer. Has no sense in exclusive features with only 3 vectors.","poster":"Voyager2","upvote_count":"1","timestamp":"1686204120.0","comment_id":"917870"},{"content":"Selected Answer: D\nD is correct, Sparse categorical cross-entropy is used in multiple label classification when output classes are label encoded and there is only one correct label. In this case output activation function is softmax.","poster":"frangm23","comment_id":"907123","timestamp":"1685083260.0","upvote_count":"1"},{"poster":"M25","comment_id":"892683","upvote_count":"1","content":"Selected Answer: C\nWent with C","timestamp":"1683608160.0"},{"upvote_count":"1","content":"Selected Answer: C\nIt's C. Sparse categorical cross-entropy is not required when the number of classes is only 3.","comment_id":"845720","timestamp":"1679390820.0","poster":"dfdrin"},{"content":"Selected Answer: D\nAnswer is D. Sparse categorical cross-entropy","upvote_count":"2","timestamp":"1678091580.0","comment_id":"830639","poster":"FDS1993"},{"poster":"Ade_jr","upvote_count":"2","content":"Selected Answer: C\nAnswer is C \nhttps://stackoverflow.com/questions/58565394/what-is-the-difference-between-sparse-categorical-crossentropy-and-categorical-c","timestamp":"1673191740.0","comment_id":"769583"},{"comment_id":"767520","upvote_count":"2","timestamp":"1673001240.0","poster":"ares81","content":"Selected Answer: D\nCategorical cross-entropy is used when true labels are one-hot encoded, for example, we have the following true values for 3-class classification problem [1,0,0], [0,1,0] and [0,0,1].\nIn sparse categorical cross-entropy , truth labels are integer encoded, for example, [1], [2] and [3] for 3-class problem."},{"upvote_count":"1","timestamp":"1672296660.0","comment_id":"760696","poster":"mnaveenkumar13","content":"Selected Answer: C\nAnswer is C"},{"content":"would change to C","comment_id":"752151","poster":"MithunDesai","upvote_count":"1","timestamp":"1671619140.0"},{"upvote_count":"2","comment_id":"743985","timestamp":"1670934060.0","poster":"Sid_Harkal","content":"Answer is C \nIf your targets are one-hot encoded, use categorical_crossentropy. Examples of one-hot encodings:\n\n[1,0,0]\n[0,1,0] \n[0,0,1]\nBut if your targets are integers, use sparse_categorical_crossentropy. Examples of integer encodings (for the sake of completion):\n\n1\n2\n3\nhttps://datascience.stackexchange.com/questions/41921/sparse-categorical-crossentropy-vs-categorical-crossentropy-keras-accuracy"},{"comment_id":"738446","upvote_count":"1","timestamp":"1670453400.0","poster":"hiromi","content":"Selected Answer: C\nC is the ans\nRef.: https://developers.google.com/machine-learning/guides/text-classification/step-4"},{"poster":"abhi0706","content":"Answer is D","timestamp":"1667325840.0","comment_id":"709353","upvote_count":"1"},{"upvote_count":"1","poster":"lawshine","comment_id":"677102","timestamp":"1663935240.0","content":"Selected Answer: D\nIf your Yi’s are one-hot encoded, use categorical_crossentropy. Examples (for a 3-class classification): [1,0,0] , [0,1,0], [0,0,1]\n\nBut if your Yi’s are not one-hot encoded, use sparse_categorical_crossentropy. Examples for above 3-class classification problem: [“drivers_license”], [“passport”], [“credit_card”]. (Should label encoding)"},{"comment_id":"647431","upvote_count":"2","poster":"ramakin","content":"Answer is D. \n\nIf your Yi’s are one-hot encoded, use categorical_crossentropy. Examples (for a 3-class classification): [1,0,0] , [0,1,0], [0,0,1]\n\nBut if your Yi’s are not one-hot encoded, use sparse_categorical_crossentropy. Examples for above 3-class classification problem: [“drivers_license”], [“passport”], [“credit_card”]. (Should label encoding)\n\nSince answer is D.\n\nref: https://www.tensorflow.org/api_docs/python/tf/keras/metrics/sparse_categorical_crossentropy\n\ny_true = [1, 2]\ny_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\nloss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)","timestamp":"1660620660.0"},{"timestamp":"1660566420.0","poster":"GCP72","upvote_count":"1","comment_id":"647179","content":"Selected Answer: D\nI think \"D\" is correct ans but not sure..sorry"},{"content":"Selected Answer: D\nI believe it's D. Given that we \"now have to train a model with the following label map\", the target variable as given is not one-hot encoded. This, plus the fact the classes are mutually exclusive for this problem, make D the best (though not the only viable) option.","upvote_count":"2","comment_id":"631040","poster":"u_phoria","timestamp":"1657741380.0"},{"poster":"rafaelgildin","timestamp":"1655244600.0","content":"Selected Answer: C\nIf your Yi's are one-hot encoded, use categorical_crossentropy. Examples (for a 3-class classification): [1,0,0] , [0,1,0], [0,0,1]\n\nBut if your Yi's are integers, use sparse_categorical_crossentropy. Examples for above 3-class classification problem: [1] , [2], [3]\n\nhttps://stats.stackexchange.com/questions/326065/cross-entropy-vs-sparse-cross-entropy-when-to-use-one-over-the-other","upvote_count":"1","comment_id":"616420"},{"poster":"Mohamed_Mossad","comment_id":"615333","content":"Selected Answer: C\nby options elimination answer between A,C\ni will vote for C simply more common to use in training deep neural networks","upvote_count":"1","timestamp":"1655039820.0"},{"comment_id":"578208","poster":"lukacs16","content":"I believe the answer is C, and because:\n\"if you use categorical-cross-entropy you need one-hot encoding, and if you use sparse-categorical-cross-entropy you encode as normal integers.\n\nUse sparse categorical cross-entropy when your classes are mutually exclusive (when each sample belongs exactly to one class) and categorical cross-entropy when one sample can have multiple classes or labels.\n\nThis allows conserving time and memory. Consider the case of 1000 classes when they are mutually exclusive – just 1 log instead of summing up 1000 for each sample, just one integer instead of 1000 floats.\n\nThe formula is the same in both cases, so no impact on accuracy should be there, sparse-cross-entropy is possibly cheaper in terms of computation \"\nSo if the formula is the same then we only have 3 labels which we can one-hot encode.","upvote_count":"1","timestamp":"1648639800.0"},{"comment_id":"568410","upvote_count":"3","poster":"baimus","timestamp":"1647350520.0","content":"Desite several incorrect statements that the answer is C, it is actually D. As stated in a subcomment below, C would be appropriate only if the problem allowed multiple classes per label, ie you could have id that was a drivers license and a passport simultaneously. As that is clearly impossible, D, which is for exclusive classes, is the correct choice."},{"content":"Selected Answer: C\nCommunity vote","timestamp":"1646015580.0","poster":"caohieu04","comment_id":"557789","upvote_count":"1"},{"timestamp":"1645306440.0","upvote_count":"1","poster":"SlipperySlope","comment_id":"551335","content":"D: use categorical cross-entropy for on-hot, sparce cce for label vectors"},{"timestamp":"1641520320.0","upvote_count":"2","comment_id":"518700","poster":"NamitSehgal","content":"C is for less classes, D is for large number of classes."},{"poster":"JobQ","comment_id":"505733","content":"Answer is C\n\nhttps://stats.stackexchange.com/questions/326065/cross-entropy-vs-sparse-cross-entropy-when-to-use-one-over-the-other","timestamp":"1640042580.0","upvote_count":"2"},{"upvote_count":"2","comment_id":"500075","timestamp":"1639321020.0","content":"I found this nice article that explains label map. \nhttps://towardsdatascience.com/what-is-a-label-map-f1066af6df70\n\nLabelmap will map classes to integers and doesn't include 0. Sparse categorical crossentropy will be the right choice in this case.","poster":"ashii007"},{"upvote_count":"1","content":"Answer B.\n\nFor multi-label classification, binary cross-entropy (sigmoid activation + cross-entropy loss) for each label is often used; although categorical cross-entropy (softmax activation + cross-entropy loss) may work; see the following article.\nhttps://gombru.github.io/2018/05/23/cross_entropy_loss/","comment_id":"473013","timestamp":"1636111140.0","poster":"ramen_lover"},{"upvote_count":"2","timestamp":"1634573220.0","content":"I guess it is C:\n\n- **Categorical entropy** is better to use when you want to **prevent the model from giving more importance to a certain class**. Or if the **classes are very unbalanced** you will get a better result by using Categorical entropy.\n\n- But **Sparse Categorical Entropy** is a more optimal coice if you have a huge amount of classes, enough to make a lot of memory usage, so since sparse categorical entropy uses less columns it **uses less memory**.","poster":"mousseUwU","comment_id":"464205"},{"content":"I would go for C.\nin both c and d you are doing encoding - in C labels are on-hot and in D are integer encoded. Sparse is useful when you have hundreds or thousands of labels, which is not the case here.","upvote_count":"2","poster":"pddddd","timestamp":"1632729960.0","comment_id":"452248"},{"poster":"gcper","comment_id":"442834","timestamp":"1631339160.0","upvote_count":"2","content":"C\n\nThe Sparse variant is used when there is a large number of classes that one should one-hot-encode. Here we only have 3 classes so this is not sparse."},{"timestamp":"1630757160.0","poster":"dxxdd7","content":"I hesitate between C and D as they fit perfectly in the case when you have multiple classes.\n\nI think I will go with C because Cross-entropy is supposed to be used when your targets are a vector (in our case, the vector looks like [passport, license_driver, credit card])\n\nSparse-cross-entropy should be use when your targets are integer\n\nBut I'm not really sure about it","comment_id":"439113","upvote_count":"2"},{"timestamp":"1626610740.0","content":"ANS: B\n\nFor multi-label classification, the last layer in the model uses a sigmoid function for label prediction, and the training process uses a binary_crossentropy function as the loss function.\nhttps://towardsdatascience.com/multi-label-classification-and-class-activation-map-on-fashion-mnist-1454f09f5925","upvote_count":"4","comment_id":"408915","poster":"celia20200410"}],"topic":"1","exam_id":13,"answer_images":[],"timestamp":"2021-06-07 18:11:00","unix_timestamp":1623082260,"answer_description":"","answer_ET":"C"},{"id":"uN5r7OLKWk39fAD29rxA","question_text":"You are an ML engineer at a manufacturing company. You need to build a model that identifies defects in products based on images of the product taken at the end of the assembly line. You want your model to preprocess the images with lower computation to quickly extract features of defects in products. Which approach should you use to build the model?","url":"https://www.examtopics.com/discussions/google/view/91724-exam-professional-machine-learning-engineer-topic-1-question/","choices":{"C":"Recurrent Neural Networks (RNN)","B":"Recommender system","A":"Reinforcement learning","D":"Convolutional Neural Networks (CNN)"},"answer_ET":"D","timestamp":"2022-12-15 17:03:00","answer_images":[],"question_images":[],"isMC":true,"answers_community":["D (100%)"],"discussion":[{"timestamp":"1740285660.0","poster":"bc3f222","content":"Selected Answer: D\nIf image then CNN, moreover other options not suitable for image problems, RNN is sequential so can be used for time series or as LSTM for text classification","comment_id":"1360410","upvote_count":"1"},{"comment_id":"1188040","poster":"MultiCloudIronMan","timestamp":"1727872740.0","upvote_count":"3","content":"Selected Answer: D\nCNN is commonly used for image classifications"},{"timestamp":"1700318040.0","poster":"Scipione_","content":"Selected Answer: D\nD for sure","upvote_count":"1","comment_id":"901202"},{"upvote_count":"1","timestamp":"1699514640.0","poster":"M25","content":"Selected Answer: D\nWent with D","comment_id":"892791"},{"poster":"TNT87","comment_id":"834323","timestamp":"1694280600.0","content":"Selected Answer: D\nAnswer D","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: D\nCNNs commonly used for image classification and recognition tasks.","comment_id":"821006","timestamp":"1692910320.0","poster":"FherRO"},{"content":"Selected Answer: D\nCNN scenario","comment_id":"817261","poster":"FherRO","upvote_count":"1","timestamp":"1692651540.0"},{"poster":"enghabeth","content":"Selected Answer: D\nbest way","upvote_count":"1","comment_id":"802817","timestamp":"1691547000.0"},{"comment_id":"750343","timestamp":"1687212480.0","poster":"hiromi","upvote_count":"1","content":"Selected Answer: D\nD\nCNN is good for images processing\n- https://developers.google.com/machine-learning/practica/image-classification/convolutional-neural-networks"},{"poster":"ares81","timestamp":"1686837780.0","content":"Selected Answer: D\nObviously D.","upvote_count":"2","comment_id":"746298"}],"answer":"D","unix_timestamp":1671120180,"topic":"1","question_id":3,"exam_id":13,"answer_description":""},{"id":"KwQPnU3nGa4jWMdyczkK","timestamp":"2022-12-16 11:34:00","choices":{"B":"Reduce the global batch size from 1024 to 256.","A":"Reduce the number of layers in the model architecture.","D":"Configure your model to use bfloat16 instead of float32.","C":"Reduce the dimensions of the images used in the model."},"answer":"D","answer_ET":"D","question_text":"You are developing an ML model intended to classify whether X-ray images indicate bone fracture risk. You have trained a ResNet architecture on Vertex AI using a TPU as an accelerator, however you are unsatisfied with the training time and memory usage. You want to quickly iterate your training code but make minimal changes to the code. You also want to minimize impact on the model’s accuracy. What should you do?","answer_images":[],"isMC":true,"discussion":[{"upvote_count":"8","content":"i think should be D\nhttps://cloud.google.com/tpu/docs/bfloat16","poster":"mymy9418","timestamp":"1686988800.0","comment_id":"747973"},{"timestamp":"1729920240.0","comment_id":"1202394","poster":"fitri001","upvote_count":"2","content":"Selected Answer: D\nConfiguring bfloat16 instead of float32 (D): This offers a good balance between speed, memory usage, and minimal code changes. Bfloat16 uses 16 bits per float value compared to 32 bits for float32.\n\npen_spark\nexpand_more This can significantly reduce memory usage while maintaining similar accuracy in many machine learning models, especially for image recognition tasks.expand_more It's a quick change with minimal impact on the code and potentially large gains in training speed."},{"comment_id":"1195792","upvote_count":"1","poster":"pinimichele01","content":"Selected Answer: D\n\"the Google hardware team chose bfloat16 for Cloud TPUs to improve hardware efficiency while maintaining the ability to train deep learning models accurately, all with minimal switching costs from float32\"","timestamp":"1728965460.0"},{"upvote_count":"1","content":"Selected Answer: B\nwhile reducing the global batch size (Option B) and configuring your model to use bfloat16 (Option D) are both valid options, reducing the global batch size is typically a safer and more straightforward choice to quickly iterate and make minimal changes to your code while still achieving reasonable model performance.","comments":[{"timestamp":"1710332880.0","poster":"pico","comment_id":"1006469","upvote_count":"1","content":"Why not D:\nNumerical Precision: bfloat16 has a lower numerical precision compared to float32\nCompatibility: Not all machine learning frameworks and libraries support bfloat16 natively.\nHyperparameter Tuning: When switching to bfloat16, you may need to adjust hyperparameters, such as learning rates and gradient clipping thresholds, to accommodate the lower numerical precision\nModel Architecture: Some model architectures and layers may be more sensitive to reduced precision than others.","comments":[{"timestamp":"1715318400.0","upvote_count":"2","content":"TPUs are optimized for operations with bfloat16 data types. By switching from float32 to bfloat16, you can benefit from the TPU's hardware acceleration capabilities, leading to faster computation and reduced memory usage without significant changes to your code.\n\nWhile bfloat16 offers a lower precision compared to float32, it maintains a similar dynamic range. This means that the reduction in numerical precision is unlikely to have a substantial impact on the accuracy of your model, especially in the context of image classification tasks like bone fracture risk assessment in X-rays.\n\nWhile reducing the batch size can decrease memory usage, it can also affect the model's convergence and accuracy. Additionally, TPUs are highly efficient with large batch sizes, so reducing the batch size might not fully leverage the TPU's capabilities.","comment_id":"1067010","poster":"tavva_prudhvi"}]}],"comment_id":"1006466","poster":"pico","timestamp":"1710332760.0"},{"content":"Selected Answer: D\nI think it should be D since they are using a TPU.https://cloud.google.com/tpu/docs/bfloat16","upvote_count":"1","comment_id":"915418","timestamp":"1701784920.0","poster":"Voyager2"},{"timestamp":"1699514700.0","content":"Selected Answer: D\nWent with D","upvote_count":"1","comment_id":"892792","poster":"M25"},{"poster":"tavva_prudhvi","content":"Selected Answer: D\nhttps://cloud.google.com/tpu/docs/bfloat16","timestamp":"1695832140.0","upvote_count":"1","comment_id":"852268"},{"upvote_count":"2","content":"Selected Answer: D\nAnswer D","poster":"TNT87","timestamp":"1694280540.0","comment_id":"834321"},{"upvote_count":"2","timestamp":"1692804900.0","comment_id":"819494","poster":"ailiba","content":"\"the Google hardware team chose bfloat16 for Cloud TPUs to improve hardware efficiency while maintaining the ability to train deep learning models accurately, all with minimal switching costs from float32\" so since its already trained on TPU, D maybe has no effect?"},{"content":"Selected Answer: D\nI go with D exactly, primarily. the rest don't make any sense at all","poster":"John_Pongthorn","timestamp":"1689510060.0","comment_id":"777763","upvote_count":"2"},{"timestamp":"1688390520.0","poster":"ares81","comment_id":"764805","content":"Selected Answer: D\nIt should be D.","upvote_count":"1"},{"poster":"hiromi","comment_id":"750346","upvote_count":"2","content":"Selected Answer: D\nD\nAgree with mymy9418","timestamp":"1687212900.0"},{"upvote_count":"1","comment_id":"749140","content":"Selected Answer: D\nAgree with D","poster":"mil_spyro","timestamp":"1687111440.0"},{"poster":"ares81","comment_id":"747056","content":"Selected Answer: B\nIt should be B.","upvote_count":"1","timestamp":"1686904440.0"}],"question_images":[],"exam_id":13,"url":"https://www.examtopics.com/discussions/google/view/91805-exam-professional-machine-learning-engineer-topic-1-question/","answer_description":"","answers_community":["D (88%)","13%"],"topic":"1","question_id":4,"unix_timestamp":1671186840},{"id":"teNtF3j9XqTsVenZXZNJ","answer_description":"","question_images":[],"question_text":"You have successfully deployed to production a large and complex TensorFlow model trained on tabular data. You want to predict the lifetime value (LTV) field for each subscription stored in the BigQuery table named subscription. subscriptionPurchase in the project named my-fortune500-company-project.\n\nYou have organized all your training code, from preprocessing data from the BigQuery table up to deploying the validated model to the Vertex AI endpoint, into a TensorFlow Extended (TFX) pipeline. You want to prevent prediction drift, i.e., a situation when a feature data distribution in production changes significantly over time. What should you do?","choices":{"C":"Add a model monitoring job where 90% of incoming predictions are sampled 24 hours.","B":"Add a model monitoring job where 10% of incoming predictions are sampled 24 hours.","A":"Implement continuous retraining of the model daily using Vertex AI Pipelines.","D":"Add a model monitoring job where 10% of incoming predictions are sampled every hour."},"answer_ET":"B","url":"https://www.examtopics.com/discussions/google/view/92165-exam-professional-machine-learning-engineer-topic-1-question/","discussion":[{"content":"Selected Answer: B\nSubscription LTV data doesn’t change rapidly → Hourly checks (D) are unnecessary.\nMonitoring 10% of data per day (B) is sufficient → Detects drift while minimizing cost.\nCost consideration → Hourly monitoring (D) increases expenses without significant added value for slow-changing data.","upvote_count":"2","timestamp":"1738893360.0","poster":"vini123","comment_id":"1352741"},{"content":"Selected Answer: D\nSampling predictions every hour will enable detect drift more quickly compared to daily sampling and react earlier.","comment_id":"1345763","timestamp":"1737683940.0","poster":"f9bc58e","upvote_count":"1"},{"content":"Selected Answer: D\nWhy D is correct:\n • Hourly monitoring ensures timely detection of prediction drift, which is critical in production systems.\n • Sampling 10% of predictions balances computational efficiency and detection accuracy.\n • Vertex AI model monitoring jobs support frequent sampling and provide detailed insights into feature distribution changes.\n\n\nA: Continuous retraining daily\nDaily retraining alone does not guarantee early detection of drift. Drift can happen and impact your predictions hours after your last retraining. Without monitoring, you might only discover the issue after a full day or more.","timestamp":"1734692220.0","comment_id":"1329411","upvote_count":"3","poster":"phani49"},{"poster":"f084277","content":"Selected Answer: A\nIt says PREVENT with no other constraints.","comment_id":"1311093","upvote_count":"2","timestamp":"1731473400.0"},{"upvote_count":"3","poster":"MultiCloudIronMan","comment_id":"1188134","content":"Selected Answer: B\nYou need to monitor it first and foremost to see if there is a drift and if there is then a measure can be devised. training every date is an over kill.","timestamp":"1712071440.0"},{"content":"Selected Answer: A\nContinuous Retraining: Continuously retraining the model allows it to adapt to changes in the data distribution, helping to mitigate prediction drift. Daily retraining provides a good balance between staying up-to-date and avoiding excessive retraining.\nOptions B, C, and D involve model monitoring but do not address the issue of keeping the model updated with the changing data distribution. Monitoring alone can help you detect drift, but it does not actively prevent it. Retraining the model is necessary to address drift effectively.","upvote_count":"3","timestamp":"1695626700.0","comments":[{"upvote_count":"1","poster":"maukaba","timestamp":"1698045960.0","comment_id":"1051549","content":"Option A can prevent drift prediction. All the other options can only detect. \nTherefore the correct answer is A unless it is possible to monitor drifts and then remediate without retrainings."},{"content":"Follow me on X (twitter): @nbcodes for more useful tips.\n\nI think you're slightly missing the point, the answer should be B, let me explain why..\n\nThe whole point of this question is to come up with a PREVENTATIVE way of handling prediction drift so you need to find a way to DETECT the drift before it occurs, this is exactly what solution B does and ensures it's done in a way that is not too frequent i.e D and not too resource intensive with the large sample i.e C remember if sampling is done well you don't need 90% of the data to detect drift.\n\nSolution A suggests retraining every day which is a CRAZY proposal, why would you retrain every day even if you don't know if your data is drifting?? Huge waste of resources and time.","comment_id":"1113845","timestamp":"1704384180.0","upvote_count":"2","poster":"Nish1729"}],"poster":"pico","comment_id":"1016574"},{"comment_id":"892793","upvote_count":"1","content":"Selected Answer: B\nWent with B","poster":"M25","timestamp":"1683609900.0"},{"content":"Selected Answer: B\nContinuous retraining (option A) is not necessarily the best solution for preventing prediction drift, as it can be time-consuming and expensive. Instead, monitoring the performance of the model in production is a better approach. Option B is a good choice because it samples a small percentage of incoming predictions and checks for any significant changes in the feature data distribution over a 24-hour period. This allows you to detect any drift and take appropriate action to address it before it affects the model's performance. Options C and D are less effective because they either sample too many or too few predictions and/or at too frequent intervals.","poster":"tavva_prudhvi","comment_id":"852270","upvote_count":"4","timestamp":"1679934660.0","comments":[{"poster":"andresvelasco","content":"I am just not sure why sampling too few (10%) is important. Is this a costly service?","comment_id":"1003882","comments":[{"content":"Model monitoring, especially at a large scale, can consume significant computational resources. Sampling a smaller percentage of predictions (like 10%) helps manage these resource demands and associated costs. The more predictions you sample, the more storage, computation, and network resources you'll need to analyze the data, potentially increasing the cost.\n\nIn many cases, a 10% sample of the data can provide statistically significant insights into the model's performance and the presence of drift. It's a balancing act between getting enough data to make informed decisions and not overburdening the system.\n\nIn some datasets, especially large ones, a lot of the data might be redundant or not particularly informative. Sampling a smaller fraction can help filter out noise and focus on the most relevant information.","upvote_count":"1","poster":"tavva_prudhvi","timestamp":"1699601160.0","comment_id":"1067016"}],"timestamp":"1694338620.0","upvote_count":"1"},{"poster":"pico","timestamp":"1699981980.0","content":"Neither B,C or D have a step to prevent the prediction drift. \n\nThe question says: \"you want to prevent prediction drift\"","upvote_count":"1","comment_id":"1070683"}]},{"content":"Selected Answer: B\nAnswer B","timestamp":"1678390140.0","upvote_count":"1","comment_id":"834320","poster":"TNT87"},{"upvote_count":"3","comment_id":"801698","content":"Selected Answer: B\nB , I got it from Machine Learning in the Enterprise course for google partnet skillboost\nyou can watch cafully on video \"Model management using Vertex AI\"\nI imply that it is default setting on typical case.","timestamp":"1675836120.0","poster":"John_Pongthorn"},{"comment_id":"768583","upvote_count":"1","content":"Selected Answer: D\nUsing 10% of hourly requests would yield a better distribution and faster feed back loop","poster":"behzadsw","timestamp":"1673097900.0"},{"timestamp":"1672198440.0","content":"I think it is B, we can say 10% to be a sample but not 90%","poster":"hargur","comment_id":"759322","upvote_count":"2"},{"content":"Selected Answer: B\nI guess 10% of 24 hours should be good enough?","upvote_count":"3","poster":"mymy9418","timestamp":"1671613260.0","comment_id":"752037"},{"comment_id":"750764","content":"Selected Answer: B\nB (not sure)\n- https://cloud.google.com/vertex-ai/docs/model-monitoring/overview\n- https://cloud.google.com/vertex-ai/docs/model-monitoring/using-model-monitoring#drift-detection","poster":"hiromi","upvote_count":"2","timestamp":"1671532560.0"}],"isMC":true,"answers_community":["B (66%)","D (17%)","A (17%)"],"answer_images":[],"unix_timestamp":1671532560,"timestamp":"2022-12-20 11:36:00","exam_id":13,"topic":"1","answer":"B","question_id":5}],"exam":{"lastUpdated":"11 Apr 2025","numberOfQuestions":304,"id":13,"isBeta":false,"provider":"Google","isImplemented":true,"isMCOnly":true,"name":"Professional Machine Learning Engineer"},"currentPage":1},"__N_SSP":true}