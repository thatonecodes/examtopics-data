{"pageProps":{"questions":[{"id":"Llp6jAGK3HN6v7KnHTyP","url":"https://www.examtopics.com/discussions/google/view/17263-exam-professional-data-engineer-topic-1-question-82/","discussion":[{"content":"Correct - B","timestamp":"1584897420.0","poster":"[Removed]","upvote_count":"19","comment_id":"67030"},{"poster":"SamuelTsch","comment_id":"1302112","timestamp":"1729699920.0","upvote_count":"1","content":"Selected Answer: B\npartitioned table is more performancer than sharded tables"},{"comment_id":"948590","upvote_count":"1","poster":"sspsp","timestamp":"1689040140.0","content":"Selected Answer: B\nB, Partition tables in BQ have different cost. If a partition is not modified (DML) for 90 days then cost will be less by 50%, while querying will be efficient since its single large table."},{"upvote_count":"1","comment_id":"725171","content":"Selected Answer: B\nalways partion large tables","timestamp":"1669210980.0","poster":"piotrpiskorski"},{"timestamp":"1636795080.0","poster":"Thierry_1","content":"B for sure","comment_id":"477365","upvote_count":"3"},{"upvote_count":"3","comment_id":"445555","poster":"nguyenmoon","content":"Correct is B","timestamp":"1631753820.0"},{"comment_id":"421776","content":"Option B for sure","upvote_count":"2","poster":"sandipk91","timestamp":"1628446080.0"},{"timestamp":"1625499540.0","upvote_count":"2","content":"https://cloud.google.com/bigquery/docs/partitioned-tables#dt_partition_shard - Supports B","poster":"awssp12345","comment_id":"399258"},{"timestamp":"1625088780.0","poster":"sumanshu","comment_id":"395235","upvote_count":"1","content":"Vote for 'B' Partitioned Table for Faster Query and Low cost (because it will process less data)"},{"poster":"alonsoRios","comment_id":"320800","upvote_count":"2","content":"B is correct","timestamp":"1616735340.0"},{"content":"Correct - B","poster":"fabenavideso","comment_id":"257651","upvote_count":"2","timestamp":"1609605540.0"},{"comments":[{"timestamp":"1611018960.0","poster":"lammingtons","comment_id":"270852","upvote_count":"3","content":"They're using BigQuery so partitioning is the better choice here. B"}],"comment_id":"239915","timestamp":"1607590680.0","content":"should be C","upvote_count":"1","poster":"ceak"},{"upvote_count":"3","comment_id":"162363","content":"B is correct","timestamp":"1597941960.0","poster":"haroldbenites"}],"isMC":true,"choices":{"B":"Create a partitioned table called tracking_table and include a TIMESTAMP column.","D":"Create a table called tracking_table with a TIMESTAMP column to represent the day.","A":"Create a table called tracking_table and include a DATE column.","C":"Create sharded tables for each day following the pattern tracking_table_YYYYMMDD."},"answer":"B","answers_community":["B (100%)"],"unix_timestamp":1584897420,"answer_ET":"B","topic":"1","answer_images":[],"question_images":[],"answer_description":"","question_id":301,"question_text":"MJTelco Case Study -\n\nCompany Overview -\nMJTelco is a startup that plans to build networks in rapidly growing, underserved markets around the world. The company has patents for innovative optical communications hardware. Based on these patents, they can create many reliable, high-speed backbone links with inexpensive hardware.\n\nCompany Background -\nFounded by experienced telecom executives, MJTelco uses technologies originally developed to overcome communications challenges in space. Fundamental to their operation, they need to create a distributed data infrastructure that drives real-time analysis and incorporates machine learning to continuously optimize their topologies. Because their hardware is inexpensive, they plan to overdeploy the network allowing them to account for the impact of dynamic regional politics on location availability and cost.\nTheir management and operations teams are situated all around the globe creating many-to-many relationship between data consumers and provides in their system. After careful consideration, they decided public cloud is the perfect environment to support their needs.\n\nSolution Concept -\nMJTelco is running a successful proof-of-concept (PoC) project in its labs. They have two primary needs:\n✑ Scale and harden their PoC to support significantly more data flows generated when they ramp to more than 50,000 installations.\n✑ Refine their machine-learning cycles to verify and improve the dynamic models they use to control topology definition.\nMJTelco will also use three separate operating environments `\" development/test, staging, and production `\" to meet the needs of running experiments, deploying new features, and serving production customers.\n\nBusiness Requirements -\n✑ Scale up their production environment with minimal cost, instantiating resources when and where needed in an unpredictable, distributed telecom user community.\n✑ Ensure security of their proprietary data to protect their leading-edge machine learning and analysis.\n✑ Provide reliable and timely access to data for analysis from distributed research workers\n✑ Maintain isolated environments that support rapid iteration of their machine-learning models without affecting their customers.\n\nTechnical Requirements -\nEnsure secure and efficient transport and storage of telemetry data\nRapidly scale instances to support between 10,000 and 100,000 data providers with multiple flows each.\nAllow analysis and presentation against data tables tracking up to 2 years of data storing approximately 100m records/day\nSupport rapid iteration of monitoring infrastructure focused on awareness of data pipeline problems both in telemetry flows and in production learning cycles.\n\nCEO Statement -\nOur business model relies on our patents, analytics and dynamic machine learning. Our inexpensive hardware is organized to be highly reliable, which gives us cost advantages. We need to quickly stabilize our large distributed data pipelines to meet our reliability and capacity commitments.\n\nCTO Statement -\nOur public cloud services must operate as advertised. We need resources that scale and keep our data secure. We also need environments in which our data scientists can carefully study and quickly adapt our models. Because we rely on automation to process our data, we also need our development and test environments to work as we iterate.\n\nCFO Statement -\nThe project is too large for us to maintain the hardware and software required for the data and analysis. Also, we cannot afford to staff an operations team to monitor so many data feeds, so we will rely on automation and infrastructure. Google Cloud's machine learning will allow our quantitative researchers to work on our high-value problems instead of problems with our data pipelines.\nGiven the record streams MJTelco is interested in ingesting per day, they are concerned about the cost of Google BigQuery increasing. MJTelco asks you to provide a design solution. They require a single large data table called tracking_table. Additionally, they want to minimize the cost of daily queries while performing fine-grained analysis of each day's events. They also want to use streaming ingestion. What should you do?","timestamp":"2020-03-22 18:17:00","exam_id":11},{"id":"ZQnRQ05NvdWSfjmjbmow","unix_timestamp":1584897180,"timestamp":"2020-03-22 18:13:00","exam_id":11,"discussion":[{"upvote_count":"25","content":"Seems like A..Data should ingest from multiple sources which might be real time or batch .","poster":"digvijay","timestamp":"1632472440.0","comment_id":"67663","comments":[{"upvote_count":"3","comment_id":"400518","timestamp":"1673077140.0","content":"How is it possible to query in real time with option A. It needs Dataflow","poster":"navemula","comments":[{"timestamp":"1673077200.0","upvote_count":"2","comment_id":"400519","poster":"navemula","content":"To use Dataflow SQL it needs BigQuery"}]}]},{"comments":[{"comment_id":"399260","poster":"awssp12345","timestamp":"1672940820.0","content":"These exams make people over analyse. People who vote A earlier in 35 seem to be confused here.. haha","upvote_count":"1"},{"comment_id":"265514","poster":"StelSen","content":"Well Done mikey007, Many people have already answered as A.","upvote_count":"2","timestamp":"1657624080.0"}],"upvote_count":"11","comment_id":"179533","poster":"mikey007","timestamp":"1647296040.0","content":"Repeated Question see ques 35"},{"timestamp":"1719490620.0","comment_id":"758602","poster":"Kyr0","content":"Selected Answer: A\nAnswer is A","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: A\nIt's A","poster":"cloudmon","comment_id":"712574","timestamp":"1715014020.0"},{"upvote_count":"1","poster":"ducc","timestamp":"1709165940.0","content":"Selected Answer: A\nA is the answer","comment_id":"653669"},{"comment_id":"550672","upvote_count":"2","content":"ingest data from a variety of global sources - cloud pub/sub\nprocess and query in real-time - cloud Dataflow\nstore the data reliably - Cloud Storage","poster":"RRK2021","timestamp":"1692414780.0"},{"poster":"medeis_jar","comment_id":"518465","timestamp":"1688660820.0","content":"Selected Answer: A\nPubSub (for global ingestion from multiple sources) + Dataflow (for process and query) + reliable (gcs).","upvote_count":"1"},{"comment_id":"483867","timestamp":"1684724520.0","poster":"lifebegins","upvote_count":"1","content":"Selected Answer: A\nusing Dataflow you can apply the propriety analytics and you can push the data in to Cloud storage"},{"upvote_count":"1","comment_id":"466261","poster":"gcp_k","content":"Also read the technical requirements section. Not just the last 3 lines of the question. \n\nWhen you do that, you'll know the answer is PubSub (for global ingestion) + Dataflow (for process and query) + reliable (gcs).\n\nAnswer is: A","timestamp":"1682186340.0"},{"timestamp":"1681011180.0","comments":[{"poster":"cualquiernick","upvote_count":"1","timestamp":"1703219460.0","content":"Cloud SQL, is not suitable and efficient for storing real time data ingested from PUB/SUB, so A is the answer","comment_id":"620107"}],"comment_id":"459475","poster":"ManojT","content":"Answer C:\nLook the 3 requirement in the question \"ingest data from a variety of global sources, process and query in real-time, and store the data reliably\"\nIngest data from global sources: Pub-Sub\nProcess and Query in realtime: Cloud SQL\nStore reliably: Cloud storage\nI can understand Databflow is required in case you need to analyze and transform data but question does not refer it.","upvote_count":"1"},{"upvote_count":"1","content":"Correct is A. \nKafka --> replace by PubSub, Streaming then Dataflow, store data reliably and not mention any other condition then Cloud Storage","timestamp":"1678935840.0","poster":"nguyenmoon","comment_id":"445557"},{"content":"Vote for 'A'\n\nSQL - will not handle the volume","timestamp":"1672530360.0","upvote_count":"2","poster":"sumanshu","comment_id":"395240"},{"upvote_count":"1","poster":"daghayeghi","comment_id":"308137","content":"Dataflow SQL cannot output to cloud storage:\nhttps://cloud.google.com/dataflow/docs/guides/sql/data-sources-destinations\nbut the main problem is that Cloud SQL can't do process, then response is A or C.","timestamp":"1662910440.0"},{"timestamp":"1648540620.0","content":"A\nI don't expect this question to come up, but if I had to write the answer, it would be A.\nThe problem statement \"Flowlogistic's management has determined that the current Apache Kafka servers cannot handle the\ndata volume for their real-time inventory tracking system.\nAs it says, \"we cannot determine the data volume\", but it doesn't say that we can't calculate it either.\n\nRequirement definition: The system must be able to\ningest data from a variety of global sources\nprocess and query in real-time\nStore the data reliably. \n\nIt says above, if you look at the Google page.\n\nLogging to multiple systems. for example, a Google Compute Engine instance can write logs to a monitoring system, to a database for later querying, and so on.\nhttps://cloud.google.com/pubsub/docs/overview#scenarios\n\nstream processing with Dataflow\nhttps://cloud.google.com/pubsub/docs/pubsub-dataflow?hl=en-419\n\nThe answer is A, since it is stated above.","poster":"kino2020","comment_id":"189517","upvote_count":"4"},{"comment_id":"183053","timestamp":"1647795600.0","comments":[{"timestamp":"1649555400.0","comment_id":"197072","content":"Dataflow SQL cannot output to cloud storage only BigQuery...so I am confused on this one.","poster":"aleedrew","upvote_count":"2","comments":[{"content":"https://cloud.google.com/pubsub/docs/pubsub-dataflow... It is possible to load the data to Cloud Storage. Can refer to above docs.","comment_id":"301412","timestamp":"1662036900.0","upvote_count":"1","comments":[{"upvote_count":"1","content":"he said correct, DataflowDataflow SQL cannot output to cloud storage:\nhttps://cloud.google.com/dataflow/docs/guides/sql/data-sources-destinations","timestamp":"1662910140.0","comments":[{"upvote_count":"2","content":"Answer should be C then?","poster":"Ral17","timestamp":"1678126020.0","comment_id":"440450"}],"poster":"daghayeghi","comment_id":"308133"}],"poster":"Jay3244"}]}],"content":"A. SQL queries can be written in Dataflow too.\nhttps://cloud.google.com/dataflow/docs/guides/sql/dataflow-sql-intro#running-queries","poster":"vakati","upvote_count":"3"},{"comment_id":"177197","poster":"kuntal8285","content":"should be E","upvote_count":"1","timestamp":"1646934840.0"},{"comment_id":"175539","poster":"Tanmoyk","timestamp":"1646714280.0","upvote_count":"2","content":"Should be A ...data need to feed to the propriority system and for that dataflow is required."},{"content":"I´m considering \"C\". A Pub/sub push subscription can be used to ingest data on Cloud SQL through an HTTP service.","timestamp":"1646221200.0","upvote_count":"1","poster":"lgdantas","comment_id":"171955"},{"content":"Pub/Sub -> Dataflow -> [BigQuery or Bigtable]","timestamp":"1645383000.0","upvote_count":"3","poster":"haroldbenites","comment_id":"162367"},{"upvote_count":"3","comment_id":"126861","poster":"Rajuuu","timestamp":"1641395880.0","content":"A ..Need Dataflow to process the data."},{"upvote_count":"1","poster":"cleroy","timestamp":"1641022200.0","comment_id":"123939","content":"I think it's C for query in realtime with Cloud SQL... But for process from multiple sources, we also need Dataflow"},{"timestamp":"1640086020.0","upvote_count":"6","comment_id":"115425","poster":"ch3n6","content":"A; need dataflow"},{"upvote_count":"3","poster":"[Removed]","timestamp":"1632323580.0","comment_id":"67027","content":"should be A / C"}],"question_text":"Flowlogistic Case Study -\n\nCompany Overview -\nFlowlogistic is a leading logistics and supply chain provider. They help businesses throughout the world manage their resources and transport them to their final destination. The company has grown rapidly, expanding their offerings to include rail, truck, aircraft, and oceanic shipping.\n\nCompany Background -\nThe company started as a regional trucking company, and then expanded into other logistics market. Because they have not updated their infrastructure, managing and tracking orders and shipments has become a bottleneck. To improve operations, Flowlogistic developed proprietary technology for tracking shipments in real time at the parcel level. However, they are unable to deploy it because their technology stack, based on Apache Kafka, cannot support the processing volume. In addition, Flowlogistic wants to further analyze their orders and shipments to determine how best to deploy their resources.\n\nSolution Concept -\nFlowlogistic wants to implement two concepts using the cloud:\n✑ Use their proprietary technology in a real-time inventory-tracking system that indicates the location of their loads\n✑ Perform analytics on all their orders and shipment logs, which contain both structured and unstructured data, to determine how best to deploy resources, which markets to expand info. They also want to use predictive analytics to learn earlier when a shipment will be delayed.\n\nExisting Technical Environment -\nFlowlogistic architecture resides in a single data center:\n✑ Databases\n- 8 physical servers in 2 clusters\n- SQL Server `\" user data, inventory, static data\n- 3 physical servers\n- Cassandra `\" metadata, tracking messages\n10 Kafka servers `\" tracking message aggregation and batch insert\n✑ Application servers `\" customer front end, middleware for order/customs\n- 60 virtual machines across 20 physical servers\n- Tomcat `\" Java services\n- Nginx `\" static content\n- Batch servers\n✑ Storage appliances\n- iSCSI for virtual machine (VM) hosts\n- Fibre Channel storage area network (FC SAN) `\" SQL server storage\nNetwork-attached storage (NAS) image storage, logs, backups\n✑ 10 Apache Hadoop /Spark servers\n- Core Data Lake\n- Data analysis workloads\n✑ 20 miscellaneous servers\n- Jenkins, monitoring, bastion hosts,\n\nBusiness Requirements -\n✑ Build a reliable and reproducible environment with scaled panty of production.\n✑ Aggregate data in a centralized Data Lake for analysis\n✑ Use historical data to perform predictive analytics on future shipments\n✑ Accurately track every shipment worldwide using proprietary technology\n✑ Improve business agility and speed of innovation through rapid provisioning of new resources\n✑ Analyze and optimize architecture for performance in the cloud\n✑ Migrate fully to the cloud if all other requirements are met\n\nTechnical Requirements -\n✑ Handle both streaming and batch data\n✑ Migrate existing Hadoop workloads\n✑ Ensure architecture is scalable and elastic to meet the changing demands of the company.\n✑ Use managed services whenever possible\n✑ Encrypt data flight and at rest\nConnect a VPN between the production data center and cloud environment\n\nSEO Statement -\nWe have grown so quickly that our inability to upgrade our infrastructure is really hampering further growth and efficiency. We are efficient at moving shipments around the world, but we are inefficient at moving data around.\nWe need to organize our information so we can more easily understand where our customers are and what they are shipping.\n\nCTO Statement -\nIT has never been a priority for us, so as our data has grown, we have not invested enough in our technology. I have a good staff to manage IT, but they are so busy managing our infrastructure that I cannot get them to do the things that really matter, such as organizing our data, building the analytics, and figuring out how to implement the CFO' s tracking technology.\n\nCFO Statement -\nPart of our competitive advantage is that we penalize ourselves for late shipments and deliveries. Knowing where out shipments are at all times has a direct correlation to our bottom line and profitability. Additionally, I don't want to commit capital to building out a server environment.\nFlowlogistic's management has determined that the current Apache Kafka servers cannot handle the data volume for their real-time inventory tracking system.\nYou need to build a new system on Google Cloud Platform (GCP) that will feed the proprietary tracking software. The system must be able to ingest data from a variety of global sources, process and query in real-time, and store the data reliably. Which combination of GCP products should you choose?","question_id":302,"answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/17262-exam-professional-data-engineer-topic-1-question-83/","answer":"A","choices":{"E":"Cloud Dataflow, Cloud SQL, and Cloud Storage","D":"Cloud Load Balancing, Cloud Dataflow, and Cloud Storage","B":"Cloud Pub/Sub, Cloud Dataflow, and Local SSD","A":"Cloud Pub/Sub, Cloud Dataflow, and Cloud Storage","C":"Cloud Pub/Sub, Cloud SQL, and Cloud Storage"},"answer_ET":"A","answer_description":"","answers_community":["A (100%)"],"question_images":[],"topic":"1","isMC":true},{"id":"c5qvscJG9ebiNSTsL4mF","discussion":[{"comment_id":"65076","timestamp":"1584430260.0","poster":"rickywck","content":"C is the only way which all records will be compared.","upvote_count":"33","comments":[{"content":"Agree with your argument","poster":"odacir","upvote_count":"2","comment_id":"737828","timestamp":"1670415000.0"}]},{"upvote_count":"16","content":"Answer: C\nDescription: Full comparison with this option, rest are comparison on sample which doesnot ensure all the data will be ok","timestamp":"1585368600.0","comment_id":"68743","poster":"[Removed]"},{"timestamp":"1729700280.0","content":"Selected Answer: C\nHash is always a good idea to compare the data","upvote_count":"1","comment_id":"1302115","poster":"SamuelTsch"},{"upvote_count":"3","poster":"midgoo","comment_id":"826548","timestamp":"1677739500.0","content":"In practice, I will do B. That means it may have error due to randomness. But that is how we normally do validation/QA in general, i.e. we test random samples\n\nIn this question, I will do C."},{"timestamp":"1676458920.0","poster":"musumusu","content":"key words here- Hash or collect value on \"EACH table\", after sorting the table. \nOption C","upvote_count":"1","comment_id":"809398"},{"timestamp":"1675037040.0","poster":"samdhimal","content":"C. Use a Dataproc cluster and the BigQuery Hadoop connector to read the data from each table and calculate a hash from non-timestamp columns of the table after sorting. Compare the hashes of each table. This approach will ensure that the data is read in a consistent order, and the hash function will provide a quick and efficient way to compare the contents of the tables and ensure that they are identical.","comments":[{"comment_id":"792201","content":"A. Selecting random samples from the tables using the RAND() function may not provide an accurate representation of the data and there is a risk that the comparison will not identify any differences between the tables.\n\nB. Selecting random samples from the tables using the HASH() function may not be an effective method for comparison, as the HASH() function may return different results for equivalent data.\n\nD. Creating stratified random samples using the OVER() function may not provide a comprehensive comparison between the tables as there is a risk that important differences could be missed in the sample data.","timestamp":"1675038180.0","upvote_count":"2","poster":"samdhimal"}],"comment_id":"792191","upvote_count":"1"},{"comments":[{"poster":"odacir","timestamp":"1670415060.0","upvote_count":"1","comment_id":"737830","content":"All records need to be checked to be sure, so C is the answer"}],"poster":"zellck","comment_id":"736813","upvote_count":"2","timestamp":"1670330340.0","content":"Selected Answer: C\nC is the answer."},{"comment_id":"727757","timestamp":"1669488360.0","content":"Selected Answer: C\nAll records","upvote_count":"1","poster":"Leeeeee"},{"comments":[{"comment_id":"712576","upvote_count":"2","timestamp":"1667760600.0","poster":"cloudmon","content":"You must have meant to say C"}],"poster":"hfuihe","comment_id":"690925","timestamp":"1665393720.0","upvote_count":"1","content":"Selected Answer: B\nB is the only way which all records will be compared."},{"upvote_count":"1","comments":[{"upvote_count":"1","content":"The hash in answer C is used to select a sample of the table, not to compare them","poster":"stefanop","comments":[{"content":"Ignore my comment, it was about answer B.\nI suggest you to go with answer C which is the only solution comparing all the rows/tables","comment_id":"594491","poster":"stefanop","timestamp":"1651239060.0","upvote_count":"1"}],"timestamp":"1651238880.0","comment_id":"594489"}],"comment_id":"518467","timestamp":"1641493740.0","poster":"medeis_jar","content":"Selected Answer: C\nHASH() to compare data skipping dates and timestamps"},{"timestamp":"1640242920.0","comment_id":"507645","upvote_count":"2","content":"Selected Answer: C\noptions A B and D only will determine that it “might” be identical since is only a sample. HASH() can be helpful when doing bulk comparisons, but you still have to compare field by field to get the final answer.\nThe only one left is C which looks good to me","poster":"MaxNRG"},{"comment_id":"474450","poster":"JayZeeLee","content":"C. \nThe rest use RAND() at some point, which makes it hard to compare for consistency, unless there's a 'seed' option, which wasn't mentioned. So C.","timestamp":"1636399380.0","upvote_count":"1"},{"content":"Since there is no PK and it is possible that set of values is commons in some records which result in same hashkey for those records. But still Anwer is C","poster":"u_t_s","comment_id":"458758","upvote_count":"3","timestamp":"1633612800.0"},{"comment_id":"395241","poster":"sumanshu","content":"Vote for 'C\"","upvote_count":"1","timestamp":"1625089920.0"},{"content":"B:\nBecause said migrated to BigQuery, then we don't need Dataproc, and samples don't mean you don't compare all of data.","poster":"daghayeghi","timestamp":"1613519220.0","comment_id":"292191","upvote_count":"3","comments":[{"poster":"yoshik","comment_id":"447133","upvote_count":"1","content":"a sample is a subset of data. then you should assure that the union of the samples contain the data set. Excessively complicated.\nYou migrate to BigQuery but need to check BigQuery output, that is why you should use another tool, Dataproc in this case.\nAgree that then you should control Dataproc output but suppositions are becoming too many.","timestamp":"1631975040.0"}]},{"poster":"atnafu2020","upvote_count":"3","comment_id":"163290","timestamp":"1598062320.0","content":"C\nUsing Cloud Storage with big data\n\nCloud Storage is a key part of storing and working with Big Data on Google Cloud. Examples include:\n\n Loading data into BigQuery.\n\n Using Dataproc, which automatically installs the HDFS-compatible Cloud Storage connector, enabling the use of Cloud Storage buckets in parallel with HDFS.\n\n Using a bucket to hold staging files and temporary data for Dataflow pipelines.\n\nFor Dataflow, a Cloud Storage bucket is required. For BigQuery and Dataproc, using a Cloud Storage bucket is optional but recommended.\n\ngsutil is a command-line tool that enables you to work with Cloud Storage buckets and objects easily and robustly, in particular in big data scenarios. For example, with gsutil you can copy many files in parallel with a single command, copy large files efficiently, calculate checksums on your data, and measure performance from your local computer to Cloud Storage."},{"comment_id":"162369","comments":[{"timestamp":"1597942380.0","content":"It Says: \"...that they are identical.\" , You must not use sample.","poster":"haroldbenites","comment_id":"162370","upvote_count":"3"}],"poster":"haroldbenites","upvote_count":"4","timestamp":"1597942260.0","content":"C is correct"},{"poster":"Rajuuu","timestamp":"1593955200.0","content":"C is correct.","comment_id":"126866","upvote_count":"4"},{"upvote_count":"4","comment_id":"119053","content":"Option [C] is most appropriate","timestamp":"1593054240.0","poster":"dambilwa"},{"content":"Should be C","comment_id":"67026","poster":"[Removed]","timestamp":"1584896760.0","upvote_count":"6"}],"question_images":[],"url":"https://www.examtopics.com/discussions/google/view/16835-exam-professional-data-engineer-topic-1-question-84/","question_text":"After migrating ETL jobs to run on BigQuery, you need to verify that the output of the migrated jobs is the same as the output of the original. You've loaded a table containing the output of the original job and want to compare the contents with output from the migrated job to show that they are identical. The tables do not contain a primary key column that would enable you to join them together for comparison.\nWhat should you do?","unix_timestamp":1584430260,"topic":"1","timestamp":"2020-03-17 08:31:00","answer_ET":"C","answer_images":[],"answer_description":"","choices":{"A":"Select random samples from the tables using the RAND() function and compare the samples.","C":"Use a Dataproc cluster and the BigQuery Hadoop connector to read the data from each table and calculate a hash from non-timestamp columns of the table after sorting. Compare the hashes of each table.","B":"Select random samples from the tables using the HASH() function and compare the samples.","D":"Create stratified random samples using the OVER() function and compare equivalent samples from each table."},"isMC":true,"answers_community":["C (88%)","13%"],"exam_id":11,"answer":"C","question_id":303},{"id":"32NjDFbUpiPhL3SnzqEp","discussion":[{"comment_id":"675304","poster":"sedado77","content":"Selected Answer: C\nI got this question on sept 2022. Answer is C","timestamp":"1711045020.0","upvote_count":"8"},{"comment_id":"1342533","upvote_count":"1","timestamp":"1737207480.0","content":"Selected Answer: C\nIn BigQuery, \"on-demand pricing\" means you pay based on the amount of data your queries scan (bytes processed), essentially paying for what you use, while \"flat-rate pricing\" involves purchasing a set number of \"slots\" (virtual CPUs) and paying a fixed fee regardless of how much data you query, essentially providing a predictable monthly cost for dedicated processing power; on-demand is best for occasional users with variable query needs, while flat-rate is better for predictable high-volume querying","poster":"grshankar9"},{"content":"answer us C","upvote_count":"1","timestamp":"1730237820.0","comment_id":"884652","poster":"email2nn"},{"content":"Selected Answer: C\nThis question is interesting.\nMy friend works as the TAM in Google and he said we could request for Quota increase if the customer is premium customer instead of changing to Flat-rate\nOtherwise, need to choose C","comment_id":"847069","poster":"midgoo","timestamp":"1727001840.0","upvote_count":"1"},{"timestamp":"1725168000.0","upvote_count":"1","comment_id":"825601","poster":"jin0","content":"why A is not a answer? when using interactive bigquery without batch bigquery it lead to run query immediately isn't it? so it seems to solve the problems isn't it?"},{"comment_id":"794559","comments":[{"timestamp":"1722447720.0","comment_id":"794560","content":"Switching to flat-rate pricing would allow you to ensure a consistent level of service and avoid running into the on-demand slot quota per project. Additionally, by establishing a hierarchical priority model for your projects, you could allocate resources based on the specific needs and priorities of each business unit, ensuring that the most critical queries are executed first. This approach would allow you to balance the needs of each business unit while maximizing the use of your BigQuery resources.","upvote_count":"4","poster":"samdhimal"}],"upvote_count":"3","timestamp":"1722447720.0","poster":"samdhimal","content":"C. Switch to flat-rate pricing and establish a hierarchical priority model for your projects."},{"comment_id":"736809","content":"Selected Answer: C\nC is the answer.\n\nhttps://cloud.google.com/bigquery/docs/reservations-intro\nThe benefits of using BigQuery Reservations include:\n- Workload management. After you purchase slots, you can allocate them to workloads. That way, a workload has a dedicated pool of BigQuery computational resources available for use. At the same time, if a workload doesn't use all of its allocated slots, any unused slots are shared automatically across your other workloads.\n- Centralized purchasing: You can purchase and allocate slots for your entire organization. You don't need to purchase slots for each project that uses BigQuery.","upvote_count":"2","timestamp":"1717670280.0","poster":"zellck"},{"comment_id":"732475","content":"C. https://cloud.google.com/bigquery/quotas - 2000 is the max no. of slots","poster":"hybridpro","timestamp":"1717228800.0","upvote_count":"1"}],"answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/83115-exam-professional-data-engineer-topic-1-question-85/","answer":"C","question_text":"You are a head of BI at a large enterprise company with multiple business units that each have different priorities and budgets. You use on-demand pricing for\nBigQuery with a quota of 2K concurrent on-demand slots per project. Users at your organization sometimes don't get slots to execute their query and you need to correct this. You'd like to avoid introducing new projects to your account.\nWhat should you do?","exam_id":11,"timestamp":"2022-09-21 18:17:00","answers_community":["C (100%)"],"answer_description":"","topic":"1","choices":{"C":"Switch to flat-rate pricing and establish a hierarchical priority model for your projects.","B":"Create an additional project to overcome the 2K on-demand per-project quota.","A":"Convert your batch BQ queries into interactive BQ queries.","D":"Increase the amount of concurrent slots per project at the Quotas page at the Cloud Console."},"isMC":true,"unix_timestamp":1663777020,"question_images":[],"question_id":304,"answer_ET":"C"},{"id":"V3y7mG3fhERrwMUvUKuu","question_text":"You have an Apache Kafka cluster on-prem with topics containing web application logs. You need to replicate the data to Google Cloud for analysis in BigQuery and Cloud Storage. The preferred replication method is mirroring to avoid deployment of Kafka Connect plugins.\nWhat should you do?","choices":{"C":"Deploy the Pub/Sub Kafka connector to your on-prem Kafka cluster and configure Pub/Sub as a Source connector. Use a Dataflow job to read from Pub/Sub and write to GCS.","D":"Deploy the Pub/Sub Kafka connector to your on-prem Kafka cluster and configure Pub/Sub as a Sink connector. Use a Dataflow job to read from Pub/Sub and write to GCS.","A":"Deploy a Kafka cluster on GCE VM Instances. Configure your on-prem cluster to mirror your topics to the cluster running in GCE. Use a Dataproc cluster or Dataflow job to read from Kafka and write to GCS.","B":"Deploy a Kafka cluster on GCE VM Instances with the Pub/Sub Kafka connector configured as a Sink connector. Use a Dataproc cluster or Dataflow job to read from Kafka and write to GCS."},"question_images":[],"question_id":305,"answer_images":[],"answer":"A","isMC":true,"topic":"1","answers_community":["A (100%)"],"unix_timestamp":1584847200,"answer_description":"","answer_ET":"A","url":"https://www.examtopics.com/discussions/google/view/17173-exam-professional-data-engineer-topic-1-question-86/","discussion":[{"comment_id":"73525","poster":"Ganshank","content":"A.\nhttps://cwiki.apache.org/confluence/pages/viewpage.action?pageId=27846330\nThe solution specifically mentions mirroring and minimizing the use of Kafka Connect plugin.\nD would be the more Google Cloud-native way of implementing the same, but the requirement is better met by A.","upvote_count":"34","timestamp":"1634011680.0"},{"comment_id":"68745","content":"Answer: A\nDescription: Question says mirroring and avoid kafka connect plugins","poster":"[Removed]","timestamp":"1632795300.0","upvote_count":"11"},{"comment_id":"916994","poster":"Qix","upvote_count":"4","timestamp":"1733565720.0","content":"Pub/Sub Kafka connector requires Kafka Connect, as described here https://cloud.google.com/pubsub/docs/connect_kafka\nDeployment of Kafka Connect is explicitly excluded by the requirements. So the only option available is A"},{"upvote_count":"3","timestamp":"1722790740.0","poster":"samdhimal","comment_id":"798270","content":"Option A: Deploy a Kafka cluster on GCE VM Instances. Configure your on-prem cluster to mirror your topics to the cluster running in GCE. Use a Dataproc cluster or Dataflow job to read from Kafka and write to GCS.\n\nThis option involves setting up a separate Kafka cluster in Google Cloud, and then configuring the on-prem cluster to mirror the topics to this cluster. The data from the Google Cloud Kafka cluster can then be read using either a Dataproc cluster or a Dataflow job and written to Cloud Storage for analysis in BigQuery.","comments":[{"content":"Option B: Deploy a Kafka cluster on GCE VM Instances with the Pub/Sub Kafka connector configured as a Sink connector. Use a Dataproc cluster or Dataflow job to read from Kafka and write to GCS.\n\nThis option is similar to Option A, but involves using the Pub/Sub Kafka connector as a sink connector instead of mirroring the topics from the on-prem cluster. This option would result in the same duplication of data and additional resources required as Option A, making it less desirable.","comments":[{"timestamp":"1722870240.0","comment_id":"799010","upvote_count":"1","content":"Sorry. I messed up. The answer is probably A. My badd....","poster":"samdhimal"}],"comment_id":"798271","poster":"samdhimal","upvote_count":"1","timestamp":"1722790800.0"},{"timestamp":"1722790860.0","upvote_count":"1","poster":"samdhimal","comments":[{"poster":"musumusu","upvote_count":"1","timestamp":"1724498520.0","comment_id":"820500","content":"you use chatgpt replies, if you instruct chat gpt that you don't need to use plugins as per question say, it will answer A"}],"content":"Option D: Deploy the Pub/Sub Kafka connector to your on-prem Kafka cluster and configure Pub/Sub as a Sink connector. Use a Dataflow job to read from Pub/Sub and write to GCS.\n\nThis option involves deploying the Pub/Sub Kafka connector on the on-prem cluster, but configuring it as a sink connector. In this case, the data from the on-prem Kafka cluster would be sent directly to Pub/Sub, which would act as the final destination for the data. A Dataflow job would then be used to read the data from Pub/Sub and write it to Cloud Storage for analysis in BigQuery. This option would result in the data being stored in both the on-prem cluster and Pub/Sub, making it less desirable compared to option C, where the data is only stored in Pub/Sub as an intermediary between the on-prem cluster and Google Cloud.","comment_id":"798273"}]},{"upvote_count":"1","poster":"zellck","content":"Selected Answer: A\nA is the answer.","comment_id":"736804","timestamp":"1717670040.0"},{"content":"Selected Answer: A\n\"The preferred replication method is mirroring to avoid deployment of Kafka Connect plugins.\"","timestamp":"1713684540.0","comment_id":"700613","upvote_count":"1","poster":"Afonya"},{"upvote_count":"3","comment_id":"692865","poster":"somnathmaddi","timestamp":"1712911140.0","content":"D is the right answer"},{"upvote_count":"2","content":"D is the right answer","comment_id":"676612","timestamp":"1711156860.0","poster":"clouditis"},{"poster":"hendrixlives","comment_id":"504066","content":"Selected Answer: A\n\"A\" is the answer which complies with the requirements (specifically, \"The preferred replication method is mirroring to avoid deployment of Kafka Connect plugins\"). Indeed, one of the uses of what is called \"Geo-Replication\" (or Cross-Cluster Data Mirroring) in Kafka is precisely cloud migrations: https://kafka.apache.org/documentation/#georeplication\n\nHowever I agree with Ganshank, and the optimal \"Google way\" way would be \"D\", installing the Pub/Sub Kafka connector to move the data from on-prem to GCP.","upvote_count":"6","timestamp":"1687065240.0"},{"poster":"gcp_k","content":"Going with \"D\"\n\nRefer: https://stackoverflow.com/questions/55277188/kafka-to-google-pub-sub-using-sink-connector","timestamp":"1682186760.0","comments":[{"content":"\"avoid deployment of Kafka Connect plugins\"","upvote_count":"1","timestamp":"1687101960.0","poster":"baubaumiaomiao","comment_id":"504354"}],"upvote_count":"3","comment_id":"466264"},{"comment_id":"395279","poster":"sumanshu","upvote_count":"1","content":"Vote for A","timestamp":"1672534680.0"},{"upvote_count":"3","timestamp":"1662916080.0","content":"Answer: A\nDescription: Question says mirroring to avoid kafka connect plugins","comment_id":"308257","poster":"daghayeghi"},{"comments":[{"poster":"sumanshu","upvote_count":"1","content":"As per question - \"avoid deployment of Kafka Connect plugins.\"","comment_id":"402268","timestamp":"1673224140.0"}],"content":"Correct is D","poster":"Allan222","comment_id":"287145","upvote_count":"1","timestamp":"1660071120.0"},{"upvote_count":"3","poster":"vakati","comment_id":"183110","content":"A. \nthe best solution would be D but given the restriction here to use mirroring and avoid connectors, A would be the natural choice","timestamp":"1647798000.0"},{"timestamp":"1646726640.0","upvote_count":"4","content":"D should be the correct answer. Configure pub/sub as sink","poster":"Tanmoyk","comment_id":"175667"},{"comments":[{"content":"Correct Answer: D\nWhy is this correct?\nYou can connect Kafka to GCP by using a connector. The 'downstream' service (Pub/Sub) will use a sink connector.","comment_id":"162528","poster":"haroldbenites","comments":[{"timestamp":"1672534500.0","upvote_count":"2","content":"Question says : avoid deployment of Kafka Connect plugins.","comment_id":"395277","poster":"sumanshu"}],"timestamp":"1645403940.0","upvote_count":"1"}],"poster":"haroldbenites","content":"C is correct.\nhttps://docs.confluent.io/current/connect/kafka-connect-gcp-pubsub/index.html","upvote_count":"2","comment_id":"162521","timestamp":"1645402800.0"},{"comment_id":"151611","timestamp":"1644120600.0","poster":"clouditis","content":"its D, why would google prefer Kafka in their own cert questions! :)","comments":[{"content":"Because the questions mentions to avoid deployment of Kafka connect plugins","upvote_count":"3","poster":"Ral17","comment_id":"440465","timestamp":"1678127040.0"}],"upvote_count":"3"},{"timestamp":"1643562240.0","comment_id":"147492","poster":"Archy","content":"answer is D, as on-prime kafka support sink connector for outging data.","upvote_count":"1"},{"comments":[{"upvote_count":"4","poster":"[Removed]","content":"Selected A","timestamp":"1632367740.0","comment_id":"67181"}],"poster":"[Removed]","content":"May be A/D\nB - wrong as kafka cluster need not be a sink connector\nC - wrong as Pubsub need not be a source connector. \nReason - CloudPubSubConnector provides both a sink connector (to copy messages from Kafka to Cloud Pub/Sub) and a source connector (to copy messages from Cloud Pub/Sub to Kafka).\nShould be A - if The preferred replication method is mirroring to avoid deployment of Kafka Connect plugins.","comment_id":"67017","timestamp":"1632321540.0","upvote_count":"6"},{"timestamp":"1632273600.0","poster":"Rajokkiyam","comments":[{"poster":"baubaumiaomiao","content":"\"avoid deployment of Kafka Connect plugins\"","comment_id":"504356","upvote_count":"1","timestamp":"1687102020.0"}],"upvote_count":"5","comment_id":"66759","content":"Correct Answer : D."}],"timestamp":"2020-03-22 04:20:00","exam_id":11}],"exam":{"name":"Professional Data Engineer","isBeta":false,"id":11,"numberOfQuestions":319,"provider":"Google","lastUpdated":"11 Apr 2025","isImplemented":true,"isMCOnly":true},"currentPage":61},"__N_SSP":true}