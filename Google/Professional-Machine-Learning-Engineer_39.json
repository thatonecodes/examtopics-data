{"pageProps":{"questions":[{"id":"eyOpU0cYNbWD4FAcEXrr","answer_description":"","answers_community":["A (79%)","B (21%)"],"exam_id":13,"question_id":191,"url":"https://www.examtopics.com/discussions/google/view/131108-exam-professional-machine-learning-engineer-topic-1-question/","topic":"1","question_images":[],"choices":{"C":"1. Create a Vertex AI pipeline with parameters you want to track as arguments to your PipelineJob. Use the Metrics, Model, and Dataset artifact types from the Kubeflow Pipelines DSL as the inputs and outputs of the components in your pipeline.\n2. Associate the pipeline with your experiment when you submit the job.","A":"1. Initialize the Vertex SDK with the name of your experiment. Log parameters and metrics for each experiment, and attach dataset and model artifacts as inputs and outputs to each execution.\n2. After a successful experiment create a Vertex AI pipeline.","D":"1. Create a Vertex AI pipeline. Use the Dataset and Model artifact types from the Kubeflow Pipelines DSL as the inputs and outputs of the components in your pipeline.\n2. In your training component, use the Vertex AI SDK to create an experiment run. Configure the log_params and log_metrics functions to track parameters and metrics of your experiment.","B":"1. Initialize the Vertex SDK with the name of your experiment. Log parameters and metrics for each experiment, save your dataset to a Cloud Storage bucket, and upload the models to Vertex AI Model Registry.\n2. After a successful experiment, create a Vertex AI pipeline."},"answer_ET":"A","discussion":[{"comment_id":"1121794","upvote_count":"5","timestamp":"1705159680.0","content":"Selected Answer: A\nOption B: Manually saving datasets and models to Cloud Storage and Model Registry introduces extra steps and potential for inconsistencies.\nOptions C and D: Prioritizing pipeline creation limits flexibility and visibility during the experimentation phase, making it harder to track artifacts and compare models effectively.","poster":"pikachu007"},{"comment_id":"1373428","upvote_count":"1","timestamp":"1741555860.0","content":"Selected Answer: A\nA.\nModel and Data artifacts allow you to retrieve the model and the data, so you don't need to explicitly store it separately. That's overkill.","poster":"5091a99"},{"upvote_count":"2","poster":"rajshiv","timestamp":"1733152080.0","content":"Selected Answer: B\nA does not specify where to store the model. I agree with bobjr","comment_id":"1320994"},{"comment_id":"1242928","content":"Option A correctly describes how to rapidly and easily transition successful experiments to production by initializing the Vertex SDK with the experiment name, logging parameters and metrics, and attaching dataset and model artifacts. The second step of creating a Vertex AI pipeline after a successful experiment allows for easy iteration on the model implementation while maintaining track of the experiment's performance.","timestamp":"1720200480.0","upvote_count":"1","poster":"AzureDP900"},{"poster":"bobjr","timestamp":"1717529100.0","upvote_count":"2","content":"Selected Answer: B\nAnswer B leverages more tools for responsability splitting : they are still tools for early experiments, but would help in the pipeline creation.\n\nC & D are overkill","comment_id":"1224318"},{"content":"Selected Answer: A\nI agree with these comments\n\n>> I will go for A, because the requirement is \"rapidly and easily\" \n\n>> B: Manually saving datasets and models to Cloud Storage and Model Registry introduces extra steps and potential for inconsistencies.\n\n>> Options C and D: Prioritizing pipeline creation limits flexibility and visibility during the experimentation phase, making it harder to track artifacts and compare models effectively.","comment_id":"1152292","poster":"guilhermebutzke","upvote_count":"4","timestamp":"1708128060.0"},{"comment_id":"1129202","content":"Selected Answer: A\nI will go for A, because the requirement is \"rapidly and easily\" transition successful experiments to production. Option B,C,D are too complex to conduct.","timestamp":"1705985580.0","poster":"daidai75","upvote_count":"3"},{"poster":"b1a8fae","upvote_count":"2","content":"Selected Answer: A\nI believe is A for the same reasons that pikachu.","comment_id":"1128766","timestamp":"1705934460.0"}],"answer":"A","unix_timestamp":1705159680,"answer_images":[],"timestamp":"2024-01-13 16:28:00","isMC":true,"question_text":"You are developing an ML model in a Vertex AI Workbench notebook. You want to track artifacts and compare models during experimentation using different approaches. You need to rapidly and easily transition successful experiments to production as you iterate on your model implementation. What should you do?"},{"id":"fOSkmziBqn2CjVZyjw27","question_images":[],"timestamp":"2024-01-13 16:33:00","answers_community":["C (88%)","12%"],"discussion":[{"timestamp":"1733344320.0","poster":"bobjr","content":"Selected Answer: C\nThe job fails, not the access to notebook","upvote_count":"2","comment_id":"1224281"},{"poster":"fitri001","upvote_count":"3","timestamp":"1729058520.0","comment_id":"1196372","content":"Selected Answer: C\nVertex AI has its own set of specific roles that control access to resources within the Vertex AI platform itself, such as datasets, models, and endpoints. The Vertex AI Notebook Runner falls under this category"},{"timestamp":"1728802560.0","comment_id":"1194709","content":"Selected Answer: C\nThe insufficient permissions error suggests your instance lacks the required authorization to access Vertex AI Pipelines resources.","poster":"omermahgoub","upvote_count":"3"},{"timestamp":"1725796920.0","upvote_count":"2","comment_id":"1168851","poster":"Yan_X","content":"Selected Answer: C\nThe question is asking 'submit a Vertex AI Pipeline job', so not just simply run notebooks on Vertex AI Workbench. The role required should be 'IAM Vertex AI User role'.\nSo it is C."},{"timestamp":"1722585900.0","content":"Selected Answer: D\nI have done the test, it is D","upvote_count":"1","poster":"daidai75","comment_id":"1138407"},{"comment_id":"1128796","timestamp":"1721653140.0","content":"Selected Answer: C\nI decided to change my mind to C after realizing we need the permissions aiplatform.pipelineJobs, present in vertex AI user. Not sure if the notebook runner role allows to run notebook from pipeline jobs + its specified that it only is allowed to run scheduled notebooks (no mention of scheduling here anywhere)","poster":"b1a8fae","upvote_count":"3"},{"poster":"b1a8fae","content":"Selected Answer: D\nI say D.\nYou want to run the code, that's your purpose, and you have insufficient permissions, so all the permissions you need to solve this problem is: being able to run the notebook. Plus, what is a \"AI user role\"? It is not a predefined role according to the docs: https://cloud.google.com/vertex-ai/docs/workbench/user-managed/iam#iam_roles","upvote_count":"1","comments":[{"comment_id":"1128786","poster":"b1a8fae","timestamp":"1721652780.0","upvote_count":"1","content":"Apparently \"Vertex AI user role\" is indeed a thing. I just did not see this link: https://cloud.google.com/vertex-ai/docs/general/access-control#predefined-roles. My point remains: not being able to run the code seems to be the inconvenient here."}],"comment_id":"1128778","timestamp":"1721652540.0"},{"content":"Selected Answer: C\nA. Region Compatibility: While regional compatibility is important, it's not the primary cause of this permission error.\nB. Subnet Matching: Subnet alignment is usually not a requirement for Vertex AI pipeline job submission.\nD. Notebooks Runner Role: This role is primarily for executing notebook code, not managing Vertex AI resources.","upvote_count":"2","comment_id":"1121797","poster":"pikachu007","timestamp":"1720877580.0"}],"question_text":"You recently created a new Google Cloud project. After testing that you can submit a Vertex AI Pipeline job from the Cloud Shell, you want to use a Vertex AI Workbench user-managed notebook instance to run your code from that instance. You created the instance and ran the code but this time the job fails with an insufficient permissions error. What should you do?","answer_images":[],"answer_description":"","answer":"C","unix_timestamp":1705159980,"choices":{"A":"Ensure that the Workbench instance that you created is in the same region of the Vertex AI Pipelines resources you will use.","D":"Ensure that the Vertex AI Workbench instance is assigned the Identity and Access Management (IAM) Notebooks Runner role.","C":"Ensure that the Vertex AI Workbench instance is assigned the Identity and Access Management (IAM) Vertex AI User role.","B":"Ensure that the Vertex AI Workbench instance is on the same subnetwork of the Vertex AI Pipeline resources that you will use."},"isMC":true,"url":"https://www.examtopics.com/discussions/google/view/131110-exam-professional-machine-learning-engineer-topic-1-question/","exam_id":13,"answer_ET":"C","topic":"1","question_id":192},{"id":"DvRghwtksMnxqBfA7PZJ","question_id":193,"discussion":[{"poster":"AzureDP900","timestamp":"1720200900.0","comment_id":"1242932","upvote_count":"2","content":"Option A is correct\nThe high-definition images of each semiconductor are taken in real-time at the end of the assembly line.\nThe images are uploaded to Cloud Storage along with tabular data that includes batch number, serial number, dimensions, and weight.\nYou need to configure model training and serving while maximizing model accuracy."},{"upvote_count":"4","comment_id":"1194712","poster":"omermahgoub","timestamp":"1712991540.0","content":"Selected Answer: A\nReal-time Processing, uploading images to Cloud Storage triggers the AutoML image classification model for immediate processing, enabling real-time quality control decisions.\nImage Classification, the scenario focuses on classifying images as \"passing\" or \"failing\" quality, making image classification the appropriate approach.\nPub/Sub Notifications, Pub/Sub messaging efficiently alerts downstream systems about failing classifications, allowing for prompt quality control actions."},{"content":"Selected Answer: A\nI go with A.","upvote_count":"3","comment_id":"1128801","poster":"b1a8fae","timestamp":"1705935720.0"},{"content":"Selected Answer: D\nOption B: Batch prediction jobs introduce latency, making them unsuitable for real-time quality control.\nOption C: K-means clustering is an unsupervised learning technique that doesn't leverage labeled data to distinguish between passing and failing semiconductors, potentially compromising accuracy.\nOption D: Tabular classification focuses on structured data, not images, and might overlook visual defects captured in the photos.","poster":"pikachu007","comments":[{"timestamp":"1705984020.0","content":"I am afraid the option D is not correct, since this is a image classification task.","poster":"daidai75","comment_id":"1129194","upvote_count":"1"},{"comment_id":"1121803","timestamp":"1705160220.0","upvote_count":"3","poster":"pikachu007","content":"The answer should be A*"}],"timestamp":"1705160160.0","comment_id":"1121801","upvote_count":"1"},{"comment_id":"1116571","timestamp":"1704712860.0","upvote_count":"2","content":"Selected Answer: A\nThe right answer should be A","poster":"daidai75"}],"question_text":"You work for a semiconductor manufacturing company. You need to create a real-time application that automates the quality control process. High-definition images of each semiconductor are taken at the end of the assembly line in real time. The photos are uploaded to a Cloud Storage bucket along with tabular data that includes each semiconductor’s batch number, serial number, dimensions, and weight. You need to configure model training and serving while maximizing model accuracy. What should you do?","question_images":[],"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/130559-exam-professional-machine-learning-engineer-topic-1-question/","answer_images":[],"answer_ET":"A","choices":{"A":"Use Vertex AI Data Labeling Service to label the images, and tram an AutoML image classification model. Deploy the model, and configure Pub/Sub to publish a message when an image is categorized into the failing class.","D":"Import the tabular data into BigQuery, use Vertex AI Data Labeling Service to label the data and train an AutoML tabular classification model. Deploy the model, and configure Pub/Sub to publish a message when a semiconductor’s data is categorized into the failing class.","B":"Use Vertex AI Data Labeling Service to label the images, and train an AutoML image classification model. Schedule a daily batch prediction job that publishes a Pub/Sub message when the job completes.","C":"Convert the images into an embedding representation. Import this data into BigQuery, and train a BigQuery ML K-means clustering model with two clusters. Deploy the model and configure Pub/Sub to publish a message when a semiconductor’s data is categorized into the failing cluster."},"answers_community":["A (90%)","10%"],"unix_timestamp":1704712860,"answer":"A","timestamp":"2024-01-08 12:21:00","isMC":true,"topic":"1","exam_id":13},{"id":"LfC7ud9tZONu2Lyod1uB","answer_description":"","question_images":[],"url":"https://www.examtopics.com/discussions/google/view/130558-exam-professional-machine-learning-engineer-topic-1-question/","timestamp":"2024-01-08 12:12:00","question_id":194,"answers_community":["A (100%)"],"answer_images":[],"answer_ET":"A","choices":{"C":"Deploy a matrix factorization model training job by using BigQuery ML","A":"Deploy the training jobs by using TPU VMs with TPUv3 Pod slices, and use the TPUEmbeading API","D":"Deploy the training jobs by using Compute Engine instances with A100 GPUs, and use the tf.nn.embedding_lookup API","B":"Deploy the training jobs in an autoscaling Google Kubernetes Engine cluster with CPUs"},"isMC":true,"topic":"1","answer":"A","question_text":"You work for a rapidly growing social media company. Your team builds TensorFlow recommender models in an on-premises CPU cluster. The data contains billions of historical user events and 100,000 categorical features. You notice that as the data increases, the model training time increases. You plan to move the models to Google Cloud. You want to use the most scalable approach that also minimizes training time. What should you do?","discussion":[{"upvote_count":"8","poster":"daidai75","timestamp":"1720429920.0","comment_id":"1116565","content":"Selected Answer: A\nTPU (Tensor Processing Units) VMs are specialized hardware accelerators designed by Google specifically for machine learning tasks.\nTPUv3 Pod slices offer high scalability and are excellent for distributed training tasks.\nThe TPUEmbedding API is optimized for handling large volumes of categorical features, which fits your scenario with 100,000 categorical features.\nThis option is likely to offer the fastest training times due to specialized hardware and optimized APIs for large-scale machine learning tasks."},{"timestamp":"1728802860.0","comment_id":"1194714","upvote_count":"4","content":"Selected Answer: A\nAddressing Bottleneck: As data size increases, CPU-based training becomes increasingly slow. TPUs are specifically designed to address this challenge, significantly accelerating training.\nLarge Categorical Features: TPUEmbedding API efficiently handles embedding lookups for a vast number of categorical features, a common characteristic of recommender system data.","poster":"omermahgoub"},{"timestamp":"1725587160.0","poster":"JG123","comment_id":"1166909","content":"Option C","upvote_count":"1"},{"timestamp":"1724068380.0","poster":"guilhermebutzke","comment_id":"1153956","upvote_count":"2","content":"Selected Answer: A\nMy Answer: \n\nA: most scalable approach that also minimizes training time: TPU using TPUEmbeading API\n\nhttps://www.tensorflow.org/api_docs/python/tf/tpu/experimental/embedding/TPUEmbedding"}],"exam_id":13,"unix_timestamp":1704712320},{"id":"ITnwCzScsj2nDj8kHkTI","answer":"A","question_text":"You are training and deploying updated versions of a regression model with tabular data by using Vertex AI Pipelines, Vertex AI Training, Vertex AI Experiments, and Vertex AI Endpoints. The model is deployed in a Vertex AI endpoint, and your users call the model by using the Vertex AI endpoint. You want to receive an email when the feature data distribution changes significantly, so you can retrigger the training pipeline and deploy an updated version of your model. What should you do?","answers_community":["A (100%)"],"choices":{"C":"In Cloud Monitoring create a logs-based metric and a threshold alert for the metric. Configure Cloud Monitoring to send an email when the alert is triggered.","D":"Export the container logs of the endpoint to BigQuery. Create a Cloud Function to run a SQL query over the exported logs and send an email. Use Cloud Scheduler to trigger the Cloud Function.","B":"In Cloud Logging, create a logs-based alert using the logs in the Vertex Al endpoint. Configure Cloud Logging to send an email when the alert is triggered.","A":"Use Vertex Al Model Monitoring. Enable prediction drift monitoring on the endpoint, and specify a notification email."},"timestamp":"2024-01-13 16:42:00","isMC":true,"url":"https://www.examtopics.com/discussions/google/view/131111-exam-professional-machine-learning-engineer-topic-1-question/","answer_images":[],"question_id":195,"topic":"1","question_images":[],"unix_timestamp":1705160520,"answer_ET":"A","answer_description":"","discussion":[{"timestamp":"1723049880.0","poster":"CHARLIE2108","upvote_count":"3","comment_id":"1143694","content":"Selected Answer: A\nI went with A"},{"comment_id":"1129191","poster":"daidai75","upvote_count":"1","timestamp":"1721701440.0","content":"Selected Answer: A\nVertex AI Model Monitoring is specifically designed for this purpose and provides out-of-the-box functionality for monitoring the data distribution of your model's predictions. It can automatically detect drift and trigger alerts based on predefined thresholds, making it the most efficient and straightforward solution.\n\nOption B,C and D are either over complex or too many manual operations."},{"timestamp":"1721653740.0","content":"Selected Answer: A\nhttps://cloud.google.com/blog/topics/developers-practitioners/monitor-models-training-serving-skew-vertex-ai","comment_id":"1128805","upvote_count":"1","poster":"b1a8fae"},{"content":"A\nPrediction drift is the change in the distribution of feature values or labels over time.","comment_id":"1122820","poster":"36bdc1e","upvote_count":"1","timestamp":"1720982400.0"},{"poster":"pikachu007","timestamp":"1720878120.0","content":"Selected Answer: A\nOptions B and C: While Cloud Logging and Cloud Monitoring can be used for general monitoring, they don't have the same specialized focus on prediction drift, potentially requiring more complex setup and analysis.\nOption D: Exporting logs to BigQuery and creating a Cloud Function for analysis can be time-consuming and less efficient compared to Vertex AI Model Monitoring's out-of-the-box capabilities.","comment_id":"1121808","upvote_count":"1"}],"exam_id":13}],"exam":{"name":"Professional Machine Learning Engineer","isImplemented":true,"numberOfQuestions":304,"provider":"Google","id":13,"isBeta":false,"lastUpdated":"11 Apr 2025","isMCOnly":true},"currentPage":39},"__N_SSP":true}