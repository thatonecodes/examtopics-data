{"pageProps":{"questions":[{"id":"VI4IeWxUKQlyQ2i2M8Xk","topic":"4","isMC":true,"answer_ET":"AB","answer_description":"","discussion":[{"poster":"raf2121","comment_id":"430405","timestamp":"1645674480.0","content":"My option it's A & B\n\nA - OK (Google Cloud compliance page will give list of products those are HIPAA compliant https://cloud.google.com/security/compliance/offerings?skip_cache=true#/regions=USA&industries=Healthcare_and_life_sciences&focusArea=Privacy)\nB - OK (BAA means HIPAA Business Associate amendment or Business Associate Agreement entered into between Google and Customer. With EHR being a leading provider of health record software, this agreement is required. https://cloud.google.com/files/gcp-hipaa-overview-guide.pdf?hl=en)\nC - Eliminated (Firebase authentication provides backend services, easy-to-use SDKs and ready-made libraries to users on App. https://firebase.google.com/docs/auth)\nD - Eliminated (more of an observability platform)\nE - Eliminated (Running distributed services in GKE private clusters gives enterprises both secure and reliable services. Not sure how this may help with Private Compliance Audit)","upvote_count":"45"},{"upvote_count":"17","poster":"SoniaJacob521","content":"A& B https://cloud.google.com/security/compliance/hipaa","timestamp":"1645627440.0","comment_id":"429958"},{"comment_id":"1198367","poster":"Dav_96","content":"Selected Answer: A\nJust got out of the exam. The option B was not in the answers, so the only option left for me was A.","timestamp":"1729319280.0","upvote_count":"9"},{"timestamp":"1686982740.0","poster":"surajkrishnamurthy","comment_id":"747868","upvote_count":"1","content":"Selected Answer: AB\nA & B is the correct answer"},{"timestamp":"1686972780.0","content":"Selected Answer: AB\nA & B is the correct answer","poster":"surajkrishnamurthy","comment_id":"747781","upvote_count":"1"},{"timestamp":"1684309020.0","comment_id":"720369","upvote_count":"1","poster":"megumin","content":"Selected Answer: AB\nAB is ok"},{"upvote_count":"1","content":"Selected Answer: AB\nA, B that's the only two things that I see related","timestamp":"1682010600.0","poster":"Mahmoud_E","comment_id":"700151"},{"content":"A, B is straight forward, I didn’t even think too much before making my mind. You need to read all case studies understand throughly before the exam. This whole set of case studies waste lot of time if you don’t prepare in advance and trying go through during exam. My approach is focus on key words..","upvote_count":"10","poster":"AzureDP900","comment_id":"627228","timestamp":"1672896420.0"},{"timestamp":"1667920860.0","upvote_count":"2","comment_id":"598559","content":"Selected Answer: AB\nAgree with raf 2121 A& B","poster":"amxexam"},{"poster":"mbenhassine1986","timestamp":"1660226160.0","upvote_count":"2","comment_id":"545405","content":"A & B \nhttps://cloud.google.com/security/compliance/hipaa#customer_responsibilities"},{"upvote_count":"2","timestamp":"1660058880.0","content":"Ans is A and B","poster":"muky31dec","comment_id":"543962"},{"content":"A and B\n\nhttps://cloud.google.com/security/compliance/hipaa\nEssential best practices:\n1. Execute a Google Cloud BAA. You can request a BAA directly from your account manager.\n2. Disable or otherwise ensure that you do not use Google Cloud Products that are not explicitly covered by the BAA (see Covered Products) when working with PHI.","poster":"Arjun1983","upvote_count":"3","comment_id":"522304","timestamp":"1657636620.0"},{"upvote_count":"2","content":"Selected Answer: AB\nI chose A&B by Elimination method.","comment_id":"520155","poster":"OrangeTiger","timestamp":"1657358880.0"},{"comment_id":"511293","timestamp":"1656424920.0","content":"A and B https://cloud.google.com/security/compliance/hipaa","poster":"Pime13","upvote_count":"1"},{"timestamp":"1655452980.0","content":"Selected Answer: AB\nhttps://cloud.google.com/security/compliance/offerings?skip_cache=true#/regions=USA&industries=Healthcare_and_life_sciences&focusArea=Privacy","poster":"PhilipKoku","comment_id":"503546","upvote_count":"3"},{"content":"Selected Answer: AB\nAB is the correct answer","comment_id":"501043","timestamp":"1655171280.0","poster":"vincy2202","upvote_count":"4"},{"upvote_count":"3","content":"Selected Answer: AB\nVote AB","poster":"pakilodi","timestamp":"1654252800.0","comment_id":"493146"},{"poster":"joe2211","upvote_count":"4","timestamp":"1653611820.0","content":"Selected Answer: AB\nvote AB","comment_id":"487794"},{"upvote_count":"3","comment_id":"467432","timestamp":"1650887160.0","content":"Answer should be A and B.\n\nWhy D is marked as answer? It is not related to compliances but about monitoring.","poster":"[Removed]"},{"upvote_count":"2","poster":"rishab86","timestamp":"1648980480.0","content":"Answer is A and B \nCustomers who are subject to HIPAA and want to use GCP for their business purposes involving PHI must enter into a BAA with Google that covers specific Google Cloud products and services. \nhttps://cloud.google.com/files/gcp-hipaa-overview-guide.pdf?hl=en","comment_id":"456509"},{"upvote_count":"5","content":"A. Verify EHR's product usage against the list of compliant products on the Google Cloud compliance page.\nB. Advise EHR to execute a Business Associate Agreement (BAA) with Google Cloud.","timestamp":"1646072220.0","poster":"victory108","comment_id":"434024"}],"unix_timestamp":1629722640,"timestamp":"2021-08-23 14:44:00","exam_id":4,"question_text":"For this question, refer to the EHR Healthcare case study. You are responsible for ensuring that EHR's use of Google Cloud will pass an upcoming privacy compliance audit. What should you do? (Choose two.)","answers_community":["AB (71%)","A (29%)"],"question_id":236,"answer_images":[],"question_images":[],"url":"https://www.examtopics.com/discussions/google/view/60388-exam-professional-cloud-architect-topic-4-question-1/","answer":"AB","choices":{"C":"Use Firebase Authentication for EHR's user facing applications.","D":"Implement Prometheus to detect and prevent security breaches on EHR's web-based applications.","A":"Verify EHR's product usage against the list of compliant products on the Google Cloud compliance page.","E":"Use GKE private clusters for all Kubernetes workloads.","B":"Advise EHR to execute a Business Associate Agreement (BAA) with Google Cloud."}},{"id":"Ro3tcUSNl7xLsxaAX9St","answer_images":[],"question_images":[],"topic":"4","isMC":true,"answers_community":["AD (43%)","AC (33%)","13%","12%"],"timestamp":"2021-08-23 21:35:00","exam_id":4,"question_text":"For this question, refer to the EHR Healthcare case study. You need to define the technical architecture for securely deploying workloads to Google Cloud. You also need to ensure that only verified containers are deployed using Google Cloud services. What should you do? (Choose two.)","url":"https://www.examtopics.com/discussions/google/view/60423-exam-professional-cloud-architect-topic-4-question-2/","discussion":[{"comments":[{"content":"Also see references to the combination of using binary authorization and vulnerability scanning here:\nhttps://cloud.google.com/binary-authorization/docs/overview","timestamp":"1649533740.0","comment_id":"583463","upvote_count":"13","poster":"cloudmon"}],"timestamp":"1629770700.0","comment_id":"430409","upvote_count":"51","content":"A & D\n\nBinary Authorization to ensure only verified containers are deployed \nTo ensure deployment are secure and and consistent, automatically scan images for vulnerabilities with container analysis (https://cloud.google.com/docs/ci-cd/overview?hl=en&skip_cache=true)","poster":"raf2121"},{"content":"IMHO its A&C","comments":[{"timestamp":"1638880800.0","comment_id":"495988","poster":"mgm7","comments":[{"timestamp":"1678044780.0","comment_id":"830249","upvote_count":"8","comments":[{"comment_id":"878056","timestamp":"1682238360.0","upvote_count":"4","content":"But that's the goal: secure the deployment process.","poster":"medi01"}],"poster":"BeCalm","content":"Dude the same applies to C. Trusted service accounts can deploy junk too."}],"upvote_count":"6","content":"I see a lot of people answered D but I don't see how it answers the question. I can securely deploy complete junk code. There is no contradiction in this phrase even if one obviously should avoid doing this."}],"timestamp":"1629892620.0","upvote_count":"32","poster":"KillerGoogle","comment_id":"431413"},{"poster":"zniffur","timestamp":"1743059220.0","content":"Selected Answer: AB\nFrom Gemini: \nBy combining options A and B, you establish a system where containers are signed (verified) as part of your development process, and Google Cloud's Binary Authorization enforces that only these signed containers can be deployed to your GKE clusters, thus meeting both requirements of secure deployment and ensuring only verified containers are used.","upvote_count":"1","comment_id":"1410765"},{"upvote_count":"1","poster":"desertlotus1211","comment_id":"1321436","content":"Selected Answer: AB\nAnswer is A&B. remember the questions ask abut securely deployment container images and verified containers. scanning for vulnerabilities does not accomplish this. \n\nI know this goes against common sense, but good code or bad code - how would you securely deploy the container? Answer is A&B.","timestamp":"1733241360.0"},{"poster":"dfizban","upvote_count":"2","timestamp":"1728923760.0","comment_id":"1297733","content":"Selected Answer: AD\nA&D I'm sure"},{"timestamp":"1727618040.0","poster":"pcamaster","content":"Selected Answer: AC\nAC\n\nQuestion is about:\n- Securing the deployment process\n- Make sure only verified containers can run on the cluster\n\nA: Covers the secondo point thanks to binary authorization. It also covers the signing requirement, as it is performed at CICD level.\nB: This is already covered by A. \nC: Makes sure that only required Service Accounts can pull the code from registry, so it covers the first part of the questione\nD. Secure scanning is about \"security vulnerability in code\". So it does not cover deployment phase, nor authorization phase.\n\nSo, it's A & C","upvote_count":"1","comment_id":"1291162"},{"upvote_count":"2","content":"Selected Answer: AB\nwho deploy is not an issue, the question is 'only verified containers' ....kritis can do that.","poster":"ukivanlamlpi","comment_id":"1245469","timestamp":"1720613400.0"},{"timestamp":"1718227860.0","upvote_count":"2","comment_id":"1229507","poster":"upliftinghut","content":"Selected Answer: AD\nA : use binary authorization then D check vulnerabilities before being able to deploy"},{"upvote_count":"13","timestamp":"1713508260.0","content":"Selected Answer: A\nJust got out of the exam. You only need to specify one answer, hence I chose A.","comment_id":"1198368","poster":"Dav_96"},{"upvote_count":"2","timestamp":"1706967600.0","comment_id":"1139263","content":"Selected Answer: AD\nad for me","poster":"Pime13"},{"comment_id":"1135102","content":"Selected Answer: AD\nhttps://cloud.google.com/docs/ci-cd/overview?hl=en&skip_cache=true \nhttps://cloud.google.com/binary-authorization/docs/overview","timestamp":"1706545380.0","poster":"Pime13","upvote_count":"2"},{"upvote_count":"1","content":"Selected Answer: AC\nFor surę AC","poster":"didek1986","timestamp":"1705956300.0","comment_id":"1129017"},{"content":"Selected Answer: AC\nAnswer should be A & C, as the ask is to ensure only verified containers to be deployed. With just Binary Authorisation and signing images, you can't fulfil the requirement, you would need to also restrict it at the IAM level, so that no bad actor can create an image in the registry and bypass Binary Authorization to deploy an image.","upvote_count":"5","timestamp":"1705180020.0","comment_id":"1122053","poster":"JohnDohertyDoe"},{"poster":"sudaraka","timestamp":"1704955800.0","comment_id":"1119422","upvote_count":"5","content":"I think A&B\n\nKritis is an admission controller webhook for Kubernetes that enforces deploy-time security policies. By configuring Jenkins to use Kritis, you can cryptographically sign containers as part of the CI/CD pipeline, ensuring only signed containers are deployed.\n\nhttps://cloud.google.com/binary-authorization/docs/creating-attestations-kritis"},{"poster":"[Removed]","upvote_count":"5","timestamp":"1704002760.0","comment_id":"1110298","content":"Selected Answer: AD\nOption C is incorrect because while limiting access to trusted service accounts enhances security, it doesn't ensure that only verified containers are deployed."},{"timestamp":"1703612580.0","comment_id":"1106267","poster":"Prudvi3266","content":"Selected Answer: AD\nChecked with standard process for this. I found the below.\nImage Building and Scanning:\nDevelopers build container images locally or using Cloud Build.\nImages are scanned for vulnerabilities using integrated tools or third-party services.\nClean images are pushed to GCR.\nImage Verification:\nBinary Authorization enforces policies for image acceptance.\nAttestations from Cloud Security Scanner or third-party tools can be used.","upvote_count":"5"},{"comment_id":"1100125","upvote_count":"2","content":"A&D.\n\nC is incorrect because you configuring Container Registry doesn't only allow trusted service accounts to create/deploy containers. With IAM permissions, anyone can create non-trusted service accounts to deploy containers, or users can still deploy containers not in Container Registry.","timestamp":"1702944000.0","poster":"oidajoi"},{"content":"Selected Answer: AC\nA & C correct","poster":"Roro_Brother","comment_id":"1097368","timestamp":"1702648620.0","upvote_count":"1"},{"poster":"PreJo","timestamp":"1702363680.0","upvote_count":"1","content":"a and c are ok","comment_id":"1094202"},{"upvote_count":"1","content":"Who has untrsuted service accounts that can deploy stuff and is doing nothing about it?. That is bad architecture. Following good architecture design, D is a given, we will already have a limited number of trusted accounts that can deploy. So A and C.","comment_id":"1086959","poster":"Jconnor","timestamp":"1701623100.0"},{"timestamp":"1701084900.0","upvote_count":"1","content":"Selected Answer: AD\nA & D - sounds more native to Google Cloud services and must required.","comment_id":"1081473","poster":"thewalker"},{"comment_id":"1023323","poster":"RKS_2021","content":"AD - correct answers.\nC -- Trusted service accounts ? does not make sense.","upvote_count":"2","comments":[{"content":"Trusted SAs - SAs that are allowed to read/write to/from Registry","timestamp":"1700466000.0","upvote_count":"1","comment_id":"1075232","poster":"LifeWins"}],"timestamp":"1696267200.0"},{"poster":"Murtuza","timestamp":"1694695440.0","upvote_count":"2","comment_id":"1007632","content":"Container Registry can allow select trusted services to access a registry that's configured with network access rules. When trusted services are allowed, a trusted service instance can securely bypass the registry's network rules and perform operations such as pull or push images. The best choice is C here","comments":[{"upvote_count":"1","timestamp":"1706133540.0","content":"Also doesn't container registry automatically scan images for vulnerabilities when an image is pushed to the registry? Answer C would imply to me vulnerabilities have already been remediated and the remaining action is SA controls. https://cloud.google.com/artifact-registry/docs/analysis","poster":"impetuousrutabaga","comment_id":"1131155"}]},{"poster":"jits1984","content":"Selected Answer: AC\nBinary authorization and Service Account controls (as the question is asking on how you would secure the deployment process and not improving the security of the application)","timestamp":"1693454760.0","upvote_count":"2","comment_id":"994732"},{"timestamp":"1693317420.0","comment_id":"993169","poster":"dn_mohammed_data","content":"Selected Answer: AD\nbinary athorization and vun checks","upvote_count":"1"},{"upvote_count":"1","timestamp":"1693217880.0","content":"Selected Answer: AC\nmy focus is on only \"verified containers are deployed\"","poster":"PKookNN","comment_id":"992037"},{"poster":"capt2101akash","timestamp":"1690234860.0","comment_id":"962081","content":"Selected Answer: AC\nI feel the emphasis is to ensure that only verified containers are deployed. So I feel A & C is correct","upvote_count":"1"},{"upvote_count":"2","timestamp":"1689664680.0","poster":"gary_cooper","content":"Selected Answer: AC\nAs given in the question statement: \"You also need to ensure that only verified containers are deployed using Google Cloud services.\"\n\nHence, C makes more sense here.\nC. Configure Container Registry to only allow trusted service accounts to create and deploy containers from the registry.","comment_id":"955088"},{"poster":"sampon279","upvote_count":"1","content":"Selected Answer: AB\nSeems A & B are correct solution: https://github.com/grafeas/kritis. C = Service account we naturally use service account for CICD - hardly anyone uses user accounts. Security scanning is required, option B Kritis ensure manages allowlistCVEs.","comment_id":"937299","timestamp":"1688000700.0"},{"content":"Selected Answer: AD\nhttps://cloud.google.com/blog/products/devops-sre/devsecops-and-cicd-using-google-cloud-built-in-services","timestamp":"1686641160.0","poster":"claorden","comment_id":"922039","upvote_count":"7"},{"poster":"stfnz","content":"Selected Answer: AC\nthe question is not focused on vulnerability scanning (D), hence correct answers are A and C","upvote_count":"3","comment_id":"894837","timestamp":"1683792720.0"},{"timestamp":"1681549200.0","comment_id":"870785","poster":"bk989123","upvote_count":"1","content":"Selected Answer: AC\ndoesn't matter if it is redundant, the question asks for different options"},{"timestamp":"1680273720.0","comment_id":"857212","content":"Selected Answer: AC\nWording: \"You also need to ensure that only verified containers are deployed using Google Cloud services.\"\n\nA - makes sense. \nD - does not do this ... but C does.","poster":"HD2023","upvote_count":"3"},{"poster":"JC0926","upvote_count":"1","content":"Selected Answer: AC\nOption D suggests using vulnerability scanning to confirm that there are no vulnerabilities before deploying the workload. While this is an important security consideration, it does not specifically address the requirement to ensure that only verified containers are deployed.","timestamp":"1679810220.0","comment_id":"850756"},{"timestamp":"1679303100.0","poster":"telp","comment_id":"844676","content":"Selected Answer: AC\nWith the context of the questions for me AC. It depends on the context of the word \"Securely \" for me it's interpretation is \"securely deploying the containers\".","upvote_count":"2"},{"timestamp":"1678735500.0","comment_id":"838220","poster":"BeCalm","content":"Selected Answer: AD\nA and C are somewhat redundant so it has to be AD.","upvote_count":"3"},{"upvote_count":"3","poster":"BeCalm","comment_id":"830250","timestamp":"1678044960.0","content":"Selected Answer: AD\nA = control who does the deployment(trust)\nD = address vulnerability(security)\n\nThe combination provides trust + security"},{"content":"Selected Answer: AC\n\"Securly\" should be intepreted here as \"securly deploying the containers\". So AC","comment_id":"826586","timestamp":"1677741300.0","poster":"amelm","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: A\nA -- to enbale binary authorziatiion \nC- Prevent any one manually adding container image --Assign permission only to service account to create and deploy conatiner account","timestamp":"1675920000.0","poster":"RVivek","comment_id":"802854"},{"poster":"tdotcat","upvote_count":"1","comment_id":"777163","content":"Selected Answer: AD\nquestion is asking about how to verify containers, so AD","timestamp":"1673824560.0"},{"content":"I'd go with A & C.\nThe key word to note is \"define the technical architecture for securely deploying workloads to Google Cloud\" \n\nD does detect vulnerabilities but doesn't ensure the deployment is from a trusted source which by best practices is required for less privilege access","comment_id":"763382","timestamp":"1672614420.0","upvote_count":"5","poster":"main_street"},{"content":"Answer is A&C, here's why\n\nA: Enable Binary Authorization on GKE, and sign containers as part of a CI/CD pipeline is a good option because Binary Authorization is a security feature that ensures that only verified containers are deployed to GKE clusters. By signing containers as part of a CI/CD pipeline, you can ensure that all containers deployed to GKE clusters are cryptographically signed, which allows Binary Authorization to verify the authenticity of the containers.\n\nC: Configure Container Registry to only allow trusted service accounts to create and deploy containers from the registry is a good option because it allows you to control which service accounts can create and deploy containers from the registry. By only allowing trusted service accounts to create and deploy containers, you can ensure that only authorized users are able to deploy workloads to Google Cloud.","upvote_count":"7","timestamp":"1672213560.0","comments":[{"timestamp":"1672213560.0","comments":[{"comment_id":"759525","upvote_count":"1","content":"Container Registry does have a built-in vulnerability scanning feature, but it is not required for securing container deployments.","poster":"omermahgoub","timestamp":"1672213680.0"}],"upvote_count":"1","content":"Why not D or B:\nB: Configure Jenkins to utilize Kritis to cryptographically sign a container as part of a CI/CD pipeline is not necessary because Binary Authorization provides a similar function for verifying the authenticity of containers.\n\nD: Configure Container Registry to use vulnerability scanning to confirm that there are no vulnerabilities before deploying the workload is not necessary for ensuring that only verified containers are deployed. Vulnerability scanning can help identify potential vulnerabilities in containers, but it does not verify the authenticity of the containers.","comment_id":"759521","poster":"omermahgoub"}],"poster":"omermahgoub","comment_id":"759519"},{"poster":"thamaster","timestamp":"1672067340.0","upvote_count":"1","comment_id":"757574","content":"Selected Answer: AD\nA is no brainer, D to use vulnerability scan as best practice"},{"timestamp":"1671680760.0","content":"Selected Answer: AD\nA: Binary Auth's definition is to only allow verified containers\nD: Can be used as part of Binary Auth's verification process / can integrate with option A\n\nWhy not B: Junkins is insecure by default + not a GCP service & A already mentions signing.\nWhy not C: \n* It's highly likely a human will have the ability to push to a registry / I don't believe a gcr.io / pkg.dev can be configured to only allow service accounts. \n* If a workload is able to pull images from arbitrary registries like docker.io, then it doesn't matter if you have a registry with secure images. / C is wrong because it lacks wording that says the cluster is only allowed to pull from the secure registry.","comment_id":"752950","upvote_count":"1","poster":"neokyle"},{"timestamp":"1671447540.0","poster":"allen_y_q_huang","comment_id":"749742","content":"A is not a doubt \nB Vul security scan https://cloud.google.com/binary-authorization/docs/creating-attestations-kritis","upvote_count":"1"},{"poster":"surajkrishnamurthy","timestamp":"1671265200.0","content":"Selected Answer: AD\nA & D is the correct answer","comment_id":"747870","upvote_count":"1"},{"comment_id":"747782","upvote_count":"1","content":"Selected Answer: AD\nA & D is the correct answer","timestamp":"1671255360.0","poster":"surajkrishnamurthy"},{"content":"Selected Answer: AC\nD doesn't answer the question, but C does, obvious answer A & C","upvote_count":"1","timestamp":"1670935740.0","comment_id":"744013","poster":"zetalexg"},{"content":"Selected Answer: AC\nonly a and c secure deploy","poster":"nosense","timestamp":"1669372200.0","comment_id":"726660","upvote_count":"1"},{"timestamp":"1666286340.0","poster":"Mahmoud_E","comment_id":"700161","upvote_count":"2","content":"Selected Answer: AD\nA and D best answers"},{"content":"Selected Answer: AC\nthe question say only verified container are deployed using Google Cloud service, so this eliminates option B. Option D provides a check on vulnerability of my code, that it's fine, but out of scope. Binary authorization and trusted service account make a better match with the requirement","timestamp":"1663657680.0","poster":"kiappy81","comment_id":"673888","upvote_count":"2"},{"content":"Selected Answer: AC\nA & C are the only one that makes sure only verified containers are deployed.","timestamp":"1663016160.0","comment_id":"667452","upvote_count":"3","poster":"alexandercamachop"},{"comment_id":"666253","poster":"6721sora","content":"Selected Answer: AB\nKritis Signer is an open source command-line tool that can create Binary Authorization attestations based on a policy that you configure. You can also use Kritis Signer to create attestations after checking an image for vulnerabilities identified by Container Analysis\nhttps://cloud.google.com/binary-authorization/docs/creating-attestations-kritis","upvote_count":"2","timestamp":"1662912000.0"},{"comment_id":"662229","timestamp":"1662538620.0","upvote_count":"1","poster":"kuboraam","content":"Selected Answer: AB\ni'd say A and B. \n\nA supports verified containers through digital signing. B is a similar solution. \n\nC is out because only trusted service accounts are able to deploy containers from the registry anyway. \nD is talking about vulnerability scanning which has nothing to do with this question."},{"upvote_count":"2","timestamp":"1662412440.0","poster":"aswani","content":"Selected Answer: AC\nits A&C","comment_id":"660555"},{"upvote_count":"3","content":"Selected Answer: AC\nA&C \"ensure that only verified containers are deployed\"","timestamp":"1659702720.0","poster":"Chute5118","comment_id":"642967"},{"timestamp":"1658831940.0","poster":"exam9391","comment_id":"637336","upvote_count":"4","content":"Selected Answer: AC\nA&C only reasonable answers."},{"content":"go for A","timestamp":"1658349660.0","comment_id":"634248","upvote_count":"2","poster":"[Removed]"},{"comment_id":"624023","timestamp":"1656426900.0","content":"A & B are same, Since A Option talk about Binary Autorization.\nOption B talk about the Tool, you can use for same .\nhttps://cloud.google.com/binary-authorization/docs/creating-attestations-kritis\n\n\nOverview\nKritis Signer is an open source command-line tool that can create Binary Authorization attestations based on a policy that you configure. You can also use Kritis Signer to create attestations after checking an image for vulnerabilities identified by Container Analysis.\n\nSo A & D","upvote_count":"1","poster":"prasan1234"},{"upvote_count":"2","content":"Selected Answer: AB\nA - Obvious\nD- Eliminated as we are talking about verified container not sceured.\nB- doable\nC- ensured trusted or verified services which we are not looking for\n\nHence going with A & B.","poster":"amxexam","timestamp":"1652016660.0","comment_id":"598563"},{"content":"This question is incorrect. Needs to remove the \"(Choose two)\" part. The test will only ask for 1 answer. Using the link previously listed here about binary-authorization/docs/overview Currently 404ing for me right now(link worked 2 days ago), but here's a clipped comment from it. \n\"Binary Authorization helps DevOps teams implement a proactive container security posture by ensuring only verified containers are admitted into the environment and that they remain trusted during runtime.\" \nTherefore I am going with binary authorization. \n\nWhen I see blatantly wrong/misleading information about a test, if makes me doubt the rest of the information on a site. Not sure how this question could have ever had 2 answers, when the test only asks for 1 with the same answers. So when you get to the test and it asks for 1 which of your 2 expected right answers would you use? \n\nAlso for reference. https://codelabs.developers.google.com/codelabs/cloud-binauthz-intro#0","poster":"ridyr","upvote_count":"5","timestamp":"1651676820.0","comment_id":"596903"},{"timestamp":"1651673940.0","comment_id":"596872","content":"Selected Answer: A\nThis question is incorrect. Needs to remove the \"(Choose two)\" part. The test will only ask for 1 answer. Using the link previously listed here about binary-authorization/docs/overview Currently 404ing for me right now(link worked 2 days ago), but here's a clipped comment from it. \n\"Binary Authorization helps DevOps teams implement a proactive container security posture by ensuring only verified containers are admitted into the environment and that they remain trusted during runtime.\" \nTherefore I am going with binary authorization. \n\nWhen I see blatantly wrong/misleading information about a test, if makes me doubt the rest of the information on a site. Not sure how this question could have ever had 2 answers, when the test only asks for 1 with the same answers. So when you get to the test and it asks for 1 which of your 2 expected right answers would you use?","comments":[{"poster":"ridyr","comment_id":"596885","content":"Forgot this link. https://codelabs.developers.google.com/codelabs/cloud-binauthz-intro#0\n3rd paragraph starts to explain the process a bit more or that's when it got interesting for me. \n\nThe codelabs site, might be worth squirreling away for a rainy day. I've found a few useful processes on there already, but haven't been through it all.","timestamp":"1651675080.0","upvote_count":"1"}],"upvote_count":"2","poster":"ridyr"},{"comment_id":"585545","poster":"learner311","upvote_count":"1","timestamp":"1649913180.0","content":"Business logic container with 0 CVEs?? That's where D throws me off. I always expect software to have some vulnerabilities active. Now severity from low ---> critical is where they get interesting. This doesn't comment on that at all."},{"upvote_count":"3","content":"Selected Answer: AD\nA & D. This seems very similar to an earlier question, which also provided options for using vulnerability scanning with binary authorization. It seems that those features go hand-in-hand. I will be selecting that combination if they come up in either format on the exam.","poster":"cloudmon","comment_id":"583462","timestamp":"1649533500.0"},{"comment_id":"574991","timestamp":"1648210560.0","content":"D is automatic. It’s out of the box. You don’t have to do anything. Thus D is eliminated","upvote_count":"1","poster":"SAMBIT"},{"poster":"[Removed]","upvote_count":"1","content":"AD \n\nhttps://cloud.google.com/container-registry/docs/container-analysis#as_a_strategic_information_api","comment_id":"571541","timestamp":"1647772500.0"},{"poster":"charlie_spooki","content":"Selected Answer: AD\nAnswer AD\nhttps://cloud.google.com/blog/products/identity-security/deploy-only-what-you-trust-introducing-binary-authorization-for-google-kubernetes-engine\nhttps://cloud.google.com/binary-authorization/docs/creating-attestations-kritis","comment_id":"533298","timestamp":"1643233620.0","upvote_count":"3"},{"content":"A & C - account security is mother of everything.","upvote_count":"5","comment_id":"526182","timestamp":"1642462260.0","poster":"sjmsummer"},{"content":"A,B,D is right. I will go with A and D \nC is more of ensuring verified SA rather than \"ensuring verified containers\". That SA could've deploy any kind of containers. \n\nContainer Registry vulnerability scanning checks for known vulnerabilities in container images stored in Google Container Registry. With Binary Authorization integration, users can gate deployments based on vulnerability findings. \n\nhttps://cloud.google.com/blog/products/identity-security/deploy-only-what-you-trust-introducing-binary-authorization-for-google-kubernetes-engine","upvote_count":"1","comment_id":"518260","poster":"timotei","comments":[{"content":"Final A n D cause i can't find anything on Jenkins + kritis signer","timestamp":"1641477840.0","poster":"timotei","comment_id":"518274","upvote_count":"2"},{"comment_id":"653217","content":"You also need to ensure that only verified containers are deployed using Google Cloud services ???\n\nIf there are security issues in the deployment or access right container ... A wrong unverified container can be deployed ... So having right security with A ...complete all the loop holes ..","comments":[{"content":"so C needs to be ensured to Reap any benefits of A ...","upvote_count":"1","timestamp":"1661733000.0","poster":"ShadowLord","comment_id":"653218"}],"timestamp":"1661732940.0","upvote_count":"1","poster":"ShadowLord"}],"timestamp":"1641476880.0"},{"content":"Selected Answer: AC\nA and C","timestamp":"1640707260.0","comments":[{"content":"after some time i've come to the conclusion that it should be A and D","upvote_count":"1","timestamp":"1643382420.0","poster":"Pime13","comment_id":"534785"}],"upvote_count":"4","comment_id":"511292","poster":"Pime13"},{"upvote_count":"2","timestamp":"1640707200.0","comment_id":"511291","content":"A and C for me","poster":"Pime13"},{"timestamp":"1640455560.0","poster":"zxcv1234","content":"Selected Answer: AB\nA & B, though B is a duplication of A with more specific details.\nC could have been, but binary auth takes care of rejecting containers not signed.\nD is irrelevant here, read the question carefully, it is about securely verifying a rouge container is deployed and nothing to do with vulnerability.","upvote_count":"4","comment_id":"509221"},{"comment_id":"506363","upvote_count":"2","poster":"ABO_Doma","timestamp":"1640112300.0","content":"Selected Answer: AD\nYou can incorporate vulnerability scanning into your CICD pipeline to ensure any runtime vulnerabilities are caught in this phase and deployed to live environment.\n\nhttps://cloud.google.com/security-command-center/docs/concepts-web-security-scanner-overview"},{"timestamp":"1639927320.0","content":"Selected Answer: AD\nA&D - the only options here","comment_id":"504967","upvote_count":"2","poster":"yack"},{"timestamp":"1639784220.0","content":"Selected Answer: AD\nsecurely deploying workloads to Google Cloud = D","upvote_count":"3","comment_id":"503935","poster":"ABO_Doma"},{"content":"Selected Answer: AB\nAB seems to be the correct answer. \nThough both A & B are relatively offering the same choices, option D can be out-rightly ruled out, since no such requirement of vulnerability scanning is specified in the question. option C is not a fool proof choice. Hence sticking with A & B.","poster":"vincy2202","timestamp":"1639480920.0","upvote_count":"1","comment_id":"501316"},{"comments":[{"poster":"timotei","content":"Which part of it shows B? I only see D in Integration with CI/CD tools.","upvote_count":"2","comment_id":"518269","timestamp":"1641477420.0"}],"timestamp":"1639147980.0","poster":"dsharu","content":"It is A&B. Please refer to this article.\nhttps://cloud.google.com/blog/products/identity-security/deploy-only-what-you-trust-introducing-binary-authorization-for-google-kubernetes-engine","upvote_count":"2","comment_id":"498717"},{"upvote_count":"3","comment_id":"493148","content":"Selected Answer: AD\nVote AD","poster":"pakilodi","timestamp":"1638535320.0"},{"poster":"mudot","timestamp":"1638013080.0","comment_id":"488085","upvote_count":"1","content":"Selected Answer: AB\nQuestion doenst ask anything about vulnarabilities \nits about securely deploying!"},{"poster":"joe2211","upvote_count":"2","timestamp":"1637980740.0","content":"Selected Answer: AD\nvote AD","comment_id":"487798"},{"comments":[{"upvote_count":"1","content":"Got this with one Option. B selected","timestamp":"1639908840.0","comment_id":"504778","comments":[{"content":"you mean you got this question with one correct answer but with same choices? Would it not be A?","poster":"Wonka","comment_id":"527755","upvote_count":"1","timestamp":"1642611480.0"}],"poster":"[Removed]"}],"timestamp":"1636906680.0","upvote_count":"1","comment_id":"478207","poster":"[Removed]","content":"A and D is correct.\n... or only B if single choice. Kritis does both BA and attestation"},{"poster":"BrijMohan08","upvote_count":"1","timestamp":"1632822480.0","content":"A & D - Answer","comment_id":"453259"},{"timestamp":"1632501480.0","upvote_count":"1","poster":"ACE_ASPIRE","comment_id":"450991","content":"A & D should be the correct answers"},{"upvote_count":"3","poster":"ACE_ASPIRE","timestamp":"1631409180.0","content":"A & D it should be","comment_id":"443213"},{"upvote_count":"1","comment_id":"443191","poster":"sudarchary","timestamp":"1631402280.0","content":"A&D are good"},{"timestamp":"1631214540.0","poster":"diaga2","content":"A is fine.\nB - 'Configure Jenkins to utilize Kritis' is bit wrong and can't a way for this combination. Instead only Kritis is a good option.\nSo I will go with A&D","upvote_count":"2","comment_id":"442151"},{"comment_id":"439878","upvote_count":"1","timestamp":"1630867500.0","poster":"xavi1","content":"A and B\nKritis is possible: https://cloud.google.com/binary-authorization/docs/creating-attestations-kritis","comments":[{"content":"But Jenkins it is not a Google Cloud Service, therefore A and D ;-)","comment_id":"439879","poster":"xavi1","timestamp":"1630867560.0","upvote_count":"6"}]},{"upvote_count":"3","poster":"victory108","comment_id":"434017","content":"A. Enable Binary Authorization on GKE, and sign containers as part of a CI/CD pipeline.\nD. Configure Container Registry to use vulnerability scanning to confirm that there are no vulnerabilities before deploying the workload.","timestamp":"1630167120.0"},{"timestamp":"1629747300.0","upvote_count":"2","comment_id":"430299","poster":"rvopoqvmtlwdlzrqxr","content":"AD - Why not A and D where D helps to deployed verified containers ?"}],"unix_timestamp":1629747300,"answer":"AD","question_id":237,"answer_description":"","choices":{"D":"Configure Container Registry to use vulnerability scanning to confirm that there are no vulnerabilities before deploying the workload.","B":"Configure Jenkins to utilize Kritis to cryptographically sign a container as part of a CI/CD pipeline.","C":"Configure Container Registry to only allow trusted service accounts to create and deploy containers from the registry.","A":"Enable Binary Authorization on GKE, and sign containers as part of a CI/CD pipeline."},"answer_ET":"AD"},{"id":"3tpFMbTKlZTnEQWapaAw","answers_community":["A (100%)"],"answer":"A","answer_images":[],"question_text":"You need to upgrade the EHR connection to comply with their requirements. The new connection design must support business-critical needs and meet the same network and security policy requirements. What should you do?","question_images":[],"discussion":[{"comments":[{"upvote_count":"1","timestamp":"1700751660.0","poster":"LaxmanTiwari","content":"spot on","comment_id":"904925"},{"timestamp":"1665251760.0","content":"This is awesome! That answers it perfectly!","upvote_count":"3","poster":"cloudmon","comment_id":"582985"}],"timestamp":"1664871300.0","content":"Selected Answer: A\nI will go A cause note in https://cloud.google.com/network-connectivity/docs/interconnect/how-to/dedicated/modifying-interconnects says\n\" It is not possible to change the link type on an Interconnect connection circuit from 10 Gbps to 100 Gbps. If you want to migrate to 100 Gbps, you must first provision a new 100-Gbps Interconnect connection alongside your existing 10-Gbps connection, and then migrate the traffic onto the 100-Gbps connection.\"","upvote_count":"35","poster":"[Removed]","comment_id":"580610"},{"content":"Answer A ; 99.9% availability requires 2 interconnect","upvote_count":"18","timestamp":"1646380860.0","comment_id":"438921","poster":"rishab86"},{"comment_id":"1549754","poster":"samsonakala","content":"Selected Answer: A\nA is the right answer","upvote_count":"1","timestamp":"1743880200.0"},{"content":"Selected Answer: A\nA is correct","comment_id":"1204944","upvote_count":"1","poster":"Gino17m","timestamp":"1730455920.0"},{"timestamp":"1719720600.0","comment_id":"1110300","poster":"[Removed]","content":"Selected Answer: A\nhttps://cloud.google.com/network-connectivity/docs/interconnect/how-to/dedicated/modifying-interconnects#:~:text=The%20link%20type,100%E2%80%91Gbps%20connection.","upvote_count":"1"},{"upvote_count":"4","timestamp":"1710413160.0","content":"I propose A.\nB - it is not possible to change interconnect link type.\nC - more problematic and probably more expensive\nD - Google does not offer a service level agreement (SLA) with Carrier Peering","comment_id":"1007388","poster":"Kamngur"},{"timestamp":"1708786680.0","poster":"dar10","comment_id":"989211","content":"Answer is A, as explained in the note at page 339 \"Implementing Hybrid Connectivity\" of the newly released book: https://a.co/d/asbIhln","upvote_count":"1"},{"upvote_count":"3","timestamp":"1686982860.0","comment_id":"747872","poster":"surajkrishnamurthy","content":"Selected Answer: A\nA is the correct answer"},{"content":"Selected Answer: A\nAnswer is A","timestamp":"1684665840.0","poster":"ashrafh","upvote_count":"2","comment_id":"723484"},{"poster":"Mahmoud_E","upvote_count":"2","timestamp":"1682011800.0","content":"Selected Answer: A\nA is best answer","comment_id":"700168"},{"poster":"alexandercamachop","upvote_count":"1","comment_id":"667454","content":"Selected Answer: A\nA.\nIn the text it mentions high connection - reliable, only dedicated interconnect could achieve that.","timestamp":"1678661880.0"},{"timestamp":"1667921640.0","comment_id":"598568","poster":"amxexam","content":"Selected Answer: A\nA is correct","upvote_count":"2"},{"poster":"vaibhav15","content":"Selected Answer: A\nVote A","upvote_count":"2","timestamp":"1654350360.0","comment_id":"493771"},{"poster":"sapsant","comment_id":"490720","timestamp":"1653911220.0","content":"Selected Answer: A\nDedicated Interconnect provides direct physical connections between your on-premises network and Google's network.\n\nhttps://cloud.google.com/network-connectivity/docs/interconnect/concepts/dedicated-overview","upvote_count":"3"},{"content":"Selected Answer: A\nvote A","upvote_count":"4","poster":"joe2211","timestamp":"1653612000.0","comment_id":"487799"},{"upvote_count":"2","content":"A is correct.\nMarked D is wrong","poster":"[Removed]","comment_id":"478209","timestamp":"1652538000.0"},{"timestamp":"1648393200.0","content":"Says \"Upgrade keeping same network requirements\". Remember infrastructure costs need to be kept low. A new dedicated connection will involve purchasing new installation equipment. Upgrading to a higher bandwidth is more cost-friendly. I go with B.","comment_id":"452529","poster":"Ari_GCP","upvote_count":"2"},{"poster":"MikeB19","timestamp":"1647855900.0","upvote_count":"3","content":"A is correct. Carrier peering is dedicated connection to google workspace not gcp\nhttps://cloud.google.com/network-connectivity/docs/carrier-peering","comment_id":"448740"},{"poster":"aa_desh","upvote_count":"5","content":"Answer: A, In Note they suggest for new connection for migrating 10GB to 100gb \nhttps://cloud.google.com/network-connectivity/docs/interconnect/how-to/dedicated/modifying-interconnects","timestamp":"1646284380.0","comment_id":"438217"},{"timestamp":"1646244420.0","content":"My answer is A","upvote_count":"2","poster":"Sarin","comment_id":"438002"},{"timestamp":"1646071740.0","comment_id":"434013","poster":"victory108","upvote_count":"2","content":"A. Add a new Dedicated Interconnect connection."},{"upvote_count":"4","poster":"raf2121","comment_id":"430410","content":"Answer : A\nA - secured connection\nB ruled out as EHR has chosen Google Cloud to replace their current colocation facilities","timestamp":"1645675740.0"},{"comments":[{"comment_id":"442152","poster":"diaga2","content":"Question talking about 'The new connection design' so it will A only.","timestamp":"1646860380.0","upvote_count":"2"}],"upvote_count":"2","comment_id":"430305","content":"B - It's said that \".. meet the same network and security policy requirements\" . So it's good to upgrade existing type of the connection.","poster":"rvopoqvmtlwdlzrqxr","timestamp":"1645652940.0"},{"comment_id":"430156","poster":"SoniaJacob521","upvote_count":"4","timestamp":"1645639320.0","content":"A - Dedicated Interconnect"}],"unix_timestamp":1629734520,"topic":"4","choices":{"D":"Add a new Carrier Peering connection.","A":"Add a new Dedicated Interconnect connection.","C":"Add three new Cloud VPN connections.","B":"Upgrade the bandwidth on the Dedicated Interconnect connection to 100 G."},"url":"https://www.examtopics.com/discussions/google/view/60403-exam-professional-cloud-architect-topic-4-question-3/","timestamp":"2021-08-23 18:02:00","answer_ET":"A","isMC":true,"exam_id":4,"question_id":238,"answer_description":""},{"id":"7Ut7NYXsmEh9AOWxxOTD","topic":"4","exam_id":4,"answer":"D","isMC":true,"question_images":[],"answer_ET":"D","timestamp":"2021-08-24 04:17:00","answer_description":"","unix_timestamp":1629771420,"discussion":[{"upvote_count":"31","comment_id":"430413","poster":"raf2121","content":"Answer : D (based on the requirement of secure and high-performance connection between on-premises systems to Google Cloud)\n\nBetween A and D, picked D as with Direct Connect EHR can get the bandwidth of 10 GBS to 100GBS (VPN ruled out as traffic is over internet and due to bandwidth. Direct Peering is more for Workspace rather than Google Cloud)","timestamp":"1645676220.0"},{"content":"If we notice this line in question - \"Google's recommended practices for production-level applications\" and then see overview of these 2 pages- https://cloud.google.com/network-connectivity/docs/interconnect/tutorials/production-level-overview and https://cloud.google.com/network-connectivity/docs/interconnect/tutorials/non-critical-overview. It is clear answer should be D , which is topology for production level applications recommended by Google","timestamp":"1648898760.0","upvote_count":"28","poster":"jask","comments":[{"poster":"cloudmon","timestamp":"1665252540.0","comment_id":"582991","comments":[{"timestamp":"1691212860.0","content":"Specifically this page https://cloud.google.com/network-connectivity/docs/interconnect/tutorials/dedicated-creating-9999-availability","upvote_count":"2","comments":[{"poster":"medi01","upvote_count":"1","comment_id":"878070","timestamp":"1698050340.0","content":"That's one more 9 than required."}],"poster":"moota","comment_id":"798666"}],"content":"^^^ This is the best explanation. Considering all of those factors, D looks best.","upvote_count":"5"}],"comment_id":"456044"},{"comment_id":"1105692","poster":"theBestStudent","timestamp":"1719368160.0","content":"Selected Answer: D\nAnswer is clearly D:\nhttps://cloud.google.com/network-connectivity/docs/interconnect/concepts/best-practices#scenarios","upvote_count":"1"},{"upvote_count":"1","timestamp":"1703522400.0","comment_id":"933693","poster":"red_panda","content":"Selected Answer: D\nFor me is D.\nBy Technical requirement we need to establish a stable, low-latency connection between on-prem and cloud"},{"content":"Selected Answer: D\nsimple hint : in this EHR case study, whenever there is a network connection, it's a dedicated interconnect answer !","upvote_count":"17","comment_id":"759615","poster":"Wael216","timestamp":"1687936140.0"},{"poster":"omermahgoub","upvote_count":"4","content":"The recommended solution for hybrid connectivity between on-premises systems and Google Cloud is to configure two Dedicated Interconnect connections in two different metros (cities). This ensures that EHR Healthcare has a redundant connection to Google Cloud, with each connection providing a separate physical path. Placing the Interconnect connections in different metro zones also helps to ensure that the connection is resilient to failures in a single geographic region. This solution meets the business requirement of providing a secure and high-performance connection between on-premises systems and Google Cloud, as well as the technical requirement of maintaining regulatory compliance. It also helps to meet the requirement of providing consistent logging, log retention, monitoring, and alerting capabilities, as Dedicated Interconnect connections can be used in conjunction with Cloud Router to establish a connection between on-premises networks and Google Cloud VPC networks.","comment_id":"759532","timestamp":"1687931520.0"},{"comment_id":"752951","poster":"neokyle","timestamp":"1687398780.0","content":"Selected Answer: D\nEHR is supposed to be massive in size. so the option of 100 GBps / Dedicated is warranted.","upvote_count":"1"},{"timestamp":"1686982920.0","poster":"surajkrishnamurthy","comment_id":"747873","content":"Selected Answer: D\nD is the correct answer","upvote_count":"1"},{"timestamp":"1684328640.0","upvote_count":"1","poster":"megumin","comment_id":"720561","content":"Selected Answer: D\nD is ok"},{"timestamp":"1681459200.0","content":"Selected Answer: D\nBusiness requirements for this case:\n\n* Provide a minimum 99.9% availability for all customer-facing systems.\n* Provide a secure and high-performance connection between on-premises systems and Google Cloud.\n\nA. - builds us a 99.9% SLA partner interconnect, covering all business requirements.\nB. - VPN is not suitable for the business requirements.\nC. - Direct peering is used for workspace, instead of DMZ, again - not suitable.\nD. - builds us a 99.99% SLA dedicated interconnect, covering all business requirements.\n\nThe answer to choosing A or D lies in the question, stating: \"You want to follow Google's recommended practices for production-level applications.\"\n\nGoogle recommends using the 99.99% SLA interconnect (dedicated or partner) for production-level applications as stated here:\nhttps://cloud.google.com/network-connectivity/docs/interconnect/tutorials/production-level-overview\n\nThe answer is D.","comment_id":"694588","poster":"exam9391","upvote_count":"9"},{"comments":[{"content":"\"Google's recommended practices for production-level applications\"\nhttps://cloud.google.com/network-connectivity/docs/interconnect/tutorials/production-level-overview\n\nGoogle's recommended practice is to use 4 Interconnect connections split across two regions. Answer D mentions 2 Interconnect connections in one metro/city (region) and another 2 Interconnect connections in another metro (region), which is clearly referring to Google's recommended practice.","comment_id":"691487","timestamp":"1681158180.0","poster":"jahiye3916","upvote_count":"3"}],"poster":"deepdowndave","timestamp":"1679246400.0","comment_id":"673407","upvote_count":"2","content":"Selected Answer: A\nThe case study requires 99.9% availability. Only the setup with two partner interconnects in two metro zones serves 99.9%\nhttps://cloud.google.com/network-connectivity/docs/interconnect/tutorials/partner-creating-999-availability\nD is wrong since 99.9% availability does not require 4 interconnects in two metros and case study mentions to keep costs low."},{"timestamp":"1678289760.0","content":"Selected Answer: A\ntwo points to note:\n- requirements says a minimum of 99.9% (not 99.99%)\n- also, \"Decrease infrastructure administration costs.\"","upvote_count":"2","poster":"kuboraam","comment_id":"663665"},{"comment_id":"648322","upvote_count":"1","timestamp":"1676707920.0","poster":"rmahendra","content":"I think D is more suitable because it requires low latency and more spread out datacenter locations. Moreover, it is not necessarily the location of the legacy datacenter support provider interconnect"},{"upvote_count":"2","timestamp":"1655202360.0","poster":"vincy2202","comment_id":"501343","content":"Selected Answer: D\nD seems to be the correct answer"},{"content":"Selected Answer: D\nvote D","upvote_count":"4","comment_id":"487801","poster":"joe2211","timestamp":"1653612120.0"},{"poster":"rishab86","timestamp":"1648980960.0","comment_id":"456513","upvote_count":"3","content":"case study says \" Provide a minimum 99.9% availability for all customer-facing systems\", i think minimum is the keyword , hence I would go with D"},{"comment_id":"456354","upvote_count":"1","poster":"BrijMohan08","timestamp":"1648949520.0","comments":[{"upvote_count":"1","poster":"BrijMohan08","content":"OPEX low","timestamp":"1648949580.0","comment_id":"456358"}],"content":"Both A and D will work, but they want the SLA 99.9% and keep the cost low, which is possible with A (cost low)"},{"comment_id":"450795","timestamp":"1648121160.0","comments":[{"upvote_count":"1","comment_id":"663663","timestamp":"1678289700.0","poster":"kuboraam","content":"This was exactly my initial thought as well. 99.9% (not 99.99%), and infra cost savings..\ni was surprised when I saw all votes for D. I'm going with A."}],"upvote_count":"2","content":"Will go with A. Decreasing Infra administration costs is a business requirement - option D won't provide that. Also, 99.9% can be achieved with partner interconnect in 2 zones in a metro which is the min requirement.","poster":"Ari_GCP"},{"poster":"diaga2","timestamp":"1646860800.0","comment_id":"442154","content":"It's asking for 'Google's recommended practices for production-level applications' and there is no cost recommendations. So the best (costly) option will be D.","upvote_count":"2"},{"poster":"Rzla","comment_id":"441604","upvote_count":"1","content":"Difficult one. I would go with A on the information. No detail on whether Google prescence in existing colos and requirement reduce infra admin costs which are high when setting up dedicated interconnect.","comments":[{"upvote_count":"1","content":"Scratch that its D. Gives the highest SLA for prod workloads.","timestamp":"1648069680.0","poster":"Rzla","comment_id":"450483"}],"timestamp":"1646771520.0"},{"comments":[{"timestamp":"1646709180.0","content":"The following resources and settings are required to achieve 99.99% availability:\n\nAt least four Interconnect connections, two connections in one metropolitan area (metro) and two connections in another metro. Interconnect connections that are in the same metro must be placed in different edge availability domains (metro availability zones).\n\nhttps://cloud.google.com/network-connectivity/docs/interconnect/tutorials/dedicated-creating-9999-availability","comments":[{"poster":"EricG77","timestamp":"1679051820.0","content":"Its definitely D as Manh says per \"Google Best Practices\". https://cloud.google.com/network-connectivity/docs/interconnect/tutorials/dedicated-creating-9999-availability","comment_id":"671384","upvote_count":"1"}],"comment_id":"441163","poster":"Manh","upvote_count":"3"}],"poster":"Manh","timestamp":"1646708940.0","upvote_count":"1","comment_id":"441161","content":"D for better performance and high SLA"},{"upvote_count":"5","timestamp":"1646492820.0","comment_id":"439705","content":"Answer: A - Provide a minimum 99.9% availability for all customer-facing systems. https://cloud.google.com/network-connectivity/docs/interconnect/tutorials/partner-creating-999-availability - Two partner interconnects in a single metro, each in a different edge availability domain.\n\nD is overkill at 99.99% SLA","poster":"chouse","comments":[{"upvote_count":"5","poster":"MikeB19","timestamp":"1647856860.0","comment_id":"448750","content":"I agree with A too. Here is the article to back it up. 99.9 means two connections in separate zones. 99.99 would 2 regions and 2 zones in each region \nhttps://cloud.google.com/network-connectivity/docs/interconnect/tutorials/partner-creating-999-availability"},{"comment_id":"454874","timestamp":"1648640220.0","upvote_count":"6","poster":"Roncy","content":"yes D will overkill the request but if you read https://cloud.google.com/network-connectivity/docs/interconnect/tutorials/partner-creating-999-availability it clearly says that This topology is suitable for non-critical applications that can tolerate some downtime"}]}],"question_text":"For this question, refer to the EHR Healthcare case study. You need to define the technical architecture for hybrid connectivity between EHR's on-premises systems and Google Cloud. You want to follow Google's recommended practices for production-level applications. Considering the EHR Healthcare business and technical requirements, what should you do?","answer_images":[],"question_id":239,"answers_community":["D (90%)","10%"],"url":"https://www.examtopics.com/discussions/google/view/60435-exam-professional-cloud-architect-topic-4-question-4/","choices":{"B":"Configure two VPN connections from on-premises to Google Cloud, and make sure the VPN devices on-premises are in separate racks.","D":"Configure two Dedicated Interconnect connections in one metro (City) and two connections in another metro, and make sure the Interconnect connections are placed in different metro zones.","C":"Configure Direct Peering between EHR Healthcare and Google Cloud, and make sure you are peering at least two Google locations.","A":"Configure two Partner Interconnect connections in one metro (City), and make sure the Interconnect connections are placed in different metro zones."}},{"id":"vgJfnw7gVFQESRKrWfwq","topic":"4","isMC":true,"discussion":[{"upvote_count":"37","comment_id":"430416","poster":"raf2121","comments":[{"poster":"A21325412","content":"Updated link:\nhttps://cloud.google.com/pubsub/docs/publish-best-practices?hl=en#configure-batch","timestamp":"1699623960.0","comment_id":"1067264","upvote_count":"4"}],"content":"Answer : C (https://cloud.google.com/pubsub/docs/publisher?hl=en#batching)\nCost of Batching is latency for individual messages,. To minimize latency batching should be turned off","timestamp":"1629772380.0"},{"upvote_count":"14","poster":"gingerbeer","comment_id":"455978","timestamp":"1633167720.0","content":"C - The cost of batching is latency for individual messages, which are queued in memory until their corresponding batch is filled and ready to be sent over the network. To minimize latency, batching should be turned off.\nhttps://cloud.google.com/pubsub/docs/publisher?hl=en#batching\n\nA incorrect. Application timeout because of publisher latency, nothing to do with timeout retry with publish request.\nD does not make sense at all.\nB is about receiver, not publisher."},{"timestamp":"1733241720.0","content":"Selected Answer: A\nThe issue at hand is increased load causing timeout errors during interactions with Pub/Sub. The lack of Pub/Sub publishing errors indicates that the problem is likely related to retries or network performance. By increasing the Total Timeout retry value, you allow the application more time to complete message publishing during high-load scenarios, reducing the chances of timeouts","upvote_count":"2","poster":"desertlotus1211","comment_id":"1321438"},{"timestamp":"1714552680.0","comment_id":"1204954","upvote_count":"2","poster":"Gino17m","content":"Selected Answer: C\n\"the application is not loggin any Pub/Sub publishing errors\", so no need to increase Pub/Sub Total Timeout."},{"content":"Latency in Pub/Sub can be of two types:\n\nEnd-to-end latency is the time it takes for a message to be published by a publisher and delivered to the corresponding subscribers for processing.\n\nPublish latency is the amount of time it takes to publish a message.\n\nWhen using batching, increasing both types of latencies is a trade off for improving efficiency and throughput.","timestamp":"1712777820.0","poster":"a53fd2c","comment_id":"1193257","upvote_count":"3"},{"upvote_count":"2","timestamp":"1703622720.0","comment_id":"1106362","poster":"decw","content":"Selected Answer: C\nC\nhttps://cloud.google.com/pubsub/docs/publish-best-practices#configure-batch"},{"comment_id":"1028149","poster":"Prakzz","content":"Selected Answer: A\nSee Solution #1 below link\nhttps://saturncloud.io/blog/how-to-fix-deadlineexceeded-when-publishing-to-a-cloud-pubsub-topic-from-compute-engine/#:~:text=The%20timeout%20limit%20for%20publishing,receive%20the%20DEADLINE_EXCEEDED%20error%20message.","upvote_count":"2","timestamp":"1696782060.0"},{"poster":"TopTalk","timestamp":"1695582480.0","comment_id":"1016120","content":"Selected Answer: C\nSee video \"Cloud Pub/Sub Publishers - ep. 4\" at 2m 25 sec where Priyanka says \"to reduce latency, batching should be turned off\" https://www.youtube.com/watch?v=ML6P1ksHcqo&list=PLIivdWyY5sqKwVLe4BLJ-vlh9r9zCdOse&index=4. Batching increases throughput but adds latency for individual messages as they are queued in memory until their batch is filled.","upvote_count":"2"},{"comment_id":"926736","poster":"BiddlyBdoyng","content":"I voted C but I think it has to be A. \"Total timeout: the amount of time after a client library stops retrying publish requests.\"\n\n\"After each publish request, the request timeout increases by the request timeout multiplier, up to the maximum request timeout.\"\n\nSo it's increasing the timeout value on the retry which seems like the best solution.","upvote_count":"2","timestamp":"1687097700.0"},{"upvote_count":"5","comment_id":"866272","poster":"JC0926","content":"Selected Answer: B\nB. Move from a Pub/Sub subscriber pull model to a push model.\n\nExplanation:\nMoving from a pull model to a push model in Google Cloud Pub/Sub can help improve the latency in your application. In a push model, the messages are pushed from the Pub/Sub service to the subscriber application, reducing the time it takes for the application to receive messages. This can help mitigate the timeout errors that you are experiencing due to increased load on the application servers.","timestamp":"1681130880.0","comments":[{"content":"Option A, increasing the Pub/Sub Total Timeout retry value, would not address the latency issue directly; it would only increase the time the publisher would wait for a response before considering it a failure.\n\nOption C, turning off Pub/Sub message batching, might actually increase latency and decrease throughput, as batching can improve the efficiency of message delivery.\n\nOption D, creating a backup Pub/Sub message queue, would not solve the latency issue directly; it might provide a failover mechanism but would not address the root cause of the problem.","upvote_count":"1","comment_id":"866273","timestamp":"1681130880.0","poster":"JC0926"}]},{"upvote_count":"3","content":"Selected Answer: A\nThis is chatgpt's choice\nI agree that Pub/Sub message batching can be a useful optimization for improving overall throughput and reducing the number of API calls required to publish messages. However, in the context of addressing timeout errors during publishing, turning off message batching may not be the most appropriate solution.\n\nIn cases where message batching is causing issues, such as network or system resource constraints, reducing the batch size or adjusting the batch duration can help improve publishing latency. However, in the case of timeout errors, increasing the Total Timeout retry value would be a more effective solution, as it allows more time for the message to be successfully published and reduces the likelihood of encountering timeout errors.","poster":"MaryMei","comments":[{"poster":"afxwin","content":"Have you read ChatGPT's disclaimer?","comment_id":"1104539","upvote_count":"5","timestamp":"1703413080.0"}],"timestamp":"1678560300.0","comment_id":"836408"},{"content":"To improve publishing latency in this scenario, it is recommended to turn off Pub/Sub message batching. By turning off message batching, you can send messages individually as soon as they are published, rather than waiting for a batch of messages to be created before sending them. This can help to reduce the risk of timeout errors and improve the overall performance of the application. It is also a good idea to monitor the application's performance and error logs to identify any other potential issues that may be contributing to the timeout errors.","upvote_count":"5","timestamp":"1672214100.0","comment_id":"759535","poster":"omermahgoub"},{"upvote_count":"1","poster":"surajkrishnamurthy","comment_id":"747876","timestamp":"1671265500.0","content":"Selected Answer: C\nC is the correct answer"},{"timestamp":"1671185340.0","poster":"Prashant2022","comment_id":"747033","content":"but how is this related to turnoff batching??","upvote_count":"1","comments":[{"comment_id":"747035","poster":"Prashant2022","content":"Let me ans: becuz we need to speed up the time to deliver the msgs to the app! and it waits and timesout..","upvote_count":"1","timestamp":"1671185460.0"}]},{"poster":"megumin","comment_id":"720593","content":"Selected Answer: C\nC is ok","timestamp":"1668699240.0","upvote_count":"1"},{"comments":[{"timestamp":"1672663140.0","poster":"BlankSong","upvote_count":"3","comment_id":"763718","content":"Batch messaging is enabled by default in a client library.\nhttps://cloud.google.com/pubsub/docs/publisher?hl=en#batching"}],"upvote_count":"1","timestamp":"1666960140.0","poster":"Tesla","comment_id":"706448","content":"But no where in the question or scenario it says they turned on Pub/Sub batching."},{"content":"Selected Answer: C\nC is better option, even though increasing total timeout would help reduce timeout errors but remember that that in this case we are getting too many messages from the server since load increased and we need to reduce latency","upvote_count":"1","comment_id":"700209","timestamp":"1666291560.0","poster":"Mahmoud_E"},{"content":"Selected Answer: C\nAgreed with C","comment_id":"598570","poster":"amxexam","upvote_count":"2","timestamp":"1652017080.0"},{"timestamp":"1641728280.0","poster":"OrangeTiger","comment_id":"520158","content":"Selected Answer: C\nI agree C.\nTy guys!","upvote_count":"2"},{"content":"Selected Answer: C\nC is the correct answer\nhttps://cloud.google.com/pubsub/docs/publisher?hl=en#batching","comment_id":"501037","timestamp":"1639452360.0","poster":"vincy2202","upvote_count":"3"},{"content":"Selected Answer: C\nVote C","comment_id":"493155","poster":"pakilodi","timestamp":"1638537300.0","upvote_count":"2"},{"upvote_count":"2","timestamp":"1638030780.0","poster":"[Removed]","comment_id":"488282","content":"Selected Answer: C\nMarked A is wrong.\nC is correct."},{"upvote_count":"4","comment_id":"487802","poster":"joe2211","timestamp":"1637980920.0","content":"Selected Answer: C\nvote C"},{"upvote_count":"4","poster":"BrijMohan08","timestamp":"1632823080.0","content":"It's not about Latency so C is wrong, it's talking about timeout, I guess increasing the total timeout retry value will help hence answer is A\n\nTotal timeout: the amount of time after a client library stops retrying publish requests.","comment_id":"453264"},{"upvote_count":"3","poster":"Sarguna","content":"Answer C\nOption A can be ruled out as increasing total timeout will delay the retry of the published request.","comment_id":"443526","timestamp":"1631458080.0"},{"comment_id":"442160","content":"Focus should be on 'The load has increased on the application servers, and now the application is logging many timeout errors' so to improve it, we need to turn off the newly implemented pub/sub batching. Answer C","timestamp":"1631215740.0","upvote_count":"3","poster":"diaga2"},{"timestamp":"1630347060.0","poster":"pr2web","content":"https://cloud.google.com/pubsub/docs/publisher?hl=en#batching\n\nThe cost of batching is latency for individual messages, which are queued in memory until their corresponding batch is filled and ready to be sent over the network. To minimize latency, batching should be turned off. \n\nAnswer C","comment_id":"435720","upvote_count":"3"},{"poster":"victory108","upvote_count":"4","comment_id":"434047","timestamp":"1630169220.0","content":"C. Turn off Pub/Sub message batching."},{"poster":"SoniaJacob521","timestamp":"1629735060.0","upvote_count":"2","comment_id":"430167","content":"A- Increase timeout retry"}],"unix_timestamp":1629735060,"choices":{"D":"Create a backup Pub/Sub message queue.","B":"Move from a Pub/Sub subscriber pull model to a push model.","C":"Turn off Pub/Sub message batching.","A":"Increase the Pub/Sub Total Timeout retry value."},"answers_community":["C (67%)","A (19%)","14%"],"timestamp":"2021-08-23 18:11:00","answer_description":"","answer_ET":"C","exam_id":4,"question_text":"For this question, refer to the EHR Healthcare case study. You are a developer on the EHR customer portal team. Your team recently migrated the customer portal application to Google Cloud. The load has increased on the application servers, and now the application is logging many timeout errors. You recently incorporated Pub/Sub into the application architecture, and the application is not logging any Pub/Sub publishing errors. You want to improve publishing latency.\nWhat should you do?","answer":"C","question_id":240,"question_images":[],"url":"https://www.examtopics.com/discussions/google/view/60405-exam-professional-cloud-architect-topic-4-question-5/","answer_images":[]}],"exam":{"id":4,"name":"Professional Cloud Architect","isBeta":false,"isImplemented":true,"lastUpdated":"11 Apr 2025","isMCOnly":false,"numberOfQuestions":279,"provider":"Google"},"currentPage":48},"__N_SSP":true}