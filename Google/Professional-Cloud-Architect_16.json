{"pageProps":{"questions":[{"id":"r3JINFpHfCbofQbNq2rX","discussion":[{"content":"Agree C","timestamp":"1610387280.0","poster":"AWS56","comment_id":"37787","upvote_count":"24"},{"upvote_count":"11","content":"A is incorrect because there is supposed to be two hypens \"--\" not one before size (https://cloud.google.com/sdk/gcloud/reference/container/clusters/resize). B is incorrect because it just adds a string to the cluster (https://cloud.google.com/sdk/gcloud/reference/compute/instances/add-tags). \"C\" is just as wrong as \"A\" because the documentation says it should be \"--max-nodes\" followed by \"--min-nodes\" (https://cloud.google.com/sdk/gcloud/reference/alpha/container/clusters/update), also the alpha command no longer works but it used to and is still up on google docs. This goes for \"D\" as well but D talks about making another, which doesn't have to be done because one it already up. So the debate is between A and C, and C used to work so C was chosen, although C also has spaces which never worked... So this question is an absolute thug tactic by a Google team to steal from the Google kingdom preventing the establishment of their library by failing people that actually know the science behind the technology. When you see this question at a test center I'd select C.","comment_id":"16966","poster":"Eroc","comments":[{"comments":[{"upvote_count":"9","comment_id":"157880","content":"To enable autoscaling for an existing node pool, run the following command:\n\ngcloud container clusters update cluster-name --enable-autoscaling \\\n --min-nodes 1 --max-nodes 10 --zone compute-zone --node-pool default-pool\n\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/cluster-autoscaler","poster":"tartar","timestamp":"1628925300.0"}],"comment_id":"151014","poster":"tartar","content":"C is ok","upvote_count":"9","timestamp":"1628152740.0"},{"poster":"svjl","upvote_count":"4","timestamp":"1639060500.0","content":"You didn't check the documentation.","comment_id":"239269"}],"timestamp":"1603458900.0"},{"comment_id":"1334662","upvote_count":"1","content":"Selected Answer: C\nAgreed with C because \n\nExplanation:\nAutoscaling in GKE\n\nKubernetes Engine supports Cluster Autoscaler, which automatically adjusts the size of a node pool based on the resource demands of the workloads.\nEnabling autoscaling allows the cluster to add or remove nodes dynamically to handle increased load or scale down during low activity.\nUpdating the Existing Cluster\n\nThe command in option C updates an existing cluster with autoscaling enabled, setting minimum (1) and maximum (10) node limits.\nThis avoids downtime and does not require recreating the cluster or redeploying applications, ensuring a seamless transition to autoscaling.","poster":"piyu1515","timestamp":"1735624440.0"},{"content":"Agree C","upvote_count":"1","comment_id":"979404","timestamp":"1723472580.0","poster":"anil23"},{"timestamp":"1709150340.0","comment_id":"825225","upvote_count":"5","content":"Selected Answer: C\nno need to create a new one, just update!","poster":"AugustoKras011111"},{"poster":"zerg0","timestamp":"1706928240.0","upvote_count":"1","comment_id":"796655","content":"Selected Answer: C\nSee the cli docs"},{"upvote_count":"2","comment_id":"746001","timestamp":"1702639860.0","content":"Selected Answer: C\nC is the correct answer","poster":"surajkrishnamurthy"},{"timestamp":"1702492560.0","content":"Selected Answer: C\nIt's C","poster":"gonlafer","upvote_count":"1","comment_id":"744365"},{"comment_id":"711649","upvote_count":"1","content":"Selected Answer: C\nok for C","timestamp":"1699177440.0","poster":"megumin"},{"timestamp":"1697393700.0","upvote_count":"2","comment_id":"695596","content":"I agree with C","poster":"AzureDP900"},{"comment_id":"501998","poster":"vincy2202","timestamp":"1671094560.0","upvote_count":"2","content":"Selected Answer: C\nC is the correct answer"},{"upvote_count":"1","timestamp":"1670577780.0","content":"Selected Answer: C\nC looks OK","poster":"Bobch","comment_id":"497516"},{"upvote_count":"6","poster":"TheCloudBoy77","timestamp":"1668994260.0","comment_id":"482964","content":"C - cluster is already running so use update instead of create new cluster."},{"upvote_count":"5","poster":"[Removed]","content":"Answer should be C. Now alpha command is not needed. seems question is older and now kubernets command is not with alpha.\ngcloud container clusters update cluster-name --enable-autoscaling ....","timestamp":"1666019100.0","comment_id":"463586"},{"content":"This couldn’t be C, you shouldn’t use alpha commands in a production(app) workload.","timestamp":"1663067400.0","upvote_count":"1","poster":"Examster1","comment_id":"443923"},{"upvote_count":"2","timestamp":"1656991860.0","poster":"kopper2019","content":"C is the way to go min and max and done","comment_id":"398804"},{"comment_id":"360079","content":"C. Update the existing Kubernetes Engine cluster with the following command: gcloud alpha container clusters update mycluster - -enable- autoscaling - -min-nodes=1 - -max-nodes=10","upvote_count":"3","poster":"victory108","timestamp":"1652851680.0"},{"poster":"Amber25","comment_id":"359906","timestamp":"1652833620.0","upvote_count":"3","content":"Answer- C.\nUpdate command and autoscaling tag will update existing running kubernetes cluster."},{"timestamp":"1651852500.0","poster":"un","content":"C is correct","comment_id":"351152","upvote_count":"2"},{"content":"C is correct","upvote_count":"2","comment_id":"350464","timestamp":"1651768500.0","poster":"un"},{"poster":"JohnWick2020","content":"C is correct! \n\nKeynotes from question - \"running\" GKE cluster and dynamic scale based on demand.\n\nExplanation:\nA - Incorrect; manual scale, wasteful and an overhead.\nB - Incorrect; tagging is necessary for automation but does not address immediate ask.\nC - CORRECT; updates \"running\" cluster config by enabling auto-scaling. This meets ask.\nD - Incorrect; can rule out straightaway. This deviates from ask by creating a new cluster. Recall the question relates to a \"running\" cluster.","upvote_count":"1","timestamp":"1649749380.0","comment_id":"333779"},{"timestamp":"1648622640.0","poster":"lynx256","upvote_count":"1","comment_id":"324048","content":"C is ok"},{"poster":"Ausias18","content":"Answer is C","upvote_count":"1","comment_id":"323119","timestamp":"1648530060.0"},{"timestamp":"1644809460.0","content":"\"C\" looks correct.","poster":"Joyjit_Deb","upvote_count":"2","comment_id":"289973"},{"upvote_count":"2","comment_id":"239277","timestamp":"1639060800.0","poster":"svjl","content":"C: https://cloud.google.com/sdk/gcloud/reference/container/clusters/update"},{"poster":"BhupalS","timestamp":"1638948900.0","upvote_count":"4","comment_id":"238002","content":"None of the options seems right , only C seems close ..\n=> actual answer should be as below \ngcloud container clusters update cluster-name --enable-autoscaling \\\n --min-nodes 1 --max-nodes 10 --zone compute-zone --node-pool default-pool"},{"poster":"Aru23","content":"C is correct","timestamp":"1634050560.0","comment_id":"198503","upvote_count":"1"},{"comment_id":"188020","timestamp":"1632701160.0","upvote_count":"1","content":"C is right","poster":"AshokC"},{"content":"C is ok","poster":"gkdinesh","comment_id":"173892","timestamp":"1630840620.0","upvote_count":"1"},{"poster":"abhi10436","comment_id":"164778","content":"C is correct","upvote_count":"1","timestamp":"1629765780.0"},{"comment_id":"151995","upvote_count":"1","poster":"mohitchy","timestamp":"1628258340.0","content":"c is correct"},{"poster":"mlantonis","upvote_count":"2","content":"It's C, but \"alpha\" is no longer needed.","timestamp":"1624380120.0","comment_id":"116611"},{"poster":"gfhbox0083","content":"C, for sure","upvote_count":"1","comment_id":"106478","timestamp":"1623300780.0"},{"upvote_count":"3","poster":"Nirms","timestamp":"1622636340.0","content":"C is the correct answer","comment_id":"100789"},{"poster":"Ziegler","upvote_count":"3","timestamp":"1622284260.0","content":"C is the correct answer","comment_id":"98166"},{"upvote_count":"2","timestamp":"1621805280.0","comment_id":"94577","poster":"Javed","content":"Answer C"},{"upvote_count":"3","comments":[{"upvote_count":"1","timestamp":"1645276260.0","content":"Manually scalling the cluster does not fit the \" to scale as demand for your application changes.\" requirement. \n\"As demand changes\" implies autoscalling","poster":"Alekshar","comment_id":"294239"},{"content":"Do not use both Cluster Autoscaler and manual resize commands simultaneously on a node pool, as this can cause interactions that result in unstable and/or incorrect node pool size.","poster":"shashu07","upvote_count":"1","comment_id":"109892","timestamp":"1623649140.0"}],"poster":"skywalker","comment_id":"88763","timestamp":"1620973200.0","content":"Vote for \"A\"...\nRemember is using gcloud command here.. not kubectl command.... thus \"resize\" keywords apply\n\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/resizing-a-cluster"},{"upvote_count":"3","timestamp":"1620848460.0","content":"C: https://cloud.google.com/sdk/gcloud/reference/alpha/container/clusters/update","poster":"jeckie2877","comment_id":"87923"},{"content":"I'll go with C.\nOption A doesn't deal with auto scaling.\nOption B seems wrong - why would tags make a difference?\nOption D is disruptive.","poster":"clouddude","upvote_count":"3","comment_id":"86176","timestamp":"1620587580.0"},{"content":"C of the correct answer","upvote_count":"4","poster":"gcp_aws","timestamp":"1620035580.0","comment_id":"82972"},{"comment_id":"70964","poster":"pollen22","timestamp":"1617523680.0","comments":[{"upvote_count":"1","comment_id":"79715","timestamp":"1619415780.0","poster":"TCSH","content":"which one you took? PR000178 (with onsite/online version)or PR000088 (with onsite version only)"}],"content":"The answer is C because the documentation says also. I passed the exam this week. Most of the questions are form this forum so make sure you do all 200 + questions. See the comments for the right answers","upvote_count":"8"},{"timestamp":"1612010820.0","content":"answer: C","comment_id":"44674","poster":"2g","upvote_count":"4"},{"timestamp":"1605538920.0","upvote_count":"6","content":"Tricky Question...\"enable your running Google Kubernetes Engine cluster \" --> Make a note of the keyword \"running\". He is asking about an existing and running GKE cluster.\n\nAnswer is C","comment_id":"21985","poster":"AWS56","comments":[{"comment_id":"274561","upvote_count":"1","timestamp":"1642940700.0","content":"https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-autoscaler#enabling_autoscaling_for_an_existing_node_pool","poster":"fraloca"}]}],"unix_timestamp":1571836500,"answer":"C","url":"https://www.examtopics.com/discussions/google/view/7073-exam-professional-cloud-architect-topic-1-question-167/","question_images":[],"exam_id":4,"topic":"1","isMC":true,"timestamp":"2019-10-23 15:15:00","choices":{"C":"Update the existing Kubernetes Engine cluster with the following command: gcloud alpha container clusters update mycluster - -enable- autoscaling - -min-nodes=1 - -max-nodes=10","B":"Add a tag to the instances in the cluster with the following command: gcloud compute instances add-tags INSTANCE - -tags enable- autoscaling max-nodes-10","A":"Add additional nodes to your Kubernetes Engine cluster using the following command: gcloud container clusters resize CLUSTER_Name ג€\" -size 10","D":"Create a new Kubernetes Engine cluster with the following command: gcloud alpha container clusters create mycluster - -enable- autoscaling - -min-nodes=1 - -max-nodes=10 and redeploy your application"},"question_text":"You want to enable your running Google Kubernetes Engine cluster to scale as demand for your application changes.\nWhat should you do?","question_id":76,"answer_images":[],"answer_ET":"C","answer_description":"","answers_community":["C (100%)"]},{"id":"w5PhA2NrWYWZa2FBP8D1","answers_community":["AC (100%)"],"isMC":true,"topic":"1","question_images":[],"answer_ET":"AC","discussion":[{"content":"A & C seems to be the correct answer.","comment_id":"373346","timestamp":"1622705520.0","poster":"rishab86","upvote_count":"27"},{"timestamp":"1625320740.0","comments":[{"poster":"J19G","comment_id":"461301","content":"Why not D?","comments":[{"upvote_count":"6","content":"Because a single GCE instance might not be able to handle the unpredictable load","poster":"Bert_77","timestamp":"1639058220.0","comment_id":"497772"}],"timestamp":"1634083740.0","upvote_count":"1"}],"upvote_count":"10","poster":"victory108","comment_id":"397607","content":"A. Use Google App Engine to serve the website and Google Cloud Datastore to store user data.\nC. Use a managed instance group to serve the website and Google Cloud Bigtable to store user data."},{"poster":"Sephethus","timestamp":"1718898420.0","upvote_count":"3","content":"Selected Answer: AC\nBoth bigtable and datastore seem like overkill solutions but A&C are the only options that make sense here. In the real world use BigQuery and either App Engine or Cloud Run.","comment_id":"1233743"},{"poster":"anil23","timestamp":"1691851080.0","upvote_count":"1","comment_id":"979409","content":"Why not B, GKE si best fit and preferred over MI","comments":[{"poster":"piyu1515","timestamp":"1735624620.0","upvote_count":"1","content":"Because they want to minimize direct operation management","comment_id":"1334663"}]},{"comment_id":"796660","content":"Selected Answer: AC\nCloud Data store and Big Table are the only solutions that can handle 500000 clicks","upvote_count":"3","timestamp":"1675392420.0","poster":"zerg0"},{"comment_id":"757243","timestamp":"1672043340.0","upvote_count":"6","poster":"omermahgoub","comments":[{"timestamp":"1672043340.0","upvote_count":"1","poster":"omermahgoub","content":"B, using a Google Container Engine cluster to serve the website and store data to persistent disk, could be a valid solution as well. However, persistent disks may not be able to scale horizontally to meet high traffic demands, which could impact the performance of the website.\n\nD, using a single Compute Engine VM to host a web server, backed by Google Cloud SQL, would not be a good choice for this scenario. A single VM would not be able to scale to meet the wide range of possible traffic levels for the promotional email campaign, and Google Cloud SQL may not be able to scale horizontally to meet high traffic demands.","comment_id":"757244"}],"content":"A: Google App Engine is a fully managed platform for building and running web applications and APIs. It can automatically scale to meet high traffic demands, making it a good choice for serving the website for the promotional email campaign. Google Cloud Datastore can also scale automatically to meet high traffic demands, making it a good choice for storing user data.\n\nC: A managed instance group are managed as a single entity and can automatically scale up or down based on demand. This makes it a good choice for serving the website for the promotional email campaign. Google Cloud Bigtable is a fully managed, high-performance NoSQL database that can store and serve large amounts of structured data with low latency. It is designed to scale horizontally and can handle high traffic demands, making it a good choice for storing user data."},{"timestamp":"1665857880.0","comment_id":"695599","poster":"AzureDP900","upvote_count":"3","content":"A and C is right choice, D is saying single VM"},{"poster":"Nirca","upvote_count":"2","content":"Selected Answer: AC\nAC (100%) !!!","timestamp":"1663488240.0","comment_id":"672107"},{"upvote_count":"1","poster":"Nirca","timestamp":"1663434000.0","content":"Selected Answer: AC\nA & C seems to be the correct answer.","comment_id":"671698"},{"timestamp":"1639558920.0","poster":"vincy2202","comment_id":"502000","content":"Selected Answer: AC\nAC is the correct answer.","upvote_count":"1"},{"timestamp":"1634990460.0","upvote_count":"1","content":"A only, choose two - App Engine + Datastore\n\nUse GAE to serve the website and Google Datastore to store user data.\n\nGCE – is too complex solution with specific OS to maintain.\nGKE – is for microservices apps, and Persistent Disk is not good solution for relational data storage;\nGAE – is fast and reliable solution, you write just code and run it on fully managed service. DataStore also matches perfectly since intended for storing user profiles, key-value pairs.","comment_id":"466543","poster":"MaxNRG"},{"timestamp":"1623538200.0","content":"A & B with less operations management. Also Containers and App Engine as the clicks varies.","comment_id":"380719","poster":"alan9999","upvote_count":"3","comments":[{"timestamp":"1627239720.0","comment_id":"414197","content":"\"Google Container Engine\" does not exist, only \"GKE\", but operating a Kubernetes cluster is not easy, in that case, an option could be Cloud Run.","upvote_count":"1","poster":"poseidon24"},{"timestamp":"1623660240.0","comment_id":"381707","poster":"AK2020","content":"But user data storing in persistent disks? Not correct to me. Seems A & C","upvote_count":"3"}]}],"answer":"AC","question_id":77,"answer_description":"","choices":{"B":"Use a Google Container Engine cluster to serve the website and store data to persistent disk.","A":"Use Google App Engine to serve the website and Google Cloud Datastore to store user data.","C":"Use a managed instance group to serve the website and Google Cloud Bigtable to store user data.","D":"Use a single Compute Engine virtual machine (VM) to host a web server, backend by Google Cloud SQL."},"question_text":"Your marketing department wants to send out a promotional email campaign. The development team wants to minimize direct operation management. They project a wide range of possible customer responses, from 100 to 500,000 click-through per day. The link leads to a simple website that explains the promotion and collects user information and preferences.\nWhich infrastructure should you recommend? (Choose two.)","answer_images":[],"timestamp":"2021-06-03 09:32:00","unix_timestamp":1622705520,"exam_id":4,"url":"https://www.examtopics.com/discussions/google/view/54373-exam-professional-cloud-architect-topic-1-question-168/"},{"id":"wOs7v1TUxP8hrsnGDRle","isMC":true,"question_text":"Your company just finished a rapid lift and shift to Google Compute Engine for your compute needs. You have another 9 months to design and deploy a more cloud-native solution. Specifically, you want a system that is no-ops and auto-scaling.\nWhich two compute products should you choose? (Choose two.)","question_id":78,"answer_description":"","timestamp":"2021-06-03 09:34:00","answer_ET":"BC","answer_images":[],"exam_id":4,"choices":{"E":"Compute Engine with managed instance groups","C":"Google App Engine Standard Environment","A":"Compute Engine with containers","B":"Google Kubernetes Engine with containers","D":"Compute Engine with custom instance types"},"answers_community":["BC (65%)","CE (35%)"],"answer":"BC","discussion":[{"poster":"PeppaPig","content":"I would go with B&C\nCloud-native, less-ops and auto-scaling all get addressed","comment_id":"417546","upvote_count":"18","timestamp":"1627666200.0"},{"comments":[{"poster":"AhmedH7793","timestamp":"1663664640.0","content":"No ops = Serverless / Almost Serverless MIG is not.","upvote_count":"8","comments":[{"timestamp":"1718263080.0","poster":"JaimeMS","upvote_count":"2","comment_id":"1229678","content":"No Ops -> Kubernetes?\nThis question is too generic to choose between MIG and GKE. In the context of this exam, I would choose B (GKE), specally with current options as Autopilot. But in my day to day I would consider way more factors."}],"comment_id":"673963"}],"comment_id":"580149","upvote_count":"9","poster":"kinghin","content":"Why E is incorrect? can't MIG also perform autoscaling? Also it needs fewer administration as GKE","timestamp":"1648972020.0"},{"content":"Selected Answer: CE\nA. Compute Engine with containers\nB. Google Kubernetes Engine with containers Most Voted\nC. Google App Engine Standard Environment Most Voted\nD. Compute Engine with custom instance types\nE. Compute Engine with managed instance groups\n\nNOT A B\nbuilding containers is time consuming, has comlicated operation overhead.\nthis is lift and shift.\nD. can't do migs\nC. E.","timestamp":"1739391120.0","upvote_count":"1","comment_id":"1355770","poster":"3a7557a"},{"poster":"plumbig11","content":"Selected Answer: BC\nno ops and more cloud native, GKE and app engine;","comment_id":"1336409","timestamp":"1736002500.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1735624800.0","poster":"piyu1515","comment_id":"1334664","content":"App Engine Standard is a Platform-as-a-Service (PaaS) offering that’s fully managed, allowing you to deploy applications with zero infrastructure management.\nIt supports auto-scaling out-of-the-box and handles traffic spikes automatically.\nIt’s great for stateless applications, APIs, and web applications that need rapid deployment and minimal operations.\nBest Use Case:\n\nIdeal for teams looking for a serverless approach without worrying about infrastructure scaling or patching."},{"timestamp":"1681438380.0","upvote_count":"5","content":"Selected Answer: BC\nOption B, Google Kubernetes Engine (GKE) with containers, is a managed Kubernetes service that automatically manages and scales containerized applications. GKE handles cluster management tasks like scaling, upgrades, and security patches, allowing you to focus on the application itself.\n\nOption C, Google App Engine Standard Environment, is a fully managed platform for building and deploying applications. It automatically scales applications based on demand and provides a no-ops experience. With App Engine Standard Environment, you don't need to worry about infrastructure management, as Google handles it for you.","poster":"JC0926","comment_id":"869914"},{"poster":"jlambdan","timestamp":"1680636960.0","upvote_count":"2","comment_id":"861450","content":"Selected Answer: BC\nB: GKE with autopilot mode for workload not requiring ingress or egress. Otherwise you will need some ops work IMHO.\nC: app engine for workload requiring ingress. It comes with autoscaling features and rolling update features without being as heavy as gke."},{"comment_id":"839376","poster":"Deb2293","content":"Selected Answer: CE\nI would still go for C & E. My take is GKE still requires some operational overhead for managing the Kubernetes cluster and ensuring high availability of the workloads.\nHence C & E would be most suitable one.","timestamp":"1678833720.0","upvote_count":"5"},{"poster":"telp","timestamp":"1676481480.0","comment_id":"809788","upvote_count":"1","content":"Selected Answer: BC\nNo ops: use container or gcp product without mangement.\nSo not VM possible in the answer"},{"poster":"habros","upvote_count":"2","content":"Selected Answer: BC\nApp Engine standard = container based (can even go to zero)\nApp Engine flexible = VM based (minimum 1)\nNo ops: container > VM","timestamp":"1669991040.0","comment_id":"733816"},{"comment_id":"699446","upvote_count":"3","content":"Selected Answer: BC\nB & C seems right to me, E needs lots of Ops to build image, instance template and instance group, ... maintain your image always","poster":"Mahmoud_E","timestamp":"1666226460.0"},{"upvote_count":"2","poster":"AzureDP900","comment_id":"695600","content":"B. Google Kubernetes Engine with containers\nC. Google App Engine Standard Environmen","timestamp":"1665858000.0"},{"comment_id":"666493","timestamp":"1662936600.0","poster":"alexandercamachop","content":"Selected Answer: BC\nNo ops = Serverless / Almost Serverless, less operational management overhead.\nKubernetes and App Engine are the only one that gives us that flexibility, plus is modernizing apps","upvote_count":"2"},{"upvote_count":"5","comment_id":"663755","content":"Selected Answer: CE\nC and E\nGKE is absolutely nor no-ops.\nMIG can be closest to no-ops among the other options","poster":"6721sora","timestamp":"1662652140.0"},{"content":"Selected Answer: BC\nB&C seem to be right for this question. In reality, whoever really proposes B as an option never ran Kubernetes in production.","upvote_count":"2","poster":"jabrrJ68w02ond1","timestamp":"1662286020.0","comment_id":"659155"},{"timestamp":"1652602980.0","poster":"JoeyCASD","content":"Vote A and B\nHowever I think option B should address more specifically, like GKE - autopilot mode.","comments":[{"upvote_count":"1","content":"Correct the answer for B and C","poster":"JoeyCASD","timestamp":"1652603040.0","comment_id":"601982"}],"upvote_count":"1","comment_id":"601981"},{"upvote_count":"1","poster":"vincy2202","content":"Selected Answer: BC\nBC are the correct answers","timestamp":"1639559160.0","comment_id":"502003"},{"poster":"Bobch","upvote_count":"1","content":"Selected Answer: BC\nAgree B and C","timestamp":"1639041960.0","comment_id":"497524"},{"timestamp":"1635667560.0","poster":"MaxNRG","comment_id":"470561","content":"Correct Answer: BC\nB: With Container Engine, Google will automatically deploy your cluster for you, update, patch, secure the nodes.\nKubernetes Engine's cluster autoscaler automatically resizes clusters based on the demands of the workloads you want to run.\nC: Solutions like Datastore, BigQuery, AppEngine, etc are truly NoOps.\nApp Engine by default scales the number of instances running up and down to match the load, thus providing consistent performance for your app at all times while minimizing idle instances and thus reducing cost.","upvote_count":"3","comments":[{"poster":"MaxNRG","timestamp":"1635667620.0","comment_id":"470562","content":"Note: At a high level, NoOps means that there is no infrastructure to build out and manage during usage of the platform. Typically, the compromise you make with NoOps is that you lose control of the underlying infrastructure.\nhttps://www.quora.com/How-well-does-Google-Container-Engine-support-Google-Cloud-Platform%E2%80%99s-NoOps-claim\nB – Google Container Engine (autoscaling)\nC – Google AppEngine Standard Environment (no ops)\nYou should understand this Q as following: after Lift-n-Shift parts of the monolith should be moved to managed services (e.g. REST API) running on GAE; and other micro-services will run in containers / pods.","upvote_count":"3"}]},{"poster":"Rzla","upvote_count":"3","timestamp":"1631039760.0","comment_id":"441058","content":"B & C. Although GKE standard is definitely not no-ops!"},{"timestamp":"1626070500.0","poster":"AshwathD","comments":[{"timestamp":"1627620540.0","content":"A, D,E are incorrect as Compute Engine is a managed service","comment_id":"417178","poster":"VishalB","upvote_count":"2"}],"upvote_count":"2","content":"B&E both talks about Compute Engine needs.","comment_id":"404415"},{"upvote_count":"4","comment_id":"397608","content":"B. Google Kubernetes Engine with containers\nC. Google App Engine Standard Environment","poster":"victory108","timestamp":"1625320800.0"},{"poster":"Areev","comment_id":"387129","timestamp":"1624280820.0","upvote_count":"1","content":"Option E provides Auto scaling but is it cloud-native?"},{"comments":[{"poster":"poseidon24","comment_id":"414199","upvote_count":"1","content":"Totally agree, just saying \"GKE\" with no more specification does not look like a good answer, whether the question or the answer should be updated.","timestamp":"1627239900.0"}],"comment_id":"384091","upvote_count":"3","timestamp":"1623924420.0","content":"The answer list seems incomplete. I would rather choose App Engine and Cloud Run / GKE Autopilot Mode (should be specified). GKE (standard mode) definitely requires operations handling and management.","poster":"cugena"},{"timestamp":"1623827280.0","content":"CE for me.\nIn B the autoscaling option is not stated, while in E the MIG feature is stated and provides autoscaling","comment_id":"383170","upvote_count":"4","poster":"__INSIDEOUT__"},{"poster":"rishab86","upvote_count":"4","content":"B & C seems to be correct.","timestamp":"1622705640.0","comment_id":"373348"}],"topic":"1","unix_timestamp":1622705640,"question_images":[],"url":"https://www.examtopics.com/discussions/google/view/54374-exam-professional-cloud-architect-topic-1-question-169/"},{"id":"ngJ7D3fjLNioplymM5Pe","url":"https://www.examtopics.com/discussions/google/view/7150-exam-professional-cloud-architect-topic-1-question-17/","question_images":[],"topic":"1","answer":"B","answers_community":["B (100%)"],"discussion":[{"timestamp":"1571932920.0","upvote_count":"37","content":"All four are correct answers. Google has built in cron job schduling with Cloud Schedule, so that would place \"D\" behind \"C\" in Google's perspective. Google also has it's own lifecycle management command line prompt gcloud lifecycle so \"A\" or \"B\" could be used. JSON is slightly faster than XML because of the \"{\" verse \"<c>\" distinguisher, with a Trie tree used for alphanumeric parsing. So between \"A\" and \"B\", choose \"B\". Between \"B\" and \"A\", \"B\" is slightly more efficient from the GCP operator perspective. So choose \"B\".","poster":"Eroc","comments":[{"upvote_count":"31","poster":"ghitesh","comment_id":"38792","timestamp":"1578988980.0","content":"gsutil command takes only json as input for lifecycle management. In case of API, both XML and json can be used.\nhttps://cloud.google.com/storage/docs/gsutil/commands/lifecycle\nhttps://cloud.google.com/storage/docs/xml-api/put-bucket-lifecycle\nhttps://cloud.google.com/storage/docs/json_api/v1/buckets/update"},{"content":"B is ok","comment_id":"151648","upvote_count":"8","poster":"tartar","timestamp":"1596685920.0"},{"comment_id":"303401","timestamp":"1614869340.0","content":"B is correct. Policy = JSON format. No matter if its AWS or GCP.","poster":"nitinz","upvote_count":"11"}],"comment_id":"17218"},{"poster":"clouddude","comment_id":"86289","timestamp":"1589071740.0","content":"I'll go with B.\nA is not reasonable because life cycle policies are not written in XML.\nB is reasonable and is cloud native.\nC requires a cron script which needs something to run the script and is a non-cloud native approach.\nD requires a cron script which needs something to run the script and is a non-cloud native approach.","upvote_count":"16"},{"timestamp":"1734347820.0","poster":"Mitthugcpguru","comment_id":"1327297","content":"Selected Answer: B\nGsutil take only json as input so B answer","upvote_count":"2"},{"content":"Selected Answer: B\n1. Lifecycle Management: Google Cloud Storage offers built-in lifecycle management rules specifically designed for automated data retention and deletion. This is the most efficient and cost-effective way to manage your backup files.\n\n2. JSON Format: Lifecycle rules are defined in JSON format.\n\n3. gsutil: The gsutil command-line tool is used to interact with Cloud Storage, including setting lifecycle configuration.","upvote_count":"1","timestamp":"1731271680.0","poster":"Ekramy_Elnaggar","comment_id":"1309626"},{"content":"choose B","upvote_count":"1","poster":"Hungdv","timestamp":"1723097220.0","comment_id":"1262336"},{"content":"The correct available answer is B. But in real life, we use Terraform tfvars file.","comment_id":"1062140","upvote_count":"1","timestamp":"1699107120.0","poster":"squishy_fishy"},{"upvote_count":"2","poster":"eka_nostra","timestamp":"1690369740.0","content":"Selected Answer: B\nCloud Storage has lifecycle management rules and could be applied with gsutil and gcloud storage buckets. It is common to use JSON for transferring data.","comment_id":"963633"},{"poster":"alekonko","comment_id":"847546","content":"Selected Answer: B\nB, gsutil can set policy using json file\nhttps://cloud.google.com/storage/docs/gsutil/commands/lifecycle#examples","upvote_count":"1","timestamp":"1679521380.0"},{"content":"To remove backup files older than 90 days from a Cloud Storage bucket and optimize ongoing Cloud Storage spend, you should consider writing a lifecycle management rule in JSON and pushing it to the bucket with gsutil, as described in option B.\n\nLifecycle management rules allow you to automatically delete objects from a Cloud Storage bucket based on age or other criteria, such as the object's storage class. By writing a rule in JSON and pushing it to the bucket with gsutil, you can specify that objects older than 90 days should be deleted, ensuring that the bucket only contains current backup files and minimizing Cloud Storage spend.\n\nOption A, C and D would not be suitable for this use case, as they do not allow you to specify lifecycle management rules that delete objects based on age.","comment_id":"750669","upvote_count":"3","timestamp":"1671525480.0","poster":"omermahgoub"},{"content":"B. Write a lifecycle management rule in JSON and push it to the bucket with gsutil\n\nTo remove backup files older than 90 days from a Cloud Storage bucket, you can use the lifecycle management feature in Cloud Storage. This feature allows you to specify rules to automatically delete objects based on their age.","upvote_count":"1","comment_id":"747819","timestamp":"1671259140.0","poster":"i_am_robot"},{"poster":"Melampos","content":"Selected Answer: B\nhttps://cloud.google.com/storage/docs/gsutil/commands/lifecycle","comment_id":"736516","upvote_count":"1","timestamp":"1670300760.0"},{"poster":"Bry_040706","upvote_count":"1","comment_id":"727302","content":"B. Life cycle management using JSON.","timestamp":"1669442760.0"},{"content":"Selected Answer: B\nB with JSON option is correct","comment_id":"721885","upvote_count":"1","poster":"AniketD","timestamp":"1668849540.0"},{"poster":"megumin","content":"Selected Answer: B\nB is ok","upvote_count":"1","comment_id":"711636","timestamp":"1667640360.0"},{"upvote_count":"1","comment_id":"701684","poster":"Mahmoud_E","timestamp":"1666459500.0","content":"Selected Answer: B\nB is the right answer"},{"comment_id":"696756","upvote_count":"1","poster":"zr79","content":"life cycle management is the answer written in JSON format. JSON is easier to write and read compared to XML which you can not use in commands","timestamp":"1665975180.0"},{"poster":"AzureDP900","comment_id":"696515","content":"Lifecycle management with JSON is right .. I will go with B","timestamp":"1665951120.0","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: B\nB. Write a lifecycle management rule in JSON and push it to the bucket with gsutil","comment_id":"693680","poster":"minmin2020","timestamp":"1665644940.0"},{"timestamp":"1663678380.0","poster":"holerina","content":"D looks correct schedule cron","upvote_count":"1","comment_id":"674176"},{"content":"B is the best practice option.","poster":"Angel_99","timestamp":"1660930140.0","comment_id":"649061","upvote_count":"1"},{"comment_id":"649059","upvote_count":"1","content":"Selected Answer: B\nB is the best practice option.","poster":"Angel_99","timestamp":"1660929540.0"},{"comment_id":"615384","upvote_count":"1","content":"Selected Answer: B\nB is fine","poster":"andrelsjunior","timestamp":"1655047080.0"},{"upvote_count":"1","poster":"Nirca","comment_id":"588482","timestamp":"1650439260.0","content":"Selected Answer: B\nB is the best practice option."},{"timestamp":"1643476140.0","content":"B is the most efficient option","upvote_count":"1","comment_id":"535606","poster":"ghadxx"},{"timestamp":"1643115180.0","upvote_count":"1","content":"B is correct.","comment_id":"532129","poster":"llanerox"},{"poster":"vincy2202","content":"Selected Answer: B\nB is the correct answer","upvote_count":"1","comment_id":"508472","timestamp":"1640346840.0"},{"poster":"haroldbenites","content":"Go for B","upvote_count":"1","comment_id":"493262","timestamp":"1638550920.0"},{"comment_id":"489475","upvote_count":"1","poster":"duocnh","content":"Selected Answer: B\nvote B","timestamp":"1638141960.0"},{"poster":"vincy2202","content":"B is the right answer.","timestamp":"1637304900.0","comment_id":"481423","upvote_count":"1"},{"upvote_count":"3","timestamp":"1624698180.0","comment_id":"391068","content":"B is the right option.\nA. XML is not fast\nC & D: Overhead of managing separate cron job and there is already inbuilt feature for Bucket lifecycle management.","poster":"aviratna"},{"timestamp":"1621325280.0","upvote_count":"1","comment_id":"360234","poster":"victory108","content":"B. Write a lifecycle management rule in JSON and push it to the bucket with gsutil"},{"content":"B is correct","poster":"un","comment_id":"353906","upvote_count":"1","timestamp":"1620663660.0"},{"poster":"lynx256","comment_id":"324120","content":"B is ok","upvote_count":"1","timestamp":"1617093780.0"},{"timestamp":"1617080760.0","poster":"Ausias18","content":"Answer is B","comment_id":"323984","upvote_count":"1"},{"poster":"dlzhang","upvote_count":"1","timestamp":"1609925280.0","comment_id":"260887","content":"B is the best answer."},{"poster":"BhupalS","comment_id":"238168","content":"B is right Ans \nThe following lifecycle management configuration JSON document specifies that all objects in this bucket that are more than 90 days old are deleted automatically:\n\n\n{\n \"rule\":\n [\n {\n \"action\": {\"type\": \"Delete\"},\n \"condition\": {\"age\": 90}\n }\n ]\n}","upvote_count":"8","timestamp":"1607425920.0"},{"poster":"AshokC","upvote_count":"2","content":"B\nhttps://cloud.google.com/storage/docs/gsutil/commands/lifecycle\nThe config-json-file specified on the command line should be a path to a local file containing the lifecycle configuration JSON document.\n1. gsutil -- JSON only\n2. API -- XML, JSON","timestamp":"1600103340.0","comment_id":"179439"},{"timestamp":"1599746460.0","content":"B is correct","comment_id":"177133","upvote_count":"1","poster":"gkdinesh"},{"comment_id":"117054","upvote_count":"2","content":"B for sure","timestamp":"1592889780.0","poster":"mlantonis"},{"poster":"gfhbox0083","timestamp":"1591767780.0","comment_id":"106503","content":"B, for sure.\nJSON","upvote_count":"2"},{"timestamp":"1591608300.0","comment_id":"105134","content":"B is answer","upvote_count":"2","poster":"woorkim"},{"upvote_count":"2","poster":"Nirms","comment_id":"100852","content":"B is the correct answer","timestamp":"1591104480.0"},{"upvote_count":"2","comment_id":"98316","content":"B is the answer indeed","poster":"Ziegler","timestamp":"1590765660.0"},{"timestamp":"1590736980.0","comment_id":"98086","poster":"AD2AD4","upvote_count":"2","content":"Final Decision to go with Option B"},{"upvote_count":"2","comment_id":"84709","content":"B is the correct answer","timestamp":"1588795020.0","poster":"gcp_aws"},{"poster":"2g","timestamp":"1580389980.0","upvote_count":"2","comment_id":"44709","content":"answer: B"}],"answer_description":"","unix_timestamp":1571932920,"question_text":"You are creating a solution to remove backup files older than 90 days from your backup Cloud Storage bucket. You want to optimize ongoing Cloud Storage spend.\nWhat should you do?","exam_id":4,"timestamp":"2019-10-24 18:02:00","answer_ET":"B","question_id":79,"isMC":true,"choices":{"A":"Write a lifecycle management rule in XML and push it to the bucket with gsutil","D":"Schedule a cron script using gsutil ls ג€\"l gs://backups/** to find and remove items older than 90 days and schedule it with cron","B":"Write a lifecycle management rule in JSON and push it to the bucket with gsutil","C":"Schedule a cron script using gsutil ls ג€\"lr gs://backups/** to find and remove items older than 90 days"},"answer_images":[]},{"id":"CKdvVDTDvINJVsopoFkl","question_text":"One of your primary business objectives is being able to trust the data stored in your application. You want to log all changes to the application data.\nHow can you design your logging system to verify authenticity of your logs?","exam_id":4,"choices":{"B":"Use a SQL database and limit who can modify the log table","C":"Digitally sign each timestamp and log entry and store the signature","A":"Write the log concurrently in the cloud and on premises","D":"Create a JSON dump of each log entry and store it in Google Cloud Storage"},"timestamp":"2019-10-23 18:53:00","unix_timestamp":1571849580,"answers_community":["C (100%)"],"discussion":[{"comment_id":"294539","timestamp":"1629391680.0","upvote_count":"31","content":"Correct answer is C (verified from Question Bank in Whizlabs.com) \n\nFeedback\nC (Correct answer) - Digitally sign each timestamp and log entry and store the signature.\nAnswer A, B, and D don’t have any added value to verify the authenticity of your logs. Besides, Logs are mostly suitable for exporting to Cloud storage, BigQuery, and PubSub. SQL database is not the best way to be exported to nor store log data.\nSimplified Explanation\nTo verify the authenticity of your logs if they are tampered with or forged, you can use a certain algorithm to generate digest by hashing each timestamp or log entry and then digitally sign the digest with a private key to generate a signature. Anybody with your public key can verify that signature to confirm that it was made with your private key and they can tell if the timestamp or log entry was modified. You can put the signature files into a folder separate from the log files. This separation enables you to enforce granular security policies.","poster":"get2dd"},{"poster":"JoeShmoe","comment_id":"21538","upvote_count":"24","content":"C is correct and common practice","timestamp":"1589459880.0"},{"content":"Selected Answer: C\nDigitally sign each timestamp and log entry and store the signature","upvote_count":"1","timestamp":"1736002620.0","poster":"plumbig11","comment_id":"1336412"},{"upvote_count":"1","content":"Selected Answer: C\nDigitally sign each timestamp and log entry and store the signature","poster":"666Amitava666","timestamp":"1729890240.0","comment_id":"1202252"},{"timestamp":"1687761360.0","poster":"omermahgoub","comments":[{"poster":"omermahgoub","content":"Writing the log concurrently in the cloud and on premises, would not necessarily help to verify the authenticity of the logs, so A is not an option\n\nB, using a SQL database and limiting who can modify the log table, could help to prevent unauthorized modification of the logs, but it would not necessarily provide a way to verify the authenticity of the logs if they are modified by an authorized user.\n\nOption D, creating a JSON dump of each log entry and storing it in Google Cloud Storage, would not necessarily help to verify the authenticity of the logs.","upvote_count":"1","comment_id":"757258","timestamp":"1687761420.0"}],"content":"I would recommend option C, digitally signing each timestamp and log entry and storing the signature. Digitally signing a log entry involves creating a cryptographic hash of the log entry and a timestamp, and then encrypting the hash using a private key. The encrypted hash, known as the signature, can be stored along with the log entry in a secure manner. To verify the authenticity of the log entry, you can use the public key associated with the private key used to create the signature to decrypt the signature and recreate the hash. If the recreated hash matches the original hash, it indicates that the log entry has not been tampered with and is authentic.","upvote_count":"4","comment_id":"757256"},{"upvote_count":"2","poster":"AzureDP900","content":"Digitally signing is correct. C is right option!","comment_id":"695613","timestamp":"1681584480.0"},{"poster":"GMats","comment_id":"513906","timestamp":"1656561840.0","content":"C is correct.You can use deterministic algorithm to validate hash values.","upvote_count":"1"},{"comment_id":"502010","poster":"vincy2202","content":"Selected Answer: C\nC is the correct answer","upvote_count":"1","timestamp":"1655277720.0"},{"timestamp":"1653610080.0","poster":"joe2211","upvote_count":"2","comment_id":"487761","content":"Selected Answer: C\nvote C"},{"poster":"MaxNRG","comments":[{"poster":"Neo_ACE","comment_id":"467237","upvote_count":"2","content":"If you attended recently, Please update some new questions too. It would be great help","timestamp":"1650859860.0"},{"upvote_count":"1","timestamp":"1658159340.0","comment_id":"526877","content":"@MaxNRG, very clearly articulated elimination technique. BTW are these questions appearing in actual exam?","poster":"Wonka"}],"content":"C – Digitally sign each timestamp and log entry and store the signature.\nThis is fun Q where all options are technically correct. But, the point is to find most efficient. Since, Q asks about verification of log entry - then you don't need to dub it. Using of much shorter timestamp-hash pair will address the request. So, when reading log from original source, you also read hash for this timestamp and then verify the entry's body. \nBTW, this is one of general purpose questions, which is not directly related to GCP. Just checks your attentiveness\nA - is about duplication, can work, but redundant;\nB / D - both have similar design, but don’t allow verification of entry. No cross-checking of entry. E.g. person having access to log can change it in one place.\nC - storing log in one place, and hash-code in another. So, even if \"trusted\" person has modified original log, then it will break correspondence with hash code in other storage. That storage should be available only for authentication program (via service account).","comment_id":"466546","upvote_count":"5","timestamp":"1650715920.0"},{"poster":"aviratna","upvote_count":"1","content":"C is correct","comment_id":"390912","timestamp":"1640499360.0"},{"comment_id":"366496","timestamp":"1637857140.0","content":"C seems like the right answer","upvote_count":"1","poster":"Amrit00009"},{"poster":"victory108","comment_id":"360110","upvote_count":"1","content":"C. Digitally sign each timestamp and log entry and store the signature","timestamp":"1637221920.0"},{"comment_id":"359917","timestamp":"1637203020.0","content":"C (Correct answer) - Digitally sign each timestamp and log entry and store the signature.\n\nOther options are possible to export logs but won't be able to verify authenticity of logs","poster":"Amber25","upvote_count":"2"},{"upvote_count":"1","content":"C is correct","comment_id":"351195","poster":"un","timestamp":"1636223220.0"},{"timestamp":"1633641480.0","upvote_count":"1","content":"I'm on the fence between C and D. C is a good practice but D can do the job as well as versioned objects might be able to do job at some level... Now, C tells that only the signature would be stored which is obviously not enough, but the owner of versioned objects might be tampered too... IDK","poster":"mrhege","comment_id":"330711"},{"comment_id":"324020","upvote_count":"1","timestamp":"1632982260.0","poster":"lynx256","content":"IMO - C is ok"},{"comment_id":"323952","timestamp":"1632975780.0","poster":"Ausias18","upvote_count":"1","content":"Answers is C"},{"content":"I think this question should require 2 answers. In that case, C + D is correct. For only one choice, C is better than D.","comment_id":"274146","poster":"bnlcnd","timestamp":"1626998700.0","upvote_count":"2"},{"content":"No other answers other than C can give you the \"authenticity\".\nWhy not C?","timestamp":"1626998160.0","upvote_count":"1","poster":"bnlcnd","comment_id":"274138"},{"poster":"egordoe","content":"Clear C. \n\nA and D add redundancy and do not help with authenticity. I have no chance to verify authenticity even if 2 copies of the entry are equal.\n \nB simply makes the authenticity problem is less likely. But the problem is still there.\n\nC. Signing entries using some stable and secure application identity completely solves the integrity and authenticity problem. This approach is a common practice.","timestamp":"1626854520.0","comment_id":"272719","upvote_count":"2"},{"comment_id":"269583","upvote_count":"2","timestamp":"1626527220.0","content":"I think this question is about the immutability of storing objects in Google Cloud Storage. Option C is a good choice since signing the time stamp and log entries digitally will work, but since GCS objects are immutable, by storing log entries as GCS objects, you can guarantee the authenticity.","poster":"HKim"},{"content":"JSON Web Tokens (JWT) are commonly used for authenticating when making API calls.\nWhen users log into services, they can receive a JWT, which they then pass to subsequent API calls. The JWT contains claims about what the holder of the token is allowed to do. JWTs are digitally signed and can be encrypted. A JWT is a JSON structure with three parts.\n■■ Header\n■■ Payload\n■■ Signature\n\nHeaders contain a type attribute indicating that the token is a JWT type of token and the name of the algorithm used to sign the token.\n\nThe payload is a set of claims. Claims make statements about the issuer, subject, or token. They may include commonly used claims such as an expiration time or the name of the subject. They may also include private claims that are known to the parties that agree to use them. These might include application-specific claims, such as a permission to query\na specific type of data.\n\nThe signature is the output of the signature algorithm generated using the header, the payload, and a secret.\nThe JWT is encoded in three Base64-encoded strings.\nI will go with D","timestamp":"1621334760.0","upvote_count":"2","comment_id":"221870","poster":"Hjameel"},{"comment_id":"214674","poster":"ybe_gcp_cert","timestamp":"1620390840.0","content":"C should be the correct answer, because of the key word : Digital sign. the higher risk you could have with this is someone erasing the log entry but definitely can't modify it..\n\nD is also a good practice, considering that GCS objects are immutable, but, this doesn't avoid someone replacing the dump with another one (no IAM specification in this answer...)\n\nC is the right answer.","upvote_count":"1"},{"timestamp":"1620011460.0","content":"The question is implying that you need to build a audit log for all application data changes. Option 1 would be expensive.Why to do it both places? option 2- I feel this is a good option. Assumption is your existing database gets a new table added to log audit history. But this assumption not true if you need to maintain a separate database for audit.if this assumption is not valid, I would go with this option. Why to log it in cloud?\nOption 3 - Just signing the timestamp won't help. You need to sign the complete log and store the hash. option 4- This is good option but assumption is your application is already in cloud. Why would you log your audit data in CLOUD with increased latency. Option 2 is much better choice. B & D comes very close.","comments":[{"poster":"bnlcnd","upvote_count":"3","timestamp":"1626998580.0","content":"C is saying - Digitally sign each timestamp and log entry. So, it is exactly as you suggested.","comment_id":"274144"}],"comment_id":"211711","upvote_count":"2","poster":"Pokchok"},{"content":"C is not correct as you can not digitally sign Time stamp. Digitally signing log is posible but not timestamp :-)","comment_id":"196446","poster":"Chandrachud","upvote_count":"2","timestamp":"1617939600.0","comments":[{"timestamp":"1619311320.0","content":"https://stackoverflow.com/questions/3829546/how-can-i-digitally-sign-logs-to-ensure-that-they-have-not-been-modified","comment_id":"205415","poster":"myawscert","upvote_count":"1"}]},{"poster":"whitley030390","content":"I agree with C","comment_id":"189097","timestamp":"1616940480.0","upvote_count":"1"},{"comment_id":"188036","poster":"AshokC","content":"C is right","timestamp":"1616812260.0","upvote_count":"1"},{"timestamp":"1615832040.0","content":"strightforward C...","comment_id":"179947","upvote_count":"1","poster":"imranrq"},{"poster":"passtest100","content":"authenticity means it is not tempered even by someone who has authorization to delete, recreate it. so the retention policy can not guarantee the log entry authenticity. only signature can do it. so it goes to C","upvote_count":"2","comment_id":"175860","timestamp":"1615212840.0"},{"upvote_count":"1","comment_id":"174028","poster":"gkdinesh","content":"Agree with D","timestamp":"1614963780.0"},{"poster":"peterbrv","timestamp":"1614041760.0","content":"D is correct","upvote_count":"1","comment_id":"163980"},{"comment_id":"156243","timestamp":"1613123520.0","comments":[{"content":"What if a eavesdropper change your data before it is persists onto the disk. The only way to stop that is to sign it before it leaves the ram.","comment_id":"274142","timestamp":"1626998460.0","upvote_count":"1","poster":"bnlcnd"}],"poster":"jespinosar","upvote_count":"2","content":"I think D is the right answer. \nEven without considering additional features not mentioned in the question (versioning, retention policy) a GCS object is immutable by definition. If a given log is deleted or recreated (with a new creation date) you are aware someone tampered with the log."},{"upvote_count":"4","content":"I would go with D because GCS supports object versioning, which combined with IAM control over who can delete files prevents alteration or removal of old logs, as well retention policies applied at a bucket level that prevent deletion of any object in the bucket:\nhttps://cloud.google.com/storage/docs/bucket-lock#retention-policy\n\" Placing a retention policy on a bucket ensures that all current and future objects in the bucket cannot be deleted or overwritten until they reach the age you define in the retention policy. \"\nWhat I don't like about this question is that none of those features are explicitly mentioned as implemented in answer D. Are we left to assume all of these controls are in place?","comment_id":"150639","poster":"ewredtrfygi","timestamp":"1612468560.0"},{"comment_id":"142854","upvote_count":"2","content":"I will go with D, since C has no option to store the log file.","timestamp":"1611509820.0","poster":"Kunalkmehta08"},{"content":"The question asks ‘How you verify Authenticity?’. Any answer that does not include encryption of some kind will not allow you authenticate the entry. The only answer that support authentication is C.","poster":"NeilNalto","upvote_count":"4","timestamp":"1611143940.0","comment_id":"139339"},{"comment_id":"116624","upvote_count":"3","poster":"mlantonis","content":"I cannot choose between C or D.","timestamp":"1608663720.0"},{"content":"Answer is C; refer to https://cloud.google.com/kms/docs/audit-logging and https://cloud.google.com/kms/docs/reference/rest/v1/projects.locations.keyRings.cryptoKeys.cryptoKeyVersions/asymmetricSign","upvote_count":"1","comment_id":"114501","poster":"syu31svc","timestamp":"1608452400.0"},{"content":"I like D better","comment_id":"109242","timestamp":"1607850480.0","upvote_count":"2","poster":"Musk"},{"poster":"Tushant","content":"C is the correct answer","timestamp":"1606980540.0","comment_id":"101430","upvote_count":"1"},{"comment_id":"100780","poster":"Nirms","timestamp":"1606918440.0","content":"C is the correct answer","upvote_count":"1"},{"upvote_count":"2","timestamp":"1606842720.0","comment_id":"100103","poster":"Ziegler","content":"D is the correct answer\nexample from https://cloud.google.com/logging/docs/reference/tools/gcloud-logging\nWrite a log entry with a structured (JSON) payload:\n\ngcloud logging write my-test-log '{ \"message\": \"My second entry\", \"weather\": \"partly cloudy\"}' --payload-type=json"},{"poster":"AD2AD4","timestamp":"1606547340.0","comment_id":"97262","content":"Final Decision to go with Option D","upvote_count":"4"},{"upvote_count":"3","timestamp":"1605343320.0","comment_id":"88784","poster":"ankit89","content":"C is the best choice."},{"upvote_count":"5","content":"I will go with C as it's the only answer that provides for data integrity at the entry level.","comment_id":"86212","poster":"clouddude","timestamp":"1604961120.0"},{"poster":"gcp_aws","content":"C is the correct answer. Keyword: Authenyicity","timestamp":"1604441940.0","upvote_count":"3","comment_id":"83237"},{"timestamp":"1598646780.0","poster":"Jeysolomon","comment_id":"56646","comments":[{"content":"That's what you are saying is about securing and auditing the logs, not proving the authenticity of the same. I think C is right because its application log and the option is the only which talks about authenticity.","upvote_count":"1","timestamp":"1625558100.0","poster":"Arimaverick","comment_id":"260899"},{"content":"And you can also sign the data in cloud storage by using keys.\nAlso you can control who can write into it.","poster":"misho","timestamp":"1606742940.0","comment_id":"99253","upvote_count":"3"}],"upvote_count":"10","content":"D is the right answer. Write to google cloud storage and enable object versioning which will help keep the original version and also can identify if any log entry is modified by anyone."},{"comment_id":"47861","upvote_count":"5","content":"Question is about - Log File Validation (Data Integrity); hence C is the correct answer.","timestamp":"1596848400.0","poster":"Smart"},{"poster":"2g","content":"answer: C","upvote_count":"3","timestamp":"1596106200.0","comment_id":"44678"},{"upvote_count":"3","comment_id":"42781","content":"C is fine","poster":"natpilot","timestamp":"1595738700.0"},{"upvote_count":"2","poster":"aviv","timestamp":"1592118540.0","content":"Agreed with option C as digitally signing the log is the only way to verify authenticity of the logs.","comment_id":"29505"},{"timestamp":"1589891280.0","comment_id":"22714","upvote_count":"10","content":"I agree with C","poster":"AWS56"},{"comment_id":"16996","comments":[{"timestamp":"1612522260.0","content":"C is ok","comments":[{"upvote_count":"1","timestamp":"1614097380.0","poster":"peterbrv","content":"no. the answer is D","comment_id":"164456"}],"comment_id":"151023","upvote_count":"7","poster":"tartar"}],"timestamp":"1587660780.0","poster":"Eroc","content":"Option A would be the most feasible, assuming you already have access to the internet and a storage option. SQL would work, but if you use one of Google's SQL services (BigQuery or Cloud SQL), it would increase pricing. Storing the signature is not feasible, unless the signature is a cache predictor (A small sequence that is run against a larger sequence to see how the smaller sequence changes). However, option \"C\" does not log the specific change, it would just show you that a change occured within the domain that was checked. A JSON dump is a popular choice, and Google loves JSON so JSON seems to be the choice.","upvote_count":"3"}],"answer_ET":"C","question_images":[],"answer_description":"","topic":"1","isMC":true,"question_id":80,"answer":"C","url":"https://www.examtopics.com/discussions/google/view/7082-exam-professional-cloud-architect-topic-1-question-170/","answer_images":[]}],"exam":{"id":4,"provider":"Google","isMCOnly":false,"lastUpdated":"11 Apr 2025","numberOfQuestions":279,"isImplemented":true,"isBeta":false,"name":"Professional Cloud Architect"},"currentPage":16},"__N_SSP":true}