{"pageProps":{"questions":[{"id":"kESOImlXsBKoQnsIIGkn","question_images":[],"question_id":286,"isMC":true,"answer_ET":"C","answers_community":["C (57%)","A (43%)"],"timestamp":"2022-12-11 18:03:00","answer_description":"","discussion":[{"poster":"5091a99","content":"Selected Answer: A\nThis is a bad question. But imho, Answer: A. \n- TFRecords will improve read speeds with its binary format. Presumably the large file was there for a reason, possibly the output of a upstream process whose data may change in the future. TFRecords is a straightforward FIRST step as a part of a pipeline.\n- The other option is parallel interleave. Also improves read speeds, but not as straightforward as a first step and requires lots of file in the database that require version control.","upvote_count":"1","timestamp":"1741314240.0","comment_id":"1366117"},{"content":"A. Preprocessing your data into TFRecord format can significantly improve I/O performance and reduce the time spent on parsing and loading data, which is critical for optimizing the input pipeline for large-scale datasets.","timestamp":"1740363840.0","comment_id":"1360829","poster":"NamitSehgal","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: A\naccording to the official doc A, C seems to pre TFX solution","poster":"bc3f222","comment_id":"1360393","timestamp":"1740281280.0"},{"timestamp":"1734637500.0","upvote_count":"2","content":"Selected Answer: A\nBased on the official documentation, Option A (converting to TFRecord format) is actually the correct first action to try, and the claim is incorrect.\n\nWhy TFRecord is the Best First Option\n\nTFRecord format is specifically recommended for large datasets because:\n\n- It provides extremely high throughput when reading from Cloud Storage, especially for large-scale training[2]\n- It's the recommended format for structured data and large files[2]\n- It's designed for efficient serialization of structured data and optimal performance with TensorFlow","comment_id":"1329134","poster":"phani49"},{"upvote_count":"1","content":"Selected Answer: C\nc is the right answer","timestamp":"1732637820.0","comment_id":"1318166","poster":"AB_C"},{"upvote_count":"3","content":"Selected Answer: A\nPreprocessing the input CSV file into a TFRecord file optimizes the input data pipeline by enabling more efficient reading and processing. TFRecord is a binary format that is faster to read and more efficient for TensorFlow to process compared to CSV, which is a text-based format. This change can significantly reduce the time spent on data input operations during model training.","comment_id":"1240501","timestamp":"1719889200.0","poster":"Prakzz"},{"comment_id":"1226036","content":"Selected Answer: A\nA) Convert CSV file into TFRecord is more effecient and processing CSV in parallel (C)","upvote_count":"1","poster":"PhilipKoku","timestamp":"1717754400.0"},{"comment_id":"1199035","content":"Selected Answer: C\nConverting a large 5 terabyte CSV file to a TFRecord can be a time-consuming process, and you would still be dealing with a single large file.","poster":"pinimichele01","timestamp":"1713604440.0","upvote_count":"4"},{"timestamp":"1699380840.0","comment_id":"1065067","upvote_count":"1","content":"Selected Answer: C\nWhile preprocessing the input CSV file into a TFRecord file (Option A) can improve the performance of your input pipeline, it is not the first action to try in this situation. Converting a large 5 terabyte CSV file to a TFRecord can be a time-consuming process, and you would still be dealing with a single large file.","poster":"tavva_prudhvi"},{"content":"Selected Answer: C\ni think C based on the consideration: \"Which action should you try first \", meaning it should be less impactful to continue using CSV.","upvote_count":"1","poster":"andresvelasco","comment_id":"1003819","timestamp":"1694334420.0"},{"content":"Selected Answer: C\nhttps://www.tensorflow.org/guide/data_performance#best_practice_summary","poster":"TNT87","timestamp":"1685879100.0","comment_id":"914515","upvote_count":"2"},{"upvote_count":"1","timestamp":"1683609540.0","poster":"M25","comment_id":"892770","content":"Selected Answer: C\nWent with C"},{"timestamp":"1682583180.0","upvote_count":"1","poster":"e707","content":"Selected Answer: C\nOption A, preprocess the input CSV file into a TFRecord file, is not as good because it requires additional processing time. Hence, I think C is the best choice.","comment_id":"882376"},{"timestamp":"1682405220.0","upvote_count":"1","comment_id":"880050","content":"Selected Answer: A\nI think it could be A.\nhttps://cloud.google.com/architecture/best-practices-for-ml-performance-cost#preprocess_the_data_once_and_save_it_as_a_tfrecord_file","poster":"frangm23"},{"poster":"[Removed]","upvote_count":"1","content":"Selected Answer: A\nClearly both A and C works here, but I can't find any documentation which suggests C is any better than A.","comment_id":"875540","timestamp":"1681991400.0"},{"content":"\"Which action should you try first\" seems to be key -- C seems more intuitive as first step!\nA is valid as well (interleave works w TFRecords) & definitely more efficient IMO, but maybe 2nd step!","upvote_count":"2","comment_id":"841829","poster":"Yajnas_arpohc","timestamp":"1679044140.0"},{"timestamp":"1677136500.0","comments":[{"comment_id":"847417","poster":"tavva_prudhvi","upvote_count":"1","content":"Please read this site https://www.tensorflow.org/tutorials/load_data/csv, its simple to implement in the same input pipeline, and we cannot judge the answer by implementation difficulties!","timestamp":"1679511240.0"}],"comment_id":"818905","poster":"shankalman717","content":"Selected Answer: A\nOption B (randomly selecting a 10 gigabyte subset of the data) could lead to a loss of useful data and may not be representative of the entire dataset. Option C (splitting into multiple CSV files and using a parallel interleave transformation) may also improve the performance, but may be more complex to implement and maintain, and may not be as efficient as converting to TFRecord. Option D (setting the reshuffle_each_iteration parameter to true in the tf.data.Dataset.shuffle method) is not directly related to the input data format and may not provide as significant a performance improvement as converting to TFRecord.","upvote_count":"3"},{"poster":"SMASL","upvote_count":"4","comments":[{"poster":"tavva_prudhvi","upvote_count":"1","timestamp":"1679511060.0","comment_id":"847409","comments":[{"content":"yes but how is it more efficient than converting to a TFRecord file?","upvote_count":"1","timestamp":"1681991460.0","poster":"[Removed]","comments":[{"poster":"tavva_prudhvi","timestamp":"1690119180.0","content":"A TFRecord file is a binary file format that is used to store TensorFlow data. It is more efficient than a CSV file because it can be read more quickly and it takes up less space. However, it is still a large file, and it would take a long time to read it into memory. Splitting the file into multiple smaller files would reduce the amount of time it takes to read the files into memory, and it would also make it easier to parallelize the reading process.","upvote_count":"1","comment_id":"960475"}],"comment_id":"875542"}],"content":"Option C, splitting into multiple CSV files and using a parallel interleave transformation, could improve the pipeline efficiency by allowing multiple workers to read the data in parallel."}],"comment_id":"808723","content":"Could anyone be kind to explain why C is preferred over A? My initial guess was on A, but everyone here seems to unanimously prefer C. Is it because it is not about optimizing I/O performance, but rather the input _pipeline_, which is about processing arrived data within that TF input pipeline (non-I/O)? I just try to understand here. Thanks for reply in advance!","timestamp":"1676401740.0"},{"poster":"enghabeth","upvote_count":"1","comment_id":"802789","timestamp":"1675912920.0","content":"Selected Answer: C\nsplit data it's best way in my opinion"},{"poster":"hiromi","comment_id":"748857","upvote_count":"2","content":"Selected Answer: C\nC\nKeywords -> You need to optimize the input pipeline performance \nhttps://www.tensorflow.org/guide/data_performance","timestamp":"1671368280.0","comments":[{"content":"- https://www.tensorflow.org/tutorials/load_data/csv","poster":"hiromi","timestamp":"1671829740.0","upvote_count":"1","comment_id":"754544"}]},{"content":"Selected Answer: C\nIt seems C, to me.","poster":"ares81","upvote_count":"1","timestamp":"1671012840.0","comment_id":"744959"},{"upvote_count":"2","comment_id":"741913","poster":"LearnSodas","timestamp":"1670778180.0","content":"Selected Answer: C\nSplitting the file we can use parallel interleave to parallel load the datasets\nhttps://www.tensorflow.org/guide/data_performance"}],"topic":"1","question_text":"You are profiling the performance of your TensorFlow model training time and notice a performance issue caused by inefficiencies in the input data pipeline for a single 5 terabyte CSV file dataset on Cloud Storage. You need to optimize the input pipeline performance. Which action should you try first to increase the efficiency of your pipeline?","exam_id":13,"answer":"C","url":"https://www.examtopics.com/discussions/google/view/91041-exam-professional-machine-learning-engineer-topic-1-question/","unix_timestamp":1670778180,"choices":{"C":"Split into multiple CSV files and use a parallel interleave transformation.","A":"Preprocess the input CSV file into a TFRecord file.","D":"Set the reshuffle_each_iteration parameter to true in the tf.data.Dataset.shuffle method.","B":"Randomly select a 10 gigabyte subset of the data to train your model."},"answer_images":[]},{"id":"B0I15atUAlka8QpIzCpH","answers_community":["B (74%)","C (16%)","11%"],"answer":"B","topic":"1","question_images":[],"exam_id":13,"answer_description":"","timestamp":"2022-12-11 18:57:00","unix_timestamp":1670781420,"choices":{"A":"1. HTTP requests are sent by the sensors to your ML model, which is deployed as a microservice and exposes a REST API for prediction\n2. Your application queries a Vertex AI endpoint where you deployed your model.\n3. Responses are received by the caller application as soon as the model produces the prediction.","D":"1. Export the data to Cloud Storage using the BigQuery command-line tool\n2. Submit a Vertex AI batch prediction job that uses your trained model in Cloud Storage to perform scoring on the preprocessed data.\n3. Export the batch prediction job outputs from Cloud Storage and import them into BigQuery.","C":"1. Export your data to Cloud Storage using Dataflow.\n2. Submit a Vertex AI batch prediction job that uses your trained model in Cloud Storage to perform scoring on the preprocessed data.\n3. Export the batch prediction job outputs from Cloud Storage and import them into Cloud SQL.","B":"1. Events are sent by the sensors to Pub/Sub, consumed in real time, and processed by a Dataflow stream processing pipeline.\n2. The pipeline invokes the model for prediction and sends the predictions to another Pub/Sub topic.\n3. Pub/Sub messages containing predictions are then consumed by a downstream system for monitoring."},"question_id":287,"discussion":[{"content":"Selected Answer: B\nB) Pub/Sub & DataFlow","timestamp":"1733573520.0","comment_id":"1226047","upvote_count":"1","poster":"PhilipKoku"},{"timestamp":"1729777440.0","upvote_count":"1","poster":"inc_dev_ml_001","content":"Selected Answer: C\nThe simplest solution that can support an eventual batch prediction (triggered by pub/sub) even the semi-real time prediction.","comment_id":"1201414"},{"poster":"Werner123","upvote_count":"1","content":"Selected Answer: B\nNeeds to be real time not batch. The data needs to be processed as a stream since multiple sensors are used. pawan94 is right. https://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning#online_real-time_prediction","comment_id":"1162304","timestamp":"1724910300.0"},{"timestamp":"1720544760.0","poster":"pawan94","comment_id":"1117768","content":"Here you go to the answer provided by google itself. I don't understand why would people use batch prediction when they its sensor data and online prediction is as well asynchronous. \nhttps://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning#offline_batch_prediction:~:text=Predictive%20maintenance%3A%20asynchronously%20predicting%20whether%20a%20particular%20machine%20part%20will%20fail%20in%20the%20next%20N%20minutes%2C%20given%20the%20averages%20of%20the%20sensor%27s%20data%20in%20the%20past%2030%20minutes.","upvote_count":"2"},{"comment_id":"1100766","poster":"vale_76_na_xxx","timestamp":"1718804640.0","content":"it refers to asincronou prediction I' go with C","upvote_count":"1"},{"content":"Selected Answer: D\nD.\nI think we have to query data from the past 12 hours for the prediction, and that's the reason for exporting the data to Cloud Storage.\nAlso, the predictions don't have to be real time.","poster":"rosenr0","timestamp":"1701168000.0","comment_id":"908443","upvote_count":"2"},{"timestamp":"1699514340.0","comment_id":"892771","upvote_count":"1","content":"Selected Answer: B\nWent with B","poster":"M25"},{"timestamp":"1695896700.0","poster":"JamesDoe","upvote_count":"2","comment_id":"853101","content":"Selected Answer: B\nB.\nOnline prediction, and need decoupling with Pub/Sub to make it asynchronous. Option A is synchronous."},{"comment_id":"847422","content":"Option C may not be the best choice for this use case because it involves using a batch prediction job in Vertex AI to perform scoring on preprocessed data. Batch prediction jobs are more suitable for scenarios where data is processed in batches, and results can be generated over a longer period, such as daily or weekly.\n\nIn this use case, the requirement is to predict whether a machine part will fail in the next N minutes, given the average of each sensor's data from the past 12 hours. Therefore, real-time processing and prediction are necessary. Batch prediction jobs are not designed for real-time processing, and there may be a delay in receiving the predictions.\n\nOption B, on the other hand, is designed for real-time processing and prediction. The Pub/Sub and Dataflow components allow for real-time processing of incoming sensor data, and the trained ML model can be invoked for prediction in real-time. This makes it ideal for mission-critical applications where timely predictions are essential.","poster":"tavva_prudhvi","upvote_count":"2","timestamp":"1695402000.0"},{"comment_id":"847419","timestamp":"1695401760.0","upvote_count":"1","content":"Its B, This architecture leverages the strengths of Pub/Sub, Dataflow, and Vertex AI. The system collects data from multiple sensors, which sends events to Pub/Sub. Pub/Sub can handle the high volume of incoming data and can buffer messages to prevent data loss. A Dataflow stream processing pipeline can consume the events in real-time and perform feature engineering and data preprocessing before invoking the trained ML model for prediction. The predictions are then sent to another Pub/Sub topic, where they can be consumed by a downstream system for monitoring.\n\nThis architecture is highly scalable, resilient, and efficient, as it can handle large volumes of data and perform real-time processing and prediction. It also separates concerns by using a separate pipeline for data processing and another for prediction, making it easier to maintain and modify the system.","poster":"tavva_prudhvi"},{"comment_id":"802794","content":"Selected Answer: B\nif you have sensors inyour architecture.. you need pub/sub...","timestamp":"1691544480.0","upvote_count":"1","poster":"enghabeth"},{"content":"Selected Answer: B\nB is most likely . if you search asynchronous on this page. it appears in \nthe question wants to focus on online prediction with asynchronous mode.\nhttps://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning#online_real-time_prediction \nand the question is the same as what has been explained in this section obviously. it is as below.\nPredictive maintenance: asynchronously predicting whether a particular machine part will fail in the next N minutes, given the averages of the sensor's data in the past 30 minutes.\n\nafte that, you can take a closer look at figure3 and read what it try to describle\n\nC and D it is the offline solution but you opt to use different tools.\nhttps://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning#offline_batch_prediction","comment_id":"789398","timestamp":"1690436460.0","upvote_count":"2","poster":"John_Pongthorn"},{"comment_id":"789382","upvote_count":"1","poster":"John_Pongthorn","content":"Asycnchromoue preciction = Batch prediction\nhttps://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning#offline_batch_prediction","comments":[{"upvote_count":"1","content":"Asynchronous prediction = Batch prediction, It is incorrect because I am reckless to read this article, Admin can delete my shitty comment above. I was mistaken","poster":"John_Pongthorn","comment_id":"789389","timestamp":"1690435740.0"}],"timestamp":"1690434420.0"},{"content":"Selected Answer: B\nB\n\"Predictive maintenance: asynchronously predicting whether a particular machine part will fail in the next N minutes, given the averages of the sensor's data in the past 30 minutes.\"\nhttps://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning#offline_batch_prediction","poster":"hiromi","upvote_count":"3","comment_id":"748904","timestamp":"1687087440.0","comments":[{"upvote_count":"1","content":"- https://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning#online_real-time_prediction","comment_id":"754547","poster":"hiromi","timestamp":"1687547400.0"}]},{"poster":"mil_spyro","content":"Selected Answer: B\nAnswer is B. \nhttps://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning#handling_dynamic_real-time_features","comment_id":"748075","timestamp":"1687001520.0","upvote_count":"1"},{"comment_id":"744980","upvote_count":"1","content":"Selected Answer: C\nC, for me.","poster":"ares81","timestamp":"1686732480.0"},{"timestamp":"1686659100.0","comment_id":"744123","content":"Selected Answer: C\nref : https://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning#offline_batch_prediction","poster":"seifou","upvote_count":"1"},{"comment_id":"741953","content":"Selected Answer: B\nAnswer B\nI though a lot, since we don't need a real-time response in this scenario, but other options have this problems:\nA - Http request for sensors data is not a good idea\nC - What's the point of use Cloud Sql to store the results?\nD - No BQ mentioned, so why use bq SDK to move data?","upvote_count":"2","timestamp":"1686499020.0","poster":"LearnSodas"}],"answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/91045-exam-professional-machine-learning-engineer-topic-1-question/","question_text":"You need to design an architecture that serves asynchronous predictions to determine whether a particular mission-critical machine part will fail. Your system collects data from multiple sensors from the machine. You want to build a model that will predict a failure in the next N minutes, given the average of each sensor’s data from the past 12 hours. How should you design the architecture?","isMC":true,"answer_ET":"B"},{"id":"yNLAE4fsdkNu78qNxhgw","choices":{"A":"Create a collaborative filtering system that recommends articles to a user based on the user’s past behavior.","D":"Manually label a few hundred articles, and then train an SVM classifier based on the manually classified articles that categorizes additional articles into their respective categories.","B":"Encode all articles into vectors using word2vec, and build a model that returns articles based on vector similarity.","C":"Build a logistic regression model for each user that predicts whether an article should be recommended to a user."},"answer_description":"","unix_timestamp":1670783400,"question_images":[],"topic":"1","discussion":[{"comment_id":"1195517","timestamp":"1728911040.0","poster":"gscharly","content":"Selected Answer: B\nWent with B","upvote_count":"1"},{"timestamp":"1699514340.0","content":"Selected Answer: B\nWent with B","upvote_count":"1","comment_id":"892773","poster":"M25"},{"timestamp":"1697524380.0","comment_id":"872402","poster":"TNT87","upvote_count":"3","content":"Selected Answer: B\nhttps://cloud.google.com/blog/topics/developers-practitioners/meet-ais-multitool-vector-embeddings\nAnswer B"},{"content":"Selected Answer: B\nCurrently reading is the keyword here. Going to need B for that, A won't work since it would be based on e.g. all reading history and not the article currently being read.","comment_id":"853103","upvote_count":"3","timestamp":"1695896880.0","poster":"JamesDoe"},{"comment_id":"847424","content":"Option A, creating a collaborative filtering system, may not be ideal for this use case because it relies on user behavior data, which may not be available or sufficient for new users or for users who have not interacted with the system much.\n\nOption C, building a logistic regression model for each user, may not be scalable because it requires building a separate model for each user, which can become difficult to manage as the number of users increases.\n\nOption D, manually labeling articles and training an SVM classifier, may not be as effective as the word2vec approach because it relies on manual labeling, which can be time-consuming and may not capture the full semantic meaning of the articles. Additionally, SVMs may not be as effective as neural network-based approaches like word2vec for capturing complex relationships between words and articles.","upvote_count":"2","timestamp":"1695402180.0","poster":"tavva_prudhvi"},{"content":"Selected Answer: B\nword2vec can easily get similar articles, but the collaborative filter isn't sure well.","comment_id":"759972","poster":"JJJJim","upvote_count":"1","timestamp":"1687956960.0"},{"poster":"hiromi","upvote_count":"3","content":"Selected Answer: B\nB\nhttps://towardsdatascience.com/recommending-news-articles-based-on-already-read-articles-627695221fe8","comment_id":"748951","timestamp":"1687087740.0"},{"upvote_count":"1","comment_id":"748071","content":"Selected Answer: B\nAnswer B","poster":"mil_spyro","timestamp":"1687000980.0"},{"poster":"ares81","content":"Selected Answer: B\nCollaborative looks at the other users, knowledge-based at me.Answer B is the most knowledge based, among these.","upvote_count":"2","comment_id":"744984","timestamp":"1686732720.0"},{"upvote_count":"2","comment_id":"743555","poster":"YangG","content":"Selected Answer: A\n\"similar to they are currently reading\". it should be a collaborative filtering problem","comments":[{"upvote_count":"3","poster":"taxberg","timestamp":"1690977660.0","content":"No, Collaborative filtering recommends articles other people read that are not necessarily similar to what the person is reading. These people are chosen on being similar to the person in question, not the article.","comment_id":"796085"}],"timestamp":"1686619380.0"},{"poster":"LearnSodas","timestamp":"1686501000.0","comment_id":"741977","content":"Selected Answer: B\nAnswer B","upvote_count":"2"}],"isMC":true,"timestamp":"2022-12-11 19:30:00","exam_id":13,"answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/91053-exam-professional-machine-learning-engineer-topic-1-question/","answer":"B","answer_ET":"B","question_text":"Your company manages an application that aggregates news articles from many different online sources and sends them to users. You need to build a recommendation model that will suggest articles to readers that are similar to the articles they are currently reading. Which approach should you use?","question_id":288,"answers_community":["B (89%)","11%"]},{"id":"bxSRqLeHtajShAFfreIB","url":"https://www.examtopics.com/discussions/google/view/91122-exam-professional-machine-learning-engineer-topic-1-question/","topic":"1","answer_images":[],"answer_description":"","unix_timestamp":1670830320,"answers_community":["D (58%)","C (23%)","B (19%)"],"question_text":"You work for a large social network service provider whose users post articles and discuss news. Millions of comments are posted online each day, and more than 200 human moderators constantly review comments and flag those that are inappropriate. Your team is building an ML model to help human moderators check content on the platform. The model scores each comment and flags suspicious comments to be reviewed by a human. Which metric(s) should you use to monitor the model’s performance?","discussion":[{"comment_id":"749697","content":"Selected Answer: D\nD\n- https://cloud.google.com/natural-language/automl/docs/beginners-guide\n- https://cloud.google.com/vertex-ai/docs/text-data/classification/evaluate-model","upvote_count":"12","timestamp":"1671443880.0","poster":"hiromi"},{"timestamp":"1694335320.0","content":"Selected Answer: C\nA. Number of messages flagged by the model per minute => NO, no measure of model performance \nB. Number of messages flagged by the model per minute confirmed as being inappropriate by humans.=> DONT THINK SO, because we need the total number of messages (flagged?)\nC. Precision and recall estimates based on a random sample of 0.1% of raw messages each minute sent to a human for review. => I think YES, because as I understand it that would be based on a sample of ALL messages not just the ones that have been flagged.\nD. Precision and recall estimates based on a sample of messages flagged by the model as potentially inappropriate each minute => I think NO, because the sample includes only flagged messages, meaning positives, so you cannot really measure recall.","comment_id":"1003831","upvote_count":"7","poster":"andresvelasco","comments":[{"upvote_count":"4","poster":"tavva_prudhvi","timestamp":"1699381740.0","comments":[{"content":"But how can you calculate recall with just flagged samples? How could you get a view of false negatives? This is surely key to a problem like this where we don't want to let inappropriate posts go unflagged.","poster":"josiejojo","timestamp":"1739531760.0","upvote_count":"1","comment_id":"1356425"}],"content":"The main issue with option C is that it uses a random sample of only 0.1% of raw messages. This random sample might not contain enough examples of inappropriate content to accurately assess the model's performance. Since the majority of messages on the platform are likely appropriate, the random sample may not capture enough inappropriate content for a robust evaluation.","comment_id":"1065078"}]},{"upvote_count":"2","content":"Selected Answer: C\nC is correct: A random sample of raw messages provides an unbiased evaluation of the model's performance across all types of content\n\nOption D is problematic because:\nCreates a biased sample by only reviewing flagged messages\nCannot detect false negatives (missed inappropriate content)","timestamp":"1734638040.0","comment_id":"1329140","poster":"phani49"},{"comment_id":"1289520","content":"Selected Answer: B\nI went with B.\nRemember how to calculate Recall: TP/(TP+FN). Since \"sample of messaged flagged by the model\" are only P cases, you won't have your F cases reviewed by a human, therefore you won't have FN, therefore it's not D.\n\nI also believe that 0.1% of raw messages is going to have too little P cases, therefore not C.\n\nAnd then we remain with option B, which is not optimal, but it is the best we can do in this situation.","upvote_count":"1","poster":"amene","timestamp":"1727360160.0"},{"timestamp":"1725812520.0","comment_id":"1280474","content":"Selected Answer: C\nIt is absolutely not possible to calculate recall with D because we only have positives in the sample we need false negatives. Because of the high quantity of total data, 0.1% is fine, the answer is C","poster":"baimus","upvote_count":"1"},{"content":"Selected Answer: D\nPrecision and recall are critical metrics for evaluating the performance of classification models, especially in contexts where both the accuracy of positive predictions (precision) and the ability to identify all positive instances (recall) are important. In this case:\nPrecision (the proportion of messages flagged by the model as inappropriate that were actually inappropriate) helps ensure that the model minimizes the burden on human moderators by not flagging too many false positives, which could overwhelm them.\nRecall (the proportion of actual inappropriate messages that were correctly flagged by the model) ensures that the model is effective at catching as many inappropriate messages as possible, reducing the risk of harmful content being missed.","poster":"ludovikush","comment_id":"1182608","upvote_count":"4","timestamp":"1711381860.0"},{"content":"Selected Answer: C\nI go with C","upvote_count":"1","poster":"etienne0","comment_id":"1172697","timestamp":"1710345240.0"},{"timestamp":"1709217180.0","poster":"pmle_nintendo","upvote_count":"3","content":"Selected Answer: D\nLet's consider below hypothetical scenario:\n\nTotal number of comments per minute: 10,000\nComments actually inappropriate: 500\nIf we use a random sample of only 0.1% of raw messages (10 comments) for evaluation, there's a high chance that this small sample may not include any or only a few inappropriate comments. As a result, the precision and recall estimates based on this sample may be skewed, leading to unreliable assessments of the model's performance. Thus, C is ruled out.","comment_id":"1162694"},{"content":"Selected Answer: D\nC does not make sense to me since it is a very small random sample. It is also only messages that have been sent to humans for review meaning that there is bias in that result set.","timestamp":"1709194980.0","upvote_count":"2","comment_id":"1162332","poster":"Werner123"},{"content":"D only caring for observations flagged by the model means we don't control for false negatives (approved actually inappropriate messages). B seems like a better option to me: the wording confuses me a bit, but I understand it as the true and false positives (human flagged comments and their modelled label)","timestamp":"1704193380.0","poster":"b1a8fae","comment_id":"1111792","upvote_count":"1"},{"upvote_count":"2","comment_id":"1071570","content":"Selected Answer: D\nIn favor of D","timestamp":"1700061480.0","poster":"Mickey321"},{"upvote_count":"1","comment_id":"1070537","poster":"pico","timestamp":"1699974660.0","content":"Selected Answer: C\nGiven the context of content moderation, a balanced approach is often preferred. Therefore, option C, precision and recall estimates based on a random sample of raw messages, is a good choice. It provides a holistic view of the model's performance, taking into account both false positives (precision) and false negatives (recall), and it reflects how well the model is handling the entire dataset."},{"content":"Selected Answer: D\nA --> Conveys model'a activity levels but nit accuracy\nB --> Accuracy to some extend but wont give full picture as it does not account False negatives\nC --> Using a random sample of the raw messages allows you to estimate precision and recall for the overall activity, not just the flagged content.\nD --> Specifically measures on the subset of data that it flagged\n\nBoth C & D work well in this case, but the specificity is higher in option D and hence will go with D","comment_id":"1068029","timestamp":"1699728780.0","upvote_count":"2","poster":"Krish6488"},{"upvote_count":"1","content":"Selected Answer: C\nGoogle Cloud used to have a service called \"continuous evaluation\", where human labelers classify data to establish a ground truth. Thinking along those lines, the answer is C as it's the logical equivalent of that service.\n\nhttps://cloud.google.com/ai-platform/prediction/docs/continuous-evaluation","comment_id":"1057951","poster":"MultipleWorkerMirroredStrategy","timestamp":"1698678900.0"},{"poster":"PST21","timestamp":"1687938780.0","comment_id":"936299","content":"Question is to measure model performance so has to be precision & recall , hence D.","upvote_count":"2"},{"content":"Selected Answer: D\nD. Precision and recall estimates based on a sample of messages flagged by the model as potentially inappropriate each minute \nYou will need precision and recall to identify fals positives and false negatives. A very small random sample doesn't help specially becasue probably you will have skewed data. So D.","comment_id":"918124","timestamp":"1686220080.0","upvote_count":"2","poster":"Voyager2"},{"comment_id":"892774","poster":"M25","upvote_count":"2","timestamp":"1683609600.0","content":"Selected Answer: D\nWent with D"},{"timestamp":"1682497680.0","upvote_count":"1","content":"Selected Answer: D\nwe need to monitor the model, so D","comment_id":"881316","poster":"lucaluca1982"},{"content":"Why not C as we need to look at both precision & recall and both B,D miss that and capture only True/ False +ves? It would be very helpful if someone can explain.","comment_id":"877717","timestamp":"1682209500.0","upvote_count":"3","poster":"Sas02"},{"upvote_count":"1","content":"D\nI go for option D because B is just another way of looking at only precision and completely ignoring recall","comment_id":"876770","poster":"Vikraju","timestamp":"1682102820.0"},{"timestamp":"1679999700.0","comments":[{"upvote_count":"2","content":"- But I guess to implement D we have to fetch the \"human result / label\", so I guess that's the better option since it uses real world performance but better or more standard metrics?\nBut B or D at least, since we want to monitor real-world performance in real time.","comment_id":"853114","poster":"JamesDoe","timestamp":"1679999880.0"}],"content":"Selected Answer: B\nIMO this is as question about monitoring real-world performance, and not a question about measuring \"model training performance\". B is the only answer where we have real time real-world labels to base our comparison on, and hence is the only option.","comment_id":"853109","upvote_count":"1","poster":"JamesDoe"},{"comment_id":"844613","content":"Selected Answer: B\nBoth C & D talk about sample set.\nB is more absolute and datapoint there can calculate Precision/Recall as explained by guilhermebutzke below","poster":"Yajnas_arpohc","upvote_count":"1","timestamp":"1679296860.0"},{"upvote_count":"4","comment_id":"819513","comments":[{"timestamp":"1679838840.0","upvote_count":"1","poster":"hghdh5454","content":"You make a valid point. Answer B is indeed a more complete answer, as it specifically mentions using the number of messages flagged by the model and confirmed as inappropriate by humans to monitor the model's performance. This approach would provide a more accurate evaluation of the model's precision, which is the proportion of flagged messages that are actually inappropriate. Additionally, the recall metric, which is the proportion of inappropriate messages that are flagged by the model, could also be calculated using this approach. Therefore, answer B is the best choice for monitoring the model's performance.","comment_id":"851108","comments":[{"content":"This option indeed focuses on true positives (messages flagged by the model and confirmed as inappropriate). However, it does not provide explicit information about false positives (messages flagged by the model but not inappropriate) or false negatives (inappropriate messages not flagged by the model).\n\nWhile you can infer the number of true positives from option B, you cannot directly calculate precision and recall without additional information about false positives and false negatives.","timestamp":"1690217760.0","comment_id":"961836","poster":"tavva_prudhvi","upvote_count":"1"}]},{"content":"I see your point, but I think that it is implied that if they are calculating precision and recall, they must know which one are actually inappropriate or not. \n\nPlus, I don't have right now the links but it is always recommended to use metrics that are not absolute as they are not that meaningful as a percentage can be. \n\nThat's why I would answer D.","poster":"frangm23","timestamp":"1681403280.0","comment_id":"869607","upvote_count":"2"},{"poster":"tavva_prudhvi","timestamp":"1690217820.0","comment_id":"961840","content":"The issue with option B is that it only focuses on the number of messages flagged by the model per minute that are confirmed as being inappropriate by humans, which corresponds to true positives. However, it does not account for other important aspects of the model's performance, such as false positives and false negatives, which are crucial for calculating precision and recall.\n\nIn option B, we don't have information about:\n\nMessages flagged by the model but confirmed as not inappropriate by humans (false positives)\nMessages not flagged by the model but confirmed as inappropriate by humans (false negatives)\n\nWithout these pieces of information, we cannot calculate precision and recall to fully evaluate the model's performance.","upvote_count":"1"}],"poster":"guilhermebutzke","content":"Selected Answer: B\nI think B is a complete answer because the answer says \"to be inappropriate for humans\". So, having this background information, we have a human-checked sample of messages, and we can use that for the following:\n- message flagged and confirmed as inappropriate as true positive,\n- message flagged and confirmed as not inappropriate as false negative,\n- message not flagged and confirmed as inappropriate as false negative,\n\nAnd indirectly, we can use that to calculate metrics like precision and recall.\n\nSo I think if we have the information verified by humans, the number of flagged messages has enough information to say if the model is good or bad.\n\nAnswer D is vaguer to me because it doesn't bring up the use of \"confirmed to be inappropriate for humans\"","timestamp":"1677174780.0"},{"poster":"pshemol","timestamp":"1675085820.0","upvote_count":"3","comment_id":"792867","content":"Selected Answer: D\nFirst thought B, but number of messages confirmed isn't any metric. (it's not even a precision alone)\nIts classification problem so precision and recall metrics are the best to decide on performance.","comments":[{"comment_id":"795906","content":"It's confusing.. \" as potentially inappropriate each minute\" so it could be TP or FP only, then it's possible to calculate precision but recall can't be calculated from this subset :)","upvote_count":"2","comments":[{"comment_id":"795908","content":"then C is best now for me","upvote_count":"2","timestamp":"1675329300.0","poster":"pshemol"}],"timestamp":"1675329120.0","poster":"pshemol"}]},{"poster":"Abhijat","upvote_count":"3","timestamp":"1672325580.0","content":"Answer is D","comment_id":"761147"},{"timestamp":"1671668160.0","content":"Selected Answer: B\nI think B","upvote_count":"2","comment_id":"752893","poster":"MithunDesai"},{"timestamp":"1671015240.0","poster":"ares81","content":"Selected Answer: B\nB is the only way to go!","comment_id":"744986","upvote_count":"2"},{"upvote_count":"1","comment_id":"742510","content":"The only answer making sense is B.","poster":"ares81","timestamp":"1670830320.0"}],"timestamp":"2022-12-12 08:32:00","isMC":true,"exam_id":13,"question_images":[],"answer_ET":"D","question_id":289,"answer":"D","choices":{"B":"Number of messages flagged by the model per minute confirmed as being inappropriate by humans.","D":"Precision and recall estimates based on a sample of messages flagged by the model as potentially inappropriate each minute","A":"Number of messages flagged by the model per minute","C":"Precision and recall estimates based on a random sample of 0.1% of raw messages each minute sent to a human for review"}},{"id":"WlwpFbcqUiFFm0rQhKHj","discussion":[{"comment_id":"749701","content":"Selected Answer: D\nD\n- https://cloud.google.com/vertex-ai/docs/ml-metadata/tracking","poster":"hiromi","timestamp":"1671444060.0","upvote_count":"6"},{"timestamp":"1720018380.0","content":"Selected Answer: D\nCorrect","comment_id":"1241478","poster":"PJ_Exams","upvote_count":"2"},{"comment_id":"1150641","upvote_count":"1","timestamp":"1707956700.0","content":"Selected Answer: D\nSelected Answer: D","poster":"SubbuJV"},{"timestamp":"1683609600.0","upvote_count":"1","content":"Selected Answer: D\nWent with D","comment_id":"892775","poster":"M25"},{"timestamp":"1675913640.0","comment_id":"802799","content":"Selected Answer: D\ntotally D","poster":"enghabeth","upvote_count":"2"},{"poster":"ares81","upvote_count":"3","comment_id":"745025","timestamp":"1671019560.0","content":"Selected Answer: D\nThis should be an easy D."},{"content":"Selected Answer: D\nhttps://codelabs.developers.google.com/vertex-mlmd-pipelines?hl=id&authuser=6#0","poster":"LearnSodas","upvote_count":"3","comment_id":"742007","timestamp":"1670785140.0"}],"url":"https://www.examtopics.com/discussions/google/view/91062-exam-professional-machine-learning-engineer-topic-1-question/","answer_ET":"D","topic":"1","answer":"D","answer_images":[],"timestamp":"2022-12-11 19:59:00","choices":{"D":"Manage your ML workflows with Vertex ML Metadata.","C":"Store all ML metadata in Google Cloud’s operations suite.","A":"Store your tf.logging data in BigQuery.","B":"Manage all relational entities in the Hive Metastore."},"unix_timestamp":1670785140,"question_id":290,"answers_community":["D (100%)"],"isMC":true,"question_text":"You are a lead ML engineer at a retail company. You want to track and manage ML metadata in a centralized way so that your team can have reproducible experiments by generating artifacts. Which management solution should you recommend to your team?","answer_description":"","question_images":[],"exam_id":13}],"exam":{"provider":"Google","name":"Professional Machine Learning Engineer","numberOfQuestions":304,"isImplemented":true,"isBeta":false,"lastUpdated":"11 Apr 2025","isMCOnly":true,"id":13},"currentPage":58},"__N_SSP":true}