{"pageProps":{"questions":[{"id":"ykVATMh6MfgVwRwoonOP","exam_id":13,"question_id":21,"answer_description":"","answer_ET":"C","unix_timestamp":1670936280,"question_images":[],"answers_community":["C (100%)"],"isMC":true,"url":"https://www.examtopics.com/discussions/google/view/91413-exam-professional-machine-learning-engineer-topic-1-question/","topic":"1","question_text":"You are an ML engineer at an ecommerce company and have been tasked with building a model that predicts how much inventory the logistics team should order each month. Which approach should you take?","discussion":[{"poster":"mil_spyro","comment_id":"744023","upvote_count":"10","content":"Selected Answer: C\nThis type of model is well-suited to predicting inventory levels because it can take into account trends and patterns in the data over time, such as seasonal fluctuations in demand or changes in customer behavior.","timestamp":"1718276280.0"},{"poster":"M25","timestamp":"1731145860.0","upvote_count":"1","comment_id":"892907","content":"Selected Answer: C\nhttps://cloud.google.com/learn/what-is-time-series\n\"For example, a large retail store may have millions of items to forecast so that inventory is available when demand is high, and not overstocked when demand is low.\""},{"upvote_count":"1","content":"Selected Answer: C\nAnswer C","comment_id":"831894","timestamp":"1725705840.0","poster":"TNT87"},{"content":"Selected Answer: C\nYup it's C (Time series forecasting)","upvote_count":"1","comment_id":"779719","timestamp":"1721281560.0","poster":"JeanEl"},{"timestamp":"1720004880.0","upvote_count":"1","comment_id":"764663","poster":"ares81","content":"Selected Answer: C\nTime-series forecasting model is the key expression, for me."},{"comment_id":"752135","timestamp":"1718958420.0","poster":"hiromi","content":"Selected Answer: C\nC (by experience)\n Use a time series forecasting model to predict each item's monthly sales. Give the results to the logistics team so they can base inventory on the amount predicted by the model.","upvote_count":"3"}],"choices":{"A":"Use a clustering algorithm to group popular items together. Give the list to the logistics team so they can increase inventory of the popular items.","D":"Use a classification model to classify inventory levels as UNDER_STOCKED, OVER_STOCKED, and CORRECTLY_STOCKEGive the report to the logistics team each month so they can fine-tune inventory levels.","B":"Use a regression model to predict how much additional inventory should be purchased each month. Give the results to the logistics team at the beginning of the month so they can increase inventory by the amount predicted by the model.","C":"Use a time series forecasting model to predict each item's monthly sales. Give the results to the logistics team so they can base inventory on the amount predicted by the model."},"timestamp":"2022-12-13 13:58:00","answer_images":[],"answer":"C"},{"id":"4R4f7QmynBHtwf4NvdE2","discussion":[{"timestamp":"1671618660.0","upvote_count":"8","poster":"hiromi","content":"Selected Answer: D\nD\nyou have built frequent checkpointing into the training process / minimize cost -> preemptible","comment_id":"752139"},{"content":"Selected Answer: C\nFor financial institutions, reliability and minimizing interruptions are crucial. While preemptible instances are cost-effective, they do come with the risk of being terminated unexpectedly, which might not be ideal for critical financial applications.","upvote_count":"1","comment_id":"1278396","timestamp":"1725469440.0","poster":"5a74493"},{"poster":"M25","upvote_count":"2","content":"Selected Answer: D\nFollows same principle as #70","timestamp":"1683619080.0","comment_id":"892911"},{"upvote_count":"1","timestamp":"1681559400.0","poster":"Antmal","comment_id":"870892","content":"Selected Answer: D\nPreemptible v3-8 TPUs are the most cost-effective option for training large TensorFlow models. They are up to 80% cheaper than non-preemptible v3-8 TPUs, and they are only preempted if Google Cloud needs the resources for other workloads.\n\nIn this case, the model is long-running and checkpointing is used. This means that the training process can be interrupted and resumed without losing any progress. Therefore, preemptible TPUs are a safe choice, as the training process will not be interrupted if the TPU is preempted.\n\nThe other options are not as cost-effective."},{"timestamp":"1678388640.0","upvote_count":"1","poster":"TNT87","comment_id":"834294","content":"Selected Answer: D\nAnswer D"},{"poster":"ares81","timestamp":"1673097180.0","upvote_count":"4","comment_id":"768575","content":"Selected Answer: D\nFrequent checkpoints --> Preemptible --> D"},{"content":"Selected Answer: D\npreemptible is the keyword to me","timestamp":"1671328020.0","poster":"mymy9418","comment_id":"748540","upvote_count":"1"}],"timestamp":"2022-12-18 02:47:00","answer":"D","answer_ET":"D","answer_description":"","question_id":22,"topic":"1","isMC":true,"exam_id":13,"url":"https://www.examtopics.com/discussions/google/view/91960-exam-professional-machine-learning-engineer-topic-1-question/","answers_community":["D (94%)","6%"],"question_images":[],"unix_timestamp":1671328020,"choices":{"C":"A Vertex AI Workbench user-managed notebooks instance running on an n1-standard-16 with a non-preemptible v3-8 TPU","A":"A Vertex AI Workbench user-managed notebooks instance running on an n1-standard-16 with 4 NVIDIA P100 GPUs","B":"A Vertex AI Workbench user-managed notebooks instance running on an n1-standard-16 with an NVIDIA P100 GPU","D":"A Vertex AI Workbench user-managed notebooks instance running on an n1-standard-16 with a preemptible v3-8 TPU"},"answer_images":[],"question_text":"You are building a TensorFlow model for a financial institution that predicts the impact of consumer spending on inflation globally. Due to the size and nature of the data, your model is long-running across all types of hardware, and you have built frequent checkpointing into the training process. Your organization has asked you to minimize cost. What hardware should you choose?"},{"id":"KaEm9eRs7xzqU8iKioLh","exam_id":13,"answer_description":"","answer":"B","question_text":"You work for a company that provides an anti-spam service that flags and hides spam posts on social media platforms. Your company currently uses a list of 200,000 keywords to identify suspected spam posts. If a post contains more than a few of these keywords, the post is identified as spam. You want to start using machine learning to flag spam posts for human review. What is the main advantage of implementing machine learning for this business case?","timestamp":"2022-12-13 14:02:00","url":"https://www.examtopics.com/discussions/google/view/91415-exam-professional-machine-learning-engineer-topic-1-question/","unix_timestamp":1670936520,"question_images":[],"choices":{"B":"New problematic phrases can be identified in spam posts.","D":"Spam posts can be flagged using far fewer keywords.","A":"Posts can be compared to the keyword list much more quickly.","C":"A much longer keyword list can be used to flag spam posts."},"answers_community":["B (100%)"],"answer_images":[],"answer_ET":"B","isMC":true,"discussion":[{"poster":"mil_spyro","timestamp":"1718276520.0","content":"Selected Answer: B\nI vote B. Machine learning algorithms can learn to identify spam posts based on a wider range of factors, such as the content of the post, the user's behavior, and the context in which the post appears.","comment_id":"744027","upvote_count":"7"},{"poster":"M25","comment_id":"892928","timestamp":"1731148800.0","upvote_count":"1","content":"Selected Answer: B\nhttps://cloud.google.com/blog/topics/developers-practitioners/how-spam-detection-taught-us-better-tech-support\n\"Borrowing spam tech\n(...) Those engineers had thought through “how do we detect a new spam campaign quickly?” Spammers rapidly send bulk messages with slight variations in content (noise, misspellings, etc.) Most classification attempts would become a game of cat and mouse since it takes classifiers some time to learn about new patterns.\nInvoking a trend identification engine using unsupervised density clustering on unstructured text unlocked the ability for Gmail to detect ephemeral spam campaigns more quickly.\""},{"upvote_count":"1","timestamp":"1725901440.0","comment_id":"834293","content":"Selected Answer: B\nAnswer B","poster":"TNT87"},{"comment_id":"764653","content":"Selected Answer: B\nB screams machine learning with every letter.","upvote_count":"3","timestamp":"1720004340.0","poster":"ares81"},{"comment_id":"752142","poster":"hiromi","content":"Selected Answer: B\nB make sense & I agree with mill_sypro","timestamp":"1718958780.0","upvote_count":"2"}],"question_id":23,"topic":"1"},{"id":"ZQ49ecnNaoJVsYvRRuoW","isMC":true,"answer_description":"","exam_id":13,"answer_ET":"C","discussion":[{"comment_id":"409197","upvote_count":"29","poster":"Celia20210714","timestamp":"1626648780.0","content":"ANS: C\n\nhttps://cloud.google.com/architecture/architecture-of-a-serverless-ml-model#architecture\nThe architecture has the following flow:\nA user writes a ticket to Firebase, which triggers a Cloud Function.\n-The Cloud Function calls 3 different endpoints to enrich the ticket:\n-An AI Platform endpoint, where the function can predict the priority.\n-An AI Platform endpoint, where the function can predict the resolution time.\n-The Natural Language API to do sentiment analysis and word salience.\n-For each reply, the Cloud Function updates the Firebase real-time database.\n-The Cloud Function then creates a ticket into the helpdesk platform using the RESTful API."},{"poster":"gcp2021go","timestamp":"1623083520.0","upvote_count":"17","content":"the answer should be C. The tickets do not include specific terms , which means, it doesn't need to be custom built. thus, we can use cloud NLP API instead of automl NLP.","comment_id":"376912"},{"comment_id":"1288595","timestamp":"1727181300.0","content":"Selected Answer: C\nANS: C\nTickets are not expected to have any domain-specific terms or jargon. Therefore we can use the Natural Language API, and we don't need to train our own model.","poster":"wishyrater","upvote_count":"2"},{"content":"Selected Answer: C\nC) Eliminate A and D as not vision or images required. From B (Auto ML Natural Language) requires custom training and C) NLP API gives you sentiment analysis out of the box.","upvote_count":"3","poster":"PhilipKoku","timestamp":"1717649460.0","comment_id":"1225144"},{"upvote_count":"3","poster":"Sum_Sum","comment_id":"1070448","content":"Selected Answer: C\nC - as Natural Language API has sentiment analysis \nand using the API over a custom model is always preferred","timestamp":"1699970880.0"},{"timestamp":"1688878800.0","upvote_count":"4","poster":"harithacML","comment_id":"946890","content":"Selected Answer: C\nReq : serverless ML system + models to (predict ticket priority -predict ticket resolution time- perform sentiment analysis )\nThe proposed architecture has the following flow:\n\nA. 1 = AI Platform, 2 = AI Platform, 3 = AutoML Vision. : No image data as input here. Only text (NLP)\nB. 1 = AI Platform, 2 = AI Platform, 3 = AutoML Natural Language : Only sentiment for 3rd endpoint. No custom model needed : https://cloud.google.com/natural-language/automl/docs/beginners-guide . So autoML not required\nC. 1 = AI Platform, 2 = AI Platform, 3 = Cloud Natural Language API : 1- for classification(priority :high low medium), 2- ticket time-regression -3- sentiment analysis the CNL api is enough \nD. 1 = Cloud Natural Language API, 2 = AI Platform, 3 = Cloud Vision API : No image data"},{"comment_id":"892687","timestamp":"1683608160.0","upvote_count":"1","content":"Selected Answer: C\nWent with C","poster":"M25"},{"upvote_count":"2","poster":"wish0035","content":"Selected Answer: C\nANS: C\n\nThis is the exact solution by Google: https://web.archive.org/web/20210618072649/https://cloud.google.com/architecture/architecture-of-a-serverless-ml-model#architecture","comment_id":"746502","timestamp":"1671138480.0"},{"comment_id":"745841","poster":"jespinosal","timestamp":"1671091980.0","content":"Selected Answer: B\nANS: B As you need to train custom regression models (Auto ML), as NLP API is not going to be able to rank your Priority and eval the Time.","upvote_count":"1"},{"timestamp":"1671091860.0","upvote_count":"1","poster":"jespinosal","comment_id":"745840","content":"ANS: C as NLP API is not able to perform custom Regression Models (predict time) and Priority. You need Auto ML o train your own"},{"poster":"EFIGO","timestamp":"1669218660.0","upvote_count":"2","content":"Selected Answer: C\nAI Platform (now Vertex AI) for both the predictions and Natural Language API for sentiment analysis since there are no specific terms (so no need to custom build something with an AutoML), so C","comment_id":"725240"},{"comment_id":"647181","content":"Selected Answer: C\nCorrect answer is \"C\"","poster":"GCP72","timestamp":"1660566660.0","upvote_count":"1"},{"poster":"Mohamed_Mossad","content":"Selected Answer: C\n- by options eliminations A,D must be dropped we have no vision tasks in this system\n- answer between B,C , question stated \"no specific domain or jargon\" so natural laguage api is prefered over automl since there no custom entinites or custom training , so I vote for C","upvote_count":"2","timestamp":"1655040960.0","comment_id":"615342"},{"content":"Selected Answer: C\nCommunity vote","upvote_count":"4","timestamp":"1646016060.0","comment_id":"557799","poster":"caohieu04"},{"upvote_count":"2","poster":"alphard","comment_id":"495070","content":"Mine is C.\n\nPriority prediction is categorical. Resolution time is linear regression. Sentiment is a NLP problem.","timestamp":"1638788580.0"},{"poster":"chohan","comments":[{"timestamp":"1625142240.0","poster":"gcp2021go","upvote_count":"7","content":"the question said \"Tickets are not expected to have any domain-specific terms or jargon.\"","comment_id":"395934"}],"comment_id":"382787","upvote_count":"1","content":"Should be B, don't forget the domain specific terms and jargons\nhttps://medium.com/google-cloud/analyzing-sentiment-of-text-with-domain-specific-vocabulary-and-topics-726b8f287aef","timestamp":"1623776460.0"},{"comments":[{"timestamp":"1623016080.0","comment_id":"376342","upvote_count":"2","content":"predict ticket priority (AI plateform : classification), predict ticket resolution time (AI plateform : regression), and perform sentiment analysis ( Cloud NLP API )","poster":"Hiba01"},{"comment_id":"416341","content":"D is wrong since Cloud Vision API is not needed.","poster":"sensev","timestamp":"1627492800.0","upvote_count":"2"}],"timestamp":"1622918400.0","poster":"inder0007","upvote_count":"1","comment_id":"375388","content":"not sure if I agree with b, I think D is a better choice"}],"question_id":24,"choices":{"C":"1 = AI Platform, 2 = AI Platform, 3 = Cloud Natural Language API","B":"1 = AI Platform, 2 = AI Platform, 3 = AutoML Natural Language","D":"1 = Cloud Natural Language API, 2 = AI Platform, 3 = Cloud Vision API","A":"1 = AI Platform, 2 = AI Platform, 3 = AutoML Vision"},"answer":"C","answer_images":[],"question_text":"You are designing an architecture with a serverless ML system to enrich customer support tickets with informative metadata before they are routed to a support agent. You need a set of models to predict ticket priority, predict ticket resolution time, and perform sentiment analysis to help agents make strategic decisions when they process support requests. Tickets are not expected to have any domain-specific terms or jargon.\nThe proposed architecture has the following flow:\n//IMG//\n\nWhich endpoints should the Enrichment Cloud Functions call?","question_images":["https://www.examtopics.com/assets/media/exam-media/03841/0000800001.png"],"url":"https://www.examtopics.com/discussions/google/view/54658-exam-professional-machine-learning-engineer-topic-1-question/","timestamp":"2021-06-05 20:40:00","unix_timestamp":1622918400,"answers_community":["C (96%)","4%"],"topic":"1"},{"id":"lCInOkMAkdjjshkIXMWE","exam_id":13,"url":"https://www.examtopics.com/discussions/google/view/91418-exam-professional-machine-learning-engineer-topic-1-question/","question_text":"One of your models is trained using data provided by a third-party data broker. The data broker does not reliably notify you of formatting changes in the data. You want to make your model training pipeline more robust to issues like this. What should you do?","topic":"1","question_id":25,"choices":{"C":"Use tf.math to analyze the data, compute summary statistics, and flag statistical anomalies.","D":"Use custom TensorFlow functions at the start of your model training to detect and flag known formatting errors.","B":"Use TensorFlow Transform to create a preprocessing component that will normalize data to the expected distribution, and replace values that don’t match the schema with 0.","A":"Use TensorFlow Data Validation to detect and flag schema anomalies."},"answers_community":["A (100%)"],"timestamp":"2022-12-13 14:07:00","isMC":true,"unix_timestamp":1670936820,"answer_images":[],"answer_description":"","answer_ET":"A","discussion":[{"upvote_count":"2","content":"Selected Answer: A\ni would choose A and B because For the model to be truly robust, it needs to adapt to new formats, not just detect and flag anomalies. In this case, combining detection with adaptive preprocessing would be the best approach","timestamp":"1725469980.0","poster":"5a74493","comment_id":"1278398"},{"comment_id":"892953","upvote_count":"1","timestamp":"1683624960.0","content":"Selected Answer: A\nWent with A","poster":"M25"},{"timestamp":"1679160720.0","poster":"Yajnas_arpohc","content":"Selected Answer: A\nYou need to know problem b4 fixing w transform, hence A","comment_id":"843010","upvote_count":"2"},{"poster":"TNT87","content":"Selected Answer: A\nAnswer A","upvote_count":"1","comment_id":"831893","timestamp":"1678193040.0"},{"timestamp":"1675920960.0","comment_id":"802866","content":"Selected Answer: A\nhttps://www.tensorflow.org/tfx/guide/tfdv#schema_based_example_validation","upvote_count":"1","poster":"John_Pongthorn"},{"content":"Selected Answer: A\nTensorflow Data Validation (TFDV) can analyze training and serving data to: compute descriptive statistics, infer a schema, detect data anomalies. A.","upvote_count":"1","poster":"ares81","timestamp":"1672750440.0","comment_id":"764645"},{"timestamp":"1671619140.0","content":"Selected Answer: A\nA\n- https://www.tensorflow.org/tfx/data_validation/get_started","upvote_count":"3","comment_id":"752148","poster":"hiromi"},{"timestamp":"1670936820.0","poster":"mil_spyro","upvote_count":"4","comment_id":"744039","content":"Selected Answer: A\nTensorFlow Data Validation (TFDV) is a library that can help you detect and flag anomalies in your dataset, such as changes in the schema or data types.\nhttps://www.tensorflow.org/tfx/data_validation/get_started"}],"question_images":[],"answer":"A"}],"exam":{"lastUpdated":"11 Apr 2025","isImplemented":true,"name":"Professional Machine Learning Engineer","provider":"Google","numberOfQuestions":304,"isMCOnly":true,"id":13,"isBeta":false},"currentPage":5},"__N_SSP":true}