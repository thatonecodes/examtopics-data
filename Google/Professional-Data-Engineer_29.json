{"pageProps":{"questions":[{"id":"gbTTBNK3SyB1ZONVKqn6","isMC":true,"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/129872-exam-professional-data-engineer-topic-1-question-225/","choices":{"D":"Migrate your data to BigLake. Refactor Spark pipelines to write and read data on Cloud Storage, and run them on Dataproc on Compute Engine.","B":"Migrate your data to Cloud Storage and register the bucket as a Dataplex asset. Refactor Spark pipelines to write and read data on Cloud Storage, and run them on Dataproc Serverless.","A":"Migrate your data to Cloud Storage and migrate the metadata to Dataproc Metastore (DPMS). Refactor Spark pipelines to write and read data on Cloud Storage, and run them on Dataproc Serverless.","C":"Migrate your data to BigQuery. Refactor Spark pipelines to write and read data on BigQuery, and run them on Dataproc Serverless."},"answer":"A","timestamp":"2023-12-30 09:51:00","question_text":"Your organization stores customer data in an on-premises Apache Hadoop cluster in Apache Parquet format. Data is processed on a daily basis by Apache Spark jobs that run on the cluster. You are migrating the Spark jobs and Parquet data to Google Cloud. BigQuery will be used on future transformation pipelines so you need to ensure that your data is available in BigQuery. You want to use managed services, while minimizing ETL data processing changes and overhead costs. What should you do?","question_id":141,"answer_images":[],"topic":"1","answer_ET":"A","answers_community":["A (42%)","C (39%)","Other"],"exam_id":11,"question_images":[],"unix_timestamp":1703926260,"discussion":[{"comment_id":"1113799","upvote_count":"5","timestamp":"1704381540.0","content":"Selected Answer: A\n- This option involves moving Parquet files to Cloud Storage, which is a common and cost-effective storage solution for big data and is compatible with Spark jobs. \n- Using Dataproc Metastore to manage metadata allows us to keep Hadoop ecosystem's structural information. \n- Running Spark jobs on Dataproc Serverless takes advantage of managed Spark services without managing clusters. \n- Once the data is in Cloud Storage, you can also easily load it into BigQuery for further analysis.","poster":"raaad"},{"upvote_count":"1","poster":"380e3c6","content":"Selected Answer: A\nA is correctbecause it minimizes ETL changes, keeps Parquet data in Cloud Storage (cost-effective and Spark-compatible), and integrates with BigQuery via external tables. C is flawed** since moving directly to BigQuery requires refactoring Spark jobs, increasing complexity and costs. B adds unnecessary governance overhead, and D focuses on infrastructure instead of pipeline efficiency.","timestamp":"1739768880.0","comment_id":"1357582"},{"upvote_count":"1","timestamp":"1739173260.0","poster":"plum21","content":"Selected Answer: D\nThe requirement:\n\"You want to use managed services\"\nexcludes Dataproc Serverless. \nDataproc on Compute Engine remains.\nNext requirement:\n\"BigQuery will be used on future transformation pipelines so you need to ensure that your data is available in BigQuery\" -> BigLake\nNext requirement:\n\"while minimizing ETL data processing changes and overhead costs\" -> Refactor Spark pipelines to write and read data on Cloud Storage\n\nNotes\n1. Dataproc Metastore (DPMS) could be used on Dataproc to read data from BQ but not the other way round.","comment_id":"1354328"},{"comment_id":"1352109","content":"Selected Answer: B\nBigQuery Integration: The requirement is to make data available in BigQuery. Dataplex has built-in integration with BigQuery. It can automatically discover data in Cloud Storage and create external tables in BigQuery, making the data readily queryable. DPMS doesn't have this direct integration with BigQuery.","timestamp":"1738794480.0","upvote_count":"3","poster":"skhaire"},{"comment_id":"1346689","timestamp":"1737844200.0","poster":"LP_PDE","content":"Selected Answer: A\nBoth Spark and BigQuery can directly access data in Cloud Storage.","upvote_count":"1"},{"poster":"hrishi19","comment_id":"1313140","content":"Selected Answer: C\nThe question states that the data should be available on BigQuery and only option C meets this requirement.","timestamp":"1731776580.0","upvote_count":"3"},{"timestamp":"1723546740.0","poster":"JamesKarianis","upvote_count":"1","comment_id":"1265115","content":"Selected Answer: A\nA is correct"},{"timestamp":"1717667700.0","comment_id":"1225349","comments":[{"comment_id":"1269437","content":"Just adding further commentary on why A is correct while why other options are incorrect is explained above.\nParquet files have schema engrained in them. Hence Spark pipelines on Hadoop Cluster may not have needed tables at all. Hence the simplest solution would be to move it to Cloud Storage instead of BigQuery and this way there would be minimal changes to the ETL pipelines - just change HDFS file system pointer to GCS file system for read writes and no need for any additional tables","poster":"aoifneofi_ef","timestamp":"1724155740.0","upvote_count":"2"}],"content":"Selected Answer: A\nOption B: Registering the bucket as a Dataplex asset adds an additional layer of data governance and management. While useful, it may not be necessary for your immediate migration needs and can introduce additional complexity.\nOption C: Migrating data directly to BigQuery would require significant changes to your Spark pipelines since they would need to be refactored to read from and write to BigQuery instead of Parquet files. This approach could introduce higher costs due to BigQuery storage and querying.\nOption D: Using BigLake and Dataproc on Compute Engine is more complex and requires more management compared to Dataproc Serverless. Additionally, it might not be as cost-effective as leveraging Cloud Storage and Dataproc Serverless.","upvote_count":"3","poster":"Anudeep58"},{"comment_id":"1213456","poster":"josech","upvote_count":"1","content":"Selected Answer: A\nThe question says \"You want to use managed services, while minimizing ETL data processing changes and overhead costs\". Dataproc is a managed service that doesn't need to refactor the data transformation Spark code you already have (you will have to refactor only the wrtie and read code), an it has a Big Query connector for future use. https://cloud.google.com/dataproc/docs/concepts/connectors/bigquery","timestamp":"1716062280.0"},{"content":"Selected Answer: C\nMigrate your data directly to BigQuery.\nRefactor Spark pipelines to read from and write to BigQuery.\nRun the Spark jobs on Dataproc Serverless.\nThe best choice for ensuring data availability in BigQuery. It allows seamless integration with BigQuery and minimizes ETL changes.","comment_id":"1167829","upvote_count":"3","poster":"52ed0e5","timestamp":"1709803320.0"},{"comment_id":"1159924","timestamp":"1708964460.0","poster":"Ramon98","upvote_count":"4","content":"Selected Answer: C\nA tricky one, because of \"you need to ensure that your data is available in BigQuery\". The easiest and most straight forward migration seems answer A to me, and then you can use external tables to make the parquet data directly available in BigQuery.\nhttps://cloud.google.com/bigquery/docs/loading-data-cloud-storage-parquet\n\nHowever creating the external tables is an extra step? So therefore maybe C is the answer?"},{"poster":"Moss2011","upvote_count":"1","content":"Selected Answer: C\nI think the key phrase here is \"you need to ensure that your data is available in BigQuery\" that's why I thing C it's the best option","timestamp":"1708913460.0","comment_id":"1159300"},{"timestamp":"1708183860.0","poster":"JyoGCP","comment_id":"1152651","comments":[{"comment_id":"1152653","poster":"JyoGCP","content":"As per question.. \"BigQuery will be used on future transformation pipelines so you need to ensure that your data is available in BigQuery. You want to use managed services (DATAPROC), while minimizing ETL data processing changes and overhead costs.\"","timestamp":"1708183980.0","upvote_count":"3"}],"content":"Selected Answer: C\nI think it's C.\n\nDataproc can use BigQuery to read and write data. \nDataproc's BigQuery connector is a library that allows Spark and Hadoop applications to process and write data from BigQuery. \n\nHere's how Dataproc can be used with BigQuery:\nProcess large datasets: Use Spark to process data stored in BigQuery.\nWrite results: Write the results back to BigQuery or other data storage for further analysis.\nRead data: The BigQuery connector can read data from BigQuery into a Spark DataFrame.\nWrite data: The connector writes data to BigQuery by buffering all the data into a Cloud Storage temporary table.","upvote_count":"3"},{"poster":"matiijax","upvote_count":"3","content":"Selected Answer: B\nI think its B and the reason is that egistering the data as a Dataplex asset enables seamless integration with BigQuery later on. Dataplex simplifies data discovery and lineage tracking, making it easier to prepare your data for BigQuery transformations.","timestamp":"1707930180.0","comment_id":"1150351"},{"upvote_count":"3","timestamp":"1707390120.0","comment_id":"1144372","poster":"saschak94","content":"Why would I select A here? Why not moving the data to BigQuery and running Dataproc Serverless jobs accessing the data in BigQuery?"},{"comment_id":"1109553","poster":"e70ea9e","timestamp":"1703926260.0","upvote_count":"3","content":"Selected Answer: A\nManaged Services: Leverages Dataproc Serverless for a fully managed Spark environment, reducing overhead and administrative tasks.\nMinimal Data Processing Changes: Keeps Spark pipelines largely intact by working with Parquet files on Cloud Storage, minimizing refactoring efforts.\nBigQuery Integration: Dataproc Serverless can directly access BigQuery, enabling future transformation pipelines without additional data movement.\nCost-Effective: Serverless model scales resources only when needed, optimizing costs for intermittent workloads."}]},{"id":"Czdgk5af2PyBduaAdHBX","question_text":"Your organization has two Google Cloud projects, project A and project B. In project A, you have a Pub/Sub topic that receives data from confidential sources. Only the resources in project A should be able to access the data in that topic. You want to ensure that project B and any future project cannot access data in the project A topic. What should you do?","unix_timestamp":1703926320,"question_id":142,"answer_ET":"B","topic":"1","answer_images":[],"isMC":true,"answers_community":["B (64%)","C (36%)"],"choices":{"B":"Configure VPC Service Controls in the organization with a perimeter around project A.","A":"Add firewall rules in project A so only traffic from the VPC in project A is permitted.","C":"Use Identity and Access Management conditions to ensure that only users and service accounts in project A. can access resources in project A.","D":"Configure VPC Service Controls in the organization with a perimeter around the VPC of project A."},"discussion":[{"poster":"datapassionate","upvote_count":"11","content":"Selected Answer: C\nAnd I would agree with GPT. The question is about that who can do what within GCP environment. It's all about permissions and access management, not about networking.","comment_id":"1123599","timestamp":"1705347360.0"},{"content":"Selected Answer: B\nOption B:\n-It allows us to create a secure boundary around all resources in Project A, including the Pub/Sub topic. \n- It prevents data exfiltration to other projects and ensures that only resources within the perimeter (Project A) can access the sensitive data. \n- VPC Service Controls are specifically designed for scenarios where you need to secure sensitive data within a specific context or boundary in Google Cloud.","comment_id":"1113819","upvote_count":"7","timestamp":"1704382800.0","poster":"raaad"},{"poster":"MithunDesai","content":"Selected Answer: B\nThe best solution to prevent project B and any future projects from accessing data in project A&#x27;s Pub/Sub topic is B. Configure VPC Service Controls in the organization with a perimeter around project A.","upvote_count":"3","comment_id":"1265090","timestamp":"1723543140.0"},{"comment_id":"1263531","poster":"meh_33","timestamp":"1723298160.0","upvote_count":"3","content":"Selected Answer: B\nB is correct Raaad is always right"},{"content":"Selected Answer: B\nSetting up a perimeter around project A is future proof, the question asks to \"ensure that project B and any future project cannot access data in the project A topic\", IAM is not future proof.\n\nReference: https://cloud.google.com/vpc-service-controls/docs/overview#isolate\n\np.s: VPC Service Controls is not the same thing as VPC, instead its a security layer on top of a VPC and it should be used together with IAM, not one or the other (https://cloud.google.com/vpc-service-controls/docs/overview#how-vpc-service-controls-works)","comment_id":"1217307","upvote_count":"4","timestamp":"1716538620.0","poster":"fabiogoma"},{"upvote_count":"2","comment_id":"1214279","timestamp":"1716200340.0","content":"Selected Answer: C\nC. Use Identity and Access Management conditions to ensure that only users and service accounts in project A. can access resources in project A. [SIMPLE!!!]","poster":"virat_kohli"},{"upvote_count":"2","comment_id":"1152685","poster":"JyoGCP","content":"Selected Answer: B\nI'll go with \"B. Configure VPC Service Controls in the organization with a perimeter around project A.\"","timestamp":"1708187040.0"},{"timestamp":"1705346460.0","comments":[{"poster":"datapassionate","upvote_count":"1","timestamp":"1705346520.0","comment_id":"1123587","comments":[{"timestamp":"1705346580.0","poster":"datapassionate","upvote_count":"1","comments":[{"timestamp":"1705412520.0","upvote_count":"1","poster":"datapassionate","content":"D. Configure VPC Service Controls in the organization with a perimeter around the VPC of project A.\n\nAnalysis: Similar to option B, this is focused on securing network boundaries rather than specific resource access within GCP. While it could provide an additional layer of security, it's not the most direct way to control access to a specific Pub/Sub topic.","comment_id":"1123589"}],"comment_id":"1123588","content":"B. Configure VPC Service Controls in the organization with a perimeter around project A.\n\nAnalysis: VPC Service Controls provide a security perimeter for your data, but they are more focused on preventing data exfiltration; this might be more complex and broader than necessary for the specific requirement of restricting access to a Pub/Sub topic."}],"content":"A. Add firewall rules in project A so only traffic from the VPC in project A is permitted.\n\nAnalysis: Firewall rules in GCP are used to control traffic to and from instances within Google Cloud Virtual Private Clouds (VPCs). However, they don't specifically control access to Pub/Sub resources. Pub/Sub access is managed through IAM, not VPC firewall rules."}],"upvote_count":"1","comment_id":"1123586","poster":"datapassionate","content":"GPT:\nC. Use Identity and Access Management conditions to ensure that only users and service accounts in project A can access resources in project A.\n\nAnalysis: This is the most appropriate option. IAM allows you to define who (which users or service accounts) has what access to your GCP resources. By setting IAM policies with conditions specific to Project A, you can ensure that only designated entities within Project A have access to its resources, including the Pub/Sub topic.\nD. Configure VPC Service Controls in the organization with a perimeter around the VPC of project A."},{"upvote_count":"4","timestamp":"1703926320.0","comment_id":"1109554","content":"Selected Answer: B\nVPC Service Controls enforce a security perimeter around entire projects, ensuring that resources within project A (including the Pub/Sub topic) are inaccessible from any other project, including project B and future projects.\nThis aligns with the requirement to prevent cross-project access.","poster":"e70ea9e"}],"timestamp":"2023-12-30 09:52:00","url":"https://www.examtopics.com/discussions/google/view/129873-exam-professional-data-engineer-topic-1-question-226/","exam_id":11,"question_images":[],"answer":"B","answer_description":""},{"id":"2UxL5U3USO6wXBE975eJ","isMC":true,"question_images":[],"question_text":"You stream order data by using a Dataflow pipeline, and write the aggregated result to Memorystore. You provisioned a Memorystore for Redis instance with Basic Tier, 4 GB capacity, which is used by 40 clients for read-only access. You are expecting the number of read-only clients to increase significantly to a few hundred and you need to be able to support the demand. You want to ensure that read and write access availability is not impacted, and any changes you make can be deployed quickly. What should you do?","topic":"1","url":"https://www.examtopics.com/discussions/google/view/129874-exam-professional-data-engineer-topic-1-question-227/","answers_community":["B (100%)"],"discussion":[{"content":"Selected Answer: B\n- Upgrading to the Standard Tier and adding read replicas is an effective way to scale and manage increased read load. \n- The additional capacity (5 GB) provides more space for data, and read replicas help distribute the read load across multiple instances.","poster":"raaad","timestamp":"1704382980.0","comments":[{"content":"Descrived here:\nhttps://cloud.google.com/memorystore/docs/redis/redis-tiers","comment_id":"1123608","upvote_count":"3","timestamp":"1705347840.0","poster":"datapassionate"}],"comment_id":"1113822","upvote_count":"10"},{"timestamp":"1730228880.0","content":"Selected Answer: B\nI don't like any answer. It seems Option B makes more senses due to read replicas.","upvote_count":"1","comment_id":"1304660","poster":"SamuelTsch"},{"poster":"e70ea9e","timestamp":"1703926380.0","upvote_count":"3","comment_id":"1109555","content":"Selected Answer: B\nScalability for Read-Only Clients: Read replicas distribute read traffic across multiple instances, significantly enhancing read capacity to support a large number of clients without impacting write performance.\nHigh Availability: Standard Tier ensures high availability with automatic failover, minimizing downtime in case of instance failure.\nMinimal Code Changes: Redis clients can seamlessly connect to read replicas without requiring extensive code modifications, enabling a quick deployment."}],"unix_timestamp":1703926380,"question_id":143,"answer":"B","answer_ET":"B","answer_description":"","exam_id":11,"timestamp":"2023-12-30 09:53:00","answer_images":[],"choices":{"D":"Create multiple new Memorystore for Redis instances with Basic Tier (4 GB capacity). Modify the Dataflow pipeline and new clients to use all instances.","B":"Create a new Memorystore for Redis instance with Standard Tier. Set capacity to 5 GB and create multiple read replicas. Delete the old instance.","C":"Create a new Memorystore for Memcached instance. Set a minimum of three nodes, and memory per node to 4 GB. Modify the Dataflow pipeline and all clients to use the Memcached instance. Delete the old instance.","A":"Create a new Memorystore for Redis instance with Standard Tier. Set capacity to 4 GB and read replica to No read replicas (high availability only). Delete the old instance."}},{"id":"ozQDKYiEuL0mzGj8xWsQ","url":"https://www.examtopics.com/discussions/google/view/129875-exam-professional-data-engineer-topic-1-question-228/","unix_timestamp":1703926440,"topic":"1","answer_ET":"D","timestamp":"2023-12-30 09:54:00","answer_description":"","question_images":[],"question_text":"You have a streaming pipeline that ingests data from Pub/Sub in production. You need to update this streaming pipeline with improved business logic. You need to ensure that the updated pipeline reprocesses the previous two days of delivered Pub/Sub messages. What should you do? (Choose two.)","answers_community":["D (46%)","B (42%)","12%"],"question_id":144,"choices":{"C":"Create a new Pub/Sub subscription two days before the deployment.","B":"Use Pub/Sub Snapshot capture two days before the deployment.","E":"Use Pub/Sub Seek with a timestamp.","D":"Use the Pub/Sub subscription retain-acked-messages flag.","A":"Use the Pub/Sub subscription clear-retry-policy flag"},"exam_id":11,"answer_images":[],"isMC":true,"discussion":[{"content":"Selected Answer: D\nDE\n\nAnother way to replay messages that have been acknowledged is to seek to a timestamp. To seek to a timestamp, you must first configure the subscription to retain acknowledged messages using retain-acked-messages. If retain-acked-messages is set, Pub/Sub retains acknowledged messages for 7 days.\n\nYou only need to do this step if you intend to seek to a timestamp, not to a snapshot.\n\nhttps://cloud.google.com/pubsub/docs/replay-message","timestamp":"1705589460.0","poster":"tibuenoc","comment_id":"1125977","upvote_count":"16","comments":[{"poster":"nadavw","timestamp":"1724653020.0","content":"this is correct as there are 2 options (timestamp and snapshot) and foreach there are 2 stages.\nSnapshot - create ('B') and seek\nTimestamp - configure 'retain' ('D') and seek ('E')\nas shown 'B' is missing the 'seek' operation","upvote_count":"1","comment_id":"1272470"},{"comment_id":"1197910","poster":"joao_01","upvote_count":"2","content":"Its BE.\n\nBy the way, you can seek to a snapshot yes:\n\"Seeks an existing subscription to a point in time or to a given snapshot, whichever is provided in the request\"\n\nLink:https://cloud.google.com/pubsub/docs/reference/rest/v1/projects.subscriptions/seek","timestamp":"1713439800.0","comments":[{"comment_id":"1332313","timestamp":"1735288260.0","upvote_count":"2","content":"Think about it - \nwhat you want to do - process last 2 days of messages.\nWhat does snapshot give you - it give you what were the un-acknowledged messages in pub/sub at that point in time 2 days ago.\nHow will that help you process messages that were sent to pub/sub in the last 2 days (i.e. after the snapshot?)","poster":"2ad2bc7"}]}]},{"upvote_count":"12","comment_id":"1126012","content":"Selected Answer: B\nB and E, already tested at cloud console.","timestamp":"1705592760.0","poster":"GCP001"},{"timestamp":"1742733900.0","upvote_count":"1","poster":"desertlotus1211","content":"Selected Answer: B\nThe only way to ensure that the updated pipeline reprocesses the previous two days of delivered Pub/Sub messages (current) is to use B&E.... for future D&E can work.","comment_id":"1402278"},{"upvote_count":"1","timestamp":"1737603060.0","content":"Selected Answer: D\nSince E is seek to a timestamp only one to replay, I think D is correct answer and not B","poster":"71083a7","comment_id":"1345070"},{"upvote_count":"1","poster":"AWSandeep","content":"D and E\n\nPlease seem to not understand that the retain_acked_messages parameter needs to be enabled for any snapshot or seeking functionality to work.","timestamp":"1734858060.0","comment_id":"1330317"},{"poster":"SamuelTsch","comments":[{"upvote_count":"2","content":"change my mind. D and E should be correct.","comment_id":"1304674","timestamp":"1730230500.0","poster":"SamuelTsch"}],"upvote_count":"1","timestamp":"1730230260.0","comment_id":"1304671","content":"Selected Answer: B\nAfter reading https://cloud.google.com/pubsub/docs/replay-overview#snapshot_overview carefully, B and E are correct."},{"comment_id":"1288785","poster":"Preetmehta1234","upvote_count":"1","timestamp":"1727214840.0","content":"Selected Answer: D\nFirst, read this document: https://cloud.google.com/pubsub/docs/replay-overview.\n\nKey Points:\n\nSeek to a Snapshot: Reprocesses only unacknowledged messages.\nSeek to a Timestamp: Reprocesses all messages (acknowledged and unacknowledged) after that time.\nSince the question asks for delivering all messages, option E is correct, as it includes both acknowledged and unacknowledged messages.\n\nRegarding Option D: Configuring a subscription with the retain_acked_messages property allows replaying previously acknowledged messages retained for up to 31 days. This satisfies the requirement to deliver all messages and retains them longer than the mentioned 2 days."},{"timestamp":"1723760280.0","content":"Selected Answer: B\nB&D - based on Vertex AI feedback","poster":"MithunDesai","upvote_count":"1","comment_id":"1266725"},{"comment_id":"1225575","content":"Selected Answer: E\nBE\nB. Use Pub/Sub Snapshot capture two days before the deployment.\n\nPub/Sub Snapshot: Creating a snapshot captures the state of the subscription at a specific point in time. You can then seek to this snapshot to replay messages from that point onwards.\nBy capturing a snapshot two days before the deployment, you can ensure that your pipeline reprocesses messages from the past two days.\nE. Use Pub/Sub Seek with a timestamp.\n\nPub/Sub Seek: This feature allows you to reset the subscription to a specific timestamp. Messages published to the topic after this timestamp are re-delivered.\nBy seeking to the timestamp from two days ago, you can instruct Pub/Sub to start re-delivering messages from that point in time","poster":"Anudeep58","upvote_count":"2","timestamp":"1717685100.0"},{"content":"Selected Answer: D\nD. Use the Pub/Sub subscription retain-acked-messages flag.\nE. Use Pub/Sub Seek with a timestamp.","comment_id":"1214299","timestamp":"1716204420.0","upvote_count":"2","poster":"virat_kohli"},{"upvote_count":"3","comment_id":"1160053","content":"Selected Answer: D\nE for sure, you need to seek from a timestamp.\nTo accomplish to that you need to \"Set the retain-acked-messages flag to true for the subscription.\"\n\nFrom google documentation: \n\n\"Note: To seek to a previous time point, your subscription must be configured to retain acknowledged messages. You can change this setting by clicking Edit on the subscription details page, and checking the box for Retain acknowledged messages.\"\n\nhttps://cloud.google.com/pubsub/docs/replay-message","poster":"cuadradobertolinisebastiancami","timestamp":"1708979520.0"},{"content":"Selected Answer: E\nD and E,\nhttps://cloud.google.com/pubsub/docs/replay-message","comment_id":"1159340","poster":"Tryolabs","timestamp":"1708919040.0","upvote_count":"3"},{"comment_id":"1151998","upvote_count":"3","timestamp":"1708087620.0","poster":"ML6","content":"Selected Answer: E\nB and E: The seek feature extends subscriber capabilities by allowing you to alter the acknowledgement state of messages in bulk. For example, you can replay previously acknowledged messages or purge messages in bulk. In addition, you can copy the acknowledgement state of one subscription to another by using seek in combination with a snapshot. Source: https://cloud.google.com/pubsub/docs/replay-overview"},{"comments":[{"upvote_count":"1","poster":"tibuenoc","timestamp":"1705589760.0","content":"But There is a problem snapshot you shoudl seek by subscriptions not by timestamp","comment_id":"1125985"}],"upvote_count":"3","poster":"Sofiia98","timestamp":"1705323120.0","content":"Selected Answer: B\nBE\nhttps://cloud.google.com/pubsub/docs/replay-overview","comment_id":"1123360"},{"comment_id":"1121533","poster":"Matt_108","upvote_count":"3","content":"Selected Answer: D\nOption D and E","timestamp":"1705144080.0"},{"comment_id":"1118575","timestamp":"1704892860.0","content":"Selected Answer: D\nDE\nSet the retain-acked-messages flag to true for the subscription.\nThis instructs Pub/Sub to store acknowledged messages for a specified retention period.\n\n\nE Use Pub/Sub Seek with a timestamp.\nAfter deploying the updated pipeline, use the Seek feature to replay messages.\nSpecify a timestamp that's two days before the current time.\nThis rewinds the subscription's message cursor, making it redeliver messages from that point onward.","poster":"task_7","upvote_count":"6"},{"comment_id":"1113833","content":"Selected Answer: B\n- Pub/Sub Snapshots allow you to capture the state of a subscription's unacknowledged messages at a particular point in time. \n- By creating a snapshot two days before deploying the updated pipeline, you can later use this snapshot to replay the messages from that point. \n=============\nOption E:\n- Pub/Sub Seek allows us to alter the acknowledgment state of messages in bulk. \n- So we can rewind a subscription to a point in time or a snapshot. \n- Using Seek with a timestamp corresponding to two days ago would allow the updated pipeline to reprocess messages from that time.","poster":"raaad","timestamp":"1704383520.0","upvote_count":"7","comments":[{"content":"Creating a snapshot of the Pub/Sub subscription two days before the deployment captures the state of unacknowledged messages at that particular point in time, which would include messages from before those two days. If our objective is to reprocess the data from the last two days specifically, then capturing a snapshot two days prior wouldn't directly address this need.","timestamp":"1705348560.0","upvote_count":"3","poster":"datapassionate","comment_id":"1123620"},{"poster":"datapassionate","comments":[{"poster":"datapassionate","timestamp":"1705349160.0","content":"nother way to replay messages that have been acknowledged is to seek to a timestamp. To seek to a timestamp, you must first configure the subscription to retain acknowledged messages using retain-acked-messages. If retain-acked-messages is set, Pub/Sub retains acknowledged messages for 7 days.","comment_id":"1123626","upvote_count":"4"}],"comment_id":"1123624","timestamp":"1705349040.0","content":"This case is described here. \nhttps://cloud.google.com/pubsub/docs/replay-message\nAnd according to this D &E would be correct.","upvote_count":"5"}]},{"timestamp":"1703926440.0","comment_id":"1109556","content":"Selected Answer: B\nBE--> correct\nPub/Sub Snapshot: Captures a point-in-time snapshot of the messages in the subscription, ensuring that the previous two days of messages are available for reprocessing even after they've been acknowledged.\nRetain-Acked-Messages Flag: While this flag prevents acknowledged messages from being deleted, it's not sufficient on its own because it only retains messages going forward from when it's enabled.","upvote_count":"4","poster":"e70ea9e"}],"answer":"D"},{"id":"EUQE7aI1ooUKQWBVMUDq","question_text":"You currently use a SQL-based tool to visualize your data stored in BigQuery. The data visualizations require the use of outer joins and analytic functions. Visualizations must be based on data that is no less than 4 hours old. Business users are complaining that the visualizations are too slow to generate. You want to improve the performance of the visualization queries while minimizing the maintenance overhead of the data preparation pipeline. What should you do?","discussion":[{"content":"Selected Answer: A\nJust a note, the question saying \"data no less than 4 hours old\" presumably means \"no more than 4 hours old\"","upvote_count":"1","timestamp":"1728117240.0","poster":"baimus","comment_id":"1293399"},{"comments":[{"content":"Yes, they do if they are non-incremental.","upvote_count":"1","comment_id":"1330319","poster":"AWSandeep","timestamp":"1734858300.0"}],"content":"Selected Answer: B\nUnfortunately the correct answer is B due to the limitations of materialized views, doesn't support any other join than inner and no analytical function is supported","upvote_count":"2","poster":"JamesKarianis","timestamp":"1723586940.0","comment_id":"1265400"},{"timestamp":"1710361140.0","upvote_count":"3","poster":"ricardovazz","comment_id":"1172880","content":"Selected Answer: A\nA\n\nhttps://cloud.google.com/bigquery/docs/materialized-views-create#non-incremental \n\nIn scenarios where data staleness is acceptable, for example for batch data processing or reporting, non-incremental materialized views can improve query performance and reduce cost.\n\nallow_non_incremental_definition option. This option must be accompanied by the max_staleness option. To ensure a periodic refresh of the materialized view, you should also configure a refresh policy."},{"comment_id":"1121543","poster":"Matt_108","content":"Selected Answer: A\nOption A is better than D, since it accounts for data staleness and is better suited for heavy querying, thanks to the allow_non_incremental_definition","timestamp":"1705144440.0","upvote_count":"2"},{"poster":"Jordan18","timestamp":"1704494220.0","upvote_count":"4","comment_id":"1114827","comments":[{"upvote_count":"2","poster":"datapassionate","content":"Seems like materialiazed views can use incremental updates only if data was not delated or updated in original table. Here the data changes so I think thats the reason why its not correct answer\nhttps://cloud.google.com/bigquery/docs/materialized-views-use#incremental_updates\n\"BigQuery combines the cached view's data with new data to provide consistent query results while still using the materialized view. For single-table materialized views, this is possible if the base table is unchanged since the last refresh, or if only new data was added. For multi-table views, no more than one table can have appended data. If more than one of a multi-table view's base tables has changed, then the view cannot be incrementally updated.\"","comment_id":"1123648","timestamp":"1705350540.0"}],"content":"A seems right but whats wrong with option D, can anybody please explain?"},{"upvote_count":"3","poster":"raaad","content":"Selected Answer: A\n- Materialized views in BigQuery precompute and store the result of a base query, which can speed up data retrieval for complex queries used in visualizations. \n- The max_staleness parameter allows us to specify how old the data can be, ensuring that the visualizations are based on data no less than 4 hours old. \n- The enable_refresh parameter ensures that the materialized view is periodically refreshed.\n- The allow_non_incremental_definition is used for enabling the creation of non-incrementally refreshable materialized views.","comment_id":"1113839","timestamp":"1704384000.0"},{"upvote_count":"3","comment_id":"1109557","poster":"e70ea9e","content":"Selected Answer: A\nPrecomputed Results: Materialized views store precomputed results of complex queries, significantly accelerating subsequent query performance, addressing the slow visualization issue.\nAllow Non-Incremental Views: Using allow_non_incremental_definition circumvents the limitation of incremental updates for outer joins and analytic functions, ensuring views can be created for the specified queries.\nNear-Real-Time Data: Setting max_staleness to 4 hours guarantees data freshness within the acceptable latency for visualizations.\nAutomatic Refresh: Enabling refresh with enable_refresh maintains view consistency with minimal maintenance overhead.\nMinimal Overhead: Materialized views automatically update as underlying data changes, reducing maintenance compared to manual exports or view definitions.","timestamp":"1703926500.0"}],"exam_id":11,"url":"https://www.examtopics.com/discussions/google/view/129876-exam-professional-data-engineer-topic-1-question-229/","answers_community":["A (86%)","14%"],"isMC":true,"answer_images":[],"topic":"1","answer":"A","answer_ET":"A","timestamp":"2023-12-30 09:55:00","choices":{"A":"Create materialized views with the allow_non_incremental_definition option set to true for the visualization queries. Specify the max_staleness parameter to 4 hours and the enable_refresh parameter to true. Reference the materialized views in the data visualization tool.","C":"Create a Cloud Function instance to export the visualization query results as parquet files to a Cloud Storage bucket. Use Cloud Scheduler to trigger the Cloud Function every 4 hours. Reference the parquet files in the data visualization tool.","B":"Create views for the visualization queries. Reference the views in the data visualization tool.","D":"Create materialized views for the visualization queries. Use the incremental updates capability of BigQuery materialized views to handle changed data automatically. Reference the materialized views in the data visualization tool."},"answer_description":"","question_id":145,"unix_timestamp":1703926500,"question_images":[]}],"exam":{"isMCOnly":true,"numberOfQuestions":319,"id":11,"provider":"Google","isImplemented":true,"lastUpdated":"11 Apr 2025","name":"Professional Data Engineer","isBeta":false},"currentPage":29},"__N_SSP":true}