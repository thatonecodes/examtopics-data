{"pageProps":{"questions":[{"id":"95sz7PzCrF8rd2cKzQZE","topic":"1","exam_id":13,"answer_ET":"C","answers_community":["C (74%)","13%","13%"],"isMC":true,"answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/99050-exam-professional-machine-learning-engineer-topic-1-question/","discussion":[{"upvote_count":"11","poster":"John_Pongthorn","timestamp":"1694090700.0","comment_id":"832020","content":"Selected Answer: C\nCustom trainer , don't overthink 1000%, this is google recommendation.\nyou don't need Vertex AI Workbench user-managed notebooks,Google Kubernetes Engine, Compute Engine at at all , it is a waste of your effort\nhttps://cloud.google.com/vertex-ai/docs/training/configure-compute#specifying_gpus\nYou can choose as your want"},{"comment_id":"1202125","upvote_count":"1","poster":"pawan94","timestamp":"1729871760.0","content":"Why in the world would you setup a Compute engine VM, when your custom training job on vertex runs \"serverless\" atleast for the user side don't have to maintain the vm. You literally just have to select the region , machine type and accelerators that's all."},{"upvote_count":"3","timestamp":"1729584840.0","comment_id":"1200037","content":"Selected Answer: C\nPre-built container: Utilizing a pre-built PyTorch container image eliminates the need to manage dependencies within your container, saving time and simplifying the process.\nVertex AI custom tier: Vertex AI custom tiers allow you to configure a machine type with the desired GPUs (4 V100 in this case) and pay only for the resources you use. This is more cost-effective than managing a dedicated VM instance.\nSetuptools packaging: Packaging your code with tools like Setuptools ensures all necessary libraries and scripts are included within the container, creating a self-contained training environment.","poster":"fitri001"},{"timestamp":"1715695800.0","content":"Selected Answer: C\nUsing Vertex AI allows you to easily leverage multiple GPUs without managing infrastructure yourself. The custom tier gives you control to specify 4 V100 GPUs.\nPackaging with Setuptools and using a pre-built container ensures a consistent and portable environment with all dependencies readily available.\nThis approach minimizes overhead and cost by relying on Vertex AI's managed service instead of setting up your own Kubernetes cluster or VMs.","comment_id":"1070613","poster":"Mickey321","upvote_count":"1"},{"upvote_count":"1","comment_id":"957331","timestamp":"1705748340.0","content":"Correct Ans is C. Below mentioned why B is incorrect.","poster":"PST21"},{"content":"Selected Answer: B\nOption B (using a Vertex AI Workbench user-managed notebooks instance with 4 V100 GPUs) is more suitable for interactive data exploration and experimentation rather than large-scale model training. Vertex AI Workbench is designed for collaborative data science, but using it for model training might not be the most efficient approach.","comment_id":"957330","upvote_count":"1","timestamp":"1705748280.0","poster":"PST21"},{"timestamp":"1700641260.0","content":"What is Supetools?","comments":[{"upvote_count":"1","timestamp":"1700903400.0","comment_id":"906466","content":"Found it. Python package that provides a mechanism for packaging, distributing, and installing Python libraries or modules.","poster":"julliet"}],"comment_id":"903767","poster":"julliet","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: C\n“Vertex AI provides flexible and scalable hardware and secured infrastructure to train PyTorch based deep learning models with pre-built containers and custom containers. (…) use PyTorch ResNet-50 as the example model and train it on ImageNet validation data (50K images) to measure the training performance for different training strategies”: https://cloud.google.com/blog/products/ai-machine-learning/efficient-pytorch-training-with-vertex-ai","timestamp":"1699638840.0","poster":"M25","comment_id":"894140","comments":[{"poster":"M25","content":"There is no indication otherwise why there would be a need for full control over the environment, provided by “user-managed workbooks” within the Vertex AI Workbench [Option B], except for the “plan to use 4 V100 GPUs”, but one can do that with “managed workbooks” as well:\nhttps://cloud.google.com/vertex-ai/docs/workbench/notebook-solution#control_your_hardware_and_framework_from_jupyterlab","timestamp":"1699638900.0","upvote_count":"1","comment_id":"894141"}]},{"comments":[{"comment_id":"1355660","content":"Use a Vertex AI Workbench notebook with 4 V100 GPUs ❌\n\nNotebooks are better suited for experimentation, not large-scale training.\nScaling beyond a single node is harder.\nLess cost-efficient than submitting a dedicated training job.","poster":"tk786786","upvote_count":"1","timestamp":"1739373900.0"},{"poster":"andresvelasco","upvote_count":"1","content":"Very likely because of the consideration: \n\"You want to quickly scale your training workload while minimizing cost\"\nBut I agfree with you ... I chose B (notebook) thinking the question was more oriented to haquickly achieving an MVP.","timestamp":"1710164160.0","comment_id":"1004705"}],"content":"Can someone explain why is B wrong?","upvote_count":"2","poster":"frangm23","comment_id":"878709","timestamp":"1698084060.0"},{"upvote_count":"1","content":"Selected Answer: C\nAnswer C\nOption A involves using Google Kubernetes Engine, which is a platform for deploying, managing, and scaling containerized applications. However, it requires more setup time and knowledge of Kubernetes, which might not be ideal for quickly scaling up training workloads. Furthermore, the use of the TensorFlow Job operator seems inappropriate for a PyTorch-based model.","poster":"TNT87","timestamp":"1697608980.0","comment_id":"873311"},{"timestamp":"1695628860.0","comments":[{"content":"The TFJob operator is designed for TensorFlow workloads, not PyTorch. So option A is out.\nVertex AI Workbench is primarily designed for interactive work with Jupyter Notebooks and not optimized for large-scale, long-running model training. Moreover, it may not provide the same level of cost optimization as Vertex AI Training, which automatically provisions and manages resources, and can scale down when not in use. So option B also out.","timestamp":"1695628980.0","comment_id":"849990","poster":"wlts","upvote_count":"1"}],"comment_id":"849985","content":"Select C","poster":"wlts","upvote_count":"1"},{"upvote_count":"1","poster":"TNT87","content":"C. Package your code with Setuptools, and use a pre-built container. Train your model with Vertex AI using a custom tier that contains the required GPUs.\n\nThe recommended approach to scale the training workload while minimizing cost would be to package the code with Setuptools and use a pre-built container, then train the model with Vertex AI using a custom tier that contains the required GPUs. This approach allows for quick and easy scaling of the training workload while minimizing infrastructure management costs.","timestamp":"1694080800.0","comment_id":"831831"},{"content":"Selected Answer: A\nVote for A, as you need to scale","timestamp":"1692812160.0","comments":[{"upvote_count":"2","timestamp":"1694067600.0","poster":"alelamb","comment_id":"831659","content":"It clearly says a Pytorch model, you cannot use a TFjob"}],"poster":"FherRO","upvote_count":"1","comment_id":"819638"},{"upvote_count":"2","content":"Selected Answer: B\nIt's B according to me, since VertexAI Notebook has alla dependencies for PyTorch that is the fastest solution","comments":[{"timestamp":"1706379240.0","upvote_count":"1","content":"It involves using a managed notebooks instance, which might have limitations in terms of customizability and flexibility compared to a containerized approach.","comment_id":"964885","poster":"tavva_prudhvi"}],"poster":"Scipione_","timestamp":"1692192840.0","comment_id":"810834"},{"poster":"TNT87","timestamp":"1691914440.0","comment_id":"807279","content":"Selected Answer: A\nGoogle Kubernetes Engine (GKE) is a powerful and easy-to-use platform for deploying and managing containerized applications. It allows you to create a cluster of virtual machines that are pre-configured with the necessary dependencies and resources to run your machine learning workloads. By creating a GKE cluster with a node pool that has 4 V100 GPUs, you can take advantage of the powerful processing capabilities of these GPUs to train your model quickly and efficiently.\n\nYou can then use the Kubernetes Framework such as TFJob operator to submit the job of training your model, which will automatically distribute the workload across the available GPUs.\n\nReferences:\n\nGoogle Kubernetes Engine\nTFJob operator\nVertex Al","comments":[{"poster":"TNT87","timestamp":"1694080860.0","comment_id":"831833","content":"Answer C","upvote_count":"2"},{"comment_id":"831660","upvote_count":"1","content":"It clearly says a Pytorch model, you cannot use a TFjob","timestamp":"1694067660.0","poster":"alelamb"}],"upvote_count":"2"}],"choices":{"B":"Create a Vertex AI Workbench user-managed notebooks instance with 4 V100 GPUs, and use it to train your model.","C":"Package your code with Setuptools, and use a pre-built container. Train your model with Vertex AI using a custom tier that contains the required GPUs.","D":"Configure a Compute Engine VM with all the dependencies that launches the training. Train your model with Vertex AI using a custom tier that contains the required GPUs.","A":"Create a Google Kubernetes Engine cluster with a node pool that has 4 V100 GPUs. Prepare and submit a TFJob operator to this node pool."},"question_id":51,"question_text":"You are developing an image recognition model using PyTorch based on ResNet50 architecture. Your code is working fine on your local laptop on a small subsample. Your full dataset has 200k labeled images. You want to quickly scale your training workload while minimizing cost. You plan to use 4 V100 GPUs. What should you do?","unix_timestamp":1676283240,"timestamp":"2023-02-13 11:14:00","answer_description":"","question_images":[],"answer":"C"},{"id":"QrSxtBMc1z5uktuzawbe","question_id":52,"question_images":["https://img.examtopics.com/professional-machine-learning-engineer/image2.png"],"question_text":"You have trained a DNN regressor with TensorFlow to predict housing prices using a set of predictive features. Your default precision is tf.float64, and you use a standard TensorFlow estimator:\n\n//IMG//\n\n\nYour model performs well, but just before deploying it to production, you discover that your current serving latency is 10ms @ 90 percentile and you currently serve on CPUs. Your production requirements expect a model latency of 8ms @ 90 percentile. You're willing to accept a small decrease in performance in order to reach the latency requirement.\nTherefore your plan is to improve latency while evaluating how much the model's prediction decreases. What should you first try to quickly lower the serving latency?","isMC":true,"answer_images":[],"answer_ET":"B","answers_community":["B (69%)","A (31%)"],"exam_id":13,"unix_timestamp":1675483320,"topic":"1","url":"https://www.examtopics.com/discussions/google/view/97893-exam-professional-machine-learning-engineer-topic-1-question/","answer_description":"","timestamp":"2023-02-04 05:02:00","discussion":[{"content":"Selected Answer: A\nB is wrong because tf.float16 quantization requires GPU accelerators; this is CPU only\n\nA isn't a great answer but it's the next best choice https://ai.google.dev/edge/litert/models/post_training_quantization#float16_quantization","upvote_count":"1","timestamp":"1735421760.0","comment_id":"1333178","poster":"0e6b9e2"},{"upvote_count":"2","poster":"baimus","content":"Selected Answer: B\nI know the answer is B becaue the question is telegraphing it so much: \"You can lower quality a bit\" (waggles eyebrows) - that obviously means quantizing (the other changes are silly). But in reality A would be much more normal thing to do. It's unusual to even attempt serving an NN on CPU these days.","timestamp":"1725974280.0","comment_id":"1281560"},{"poster":"fitri001","upvote_count":"4","content":"Selected Answer: B\nReduced model size: Quantization reduces the model size by using lower precision data types like tf.float16 instead of the default tf.float64. This smaller size leads to faster loading and processing during inference.\nMinimal performance impact: Quantization often introduces a small decrease in model accuracy, but it's a good initial step to explore due to the potential latency gains with minimal performance trade-offs.","comment_id":"1200039","timestamp":"1713773880.0"},{"content":"Selected Answer: B\nI went with B.","comment_id":"1195905","poster":"gscharly","timestamp":"1713169860.0","upvote_count":"1"},{"upvote_count":"1","poster":"Carlose2108","timestamp":"1709161320.0","content":"Selected Answer: B\nI went with B.","comment_id":"1162034"},{"comment_id":"1103565","poster":"Tayoso","upvote_count":"4","content":"Selected Answer: B\nSwitching from CPU to GPU serving could also improve latency, but it may not be considered a \"quick\" solution compared to model quantization because it involves additional hardware requirements and potentially more complex deployment changes. Additionally, not all models see a latency improvement on GPUs, especially if the model is not large enough to utilize the GPU effectively or if the infrastructure does not support GPU optimizations.\nTherefore, the first thing to try would be quantization, which can be done relatively quickly and directly within the TensorFlow framework. After applying quantization, you should evaluate the model to ensure that the decrease in precision does not lead to an unacceptable drop in prediction accuracy.","timestamp":"1703265780.0"},{"comments":[{"upvote_count":"2","content":"Changed to B","comment_id":"1072548","timestamp":"1700149320.0","poster":"Mickey321"}],"comment_id":"1070633","content":"Selected Answer: A\nVery confusing A or B but leaning to A","poster":"Mickey321","timestamp":"1699978920.0","upvote_count":"1"},{"content":"Selected Answer: B\nB based on the consideration: \"Therefore your plan is to improve latency while evaluating how much the model's prediction decreases\"","comment_id":"1005329","poster":"andresvelasco","upvote_count":"2","timestamp":"1694488800.0"},{"comment_id":"918366","comments":[{"timestamp":"1686622200.0","upvote_count":"2","content":"according to the documentation we have to convert to TensorFlowLite before applying quantization or use an API https://www.tensorflow.org/model_optimization/guide/quantization/training\ndoesn't look to be first option. Second maybe?","comment_id":"921873","poster":"julliet"}],"content":"Selected Answer: B\nTo me is \nB. Apply quantization to your SavedModel by reducing the floating point precision to tf.float16.\nObviously that switthing to GPU improve latency BUT.... it says \"Therefore your plan is to improve latency while evaluating how much the model's prediction decreases.\" If you want evaluate how much decrease is because you are going to make changes that affect the prediction","poster":"Voyager2","timestamp":"1686235140.0","upvote_count":"4"},{"poster":"Voyager2","upvote_count":"1","timestamp":"1685269680.0","comment_id":"908531","content":"Selected Answer: A\nGoing with A:\nMy reason to discard B: from https://www.tensorflow.org/lite/performance/post_training_quantization#float16_quantization\nThe advantages of float16 quantization are as follows:\n\nIt reduces model size by up to half (since all weights become half of their original size).\nIt causes minimal loss in accuracy.\nIt supports some delegates (e.g. the GPU delegate) which can operate directly on float16 data, resulting in faster execution than float32 computations.\nThe disadvantages of float16 quantization are as follows:\n\nIt does not reduce latency as much as a quantization to fixed point math.\nBy default, a float16 quantized model will \"dequantize\" the weights values to float32 when run on the CPU. (Note that the GPU delegate will not perform this dequantization, since it can operate on float16 data.)"},{"poster":"aryaavinash","timestamp":"1684235400.0","content":"Going with B because quantization can reduce the model size and inference latency by using lower-precision arithmetic operations, while maintaining acceptable accuracy. The other options are either not feasible or not effective for lowering the serving latency. Switching from CPU to GPU serving may not be possible or cost-effective, increasing the dropout rate may degrade the model performance significantly, and dropout is not applied in _PREDICT mode by default.","comment_id":"899098","upvote_count":"4"},{"upvote_count":"2","timestamp":"1683734160.0","comment_id":"894142","content":"Selected Answer: A\nFor tf.float16 [Option B], we would have to be on TFLite:\nhttps://discuss.tensorflow.org/t/convert-tensorflow-saved-model-from-float32-to-float16/12130 and resp.\nhttps://www.tensorflow.org/lite/performance/post_training_quantization#float16_quantization (plus “By default, a float16 quantized model will \"dequantize\" the weights values to float32 when run on the CPU. (Note that the GPU delegate will not perform this dequantization, since it can operate on float16 data.)”","comments":[{"timestamp":"1683734160.0","upvote_count":"1","comments":[{"upvote_count":"1","comment_id":"894144","timestamp":"1683734160.0","poster":"M25","content":"But then, “On CPUs, mixed precision will run significantly slower, however.”: https://www.tensorflow.org/guide/mixed_precision#supported_hardware.\nAnd, “The policy will run on other GPUs and CPUs but may not improve performance.”: https://www.tensorflow.org/guide/mixed_precision#setting_the_dtype_policy."}],"poster":"M25","content":"But even before that, tf.estimator.DNNRegressor is deprecated, “Use tf.keras instead”: https://www.tensorflow.org/api_docs/python/tf/estimator/DNNRegressor. \nWhen used with Keras (a high-level NN library that runs on top of TF), for training though, “It is not recommended to set this to float16 for training, as this will likely cause numeric stability issues. Instead, mixed precision, which is using a mix of float16 and float32, can be used”: https://www.tensorflow.org/api_docs/python/tf/keras/backend/set_floatx.","comment_id":"894143"},{"poster":"M25","upvote_count":"1","comment_id":"894147","content":"“This can take around 500ms to process a single Tweet (of at most 128 tokens) on a CPU-based machine. The processing time can be greatly reduced to 20ms by running the model on a GPU instance (…). An option to dynamically quantize a TensorFlow model wasn’t available, so we updated the script to convert the TensorFlow models into TFLite and created the options to apply int8 or fp16 quantization.”: https://blog.twitter.com/engineering/en_us/topics/insights/2021/speeding-up-transformer-cpu-inference-in-google-cloud","timestamp":"1683734220.0"}],"poster":"M25"},{"comment_id":"879292","content":"Selected Answer: A\nA and B both work well here, but I prefer A since B would imply some minor tradeoff between latency and model accuracy, which isn't the case for A. So I would consider quantization after switching to GPU serving. Can anyone explain why B might be better than A?","timestamp":"1682337900.0","poster":"[Removed]","upvote_count":"1","comments":[{"content":"Since you're allowing a small decrease in accuracy, you should choose B as it is more cost effective than A.","timestamp":"1682408820.0","comments":[{"upvote_count":"2","poster":"[Removed]","comment_id":"898470","timestamp":"1684166220.0","content":"There's no mention that we're cost sensitive in this scenario. Why should we assume that reducing cost is more important than model accuracy?","comments":[{"comment_id":"964034","timestamp":"1690392360.0","upvote_count":"2","poster":"tavva_prudhvi","content":"Yes you're right. But, Quantization reduces the floating point precision, which can result in a smaller model size and lower memory footprint. This can lead to faster serving times and improved latency. In comparison, switching to GPU serving doesn't necessarily reduce the model size or memory footprint.Also, it provides a more direct way to balance the trade-off between model performance and latency, as it directly impacts the model's precision. Switching to GPU serving may improve latency but doesn't directly address the trade-off between performance and latency. While Switching to GPU serving may require changes to your existing serving infrastructure, which can be time-consuming and may not be compatible with your current setup. Quantization, on the other hand, is a model optimization technique that can be applied directly to the model without requiring changes to the serving infrastructure."}]}],"poster":"frangm23","comment_id":"880091","upvote_count":"3"}]},{"upvote_count":"4","comments":[{"comment_id":"834277","timestamp":"1678387680.0","poster":"TNT87","content":"But answer is B , i dnt know how i clicked A","upvote_count":"2"},{"content":"GPU serving can significantly speed up the serving of models due to the parallel processing power of GPUs. By switching from CPU to GPU serving, you can quickly lower the serving latency without making changes to the model architecture or precision. Once you have switched to GPU serving, you can evaluate the impact on the model's prediction quality and consider further optimization techniques if necessary. Therefore, the correct option is A.","comment_id":"831853","timestamp":"1678191420.0","upvote_count":"2","poster":"TNT87"}],"comment_id":"831852","poster":"TNT87","content":"Selected Answer: A\nA makes sense too","timestamp":"1678191360.0"},{"comment_id":"807274","upvote_count":"1","poster":"TNT87","content":"Answer B\nApplying quantization to your SavedModel by reducing the floating point precision can help reduce the serving latency by decreasing the amount of memory and computation required to make a prediction. TensorFlow provides tools such as the tf.quantization module that can be used to quantize models and reduce their precision, which can significantly reduce serving latency without a significant decrease in model performance","timestamp":"1676282940.0"},{"timestamp":"1675483320.0","content":"Selected Answer: B\nVote B. https://www.tensorflow.org/lite/performance/post_training_float16_quant","poster":"imamapri","upvote_count":"4","comment_id":"797593"}],"choices":{"D":"Increase the dropout rate to 0.8 in _PREDICT mode by adjusting the TensorFlow Serving parameters.","C":"Increase the dropout rate to 0.8 and retrain your model.","B":"Apply quantization to your SavedModel by reducing the floating point precision to tf.float16.","A":"Switch from CPU to GPU serving."},"answer":"B"},{"id":"xt6WWw24PV71nARt9YWx","discussion":[{"poster":"gscharly","timestamp":"1728981180.0","comment_id":"1195906","upvote_count":"2","content":"Selected Answer: C\nwent with c"},{"comment_id":"1179021","timestamp":"1726894860.0","content":"Selected Answer: C\nwent with c","poster":"Aastha_Vashist","upvote_count":"2"},{"comment_id":"1069181","content":"Selected Answer: C\nOption D is not as efficient because using Google Data Studio for time plots may not be as well-suited for handling large datasets, and it's more focused on data visualization. Option A involves importing data into Vertex AI Workbench first, which may not be the most efficient way to leverage BigQuery for handling large-scale data computations.","upvote_count":"3","timestamp":"1715585100.0","poster":"pico"},{"content":"Selected Answer: D\nI would go with D, thinking that Bigquery + Datastudio avoid having to load 100s of MILLIONS of records in memory for the most basic tasks, as required by the Notebook.","timestamp":"1710221760.0","poster":"andresvelasco","comment_id":"1005335","upvote_count":"4"},{"comment_id":"930856","upvote_count":"3","content":"Selected Answer: D\nD minimizes resources the most, since it minimizes the usage of Vertex AI notebooks, which basically require provisioning a VM in the background for the entire duration of development.","timestamp":"1703274120.0","poster":"friedi"},{"content":"Why not D ?","comments":[{"poster":"tavva_prudhvi","content":"Using BigQuery to calculate the descriptive statistics is a good choice, but using Google Data Studio for visualizations may not be as flexible as using Vertex AI Workbench user-managed notebooks. Google Data Studio is a great tool for creating dashboards and reports, but it may not allow for the level of customization that is required for detailed exploratory data analysis.","comment_id":"944852","upvote_count":"1","timestamp":"1704566700.0"}],"comment_id":"923513","upvote_count":"4","poster":"bechir141bf","timestamp":"1702588620.0"},{"comment_id":"894148","timestamp":"1699639020.0","upvote_count":"1","poster":"M25","content":"Selected Answer: C\nhttps://cloud.google.com/architecture/data-science-with-r-on-gcp-eda#ai_platform_notebooks\nhttps://cloud.google.com/vertex-ai-workbench#section-5"},{"timestamp":"1698152340.0","content":"Selected Answer: C\nI think the key here is that it says the dataset would be imported into the notebook for B, therefore no longer utilising BigQuery for calculating the descriptive stats, otherwise I would pick B. Therefore I think C is better. Can anyone find any documentation where Google gives best practice on this? It seems quite subjective","comment_id":"879352","upvote_count":"1","poster":"[Removed]"},{"comments":[{"comment_id":"882854","poster":"Gudwin","content":"What is the point of actually giving chatGPT answers, some of which are incorrect?","timestamp":"1698426120.0","upvote_count":"7"}],"content":"Selected Answer: C\nC. Use BigQuery to calculate the descriptive statistics. Use Vertex AI Workbench user-managed notebooks to visualize the time plots and run the statistical analyses.\n\nBigQuery is a powerful data analysis tool that can handle massive datasets, making it an ideal solution for calculating descriptive statistics for hundreds of millions of records. It can also perform complex statistical tests for hypothesis testing. For time series analysis, using Vertex AI Workbench user-managed notebooks would be the best solution as it provides a flexible environment for data exploration, visualization, and statistical analysis. By using the two tools together, the data science team can efficiently analyze the sales data while minimizing computational resources.\nIts C not B","comment_id":"831849","timestamp":"1694081520.0","poster":"TNT87","upvote_count":"3"},{"upvote_count":"1","comments":[{"upvote_count":"1","comment_id":"834274","content":"Answer C not B","poster":"TNT87","timestamp":"1694278020.0"}],"timestamp":"1694072280.0","content":"Answer B\nhttps://cloud.google.com/vertex-ai-workbench","poster":"TNT87","comment_id":"831708"},{"comments":[{"upvote_count":"1","timestamp":"1704566580.0","comment_id":"944850","poster":"tavva_prudhvi","content":"Option B is also a viable solution, but it has some drawbacks compared to option C. While it is true that you can spin up a Vertex AI Workbench user-managed notebooks instance and import the dataset, this option requires more computational resources and may not be as cost-effective as using BigQuery to calculate the descriptive statistics. Additionally, while you can create statistical and visual analyses within the Vertex AI Workbench user-managed notebooks, it may not be as easy to create custom visualizations as it is with Google Data Studio.\n\nTherefore, while option B is a valid solution, option C is likely to be more efficient and cost-effective, as it takes advantage of the strengths of each tool."}],"comment_id":"797596","poster":"imamapri","content":"Selected Answer: B\nVote B. You can do all of the task in vertex AI workbench while minimizing computational resources.","timestamp":"1691114760.0","upvote_count":"3"}],"choices":{"D":"Use BigQuery to calculate the descriptive statistics, and use Google Data Studio to visualize the time plots. Use Vertex Al Workbench user-managed notebooks to run the statistical analyses.","C":"Use BigQuery to calculate the descriptive statistics. Use Vertex Al Workbench user-managed notebooks to visualize the time plots and run the statistical analyses.","B":"Spin up a Vertex Al Workbench user-managed notebooks instance and import the dataset. Use this data to create statistical and visual analyses.","A":"Visualize the time plots in Google Data Studio. Import the dataset into Vertex Al Workbench user-managed notebooks. Use this data to calculate the descriptive statistics and run the statistical analyses."},"answer_images":[],"question_text":"You work on the data science team at a manufacturing company. You are reviewing the company’s historical sales data, which has hundreds of millions of records. For your exploratory data analysis, you need to calculate descriptive statistics such as mean, median, and mode; conduct complex statistical tests for hypothesis testing; and plot variations of the features over time. You want to use as much of the sales data as possible in your analyses while minimizing computational resources. What should you do?","unix_timestamp":1675483560,"topic":"1","answer_ET":"C","question_id":53,"url":"https://www.examtopics.com/discussions/google/view/97895-exam-professional-machine-learning-engineer-topic-1-question/","answer":"C","answer_description":"","exam_id":13,"isMC":true,"timestamp":"2023-02-04 05:06:00","question_images":[],"answers_community":["C (55%)","D (32%)","14%"]},{"id":"069IbPZYyi3wgSBnNTgt","question_text":"Your data science team needs to rapidly experiment with various features, model architectures, and hyperparameters. They need to track the accuracy metrics for various experiments and use an API to query the metrics over time. What should they use to track and report their experiments while minimizing manual effort?","question_images":[],"answers_community":["A (100%)"],"exam_id":13,"discussion":[{"content":"Selected Answer: A\nVertex AI Pipelines: Pipelines are designed for automating experiment execution. You can define different steps like data preprocessing, training with various configurations, and evaluation. This allows rapid experimentation with minimal manual intervention.\nVertex ML Metadata: Vertex AI Pipelines integrate seamlessly with Vertex ML Metadata, which automatically tracks experiment runs, metrics, and artifacts. This eliminates the need for manual data collection in spreadsheets.\nVertex AI API: The Vertex AI API allows you to programmatically query the Vertex ML Metadata store. You can retrieve experiment details, including accuracy metrics, for further analysis or visualization.","upvote_count":"3","poster":"fitri001","comment_id":"1200411","timestamp":"1729641180.0","comments":[{"timestamp":"1729641180.0","content":"B. BigQuery and Monitoring are not designed for experiment tracking: BigQuery excels at large-scale data analysis, and Cloud Monitoring is primarily for monitoring system health. While you could store metrics, querying them for experiment comparisons would be cumbersome.\nC. Manual collection in Google Sheets: This approach is highly error-prone and inefficient for rapid experimentation. Version control and querying metrics across multiple experiments would be challenging.\nD. Vertex AI Training (standalone): While Vertex AI Training can run experiments, it lacks built-in experiment tracking and querying functionalities.\n You'd need to develop custom solutions for managing metrics.","poster":"fitri001","comment_id":"1200412","upvote_count":"1"}]},{"poster":"M25","comment_id":"894149","timestamp":"1699639080.0","upvote_count":"2","content":"Selected Answer: A\nWent with A"},{"upvote_count":"1","content":"Selected Answer: A\nOption A is the best approach to track and report experiments while minimizing manual effort. The Vertex AI Pipelines provide a powerful tool for automating machine learning workflows, including data preparation, training, and deployment. MetadataStore can be used to track the performance of different models by logging accuracy metrics and other important information. The Vertex AI API can then be used to query the metadata store and retrieve the results of different experiments.","poster":"TNT87","comment_id":"834272","timestamp":"1694277960.0"},{"content":"Selected Answer: A\nVertex AI Pipelines covers everything.\n\n\"Vertex AI Pipelines helps you to automate, monitor, and govern your ML systems by orchestrating your ML workflow in a serverless manner, and storing your workflow's artifacts using Vertex ML Metadata. By storing the artifacts of your ML workflow in Vertex ML Metadata, you can analyze the lineage of your workflow's artifacts — for example, an ML model's lineage may include the training data, hyperparameters, and code that were used to create the model.\"","comment_id":"822779","upvote_count":"1","poster":"chidstar","timestamp":"1693065960.0"},{"upvote_count":"1","timestamp":"1692679080.0","content":"https://cloud.google.com/vertex-ai/docs/ml-metadata/analyzing","poster":"TNT87","comment_id":"817536"},{"timestamp":"1692175980.0","upvote_count":"1","poster":"Scipione_","content":"Selected Answer: A\nYour goal is to use API to query results while minimizing manual effort. The answer 'A' achieves the goal and requires less manual effort","comment_id":"810563"},{"poster":"RaghavAI","upvote_count":"1","timestamp":"1691239140.0","comment_id":"798901","content":"Selected Answer: A\nits A - Use Vertex Al Pipelines to execute the experiments. Query the results stored in MetadataStore using the Vertex Al API."}],"question_id":54,"answer_description":"","answer_ET":"A","answer":"A","url":"https://www.examtopics.com/discussions/google/view/98088-exam-professional-machine-learning-engineer-topic-1-question/","choices":{"A":"Use Vertex Al Pipelines to execute the experiments. Query the results stored in MetadataStore using the Vertex Al API.","D":"Use Vertex Al Workbench user-managed notebooks to execute the experiments. Collect the results in a shared Google Sheets file, and query the results using the Google Sheets API.","B":"Use Vertex Al Training to execute the experiments. Write the accuracy metrics to BigQuery, and query the results using the BigQuery API.","C":"Use Vertex Al Training to execute the experiments. Write the accuracy metrics to Cloud Monitoring, and query the results using the Monitoring API."},"isMC":true,"answer_images":[],"topic":"1","timestamp":"2023-02-05 15:39:00","unix_timestamp":1675607940},{"id":"vfLWHN4GfUy5sP34H7im","topic":"1","unix_timestamp":1675484040,"question_images":[],"url":"https://www.examtopics.com/discussions/google/view/97897-exam-professional-machine-learning-engineer-topic-1-question/","question_id":55,"exam_id":13,"answer_description":"","question_text":"You are training an ML model using data stored in BigQuery that contains several values that are considered Personally Identifiable Information (PII). You need to reduce the sensitivity of the dataset before training your model. Every column is critical to your model. How should you proceed?","timestamp":"2023-02-04 05:14:00","answers_community":["B (77%)","D (15%)","8%"],"answer_images":[],"answer_ET":"B","answer":"B","choices":{"D":"Before training, use BigQuery to select only the columns that do not contain sensitive data. Create an authorized view of the data so that sensitive values cannot be accessed by unauthorized individuals.","C":"Use the Cloud Data Loss Prevention (DLP) API to scan for sensitive data, and use Dataflow to replace all sensitive data by using the encryption algorithm AES-256 with a salt.","A":"Using Dataflow, ingest the columns with sensitive data from BigQuery, and then randomize the values in each sensitive column.","B":"Use the Cloud Data Loss Prevention (DLP) API to scan for sensitive data, and use Dataflow with the DLP API to encrypt sensitive values with Format Preserving Encryption."},"discussion":[{"poster":"AnnaR","content":"Selected Answer: B\nNot A: Randomizing values alters the data in a way that it can significantly degrade the utility of the data for machine learning purposes (does not preserve original distributions, ...)\nNot C: Encryption with AES-256 secures the data, but does not preserve the format and would make the data unusable for ML models \nNot D: This ignores columns with sensitive data, which is not viable here as every column is critical to the model. + Creating an authorized view does not alter the data itself but restricts access, which does not address the need to reduce data sensitivity for model training","timestamp":"1730123460.0","upvote_count":"4","comment_id":"1203547"},{"timestamp":"1699639080.0","comment_id":"894151","content":"Selected Answer: B\nhttps://cloud.google.com/dlp/docs/transformations-reference#types_of_de-identification_techniques\nhttps://cloud.google.com/dlp/docs/transformations-reference#crypto","poster":"M25","upvote_count":"1"},{"timestamp":"1692694020.0","comment_id":"817726","upvote_count":"1","poster":"TNT87","comments":[{"timestamp":"1694277900.0","poster":"TNT87","content":"model. The Cloud Data Loss Prevention (DLP) API can scan for sensitive data in the dataset and can help to encrypt the sensitive data using Format Preserving Encryption. This approach will allow for the preservation of the data distribution and format, enabling the model to maintain its accuracy. Additionally, using Dataflow with the DLP API can help to efficiently process the data at scale.","comment_id":"834271","upvote_count":"1"}],"content":"Selected Answer: B\nAnswer B"},{"upvote_count":"3","content":"Selected Answer: B\nFormat Preserving Encryption uses deidentify configuration in which you can specify the param wrapped_key (the encrypted ('wrapped') AES-256 key to use).\nAnswer is B according to me.\nRef: https://cloud.google.com/dlp/docs/samples/dlp-deidentify-fpe","comment_id":"810843","timestamp":"1692193200.0","poster":"Scipione_"},{"upvote_count":"2","poster":"TNT87","comments":[{"poster":"alelamb","comments":[{"upvote_count":"1","timestamp":"1692676200.0","comment_id":"817503","poster":"TNT87","content":"Hence i provided a link, that should answer your flimsy question. it says \"BigQuery that contains several values that are considered Personally Identifiable Information (PII)\" i dnt know where you are getting it wrong. the \"every\" means you cant leave out sensitive data to train your model because every column is critical. its not difficult its easy bro...."}],"upvote_count":"2","timestamp":"1692601980.0","content":"It says \"every\" column is critical to your model, why would select specific columns?","comment_id":"816387"}],"timestamp":"1692168420.0","content":"Selected Answer: D\nThis approach would allow you to keep the critical columns of data while reducing the sensitivity of the dataset by removing the personally identifiable information (PII) before training the model. By creating an authorized view of the data, you can ensure that sensitive values cannot be accessed by unauthorized individuals.\n\nhttps://cloud.google.com/bigquery/docs/data-governance#data_loss_prevention","comment_id":"810467"},{"poster":"RaghavAI","upvote_count":"1","content":"Selected Answer: B\nhttps://cloud.google.com/dlp/docs/samples/dlp-deidentify-fpe","timestamp":"1691238900.0","comment_id":"798896"},{"poster":"imamapri","comment_id":"797602","content":"Selected Answer: C\nVote C. https://cloud.google.com/dlp/docs/samples/dlp-deidentify-fpe","upvote_count":"1","timestamp":"1691115240.0"}],"isMC":true}],"exam":{"isBeta":false,"id":13,"lastUpdated":"11 Apr 2025","name":"Professional Machine Learning Engineer","isMCOnly":true,"provider":"Google","isImplemented":true,"numberOfQuestions":304},"currentPage":11},"__N_SSP":true}