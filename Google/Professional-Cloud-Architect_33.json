{"pageProps":{"questions":[{"id":"YwSQRwkB8QlNQ9UXJFBM","question_images":[],"answer_description":"","timestamp":"2021-12-28 08:36:00","choices":{"B":"Compute Engine","A":"Cloud Functions","C":"Google Kubernetes Engine","D":"AppEngine flexible environment"},"isMC":true,"topic":"1","url":"https://www.examtopics.com/discussions/google/view/68714-exam-professional-cloud-architect-topic-1-question-63/","unix_timestamp":1640676960,"discussion":[{"comment_id":"668413","poster":"abirroy","upvote_count":"29","comments":[{"timestamp":"1665939780.0","upvote_count":"3","poster":"AzureDP900","comment_id":"696408","content":"Agree with A"},{"content":"GKE is a managed service.","timestamp":"1708940880.0","upvote_count":"4","poster":"NiveusSol","comment_id":"1159566"}],"timestamp":"1663102200.0","content":"Selected Answer: A\nA. Cloud Functions - managed service scales down to 0\nB. Compute Engine - not a managed service\nC. Google Kubernetes Engine - not a managed service and wont scale down to 0\nD. AppEngine flexible environment - managed service but wont scale down to 0"},{"content":"A. Cloud Functions","upvote_count":"11","comments":[{"content":"Cloud function is more for event driven computing. We surely need k8s or app engine. Flex always have 1 instance running. So GKE should be the option","comments":[{"upvote_count":"1","poster":"YAS007","comment_id":"598176","timestamp":"1651937220.0","comments":[{"content":"But no cost for System/Control nodes","timestamp":"1661362920.0","upvote_count":"1","comment_id":"651391","poster":"6721sora"}],"content":"from the doc :https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler\n\nNote: If you specify a minimum of zero nodes, an idle node pool can scale down completely. However, at least one node must always be available in the cluster to run system Pods."}],"timestamp":"1644839400.0","comment_id":"547089","poster":"vpatiltech","upvote_count":"5"}],"poster":"victory108","timestamp":"1641076260.0","comment_id":"514729"},{"timestamp":"1740318000.0","comment_id":"1360514","poster":"halifax","upvote_count":"1","content":"Selected Answer: A\nThere is no correct answer, but I think, they meant to say cloud run (not cloud function). Cloud functions do not inherently scale to zero in the same way as Cloud Run."},{"content":"The only correct answer here is cloud run which isn't listed as an option.\n(A) can scale to zero, but is mean't for event driven workloads,not full sized applications, especially in the routing department\n(B) Needs manual stoppage, managed groups also keep one instance alive always\n(C) You're billed for the control plane even if the node pools are empty\n(D) Flex can't scale down to zero currently","upvote_count":"4","poster":"Toothpick","comment_id":"1255243","timestamp":"1721940300.0"},{"timestamp":"1698639300.0","upvote_count":"1","poster":"MiguelMiguel","content":"This question is confused. It's seems that the answer is cloud function because is the only platform that can scale 0 natively. If you want to scale to zero k8s, you have to create a solution based in scheduler, function and pub sub. Compute Engine is not a managed service, and app engine flex doesn't scale to 0. I would go to A but it's not clear.","comment_id":"1057339"},{"content":"agree A","comment_id":"896939","timestamp":"1683999120.0","poster":"LaxmanTiwari","upvote_count":"1"},{"poster":"grejao","timestamp":"1680427380.0","comments":[{"poster":"Bedmed","upvote_count":"4","comment_id":"882664","timestamp":"1682601540.0","content":"yes, but only Standard environment, not flexible environment"}],"comment_id":"858698","content":"Selected answer: D\nI choosed D, it appears that we do not. have a right answer here. \nA. Cloud Functions - its more for event driven computing, not for full application\nB. Compute Engine - not a managed service\nC. Google Kubernetes Engine - not a managed service and wont scale down to 0\nD. AppEngine flexible environment - Only Standard App Engine can scale to 0.","upvote_count":"3"},{"content":"For an application that is only used during business hours and needs to scale to zero during periods of inactivity to minimize costs, a good choice would be a Function-as-a-Service (FaaS) product like AWS Lambda or Google Cloud Functions.","poster":"GCPAnji","timestamp":"1679840820.0","upvote_count":"1","comment_id":"851146"},{"upvote_count":"1","comment_id":"845265","timestamp":"1679348220.0","comments":[{"timestamp":"1696820340.0","comments":[{"poster":"theBestStudent","timestamp":"1702520640.0","content":"That is not the concept of managed: https://cloud.google.com/blog/topics/developers-practitioners/serverless-vs-fully-managed-whats-difference","upvote_count":"1","comment_id":"1095960"}],"poster":"jrisl1991","upvote_count":"1","content":"App Engine is not an application on its own either, nor B or C options. B and C are not managed products; \"managed\" means that all of the infrastructure work (such as setting up an autoscheduler or autoscaler) will be managed by Google and not by the user. Using an IaaS solution for a managed use case is already contradictory. \n\nApp Engine Flex will always have at least 1 instance running. \n\nWhile Cloud Functions is not an application itself, it's the only resource that can scale to 0. Plus, the requirement states that they will have an application running during business hours, but they never mentioned that the app will only be running in GCP or in Cloud Functions. They could be running their app anywhere else, and only call Cloud Functions when needed.","comment_id":"1028434"}],"content":"Selected Answer: B\nA a cloud function is not an application\n\nB compute engine via MIG you can use an autoscaler with a schedule.\nhttps://cloud.google.com/compute/docs/autoscaler/scaling-schedules\nYou then can go from 0 to more instance when required\n\nC K8s is two complex for this. \nYou can have an autoscaler for the cluster in order to get the node number to 0, but it require the node to have no pods running. So you have to configure your deployments and all your workload to scale to 0 too.\nOther interference will be pod affinity, anti-affinity, disruption budget or unmanaged pod preventing pod eviction from node. But if the pod is not evicted, the node cannot be deleted.\n\"autoscaler respects scheduling and eviction rules set on Pods. These restrictions can prevent a node from being deleted by the autoscaler. \" => https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler#scheduling-and-disruption\n\nD flexible cannot scale to 0","poster":"jlambdan"},{"comment_id":"832077","poster":"CGS22","content":"Selected Answer: A\nA. Cloud Functions","timestamp":"1678204140.0","upvote_count":"2"},{"timestamp":"1673165400.0","comments":[{"content":"There are 2 mode for App engines, standard and flexible.\n\n\nThe standard environment can scale from zero instances up to thousands very quickly. In contrast, the flexible environment must have at least one instance running for each active version and can take longer to scale out in response to traffic.","timestamp":"1674777300.0","comment_id":"789178","poster":"CkWongCk","upvote_count":"4"}],"content":"Why not D? App Engines also can scale down to zero when there is no activity. https://cloud.google.com/appengine/docs/the-appengine-environments#:~:text=Intended%20to%20run%20for%20free,when%20there%20is%20no%20traffic. Intended to run for free or at very low cost, where you pay only for what you need and when you need it. For example, your application can scale to 0 instances when there is no traffic.","comment_id":"769182","upvote_count":"3","poster":"WFCheong"},{"content":"Selected Answer: A\nthe only to scale to 0 is A","upvote_count":"2","timestamp":"1671904260.0","poster":"thamaster","comment_id":"755062"},{"timestamp":"1671631080.0","comment_id":"752398","content":"The correct answer is A. Cloud Functions.\n\nCloud Functions is a serverless compute service that lets you run code without provisioning or managing infrastructure. One of the key benefits of using Cloud Functions is that it automatically scales to meet the demands of your workload and automatically scales down to zero when there is no activity. This means that you only pay for the compute resources that you consume, which can help to reduce costs when your application is not in use. Additionally, Cloud Functions is easy to use and allows you to deploy your code with minimal effort, making it a good choice for a minimum viable product release.","upvote_count":"3","poster":"omermahgoub"},{"upvote_count":"2","timestamp":"1660040400.0","comment_id":"644446","poster":"backhand","content":"vote A\nthis is easy one. key word: managed product, scales to zero\nscale to zero: app engine standard, cloud function"},{"content":"Selected Answer: B\nAnswer is B","poster":"vijbabu","upvote_count":"1","timestamp":"1657611180.0","comment_id":"630363"},{"upvote_count":"3","poster":"Dhiraj03","timestamp":"1655540340.0","content":"Selected Answer: A\nCloud Functions can scale to zero when not in use","comment_id":"618138"},{"upvote_count":"4","poster":"Sskhan","timestamp":"1642629180.0","content":"Selected Answer: A\nAnswer is A, As Option D App engine flexible can have minimum 1 instance active.","comment_id":"527956"},{"timestamp":"1642629120.0","upvote_count":"1","content":"Answer is A, As Option D App engine flexible can have minimum 1 instance active.","comment_id":"527955","poster":"Sskhan"},{"timestamp":"1641219840.0","upvote_count":"3","poster":"elenamatay","comment_id":"515829","comments":[{"upvote_count":"3","content":"\"However, at least one node must always be available in the cluster to run system Pods\"","comments":[{"poster":"meokey","content":"\"You are not charged for system Pods\" - so I guess it still fulfills the requirement \"you don't incur costs when there is no activity\"\nhttps://cloud.google.com/kubernetes-engine/pricing","timestamp":"1650046020.0","comment_id":"586437","upvote_count":"1"}],"comment_id":"571708","poster":"jvale","timestamp":"1647790440.0"}],"content":"Isn't it that with Kubernetes version 1.7 you can have a minimum of 0 in your node pool, being able to scale to 0? See https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler#minimum_and_maximum_node_pool_size blue note"},{"upvote_count":"2","comment_id":"510888","timestamp":"1640676960.0","content":"Functions is the only service billed against actual usage","poster":"HenkH"}],"answer_images":[],"answer":"A","answers_community":["A (95%)","5%"],"question_id":161,"exam_id":4,"answer_ET":"A","question_text":"You are designing an application for use only during business hours. For the minimum viable product release, you'd like to use a managed product that automatically `scales to zero` so you don't incur costs when there is no activity.\nWhich primary compute resource should you choose?"},{"id":"CTW54bEnOmQTMGUr3W7F","question_images":[],"answer":"A","answer_ET":"A","topic":"1","url":"https://www.examtopics.com/discussions/google/view/7290-exam-professional-cloud-architect-topic-1-question-64/","question_id":162,"answer_description":"","unix_timestamp":1572105900,"answer_images":[],"exam_id":4,"choices":{"B":"Create the Key object for each Entity and run multiple get operations, one operation for each entity","D":"Use the identifiers to create a query filter and run multiple query operations, one operation for each entity","A":"Create the Key object for each Entity and run a batch get operation","C":"Use the identifiers to create a query filter and run a batch query operation"},"question_text":"You are creating an App Engine application that uses Cloud Datastore as its persistence layer. You need to retrieve several root entities for which you have the identifiers. You want to minimize the overhead in operations performed by Cloud Datastore. What should you do?","discussion":[{"timestamp":"1608034020.0","comment_id":"110721","poster":"shashu07","content":"Correct Answer: A\nCreate the Key object for each Entity and run a batch get operation\nhttps://cloud.google.com/datastore/docs/best-practices \nUse batch operations for your reads, writes, and deletes instead of single operations. Batch operations are more efficient because they perform multiple operations with the same overhead as a single operation.\nFirestore in Datastore mode supports batch versions of the operations which allow it to operate on multiple objects in a single Datastore mode call.\nSuch batch calls are faster than making separate calls for each individual entity because they incur the overhead for only one service call. If multiple entity groups are involved, the work for all the groups is performed in parallel on the server side.","comments":[{"timestamp":"1681664520.0","poster":"AzureDP900","upvote_count":"2","comment_id":"696407","content":"works fine .. A is right"}],"upvote_count":"50"},{"poster":"AWS56","comment_id":"38076","upvote_count":"7","timestamp":"1594553040.0","content":"Agree A"},{"poster":"halifax","comment_id":"1360517","timestamp":"1740319380.0","content":"Selected Answer: A\nAn old question, no more datastore. in 2025 you can use Firestore in datastore mode.","upvote_count":"1"},{"poster":"de1001c","upvote_count":"4","timestamp":"1732774860.0","content":"Keep in mind that datastore is discontinued, Firestore being the recommended alternative.","comment_id":"1219984"},{"poster":"don_v","timestamp":"1720987080.0","upvote_count":"3","content":"According to \n\"A. Create the Key object for each Entity and run a batch get operation\"\nwhich is wrong as the key is already created for each entity whenever it's persisted.\n\nI believe the correct answer is C, -- to use a bulk query (a.k.a. \"batch\" in their terms).\nYou need a query with a criteria anyway to find a resultset, and not just a fetch by \"get\" operation to load by surrogate keys.","comment_id":"1122896"},{"content":"By using the \"lookup by key\" API of Cloud Datastore, you can minimize the overhead in operations performed by Cloud Datastore and optimize the performance of your App Engine application.\nfrom google.cloud import datastore\n\nclient = datastore.Client()\nkeys = [client.key('EntityKind', id) for id in entity_ids]\nentities = client.get_multi(keys)","timestamp":"1690785060.0","upvote_count":"3","poster":"vamgcp","comment_id":"793849"},{"comment_id":"752411","comments":[{"upvote_count":"6","timestamp":"1687349880.0","comment_id":"752414","poster":"omermahgoub","content":"Option B, running multiple get operations, one operation for each entity, would not be an efficient way to retrieve the entities because it would require multiple API calls to Cloud Datastore, which would increase the overhead and decrease the efficiency of the application.\n\nOption C, using the identifiers to create a query filter and running a batch query operation, would not be an efficient way to retrieve the entities because it would require performing a query operation, which is generally more expensive than a get operation.\n\nOption D, using the identifiers to create a query filter and running multiple query operations, one operation for each entity, would not be an efficient way to retrieve the entities because it would require performing multiple query operations, which are generally more expensive than get operations."}],"upvote_count":"6","poster":"omermahgoub","content":"A. Create the Key object for each Entity and run a batch get operation\n\nTo minimize the overhead in operations performed by Cloud Datastore, you should use the batch get operation to retrieve multiple entities in a single API call. To do this, you should create a Key object for each entity that you want to retrieve, then pass the Key objects to the batch get operation. This will allow you to retrieve multiple entities in a single API call, reducing the number of operations performed by Cloud Datastore and improving the efficiency of your application.","timestamp":"1687349880.0"},{"poster":"megumin","timestamp":"1683439500.0","content":"Selected Answer: A\nA is ok","upvote_count":"1","comment_id":"712906"},{"comment_id":"701834","poster":"Mahmoud_E","timestamp":"1682210100.0","content":"Selected Answer: A\nA is correct https://cloud.google.com/datastore/docs/best-practices#api_calls","upvote_count":"2"},{"timestamp":"1676744700.0","poster":"RitwickKumar","comment_id":"648519","upvote_count":"2","content":"Selected Answer: A\nhttps://cloud.google.com/datastore/docs/concepts/entities#datastore-datastore-batch-lookup-python"},{"poster":"haroldbenites","content":"go for A.","timestamp":"1654549740.0","comment_id":"495461","upvote_count":"2"},{"poster":"vincy2202","comment_id":"488302","content":"A is the right answer","timestamp":"1653663360.0","upvote_count":"2"},{"timestamp":"1653392220.0","upvote_count":"2","poster":"joe2211","content":"Selected Answer: A\nvote A","comment_id":"486014"},{"content":"A – create a key object for each entity, and run a batch get operations.\nSee Batch Operations section here: https://cloud.google.com/datastore/docs/concepts/entities\nvar keys = new Key[] { _keyFactory.CreateKey(1), _keyFactory.CreateKey(2) };\nvar tasks = _db.Lookup(keys[0], keys[1]);\n\n1 and 2 are identifiers of the Key. Check Key / Identifier definition on the same link (top of that page)\nSuch batch calls are faster than making separate calls for each individual entity because they incur the overhead for only one service call.","upvote_count":"1","comment_id":"468103","poster":"MaxNRG","timestamp":"1650985320.0"},{"poster":"victory108","upvote_count":"1","timestamp":"1637325780.0","content":"A. Create the Key object for each Entity and run a batch get operation","comment_id":"361278"},{"timestamp":"1636807140.0","comment_id":"356264","content":"A is correct","poster":"un","upvote_count":"1"},{"comment_id":"325537","timestamp":"1633066560.0","upvote_count":"1","content":"Answer is A","poster":"Ausias18"},{"poster":"Ausias18","content":"Answer is A","timestamp":"1633066500.0","comment_id":"325536","upvote_count":"1"},{"comment_id":"321221","content":"Agree A.\nLook at best practices: Ref: https://cloud.google.com/datastore/docs/best-practices#api_calls\n\"Use batch operations for your reads, writes, and deletes instead of single operations. Batch operations are more efficient because they perform multiple operations with the same overhead as a single operation.\"\nC is also good but is no longer recommended: https://cloud.google.com/appengine/docs/standard/java/datastore/queries","upvote_count":"3","poster":"lynx256","timestamp":"1632660420.0"},{"timestamp":"1632636720.0","upvote_count":"2","comment_id":"320926","poster":"Steve21","content":"Ans. A like this in Java Iterator<Entity> tasks = datastore.get(datastore.get(taskKey1), datastore.get(taskKey2)); \nand taskKey1"},{"comment_id":"290203","upvote_count":"1","poster":"CloudGenious","content":"if there is identifier for each data then you need to query every time for each identifier .that will increase the hit ratio. and down the optimization","timestamp":"1628931420.0"},{"content":"I would go with A.\n(https://cloud.google.com/datastore/docs/concepts/entities#datastore-datastore-named-key-python)\nEach entity in a Datastore mode database has a key that uniquely identifies it. The key consists of the following components:\n• The namespace of the entity, which allows for multitenancy\n• The kind of the entity, which categorizes it for the purpose of queries\n• An identifier for the individual entity, which can be either\n• a key name string\n• an integer numeric ID\n• An optional ancestor path locating the entity within the database hierarchy\nAn application can fetch an individual entity from the database using the entity's key, or it can retrieve one or more entities by issuing a query based on the entities' keys or property values.","poster":"Kevinzhang","comment_id":"251417","timestamp":"1624498440.0","upvote_count":"2"},{"timestamp":"1624170960.0","content":"I think the correct answer is C, create a query filter and run a batch operation.\nhttps://cloud.google.com/datastore/docs/concepts/entities#batch_operations","comments":[{"content":"That is a batch operation, not a batch query operation.","upvote_count":"1","timestamp":"1646036940.0","poster":"amxexam","comment_id":"435165"},{"content":"there's no such thing as \"batch query operation\"","upvote_count":"1","poster":"cucu","comment_id":"282016","timestamp":"1627912680.0"}],"upvote_count":"2","poster":"CosminCiuc","comment_id":"248458"},{"upvote_count":"4","timestamp":"1621842480.0","poster":"fankan","comments":[{"upvote_count":"3","poster":"fankan","timestamp":"1621842540.0","comment_id":"226560","content":"Sorry, I mean C."}],"content":"Batch Operations is true. But \" for which you have the identifiers\", so D is the answer","comment_id":"226559"},{"poster":"AshokC","upvote_count":"1","timestamp":"1615864860.0","content":"Answer: A","comment_id":"180102"},{"content":"\\agree A","poster":"WannaBeCloudArch","upvote_count":"2","comment_id":"174447","timestamp":"1615030140.0"},{"upvote_count":"1","comment_id":"117383","timestamp":"1608729540.0","poster":"mlantonis","content":"Batch operations are more efficient. A makes sense as the correct answer."},{"content":"A, for sure","poster":"gfhbox0083","timestamp":"1607594460.0","comment_id":"106583","upvote_count":"1"},{"upvote_count":"4","poster":"rehma017","timestamp":"1607384580.0","comment_id":"104870","content":"A because it is a root entity, you only need the keys to get the values. else it would be D"},{"content":"A is very correct","upvote_count":"2","comment_id":"102352","poster":"Ziegler","timestamp":"1607091000.0"},{"content":"A is the correct answer","comment_id":"101076","timestamp":"1606939800.0","poster":"Nirms","upvote_count":"2"},{"poster":"gcp_aws","timestamp":"1606225620.0","content":"A is the answer","comment_id":"94875","upvote_count":"2"},{"upvote_count":"2","poster":"[Removed]","comment_id":"56418","content":"Answer : A. Selected A in the exam","timestamp":"1598596380.0"},{"upvote_count":"3","content":"Answer: A","comment_id":"44810","timestamp":"1596112200.0","poster":"2g"},{"poster":"Eroc","timestamp":"1587917100.0","upvote_count":"4","comment_id":"17667","content":"https://cloud.google.com/datastore/docs/concepts/entities#batch_operations ... See \"Batch Operations\""}],"answers_community":["A (100%)"],"isMC":true,"timestamp":"2019-10-26 18:05:00"},{"id":"1UGdFfFbAf8tt2EBYB8w","question_id":163,"question_text":"You need to upload files from your on-premises environment to Cloud Storage. You want the files to be encrypted on Cloud Storage using customer-supplied encryption keys. What should you do?","timestamp":"2019-10-09 06:51:00","discussion":[{"timestamp":"1570596660.0","comment_id":"14369","comments":[{"poster":"JaimeMS","comment_id":"1228869","upvote_count":"13","content":"The documentation is here:\nhttps://cloud.google.com/storage/docs/encryption/using-customer-supplied-keys#upload-encrypt\n\nOption C is correct. You can upload a file using customer-supplied encryption with the command:\ngcloud storage cp SOURCE_DATA gs://BUCKET_NAME/OBJECT_NAME --encryption-key=YOUR_ENCRYPTION_KEY","comments":[{"content":"Option C doesn't say \"use gcloud storage cp\", it says \"use gsutil\".","poster":"ryaryarya","comment_id":"1339754","upvote_count":"6","timestamp":"1736729640.0"}],"timestamp":"1718175180.0"},{"poster":"tartar","content":"A is ok","timestamp":"1596708300.0","upvote_count":"16","comment_id":"151860"},{"poster":"nitinz","timestamp":"1614901500.0","comment_id":"303702","upvote_count":"4","content":"A is correct"},{"timestamp":"1604239800.0","content":".boto file with encryption key, but it will works for individual users, every user should update their own .boto with same key. Also while retrieving you should use the same key to decryption.","poster":"kumarp6","comment_id":"210508","upvote_count":"3"}],"content":"In GCP document, key could be configured in .boto.\nI didn't find information show gsutil suppots flag \"--encryption-key\".\n\nhttps://cloud.google.com/storage/docs/encryption/customer-supplied-keys","upvote_count":"48","poster":"KouShikyou"},{"timestamp":"1572106080.0","upvote_count":"18","content":"I agree, A.(https://cloud.google.com/storage/docs/gsutil/addlhelp/UsingEncryptionKeys#generating-customer-supplied-encryption-keys)","poster":"Eroc","comment_id":"17670"},{"comment_id":"1399635","poster":"Jamee","timestamp":"1742214180.0","upvote_count":"1","content":"Selected Answer: C\nusing command line is more convenient."},{"poster":"cloud_rider","comment_id":"1363859","content":"Selected Answer: A\nA is Correct. Here is the documentation -https://cloud.google.com/storage/docs/boto-gsutil","timestamp":"1740902580.0","upvote_count":"1"},{"poster":"09bd94b","upvote_count":"2","comment_id":"1347328","timestamp":"1737969600.0","content":"Selected Answer: A\nA is the only correct answer. Those pointing to 'C' need to realize that the flag \"--encryption-key\" works with *gcloud storage cp* and NOT with gsutil"},{"upvote_count":"1","timestamp":"1737551580.0","comment_id":"1344755","content":"Selected Answer: D\nhttps://cloud.google.com/storage/docs/gsutil\nEven it is recommended to use gcloud, this is the lonely answer which covers all the lifecycle of the process : create the storage, use a CEK, and upload files.\nI also agree that files have not to be encrypted individually, but the encryption has to be done at the storage level, so it will be transparent for a future consumer service.\n\nAnswer B would be correct but it is not only a question of supplying the key, but also to update the bucket. So it is not enough","poster":"hpf97"},{"upvote_count":"2","timestamp":"1737403500.0","content":"Selected Answer: A\nOnly A is 100% correct","comment_id":"1343838","poster":"09bd94b"},{"timestamp":"1737070920.0","content":"Selected Answer: A\nC is wrong because gsutil does not have a --encryption-key option","comment_id":"1341911","poster":"Lrzo","upvote_count":"1"},{"upvote_count":"2","comment_id":"1333001","timestamp":"1735396140.0","content":"Selected Answer: C\nThe correct answer is C. Here's why:\n\nCustomer-Supplied Encryption Keys (CSEK): When using CSEK, you provide the encryption key yourself. Google doesn't store your key on their servers. You're responsible for managing and protecting it.\ngsutil and --encryption-key: The gsutil command-line tool is the primary way to interact with Cloud Storage. To use CSEK with gsutil, you use the --encryption-key flag directly with the upload command. This flag takes the base64 encoded encryption key as its valu","poster":"klayytech"},{"comment_id":"1332099","poster":"rrope","upvote_count":"2","timestamp":"1735242240.0","content":"Selected Answer: C\nCustomer-Supplied Encryption Keys (CSEK) are provided on a per-request basis. This means you provide the key during the upload operation itself, not when creating the bucket or through persistent configuration files.\n\ngsutil is the command-line tool for interacting with Cloud Storage. The --encryption-key flag specifically allows you to provide the base64 encoded encryption key when uploading objects."},{"comment_id":"1332085","upvote_count":"2","poster":"rahuld19","timestamp":"1735239660.0","content":"Selected Answer: A\nright answer is A"},{"upvote_count":"2","timestamp":"1734474000.0","poster":"mahi_h","content":"Selected Answer: D\nI see option D is not even discussed. The question said \"upload files\", meaning multiple object. Isn't the encrypted bucked creation a secured way to store them in cloud storage?","comment_id":"1328192"},{"upvote_count":"2","content":"Selected Answer: A\n[GSUtil]\n check_hashes\n content_language\n decryption_key1 ... 100\n default_api_version\n disable_analytics_prompt\n encryption_key","timestamp":"1734357600.0","comment_id":"1327403","poster":"kip21"},{"upvote_count":"2","content":"Selected Answer: C\nOption C: Use gsutil to upload the files and use the flag --encryption-key to supply the encryption key. This is the correct approach, as it allows you to specify the CSEK directly at the time of upload, ensuring that your files are encrypted using your provided key.","timestamp":"1733970480.0","comment_id":"1325363","poster":"deep316"},{"comment_id":"1323765","timestamp":"1733690520.0","poster":"klayytech","upvote_count":"4","content":"Selected Answer: D\nD. Use gsutil to create a bucket, and use the flag --encryption-key to supply the encryption key. Use gsutil to upload the files to that bucket.\n\nThis option provides the most comprehensive and secure approach:\n\nCreate an encrypted bucket:\n\nUse gsutil mb -b location gs://your-bucket-name --encryption-key=your_encryption_key\nThis ensures that all objects uploaded to this bucket will be encrypted with your provided key.\nUpload files to the encrypted bucket:\n\nUse gsutil cp your_local_file gs://your-bucket-name\nBy following this approach, you guarantee that your files are encrypted both at rest and in transit on Cloud Storage, providing a robust security posture.\n\nThe other options either lack the encryption key specification or do not create an encrypted bucket, leaving your data vulnerable."},{"comment_id":"1317576","upvote_count":"2","timestamp":"1732547880.0","content":"Selected Answer: A\nThe boto configuration file in Google Cloud Platform (GCP) controls how the gsutil command behaves:\n\nSetting up gsutil\nYou can use the boto configuration file to set up gsutil to work through a proxy.\n\nUsing encryption keys\nYou can use the boto configuration file to use customer-managed or customer-supplied encryption keys.","comments":[{"comment_id":"1317577","poster":"desertlotus1211","timestamp":"1732547940.0","content":".boto is smoother to use consistently...","upvote_count":"1"}],"poster":"desertlotus1211"},{"comment_id":"1316304","poster":"icarogsm","content":"Selected Answer: A\nA! I agree that the boto file sounds better","upvote_count":"2","timestamp":"1732279140.0"}],"answer":"A","exam_id":4,"answers_community":["A (53%)","C (42%)","6%"],"answer_ET":"A","answer_description":"","answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/6308-exam-professional-cloud-architect-topic-1-question-65/","choices":{"D":"Use gsutil to create a bucket, and use the flag --encryption-key to supply the encryption key. Use gsutil to upload the files to that bucket.","B":"Supply the encryption key using gcloud config. Use gsutil to upload the files to that bucket.","A":"Supply the encryption key in a .boto configuration file. Use gsutil to upload the files.","C":"Use gsutil to upload the files, and use the flag --encryption-key to supply the encryption key."},"question_images":[],"topic":"1","isMC":true,"unix_timestamp":1570596660},{"id":"ZZ2Op9qItTKYa6UNfTxf","question_images":[],"answer_description":"","timestamp":"2019-11-27 16:57:00","choices":{"C":"Schedule BigQuery load jobs to ingest analytics files uploaded to Cloud Storage every ten minutes, and visualize the results in Google Data Studio.","B":"Output custom metrics to Stackdriver from the game servers, and create a Dashboard in Stackdriver Monitoring Console to view them.","D":"Insert the KPIs into Cloud Datastore entities, and run ad hoc analysis and visualizations of them in Cloud Datalab.","A":"Store time-series data from the game servers in Google Bigtable, and view it using Google Data Studio."},"isMC":true,"topic":"1","url":"https://www.examtopics.com/discussions/google/view/9222-exam-professional-cloud-architect-topic-1-question-66/","unix_timestamp":1574870220,"discussion":[{"comment_id":"31374","upvote_count":"33","comments":[{"content":"A is correct","comment_id":"446906","poster":"Raja101","upvote_count":"4","timestamp":"1631938620.0"},{"upvote_count":"7","poster":"ErenYeager","comments":[{"poster":"anshumankmr80","comment_id":"746373","content":"Source?\n\nhttps://lookerstudio.google.com/data?search=big","comments":[{"content":"That’s BigQuery, not BigTable, no?","timestamp":"1679921340.0","comment_id":"852110","upvote_count":"2","poster":"HD2023"},{"poster":"jrisl1991","upvote_count":"2","comment_id":"1053386","content":"Looker is not the same as Data Studio. I know it came to replace it, but these questions are kind of old, so unless it clearly says \"looker\", I wouldn't take both as the same.","timestamp":"1698199560.0"}],"upvote_count":"2","timestamp":"1671126180.0"}],"timestamp":"1667839140.0","comment_id":"713165","content":"As of today you can"}],"timestamp":"1576878420.0","content":"Ans is B. Data studio cannot be used with BigTable\nhttps://datastudio.google.com/datahttps://datastudio.google.com/data","poster":"suryalsp"},{"timestamp":"1575736140.0","comment_id":"27651","content":"correct is B","poster":"kolcsarzs","upvote_count":"12"},{"comment_id":"1302169","content":"Answer is B","poster":"nareshthumma","upvote_count":"1","timestamp":"1729710840.0"},{"comment_id":"1301587","timestamp":"1729601760.0","upvote_count":"1","content":"Selected Answer: B\nBigtable is not real time solution","poster":"belouh"},{"upvote_count":"1","comments":[{"poster":"Saxena_Vibhor","timestamp":"1708342740.0","upvote_count":"1","content":"Why is A not a real time solution? https://cloud.google.com/bigtable/docs/integrations#opentsdb \n\nit says: OpenTSDB is a time-series database that can use Bigtable for storage. Monitoring time-series data with OpenTSDB on Bigtable and GKE shows how to use OpenTSDB to collect, record, and monitor time-series data on Google Cloud. The OpenTSDB documentation provides additional information to help you get started.","comment_id":"1153880"}],"content":"B - Correct\nA - It is not a real-time solution","poster":"kip21","comment_id":"1122930","timestamp":"1705271460.0"},{"comment_id":"1026138","timestamp":"1696556880.0","poster":"AdityaGupta","content":"Selected Answer: B\nBigTable doesn't integrate with Data Studio\n\nhttps://cloud.google.com/bigtable/docs/integrations","upvote_count":"1"},{"comment_id":"896941","timestamp":"1683999420.0","content":"B is correct, you should create custom KPI in Stack Driver","upvote_count":"1","poster":"LaxmanTiwari"},{"poster":"omermahgoub","comment_id":"752985","upvote_count":"10","timestamp":"1671688260.0","content":"B. Output custom metrics to Stackdriver from the game servers, and create a Dashboard in Stackdriver Monitoring Console to view them.\n\nTo capture multiple GBs of aggregate real-time KPIs from game servers running on Google Cloud Platform and monitor them with low latency, the customer should output custom metrics to Stackdriver from the game servers. Stackdriver allows you to collect and store custom metrics, as well as view and analyze them in real-time using the Stackdriver Monitoring Console. The customer can create a Dashboard in the Monitoring Console to view the KPIs and monitor them with low latency.","comments":[{"timestamp":"1671688260.0","comments":[{"comment_id":"845272","poster":"jlambdan","content":"big table is not for batch. It is used in IOT...\nhttps://cloud.google.com/bigtable","upvote_count":"4","timestamp":"1679349000.0"}],"content":"Option A, storing time-series data in Bigtable and viewing it using Data Studio, would not be suitable for capturing and monitoring real-time KPIs with low latency. Bigtable is a scalable NoSQL database that is optimized for large-scale batch processing, and Data Studio is a visualization tool that is not designed for real-time data analysis.\n\nOption C, scheduling BigQuery load jobs to ingest analytics files uploaded to Cloud Storage every ten minutes and visualizing the results in Data Studio, would not be suitable for capturing and monitoring real-time KPIs with low latency. BigQuery is a data warehouse that is optimized for batch processing, and it is not designed for real-time data analysis.\n\nOption D, inserting the KPIs into Cloud Datastore entities and running ad hoc analysis and visualizations of them in Cloud Datalab, would not be suitable for capturing and monitoring real-time KPIs with low latency. Cloud Datastore is a NoSQL document database, and Cloud Datalab is a data analysis and visualization tool that is not designed for real-time data analysis.","upvote_count":"9","comment_id":"752986","poster":"omermahgoub"}]},{"timestamp":"1668557040.0","upvote_count":"1","poster":"Raja101","content":"Selected Answer: A\nA is correct","comment_id":"719193"},{"poster":"megumin","comment_id":"714718","content":"Selected Answer: B\nB is ok","timestamp":"1668010740.0","upvote_count":"1"},{"timestamp":"1666486620.0","poster":"Mahmoud_E","upvote_count":"4","comment_id":"701840","content":"Selected Answer: B\nB is correct as Data studio does not support bigtable as a source"},{"content":"KPI, SLO,SLI all those work with observability which stackdriver","poster":"zr79","timestamp":"1665987600.0","upvote_count":"2","comment_id":"696979"},{"upvote_count":"1","timestamp":"1665838980.0","poster":"jay9114","comment_id":"695402","content":"The reference provided seems irrelevant to this question."},{"content":"A is the correct answer, the key word here, real time and low latency.","timestamp":"1654613040.0","comment_id":"612803","poster":"bolington","upvote_count":"1"},{"timestamp":"1652067660.0","poster":"Nirca","content":"Selected Answer: B\nBigTable has no connection to data studio. \nhttps://datastudio.google.com/data?search=Big","upvote_count":"6","comment_id":"598824"},{"timestamp":"1642857180.0","content":"Selected Answer: B\nB, it is","comment_id":"529863","upvote_count":"2","poster":"BeetleJuice"},{"content":"I don't think there is a correct answer, but B looks correct in this.\nIf use Bigquery, then A is correct .\nC is not for realtime.\nD Datastore is for small usecase.\nKeywords 'real time' ,'analytics' \nhttps://events.withgoogle.com/solution-design-pattern-gaming/analytics-pattern/","upvote_count":"2","poster":"OrangeTiger","comment_id":"516320","timestamp":"1641282840.0"},{"comment_id":"510982","poster":"Wonka","timestamp":"1640685780.0","content":"B is okay","upvote_count":"1"},{"poster":"gcp_learner","content":"Selected Answer: B\nI will go with B because you can meet the requirement with Cloud Monitoring, formerly Stackdriver monitoring","timestamp":"1639425900.0","comment_id":"500867","upvote_count":"1"},{"timestamp":"1639425840.0","upvote_count":"1","poster":"gcp_learner","comment_id":"500864","content":"Selected Answer: B\nYou can do this with Cloud Monitoring, formerly Stackdriver Monitoring"},{"upvote_count":"7","poster":"ABO_Doma","content":"Selected Answer: B\nWhile Google Bigtable may be a good place for this data, there is no direct connector between Bigtable and Google Data Studio. To enable Google Data Studio to pick up this information, we would have to use something like BigQuery (https://cloud.google.com/bigquery/external-data-Bigtable) to query data stored in Bigtable, and Google Data Studio can then make use of this data.","timestamp":"1639050420.0","comment_id":"497680"},{"upvote_count":"1","poster":"pakilodi","timestamp":"1638972900.0","content":"Selected Answer: B\nB is the answer here","comment_id":"496880"},{"timestamp":"1638895860.0","comment_id":"496193","upvote_count":"1","content":"Selected Answer: B\nStackdriver is designed to Monitor GCP","poster":"PhilipKoku"},{"comments":[{"timestamp":"1665939600.0","content":"B is right","comment_id":"696405","poster":"AzureDP900","upvote_count":"1"}],"upvote_count":"2","content":"Go for B.\nBigtable don't support integrations with datastudio\nhttps://cloud.google.com/bigtable/docs/integrations","timestamp":"1638834000.0","comment_id":"495468","poster":"haroldbenites"},{"upvote_count":"2","comment_id":"490160","content":"Selected Answer: B\nAnswer is B. Stackdriver is used for creating custom metrics and displaying them","poster":"pnvijay","timestamp":"1638213900.0"},{"timestamp":"1637797080.0","upvote_count":"2","poster":"joe2211","comment_id":"486320","content":"Selected Answer: B\nvote B"},{"timestamp":"1637667540.0","comment_id":"484983","upvote_count":"1","poster":"sam1972","content":"A is correct \nhttps://cloud.google.com/bigquery/docs/visualize-data-studio"},{"poster":"MaxNRG","content":"B - output custom metrics to Stackdriver from game servers and create a Dashboard in Stackdriver Console to view them.\nStackdriver is designed for KPIs/metrics monitoring:\nhttps://cloud.google.com/monitoring/api/v3/metrics\nA – potentially could be, though Bigtable can’t be connected to DataStudio directly. Data Studio can read data from BiqQuery, MySQL, PostgreSQL, or CSV file uploaded to Cloud Storage.\nhttps://datastudio.google.com/data","comment_id":"468791","timestamp":"1635357120.0","upvote_count":"6"},{"content":"B - output custom metrics to Stackdriver from game servers and create a Dashboard in Stackdriver Console to view them.\nStackdriver is designed for KPIs/metrics monitoring:\nhttps://cloud.google.com/monitoring/api/v3/metrics\nA – potentially could be, though Bigtable can’t be connected to DataStudio directly. Data Studio can read data from BiqQuery, MySQL, PostgreSQL, or CSV file uploaded to Cloud Storage.\nhttps://datastudio.google.com/data","timestamp":"1635267180.0","poster":"MaxNRG","upvote_count":"1","comment_id":"468181"},{"comment_id":"402097","poster":"MamthaSJ","content":"Answer is B","upvote_count":"3","timestamp":"1625764260.0"},{"poster":"kopper2019","timestamp":"1625019540.0","content":"all New Questions released in June 2021 are in Question number 3 or share you email","upvote_count":"4","comment_id":"394390","comments":[{"comment_id":"460799","upvote_count":"1","content":"@koper2019 cant find these questions?","timestamp":"1633986660.0","poster":"JasonL_GCP"}]},{"content":"B:\nCloud BigTable can not be used with DataStudio\nDatastudio is report builder\nrequirement is need realtime KPI monitoring\nStackdriver can collect the custom metrics from Gameserver","comments":[{"poster":"anshumankmr80","content":"What I am not able to understand is we totally can report custom KPIs on Google Datastudio. I have worked on similar requirements myself.","timestamp":"1671158580.0","comment_id":"746720","upvote_count":"1"},{"timestamp":"1671158700.0","content":"However, the low latency requirement is not being fulfilled by Datastudio, as the frequency is mentioned as 10 minutes ( from what I can recollect, it can go upto a max of one minute) and it would also require an ETL pipeline to be setup which would be complex and time consuming, but not impossible.","upvote_count":"1","comment_id":"746721","poster":"anshumankmr80"}],"upvote_count":"2","poster":"aviratna","timestamp":"1624878300.0","comment_id":"392846"},{"timestamp":"1621419300.0","comment_id":"361260","content":"B. Output custom metrics to Stackdriver from the game servers, and create a Dashboard in Stackdriver Monitoring Console to view them.","upvote_count":"2","poster":"victory108"},{"poster":"un","upvote_count":"1","comment_id":"356496","timestamp":"1620919980.0","content":"B is correct"},{"upvote_count":"1","poster":"Ausias18","timestamp":"1617693180.0","comment_id":"329394","content":"Answer is B"},{"timestamp":"1617412800.0","poster":"AD3","comments":[{"comment_id":"417313","content":"Data Studio cannot connect with Cloud Bigtable, kindly check Data Studio connectors","timestamp":"1627640520.0","poster":"VishalB","upvote_count":"1"}],"upvote_count":"3","content":"A is the answer time series data and low latency Bigtable. there is no such requirement for Data Studio. However, data studio can be used in conjunction with Bigquerry. More important keywords are GB sizes of data, time series and low latency. both point to bigtable.","comment_id":"327078"},{"content":"A vs B ?\nMany of you wrote \"A is not correct because you cannot connect Bigtable to Data Studio\".\nThis sentence is not exactly true. \nBigtable cannot be connected DIRECTLY to Data Studio but you can create in BigQuery an external table to Bigtable and connect Data Studio to this external table in BigQuery (Ref: https://cloud.google.com/bigquery/external-data-bigtable#permanent-tables).\nSo, if this is the only reason to choose B, it is wrong.\nNext. I assume the KPI in the question concern technical issues (latency, CPU utilization and so on). \n - If it's true assumption, we do not need custom metrics ==> hence A. \n - If the assumption is wrong, we do NEED custom metrics ==> hence B. \nWhat do you think about assumption above ?","comments":[{"upvote_count":"2","poster":"rsamant","timestamp":"1623375600.0","comment_id":"379381","content":"if you connect data studio using external tables in bigquery. what will happen to low latency requirement ? answer should be B"}],"upvote_count":"5","timestamp":"1616839140.0","comment_id":"321748","poster":"lynx256"},{"poster":"joshuaquek","comment_id":"263074","content":"The answer is B. You cannot use Data Studio with Bigtable.","timestamp":"1610178600.0","upvote_count":"2"},{"poster":"Arimaverick","timestamp":"1609977180.0","upvote_count":"2","comment_id":"261369","content":"Answer should be B. Google recommends to use Custom metrics to achieve any sort of goals to improve KPIs. like number of players, response metrics."},{"timestamp":"1605980040.0","content":"The Google Data Studio BigQuery connector allows you to access data from your BigQuery tables within Google Data Studio - first result from google search.","upvote_count":"1","comments":[{"comment_id":"231893","poster":"cloud_g","timestamp":"1606823520.0","content":"But you know BigQuery is a different product from BigTable, don´t you?","upvote_count":"1"}],"comment_id":"224446","poster":"krakoziabl"},{"comment_id":"206484","timestamp":"1603735500.0","upvote_count":"1","content":"The terms \"Real-time KPI\" and \"Low latency\" are creating confusion here, but its not real-time data, it is KPI which is obtained from Stackdriver. Also Cloud Big Table doesn't integrate with Data Studio. I will go with option B for this question.","poster":"AdityaGupta"},{"poster":"N1_arch","upvote_count":"2","content":"Won't Big Table be an overkill for several GBs of aggregated data? I think it's C >> BigQuery + Data Studio","timestamp":"1603576800.0","comment_id":"205336"},{"upvote_count":"2","timestamp":"1601399820.0","content":"Obviously, A. But, every book, the GCP docs, and every video on Udemy and YouTube you tell you: Pub/Sub then Bigtable.","poster":"kimberjdaw","comment_id":"189813"},{"content":"Answer B","poster":"AshokC","upvote_count":"2","comment_id":"180113","timestamp":"1600219980.0"},{"timestamp":"1592911500.0","upvote_count":"2","comment_id":"117386","content":"Even if we need low latency and Bigtable is the ideal option, Bigtable does not provide interaction with Data Studio.\nhttps://cloud.google.com/bigtable/docs/integrations\n\nB is the correct answer","poster":"mlantonis"},{"poster":"Tushant","upvote_count":"1","timestamp":"1592646060.0","comment_id":"114657","content":"B is the correct answer"},{"timestamp":"1592366820.0","poster":"hafid","content":"the answer is B similar question as in google ungraded exam course in coursera","upvote_count":"1","comment_id":"112119"},{"upvote_count":"2","timestamp":"1592056920.0","comment_id":"109507","poster":"gfhbox0083","content":"B, for sure.\nB. Output custom metrics to Stackdriver from the game servers, and create a Dashboard in Stackdriver Monitoring Console to view them."},{"poster":"syu31svc","upvote_count":"1","content":"No way to connect BigTable to Data Studio so answer is B","comment_id":"109319","comments":[{"poster":"olg","timestamp":"1600236420.0","comment_id":"180197","upvote_count":"1","content":"you can - https://cloud.google.com/bigquery/docs/visualize-data-studio but I vote for B anyways"}],"timestamp":"1592040600.0"},{"poster":"Ziegler","content":"The question is too clever to divert ones attention to A but this this not a time series data but PKI which is the role of stackdriver. In view of this B is the right answer.","comment_id":"102377","upvote_count":"2","timestamp":"1591274880.0"},{"upvote_count":"1","comment_id":"101080","poster":"Nirms","content":"B is the correct answer","timestamp":"1591121640.0"},{"upvote_count":"1","timestamp":"1590658200.0","content":"Final Decision to go with Option B","comment_id":"97405","poster":"AD2AD4"},{"comment_id":"95010","timestamp":"1590338100.0","content":"I guess it is B... For example, number of users is a real time KPI.. you can very well add this is as a custom metric in stack driver and use it in the dashboard - I think.","poster":"gcp_aws","upvote_count":"1"},{"content":"B.\nA is wrong because Google Data Studio cannot use BigTable as data source. Verified in GCP console.","comment_id":"92582","upvote_count":"5","poster":"huangmeiguai","timestamp":"1589954520.0"},{"poster":"laksg","timestamp":"1589682780.0","comment_id":"90285","upvote_count":"2","content":"B , Monitoring is the Key. stack driver Custom metrics"},{"poster":"alilog","timestamp":"1589530320.0","comment_id":"89375","content":"Answer is B. The exact same question is on LinuxAcademy and the answer is B.","upvote_count":"3"},{"comment_id":"86045","content":"A is my choice","poster":"skywalker","timestamp":"1589025960.0","upvote_count":"1"},{"content":"For KPIs would you use Stackdriver? These KPIs are likely to be business KPIs, how will Stackdriver provide that information? I guess, it should be A.","timestamp":"1587455280.0","upvote_count":"1","poster":"PRC","comment_id":"77341"},{"timestamp":"1582878840.0","upvote_count":"5","comment_id":"56420","poster":"[Removed]","content":"Answer: B. Selected B in the exam"},{"poster":"mawsman","content":"Yep, B is the best option. \"When you use the Monitoring API to write a new data point to an existing time series, you can access the data in a few seconds.\" https://cloud.google.com/monitoring/api/v3/metrics-details#metric-kinds","timestamp":"1582074900.0","comment_id":"52359","upvote_count":"4"},{"poster":"2g","timestamp":"1580394900.0","content":"Answer: B","comment_id":"44815","upvote_count":"3"},{"poster":"natpilot","comment_id":"42851","content":"B is better","upvote_count":"3","timestamp":"1580035140.0"},{"comment_id":"40436","poster":"sri007","content":"A is correct","upvote_count":"1","timestamp":"1579374900.0"},{"comment_id":"38079","timestamp":"1578835680.0","upvote_count":"3","poster":"AWS56","content":"Agree B"},{"comment_id":"33612","content":"We can query Bigtable data using permanent external tables in Data Studio?\nhttps://cloud.google.com/bigquery/external-data-bigtable","poster":"Gurkan","upvote_count":"1","timestamp":"1577644380.0"},{"poster":"asimha","timestamp":"1574870220.0","comments":[{"poster":"droogie","content":"Yes, according to the console","comments":[{"poster":"tartar","upvote_count":"6","comments":[{"upvote_count":"7","timestamp":"1597446780.0","poster":"tartar","comment_id":"158357","content":"Big Table won't talk to Google Data Studio","comments":[{"upvote_count":"1","comment_id":"321732","timestamp":"1616838060.0","content":"This is no exactly true....\nBigtable cannot talk DIRECTLY to Data Studio but you can create in BigQuery an external table to Bigtable and connect Data Studio to this external table in BigQuery.\nRef: https://cloud.google.com/bigquery/external-data-bigtable#permanent-tables","poster":"lynx256","comments":[{"upvote_count":"1","comment_id":"333656","timestamp":"1618196520.0","content":"yes, you are right. So the answer is A right?","poster":"cmfchong"}]}]}],"content":"B is ok","comment_id":"151867","timestamp":"1596708780.0"}],"timestamp":"1591896660.0","upvote_count":"1","comment_id":"107971"},{"comment_id":"210511","upvote_count":"5","poster":"kumarp6","timestamp":"1604239920.0","content":"B is correct, you should create custom KPI in Stack Driver"},{"content":"Store KPIs as custom metrics in Cloud Monitoring, and build dashboards in Cloud Monitoring to visualize KPIs. is the right answer.\n\nYou can save the KPIs as custom metrics in Cloud Monitoring and define a custom dashboard from these metrics.\n\nRef: https://cloud.google.com/monitoring/custom-metrics\n\nCloud Monitoring provides predefined dashboards for Google Cloud services, and it provides the ability to create custom dashboards. With custom dashboards, you determine which charts are displayed and their configuration. You can create custom dashboards by using the Google Cloud Console or by using the Dashboard endpoint in the Cloud Monitoring API.\n\nRef: https://cloud.google.com/monitoring/dashboards\n\nAll of this happens in near real-time. If you are using custom metrics and you use the Monitoring API to write a new data point to an existing time series, you can retrieve the data in a few seconds.\n\nRef: https://cloud.google.com/monitoring/api/v3/latency-n-retention#latency","comment_id":"230105","poster":"techalik","upvote_count":"1","timestamp":"1606629060.0"},{"upvote_count":"2","comment_id":"303708","poster":"nitinz","timestamp":"1614901740.0","content":"B, datastudio works with SQL. Bigtable is NoSQL"}],"content":"can we use Data studio with BigTable?","upvote_count":"1","comment_id":"24847"}],"answer_images":[],"answer":"B","question_id":164,"answers_community":["B (97%)","3%"],"exam_id":4,"question_text":"Your customer wants to capture multiple GBs of aggregate real-time key performance indicators (KPIs) from their game servers running on Google Cloud Platform and monitor the KPIs with low latency. How should they capture the KPIs?","answer_ET":"B"},{"id":"EsmfEEiNIwwUEwSAerBs","choices":{"A":"Perform the following: 1. Create a managed instance group with f1-micro type machines. 2. Use a startup script to clone the repository, check out the production branch, install the dependencies, and start the Python app. 3. Restart the instances to automatically deploy new production releases.","B":"Perform the following: 1. Create a managed instance group with n1-standard-1 type machines. 2. Build a Compute Engine image from the production branch that contains all of the dependencies and automatically starts the Python app. 3. Rebuild the Compute Engine image, and update the instance template to deploy new production releases.","C":"Perform the following: 1. Create a Google Kubernetes Engine (GKE) cluster with n1-standard-1 type machines. 2. Build a Docker image from the production branch with all of the dependencies, and tag it with the version number. 3. Create a Kubernetes Deployment with the imagePullPolicy set to 'IfNotPresent' in the staging namespace, and then promote it to the production namespace after testing.","D":"Perform the following: 1. Create a GKE cluster with n1-standard-4 type machines. 2. Build a Docker image from the master branch with all of the dependencies, and tag it with 'latest'. 3. Create a Kubernetes Deployment in the default namespace with the imagePullPolicy set to 'Always'. Restart the pods to automatically deploy new production releases."},"topic":"1","question_id":165,"exam_id":4,"question_images":[],"answer_ET":"C","url":"https://www.examtopics.com/discussions/google/view/6890-exam-professional-cloud-architect-topic-1-question-67/","answer":"C","question_text":"You have a Python web application with many dependencies that requires 0.1 CPU cores and 128 MB of memory to operate in production. You want to monitor and maximize machine utilization. You also want to reliably deploy new versions of the application. Which set of steps should you take?","unix_timestamp":1571665260,"isMC":true,"answers_community":["C (57%)","A (34%)","8%"],"discussion":[{"comments":[{"timestamp":"1682002800.0","content":"ifnotpresent won't pull new version.","upvote_count":"4","poster":"medi01","comment_id":"875725"},{"comment_id":"989137","poster":"heretolearnazure","content":"yes i agree","timestamp":"1692877320.0","upvote_count":"1"}],"content":"C is correct, need \"ifnotpresent\"when uploads to container registry","poster":"jcmoranp","comment_id":"17504","upvote_count":"41","timestamp":"1572076620.0"},{"upvote_count":"24","timestamp":"1574519160.0","content":"C is the best choice. You can create a k8s cluster with just one node and use a different namespaces for staging and production. In staging, you will test the changes","comment_id":"23861","poster":"TosO","comments":[{"poster":"AzureDP900","comment_id":"696404","timestamp":"1665939300.0","upvote_count":"1","content":"Agreed"}]},{"poster":"Gayathri1608","content":"Selected Answer: C\ni think suitable for the given scenario","upvote_count":"2","comment_id":"1318013","timestamp":"1732616700.0"},{"poster":"nareshthumma","comment_id":"1302171","upvote_count":"2","content":"Answer is C","timestamp":"1729711020.0"},{"comment_id":"1271939","content":"Selected Answer: C\nshould be option C because if you are working in real world, GKE is the best solution for such a case. Furthermore, its reliable, scalable, flexible, at least the best option among the other three.","poster":"44fa527","upvote_count":"2","timestamp":"1724544720.0"},{"comments":[{"content":"Also you can deploy COS Containerd in a VM","poster":"cai_engineer","comment_id":"1271076","upvote_count":"1","timestamp":"1724388600.0"}],"timestamp":"1724388540.0","poster":"cai_engineer","upvote_count":"1","content":"Selected Answer: A\nNgl it's A. Don't use GKE, it won't schedule the deployment as most of the resources already occupied by kube-system","comment_id":"1271074"},{"poster":"awsgcparch","timestamp":"1722009660.0","upvote_count":"1","comment_id":"1255779","content":"Selected Answer: D\nimagePullPolicy: Always ensures that the latest version of the image is always pulled, which guarantees that the most recent code is deployed.\nRestarting pods ensures that the new version is deployed without requiring manual intervention."},{"content":"Selected Answer: B\nB because Kubernetes will be overkill and A is not reliable","upvote_count":"1","comment_id":"1163634","timestamp":"1709312280.0","poster":"krokskan"},{"comment_id":"1142027","upvote_count":"2","poster":"Gall","content":"Selected Answer: C\nA is wrong as after the restart the script will be rerun and fetch the code directly from the repo (even if production). The load of the massive number of dependencies will take a lot of timee, and the application version will be fuzzy.","timestamp":"1707215220.0"},{"content":"C is correct, B (instance template cannote be updated once created.","timestamp":"1706255820.0","poster":"moumou","comment_id":"1132343","upvote_count":"2"},{"content":"C - Correct","comment_id":"1123440","poster":"kip21","upvote_count":"1","timestamp":"1705329000.0"},{"content":"The correct answer is C. Because it is the only option that RELIABLY tests the app in staging before it is applied to production. Remember that one of the requirements in the question is to reliably deploy the app.","timestamp":"1704849600.0","upvote_count":"3","poster":"AWS_Sam","comment_id":"1118000"},{"upvote_count":"3","content":"Selected Answer: A\nYou don't need GKE for 0.1 CPU, only A meet hte needs","poster":"Roro_Brother","comment_id":"1099535","timestamp":"1702888020.0"},{"comment_id":"1090684","content":"Selected Answer: A\nFor 0.1 CPU I will never use GKE, considering the associated cost with control plane and not even one option in the question mentioning micro instances for the node pool","poster":"MahAli","timestamp":"1701995160.0","upvote_count":"4"},{"timestamp":"1700821080.0","upvote_count":"4","poster":"mastrrrr","comment_id":"1079172","content":"Selected Answer: A\nWhen we read the question - \"0.1 CPU cores and 128 MB of memory\" to operate in production. You want to monitor and \"maximize machine utilization\"... Answer A should be a fit based on the question details. Would GKE for tiny application be overkill?"},{"upvote_count":"2","comment_id":"1041740","timestamp":"1697115060.0","poster":"Arun_m_123","content":"Selected Answer: C\npython app on compute engine is a disastrous architecture. C is the correct architecture which tests the app before putting to prod"},{"poster":"AdityaGupta","comment_id":"1026141","content":"Selected Answer: C\nYou should use GKE, because your can scale up and down based on your demand. Also you can specifiy the resource size like 0.1 CPU and 128 MB of memory per Pod. \n\nSecondly, Kubernetes Deployment with the imagePullPolicy set to “IfNotPresent” in the staging namespace, and then promote it to production namespace after testing. is best practice.","timestamp":"1696557180.0","upvote_count":"10","comments":[{"comment_id":"1066742","upvote_count":"3","poster":"A21325412","content":"Ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/\n\nCorrect. Because we can spec the resources on our pods is why C is chosen over A (f1-micro). This is what allows us to \"maximize machine utilization\"!","timestamp":"1699564980.0"}]}],"answer_images":[],"answer_description":"","timestamp":"2019-10-21 15:41:00"}],"exam":{"name":"Professional Cloud Architect","id":4,"numberOfQuestions":279,"lastUpdated":"11 Apr 2025","isImplemented":true,"provider":"Google","isMCOnly":false,"isBeta":false},"currentPage":33},"__N_SSP":true}