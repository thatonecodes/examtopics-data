{"pageProps":{"questions":[{"id":"tLJSIoOTH0ga7SJU98Fk","question_id":236,"exam_id":13,"url":"https://www.examtopics.com/discussions/google/view/54966-exam-professional-machine-learning-engineer-topic-1-question/","question_text":"You are developing models to classify customer support emails. You created models with TensorFlow Estimators using small datasets on your on-premises system, but you now need to train the models using large datasets to ensure high performance. You will port your models to Google Cloud and want to minimize code refactoring and infrastructure overhead for easier migration from on-prem to cloud. What should you do?","answer":"A","question_images":[],"choices":{"B":"Create a cluster on Dataproc for training.","C":"Create a Managed Instance Group with autoscaling.","A":"Use AI Platform for distributed training.","D":"Use Kubeflow Pipelines to train on a Google Kubernetes Engine cluster."},"timestamp":"2021-06-09 11:49:00","unix_timestamp":1623232140,"answer_ET":"A","answers_community":["A (93%)","7%"],"topic":"1","discussion":[{"timestamp":"1640177460.0","poster":"maartenalexander","comment_id":"387855","content":"A. AI platform provides lower infrastructure overhead and allows you to not have to refactor your code too much (no containerization and such, like in KubeFlow).","upvote_count":"29"},{"timestamp":"1733501820.0","comment_id":"1225567","content":"Selected Answer: A\nA) AI Platform","upvote_count":"1","poster":"PhilipKoku"},{"poster":"girgu","upvote_count":"2","content":"The most suitable option for minimizing code refactoring and infrastructure overhead while enabling large-scale training on Google Cloud is:\n\nA. Use AI Platform for distributed training.\n* **Simplified Workflow:** AI Platform offers a managed service for training machine learning models. You can train your existing TensorFlow Estimator code with minimal changes, reducing the need for extensive code refactoring.\n* **Distributed Training:** AI Platform automatically handles distributing your training job across multiple machines, allowing you to leverage the power of Google's cloud infrastructure to train on large datasets efficiently.\n* **Reduced Infrastructure Overhead:** You don't need to manage the underlying infrastructure (e.g., setting up and maintaining a cluster) yourself. AI Platform takes care of all the infrastructure provisioning and management, minimizing the workload on your team.","timestamp":"1732643340.0","comment_id":"1219031"},{"poster":"fragkris","timestamp":"1717567200.0","upvote_count":"2","comment_id":"1088258","content":"Selected Answer: A\nI chose A. Even though D is a working option, it requires us to create a GKE cluster, which requires more work."},{"upvote_count":"1","timestamp":"1715794920.0","comment_id":"1071824","content":"Selected Answer: A\nA - because it has native support for TF","poster":"Sum_Sum"},{"content":"Selected Answer: A\nA. Use AI Platform for distributed training. : Managed , low infra change migration: yes , although need code refactoring to bigquery sql\nB. Create a cluster on Dataproc for training. : only cluster ? what about training?\nC. Create a Managed Instance Group with autoscaling. : Same Q?\nD. Use Kubeflow Pipelines to train on a Google Kubernetes Engine cluster : only training?","upvote_count":"2","timestamp":"1705161780.0","comment_id":"950719","poster":"harithacML"},{"timestamp":"1699513500.0","poster":"M25","comment_id":"892716","upvote_count":"1","content":"Selected Answer: A\nWent with A"},{"comment_id":"825030","upvote_count":"1","timestamp":"1693233180.0","poster":"Fatiy","content":"Selected Answer: A\nOption A is the best choice as AI Platform provides a distributed training framework, enabling you to train large-scale models faster and with less effort"},{"content":"Selected Answer: A\nusing options eliminations answer between A,D will vote for A as it is easier","upvote_count":"1","poster":"Mohamed_Mossad","comment_id":"615895","timestamp":"1670960040.0"},{"upvote_count":"2","content":"Selected Answer: A\nThe answer is A. AI platform also contains kubeflow pipelines. you don't need to set up infrastructure to use it. For D you need to set up a kubernetes cluster engine. The question asks us to minimize infrastructure overheard.","comment_id":"599252","timestamp":"1668029280.0","poster":"David_ml"},{"upvote_count":"1","comment_id":"585813","comments":[{"content":"The answer is A. AI platform also contains kubeflow pipelines. you don't need to set up infrastructure to use it. For D you need to set up a kubernetes cluster engine. The question asks us to minimize infrastructure overheard.","timestamp":"1668029160.0","poster":"David_ml","comment_id":"599250","upvote_count":"2"}],"poster":"mmona19","timestamp":"1665756120.0","content":"Selected Answer: D\nD- Kubeflow pipelines with Vertex ai provides you ability to reuse existing code using a TF conatiner in a pipeline. it helps automate the process. there is a qwiklab walking through this. \nA-incorrect, question is asking resuse existing code with minimum changes. distributed deployment does not address that."},{"content":"A - better to go with managed service and distributed","poster":"A4M","comment_id":"531887","timestamp":"1658721060.0","upvote_count":"2"},{"content":"I am 100% sure that the answer is D.\nKubeflow pipelines were designed keeping:\n\nA) Portability.\nB) Composability.\nC) Flexibility in mind. \n\nThis is the pain point that the kubeflow pipelines address","timestamp":"1657967220.0","comments":[{"upvote_count":"2","poster":"David_ml","comment_id":"599251","content":"The answer is A. AI platform also contains kubeflow pipelines. you don't need to set up infrastructure to use it. For D you need to set up a kubernetes cluster engine. The question asks us to minimize infrastructure overheard.","timestamp":"1668029220.0"}],"poster":"DHEEPAK","comment_id":"524936","upvote_count":"1"},{"poster":"NamitSehgal","upvote_count":"3","content":"Selected Answer: A\nTensorFlow Estimators means require distributed and that is key feature for AI platform or later Vertex AI.","comment_id":"514509","timestamp":"1656673020.0"},{"content":"I think is A","poster":"JobQ","comment_id":"505663","upvote_count":"1","timestamp":"1655748600.0"},{"content":"I think the answer is either A or B, but personally think it is likely B because dataproc is a common tool box on GCP used for ML while AI platform might require refactoring. However, I dont really know A or B","comments":[{"upvote_count":"5","content":"Another vote for answer A. AI Platform distributed training here. \n\nHowever, I wanted to share my logic why its not B as well. Dataproc is a managed Hadoop and as such needs a processing engine for ML tasks. Most likely Spark and SparkML. Now Spark code is quite different than pure Python and SparkML is even more different than TFcode. I imagine there might me a way to convert TF code to run on SparkML, but this seems a lot of work. And besides the question specifically wants us to minimize refactoring, so there you have it, we can eliminate option B 100%.","comment_id":"461489","timestamp":"1649846160.0","poster":"george_ognyanov"}],"comment_id":"449568","timestamp":"1647966000.0","poster":"q4exam","upvote_count":"3"},{"poster":"Danny2021","timestamp":"1646790300.0","upvote_count":"4","comment_id":"441683","content":"A. D involves more infra overhead."},{"poster":"salsabilsf","timestamp":"1639050540.0","comment_id":"378151","comments":[{"comment_id":"510504","poster":"GCP_Guru","content":"why?????","upvote_count":"1","timestamp":"1656343140.0"}],"upvote_count":"1","content":"Should be D"}],"answer_images":[],"answer_description":"","isMC":true},{"id":"WPIuNswfp5COFLH0aIw3","unix_timestamp":1623232320,"answer":"A","answer_ET":"A","choices":{"B":"Deploy and version the model on AI Platform.","C":"Use Dataflow with the SavedModel to read the data from BigQuery.","A":"Export the model to BigQuery ML.","D":"Submit a batch prediction job on AI Platform that points to the model location in Cloud Storage."},"exam_id":13,"topic":"1","answer_description":"","url":"https://www.examtopics.com/discussions/google/view/54967-exam-professional-machine-learning-engineer-topic-1-question/","question_images":[],"isMC":true,"question_id":237,"timestamp":"2021-06-09 11:52:00","discussion":[{"poster":"maartenalexander","upvote_count":"21","content":"A. You would want to minimize computational overhead–BigQuery minimizes such overhead","comments":[{"comments":[{"content":"you can import a TF model in BQ ML","timestamp":"1633836240.0","comments":[{"content":"agree. https://cloud.google.com/bigquery-ml/docs/making-predictions-with-imported-tensorflow-models","comment_id":"469524","timestamp":"1635467940.0","poster":"gcp2021go","upvote_count":"5"}],"comment_id":"459858","poster":"ms_lemon","upvote_count":"11"},{"upvote_count":"3","timestamp":"1689257400.0","content":"No need . This is a text classification problem. need to convert words to numbers and use a classifier.","comment_id":"950731","poster":"harithacML"}],"poster":"q4exam","upvote_count":"3","timestamp":"1632321060.0","comment_id":"449572","content":"BQML doesnt support NLP model"}],"timestamp":"1624359180.0","comment_id":"387858"},{"comment_id":"384810","timestamp":"1624019400.0","content":"I think it's A\nhttps://cloud.google.com/bigquery-ml/docs/making-predictions-with-imported-tensorflow-models#importing_models","upvote_count":"11","poster":"chohan"},{"poster":"bc3f222","content":"Selected Answer: A\nBQML can rum imported TF models","upvote_count":"1","comment_id":"1399129","timestamp":"1742099160.0"},{"upvote_count":"1","timestamp":"1736328000.0","comment_id":"1337879","poster":"Sivaram06","content":"Selected Answer: D\nNot option A because BigQuery ML can be useful for certain tasks, it might not be the most efficient for batch predictions with a custom TensorFlow model trained on AI Platform."},{"upvote_count":"1","comment_id":"1331943","comments":[{"comment_id":"1331947","upvote_count":"1","poster":"desertlotus1211","timestamp":"1735223100.0","content":"However BQ ML requires storing to Cloud Storage first. the question doesn't state this (should we assume?), which make Answer D better as it state cloud storage."}],"timestamp":"1735222860.0","content":"Selected Answer: D\nA is incorrect:\nBigQuery ML is used to train and deploy models directly within BigQuery, but it does not support importing and deploying external TensorFlow models.\nYou cannot export a TensorFlow model directly to BigQuery ML; AI Platform is the correct service for TensorFlow-based models.","poster":"desertlotus1211"},{"content":"Selected Answer: A\nA) BigQuery ML","comment_id":"1225569","timestamp":"1717684080.0","poster":"PhilipKoku","upvote_count":"1"},{"comments":[{"content":"in the option D, it just mentioned GCS , BQ is no where to be found","poster":"Jason_Cloud_at","upvote_count":"2","comment_id":"1272657","timestamp":"1724671560.0"}],"upvote_count":"1","comment_id":"1219036","content":"Selected Answer: D\nUse the gcloud command to submit a batch prediction job, specifying the model location in Cloud Storage and the BigQuery table as the input source.","timestamp":"1716739140.0","poster":"girgu"},{"upvote_count":"2","timestamp":"1710833640.0","comment_id":"1177057","content":"Selected Answer: A\nBquery to minimize computational overhead","poster":"Aastha_Vashist"},{"upvote_count":"1","poster":"MrTracer","timestamp":"1703699160.0","comment_id":"1107044","content":"Selected Answer: D\nWould go with D"},{"timestamp":"1700077500.0","poster":"Sum_Sum","content":"Selected Answer: A\nA - you can import TF models to BQ","upvote_count":"2","comment_id":"1071825"},{"poster":"harithacML","timestamp":"1689257640.0","comment_id":"950734","content":"Selected Answer: A\nModel : AI Platform. \npred batch data : BigQuery \nconstraint : computational overhead\n\nSame platform as data == less computation required to load and pass it to model","upvote_count":"2"},{"content":"Selected Answer: A\nminimize computational overhead–>BigQuery","comment_id":"945721","poster":"Liting","timestamp":"1688736240.0","upvote_count":"2"},{"content":"Not sure if when you have the saved model in Cloud storage that means that you don't use compute in vertex. I think that the option compute-free is bigquery","upvote_count":"1","timestamp":"1686211800.0","comment_id":"917994","poster":"Voyager2"},{"comment_id":"917980","poster":"Voyager2","timestamp":"1686211140.0","upvote_count":"1","content":"Not sure \nText Classification Using BigQuery ML and ML.NGRAMS\nhttps://medium.com/@jeffrey.james/text-classification-using-bigquery-ml-and-ml-ngrams-6e365f0b5505"},{"content":"Selected Answer: A\nI think D have extra compute on extrating data frm BQ","upvote_count":"2","comment_id":"902982","timestamp":"1684653060.0","poster":"rexduo"},{"timestamp":"1684339800.0","comment_id":"900343","poster":"Darshan12","upvote_count":"2","content":"There are some drawbacks to option D.\n\nCost: Submitting a batch prediction job on AI Platform is a paid service. The cost will depend on the size of the model and the amount of data that you are predicting.\nComplexity: Submitting a batch prediction job on AI Platform requires you to write some code. This can be a challenge if you are not familiar with AI Platform.\nPerformance: Submitting a batch prediction job on AI Platform may not be as efficient as using BigQuery ML. This is because AI Platform needs to load the model into memory before it can run the predictions.\nOverall, option D is a viable option, but it may not be the best option for all situations."},{"poster":"M25","comment_id":"892717","upvote_count":"1","timestamp":"1683608700.0","content":"Selected Answer: D\nWent with D"},{"content":"Selected Answer: C\nwhy not C?","timestamp":"1682389980.0","upvote_count":"1","poster":"lucaluca1982","comment_id":"879833"},{"timestamp":"1681992960.0","poster":"lucaluca1982","content":"Selected Answer: C\nwhat about C?","comments":[{"comments":[{"timestamp":"1700997900.0","comment_id":"1080590","upvote_count":"1","poster":"king31","content":"Although it's more complex, the question doesn't imply any restrictions on complexity, only computational overheard"}],"upvote_count":"2","poster":"tavva_prudhvi","content":"This is an option that can be used to minimize computational overhead, but it is more complex to set up and requires you to have Dataflow installed.","comment_id":"940716","timestamp":"1688290740.0"}],"upvote_count":"1","comment_id":"875563"},{"upvote_count":"1","comment_id":"868969","content":"Selected Answer: D\nD is more straightforward","timestamp":"1681348560.0","poster":"lucaluca1982"},{"content":"is it A or D?","upvote_count":"1","timestamp":"1681076040.0","comment_id":"865831","poster":"studybrew"},{"poster":"tavva_prudhvi","upvote_count":"1","timestamp":"1678287660.0","comment_id":"833079","content":"In this document(https://cloud.google.com/vertex-ai/docs/tabular-data/classification-regression/get-batch-predictions), it mentions \"For classification or regression models, you can provide input data in one of two formats: BigQuery tables, CSV objects in Cloud Storage.\n\nNow, is it A/D?"},{"comment_id":"825042","poster":"Fatiy","upvote_count":"1","content":"Selected Answer: D\nTo perform batch predictions on text data stored in BigQuery using a trained TensorFlow model, you can submit a batch prediction job on AI Platform. The batch prediction job reads the input data from BigQuery and the model from Cloud Storage. This approach minimizes computational overhead since the job is handled by AI Platform, and it allows you to easily scale up or down depending on the size of the data.","timestamp":"1677602760.0"},{"upvote_count":"2","comments":[{"content":"The data is not on Cloud Storage.","poster":"neochaotic","comment_id":"847628","upvote_count":"1","timestamp":"1679531460.0"}],"comment_id":"824372","timestamp":"1677557160.0","content":"Selected Answer: D\nA is the correct in case you have ml model from local development, it is answer based on obsolete technology\nbut I insist on D , it is best choice . Given that you have trained model on VertextAI ,why you waste your time importing it to BigQueryML? Vertext-AI accommodates all things to make batch predictions.\nhttps://cloud.google.com/vertex-ai/docs/tabular-data/classification-regression/get-batch-predictions","poster":"John_Pongthorn"},{"poster":"shankalman717","comment_id":"816466","upvote_count":"2","timestamp":"1676977200.0","content":"Selected Answer: D\nYou can use AI Platform Prediction to deploy and version the model, but this is not necessary for batch predictions. Instead, you can simply upload the trained model to a Cloud Storage bucket and then point the batch prediction job to that location. This will allow you to perform predictions on large volumes of data while minimizing computational overhead."},{"comment_id":"615901","upvote_count":"1","content":"Selected Answer: A\nhttps://cloud.google.com/bigquery-ml/docs/making-predictions-with-imported-tensorflow-models#bq","timestamp":"1655142420.0","poster":"Mohamed_Mossad"},{"content":"Selected Answer: A\nCREATE OR REPLACE MODEL example_dataset.imported_tf_model\n OPTIONS (MODEL_TYPE='TENSORFLOW',\n MODEL_PATH='gs://cloud-training-demos/txtclass/export/exporter/1549825580/*')","upvote_count":"3","poster":"NamitSehgal","timestamp":"1641042300.0","comment_id":"514511"},{"content":"You can call TF models from BQML, so A.","timestamp":"1640031060.0","comment_id":"505664","poster":"JobQ","upvote_count":"2"},{"poster":"kaike_reis","upvote_count":"2","comments":[{"upvote_count":"1","poster":"tavva_prudhvi","timestamp":"1678287720.0","comment_id":"833080","content":"In this document(https://cloud.google.com/vertex-ai/docs/tabular-data/classification-regression/get-batch-predictions), it mentions \"For classification or regression models, you can provide input data in one of two formats: BigQuery tables, CSV objects in Cloud Storage."}],"timestamp":"1636889880.0","content":"A is the correct. The required job would be to save your TF model into GS. \nD is not correct. Yes, AI Platform has a batch prediction job, but you need to have input files into GS, so you would have to move your BQ table to a csv file for example, which is impractical.","comment_id":"478081"},{"upvote_count":"2","comment_id":"469525","timestamp":"1635468000.0","content":"I think a good mental model is - when you can solve problem with BQML, stay with BQML.","poster":"gcp2021go"},{"comment_id":"456076","upvote_count":"1","timestamp":"1633178100.0","content":"should be B. Bigquery doesnt support for NLP text classification models. In D trained model load from cloud storage which is not a option with cloud storage https://cloud.google.com/ai-platform/training/docs/working-with-cloud-storage","poster":"senura96"},{"content":"Should be D","poster":"salsabilsf","comments":[{"comment_id":"441633","timestamp":"1631132700.0","poster":"aj2aj2","content":"Why d ,Any explanations","comments":[{"upvote_count":"2","comment_id":"449569","content":"There is a batch prediction API on AI platform....","timestamp":"1632320580.0","poster":"q4exam","comments":[{"comment_id":"478080","upvote_count":"1","poster":"kaike_reis","content":"You are correct, but look at the documentation: https://cloud.google.com/ai-platform/prediction/docs/batch-predict. The input must be located in GS, so you need the work to move your table from BQ to GS, which is not practical","comments":[{"comment_id":"644795","upvote_count":"1","content":"on Vertex AI doc, you can use BQ as input:\nhttps://cloud.google.com/vertex-ai/docs/tabular-data/classification-regression/get-batch-predictions","poster":"felden","timestamp":"1660109520.0"}],"timestamp":"1636889580.0"}]}],"upvote_count":"1"}],"timestamp":"1623232320.0","comment_id":"378153","upvote_count":"2"}],"answers_community":["A (55%)","D (38%)","7%"],"answer_images":[],"question_text":"You have trained a text classification model in TensorFlow using AI Platform. You want to use the trained model for batch predictions on text data stored in\nBigQuery while minimizing computational overhead. What should you do?"},{"id":"WNAUPrtfponjw3h6ycTb","answer_images":[],"isMC":true,"answer_ET":"C","answers_community":["C (89%)","11%"],"question_id":238,"choices":{"C":"Configure a Cloud Storage trigger to send a message to a Pub/Sub topic when a new file is available in a storage bucket. Use a Pub/Sub-triggered Cloud Function to start the training job on a GKE cluster.","A":"Configure your pipeline with Dataflow, which saves the files in Cloud Storage. After the file is saved, start the training job on a GKE cluster.","B":"Use App Engine to create a lightweight python client that continuously polls Cloud Storage for new files. As soon as a file arrives, initiate the training job.","D":"Use Cloud Scheduler to schedule jobs at a regular interval. For the first step of the job, check the timestamp of objects in your Cloud Storage bucket. If there are no new files since the last run, abort the job."},"question_text":"You work with a data engineering team that has developed a pipeline to clean your dataset and save it in a Cloud Storage bucket. You have created an ML model and want to use the data to refresh your model as soon as new data is available. As part of your CI/CD workflow, you want to automatically run a Kubeflow\nPipelines training job on Google Kubernetes Engine (GKE). How should you architect this workflow?","question_images":[],"exam_id":13,"discussion":[{"comment_id":"391059","content":"C\nhttps://cloud.google.com/architecture/architecture-for-mlops-using-tfx-kubeflow-pipelines-and-cloud-build#triggering-and-scheduling-kubeflow-pipelines","poster":"Paul_Dirac","upvote_count":"16","timestamp":"1640516160.0"},{"poster":"ori5225","content":"On a schedule, using Cloud Scheduler.\nResponding to an event, using Pub/Sub and Cloud Functions. For example, the event can be the availability of new data files in a Cloud Storage bucket.","timestamp":"1644672480.0","upvote_count":"1","comment_id":"423573","comments":[{"poster":"tavva_prudhvi","timestamp":"1704196020.0","content":"Option D requires the job to be scheduled at regular intervals, even if there are no new files. This can waste resources and lead to unnecessary delays in the training process.","upvote_count":"1","comment_id":"940722"}]},{"comment_id":"1225571","upvote_count":"1","content":"Selected Answer: C\nC) PUB/sub trigger from Cloud Storage & Cloud Function","timestamp":"1733502780.0","poster":"PhilipKoku"},{"timestamp":"1717569180.0","poster":"fragkris","upvote_count":"1","comment_id":"1088288","content":"Selected Answer: C\nC - This is the google reccomended method."},{"timestamp":"1715795220.0","content":"Selected Answer: C\nC- because you don't want to re-engineer the pipeline","comment_id":"1071827","poster":"Sum_Sum","upvote_count":"1"},{"upvote_count":"1","timestamp":"1699513500.0","content":"Selected Answer: C\nWent with C","poster":"M25","comment_id":"892718"},{"poster":"Fatiy","upvote_count":"1","content":"Selected Answer: C\nThe scenario involves automatically running a Kubeflow Pipelines training job on GKE as soon as new data becomes available. To achieve this, we can use Cloud Storage to store the cleaned dataset, and then configure a Cloud Storage trigger that sends a message to a Pub/Sub topic whenever a new file is added to the storage bucket. We can then create a Pub/Sub-triggered Cloud Function that starts the training job on a GKE cluster.","timestamp":"1693234200.0","comment_id":"825047"},{"content":"Selected Answer: A\nThe question says: As part of your CI/CD workflow, you want to automatically run a Kubeflow..\n\nC is also an option but it seems more cumbersome. \nOne thing hat could be against A is that the data engineering team is separate team so they might not access your CI/CD if any changes from their side is needed..","timestamp":"1688488680.0","poster":"behzadsw","comment_id":"765993","comments":[],"upvote_count":"1"},{"poster":"hiromi","content":"Selected Answer: C\nC\nPubsub is the keyword","comment_id":"739542","timestamp":"1686256680.0","upvote_count":"2"},{"timestamp":"1673282040.0","upvote_count":"1","poster":"Mohamed_Mossad","content":"Selected Answer: C\nevent driven architecture is better than polling based architecure so I will vote for C","comment_id":"629210"}],"topic":"1","unix_timestamp":1624697760,"timestamp":"2021-06-26 10:56:00","answer":"C","answer_description":"","url":"https://www.examtopics.com/discussions/google/view/56104-exam-professional-machine-learning-engineer-topic-1-question/"},{"id":"fjlcJvuwyHp6kI21Cpjx","question_text":"You want to rebuild your ML pipeline for structured data on Google Cloud. You are using PySpark to conduct data transformations at scale, but your pipelines are taking over 12 hours to run. To speed up development and pipeline run time, you want to use a serverless tool and SQL syntax. You have already moved your raw data into Cloud Storage. How should you build the pipeline on Google Cloud while meeting the speed and processing requirements?","question_images":[],"topic":"1","discussion":[{"poster":"nunzio144","timestamp":"1726853460.0","comment_id":"411618","content":"It should be D .... Data Fusion is not SQL syntax ....","upvote_count":"22","comments":[{"timestamp":"1631109240.0","poster":"q4exam","content":"Agree, BQ is the only serverless that support SQL","comment_id":"441488","upvote_count":"4"},{"upvote_count":"1","comment_id":"528157","timestamp":"1642656120.0","poster":"A4M","content":"Needs to be D as the most suitable answer given the req's in question Datafusion is more of a no code Data transformation tool"}]},{"timestamp":"1626646620.0","upvote_count":"12","comments":[{"timestamp":"1723795260.0","upvote_count":"1","poster":"TornikePirveli","comment_id":"1266902","content":"By your logic it should be D, because BQ is fully serverless and supports SQL"},{"comments":[{"timestamp":"1678089720.0","upvote_count":"2","poster":"tavva_prudhvi","comment_id":"830626","content":"I think you're only viewing the sentence \"A serverless approach leveraging the scalability and reliability of Google services like Dataproc means Data Fusion offers the best of data integration capabilities with a lower total cost of ownership\", The sentence implies that Data Fusion leverages a serverless approach, but it does not explicitly state that Data Fusion itself is serverless. It states that Data Fusion offers the best of data integration capabilities by using a serverless approach that leverages the scalability and reliability of Google services like Dataproc. So, while Data Fusion may not be fully serverless, it is designed to take advantage of serverless capabilities through its integration with Google services."}],"timestamp":"1634556600.0","comment_id":"464086","content":"Data Fusion is serverless: https://cloud.google.com/data-fusion#all-features","poster":"mousseUwU","upvote_count":"3"},{"poster":"mousseUwU","content":"Agree, A is correct","comment_id":"464084","upvote_count":"2","timestamp":"1634556540.0"}],"comment_id":"409184","content":"ANS: A\nhttps://cloud.google.com/data-fusion#section-1\n- Data Fusion is a serverless approach leveraging the scalability and reliability of Google services like Dataproc means Data Fusion offers the best of data integration capabilities with a lower total cost of ownership.\n- BigQuery is serverless and supports SQL. \n- Dataproc is not serverless, you have to manage clusters. \n- Cloud SQL is not serverless, you have to manage instances.","poster":"Celia20210714"},{"upvote_count":"2","comment_id":"1336073","timestamp":"1735919700.0","content":"Selected Answer: B\nBoth B and D are vaild IMHO (SQL + Severless https://cloud.google.com/dataproc-serverless/docs/overview), but the book Official Google Cloud Certified Professional Machine Learning Engineer Study Guide says B (I think there several errors in the book)","poster":"manualrg"},{"content":"Selected Answer: D\nPeople giving other answers are to hang up on the fact that it currently runs in PySpark. The data is in GCS, you want quick serverless solution and use SQL syntax - BigQuery is the only good option that \"meets the speed and processing requirements\".","upvote_count":"2","poster":"joqu","timestamp":"1731855600.0","comment_id":"1313591"},{"poster":"LeumaS_NoswaY","timestamp":"1726593780.0","content":"B. You need Cloud Dataproc to transform the data from PySpark to Spark SQL","upvote_count":"1","comment_id":"1285379"},{"timestamp":"1723795020.0","upvote_count":"3","poster":"TornikePirveli","content":"Serverless, SQL syntax -> BigQuery, simple as that","comment_id":"1266900"},{"upvote_count":"2","timestamp":"1722628980.0","content":"I am very curious. Why are the solutions (when I click Reveal Solution) generally WRONG?","comment_id":"1260015","poster":"jsalvasoler"},{"comment_id":"1248865","timestamp":"1721131140.0","content":"option D because needs a serveless solution and sql sintax and BigQuery offer this. Datarproc is not serverless, so B is incorrect, D is correct option.","poster":"tadeupan","upvote_count":"2"},{"timestamp":"1720442940.0","upvote_count":"2","comment_id":"1244334","content":"Selected Answer: D\nThere's an updated version of this question in the official Google Cloud certified PMLE study guide. Option D is marked as correct","comments":[{"comment_id":"1266904","poster":"TornikePirveli","timestamp":"1723795320.0","upvote_count":"1","content":"Can you link the updated version? On Amazon it's still 1st version and marked B"}],"poster":"Yorko"},{"poster":"PhilipKoku","upvote_count":"2","content":"Selected Answer: D\nThe best approach is option D: Ingest data into BigQuery and use SQL queries for transformations. This leverages BigQuery’s serverless capabilities, efficient processing, and seamless integration with other Google Cloud services.","timestamp":"1717606860.0","comment_id":"1224869"},{"upvote_count":"1","poster":"fragkris","comment_id":"1085161","timestamp":"1701428520.0","content":"Selected Answer: D\nD - BigQuery is the only serverless and SQL-syntax option."},{"poster":"Sum_Sum","upvote_count":"2","timestamp":"1699968420.0","content":"Selected Answer: D\nD - as BQ is server less and supports SQL\nnone of the other options match both criteria","comment_id":"1070400"},{"timestamp":"1688771640.0","upvote_count":"1","content":"Selected Answer: D\nI'll go with D.","poster":"12112","comment_id":"946043"},{"timestamp":"1683607980.0","poster":"M25","upvote_count":"3","comment_id":"892677","content":"Selected Answer: D\nWent with D"},{"poster":"asava","comments":[{"poster":"TornikePirveli","content":"But using dataproc is not serverless, so answer should be D","comment_id":"1266906","timestamp":"1723795380.0","upvote_count":"1"}],"comment_id":"839129","content":"Selected Answer: B\nBQ is the serverless solution","timestamp":"1678818540.0","upvote_count":"3"},{"poster":"mellowed","timestamp":"1673678880.0","upvote_count":"1","comment_id":"775149","content":"Correct option is D"},{"poster":"ssaporylo","upvote_count":"1","timestamp":"1673437680.0","content":"Vote D","comment_id":"772447"},{"comment_id":"767543","poster":"ares81","upvote_count":"1","timestamp":"1673002740.0","content":"Selected Answer: A\nIt should be A."},{"content":"Selected Answer: D\nData Fusion is not in SQL syntax, so no A;\nDataproc is not serverless, so no B;\nPassing through Cloud SQL is uselss, just go with BigQuery, so no C;\nD is correct","comment_id":"725163","poster":"EFIGO","timestamp":"1669210620.0","upvote_count":"4","comments":[{"poster":"Franui","comment_id":"1322726","upvote_count":"1","timestamp":"1733487660.0","content":"but pyspark can run serverless now"}]},{"content":"C,D booth can be implemented as will work\nbut D is faster for implementation","poster":"abhi0706","upvote_count":"2","timestamp":"1667210400.0","comment_id":"708326"},{"comment_id":"647165","upvote_count":"2","poster":"GCP72","timestamp":"1660565280.0","content":"Selected Answer: D\nCorrect answer is \"D\""},{"poster":"alejo_1053","upvote_count":"4","content":"Selected Answer: B\nI was thinking B, but now I'm kind of confused that nobody voted it","comment_id":"641287","timestamp":"1659447660.0"},{"content":"Selected Answer: D\n\"write sql syntax\" will drop A , as datafusion is drag drop tool\n\"serveless' will drop B as dataproc is not serverless\nanswer will be in C,D booth can be implemented as will work\nbut D is faster for implementation","comment_id":"615261","upvote_count":"2","timestamp":"1655029740.0","poster":"Mohamed_Mossad"},{"upvote_count":"2","poster":"David_ml","content":"Selected Answer: D\nD is correct","timestamp":"1649823060.0","comment_id":"584988"},{"poster":"morgan62","upvote_count":"2","timestamp":"1649214120.0","comment_id":"581566","comments":[{"content":"For these questions, you have to choose the answer that requires least effort. yes B is a viable option since u can set up dataproc to be serverless. However D is the right answer since it requires least effort and time.","timestamp":"1652125620.0","poster":"David_ml","upvote_count":"2","comment_id":"599264"}],"content":"I think the answer is B...\nBut feeling very frustrated after seeing no one choosing B as the answer."},{"timestamp":"1647168060.0","upvote_count":"2","poster":"giaZ","content":"Selected Answer: D\nThe question asks you to rebuild your pipeline in the Cloud, using SQL syntax. Why would you do it in Data Fusion then? I think it clearly wants you to convert your code into SLQ queries and do everything in BQ.","comment_id":"566729"},{"poster":"lordcenzin","content":"Selected Answer: A\nme too...i think it is A, fusion is serverless as it is BQ and BQ supports sql. moreover their integration is full","upvote_count":"1","timestamp":"1645094580.0","comment_id":"549286"},{"poster":"tangwei","comment_id":"540901","content":"D is the right answer.","timestamp":"1644059460.0","upvote_count":"2"},{"comment_id":"537711","poster":"Sylh","upvote_count":"1","content":"I think it should be A as well","timestamp":"1643701860.0"},{"content":"another vote for A","upvote_count":"1","timestamp":"1643206020.0","poster":"sid515","comment_id":"532937"},{"content":"Selected Answer: D\nD should be correct as I have worked with Cloud Fusion and it is not very nice tool. \nGoogle has bought this software and support for this tool is not good. \nSQL can work in Cloud fusion pipelines too but I would prefer to use a single tool like Bigquery to both transform and store data.","poster":"NamitSehgal","comment_id":"518683","upvote_count":"1","timestamp":"1641516180.0"},{"poster":"MisterHairy","comment_id":"507154","timestamp":"1640183160.0","upvote_count":"3","content":"=New Question4=\nYou are an ML engineer in the contact center of a large enterprise. You need to build a sentiment analysis tool that predicts customer sentiment from recorded phone conversations. You need to identify the best approach to building a model while ensuring that the gender, age, and cultural differences of the customers who called the contact center do not impact any stage of the model development pipeline and results. What should you do? \n\nA. Convert the speech to text and extract sentiments based on the sentences\nB. Convert the speech to text and extract sentiment using syntactical analysis\nC. Extract sentiment directly from the voice recordings\nD. Convert the speech to text and build a model based on the words","comments":[{"upvote_count":"1","comment_id":"514598","content":"Probably go with Option A","timestamp":"1641056880.0","poster":"Blueocean"},{"poster":"A4M","upvote_count":"6","timestamp":"1642656060.0","comment_id":"528156","content":"Answer - A \nB - extract sentiment using syntactical analysis - overkill\nC - not sure how you can do this i.e. direct from voice recording even if you can it's not ideal\nD - not a good idea to build a model to capture sentiment based on just words - no context etc"},{"poster":"MisterHairy","upvote_count":"1","timestamp":"1640183460.0","content":"Answer?","comment_id":"507163"},{"upvote_count":"1","timestamp":"1671501600.0","comment_id":"750391","poster":"evilmaax","content":"Option B. \nIts not the A, because based on sentences you will be fouled by the gender, for example."}]},{"upvote_count":"2","timestamp":"1639319700.0","comment_id":"500060","content":"A. Use Data Fusionג€™s GUI to build the transformation pipelines, and then write the data into BigQuery. - wrong because 1. datafusion is used for data transformation and not for analytics. 2. writing data into BQ is redundant when you can write data directly to BQ\n\nB. Convert your PySpark into SparkSQL queries to transform the data, and then run your pipeline on Dataproc to write the data into BigQuery. - Wrong because dataproc is not serverless.\n\nC. Ingest your data into Cloud SQL, convert your PySpark commands into SQL queries to transform the data, and then use federated queries from BigQuery for machine learning. 0 wrong because 1. cloud SQL requires server 2. it is meant for OLTP not OLAP processing. 3. federated queryng is unnecessary when data itself can be stored in BQ.","poster":"ashii007"},{"timestamp":"1638785880.0","upvote_count":"2","comment_id":"495035","poster":"alphard","content":"D is correct.\n\n\"serverless tool and SQL syntax\" means BigQuery."},{"comment_id":"487562","content":"D is correct","upvote_count":"1","timestamp":"1637951820.0","poster":"JobQ"},{"comment_id":"442824","timestamp":"1631337840.0","content":"D\n\nBigQuery is a serverless tool with SQL syntax that scales to the moon.","poster":"gcper","upvote_count":"2"},{"poster":"ralf_cc","content":"A - https://cloud.google.com/data-fusion/docs/concepts/overview","comments":[{"timestamp":"1626436140.0","content":"not A - Question asks \"...you want to use a serverless tool and SQL syntax.\"","upvote_count":"3","poster":"cetanx","comment_id":"407838"}],"timestamp":"1625740500.0","upvote_count":"2","comment_id":"401786"}],"url":"https://www.examtopics.com/discussions/google/view/57450-exam-professional-machine-learning-engineer-topic-1-question/","unix_timestamp":1625740500,"answer_ET":"D","exam_id":13,"answers_community":["D (70%)","B (24%)","5%"],"answer_description":"","choices":{"D":"Ingest your data into BigQuery using BigQuery Load, convert your PySpark commands into BigQuery SQL queries to transform the data, and then write the transformations to a new table.","C":"Ingest your data into Cloud SQL, convert your PySpark commands into SQL queries to transform the data, and then use federated queries from BigQuery for machine learning.","A":"Use Data Fusion's GUI to build the transformation pipelines, and then write the data into BigQuery.","B":"Convert your PySpark into SparkSQL queries to transform the data, and then run your pipeline on Dataproc to write the data into BigQuery."},"question_id":239,"timestamp":"2021-07-08 12:35:00","isMC":true,"answer_images":[],"answer":"D"},{"id":"t6h6JPAvtirWjlZY0x1u","answer_images":[],"question_images":[],"answers_community":["CE (56%)","CD (23%)","8%","5%"],"question_text":"You have a functioning end-to-end ML pipeline that involves tuning the hyperparameters of your ML model using AI Platform, and then using the best-tuned parameters for training. Hypertuning is taking longer than expected and is delaying the downstream processes. You want to speed up the tuning job without significantly compromising its effectiveness. Which actions should you take? (Choose two.)","question_id":240,"choices":{"C":"Set the early stopping parameter to TRUE.","D":"Change the search algorithm from Bayesian search to random search.","B":"Decrease the range of floating-point values.","E":"Decrease the maximum number of trials during subsequent training phases.","A":"Decrease the number of parallel trials."},"timestamp":"2021-06-26 16:19:00","topic":"1","isMC":true,"answer":"CE","discussion":[{"poster":"gcp2021go","upvote_count":"20","comment_id":"418458","timestamp":"1627865460.0","content":"I think should CE. I can't find any reference regarding B can reduce tuning time."},{"poster":"Paul_Dirac","timestamp":"1624717140.0","upvote_count":"16","content":"Answer: B & C (Ref: https://cloud.google.com/ai-platform/training/docs/using-hyperparameter-tuning)\n(A) Decreasing the number of parallel trials will increase tuning time.\n(D) Bayesian search works better and faster than random search since it's selective in points to evaluate and uses knowledge of previouls evaluated points.\n(E) maxTrials should be larger than 10*the number of hyperparameters used. And spanning the whole minimum space (10*num_hyperparams) already takes some time. So, lowering maxTrials has little effect on reducing tuning time.","comments":[{"timestamp":"1717692180.0","comment_id":"1225636","upvote_count":"1","poster":"Goosemoose","content":"Bayesian search should cost more time, because it can converge in fewer iterations than the other algorithms but not necessarily in a faster time because trials are dependent and thus require sequentiality"},{"comment_id":"438937","poster":"dxxdd7","timestamp":"1630736700.0","content":"In your link, when they mentionned maxTrials they said that \"In most cases there is a point of diminishing returns after which additional trials have little or no effect on the accuracy\"\nThey also say that it can affect time and cost\nI think i'd rather go with CE","upvote_count":"10"}],"comment_id":"391311"},{"upvote_count":"1","comment_id":"1399128","content":"Selected Answer: CE\napart from early stopping which no one has doubt about E (reduce max trails has the lowest propensity to reduce performance)","timestamp":"1742098980.0","poster":"bc3f222"},{"timestamp":"1736800980.0","content":"Selected Answer: BC\nDecreasing the range of floating-point values reduces the search space for the hyperparameter tuning process. A smaller search space allows the algorithm to converge faster to an optimal solution by focusing only on a narrower range of values.\nThis approach speeds up tuning without significantly compromising effectiveness, as the range is constrained to more reasonable values.\nWhy C is correct:\nSetting the early stopping parameter to TRUE enables the tuning process to stop trials early if it becomes clear that a given trial is not improving or yielding promising results.\nThis prevents unnecessary computation and saves time by discarding underperforming configurations early in the process.","poster":"vinevixx","comment_id":"1340049","upvote_count":"1"},{"upvote_count":"1","timestamp":"1735445700.0","content":"Selected Answer: CE\nC & E are the choices","comment_id":"1333315","poster":"Ankit267"},{"timestamp":"1723797300.0","content":"In the PMLE book it's grid search instead of Bayesian search and that makes sense, but there is also marked Decrease the number of parallel trials as correct answer, which I think should be wrong.","upvote_count":"1","comment_id":"1266920","poster":"TornikePirveli"},{"comment_id":"1260905","timestamp":"1722826980.0","content":"Selected Answer: AB\nWith Vertex AI hyperparameter tuning, you can configure the number of trials and the search algorithm as well as range of parameters.","poster":"nktyagi","upvote_count":"1"},{"poster":"PhilipKoku","comment_id":"1225574","timestamp":"1717684680.0","content":"Selected Answer: CD\nC) and D)","upvote_count":"2"},{"upvote_count":"3","timestamp":"1713103440.0","poster":"pinimichele01","comment_id":"1195556","content":"Selected Answer: CE\nsee pawan94"},{"comment_id":"1115991","poster":"pawan94","upvote_count":"4","timestamp":"1704643260.0","content":"C and E, if you reference the latest docs of hptune job on vertex ai :\n1. A not possible (refer: https://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning#:~:text=the%20benefit%20of%20reducing%20the%20time%20the) , if you reduce the number of parallel trials then the speed of overall completion gets negatively affected. \n. The question is about how to speed up the process but not changing the model params. Changing the optimization algorithm would lead to unexpected results.\n\nSo in my opinion C and E ( after carefully reading the updated docs) and please don't believe everything CHATGPT says . I encountered so many questions where the LLM's are giving completely wrong answers"},{"comment_id":"1088290","upvote_count":"3","poster":"fragkris","timestamp":"1701765420.0","content":"Selected Answer: CD\nI chose C and D"},{"timestamp":"1700077920.0","upvote_count":"3","comment_id":"1071831","poster":"Sum_Sum","content":"Selected Answer: CD\nChat GPT says:\n. Set the early stopping parameter to TRUE.\n\nEarly Stopping: Enabling early stopping allows the tuning process to terminate a trial if it becomes clear that it's not producing promising results. This prevents wasting time on unpromising trials and can significantly speed up the hyperparameter tuning process. It helps to focus resources on more promising parameter combinations.\nD. Change the search algorithm from Bayesian search to random search.\n\nRandom Search Algorithm: Random search, as opposed to Bayesian optimization, doesn't attempt to build a model of the objective function. While Bayesian search can be more efficient in finding the optimal parameters, random search is often faster per iteration. Random search can be particularly effective when the hyperparameter space is large, as it doesn't require as much computational power to select the next set of parameters to evaluate."},{"comment_id":"916974","upvote_count":"4","content":"Selected Answer: CE\nC&E\nThis video explains very well the max trials and parallel trials\nhttps://youtu.be/8hZ_cBwNOss\nThis link explains early stopping\nSee https://cloud.google.com/ai-platform/training/docs/using-hyperparameter-tuning#early-stopping","poster":"Voyager2","timestamp":"1686122160.0"},{"content":"Selected Answer: CE\nA increase time, B HP tuning job normally bottle neck is not at model size, D did reduce time, but might significantly hurt effectiveness","poster":"rexduo","timestamp":"1684654140.0","upvote_count":"2","comment_id":"902994"},{"poster":"CloudKida","upvote_count":"1","timestamp":"1683619560.0","comment_id":"892913","content":"Selected Answer: AC\nRunning parallel trials has the benefit of reducing the time the training job takes (real time—the total processing time required is not typically changed). However, running in parallel can reduce the effectiveness of the tuning job overall. That is because hyperparameter tuning uses the results of previous trials to inform the values to assign to the hyperparameters of subsequent trials. When running in parallel, some trials start without having the benefit of the results of any trials still running.\nYou can specify that AI Platform Training must automatically stop a trial that has become clearly unpromising. This saves you the cost of continuing a trial that is unlikely to be useful.\n\nTo permit stopping a trial early, set the enableTrialEarlyStopping value in the HyperparameterSpec to TRUE."},{"content":"Selected Answer: CE\nWent with C & E","poster":"M25","comment_id":"892719","upvote_count":"2","timestamp":"1683608700.0"},{"timestamp":"1680243000.0","poster":"kucuk_kagan","content":"Selected Answer: AD\nTo speed up the tuning job without significantly compromising its effectiveness, you can take the following actions:\n\nA. Decrease the number of parallel trials: By reducing the number of parallel trials, you can limit the amount of computational resources being used at a given time, which may help speed up the tuning job. However, reducing the number of parallel trials too much could limit the exploration of the parameter space and result in suboptimal results.\n\nD. Change the search algorithm from Bayesian search to random search: Bayesian optimization is a computationally intensive method that requires more time and resources than random search. By switching to a simpler method like random search, you may be able to speed up the tuning job without compromising its effectiveness. However, random search may not be as efficient in finding the best hyperparameters as Bayesian optimization.","comment_id":"856650","upvote_count":"1"},{"timestamp":"1679640540.0","poster":"Yajnas_arpohc","content":"Selected Answer: DE\nEarly stopping is for training, not hyperparameter tuning","upvote_count":"1","comment_id":"849067"},{"content":"Selected Answer: AD\nThe two actions that can speed up hyperparameter tuning without compromising effectiveness are decreasing the number of parallel trials and changing the search algorithm from Bayesian search to random search.","upvote_count":"2","timestamp":"1677603720.0","comment_id":"825055","poster":"Fatiy"},{"content":"Selected Answer: CD\nB. Decrease the range of floating-point values: Reducing the range of the hyperparameters will decrease the search space and the time it takes to find the optimal hyperparameters. However, if the range is too narrow, it may not be possible to find the best hyperparameters.\n\nC. Set the early stopping parameter to TRUE: Setting the early stopping parameter to true will stop the trial when the performance has stopped improving. This will help to reduce the number of trials needed and thus speed up the hypertuning job without compromising its effectiveness.\nD.Changing the search algorithm from Bayesian search to random search could also be a valid action to speed up the hypertuning job. Random search can explore the hyperparameter space more efficiently and with less computation cost compared to Bayesian search, especially when the search space is large and complex. However, it may not be as effective as Bayesian search in finding the best hyperparameters in some cases.","poster":"shankalman717","comment_id":"816493","timestamp":"1676979300.0","comments":[{"comment_id":"833113","upvote_count":"1","timestamp":"1678289280.0","content":"D might not be the correct option, as for random search it might be faster but there might be a chance of decreased accuracy and this violates the questions as it says, to not comprise efficiency!","poster":"tavva_prudhvi"}],"upvote_count":"1"},{"comment_id":"629218","upvote_count":"3","content":"Selected Answer: CE\nAnswer C,E\n=========\nExplanation :\nA. Decrease the number of parallel trials : doing this will of course make Hypertuning take more time , we need to increase parallel trials not decrease\nB.Decrease the range of floating-point values : theoretically this should speed up the computation but this is not the most correct answer\nC. Set the early stopping parameter to TRUE : this is very good option\nD. Change the search algorithm from Bayesian search to random search : also searching the search algorithm will not have a great impact\nE. Decrease the maximum number of trials during subsequent training phases : very good option","timestamp":"1657378740.0","poster":"Mohamed_Mossad"},{"upvote_count":"2","content":"Selected Answer: CE\nCE for me.","poster":"David_ml","timestamp":"1652126460.0","comment_id":"599277"},{"content":"Selected Answer: CE\nI vote for C,E.\n\nA: If you decrease # of parallel trials, training takes more time.\nB: Even if you decrease the range, training takes the same time becuz # trial remained still.\nD: Going worse.","timestamp":"1649650920.0","poster":"morgan62","upvote_count":"2","comment_id":"584031"},{"timestamp":"1642610760.0","content":"Selected Answer: AC\nas majejim said:\n\nA: \"However, running in parallel can reduce the effectiveness of the tuning job overall. That is because hyperparameter tuning uses the results of previous trials\" -> See https://cloud.google.com/ai-platform/training/docs/using-hyperparameter-tuning#running_parallel_trials\n\nC: \"Training must automatically stop a trial that has become clearly unpromising. This saves you the cost of continuing a trial that is unlikely to be useful\" -> See https://cloud.google.com/ai-platform/training/docs/using-hyperparameter-tuning#early-stopping","poster":"ggorzki","upvote_count":"1","comment_id":"527748"},{"comment_id":"514520","timestamp":"1641043140.0","content":"Selected Answer: CE\nC reduces the time by stopping early and it is one of the technique of regularization to avoid overfitting but it is part defined with hypertuning,\nE for sure reduce the time.","upvote_count":"2","poster":"NamitSehgal"},{"comment_id":"473788","poster":"ramen_lover","timestamp":"1636268340.0","content":"Answer: B and E.\nDon't be too serious.\n(B) Decreasing the range of the searching space always works. The narrower the searching space, the less the optimizer needs to work.\n(E) Decreasing the number of trials definitely works.\n\n(C) is not correct. \"Early stopping\" is a form of regularization used to avoid overfitting.","upvote_count":"4"},{"content":"I guess it's A & C:\n\nA: \"However, running in parallel can reduce the effectiveness of the tuning job overall. That is because hyperparameter tuning uses the results of previous trials\" -> See https://cloud.google.com/ai-platform/training/docs/using-hyperparameter-tuning#running_parallel_trials\n\nC: \"Training must automatically stop a trial that has become clearly unpromising. This saves you the cost of continuing a trial that is unlikely to be useful\" -> See https://cloud.google.com/ai-platform/training/docs/using-hyperparameter-tuning#early-stopping","comment_id":"467979","timestamp":"1635245640.0","upvote_count":"4","poster":"majejim435"},{"content":"isn't B more relevant to memory reduce? Think it's C, E","timestamp":"1629935520.0","poster":"Jijiji","comment_id":"431816","upvote_count":"2"}],"unix_timestamp":1624717140,"answer_ET":"CE","exam_id":13,"url":"https://www.examtopics.com/discussions/google/view/56120-exam-professional-machine-learning-engineer-topic-1-question/","answer_description":""}],"exam":{"isBeta":false,"id":13,"isImplemented":true,"lastUpdated":"11 Apr 2025","isMCOnly":true,"name":"Professional Machine Learning Engineer","numberOfQuestions":304,"provider":"Google"},"currentPage":48},"__N_SSP":true}