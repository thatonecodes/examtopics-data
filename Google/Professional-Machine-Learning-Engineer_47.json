{"pageProps":{"questions":[{"id":"ABODkRvhtYTLNDVTdFsd","isMC":true,"exam_id":13,"answer_description":"","choices":{"D":"Recompile TensorFlow Serving using the source to support CPU-specific optimizations. Instruct GKE to choose an appropriate baseline minimum CPU platform for serving nodes.","C":"Significantly increase the max_enqueued_batches TensorFlow Serving parameter.","A":"Significantly increase the max_batch_size TensorFlow Serving parameter.","B":"Switch to the tensorflow-model-server-universal version of TensorFlow Serving."},"timestamp":"2021-07-01 07:58:00","question_images":[],"question_text":"You developed an ML model with AI Platform, and you want to move it to production. You serve a few thousand queries per second and are experiencing latency issues. Incoming requests are served by a load balancer that distributes them across multiple Kubeflow CPU-only pods running on Google Kubernetes Engine\n(GKE). Your goal is to improve the serving latency without changing the underlying infrastructure. What should you do?","answer_images":[],"question_id":231,"unix_timestamp":1625119080,"url":"https://www.examtopics.com/discussions/google/view/56675-exam-professional-machine-learning-engineer-topic-1-question/","answers_community":["D (59%)","A (27%)","14%"],"answer_ET":"D","discussion":[{"upvote_count":"31","content":"D is correct since this question is focusing on server performance which development env is higher than production env. It's already throttling so increase the pressure on them won't help. Both A and C is essentially doing this. B is a bit mysterious, but we definitely know that D would work.","comments":[{"comment_id":"465040","upvote_count":"3","content":"I think it's D too","poster":"mousseUwU","timestamp":"1634719080.0"}],"comment_id":"444899","poster":"Y2Data","timestamp":"1631667600.0"},{"poster":"pico","content":"Selected Answer: C\nhttps://github.com/tensorflow/serving/blob/master/tensorflow_serving/batching/README.md#batch-scheduling-parameters-and-tuning\n\nA may help to some extent, but it primarily affects how many requests are processed in a single batch. It might not directly address latency issues.\n\nD is a valid approach for optimizing TensorFlow Serving for CPU-specific optimizations, but it's a more involved process and might not be the quickest way to address latency issues.","timestamp":"1699893720.0","comment_id":"1069511","upvote_count":"5"},{"timestamp":"1735222380.0","upvote_count":"2","poster":"desertlotus1211","content":"Selected Answer: D\nA is wrong\n- Increasing max_batch_size reduces latency by batching more requests together, but this introduces delays since the system must wait to accumulate a full batch.\n- This approach can improve throughput but may increase per-query latency, which contradicts the goal of reducing latency.","comment_id":"1331937"},{"upvote_count":"1","poster":"rajshiv","timestamp":"1733509500.0","content":"Selected Answer: A\nI do not think D is correct as D is focused on optimizing CPU utilization, not on the batching process or managing latency. Since our goal is to improve serving latency, optimizing batching via the max_batch_size parameter is a more straightforward and effective solution.","comment_id":"1322876"},{"content":"Selected Answer: A\nA would work","comment_id":"1317747","poster":"AB_C","timestamp":"1732565760.0","upvote_count":"1"},{"poster":"desertlotus1211","content":"max_batch_size: Increasing the max_batch_size parameter allows TensorFlow Serving to process more requests in a single batch. This can improve throughput and reduce latency, especially in high-query environments, as it allows more efficient utilization of CPU resources by processing larger batches of requests at once.\n\nAnswer A","upvote_count":"2","timestamp":"1729545360.0","comment_id":"1301266"},{"poster":"taksan","upvote_count":"2","timestamp":"1723924140.0","content":"Selected Answer: D\nI think the correct is D, because the question is about reducing latency. As for A, increasing the batch size might event hurt latency if the system is overwhelmed to serve more multiple requests","comment_id":"1267826"},{"comment_id":"1236710","poster":"chirag2506","timestamp":"1719294540.0","content":"Selected Answer: D\nit is D","upvote_count":"2"},{"timestamp":"1717680840.0","comment_id":"1225547","poster":"PhilipKoku","upvote_count":"1","content":"Selected Answer: C\nC) Batch enqueued"},{"timestamp":"1713102540.0","upvote_count":"2","comment_id":"1195547","content":"Selected Answer: D\nincreasing the max_batch_size TensorFlow Serving parameter, is not the best choice because increasing the batch size may not necessarily improve latency. In fact, it may even lead to higher latency for individual requests, as they will have to wait for the batch to be filled before processing. This may be useful when optimizing for throughput, but not for serving latency, which is the primary goal in this scenario.","poster":"pinimichele01"},{"comments":[{"timestamp":"1709562180.0","content":"This is purely a software optimization and on how GKE handles requests. GKE should be able to choose different CPU types for nodes within the same cluster, which doesn't represent a change in architecture.","poster":"edoo","comment_id":"1165664","upvote_count":"1"}],"content":"Selected Answer: A\nI think A is correct, as D implies changes to the infrastructure (question says you must not do that).","comment_id":"1068013","timestamp":"1699726740.0","upvote_count":"1","poster":"ichbinnoah"},{"content":"Selected Answer: D\nincreasing the max_batch_size TensorFlow Serving parameter, is not the best choice because increasing the batch size may not necessarily improve latency. In fact, it may even lead to higher latency for individual requests, as they will have to wait for the batch to be filled before processing. This may be useful when optimizing for throughput, but not for serving latency, which is the primary goal in this scenario.","poster":"tavva_prudhvi","comment_id":"977741","upvote_count":"2","timestamp":"1691675160.0"},{"content":"Selected Answer: D\nmax_batch_size parameter controls the maximum number of requests that can be batched together by TensorFlow Serving. Increasing this parameter can help reduce the number of round trips between the client and server, which can improve serving latency. However, increasing the batch size too much can lead to higher memory usage and longer processing times for each batch.","timestamp":"1689157920.0","comment_id":"949680","upvote_count":"2","poster":"harithacML"},{"content":"Selected Answer: D\nDefinetely D\nto improve the serving latency of an ML model on AI Platform, you can recompile TensorFlow Serving using the source to support CPU-specific optimizations and instruct GKE to choose an appropriate baseline minimum CPU platform for serving nodes, this way GKE will schedule the pods on nodes with at least that CPU platform.","comment_id":"945713","timestamp":"1688735940.0","poster":"Liting","upvote_count":"2"},{"poster":"M25","comment_id":"892710","upvote_count":"2","timestamp":"1683608580.0","content":"Selected Answer: D\nWent with D"},{"upvote_count":"2","poster":"SergioRubiano","timestamp":"1679663700.0","comment_id":"849307","content":"Selected Answer: A\nA is correct. max_batch_size TensorFlow Serving parameter"},{"upvote_count":"3","poster":"Yajnas_arpohc","comment_id":"849033","content":"Selected Answer: A\nCPU-only: One Approach\nIf your system is CPU-only (no GPU), then consider starting with the following values: num_batch_threads equal to the number of CPU cores; max_batch_size to a really high value; batch_timeout_micros to 0. Then experiment with batch_timeout_micros values in the 1-10 millisecond (1000-10000 microsecond) range, while keeping in mind that 0 may be the optimal value.\n\nhttps://github.com/tensorflow/serving/tree/master/tensorflow_serving/batching","comments":[{"comment_id":"879609","content":"In that very link, what it says is that max_batch_size is the parameter that governs the latency/troughput tradeoff, and as I understand, the higher the batch size, the higher the throughput, but that doesn't assure that latency will be lower.\nI would go with D","upvote_count":"4","timestamp":"1682359680.0","poster":"frangm23"}],"timestamp":"1679638740.0"},{"content":"Answer: D\nhttps://www.youtube.com/watch?v=fnZTVQ1SnDg","poster":"Omi_04040","comment_id":"757767","upvote_count":"2","timestamp":"1672081260.0"},{"poster":"wish0035","upvote_count":"2","comment_id":"746554","content":"Selected Answer: D\nans: D","timestamp":"1671142140.0"},{"comment_id":"643310","upvote_count":"2","content":"Selected Answer: D\nD is the right one","poster":"sachinxshrivastav","timestamp":"1659781620.0"},{"poster":"sachinxshrivastav","content":"Selected Answer: D\nD is the correct one","upvote_count":"2","timestamp":"1659700740.0","comment_id":"642960"},{"content":"Selected Answer: D\nA would further increase latency. It may only help to improve the throughput if the memory and computation power of the GKE pods are not saturated.","upvote_count":"2","comment_id":"638334","timestamp":"1658957880.0","poster":"felden"},{"poster":"dunhill","timestamp":"1656174000.0","upvote_count":"2","content":"Selected Answer: D\naggree with Y2Data's viewpoint","comment_id":"622196"},{"poster":"u_phoria","comments":[{"upvote_count":"4","poster":"u_phoria","content":"Coming back to this 3 weeks on - it's actually D.\nA helps more with throughput. On the other hand, TF Serving is by default built with modest assumptions around CPU support for dense comping features. And \"building TensorFlow Serving from source is relatively easy using Docker\" (per same link in original post - https://www.tensorflow.org/tfx/serving/performance).","timestamp":"1657959600.0","comment_id":"632058"}],"comment_id":"621227","upvote_count":"2","timestamp":"1656010260.0","content":"Selected Answer: A\nTricky one... I'd go for A, based on https://www.tensorflow.org/tfx/serving/performance#batch_size:\n\"Configuring the latter kind of batching allows you to hit TensorFlow Serving at extremely high QPS, while allowing it to sub-linearly scale the compute resources needed to keep up.\"\n\nIn other words, trade a bit of latency to batch up requests (that latency being of the order of very few milliseconds or less, based on https://github.com/tensorflow/serving/blob/master/tensorflow_serving/batching/README.md#batch-scheduling-parameters-and-tuning) in order to gain sub-linear scalability when under heavy load. This assumes that latency is a consequence of being processing bound - which is implied but not explicitly stated.\n\nD would likely also work, but is more involved. By manually recompiling TF Serving, we are starting to move away from the goodness of a fully managed solution..."},{"content":"Selected Answer: A\nwill vote for A\nhttps://github.com/tensorflow/serving/blob/master/tensorflow_serving/batching/README.md#batch-scheduling-parameters-and-tuning","poster":"Mohamed_Mossad","timestamp":"1655138520.0","comment_id":"615864","upvote_count":"2"},{"timestamp":"1636264500.0","upvote_count":"3","content":"Answer D.\n> \"In addition, optimizing the saved model before deploying it (for example, by stripping unused parts) can reduce prediction latency. If you're training a TensorFlow model, we recommend that you optimize the SavedModel using the Graph Transformation Tools.\"\nhttps://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning#optimizing_models_for_serving\nHowever, I currently do not understand what \"CPU-specific optimizations\" exactly means. Any ideas?\n\nA is not correct: bigger batch size => increase latency (i.e., the opposite outcome)","poster":"ramen_lover","comment_id":"473763"},{"upvote_count":"3","comments":[{"upvote_count":"12","content":"I don't think we can scale the batch size to get what we want here, the underlying system can't handle the number of requests coming in already sending a bigger batch is not reducing the number of calc. D is correct, infra is still GKE its just nodes info being changed u won't experience downtime if done properly","poster":"inder0007","comment_id":"399361","timestamp":"1625508120.0"}],"content":"it should be A.\nhttps://github.com/tensorflow/serving/blob/master/tensorflow_serving/batching/README.md#batch-scheduling-parameters-and-tuning\nmax_batch_size: The maximum size of any batch. This parameter governs the throughput/latency tradeoff, and also avoids having batches that are so large they exceed some resource constraint (e.g. GPU memory to hold a batch's data).\nAs with D, it will change the infrastructure.","poster":"DucLee3110","timestamp":"1625119080.0","comment_id":"395587"}],"topic":"1","answer":"D"},{"id":"X3Yyubj0fLX1rpRSu7Ve","isMC":true,"exam_id":13,"answer_description":"","timestamp":"2021-06-22 12:27:00","choices":{"A":"Normalize the data using Google Kubernetes Engine.","D":"Normalize the data with Apache Spark using the Dataproc connector for BigQuery.","C":"Use the normalizer_fn argument in TensorFlow's Feature Column API.","B":"Translate the normalization algorithm into SQL for use with BigQuery."},"question_images":[],"question_text":"You have a demand forecasting pipeline in production that uses Dataflow to preprocess raw data prior to model training and prediction. During preprocessing, you employ Z-score normalization on data stored in BigQuery and write it back to BigQuery. New training data is added every week. You want to make the process more efficient by minimizing computation time and manual intervention. What should you do?","unix_timestamp":1624357620,"answer_images":[],"question_id":232,"answers_community":["B (84%)","Other"],"url":"https://www.examtopics.com/discussions/google/view/55827-exam-professional-machine-learning-engineer-topic-1-question/","answer_ET":"B","discussion":[{"comment_id":"387836","comments":[{"upvote_count":"2","comment_id":"465704","comments":[{"comment_id":"477724","poster":"kaike_reis","upvote_count":"4","content":"Dataflow uses Beam, different from dataproc that uses Spark.\n\nI think that D would be wrong because we would add one more service into the pipeline for a simple transformation (minus the mean and divide by std).","timestamp":"1652466300.0"}],"content":"I agree that B would definitely get the job done. But wouldn't D work as well and keep all the data pre-processing in Dataflow?","poster":"93alejandrosanchez","timestamp":"1650542280.0"}],"upvote_count":"22","poster":"maartenalexander","content":"B. I think. BiqQuery definitely minimizes computational time for normalization. I think it would also minimize manual intervention. For data normalization in dataflow you'd have to pass in values of mean and standard deviation as a side-input. That seems more work than a simple SQL query","timestamp":"1640176020.0"},{"upvote_count":"1","content":"Selected Answer: B\nB) Using BigQuery","timestamp":"1733499300.0","poster":"PhilipKoku","comment_id":"1225548"},{"timestamp":"1715794320.0","comment_id":"1071819","upvote_count":"2","content":"Selected Answer: B\nz-scores is very easy to do in BQ - no need for more complex solutions","poster":"Sum_Sum"},{"comment_id":"1005433","timestamp":"1710230280.0","content":"B. All that maartenalexander said, + BigQuery already has a function for that: https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-standard-scaler , we could even schedule the query for calculating this automatically :)","poster":"elenamatay","upvote_count":"3"},{"comment_id":"951410","content":"Selected Answer: C\nEvery week when new data is loaded mean and standard deviation is calculated for it and passed as parameter to calculate z score at serving \nhttps://towardsdatascience.com/how-to-normalize-features-in-tensorflow-5b7b0e3a4177","comments":[{"timestamp":"1705771800.0","upvote_count":"1","poster":"tavva_prudhvi","comment_id":"957749","content":"owever, in the given scenario, you are using Dataflow for preprocessing and BigQuery for storing data.\n\nTo make the process more efficient by minimizing computation time and manual intervention, you should still opt for option B: Translate the normalization algorithm into SQL for use with BigQuery. This way, you can perform the normalization directly in BigQuery, which will save time and resources compared to using an external tool."}],"upvote_count":"1","timestamp":"1705231620.0","poster":"aaggii"},{"poster":"SamuelTsch","content":"Selected Answer: B\nA, D usually need additional configuration, which could cost much more time.","timestamp":"1704652680.0","upvote_count":"1","comment_id":"945861"},{"comment_id":"892711","poster":"M25","content":"Selected Answer: B\nWent with B","upvote_count":"2","timestamp":"1699513440.0"},{"comment_id":"849311","timestamp":"1695554280.0","poster":"SergioRubiano","upvote_count":"2","content":"Selected Answer: B\nBest way is B"},{"timestamp":"1693232220.0","upvote_count":"2","comment_id":"825010","poster":"Fatiy","content":"Selected Answer: D\nOption D is the best solution because Apache Spark provides a distributed computing platform that can handle large-scale data processing with ease. By using the Dataproc connector for BigQuery, Spark can read data directly from BigQuery and perform the normalization process in a distributed manner. This can significantly reduce computation time and manual intervention. Option A is not a good solution because Kubernetes is a container orchestration platform that does not directly provide data normalization capabilities. Option B is not a good solution because Z-score normalization is a data transformation technique that cannot be easily translated into SQL. Option C is not a good solution because the normalizer_fn argument in TensorFlow's Feature Column API is only applicable for feature normalization during model training, not for data preprocessing."},{"timestamp":"1689078360.0","comments":[{"upvote_count":"1","content":"SQL is not as flexible as other programming languages like Python, which can limit the ability to customize the normalization process or incorporate new features in the future.","poster":"Fatiy","timestamp":"1693232340.0","comment_id":"825014"}],"poster":"ares81","content":"Selected Answer: B\nBest way to proceed is B.","comment_id":"772625","upvote_count":"2"},{"upvote_count":"4","content":"Selected Answer: B\nB is the most efficient as you will not load --> process --> save , no you will only write some sql in bigquery and voila :D","comment_id":"615867","poster":"Mohamed_Mossad","timestamp":"1670957220.0"},{"poster":"baimus","content":"It's B, bigquery can do this internally, no need for dataflow","comment_id":"569763","upvote_count":"2","timestamp":"1663415940.0","comments":[]},{"timestamp":"1659319260.0","comment_id":"537626","upvote_count":"2","content":"Selected Answer: B\nI agree with B.","poster":"xiaoF"},{"upvote_count":"3","poster":"alashin","comment_id":"399691","content":"B. I agree with B as well.","timestamp":"1641456180.0"}],"topic":"1","answer":"B"},{"id":"cFd3AIW9W0kIs3s4SNeW","url":"https://www.examtopics.com/discussions/google/view/54964-exam-professional-machine-learning-engineer-topic-1-question/","answers_community":["D (85%)","C (15%)"],"answer_description":"","exam_id":13,"discussion":[{"content":"D - https://www.kubeflow.org/docs/about/use-cases/","comment_id":"402454","upvote_count":"14","poster":"ralf_cc","timestamp":"1641716280.0"},{"timestamp":"1639049880.0","comment_id":"378142","upvote_count":"6","poster":"salsabilsf","content":"Should be D"},{"comment_id":"1225549","content":"Selected Answer: D\nD) Experiments is the way forward","upvote_count":"1","timestamp":"1733499420.0","poster":"PhilipKoku"},{"content":"I would vote for D but if C had said instead \"different job names\" .. would that have been a better option?","poster":"tikka0804","comment_id":"1078480","timestamp":"1716466920.0","upvote_count":"2"},{"upvote_count":"1","timestamp":"1715794560.0","poster":"Sum_Sum","content":"Selected Answer: D\nD - everything else is just nonsense","comment_id":"1071820"},{"timestamp":"1704652980.0","poster":"SamuelTsch","content":"Selected Answer: D\nD should be correct","upvote_count":"2","comment_id":"945875"},{"content":"Selected Answer: D\nC has similar job name, which make it wrong\nSo correct answer should be D","upvote_count":"1","timestamp":"1704640860.0","comment_id":"945715","poster":"Liting"},{"upvote_count":"6","comment_id":"940701","poster":"tavva_prudhvi","timestamp":"1704194760.0","content":"Selected Answer: D\nThe best approach is to create an experiment in Kubeflow Pipelines to organize multiple runs. \n\nOption A is incorrect because AutoML Tables is a managed machine learning service that automates the process of building machine learning models from tabular data. It does not provide the flexibility to customize the model architecture or explore multiple model architectures.\n\nOption B is incorrect because Cloud Composer is a managed workflow orchestration service that can be used to automate machine learning workflows. However, it does not provide the same level of flexibility or scalability as Kubeflow Pipelines.\n\nOption C is incorrect because running multiple training jobs on AI Platform with similar job names will not allow you to easily organize and compare the results."},{"comment_id":"892712","upvote_count":"1","timestamp":"1699513440.0","content":"Selected Answer: D\nWent with D","poster":"M25"},{"content":"Selected Answer: D\nWith Kubeflow Pipelines, you can create experiments that help you keep track of multiple training runs with different model architectures and hyperparameters.","comment_id":"825021","upvote_count":"1","timestamp":"1693232460.0","poster":"Fatiy"},{"content":"Selected Answer: C\nhttps://cloud.google.com/vertex-ai/docs/experiments/user-journey/uj-compare-models","poster":"mymy9418","timestamp":"1688028720.0","comment_id":"760905","upvote_count":"2"},{"upvote_count":"1","poster":"suresh_vn","comment_id":"650548","content":"D\noption C does not work since CAIP have updated to VertexAI","timestamp":"1677129780.0"},{"content":"Selected Answer: D\nhttps://www.kubeflow.org/docs/components/pipelines/concepts/experiment/\nhttps://www.kubeflow.org/docs/components/pipelines/concepts/run/","comment_id":"629197","poster":"Mohamed_Mossad","timestamp":"1673279640.0","upvote_count":"1"},{"timestamp":"1665755400.0","content":"Selected Answer: D\nD- we need to use experiments feature to comapre models,having different jobnames is not going to help track experiments.","upvote_count":"3","comment_id":"585799","poster":"mmona19"},{"timestamp":"1658461260.0","upvote_count":"2","content":"C for me. It only talks about experimentation .. thats where AI platform fits better.","poster":"sid515","comment_id":"529625"},{"comment_id":"514502","timestamp":"1656671220.0","content":"Selected Answer: C\nSimilar job names is a bit of a confusion creator as we can not use same job names for sure. D sounds better but better in vertex AI during experiment phase only.","upvote_count":"1","poster":"NamitSehgal"},{"content":"C anyone? D seems to me like an overkill.","comment_id":"469892","comments":[{"comment_id":"477733","content":"(C) presents the most specific solution for what the question asks for: experimenting with models with their due comparisons. All of this is possible with the AI Platform. Furthermore, the question only speaks of experimentation. Kubeflow would be more powerfull if was a necessity for end-to-end pipeline.","upvote_count":"3","timestamp":"1652467560.0","poster":"kaike_reis"}],"upvote_count":"4","timestamp":"1651252500.0","poster":"kfrd"},{"comments":[{"poster":"tavva_prudhvi","comment_id":"957764","upvote_count":"1","timestamp":"1705772280.0","content":"How can we track the progress of each run and compare the results in the vertex AI dashboard?"}],"content":"D. In the new Vertex AI, it now supports experimentation with hyper parameter tuning.","timestamp":"1646789640.0","upvote_count":"4","poster":"Danny2021","comment_id":"441679"}],"topic":"1","timestamp":"2021-06-09 11:38:00","unix_timestamp":1623231480,"choices":{"B":"Automate multiple training runs using Cloud Composer.","D":"Create an experiment in Kubeflow Pipelines to organize multiple runs.","A":"Create multiple models using AutoML Tables.","C":"Run multiple training jobs on AI Platform with similar job names."},"question_images":[],"answer_ET":"D","question_id":233,"answer":"D","question_text":"You need to design a customized deep neural network in Keras that will predict customer purchases based on their purchase history. You want to explore model performance using multiple model architectures, store training data, and be able to compare the evaluation metrics in the same dashboard. What should you do?","isMC":true,"answer_images":[]},{"id":"TphJ3Yo4gMtrBSIS32Np","discussion":[{"comments":[{"timestamp":"1627851780.0","comment_id":"418420","content":"agree, links: https://github.com/kubeflow/pipelines/blob/master/components/gcp/bigquery/query/sample.ipynb; https://v0-5.kubeflow.org/docs/pipelines/reusable-components/","upvote_count":"6","poster":"gcp2021go"}],"timestamp":"1624357980.0","upvote_count":"23","content":"D. Kubeflow pipelines have different types of components, ranging from low- to high-level. They have a ComponentStore that allows you to access prebuilt functionality from GitHub.","poster":"maartenalexander","comment_id":"387842"},{"upvote_count":"7","poster":"NamitSehgal","content":"Selected Answer: D\nNot sure what is the reason behind putting A as it is manual and manual steps can not be part of automation. I would say Answer is D as it just require a clone of the component from github. Using a Python and import bigquery component may sounds good too, but ask was what is easiest. It depends how word \"easy\" is taken by individuals but definitely not A.","timestamp":"1641039780.0","comment_id":"514501"},{"comment_id":"1267828","timestamp":"1723924380.0","poster":"taksan","content":"Selected Answer: D\nD is the correct answer, as reusing an existing component is the most streamlined way to interact with BigQuery.","upvote_count":"2"},{"upvote_count":"2","comment_id":"1260901","poster":"nktyagi","content":"Selected Answer: B\nmuch simpler to just write a couple of lines of python","timestamp":"1722826380.0","comments":[{"timestamp":"1729545780.0","comment_id":"1301268","poster":"desertlotus1211","upvote_count":"1","content":"Writing a Python script using the BigQuery API is possible, but it's more complex than using an existing component. It requires more development effort and doesn't take advantage of the pre-built components available in Kubeflow."}]},{"timestamp":"1722672180.0","comment_id":"1260173","content":"Selected Answer: B\nClearly B","upvote_count":"1","poster":"jsalvasoler"},{"poster":"PhilipKoku","comment_id":"1225551","content":"Selected Answer: B\nB) Python API","upvote_count":"3","timestamp":"1717681260.0"},{"upvote_count":"3","comment_id":"1207050","poster":"Amabo","timestamp":"1714938060.0","content":"from kfp.components import load_component_from_url\n\nbigquery_query_op = load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/master/components/gcp/bigquery/query/component.yaml')\n\ndef my_pipeline():\n query_result = bigquery_query_op(\n project_id='my-project',\n query='SELECT * FROM my_dataset.my_table'\n )\n # Use the query_result as input to the next step in the pipeline"},{"content":"Selected Answer: B\nIm going \"against the flow\" and chosing B. It just sounds a lot easier option than D.","poster":"fragkris","comment_id":"1088255","timestamp":"1701762840.0","upvote_count":"3"},{"poster":"friedi","comments":[{"content":"Actually, the problem statement even says that the query result has to be used as input to the next step, meaning with answer D) we would have to download the results before passing them to the next step. Additionally, we would have to handle potentially existing files in Google Cloud Storage if the pipeline is either executed multiple times or even in parallel. (I will die on this hill 😆 ).","poster":"friedi","timestamp":"1687409100.0","comment_id":"930158","upvote_count":"2"},{"timestamp":"1699204260.0","upvote_count":"2","poster":"tavva_prudhvi","content":"Yup, you raised valid points. Depending on your specific requirements and familiarity with Python, writing a custom script using the BigQuery API (Option B) can be a simpler and more flexible approach.\n\nWith Option B, you can write a Python script that uses the BigQuery API to execute queries against BigQuery and fetch the data directly into your pipeline. This way, you can process the data as needed and pass it to the next step in the pipeline without the need to fetch it from Google Cloud Storage.\n\nWhile using the reusable BigQuery Query Component (Option D) provides a pre-built solution, it does require additional steps to fetch the data from Google Cloud Storage for the next step in the pipeline, which might not be the simplest approach.","comment_id":"1063132"}],"comment_id":"928453","upvote_count":"2","timestamp":"1687264260.0","content":"Selected Answer: B\nVery confused as to why D is the correct answer. To me it seems a) much simpler to just write a couple of lines of python (https://cloud.google.com/bigquery/docs/reference/libraries#client-libraries-install-python) and b) the documentation for the BigQuery reusable component (https://v0-5.kubeflow.org/docs/pipelines/reusable-components/) states that the data is written to Google Cloud Storage, which means we have to write the fetching logic in the next pipeline step, going against the \"as simple as possible\" requirement. Would be interested to hear why I am wrong."},{"content":"Selected Answer: D\nWent with D","comment_id":"892714","upvote_count":"2","timestamp":"1683608640.0","poster":"M25"},{"comments":[{"upvote_count":"1","comment_id":"629201","poster":"Mohamed_Mossad","timestamp":"1657375440.0","content":"answer between C,D but above link has an article which uses a ready .yml file for bigquery component on official kubeflow pipelines repo"}],"upvote_count":"2","timestamp":"1657375320.0","poster":"Mohamed_Mossad","content":"Selected Answer: D\nhttps://linuxtut.com/en/f4771efee37658c083cc/","comment_id":"629199"},{"comment_id":"599346","timestamp":"1652145360.0","content":"Selected Answer: D\nAnswer is D.","upvote_count":"3","poster":"David_ml"},{"poster":"donchoripan","timestamp":"1648599240.0","upvote_count":"1","comments":[{"poster":"David_ml","content":"A is wrong. Answer is D. It's a pipeline which means you will run it multiple times? Do you always want to make the query manually each time you run your pipeline?","comment_id":"599345","upvote_count":"3","timestamp":"1652145300.0"}],"content":"A. it says the easiest way possible so it sounds like just running the query on the console should be enogh. It doesn't says that the data will need to be uploaded again anytime soon, so we can asume that its just a one time query to be run.","comment_id":"577940"},{"timestamp":"1643688660.0","content":"D is good.","upvote_count":"3","poster":"xiaoF","comment_id":"537629"},{"timestamp":"1638187860.0","comment_id":"489855","poster":"aepos","content":"The result of D is just the path to the Cloud Storage where the result is stored not the data itself. So the input to the next step is this path, where you still have to load the data? So i would guess B. Can anyone explain if i am wrong?","upvote_count":"2"},{"poster":"kaike_reis","timestamp":"1636836660.0","content":"D. The easiest way possible in developer's world: copy code from stackoverflow or github hahaha. Jokes a part, I think D is the correct. (A) is manual, so you have to do always. (B) could be, but is not the easiest one because you need to write a script for this. (C) uses Kubeflow intern solution, but you need to work to create a custom component. (D) is the (C) solution, but easier using a component created previously to do the job.","upvote_count":"3","comment_id":"477736"},{"comments":[{"content":"why create a custom component when a big query's reusable component is already present. Answer is D.","comment_id":"439493","poster":"raviperi","upvote_count":"6","timestamp":"1630821000.0"}],"upvote_count":"1","comment_id":"410331","poster":"celia20200410","timestamp":"1626790020.0","content":"ans: c\nhttps://medium.com/google-cloud/using-bigquery-and-bigquery-ml-from-kubeflow-pipelines-991a2fa4bea8\nhttps://cloud.google.com/architecture/architecture-for-mlops-using-tfx-kubeflow-pipelines-and-cloud-build#kubeflow-piplines-components\nKubeflow Pipelines, a containerized task can invoke other services such as BigQuery jobs, AI Platform (distributed) training jobs, and Dataflow jobs."},{"timestamp":"1624018140.0","upvote_count":"4","content":"Should be B","poster":"chohan","comment_id":"384793"}],"choices":{"C":"Use the Kubeflow Pipelines domain-specific language to create a custom component that uses the Python BigQuery client library to execute queries.","A":"Use the BigQuery console to execute your query, and then save the query results into a new BigQuery table.","B":"Write a Python script that uses the BigQuery API to execute queries against BigQuery. Execute this script as the first step in your Kubeflow pipeline.","D":"Locate the Kubeflow Pipelines repository on GitHub. Find the BigQuery Query Component, copy that component's URL, and use it to load the component into your pipeline. Use the component to execute queries against BigQuery."},"unix_timestamp":1624018140,"timestamp":"2021-06-18 14:09:00","question_images":[],"topic":"1","answer_description":"","question_id":234,"question_text":"You are developing a Kubeflow pipeline on Google Kubernetes Engine. The first step in the pipeline is to issue a query against BigQuery. You plan to use the results of that query as the input to the next step in your pipeline. You want to achieve this in the easiest way possible. What should you do?","answer":"D","exam_id":13,"answer_images":[],"answer_ET":"D","url":"https://www.examtopics.com/discussions/google/view/55564-exam-professional-machine-learning-engineer-topic-1-question/","answers_community":["D (59%)","B (41%)"],"isMC":true},{"id":"TN9RHwudNhqpggfQJ01C","question_id":235,"answers_community":["B (84%)","D (16%)"],"topic":"1","url":"https://www.examtopics.com/discussions/google/view/55829-exam-professional-machine-learning-engineer-topic-1-question/","choices":{"C":"Add more data to your test set to ensure that you have a fair distribution and sample for testing.","A":"Normalize the data for the training, and test datasets as two separate steps.","D":"Apply data transformations before splitting, and cross-validate to make sure that the transformations are applied to both the training and test sets.","B":"Split the training and test data based on time rather than a random split to avoid leakage."},"unix_timestamp":1624358760,"timestamp":"2021-06-22 12:46:00","discussion":[{"upvote_count":"35","timestamp":"1624358760.0","comment_id":"387851","content":"B. If you do time series prediction, you can't borrow information from the future to predict the future. If you do, you are artificially increasing your accuracy.","poster":"maartenalexander"},{"timestamp":"1735222680.0","poster":"desertlotus1211","content":"Selected Answer: B\nD is incorrect:\n\nApplying transformations before splitting is important, but it does not resolve the issue of time leakage.\nEven if transformations are done correctly, the random split will still lead to inflated test accuracy and poor production performance.\nThis option focuses on correct data processing, but it does not address the leakage caused by random splitting in time series data.","upvote_count":"1","comment_id":"1331940"},{"comment_id":"1280187","comments":[{"content":"B I mean. Sorry I wrote that comment very early and there is no delete key!","timestamp":"1725766440.0","poster":"baimus","upvote_count":"1","comment_id":"1280188"}],"upvote_count":"1","timestamp":"1725766380.0","content":"Selected Answer: D\nIt's D","poster":"baimus"},{"poster":"jsalvasoler","timestamp":"1722690000.0","upvote_count":"1","comment_id":"1260297","content":"Selected Answer: B\ntemporal split is a must in time series forecasting evaluation"},{"content":"Selected Answer: B\nB) Time split to avoid leaking data.","upvote_count":"1","poster":"PhilipKoku","comment_id":"1225565","timestamp":"1717683180.0"},{"upvote_count":"1","timestamp":"1701762900.0","comment_id":"1088256","content":"Selected Answer: B\nDefinetely B","poster":"fragkris"},{"upvote_count":"1","content":"Selected Answer: B\nthey did not explicitly say forecasting, but splitting by time is the number one rule you learn","timestamp":"1700077200.0","poster":"Sum_Sum","comment_id":"1071823"},{"content":"Selected Answer: B\nWent with B","timestamp":"1683608640.0","upvote_count":"1","comment_id":"892715","poster":"M25"},{"timestamp":"1679664840.0","upvote_count":"2","comment_id":"849320","poster":"SergioRubiano","content":"Selected Answer: D\nD is correct. cross-validate"},{"content":"Selected Answer: B\ntrain accuracy 97% , production accuracy 66% ---> time series data ---> random split ---> cause leakage , answer is B","upvote_count":"2","timestamp":"1655141220.0","comment_id":"615889","poster":"Mohamed_Mossad"},{"content":"Selected Answer: B\nYou don't split data randomly for time series prediction.","upvote_count":"3","poster":"David_ml","comment_id":"599253","timestamp":"1652124660.0"},{"comment_id":"585811","content":"Selected Answer: B\nB should be the answer. D is incorrect as normalize before split is going to do data leak https://community.rapidminer.com/discussion/32592/normalising-data-before-data-split-or-after","upvote_count":"2","timestamp":"1649944680.0","poster":"mmona19"},{"content":"Selected Answer: B\nIf you do random split in a time series, your risk that training data will contain information about the target (definition of leakage), but similar data won't be available when the model is used for prediction. Leakage causes the model to look accurate until you start making actual predictions with it.","upvote_count":"3","poster":"giaZ","timestamp":"1646748480.0","comment_id":"563303"},{"poster":"xiaoF","timestamp":"1643688840.0","upvote_count":"2","content":"agree B as well","comment_id":"537631"},{"content":"I think is B","upvote_count":"2","comment_id":"505660","poster":"JobQ","timestamp":"1640030700.0"},{"upvote_count":"3","content":"B. D doesn't improve anything at all. Split and Transform is no different than Transform and Split if the transform logic is the same.","timestamp":"1631144340.0","comment_id":"441681","poster":"Danny2021"},{"timestamp":"1629934200.0","upvote_count":"1","comment_id":"431805","content":"seems like D","poster":"Jijiji"}],"isMC":true,"question_text":"You are building a model to predict daily temperatures. You split the data randomly and then transformed the training and test datasets. Temperature data for model training is uploaded hourly. During testing, your model performed with 97% accuracy; however, after deploying to production, the model's accuracy dropped to 66%. How can you make your production model more accurate?","answer":"B","answer_ET":"B","question_images":[],"exam_id":13,"answer_description":"","answer_images":[]}],"exam":{"isImplemented":true,"name":"Professional Machine Learning Engineer","id":13,"lastUpdated":"11 Apr 2025","isMCOnly":true,"numberOfQuestions":304,"isBeta":false,"provider":"Google"},"currentPage":47},"__N_SSP":true}