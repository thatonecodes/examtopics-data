{"pageProps":{"questions":[{"id":"7SGXuW9h8BqydwEiWrlO","answer_ET":"B","answer_images":[],"question_images":[],"topic":"1","question_text":"You have a network of 1000 sensors. The sensors generate time series data: one metric per sensor per second, along with a timestamp. You already have 1 TB of data, and expect the data to grow by 1 GB every day. You need to access this data in two ways. The first access pattern requires retrieving the metric from one specific sensor stored at a specific timestamp, with a median single-digit millisecond latency. The second access pattern requires running complex analytic queries on the data, including joins, once a day. How should you store this data?","answer_description":"","discussion":[{"timestamp":"1704842040.0","comment_id":"1117914","poster":"raaad","content":"Selected Answer: B\n- Bigtable excels at incredibly fast lookups by row key, often reaching single-digit millisecond latencies. \n- Constructing the row key with sensor ID and timestamp enables efficient retrieval of specific sensor readings at exact timestamps.\n- Bigtable's wide-column design effectively stores time series data, allowing for flexible addition of new metrics without schema changes.\n- Bigtable scales horizontally to accommodate massive datasets (petabytes or more), easily handling the expected data growth.","upvote_count":"16"},{"poster":"fitri001","comment_id":"1231859","timestamp":"1718618220.0","upvote_count":"2","content":"Selected Answer: B\nagree with raaad"},{"upvote_count":"1","poster":"hanoverquay","content":"Selected Answer: B\nvoted b","comment_id":"1174366","timestamp":"1710518640.0"},{"comment_id":"1155430","content":"Selected Answer: B\nOption B","poster":"JyoGCP","upvote_count":"1","timestamp":"1708510560.0"},{"timestamp":"1705164300.0","upvote_count":"1","comment_id":"1121866","content":"Selected Answer: B\nOption B - agree with raaad","poster":"Matt_108"},{"content":"Selected Answer: B\nB. Store your data in Bigtable. Concatenate the sensor ID and timestamp and use it as the row key. Perform an export to BigQuery every day.","timestamp":"1704350220.0","comment_id":"1113373","comments":[{"upvote_count":"4","timestamp":"1704645480.0","comment_id":"1116010","content":"Based on your requirements, Option B seems most suitable. Bigtable's design caters to the low-latency access of time-series data (your first requirement), and the daily export to BigQuery enables complex analytics (your second requirement). The use of sensor ID and timestamp as the row key in Bigtable would facilitate efficient access to specific sensor data at specific times.","poster":"Smakyel79"}],"poster":"scaenruy","upvote_count":"3"}],"answers_community":["B (100%)"],"url":"https://www.examtopics.com/discussions/google/view/130273-exam-professional-data-engineer-topic-1-question-284/","question_id":206,"unix_timestamp":1704350220,"exam_id":11,"answer":"B","isMC":true,"choices":{"A":"Store your data in BigQuery. Concatenate the sensor ID and timestamp, and use it as the primary key.","C":"Store your data in Bigtable. Concatenate the sensor ID and metric, and use it as the row key. Perform an export to BigQuery every day.","D":"Store your data in BigQuery. Use the metric as a primary key.","B":"Store your data in Bigtable. Concatenate the sensor ID and timestamp and use it as the row key. Perform an export to BigQuery every day."},"timestamp":"2024-01-04 07:37:00"},{"id":"ZVPVFkewHvvy0S7h9q8a","question_id":207,"topic":"1","answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/130511-exam-professional-data-engineer-topic-1-question-285/","answers_community":["D (100%)"],"answer_ET":"D","answer_description":"","question_images":[],"exam_id":11,"question_text":"You have 100 GB of data stored in a BigQuery table. This data is outdated and will only be accessed one or two times a year for analytics with SQL. For backup purposes, you want to store this data to be immutable for 3 years. You want to minimize storage costs. What should you do?","isMC":true,"answer":"D","choices":{"C":"1. Perform a BigQuery export to a Cloud Storage bucket with archive storage class.\n2. Enable versioning on the bucket.\n3. Create a BigQuery external table on the exported files.","B":"1. Create a BigQuery table snapshot.\n2. Restore the snapshot when you need to perform analytics.","A":"1. Create a BigQuery table clone.\n2. Query the clone when you need to perform analytics.","D":"1. Perform a BigQuery export to a Cloud Storage bucket with archive storage class.\n2. Set a locked retention policy on the bucket.\n3. Create a BigQuery external table on the exported files."},"discussion":[{"content":"Selected Answer: D\nStraight Forward","comment_id":"1119671","poster":"raaad","timestamp":"1704974040.0","upvote_count":"5"},{"timestamp":"1718618340.0","comment_id":"1231860","poster":"fitri001","content":"Selected Answer: D\nFor data keeping till last 3 years, use bucket lock with rentention policy","upvote_count":"2"},{"timestamp":"1710514800.0","comment_id":"1174328","poster":"hanoverquay","content":"Selected Answer: D\nvoted D","upvote_count":"1"},{"upvote_count":"1","comment_id":"1155433","poster":"JyoGCP","timestamp":"1708510620.0","content":"Selected Answer: D\nOption D"},{"content":"Selected Answer: D\nOption D, clearly","upvote_count":"2","comment_id":"1121868","timestamp":"1705164360.0","poster":"Matt_108"},{"comment_id":"1115962","content":"D. \nFor data keeping till last 3 years, use bucket lock with rentention policy","timestamp":"1704641160.0","upvote_count":"2","poster":"GCP001"}],"timestamp":"2024-01-07 16:26:00","unix_timestamp":1704641160},{"id":"q8ID9GwQBss60Kf2g2tM","topic":"1","answer_images":[],"exam_id":11,"discussion":[{"poster":"hussain.sain","content":"Selected Answer: D\nD is correct.\nDataproc is the most suitable choice for migrating your existing Apache Spark jobs to Google Cloud because it is a fully managed service that supports Apache Spark and Hadoop workloads with minimal changes to your existing code. Moving your data to Cloud Storage and running jobs on Dataproc offers a fast, efficient, and scalable solution for your needs.","upvote_count":"1","comment_id":"1332388","timestamp":"1735302180.0"},{"upvote_count":"1","poster":"meh_33","content":"Selected Answer: D\noption D, minimum code changes","timestamp":"1723275420.0","comment_id":"1263366"},{"poster":"hanoverquay","timestamp":"1710514560.0","comment_id":"1174325","upvote_count":"2","content":"Selected Answer: D\noption D, minimum code changes"},{"poster":"JyoGCP","content":"Selected Answer: D\nOption D","timestamp":"1708510920.0","comment_id":"1155436","upvote_count":"2"},{"content":"Selected Answer: D\nD) That is what Dataproc is made for. It is a fully managed and highly scalable service for running Apache Hadoop, Apache Spark, etc.","timestamp":"1708275540.0","comment_id":"1153442","upvote_count":"3","poster":"ML6"},{"comment_id":"1121892","upvote_count":"2","content":"Selected Answer: D\nClearly D","poster":"Matt_108","timestamp":"1705165860.0"},{"poster":"Sofiia98","timestamp":"1704875220.0","upvote_count":"2","comment_id":"1118373","content":"Selected Answer: D\nof course D"},{"upvote_count":"3","poster":"GCP001","comment_id":"1115965","timestamp":"1704641340.0","content":"D. Move your data to Cloud Storage. Run your jobs on Dataproc. \nDataproc is managed service and not needed much code changes."},{"timestamp":"1704361260.0","upvote_count":"3","content":"Selected Answer: D\nD. Move your data to Cloud Storage. Run your jobs on Dataproc.","poster":"scaenruy","comment_id":"1113498"}],"answer_ET":"D","answer":"D","isMC":true,"timestamp":"2024-01-04 10:41:00","question_text":"You have thousands of Apache Spark jobs running in your on-premises Apache Hadoop cluster. You want to migrate the jobs to Google Cloud. You want to use managed services to run your jobs instead of maintaining a long-lived Hadoop cluster yourself. You have a tight timeline and want to keep code changes to a minimum. What should you do?","choices":{"C":"Copy your data to Compute Engine disks. Manage and run your jobs directly on those instances.","A":"Move your data to BigQuery. Convert your Spark scripts to a SQL-based processing approach.","B":"Rewrite your jobs in Apache Beam. Run your jobs in Dataflow.","D":"Move your data to Cloud Storage. Run your jobs on Dataproc."},"answers_community":["D (100%)"],"question_images":[],"url":"https://www.examtopics.com/discussions/google/view/130287-exam-professional-data-engineer-topic-1-question-286/","answer_description":"","question_id":208,"unix_timestamp":1704361260},{"id":"136aS7WngDUm5RX38pYs","question_id":209,"answer_ET":"C","answer":"C","choices":{"B":"Establish a BigQuery quota for the marketing team, and limit the maximum number of bytes scanned each day.","C":"Create a BigQuery reservation with a baseline of 500 slots with no autoscaling for the marketing team, and bill them back accordingly.","A":"Create a BigQuery Enterprise reservation with a baseline of 250 slots and autoscaling set to 500 for the marketing team, and bill them back accordingly.","D":"Create a BigQuery Standard pay-as-you go reservation with a baseline of 0 slots and autoscaling set to 500 for the marketing team, and bill them back accordingly."},"question_text":"You are administering shared BigQuery datasets that contain views used by multiple teams in your organization. The marketing team is concerned about the variability of their monthly BigQuery analytics spend using the on-demand billing model. You need to help the marketing team establish a consistent BigQuery analytics spend each month. What should you do?","answers_community":["C (38%)","B (36%)","A (22%)","5%"],"timestamp":"2024-01-04 11:08:00","answer_images":[],"isMC":true,"question_images":[],"discussion":[{"content":"Selected Answer: C\nReservations guarantee a fixed number of slots (computational resources) for BigQuery queries, ensuring a predictable monthly cost, addressing the marketing team's concern about variability.","upvote_count":"13","poster":"raaad","timestamp":"1704844500.0","comments":[{"upvote_count":"4","poster":"AllenChen123","timestamp":"1705822080.0","comment_id":"1127685","comments":[{"upvote_count":"3","timestamp":"1706164080.0","comments":[{"comments":[{"upvote_count":"8","comment_id":"1145307","content":"If you use B - the marketing team wouldn't be able to run their queries when the quota is reached, which could harm the business. \n\nHaving a reservation for 500 slots and no autoscaling gives you exact predictable cost for each month without harming the business or have variable cost with autoscaling \n\nSo C should be the right answer","poster":"saschak94","timestamp":"1707463620.0"}],"content":"\"However, you can set limits on the amount of data users can query by creating custom quotas to control query usage per day or query usage per day per user.\"\nhttps://cloud.google.com/blog/products/data-analytics/manage-bigquery-costs-with-custom-quotas\nB would be correct","upvote_count":"2","timestamp":"1706528880.0","comment_id":"1134895","poster":"datapassionate"}],"poster":"AllenChen123","content":"But seems only C makes sense.\nhttps://cloud.google.com/bigquery/quotas#query_jobs\n\"There is no limit to the number of bytes that can be processed by queries in a project.\"","comment_id":"1131370"}],"content":"Why 500 slots?"}],"comment_id":"1117937"},{"upvote_count":"1","poster":"desertlotus1211","comment_id":"1411712","timestamp":"1743255720.0","content":"Selected Answer: C\nAnswer A - Autoscaling introduces variable costs — which defeats the goal of cost consistency\n\nAnswer B: it doesn’t convert to predictable costs — on-demand billing still applies per scan.\n\nAnswer C is best."},{"content":"Selected Answer: B\nThe input doesn't specify the consistent monthly spent. hence, A, C, and D can't be used","timestamp":"1739888640.0","comments":[{"comment_id":"1411713","comments":[{"timestamp":"1743255840.0","poster":"desertlotus1211","comment_id":"1411715","upvote_count":"1","content":"'The marketing team is concerned about the variability of their monthly BigQuery analytics spend using the on-demand billing model' so yes - this implies wanting consistent spend."}],"timestamp":"1743255780.0","content":"Answer A - Autoscaling introduces variable costs — which defeats the goal of cost consistency\n\nAnswer B: it doesn’t convert to predictable costs — on-demand billing still applies per scan.\n\nAnswer C is best.","upvote_count":"1","poster":"desertlotus1211"}],"comment_id":"1358320","poster":"MarcoPellegrino","upvote_count":"1"},{"comment_id":"1351906","content":"Selected Answer: A\nEstimate a consistent spending doesn't mean overpay...","timestamp":"1738764840.0","upvote_count":"1","poster":"Augustax"},{"comment_id":"1351465","content":"A because allow flexibility and scaling, so setting a baseline with autoscaling ensures that the marketing team can handle their queries without large fluctuations in cost.","upvote_count":"1","timestamp":"1738682700.0","poster":"Maxd"},{"poster":"b3e59c2","comment_id":"1337904","upvote_count":"1","content":"Selected Answer: C\nC seems much more robust and reliable than B. We can keep spend consistent whilst not sacrificing on performance (if we do B, once the byte scan limit has been reached, users will not be able to perform any analysis which could be detrimental to business)","timestamp":"1736333220.0"},{"poster":"himadri1983","upvote_count":"2","timestamp":"1734202920.0","comment_id":"1326593","content":"Selected Answer: C\nThis is trick question. The answer B is setting quota on bytes but it does not address the cost variability. The C will give the predictable monthly cost."},{"comment_id":"1325811","timestamp":"1734029400.0","content":"Selected Answer: C\nAnswer appears to be C. Check the example from docs: https://cloud.google.com/bigquery/docs/reservations-workload-management#managing_your_workloads_and_departments_using_reservations","poster":"m_a_p_s","upvote_count":"1"},{"poster":"CloudAdrMX","comment_id":"1322606","timestamp":"1733455380.0","content":"It's a treaking question but it's C, they are asking for establish a consistent Bigquery analytics spend each month, if you put 500 slots as baseline and with no autoscaling, each month they'll get the a consistent Bigquery analytics spend.","upvote_count":"1"},{"poster":"cloud_rider","content":"Selected Answer: B\nA, C and D talks about slot counts, whereas question does not talk about any such requirement, we should not make assumption on slots required or not required. Option B provides the visibility of cost to the team and can be revised as needed. So B is the right option.","timestamp":"1732876860.0","comment_id":"1319665","upvote_count":"1"},{"timestamp":"1730564580.0","upvote_count":"4","poster":"8284a4c","comment_id":"1306238","content":"Selected Answer: A\nThe correct answer is:\nA. Create a BigQuery Enterprise reservation with a baseline of 250 slots and autoscaling set to 500 for the marketing team, and bill them back accordingly.\nHere's the rationale:\n\n Consistent Spend with Reservation: Creating a BigQuery Enterprise reservation provides the marketing team with dedicated slots, which can help stabilize and predict their monthly costs. By having a reservation baseline of 250 slots, they are guaranteed a certain level of performance and cost each month.\n Autoscaling for Flexibility: The autoscaling up to 500 slots allows the team to handle spikes in demand without being constrained by the fixed slot count. Autoscaling in this scenario enables some flexibility while still providing predictable spending due to the baseline.\n Billing Back: The reservation model allows for internal chargeback by department based on slot usage, helping the marketing team plan a predictable budget."},{"timestamp":"1730277780.0","content":"Answer is B. Custom quotas are a powerful feature that allow you to set hard limits on specific resource usage. In the case of BigQuery, quotas allow you to control query usage (number of bytes processed) at a project- or user-level. Project-level custom quotas limit the aggregate usage of all users in that project, while user-level custom quotas are separately applied to each user or service account within a project.\n\nCustom quotas are relevant when you are using BigQuery’s on-demand pricing model, which charges for the number of bytes processed by each query. When you are using the capacity pricing model, you are charged for compute capacity (measured in slots) used to run queries, so limiting the number of bytes processed is less useful.\n\nBy setting custom quotas, you can control the amount of query usage by different teams, applications, or users within your organization, preventing unexpected spikes in usage and costs.","comment_id":"1304910","upvote_count":"2","poster":"mi_yulai"},{"poster":"baimus","upvote_count":"1","content":"Selected Answer: C\nJust to clarify a point of confusion: setting a quota does not affect variability (as specified in the question). It means there is a limit to the maximum but it can still vary anywhere between zero and that maximum each month. It would also prevent the marking team actually performing the queries if set too low. C is the only one that makes sense, though the question \"why 500\" is a valid one, all the other answers simply do not deliver the requirements.","comment_id":"1294688","timestamp":"1728385320.0"},{"content":"Selected Answer: B\nIt should be B\nC is a subset of B. I mean you can put custom quota == 500 slots and obviously there wont be any auto scaling. that exactly the purpose of quota","comment_id":"1289230","poster":"Preetmehta1234","timestamp":"1727310600.0","upvote_count":"2"},{"content":"A Create a BigQuery Enterprise reservation with a baseline of 250 slots and autoscaling set to 500 for the marketing team, and bill them back accordingly.\n\nGIves a consistent baseline cost and allocation for peak times","upvote_count":"1","poster":"chrissamharris","timestamp":"1727288100.0","comment_id":"1289128"},{"timestamp":"1727218140.0","comment_id":"1288809","content":"Selected Answer: B\nThe objective here is not performance. It's more concerned about the spend each month. It's not about 250 slots or 500 slots. Selecting a custom quota will let you select what ever slot you want but stay consistent with it, rather than getting stick by a particular slot option.","poster":"Preetmehta1234","upvote_count":"3"},{"upvote_count":"2","comment_id":"1288807","timestamp":"1727217960.0","poster":"Preetmehta1234","content":"Selected Answer: B\nCustom Quota\nIf you have multiple BigQuery projects and users, you can manage costs by requesting a custom quota that specifies a limit on the amount of query data processed per day. Daily quotas are reset at midnight Pacific Time.\n\nCustom quota is proactive, so you can't run an 11 TB query if you have a 10 TB quota. Creating a custom quota on query data lets you control costs at the project level or at the user level.\n\nProject-level custom quotas limit the aggregate usage of all users in that project."},{"timestamp":"1726016760.0","content":"Selected Answer: A\nI think is A","poster":"4a8ffd7","comment_id":"1281832","upvote_count":"3"},{"timestamp":"1723940520.0","comment_id":"1267889","content":"Selected Answer: A\nBalance of Cost and Performance: By setting a baseline of slots to ensure a minimum cost, and enabling autoscaling for flexible scaling based on demand, we can achieve an optimal balance between cost and performance.\nCost Control: Setting a maximum number of slots allows us to clearly define a cost ceiling.\nBenefits of Enterprise Reservations: Enterprise reservations offer a higher discount rate compared to Standard reservations, and provide more stable performance.","poster":"viciousjpjp","upvote_count":"3"},{"content":"Selected Answer: C\nYes, you can reserve slots in BigQuery for on-demand billing:\nSlot recommender\nUse the slot recommender to get cost-optimized recommendations for on-demand workloads.\nCapacity commitment\nPurchase a capacity commitment to reserve capacity for a minimum amount of time and save on costs.\nCapacity-based pricing\nUse reservations to switch to capacity-based pricing, which lets you reserve a volume of slots. You pay for that capacity continuously every second it's deployed.","comment_id":"1263363","poster":"meh_33","timestamp":"1723274760.0","upvote_count":"2"},{"content":"Selected Answer: A\nExplanation:\nPredictability: The baseline of 250 slots ensures a predictable minimum spend each month.\nFlexibility: Autoscaling up to 500 slots allows the team to handle peak workloads without interruptions.\nBalanced Cost: While Option B limits daily spend, it can lead to disruptions in service. Option A offers a consistent monthly cost while still accommodating variable workloads efficiently.","upvote_count":"3","poster":"987af6b","comment_id":"1252598","timestamp":"1721580240.0"},{"timestamp":"1719495840.0","content":"Selected Answer: B\nWhy 500 slots?","comment_id":"1238195","upvote_count":"2","poster":"8ad5266"},{"content":"Selected Answer: C\nC. Create a BigQuery reservation with a baseline of 500 slots with no autoscaling for the marketing team, and bill them back accordingly.","poster":"fitri001","timestamp":"1718619540.0","upvote_count":"1","comment_id":"1231867"},{"timestamp":"1716458160.0","content":"Selected Answer: C\nC. Create a BigQuery reservation with a baseline of 500 slots with no autoscaling for the marketing team, and bill them back accordingly.","upvote_count":"1","comment_id":"1216441","poster":"virat_kohli"},{"comment_id":"1213316","poster":"Anudeep58","content":"Selected Answer: B\nThe question clearly mentions, that team is using the on-demand billing mode in BiqQuery, which charges for the number of bytes processed by each query. So limiting the bytes processed will be the solution.\nhttps://cloud.google.com/blog/products/data-analytics/manage-bigquery-costs-with-custom-quotas","comments":[{"timestamp":"1718508060.0","upvote_count":"1","poster":"Anudeep58","content":"C. Create a BigQuery reservation with a baseline of 500 slots with no autoscaling for the marketing team, and bill them back accordingly.\n\nThis option provides the marketing team with a predictable monthly cost by reserving a fixed number of slots, ensuring that they have dedicated resources without the variability introduced by autoscaling or on-demand pricing. This setup also simplifies budgeting and financial planning for the marketing team, as they will have a consistent expense each month.","comment_id":"1231188"}],"timestamp":"1716036000.0","upvote_count":"4"},{"comment_id":"1201189","comments":[{"timestamp":"1714122540.0","upvote_count":"1","poster":"LaxmanTiwari","content":"u spot on MissK1371","comment_id":"1202502"}],"upvote_count":"3","poster":"MissK1371","content":"Selected Answer: B\nat first I thought C for best practices but the questions does not ask to lower the cost just to make the spend consistent","timestamp":"1713945540.0"},{"content":"Selected Answer: B\nAs wrote @Sofia98 the company using \"on-demand billing model\", so the best solution should be the B, https://cloud.google.com/blog/products/data-analytics/manage-bigquery-costs-with-custom-quotas","comment_id":"1193999","timestamp":"1712867040.0","upvote_count":"1","poster":"BigDataBB"},{"content":"I agree that at first sight option C seems best. However, the question mentions that they are currently on the on-demand billing model and option C does not mention anything about changing the pricing model from on-demand to capacity computing (BigQuery Standard or Enterprise edition). I don't believe it is possible to reserve slots with on-demand billing.","timestamp":"1708276620.0","comment_id":"1153449","upvote_count":"2","poster":"ML6"},{"poster":"tibuenoc","upvote_count":"3","timestamp":"1706864460.0","content":"Selected Answer: D\nOption D - https://cloud.google.com/bigquery/pricing\nStandard Pay-as-you-go Reservation: This model charges only for the slots used, aligning with the marketing team's need for predictable costs. On-demand pricing would lead to variable costs, while committed use discounts or reservations with fixed costs wouldn't provide the needed flexibility.\n\nBaseline of 0 Slots: Setting a baseline of 0 ensures no upfront commitment and avoids unused capacity charges if the marketing team's usage is lower than expected.\n\nAutoscaling Up to 500 Slots: Autoscaling provides the flexibility to handle unexpected spikes in usage without incurring on-demand pricing costs. The 500-slot limit sets a reasonable upper bound to control spending.\n\nBilling Back: Billing the marketing team based on their actual usage promotes cost awareness and encourages responsible resource utilization.","comment_id":"1138334","comments":[{"timestamp":"1708518780.0","upvote_count":"1","comment_id":"1155524","content":"Looks like only the baseline slots are guaranteed and immediately available for use where as autoscaling slots are not guaranteed immediately.","comments":[{"comments":[{"timestamp":"1708518900.0","poster":"JyoGCP","content":"which will give a consistent bill per month","upvote_count":"1","comment_id":"1155527"}],"timestamp":"1708518780.0","upvote_count":"1","poster":"JyoGCP","content":"So C is a better option","comment_id":"1155525"}],"poster":"JyoGCP"}]},{"poster":"lipa31","upvote_count":"3","comment_id":"1130035","content":"anybody for D ? https://cloud.google.com/bigquery/docs/slots-autoscaling-intro","timestamp":"1706042100.0"},{"upvote_count":"4","poster":"Sofiia98","content":"Selected Answer: B\nhttps://cloud.google.com/blog/products/data-analytics/manage-bigquery-costs-with-custom-quotas","timestamp":"1704876060.0","comment_id":"1118377"},{"upvote_count":"1","comments":[{"upvote_count":"1","comment_id":"1118376","timestamp":"1704876000.0","content":"Provide, please, the reference","poster":"Sofiia98"}],"timestamp":"1704362880.0","content":"Selected Answer: C\nC. Create a BigQuery reservation with a baseline of 500 slots with no autoscaling for the marketing team, and bill them back accordingly.","poster":"scaenruy","comment_id":"1113519"}],"exam_id":11,"unix_timestamp":1704362880,"topic":"1","url":"https://www.examtopics.com/discussions/google/view/130289-exam-professional-data-engineer-topic-1-question-287/","answer_description":""},{"id":"x294au2VgfRDVKb12GPo","answer":"D","answers_community":["D (100%)"],"answer_ET":"D","exam_id":11,"unix_timestamp":1704363000,"question_images":[],"topic":"1","timestamp":"2024-01-04 11:10:00","question_text":"You are part of a healthcare organization where data is organized and managed by respective data owners in various storage services. As a result of this decentralized ecosystem, discovering and managing data has become difficult. You need to quickly identify and implement a cost-optimized solution to assist your organization with the following:\n\n• Data management and discovery\n• Data lineage tracking\n• Data quality validation\n\nHow should you build the solution?","answer_images":[],"choices":{"B":"Build a new data discovery tool on Google Kubernetes Engine that helps with new source onboarding and data lineage tracking.","D":"Use Dataplex to manage data, track data lineage, and perform data quality validation.","A":"Use BigLake to convert the current solution into a data lake architecture.","C":"Use BigQuery to track data lineage, and use Dataprep to manage data and perform data quality validation."},"answer_description":"","isMC":true,"discussion":[{"upvote_count":"1","comment_id":"1252607","poster":"987af6b","content":"Selected Answer: D\nD. Dataplex","timestamp":"1721580720.0"},{"content":"Selected Answer: D\nOption D, no doubt","timestamp":"1718619780.0","upvote_count":"2","poster":"fitri001","comment_id":"1231870"},{"timestamp":"1710513780.0","upvote_count":"2","poster":"hanoverquay","comment_id":"1174319","content":"Selected Answer: D\nOption D, no doubt"},{"upvote_count":"2","content":"Selected Answer: D\nOption D","poster":"JyoGCP","comment_id":"1155702","timestamp":"1708533240.0"},{"poster":"Matt_108","upvote_count":"3","comment_id":"1121900","content":"Selected Answer: D\nClearly D","timestamp":"1705166520.0"},{"poster":"Sofiia98","comment_id":"1118379","timestamp":"1704876660.0","upvote_count":"2","content":"Selected Answer: D\nAgree with Dataplex option"},{"comment_id":"1117938","poster":"raaad","content":"Selected Answer: D\nStraight forward","upvote_count":"4","timestamp":"1704844560.0"},{"upvote_count":"1","content":"Selected Answer: D\nD. Use Dataplex to manage data, track data lineage, and perform data quality validation.","comment_id":"1113521","poster":"scaenruy","timestamp":"1704363000.0"}],"url":"https://www.examtopics.com/discussions/google/view/130291-exam-professional-data-engineer-topic-1-question-288/","question_id":210}],"exam":{"isMCOnly":true,"isImplemented":true,"provider":"Google","lastUpdated":"11 Apr 2025","isBeta":false,"numberOfQuestions":319,"name":"Professional Data Engineer","id":11},"currentPage":42},"__N_SSP":true}