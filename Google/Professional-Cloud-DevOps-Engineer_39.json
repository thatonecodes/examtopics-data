{"pageProps":{"questions":[{"id":"9XRvkoue8KM6ZQUgWqe1","answer":"C","unix_timestamp":1697505900,"question_text":"You are the on-call Site Reliability Engineer for a microservice that is deployed to a Google Kubernetes Engine (GKE) Autopilot cluster. Your company runs an online store that publishes order messages to Pub/Sub, and a microservice receives these messages and updates stock information in the warehousing system. A sales event caused an increase in orders, and the stock information is not being updated quickly enough. This is causing a large number of orders to be accepted for products that are out of stock. You check the metrics for the microservice and compare them to typical levels:\n\n//IMG//\n\n\nYou need to ensure that the warehouse system accurately reflects product inventory at the time orders are placed and minimize the impact on customers. What should you do?","topic":"1","question_id":191,"url":"https://www.examtopics.com/discussions/google/view/123804-exam-professional-cloud-devops-engineer-topic-1-question-94/","timestamp":"2023-10-17 03:25:00","answer_images":[],"discussion":[{"comment_id":"1045433","comments":[{"comments":[{"content":"The increase in the number of undelivered messages indicates that the microservice is not fully processing the messages.\nIt is believed that the number of messages that can be processed can be increased by increasing the number of pods.","upvote_count":"3","timestamp":"1714339260.0","poster":"YushiSato","comment_id":"1056460"}],"comment_id":"1046413","upvote_count":"1","poster":"ABZ10","content":"How does increasing the pod replica help? The pod memory and cpu have barely gone up. Shouldn't it be B?","timestamp":"1713395040.0"}],"timestamp":"1713317100.0","content":"Selected Answer: C\nAgree with C as answer.","poster":"activist","upvote_count":"6"},{"poster":"piyu1515","content":"Given the scenario where a sales event has caused an increase in orders and the stock information is not being updated quickly enough, leading to a large number of orders being accepted for out-of-stock products, the most appropriate action to ensure that the warehouse system accurately reflects product inventory at the time of order placement and minimize the impact on customers would be:\n\nC. Increase the number of Pod replicas.\n\nIncreasing the number of Pod replicas can help scale out the microservice to handle the increased load efficiently. This will allow the microservice to process the order messages and update the stock information in the warehousing system more quickly, reducing the likelihood of accepting orders for out-of-stock products. This approach addresses the issue at its root by ensuring that there are enough resources available to handle the increased workload effectively.","comment_id":"1142892","timestamp":"1722989400.0","upvote_count":"1"},{"upvote_count":"1","poster":"xhilmi","timestamp":"1717629480.0","content":"Selected Answer: C\nIncreasing the number of Pod replicas (Option C) is a suitable solution as it allows the microservice deployed in the Google Kubernetes Engine (GKE) Autopilot cluster to efficiently handle a surge in orders during a sales event.\n\nBy scaling horizontally, additional instances of the microservice are created, enabling parallel processing of order messages and reducing the backlog. This approach leverages the flexibility and automation of GKE Autopilot, ensuring that the infrastructure dynamically adapts to the increased load, providing a more responsive and scalable solution to minimize the impact on customers and maintain accurate stock information in the warehousing system.","comment_id":"1088923"},{"poster":"filipemotta","timestamp":"1717523700.0","comment_id":"1087925","content":"Selected Answer: C\nAnswer C. This action would help accommodate the increased load by parallelizing the work and should be implemented first to mitigate the issue. If after scaling the number of replicas the CPU or memory usage approaches the limits, then considering scaling up the resources (CPU and memory limits) for each Pod might be the next step.","upvote_count":"1"},{"poster":"Andrei_Z","comment_id":"1080196","upvote_count":"1","timestamp":"1716650820.0","content":"Selected Answer: C\nThe average acknowledgement latency from Pub/Sub has not significantly increased, which suggests that Pub/Sub is still able to handle the message load effectively. However, the significant increase in the oldest unacknowledged message age and the number of undelivered messages indicates that the pods are not processing the messages quickly enough.\n\nIncreasing the number of Pod replicas would allow your microservice to process more messages concurrently, reducing the backlog and ensuring that stock information is updated more quickly."},{"poster":"PGontijo","content":"B for sure.","comment_id":"1052134","comments":[{"content":"It would work but degrade the user experience and therefore not suitable.","comment_id":"1102875","timestamp":"1718988240.0","poster":"Raz0r","upvote_count":"1"}],"upvote_count":"1","timestamp":"1713894660.0"}],"exam_id":6,"answer_description":"","answer_ET":"C","isMC":true,"question_images":["https://img.examtopics.com/professional-cloud-devops-engineer/image1.png"],"choices":{"D":"Increase the Pod CPU and memory limits.","A":"Decrease the acknowledgment deadline on the subscription.","C":"Increase the number of Pod replicas.","B":"Add a virtual queue to the online store that allows typical traffic levels."},"answers_community":["C (100%)"]},{"id":"bzszwYCNcU0LCpV9s81t","topic":"1","discussion":[{"poster":"ManishKS","comment_id":"1022133","timestamp":"1696150140.0","content":"Option C is not as effective as Option D because it does not enforce the network policies and DaemonSet configurations. This means that unauthorized changes could still be made to the configurations.\n\nConfig Sync is a tool that can be used to synchronize Kubernetes configurations across multiple clusters. However, it does not prevent unauthorized changes from being made to the configurations.\n\nPolicy Controller is a tool that can be used to enforce Kubernetes configurations. It does this by monitoring the Kubernetes API for changes to the configurations and automatically reverting unauthorized changes.\n\nTherefore, Option D is a more secure and reliable option for ensuring that the network policies and DaemonSet are enforced and installed consistently across the three environments.","upvote_count":"9"},{"timestamp":"1741899540.0","comment_id":"1394257","content":"Selected Answer: C\nWhy use Cloud Build?\n- Cloud Build allows you to automate rendering and deploying Kubernetes manifests.\n- It ensures that changes are version-controlled and applied consistently across all environments.\n\nWhy use Config Sync?\n- Config Sync ensures that your GKE clusters stay in sync with the GitHub repository (source of truth).\n- It automatically detects and corrects drift, enforcing the desired state.\n- It is Google's recommended practice for managing multi-cluster configurations.","upvote_count":"2","poster":"cachopo"},{"poster":"manishk39","upvote_count":"1","content":"Selected Answer: C\nUse Cloud Build to render and deploy the network policies and the DaemonSet. Set up Config Sync to sync the configurations for the three environments. This approach uses Cloud Build for CI/CD processes and leverages Config Sync for GitOps practices, ensuring that your GKE environments are consistent with the configurations defined in your GitHub repositories. Config Sync will continuously reconcile the cluster state with the desired state in the repository, and Policy Controller which is part of Config Sync will enforce the network policies.","comment_id":"1359280","timestamp":"1740060540.0"},{"timestamp":"1739750760.0","upvote_count":"1","poster":"JonathanSJ","content":"Selected Answer: C\nI will go for C.\n\nCloud Build: Cloud Build is excellent for building and deploying configurations. It can be used to render your network policies and DaemonSet configurations from your GitHub repository.   \nConfig Sync: Config Sync is specifically designed for managing and synchronizing configurations across multiple Kubernetes clusters.1 It allows you to define your desired state in a Git repository (your GitHub repo) and then automatically applies those configurations to your GKE clusters (development, staging, and production).2 This ensures consistency across environments and provides version control for your infrastructure configurations.","comment_id":"1357495"},{"timestamp":"1723395840.0","comment_id":"1264242","upvote_count":"2","poster":"surfer111","content":"Its a toss up here. C hits all the keywords in \"https://cloud.google.com/kubernetes-engine/enterprise/config-sync/docs/overview\" - source of truth, gitops, but does not talk about enforcement. However, policy controller is a subset of configsync and it does handle enforcement.\n\n\"Constraints can be applied directly to your clusters using the Kubernetes API, or distributed to a set of clusters from a centralized source, like a Git repository, by using Config Sync.\" https://cloud.google.com/kubernetes-engine/enterprise/policy-controller/docs/overview#constraints\n\nReally seems like a trick question leading you to D when C is the right answer with the knowledge that you would configure policy controller as a sub step when setting up config sync for your gitops."},{"content":"Selected Answer: C\nThis method leverages Cloud Build for rendering and deploying configurations, and Config Sync to ensure that the desired state specified in your GitHub repositories is consistently applied across all GKE clusters. This approach provides robust management and automatic synchronization, ensuring that configurations remain consistent across development, staging, and production environments.","poster":"winston9","comments":[{"poster":"winston9","comment_id":"1225334","timestamp":"1717666740.0","upvote_count":"1","content":"C better than D: While Cloud Build and Policy Controller can work together, Config Sync provides a more complete solution for synchronizing configurations across multiple clusters."}],"comment_id":"1225333","timestamp":"1717666620.0","upvote_count":"1"},{"comment_id":"1209899","content":"Selected Answer: D\nShould be D","poster":"dija123","timestamp":"1715449140.0","upvote_count":"1"},{"timestamp":"1708044780.0","upvote_count":"1","poster":"Xoxoo","comment_id":"1151658","content":"Selected Answer: D\nPolicy Controller can enforce the configurations specified in the repositories, ensuring consistency across the environments and enforcing compliance with defined policies."},{"timestamp":"1707049020.0","comment_id":"1140070","content":"Option C is the right one.\"\nCloud Build:\n\nIdeal for building and deploying software artifacts based on your GitHub repositories, your chosen source of truth.\nRenders your network policies and DaemonSet configurations, ensuring consistency before deployment.\nConfig Sync:\n\nDesigned for configuration management across GKE clusters.\nContinuously synchronizes your rendered configurations (network policies and DaemonSet) from GitHub to all three environments (development, staging, production).\nProvides automated drift detection and remediation, ensuring consistency remains enforced.","poster":"medox89","upvote_count":"3"},{"poster":"xhilmi","content":"Selected Answer: D\nOption D is the recommended approach for ensuring consistency across the three Google Kubernetes Engine (GKE) environments—development, staging, and production—while adhering to Google-recommended practices.\n\nBy using Cloud Build to render and deploy network policies and a DaemonSet, and implementing Policy Controller, you can enforce configurations uniformly across environments.\n\nPolicy Controller ensures that the deployed configurations align with your desired state, providing a consistent and policy-driven approach. This method leverages the declarative nature of Kubernetes configurations, facilitating configuration management.\n\nOverall, Option D combines infrastructure-as-code principles with policy enforcement to maintain consistency and enhance manageability across GKE clusters in different environments.","comment_id":"1088927","timestamp":"1701825720.0","upvote_count":"1"},{"comment_id":"1075599","poster":"Andrei_Z","upvote_count":"1","timestamp":"1700497800.0","content":"Selected Answer: D\nI would go for D as well"},{"upvote_count":"1","content":"Selected Answer: D\n\"Policy Controller can catch and enforce policy violations on those resources before they are deployed. \"\n\nhttps://cloud.google.com/anthos-config-management/docs/concepts/config-controller-overview","poster":"lelele2023","comment_id":"1061985","timestamp":"1699090080.0"}],"answer":"D","question_id":192,"exam_id":6,"answers_community":["D (50%)","C (50%)"],"question_text":"Your team deploys applications to three Google Kubernetes Engine (GKE) environments: development, staging, and production. You use GitHub repositories as your source of truth. You need to ensure that the three environments are consistent. You want to follow Google-recommended practices to enforce and install network policies and a logging DaemonSet on all the GKE clusters in those environments. What should you do?","question_images":[],"unix_timestamp":1696150140,"url":"https://www.examtopics.com/discussions/google/view/122018-exam-professional-cloud-devops-engineer-topic-1-question-95/","isMC":true,"answer_ET":"D","choices":{"B":"Use Google Cloud Deploy to deploy the DaemonSet and use Policy Controller to configure the network policies. Use Cloud Monitoring to detect drifts from the source in the repository and Cloud Functions to correct the drifts.","D":"Use Cloud Build to render and deploy the network policies and the DaemonSet. Set up a Policy Controller to enforce the configurations for the three environments.","A":"Use Google Cloud Deploy to deploy the network policies and the DaemonSet. Use Cloud Monitoring to trigger an alert if the network policies and DaemonSet drift from your source in the repository.","C":"Use Cloud Build to render and deploy the network policies and the DaemonSet. Set up Config Sync to sync the configurations for the three environments."},"answer_description":"","answer_images":[],"timestamp":"2023-10-01 10:49:00"},{"id":"qrhPPlNrBnrKwwVS2TOz","url":"https://www.examtopics.com/discussions/google/view/122634-exam-professional-cloud-devops-engineer-topic-1-question-96/","answers_community":["B (100%)"],"exam_id":6,"question_text":"You are using Terraform to manage infrastructure as code within a CI/CD pipeline. You notice that multiple copies of the entire infrastructure stack exist in your Google Cloud project, and a new copy is created each time a change to the existing infrastructure is made. You need to optimize your cloud spend by ensuring that only a single instance of your infrastructure stack exists at a time. You want to follow Google-recommended practices. What should you do?","discussion":[{"content":"Selected Answer: B\nThe Terraform tfstate file contains the state of the infrastructure managed by Terraform. Storing this file in Cloud Storage with the Terraform Google Cloud Storage (GCS) backend ensures that it is shared across executions of the pipeline.","upvote_count":"1","comment_id":"1151659","timestamp":"1723762500.0","poster":"Xoxoo"},{"upvote_count":"1","content":"Selected Answer: B\nhttps://cloud.google.com/docs/terraform/resource-management/managing-infrastructure-as-code#configuring_terraform_to_store_state_in_a_cloud_storage_bucket","comment_id":"1144529","timestamp":"1723118640.0","poster":"alpha_canary"},{"timestamp":"1719407880.0","poster":"kish18","upvote_count":"1","comment_id":"1106128","content":"B is correct, storing the state file to the gcs bucket maintains the state correctly"},{"content":"Selected Answer: B\nOption B is the recommended approach to optimize cloud spending and ensure a single instance of your infrastructure stack exists at a time when using Terraform within a CI/CD pipeline.\n\nBy storing and retrieving the terraform.tfstate file from Cloud Storage with the Terraform Google Cloud Storage (GCS) backend, you centralize the state management. This ensures that Terraform has a single source of truth for the infrastructure state, preventing the creation of redundant instances.\n\nThe GCS backend enables state locking, consistency, and collaboration across the CI/CD pipeline, allowing for proper tracking and management of infrastructure changes. This practice helps avoid unnecessary duplication of resources and promotes efficient cloud spend management.","poster":"xhilmi","comment_id":"1088928","upvote_count":"1","timestamp":"1717630080.0"},{"comment_id":"1063508","upvote_count":"1","content":"Selected Answer: B\nB is correct.","timestamp":"1714963620.0","poster":"mshafa"},{"poster":"nhiguchi","timestamp":"1713760260.0","content":"Selected Answer: B\nB is correct.","upvote_count":"3","comment_id":"1050206"},{"timestamp":"1712373360.0","comment_id":"1026194","poster":"PrayasMohanty","content":"B appears correct to me.","upvote_count":"3"}],"unix_timestamp":1696562160,"question_id":193,"answer_images":[],"timestamp":"2023-10-06 05:16:00","answer":"B","choices":{"A":"Create a new pipeline to delete old infrastructure stacks when they are no longer needed.","D":"Update the pipeline to remove any existing infrastructure before you apply the latest configuration.","B":"Confirm that the pipeline is storing and retrieving the terraform.tfstate file from Cloud Storage with the Terraform gcs backend.","C":"Verify that the pipeline is storing and retrieving the terraform.tfstate file from a source control."},"question_images":[],"answer_ET":"B","topic":"1","isMC":true,"answer_description":""},{"id":"Op8in8vHhfhamj9lHjNJ","timestamp":"2023-10-01 11:21:00","url":"https://www.examtopics.com/discussions/google/view/122020-exam-professional-cloud-devops-engineer-topic-1-question-97/","question_images":[],"exam_id":6,"question_text":"You are creating Cloud Logging sinks to export log entries from Cloud Logging to BigQuery for future analysis. Your organization has a Google Cloud folder named Dev that contains development projects and a folder named Prod that contains production projects. Log entries for development projects must be exported to dev_dataset, and log entries for production projects must be exported to prod_dataset. You need to minimize the number of log sinks created, and you want to ensure that the log sinks apply to future projects. What should you do?","isMC":true,"topic":"1","answers_community":["D (100%)"],"discussion":[{"poster":"PrayasMohanty","comment_id":"1026220","upvote_count":"6","timestamp":"1728186960.0","content":"I vote for D"},{"timestamp":"1733448660.0","upvote_count":"2","comment_id":"1088930","poster":"xhilmi","content":"Selected Answer: D\nTo minimize the number of log sinks and ensure that the log sink configurations are applied to future projects, the optimal choice is Option D.\n\nBy creating an aggregated log sink at the folder level for both the Dev and Prod folders, you can enforce consistent export configurations for all existing and forthcoming projects within each folder.\n\nThis approach facilitates centralized management, eliminating the need for creating individual sinks for each project. It reduces complexity and ensures that any new projects added to the Dev or Prod folders will automatically adopt the log sink configuration, streamlining the process and fostering uniformity throughout the organization."},{"comment_id":"1070358","timestamp":"1731586920.0","upvote_count":"1","poster":"khoukha","content":"Selected Answer: D\nneed to create two sinks at folder level. if there is new projects inside each folder, the rule will be applied to them too"},{"content":"D for sure.","timestamp":"1729706160.0","poster":"PGontijo","comment_id":"1052138","upvote_count":"3"},{"timestamp":"1727774460.0","content":"C. Create two aggregated log sinks at the organization level, and filter by project ID.\n\nBy creating two aggregated log sinks at the organization level and applying filters based on project ID, you can achieve the desired log entry routing for both development and production projects. This approach allows for scalability and ensures that future projects in the respective folders will inherit the log sink configurations.","comments":[{"timestamp":"1731174420.0","content":"but doesn't apply for future projects hence D is a better option","poster":"elskander","upvote_count":"3","comment_id":"1066578"},{"timestamp":"1727953140.0","content":"I vote for D. If you use project ID, you can't scale. Your explanation also suggests D, because if you configure the sink at folder level, all projects into folder (present and future) inherit the sink.","comment_id":"1023857","poster":"syslog","upvote_count":"2"}],"poster":"ManishKS","comment_id":"1022145","upvote_count":"2"}],"unix_timestamp":1696152060,"answer_description":"","question_id":194,"choices":{"C":"Create two aggregated log sinks at the organization level, and filter by project ID.","A":"Create a single aggregated log sink at the organization level.","B":"Create a log sink in each project.","D":"Create an aggregated log sink in the Dev and Prod folders."},"answer_images":[],"answer":"D","answer_ET":"D"},{"id":"9Mgm9klCrUOOhPp3IkxG","question_text":"Your company runs services by using multiple globally distributed Google Kubernetes Engine (GKE) clusters. Your operations team has set up workload monitoring that uses Prometheus-based tooling for metrics, alerts, and generating dashboards. This setup does not provide a method to view metrics globally across all clusters. You need to implement a scalable solution to support global Prometheus querying and minimize management overhead. What should you do?","isMC":true,"question_images":[],"exam_id":6,"answers_community":["D (100%)"],"timestamp":"2023-10-04 14:35:00","answer":"D","topic":"1","unix_timestamp":1696422900,"answer_ET":"D","url":"https://www.examtopics.com/discussions/google/view/122383-exam-professional-cloud-devops-engineer-topic-1-question-98/","question_id":195,"answer_images":[],"discussion":[{"upvote_count":"7","timestamp":"1729165800.0","poster":"Skadian","comment_id":"1045941","content":"D options is looking more sutaible.\nhttps://cloud.google.com/stackdriver/docs/managed-prometheus"},{"poster":"xhilmi","content":"Selected Answer: D\nThe most effective solution for achieving scalable global Prometheus querying while minimizing management complexity is Option D, \"Configure Google Cloud Managed Service for Prometheus.\"\n\nThis approach involves leveraging the fully managed service offered by Google Cloud for Prometheus, which streamlines the collection, querying, and alerting on metrics. With this service, you can effortlessly centralize metrics from various globally distributed GKE clusters, eliminating the need for intricate configurations or federation mechanisms.\n\nBy opting for the Google Cloud Managed Service for Prometheus, you simplify operational tasks, reduce administrative overhead, and gain a unified and straightforward method for monitoring and analyzing metrics on a global scale across all clusters within the Google Cloud environment.","comment_id":"1088936","upvote_count":"1","timestamp":"1733448900.0"},{"upvote_count":"2","timestamp":"1730139180.0","content":"Selected Answer: D\nhttps://cloud.google.com/stackdriver/docs/managed-prometheus","comment_id":"1056322","poster":"koo_kai"},{"comment_id":"1052417","timestamp":"1729734720.0","poster":"activist","upvote_count":"1","content":"Answer D is the best."},{"poster":"PrayasMohanty","content":"Appear D is right","comment_id":"1024749","timestamp":"1728045300.0","upvote_count":"4","comments":[{"comment_id":"1026174","timestamp":"1728182760.0","poster":"Trueeye","upvote_count":"1","content":"@PrayasMohanty : Could you please review the questions set all the way to the end","comments":[{"upvote_count":"2","comment_id":"1040098","content":"Google Cloud Managed Service for Prometheus is Google Cloud's fully managed, multi-cloud, cross-project solution for Prometheus metrics. It lets you globally monitor and alert on your workloads, using Prometheus, without having to manually manage and operate Prometheus at scale.\n\nManaged Service for Prometheus collects metrics from Prometheus exporters and lets you query the data globally using PromQL, meaning that you can keep using any existing Grafana dashboards, PromQL-based alerts, and workflows. It is hybrid- and multi-cloud compatible, can monitor both Kubernetes and VM workloads, retains data for 24 months, and maintains portability by staying compatible with upstream Prometheus.\n\nReference: https://cloud.google.com/stackdriver/docs/managed-prometheus","poster":"PrayasMohanty","timestamp":"1728613500.0"}]}]}],"answer_description":"","choices":{"A":"Configure Prometheus cross-service federation for centralized data access.","B":"Configure workload metrics within Cloud Operations for GKE.","C":"Configure Prometheus hierarchical federation for centralized data access.","D":"Configure Google Cloud Managed Service for Prometheus."}}],"exam":{"numberOfQuestions":196,"lastUpdated":"11 Apr 2025","isBeta":false,"isMCOnly":true,"name":"Professional Cloud DevOps Engineer","id":6,"provider":"Google","isImplemented":true},"currentPage":39},"__N_SSP":true}