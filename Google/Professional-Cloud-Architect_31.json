{"pageProps":{"questions":[{"id":"v0BVcln2QkHOm1d3NxHl","answer":"A","unix_timestamp":1571965020,"topic":"1","url":"https://www.examtopics.com/discussions/google/view/7180-exam-professional-cloud-architect-topic-1-question-54/","timestamp":"2019-10-25 02:57:00","answer_images":[],"exam_id":4,"answer_description":"","question_id":151,"question_images":[],"discussion":[{"upvote_count":"55","comments":[{"poster":"tartar","comment_id":"151802","content":"A is ok","timestamp":"1596703140.0","upvote_count":"10"},{"poster":"kumarp6","timestamp":"1604238660.0","content":"A is the answer","upvote_count":"3","comment_id":"210488"},{"timestamp":"1614900000.0","upvote_count":"3","comment_id":"303680","content":"A is the best answer.","poster":"nitinz"}],"comment_id":"17294","content":"Internet access is not allowed so it should be A. CMIIW","poster":"zaki_b","timestamp":"1571965020.0"},{"upvote_count":"20","comment_id":"49599","poster":"KNG","content":"Should be A\nhttps://cloud.google.com/vpc/docs/configure-private-services-access\nNote: Even though the IP addresses for Google APIs and services are public, the traffic path from instances that are using Private Google Access to the Google APIs remains within Google's network.","timestamp":"1581527340.0"},{"upvote_count":"1","comment_id":"1335277","poster":"plumbig11","content":"Selected Answer: A\nPrivate Google Access subnet with cloud storage. A","timestamp":"1735761420.0"},{"timestamp":"1731962160.0","upvote_count":"1","comment_id":"1314212","poster":"Ekramy_Elnaggar","content":"Selected Answer: A\nPrivate Google Access allows your VM to access Google APIs and services (like Cloud Storage) without needing a public IP address. This is crucial in your restricted environment."},{"timestamp":"1725513780.0","content":"Selected Answer: A\nPrivate Google Access ensures the VM can reach Cloud Storage using its internal IP, while still restricting public internet access.","comment_id":"1278670","upvote_count":"3","poster":"maxdanny"},{"poster":"arotesa","comment_id":"1276641","timestamp":"1725276180.0","upvote_count":"1","content":"The Answer is D"},{"content":"Selected Answer: A\nA. Upload the required installation files to Cloud Storage. Configure the VM on a subnet with a Private Google Access subnet. Assign only an internal IP address to the VM. Download the installation files to the VM using gsutil.","timestamp":"1696518180.0","upvote_count":"1","poster":"AdityaGupta","comment_id":"1025789"},{"upvote_count":"5","content":"Those who are opting for B, Can please explain without Internet access and without Private Google Access enabled how will they communicate with Cloud Storage ? :)","comment_id":"757055","poster":"ppandher","timestamp":"1672029060.0"},{"comments":[{"upvote_count":"6","timestamp":"1671620520.0","content":"Option B: Uploading the required installation files to Cloud Storage and using firewall rules to block all traffic except the IP address range for Cloud Storage is not a valid option, as it does not allow the VM to access the installation files without public internet access.\n\nOption C: Uploading the required installation files to Cloud Source Repositories and using gcloud to download the files to the VM is not a valid option, as Cloud Source Repositories does not support storing large binary files such as installation files.\n\nOption D: Uploading the required installation files to Cloud Source Repositories and using firewall rules to block all traffic except the IP address range for Cloud Source Repositories is not a valid option, as it does not allow the VM to access the installation files without public internet access.","comment_id":"752186","poster":"omermahgoub"}],"upvote_count":"8","content":"The correct answer is A: Upload the required installation files to Cloud Storage. Configure the VM on a subnet with a Private Google Access subnet. Assign only an internal IP address to the VM. Download the installation files to the VM using gsutil.\n\nTo install specific software on a Compute Engine instance in a highly secured environment where public Internet access is not allowed, you can follow these steps:\n\nUpload the required installation files to Cloud Storage.\nConfigure the VM on a subnet with a Private Google Access subnet. This will allow the VM to access Google APIs and services, such as Cloud Storage, without requiring a public IP address or internet access.\nAssign only an internal IP address to the VM. This will ensure that the VM is not accessible from the public internet.\nDownload the installation files to the VM using gsutil, which is a command-line tool that allows you to access Cloud Storage from the VM.","timestamp":"1671620520.0","comment_id":"752185","poster":"omermahgoub"},{"timestamp":"1669556220.0","content":"Selected Answer: A\nEliminate B&D as it connect via public networks despite it being a Google Cloud service.","comment_id":"728255","poster":"habros","upvote_count":"1"},{"upvote_count":"1","timestamp":"1667751540.0","content":"Selected Answer: A\nok for A","comment_id":"712461","poster":"megumin"},{"comment_id":"711589","poster":"stevehlw","upvote_count":"2","comments":[{"upvote_count":"3","content":"Private Google access means - refer to https://www.youtube.com/watch?v=yd5FtV8aJkk","comment_id":"731084","poster":"ppandher","timestamp":"1669782120.0"}],"content":"With private Google access subnet, the vm can reach external network. With this setting, it violates “public Internet access from the Compute Engine VMs is not allowed”. Can someone explain why it’s not B instead?","timestamp":"1667628600.0"},{"timestamp":"1665931380.0","content":"A is good","upvote_count":"1","comment_id":"696293","poster":"AzureDP900"},{"poster":"minmin2020","comment_id":"694585","content":"Selected Answer: A\nA. Upload the required installation files to Cloud Storage. Configure the VM on a subnet with a Private Google Access subnet. Assign only an internal IP address to the VM. Download the installation files to the VM using gsutil.","timestamp":"1665734280.0","upvote_count":"1"},{"timestamp":"1662487740.0","upvote_count":"4","poster":"muneebarshad","comment_id":"661550","content":"Selected Answer: B\nConfiguring Private Google Access is the best way to access Google Services for VM that does not have access to the internet. In order to access Google Private APIs egress should be opened to the following IP Address restricted.googleapis.com (199.36.153.4/30). VM will leverage internal networking to access Cloud Storage \n\nhttps://cloud.google.com/vpc/docs/configure-private-google-access"},{"comment_id":"650545","content":"C because Cloud repositories is a private Git within Google cloud. Hence it is ideal for simple pull, push, clone type \"git\" operations. As this is within Google cloud and is a private git, you do not need public internet access","timestamp":"1661224200.0","upvote_count":"1","poster":"6721sora","comments":[{"comment_id":"684213","poster":"BiddlyBdoyng","content":"I think it's not this because Clouse Source Repositories is for source code. Sounds like we are looking for an executable?","timestamp":"1664629080.0","upvote_count":"1"}]},{"content":"Selected Answer: A\nC&D we are all eliminating becoz of source storage repo\nBetween A& B B looks more tempting to select because it mentions fire wallrule But the problem with B is the statement is wrong the access will happen from VM to storage and the statement mentions traffic from storage to Vm.\nHence A","timestamp":"1652284740.0","comment_id":"600204","poster":"amxexam","upvote_count":"3"},{"timestamp":"1641195720.0","comment_id":"515443","poster":"celina123123","upvote_count":"3","content":"Selected Answer: A\nYou have to set Private Google Access for communicating between VM and Storage"},{"content":"Unfortunately the question it's poorly designed.\nB is correct: https://cloud.google.com/vpc/docs/configure-private-google-access","timestamp":"1640630040.0","poster":"ehgm","upvote_count":"3","comment_id":"510550"},{"content":"A is the correct answer","upvote_count":"1","poster":"vincy2202","comment_id":"509106","timestamp":"1640436180.0"},{"comments":[{"timestamp":"1639702200.0","upvote_count":"3","content":"Cloud Source Repositories = Git repositories (for storing source code). \nCloud Storage is perfectly suitable for storing things like installation files.\nSo it's A :) \nhttps://cloud.google.com/source-repositories/docs/features","poster":"vartiklis","comment_id":"503272"}],"content":"It cannot be B because I don’t think anything like “restricted IP range for GCS” exists, at best we can use the private access feature. So while I agree the answer is A, can someone explain why it’s not C please?","timestamp":"1639292640.0","poster":"gcp_learner","upvote_count":"1","comment_id":"499834"},{"timestamp":"1638893100.0","comment_id":"496166","content":"Selected Answer: A\nGoogle Cloud Storage + Private Google Access","poster":"PhilipKoku","upvote_count":"2"},{"poster":"menon_sarath","timestamp":"1638858960.0","upvote_count":"1","comment_id":"495678","content":"Selected Answer: B\nOption B is better suited as it explicitly restricts external access only to DataStore"},{"upvote_count":"1","comment_id":"494735","content":"Go for A.\nTraffic from internet is not allowed.","timestamp":"1638747720.0","poster":"haroldbenites"},{"upvote_count":"2","comment_id":"494228","content":"Selected Answer: A\nA is correct here","poster":"pakilodi","timestamp":"1638700020.0"},{"poster":"ggzzzzzzz","timestamp":"1638614040.0","upvote_count":"1","comment_id":"493648","content":"Selected Answer: A\nA is the answer, internet access is not allowed"},{"upvote_count":"1","comment_id":"484537","content":"Is answer A or B?","timestamp":"1637612340.0","poster":"pnvijay"},{"poster":"lgonzf","timestamp":"1637513040.0","content":"Selected Answer: A\nAnswer is A, Internet is not allowed","upvote_count":"2","comment_id":"483453"},{"poster":"TheCloudBoy77","timestamp":"1637322120.0","upvote_count":"1","content":"Selected Answer: A\nA is correct answer.","comment_id":"481600"},{"timestamp":"1637152620.0","comment_id":"480018","content":"Answer is A - Transfer Appliance is recommended for data that exceeds 20 TB or would take more than a week to upload.","poster":"ravisar","upvote_count":"1"},{"poster":"[Removed]","timestamp":"1635448200.0","upvote_count":"1","content":"A is correct by using VM with private VPC.\nB is wrong selection.","comment_id":"469441"},{"upvote_count":"1","comments":[{"timestamp":"1640674140.0","upvote_count":"1","comment_id":"510857","comments":[{"comment_id":"648495","timestamp":"1660835760.0","content":"The hostname in the GCS URL will resolve to the \"Public\" IP address which is what we don't want.","upvote_count":"2","poster":"RitwickKumar"}],"content":"The hostname in the URL however will resolve to an IP address.\nThe wording of this answer still is weird.","poster":"HenkH"}],"comment_id":"467582","content":"A – enable Google private access for VM, upload file to GCS, and download it with gsutil.\nB – doesn’t work since GCS is managed service, so is accessed via URL, it doesn’t have IP needed by firewall.\nA – first enables Private access for VM, which only an option for VMs with internal IP only to access Google services. And such service in GCS. Moreover, without Private access mode, VM can access only VMs on the same subnet (and not to Google APIs, services).\nhttps://cloud.google.com/vpc/docs/private-access-options","poster":"MaxNRG","timestamp":"1635180840.0"},{"upvote_count":"1","timestamp":"1634975400.0","comment_id":"466468","content":"A is on point","poster":"Bakili"},{"poster":"sandipk91","timestamp":"1630409220.0","upvote_count":"2","comment_id":"436276","content":"A is correct"},{"comment_id":"402056","content":"Answer is A","poster":"MamthaSJ","timestamp":"1625762160.0","upvote_count":"2"},{"poster":"kopper2019","timestamp":"1624903080.0","comment_id":"393150","upvote_count":"1","comments":[{"content":"One question kopper2019, do you know whether the questions you have added has come in the real exam after 1st May,2021\n?","timestamp":"1625420160.0","upvote_count":"1","comment_id":"398552","poster":"gcpexam_ca"},{"comments":[{"comment_id":"398551","comments":[{"timestamp":"1625501820.0","comment_id":"399278","upvote_count":"1","poster":"Subodh_sar","content":"Is this examtopic good enough to pass exam after referring Q3."},{"content":"Can you please share the new question number, I checked the Q3 discussion but no luck","poster":"narendra4041","comment_id":"435277","timestamp":"1630312740.0","upvote_count":"1"}],"poster":"gcpexam_ca","timestamp":"1625420100.0","upvote_count":"1","content":"kopper2019 has added all the new questions probably appeared after 1st may,2021 in discussion section of Q3."}],"content":"What is Q3?","poster":"shiren","timestamp":"1625051640.0","comment_id":"394617","upvote_count":"1"},{"timestamp":"1654079100.0","content":"speaking this, in the week, i found questions decreased from 274 to 262.","poster":"elaineshi","upvote_count":"1","comment_id":"610106"}],"content":"hey guys check Q3 for new Qs, 49 New Qs"},{"timestamp":"1624782060.0","comment_id":"391890","upvote_count":"1","content":"A is correct because. Once file is uploaded Cloud storage. VM with private IP can access the installer","poster":"aviratna"},{"timestamp":"1624279020.0","content":"read the question: B says ip range for cloud storage, there is not such a thing, answer is A","poster":"cloudj","upvote_count":"3","comment_id":"387107"},{"content":"A is ok","poster":"areza","timestamp":"1623233400.0","comment_id":"378164","upvote_count":"1"},{"timestamp":"1623232620.0","comment_id":"378155","upvote_count":"1","content":"A is ok","poster":"areza"},{"timestamp":"1621408080.0","content":"A. Upload the required installation files to Cloud Storage. Configure the VM on a subnet with a Private Google Access subnet. Assign only an internal IP address to the VM. Download the installation files to the VM using gsutil.","poster":"victory108","upvote_count":"2","comment_id":"361121"},{"poster":"Ausias18","upvote_count":"2","comment_id":"325504","content":"Answer is A, gsutil","timestamp":"1617252300.0"},{"comments":[{"content":"As per my understanding, git repositories can be set-up as 'private' by individuals or organizations for restricted access?","comment_id":"861026","upvote_count":"1","poster":"[Removed]","timestamp":"1680611940.0"}],"poster":"CloudGenious","upvote_count":"5","comment_id":"290168","timestamp":"1613297460.0","content":"cloud repository is not good option as storing file in git repo it become public to all\n\nGoogle does not publish IP addess range for google cloud storage .Google publish the ip range used by its default domain such as *.googleapicom .These is not categories by service. so you can't tell which ip should bee used for cloud storage.\n\ncloud storage is best choice for file storage. when you configure the vm with internal ip and private google access is enable for a subnet and n/w require for google api and services for the vpc n/w are me. Google allowed vm with n/w interface in the enable subnet to send the packet from internal ip to external ip address of google api and services . So vm can now access the cloud storage and download the installation file to the vm using gsutil."},{"poster":"GS14","upvote_count":"1","comment_id":"279196","content":"I would go with A, private access to Google is secured, it will allow to download files from google storage on private network, VM with internal IP should be sufficient\n\nOn the contrary, in option B, if you block all the traffic except cloud storage, how will the application work if it needs any other ports ? Might be that this option is poorly worded","timestamp":"1611910560.0"},{"comment_id":"277488","content":"gsutil cp command does not go by ip or url. so, B is not properly worded. If B change to allow outbound traffic while block inbound traffic, it can be right.\nA is the answer","upvote_count":"1","poster":"bnlcnd","timestamp":"1611716280.0"},{"comment_id":"261337","content":"A is correct. As Private Google Access on Subnet level will allow gsutil command to run from VM using a secure VPC endpoint.","timestamp":"1609972500.0","upvote_count":"3","poster":"Arimaverick"},{"comment_id":"252115","timestamp":"1608905820.0","content":"A is correct","upvote_count":"1","poster":"Prakzz"},{"timestamp":"1608571320.0","poster":"sdsdfasdf4","upvote_count":"1","comment_id":"249518","content":"I've read documentation for quite a while. The only explicit ips that are mentioned in GCP docs is health check ranges. A is my answer."},{"comment_id":"232754","content":"A is correct.. you can't set up firewall rules diectly on cloud stg buckets it doesn't go by an ip","timestamp":"1606898100.0","upvote_count":"2","poster":"Chulbul_Pandey"},{"comment_id":"226799","content":"Option B is more secure, i don't need to allow access to google API Google internal services.\nall i need to allow access to the Storage service","timestamp":"1606235640.0","poster":"Hjameel","upvote_count":"3"},{"comment_id":"211196","upvote_count":"2","content":"Don't forget the firewall rule then must be created using a storage service account as target.","timestamp":"1604319600.0","poster":"occupatissimo"},{"upvote_count":"1","comment_id":"211189","content":"Due the implicit egress allow all firewall rule existence, to block all internet traffic, is necessary to insert a custom deny policy for 0.0.0.0/0. Then with an highest priority another allow firewall rule to reach storage is necessary.\nIn A is written to move the existing VM to another subnet, why? To apply private access? it seems too much.\nSo it should be B.","poster":"occupatissimo","timestamp":"1604318820.0"},{"poster":"francisco_guerra","content":"A the documentation says:\n \"You can allow these VMs to connect to the set of external IP addresses used by Google APIs and services by enabling Private Google Access on the subnet used by the VM's network interface.\"\n\nhttps://cloud.google.com/vpc/docs/configure-private-google-access\n\nand one of them is:\nCloud Storage JSON API Stores and retrieves potentially large, immutable data objects.","timestamp":"1604038620.0","upvote_count":"1","comment_id":"209052"},{"timestamp":"1602223380.0","poster":"LoganIsh","content":"as per udemy practice exam the answer had mentioned was B, despite using firewall rules to allow and deny the ip address to achieve.","upvote_count":"2","comment_id":"196521"},{"poster":"asheesh0574","upvote_count":"3","content":"Correct answer is A as the ideal solution would be to host the files internally in Cloud Storage. The access to Cloud Storage from VM can be enabled using Private Google Access, which can done using internal IPs.","timestamp":"1600945620.0","comment_id":"186092"},{"poster":"AshokC","upvote_count":"1","content":"A - \nhttps://cloud.google.com/vpc/docs/configure-private-google-access","timestamp":"1600190640.0","comment_id":"179980"},{"upvote_count":"1","timestamp":"1600062480.0","comment_id":"179087","poster":"passtest100","content":"C. Upload to Source Repositories is better than to Cloud Storage for deploying app on VM"},{"comment_id":"162508","upvote_count":"1","timestamp":"1597958880.0","poster":"wiqi","content":"A is the right solution. The purpose of Private Google Access is to do the same thing."},{"timestamp":"1597081140.0","poster":"haidertanveer0808","upvote_count":"1","content":"https://cloud.google.com/vpc/docs/private-access-options\nAnswer should be A","comment_id":"154727"},{"upvote_count":"2","poster":"TusharPinjan","content":"A should be the correct answer.. it has explicitly mentioned to assign only private IP to VM.","timestamp":"1595050260.0","comment_id":"137670"},{"comment_id":"127694","content":"A is right answer","timestamp":"1594032480.0","upvote_count":"1","poster":"Gobblegobble"},{"upvote_count":"2","poster":"mlantonis","comment_id":"117325","content":"A is the correct. You can use Private Google Access.\nhttps://cloud.google.com/vpc/docs/private-access-options#pga-supported","timestamp":"1592908140.0"},{"poster":"gfhbox0083","comment_id":"109504","content":"A, for sure.\nUsing Cloud Storage and Private Google Access subnet","timestamp":"1592056800.0","upvote_count":"1"},{"comment_id":"107349","content":"Its A https://cloud.google.com/vpc/docs/private-access-options#pga-supported","timestamp":"1591842720.0","upvote_count":"1","poster":"Tushant"},{"timestamp":"1591476780.0","poster":"asure","upvote_count":"2","comment_id":"104102","content":"All are in Agreement here, but dont we update/install software from repos, like git or any other. Dont you think answer should be around Cloud Source Repositories, private Git repository."},{"content":"A is the correct answer","timestamp":"1591266480.0","upvote_count":"1","poster":"Ziegler","comment_id":"102298"},{"upvote_count":"1","content":"A is the correct answer","poster":"Nirms","timestamp":"1591119660.0","comment_id":"101057"},{"poster":"AD2AD4","comment_id":"97374","content":"Final Decision to go with Option A - https://cloud.google.com/vpc/docs/configure-private-google-access","upvote_count":"3","timestamp":"1590654060.0"},{"poster":"chauvinhloi","timestamp":"1590449220.0","comment_id":"95701","upvote_count":"1","content":"The question is incorrect. It should be User -- access --> bastion host -- SSH --> private VM (GCE). The correct answer is A with Private Google Access. The same question can be found on Linux Academy."},{"timestamp":"1590264720.0","upvote_count":"1","poster":"gcp_aws","comment_id":"94544","content":"A is the correct answer"},{"timestamp":"1589681520.0","content":"Answer is A , Key here is private access","poster":"laksg","upvote_count":"1","comment_id":"90274"},{"timestamp":"1589506200.0","content":"A is the option. B is wrong as there is no IP range for Cloud storage.","upvote_count":"3","poster":"Jack_in_Large","comment_id":"89241"},{"content":"Must be A, @Eroc is right","poster":"YuriP","comment_id":"67531","timestamp":"1585030920.0","upvote_count":"1"},{"timestamp":"1584228240.0","content":"A it is since one can not expose GCS IP range with firewall rules. Access to GCS is controlled only via IAM and ACLs","poster":"SMS","upvote_count":"3","comment_id":"64071"},{"upvote_count":"1","content":"A should be correct. Selected A in exam","poster":"[Removed]","timestamp":"1582872540.0","comment_id":"56366"},{"comment_id":"52303","upvote_count":"1","content":"Yes, A is correct as many of you mentioned, only way to privately connect to cloud storage.","timestamp":"1582071540.0","poster":"mawsman"},{"content":"It's A, I never seen Cloud Storage IP range","poster":"ADVIT","upvote_count":"4","comment_id":"50133","timestamp":"1581616320.0"},{"upvote_count":"2","content":"Answer: A","timestamp":"1580393220.0","poster":"2g","comment_id":"44788"},{"timestamp":"1580031480.0","comment_id":"42833","upvote_count":"3","content":"A is correct. private subnet could reach google API without public ip address. see https://cloud.google.com/vpc/docs/configure-private-google-access","poster":"natpilot"},{"comment_id":"38057","timestamp":"1578834000.0","content":"Agree with B, because \"where public Internet access from the Compute Engine VMs\" is blocked by using the firewalls except the ip range for cloud storage. so it satisfies what is asked in the question. I will go for B","upvote_count":"1","poster":"AWS56"},{"comments":[{"poster":"res3","content":"And B doesn't work since GCS is accessed only via URL and not IP-address, as it was mentioned at B option.","comment_id":"124038","timestamp":"1593586800.0","upvote_count":"2"},{"poster":"bogd","comment_id":"295258","content":"There is no mention here of a 75TB dataset - I think you are referring to the next question...","timestamp":"1613843160.0","upvote_count":"1"}],"poster":"Hit1979","content":"Correct Answer is - A https://cloud.google.com/solutions/migration-to-google-cloud-transferring-your-large-datasets\n\nC and D is not applicable because they are talking about 75 TB which is huge. B is wrong because Dataprep is not required and insted Data Rehydrator is correct way.","comment_id":"32088","timestamp":"1577141100.0","upvote_count":"5"},{"upvote_count":"7","poster":"aviv","content":"Agreed with A","comment_id":"29658","timestamp":"1576383960.0"},{"comment_id":"17614","upvote_count":"4","comments":[{"timestamp":"1593586740.0","poster":"res3","content":"OK, Private Google Access is a nice point, but how do you expect from Google to get GCS IP-address since it's managed service?","upvote_count":"3","comment_id":"124035"}],"poster":"Eroc","timestamp":"1572098040.0","content":"It is B. Private Google Access opens internal IPs to the public. (https://cloud.google.com/vpc/docs/configure-private-google-access)"}],"question_text":"You are working in a highly secured environment where public Internet access from the Compute Engine VMs is not allowed. You do not yet have a VPN connection to access an on-premises file server. You need to install specific software on a Compute Engine instance. How should you install the software?","answers_community":["A (82%)","B (18%)"],"choices":{"A":"Upload the required installation files to Cloud Storage. Configure the VM on a subnet with a Private Google Access subnet. Assign only an internal IP address to the VM. Download the installation files to the VM using gsutil.","D":"Upload the required installation files to Cloud Source Repositories and use firewall rules to block all traffic except the IP address range for Cloud Source Repositories. Download the files to the VM using gsutil.","C":"Upload the required installation files to Cloud Source Repositories. Configure the VM on a subnet with a Private Google Access subnet. Assign only an internal IP address to the VM. Download the installation files to the VM using gcloud.","B":"Upload the required installation files to Cloud Storage and use firewall rules to block all traffic except the IP address range for Cloud Storage. Download the files to the VM using gsutil."},"isMC":true,"answer_ET":"A"},{"id":"9q75tBtGlUZ5o91hcAG2","unix_timestamp":1571810820,"question_id":152,"answer_description":"","discussion":[{"poster":"AshishK","comments":[{"comment_id":"33857","poster":"MyPractice","comments":[{"content":"Here is the link:\nhttps://cloud.google.com/transfer-appliance/docs/2.2/overview","comment_id":"240656","timestamp":"1607662140.0","poster":"onashwani","comments":[{"poster":"gcp_learner","comments":[{"content":"The request transfer appliance UI seems to suggest that it is not cost effective under 20TB of data.","timestamp":"1698454980.0","upvote_count":"2","comment_id":"1055933","poster":"mindhoc"}],"content":"But that link mentions a few hundred terabytes to 1 petabyte not 20TB or did I read that incorrectly?","timestamp":"1639368840.0","comment_id":"500368","upvote_count":"2"}],"upvote_count":"4"}],"content":"where did u get that 20 TB number - can help to share link?","timestamp":"1577758980.0","upvote_count":"1"},{"timestamp":"1594182420.0","poster":"Yahowmy","comments":[{"poster":"ccpmad","content":"stupid answer","comment_id":"1232478","timestamp":"1718722140.0","upvote_count":"2"},{"content":"There is NO mention of region - you dont assume anything NOT mentioned in the question therefore - Ans =A","upvote_count":"1","comment_id":"1132547","timestamp":"1706276220.0","poster":"NoCrapEva"},{"content":"Why assume a scenario no provided in the question. We need to choose the best case scenario based on available information instead of making assumptions. So A should be good.","comment_id":"649147","timestamp":"1660966380.0","poster":"Ramheadhunter","upvote_count":"5"}],"upvote_count":"11","content":"To this date Transfer Appliance supported locations are only\nUnited States \nCanada \nEuropean Union \nNorway \nSwitzerland.\nWhat if data reside in a location other than this?\nC is the most convenience for this scenario.","comment_id":"129450"}],"content":"It should be 'A'\nTransfer Appliance lets you quickly and securely transfer large amounts of data to Google Cloud Platform via a high capacity storage server that you lease from Google and ship to our datacenter. Transfer Appliance is recommended for data that exceeds 20 TB or would take more than a week to upload.","timestamp":"1576595820.0","comment_id":"30396","upvote_count":"35"},{"comment_id":"16870","content":"Why not A?","timestamp":"1571810820.0","upvote_count":"30","poster":"KouShikyou","comments":[{"content":"A is ok","timestamp":"1597396560.0","comment_id":"157972","upvote_count":"9","poster":"tartar"},{"comment_id":"210490","upvote_count":"2","poster":"kumarp6","content":"It is A","timestamp":"1604238720.0"},{"comment_id":"303683","timestamp":"1614900000.0","comments":[{"comments":[{"upvote_count":"1","timestamp":"1665985560.0","comment_id":"696956","content":"takes longer though","poster":"zr79"}],"timestamp":"1664457780.0","upvote_count":"1","comment_id":"682728","content":"I have moved 120 TB using gsutil- cost effectively!","poster":"Begum"}],"upvote_count":"14","content":"A, anything over 10TB goes via appliance.","poster":"nitinz"},{"comment_id":"348673","content":"Answer is A.","poster":"Koushick","timestamp":"1620049800.0","upvote_count":"4"},{"content":"IT IS C https://cloud.google.com/transfer-appliance/docs/4.0#suitability","timestamp":"1621696140.0","poster":"Trappatoni","upvote_count":"3","comment_id":"363759"},{"upvote_count":"2","comment_id":"364469","content":"https://cloud.google.com/transfer-appliance/docs/4.0\nanswer is C : Transfer Appliance is a hardware appliance you can use to securely migrate large volumes of data (from hundreds of terabytes up to 1 petabyte) to Google Cloud Platform without disrupting business operations.","poster":"Trappatoni","timestamp":"1621770600.0"},{"comment_id":"267818","content":"\"The gsutil tool is the standard tool for small- to medium-sized transfers (less than a few TB)\" \nhttps://cloud.google.com/solutions/migration-to-google-cloud-transferring-your-large-datasets#transfer-options","upvote_count":"4","timestamp":"1610705820.0","poster":"fraloca"}]},{"comment_id":"1360137","content":"Selected Answer: A\nwhat is a Rehydrator? a cloud version of WinRAR (compress, encrypt, decrypt, and decompress the data)","timestamp":"1740231600.0","upvote_count":"1","poster":"halifax"},{"poster":"alihabib","timestamp":"1737874980.0","upvote_count":"1","content":"Selected Answer: A\nA - Although I dont understand the Rehydration concept, but DataPrep is not for decryption. and gsutil is limited only for file size < 10GB ....","comment_id":"1346827"},{"upvote_count":"1","comment_id":"1335278","poster":"plumbig11","content":"Selected Answer: A\n75tb Move your data onto a Transfer Appliance. Use a Transfer Appliance Rehydrator to decrypt the data into Cloud Storage.","timestamp":"1735761480.0"},{"timestamp":"1735517700.0","content":"Selected Answer: A\nI will go for A.","upvote_count":"1","comment_id":"1333808","poster":"JonathanSJ"},{"poster":"kip21","comment_id":"1122709","upvote_count":"1","content":"A - Correct","timestamp":"1705252980.0"},{"comment_id":"1025791","poster":"AdityaGupta","content":"Selected Answer: A\nSending 75 TB of data on reliable 1.5 Gbps line will take about 6 days to complete data transfer online at the same time it will consume entire bandwidth. Hence use of Transfer appliace is required.","upvote_count":"1","timestamp":"1696518480.0"},{"comment_id":"928315","poster":"MaheshKaswan","upvote_count":"2","content":"Selected Answer: C\nOpion A is partially correct as you would not use a Transfer Appliance Rehydrator to decrypt the data. The Transfer Appliance itself is used to encrypt and decrypt the data. Option C is correct.","timestamp":"1687253460.0"},{"poster":"RVivek","comment_id":"790795","timestamp":"1674922800.0","content":"Selected Answer: A\ngsutil is recommanded for data size less than a TB. That rules out C and D\nB says decrypt data using Dataprep not sure this is possible. \nhttps://cloud.google.com/solutions/migration-to-google-cloud-transferring-your-large-datasets#transfer-options","upvote_count":"3"},{"upvote_count":"3","timestamp":"1673471580.0","comment_id":"772900","poster":"n_nana","content":"Selected Answer: A\nIt will be more precise with info about the bandwidth\nGoogle says: \nThe two main criteria to consider with Transfer Appliance are cost and speed. With reasonable network connectivity (for example, 1 Gbps), transferring 100 TB of data online takes over 10 days to complete. If this rate is acceptable, an online transfer is likely a good solution for your needs. If you only have a 100 Mbps connection (or worse from a remote location), the same transfer takes over 100 days. At this point, it's worth considering an offline-transfer option such as Transfer Appliance.\nSo even for such 100 TB google choose between transfer appliance or online transfer. not going with gsutil at all. it is clear gsutil is suitable for small to medium size (less than 1 TB)\n\nso with no more details, google recommendation is A"},{"timestamp":"1671620640.0","poster":"omermahgoub","comment_id":"752188","comments":[{"timestamp":"1671620700.0","content":"Option B: Using Cloud Dataprep to decrypt the data into Cloud Storage is not a valid option, as Cloud Dataprep is a data preparation tool that does not support data transfer or decryption.\n\nOption C: Using resumable transfers to upload the data into Cloud Storage is not a recommended option for moving large amounts of data, as resumable transfers are designed for smaller data sets and may not be efficient for transferring large amounts of data.\n\nOption D: Using streaming transfers to upload the data into Cloud Storage is not a recommended option for moving large amounts of data, as streaming transfers are designed for transferring real-time data streams and may not be efficient for transferring large amounts of data.\n\nTherefore, the correct answer is A: Move your data onto a Transfer Appliance. Use a Transfer Appliance Rehydrator to decrypt the data into Cloud Storage.","poster":"omermahgoub","upvote_count":"6","comment_id":"752189"}],"upvote_count":"3","content":"The correct answer is A: Move your data onto a Transfer Appliance. Use a Transfer Appliance Rehydrator to decrypt the data into Cloud Storage.\n\nTo move large amounts of data into Google Cloud, it is recommended to use Transfer Appliance. Transfer Appliance is a physical storage device that you can use to transfer large amounts of data to Google Cloud quickly and securely. Once you have moved your data onto a Transfer Appliance, you can use a Transfer Appliance Rehydrator to decrypt the data and load it into Cloud Storage."},{"upvote_count":"1","content":"Selected Answer: A\nok for A","timestamp":"1667751660.0","poster":"megumin","comment_id":"712463"},{"upvote_count":"1","timestamp":"1665931500.0","comment_id":"696298","content":"Option A Use a Transfer Appliance Rehydrator","poster":"AzureDP900"},{"timestamp":"1665734760.0","comment_id":"694590","poster":"minmin2020","content":"Selected Answer: A\nA. Move your data onto a Transfer Appliance. Use a Transfer Appliance Rehydrator to decrypt the data into Cloud Storage.","upvote_count":"1"},{"timestamp":"1659283500.0","comment_id":"640167","poster":"sgofficial","content":"Selected Answer: A\nA is correct ...\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#transfer-options \n1. gsutil is for lessthan <1 TB data with enough bandwidth, so C and D can be eliminated\n2. option b can be eliminated since dataprep for decription is not correct\n3. so only left over is a and its offline transfer, since the question did not give any time line when the transfer to be completed","upvote_count":"3"},{"comment_id":"604230","upvote_count":"1","content":"Selected Answer: A\nhttps://cloud.google.com/transfer-appliance/docs/4.0/overview#suitability","poster":"[Removed]","timestamp":"1653017940.0"},{"comment_id":"600208","timestamp":"1652285340.0","content":"Selected Answer: A\nI would eliminate B & C as the question clearly methos google recomendations.About 10 TB or canal to we need to me transfer appliance. Let's not worry about what regions Ohk for now.\nBetween A & B B is over kill as transfer applion allows decryption. Hence A","poster":"amxexam","upvote_count":"1"},{"comment_id":"586405","content":"Selected Answer: A\nIs Transfer Appliance suitable for me?\n\"Your data size is greater than or equal to 10TB.\"\nhttps://cloud.google.com/transfer-appliance/docs/4.0/overview#suitability","timestamp":"1650037620.0","poster":"meokey","upvote_count":"3"},{"comment_id":"545627","timestamp":"1644636000.0","content":"Selected Answer: A\nhttps://cloud.google.com/blog/ja/topics/developers-practitioners/how-transfer-your-data-google-cloud","poster":"awsarchitect5","upvote_count":"3"},{"poster":"Narinder","timestamp":"1642688880.0","comment_id":"528469","upvote_count":"2","content":"A, is the correct option for transferring data of few TB to PB.\n gsutil is viable option if the data size is about 1TB or less than that. \n\nReference: https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets"},{"upvote_count":"1","comment_id":"516261","content":"Selected Answer: A\nI vote A is correct.\nAgree with this thread.\nhttps://cloud.google.com/blog/ja/topics/developers-practitioners/how-transfer-your-data-google-cloud","poster":"OrangeTiger","timestamp":"1641277140.0"},{"timestamp":"1641076140.0","comment_id":"514725","content":"The gsutil tool is the standard tool for small- to medium-sized transfers (less than 1 TB) \nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#transfer-options","upvote_count":"2","comments":[{"comment_id":"514727","upvote_count":"1","poster":"Atnafu","timestamp":"1641076140.0","content":"A is the Answer"}],"poster":"Atnafu"},{"content":"A is the correct answer\nhttps://cloud.google.com/blog/products/storage-data-transfer/introducing-transfer-appliance-in-the-eu-for-cloud-data-migration","upvote_count":"1","comment_id":"509107","poster":"vincy2202","timestamp":"1640436480.0"},{"upvote_count":"2","comments":[{"content":"Nope - it's A because using C would take an unreasonably long time.","upvote_count":"1","comment_id":"503275","poster":"vartiklis","timestamp":"1639702560.0","comments":[{"comment_id":"620489","poster":"amonzo","timestamp":"1655912280.0","content":"As per https://cloud.google.com/transfer-appliance/docs/4.0/overview#transfer-speeds, assuming you have a dedicated interconnect with min bandwith available (10 GB), it will take less than 24 hours. It will take even less if you have a better bandwith in your dedeicated interconnect.\nSo, I'd say it is C","upvote_count":"1"}]}],"content":"Selected Answer: C\nI’ll go with C because the file size ie 75TB is smaller than the recommended size for Transfer Appliance (btw a few hundred TBs to 1 Petabyte.","timestamp":"1639369140.0","comment_id":"500370","poster":"gcp_learner"},{"comment_id":"467587","poster":"MaxNRG","upvote_count":"1","timestamp":"1635181140.0","content":"A – move data on Transfer appliance. Use Data Transfer rehydrator to decrypt data for Cloud Storage,\nGoogle recommends to use Transfer applicance, for moving 20+ TB of data or if upload takes more than a week.\nhttps://cloud.google.com/blog/products/storage-data-transfer/introducing-transfer-appliance-in-the-eu-for-cloud-data-migration"},{"content":"A is correct. Some suggesting C is not Ok as per google guidelines.","upvote_count":"1","timestamp":"1635049860.0","poster":"[Removed]","comment_id":"466816"},{"upvote_count":"1","comment_id":"464962","content":"Google recommends that enterprises use Transfer Appliance in cases where it would take them over a week to upload data to the cloud via the internet, or when an enterprise needs to migrate over 60 TB of data.","poster":"alexgrig","timestamp":"1634713140.0"},{"comment_id":"459652","content":"Transfer Appliance Rehydrator appears to be a deprecated (or at least obscured) piece of technology. If you look all through the docs there is no longer a reference to this. So I believe it is A, but I also bet the test does not include a reference to the Rehydrator. \n\nhttps://cloud.google.com/transfer-appliance/docs/4.0/installation-guide?hl=en#validating","upvote_count":"1","timestamp":"1633786860.0","poster":"[Removed]"},{"poster":"pr2web","comment_id":"441574","content":"Answer is A. \n\nTransfer appliance is a good use case for data size greater than or equal to 10TB\n\nhttps://cloud.google.com/transfer-appliance/docs/4.0/overview","timestamp":"1631121480.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"416430","content":"A is the correct answer.\nTo understand all types of the data transfer (gsutil, Transfer Service, Transfer Appliance) refer to https://www.youtube.com/watch?v=9H8abw_bOL8","poster":"hongha","timestamp":"1627500780.0"},{"content":"Ans should be A.\n\"Google recommends that enterprises use Transfer Appliance in cases where it would take them over a week to upload data to the cloud via the internet, or when an enterprise needs to migrate over 60 TB of data.\n\"https://searchcloudcomputing.techtarget.com/definition/Google-Transfer-Appliance","comment_id":"411328","timestamp":"1626925140.0","poster":"rm_2495","upvote_count":"2"},{"content":"Answer is A","upvote_count":"3","comment_id":"402057","poster":"MamthaSJ","timestamp":"1625762160.0"},{"comment_id":"393149","upvote_count":"1","content":"hey guys check Q3 for new Qs, 49 New Qs","comments":[{"timestamp":"1633157160.0","upvote_count":"1","comment_id":"455904","poster":"zira","content":"What does Q3 mean? and where I can find it?"}],"timestamp":"1624903020.0","poster":"kopper2019"},{"timestamp":"1624782360.0","poster":"aviratna","content":"A: Because data size is big and its one time.","comment_id":"391894","upvote_count":"1"},{"content":"A is ok","comment_id":"378165","upvote_count":"2","timestamp":"1623233400.0","poster":"areza"},{"timestamp":"1622374620.0","poster":"ArthurL20","upvote_count":"1","content":"Having 1Gbps connection doesn't ensure that you have that Upload capabilities, in many countries upload is X10 time slower than upload.\nSpecially when you transfer the data abroad, nether the less you should expect that the networking team will not allow you to allocate all the bandwidth of the organization.\nThis question is lacking basic information that a Architect would request:\n- Upload bandwidth speeds\n- Type of data\n- Files sizes.\nAnd much more.\nIn my company (Enterprise billion+) we would probably use the appliance.","comment_id":"370142"},{"upvote_count":"2","comment_id":"361624","poster":"Partha84","content":"Answer is definitely A. As per Google documentation, gsutil is good for less than 1 TB.","timestamp":"1621450560.0"},{"poster":"victory108","content":"A. Move your data onto a Transfer Appliance. Use a Transfer Appliance Rehydrator to decrypt the data into Cloud Storage.","timestamp":"1621407900.0","comment_id":"361120","upvote_count":"1"},{"upvote_count":"1","timestamp":"1620839460.0","poster":"un","content":"I will go with A","comment_id":"355671"},{"upvote_count":"1","comment_id":"353891","content":"It is C , because Transfer appliance is to be used for 100's of TBs to PBs, whereas the question is about 75TB , less than 100 TB","timestamp":"1620663000.0","poster":"ajaykg"},{"poster":"Trappatoni","timestamp":"1619184900.0","comments":[{"comment_id":"363758","timestamp":"1621696020.0","poster":"Trappatoni","upvote_count":"1","content":"https://cloud.google.com/transfer-appliance/docs/4.0#suitability"}],"upvote_count":"1","comment_id":"341620","content":"Answer is C:\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets\n\nYour private data center to Google Cloud Enough bandwidth to meet your project deadline\nfor less than 1 TB of data gsutil"},{"poster":"Ausias18","upvote_count":"1","content":"Answer is A","comment_id":"325505","timestamp":"1617252360.0"},{"content":"I think C is ok.\nLook at this: https://cloud.google.com/transfer-appliance/docs/2.2/overview#suitability\nIs Transfer Appliance suitable for me? Transfer Appliance is a good fit for your data transfer needs if:\n - You are an existing Google Cloud Platform (GCP) customer (fits)\n - Your data size is greater than or equal to 20 TB (fits, we have 75 TB)\n - Your data resides in locations that Transfer Appliance is available (we don't know)\n - IT WOULD TAKE MORE THAN ONE WEEK TO UPLOAD YOUR DATA (we don't know what is their bandwidth... Look at the calculator (https://cloud.google.com/solutions/migration-to-google-cloud-transferring-your-large-datasets#time) --> if 1Gbps, it'll take 7,5 days to move the data).","comments":[{"content":"Sorry, I've had \"A\" in my mind.","comment_id":"321203","poster":"lynx256","timestamp":"1616768700.0","upvote_count":"1"}],"comment_id":"321200","poster":"lynx256","timestamp":"1616768520.0","upvote_count":"1"},{"comment_id":"313982","upvote_count":"1","timestamp":"1616060940.0","content":"In the link shared, it's clearly stated:\nThe gsutil tool is the standard tool for small- to medium-sized transfers (less than 1 TB) over a typical enterprise-scale network, from a private data center to Google Cloud\n\nSo you need to use an appliance, since we have 75 TBs.","poster":"AndreUanKenobi"},{"upvote_count":"2","poster":"nitinz","comment_id":"302653","content":"I will go with A, min appliance is 10 TB. My answer will change if location of my source data was provided. If my 75TB data resides in Timbuktu then I will have to change my answer. But for sake of this discussion I will take for granted that data resides where Google can send it to me via FedEx.","timestamp":"1614779760.0"},{"poster":"rice","comment_id":"295794","content":"The provider of the transfer appliance for Google is Sureline Systems and they redeveloped the rehydrator and increased performance 6X. It is the quickest and most efficient way to transfer this much data with the choices provided - Answer \"A\"","timestamp":"1613914980.0","upvote_count":"1"},{"content":"This is really ambiguous, many other imp data points are missing.. though from data size of 75TB google recommends to use transfer appliance but transfer appliance (Option A) will have its own challenges. How you would connect transfer appliance to your on-prem datacenter ? you will need to pass through lot of physical & network security reviews to allow an external transfer appliance from google to be connected. Moreover transfer appliance is not available in many countries.\nOn the other hand in Option C, gsutil will do the job,, however will take more time than 1 week. But again no SLA is provided that data transfer needs to happen in x days. I would still bet on this choice","comment_id":"279202","timestamp":"1611911220.0","poster":"GS14","upvote_count":"1"},{"poster":"Rothmansua","timestamp":"1611863700.0","comment_id":"278789","comments":[{"comment_id":"278790","upvote_count":"1","content":"I have 1Gbps at home (for download, as from the point of view of GCS). Your company should have more.","poster":"Rothmansua","timestamp":"1611863880.0"}],"upvote_count":"1","content":"Even with only 3GBit/sec (Cloud VPN) 75TB transfer is going to take 3 hours (see calculator)\nhttps://cloud.google.com/solutions/migration-to-google-cloud-transferring-your-large-datasets\nWhich Transfer Appliance are you talking about?\nC is alright."},{"poster":"bnlcnd","content":"https://cloud.google.com/transfer-appliance/docs/2.2/overview\nON this page, there is a speed and time spreadsheet. 100M speed for 10T needs 12 days. and 1G speed is 30 hours. So, the question is not very clear.\nI will just select A.\nNot many small to medium companies have interconnect to GCP with GB speed.","timestamp":"1611716760.0","comment_id":"277492","upvote_count":"1"},{"comment_id":"269055","poster":"alii","content":"This question is missing key informations like:\n1. what's the network bandwidth they have\n2. What's the deadline to transfer this data\n\nBased on the documentation: https://cloud.google.com/solutions/migration-to-google-cloud-transferring-your-large-datasets#options_available_from_google\n\ngsutil doesn't fit for 75TB. Transfer service | on premises could have been the right choice if was given as an option.\n\nThis left us with option \"A\", Transfer appliance and rehydrated.","upvote_count":"1","timestamp":"1610824860.0"},{"timestamp":"1609972620.0","comment_id":"261340","content":"A is correct.","poster":"Arimaverick","upvote_count":"1"},{"timestamp":"1608491520.0","poster":"MaverickSri","upvote_count":"1","comment_id":"248887","content":"I believe it should be 'A', since no mention of network anywhere in the question."},{"upvote_count":"1","comment_id":"241831","timestamp":"1607795940.0","content":"A.\nUse transfer appliance for > 20TB. Find the link ref in other answers here.","poster":"Mndwsk"},{"content":"Normally transfer appliance used for data transfer from another cloud provider,","timestamp":"1607085780.0","poster":"BhupalS","comment_id":"234909","upvote_count":"1"},{"content":"It's C, Transfer Appliance not the same as the Storage Transfer Service.","timestamp":"1605697920.0","upvote_count":"3","poster":"xigzhou","comment_id":"221812"},{"comment_id":"209054","timestamp":"1604039520.0","content":"where do you read that the scenario is in a supported location?\nbut the other one only supports 5G so its A; Maybe","upvote_count":"1","poster":"francisco_guerra"},{"poster":"AdityaGupta","comment_id":"206419","content":"Both A and C are correction options, but referring to table at https://cloud.google.com/solutions/migration-to-google-cloud-transferring-your-large-datasets\n\nEnough bandwidth to meet your project deadline for less than a few TB of data -> User gsutil\n\nEnough bandwidth to meet your project deadline for more than a few TB of data -> user Storage Transfer Service for on-premises data\n\n\nNot enough bandwidth to meet your project deadline -> user Transfer Appliance\n\nSince network bandwidth and cost implications are not mentioned, I would go for option A.","timestamp":"1603731300.0","upvote_count":"2"},{"comments":[{"poster":"rungcpnow","upvote_count":"1","comment_id":"202926","content":"Can you please help with the recommended practices.. In the question no where they mentioned it will have time constraint. Hence we dont have to factor time constraint.. .. Also for using gsutil to transfer it depends on bandwidth the enterprise uses...so best is opt A..","timestamp":"1603162140.0"}],"comment_id":"200506","timestamp":"1602767640.0","upvote_count":"1","poster":"Mihai_","content":"C is correct.\nA is not: \"The expected turnaround time for a network appliance to be shipped, loaded with your data, shipped back, and rehydrated on Google Cloud is 50 days. If your online transfer timeframe is calculated to be substantially more than this timeframe, consider Transfer Appliance.\"\nhttps://cloud.google.com/solutions/migration-to-google-cloud-transferring-your-large-datasets"},{"upvote_count":"1","timestamp":"1602625020.0","poster":"[Removed]","content":"100TB in 3 hours, please see https://cloud.google.com/solutions/images/big-data-transfer-how-to-get-started-transfer-size-and-speed.png , answer it's C","comment_id":"199432"},{"content":"as per udemy practice exam the answer was C.","comment_id":"196531","upvote_count":"2","poster":"LoganIsh","timestamp":"1602223860.0"},{"content":"I believe the answer is A.\n\nHere it shows gsutil is good for private data center to Google Cloud where you are using \" less than a few TB of data\": https://cloud.google.com/solutions/migration-to-google-cloud-transferring-your-large-datasets\n\nThis show where data size is greater than or equal to 10TB which is from our question: https://cloud.google.com/transfer-appliance/docs/2.0/overview\n\nThis leaves A or B only. Data rehydration is used with transfer appliance: https://cloud.google.com/transfer-appliance/docs/2.0/data-rehydration\n\nGive these, I'll go with A also.","comment_id":"191507","timestamp":"1601630820.0","upvote_count":"2","poster":"f0x"},{"timestamp":"1601473680.0","comment_id":"190379","poster":"Firask","upvote_count":"1","content":"From https://cloud.google.com/solutions/migration-to-google-cloud-transferring-your-large-datasets \ngsutil for smaller transfers of on-premises data. The gsutil tool is the standard tool for small- to medium-sized transfers (less than a few TB) over a typical enterprise-scale network, from a private data center to Google Cloud.\n\nAns. A"},{"content":"Is it coming from one office? A. Coming from 8 offices? C. You shouldn't be making assumptions as an Architect. This is a broken question.","upvote_count":"1","timestamp":"1601335620.0","poster":"kimberjdaw","comment_id":"189317"},{"upvote_count":"1","poster":"kim0","timestamp":"1601248260.0","comment_id":"188687","content":"A is correct"},{"timestamp":"1601243940.0","poster":"subhala","comment_id":"188667","content":"https://cloud.google.com/solutions/migration-to-google-cloud-transferring-your-large-datasets - per this link, the process of using transfer appliance to load data takes about 50 days. hence if transfer using gsutil takes less than 50 days, then we should opt for gsutil. I would go with option c assuming we have 10 gbps link","upvote_count":"1"},{"timestamp":"1600191060.0","poster":"AshokC","comment_id":"179981","content":"A - \nTransfer Appliance\nTransfer Appliance is a good fit for your data transfer needs if:\n\nYou are an existing Google Cloud Platform (GCP) customer.\nYour data size is greater than or equal to 10TB.\nYour data resides in locations that Transfer Appliance is available.\nIt would take more than one week to upload your data.","upvote_count":"1"},{"poster":"mailraj","comment_id":"178172","upvote_count":"2","timestamp":"1599908580.0","content":"I say the question should have more requirements. Would go with C based on below\n\nThe expected turnaround time for a network appliance to be shipped, loaded with your data, shipped back, and rehydrated on Google Cloud is 50 days. If your online transfer timeframe is calculated to be substantially more than this timeframe, consider Transfer Appliance. The total cost for the 480 TB device process is less than $3,000"},{"comment_id":"173068","upvote_count":"1","content":"It's A based on Google cloud best practices","poster":"kumarp6","timestamp":"1599192600.0"},{"timestamp":"1597962900.0","comment_id":"162525","upvote_count":"1","content":"A seems to make more sense here.","poster":"wiqi"},{"comment_id":"159774","comments":[{"upvote_count":"1","content":"We should compare 50 days turnaround with Storage Transfer Applicance (answer A) vs 9 days online with gsutil (answer C) assuming 1 Gb/s (https://cloud.google.com/transfer-appliance/docs/2.2/overview). I vote for C.","poster":"gianberg","timestamp":"1607357640.0","comment_id":"237476"}],"timestamp":"1597653480.0","content":"It's A) even assuming a (generous) 1 Gb/s link.\nSee https://cloud.google.com/transfer-appliance/docs/2.0/overview\nkey point of the above article is ...\"(best practice using Transfer Appliance when) ...It would take more than one week to upload your data.\"\n70 TB, 1 Gb/s (no other use for the company)...9 days, so C) is excluded.","poster":"jespinosar","upvote_count":"1"},{"comment_id":"149044","timestamp":"1596363780.0","content":"As per https://cloud.google.com/solutions/migration-to-google-cloud-transferring-your-large-datasets\n\nThe expected turnaround time for a network appliance to be shipped, loaded with your data, shipped back, and rehydrated on Google Cloud is 50 days.\n\nAlso Although gsutil can support small transfer sizes (up to a few TB), Storage Transfer Service for on-premises data is designed for large-scale transfers (up to petabytes of data, billions of files).\nHence its clear Option C is out ,& given Option A is also not so Good as it will mean ~ 50 days ,We are not given the Bandwidth available and urgency hence we may pick A although its not a very elegant answer","poster":"kaush","upvote_count":"1"},{"comment_id":"142028","content":"I will go with Option - A\nThe question is ambiguous, there are many open points to pin down to the correct options between Storage Transfer Appliance Vs. gsutil. However, google recommended options are that Storage Transfer to be used under 3 scenarios - From other Cloud providers, more particularly AWS S3, over public Internet , within GCloud. \nIn this case, considering the data size (75TB), google recommended options will be to Storage Transfer Appliance. Again we could argue that Transfer Appliance is not available in all regions and this also takes a longer lead time (e2e)... but if we go with google recommendation it is option A","upvote_count":"1","poster":"Alasmindas","timestamp":"1595509680.0"},{"comment_id":"140023","timestamp":"1595304780.0","poster":"Aamir25","comments":[{"comment_id":"236917","upvote_count":"2","content":"If they said that it was 1 PB I would probably say for sure Storage Transfer Appliance. For 75TB most businesses could do an online transfer. I think it would be silly to use gsutil in this situation when you could easily use the Storage Transfer Server (not the Appliance), but GSutil could probably work. The difference is that using GSutil it costs $0 and takes you two weeks. Storage Transfer Appliance costs you $3,000 and takes 2 months. The choice is clear in my mind.","poster":"cate0012","timestamp":"1607304000.0"}],"upvote_count":"2","content":"As per google doc: The gsutil tool is the standard tool for small- to medium-sized transfers (less than a few TB) over a typical enterprise-scale network, from a private data center to Google Cloud... The question says company is moving 75 TB of data so in this case option A is the correct answer..."},{"upvote_count":"1","timestamp":"1594992960.0","content":"A is the correct answer it will take 12 days on 1 GPS link","comment_id":"137234","poster":"elnagmy"},{"timestamp":"1594812600.0","content":"Selected A I. My exam","comment_id":"135668","poster":"daurib","upvote_count":"2"},{"timestamp":"1594658460.0","content":"A is correct answer. Transfer appliance is advised for anything more than 10Tb \nalso please refer below link to read about rehydratons.\nhttps://cloud.google.com/transfer-appliance/docs/2.0/data-rehydration","comment_id":"134088","poster":"saurabh1805","upvote_count":"1"},{"content":"A is right answer","upvote_count":"1","poster":"Gobblegobble","timestamp":"1594032660.0","comment_id":"127698"},{"timestamp":"1592960940.0","poster":"kaush","comment_id":"117964","upvote_count":"1","content":"Its A ,Enough bandwidth to meet your project deadline\nfor less than a few TB of data gsutil is recommended"},{"upvote_count":"1","content":"It's clear that you need Transfer Appliance. A is the correct for sure","comment_id":"117314","poster":"mlantonis","timestamp":"1592907000.0"},{"comment_id":"110591","poster":"shashu07","timestamp":"1592202060.0","comments":[{"timestamp":"1607303700.0","comment_id":"236915","content":"I think you are confusing Storage Transfer Service with the Storage Transfer Appliance. The first one is online and the second is offline. This question is talking about using gsutil or the Storage Transfer appliance. The answer should be C unless the customer has very slow internet connection speeds, but that is not stated. A 1 Gb connection would transfer 75TB in less than 2 weeks. Most businesses that have 75TB of data would have at least a 1Gb pipe.\nThe turn time on Transfer Appliance is closer to two months. I'd go with an online option if possible. Storage Transfer Service if it was an option, but it's not, so I guess gsutil.\n\nHere's some info in the docs. https://cloud.google.com/solutions/migration-to-google-cloud-transferring-your-large-datasets#online_versus_offline_transfer","poster":"cate0012","upvote_count":"1"}],"upvote_count":"3","content":"Correct Answer: A\nMove your data onto a Transfer Appliance. Use a Transfer Appliance Rehydrator to decrypt the data into Cloud Storage.\nhttps://cloud.google.com/storage-transfer/docs/overview\nShould you use gsutil or Storage Transfer Service?\n\n\nThe gsutil command-line tool also enables you to transfer data between Cloud Storage and other locations. While you can use gsutil to work with Amazon S3 buckets and transfer data from Amazon S3 to Cloud Storage, Storage Transfer Service is recommended for this use case.\n\nFollow these rules of thumb when deciding whether to use gsutil or Storage Transfer Service:\nTransfer scenario Recommendation\nTransferring from another cloud storage provider Use Storage Transfer Service\nTransferring less than 1 TB from on-premises Use gsutil\nTransferring more than 1 TB from on-premises Use Transfer service for on-premises data"},{"content":"A is correct answer","poster":"Tushant","comment_id":"110270","upvote_count":"1","timestamp":"1592154720.0"},{"content":"A, for sure.","timestamp":"1591774020.0","upvote_count":"1","comment_id":"106567","poster":"gfhbox0083"},{"upvote_count":"1","content":"A is the right answer","comment_id":"102304","timestamp":"1591267800.0","poster":"Ziegler"},{"upvote_count":"1","content":"A is the correct answer","comment_id":"101059","poster":"Nirms","timestamp":"1591119960.0"},{"timestamp":"1590960600.0","comment_id":"99595","upvote_count":"1","poster":"coldpar","content":"A : https://cloud.google.com/transfer-appliance/docs/2.0/data-rehydration"},{"comment_id":"97724","poster":"amralieg","timestamp":"1590688320.0","upvote_count":"1","content":"A is the answer, Transfer appliance should be used where size > 20TB"},{"timestamp":"1590654300.0","comment_id":"97375","poster":"AD2AD4","upvote_count":"1","content":"Final Decision to go with Option A as i think this wanted to test knowledge of Rehydrate in Transfer Appliance."},{"upvote_count":"1","content":"A is the correct answer","comment_id":"94547","poster":"gcp_aws","timestamp":"1590265320.0"},{"comment_id":"91974","poster":"Kimi37LP","timestamp":"1589882400.0","comments":[{"comment_id":"236914","poster":"cate0012","timestamp":"1607303580.0","upvote_count":"1","content":"I think you are confusing Storage Transfer Service with the Storage Transfer Appliance. The first one is online and the second is offline. This question is talking about using gsutil or the Storage Transfer appliance. The answer should be C unless the customer has very slow internet connection speeds, but that is not stated. A 1 Gb connection would transfer 75TB in less than 2 weeks. Most businesses that have 75TB of data would have at least a 1Gb pipe. \n The turn time on Transfer Appliance is closer to two months. I'd go with an online option if possible. Storage Transfer Service if it was an option, but it's not, so I guess gsutil.\n\nHere's some info in the docs. https://cloud.google.com/solutions/migration-to-google-cloud-transferring-your-large-datasets#online_versus_offline_transfer"}],"content":"Follow these rules of thumb when deciding whether to use gsutil or Storage Transfer Service:\n\nTransfer scenario Recommendation\nTransferring from another cloud storage provider Use Storage Transfer Service\nTransferring less than 1 TB from on-premises Use gsutil\nTransferring more than 1 TB from on-premises Use Transfer service for on-premises data\nhttps://cloud.google.com/storage-transfer/docs/overview\nThat is A","upvote_count":"1"},{"upvote_count":"2","content":"Transfer appliance takes 25 days. Next better option is Storage transfer service. Since its not there i would choose GSutil","comment_id":"90276","poster":"laksg","timestamp":"1589681820.0"},{"poster":"Jack_in_Large","timestamp":"1589506860.0","upvote_count":"1","content":"A is the most efficient way.","comment_id":"89247"},{"upvote_count":"3","timestamp":"1589267160.0","poster":"Gini","comment_id":"87471","content":"This question is really ambiguous. More background information is needed to decide between Transfer Appliance and gsutil... I would choose A because 75TB is really a lot of data. (But if this 75TB of data is stored across multiple servers, gsutil is good enough)"},{"content":"Why is A? please read the question carefully which is telling \"You want to use Cloud Storage\". I will choose C at last.","comment_id":"67153","upvote_count":"1","timestamp":"1584930060.0","poster":"ff2107","comments":[{"upvote_count":"2","comment_id":"78682","content":"Provided your company has 1Gbps Internet connection, how long are you going to copy 75Tb of data? If it's okay to wait for a week just to copy data, then yep - choose C. :)","poster":"Ayzen","timestamp":"1587654600.0"}]},{"poster":"[Removed]","comment_id":"56364","content":"Selected A in the exam","timestamp":"1582872420.0","upvote_count":"3"},{"comment_id":"55456","content":"Link:https://cloud.google.com/transfer-appliance/docs/2.0/overview\nIs Transfer Appliance suitable for me?\nTransfer Appliance is a good fit for your data transfer needs if:\n\nYou are an existing Google Cloud Platform (GCP) customer.\nYour data size is greater than or equal to 10TB.\nYour data resides in locations that Transfer Appliance is available.\nIt would take more than one week to upload your data.\nwhy not gsutil[Link:https://cloud.google.com/storage-transfer/docs/overview]\n\nShould you use gsutil or Storage Transfer Service?\nThe gsutil command-line tool also enables you to transfer data between Cloud Storage and other locations. While you can use gsutil to work with Amazon S3 buckets and transfer data from Amazon S3 to Cloud Storage, Storage Transfer Service is recommended for this use case.\n\nFollow these rules of thumb when deciding whether to use gsutil or Storage Transfer Service:\n\nTransfer scenario Recommendation\nTransferring from another cloud storage provider Use Storage Transfer Service\nTransferring less than 1 TB from on-premises Use gsutil\n\nSo the Correct Answer is A","poster":"Dannyygcp","comments":[{"comment_id":"263054","content":"It would take 9 days to upload your data, according to the period calculator tool here: https://cloud.google.com/solutions/migration-to-google-cloud-transferring-your-large-datasets\n\nLike what you mentioned, if it takes more than a week to upload (https://cloud.google.com/transfer-appliance/docs/2.0/overview) , then we should choose the Transfer Appliance. \n\nTherefore the correct answer is A, since 9 days is more than 1 week (7 days).","poster":"joshuaquek","upvote_count":"1","timestamp":"1610175480.0"}],"timestamp":"1582718160.0","upvote_count":"8"},{"comment_id":"52305","content":"Skimpy question, but C is correc. Default for op-prem to cloud is gsutil. Transfer appliance would only make sense if there was a limitation of speed or cost on the network, gsutil is free.","timestamp":"1582071660.0","poster":"mawsman","upvote_count":"1"},{"timestamp":"1580393220.0","comment_id":"44789","upvote_count":"1","content":"Answer: A","poster":"2g"},{"timestamp":"1580031540.0","poster":"natpilot","upvote_count":"2","comment_id":"42834","content":"Correct answer is A."},{"poster":"AWS56","content":"Yes, It's A (https://cloud.google.com/transfer-appliance/docs/2.0/overview). Minimum transfer from a Transfer Appliance is 10 GB. b/w or speed is not part of the question. If you transfer more than 1~TB google recommends Transfer Appliance (https://cloud.google.com/storage-transfer/docs/overview).\n\nB is incorrect as Cloud Dataprep cannot be used to decrypt data. \nC,D are not recommended by Google.\n\nCorrect Answer: A","comment_id":"37382","timestamp":"1578648720.0","upvote_count":"7"},{"timestamp":"1578648660.0","comments":[{"content":"The link you posted (https://cloud.google.com/storage-transfer/docs/overview) discusses STS (Storage Transfer Service), which is not the same as the Transfer Appliance.\n\nAs per your second link (https://cloud.google.com/transfer-appliance/docs/2.2/overview), the Transfer Appliance is recommended when \"Your data size is greater than or equal to 20 TB\". \n\nI agree with A, though.","timestamp":"1613843460.0","upvote_count":"1","poster":"bogd","comment_id":"295264"}],"comment_id":"37381","upvote_count":"6","content":"Yes, It's A (https://cloud.google.com/transfer-appliance/docs/2.0/overview). Minimum transfer from a Transfer Appliance is 10 GB. b/w or speed is not part of the question. If you transfer more than 1~TB google recommends Transfer Appliance (https://cloud.google.com/storage-transfer/docs/overview).\n\nB is incorrect as Cloud Dataprep cannot be used to decrypt data. \nC,D are not recommended by Google.\n\nCorrect Answer: A","poster":"AWS56"},{"comment_id":"23853","content":"How can we propose a solution when we don't know the bandwidth available for the data transfer? In my opinion, the correct answer is A","timestamp":"1574516460.0","upvote_count":"12","poster":"TosO"},{"content":"It's C, minimum transfer appliance is 100 TB","timestamp":"1572195300.0","upvote_count":"3","comments":[{"upvote_count":"5","comments":[{"poster":"Hanmant","timestamp":"1607719800.0","content":"Your data size is greater than or equal to 20 TB.","upvote_count":"2","comment_id":"241209"}],"comment_id":"114516","poster":"Shome","content":"https://cloud.google.com/transfer-appliance/docs/2.0/overview\n\nNo its 10 TB","timestamp":"1592634960.0"}],"comment_id":"17799","poster":"jcmoranp"},{"comment_id":"17624","timestamp":"1572099240.0","poster":"Eroc","upvote_count":"2","content":"Correct answer is C"},{"comment_id":"17623","timestamp":"1572099180.0","upvote_count":"1","content":"It is cheaper. Google's recommended practices are here (https://cloud.google.com/solutions/transferring-big-data-sets-to-gcp)... so it doesn't tak months atleast 1 Gbps is recommended. The transfer appliance costs a lot compared to 1 Gbps or more... (https://cloud.google.com/transfer-appliance/docs/2.0/pricing) verse (https://www.tomsguide.com/us/gig-speed-internet,review-5134.html)","poster":"Eroc"}],"timestamp":"2019-10-23 08:07:00","answers_community":["A (86%)","14%"],"choices":{"A":"Move your data onto a Transfer Appliance. Use a Transfer Appliance Rehydrator to decrypt the data into Cloud Storage.","C":"Install gsutil on each server that contains data. Use resumable transfers to upload the data into Cloud Storage.","D":"Install gsutil on each server containing data. Use streaming transfers to upload the data into Cloud Storage.","B":"Move your data onto a Transfer Appliance. Use Cloud Dataprep to decrypt the data into Cloud Storage."},"isMC":true,"url":"https://www.examtopics.com/discussions/google/view/7043-exam-professional-cloud-architect-topic-1-question-55/","topic":"1","answer_images":[],"answer":"A","question_images":[],"question_text":"Your company is moving 75 TB of data into Google Cloud. You want to use Cloud Storage and follow Google-recommended practices. What should you do?","exam_id":4,"answer_ET":"A"},{"id":"MeEuNgiQqQHHlErwOmOZ","timestamp":"2019-10-25 06:28:00","answer_description":"","exam_id":4,"discussion":[{"poster":"ffk","comments":[{"poster":"AmitAr","upvote_count":"6","comments":[{"content":"Use the rolling update functionality of the >[Instance Group behind the Kubernetes cluster]<. It's not a rolling update of a Deployment. Read carefully.","upvote_count":"4","poster":"monopfm","comment_id":"1203925","timestamp":"1714381560.0"}],"comment_id":"602501","timestamp":"1652689800.0","content":"B looks most sensible (not funny). rolling update is a deployment strategy, which will deploy on pods 1 by 1,. i.e. by the time first pod is getting newer version of application, other pods are running with older version... In this way, there will be no downtime of application.. which is real ask from this question.\nI recommend B"},{"upvote_count":"14","timestamp":"1596703980.0","content":"A is ok","comment_id":"151822","poster":"tartar"},{"poster":"kumarp6","timestamp":"1604238840.0","upvote_count":"3","content":"Yes A is correct","comment_id":"210491"},{"upvote_count":"1","timestamp":"1614900120.0","poster":"nitinz","content":"Only logical answer is A.","comment_id":"303684"},{"poster":"DrLu","upvote_count":"3","timestamp":"1575659040.0","content":"This question asked \"You need to perform an update to the application with minimal downtime to the application.\"","comment_id":"27320"}],"content":"A is correct. \n\nB is funny","comment_id":"17313","timestamp":"1571977680.0","upvote_count":"47"},{"comment_id":"17493","poster":"jcmoranp","timestamp":"1572073860.0","upvote_count":"13","content":"Correct is A"},{"poster":"askith","content":"Selected Answer: A\nA is ok.","upvote_count":"1","comment_id":"1513814","timestamp":"1743828600.0"},{"timestamp":"1743605700.0","poster":"gaufchamp","upvote_count":"1","content":"The correct answer is:\n\nA. Use kubectl set image deployment/echo-deployment <new-image>\n\nExplanation:\nRolling Updates: Kubernetes deployments support rolling updates by default. This means that when you update the deployment, Kubernetes will gradually replace the old pods with new ones, ensuring minimal downtime.\n\nWhy A is Correct:\nThe command kubectl set image deployment/echo-deployment <new-image> updates the container image in the deployment. Kubernetes will handle the rolling update for you, ensuring that some pods are always running during the update process.","comment_id":"1423311"},{"poster":"JonathanSJ","content":"Selected Answer: A\nI will go for A.","timestamp":"1735517820.0","comment_id":"1333809","upvote_count":"1"},{"comment_id":"1309318","poster":"sim7243","timestamp":"1731220920.0","content":"Selected Answer: A\noption A","upvote_count":"1"},{"poster":"isa_pr","upvote_count":"4","content":"It's A. As per K8s documentation:\nTo update the image of the application to version 2, use the set image subcommand, followed by the deployment name and the new image version:\n\nkubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=docker.io/jocatalin/kubernetes-bootcamp:v2\n\nThe command notified the Deployment to use a different image for your app and initiated a rolling update.\"\n\"","timestamp":"1720447980.0","comment_id":"1244373"},{"upvote_count":"1","comment_id":"1122712","poster":"kip21","timestamp":"1705253220.0","content":"Option C - is the best option to perform an update to an application deployed on Google Kubernetes Engine with minimal downtime because it provides control over the update process, ensures high availability, and minimizes disruption. Rolling update functionality can also be used but requires more effort to implement. \nOption A and Option D may result in downtime if the new image is incompatible with the existing application."},{"timestamp":"1699706520.0","content":"A\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/updating-apps#updating_an_application","comment_id":"1067829","poster":"thewalker","upvote_count":"4"},{"timestamp":"1696518840.0","comments":[{"timestamp":"1703960400.0","comment_id":"1109891","poster":"cacharritos","upvote_count":"1","content":"delete.. minimal downtime.. A is correct ^_^U"}],"comment_id":"1025794","poster":"AdityaGupta","upvote_count":"1","content":"Selected Answer: C\nC. Update the deployment yaml file with the new container image. Use kubectl delete deployment/echo-deployment and kubectl create ג€\"f <yaml-file>\n\nI agreed with omermahgoub with his explanation."},{"poster":"vamgcp","upvote_count":"4","timestamp":"1675150920.0","content":"To perform an update to the application with minimal downtime on Google Kubernetes Engine (GKE), you can use a rolling update strategy, which involves updating the application incrementally, one pod at a time, while ensuring that the updated pods are functioning properly before updating the next set. Here's the general process:\nkubectl set image deployment/echo-deployment echo=<new_image_tag>","comment_id":"793779"},{"content":"Selected Answer: A\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/updating-apps \nsays rolling updates and mentions same command . \nSo 100 % A","timestamp":"1672773540.0","comment_id":"764958","upvote_count":"9","poster":"roaming_panda"},{"comments":[{"comment_id":"752217","upvote_count":"1","poster":"omermahgoub","content":"using kubectl set image deployment/deployment <new-image> will not allow you to perform an update to the application with minimal downtime, even if the deployment is exposed using a Service.\n\nThis command will update the image of the containers in the deployment, but it will not perform a rolling update. A rolling update allows you to update your application with minimal downtime by replacing the old version of the application with the new version one pod at a time, while ensuring that there is always at least one pod available to serve traffic. Without a rolling update, all of the pods in the deployment will be replaced at the same time, which may result in downtime for the application.","timestamp":"1671621720.0"},{"content":"Option A: Using kubectl set image deployment/echo-deployment <new-image> will update the image of the containers in the deployment, but it will not perform a rolling update and may result in downtime for the application.\n\nOption B: Using the rolling update functionality of the Instance Group behind the Kubernetes cluster is not a valid option, as the rolling update functionality is used to update the instances in the instance group, not the containers in a deployment.\n\nOption D: Updating the service yaml file with the new container image and using kubectl delete service/echo-service and kubectl create –f <yaml-file> is not a valid option, as the service is not responsible for running the application containers and updating the service will not update the application.","comments":[{"poster":"CkWongCk","content":"A is correct, update template spec image in deployment yml will trigger rollout deploy","timestamp":"1674653460.0","comment_id":"787677","upvote_count":"1"},{"timestamp":"1697465040.0","poster":"_kartik_raj","content":"Answer is A, you are wrong as hell, if you delete deployment its obvious app will face downtime","upvote_count":"3","comment_id":"1045041"}],"timestamp":"1671621360.0","upvote_count":"3","poster":"omermahgoub","comment_id":"752207"}],"upvote_count":"3","poster":"omermahgoub","content":"The correct answer is C: Update the deployment yaml file with the new container image. Use kubectl delete deployment/echo-deployment and kubectl create –f <yaml-file>.\n\nTo perform an update to an application deployed on Google Kubernetes Engine with minimal downtime, you can follow these steps:\n\nUpdate the deployment yaml file with the new container image.\nUse the kubectl delete deployment/echo-deployment command to delete the existing deployment.\nUse the kubectl create –f <yaml-file> command to create a new deployment using the updated yaml file.\nThis process, known as a rolling update, allows you to update your application with minimal downtime by replacing the old version of the application with the new version one pod at a time, while ensuring that there is always at least one pod available to serve traffic.","timestamp":"1671621360.0","comment_id":"752206"},{"timestamp":"1670013780.0","poster":"jasenmornin","upvote_count":"3","comment_id":"734055","content":"Selected Answer: A\nI think A is correct:\n\nB. I don't understand the objective of this option.\nC and D. These are eliminated because they involve suffering a downtime when the resources are eliminated, so they are not fulfilling one of the requirements."},{"upvote_count":"5","poster":"markus_de","comment_id":"731338","timestamp":"1669803300.0","content":"Selected Answer: A\nExample from official Kubernetes docu (for NGINX):\nkubectl set image deployment/nginx-deployment nginx=nginx:1.16.1\n\nhttps://kubernetes.io/docs/concepts/workloads/controllers/deployment/"},{"poster":"megumin","upvote_count":"2","comment_id":"712473","timestamp":"1667752380.0","content":"Selected Answer: A\nok for A"},{"timestamp":"1665931560.0","poster":"AzureDP900","comment_id":"696301","content":"A is right -- kubectl set image deployment/echo-deployment","upvote_count":"1"},{"upvote_count":"8","poster":"RitwickKumar","comment_id":"648503","content":"Selected Answer: A\nSource: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#updating-a-deployment\n\nDeployment ensures that only a certain number of Pods are down while they are being updated. By default, it ensures that at least 75% of the desired number of Pods are up (25% max unavailable).\n\nDeployment also ensures that only a certain number of Pods are created above the desired number of Pods. By default, it ensures that at most 125% of the desired number of Pods are up (25% max surge).","timestamp":"1660836420.0"},{"poster":"Mikado211","upvote_count":"2","comment_id":"642880","content":"Selected Answer: A\nAnswer is A\n\nIt can't be C, if you delete and recreate the deployment you will have a downtime between the deletion and the recreation.","timestamp":"1659691980.0"},{"poster":"Ric350","content":"It's definitely A. See here under updating a deployment on the right hand side. \nhttps://kubernetes.io/docs/concepts/workloads/controllers/deployment/#updating-a-deployment","comment_id":"633628","upvote_count":"2","timestamp":"1658241600.0"},{"content":"Selected Answer: A\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/updating-apps#updating_an_application","upvote_count":"3","comment_id":"630986","timestamp":"1657731600.0","poster":"JohnPi"},{"poster":"amxexam","content":"Selected Answer: C\nD - we can all eliminate as option is wrong as image is set in deployment ym I and not service yml.\nA - I will eliminate. as just updating wont put new image This will happen if only the continen gets redeployed due to load or restart.\nB- is also a wrong statement for k 8s.\nHence C . It could be the grableled text is new yaml.","comment_id":"600209","upvote_count":"2","timestamp":"1652285880.0"},{"poster":"Nirca","content":"Selected Answer: A\nYes A is correct","upvote_count":"2","comment_id":"597388","timestamp":"1651768020.0"},{"poster":"OrangeTiger","content":"I think B.","upvote_count":"2","comments":[{"timestamp":"1641278460.0","content":"I really confused by a sentence 'Instance Group behind the Kubernetes cluster' .\nIt's correct to use the rolling update feature,but in gke, do it with a pod and replica.\nA is how to work rolling updatee in GKE.So A is correct.","upvote_count":"2","poster":"OrangeTiger","comment_id":"516272"}],"comment_id":"516265","timestamp":"1641277560.0"},{"comment_id":"509116","upvote_count":"2","timestamp":"1640437200.0","content":"Selected Answer: A\nA is the correct answer","poster":"vincy2202"},{"comments":[{"content":"B is not okay. You aren't supposed to touch the instance groups manually when using GKE. Let GKE manage them.","poster":"AWSPro24","timestamp":"1636654740.0","comment_id":"476405","upvote_count":"3"}],"timestamp":"1635229860.0","content":"A – kubectl set image deployment/my-deployment mycontainer=myimage\nB is also possible, it is basically same as A but described verbally. A is just more specific.\n\nSo, pay attention to such Q, maybe there could be rough mistake in one of them, so you would choose another. But, the rule of thumb – more specific is right.\nAlso, managed Instance group is a “bad smell” in B, normally it is not mentioned in GKE design (though GKE underneath is based on GCE and instance group).","upvote_count":"3","poster":"MaxNRG","comment_id":"467843"},{"poster":"MamthaSJ","content":"Answer is A","timestamp":"1625762340.0","comment_id":"402058","upvote_count":"3"},{"comments":[{"content":"hey kopper2019 , are any of old questions appearing on new exam or only new Qs ?","poster":"ale183","timestamp":"1632153540.0","comment_id":"448330","upvote_count":"2"}],"poster":"kopper2019","timestamp":"1624903020.0","content":"hey guys check Q3 for new Qs, 49 New Qs","comment_id":"393148","upvote_count":"2"},{"upvote_count":"3","poster":"aviratna","content":"A is correct. No disruption, it will rollout in stages for each POD running behind service. Other option there is a disruption.","comment_id":"391896","timestamp":"1624782480.0"},{"upvote_count":"2","content":"A. Use kubectl set image deployment/echo-deployment <new-image>","comment_id":"361119","poster":"victory108","timestamp":"1621407840.0"},{"content":"A is correct","poster":"un","comment_id":"355656","timestamp":"1620838320.0","upvote_count":"1"},{"timestamp":"1617252480.0","upvote_count":"1","content":"Answer is A","poster":"Ausias18","comment_id":"325506"},{"poster":"nitinz","comment_id":"302657","content":"Correct Answer is A, reason check the k8s cheat-sheet: -\nkubectl set image deployment/frontend www=image:v2 # Rolling update \"www\" containers of \"frontend\" deployment, updating the image","timestamp":"1614780060.0","upvote_count":"3"},{"poster":"GS14","comment_id":"279204","timestamp":"1611911640.0","upvote_count":"1","content":"I would go with \"A\", command mentioned in \"A\" does the rolling update, B is just confusing as there is no instance group for K8S"},{"poster":"bnlcnd","timestamp":"1611717480.0","comment_id":"277500","upvote_count":"2","content":"kubectl set image deployment nginx nginx=nginx:1.9.1\nor: change the deployment setting in yaml and then\nkubectl apply -f abc.yaml\n\nA is the answer"},{"poster":"Prakzz","comment_id":"252118","timestamp":"1608906000.0","content":"A is correct","upvote_count":"1"},{"poster":"sdsdfasdf4","comment_id":"249574","upvote_count":"2","content":"A is correct, B is incorrect because you're rolling the pods not the nodes of the kubernetes.","timestamp":"1608575820.0"},{"upvote_count":"1","content":"A.\n\"set image ...\" initiates a k8s rolling update of the deployment. Search for rolling update deployment strategy. It minimizes downtime during deployment.","poster":"Mndwsk","timestamp":"1607796420.0","comment_id":"241849"},{"content":"A is correct","upvote_count":"1","poster":"Chulbul_Pandey","timestamp":"1606898160.0","comment_id":"232755"},{"comment_id":"215608","content":"A is correct.\n\nFor example, to update a Deployment from nginx version 1.7.9 to 1.9.1, run the following command:\n\nkubectl set image deployment nginx nginx=nginx:1.9.1\n\nThe kubectl set image command updates the nginx image of the Deployment's Pods one at a time.","timestamp":"1604883540.0","upvote_count":"1","poster":"pepYash"},{"poster":"occupatissimo","timestamp":"1604320260.0","comment_id":"211209","content":"Be careful to the B answer that says \"of the Instance Group behind the Kubernetes cluster\"!!! Its target is the instance group BEHIND the GKE !!! Rolling update is applied to GKE and not the VM.\nSo it should be A.","upvote_count":"2"},{"comment_id":"190383","content":"Ans. A\nFrom Kubernetes website:\nkubectl set image deployment/frontend www=image:v2 # Rolling update \"www\" containers of \"frontend\" deployment, updating the image","poster":"Firask","timestamp":"1601474340.0","upvote_count":"1"},{"comment_id":"188688","timestamp":"1601248260.0","upvote_count":"1","content":"A is correct","poster":"kim0"},{"upvote_count":"1","comment_id":"186530","poster":"noussy","timestamp":"1600981920.0","content":"B :o is that a joke ? depuis quand il y a des instance group dans gke ?"},{"upvote_count":"1","content":"Answer A","poster":"AshokC","comment_id":"180012","timestamp":"1600198200.0"},{"comment_id":"178900","upvote_count":"1","poster":"olg","timestamp":"1600024380.0","content":"Minimal downtime to the application provided by RollingUpdate strategy, which is not default: RollingUpdate implements automated, rolling updates for the Pods in the StatefulSet. It is needed, and if applied already, need then just to set an image. So there actually two steps (patching deployment with RollingUpdate and set image) if no conditions here. Tricky question. If B, you don't apply an image, if A, it has downtime. It should be B then A together."},{"timestamp":"1597963020.0","poster":"wiqi","content":"A is correct.","upvote_count":"1","comment_id":"162526"},{"timestamp":"1597210920.0","poster":"BigB","content":"A is right answer : Look at step 7 of this UseCase \n\n: https://cloud.google.com/kubernetes-engine/docs/tutorials/hello-app","upvote_count":"1","comment_id":"156176"},{"timestamp":"1596364260.0","comment_id":"149048","upvote_count":"1","poster":"kaush","content":"Answer is A as per \nkubectl set image deployment nginx nginx=nginx:1.9.1"},{"poster":"NeilNalto","content":"Even the documentation in the given explanation answer says 'A'!! and NOT B...!","comment_id":"141943","timestamp":"1595501340.0","upvote_count":"1"},{"poster":"daurib","content":"Selected A in the exam","comment_id":"135671","timestamp":"1594812780.0","upvote_count":"1"},{"timestamp":"1594032720.0","comment_id":"127700","upvote_count":"1","content":"A is right answer","poster":"Gobblegobble"},{"content":"Come on there is no Instance Group behind the Kubernetes cluster. It is definitely A.","poster":"mlantonis","upvote_count":"1","comment_id":"117303","timestamp":"1592906400.0"},{"timestamp":"1592644980.0","content":"A is the correct answer","poster":"Tushant","upvote_count":"2","comment_id":"114637"},{"content":"Correct Ans A : Use Kubectlset image deployment echo-deployment <new-image>\nExplanation\nYou can use kubectl to update application image\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/updating-apps#updating_an_application\n\nB. Not required. – We need to update to the application\nYou can perform a rolling update to update the images, configuration, labels, annotations, and resource limits/requests of the workloads in your clusters. Rolling updates incrementally replace your resource's Pods with new ones, which are then scheduled on nodes with available resources. Rolling updates are designed to update your workloads without downtime\nA, D. If you delete service or deployment - Application is down.","timestamp":"1592202480.0","poster":"shashu07","comment_id":"110594","comments":[{"upvote_count":"1","timestamp":"1602219600.0","content":"Am applicant can be a image.","poster":"vibhavchavan","comment_id":"196484"}],"upvote_count":"2"},{"poster":"syu31svc","comment_id":"109271","upvote_count":"1","timestamp":"1592034360.0","content":"Answer is A; the reference ink provided is correct and you can see the syntax at the start of the section Updating an application for kubectl set"},{"timestamp":"1591938480.0","content":"i am going to B too.\n\"A\" just one of the step to create a image. For me, instance group = node at the kubernete cluster.\n\nThe question is making us confuse..............","poster":"mamh","upvote_count":"1","comment_id":"108373"},{"comment_id":"105366","upvote_count":"1","content":"Answer A IS the Rolling Update functionality... Answer A is the syntax used...","poster":"desertlotus1211","timestamp":"1591632480.0"},{"upvote_count":"1","content":"B is the correct answer based on this\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/updating-apps#updating_an_application","poster":"Ziegler","comment_id":"102309","timestamp":"1591268580.0"},{"timestamp":"1590656460.0","content":"Final Decision to go with Option A","upvote_count":"2","poster":"AD2AD4","comment_id":"97389"},{"timestamp":"1590267420.0","upvote_count":"2","content":"The answer is A.. Thanks AWS56 for the link","poster":"gcp_aws","comment_id":"94558"},{"content":"You need to perform an update to the application with minimal downtime to the application. \nIt should be B","upvote_count":"1","poster":"Applehph","comment_id":"85056","comments":[{"timestamp":"1607305140.0","comment_id":"236929","content":"B is talking about going into GCE and doing something to the Instance Group. You are trying to update an application. It's A.\n\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/updating-apps#updating_an_application","upvote_count":"1","poster":"cate0012"}],"timestamp":"1588846560.0"},{"content":"Answer: A","poster":"Zarmi","upvote_count":"1","comment_id":"83390","timestamp":"1588564380.0"},{"comment_id":"67156","timestamp":"1584930360.0","content":"Why B is funny if don't see this practice before? \nhttps://chriskyfung.github.io/blog/qwiklabs/Scale-Out-and-Update-a-Containerized-Application-on-a-Kubernetes-Cluster","upvote_count":"2","poster":"ff2107"},{"timestamp":"1582872480.0","poster":"[Removed]","upvote_count":"2","comment_id":"56365","content":"A is correct. Selected A in exam","comments":[{"poster":"ESP_SAP","comments":[{"poster":"zversh","comment_id":"188859","timestamp":"1601276580.0","content":"You'll get no useful information in any case, if he pass that exam or not))","upvote_count":"2"}],"timestamp":"1599697920.0","upvote_count":"1","comment_id":"176797","content":"A lot of comments like this, not helpful at all. but did you pass it?"}]},{"poster":"Smart","comment_id":"53093","timestamp":"1582220220.0","content":"B (Incorrect): Going outside Kubernetes Platform to update Deployment is illogical. Also, there is no instance group behind these deployments.\nC (Incorrect): Possible but will have downtime. \nD (Incorrect): Service yaml won't have option to specify container image. \nA (Correct): A is the best option here; although, it is partly incorrect. As pointed out by Shariq, not mentioning a version/tag will lead to Image Pull loop.","upvote_count":"1"},{"poster":"mawsman","content":"A is correct as per https://kubernetes.io/docs/concepts/workloads/controllers/deployment/ \nTo update your Deployment ... simply use the following command:\nkubectl set image...\nB would update the kubernetes host instances not the application on kubernetes.","timestamp":"1582071840.0","comment_id":"52310","upvote_count":"4"},{"timestamp":"1581616860.0","poster":"ADVIT","content":"It's A. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#updating-a-deployment","upvote_count":"3","comment_id":"50136"},{"timestamp":"1580393280.0","poster":"2g","content":"Answer: A","upvote_count":"1","comment_id":"44791"},{"content":"A is correct","timestamp":"1580031660.0","comment_id":"42835","poster":"natpilot","upvote_count":"1"},{"comment_id":"37385","poster":"AWS56","content":"You will have to use rolling update functionality (https://cloud.google.com/kubernetes-engine/docs/how-to/updating-apps) which is \"kubectl set image deployment/echo-deployment <new-image>\"\n\nB,C,D are useless and makes no sense","upvote_count":"5","timestamp":"1578649140.0"},{"comment_id":"32182","upvote_count":"3","timestamp":"1577192280.0","content":"A ， what is “the Instance Group behind the Kubernetes cluster” ？","poster":"MrBog1"},{"upvote_count":"2","timestamp":"1574992800.0","comment_id":"25096","poster":"Shariq","content":"B is correct. option A is invalid does not have tag version"},{"content":"A is right. https://cloud.google.com/kubernetes-engine/docs/how-to/updating-apps","upvote_count":"3","timestamp":"1573744800.0","comment_id":"21561","poster":"JoeShmoe"},{"poster":"jcmoranp","content":"There is no \"instance group behing the kubernetes engine\", it's A.","comment_id":"17801","timestamp":"1572195420.0","upvote_count":"3"},{"content":"B uses A and avoids downtown... Correct answer is B, https://cloud.google.com/kubernetes-engine/docs/how-to/updating-apps","comment_id":"17626","timestamp":"1572099420.0","comments":[{"poster":"bogd","timestamp":"1613843640.0","comment_id":"295267","upvote_count":"4","content":"The link you posted clearly states this:\n\"For example, to update a Deployment from nginx version 1.7.9 to 1.9.1, run the following command: kubectl set image deployment nginx nginx=nginx:1.9.1\"\n\nSo the answer is clearly A. \n\nDo not confuse a rolling update of a K8s application (A) with a rolling update of the K8s nodes (B)."}],"poster":"Eroc","upvote_count":"3"}],"answer":"A","url":"https://www.examtopics.com/discussions/google/view/7184-exam-professional-cloud-architect-topic-1-question-56/","question_text":"You have an application deployed on Google Kubernetes Engine using a Deployment named echo-deployment. The deployment is exposed using a Service called echo-service. You need to perform an update to the application with minimal downtime to the application. What should you do?","question_images":[],"answer_ET":"A","choices":{"D":"Update the service yaml file which the new container image. Use kubectl delete service/echo-service and kubectl create ג€\"f <yaml-file>","C":"Update the deployment yaml file with the new container image. Use kubectl delete deployment/echo-deployment and kubectl create ג€\"f <yaml-file>","A":"Use kubectl set image deployment/echo-deployment <new-image>","B":"Use the rolling update functionality of the Instance Group behind the Kubernetes cluster"},"question_id":153,"unix_timestamp":1571977680,"isMC":true,"answer_images":[],"topic":"1","answers_community":["A (93%)","7%"]},{"id":"yH3C0bEmS4XtTmwnDZxz","topic":"1","question_id":154,"answer_images":[],"isMC":true,"discussion":[{"content":"Selected Answer: C\nBoth A & C are correct but using the principle of least privileges C is the most appropriate.\n\nBigQuery User: (roles/bigquery.user)\nWhen applied to a dataset, this role provides the ability to read the dataset's metadata and list tables in the dataset.\nWhen applied to a project, this role also provides the ability to run jobs, including queries, within the project. A principal with this role can enumerate their own jobs, cancel their own jobs, and enumerate datasets within a project. <b>Additionally, allows the creation of new datasets within the project; the creator is granted the BigQuery Data Owner role(roles/bigquery.dataOwner) on these new datasets.</b> \nLowest-level resources where you can grant this role: Dataset\n\nBigQuery Job User: (roles/bigquery.jobUser)\nProvides permissions to run jobs, including queries, within the project.\nLowest-level resources where you can grant this role: Project\n\nSource: https://cloud.google.com/bigquery/docs/access-control","comment_id":"648511","upvote_count":"26","poster":"RitwickKumar","timestamp":"1676742000.0"},{"content":"Selected Answer: C\nC is the correct Answer ,\nA is wrong because bq User Permission will allow you to edit the dataset, which is something that we don't want in this scenario.\nB and D is wrong because \"You want to make sure that no query costs are incurred on the projects that contain the data\" so you don't want users to fire quires on the Project that contains the dataset , hence the \"dataViewer\" permission\n\nhttps://cloud.google.com/bigquery/docs/access-control","upvote_count":"21","timestamp":"1659701460.0","poster":"kimharsh","comment_id":"541034","comments":[{"timestamp":"1695280200.0","poster":"kratosmat","comment_id":"845695","upvote_count":"1","content":"It seems that User Permission doesn't allow to edit data, isn't it?"}]},{"poster":"plumbig11","timestamp":"1735761660.0","content":"Selected Answer: C\nBigQuery jobUser on the billing project and BigQuery dataViewer on the projects that contain the data.","upvote_count":"1","comment_id":"1335280"},{"timestamp":"1735518060.0","poster":"JonathanSJ","comment_id":"1333810","upvote_count":"1","content":"Selected Answer: C\nI will go for C."},{"upvote_count":"1","poster":"Edgo97","comment_id":"1153779","timestamp":"1724048700.0","content":"The link to refer here: https://cloud.google.com/bigquery/docs/access-control"},{"comment_id":"853875","upvote_count":"3","timestamp":"1695949260.0","content":"Selected Answer: C\nThe \"roles/bigquery.jobUser\" role provides the permission to run jobs, including querying, exporting and copying data, and creating views and materialized views. This role does not provide permissions to create, update, or delete BigQuery resources, such as datasets, tables, and models. Users with this role can only interact with BigQuery through jobs.\n\nThe \"roles/bigquery.User\" role, on the other hand, provides the permission to create, update, and delete BigQuery resources, as well as run jobs. This role includes all the permissions of the \"roles/bigquery.jobUser\" role, and in addition allows users to manage BigQuery resources, such as creating datasets, tables, and models, and modifying their schema and access controls.","poster":"SidsA"},{"comment_id":"845219","poster":"jlambdan","timestamp":"1695234840.0","upvote_count":"1","content":"Selected Answer: C\nA is wrong because https://cloud.google.com/bigquery/docs/access-control#bigquery.user\nC is correct because https://cloud.google.com/bigquery/docs/access-control#bigquery.jobUser"},{"upvote_count":"7","content":"Selected Answer: C\nImportant statements from the prompt\n1. All queries need to be billed to a single project - one project that queries data stored on other projects. Let's call this our billing project. \n a. jobUser is the best role to satisfy this need, because it provides permission to run jobs \n and queries within a project.\n\n2. Other projects is where the data resides. These projects don't need much access besides the ability to be viewed (not edited).\n a. The dataViewer role provide permission to read all datasets in the project.","comment_id":"755089","timestamp":"1687625400.0","poster":"jay9114"},{"comment_id":"752220","comments":[{"comments":[{"comment_id":"752228","poster":"omermahgoub","content":"Here is a summary of the differences between the BigQuery Job User role and the BigQuery User role:\n\nBigQuery Job User role (roles/bigquery.jobUser):\n\nCan create and modify query jobs\nCannot run queries or incur costs on the project\nBigQuery User role (roles/bigquery.user):\n\nCan create and modify query jobs\nCan run queries and incur costs on the project\nIf you want to grant users the ability to create and modify query jobs, but not run queries or incur costs on the project, you should use the BigQuery Job User role. If you want to grant users the ability to run queries and incur costs on the project, in addition to the ability to create and modify query jobs, you should use the BigQuery User role.","timestamp":"1687339860.0","upvote_count":"2"}],"timestamp":"1687339860.0","upvote_count":"3","comment_id":"752227","poster":"omermahgoub","content":"The BigQuery Job User role (roles/bigquery.jobUser) and the BigQuery User role (roles/bigquery.user) have similar permissions, but they differ in the scope of their permissions.\n\nThe BigQuery Job User role grants users the ability to create and modify query jobs, but it does not grant them the ability to run queries or incur costs on the project. This role is intended for users who need to create and manage query jobs, but who should not be able to run queries or incur costs.\n\nThe BigQuery User role grants users the ability to run queries and incur costs on the project, in addition to the ability to create and modify query jobs. This role is intended for users who need to run queries and incur costs on the project, as well as create and manage query jobs."},{"timestamp":"1687339620.0","content":"Option B: Granting the group the roles of BigQuery dataViewer on the billing project and BigQuery user on the projects that contain the data will not allow the group to incur costs on the billing project and will not meet the requirements of the scenario.\n\nOption C: Granting the group the roles of BigQuery jobUser on the billing project and BigQuery dataViewer on the projects that contain the data will not allow the group to incur costs on the billing project and will not meet the requirements of the scenario.\n\nOption D: Granting the group the roles of BigQuery dataViewer on the billing project and BigQuery jobUser on the projects that contain the data will not allow the group to incur costs on the billing project and will not meet the requirements of the scenario.","poster":"omermahgoub","comment_id":"752221","upvote_count":"1"},{"content":"BigQuery User \n(roles/bigquery.user)\nWhen applied to a dataset, this role provides the ability to read the dataset's metadata and list tables in the dataset.\n\nWhen applied to a project, this role also provides the ability to run jobs, including queries, within the project. A principal with this role can enumerate their own jobs, cancel their own jobs, and enumerate datasets within a project. Additionally, allows the creation of new datasets within the project; the creator is granted the BigQuery Data Owner role (roles/bigquery.dataOwner) on these new datasets.\n\nBigquery.user has potential to create a dataset inside the project and creates becomes owner of the dataset. This is not the requirement stated in the question scenario.\nAnswer is C","upvote_count":"1","poster":"Diwz","timestamp":"1728552120.0","comment_id":"1192854"}],"poster":"omermahgoub","timestamp":"1687339620.0","upvote_count":"1","content":"The correct answer is A: Add all users to a group. Grant the group the role of BigQuery user on the billing project and BigQuery dataViewer on the projects that contain the data.\n\nTo make sure that no query costs are incurred on the projects that contain the data and allow users to query the datasets but not edit them, you should follow these steps:\n\nAdd all users to a group.\nGrant the group the role of BigQuery user on the billing project. This will allow the group to run queries on BigQuery and incur costs on the billing project.\nGrant the group the role of BigQuery dataViewer on the projects that contain the data. This will allow the group to view the datasets and run queries on them, but not edit them."},{"content":"C is right \nAdd all users to a group. Grant the group the roles of BigQuery jobUser on the billing project and BigQuery dataViewer on the projects that contain the data.","comment_id":"696303","poster":"AzureDP900","timestamp":"1681656480.0","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: C\nC. Add all users to a group. Grant the group the roles of BigQuery jobUser on the billing project and BigQuery dataViewer on the projects that contain the data.","comment_id":"694597","poster":"minmin2020","timestamp":"1681460100.0"},{"timestamp":"1680898080.0","poster":"Vedjha","comment_id":"688883","upvote_count":"1","content":"D is the answer:\nCloud BigQuery Roles\nCloud BigQuery IAM Roles\nBigQuery Admin - bigquery.*\nBigQuery Data Owner - bigquery.datasets.*, bigquery.models.*, bigquery.routines.*,\nbigquery.tables.* (Does NOT have access to Jobs!)\nBigQuery Data Editor - bigquery.tables.(create/delete/export/get/getData/getIamPolicy/\nlist/update/updateData/updateTag), bigquery.models.*, bigquery.routines.*,\nbigquery.datasets.(create/get/getIamPolicy/updateTag)\nBigQuery Data Viewer - get/list bigquery.(datasets/models/routines/tables)\nBigQuery Job User - bigquery.jobs.create\nBigQuery User - BigQuery Data Viewer + get/list (jobs, capacityCommitments, reservations\netc)\nTo see data, you need either BigQuery User or BigQuery Data Viewer roles\nYou CANNOT see data with BigQuery Job User roles\nBigQuery Data Owner or Data Viewer roles do NOT have access to jobs!"},{"content":"C is the correct Answer , \nA is wrong because bq User Permission will allow you to edit the dataset, which is something that we don't want in this scenario.\nB and D is wrong because \"You want to make sure that no query costs are incurred on the projects that contain the data\" so you don't want users to fire quires on the Project that contains the dataset , hence the \"dataViewer\" permission\n\nhttps://cloud.google.com/bigquery/docs/access-control","upvote_count":"1","poster":"kimharsh","timestamp":"1659701400.0","comment_id":"541032"},{"timestamp":"1656707340.0","content":"C. Add all users to a group. Grant the group the roles of BigQuery jobUser on the billing project and BigQuery dataViewer on the projects that contain the data.","comment_id":"514726","poster":"victory108","upvote_count":"4"},{"content":"C looks to be the correct answer","upvote_count":"2","comment_id":"510932","poster":"LoveT","timestamp":"1656400440.0"},{"timestamp":"1656392940.0","content":"Selected Answer: C\nJobUser is the correct terminology for bq. Only read access to data sources is required.","comment_id":"510868","poster":"HenkH","upvote_count":"1"},{"content":"bq is using jobs - so \"user\" isn't specific enough, jobuser is.","timestamp":"1656392880.0","poster":"HenkH","upvote_count":"2","comments":[{"timestamp":"1656849180.0","comment_id":"515806","upvote_count":"1","content":"Hence C","poster":"elenamatay"}],"comment_id":"510867"}],"unix_timestamp":1640675280,"answers_community":["C (100%)"],"timestamp":"2021-12-28 08:08:00","question_text":"Your company is using BigQuery as its enterprise data warehouse. Data is distributed over several Google Cloud projects. All queries on BigQuery need to be billed on a single project. You want to make sure that no query costs are incurred on the projects that contain the data. Users should be able to query the datasets, but not edit them.\nHow should you configure users' access roles?","choices":{"C":"Add all users to a group. Grant the group the roles of BigQuery jobUser on the billing project and BigQuery dataViewer on the projects that contain the data.","B":"Add all users to a group. Grant the group the roles of BigQuery dataViewer on the billing project and BigQuery user on the projects that contain the data.","A":"Add all users to a group. Grant the group the role of BigQuery user on the billing project and BigQuery dataViewer on the projects that contain the data.","D":"Add all users to a group. Grant the group the roles of BigQuery dataViewer on the billing project and BigQuery jobUser on the projects that contain the data."},"url":"https://www.examtopics.com/discussions/google/view/68708-exam-professional-cloud-architect-topic-1-question-57/","exam_id":4,"answer":"C","question_images":[],"answer_description":"","answer_ET":"C"},{"id":"CmVY0TWyoFWBDsZTRlj3","question_id":155,"question_images":[],"topic":"1","timestamp":"2019-10-21 15:21:00","answers_community":["B (100%)"],"exam_id":4,"url":"https://www.examtopics.com/discussions/google/view/6889-exam-professional-cloud-architect-topic-1-question-58/","unix_timestamp":1571664060,"choices":{"D":"Create an App Engine web application where users can upload images for the next 24 hours. Authenticate users via Cloud Identity.","B":"Have users upload the images to Cloud Storage using a signed URL that expires after 24 hours.","C":"Create an App Engine web application where users can upload images. Configure App Engine to disable the application after 24 hours. Authenticate users via Cloud Identity.","A":"Have users upload the images to Cloud Storage. Protect the bucket with a password that expires after 24 hours."},"answer_description":"","answer":"B","discussion":[{"comment_id":"16428","content":"Correct answer is B","upvote_count":"44","poster":"jcmoranp","comments":[{"upvote_count":"8","timestamp":"1596704520.0","comment_id":"151826","poster":"tartar","content":"B is ok"},{"timestamp":"1604239080.0","poster":"kumarp6","upvote_count":"3","comment_id":"210495","content":"Signed URL ... B is correct"},{"upvote_count":"3","comment_id":"303691","content":"B signed URL","poster":"nitinz","timestamp":"1614900600.0"}],"timestamp":"1571664060.0"},{"poster":"MyPractice","timestamp":"1577785980.0","comment_id":"33951","upvote_count":"29","content":"Ans B\n\"When should you use a signed URL? In some scenarios, you might not want to require your users to have a Google account in order to access Cloud Storage\" \"Signed URLs contain authentication information in their query string, allowing users without credentials to perform specific actions on a resource\"\nhttps://cloud.google.com/storage/docs/access-control/signed-urls"},{"upvote_count":"1","poster":"JonathanSJ","comment_id":"1333811","content":"Selected Answer: B\nI will go for B.","timestamp":"1735518240.0"},{"timestamp":"1731220920.0","comment_id":"1309319","upvote_count":"1","content":"Selected Answer: B\nCorrect answer is B","poster":"sim7243"},{"timestamp":"1704595260.0","poster":"gun123","content":"Selected Answer: B\nCorrect answer is B","comment_id":"1115564","upvote_count":"1"},{"comment_id":"958590","poster":"red_panda","upvote_count":"1","timestamp":"1689948420.0","content":"Selected Answer: B\nB is the answer"},{"comment_id":"850559","upvote_count":"5","timestamp":"1679788080.0","poster":"fussili","content":"The correct answer is B.\n\nA is not a good choice because it is not possible to set an expiration time for a password protected Cloud Storage bucket. This means that if a user had the password, they would be able to upload images to the bucket even after the 24 hour period has expired.\n\nB is the correct answer because a signed URL can be generated to allow specific users to upload images to Cloud Storage without requiring them to have a Google Account. The URL can be set to expire after 24 hours, which ensures that users can only upload images during the allowed time period.\n\nC is not the best choice because it involves creating an App Engine web application, which is more complex than using Cloud Storage with a signed URL. Additionally, App Engine instances cannot be turned off programmatically, so it would not be possible to disable the application after 24 hours.\n\nD option is similar to option C, but it involves creating an App Engine web application. This would add unnecessary complexity to the solution, and it would not provide any additional benefits compared to using Cloud Storage with a signed URL."},{"poster":"omermahgoub","upvote_count":"3","comments":[{"content":"Option A: Protecting the bucket with a password that expires after 24 hours would not be a secure or scalable solution, as it would require you to distribute the password to all users and to update the password every 24 hours.\n\nOption C: Creating an App Engine web application where users can upload images, and configuring App Engine to disable the application after 24 hours, would not allow users to upload images after the application is disabled.\n\nOption D: Creating an App Engine web application where users can upload images for the next 24 hours and authenticating users via Cloud Identity would not allow users to upload images if they do not have a Google Account.","timestamp":"1671622740.0","comment_id":"752234","poster":"omermahgoub","upvote_count":"1"}],"timestamp":"1671622740.0","content":"The correct answer is B: Have users upload the images to Cloud Storage using a signed URL that expires after 24 hours.\n\nTo allow specific users to upload images to Cloud Storage for testing your Cloud ML Engine application, and to not require all users to have a Google Account, you should use signed URLs. A signed URL is a URL that allows access to a specific resource in Cloud Storage, and that is only valid for a specified period of time.\n\nTo create a signed URL that expires after 24 hours, you can use the gsutil signurl command. For example:\n\nCopy code\ngsutil signurl -d 24h service-account.json gs://bucket-name/object-name\nThis will generate a signed URL that allows users to upload an object to the specified bucket with the specified name, and that will only be valid for 24 hours.","comment_id":"752233"},{"upvote_count":"1","comment_id":"712874","timestamp":"1667805540.0","poster":"megumin","content":"Selected Answer: B\nB is ok"},{"poster":"AzureDP900","comment_id":"696304","timestamp":"1665931800.0","content":"B is right, Signed URL's will help in this scnerio.","upvote_count":"1"},{"timestamp":"1665735720.0","upvote_count":"1","comment_id":"694600","poster":"minmin2020","content":"Selected Answer: B\nB. Have users upload the images to Cloud Storage using a signed URL that expires after 24 hours."},{"comment_id":"625824","poster":"mv2000","content":"On 06/30/2022 Exam.","timestamp":"1656691800.0","upvote_count":"2"},{"poster":"mygcpjourney2712","comment_id":"577739","content":"Selected Answer: B\nsigned url","upvote_count":"1","timestamp":"1648572900.0"},{"timestamp":"1640438100.0","upvote_count":"2","content":"B is the correct answer","poster":"vincy2202","comment_id":"509124"},{"upvote_count":"1","timestamp":"1638830340.0","content":"Go for B.","comment_id":"495449","poster":"haroldbenites"},{"content":"B – Have users upload the images to Cloud Storage via signed URL which expires after 24 hours.\nSigned URL is a preferable way to allow something with limited timeframe, doesn't require the account","upvote_count":"1","timestamp":"1635230460.0","poster":"MaxNRG","comment_id":"467855"},{"poster":"[Removed]","timestamp":"1635050160.0","content":"B is right. Signed URL are best for users for short term access.","upvote_count":"1","comment_id":"466819"},{"content":"Answer is B","timestamp":"1625763060.0","comment_id":"402067","upvote_count":"3","poster":"MamthaSJ"},{"timestamp":"1623054840.0","upvote_count":"4","poster":"rishab86","content":"Answer B\nA signed URL is a URL that provides limited permission and time to make a request. Signed URLs contain authentication information in their query string, allowing users without credentials to perform specific actions on a resource. When you generate a signed URL, you specify a user or service account which must have sufficient permission to make the request that the signed URL will make. After you generate a signed URL, anyone who possesses it can use the signed URL to perform specified actions, such as reading an object, within a specified period of time.\n\nWhen should you use a signed URL?\nIn some scenarios, you might not want to require your users to have a Google account in order to access Cloud Storage, but you still want to control access using your application-specific logic.","comment_id":"376631"},{"comment_id":"361128","timestamp":"1621408380.0","poster":"victory108","upvote_count":"2","content":"B. Have users upload the images to Cloud Storage using a signed URL that expires after 24 hours."},{"poster":"un","upvote_count":"1","content":"B is correct","comment_id":"356046","timestamp":"1620885720.0"},{"poster":"Ausias18","timestamp":"1617254760.0","content":"Answer is B","comment_id":"325526","upvote_count":"1"},{"content":"B is the simplest.... But isn't it too simple ?\nI assume we will create a Cloud Finction to get the uploaded images from the bucket where users will upload images and this Cloud Function will be triggered and will move the objects (images) to final destination, where users haven't access. That's ok.\nSo. Do we want users to read (get) or delete the images they uploaded for a few minutes after uploading (before Cloud Function moves them)? If NOT - it would've be too strange and restrictive. In the other hand, if we created Signed URL also with read/delete permission, all users can read (get) and/or delete images uploaded from others (imagine a malicious app).\nMaybe we should consider C or D ? What do you think ?","comments":[{"poster":"lynx256","timestamp":"1616773440.0","content":"Sorry, I went too far...\nThis is only testing app and users will upload well-known paintings (not pictures of people) - so in such scenario no one would create malicious app I wrote above.","comment_id":"321257","upvote_count":"1"}],"timestamp":"1616773020.0","upvote_count":"1","comment_id":"321249","poster":"lynx256"},{"poster":"AD3","comment_id":"318711","content":"'B', is correct, search on google for \"google signed url\" the page was updated on Jan 2021, it says in the beginning \"This page provides a overview of signed URLs, which you use to give time-limited resource access to anyone in possession of the URL, regardless of whether they have a Google account....\"","upvote_count":"1","timestamp":"1616554800.0"},{"poster":"Kiroo","comment_id":"318525","timestamp":"1616536560.0","upvote_count":"1","content":"I thought about B and D because allow only a specific set of users , but since app engine standard and would be hard to create a user and password to everyone I think that would be easier to use a signed url. A is just wrong"},{"timestamp":"1616505240.0","upvote_count":"1","poster":"padamdha","comment_id":"318105","content":"B is correct."},{"upvote_count":"1","timestamp":"1610176800.0","content":"It should be B. There is no such \"bucket password\" feature available for Google Cloud Storage.","poster":"joshuaquek","comment_id":"263064"},{"poster":"bolu","upvote_count":"1","content":"if you practice all 230 questions and come back to this answers list, you will choose B by default since A (what chosen as answer in question is illogical). B is appropriate.","timestamp":"1609713300.0","comment_id":"258923"},{"upvote_count":"1","timestamp":"1608906240.0","content":"B is correct","poster":"Prakzz","comment_id":"252121"},{"content":"B.\n\"\nUse signed URLs to give time-limited read or write access to an object through a URL you generate. Anyone with whom you share the URL can access the object for the duration of time you specify, regardless of whether or not they have a Google account.\n\"","poster":"Mndwsk","upvote_count":"1","comment_id":"242057","timestamp":"1607811000.0"},{"comment_id":"232763","upvote_count":"2","timestamp":"1606898700.0","poster":"Chulbul_Pandey","content":"B is correct"},{"upvote_count":"1","timestamp":"1603732260.0","content":"I will go with Option B, as the Signed-URL can be distributed among specific set of users to allow limited period access.","poster":"AdityaGupta","comment_id":"206433"},{"upvote_count":"1","timestamp":"1602908580.0","poster":"LoganIsh","content":"B is the answer for it similarly AWS has the feature as well.","comment_id":"201331"},{"content":"B for sure","timestamp":"1601248320.0","poster":"kim0","upvote_count":"1","comment_id":"188689"},{"timestamp":"1601116740.0","comment_id":"187606","upvote_count":"1","content":"Answer A is funny. The question already stated not all have account in google. Even you have it password protected, without an account, how are you going to \"use\" the password.\n\nFacepalm.","poster":"VedaSW"},{"upvote_count":"2","comment_id":"180018","timestamp":"1600199940.0","content":"B - Signed URL (Cloud storage) that expires after 24 hours\nhttps://cloud.google.com/storage/docs/access-control/signed-urls","poster":"AshokC"},{"timestamp":"1599384060.0","content":"Correct Ans : B\nhttps://cloud.google.com/storage/docs/access-control/signed-urls","poster":"WannaBeCloudArch","upvote_count":"1","comment_id":"174438"},{"timestamp":"1598582880.0","upvote_count":"1","poster":"vmrandy","comment_id":"168033","content":"Got to be B"},{"timestamp":"1597966200.0","poster":"wiqi","upvote_count":"1","comment_id":"162545","content":"B is correct"},{"poster":"Alasmindas","upvote_count":"1","timestamp":"1595517720.0","content":"Correct answer is B.Signed URL would be the most preferred options considering Google recommendations,","comment_id":"142122"},{"poster":"elnagmy","timestamp":"1594993980.0","comment_id":"137249","upvote_count":"1","content":"Correct answer is B"},{"poster":"PrithviPatil","comment_id":"135406","content":"It has to be B","timestamp":"1594784100.0","upvote_count":"1"},{"timestamp":"1594634820.0","upvote_count":"1","poster":"Skt10","comment_id":"133825","content":"Option B is correct"},{"comment_id":"127721","content":"B is right answer","upvote_count":"2","timestamp":"1594033860.0","poster":"Gobblegobble"},{"content":"What about \"D\"? You want to allow specific people. D is much more complex but allows restricting users thorugh Cloud Identity","poster":"Musk","timestamp":"1593326520.0","upvote_count":"1","comment_id":"121674"},{"comment_id":"117369","timestamp":"1592910420.0","content":"We definitely need signed URL.\nCorrect answer is B","upvote_count":"2","poster":"mlantonis"},{"content":"B is correct","poster":"Tushant","timestamp":"1592645340.0","comment_id":"114642","upvote_count":"2"},{"comment_id":"110604","timestamp":"1592203260.0","content":"Correct Answer: B \nHave users upload the images to Cloud Storage using a signed URL that expires after 24 hours\nExplanation\nCloud Storage Signed URL can be used to provide timed access to cloud storage resources.\nhttps://cloud.google.com/blog/products/storage-data-transfer/uploading-images-directly-to-cloud-storage-by-using-signed-url\nSigned URL use to give time-limited resource access to anyone in possession of the URL, regardless of whether they have a Google account","poster":"shashu07","upvote_count":"2"},{"comment_id":"106573","upvote_count":"2","poster":"gfhbox0083","content":"B, for sure.\nSigned URL","timestamp":"1591774980.0"},{"timestamp":"1591269120.0","poster":"Ziegler","content":"B is the correct answer. I have implemented this.","upvote_count":"1","comment_id":"102315"},{"comment_id":"101066","poster":"Nirms","timestamp":"1591120500.0","content":"B is the correct answer","upvote_count":"1"},{"poster":"AD2AD4","content":"Final Decision to go with Option B","timestamp":"1590656760.0","comment_id":"97394","upvote_count":"1"},{"timestamp":"1590268680.0","poster":"gcp_aws","upvote_count":"1","content":"B is the answer","comment_id":"94571"},{"poster":"Jack_in_Large","content":"Absolutely B!","timestamp":"1589455620.0","comment_id":"88878","upvote_count":"1"},{"comment_id":"83392","content":"Answer: B","upvote_count":"1","poster":"Zarmi","timestamp":"1588565280.0"},{"upvote_count":"1","poster":"anton_royce","content":"B is correct Answer","timestamp":"1585903200.0","comment_id":"70675"},{"upvote_count":"2","comment_id":"56411","timestamp":"1582878600.0","content":"Correct: B. Selected B in the exam","poster":"[Removed]"},{"poster":"KNG","content":"Should be B\nhttps://cloud.google.com/blog/products/storage-data-transfer/uploading-images-directly-to-cloud-storage-by-using-signed-url","timestamp":"1581605220.0","comment_id":"50027","upvote_count":"1"},{"upvote_count":"2","comment_id":"44794","timestamp":"1580393400.0","content":"Answer: B","poster":"2g"},{"timestamp":"1580031840.0","content":"B is correct","upvote_count":"2","poster":"natpilot","comment_id":"42837"},{"comment_id":"39849","upvote_count":"3","poster":"sri007","content":"B is the ans","timestamp":"1579203720.0"},{"upvote_count":"2","content":"I chose be as well","poster":"passnow","timestamp":"1576664700.0","comment_id":"30578"},{"comment_id":"30218","timestamp":"1576545000.0","poster":"yglee8048","content":"B is correct.","upvote_count":"4"},{"comment_id":"21563","poster":"JoeShmoe","timestamp":"1573744980.0","content":"B is correct - no such thing as a bucket password","upvote_count":"7"},{"comment_id":"17653","upvote_count":"2","poster":"Eroc","content":"https://cloud.google.com/storage/docs/access-control/signed-urls ... I agree B works, see \"Using signed URLs with resumable uploads\" in the link provided.","timestamp":"1572104580.0"},{"content":"Agree.","upvote_count":"5","comment_id":"16934","poster":"KouShikyou","timestamp":"1571829900.0"}],"isMC":true,"answer_images":[],"question_text":"You have developed an application using Cloud ML Engine that recognizes famous paintings from uploaded images. You want to test the application and allow specific people to upload images for the next 24 hours. Not all users have a Google Account. How should you have users upload images?","answer_ET":"B"}],"exam":{"isBeta":false,"name":"Professional Cloud Architect","provider":"Google","isImplemented":true,"lastUpdated":"11 Apr 2025","isMCOnly":false,"id":4,"numberOfQuestions":279},"currentPage":31},"__N_SSP":true}