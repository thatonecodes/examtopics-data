{"pageProps":{"questions":[{"id":"XqXfiWiPWCgM8RZeGw8w","url":"https://www.examtopics.com/discussions/google/view/16659-exam-professional-data-engineer-topic-1-question-41/","answer_description":"","discussion":[{"upvote_count":"95","comments":[{"comment_id":"497959","content":"A - \"Date#Device_Id\" is not the same that \"Timestamp#Device_Id\". If you want to query historical data, rowkey as \"2021-12-09#12345device\" is optimal design. Nevertheless, \"2021-12-09:09:10:47:2000#12345device\" isn't it. Each record has a date (2021-12-09) and unique devide id (12345, 12346, 12347...).","upvote_count":"23","poster":"Jlozano","timestamp":"1639074240.0"},{"upvote_count":"6","timestamp":"1594702920.0","content":"A is a better option then other ..though not perfect as you mentioned.","poster":"Rajuuu","comment_id":"134576"},{"upvote_count":"1","content":"Totally agree if we have to avoid hotspotting! , but, incase we need to choose one of the options below, would you be going for A?","timestamp":"1642122660.0","poster":"sraakesh95","comment_id":"523237"},{"timestamp":"1625752320.0","poster":"sumanshu","upvote_count":"4","comment_id":"401970","content":"For READ operation it's is correct. i.e. Date#Device (so that data read from single node) -\nFor write operation it should be DeviceID#Date (so that data write via multiple nodes)"},{"poster":"Ankit267","upvote_count":"17","comment_id":"142235","content":"A - Key should be less granular item first to more granular item, there are more devices than date key (every 15 min)","timestamp":"1595529240.0"},{"content":"Actually it depends. You will only have 365 x 2 dates unique dates at a given tame, since is a 2 year history, while most likely have more devices than that. So it will make more sence to start with the date first instead of the the device","comment_id":"689998","upvote_count":"15","timestamp":"1665304860.0","poster":"maxdataengineer"}],"timestamp":"1587225540.0","comment_id":"76117","poster":"itche_scratche","content":"None, rowkey should be Device_Id+Date(reverse)"},{"comments":[{"upvote_count":"13","comment_id":"441952","comments":[{"poster":"Whoswho","upvote_count":"2","content":"I remember seeing it as well. the answer should be A. (reversed)","comment_id":"753567","timestamp":"1671733440.0"},{"timestamp":"1668949620.0","comments":[{"content":"The date is even worse than timestamp for the problem of hot-spotting","timestamp":"1693038840.0","poster":"FP77","comment_id":"990598","upvote_count":"1"}],"poster":"wan2three","upvote_count":"5","content":"but it didnt say cant use date, date and timestamp are different","comment_id":"722620"}],"timestamp":"1631188560.0","content":"Google specifically mentions that it's a bad idea to use a timestamp at the start of a rowkey\nhttps://cloud.google.com/bigtable/docs/schema-design#row-keys-avoid\nThe answer really should be Device_id#Timestamp but with the answers we were given you would be better off leaving the timestamp out all together","poster":"michaelkhan3"}],"poster":"jvg637","upvote_count":"19","timestamp":"1584276120.0","comment_id":"64259","content":"think is A, since “The most common query is for all the data for a given device for a given day”, rowkey should have info for both devcie and date."},{"comment_id":"1335053","upvote_count":"1","timestamp":"1735683180.0","poster":"Ronn27","content":"Selected Answer: A\nIts very confusing but what I found is timebucket concept and day can be used instead of timestamp. \n\nhttps://cloud.google.com/bigtable/docs/schema-design-time-series#time-buckets"},{"comment_id":"1318857","upvote_count":"3","timestamp":"1732735380.0","content":"Selected Answer: C\nThe correct option should be device_id#Date as it will distribute the load while writing and also be performant while reading. C is the second best option in my understanding as device Id will ensure that data sent by all the devices on a day is distributed between nodes and will not create hotspot.","poster":"cloud_rider"},{"content":"Selected Answer: A\nI would go to date#device_id. However, i don't find this combination. A should be then chosen.","comment_id":"1301393","timestamp":"1729570800.0","poster":"SamuelTsch","upvote_count":"2"},{"comment_id":"1264653","upvote_count":"1","timestamp":"1723467000.0","poster":"cmira123","content":"A-https://cloud.google.com/bigtable/docs/schema-design-time-series?hl=es-419#use_tall_and_narrow_tables"},{"timestamp":"1720271940.0","comment_id":"1243377","upvote_count":"1","poster":"Lenifia","content":"Selected Answer: A\nshowed up in my exam. picked A. passed the exam. still not sure it's correct though"},{"timestamp":"1716135060.0","content":"A. Rowkey: date#device_id Column data: data_point\n\nExplanation:\n\nOptimized for Most Common Query: The most common query is for all data for a given device on a given day. This schema directly matches the query pattern by including both date and device_id in the row key. This enables efficient retrieval of the required data using a single row key prefix scan.\nScalability: As the number of devices and data points increases, this schema distributes the data evenly across nodes in the Bigtable cluster, avoiding hotspots and ensuring scalability.\nData Organization: By storing data points as column values within each row, you can easily add new data points or timestamps without modifying the table structure.","comment_id":"1213871","upvote_count":"1","poster":"39405bb"},{"timestamp":"1715935800.0","content":"Answer C:\nhttps://cloud.google.com/bigtable/docs/schema-design#time-based:~:text=Don%27t%20use%20a%20timestamp%20by%20itself%20or%20at%20the%20beginning%20of%20a%20row%20key%2C","poster":"mark1223jkh","upvote_count":"1","comment_id":"1212797"},{"content":"Selected Answer: C\nc without any doubt","upvote_count":"2","poster":"0725f1f","comment_id":"1166641","timestamp":"1709659200.0"},{"upvote_count":"1","content":"The right answer should be Reverse A, but since we don't have that, the best answer is C.","comment_id":"1140358","poster":"philli1011","timestamp":"1707067440.0"},{"upvote_count":"3","content":"Selected Answer: C\nC. This schema is best suited for historical analysis of device data over time when the most common query is to retrieve all data for a **specific device** on a **given day**.\n\n* **Row Key as `device_id`:** This allows for efficient retrieval of all data points related to a particular device in a single operation. Bigtable sorts data lexicographically by row key, so all data for a single device will be stored together.\n\n* **Column with `date` and `data_point`:** \n - Using `date` as a column name or part of the column qualifier allows you to quickly filter and retrieve data for specific date ranges. \n - Storing `data_point` as the column value provides the actual data associated with each timestamp.\n\n**Example:**\n\nWith this schema, a query to get all data for `device_12345` on `2023-12-20` would efficiently target the specific row key `device_12345` and fetch the relevant columns (with dates around `2023-12-20`).","poster":"gise","comment_id":"1126660","timestamp":"1705667520.0"},{"poster":"JonFrow","comment_id":"1096145","timestamp":"1702539420.0","content":"C - the answer should the right answer.\nKey is \"all the data for a given device for a given day\"\nas in, Device first, and all the data + data points after. \nThis has nothing to do with Date-based search.","upvote_count":"1"},{"content":"Selected Answer: A\nA - Key should be less granular item first to more granular item, there are more devices than date key (every 15 min)","poster":"rocky48","upvote_count":"1","comment_id":"1065335","timestamp":"1699421880.0"},{"timestamp":"1696732020.0","poster":"imran79","content":"the closest match to this in the provided options is:\n\nC. Rowkey: device_id Column data: date, data_point\n\nThus, option C would be the best choice from the given option","upvote_count":"1","comment_id":"1027681"},{"poster":"kenwilliams","content":"Selected Answer: A\nIt all comes down to the most common query","upvote_count":"3","comment_id":"906801","timestamp":"1685034300.0","comments":[{"upvote_count":"1","poster":"FP77","comment_id":"990599","content":"Exactly\n\"all the data for a given device for a given day\" \nThat's why the answer is C. You start by selecting the device and then the date. This solution is not prone to hot-spotting, yours is.","timestamp":"1693039020.0"}]},{"timestamp":"1674733860.0","comment_id":"788670","content":"Selected Answer: A\nA. Rowkey: date#device_id Column data: data_point This schema would allow querying all data for a given device for a given day by looking up the row key, which would be the date followed by the device_id. This would be the most efficient way to access the data as it would be stored in sorted order by date and device_id.","poster":"PolyMoe","upvote_count":"2"},{"timestamp":"1674021240.0","comment_id":"779647","content":"A is the answer.","poster":"GCPpro","upvote_count":"2"},{"poster":"Jackalski","upvote_count":"2","content":"Selected Answer: A\nI vote on A, none is the ideal answer as often here \nneeds to distribute data within cluster by date and device. \nthis key will answer most used query - so OK for me","timestamp":"1672323300.0","comment_id":"761103"},{"comment_id":"746678","timestamp":"1671155100.0","content":"Answer : A","poster":"slade_wilson","upvote_count":"2"},{"upvote_count":"1","comment_id":"744484","timestamp":"1670968680.0","content":"Selected Answer: D\nI would with option D because it clearly mention the access pattern - all the data for a given device for a given day.","poster":"DGames"},{"upvote_count":"3","comment_id":"731223","timestamp":"1669796040.0","content":"Obv you can't have date as the key, that would mean getting yourself a nice hotspot. The answer is C.","poster":"Gudwin"},{"poster":"ejlp","content":"Selected Answer: A\nfocus on \"for a given device for a given day\"","timestamp":"1669174440.0","comment_id":"724862","upvote_count":"1"},{"timestamp":"1657600560.0","poster":"NM1212","comment_id":"630286","content":"Out of the given options, only option A gives you the read use case of retrieving data by device by date. All other options will require a full scan of the table which is not ideal.\n\nEven though that option is not write optimized as it will cause hot-spotting, it is still the only possible option for the given use case.","upvote_count":"5"},{"timestamp":"1656908640.0","poster":"gcpdata","comment_id":"626839","upvote_count":"3","content":"Selected Answer: C\nneed to make a key of device id and date, date is a secondary key."},{"upvote_count":"2","timestamp":"1655129880.0","poster":"rr4444","comment_id":"615822","content":"Yup this Q&A is a mess"},{"upvote_count":"2","content":"Selected Answer: A\nClassic answer will be Device_Id#Timestamp, but it's said query by date.\nDate as less values than a timestamp, so Date#Device_Id is a good key.\nMoreover, all other answer can't work","timestamp":"1649853540.0","comment_id":"585195","poster":"CedricLP"},{"timestamp":"1649116920.0","upvote_count":"2","poster":"devric","content":"Selected Answer: C\nOption C. The most common query retrieve data from a given device in a given day. There's no sense in not using device_id as a beginning of the rowkey.","comment_id":"580967"},{"poster":"Arkon88","content":"Selected Answer: C\nC. Rowkey: device_id Column data: date, data_point\n\n1) The most common query is for all the data for a given device for a given day so\ndevice id + date will be required\n2) to avoid hot spotting device id need to be first\nhttps://cloud.google.com/bigtable/docs/schema-design#row-keys-avoid","upvote_count":"4","timestamp":"1646381820.0","comment_id":"560608"},{"comment_id":"530205","timestamp":"1642903560.0","content":"Selected Answer: A\nclearly A","poster":"exnaniantwort","upvote_count":"1","comments":[{"upvote_count":"1","comment_id":"560611","comments":[{"poster":"jin0","upvote_count":"1","timestamp":"1676994420.0","content":"I agree yours. cause of hot spotting A should not be a answer.","comment_id":"816767"}],"content":"2) to avoid hot spotting device id need to be first\nhttps://cloud.google.com/bigtable/docs/schema-design#row-keys-avoid","timestamp":"1646381880.0","poster":"Arkon88"}]},{"upvote_count":"1","comment_id":"523238","poster":"sraakesh95","timestamp":"1642122720.0","content":"Selected Answer: A\navoids hotspotting to some extent to the combination of the device ID and the timestamp"},{"timestamp":"1641299100.0","content":"Selected Answer: D\n\"The most common query is for all the data for a given device for a given day\"\nrow: data_point column: device_id, date","poster":"medeis_jar","upvote_count":"1","comment_id":"516572"},{"poster":"sr5977","content":"Since row_key should be unique and all the options have date (not timestamp), so any combination of device_id and date will cause duplicate row_key as data is being ingested at every 15 minutes. So even though it is not efficient and should not be done, i think D is the best answer here - hoping data will be unique each time.","comment_id":"492456","timestamp":"1638447900.0","upvote_count":"2"},{"upvote_count":"4","timestamp":"1636998600.0","comment_id":"478886","comments":[{"comment_id":"535803","upvote_count":"1","timestamp":"1643499120.0","content":"And if the same or a similar question is asked outside of a case study?","poster":"Capitano"}],"poster":"MaxNRG","content":"Data Engineer exam doesn't contains case studies, so you can skip these questions :)"},{"timestamp":"1635900180.0","upvote_count":"1","comment_id":"471872","content":"C. \nABE are not correct, as using dates as prefix will likely cause hotspotting. \nD is not the best option. Putting data point before device ID might not cause hot spotting like ABE, but it would make it difficult for analysis in the future.","poster":"JayZeeLee"},{"comment_id":"466047","upvote_count":"1","timestamp":"1634890320.0","content":"Rowkey is based on your query. See below excerpt from Bigtable Schema design...\n\"For example, if your application tracks mobile device data, you can have a row key that consists of device type, device ID, and the day the data is recorded. Row keys for this data might look like this:\n\n\n phone#4c410523#20200501\n phone#4c410523#20200502\n tablet#a0b81f74#20200501\n tablet#a0b81f74#20200502\nThis row key design lets you retrieve data with a single request for:\n\nA device type\nA combination of device type and device ID\nThis row key design would not be optimal if you want to retrieve all data for a given day. Because the day is stored in the third segment, or the row key suffix, you cannot just request a range of rows based on the suffix or a middle segment of the row key. Instead, you have to send a read request with a filter that scans the entire table looking for the day value.\"\n\nA rowkey of date#device_id will be most efficient as this will not require a full table scan.","poster":"mbkim"},{"timestamp":"1634319720.0","comment_id":"462732","upvote_count":"3","poster":"anji007","content":"Ans: A\nFrom the question \"Most common query is all data for a given device for a given day\".\nso A, B, C Rowkey will work to query.\nBut B and C will cause larger Rows hence poor performance.\nAlso, considering granularity of date & device_id RowKey with \"date#device_id\" performs better."},{"comment_id":"458331","content":"A: its date and device id as row key. Date is less granular and in a single day you will have many entries for the same device. A is perfect choice.","timestamp":"1633534020.0","poster":"ManojT","upvote_count":"3"},{"content":"A because, as per guidelines, rowkey should have less granular data first followed by next granular data.. In question, its for a given day, so date is the least granular. Date comes first followed by device id. This followed by the data gives all columns","poster":"safiyu","comment_id":"422610","upvote_count":"2","timestamp":"1628589360.0"},{"poster":"sumanshu","timestamp":"1624795860.0","upvote_count":"3","comment_id":"392026","content":"Vote for 'A', only A is satisying the given query - The most common query is for all the data for a given device for a given day."},{"comments":[{"poster":"Jphix","comment_id":"363971","timestamp":"1621722780.0","content":"Literally from that link: \"If you put a timestamp in a row key, you need to precede it with a high-cardinality value like a user ID to avoid hotspotting.\" I don't see how this makes A a good candidate. C is best.","upvote_count":"4"}],"timestamp":"1618336680.0","comment_id":"334856","poster":"mds_mds","upvote_count":"4","content":"A - Especially where the date part in a key should appear based on query pattern in the question. Pls look at this GCP documentation - https://cloud.google.com/bigtable/docs/schema-design#row-keys"},{"timestamp":"1615387020.0","upvote_count":"12","poster":"daghayeghi","comment_id":"307241","content":"None of them is correct answer. The rowkey should be device_id#date. The next best answer could be C. the rowkey should start with device_id, not date."},{"poster":"naga","content":"Correct D","comment_id":"285644","upvote_count":"2","timestamp":"1612717680.0"},{"content":"for me dont doubt its D because it cleary mention in uestion that its access by device and date","comment_id":"228466","poster":"federicohi","timestamp":"1606408380.0","upvote_count":"2"},{"comments":[{"upvote_count":"1","comments":[{"timestamp":"1613756280.0","poster":"funtoosh","content":"Correct ans: A","comment_id":"294480","upvote_count":"1"}],"timestamp":"1613756280.0","comment_id":"294479","poster":"funtoosh","content":"It should not be device id first followed by date because we are going to query for 1 day and the date is the least granular column here. My understanding is that in the row key the least granular column should come 1st"}],"upvote_count":"4","content":"The correct answer in all these option is A. Look for https://cloud.google.com/bigtable/docs/schema-design\n\nThe common query is for all the data for a given device for a given day.\nIn this case to avoid sort and search , the row key will have date and device id so that we can have all data for a given device for a given day.\nThough it should be device id first followed by date to limit query for a device id.","timestamp":"1605709740.0","comment_id":"221959","poster":"Radhika7983"},{"content":"# Device = 50000\n# Unique date time points = 4*24*365*2 = 70000+ (4 -> every 15 mins, 24 hours, 365 days, 2 years).\nSp Device should come first, Next should be date. \nThere is no restrition includinga few data points such as location","upvote_count":"2","timestamp":"1603818720.0","poster":"Insane7","comment_id":"207192"},{"content":"None of them is correct answer. The rowkey should be device_id#date. The next best answer could be A.","timestamp":"1600418940.0","comment_id":"181497","poster":"SteelWarrior","upvote_count":"4"},{"upvote_count":"3","poster":"haroldbenites","comment_id":"161141","content":"C. Correct . Most common queries is by Device id an then for date.","timestamp":"1597791960.0"},{"timestamp":"1597693440.0","poster":"saurabh1805","comment_id":"160319","content":"it should be C, Having date as first entry in row key will kill the database.","upvote_count":"4"},{"timestamp":"1596110220.0","poster":"yoRob","content":"A. The questions clearly states the access method most often required on the dataset.","comment_id":"147385","upvote_count":"2"},{"content":"\"C\" is the best option out of given choices. Datapoint can never be the key. Date as key will result in hotspots. Using only device id as row key is not optimal but it is still best best choice out of those presented.","comment_id":"126358","upvote_count":"7","poster":"dg63","timestamp":"1593888000.0"},{"poster":"yxyj","comments":[{"poster":"Ankit267","comment_id":"142236","timestamp":"1595529300.0","upvote_count":"1","content":"Not true, more granular variable should come at last..Key should be less granular -->More granular"}],"comment_id":"121300","upvote_count":"10","content":"Should be C. Never use Date in first place of Row Key.","timestamp":"1593269580.0"},{"poster":"[Removed]","timestamp":"1584766620.0","comment_id":"66402","upvote_count":"9","content":"Should be A"}],"answer_ET":"A","question_text":"MJTelco Case Study -\n\nCompany Overview -\nMJTelco is a startup that plans to build networks in rapidly growing, underserved markets around the world. The company has patents for innovative optical communications hardware. Based on these patents, they can create many reliable, high-speed backbone links with inexpensive hardware.\n\nCompany Background -\nFounded by experienced telecom executives, MJTelco uses technologies originally developed to overcome communications challenges in space. Fundamental to their operation, they need to create a distributed data infrastructure that drives real-time analysis and incorporates machine learning to continuously optimize their topologies. Because their hardware is inexpensive, they plan to overdeploy the network allowing them to account for the impact of dynamic regional politics on location availability and cost.\nTheir management and operations teams are situated all around the globe creating many-to-many relationship between data consumers and provides in their system. After careful consideration, they decided public cloud is the perfect environment to support their needs.\n\nSolution Concept -\nMJTelco is running a successful proof-of-concept (PoC) project in its labs. They have two primary needs:\n✑ Scale and harden their PoC to support significantly more data flows generated when they ramp to more than 50,000 installations.\n✑ Refine their machine-learning cycles to verify and improve the dynamic models they use to control topology definition.\nMJTelco will also use three separate operating environments `\" development/test, staging, and production `\" to meet the needs of running experiments, deploying new features, and serving production customers.\n\nBusiness Requirements -\n✑ Scale up their production environment with minimal cost, instantiating resources when and where needed in an unpredictable, distributed telecom user community.\n✑ Ensure security of their proprietary data to protect their leading-edge machine learning and analysis.\n✑ Provide reliable and timely access to data for analysis from distributed research workers\n✑ Maintain isolated environments that support rapid iteration of their machine-learning models without affecting their customers.\n\nTechnical Requirements -\nEnsure secure and efficient transport and storage of telemetry data\n//IMG//\n\n✑ Rapidly scale instances to support between 10,000 and 100,000 data providers with multiple flows each.\n✑ Allow analysis and presentation against data tables tracking up to 2 years of data storing approximately 100m records/day\n✑ Support rapid iteration of monitoring infrastructure focused on awareness of data pipeline problems both in telemetry flows and in production learning cycles.\n\nCEO Statement -\nOur business model relies on our patents, analytics and dynamic machine learning. Our inexpensive hardware is organized to be highly reliable, which gives us cost advantages. We need to quickly stabilize our large distributed data pipelines to meet our reliability and capacity commitments.\n\nCTO Statement -\nOur public cloud services must operate as advertised. We need resources that scale and keep our data secure. We also need environments in which our data scientists can carefully study and quickly adapt our models. Because we rely on automation to process our data, we also need our development and test environments to work as we iterate.\n\nCFO Statement -\nThe project is too large for us to maintain the hardware and software required for the data and analysis. Also, we cannot afford to staff an operations team to monitor so many data feeds, so we will rely on automation and infrastructure. Google Cloud's machine learning will allow our quantitative researchers to work on our high-value problems instead of problems with our data pipelines.\nMJTelco needs you to create a schema in Google Bigtable that will allow for the historical analysis of the last 2 years of records. Each record that comes in is sent every 15 minutes, and contains a unique identifier of the device and a data record. The most common query is for all the data for a given device for a given day.\nWhich schema should you use?","question_id":256,"exam_id":11,"answer_images":[],"answers_community":["C (47%)","A (47%)","6%"],"answer":"C","question_images":["https://www.examtopics.com/assets/media/exam-media/04341/0003400007.png"],"choices":{"C":"Rowkey: device_id Column data: date, data_point","A":"Rowkey: date#device_id Column data: data_point","B":"Rowkey: date Column data: device_id, data_point","D":"Rowkey: data_point Column data: device_id, date","E":"Rowkey: date#data_point Column data: device_id"},"unix_timestamp":1584276120,"topic":"1","isMC":true,"timestamp":"2020-03-15 13:42:00"},{"id":"uuDCPBdyC1pcl76mzVGl","answer":"B","isMC":true,"exam_id":11,"answer_ET":"B","discussion":[{"comments":[{"upvote_count":"1","content":"But it requires much more memory causing it more expensive, which is not what we're aiming for here..","poster":"Trocinek","comment_id":"1176580","timestamp":"1726664460.0"}],"comment_id":"64260","timestamp":"1600166580.0","upvote_count":"36","content":"I would say B since Apache Spark is faster than Hadoop/Pig/MapReduce","poster":"jvg637"},{"comment_id":"765322","poster":"ler_mp","timestamp":"1688446080.0","content":"Wow, a question that does not recommend to use Google product","upvote_count":"19"},{"upvote_count":"1","content":"Selected Answer: B\nJust a regular Spark. B","timestamp":"1716304740.0","poster":"axantroff","comment_id":"1076488"},{"poster":"DataFrame","timestamp":"1715927400.0","content":"C. I think it should be C because intent of asking question is to realize the problem of on-prem auto-scaling not the optimization that we achieve using spark in-memory features. Its GCP exam they want to highlight if hadoop cluster commodity hard doesn't increase when data increases then it can create problem unlike GCP. Hence migrate to GCP.","upvote_count":"1","comment_id":"1073109"},{"comment_id":"948077","content":"None. Being a GCP exam, it must be either Dataflow or BigQuery :D","poster":"itsmynickname","timestamp":"1704895920.0","upvote_count":"11"},{"comment_id":"880583","timestamp":"1698247980.0","upvote_count":"5","content":"I would like to take a moment to thank you all guys\nYou guys are awesome!!!","poster":"KHAN0007"},{"poster":"Whoswho","content":"looks like he's trying to spark the company up.","timestamp":"1687451220.0","upvote_count":"8","comment_id":"753570","comments":[{"upvote_count":"2","content":"It seems he's not well paid.","timestamp":"1704895800.0","comment_id":"948075","poster":"itsmynickname"}]},{"comment_id":"750310","poster":"Krish6488","content":"Selected Answer: B\nBoth Pig & Spark requires rewriting the code so its an additional overhead, but as an architect I would think about a long lasting solution. Resizing Hadoop cluster can resolve the problem statement for the workloads at that point in time but not on longer run. So Spark is the right choice, although its a cost to start with, it will certainly be a long lasting solution","timestamp":"1687209060.0","upvote_count":"4"},{"timestamp":"1672081680.0","poster":"Mamta072","comment_id":"622693","upvote_count":"2","content":"Ans is B . Apache spark."},{"poster":"alecuba16","content":"Selected Answer: B\nSPARK > hadoop, pig, hive","timestamp":"1666271820.0","upvote_count":"4","comment_id":"588703"},{"content":"B - Apache Spark","poster":"kped21","comment_id":"544160","upvote_count":"1","comments":[{"timestamp":"1696585320.0","content":"https://www.ibm.com/cloud/blog/hadoop-vs-spark","comment_id":"862826","poster":"luamail","upvote_count":"2"}],"timestamp":"1660080660.0"},{"upvote_count":"1","poster":"kped21","timestamp":"1659044640.0","content":"B Spark for optimization and processing.","comment_id":"535007"},{"poster":"sraakesh95","comment_id":"523239","upvote_count":"1","timestamp":"1657753980.0","content":"Selected Answer: B\nB: Spark is suitable for the given operation is much more powerful"},{"comment_id":"516579","content":"Selected Answer: B\nas explained by pr2web","upvote_count":"1","poster":"medeis_jar","timestamp":"1656930480.0"},{"comment_id":"498821","poster":"pr2web","content":"Selected Answer: B\nAns B: \nSpark is a 100 times faster and utilizes memory, instead of Hadoop Mapreduce's two-stage paradigm.","upvote_count":"1","timestamp":"1654870380.0"},{"poster":"MaxNRG","content":"B as Spark can improve the performance as it performs lazy in-memory execution.\nSpark is important because it does part of its pipeline processing in memory rather than copying from disk. For some applications, this makes Spark extremely fast.","comments":[{"poster":"MaxNRG","content":"With a Spark pipeline, you have two different kinds of operations, transforms and actions. Spark builds its pipeline used an abstraction called a directed graph. Each transform builds additional nodes into the graph but spark doesn't execute the pipeline until it sees an action.\nSpark waits until it has the whole story, all the information. This allows Spark to choose the best way to distribute the work and run the pipeline. The process of waiting on transforms and executing on actions is called, lazy execution. For a transformation, the input is an RDD and the output is an RDD. When Spark sees a transformation, it registers it in the directed graph and then it waits. An action triggers Spark to process the pipeline, the output is usually a result format, such as a text file, rather than an RDD.","timestamp":"1652630040.0","upvote_count":"1","comment_id":"478891","comments":[{"comments":[{"upvote_count":"1","content":"Wont Option B increase the cost ? Cost of re-writing the job in Spark + Cost of additional memory ?","timestamp":"1681894680.0","comment_id":"698847","poster":"kastuarr"}],"poster":"MaxNRG","timestamp":"1652630040.0","comment_id":"478892","upvote_count":"3","content":"Option A is wrong as Pig is wrapper and would initiate Map Reduce jobs\nOption C is wrong as it would increase the cost.\nOption D is wrong Hive is wrapper and would initiate Map Reduce jobs. Also, reducing the size would reduce performance."}]}],"comment_id":"478890","timestamp":"1652629980.0","upvote_count":"1"},{"timestamp":"1650044460.0","content":"Ans: B\nSpark performs better than MapReduce due to in memory processing.","poster":"anji007","comment_id":"462731","upvote_count":"2"},{"poster":"[Removed]","comment_id":"462234","timestamp":"1649967240.0","upvote_count":"1","content":"All are wrong\n\nA D runs in map-reduce , so rejected\nB - spark costlier than map-reduce as in-memory. Data also increased\n\nC - adding Hadoop nodes also costlier.."},{"comment_id":"458333","timestamp":"1649259000.0","upvote_count":"1","content":"B: The objective is to not increase the cost at the sametime do the analyitics required. Mapreduce jobs are not efficient and fast as spark so it will avoid failing the jobs.","poster":"ManojT"},{"comment_id":"456592","upvote_count":"1","content":"None of the given answers are correct.\nA. Rewrite the job in Pig. -- Wrong, because of the developer time cost.\nB. Rewrite the job in Apache Spark. -- Wrong, because of the developer time cost.\nC. Increase the size of the Hadoop cluster. This will increase cost too. \nD. Wrong.","poster":"squishy_fishy","timestamp":"1648991340.0"},{"content":"Very unhelpful","poster":"wubston","upvote_count":"2","timestamp":"1684059540.0","comment_id":"717935"},{"upvote_count":"1","comment_id":"449576","poster":"hadoopdk","content":"pig/hive both slower then MapReduce, & MapReduce is slower then spark.\nIf we increase cluster size then only pig is best choice. Otherwise to improve performance we should go with Spark.\nAnswer: B","timestamp":"1647967200.0"},{"comments":[{"timestamp":"1647419640.0","content":"Yes but it's much less if you compare to increasing size of cluster","upvote_count":"1","comment_id":"445692","poster":"jonabc123"}],"timestamp":"1646418720.0","comment_id":"439265","poster":"squishy_fishy","upvote_count":"2","content":"The requirement is not to increase the cost, please don't ignore the fact that re-write jobs would increase the developer time = cost, it should be C."},{"poster":"safiyu","timestamp":"1644495660.0","content":"A is the right answer. Question specifies \"without increasing costs\". Even though spark and hadoop cluster are better answers, they do increase cost. Even though Pig is based on mapreduce, its designed to process high volumes of data. The performance issues should be reduced with Pig.","comments":[{"timestamp":"1646834520.0","poster":"michaelkhan3","content":"Rewriting a job in different language isn't free either","comment_id":"441960","upvote_count":"1"}],"comment_id":"422630","upvote_count":"3"},{"poster":"sumanshu","upvote_count":"2","comment_id":"401972","content":"Vote for B, Spark is faster than MR","timestamp":"1641657360.0"},{"poster":"SageDO","timestamp":"1632901020.0","content":"C is the answer. Everyone saying B is ignoring the point about not increasing cost. Spark is significantly more expensive than Hadoop. That eliminates B.","comments":[{"content":"What do you mean by \"expensive\"? Harder than Hive (SQL) and slower to get started with? Maybe","poster":"lollo1234","comments":[{"poster":"yoshik","comment_id":"443559","upvote_count":"1","timestamp":"1647111360.0","content":"I guess he means monetary cost indeed, so his reasoning is wrong."}],"timestamp":"1637599440.0","comment_id":"363744","upvote_count":"1"},{"poster":"sandipk91","comment_id":"421242","timestamp":"1644251040.0","upvote_count":"1","content":"what about the increase cost in scaling the infrastructure"}],"comment_id":"323220","upvote_count":"1"},{"upvote_count":"2","comment_id":"309279","poster":"BhupiSG","timestamp":"1631482200.0","content":"Correct: B"},{"upvote_count":"2","content":"B:\nsince Apache Spark is faster than Hadoop/Pig/MapReduce","comment_id":"307244","timestamp":"1631277780.0","poster":"daghayeghi"},{"poster":"naga","timestamp":"1628349180.0","content":"Correct B","comment_id":"285645","upvote_count":"2"},{"content":"Pig is also MapReduce behind the scenes. So I don't see how it can increase the responsiveness of the jobs. B - Apache Spark seems to be the better option","timestamp":"1625991120.0","poster":"adigabp","upvote_count":"3","comment_id":"264616"},{"timestamp":"1625693160.0","content":"Agree with c as B is much expensive to use memory","poster":"amarkan","comment_id":"262222","upvote_count":"2"},{"timestamp":"1623793680.0","content":"B to get in memory processing.","upvote_count":"2","comment_id":"245048","poster":"NamitSehgal"},{"comment_id":"222691","timestamp":"1621412940.0","content":"In this question the most significant factor is cost. Both platforms are open-source and completely free. Hadoop relies on any type of disk storage for data processing, the cost of running it is relatively low. park depends on in-memory computations for real-time data processing. So, spinning up nodes with lots of RAM increases the cost of ownership considerably. Spark processes data much faster but as said it would require large amount of RAM for executing everything in memory and hence more cost. \nBased on what I read and understood, The memory in the Spark cluster should be at least as large as the amount of data you need to process, because the data has to fit in-memory for optimal performance. If we need to process extremely large quantities of data, Hadoop will definitely be the cheaper option, since hard disk space is much less expensive than memory space.\nHence I go with option C.","upvote_count":"3","comments":[{"poster":"sumanshu","content":"Increase the cluster, will increase the cost","comment_id":"392053","upvote_count":"1","timestamp":"1640616480.0"}],"poster":"Radhika7983"},{"content":"Answer should be B: Apache Spark provide significant high performance when compared with MapReduce.","poster":"SteelWarrior","comment_id":"181501","upvote_count":"3","timestamp":"1616064900.0"},{"upvote_count":"3","content":"B is correct answer here.","poster":"saurabh1805","comment_id":"160324","timestamp":"1613598480.0"},{"upvote_count":"4","content":"B\nis correct answer","comment_id":"139790","timestamp":"1611176760.0","poster":"atnafu2020"},{"upvote_count":"4","timestamp":"1610505420.0","poster":"tprashanth","content":"I'll first go for C to see if the performance gain is as . Note that option B is costly than option C even though performance is better.","comment_id":"133357"},{"comments":[{"poster":"[Removed]","upvote_count":"1","timestamp":"1627038240.0","comment_id":"274594","content":"Hive , pig everything runs mapreduce behind the scene. So both are not correct.\n\nSpark needs more memory which costs more than increasing nodes"},{"comment_id":"126583","poster":"Rajuuu","comments":[{"poster":"sh2020","timestamp":"1611784080.0","content":"Spark is faster but also more expensive. Question asks for the same cost. So I guess A makes more sense here","upvote_count":"3","comment_id":"145241"}],"upvote_count":"2","content":"But is not as efficient and responsive as Apache Spark","timestamp":"1609828440.0"}],"comment_id":"101479","upvote_count":"3","poster":"lgdantas","content":"I think that \"A\" is reasonable too because pig would enable parallelization.","timestamp":"1606990140.0"},{"poster":"Ganshank","comments":[{"upvote_count":"1","content":"Data can be increase in future, which means we need to add more clusters...\nif we rewrite the jobs in spark that is best option.","timestamp":"1640616660.0","poster":"sumanshu","comment_id":"392058"}],"content":"This is a toss up between B or C. But neither can be achieved without increasing costs.\nB - Apache Spark will perform processing faster in-memory, but it also means that we need machines with high memory, which in turn increases cost.\nC - Increasing size of Hadoop cluster. This is the simplest, most straightforward solution, but again, this increases cost. \nToo bad there aren't any other practical options presented here.","comment_id":"72903","upvote_count":"4","timestamp":"1602322260.0"},{"comment_id":"68585","timestamp":"1601204280.0","upvote_count":"17","poster":"[Removed]","content":"Answer: B\nDescription: Spark performs in-memory processing and faster, which results in optimization of job’s processing time"},{"timestamp":"1600657380.0","upvote_count":"6","poster":"[Removed]","comment_id":"66403","content":"Should be B - without increasing the cost"}],"question_id":257,"question_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/google/view/16660-exam-professional-data-engineer-topic-1-question-42/","answer_images":[],"answers_community":["B (100%)"],"timestamp":"2020-03-15 13:43:00","choices":{"D":"Decrease the size of the Hadoop cluster but also rewrite the job in Hive.","A":"Rewrite the job in Pig.","B":"Rewrite the job in Apache Spark.","C":"Increase the size of the Hadoop cluster."},"unix_timestamp":1584276180,"answer_description":"","question_text":"Your company has recently grown rapidly and now ingesting data at a significantly higher rate than it was previously. You manage the daily batch MapReduce analytics jobs in Apache Hadoop. However, the recent increase in data has meant the batch jobs are falling behind. You were asked to recommend ways the development team could increase the responsiveness of the analytics without increasing costs. What should you recommend they do?"},{"id":"t3m9QwZcLLgr4oB60OvJ","question_images":[],"url":"https://www.examtopics.com/discussions/google/view/16819-exam-professional-data-engineer-topic-1-question-43/","answer_images":[],"timestamp":"2020-03-17 04:39:00","question_id":258,"answers_community":["A (45%)","B (30%)","C (19%)","6%"],"answer":"A","unix_timestamp":1584416340,"answer_description":"","answer_ET":"A","topic":"1","discussion":[{"poster":"[Removed]","content":"Answer will be A because when you create View it does not store extra space and its a logical representation, for rest of the option you need to write large code and extra processing for dataflow/dataproc","comments":[{"timestamp":"1585018800.0","poster":"[Removed]","comment_id":"67428","comments":[{"upvote_count":"1","poster":"cloud_rider","timestamp":"1732735740.0","content":"This was try in Oracle era, BQ prunes the query before running, so having a view as an intermediate layer does not have any impact, unless there is a heavy filtering happening within the view definition.","comment_id":"1318859"},{"timestamp":"1596792300.0","comments":[{"timestamp":"1621695300.0","upvote_count":"4","content":"You're right, BigQuery bills on number of bytes processed, regardless of them being materialized. If you don't create a new column and use a view instead, you will probably have a small performance hit but query costs would be the same and storage cost wouldn't increase (unlike storing a new column)","poster":"lollo1234","comment_id":"363749","comments":[{"upvote_count":"12","comment_id":"443566","comments":[{"content":"good catch, yoshik.","poster":"HarshKothari21","timestamp":"1662648480.0","upvote_count":"1","comment_id":"663709"}],"content":"You are asked to modify the schema and data. By using a view, the underlined table remains intact.","poster":"yoshik","timestamp":"1631466720.0"}]}],"poster":"lgdantas","comment_id":"152459","content":"Wouldn´t \"total amount of data in all table fields referenced directly or indirectly by the top-level query\" be FirstName and LastName?","upvote_count":"3"},{"content":"Views are cached the same as regular tables are, so I don't get the point of billing. It will cost the same as query to a regular table.","upvote_count":"3","comments":[{"comment_id":"729459","content":"the point of billing is extra storage costs for a new concatenated column","timestamp":"1669655580.0","poster":"ovokpus","upvote_count":"3"}],"timestamp":"1652186040.0","comment_id":"599586","poster":"alecuba16"}],"content":"Because views are not materialized, the query that defines the view is run each time the view is queried. Queries are billed according to the total amount of data in all table fields referenced directly or indirectly by the top-level query","upvote_count":"12"},{"content":"Can't be A","poster":"[Removed]","timestamp":"1585018860.0","comment_id":"67429","upvote_count":"5"},{"timestamp":"1667674680.0","content":"I agree that A is correct. Also, I think B is wrong as the UPDATE statement is used to update values in existing columns, not to create a new column.","poster":"beowulf_kat","comment_id":"711962","comments":[{"comments":[{"content":"What happen if there are new employees joining the company, update every single time?","poster":"[Removed]","timestamp":"1677008160.0","upvote_count":"1","comment_id":"817071"}],"content":"Of course, you use UPDATE after creating the new column, that is what the option said","comment_id":"721708","poster":"ovokpus","upvote_count":"2","timestamp":"1668822960.0"}],"upvote_count":"2"},{"upvote_count":"17","poster":"funtoosh","comment_id":"294489","timestamp":"1613756880.0","content":"cannot be 'A'as it clearly says that you need to change the schema and data.","comments":[{"poster":"exnaniantwort","comment_id":"530206","comments":[{"comment_id":"656315","content":"A yes, That make a lot of sense and also if you update the table only once with UPDATE if there is a new employee it will not be up to date with the new column, if the app use a view it will be up to date every time it query.\nBut in any case the cost will not be minimized.","timestamp":"1662040020.0","upvote_count":"4","poster":"YorelNation"},{"timestamp":"1642904340.0","comment_id":"530211","content":"There is always different application requirement to use different format. That way you will just creating more and more redundant columns in different formats. That is tedious.","upvote_count":"6","poster":"exnaniantwort"}],"timestamp":"1642903980.0","upvote_count":"15","content":"Your primary task is to \"make data available\".\nChanging the schema is just the request from the member \"A member of IT is building an application and ***asks you to modify the schema and data*** in BigQuery\". You don't have to follow it if it does not make sense."}]}],"comment_id":"67019","timestamp":"1584895500.0","upvote_count":"68"},{"comments":[{"timestamp":"1621695360.0","content":"DML statements don't increase costs, but storing a new column does. I see A is correct (also see my comment above)","poster":"lollo1234","comments":[{"timestamp":"1642904100.0","comments":[{"comment_id":"765324","content":"Storage is cheap compared to computation","poster":"ler_mp","timestamp":"1672815000.0","upvote_count":"8"}],"comment_id":"530207","poster":"exnaniantwort","upvote_count":"3","content":"Exactly. Cost is the reason to reject B.\nHow come so many people vote for this wrong option?"}],"comment_id":"363750","upvote_count":"5"},{"content":"But you need to maintain table means regularly you have to execute the update query whenever new data comes.","poster":"DGames","timestamp":"1670969040.0","comment_id":"744487","upvote_count":"2"},{"poster":"lollo1234","timestamp":"1621695660.0","comment_id":"363754","content":"I will also add that B would imply changing upstream workloads to write the new field every time a records gets added","upvote_count":"8"}],"poster":"BhupiSG","comment_id":"309281","timestamp":"1615592220.0","upvote_count":"47","content":"Correct: B\nBigQuery has no quota on the DML statements. (Search Google - does bigquery have quota for update).\nWhy not C: This is a one time activity and SQL is the easiest way to program it. DataFlow is way overkill for this. You will need to find an engineer who can develop DataFlow pipelines. Whereas, SQL is so much more widely known and easier. One of the great features about BigQuery is its SQL interface. Even for BigQueryML services."},{"poster":"willyunger","upvote_count":"1","content":"Selected Answer: A\nMinimal cost: no extra space, no cost to set up, no need to write code, rest of applications see no change, no need to offload/reprocess/reload (although batch load is free).","comment_id":"1400620","timestamp":"1742406780.0"},{"comment_id":"1346652","content":"Selected Answer: B\nI would say A but since it specifically says \"modify\" then the answer is B.","timestamp":"1737838320.0","poster":"LP_PDE","upvote_count":"1"},{"comment_id":"1259069","timestamp":"1722454260.0","poster":"iooj","upvote_count":"2","content":"E. Say to the IT specialist to take care of it on the app side...\nB would work for historical data if we had an underlying change made to automate the concatenation for new records. It is not clear, so I would say A is a quick solution."},{"content":"Selected Answer: B\nRequirement is to be able to filter on full name. So, you would be querying all data unless you have materialized full name column.","timestamp":"1714544160.0","poster":"Ramanaiah","upvote_count":"1","comment_id":"1204882"},{"poster":"philli1011","timestamp":"1706448300.0","content":"Definitely A","comment_id":"1134098","upvote_count":"1"},{"content":"Selected Answer: A\nThe question might be outdated, but I would like to offer my perspective:\n\n1. Ideally, I would opt for a materialized view to avoid updating pipelines\n2. In 2023, I see no concerns regarding the costs involved in storing denormalized data for analytical needs\n3. Regarding this question I would choose option A, although the concern about extra costs due to recalculations is valid for me","poster":"axantroff","comment_id":"1076522","timestamp":"1700589240.0","comments":[{"upvote_count":"2","comment_id":"1100468","content":"Did u pass the exam ?","poster":"LaxmanTiwari","timestamp":"1702977960.0"}],"upvote_count":"2"},{"timestamp":"1699517460.0","upvote_count":"1","content":"Answer should be A 'cos the First request is: make that data available.","poster":"steghe","comment_id":"1066235"},{"upvote_count":"1","content":"Its A ..... \"asked to change schema\" is a trick to test your skills. Better to make use of MV's if anyhow the application is gonna query repeatedly. MV's will rebuild itself, if query invalidates from cache results","timestamp":"1691252280.0","comment_id":"973158","poster":"alihabib"},{"comment_id":"967605","poster":"nescafe7","timestamp":"1690761900.0","upvote_count":"2","content":"Selected Answer: A\nIn the case of B, the data pipeline that adds new employee information must also be modified, which is not the correct answer in terms of cost minimization."},{"poster":"Mathew106","comment_id":"961381","timestamp":"1690190340.0","upvote_count":"1","content":"Selected Answer: A\nIt's A. If you add a column to the table, you will be billed every time you query that new column. The same way you would be billed with the view created by A. \n\nB,C and D create a new column. A does not create a new column. It just provides the interface for the application to access the data. B,C and D will have to be rerun to compute the column value of new customers. \n\nA is done only once, costs 0 for storage, and is charged about the same as all the others when it comes to compute because even if you choose B C and D you would have to query the data in the end anyway."},{"poster":"autumn2005","upvote_count":"1","timestamp":"1690020120.0","content":"Selected Answer: C\nmodify the schema","comment_id":"959404"},{"poster":"theseawillclaim","timestamp":"1689617520.0","comment_id":"954502","upvote_count":"1","content":"Can you code a script for a BQ Column? I don't think it's \"B\", it is pretty tricky"},{"timestamp":"1687864020.0","poster":"KC_go_reply","content":"Selected Answer: A\nEverything but A) new view is wrong.\n\nB) sounds okay, but introduces a new column which means more storage, thus increasing cost.\nC) Dataflow is obvious overkill for a simple task such as concatenating two strings.\nD) Starting up a Dataproc cluster just for string concatenation is super overkill.","comment_id":"935272","upvote_count":"1"},{"content":"Selected Answer: A\nif a new field is only necessary for one project, and it is only the concatenation of two existing fields, it is ok to create a view that gets used for a specific task.","upvote_count":"1","comment_id":"903287","poster":"vaga1","timestamp":"1684676520.0"},{"timestamp":"1682447940.0","comment_id":"880757","upvote_count":"2","poster":"Jarek7","content":"Selected Answer: A\nI'd go for A.\nThe main issue with answers B,C,D is that they are just temporary solution. Whenever a new employee comes in (there are 400.000 of them at the moment, so we can expect every day a few new guys) we need to update the fullname table/field again. Additionally each of these answers need twice as much capacity (BigQuery stores data in a columnar format, so optimizing is not possible). Although the price for the needed capacity will be far below 0.01$/month.\nThe main argument against A is that compute power costs more than capacity. Please look how BQ is priced: https://cloud.google.com/bigquery/pricing#query_pricing\nIn the default On-demand compute pricing it is charged for \"the number of bytes processed by each query\" so there will be no any difference in computing costs for any option.\nYeah, there is also this argument about modyfing schema in the requirements. Lets be professional - it is not a requirement for OUR schema. If you can resolve the issue with 0 change to YOUR schema then you are more than ok. And anyway, from requestor point of view, the schema HE uses in his app will be modifed as he needed."},{"content":"Selected Answer: A\nShould be A.","poster":"izekc","upvote_count":"1","timestamp":"1682228580.0","comment_id":"877917"},{"content":"Selected Answer: B\nI vote B.\n\nA is expensive, the requirements say we need to minimize cost. \n\nB works and meets the requirements. We create an empty column and then UPDATE it to set it to a desired values.\n\nhttps://cloud.google.com/bigquery/docs/managing-table-schemas#console\n\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/dml-syntax#update_statement","upvote_count":"2","comment_id":"867621","timestamp":"1681241460.0","poster":"Adswerve"},{"poster":"juliobs","content":"Selected Answer: A\nA is the simplest.\nB will not only increase the cost of storage for the *duplicated* data as it would be a pain to keep. Would require a trigger on inserts and updates.","timestamp":"1679154240.0","upvote_count":"1","comment_id":"842903"},{"poster":"mcrokz","comments":[{"timestamp":"1688992380.0","upvote_count":"1","poster":"itsmynickname","content":"Exactly..","comment_id":"948101"}],"content":"I'd choose neither, I'd advise to concatenate on the application level the IT Member is developing than changing schema and needing to worry about everything up or downstream that the schema change could impact","upvote_count":"3","comment_id":"825460","timestamp":"1677634620.0"},{"poster":"midgoo","comment_id":"818950","content":"Selected Answer: A\nI choose A as it is the easiest way.\nFurthermore, B is not cheaper. When we SELECT based on full_name after concatenated, it is the same cost and SELECT the full_name on the VIEW.","upvote_count":"1","timestamp":"1677139140.0"},{"poster":"techtitan","timestamp":"1676832120.0","content":"Selected Answer: B\nA vs B - I'd choose B because it is ideal to do bulk updates and minimize the cost that comes from view/concatenation.","upvote_count":"1","comment_id":"814394"},{"upvote_count":"1","poster":"Tarek_74","content":"Selected Answer: B\nB. Add a new column called FullName to the Users table. Run an UPDATE statement that updates the FullName column for each user with the concatenation of the FirstName and LastName values. Most Voted","comment_id":"813156","timestamp":"1676732640.0"},{"upvote_count":"2","timestamp":"1675959000.0","comment_id":"803443","content":"Selected Answer: A\nAnswer should be A. create View is easy and cost-effective way","comments":[{"poster":"tsontson","content":"How is it cost effective ? You will have to compute the view each time thus inducing costs.","comment_id":"811876","timestamp":"1676638560.0","upvote_count":"2"}],"poster":"JJJJim"},{"timestamp":"1675582380.0","poster":"Tarek_74","comment_id":"798669","upvote_count":"1","content":"Selected Answer: B\nits B to minimizing Cost"},{"content":"Selected Answer: A\nIs not B because you are increasing costs in storage for the new column.","poster":"chatocoral","timestamp":"1675545060.0","comment_id":"798367","upvote_count":"1"},{"content":"Selected Answer: A\nA. Create a view in BigQuery that concatenates the FirstName and LastName field values to produce the FullName.\n\nThis option is the most cost-effective and efficient, as creating a view in BigQuery doesn't result in any data duplication, and the view is automatically updated whenever data in the underlying table changes. Additionally, querying a view is as efficient as querying a table, so performance will not be impacted.","timestamp":"1675504860.0","poster":"donbigi","comment_id":"797796","upvote_count":"1"},{"timestamp":"1674734400.0","poster":"PolyMoe","comment_id":"788682","content":"Selected Answer: A\nA. Create a view in BigQuery that concatenates the FirstName and LastName field values to produce the FullName. This approach allows the IT member to query the FullName field without modifying the existing Users table, and it also doesn't require any additional storage or data processing costs.\n\nB. Adding a new column called FullName to the Users table and running an update statement also will work, but it will require additional storage and cost for the new column and also the cost of running the update statement.","upvote_count":"2"},{"timestamp":"1672737720.0","comment_id":"764354","upvote_count":"1","content":"Selected Answer: A\nA is the only right answer!!!!","poster":"Nirca"},{"poster":"Jackalski","upvote_count":"3","comment_id":"761110","content":"Selected Answer: A\nto minimizing cost - I vote on A - simple and done in 2 minutes\nB - will not work for new data/ employees \nC,D are just overkill","timestamp":"1672323720.0"},{"timestamp":"1672238280.0","content":"Selected Answer: A\nI vote A because with B the Fullname field will not be filled when a new row is added.","comment_id":"759956","upvote_count":"1","poster":"Catweazle1983"},{"timestamp":"1670968980.0","upvote_count":"3","comment_id":"744486","poster":"DGames","content":"Selected Answer: C\nAnswer C - here is some point to think on each option \noption A - View - every time we need execute query adding computation cost. \nOption B - Create another Table but nothing mention about to maintain table when new data come again we need to execute Update and adding cost.\nOption C - it s best option design pipeline so it fix the issue\nOption D- CSV doesn't make sense here."},{"timestamp":"1668499980.0","content":"Selected Answer: B\nBest approach is B","upvote_count":"1","comment_id":"718584","poster":"solar_maker"},{"comment_id":"712235","poster":"tikki_boy","timestamp":"1667728800.0","comments":[{"poster":"connorscion","content":"You are paying for processing power (for the dataproc job), storage (for the staging folder of the DP job) and storage from the second table instead of just storage from the second table from a view (from option A).\n\nAlso you need to re-run that job to prevent data staleness. A view is the easiest and cheapest option.","comment_id":"751796","upvote_count":"2","timestamp":"1671589860.0"}],"content":"Selected Answer: D\nHas to be D. It's the cheapest. BigQuery export/Load is free","upvote_count":"1"},{"upvote_count":"3","comment_id":"708419","poster":"MisuLava","content":"Selected Answer: B\nBigQuery's views are logical views, not materialized views. Because views are not materialized, the query that defines the view is run each time the view is queried. Queries are billed according to the total amount of data in all table fields referenced directly or indirectly by the top-level query\n\na view would be very expensive being billed for all the data in the table when you only need one record, for example.","timestamp":"1667221140.0"},{"content":"Selected Answer: C\nits google recommended","timestamp":"1666450620.0","comment_id":"701592","upvote_count":"2","poster":"hfuihe"},{"upvote_count":"3","comments":[{"content":"Selected Answer: A","timestamp":"1665469020.0","poster":"Nirca","comment_id":"691812","upvote_count":"3"}],"content":"Selected Answer: C\nAs a data Engineer, I would have gone to management and do my best to fire this IT guy. Changing DB Schema for visualization needs?! On any level (reality, for the examination and mostly best practice) I'm going with VIEWs","poster":"Nirca","comment_id":"691810","timestamp":"1665468900.0"},{"content":"Selected Answer: B\nCorrect Answer B:\n\nThey key phrases here are:\n- A member of IT is building an application and asks you to modify the schema and data in BigQuery\n- Minimize cost\n\n❌ A: Wouldn't change the schema of the users table\n\n✅ B: This would update the schema in the users table according to what the IT member requested.\n\n❌ C: This wouldn't update the schema of the existing table, this would do it in a NEW table which is not what was requested\n\n❌ D: Why would you need Dataproc (Spark and Hadoop) to process a CSV file? not sure if possible but this definitely would NOT MINIMIZE COST","comment_id":"681804","timestamp":"1664376360.0","upvote_count":"2","poster":"Ender_H"},{"poster":"crismo04","timestamp":"1662838320.0","content":"Selected Answer: A\nAs exnaniantwort say, Your primary task is to \"make data available\". You don't have to follow the IT member instruction if it does not make sense.","upvote_count":"1","comment_id":"665642"},{"upvote_count":"4","timestamp":"1654835820.0","comment_id":"614322","poster":"willymac2","content":"I believe the answer is A, and surprisingly not necessarily for costs reasons.\nThe main thing is that IT is building an application so will expect the full name column to be available at all times for ALL current users, meaning that the full name list should stay up to date.\nIn the 4 answers, B, C and D are running once, updating the tables or creating a table once, i.e. making a snapshot when the task was done. None of these handle new users, or user being removed. So to accept B, C or D, we would also need to change the system handling the user creation to provide this full name.\n\nA on the other hand will stay up to date without having to change the system handling the users."},{"timestamp":"1652267640.0","upvote_count":"1","comment_id":"600077","comments":[{"poster":"NickNtaken","upvote_count":"2","content":"It said add a new column first, and then run update. As I tested, you can manually add a new column and then fill values from other columns using UPDATE. So B is correct, straightforward and simple. Both C and D are overkill.","timestamp":"1653671100.0","comment_id":"608153"}],"poster":"sw52099","content":"- A is wrong since it clearly says need to change schema\n- B is wrong because UPDATE cannot be used to add column, according to https://cloud.google.com/bigquery/docs/managing-table-schemas.\n- C Cloud Dataflow is an overkill way, but is simpler than D, so C maybe the right choice\n- D too complicated"},{"timestamp":"1651757580.0","comment_id":"597322","poster":"freeman1715","content":"Selected Answer: D\nD \n100% https://cloud.google.com/bigquery/docs/manually-changing-schemas#advantages_2","upvote_count":"3"},{"upvote_count":"1","poster":"diyadagar","content":"Why 'D' couldn't be the answer, creating the table is one of the cost effective way","timestamp":"1651524600.0","comment_id":"596212"},{"poster":"richardchan66","comment_id":"584143","content":"C: Using a SQL query: choose this option if you are more concerned about simplicity and ease of use, and you are less concerned about costs. \nRecreating the table: choose this option if you are more concerned about costs, and you are less concerned about simplicity and ease of use.\n\nSimilarily, if you need to create a new column, using Update query is not cost-effective. Creating a new table is cost-effective way.\n\nReference:\nhttps://cloud.google.com/bigquery/docs/manually-changing-schemas?hl=en#changing_a_columns_name","upvote_count":"2","timestamp":"1649672400.0"},{"content":"Selected Answer: A\nAdding extra column increases the cost and updating is not preferred in BQ","upvote_count":"3","timestamp":"1648891140.0","comment_id":"579781","poster":"VivekA11"},{"timestamp":"1647454560.0","content":"Selected Answer: A\nAns: A\nExports and imports jobs are free but not the Dataflow and Dataproc jobs ==> So C & D are out.\nSo either in A or B:\nCost => Storage + Query.\nA: View: => No extra Storage + Querying it is proportional to data size read (First Name + Last Name).\nB: Update table => Storage: 1 new column + Querying column of similar size (First Name + space + Last Name)\nSo I think B could be costlier than A.","poster":"anji007","comment_id":"569243","comments":[{"content":"So. querying doesn't cost you much? whenever there is a change in the schema you must run the query instead B is the right one, its one-time.","upvote_count":"1","timestamp":"1651234680.0","poster":"tavva_prudhvi","comment_id":"594441"}],"upvote_count":"1"},{"content":"Selected Answer: C\nCorrect is C.\n\nB is good but not cost effective\n\nAccording to google doc:\nhttps://cloud.google.com/bigquery/docs/manually-changing-schemas?hl=en#changing_a_columns_name\n\nThere are two ways to manually rename a column:\n\nUsing a SQL query: choose this option if you are more concerned about simplicity and ease of use, and you are less concerned about costs.\nRecreating the table: choose this option if you are more concerned about costs, and you are less concerned about simplicity and ease of use.\n\nSimilarily, if you need to create a new column, using Update query is not cost-effective. Creating a new table is cost-effective way,","poster":"Arkon88","comments":[{"poster":"anji007","comment_id":"569236","timestamp":"1647454200.0","upvote_count":"1","content":"Was is renaming the column or concatenating 2 cols to other one (Full Name)?"}],"timestamp":"1646910780.0","comment_id":"564751","upvote_count":"3"},{"poster":"Prashant2022","timestamp":"1646106840.0","content":"Correct ans is A because views or table both add cost when queried but if you create a new table or new col it will add space and hence cost and any uodate to the name will also need and update hence view is most cost effective as best p.","upvote_count":"1","comment_id":"558549"},{"upvote_count":"1","comment_id":"523245","poster":"sraakesh95","content":"Selected Answer: B\nB: A might not be possible to be updated whenever a new employee join unless the query is re-run which is a huge CTC","timestamp":"1642123200.0"},{"content":"Selected Answer: B\nThe view is not materialized data + using view will use query cost. DML update is the best option. \nhttps://cloud.google.com/blog/products/data-analytics/dml-without-limits-now-in-bigquery","timestamp":"1641299640.0","upvote_count":"3","poster":"medeis_jar","comment_id":"516591","comments":[{"upvote_count":"1","timestamp":"1646285340.0","poster":"kisame20","comment_id":"559816","content":"Good answer"}]},{"content":"Its B,\ncopied a public dataset to my local project and added a new column. The table had 1.4m records, and the update ran absolutely fine updating all the records in the added column with the contact function.","poster":"kishanu","comment_id":"507055","timestamp":"1640174760.0","upvote_count":"3"},{"upvote_count":"2","poster":"Jlozano","timestamp":"1639076520.0","comment_id":"497983","content":"Selected Answer: B\n1º An aplication will query full name (maybe constantly).\n2º Make data available while save costs.\n\nA- You don't materialize view so you don't keep data available and you increase costs for each query.\n(Also see: https://cloud.google.com/bigquery/docs/best-practices-performance-compute)\n\nB- It's ok, you materialize your data. Table is not too big and DML procedure could be a option.\n\nC- Years ago could be an option, but not now.\n\nD- The worst option."},{"timestamp":"1636998300.0","upvote_count":"1","comment_id":"478881","content":"B, DML without limits, now in BigQuery\nhttps://cloud.google.com/blog/products/data-analytics/dml-without-limits-now-in-bigquery","poster":"MaxNRG"},{"content":"I go for A. Besides it's not optimal for request as number of records is only 400.000 it should not be a problem. And it's really the easiest way: no more than 5 minutes and all existing processes are not impacted","comment_id":"473393","upvote_count":"2","poster":"Thierry_1","timestamp":"1636194240.0"},{"upvote_count":"2","comment_id":"469638","content":"Correct: B Same question in Whizlabs in test 2 question 22","poster":"Track22","timestamp":"1635493080.0"},{"poster":"anji007","comments":[{"timestamp":"1647454500.0","comment_id":"569242","upvote_count":"1","content":"Ans: A\nExports and imports jobs are free but not the Dataflow and Dataproc jobs ==> So C & D are out.\nSo either in A or B:\nCost => Storage + Query.\nA: View: => No extra Storage + Querying it is proportional to data size read (First Name + Last Name).\nB: Update table => Storage: 1 new column + Querying column of similar size (First Name + space + Last Name)\nSo I think B could be costlier than A.","poster":"anji007"}],"upvote_count":"3","comment_id":"462736","timestamp":"1634320020.0","content":"Ans: B\nFrom question \"asks you to modify schema and data in BugQuery\" and \"minimizing cost\".\nA: Expensive, every query you run need to concatenate repeatedly.\nC & D: Dataflow & Dataproc they are too much for this task."},{"content":"Correct is C.\n\nAccording to google doc: \nhttps://cloud.google.com/bigquery/docs/manually-changing-schemas?hl=en#changing_a_columns_name\n\nThere are two ways to manually rename a column:\n\nUsing a SQL query: choose this option if you are more concerned about simplicity and ease of use, and you are less concerned about costs.\nRecreating the table: choose this option if you are more concerned about costs, and you are less concerned about simplicity and ease of use.\n\nSimilarily, if you need to create a new column, using Update query is not cost-effective. Creating a new table is cost-effective way,","comments":[{"comment_id":"472333","timestamp":"1635984240.0","poster":"JayZeeLee","content":"It's only true when you don't need to create a Dataflow job, which increases the cost. It'd make more sense if the table is created within BigQuery.","upvote_count":"3"},{"comment_id":"489990","timestamp":"1638198180.0","content":"Since now, the previous columns are not useful anymore. There is no need to keep them in the dataset (paying storage for them). Just create a new dataset without them. It is a one shot inversion to solve a long term issue.","poster":"maurodipa","upvote_count":"1"}],"comment_id":"461328","poster":"Chelseajcole","timestamp":"1634089260.0","upvote_count":"3"},{"timestamp":"1633534500.0","upvote_count":"2","content":"B: You should not use dataflow or dataproc when transformation can be done by a simple SQL statemen. In this case, you can create a simple job which would concatenate 2 columns and update.","comment_id":"458334","poster":"ManojT"},{"timestamp":"1633266780.0","comment_id":"456595","upvote_count":"1","poster":"squishy_fishy","content":"The answer is C. \nAnswer B couldn't handle the future data properly. Only C can handle that."},{"poster":"umagayathri08","comment_id":"432660","timestamp":"1630027500.0","upvote_count":"2","content":"C answer"},{"poster":"umagayathri08","comment_id":"432659","timestamp":"1630027440.0","content":"C answer","upvote_count":"2"},{"comment_id":"424489","poster":"safiyu","content":"answer is C. view is expensive for concurrent users. also the administrator asked to change the schema. updating is not possible as there is 1500 daily limit on dml. so create new table","comments":[{"comment_id":"451850","poster":"MrXBasit","upvote_count":"1","content":"You are wrong. There is no daily limit","timestamp":"1632660660.0"}],"timestamp":"1628876040.0","upvote_count":"3"},{"timestamp":"1625302680.0","poster":"gcp_learner","content":"I will go with option A because it creates the concatenated field at zero cost. Option C, while creating the new field will incur DataFlow costs.","comment_id":"397438","upvote_count":"1"},{"comment_id":"392061","poster":"sumanshu","upvote_count":"3","timestamp":"1624798620.0","content":"vote for B, Schema is modifying only in 'B'"},{"timestamp":"1621780320.0","content":"DML without limits, now in BigQuery\nhttps://cloud.google.com/blog/products/data-analytics/dml-without-limits-now-in-bigquery\nanswer is B mates","poster":"H_S","comment_id":"364637","upvote_count":"6"},{"content":"B:\nI think this question was asked in a moment when BQ didn´t allow DML, so you had to go out of BQ to do transformations. But now you can do DML within BQ, and quotas aren't a problem for this case. Adding a new column will modify the schema.\nhttps://cloud.google.com/blog/products/data-analytics/dml-without-limits-now-in-bigquery","poster":"daghayeghi","upvote_count":"7","timestamp":"1615392000.0","comments":[],"comment_id":"307283"},{"poster":"RT30","content":"Looks C : https://cloud.google.com/bigquery/docs/managing-table-schemas You can overwirte table or delete the old one and cratea a new one","comment_id":"304878","timestamp":"1615075800.0","upvote_count":"2"},{"poster":"naga","content":"Correct C","timestamp":"1612718160.0","upvote_count":"2","comment_id":"285648"},{"upvote_count":"1","comment_id":"265402","content":"I think it should be A. Views don't have extra cost. It's the same cost as running the underlying query which is same as running queries on the base table. \nThe other reasonable option is B where we can add a column and update the value. However, this is ok if it is a one-time update. Otherwise ,we would need to modify the pipeline that loads into this table as well.","timestamp":"1610443020.0","poster":"adigabp"},{"content":"Option D - ref https://cloud.google.com/bigquery/docs/managing-table-schemas#adding_columns_to_a_tables_schema_definition\n\nAs mentioned, You can add columns to an existing table's schema definition:\n- Manually (creates an empty column)\n- When you use a load or query job to overwrite a table\n- When you append data to a table using a load or query job\n\nBy using a new CSV file that contains the concatenated Fullname, you can load the new CSV file as described in the documentation https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv#appending_to_or_overwriting_a_table_with_csv_data.","timestamp":"1608297540.0","comments":[{"content":"I think this option was not avaqialbe at the time when the question was initially created. So it used to be C, now it's inclining towards D","upvote_count":"2","timestamp":"1609144440.0","comment_id":"253918","poster":"zh31427"}],"poster":"cosmidumi","upvote_count":"3","comment_id":"247334"},{"upvote_count":"4","content":"Correct answer is C as the best option is to create a new table with the updated columns. Dataflow provides a serverless NoOps option to convert data.","poster":"Nileshk611","timestamp":"1606901820.0","comment_id":"232815"},{"upvote_count":"2","content":"A is not the correct answer in this case. Views should be created when other application or business needs to query multiple table and only certain columns. Also, when we don't the other application or user to have access to all the underlying tables and all columns data, we think of creating view. Also, please this https://cloud.google.com/bigquery/docs/views-intro#view_pricing\nYou will have to pay for view, In this scenario the IT is building an application and will constantly need data, it is better to modify the table and add a column.\nUse an alter table to add a new column and option B should be use to update the data, please see this\nhttps://cloud.google.com/blog/products/data-analytics/dml-without-limits-now-in-bigquery","poster":"Radhika7983","timestamp":"1605784500.0","comment_id":"222721"},{"poster":"federicohi","comment_id":"221373","upvote_count":"2","content":"A becuase the cost of bringing data of a view its the same than table Ex\ntable AAA View V_AAA its th same cost run select columns from AAA than select columns from V_AAA","timestamp":"1605644040.0"},{"content":"I choose B, also remember in support of B is the fact that the first 1 TB queries is free! Hence the cost of doing the update in B is free --- or 4 dollars per TB On demand (https://cloud.google.com/bigquery/pricing). According to https://stackoverflow.com/questions/56600093/what-is-the-cost-of-adding-a-new-column-in-bigquery-table, there is no cost to adding a column in BQ. Using Dataproc costs $0.010 * # of vCPUs * hourly duration see https://cloud.google.com/dataproc/pricing. This hourly price only applies if you use up to 30 minutes!!, else you will be billed by the second!. In addition to the cost of Compute Engine per instance. How many instances do you need per 400,000 employees --- well give it a guess","timestamp":"1605348420.0","upvote_count":"7","comment_id":"219044","poster":"GeeBeeEl"},{"upvote_count":"1","comment_id":"209328","timestamp":"1604067600.0","content":"A is the best solution in C you are giving 3 columns while requirement is for firstname and lastname concatenation.","poster":"Surjit24"},{"content":"I will go with option B , as this is just 1 time activity and most cost effective according to my understanding.","upvote_count":"3","timestamp":"1601202900.0","comment_id":"188307","poster":"Tanmoyk"},{"poster":"alek6dj","comment_id":"187668","upvote_count":"3","content":"My vote is B. I think this question was asked in a moment when BQ didn´t allow DML, so you had to go out of BQ to do transformations. But now you can do DML within BQ, and quotas aren't a problem for this case. Adding a new column will modify the schema.","timestamp":"1601124780.0"},{"content":"Answer should be C:\nA & B both do not modify the schema and A does not modify the data also.\nD is not a complete solution as it doesn't mention new table that needs to be created in BigQuery with new column.","timestamp":"1600420980.0","poster":"SteelWarrior","comment_id":"181519","upvote_count":"1"},{"content":"B\nBigQuery allows partial modification on an existing table's schema definition. The following action are allowed -\n1. Adding columns to a schema design\n2. Relaxing a columns's mode from REQUIRED to Nullable.\nMeanwhile, Use Dataflow is not a cheap or simple solution comparing to update the table directly from BQ.\nThis is the same question that I got from Wiz Labs.","timestamp":"1600225860.0","upvote_count":"6","comment_id":"180152","poster":"VIncent9261111"},{"upvote_count":"1","poster":"wenxiang","comment_id":"172390","timestamp":"1599105600.0","content":"None. its only 400k, download csv, use python with pandas/spark, and reload. FOC"},{"poster":"haroldbenites","comment_id":"161147","content":"C is the best option. In this case, bacause of It has one only table \"Users\", the view scan the same table and it return the columns originals and the concatened column. If we used more than one table, Would be better create another table with the concatened column","timestamp":"1597792740.0","upvote_count":"1"},{"content":"Correct Answer: C\nExplanation:-This option is correct as the best option is to create a new\ntable with the updated columns. Dataflow provides a serverless NoOps\noption to convert data.","timestamp":"1595746680.0","comment_id":"143874","upvote_count":"2","poster":"VishalB"},{"poster":"tprashanth","content":"C is correct if its a one time ask.\nB seems to be more appropriate if the report would be generated often in the future also that includes full name. This way, compute costs are reduced though storage is increased.\n\nBecause it doesn;t say anything about one time query or for many times, may be assume its a one time ask.","timestamp":"1594600380.0","comment_id":"133355","upvote_count":"2"},{"comments":[{"comment_id":"129284","poster":"andibaso","upvote_count":"2","timestamp":"1594157160.0","content":"https://cloud.google.com/blog/products/data-analytics/dml-without-limits-now-in-bigquery"}],"poster":"andibaso","content":"B. View makes sense, but you need to pay extra 2 bytes every time query executed compare to single column. C-Extra cost for storage and dataproc. B-One time cost read and write. Remember pricing base on size read/write.","timestamp":"1594156980.0","comment_id":"129283","upvote_count":"3"},{"timestamp":"1592659560.0","upvote_count":"3","content":"B is correct.\nwhy not A: BigQuery's views are logical views, not materialized views. Because views are not materialized, the query that defines the view is run each time the view is queried. https://cloud.google.com/bigquery/docs/views-intro#view_pricing\nwhy B: DML pricing for UPDATE statement is reasonable. https://cloud.google.com/bigquery/pricing#dml_pricing_for_non-partitioned_tables\nwhy not C: unnecessarily created another table. cost of Dataproc.\nwhy not D: unnecessarily created another table. cost of Dataproc.","poster":"ch3n6","comment_id":"114801"},{"comment_id":"75538","content":"I think it should be D. If u think of cost. Import and Export to Bigquery from Cloud Storage is FREE. Also, when u store the csv files, Cloud Storage is cheaper than Bigquery. For processing Dataproc is cheaper than Dataflow.","poster":"arnabbis4u","upvote_count":"7","timestamp":"1587092160.0","comments":[{"comment_id":"219035","timestamp":"1605347040.0","poster":"GeeBeeEl","content":"Is the cost of Dataproc less than the cost of an update statement? Why compare with C, why not B? A is obviously wrong because of https://cloud.google.com/bigquery/docs/views-intro#view_pricing","upvote_count":"2"}]},{"timestamp":"1586512500.0","upvote_count":"4","content":"B.\nAmongst the options listed, this seems to be the one incurring minimal cost.","poster":"Ganshank","comment_id":"72910"},{"upvote_count":"9","poster":"digvijay","content":"Answer should be C ...As the Organization want to update the schema and data specifically mentioned in question","timestamp":"1585101180.0","comment_id":"68015"},{"content":"Confused - B or C","upvote_count":"3","comments":[{"upvote_count":"2","comments":[{"content":"Can't be B. UPDATE sentence has a quota.","poster":"jvg637","comment_id":"67074","timestamp":"1584909300.0","upvote_count":"1"}],"content":"Can't be A - https://cloud.google.com/bigquery/docs/views-intro#view_pricing","poster":"[Removed]","comment_id":"66420","timestamp":"1584774960.0"},{"upvote_count":"1","poster":"Rajokkiyam","comment_id":"66631","timestamp":"1584815220.0","content":"DML operations are costly in BiqQuery. Option C looks better."}],"timestamp":"1584771900.0","comment_id":"66411","poster":"[Removed]"},{"comment_id":"65001","timestamp":"1584416340.0","comments":[{"poster":"Bharat108","upvote_count":"3","timestamp":"1595845560.0","comment_id":"144845","content":"This will incur extra cost during each query fired on the table.","comments":[{"content":"The answer is A, beacause this is the only solution that don’t take extra effort when any employees leave or join the company","comment_id":"164392","comments":[{"poster":"g2000","content":"going forward, two columns are selected if full name is selected. BQ charges in proportional to the # of columns.. it's $$$ (not minimizing cost)","timestamp":"1611452100.0","comment_id":"274939","upvote_count":"1"}],"poster":"dragon123","upvote_count":"1","timestamp":"1598184960.0"}]}],"poster":"rickywck","upvote_count":"5","content":"Why not A? For all the other options at least storage cost will be increased."}],"choices":{"B":"Add a new column called FullName to the Users table. Run an UPDATE statement that updates the FullName column for each user with the concatenation of the FirstName and LastName values.","C":"Create a Google Cloud Dataflow job that queries BigQuery for the entire Users table, concatenates the FirstName value and LastName value for each user, and loads the proper values for FirstName, LastName, and FullName into a new table in BigQuery.","D":"Use BigQuery to export the data for the table to a CSV file. Create a Google Cloud Dataproc job to process the CSV file and output a new CSV file containing the proper values for FirstName, LastName and FullName. Run a BigQuery load job to load the new CSV file into BigQuery.","A":"Create a view in BigQuery that concatenates the FirstName and LastName field values to produce the FullName."},"exam_id":11,"isMC":true,"question_text":"You work for a large fast food restaurant chain with over 400,000 employees. You store employee information in Google BigQuery in a Users table consisting of a FirstName field and a LastName field. A member of IT is building an application and asks you to modify the schema and data in BigQuery so the application can query a FullName field consisting of the value of the FirstName field concatenated with a space, followed by the value of the LastName field for each employee. How can you make that data available while minimizing cost?"},{"id":"jKntfugAqkzd8rj5mbUy","question_images":[],"timestamp":"2022-09-03 13:26:00","discussion":[{"upvote_count":"7","content":"Selected Answer: A\nCorrect answer is A \nRead in reference : https://cloud.google.com/datastore/docs/concepts/indexes#index_limits\nn this case, you can circumvent the exploding index by manually configuring an index in your index configuration file:\nindexes:\n- kind: Task\n properties:\n - name: tags\n - name: created\n- kind: Task\n properties:\n - name: collaborators\n - name: created\nThis reduces the number of entries needed to only (|tags| * |created| + |collaborators| * |created|), or 6 entries instead of 9","comment_id":"668430","timestamp":"1710372840.0","poster":"Wasss123"},{"comment_id":"750783","timestamp":"1718875620.0","content":"Selected Answer: A\nyou can circumvent the exploding index by manually configuring an index in your index configuration file:\n\nhttps://cloud.google.com/datastore/docs/concepts/indexes#index_limits","upvote_count":"1","poster":"jkhong"},{"upvote_count":"3","timestamp":"1718832120.0","poster":"Krish6488","comment_id":"750314","content":"Selected Answer: D\nTempted to go with D as the syntax in Option A seems incorrect. D is still a possible answer because one of the ways to get rid of index errors is to remove the entities that are causing the index to explode. In this case its date_released and hence D appears right to me"},{"poster":"DGames","timestamp":"1718309520.0","comment_id":"744492","upvote_count":"3","content":"Selected Answer: A\nOption B & D reject because mention date_publised in question date_released is column\nOption C also not correct, I would go with option A."},{"content":"Selected Answer: D\nCorrect Answer D:\n\nThis is the way the DB is typically queried:\n- movies with actor=<actorname> ordered by date_released \n- movies with tag=Comedy ordered by date_released\n\nso it seems that we need indices in actor,tag and date_released for sorting. \n\n❌ A: this would be the correct answer, however, the format is incorrect, the correct format would be '- name: date_released' correctly indented.\n\n❌ B: This seems to be unnecessary, since typically actor and tag are not queried together. also, there is a clear indentation issue\n\n❌ C: We don't want to ignore actor and tag, we need those indices.\n\n✅ D: If we leave datastore to automatically create the indices and if we specify that the 'date_released' property needs to be excluded from indices, then we would have less indices (but maybe slower queries when ordering them, but hey, how many 'comedies' there could be in the world)","comment_id":"681862","poster":"Ender_H","timestamp":"1711647000.0","upvote_count":"3","comments":[{"upvote_count":"3","timestamp":"1711647240.0","poster":"Ender_H","comment_id":"681867","content":"And here is the correct way to configure indices:\nhttps://cloud.google.com/datastore/docs/tools/indexconfig\n\nso this would be the best answer:\nindexes:\n- kind: Movie\n properties:\n - name: actors\n - name: date_released\n direction: asc. <This could be left out, it defaults to direction: asc if excluded>\n\n- kind: Movie\n properties:\n - name: tag\n - name: date_released\n direction: asc. <This could be left out, it defaults to direction: asc if excluded>"},{"content":"*Findings for this answer*:\nIndices, if not defined, will be automatically created:\n\"By default, a Datastore mode database automatically predefines an index for each property of each entity kind. These single property indexes are suitable for simple types of queries.\"\nsource: https://cloud.google.com/datastore/docs/concepts/indexes\n\nIn the index limits section we see this:\n\"a Datastore mode database creates an entry in a predefined index for every property of every entity except those you have explicitly declared as excluded from your indexes.\"\nsource: https://cloud.google.com/datastore/docs/concepts/indexes#index_limits","upvote_count":"1","comment_id":"681863","timestamp":"1711647000.0","poster":"Ender_H"}]},{"timestamp":"1710780180.0","poster":"Hm92730","upvote_count":"1","comment_id":"672466","content":"What do people think about C? The question is asking how to avoid a combinatorial explosion in the number of indexes. It says \"You have entities with multiple properties, some of which can take on multiple values\". Put this with the below text from the documentation for Datastore indexes, it seems they're looking for \"exclude the properties that will cause combinatorial explosion\" which would be C.\n\n\"The situation becomes worse in the case of entities with multiple properties, each of which can take on multiple values. To accommodate such an entity, the index must include an entry for every possible combination of property values. Custom indexes that refer to multiple properties, each with multiple values, can \"explode\" combinatorially, requiring large numbers of entries for an entity with only a relatively small number of possible property values.\"[1]\n[1] https://cloud.google.com/datastore/docs/concepts/indexes#index_limits"},{"comments":[{"timestamp":"1710372960.0","content":"Correct answer is A \nIn the same reference you provided \nIn this case, you can circumvent the exploding index by manually configuring an index in your index configuration file:\nindexes:\n- kind: Task\nproperties:\n- name: tags\n- name: created\n- kind: Task\nproperties:\n- name: collaborators\n- name: created\nThis reduces the number of entries needed to only (|tags| * |created| + |collaborators| * |created|), or 6 entries instead of 9","poster":"Wasss123","comment_id":"668431","upvote_count":"1"}],"comment_id":"661122","timestamp":"1709729940.0","poster":"soichirokawa","content":"B. is correct\nTo avoid combinatoric explosion of indexes.\n\"Two queries of the same form but with different filter values use the same index.\"\nhttps://cloud.google.com/datastore/docs/concepts/indexes","upvote_count":"1"},{"timestamp":"1709472360.0","comment_id":"658383","upvote_count":"1","poster":"AWSandeep","content":"Selected Answer: A\nA. Manually configure the index in your index config as follows:"}],"answer":"A","answers_community":["A (67%)","D (33%)"],"url":"https://www.examtopics.com/discussions/google/view/79762-exam-professional-data-engineer-topic-1-question-44/","isMC":true,"answer_images":[],"unix_timestamp":1662204360,"answer_description":"","answer_ET":"A","topic":"1","question_text":"You are deploying a new storage system for your mobile application, which is a media streaming service. You decide the best fit is Google Cloud Datastore. You have entities with multiple properties, some of which can take on multiple values. For example, in the entity 'Movie' the property 'actors' and the property\n'tags' have multiple values but the property 'date released' does not. A typical query would ask for all movies with actor=<actorname> ordered by date_released or all movies with tag=Comedy ordered by date_released. How should you avoid a combinatorial explosion in the number of indexes?","question_id":259,"choices":{"D":"Set the following in your entity options: exclude_from_indexes = 'date_published'","B":"Manually configure the index in your index config as follows:","C":"Set the following in your entity options: exclude_from_indexes = 'actors, tags'","A":"Manually configure the index in your index config as follows:"},"exam_id":11},{"id":"dR9PZDgSxLd3BoaH6rJS","timestamp":"2020-03-21 07:34:00","question_images":[],"topic":"1","answer_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/google/view/17080-exam-professional-data-engineer-topic-1-question-45/","unix_timestamp":1584772440,"answer_ET":"C","choices":{"B":"Manually start the Cloud Dataflow job each morning when you get into the office.","A":"Change the processing job to use Google Cloud Dataproc instead.","C":"Create a cron job with Google App Engine Cron Service to run the Cloud Dataflow job.","D":"Configure the Cloud Dataflow job as a streaming job so that it processes the log data immediately."},"discussion":[{"content":"Answer: C","comment_id":"66416","upvote_count":"22","poster":"[Removed]","timestamp":"1600662840.0"},{"timestamp":"1601209320.0","poster":"[Removed]","content":"Answer: C\nDescription: Scheduler for adhoc jobs – 3 jobs free and $0.10 per job","upvote_count":"13","comment_id":"68613"},{"timestamp":"1737181380.0","poster":"grshankar9","comment_id":"1342433","content":"Selected Answer: C\nApp Engine Cron is limited to scheduling tasks within your App Engine application, whereas Cloud Scheduler can trigger actions on various Google Cloud services like Cloud Functions, Pub/Sub topics, or external HTTP endpoints.","upvote_count":"1"},{"upvote_count":"1","poster":"axantroff","timestamp":"1716363120.0","content":"Selected Answer: C\nService was renamed, but the answer is still - C","comment_id":"1077180"},{"timestamp":"1712544060.0","comment_id":"1027684","upvote_count":"1","poster":"imran79","content":"C. Using the Google App Engine Cron Service to run the Cloud Dataflow job allows you to automate the execution of the job. By creating a cron job, you can ensure that the Dataflow job is triggered exactly once per day at a specified time. This approach is automated, reliable, and fits the requirement of processing the log file once per day."},{"upvote_count":"5","content":"C. For a modern solution, Cloud Scheduler","comment_id":"948112","timestamp":"1704897600.0","poster":"itsmynickname"},{"timestamp":"1701306660.0","content":"Selected Answer: C\nCurrently, Cloud Scheduler takes over the scheduling functions.","poster":"Maurilio_Cardoso","upvote_count":"2","comment_id":"909773"},{"poster":"jin0","content":"I don't understand why that dataflow is used for processing? even though it should be processed once per a day?? is it more suitable for processing by using Dataproc instead?","timestamp":"1692700020.0","upvote_count":"2","comments":[{"upvote_count":"2","poster":"mark1223jkh","comment_id":"1212812","content":"Actually, google recommends Dataflow over Dataproc for both batch and streaming. Dataproc is only recommended if you are coming from hadoop, spark, ....","timestamp":"1731842100.0"}],"comment_id":"817810"},{"poster":"captainbu","content":"Selected Answer: C\nC was correct but nowadays you'd schedule a Dataflow job with Cloud Scheduler: https://cloud.google.com/community/tutorials/schedule-dataflow-jobs-with-cloud-scheduler","timestamp":"1688899920.0","upvote_count":"6","comment_id":"770374"},{"timestamp":"1680018900.0","poster":"Ender_H","content":"Selected Answer: C\nCorrect Answer: C.\n\n❌ A: Dataproc is a managed Apache Spark and Apache Hadoop service, makes no sense to use it\n\n ❌ B: This might sound as the cheapest, but is highly error prone, besides, anyone in charge of this has a salary and I doubt is a low one.\n\n✅ C: This is the easiest/fastest/cheapest way to trigger job runs, you can even set retry attempts.\nsource: https://cloud.google.com/appengine/docs/flexible/nodejs/scheduling-jobs-with-cron-yaml.\n\n❌ D: Setting this would be much more expensive than the cron-job","comment_id":"681890","upvote_count":"2"},{"upvote_count":"1","comment_id":"619274","poster":"noob_master","content":"Selected Answer: C\nAnswer: C","timestamp":"1671546840.0"},{"content":"Ans: C","upvote_count":"2","poster":"anji007","timestamp":"1650045180.0","comment_id":"462738"},{"comment_id":"461330","comments":[{"upvote_count":"3","timestamp":"1670341440.0","poster":"AmirN","comment_id":"612381","content":"Would you rather pay someone $100,000 a year to click 'run' on jobs all day, or have them automate it and do more cutting edge work? This would be opportunity cost."}],"timestamp":"1649814360.0","upvote_count":"3","content":"I know probably this question is testing on if you know corn.yaml and its function in App Engine. But why B will be more expensive? Human capital cost? Let's say if hiring a person click the button will be cheaper than launch an app engine, should we reconsider B?","poster":"Chelseajcole"},{"comment_id":"454304","content":"Scheduling Jobs with cron.yaml\n\nFree applications can have up to 20 scheduled tasks. Paid applications can have up to 250 scheduled tasks.","timestamp":"1648570860.0","upvote_count":"3","poster":"Chelseajcole"},{"content":"Vote for 'C'","timestamp":"1640617620.0","poster":"sumanshu","comment_id":"392072","upvote_count":"2"},{"comment_id":"285654","timestamp":"1628350080.0","content":"Correct C","poster":"naga","upvote_count":"3"},{"upvote_count":"5","timestamp":"1621432020.0","content":"Answer is C. https://cloud.google.com/appengine/docs/flexible/nodejs/scheduling-jobs-with-cron-yaml","poster":"Radhika7983","comment_id":"222897"},{"timestamp":"1613697600.0","upvote_count":"4","comment_id":"161148","poster":"haroldbenites","content":"C Correct"}],"question_text":"You work for a manufacturing plant that batches application log files together into a single log file once a day at 2:00 AM. You have written a Google Cloud\nDataflow job to process that log file. You need to make sure the log file in processed once per day as inexpensively as possible. What should you do?","answer":"C","question_id":260,"answer_description":"","exam_id":11,"answers_community":["C (100%)"]}],"exam":{"provider":"Google","isMCOnly":true,"isImplemented":true,"lastUpdated":"11 Apr 2025","numberOfQuestions":319,"name":"Professional Data Engineer","id":11,"isBeta":false},"currentPage":52},"__N_SSP":true}