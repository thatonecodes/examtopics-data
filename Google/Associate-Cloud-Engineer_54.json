{"pageProps":{"questions":[{"id":"fmDB7wOOgMGh2b5Rmwjs","question_id":266,"timestamp":"2020-06-03 21:22:00","discussion":[{"content":"D is the correct answer.\n\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/gpus","comments":[{"content":"the documentation states \"Limitations\nBefore using GPUs on GKE, keep in mind the following limitations:\n\nYou cannot add GPUs to existing node pools.\nGPU nodes cannot be live migrated during maintenance events.\"","comment_id":"152212","comments":[{"timestamp":"1639414200.0","comments":[{"content":"You're correct that D says that, except that the question also says to use the most cost-effective method. Two node-pools would be more expensive than rebuilding the current one with GPU enabled.","upvote_count":"1","poster":"fragment137","comments":[{"timestamp":"1704960000.0","poster":"Gulithor","comment_id":"772204","upvote_count":"1","content":"It also says to minimize effort, wouldn't recreating all the pools take way longer than just adding 1?"}],"comment_id":"730678","timestamp":"1701279240.0"}],"upvote_count":"16","poster":"nightflyer","content":"In this case it is about adding a GPU enabled node pool not a GPU to an existing node-pool","comment_id":"242770"}],"poster":"tablet444","upvote_count":"10","timestamp":"1628292720.0"}],"timestamp":"1622748120.0","comment_id":"101847","poster":"John_Iam","upvote_count":"47"},{"timestamp":"1633953780.0","poster":"glam","comment_id":"197666","upvote_count":"15","content":"D. Add a new, GPU-enabled, node pool to the GKE cluster. Ask your ML team to add the cloud.google.com/gke -accelerator: nvidia-tesla-p100 nodeSelector to their pod specification."},{"poster":"BAofBK","timestamp":"1730890860.0","comment_id":"1063754","upvote_count":"1","content":"The correct answer is D"},{"timestamp":"1719705300.0","poster":"trainingexam","upvote_count":"1","content":"Selected Answer: D\nD is the correct answer.","comment_id":"938640"},{"timestamp":"1713346140.0","upvote_count":"1","poster":"sabrinakloud","content":"Selected Answer: D\nOption D is good","comment_id":"872539"},{"content":"Selected Answer: D\nCreating new node pool w/ GPU-enabled instances is cost - saving solution. This way ML team workload will GPU instance for their ML and other team workload will run smoothly","poster":"sakdip66","upvote_count":"2","timestamp":"1712978280.0","comment_id":"869038"},{"content":"Selected Answer: D\nD makes more cost effective","upvote_count":"1","timestamp":"1711840080.0","poster":"Prat25200607","comment_id":"856276"},{"poster":"Buruguduystunstugudunstuy","comment_id":"818219","timestamp":"1708626480.0","content":"Selected Answer: D\nAnswer D. Add a new, GPU-enabled, node pool to the GKE cluster. Ask your ML team to add the cloud.google.com/gke-accelerator: nvidia-tesla-p100 nodeSelector to their pod specification.\n\nAdding a new node pool with GPUs is the best option because it allows for a separate set of nodes that can be specifically allocated to workloads that require GPU acceleration, such as the Machine Learning (ML) team's workloads. This approach will not affect other workloads running on the original nodes, keeping the costs low and the overall cluster performance stable.","upvote_count":"6"},{"timestamp":"1702010400.0","comment_id":"738622","content":"Selected Answer: D\nD is the correct answer.","upvote_count":"1","poster":"cslince"},{"upvote_count":"1","content":"D is correct","poster":"leogor","timestamp":"1699777200.0","comment_id":"716549"},{"timestamp":"1694860500.0","comment_id":"670697","content":"Selected Answer: D\nD is correct","upvote_count":"1","poster":"raghu09"},{"upvote_count":"2","timestamp":"1694694180.0","comment_id":"669001","poster":"iadarsh","content":"Selected Answer: D\nD is correct\nBecause if you create entirely new node pool then its not cost efficient and also the pods which not require that much high GPU is get scheduled into it. So instead of that add a new node pool with GPU and in the pod YAML file mention the node affinity to get scheduled into the GPU enabled node pool."},{"comment_id":"621157","upvote_count":"2","poster":"AzureDP900","timestamp":"1687540740.0","content":"By looking at all answers first 3 can be eliminated without any second thought. D is correct."},{"timestamp":"1685885280.0","content":"Go for D","comment_id":"611441","upvote_count":"1","poster":"haroldbenites"},{"poster":"LaxmanTiwari","upvote_count":"2","comment_id":"606169","timestamp":"1684855200.0","content":"Selected Answer: D\nIn this case it is about adding a GPU enabled node pool not a GPU to an existing node-pool"},{"content":"Selected Answer: D\nD is the correct answer","comment_id":"553151","timestamp":"1677006960.0","poster":"luciorifa","upvote_count":"1"},{"poster":"jaffarali","upvote_count":"3","timestamp":"1670911080.0","comment_id":"500399","content":"Selected Answer: D\nD would be the right option when there is possibility to add GPUs without recreating the nodes."},{"poster":"ME_MYSELF","timestamp":"1670821140.0","comments":[{"comment_id":"513733","poster":"ahsangh","upvote_count":"3","timestamp":"1672434900.0","content":"from past experience yes, haven't written this one yet, will revert after writing in a few days."}],"comment_id":"499788","upvote_count":"5","content":"I have a doubt. Will these questions repeat in ace exam or not??Please answer if u have written the exam...it will be very helpful"},{"comment_id":"496761","upvote_count":"1","content":"Selected Answer: D\nANSWER D","poster":"pnVino27","timestamp":"1670497980.0"},{"upvote_count":"1","timestamp":"1668946800.0","comment_id":"482513","content":"D. Add a new, GPU-enabled, node pool to the GKE cluster. Ask your ML team to add the cloud.google.com/gke -accelerator: nvidia-tesla-p100 nodeSelector to their pod specification.","poster":"vishnukumartr"},{"poster":"Litan","timestamp":"1664726400.0","comment_id":"456148","upvote_count":"1","content":"yes D right"},{"content":"D is a very wrong candidate but I would say B. because we also need to care take of cost .\nif we create a new node pool with GPU and don't delete exiting node pool it's increase pricing.\ni would go with B to recreate node pool with GPU as it's test env so I don't need to care about downtime.","comments":[{"timestamp":"1664213820.0","poster":"boof","upvote_count":"5","comment_id":"451963","content":"You are operating a Google Kubernetes Engine (GKE) cluster for your company where DIFFERENT TEAMS can run non-production workloads.\n---\nNote the different teams part. Recreating the entire GKE cluster when only the ML team needs access to the nvidia-tesla GPUs would be very disruptive and waste a lot of resources as it's redundant for the other teams. D would be the best pick in this case since it only marginally adds to the cluster instead of recreating the whole thing."}],"timestamp":"1653581820.0","comment_id":"367298","poster":"shankyomre01","upvote_count":"3"},{"comment_id":"356210","poster":"mcaromit","upvote_count":"1","content":"D is correct","timestamp":"1652434860.0"},{"upvote_count":"2","timestamp":"1648198260.0","content":"D is correct. Add a new, GPU-enabled, node pool to the GKE cluster. Ask your ML team to add the cloud.google.com/gke -accelerator: nvidia-tesla-p100 nodeSelector to their pod specification.","poster":"[Removed]","comment_id":"319969"},{"upvote_count":"1","content":"D & B can add gpu to this cluster ,but \"minimize effort\" so I pick D","poster":"pondai","timestamp":"1647914160.0","comment_id":"316781"},{"timestamp":"1645787820.0","comment_id":"298957","content":"D for sure","poster":"ARDY","upvote_count":"1"},{"timestamp":"1645583940.0","comment_id":"297106","content":"D - Add a new, GPU-enabled node pool to the GKE cluster. Ask your ML team to add the cloud.google.com/gke -accelerator: nvidia-tesla-p100 nodeSelector to their pod specification.","upvote_count":"2","poster":"GCP_Student1"},{"content":"Add a new, GPU-enabled, node pool to the GKE cluster. Ask your ML team to add the cloud.google.com/gke -accelerator: nvidia-tesla-p100 nodeSelector to their pod specification.","upvote_count":"1","poster":"DucSiu","timestamp":"1643683980.0","comment_id":"280944"},{"comment_id":"280015","content":"D is the correct answer","poster":"rvgcp","upvote_count":"1","timestamp":"1643558100.0"},{"upvote_count":"1","poster":"EABDAJA","content":"D is Correct","comment_id":"276663","timestamp":"1643179680.0"},{"content":"D is correct as nodepools are way to logically separate your workloads and machinetypes based on your organization team/project structure and requirements.","comment_id":"274533","upvote_count":"2","poster":"guid1984","timestamp":"1642937640.0"},{"poster":"victory108","timestamp":"1642935540.0","content":"D - Add a new, GPU-enabled, node pool to the GKE cluster. Ask your ML team to add the cloud.google.com/gke -accelerator: nvidia-tesla-p100 nodeSelector to their pod specification.","upvote_count":"1","comment_id":"274508"},{"timestamp":"1638872160.0","content":"can you pls verify once..to confirm whether D is correct.","upvote_count":"3","comment_id":"237205","poster":"Bhagirathi"},{"timestamp":"1637156160.0","content":"• D. Add a new, GPU-enabled, node pool to the GKE cluster. Ask your ML team to add the cloud.google.com/gke -accelerator: nvidia-tesla-p100 nodeSelector to their pod specification.","comment_id":"221094","poster":"swatititame","upvote_count":"1"},{"comment_id":"220461","poster":"ayj","upvote_count":"3","content":"I think D:\nWith GKE, you can create node pools equipped with NVIDIA Tesla® K80, P100, P4, V100, and T4 GPUs. \nYou cannot add GPUs to existing node pools. So creating a new one seems like the right answer. B is a bit meh as an answer I feel","timestamp":"1637080380.0"},{"upvote_count":"2","content":"I think D:\nWith GKE, you can create node pools equipped with NVIDIA Tesla® K80, P100, P4, V100, and T4 GPUs. \nYou cannot add GPUs to existing node pools. So creating a new one seems like the right answer. B is a bit meh as an answer I feel","timestamp":"1637080140.0","comment_id":"220460","poster":"ayj"},{"poster":"Anand2608","upvote_count":"1","content":"Option D is correct as we need to effort as well","timestamp":"1636460460.0","comment_id":"215910"},{"comment_id":"200865","timestamp":"1634356140.0","content":"B more cost effective???","poster":"Surjit24","upvote_count":"1"},{"comment_id":"196344","upvote_count":"1","content":"B is correct.\nPod templates contain a Pod specification which determines how each Pod should run, including which containers should be run within the Pods and which volumes the Pods should mount. Pod templates are used by controller objects, such as Deployments:\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/pod#pod-templates\nIt is HIGHLY UNLIKELY that a ML team will have knowledge of pod templates, pod specifications or deployments. Therefore options A and D are wrong.\nOption C requires the creation of a dedicated cluster for the ML team.\nOption B requires less effort and costs less (as per the question). So B is right.","poster":"JJ_ME","timestamp":"1633731900.0"},{"poster":"GopinathM","comment_id":"183061","upvote_count":"1","timestamp":"1632150480.0","content":"B is correct answer"},{"poster":"ESP_SAP","comments":[{"poster":"ESP_SAP","timestamp":"1629751980.0","comment_id":"164695","upvote_count":"7","content":"Correct Answer is (D): (Continuation)\nKey: nvidia.com/gpu\nEffect: NoSchedule\nNote: If a GPU node pool is added to a cluster where all the existing node pools are GPU node pools, or if you are creating a new cluster with a GPU attached default pool, the above taint will not be added to the GPU nodes. The taint will also not be added to the existing GPU nodes retrospectively when a non-GPU node pool is added afterwards.\nAdditionally, GKE automatically applies the corresponding tolerations to Pods requesting GPUs by running the ExtendedResourceToleration admission controller.\n\nThis causes only Pods requesting GPUs to be scheduled on GPU nodes, which enables more efficient autoscaling: your GPU nodes can quickly scale down if there are not enough Pods requesting GPUs.\n\nYou create a GPU node pool in an existing cluster using Cloud Console or the gcloud command-line tool."}],"timestamp":"1629751920.0","upvote_count":"6","comment_id":"164694","content":"Correct Answer is (D):\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/gpus\n\nRunning GPUs\nThe following sections explain how to run GPUs in GKE clusters.\n\nCreating an autoscaling GPU node pool\nTo take the best, most cost-effective advantage of GPUs on GKE, and to take advantage of cluster autoscaling,\nwe recommend creating separate GPU node pools in your clusters.\n\nWhen you add a GPU node pool to an existing cluster that already runs a non-GPU node pool, GKE automatically\ntaints the GPU nodes with the following node taint:"},{"upvote_count":"2","comment_id":"136259","timestamp":"1626418140.0","comments":[{"timestamp":"1626610980.0","comment_id":"137888","poster":"someoneinthecloud","upvote_count":"2","content":"Answer is D. B adds GPU based nodes which is very expensive and not necessary. D clearly states that a new node pool (and not a new cluster) is going to be added for the ML team to use and they will add the gpu specification in their yaml to request the GPU enabled pool","comments":[{"upvote_count":"1","poster":"SSPC","content":"I agree with you. The correct answer is \"D\"","timestamp":"1629462780.0","comment_id":"162223"}]},{"poster":"jilly","upvote_count":"2","comment_id":"137085","comments":[{"comment_id":"237808","content":"This will be a lot expensive as every non-production workload and various teams will trigger GPU based nodes which is waste of resources and cost. Best and easy solution is to add a new node-pool with GPU and add node-selector config in the yaml file.","poster":"alpharomeo9","upvote_count":"1","timestamp":"1638930660.0"}],"timestamp":"1626519540.0","content":"Answer is B as we cannot add node pools to existing...\nLimitations\nBefore using GPUs on GKE, keep in mind the following limitations:\n\nYou cannot add GPUs to existing node pools.\nGPU nodes cannot be live migrated during maintenance events."}],"content":"This is tricky. It seems to me that because of this statement: \"different teams can run non-production workloads\", B wins against D. Also, B is cheaper than D.","poster":"szakaria"},{"comment_id":"120460","timestamp":"1624705440.0","upvote_count":"4","content":"option D is right. \nBefore using GPUs on GKE, keep in mind the following limitations:\n\nYou cannot add GPUs to existing node pools.\nGPU nodes cannot be live migrated during maintenance events.","poster":"garyroks"},{"timestamp":"1624646400.0","content":"https://cloud.google.com/kubernetes-engine/docs/how-to/gpus","comment_id":"119713","poster":"ahmed812","upvote_count":"1"},{"poster":"cloudenthu01","timestamp":"1624643340.0","content":"D is Correct as it allows only ML team to have access to GPU enabled node pool which are expensive.\n\nB is incorrect as GKE cluster is shared by multiple teams & not only by ML team, recreating/updating the cluster with --max-accelerator can update the nodes with GPU's but this would be far too costly as not everybody (except ML team) needs GPU's for processing their workloads.","upvote_count":"4","comment_id":"119673"},{"upvote_count":"5","poster":"Ciumela","timestamp":"1624190220.0","content":"D is correct, B is expensive","comment_id":"114744"},{"upvote_count":"4","timestamp":"1623319980.0","content":"D is correct","comment_id":"106684","poster":"[Removed]"}],"unix_timestamp":1591212120,"answer_ET":"D","question_images":[],"choices":{"C":"Create your own Kubernetes cluster on top of Compute Engine with nodes that have GPUs. Dedicate this cluster to your ML team.","A":"Ask your ML team to add the ג€accelerator: gpuג€ annotation to their pod specification.","B":"Recreate all the nodes of the GKE cluster to enable GPUs on all of them.","D":"Add a new, GPU-enabled, node pool to the GKE cluster. Ask your ML team to add the cloud.google.com/gke -accelerator: nvidia-tesla-p100 nodeSelector to their pod specification."},"exam_id":1,"answer":"D","question_text":"You are operating a Google Kubernetes Engine (GKE) cluster for your company where different teams can run non-production workloads. Your Machine Learning\n(ML) team needs access to Nvidia Tesla P100 GPUs to train their models. You want to minimize effort and cost. What should you do?","answer_description":"","answer_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/google/view/22026-exam-associate-cloud-engineer-topic-1-question-81-discussion/","answers_community":["D (100%)"],"isMC":true},{"id":"LTQn9m2OxXPoz2oR5Der","question_id":267,"answer_images":[],"answer_ET":"A","isMC":true,"question_text":"Your VMs are running in a subnet that has a subnet mask of 255.255.255.240. The current subnet has no more free IP addresses and you require an additional\n10 IP addresses for new VMs. The existing and new VMs should all be able to reach each other without additional routes. What should you do?","answer":"A","url":"https://www.examtopics.com/discussions/google/view/22525-exam-associate-cloud-engineer-topic-1-question-82-discussion/","topic":"1","discussion":[{"comment_id":"104835","content":"A: Expand the existing subnet. \n https://cloud.google.com/sdk/gcloud/reference/compute/networks/subnets/expand-ip-range","poster":"JustLearning","timestamp":"1607380920.0","upvote_count":"56"},{"timestamp":"1618147380.0","upvote_count":"11","comment_id":"197715","content":"A. Use gcloud to expand the IP range of the current subnet.","poster":"glam"},{"poster":"JB28","timestamp":"1720572240.0","upvote_count":"2","comment_id":"1118076","content":"The correct answer is A. Use gcloud to expand the IP range of the current subnet.\n\nExpanding the primary IPv4 address range of a subnet does not cause a break or gap in network connectivity2. DHCP leases are not broken. IP addresses of running VMs at the time of the expansion do not change. You cannot “un-expand” or contract the range after it’s expanded. Expansion is permanent."},{"content":"The correct answer is A","comment_id":"1063757","upvote_count":"1","timestamp":"1714986300.0","poster":"BAofBK"},{"timestamp":"1709376960.0","poster":"Captain1212","content":"Selected Answer: A\nA is correct , as you need to expand it","comment_id":"996743","upvote_count":"2"},{"comment_id":"938643","upvote_count":"2","timestamp":"1703901300.0","poster":"trainingexam","content":"Selected Answer: A\ngcloud compute networks subnets expand-ip-range NAME --prefix-length=PREFIX_LENGTH [--region=REGION] [GCLOUD_WIDE_FLAG …]"},{"comments":[{"comment_id":"915214","timestamp":"1701771720.0","content":"that's exactly why you expand the subnet =D","poster":"KerolesKhalil","upvote_count":"8"}],"comment_id":"852346","upvote_count":"3","timestamp":"1695837180.0","poster":"dasgcp","content":"Selected Answer: C\nYou can't expand the subnet because the question states \"The current subnet has no more free IP addresses\", correct answer is C."},{"comment_id":"842589","content":"Selected Answer: A\nUse gcloud to expand the IP range of the current subnet.","poster":"Jimut","timestamp":"1695019020.0","upvote_count":"1"},{"comment_id":"825028","poster":"Mobin92","upvote_count":"1","timestamp":"1693232820.0","content":"Selected Answer: A\nA. Use gcloud to expand the IP range of the current subnet."},{"timestamp":"1692722580.0","poster":"Buruguduystunstugudunstuy","comment_id":"818236","upvote_count":"3","content":"Selected Answer: A\nAnswer A (Use gcloud to expand the IP range of the current subnet): This option is correct because it allows you to expand the primary IP range of the existing subnet to accommodate the additional 10 IP addresses required for the new VMs. This can be done without deleting or recreating the subnet, which saves time and avoids disrupting any existing resources that are using the subnet."},{"upvote_count":"3","poster":"Bobbybash","content":"Selected Answer: A\nA\nTo get an additional 10 IP addresses for new VMs running in a subnet that has no more free IP addresses and where existing and new VMs should be able to reach each other without additional routes, you should use gcloud to expand the IP range of the current subnet. This can be achieved by modifying the IP range of the subnet with the gcloud command-line tool. You can use the following command to increase the range of the subnet:\n\n\ngcloud compute networks subnets expand [SUBNET_NAME] --range=[NEW_IP_RANGE]\n\n\nMake sure to specify the name of the subnet you want to modify, and choose a new IP range that includes the current range and 10 additional IP addresses.","timestamp":"1691901000.0","comment_id":"807116"},{"timestamp":"1686396960.0","poster":"ChillinBoy","upvote_count":"1","comment_id":"741005","content":"Selected Answer: A\nA 100%"},{"poster":"cslince","content":"Selected Answer: A\nA: Expand the existing subnet.","upvote_count":"1","timestamp":"1686195180.0","comment_id":"738645"},{"upvote_count":"1","poster":"leogor","timestamp":"1683872520.0","content":"Selected Answer: A\nexpand the IP range of the current subnet","comment_id":"716553"},{"timestamp":"1682683920.0","poster":"dennydream","comment_id":"706433","upvote_count":"1","content":"Why in the world would C be the answer? A is def the answer."},{"timestamp":"1680722880.0","content":"A. is correct,\n\ngcloud compute networks subnets expand-ip-range - expand the IP range of a Compute Engine subnetwork\n\ngcloud compute networks subnets expand-ip-range NAME --prefix-length=PREFIX_LENGTH [--region=REGION] [GCLOUD_WIDE_FLAG …]","comment_id":"687178","upvote_count":"1","poster":"Charumathi"},{"timestamp":"1679881980.0","upvote_count":"1","comment_id":"680309","poster":"gcpBeginner","content":"correct answer is A. Use gcloud to expand the IP range of the current subnet."},{"content":"Selected Answer: A\nOption A is the answer","comment_id":"658284","upvote_count":"1","poster":"sandipk91","timestamp":"1677843480.0"},{"poster":"AzureDP900","comment_id":"621158","content":"A is right answer","timestamp":"1671823200.0","upvote_count":"1"},{"content":"Go for A","poster":"haroldbenites","comment_id":"611448","upvote_count":"1","timestamp":"1670169060.0"},{"content":"255.255.255.240 = /28 which has 8 Ips , if expand it will go to /29 which has 4 Ips so A is wrong and C is correct","comments":[{"content":"When you expand the ip range it should go down to /27 or lower not increase. You can't even reduce the IP range to /29","comment_id":"613104","upvote_count":"3","timestamp":"1670487900.0","poster":"mplibunao"}],"upvote_count":"2","timestamp":"1665805200.0","comment_id":"586142","poster":"Harish_AP"},{"timestamp":"1664976900.0","content":"Selected Answer: A\nhttps://cloud.google.com/sdk/gcloud/reference/compute/networks/subnets/expand-ip-range","poster":"[Removed]","upvote_count":"2","comment_id":"581281"},{"poster":"luciorifa","content":"Selected Answer: A\nA is correct, it's better and easier if you expand the IP range","comment_id":"553152","upvote_count":"2","timestamp":"1661102220.0"},{"upvote_count":"7","timestamp":"1659330060.0","poster":"ayushrockzzz","comment_id":"537698","content":"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA"},{"content":"Selected Answer: A\nA is the right option","poster":"jaffarali","timestamp":"1655458500.0","upvote_count":"1","comment_id":"503648"},{"comment_id":"496767","content":"Answer A","poster":"pnVino27","timestamp":"1654680300.0","upvote_count":"1"},{"content":"Selected Answer: A\nExpand","upvote_count":"1","comment_id":"494494","poster":"look1","timestamp":"1654439760.0"},{"upvote_count":"3","timestamp":"1654271580.0","content":"Selected Answer: A\nA. According to the subnet mask, there's still some bits left for sizing it up.","poster":"jabrrJ68w02ond1","comment_id":"493291"},{"content":"Selected Answer: A\nAAAAAAAAAAAAAAA","poster":"sid0127","timestamp":"1653366360.0","upvote_count":"1","comment_id":"485656"},{"content":"A. Use gcloud to expand the IP range of the current subnet.","poster":"vishnukumartr","timestamp":"1653042060.0","upvote_count":"1","comment_id":"482514"},{"content":"Selected Answer: A\nA is correct","poster":"alaahakim","timestamp":"1652980320.0","comment_id":"482024","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: A\nA is correct","poster":"TenshiD","timestamp":"1652901480.0","comment_id":"481049"},{"timestamp":"1648915320.0","comment_id":"456150","upvote_count":"1","content":"A seems correct","poster":"Litan"},{"upvote_count":"1","content":"Option C is irrelevant as i know \nwe can simply expand the subnet ip range so Option A suites better","timestamp":"1647836280.0","comment_id":"448585","poster":"ahnaa"},{"upvote_count":"1","content":"A is correct , expand the existing subnet . \nsubnets cat expand but can not shrink .","timestamp":"1641811620.0","comment_id":"403220","poster":"kashi123"},{"timestamp":"1636803720.0","comment_id":"356213","poster":"mcaromit","content":"A is corect","upvote_count":"1"},{"upvote_count":"1","timestamp":"1633448040.0","comment_id":"328812","content":"A: Expand the existing subnet","poster":"NARWAL"},{"content":"A is correct. Use gcloud to expand the IP range of the current subnet.","poster":"[Removed]","upvote_count":"2","timestamp":"1632552720.0","comment_id":"319970"},{"timestamp":"1631244480.0","poster":"GCP_Student1","content":"A. Use gcloud to expand the IP range of the current subnet.","upvote_count":"1","comment_id":"306873"},{"comment_id":"305923","poster":"EABDAJA","content":"A is correct.","upvote_count":"1","timestamp":"1631123820.0"},{"content":"its A, subnet can be expanded.","comment_id":"301394","upvote_count":"1","poster":"raheja21","timestamp":"1630498980.0"},{"timestamp":"1629883260.0","comment_id":"298961","poster":"ARDY","content":"it's A. \nExpanding subnet is making it from /28 to /26 or less. Not /30.","upvote_count":"3"},{"upvote_count":"1","content":"A. Use gcloud to expand the IP range of the current subnet.","poster":"DucSiu","comment_id":"280945","timestamp":"1627779240.0"},{"poster":"LearningGCP","upvote_count":"1","timestamp":"1627428720.0","content":"A: Expand the existing subnet.","comment_id":"278184"},{"timestamp":"1624101360.0","poster":"Bhagirathi","upvote_count":"2","comment_id":"247947","content":"200% A."},{"upvote_count":"3","comment_id":"240500","content":"The correct answer is A. I thought C was the right option but again this is GCP and not AWS.\nhttps://cloud.google.com/sdk/gcloud/reference/compute/networks/subnets/expand-ip-range","comments":[{"content":"I like your comment. It is GCP not AWS","timestamp":"1638425880.0","comment_id":"372351","upvote_count":"1","comments":[{"poster":"sanhoo","upvote_count":"1","comment_id":"372352","content":"and thanks for providing the link.","timestamp":"1638425940.0"}],"poster":"sanhoo"}],"poster":"Range2019","timestamp":"1623359940.0"},{"timestamp":"1621251540.0","upvote_count":"2","comment_id":"221097","poster":"swatititame","content":"• A. Use gcloud to expand the IP range of the current subnet."},{"poster":"nwk","content":"Vote A","timestamp":"1619599440.0","upvote_count":"2","comment_id":"207680"},{"poster":"rungcpnow","content":"You can increase to /24.. Create a vpc and add a subnet 192.168.10.0/28. Save it.. Again go back and expand it /24. It is possible. So Ans is A.","upvote_count":"3","timestamp":"1617698340.0","comment_id":"194148"},{"comment_id":"183062","comments":[{"upvote_count":"1","comment_id":"307231","timestamp":"1631276580.0","poster":"pentium2000","content":"If you go C, you must add VPC peering, and of course, you need to create the additional route. C is a wrong answer."},{"comments":[{"poster":"Eshkrkrkr","content":"Just ignore GopinathM in discussions :-)","comment_id":"216103","upvote_count":"8","timestamp":"1620573420.0"},{"comment_id":"277736","content":"Easy: gcloud compute networks subnets expand-ip-range","timestamp":"1627377780.0","upvote_count":"3","poster":"Perv"}],"timestamp":"1616505000.0","upvote_count":"1","comment_id":"185208","poster":"protn1221","content":"just how , we can just expand subnet ?!\ncan you explain .."}],"upvote_count":"1","timestamp":"1616260080.0","poster":"GopinathM","content":"C should be correct"},{"content":"its not A. Since the IP is already 255.255.255.40.","poster":"sapguru","upvote_count":"3","timestamp":"1613976300.0","comments":[{"timestamp":"1615100340.0","poster":"Ale1973","comments":[{"upvote_count":"1","timestamp":"1616292360.0","comment_id":"183338","content":"its a /28 we can increase to few more subnets /30","poster":"raj231"}],"comment_id":"174956","upvote_count":"3","content":"the subnet mask is 255.255.255.240"}],"comment_id":"163345"},{"content":"A is Correct - \"gcloud compute networks subnets expand-ip-range <SUBNET NAME> --region=<> --prefix-length=16\"","comment_id":"142244","poster":"samvegas","upvote_count":"5","timestamp":"1611434520.0"},{"comment_id":"114751","upvote_count":"9","content":"A is correct: The primary IP range for the subnet can be expanded, but not replaced or shrunk, after the subnet has been created","poster":"Ciumela","timestamp":"1608473040.0"},{"content":"You can expand an existing Subnet, but you cannot maker it smaller. So I would choose A.","upvote_count":"8","comment_id":"106107","timestamp":"1607538000.0","poster":"mlantonis"}],"exam_id":1,"unix_timestamp":1591562520,"answer_description":"","answers_community":["A (90%)","10%"],"question_images":[],"choices":{"C":"Create a new project. Use Shared VPC to share the current network with the new project.","B":"Delete the subnet, and recreate it using a wider range of IP addresses.","D":"Create a new subnet with the same starting IP but a wider range to overwrite the current subnet.","A":"Use gcloud to expand the IP range of the current subnet."},"timestamp":"2020-06-07 22:42:00"},{"id":"Uf7siTOuu30qVD9s1Mt1","discussion":[{"timestamp":"1623418680.0","comment_id":"107789","content":"B is correct","poster":"austinl","upvote_count":"30"},{"content":"B is correct: To actively adopt the Organization resource, the G Suite or Cloud Identity super admins need to assign the Organization Administrator Cloud IAM role to a user or group","timestamp":"1624191840.0","comment_id":"114769","upvote_count":"22","poster":"Ciumela"},{"poster":"BAofBK","content":"The correct answer is B","upvote_count":"1","timestamp":"1730891400.0","comment_id":"1063760"},{"poster":"Captain1212","comment_id":"996745","content":"Selected Answer: B\nB seems more correct as per thhe google pratcices","timestamp":"1725267420.0","upvote_count":"3"},{"content":"Selected Answer: B\nB is correct","timestamp":"1719705360.0","poster":"trainingexam","comment_id":"938645","upvote_count":"1"},{"timestamp":"1712116380.0","comment_id":"859524","poster":"zwwdplay","upvote_count":"1","content":"Dear friends,\nGreat responses in question.\nCan someone with contributor access, send me the remaining questions to this email: zwwdplay@hotmail.com"},{"content":"Selected Answer: B\nAnswer B. Grant them the required IAM roles using their G Suite email address.\n\nTo grant G Suite users access to a Cloud Platform project, you should use their G Suite email addresses to grant them the required IAM roles. \n\nAnswer A is incorrect because enabling Cloud Identity is not necessary for granting G Suite users access to a Cloud Platform project. Cloud Identity provides a centralized identity management system for G Suite and Cloud Platform, but it is not required for this use case.\n\nAnswer C is incorrect because there is no need to convert G Suite email addresses into Google Cloud Platform accounts. G Suite users already have Google accounts and can be granted access to Cloud Platform using their G Suite email addresses.\n\nAnswer D is incorrect because there is no default behavior in the Cloud Platform to grant access to users who are members of a particular group. Access to Cloud Platform resources is granted based on IAM roles and policies, not group membership.","poster":"Buruguduystunstugudunstuy","timestamp":"1708627620.0","comment_id":"818244","upvote_count":"9"},{"content":"Selected Answer: B\nGrant them the required IAM roles using their G Suite email address","upvote_count":"1","timestamp":"1699777500.0","comment_id":"716554","poster":"leogor"},{"upvote_count":"1","comment_id":"621159","timestamp":"1687540920.0","content":"B is right","poster":"AzureDP900"},{"content":"Go for B","poster":"haroldbenites","upvote_count":"1","timestamp":"1685886840.0","comment_id":"611449"},{"comment_id":"550215","poster":"Majkl93","content":"Selected Answer: B\nB as per the comments","timestamp":"1676723880.0","upvote_count":"2"},{"upvote_count":"2","timestamp":"1668946920.0","poster":"vishnukumartr","comment_id":"482515","content":"B. Grant them the required IAM roles using their G Suite email address."},{"content":"B is Correct","upvote_count":"1","timestamp":"1668885180.0","poster":"alaahakim","comment_id":"482025"},{"timestamp":"1665481440.0","upvote_count":"2","poster":"bubblegumbeach","comment_id":"460499","content":"B is correct"},{"content":"B is correct","timestamp":"1652435040.0","upvote_count":"2","poster":"mcaromit","comment_id":"356214"},{"content":"B. Grant them the required IAM roles using their G Suite email address.","poster":"Christiank","upvote_count":"2","comment_id":"321961","timestamp":"1648390440.0"},{"timestamp":"1648198380.0","poster":"[Removed]","comment_id":"319973","content":"B is correct. Grant them the required IAM roles using their G Suite email address.","upvote_count":"3"},{"timestamp":"1645627200.0","upvote_count":"1","poster":"GCP_Student1","comment_id":"297443","content":"B. Grant them the required IAM roles using their G Suite email address."},{"poster":"victory108","content":"B - Grant them the required IAM roles using their G Suite email address.","upvote_count":"1","comment_id":"274506","timestamp":"1642935360.0"},{"content":"• A. Use gcloud to expand the IP range of the current subnet.","upvote_count":"3","timestamp":"1637156520.0","comments":[{"content":"this is for previous question :)","comment_id":"293790","upvote_count":"5","timestamp":"1645224600.0","poster":"TAvenger"}],"comment_id":"221103","poster":"swatititame"},{"timestamp":"1635411000.0","comment_id":"207691","upvote_count":"4","content":"Should be B\nDefault behavior does not grant access to the \"your GCP Project\"\nDefault behavior allow only create billing account and project - When the organization is created, all users in your domain are automatically granted Project Creator and Billing Account Creator IAM roles at the organization level. This enables users in your domain to continue creating projects with no disruption.","poster":"nwk"},{"content":"B. Grant them the required IAM roles using their G Suite email address.","upvote_count":"6","comment_id":"197751","poster":"glam","timestamp":"1633961520.0"},{"comments":[{"content":"Take back B is correct. There are a lot of uncertain words in the option.","comment_id":"183767","timestamp":"1632236280.0","upvote_count":"1","poster":"[Removed]"}],"content":"D is better with best practice.","poster":"[Removed]","comment_id":"183066","upvote_count":"1","timestamp":"1632150600.0"}],"question_id":268,"timestamp":"2020-06-11 15:38:00","unix_timestamp":1591882680,"answer_description":"","question_text":"Your organization uses G Suite for communication and collaboration. All users in your organization have a G Suite account. You want to grant some G Suite users access to your Cloud Platform project. What should you do?","answer":"B","topic":"1","answer_ET":"B","exam_id":1,"answers_community":["B (100%)"],"question_images":[],"url":"https://www.examtopics.com/discussions/google/view/22857-exam-associate-cloud-engineer-topic-1-question-83-discussion/","isMC":true,"answer_images":[],"choices":{"B":"Grant them the required IAM roles using their G Suite email address.","D":"In the G Suite console, add the users to a special group called cloud-console-users@yourdomain.com. Rely on the default behavior of the Cloud Platform to grant users access if they are members of this group.","C":"Create a CSV sheet with all users' email addresses. Use the gcloud command line tool to convert them into Google Cloud Platform accounts.","A":"Enable Cloud Identity in the GCP Console for your domain."}},{"id":"HLWgRDXCfzVB4ljCXxRl","unix_timestamp":1593107700,"answers_community":["A (100%)"],"isMC":true,"timestamp":"2020-06-25 19:55:00","question_images":[],"url":"https://www.examtopics.com/discussions/google/view/24058-exam-associate-cloud-engineer-topic-1-question-84-discussion/","question_id":269,"answer_ET":"A","discussion":[{"poster":"cloudenthu01","timestamp":"1624643700.0","comment_id":"119678","content":"A is correct","upvote_count":"38"},{"timestamp":"1633961700.0","content":"A. Create two configurations using gcloud config. Write a script that sets configurations as active, individually. For each configuration, use gcloud compute instances list to get a list of compute resources.","comment_id":"197752","upvote_count":"15","poster":"glam"},{"content":"A is the best","comment_id":"1063763","timestamp":"1730891580.0","poster":"BAofBK","upvote_count":"1"},{"timestamp":"1725268080.0","poster":"Captain1212","content":"Selected Answer: A\nA is correct , first list , then acitvate it","comment_id":"996751","upvote_count":"3"},{"poster":"Neha_Pallavi","upvote_count":"1","content":"A. Create two configurations using gcloud config. Write a script that sets configurations as active, individually. For each configuration, use gcloud compute instances list to get a list of compute resources.","timestamp":"1724587920.0","comment_id":"990039"},{"content":"Selected Answer: A\nActivate each config and list the instances","comment_id":"938646","timestamp":"1719705480.0","poster":"trainingexam","upvote_count":"1"},{"upvote_count":"1","comment_id":"848194","poster":"Partha117","timestamp":"1711197240.0","content":"Selected Answer: A\nA is correct"},{"poster":"Buruguduystunstugudunstuy","content":"Selected Answer: A\nAnswer A is the correct answer.\n\nThe most straightforward way to list all compute instances in development and production projects is to use gcloud compute instances list command. However, since the account has access to both production and development projects, it's necessary to create two configurations with different project IDs. \n\nAnswer B is incorrect because gsutil is used for object storage operations and not compute instances. (DISTRACTOR)\n\nAnswers C and D are incorrect because they do not provide a straightforward solution for listing compute instances in multiple projects.","timestamp":"1708628160.0","comment_id":"818257","upvote_count":"10"},{"upvote_count":"1","poster":"Shwom","content":"Selected Answer: A\nA is correct","comment_id":"796493","timestamp":"1706913300.0"},{"comment_id":"738675","poster":"cslince","timestamp":"1702016100.0","upvote_count":"1","content":"Selected Answer: A\nA is correct"},{"poster":"leogor","upvote_count":"1","comment_id":"716558","timestamp":"1699777680.0","content":"Selected Answer: A\ngcloud instead of gsutil"},{"comment_id":"669880","comments":[{"poster":"Shlok27_gcloud","timestamp":"1695186000.0","content":"Which one is correct in all cases , suggested one or community one . Im confused totally for all questions","comment_id":"673804","upvote_count":"2"}],"timestamp":"1694778600.0","poster":"[Removed]","content":"Selected Answer: A\n\"gcloud\" can create and manage Google Cloud resources while \"gsutil\" cannot do so. \"gsutil\" can manipulate buckets, bucket's objects and bucket ACLs on GCS(Google Cloud Storage) while \"gcloud\" cannot do so","upvote_count":"3"},{"comment_id":"644318","timestamp":"1691549580.0","upvote_count":"1","content":"A. Gsutil is used for cloud storage bucket","poster":"habros"},{"poster":"AzureDP900","content":"A is right, This is part of Tutorial Dojo practice questions","timestamp":"1687541760.0","comment_id":"621168","upvote_count":"4"},{"poster":"haroldbenites","content":"Go for A","comment_id":"611459","upvote_count":"1","timestamp":"1685887680.0"},{"timestamp":"1681143540.0","content":"Selected Answer: A\nObviously A is correct","poster":"cysteine","upvote_count":"1","comment_id":"583789"},{"upvote_count":"1","poster":"RegisFTM","content":"This looks to be a multiple-choice question. The answer A is correct, and the C completes the task... Does it make sense?","timestamp":"1671665820.0","comments":[{"content":"C is not automated task \n\nA is correct","poster":"Zufair","comment_id":"512441","upvote_count":"3","timestamp":"1672332240.0"}],"comment_id":"506508"},{"upvote_count":"2","poster":"vishnukumartr","content":"A. Create two configurations using gcloud config. Write a script that sets configurations as active, individually. For each configuration, use gcloud compute instances list to get a list of compute resources.","timestamp":"1668947040.0","comment_id":"482519"},{"poster":"alaahakim","upvote_count":"1","comment_id":"482026","timestamp":"1668885300.0","content":"A is Correct"},{"content":"A. Create two configurations using gcloud config. Write a script that sets configurations as active, individually. For each configuration, use gcloud compute instances list to get a list of compute resources.","poster":"shawnkkk","timestamp":"1668870720.0","comment_id":"481857","upvote_count":"1"},{"comment_id":"356219","timestamp":"1652435160.0","poster":"mcaromit","content":"A is correct","upvote_count":"3"},{"timestamp":"1648198440.0","upvote_count":"4","comment_id":"319974","content":"A is correct. Create two configurations using gcloud config. Write a script that sets configurations as active, individually. For each configuration, use gcloud compute instances list to get a list of compute resources.","poster":"[Removed]"},{"comment_id":"297447","timestamp":"1645627500.0","content":"A. Create two configurations using gcloud config. Write a script that sets configurations as active, individually. For each configuration, use gcloud compute instances list to get a list of compute resources.","poster":"GCP_Student1","upvote_count":"4"},{"comment_id":"282767","content":"A is the correct answer","timestamp":"1643897040.0","upvote_count":"2","poster":"GCP_Student1"},{"timestamp":"1643684220.0","content":"A. Create two configurations using gcloud config. Write a script that sets configurations as active, individually. For each configuration, use gcloud compute instances list to get a list of compute resources.","poster":"DucSiu","comment_id":"280947","upvote_count":"2"},{"upvote_count":"1","timestamp":"1642590780.0","comment_id":"271161","content":"Answer A","poster":"EABDAJA"},{"comment_id":"247951","poster":"Bhagirathi","content":"200% A.","upvote_count":"5","timestamp":"1639920060.0"},{"content":"• A. Create two configurations using gcloud config. Write a script that sets configurations as active, individually. For each configuration, use gcloud compute instances list to get a list of compute resources.","upvote_count":"1","poster":"swatititame","comment_id":"221110","timestamp":"1637156940.0"},{"comment_id":"165767","timestamp":"1629874920.0","content":"since gsutil is for storage, B is definitely wrong.","upvote_count":"8","poster":"stepkurniawan"},{"upvote_count":"13","timestamp":"1629211920.0","comment_id":"160117","poster":"ESP_SAP","content":"Correct Answer is (A):\n\ngcloud compute instances list - list Google Compute Engine instances\ngcloud compute instances list displays all Google Compute Engine instances in a project.\nBy default, instances from all zones are listed. The results can be narrowed down using a filter: --filter=\"zone:( ZONE … )\".\n\nhttps://cloud.google.com/sdk/gcloud/reference/compute/instances/list"}],"exam_id":1,"choices":{"B":"Create two configurations using gsutil config. Write a script that sets configurations as active, individually. For each configuration, use gsutil compute instances list to get a list of compute resources.","C":"Go to Cloud Shell and export this information to Cloud Storage on a daily basis.","D":"Go to GCP Console and export this information to Cloud SQL on a daily basis.","A":"Create two configurations using gcloud config. Write a script that sets configurations as active, individually. For each configuration, use gcloud compute instances list to get a list of compute resources."},"topic":"1","answer_images":[],"question_text":"You have a Google Cloud Platform account with access to both production and development projects. You need to create an automated process to list all compute instances in development and production projects on a daily basis. What should you do?","answer_description":"","answer":"A"},{"id":"GnkEThO82bUzXrnGcXpC","question_images":[],"unix_timestamp":1591893720,"timestamp":"2020-06-11 18:42:00","answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/22875-exam-associate-cloud-engineer-topic-1-question-85-discussion/","exam_id":1,"answers_community":["C (100%)"],"topic":"1","discussion":[{"upvote_count":"154","poster":"mohdafiuddin","comment_id":"254178","timestamp":"1624889760.0","content":"Breaking down the question into key points - \n\n\n1. 5-TB AVRO file stored in a Cloud Storage bucket.\n2. Analysts are proficient only in SQL \n3. cost-effective way to complete their request as soon as possible\n\n\nA. ....Load data in Cloud Datastore... (Not Correct because Cloud Datastore is not a good option to run SQL Queries)\n\nB. ...Load data in BigQuery.... (Not Cost Effective because loading the data which is already present in the bucket into BigQuery again is expensive)\n\nC. Create external tables in BigQuery that point to Cloud Storage buckets and run a SQL query on these external tables to complete your request.\n(This is the right answer as it meets all the requirements from the question)\n\nD. Create a Hadoop cluster and copy the AVRO file to NDFS by compressing it. Load the file in a hive table and provide access to your analysts so that they can run SQL queries.\n(Too roundabout and indirect. Not the right option)","comments":[{"comment_id":"316785","content":"listem this guy","poster":"pondai","upvote_count":"17","timestamp":"1632269880.0"}]},{"content":"C is correct: https://cloud.google.com/bigquery/external-data-sources","timestamp":"1608535740.0","comment_id":"115244","poster":"Ciumela","upvote_count":"23"},{"upvote_count":"1","content":"D - The ONLY answer that provides data for the analysts is D. The question states that the analysts need access to the data and that they only know SQL (not that *you* only know SQL). The other 3 answers don't provide the data to analysts. You might have to fill in the blanks that you will pass it to them in a spreadsheet format, but that very likely won't satisfy their needs to query the data using SQL and at 5TB size, that isn't ideal. Therefore D is the only answer that satisfies the requirement.","comment_id":"1194521","timestamp":"1728764100.0","poster":"dck4893"},{"timestamp":"1720572600.0","content":"The most cost-effective and efficient option would be Option C: Create external tables in BigQuery that point to Cloud Storage buckets and run a SQL query on these external tables to complete your request.\n\nThis approach allows you to query data directly from the AVRO file stored in the Cloud Storage bucket without having to load the data into BigQuery first. This saves both time and money as you are not charged for the storage of data within BigQuery. Plus, BigQuery is designed to be able to handle large datasets, making it a suitable choice for a 5-TB AVRO file. Your analysts, who are proficient in SQL, can easily work with BigQuery as it uses a SQL interface.","upvote_count":"1","poster":"JB28","comment_id":"1118080"},{"upvote_count":"1","content":"The correct answer is C","poster":"BAofBK","timestamp":"1714986960.0","comment_id":"1063767"},{"content":"Selected Answer: C\nC is the right answer , as","comment_id":"996753","upvote_count":"1","poster":"Captain1212","timestamp":"1709377920.0"},{"comment_id":"990037","content":"C. Create external tables in BigQuery that point to Cloud Storage buckets and run a SQL query on these external tables to complete your request.","poster":"Neha_Pallavi","upvote_count":"2","timestamp":"1708870080.0"},{"content":"C. Create external tables in BigQuery that point to Cloud Storage buckets and run a SQL query on these external tables to complete your request.","upvote_count":"1","poster":"Neha_Pallavi","timestamp":"1708869660.0","comment_id":"990033"},{"timestamp":"1692723720.0","poster":"Buruguduystunstugudunstuy","comment_id":"818264","upvote_count":"7","content":"Selected Answer: C\nAnswer C is the most cost-effective and efficient way to provide analysts access to the data stored in the 5-TB AVRO file in Cloud Storage. \n\nHere's why:\n\nYou can create external tables in BigQuery that point to the 5-TB AVRO file stored in Cloud Storage. External tables allow you to query data stored in Cloud Storage without the need to load the data into BigQuery. This is a cost-effective way to provide your analysts' access to the data they need, and it is also an efficient solution since you can run SQL queries on the data directly in BigQuery."},{"content":"External tables in BigQuery","comment_id":"763605","poster":"Emmanski08","upvote_count":"1","timestamp":"1688280840.0"},{"timestamp":"1686197880.0","comment_id":"738681","upvote_count":"1","poster":"cslince","content":"Selected Answer: C\nC is correct: https://cloud.google.com/bigquery/external-data-sources"},{"poster":"leogor","content":"Selected Answer: C\nexternal tables in BigQuery","comment_id":"716586","timestamp":"1683875160.0","upvote_count":"1"},{"comment_id":"701538","upvote_count":"1","poster":"Untamables","content":"Selected Answer: C\nSimilar to Athena","timestamp":"1682168460.0"},{"comment_id":"687182","poster":"Charumathi","upvote_count":"2","content":"C. is correct,\nAn external data source is a data source that you can query directly from BigQuery, even though the data is not stored in BigQuery storage.\n\nBigQuery supports the following external data sources:\n\nAmazon S3\nAzure Storage\nCloud Bigtable\nCloud Spanner\nCloud SQL\nCloud Storage\nDrive","timestamp":"1680724140.0"},{"upvote_count":"1","content":"answer is c","timestamp":"1678624140.0","poster":"fifi1907","comment_id":"666825"},{"comment_id":"621170","upvote_count":"1","timestamp":"1671824400.0","content":"mohdafiuddin explanation is very detailed .. C is right","poster":"AzureDP900"},{"content":"Go for C","comment_id":"611460","poster":"haroldbenites","upvote_count":"2","timestamp":"1670170200.0"},{"upvote_count":"3","timestamp":"1665651180.0","comment_id":"585108","poster":"JelloMan","content":"Selected Answer: C\n@mohdafuiddin covered it exactly but heres more info:\nhttps://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro"},{"content":"Selected Answer: C\nC is the correct answer","comment_id":"553159","timestamp":"1661102460.0","upvote_count":"2","poster":"luciorifa"},{"content":"C is good. https://cloud.google.com/bigquery/external-data-cloud-storage","timestamp":"1654387440.0","poster":"ericyev","comment_id":"494039","upvote_count":"1"},{"timestamp":"1653042360.0","upvote_count":"1","content":"C. Create external tables in BigQuery that point to Cloud Storage buckets and run a SQL query on these external tables to complete your request.","poster":"vishnukumartr","comment_id":"482520"},{"comment_id":"482040","upvote_count":"1","poster":"alaahakim","timestamp":"1652981760.0","content":"Selected Answer: C\nThe Correct Ans: C"},{"content":"C. Create external tables in BigQuery that point to Cloud Storage buckets and run a SQL query on these external tables to complete your request.","comment_id":"481860","poster":"shawnkkk","timestamp":"1652966100.0","upvote_count":"1"},{"upvote_count":"3","comment_id":"356222","content":"C is the most cost effective option","timestamp":"1636804140.0","poster":"mcaromit"},{"poster":"mistryminded","upvote_count":"2","comment_id":"344237","content":"C seems to be correct https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro#avro_schemas","timestamp":"1635371400.0"},{"timestamp":"1632555480.0","upvote_count":"2","comment_id":"320001","content":"C is correct. Create external tables in BigQuery that point to Cloud Storage buckets and run a SQL query on these external tables to complete your request.","poster":"[Removed]"},{"content":"has anyone actually tried this? I don't know how to have a table point to a bucket without actually loading the data into the table.","upvote_count":"3","comment_id":"313429","timestamp":"1631887860.0","poster":"drizzydroo"},{"upvote_count":"2","content":"C. Create external tables in BigQuery that point to Cloud Storage buckets and run a SQL query on these external tables to complete your request.","comment_id":"297449","timestamp":"1629722940.0","poster":"GCP_Student1"},{"timestamp":"1621252380.0","poster":"swatititame","content":"• C. Create external tables in BigQuery that point to Cloud Storage buckets and run a SQL query on these external tables to complete your request.","upvote_count":"3","comment_id":"221113"},{"poster":"nwk","content":"(C) - An external data source (also known as a federated data source) is a data source that you can query directly even though the data is not stored in BigQuery. Instead of loading or streaming the data, you create a table that references the external data source.","timestamp":"1619599920.0","upvote_count":"5","comment_id":"207697"},{"timestamp":"1618197300.0","comment_id":"198111","poster":"glam","content":"C. Create external tables in BigQuery that point to Cloud Storage buckets and run a SQL query on these external tables to complete your request.","upvote_count":"6"},{"poster":"stepkurniawan","comment_id":"165769","content":"B will be too expensive to load all the 5 TB data to BigQuery, and then dropping it again directly.","upvote_count":"3","timestamp":"1614244020.0"},{"comment_id":"107939","content":"C is correct","timestamp":"1607712120.0","poster":"[Removed]","upvote_count":"8"}],"isMC":true,"answer_ET":"C","question_id":270,"answer":"C","choices":{"D":"Create a Hadoop cluster and copy the AVRO file to NDFS by compressing it. Load the file in a hive table and provide access to your analysts so that they can run SQL queries.","C":"Create external tables in BigQuery that point to Cloud Storage buckets and run a SQL query on these external tables to complete your request.","A":"Load data in Cloud Datastore and run a SQL query against it.","B":"Create a BigQuery table and load data in BigQuery. Run a SQL query on this table and drop this table after you complete your request."},"answer_description":"","question_text":"You have a large 5-TB AVRO file stored in a Cloud Storage bucket. Your analysts are proficient only in SQL and need access to the data stored in this file. You want to find a cost-effective way to complete their request as soon as possible. What should you do?"}],"exam":{"isMCOnly":true,"lastUpdated":"11 Apr 2025","numberOfQuestions":285,"id":1,"isBeta":false,"isImplemented":true,"provider":"Google","name":"Associate Cloud Engineer"},"currentPage":54},"__N_SSP":true}