{"pageProps":{"questions":[{"id":"G9VNLu5s0qe5nHYfFVV7","exam_id":11,"topic":"1","url":"https://www.examtopics.com/discussions/google/view/17105-exam-professional-data-engineer-topic-1-question-64/","answers_community":["C (100%)"],"answer_ET":"C","answer":"C","discussion":[{"comment_id":"66540","content":"Correct: C","upvote_count":"27","timestamp":"1632227160.0","poster":"[Removed]"},{"poster":"[Removed]","content":"Answer: C\nDescription: Service Account are best option when granting access from tools/appllications","comment_id":"68725","timestamp":"1632791760.0","upvote_count":"16"},{"content":"C. Create a service account and grant dataset access to that account. Use the service account's private key to access the dataset.\n\nCreating a service account and granting dataset access to that account is the most secure way to access BigQuery from an IT application. Service accounts are designed for use in automated systems and do not require user interaction, eliminating the need for individual users to authenticate to BigQuery. Additionally, by using the private key of the service account to access the dataset, you can ensure that the authentication process is secure and that only authorized users have access to the data.","timestamp":"1721696940.0","upvote_count":"4","comment_id":"784914","comments":[{"comment_id":"784915","poster":"samdhimal","content":"Option A: Create groups for your users and give those groups access to the dataset, is not the best option because it still requires users to authenticate to BigQuery\n\nOption B: Integrate with a single sign-on (SSO) platform, and pass each user's credentials along with the query request is not the best option because it still requires users to authenticate to BigQuery.\n\nOption D: Create a dummy user and grant dataset access to that user. Store the username and password for that user in a file on the files system, and use those credentials to access the BigQuery dataset is not a secure option because it involves storing sensitive information in a file on the file system, which can be easily accessed by unauthorized users.","upvote_count":"4","timestamp":"1721697000.0"}],"poster":"samdhimal"},{"poster":"DGames","upvote_count":"1","comment_id":"745527","content":"Selected Answer: C\nService account approach is secure in GCP to communicate with between service or application","timestamp":"1718401020.0"},{"comment_id":"712328","content":"Selected Answer: C\n[C]\nThe reason in https://cloud.google.com/bigquery/docs/data-governance#identity\n\"Users of BigQuery might be humans, but they might also be nonhuman applications that communicate using a BigQuery client library or the REST API. These applications should identify themselves using a service account, the special type of Google identity intended to represent a nonhuman user.\"","poster":"NicolasN","upvote_count":"1","timestamp":"1714995480.0"},{"upvote_count":"2","comment_id":"548761","timestamp":"1692196140.0","content":"Selected Answer: C\nsuch kind of questions are always service account oriented. and sa can be used as a user ..not just for machine2machine\n\nusers may or may not enter their credentials app login window. that's not main point by the way","poster":"Tanzu"},{"content":"Selected Answer: C\nCorrect: C","poster":"rcruz","timestamp":"1690990500.0","comment_id":"539030","upvote_count":"2"},{"comment_id":"463636","timestamp":"1681751160.0","upvote_count":"2","content":"Ans: C","poster":"anji007"},{"timestamp":"1662844740.0","poster":"daghayeghi","content":"C:\nIt says \"do not want individual users to authenticate to BigQuery and you do not want to give them access to the dataset\", then C is the best choice.","comment_id":"307490","upvote_count":"3"},{"comment_id":"215817","content":"Granting access to the app through a service account would mean all of the users that access the app have access to the BQ. Question was to filter it out, so I believe each user would have to be added to a group that does or doesn't have access to the dataset.","upvote_count":"4","comments":[{"upvote_count":"1","content":"The answer is C. \nWhen access data through application, Google recommendation is using service account.","comment_id":"464332","timestamp":"1681859700.0","poster":"squishy_fishy"},{"comment_id":"220877","poster":"kavs","content":"Yes A seems to be right","comments":[{"poster":"kavs","upvote_count":"3","content":"It says individually don't want to authorise service account could be right too","timestamp":"1652765160.0","comment_id":"220881"}],"timestamp":"1652764920.0","upvote_count":"4"}],"timestamp":"1652082060.0","poster":"ave4000"},{"comment_id":"190017","poster":"Ankush_j","content":"Ans is C, Service account is best for secure data","upvote_count":"3","timestamp":"1648608360.0"},{"comment_id":"161512","content":"C is correct","timestamp":"1645283280.0","poster":"haroldbenites","upvote_count":"3"},{"upvote_count":"4","timestamp":"1641391200.0","content":"Correct C.","comment_id":"126799","poster":"Rajuuu"}],"answer_images":[],"question_text":"You are integrating one of your internal IT applications and Google BigQuery, so users can query BigQuery from the application's interface. You do not want individual users to authenticate to BigQuery and you do not want to give them access to the dataset. You need to securely access BigQuery from your IT application. What should you do?","choices":{"B":"Integrate with a single sign-on (SSO) platform, and pass each user's credentials along with the query request","D":"Create a dummy user and grant dataset access to that user. Store the username and password for that user in a file on the files system, and use those credentials to access the BigQuery dataset","A":"Create groups for your users and give those groups access to the dataset","C":"Create a service account and grant dataset access to that account. Use the service account's private key to access the dataset"},"answer_description":"","unix_timestamp":1584800760,"timestamp":"2020-03-21 15:26:00","question_id":281,"question_images":[],"isMC":true},{"id":"H6wAW58CSedUd9VVmw4t","question_text":"You are building a data pipeline on Google Cloud. You need to prepare data using a casual method for a machine-learning process. You want to support a logistic regression model. You also need to monitor and adjust for null values, which must remain real-valued and cannot be removed. What should you do?","answer_ET":"B","timestamp":"2020-03-13 13:45:00","isMC":true,"answers_community":["B (83%)","D (17%)"],"question_id":282,"answer":"B","url":"https://www.examtopics.com/discussions/google/view/16476-exam-professional-data-engineer-topic-1-question-65/","exam_id":11,"answer_description":"","unix_timestamp":1584103500,"discussion":[{"timestamp":"1584369060.0","content":"real-valued can not be null N/A or empty, have to be “0”, so it has to be B.","comment_id":"64738","poster":"jvg637","upvote_count":"40"},{"poster":"[Removed]","content":"Should be B","comment_id":"66548","upvote_count":"16","timestamp":"1584802260.0"},{"content":"Selected Answer: B\nUsually, None values are converted to 0 in data cleaning and preparation process. The key point here is, we don't require any other tool than DataPrep to identify and modify the value","comment_id":"1410086","poster":"monyu","upvote_count":"1","timestamp":"1742916060.0"},{"timestamp":"1742050560.0","content":"Selected Answer: D\nCloud Dataflow is ideal for scalable data processing and allows for real-time transformations.\nLogistic regression requires numerical (real-valued) inputs, and null values cannot remain as they are.","poster":"Parandhaman_Margan","upvote_count":"1","comment_id":"1398897"},{"content":"Selected Answer: D\nOption D:\nUsing null value conversion to 0 is the most correct practice for this case. Accompanying it with a script allows us to implement the necessary logic to handle null cases properly, adapting to the model while maintaining data integrity.","comments":[{"timestamp":"1733414880.0","poster":"certs4pk","upvote_count":"1","content":"y use a data flow job when it can b done via data prep (much simpler & straight forward, less resource intensive)..","comment_id":"1322430"}],"comment_id":"1306761","timestamp":"1730692740.0","upvote_count":"2","poster":"Erg_de"},{"comment_id":"1189406","timestamp":"1712241300.0","upvote_count":"2","poster":"AjoeT","content":"Selected Answer: B\nB. Dataprep has the feature to convert it into 0."},{"content":"0 is still a value, which can add bias in the model and the model will take that into account while making predictions so 'none'","upvote_count":"1","comment_id":"1163235","poster":"niru12376","timestamp":"1709271060.0"},{"timestamp":"1702215120.0","content":"Why not D? keyword is Monitor, B would replace all empty fields and also cause unintended bias.","poster":"Nandababy","upvote_count":"1","comment_id":"1092512","comments":[{"upvote_count":"1","content":"However, Sergiomujica is right. If we need to prepare data using a casual method then its B \"Dataprep\".","poster":"Nandababy","comment_id":"1092516","timestamp":"1702215480.0"}]},{"poster":"sergiomujica","content":"The questions says \"You need to prepare data using a casual method \", thats dataprep and values should be 0 so the right answer is B","timestamp":"1693877400.0","comment_id":"998937","upvote_count":"1"},{"poster":"Mathew106","upvote_count":"2","content":"Selected Answer: B\nNo brainer. We need a real value and Dataprep is made for this. Dataflow is mainly for pre-processing before BigQuery ingests the data.","timestamp":"1690036740.0","comment_id":"959630"},{"poster":"theseawillclaim","upvote_count":"2","timestamp":"1689620940.0","comment_id":"954555","content":"Selected Answer: B\nDataprep is made for this kind of stuff, no reason to use a streaming service such as Dataflow."},{"upvote_count":"1","timestamp":"1682399940.0","poster":"Oleksandr0501","comment_id":"879966","content":"Selected Answer: B\ngpt:Cloud Dataprep is a data preparation service that can be used to transform, clean and shape data in a visually interactive way. It provides an easy-to-use interface to find and replace null values.\n\nCloud Dataflow is a fully-managed service for executing data processing pipelines, which allows for parallel execution of data processing tasks. However, it requires more expertise to set up and operate than Cloud Dataprep, and is usually used for more complex data processing needs.\n\nTherefore, option B is the most suitable approach for the given requirements."},{"timestamp":"1674444240.0","comments":[{"upvote_count":"2","poster":"rajm893","content":"The \"casual way\" or easy way to convert to to 0 is using Dataprep job rather than using the custom script.","timestamp":"1684222980.0","comment_id":"898965"},{"timestamp":"1684935360.0","comment_id":"905930","upvote_count":"1","poster":"AmmarFasih","content":"A simple rule. Whenever any service is available by GCP for a task, always recommend to use GCP service over any other."}],"comment_id":"784939","upvote_count":"2","content":"Seems to me like Answers are both B and D.\nB is faster to implement while D takes time.\nDoesnt mean that it's wrong though. I m not sure why everyone has picked just B. Why not D? D works and does the same job. And also having custom script provides more flexibility and control over the data processing tasks and it allows you to handle missing values in a more flexible and efficient way.","poster":"samdhimal"},{"poster":"GCPpro","timestamp":"1674194400.0","content":"B is the correct answer.","comment_id":"781914","upvote_count":"1"},{"poster":"AzureDP900","upvote_count":"3","comment_id":"766094","timestamp":"1672866360.0","content":"Answer is Use Cloud Dataprep to find null values in sample source data. Convert all nulls to 0 using a Cloud Dataprep job.\n\nKey phrases are \"casual method\", \"need to replace null with real values\", \"logistic regression\". Logistic regression works on numbers. Null need to be replaced with a number. And Cloud dataprep is best casual tool out of given options."},{"comment_id":"745530","upvote_count":"1","timestamp":"1671061260.0","content":"Selected Answer: B\nreal value 0","poster":"DGames"},{"poster":"byash1","upvote_count":"2","content":"Selected Answer: B\nIt is B","timestamp":"1643032320.0","comment_id":"531339"},{"content":"Selected Answer: B\nDataprep + real value (0)","timestamp":"1641395160.0","upvote_count":"1","poster":"medeis_jar","comment_id":"517595"},{"comment_id":"505626","poster":"MaxNRG","upvote_count":"3","content":"Selected Answer: B\nDataprep is the tool. A or B.\nSince they need to have a real-valued cannot be null N/A or empty, have to be “0”, so it has to be B.","timestamp":"1640024640.0"},{"poster":"anji007","content":"Ans: B\nDataprep suites this, so none of dataflow options qualify as answer. Then 0 can be real-value than a \"~none'.","upvote_count":"2","comment_id":"463638","timestamp":"1634490720.0"},{"comment_id":"393232","content":"Vote for 'B'","upvote_count":"4","comments":[{"content":"why not D?","comments":[{"comment_id":"503221","content":"Because why writing custom script when you can do it easily directly in Dataprep? D technically would also work, but B is just simpler thus preferred option.","poster":"szefco","upvote_count":"3","timestamp":"1639691760.0"}],"upvote_count":"1","timestamp":"1631827920.0","comment_id":"446183","poster":"Ral17"}],"poster":"sumanshu","timestamp":"1624913460.0"},{"timestamp":"1613051880.0","content":"It is B as dataprep has a feature to convert values as desired","comment_id":"288333","poster":"Sush12","upvote_count":"2"},{"poster":"NamitSehgal","content":"Should be B\nDataPrep runs a dataflow job at background.","comment_id":"246128","timestamp":"1608172080.0","upvote_count":"4"},{"poster":"Tanmoyk","comment_id":"175054","content":"Casual approach Dataprep ...and convert null value to numerical value to 0 ...answer B","timestamp":"1599466740.0","upvote_count":"5"},{"comment_id":"162548","timestamp":"1597966800.0","upvote_count":"2","content":"B \nis the correct answer with Trifacta that is data ranging software","poster":"atnafu2020","comments":[{"comment_id":"162549","upvote_count":"2","poster":"atnafu2020","timestamp":"1597966860.0","content":"Dataprep by Trifacta"}]},{"poster":"haroldbenites","comment_id":"161813","content":"B is correct","timestamp":"1597875480.0","upvote_count":"2"},{"poster":"haroldbenites","content":"B is correct","timestamp":"1597842540.0","upvote_count":"2","comment_id":"161513"},{"content":"Key phrases are \"casual method\", \"need to replace null with real values\", \"logistic regression\". Logistic regression works on numbers. Null need to be replaced with a number. And Cloud dataprep is best casual tool out of given options. \nAnswer should be \"B\" -","comment_id":"127820","timestamp":"1594039680.0","upvote_count":"8","poster":"dg63"},{"timestamp":"1585365540.0","poster":"[Removed]","comment_id":"68727","content":"Answer: B\nDescription: Dataprep is best suited for this kind of activity, as we need to just do a small function of modulating the data and dataprep is UI based","upvote_count":"5"},{"poster":"Rajokkiyam","content":"Answer B","comment_id":"66699","timestamp":"1584838620.0","upvote_count":"4"},{"content":"Instead of having to write the custom script from scratch (option D), dataprep already has preconfigured tools for your use to perform the necessary data wrangling. As mentioned by jvg637, real-values have to be \"0\". Considering both points above, answer should be 'B'","timestamp":"1584600180.0","poster":"Snobid","comment_id":"65900","upvote_count":"8"},{"comments":[{"comment_id":"66701","upvote_count":"3","poster":"Rajokkiyam","content":"Feature should have \"Numerical\" data.","timestamp":"1584838680.0"}],"timestamp":"1584103500.0","content":"Answer should be A","upvote_count":"1","comment_id":"63488","poster":"madhu1171"}],"choices":{"C":"Use Cloud Dataflow to find null values in sample source data. Convert all nulls to 'none' using a Cloud Dataprep job.","A":"Use Cloud Dataprep to find null values in sample source data. Convert all nulls to 'none' using a Cloud Dataproc job.","D":"Use Cloud Dataflow to find null values in sample source data. Convert all nulls to 0 using a custom script.","B":"Use Cloud Dataprep to find null values in sample source data. Convert all nulls to 0 using a Cloud Dataprep job."},"topic":"1","answer_images":[],"question_images":[]},{"id":"eMemzU2vYBOuQSBQ66EI","exam_id":11,"choices":{"B":"Create encryption keys in Cloud Key Management Service. Use those keys to encrypt your data in all of the Compute Engine cluster instances.","D":"Create encryption keys in Cloud Key Management Service. Reference those keys in your API service calls when accessing the data in your Compute Engine cluster instances.","A":"Create a dedicated service account, and use encryption at rest to reference your data stored in your Compute Engine cluster instances as part of your API service calls.","C":"Create encryption keys locally. Upload your encryption keys to Cloud Key Management Service. Use those keys to encrypt your data in all of the Compute Engine cluster instances."},"discussion":[{"comment_id":"469979","content":"Dear Admin, almost every answer is incorrect . Please check the comments and update your website.","timestamp":"1651263540.0","poster":"SonuKhan1","upvote_count":"57"},{"comment_id":"66561","content":"correct: B","upvote_count":"22","comments":[{"upvote_count":"4","poster":"[Removed]","content":"https://cloud.google.com/security/encryption-at-rest/","comments":[{"poster":"tprashanth","timestamp":"1610554680.0","comments":[{"upvote_count":"2","comment_id":"502477","content":"If you create it locally, you can't rotate keys. Answer should be B","poster":"baubaumiaomiao","timestamp":"1655318940.0"}],"content":"Based on the info at the link you referred, it seems C is the right answer","comment_id":"133999","upvote_count":"4"}],"comment_id":"66562","timestamp":"1600694280.0"}],"timestamp":"1600694160.0","poster":"[Removed]"},{"upvote_count":"1","content":"Selected Answer: B\nD is incorrect because referencing keys in API service calls doesn't meet the requirements for encrypting data at rest. This approach is more related to accessing data at runtime, not storing it securely.","timestamp":"1735998300.0","poster":"and88x","comment_id":"1336397"},{"upvote_count":"1","timestamp":"1734168300.0","content":"Selected Answer: C\nCMEK is where customer managers keys, but are still created by Google (this is for KMS). CSEK is where keys are created outside GCP and used by API calls. So if customer has to create keys, it has to be outside KMS","poster":"AmitK121981","comment_id":"1326401"},{"content":"Selected Answer: D\nBest Option: This is the most accurate approach. Cloud KMS provides the ability to create, manage, and rotate encryption keys. You can use the KMS API to reference the keys when encrypting and decrypting your data. In this case, you would integrate the KMS keys into your application logic (e.g., Kafka producers/consumers, Redis clients) to encrypt and decrypt data as it is stored or processed. This approach leverages the full functionality of Cloud KMS, including the ability to rotate and destroy keys as needed.","upvote_count":"2","timestamp":"1733984520.0","comment_id":"1325429","poster":"jatinbhatia2055"},{"comment_id":"1196431","content":"But KMS doesnt create keys. It only stores them right?","poster":"zevexWM","timestamp":"1729063080.0","upvote_count":"1"},{"content":"Selected Answer: B\nGoogle Cloud Key Management Service (KMS) provides a centralized cloud service for managing cryptographic keys. By creating encryption keys in Cloud KMS, you can easily manage the lifecycle of these keys, including creation, rotation, and destruction.\n WYY NOT Create Keys Locally and Upload to Cloud KMS? \n While it’s possible to create keys locally and then upload them to Cloud KMS, it’s generally simpler and more secure to create the keys directly in Cloud KMS. This reduces the risk associated with transferring keys and leverages the security and compliance features of Cloud KMS.","timestamp":"1718370000.0","comment_id":"1096578","poster":"TVH_Data_Engineer","upvote_count":"1"},{"upvote_count":"2","poster":"emmylou","content":"Help!\nI chose \"C\" because of the statement, \"encrypt data at rest with encryption keys that you can create, rotate, and destroy as needed\" and read that as needing to generate the keys locally. Can you please explain where I went wrong?","comment_id":"1060006","timestamp":"1714583220.0"},{"poster":"odiez3","timestamp":"1706033460.0","upvote_count":"1","comment_id":"960615","content":"the answer is C Read the full statement.\n\n\" You need to encrypt data at rest with encryption keys that you can create \""},{"upvote_count":"1","content":"Selected Answer: B\nB!\nC is useless overhead and you cannot rotate that easily!","timestamp":"1705526520.0","poster":"theseawillclaim","comment_id":"954560"},{"timestamp":"1700087520.0","poster":"Kiroo","content":"Well for what I remember from cloud arch and what I found in https://cloud.google.com/compute/docs/disks/customer-managed-encryption\n\nThere is two options or the customer manage entirely or he will use the service to generate the keys so based on that is the B","upvote_count":"1","comment_id":"898700"},{"poster":"samdhimal","content":"B. Create encryption keys in Cloud Key Management Service. Use those keys to encrypt your data in all of the Compute Engine cluster instances.\n\nCloud Key Management Service (KMS) is a fully managed service that allows you to create, rotate, and destroy encryption keys as needed. By creating encryption keys in Cloud KMS, you can use them to encrypt your data at rest in the Compute Engine cluster instances, which is running your Redis and Kafka clusters. This ensures that your data is protected even when it is stored on disk.","upvote_count":"2","timestamp":"1690075620.0","comments":[{"comment_id":"784943","content":"Option A: Create a dedicated service account, and use encryption at rest to reference your data stored in your Compute Engine cluster instances as part of your API service calls is not the best option as it does not provide encryption at rest.\n\nOption C: Create encryption keys locally. Upload your encryption keys to Cloud Key Management Service. Use those keys to encrypt your data in all of the Compute Engine cluster instances, is not the best option as it does not provide a way to manage the encryption keys centrally.\n\nOption D: Create encryption keys in Cloud Key Management Service. Reference those keys in your API service calls when accessing the data in your Compute Engine cluster instances, is not the best option as it does not provide encryption at rest, it only secure the data in transit.","timestamp":"1690075620.0","upvote_count":"3","poster":"samdhimal"}],"comment_id":"784942"},{"content":"Selected Answer: B\nB is correct answer generate key using KMS, why locally again it is overhead to upload and use everywhere.","upvote_count":"1","comment_id":"745537","timestamp":"1686779880.0","poster":"DGames"},{"timestamp":"1684996140.0","poster":"Atnafu","upvote_count":"1","comment_id":"726509","content":"B\nIf you use Google Cloud, Cloud Key Management Service lets you create your own encryption keys that you can use to add envelope encryption to your data. Using Cloud KMS, you can create, rotate, track, and delete keys. \nhttps://cloud.google.com/docs/security/encryption/default-encryption#:~:text=If%20you%20use%20Google%20Cloud%2C%20Cloud%20Key%20Management%20Service%20lets%20you%20create%20your%20own%20encryption%20keys%20that%20you%20can%20use%20to%20add%20envelope%20encryption%20to%20your%20data.%20Using%20Cloud%20KMS%2C%20you%20can%20create%2C%20rotate%2C%20track%2C%20and%20delete%20keys."},{"timestamp":"1657026480.0","poster":"medeis_jar","comment_id":"517604","content":"Selected Answer: B\nhttps://cloud.google.com/security/encryption-at-rest/","upvote_count":"1"},{"upvote_count":"8","poster":"MaxNRG","comment_id":"506277","content":"Selected Answer: B\nA makes no sense, you need to use your own keys.\nYou don’t create keys locally and upload them, you should import it to make it work..using the kms public key…not just “uploading” it. C is also out.\nIT’s between B and D\nCloud KMS is a cloud-hosted key management service that lets you manage cryptographic keys for your cloud services the same way you do on-premises, You can generate, use, rotate, and destroy cryptographic keys from there.\nSince you want to encrypt data at rest, is B, you don’t use them for any API calls.\nhttps://cloud.google.com/compute/docs/disks/customer-managed-encryption","timestamp":"1655822640.0"},{"timestamp":"1650908280.0","comment_id":"467613","upvote_count":"2","poster":"lg1234","content":"I believe you cannot upload custom keys to KMS for Compute Engine. Only via API Calls. See: https://cloud.google.com/security/encryption/customer-supplied-encryption-keys\nWith that said, option B"},{"upvote_count":"2","timestamp":"1648466760.0","poster":"Ysance_AGS","comment_id":"453321","content":"Answer is B : both clusters are on GCP, so we can use KMS to manage the keys."},{"content":"Well there are two things that a user a user can do via KMS-\n1. You may be using existing cryptographic keys that were created on your premises or in an external key management system. If you migrate an application to Google Cloud or if you add cryptographic support to an existing Google Cloud application, you can import the relevant keys into Cloud KMS. In the given situation, user is not having any key. So let's check 2nd option.\n\n2.Cloud Key Management Service allows you to create, import, and manage cryptographic keys-- a general KMS definition.\n\nNow if you don't have a key, why would you generate it locally then import it in KMS if you have a option to create a key yourself in KMS.\n\nhttps://cloud.google.com/kms/docs\nAns - B","timestamp":"1645212360.0","poster":"Sakshi_12","comment_id":"426996","upvote_count":"3"},{"comment_id":"402141","content":"it's B.\nWith KMS (Key Management Service), customer can create and destroy the keys as shown here: https://cloud.google.com/kms/docs/quickstart\nCustomer can also rotate key: https://cloud.google.com/kms/docs/rotating-keys","poster":"gaco","upvote_count":"2","timestamp":"1641673440.0"},{"poster":"awssp12345","upvote_count":"3","comments":[{"upvote_count":"2","timestamp":"1641680160.0","content":"https://cloud.google.com/kms/docs/creating-keys#create_a_key_ring","poster":"sumanshu","comment_id":"402209"},{"comment_id":"402132","content":"https://www.testpreptraining.com/tutorial/managing-customer-managed-encryption-keys-with-cloud-kms/","poster":"awssp12345","timestamp":"1641672300.0","upvote_count":"2"},{"timestamp":"1641671880.0","poster":"awssp12345","content":"Please see https://cloud.google.com/kms/docs/faq#import_keys","comment_id":"402125","upvote_count":"1"}],"comment_id":"398718","timestamp":"1641347400.0","content":"I think people are missing on the fact that the customer wants to create their own key. This is only possible with option C. In optionB, GCP KMS creates and manages the key for you.\n\nGCP certification does not always mean use all gcp managed services."},{"timestamp":"1640738340.0","content":"Vote for B","poster":"sumanshu","comment_id":"393288","upvote_count":"1"},{"upvote_count":"1","poster":"daghayeghi","content":"B:\nhttps://cloud.google.com/security/encryption-at-rest/\nbased on information from this document it seems to be CMEK, and not CSEK, since in question said: \"you can create, rotate, and destroy\" and in the CMEK description said as well.","timestamp":"1631351580.0","comment_id":"307794"},{"content":"I will recommend B. \nMost of the enterprise companies will not recommend to create a Encryption key locally. It doesnt mentioned as On-premise. KMS does everything https://cloud.google.com/kms/docs/destroy-restore\nSo i will go with B.","poster":"bobby8521","comment_id":"258771","timestamp":"1625330520.0","upvote_count":"2"},{"timestamp":"1621192080.0","upvote_count":"3","poster":"xfoursea","content":"very bad question, confusing, do not know what it is intended to ask...","comment_id":"220642"},{"poster":"aleedrew","upvote_count":"2","content":"Confused about this one too. B would be correct according to this doc https://cloud.google.com/kms/docs/cmek?authuser=1. C works too.","comment_id":"197024","timestamp":"1618008900.0"},{"content":"Its between B and C. Both are correct & open to ones interpretation. Since its Google's Certification, and a solution/answer is fully available in GCP, I'll always go for that solution. so answer is B.","upvote_count":"3","timestamp":"1616637660.0","poster":"DeepakKhattar","comment_id":"186617","comments":[{"comment_id":"393292","upvote_count":"1","timestamp":"1640738520.0","poster":"sumanshu","content":"What does encrypted at rest mean?\n\nBy encrypting data at rest, you're essentially converting your customer's sensitive data into another form of data. This usually happens through an algorithm that can't be understood by a user who does not have an encryption key to decode it."}]},{"content":"B is the correct answer, check this: https://cloud.google.com/security/encryption-at-rest \nit says when you use cloud KMS, You can create, rotate, automatically rotate and destroy symmetric encryption keys","comment_id":"172076","poster":"sh2020","upvote_count":"2","timestamp":"1614699180.0"},{"poster":"haroldbenites","upvote_count":"1","timestamp":"1613780460.0","comment_id":"161814","content":"Correct C"},{"upvote_count":"1","content":"A, as you need to maintain your keys.","comment_id":"150713","timestamp":"1612478640.0","poster":"Archy"},{"timestamp":"1611213180.0","upvote_count":"3","content":"B seems Correct answer . As it is mentioned in docs.","comment_id":"140051","poster":"bhavesh_wadhwani"},{"poster":"AJKumar","content":"Question says - need to encrypt data at rest with encryption keys that you can create, rotate, and destroy as needed means local keys. Option C is correct.","comment_id":"118167","upvote_count":"8","timestamp":"1608799680.0"},{"comment_id":"76619","content":"B.\nhttps://cloud.google.com/compute/docs/disks/customer-managed-encryption","timestamp":"1603153140.0","upvote_count":"8","poster":"Ganshank"},{"upvote_count":"15","comment_id":"68729","timestamp":"1601256300.0","poster":"[Removed]","content":"Answer: B\nDescription: KMS stored on cloud helps in creating, rotating and destroying keys as needed and also it can be used to encrypt compute engines"},{"poster":"Rajokkiyam","upvote_count":"2","timestamp":"1600733820.0","comment_id":"66722","content":"Correct : A"}],"topic":"1","answers_community":["B (81%)","Other"],"answer_images":[],"isMC":true,"answer_description":"","question_id":283,"question_images":[],"question_text":"You set up a streaming data insert into a Redis cluster via a Kafka cluster. Both clusters are running on Compute Engine instances. You need to encrypt data at rest with encryption keys that you can create, rotate, and destroy as needed. What should you do?","answer_ET":"B","url":"https://www.examtopics.com/discussions/google/view/17109-exam-professional-data-engineer-topic-1-question-66/","answer":"B","unix_timestamp":1584803760,"timestamp":"2020-03-21 16:16:00"},{"id":"RqWOqloHcJIn31jpjWOk","answer_ET":"C","answer":"C","topic":"1","question_id":284,"answers_community":["C (100%)"],"unix_timestamp":1584804300,"question_text":"You are developing an application that uses a recommendation engine on Google Cloud. Your solution should display new videos to customers based on past views. Your solution needs to generate labels for the entities in videos that the customer has viewed. Your design must be able to provide very fast filtering suggestions based on data from other customer preferences on several TB of data. What should you do?","exam_id":11,"question_images":[],"discussion":[{"upvote_count":"36","comments":[{"poster":"jin0","comment_id":"824732","timestamp":"1709120820.0","content":"I don't understand why Vision API should be a answer for labeling? there is no information about input data. isn't it?","upvote_count":"1"},{"content":"Is there any notice that has to reject own model in question..?","upvote_count":"1","comment_id":"824731","poster":"jin0","timestamp":"1709120760.0"}],"comment_id":"66563","timestamp":"1616340300.0","content":"Answer: C\nA & B - Need to build your own model, so discarded as options C D can do the job here using Cloud Video Intelligence API. BigTable is better option. So C is correct","poster":"[Removed]"},{"poster":"[Removed]","comment_id":"68730","timestamp":"1616894940.0","content":"Answer: C\nDescription: Why to build own model, Video API with Bigtable is best solution","upvote_count":"14"},{"upvote_count":"2","content":"Selected Answer: C\nI don't even know if MLLib has out-of-the-box Computer Vision models. Developing this in Dataproc would be a nightmare.\n\nUsing the computer vision API on the other hand makes perfect sense.\n\nThe fact that the filtering must happen very fast and that this is a customer facing application points to BigTable so that there is very little latency and high availability. BigTable is eventually consistent but that doesn't really matter for this application.\n\nCloudSQL will ensure strong consistency which we don't really need but it is slower and supports max 64 TB. The description mentions multiple TBs. Not really sure what several means here, but Cloud SQL doesn't have a high cap.","comment_id":"959634","poster":"Mathew106","timestamp":"1721659920.0"},{"upvote_count":"1","poster":"euro202","timestamp":"1720163700.0","comment_id":"943407","content":"Selected Answer: C\nWe need a model that extracts labels from videos, so Vision API could be used.\nThen we need a DB very fast and that can handle several TB of data, so BigTable is the best choice.\nAnswer is C."},{"upvote_count":"2","poster":"samdhimal","timestamp":"1705887300.0","content":"Option C is the correct choice because it utilizes the Cloud Video Intelligence API to generate labels for the entities in the videos, which would save time and resources compared to building and training a model from scratch. Additionally, by storing the data in Cloud Bigtable, it allows for fast and efficient filtering of the predicted labels based on the user's viewing history and preferences. This is a more efficient and cost-effective approach than storing the data in Cloud SQL and performing joins and filters.","comment_id":"783858"},{"upvote_count":"1","content":"Answer is C\n Build an application that calls the Cloud Video Intelligence API to generate labels. Store data in Cloud Bigtable, and filter the predicted labels to match the user's viewing history to generate preferences.\n\n1. Rather than building a new model - it is better to use Google provide APIs, here - Google Video Intelligence. So option A and B rules out\n2. Between SQL and Bigtable - Bigtable is the better option as Bigtable support row-key filtering. Joining the filters is not required.\n\nReference:\nhttps://cloud.google.com/video-intelligence/docs/feature-label-detection","timestamp":"1704402480.0","poster":"AzureDP900","comment_id":"766095"},{"upvote_count":"2","poster":"MaxNRG","comment_id":"506283","content":"Selected Answer: C\nC.\nThe cloud video intelillence api does the label generation without the need of building any model, A and B are excluded. Now, the bbdd most suitable for this is bigtable and not SQL (this big joins would be anything but fast).\nhttps://cloud.google.com/video-intelligence/docs/feature-label-detection","timestamp":"1671641520.0"},{"content":"Vote for C","timestamp":"1656456720.0","poster":"sumanshu","upvote_count":"4","comment_id":"393299"},{"comment_id":"318511","timestamp":"1648071000.0","upvote_count":"2","content":"Answer: C\nReference https://cloud.google.com/video-intelligence/docs/feature-label-detection","poster":"timolo"},{"upvote_count":"7","content":"answer C:\nIf we presume that use label of video as a rowkey, Bigtable will be the best option. because it can store several TB, but Cloud SQL is limited to 30TB.","comment_id":"307799","timestamp":"1646997780.0","poster":"daghayeghi"},{"content":"Answer: C","upvote_count":"3","comment_id":"246152","poster":"NamitSehgal","timestamp":"1639709880.0"},{"poster":"Alasmindas","comment_id":"219034","upvote_count":"7","content":"Option C is the correct answer.\n1. Rather than building a new model - it is better to use Google provide APIs, here - Google Video Intelligence. \nSo option A and B rules out\n2) Between SQL and Bigtable - Bigtable is the better option as Bigtable support row-key filtering. Joining the filters is not required.","timestamp":"1636882920.0"},{"timestamp":"1632405540.0","content":"Answer is D : BigTable doesnt support JOIN and not built for transactions - https://cloud.google.com/bigtable/docs/overview","upvote_count":"2","comment_id":"185359","poster":"SureshKotla","comments":[{"content":"There are no joins but filtering based on condition.","comment_id":"207156","poster":"Surjit24","upvote_count":"4","comments":[{"comment_id":"289947","content":"but the requirement involves join as well, it is stated in the problem.","poster":"karthik89","timestamp":"1644807000.0","upvote_count":"2","comments":[{"content":"Where? Though it's mention - \" very fast filtering suggestions\" - which means something like dictionary in python OR Key: Value (which is Bigtable)","upvote_count":"1","comment_id":"402212","poster":"sumanshu","timestamp":"1657311600.0","comments":[{"timestamp":"1673809620.0","poster":"sraakesh95","upvote_count":"1","comment_id":"524378","comments":[{"upvote_count":"1","poster":"Deepakd","timestamp":"1680417000.0","comment_id":"579739","content":"Recommendation based on other customer”s views cannot be achieved through simple joins. A class pf machine learning algorithms called collaborative filtering is required for that. You need big table to run these algorithms."}],"content":"I think \"based on other customer preferences\" from the questions requires a join before a filter is applied for collaborative filtering"}]}]}],"timestamp":"1635343740.0"}]},{"poster":"haroldbenites","upvote_count":"2","timestamp":"1629412320.0","comment_id":"161820","content":"Correct C"},{"upvote_count":"2","timestamp":"1625588040.0","content":"I doubt if C can be an answer. Will Bigtable allow filtering on labels?","poster":"dg63","comments":[{"timestamp":"1626186060.0","content":"Yes, if its part of the rowkey","poster":"tprashanth","comment_id":"134000","upvote_count":"3"}],"comment_id":"127966"},{"timestamp":"1625488020.0","content":"Answer is C.","comment_id":"126835","poster":"Rajuuu","upvote_count":"4"},{"timestamp":"1618123260.0","upvote_count":"7","comment_id":"73186","content":"C.\nThe recommendation requires filtering based on several TB of data, therefore BigTable is the recommended option vs Cloud SQL which is limited to 10TB.","poster":"Ganshank"}],"choices":{"C":"Build an application that calls the Cloud Video Intelligence API to generate labels. Store data in Cloud Bigtable, and filter the predicted labels to match the user's viewing history to generate preferences.","B":"Build and train a classification model with Spark MLlib to generate labels. Build and train a second classification model with Spark MLlib to filter results to match customer preferences. Deploy the models using Cloud Dataproc. Call the models from your application.","A":"Build and train a complex classification model with Spark MLlib to generate labels and filter the results. Deploy the models using Cloud Dataproc. Call the model from your application.","D":"Build an application that calls the Cloud Video Intelligence API to generate labels. Store data in Cloud SQL, and join and filter the predicted labels to match the user's viewing history to generate preferences."},"isMC":true,"answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/17110-exam-professional-data-engineer-topic-1-question-67/","timestamp":"2020-03-21 16:25:00","answer_description":""},{"id":"YaditxkyOwcPoIcvoDrB","timestamp":"2020-03-13 14:01:00","exam_id":11,"answers_community":["C (94%)","3%"],"unix_timestamp":1584104460,"isMC":true,"question_images":[],"url":"https://www.examtopics.com/discussions/google/view/16478-exam-professional-data-engineer-topic-1-question-68/","answer":"C","question_id":285,"choices":{"C":"Use Cloud Dataflow to run your transformations. Monitor the job system lag with Stackdriver. Use the default autoscaling setting for worker instances.","A":"Use Cloud Dataproc to run your transformations. Monitor CPU utilization for the cluster. Resize the number of worker nodes in your cluster via the command line.","B":"Use Cloud Dataproc to run your transformations. Use the diagnose command to generate an operational output archive. Locate the bottleneck and adjust cluster resources.","D":"Use Cloud Dataflow to run your transformations. Monitor the total execution time for a sampling of jobs. Configure the job to use non-default Compute Engine machine types when needed."},"answer_images":[],"answer_ET":"C","answer_description":"","question_text":"You are selecting services to write and transform JSON messages from Cloud Pub/Sub to BigQuery for a data pipeline on Google Cloud. You want to minimize service costs. You also want to monitor and accommodate input data volume that will vary in size with minimal manual intervention. What should you do?","topic":"1","discussion":[{"poster":"madhu1171","timestamp":"1599994860.0","content":"Answer should be C","upvote_count":"36","comment_id":"63491"},{"content":"Answer: C - best suitable for the purpose with autoscaling and google recommended transform engine between pubsub n bq","comment_id":"66566","timestamp":"1600695000.0","poster":"[Removed]","upvote_count":"26"},{"comment_id":"1365392","timestamp":"1741178760.0","upvote_count":"1","content":"Selected Answer: C\nC for me","poster":"Abizi"},{"poster":"yassoraa88","content":"Selected Answer: C\nusing Cloud Dataflow for transformations with monitoring via Stackdriver and leveraging its default autoscaling settings, is the best choice. Cloud Dataflow is purpose-built for this type of workload, providing seamless scalability and efficient processing capabilities for streaming data. Its autoscaling feature minimizes manual intervention and helps manage costs by dynamically adjusting resources based on the actual processing needs, which is crucial for handling fluctuating data volumes efficiently and cost-effectively.","comment_id":"1208786","upvote_count":"2","timestamp":"1731153060.0"},{"poster":"barnac1es","comments":[{"comment_id":"1021629","content":"Additionally, having the flexibility to adjust machine types based on workload characteristics ensures that you can achieve the desired performance levels without overspending on unnecessary resources. This level of customization is not provided by simply relying on the default autoscaling settings, making option D a more comprehensive and cost-effective solution for managing varying data volumes.","poster":"barnac1es","timestamp":"1711820340.0","upvote_count":"2"}],"comment_id":"1021628","timestamp":"1711820340.0","upvote_count":"1","content":"Selected Answer: D\nOption C suggests using Cloud Dataflow to run the transformations and monitoring the job system lag with Stackdriver while using the default autoscaling setting for worker instances.\n\nWhile using Cloud Dataflow is a suitable choice for processing data from Cloud Pub/Sub to BigQuery, and monitoring with Stackdriver provides valuable insights, the specific emphasis on configuring non-default Compute Engine machine types (as mentioned in option D) gives you more control over cost optimization and performance tuning.\n\nBy configuring non-default machine types, you can precisely tailor the computational resources to match the specific requirements of your workload. This fine-grained control allows you to optimize costs further by avoiding over-provisioning of resources, especially if your workload is memory-intensive, CPU-bound, or requires specific configurations that are not met by the default settings."},{"upvote_count":"1","content":"Selected Answer: B\nAt first I answered C. However, Dataproc is indeed cheaper than Dataflow. And both of them can scale automatically horizontically.\n\nDataflow horizontal scaling applies to both primary and secondary nodes. Scaling secondary nodes scales up CPU/compute and scaling primary nodes scales up both memory and CPU/compute.\n\nI don't quite understand the second part of answer B where it says I should allocate resources accordingly. I guess I could do that, but auto-scaling should be enough.","timestamp":"1705942860.0","comment_id":"959641","poster":"Mathew106"},{"content":"Answer should be C","comment_id":"820124","poster":"AbdullahAnwar","timestamp":"1692846420.0","upvote_count":"2"},{"timestamp":"1690132200.0","comment_id":"785743","content":"C. Use Cloud Dataflow to run your transformations. Monitor the job system lag with Stackdriver. Use the default autoscaling setting for worker instances.\n\nCloud Dataflow is a managed service that allows you to write and execute data transformations in a highly scalable and fault-tolerant way. By default, it will automatically scale the number of worker instances based on the input data volume and job performance, which can help minimize costs. Monitoring the job system lag with Stackdriver can help you identify any issues that may be impacting performance and take action as needed. Additionally, using the default autoscaling setting for worker instances can help you minimize manual intervention and ensure that resources are used efficiently.","poster":"samdhimal","upvote_count":"3"},{"timestamp":"1686171540.0","content":"Selected Answer: C\nC is the answer.","poster":"zellck","comment_id":"738455","upvote_count":"1"},{"upvote_count":"11","comments":[{"content":"you need to look at community vote distribution and comments, and not the suggested answer.","timestamp":"1686171600.0","comment_id":"738456","poster":"zellck","upvote_count":"9"}],"content":"Selected Answer: C\n@admin why all the answers are wrong. I paid 30 euros for this web and its garbage.\nDataproc has no sense in this scenario, because you want to have minimal intervention/operation. D is not a good practice, the answer is C.","timestamp":"1686126060.0","poster":"odacir","comment_id":"737728"},{"comment_id":"517606","upvote_count":"4","content":"Selected Answer: C\nC only as referred by MaxNRG","timestamp":"1657026780.0","poster":"medeis_jar"},{"upvote_count":"9","comment_id":"506284","timestamp":"1655823120.0","content":"Selected Answer: C\nC.\nDataproc does not seem to be a good solution in this case as it always require a manual intervention to adjust resources.\nAutoscaling with dataflow will automatically handle changing data volumes with no manual intervention, and monitoring through Stackdriver can be used to spot bottleneck. Total execution time is not good there as it does not provide a precise view on potential bottleneck.","poster":"MaxNRG"},{"content":"Selected Answer: C\nDataflow, Stackdriver and autoscaling","timestamp":"1653814740.0","upvote_count":"3","comment_id":"489813","poster":"StefanoG"},{"comment_id":"476770","upvote_count":"4","timestamp":"1652334780.0","content":"Admin, please take a look on the comments. Almost all answers are wrong","poster":"victorlie"},{"content":"Answer should be C as dataflow is unpredictable size ( input that will vary in size), dataproc is with known size","poster":"nguyenmoon","comment_id":"445007","comments":[{"upvote_count":"1","content":"dataflow over dataproc is always the preferred way in gcp. use dataproc only there is specific client requirements such as existing hadoop workloads, etc..","timestamp":"1660665480.0","poster":"Tanzu","comment_id":"548812"}],"upvote_count":"4","timestamp":"1647329400.0"},{"content":"Option C is the answer","comment_id":"421715","timestamp":"1644342780.0","poster":"sandipk91","upvote_count":"3"},{"poster":"sumanshu","content":"Vote for C","timestamp":"1640739360.0","comment_id":"393302","upvote_count":"1"},{"comment_id":"255474","poster":"apnu","upvote_count":"2","content":"B , it is correct , as it says minimum service cost, dataflow is more expansive than dataproc.","timestamp":"1625037480.0","comments":[{"timestamp":"1633871940.0","poster":"Believerath","content":"You have to transform the JSON messages. Hence, you need to use dataflow.","comment_id":"332563","upvote_count":"1"},{"timestamp":"1631353200.0","content":"but it said \"with minimal manual intervention\" and for Dataproc you need to manage cluster manually, then C is the best option.","upvote_count":"1","poster":"daghayeghi","comment_id":"307817"}]},{"content":"B , it is correct as it says minimum service cost, data flow is more expensive than dataproc.","upvote_count":"2","poster":"apnu","timestamp":"1625037420.0","comment_id":"255473"},{"upvote_count":"2","content":"Should be C. Only concern here is we do not have Window which can monitor size of incoming messages in dataflow.","timestamp":"1623891600.0","poster":"NamitSehgal","comment_id":"246155"},{"comment_id":"175057","content":"Answer should be C","upvote_count":"2","timestamp":"1615113180.0","poster":"Tanmoyk"},{"timestamp":"1613883360.0","upvote_count":"2","comment_id":"162639","poster":"atnafu2020","content":"C\ncorrect"},{"timestamp":"1613781240.0","upvote_count":"2","content":"C is correct","comment_id":"161822","poster":"haroldbenites"},{"content":"C\nis correct answer","comment_id":"144438","upvote_count":"2","timestamp":"1611703620.0","poster":"atnafu2020"},{"poster":"VishalB","content":"Correct Answer: C\nExplanation:-This option is correct as Dataflow, provides a cost-effective solution to perform transformations on the streaming data, with autoscaling provides scaling without any intervention. System lag with\nStackdriver provides monitoring for the streaming data. With autoscaling enabled, the Cloud Dataflow service automatically chooses the appropriate number of worker instances required to run your job.","comment_id":"139531","upvote_count":"11","timestamp":"1611155340.0"},{"poster":"Rajuuu","upvote_count":"2","timestamp":"1609879920.0","content":"Answer is C","comment_id":"127127"},{"poster":"[Removed]","timestamp":"1601256720.0","comment_id":"68731","content":"nswer: C\nDescription: Dataflow is goot with autoscaling and stackdriver to monitor CPU and Storage","upvote_count":"7"}]}],"exam":{"isImplemented":true,"isMCOnly":true,"lastUpdated":"11 Apr 2025","isBeta":false,"provider":"Google","name":"Professional Data Engineer","id":11,"numberOfQuestions":319},"currentPage":57},"__N_SSP":true}