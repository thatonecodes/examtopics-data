{"pageProps":{"questions":[{"id":"5Ske1VQaDpteNiivPxza","answer_images":[],"question_id":71,"unix_timestamp":1671575820,"answers_community":["A (100%)"],"discussion":[{"timestamp":"1687293420.0","poster":"fredcaram","comment_id":"751600","upvote_count":"5","content":"Selected Answer: A\nA makes more sense than changing products"},{"timestamp":"1721312760.0","upvote_count":"1","comment_id":"1126048","poster":"LaxmanTiwari","content":"Selected Answer: A"},{"timestamp":"1719397860.0","content":"Selected Answer: A\nChanging service is too risky. ==> A","upvote_count":"1","poster":"vodkaHN","comment_id":"1105990"},{"timestamp":"1694548800.0","content":"A.\nMigrating to a different platform is a bit extreme, so eliminate B. C would be a good idea in any event, but is not the best answer here. D is not how you manage maintenance windows on Cloud SQL. A is the best answer.","comment_id":"837437","poster":"dynamic_dba","upvote_count":"1"},{"upvote_count":"4","comment_id":"755855","timestamp":"1687706280.0","poster":"GCP72","content":"Selected Answer: A\nA is the correct answer"},{"timestamp":"1687697580.0","upvote_count":"2","content":"A: Set your production instance's maintenance window to ***** non-business hours.","poster":"pk349","comment_id":"755737"},{"comment_id":"754526","timestamp":"1687545540.0","poster":"lapeyus","upvote_count":"3","content":"Selected Answer: A\nmaintenance window"},{"timestamp":"1687366380.0","poster":"Kloudgeek","upvote_count":"2","content":"Correct Answer is A","comment_id":"752650"}],"exam_id":5,"isMC":true,"topic":"1","answer":"A","choices":{"A":"Set your production instance's maintenance window to non-business hours.","B":"Migrate the Cloud SQL instance to Cloud Spanner to avoid any future disruptions due to maintenance.","D":"Use Cloud Scheduler to schedule a maintenance window of no longer than 5 minutes.","C":"Contact Support to understand why your Cloud SQL instance had a downtime of more than 5 minutes."},"timestamp":"2022-12-20 23:37:00","question_text":"You manage a meeting booking application that uses Cloud SQL. During an important launch, the Cloud SQL instance went through a maintenance event that resulted in a downtime of more than 5 minutes and adversely affected your production application. You need to immediately address the maintenance issue to prevent any unplanned events in the future. What should you do?","answer_description":"","answer_ET":"A","question_images":[],"url":"https://www.examtopics.com/discussions/google/view/92244-exam-professional-cloud-database-engineer-topic-1-question/"},{"id":"svMpykJJ8F9MF7ECk67W","answer_description":"","question_id":72,"timestamp":"2022-12-24 23:22:00","url":"https://www.examtopics.com/discussions/google/view/92703-exam-professional-cloud-database-engineer-topic-1-question/","choices":{"A":"Deploy 2 Cloud SQL instances in the us-central1 region with HA enabled, and create read replicas in us-east1 and us-west1.","B":"Deploy 2 Cloud SQL instances in the us-central1 region, and create read replicas in us-east1 and us-west1.","C":"Deploy 4 Cloud SQL instances in the us-central1 region with HA enabled, and create read replicas in us-central1, us-east1, and us-west1.","D":"Deploy 4 Cloud SQL instances in the us-central1 region, and create read replicas in us-central1, us-east1 and us-west1."},"answer_ET":"A","topic":"1","answer":"A","exam_id":5,"discussion":[{"comments":[{"upvote_count":"1","comment_id":"830335","poster":"H_S","content":"although I agree with your answe, table limit is for mysql and not postgresql\nhttps://cloud.google.com/sql/docs/mysql/quotas#table_limit","timestamp":"1693941360.0"}],"timestamp":"1688631780.0","comment_id":"767510","content":"A is correct. We only have 8000 tables. More than 1 HA env is only required beyond 50000 tables\nIf you have 50,000 or more database tables on a single instance, it could result in the instance becoming unresponsive or unable to perform maintenance operations, and the instance is not covered by the SLA.","upvote_count":"8","poster":"SandyZA"},{"comment_id":"837446","upvote_count":"7","poster":"dynamic_dba","content":"A.\nB and D do not mention HA, so eliminate those. That leaves A and C. C talks about 4 instances with HA which presumably means 2 primaries each with an HA standby. Oddly, there are 4 zones in us-central1. The killer is having a read replica also in us-central1 which would mean the same zone would have a read replica and either a primary or HA standby. Not a good idea. Option A is the best choice. A primary and an HA standby in us-central1 (different zones) and then read replicas in us-east1 and us-west1.","timestamp":"1694549580.0"},{"content":"Selected Answer: A\nas I focus on low latency","comment_id":"1126659","poster":"PKookNN","timestamp":"1721385120.0","upvote_count":"2"},{"comment_id":"1080125","upvote_count":"1","timestamp":"1716644340.0","poster":"BIGQUERY_ALT_ALT","content":"The answer is C. you need a instance for a read replica not 2 per read replica."},{"timestamp":"1713878220.0","content":"Selected Answer: A\nAnswer C is wrong because the question suggests low-latency and replication for 4 region INCREASE latency.\nAnswer A is correct because attend the two requirements high availability and low latency.\nThe Key for this question is low latency.","upvote_count":"5","comments":[{"upvote_count":"2","content":"Thanks for the clarification.","poster":"dija123","timestamp":"1732120500.0","comment_id":"1214433"}],"poster":"Blackstile","comment_id":"1051865"},{"timestamp":"1709692200.0","upvote_count":"1","comment_id":"1000019","poster":"learnazureportal","content":"The correct answer is == C. Deploy 4 Cloud SQL instances in the us-central1 region with HA enabled, and create read replicas in us-central1, us-east1, and us-west1.","comments":[{"timestamp":"1724292540.0","upvote_count":"1","poster":"Jason_Cloud_at","comment_id":"1156117","content":"we can't have both primary and read replica in same region, it will not be considered as HA\ni will stick with A."}]},{"content":"Selected Answer: C\nThis option provides high availability with four instances in the primary region, ensuring redundancy and fault tolerance. By creating read replicas in us-central1, us-east1, and us-west1, read operations can be distributed across multiple regions, reducing latency for applications in those regions. This design allows for efficient and low-latency read operations while maintaining high availability.\nOption C is the recommended choice as it combines HA with multiple read replicas in different regions, providing both high availability and low-latency read operations for your multi-region application setup.","timestamp":"1701144480.0","upvote_count":"3","poster":"KennyHuang","comment_id":"908280"},{"comment_id":"759473","timestamp":"1687929180.0","upvote_count":"3","content":"Selected Answer: C\nWhy not \" C\" is an answer","poster":"GCP72"},{"poster":"pk349","upvote_count":"4","timestamp":"1687697520.0","content":"A: Deploy 2 Cloud SQL instances in the us-central1 region ***** with HA enabled, and create read replicas in us-east1 and us-west1.","comment_id":"755735"},{"comment_id":"755200","timestamp":"1687638120.0","content":"Selected Answer: A\nWe only need options with HA enabled and 4 databases with 3 read replicas (each?) seems overkill to me.","poster":"chelbsik","upvote_count":"6"}],"answer_images":[],"answers_community":["A (68%)","C (32%)"],"isMC":true,"question_text":"You are designing a highly available (HA) Cloud SQL for PostgreSQL instance that will be used by 100 databases. Each database contains 80 tables that were migrated from your on-premises environment to Google Cloud. The applications that use these databases are located in multiple regions in the US, and you need to ensure that read and write operations have low latency. What should you do?","question_images":[],"unix_timestamp":1671920520},{"id":"I3QCNPUd7NlEUUSdQpHW","url":"https://www.examtopics.com/discussions/google/view/92707-exam-professional-cloud-database-engineer-topic-1-question/","answer_images":[],"discussion":[{"upvote_count":"8","timestamp":"1687638360.0","poster":"chelbsik","comment_id":"755204","content":"Selected Answer: B\nhttps://cloud.google.com/blog/topics/developers-practitioners/scheduling-cloud-sql-exports-using-cloud-functions-and-cloud-scheduler"},{"content":"Selected Answer: B\nhttps://cloud.google.com/sql/docs/mysql/backup-recovery/scheduling-backups#creating_a_pub_sub_topic_a_cloud_function_and_a_cloud_scheduler_job","comment_id":"1204126","upvote_count":"2","poster":"Pime13","timestamp":"1730227140.0"},{"timestamp":"1714860720.0","poster":"nhiguchi","content":"Selected Answer: C\nC is correct.","upvote_count":"2","comment_id":"1062486"},{"content":"The most accurate answer based on Google-recommended practices would be C","upvote_count":"1","poster":"learnazureportal","comment_id":"1016473","timestamp":"1711351980.0"},{"upvote_count":"2","poster":"KennyHuang","content":"Selected Answer: C\nC is the recommended choice as it leverages Cloud Composer's workflow orchestration capabilities to schedule and automate the export process. By calling the cloudsql.instances.export API within the workflow, you can ensure that the data is exported from Cloud SQL for MySQL in CSV format as needed by your data analysis team.","timestamp":"1701145920.0","comment_id":"908291"},{"poster":"KennyHuang","comment_id":"908285","content":"Selected Answer: C\nCloud Composer is a fully managed workflow orchestration service. It allows you to define and manage complex workflows using Apache Airflow. By using Cloud Composer, you can create a workflow that includes a task to export data from Cloud SQL for MySQL using the cloudsql.instances.export API. You can specify the export format as CSV to meet the requirement of your data analysis team. This approach provides a scalable and manageable solution for regular data exports.\nTherefore, option C is the recommended choice as it leverages Cloud Composer's workflow orchestration capabilities to schedule and automate the export process. By calling the cloudsql.instances.export API within the workflow, you can ensure that the data is exported from Cloud SQL for MySQL in CSV format as needed by your data analysis team.","timestamp":"1701145020.0","upvote_count":"1"},{"poster":"dynamic_dba","timestamp":"1694550420.0","comment_id":"837451","comments":[{"timestamp":"1710242340.0","comment_id":"1005578","upvote_count":"3","poster":"njda","content":"why will someone use 3 services when it can be done using 1 service only, in this case composer. Secondly export can be for multiple tables, and ,cloud functions has a execution time limit.?\nC still looks most appropriate solution."}],"upvote_count":"4","content":"B.\nPerforming a â€œSELECT * FROM TABLEâ€ wouldnâ€™t give you CSV output and that alone wouldnâ€™t call an API. Eliminate A. Thereâ€™s no need for Cloud Composer in this scenario, especially when the solution is a known combination of Cloud Scheduler, Cloud Function and a Pub/Sub Topic, which is B."},{"comment_id":"830909","content":"Selected Answer: B\nhttps://cloud.google.com/blog/topics/developers-practitioners/scheduling-cloud-sql-exports-using-cloud-functions-and-cloud-scheduler","upvote_count":"2","poster":"Nirca","timestamp":"1694003760.0"},{"content":"what if the query is large, Cloud Function has limit? shouldn't it be C?","comment_id":"774251","poster":"[Removed]","upvote_count":"2","timestamp":"1689230520.0"},{"upvote_count":"3","timestamp":"1688073960.0","content":"As noted in text from this link, Cloud Function can be triggered with/without pub/sub. I think distinction between A & B is suggestion in A the API s called from query, which is not correct - the 'select query' is passed as parameter in call, so I think B is correct.\n\n\"Note that we could also have a Pub/Sub Trigger configured Cloud Function get triggered by the Scheduler by using the same steps given above to create a job. Except that in the target, you will select Pub/Sub and provide the Pub/Sub Topic name. At the scheduler trigger time, the Cloud Scheduler will publish a message to the topic with the message body that you specify.\"\n\nhttps://rominirani.com/google-cloud-functions-tutorial-using-the-cloud-scheduler-to-trigger-your-functions-756160a95c43","comment_id":"762359","poster":"sp57"},{"poster":"GCP72","comment_id":"759487","content":"Selected Answer: A\nA is the correct answer, PUSUB is not required for this task","comments":[{"upvote_count":"1","poster":"cardareel","comment_id":"1008627","content":"Doing a SELECT * won't call the API. Pub/Sub is not needed, you are correct, the problem is the SELECT * (nothing to do there).","timestamp":"1710532500.0"}],"upvote_count":"3","timestamp":"1687929600.0"},{"comments":[{"content":"The answer is letter B. Google recomends this pattern to perform this task: https://cloud.google.com/sql/docs/mysql/backup-recovery/scheduling-backups","comment_id":"817263","upvote_count":"2","timestamp":"1692651660.0","poster":"gabrielosluz"}],"comment_id":"755734","poster":"pk349","upvote_count":"2","content":"A: Use Cloud Scheduler to trigger a Cloud Function that will run a select * from table(s) query to call the cloudsql.instances.export *** API.","timestamp":"1687697460.0"}],"unix_timestamp":1671920760,"exam_id":5,"question_images":[],"question_text":"You work in the logistics department. Your data analysis team needs daily extracts from Cloud SQL for MySQL to train a machine learning model. The model will be used to optimize next-day routes. You need to export the data in CSV format. You want to follow Google-recommended practices. What should you do?","answers_community":["B (60%)","C (25%)","A (15%)"],"timestamp":"2022-12-24 23:26:00","isMC":true,"answer_description":"","question_id":73,"answer_ET":"B","answer":"B","topic":"1","choices":{"A":"Use Cloud Scheduler to trigger a Cloud Function that will run a select * from table(s) query to call the cloudsql.instances.export API.","B":"Use Cloud Scheduler to trigger a Cloud Function through Pub/Sub to call the cloudsql.instances.export API.","D":"Use Cloud Composer to execute a select * from table(s) query and export results.","C":"Use Cloud Composer to orchestrate an export by calling the cloudsql.instances.export API."}},{"id":"eCDvj4Af6MKSGbhqs6k1","answers_community":["D (92%)","8%"],"answer_images":[],"question_text":"You are choosing a database backend for a new application. The application will ingest data points from IoT sensors. You need to ensure that the application can scale up to millions of requests per second with sub-10ms latency and store up to 100 TB of history. What should you do?","timestamp":"2022-12-20 23:57:00","url":"https://www.examtopics.com/discussions/google/view/92248-exam-professional-cloud-database-engineer-topic-1-question/","choices":{"D":"Use Bigtable, and add nodes as necessary to achieve the required throughput.","A":"Use Cloud SQL with read replicas for throughput.","C":"Use Memorystore for Memcached, and add nodes as necessary to achieve the required throughput.","B":"Use Firestore, and rely on automatic serverless scaling."},"discussion":[{"comment_id":"751606","upvote_count":"7","poster":"fredcaram","content":"Selected Answer: D\nThis is a bigtable use case","timestamp":"1671577020.0"},{"content":"point to consider is :\nIOT data,\nscaling for accepting millions of request.\nstoring historic data of 100TB.\nBig Table is used for low latency but its for operational workload and \nwe need Fully managed Redis and Memcached for sub-millisecond data access.\n\nthat's the only argument here.\n\ni think option c is correct.","poster":"hussain.sain","comments":[{"content":"https://medium.com/google-cloud/which-database-should-i-choose-44be039179ea","upvote_count":"1","poster":"hussain.sain","timestamp":"1718637240.0","comment_id":"1231986"}],"upvote_count":"1","comment_id":"1231985","timestamp":"1718637180.0"},{"timestamp":"1695718380.0","poster":"theseawillclaim","upvote_count":"1","content":"Selected Answer: D\nKey is 100TB of historical data.\nBigTable is the only service (among the mentioned) that is able to handle that load within the required latency.","comment_id":"1017565"},{"timestamp":"1687114680.0","comment_id":"926867","poster":"fsmathias","content":"Selected Answer: C\nCorrect answer is C","upvote_count":"1"},{"comment_id":"837435","timestamp":"1678657980.0","upvote_count":"2","content":"D.\nSimply a classic use case for Bigtable. Neither Cloud SQL, Firestore nor Memorystore have either the capacity or latency to provide the solution.","poster":"dynamic_dba"},{"poster":"lepach","timestamp":"1673030940.0","comment_id":"767980","upvote_count":"1","comments":[{"content":"You just forgot the 100TB of data that needs to be stored ðŸ™","timestamp":"1682587140.0","comment_id":"882444","poster":"felipeschossler","upvote_count":"2"}],"content":"https://cloud.google.com/memorystore/docs/redis/redis-overview#what_its_good_for\nWhat it's good for:\n\nStream Processing: Whether processing a Twitter feed or stream of data from IoT devices, Memorystore for Redis is a perfect fit for streaming solutions.\nCorrect answer is C"},{"content":"Selected Answer: D\nC is not a correct answer, D- Bigtable is correct choice for this user case.\nhttps://cloud.google.com/memorystore/docs/redis/redis-overview","comment_id":"759520","poster":"GCP72","timestamp":"1672213560.0","upvote_count":"2"},{"poster":"range9005","upvote_count":"2","timestamp":"1671665040.0","comment_id":"752857","content":"Selected Answer: D\nIOT -->> Bigtable"},{"upvote_count":"3","timestamp":"1671649020.0","content":"Answer is D","comment_id":"752653","poster":"Kloudgeek"}],"exam_id":5,"isMC":true,"topic":"1","unix_timestamp":1671577020,"question_id":74,"answer_description":"","answer":"D","question_images":[],"answer_ET":"D"},{"id":"6liV1rebLzIajLQgZWeS","discussion":[{"comment_id":"1258918","upvote_count":"1","content":"Selected Answer: A\nA, failt region","timestamp":"1722434280.0","poster":"zeta_xs"},{"poster":"dynamic_dba","comment_id":"837368","upvote_count":"4","content":"A.\nAvoiding user disruption if a regional failure occurs means you need to pick a multi-region service. That rules out C and D. Having more control over the EKs means CMEK. That eliminates B.","timestamp":"1678652700.0"},{"comment_id":"759527","content":"Selected Answer: A\nA is the correct answer because \" you want to control where you store the encryption key\"","timestamp":"1672213740.0","poster":"GCP72","upvote_count":"4"},{"timestamp":"1671979740.0","content":"C: Use Cloud SQL with a customer-managed encryption key (CMEK).","upvote_count":"1","poster":"pk349","comment_id":"755731"},{"poster":"chelbsik","comment_id":"755215","content":"Selected Answer: A\nA for me: it's A or C because we want to control keys, but C would cause downtime, since we would need to manually failover to another region if a regional failure occurs, and we don't want that.","upvote_count":"2","timestamp":"1671921600.0"},{"timestamp":"1671766860.0","upvote_count":"1","comment_id":"753829","content":"Selected Answer: A\nCorrect Answer - A","poster":"jitu028"},{"comment_id":"752860","upvote_count":"1","timestamp":"1671665280.0","comments":[{"comment_id":"783913","timestamp":"1674359880.0","content":"Yes default encryption comes with AES-256 but the question states that you need to control where you store the encryption keys. that can be achieved by CMEK.","poster":"Tharun1125438","upvote_count":"1"}],"poster":"range9005","content":"Selected Answer: B\nI guess B\nSince Google cloud default encryption comes with AES-256 encryption"},{"timestamp":"1671577080.0","poster":"fredcaram","content":"A and C would work for this scenario","comment_id":"751607","upvote_count":"2","comments":[{"poster":"chelbsik","upvote_count":"2","timestamp":"1671921480.0","comment_id":"755213","content":"Right, but C would cause downtime, since you would need to manually failover to another region"}]}],"question_text":"You are designing a payments processing application on Google Cloud. The application must continue to serve requests and avoid any user disruption if a regional failure occurs. You need to use AES-256 to encrypt data in the database, and you want to control where you store the encryption key. What should you do?","question_images":[],"question_id":75,"exam_id":5,"answer_ET":"A","answer":"A","timestamp":"2022-12-20 23:58:00","answer_images":[],"topic":"1","answer_description":"","url":"https://www.examtopics.com/discussions/google/view/92249-exam-professional-cloud-database-engineer-topic-1-question/","answers_community":["A (89%)","11%"],"unix_timestamp":1671577080,"choices":{"A":"Use Cloud Spanner with a customer-managed encryption key (CMEK).","C":"Use Cloud SQL with a customer-managed encryption key (CMEK).","D":"Use Bigtable with default encryption.","B":"Use Cloud Spanner with default encryption."},"isMC":true}],"exam":{"isImplemented":true,"isMCOnly":true,"isBeta":false,"lastUpdated":"11 Apr 2025","numberOfQuestions":132,"provider":"Google","name":"Professional Cloud Database Engineer","id":5},"currentPage":15},"__N_SSP":true}