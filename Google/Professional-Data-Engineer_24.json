{"pageProps":{"questions":[{"id":"FUsyMY5JE2SkrinsPVO0","question_images":[],"question_text":"Your platform on your on-premises environment generates 100 GB of data daily, composed of millions of structured JSON text files. Your on-premises environment cannot be accessed from the public internet. You want to use Google Cloud products to query and explore the platform data. What should you do?","answers_community":["C (90%)","10%"],"exam_id":11,"answer_images":[],"answer":"C","discussion":[{"comment_id":"878814","upvote_count":"10","comments":[{"timestamp":"1705306980.0","upvote_count":"1","content":"With BigQuery Data Transfer Service we can copy files not only from other BigQuery, but also a bunch of cloud services listed here: \nhttps://cloud.google.com/bigquery/docs/dts-introduction\nBut you are right. It wont work with on-premises.","poster":"datapassionate","comment_id":"1123179"}],"poster":"muhusman","content":"Therefore, the correct option is C. Use Transfer Service for on-premises data to copy data from your on-premises environment to Cloud Storage. Use the BigQuery Data Transfer Service to import data into BigQuery.\n\nOption A is incorrect because Cloud Scheduler is not designed for data transfer, but rather for scheduling the execution of Cloud Functions, Cloud Run, or App Engine applications.\n\nOption B is incorrect because Transfer Appliance is designed for large-scale data transfers from on-premises environments to Google Cloud and is not suitable for transferring data on a daily basis.\n\nOption D is also incorrect because the BigQuery Data Transfer Service dataset copy feature is designed for copying datasets between BigQuery projects and not suitable for copying data from on-premises environments to BigQuery.","timestamp":"1682284680.0"},{"poster":"cetanx","timestamp":"1687861920.0","content":"Selected Answer: C\n\"Your on-premises environment cannot be accessed from the public internet\" statement suggests that inbound traffic from internet is NOT allowed however, it doesn't mean that outbound internet connectivity from on-prem resources is not possible. Any on-prem system with outbound internet access can copy/transfer the CSV files.\n\nCSV files are located on a filesystem, therefore you cannot copy them with BQ Transfer Service.\n\nLeaving only possible option;\nfirst copy CSVs to cloud storage\nthen run BQ Transfer Service\n\npls refer to https://cloud.google.com/bigquery/docs/dts-introduction#supported_data_sources","comment_id":"935242","upvote_count":"7"},{"upvote_count":"1","poster":"desertlotus1211","comment_id":"1401726","content":"Selected Answer: C\nI'm torn on this question. Okay no access from public internet... does that mean they don't have private lines (e.g. Ded/Partner interconnects)?\n\nPoorly worded. IMO it can either be: Answer B or C based on interpretation of Public Internet.","timestamp":"1742600580.0"},{"timestamp":"1736358540.0","comment_id":"1338036","content":"Selected Answer: B\nI vote B, because in \"Your on-premises environment cannot be accessed from the public internet.\", it would only allow data to be extracted internally within the company. So Transfer Appliance is the most appropriate tool.","upvote_count":"1","poster":"marlon.andrei"},{"upvote_count":"1","content":"Selected Answer: C\nTransfer Service for on-premises data is designed specifically for this scenario. It uses a private, secure agent-based approach to move data from on-premises environments to Google Cloud Storage.","comments":[{"timestamp":"1734511140.0","upvote_count":"1","poster":"namesgeo","content":"https://cloud.google.com/blog/products/storage-data-transfer/introducing-storage-transfer-service-for-on-premises-data?hl=en","comment_id":"1328351"}],"timestamp":"1734511080.0","poster":"namesgeo","comment_id":"1328350"},{"comment_id":"1293085","poster":"baimus","content":"They don't define \"cannot be accessed from the public internet\" - does this mean no incoming traffic, or no traffic or any kind regardless of the initiation point? We simply do not know, and so are left guessing. C? Probably, but could be B, just depending.","upvote_count":"1","timestamp":"1728035520.0"},{"content":"Selected Answer: C\nthe correct option is C","upvote_count":"2","timestamp":"1686741540.0","poster":"Takshashila","comment_id":"923034"},{"comment_id":"833884","poster":"wjtb","content":"I would say B. It is the ONLY option that is possible without data being accessible over the public (unless we assume that a direct interconnect is already set up, which seems farfetched). Also, nowhere does it say how up-to-date the data needs to be that we are querying or how often we need to query, only that the data increases in size by 100gb per day (indicating that its going to be a lot of data)","timestamp":"1678360140.0","upvote_count":"3"},{"poster":"musumusu","comment_id":"815162","content":"Answer C,\nWhat is wrong with B ? Key words = Daily transfer .. so no to transfer appliance,","upvote_count":"2","timestamp":"1676891940.0"},{"comment_id":"729251","content":"Selected Answer: C\nC is the answer.\n\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#storage-transfer-service-for-large-transfers-of-on-premises-data\nStorage Transfer Service for on-premises data enables transfers from network file system (NFS) storage to Cloud Storage.\n\nhttps://cloud.google.com/bigquery/docs/cloud-storage-transfer-overview\nThe BigQuery Data Transfer Service for Cloud Storage lets you schedule recurring data loads from Cloud Storage buckets to BigQuery.","poster":"zellck","upvote_count":"3","timestamp":"1669647780.0","comments":[{"timestamp":"1672621860.0","poster":"AzureDP900","upvote_count":"1","content":"yes, It is C","comment_id":"763428"}]},{"content":"C\nD-no answer because bq transfer service don't support from on-prem","comments":[{"poster":"Atnafu","content":"B-is not answer because you want transfer appliance for one time bulk transfer but the question is You want to use Google Cloud products to query and explore the platform data. \n\n query and explore is the key","upvote_count":"1","comment_id":"727063","timestamp":"1669406880.0"}],"timestamp":"1669275780.0","upvote_count":"1","poster":"Atnafu","comment_id":"725635"},{"content":"Selected Answer: C\nTransfer Service for on-premises is optimal for on-premises google ( large files (< 1 TB) and bandwidth available and scheduling)\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#transfer-options\nhttps://cloud.google.com/blog/products/storage-data-transfer/introducing-storage-transfer-service-for-on-premises-data\n\nBigQuery Data Transfer Service is good for gcs to bigquery\nhttps://cloud.google.com/bigquery/docs/cloud-storage-transfer","comment_id":"686267","poster":"John_Pongthorn","upvote_count":"1","comments":[{"poster":"John_Pongthorn","content":"Sorry I am wrong \n ( large files > 1 TB + bandwidth available on internal IP address communication + daily scheduling)","upvote_count":"1","comment_id":"686269","timestamp":"1664895900.0"},{"poster":"John_Pongthorn","timestamp":"1664895780.0","comment_id":"686268","content":"Your on-premises environment cannot be accessed from the public internet.\nIt signifies that we can apply private connection like Cloud Interconnect https://cloud.google.com/network-connectivity/docs/interconnect/concepts/overview","upvote_count":"2"}],"timestamp":"1664895780.0"},{"poster":"Wasss123","content":"Selected Answer: C\nI will go with C","upvote_count":"3","timestamp":"1663107420.0","comment_id":"668445"},{"comment_id":"665459","content":"I will g with C\n\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#transfer-options","upvote_count":"1","poster":"MounicaN","timestamp":"1662819000.0"},{"timestamp":"1662641700.0","comment_id":"663630","content":"C is correct, b is suitable for weekly .\nhttps://cloud.google.com/transfer-appliance/docs/4.0/overview","comments":[{"timestamp":"1664895300.0","upvote_count":"1","poster":"John_Pongthorn","comment_id":"686264","content":"C\nYour on-premises environment cannot be accessed from the public internet. \nIt signifies that we can apply private connection like Cloud Interconnect https://cloud.google.com/network-connectivity/docs/interconnect/concepts/overview"}],"poster":"John_Pongthorn","upvote_count":"2"},{"timestamp":"1662610500.0","content":"Selected Answer: C\nAns C\nhttps://cloud.google.com/storage-transfer/docs/on-prem-agent-best-practices","upvote_count":"1","poster":"TNT87","comment_id":"663088"},{"content":"I would go with option C. \nYou need a service to transfer data from on-premises to cloud storage. so \"Transfer service\" is the best option & additionally you can easily configure the network so that data flows through private network. \n\ncloud scheduler on other hand is used mostly for automation. You can schedule a service but in my view cannot be used solo to transfer data.","comment_id":"662637","timestamp":"1662563580.0","poster":"HarshKothari21","upvote_count":"1"},{"content":"Data is generated daily. Unlikely to ship Transfer Appliance every day.\n\nVote for C instead. \"Transfer Service for on-premises data is a free Google Cloud service that's intended to streamline the process of uploading data into Google Cloud Storage buckets\"\n\nhttps://cloud.google.com/blog/products/storage-data-transfer/introducing-storage-transfer-service-for-on-premises-data","poster":"nwk","timestamp":"1662356640.0","comment_id":"659764","upvote_count":"2"},{"content":"Selected Answer: B\nB. Use a Transfer Appliance to copy data from your on-premises environment to Cloud Storage. Use the BigQuery Data Transfer Service to import data into BigQuery.","comment_id":"657979","timestamp":"1662171300.0","poster":"ducc","upvote_count":"1"}],"isMC":true,"answer_description":"","unix_timestamp":1662171300,"question_id":116,"topic":"1","timestamp":"2022-09-03 04:15:00","url":"https://www.examtopics.com/discussions/google/view/79652-exam-professional-data-engineer-topic-1-question-202/","answer_ET":"C","choices":{"C":"Use Transfer Service for on-premises data to copy data from your on-premises environment to Cloud Storage. Use the BigQuery Data Transfer Service to import data into BigQuery.","A":"Use Cloud Scheduler to copy data daily from your on-premises environment to Cloud Storage. Use the BigQuery Data Transfer Service to import data into BigQuery.","B":"Use a Transfer Appliance to copy data from your on-premises environment to Cloud Storage. Use the BigQuery Data Transfer Service to import data into BigQuery.","D":"Use the BigQuery Data Transfer Service dataset copy to transfer all data into BigQuery."}},{"id":"nwrQHp83lsDacG0bAavl","question_id":117,"answer_ET":"C","question_text":"A TensorFlow machine learning model on Compute Engine virtual machines (n2-standard-32) takes two days to complete training. The model has custom TensorFlow operations that must run partially on a CPU. You want to reduce the training time in a cost-effective manner. What should you do?","answers_community":["C (100%)"],"isMC":true,"answer_description":"","question_images":[],"answer":"C","topic":"1","choices":{"C":"Train the model using a VM with a GPU hardware accelerator.","D":"Train the model using a VM with a TPU hardware accelerator.","B":"Change the VM type to e2-standard-32.","A":"Change the VM type to n2-highmem-32."},"exam_id":11,"unix_timestamp":1669727700,"url":"https://www.examtopics.com/discussions/google/view/89246-exam-professional-data-engineer-topic-1-question-203/","answer_images":[],"timestamp":"2022-11-29 14:15:00","discussion":[{"comment_id":"1103573","upvote_count":"5","poster":"MaxNRG","content":"Selected Answer: C\nThe best way to reduce the TensorFlow training time in a cost-effective manner is to use a VM with a GPU hardware accelerator. TensorFlow can take advantage of GPUs to significantly speed up training time for many models.\n\nSpecifically, option C is the best choice.\n\nChanging the VM to another standard type like n2-highmem-32 or e2-standard-32 (options A and B) may provide some improvement, but likely not a significant speedup.\n\nUsing a TPU (option D) could speed up training, but TPUs are more costly than GPUs. For a cost-effective solution, GPU acceleration provides the best performance per dollar.\n\nSince the model must run partially on CPUs, a VM instance with GPUs added will allow TensorFlow to offload appropriate operations to the GPUs while keeping CPU-specific operations on the CPU. This can provide a significant reduction in training time for many common TensorFlow models while keeping costs reasonable","timestamp":"1719070320.0"},{"upvote_count":"5","timestamp":"1687060080.0","poster":"jkhong","comment_id":"748626","content":"Selected Answer: C\nCost effective - among the choices, it is cheaper to have a temporary accelerator instead of increasing our VM cost for an indefinite amount of time\nD -> TPU accelerator cannot support custom operations"},{"timestamp":"1730810880.0","poster":"wences","comment_id":"1206845","upvote_count":"1","content":"Selected Answer: C\nkey pjrse is \"run partially on a CPU\" from https://cloud.google.com/tpu/docs/intro-to-tpu#when_to_use_tpus refers to GPU"},{"upvote_count":"4","poster":"spicebits","comment_id":"1064295","timestamp":"1715025420.0","content":"Selected Answer: C\nhttps://cloud.google.com/tpu/docs/intro-to-tpu#when_to_use_tpus"},{"upvote_count":"1","timestamp":"1688253120.0","comment_id":"763429","content":"C. Train the model using a VM with a GPU hardware accelerator.","poster":"AzureDP900"},{"content":"C\nhttps://cloud.google.com/tpu/docs/tpus#when_to_use_tpus:~:text=Models%20with%20a%20significant%20number%20of%20custom%20TensorFlow%20operations%20that%20must%20run%20at%20least%20partially%20on%20CPUs","comment_id":"732031","poster":"Atnafu","upvote_count":"1","timestamp":"1685477100.0","comments":[{"comment_id":"746942","timestamp":"1686895680.0","poster":"Atnafu","upvote_count":"3","content":"The model has custom TensorFlow operations that must run partially on a CPU. is the key for GPU"}]},{"comment_id":"730593","timestamp":"1685368860.0","poster":"zellck","upvote_count":"4","content":"Selected Answer: C\nC is the answer.\n\nhttps://cloud.google.com/tpu/docs/tpus#when_to_use_tpus\nGPUs\n- Models with a significant number of custom TensorFlow operations that must run at least partially on CPUs"},{"comments":[{"comment_id":"732462","content":"C is not cost-effective, so I stand corrected. I do not know the answer.","timestamp":"1685605740.0","upvote_count":"1","poster":"gudiking"}],"timestamp":"1685358900.0","content":"Selected Answer: C\nI agree with C, for choosing a GPU one of the cases says:\n\"Models with a significant number of custom TensorFlow operations that must run at least partially on CPUs\"\nhttps://cloud.google.com/tpu/docs/tpus#when_to_use_tpus","upvote_count":"1","comment_id":"730424","poster":"gudiking"}]},{"id":"g5NCwo4QP6obAXvrEc91","question_images":[],"answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/89456-exam-professional-data-engineer-topic-1-question-204/","question_id":118,"question_text":"You want to create a machine learning model using BigQuery ML and create an endpoint for hosting the model using Vertex AI. This will enable the processing of continuous streaming data in near-real time from multiple vendors. The data may contain invalid values. What should you do?","answers_community":["D (100%)"],"choices":{"A":"Create a new BigQuery dataset and use streaming inserts to land the data from multiple vendors. Configure your BigQuery ML model to use the \"ingestion\" dataset as the framing data.","C":"Create a Pub/Sub topic and send all vendor data to it. Connect a Cloud Function to the topic to process the data and store it in BigQuery.","B":"Use BigQuery streaming inserts to land the data from multiple vendors where your BigQuery dataset ML model is deployed.","D":"Create a Pub/Sub topic and send all vendor data to it. Use Dataflow to process and sanitize the Pub/Sub data and stream it to BigQuery."},"timestamp":"2022-11-30 23:08:00","answer_ET":"D","unix_timestamp":1669846080,"discussion":[{"comment_id":"1244970","poster":"anyone_99","timestamp":"1720538100.0","content":"Why is the answer A? After paying $44 I am getting wrong answers.","comments":[{"comment_id":"1253310","poster":"987af6b","content":"The discussion is where the real answer is.","timestamp":"1721687280.0","upvote_count":"2"}],"upvote_count":"1"},{"timestamp":"1705178160.0","comment_id":"1122035","poster":"Matt_108","upvote_count":"2","content":"Selected Answer: D\nOption D"},{"upvote_count":"2","comment_id":"960761","poster":"vamgcp","timestamp":"1690139280.0","content":"Selected Answer: D\nOption D -Dataflow provides a scalable and flexible way to process and clean the incoming data in real-time before loading it into BigQuery."},{"content":"D. Create a Pub/Sub topic and send all vendor data to it. Use Dataflow to process and sanitize the Pub/Sub data and stream it to BigQuery.","timestamp":"1672621980.0","comment_id":"763431","upvote_count":"1","poster":"AzureDP900"},{"comment_id":"739979","poster":"odacir","upvote_count":"2","timestamp":"1670580480.0","content":"Selected Answer: D\nD is the best option to sanitize the data to its D"},{"content":"Selected Answer: D\nBetter to use pubsub for streaming and reading message data\n\nDataflow ParDo can perform filtering of data","timestamp":"1670146260.0","comment_id":"734966","poster":"jkhong","upvote_count":"2"},{"comment_id":"734415","content":"Selected Answer: D\nD is the answer.","poster":"zellck","upvote_count":"1","timestamp":"1670068860.0"},{"poster":"vidts","upvote_count":"2","comment_id":"732427","content":"Selected Answer: D\nIt's D","timestamp":"1669884960.0"},{"poster":"Atnafu","comment_id":"732037","timestamp":"1669846080.0","content":"Answer is D","upvote_count":"2"}],"topic":"1","answer_description":"","answer":"D","isMC":true,"exam_id":11},{"id":"fNlsoTfh7huB5WYQBrat","choices":{"C":"Use GKE to autoscale containers, and use gcloud commands to provision the infrastructure.","A":"Use Compute Engine startup scripts to pull container images, and use gcloud commands to provision the infrastructure.","B":"Use Cloud Build to schedule a job using Terraform build to provision the infrastructure and launch with the most current container images.","D":"Use Dataflow to provision the data pipeline, and use Cloud Scheduler to run the job."},"answer_images":[],"answers_community":["B (100%)"],"topic":"1","question_id":119,"answer_ET":"B","question_text":"You have a data processing application that runs on Google Kubernetes Engine (GKE). Containers need to be launched with their latest available configurations from a container registry. Your GKE nodes need to have GPUs, local SSDs, and 8 Gbps bandwidth. You want to efficiently provision the data processing infrastructure and manage the deployment process. What should you do?","exam_id":11,"timestamp":"2022-11-30 23:10:00","question_images":[],"unix_timestamp":1669846200,"isMC":true,"answer_description":"","discussion":[{"comments":[{"comment_id":"1103586","content":"• Cloud Build allows you to automate the building, testing, and deployment of your application using Docker containers.\n• Using Terraform with Cloud Build provides Infrastructure as Code capabilities to provision the GKE cluster with GPUs, SSDs, and network bandwidth.\n• Terraform can be configured to pull the latest container images from the registry when deploying.\n• Cloud Build triggers provide event-based automation to rebuild and redeploy when container images are updated.\n• This provides an automated CI/CD pipeline to launch the application on GKE using the desired infrastructure and latest images.\n• Dataflow and Cloud Scheduler don't directly provide infrastructure provisioning or deployment orchestration for GKE.\n• gcloud commands can be used but don't provide the same automation benefits as Cloud Build + Terraform.","poster":"MaxNRG","timestamp":"1703266980.0","upvote_count":"2","comments":[{"content":"So using Cloud Build with Terraform templates provides the most efficient way to provision and deploy this data processing application on GKE.","timestamp":"1703305020.0","poster":"MaxNRG","upvote_count":"2","comment_id":"1103587"}]}],"content":"Selected Answer: B\nB is the best option to efficiently provision and manage the deployment process for this data processing application on GKE:","timestamp":"1703266980.0","upvote_count":"5","poster":"MaxNRG","comment_id":"1103585"},{"upvote_count":"1","comment_id":"1304169","timestamp":"1730146920.0","poster":"SamuelTsch","content":"Selected Answer: B\nI would go to option B. That is from my point of view a CI/CD question. Only B covers the deployement and set up the latest container image."},{"poster":"anyone_99","upvote_count":"2","content":"another wrong answer?","timestamp":"1720538280.0","comment_id":"1244971"},{"comment_id":"1112141","content":"Selected Answer: B\n- Dataflow is a fully managed service for stream and batch data processing and is well-suited for real-time data processing tasks like identifying longtail and outlier data points. \n- Using BigQuery as a sink allows to efficiently store the cleansed and processed data for further analysis and serving it to AI models.","timestamp":"1704220560.0","upvote_count":"2","poster":"raaad"},{"poster":"spicebits","content":"Selected Answer: B\nI don't really like B or C... but given the choices I would go with B.\nB-Use Cloud Build to schedule a job using Terraform build to provision the infrastructure and launch with the most current container images. {The Terraform command is Terraform Apply and not Terraform build, but also why not use gcloud container command instead of introducing 3rd party builder image?)... I don't like this choice but it is the best one.\nC. Use GKE to autoscale containers, and use gcloud commands to provision the infrastructure. {This doesn't handle the building of the infra, or the deployment of the latest images, this one is clearly wrong, not sure why it is marked as the right choice}","timestamp":"1699305360.0","upvote_count":"1","comment_id":"1064254"},{"upvote_count":"2","content":"Selected Answer: B\nB is correct","comment_id":"960753","timestamp":"1690138860.0","poster":"vamgcp"},{"comment_id":"870119","content":"Selected Answer: B\nB is correct","timestamp":"1681467840.0","poster":"whorillo","upvote_count":"1"},{"comment_id":"806611","timestamp":"1676223300.0","upvote_count":"1","content":"Selected Answer: B\nb is ok","poster":"charline"},{"upvote_count":"1","comment_id":"763432","poster":"AzureDP900","content":"B. Use Cloud Build to schedule a job using Terraform build to provision the infrastructure and launch with the most current container images.","timestamp":"1672622100.0"},{"poster":"zellck","content":"Selected Answer: B\nB is the answer.","timestamp":"1670234280.0","comment_id":"735823","upvote_count":"2"},{"upvote_count":"3","timestamp":"1670138880.0","comment_id":"734909","content":"Selected Answer: B\nMaybe B\nref: https://cloud.google.com/architecture/managing-infrastructure-as-code","poster":"hauhau"},{"upvote_count":"1","content":"C is correct answer","comment_id":"732040","timestamp":"1669846200.0","poster":"Atnafu","comments":[{"poster":"Atnafu","timestamp":"1669875600.0","content":"Sorry I meant B","comment_id":"732324","upvote_count":"3"}]}],"answer":"B","url":"https://www.examtopics.com/discussions/google/view/89458-exam-professional-data-engineer-topic-1-question-205/"},{"id":"7cFwLbilyvGfHi9yfeom","question_images":[],"answer_images":[],"question_id":120,"url":"https://www.examtopics.com/discussions/google/view/129853-exam-professional-data-engineer-topic-1-question-206/","question_text":"You need ads data to serve AI models and historical data for analytics. Longtail and outlier data points need to be identified. You want to cleanse the data in near-real time before running it through AI models. What should you do?","answers_community":["B (100%)"],"choices":{"B":"Use Dataflow to identify longtail and outlier data points programmatically, with BigQuery as a sink.","A":"Use Cloud Storage as a data warehouse, shell scripts for processing, and BigQuery to create views for desired datasets.","D":"Use Cloud Composer to identify longtail and outlier data points, and then output a usable dataset to BigQuery.","C":"Use BigQuery to ingest, prepare, and then analyze the data, and then run queries to create views."},"timestamp":"2023-12-30 09:24:00","answer_ET":"B","unix_timestamp":1703924640,"discussion":[{"timestamp":"1726226100.0","upvote_count":"2","content":"Selected Answer: B\nDataflow for Real-Time Processing: Dataflow allows you to process data in near-real time, making it well-suited for identifying longtail and outlier data points as they occur. You can use Dataflow to implement custom data cleansing and outlier detection algorithms that operate on streaming data.\n\nBigQuery as a Sink: Using BigQuery as a sink allows you to store the cleaned and processed data efficiently for further analysis or use in AI models. Dataflow can write the cleaned data to BigQuery tables, enabling seamless integration with downstream processes.","poster":"Y___ash","comment_id":"1172576"},{"poster":"JyoGCP","timestamp":"1723728660.0","content":"Selected Answer: B\nB. Use Dataflow to identify longtail and outlier data points programmatically, with BigQuery as a sink.","comment_id":"1151086","upvote_count":"1"},{"poster":"datapassionate","upvote_count":"1","timestamp":"1721025720.0","comment_id":"1123188","content":"Selected Answer: B\nB. Use Dataflow to identify longtail and outlier data points programmatically, with BigQuery as a sink."},{"timestamp":"1720851720.0","upvote_count":"1","poster":"Matt_108","comment_id":"1121412","content":"Selected Answer: B\nB: Dataflow, solves exactly the use case described"},{"comments":[{"upvote_count":"1","content":"So B is the best architecture here to meet the needs of near real-time cleansing, identification of longtail/outlier data points, and integration with BigQuery for serving AI models.","timestamp":"1720339560.0","comment_id":"1115703","poster":"MaxNRG"}],"upvote_count":"2","poster":"MaxNRG","timestamp":"1720339560.0","content":"Selected Answer: B\nB is the best option for cleansing the ads data in near real-time before running it through AI models.\nThe key reasons are:\n• Dataflow allows for stream processing of data in near real-time. This allows you to identify and cleanse longtail and outlier data points as the data is streamed in.\n• Dataflow has built-in capabilities for detecting and handling outliers and anomalies in streaming data. This makes it well-suited for programmatically identifying longtail and outlier data points.\n• Using BigQuery as the output sink allows the cleansed data to be immediatley available for analysis and serving to AI models. BigQuery can act as a serving layer for the models.\n• Options A, C, and D either don't provide real-time processing (A and C) or don't easily integrate with BigQuery for analysis and serving (D).","comment_id":"1115702"},{"upvote_count":"1","comment_id":"1112142","content":"Selected Answer: B\n- Dataflow is a fully managed service for stream and batch data processing and is well-suited for real-time data processing tasks like identifying longtail and outlier data points.\n- Using BigQuery as a sink allows to efficiently store the cleansed and processed data for further analysis and serving it to AI models.","timestamp":"1719938220.0","poster":"raaad"},{"poster":"e70ea9e","content":"Selected Answer: B\nReal-time Data Processing: Dataflow excels at handling large-scale, streaming data with low latency, enabling near-real-time cleansing.\nScalability: Easily scales to handle growing data volumes and processing needs.\nProgrammatic Data Cleaning: Allows you to write custom logic in Apache Beam for identifying longtail and outlier data points accurately and efficiently.\nIntegration with BigQuery: Seamless integration with BigQuery enables you to store cleansed data for AI model training and historical analytics.\nCost-Effective: Dataflow's pay-as-you-go model optimizes costs for real-time data processing.","upvote_count":"1","timestamp":"1719728640.0","comment_id":"1109522"}],"topic":"1","answer_description":"","answer":"B","isMC":true,"exam_id":11}],"exam":{"numberOfQuestions":319,"id":11,"name":"Professional Data Engineer","isBeta":false,"isImplemented":true,"lastUpdated":"11 Apr 2025","provider":"Google","isMCOnly":true},"currentPage":24},"__N_SSP":true}