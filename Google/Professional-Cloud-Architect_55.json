{"pageProps":{"questions":[{"id":"DCI5qwjHp2m1PO3pTw2m","unix_timestamp":1570857300,"isMC":true,"choices":{"B":"Use multiple Google Container Engine clusters running FTP servers located in different regions. Save the data to Multi-Regional buckets in US, EU, and Asia. Run the ETL process using the data in the bucket","A":"Use one Google Container Engine cluster of FTP servers. Save the data to a Multi-Regional bucket. Run the ETL process using data in the bucket","C":"Directly transfer the files to different Google Cloud Multi-Regional Storage bucket locations in US, EU, and Asia using Google APIs over HTTP(S). Run the ETL process using the data in the bucket","D":"Directly transfer the files to a different Google Cloud Regional Storage bucket location in US, EU, and Asia using Google APIs over HTTP(S). Run the ETL process to retrieve the data from each Regional bucket"},"url":"https://www.examtopics.com/discussions/google/view/6485-exam-professional-cloud-architect-topic-8-question-7/","topic":"8","timestamp":"2019-10-12 07:15:00","answer_ET":"D","answers_community":["D (68%)","C (32%)"],"answer":"D","answer_description":"","question_id":271,"question_images":[],"question_text":"To speed up data retrieval, more vehicles will be upgraded to cellular connections and be able to transmit data to the ETL process. The current FTP process is error-prone and restarts the data transfer from the start of the file when connections fail, which happens often. You want to improve the reliability of the solution and minimize data transfer time on the cellular connections.\nWhat should you do?","exam_id":4,"answer_images":[],"discussion":[{"timestamp":"1574113260.0","comment_id":"22564","content":"c)\nMulti-Region Name Multi-Region Description\nasia Data centers in Asia\neu Data centers in the European Union1\nus Data centers in the United States\n\n multi-region is a large geographic area, such as the United States, that contains two or more geographic places.","upvote_count":"39","poster":"dabrat"},{"upvote_count":"16","timestamp":"1575740400.0","poster":"JJu","comment_id":"27666","content":"I think answer is C.\nUse a multi-region when you want to serve content to data consumers that are outside of the Google network and distributed across large geographic areas, or when you want the higher availability that comes with being geo-redundant."},{"content":"Selected Answer: C\nmulti-region bucks are for continental customers which meets the requirements","timestamp":"1737135900.0","upvote_count":"1","poster":"Clouddude123","comment_id":"1342274"},{"upvote_count":"1","comment_id":"1326087","timestamp":"1734087660.0","content":"Selected Answer: C\nThey currently have over 500 dealers and service centers in 100 countries","poster":"valgorodetsky"},{"content":"Selected Answer: C\nOption C is the best choice. It leverages Google Cloud Multi-Regional Storage buckets, which ensures faster global access, improves reliability, and minimizes data transfer time, especially for cellular connections. Using Google APIs over HTTP(S) for transfer further improves the overall transfer speed and reliability, as HTTP(S) supports automatic retries and resumable uploads, which significantly reduces the impact of intermittent connectivity issues on data transfers.","poster":"Anj_li","timestamp":"1732204260.0","upvote_count":"1","comment_id":"1315884"},{"content":"Everyone voting C are missing the point, you are not serving the data, merely ingesting it.\nFor that , regional buckets provide better latency and bandwidth with lower costs.\nMulti-regions can be considered for redundancy in the case of regional failures , but it's costly and extremely unlikely for an entire region to go down , zonal replication is good enough in this case","upvote_count":"2","comment_id":"1256357","timestamp":"1722096840.0","poster":"Toothpick"},{"poster":"46f094c","timestamp":"1719677700.0","comments":[{"timestamp":"1738250760.0","content":"If you put the bucket to us-central, the latency is below 50 ms. Can be seen here https://cloud.google.com/network-intelligence-center/docs/performance-dashboard/how-to/view-google-cloud-latency#global-latency. I don't think that's a big problem.","comment_id":"1349128","upvote_count":"1","poster":"user263263"}],"content":"Selected Answer: C\nin option D imagine you have 1 regional bucket in US-west and the client is in US-east... latency problems right?\nwith multiregional you don't have the issue... I go for C","upvote_count":"2","comment_id":"1239351"},{"poster":"xaqanik","comment_id":"1168555","content":"Selected Answer: D\nGo for D. \nTelemetry data stored in regional (US, Europe and Asia) bucket.","upvote_count":"2","timestamp":"1709873940.0"},{"upvote_count":"3","content":"Selected Answer: C\nAt the beginning I was for B, since it was not clear for me that resumable uploads were available in cloud storage, but they are:\nhttps://cloud.google.com/storage/docs/resumable-uploads","timestamp":"1707902820.0","poster":"edoo","comment_id":"1149982"},{"timestamp":"1706034720.0","upvote_count":"1","comment_id":"1129903","content":"Selected Answer: C\nI would say it is C","poster":"didek1986"},{"comment_id":"1127811","poster":"35cd41b","upvote_count":"1","timestamp":"1705841580.0","content":"C is correct,"},{"timestamp":"1704011040.0","comment_id":"1110388","content":"Selected Answer: D\nThe real debate is C vs D. Lets look at some docs:\n\nLow cost is one of the business requirements. Also, regional has higher Performance than multi-regional \nhttps://cloud.google.com/storage/docs/locations#considerations\n\nMulti regional will be an overkill for analytics as mentioned by Google. \nhttps://cloud.google.com/storage/docs/locations#location_recommendations","poster":"[Removed]","upvote_count":"7"},{"content":"Selected Answer: C\nOption C seems right here. Directly transferring the files to a different Google Cloud Regional Storage bucket location in US, EU, and Asia using Google APIs over HTTP(S), would not be an effective solution as it would not improve the reliability of the solution. Using Google Cloud Multi-Regional Storage, which stores the data in multiple locations, would be a more reliable solution.","upvote_count":"3","timestamp":"1703697240.0","poster":"parthkulkarni998","comment_id":"1107029"},{"timestamp":"1693457220.0","comment_id":"994750","upvote_count":"2","poster":"jits1984","content":"Selected Answer: C\nMulti-region"},{"upvote_count":"2","content":"Selected Answer: D\njust look at this: https://cloud.google.com/storage/docs/locations#location_recommendations","poster":"rusll","timestamp":"1693337940.0","comment_id":"993469"},{"upvote_count":"1","comment_id":"945133","comments":[{"content":"Container engine which has recyclable pods as FTP server is far fetched. Compute engine and containers should not be used for storage. GCS bucket is better as files can be uploaded from anywhere and is reliable and fast.","upvote_count":"4","comment_id":"1057523","poster":"nideesh","timestamp":"1698659340.0"}],"content":"Can someone explain why A and B are incorrect ? All the responses focus on C and D.","timestamp":"1688694360.0","poster":"gotcertified"},{"upvote_count":"5","content":"Selected Answer: D\nD, multi-region increases latency","timestamp":"1687235580.0","comment_id":"928144","poster":"BiddlyBdoyng"}]},{"id":"wGXNFzjQtElS3OA6BAgy","unix_timestamp":1573810140,"question_text":"TerramEarth's 20 million vehicles are scattered around the world. Based on the vehicle's location, its telemetry data is stored in a Google Cloud Storage (GCS) regional bucket (US, Europe, or Asia). The CTO has asked you to run a report on the raw telemetry data to determine why vehicles are breaking down after 100 K miles. You want to run this job on all the data.\nWhat is the most cost-effective way to run this job?","discussion":[{"content":"I will look at it from a different perspective;\nA, B says \"move all data\" but analysis will try to reveal breaking down after 100K miles so there is no point of transferring data of the vehicles with less than 100K milage.\nTherefore, transferring all data is just waste of time and money.\n\nThere is one thing for sure here. If we move/copy data between continents it will cost us money therefore compressing the data before copying to another region/continent makes sense.\nPreprocessing also makes sense because we probably want to process smaller chunks of data first (remember 100K milage).\nSo now type of target bucket; multi-region or standard? multi-region is good for high-availability and low latency with a little more cost however question doesn't require any of these features.\nTherefore I think standard storage option is good to go given lower costs are always better.\n\nSo my answer would be D","comment_id":"129662","timestamp":"1625742060.0","poster":"cetanx","upvote_count":"66","comments":[{"upvote_count":"2","poster":"DiegoQ","content":"I totally agree with you, and I think that what confuse people here is the \"run a raw data\", but preprocess doesnÂ´t mean to mandatory transform raw data, it could be to only select the data that you need (as you said: vehicles with less than 100K milage)","timestamp":"1633066620.0","comment_id":"190740"},{"timestamp":"1649454000.0","content":"You will need data from non-broken machines too for labelling.","comments":[{"comment_id":"900942","timestamp":"1716021540.0","poster":"stfnz","upvote_count":"1","content":"yes, still you will be interested in 100K+ mileage, whether broken or not"}],"upvote_count":"1","poster":"mrhege","comment_id":"331567"}]},{"poster":"JoeShmoe","comments":[{"poster":"nitinz","upvote_count":"1","timestamp":"1646451720.0","comment_id":"303897","content":"It is D."},{"upvote_count":"2","comments":[{"comment_id":"293288","poster":"guid1984","timestamp":"1645182000.0","upvote_count":"2","content":"why not it's a RAW data, so can be pre-processed for optimization"}],"comment_id":"104275","content":"Hold on guys, you do not need to 'preprocess' the data. This rules out C,D.","poster":"Rafaa","timestamp":"1623038220.0"},{"comments":[{"upvote_count":"11","poster":"tartar","timestamp":"1628673060.0","comment_id":"155251","content":"D is ok"},{"upvote_count":"2","poster":"passnow","timestamp":"1608328440.0","comments":[{"poster":"vindahake","timestamp":"1615817460.0","content":"I think running additional compute regionally will be more expensive than data transfer charges and centrally processing them","comment_id":"64305","upvote_count":"4"}],"content":"Honestly, if we read the question well and factor in cost, D would be a better option","comment_id":"30783"}],"upvote_count":"1","comment_id":"30782","timestamp":"1608328320.0","content":"Dataproc can be use global end points too.","poster":"passnow"}],"timestamp":"1605432540.0","content":"D is the most cost effective and DataProc is regional","comment_id":"21726","upvote_count":"32"},{"content":"Selected Answer: C\nWhile regional preprocessing can be efficient, moving the data back to regional buckets after compression defeats the purpose of a multi-region bucket. It adds unnecessary data transfer costs and reduces the availability of the preprocessed data for global analysis.","poster":"msahdra","timestamp":"1733184240.0","comment_id":"1086487","upvote_count":"2"},{"timestamp":"1731325740.0","poster":"thewalker","comment_id":"1067806","content":"D\nConsidering https://cloud.google.com/storage/docs/locations#considerations","upvote_count":"2"},{"poster":"Jeena345","timestamp":"1706970660.0","comment_id":"797101","upvote_count":"1","content":"Selected Answer: D\nD should be fine"},{"content":"Answer is C\nTo run the report on all of the raw telemetry data for TerramEarth's vehicles in the most cost-effective way, it would be best to launch a cluster in each region to preprocess and compress the raw data. This will allow you to process the data in place, which will minimize the amount of data that needs to be transferred between regions. After the data has been preprocessed and compressed, you can then move it into a multi-region bucket and use a Dataproc cluster to finish the job.","upvote_count":"2","comment_id":"760014","timestamp":"1703776740.0","comments":[{"comment_id":"760016","poster":"omermahgoub","upvote_count":"1","content":"D, moving the data into a region bucket and using a Cloud Dataproc cluster to finish the job, would also not be as cost-effective as moving the data into a multi-region bucket, as it would not take advantage of the lower costs of storing data in a multi-region bucket.","timestamp":"1703776740.0"}],"poster":"omermahgoub"},{"upvote_count":"1","timestamp":"1699194780.0","content":"Selected Answer: D\nok for D","poster":"megumin","comment_id":"711817"},{"timestamp":"1697839020.0","content":"Selected Answer: D\nD seems better","poster":"Mahmoud_E","comment_id":"700330","upvote_count":"1"},{"content":"What is the use of Multi-Regional DataProc if ur Storage Data is Regional","poster":"AMohanty","comment_id":"643681","timestamp":"1691404680.0","upvote_count":"2"},{"timestamp":"1688436780.0","poster":"AzureDP900","upvote_count":"2","content":"D is fine, There is no need of multi-region as mentioned in C. D is right in my opinion.","comment_id":"626786"},{"content":"Selected Answer: D\nD is the correct answer. Regional bucket is required, since multi regional bucket will incur additional cost to transfer the data to a centralized location.","upvote_count":"2","timestamp":"1670728440.0","comment_id":"499062","poster":"vincy2202"},{"timestamp":"1670678160.0","upvote_count":"1","comment_id":"498648","poster":"vincy2202","content":"D seems to be the correct answer"},{"content":"Selected Answer: D\nvote D","poster":"joe2211","upvote_count":"2","comment_id":"487926","timestamp":"1669533420.0"},{"timestamp":"1666609260.0","comment_id":"466919","poster":"MaxNRG","upvote_count":"6","content":"D â Launch a cluster in each region to pre-process and compress the raw data, then move the data into a regional bucket and use Cloud Dataproc cluster.\nEgress rates are most important. It is free inside of region - so make sense to move all data into one region for processing/performance (from all continents). Cross-region cost is 0.01$ per GB, and inter-continent 0.12$ per GB. \nIf to consider just option B (moving all raw data into one region) then just monthly volume would cost:\n900 TB (all 20M units daily) 30 days 0.12 $ = 3.24 M $ (just for data transfer). So, it definitely makes sense to preprocess/compress data per region, and then move all that data into one region for final analysis. That would save up to 10-100 times on egress costs. Also, important aspect is processing time - running it in parallel on all regions accelerates overall analysis effort. Faster result - faster in-field improvements.\nLook this interesting video about price optimization in GCP (first 11.5 mins are about Storage/Network)\nhttps://cloud.google.com/storage/docs/locations#considerations"},{"poster":"victory108","timestamp":"1657863600.0","comment_id":"406804","content":"D. Launch a cluster in each region to preprocess and compress the raw data, then move the data into a region bucket and use a Cloud Dataproc cluster to finish the job","upvote_count":"1"},{"upvote_count":"3","timestamp":"1657215120.0","comment_id":"401139","content":"Answer is D","poster":"MamthaSJ"},{"content":"Answer D:\n\nmoving data from one region to another region will incur network egress cost. By compressing data and then moving would reduce this cost. Though running Dataproc for preprocessing in each region will incur additional cost but it will also reduce cost of running Dataproc job on all pre-processed data will also reduce cost offsetting additional cost of Dataproc cluster at regional level.","timestamp":"1654393620.0","comment_id":"374654","poster":"Yogikant","upvote_count":"1"},{"poster":"Pravin3c","upvote_count":"1","comment_id":"340741","timestamp":"1650596880.0","content":"Dataproc supports both a single \"global\" endpoint and regional endpoints based on Compute Engine zones."},{"poster":"Ausias18","content":"Answer is D","timestamp":"1648818000.0","comment_id":"325844","upvote_count":"1"},{"comment_id":"323367","poster":"lynx256","timestamp":"1648552380.0","content":"IMO - D is ok","upvote_count":"1"},{"comment_id":"310558","timestamp":"1647264660.0","poster":"pawel_ski","upvote_count":"1","content":"âits telemetry data is stored in a Google Cloud Storage (GCS) regional bucket (US, Europe, or Asia)â\nSo keeping data on a regional bucket requires one copy operation less."},{"poster":"cert2020","content":"Answer D, regional bucket and Cloud DataProc cluster certainly helps in a cost effective way.","upvote_count":"1","timestamp":"1645217520.0","comment_id":"293708"},{"content":"Although many people think D is right. I still think C make some sense too. \n1) DataProc can be global.\n2) global DP should be able to handle mutil regional buket better. In theory, it shoul avoid transferring data cross regions. It actually distribute the computation across regions. This will reduce the cost a lot.\nRemember, the data load and analytic need to be done every day.","poster":"bnlcnd","comment_id":"281607","timestamp":"1643774280.0","upvote_count":"3"},{"timestamp":"1643358180.0","poster":"ahmedemad3","upvote_count":"1","content":"ANS :D\n is the most cost effective and DataProc is regional","comment_id":"278384"},{"content":"I think C is more cost effective than D since moving from regional buckets to a multi-region bucket in the same continent cost nothing while moving from regional to regional in the same continent incurs cost. Cross continent data moves will incur charges regardless.","poster":"HKim","timestamp":"1641665280.0","comment_id":"262758","upvote_count":"2","comments":[{"comment_id":"522315","upvote_count":"1","content":"multi-regional bucket is more expensive than regional - 75% more...\nAdditionally, multi-regional is recommended for streaming videos or holding web static content.","poster":"pddddd","timestamp":"1673541960.0"}]},{"timestamp":"1638456660.0","comment_id":"233084","content":"D is right","poster":"Chulbul_Pandey","upvote_count":"1"},{"timestamp":"1637089860.0","comment_id":"220557","content":"right id D. Dataproc is Zone-specific resources.","poster":"zsylwek6","upvote_count":"1"},{"comment_id":"208078","poster":"AdityaGupta","upvote_count":"1","timestamp":"1635445080.0","content":"Moving data from a multi-regional bucket to a regional bucket incurs egress charges at a rate of $0.01/GB."},{"timestamp":"1634902320.0","comment_id":"204171","upvote_count":"1","content":"The right answer is D Benefits of regional endpoints:\nâ¢ If you use Dataproc in multiple regions, specifying a regional endpoint can provide better regional isolation and protection.\nâ¢ You may notice better performance by selecting regional endpoints, particularly based on geography, compared to the \"global\" multi-region namespace.\nâ¢ If you specify a regional endpoint when you create a cluster, you do not need to specify a zone within the region. Dataproc Auto Zone Placement will choose the zone for you.","poster":"homer_simpson"},{"poster":"Here4cloud","comment_id":"192171","comments":[{"timestamp":"1633259340.0","content":"Actually read somewhere that âInput into Dataproc or compute processingâ is a use case for regional storage class..so looks like D is correct.","comment_id":"192194","upvote_count":"2","poster":"Here4cloud"}],"content":"Why are we compressing and pre processing the data?","upvote_count":"1","timestamp":"1633258620.0"},{"poster":"RomiAwasthy","content":"C as all the data across regions need to be analyzed, so would need multi region cluster","timestamp":"1632332700.0","comment_id":"184641","upvote_count":"2"},{"content":"D) Is the right option because it is the most cost effective.\nIf you use the pricing calculator, 10 TB/Month of Regional is ~ USD 200 and Multi-Regional ~USD 260 (US vs us-central1)\nBecause we used the regional map-reduce aproach, we can use the region with most data to move the other regions' data to that region and then launch DataProc regional.\nThe cost of moving to a regional or multi-regional would be the same: if continents are crossed, you pay egress data.\nTo sum up: D) is cheaper, so D) is the right answer\nPlease notice that should the question state \"the most reliable\" or \"the less latency\", then multi-regional should be used.","timestamp":"1629360540.0","upvote_count":"4","comment_id":"161334","poster":"jespinosar"},{"poster":"saurabh1805","timestamp":"1626467640.0","comment_id":"136738","content":"in question CTO has asked report to be run on raw data hence C and D can never be correct answer. For me B seems to be correct answer as that is what i am using in my current project.","upvote_count":"2","comments":[{"content":"The CTO is not solution architect, so \"run a raw data\" is a solution in addressing a need/requirement.\n\nAs a solution architect, I would focus on the objective: \"finding out why vehicles are breaking down?\" So long the solution can achieve the result and better than the \"suggested\" solution, I will likely go for the better solution.","upvote_count":"2","comment_id":"188109","poster":"VedaSW","timestamp":"1632717480.0"}]},{"timestamp":"1625073000.0","comment_id":"123662","content":"I'm torn in between C and D - but realistically speaking I'd probably lean on Cloud Dataflow to perform Log ETL. I find that Dataproc might be overkill for this use case and, if this is something the c/x would want more often, Dataflow job is already written, etc...","poster":"cweather328","upvote_count":"1"},{"upvote_count":"2","timestamp":"1624523100.0","comment_id":"118239","content":"It should be D with the Regional bucket.","poster":"mlantonis"},{"content":"D would be more cost-effective. You can't afford to move that large of a dataset.","comment_id":"114871","timestamp":"1624205520.0","upvote_count":"2","comments":[{"upvote_count":"1","poster":"hafid","content":"you realize D is also move the data right \"then move the data into a region bucket and use a Cloud Dataproc cluster to finish the job\"","comments":[{"poster":"st003","upvote_count":"1","comment_id":"256113","timestamp":"1640935620.0","content":"move data in the same range is free..................\nCross range is paid"}],"timestamp":"1624926900.0","comment_id":"122323"}],"poster":"Mimgq"},{"timestamp":"1624115580.0","comment_id":"114056","upvote_count":"2","poster":"Mimgq","content":"This is B, there is nothing that says they pre-process the 120 fields"},{"comment_id":"97631","poster":"AD2AD4","content":"Final Decision to go with Option D","upvote_count":"2","timestamp":"1622213100.0"},{"upvote_count":"1","comment_id":"90787","timestamp":"1621275960.0","poster":"Jack_in_Large","content":"D). It really depends on who is audience of the analysis report. If it's only for the company, the choose the region the company located is the most cost-effective way."},{"comment_id":"88567","content":"Between C and D, i prefer D...\n\nSince as long as moving data outside the same continent regardless whether using region or multi-region will still charge with $$$... it is more cost saving if just use regional bucket to store data instead.","upvote_count":"1","timestamp":"1620951360.0","poster":"skywalker"},{"comment_id":"76119","upvote_count":"3","comments":[{"upvote_count":"5","comments":[{"timestamp":"1622481960.0","content":"https://cloud.google.com/storage/docs/locations","poster":"Rafaa","comment_id":"99461","upvote_count":"1"}],"comment_id":"99459","poster":"Rafaa","content":"I agree with you on removing C,D. I think it should be B because there is no 'Zone' as such with respect to GCS right?","timestamp":"1622481780.0"}],"content":"Question says - \"run a report on the raw telemetry data\". Option C & D are pre-processing the data which violates the principles laid out in the question - How about A? Move data into one zone and then do the needful.","poster":"PRC","timestamp":"1618762080.0"},{"timestamp":"1613878680.0","upvote_count":"2","content":"I think it is D\n\nWhen used in a region, Standard Storage is appropriate for storing data in the same location as Google Kubernetes Engine clusters or Compute Engine instances that use the data. Co-locating your resources maximizes the performance for data-intensive computations and can reduce network charges.\n\nWhen used in a dual-region, you still get optimized performance when accessing Google Cloud products that are located in one of the associated regions, but you also get the improved availability that comes from storing data in geographically separate locations.\n\nWhen used in a multi-region, Standard Storage is appropriate for storing data that is accessed around the world, such as serving website content, streaming videos, executing interactive workloads, or serving data supporting mobile and gaming applications.","poster":"KNG","comments":[{"upvote_count":"1","poster":"shashu07","content":"Answer D, not seem to be correct since telemetry data is stored in a Google Cloud Storage (GCS) regional bucket then why to move the data into a region bucket ...","timestamp":"1623897660.0","comment_id":"112076"}],"comment_id":"53262"},{"comments":[{"timestamp":"1620262740.0","comments":[{"poster":"ohcan","upvote_count":"2","timestamp":"1626989820.0","comment_id":"141410","content":"right, and they have 20 million vehicles"}],"content":"Moving data between region it's traffic, and it's not cheap.\nI think D.","upvote_count":"4","comment_id":"84374","poster":"Zarmi"}],"content":"I think that most cost-effective way to do it is moving all to ONE storage location so that only ONE cluster is deployed (cluster cost money, moving data between buckets dont)","poster":"Jos","upvote_count":"6","comment_id":"43370","timestamp":"1611768180.0"}],"answer":"D","exam_id":4,"topic":"8","answer_images":[],"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/8248-exam-professional-cloud-architect-topic-8-question-8/","answers_community":["D (78%)","C (22%)"],"timestamp":"2019-11-15 10:29:00","choices":{"D":"Launch a cluster in each region to preprocess and compress the raw data, then move the data into a region bucket and use a Cloud Dataproc cluster to finish the job","B":"Move all the data into 1 region, then launch a Google Cloud Dataproc cluster to run the job","A":"Move all the data into 1 zone, then launch a Cloud Dataproc cluster to run the job","C":"Launch a cluster in each region to preprocess and compress the raw data, then move the data into a multi-region bucket and use a Dataproc cluster to finish the job"},"answer_ET":"D","question_id":272,"question_images":[],"isMC":true},{"id":"cPC4ESLur2v0aJPzwbSk","question_text":"TerramEarth has equipped all connected trucks with servers and sensors to collect telemetry data. Next year they want to use the data to train machine learning models. They want to store this data in the cloud while reducing costs.\nWhat should they do?","url":"https://www.examtopics.com/discussions/google/view/8249-exam-professional-cloud-architect-topic-8-question-9/","question_id":273,"discussion":[{"poster":"JoeShmoe","upvote_count":"33","content":"D is most cost effective as don't want to use until 'next year'","timestamp":"1573810200.0","comments":[{"comment_id":"155256","content":"D is ok","upvote_count":"8","poster":"tartar","timestamp":"1597137480.0"},{"poster":"nitinz","upvote_count":"2","timestamp":"1614915960.0","comment_id":"303902","content":"D is most cost effective"},{"poster":"HCL","comment_id":"304654","content":"Hourly snapshots in answer D does not make any sense. \nThe answer is B.","timestamp":"1615031280.0","upvote_count":"1"}],"comment_id":"21727"},{"timestamp":"1743889140.0","poster":"samsonakala","content":"Selected Answer: D\nNext year? Obviously coldline (D)","comment_id":"1550953","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: A\nThey want to use the data next year, it means they still need to access the data for data early to perform data cleaning and prepare the data to access it next year","timestamp":"1743617580.0","comment_id":"1423479","poster":"Foxy2021"},{"content":"Selected Answer: A\nA next year...","timestamp":"1735256940.0","comment_id":"1332150","upvote_count":"1","poster":"rrope"},{"poster":"desertlotus1211","comment_id":"1321529","upvote_count":"1","content":"Selected Answer: A\nThe data is going to be used next year. Which mean infrequent. Coldline is long term storage that doesn't need to be accessed maybe once every 5 year. retrieval cost is high","timestamp":"1733256060.0"},{"content":"I hope this question isn't on the test. This quesiton is the most obnoxiously thoughtless and ill considered question I've come across and there are a lot of really bad ones on this test. I would probably pick D. Dataflow can be really expensive.","poster":"Sephethus","comment_id":"1244370","upvote_count":"1","timestamp":"1720447680.0"},{"upvote_count":"1","comment_id":"841441","content":"Selected Answer: D\nIf the words 'next year' wouldn't have been there then Big Table ð¯ . But as it will be required next year so Coldline bucket would be the most cost effective solution.","poster":"Deb2293","timestamp":"1679011620.0"},{"poster":"omermahgoub","content":"One option that TerramEarth could consider is storing the telemetry data in a Google Cloud Storage (GCS) Nearline bucket. This would allow them to store the data in the cloud at a lower cost than other storage options, while still providing quick access to the data when needed. By having the vehicle's computer compress the data in hourly snapshots, they can reduce the amount of storage needed and further reduce costs.","timestamp":"1672240860.0","upvote_count":"2","comment_id":"760021"},{"timestamp":"1671276600.0","comment_id":"748015","poster":"surajkrishnamurthy","upvote_count":"4","content":"Selected Answer: D\nD is the correct answer\nClue is \"next year they want to use the data\" \nTherefore moving the data to coldline storage makes more sense"},{"comment_id":"711824","timestamp":"1667659320.0","upvote_count":"1","poster":"megumin","content":"Selected Answer: D\nok for D"},{"poster":"Mahmoud_E","comment_id":"700335","content":"Selected Answer: D\nD is the correct answer","upvote_count":"1","timestamp":"1666303560.0"},{"content":"I miss the point about cost optimization and I thought C is right. After reading the discussions I realized D is right answer. I am going with D","poster":"AzureDP900","comment_id":"626788","timestamp":"1656900960.0","upvote_count":"1"},{"comments":[{"comment_id":"528583","upvote_count":"4","content":"It cost same as nearline when not accessed, but coldline is cheaper than BQ","poster":"Wonka","timestamp":"1642695780.0"}],"comment_id":"523156","content":"Big query does it. B...when it's long term storage, it costs same as coldline\nhttps://cloud.google.com/bigquery/docs/best-practices-storage","timestamp":"1642111200.0","poster":"Aiffone","upvote_count":"1"},{"content":"the highlight here is machine learning and not disaster recovery or data arhiving which is what coldline storages are for. You also dont pay for datawarehousing in bigquery until you read from it for machine learning. So its cheap and good for ML. i go with B","comment_id":"516380","poster":"Aiffone","timestamp":"1641286800.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1639140840.0","poster":"vincy2202","content":"D is the correct answer","comment_id":"498633"},{"comment_id":"496331","upvote_count":"1","content":"D makes sense IF the \"computer\" on the vehicle can compress data and can take snapshots. Are we supposed to to assume that these \"computers\" have snapshot capability though it is no stated anywhere in the question? Yet, if magically this was possible, this is the correct answer. If this indeed is the correct answer then the only logical deduction is that the questions is stated horribly. I only can hope the real exam isn't like this.","timestamp":"1638910920.0","poster":"mgm7"},{"upvote_count":"2","poster":"joe2211","comment_id":"487927","timestamp":"1637997480.0","content":"Selected Answer: D\nvote D"},{"poster":"MaxNRG","timestamp":"1635073380.0","comment_id":"466920","upvote_count":"3","content":"D â Have vehicleâs computer compress data in hourly snapshots, and store in GCS Coldline bucket.\nA â doesnât work, since Nearline is more expensive than Coldline in D (0.01$ vs 0.007$ GB/month).\nB / C â stores compressed data in relational DB, which may not be possible. Even it is implemented, then B (BigQuery) is more expensive than Cloud Storage Coldline (0.01$ vs 0.007$ GB/month)\nC â Bigtable is most expensive option (0.026$ GB/month) and also it is not integrated with Cloud ML (Dataflow, BiqQuery and Cloud Storage are integrated)\nD â Coldline fits perfectly â blob storage, cheapest price, integration with ML"},{"poster":"victory108","comment_id":"406798","upvote_count":"1","timestamp":"1626327300.0","content":"D. Have the vehicle×â¬â¢s computer compress the data in hourly snapshots, and store it in a GCS Coldline bucket"},{"poster":"MamthaSJ","comment_id":"401141","timestamp":"1625679180.0","upvote_count":"2","content":"Answer is D"},{"comment_id":"325849","timestamp":"1617282180.0","poster":"Ausias18","content":"Answer is D","upvote_count":"1"},{"timestamp":"1616432280.0","poster":"lynx256","comment_id":"317356","upvote_count":"4","content":"IMO B and C are not correct because in both of them there is \"[...] compress the data [...]\" - and neiter BigQuery nor Bigtable are suitable for \"compressed data\".\nD is cheaper than A -- so I'll go with D ."},{"timestamp":"1614257580.0","poster":"ga","comment_id":"299043","upvote_count":"1","content":"D is most cost-effective option as per my view"},{"content":"Seems backwards to have the vehicle itself dictate the storage. The vehicle should just stream the data as soon as possible to GCP and let the cloud environment decide what to do next.","timestamp":"1614002700.0","comments":[{"content":"You cant directly ingest the data as is to Cloud Storage (or BigQuery), due to the write limitations of the service. For Cloud Storage its around 1000 writes/second. For BigQuery its 500 000 writes per second. Terram earth has 200 000 connected vehicles with 120 fields per second of data, which is 200 000 * 120 = 24 000 000 writes per second. Therefore you have to compress the data in any case before writing to the service (does not hold for Dataflow as first ingestion point).","upvote_count":"2","poster":"Jambalaja","comment_id":"332287","timestamp":"1618029360.0"}],"poster":"gg_robin","upvote_count":"1","comment_id":"296705"},{"content":"https://cloud.google.com/certification/guides/cloud-architect/casestudy-terramearth-rev2 \nThey collect 9TB per day from connected devices, that means 9 TB x 365 days = 3,285.00 TB per year. If they will use all stored data for ML training \"data retrieval size\" will be equal to \"total amount of storage\" . Nearline and Coldline have same Class A operations pricing (storage.*.insert) but Nearline is cheaper than Coldline for Class B operations (storage.*.get) https://cloud.google.com/storage/pricing\n\nAlso please use pricing calculator https://cloud.google.com/products/calculator.\n\nOn the other hand, if you compress data in hourly snapshots , 22 hours x 200,000 vehicle you will see 4,400,000 files in one bucket per day. Last but not least :) You have to deal many compressed files when you want to train your ML.\n\n or I have a very big misunderstanding...","poster":"zeustr","timestamp":"1607465400.0","comment_id":"238710","upvote_count":"1"},{"poster":"Chulbul_Pandey","upvote_count":"1","comment_id":"233088","content":"D is right","timestamp":"1606920720.0"},{"comments":[{"poster":"cate0012","comment_id":"237550","timestamp":"1607364300.0","content":"How is data compressed in BQ. I think it has to be stored in a bucket until they want to use it. Coldline may make sense for this being it is a year out.","upvote_count":"1"}],"poster":"hems4all","comment_id":"218388","content":"B is Right Answer:\n\nThis option ensures that we capture all sensor data, which is very important for training machine learning models. The more data a model trains with, and the more use cases it sees, the better it's resulting predictions. And we compress it before storing it in Google BigQuery so we can minimize the data storage charges. AI platform lets your training models pick up data from BigQuery directly.","upvote_count":"3","timestamp":"1605263220.0"},{"timestamp":"1604122860.0","poster":"gcparchitect007","comment_id":"209736","content":"Correct Answer is B, Bigquery, You can not save telemetry data in Cloud storage.","upvote_count":"1"},{"upvote_count":"1","timestamp":"1603932480.0","content":"Answer is B","comment_id":"208188","poster":"N1_arch"},{"poster":"VedaSW","timestamp":"1601181840.0","upvote_count":"1","comments":[{"comment_id":"237551","upvote_count":"1","poster":"cate0012","content":"You're assuming that they delete the data from the bucket for the data that is put there from day 276 to 365. Maybe it just stays there as a backup to the data that is ingested by BQ? It is something to think about though. Perhaps at Day 276 you start writing to nearline or standard bucket and promote the older data when it is required.","timestamp":"1607364420.0"}],"comment_id":"188112","content":"I would go for D - cold line. However, got to watch out on what actually is happening.\n\nStarting year\nDay 1 data -- Cold line\nDay 2 data -- Cold line\n...\nDay 365 data - Cold line\n\nNext year\nPerform analytics on all the collected data.\nSo, technically, Day 276 - 365 data is less than 90 days, so you will still pay full price for 90 days. (which maybe is kind of acceptable? Food for thought)"},{"upvote_count":"2","poster":"saurabh1805","timestamp":"1594931760.0","comment_id":"136739","content":"Vote for D just because data is required after a year !!"},{"upvote_count":"3","comment_id":"118241","poster":"mlantonis","content":"They want to store the data, in order to use it next year. Coldine is the best option.\n\nD is the correct answer.","timestamp":"1592987280.0"},{"content":"D, for sure. \nMost Cost Effective","timestamp":"1591882680.0","comment_id":"107790","upvote_count":"3","poster":"gfhbox0083"},{"upvote_count":"3","timestamp":"1591470360.0","poster":"Ziegler","comment_id":"104053","content":"D is the correct answer"},{"poster":"skywalker","content":"vote for D..\n\nBetween storing inside BigQuery vs Coldline (Since is meant for next year).. \nBigquery pricing after storing data over 90 days is $0.010 per GB .\n\nColdline storage pricing is $0.007 per GB (for storage) plus $0.02 per GB (for data retrieval). No additional charge for loading data into BigQuery.. Thus adding both storage and retrieval is still cheaper than storing under BigQuery.\n\n\nRef:\n1)https://cloud.google.com/storage/pricing\n2)https://cloud.google.com/bigquery/pricing","comment_id":"88574","timestamp":"1589416320.0","upvote_count":"3"},{"poster":"JJu","comments":[{"comment_id":"38470","upvote_count":"2","timestamp":"1578920700.0","poster":"AWS56","content":"That can be done but it is not as cost effective as D. So D is the answer."}],"comment_id":"27669","content":"I find explain about this case.\nif store compression file, upload Cloud Store. and then we can data transfer to Bigquery.\nYou can see architecture lecture on couresera.","upvote_count":"1","timestamp":"1575742320.0"}],"choices":{"B":"Push the telemetry data in real-time to a streaming dataflow job that compresses the data, and store it in Google BigQuery","A":"Have the vehicle's computer compress the data in hourly snapshots, and store it in a Google Cloud Storage (GCS) Nearline bucket","D":"Have the vehicle's computer compress the data in hourly snapshots, and store it in a GCS Coldline bucket","C":"Push the telemetry data in real-time to a streaming dataflow job that compresses the data, and store it in Cloud Bigtable"},"exam_id":4,"isMC":true,"answer_description":"","answer":"D","timestamp":"2019-11-15 10:30:00","question_images":[],"unix_timestamp":1573810200,"answer_images":[],"answer_ET":"D","answers_community":["D (77%)","A (23%)"],"topic":"8"},{"id":"IKGeCQREnsu6HmiyzIGR","isMC":true,"question_id":274,"choices":{"A":"Create a BigQuery table for the European data, and set the table retention period to 36 months. For Cloud Storage, use gsutil to enable lifecycle management using a DELETE action with an Age condition of 36 months.","D":"Create a BigQuery time-partitioned table for the European data, and set the partition expiration period to 36 months. For Cloud Storage, use gsutil to create a SetStorageClass to NONE action with an Age condition of 36 months.","C":"Create a BigQuery time-partitioned table for the European data, and set the partition expiration period to 36 months. For Cloud Storage, use gsutil to enable lifecycle management using a DELETE action with an Age condition of 36 months.","B":"Create a BigQuery table for the European data, and set the table retention period to 36 months. For Cloud Storage, use gsutil to create a SetStorageClass to NONE action when with an Age condition of 36 months."},"answer":"C","question_images":[],"answer_description":"","unix_timestamp":1570877760,"discussion":[{"comments":[{"content":"C\nEnable a bucket lifecycle management rule to delete objects older than 36 months. Use partitioned tables in BigQuery and set the partition expiration period to 36 months. is the right answer.\n\nWhen you create a table partitioned by ingestion time, BigQuery automatically loads data into daily, date-based partitions that reflect the data's ingestion or arrival time.\n\nRef: https://cloud.google.com/bigquery/docs/partitioned-tables#ingestion_time\n\nAnd Google recommends you configure the default table expiration for your datasets, configure the expiration time for your tables, and configure the partition expiration for partitioned tables.\n\nRef: https://cloud.google.com/bigquery/docs/best-practices-storage#use_the_expiration_settings_to_remove_unneeded_tables_and_partitions\n\nIf the partitioned table has a table expiration configured, all the partitions in it are deleted according to the table expiration settings. For our specific requirement, we could set the partition expiration to 36 months so that partitions older than 36 months (and the data within) are automatically deleted.\n\nRef: https://cloud.google.com/bigquery/docs/managing-partitioned-tables#partition-expiration","comment_id":"229700","upvote_count":"24","timestamp":"1653745020.0","poster":"techalik"},{"poster":"nitinz","content":"C, partition the data and expire it in big query and use life cycle on GS bucket.","timestamp":"1662342780.0","comment_id":"303907","upvote_count":"3"},{"upvote_count":"3","poster":"AMohanty","comment_id":"643706","timestamp":"1707312600.0","content":"There is Nothing as Storage Class as NONE"},{"upvote_count":"2","comment_id":"15100","poster":"mister","content":"why not A","timestamp":"1618365360.0","comments":[{"comment_id":"155271","content":"C is ok","poster":"tartar","timestamp":"1644578880.0","upvote_count":"11"},{"comment_id":"408004","upvote_count":"2","content":"bcoz you would land up creating a table for each day which is not a good practice","timestamp":"1673893860.0","poster":"VishalB","comments":[{"timestamp":"1688279700.0","poster":"Wonka","content":"or rather it will delete the entire table and all the data in it i.e. records less than 36 months old","upvote_count":"2","comment_id":"514884"}]},{"content":"C is the correct ans.","timestamp":"1656424080.0","upvote_count":"2","comment_id":"254154","poster":"Pankonics"},{"upvote_count":"8","comments":[{"upvote_count":"1","content":"Thats correcr","poster":"Jambalaja","comment_id":"332285","timestamp":"1665375900.0"}],"comment_id":"15342","poster":"KouShikyou","content":"My understanding is table expiration is for table deletion. \nSince we just want delete the old records in the table but not the entire table.\nI thought what is the time-partitioned table for?\nAny comment?","timestamp":"1618467480.0"}]}],"content":"I thought C was correct.\nSetStorageClass could not be set to NONE. After data expired, data should be deleted not table.\nany comment?","upvote_count":"41","comment_id":"14851","timestamp":"1618224960.0","poster":"KouShikyou"},{"comments":[{"poster":"passnow","comment_id":"30791","timestamp":"1624047000.0","upvote_count":"4","content":"i vote C"}],"timestamp":"1618825440.0","upvote_count":"24","poster":"MeasService","content":"answer C is the right choice here. Table expiration in BigQuery and life cycle management in GSC","comment_id":"16023"},{"poster":"thamaster","comment_id":"760336","upvote_count":"1","timestamp":"1719601260.0","content":"Selected Answer: C\nit's C, I'm sure at 100% the other answer are incorrect there is no None as storage class and you need to actually delete data"},{"content":"Selected Answer: C\nok for C","timestamp":"1714916760.0","upvote_count":"1","comment_id":"711864","poster":"megumin"},{"content":"Selected Answer: C\nC is correct","timestamp":"1713651120.0","comment_id":"700340","upvote_count":"1","poster":"Mahmoud_E"},{"timestamp":"1704342300.0","upvote_count":"2","poster":"AzureDP900","content":"C is right","comment_id":"626797"},{"comment_id":"602586","upvote_count":"1","content":"Selected Answer: C\npartion is way to go with big query hence B & C. \nfor block storage C isvtye waybto go.\nhence C.","poster":"amxexam","timestamp":"1700146080.0"},{"comments":[{"comment_id":"520673","comments":[{"timestamp":"1702314480.0","content":"you also screwed up the percentage of the correct answers now :P","poster":"kimharsh","comment_id":"615002","upvote_count":"3"}],"timestamp":"1688960400.0","upvote_count":"1","content":"I made a mistake in the question to post a comment","poster":"OrangeTiger"}],"upvote_count":"1","poster":"OrangeTiger","content":"Selected Answer: D\n'Next year they want to use the data to train machine learning models.'\nI agree with D.","comment_id":"520672","timestamp":"1688960340.0"},{"poster":"vincy2202","upvote_count":"1","comment_id":"498635","timestamp":"1686394680.0","content":"Selected Answer: C\nC is the correct answer"},{"poster":"joe2211","content":"Selected Answer: C\nvote C","comment_id":"487930","timestamp":"1685164980.0","upvote_count":"1"},{"content":"Correct Answer: C\nA â doesnât work since there is no âretention periodâ for table, there is only âexpiration timeâ after which it is removed completely.\nB/D â doesnât work, since no such storage class like NONE.","timestamp":"1682335620.0","comment_id":"466930","poster":"MaxNRG","upvote_count":"1"},{"upvote_count":"2","poster":"[Removed]","content":"C is correct.\nTime-partioned tables AND DELETE data after 36 months using GCS life cycle management.","timestamp":"1681663500.0","comment_id":"463185"},{"poster":"victory108","timestamp":"1673767440.0","comment_id":"406789","upvote_count":"1","content":"C. Create a BigQuery time-partitioned table for the European data, and set the partition expiration period to 36 months. For Cloud Storage, use gsutil to enable lifecycle management using a DELETE action with an Age condition of 36 months."},{"content":"Answer is C","poster":"MamthaSJ","upvote_count":"1","timestamp":"1673120220.0","comment_id":"401146"},{"content":"Answer is C","comment_id":"325855","poster":"Ausias18","upvote_count":"1","timestamp":"1664629680.0"},{"poster":"lynx256","comment_id":"323404","content":"IMO - C is ok (assuming DAY or lower level time-partitioning). \nWe want to delete only partitions older than 36 month not THE WHOLE tables when aged 36 months.","timestamp":"1664453460.0","upvote_count":"1"},{"poster":"okixavi","timestamp":"1655477160.0","comment_id":"246679","content":"C is the right answer","upvote_count":"2"},{"timestamp":"1654174920.0","upvote_count":"3","content":"C is right","poster":"Chulbul_Pandey","comment_id":"233100"},{"timestamp":"1652955660.0","poster":"Chulbul_Pandey","content":"C is the correct one","upvote_count":"2","comment_id":"222753"},{"poster":"Chulbul_Pandey","comment_id":"219690","timestamp":"1652612520.0","content":"Certainly \"C\"","upvote_count":"1"},{"comment_id":"209742","content":"The correct answer is A, without a second thought. Thanks","poster":"gcparchitect007","timestamp":"1651290780.0","comments":[{"poster":"practicioner","upvote_count":"3","content":"You will drop whole table after 36 months. Congrats )","comment_id":"214722","timestamp":"1651932300.0"}],"upvote_count":"2"},{"poster":"AdityaGupta","content":"I will got with Answer A,\nhttps://cloud.google.com/bigquery/docs/managing-tables#updating_a_tables_expiration_time","comments":[{"content":"Correction, I will go with C\nhttps://cloud.google.com/bigquery/docs/best-practices-storage#use_the_expiration_settings_to_remove_unneeded_tables_and_partitions\n\nIf your tables are partitioned by date, the dataset's default table expiration applies to the individual partitions. You can also control partition expiration using the time_partitioning_expiration flag in the bq command-line tool or the expirationMs configuration setting in the API","comment_id":"208102","timestamp":"1651172280.0","upvote_count":"1","poster":"AdityaGupta"}],"timestamp":"1651171620.0","upvote_count":"1","comment_id":"208095"},{"content":"Agreed with C - wondering why solution is not even talking about DLP API to first identify personal data as 36 months delete is only applicable to personal data..?","upvote_count":"2","timestamp":"1650927600.0","comment_id":"205944","poster":"N1_arch"},{"comment_id":"182574","upvote_count":"3","timestamp":"1647745080.0","content":"Agree with C .. we need to check for 36 months on the data , not the table or bucket. Hence deleting the table after 36 months would also delete data that have been inserted after the table creation and are less than 36 months old , hence partition works for BQ.","poster":"brati_sankar"},{"timestamp":"1646063880.0","comment_id":"169292","upvote_count":"1","poster":"Kabiliravi","content":"C is correct"},{"comment_id":"166615","timestamp":"1645877340.0","poster":"wiqi","upvote_count":"1","content":"C makes sense here."},{"poster":"OnomeOkuma","timestamp":"1643212500.0","upvote_count":"1","content":"C is the correct answer","comment_id":"144153"},{"comment_id":"131975","content":"Ans: C seems to be correct.","poster":"RM07","timestamp":"1641899160.0","upvote_count":"1"},{"comment_id":"118278","upvote_count":"1","comments":[{"comment_id":"208097","content":"A partitioned table is a special table that is divided into segments, called partitions, that make it easier to manage and query your data. By dividing a large table into smaller partitions, you can improve query performance, and you can control costs by reducing the number of bytes read by a query.\n\nhttps://cloud.google.com/bigquery/docs/partitioned-tables","timestamp":"1651171740.0","upvote_count":"2","poster":"AdityaGupta"}],"content":"Table expiration in BigQuery and life cycle management in GSC with DELETE Action.\n\nThe correct is C.","poster":"mlantonis","timestamp":"1640345340.0"},{"content":"There is no such thing as \"retention\" for BQ.\nC is correct as it properly defines BQ action and Lifecycle management ( it is DELETE )","comments":[{"poster":"Pupina","upvote_count":"1","timestamp":"1644877740.0","comment_id":"158299","content":"the property is expiration time\nhttps://cloud.google.com/bigquery/docs/best-practices-storage"}],"timestamp":"1640293980.0","upvote_count":"1","poster":"motty","comment_id":"117768"},{"comment_id":"110465","content":"For sure, C is correct.","timestamp":"1639540980.0","upvote_count":"1","poster":"hbansal077"},{"timestamp":"1639482240.0","upvote_count":"1","poster":"syu31svc","comment_id":"110011","content":"A and D are wrong as you need to use life cycle management to delete data. In BigQquery, you make use of time partitioned tables to get ready for deletion. Answer is C"},{"timestamp":"1639321860.0","poster":"gfhbox0083","comment_id":"108709","content":"C, for sure.\nBigQuery time-partitioned table","upvote_count":"1"},{"timestamp":"1639248300.0","poster":"mikey007","content":"Answer:C","upvote_count":"2","comment_id":"107943"},{"comment_id":"104064","upvote_count":"4","poster":"Ziegler","timestamp":"1638825840.0","content":"C is the correct answer\nhttps://cloud.google.com/bigquery/docs/managing-partitioned-tables#partition-expiration\nhttps://cloud.google.com/storage/docs/lifecycle"},{"poster":"John9999","upvote_count":"2","comment_id":"99597","timestamp":"1638315720.0","content":"A, retention will remove the old data and not the table. Expiration, table is deleted completely. That not right?"},{"comment_id":"96011","poster":"shiwenupper","timestamp":"1637935500.0","upvote_count":"1","content":"C is correct I think"},{"upvote_count":"1","content":"C is the right answer","timestamp":"1637501880.0","poster":"amralieg","comment_id":"93358"},{"comments":[{"timestamp":"1651171800.0","content":"A partitioned table is a special table that is divided into segments, called partitions, that make it easier to manage and query your data. By dividing a large table into smaller partitions, you can improve query performance, and you can control costs by reducing the number of bytes read by a query.\n\nhttps://cloud.google.com/bigquery/docs/partitioned-tables","poster":"AdityaGupta","upvote_count":"1","comment_id":"208098"}],"poster":"Jack_in_Large","timestamp":"1637183580.0","upvote_count":"1","comment_id":"90807","content":"Two reasons we can't choose B:\n1. can't delete the table, but data older than 36 months\n2. https://cloud.google.com/storage/docs/lifecycle"},{"content":"Vote for \"C\"","timestamp":"1636616880.0","upvote_count":"1","comment_id":"86982","poster":"skywalker"},{"timestamp":"1628954700.0","poster":"ADVIT","content":"It's C","comment_id":"50576","upvote_count":"2"},{"comment_id":"27700","poster":"kolcsarzs","content":"correct answer is C","upvote_count":"4","timestamp":"1623090000.0"},{"timestamp":"1621508220.0","content":"B could be correct, https://cloud.netapp.com/blog/google-cloud-storage-bucket-lifecycle-rules-how-to-change-them","poster":"VenkatGCP1","comments":[{"timestamp":"1621702740.0","comment_id":"23691","poster":"cjsammaejs","upvote_count":"6","content":"I think your misreading the items in the link. The lifecyle is set to NONE not the storage class. There is no storage class NONE. see https://cloud.google.com/storage/docs/changing-storage-classes and https://cloud.google.com/storage/docs/storage-classes. I think C is correct."}],"comment_id":"23021","upvote_count":"1"},{"timestamp":"1621063920.0","poster":"JoeShmoe","comment_id":"21729","content":"C is correct","upvote_count":"6"}],"answers_community":["C (86%)","14%"],"question_text":"For this question, refer to the TerramEarth case study. To be compliant with European GDPR regulation, TerramEarth is required to delete data generated from its\nEuropean customers after a period of 36 months when it contains personal data. In the new architecture, this data will be stored in both Cloud Storage and\nBigQuery. What should you do?","answer_ET":"C","exam_id":4,"answer_images":[],"timestamp":"2019-10-12 12:56:00","url":"https://www.examtopics.com/discussions/google/view/6489-exam-professional-cloud-architect-topic-9-question-1/","topic":"9"},{"id":"x5WD1JLk0EzP2oVlHzCw","question_text":"For this question, refer to the TerramEarth case study. TerramEarth has decided to store data files in Cloud Storage. You need to configure Cloud Storage lifecycle rule to store 1 year of data and minimize file storage cost.\nWhich two actions should you take?","url":"https://www.examtopics.com/discussions/google/view/57128-exam-professional-cloud-architect-topic-9-question-2/","isMC":true,"unix_timestamp":1625447040,"exam_id":4,"topic":"9","question_images":[],"question_id":275,"discussion":[{"content":"Answer A\no When Only Option A & D talks about deleting the file after 1 Year. In Option D at Age 30 the storage Class is set to Coldline and while deleting they have used the condition Storage Class: \"Nearline\" which is incorrect.","comments":[{"upvote_count":"2","comment_id":"614942","timestamp":"1702298460.0","poster":"H_S","content":"thank you man"}],"poster":"VishalB","comment_id":"408013","timestamp":"1673894760.0","upvote_count":"23"},{"content":"Selected Answer: A\nA) is the correct answer","comment_id":"761151","upvote_count":"1","timestamp":"1719665760.0","poster":"ale_brd_111"},{"comment_id":"700341","poster":"Mahmoud_E","content":"Selected Answer: A\nA is correct","timestamp":"1713651180.0","upvote_count":"1"},{"upvote_count":"4","comment_id":"669984","timestamp":"1710517860.0","content":"Selected Answer: A\nA â Create Cloud Storage lifecycle rule with Age: â30â, Storage Class: âStandardâ and Action: âSet to Coldlineâ;\nand create a 2nd GCS life-cycle rule with age â365â, Storage Class: âColdlineâ and action âDeleteâ.","poster":"Nirca"},{"upvote_count":"2","timestamp":"1704424560.0","comment_id":"627176","content":"A is right!","poster":"AzureDP900"},{"comment_id":"498583","poster":"vincy2202","content":"Selected Answer: A\nA is thee correct answer","upvote_count":"1","timestamp":"1686391080.0"},{"content":"Selected Answer: A\nvote A","upvote_count":"1","comment_id":"487931","timestamp":"1685165040.0","poster":"joe2211"},{"poster":"[Removed]","upvote_count":"1","content":"A is correct.","comment_id":"467640","timestamp":"1682447940.0"},{"upvote_count":"2","poster":"MaxNRG","timestamp":"1682335980.0","comment_id":"466932","content":"A â Create Cloud Storage lifecycle rule with Age: â30â, Storage Class: âStandardâ and Action: âSet to Coldlineâ;\nand create a 2nd GCS life-cycle rule with age â365â, Storage Class: âColdlineâ and action âDeleteâ.\nD â doesnât work since 2nd life-cyle rule requires âNearlineâ storage, while now data is in âColdlineâ."},{"content":"The optimal answer is A, but is it Archival for 365 as per docs\nhttps://cloud.google.com/storage/docs/storage-classes#available_storage_classes","upvote_count":"1","comment_id":"447640","timestamp":"1679241540.0","poster":"amxexam"},{"content":"A. Create a Cloud Storage lifecycle rule with Age: ×â¬30×â¬, Storage Class: ×â¬Standard×â¬, and Action: ×â¬Set to Coldline×â¬, and create a second GCS life-cycle rule with Age: ×â¬365×â¬, Storage Class: ×â¬Coldline×â¬, and Action: ×â¬Delete×â¬.","upvote_count":"2","comment_id":"406816","poster":"victory108","timestamp":"1673769300.0"},{"timestamp":"1673144400.0","upvote_count":"2","content":"A is ok","comment_id":"401374","poster":"JeffClarke111"},{"content":"Answer is A","poster":"MamthaSJ","timestamp":"1673120280.0","upvote_count":"4","comment_id":"401147"},{"poster":"umashankar_a","timestamp":"1672982700.0","upvote_count":"2","comment_id":"399621","content":"Answer A \nis the correct answer"},{"comment_id":"398752","timestamp":"1672887840.0","poster":"shaw2021","content":"The correct answer is A","upvote_count":"2"}],"answer_description":"","timestamp":"2021-07-05 03:04:00","choices":{"A":"Create a Cloud Storage lifecycle rule with Age: ×â¬30×â¬, Storage Class: ×â¬Standard×â¬, and Action: ×â¬Set to Coldline×â¬, and create a second GCS life-cycle rule with Age: ×â¬365×â¬, Storage Class: ×â¬Coldline×â¬, and Action: ×â¬Delete×â¬.","B":"Create a Cloud Storage lifecycle rule with Age: ×â¬30×â¬, Storage Class: ×â¬Coldline×â¬, and Action: ×â¬Set to Nearline×â¬, and create a second GCS life-cycle rule with Age: ×â¬91×â¬, Storage Class: ×â¬Coldline×â¬, and Action: ×â¬Set to Nearline×â¬.","C":"Create a Cloud Storage lifecycle rule with Age: ×â¬90×â¬, Storage Class: ×â¬Standard×â¬, and Action: ×â¬Set to Nearline×â¬, and create a second GCS life-cycle rule with Age: ×â¬91×â¬, Storage Class: ×â¬Nearline×â¬, and Action: ×â¬Set to Coldline×â¬.","D":"Create a Cloud Storage lifecycle rule with Age: ×â¬30×â¬, Storage Class: ×â¬Standard×â¬, and Action: ×â¬Set to Coldline×â¬, and create a second GCS life-cycle rule with Age: ×â¬365×â¬, Storage Class: ×â¬Nearline×â¬, and Action: ×â¬Delete×â¬."},"answer_images":[],"answer":"A","answers_community":["A (100%)"],"answer_ET":"A"}],"exam":{"isMCOnly":false,"lastUpdated":"11 Apr 2025","id":4,"numberOfQuestions":279,"isImplemented":true,"provider":"Google","isBeta":false,"name":"Professional Cloud Architect"},"currentPage":55},"__N_SSP":true}