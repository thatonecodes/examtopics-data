{"pageProps":{"questions":[{"id":"bEnZL4tJI34wK9utvg1q","topic":"1","isMC":true,"timestamp":"2025-02-27 18:26:00","answer_description":"","unix_timestamp":1740677160,"answers_community":["A (100%)"],"answer_ET":"A","url":"https://www.examtopics.com/discussions/google/view/157234-exam-associate-data-practitioner-topic-1-question-19/","choices":{"C":"Use customer-supplied encryption keys (CSEK).","A":"Use Google-managed encryption keys (GMEK).","B":"Use customer-managed encryption keys (CMEK).","D":"Use customer-supplied encryption keys (CSEK) for the sensitive data and customer-managed encryption keys (CMEK) for the less sensitive data."},"answer":"A","discussion":[{"timestamp":"1740677160.0","comment_id":"1362648","content":"Selected Answer: A\nGoogle can manage the encryption keys for you using GMEK.\nCMEK is customer managed within Cloud KMS\nCSEK is keys that are supplied outside, and managed outside.","poster":"n2183712847","upvote_count":"1"}],"answer_images":[],"question_images":[],"exam_id":2,"question_text":"Your retail organization stores sensitive application usage data in Cloud Storage. You need to encrypt the data without the operational overhead of managing encryption keys. What should you do?","question_id":11},{"id":"EJXNwLn2p69XeqszasoN","exam_id":2,"discussion":[{"comment_id":"1365744","poster":"n2183712847","timestamp":"1741236660.0","upvote_count":"1","content":"Selected Answer: C\nThe best option is C. Option C is best because it correctly partitions by store_id to calculate the moving average for each store individually, and orders by date to ensure the moving average is calculated chronologically over the preceding 7 days (including the current day, thus 6 preceding). ROWS BETWEEN 6 PRECEDING AND CURRENT ROW then correctly defines the 7-day window for the average. Option A is incorrect because it orders by total_sales instead of date, making the rolling average based on sales value order, not time, which is illogical for a weekly trend. Option B is incorrect because it partitions by date instead of store_id, calculating a daily moving average across all stores, not per store. Option D is incorrect because it partitions by total_sales, which is nonsensical for analyzing trends by location, and orders by date within that illogical partition. Therefore, Option C is the only query that correctly calculates a weekly moving average of sales by store location."},{"timestamp":"1737549960.0","comment_id":"1344744","poster":"trashbox","content":"Selected Answer: C\nShould be partitioned by store_id to keep averages separate: A or C\nThen, should be ordered by date for chronological window calculation: C","upvote_count":"1"}],"answers_community":["C (100%)"],"isMC":true,"url":"https://www.examtopics.com/discussions/google/view/155226-exam-associate-data-practitioner-topic-1-question-2/","choices":{"B":"-------------------------","C":"-------------------------","A":"-------------------------","D":"-------------------------"},"question_images":[],"question_id":12,"timestamp":"2025-01-22 13:46:00","topic":"1","answer_description":"","answer_images":[],"answer":"C","unix_timestamp":1737549960,"answer_ET":"C","question_text":"Your company has several retail locations. Your company tracks the total number of sales made at each location each day. You want to use SQL to calculate the weekly moving average of sales by location to identify trends for each store. Which query should you use?"},{"id":"TNgzILRxxMPvcrPsF8B8","question_images":[],"unix_timestamp":1740677040,"question_id":13,"timestamp":"2025-02-27 18:24:00","answer":"B","choices":{"D":"Export the BigQuery tables to Cloud Storage daily, and enforce a lifecycle management policy that has a seven-year retention rule.","A":"Create a partition by transaction date, and set the partition expiration policy to seven years.","B":"Set the table-level retention policy in BigQuery to seven years.","C":"Set the dataset-level retention policy in BigQuery to seven years."},"answer_images":[],"question_text":"You work for a financial organization that stores transaction data in BigQuery. Your organization has a regulatory requirement to retain data for a minimum of seven years for auditing purposes. You need to ensure that the data is retained for seven years using an efficient and cost-optimized approach. What should you do?","answer_ET":"B","url":"https://www.examtopics.com/discussions/google/view/157233-exam-associate-data-practitioner-topic-1-question-20/","exam_id":2,"answer_description":"","topic":"1","answers_community":["B (100%)"],"discussion":[{"upvote_count":"1","content":"Selected Answer: B\nIn the context of the question, both are efficient and cost-optimized. If the regulatory requirement applies to all transaction data which is logically grouped into a dataset, dataset-level retention (Option C) might be slightly more practical for management. However, if the requirement is specifically for this BigQuery table, table-level (Option B) is equally valid and directly applicable. In a multiple choice scenario, both are strong candidates. However, for the most direct answer and often simpler approach when the requirement is somewhat broad (\"transaction data\"), dataset level policy (Option C) might be slightly favoured for ease of management if the retention is consistent for all transaction data. But for a single table scenario implied by \"BigQuery table\", table level is also perfectly valid. Given the options, and aiming for the most directly applicable and efficient, table-level retention is a very strong and valid choice.\n\nGiven the wording of the question is slightly less about dataset context and more about \"the data\" in BigQuery, table-level retention is a very direct and correct answer.","comment_id":"1362647","timestamp":"1740677040.0","poster":"n2183712847"}],"isMC":true},{"id":"1Dy7OQX0OnghwdlL5HTx","question_id":14,"isMC":true,"answers_community":["D (50%)","B (50%)"],"topic":"1","question_text":"You need to create a weekly aggregated sales report based on a large volume of data. You want to use Python to design an efficient process for generating this report. What should you do?","url":"https://www.examtopics.com/discussions/google/view/157232-exam-associate-data-practitioner-topic-1-question-21/","answer_description":"","question_images":[],"discussion":[{"poster":"Rio55","upvote_count":"1","timestamp":"1742716860.0","content":"Selected Answer: D\nOption D (Dataflow DAG coded in Python) and Option B using (bigframes.pandas in Colab Enterprise) seem to be the most suitable. \n\nOption D, Dataflow is designed for parallel processing of large datasets, making it efficient. Python is the chosen language, and Cloud Scheduler handles the weekly scheduling.\n\nOption B, using bigframes.pandas in Colab Enterprise is also a possibility for leveraging Python and BigQuery, but Dataflow is generally more robust and scalable for production ETL pipelines involving large datasets.\n\nOption A with Cloud Run might face limitations with large data and execution time. Option C doesn't directly use Python code for the process design.\n\nI would choose D.","comment_id":"1402160"},{"comment_id":"1362646","content":"Selected Answer: B\nOption B (Colab Enterprise + bigframes.pandas) and Option D (Dataflow DAG in Python) are the most efficient options for handling large data volumes and using Python.\n\nOption B is likely more straightforward and faster to implement for generating a report, especially if the analyst is familiar with Pandas and notebooks. bigframes.pandas simplifies interaction with BigQuery data within a Python environment for reporting purposes.\n\nOption D is more robust and scalable for general data pipelines and potentially more complex transformations, but might be overkill for a weekly reporting task compared to the ease of use offered by bigframes.pandas in Colab Enterprise.\n\nGiven the need for an \"efficient process for generating this report\" and the desire to use Python, Option B provides a very efficient and relatively simpler path for creating a weekly aggregated sales report directly from BigQuery using Python's familiar Pandas-like syntax via bigframes.pandas in a scheduled Colab Enterprise notebook.\n\nFinal Answer: The final answer is \nB\nB","timestamp":"1740676920.0","upvote_count":"1","poster":"n2183712847"}],"timestamp":"2025-02-27 18:22:00","choices":{"C":"Create a Cloud Data Fusion and Wrangler flow. Schedule the flow to run once a week.","B":"Create a Colab Enterprise notebook and use the bigframes.pandas library. Schedule the notebook to execute once a week.","D":"Create a Dataflow directed acyclic graph (DAG) coded in Python. Use Cloud Scheduler to schedule the code to run once a week.","A":"Create a Cloud Run function that uses NumPy. Use Cloud Scheduler to schedule the function to run once a week."},"unix_timestamp":1740676920,"answer_ET":"D","exam_id":2,"answer":"D","answer_images":[]},{"id":"zxt37IqrQ4KDAbuJ0xlk","exam_id":2,"answer_images":[],"answers_community":["A (100%)"],"answer":"A","answer_ET":"A","url":"https://www.examtopics.com/discussions/google/view/157231-exam-associate-data-practitioner-topic-1-question-22/","unix_timestamp":1740676800,"choices":{"B":"Configure a Google Kubernetes Engine cluster with Spark operators, and deploy the Spark jobs.","A":"Migrate the Spark jobs to Dataproc Serverless.","D":"Migrate the Spark jobs to Dataproc on Compute Engine.","C":"Migrate the Spark jobs to Dataproc on Google Kubernetes Engine."},"question_id":15,"timestamp":"2025-02-27 18:20:00","answer_description":"","question_text":"Your organization has decided to move their on-premises Apache Spark-based workload to Google Cloud. You want to be able to manage the code without needing to provision and manage your own cluster. What should you do?","topic":"1","question_images":[],"discussion":[{"poster":"n2183712847","comment_id":"1362645","upvote_count":"2","content":"Selected Answer: A\nDataproc serverless to for spark workload without provisioning + managing your own infrastructure.","timestamp":"1740676800.0"}],"isMC":true}],"exam":{"isImplemented":true,"isMCOnly":true,"provider":"Google","isBeta":false,"numberOfQuestions":72,"name":"Associate Data Practitioner","id":2,"lastUpdated":"11 Apr 2025"},"currentPage":3},"__N_SSP":true}