{"pageProps":{"questions":[{"id":"mL5o9Q8Q406rQXz1rkHG","unix_timestamp":1585794300,"url":"https://www.examtopics.com/discussions/google/view/17785-exam-professional-cloud-security-engineer-topic-1-question-4/","topic":"1","timestamp":"2020-04-02 04:25:00","exam_id":9,"answer_description":"","answer_ET":"A","question_images":[],"isMC":true,"question_id":256,"answer_images":[],"choices":{"A":"Set up Cloud Directory Sync to sync groups, and set IAM permissions on the groups.","C":"Use the Cloud Identity and Access Management API to create groups and IAM permissions from Active Directory.","B":"Set up SAML 2.0 Single Sign-On (SSO), and assign IAM permissions to the groups.","D":"Use the Admin SDK to create groups and assign IAM permissions from Active Directory."},"answer":"A","answers_community":["A (86%)","14%"],"discussion":[{"upvote_count":"30","comment_id":"132750","content":"Answer. is A. B is just the method of authentication, all the heavy lifting is done in A","timestamp":"1610437080.0","poster":"droogie"},{"comment_id":"300171","content":"Correct Answer is A as explained here https://www.udemy.com/course/google-security-engineer-certification/?referralCode=E90E3FF49D9DE15E2855\n\n\"In order to be able to keep using the existing identity management system, identities need to be synchronized between AD and GCP IAM. To do so google provides a tool called Cloud Directory Sync. This tool will read all identities in AD and replicate those within GCP.\n\n Once the identities have been replicated then it's possible to apply IAM permissions on the groups. After that you will configure SAML so google can act as a service provider and either you ADFS or other third party tools like Ping or Okta will act as the identity provider. This way you effectively delegate the authentication from Google to something that is under your control.\"","poster":"johnsm","upvote_count":"10","timestamp":"1630050000.0"},{"upvote_count":"1","comment_id":"1332959","poster":"goat112","content":"Selected Answer: A\nExplanation:\n\nCloud Directory Sync (CDS) is the crucial first step. It's the mechanism that synchronizes your on-premises Active Directory groups with your Google Cloud environment. This allows GCP to recognize and utilize the group structures already defined in your AD.\n\nOnce the groups are synced, you can then:\n\nCreate IAM roles with the appropriate permissions for your GCP resources.\nGrant those IAM roles to the synced AD groups. This effectively ties your existing AD group structure directly to the authorization levels within your GCP environment.\nWhy SAML 2.0 SSO alone is insufficient:\n\nWhile SAML 2.0 SSO is essential for single sign-on capabilities (allowing users to access GCP with their existing AD credentials), it doesn't directly address the core requirement: managing GCP IAM permissions based on existing AD group memberships.","timestamp":"1735391160.0"},{"timestamp":"1730479020.0","upvote_count":"1","poster":"ManuelY","content":"Selected Answer: B\nAnswer is B. \"Centrally manage from their ...\", so, SAML and manage in the on-premise AD","comment_id":"1205130"},{"upvote_count":"1","poster":"PleeO","comment_id":"1202925","timestamp":"1730003340.0","content":"the correct answer is indeed A as Cloud directory sync is the best approach"},{"content":"Selected Answer: A\nCloud directory sync is for this purpose.","comment_id":"1165727","upvote_count":"1","timestamp":"1725454500.0","poster":"cloud_monk"},{"comment_id":"1103377","timestamp":"1719054720.0","poster":"K3rber0s","upvote_count":"3","content":"Correct Answer is A. The keyword is on-prem AD groups which can be synced using Google Dir Sync which then you can apply IAM roles in it.. Without Google Dir Sync, how can you pull the on-prem AD groups? Without it, SSO solution will not work."},{"comment_id":"933815","content":"Selected Answer: A\nCorrect answer is A.","upvote_count":"1","timestamp":"1703535900.0","poster":"f1veo"},{"upvote_count":"1","content":"answer is A","timestamp":"1700935320.0","comment_id":"906767","poster":"ejlp"},{"content":"Answer is A. GCP Cloud Skills Boost has an exact example on this using the fictitious bank called Cymbal Bank, and clearly call out the GCDS process to push Microsoft AD/LDAP into established Users and Groups in your GCP identity domain","poster":"Pachuco","timestamp":"1692297180.0","comment_id":"812340","upvote_count":"2"},{"poster":"DevXr","comment_id":"745255","upvote_count":"1","content":"Selected Answer: B\nUsing third-party IDP connectors for sync\nMany identity management vendors (such as Ping and Okta) provide a connector for G Suite and Cloud Identity Global Directory, which sync changes to users via the Admin SDK Directory API. \n\nThe identity providers control usernames, passwords and other information used to identify, authenticate and authorize users for web applications that Google hosts—in this context, it’s the GCP console. There are a number of existing open source and commercial identity provider solutions that can help you implement SSO with Google. (Read more about SAML-based federated SSO if you’re interested in using Google as the identity provider.)","timestamp":"1686751800.0"},{"comment_id":"743667","upvote_count":"1","content":"Selected Answer: A\nA will do","timestamp":"1686631620.0","poster":"shayke"},{"comment_id":"715177","poster":"Meyucho","upvote_count":"1","content":"Selected Answer: A\nWith A the user and groups management is done in AD as it's asked.","timestamp":"1683710580.0"},{"upvote_count":"1","timestamp":"1681982460.0","content":"Selected Answer: A\nThe question clearly states that, centrally manage. So, Cloud Sync is correct one.","comment_id":"699739","poster":"Premumar"},{"timestamp":"1679423040.0","comment_id":"675318","poster":"thoadmin","upvote_count":"2","content":"Selected Answer: A\nA is correct for me"},{"comment_id":"669885","timestamp":"1678888620.0","poster":"Meyucho","content":"Selected Answer: A\nSSO will only validate identity, that doesn't sync the groups! Answer is A","upvote_count":"2"},{"upvote_count":"1","comment_id":"651848","timestamp":"1677346080.0","content":"Selected Answer: A\nThe correct answer is A","poster":"GCP72"},{"content":"the questions are not the same, took the test today. All questions are updated, mostly focussed around dlp, vpc networks, encryption","comment_id":"640835","upvote_count":"2","timestamp":"1675293360.0","poster":"ssk119"},{"upvote_count":"1","timestamp":"1653265860.0","poster":"mistryminded","comment_id":"484708","content":"Selected Answer: A\nCorrect answer is A"},{"timestamp":"1651698660.0","content":"I took the test yesterday and didn't pass, NO ISSUE is from here. The questions are totally new","poster":"SuperDevops","comment_id":"472812","upvote_count":"4"},{"content":"Reference: itself says answer is A\nhttps://cloud.google.com/blog/products/identity-security/using-your-existing-identity-management-system-with-google-cloud-platform","comment_id":"252448","poster":"BhupalS","upvote_count":"2","timestamp":"1624674960.0"},{"poster":"jonclem","comment_id":"209137","upvote_count":"1","content":"Interestingly, the link is correct, however, the answer is wrong. I agree with the consensus of the actual answer being A.","timestamp":"1619766600.0"},{"comment_id":"199311","upvote_count":"1","content":"A is correct option and also its best practice to use Group to manage permission","timestamp":"1618332480.0","poster":"saurabh1805"},{"timestamp":"1613705460.0","content":"Answer. is A","comment_id":"161178","poster":"aiwaai","upvote_count":"2"},{"poster":"bigdo","content":"A is correct as it makes use of existing group therefore existing user and ad policy","upvote_count":"3","comment_id":"149399","timestamp":"1612308420.0"},{"upvote_count":"2","content":"A is the correct option","comment_id":"144070","timestamp":"1611667920.0","poster":"ArizonaClassics"},{"timestamp":"1601605500.0","content":"B is correct","comment_id":"70324","upvote_count":"1","poster":"xhova"}],"question_text":"Your team wants to centrally manage GCP IAM permissions from their on-premises Active Directory Service. Your team wants to manage permissions by AD group membership.\nWhat should your team do to meet these requirements?"},{"id":"uYfOJeMlrZfxZvwzisHQ","question_id":257,"answer_images":[],"timestamp":"2022-09-07 07:23:00","question_text":"You want to evaluate your organization's Google Cloud instance for PCI compliance. You need to identify Google's inherent controls.\nWhich document should you review to find the information?","unix_timestamp":1662528180,"answer_description":"","answer":"A","answers_community":["A (77%)","C (15%)","8%"],"isMC":true,"question_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/google/view/80812-exam-professional-cloud-security-engineer-topic-1-question/","discussion":[{"poster":"3d9563b","upvote_count":"1","timestamp":"1721724180.0","comment_id":"1253528","content":"Selected Answer: A\nThe Customer Responsibility Matrix is the most relevant document for identifying Google's inherent controls related to PCI compliance, as it explicitly details the security controls managed by Google versus those managed by the customer."},{"timestamp":"1708273380.0","comment_id":"1153416","poster":"okhascorpio","upvote_count":"1","content":"Selected Answer: A\nProbably an outdated question, because there is a specific PCI DSS responsibility matrix available source: https://cloud.google.com/security/compliance/pci-dss\nbut a close enough answer is A because it directly addresses Google's inherent controls while others don't."},{"comment_id":"1148106","poster":"techdsmart","content":"but here controls isn't the same as responsibility? Don't understand how A is the answer since by controls we are referring this from a security and compliance perspective i.e. security controls.\nC is still the correct answer.","timestamp":"1707741840.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1695582900.0","content":"answer is A, https://cloud.google.com/files/GCP_Client_Facing_Responsibility_Matrix_PCI_2018.pdf","comment_id":"1016125","poster":"rottzy"},{"timestamp":"1695352980.0","content":"Selected Answer: A\nTo identify Google's inherent controls for PCI compliance, you should review:\n\nA. Google Cloud Platform: Customer Responsibility Matrix\n\nThe Google Cloud Platform: Customer Responsibility Matrix provides information about the shared responsibility model between Google Cloud and the customer. It outlines which security controls are managed by Google and which are the customer's responsibility. This document will help you understand Google's inherent controls as they relate to PCI compliance.","upvote_count":"2","poster":"Xoxoo","comment_id":"1013551"},{"comment_id":"937867","timestamp":"1688032680.0","content":"The correct answer is A. Google Cloud Platform: Customer Responsibility Matrix.\n\nThe Google Cloud Platform: Customer Responsibility Matrix (CRM) is a document that outlines the responsibilities of Google and its customers for PCI compliance. The CRM identifies the inherent controls that Google provides, which are the security controls that are built into Google Cloud Platform.\n\nThe PCI DSS Requirements and Security Assessment Procedures (SAQs) are a set of requirements that organizations must meet to be PCI compliant. The SAQs do not identify Google's inherent controls.\n\nThe PCI SSC Cloud Computing Guidelines are a set of guidelines that organizations can use to help them achieve PCI compliance when using cloud computing services. The guidelines do not identify Google's inherent controls.\n\nThe product documentation for Compute Engine is a document that provides information about the features and capabilities of Compute Engine. The documentation does not identify Google's inherent controls.","poster":"amanshin","upvote_count":"1"},{"upvote_count":"2","comment_id":"904042","poster":"gcpengineer","timestamp":"1684759500.0","content":"Selected Answer: C\nC is the ans"},{"timestamp":"1684058400.0","content":"Selected Answer: B\nB is the ans. as the pci-dss req in gcp","comments":[],"poster":"gcpengineer","comment_id":"897452","upvote_count":"1"},{"content":"Selected Answer: A\nThe answer is A. Google Cloud Platform: Customer Responsibility Matrix. This document outlines the responsibilities of both the customer and Google for securing the cloud environment and is an important resource for understanding Google's inherent controls for PCI compliance. The PCI DSS Requirements and Security Assessment Procedures and the PCI SSC Cloud Computing Guidelines are both helpful resources for understanding the PCI compliance requirements, but they do not provide information on Google's specific inherent controls. The product documentation for Compute Engine is focused on the technical aspects of using that service and is unlikely to provide a comprehensive overview of Google's inherent controls.","timestamp":"1681548960.0","comment_id":"870782","upvote_count":"3","poster":"aashissh"},{"content":"https://cloud.google.com/architecture/pci-dss-compliance-in-gcp\nB is correct answer","upvote_count":"3","comment_id":"845435","poster":"1explorer","timestamp":"1679363040.0"},{"comment_id":"821897","poster":"tailesley","upvote_count":"3","timestamp":"1677368100.0","content":"It is B:: The PCI DSS Requirements and Security Assessment Procedures is the document that outlines the specific requirements for PCI compliance. It is created and maintained by the Payment Card Industry Security Standards Council (PCI SSC), which is the organization responsible for establishing and enforcing security standards for the payment card industry. This document is used by auditors to evaluate the security of an organization's payment card systems and processes.\n\nWhile the other options may provide information about Google's security controls and the customer's responsibilities for security, they do not provide the specific requirements for PCI compliance that the PCI DSS document does."},{"timestamp":"1665027720.0","poster":"AwesomeGCP","content":"Selected Answer: A\nA. Google Cloud Platform: Customer Responsibility Matrix","upvote_count":"1","comment_id":"687420"},{"poster":"tangac","content":"Selected Answer: A\nhttps://services.google.com/fh/files/misc/gcp_pci_shared_responsibility_matrix_aug_2021.pdf","comment_id":"662032","upvote_count":"2","timestamp":"1662528180.0"}],"exam_id":9,"answer_ET":"A","choices":{"C":"PCI SSC Cloud Computing Guidelines","B":"PCI DSS Requirements and Security Assessment Procedures","A":"Google Cloud Platform: Customer Responsibility Matrix","D":"Product documentation for Compute Engine"}},{"id":"Cj2jd87VfH1c6aqAZkuS","url":"https://www.examtopics.com/discussions/google/view/16482-exam-professional-cloud-security-engineer-topic-1-question/","answer_ET":"C","unix_timestamp":1584111660,"exam_id":9,"question_id":258,"topic":"1","discussion":[{"poster":"KILLMAD","comments":[{"timestamp":"1600539360.0","comment_id":"66086","poster":"mozammil89","content":"Answer C is correct. The TTL is common use case of Cloud Storage life cycle management. Here is what GCP says:\n\n\"To support common use cases like setting a Time to Live (TTL) for objects, retaining noncurrent versions of objects, or \"downgrading\" storage classes of objects to help manage costs, Cloud Storage offers the Object Lifecycle Management feature. This page describes the feature as well as the options available when using it. To learn how to enable Object Lifecycle Management, and for examples of lifecycle policies, see Managing Lifecycles.\"\n\nhttps://cloud.google.com/storage/docs/lifecycle","upvote_count":"7","comments":[{"timestamp":"1732325760.0","upvote_count":"1","comment_id":"1216078","content":"This answer is still valid till 2024","poster":"PleeO"}]}],"upvote_count":"14","timestamp":"1600002060.0","content":"I believe the Answer is C not B.\n\nThis isn't data which needs to be analyzed, so I don't understand why would it be stored in BQ when having data stored in GCS seems much more reasonable.\n\nI think the only thing about answer C which throws me off is the fact that they don't mention object life cycle management","comment_id":"63518"},{"timestamp":"1730709240.0","poster":"trashbox","content":"Selected Answer: C\nBucket lock and TTL are the key features of Cloud Storage.","comment_id":"1206364","upvote_count":"1"},{"upvote_count":"1","comment_id":"1154206","poster":"Bypoo","timestamp":"1724087280.0","content":"Selected Answer: C\nCloud Storage life cycle management"},{"comment_id":"991827","timestamp":"1709106420.0","content":"Selected Answer: C\nAnswer is C","poster":"Echizen06","upvote_count":"2"},{"poster":"cyberpunk21","upvote_count":"1","timestamp":"1708585860.0","content":"B is correct, all forgot this \"Data that has not yet reached the time period should not be deleted.\" from question this means data is keep on updating if we enforce TTL for a bucker the whole bucket will be deleted including updated data, so with Big query we do updating using pipeline jobs and delete data using expiration time","comment_id":"987092"},{"poster":"mahi9","content":"Selected Answer: C\nstore it in a bucket for TTL","timestamp":"1693060980.0","comment_id":"822703","upvote_count":"2"},{"comment_id":"750033","poster":"PST21","timestamp":"1687185240.0","upvote_count":"1","content":"CS does not delete promptly , hence BQ as it is sensitive data"},{"content":"Selected Answer: B\nLife Cycle Management for Cloud storage is used to manage the Storage class to save cost. For data management, you have set retention time on the bucket. I will opt for B as the correct answer.","upvote_count":"1","poster":"csrazdan","timestamp":"1685926200.0","comment_id":"735620"},{"content":"Selected Answer: C\nCorrect Answer: C","poster":"AwesomeGCP","timestamp":"1680755100.0","upvote_count":"2","comment_id":"687437"},{"content":"I would go for C, but all the 4 answers are in my opinion incomplete. all of them say \"single\" bucket or table, which means that if different dated rows/elements are stored in the same bucket or table, they will expire together and be deleted probably before their real expiration time. so i expected to see partitioning or multiple bucket.","upvote_count":"2","poster":"giovy_82","timestamp":"1677312960.0","comment_id":"651667"},{"timestamp":"1661585700.0","content":"Outdated question again- should be bucket locks now.","comment_id":"557217","poster":"mynk29","upvote_count":"1"},{"upvote_count":"2","poster":"DebasishLowes","comment_id":"318384","content":"Ans : C","timestamp":"1632415560.0"},{"poster":"[Removed]","timestamp":"1619793900.0","upvote_count":"4","comment_id":"209440","content":"Ans - C"},{"comment_id":"161850","upvote_count":"3","timestamp":"1613784000.0","poster":"aiwaai","content":"Correct Answer: C"},{"upvote_count":"3","comments":[{"comment_id":"125561","timestamp":"1609686780.0","poster":"smart123","upvote_count":"4","content":"The Buckets do have \"Time to Live\" feature.\nhttps://cloud.google.com/storage/docs/lifecycle\n\nHence 'C' is the answer"}],"timestamp":"1606196520.0","content":"The answers need to be worded better.\nIf we're taking the terms literally as specified in the options, then C cannot be the correction answer since there's no Time to Live configuration for a GCS bucket, only Lifecycle Policy.\nWith BigQuery, there is no row-level expiration, although we could create this behavior using Partitioned Tables. So this could be a potential answer.\nD - it is possible to simulate cell-level TTL (https://cloud.google.com/bigtable/docs/gc-cell-level), so this too could be a potential answer, especially when different cells need different TTLs.\nBetweem B & D, BigQuery follows a pay-as-you-go model and its storage costs are comparable to GCS storage costs. So this would be the more appropriate solution.","comment_id":"94664","poster":"Ganshank"},{"timestamp":"1601132040.0","comments":[{"upvote_count":"6","timestamp":"1601785200.0","content":"Answer is C. You dont need the bucket to be deleted, you need the PII data stored to be deleted.","poster":"xhova","comment_id":"70922"}],"poster":"jonclem","content":"I believe B is correct. \n\nSetting a TTL of 14 days on the bucket via LifeCycle will not cause the bucket itself to be deleted after 14 days, instead it will cause each object uploaded to that bucket to be deleted 14 days after it was created","upvote_count":"3","comment_id":"68396"}],"answers_community":["C (89%)","11%"],"question_text":"Your company runs a website that will store PII on Google Cloud Platform. To comply with data privacy regulations, this data can only be stored for a specific amount of time and must be fully deleted after this specific period. Data that has not yet reached the time period should not be deleted. You want to automate the process of complying with this regulation.\nWhat should you do?","answer":"C","timestamp":"2020-03-13 16:01:00","answer_description":"","question_images":[],"answer_images":[],"isMC":true,"choices":{"A":"Store the data in a single Persistent Disk, and delete the disk at expiration time.","B":"Store the data in a single BigQuery table and set the appropriate table expiration time.","C":"Store the data in a single Cloud Storage bucket and configure the bucket's Time to Live.","D":"Store the data in a single BigTable table and set an expiration time on the column families."}},{"id":"wGeYVuIrHdgB63YKtg4t","answer":"B","url":"https://www.examtopics.com/discussions/google/view/17846-exam-professional-cloud-security-engineer-topic-1-question/","answer_description":"","choices":{"B":"Build small containers using small base images.","A":"Use Cloud Build to build the container images.","D":"Use a Continuous Delivery tool to deploy the application.","C":"Delete non-used versions from Container Registry."},"isMC":true,"topic":"1","question_text":"A DevOps team will create a new container to run on Google Kubernetes Engine. As the application will be internet-facing, they want to minimize the attack surface of the container.\nWhat should they do?","answer_images":[],"answers_community":["B (70%)","C (30%)"],"question_id":259,"timestamp":"2020-04-04 07:11:00","unix_timestamp":1585977060,"exam_id":9,"discussion":[{"content":"Ans is B\n\n Small containers usually have a smaller attack surface as compared to containers that use large base images.\n\nhttps://cloud.google.com/blog/products/gcp/kubernetes-best-practices-how-and-why-to-build-small-container-images","timestamp":"1585977060.0","upvote_count":"31","poster":"xhova","comment_id":"70932","comments":[{"timestamp":"1594467600.0","upvote_count":"2","content":"I agree","poster":"smart123","comment_id":"132046"}]},{"upvote_count":"1","poster":"3d9563b","comment_id":"1253529","timestamp":"1721724300.0","content":"Selected Answer: B\nBuilding small containers using minimal and well-maintained base images directly reduces the attack surface and improves the security posture of your containers when they are deployed on GKE."},{"upvote_count":"1","timestamp":"1708275300.0","poster":"okhascorpio","comment_id":"1153439","content":"Selected Answer: B\nthe correct answer is having as few tools in your image as possible, Source: Remove unnecessary tools https://cloud.google.com/architecture/best-practices-for-building-containers?hl=en\nI guess it can be achieved by option \"B\" building a small container from a small source image."},{"poster":"Afe3saa7","upvote_count":"1","timestamp":"1707654780.0","content":"Selected Answer: B\nA. Use Cloud Build to build the container images.\nWill give you the tools to build an image but not ensure any risk reduction\n\nB. Build small containers using small base images.\nImages with a smaller footprint, stripped of all binaries/libraries/functions that are not used will make it harder for an attacker to find leverage to move laterally or vertically, hence >>reducing the attack/risk surface<< for the image.\n\nC. Delete non-used versions from Container Registry.\nNon-used images are not running live and hence are not exploitable. Removing non-used images from the registry will not reduce the attack surface of the running application.\n\nD. Use a Continuous Delivery tool to deploy the application.\nSame as A.","comment_id":"1147217"},{"comment_id":"1013548","upvote_count":"3","poster":"Xoxoo","content":"Selected Answer: B\nTo minimize the attack surface of a container that will run on Google Kubernetes Engine and be internet-facing, the DevOps team should:\n\nB. Build small containers using small base images.\n\nBuilding small containers using minimal base images reduces the attack surface by eliminating unnecessary software and dependencies, which can potentially contain vulnerabilities. This approach enhances security and reduces the risk of potential attacks. Using small base images, such as Alpine Linux or distroless images, is a best practice for container security.","timestamp":"1695352860.0"},{"comment_id":"967290","upvote_count":"2","content":"Answer is B, because this GCP exam, the GCP docs are always source of truth even though you might not be agree with them occasionally but even if you are not agree you need to choose the answer proposed in GCP docs as the best practice. \nHere is the link to google official best practices for building containers. and here is the snippet regarding this particular question: https://cloud.google.com/architecture/best-practices-for-building-containers#build-the-smallest-image-possible\n\nBuild the smallest image possible\nBuilding a smaller image offers advantages such as faster upload and download times, which is especially important for the cold start time of a pod in Kubernetes: the smaller the image, the faster the node can download it. However, building a small image can be difficult because you might inadvertently include build dependencies or unoptimized layers in your final image.","timestamp":"1690734780.0","poster":"civilizador"},{"comment_id":"958978","timestamp":"1689975240.0","poster":"[Removed]","upvote_count":"2","content":"Selected Answer: B\n\"B\"\nFor smaller attacker surface, use smaller images by removing any unnecessary tools/software from the image.\n\nhttps://cloud.google.com/solutions/best-practices-for-building-containers"},{"timestamp":"1680460680.0","comment_id":"859255","comments":[{"upvote_count":"1","timestamp":"1701032880.0","poster":"adb4007","content":"So build a small image is the answer, not ?","comment_id":"1081017"}],"content":"Selected Answer: C\nImportance: MEDIUM\n\nTo protect your apps from attackers, try to reduce the attack surface of your app by removing any unnecessary tools.\n\nhttps://cloud.google.com/architecture/best-practices-for-building-containers","poster":"alleinallein","upvote_count":"2"},{"timestamp":"1677429900.0","upvote_count":"1","poster":"mahi9","content":"Selected Answer: C\nit is viable","comment_id":"822705"},{"comment_id":"698081","content":"Selected Answer: B\nB definitely","poster":"rotorclear","upvote_count":"1","timestamp":"1666084980.0"},{"upvote_count":"1","content":"Selected Answer: B\nB is the correct answer.","timestamp":"1665030420.0","comment_id":"687439","poster":"AwesomeGCP"},{"poster":"zellck","comment_id":"683978","content":"Selected Answer: B\nB is the answer.","upvote_count":"1","timestamp":"1664598780.0"},{"upvote_count":"3","timestamp":"1664586000.0","content":"Ans is B - https://cloud.google.com/blog/products/gcp/kubernetes-best-practices-how-and-why-to-build-small-container-images\n\nSecurity and vulnerabilities\nAside from performance, there are significant security benefits from using smaller containers. Small containers usually have a smaller attack surface as compared to containers that use large base images.","comment_id":"683904","poster":"jitu028"},{"comment_id":"651668","upvote_count":"3","content":"Selected Answer: B\nthe only answer that will really reduce attack surface while exposing apps to internet is B, small containers (e.g. single web page?)","timestamp":"1661408400.0","poster":"giovy_82"},{"upvote_count":"1","comment_id":"584269","timestamp":"1649688840.0","content":"B. Because you will have less programs in the image thus less vulnerabilities","poster":"Medofree"},{"content":"Selected Answer: C\nA. Use Cloud Build to build the container images.\nIf you build a container using Cloud Build or not the surface is the same\nB. Build small containers using small base images.\nIt is indeed best practice, but I doubt if small base images can reduce the surface. It is still the same app version with the same vulnerabilities etc. \nC. Delete non-used versions from Container Registry.\nUnused, historical versions are additional attack surface. attacker can exploit old, unpatched image which indeed the surface extention.\nD. Use a Continuous Delivery tool to deploy the application.\nThis is just a method of image delivery. The app is the same.","comments":[{"poster":"Afe3saa7","comment_id":"1147214","upvote_count":"1","content":"non-used images in containter registry are as they suggest not running live, hence are not exploitable. deleting images in the registry will not change the attack surface of the mentioned image.","timestamp":"1707654540.0"}],"comment_id":"495141","upvote_count":"3","timestamp":"1638796140.0","poster":"lxs"},{"upvote_count":"2","poster":"DebasishLowes","comment_id":"311690","content":"Ans : B. Small the base image there is less vulnerability and less chance of attack.","timestamp":"1615835220.0"},{"timestamp":"1607771820.0","comment_id":"241531","poster":"soukumar369","upvote_count":"3","content":"Answer is B : at the cost of more space and a slightly higher surface area for attacks."},{"comment_id":"210458","timestamp":"1604236080.0","poster":"[Removed]","content":"should be A, because cloud build has a feature for vulnerability scan","upvote_count":"3"},{"upvote_count":"1","content":"Ans - B","poster":"[Removed]","comment_id":"208680","timestamp":"1603986180.0"},{"timestamp":"1603985400.0","poster":"[Removed]","upvote_count":"1","content":"Ans - B","comment_id":"208677"},{"timestamp":"1601372340.0","upvote_count":"1","comment_id":"189573","content":"B is the answer, small containers have smaller attack surface.","poster":"CHECK666"},{"timestamp":"1597899720.0","content":"The Correct answer is B","upvote_count":"1","comment_id":"161962","poster":"ArizonaClassics"}],"question_images":[],"answer_ET":"B"},{"id":"LvqzVHSIHF1UQenQHRcI","topic":"1","exam_id":9,"answers_community":["B (91%)","9%"],"question_id":260,"timestamp":"2020-10-26 20:38:00","url":"https://www.examtopics.com/discussions/google/view/35256-exam-professional-cloud-security-engineer-topic-1-question/","answer_description":"","unix_timestamp":1603741080,"choices":{"D":"Users sign in using OpenID (OIDC) compatible IdP, receive an authentication token, then use that token to log in to the GCP Console.","A":"Manually synchronize the data in Google domain with your existing Active Directory or LDAP server.","C":"Users sign in directly to the GCP Console using the credentials from your on-premises Kerberos compliant identity provider.","B":"Use Google Cloud Directory Sync to synchronize the data in Google domain with your existing Active Directory or LDAP server."},"discussion":[{"poster":"sudarchary","timestamp":"1659449040.0","content":"Selected Answer: B\nhttps://cloud.google.com/architecture/identity/federating-gcp-with-active-directory-configuring-single-sign-on","comment_id":"538881","upvote_count":"7"},{"upvote_count":"5","poster":"DebasishLowes","timestamp":"1631376420.0","comment_id":"308182","content":"Ans : B"},{"upvote_count":"1","content":"Selected Answer: B\nhttps://cloud.google.com/architecture/identity/federating-gcp-with-active-directory-configuring-single-sign-on","poster":"dbf0a72","comment_id":"1114684","timestamp":"1720192440.0"},{"timestamp":"1680758700.0","upvote_count":"2","poster":"AwesomeGCP","content":"Selected Answer: B\nhttps://cloud.google.com/architecture/identity/federating-gcp-with-active-directory-configuring-single-sign-on","comment_id":"687485"},{"timestamp":"1670735340.0","content":"B of course","poster":"absipat","upvote_count":"2","comment_id":"614803"},{"content":"Selected Answer: D\nMy vote goes for D.\n\nFrom the blog post linked below \" users’ passwords are not synchronized by default. Only the identities are synchronized, unless you make an explicit choice to synchronize passwords (which is not a best practice and should be avoided)\".\n \nAlso, from GCP documentation \"Authenticating with OIDC and AD FS\" https://cloud.google.com/anthos/clusters/docs/on-prem/1.6/how-to/oidc-adfs\n\nBlog post quoted above https://cloud.google.com/blog/products/identity-security/using-your-existing-identity-management-system-with-google-cloud-platform","comments":[{"upvote_count":"3","comment_id":"514144","timestamp":"1656596280.0","poster":"rr4444","content":"D sounds nice, but the user doesn't \"use\" the token.... that's used in the integration with Cloud Identity. So answer must be B, GCDS"}],"comment_id":"502927","poster":"ThisisJohn","timestamp":"1655377080.0","upvote_count":"1"},{"comment_id":"209471","poster":"[Removed]","content":"Ans - B","upvote_count":"4","timestamp":"1619796780.0"},{"comment_id":"206542","content":"B is correct answer here.","poster":"saurabh1805","upvote_count":"4","timestamp":"1619458680.0"}],"answer_images":[],"answer_ET":"B","question_text":"While migrating your organization's infrastructure to GCP, a large number of users will need to access GCP Console. The Identity Management team already has a well-established way to manage your users and want to keep using your existing Active Directory or LDAP server along with the existing SSO password.\nWhat should you do?","question_images":[],"answer":"B","isMC":true}],"exam":{"isBeta":false,"provider":"Google","id":9,"isMCOnly":false,"isImplemented":true,"name":"Professional Cloud Security Engineer","numberOfQuestions":321,"lastUpdated":"11 Apr 2025"},"currentPage":52},"__N_SSP":true}