{"pageProps":{"questions":[{"id":"yTUqP6fBVy3tOtotKWWo","discussion":[{"poster":"forport","timestamp":"1721885160.0","upvote_count":"1","comment_id":"1254732","content":"Selected Answer: B\n'for ios mobile' = edge\n'.mlmodel directly' = minimizes the cost"},{"comments":[{"timestamp":"1713686280.0","comment_id":"1199529","content":"A. AutoML with Batch Requests: While AutoML offers powerful model training, batch requests for prediction still incur network latency and might not be ideal for real-time mobile applications.\nC & D. TFLite and Vertex AI Endpoint: Both TFLite and Vertex AI endpoints are viable options, but they require additional steps for mobile integration compared to Core ML, which is native to iOS. Additionally, a Vertex AI endpoint introduces cloud communication and potential costs.","upvote_count":"1","poster":"fitri001"}],"timestamp":"1713686280.0","upvote_count":"2","content":"Selected Answer: B\nNo-code Training: AutoML Edge simplifies model training without needing extensive coding knowledge.\nOn-device Processing: Core ML models run directly on the iOS device, minimizing latency by eliminating the need for network calls to a cloud endpoint.\nCost-effective: Training on AutoML Edge and deploying the model on the device avoids ongoing costs associated with Vertex AI endpoints.","comment_id":"1199528","poster":"fitri001"},{"comment_id":"1190983","content":"Selected Answer: B\nCore ML is specifically designed for iOS devices, ensuring efficient inference and low latency.","poster":"pinimichele01","upvote_count":"1","timestamp":"1712497500.0"},{"poster":"guilhermebutzke","comment_id":"1153464","content":"Selected Answer: B\nMy Answer: B\n\nAutoML Edge or Vertex AI endpoint?: This option is specifically designed for training models that run on edge devices like mobile phones. It optimizes models for size and efficiency, minimizing cost and latency. While AutoML can train the model, using a Vertex AI endpoint adds unnecessary overhead and potential latency for mobile predictions. Batch requests wouldn't significantly improve latency here.\n\nCore ML or TFLite: While TFLite is compatible with some mobile platforms, Core ML is specifically designed for iOS and offers better performance and integration.","timestamp":"1708278660.0","upvote_count":"1"},{"comment_id":"1123353","content":"Selected Answer: B\nB.\n\nConfused as AutoML Vision Edge seems like the right tool for this problematic but is deprecated according to docs: https://firebase.google.com/docs/ml/automl-image-labeling\n\nI will assume that the question needs updating but we should go with that + core ML is specifically designed for iOS apps. https://www.netguru.com/blog/coreml-vs-tensorflow-lite-mobile","timestamp":"1705323000.0","upvote_count":"1","poster":"b1a8fae"},{"timestamp":"1705266420.0","content":"Selected Answer: B\nit's possible to use either Core ML or TF Lite, but since it's necessary to ensure the lowest possible latency, choose Core ML\nhttps://cloud.google.com/vertex-ai/docs/export/export-edge-model#classification","poster":"BlehMaks","upvote_count":"2","comment_id":"1122848"},{"timestamp":"1705168860.0","poster":"36bdc1e","upvote_count":"1","comment_id":"1121934","content":"B\nFor no code , automl is the best , for minimizing the cost we export as Core ML model"},{"comment_id":"1121255","timestamp":"1705114500.0","poster":"pikachu007","content":"Selected Answer: B\nNo-code model development: AutoML Edge provides a no-code interface for model training, aligning with the requirement.\nOptimized for mobile devices: Core ML is specifically designed for iOS devices, ensuring efficient inference and low latency.\nOffline capability: The app can run predictions locally without requiring network calls, reducing costs and ensuring availability even without internet connectivity.\nNo ongoing endpoint costs: Unlike using a Vertex AI endpoint, there are no extra costs associated with hosting and serving the model.","upvote_count":"2"}],"isMC":true,"answer_ET":"B","question_images":[],"exam_id":13,"timestamp":"2024-01-13 03:55:00","url":"https://www.examtopics.com/discussions/google/view/131010-exam-professional-machine-learning-engineer-topic-1-question/","question_id":106,"topic":"1","unix_timestamp":1705114500,"choices":{"B":"Train the model by using AutoML Edge, and export it as a Core ML model. Configure your mobile application to use the .mlmodel file directly.","D":"Train the model by using AutoML, and expose the model as a Vertex AI endpoint. Configure your mobile application to invoke the endpoint during prediction.","C":"Train the model by using AutoML Edge, and export the model as a TFLite model. Configure your mobile application to use the .tflite file directly.","A":"Train the model by using AutoML, and register the model in Vertex AI Model Registry. Configure your mobile application to send batch requests during prediction."},"answer":"B","answer_description":"","question_text":"You work for a social media company. You want to create a no-code image classification model for an iOS mobile application to identify fashion accessories. You have a labeled dataset in Cloud Storage. You need to configure a training workflow that minimizes cost and serves predictions with the lowest possible latency. What should you do?","answers_community":["B (100%)"],"answer_images":[]},{"id":"dkfXTwVj3dPzeNX0yCL7","answer_images":[],"exam_id":13,"choices":{"A":"Create a BigQuery table. Use BigQuery ML to build a boosted tree classifier. Inspect the partition rules of the trees to understand how each prediction flows through the trees.","D":"Create a Vertex AI tabular dataset. Train an AutoML model to predict customer purchases. Deploy the model to a Vertex AI endpoint. At each prediction, enable L1 regularization to detect non-informative features.","B":"Create a Vertex AI tabular dataset. Train an AutoML model to predict customer purchases. Deploy the model to a Vertex AI endpoint and enable feature attributions. Use the “explain” method to get feature attribution values for each individual prediction.","C":"Create a BigQuery table. Use BigQuery ML to build a logistic regression classification model. Use the values of the coefficients of the model to interpret the feature importance, with higher values corresponding to more importance"},"topic":"1","question_id":107,"timestamp":"2024-01-13 03:59:00","answers_community":["B (100%)"],"answer_ET":"B","answer_description":"","isMC":true,"question_images":[],"unix_timestamp":1705114740,"discussion":[{"content":"Selected Answer: B\n\" simplest approach\", the option B is the best choice.","comment_id":"1239556","upvote_count":"1","poster":"LaxmanTiwari","timestamp":"1719733260.0"},{"comment_id":"1199534","poster":"fitri001","content":"Selected Answer: B\nIndividual Prediction Explanation: Vertex AI feature attributions provide insights into how each feature (e.g., days_since_last_purchase, average_purchase_frequency) contributes to a specific prediction for a customer-product combination. This allows you to understand the rationale behind the model's prediction for each instance.\nAutoML Convenience: AutoML simplifies model training without extensive configuration.","timestamp":"1713686760.0","comments":[{"timestamp":"1713686760.0","content":"A. BigQuery ML with Boosted Trees: While BigQuery ML can build boosted tree models, interpreting individual predictions by inspecting partition rules can be cumbersome and less intuitive compared to feature attributions.\nC. BigQuery ML Logistic Regression: Logistic regression coefficients indicate feature importance, but they don't directly explain how a specific feature value influences a single prediction.\nD. L1 Regularization: L1 regularization can help identify potentially unimportant features during training, but it doesn't directly explain individual predictions.","upvote_count":"2","comment_id":"1199535","poster":"fitri001"}],"upvote_count":"2"},{"timestamp":"1706782500.0","content":"Selected Answer: B\nVertex AI feature attributions: This is the most direct approach. By enabling feature attributions, you get explanations for each prediction, highlighting how individual features contribute to the model's output. This is crucial for understanding specific customer purchase predictions.","upvote_count":"1","comment_id":"1137492","poster":"ddogg"},{"content":"Selected Answer: B\nB is correct","poster":"BlehMaks","upvote_count":"2","comment_id":"1122876","timestamp":"1705267860.0"},{"poster":"36bdc1e","content":"B\nloca interpretability we Use the \"explain\" method to get feature attribution values for each individual prediction.","comment_id":"1121936","timestamp":"1705168980.0","upvote_count":"1"},{"comment_id":"1121256","content":"Selected Answer: B\nIndividual prediction interpretability: Feature attributions specifically address the need to understand how features contribute to individual predictions, providing fine-grained insights.\nVertex AI integration: Vertex AI offers seamless integration of feature attributions with AutoML models, simplifying the process.\nModel flexibility: AutoML can explore various model architectures, potentially finding the most suitable one for this task, while still providing interpretability.","poster":"pikachu007","upvote_count":"1","timestamp":"1705114740.0"}],"question_text":"You work for a retail company. You have been asked to develop a model to predict whether a customer will purchase a product on a given day. Your team has processed the company’s sales data, and created a table with the following rows:\n• Customer_id\n• Product_id\n• Date\n• Days_since_last_purchase (measured in days)\n• Average_purchase_frequency (measured in 1/days)\n• Purchase (binary class, if customer purchased product on the Date)\n\nYou need to interpret your model’s results for each individual prediction. What should you do?","answer":"B","url":"https://www.examtopics.com/discussions/google/view/131011-exam-professional-machine-learning-engineer-topic-1-question/"},{"id":"BULkNR7KB4hp3MNleK9v","topic":"1","url":"https://www.examtopics.com/discussions/google/view/130805-exam-professional-machine-learning-engineer-topic-1-question/","discussion":[{"upvote_count":"1","timestamp":"1723551360.0","comment_id":"1265149","poster":"tardigradum","content":"Selected Answer: A\nIt makes sense to use Vertex AI Vision Occupancy to reduce the effort of obtaining a model that identifies the number of people in a video, although I am hesitant about the fact that it says 'BUILD a model' and strictly speaking, no model is actually built with that option."},{"poster":"Prakzz","timestamp":"1719750780.0","upvote_count":"1","comment_id":"1239637","content":"Selected Answer: B\nhttps://console.cloud.google.com/vertex-ai/publishers/google/model-garden/vehicle-detector\nOccupancy analytics has other features too like zone detection, dwell time, and more, which is not needed in this scenario."},{"content":"Selected Answer: A\nA. Use the Vertex AI Vision Occupancy Analytics model: This is a pre-built model specifically designed for analyzing occupancy in videos. It's ideal for this scenario as it requires minimal configuration and can likely be deployed quickly.","upvote_count":"4","poster":"fitri001","comment_id":"1197837","timestamp":"1713434160.0","comments":[{"comment_id":"1197838","timestamp":"1713434160.0","content":"C. Train an AutoML object detection model: While this could be a good solution in the long run, training a custom model requires creating an annotated dataset and takes time.\nD. Seq2Seq+ object detection model: This is an overly complex approach for this task. Seq2Seq models are used for sequence-to-sequence prediction tasks and are not necessary here.","poster":"fitri001","upvote_count":"1"}]},{"timestamp":"1707684480.0","comment_id":"1147666","upvote_count":"4","content":"Selected Answer: A\nMy Answer: A:\n\nVertex AI Vision Occupancy Analytics is a pre-trained model specifically designed to count people in live video streams. This removes the need for expensive and time-consuming data labeling and training, making it ideal for quick implementation. ****Vertex AI Vision Person/Vehicle Detector model detects individual people and vehicles, not specifically focusing on occupancy counting. It would require further processing to estimate the number of waiting customers. Option C and D requires labeling data and training, which adds effort and time.\nhttps://cloud.google.com/vision-ai/docs/overview","poster":"guilhermebutzke"},{"timestamp":"1706782860.0","upvote_count":"2","comment_id":"1137495","content":"Selected Answer: A\nA. Use the Vertex AI Vision Occupancy Analytics model.\n\nHere's why:\n\nPre-trained and optimized: Occupancy Analytics is a pre-trained and optimized model specifically designed for counting people in video footage, aligning perfectly with your task. This eliminates the need for extensive data collection, annotation, and training, saving time and effort.\n\nNear real-time performance: The model is designed for low latency and near real-time inference, providing results quickly with minimal delay, important for live video analysis.\n\nMinimal configuration: Compared to training your own model, this option requires minimal configuration within the Vertex AI console, allowing for a quicker setup and deployment.","poster":"ddogg"},{"content":"Selected Answer: B\nAll you need is counting the number of customers in the video stream. I would say no need to have the extra functionalities of occupancy analytics, person/vehicle is enough for this use case. https://cloud.google.com/vision-ai/docs/person-vehicle-model","comment_id":"1123375","timestamp":"1705323840.0","upvote_count":"1","poster":"b1a8fae"},{"content":"Selected Answer: A\nhttps://codelabs.developers.google.com/vertex-ai-vision-queue-detection#0","upvote_count":"3","comment_id":"1118811","timestamp":"1704903780.0","poster":"winston9"}],"choices":{"D":"Train a Seq2Seq+ object detection model on an annotated dataset by using Vertex AutoML.","C":"Train an AutoML object detection model on an annotated dataset by using Vertex AutoML.","A":"Use the Vertex AI Vision Occupancy Analytics model.","B":"Use the Vertex AI Vision Person/vehicle detector model."},"answer_ET":"A","question_id":108,"answer_description":"","question_images":[],"isMC":true,"answers_community":["A (88%)","13%"],"question_text":"You work for a company that captures live video footage of checkout areas in their retail stores. You need to use the live video footage to build a model to detect the number of customers waiting for service in near real time. You want to implement a solution quickly and with minimal effort. How should you build the model?","unix_timestamp":1704903780,"timestamp":"2024-01-10 17:23:00","answer_images":[],"answer":"A","exam_id":13},{"id":"UUe8PNQ3NxacuSGLvz8O","choices":{"D":"Use Cloud Composer to build the training pipelines for custom deep learning-based models","B":"Use Google Kubernetes Engine to build a custom training pipeline for XGBoost-based models","C":"Use Tabular Workflow for TabNet through Vertex AI Pipelines to train attention-based models","A":"Use Tabular Workflow for Wide & Deep through Vertex AI Pipelines to jointly train wide linear models and deep neural networks"},"topic":"1","answer_description":"","answer":"C","question_images":[],"discussion":[{"comment_id":"1147749","content":"Selected Answer: C\nMy Answer: C\n\nLink: https://cloud.google.com/vertex-ai/docs/tabular-data/tabular-workflows/overview","upvote_count":"3","poster":"guilhermebutzke","timestamp":"1723415640.0"},{"poster":"ddogg","content":"Selected Answer: C\nhttps://www.sciencedirect.com/science/article/pii/S0957417423000441\n•\nWhen compared to XGBoost & GLM, TabNet provides better or comparable performance.\n\n•\nUnlike other Deep Learning models, TabNet is highly interpretable.","timestamp":"1722501480.0","comment_id":"1137521","upvote_count":"3"},{"timestamp":"1722339600.0","poster":"sonicclasps","content":"Selected Answer: C\nagree, C, as this is specifically one of Tabnet's strengths","upvote_count":"1","comment_id":"1135835"},{"timestamp":"1720854300.0","poster":"winston9","comment_id":"1121435","content":"Selected Answer: C\naccording to the documentation: \"TabNet uses sequential attention to choose which features to reason from at each decision step. This promotes interpretability and more efficient learning because the learning capacity is used for the most salient features.\"","upvote_count":"1"},{"content":"Selected Answer: C\nTabNet models are inherently more interpretable than deep neural networks or XGBoost models due to their attention mechanism. This aligns with the primary focus on interpretability.","poster":"pikachu007","comment_id":"1121257","upvote_count":"1","timestamp":"1720832820.0"}],"url":"https://www.examtopics.com/discussions/google/view/131012-exam-professional-machine-learning-engineer-topic-1-question/","unix_timestamp":1705115220,"answer_images":[],"isMC":true,"exam_id":13,"answer_ET":"C","timestamp":"2024-01-13 04:07:00","question_id":109,"question_text":"You work as an analyst at a large banking firm. You are developing a robust scalable ML pipeline to tram several regression and classification models. Your primary focus for the pipeline is model interpretability. You want to productionize the pipeline as quickly as possible. What should you do?","answers_community":["C (100%)"]},{"id":"ao9QonZ5vNe8qGYYtCZH","discussion":[{"comment_id":"1197844","upvote_count":"3","comments":[{"content":"B. Reduction Server: While Vertex AI supports Reduction Servers, it's generally not required for text translation with Transformers. It's more commonly used for distributed training with specific model architectures.\nC. Cloud TPU VMs: While Cloud TPUs offer excellent performance, they require significant code modifications to work with Transformer models in TensorFlow. Additionally, managing Cloud TPU VMs involves more complexity compared to Vertex AI custom training jobs.\nD. Single worker pool: This limits training to a single machine, negating the benefits of distributed training.","upvote_count":"1","timestamp":"1729245720.0","comment_id":"1197845","poster":"fitri001"}],"content":"Selected Answer: A\nVertex AI custom training job: This leverages a managed service within GCP, reducing cluster configuration and management overhead.\nGPU accelerators for the second worker pool: This allows for distributed training across multiple GPUs, significantly speeding up training compared to a single worker pool.\ntf.distribute.MultiWorkerMirroredStrategy: This is a TensorFlow strategy specifically designed for distributed training on multiple machines. It minimizes code changes as it handles data parallelization and model replication across devices.","poster":"fitri001","timestamp":"1729245720.0"},{"poster":"Carlose2108","comments":[{"timestamp":"1728309060.0","comment_id":"1190987","content":"for me is C","upvote_count":"1","poster":"pinimichele01"},{"upvote_count":"2","comment_id":"1204418","content":"Yeah, but as the question mentions \"minimizing the effort required to modify code and to manage the cluster’s configuration\", and TPus may require specific adaptations in the model code to fully exploit TPU capabilities.","poster":"tavva_prudhvi","timestamp":"1730284260.0"}],"content":"Why not C?","upvote_count":"2","comment_id":"1160457","timestamp":"1724748240.0"},{"timestamp":"1723586040.0","content":"Selected Answer: A\nMy Answer: A\n\n- Distributed training: Utilizes GPUs in 2nd worker pool for speedup.\n- Minimal code changes: Vertex AI custom job for ease of use.\n- Managed cluster: No manual configuration needed.\n\nOther options:\n- B: Complex setup with different machine types and Reduction Server.\n- C: TPUs may not be optimal for Transformers and require code changes.\n- D: Lacks distributed training, limiting speed improvement.","poster":"guilhermebutzke","comment_id":"1149670","upvote_count":"2"},{"timestamp":"1720833060.0","comment_id":"1121261","poster":"pikachu007","content":"Selected Answer: A\nMinimizes code modification: MultiWorkerMirroredStrategy often requires minimal code changes to distribute training across multiple workers, aligning with the goal of minimizing effort.\nSimplifies cluster management: Vertex AI handles cluster configuration and scaling for custom training jobs, reducing the need for manual management.\nEffective distributed training: MultiWorkerMirroredStrategy is well-suited for large models and datasets, efficiently distributing training across GPUs.","upvote_count":"3"}],"isMC":true,"answer_ET":"A","question_images":[],"exam_id":13,"timestamp":"2024-01-13 04:11:00","question_id":110,"url":"https://www.examtopics.com/discussions/google/view/131013-exam-professional-machine-learning-engineer-topic-1-question/","topic":"1","unix_timestamp":1705115460,"choices":{"A":"Create a Vertex AI custom training job with GPU accelerators for the second worker pool. Use tf.distribute.MultiWorkerMirroredStrategy for distribution.","D":"Create a Vertex AI custom training job with a single worker pool of A2 GPU machine type instances. Use tf.distribute.MirroredStrategv for distribution.","C":"Create a training job that uses Cloud TPU VMs. Use tf.distribute.TPUStrategy for distribution.","B":"Create a Vertex AI custom distributed training job with Reduction Server. Use N1 high-memory machine type instances for the first and second pools, and use N1 high-CPU machine type instances for the third worker pool."},"answer_description":"","answer":"A","question_text":"You developed a Transformer model in TensorFlow to translate text. Your training data includes millions of documents in a Cloud Storage bucket. You plan to use distributed training to reduce training time. You need to configure the training job while minimizing the effort required to modify code and to manage the cluster’s configuration. What should you do?","answers_community":["A (100%)"],"answer_images":[]}],"exam":{"provider":"Google","numberOfQuestions":304,"isBeta":false,"name":"Professional Machine Learning Engineer","isMCOnly":true,"isImplemented":true,"lastUpdated":"11 Apr 2025","id":13},"currentPage":22},"__N_SSP":true}