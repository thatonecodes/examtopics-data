{"pageProps":{"questions":[{"id":"IklIaZrKiFu4J5UtGRT3","answer_ET":"C","unix_timestamp":1671664740,"exam_id":5,"question_id":66,"answers_community":["C (100%)"],"isMC":true,"answer_description":"","choices":{"B":"Use Cloud Functions to run the export in text format.","D":"Use Dataflow to run the export in text format.","A":"Use Cloud Functions to run the export in Avro format.","C":"Use Dataflow to run the export in Avro format."},"discussion":[{"upvote_count":"5","poster":"dynamic_dba","timestamp":"1726159920.0","comment_id":"837308","content":"C.\nA and B are wrong because Spanner exports are run as Dataflow jobs. The question says you need a copy of your entire database which means all the tables. You cannot export an entire database using the CSV (text) format, but you can using the Avro format. So that would make it the better option.\nhttps://cloud.google.com/spanner/docs/import-export-overview#file-format"},{"content":"Although I agree with C, I don't know why not D","poster":"H_S","comment_id":"830315","timestamp":"1725562020.0","upvote_count":"1"},{"timestamp":"1719320160.0","comment_id":"755741","poster":"pk349","content":"C: Use Dataflow to run the export in Avro format.","upvote_count":"2"},{"upvote_count":"3","poster":"GCP72","content":"Selected Answer: C\nAnswer is C, Dataflow and Avro format.\nCloud functions has timeout Gen-1 6mins Gen-2 1hr","timestamp":"1719298440.0","comment_id":"755528"},{"content":"Selected Answer: C\nUse Dataflow to run the export in Avro format.\n.\nhttps://cloud.google.com/spanner/docs/export","poster":"range9005","timestamp":"1719004740.0","upvote_count":"4","comment_id":"752848"}],"question_images":[],"url":"https://www.examtopics.com/discussions/google/view/92401-exam-professional-cloud-database-engineer-topic-1-question/","question_text":"You are running an instance of Cloud Spanner as the backend of your ecommerce website. You learn that the quality assurance (QA) team has doubled the number of their test cases. You need to create a copy of your Cloud Spanner database in a new test environment to accommodate the additional test cases. You want to follow Google-recommended practices. What should you do?","answer":"C","topic":"1","timestamp":"2022-12-22 00:19:00","answer_images":[]},{"id":"2DXeZevuYcfHcBAFpBSG","answer_ET":"C","isMC":true,"question_text":"Your customer has a global chat application that uses a multi-regional Cloud Spanner instance. The application has recently experienced degraded performance after a new version of the application was launched. Your customer asked you for assistance. During initial troubleshooting, you observed high read latency. What should you do?","timestamp":"2022-12-19 04:12:00","unix_timestamp":1671419520,"answer":"C","url":"https://www.examtopics.com/discussions/google/view/92034-exam-professional-cloud-database-engineer-topic-1-question-4/","discussion":[{"content":"Selected Answer: C\nRead statistics provide insight into how an application is using the database, and are useful when investigating performance issues.\n\nhttps://cloud.google.com/spanner/docs/introspection/read-statistics","timestamp":"1702414380.0","upvote_count":"5","comment_id":"1094955","poster":"jnya_1991"},{"upvote_count":"2","comment_id":"1333772","content":"Selected Answer: C\nSPANNER_SYS.READ_STATS* contains statistics about reads.","poster":"Ral17","timestamp":"1735512660.0"},{"upvote_count":"1","poster":"Zakky_09","content":"Selected Answer: D\nWhen experiencing high read latency in a Cloud Spanner instance, analyzing the SPANNER_SYS.QUERY_STATS tables provides valuable insights into the performance of your queries. These tables contain statistics about query execution, such as latency, execution count, and resource usage. This information helps identify slow or inefficient queries that may be causing the performance degradation","comment_id":"1331501","timestamp":"1735121820.0"},{"content":"Selected Answer: D\nSPANNER_SYS.QUERY_STATS tables: the first step in diagnosing latency after a code change is often to look at the queries themselves so ans is D https://cloud.google.com/spanner/docs/introspection/query-statistics","upvote_count":"1","comment_id":"1326002","poster":"sky09","timestamp":"1734061080.0"},{"poster":"theseawillclaim","content":"C! You should analyze the situation before changing the architecture so drastically.","upvote_count":"2","timestamp":"1695667680.0","comment_id":"1017135"},{"comment_id":"1012729","timestamp":"1695260160.0","upvote_count":"2","poster":"kpkakadiya","content":"Selected Answer: C\nC is the correct answer"},{"comment_id":"1010283","poster":"goodsport","content":"C is definitely the correct answer here. SPANNER_SYS.READ_STATS* contains statistics about reads.","timestamp":"1695014880.0","upvote_count":"2"},{"upvote_count":"2","comment_id":"972659","content":"C is the correct answer","poster":"RahulHanumante","timestamp":"1691208720.0"},{"poster":"jamalkhan","upvote_count":"3","content":"Selected Answer: C\nC. Read stats","timestamp":"1687290180.0","comment_id":"928751"},{"upvote_count":"3","timestamp":"1678538460.0","comment_id":"835978","content":"C.\nA Query parameters is vague at best. B would not achieve anything. C and D look interesting, but as others have stated, querying the READ_STATS* tables would give you information about what is causing read issues. So C is the best answer.","poster":"dynamic_dba"},{"poster":"H_S","content":"Selected Answer: C\nC. Use SQL statements to analyze SPANNER_SYS.READ_STATS* tables","timestamp":"1678034580.0","upvote_count":"4","comment_id":"830099"},{"poster":"H_S","upvote_count":"3","content":"Selected Answer: C\nC. Use SQL statements to analyze SPANNER_SYS.READ_STATS* tables","comment_id":"829947","timestamp":"1678024740.0"},{"poster":"omermahgoub","upvote_count":"4","timestamp":"1672044660.0","content":"C. Use SQL statements to analyze SPANNER_SYS.READ_STATS* tables.\n\nTo troubleshoot high read latency, you can use SQL statements to analyze the SPANNER_SYS.READ_STATS* tables. These tables contain statistics about read operations in Cloud Spanner, including the number of reads, read latency, and the number of read errors. By analyzing these tables, you can identify the cause of the high read latency and take appropriate action to resolve the issue. Other options, such as using query parameters to speed up frequently executed queries or changing the Cloud Spanner configuration from multi-region to single region, may not be directly related to the issue of high read latency. Similarly, analyzing the SPANNER_SYS.QUERY_STATS* tables, which contain statistics about query operations, may not be relevant to the issue of high read latency.","comment_id":"757280"},{"comment_id":"755787","content":"C: Use SQL statements to analyze ***** SPANNER_SYS.READ_STATS* tables.","upvote_count":"3","timestamp":"1671982200.0","poster":"pk349"},{"timestamp":"1671771840.0","comment_id":"753860","content":"Selected Answer: C\nC is the correct answer","poster":"GCP72","upvote_count":"4"},{"upvote_count":"1","comment_id":"749874","timestamp":"1671457740.0","content":"B - Is correct","poster":"yylbgevkujgphocvyh"},{"timestamp":"1671419520.0","poster":"range9005","comment_id":"749404","content":"Selected Answer: C\nRead statistics provide insight into how an application is using the database, and are useful when investigating performance issues\n.\nhttps://cloud.google.com/spanner/docs/introspection/read-statistics#when_to_use_read_statistics","upvote_count":"3"}],"exam_id":5,"question_images":[],"choices":{"A":"Use query parameters to speed up frequently executed queries.","D":"Use SQL statements to analyze SPANNER_SYS.QUERY_STATS* tables.","B":"Change the Cloud Spanner configuration from multi-region to single region.","C":"Use SQL statements to analyze SPANNER_SYS.READ_STATS* tables."},"answer_description":"","answer_images":[],"answers_community":["C (93%)","7%"],"topic":"1","question_id":67},{"id":"ZQVUhaIapfIsg5V93FgI","answer":"D","answer_ET":"D","question_text":"You need to redesign the architecture of an application that currently uses Cloud SQL for PostgreSQL. The users of the application complain about slow query response times. You want to enhance your application architecture to offer sub-millisecond query latency. What should you do?","answer_description":"","url":"https://www.examtopics.com/discussions/google/view/92699-exam-professional-cloud-database-engineer-topic-1-question/","timestamp":"2022-12-24 22:54:00","topic":"1","question_id":68,"unix_timestamp":1671918840,"answer_images":[],"choices":{"B":"Configure Bigtable, and modify your application to offload queries.","D":"Configure Memorystore, and modify your application to offload queries.","A":"Configure Firestore, and modify your application to offload queries.","C":"Configure Cloud SQL for PostgreSQL read replicas to offload queries."},"answers_community":["D (81%)","Other"],"discussion":[{"poster":"pk349","upvote_count":"8","content":"D: Configure Memorystore and modify your application to offload queries.","timestamp":"1687697700.0","comment_id":"755740"},{"timestamp":"1689046440.0","comment_id":"772056","upvote_count":"6","content":"Selected Answer: D\nThe question says \"redesign the architecture of an application\" - Caching data using MemoryStore will trigger this. Read replica will require only updating connection details which would not be considered an application redesign.","poster":"csrazdan"},{"upvote_count":"1","timestamp":"1737666240.0","content":"Selected Answer: D\nMemorystore for caching","poster":"887ad17","comment_id":"1345680"},{"content":"Selected Answer: D\n\"sub-millisecond latency\" always involves Memorystore. \nFurthermore, as we are talking about a relational DB (Cloud SQL), BigTable is not a solution to be considered.","upvote_count":"5","comment_id":"1017560","comments":[{"poster":"KIDDO24","comment_id":"1019243","upvote_count":"1","timestamp":"1711585560.0","content":"MEMORY STORE IS ALSO A NO RELATIONAL DB LIKE BIGTABLE"},{"poster":"RaphaelG","content":"\"sub-milisecond latency\" always involves Bigtable if anything","timestamp":"1721457720.0","comment_id":"1127148","upvote_count":"2"}],"poster":"theseawillclaim","timestamp":"1711449960.0"},{"comments":[{"upvote_count":"1","timestamp":"1709691180.0","comment_id":"1000007","content":"Memorystore is an in-memory data store that can provide low-latency access to cached data, but it may not be suitable for all types of queries, and achieving sub-millisecond latency depends on factors such as data size and query complexity.","poster":"learnazureportal"}],"content":"The correct answer is ==> B. Configure Bigtable, and modify your application to offload queries.","poster":"learnazureportal","comment_id":"1000006","timestamp":"1709691060.0","upvote_count":"1"},{"timestamp":"1701017100.0","poster":"KennyHuang","comment_id":"907454","content":"Selected Answer: D\nThe recommended approach is to configure Memorystore (Redis) and modify your application to offload queries. This will allow you to take advantage of the sub-millisecond query latency provided by in-memory caching and significantly improve the performance of your application.","upvote_count":"2"},{"upvote_count":"2","comment_id":"881833","poster":"felipeschossler","content":"Selected Answer: D\nD. The only thing that achieves submilli seconds of response is Redis: https://blog.bytebytego.com/p/ep22-latency-numbers-you-should-know","timestamp":"1698338040.0"},{"content":"D.\nTo meet demands of low latency at increased scale and reduced cost you need an in-memory datastore. Redis and Memchaced are among the most popular. Memorystore is a fully managed in-memory data store service for Redis and Memcached at Google Cloud.","upvote_count":"1","timestamp":"1697134200.0","poster":"PATILDXB","comment_id":"868676"},{"timestamp":"1694538060.0","content":"D.\nA is wrong since Firestore would not offer sub-millisecond response. C would help, but sub-millisecond would still be hard to achieve. The question gives no justification for Bigtable and sub-millisecond response does strongly suggest reads from memory rather than disk. That leaves D as the best answer.","poster":"dynamic_dba","upvote_count":"4","comment_id":"837314"},{"comment_id":"788790","timestamp":"1690373640.0","poster":"muky31dec","content":"Selected Answer: D\nAns is D","upvote_count":"1"},{"timestamp":"1688450880.0","content":"C: read replica","comment_id":"765369","poster":"ssaporylo","upvote_count":"2"},{"upvote_count":"1","comment_id":"764770","poster":"SVGoogle89","content":"sub-millisecs -> BigTable","comments":[],"timestamp":"1688388660.0"},{"comments":[{"upvote_count":"7","content":"Sorry D is the correct answer\nhttps://cloud.google.com/blog/topics/developers-practitioners/what-memorystore/","comment_id":"755840","timestamp":"1687705200.0","poster":"GCP72"}],"content":"Selected Answer: C\nC is the correct answer","poster":"GCP72","timestamp":"1687679520.0","upvote_count":"2","comment_id":"755571"},{"content":"Selected Answer: B\nCan't find any proper docs for this case, so let's try elimination. I'm not so sure though.\nA and C - don't give us sub-millisecond query latency.\nI don't see how Memorystore can help with queries, since it's cache, so eliminate D as well.","timestamp":"1687636440.0","comments":[{"comment_id":"766331","upvote_count":"1","timestamp":"1688535480.0","content":"C read replica with additional indexes give benefits. PG => BT dev efforts to migrate","poster":"ssaporylo"}],"upvote_count":"2","comment_id":"755189","poster":"chelbsik"}],"question_images":[],"exam_id":5,"isMC":true},{"id":"jir0oOjwE3s6eqY3bFo4","unix_timestamp":1671533880,"question_id":69,"url":"https://www.examtopics.com/discussions/google/view/92167-exam-professional-cloud-database-engineer-topic-1-question/","choices":{"C":"Create a SQL Server 2019 Standard on High Memory machine type with 16 vCPUs, 104 GB of RAM, and 4 TB of SSD.","A":"Create a SQL Server 2019 Standard on Standard machine type with 4 vCPUs, 15 GB of RAM, and 800 GB of solid-state drive (SSD).","B":"Create a SQL Server 2019 Standard on High Memory machine type with at least 16 vCPUs, 104 GB of RAM, and 200 GB of SSD.","D":"Create a SQL Server 2019 Enterprise on High Memory machine type with 16 vCPUs, 104 GB of RAM, and 500 GB of SSD."},"answer_description":"","timestamp":"2022-12-20 11:58:00","answer_images":[],"answer":"C","answers_community":["C (100%)"],"question_images":[],"isMC":true,"question_text":"You need to migrate existing databases from Microsoft SQL Server 2016 Standard Edition on a single Windows Server 2019 Datacenter Edition to a single Cloud SQL for SQL Server instance. During the discovery phase of your project, you notice that your on-premises server peaks at around 25,000 read IOPS. You need to ensure that your Cloud SQL instance is sized appropriately to maximize read performance. What should you do?","answer_ET":"C","topic":"1","exam_id":5,"discussion":[{"content":"Selected Answer: C\nGiven that Google SSD performance is related to the size of the disk in an order of 30 IOPS for each GB, ti would require at least 833 GB to handle 25000 IOPS, the only answer that exceeds this value is C.\nhttps://cloud.google.com/compute/docs/disks/performance","comment_id":"751589","timestamp":"1703110800.0","poster":"fredcaram","upvote_count":"11"},{"timestamp":"1725581880.0","poster":"learnazureportal","comment_id":"1000010","content":"The correct answer is ==> B. Create a SQL Server 2019 Standard on High Memory machine type with at least 16 vCPUs, 104 GB of RAM, and 200 GB of SSD\n\nfocus on \"at least\" keyword","upvote_count":"2"},{"comment_id":"837323","timestamp":"1710270960.0","poster":"dynamic_dba","content":"C.\nD is wrong since the IOPS would not improve based upon the edition of SQL Server. IOPS increases with the amount of storage, so the most amount of storage is C with 4 TB. I checked this using the GCP console and C is correct.","upvote_count":"2"},{"comment_id":"755844","upvote_count":"2","content":"Selected Answer: C\nAgree C is the correct answer","timestamp":"1703523840.0","poster":"GCP72"},{"upvote_count":"1","comment_id":"755739","poster":"pk349","content":"C: Create a SQL Server 2019 Standard on High Memory machine type with 16 vCPUs, ***** 104 GB of RAM, and 4 TB of SSD.","timestamp":"1703516040.0"},{"timestamp":"1703069880.0","upvote_count":"3","comment_id":"750774","content":"Selected Answer: C\nA disk size of 4TB or greater provides more throughput and IOPS. Storage: >= 4TB for the best IOPS https://cloud.google.com/sql/docs/sqlserver/best-practices#admin","poster":"Popa"}]},{"id":"Umk4KKbCCscZDoQs5ed5","url":"https://www.examtopics.com/discussions/google/view/92243-exam-professional-cloud-database-engineer-topic-1-question/","answer_description":"","answer_images":[],"question_text":"You are managing a small Cloud SQL instance for developers to do testing. The instance is not critical and has a recovery point objective (RPO) of several days. You want to minimize ongoing costs for this instance. What should you do?","isMC":true,"exam_id":5,"choices":{"A":"Take no backups, and turn off transaction log retention.","D":"Turn on automated backup, and turn on transaction log retention.","B":"Take one manual backup per day, and turn off transaction log retention.","C":"Turn on automated backup, and turn off transaction log retention."},"timestamp":"2022-12-20 23:22:00","unix_timestamp":1671574920,"question_images":[],"topic":"1","answers_community":["C (100%)"],"discussion":[{"poster":"hussain.sain","content":"Why not A. its a testing environment. why we need to have backup when we are low on budget.","timestamp":"1718640180.0","comment_id":"1232010","upvote_count":"2"},{"timestamp":"1696305300.0","poster":"juliorevk","comment_id":"1023539","content":"Selected Answer: C\nC and not B because as per: https://cloud.google.com/sql/docs/mysql/backup-recovery/backups\nOn-demand backups are not automatically deleted the way automated backups are. They persist until you delete them or until their instance is deleted. Because they are not automatically deleted, on-demand backups can have a long-term effect on your billing charges.","upvote_count":"1"},{"timestamp":"1695718020.0","poster":"theseawillclaim","upvote_count":"1","comment_id":"1017562","content":"Selected Answer: C\nC is the one. \nManual backups are always discouraged, and transaction log can be removed for a cheap, dev DB."},{"timestamp":"1682584620.0","poster":"felipeschossler","content":"Selected Answer: C\nI think that is C but I didn't find any link that corroborates with my opinion ðŸ˜¢","comment_id":"882405","upvote_count":"1"},{"poster":"Pilot50","upvote_count":"1","content":"Selected Answer: C\nB is manual process and can't be the right approach for automation","comment_id":"861498","timestamp":"1680643260.0"},{"timestamp":"1678648860.0","comment_id":"837327","upvote_count":"3","content":"C.\nA is wrong since there is an RPO. B requires manual intervention which partly defeats the object of using a managed service like Cloud SQL. D is wrong since retaining transaction logs would permit point-in-time recovery which is not required. That leaves C.","poster":"dynamic_dba"},{"upvote_count":"1","content":"Selected Answer: C\nAutomatic backups are incremental where as manual backups are full. Other than compute time for manual backup, storage costs will also increase.","poster":"csrazdan","timestamp":"1673415780.0","comment_id":"772069"},{"comment_id":"755848","timestamp":"1671987960.0","upvote_count":"3","poster":"GCP72","content":"Selected Answer: C\nC. Turn on automated backup, and turn off transaction log retention."},{"poster":"pk349","upvote_count":"1","comment_id":"755738","timestamp":"1671980040.0","content":"C: Turn on automated backup, and turn off transaction log retention."},{"poster":"fredcaram","comment_id":"751591","upvote_count":"2","timestamp":"1671574920.0","content":"Selected Answer: C\nThere is no need to have the overload of using a manual backup, you could schedule an automatic one once a day"}],"answer":"C","answer_ET":"C","question_id":70}],"exam":{"provider":"Google","isBeta":false,"isImplemented":true,"id":5,"isMCOnly":true,"name":"Professional Cloud Database Engineer","lastUpdated":"11 Apr 2025","numberOfQuestions":132},"currentPage":14},"__N_SSP":true}