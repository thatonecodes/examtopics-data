{"pageProps":{"questions":[{"id":"lyLy54fzd1BgtLxdVB4m","question_images":[],"timestamp":"2022-12-20 00:08:00","answer_images":[],"answer_ET":"B","isMC":true,"question_id":301,"unix_timestamp":1671491280,"exam_id":13,"question_text":"You are training an object detection machine learning model on a dataset that consists of three million X-ray images, each roughly 2 GB in size. You are using Vertex AI Training to run a custom training application on a Compute Engine instance with 32-cores, 128 GB of RAM, and 1 NVIDIA P100 GPU. You notice that model training is taking a very long time. You want to decrease training time without sacrificing model performance. What should you do?","discussion":[{"comment_id":"779882","comments":[{"comment_id":"949838","poster":"djo06","timestamp":"1689168720.0","content":"tf.distribute.OneDeviceStrategy uses parallel training on one GPU","upvote_count":"2"}],"poster":"smarques","timestamp":"1674038700.0","upvote_count":"7","content":"Selected Answer: C\nI would say C.\n\nThe question asks about time, so the option \"early stopping\" looks fine because it will no impact the existent accuracy (it will maybe improve it).\n\nThe tf.distribute.Strategy reading the TF docs says that it's used when you want to split training between GPUs, but the question says that we have a single GPU.\n\nOpen to discuss. :)"},{"timestamp":"1675915380.0","upvote_count":"5","content":"Selected Answer: B\nWe don't have money problems, and we need something that doesn't impair the performance of the model.\nSo I think it's good to change GPU for TPU","poster":"enghabeth","comment_id":"802811","comments":[{"content":"replacing the NVIDIA P100 GPU with a v3-32 TPU, could potentially speed up the training process, but it may require modifying the custom training application to be compatible with TPUs.","poster":"tavva_prudhvi","upvote_count":"1","comment_id":"850216","timestamp":"1679755620.0"}]},{"upvote_count":"4","poster":"phani49","comment_id":"1329159","timestamp":"1734641100.0","content":"Selected Answer: D\nD. Use the tf.distribute.Strategy API and run a distributed training job:\n • Why it’s correct:\n • Distributed training splits the dataset and workload across multiple machines and GPUs/TPUs, dramatically reducing training time.\n • The tf.distribute.Strategy API supports both synchronous and asynchronous distributed training, allowing scaling across multiple GPUs or TPUs in Vertex AI.\n • It is specifically designed for handling large datasets and computationally intensive tasks.\n • Example Strategies:\n • MultiWorkerMirroredStrategy: For synchronous training on multiple machines with GPUs.\n • TPUStrategy: For training across multiple TPUs.\n • Scales horizontally, effectively handling massive datasets like the 3-million-image X-ray dataset."},{"upvote_count":"2","timestamp":"1732638480.0","content":"Selected Answer: D\nD is thr right answer","poster":"AB_C","comment_id":"1318178"},{"upvote_count":"2","comment_id":"1286137","content":"Selected Answer: B\nsince compute engine is being used, seems like GPU upgrade makes sense","timestamp":"1726721040.0","poster":"Th3N1c3Guy"},{"timestamp":"1725876360.0","content":"Selected Answer: D\nThe difficulty of this question is it's pure ambiguity. Two of the answer DO change the hardware, so this is obviously an option. The distribute strategy is clearly the right choice (D) assuming we are allowed more hardware to distribute it over.\nPeople are saying \"we cannot change the hardware so it's B\", but B is a change of hardware to TPU anyway, which would require a code change, at which point D would be implemented anyway.","upvote_count":"2","poster":"baimus","comment_id":"1280833"},{"poster":"MultiCloudIronMan","timestamp":"1725622800.0","content":"Selected Answer: D\nI have seen two or even 3 of this question and there are strong debates on the answer, I want to suggest D, because Yes, distributed training can work with your setup of 32 cores, 128 GB of RAM, and 1 NVIDIA P100 GPU. However, the efficiency and performance will depend on the specific framework and strategy you use. The important thing about this answer is that it does not affect quality","comment_id":"1279537","upvote_count":"2"},{"comment_id":"1274310","upvote_count":"2","content":"Selected Answer: B\nin the question it says 3 Million xrays each with 2 GB , it will round upto 6M in size, TPU are exactly designed to accelerate ML tasks and it does massive parallelism, so i would go with B , i would directly omit A , C coz it is more about preventing and not directly aimed at reducing downtime, D is viable solution but comapring with B it is not.","poster":"Jason_Cloud_at","timestamp":"1724900760.0"},{"content":"Selected Answer: B\nAgree with B","comment_id":"1233723","upvote_count":"2","timestamp":"1718896920.0","poster":"dija123"},{"timestamp":"1713983220.0","comment_id":"1201578","upvote_count":"3","poster":"inc_dev_ml_001","content":"Selected Answer: B\nI would say B:\n\nA. Increse memory doesn't mean necessary a speed up of the process, it's not a batch-size problem\nB. It seems a image -> Tensorflow situation. So transforming image into tensors means that a TPU works better and maybe faster\nC. It's not a overfitting problem\nD. Same here, it's not a memory or input-size problem"},{"upvote_count":"1","timestamp":"1713717060.0","comment_id":"1199757","comments":[{"content":"https://www.tensorflow.org/guide/distributed_training#onedevicestrategy\n-> D","comment_id":"1199758","upvote_count":"1","poster":"pinimichele01","timestamp":"1713717060.0"}],"poster":"pinimichele01","content":"https://www.tensorflow.org/guide/distributed_training#onedevicestrategy"},{"content":"Selected Answer: D\nIn my eyes the only solution is distributed training. 3 000 000 x 2GB = 6 Petabytes worth of data. No single device will get you there.","comment_id":"1162426","upvote_count":"3","poster":"Werner123","timestamp":"1709202180.0"},{"upvote_count":"2","content":"Selected Answer: B\nAgree with JamesDoes","poster":"ludovikush","timestamp":"1709136540.0","comment_id":"1161781"},{"upvote_count":"4","timestamp":"1700065020.0","comment_id":"1071613","content":"Selected Answer: B\nB as it have only one GPU hence in D distributed not efficient","poster":"Mickey321"},{"timestamp":"1699978500.0","poster":"pico","upvote_count":"1","content":"f the question didn't specify the framework used, and you want to choose an option that is more framework-agnostic, it's important to consider the available options. \n\nGiven the context and the need for a framework-agnostic approach, you might consider a combination of options A and D. Increasing instance memory and batch size can still be beneficial, and if you're using a deep learning framework that supports distributed training (like TensorFlow or PyTorch), implementing distributed training (Option D) can further accelerate the process.","comment_id":"1070618"},{"timestamp":"1699725660.0","content":"Selected Answer: B\nI would go with B as v3-32 TPU offers much more computational power than a single P100 GPU, and this upgrade should provide a substantial decrease in training time.\n\nAlso tf.distributestrategy is good to perform distreibuted training on multiple GPUs or TPUs but the current setup has just one GPU which makes it the second best option provided the architecture uses multiple GPUs.\n\nIncrease in memory may allow large batch size but wont address the fundamental problem which is over utilised GPU\n\nEarly stopping is good for avoiding overfitting when model already starts performing at its best. Its good to reduce overall training time but wont improve the training speed","comment_id":"1067999","poster":"Krish6488","upvote_count":"5"},{"timestamp":"1694596800.0","comments":[{"content":"TPUs (Tensor Processing Units) are Google's custom-developed application-specific integrated circuits (ASICs) used to accelerate machine learning workloads. They are often faster than GPUs for specific types of computations. However, not all models or training pipelines will benefit from TPUs, and they might require code modification to fully utilize the TPU capabilities.","timestamp":"1699383600.0","upvote_count":"1","poster":"tavva_prudhvi","comment_id":"1065095"}],"comment_id":"1006409","upvote_count":"3","content":"Selected Answer: B\nGiven the options and the goal of decreasing training time, options B (using TPUs) and D (distributed training) are the most effective ways to achieve this goal\n\nC. Enable early stopping in your Vertex AI Training job:\n\nEarly stopping is a technique that can help save training time by monitoring a validation metric and stopping the training process when the metric stops improving. While it can help in terms of stopping unnecessary training runs, it may not provide as substantial a speedup as other options.","poster":"pico"},{"comment_id":"1003855","timestamp":"1694336880.0","content":"Selected Answer: C\nA. Increase the instance memory to 512 GB and increase the batch size.\n> this will not necessarily decrease training time\nB. Replace the NVIDIA P100 GPU with a v3-32 TPU in the training job. Most Voted\n> TPU can sacrifice performance\nC. Enable early stopping in your Vertex AI Training job. \n> YES, this decreases training time without sacrificing performance, if set properly\nD. Use the tf.distribute.Strategy API and run a distributed training job. \n> No idea .... But I believe the type of machine and architecture cannot be changed as per the wording of the question.","comments":[{"comment_id":"1065096","timestamp":"1699383660.0","poster":"tavva_prudhvi","content":"Early stopping is a method that allows you to stop training once the model performance stops improving on a validation dataset. While it can prevent overfitting and save time by stopping unnecessary training epochs, it does not inherently speed up the training process.","upvote_count":"1"}],"poster":"andresvelasco","upvote_count":"1"},{"upvote_count":"2","timestamp":"1690808220.0","content":"Option D, using the tf.distribute.Strategy API for distributed training, can be beneficial for improving training efficiency, but it would require additional resources and complexity to set up compared to simply using a TPU.\n\nTherefore, replacing the NVIDIA P100 GPU with a v3-32 TPU in the Vertex AI Training job would be the most effective way to decrease training time while maintaining or even improving model performance","comment_id":"968123","poster":"PST21"},{"comment_id":"958107","comments":[{"content":"I should note that the batch size should be lower than even 6-8 images because the model weights will also take the GPU memory.","timestamp":"1690301040.0","poster":"[Removed]","upvote_count":"1","comment_id":"962867"}],"upvote_count":"5","timestamp":"1689919500.0","content":"Selected Answer: B\nI don't understand why so many people are voting for D (tf.distribute.Strategy API). If we look at our training infrastructure, we can see the bottleneck is obviously the GPU, which has 12GB or 16GB memory depending on the model (https://www.leadtek.com/eng/products/ai_hpc(37)/tesla_p100(761)/detail). This means we can afford to have a batch size of only 6-8 images (2GB each) even if we assume the GPU is utilized 100%. And remember the training size is 3M, which means each epoch will have 375-500K steps in the best case.\n\nWith 32-cores and 128GB memory, we are able to afford higher batch sizes (e.g., 32), so moving to TPU will accelerate the training.\n\nA is wrong because we can't afford a larger batch size with the current GPU. D is wrong because you don't have multiple GPUs and your current GPU is saturated. C is a viable option, but it seems less optimal than B.","poster":"[Removed]"},{"poster":"djo06","content":"Selected Answer: D\ntf.distribute.OneDeviceStrategy uses parallel training on one GPU","timestamp":"1689234420.0","comment_id":"950455","upvote_count":"1"},{"poster":"SamuelTsch","timestamp":"1688789400.0","upvote_count":"1","content":"Selected Answer: C\nwent with C","comment_id":"946154"},{"timestamp":"1685965260.0","poster":"Voyager2","comments":[{"timestamp":"1686793560.0","comment_id":"923645","poster":"julliet","upvote_count":"1","content":"to run distribution job you need to have more than 1 GPU. we have exactly one here"}],"upvote_count":"1","comment_id":"915397","content":"Selected Answer: D\nD. Use the tf.distribute.Strategy API and run a distributed training job\nOption B replaces GPU with TPU which is not the best option for image procesing. Early stop will affect model performance."},{"poster":"julliet","comment_id":"905375","upvote_count":"1","timestamp":"1684889640.0","content":"Selected Answer: A\nwent with A"},{"poster":"M25","upvote_count":"2","timestamp":"1683609780.0","content":"Selected Answer: B\nWent with B","comment_id":"892787"},{"comment_id":"853173","timestamp":"1680003060.0","comments":[{"poster":"JamesDoe","comments":[{"comment_id":"853178","timestamp":"1680003120.0","content":"GPU*...","poster":"JamesDoe","upvote_count":"1"}],"content":"Single CPU/woker* \nCPU cores does not apply here, right? \nhttps://www.tensorflow.org/guide/distributed_training","timestamp":"1680003120.0","comment_id":"853177","upvote_count":"1"}],"content":"Selected Answer: B\nI think it's B. \nEarly stopping would sacrifice model performance. We're also running on a single compute engine machine with, so distributed training is not available to us - right? Only have one single GPU/CPU/worker imo...\nSo throw money at the problem and switch to TPUs, hence B?","poster":"JamesDoe","upvote_count":"3"},{"content":"Selected Answer: C\nC seems most conservative & the easiest 1st step, before moving to expensive options","timestamp":"1679118060.0","upvote_count":"1","poster":"Yajnas_arpohc","comment_id":"842494"},{"timestamp":"1678390920.0","upvote_count":"3","poster":"TNT87","content":"Selected Answer: D\nAnswer D","comments":[{"comment_id":"872371","upvote_count":"1","timestamp":"1681711380.0","poster":"TNT87","content":"The simplest way to get started with distributed training is a single machine with multiple GPU devices. A TensorFlow distribution strategy from the tf.distribute.Strategy API will manage the coordination of data distribution and gradient updates across all GPUs\n\nhttps://cloud.google.com/blog/topics/developers-practitioners/distributed-training-and-hyperparameter-tuning-tensorflow-vertex-ai"}],"comment_id":"834340"},{"comment_id":"825566","comments":[{"content":"ML-Google , it is make any sense to apply google service as much as possible.","poster":"John_Pongthorn","comments":[{"comment_id":"825570","content":"ML-Google , it makes any sense to apply google services as much as possible first","poster":"John_Pongthorn","upvote_count":"1","timestamp":"1677652680.0"}],"timestamp":"1677652620.0","comment_id":"825568","upvote_count":"1"}],"timestamp":"1677652560.0","poster":"John_Pongthorn","upvote_count":"3","content":"Selected Answer: D\nD: https://www.tensorflow.org/guide/distributed_training#overview"},{"upvote_count":"4","comment_id":"820117","poster":"shankalman717","content":"Selected Answer: D\nD. Use the tf.distribute.Strategy API and run a distributed training job.\n\nGiven that the dataset is very large and the current instance is not making the most of its 32 cores, it is necessary to use distributed training to parallelize the training process and reduce training time. Using the tf.distribute.Strategy API, one can distribute the training across multiple GPUs or TPUs, allowing for faster model training without sacrificing performance. Increasing instance memory and enabling early stopping may not necessarily lead to significant improvements in training time, while replacing the GPU with a TPU may require additional changes to the model and code, which can be time-consuming.","timestamp":"1677214200.0"},{"upvote_count":"3","content":"Selected Answer: D\nhas to be d \nOption C suggests enabling early stopping, which can help to improve model performance by allowing the training process to terminate early if the model stops improving. However, this will not directly reduce training time and may not be sufficient to significantly reduce training time when working with a large dataset.\n\nUsing the tf.distribute.Strategy API to run a distributed training job is the most likely to significantly reduce training time without sacrificing model performance when working with a large dataset like the one described in the scenario.","timestamp":"1672921860.0","comment_id":"766609","comments":[{"comments":[{"timestamp":"1699383780.0","content":"However, the reference to tf.distribute.Strategy implies that we are working within the TensorFlow ecosystem, as this is a TensorFlow-specific API. In a more general sense, the recommendation would be to use a distributed training strategy appropriate to the machine learning framework being used.\n\nIf we are not constrained to TensorFlow, the equivalent recommendation would be:\n\nUse a distributed training approach appropriate for your machine learning framework","poster":"tavva_prudhvi","comment_id":"1065097","upvote_count":"1"}],"poster":"mlgh","comment_id":"775552","content":"But it doesn't say it is a TF model.","upvote_count":"2","timestamp":"1673708520.0"}],"poster":"zeic"},{"content":"Selected Answer: C\nC\n- https://cloud.google.com/blog/products/gcp/hyperparameter-tuning-on-google-cloud-platform-is-now-faster-and-smarter","comment_id":"750307","upvote_count":"2","timestamp":"1671491280.0","poster":"hiromi"}],"topic":"1","answer":"B","answers_community":["B (48%)","D (35%)","Other"],"choices":{"A":"Increase the instance memory to 512 GB and increase the batch size.","D":"Use the tf.distribute.Strategy API and run a distributed training job.","C":"Enable early stopping in your Vertex AI Training job.","B":"Replace the NVIDIA P100 GPU with a v3-32 TPU in the training job."},"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/92135-exam-professional-machine-learning-engineer-topic-1-question/"},{"id":"bh7eNLbX4BmBmbaTaMLv","answer_ET":"D","isMC":true,"answer_images":[],"question_text":"You are a data scientist at an industrial equipment manufacturing company. You are developing a regression model to estimate the power consumption in the company’s manufacturing plants based on sensor data collected from all of the plants. The sensors collect tens of millions of records every day. You need to schedule daily training runs for your model that use all the data collected up to the current date. You want your model to scale smoothly and require minimal development work. What should you do?","choices":{"B":"Develop a custom TensorFlow regression model, and optimize it using Vertex AI Training.","D":"Develop a regression model using BigQuery ML.","A":"Train a regression model using AutoML Tables.","C":"Develop a custom scikit-learn regression model, and optimize it using Vertex AI Training."},"answer_description":"","timestamp":"2022-12-17 11:12:00","unix_timestamp":1671271920,"answers_community":["D (66%)","A (34%)"],"exam_id":13,"answer":"D","question_images":[],"topic":"1","question_id":302,"url":"https://www.examtopics.com/discussions/google/view/91897-exam-professional-machine-learning-engineer-topic-1-question/","discussion":[{"timestamp":"1681381680.0","comment_id":"869285","upvote_count":"14","content":"Selected Answer: D\nThe key is to understand the amount of data that needs to be used for training - the sensor collects tens of millions of records every day and the model needs to use all the data up to the current date. \nThere is a limitation for AutoML is 100M rows -> https://cloud.google.com/vertex-ai/docs/tabular-data/classification-regression/prepare-data","poster":"niketd"},{"comments":[{"comment_id":"1324784","content":"AutoML modal limits: https://cloud.google.com/vertex-ai/docs/quotas#tabular_1","upvote_count":"1","poster":"Laur_C","timestamp":"1733878560.0"}],"timestamp":"1733878560.0","content":"Selected Answer: A\nOld question - quota for AutoML (now Vertex AI) is Between 1,000 and 200,000,000 rows so should be able to handle well. Plus \"minimal development work\" is usually a key word for AutoML (Vertex Ai)","upvote_count":"2","comment_id":"1324783","poster":"Laur_C"},{"comment_id":"1200171","upvote_count":"1","poster":"pinimichele01","content":"Selected Answer: D\nThere is a limitation for AutoML is 100M rows -> https://cloud.google.com/vertex-ai/docs/tabular-data/classification-regression/prepare-data","timestamp":"1713788520.0"},{"comment_id":"1094587","upvote_count":"2","timestamp":"1702389300.0","content":"I go for A","poster":"vale_76_na_xxx","comments":[]},{"poster":"Mickey321","timestamp":"1700065560.0","content":"Selected Answer: A\nEither A or D . Since not stated where is sensor data stored . hence go for A","comment_id":"1071621","upvote_count":"2"},{"poster":"PST21","timestamp":"1687961700.0","comments":[{"comment_id":"1067032","content":"it says \"use all the data collected up to the current date\" not a just a selection of \"relevant\" (?!) data","poster":"maukaba","timestamp":"1699602660.0","upvote_count":"1"}],"comment_id":"936749","upvote_count":"2","content":"Ans D. BigQuery ML allows you to schedule daily training runs by incorporating the latest data collected up to the current date. By specifying the appropriate SQL query, you can include all the relevant data in the training process, ensuring that your model is updated regularly."},{"upvote_count":"3","content":"Selected Answer: A\nI would go with A because it states that it requires minimal development work. Not sure tho, correct me if I’m wrong","poster":"ggwp1999","comment_id":"893042","timestamp":"1683631140.0"},{"comment_id":"892788","poster":"M25","content":"Selected Answer: D\nWent with D","upvote_count":"1","timestamp":"1683609780.0"},{"timestamp":"1680004980.0","poster":"JamesDoe","upvote_count":"3","comment_id":"853210","content":"Selected Answer: A\nOld question, the quotas were removed when they moved AutoML into VertexAI.\nhttps://cloud.google.com/vertex-ai/docs/quotas#model_quotas#tabular"},{"comment_id":"842497","poster":"Yajnas_arpohc","upvote_count":"1","timestamp":"1679118540.0","content":"Would go w A given the specifics mentioned in question. \n\nBigQuery is an unnecessary distraction IMO (e.g. why would we assume BigQuery and not BigTable!)"},{"timestamp":"1678390860.0","upvote_count":"2","comment_id":"834338","content":"Selected Answer: D\nAnswer D\nhttps://cloud.google.com/blog/products/data-analytics/automl-tables-now-generally-available-bigquery-ml\n\nThis legacy version of AutoML Tables is deprecated and will no longer be available on Google Cloud after January 23, 2024. All the functionality of legacy AutoML Tables and new features are available on the Vertex AI platform. See Migrate to Vertex AI to learn how to migrate your resources.","poster":"TNT87"},{"content":"Selected Answer: A\nYou require minimal development work and the question doesn't mention if your data is stored in BQ","upvote_count":"1","timestamp":"1677018840.0","poster":"FherRO","comment_id":"817240"},{"timestamp":"1673250180.0","content":"Selected Answer: D\nAnswer is D, AutoML has 200M rows as limits","comment_id":"770132","poster":"Ade_jr","upvote_count":"3"},{"upvote_count":"1","comment_id":"769477","content":"Selected Answer: A\nA and D seem both good, but A works better, for me.","timestamp":"1673185740.0","poster":"ares81"},{"content":"Selected Answer: A\nBut BQML also has limits on training data\nhttps://cloud.google.com/bigquery-ml/quotas","comment_id":"760985","poster":"mymy9418","timestamp":"1672316700.0","upvote_count":"2"},{"comment_id":"750325","poster":"hiromi","upvote_count":"3","content":"Selected Answer: D\nVote for D\nA dosen't work because AutoML has limits on training data\n- https://www.examtopics.com/exams/google/professional-machine-learning-engineer/view/10/","timestamp":"1671492540.0","comments":[{"comment_id":"768556","timestamp":"1673095620.0","poster":"behzadsw","upvote_count":"1","comments":[{"timestamp":"1679450760.0","content":"it's more than 10M. the training needs to use all the data collected up to the current date","poster":"adarifian","upvote_count":"2","comment_id":"846584"}],"content":"Wrong. The limit is 200 M records. We have 10M records. see:\nhttps://cloud.google.com/automl-tables/docs/quotas"}]},{"timestamp":"1671271920.0","comment_id":"747986","content":"Selected Answer: D\nBigQuery ML can scale smoothly and requires minimal development work.\nModel can be build using SQL queries rather than writing custom code.","upvote_count":"3","poster":"mil_spyro"}]},{"id":"fGKe5CHeFzKQmm1TXGwy","question_id":303,"answer_description":"","isMC":true,"question_text":"You built a custom ML model using scikit-learn. Training time is taking longer than expected. You decide to migrate your model to Vertex AI Training, and you want to improve the model’s training time. What should you try out first?","unix_timestamp":1671271740,"answer_images":[],"timestamp":"2022-12-17 11:09:00","choices":{"A":"Migrate your model to TensorFlow, and train it using Vertex AI Training.","B":"Train your model in a distributed mode using multiple Compute Engine VMs.","C":"Train your model with DLVM images on Vertex AI, and ensure that your code utilizes NumPy and SciPy internal methods whenever possible.","D":"Train your model using Vertex AI Training with GPUs."},"answers_community":["C (68%)","D (25%)","4%"],"exam_id":13,"discussion":[{"poster":"bc3f222","timestamp":"1742109900.0","content":"Selected Answer: C\nscikit-learn so Option D no good, Option C, training with DLVM images on Vertex AI and optimizing code with NumPy and SciPy, would be more appropriate in your scenario.","comment_id":"1399156","upvote_count":"1"},{"content":"Selected Answer: B\nScikit-learn models are typically CPU-based, and many of their algorithms can benefit from parallelization when the workload is distributed","timestamp":"1740245700.0","comment_id":"1360197","poster":"desertlotus1211","upvote_count":"1"},{"comment_id":"1322489","poster":"rajshiv","timestamp":"1733424540.0","upvote_count":"1","content":"Selected Answer: D\nDLVM are typically designed for deep learning workloads and do not provide as much benefit for scikit-learn training. Utilizing GPUs for acceleration is best, as scikit-learn can benefit from GPU-accelerated libraries."},{"comment_id":"1320679","timestamp":"1733082240.0","poster":"FireAtMe","content":"Selected Answer: A\nD is wrong. Not every model in scikit-learn need GPUs or gradients.","upvote_count":"1","comments":[{"upvote_count":"1","comment_id":"1320682","poster":"FireAtMe","timestamp":"1733082300.0","content":"C , I chose the wrong number"}]},{"comment_id":"1318156","content":"Selected Answer: D\nGPU Acceleration: Scikit-learn can leverage GPUs for certain algorithms, especially those involving matrix operations, which are common in many machine learning models. GPUs excel at parallel processing, significantly reducing training time compared to CPUs.\nVertex AI Training: Vertex AI Training makes it easy to use GPUs. You can specify the type and number of GPUs in your training job configuration, and Vertex AI handles the infrastructure setup.\nMinimal Code Changes: You might need to make minor adjustments to your code to ensure it utilizes the GPU, but generally, scikit-learn integrates well with GPUs.","poster":"AB_C","upvote_count":"1","timestamp":"1732636920.0"},{"content":"Selected Answer: D\nOptions B and C may also be relevant in certain scenarios, but they are generally more involved and might require additional considerations. Option B can be effective for large-scale training tasks, but it might add complexity and overhead. Option C could be helpful, but the impact on training time might not be as immediate and substantial as using GPUs.","upvote_count":"2","timestamp":"1699979760.0","comment_id":"1070648","poster":"pico"},{"content":"Selected Answer: D\nD: Training your model with GPUs can provide a substantial speedup, especially for deep learning models or models that require a lot of computation. This option is likely to have a significant impact on training time.\n\nNOT C: While optimizing code can help improve training time to some extent, it may not provide as significant a speedup as the other options. However, it's still a good practice to optimize your code.","timestamp":"1694597520.0","upvote_count":"1","comment_id":"1006421","poster":"pico"},{"comment_id":"1003864","poster":"andresvelasco","upvote_count":"3","timestamp":"1694337300.0","content":"Selected Answer: C\nI dont think scikit-learn would support GPU or distribution, so based on \"What should you try out first?\" I think > C. Train your model with DLVM images on Vertex AI, and ensure that your code utilizes NumPy and SciPy internal methods whenever possible."},{"upvote_count":"4","comment_id":"947523","timestamp":"1688932260.0","content":"why not B? Vertex AI provides the ability to distribute training tasks across multiple Compute Engine VMs, which can parallelize the workload and significantly reduce the training time for large datasets and complex models.","poster":"blobfishtu"},{"poster":"PST21","comment_id":"936764","upvote_count":"2","content":"Option D is not the optimal choice for a scikit-learn model since scikit-learn does not have native GPU support. Option C, training with DLVM images on Vertex AI and optimizing code with NumPy and SciPy, would be more appropriate in your scenario.","timestamp":"1687962300.0"},{"upvote_count":"1","content":"Ans - D. quickest improvement in training time with minimal modifications to your existing scikit-learn model, trying out Option D and training your model using Vertex AI Training with GPUs is the recommended first step.","poster":"PST21","timestamp":"1687962180.0","comment_id":"936762"},{"upvote_count":"4","poster":"Scipione_","timestamp":"1684412760.0","comment_id":"901195","content":"Selected Answer: C\nA) Migrate your model to TensorFlow, and train it using Vertex AI Training.\nNot the first thing to do.\nB) Train your model in a distributed mode using multiple Compute Engine VMs.\nCould be not easy and fast.\nD)Train your model using Vertex AI Training with GPUs\nsklearn does not support GPUs\n\nAlso, most of scikit-learn assumes data is in NumPy arrays or SciPy sparse matrices of a single numeric dtype.\nI choose C as the correct answer."},{"content":"Selected Answer: C\nWent with C","timestamp":"1683609780.0","upvote_count":"1","comment_id":"892789","poster":"M25"},{"comment_id":"872364","upvote_count":"1","content":"Selected Answer: C\nAnswer C","timestamp":"1681711140.0","poster":"TNT87"},{"upvote_count":"1","timestamp":"1677272160.0","content":"How about using sklearn's multi-core? Considering multiple jobs, could we choose item B?\nhttps://machinelearningmastery.com/multi-core-machine-learning-in-python/","comment_id":"820936","poster":"guilhermebutzke"},{"comment_id":"802815","poster":"enghabeth","timestamp":"1675915620.0","content":"Selected Answer: C\nhttps://scikit-learn.org/stable/faq.html#will-you-add-gpu-support","upvote_count":"1"},{"timestamp":"1674912840.0","comment_id":"790620","poster":"John_Pongthorn","upvote_count":"1","content":"Selected Answer: C\nC is correct absolutely\nhttps://console.cloud.google.com/marketplace/details/click-to-deploy-images/deeplearning?_ga=2.139171125.787784554.1674450530-1146240914.1659613735&project=quantum-hash-240404"},{"poster":"behzadsw","timestamp":"1673096700.0","comment_id":"768568","upvote_count":"4","content":"Selected Answer: C\nScikit learn does not support GPU:s\nhttps://scikit-learn.org/stable/faq.html#will-you-add-gpu-support"},{"upvote_count":"1","poster":"emma_aic","timestamp":"1672131240.0","comment_id":"758287","content":"C\n\nNo D \nhttps://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers?hl=ko#scikit-learn"},{"poster":"mymy9418","timestamp":"1671613140.0","upvote_count":"3","comment_id":"752035","content":"Selected Answer: C\nGPU is not useful for sciki-learn model\nhttps://scikit-learn.org/stable/faq.html#will-you-add-gpu-support\nbut DLVM did mention it is support scikit-learn framework\nhttps://cloud.google.com/deep-learning-vm"},{"comment_id":"750339","content":"Selected Answer: D\nD (not sure)\n- https://cloud.google.com/vertex-ai/docs/training/code-requirements#gpus","timestamp":"1671494400.0","upvote_count":"1","comments":[{"content":"Changind my vote to C","timestamp":"1671996960.0","upvote_count":"2","comment_id":"755954","poster":"hiromi"}],"poster":"hiromi"},{"comment_id":"747983","timestamp":"1671271740.0","upvote_count":"1","content":"Selected Answer: D\nTraining a machine learning model on a GPU can significantly improve the training time compared to training on a CPU.","poster":"mil_spyro"}],"question_images":[],"answer":"C","url":"https://www.examtopics.com/discussions/google/view/91896-exam-professional-machine-learning-engineer-topic-1-question/","topic":"1","answer_ET":"C"},{"id":"As3gTcLuFzpLLyWH5bHB","answer_ET":"D","exam_id":13,"unix_timestamp":1671267600,"answer":"D","url":"https://www.examtopics.com/discussions/google/view/91884-exam-professional-machine-learning-engineer-topic-1-question/","isMC":true,"answer_images":[],"discussion":[{"poster":"Pau1234","content":"Selected Answer: B\nhttps://cloud.google.com/vertex-ai/docs/evaluation/introduction","timestamp":"1733170680.0","comment_id":"1321088","upvote_count":"1"},{"poster":"pinimichele01","upvote_count":"1","comment_id":"1202716","timestamp":"1714151940.0","content":"Selected Answer: B\nhttps://cloud.google.com/vertex-ai/docs/model-registry/versioning\nModel versioning lets you create multiple versions of the same model. With model versioning, you can organize your models in a way that helps navigate and understand which changes had what effect on the models. With Vertex AI Model Registry you can view your models and all of their versions in a single view. You can drill down into specific model versions and see exactly how they performed."},{"comment_id":"1199544","upvote_count":"1","timestamp":"1713688020.0","content":"Selected Answer: B\nagree with pico","poster":"gscharly"},{"poster":"Mickey321","content":"Selected Answer: B\neither B or D so leaning towards B","upvote_count":"1","timestamp":"1700066280.0","comment_id":"1071642"},{"timestamp":"1694598060.0","comment_id":"1006428","content":"Selected Answer: B\nVertex AI provides a managed environment for machine learning, and creating model versions for each season per year is a structured way to organize and compare models. You can use the Evaluate tab to compare performance metrics easily. This approach is well-suited for the task.","upvote_count":"2","comments":[{"timestamp":"1694598120.0","content":"not D: \n\nVertex ML Metadata is designed for tracking metadata and lineage in machine learning pipelines. While it can store model version information and performance statistics, it might not provide as straightforward a way to compare models across years and seasons as Vertex AI's model versioning and evaluation tools.","comment_id":"1006429","poster":"pico","upvote_count":"1"}],"poster":"pico"},{"content":"Selected Answer: D\nI absolutely do not master this topicm but I would say correct answer is D.\nIt does not sound right to systematically create versions of a model beased on seasonality, if the model has not changed. \"Events\" in metadata sound right.","timestamp":"1694338080.0","poster":"andresvelasco","comment_id":"1003874","upvote_count":"2"},{"comment_id":"936777","content":"Ans D- With Vertex ML Metadata, you can store the performance statistics of each version of your models as events. You can associate these events with specific seasons and years, making it easy to organize and retrieve the data based on the relevant time periods. By storing performance statistics as events, you can capture the necessary information for comparing model versions across years.","poster":"PST21","timestamp":"1687962960.0","upvote_count":"2"},{"comments":[{"timestamp":"1694598300.0","content":"https://cloud.google.com/vertex-ai/docs/evaluation/using-model-evaluation#console","poster":"pico","comment_id":"1006431","upvote_count":"1"}],"upvote_count":"1","comment_id":"915413","content":"Selected Answer: D\nD. Store the performance statistics of each version of your models using seasons and years as events in Vertex ML Metadata. Compare the results across the slices.\nhttps://cloud.google.com/vertex-ai/docs/ml-metadata/analyzing#filtering\nWhich versions of a trained model achieved a certain quality threshold?","poster":"Voyager2","timestamp":"1685966100.0"},{"comment_id":"892790","upvote_count":"1","content":"Selected Answer: D\nWent with D","timestamp":"1683609840.0","poster":"M25","comments":[{"comment_id":"909096","poster":"iskorini","content":"why choose D instead of B?","upvote_count":"1","timestamp":"1685342040.0"}]},{"timestamp":"1683608640.0","comment_id":"892713","poster":"CloudKida","upvote_count":"1","content":"Selected Answer: B\nhttps://cloud.google.com/vertex-ai/docs/model-registry/versioning\nModel versioning lets you create multiple versions of the same model. With model versioning, you can organize your models in a way that helps navigate and understand which changes had what effect on the models. With Vertex AI Model Registry you can view your models and all of their versions in a single view. You can drill down into specific model versions and see exactly how they performed."},{"content":"Selected Answer: B\nYou can compare evaluation results across different models, model versions, and evaluation jobs --> https://cloud.google.com/vertex-ai/docs/evaluation/using-model-evaluation\n\nMetadata mgmt has a very different purpose","poster":"Yajnas_arpohc","timestamp":"1679119260.0","comment_id":"842502","upvote_count":"1"},{"upvote_count":"1","poster":"TNT87","content":"Selected Answer: D\nAnswer D","timestamp":"1678390260.0","comment_id":"834324"},{"content":"Selected Answer: D\nD\n- https://cloud.google.com/vertex-ai/docs/ml-metadata/introduction","timestamp":"1671494640.0","poster":"hiromi","upvote_count":"2","comment_id":"750342"},{"upvote_count":"2","comment_id":"747978","content":"Selected Answer: D\nVote D. It is easy to compare via Vertex ML Metadata UI the performance statistics across the different slices and see how the model performance varies over time.","timestamp":"1671271560.0","poster":"mil_spyro"},{"timestamp":"1671267600.0","upvote_count":"1","comment_id":"747901","poster":"mymy9418","content":"Selected Answer: D\ni think it is D"}],"topic":"1","answers_community":["D (56%)","B (44%)"],"question_images":[],"timestamp":"2022-12-17 10:00:00","answer_description":"","question_id":304,"question_text":"You are an ML engineer at a travel company. You have been researching customers’ travel behavior for many years, and you have deployed models that predict customers’ vacation patterns. You have observed that customers’ vacation destinations vary based on seasonality and holidays; however, these seasonal variations are similar across years. You want to quickly and easily store and compare the model versions and performance statistics across years. What should you do?","choices":{"D":"Store the performance statistics of each version of your models using seasons and years as events in Vertex ML Metadata. Compare the results across the slices.","B":"Create versions of your models for each season per year in Vertex AI. Compare the performance statistics across the models in the Evaluate tab of the Vertex AI UI.","C":"Store the performance statistics of each pipeline run in Kubeflow under an experiment for each season per year. Compare the results across the experiments in the Kubeflow UI.","A":"Store the performance statistics in Cloud SQL. Query that database to compare the performance statistics across the model versions."}}],"exam":{"isMCOnly":true,"isImplemented":true,"name":"Professional Machine Learning Engineer","lastUpdated":"11 Apr 2025","id":13,"numberOfQuestions":304,"isBeta":false,"provider":"Google"},"currentPage":61},"__N_SSP":true}