{"pageProps":{"questions":[{"id":"C0JSsadnTph3DWnOZXue","discussion":[{"content":"it's B","comment_id":"440661","poster":"Manh","timestamp":"1646627400.0","upvote_count":"16"},{"timestamp":"1646043000.0","comment_id":"433682","content":"B. Create a Google Group per department and add all department members to their respective groups. Create a folder per department and grant the respective group the required IAM permissions at the folder level. Add the projects under the respective folders.","poster":"victory108","upvote_count":"10"},{"timestamp":"1735968360.0","upvote_count":"1","poster":"plumbig11","content":"Selected Answer: B\nB. Create a Google Group per department and add all department members to their respective groups. Create a folder per department and grant the respective group the required IAM permissions at the folder level. Add the projects under the respective folders.","comment_id":"1336270"},{"upvote_count":"1","timestamp":"1731277320.0","comment_id":"1209521","content":"it's B","poster":"afsarkhan"},{"content":"Selected Answer: B\nB is ok","comment_id":"719549","timestamp":"1684226040.0","poster":"megumin","upvote_count":"1"},{"timestamp":"1679122920.0","poster":"zellck","content":"Selected Answer: B\nB is the answer.\n\nhttps://cloud.google.com/resource-manager/docs/access-control-folders#best-practices-folders-iam\nUse groups whenever possible to manage principals.\n\nhttps://cloud.google.com/resource-manager/docs/creating-managing-folders\nA folder can contain projects, other folders, or a combination of both. Organizations can use folders to group projects under the organization node in a hierarchy. For example, your organization might contain multiple departments, each with its own set of Google Cloud resources. Folders allow you to group these resources on a per-department basis.","comment_id":"671994","upvote_count":"5"},{"poster":"Nirca","timestamp":"1679072340.0","comment_id":"671614","upvote_count":"1","content":"Selected Answer: B\nB is most appropriate for the use case and principle of least privilege."},{"content":"B is most appropriate for the use case and principle of least privilege.","comment_id":"626633","timestamp":"1672769400.0","upvote_count":"1","poster":"AzureDP900"},{"upvote_count":"1","timestamp":"1668624840.0","poster":"coutcin","comment_id":"602654","content":"Selected Answer: B\nB is correct"},{"comment_id":"520901","timestamp":"1657455600.0","upvote_count":"1","poster":"lxgywil","content":"B is ok"},{"poster":"edilramos","content":"B is ideal for minimal maintenance and maximum overview of IAM permissions as each department's projects start and end.\nManage the users inside Groups will turn it easer.","upvote_count":"5","comment_id":"506798","timestamp":"1655876640.0"},{"upvote_count":"1","poster":"anjuagrawal","content":"Voted B","timestamp":"1654741920.0","comment_id":"497317"},{"upvote_count":"1","timestamp":"1654417920.0","comment_id":"494231","poster":"vincy2202","content":"B is the correct answer"},{"poster":"nqthien041292","timestamp":"1653898560.0","upvote_count":"2","comment_id":"490573","content":"Selected Answer: B\nVote B"},{"content":"I would select B.","timestamp":"1650524340.0","comment_id":"465605","upvote_count":"2","poster":"danielfootc"},{"timestamp":"1648868520.0","content":"B is correct, folder restructure per department and IAM permission for Group is recommended.","poster":"AnilKr","comment_id":"455808","upvote_count":"4"},{"comment_id":"447912","upvote_count":"2","content":"Answer is B","timestamp":"1647742320.0","poster":"Sonu_xyz"},{"content":"Yes, B","comment_id":"438602","poster":"diaga2","upvote_count":"4","timestamp":"1646328780.0"},{"timestamp":"1646029800.0","content":"B is ok","comment_id":"433518","upvote_count":"3","poster":"serious_user"},{"timestamp":"1645887120.0","content":"B is ok","upvote_count":"4","poster":"meh_33","comment_id":"432319"}],"question_text":"You are responsible for the Google Cloud environment in your company. Multiple departments need access to their own projects, and the members within each department will have the same project responsibilities. You want to structure your Google Cloud environment for minimal maintenance and maximum overview of\nIAM permissions as each department's projects start and end. You want to follow Google-recommended practices. What should you do?","answer_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/google/view/60743-exam-professional-cloud-architect-topic-1-question-144/","question_images":[],"choices":{"C":"Create a folder per department and grant the respective members of the department the required IAM permissions at the folder level. Structure all projects for each department under the respective folders.","B":"Create a Google Group per department and add all department members to their respective groups. Create a folder per department and grant the respective group the required IAM permissions at the folder level. Add the projects under the respective folders.","A":"Grant all department members the required IAM permissions for their respective projects.","D":"Create a Google Group per department and add all department members to their respective groups. Grant each group the required IAM permissions for their respective projects."},"question_id":51,"unix_timestamp":1629982320,"answer_ET":"B","timestamp":"2021-08-26 14:52:00","answer":"B","topic":"1","exam_id":4,"answers_community":["B (100%)"],"answer_description":""},{"id":"EkRLaMdBV7eAf7Ds5ZHB","unix_timestamp":1629774600,"question_text":"Your company has an application running as a Deployment in a Google Kubernetes Engine (GKE) cluster. You have separate clusters for development, staging, and production. You have discovered that the team is able to deploy a Docker image to the production cluster without first testing the deployment in development and then staging. You want to allow the team to have autonomy but want to prevent this from happening. You want a Google Cloud solution that can be implemented quickly with minimal effort. What should you do?","timestamp":"2021-08-24 05:10:00","exam_id":4,"question_id":52,"answer_description":"","topic":"1","answer_ET":"C","url":"https://www.examtopics.com/discussions/google/view/60438-exam-professional-cloud-architect-topic-1-question-145/","answers_community":["C (100%)"],"answer":"C","isMC":true,"answer_images":[],"discussion":[{"content":"C is s fine.","upvote_count":"15","comment_id":"438603","poster":"diaga2","timestamp":"1677865020.0"},{"content":"A good option for quickly implementing a solution to prevent deployments to the production cluster without first testing in development and staging would be to configure binary authorization policies for the development, staging, and production clusters. You can then create attestations as part of the continuous integration pipeline.\n\nOption C, \"Configure binary authorization policies for the development, staging, and production clusters. Create attestations as part of the continuous integration pipeline,\" would be the correct choice for this scenario.\n\nBinary authorization is a feature of Google Kubernetes Engine that allows you to enforce policies on the images that are deployed to your clusters. By configuring binary authorization policies for the development, staging, and production clusters, you can ensure that only images that have been attested by an authorized entity are allowed to be deployed to those clusters. You can create the attestations as part of the continuous integration pipeline, which will allow you to verify that the image has been tested before it is deployed to the next environment.","poster":"omermahgoub","upvote_count":"12","comments":[{"poster":"omermahgoub","content":"Option A, \"Configure a Kubernetes lifecycle hook to prevent the container from starting if it is not approved for usage in the given environment,\" would not be a good choice because it would not prevent the deployment of the container to the cluster in the first place.\n\nOption D, \"Create a Kubernetes admissions controller to prevent the container from starting if it is not approved for usage in the given environment,\" would also not be a good choice because it would not prevent the deployment of the container to the cluster in the first place.\n\nOption B, \"Implement a corporate policy to prevent teams from deploying Docker images to an environment unless the Docker image was tested in an earlier environment,\" would be a good option, but it would not be as effective as using binary authorization policies, as it would rely on the team following the policy rather than enforcing it automatically.","upvote_count":"2","timestamp":"1719375600.0","comment_id":"757113"}],"timestamp":"1719375600.0","comment_id":"757111"},{"content":"Selected Answer: C\nBinary authorization for sure.","comment_id":"1336271","poster":"plumbig11","upvote_count":"1","timestamp":"1735968420.0"},{"timestamp":"1726091640.0","content":"Selected Answer: C\nC it is","poster":"Deb2293","comment_id":"836611","upvote_count":"2"},{"timestamp":"1715849040.0","poster":"megumin","content":"Selected Answer: C\nC is ok","comment_id":"719553","upvote_count":"1"},{"poster":"Thornadoo","timestamp":"1706654340.0","upvote_count":"1","content":"Why not A? Need something to be implemented quickly is what the q asks.","comment_id":"639804"},{"comments":[{"timestamp":"1704305760.0","content":"https://cloud.google.com/binary-authorization/docs/overview#policy_model","comment_id":"626640","poster":"AzureDP900","upvote_count":"3"}],"poster":"AzureDP900","comment_id":"626638","timestamp":"1704305700.0","upvote_count":"4","content":"C is right.. \n\nBinary Authorization implements a policy model, where a policy is a set of rules that governs the deployment of container images. Rules in a policy provide specific criteria that an image must satisfy before it can be deployed.\n\nFor more information about the Binary Authorization policy model and other concepts, see Key concepts."},{"comment_id":"545515","upvote_count":"11","poster":"[Removed]","content":"Selected Answer: C\nI got similar question on my exam. Answered C.","timestamp":"1691779920.0"},{"poster":"yogi_508","content":"where the case study questions are available in this website?","upvote_count":"1","comment_id":"505336","timestamp":"1687250340.0"},{"upvote_count":"6","comment_id":"494235","content":"C is the correct answer\nhttps://cloud.google.com/binary-authorization/docs/overview","timestamp":"1685954580.0","poster":"vincy2202"},{"comment_id":"468025","upvote_count":"1","timestamp":"1682511360.0","poster":"Jimjiang","content":"C is fine"},{"upvote_count":"1","comment_id":"465606","timestamp":"1682060400.0","poster":"danielfootc","content":"I think C is the correct answer."},{"content":"C is correct, binary authorization is the solution.","comment_id":"455809","poster":"AnilKr","timestamp":"1680404580.0","upvote_count":"2"},{"upvote_count":"2","poster":"victory108","content":"C. Configure binary authorization policies for the development, staging, and production clusters. Create attestations as part of the continuous integration pipeline.","timestamp":"1677578880.0","comment_id":"433679"},{"comment_id":"433521","timestamp":"1677565920.0","poster":"serious_user","content":"C is ok","upvote_count":"2"},{"content":"C is ok","upvote_count":"2","timestamp":"1677337500.0","poster":"vladik820","comment_id":"431480"},{"timestamp":"1677215460.0","poster":"SweetieS","content":"Sorry, it's C : Configure binary authorization policies for the development, staging, and production clusters. Create attestations as part of the continuous integration pipeline.","upvote_count":"3","comment_id":"430426"},{"poster":"SweetieS","timestamp":"1677215400.0","content":"D is ok.\nhttps://cloud.google.com/binary-authorization/docs/overview","upvote_count":"1","comments":[{"content":"You meant C I guess","upvote_count":"1","poster":"cugena","comment_id":"447415","timestamp":"1679211480.0"}],"comment_id":"430425"}],"choices":{"B":"Implement a corporate policy to prevent teams from deploying Docker images to an environment unless the Docker image was tested in an earlier environment.","A":"Configure a Kubernetes lifecycle hook to prevent the container from starting if it is not approved for usage in the given environment.","C":"Configure binary authorization policies for the development, staging, and production clusters. Create attestations as part of the continuous integration pipeline.","D":"Create a Kubernetes admissions controller to prevent the container from starting if it is not approved for usage in the given environment."},"question_images":[]},{"id":"zacYIGyhTCxsMV33PDx1","unix_timestamp":1629974820,"question_id":53,"exam_id":4,"question_text":"Your company wants to migrate their 10-TB on-premises database export into Cloud Storage. You want to minimize the time it takes to complete this activity, the overall cost, and database load. The bandwidth between the on-premises environment and Google Cloud is 1 Gbps. You want to follow Google-recommended practices. What should you do?","timestamp":"2021-08-26 12:47:00","answer_description":"","topic":"1","answer_ET":"B","url":"https://www.examtopics.com/discussions/google/view/60720-exam-professional-cloud-architect-topic-1-question-146/","answers_community":["B (55%)","D (37%)","8%"],"answer":"B","isMC":true,"answer_images":[],"discussion":[{"comments":[{"poster":"mickeythecraycray","upvote_count":"3","comment_id":"582261","timestamp":"1649315340.0","content":"Will that not increase the Database load?, one of the requirement is to reduce the load of the DB during this operation."},{"comment_id":"512090","poster":"Aiffone","content":"If I can do it in 30hrs, why choose 1 week? i'd go with B","upvote_count":"3","comments":[{"poster":"Aiffone","comment_id":"512257","upvote_count":"2","content":"I mean I'd go with A rather...questions says to spend minimum time and we have 1Gbps to do 10Tb in 30hrs","comments":[{"upvote_count":"2","poster":"Aiffone","timestamp":"1641949980.0","comment_id":"521853","content":"Transfer appliance -A"}],"timestamp":"1640788320.0"},{"timestamp":"1678579020.0","upvote_count":"6","content":"Go home you are drunk","comment_id":"836614","poster":"Deb2293"}],"timestamp":"1640776320.0"},{"timestamp":"1638365940.0","upvote_count":"8","poster":"joe2211","comment_id":"491681","content":"Not about time but \"Google-recommended practices\""},{"timestamp":"1631521680.0","comment_id":"443833","comments":[{"content":"\"If It would take more than one week to upload your data over the network.\" You always need to take in account you connection speed.","comment_id":"1343199","upvote_count":"1","timestamp":"1737318840.0","poster":"valgorodetsky"}],"upvote_count":"3","poster":"MikeB19","content":"This is the correct article to support this question but the article proves the transfer appliance is the correct answer. Right below the transfer calc chart is recommended amount of data for gsutil. Gsutil should be used for data transfer under 1 tb\n\n“Your private data center to Google Cloud Enough bandwidth to meet your project deadline\nfor less than 1 TB of data gsutil”"}],"content":"This is pretty simple. \nTime to transfer using Transfer Appliance: 1-3 weeks (I've used it twice and had a 2-3 week turnaround total)\nTime to transfer using 1Gbps : 30 hours (https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets) \n\nAnswer is D, using gsutil","upvote_count":"104","poster":"pr2web","timestamp":"1631149680.0","comment_id":"441704"},{"comments":[{"content":"Storage transfer service is for online data. It can't serve the purpose if you don't have the connectivity established between on prem and gcp. Which is what we can't assume ourselves in this question.","poster":"RitwickKumar","upvote_count":"1","comment_id":"649156","timestamp":"1660972680.0"}],"timestamp":"1632806820.0","comment_id":"453053","upvote_count":"21","content":"No perfect answer as B and D both have flaws. B is time latency as transfer appliance usually takes weeks; D gsutil applies for less than 1TB. The answer should be storage transfer service for on-premises data, which is not available here. \n\nIf have to choose one I go for B","poster":"gingerbeer"},{"upvote_count":"1","poster":"david_tay","comment_id":"1361026","content":"Selected Answer: B\nB is correct. Search at Gemini it explains in detail why so.","timestamp":"1740403680.0"},{"poster":"hpf97","comment_id":"1344764","upvote_count":"1","timestamp":"1737552300.0","content":"Selected Answer: D\nAnswer A/C : very to setup and does not improve network limitation\nAnswer D: Interesting for PB, not TB... and minimum 2 weeks after purchase to get the appliance (see https://cloud.google.com/blog/products/storage-data-transfer/transfer-appliances-for-simple-secure-performant-data-movement)\nAnswer D is the best : compress would reduce the size, and even with a 1Gbps network, the transfert would take less 3 hours at full speed. Furthermore -m option use multithread."},{"poster":"T12344223","upvote_count":"1","timestamp":"1733504040.0","content":"Selected Answer: B\nB and D sounds feasible but gsutil is not recommended any more so definitely B.\nhttps://cloud.google.com/storage/docs/gsutil\nHowever, I'm not sure if D is replaced with gcloud storage cp.","comment_id":"1322848"},{"content":"Selected Answer: B\nD says compress data, ¿in a single file? it will be more than the limit 5 TB of gsutil, so it is B.","poster":"ccpmad","comment_id":"1226657","timestamp":"1717838280.0","upvote_count":"1"},{"content":"I think the answer is B.\n\nThe main consideration is between B and D. Just thinking if they want the answer to be online transfer, they should have added Online Transfer Service instead of gsutils. Just guessing Google must want us to choose B :)","timestamp":"1716615000.0","poster":"huuthanhdlv","comment_id":"1218017","upvote_count":"1"},{"content":"Selected Answer: B\nB fo sho","comment_id":"1213676","poster":"seetpt","timestamp":"1716107940.0","upvote_count":"1"},{"upvote_count":"2","comment_id":"1209524","content":"D will be most cost effective where as B will incur cost (question asking to consider cost effective solution as well) so D is my answer","poster":"afsarkhan","timestamp":"1715372760.0"},{"content":"Selected Answer: B\nOption B (Data Transfer appliance) is the best choice for efficient and cost-effective data migration while minimizing database load and transfer time. This solution bypasses network limitations and reduces the impact on the on-premises environment, making it ideal for migrating large data sets to the cloud.","comment_id":"1205249","poster":"MFay","upvote_count":"2","timestamp":"1714610220.0"},{"comment_id":"1201116","poster":"gbemimatti","content":"Selected Answer: B\nCompressing the data and uploading it with gsutil -m can be a good optimization for your transfer, but it has limitations to consider:\n\nCompression Overhead: While compressing the data can reduce upload size and potentially speed up transfer, the compression and decompression processes themselves take time and resources. Depending on your data type, the benefit of reduced size might be offset by the processing overhead.\nTransfer Appliance: The recommended approach with the Transfer Appliance already utilizes parallel transfers for faster uploads, potentially making gsutil -m less impactful.\nI will go with B","upvote_count":"2","timestamp":"1713930600.0"},{"upvote_count":"1","timestamp":"1713930540.0","comment_id":"1201115","content":"Compressing the data and uploading it with gsutil -m can be a good optimization for your transfer, but it has limitations to consider:\n\nCompression Overhead: While compressing the data can reduce upload size and potentially speed up transfer, the compression and decompression processes themselves take time and resources. Depending on your data type, the benefit of reduced size might be offset by the processing overhead.\nTransfer Appliance: The recommended approach with the Transfer Appliance already utilizes parallel transfers for faster uploads, potentially making gsutil -m less impactful.\nI will go with B","poster":"gbemimatti"},{"content":"Selected Answer: D\nwith 1 Gbps it will take only 30 hrs so best option is D","comment_id":"1187316","timestamp":"1711960320.0","upvote_count":"2","poster":"342f1c6"},{"poster":"RajSelvaraj","upvote_count":"1","comment_id":"1175384","content":"Option B and D are most feasible options\nOption B will be okay if the size of the data is too huge\nOption D will be good for a few TBs of data. I am assuming 10 TB will fit in this case.\n\nhttps://cloud.google.com/blog/topics/developers-practitioners/how-transfer-your-data-google-cloud","timestamp":"1710630420.0"},{"poster":"madcloud32","content":"Selected Answer: B\nAnswer B. Cp limit is 5 TB max","upvote_count":"3","comment_id":"1163658","timestamp":"1709315760.0"},{"comments":[{"timestamp":"1717838220.0","comment_id":"1226656","upvote_count":"1","content":"D says compress data, in a single file? it will be more than the limit 5 TB of gsutil","poster":"ccpmad"}],"comment_id":"1143072","poster":"OrangeTiger","upvote_count":"2","content":"Selected Answer: D\nI chose D.\nAccording to the link below, 10TB of data can be transferred in 30h. The light blue area is the acceptable line for online transfer.\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets?hl=ja#online_versus_offline_transfer","timestamp":"1707295440.0"},{"comment_id":"1138198","poster":"Pime13","upvote_count":"2","content":"Selected Answer: D\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets","timestamp":"1706856240.0"}],"choices":{"C":"Use a commercial partner ETL solution to extract the data from the on-premises database and upload it into Cloud Storage.","D":"Compress the data and upload it with gsutil -m to enable multi-threaded copy.","B":"Use the Data Transfer appliance to perform an offline migration.","A":"Develop a Dataflow job to read data directly from the database and write it into Cloud Storage."},"question_images":[]},{"id":"yRkGAQ8zntogZtAVhVp6","isMC":true,"discussion":[{"comment_id":"431188","poster":"juma_david","upvote_count":"44","content":"Answer C\nhttps://cloud.google.com/compute/docs/disks/repd-failover","timestamp":"1629872700.0"},{"comment_id":"466595","upvote_count":"42","poster":"[Removed]","timestamp":"1634996820.0","content":"C is right answer. \nC. 1. Attach a regional SSD persistent disk to the first instance. 2. In case of a zone outage, force-attach the disk to the other instance.\ngcs-fuse is slower than of regional SSD PD. \n\n**** Admin: You need to correct lots of questions. Some of the marked answers are nonsense, these must be revisited based on experts comments."},{"comment_id":"1336272","poster":"plumbig11","timestamp":"1735968540.0","upvote_count":"1","content":"Selected Answer: C\nRegional persistent disk"},{"poster":"Sephethus","content":"Selected Answer: B\nBigQuery cannot use customer supplied KMs keys only customer managed keys. The other options add too much complexity to the problem.","comments":[{"timestamp":"1718816580.0","comment_id":"1232982","content":"Somehow I commented on the wrong answer please delete.","poster":"Sephethus","upvote_count":"1"}],"timestamp":"1718816520.0","upvote_count":"1","comment_id":"1232981"},{"upvote_count":"1","timestamp":"1715373060.0","poster":"afsarkhan","comment_id":"1209526","content":"Selected Answer: C\nC makes a better sense than any other option"},{"timestamp":"1713549000.0","poster":"dija123","upvote_count":"1","content":"Selected Answer: C\nAgree with Regional SSD persistent","comment_id":"1198797"},{"timestamp":"1703840400.0","upvote_count":"2","poster":"[Removed]","content":"C\n\nIn the event that the primary zone fails, you can fail over your regional Persistent Disk volume to a VM in another zone by using a force-attach operation. When there's a failure in the primary zone, you might not be able to detach the disk from the VM because the VM can't be reached to perform the detach operation. Force-attach operation lets you attach a regional Persistent Disk volume to a VM even if that volume is attached to another VM","comment_id":"1108509"},{"timestamp":"1694084160.0","upvote_count":"1","comment_id":"1001442","poster":"RaviRS","content":"Selected Answer: C\nI don't get why B has been given as answer... GCS-FUSE brings in additional complexity and it also doesn't serve the same purpose as effectively as regional SSD does."},{"comment_id":"924845","timestamp":"1686889980.0","upvote_count":"1","poster":"DS2023","content":"Selected Answer: C\nAns: C, please check - https://cloud.google.com/compute/docs/disks/high-availability-regional-persistent-disk"},{"poster":"dbsmk","content":"https://cloud.google.com/compute/docs/disks/repd-failover\n\nSeems C is correct","upvote_count":"1","timestamp":"1680147780.0","comment_id":"855245"},{"content":"C is the correct answer,\n\nhttps://cloud.google.com/compute/docs/disks/repd-failover#zonal_failures","comment_id":"768457","poster":"examch","timestamp":"1673089320.0","upvote_count":"1"},{"poster":"surajkrishnamurthy","timestamp":"1671086760.0","upvote_count":"1","comment_id":"745772","content":"Selected Answer: C\nAnswer is C"},{"comment_id":"743154","poster":"zetalexg","content":"Admins please take some time and redo the answers, put them to match at least the most voted ones, would help a lot.","timestamp":"1670868840.0","upvote_count":"7"},{"poster":"ashrafh","timestamp":"1668867300.0","comment_id":"722043","upvote_count":"2","content":"Selected Answer: C\nRegional persistent disk is a storage option that provides synchronous replication of data between two zones in a region. Regional persistent disks can be a good building block to use when you implement HA services in Compute Engine."},{"upvote_count":"1","content":"Selected Answer: C\nC is ok","comment_id":"719560","timestamp":"1668596100.0","poster":"megumin"},{"upvote_count":"1","poster":"Mahmoud_E","timestamp":"1666206180.0","comment_id":"699285","content":"Selected Answer: C\nC is the right answer"},{"content":"Selected Answer: C\nYou want to maximize performance while minimizing downtime and data loss","comment_id":"672039","timestamp":"1663482780.0","poster":"Nirca","upvote_count":"1"},{"comment_id":"649170","poster":"RitwickKumar","content":"Selected Answer: C\nInline with the current architecture itself \"The application writes data to a persistent disk.\"","upvote_count":"1","timestamp":"1660973340.0"},{"upvote_count":"5","poster":"AzureDP900","comment_id":"626647","timestamp":"1656865620.0","content":"C is right.. \n\nhttps://cloud.google.com/compute/docs/disks/high-availability-regional-persistent-disk \nRegional persistent disk is a storage option that provides synchronous replication of data between two zones in a region. Regional persistent disks can be a good building block to use when you implement HA services in Compute Engine.\n\nThe benefit of regional persistent disks is that in the event of a zonal outage, where your virtual machine (VM) instance might become unavailable, you can usually force attach a regional persistent disk to a VM instance in a secondary zone in the same region. To perform this task, you must either start another VM instance in the same zone as the regional persistent disk that you are force attaching, or maintain a hot standby VM instance in that zone. A hot standby is a running VM instance that is identical to the one you are using. The two instances have the same data.\n\nThe force-attach operation executes in less than one minute."},{"timestamp":"1654813860.0","content":"Selected Answer: C\nC would be the answer. Storage bucket in comparison of SSD is not high performance","comment_id":"614242","poster":"munnysh","upvote_count":"1"},{"upvote_count":"3","comment_id":"609898","content":"Selected Answer: B\nAs availability is immediate (storage being redundant across zones), so cheaper compared to regional persistent disk. \nIf performance(IOPS) was a requirement, C would've been apt.","timestamp":"1654029600.0","poster":"sarath"},{"comment_id":"580504","timestamp":"1649032740.0","content":"C In the event that the primary zone fails, you can fail over your regional persistent disk to a VM instance in another zone by using the --force-attach flag with the attach-disk command.","poster":"conmarch2022","upvote_count":"1"},{"upvote_count":"2","timestamp":"1646357100.0","poster":"sri7","content":"Selected Answer: C\nC is the right answer.","comment_id":"560456"},{"upvote_count":"3","content":"Selected Answer: C\nIn the event that the primary zone fails, you can fail over your regional persistent disk to a VM instance in another zone by using the --force-attach flag with the attach-disk command","comment_id":"547474","poster":"bkrish","timestamp":"1644889740.0"},{"upvote_count":"1","content":"Answer C","timestamp":"1644531060.0","comment_id":"544898","poster":"kjindal2003"},{"poster":"tmnd91","upvote_count":"2","comment_id":"522416","timestamp":"1642016700.0","content":"Selected Answer: C\nB is not correct because GCS mounted as a disk has high latency"},{"content":"I agree C.Thx All!","timestamp":"1641708240.0","poster":"OrangeTiger","comment_id":"519958","upvote_count":"2"},{"upvote_count":"2","comment_id":"502036","poster":"vincy2202","content":"Selected Answer: C\nC is the correct answer\nhttps://cloud.google.com/compute/docs/disks#repds","timestamp":"1639562640.0"},{"content":"Selected Answer: C\nhttps://cloud.google.com/compute/docs/disks/repd-failover","comment_id":"493508","upvote_count":"3","timestamp":"1638596100.0","poster":"sapsant"},{"comment_id":"483600","timestamp":"1637522760.0","content":"Selected Answer: C\nAnswer should be C","upvote_count":"6","poster":"pakilodi"},{"upvote_count":"1","timestamp":"1634967600.0","comment_id":"466439","poster":"MaxNRG","content":"Not B - Performance: Cloud Storage FUSE has much higher latency than a local file system.\nhttps://cloud.google.com/storage/docs/gcs-fuse#notes"},{"comment_id":"460035","upvote_count":"4","poster":"rottzy","content":"C\nRegional persistent disk is a storage option that provides synchronous replication of data between two zones in a region. Regional persistent disks can be a good building block to use when you implement HA services in Compute Engine.\n\nThe benefit of regional persistent disks is that in the event of a zonal outage, where your virtual machine (VM) instance might become unavailable, you can usually force attach a regional persistent disk to a VM instance in a secondary zone in the same region. To perform this task, you must either start another VM instance in the same zone as the regional persistent disk that you are force attaching, or maintain a hot standby VM instance in that zone. A hot standby is a running VM instance that is identical to the one you are using. The two instances have the same data.","timestamp":"1633866060.0"},{"comment_id":"450365","poster":"Ari_GCP","upvote_count":"3","content":"https://cloud.google.com/compute/docs/disks/high-availability-regional-persistent-disk\nOption C","timestamp":"1632408900.0"},{"timestamp":"1632301860.0","upvote_count":"2","content":"Again gcsfuse is good for nothing. C is correct","comment_id":"449387","poster":"MikeB19"},{"poster":"ShadowDragon","upvote_count":"2","content":"Answer is C. https://cloud.google.com/compute/docs/disks#repds","timestamp":"1631877180.0","comment_id":"446558"},{"timestamp":"1630844640.0","content":"C. 1. Attach a regional SSD persistent disk to the first instance. 2. In case of a zone outage, force-attach the disk to the other instance.","comment_id":"439684","poster":"victory108","upvote_count":"1"},{"content":"It's C","comment_id":"438620","poster":"diaga2","upvote_count":"1","timestamp":"1630686840.0"},{"poster":"Nik22","upvote_count":"1","timestamp":"1630550520.0","content":"C is Correct","comment_id":"437592"},{"poster":"Sarin","comment_id":"437552","upvote_count":"1","timestamp":"1630542960.0","content":"My answer is C"}],"answer_ET":"C","question_text":"Your company has an enterprise application running on Compute Engine that requires high availability and high performance. The application has been deployed on two instances in two zones in the same region in active-passive mode. The application writes data to a persistent disk. In the case of a single zone outage, that data should be immediately made available to the other instance in the other zone. You want to maximize performance while minimizing downtime and data loss.\nWhat should you do?","url":"https://www.examtopics.com/discussions/google/view/60583-exam-professional-cloud-architect-topic-1-question-147/","timestamp":"2021-08-25 08:25:00","answer_description":"","choices":{"C":"1. Attach a regional SSD persistent disk to the first instance. 2. In case of a zone outage, force-attach the disk to the other instance.","D":"1. Attach a local SSD to the first instance disk. 2. Execute an rsync command every hour where the target is a persistent SSD disk attached to the second instance. 3. In case of a zone outage, use the second instance.","B":"1. Create a Cloud Storage bucket. 2. Mount the bucket into the first instance with gcs-fuse. 3. In case of a zone outage, mount the Cloud Storage bucket to the second instance with gcs-fuse.","A":"1. Attach a persistent SSD disk to the first instance. 2. Create a snapshot every hour. 3. In case of a zone outage, recreate a persistent SSD disk in the second instance where data is coming from the created snapshot."},"answers_community":["C (89%)","11%"],"exam_id":4,"answer_images":[],"unix_timestamp":1629872700,"question_id":54,"question_images":[],"topic":"1","answer":"C"},{"id":"qHzc4qO871pxKlBu4g3g","answer_images":[],"timestamp":"2021-08-24 05:15:00","choices":{"D":"Import a key in Cloud KMS. Create a dataset in BigQuery using the customer-supplied key option and select the created key.","B":"Generate a new key in Cloud KMS. Create a dataset in BigQuery using the customer-managed key option and select the created key.","C":"Import a key in Cloud KMS. Store all data in Cloud Storage using the customer-managed key option and select the created key. Set up a Dataflow pipeline to decrypt the data and to store it in a new BigQuery dataset.","A":"Generate a new key in Cloud Key Management Service (Cloud KMS). Store all data in Cloud Storage using the customer-managed key option and select the created key. Set up a Dataflow pipeline to decrypt the data and to store it in a new BigQuery dataset."},"question_id":55,"question_text":"You are designing a Data Warehouse on Google Cloud and want to store sensitive data in BigQuery. Your company requires you to generate the encryption keys outside of Google Cloud. You need to implement a solution. What should you do?","discussion":[{"content":"Selected Answer: D\nThe answer is easy. It says keys must be left outside of Google Cloud.\nThis automatically eliminates A / B.\nNow the C option says decrypts before storing it in BigQuery which the point is to encrypt the data while been in BigQuery, D is the only possible answer.","comment_id":"666464","timestamp":"1662932880.0","poster":"alexandercamachop","upvote_count":"18","comments":[{"poster":"Sephethus","upvote_count":"3","timestamp":"1718816700.0","comment_id":"1232984","content":"Except that BigQuery doesn't support customer supplied keys outside of GCP."}]},{"comment_id":"430427","upvote_count":"17","comments":[{"timestamp":"1686881460.0","comment_id":"924792","comments":[{"poster":"[Removed]","comments":[{"poster":"[Removed]","upvote_count":"1","timestamp":"1692910680.0","comment_id":"989493","content":"I mean after being imported to KMS you key is handled like a CMEK and available to BQ service."}],"upvote_count":"2","content":"It is a tricky distinction because of the term collision.\nHowever, \"import key to KMS\" does not mean CSEK.\nCSEK does not get imported or stored in KMS at all. CSEK \"customer supplied\" is per-transaction uploaded by every API call by the user/client (no KMS). \nThis situation \"customer supplied\" means created from non-GCP KMS (could be on-prem or EKM). Once a key is imported to KMS it is treated as CMEK. The API client calling GCS doesn't need to upload the key. It lives in KMS. That is not the same \"per-transaction\" upload as CSEK.","timestamp":"1692910380.0","comment_id":"989489"}],"upvote_count":"2","poster":"SR23222","content":"But CSEK is not supported in BigQuery"}],"timestamp":"1629774900.0","poster":"SweetieS","content":"D is OK"},{"comment_id":"1336273","content":"Selected Answer: D\nImport key and big query;","poster":"plumbig11","timestamp":"1735968600.0","upvote_count":"2"},{"content":"Selected Answer: D\nC - won't encrypt data in BQ with customer key.\nA,B - you will generate key inside the GCP (what is also wrong by requirements)\nD - looks good, but say to select CSEK... but after importing the key to KMS it becomes a customer-managed.\n\nI would select the D","poster":"25lion52","upvote_count":"2","timestamp":"1727518380.0","comment_id":"1290602"},{"poster":"Sephethus","upvote_count":"1","content":"Selected Answer: B\nThe answer cannot be D since BigQuery does not support customer provided keys, only customer managed keys generated in Cloud KMS. So B is the only viable option that doesn't add complexity.","comment_id":"1232983","timestamp":"1718816640.0"},{"comment_id":"1232980","timestamp":"1718816460.0","content":"It cannot be D, BigQuery does not support customer supplied KMS keys, only customer managed keys, B.","upvote_count":"1","poster":"Sephethus"},{"poster":"odacir","comment_id":"1074169","content":"Selected Answer: D\nhttps://cloud.google.com/bigquery/docs/customer-managed-encryption","timestamp":"1700328060.0","upvote_count":"1"},{"timestamp":"1699713780.0","comment_id":"1067901","poster":"thewalker","upvote_count":"2","content":"A, B, C are ruled out as they say Customer Managed keys.\nHence, D."},{"timestamp":"1692812880.0","poster":"devnul","content":"GCP docu says \"BigQuery and BigLake tables don't support Customer-Supplied Encryption Keys (CSEK).\" \n\nHowever, I just tested it and it worked:\n\n1. Create Key\nopenssl rand 32 > ./key2\n\n2. Import into KMS\ngcloud kms keys versions import --import-job csek1 --location us-west1 --keyring csek --key csek --algorithm google-symmetric-encryption --target-key-file ./key2\n\n3. In Cloud Console: select the key when creating a new data set and table in BigQuery","upvote_count":"4","comments":[{"comment_id":"989491","poster":"[Removed]","timestamp":"1692910440.0","upvote_count":"2","content":"Right, term collision with \"customer supplied\" key. However, \"import key to KMS\" does not mean CSEK."}],"comment_id":"988534"},{"comment_id":"874021","timestamp":"1681844100.0","content":"Selected Answer: C\nC - as BigQuery doesn't support Customer Supplier Keys.","poster":"jits1984","upvote_count":"2"},{"poster":"n_nana","timestamp":"1678704420.0","content":"Selected Answer: C\nBigQuery doesn't support CSEK","upvote_count":"1","comments":[{"timestamp":"1682177220.0","upvote_count":"1","content":"BG DOES support CSEK.","comment_id":"877393","poster":"medi01"},{"poster":"n_nana","upvote_count":"1","timestamp":"1678704660.0","comments":[{"timestamp":"1698876720.0","upvote_count":"1","comment_id":"1060088","poster":"A21325412","content":"I would go with C.\n\nhttps://cloud.google.com/bigquery/docs/customer-managed-encryption\nRead that document in the link carefully.\n\n1st paragraph: \"By Default, BigQuery encrypts your content stored at rest\";\n1st bullet point, 2nd paragraph under the [Before you Begin] section: \"BigQuery and BigLake tables don't support Customer-Supplied Encryption Keys (CSEK)\"\n\nThere is also a difference between CMEK and CSEK.\nCMEK: you can create and manage a key using Cloud KMS;\nCSEK: you specify the contents of the key;\n\nRef for CMEK vs CSEK:\nhttps://cloud.google.com/sql/docs/mysql/cmek#:~:text=Note%3A%20Customer%2Dmanaged%20encryption%20keys,specific%20resources%20across%20Google%20Cloud.","comments":[{"timestamp":"1698877140.0","comment_id":"1060092","upvote_count":"1","poster":"A21325412","content":"Even though I'll chose C for the answer over D, because of the terminology in \"BQ using customer-supplied key\", I have an issue with this: \n\nTo me it does not make any sense.\nThe data is being encrypted by some key say K1 to store in Cloud Storage, then Decrypted, to be Re-Encrypted (automatically by say K2 [a google created key]) by BigQuery when being stored. This negates the use of K1 on your Data Storage in BigQuery. \n\nIt makes no sense. If someone sees this differently, I'd love to hear it. Thanks."}]}],"content":"Sorry even C is not correct, why to store the data in bq without encryption.\ndata should be passed encrypted from storage to bq.\nthen Answer is B","comment_id":"837869"},{"upvote_count":"1","poster":"nandoD","comments":[{"timestamp":"1686880980.0","poster":"SR23222","comment_id":"924787","content":"There is a difference between customer managed and customer supplied. Link that you have shared talks about customer managed and not customer supplied","upvote_count":"1"}],"timestamp":"1683008100.0","comment_id":"887067","content":"If you want to control encryption yourself, you can use customer-managed encryption keys (CMEK) for BigQuery. \nhttps://cloud.google.com/bigquery/docs/customer-managed-encryption"}],"comment_id":"837866"},{"content":"Selected Answer: D\nKey work: \"keys outside of Google Cloud\" so you have to import the key. between C and D I go with D.","timestamp":"1678112220.0","poster":"AugustoKras011111","comment_id":"830886","upvote_count":"1"},{"comment_id":"777614","poster":"smachnio","upvote_count":"2","timestamp":"1673869740.0","content":"Selected Answer: D\nD is correct. I had this question on the exam toaday and I go with D.\nExplanation is - Generate the key outside the GCP so C and D are correct.\n\"Set up a Dataflow pipeline to decrypt the data and to store it in a new BigQuery dataset\" is not correct becuase it means that data exist on GCP what is not correct. Only D is correct."},{"upvote_count":"2","comment_id":"768517","content":"Selected Answer: C\nYes, BigQuery and BigLake tables don't support Customer-Supplied Encryption Keys (CSEK). Answer must be either A or C, since the say generate key outside Google Cloud, import the key, hence I go for the answer C.\nhttps://cloud.google.com/bigquery/docs/customer-managed-encryption#before_you_begin\n\nhttps://cloud.google.com/kms/docs/importing-a-key","poster":"examch","timestamp":"1673092620.0"},{"comment_id":"763311","timestamp":"1672599900.0","comments":[{"upvote_count":"1","comment_id":"768496","content":"Yes, BigQuery and BigLake tables don't support Customer-Supplied Encryption Keys (CSEK). Answer must be either A or C, since the say generate key outside Google Cloud, import the key, hence I go for the answer C.","comments":[{"comments":[{"content":"https://cloud.google.com/kms/docs/importing-a-key","timestamp":"1673092380.0","comment_id":"768510","poster":"examch","upvote_count":"2"}],"upvote_count":"1","comment_id":"768497","poster":"examch","content":"https://cloud.google.com/bigquery/docs/customer-managed-encryption#before_you_begin","timestamp":"1673091900.0"}],"timestamp":"1673091840.0","poster":"examch"},{"upvote_count":"1","poster":"[Removed]","comment_id":"989492","content":"You have to know the difference between CSEK and \"imported keys to KMS\". Those are not the same things. CSEK is never stored in KMS, obviously an imported key is. It is then as available as any CMEK to BQ.","timestamp":"1692910620.0"}],"content":"Answer D is incorrect because BigQuery does not support the use of customer-supplied keys to encrypt data at rest. Instead, you can use customer-managed encryption keys in Cloud KMS to encrypt the data in BigQuery. To do this, you can either generate a new key in Cloud KMS (answer A) or import an existing key (answer C). Once you have a key in Cloud KMS, you can create a BigQuery dataset and select the key as the customer-managed key for the dataset. This will enable BigQuery to use the key to encrypt the data in the dataset.","upvote_count":"4","poster":"NodummyIQ"},{"content":"Selected Answer: D\nCorrect answer is D","comment_id":"745774","timestamp":"1671086940.0","upvote_count":"1","poster":"surajkrishnamurthy"},{"poster":"ale_brd_111","upvote_count":"1","comment_id":"735338","timestamp":"1670177820.0","content":"Selected Answer: D\nanswer is D\nhttps://cloud.google.com/bigquery/docs/customer-managed-encryption"},{"poster":"tomahawk003","comment_id":"733750","content":"Answer D.\nQuestions says \"...design data warehouse...\" - would prefer BigQuery","upvote_count":"1","timestamp":"1669987440.0"},{"content":"Selected Answer: D\nD is ok","comment_id":"719635","timestamp":"1668602820.0","poster":"megumin","upvote_count":"1"},{"poster":"Mahmoud_E","content":"Selected Answer: D\nD seems right to me","timestamp":"1666207020.0","comment_id":"699295","upvote_count":"1"},{"upvote_count":"4","comment_id":"652464","timestamp":"1661576640.0","content":"Selected Answer: D\nFor those that saying BigQuery does not support CSEK, read the below. You will need to import you CSEK and it will become CMEK. From there you can use it for BigQuery\nhttps://cloud.google.com/bigquery/docs/customer-managed-encryption","poster":"midgoo"},{"comment_id":"651371","content":"Selected Answer: C\nI spent last 2 hours verifying this info on documentation and this is my result: CSEK works only with GCE e GCS, whereas Bq and Dataflow are fine with CMEK. \nCMEK generates a key inside google, in KMS, whereas CSEK generates the key on prem and then you import in KMS.","comments":[{"poster":"6721sora","upvote_count":"2","timestamp":"1661708760.0","comment_id":"653066","content":"The question clearly says that the keys should be generated outside of Google cloud \nAnswer is D"},{"comment_id":"696343","poster":"melono","upvote_count":"1","timestamp":"1665934920.0","content":"Cloud Dataflow do not currently support objects encrypted with customer-supplied encryption keys.\nhttps://cloud.google.com/storage/docs/encryption/customer-supplied-keys"},{"content":"Id go for D\nhttps://www.trendmicro.com/cloudoneconformity/knowledge-base/gcp/BigQuery/dataset-encryption-cmek.html#","comment_id":"696354","upvote_count":"1","poster":"melono","timestamp":"1665935640.0"}],"poster":"kiappy81","timestamp":"1661359980.0","upvote_count":"2"},{"timestamp":"1661348640.0","comment_id":"651280","upvote_count":"2","content":"Selected Answer: D\nThat's my idea:\nScenario 1. there are Google generated and managed encryption key, which are the default with BigQuery and used to encrypt the data at rest (no action/management needed by the customer)\nScenario 2. Then there are CMEK (customer-managed encryption keys): customer generate encryption keys and then manage them within GCP with Cloud Key Management Service (Cloud KMS), here keys are provided by the customer, managed with Cloud KMS and then provided to BigQuery to encrypt their data at rest\nScenario 3. At the end there are CSEK: customer generate and manage keys outside GCP and have to provide those keys every time is needed through API call to the service. BigQuery seems do not support this. \n\nI think here the correct answer could be D, because we can be in scenario 2 which is supported by BigQuery.","poster":"bossdellacert"},{"comment_id":"647694","timestamp":"1660658760.0","upvote_count":"2","poster":"bchmni","content":"Selected Answer: C\nBecause BigQuery doesn't support CSEK. As of today it can be used in GCS and GCE only."},{"upvote_count":"3","poster":"igor_nov1","content":"Selected Answer: D\nFor those who voting for C.\nIn that case data will be decrypted and stored in BQ which is violation of security policy.","comments":[{"poster":"6721sora","comment_id":"653076","content":"Even if you decrypt and store into bq, it will be encrypted by default but via Google managed keys","upvote_count":"1","timestamp":"1661710200.0"}],"timestamp":"1659951780.0","comment_id":"644049"},{"upvote_count":"1","content":"We can use CSEK with BQ: https://cloud.google.com/docs/security/encryption/customer-supplied-encryption-keys","timestamp":"1658535660.0","poster":"wykofc","comment_id":"635387"},{"poster":"Ric350","timestamp":"1658014740.0","comment_id":"632404","comments":[{"timestamp":"1658493840.0","content":"Ric350, as far as I can see here, BQ supports CSEK https://cloud.google.com/bigquery/docs/customer-managed-encryption","comment_id":"635207","poster":"kiappy81","comments":[{"timestamp":"1659623700.0","upvote_count":"1","content":"\"Your company requires you to generate the encryption keys outside of Google Cloud\" Using CMEK generates a key inside of Google Cloud which does not meet the requirement","poster":"Ric350","comment_id":"642478"}],"upvote_count":"1"},{"content":"I think D is the answer\nhttps://cloud.google.com/bigquery/docs/customer-managed-encryption","poster":"gardislan18","comment_id":"635934","timestamp":"1658651100.0","upvote_count":"1","comments":[]}],"content":"CSEK are NOT supported with BQ and below is the link documentation. CSEK only work with compute engine and cloud storage. I know because it tripped me on on another question. D says to use the CSEK which makes D wrong. The requirement is to generate the key OUTSIDE of google cloud. So that eliminates A and B. This makes C the only option available which meets the requirement of generating the key outside of GCP. You can also import it and use it with BQ.\nhttps://cloud.google.com/docs/security/encryption/customer-supplied-encryption-keys","upvote_count":"3"},{"upvote_count":"1","comment_id":"631403","timestamp":"1657806660.0","content":"D is ok","poster":"mahima123k"},{"content":"Looks like BQ works with CSEK but in a different manner - https://cloud.google.com/bigquery/docs/encryption-at-rest\n\nClient-side encryption\nClient-side encryption is separate from BigQuery encryption at rest. If you choose to use client-side encryption, you are responsible for the client-side keys and cryptographic operations. You would encrypt data before writing it to BigQuery. In this case, your data is encrypted twice, first with your keys and then with Google's keys. Similarly, data read from BigQuery is decrypted twice, first with Google's keys and then with your keys.\n\nImportant: BigQuery does not know if your data has already been encrypted client-side, nor does BigQuery have any knowledge of your client-side encryption keys. If you use client-side encryption, you must securely manage your encryption keys and all aspects of client-side encryption and decryption.","upvote_count":"2","comment_id":"627234","poster":"Deepanshd","timestamp":"1656993600.0"},{"upvote_count":"2","comment_id":"626650","content":"https://cloud.google.com/kms/docs/key-import D is right","poster":"AzureDP900","timestamp":"1656866220.0"},{"content":"Selected Answer: C\nCSEK is not supported in BQ, hence it is not D","poster":"Gwendal","comment_id":"624436","upvote_count":"1","timestamp":"1656480120.0","comments":[{"content":"Wrong\nhttps://cloud.google.com/bigquery/docs/customer-managed-encryption#create_table","comments":[{"content":"CSEK are NOT supported with BQ and below is the link documentation. CSEK only work with compute engine and cloud storage. I know because it tripped me on on another question. D says to use the CSEK which makes D wrong. It's a small detail that easily can be overlooked. Remember, the requirement is to generate the key OUTSIDE of google cloud. So that eliminates A and B. This makes C the only option available which meets the requirement of generating the key outside of GCP. You can also import it and use it with BQ.\nhttps://cloud.google.com/docs/security/encryption/customer-supplied-encryption-keys","poster":"Ric350","timestamp":"1658014800.0","comment_id":"632405","upvote_count":"1"}],"poster":"JohnnyBG","upvote_count":"1","comment_id":"626064","timestamp":"1656753300.0"}]},{"content":"Selected Answer: C\nA and B are out due to requirement stated in question. That leaves C and D. Pretty sure BQ does not support CSEK option, and D clearly states \"customer-supplied key option\". So that leaves C although more complicated but is applicable and works from my understatnding.","timestamp":"1653075960.0","upvote_count":"1","poster":"cjsammaejs","comment_id":"604619"},{"upvote_count":"1","comment_id":"600997","timestamp":"1652424780.0","content":"D is ok, because Now that you can import keys into Cloud KMS, you can import your keys and use them with CMEK-enabled services instead of relying on CSEK. but exact name of BigQuery feature is CMEK option.","poster":"q_on_cloud"},{"timestamp":"1652079540.0","comment_id":"598908","comments":[{"comment_id":"598911","upvote_count":"2","timestamp":"1652079720.0","content":"however C makes no sense as well because it wants to decrypt the data before storing in BQ :(","poster":"ryzior"}],"content":"Selected Answer: C\nI think C is valid and D is not ok - D says CSEK which is not supposed to import/store the key into Cloud KMS (this is the difference between CSEK and CMEK actually) and you provide the key with each api call. The key is stored in .boto locally not in the KMS on the cloud.","poster":"ryzior","upvote_count":"2"},{"timestamp":"1650874440.0","upvote_count":"4","comment_id":"591471","poster":"pacosoft","content":"With answer C, finally after the Dataflow job, the data is not encrypted internally on Big Query with a customer key, you loose the customer key used with Google Store due to the Dataflow process .... External Key Manager with Cloud KMS CMEK would be the correct answer ( B )"},{"upvote_count":"3","poster":"chirischiris","content":"Selected Answer: B\nB Generate a new key in Cloud KMS. Create a dataset in BigQuery using the customer-managed key option and select the created key.\nThis is because I can generate a Externally managed key in KMS for CMEK, but it is not explicit in the answer","comment_id":"557710","timestamp":"1646004240.0","comments":[{"content":"As stated in the question you have to generate the key outside GCP (using your on-premise CA) and bring the key to GCP - so B) is wrong. The correct is D.","timestamp":"1647728340.0","upvote_count":"1","comment_id":"571303","poster":"gcmrjbr"}]},{"content":"Selected Answer: D\nAgree with D","poster":"LoveT","comment_id":"550306","timestamp":"1645197480.0","upvote_count":"2"},{"content":"Selected Answer: D\nI got similar question on my exam. Answered D.","timestamp":"1644612900.0","comment_id":"545518","upvote_count":"2","poster":"[Removed]"},{"content":"Go for D.","timestamp":"1644473460.0","comment_id":"544327","poster":"haroldbenites","upvote_count":"1"},{"content":"Got this question in my exam, answered D","poster":"technodev","comment_id":"527718","upvote_count":"3","timestamp":"1642609500.0"},{"poster":"SolutionArchitect27","content":"B seems to be correct \n\nD : Bigquery does not work with Customer Supplied key (CSEK)\nC : Cloud storage does not work with Customer managed key option ( CMEK)","upvote_count":"1","comments":[{"poster":"gcmrjbr","upvote_count":"1","content":"BQ works with Cloud KMS and KMS supports customer managed key. So D) is correct.","timestamp":"1647728640.0","comment_id":"571305"},{"comment_id":"586424","poster":"[Removed]","upvote_count":"1","content":"I are wrong in C. Cloud store support the both methods: https://cloud.google.com/storage/docs/encryption/customer-managed-keys#customer-supplied","timestamp":"1650043560.0"}],"timestamp":"1641836580.0","comment_id":"521006"},{"content":"Customer-supplied keys (CSEK) are only available for GCS and GCE (https://cloud.google.com/security/encryption/default-encryption#additional_encryption_options_for_cloud_customers). What you can use in BQ is CMEK: keys managed by the client, but generated in KMS, so not outside GCP.\n\nTherefore, I would go for option C, more complex but the only possible.","timestamp":"1641552540.0","poster":"wisss","upvote_count":"2","comment_id":"518901"},{"poster":"SamGCP","timestamp":"1639681860.0","content":"All answers are wrong. Right solution is to use External Key Manager with Cloud KMS\nhttps://cloud.google.com/kms/docs/ekm","upvote_count":"2","comment_id":"503141"},{"timestamp":"1639565760.0","content":"Selected Answer: D\nD seems to be the correct option\nhttps://cloud.google.com/bigquery/docs/customer-managed-encryption","upvote_count":"1","comment_id":"502064","poster":"vincy2202"},{"upvote_count":"4","content":"Selected Answer: D\nvote D","comment_id":"487151","poster":"joe2211","timestamp":"1637913120.0"},{"comment_id":"485589","upvote_count":"2","timestamp":"1637726220.0","content":"BigQuery is using customer-managed key CMEK","poster":"dmc123"},{"poster":"dmc123","content":"Bigquery only support CMEK option, not CSEK, correct?","timestamp":"1637578860.0","comment_id":"484106","comments":[{"timestamp":"1637953560.0","comment_id":"487576","upvote_count":"2","content":"You are correct, with that said none of the other options really seem feasible tho.","poster":"Craigenator"}],"upvote_count":"3"},{"timestamp":"1632497220.0","upvote_count":"1","comment_id":"450956","content":"But BQ doesnt support CSEK, only GCS and compute engine support it","poster":"ACE_ASPIRE","comments":[{"comment_id":"454929","content":"The question is asking to 'generate' the key outside GCP. So, you can create your own CSEK and upload it in KMS, to be used by BigQuery. \nD is correct.","poster":"jask","timestamp":"1633007400.0","upvote_count":"4"},{"poster":"J19G","upvote_count":"3","content":"\"If you want to control encryption yourself, you can use customer-managed encryption keys (CMEK) for BigQuery. Instead of Google managing the key encryption keys that protect your data, you control and manage key encryption keys in Cloud KMS. This topic provides details about this technique.\" - https://cloud.google.com/bigquery/docs/customer-managed-encryption.\n\nTherefore D seems correct. toughs ?","timestamp":"1633310040.0","comment_id":"456830"}]},{"timestamp":"1630543560.0","upvote_count":"4","poster":"Sarin","content":"D looks correct","comment_id":"437556"},{"upvote_count":"1","comment_id":"436221","poster":"XiaobinJiang","timestamp":"1630404780.0","content":"https://cloud.google.com/security/encryption/default-encryption"},{"timestamp":"1630139460.0","content":"D. Import a key in Cloud KMS. Create a dataset in BigQuery using the customer-supplied key option and select the created key.","comment_id":"433707","upvote_count":"3","poster":"victory108"}],"answer_description":"","answers_community":["D (74%)","C (20%)","6%"],"isMC":true,"exam_id":4,"url":"https://www.examtopics.com/discussions/google/view/60439-exam-professional-cloud-architect-topic-1-question-148/","answer":"D","question_images":[],"answer_ET":"D","unix_timestamp":1629774900,"topic":"1"}],"exam":{"provider":"Google","lastUpdated":"11 Apr 2025","isMCOnly":false,"numberOfQuestions":279,"id":4,"isImplemented":true,"isBeta":false,"name":"Professional Cloud Architect"},"currentPage":11},"__N_SSP":true}