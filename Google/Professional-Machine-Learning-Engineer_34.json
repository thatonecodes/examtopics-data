{"pageProps":{"questions":[{"id":"Kl7UzME73zBNMsk9vbJf","question_text":"You trained a model packaged it with a custom Docker container for serving, and deployed it to Vertex AI Model Registry. When you submit a batch prediction job, it fails with this error: \"Error model server never became ready. Please validate that your model file or container configuration are valid. \" There are no additional errors in the logs. What should you do?","isMC":true,"question_id":166,"choices":{"C":"Change the healthRoute value in your model’s configuration to /healthcheck","D":"Pull the Docker image locally, and use the docker run command to launch it locally. Use the docker logs command to explore the error logs","B":"Change the HTTP port in your model’s configuration to the default value of 8080","A":"Add a logging configuration to your application to emit logs to Cloud Logging"},"exam_id":13,"answer_ET":"D","answer_description":"","question_images":[],"unix_timestamp":1705134060,"topic":"1","url":"https://www.examtopics.com/discussions/google/view/131068-exam-professional-machine-learning-engineer-topic-1-question/","timestamp":"2024-01-13 09:21:00","answer":"D","answers_community":["D (63%)","C (19%)","B (19%)"],"answer_images":[],"discussion":[{"comment_id":"1286072","poster":"wences","content":"Selected Answer: B\nFrom StackOverflow:\"Validate the container configuration port; it should use port 8080. This configuration is important because Vertex AI sends liveness checks, health checks, and prediction requests to this port on the container. \" Pulling the container to the local machine is like stepping back and saying, \"It works on my computer,\" then solving the problem as it arises.","upvote_count":"1","timestamp":"1726706340.0"},{"content":"Selected Answer: D\nIsolating the Issue: Running the container locally helps determine if the problem originates from the container configuration or the Vertex AI deployment environment. If the container runs successfully locally, the issue likely lies with Vertex AI.\nDetailed Error Messages: Examining the container logs using docker logs provides detailed error messages specific to the container startup process. These messages can pinpoint the root cause of the model server failure, such as missing dependencies, incorrect model format, or resource limitations.","comment_id":"1197546","upvote_count":"2","timestamp":"1713392340.0","poster":"fitri001"},{"timestamp":"1713175320.0","comment_id":"1195965","content":"Selected Answer: D\nI vote for D. Pull the Docker image locally, and use the docker run command to launch it locally. Use the docker logs command to explore the error logs. Here's why:\n\n1. Local Testing by running the Docker image locally to replicate the environment the model server encounters within Vertex AI.\n2. Using docker logs allows to inspect the detailed error messages generated by the model server during startup. These logs might provide specific clues about the cause of the \"model server never became ready\" error.","upvote_count":"2","poster":"omermahgoub"},{"timestamp":"1710028920.0","content":"Selected Answer: B\nWhen deploying a custom container to Vertex AI Model Registry, need to follow some requirements for the container configuration. One of these requirements is to use the HTTP port 8080 forserving predictions. If using a different port, the model server might not be able to communicate with Vertex AI and cause the error “Error model server never became ready”. To fix this error, change the HTTP port in your model’s configuration to the default value of 8080 and redeploy the container.","upvote_count":"1","comment_id":"1169956","poster":"CMMC"},{"timestamp":"1708109160.0","poster":"guilhermebutzke","content":"Selected Answer: D\nMy Answer: D\n\nA: Not correct: While logging can be helpful for monitoring and debugging, it won't directly address the issue of the model server not becoming ready. \n\nB: Not correct: The error message doesn't indicate a port issue, changing it preemptively might not resolve the underlying problem.\n\nC: Not correct: changing the health route, which could be helpful if the issue is related to health checks, but without further information, it's not the most conclusive option.\n\nD: CORRECT: This option allows you to simulate the deployment environment locally and inspect the logs directly, which can help diagnose the issue with the model server not becoming ready.","upvote_count":"2","comment_id":"1152174"},{"upvote_count":"1","timestamp":"1707362100.0","content":"Selected Answer: C\nDue to Model size or other reasons so that it cannot pass health check before timeout.\n\nhttps://cloud.google.com/knowledge/kb/unable-to-deploy-a-large-model-into-a-vertex-endpoint-000010439","poster":"Yan_X","comment_id":"1144015","comments":[{"comment_id":"1169373","content":"I would revise my answer to D, as healthRoute should be defaulted to /healthcheck.","timestamp":"1709975880.0","upvote_count":"2","poster":"Yan_X"}]},{"timestamp":"1707017280.0","content":"Selected Answer: B\nValidate the container configuration port, it should use port 8080. This configuration is important because Vertex AI sends liveness checks, health checks, and prediction requests to this port on the container. \nhttps://www.appsloveworld.com/coding/flask/15/vertex-ai-deployment-failed","poster":"vaibavi","upvote_count":"1","comment_id":"1139746"},{"content":"Selected Answer: C\nwhen not specifying the health check, the endpoint uses a default health check which only indicates if the http server is ready, not if the model is ready. \nhttps://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#health","upvote_count":"2","timestamp":"1706699040.0","comment_id":"1136701","poster":"sonicclasps"},{"timestamp":"1705134060.0","upvote_count":"4","poster":"pikachu007","comment_id":"1121410","content":"Selected Answer: D\nOption A: Adding logging to Cloud Logging is useful for long-term monitoring but might not provide immediate insights for this specific error.\nOptions B and C: Changing port and health check configuration might be necessary if incorrect, but local debugging often reveals the root cause more effectively."}]},{"id":"U6ha1TNASmATCYCx3eRW","exam_id":13,"topic":"1","question_id":167,"answer_description":"","answer_ET":"C","timestamp":"2024-01-13 09:23:00","url":"https://www.examtopics.com/discussions/google/view/131069-exam-professional-machine-learning-engineer-topic-1-question/","question_images":[],"discussion":[{"content":"Selected Answer: C\nOption A: Cloud Storage FUSE can be slower for large datasets and adds complexity.\nOption B: Vertex AI managed datasets offer convenience but might not match TFRecord performance for large-scale image training.\nOption D: CSV files require manual loading and parsing, increasing overhead.","comment_id":"1121413","timestamp":"1720851780.0","upvote_count":"5","poster":"pikachu007"},{"upvote_count":"4","content":"Selected Answer: C\nTFRecords is a binary storage format optimized for TensorFlow. By storing images as TFRecords, you can improve the I/O efficiency as the data is serialized and can be efficiently loaded off-disk in a batched manner. TFRecordDataset is specifically designed for reading these files efficiently, which helps in minimizing I/O bottlenecks. This approach is typically recommended for large-scale image datasets as it ensures data is read efficiently in a manner suitable for distributed training.","timestamp":"1731439200.0","comment_id":"1210291","poster":"tavva_prudhvi"},{"content":"Selected Answer: C\nagree with pikachu007","upvote_count":"1","timestamp":"1729439760.0","poster":"gscharly","comment_id":"1199233"},{"timestamp":"1729203780.0","comments":[{"poster":"fitri001","content":"B. Vertex AI Managed Dataset: While managed datasets offer convenience, accessing them might involve additional network overhead compared to Cloud Storage FUSE.\nC. TFRecords: Converting images to TFRecords can be an additional processing step, potentially introducing I/O overhead. While TFRecord format might be efficient for some models, it's not strictly necessary for minimizing I/O during data access.\nD. CSV with Image URLs: Reading image URLs from a CSV and fetching each image individually creates significant network traffic, leading to I/O bottlenecks. It's less efficient than directly accessing the images through Cloud Storage FUSE.","upvote_count":"1","comment_id":"1197551","timestamp":"1729203900.0","comments":[{"upvote_count":"1","timestamp":"1729203960.0","poster":"fitri001","content":"TensorFlow Datasets (TFDs): Consider implementing TFDs within your training script. They offer functionalities like parallelized data loading and on-the-fly data augmentation to further optimize training efficiency.\nPreprocessing and Caching: Preprocess data (resizing, normalization) within your TFD pipeline or training script. Cache preprocessed data locally on the VM to avoid redundant processing during training iterations.","comment_id":"1197552"}]}],"comment_id":"1197547","poster":"fitri001","content":"Selected Answer: A\nRead the images by using the tf.data.Dataset.from_tensor_slices function.\n\nHere's why this option is most efficient:\n\nCloud Storage FUSE: This mounts your Cloud Storage bucket directly to the training VM, allowing on-demand access to image data as local files. It minimizes network overhead and data transfer compared to downloading the entire dataset beforehand.\ntf.data.Dataset.from_tensor_slices: This function is suitable for reading data directly from memory. Since Cloud Storage FUSE presents the images as local files, you can leverage this function for efficient data access within your training script.","upvote_count":"1"},{"comment_id":"1157054","upvote_count":"2","poster":"felipepin","content":"Selected Answer: C\nThe TFRecord format is a simple format for storing a sequence of binary records.\n\nProtocol buffers are a cross-platform, cross-language library for efficient serialization of structured data.","timestamp":"1724402460.0"}],"answer":"C","question_text":"You are developing an ML model to identify your company’s products in images. You have access to over one million images in a Cloud Storage bucket. You plan to experiment with different TensorFlow models by using Vertex AI Training. You need to read images at scale during training while minimizing data I/O bottlenecks. What should you do?","answers_community":["C (92%)","8%"],"unix_timestamp":1705134180,"isMC":true,"choices":{"D":"Store the URLs of the images in a CSV file. Read the file by using the tf.data.experimental.CsvDataset function.","A":"Load the images directly into the Vertex AI compute nodes by using Cloud Storage FUSE. Read the images by using the tf.data.Dataset.from_tensor_slices function","C":"Convert the images to TFRecords and store them in a Cloud Storage bucket. Read the TFRecords by using the tf.data.TFRecordDataset function.","B":"Create a Vertex AI managed dataset from your image data. Access the AIP_TRAINING_DATA_URI environment variable to read the images by using the tf.data.Dataset.list_files function."},"answer_images":[]},{"id":"zJTD9IJp1bmXex94I9iQ","exam_id":13,"choices":{"C":"Increase the number of false positives.","D":"Decrease the number of false negatives.","B":"Decrease the recall.","A":"Increase the recall."},"answer_images":[],"question_text":"You work for a social media company. You need to detect whether posted images contain cars. Each training example is a member of exactly one class. You have trained an object detection neural network and deployed the model version to AI Platform Prediction for evaluation. Before deployment, you created an evaluation job and attached it to the AI Platform Prediction model version. You notice that the precision is lower than your business requirements allow. How should you adjust the model's final layer softmax threshold to increase precision?","timestamp":"2021-06-02 22:57:00","question_id":168,"answer_description":"","discussion":[{"comment_id":"389573","upvote_count":"32","poster":"Paul_Dirac","comments":[{"upvote_count":"4","poster":"Swagluke","comment_id":"437436","timestamp":"1646169780.0","content":"I do believe B is the right answer.\nBut D and A aren't exactly the same.\nA. Increase recall can be either \n 1. keeping TP + FN the same but increase TP and decrease FN. Which isn't sure how that's gonna affect Precision since both TP and TP+FP increase.\n 2. keeping TP the same but increase (TP + FN), which is increasing FN (Same as D), not sure how that will affect Precision as well."}],"content":"Decreasing FN increases recall (D). So D and A are the same.\nIncreasing FP decreases precision (C). \n\nAnswer: B (\"improving precision typically reduces recall and vice versa\", https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall)","timestamp":"1640355780.0"},{"poster":"Danny2021","content":"Precision = TruePositives / (TruePositives + FalsePositives)\nRecall = TruePositives / (TruePositives + FalseNegatives)\nA. Increase recall -> will decrease precision\nB. Decrease recall -> will increase precision\nC. Increase the false positives -> will decrease precision\nD. Decrease the false negatives -> will increase recall, reduce precision\nThe correct answer is B.","timestamp":"1646782860.0","comment_id":"441660","upvote_count":"24"},{"upvote_count":"1","poster":"PhilipKoku","comment_id":"1225526","content":"Selected Answer: B\nB) Decrease Recall (increases precision)","timestamp":"1733497920.0"},{"comment_id":"945579","content":"Selected Answer: B\nTo increase precision, you have to decrese recall, increse true positives, increse false negatives and decrease false positives","timestamp":"1704632580.0","poster":"SamuelTsch","upvote_count":"2"},{"comment_id":"892702","content":"Selected Answer: B\nWent with B","poster":"M25","timestamp":"1699513260.0","upvote_count":"3"},{"content":"Selected Answer: B\nOption B is the best approach because decreasing the threshold will increase the precision by reducing the number of false positives.","poster":"Fatiy","comment_id":"824898","upvote_count":"1","timestamp":"1693225260.0"},{"comments":[{"poster":"John_Pongthorn","upvote_count":"1","timestamp":"1689227100.0","content":"WE want to increase Precision, it is the same as decreasing recall. Both are opposed each other.\nhttps://developers.google.com/machine-learning/crash-course/classification/precision-and-recall","comment_id":"774208"}],"content":"Selected Answer: B\nA , C , D they are the same. So I go with B , it is threshold adjustment from 0.5 +-","timestamp":"1689221340.0","comment_id":"774159","poster":"John_Pongthorn","upvote_count":"1"},{"upvote_count":"1","comment_id":"746545","poster":"wish0035","content":"ans: B.\nA: should decrease even more the precission.\nC: will decrease precision\nD: will increase recall (precision would be the same)","timestamp":"1686858540.0"},{"comment_id":"725726","upvote_count":"2","poster":"EFIGO","content":"Selected Answer: B\nPrecision and recall are negatively correlated, when one goes up the other goes down and vice-versa; to increase precidion we need to decrease recall, therefore answer B.\n(To be more complete, answer C and D are wrong because they both would increase recall, according to the recall formula)","timestamp":"1684915620.0"},{"comments":[{"timestamp":"1676472780.0","content":"sorry correct ans is \" B\"","upvote_count":"1","poster":"GCP72","comment_id":"647202"}],"poster":"GCP72","upvote_count":"1","content":"Selected Answer: C\nCorrect answer is \"C\"","timestamp":"1676472720.0","comment_id":"647201"},{"upvote_count":"2","timestamp":"1676317560.0","content":"Answer is D\nIf the dataset does not change, TP + FN is constant.\nFN goes down then TP goes up.\nHence Precision = TP / TP + FP goes up.","comment_id":"646429","poster":"originalliang"},{"timestamp":"1670887140.0","content":"Selected Answer: B\nprecision and recall have negative proportion , so to increase precision reduce recall","poster":"Mohamed_Mossad","upvote_count":"1","comment_id":"615500"},{"poster":"morgan62","upvote_count":"1","timestamp":"1665125400.0","comment_id":"582233","content":"Selected Answer: B\nIt's B.\nC,D is basically ruining your model."},{"upvote_count":"2","poster":"sonxxx","comment_id":"563316","content":"Answer: D\nBecause of Precision should respond the answer how many retrieved items are relevant? In the relation of False Negative / true positives an optima precision need a high number of true positives. If your model is precision is lower than your business requirement is because the model has a high number of false negatives. Check it in: https://en.wikipedia.org/wiki/Precision_and_recall","timestamp":"1662640500.0"},{"comment_id":"534267","content":"Selected Answer: B\ndefinitely B","timestamp":"1658966460.0","upvote_count":"1","poster":"xiaoF"},{"poster":"Sangy22","comment_id":"521638","content":"I think this should be C. The reason is, for one to increase precision, the classification threshold for whether the car is there or not should be kept low. That way, even when the model is not very confident (say only 60% confident), it will say, yes, car is there. What this does is it will crease the times the model says car is present, driving up precision (when it says car is there, car is really there). The consequence of this is, False positives will increase too, reducing recall.\nSo C is my choice.\n\nChoices A and B are not really right, as precision and recall are after-effects, not something you will control ahead.","timestamp":"1657549620.0","upvote_count":"1"},{"comment_id":"498806","upvote_count":"3","timestamp":"1654868340.0","poster":"Bemnet","content":"Answer is B . 100% sure . The only way to affect precision and recall is by adjusting threshold. FN and FP go in opposite direction so C & D are the same. A increasing recall decreases precision ."},{"poster":"santy79","timestamp":"1653057300.0","comment_id":"482694","content":"Selected Answer: B\nexamtopics, It would be good if justification is attached with correct answer\nDecrease recall -> will increase precision","upvote_count":"2"},{"content":"D is correct because if you decrease FN (False Negatives) you will get more TP (True Positives), since the formulas of Recall and Precision are:\n\nPrecision = TP / TP + FP\nRecall = TP / TP + FN\nsource of formulas: scikitlearn documentation\n\nBy increasing the TP you will get higher Precision!\n\n- B is wrong because you CAN increase both Precision and Recall\n- A is incorrect because you CAN increase Precision by decreasing Recall\n- C is incorrect because by increasing False Positives you will reduce precision because of the formula\n\nAnd a picture that might help is: https://en.wikipedia.org/wiki/Precision_and_recall#/media/File:Precisionrecall.svg","comment_id":"464774","timestamp":"1650391140.0","poster":"mousseUwU","upvote_count":"1","comments":[{"poster":"danielp14021990","upvote_count":"1","content":"I think D is not correct. If you change threshold to decrease FN you are very likely to increase False Positive as well. B is exactly what you can influence by using the softmax treshold.","comment_id":"475806","timestamp":"1652209920.0"}]},{"content":"I think it should be D.\nThis question ask the accuracy.\nRecall(A/B) has no relation.\nC will be getting worse.","poster":"pict3","comment_id":"418059","comments":[{"poster":"neohanju","upvote_count":"1","comment_id":"433943","timestamp":"1646064660.0","content":"This question asks to increase precision, which is the accuracy about positives. Decrease the number of false negatives increases recall, or sensitivity. So, only thing that is relevant precision is A."}],"timestamp":"1643684460.0","upvote_count":"2"},{"content":"Should be D","poster":"chohan","upvote_count":"1","comment_id":"384975","timestamp":"1639856460.0"},{"timestamp":"1638896340.0","upvote_count":"1","poster":"salsabilsf","comment_id":"376861","content":"Should be C"},{"comments":[{"comment_id":"389562","poster":"Paul_Dirac","timestamp":"1640355000.0","upvote_count":"5","content":"Wrong formula."}],"upvote_count":"4","poster":"[Removed]","comment_id":"373054","content":"D. Precision = TP/(TP+FN). So to increase precision we need to reduce FN\nhttps://developers.google.com/machine-learning/crash-course/classification/precision-and-recall","timestamp":"1638485820.0"}],"answer_ET":"B","unix_timestamp":1622667420,"question_images":[],"answers_community":["B (94%)","6%"],"topic":"1","answer":"B","isMC":true,"url":"https://www.examtopics.com/discussions/google/view/54314-exam-professional-machine-learning-engineer-topic-1-question/"},{"id":"DGWoea3vZWYG9jP6VBXn","question_id":169,"answer_images":[],"question_text":"You work at an ecommerce startup. You need to create a customer churn prediction model. Your company’s recent sales records are stored in a BigQuery table. You want to understand how your initial model is making predictions. You also want to iterate on the model as quickly as possible while minimizing cost. How should you build your first model?","discussion":[{"comment_id":"1228328","content":"Selected Answer: C\nC) Data preparation in BigQuery. Ease of implementation with AutoML","upvote_count":"5","poster":"PhilipKoku","timestamp":"1733912400.0"},{"timestamp":"1729204260.0","poster":"fitri001","upvote_count":"1","content":"Selected Answer: C\nCost-Effectiveness:\nLeverages BigQuery for data storage and preprocessing, minimizing data movement costs.\nUtilizes Vertex AI's AutoML Tabular training, which is a pay-per-use service, reducing upfront costs compared to custom training environments.\nRapid Iteration:\nAutoML Tabular automates feature engineering and model selection, allowing you to experiment with various configurations quickly.\nYou can focus on refining feature engineering and interpreting model behavior based on AutoML's generated explanations.","comments":[{"content":"why not B?\nImplementing a deep neural network from scratch requires significant development effort and might be overkill for an initial model. Interpretability of deep neural networks can also be challenging.\nWhile TensorFlow BigQueryClient allows data access, it requires writing custom training scripts, increasing development time.","comment_id":"1197556","poster":"fitri001","upvote_count":"1","timestamp":"1729204320.0"}],"comment_id":"1197555"},{"poster":"omermahgoub","upvote_count":"1","timestamp":"1728986640.0","comment_id":"1195966","content":"Selected Answer: C\nYou work at an ecommerce startup. You need to create a customer churn prediction model. Your company’s recent sales records are stored in a BigQuery table. You want to understand how your initial model is making predictions. You also want to iterate on the model as quickly as possible while minimizing cost. How should you build your first model?\n\nA. Export the data to a Cloud Storage bucket. Load the data into a pandas DataFrame on Vertex AI Workbench and train a logistic regression model with scikit-learn.\nB. Create a tf.data.Dataset by using the TensorFlow BigQueryClient. Implement a deep neural network in TensorFlow.\nC. Prepare the data in BigQuery and associate the data with a Vertex AI dataset. Create an AutoMLTabularTrainingJob to tram a classification model.\nD. Export the data to a Cloud Storage bucket. Create a tf.data.Dataset to read the data from Cloud Storage. Implement a deep neural network in TensorFlow."},{"timestamp":"1724510400.0","poster":"Carlose2108","upvote_count":"1","comment_id":"1158043","content":"Selected Answer: C\nI went Option C"},{"upvote_count":"4","timestamp":"1720851900.0","poster":"pikachu007","comment_id":"1121415","content":"Selected Answer: C\nOption A: While logistic regression is interpretable, manual training in Vertex AI Workbench adds time and complexity.\nOptions B and D: Deep neural networks can be powerful but often lack interpretability, making it challenging to understand model decisions. They also require more hands-on model development and infrastructure management."}],"answers_community":["C (100%)"],"timestamp":"2024-01-13 09:25:00","topic":"1","answer":"C","unix_timestamp":1705134300,"answer_ET":"C","question_images":[],"choices":{"C":"Prepare the data in BigQuery and associate the data with a Vertex AI dataset. Create an AutoMLTabularTrainingJob to tram a classification model.","D":"Export the data to a Cloud Storage bucket. Create a tf.data.Dataset to read the data from Cloud Storage. Implement a deep neural network in TensorFlow.","B":"Create a tf.data.Dataset by using the TensorFlow BigQueryClient. Implement a deep neural network in TensorFlow.","A":"Export the data to a Cloud Storage bucket. Load the data into a pandas DataFrame on Vertex AI Workbench and train a logistic regression model with scikit-learn."},"answer_description":"","isMC":true,"url":"https://www.examtopics.com/discussions/google/view/131070-exam-professional-machine-learning-engineer-topic-1-question/","exam_id":13},{"id":"VzbwvpVKTmYZpun4Ej6d","exam_id":13,"choices":{"D":"1. In BigQuery ML, use the CREATE MODEL statement with BOOSTED_TREE_CLASSIFIER as the model type and use BigQuery to handle the data splits.\n2. Use ML TRANSFORM to specify the feature engineering transformations and tram the model using the data in the table.\n3. Compare the evaluation metrics of the models by using a SQL query with the ML.TRAINING_INFO statement.","C":"1. In BigQuery ML, use the CREATE MODEL statement with BOOSTED_TREE_CLASSIFIER as the model type and use BigQuery to handle the data splits.\n2. Use a SQL view to apply feature engineering and train the model using the data in that view.\n3. Compare the evaluation metrics of the models by using a SQL query with the ML.TRAINING_INFO statement.","A":"1. Using Vertex AI Pipelines, add a component to divide the data into training and evaluation sets, and add another component for feature engineering.\n2. Enable autologging of metrics in the training component.\n3. Compare pipeline runs in Vertex AI Experiments.","B":"1. Using Vertex AI Pipelines, add a component to divide the data into training and evaluation sets, and add another component for feature engineering.\n2. Enable autologging of metrics in the training component.\n3. Compare models using the artifacts’ lineage in Vertex ML Metadata."},"answer_images":[],"question_text":"You are developing a training pipeline for a new XGBoost classification model based on tabular data. The data is stored in a BigQuery table. You need to complete the following steps:\n\n1. Randomly split the data into training and evaluation datasets in a 65/35 ratio\n2. Conduct feature engineering\n3. Obtain metrics for the evaluation dataset\n4. Compare models trained in different pipeline executions\n\nHow should you execute these steps?","timestamp":"2024-01-13 15:00:00","question_id":170,"answer_description":"","discussion":[{"timestamp":"1705154400.0","content":"Selected Answer: A\nOption B: While Vertex ML Metadata provides artifact lineage, it's less comprehensive for model comparison than Experiments.\nOptions C and D: BigQuery ML is powerful for in-database model training, but it has limitations in pipeline orchestration, complex feature engineering, and detailed model comparison features, making it less suitable for this scenario.","poster":"pikachu007","upvote_count":"8","comment_id":"1121702"},{"poster":"wences","content":"Selected Answer: A\nCan anyone give a good reason for the answers without using ChatGPT or Gemini?","timestamp":"1726697460.0","comment_id":"1286023","upvote_count":"1"},{"content":"Selected Answer: A\nBQ ML falls a bit short when it comes to building pipelines that include feature engineering and experiment comparison (it's better to use Vertex Pipelines and do the comparisons using Vertex Experiments).","comment_id":"1265498","upvote_count":"1","poster":"tardigradum","timestamp":"1723611840.0"},{"content":"Selected Answer: A\nFlexibility and Control: Vertex AI Pipelines allow you to define a custom pipeline with separate components for data splitting, feature engineering, and XGBoost training using your preferred libraries (like BigQueryClient and xgboost). This provides more control and customization compared to BigQuery ML's limited model types and functionality.\nFeature Engineering and Data Splitting: Separate components enable clear separation of concerns and potentially parallel execution for efficiency.\nAutologging and Model Comparison: Vertex AI autologging simplifies capturing evaluation metrics during training. Vertex AI Experiments offer a centralized interface to compare metrics across different pipeline runs (potentially with varying hyperparameter configurations).","comment_id":"1197558","comments":[{"comment_id":"1197559","upvote_count":"1","content":"why not C & D?\nC & D. BigQuery ML: While BigQuery ML offers some XGBoost functionality, it has limitations:\nLimited Model Types: BigQuery ML doesn't provide the full flexibility of using custom XGBoost libraries with advanced configurations.\nLess Control over Feature Engineering: Feature engineering using SQL views might be restrictive compared to a dedicated component in Vertex AI Pipelines.\nLimited Model Comparison: While ML.TRAINING_INFO provides some insights, Vertex AI Experiments offer a more comprehensive view for comparing models across pipeline runs.","poster":"fitri001","timestamp":"1713393540.0"}],"upvote_count":"1","timestamp":"1713393540.0","poster":"fitri001"},{"poster":"pinimichele01","timestamp":"1713071880.0","comment_id":"1195281","content":"Selected Answer: A\nsee b1a8fae","upvote_count":"1"},{"comment_id":"1194777","upvote_count":"1","timestamp":"1712995140.0","poster":"omermahgoub","content":"Selected Answer: A\nA: Leverage Vertex AI Pipelines and Experiments"},{"poster":"guilhermebutzke","comment_id":"1153636","timestamp":"1708302900.0","upvote_count":"2","content":"Selected Answer: A\nMy Answer: A\n\nA: CORRECT: It involves proper data splitting into training and evaluation sets and conducting feature engineering within the pipeline, fulfilling steps 1 and 2. Enabling autologging of metrics ensures that you can track and compare the performance of different model executions, fulfilling step 3.\n\nB: Not Correct: Better use Vertex AI Experiments\n\nC and D: Not Correct: BigQuery ML lacks functionalities for comparing models across pipeline runs. You would need to rely on external tools or custom scripts to extract and compare evaluation metrics, making the process less streamlined."},{"content":"Selected Answer: A\nCompare models in different pipeline executions -> go for Vertex AI experiments","poster":"b1a8fae","timestamp":"1705911060.0","comment_id":"1128421","upvote_count":"3"}],"answer_ET":"A","unix_timestamp":1705154400,"answers_community":["A (100%)"],"question_images":[],"topic":"1","answer":"A","url":"https://www.examtopics.com/discussions/google/view/131087-exam-professional-machine-learning-engineer-topic-1-question/","isMC":true}],"exam":{"isImplemented":true,"isMCOnly":true,"isBeta":false,"lastUpdated":"11 Apr 2025","name":"Professional Machine Learning Engineer","numberOfQuestions":304,"provider":"Google","id":13},"currentPage":34},"__N_SSP":true}