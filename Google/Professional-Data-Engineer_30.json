{"pageProps":{"questions":[{"id":"Q6VV7uWxg5pMNnCebqO8","choices":{"B":"Send the data to Google Cloud Pub/Sub, stream Cloud Pub/Sub to Google Cloud Dataflow, and store the data in Google BigQuery.","D":"Export logs in batch to Google Cloud Storage and then spin up a Google Cloud SQL instance, import the data from Cloud Storage, and run an analysis as needed.","A":"Send the data to Google Cloud Datastore and then export to BigQuery.","C":"Send the data to Cloud Storage and then spin up an Apache Hadoop cluster as needed in Google Cloud Dataproc whenever analysis is required."},"question_text":"You are deploying 10,000 new Internet of Things devices to collect temperature data in your warehouses globally. You need to process, store and analyze these very large datasets in real time. What should you do?","discussion":[{"upvote_count":"30","timestamp":"1600436280.0","poster":"[Removed]","comments":[{"comment_id":"65689","timestamp":"1600436700.0","poster":"[Removed]","upvote_count":"9","content":"https://cloud.google.com/blog/products/iot-devices/quick-and-easy-way-set-end-end-iot-solution-google-cloud-platform"}],"content":"Answer: B","comment_id":"65684"},{"upvote_count":"26","timestamp":"1601183460.0","poster":"[Removed]","content":"Answer: B\nDescription: Pubsub for realtime, Dataflow for pipeline, Bigquery for analytics","comment_id":"68530"},{"upvote_count":"1","poster":"Abizi","timestamp":"1741086900.0","content":"Selected Answer: B\nB is the correct answer","comment_id":"1364871"},{"poster":"regal_2010","timestamp":"1728912000.0","content":"Selected Answer: B\nAnswer is B","upvote_count":"1","comment_id":"1195530"},{"timestamp":"1716292620.0","poster":"axantroff","comment_id":"1076336","upvote_count":"1","content":"Selected Answer: B\nIn short, B is less complex and more recommended other than D"},{"timestamp":"1713787200.0","comment_id":"1050527","upvote_count":"1","content":"Selected Answer: B\nB. Send the data to Google Cloud Pub/Sub, stream Cloud Pub/Sub to Google Cloud Dataflow, and store the data in Google BigQuery.\n\nHere's why this approach is preferred:\n\nGoogle Cloud Pub/Sub allows for efficient ingestion and real-time data streaming.\nGoogle Cloud Dataflow can process and transform the streaming data in real-time.\nGoogle BigQuery is a fully managed, highly scalable data warehouse that is well-suited for real-time analysis and querying of large datasets.","poster":"rtcpost"},{"poster":"GCP_PDE_AG","comment_id":"975408","timestamp":"1707392820.0","upvote_count":"1","content":"Obviously B."},{"upvote_count":"2","poster":"Maurilio_Cardoso","content":"Selected Answer: B\nPubSub for queue in real time, Dataflow for processing (pipeline) and Bigquery for analyses.","timestamp":"1701302700.0","comment_id":"909752"},{"comment_id":"835672","upvote_count":"1","timestamp":"1694401440.0","content":"Selected Answer: B\nB is correct","poster":"bha11111"},{"timestamp":"1686605400.0","content":"Selected Answer: B\nGCP recommend best practice for streaming data pipeline as option B - pub/sub, dataflow & Bigquery","poster":"DGames","comment_id":"743416","upvote_count":"1"},{"poster":"Nirca","comment_id":"741909","timestamp":"1686495240.0","upvote_count":"1","content":"Selected Answer: B\nB. Send the data to Google Cloud Pub/Sub, stream Cloud Pub/Sub to Google Cloud Dataflow, and store the data in Google BigQuery."},{"comment_id":"731543","content":"B aris tqve yleebo","poster":"gitaexams","upvote_count":"1","timestamp":"1685447280.0"},{"upvote_count":"1","comment_id":"688845","content":"Selected Answer: B\nB of course","poster":"devaid","timestamp":"1680888900.0"},{"poster":"Dip1994","content":"B is the correct answer","comment_id":"641759","timestamp":"1675426500.0","upvote_count":"1"},{"timestamp":"1671141480.0","content":"Selected Answer: B\nAnswer: B\n\nDeafult ETL streaming process: Pub/Sub + Dataflow + Bigquery.","upvote_count":"1","comment_id":"616937","poster":"noob_master"},{"content":"Definitely B","poster":"nexus1_","comment_id":"615438","timestamp":"1670872140.0","upvote_count":"1"},{"content":"Selected Answer: B\nB is the only option for real time process & analysis","poster":"vw13","upvote_count":"1","comment_id":"586419","timestamp":"1665852900.0"},{"content":"The most appropriate is B but BQ can't solve Analyzing data in RT.","poster":"devric","timestamp":"1664915820.0","comment_id":"580912","upvote_count":"1"},{"poster":"samdhimal","timestamp":"1658529180.0","comment_id":"530169","content":"correct answer is Cloud Pub/Sub ---> Dataflow ---> Bigquery\n\nCollected messages containing temperature values will be published to a topic on Cloud Pub/Sub, Messages will be read in streaming mode by Cloud Dataflow, a simplified stream and batch data processing solution, Cloud Datastore will save data to be displayed directly into the UI of the App Engine application, while BigQuery will act as a data warehouse that will enable the execution of more in depth analysis.\n\nReference:\nhttps://cloud.google.com/blog/products/iot-devices/quick-and-easy-way-set-end-end-iot-solution-google-cloud-platform","upvote_count":"2"},{"timestamp":"1652188260.0","upvote_count":"1","content":"B as the need to ingest it, transform and store the Cloud Pub/Sub, Cloud Dataflow, BigQuery is ideal stack to handle the IoT data.\nhttps://cloud.google.com/solutions/mobile/mobile-gaming-analysis-telemetry\nOption A is wrong as the Datastore is not an ideal ingestion service.\nOption C is wrong as Cloud Storage is not an ideal ingestion service and Dataproc is not a data warehousing solution.\nOption D is wrong as Cloud SQL is not a data warehousing solution.","poster":"MaxNRG","comment_id":"475593"},{"content":"Ans: B\nAll options are speaking about cloud storage, but need pub/sub in between to take streaming (IoT) data till storage/ into the cloud.","upvote_count":"2","poster":"anji007","comment_id":"462686","timestamp":"1650037920.0"},{"comment_id":"426366","poster":"sandipk91","upvote_count":"2","timestamp":"1645112520.0","content":"B for sure"},{"upvote_count":"2","timestamp":"1640526720.0","poster":"sumanshu","comment_id":"391193","content":"Vote for B"},{"poster":"amarkan","upvote_count":"3","timestamp":"1626027660.0","content":"no doubt B","comment_id":"265019"},{"timestamp":"1624269060.0","poster":"DataExpert","content":"B is more correct than other options so B is the answer. But if this is actual use case you have to deal with use Cloud BigTable instead of bigquery. So the pipeline will be like this. IOT-Devices -> Cloud Pub/Sub -> Cloud BigTable -> Cloud Data Studio (For real-time analytics)","comment_id":"249268","upvote_count":"7"},{"content":"B is the right answer. You can use cloud data flow for both batch and streaming pipelines. Pub sub will be used to stream data into cloud data flow.","upvote_count":"2","timestamp":"1621056300.0","comment_id":"219556","poster":"Radhika7983"},{"poster":"arghya13","timestamp":"1619796960.0","upvote_count":"3","comment_id":"209477","content":"B is the answer"}],"topic":"1","isMC":true,"question_id":146,"question_images":[],"answer_ET":"B","url":"https://www.examtopics.com/discussions/google/view/16931-exam-professional-data-engineer-topic-1-question-23/","answer":"B","timestamp":"2020-03-18 16:38:00","unix_timestamp":1584545880,"answers_community":["B (100%)"],"answer_images":[],"exam_id":11,"answer_description":""},{"id":"ZiSlqBgGLxZQuHwNAPoa","url":"https://www.examtopics.com/discussions/google/view/130174-exam-professional-data-engineer-topic-1-question-230/","answer_images":[],"isMC":true,"question_text":"You need to modernize your existing on-premises data strategy. Your organization currently uses:\n• Apache Hadoop clusters for processing multiple large data sets, including on-premises Hadoop Distributed File System (HDFS) for data replication.\n• Apache Airflow to orchestrate hundreds of ETL pipelines with thousands of job steps.\n\nYou need to set up a new architecture in Google Cloud that can handle your Hadoop workloads and requires minimal changes to your existing orchestration processes. What should you do?","answer_ET":"B","unix_timestamp":1704282180,"question_id":147,"discussion":[{"timestamp":"1720101660.0","poster":"raaad","upvote_count":"7","content":"Selected Answer: B\nStraight forward","comment_id":"1113840"},{"upvote_count":"1","poster":"datasmg","timestamp":"1725488160.0","content":"Selected Answer: B\nYou can use Dataproc for doing Apache Hadoop process, then Cloud Storage to replace the HDFS, and using Cloud Composer (built in Apache Airflow) for orchestrator.","comment_id":"1166109"},{"poster":"cuadradobertolinisebastiancami","comment_id":"1156775","content":"Selected Answer: B\nAirflow -> composer\nMinimum changes -> Dataproc","timestamp":"1724358540.0","upvote_count":"2"},{"comment_id":"1152734","poster":"JyoGCP","content":"Selected Answer: B\nOption B","timestamp":"1723910220.0","upvote_count":"1"},{"timestamp":"1720862160.0","content":"Selected Answer: B\ndefinitely B","poster":"Matt_108","comment_id":"1121546","upvote_count":"2"},{"upvote_count":"4","poster":"scaenruy","timestamp":"1719999780.0","content":"Selected Answer: B\nCloud Composer -> Airflow","comment_id":"1112717"}],"choices":{"B":"Use Dataproc to migrate Hadoop clusters to Google Cloud, and Cloud Storage to handle any HDFS use cases. Orchestrate your pipelines with Cloud Composer.","C":"Use Dataproc to migrate Hadoop clusters to Google Cloud, and Cloud Storage to handle any HDFS use cases. Convert your ETL pipelines to Dataflow.","D":"Use Dataproc to migrate your Hadoop clusters to Google Cloud, and Cloud Storage to handle any HDFS use cases. Use Cloud Data Fusion to visually design and deploy your ETL pipelines.","A":"Use Bigtable for your large workloads, with connections to Cloud Storage to handle any HDFS use cases. Orchestrate your pipelines with Cloud Composer."},"answer_description":"","exam_id":11,"question_images":[],"topic":"1","answer":"B","timestamp":"2024-01-03 12:43:00","answers_community":["B (100%)"]},{"id":"VtunCdfp0iCHc3pkyLp6","url":"https://www.examtopics.com/discussions/google/view/130340-exam-professional-data-engineer-topic-1-question-231/","answer_ET":"C","answer":"C","question_id":148,"question_text":"You recently deployed several data processing jobs into your Cloud Composer 2 environment. You notice that some tasks are failing in Apache Airflow. On the monitoring dashboard, you see an increase in the total workers memory usage, and there were worker pod evictions. You need to resolve these errors. What should you do? (Choose two.)","answer_images":[],"isMC":true,"discussion":[{"content":"Selected Answer: D\nIf an Airflow worker pod is evicted, all task instances running on that pod are interrupted, and later marked as failed by Airflow. The majority of issues with worker pod evictions happen because of out-of-memory situations in workers.\nYou might want to:\n- (D) Increase the memory available to workers.\n- (C) Reduce worker concurrency. In this way, a single worker handles fewer tasks at once. This provides more memory or storage to each individual task. If you change worker concurrency, you might also want to increase the maximum number of workers. In this way, the number of tasks that your environment can handle at once stays the same. For example, if you reduce worker Concurrency from 12 to 6, you might want to double the maximum number of workers.\n\nSource: https://cloud.google.com/composer/docs/composer-2/optimize-environments","timestamp":"1723809480.0","comment_id":"1152041","poster":"ML6","upvote_count":"7"},{"timestamp":"1743041760.0","poster":"taka5094","content":"Selected Answer: C\nCD\nOn the Monitoring dashboard, in the Workers section, observe the Worker Pods evictions graphs for your environment.\nThe Total workers memory usage graph shows a total perspective of the environment. A single worker can still exceed the memory limit, even if the memory utilization is healthy at the environment level.\nAccording to your observations, you might want to:\n- Increase the memory available to workers.\n- Reduce worker concurrency. \nIn this way, a single worker handles fewer tasks at once. This provides more memory or storage to each individual task. If you change worker concurrency, you might also want to increase the maximum number of workers. In this way, the number of tasks that your environment can handle at once stays the same. For example, if you reduce worker Concurrency from 12 to 6, you might want to double the maximum number of workers.","comment_id":"1410708","upvote_count":"1"},{"poster":"desertlotus1211","comment_id":"1402280","timestamp":"1742734260.0","content":"Answer is B,D","upvote_count":"1"},{"content":"Selected Answer: D\nAnswer C,D \n\nAccording to your observations, you might want to:\n\nIncrease the memory available to workers.\nReduce worker concurrency. In this way, a single worker handles fewer tasks at once. This provides more memory or storage to each individual task. If you change worker concurrency, you might also want to increase the maximum number of workers. In this way, the number of tasks that your environment can handle at once stays the same. For example, if you reduce worker Concurrency from 12 to 6, you might want to double the maximum number of workers.","upvote_count":"1","comments":[],"comment_id":"1216178","poster":"Anudeep58","timestamp":"1732340160.0"},{"content":"Selected Answer: D\nC. Increase the maximum number of workers and reduce worker concurrency. Most Voted\nD. Increase the memory available to the Airflow workers.","poster":"virat_kohli","upvote_count":"1","timestamp":"1732109460.0","comment_id":"1214301"},{"content":"Selected Answer: D\nIf an Airflow worker pod is evicted, all task instances running on that pod are interrupted, and later marked as failed by Airflow. The majority of issues with worker pod evictions happen because of out-of-memory situations in workers.\nYou might want to:\n- Increase the memory available to workers.\n- Reduce worker concurrency. In this way, a single worker handles fewer tasks at once. This provides more memory or storage to each individual task. If you change worker concurrency, you might also want to increase the maximum number of workers. In this way, the number of tasks that your environment can handle at once stays the same. For example, if you reduce worker Concurrency from 12 to 6, you might want to double the maximum number of workers.\nSource: https://cloud.google.com/composer/docs/composer-2/optimize-environments","poster":"ML6","upvote_count":"2","timestamp":"1723809360.0","comment_id":"1152039"},{"content":"Selected Answer: C\nCD It is clear","upvote_count":"3","poster":"qq589539483084gfrgrgfr","comment_id":"1125093","timestamp":"1721223600.0"},{"comment_id":"1121551","timestamp":"1720862340.0","content":"Selected Answer: C\nC & D to me","upvote_count":"2","poster":"Matt_108"},{"upvote_count":"4","poster":"GCP001","timestamp":"1720645200.0","comments":[{"content":"Agree. Straightforward.\nhttps://cloud.google.com/composer/docs/composer-2/optimize-environments#monitor-scheduler\n-> Figure 3. Graph that displays worker pod evictions","comment_id":"1122176","poster":"AllenChen123","upvote_count":"4","timestamp":"1720915260.0"}],"content":"Selected Answer: C\nC and D\nCheck ref for memory optimization - https://cloud.google.com/composer/docs/composer-2/optimize-environments","comment_id":"1119126"},{"comment_id":"1116511","upvote_count":"1","timestamp":"1720423440.0","poster":"qq589539483084gfrgrgfr","content":"Selected Answer: B\nB&D See this-\nhttps://cloud.google.com/composer/docs/composer-2/troubleshooting-dags#task-fails-without-logs\ngo through the suggested fixes for If there are airflow-worker pods that show Evicted"},{"upvote_count":"2","comment_id":"1115296","timestamp":"1720275000.0","content":"Selected Answer: C\nC and D","poster":"Jordan18"},{"comment_id":"1113853","upvote_count":"3","content":"Selected Answer: B\nB&D:\nB : \n- Scaling up the environment size can provide more resources, including memory, to the Airflow workers. If worker pod evictions are occurring due to insufficient memory, increasing the environment size to allocate more resources could alleviate the problem and improve the stability of your data processing jobs.\n\nD:\n- Increase the memory available to the Airflow workers. - Directly increasing the memory allocation for Airflow workers can address the issue of high memory usage and worker pod evictions. More memory per worker means that each worker can handle more demanding tasks or a higher volume of tasks without running out of memory.","timestamp":"1720102140.0","comments":[{"comment_id":"1125731","upvote_count":"2","timestamp":"1721291100.0","poster":"GCP001","content":"why not B ) It s not decreasing concurrency which may cause issue again"}],"poster":"raaad"}],"question_images":[],"topic":"1","answer_description":"","choices":{"E":"Increase the memory available to the Airflow triggerer.","A":"Increase the directed acyclic graph (DAG) file parsing interval.","C":"Increase the maximum number of workers and reduce worker concurrency.","D":"Increase the memory available to the Airflow workers.","B":"Increase the Cloud Composer 2 environment size from medium to large."},"timestamp":"2024-01-04 17:09:00","unix_timestamp":1704384540,"exam_id":11,"answers_community":["C (44%)","D (41%)","B (15%)"]},{"id":"3FQ1WNzT76NHQo8B17C7","choices":{"C":"Set the constraints/gcp.resourceLocations organization policy constraint to in:eu-locations.","A":"Set the constraints/gcp.resourceLocations organization policy constraint to in:europe-west3-locations.","B":"Deploy resources with Terraform and implement a variable validation rule to ensure that the region is set to the europe-west3 region for all resources.","D":"Create a Cloud Function to monitor all resources created and automatically destroy the ones created outside the europe-west3 region."},"question_images":[],"answer_ET":"A","answer":"A","timestamp":"2024-01-03 12:47:00","topic":"1","answer_description":"","unix_timestamp":1704282420,"exam_id":11,"question_text":"You are on the data governance team and are implementing security requirements to deploy resources. You need to ensure that resources are limited to only the europe-west3 region. You want to follow Google-recommended practices.\n\nWhat should you do?","question_id":149,"answers_community":["A (100%)"],"discussion":[{"content":"Selected Answer: A\n- The constraints/gcp.resourceLocations organization policy constraint is used to define where resources in the organization can be created. \n- Setting it to in:europe-west3-locations would specify that resources can only be created in the europe-west3 region.","poster":"raaad","timestamp":"1704385020.0","upvote_count":"11","comments":[{"content":"I am new to this forum. In almost all the questions, the reveal solution is different than the once's discussed here??","poster":"SanjeevRoy91","timestamp":"1710905820.0","upvote_count":"3","comment_id":"1177898"}],"comment_id":"1113862"},{"upvote_count":"2","content":"Selected Answer: A\nhttps://cloud.google.com/resource-manager/docs/organization-policy/defining-locations#location_types","timestamp":"1736246160.0","poster":"b3e59c2","comment_id":"1337524"},{"poster":"chrissamharris","content":"B, D\nB - Increase the Cloud Composer 2 environment size from medium to large.\nIncreasing the environment size will provide more resources (including memory) to the entire environment, which should help mitigate memory usage issues. This will also support scaling if the jobs demand more resources.\nD. Increase the memory available to the Airflow workers.\nIncreasing memory for Airflow workers will directly address the memory usage issue that's causing the pod evictions. By allocating more memory to the workers, they can handle larger tasks or more intensive workloads without failing due to memory constraints.","comment_id":"1287733","timestamp":"1727012280.0","comments":[{"timestamp":"1731804960.0","upvote_count":"2","comment_id":"1313322","content":"this comment is for previous question, not this one.","poster":"hrishi19"}],"upvote_count":"1"},{"comment_id":"1154425","poster":"JyoGCP","timestamp":"1708396080.0","content":"Selected Answer: A\nOption A","upvote_count":"1"},{"comment_id":"1121553","poster":"Matt_108","upvote_count":"2","timestamp":"1705144860.0","content":"Selected Answer: A\nOption A"},{"poster":"scaenruy","content":"Selected Answer: A\nSet the constraints/gcp.resourceLocations organization policy constraint to in:europe-west3-locations.","upvote_count":"2","timestamp":"1704282420.0","comment_id":"1112718"}],"isMC":true,"answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/130175-exam-professional-data-engineer-topic-1-question-232/"},{"id":"CNEtPjuOTeLBMjm8ozlZ","exam_id":11,"timestamp":"2024-01-03 12:52:00","answers_community":["C (100%)"],"topic":"1","answer":"C","answer_ET":"C","discussion":[{"content":"Selected Answer: C\n- BigQuery provides administrative resource charts that show slot utilization and job performance, which can help identify patterns of heavy usage or contention. \n- Additionally, querying the INFORMATION_SCHEMA with the JOBS or JOBS_BY_PROJECT view can provide detailed information about specific queries, including execution time, slot usage, and whether they were queued.","upvote_count":"10","timestamp":"1704385140.0","comments":[{"comment_id":"1124003","timestamp":"1705392600.0","poster":"datapassionate","upvote_count":"3","content":"descrived here:\nhttps://cloud.google.com/blog/products/data-analytics/troubleshoot-bigquery-performance-with-these-dashboards"}],"comment_id":"1113865","poster":"raaad"},{"timestamp":"1730406600.0","upvote_count":"1","comment_id":"1305585","poster":"ToiToi","content":"Selected Answer: C\nWithout doubt, C!"},{"content":"Selected Answer: C\nhttps://cloud.google.com/blog/topics/developers-practitioners/monitor-analyze-bigquery-performance-using-information-schema","upvote_count":"1","poster":"JyoGCP","comment_id":"1152743","timestamp":"1708193820.0"},{"upvote_count":"1","timestamp":"1705144980.0","poster":"Matt_108","comment_id":"1121555","content":"Selected Answer: C\nOption C"},{"content":"Selected Answer: C\nC. Use available administrative resource charts to determine how slots are being used and how jobs are performing over time. Run a query on the INFORMATION_SCHEMA to review query performance.","poster":"scaenruy","comment_id":"1112722","upvote_count":"1","timestamp":"1704282720.0"}],"question_text":"You are a BigQuery admin supporting a team of data consumers who run ad hoc queries and downstream reporting in tools such as Looker. All data and users are combined under a single organizational project. You recently noticed some slowness in query results and want to troubleshoot where the slowdowns are occurring. You think that there might be some job queuing or slot contention occurring as users run jobs, which slows down access to results. You need to investigate the query job information and determine where performance is being affected. What should you do?","unix_timestamp":1704282720,"isMC":true,"question_id":150,"choices":{"D":"Use Cloud Logging to determine if any users or downstream consumers are changing or deleting access grants on tagged resources.","C":"Use available administrative resource charts to determine how slots are being used and how jobs are performing over time. Run a query on the INFORMATION_SCHEMA to review query performance.","A":"Use slot reservations for your project to ensure that you have enough query processing capacity and are able to allocate available slots to the slower queries.","B":"Use Cloud Monitoring to view BigQuery metrics and set up alerts that let you know when a certain percentage of slots were used."},"url":"https://www.examtopics.com/discussions/google/view/130176-exam-professional-data-engineer-topic-1-question-233/","answer_description":"","question_images":[],"answer_images":[]}],"exam":{"id":11,"name":"Professional Data Engineer","isMCOnly":true,"provider":"Google","numberOfQuestions":319,"isImplemented":true,"lastUpdated":"11 Apr 2025","isBeta":false},"currentPage":30},"__N_SSP":true}