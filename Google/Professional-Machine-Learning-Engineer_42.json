{"pageProps":{"questions":[{"id":"au2HCz22URNfIiPxI9qm","question_id":206,"question_images":[],"choices":{"C":"Configure a TensorFlow Extended (TFX) ML Metadata database, and use the ML Metadata API.","A":"Use the Vertex AI Metadata API inside the custom job to create context, execution, and artifacts for each model, and use events to link them together.","B":"Create a Vertex AI experiment, and enable autologging inside the custom job.","D":"Register each model in Vertex AI Model Registry, and use model labels to store the related dataset and model information."},"question_text":"You have a custom job that runs on Vertex AI on a weekly basis. The job is implemented using a proprietary ML workflow that produces the datasets, models, and custom artifacts, and sends them to a Cloud Storage bucket. Many different versions of the datasets and models were created. Due to compliance requirements, your company needs to track which model was used for making a particular prediction, and needs access to the artifacts for each model. How should you configure your workflows to meet these requirements?","url":"https://www.examtopics.com/discussions/google/view/134193-exam-professional-machine-learning-engineer-topic-1-question/","exam_id":13,"unix_timestamp":1708353660,"answer":"A","timestamp":"2024-02-19 15:41:00","answer_ET":"A","answers_community":["A (100%)"],"answer_images":[],"answer_description":"","discussion":[{"comment_id":"1194773","upvote_count":"4","poster":"omermahgoub","timestamp":"1728806220.0","content":"Selected Answer: A\nTrack Lineage with Vertex AI Metadata API"},{"upvote_count":"2","poster":"emsherff","content":"Selected Answer: A\nA - Vertex AI Metadata API provides low-level primitives for creating custom metadata entities and relationships (contexts, executions, artifacts, and events).\n\nB - Autologging might not capture all the custom artifacts your job produces.","timestamp":"1728535620.0","comment_id":"1192703"},{"comment_id":"1154020","content":"Selected Answer: A\nMy Answer: A\n\nFocus on “Due to compliance requirements, your company needs to track which model was used for making a particular prediction” and “workflow that produces the datasets, models, and custom artifacts, and sends them to a Cloud Storage bucket”, use Vertex AI Metadata API is the best approach.","upvote_count":"4","comments":[{"upvote_count":"1","poster":"pinimichele01","timestamp":"1729228560.0","comment_id":"1197707","content":"where you find the question? do you pass the exam?"}],"timestamp":"1724071260.0","poster":"guilhermebutzke"}],"topic":"1","isMC":true},{"id":"CrRprdvn0AiwE3s5BS2s","answer_images":[],"choices":{"A":"Train an AutoML image classification model.","C":"Create a Vertex AI hyperparameter tuning job.","D":"Create a Vertex AI pipeline that runs different model training jobs in parallel.","B":"Create a custom training job that uses the Vertex AI Vizier SDK for parameter optimization."},"unix_timestamp":1708353780,"answer_description":"","topic":"1","timestamp":"2024-02-19 15:43:00","answer_ET":"C","answers_community":["C (100%)"],"exam_id":13,"question_id":207,"answer":"C","discussion":[{"comment_id":"1154023","content":"Selected Answer: C\nMy Answer: C\n\nVertex AI provides a service for hyperparameter tuning which allows you to specify the hyperparameters you want to optimize, such as learning rate, number of layers, and kernel size, and then it automatically runs multiple training jobs with different combinations of these hyperparameters to find the configuration that maximizes performance.","upvote_count":"9","poster":"guilhermebutzke","timestamp":"1708353780.0"},{"comment_id":"1239318","timestamp":"1719670260.0","content":"Selected Answer: C\nhttps://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning","upvote_count":"3","poster":"VinaoSilva"},{"content":"Selected Answer: C\nLeverage Vertex AI Hyperparameter Tuning","comments":[{"upvote_count":"1","content":"why not b?","timestamp":"1714314960.0","comment_id":"1203613","poster":"pinimichele01","comments":[{"upvote_count":"1","timestamp":"1723448760.0","poster":"YushiSato","content":"Additional code is required to use the Vertex AI Vizier SDK.","comment_id":"1264531"}]}],"upvote_count":"2","poster":"omermahgoub","comment_id":"1194775","timestamp":"1712995080.0"},{"content":"Selected Answer: C\nTrue that","upvote_count":"1","timestamp":"1711120680.0","comment_id":"1180130","poster":"alfieroy16"}],"isMC":true,"url":"https://www.examtopics.com/discussions/google/view/134194-exam-professional-machine-learning-engineer-topic-1-question/","question_text":"You have recently developed a custom model for image classification by using a neural network. You need to automatically identify the values for learning rate, number of layers, and kernel size. To do this, you plan to run multiple jobs in parallel to identify the parameters that optimize performance. You want to minimize custom code development and infrastructure management. What should you do?","question_images":[]},{"id":"RBJkzT4zo4dy71oWIkwO","topic":"1","answer_images":[],"exam_id":13,"unix_timestamp":1732720080,"choices":{"C":"Create a BigQuery ML classification model to classify important images. Use the model to predict which new images are important to help specialists with the filtering task.","B":"Use the Cloud Vision API to automatically annotate objects in the images to help specialists with the annotation task.","D":"Use Vertex AI to train an open source object detection to annotate the objects in the images to help specialists with the annotation task.","A":"Train an AutoML object detection model to annotate the objects in the images to help specialists with the annotation task."},"isMC":true,"answer":"A","timestamp":"2024-11-27 16:08:00","answer_description":"","url":"https://www.examtopics.com/discussions/google/view/152151-exam-professional-machine-learning-engineer-topic-1-question/","answers_community":["A (63%)","B (38%)"],"question_images":[],"answer_ET":"A","question_id":208,"discussion":[{"poster":"strafer","comment_id":"1348623","upvote_count":"1","timestamp":"1738163640.0","content":"Selected Answer: A\nCloud Vision API: Pay-as-you-go and Ready-to-use: The Cloud Vision API offers pre-trained models for object detection (and many other image analysis tasks). It's a pay-as-you-go service, meaning you only pay for the API calls you make. This translates to minimal up-front cost since there's no model training or infrastructure setup required on your end. You can immediately start using the API with your existing image data."},{"upvote_count":"2","poster":"thescientist","content":"Selected Answer: B\nAutoML requires training data and incurs training costs - for no upfront cost: B","comment_id":"1334832","timestamp":"1735653360.0"},{"content":"Selected Answer: B\nCloud Vision API - Pay-per-use based on the number of images processed.\nNo training required – it's a pre-trained API.","poster":"vladik820","comment_id":"1327964","upvote_count":"1","timestamp":"1734446640.0"},{"timestamp":"1733816100.0","content":"Selected Answer: A\nSince we have corpus of images and custom lables, 'Cloud Vision API' wont help, also its not advisable to use BigQuery ML classification for Image data\nHence ans is A","comment_id":"1324423","upvote_count":"2","poster":"Omi_04040"},{"timestamp":"1732720080.0","poster":"AB_C","comment_id":"1318735","content":"Selected Answer: A\nWhile the Vision API can detect objects, it might not be as accurate or specific as a custom-trained model for this particular use case (bridge construction).","upvote_count":"2"}],"question_text":"You work for a company that builds bridges for cities around the world. To track the progress of projects at the construction sites, your company has set up cameras at each location. Each hour, the cameras take a picture that is sent to a Cloud Storage bucket. A team of specialists reviews the images, filters important ones, and then annotates specific objects in them. You want to propose using an ML solution that will help the company scale and reduce costs. You need the solution to have minimal up-front cost. What method should you propose?"},{"id":"bCuVIkRsP2Lwlsn3RYjs","question_images":[],"question_text":"You are tasked with building an MLOps pipeline to retrain tree-based models in production. The pipeline will include components related to data ingestion, data processing, model training, model evaluation, and model deployment. Your organization primarily uses PySpark-based workloads for data preprocessing. You want to minimize infrastructure management effort. How should you set up the pipeline?","answers_community":["B (100%)"],"url":"https://www.examtopics.com/discussions/google/view/150324-exam-professional-machine-learning-engineer-topic-1-question/","answer_ET":"B","isMC":true,"unix_timestamp":1729961220,"question_id":209,"answer":"B","topic":"1","answer_description":"","choices":{"B":"Set up a Vertex AI Pipelines to orchestrate the MLOps pipeline. Use the predefined Dataproc component for the PySpark-based workloads.","D":"Set up Cloud Composer to orchestrate the MLOps pipeline. Use Dataproc workflow templates for the PySpark-based workloads in Cloud Composer.","C":"Set up Kubeflow Pipelines on Google Kubernetes Engine to orchestrate the MLOps pipeline. Write a custom component for the PySparkbased workloads on Dataproc.","A":"Set up a TensorFlow Extended (TFX) pipeline on Vertex AI Pipelines to orchestrate the MLOps pipeline. Write a custom component for the PySpark-based workloads on Dataproc."},"discussion":[{"upvote_count":"1","poster":"Pau1234","timestamp":"1734218400.0","comment_id":"1326633","content":"Selected Answer: B\nminimize infrastructure management effort -- hence B"},{"timestamp":"1733816400.0","content":"Selected Answer: B\nA- Rejected due to component for the PySpark-based\nC- Kubeflow Pipelines not a managed service and the question mentions 'minimize infrastructure management effort'\nD-","upvote_count":"1","comment_id":"1324424","poster":"Omi_04040","comments":[{"content":"D- Cloud Composer to orchestrate is an overhead\n\nhence B","poster":"Omi_04040","timestamp":"1733816400.0","upvote_count":"1","comment_id":"1324425"}]},{"upvote_count":"2","content":"Selected Answer: B\nThis is the most suitable approach","comment_id":"1318737","poster":"AB_C","timestamp":"1732720200.0"},{"upvote_count":"2","content":"Selected Answer: B\nB) Best option due to higher ease of use, integration with existing PySpark infrastructure (via Dataproc) and minimal infrastructure management overhead, because:\nVertex AI Pipelines is fully managed, minimizing infra management effort and natively integrated with Dataproc for PySpark (while Composer is not);\nDataproc’s predefined component for PySpark workload reduces effort and error probability;\nIt is suitable for tree-based models (other options are too, but with more effort)","comment_id":"1303343","poster":"carolctech","timestamp":"1729961220.0"}],"exam_id":13,"timestamp":"2024-10-26 18:47:00","answer_images":[]},{"id":"QtQWUShiZ5EMjUswb7oM","answer_description":"","exam_id":13,"question_text":"You have developed an AutoML tabular classification model that identifies high-value customers who interact with your organization's website. You plan to deploy the model to a new Vertex AI endpoint that will integrate with your website application. You expect higher traffic to the website during nights and weekends. You need to configure the model endpoint's deployment settings to minimize latency and cost. What should you do?","answer":"B","choices":{"A":"Configure the model deployment settings to use an n1-standard-32 machine type.","D":"Configure the model deployment settings to use an n1-standard-8 machine type and a GPU accelerator.","C":"Configure the model deployment settings to use an n1-standard-4 machine type and a GPU accelerator. Set the minReplicaCount value to 1 and the maxReplicaCount value to 4.","B":"Configure the model deployment settings to use an n1-standard-4 machine type. Set the minReplicaCount value to 1 and the maxReplicaCount value to 8."},"topic":"1","question_images":[],"timestamp":"2024-10-26 18:27:00","isMC":true,"question_id":210,"answer_ET":"B","answer_images":[],"answers_community":["B (100%)"],"discussion":[{"timestamp":"1732720380.0","upvote_count":"3","comment_id":"1318740","poster":"AB_C","content":"Selected Answer: B\nA (n1-standard-32): This is a much larger machine type and will likely be more expensive than necessary for your model. It could lead to unnecessary costs, especially during periods of low traffic.\nC and D (GPU Accelerators): While GPUs can be beneficial for some models, they are generally not required for tabular models. Adding a GPU would increase the cost without providing significant performance gains."},{"upvote_count":"2","poster":"carolctech","content":"Selected Answer: B\nB) This option provides the most cost-effective and efficient solution because:\n1) Uses a suitably powerful machine type (n1-standard-4 machine) \n2) Autoscales with minReplicaCount and maxReplicaCount to adapt to the fluctuating traffic\n3) A larger machine type or accelerator is unnecessary. GPU provide better performance for DL models with massive datasets and complex architectures, not for tabular classification models.","timestamp":"1729960020.0","comment_id":"1303332"}],"unix_timestamp":1729960020,"url":"https://www.examtopics.com/discussions/google/view/150321-exam-professional-machine-learning-engineer-topic-1-question/"}],"exam":{"id":13,"lastUpdated":"11 Apr 2025","isImplemented":true,"isMCOnly":true,"provider":"Google","numberOfQuestions":304,"name":"Professional Machine Learning Engineer","isBeta":false},"currentPage":42},"__N_SSP":true}