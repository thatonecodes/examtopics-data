{"pageProps":{"questions":[{"id":"uwXre0lhUWMb0rNbwMAh","question_id":111,"answer_ET":"A","question_images":[],"unix_timestamp":1670821680,"url":"https://www.examtopics.com/discussions/google/view/91110-exam-professional-cloud-developer-topic-1-question-199/","question_text":"You are using Cloud Run to host a web application. You need to securely obtain the application project ID and region where the application is running and display this information to users. You want to use the most performant approach. What should you do?","discussion":[{"content":"Selected Answer: A\nhttps://cloud.google.com/run/docs/container-contract#metadata-server:~:text=Project%20ID%20of%20the%20project%20the%20Cloud%20Run%20service%20or%20job%20belongs%20to","poster":"alpha_canary","comment_id":"1194737","timestamp":"1728804120.0","upvote_count":"1"},{"comment_id":"1194736","content":"https://cloud.google.com/run/docs/container-contract#metadata-server:~:text=Project%20ID%20of%20the%20project%20the%20Cloud%20Run%20service%20or%20job%20belongs%20to","poster":"alpha_canary","upvote_count":"1","timestamp":"1728804060.0"},{"poster":"__rajan__","timestamp":"1711204260.0","comment_id":"1014895","content":"Selected Answer: A\nA is correct.","upvote_count":"1"},{"poster":"telp","timestamp":"1689057660.0","upvote_count":"2","content":"Selected Answer: A\nAnswer A\nhttps://cloud.google.com/run/docs/container-contract#metadata-server","comment_id":"772246"},{"comment_id":"765468","content":"Selected Answer: A\nhttps://cloud.google.com/run/docs/container-contract#metadata-server\nAnswer A","timestamp":"1688458140.0","poster":"TNT87","upvote_count":"1"},{"comment_id":"749646","content":"Selected Answer: A\nDefinitely A, it's clear in the docs.\n\nhttps://cloud.google.com/run/docs/container-contract#metadata-server","poster":"micoams","timestamp":"1687157760.0","upvote_count":"2"},{"upvote_count":"1","comment_id":"744745","timestamp":"1686713040.0","poster":"sharath25","content":"Selected Answer: A\noption A"},{"timestamp":"1686650340.0","upvote_count":"1","poster":"zellck","comment_id":"743957","content":"Selected Answer: A\nA is the answer.\n\nhttps://cloud.google.com/run/docs/container-contract#metadata-server\nCloud Run container instances expose a metadata server that you can use to retrieve details about your container instance, such as the project ID, region, instance ID or service accounts.\n\nYou can access this data from the metadata server using simple HTTP requests to the http://metadata.google.internal/ endpoint with the Metadata-Flavor: Google header: no client libraries are required."},{"poster":"melisargh","content":"Selected Answer: B\nvoting B because \nis not A since thats compute metadata, if you can access you can query project id but not the region necessarily from cloud run but gce\nis not D since you cannot query project id \nC is too manual and static\nso by discard I guess is B","upvote_count":"1","comment_id":"742418","timestamp":"1686539280.0"}],"topic":"1","answer_description":"","choices":{"C":"In the Google Cloud console, navigate to the Project Dashboard and gather configuration details. Write the application configuration information to Cloud Run's in-memory container filesystem.","B":"In the Google Cloud console, navigate to the Project Dashboard and gather configuration details. Navigate to the Cloud Run “Variables & Secrets” tab, and add the desired environment variables in Key:Value format.","D":"Make an API call to the Cloud Asset Inventory API from the application and format the request to include instance metadata.","A":"Use HTTP requests to query the available metadata server at the http://metadata.google.internal/ endpoint with the Metadata-Flavor: Google header."},"exam_id":7,"answer":"A","answer_images":[],"timestamp":"2022-12-12 06:08:00","answers_community":["A (90%)","10%"],"isMC":true},{"id":"Mk4JcFjXbX1D41OoqxqA","timestamp":"2020-07-14 13:33:00","isMC":true,"choices":{"D":"Migrate some traffic back to your old platform and perform AB testing on the two platforms concurrently.","A":"Replace your entire monitoring platform with Stackdriver.","B":"Install the Stackdriver agents on your Compute Engine instances.","C":"Use Stackdriver to capture and alert on logs, then ship them to your existing platform."},"answer_ET":"C","answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/25708-exam-professional-cloud-developer-topic-1-question-2/","question_images":[],"answer":"C","exam_id":7,"topic":"1","question_id":112,"answer_description":"","answers_community":["C (67%)","A (22%)","11%"],"unix_timestamp":1594726380,"question_text":"You migrated your applications to Google Cloud Platform and kept your existing monitoring platform. You now find that your notification system is too slow for time critical problems.\nWhat should you do?","discussion":[{"timestamp":"1618591440.0","comment_id":"201147","poster":"gcper","upvote_count":"18","content":"C\n\nThe task does not indicate that we should get rid of the old software. The pain point is slowness for time critical problems only. Thus we would use Stackdriver for the time critical alerts and still utilize the old platform for further analysis/storing of logs or whatever its business case is."},{"timestamp":"1742788380.0","comment_id":"1409529","upvote_count":"1","content":"Selected Answer: C\nwe can directly omit B and D.\nB. Just installing Stackdriver wont do any good. we need notification system\nD. Testing is irrelevant here.\nbased on A&C, i would go with C","poster":"Jason_Cloud_at"},{"upvote_count":"1","poster":"Hila_Sh","content":"Selected Answer: C\nAddresses Immediate Problem:\n* Solves the critical notification speed issue\n* Maintains existing monitoring infrastructure\n* Provides a hybrid approach that leverages the best of both systems\nBenefits of this approach:\n* Uses Stackdriver's real-time alerting capabilities\n* Preserves investment in existing monitoring platform\n* Minimizes disruption to current operations\n* Allows for gradual transition if needed","comment_id":"1330904","timestamp":"1734978360.0"},{"poster":"santoshchauhan","comment_id":"1168022","timestamp":"1725708180.0","upvote_count":"1","content":"Selected Answer: C\nthe most balanced and least disruptive approach would likely be option C, \"Use Stackdriver to capture and alert on logs, then ship them to your existing platform\". This method allows for a seamless integration of Stackdriver's real-time alerting capabilities into your existing monitoring workflow without the need for a complete overhaul of your system."},{"upvote_count":"1","poster":"blackshuai","timestamp":"1715643660.0","content":"Selected Answer: B\nB is correct answer","comment_id":"1069897"},{"poster":"wanrltw","upvote_count":"3","timestamp":"1715104200.0","comment_id":"1065133","content":"Option C.\n\nNotifications from on-prem monitoring system are too slow & applications are in GCP now => Stackdriver alerts on logs.\n\nThere's no mentioning whether the apps have been migrated to GCE, GKE, App Engine or Cloud Run, so \"Compute Engine instances\" come from an assumption."},{"timestamp":"1710862560.0","upvote_count":"1","content":"Selected Answer: C\nI would go with C.","poster":"__rajan__","comment_id":"1011366"},{"comment_id":"986086","upvote_count":"1","poster":"SARAVANA25","content":"Option B , Install stack driver agents on the compute instances is the correct answer .\nusing stackdriver and shipping it to existing platform will have some delay","timestamp":"1708473060.0"},{"content":"Selected Answer: C\nC\nyou have problems with notifications.\nC option allows you to use stackdriver to send alerts immediately and straight away after sends all this data to your on-prem monitoring platform","timestamp":"1699022640.0","upvote_count":"3","poster":"closer89","comment_id":"888522"},{"comment_id":"795886","content":"Selected Answer: B\nB is the correct answer. \nhttps://cloud.google.com/monitoring/agent/monitoring/installation","upvote_count":"1","timestamp":"1690957860.0","poster":"Foxal"},{"upvote_count":"3","content":"Selected Answer: C\nThink twice. You have working an expensive monitoring system i.e Splunk and you have the problem with unacceptable delay time between incident and notification. You need to fix this problem, not doing a revolution (changing monitoring system). You can leverage GCP Monitoring with alerting system which is out-of-the-box with no huge effort, because if you want or not logs are in cloud logging. Simply implement alerts and push logs to Splunk. Simples.","comment_id":"763944","poster":"lxs","timestamp":"1688319420.0"},{"poster":"tomato123","content":"Selected Answer: A\nA is correct","timestamp":"1676810220.0","upvote_count":"2","comment_id":"648888"},{"content":"It's the rightest answer. C cannot work without Agent and there's not sense to send log to old monitoring","timestamp":"1669397760.0","poster":"ruben82","upvote_count":"1","comment_id":"607288"},{"comment_id":"607282","timestamp":"1669397520.0","upvote_count":"2","poster":"ruben82","content":"It's A 'cos you cannot use Stackdriver if you don't install Stackdriver agent on your compute engine.","comments":[{"poster":"mjdubal","content":"Hi\nDid you find any questions from here?","comment_id":"618204","timestamp":"1671367680.0","upvote_count":"1"}]},{"comment_id":"580596","upvote_count":"2","poster":"morenocasado","content":"Selected Answer: C\nCommunity choice is C","timestamp":"1664868840.0"},{"poster":"GCPCloudArchitectUser","timestamp":"1660786920.0","comment_id":"549964","upvote_count":"2","comments":[{"content":"Nvm I think it should be C","upvote_count":"1","comment_id":"549966","poster":"GCPCloudArchitectUser","timestamp":"1660787100.0"}],"content":"Selected Answer: A\nI think it should be A than C \nApps have been migrated and why would you invest on C to send data back to existing system instead fix old system using direct connect or something"},{"comment_id":"487233","upvote_count":"1","poster":"chelovalpo","comments":[],"content":"The answer is C. The point is notification problem with low performance, not monitoring.","timestamp":"1653551400.0"},{"upvote_count":"3","comment_id":"410582","timestamp":"1642735020.0","content":"Agree C","poster":"wilwong"},{"timestamp":"1640330520.0","poster":"yuchun","content":"This question is a little tricky.\nfirst, we can rule out B, D\nB is just the first step to collect metric, not involve how to handle metric in monitoring and alerting; D is about CI/CD.\nFor A and C, the question doesn't mention why the notification is slow, is it because of latency in collecting metric, or it's because transferring data from GCP to on-prem, for me, no matter what, I think only A can solve whole problem.","comment_id":"389235","upvote_count":"3"},{"timestamp":"1638252000.0","comment_id":"369845","content":"Since it is already on GCP, just make use of GCP services.\n\nAnswer is A","poster":"syu31svc","upvote_count":"1","comments":[{"content":"Which is C not A after reading the part on \"migrated your applications to Google Cloud Platform\"","timestamp":"1643598540.0","upvote_count":"2","poster":"syu31svc","comment_id":"417659"}]},{"upvote_count":"2","timestamp":"1611383580.0","comments":[{"timestamp":"1662065040.0","comment_id":"559056","poster":"amanshin","content":"Stack driver agents are not installed on compute engine instances by default, so adding agents will just resolve the point of getting logs from VMs. This will not change the fact that notifications are slow. Plus, stack driver is cloud native. Who knows what system is in use and is slow. I go with A.","upvote_count":"1"}],"poster":"serg3d","comment_id":"141652","content":"How Stackdriver agents can help with speed of delivering notifications from other monitoring system? Answer A"},{"upvote_count":"3","timestamp":"1610631180.0","comments":[{"upvote_count":"2","timestamp":"1624531020.0","content":"B is mandatory, but not sufficient. Infact we need to allerting system too. \nI thinks that the answer is C, becouse in GCP project there aren't latency problem: https://cloud.google.com/monitoring/alerts/concepts-indepth#notification-latency","comments":[{"poster":"ruben82","content":"Right, but C cannot work without B. Compute Engine doesn't have agent installed, so How can It work properly?","timestamp":"1669397640.0","upvote_count":"1","comment_id":"607284"}],"poster":"fraloca","comment_id":"251574"}],"comment_id":"134801","poster":"mlyu","content":"should be B.\nA whole migration is too complicated and overkill for only one system\nC. Ship back to exisiting platform impose latency\nD. Question is nothing about AB testing"}]},{"id":"lDaxi9zrVDgEGbWiJgIW","discussion":[{"poster":"saurabh1805","comment_id":"215416","upvote_count":"18","timestamp":"1620489960.0","content":"I will go with Option B for this. \n\nRollingUpdate: New pods are added gradually, and old pods are terminated gradually\nRecreate: All old pods are terminated before any new pods are added\n\nQuestion ask us to retain current version hence rolling update is better option here."},{"poster":"syu31svc","content":"https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-upgrades:\n\"The simplest way to take advantage of surge upgrade is to configure maxSurge=1 maxUnavailable=0. This means that only 1 surge node can be added to the node pool during an upgrade so only 1 node will be upgraded at a time. This setting is superior to the existing upgrade configuration (maxSurge=0 maxUnavailable=1) because it speeds up Pod restarts during upgrades while progressing conservatively.\"\n\nAnswer is B","upvote_count":"7","comment_id":"386069","timestamp":"1639994940.0"},{"poster":"santoshchauhan","content":"Selected Answer: B\nB. Set the Deployment strategy to RollingUpdate with maxSurge set to 1, maxUnavailable set to 0:\n\nmaxSurge set to 1 allows the Deployment to exceed the desired number of Pods by one, permitting the creation of an additional new Pod before terminating the old ones, which aligns with the requirement.\nmaxUnavailable set to 0 ensures that all existing Pods must remain available during the update, which again meets the requirement.","upvote_count":"1","comment_id":"1168182","timestamp":"1725720060.0"},{"comment_id":"1148975","upvote_count":"1","poster":"theseawillclaim","timestamp":"1723528440.0","content":"Selected Answer: B\n\"maxSurge=1\" means that at least one VM has to stay up during the RU process."},{"comment_id":"1011003","content":"Selected Answer: B\nOption B is the correct one.","upvote_count":"1","poster":"__rajan__","timestamp":"1710824520.0"},{"comments":[{"timestamp":"1688213460.0","comment_id":"763215","content":"The Recreate Deployment strategy (C and D) would not be suitable for this use case, as it would involve replacing all of the replicas at once, rather than rolling out the update gradually.","poster":"omermahgoub","upvote_count":"1"}],"poster":"omermahgoub","upvote_count":"2","content":"To deploy at least 1 replica of the new version and maintain the previous replicas until the new replica is healthy, you should set the Deployment strategy to RollingUpdate with maxSurge set to 1 and maxUnavailable set to 0 (B).\n\nThe RollingUpdate Deployment strategy allows you to specify the number of replicas that can be created or removed at a time as part of the update process. The maxSurge parameter specifies the maximum number of replicas that can be created in excess of the desired number of replicas, and the maxUnavailable parameter specifies the maximum number of replicas that can be unavailable at any given time.\n\nBy setting maxSurge to 1 and maxUnavailable to 0, you are telling the Deployment to create at least 1 new replica of the new version and to maintain all of the previous replicas until the new replica is healthy. This will ensure that at least 1 replica of the new version is always available, while allowing the Deployment to gradually roll out the update to the rest of the replicas.","timestamp":"1688213460.0","comment_id":"763214"},{"content":"Selected Answer: B\nB is obvious","poster":"tuanbo91","comment_id":"750552","timestamp":"1687233900.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1685460300.0","poster":"jcataluna","content":"Selected Answer: B\nB is correct","comment_id":"731795"},{"upvote_count":"2","timestamp":"1676878500.0","comment_id":"649181","content":"Selected Answer: B\nB is correct","poster":"tomato123"},{"upvote_count":"1","timestamp":"1658188380.0","content":"Obviously it's B","comment_id":"527146","poster":"herocc"},{"comment_id":"519405","upvote_count":"1","timestamp":"1657265160.0","poster":"ParagSanyashiv","content":"Selected Answer: B\nAnswer is B"},{"poster":"ParagSanyashiv","upvote_count":"1","content":"Selected Answer: B\nAnswer is B","timestamp":"1657263360.0","comment_id":"519390"},{"comment_id":"515080","content":"the answer is B.","poster":"Flavio80","timestamp":"1656773040.0","upvote_count":"1"},{"poster":"mishsanjay","content":"Recreate can't be maintain previous replica. Answer must be B.","comment_id":"285160","timestamp":"1628287920.0","upvote_count":"3"},{"content":"B is correct answer","comment_id":"255402","upvote_count":"2","poster":"donchick","timestamp":"1625026860.0"},{"content":"You are not maintaining anything with Recreate strategy","timestamp":"1624132860.0","upvote_count":"1","poster":"beranm","comment_id":"248213"}],"question_images":["https://www.examtopics.com/assets/media/exam-media/04137/0001300001.jpg"],"isMC":true,"timestamp":"2020-11-08 19:06:00","choices":{"C":"Set the Deployment strategy to Recreate with maxSurge set to 0, maxUnavailable set to 1.","D":"Set the Deployment strategy to Recreate with maxSurge set to 1, maxUnavailable set to 0.","B":"Set the Deployment strategy to RollingUpdate with maxSurge set to 1, maxUnavailable set to 0.","A":"Set the Deployment strategy to RollingUpdate with maxSurge set to 0, maxUnavailable set to 1."},"exam_id":7,"topic":"1","answer_description":"","question_text":"Your application is deployed in a Google Kubernetes Engine (GKE) cluster. When a new version of your application is released, your CI/CD tool updates the spec.template.spec.containers[0].image value to reference the Docker image of your new application version. When the Deployment object applies the change, you want to deploy at least 1 replica of the new version and maintain the previous replicas until the new replica is healthy.\nWhich change should you make to the GKE Deployment object shown below?\n//IMG//","answer_ET":"B","question_id":113,"unix_timestamp":1604858760,"url":"https://www.examtopics.com/discussions/google/view/36478-exam-professional-cloud-developer-topic-1-question-20/","answer_images":[],"answers_community":["B (100%)"],"answer":"B"},{"id":"4jaEIBr2vvujXKvXLRGd","question_images":[],"question_id":114,"exam_id":7,"answer_description":"","timestamp":"2022-12-12 14:45:00","isMC":true,"topic":"1","unix_timestamp":1670852700,"discussion":[{"comment_id":"753665","content":"Selected Answer: B\nA&D assume that you download and store SA keys, which violates best practices, since you potentially loose control over what happens to those credentials and makes it impossible to track who actually uses the SA. D makes it even worse since it requires you to maintain you own secret management to minimize the risk.\n\nC does nothing that would give you the SA permissions you need.\n\nB follows best practices, since impersonation permissions can be managed transparently via IAM and via logs you can also see who impersonated/used the SA.","poster":"Underverse","upvote_count":"5","timestamp":"1671745620.0"},{"upvote_count":"1","poster":"mrgarfield","timestamp":"1725611520.0","content":"Selected Answer: C\nApplication Default Credentials (ADC): gcloud auth application-default login sets up the Application Default Credentials (ADC) which are a secure way to authenticate applications running on Google Cloud.\nMinimal Permissions: Your Cloud Identity already has the necessary permissions (roles/iam.serviceAccountTokenCreator) to create service account tokens and the required permissions to deploy resources using Terraform.\nSecurity Best Practices: Using ADC avoids storing the service account key file locally on your laptop, which minimizes the risk of exposure and adheres to security best practices.","comment_id":"1279490"},{"poster":"alpha_canary","upvote_count":"1","timestamp":"1712993160.0","content":"Selected Answer: B\nhttps://cloud.google.com/docs/authentication/use-service-account-impersonation\nhttps://medium.com/bluetuple-ai/terraform-remote-state-on-gcp-d50e2f69b967","comment_id":"1194740"},{"timestamp":"1710753780.0","poster":"namanj71","upvote_count":"1","content":"B is the correct Answer","comment_id":"1176363"},{"timestamp":"1695472500.0","poster":"__rajan__","content":"Selected Answer: B\nB is correct.","comment_id":"1014898","upvote_count":"1"},{"timestamp":"1691413800.0","poster":"purushi","upvote_count":"1","comment_id":"974723","content":"Selected Answer: B\nB is the best option here.\nD is more complicated.\nA & C do not follow google best practices."},{"comment_id":"877580","upvote_count":"1","content":"Selected Answer: B\nB\n1. impersonation\n2. securely set up env variable that will be used by terraform to deploy","poster":"closer89","timestamp":"1682191320.0"},{"timestamp":"1680454080.0","poster":"saketmurari","comment_id":"859100","content":"I think its A","upvote_count":"1"},{"poster":"telp","content":"Selected Answer: B\nAnswer is B \nhttps://cloud.google.com/sdk/gcloud/reference/config/set#impersonate_service_account","timestamp":"1673426820.0","comment_id":"772251","upvote_count":"1"},{"content":"Selected Answer: B\nhttps://cloud.google.com/blog/topics/developers-practitioners/using-google-cloud-service-account-impersonation-your-terraform-code\nAnswer B not D","comment_id":"765462","timestamp":"1672826460.0","upvote_count":"4","poster":"TNT87"},{"comment_id":"750666","upvote_count":"1","content":"https://cloud.google.com/docs/terraform/best-practices-for-terraform#default-credhttps://cloud.google.com/docs/terraform/best-practices-for-terraform#storing-secrets\nAnswer D.","comments":[{"timestamp":"1672826580.0","comment_id":"765464","content":"Answer B not D","upvote_count":"1","poster":"TNT87"}],"poster":"TNT87","timestamp":"1671525120.0"},{"comment_id":"749650","timestamp":"1671440520.0","upvote_count":"2","poster":"micoams","content":"Selected Answer: B\nI think it's option B.\n\nThe question already says that you have the role for impersonating the service account.\n\nThis means that option B is a viable, as you can impersonate that service account, and get a token that has the required level of access to create resources."},{"comment_id":"742858","poster":"zellck","comments":[{"timestamp":"1673006940.0","upvote_count":"1","content":"Answer B not D","poster":"TNT87","comment_id":"767607"}],"upvote_count":"1","content":"Selected Answer: D\nD is the answer.\n\nhttps://cloud.google.com/iam/docs/best-practices-for-managing-service-account-keys#file-system\nWhenever possible, avoid storing service account keys on a file system. If you can't avoid storing keys on disk, make sure to restrict access to the key file, configure file access auditing, and encrypt the underlying disk.\n\nhttps://cloud.google.com/iam/docs/best-practices-for-managing-service-account-keys#software-keystore\nIn situations where using a hardware-based key store isn't viable, use a software-based key store to manage service account keys. Similar to hardware-based options, a software-based key store lets users or applications use service account keys without revealing the private key. Software-based key store solutions can help you control key access in a fine-grained manner and can also ensure that each key access is logged.","timestamp":"1670852700.0"}],"answer_images":[],"answers_community":["B (89%)","6%"],"url":"https://www.examtopics.com/discussions/google/view/91207-exam-professional-cloud-developer-topic-1-question-200/","question_text":"You need to deploy resources from your laptop to Google Cloud using Terraform. Resources in your Google Cloud environment must be created using a service account. Your Cloud Identity has the roles/iam.serviceAccountTokenCreator Identity and Access Management (IAM) role and the necessary permissions to deploy the resources using Terraform. You want to set up your development environment to deploy the desired resources following Google-recommended best practices. What should you do?","choices":{"C":"1. Run the following command from a command line: gcloud auth application-default login.\n2. In the browser window that opens, authenticate using your personal credentials.","D":"1. Store the service account's key file in JSON format in Hashicorp Vault.\n2. Integrate Terraform with Vault to retrieve the key file dynamically, and authenticate to Vault using a short-lived access token.","B":"1. Run the following command from a command line: gcloud config set auth/impersonate_service_account service-account-name@project.iam.gserviceacccount.com.\n2. Set the GOOGLE_OAUTH_ACCESS_TOKEN environment variable to the value that is returned by the gcloud auth print-access-token command.","A":"1. Download the service account’s key file in JSON format, and store it locally on your laptop.\n2. Set the GOOGLE_APPLICATION_CREDENTIALS environment variable to the path of your downloaded key file."},"answer":"B","answer_ET":"B"},{"id":"5lmSP0IuhXmRPYVqO6mO","question_images":[],"question_id":115,"choices":{"D":"Create a Cloud Function to read Cloud Logging log entries and send them to the third-party application.","A":"Create a Cloud Logging log export to Pub/Sub.","B":"Create a Cloud Logging log export to BigQuery.","C":"Create a Cloud Logging log export to Cloud Storage."},"topic":"1","url":"https://www.examtopics.com/discussions/google/view/89886-exam-professional-cloud-developer-topic-1-question-201/","isMC":true,"answers_community":["A (71%)","B (24%)","6%"],"answer_ET":"A","answer_description":"","discussion":[{"poster":"alpha_canary","comments":[{"poster":"alpha_canary","content":"https://cloud.google.com/logging/docs/export/pubsub#integrate-thru-pubsub","comment_id":"1194746","upvote_count":"1","timestamp":"1728804600.0"}],"content":"Selected Answer: A\nA: Creating a Cloud Logging log export to Pub/Sub is the correct solution for this scenario. Pub/Sub is designed for real-time messaging and can push messages (in this case, log entries) to a third-party application for processing.\n\n\nB: While BigQuery is great for analyzing large volumes of data, it's not designed for real-time data pushing to third-party applications.\nD: Creating a Cloud Function to read log entries and send them to a third-party application could work, but it would add unnecessary complexity. Using Pub/Sub is a simpler and more efficient solution.","timestamp":"1728804540.0","upvote_count":"1","comment_id":"1194745"},{"content":"Selected Answer: A\nA is correct.","timestamp":"1711204620.0","comment_id":"1014900","poster":"__rajan__","upvote_count":"1"},{"upvote_count":"2","comment_id":"974730","poster":"purushi","timestamp":"1707319020.0","content":"Selected Answer: A\nMy answer is A.\nThird party service is the one responsible for analytics.\nFrom Google cloud we just need to push the log messages to a third party application for analytics that is the part of analytics architecture.\nReal time push means, I go with Pub-sub."},{"upvote_count":"1","comment_id":"824550","poster":"Pime13","timestamp":"1693203840.0","content":"Selected Answer: A\nhttps://cloud.google.com/logging/docs/export/configure_export_v2#overview\nhttps://cloud.google.com/logging/docs/export/pubsub: \nThis document explains how you can find log entries that you routed from Cloud Logging to Pub/Sub topics, which occurs in near real-time. We recommend using Pub/Sub for integrating Cloud Logging logs with third-party software.\n\nWhen you route logs to a Pub/Sub topic, Logging publishes each log entry as a Pub/Sub message as soon as Logging receives that log entry. Routed logs are generally available within seconds of their arrival to Logging, with 99% of logs available in less than 60 seconds."},{"poster":"telp","comment_id":"780194","upvote_count":"1","timestamp":"1689689640.0","content":"Selected Answer: A\nThe processing will be done in a third-party application so we need a solution to pass logs from gcp to thirs party in real time and no need for analytics. So the solution is pub/sub.\nExample on a case corresponding to the question by google:\nhttps://cloud.google.com/architecture/exporting-stackdriver-logging-for-splunk","comments":[{"comments":[{"poster":"mrvergara","upvote_count":"1","comment_id":"802173","timestamp":"1691500800.0","content":"While BigQuery can be used for log analysis, it is not well suited for real-time log processing. BigQuery is designed for batch processing of large amounts of data and may not be able to provide the low latency and real-time processing capabilities required for real-time log analysis. Additionally, BigQuery may be more expensive than other options for real-time log analysis, as it charges for both storage and processing.\n\nTherefore, for real-time log analysis, it is more appropriate to use a solution like Cloud Pub/Sub, which is specifically designed for real-time streaming of data.\n\nMy understanding is that third-party application may not be a GCP solution.\n\nI would go for A"}],"poster":"TNT87","comment_id":"785244","upvote_count":"1","content":"no need for analytics??? the question says \"You need to build a real-time log analysis architecture that pushes logs to a third-party application for processing \" its bigquery, it can connect to others cloud providers..https://cloud.google.com/bigquery/docs/introduction#bigquery-analytics\nhttps://cloud.google.com/blog/products/data-analytics/bigquery-performance-powers-real-time-analytics","timestamp":"1690102320.0"}]},{"poster":"TNT87","upvote_count":"1","timestamp":"1688453580.0","comment_id":"765404","content":"Selected Answer: B\nAnswer B\nThird party transfers for BigQuery Data Transfer Service allow you to automatically schedule and manage recurring load jobs for external data sources such as Salesforce CRM, Adobe Analytics, and Facebook Ads.\n\nhttps://cloud.google.com/bigquery/docs/introduction#bigquery-analytics\nhttps://cloud.google.com/blog/products/data-analytics/bigquery-performance-powers-real-time-analytics\n\nPub/sub does real time streaming not analytics. analytics its biquery and dataflow those can do realtime analytics."},{"poster":"x_cath","timestamp":"1686832080.0","upvote_count":"2","comments":[{"upvote_count":"1","poster":"TNT87","comment_id":"765399","timestamp":"1688453280.0","content":"Can pub/sub analyse data?? Kindly revisit the documentation, the question says You need to build a real-time log analysis architecture, not real time streaming, pub/sub does realtime streaming not analysis, so its bigquery , i dnt know if you practically worked on gcp then you will know and understand these solutions"},{"comment_id":"765400","timestamp":"1688453340.0","content":"Third party transfers for BigQuery Data Transfer Service allow you to automatically schedule and manage recurring load jobs for external data sources such as Salesforce CRM, Adobe Analytics, and Facebook Ads.\n\nto do thrid party transfers bigquery has this above mentioned capability","upvote_count":"1","poster":"TNT87"}],"comment_id":"746181","content":"Selected Answer: A\nA is the only option that meets all of these requirements:\n- Handles large volumes of log data\n- Sends messages (logs) to 3rd party applications in real time"},{"comment_id":"744746","poster":"sharath25","timestamp":"1686713340.0","upvote_count":"1","content":"Selected Answer: B\noption B"},{"upvote_count":"4","timestamp":"1686700140.0","poster":"jcataluna","comments":[{"comment_id":"746178","poster":"x_cath","content":"Agree with this explanation.","upvote_count":"1","timestamp":"1686831900.0"},{"content":"Third party transfers for BigQuery Data Transfer Service allow you to automatically schedule and manage recurring load jobs for external data sources such as Salesforce CRM, Adobe Analytics, and Facebook Ads.\n\nyou cant analyse data on pub/sub but you stream, so understand the difference, answer is Bigquery","upvote_count":"1","comment_id":"765402","poster":"TNT87","timestamp":"1688453400.0"}],"content":"Selected Answer: A\nA -> real-time to a third party app . pubsub..\n!C-> GCS not realtime\n!B -> No third party","comment_id":"744619"},{"poster":"zellck","comment_id":"742792","upvote_count":"1","content":"Selected Answer: B\nB is the answer.","timestamp":"1686566700.0"},{"upvote_count":"1","poster":"test010101","timestamp":"1686481860.0","comment_id":"741721","content":"Selected Answer: B\nvote B"},{"content":"Answer B \nBigquery performs real time analysis","poster":"TNT87","timestamp":"1686297180.0","upvote_count":"2","comment_id":"739970"},{"comment_id":"734548","poster":"melisargh","comments":[{"upvote_count":"2","content":"B is the answer. \" real time log analysis csan be done by Bigquery, C isnt correct.","poster":"TNT87","timestamp":"1686297240.0","comment_id":"739971"}],"content":"Selected Answer: C\nkey is “large volume”\nhttps://cloud.google.com/architecture/exporting-stackdriver-logging-for-compliance-requirements","upvote_count":"1","timestamp":"1685800440.0"}],"unix_timestamp":1670082840,"question_text":"Your company uses Cloud Logging to manage large volumes of log data. You need to build a real-time log analysis architecture that pushes logs to a third-party application for processing. What should you do?","answer_images":[],"answer":"A","exam_id":7,"timestamp":"2022-12-03 16:54:00"}],"exam":{"isImplemented":true,"isBeta":false,"name":"Professional Cloud Developer","provider":"Google","lastUpdated":"11 Apr 2025","isMCOnly":false,"numberOfQuestions":338,"id":7},"currentPage":23},"__N_SSP":true}