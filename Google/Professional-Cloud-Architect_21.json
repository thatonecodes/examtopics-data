{"pageProps":{"questions":[{"id":"CsNhnItyx1viqfpeaW8P","question_id":101,"isMC":true,"unix_timestamp":1571950020,"topic":"1","answer_ET":"C","choices":{"A":"Increase the virtual machine's memory to 64 GB","D":"Migrate their performance metrics warehouse to BigQuery","E":"Modify all of their batch jobs to use bulk inserts into the database","B":"Create a new virtual machine running PostgreSQL","C":"Dynamically resize the SSD persistent disk to 500 GB"},"answer_images":[],"answers_community":["C (72%)","12%","Other"],"discussion":[{"upvote_count":"71","content":"Answer is C because persistent disk performance is based on the total persistent disk capacity attached to an instance and the number of vCPUs that the instance has. Incrementing the persistent disk capacity will increment its throughput and IOPS, which in turn improve the performance of MySQL.","timestamp":"1574791260.0","poster":"shandy","comment_id":"24665"},{"timestamp":"1571950020.0","comment_id":"17263","content":"Assuming that the database is approaching its hardware limits... both options A and C would improve performance, A would increase number of CPUs and memory, but C would increase memory by more. If it a software problem, it is likly it is a hashing problem (the search and sort algorithms are not specific enough to search within the database). This problem would not be fixed just by migrating to PostgreSQL or BigQuery but modifying the inserts would help the situation because it would entail specifications of data lookups. However, it wouldn't help with search performance just inserts and it doesn't help in normalization. So B, D, and E are eliminated. Since statistics is based on sets, the larger the number of sets the better the predictions. This means that the largest amount of memory would not only increase computer performance but also knowledge enhancements. So C beats A.","poster":"Eroc","comments":[{"comment_id":"303406","content":"C. universal truth - OLTP D/B performance is depended on IOPs. SSD is the best solution for higher IOPs. In GCP bigger the disk size higher the IOPs.","poster":"nitinz","upvote_count":"8","timestamp":"1614869580.0"},{"content":"Also, if you increased the memory size, it would not be a n1-standard-8 anymore. You should eventually change machine type, not simply increase memory.","upvote_count":"7","poster":"trainor","timestamp":"1607255220.0","comment_id":"236391"},{"timestamp":"1596686100.0","poster":"tartar","comment_id":"151650","upvote_count":"8","content":"C is ok."},{"upvote_count":"2","poster":"haroldbenites","comments":[{"poster":"Ric350","upvote_count":"1","comment_id":"1268796","content":"Repectfully, this isn't accurate. On Google Compute Engine, you can often increase the memory of a running virtual machine without needing to shut it down. This is known as live migration or memory hot-add.","timestamp":"1724080500.0"},{"comment_id":"1230304","content":"Since its using SQL, so there will be a Maintenance Window, so this change can be implemented during the downtime( also there is no mention that the system should be always avaliable)","upvote_count":"1","poster":"Mission94","timestamp":"1718352060.0"},{"content":"there isn't \"without downtime\"","poster":"Dclaiborne41","upvote_count":"2","timestamp":"1652801160.0","comment_id":"602979"},{"content":"I wanted to put letter C.","poster":"haroldbenites","upvote_count":"2","timestamp":"1638551220.0","comment_id":"493267"}],"timestamp":"1638551160.0","content":"When you increase the memory yo need to shutdown the machine, but when you increase the disk, it is not necessary. Answer is B.","comment_id":"493265"},{"upvote_count":"12","timestamp":"1595178060.0","comment_id":"138808","content":"The IOPS on SSD on GCP increase with the size.","poster":"Matro71"},{"upvote_count":"5","content":"Nice explanation, I will go with C","comment_id":"696519","poster":"AzureDP900","timestamp":"1665951360.0"}],"upvote_count":"36"},{"upvote_count":"1","comment_id":"1333718","poster":"JonathanSJ","content":"Selected Answer: C\nI will go for C","timestamp":"1735504920.0"},{"comment_id":"1309634","poster":"Ekramy_Elnaggar","upvote_count":"3","content":"Selected Answer: C\nOLTP D/B performance is depended on IOPs. SSD is the best solution for higher IOPs. In GCP bigger the disk size higher the IOPs.","timestamp":"1731272220.0"},{"timestamp":"1723097460.0","comment_id":"1262338","poster":"Hungdv","upvote_count":"1","content":"Choose C"},{"upvote_count":"2","comment_id":"1244884","content":"Selected Answer: A\nincrease size will not increase performance, it either increase RAM or serverless. A or D. if no cost concern will pick D","poster":"ukivanlamlpi","timestamp":"1720523580.0"},{"poster":"ashishdwi007","timestamp":"1705843560.0","upvote_count":"1","content":"Selected Answer: C\nI was looking for CloudSQL in options, since it is not there, C is best","comment_id":"1127830"},{"timestamp":"1705157580.0","content":"Selected Answer: E\nThe fact that the database is used for importing and normalizing performance statistics suggests frequent data insertions. Optimizing this process through bulk inserts directly addresses a likely performance bottleneck.","upvote_count":"1","comment_id":"1121764","poster":"hzaoui"},{"timestamp":"1704821400.0","comments":[{"comment_id":"1227460","timestamp":"1717956120.0","poster":"ccpmad","upvote_count":"1","content":"Yes, it is, and says it is C."}],"upvote_count":"1","comment_id":"1117717","poster":"JohnDohertyDoe","content":"Selected Answer: A\nThe answer according to Google is A. This question is part of the Google's sample questions for the certification."},{"content":"I see most of the people here replying C, but I do not think that the size of the disk we bring much gains in performance. D, yes, seems to me that will bring much improvements in performance, management, operations and cost. So B, Migrate their performance metrics warehouse to BigQuery","timestamp":"1696759380.0","upvote_count":"4","comment_id":"1027868","poster":"JPA210"},{"upvote_count":"1","content":"I would go with option E because Bulk Insert improves performance drastically unless it is been implemented already.","comment_id":"984336","poster":"Palan","timestamp":"1692351300.0"},{"upvote_count":"4","timestamp":"1690370400.0","content":"Selected Answer: C\nIncreasing disk size will also increase its performance.\n\nhttps://cloud.google.com/compute/docs/disks/performance#optimize_disk_performance","comment_id":"963646","poster":"eka_nostra"},{"timestamp":"1681803840.0","poster":"JC0926","content":"Selected Answer: C\nC. Dynamically resize the SSD persistent disk to 500 GB\n\nBy increasing the size of the SSD persistent disk, the database server can achieve better performance. A larger SSD persistent disk provides higher IOPS (input/output operations per second) and throughput, allowing for faster read and write operations. This can help improve the performance of the MySQL database server running on the Google Compute Engine instance.","upvote_count":"3","comment_id":"873388"},{"comment_id":"849809","timestamp":"1679722440.0","poster":"mifrah","content":"On another website I found the question with the hint \"you are not allowed to reboot the VM before next maintenance window\". That makes it more clear --> C.","upvote_count":"1"},{"comment_id":"837777","timestamp":"1678697220.0","poster":"JC0926","upvote_count":"4","content":"Selected Answer: E\nE. Modify all of their batch jobs to use bulk inserts into the database: This can be a very effective solution for improving performance. Bulk inserts can greatly reduce the number of round-trips to the database, which can help to minimize latency and improve overall throughput.\n\nTherefore, option E is the best choice for improving performance in this scenario."},{"comment_id":"740839","upvote_count":"3","poster":"Jackalski","timestamp":"1670668080.0","content":"Selected Answer: D\nin option C - even increasing disc can gain performance - that will take few months to face new limits. mySQL is not desiged for OLAP/analytics - but OLTP.\nso I vote on D"},{"upvote_count":"2","poster":"AniketD","content":"Selected Answer: C\nCorrect answer is C. Increased disk capacity improved I/O and direct impacts the performance","timestamp":"1668850020.0","comment_id":"721893"},{"comment_id":"701641","upvote_count":"11","poster":"BobLoblawsLawBlog","timestamp":"1666455720.0","content":"Selected Answer: C\nC, because...\nN1 8cpu max IOPS = 15,000 https://cloud.google.com/compute/docs/disks/performance#n1_vms\n\nSSD persistent disks can reach up to 30 IOPS per GB of disk. https://cloud.google.com/compute/docs/disks/performance#example\n80 GB X 30 IOPS = 2,400 IOPS\n500 GB (answer C) X 30 IOPS = 15,000 IOPS = N1 8 cpu max IOPS"},{"upvote_count":"5","poster":"zr79","timestamp":"1665975480.0","content":"adding memory to VM will need to be shut down which means the business will be impacted, not good for any option \nremember this for your exam","comment_id":"696758"},{"content":"Selected Answer: C\nC. Dynamically resize the SSD persistent disk to 500 GB","timestamp":"1665645600.0","comment_id":"693689","poster":"minmin2020","upvote_count":"2"},{"comment_id":"665447","poster":"Amit_arch","upvote_count":"2","content":"Selected Answer: A\nThe ask is to improve the performance of this system. MySQL server or any relational DB server for this matter gains in performance from memory increase, as it provides to have larger number of queries be cached. As far system is not just writes, QC always results in better performance. In an ideal scenario if possible a memory equal to DB size, will give highest possible performance.","timestamp":"1662817500.0"},{"comment_id":"612222","upvote_count":"1","timestamp":"1654498080.0","content":"answer: C","poster":"szanio"},{"timestamp":"1653286800.0","content":"https://cloud.google.com/compute/docs/disks/performance\nthere is no relationship between a disk size and IOPS/Thruput , just the disk type and vcpu has any significance. The question gives no details which type of performance improvement they expect (batch loads or data 'normalization' or report generation) so you can only guess. I'd go with RAM but I have no idea what is the current RAM utilization, I'd go with separate disks if the batch loads are in scope, but 80GB for an SQL server sounds like a joke as well , since this is the cheapest \"let'st try and see\" method, Id go first with the disk. :)","comments":[{"timestamp":"1659428580.0","poster":"Chute5118","comment_id":"641165","upvote_count":"4","content":"There is a table that shows the \"maximum sustained IOPS\" and how it varies with \"Read IOPS per GB\" and \"Write IOPS per GB\".\n\nhttps://cloud.google.com/compute/docs/disks/performance#zonal-persistent-disks"},{"poster":"elaineshi","comment_id":"609014","timestamp":"1653884160.0","upvote_count":"3","content":"There is, in the page, it's said \"Persistent disk performance scales with the size of the disk and with the number of vCPUs on your VM instance.\""}],"poster":"ryzior","comment_id":"605856","upvote_count":"1"},{"timestamp":"1651937460.0","comment_id":"598181","content":"Selected Answer: C\nGoing with C","poster":"amxexam","upvote_count":"1"},{"content":"Selected Answer: C\nThe answer is C. \nIOPs and VCPU are linked closely together. \nIn our case we have a server with 8VCPs. \nGCP best practice is ~ 1 VCPU for 2000-2500 IOPS. \n500G PD SSD can produce 15,000 IOPS with 8 VCPs it will work - best practice mode. \nIf the server was < 8VCPs the answer (C) might not be correct. \n\nDocumentation to support: \nhttps://cloud.google.com/compute/docs/disks/performance#optimize_disk_performance","comment_id":"588492","timestamp":"1650440640.0","poster":"Nirca","upvote_count":"4"},{"timestamp":"1648650240.0","comment_id":"578360","content":"This question came up on the official google sample review question forum. The correct answer is C.","poster":"nm97","upvote_count":"2"},{"timestamp":"1647125160.0","content":"Answer is C\nA is not correct because increasing the memory size requires a VM restart.\n\nB is not correct because the DB administration team is requesting help with their MySQL instance. Migration to a different product should not be the solution when other optimization techniques can still be applied first.\n\nC is correct because persistent disk performance is based on the total persistent disk capacity attached to an instance and the number of vCPUs that the instance has. Incrementing the persistent disk capacity will increment its throughput and IOPS, which in turn improve the performance of MySQL.\n\nD is not correct because the DB administration team is requesting help with their MySQL instance. Migration to a different product should not be the solution when other optimization techniques can still be applied first.","comment_id":"566419","upvote_count":"4","poster":"tluu"},{"timestamp":"1647051180.0","content":"D...migrate it. Simple","poster":"SAMBIT","upvote_count":"1","comment_id":"565897"},{"timestamp":"1644610860.0","upvote_count":"2","content":"Selected Answer: C\nI got this question on my exam.","comment_id":"545474","poster":"[Removed]"},{"upvote_count":"2","content":"Selected Answer: C\nC is the answer","comment_id":"529023","poster":"Sreedharveluru","timestamp":"1642754460.0"},{"timestamp":"1642606500.0","comment_id":"527669","upvote_count":"1","poster":"KevPinto","content":"Selected Answer: A\nAn n1-standard-8 comes with 30GB as Standard, So Increasing (More than doubling) the memory will boost the Read cache leading to more data being consumed per IO cycle."},{"poster":"OrangeTiger","content":"I saw same question in GCP official moc exam.\nC was correct here.\nBut seems either is ok.All choice are works for repair performance :(\nI shoud read this thread...","timestamp":"1640769360.0","upvote_count":"1","comment_id":"511998"},{"poster":"vincy2202","content":"Answer is C","upvote_count":"2","timestamp":"1638015480.0","comment_id":"488113"},{"content":"C: Correct. Increase in disk size increases the performance & IOPS","poster":"aviratna","timestamp":"1624699860.0","upvote_count":"1","comment_id":"391085"},{"upvote_count":"1","poster":"victory108","comment_id":"360247","content":"C. Dynamically resize the SSD persistent disk to 500 GB","timestamp":"1621325940.0"},{"content":"I will go with C","timestamp":"1620664020.0","poster":"un","upvote_count":"1","comment_id":"353913"},{"upvote_count":"1","content":"C. IOPS will increase with larger capacity.","timestamp":"1617934260.0","poster":"jeff001","comment_id":"331657"},{"content":"C is ok","poster":"lynx256","upvote_count":"1","timestamp":"1617092520.0","comment_id":"324096"},{"poster":"Ausias18","upvote_count":"2","comment_id":"323987","timestamp":"1617080940.0","content":"Answer is C"},{"content":"A is incorrect because n1 standard 8 has 8 cpu cores and memory can be 0.95 - 6.5 GB per core. So If assign max 6.5 GB memory per core, total memory comes out to be 52 GB. Increase to 64 GB does not make sense since 8 cores will be a bottleneck. Hence Answer is C.","timestamp":"1613839920.0","comment_id":"295224","poster":"Shruti1997","upvote_count":"4"},{"content":"I have no idea why size matters in GCP until I see this guide: https://cloud.google.com/compute/docs/disks/performance#performance_by_disk_size\n\nbelieve it or not, C is correct.\n\nMaybe this is how google make money. I don't remember AWS limit IOPS just by size. LOL.","poster":"bnlcnd","upvote_count":"1","comment_id":"274967","timestamp":"1611458280.0"},{"comment_id":"207472","content":"Persistent disk performance scales with the size of the disk and with the number of vCPUs on your VM instance.\n\nPerformance scales until it reaches either the limits of the disk or the limits of the VM instance to which the disk is attached. The VM instance limits are determined by the machine type and the number of vCPUs on the instance.\n\nso the memory does not matter really so answer is C","poster":"francisco_guerra","timestamp":"1603853700.0","comments":[{"timestamp":"1609868100.0","comment_id":"260447","upvote_count":"2","content":"Not true that memory doesn't matter, but we have no basis to think that it matters in this case. The obvious problem is that the small disk limits the IOPS.","poster":"mwilbert"}],"upvote_count":"1"},{"upvote_count":"1","timestamp":"1603737960.0","content":"Its C.","comment_id":"206510","poster":"nimso"},{"timestamp":"1600103880.0","poster":"AshokC","upvote_count":"2","comment_id":"179444","content":"C - Makes sense because disk throughput is tied to the size of the disk."},{"content":"Answer is C . Burst performance based on Size","comment_id":"151640","upvote_count":"1","timestamp":"1596684480.0","poster":"Sreekey"},{"content":"higher capacity better IOPS, answer is C","poster":"nezih","upvote_count":"2","comment_id":"121816","timestamp":"1593342780.0"},{"timestamp":"1592630280.0","content":"C is the right answer","poster":"Tushant","comment_id":"114440","upvote_count":"1"},{"poster":"gfhbox0083","comment_id":"106506","upvote_count":"1","timestamp":"1591768020.0","content":"C, for sure."},{"timestamp":"1591456260.0","content":"An n1-standard-16 provides 60GB memory (not 64). Also, you'd need to mysqldump/snapshot, and recreate the database, which would take time. Quickest fix = larger SSD and using resize2fs to increase partition - more GB = more IOPS = faster transactions.https://cloud.google.com/compute/docs/disks/performance","poster":"rehma017","comment_id":"103887","upvote_count":"1"},{"timestamp":"1591104720.0","comment_id":"100855","content":"C is the correct answer","poster":"Nirms","upvote_count":"1"},{"timestamp":"1590765780.0","comment_id":"98319","content":"C is correct","poster":"Ziegler","upvote_count":"1"},{"comment_id":"97295","upvote_count":"2","poster":"AD2AD4","content":"Final Decision to go with Option C, its an Official Google Practice Test question.","timestamp":"1590645780.0"},{"poster":"arunsvoice","content":"Bigger the disk, more IOPS, means better performance. Answer C","timestamp":"1590594660.0","upvote_count":"1","comment_id":"96895"},{"comment_id":"90221","timestamp":"1589672280.0","poster":"laksg","content":"answer is C","upvote_count":"1"},{"content":"I'll go with C.\nA is not reasonable because we are given no information that memory is an issue.\nB is not reasonable because it requires an architectural change in the database.\nC makes sense because disk throughput is tied to the size of the disk.\nD is not reasonable because exporting metrics by itself accomplishes nothing in terms of performance dains.\nE may be reasonable but it would require code changes.","timestamp":"1589071980.0","comment_id":"86291","upvote_count":"2","poster":"clouddude"},{"upvote_count":"1","content":"C is the correct answer","comment_id":"84711","poster":"gcp_aws","timestamp":"1588795320.0"},{"comment_id":"81689","content":"C, increasing storage would improve IOPS as well...","poster":"Ashutosh007","upvote_count":"2","timestamp":"1588251060.0"},{"comment_id":"53982","content":"The answer is correct, this if from GCP practice exam","timestamp":"1582424040.0","upvote_count":"5","poster":"MrGreatMan"},{"comment_id":"44711","content":"answer: C","timestamp":"1580390040.0","upvote_count":"5","poster":"2g"}],"timestamp":"2019-10-24 22:47:00","url":"https://www.examtopics.com/discussions/google/view/7161-exam-professional-cloud-architect-topic-1-question-19/","question_text":"The database administration team has asked you to help them improve the performance of their new database server running on Google Compute Engine. The database is for importing and normalizing their performance statistics and is built with MySQL running on Debian Linux. They have an n1-standard-8 virtual machine with 80 GB of SSD persistent disk.\nWhat should they change to get better performance from this system?","answer":"C","question_images":[],"exam_id":4,"answer_description":""},{"id":"dfTwbQZAfBH4wGp68ANR","isMC":true,"discussion":[{"comment_id":"699971","content":"Selected Answer: C\nC is correct answer:\nA. Change the Target type to DELTA_PER_MINUTE. (in this case the utlization tagret need to be in minutes which is not the case its percentage % and not time based.\nB. Change the Metric identifier to agent.googleapis.com/memory/bytes_used. (not applicable) \nC. Change the filter to metric.label.state = 'used' AND metric.label.state = 'buffered' AND metric.label.state = 'cached' AND metric.label.state = 'slab'. (this gives total memory used)\nD. Change the filter to metric.label.state = 'free' and the Target utilization to 20. (you would still need to change the the percent_used to percent_free)\n\nhttps://stackoverflow.com/questions/69267526/what-is-disk-data-cached-in-the-memory-usage-chart-metrics-of-gcp-compute-in\n\nhttps://cloud.google.com/compute/docs/autoscaler/scaling-cloud-monitoring-metrics","comments":[{"comment_id":"1303750","timestamp":"1730069640.0","upvote_count":"3","content":"C is correct - the link provided actually specs exactly what needs to be done\nhttps://cloud.google.com/compute/docs/autoscaler/scaling-cloud-monitoring-metrics#autoscale_based_on_memory_usage","poster":"chickennuggets"}],"upvote_count":"35","timestamp":"1666272600.0","poster":"Mahmoud_E"},{"timestamp":"1662376380.0","poster":"kiappy81","comment_id":"660090","content":"Selected Answer: A\nTARGET_TYPE: the value type for the metric.\ngauge: the autoscaler computes the average value of the data collected in the last couple of minutes and compares that to the utilization target.\ndelta-per-minute: the autoscaler calculates the average rate of growth per minute and compares that to the utilization target.\ndelta-per-second: the autoscaler calculates the average rate of growth per second and compares that to the utilization target. For accurate comparisons, if you set the utilization target in seconds, use delta-per-second as the target type. Likewise, use delta-per-minute for a utilization target in minutes.","upvote_count":"16"},{"upvote_count":"2","poster":"venkee","timestamp":"1738205880.0","content":"Selected Answer: A\nQ: How come filter condition using \"AND\" for same column with different value? 'OR' is more appropriate here to add up all rows to check the metric sum exceed 80%, IMHO. If I'm missing \nsomething, please let me know. BTW I'm forced to choose an answer, hence I selected 'A'. But I feel the question itself is malformed...","comment_id":"1348886"},{"upvote_count":"2","poster":"Ekramy_Elnaggar","timestamp":"1732902840.0","content":"Selected Answer: C\nThis question is wrong and contradict with question 199, Q199 is the right one","comment_id":"1319828"},{"poster":"MarcoPellegrino","timestamp":"1729076220.0","comment_id":"1298662","content":"There should always be a time reference when having a metric","upvote_count":"1"},{"poster":"pcamaster","content":"exam done today, This question has been changed in the exam and the filter in the text of the question is actually \"metric.label.state = 'used' AND metric.label.state = 'buffered' AND metric.label.state = 'cached' AND metric.label.state = 'slab'.\"","timestamp":"1722320940.0","comment_id":"1257921","upvote_count":"4"},{"comment_id":"1204462","poster":"Gino17m","content":"Selected Answer: C\n...but....propbably thwre is a mistake in the question. I assume that metric.label.state can't have many values in the same time so instead AND operator OR should be used ??????","upvote_count":"5","comments":[{"timestamp":"1716624900.0","upvote_count":"1","content":"Have same thought as you. Misunderstood this point thus selected wrong answer. All answers seem wrong...","poster":"huuthanhdlv","comment_id":"1218123"}],"timestamp":"1714470360.0"},{"content":"To configure autoscaling based on the percent of used memory, specify the percent_used metric provided by the memory Ops Agent metrics. You should filter the metric by state to use only the used memory state. If you do not specify the filter, then the autoscaler takes the sum of memory usage by all memory states labeled as buffered, cached, free, slab, and used.","poster":"[Removed]","comment_id":"1196886","upvote_count":"2","timestamp":"1713311040.0"},{"timestamp":"1712836380.0","comment_id":"1193740","content":"I'm preparing for a test and see that questions from 115 onwards are considered valid. Can anyone who's taken the test offer any insights or advice? Thank you!","upvote_count":"1","poster":"kalyan_krishna742020"},{"upvote_count":"1","poster":"a53fd2c","timestamp":"1712765640.0","content":"C is the right option ( use used and gauge as options as in the guide listed here\nhttps://cloud.google.com/compute/docs/autoscaler/scaling-cloud-monitoring-metrics#autoscale_based_on_memory_usage","comment_id":"1193151"},{"upvote_count":"4","comment_id":"1175026","timestamp":"1710599880.0","poster":"nuts_bee","content":"In the real exam \n\"Filter: metric.label.state = 'used' AND metric.label.state = 'buffered' AND metric.label.state = 'cached' AND metric.label.state = 'slab'\" is in the QUESTION.\n\nThe answer should be:\nC. Change the filter to metric.label.state = 'used'"},{"content":"Selected Answer: C\nC is the correct approach. The current filter only considers memory in the \"used\" state. However, the operating system also uses memory for caching, buffering, and other purposes. By modifying the filter we ensure the autoscaling policy considers all memory states, providing a more accurate representation of total memory usage.","upvote_count":"2","timestamp":"1709499660.0","poster":"mesodan","comment_id":"1165088"},{"content":"This question came in recent exam and default state already have all metric.label.state . Went with DELTE per minute option A","poster":"agadd2","upvote_count":"1","timestamp":"1709319900.0","comment_id":"1163685"},{"comment_id":"1152435","upvote_count":"3","timestamp":"1708157100.0","content":"Selected Answer: C\nQuestion has a mistake\n\"Filter: metric.label.state = 'used'\" is in option C\n\n\"Change the filter to metric.label.state = 'used' AND metric.label.state = 'buffered' AND metric.label.state = 'cached' AND metric.label.state = 'slab'.\"\nis actually in the queston.\n\n\nhttps://cloud.google.com/compute/docs/autoscaler/scaling-cloud-monitoring-metrics#autoscale_based_on_memory_usage\nYou should use: Filter: metric.label.state = 'used'","poster":"alpha_canary"},{"timestamp":"1706449260.0","comment_id":"1134109","upvote_count":"4","content":"Selected Answer: C\nThe question in actual exam is reverse. The filter in the question is metric.label.state = 'used' AND metric.label.state = 'buffered' AND metric.label.state = 'cached' AND metric.label.state = 'slab'.\n\nand the option C is:\nFilter: metric.label.state = 'used'\n\nC is the correct answer in that case\nhttps://cloud.google.com/compute/docs/autoscaler/scaling-cloud-monitoring-metrics#autoscale_based_on_memory_usage","poster":"[Removed]"},{"comment_id":"1108286","timestamp":"1703820180.0","comments":[{"upvote_count":"1","poster":"e5019c6","content":"Now D, for me is the closest one to being true. If you ask only for the free percentage_used and change the target to 20, you should be done.\nBut a question I read here was very interesting, and connects with the * used above...\nHow does it know that it should scale when the metric is above or below? We don't set that filter. We can hope that the autoscaling is smart enough to know that when we use 'used' we mean more than and when using 'free' we mean less than.\nI couldn't find any information about that, so if anyone gets any additional info, please share it.","timestamp":"1703820240.0","comment_id":"1108287"}],"poster":"e5019c6","content":"Selected Answer: D\nActually this question is kinda weird\nWe can discard A & B right away:\nA: If you change to DELTA_PER_MINUTE it'll calculate the difference in memory used from one minute to the other, and if that difference is bigger* than 80%, it'll trigger. Not what we want.\nB: If we change the metric to bytes_used, we must change the value of the gauge too. Not an option.\nNow comes the messy part.\nFollowing what is said in this page: https://cloud.google.com/monitoring/api/metrics_opsagent#agent-memory\nThe metric.label.state should be ONE of these: [buffered, cached, free, slab, used]\nAnd it also states that: 'Summing the values of all states yields the total memory on the machine'. So, using a simple equation, if we remove the 'free' one from them, then that would give us the total memory that is being used. But remember, it said ONE of them, so that would discard it.","upvote_count":"1"},{"content":"Selected Answer: C\nC is the correct one","poster":"Roro_Brother","timestamp":"1702646400.0","upvote_count":"1","comment_id":"1097336"},{"comments":[{"content":"How does it work for autoscaling? \n\"AND\" is considered as \"OR\"? \nWhen you already have metric.label.state = 'used' in the problem statement and have an issue, then you trying to add more conditions and hope that this will solve the problem?!?\nStrange....","poster":"ammonia_free","upvote_count":"1","timestamp":"1706099940.0","comment_id":"1130596"}],"timestamp":"1698805980.0","content":"Selected Answer: C\nIn the real exam, questions metric label state was mentioned as \n\"metric.label.state = 'used' AND metric.label.state = 'buffered' AND metric.label.state = 'cached' AND metric.label.state = 'slab'\"\nand \"metric.label.state = 'used'\" was given in answer C.","comment_id":"1059324","upvote_count":"7","poster":"rsvd"},{"timestamp":"1698586200.0","poster":"rsvd","content":"Selected Answer: C\nhttps://cloud.google.com/monitoring/api/metrics_agent#agent-memory\npercent_used\nCurrent percentage of memory used by memory state. Summing percentages over all states yields 100 percent. Sampled every 60 seconds.\nstate: One of [buffered, cached, free, slab, used].","upvote_count":"1","comment_id":"1056841"},{"content":"Selected Answer: A\nOption A : This is the best option because it will allow the autoscaler to scale the instance group based on the rate of change of the memory usage metric. This is more accurate than using the average utilization, as it takes into account the fact that the memory usage can change rapidly under high load.","comment_id":"1051418","timestamp":"1698033900.0","poster":"muh21","upvote_count":"1"},{"timestamp":"1696120980.0","content":"Answer should be A: The Gauge setting specifies that the autoscaler should compute the average value of the data collected over the last few minutes and compare it to the target value. By contrast, setting Target mode to DELTA_PER_MINUTE or DELTA_PER_SECOND autoscales based on the observed rate of change rather than an average value","upvote_count":"1","comment_id":"1021916","poster":"bob_builder"},{"timestamp":"1695967260.0","comment_id":"1020512","poster":"someone2011","content":"C is not good because of \"AND\"\nD is not good \"Utilization target: If you want the autoscaler to maintain a metric at a specific value, configure a utilization target. The autoscaler creates VMs when the metric value is above the target and deletes VMs when the metric value is below the target\"\nhttps://cloud.google.com/compute/docs/autoscaler/scaling-cloud-monitoring-metrics\nBandA and against the example found here:\nhttps://cloud.google.com/compute/docs/autoscaler/scaling-cloud-monitoring-metrics#autoscale_based_on_memory_usage\nI am pretty sure at the real test the question is a little different, because as presented here none of the options are ok.","upvote_count":"3"},{"comment_id":"981994","comments":[{"poster":"[Removed]","content":"D = \"Change the filter to metric.label.state = 'free' and the Target utilization to 20\"\nIt doesn't say BELOW 20. So Threshold of 20 would match when it was 20, 30, 40+..., right?","comment_id":"989554","timestamp":"1692920880.0","upvote_count":"1"}],"timestamp":"1692134640.0","poster":"abhi52","upvote_count":"1","content":"Selected Answer: D\nIts D. Given the choices, the most suitable one is to monitor the amount of free memory and trigger autoscaling when the free memory drops below 20%, implying that 80% or more is being used."},{"comment_id":"972533","poster":"Joellaw321","timestamp":"1691186160.0","comments":[{"poster":"e5019c6","timestamp":"1703819520.0","content":"In my exam I had question from the 103 onwards","upvote_count":"1","comment_id":"1108273"}],"content":"Does anyone know if the current PCA exam still has questions covering the older material prior to question 115 from this Q&A.","upvote_count":"1"},{"upvote_count":"4","poster":"Pau123","content":"This question is not accurate. All the options are wrong. It seems it is not well redacted. Actually, the initial statement is correct, take a look at https://cloud.google.com/compute/docs/autoscaler/scaling-cloud-monitoring-metrics#console_5\n\ngcloud compute instance-groups managed set-autoscaling MIG_NAME \\\n --max-num-replicas=MAX_INSTANCES \\\n --min-num-replicas=MIN_INSTANCES \\\n --update-stackdriver-metric=agent.googleapis.com/disk/operation_count \\\n --stackdriver-metric-utilization-target=TARGET_VALUE \\\n --stackdriver-metric-utilization-target-type=TARGET_TYPE","timestamp":"1690576380.0","comment_id":"965806"},{"poster":"gary_cooper","upvote_count":"2","timestamp":"1689616200.0","comment_id":"954470","content":"Selected Answer: C\nChange the filter to metric.label.state = 'used' AND metric.label.state = 'buffered' AND metric.label.state = 'cached' AND metric.label.state = 'slab'."},{"comment_id":"947945","upvote_count":"2","content":"Selected Answer: D\nfree is an available state filter for the percent_used metric.\n\nhttps://cloud.google.com/compute/docs/autoscaler/scaling-cloud-monitoring-metrics#autoscale_based_on_memory_usage","poster":"MortalWombat","timestamp":"1688981340.0"},{"poster":"[Removed]","comment_id":"935860","content":"Selected Answer: C\nKey words: \"when total memory usage exceeds 80%\" \n\nCorrect answer: C\n\nsource: https://cloud.google.com/compute/docs/autoscaler/scaling-cloud-monitoring-metrics#autoscale_based_on_memory_usage","upvote_count":"3","timestamp":"1687901880.0"},{"timestamp":"1687705500.0","comment_id":"933710","poster":"kapa900","content":"Selected Answer: A\nhttps://cloud.google.com/compute/docs/autoscaler/scaling-cloud-monitoring-metrics#autoscale_based_on_memory_usage","upvote_count":"1"},{"poster":"kapa900","comment_id":"933707","upvote_count":"2","content":"Selected Answer: C\nC\nhttps://cloud.google.com/compute/docs/autoscaler/scaling-cloud-monitoring-metrics#autoscale_based_on_memory_usage","timestamp":"1687705320.0"},{"comment_id":"928178","upvote_count":"1","poster":"red_panda","content":"Selected Answer: A\nA is correct. With C we are autoscale with totally sum of memory, and we do not want to scale except for the memory used","timestamp":"1687239360.0"},{"comment_id":"899988","upvote_count":"3","timestamp":"1684318980.0","content":"Selected Answer: A\nA is the best option","poster":"TheCloudGuruu"},{"timestamp":"1682254260.0","poster":"mateuszma","content":"Selected Answer: D\nRight answer is D","upvote_count":"1","comment_id":"878368"},{"content":"Selected Answer: D\nD. Change the filter to metric.label.state = 'free' and the Target utilization to 20.\n\nThe original filter you used (metric.label.state = 'used') is not suitable for measuring memory usage, as it only considers the \"used\" memory and ignores other aspects such as buffered, cached, and slab memory. \n\nBy changing the filter to metric.label.state = 'free' and adjusting the target utilization level to 20, you will be able to monitor the free memory, which will provide a more accurate representation of memory usage. When the free memory drops to 20% or below, the autoscaling policy will be triggered, allowing the application to scale under high load.","comments":[{"comment_id":"870015","upvote_count":"1","timestamp":"1681453560.0","poster":"JC0926","content":"A. Change the Target type to DELTA_PER_MINUTE:\nThis is not appropriate because the target type \"DELTA_PER_MINUTE\" is used to measure the change in the metric value per minute. In this case, we want to monitor the percentage of memory used, not how it changes over time. Using \"GAUGE\" as the target type is correct as it represents the current value of the metric."}],"comment_id":"862730","upvote_count":"3","timestamp":"1680762840.0","poster":"JC0926"},{"comment_id":"845050","upvote_count":"6","timestamp":"1679328300.0","poster":"Certolony","content":"Well... in Metrics Explorer, this filter with AND-ing of several memory types will give no result, because these memory types are disjoint so you cannot get usage of memory which is of type used and cached in the same time! It should be aggregated and summed up, not AND-ed.\nFree memory type makes no sense, as we would require signal to scale up when metrics goes below target and not above (I have tested it).\nBytes_used gives just huge number of used memory bytes.\nAnd DELTA_PER_MINUTE also is wrong, because mem utilization is a guage metrics type. When it is set, an error, that specified metric does not exist.\nThere is no valid answer...?"},{"poster":"Deb2293","timestamp":"1678845480.0","upvote_count":"2","content":"Selected Answer: C\nOption C proposes changing the filter to include multiple states ('used', 'buffered', 'cached', and 'slab') instead of just 'used'. This approach could potentially provide a more comprehensive view of the memory usage and ensure that the autoscaling policy is triggered when the application's memory usage reaches 80%","comment_id":"839463"},{"timestamp":"1677026400.0","content":"C.\n\"To configure autoscaling based on the percent of used memory, specify the percent_used metric provided by the memory Ops Agent metrics. You should filter the metric by state to use only the used memory state. If you do not specify the filter, then the autoscaler takes the sum of memory usage by all memory states labeled as buffered, cached, free, slab, and used.\"","upvote_count":"1","comment_id":"817312","poster":"nick_name_1"},{"poster":"pepigeon","timestamp":"1676984400.0","upvote_count":"3","content":"A would not prevent you running out of memory, it only responds to quick increases/decreases of usage\nB we could change the specs of the server at some point, definitely want to use %\nC as someone else mentioned if you add all these up that's the total amount of memory your instance has\nD seems like the best option because your used memory can stay stable whilst cached and buffered can increase. You can verify this in Metrics explorer. Free always changes unlike used.","comment_id":"816591"},{"poster":"Mannpal","comment_id":"800611","upvote_count":"1","content":"how to get full access 192 to 273","comments":[{"content":"purchase subscriber access","upvote_count":"3","poster":"r1ck","timestamp":"1676844900.0","comment_id":"814619"}],"timestamp":"1675751160.0"},{"content":"Selected Answer: C\nI vote for C to account for buffered, cached and slabs.","poster":"moota","comment_id":"798638","upvote_count":"1","timestamp":"1675577640.0"},{"timestamp":"1673580120.0","comment_id":"774065","content":"Selected Answer: A\nkiappy81 Highly Voted 4 months, 1 week ago\nSelected Answer: A\nTARGET_TYPE: the value type for the metric.\ngauge: the autoscaler computes the average value of the data collected in the last couple of minutes and compares that to the utilization target.\ndelta-per-minute: the autoscaler calculates the average rate of growth per minute and compares that to the utilization target.\ndelta-per-second: the autoscaler calculates the average rate of growth per second and compares that to the utilization target. For accurate comparisons, if you set the utilization target in seconds, use delta-per-second as the target type. Likewise, use delta-per-minute for a utilization target in minutes.","poster":"WFCheong","upvote_count":"4"},{"content":"Selected Answer: C\nwe are using percent so why use delta time? answer D is discarded as you need to change memory/percent_used also, B is nonsense.","comment_id":"757550","timestamp":"1672065480.0","poster":"thamaster","upvote_count":"2"},{"comment_id":"757529","poster":"omermahgoub","comments":[{"poster":"omermahgoub","content":"You should not change the target type to DELTA_PER_MINUTE or the metric identifier to agent.googleapis.com/memory/bytes_used, as these changes would not address the root cause of the problem. Similarly, changing the filter to include all states of memory (used, buffered, cached, and slab) would not solve the issue, as it would still be based on the percentage of memory used rather than the amount of free memory available.","upvote_count":"1","timestamp":"1672064160.0","comment_id":"757530"}],"timestamp":"1672064160.0","content":"Selected Answer: D\nThe issue with the current configuration is that the autoscaling policy is based on the percentage of memory used, rather than the amount of free memory available. Therefore, even if the memory usage is high, the autoscaling policy may not trigger because the percentage of memory used is still below the target utilization level of 80%.\n\nTo fix this issue, you should change the filter to metric.label.state = 'free' and the target utilization to 20. This will cause the autoscaling policy to scale the application when the amount of free memory falls below 20%. This is the opposite of the current configuration, which scales the application when the percentage of memory used exceeds 80%.","upvote_count":"4"},{"comment_id":"748861","timestamp":"1671368520.0","upvote_count":"3","poster":"AB_7","content":"Selected Answer: A\nWhy C is wrong ?\n--> To configure autoscaling based on the percent of used memory, specify the percent_used metric provided by the memory Ops Agent metrics. You should filter the metric by state to use only the used memory state. If you do not specify the filter, then the autoscaler takes the sum of memory usage by all memory states labeled as buffered, cached, free, slab, and used."},{"content":"Selected Answer: A\nA is ok:\nhttps://cloud.google.com/compute/docs/autoscaler/scaling-cloud-monitoring-metrics\nPoint #13 explains the difference between Gauge and DELTA_PER _MINUTE.\nGauge: The autoscaler computes the average value of the data collected in the last couple of\nminutes and compares that to the utilization target.\nDelta / min: The autoscaler calculates the average rate of growth per minute and compares that to the utilization target.","upvote_count":"2","timestamp":"1669023900.0","comment_id":"723354","poster":"megumin"},{"content":"C is correct","comment_id":"715933","poster":"sfsdeniso","upvote_count":"2","timestamp":"1668160680.0"},{"comment_id":"695672","timestamp":"1665863160.0","content":"I will go with A","upvote_count":"1","poster":"AzureDP900"},{"upvote_count":"3","timestamp":"1665293700.0","comment_id":"689926","content":"Selected Answer: D\nWe have to think in different way\nOption A: TARGET_TYPE: the autoscaler calculates the average rate of growth per minute and compares that to the utilization target.\nThis will not fit question requirments, only the memory utilization growth will be compared with the target value.\nOption C: Adding all the memory cached, buffer, used and free will always be constant. So, this will also not fit the question requirements.\nOption D: I want the \"free\" memory utilization will be around 20. This will solve the problem.","poster":"AMEJack"},{"poster":"6721sora","comments":[{"content":"C is the correct answer: https://stackoverflow.com/questions/69267526/what-is-disk-data-cached-in-the-memory-usage-chart-metrics-of-gcp-compute-in\n\nAs you see from the images, the TOTAL memory used by a linux Compute Engine instance in GCP is comprised of used, buffered, cache and slab memory.\n\nThe question says that your application doesn't scale under load. Why is that? Because you are just filtering by used memory, thus excluding buffered, cached and slab memory from the TOTAL memory count.","poster":"jahiye3916","upvote_count":"4","comments":[{"poster":"jahiye3916","content":"Further reference: https://cloud.google.com/monitoring/api/metrics_agent#memory/percent_used","comment_id":"690137","upvote_count":"2","timestamp":"1665316920.0"}],"timestamp":"1665305100.0","comment_id":"690002"}],"content":"Selected Answer: C\nI choose C since I don't find any of the others as appropriate","comment_id":"664804","timestamp":"1662741120.0","upvote_count":"3"},{"comments":[{"content":"I choose A as well.\n\nI believe this link explains the target type better. https://cloud.google.com/compute/docs/autoscaler/scaling-cloud-monitoring-metrics\n\nPoint #13 explains the difference between Gauge and DELTA_PER_MINUTE.\n\n\"Gauge: The autoscaler computes the average value of the data collected in the last couple of minutes and compares that to the utilization target.\nDelta / min: The autoscaler calculates the average rate of growth per minute and compares that to the utilization target.\"","poster":"kuboraam","upvote_count":"3","comment_id":"663335","comments":[{"poster":"6721sora","upvote_count":"4","timestamp":"1662741000.0","content":"A gauge metric, in which the value measures a specific instant in time. For example, metrics measuring CPU utilization are gauge metrics; each point records the CPU utilization at the time of measurement. Another example of a gauge metric is the current temperature.\n\nA delta metric, in which the value measures the change since it was last recorded. For example, metrics measuring request counts are delta metrics; each value records how many requests were received since the last data point was recorded.\n\n\nDelta is inappropriate for memory utilization monitoring","comment_id":"664802"},{"poster":"6721sora","timestamp":"1662740760.0","upvote_count":"4","comment_id":"664798","content":"Delta per minute gives rate of growth. So rate of growth of memory utilization could be 10% growth per minute. We don't want the rate of growth to be compared to 80% but the actual utilization. None of the answers make sense"}],"timestamp":"1662623640.0"}],"poster":"jabrrJ68w02ond1","comment_id":"659187","timestamp":"1662289140.0","upvote_count":"5","content":"I'd vote A, as this sounds like the application has memory spikes that the autoscaler does not grasp.\n\nAlthough the describe autoscaling tactic is presented in the documentation [1], A is the only option that comes to my mind that makes sense.\n\n[1] https://cloud.google.com/compute/docs/autoscaler/scaling-cloud-monitoring-metrics#autoscale_based_on_memory_usage"}],"answers_community":["C (61%)","A (27%)","12%"],"question_text":"You have a Compute Engine application that you want to autoscale when total memory usage exceeds 80%. You have installed the Cloud Monitoring agent and configured the autoscaling policy as follows:\n✑ Metric identifier: agent.googleapis.com/memory/percent_used\n✑ Filter: metric.label.state = 'used'\n✑ Target utilization level: 80\n✑ Target type: GAUGE\nYou observe that the application does not scale under high load. You want to resolve this. What should you do?","answer_description":"","exam_id":4,"question_images":[],"answer":"C","topic":"1","choices":{"C":"Change the filter to metric.label.state = 'used' AND metric.label.state = 'buffered' AND metric.label.state = 'cached' AND metric.label.state = 'slab'.","B":"Change the Metric identifier to agent.googleapis.com/memory/bytes_used.","A":"Change the Target type to DELTA_PER_MINUTE.","D":"Change the filter to metric.label.state = 'free' and the Target utilization to 20."},"timestamp":"2022-09-04 12:59:00","url":"https://www.examtopics.com/discussions/google/view/80040-exam-professional-cloud-architect-topic-1-question-190/","question_id":102,"answer_ET":"C","unix_timestamp":1662289140,"answer_images":[]},{"id":"h28jeGKyf4o8Ph5Jh5P1","choices":{"D":"A single Cloud VPN gateway connected to an on-premises VPN gateway","A":"An HA Cloud VPN gateway connected with two tunnels to an on-premises VPN gateway","C":"Two HA Cloud VPN gateways connected to two on-premises VPN gateways Configure each HA Cloud VPN gateway to have two tunnels, each connected to different on-premises VPN gateways","B":"Two Classic Cloud VPN gateways connected to two on-premises VPN gateways Configure each Classic Cloud VPN gateway to have two tunnels, each connected to different on-premises VPN gateways"},"answer_description":"","timestamp":"2022-09-05 21:38:00","unix_timestamp":1662406680,"answer_ET":"A","answer":"A","question_text":"You are deploying an application to Google Cloud. The application is part of a system. The application in Google Cloud must communicate over a private network with applications in a non-Google Cloud environment. The expected average throughput is 200 kbps. The business requires:\n✑ as close to 100% system availability as possible\n✑ cost optimization\nYou need to design the connectivity between the locations to meet the business requirements. What should you provision?","isMC":true,"answers_community":["A (82%)","Other"],"question_images":[],"url":"https://www.examtopics.com/discussions/google/view/80419-exam-professional-cloud-architect-topic-1-question-191/","question_id":103,"exam_id":4,"topic":"1","discussion":[{"content":"Selected Answer: A\nA is true only if the on-prem (peer) gateway has two separate external P addresses. The HA VPN gateway uses two tunnels, one tunnel to each external IP address on the peer device as described in https://cloud.google.com/network-connectivity/docs/vpn/concepts/topologies#configurations_that_support_9999_availability\n\nC is a complete solution that provides full redundancy of the on-prem gateway. This is probably more expensive and having two HA VPN Gateways is an unusual configuration as the online documentation only describes using one HA VPN Gateway\n\nA appears to be correct with assumptions...!","poster":"minmin2020","upvote_count":"7","timestamp":"1682062980.0","comment_id":"700625"},{"comment_id":"667394","upvote_count":"5","content":"Selected Answer: A\nA can provide 99.99% availability as well, and no need for C which will be more expensive.\n\nhttps://cloud.google.com/network-connectivity/docs/vpn/concepts/topologies#1-peer-1-address","timestamp":"1678655580.0","poster":"alexandercamachop"},{"comments":[{"poster":"Brainstorm","timestamp":"1729593360.0","comment_id":"1200104","content":"Sorry, I meant C is correct.","upvote_count":"1"}],"upvote_count":"1","timestamp":"1729593240.0","content":"Selected Answer: D\nD is correct. \nA. Single HA Cloud VPN with Two Tunnels: This offers redundancy, but a single point of failure exists at the Cloud VPN gateway itself. A second HA Cloud VPN gateway provides additional fault tolerance.\nB. Two Classic Cloud VPNs with Multiple Tunnels: Classic Cloud VPNs are being phased out and might be less cost-effective than HA Cloud VPNs. Additionally, managing multiple Classic Cloud VPN gateways can be more complex.\nD. Single Cloud VPN Gateway: This offers a single point of failure and wouldn't achieve the desired level of high availability.","poster":"Brainstorm","comment_id":"1200103"},{"content":"why not B? See https://cloud.google.com/network-connectivity/docs/vpn/concepts/classic-topologies#option-3\n\nDon't you need redundant VPN gateways on the other side (on-prem) to reach close to 100% availability? A) has a single VPN gateway on-prem.","comment_id":"1053933","comments":[{"timestamp":"1730293260.0","poster":"Gino17m","comment_id":"1204513","content":"Because B i about increasing throughput and load balancing not about availability ??? Expected avarage throughput is \"only\" 200 kbps.\nA) has a single VPN...right but I think the clue is in \"You are deploying an application to Google Cloud. The application is part of a system.\"....so probably you have no leverage on on-premises solutions ?????? ....but in fact I'm not sure wchich answer is right :(","upvote_count":"1"}],"upvote_count":"1","poster":"devnul","timestamp":"1714068180.0"},{"comment_id":"925860","upvote_count":"3","content":"A: looks an exact match for the requirements, 99.99% availability\n\nB: Is a manual implementation of HA, not optimizing cost\n\nC: Is behoynd HA, no longer optimizing cost.\n\nD: Does not provide close to 100% as possible","timestamp":"1702808640.0","poster":"BiddlyBdoyng"},{"upvote_count":"3","comment_id":"830880","content":"Selected Answer: A\nUse HA (High Availability) VPN as required in the question. A is better aswer.","poster":"AugustoKras011111","timestamp":"1694002380.0"},{"content":"Selected Answer: A\nboth A and C are possible solutions but A is cheaper.","comment_id":"752429","upvote_count":"2","poster":"ale_brd_111","timestamp":"1687350960.0"},{"poster":"megumin","upvote_count":"2","comment_id":"723355","content":"Selected Answer: A\nA is ok","timestamp":"1684655280.0"},{"content":"Selected Answer: D\nCorrect Answer is D,\nYou cannot migrate an existing Cloud VPN tunnel or tunnels on a Classic VPN gateway to an HA VPN gateway. Instead, you need to create new tunnels and delete the old ones.\nhttps://cloud.google.com/network-connectivity/docs/vpn/how-to/moving-to-ha-vpn#general_guidelines","poster":"winter0w","comment_id":"722085","timestamp":"1684502880.0","comments":[{"comment_id":"1204518","content":"You cannot migrate.....but....\"You are deploying an application to Google Cloud\"....nothing to migrate....","poster":"Gino17m","upvote_count":"1","timestamp":"1730293440.0"}],"upvote_count":"2"},{"content":"Selected Answer: A\nA satisfty both requriements","poster":"Mahmoud_E","comments":[{"comment_id":"700015","content":"Satisfy both requirements for close to 100% availability and cost containment","timestamp":"1682000280.0","poster":"Mahmoud_E","upvote_count":"1"}],"upvote_count":"3","comment_id":"700014","timestamp":"1682000160.0"},{"poster":"AzureDP900","timestamp":"1681588140.0","content":"A is right","comment_id":"695673","upvote_count":"1"},{"comment_id":"672392","content":"Selected Answer: A\nbest explained in https://jayendrapatil.com/tag/classic-vpn-vs-ha-vpn/\nHA VPN provides an SLA of 99.99% service availability, when configured with two interfaces and two external IP addresses.","poster":"ForkMeSoftly","timestamp":"1679153100.0","upvote_count":"2"},{"content":"Selected Answer: A\nTo meet the 99.99% SLA on the Google Cloud side, there must be a tunnel from each of the two interfaces on the HA VPN gateway to the corresponding interfaces on the peer gateway.","poster":"alvinlxw","upvote_count":"1","timestamp":"1679055180.0","comment_id":"671419"},{"content":"Selected Answer: A\nA can provide 99.99% availability as well, and no need for C which will be more expensive.\n\nhttps://cloud.google.com/network-connectivity/docs/vpn/concepts/topologies#1-peer-1-address","upvote_count":"3","timestamp":"1678640700.0","poster":"zellck","comment_id":"667130"},{"timestamp":"1678199220.0","comment_id":"662463","content":"Selected Answer: C\nC is full mash. real HR with redundancy on the on premises site","upvote_count":"1","poster":"Nirca"},{"poster":"kuboraam","comment_id":"662231","content":"Selected Answer: A\nI choose A. Gives you 99.99% availability, and is certainly cheaper than B, C and is more reliable than D.\n\nhttps://cloud.google.com/network-connectivity/docs/vpn/concepts/topologies","upvote_count":"3","timestamp":"1678184460.0"},{"comment_id":"660504","timestamp":"1678052280.0","poster":"aswani","content":"Selected Answer: C\nhttps://cloud.google.com/static/network-connectivity/docs/vpn/images/ha-vpn-gcp-to-on-prem-2-a.svg","upvote_count":"3"}],"answer_images":[]},{"id":"fdmOmXpryEI8zfvKwzEQ","timestamp":"2023-09-23 08:10:00","question_text":"Your company has an application running on App Engine that allows users to upload music files and share them with other people. You want to allow users to upload files directly into Cloud Storage from their browser session. The payload should not be passed through the backend. What should you do?","answer_description":"","question_images":[],"answer_images":[],"answer_ET":"A","answers_community":["A (84%)","C (16%)"],"isMC":true,"choices":{"C":"1. Use the Cloud Storage Signed URL feature to generate a POST URL.\n2. Use App Engine default credentials to sign requests against Cloud Storage.","D":"1. Assign the Cloud Storage WRITER role to users who upload files.\n2. Use App Engine default credentials to sign requests against Cloud Storage.","A":"1. Set a CORS configuration in the target Cloud Storage bucket where the base URL of the App Engine application is an allowed origin.\n2. Use the Cloud Storage Signed URL feature to generate a POST URL.","B":"1. Set a CORS configuration in the target Cloud Storage bucket where the base URL of the App Engine application is an allowed origin.\n2. Assign the Cloud Storage WRITER role to users who upload files."},"topic":"1","exam_id":4,"discussion":[{"comment_id":"1014941","upvote_count":"10","content":"Selected Answer: A\nIt should be A. Since it is stated that the payload should not passed from the backend and be send directly to the bucket, then a CORS configuration should be set to the bucket.","poster":"mmathiou","timestamp":"1695476040.0"},{"comment_id":"1336742","upvote_count":"1","poster":"plumbig11","timestamp":"1736078040.0","content":"Selected Answer: A\nCORS with signed URL"},{"poster":"cruel_sun","upvote_count":"4","timestamp":"1719408420.0","content":"Selected Answer: A\nA. 1. Set a CORS configuration in the target Cloud Storage bucket where the base URL of the App Engine application is an allowed origin. 2. Use the Cloud Storage Signed URL feature to generate a POST URL.\nHere's why this approach is most suitable:\n • CORS configuration: This allows cross-origin requests from your App Engine application to access the Cloud Storage bucket for uploads. Setting the App Engine base URL as an allowed origin ensures secure communication.\n • Cloud Storage Signed URL: This feature generates a temporary URL with specific permissions and expiration time. You can provide this signed URL to the user's browser for uploading files directly to Cloud Storage. The payload (music file) doesn't pass through your backend, reducing server load.","comment_id":"1237482"},{"timestamp":"1709478120.0","upvote_count":"1","poster":"madcloud32","content":"Selected Answer: A\nI would go for A as per its definition and it works good that way","comment_id":"1164868"},{"upvote_count":"1","poster":"Amrita2012","timestamp":"1707569160.0","content":"It's between A and C.\n\nBut if you select C then you have to justify the use of \"Use App Engine default credentials to sign requests against Cloud Storage. \" hence go with option A.","comment_id":"1146311"},{"upvote_count":"2","timestamp":"1706328120.0","comment_id":"1133051","poster":"Namanjain7206","content":"Selected Answer: C\nhttps://cloud.google.com/blog/products/storage-data-transfer/uploading-images-directly-to-cloud-storage-by-using-signed-url"},{"poster":"91d8ca7","content":"Selected Answer: C\nI'm also not sure either A or C. But in my PCA exam today, I choose C. And I have passed.","comment_id":"1117435","timestamp":"1704799920.0","upvote_count":"1"},{"upvote_count":"2","timestamp":"1702262220.0","content":"C is the Answer. https://cloud.google.com/storage/docs/cross-origin","comments":[{"upvote_count":"2","timestamp":"1702520520.0","comment_id":"1095959","poster":"Patrick2708","content":"if its cross-origin. Then why C is answer? Shouldn't it be A"}],"comment_id":"1093022","poster":"Anudeep58"},{"poster":"Prakzz","content":"Signed URL is for TIme-Based access. This needs access all the time.","timestamp":"1696416840.0","upvote_count":"3","comment_id":"1024662"},{"timestamp":"1695965940.0","comment_id":"1020496","poster":"someone2011","content":"A:\nhttps://cloud.google.com/storage/docs/cross-origin#server-side-support\n\"Cloud Storage supports this specification by allowing you to configure your buckets to support CORS. Continuing the above example, you can configure the example.storage.googleapis.com bucket so that a browser can share its resources with scripts from example.appspot.com.\"","upvote_count":"3"},{"content":"The correct answer is A","poster":"Murtuza","upvote_count":"1","timestamp":"1695570900.0","comment_id":"1015969"},{"comments":[{"upvote_count":"3","comment_id":"1052642","poster":"xaqanik","content":"There is no any relationship between App engine application and cloud storage. \nYou need bind them.","timestamp":"1698133200.0"}],"upvote_count":"3","comment_id":"1014693","content":"Not sure is A or C. I will go with C.\n\nhttps://cloud.google.com/blog/products/storage-data-transfer/uploading-images-directly-to-cloud-storage-by-using-signed-url","poster":"sheucm89","timestamp":"1695449400.0"}],"question_id":104,"url":"https://www.examtopics.com/discussions/google/view/121240-exam-professional-cloud-architect-topic-1-question-192/","unix_timestamp":1695449400,"answer":"A"},{"id":"w8Kg06b1rxqrMGvroADn","answer":"B","exam_id":4,"question_text":"You are configuring the cloud network architecture for a newly created project in Google Cloud that will host applications in Compute Engine. Compute Engine virtual machine instances will be created in two different subnets (sub-a and sub-b) within a single region:\n• Instances in sub-a will have public IP addresses.\n• Instances in sub-b will have only private IP addresses.\n\nTo download updated packages, instances must connect to a public repository outside the boundaries of Google Cloud. You need to allow sub-b to access the external repository. What should you do?","choices":{"C":"Configure a bastion host instance in sub-a to connect to instances in sub-b.","D":"Enable Identity-Aware Proxy for TCP forwarding for instances in sub-b.","B":"Configure Cloud NAT and select sub-b in the NAT mapping section.","A":"Enable Private Google Access on sub-b."},"discussion":[{"poster":"piyu1515","upvote_count":"1","content":"Selected Answer: C\nAnswer is B. Cloud Nat is the right service","comment_id":"1334741","timestamp":"1735641840.0"},{"poster":"Amrita2012","timestamp":"1723287180.0","comment_id":"1146316","upvote_count":"1","content":"Selected Answer: B\nhttps://www.youtube.com/watch?v=4uskhIk7LdM"},{"poster":"cchiaramelli","content":"Selected Answer: B\nIMHO\n\nA -> It doesn't make sense, Public Google Access allows you to access Google APIs without an external IP, which doesnt solve the problem\nC -> Bastion host is for the opposite purpose; accessing a machine administratively from the outside without an external IP, not a machine without an external IP accessing the outside.\nD -> It doesn't make sense.\nB -> It's the recommended solution for GCP","upvote_count":"4","comment_id":"1056886","timestamp":"1714394520.0"},{"upvote_count":"2","comment_id":"1054536","content":"Selected Answer: B\nAnswer is B. Cloud Nat is the right service to use when you want to connect to reach services on internet without exposing the vm with an external IP","poster":"ampmusic","timestamp":"1714133760.0"},{"upvote_count":"1","content":"I will Select C. As there will many Instances will require internet access to update the OS.","comment_id":"1021810","comments":[{"poster":"RKS_2021","content":"Changed answer to B, Cloud NAT","upvote_count":"1","comment_id":"1021813","timestamp":"1711831320.0"}],"poster":"RKS_2021","timestamp":"1711830900.0"},{"content":"Selected Answer: B\nnat is what you need for non-external vm can reach the internet\nB is the only 1","comment_id":"1020020","upvote_count":"1","poster":"ductrinh","timestamp":"1711694700.0"},{"content":"Selected Answer: B\nCloud NAT allows the resources in a private subnet to access the internet—for updates, patching, config management, and more—in a controlled and efficient manner.","timestamp":"1711532040.0","upvote_count":"2","comment_id":"1018576","poster":"dsyouness"},{"comment_id":"1015979","timestamp":"1711303560.0","poster":"Murtuza","content":"Correct answer is B you will need NAT to access repositories hosted on the public internet","upvote_count":"1"}],"answer_images":[],"question_id":105,"topic":"1","unix_timestamp":1695571560,"url":"https://www.examtopics.com/discussions/google/view/121322-exam-professional-cloud-architect-topic-1-question-193/","question_images":[],"isMC":true,"answers_community":["B (91%)","9%"],"timestamp":"2023-09-24 18:06:00","answer_description":"","answer_ET":"B"}],"exam":{"provider":"Google","id":4,"name":"Professional Cloud Architect","lastUpdated":"11 Apr 2025","isBeta":false,"isImplemented":true,"numberOfQuestions":279,"isMCOnly":false},"currentPage":21},"__N_SSP":true}