{"pageProps":{"questions":[{"id":"LDTJqcsYkxtZMiI0tnzG","question_text":"After a recent security incident, your startup company wants better insight into what is happening in the Google Cloud environment. You need to monitor unexpected firewall changes and instance creation. Your company prefers simple solutions. What should you do?","topic":"1","unix_timestamp":1704044100,"discussion":[{"comment_id":"1135875","poster":"Stargazer11","upvote_count":"6","timestamp":"1706624460.0","content":"Selected Answer: B\nlog sink is advanced and it is used for routing logs to specific destinations. \nso answer B"},{"poster":"Pime13","comment_id":"1268713","content":"Selected Answer: D\nhttps://cloud.google.com/firewall/docs/firewall-rules-logging","timestamp":"1724074080.0","comments":[{"poster":"peddyua","timestamp":"1738266120.0","upvote_count":"1","content":"doesn't cover VM actions create|start|stop|terminate etc.","comment_id":"1349203"}],"upvote_count":"1"},{"comment_id":"1250673","upvote_count":"3","poster":"BuenaCloudDE","timestamp":"1721333520.0","content":"Selected Answer: B\nI think that key-words is \"After a recent security incident\", you need be notified if something happening with so important thing like secure."},{"timestamp":"1708479480.0","content":"Selected Answer: B\nB is a simple solution.","poster":"blackBeard33","comment_id":"1155140","upvote_count":"3"},{"comment_id":"1123536","poster":"interesting_owl","content":"Selected Answer: A\nthis is simple.","timestamp":"1705338240.0","upvote_count":"1"},{"upvote_count":"2","comment_id":"1110741","content":"Selected Answer: B\nPer ChatGPT, Option B provides a simple and effective solution using native Google Cloud services (Cloud Logging and log-based metrics) to monitor unexpected firewall changes and instance creation, while also allowing for the setup of reasonable alerts to ensure timely response to any security incidents.","poster":"KelvinToo","timestamp":"1704044100.0"}],"question_images":[],"question_id":176,"answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/130017-exam-associate-cloud-engineer-topic-1-question-257/","exam_id":1,"answer":"B","timestamp":"2023-12-31 18:35:00","answer_description":"","answers_community":["B (88%)","6%"],"isMC":true,"choices":{"A":"Create a log sink to forward Cloud Audit Logs filtered for firewalls and compute instances to Cloud Storage. Use BigQuery to periodically analyze log events in the storage bucket.","B":"Use Cloud Logging filters to create log-based metrics for firewall and instance actions. Monitor the changes and set up reasonable alerts.","D":"Turn on Google Cloud firewall rules logging, and set up alerts for any insert, update, or delete events.","C":"Install Kibana on a compute instance. Create a log sink to forward Cloud Audit Logs filtered for firewalls and compute instances to Pub/Sub. Target the Pub/Sub topic to push messages to the Kibana instance. Analyze the logs on Kibana in real time."},"answer_ET":"B"},{"id":"gbS4G3z7KEtVZqtXFYTK","answer":"D","question_id":177,"topic":"1","choices":{"D":"Grant roles/bigquery.dataViewer role to crm-databases and appropriate roles to web-applications.","A":"Grant \"project owner\" for web-applications appropriate roles to crm-databases.","B":"Grant \"project owner\" role to crm-databases and the web-applications project.","C":"Grant \"project owner\" role to crm-databases and roles/bigquery.dataViewer role to web-applications."},"question_text":"You are configuring service accounts for an application that spans multiple projects. Virtual machines (VMs) running in the web-applications project need access to BigQuery datasets in the crm-databases project. You want to follow Google-recommended practices to grant access to the service account in the web-applications project. What should you do?","answer_ET":"D","discussion":[{"content":"Selected Answer: D\nD is the least privilege and Google's recommended practices.","comment_id":"1116373","poster":"Gocool28","timestamp":"1704686160.0","upvote_count":"5"},{"comment_id":"1413867","timestamp":"1743382620.0","upvote_count":"1","poster":"85c887f","content":"Selected Answer: C\nC option is not the best in part \"project owner\", but at least Project Owner will enable possibility to grant roles on this project. Option D is more confusion to me. Why in option D we \"Grant roles/bigquery.dataViewer role to crm-databases\"? Should not it be granted to the service account of the web-applications project as it supposed to need to access to datasets on crm-databases?"},{"comment_id":"1295171","poster":"yomi95","timestamp":"1728480060.0","upvote_count":"1","content":"Selected Answer: D\nThe question does not describe any project requiring \"owner\" role access, hence granting that role to any of the project would violate least privilege. \nCan argue that crm-databases should have full access hence need owner role, but question does not mention specifically, and we only assume that."},{"poster":"d52e44d","upvote_count":"1","content":"Selected Answer: A\nI had my exam today and select A. I did only because of these sentence \"service accounts for an application that spans multiple projects .\" not 100% sure if it's correct but service account for web apps needs permissions to span projects. Maybe I got it wrong but A makes sense.\nIt's tricky cause you don't know if web-apps will also do some updates on BigQuery or not.","comment_id":"1201597","timestamp":"1713986280.0"},{"content":"Selected Answer: D\nD is the best answer and, for me, it was a process of elimination. The Project Owner role grants far-reaching permissions beyond what's needed for reading BQ datasets, violating the principle of least privilege.","upvote_count":"2","timestamp":"1709572140.0","comment_id":"1165832","poster":"PiperMe"},{"upvote_count":"1","poster":"Cynthia2023","timestamp":"1704244560.0","comment_id":"1112399","content":"Selected Answer: A\nInterpreting 'Project Owner' as the responsible entity, and not as the 'Project Owner' IAM role in Google Cloud: In this case, the instruction directs the person or entity managing the 'web-applications' project to grant appropriate roles for accessing the 'crm-databases' project. If this interpretation aligns with the intent of Option A, then it would indeed be a correct approach. Otherwise, none of the provided options would be correct.","comments":[{"timestamp":"1726535460.0","content":"We need to assign roles to the service account. It should have read access on the crm project.\n\nD is correct.","poster":"RKS_2021","comment_id":"1284996","upvote_count":"1"},{"upvote_count":"4","timestamp":"1707231420.0","comment_id":"1142277","poster":"LautaroBarone","content":"You're managing the service accounts, why would you grant any role to 'web-applications' project owner? The most appropiate should be D, because you are granting a wrong role to the service accounts in 'crm-databases' project, but then the option says that appropiate roles will be granted to service accounts in 'web-applications' project."}]},{"timestamp":"1704123120.0","upvote_count":"4","content":"It is 116 question. The answer is D.","poster":"dan12q","comment_id":"1111312"},{"timestamp":"1704044040.0","poster":"KelvinToo","comments":[{"content":"why give the role to the project crm-databases, it makes no sense.","poster":"Cynthia2023","upvote_count":"2","comment_id":"1112397","timestamp":"1704244200.0"}],"content":"Selected Answer: D\nPer ChatGPT, Option D aligns with the principle of least privilege, provides separation of concerns between projects, and allows for granular access control, making it the best choice for granting access to the service account in the web-applications project to access BigQuery datasets in the crm-databases project while following Google-recommended practices.","comment_id":"1110739","upvote_count":"1"},{"content":"D. Grant roles/bigquery.dataViewer role to crm-databases and appropriate roles to web-applications.","comment_id":"1109454","timestamp":"1703915760.0","upvote_count":"2","poster":"shiowbah"}],"isMC":true,"url":"https://www.examtopics.com/discussions/google/view/129843-exam-associate-cloud-engineer-topic-1-question-258/","answer_description":"","answer_images":[],"unix_timestamp":1703915760,"answers_community":["D (75%)","A (17%)","8%"],"exam_id":1,"question_images":[],"timestamp":"2023-12-30 06:56:00"},{"id":"E51uvYSedSc1LXcFbGoQ","question_images":[],"isMC":true,"answer_description":"","timestamp":"2023-12-30 06:56:00","url":"https://www.examtopics.com/discussions/google/view/129844-exam-associate-cloud-engineer-topic-1-question-259/","answer_images":[],"discussion":[{"comment_id":"1112811","timestamp":"1704290160.0","upvote_count":"9","poster":"apb98","comments":[{"comment_id":"1138043","timestamp":"1706837400.0","content":"not the same question. \nQuestion #: 259 : \"There are no private IP addresses available in the subnetwork\"\nQuestion #: 129 : \"There are no private IP addresses available in the VPC network.\"","upvote_count":"4","poster":"ashiqnazeem"},{"upvote_count":"1","timestamp":"1706430900.0","comment_id":"1133936","poster":"Arjun727","content":"Modify is not equals to Expanding"}],"content":"Selected Answer: A\nA. Same as question 129. Option A involves modifying the subnet range of the existing VPC network to increase the number of available IP addresses. By changing the subnet range to 172.16.20.0/24, you will have a larger IP address range to allocate to new VMs, allowing them to communicate with the Dataproc cluster.\n\nTo expand the IP range of a Compute Engine subnetwork, you can use:\ngcloud compute networks subnets expand-ip-range NAME"},{"comment_id":"1432990","timestamp":"1743706200.0","content":"Selected Answer: A\npls refer to https://cloud.google.com/blog/products/gcp/subnetwork-expansion-adds-even-more-flexibility-to-your-google-cloud-platform-private-networks","poster":"Jordarlu","upvote_count":"1"},{"poster":"longph8","content":"Selected Answer: B\nalthough “modifying” (expanding) the primary range might be an option in some cases, the recommended solution in this scenario is Option B, as it is simpler, safer, and minimizes any potential disruptions to your existing resources.","timestamp":"1742722500.0","comment_id":"1402201","upvote_count":"2"},{"comment_id":"1362719","poster":"Esteban08","upvote_count":"2","timestamp":"1740688440.0","content":"Selected Answer: B\nB. Create a new Secondary IP Range in the VPC and configure the VMs to use that range."},{"timestamp":"1739091840.0","comment_id":"1353810","upvote_count":"1","poster":"1826c27","content":"Selected Answer: C\nA is incorrect because: \"You can expand the primary IPv4 range of an existing subnet by modifying its subnet mask, setting the prefix length to a smaller number\". PREFIX!"},{"comment_id":"1295939","timestamp":"1728639720.0","poster":"denno22","content":"Selected Answer: A\ngcloud compute networks subnets expand-ip-range - expand the IP range of a Compute Engine subnetwork","upvote_count":"1"},{"poster":"omunoz","upvote_count":"2","content":"The question state \"using the minimum number of steps\" , then it should be A.","comment_id":"1213004","timestamp":"1715970060.0"},{"content":"I would say A is the answer, but I have no idea what the Q means when specifying \"You want to add new VMs to communicate with your cluster using the minimum number of steps.\" \n\nDoes it mean that you want to add VMs and use the same subnet or add new VMs and use another subnet and then want those VMs communicating with the VMs in the other subnet?","timestamp":"1709547540.0","comment_id":"1165497","upvote_count":"1","poster":"kuracpalac"},{"poster":"STEVE_PEGLEG","timestamp":"1706789280.0","content":"Selected Answer: C\nThe reason A isn't correct is because you can only expand a subnet by \"setting the prefix length to a smaller number\"\nSee: https://cloud.google.com/vpc/docs/create-modify-vpc-networks#expand-subnet\n\nThe reason B isn't correct is because you can only use a secondary (aka 'alias') IP address when there is a primary already in place. In this scenario this isn't possible to do for the new VMs because there are no primary IP addresses available.\n\nTherefore C seems like a feasible approach, with fewer steps than D (even if D is possible, which I don't know).","comment_id":"1137593","upvote_count":"2"},{"content":"The correct answer is **A**.\n\nTo add new VMs to communicate with your Dataproc cluster using the minimum number of steps, you should:\n- **Modify the existing subnet range** to 172.16.20.0/24. This will expand the range of available IP addresses in the subnet, allowing you to add new VMs.\n\nThe other options (B, C, and D) are not correct because they involve more steps than necessary (such as creating a new Secondary IP Range, a new VPC network, or enabling VPC Peering), which is not aligned with the requirement of using the minimum number of steps.","comment_id":"1122796","timestamp":"1705262940.0","poster":"JB28","upvote_count":"3"},{"comment_id":"1112021","content":"Answer is A","poster":"venomblade","timestamp":"1704211020.0","upvote_count":"3"},{"comment_id":"1111506","poster":"kaby1987","upvote_count":"2","timestamp":"1704153300.0","content":"Selected Answer: C\nOption C is right one as we dont have additional private ips left\nFor option D, This option is viable but is more complex than simply creating a new VPC and establishing VPC Peering."},{"poster":"KelvinToo","timestamp":"1704043980.0","upvote_count":"2","content":"Selected Answer: B\nPer ChatGPT, Option B aligns with the requirement of adding new VMs to communicate with the Dataproc cluster using the minimum number of steps while addressing the constraint of no available private IP addresses in the existing subnetwork.","comment_id":"1110738"},{"content":"B. Create a new Secondary IP Range in the VPC and configure the VMs to use that range.","timestamp":"1703915760.0","comment_id":"1109455","upvote_count":"2","poster":"shiowbah"}],"exam_id":1,"question_text":"Your Dataproc cluster runs in a single Virtual Private Cloud (VPC) network in a single subnetwork with range 172.16.20.128/25. There are no private IP addresses available in the subnetwork. You want to add new VMs to communicate with your cluster using the minimum number of steps. What should you do?","unix_timestamp":1703915760,"question_id":178,"answer_ET":"A","choices":{"C":"Create a new VPC network for the VMs. Enable VPC Peering between the VMs'VPC network and the Dataproc cluster VPC network.","D":"Create a new VPC network for the VMs with a subnet of 172.32.0.0/16. Enable VPC network Peering between the Dataproc VPC network and the VMs VPC network. Configure a custom Route exchange.","B":"Create a new Secondary IP Range in the VPC and configure the VMs to use that range.","A":"Modify the existing subnet range to 172.16.20.0/24."},"topic":"1","answer":"A","answers_community":["A (50%)","B (27%)","C (23%)"]},{"id":"cV1fZfigLgo73oHBL3OI","question_id":179,"choices":{"B":"Create a service account with an access scope. Use the access scope 'https://www.googleapis.com/auth/cloud-platform'.","D":"Create a service account and add it to the IAM role 'storage.objectAdmin' for that bucket.","C":"Create a service account and add it to the IAM role 'storage.objectCreator' for that bucket.","A":"Create a service account with an access scope. Use the access scope 'https://www.googleapis.com/auth/devstorage.write_only'."},"answer_ET":"C","question_text":"You need to set up permissions for a set of Compute Engine instances to enable them to write data into a particular Cloud Storage bucket. You want to follow\nGoogle-recommended practices. What should you do?","answer_description":"","isMC":true,"answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/16698-exam-associate-cloud-engineer-topic-1-question-26-discussion/","discussion":[{"upvote_count":"60","timestamp":"1584306660.0","poster":"coldpar","content":"As per as the least privileage recommended by google, C is the correct Option, A is incorrect because the scope doesnt exist. B incorrect because it will give him full of control","comment_id":"64477","comments":[{"upvote_count":"1","comments":[{"upvote_count":"1","poster":"Bedmed","timestamp":"1672071420.0","comment_id":"757644","content":"In the Document, it includes https://www.googleapis.com/auth/devstorage.read_write scope"},{"upvote_count":"1","poster":"CVGCP","comment_id":"909033","content":"There is no scope called write-only, as per the reference document.","timestamp":"1685333820.0"},{"content":"In the Document, 'write -only' does not exist. Just read-only","upvote_count":"2","poster":"karim1321","comment_id":"923253","timestamp":"1686750000.0"}],"timestamp":"1656307200.0","poster":"johnconnor","content":"Check here, it is A-> https://cloud.google.com/storage/docs/authentication\nhttps://cloud.google.com/storage/docs/authentication","comment_id":"623067"},{"upvote_count":"2","content":"The scope does exist - https://download.huihoo.com/google/gdgdevkit/DVD1/developers.google.com/compute/docs/api/how-tos/authorization.html","comments":[{"comment_id":"580854","content":"it doesn't exist. show us this on official google website","poster":"gielda211","upvote_count":"2","timestamp":"1649092200.0"},{"upvote_count":"4","timestamp":"1631791320.0","poster":"peter77","comment_id":"445886","content":"No it doesn't. You have read-only, read-write, full-control and others... but \"write-only\" is not a thing.\n\nhttps://cloud.google.com/storage/docs/authentication"}],"comment_id":"233286","timestamp":"1606933920.0","poster":"robor97"}]},{"comments":[{"timestamp":"1659416460.0","content":"There are many access scopes available to choose from, but a best practice is to set the cloud-platform access scope, which is an OAuth scope for most Google Cloud services, and then control the service account's access by granting it IAM roles..you have an app that reads and writes files on Cloud Storage, it must first authenticate to the Cloud Storage API. You can create an instance with the cloud-platform scope and attach a service account to the instance\nhttps://cloud.google.com/compute/docs/access/service-accounts","upvote_count":"1","comment_id":"641049","poster":"nickyshil"},{"poster":"ryumada","content":"Reading the second point of the best practice. You should grant your VM the https://www.googleapis.com/auth/cloud-platform scope to allow access to most of Google Cloud APIs.\n\nSo, that the IAM permissions are completely determined by the IAM roles you granted to the service account.\n\nThe conclusion is you should not mess up with the VM scopes to grant access to Google Services, instead you should grant the access via IAM roles of the service account you attached to the VM.\n\nhttps://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#best_practices","upvote_count":"2","timestamp":"1659425880.0","comment_id":"641149"}],"poster":"XRiddlerX","content":"In reviewing this, it looks to be a multiple answer question. According to Best Practices in this Google Doc (https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#best_practices) you grant the instance the scope and the permissions are determined by the IAM roles of the service account. In this case, you would grant the instance the scope and the role (storage.objectCreator) to the service account.\n\nAns B and C\n\nRole from GCP Console:\nID = roles/storage.objectCreator\nRole launch stage = General Availability\nDescription = Access to create objects in GCS.\n\n3 assigned permissions\nresourcemanager.projects.get\nresourcemanager.projects.list\nstorage.objects.create","upvote_count":"18","comment_id":"128774","timestamp":"1594113420.0"},{"timestamp":"1736077380.0","comments":[{"upvote_count":"1","comment_id":"1336740","content":"You have sensitive data stored in three Cloud Storage buckets and have enabled data access logging. You want to verify activities for a particular user for these buckets, using the fewest possible steps. You need to verify the addition of metadata labels and which files have been viewed from those buckets. What should you do?","poster":"Hanu17","timestamp":"1736077500.0"}],"upvote_count":"1","comment_id":"1336738","content":"Selected Answer: B\nThe reason why A is not an answer.\n\nThe Activity log in the GCP Console is part of the Cloud Audit Logs but focuses on high-level admin activities, not specific data access or detailed operations like viewing files or adding metadata labels","poster":"Hanu17"},{"poster":"Enamfrancis","timestamp":"1727023560.0","upvote_count":"2","comment_id":"1287861","content":"C because of 'storage.objectCreator'"},{"content":"Selected Answer: C\nCorrect answer is C","poster":"andreiboaghe95","comment_id":"1227851","timestamp":"1718021700.0","upvote_count":"2"},{"comment_id":"1063349","upvote_count":"1","timestamp":"1699223460.0","poster":"BAofBK","content":"Correct answer is D"},{"poster":"gsmasad","timestamp":"1698830700.0","content":"Selected Answer: C\nstorage.objectCreator contains sufficient privileges to do the job & so admin is not required","upvote_count":"1","comment_id":"1059494"},{"upvote_count":"4","comment_id":"1019919","content":"Selected Answer: C\nThe correct answer is C. \n\nThe other options are not accurate and go against the principle of giving least required access. \n\nA is incorrect as there is no role as write_only\nB is not a good option as it gives full control of google cloud services where as we are looking for write data into a particular cloud storage bucket\nD. is not a good option as it gives full control over objects\n\nSources:\nhttps://cloud.google.com/storage/docs/authentication\nhttps://cloud.google.com/storage/docs/access-control/iam-roles","timestamp":"1695908640.0","poster":"YourCloudGuru"},{"timestamp":"1693583820.0","comment_id":"996213","content":"Selected Answer: C\nc seems more correct, you need to go iam to provide the permissions , b and d will give it more or full access","poster":"Captain1212","upvote_count":"1"},{"poster":"Neha_Pallavi","comments":[{"content":"Hi Neha, Please let me know how your exam was? I am taking the exam soon. Thanks","poster":"Shubha1","comment_id":"970923","upvote_count":"1","timestamp":"1691053260.0"}],"content":"Associate Cloud Engineer exam booked very soon. kindly share the all the questions and any other support exam to clear this","timestamp":"1689910320.0","upvote_count":"2","comment_id":"958022"},{"content":"Selected Answer: C\nC is correct","poster":"ExamsFR","comment_id":"957431","upvote_count":"1","timestamp":"1689851040.0"},{"content":"C is correct","comment_id":"956728","poster":"rosh199","timestamp":"1689776700.0","upvote_count":"1"},{"timestamp":"1685333700.0","upvote_count":"3","comment_id":"909032","content":"Selected Answer: C\nC is correct answer","poster":"CVGCP"},{"timestamp":"1685289360.0","content":"Selected Answer: C\nThe ask is how the “Compute Engine instances to enable them to write data into a particular Cloud Storage bucket”. A service account is a special kind of account used by an application or compute workload, rather than a person. When you set up an instance to run as a service account, you determine the level of access the service account has by the IAM roles that you grant to the service account. If the service account has no IAM roles, then no resources can be accessed using the service account on that instance.\nThe best Practice suggested by Google is refer in this link: https://cloud.google.com/compute/docs/access/service-accounts#scopes_best_practice https://cloud.google.com/storage/docs/access-control/iam-roles shows that storage.objectCreator is best choice of the role for this problem statement.","comment_id":"908722","upvote_count":"2","poster":"trainingexam"},{"timestamp":"1682452740.0","content":"Selected Answer: C\nThe correct answer is C. \n\nThere is no role as write only its read only hence A is incorrect.","comment_id":"880819","poster":"Praxii","upvote_count":"1"},{"timestamp":"1680580560.0","upvote_count":"1","comment_id":"860597","content":"Selected Answer: C\nIAM Work on Principal of least privilege,","poster":"Ashish_Tayal"},{"comment_id":"843397","upvote_count":"2","content":"Option C is the correct answer. To grant a set of Compute Engine instances permissions to write data to a particular Cloud Storage bucket, you should create a service account and add it to the IAM role 'storage.objectCreator' for that bucket. This IAM role allows the service account to create new objects in the bucket, but it does not allow it to modify or delete existing objects. Option A is incorrect because the access scope 'https://www.googleapis.com/auth/devstorage.write_only' does not exist. Option B is incorrect because the access scope 'https://www.googleapis.com/auth/cloud-platform' grants permissions for all Google Cloud Platform services, which is overly broad and not recommended. Option D is incorrect because the IAM role 'storage.objectAdmin' provides full control over the bucket, which is more access than necessary to allow the Compute Engine instances to write data to the bucket.","poster":"smanoj85","timestamp":"1679196660.0"}],"topic":"1","question_images":[],"unix_timestamp":1584306660,"answers_community":["C (96%)","2%"],"timestamp":"2020-03-15 22:11:00","answer":"C","exam_id":1},{"id":"5CVFh5D8SNoAWbx1hXvL","question_id":180,"answer_images":[],"question_text":"You are building a backend service for an ecommerce platform that will persist transaction data from mobile and web clients. After the platform is launched, you expect a large volume of global transactions. Your business team wants to run SQL queries to analyze the data. You need to build a highly available and scalable data store for the platform. What should you do?","answer_ET":"A","topic":"1","answers_community":["A (100%)"],"unix_timestamp":1703915760,"answer":"A","question_images":[],"discussion":[{"comment_id":"1112818","timestamp":"1704290400.0","content":"Selected Answer: A\nA. Key is “large volume of global transactions”, so Cloud Spanner would be a good choice.","poster":"apb98","upvote_count":"7"},{"comment_id":"1273582","content":"Selected Answer: A\nPiperMe perfectly summed it up. Rember: Global + SQL = Cloud Spanner","timestamp":"1724776140.0","poster":"Timfdklfajlksdjlakf","upvote_count":"3"},{"timestamp":"1709578320.0","comment_id":"1165906","poster":"PiperMe","content":"Selected Answer: A\nA. Global + SQL = Cloud Spanner","upvote_count":"4"},{"content":"Selected Answer: A\nPer ChatGPT, Option A, creating a multi-region Cloud Spanner instance with an optimized schema, is the best choice for building a highly available and scalable data store that can efficiently handle global transactions and support SQL queries for analysis.","comment_id":"1110737","upvote_count":"4","timestamp":"1704043920.0","poster":"KelvinToo"},{"upvote_count":"4","comment_id":"1109456","content":"A. Create a multi-region Cloud Spanner instance with an optimized schema.","poster":"shiowbah","timestamp":"1703915760.0"}],"timestamp":"2023-12-30 06:56:00","url":"https://www.examtopics.com/discussions/google/view/129845-exam-associate-cloud-engineer-topic-1-question-260/","choices":{"B":"Create a multi-region Firestore database with aggregation query enabled.","D":"Create a multi-region BigQuery dataset with optimized tables.","A":"Create a multi-region Cloud Spanner instance with an optimized schema.","C":"Create a multi-region Cloud SQL for PostgreSQL database with optimized indexes."},"isMC":true,"exam_id":1,"answer_description":""}],"exam":{"isBeta":false,"isImplemented":true,"id":1,"provider":"Google","lastUpdated":"11 Apr 2025","name":"Associate Cloud Engineer","isMCOnly":true,"numberOfQuestions":285},"currentPage":36},"__N_SSP":true}