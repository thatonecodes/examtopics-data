{"pageProps":{"questions":[{"id":"0Gdf87ZTxUb8ZfKtDWPt","answer_images":[],"answers_community":["A (100%)"],"isMC":true,"url":"https://www.examtopics.com/discussions/google/view/124848-exam-professional-cloud-devops-engineer-topic-1-question-135/","unix_timestamp":1698515100,"question_images":[],"choices":{"B":"Grant the roles/artifactregistry.writer role to the Cloud Build service account. Confirm that no employee has Artifact Registry write permission.","C":"Use Cloud Run to write and deploy a custom validator. Enable an Eventarc trigger to perform validations when new images are uploaded.","A":"Configure Binary Authorization in your GKE clusters to enforce deploy-time security policies.","D":"Configure Kritis to run in your GKE clusters to enforce deploy-time security policies."},"timestamp":"2023-10-28 19:45:00","discussion":[{"timestamp":"1733590500.0","comment_id":"1090000","poster":"xhilmi","content":"Selected Answer: A\nChoose Ooption A:\nConfigure Binary Authorization in your GKE clusters to enforce deploy-time security policies. \n\nBinary Authorization allows you to define and enforce policies that determine which container images can be deployed based on image signatures. By configuring Binary Authorization, you can enforce deploy-time security policies, ensuring that only trusted and verified container images are allowed to run in your GKE clusters.\n\nThis approach provides a robust security mechanism without requiring additional custom validators or complex configurations, minimizing management overhead while meeting the stringent security requirements of a highly regulated domain.","upvote_count":"2"},{"upvote_count":"2","timestamp":"1730628780.0","content":"A is the answer.","comment_id":"1061278","poster":"mshafa"},{"poster":"lelele2023","content":"Selected Answer: A\nusing binary-authorization","comment_id":"1059599","timestamp":"1730457780.0","upvote_count":"2"},{"timestamp":"1730144700.0","upvote_count":"3","poster":"koo_kai","content":"Selected Answer: A\nhttps://cloud.google.com/binary-authorization/docs/overview","comment_id":"1056361"}],"answer_ET":"A","question_id":41,"exam_id":6,"topic":"1","answer":"A","question_text":"Your company operates in a highly regulated domain. Your security team requires that only trusted container images can be deployed to Google Kubernetes Engine (GKE). You need to implement a solution that meets the requirements of the security team while minimizing management overhead. What should you do?","answer_description":""},{"id":"FMG39GgqjZRuJvwyF4GG","answer_description":"","url":"https://www.examtopics.com/discussions/google/view/124238-exam-professional-cloud-devops-engineer-topic-1-question-136/","answer":"CE","exam_id":6,"discussion":[{"timestamp":"1697907300.0","comment_id":"1049681","poster":"activist","content":"I think the answers are B & D.","upvote_count":"8","comments":[{"comments":[{"content":"Not D, I don't think it's always possible to detail how the incident was resolved, may be too complicated. B and C for me.","poster":"winston9","upvote_count":"1","timestamp":"1717747200.0","comment_id":"1225970"}],"upvote_count":"1","poster":"Feliphus","timestamp":"1703413380.0","comment_id":"1104541","content":"Me too\nE -> include all incident participants in postmortem authoring, no much sense, the incident commander is the author of the postmortem\nA -> identify the person or team responsible for causing the incident\nC -> without naming internal system components, the postmortem has to be focus on the processes/components"}]},{"upvote_count":"7","comment_id":"1061806","timestamp":"1699050720.0","content":"Selected Answer: CE\nOption B is incorrect because it states that the postmortem should include how the incident could have been worse.The focus of the postmortem should be on identifying the root cause of the incident and developing recommendations for preventing future occurrences.","poster":"mshafa"},{"poster":"surfer111","comment_id":"1237138","content":"C, E - Don't share internal info and share as wide as possible. Post Mortems and RCAs typically are shared with customers.","upvote_count":"1","timestamp":"1719361020.0"},{"poster":"alpha_canary","timestamp":"1708077240.0","content":"Selected Answer: CE\nC: https://sre.google/workbook/postmortem-culture/#:~:text=away%20from%20us%E2%80%9D).-,Preventative%20action,Disallow%20any%20single%20operation%20from%20affecting%20servers%20spanning%20namespace/class%20boundaries%E2%80%9D).,-Blamelessness\n\nE: https://sre.google/workbook/postmortem-culture/#:~:text=Include%20all%20incident%20participants%20in%20postmortem%20authoring\nhttps://sre.google/workbook/postmortem-culture/#:~:text=In%20order%20to%20maintain%20a%20healthy%20postmortem%20culture%20within%20an%20organization%2C%20it%E2%80%99s%20important%20to%20share%20postmortems%20as%20widely%20as%20possible","upvote_count":"1","comment_id":"1151874"},{"content":"Selected Answer: CE\nChoose C & E\n\nOption C emphasizes including the severity of the incident, prevention strategies for future occurrences, and an analysis of what caused the incident without necessarily naming internal system components. This approach ensures a balance between transparency and security, providing valuable insights without exposing sensitive internal details.\n\nOption E, which advocates involving all incident participants in postmortem authoring and sharing postmortems widely, promotes a collaborative and inclusive culture. Involving all relevant stakeholders ensures a comprehensive understanding of the incident, and sharing postmortems widely fosters transparency, enabling the organization to learn from incidents collectively.\n\nTogether, these practices contribute to a successful postmortem policy that promotes continuous improvement and a culture of learning from incidents.","poster":"xhilmi","comment_id":"1090009","upvote_count":"3","timestamp":"1701932340.0"},{"comment_id":"1086699","poster":"nqthien041292","upvote_count":"2","content":"Selected Answer: CE\nVote CE","timestamp":"1701593940.0"},{"comment_id":"1081970","upvote_count":"2","content":"Selected Answer: CE\nI'll go for C & E","poster":"bhunias","timestamp":"1701121740.0"},{"timestamp":"1700074260.0","upvote_count":"4","poster":"pharao89","comment_id":"1071796","content":"Selected Answer: CE\nA. We don't blame\nB. I can't imagine a postmortem with information on how the incident could have been worse.\nC. Correct answer.\nD. It's nearly the same as C but doesn't include recommendations for the future, so I go with C.\nE. Correct, include all participants of the incident in authoring postmortem to not miss something important."},{"comment_id":"1070978","timestamp":"1700008980.0","content":"I thing is CE \nhttps://sre.google/workbook/postmortem-culture/","upvote_count":"3","poster":"TereRolon"},{"content":"Selected Answer: BD\nShouldn't mention customer information, it's not useful to spread it widely, might be causing negative impact.","poster":"lelele2023","comment_id":"1059610","upvote_count":"3","timestamp":"1698835860.0"},{"comment_id":"1056153","upvote_count":"4","content":"B & D is the answer","poster":"khoukha","timestamp":"1698493260.0"},{"content":"I would go with B & C","comment_id":"1054701","poster":"Jason_Cloud_at","upvote_count":"1","timestamp":"1698333840.0"}],"timestamp":"2023-10-21 18:55:00","isMC":true,"question_text":"Your CTO has asked you to implement a postmortem policy on every incident for internal use. You want to define what a good postmortem is to ensure that the policy is successful at your company. What should you do? (Choose two.)","choices":{"D":"Ensure that all postmortems include how the incident was resolved and what caused the incident without naming customer information.","B":"Ensure that all postmortems include what caused the incident, how the incident could have been worse, and how to prevent a future occurrence of the incident.","A":"Ensure that all postmortems include what caused the incident, identify the person or team responsible for causing the incident, and how to prevent a future occurrence of the incident.","E":"Ensure that all postmortems include all incident participants in postmortem authoring and share postmortems as widely as possible.","C":"Ensure that all postmortems include the severity of the incident, how to prevent a future occurrence of the incident, and what caused the incident without naming internal system components."},"unix_timestamp":1697907300,"answers_community":["CE (86%)","14%"],"question_images":[],"answer_ET":"CE","question_id":42,"topic":"1","answer_images":[]},{"id":"jGN6aavh2nlf8HYU0BsX","exam_id":6,"isMC":true,"answer_description":"","question_text":"You are developing reusable infrastructure as code modules. Each module contains integration tests that launch the module in a test project. You are using GitHub for source control. You need to continuously test your feature branch and ensure that all code is tested before changes are accepted. You need to implement a solution to automate the integration tests. What should you do?","topic":"1","answers_community":["D (73%)","C (27%)"],"unix_timestamp":1698000420,"answer_images":[],"timestamp":"2023-10-22 20:47:00","question_images":[],"answer":"D","url":"https://www.examtopics.com/discussions/google/view/124375-exam-professional-cloud-devops-engineer-topic-1-question-137/","question_id":43,"choices":{"C":"Use Cloud Build to run the tests. Trigger all tests to run after a pull request is merged.","D":"Use Cloud Build to run tests in a specific folder. Trigger Cloud Build for every GitHub pull request.","A":"Use a Jenkins server for CI/CD pipelines. Periodically run all tests in the feature branch.","B":"Ask the pull request reviewers to run the integration tests before approving the code."},"answer_ET":"D","discussion":[{"upvote_count":"1","poster":"6a8c7ad","timestamp":"1723547400.0","comment_id":"1265118","content":"Selected Answer: C\nD mentions folder. Huh? Iâ€™ll take C"},{"poster":"xhilmi","upvote_count":"1","timestamp":"1701932760.0","comment_id":"1090013","content":"Selected Answer: D\nChoose option D\nUse Cloud Build to run tests in a specific folder and trigger Cloud Build for every GitHub pull request.\n\nBy configuring Cloud Build to run tests in a specific folder, you can focus on the relevant tests for the modified code in the feature branch. Triggering Cloud Build for every GitHub pull request ensures that tests are automatically executed whenever changes are proposed, providing continuous integration.\n\nThis approach allows for automated testing of each pull request, providing early feedback to developers and ensuring that changes are thoroughly tested before being merged, contributing to a more reliable and efficient development process."},{"upvote_count":"2","content":"Selected Answer: D\nD is what I do at work for this scenario and it works well.","timestamp":"1700074440.0","comment_id":"1071798","poster":"pharao89"},{"poster":"mshafa","comment_id":"1068396","content":"Selected Answer: D\nThis approach automates the testing process, integrates well with your existing tools (GitHub and GCP), and ensures that code is tested in the most relevant part of the development lifecycle - before merging into the main branch.","upvote_count":"2","timestamp":"1699781400.0"},{"content":"Selected Answer: D\nshouldn't be C since it makes the test to happen after merge. Answer is D - as soon as a PR gets created then it runs tests.","timestamp":"1698836400.0","upvote_count":"1","comment_id":"1059617","poster":"lelele2023"},{"poster":"koo_kai","comment_id":"1056364","timestamp":"1698515460.0","content":"Selected Answer: D\nall code is tested before changes are accepted","upvote_count":"2"},{"content":"Selected Answer: C\nI would go with C","comment_id":"1054708","timestamp":"1698334020.0","poster":"Jason_Cloud_at","upvote_count":"2"},{"upvote_count":"3","poster":"Jay09812","timestamp":"1698084060.0","content":"Wouldnt it be D considering you would want to run the tests when the PR is created and not after the code is already merged","comment_id":"1052142"},{"content":"Answer C is correct. \nhttps://cloud.google.com/build/docs/automating-builds/create-manage-triggers","upvote_count":"2","comments":[],"poster":"activist","timestamp":"1698000420.0","comment_id":"1050983"}]},{"id":"lHDWlOOVHBy30fOzwaOI","url":"https://www.examtopics.com/discussions/google/view/124814-exam-professional-cloud-devops-engineer-topic-1-question-138/","timestamp":"2023-10-28 13:57:00","answer_ET":"D","isMC":true,"question_text":"Your company processes IoT data at scale by using Pub/Sub, App Engine standard environment, and an application written in Go. You noticed that the performance inconsistently degrades at peak load. You could not reproduce this issue on your workstation. You need to continuously monitor the application in production to identify slow paths in the code. You want to minimize performance impact and management overhead. What should you do?","exam_id":6,"question_id":44,"answer":"D","topic":"1","answers_community":["D (100%)"],"choices":{"D":"Configure Cloud Profiler, and initialize the cloud.google.com/go/profiler library in the application.","B":"Install a continuous profiling tool into Compute Engine. Configure the application to send profiling data to the tool.","A":"Use Cloud Monitoring to assess the App Engine CPU utilization metric.","C":"Periodically run the go tool pprof command against the application instance. Analyze the results by using flame graphs."},"answer_images":[],"unix_timestamp":1698494220,"answer_description":"","question_images":[],"discussion":[{"content":"answer is D:\nhttps://cloud.google.com/profiler/docs/profiling-go#app-engine","comment_id":"1056162","upvote_count":"7","poster":"khoukha","timestamp":"1730123820.0"},{"timestamp":"1733555580.0","poster":"xhilmi","upvote_count":"2","comment_id":"1090014","content":"Selected Answer: D\nChoose option D\n\nConfigure Cloud Profiler and initialize the cloud.google.com/go/profiler library in the application. Cloud Profiler is designed for low-overhead continuous profiling in production environments.\n\nBy configuring Cloud Profiler and initializing the corresponding library in the Go application, you can collect detailed performance data without significantly impacting the application's performance.\n\nThis approach allows you to analyze profiling information, identify slow paths in the code, and gain insights into performance bottlenecks, providing a powerful and efficient way to troubleshoot and optimize the application in a production environment."},{"content":"Selected Answer: D\nD is the answer.","upvote_count":"2","poster":"mshafa","timestamp":"1731403920.0","comment_id":"1068398"},{"timestamp":"1730459340.0","content":"Selected Answer: D\n// appengine is an example of starting cloud.google.com/go/profiler on\n// App Engine.\npackage main\n\nimport (\n \"cloud.google.com/go/profiler\"\n)","upvote_count":"2","poster":"lelele2023","comment_id":"1059626"}]},{"id":"GqDZrNHTgTg8Zxdnp6W5","question_images":[],"question_id":45,"discussion":[{"poster":"cachopo","content":"Selected Answer: A\n- The --logging=SYSTEM option disables workload logging (application logs) while keeping GKE system logs (like node, control plane, and Kubernetes system logs).\n- This helps reduce costs by not storing verbose application logs while still collecting critical GKE operational logs for monitoring.\n- Since developers use kubectl logs, they can still see logs without needing Cloud Logging.","timestamp":"1740584520.0","upvote_count":"1","comment_id":"1362137","comments":[{"content":"Why not the other options?\nB. \n--logging=WORKLOAD disables system logs but keeps workload logs (application logs).\n- This increases costs because verbose application logs continue being collected in Cloud Logging.\n\nC. \n- This completely disables all logging, including GKE system logs, which is not recommended because operational logs are needed for debugging and monitoring.\n\nD. \n- This filters logs based on severity but still keeps some workload logs, not fully disabling application logging.\n- Also, not all logs use severity labels, so some logs may still be ingested, leading to unnecessary costs.","comments":[{"content":"By choosing A, you ensure that GKE system logs are collected while stopping application logs from being stored, minimizing logging costs without impacting developers' ability to use kubectl logs","timestamp":"1740584700.0","upvote_count":"1","comment_id":"1362139","poster":"cachopo"}],"timestamp":"1740584580.0","upvote_count":"1","poster":"cachopo","comment_id":"1362138"}]},{"timestamp":"1718641500.0","content":"Selected Answer: B\nNot sure I agree with any answers. But I disagree the least with A. \n\nA) Would send all the logs, including memory, CPU, and other system information. This is too verbose.\n\nB) WORKLOAD logs should be sufficient I believe\n\nC) Not sure disabling _Default helps here\n\nD) An exclusion filter of \"severity >= DEBUG...\" would literally exclude *all* logs?","comment_id":"1232020","comments":[{"poster":"SahandJ","content":"I just realised I completely misunderstood the question. For some reason I had the notion in my head that the Developers were to migrate to Cloud Logging as well and that not only do we need to minimize cost, but also remove redundant logs (like system logs) in Cloud Logging.\n\nHowever, obviously, the only requirement is to minimize costs for cloud logging. The devs are already reading the logs directly using kubectl. As such we want to remove as much logging as possible from the _Default bucket. Therefore (D) is the correct answer!","timestamp":"1718806920.0","comment_id":"1232889","upvote_count":"1"},{"poster":"SahandJ","upvote_count":"1","comment_id":"1232818","timestamp":"1718791200.0","content":"I meant to say that I disagree the least with *B* as my selected answer"}],"poster":"SahandJ","upvote_count":"1"},{"content":"Selected Answer: A\nA is correct","comment_id":"1175923","upvote_count":"2","poster":"asdasdfczxhjkvz","timestamp":"1710690000.0"},{"poster":"xhilmi","content":"Selected Answer: D\nTo minimize costs associated with application logging in Google Kubernetes Engine (GKE) while still collecting operational logs, the recommended approach is (option D).\n\nAdd the severity >= DEBUG resource.type = \"k8s_container\" exclusion filter to the _Default logging sink in the project associated with the development environment.\n\nThis filter excludes logs with a severity level of DEBUG or lower for the specified resource type, \"k8s_container,\" effectively reducing the volume of verbose application logs being ingested into Cloud Logging.\n\nThis allows you to focus on collecting GKE operational logs while excluding less critical and potentially costly application logs. It helps strike a balance between maintaining visibility into operational aspects and optimizing costs associated with log storage and processing.","comments":[{"content":"If the filter in the exclusion filter is for severity >= DEBUG... would not exclude severity >= DEBUG instead of severity <= DEBUG?","upvote_count":"1","poster":"Luu13","comment_id":"1204394","timestamp":"1714462140.0"}],"upvote_count":"1","comment_id":"1090017","timestamp":"1701933420.0"},{"comments":[{"poster":"lelele2023","upvote_count":"1","timestamp":"1698837600.0","content":"C is invalid since a valid gcloud CLI will be: gcloud logging settings update --folder=FOLDER_ID--disable-default-sink\nhttps://cloud.google.com/logging/docs/default-settings#disable-default-sink","comment_id":"1059636"}],"poster":"lelele2023","timestamp":"1698837240.0","content":"Selected Answer: D\nD The idea is to prevent/minimize container logs from getting sent to the sink,","upvote_count":"3","comment_id":"1059632"},{"comment_id":"1054712","poster":"Jason_Cloud_at","content":"Selected Answer: D\nright answer","upvote_count":"4","timestamp":"1698334500.0"}],"unix_timestamp":1698334500,"timestamp":"2023-10-26 17:35:00","url":"https://www.examtopics.com/discussions/google/view/124674-exam-professional-cloud-devops-engineer-topic-1-question-139/","choices":{"A":"Run the gcloud container clusters update --logging=SYSTEM command for the development cluster.","B":"Run the gcloud container clusters update --logging=WORKLOAD command for the development cluster.","D":"Add the severity >= DEBUG resource.type = \"k8s_container\" exclusion filter to the _Default logging sink in the project associated with the development environment.","C":"Run the gcloud logging sinks update _Default --disabled command in the project associated with the development environment."},"exam_id":6,"answer_ET":"D","question_text":"Your company runs services by using Google Kubernetes Engine (GKE). The GKE dusters in the development environment run applications with verbose logging enabled. Developers view logs by using the kubectl logs command and do not use Cloud Logging. Applications do not have a uniform logging structure defined. You need to minimize the costs associated with application logging while still collecting GKE operational logs. What should you do?","answers_community":["D (67%)","A (25%)","8%"],"topic":"1","answer_description":"","answer":"D","isMC":true,"answer_images":[]}],"exam":{"name":"Professional Cloud DevOps Engineer","isBeta":false,"provider":"Google","isImplemented":true,"id":6,"isMCOnly":true,"lastUpdated":"11 Apr 2025","numberOfQuestions":196},"currentPage":9},"__N_SSP":true}