{"pageProps":{"questions":[{"id":"UKpTQGHQDLnVRr26myzQ","discussion":[{"timestamp":"1729585680.0","content":"Selected Answer: C\nSince it is already given that we will be using a TF-Model and do experiments exclusevly there, I don't see why we wouldn't use TF-Layers to preprocess the data. We would minimize costs by not having to store additional data. Time would be around the same as the layer transforms the attribute during training time and development would also be simpler, since if you are using keras it would literally be 2 more lines of code. However I see the Argument for B as well but I would still go with C in this case.\n\nSpecifically in this case I would use Normalization layer for normalization and Discretization layer for binning/bucketing.","upvote_count":"1","comment_id":"1301473","poster":"cert_pz"},{"timestamp":"1713745440.0","poster":"fitri001","content":"Selected Answer: B\nIn-place Transformation: BigQuery allows you to perform data transformations directly within the data warehouse using SQL queries. This eliminates the need for data movement and reduces processing time compared to other options that involve data transfer.\nMinimized Development Effort: Since you're already familiar with SQL, writing queries for mm-max scaling and bucketing requires minimal additional development effort compared to learning and implementing new frameworks like Spark or Dataflow.\nCost-Effective: BigQuery's serverless architecture scales processing power based on your workload. This can be more cost-effective than managing separate processing clusters like Dataproc.","comment_id":"1199883","upvote_count":"3"},{"timestamp":"1705266780.0","content":"Selected Answer: B\nB - Keeps the preprocessing algorithm seperate from the model","poster":"shadz10","comment_id":"1122857","upvote_count":"2"},{"comment_id":"1121853","poster":"36bdc1e","timestamp":"1705163580.0","content":"C \nThis option allows you to leverage the power and simplicity of TensorFlow to preprocess and transform the data with simple Python code","upvote_count":"2"},{"poster":"BlehMaks","upvote_count":"1","comment_id":"1121133","timestamp":"1705095900.0","content":"Selected Answer: B\nBigQuery can do both transformations https://cloud.google.com/bigquery/docs/manual-preprocessing#numerical_functions"},{"comment_id":"1118764","timestamp":"1704901380.0","upvote_count":"1","content":"Selected Answer: B\nBigQuery (SQL) is the easiest, cheapest approach","poster":"b1a8fae"}],"question_id":86,"unix_timestamp":1704901380,"answer_ET":"B","url":"https://www.examtopics.com/discussions/google/view/130800-exam-professional-machine-learning-engineer-topic-1-question/","topic":"1","isMC":true,"answer_description":"","question_images":[],"answer":"B","choices":{"B":"Write SQL queries to transform the data in-place in BigQuery.","A":"Write the transformations into Spark that uses the spark-bigquery-connector, and use Dataproc to preprocess the data.","D":"Create a Dataflow pipeline that uses the BigQuerylO connector to ingest the data, process it, and write it back to BigQuery.","C":"Add the transformations as a preprocessing layer in the TensorFlow models."},"answers_community":["B (88%)","13%"],"exam_id":13,"question_text":"You work for a food product company. Your company’s historical sales data is stored in BigQuery.You need to use Vertex AI’s custom training service to train multiple TensorFlow models that read the data from BigQuery and predict future sales. You plan to implement a data preprocessing algorithm that performs mm-max scaling and bucketing on a large number of features before you start experimenting with the models. You want to minimize preprocessing time, cost, and development effort. How should you configure this workflow?","answer_images":[],"timestamp":"2024-01-10 16:43:00"},{"id":"qBxlbcvJQeuovQJHIjUt","answers_community":["D (82%)","Other"],"topic":"1","discussion":[{"upvote_count":"1","comment_id":"1323332","poster":"lunalongo","content":"Selected Answer: B\nB) The preprocessing step is already complete and its output is stored is in GCS, so a separate, smaller pipeline just for training is the most efficient solution.\n\n*A) Conditional logic still performs prepocessing steps when the logic points to not skipping it, increasing costs; \nC) While reducing preprocessing time, this solution would increase this step's cost; \nD) Would still include unnecessary preprocessing for each algorithm test before it's cached.","timestamp":"1733625420.0"},{"content":"Selected Answer: D\nCaching Preprocessed Data: Since the preprocessed data (10 TB) is the same for different model training runs, enabling caching allows Vertex AI to reuse it for subsequent pipeline executions. This significantly reduces execution time and cost, especially for large datasets.\nDisabling Model Training Cache: Model training is typically non-deterministic due to factors like random initialization. Caching the model training step could lead to stale models and inaccurate results. Disabling caching ensures the model is re-trained each time with potentially updated code for different algorithms.","timestamp":"1713745800.0","poster":"fitri001","comment_id":"1199886","upvote_count":"1"},{"content":"Selected Answer: D\nagree with guilhermebutzke","upvote_count":"1","poster":"gscharly","timestamp":"1713695880.0","comment_id":"1199604"},{"upvote_count":"3","content":"Selected Answer: D\nAccording to this documentation cited: https://cloud.google.com/vertex-ai/docs/pipelines/configure-caching\n\nit is possible to write a pipeline setting True or False for each task component, like this:\n\n# Model training step with caching disabled\ntrain_model_task = train_model_op()\ntrain_model_task.set_caching_options(False) # Disable caching for this step\n\n# Model training step depends on the preprocessing step\ntrain_model_task.after(preprocess_task)\n\nSo, with this, letter D is the best option. Furthermore, letter A and, Adding a pipeline parameter and an additional pipeline step introduces unnecessary complexity when caching can handle conditional execution efficiently and in letter C, configuring a machine with more CPU and RAM for preprocessing does not address the goal of minimizing pipeline changes and reducing execution time/cost effectively.","timestamp":"1707502680.0","comment_id":"1145686","poster":"guilhermebutzke"},{"timestamp":"1704973440.0","poster":"b1a8fae","content":"Selected Answer: D\nNot A. Adding a pipeline parameter and new pipeline steps does not minimise pipeline changes.\n\nNot C. The idea is not to re-run the preprocessing step at all.\n\nNot B. Creating a whole new pipeline implies a significant investment of effort.\n\nI opt for D: Enabling caching only for preprocessing job (although it says “pipeline job” in the option, I think that is a typo). Quoting Vertex AI docs: “If there is a matching execution in Vertex ML Metadata, the outputs of that execution are used and the step is skipped. This helps to reduce costs by skipping computations that were completed in a previous pipeline run.” https://cloud.google.com/vertex-ai/docs/pipelines/configure-caching","comment_id":"1119654","upvote_count":"4"},{"poster":"pikachu007","content":"Selected Answer: A\nThe pipeline already generates the preprocessed dataset and stores, there's no need to preprocess again for another model","upvote_count":"1","comment_id":"1119602","timestamp":"1704968640.0","comments":[{"poster":"pikachu007","upvote_count":"1","timestamp":"1705111860.0","comment_id":"1121223","content":"rereading the question, I agree with b1a8fae that its D"}]}],"question_id":87,"isMC":true,"timestamp":"2024-01-11 11:24:00","url":"https://www.examtopics.com/discussions/google/view/130839-exam-professional-machine-learning-engineer-topic-1-question/","answer":"D","answer_ET":"D","question_images":[],"answer_images":[],"answer_description":"","exam_id":13,"unix_timestamp":1704968640,"choices":{"A":"Add a pipeline parameter and an additional pipeline step. Depending on the parameter value, the pipeline step conducts or skips data preprocessing, and starts model training.","B":"Create another pipeline without the preprocessing step, and hardcode the preprocessed Cloud Storage file location for model training.","C":"Configure a machine with more CPU and RAM from the compute-optimized machine family for the data preprocessing step.","D":"Enable caching for the pipeline job, and disable caching for the model training step."},"question_text":"You have created a Vertex AI pipeline that includes two steps. The first step preprocesses 10 TB data completes in about 1 hour, and saves the result in a Cloud Storage bucket. The second step uses the processed data to train a model. You need to update the model’s code to allow you to test different algorithms. You want to reduce pipeline execution time and cost while also minimizing pipeline changes. What should you do?"},{"id":"5cG5C7YQvB0rtCCqp1fS","question_text":"You work for a bank. You have created a custom model to predict whether a loan application should be flagged for human review. The input features are stored in a BigQuery table. The model is performing well, and you plan to deploy it to production. Due to compliance requirements the model must provide explanations for each prediction. You want to add this functionality to your model code with minimal effort and provide explanations that are as accurate as possible. What should you do?","isMC":true,"unix_timestamp":1704969000,"discussion":[{"timestamp":"1729557720.0","upvote_count":"2","poster":"fitri001","comment_id":"1199889","content":"Selected Answer: C\nExisting Custom Model: This approach leverages your already-developed, well-performing model. There's no need to rebuild it using AutoML or BigQuery ML, which might require significant code changes.\nVertex Explainable AI (XAI): Vertex AI offers XAI integration with custom models through feature-based attribution methods like sampled Shapley. This provides explanations for each prediction without requiring major modifications to your model code.\nSampled Shapley with Baselines: Sampled Shapley is a robust attribution method for explaining model predictions. Using input baselines (like zero values) helps improve the interpretability of explanations, especially for features with large ranges."},{"content":"Selected Answer: C\nAccording to the documentation at https://cloud.google.com/vertex-ai/docs/explainable-ai/overview, we can utilize both feature-based attribution and sampled Shapley-based explanations. Therefore, for providing explanations for each prediction in a loan classification problem, I believe that feature-based attribution is the optimal approach. Furthermore, updating the custom serving container to include sampled Shapley-based explanations, as suggested in option D, might require more effort, considering that the custom model deployed on Vertex AI already provides this option for explanations.","timestamp":"1723393800.0","comment_id":"1147575","poster":"guilhermebutzke","upvote_count":"3"},{"upvote_count":"2","content":"Selected Answer: C\n\"minimal effort and provide explanations that are as accurate as possible\"\nthis makes the answer C, based on this:\nhttps://cloud.google.com/vertex-ai/docs/explainable-ai/improving-explanations","timestamp":"1722769560.0","poster":"sonicclasps","comment_id":"1140116"},{"timestamp":"1722412380.0","comment_id":"1136611","upvote_count":"3","content":"Selected Answer: C\nFeature attribution is supported for all types of models (both AutoML and custom-trained), frameworks (TensorFlow, scikit, XGBoost), BigQuery ML models, and modalities (images, text, tabular, video).\nhttps://cloud.google.com/vertex-ai/docs/explainable-ai/overview","poster":"daidai75"},{"comment_id":"1121861","content":"C\nyou find the answer here https://cloud.google.com/vertex-ai/docs/explainable-ai/overview","timestamp":"1720881720.0","poster":"36bdc1e","upvote_count":"2"},{"poster":"b1a8fae","comment_id":"1120854","timestamp":"1720788360.0","content":"Selected Answer: D\npikachu007 answer made me reconsider","upvote_count":"1","comments":[{"content":"https://cloud.google.com/vertex-ai/docs/explainable-ai/overview. According to this web link, Feature attribution is supported for all types of models (both AutoML and custom-trained), frameworks (TensorFlow, scikit, XGBoost), BigQuery ML models, and modalities (images, text, tabular, video).","poster":"daidai75","upvote_count":"1","timestamp":"1722412320.0","comment_id":"1136609"}]},{"timestamp":"1720691520.0","poster":"b1a8fae","comment_id":"1119667","upvote_count":"1","content":"Selected Answer: A\nNot a deep neural network for sure (B). Out of the remaining 3, A is the simplest approach."},{"comments":[{"timestamp":"1721594880.0","upvote_count":"1","content":"Why does not C provide an explanation for each prediction? As for me both C and D options provide an explanation for each prediction, the difference is only in the amount of effort required to configure explanations","comment_id":"1128227","poster":"BlehMaks"}],"content":"Selected Answer: D\nA and B is out because you already have a model, C does not provide an explanation for each prediction. Therefore D meets all the criteria.","timestamp":"1720686600.0","comment_id":"1119603","poster":"pikachu007","upvote_count":"1"}],"question_images":[],"timestamp":"2024-01-11 11:30:00","answer_ET":"C","exam_id":13,"answer":"C","answer_images":[],"answer_description":"","choices":{"C":"Upload the custom model to Vertex AI Model Registry and configure feature-based attribution by using sampled Shapley with input baselines.","D":"Update the custom serving container to include sampled Shapley-based explanations in the prediction outputs.","B":"Create a BigQuery ML deep neural network model and use the ML.EXPLAIN_PREDICT method with the num_integral_steps parameter.","A":"Create an AutoML tabular model by using the BigQuery data with integrated Vertex Explainable AI."},"question_id":88,"url":"https://www.examtopics.com/discussions/google/view/130840-exam-professional-machine-learning-engineer-topic-1-question/","topic":"1","answers_community":["C (77%)","D (15%)","8%"]},{"id":"Y9cJiZFVVI9ggU05N1T9","answers_community":["C (57%)","D (21%)","B (21%)"],"topic":"1","discussion":[{"poster":"ddogg","comment_id":"1136625","timestamp":"1706696760.0","upvote_count":"6","content":"Selected Answer: C\nUse the Predictor interface to implement a custom prediction routine. This allows you to include the preprocessing and postprocessing steps in the same deployment package as your model.\n\nBuild the custom container, which packages your model and the associated preprocessing and postprocessing code together, simplifying deployment.\n\nUpload the container to Vertex AI Model Registry. This makes your model available for deployment on Vertex AI.\n\nDeploy it to a Vertex AI endpoint. This allows your model to be used for online serving.\n\nhttps://blog.thecloudside.com/custom-predict-routines-in-vertex-ai-46a7473c95db"},{"content":"Selected Answer: B\nThis approach minimizes code changes and infrastructure maintenance by leveraging Vertex AI's managed services for deployment. Implementing the preprocessing and postprocessing steps in a FastAPI server within a Docker container allows you to handle these steps at serving time efficiently. Deploying this Docker image to a Vertex AI endpoint simplifies the deployment process and reduces the burden of managing the infrastructure.","timestamp":"1719928800.0","poster":"Prakzz","upvote_count":"2","comment_id":"1240830"},{"upvote_count":"1","comment_id":"1234976","content":"Option C is a good choice if\nYou have specific requirements for preprocessing or postprocessing that can't be met by the prebuilt XGBoost serving container.\nYou need more control over the deployment process or want to integrate with other services.\nYou're comfortable building and managing custom containers.\nHowever, if you just want a simple, straightforward way to deploy your model as a RESTful API, Option D (using the XGBoost prebuilt serving container) might be a better fit!","poster":"AzureDP900","timestamp":"1719005100.0"},{"content":"Selected Answer: B\nFastAPI allows to create a lightweight HTTP server with minimal code.","comment_id":"1219384","timestamp":"1716796020.0","upvote_count":"1","poster":"livewalk"},{"content":"Selected Answer: D\nし\n\nPre-built XGBoost container already includes pre- and postprocessing steps.","poster":"Yan_X","upvote_count":"1","comment_id":"1172552","timestamp":"1710334020.0"},{"upvote_count":"2","poster":"guilhermebutzke","comment_id":"1147627","content":"Selected Answer: C\nMy answer C:\n\nConsidering pre- and postprocessing implementation, The option C directly deals with implementing the processing steps in a custom container, offering full control over their placement and execution. \n\nThis documentation says: “Custom prediction routines (CPR) lets you build [custom containers](https://cloud.google.com/vertex-ai/docs/predictions/use-custom-container) with pre/post processing code easily, without dealing with the details of setting up an HTTP server or building a container from scratch.”\nhttps://cloud.google.com/vertex-ai/docs/predictions/custom-prediction-routines\n\nSo, it is better to use C instead of A or B. D is better because it offers the option of pre and post-processing, which is not available in D due to its use of prebuilt serving.","timestamp":"1707681720.0"},{"upvote_count":"4","content":"C\n. Build the custom\ncontainer, upload the container to Vertex AI Model Registry, and deploy it to a Vertex AI endpoint.\nThis option allows you to leverage the power and simplicity of Vertex AI to serve your XGBoost model\nwith minimal effort and customization. Vertex AI is a unified platform for building and deploying\nmachine learning solutions on Google Cloud. Vertex AI can deploy a trained XGBoost model to an online prediction endpoint, which can provide low-latency predictions for individual instances. A custom prediction routine (CPR) is a Python script that defines the logic for preprocessing the input data, running the prediction, and postprocessing the output data.","timestamp":"1705164780.0","comment_id":"1121875","poster":"36bdc1e"},{"content":"Selected Answer: D\nConsidering the goal of minimizing code changes, infrastructure maintenance, and quickly deploying the model into production, option D seems to be a pragmatic approach. It leverages the prebuilt XGBoost serving container in Vertex AI, providing a managed environment for serving. The pre- and postprocessing steps can be implemented in the Golang backend service, maintaining consistency with the existing Golang implementation and reducing the need for significant code changes.","timestamp":"1704969240.0","poster":"pikachu007","comment_id":"1119610","upvote_count":"2"},{"poster":"vale_76_na_xxx","comment_id":"1116920","upvote_count":"1","timestamp":"1704740760.0","content":"I would say D"}],"question_id":89,"isMC":true,"timestamp":"2024-01-08 20:06:00","url":"https://www.examtopics.com/discussions/google/view/130625-exam-professional-machine-learning-engineer-topic-1-question/","answer":"C","answer_ET":"C","question_images":[],"answer_images":[],"answer_description":"","exam_id":13,"unix_timestamp":1704740760,"choices":{"A":"Use FastAPI to implement an HTTP server. Create a Docker image that runs your HTTP server, and deploy it on your organization’s GKE cluster.","C":"Use the Predictor interface to implement a custom prediction routine. Build the custom container, upload the container to Vertex AI Model Registry and deploy it to a Vertex AI endpoint.","D":"Use the XGBoost prebuilt serving container when importing the trained model into Vertex AI. Deploy the model to a Vertex AI endpoint. Work with the backend engineers to implement the pre- and postprocessing steps in the Golang backend service.","B":"Use FastAPI to implement an HTTP server. Create a Docker image that runs your HTTP server, Upload the image to Vertex AI Model Registry and deploy it to a Vertex AI endpoint."},"question_text":"You recently used XGBoost to train a model in Python that will be used for online serving. Your model prediction service will be called by a backend service implemented in Golang running on a Google Kubernetes Engine (GKE) cluster. Your model requires pre and postprocessing steps. You need to implement the processing steps so that they run at serving time. You want to minimize code changes and infrastructure maintenance, and deploy your model into production as quickly as possible. What should you do?"},{"id":"vixzp2pyzWBGQU4EpwAX","exam_id":13,"isMC":true,"answer_description":"","question_text":"You work for a large hotel chain and have been asked to assist the marketing team in gathering predictions for a targeted marketing strategy. You need to make predictions about user lifetime value (LTV) over the next 20 days so that marketing can be adjusted accordingly. The customer dataset is in BigQuery, and you are preparing the tabular data for training with AutoML Tables. This data has a time signal that is spread across multiple columns. How should you ensure that\nAutoML fits the best model to your data?","answer_images":[],"timestamp":"2021-07-01 14:50:00","answers_community":["D (67%)","C (33%)"],"answer_ET":"D","discussion":[{"comment_id":"413785","comments":[{"content":"Also think it is D, since it mentioned that the time signal is spread across multiple columns.","comments":[{"comment_id":"463673","upvote_count":"7","timestamp":"1634501220.0","content":"Correct answer is C - AutoML handles training, validation, test splits automatically for you when you specify a Time column. There is no requirement to do this manually.","poster":"GogoG","comments":[{"upvote_count":"9","comments":[{"upvote_count":"1","comment_id":"528615","content":"this comment is only about time information in different columns, not about time itself. C is correct as for me","comments":[{"comment_id":"528617","poster":"irumata","content":"but if time signal means time mark not the business signal the D is the correct - very controversial","upvote_count":"1","timestamp":"1642699440.0"}],"timestamp":"1642699320.0","poster":"irumata"},{"timestamp":"1645129500.0","upvote_count":"1","content":"was postiing the same link...\nagree with you: D","comment_id":"549695","poster":"lordcenzin"}],"content":"Correct answer is D. It clearly says the time signal data is spread across different columns. If it weren't then C would be correct and your point would be valid. However, in this case the answer is D 100%. \n\nhttps://cloud.google.com/automl-tables/docs/data-best-practices#time","comment_id":"466891","timestamp":"1635066960.0","poster":"george_ognyanov"}]}],"comment_id":"416353","timestamp":"1627493700.0","poster":"sensev","upvote_count":"4"},{"timestamp":"1708945560.0","content":"I think the answer is C. In this case I am interpreting time signal as the features that hold predictive power as a function of time i.e. time signal. There is no indication to how much data is available so using the 30 days after mark is not wise. You only have 30 days worth of data for validation set. If you have a few years worth of data this seems like a unnecessary small validation set.","upvote_count":"4","poster":"Werner123","comment_id":"1159623"}],"upvote_count":"24","poster":"kkd14","content":"Should be D. As time signal that is spread across multiple columns so manual split is required.","timestamp":"1627207380.0"},{"poster":"DucLee3110","content":"C \nYou use the Time column to tell AutoML Tables that time matters for your data; it is not randomly distributed over time. When you specify the Time column, AutoML Tables use the earliest 80% of the rows for training, the next 10% of rows for validation, and the latest 10% of rows for testing.\nAutoML Tables treats each row as an independent and identically distributed training example; setting the Time column does not change this. The Time column is used only to split the data set.\nYou must include a value for the Time column for every row in your dataset. Make sure that the Time column has enough distinct values, so that the evaluation and test sets are non-empty. Usually, having at least 20 distinct values should be sufficient.\nhttps://cloud.google.com/automl-tables/docs/prepare#time","timestamp":"1625201340.0","upvote_count":"14","comments":[{"poster":"salsabilsf","content":"From the link you provided, I think it's A :\n\nThe Time column must have a data type of Timestamp.\n\nDuring schema review, you select this column as the Time column. (In the API, you use the timeColumnSpecId field.) This selection takes effect only if you have not specified the data split column.\n\nIf you have a time-related column that you do not want to use to split your data, set the data type for that column to Timestamp but do not set it as the Time column.","timestamp":"1627913760.0","comment_id":"418757","upvote_count":"2"}],"comment_id":"396564"},{"content":"Selected Answer: C\nC is correct answer","comment_id":"1496888","poster":"shahriar096","timestamp":"1743809940.0","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: D\nD is Correct - A time signal spread across multiple columns in a spreadsheet or data table would typically represent a time-series data where each column corresponds to a specific time point or interval, and the values in each column represent the signal's value at that time","poster":"coupet","comment_id":"1443621","timestamp":"1743735000.0"},{"upvote_count":"1","timestamp":"1733538240.0","poster":"rajshiv","content":"Selected Answer: C\nC is the right answer as manually splitting data data based on time adds unnecessary complexity. AutoML Tables can handle the time-based splits for us automatically when we specify the time column. Option D requires more manual intervention and introduces the risk of making errors in the data splitting process.","comment_id":"1322980"},{"content":"D could work, but I'm still leaning towards C","poster":"Dirtie_Sinkie","timestamp":"1727710440.0","comment_id":"1291601","upvote_count":"1"},{"comment_id":"1260864","poster":"nktyagi","content":"Selected Answer: C\nAutoML handles training, validation, test splits automatically for you when you specify a Time column. There is no requirement to do this manually.","timestamp":"1722819600.0","upvote_count":"1"},{"content":"Selected Answer: D\nD)D is correct, as this would satisfy the days criteria mentioned in the question. 30 days is more than 20 days, and the prediction model can be used on a validation dataset to validate the results for the next 20 days.","comment_id":"1225452","timestamp":"1717674480.0","upvote_count":"2","poster":"PhilipKoku"},{"content":"Selected Answer: D\nthinking that \"spread across multiple columns\" seems like \"columns with redundant information,\" and considering how AutoML can deal with correlated columns, I think option C is the best choice, with no need for a manual split.\n\nHowever, \"time information is not contained in a single column\" is the same thing as \"time signal that is spread across multiple columns.\" I agree that D could be the best option.\n\nThen, I tend to think that D is the best choice because the text could be more clearly expressed in redundant options.","poster":"guilhermebutzke","upvote_count":"3","timestamp":"1705863300.0","comment_id":"1128068"},{"upvote_count":"2","comment_id":"1073465","content":"Selected Answer: C\nEither C or D but leaning towards C as not get the 30 days in D","timestamp":"1700237340.0","poster":"Mickey321"},{"comment_id":"1070481","content":"Selected Answer: D\n\"data has a time signal that is spread across multiple columns\" - I interpret as having > 1 timeseries column.\nAutoML knows how to deal with a single column but not multiple\nhence answer is D","timestamp":"1699972260.0","upvote_count":"2","poster":"Sum_Sum"},{"timestamp":"1699307520.0","comment_id":"1064288","upvote_count":"1","content":"Selected Answer: C\nSince AutoML is good enough to perform the splits, C appears to be the right answer. Moreover, time information across multiple columns which requires manual split as per option D is different from the question's scenario where the time signal is spread across multiple columns which can be hours, months, days, etc. if we can define in AutoML the right time signal column, its enojugh to split the data and pick most recent data as test data and earliest data as test data","poster":"Krish6488"},{"content":"Selected Answer: D\nA Wrong, Even if columns are combines into a 1D-array(column), the time signal should be noticed to autoML anyway. Automatic split cannot work since we need more than 20 days history\nB Wrong, Without indicating time signal to AutoML, data would leak in (time leakage) in training/validation/test sets\nC Wrong, but might be possible if time signal wouldn't have bee spread across multiple columns\nD True, because time signal is spread accross multiple columns require to manually split the data. Since we want to predict LTV over the next 20 days, it is necessary to have at least 20 days history between the splits (30 seems okay: 10 days predictions) Validating and testing on the last 2 months seems reasonable for marketing purpose (usually seasonal).","upvote_count":"2","timestamp":"1692700380.0","poster":"atlas_lyon","comment_id":"987347"},{"content":"Why 30 days after each data sets, even though we need to predict only for 20 days?","timestamp":"1688872980.0","poster":"12112","comment_id":"946863","upvote_count":"1"},{"upvote_count":"1","timestamp":"1688735220.0","poster":"Liting","content":"Selected Answer: D\nAgree with kkd14. D should be the correct answer.","comment_id":"945698"},{"poster":"SamuelTsch","content":"Selected Answer: C\nAs far as I understand, that AutoML table can handle time-signal column full automatically. Thus, I went to C.","upvote_count":"1","timestamp":"1688712060.0","comment_id":"945412"},{"timestamp":"1683608340.0","content":"Selected Answer: D\nWent with D","comment_id":"892694","poster":"M25","upvote_count":"1"},{"upvote_count":"1","comment_id":"851304","poster":"hghdh5454","timestamp":"1679851020.0","content":"C. Submit the data for training without performing any manual transformations, and indicate an appropriate column as the Time column. Allow AutoML to split your data based on the time signal provided, and reserve the more recent data for the validation and testing sets.\n\nThis approach ensures that AutoML can handle the time-based nature of the data properly. By providing the Time column, AutoML can automatically split the data in a way that respects the time-based structure, using more recent data for validation and testing. This approach is especially important for time-series data, as it helps prevent leakage of future information into the training set, ensuring a more accurate and reliable model."},{"poster":"enghabeth","comment_id":"800386","timestamp":"1675729560.0","upvote_count":"1","content":"Selected Answer: D\nhttps://cloud.google.com/automl-tables/docs/data-best-practices#time\n\n- If the time information is not contained in a single column, you can use a manual data split to use the most recent data as the test data, and the earliest data as the training data."},{"content":"Selected Answer: D\nI go with D: https://cloud.google.com/automl-tables/docs/data-best-practices#time \nRead it carefully at the last paragraph of the topic: If the time information is not contained in a single column, you can use a manual data split to use the most recent data as the test data, and the earliest data as the training data.","upvote_count":"1","comment_id":"773617","poster":"John_Pongthorn","timestamp":"1673535840.0"},{"timestamp":"1672059720.0","comment_id":"757425","content":"Answer: D\nhttps://cloud.google.com/automl-tables/docs/data-best-practices#time","poster":"Omi_04040","upvote_count":"1"},{"comment_id":"738469","content":"Selected Answer: D\nD\nhttps://cloud.google.com/automl-tables/docs/data-best-practices","upvote_count":"2","timestamp":"1670456580.0","poster":"hiromi"},{"upvote_count":"1","comment_id":"725667","poster":"EFIGO","content":"Selected Answer: D\nAutomatic splitting is wrong for time-series, you need to split the data in older-newer, so A and B are wrong.\nSince the time info is split in more columns, we can't use the option provided by C for the timestamps, but we need to go with D.","timestamp":"1669279200.0"},{"upvote_count":"1","timestamp":"1661220060.0","content":"D\nTime series - \n- manual split data set when spread across columns.(exp: hour day week year separate) \n- single time column: AutoAI\n- Not many time value: manual\"","comment_id":"650514","poster":"suresh_vn"},{"content":"Selected Answer: D\nCorrect answer is \"D\"","poster":"GCP72","upvote_count":"1","timestamp":"1660567680.0","comment_id":"647193"},{"content":"Selected Answer: C\nAnswer is C","timestamp":"1658602680.0","poster":"rafaelgildin","comment_id":"635713","upvote_count":"1"},{"poster":"datawizard","upvote_count":"2","timestamp":"1655459640.0","comment_id":"617655","content":"Selected Answer: D\nShoud be D, since time signal is in multiple columns. See https://cloud.google.com/automl-tables/docs/data-best-practices#time"},{"content":"Selected Answer: C\nhttps://cloud.google.com/automl-tables/docs/prepare#time","timestamp":"1655064000.0","comment_id":"615481","poster":"Mohamed_Mossad","upvote_count":"1"},{"upvote_count":"3","timestamp":"1651493820.0","content":"Selected Answer: D\nCF this link : https://cloud.google.com/automl-tables/docs/data-best-practices#time\nTime is spread over multiple column thus leading to manual split","poster":"rgrand8","comment_id":"596030"},{"poster":"Shilpika98","timestamp":"1648404000.0","comment_id":"576377","upvote_count":"1","content":"Should be D, for multiple time columns, a manual split is required.\n\"f the time information is not contained in a single column, you can use a manual data split to use the most recent data as the test data, and the earliest data as the training data.\"\nsource: https://cloud.google.com/automl-tables/docs/data-best-practices#time"},{"poster":"giaZ","upvote_count":"1","content":"Looking at link: https://cloud.google.com/automl-tables/docs/data-best-practices#time \nI'm really undecided between C and D. The latter seems the right one but it should say 20 days rather than 30..So, maybe C is the only option.. But, is it ok to set one column as the Time column in AutoML Tables even if the time signal is not contained in a single column?","comment_id":"572013","timestamp":"1647848220.0"},{"comment_id":"569109","timestamp":"1647440520.0","upvote_count":"1","content":"This is D, because you cannot select a time column (C) due to it being split over multiple columns. You cannot have autoML do the split, and concatenating the columns into an array without specifying what the components are is non-sensical.","poster":"baimus"},{"upvote_count":"1","timestamp":"1643001180.0","comment_id":"531065","poster":"A4M","content":"Selected Answer: D\ntime signal, and better data splitting thank option C"},{"poster":"ramen_lover","content":"Selected Answer: C\nYou may have multiple columns that have a data type of Timestamp.\nHowever, you can only choose one \"Time column\".\nThis Time column is used only for splitting the dataset.\nTherefore, you don't need to manually split the dataset even if you have multiple timestamp columns.\nAlso, \"30 days\" should be \"20 days\" in D; AutoML does it for you if you correctly specify \"Time column\".\nhttps://cloud.google.com/automl-tables/docs/prepare#time","comment_id":"495481","timestamp":"1638836760.0","upvote_count":"3"},{"upvote_count":"3","poster":"pddddd","content":"A. you need timestamp column, not array\nB. You need timestamp column, so this wont work - AutoML will not automatically create a timestamp colument\nC. You do not have a timestamp column, so this won't work\nD. Seems the only one viable","timestamp":"1632733200.0","comment_id":"452285"},{"comments":[{"comment_id":"445619","poster":"jk73","upvote_count":"1","content":"sure, we can do that. what is your slack?","timestamp":"1631766420.0"}],"upvote_count":"2","poster":"adydigi","comment_id":"431440","content":"Anyone wants to discuss all the questions in slack?","timestamp":"1629894540.0"},{"content":"it should be C, since Automl tables give you the option of indicate the time column. But in practice, D could work.","comment_id":"395968","poster":"gcp2021go","timestamp":"1625143800.0","upvote_count":"4"}],"topic":"1","url":"https://www.examtopics.com/discussions/google/view/56782-exam-professional-machine-learning-engineer-topic-1-question/","choices":{"D":"Submit the data for training without performing any manual transformations. Use the columns that have a time signal to manually split your data. Ensure that the data in your validation set is from 30 days after the data in your training set and that the data in your testing sets from 30 days after your validation set.","A":"Manually combine all columns that contain a time signal into an array. AIlow AutoML to interpret this array appropriately. Choose an automatic data split across the training, validation, and testing sets.","C":"Submit the data for training without performing any manual transformations, and indicate an appropriate column as the Time column. AIlow AutoML to split your data based on the time signal provided, and reserve the more recent data for the validation and testing sets.","B":"Submit the data for training without performing any manual transformations. AIlow AutoML to handle the appropriate transformations. Choose an automatic data split across the training, validation, and testing sets."},"unix_timestamp":1625143800,"answer":"D","question_images":[],"question_id":90}],"exam":{"id":13,"lastUpdated":"11 Apr 2025","name":"Professional Machine Learning Engineer","isMCOnly":true,"isImplemented":true,"isBeta":false,"numberOfQuestions":304,"provider":"Google"},"currentPage":18},"__N_SSP":true}