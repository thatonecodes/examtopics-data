{"pageProps":{"questions":[{"id":"O60ZGJ8dw1KL30dXDIdG","question_images":[],"question_text":"You are planning to use Cloud Storage as part of your data lake solution. The Cloud Storage bucket will contain objects ingested from external systems. Each object will be ingested once, and the access patterns of individual objects will be random. You want to minimize the cost of storing and retrieving these objects. You want to ensure that any cost optimization efforts are transparent to the users and applications. What should you do?","isMC":true,"unix_timestamp":1704298140,"url":"https://www.examtopics.com/discussions/google/view/130210-exam-professional-data-engineer-topic-1-question-257/","question_id":176,"answer_images":[],"answers_community":["A (100%)"],"answer":"A","answer_ET":"A","exam_id":11,"timestamp":"2024-01-03 17:09:00","choices":{"B":"Create a Cloud Storage bucket with an Object Lifecycle Management policy to transition objects from Standard to Coldline storage class if an object age reaches 30 days.","C":"Create a Cloud Storage bucket with an Object Lifecycle Management policy to transition objects from Standard to Coldline storage class if an object is not live.","A":"Create a Cloud Storage bucket with Autoclass enabled.","D":"Create two Cloud Storage buckets. Use the Standard storage class for the first bucket, and use the Coldline storage class for the second bucket. Migrate objects from the first bucket to the second bucket after 30 days."},"topic":"1","discussion":[{"comment_id":"1410147","content":"Selected Answer: A\nBy chance is this a repeat question?","poster":"desertlotus1211","timestamp":"1742931540.0","upvote_count":"1"},{"timestamp":"1722620160.0","content":"Selected Answer: A\nThanks to you guys, I found out about this feature :D \n\nThe feature was released on November 3, 2023. Note that enabling Autoclass on an existing bucket incurs additional charges.","upvote_count":"2","poster":"iooj","comment_id":"1259971"},{"timestamp":"1708404120.0","comment_id":"1154491","poster":"JyoGCP","content":"Selected Answer: A\nA. Autoclass","upvote_count":"1"},{"upvote_count":"1","poster":"Matt_108","timestamp":"1705156380.0","content":"Selected Answer: A\nOption A","comment_id":"1121740"},{"content":"Selected Answer: A\n- Autoclass automatically analyzes access patterns of objects and automatically transitions them to the most cost-effective storage class within Standard, Nearline, Coldline, or Archive.\n- This eliminates the need for manual intervention or setting specific age thresholds.\n- No user or application interaction is required, ensuring transparency.","poster":"raaad","upvote_count":"4","comment_id":"1114556","timestamp":"1704464460.0"},{"upvote_count":"1","poster":"scaenruy","comment_id":"1112918","content":"Selected Answer: A\nA. Create a Cloud Storage bucket with Autoclass enabled.","timestamp":"1704298140.0"}],"answer_description":""},{"id":"bRJTaYLPzZD24LkSSRg2","exam_id":11,"answer_ET":"B","timestamp":"2024-01-03 17:12:00","question_text":"You have several different file type data sources, such as Apache Parquet and CSV. You want to store the data in Cloud Storage. You need to set up an object sink for your data that allows you to use your own encryption keys. You want to use a GUI-based solution. What should you do?","question_images":[],"url":"https://www.examtopics.com/discussions/google/view/130211-exam-professional-data-engineer-topic-1-question-258/","answer_description":"","unix_timestamp":1704298320,"isMC":true,"answers_community":["B (88%)","13%"],"topic":"1","choices":{"D":"Use BigQuery Data Transfer Service to move files into BigQuery.","B":"Use Cloud Data Fusion to move files into Cloud Storage.","C":"Use Dataflow to move files into Cloud Storage.","A":"Use Storage Transfer Service to move files into Cloud Storage."},"question_id":177,"answer":"B","answer_images":[],"discussion":[{"poster":"raaad","comment_id":"1114557","upvote_count":"10","timestamp":"1704464700.0","comments":[{"timestamp":"1705733700.0","comment_id":"1127098","upvote_count":"4","poster":"AllenChen123","content":"Agree. https://cloud.google.com/data-fusion/docs/how-to/customer-managed-encryption-keys#create-instance"}],"content":"Selected Answer: B\n- Cloud Data Fusion is a fully managed, code-free, GUI-based data integration service that allows you to visually connect, transform, and move data between various sources and sinks. - It supports various file formats and can write to Cloud Storage. \n- You can configure it to use Customer-Managed Encryption Keys (CMEK) for the buckets where it writes data."},{"timestamp":"1705798440.0","poster":"Helinia","upvote_count":"6","comment_id":"1127560","content":"Selected Answer: B\nEven though storage transfer service can be used in GUI, it does not support CMEK which is required in this question. \n\n\"Storage Transfer Service does not encrypt data on your behalf, such as in customer-managed encryption keys (CMEK). We only encrypt data in transit.\"\n\nRef: https://cloud.google.com/storage-transfer/docs/on-prem-security"},{"poster":"vishavpreet","upvote_count":"1","timestamp":"1740651240.0","comment_id":"1362477","content":"Selected Answer: B\nCloud Data Fusion is a fully-managed, cloud native, enterprise data integration service for quickly building and managing data pipelines.\nGraphically, no coding solution."},{"poster":"hussain.sain","content":"Selected Answer: B\nB is answer as requirement is for GUI and sink","upvote_count":"1","timestamp":"1735262400.0","comment_id":"1332161"},{"upvote_count":"1","timestamp":"1728548940.0","poster":"baimus","content":"I acknolwedge that the question wants the answer to be B, so I'm calling it as B. I don't like this though, as can't we just create a bucket with a CMEK upfront and then use the storage transfer service? It would be easier and cheaper, and achieve the same thing.\nThe language of \"sink\" strongly suggests to me they intend this to be B though, as datafusion uses that terminology, and the CMEK thing is probably indicating that datafusion can encrypt for you with CMEK.","comment_id":"1295490"},{"upvote_count":"1","comment_id":"1177686","poster":"hanoverquay","timestamp":"1710878580.0","content":"Selected Answer: B\noption 8"},{"content":"Selected Answer: B\nB just because tranfer service does not support CMEK\nA: \n* GUI + encryption but no CMEK.\nB:\n* Its GUI ETL + support CMEK but not sure why you need an ETL tool for transfering something once? (No scheduling or event-driven trigger is mentioned)","comment_id":"1157919","poster":"ashdam","timestamp":"1708776480.0","upvote_count":"1"},{"upvote_count":"1","poster":"casadocc","comment_id":"1147132","timestamp":"1707646620.0","content":"B if data fusion creates the bucket. We could create the bucket and associate, in this case A is better."},{"content":"Selected Answer: A\nA. Use Storage Transfer Service to move files into Cloud Storage.\n move files into Cloud Storage should be Storage Transfer Service \nCloud Data Fusion is like using a tank to kill an ant","upvote_count":"3","poster":"task_7","comment_id":"1119556","timestamp":"1704965820.0"},{"comment_id":"1112919","poster":"scaenruy","timestamp":"1704298320.0","upvote_count":"1","content":"Selected Answer: B\nB. Use Cloud Data Fusion to move files into Cloud Storage."}]},{"id":"CSgqe51aGVUDEGOiA5Wk","answer_ET":"A","timestamp":"2024-01-03 17:26:00","isMC":true,"answer_images":[],"question_images":[],"discussion":[{"poster":"Sofiia98","upvote_count":"11","comment_id":"1117373","timestamp":"1720512660.0","content":"Selected Answer: A\nIf only all the questions were like this..."},{"comment_id":"1114559","upvote_count":"5","timestamp":"1720182660.0","poster":"raaad","content":"Selected Answer: A\n- Allow business users to perform their analysis in a familiar spreadsheet interface via Connected Sheets."},{"upvote_count":"4","timestamp":"1732043160.0","comment_id":"1213890","poster":"josech","content":"Selected Answer: A\nhttps://cloud.google.com/bigquery/docs/connected-sheets\nhttps://cloud.google.com/dataprep"},{"poster":"hanoverquay","timestamp":"1726768860.0","content":"Selected Answer: A\nvote A","comment_id":"1177685","upvote_count":"1"},{"comment_id":"1121742","timestamp":"1720874100.0","upvote_count":"1","content":"Selected Answer: A\nClearly option A","poster":"Matt_108"},{"upvote_count":"3","poster":"scaenruy","content":"Selected Answer: A\nA. Use Dataprep to clean the data, and write the results to BigQuery. Analyze the data by using Connected Sheets.","comment_id":"1112934","timestamp":"1720016760.0"}],"choices":{"B":"Use Dataprep to clean the data, and write the results to BigQuery. Analyze the data by using Looker Studio.","C":"Use Dataflow to clean the data, and write the results to BigQuery. Analyze the data by using Connected Sheets.","A":"Use Dataprep to clean the data, and write the results to BigQuery. Analyze the data by using Connected Sheets.","D":"Use Dataflow to clean the data, and write the results to BigQuery. Analyze the data by using Looker Studio."},"unix_timestamp":1704299160,"url":"https://www.examtopics.com/discussions/google/view/130212-exam-professional-data-engineer-topic-1-question-259/","question_id":178,"answer":"A","answers_community":["A (100%)"],"exam_id":11,"answer_description":"","topic":"1","question_text":"Your business users need a way to clean and prepare data before using the data for analysis. Your business users are less technically savvy and prefer to work with graphical user interfaces to define their transformations. After the data has been transformed, the business users want to perform their analysis directly in a spreadsheet. You need to recommend a solution that they can use. What should you do?"},{"id":"C9TucfPUcH7bn1jqTfUH","answers_community":["D (78%)","B (22%)"],"answer_description":"","choices":{"B":"Grant the consultant the Cloud Dataflow Developer role on the project.","D":"Create an anonymized sample of the data for the consultant to work with in a different project.","A":"Grant the consultant the Viewer role on the project.","C":"Create a service account and allow the consultant to log on with it."},"topic":"1","answer":"D","answer_ET":"D","unix_timestamp":1583950680,"answer_images":[],"question_text":"You are working on a sensitive project involving private user data. You have set up a project on Google Cloud Platform to house your work internally. An external consultant is going to assist with coding a complex transformation in a Google Cloud Dataflow pipeline for your project. How should you maintain users' privacy?","timestamp":"2020-03-11 19:18:00","isMC":true,"exam_id":11,"url":"https://www.examtopics.com/discussions/google/view/16288-exam-professional-data-engineer-topic-1-question-26/","discussion":[{"comments":[{"timestamp":"1654824900.0","comments":[{"poster":"willymac2","timestamp":"1654825200.0","upvote_count":"2","comment_id":"614291","content":"Sorry I did a wrong copy/paste on the link, I wanted to send:\n\nhttps://cloud.google.com/dataflow/docs/concepts/security-and-permissions#security_and_permissions_for_local_pipelines\n\nhttps://cloud.google.com/dataflow/docs/guides/setting-pipeline-options#LocalExecution"}],"content":"The answer should be D.\nYou do not need any DataFlow permission to implement a pipeline. \nIf needed, you can test using the DirectRunner which runs locally:\n\nttps://cloud.google.com/dataflow/docs/concepts/access-control#example_role_assignment","comment_id":"614288","poster":"willymac2","upvote_count":"3"},{"comment_id":"65170","content":"Remember he's an external consultant. You need to create a service account for him, you can't grant before that... I think C is correct in this case.","upvote_count":"5","comments":[{"poster":"Rajuuu","content":"Service account is between applications and non human entry.","timestamp":"1594325760.0","upvote_count":"36","comments":[{"content":"u can enable a service account as user so that externals can use to login","comment_id":"530735","timestamp":"1642961760.0","upvote_count":"1","poster":"Tanzu"},{"comment_id":"530740","timestamp":"1642962360.0","poster":"Tanzu","content":"u can enable a service account as user so that externals can use to login.\n\nbut the problem is service account is about login. not the minimum resources to do the dataflow related staffs. so C is not enough !. \n\nso the answer should be B. \n\nif the question was about \"doing the 1st thing\", then yeah may be creating a service account could be the 1st thing.","upvote_count":"3"}],"comment_id":"130963"}],"poster":"cleroy","timestamp":"1584445980.0"},{"poster":"VincentMenzel","timestamp":"1690885320.0","upvote_count":"4","content":"Im not sure how you expect the consultant to implement a pipeline without having access to any data that is being processed. Having test data is a prerequisite.","comment_id":"968943"},{"content":"and now? For seeing test data, (D) would be right. And the system tells me (C) is the right answer. What shall I click in the exam?","timestamp":"1681203000.0","comment_id":"867107","poster":"ThorstenStaerk","upvote_count":"5"}],"timestamp":"1583950680.0","content":"The Answer should be B. The Dataflow developer role will not provide access to the underlying data.","comment_id":"62606","upvote_count":"76","poster":"jvg637"},{"timestamp":"1585301460.0","upvote_count":"18","comment_id":"68550","poster":"[Removed]","content":"Answer: B\nDescription: Provides the permissions necessary to execute and manipulate Dataflow jobs."},{"upvote_count":"1","timestamp":"1741089720.0","poster":"Abizi","content":"Selected Answer: D\nlogical answer for me","comment_id":"1364883"},{"content":"Selected Answer: D\nThe answer cannot be B, because B is too retrictive, it can only create and manage dataflow jobs, but cannot view data. I acknowledge that is secure, but no consultant can do the job without seeing representative test data. D is the only one that provides enough to do the job, while still remaining totally private.","comment_id":"1287244","poster":"baimus","timestamp":"1726908780.0","upvote_count":"5"},{"comment_id":"1285978","upvote_count":"2","content":"D cannot be the answer because the question clearly states the developer has to work in your project. Creating another project is not in scope and is a waste of time. Correct answer is B. Developer role has developer rights only, no view rights.","poster":"mouthwash","timestamp":"1726689540.0"},{"timestamp":"1722446400.0","content":"Selected Answer: D\nA. Grant the consultant the Viewer role on the project. \n This role provides read-only access to all resources in the project, which could expose sensitive data to the consultant, violating privacy principles.\n \nB. Grant the consultant the Cloud Dataflow Developer role on the project.\n This role allows the consultant to create and manage Dataflow jobs but does not give them access the underlying data, it is not sufficient, the developer still needs data.\n \n- C. Create a service account and allow the consultant to log on with it.\n Allowing the consultant to log on with a service account could grant them access to sensitive data if the service account has broad permissions. This approach does not address the need to limit data exposure.\n \n- D. Create an anonymized sample of the data for the consultant to work with in a different project.\n- this fits the requrements","poster":"iooj","comment_id":"1259009","upvote_count":"3"},{"timestamp":"1710301740.0","poster":"Shash_88","content":"D\nB is a good option to maintain privacy of sensitive data, but he also need some test data to validate the transformation logic right, so creating sample data and allow him to test in another project seems good.","upvote_count":"1","comment_id":"1172220"},{"content":"Selected Answer: B\nData flow data privacy rules cant allow the developer to see what the data, He/she just designs the pipelines and the flow as the interdependent tasks for the composer","timestamp":"1707701220.0","poster":"hamzad_basha","upvote_count":"1","comment_id":"1147766"},{"comment_id":"1098032","timestamp":"1702719660.0","upvote_count":"3","poster":"MaxNRG","content":"Selected Answer: B\nB as the Dataflow developer role would help provide the third-party consultant access to create and work on the Dataflow pipeline. However, it does not provide access to view the data, thus maintaining user's privacy.\nRefer GCP documentation - Dataflow roles:\nhttps://cloud.google.com/dataflow/docs/concepts/access-control#roles\nOption A is wrong as it would not allow the consultant to work on the pipeline.\nOption C is wrong as the consultant cannot use the service account to login.\nOption D is wrong as it does not enable collaboration."},{"upvote_count":"1","poster":"Jconnor","timestamp":"1701718140.0","content":"C and A will not maintain user's privacy so out. B without data will be enough. D will give a good sample data, maintain privacy and the consultant will help creating the dataflow pipe for the project as requested. so D.","comment_id":"1087900"},{"poster":"axantroff","upvote_count":"1","timestamp":"1700576520.0","content":"Selected Answer: D\nI follow the corresponding logic choosing between B and D:\n\nYes, with the Dataflow Developer role it is possible to execute and manipulate Dataflow jobs, but do we need to execute it? Based on my understanding we only need to ask for help to write it down. Is it possible without having access to test the data? I don't think so. At the same time, we need to perform an anonymization on it. So the answer D is more appropriate for me","comment_id":"1076362"},{"comment_id":"1064171","poster":"rocky48","content":"Selected Answer: D\nBy creating an anonymized sample of the data, you can provide the consultant with a realistic dataset that doesn't contain sensitive or private information. This way, the consultant can work on the project without direct access to sensitive data, reducing privacy risks.\n\nOptions A and B involve granting the consultant access to the project, which may expose sensitive data, even if they have limited permissions.\n\nOption C involves creating a service account, but it doesn't address the need to anonymize the data or provide a separate, safe environment for the consultant to work with.\n\nOption D provides a controlled environment that allows the consultant to work effectively while maintaining data privacy.","upvote_count":"1","timestamp":"1699296780.0"},{"poster":"rtcpost","timestamp":"1697976360.0","content":"Selected Answer: D\nD. Create an anonymized sample of the data for the consultant to work within a different project.\n\nBy creating an anonymized sample of the data, you can provide the consultant with a realistic dataset that doesn't contain sensitive or private information. This way, the consultant can work on the project without direct access to sensitive data, reducing privacy risks.\n\nOptions A and B involve granting the consultant access to the project, which may expose sensitive data, even if they have limited permissions.\n\nOption C involves creating a service account, but it doesn't address the need to anonymize the data or provide a separate, safe environment for the consultant to work with.\n\nOption D provides a controlled environment that allows the consultant to work effectively while maintaining data privacy.","upvote_count":"2","comment_id":"1050533"},{"comment_id":"1027186","upvote_count":"1","content":"D. Creating an anonymized sample of the data for the consultant to work with in a different project is the safest option. This way, the consultant can develop and test the transformation logic without accessing the real, sensitive data.","poster":"imran79","comments":[{"poster":"ruben82","upvote_count":"1","comment_id":"1058661","content":"The question says \" with coding a complex transformation\", so I don't' think that a sample of data is enough. I thiknk that the most suitable way is C, 'cos with a service account you can handle access fine-grained","timestamp":"1698745800.0"}],"timestamp":"1696666260.0"},{"poster":"navioshi","content":"I think C would be correct, as the question says external consultants want to do some work and how we can maintain the 'external consultant' user privacy. Question didn't mention about the company user data or customer information.","timestamp":"1695205860.0","upvote_count":"2","comment_id":"1012206"},{"timestamp":"1693860240.0","comment_id":"998837","content":"Answer: C","poster":"hxy8","upvote_count":"1"},{"timestamp":"1692330900.0","poster":"madhu15","comment_id":"984160","content":"Dataflow Developer \n(roles/dataflow.developer)\n\nProvides the permissions necessary to execute and manipulate Dataflow jobs.","upvote_count":"1"},{"upvote_count":"2","timestamp":"1689178320.0","comment_id":"949984","content":"Unfortunately it's the Service Account answer: \"The developer who creates and examines jobs needs the roles/iam.serviceAccountUser role.\" - https://cloud.google.com/dataflow/docs/concepts/access-control#example","poster":"marek_skopowski"},{"timestamp":"1688913060.0","poster":"itsmynickname","content":"None of the answer convinced me. there is no information about the service account (the granted permissions). Accessing to a project doesnt mean accessing to its data, so it is not necessary to oblige the external consultant to work in another project, still giving sample (anonymized) data to the develop is important. Giving Dataflow permissions is not necessary as it is possible to run the apache beam job locally using DirectRunner (if we keep performance tuning problematic aside).. Having viewer role on the project, hell no!","upvote_count":"2","comment_id":"947331"},{"content":"Selected Answer: D\nThe answer is D. With only an anonymized sample of the data, the consultant can work on the project.","comment_id":"920856","upvote_count":"1","poster":"baht","timestamp":"1686502680.0"},{"comment_id":"886433","upvote_count":"1","content":"Selected Answer: D\nIt should be D","poster":"boca_2022","timestamp":"1682954400.0"},{"upvote_count":"2","content":"D. Create an anonymized sample of the data for the consultant to work with in a different project.\n\nAs the project involves sensitive user data, it is important to protect users' privacy. Granting the consultant the Viewer or Cloud Dataflow Developer role would give them too much access to the data. Creating a service account and allowing the consultant to log on with it would provide access to the data as well.\n\nCreating an anonymized sample of the data for the consultant to work with in a different project would allow them to complete their work without exposing sensitive user data. This way, the consultant can still work on the complex transformation in a controlled environment without putting user data at risk.","comment_id":"879200","timestamp":"1682329800.0","poster":"Oleksandr0501"},{"timestamp":"1681208880.0","upvote_count":"1","comment_id":"867186","content":"Selected Answer: B\nIt should be B","poster":"izekc"},{"timestamp":"1679082900.0","poster":"juliobs","upvote_count":"2","content":"Selected Answer: D\nThe consultant has to test the pipeline with some data. To keep privacy you anonymize it.","comment_id":"842256"},{"comment_id":"835212","timestamp":"1678465020.0","poster":"tibuenoc","content":"Selected Answer: B\ndataflow.developer role enable the developer interacting with the Cloud Dataflow job , with data privacy.","upvote_count":"2"},{"content":"B is the answer , developer role is sufficient to secure data","comment_id":"790282","poster":"NamitSehgal","upvote_count":"3","timestamp":"1674880020.0"},{"comment_id":"787714","poster":"PolyMoe","upvote_count":"2","comments":[{"content":"Can you back this statement up with a reference or source?","comment_id":"1010809","upvote_count":"1","poster":"ckanaar","timestamp":"1695061140.0"}],"timestamp":"1674655800.0","content":"Selected Answer: B\nof course it is answer B. Providing dataflow developer role will not give access to the underlying data !"},{"poster":"desertlotus1211","upvote_count":"2","content":"The external consultant will not access the data. The dataflow pipeline will retrieve the data wherever it's located based in a service account used by dataflow to access the data. So the consultant will never see the data... The consultant is not the one running the job...\n\nSo maybe Answer B is valid...","comment_id":"773894","timestamp":"1673556120.0"},{"timestamp":"1673365320.0","poster":"KGCP25","upvote_count":"3","content":"Selected Answer: B\nExample role assignment\nTo illustrate the utility of the different Dataflow roles, consider the following breakdown:\n\nThe developer who creates and examines jobs needs the roles/iam.serviceAccountUser role.\nFor more sophisticated permissions management, the developer interacting with the Dataflow job needs the roles/dataflow.developer role.\nThey need the roles/storage.objectAdmin or a related role to stage the required files.\nFor debugging and quota checking, they need the project roles/compute.viewer role.\nAbsent other role assignments, this role lets the developer create and cancel Dataflow jobs, but not interact with the individual VMs or access other Cloud services.","comment_id":"771576"},{"comment_id":"749330","timestamp":"1671411720.0","content":"Selected Answer: D\nPrivacy is the key here","poster":"Aamir185","upvote_count":"2"},{"comment_id":"734054","timestamp":"1670013780.0","poster":"Asheesh1909","upvote_count":"7","content":"Selected Answer: D\nI think it should be D, if we give the consultant developer role he can print the data while developing jobs so he can have indirect access to the data so D sounds to be a good option."},{"upvote_count":"1","content":"should be B","comment_id":"728453","poster":"Naazid","timestamp":"1669568220.0"},{"comment_id":"727217","content":"Selected Answer: D\nD make sense to me","poster":"funnybunny11","timestamp":"1669430280.0","upvote_count":"1"},{"comment_id":"720658","upvote_count":"4","content":"Selected Answer: D\nThe question is - \"How should you maintain users' privacy?\" Not what permission external developers need.. So, D.","timestamp":"1668703320.0","poster":"assU2"},{"timestamp":"1667661960.0","upvote_count":"8","poster":"beowulf_kat","comment_id":"711847","content":"Selected Answer: D\nThe consultant will need access to the underlying data and transformed data to see if the transformation logic is coded correctly. This can be best handled by anonymizing the data."},{"content":"Selected Answer: D\nThe key ask here is maintaining the user's privacy. So only way to do is annonymize data.\nDeveloper role as stated in option B will only limit the access for the consultant to use dataflow fully. That does not help to prevent him from sampling the actual dataset where sensitive data is. Also the other point about giving him access to a separate project is quite valid. In an internal project we do not want an external consultant to get access to other parts of GCP. So my answer is D.","timestamp":"1662793260.0","upvote_count":"7","poster":"TOXICcharlie","comment_id":"665185"},{"upvote_count":"1","poster":"ducc","timestamp":"1661233920.0","comment_id":"650606","content":"Selected Answer: D\nD is correct\nB is not Correct because the Consultant need to test the pipeline"},{"comment_id":"637636","timestamp":"1658871000.0","content":"the answer should be C. The Dataflow developer role will not provide access to the underlying data.\nhttps://cloud.google.com/dataflow/docs/concepts/access-control#example_role_assignment","upvote_count":"2","poster":"aaristizabal"},{"timestamp":"1655323980.0","content":"Selected Answer: D\nAnswer: D\n\nIt is possible to code an Apache Beam Pipeline without permission on GCP (ex: Dataflow developer to run the pipeline on cloud), but the external consultant needs access to the data to test his pipeline. The problem is with cloud access and sensitive information, that is resolved in D","comment_id":"616942","poster":"noob_master","upvote_count":"2"},{"comments":[{"upvote_count":"3","poster":"tavva_prudhvi","content":"Wow, could be a better idea if you could provide some explanation.","comment_id":"615673","timestamp":"1655102520.0"}],"comment_id":"615446","upvote_count":"1","content":"Selected Answer: D\nAnswer is D","poster":"nexus1_","timestamp":"1655055360.0"},{"timestamp":"1654825140.0","poster":"willymac2","content":"The answer should be D.\nWe are asking the consultant to implement the code, not to run it in the real infrastructure.\nBy providing a sample anonymised dataset, they will be able to do the work. They don't even meed any DataFlow permission in the new project, as they can use a DirectRunner to run a pipeline locally: \n\nhttps://cloud.google.com/dataflow/docs/concepts/security-and-permissions#security_and_permissions_for_local_pipelines\n\nhttps://cloud.google.com/dataflow/docs/guides/setting-pipeline-options#LocalExecution","upvote_count":"3","comment_id":"614290"},{"content":"Answer should be B.","poster":"Subhranil1010","upvote_count":"1","comment_id":"610712","timestamp":"1654189680.0"},{"content":"Selected Answer: D\nThe question explicitly asks for \"How should you maintain users' privacy?\n\", not how to give the consultant access to the data, hence D","comment_id":"609520","poster":"hevictor","upvote_count":"3","timestamp":"1653963960.0"},{"content":"Selected Answer: D\nThe answer is D.\n\nThe other options lets the external consultor access to the (not anonymized) data with personal details.\n\nOption D is a common procedure to protect any kind of private information to 3rd parties.","poster":"alecuba16","comment_id":"588594","upvote_count":"6","timestamp":"1650454080.0","comments":[{"upvote_count":"1","comments":[{"timestamp":"1654824660.0","comment_id":"614286","upvote_count":"3","content":"If the sample and anonymised data is small enough, you can use a DirectRunner to run your pipeline locally\n\nhttps://cloud.google.com/dataflow/docs/concepts/security-and-permissions#security_and_permissions_for_local_pipelines\n\nhttps://cloud.google.com/dataflow/docs/guides/setting-pipeline-options#LocalExecution","poster":"willymac2"}],"content":"How would he execute the query if he has no developer permissions?","poster":"tavva_prudhvi","comment_id":"594453","timestamp":"1651235580.0"}]},{"content":"D\nsensitive data = security","timestamp":"1649931000.0","upvote_count":"2","poster":"ip_7","comment_id":"585701"},{"comments":[{"poster":"tavva_prudhvi","content":"and you want him to assist him the complex transformation? So, how could he help you with the right logic if doesn't had the right data?","timestamp":"1651235700.0","upvote_count":"1","comment_id":"594455"}],"timestamp":"1649661120.0","comment_id":"584086","poster":"GLEM","upvote_count":"3","content":"Selected Answer: D\n\"sensitive project involving private user data\".\nAnswer should be D, we don't want the external user access any real data."},{"timestamp":"1648823220.0","poster":"VivekA11","comment_id":"579564","content":"Selected Answer: B\nAs external consultant just going to assist with coding, it means he is not going to test pipeline himself most likely internal developer will perform this task(as project has private data) thus consultant does not need data access. B seems most appropriate option here as it will only allow consultant to verify logic or flow of the pipeline.","upvote_count":"2"},{"poster":"anji007","comment_id":"569113","content":"Selected Answer: D\nSee this: https://stackoverflow.com/questions/71436000/question-about-permissions-on-google-cloud-dataflow","upvote_count":"1","comments":[{"content":"Yes, you are right, but still to run the job you need a developer role.","timestamp":"1649226840.0","upvote_count":"1","comments":[{"upvote_count":"1","comment_id":"592910","comments":[{"content":"Assisting in coding the complex pipeline!!!, if you wanna code, then even if you have the anonymous data, can you run the pipeline?","poster":"tavva_prudhvi","timestamp":"1651669200.0","comment_id":"596820","upvote_count":"1"}],"content":"The consultant is only assisting, so he/she will not need to run the job","timestamp":"1651041240.0","poster":"stefanop"},{"poster":"willymac2","timestamp":"1654824720.0","upvote_count":"1","content":"You can still run a job using a DirectRunner which does not require DataFlow permissions as it runs locally:\n\nhttps://cloud.google.com/dataflow/docs/concepts/security-and-permissions#security_and_permissions_for_local_pipelines\n\nhttps://cloud.google.com/dataflow/docs/guides/setting-pipeline-options#LocalExecution","comment_id":"614287"}],"comment_id":"581632","poster":"tavva_prudhvi"}],"timestamp":"1647440820.0"},{"timestamp":"1645531140.0","upvote_count":"5","content":"With B, the user can possibly print the data and view it from logs. \nD seems to be the more sensible choice.","poster":"Munmaya","comments":[{"timestamp":"1645715640.0","poster":"szore","content":"totally agree","comment_id":"555319","upvote_count":"1"},{"comments":[{"content":"It's a different project which is just created for completing this coding task so implicit of having such role to run the dataflow pipeline, or you've DirectRunner anyways. In case of B, there is no provision of data to test complex transformation","timestamp":"1672295400.0","upvote_count":"1","comment_id":"760662","poster":"Wonka87"}],"upvote_count":"2","timestamp":"1649227380.0","poster":"tavva_prudhvi","comment_id":"581639","content":"So, in D you will assign a different project with sample data, but still you will need a Developer role to run the dataflow pipeline, right? If you just see the official documentations, developer doesn't help you to see the data."}],"comment_id":"553643"},{"content":"Selected Answer: D\nhttps://policies.google.com/technologies/anonymization?hl=en-US","upvote_count":"4","comment_id":"536587","poster":"sraakesh95","timestamp":"1643590560.0"},{"upvote_count":"1","timestamp":"1642254900.0","poster":"deep_ROOT","content":"Selected Answer: B\nDataflow Developer\n(roles/dataflow.developer)\nProvides the permissions necessary to execute and manipulate Dataflow jobs.\nPermissions:\ncompute.projects.get\ncompute.regions.list\ncompute.zones.list\ndataflow.jobs.*\ndataflow.messages.*\ndataflow.metrics.*\ndataflow.snapshots.*\nresourcemanager.projects.get\nresourcemanager.projects.list","comment_id":"524205"},{"poster":"sraakesh95","content":"Selected Answer: D\nD secures user data by not providing all of it and just enough to perform transforms for verification. As the transformations are complex, a part of the data to be viewed is necessary","comments":[{"comments":[{"upvote_count":"1","content":"https://policies.google.com/technologies/anonymization?hl=en-US\nThis concept is specifically designed by Google for such use-cases. But, as the complexity factor comes into the picture, am doubtful, but, the most efficient answer seems to be this","poster":"sraakesh95","comment_id":"536589","timestamp":"1643590680.0"}],"content":"the misunderstanding here is the external consultant does not have to transform the user data wherever resides (may be in GCS or somewhere else). \n\nthe second part is, transformations are complex, so u can not provide sample data to do the job. that is simple just not possible to put a sampling strategy ...","comment_id":"530744","poster":"Tanzu","upvote_count":"1","timestamp":"1642962960.0"}],"timestamp":"1642106580.0","comment_id":"523113","upvote_count":"3"},{"poster":"santoshindia","content":"Selected Answer: D\nOption D sounds more practical and secure a private user data to me as user data is anomalize rest of the options are secondary","timestamp":"1641851880.0","comment_id":"521160","upvote_count":"1"},{"content":"Selected Answer: B\nIt is B. \nhttps://cloud.google.com/dataflow/docs/concepts/access-control#example_role_assignment","poster":"medeis_jar","comment_id":"516528","timestamp":"1641296220.0","upvote_count":"2"},{"timestamp":"1639878120.0","upvote_count":"2","comment_id":"504570","content":"Selected Answer: D\nD - should be the answer","poster":"aet2123"},{"timestamp":"1639582740.0","comment_id":"502289","poster":"freeman1715","upvote_count":"2","content":"C https://cloud.google.com/dataflow/docs/concepts/security-and-permissions#service_account"},{"timestamp":"1639148700.0","upvote_count":"1","poster":"pr2web","comment_id":"498757","content":"Selected Answer: B\nProvides the permissions necessary to execute and manipulate Dataflow jobs."},{"poster":"StefanoG","timestamp":"1637922720.0","content":"Selected Answer: B\nDescription: Provides the permissions necessary to execute and manipulate Dataflow jobs.","comment_id":"487255","upvote_count":"2"},{"timestamp":"1634086020.0","comment_id":"461313","upvote_count":"4","poster":"Chelseajcole","content":"It is B. Here is the link:\nhttps://cloud.google.com/dataflow/docs/concepts/access-control#example_role_assignment"},{"comment_id":"461152","poster":"anji007","comments":[{"content":"updated: https://stackoverflow.com/questions/71436000/question-about-permissions-on-google-cloud-dataflow","upvote_count":"1","comment_id":"569112","timestamp":"1647440760.0","poster":"anji007"}],"content":"Ans: B","timestamp":"1634054520.0","upvote_count":"1"},{"poster":"Anirkent","content":"Not sure who it could be anything apart from D. if I put myself in developer shoes then without seeing the data how can I develop any logic let alone the complex one. and if I have access thru any means (ex. service account) then I can just print the logs and see the data in the logs anyway. So option D appears to be the only option.","upvote_count":"8","timestamp":"1633515300.0","comment_id":"458175"},{"upvote_count":"2","poster":"Vandy10","timestamp":"1633444740.0","comment_id":"457762","content":"going with D"},{"upvote_count":"4","poster":"squishy_fishy","content":"The answer is D.","timestamp":"1633216020.0","comment_id":"456275"},{"comments":[{"content":"data cannot be accessed with option B, but how can you define complex transformation with out seeing data...","upvote_count":"1","poster":"msaqib934","timestamp":"1650700980.0","comment_id":"590465"}],"timestamp":"1632807120.0","comment_id":"453055","upvote_count":"1","poster":"retep007","content":"I'd go for B, Dataflow developer doesn't grant access to any data on it's own"},{"comment_id":"447078","content":"I would prefer D, more secure way and able to grant full access to enable development can test out fully. The impact of leaking out the data is more costly than create another DEV project for additional cost.","comments":[{"comment_id":"454735","timestamp":"1632981540.0","poster":"merkur","content":"I agree. It's a bit difficult, to build a complex transformation without accessing the data itself. But the person, who created the question don't have much corporate experience :).","upvote_count":"5"}],"poster":"NUSTemple","timestamp":"1631969340.0","upvote_count":"6"},{"timestamp":"1631793540.0","poster":"stenio123","upvote_count":"2","comment_id":"445908","content":"Yes it is B. From https://cloud.google.com/dataflow/docs/concepts/access-control\n\"The developer who creates and examines jobs needs the roles/dataflow.admin role.\nFor more sophisticated permissions management, the developer interacting with the Dataflow job needs the roles/dataflow.developer role.\nThey need the roles/storage.objectAdmin or a related role to stage the required files.\nFor debugging and quota checking, they need the project roles/compute.viewer role.\nAbsent other role assignments, this role lets the developer create and cancel Dataflow jobs, but not interact with the individual VMs or access other Cloud services.\nThe worker service account needs the roles/dataflow.worker role to process data for the Dataflow service. To access job data, the service account needs other roles such as roles/storage.objectAdmin.\""},{"comment_id":"421156","poster":"sandipk91","content":"Option D sounds more practical and secure to me as user data is anomalized","upvote_count":"7","timestamp":"1628333220.0"},{"comments":[{"upvote_count":"1","poster":"sumanshu","content":"Then, Complex Pipeline need to built again in main project (unnecessary overhead)","comment_id":"401834","timestamp":"1625742420.0"},{"comments":[{"poster":"squishy_fishy","content":"I would say masked data at the lower environment which is also another project.","upvote_count":"1","timestamp":"1633194180.0","comment_id":"456176"}],"timestamp":"1633194120.0","upvote_count":"1","poster":"squishy_fishy","content":"Exactly, the developer needs to do the developer testing and data verifications. So the developer has to have access to the data stored in cloud storage.","comment_id":"456175"}],"upvote_count":"3","content":"I would pick D. Because…\n1. The external consultant will have see the private data if he has access to the Dataflow as a DEVELOPER - he will need to test the transformations \n2. If he’s in a different project using synthetic data, then he develops the DataFlow transformations away from the private data","poster":"gcp_learner","timestamp":"1625110200.0","comment_id":"395436"},{"content":"Vote for 'B', Developer ROLE is sufficient, it will not provide the access for the underline storage resources","comment_id":"391360","comments":[{"timestamp":"1650701100.0","poster":"msaqib934","content":"its good, but how can the consultant perform transformation on data without seeing it.","upvote_count":"1","comment_id":"590467"}],"timestamp":"1624721700.0","poster":"sumanshu","upvote_count":"2"},{"upvote_count":"1","content":"B - Dataflow Developer role ...we need to avoid any elevated privileges that can expose private user data .","comment_id":"351185","timestamp":"1620318000.0","poster":"userd83"},{"poster":"trannammai","content":"The answer is B. Developer role does not allow to view data","timestamp":"1617198120.0","comment_id":"325123","comments":[{"poster":"pddddd","timestamp":"1639086000.0","comment_id":"498047","upvote_count":"1","content":"and guys cannot do his job..."}],"upvote_count":"2"},{"comment_id":"309111","timestamp":"1615578420.0","poster":"BhupiSG","content":"I would chose D.","upvote_count":"6"},{"comment_id":"306358","comments":[{"content":"service account is not assigned to a person :\nhttps://cloud.google.com/iam/docs/service-accounts","comment_id":"384720","poster":"YAS007","timestamp":"1624009680.0","upvote_count":"2"}],"timestamp":"1615291740.0","upvote_count":"2","content":"answer C:\nB alone is not enough to protect the privacy:\n\"The developer who creates and examines jobs needs the roles/dataflow.admin role.\nFor more sophisticated permissions management, the developer interacting with the Dataflow job needs the roles/dataflow.developer role.\nThey need the roles/storage.objectAdmin or a related role to stage the required files.\nFor debugging and quota checking, they need the project roles/compute.viewer role.\"\n\nhttps://cloud.google.com/dataflow/docs/concepts/access-control#example_role_assignment","poster":"daghayeghi"},{"content":"Correct C","timestamp":"1612634100.0","comments":[{"upvote_count":"1","comment_id":"401838","content":"Only applications can use a service account, (Not users)...So C is wrong...\nand also in Service account - nothing mention related to access","poster":"sumanshu","timestamp":"1625742480.0"}],"upvote_count":"2","poster":"naga","comment_id":"285004"},{"timestamp":"1612341120.0","content":"Service Account is b/w gcp components and not with human. Hence option B is right","upvote_count":"1","poster":"someshsehgal","comment_id":"282566"},{"poster":"NamitSehgal","content":"Surely Answer is B\nService account is used for services, not handed over to humans.","timestamp":"1607931120.0","upvote_count":"3","comment_id":"243357"},{"upvote_count":"7","comments":[{"poster":"sumanshu","timestamp":"1624721580.0","content":"More Cost - to put data in some other project\nMore overhead of management - to annoyize the data, and to create pproject and set up first in newly project and then in original project.\n\nI don't think - Google recommend this. Definelty it can be done by ROLES","comment_id":"391359","upvote_count":"1"}],"content":"I chose D.\n\nI wouldn't want to be a consultant who only gets the permissions in B:\n*This would not allow you to run the pipeline locally,\n*This would not allow you to verify that the data you're producing is in any way correct.\nSecurity-wise, you're also not helping yourself much by choosing B, as the consultant could use the permissions granted to the workers he's controlling to direct the data to a place where he can read it. An anonymized data set put in a separate project solves all of these issues. \n\nTo reinforce the last point: GCP has recently recognized this by requiring you to to be able to impersonate the controller service account (compute engine default service account by default) if you want to be able to submit jobs. Dataflow Developer alone just won't allow the consultant to run jobs at all.","timestamp":"1607028840.0","comment_id":"234358","poster":"IKGx1iGetOWGSjAQDD2x3"},{"timestamp":"1605507420.0","content":"The correct answer is B. Service account is at a higher level and is to manipulate resources on your behalf. You can use Identity and Access Management (IAM) roles for Dataflo. Below are the roles\n\nroles/dataflow.admin\nroles/dataflow.developer\nroles/dataflow.viewer\nroles/dataflow.worker\n\nData flow developer provides the permissions necessary to execute and manipulate Dataflow jobs. \ndataflow.*\nresourcemanager.projects.get\nresourcemanager.projects.list\n\nThere is no data access here which you can by dataflow admin and dataflow worker.","comments":[{"content":"agree.\n\nhttps://cloud.google.com/dataflow/docs/concepts/access-control\n\nthe developer interacting with the Dataflow job needs the roles/dataflow.developer role.\nThey need the roles/storage.objectAdmin or a related role to stage the required files.\nAbsent other role assignments, this role allows the developer to create and cancel Dataflow jobs, but not interact with the individual VMs or access other Cloud services.\n...To access job data, the service accounts needs other roles such as roles/storage.objectAdmin.","poster":"moonlightbeamer","comment_id":"386610","upvote_count":"2","timestamp":"1624230540.0"}],"poster":"Radhika7983","comment_id":"220096","upvote_count":"6"},{"upvote_count":"1","comment_id":"219835","comments":[{"comment_id":"219839","content":"B alone is not enough to protect the privacy:\n\"The developer who creates and examines jobs needs the roles/dataflow.admin role.\nFor more sophisticated permissions management, the developer interacting with the Dataflow job needs the roles/dataflow.developer role.\nThey need the roles/storage.objectAdmin or a related role to stage the required files.\nFor debugging and quota checking, they need the project roles/compute.viewer role.\n\"\nhttps://cloud.google.com/dataflow/docs/concepts/access-control#example_role_assignment","timestamp":"1605461580.0","poster":"xfoursea","upvote_count":"2"}],"timestamp":"1605461280.0","content":"to those who picked B: when develop pipeline, do not we need to constantly verify each transformation step by looking at the data before and after transform?","poster":"xfoursea"},{"timestamp":"1604244420.0","comment_id":"210562","upvote_count":"1","poster":"arghya13","content":"I think it is B.."},{"upvote_count":"1","content":"Sorry it should be B. These are the following reasons \n1. Without Developer role the developer will not be able to execute the pipeline\n2. Developer will not be able to see the data which is processing through the Pipeline","comment_id":"194857","timestamp":"1602046380.0","poster":"Tanmoyk"},{"upvote_count":"1","comment_id":"188094","content":"Should be D. The role of external consultant is to build a data pipeline and data should not be exposed to the outsider. Secondly to Build a pipeline it is a development env. which should be separated from the main project","timestamp":"1601178720.0","poster":"Tanmoyk"},{"upvote_count":"1","timestamp":"1600334820.0","content":"Answer should be D. Two reasons the external consultant is a developer who will need access to the data for development and testing of the pipeline with complex transformation and secondly, this addresses the concern of the data privacy as the data is annonymized.","comment_id":"180801","poster":"SteelWarrior"},{"comment_id":"165282","content":"Answer B\nFrom a LinuxAcademy mock exam:\n\"With the Developer IAM role, the developer will be able to create and cancel Dataflow jobs. Without other Google Cloud IAM roles, they will not be able to view the data that will be going through the pipeline\"","poster":"Koja","timestamp":"1598279400.0","upvote_count":"5"},{"content":"B\nthis dude comes for coding. So s/he got, service account either way. To control on service account, the service account needs other roles. \nGoogle comment \nThe controller service account needs the roles/dataflow.worker role to process data for the Dataflow service. To access job data, the service accounts needs other roles such as roles/storage.objectAdmin.\nRef: https://cloud.google.com/dataflow/docs/concepts/access-control","timestamp":"1597878480.0","upvote_count":"1","poster":"atnafu2020","comment_id":"161837"},{"timestamp":"1597688220.0","upvote_count":"1","content":"If you have anonymised sample data, then best is to use different project than providing access to existing project..so i will go with D option.","poster":"saurabh1805","comment_id":"160268"},{"upvote_count":"1","poster":"amityleo","comment_id":"151463","content":"Should be B\n\nThe purpose of creating the service account is the communication between the services.","timestamp":"1596665820.0"},{"content":"B should be the right answer. As this role is ment for someone who is accessing only pipeline but not input and output source data.","upvote_count":"3","poster":"bhavesh_wadhwani","comment_id":"138621","timestamp":"1595157420.0"},{"timestamp":"1594590600.0","poster":"tprashanth","content":"B.\nGoal is to protect user's data from accessing and allow the consultant to create the complex program. Answer B will not give access to the underlying data but provides the permissions necessary to execute and manipulate Dataflow jobs.","upvote_count":"2","comment_id":"133272"},{"upvote_count":"2","timestamp":"1594029480.0","comment_id":"127641","poster":"kaush","content":"B is too risky given its has all the following permissions \ndataflow.*\nresourcemanager.projects.get\nresourcemanager.projects.list\nBest is to provide a sample of data which is D"},{"poster":"dg63","timestamp":"1593864180.0","comment_id":"126131","content":"It should be \"D\". With option B developer will be able to log the data and view it.\nService account is to be used by an application or service and is not used for interactive login.","upvote_count":"3"},{"content":"I think it should be D. Dataflow developer can peek the data by print for example","timestamp":"1593593100.0","upvote_count":"4","poster":"norwayping","comment_id":"124099"},{"content":"Why not D?\nit solves the user privacy issue which is a key requirement","poster":"yasoy","comment_id":"121721","timestamp":"1593331140.0","upvote_count":"3"},{"content":"B; Provides the permissions necessary to execute and manipulate Dataflow jobs; no machine type/storage bucket access.","timestamp":"1587219900.0","poster":"itche_scratche","comment_id":"76097","upvote_count":"1"},{"content":"Answer B\nA service account is a special type of Google account intended to represent a non-human user that needs to authenticate and be authorized to access data in Google APIs.\nhttps://cloud.google.com/iam/docs/understanding-service-accounts","timestamp":"1586639700.0","upvote_count":"1","poster":"anton_royce","comment_id":"73454"},{"comment_id":"65943","poster":"[Removed]","timestamp":"1584611340.0","content":"Should be B","upvote_count":"4"}],"question_images":[],"question_id":179},{"id":"Wew6nysRCglnfVXE7aPl","discussion":[{"comment_id":"1114569","timestamp":"1704465480.0","poster":"raaad","comments":[{"upvote_count":"2","content":"Critical jobs can spike up to 800 slots, making option B wrong","comments":[{"comments":[{"timestamp":"1711544580.0","content":"I dont think thats correct. \"up to 500 slots\" means the maximum limit is 500 slots - it doesnt specify autoscaling an additional 500 slots.","upvote_count":"1","comment_id":"1184109","poster":"chrissamharris"}],"upvote_count":"2","timestamp":"1710961140.0","content":"in this context, \"enable autoscaling up to 500 slots\" means that the system can add up to 500 slots beyond the baseline","poster":"barrru","comment_id":"1178622"}],"timestamp":"1710295200.0","comment_id":"1172193","poster":"ce9e395"}],"upvote_count":"11","content":"Selected Answer: B\n- The SLA project gets a dedicated reservation with autoscaling to handle spikes, ensuring it meets its strict completion time SLAs. \n- The ad-hoc project uses on-demand billing, which means it will be billed based on the amount of data scanned rather than slot capacity, fitting the billing preference for ad-hoc queries."},{"comment_id":"1154546","poster":"JyoGCP","upvote_count":"5","timestamp":"1708414800.0","content":"Selected Answer: B\nOption B.\n\nNot D because \"In Project-2, ad-hoc queries need to be billed based on how much data users scan rather than by slot capacity.\""},{"poster":"Abizi","upvote_count":"1","timestamp":"1743151620.0","content":"Selected Answer: B\nanswer B, because of the pay as you go for the Adhoc projet","comment_id":"1411226"},{"poster":"ToiToi","upvote_count":"2","timestamp":"1730400180.0","content":"Selected Answer: B\n100% B \nI work with BQ on a daily basis, did the transition from falt rate to editions last year, have been configuring this for so many customers. \nBilling for data analysed rather than slots is on demand - so no other option other than B makes sense.","comment_id":"1305555"},{"comment_id":"1305154","content":"Selected Answer: B\nhttps://cloud.google.com/bigquery/docs/slots-autoscaling-intro#using_reservations_with_baseline_and_autoscaling_slots says clearly, \"Autoscaling slots are only added after all of the baseline slots (and idle slots if applicable) are consumed.\"","timestamp":"1730318640.0","upvote_count":"2","poster":"SamuelTsch"},{"comment_id":"1191302","content":"Selected Answer: B\nSeparate Reservations: This approach provides tailored resource allocation and billing models to match the distinct needs of each project.\nSLA Project Reservation:\nEnterprise Edition: Guarantees consistent slot availability for your production jobs.\nBaseline of 300 slots: Ensures resources are always available to meet your core usage at a predictable cost.\nAutoscaling up to 500 slots: Accommodates bursts in workload while controlling costs.\nAd-hoc Project On-demand:\nOn-demand billing: Charges based on data scanned, ideal for unpredictable and variable query patterns by your ad-hoc users.","upvote_count":"3","timestamp":"1712542800.0","poster":"CGS22"},{"upvote_count":"1","poster":"chrissamharris","comments":[{"timestamp":"1730400720.0","content":"\"You want these ad-hoc queries to be billed based on how much data users scan rather than by slot capacity.\" this is the only thing you need to read out of the question. \nHaving spikes of 500 slots does not mean you should set a baseline of 800, it is WAY too expensive to do that for spikes.","comment_id":"1305559","upvote_count":"1","poster":"ToiToi"},{"poster":"chrissamharris","timestamp":"1711549620.0","content":"Scratch this - Option B: https://cloud.google.com/bigquery/docs/slots-autoscaling-intro#using_reservations_with_baseline_and_autoscaling_slots \nBaseline Slots and AutoScaling Slots are treated as two different entities in the documentation. Therefore B is right despite the horrific wording of the answers.","comment_id":"1184172","upvote_count":"3"}],"content":"Selected Answer: D\nNote, Option A states autoscale \"up to\" (not an additional) 500 slots, whereas the requirement is 800 slots. Making option D the only viable option.","comment_id":"1184111","timestamp":"1711544640.0"},{"timestamp":"1711277940.0","upvote_count":"3","comment_id":"1181467","content":"\"You want these ad-hoc queries to be billed based on how much data users scan rather than by slot capacity.\" So D is out. Choose B","poster":"potatoKiller"},{"poster":"hanoverquay","upvote_count":"1","timestamp":"1710878340.0","content":"Selected Answer: D\n500 (additional) +300 = 800, so answer is D","comment_id":"1177683"},{"comment_id":"1126324","timestamp":"1705630380.0","upvote_count":"1","poster":"danisxp","content":"Selected Answer: D\nConsidering the emphasis on strict completion time SLA's.I go with option D. However I think both B and D are not the best solution here."},{"poster":"Matt_108","comment_id":"1121748","timestamp":"1705156740.0","upvote_count":"3","content":"Selected Answer: B\nOption B - first project works well with dedicated reservation and autoscaling. The second one requires on demand billing, as per question requires."},{"comment_id":"1119023","timestamp":"1704919800.0","content":"Selected Answer: D\n\"These jobs generally never go below a 300 slot utilization, but occasionally spike up an additional 500 slots.\" -> if it spikes up an ADITIONAL 500 slots, on top of the regular 300, shouldn't we reserve at a minimum 800? open to explanations as to why this is not the case.","poster":"ElenaL","upvote_count":"3"},{"poster":"scaenruy","content":"Selected Answer: B\nB. Create two reservations, one for each of the projects. For the SLA project, use an Enterprise Edition with a baseline of 300 slots and enable autoscaling up to 500 slots. For the ad-hoc project, configure on-demand billing.","comment_id":"1112935","timestamp":"1704299340.0","upvote_count":"3"}],"question_text":"You have two projects where you run BigQuery jobs:\n• One project runs production jobs that have strict completion time SLAs. These are high priority jobs that must have the required compute resources available when needed. These jobs generally never go below a 300 slot utilization, but occasionally spike up an additional 500 slots.\n• The other project is for users to run ad-hoc analytical queries. This project generally never uses more than 200 slots at a time. You want these ad-hoc queries to be billed based on how much data users scan rather than by slot capacity.\n\nYou need to ensure that both projects have the appropriate compute resources available. What should you do?","exam_id":11,"isMC":true,"answer_ET":"B","timestamp":"2024-01-03 17:29:00","answers_community":["B (83%)","D (17%)"],"answer":"B","question_images":[],"choices":{"D":"Create two Enterprise Edition reservations, one for each of the projects. For the SLA project, set a baseline of 800 slots. For the ad-hoc project, enable autoscaling up to 200 slots.","B":"Create two reservations, one for each of the projects. For the SLA project, use an Enterprise Edition with a baseline of 300 slots and enable autoscaling up to 500 slots. For the ad-hoc project, configure on-demand billing.","C":"Create two Enterprise Edition reservations, one for each of the projects. For the SLA project, set a baseline of 300 slots and enable autoscaling up to 500 slots. For the ad-hoc project, set a reservation baseline of 0 slots and set the ignore idle slots flag to False.","A":"Create a single Enterprise Edition reservation for both projects. Set a baseline of 300 slots. Enable autoscaling up to 700 slots."},"answer_description":"","question_id":180,"unix_timestamp":1704299340,"url":"https://www.examtopics.com/discussions/google/view/130213-exam-professional-data-engineer-topic-1-question-260/","topic":"1","answer_images":[]}],"exam":{"isBeta":false,"isImplemented":true,"provider":"Google","isMCOnly":true,"id":11,"lastUpdated":"11 Apr 2025","numberOfQuestions":319,"name":"Professional Data Engineer"},"currentPage":36},"__N_SSP":true}