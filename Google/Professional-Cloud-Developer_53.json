{"pageProps":{"questions":[{"id":"T4gUwNlPJFUpD2WNBaCY","question_id":261,"isMC":true,"answer":"C","exam_id":7,"answer_images":[],"discussion":[{"comment_id":"98284","comments":[{"poster":"syu31svc","timestamp":"1640571960.0","comment_id":"391636","content":"Good find; C is supported by this","upvote_count":"5"}],"timestamp":"1606667220.0","upvote_count":"20","poster":"emmet","content":"My vote is answer C)\n\"Virtual Private Cloud networks on Google Cloud have an internal DNS service that lets instances in the same network access each other by using internal DNS names\" \nThis name can be used for access: [INSTANCE_NAME].[ZONE].c.[PROJECT_ID].internal \nhttps://cloud.google.com/compute/docs/internal-dns#access_by_internal_DNS"},{"timestamp":"1725732720.0","content":"Selected Answer: C\nC. Ensure that clients use Compute Engine internal DNS by connecting to the instance name with the URL https://[INSTANCE_NAME].[ZONE].c.[PROJECT_ID].internal/.\n\nThis option allows clients within the same Virtual Private Cloud (VPC) to resolve the Compute Engine instance's internal IP address using internal DNS, which is a feature provided by Compute Engine. The internal DNS name is constructed using the instance name, zone, and project ID, and this DNS entry is automatically created and managed by Google Cloud. This method ensures that traffic between clients and the API does not leave the Google Cloud network, providing lower latency and enhanced security.","comment_id":"1168331","upvote_count":"1","poster":"santoshchauhan"},{"upvote_count":"1","comment_id":"1011090","poster":"__rajan__","content":"Selected Answer: C\nThis is a simple and effective way to enable communication between services within the same VPC without the need for external IP addresses or load balancing services.","timestamp":"1710842580.0"},{"upvote_count":"1","comment_id":"768306","timestamp":"1688708340.0","poster":"omermahgoub","content":"D, connecting to the instance name with the url https://[API_NAME]/[API_VERSION]/, may not work as it is not specified how clients would know the correct API name and version. \n\nC is the correct answer: By connecting to the instance name with the url https://[INSTANCE_NAME].[ZONE].c. [PROJECT_ID].internal/, clients can use Compute Engine's internal DNS to access the API hosted on the virtual machine instance within the same VPC. This will allow clients to access the API with low latency and without the need for a static external IP address."},{"content":"Selected Answer: C\nC is correct","upvote_count":"1","comment_id":"737151","poster":"jcataluna","timestamp":"1686071160.0"},{"comment_id":"653735","poster":"brunoguzzo18","upvote_count":"1","content":"Selected Answer: C\nhttps://cloud.google.com/compute/docs/internal-dns\nThe answer ic C.","timestamp":"1677555540.0"},{"timestamp":"1676878680.0","poster":"tomato123","upvote_count":"3","content":"Selected Answer: C\nC is correct","comment_id":"649195"},{"upvote_count":"1","content":"Selected Answer: C\nvote for C","timestamp":"1674145320.0","poster":"nhadi82","comment_id":"633624"},{"upvote_count":"1","comment_id":"527175","content":"Vote for C","poster":"herocc","timestamp":"1658191020.0"},{"upvote_count":"1","timestamp":"1657267800.0","poster":"ParagSanyashiv","content":"Selected Answer: C\nWith no doubt, it is C","comment_id":"519446"},{"timestamp":"1656777960.0","upvote_count":"1","poster":"Flavio80","content":"right answer is C","comment_id":"515121"},{"content":"For me the right answer is C.","upvote_count":"2","comment_id":"394552","timestamp":"1640862480.0","poster":"kernel1973"},{"comment_id":"269597","timestamp":"1626528960.0","poster":"AtulYadav","content":"I vote for C.","upvote_count":"4"},{"poster":"saurabh1805","content":"my vote for C as well.","upvote_count":"3","comment_id":"215536","timestamp":"1620501660.0"}],"answers_community":["C (100%)"],"timestamp":"2020-05-29 16:27:00","question_images":[],"question_text":"You are developing an HTTP API hosted on a Compute Engine virtual machine instance that needs to be invoked by multiple clients within the same Virtual\nPrivate Cloud (VPC). You want clients to be able to get the IP address of the service.\nWhat should you do?","unix_timestamp":1590762420,"url":"https://www.examtopics.com/discussions/google/view/21587-exam-professional-cloud-developer-topic-1-question-34/","answer_description":"","choices":{"B":"Reserve a static external IP address and assign it to an HTTP(S) load balancing service's forwarding rule. Then, define an A record in Cloud DNS. Clients should use the name of the A record to connect to the service.","D":"Ensure that clients use Compute Engine internal DNS by connecting to the instance name with the url https://[API_NAME]/[API_VERSION]/.","A":"Reserve a static external IP address and assign it to an HTTP(S) load balancing service's forwarding rule. Clients should use this IP address to connect to the service.","C":"Ensure that clients use Compute Engine internal DNS by connecting to the instance name with the url https://[INSTANCE_NAME].[ZONE].c. [PROJECT_ID].internal/."},"topic":"1","answer_ET":"C"},{"id":"vOttcc16qKYwthRkRweA","answer_ET":"D","exam_id":7,"choices":{"A":"Save user-uploaded images to a temporary Cloud Storage bucket. Implement code on the backend server to retrieve the image content and call the Vision API to process each new uploaded image.","B":"Save user-uploaded images to a Cloud Storage bucket. Configure a Cloud Function that is triggered when a new image is uploaded and calls one or more Cloud Run services. Create additional Cloud Run services that call the Vision API to process each new uploaded image.","C":"Save user-uploaded images to a Cloud Storage bucket. Configure a Cloud Function that is triggered when a new image is uploaded and publishes a message to a Pub/Sub topic. Deploy microservices in GKE that subscribe to the Pub/Sub topic and call the Vision API to process each new uploaded image.","D":"Save user-uploaded images to a Cloud Storage bucket. Create an Eventarc trigger that connects the bucket to the Workflows event receiver when a new image is uploaded. Create a workflow in Workflows with multiple Cloud Functions that call the Vision API to process each new uploaded image."},"isMC":true,"topic":"1","question_text":"You are developing an online chat application where users can upload profile pictures. Uploaded profile pictures must comply with content policies. You need to detect inappropriate images and label those images automatically when they are uploaded. In the future, this process will need to be expanded to include additional processing tasks such as watermarking and image compression.\n\nYou want to simplify orchestration and minimize operational overhead of the image scanning and labeling steps while also ensuring that additional steps can be added and removed easily later on. What should you do?","answer":"D","question_id":262,"answer_description":"","timestamp":"2025-03-01 15:56:00","unix_timestamp":1740840960,"question_images":[],"answer_images":[],"discussion":[{"upvote_count":"1","timestamp":"1740840960.0","poster":"Sandesh24","content":"Selected Answer: D\nUsing Cloud Storage with an Eventarc trigger to initiate a Workflows orchestration allows you to easily add or remove processing steps, such as additional image analysis, watermarking, or compression, without the need to manage complex infrastructure. This design minimizes operational overhead by leveraging serverless components that automatically scale and are managed by Google Cloud.","comment_id":"1363586"}],"answers_community":["D (100%)"],"url":"https://www.examtopics.com/discussions/google/view/157334-exam-professional-cloud-developer-topic-1-question-340/"},{"id":"KKTyZW0toOqhD4aBuJoW","answers_community":["B (100%)"],"question_id":263,"isMC":true,"answer_images":[],"discussion":[{"upvote_count":"1","timestamp":"1740844080.0","poster":"Sandesh24","comment_id":"1363624","content":"Selected Answer: B\nArtifact Analysis archives vulnerability metadata in Cloud Storage buckets. When images are scanned, recent vulnerability data is typically available through Artifact Analysis APIs, but metadata older than 30 days is retained in Cloud Storage. To compile a compliance report that includes older images, you need to check the Artifact Analysis storage buckets where this archived data is kept.\n • Option A: A Pub/Sub subscription only captures events from the point of subscription onward and would not contain older metadata.\n • Option C: Pulling images from Artifact Registry does not provide historical vulnerability metadata.\n • Option D: Cloud Trace logs do not store the detailed vulnerability findings required for your compliance report."}],"unix_timestamp":1740844080,"topic":"1","exam_id":7,"answer":"B","url":"https://www.examtopics.com/discussions/google/view/157356-exam-professional-cloud-developer-topic-1-question-348/","choices":{"A":"Create a Pub/Sub subscription to pull from Artifact Analysis topics.","D":"Check Cloud Trace logs for Artifact Analysis findings.","B":"Check Artifact Analysis storage buckets in Cloud Storage.","C":"Push or pull the images from Artifact Registry."},"question_text":"You are compiling a compliance report on vulnerability metadata for a specific set of images identified by Artifact Analysis. Metadata from images scanned more than 30 days ago are missing from the compliance report. You need to access the vulnerability metadata for these older images. What should you do?","answer_description":"","timestamp":"2025-03-01 16:48:00","answer_ET":"C","question_images":[]},{"id":"zMX7jelpyEXo4jFLWWnY","isMC":true,"answer_description":"","unix_timestamp":1740909360,"question_id":264,"discussion":[{"poster":"09bd94b","comment_id":"1363887","upvote_count":"1","timestamp":"1740909360.0","content":"Selected Answer: A\nthis approach makes sense"}],"question_images":[],"topic":"1","question_text":"Your team runs a Python job that reads millions of customer record files stored in a Cloud Storage bucket. To comply with regulatory requirements, you need to ensure that customer data is immediately deleted once the job is completed. You want to minimize the time required to complete this task. What should you do?","answer":"A","choices":{"B":"Configure Object Lifecycle Management on the Cloud Storage bucket that deletes all the objects in the bucket at the end of the job execution.","A":"Add a final step in the job that deletes all the objects in the bucket in bulk by using batch requests to the Cloud Storage API.","D":"Use the gcloud CLI to execute the gcloud storage rm --recursive gs://BUCKET_NAME/ command.","C":"Remove the bucket from the Google Cloud console when the job is completed"},"exam_id":7,"answer_ET":"A","url":"https://www.examtopics.com/discussions/google/view/157378-exam-professional-cloud-developer-topic-1-question-349/","timestamp":"2025-03-02 10:56:00","answers_community":["A (100%)"],"answer_images":[]},{"id":"SUBYrORACTkn5iEfR3GE","url":"https://www.examtopics.com/discussions/google/view/30620-exam-professional-cloud-developer-topic-1-question-35/","topic":"1","question_id":265,"question_images":[],"isMC":true,"discussion":[{"poster":"woriheck93","comment_id":"439511","upvote_count":"17","timestamp":"1630823040.0","content":"Ans: B\nB have the correct endpoint /api/alpha/*,\nA only get one endpoint counter","comments":[{"timestamp":"1672813980.0","comment_id":"765312","poster":"siwang","upvote_count":"1","content":"Agree. counter metric with applying regression filter to httpRequest.requestUrl should be able to get the count value. refer to: https://cloud.google.com/logging/docs/log4j2-vulnerability#log4j-search"},{"content":"B is the correct answer\n\"Create a filter that collects only the log entries that you want to count in your metric using the logging query language. You can also use regular expressions to create your metric's filters.\"\nhttps://cloud.google.com/logging/docs/logs-based-metrics/counter-metrics#console","comment_id":"769604","timestamp":"1673192940.0","upvote_count":"2","poster":"fraloca"}]},{"comment_id":"173752","poster":"google_learner123","content":"Answer should be A","timestamp":"1599286380.0","comments":[{"poster":"fraloca","timestamp":"1608844260.0","content":"https://cloud.google.com/logging/docs/logs-based-metrics/counter-metrics#console","upvote_count":"3","comment_id":"251772","comments":[{"comment_id":"312894","poster":"hug_c0sm0s","upvote_count":"4","content":"a bit confused about A / B, it seems they mean the same thing.","timestamp":"1615948260.0"}]}],"upvote_count":"10"},{"timestamp":"1731622200.0","poster":"Shereef","upvote_count":"1","content":"Selected Answer: A\nThe correct answer is:\n\nA. Add a Stackdriver counter metric for path:/api/alpha/\n\nExplanation:\nA. Add a Stackdriver counter metric for path:/api/alpha/:\nThis option is effective because Stackdriver (now known as Google Cloud Monitoring) supports creating custom metrics that can count log entries matching specific patterns in log fields, such as path:/api/alpha/.\nBy adding a counter metric for the path field, you can automatically track the count of all requests to /api/alpha/* without needing to export and process the logs externally.\nWhy Not the Other Options?\nB. Add a Stackdriver counter metric for endpoint:/api/alpha/:\nThis answer is ambiguous and less likely to work, as \"endpoint\" is not a standard log field in Stackdriver. Instead, path or httpRequest.requestUrl would be the correct field to use.","comment_id":"1312334"},{"upvote_count":"2","timestamp":"1721197080.0","comment_id":"1249425","content":"Selected Answer: A\nThe best approach here is A. Add a Stackdriver counter metric for path:/api/alpha/.\n\nHere's why:\n\nStackdriver Metrics: Stackdriver metrics are specifically designed for tracking and aggregating data points over time. They are ideal for counting events like requests.\nPath-Based Filtering: You can define Stackdriver metrics with specific filters based on the request path. In this case, path:/api/alpha/ will capture all requests matching that pattern.\nEfficient Aggregation: Stackdriver automatically aggregates the metric data, providing you with the total count of requests to the /api/alpha/* endpoints.","poster":"thewalker","comments":[{"timestamp":"1721197080.0","upvote_count":"1","content":"Why other options are less suitable:\n\n**B. Endpoint:/api/alpha/*: ** While this might seem like a good option, Stackdriver doesn't typically use the term \"endpoint\" for filtering. It primarily uses \"path\" for request path-based filtering.\nC. Export logs to Cloud Storage and count lines: This is a less efficient and more complex approach. You'd need to write custom code to parse the logs and count the matching lines, which adds overhead and potential for errors.\nD. Export logs to Cloud Pub/Sub and count lines: Similar to option C, this involves exporting logs and then processing them externally, which is less efficient than using Stackdriver metrics.\nIn summary: Adding a Stackdriver counter metric with the path:/api/alpha/ filter is the most efficient and straightforward way to get the count of all requests on all /api/alpha/* endpoints.","poster":"thewalker","comment_id":"1249426"}]},{"timestamp":"1709842740.0","upvote_count":"3","comment_id":"1168332","content":"Selected Answer: A\nA. Add a Stackdriver counter metric for path:/api/alpha/.\n\nIn Google Cloud's operations suite (formerly Stackdriver), you can create custom metrics to count specific events within your logs. You would set up a counter metric to capture and count log entries where the request path matches your specified pattern, such as /api/alpha/*. This would allow you to query and visualize the count of requests to these endpoints directly within Stackdriver Monitoring without the need to export the logs elsewhere.\nB. This option seems to be suggesting the correct action (creating a counter metric), but the syntax endpoint:/api/alpha/* is not correct for Stackdriver Monitoring. Custom metrics in Stackdriver are based on log data and the filter that matches the log entries, so you would specify the filter as part of creating the metric.","poster":"santoshchauhan"},{"timestamp":"1707812280.0","comment_id":"1148987","upvote_count":"1","poster":"theseawillclaim","content":"Selected Answer: B\nIf you don't export the metric, then you have nothing to count. \nI choose B because \"endpoint\" is more specific than \"path\"."},{"timestamp":"1695110820.0","comment_id":"1011096","upvote_count":"1","content":"Selected Answer: B\nAns: B","poster":"__rajan__"},{"comment_id":"989109","poster":"maxdanny","upvote_count":"1","timestamp":"1692874800.0","content":"Selected Answer: B\nThis option will accurately track the number of requests made to all endpoints nested under /api/alpha/*."},{"poster":"kennyloo","content":"B is for counter","timestamp":"1692268200.0","upvote_count":"1","comment_id":"983493"},{"upvote_count":"1","content":"Selected Answer: B\nsubmiting just to confirm community response.","poster":"Pime13","comment_id":"823837","timestamp":"1677509640.0"},{"timestamp":"1675340400.0","poster":"Foxal","comment_id":"796014","content":"Selected Answer: B\nB is the only one","upvote_count":"1"},{"content":"Selected Answer: B\nB is the correct answer","upvote_count":"1","comment_id":"772144","timestamp":"1673420040.0","poster":"ash_meharun"},{"upvote_count":"1","content":"Selected Answer: B\nanswer is B with the correct endpoint and the goal of counter metris is to resolve the need to count calls","poster":"telp","timestamp":"1673276400.0","comment_id":"770543"},{"content":"Option B is the correct choice because it involves creating a counter metric in Stackdriver specifically for requests to the /api/alpha/* endpoints. This will allow you to track the number of requests to these endpoints and view the data in Stackdriver.","poster":"omermahgoub","comments":[{"upvote_count":"1","content":"Option C is incorrect because it involves exporting the logs to Cloud Storage and manually counting the lines that match /api/alpha. This is a more time-consuming and error-prone approach compared to using a counter metric in Stackdriver.","poster":"omermahgoub","timestamp":"1673077320.0","comment_id":"768311"}],"comment_id":"768309","upvote_count":"3","timestamp":"1673077320.0"},{"content":"Selected Answer: C\nC is correct","upvote_count":"2","comment_id":"649196","poster":"tomato123","timestamp":"1660973940.0"},{"comment_id":"588184","upvote_count":"5","timestamp":"1650374460.0","poster":"[Removed]","content":"Ans should be B -> (https://cloud.google.com/blog/products/management-tools/cloud-logging-gets-regular-expression-support)\npath=~\"/api/alpha/*\""},{"content":"ans: a\nhttps://cloud.google.com/logging/docs/view/basic-queries#searching_specific_fields\n\nhttps://cloud.google.com/monitoring/charts/metrics-selector#filter-option\n\nTo match any US zone that ends with “a”, you could use the the regular expression ^us.*.a$.","timestamp":"1626741960.0","upvote_count":"3","poster":"celia20200410","comments":[{"timestamp":"1653548700.0","poster":"ruben82","upvote_count":"1","comment_id":"607507","content":"documentation says: \"ends with a\". This question is different."}],"comment_id":"409939"},{"timestamp":"1626568020.0","poster":"syu31svc","upvote_count":"1","comment_id":"408656","content":"https://cloud.google.com/logging/docs/logs-based-metrics/troubleshooting#metric-name-restrictions\n\nI would take C"}],"answer":"B","question_text":"Your application is logging to Stackdriver. You want to get the count of all requests on all /api/alpha/* endpoints.\nWhat should you do?","unix_timestamp":1599286380,"answers_community":["B (47%)","A (40%)","13%"],"timestamp":"2020-09-05 08:13:00","choices":{"D":"Export the logs to Cloud Pub/Sub and count lines matching /api/alpha.","B":"Add a Stackdriver counter metric for endpoint:/api/alpha/*.","C":"Export the logs to Cloud Storage and count lines matching /api/alpha.","A":"Add a Stackdriver counter metric for path:/api/alpha/."},"exam_id":7,"answer_ET":"B","answer_images":[],"answer_description":""}],"exam":{"name":"Professional Cloud Developer","isBeta":false,"id":7,"provider":"Google","lastUpdated":"11 Apr 2025","isImplemented":true,"numberOfQuestions":338,"isMCOnly":false},"currentPage":53},"__N_SSP":true}