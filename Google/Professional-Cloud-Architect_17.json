{"pageProps":{"questions":[{"id":"QWdoXgQjcLt0mT5VHsJH","choices":{"C":"1. Create folders under the Organization resource named ג€Developmentג€ and ג€Production.ג€ 2. Grant all developers the Project Creator IAM role on the ג€Developmentג€ folder. 3. Move the developer projects into the ג€Developmentג€ folder. 4. Set the policies for all projects on the Organization. 5. Additionally, set the production policies on the ג€Productionג€ folder.","A":"1. Create a second Google Workspace account and Organization. 2. Grant all developers the Project Creator IAM role on the new Organization. 3. Move the developer projects into the new Organization. 4. Set the policies for all projects on both Organizations. 5. Additionally, set the production policies on the original Organization.","B":"1. Create a folder under the Organization resource named ג€Production.ג€ 2. Grant all developers the Project Creator IAM role on the new Organization. 3. Move the developer projects into the new Organization. 4. Set the policies for all projects on the Organization. 5. Additionally, set the production policies on the ג€Productionג€ folder.","D":"1. Designate the Organization for production projects only. 2. Ensure that developers do not have the Project Creator IAM role on the Organization. 3. Create development projects outside of the Organization using the developer Google Workspace accounts. 4. Set the policies for all projects on the Organization. 5. Additionally, set the production policies on the individual production projects."},"discussion":[{"timestamp":"1680889860.0","poster":"cloudmon","content":"Selected Answer: C\nC, because managing multiple organizations is not a Google best practice","comment_id":"582559","upvote_count":"14"},{"comment_id":"1336413","upvote_count":"1","poster":"plumbig11","timestamp":"1736002680.0","content":"Selected Answer: C\nCreate different folders development e productions"},{"upvote_count":"1","content":"the requirement is \"...You want to manage policies for all projects centrally...\" With multiple organizations that wont be possible as you would have to set policies on multiple organizations. Therefore I opt for \"C\".","poster":"devnul","comment_id":"995410","timestamp":"1725129540.0"},{"comment_id":"946066","content":"for everyone commenting that multiple organizations is bad practice according to google check https://cloud.google.com/architecture/identity/best-practices-for-planning","comments":[{"comment_id":"1088830","upvote_count":"1","timestamp":"1733435820.0","poster":"cerveza7","content":"\"The right number of organizations to use depends on the number of independent groups of administrative users in your company:\n- If your company is organized by function, you might have a single department that's in charge of overseeing all Google Cloud deployments.\n- If your company is organized by division or owns a number of autonomously-run subsidiaries, then there might not be a single department that's in charge.\n\nNo divisions is mentioned in the questions. Developer is a function."}],"upvote_count":"3","timestamp":"1720398660.0","poster":"aaa7"},{"upvote_count":"1","timestamp":"1709155020.0","poster":"AugustoKras011111","content":"Selected Answer: C\nC, Bcuz manage multiple organizations is not a Google best practice","comment_id":"825305"},{"content":"clearly C. Two orgs is a BAD practice","timestamp":"1705334220.0","comment_id":"776765","poster":"OttomanSheikhIran","upvote_count":"1"},{"comments":[{"timestamp":"1703580000.0","poster":"omermahgoub","upvote_count":"1","content":"Option A, creating a second Google Workspace account and Organization, would not be a recommended practice as it would create unnecessary complexity and make it more difficult to manage policies and move projects between environments.\n\nOption B, creating a single folder under the Organization resource and placing all projects in that folder, would not allow you to set different policies for development and production projects.","comment_id":"757263"}],"timestamp":"1703579940.0","upvote_count":"3","comment_id":"757262","poster":"omermahgoub","content":"I would recommend option C, creating two folders under the Organization resource named \"Development\" and \"Production\" and placing developer and production projects in the respective folders. This approach would allow you to centrally manage policies for all projects, while also being able to set more restrictive policies for production projects. It would also allow you to easily move projects between the Development and Production folders as business needs change, without disrupting users or developers.\n\nOption D, designating the Organization for production projects only, would not allow developers to create projects within the Organization and could lead to confusion around project ownership and management. It would also make it more difficult to move projects between development and production environments."},{"poster":"surajkrishnamurthy","upvote_count":"1","comment_id":"747025","timestamp":"1702720980.0","content":"Selected Answer: C\nC Is the Correct Answer"},{"timestamp":"1700468700.0","comment_id":"722463","upvote_count":"2","poster":"ashrafh","content":"all 4 answers seems stupid"},{"poster":"megumin","upvote_count":"1","content":"Selected Answer: C\nC is ok","comment_id":"720602","timestamp":"1700235720.0"},{"poster":"AzureDP900","comment_id":"695618","timestamp":"1697395800.0","upvote_count":"2","content":"C is the best option"},{"content":"Selected Answer: C\nC is Ok","poster":"6721sora","upvote_count":"2","timestamp":"1694188920.0","comment_id":"663767"},{"poster":"cloudinit","comment_id":"648920","content":"Selected Answer: C\nI don't think anyone can create projects outside the organization using the workspace account as it redirects the users into the organization.","upvote_count":"2","timestamp":"1692443160.0"},{"timestamp":"1690202760.0","poster":"gardislan18","comment_id":"636052","upvote_count":"1","content":"Answer is C\nA - you only want to create and Organization structure not Google Workspace\nB - best practice is to move your projects to a folders\nD - developers are allowed to create projects"},{"content":"Selected Answer: C\nC makes most sense in this scenario","timestamp":"1689268740.0","upvote_count":"1","comment_id":"630996","poster":"szefco"},{"content":"Selected Answer: D\nD is better than C.","upvote_count":"1","timestamp":"1684752720.0","comment_id":"605362","poster":"amxexam"},{"upvote_count":"3","timestamp":"1674095100.0","poster":"sjmsummer","content":"Selected Answer: C\nC seems to be more organized solution than D.","comment_id":"527167"},{"timestamp":"1672810680.0","upvote_count":"2","poster":"technodev","content":"Selected Answer: C\nI would go with C","comment_id":"516242"},{"content":"i'd go with C. option D is not a recommended practice","poster":"Aiffone","upvote_count":"1","comment_id":"514463","timestamp":"1672568460.0"},{"content":"C. 1. Create folders under the Organization resource named ג€Developmentג€ and ג€Production.ג€ 2. Grant all developers the Project Creator IAM role on the ג€Developmentג€ folder. 3. Move the developer projects into the ג€Developmentג€ folder. 4. Set the policies for all projects on the Organization. 5. Additionally, set the production policies on the ג€Productionג€ folder.","upvote_count":"1","timestamp":"1672547820.0","poster":"victory108","comment_id":"514358"},{"content":"I go for C","comment_id":"514084","poster":"BeetleJuice","upvote_count":"1","timestamp":"1672492620.0"},{"comment_id":"513858","upvote_count":"1","content":"The Correct answer is C if you Designate the Organization for production projects only. You cannot manage policies for projects created under Google work space An Organization resource is available for Google Workspace and Cloud Identity customers:\n\nGoogle Workspace: Sign up for Google Workspace.\nCloud Identity: Sign up for Cloud Identity. Hence Ans C is correct according to me","poster":"Gimyjony","timestamp":"1672458420.0"},{"content":"Selected Answer: C\nThe correct answer is C","timestamp":"1672304700.0","comment_id":"511992","upvote_count":"2","poster":"simbu1299"},{"timestamp":"1672304640.0","upvote_count":"2","content":"The correct answer is C","comment_id":"511991","poster":"simbu1299"},{"upvote_count":"2","poster":"edilramos","timestamp":"1672226100.0","comment_id":"511025","content":"Selected Answer: C\nI think C is the best answer.\n1 - Basic principle of the IAM hierarchy: Org>Folders>Projects;\n2 - To maintain the centralized management of Access Security, Costs in the same organization."},{"content":"I think is C","poster":"Mikelala31","upvote_count":"2","timestamp":"1672219680.0","comment_id":"510946"},{"comment_id":"510736","upvote_count":"3","content":"Option-C is correct.","timestamp":"1672193940.0","poster":"StelSen"}],"topic":"1","answer":"C","answer_ET":"C","question_text":"Your company has a Google Workspace account and Google Cloud Organization. Some developers in the company have created Google Cloud projects outside of the Google Cloud Organization.\nYou want to create an Organization structure that allows developers to create projects, but prevents them from modifying production projects. You want to manage policies for all projects centrally and be able to set more restrictive policies for production projects.\nYou want to minimize disruption to users and developers when business needs change in the future. You want to follow Google-recommended practices. Now should you design the Organization structure?","exam_id":4,"question_id":81,"answers_community":["C (97%)","3%"],"url":"https://www.examtopics.com/discussions/google/view/68682-exam-professional-cloud-architect-topic-1-question-171/","timestamp":"2021-12-28 03:19:00","answer_images":[],"question_images":[],"isMC":true,"answer_description":"","unix_timestamp":1640657940},{"id":"BvGwPLYX5bv0wo0KTM7z","answers_community":["D (98%)","3%"],"timestamp":"2021-12-28 03:23:00","isMC":true,"answer_images":[],"answer":"D","question_images":[],"discussion":[{"timestamp":"1710012600.0","upvote_count":"10","poster":"CGS22","comment_id":"834322","content":"Selected Answer: D\nThe correct answer is: D. Create a managed instance group with Compute Engine instances. Create a global load balancer and configure it with two backends: Managed instance group, Cloud Storage bucket. Enable Cloud CDN on the bucket backend.\n\nThis solution will improve the performance of the application by:\n\nAutomatically scaling the number of Compute Engine instances to meet demand.\nDistributing traffic across multiple instances to reduce load on each instance.\nCaching popular songs in memory to reduce the number of times that they need to be loaded from Cloud Storage.\nUsing a global load balancer to distribute traffic evenly across all regions.\nUsing Cloud CDN to deliver files to users from a location that is closer to them.\nThis solution is the most efficient and cost-effective way to improve the performance of the application."},{"comment_id":"1086809","upvote_count":"6","comments":[{"timestamp":"1733598360.0","comment_id":"1090539","content":"Most of the \"official\" answers are, unfortunately. I've pretty much defaulted to the community answer distributions exclusively.","upvote_count":"3","poster":"MikeH20"}],"poster":"Jconnor","content":"A is Ridiculous.","timestamp":"1733229660.0"},{"poster":"plumbig11","comment_id":"1336414","upvote_count":"1","content":"Selected Answer: D\nThe point here is thar music and cloud CDN is something that it's a good match.","timestamp":"1736002800.0"},{"timestamp":"1726040280.0","poster":"nocrush","content":"Selected Answer: D\nThe correct answer is D.\n\nhttps://cloud.google.com/cdn?hl=en#static-content","comment_id":"1004526","upvote_count":"2"},{"comment_id":"1004522","timestamp":"1726040220.0","poster":"nocrush","content":"D. Absolutely","upvote_count":"1"},{"timestamp":"1725965160.0","comment_id":"1003911","upvote_count":"1","poster":"ranbatrekker","content":"Not agree on A , why pay unnecessary for same data for VM disk size."},{"upvote_count":"3","content":"Selected Answer: D\nThe Cloud CDN is the best practice for the content caching.","poster":"zerg0","timestamp":"1706928660.0","comment_id":"796664"},{"comments":[{"timestamp":"1703580120.0","content":"Option B, creating a Cloud Filestore NFS volume and attaching it to the backend Compute Engine instances, would not provide a way to scale the number of instances to handle increased demand for the application.\n\nOption C, copying popular songs into CloudSQL as a blob and updating the application code to retrieve data from CloudSQL when Cloud Storage is overloaded, would not provide a way to scale the number of instances to handle increased demand for the application. Additionally, using CloudSQL to store and serve music files may not be the most appropriate use case for the service, as it is designed for storing and querying structured data, rather than serving large files.","comment_id":"757266","upvote_count":"2","poster":"omermahgoub"}],"poster":"omermahgoub","timestamp":"1703580060.0","upvote_count":"3","comment_id":"757264","content":"I would recommend option D, creating a managed instance group with Compute Engine instances and a global load balancer with two backends: the managed instance group and the Cloud Storage bucket, and enabling Cloud CDN on the bucket backend. This approach would allow you to scale the number of instances in the managed instance group as needed to handle the demand for the application, and would also use the Cloud CDN to improve the performance of the application by caching the music files closer to the users.\n\nOption A, mounting the Cloud Storage bucket using gcsfuse on all backend Compute Engine instances, would not provide a way to scale the number of instances to handle increased demand for the application."},{"upvote_count":"2","timestamp":"1702721100.0","content":"Selected Answer: D\nD Is the Correct Answer","comment_id":"747029","poster":"surajkrishnamurthy"},{"content":"Selected Answer: D\nD is ok","poster":"megumin","upvote_count":"1","timestamp":"1700236020.0","comment_id":"720607"},{"comment_id":"700976","timestamp":"1697896860.0","upvote_count":"1","poster":"adelynllllllllll","content":"Selected Answer: A\nI think it should be A,"},{"content":"Why not A, I think it should be A, since it mentioned popular songs and file store is faster then the cloud storage, I vote for A","poster":"adelynllllllllll","comment_id":"700975","timestamp":"1697896800.0","upvote_count":"2"},{"timestamp":"1697395920.0","comment_id":"695622","content":"I will go with D","poster":"AzureDP900","upvote_count":"2"},{"poster":"jabrrJ68w02ond1","timestamp":"1693822260.0","comment_id":"659159","upvote_count":"4","content":"Selected Answer: D\nDo not trust the official answers here, D is correct. In special for this question, never use gcsfuse in production. Performance is bad and reliability is trashy - Google states it themselves."},{"content":"D is correct","timestamp":"1689268860.0","upvote_count":"2","poster":"szefco","comment_id":"630999"},{"content":"Remember Gcsfuse performance is not good, reference\nhttps://cloud.google.com/storage/docs/gcs-fuse#notes","comment_id":"602807","poster":"JoeyCASD","timestamp":"1684304400.0","upvote_count":"3"},{"upvote_count":"4","comment_id":"590664","timestamp":"1682257860.0","content":"Selected Answer: D\nA is wrong because you can't be serving files directly from Compute Engine instance.\nGCS + CDN is best option","poster":"nkit"},{"poster":"cloudmon","comment_id":"582563","upvote_count":"1","timestamp":"1680890100.0","content":"Selected Answer: D\nD for sure"},{"poster":"Tan1234","comment_id":"541945","timestamp":"1675707960.0","content":"Clarification on D: Question states there are a fixed number of instances. So creating MIG","upvote_count":"3"},{"content":"Selected Answer: D\nGot this question in my exam, answered D","timestamp":"1674145980.0","comment_id":"527731","poster":"technodev","upvote_count":"3"},{"timestamp":"1673256540.0","upvote_count":"1","poster":"OrangeTiger","comment_id":"520069","content":"Selected Answer: D\nmost logical answer is D."},{"timestamp":"1673150100.0","content":"Selected Answer: D\ni'll go with D","comment_id":"519305","upvote_count":"1","poster":"dhikaisfahan2"},{"comment_id":"518275","timestamp":"1673014200.0","content":"Vote D.","poster":"AJapieGuru","upvote_count":"2"},{"upvote_count":"1","content":"Selected Answer: D\nI think is D","timestamp":"1672885680.0","comment_id":"517122","poster":"Tim_Chiang"},{"poster":"technodev","upvote_count":"2","comment_id":"516245","timestamp":"1672810920.0","content":"D is the correct answer, CDN serves the purpose."},{"comment_id":"514467","timestamp":"1672569180.0","upvote_count":"1","content":"Selected Answer: D\nD is the answer. CDN is for this","poster":"timotei"},{"poster":"victory108","comment_id":"514357","timestamp":"1672547760.0","upvote_count":"1","content":"D. 1. Create a managed instance group with Compute Engine instances. 2. Create a global load balancer and configure it with two backends: ג—‹ Managed instance group ג—‹ Cloud Storage bucket 3. Enable Cloud CDN on the bucket backend."},{"poster":"Pravin269","content":"D should be the correct answer","comment_id":"513213","upvote_count":"2","timestamp":"1672394460.0"},{"comment_id":"511032","timestamp":"1672226700.0","poster":"edilramos","upvote_count":"4","content":"Selected Answer: D\nThe Answer is D.\nThis is the meaning of using CND.\nCache content closer to the end user to optimize delivery time and other benefits."},{"content":"I believe the correct answer is D","poster":"simbu1299","upvote_count":"1","timestamp":"1672222680.0","comment_id":"510991"},{"timestamp":"1672219440.0","comment_id":"510943","upvote_count":"1","poster":"Mikelala31","content":"I think is D"},{"upvote_count":"2","content":"Option-D is correct. Need CDN for caching to get better performance","timestamp":"1672194180.0","comment_id":"510738","poster":"StelSen"}],"choices":{"A":"1. Mount the Cloud Storage bucket using gcsfuse on all backend Compute Engine instances. 2. Serve music files directly from the backend Compute Engine instance.","B":"1. Create a Cloud Filestore NFS volume and attach it to the backend Compute Engine instances. 2. Download popular songs in Cloud Filestore. 3. Serve music files directly from the backend Compute Engine instance.","D":"1. Create a managed instance group with Compute Engine instances. 2. Create a global load balancer and configure it with two backends: ג—‹ Managed instance group ג—‹ Cloud Storage bucket 3. Enable Cloud CDN on the bucket backend.","C":"1. Copy popular songs into CloudSQL as a blob. 2. Update application code to retrieve data from CloudSQL when Cloud Storage is overloaded."},"answer_description":"","topic":"1","question_text":"Your company has an application running on Compute Engine that allows users to play their favorite music. There are a fixed number of instances. Files are stored in Cloud Storage, and data is streamed directly to users. Users are reporting that they sometimes need to attempt to play popular songs multiple times before they are successful. You need to improve the performance of the application. What should you do?","answer_ET":"D","question_id":82,"exam_id":4,"url":"https://www.examtopics.com/discussions/google/view/68683-exam-professional-cloud-architect-topic-1-question-172/","unix_timestamp":1640658180},{"id":"Dggj2JK50KoHEp9Jrs3K","answer_ET":"A","answer_description":"","question_id":83,"question_images":[],"answer":"A","answers_community":["A (100%)"],"unix_timestamp":1640658300,"isMC":true,"topic":"1","exam_id":4,"timestamp":"2021-12-28 03:25:00","discussion":[{"comment_id":"1336415","content":"Selected Answer: A\nSave logs for 1 year you must include cloud storage.","upvote_count":"1","poster":"plumbig11","timestamp":"1736002860.0"},{"upvote_count":"1","timestamp":"1729598820.0","comment_id":"1200164","content":"Selected Answer: A\nIt is A","poster":"gregorgrinc"},{"content":"Selected Answer: A\nI guess a similar approach to this shuld be followed https://cloud.google.com/architecture/exporting-stackdriver-logging-for-compliance-requirements","poster":"theBestStudent","upvote_count":"1","timestamp":"1716676680.0","comment_id":"1080403"},{"poster":"Deb2293","comment_id":"839393","timestamp":"1694728380.0","upvote_count":"4","content":"Selected Answer: A\nArchival storage: Cloud Storage is the best"},{"poster":"tdotcat","comment_id":"776088","timestamp":"1689378180.0","content":"Selected Answer: A\nA is ok","upvote_count":"1"},{"comment_id":"720610","timestamp":"1684331400.0","poster":"megumin","upvote_count":"1","content":"Selected Answer: A\nA is ok"},{"poster":"AzureDP900","content":"I would like to go with Option A","upvote_count":"2","timestamp":"1681584900.0","comment_id":"695625"},{"timestamp":"1678299120.0","upvote_count":"3","content":"Selected Answer: A\nLogs needed for a year. Coldline or Archive storage classes available.\nA seems fine","poster":"6721sora","comment_id":"663779"},{"content":"Selected Answer: A\nA is ok","upvote_count":"1","comment_id":"634440","timestamp":"1674293760.0","poster":"exam9391"},{"timestamp":"1664321880.0","upvote_count":"1","poster":"[Removed]","comment_id":"576486","content":"A should be right."},{"poster":"technodev","comments":[{"content":"Ignore A is correct. CDN was an answer to a diff question.","timestamp":"1656906120.0","comment_id":"516244","upvote_count":"1","poster":"technodev"}],"upvote_count":"1","comment_id":"516243","timestamp":"1656906000.0","content":"It should be D, CDN serves the purpose."},{"comment_id":"514356","poster":"victory108","timestamp":"1656642960.0","content":"A. Set up a filter in Cloud Logging and a Cloud Storage bucket as an export target for the logs you want to save.","upvote_count":"1"},{"comment_id":"511035","timestamp":"1656408660.0","upvote_count":"4","content":"Selected Answer: A\nA Is correct.\nSet up a filter in Cloud Logging and a Cloud Storage bucket as an export target for the logs you want to save.\n\nFilter in Cloud Loggin for specific content, and Cloud Storage for storage.","poster":"edilramos"},{"upvote_count":"4","timestamp":"1656375900.0","content":"Option-A is correct. Need cloud storage bucket for long time storage.","poster":"StelSen","comment_id":"510739"}],"choices":{"B":"Enable the Compute Engine API, and then enable logging on the firewall rules that match the traffic you want to save.","D":"Set up a filter in Cloud Logging and a topic in Pub/Sub to publish the logs.","C":"Set up a Cloud Logging Dashboard titled Cloud VPN Logs, and then add a chart that queries for the VPN metrics over a one-year time period.","A":"Set up a filter in Cloud Logging and a Cloud Storage bucket as an export target for the logs you want to save."},"question_text":"The operations team in your company wants to save Cloud VPN log events for one year. You need to configure the cloud infrastructure to save the logs. What should you do?","answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/68684-exam-professional-cloud-architect-topic-1-question-173/"},{"id":"PXlKEyHFSWmqBWW64XaP","answer":"A","question_id":84,"question_images":[],"topic":"1","answer_description":"","answer_images":[],"isMC":true,"answers_community":["A (75%)","C (25%)"],"discussion":[{"timestamp":"1656376020.0","upvote_count":"51","poster":"StelSen","comment_id":"510743","content":"Option-A is correct. Although Option-C sounds good, ultimately we should not store PI data at all as per question says."},{"timestamp":"1656409260.0","content":"Selected Answer: A\nThe correct answer is A.\nOption C seems to be an option, but there are two non-conformities there. In addition to storing personal data in the GCS, it is being improperly retained.","upvote_count":"14","comment_id":"511045","poster":"edilramos"},{"poster":"plumbig11","timestamp":"1736002980.0","content":"Selected Answer: A\nIn this case Dataflow pipeline with Cloud dataloss prevenction; You must have a dataflow pipeline with cloud DLP working on it.","upvote_count":"1","comment_id":"1336416"},{"comment_id":"1192744","content":"Selected Answer: A\nAgree with A","poster":"dija123","upvote_count":"1","timestamp":"1728541080.0"},{"poster":"ptapia_el","timestamp":"1725993900.0","content":"Selected Answer: A\nThis best option.","comment_id":"1170610","upvote_count":"1"},{"upvote_count":"1","comment_id":"1048226","timestamp":"1713555720.0","poster":"tamj123","content":"A is best answer, C seems be an extract step and security risk to upload to a bucket first"},{"comments":[{"upvote_count":"1","comment_id":"999732","timestamp":"1709665740.0","poster":"johny_doe","content":"exactly"}],"upvote_count":"3","content":"The problem with C is the data is stored in the bucket with the PII data even though the BigQuery data has it removed?","comment_id":"924314","poster":"BiddlyBdoyng","timestamp":"1702661220.0"},{"comment_id":"825318","poster":"AugustoKras011111","timestamp":"1693250760.0","content":"Selected Answer: A\noption A, the question say dont store data...","upvote_count":"1"},{"timestamp":"1692533580.0","content":"Selected Answer: A\nA is correct.","poster":"someCloudUser","comment_id":"815349","upvote_count":"1"},{"comment_id":"809796","poster":"telp","upvote_count":"1","timestamp":"1692113100.0","content":"Selected Answer: A\nAnswer A. The question say do not store PII data so need to remove it before storing."},{"comment_id":"805945","poster":"rotorclear","content":"Selected Answer: A\nAnswer should be A because the question emphasises on processing the data without storing it. That rules out C.","timestamp":"1691805060.0","upvote_count":"1"},{"poster":"RVivek","upvote_count":"2","comment_id":"800930","timestamp":"1691407620.0","content":"Selected Answer: A\nC -- is wrong because PII data is uploaded and the bucket is locked which means the data cannot be deleted\nB and D are wron as they do not use Data loss prevention to protect data"},{"upvote_count":"1","content":"PII --> Cloud DLP. So that narrows the choices down to A or C. C says \"Ask the external partners to upload all data on Cloud Storage\" which is not generally a feasible or recommended practice. Also, we cannot store PII anywhere, including in GCS. Answer is A.","poster":"dataqueen_3110","comment_id":"799278","timestamp":"1691273520.0"},{"comment_id":"759568","content":"Selected Answer: A\nA i s correct, C sounds good but storing the data in GCS is already a violation of the PII requirements","upvote_count":"1","poster":"Wael216","timestamp":"1687933560.0"},{"poster":"omermahgoub","timestamp":"1687761900.0","content":"I would recommend option A, creating a Dataflow pipeline to retrieve the data from the external sources and using the Cloud Data Loss Prevention (Cloud DLP) API to remove any PII data. Storing the result in BigQuery would allow the data warehousing team to easily perform analysis on the data.\n\nOption C, using Bucket Lock to protect the data and using the Cloud DLP API to remove PII data, would protect the data from unauthorized access, but would not allow the data warehousing team to easily perform analysis on the data.","comment_id":"757270","upvote_count":"5","comments":[{"comment_id":"757272","poster":"omermahgoub","upvote_count":"2","timestamp":"1687761900.0","content":"Option B, storing non-PII data in BigQuery and PII data in a Cloud Storage bucket with a retention policy set, would not fully protect the PII data and could potentially lead to data breaches.\n\nOption D, copying the data into a new table and skipping columns with PII data, would not fully protect the PII data and could potentially lead to data breaches. It would also require the data warehousing team to manually skip certain columns when performing analysis, which could be time-consuming and error-prone."}]},{"content":"Selected Answer: A\nA Is the Correct Answer","timestamp":"1686902940.0","upvote_count":"1","comment_id":"747031","poster":"surajkrishnamurthy"},{"poster":"ardit","comment_id":"738962","content":"Selected Answer: A\nA is the right one.","timestamp":"1686216840.0","upvote_count":"1"},{"poster":"jaxclain","timestamp":"1685385960.0","comment_id":"730844","upvote_count":"2","content":"Selected Answer: A\nOf course the correct answer is A, not sure how some people think C is valid, probably trolling trying to confuse some here."},{"comment_id":"725881","timestamp":"1684929060.0","poster":"Aninina","content":"Selected Answer: A\nIt's A, not C because we cannot ask the external partners to upload all data on Cloud Storage","upvote_count":"1"},{"poster":"megumin","content":"Selected Answer: A\nA is ok","comment_id":"721149","timestamp":"1684388820.0","upvote_count":"1"},{"poster":"AzureDP900","comment_id":"695627","upvote_count":"1","content":"I will go with A","timestamp":"1681585080.0"},{"content":"Selected Answer: A\nA is fine","upvote_count":"2","comment_id":"663783","poster":"6721sora","timestamp":"1678299360.0"},{"upvote_count":"2","content":"Selected Answer: A\nA is correct. C also fulfills the technical requirement, but it is a business / regulatory requirement to not store the PII data at all.","comment_id":"659161","timestamp":"1677932160.0","poster":"jabrrJ68w02ond1"},{"comment_id":"653208","content":"Selected Answer: A\nI will go with A. IMO, key to decipher is the wording in the question, \" Process and Store data \". Dataflow (process) comes first and Store (Storage) comes next.","poster":"pp0709","upvote_count":"2","timestamp":"1677550380.0"},{"content":"Selected Answer: A\nWe are not allowed to store the data because of the PII. We have to get it directly from source, process it (remove PII) then store","timestamp":"1676919120.0","poster":"faagee01","upvote_count":"4","comment_id":"649499"},{"poster":"JohnnyBG","timestamp":"1675768020.0","comment_id":"643648","content":"Selected Answer: A\nA, it can't be C since you will have the PII on cloud storage which is not not conform to the statement in the question.","upvote_count":"2"},{"content":"The question said that \"You need to process and store the data\" so. C and D are no good\nQuestion also said that \"without storing any of the PIIE data\", and B said \"store all PII data\"\n\nSo answer is A","comment_id":"636055","poster":"gardislan18","timestamp":"1674572520.0","upvote_count":"2"},{"poster":"exam9391","comment_id":"634443","content":"Selected Answer: A\nA is ok","timestamp":"1674293940.0","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: A\nC is poor option and PI violation. The first stage: \"upload data on Cloud Storage\" is storing personal data. Go for A","comment_id":"629205","timestamp":"1673280780.0","poster":"Nirca"},{"content":"Selected Answer: A\nThe correct answer is A.\nOption C increases the surface of attack by uploading PII data to Cloud Storage bucket. Bucket Lock is for data retention (it does not offer protection against unauthorised access).","poster":"JoeThach","upvote_count":"1","timestamp":"1671409800.0","comment_id":"618433"},{"timestamp":"1670850660.0","content":"Selected Answer: A\nOption-A is correct. Although Option-C sounds good, ultimately we should not store PI data at all as per question says.","poster":"H_S","upvote_count":"1","comment_id":"615275"},{"content":"Selected Answer: A\nCreate a Dataflow pipeline to retrieve the data from the external sources, he did not specify the way he is going to create it, it might be a pub/sub or external table or whatever.\nBut for sure A is the only option where PII is removed.","poster":"H_S","comment_id":"614723","timestamp":"1670714220.0","upvote_count":"1"},{"comment_id":"612382","poster":"nhadi82","timestamp":"1670341500.0","upvote_count":"1","content":"Selected Answer: A\nAs not allowed to store data on GCP"},{"content":"Selected Answer: A\nask is NOT to store PII data","upvote_count":"1","poster":"Superr","timestamp":"1669897320.0","comment_id":"610105"},{"upvote_count":"1","comment_id":"608756","timestamp":"1669733820.0","poster":"Superr","content":"Selected Answer: A\nA is correct, because Q says 0 we should not store data"},{"poster":"amxexam","content":"Selected Answer: A\nDLP = Pll data cleahing.","upvote_count":"1","timestamp":"1669121880.0","comment_id":"605366"},{"comment_id":"599948","upvote_count":"1","timestamp":"1668158940.0","content":"Selected Answer: A\nC will store PII on GCS , hence A.","poster":"ryzior"},{"timestamp":"1666885440.0","content":"Selected Answer: A\nIt seems obvious to me that is A because with C we are storing PI data.","comment_id":"593275","poster":"Nick89GR","upvote_count":"1"},{"upvote_count":"1","comment_id":"591356","content":"Selected Answer: A\nWhy not C?\nQ: \"(...) You need to process and store the data without storing any of the PIIE data (...)\"\nC -> \"(...) upload all data on Cloud Storage. Configure Bucket Lock for the bucket. (...)\" -> you've just stored PII in Cloud Storage","timestamp":"1666668540.0","poster":"mad314"},{"comment_id":"590281","upvote_count":"1","timestamp":"1666480260.0","content":"Selected Answer: A\nA looks correct. C, does store PII data in GCS. So, A is the only right answer.","poster":"brvinod"},{"upvote_count":"1","content":"Selected Answer: A\nBecause C does not tell how will the data be moved to GCS and other point is this option will give you data in clean text on GCS, which not good","timestamp":"1666340820.0","poster":"nkit","comment_id":"589209","comments":[{"timestamp":"1668673680.0","content":"Yeah exactly, C didn't explain what will the data be processed after being upload to the GCS, just let all of these PII information expose on GCS without any DLP processing on it?","poster":"JoeyCASD","upvote_count":"1","comment_id":"602808"}]},{"timestamp":"1664537460.0","poster":"slars2k","comment_id":"578814","content":"Except A, in all other options the PII data getting stored in one of the storage options, which is not the intended objective. So, A is the correct answer","upvote_count":"2"},{"poster":"osanchez","comment_id":"575849","upvote_count":"2","content":"Answer C\nDataflow SQL can query the following sources:\n\nStreaming data from Pub/Sub topics\nStreaming and batch data from Cloud Storage filesets\nBatch data from BigQuery tables","timestamp":"1664234760.0"},{"content":"Asking external partners is not practicable. Let’s say you have 10 different partners. Setting up partners & convincing them is going to take months & they will charge money as that’s an effort, contract renewals, legal etc etc. so, don’t even think of it as a Choice.","upvote_count":"1","comment_id":"575673","timestamp":"1664205420.0","poster":"SAMBIT"},{"comment_id":"570645","poster":"gcmrjbr","content":"My vote is A.","timestamp":"1663511220.0","upvote_count":"1"},{"comment_id":"570644","poster":"gcmrjbr","upvote_count":"1","content":"C stores PII and question restrict this.","timestamp":"1663511160.0"},{"timestamp":"1662170520.0","poster":"timotei","content":"A\nI would choose C if dataflow cannot be connected to external services is the problem, but you will store all PII data on a bucket in C.","comment_id":"559806","upvote_count":"1"},{"content":"Selected Answer: A\nAnswer should be A., The answer C is not possible as there is a bucket lock which will not allow to modify any of the data , so you cant remove any PII related data as mentioned.","comment_id":"559785","timestamp":"1662164700.0","upvote_count":"1","poster":"googlearch"},{"content":"Selected Answer: C\nAgree with C. Dataflow SQL can query the following sources:\n- Streaming data from Pub/Sub topics\n- Streaming and batch data from Cloud Storage filesets\n- Batch data from BigQuery tables\nhttps://cloud.google.com/dataflow/docs/guides/sql/data-sources-destinations","comment_id":"551805","poster":"mesodan","upvote_count":"1","timestamp":"1660994760.0"},{"timestamp":"1660313880.0","poster":"jithkkl","comment_id":"545992","upvote_count":"4","content":"Selected Answer: C\nlooks like C is the correct answer \"https://cloud.google.com/architecture/running-automated-dataflow-pipeline-de-identify-pii-dataset\""},{"upvote_count":"1","timestamp":"1660058760.0","poster":"muky31dec","comment_id":"543959","content":"A is correct"},{"poster":"Pime13","timestamp":"1659011520.0","upvote_count":"3","content":"Selected Answer: C\nI believe this question is incomplete","comment_id":"534769"},{"poster":"KevPinto","timestamp":"1658669280.0","comment_id":"531406","upvote_count":"4","content":"C -- Dataflow SQL can query the following sources:\n\nStreaming data from Pub/Sub topics\nStreaming and batch data from Cloud Storage filesets\nBatch data from BigQuery tables\n\nSo we need the data in the Cloud for DataFlow to Process -- Hence C"},{"poster":"Pime13","timestamp":"1658582460.0","content":"Selected Answer: C\nvote c: https://cloud.google.com/dataflow/docs/guides/sql/data-sources-destinations","upvote_count":"4","comment_id":"530619"},{"poster":"GCPCloudArchitectUser","content":"Big Query dataset comes with two categories (internal and external).. I would take the assumption that external sources can be connected with external dataset of BigQuery","comment_id":"527860","timestamp":"1658252640.0","upvote_count":"2"},{"content":"Selected Answer: C\nCorrect Answer is C","poster":"GauravLahoti","timestamp":"1657871640.0","comment_id":"524061","upvote_count":"3"},{"poster":"Moss2011","comment_id":"523234","timestamp":"1657753680.0","upvote_count":"4","content":"Selected Answer: C\nIf you read about it, dataflow cannot be connected to external services although they were public, but is personal data. I think it should be C"},{"poster":"victory108","content":"A. Create a Dataflow pipeline to retrieve the data from the external sources. As part of the pipeline, use the Cloud Data Loss Prevention (Cloud DLP) API to remove any PII data. Store the result in BigQuery.","comment_id":"514355","upvote_count":"2","timestamp":"1656642900.0"},{"timestamp":"1656561000.0","upvote_count":"1","content":"I am inclined to A but not sure Dataflow pipeline can connect to external sources directly?","poster":"GMats","comment_id":"513902"}],"unix_timestamp":1640658420,"timestamp":"2021-12-28 03:27:00","choices":{"D":"Ask the external partners to import all data in your BigQuery dataset. Create a dataflow pipeline to copy the data into a new table. As part of the Dataflow bucket, skip all data in columns that have PII data","B":"Create a Dataflow pipeline to retrieve the data from the external sources. As part of the pipeline, store all non-PII data in BigQuery and store all PII data in a Cloud Storage bucket that has a retention policy set.","A":"Create a Dataflow pipeline to retrieve the data from the external sources. As part of the pipeline, use the Cloud Data Loss Prevention (Cloud DLP) API to remove any PII data. Store the result in BigQuery.","C":"Ask the external partners to upload all data on Cloud Storage. Configure Bucket Lock for the bucket. Create a Dataflow pipeline to read the data from the bucket. As part of the pipeline, use the Cloud Data Loss Prevention (Cloud DLP) API to remove any PII data. Store the result in BigQuery."},"question_text":"You are working with a data warehousing team that performs data analysis. The team needs to process data from external partners, but the data contains personally identifiable information (PII). You need to process and store the data without storing any of the PIIE data. What should you do?","answer_ET":"A","url":"https://www.examtopics.com/discussions/google/view/68685-exam-professional-cloud-architect-topic-1-question-174/","exam_id":4},{"id":"ZgzDDdYKOVvturoAdapY","question_images":[],"isMC":true,"answer_ET":"A","answer_images":[],"question_text":"You want to allow your operations team to store logs from all the production projects in your Organization, without including logs from other projects. All of the production projects are contained in a folder. You want to ensure that all logs for existing and new production projects are captured automatically. What should you do?","discussion":[{"timestamp":"1640686260.0","comment_id":"510984","content":"The correct answer is A","upvote_count":"16","poster":"simbu1299"},{"poster":"Atanu","comment_id":"910813","content":"The admin must have failed this exam multiple times. How can one select option B here.","timestamp":"1685509860.0","upvote_count":"9"},{"comment_id":"1336418","timestamp":"1736003100.0","poster":"plumbig11","content":"Selected Answer: A\nAggregated export on PRODUCTION FOLDER and the sink in cloud storage.","upvote_count":"1"},{"comment_id":"1258671","timestamp":"1722410160.0","upvote_count":"1","content":"Selected Answer: B\nCreating an aggregated export at the organization level and setting the log sink to a Cloud Storage bucket in an operations project ensures comprehensive, automatic, and centralized log capture for all existing and new production projects. Therefore, Option B is the correct choice.","poster":"lucaluca1982"},{"poster":"Namshru","comment_id":"1146901","upvote_count":"2","timestamp":"1707612780.0","content":"Correct Answer is A.Looks like most of tje Admin answers are all incorrect to avoid sharing PDF to others."},{"upvote_count":"4","timestamp":"1678391160.0","poster":"CGS22","comment_id":"834343","content":"Selected Answer: A\nThe correct answer is: A. Create an aggregated export on the Production folder. Set the log sink to be a Cloud Storage bucket in an operations project.\n\nThis solution will allow the operations team to store logs from all the production projects in your Organization, without including logs from other projects. All of the production projects are contained in a folder, so you can create an aggregated export on the Production folder. You can then set the log sink to be a Cloud Storage bucket in an operations project. This will allow the operations team to store all of the logs from the production projects in one place."},{"comment_id":"757277","timestamp":"1672044480.0","upvote_count":"3","poster":"omermahgoub","comments":[{"poster":"omermahgoub","comment_id":"757279","timestamp":"1672044480.0","upvote_count":"3","content":"Option C is not the correct solution because it requires you to create log exports in each production project, which can be time-consuming and error-prone. Additionally, setting the log sink to a Cloud Storage bucket in an operations project will not automatically capture logs for new production projects.\n\nOption D is not the correct solution because it requires you to create log exports in each production project, which can be time-consuming and error-prone. Additionally, storing the logs in BigQuery datasets in the production projects will not allow the operations team to easily access the logs. Instead, they would need to be granted IAM access to run queries on the datasets."}],"content":"Option B is not the correct solution because it creates an aggregated export on the Organization resource, which will capture logs from all projects in the Organization, including those outside the Production folder.\n\nThe best option to achieve the desired result is to create an aggregated export on the Production folder. Set the log sink to be a Cloud Storage bucket in an operations project. This will allow the operations team to store logs from all the production projects in the Organization, without including logs from other projects. Additionally, this setup will automatically capture logs for existing and new production projects.\n\nOption A is the correct solution because it allows you to create an aggregated export on the Production folder, which will capture logs from all the production projects contained in the folder. Setting the log sink to a Cloud Storage bucket in an operations project will allow the operations team to store the logs in a central location."},{"content":"Selected Answer: A\nA is ok","poster":"megumin","comment_id":"721155","timestamp":"1668758460.0","upvote_count":"1"},{"upvote_count":"6","content":"Selected Answer: A\nA is the right answer https://cloud.google.com/logging/docs/export/aggregated_sinks","comment_id":"699472","timestamp":"1666229880.0","poster":"Mahmoud_E"},{"timestamp":"1665860520.0","comment_id":"695631","upvote_count":"1","content":"I will choose A as right answer","poster":"AzureDP900"},{"timestamp":"1663166820.0","poster":"zellck","upvote_count":"3","comment_id":"669117","content":"Selected Answer: A\nA is the answer.\n\nhttps://cloud.google.com/logging/docs/export/aggregated_sinks\nAggregated sinks combine and route log entries from the Google Cloud resources contained by an organization or folder."},{"content":"Selected Answer: A\nA, is the only one that creates the policy in the Production Folder.\nHence doing what the question says \"make sure existing / all future projects gets automatically logs sabed\"","poster":"alexandercamachop","comment_id":"666495","upvote_count":"3","timestamp":"1662937440.0"},{"timestamp":"1658389320.0","upvote_count":"3","content":"Selected Answer: A\nA is ok","comment_id":"634447","poster":"exam9391"},{"comment_id":"582570","content":"Selected Answer: A\nThe correct answer is A","timestamp":"1649355480.0","poster":"cloudmon","upvote_count":"3"},{"timestamp":"1648974360.0","comments":[{"poster":"9xnine","content":"Projects vs. folder. IF you create the export on the folder it will apply to all new projects under that folder.","upvote_count":"10","timestamp":"1654647180.0","comments":[{"upvote_count":"2","content":"Now this is a really cool feature, thanks for the explanation!","timestamp":"1662286680.0","poster":"jabrrJ68w02ond1","comment_id":"659163"}],"comment_id":"612980"}],"comment_id":"580162","content":"Anyone can explain the difference between A and C? Both options look similar...","poster":"kinghin","upvote_count":"1"},{"comment_id":"540223","upvote_count":"2","timestamp":"1643955420.0","content":"\"without including logs from other projects. \" << Chose A as answer","poster":"AsadZaidi"},{"comment_id":"524176","upvote_count":"5","timestamp":"1642252140.0","content":"Selected Answer: A\nDon't understand why we need to choose at organization level, Please explain if B is correct","poster":"kongae"},{"upvote_count":"9","content":"Selected Answer: A\nA is the right answer.","poster":"technodev","timestamp":"1642129200.0","comment_id":"523291"},{"timestamp":"1641721140.0","comment_id":"520073","upvote_count":"1","content":"'All of the production projects are contained in a folder.'\n 'existing and new production projects are captured automatically'\nA can both.","poster":"OrangeTiger"},{"comment_id":"518254","timestamp":"1641476460.0","poster":"AJapieGuru","content":"Go for A","upvote_count":"1"},{"timestamp":"1641455040.0","poster":"esnecho","content":"Answer A:","comment_id":"518036","upvote_count":"1"},{"poster":"[Removed]","comment_id":"515919","content":"Selected Answer: A\nA is right answer.\nYou have to aggregate logs at Project Level not at Org level. Marked B is wrong.","upvote_count":"2","timestamp":"1641228840.0"},{"comment_id":"514353","upvote_count":"1","poster":"victory108","content":"A. Create an aggregated export on the Production folder. Set the log sink to be a Cloud Storage bucket in an operations project.","timestamp":"1641011700.0"},{"timestamp":"1640765580.0","comment_id":"511951","upvote_count":"3","content":"Selected Answer: A\nA is more likely, because clearly it's stated in the question that they are interested in \"Production\" folder/projects logging, not the entire organization.","poster":"BattleSlim"},{"timestamp":"1640692200.0","content":"Selected Answer: B\nB is correct;\nSinks control how Cloud Logging routes logs. Using sinks, you can route some or all of your logs to supported destinations.\n\nSinks belong to a given Google Cloud resource: Cloud projects, billing accounts, folders, and organizations. When the resource receives a log entry, it routes the log entry according to the sinks contained by that resource. The log entry is sent to the destination associated with each matching sink.\n\nhttps://cloud.google.com/logging/docs/export/configure_export_v2","comment_id":"511053","poster":"edilramos","upvote_count":"1","comments":[{"comment_id":"527171","content":"The text you quoted include \"folders\". So isn't the A more accurate answer?","timestamp":"1642559460.0","poster":"sjmsummer","upvote_count":"2"}]},{"upvote_count":"2","poster":"StelSen","comment_id":"510747","content":"Option-A seems correct. https://cloud.google.com/logging/docs/export/aggregated_sinks","timestamp":"1640658780.0"}],"exam_id":4,"choices":{"C":"Create log exports in the production projects. Set the log sinks to be a Cloud Storage bucket in an operations project.","D":"Create log exports in the production projects. Set the log sinks to be BigQuery datasets in the production projects, and grant IAM access to the operations team to run queries on the datasets.","B":"Create an aggregated export on the Organization resource. Set the log sink to be a Cloud Storage bucket in an operations project.","A":"Create an aggregated export on the Production folder. Set the log sink to be a Cloud Storage bucket in an operations project."},"answer_description":"","timestamp":"2021-12-28 03:33:00","answers_community":["A (96%)","4%"],"question_id":85,"answer":"A","url":"https://www.examtopics.com/discussions/google/view/68686-exam-professional-cloud-architect-topic-1-question-175/","topic":"1","unix_timestamp":1640658780}],"exam":{"isBeta":false,"name":"Professional Cloud Architect","lastUpdated":"11 Apr 2025","numberOfQuestions":279,"isMCOnly":false,"id":4,"provider":"Google","isImplemented":true},"currentPage":17},"__N_SSP":true}