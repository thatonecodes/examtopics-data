{"pageProps":{"questions":[{"id":"6YrwXZNTTIeNZcvBuZgr","timestamp":"2019-11-11 14:49:00","isMC":true,"question_images":[],"choices":{"B":"Implement retry logic using a truncated exponential backoff strategy.","D":"Monitor https://status.cloud.google.com/feed.atom and only make requests if Cloud Storage is not reporting an incident.","A":"Use gRPC instead of HTTP for better performance.","C":"Make sure the Cloud Storage bucket is multi-regional for geo-redundancy."},"topic":"1","answers_community":["B (100%)"],"answer_images":[],"question_id":196,"answer_ET":"B","url":"https://www.examtopics.com/discussions/google/view/7962-exam-professional-cloud-architect-topic-1-question-95/","answer":"B","answer_description":"","question_text":"You have an application that makes HTTP requests to Cloud Storage. Occasionally the requests fail with HTTP status codes of 5xx and 429.\nHow should you handle these types of errors?","unix_timestamp":1573480140,"exam_id":4,"discussion":[{"comment_id":"20535","content":"Answer is B\nYou should use exponential backoff to retry your requests when receiving errors with 5xx or 429 response codes from Cloud Storage.\nhttps://cloud.google.com/storage/docs/request-rate","poster":"bigob4ek","upvote_count":"43","timestamp":"1620733740.0","comments":[{"upvote_count":"1","poster":"nitinz","comment_id":"303821","content":"It is B","timestamp":"1662335880.0"},{"upvote_count":"1","poster":"AzureDP900","content":"I agree with you, B should be right","comment_id":"696325","timestamp":"1713280560.0"}]},{"timestamp":"1709879520.0","content":"HTTP 408, 429, and 5xx response codes.\n\nExponential backoff algorithm\nFor requests that meet both the response and idempotency criteria, you should generally use truncated exponential backoff.\n\nTruncated exponential backoff is a standard error handling strategy for network applications in which a client periodically retries a failed request with increasing delays between requests.\n\nAn exponential backoff algorithm retries requests exponentially, increasing the waiting time between retries up to a maximum backoff time. See the following workflow example to learn how exponential backoff works:\n\nYou make a request to Cloud Storage.\n\nIf the request fails, wait 1 + random_number_milliseconds seconds and retry the request.\n\nIf the request fails, wait 2 + random_number_milliseconds seconds and retry the request.\n\nIf the request fails, wait 4 + random_number_milliseconds seconds and retry the request.\n\nAnd so on, up to a maximum_backoff time.\n\nContinue waiting and retrying up to a maximum amount of time (deadline), but do not increase the maximum_backoff wait period between retries","comment_id":"663101","poster":"Sbgani","upvote_count":"13"},{"poster":"JonathanSJ","comment_id":"1333914","content":"Selected Answer: B\nI will go for B.","timestamp":"1735532580.0","upvote_count":"1"},{"timestamp":"1719043080.0","comments":[{"content":"Option A, using gRPC instead of HTTP for better performance, is not directly related to handling HTTP status codes of 5xx and 429. gRPC is a high-performance RPC framework that can be used in place of HTTP, but it is not a solution for handling errors.\n\nOption C, making sure the Cloud Storage bucket is multi-regional for geo-redundancy, may help improve the reliability of the service, but it is not a solution for handling errors.\n\nOption D, monitoring https://status.cloud.google.com/feed.atom and only making requests if Cloud Storage is not reporting an incident, is not a practical solution for handling errors. This approach would require constantly monitoring the status page and could result in significant delays in processing requests. Instead, it is generally recommended to implement retry logic in your application to handle errors.","comment_id":"753171","poster":"omermahgoub","upvote_count":"2","timestamp":"1719043080.0"}],"content":". Implement retry logic using a truncated exponential backoff strategy.\n\nHTTP status codes of 5xx and 429 typically indicate that there is a temporary issue with the service or that the rate of requests is too high. To handle these types of errors, it is generally recommended to implement retry logic in your application using a truncated exponential backoff strategy.\n\nTruncated exponential backoff involves retrying the request after an initial delay, and then increasing the delay exponentially for each subsequent retry up to a maximum delay. This approach helps to reduce the number of failed requests and can improve the reliability of your application.","comment_id":"753170","poster":"omermahgoub","upvote_count":"4"},{"comment_id":"715070","timestamp":"1715324520.0","upvote_count":"1","content":"Selected Answer: B\nB is ok","poster":"megumin"},{"poster":"Sbgani","content":"Selected Answer: B\nhttps://cloud.google.com/storage/docs/retry-strategy","upvote_count":"2","timestamp":"1709879460.0","comment_id":"663100"},{"upvote_count":"2","timestamp":"1707068640.0","poster":"DrishaS4","comment_id":"642514","content":"Selected Answer: B\n2xx – successful requests;\n4xx, 5xx – failed requests;\n3xx – requests that require redirect.\nhttps://cloud.google.com/storage/docs/json_api/v1/status-codes"},{"upvote_count":"2","poster":"haroldbenites","content":"Go for B","timestamp":"1686239460.0","comment_id":"497014"},{"poster":"vincy2202","timestamp":"1685291520.0","comment_id":"489343","content":"B is the correct answer","upvote_count":"2"},{"timestamp":"1685148060.0","comment_id":"487800","upvote_count":"2","content":"Selected Answer: B\nVote B","poster":"nqthien041292"},{"upvote_count":"1","comment_id":"486346","content":"Selected Answer: B\nvote B","poster":"joe2211","timestamp":"1684967340.0"},{"content":"B – Implement retry logic using a truncated exponential backoff strategy.\nPer HTTP status and error codes for JSON the status codes are:\n2xx – successful requests;\n4xx, 5xx – failed requests;\n3xx – requests that require redirect.\nhttps://cloud.google.com/storage/docs/json_api/v1/status-codes\n429 – Too many requests: your app tries to use more that its limit, additional requests will fail. Decrease your client’s requests and/or use truncated exponential backoff (used for all requests with 5xx and 429 errors).\nhttps://cloud.google.com/storage/docs/retry-strategy","poster":"MaxNRG","comment_id":"470083","timestamp":"1682832120.0","upvote_count":"2"},{"poster":"victory108","timestamp":"1668867960.0","comments":[{"upvote_count":"1","content":"This B. Implement retry logic using a truncated exponential backoff strategy.","timestamp":"1668868080.0","poster":"victory108","comment_id":"361371"}],"comment_id":"361369","content":"B. Use Deployment Manager to automate service provisioning. Use Stackdriver to monitor and debug your tests.","upvote_count":"2"},{"content":"Answer is B.\nLink provided by bigob4ek has details","comment_id":"358664","upvote_count":"1","timestamp":"1668607080.0","poster":"un"},{"content":"Answer is B","comment_id":"325593","timestamp":"1664607360.0","upvote_count":"1","poster":"Ausias18"},{"content":"As per google, if you run into any issue as increase latency or erroe rate ,pause your ramp up this give cloudstorage more time to scale your bucket . Best is backoff when 5xx ,429,408 response code","upvote_count":"2","comment_id":"291144","poster":"CloudGenious","timestamp":"1660574640.0"},{"content":"https://cloud.google.com/storage/docs/exponential-backoff\n- B","timestamp":"1659207780.0","upvote_count":"2","poster":"bnlcnd","comment_id":"280181"},{"timestamp":"1649394900.0","content":"B is correct","upvote_count":"1","comment_id":"195753","poster":"awadheshk"},{"content":"Answer: B","comment_id":"180615","timestamp":"1647483060.0","upvote_count":"1","poster":"AshokC"},{"comment_id":"165342","upvote_count":"1","content":"B is correct","poster":"wiqi","timestamp":"1645724820.0"},{"timestamp":"1640277540.0","comment_id":"117554","upvote_count":"1","poster":"mlantonis","content":"Yeah B is correct"},{"comment_id":"114766","timestamp":"1640010120.0","poster":"Tushant","content":"B is the right answer","upvote_count":"1"},{"content":"Using Exponential Backoff Strategy.","comment_id":"106704","upvote_count":"1","poster":"gfhbox0083","timestamp":"1639139580.0"},{"comment_id":"103294","timestamp":"1638730800.0","content":"B is the right answer\nhttps://cloud.google.com/iot/docs/how-tos/exponential-backoff\nTruncated exponential backoff is a standard error-handling strategy for network applications. In this approach, a client periodically retries a failed request with increasing delays between requests. Clients should use truncated exponential backoff for all requests to Cloud IoT Core that return HTTP 5xx and 429 response codes, a well as for disconnections from the MQTT server.","upvote_count":"4","poster":"Ziegler"},{"upvote_count":"2","poster":"AD2AD4","comment_id":"97531","timestamp":"1638109380.0","content":"Final Decision to go with Option B"},{"upvote_count":"4","timestamp":"1637771940.0","comment_id":"94965","poster":"Ziegler","content":"B is correct for this reason\nhttps://cloud.google.com/storage/docs/exponential-backoff"},{"poster":"skywalker","timestamp":"1636809960.0","comment_id":"88227","content":"\"B\" \nhttps://cloud.google.com/storage/docs/exponential-backoff","upvote_count":"2"}]},{"id":"Fn14CvrR92LQlmZ15yP6","exam_id":4,"unix_timestamp":1571915880,"isMC":true,"topic":"1","answer_ET":"B","answers_community":["B (89%)","11%"],"answer_description":"","question_images":[],"answer_images":[],"question_text":"You need to develop procedures to test a disaster plan for a mission-critical application. You want to use Google-recommended practices and native capabilities within GCP.\nWhat should you do?","timestamp":"2019-10-24 13:18:00","url":"https://www.examtopics.com/discussions/google/view/7125-exam-professional-cloud-architect-topic-1-question-96/","answer":"B","choices":{"D":"Use gcloud scripts to automate service provisioning. Use Stackdriver to monitor and debug your tests.","B":"Use Deployment Manager to automate service provisioning. Use Stackdriver to monitor and debug your tests.","C":"Use gcloud scripts to automate service provisioning. Use Activity Logs to monitor and debug your tests.","A":"Use Deployment Manager to automate service provisioning. Use Activity Logs to monitor and debug your tests."},"discussion":[{"poster":"crypt0","comments":[{"upvote_count":"12","timestamp":"1646445540.0","comment_id":"303823","content":"It is B, Google Best practice ---> never use scripts. They do not trust anyone else's code it seems.","poster":"nitinz"},{"timestamp":"1642444320.0","upvote_count":"2","comment_id":"269728","content":"https://cloud.google.com/solutions/dr-scenarios-planning-guide#test_your_plan_regularly","poster":"fraloca"},{"timestamp":"1635780840.0","content":"B is correct","comment_id":"210567","upvote_count":"2","poster":"kumarp6"},{"poster":"tartar","upvote_count":"11","comment_id":"152423","timestamp":"1628324100.0","content":"B is ok"}],"timestamp":"1603538280.0","content":"I think answer B is correct:\nhttps://cloud.google.com/solutions/dr-scenarios-planning-guide","upvote_count":"54","comment_id":"17156"},{"upvote_count":"23","poster":"passnow","comment_id":"30771","content":"Boom, everyone studied and did their labs, stackdriver is google's recommended tool for monitoring and debbuging. I agree with u all that B is the correct answer","timestamp":"1608323460.0"},{"poster":"JonathanSJ","comment_id":"1333915","timestamp":"1735532580.0","upvote_count":"1","content":"Selected Answer: B\nI will go for B."},{"comment_id":"1003197","content":"Selected Answer: D\nD its correct cause are 3 multiregions-availables and one bucket only can deploy in one multi region https://cloud.google.com/storage/docs/locations?hl=es-419#location-mr","poster":"duzapo","timestamp":"1725885180.0","upvote_count":"1"},{"timestamp":"1723901160.0","upvote_count":"1","comment_id":"983675","content":"Selected Answer: B\nI think B is the correct answer","poster":"jalberto"},{"upvote_count":"2","content":"Why not A?","poster":"faridomu","comment_id":"921862","timestamp":"1718241120.0"},{"content":"Selected Answer: B\nDeploy managment + Stackdriver trained ig GCSB","poster":"Jlharidon","comment_id":"744143","timestamp":"1702478340.0","upvote_count":"1"},{"content":"Selected Answer: B\nB is ok","poster":"megumin","timestamp":"1699606980.0","comment_id":"715071","upvote_count":"1"},{"poster":"AzureDP900","timestamp":"1697469240.0","comment_id":"696323","content":"B is right","upvote_count":"1"},{"poster":"tycho","content":"in practice, D could work as well..","comment_id":"647171","timestamp":"1692101820.0","upvote_count":"1","comments":[{"comment_id":"799739","upvote_count":"1","content":"yeah, but only native solutions should be taken into consideration (as stated in requirements), so scripts are basically ruled out","poster":"adacek1","timestamp":"1707224520.0"}]},{"comment_id":"582344","poster":"gaojun","content":"Answer B is correct","upvote_count":"1","timestamp":"1680861360.0"},{"poster":"Skr6266","content":"Selected Answer: B\nDeployment Manager + Cloud Monitoring and Logging solution.","timestamp":"1677862140.0","upvote_count":"1","comment_id":"560229"},{"poster":"haroldbenites","timestamp":"1670521980.0","comment_id":"497019","content":"Go for B","upvote_count":"1"},{"timestamp":"1669774140.0","content":"Selected Answer: B\nB is the correct answer","comment_id":"490341","upvote_count":"1","poster":"vincy2202"},{"comment_id":"487816","content":"Selected Answer: B\nVote B","poster":"nqthien041292","upvote_count":"1","timestamp":"1669518120.0"},{"upvote_count":"1","timestamp":"1668785280.0","comment_id":"480811","content":"Selected Answer: B\nGoogle recommended Practice","poster":"ganeshrev"},{"timestamp":"1652963160.0","comment_id":"361367","content":"B. Use Deployment Manager to automate service provisioning. Use Stackdriver to monitor and debug your tests.","upvote_count":"4","poster":"victory108"},{"timestamp":"1652702340.0","upvote_count":"1","comment_id":"358668","content":"B is correct","poster":"un"},{"content":"B in the answer","upvote_count":"1","timestamp":"1651263900.0","comment_id":"345665","poster":"hero_india"},{"upvote_count":"1","content":"Answer is B","timestamp":"1648796220.0","comment_id":"325595","poster":"Ausias18"},{"content":"Answer is B","timestamp":"1645365840.0","poster":"BobBui","upvote_count":"2","comment_id":"295084"},{"poster":"doumx","upvote_count":"1","content":"B is correct","timestamp":"1638832020.0","comment_id":"236869"},{"content":"B should be correct because we use Logging and monitoring for it.","comment_id":"209669","upvote_count":"2","poster":"gcparchitect007","timestamp":"1635649380.0"},{"timestamp":"1634244240.0","content":"B is correct, use Deployment Manager for provisioning service and use SD logging and monitoring feature to debug test.","upvote_count":"1","poster":"Bharathy","comment_id":"200097"},{"upvote_count":"1","timestamp":"1631837520.0","comment_id":"180616","poster":"AshokC","content":"Answer: B"},{"poster":"wiqi","timestamp":"1629827160.0","content":"B is correct...","comment_id":"165427","upvote_count":"1"},{"poster":"theWildOne","timestamp":"1626257340.0","content":"Wait, Activity Logs and not StackDriver? on GCP exam? Yeah right... its B FOR SURE","comment_id":"134739","upvote_count":"2"},{"content":"Stackdriver provides a better solution.\n\nB is the correct","upvote_count":"2","timestamp":"1624459200.0","poster":"mlantonis","comment_id":"117558"},{"comment_id":"114767","timestamp":"1624191720.0","poster":"Tushant","content":"B is the right answer","upvote_count":"1"},{"timestamp":"1623732780.0","upvote_count":"1","comment_id":"110543","poster":"HD1803","content":"B is the correct amswer"},{"poster":"Ziegler","content":"B is the correct answer","upvote_count":"1","comment_id":"103295","timestamp":"1622912520.0"},{"comment_id":"100649","poster":"gfhbox0083","content":"B, for sure","upvote_count":"1","timestamp":"1622619000.0"},{"upvote_count":"1","poster":"AD2AD4","comment_id":"97532","content":"Final Decision to go with Option B","timestamp":"1622204640.0"},{"poster":"sssz","content":"B is better","timestamp":"1613442840.0","comment_id":"51078","upvote_count":"2"},{"poster":"sri007","comment_id":"39981","timestamp":"1610862600.0","upvote_count":"3","content":"B is correct"},{"timestamp":"1606518120.0","upvote_count":"4","comment_id":"24911","poster":"jobs","content":"Agree B"},{"upvote_count":"5","comment_id":"17305","poster":"KouShikyou","timestamp":"1603600920.0","content":"Agree."}],"question_id":197},{"id":"WmEFNR2BgwZQOjipPOGn","timestamp":"2019-10-11 14:58:00","question_images":[],"answer_ET":"D","isMC":true,"topic":"1","answer_images":[],"question_text":"Your company creates rendering software which users can download from the company website. Your company has customers all over the world. You want to minimize latency for all your customers. You want to follow Google-recommended practices.\nHow should you store the files?","exam_id":4,"url":"https://www.examtopics.com/discussions/google/view/6466-exam-professional-cloud-architect-topic-1-question-97/","answer":"D","question_id":198,"discussion":[{"comments":[{"upvote_count":"19","timestamp":"1652768100.0","content":"What is point of Multi-Regional bucket, if this need to saved multiple times. I believe option (D) is for creating confusion only. It should be (A)..","comment_id":"602805","poster":"AmitAr","comments":[{"timestamp":"1717354320.0","comment_id":"1223249","content":"Let's try option A: you select a single multi-region bucket (e.g. Americas). Are you improving the latency of your clients in Asia? You do not.\n\nThus, Option A is not complete.","upvote_count":"2","poster":"JaimeMS"},{"timestamp":"1652768280.0","comment_id":"602806","upvote_count":"10","content":"Read the question again.. I think (d) is correct.. eg. 1 bucket in US-multi-region, 2nd in AS-multi-region, 3rd in EU-multi-region","comments":[{"comment_id":"847595","content":"Yes, D seems correct. There are 3 multi-regions: ASIA, EU and US. In order to be global, there must be multi-region buckets in this 3 locations.\n\nReference: https://cloud.google.com/storage/docs/locations#location-mr","poster":"giovanicascaes","upvote_count":"6","timestamp":"1679527920.0"}],"poster":"AmitAr"},{"poster":"turbo8p","upvote_count":"13","content":"Check the current create bucket UI. You cannot select Asia multi-region and US multi-region at the same go. So to support global customer, you need to create multiple Multi-region buckets.","comment_id":"720104","timestamp":"1668646200.0"}]},{"timestamp":"1639446180.0","poster":"Urban_Life","content":"This can't be D. It should be A.","upvote_count":"7","comment_id":"500985"},{"timestamp":"1689321600.0","upvote_count":"1","poster":"kilo10x","comment_id":"951355","content":"wrong its A"},{"content":"why \" multiple Multi-Regional\"? - A should be the right ans & addressing the global users - \"More importantly, is that multiregional heavily leverages Edge caching and CDNs to provide the content to the end user\"\nhttps://medium.com/google-cloud/google-cloud-storage-what-bucket-class-for-the-best-performance-5c847ac8f9f2","timestamp":"1577938680.0","comments":[{"comment_id":"428167","content":"because a multi-regional includes all the locations of ONE region, not the others.","timestamp":"1629465600.0","poster":"xavi1","upvote_count":"13"}],"comment_id":"34416","poster":"MyPractice","upvote_count":"14"}],"timestamp":"1573749960.0","content":"Its D, create multi region buckets in Americas, Europe and Asia","upvote_count":"65","poster":"JoeShmoe","comment_id":"21579"},{"timestamp":"1571389200.0","poster":"MeasService","content":"I would go with A (https://cloud.google.com/storage/docs/locations)","comment_id":"15870","upvote_count":"34"},{"comment_id":"1559181","poster":"francisco94","content":"Selected Answer: A\nWhy not the other options?\nB: Regional buckets only store data in a single region — this would increase latency for users far from that region.\n\nC: You can’t create buckets per zone; Cloud Storage is regional or multi-regional, not zonal.\n\nD: Managing multiple multi-regional buckets is unnecessary and adds complexity without performance benefits.","timestamp":"1744187640.0","upvote_count":"1"},{"timestamp":"1743002280.0","content":"Selected Answer: D\nIt's option D. Is not Google best practice, but it’s the only way to make it global as the answers don’t mention the use of Multi-Regional bucket + Cloud CDN or edge caching. So D is the only one.","poster":"apb98","upvote_count":"1","comment_id":"1410455"},{"content":"Selected Answer: A\nWhile Option D would work, it is not needed. A single multi-regional bucket handles this case, and is the google recomended practice. Also, it is more complex to manage multiple buckets.","comment_id":"1409534","timestamp":"1742789880.0","poster":"OnoPa","upvote_count":"1"},{"content":"Selected Answer: A\nWhen you need to serve content to users globally with minimal latency, a Multi-Regional Cloud Storage bucket is the recommended solution. It handles the distribution and redundancy for you.","comment_id":"1363876","timestamp":"1740906960.0","upvote_count":"1","poster":"halifax"},{"timestamp":"1740732420.0","upvote_count":"1","comment_id":"1362906","poster":"kahinah","content":"Selected Answer: A\nMulti-regional buckets replicate data across geographically dispersed regions within a continent (e.g., ASIA, EU, or US multi-regions), ensuring users download from the nearest available location"},{"timestamp":"1740142380.0","poster":"mselmi","upvote_count":"1","comment_id":"1359747","content":"Selected Answer: D\nA multi-region is a large geographic area, such as the United States, that contains two or more geographic places. https://cloud.google.com/storage/docs/locations"},{"upvote_count":"1","poster":"Jamee","content":"Selected Answer: A\nMulti-Regional Cloud Storage automatically replicates your data across multiple regions within a continent, providing low-latency access to customers worldwide. This ensures that no matter where your customers are located, they can retrieve the files from the nearest location, significantly reducing latency.","timestamp":"1739784240.0","comment_id":"1357710"},{"timestamp":"1735535100.0","content":"Selected Answer: D\nI will go for D because the clients are global, and the primary objective is minimizing latency for ALL.","comment_id":"1333921","upvote_count":"2","poster":"JonathanSJ"},{"upvote_count":"1","poster":"Amrx","content":"Selected Answer: D\nD is correct. Multi-region buckets are still specific to their own regional area, Americas, Europe and Asia. It's not A, doesn't cover the whole world.","timestamp":"1729776840.0","comment_id":"1302477"},{"content":"I go for A. I never came across\"multiple multi-region\"","upvote_count":"1","comment_id":"1300920","timestamp":"1729507620.0","poster":"hehe_24"},{"timestamp":"1717354380.0","upvote_count":"2","comment_id":"1223250","poster":"JaimeMS","content":"Selected Answer: D\nIts D, create multi region buckets in Americas, Europe and Asia"},{"upvote_count":"1","poster":"tlopsm","timestamp":"1716900180.0","content":"I think it is D. \nKeyword: Your company has customers all over the world.\nLists Multi Regional cloud\nMulti-Region Name Multi-Region Description\nASIA Data centers in Asia, excluding Hong Kong and Indonesia\nEU Data centers within member states of the European Union*\nUS Data centers in the United States\n\n\nAns A. suggests \"A\" multi-regional Cloud. that means one of the above multi-regional cloud\nAns D: suggests \"multiple\" Multi-Regional so 2 or (preferably) all of the multi-regional cloud with one bucket per multi-region (less task)","comment_id":"1220242"},{"comment_id":"1190839","upvote_count":"2","poster":"dija123","content":"Selected Answer: A\nAgree with A","timestamp":"1712475300.0"},{"comment_id":"1171360","poster":"5091a99","content":"Answer is A.\nAs for D: This would lead to data duplication and increased storage costs, as well as potential data consistency issues across different multi-regional buckets.","upvote_count":"1","timestamp":"1710201600.0"},{"content":"Selected Answer: A\nBehind Multi-Regional Cloud Storage bucket, CDN is used hence good option to use for software download service.","timestamp":"1708251180.0","upvote_count":"2","poster":"Amrita2012","comment_id":"1153183"}],"choices":{"A":"Save the files in a Multi-Regional Cloud Storage bucket.","D":"Save the files in multiple Multi-Regional Cloud Storage buckets, one bucket per multi-region.","B":"Save the files in a Regional Cloud Storage bucket, one bucket per zone of the region.","C":"Save the files in multiple Regional Cloud Storage buckets, one bucket per zone per region."},"unix_timestamp":1570798680,"answers_community":["D (61%)","A (28%)","11%"],"answer_description":""},{"id":"F8JkYTxmxX0HiOhNtsfs","unix_timestamp":1572275580,"question_images":[],"answer_images":[],"choices":{"D":"Store the data in Cloud Storage and run a nightly batch script that deletes all expired data.","C":"Store the data in Cloud Storage and use lifecycle management to delete files when they expire.","B":"Anonymize the data using the Cloud Data Loss Prevention API and store it indefinitely.","A":"Store the data in Google Drive and manually delete records as they expire."},"isMC":true,"answer_ET":"C","answers_community":["C (88%)","13%"],"question_id":199,"exam_id":4,"answer":"C","discussion":[{"timestamp":"1578840060.0","upvote_count":"23","poster":"AWS56","comment_id":"38117","content":"Agree C"},{"timestamp":"1743261720.0","poster":"Abhi_ma_cdl_22","upvote_count":"1","comment_id":"1411737","content":"Selected Answer: C\nC is the right answer"},{"poster":"desertlotus1211","upvote_count":"1","comments":[{"upvote_count":"1","poster":"Peto12","comment_id":"1330197","timestamp":"1734818040.0","content":"Because the requirement says 4 more years retention, not indefinitely."}],"content":"Selected Answer: B\nWhy not B? It's patients' health records...","comment_id":"1317828","timestamp":"1732578660.0"},{"comment_id":"822911","timestamp":"1677441900.0","content":"Selected Answer: C\nIt's C","upvote_count":"1","poster":"i_maddog_i"},{"comment_id":"715077","poster":"megumin","timestamp":"1668071460.0","upvote_count":"1","content":"Selected Answer: C\nC is ok"},{"comment_id":"696318","content":"I agree with C","timestamp":"1665932700.0","poster":"AzureDP900","upvote_count":"1"},{"content":"I got this question in exam.","comment_id":"646503","upvote_count":"4","poster":"ACE_ASPIRE","timestamp":"1660441320.0"},{"comment_id":"642667","timestamp":"1659662400.0","content":"Selected Answer: C\ngo for C","upvote_count":"1","poster":"DrishaS4"},{"content":"Options C undoubtedly","upvote_count":"1","timestamp":"1655660340.0","poster":"Dhiraj03","comment_id":"618769"},{"comment_id":"582349","timestamp":"1649325780.0","poster":"gaojun","content":"Go for C","upvote_count":"1"},{"comment_id":"545489","content":"I got similar question on my exam which involved life cycle management and bucket lock.","upvote_count":"3","poster":"[Removed]","timestamp":"1644611820.0"},{"content":"Selected Answer: C\nGo for C","poster":"Rajasa","comment_id":"510254","upvote_count":"1","timestamp":"1640603700.0"},{"poster":"haroldbenites","comment_id":"497063","upvote_count":"1","timestamp":"1638987240.0","content":"Go for C"},{"comment_id":"491402","timestamp":"1638346260.0","content":"Selected Answer: C\nC is the correct answer","poster":"vincy2202","upvote_count":"1"},{"poster":"gabrielzeven","timestamp":"1638277620.0","comment_id":"490692","content":"D sounds like i would do it, but C sound like a lab or exam","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: C\nVote C","comment_id":"487820","timestamp":"1637982300.0","poster":"nqthien041292"},{"upvote_count":"4","comment_id":"362100","timestamp":"1621511100.0","content":"C. Store the data in Cloud Storage and use lifecycle management to delete files when they expire.","poster":"victory108"},{"timestamp":"1621167300.0","comment_id":"358687","poster":"un","upvote_count":"1","content":"C is correct"},{"content":"Agree C","comment_id":"341597","poster":"Pdk123","upvote_count":"1","timestamp":"1619182980.0"},{"comments":[{"poster":"spuyol","upvote_count":"1","comment_id":"1092464","timestamp":"1702208340.0","content":"C is not the correct answer unless you take into account your note. C requires to develop a custom load."}],"comment_id":"334913","poster":"JohnWick2020","upvote_count":"4","timestamp":"1618345080.0","content":"Answer is C.\n\nNoteworthy if you are moving PHI from an on-prem source into cloud storage bucket, the object creation date recorded is the current date and not the original creation date as seen in on-prem source. To port original creation date you could script a function to write to the object metadata field called \"Custom time\" which is referenced in object lifecycle rules.\n\nSo to delete objects up to 4 years, you add an object lifecycle rule specifying the following form parameters:\nAction = \"Delete object\"\nObject conditions = select \"\"Days since custom time\" checkbox and specify 1460 days."},{"content":"Answer is C","comment_id":"330040","poster":"Ausias18","upvote_count":"1","timestamp":"1617769020.0"},{"upvote_count":"1","comments":[{"upvote_count":"2","content":"In a cloud term language called its a life cycle policy not a retention policy, retention policy is called in monolithic solution deployment Mate!","poster":"LoganIsh","timestamp":"1601877240.0","comment_id":"193378"}],"timestamp":"1601578380.0","comment_id":"191138","content":"None of the above. It must be an old question. You need a retention policy, not a life-cycle policy.","poster":"kimberjdaw"},{"comment_id":"180620","upvote_count":"1","timestamp":"1600302120.0","content":"C is right","poster":"AshokC"},{"comment_id":"126843","comments":[{"upvote_count":"2","timestamp":"1594082580.0","comments":[{"content":"question says \" as soon as\" not exactly after 4 years. Also if you have a older files, you can upload on Cloud Storage and use the life cycle management that it's easier","upvote_count":"3","timestamp":"1594213740.0","comment_id":"129776","poster":"pupi08"},{"comment_id":"163019","timestamp":"1598020200.0","upvote_count":"1","content":"If we use script also it will check for object creation time to delete file ? how script will come to know actual creation time of file ?","poster":"Krishna2401"},{"timestamp":"1599242040.0","content":"D says the same thing as C, you're still storing in cloud storage so your point is mute. its always best practice to use a native solution over creating a script.","upvote_count":"1","poster":"Carsonza","comment_id":"173543"},{"comment_id":"395333","upvote_count":"1","poster":"TotoroChina","content":"It mentioned \"depending on when it was created\", that will work with lifecycle management, which should be C.","timestamp":"1625100120.0"}],"comment_id":"128466","poster":"hafid","content":"agree, correct answer is D, the object is from the last company which have its own \"age\" if you put it now in GCS the \"age\" will not reflect the real \"age\" but it reflects GCS creation time which is wrong\n\nfrom google site\nAge: This condition is satisfied when an object reaches the specified age (in days). Age is measured from the object's creation time."}],"timestamp":"1593952620.0","upvote_count":"2","poster":"Musk","content":"For me it's D."},{"upvote_count":"2","timestamp":"1592924100.0","comment_id":"117568","poster":"mlantonis","content":"C is correct"},{"timestamp":"1591786140.0","comment_id":"106717","upvote_count":"2","poster":"gfhbox0083","content":"C, for sure."},{"comment_id":"103300","timestamp":"1591377000.0","upvote_count":"3","poster":"Ziegler","content":"C is correct"},{"content":"I agree with C.\nClues-- 4 years and delete after that\ncloud storage with life cycle management meets the requirement","timestamp":"1589716980.0","upvote_count":"3","poster":"GunjGupta","comment_id":"90529"},{"upvote_count":"4","comment_id":"62954","content":"Agree C","timestamp":"1584009360.0","poster":"Javed"},{"content":"https://cloud.google.com/storage/docs/lifecycle","comment_id":"17962","timestamp":"1572275580.0","poster":"Eroc","upvote_count":"2","comments":[{"comments":[{"timestamp":"1609808940.0","comment_id":"259839","content":"If you know how to write a script that will delete a specific set of objects when they expire, you can just as well write a script that will set the custom time on those objects when they are created, and then you can let the automatic expiration mechanism work and not have to worry about the script working for the next 4 or more years. C is the best option.","poster":"mwilbert","upvote_count":"1"}],"comment_id":"205508","timestamp":"1603612620.0","content":"I can see a custom_time metadata predefined attribute against object. And a lifecycle rule to delete based on custom_time. So will go with C","upvote_count":"2","poster":"dhanapaal"},{"upvote_count":"1","poster":"nitinz","comment_id":"303837","content":"It is C","timestamp":"1614910440.0"}]}],"answer_description":"","topic":"1","question_text":"Your company acquired a healthcare startup and must retain its customers' medical information for up to 4 more years, depending on when it was created. Your corporate policy is to securely retain this data, and then delete it as soon as regulations allow.\nWhich approach should you take?","timestamp":"2019-10-28 16:13:00","url":"https://www.examtopics.com/discussions/google/view/7376-exam-professional-cloud-architect-topic-1-question-98/"},{"id":"O52A8yTN7VqKrgM28MhM","exam_id":4,"discussion":[{"comments":[{"comment_id":"1071876","content":"Good job bro","upvote_count":"1","poster":"ArtistS","timestamp":"1731703200.0"}],"timestamp":"1639421760.0","content":"A dedicated memset is always better than shared until cost-effectiveness specify in the exam as objective. So Option C and D are ruled out. \n\nFrom A and B, Option B is sending and updating query every minutes which is over killing. So reasonable option left with A which balance performance and cost. \n\nMy answer will be A","poster":"hiteshrup","upvote_count":"30","comment_id":"242882"},{"comments":[{"timestamp":"1646446500.0","content":"A is correct","comment_id":"303838","upvote_count":"6","poster":"nitinz"},{"comment_id":"378298","content":"https://cloud.google.com/memorystore/docs/redis/redis-overview","timestamp":"1654780380.0","poster":"dlzhang","upvote_count":"2"},{"content":"A is ok","timestamp":"1628324760.0","upvote_count":"11","poster":"tartar","comment_id":"152429"}],"timestamp":"1603898280.0","content":"https://cloud.google.com/appengine/docs/standard/php/memcache/using","comment_id":"17963","upvote_count":"23","poster":"Eroc"},{"timestamp":"1744187940.0","upvote_count":"1","poster":"francisco94","comment_id":"1559183","content":"Selected Answer: A\nSince Memcache is deprecated in newer runtimes, you should:\n\n✅ Use Memorystore for Redis, a managed and scalable caching layer.\n❌ There's no \"service level\" to set anymore like with the legacy memcache — Redis handles performance and availability via tier selection (Basic or Standard)."},{"poster":"Abhi_ma_cdl_22","content":"Selected Answer: A\nA is correct","upvote_count":"1","comment_id":"1411739","timestamp":"1743262080.0"},{"poster":"Sur_Nikki","comment_id":"892662","timestamp":"1715229180.0","content":"Best is A","upvote_count":"2"},{"comment_id":"715080","timestamp":"1699607640.0","content":"Selected Answer: A\nA is ok","upvote_count":"1","poster":"megumin"},{"comment_id":"696314","timestamp":"1697468520.0","poster":"AzureDP900","content":"A is fine.. dedicated mem cache","upvote_count":"1"},{"comment_id":"646504","content":"I got this question in exam.","upvote_count":"5","poster":"ACE_ASPIRE","timestamp":"1691977440.0"},{"poster":"DrishaS4","comment_id":"642670","timestamp":"1691198580.0","upvote_count":"1","content":"Selected Answer: A\nhttps://cloud.google.com/appengine/docs/standard/php/memcache/using"},{"content":"Obviously, the answer is A","upvote_count":"1","comment_id":"582351","timestamp":"1680861900.0","poster":"gaojun"},{"poster":"ehgm","comment_id":"511296","upvote_count":"4","timestamp":"1672243440.0","content":"Selected Answer: A\nDedicated and shared will resolve the problem, the key is: store all queries in only one key \"cached_queries\" is not good, we have limits: https://cloud.google.com/appengine/docs/standard/python/memcache\nCreate a key of each query is better."},{"poster":"vincy2202","content":"A is the correct answer","upvote_count":"2","comment_id":"491404","timestamp":"1669882440.0"},{"poster":"nqthien041292","comment_id":"487821","timestamp":"1669518360.0","upvote_count":"1","content":"Selected Answer: A\nVote A"},{"content":"Selected Answer: A\nvote A","upvote_count":"1","poster":"joe2211","comment_id":"486647","timestamp":"1669380300.0"},{"upvote_count":"3","timestamp":"1653047100.0","poster":"victory108","comment_id":"362099","content":"A. Set the memcache service level to dedicated. Create a key from the hash of the query, and return database values from memcache before issuing a query to Cloud SQL."},{"timestamp":"1652703360.0","content":"A is correct","poster":"un","comment_id":"358690","upvote_count":"1"},{"upvote_count":"1","poster":"Ausias18","timestamp":"1649305080.0","content":"Answer is A","comment_id":"330041"},{"upvote_count":"1","timestamp":"1645636020.0","content":"A is correct","poster":"ga","comment_id":"297586"},{"upvote_count":"1","content":"My answer is A","timestamp":"1645366140.0","poster":"BobBui","comment_id":"295089"},{"timestamp":"1643588940.0","comment_id":"280250","poster":"bnlcnd","upvote_count":"7","content":"Due to \"Create a key called \"cached-queries\", C&D is wrong. You cannot have only one query. every query need a key.\nA vs B: run cron job every minute is not minimizing the call to DB. call DB only when cache missed. so, A is right."},{"poster":"BobirmirzoArs","upvote_count":"1","timestamp":"1636272780.0","comment_id":"214472","content":"I think A is right"},{"comment_id":"180624","content":"A is right","poster":"AshokC","upvote_count":"1","timestamp":"1631838960.0"},{"upvote_count":"2","timestamp":"1631024520.0","comment_id":"175265","content":"D, for sure","poster":"Tonyms04"},{"content":"A, for sure","poster":"gfhbox0083","timestamp":"1623480480.0","upvote_count":"3","comment_id":"108441"},{"timestamp":"1610462460.0","upvote_count":"8","content":"Agree A","comment_id":"38118","poster":"AWS56"}],"question_images":[],"unix_timestamp":1572275880,"answer":"A","choices":{"D":"Set the memcache service level to shared. Create a key called ג€cached_queriesג€, and return database values from the key before using a query to Cloud SQL.","B":"Set the memcache service level to dedicated. Create a cron task that runs every minute to populate the cache with keys containing query results.","C":"Set the memcache service level to shared. Create a cron task that runs every minute to save all expected queries to a key called ג€cached_queriesג€.","A":"Set the memcache service level to dedicated. Create a key from the hash of the query, and return database values from memcache before issuing a query to Cloud SQL."},"answer_description":"","timestamp":"2019-10-28 16:18:00","answer_ET":"A","question_id":200,"answer_images":[],"isMC":true,"answers_community":["A (100%)"],"url":"https://www.examtopics.com/discussions/google/view/7377-exam-professional-cloud-architect-topic-1-question-99/","topic":"1","question_text":"You are deploying a PHP App Engine Standard service with Cloud SQL as the backend. You want to minimize the number of queries to the database.\nWhat should you do?"}],"exam":{"isImplemented":true,"numberOfQuestions":279,"lastUpdated":"11 Apr 2025","id":4,"provider":"Google","isMCOnly":false,"name":"Professional Cloud Architect","isBeta":false},"currentPage":40},"__N_SSP":true}