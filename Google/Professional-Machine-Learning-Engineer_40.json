{"pageProps":{"questions":[{"id":"tSTJSO7qtVaNk1IFTmd9","discussion":[{"content":"Selected Answer: A\nSampled Shapley is a method suitable for XGBoost models. A lower path count (like 5) would indeed ensure lower latency in explanations, but might compromise on the precision of the explanations.Model Monitoring - Prediction Drift: This monitors the change in model predictions over time, which can indirectly indicate a change in feature attributions, but it's not directly monitoring the attributions themselves.","poster":"daidai75","upvote_count":"3","timestamp":"1721701500.0","comment_id":"1129192"},{"content":"Selected Answer: A\nnot B as integrated gradients is only for Custom-trained TensorFlow models that use a TensorFlow prebuilt container to serve predictions and AutoML image models","upvote_count":"3","comments":[{"timestamp":"1721035800.0","content":"https://cloud.google.com/vertex-ai/docs/explainable-ai/overview","poster":"shadz10","upvote_count":"1","comment_id":"1123286"}],"timestamp":"1721035740.0","comment_id":"1123284","poster":"shadz10"},{"poster":"36bdc1e","comment_id":"1122822","content":"A\nSampled Shapley is a fast and scalable approximation of the Shapley value, which is a game-theoretic concept that measures the contribution of each feature to the model prediction. Sampled Shapley is suitable for online prediction requests, as it can return feature attributions with minimal latency. The path count parameter controls the number of samples used to estimate the Shapley value, and a lower value means faster computation. Integrated Gradients is another explanation method that computes the average gradient along the path from a baseline input to the actual input. Integrated Gradients is more accurate than Sampled Shapley, but also more computationally intensive","timestamp":"1720982520.0","upvote_count":"4"},{"content":"Selected Answer: A\nExplanation Method:\n\nSampled Shapley: This method provides high-fidelity feature attributions while being computationally efficient, making it ideal for low-latency online predictions.\nIntegrated Gradients: While also accurate, it's generally more computationally intensive than sampled Shapley, potentially introducing latency.\nPath Count:\n\nLower Path Count (5): Reducing path count further decreases computation time, optimizing for faster prediction responses.\nMonitoring Objective:\n\nPrediction Drift: This type of monitoring detects changes in feature importance over time, aligning with the goal of tracking feature attribution shifts.\nTraining-Serving Skew: This monitors discrepancies between training and serving data distributions, which isn't directly related to feature attributions.","poster":"pikachu007","upvote_count":"4","comment_id":"1121809","timestamp":"1720878180.0"}],"answer":"A","question_id":196,"answer_images":[],"timestamp":"2024-01-13 16:43:00","answer_ET":"A","exam_id":13,"url":"https://www.examtopics.com/discussions/google/view/131112-exam-professional-machine-learning-engineer-topic-1-question/","answer_description":"","question_text":"You have trained an XGBoost model that you plan to deploy on Vertex AI for online prediction. You are now uploading your model to Vertex AI Model Registry, and you need to configure the explanation method that will serve online prediction requests to be returned with minimal latency. You also want to be alerted when feature attributions of the model meaningfully change over time. What should you do?","topic":"1","unix_timestamp":1705160580,"answers_community":["A (100%)"],"isMC":true,"question_images":[],"choices":{"C":"1. Specify sampled Shapley as the explanation method with a path count of 50.\n2. Deploy the model to Vertex AI Endpoints.\n3. Create a Model Monitoring job that uses training-serving skew as the monitoring objective.","B":"1. Specify Integrated Gradients as the explanation method with a path count of 5.\n2. Deploy the model to Vertex AI Endpoints.\n3. Create a Model Monitoring job that uses prediction drift as the monitoring objective.","A":"1. Specify sampled Shapley as the explanation method with a path count of 5.\n2. Deploy the model to Vertex AI Endpoints.\n3. Create a Model Monitoring job that uses prediction drift as the monitoring objective.","D":"1. Specify Integrated Gradients as the explanation method with a path count of 50.\n2. Deploy the model to Vertex AI Endpoints.\n3. Create a Model Monitoring job that uses training-serving skew as the monitoring objective."}},{"id":"DtE5BqbRLuLEiQYf9hBR","unix_timestamp":1707724920,"answer":"B","isMC":true,"question_text":"You work at a gaming startup that has several terabytes of structured data in Cloud Storage. This data includes gameplay time data, user metadata, and game metadata. You want to build a model that recommends new games to users that requires the least amount of coding. What should you do?","exam_id":13,"answer_description":"","question_id":197,"choices":{"A":"Load the data in BigQuery. Use BigQuery ML to train an Autoencoder model.","B":"Load the data in BigQuery. Use BigQuery ML to train a matrix factorization model.","D":"Read data to a Vertex AI Workbench notebook. Use TensorFlow to train a matrix factorization model.","C":"Read data to a Vertex AI Workbench notebook. Use TensorFlow to train a two-tower model."},"url":"https://www.examtopics.com/discussions/google/view/133595-exam-professional-machine-learning-engineer-topic-1-question/","answers_community":["B (91%)","9%"],"answer_images":[],"answer_ET":"B","question_images":[],"timestamp":"2024-02-12 09:02:00","topic":"1","discussion":[{"comments":[{"upvote_count":"3","timestamp":"1728803340.0","poster":"omermahgoub","content":"Matrix Factorization: This collaborative filtering technique is commonly used for recommender systems. BigQuery ML offers built-in support for matrix factorization, making it a good choice for your scenario.","comments":[{"upvote_count":"1","content":"it means you choose B?","comment_id":"1197045","poster":"fitri001","comments":[{"timestamp":"1729421940.0","content":"Yes, voted for A by mistake.\nThe answer is B","comment_id":"1199110","poster":"omermahgoub","upvote_count":"3"}],"timestamp":"1729148160.0"}],"comment_id":"1194725"}],"upvote_count":"1","timestamp":"1728803280.0","content":"Selected Answer: A\nMinimal Coding: BigQuery ML provides a user-friendly interface for training models, minimizing the need for extensive coding in tools like TensorFlow (C & D)\nEfficient Data Processing: Training directly in BigQuery eliminates data movement and leverages BigQuery's scalable infrastructure.","comment_id":"1194722","poster":"omermahgoub"},{"upvote_count":"4","comment_id":"1154699","timestamp":"1724149260.0","poster":"vaibavi","content":"Selected Answer: B\nleast amount of coding--> BQML\nrecommendations--> matrix factorization"},{"upvote_count":"3","content":"Selected Answer: B\nUsing BigQuery ML for training a matrix factorization model would require less coding compared to building a custom model with TensorFlow in a Vertex AI Workbench notebook. BigQuery ML provides high-level APIs for machine learning tasks directly within the BigQuery environment, thus reducing the amount of coding needed for data preprocessing and model training. Matrix factorization is a commonly used technique for recommendation systems, making it a suitable choice for recommending new games to users based on their gameplay time data, user metadata, and game metadata.","poster":"guilhermebutzke","comment_id":"1153970","timestamp":"1724068860.0"},{"timestamp":"1723442520.0","comment_id":"1147886","poster":"Yan_X","content":"Selected Answer: B\nB\n\nhttps://developers.google.com/machine-learning/recommendation/collaborative/matrix","upvote_count":"3"}]},{"id":"9uQmxT3m947b8AnxaSZE","answer_images":[],"answer_ET":"C","question_images":[],"question_text":"You work for a large bank that serves customers through an application hosted in Google Cloud that is running in the US and Singapore. You have developed a PyTorch model to classify transactions as potentially fraudulent or not. The model is a three-layer perceptron that uses both numerical and categorical features as input, and hashing happens within the model.\n\nYou deployed the model to the us-central1 region on nl-highcpu-16 machines, and predictions are served in real time. The model's current median response latency is 40 ms. You want to reduce latency, especially in Singapore, where some customers are experiencing the longest delays. What should you do?","exam_id":13,"topic":"1","answers_community":["C (63%)","D (38%)"],"answer":"C","discussion":[{"comments":[{"comment_id":"1186284","timestamp":"1711824480.0","content":"Deploying in additional regions (D) does not necessarily negate or underutilize existing deployments but rather complements them to provide a better global service.","poster":"tavva_prudhvi","upvote_count":"4"}],"upvote_count":"9","timestamp":"1708351500.0","content":"Selected Answer: C\nMy Answer: C \n The bottleneck is network latency. So, \nA: Not Correct: might improve performance, but it's an expensive solution and may not be necessary if the bottleneck is network latency.\nB: Not Correct: might offer slight improvement, but the primary issue is geographical distance between users and the model.\nC: CORRECT: This approach leverages the geographical proximity of the endpoints to the users, reducing latency for customers in Singapore without neglecting customers in the US. Additionally, using Vertex AI private endpoints ensures secure and efficient communication between the application and the model.\nD: Not Correct: it's not the most efficient approach because it does not utilize the existing infrastructure in the us-central1 region, and managing multiple endpoints might introduce additional complexity.","comment_id":"1153975","poster":"guilhermebutzke"},{"content":"Selected Answer: C\nusing private endpoints shorten the network path between the model and the client app","upvote_count":"1","poster":"devops_bms","timestamp":"1739158920.0","comment_id":"1354201"},{"comment_id":"1315679","upvote_count":"1","poster":"uatud3","content":"Selected Answer: D\nI picked D. Sounds like the most logical answer","timestamp":"1732171740.0"},{"content":"Selected Answer: C\nYou work for a BANK. An application accesses the model and serves predictions to customers. The application is in US and Singapore. The application should never access the model over the public internet. Therefore, private endpoints.","timestamp":"1731694080.0","comment_id":"1312753","poster":"f084277","upvote_count":"3"},{"content":"Selected Answer: D\nI don't have any link to support this other than a simple analysis; if you want the data or process to be low latency, you need to deploy closes where it is required, in this case, to Singapore customers, which reduces latetency addressing the requirement.","poster":"wences","upvote_count":"1","timestamp":"1726351200.0","comment_id":"1283785"},{"poster":"inc_dev_ml_001","comment_id":"1226616","comments":[{"poster":"f084277","content":"\"You work for a bank\".... it's C","comment_id":"1312752","upvote_count":"1","timestamp":"1731693900.0"}],"content":"Selected Answer: D\nI think it's D because C and D should work in the same way, but ensuring the connection through a private endpoint it's not necessary because in the question there's nothing about security or sensitive informations. So the scope for a generic endpoint is \"Accessible from anywhere\", the scope for a private endpoint is \"Accessible only within VPC or private connections\". Don't see why to do that, it's only a matter of latency, not a matter of safety.","upvote_count":"2","timestamp":"1717833060.0"},{"timestamp":"1716051960.0","content":"Selected Answer: D\nNot sure why I'd choose C over D, my choice is D.\nModel is already deployed to us-central1 so now it's only a matter of deploying it to asia-southeast1 and letting the app choose the closer endpoint.\nWhy the need for private endpoints and what will happen with the current already deployed model in us-central1?","poster":"GuineaPigHunter","comment_id":"1213391","upvote_count":"2"},{"timestamp":"1712992380.0","upvote_count":"2","comment_id":"1194728","content":"Selected Answer: C\nDeploying the model to a Vertex AI private endpoint in the Singapore region brings the model closer to users in that region. This significantly reduces network latency for those users compared to accessing the model hosted in us-central1.\nAllowing the application to choose the appropriate endpoint based on user location (through private endpoints) ensures users access the geographically closest model replica, optimizing latency.\nWhy not D: creating a separate endpoint in Singapore would allow regional deployment, it wouldn't automatically route users to the closest endpoint. You still need additional logic within the application for regional routing, increasing complexity.","poster":"omermahgoub"},{"comment_id":"1185363","timestamp":"1711709640.0","upvote_count":"1","comments":[{"upvote_count":"1","content":"Yes, using private endpoints does introduce some overhead.\nAdditional latency: Establishing a connection to a private endpoint may add some latency compared to using the public endpoint.\nIncreased complexity: Managing private endpoints requires additional configuration and management, which can increase the overall complexity of your deployment.\nHowever, in this scenario, the benefits of using private endpoints (security, control, and isolation) outweigh the potential overhead. The goal is to reduce latency for users in Singapore, and by deploying a private endpoint closer to them, you can achieve this while maintaining security and control over access to your model.","comment_id":"1242941","timestamp":"1720201980.0","poster":"AzureDP900","comments":[{"upvote_count":"1","poster":"AzureDP900","comment_id":"1242943","content":"I will go with C.\nIn this scenario, deploying the model to Vertex AI private endpoints in both us-central1 and asia-southeast1 regions is necessary because:\n\nThe application is hosted in Google Cloud and serves customers through APIs.\nBy using private endpoints, you can create a secure connection between your application and the Vertex AI endpoint without exposing the model or data to the public internet. This ensures that sensitive information remains within the cloud.\nPrivate endpoints provide an IP address that is unique to your project, making it easier to manage access control and network policies.\nWithout private endpoints, you would need to expose your model or data to the public internet, which increases the risk of unauthorized access and security breaches. Private endpoints provide a secure and controlled environment for hosting your model, ensuring that only authorized users can access it.","timestamp":"1720202220.0"}]},{"content":"see guilhermebutzke","comment_id":"1195312","poster":"pinimichele01","timestamp":"1713075660.0","upvote_count":"1"}],"content":"Selected Answer: D\nI think it is D. C is questionable as why do you need a private endpoint?","poster":"shuvs"}],"isMC":true,"question_id":198,"timestamp":"2024-02-19 15:05:00","answer_description":"","choices":{"D":"Create another Vertex AI endpoint in the asia-southeast1 region, and allow the application to choose the appropriate endpoint.","B":"Change the machines being used for online inference to nl-highcpu-32.","C":"Deploy the model to Vertex AI private endpoints in the us-central1 and asia-southeast1 regions, and allow the application to choose the appropriate endpoint.","A":"Attach an NVIDIA T4 GPU to the machines being used for online inference."},"url":"https://www.examtopics.com/discussions/google/view/134187-exam-professional-machine-learning-engineer-topic-1-question/","unix_timestamp":1708351500},{"id":"PFunxS5PvemrFy7eHAUG","answers_community":["A (61%)","C (22%)","B (17%)"],"question_images":[],"topic":"1","question_text":"You need to train an XGBoost model on a small dataset. Your training code requires custom dependencies. You want to minimize the startup time of your training job. How should you set up your Vertex AI custom training job?","question_id":199,"answer_images":[],"isMC":true,"answer_description":"","answer_ET":"A","unix_timestamp":1708351800,"answer":"A","choices":{"B":"Use the XGBoost prebuilt custom container. Create a Python source distribution that includes the data and installs the dependencies at runtime. In your training application, load the data into a pandas DataFrame and train the model.","A":"Store the data in a Cloud Storage bucket, and create a custom container with your training application. In your training application, read the data from Cloud Storage and train the model.","C":"Create a custom container that includes the data. In your training application, load the data into a pandas DataFrame and train the model.","D":"Store the data in a Cloud Storage bucket, and use the XGBoost prebuilt custom container to run your training application. Create a Python source distribution that installs the dependencies at runtime. In your training application, read the data from Cloud Storage and train the model."},"url":"https://www.examtopics.com/discussions/google/view/134188-exam-professional-machine-learning-engineer-topic-1-question/","exam_id":13,"timestamp":"2024-02-19 15:10:00","discussion":[{"poster":"guilhermebutzke","timestamp":"1708351800.0","upvote_count":"5","content":"Selected Answer: A\nMy Answer: A\n\nFocus on “training code requires custom dependencies” and “ minimize the startup time of your training job”, the best choice is A because use custom container and read the data from GCS is he faster way","comment_id":"1153978"},{"timestamp":"1728922560.0","poster":"Foxy2021","upvote_count":"1","content":"I select D: While A could work, D is the optimal solution because it balances efficiency, ease of setup, and performance. It minimizes startup time by leveraging Google’s prebuilt XGBoost container and offers flexibility by installing custom dependencies at runtime. This approach avoids the overhead of building and maintaining a custom container from scratch, which is unnecessary for a small dataset with only specific custom dependency needs.","comment_id":"1297721"},{"poster":"wences","timestamp":"1727561940.0","upvote_count":"1","comment_id":"1290828","content":"Selected Answer: A\nThe fastest way is to have most of the things already installed, so that is why option A fits the best"},{"timestamp":"1718880540.0","comment_id":"1233579","upvote_count":"3","content":"Selected Answer: C\nThe focus is on startup time, and the dataset is small, so the container should still be of reasonable size.\nDownloading data from Cloud Storage introduces a delay.","poster":"omribt"},{"content":"Selected Answer: C\nThe dataset is small, xgboost is implemented in python... (correcting my error A answer)","upvote_count":"1","comment_id":"1224259","poster":"bobjr","timestamp":"1717525020.0"},{"timestamp":"1717524840.0","comment_id":"1224255","poster":"bobjr","upvote_count":"1","content":"Selected Answer: A\nThe dataset is small, xgboost is implemented in python..."},{"upvote_count":"4","poster":"omermahgoub","timestamp":"1712993520.0","comment_id":"1194747","content":"Selected Answer: A\nGiven the focus on minimizing startup time, and based on the information about XGBoost prebuilt container dependencies available here https://cloud.google.com/vertex-ai/docs/training/pre-built-containers#xgboost\n\nA: Separate Data and Custom Container is the best approach for minimizing startup time, especially for small datasets. Separating data in Cloud Storage keeps the container image lean, leading to faster download and startup compared to bundling data within the container.\nB. The prebuilt Container could have unnecessary components, potentially increasing the image size and impacting startup time."},{"content":"Why not C?","comments":[{"poster":"tavva_prudhvi","upvote_count":"3","comments":[{"upvote_count":"1","comment_id":"1205781","poster":"raidenrock","content":"But the description mentioned it is a small dataset and requires minimizing latency which makes C the best per requirement, there is no mentioning to make the container reusable whatsoever","timestamp":"1714676160.0"}],"timestamp":"1711822800.0","comment_id":"1186244","content":"Because, Including the data in the container image is not recommended as it increases the image size and makes it less reusable."}],"timestamp":"1711023180.0","comment_id":"1179186","poster":"CHARLIE2108","upvote_count":"1"},{"upvote_count":"3","poster":"Yan_X","comment_id":"1168823","content":"Selected Answer: B\nB\n\nXGBoost prebuilt customer container already includes XGBoost library and all of its dependencies.\nPython source distribution to avoid overhead of reading the data from Cloud storage the 2nd time.\nLoad data to a Pandas DataFrame is convenient to work with Python. Pandas is for data analysis and manipulation.","timestamp":"1709904000.0","comments":[{"upvote_count":"2","comments":[{"timestamp":"1711823100.0","comment_id":"1186250","content":"Also, creating a Python source distribution that includes the data and installs the dependencies at runtime can increase startup time since dependencies have to be installed every time the job runs","poster":"tavva_prudhvi","upvote_count":"1"}],"poster":"tavva_prudhvi","content":"However, the question specifically says that the training code requires custom dependencies beyond those included in the prebuilt container. Therefore, using the prebuilt container alone would not be sufficient in this case.\n&\nregarding the use of a Python source distribution to avoid reading data from Cloud Storage multiple times, it's important to consider the trade-off between startup time and potential performance gains. While including the data in the source distribution might save some time during training, it also increases the size of the container and can lead to longer startup times. For small datasets, the overhead of reading data from Cloud Storage is typically negligible compared to the benefits of a smaller container and faster startup.","comment_id":"1186248","timestamp":"1711823040.0"}]}]},{"id":"c5YUPDlvFnwRaRocmr60","answer_ET":"C","answer_images":[],"choices":{"D":"Deploy a Cloud Composer directed acyclic graph (DAG) with a GCSObjectUpdateSensor class that detects when a new file is added to the Cloud Storage bucket.","C":"Create a pipeline in Vertex AI Pipelines. Create a Cloud Function that uses a Cloud Storage trigger and deploys the pipeline.","A":"Create a pipeline in Vertex AI Pipelines. Configure the first step to compare the contents of the bucket to the last time the pipeline was run. Use the scheduler API to run the pipeline periodically.","B":"Create a Cloud Function that uses a Cloud Storage trigger and deploys a Cloud Composer directed acyclic graph (DAG)."},"discussion":[{"comment_id":"1218875","upvote_count":"5","comments":[{"timestamp":"1716715680.0","poster":"fitri001","comments":[{"timestamp":"1723394100.0","comment_id":"1264226","upvote_count":"1","content":"I think we should use Cloud Composer here because of \"that uses different Google Cloud services\". Vertex AI is less integrated with the rest of services than Cloud Composer, which was designed exactly for that.","poster":"tardigradum"}],"upvote_count":"1","content":"why not D?\nPros:\nCloud Composer provides a powerful orchestration framework that can handle complex dependencies and workflows.GCSObjectUpdateSensor can efficiently detect new files in the bucket and trigger the pipeline. \nCons:\nCloud Composer can be relatively costly due to the continuous operation of its environment. Overhead of maintaining Cloud Composer for potentially simple file-triggered tasks.","comment_id":"1218877"}],"timestamp":"1716715560.0","poster":"fitri001","content":"Selected Answer: C\nOption C appears to be the best choice for balancing the requirements of efficient orchestration, cost minimization, and ensuring the pipeline only runs when new files are present. By using a Cloud Function triggered by Cloud Storage events to deploy a Vertex AI Pipeline, you can leverage the event-driven model of Cloud Functions to minimize unnecessary runs and associated costs, while still using the powerful orchestration capabilities of Vertex AI Pipelines."},{"timestamp":"1731869580.0","upvote_count":"2","poster":"juliorevk","comment_id":"1313699","content":"Probably C because while D would be good, it specifically says to minimize compute costs which cloud composer does incur whereas C is more serverless."},{"poster":"Foxy2021","comment_id":"1297726","timestamp":"1728923100.0","upvote_count":"1","content":"My answer is D: While C (Cloud Function + Vertex AI Pipelines) is a viable approach for triggering ML pipelines, D (Cloud Composer DAG with GCSObjectUpdateSensor) is the more appropriate and scalable solution when your orchestration spans multiple Google Cloud services and you want to minimize costs by only triggering the pipeline when new files appear."},{"poster":"tardigradum","upvote_count":"1","content":"Selected Answer: D\nThe key here is \"that uses different Google Cloud services\". Taking this into account, Cloud Composer is the correct answer (for instance, Vertex AI pipelines is not integrated with classic Dataproc or Cloud Composer DAGs). Moreover, GCSObjectUpdateSensor is more efficient than a Cloud Function.","comment_id":"1264225","timestamp":"1723393980.0"},{"content":"Selected Answer: D\n\"Different Google Cloud services\" and GCSObjectUpdateSensor: This sensor class specifically checks for updates to Cloud Storage objects. This ensures the DAG only triggers when there's a new file in the bucket, minimizing unnecessary executions.","comment_id":"1215868","poster":"Kili1","upvote_count":"1","timestamp":"1716398760.0"},{"comment_id":"1180975","poster":"CHARLIE2108","upvote_count":"1","timestamp":"1711208040.0","content":"Why not D?"},{"timestamp":"1709904540.0","poster":"Yan_X","upvote_count":"3","content":"Selected Answer: C\nC\n\nCloud Function to be triggered by Cloud storage trigger, and then deploy the Vertex AI pipeline.","comment_id":"1168829"},{"poster":"JG123","upvote_count":"1","timestamp":"1709699100.0","comment_id":"1166918","content":"Its C. Vertex pipelines are recommened to run ML pipeline!"},{"upvote_count":"1","comments":[{"comment_id":"1156592","timestamp":"1708624080.0","content":"Cloud Composer already provides a way to orchestrate tasks, and creating a Cloud Function to deploy a DAG is not a common practice. The Cloud Function with a Cloud Storage trigger would be redundant since the GCSObjectUpdateSensor within the DAG itself can handle the file detection.","poster":"tavva_prudhvi","upvote_count":"5"}],"timestamp":"1708352100.0","content":"Selected Answer: B\nMy Answer: B \n\nCloud Function that uses a Cloud Storage trigger (”run if new files are present in your dataset in a Cloud Storage bucket”) and Cloud Composer directed acyclic graph (DAG) (”model deployment that uses different Google Cloud services”, ”orchestration layer on top of these tasks”,)","comment_id":"1153986","poster":"guilhermebutzke"}],"url":"https://www.examtopics.com/discussions/google/view/134189-exam-professional-machine-learning-engineer-topic-1-question/","unix_timestamp":1708352100,"topic":"1","question_text":"You are creating an ML pipeline for data processing, model training, and model deployment that uses different Google Cloud services. You have developed code for each individual task, and you expect a high frequency of new files. You now need to create an orchestration layer on top of these tasks. You only want this orchestration pipeline to run if new files are present in your dataset in a Cloud Storage bucket. You also want to minimize the compute node costs. What should you do?","answer":"C","question_images":[],"timestamp":"2024-02-19 15:15:00","exam_id":13,"question_id":200,"answers_community":["C (73%)","D (18%)","9%"],"isMC":true,"answer_description":""}],"exam":{"numberOfQuestions":304,"isBeta":false,"lastUpdated":"11 Apr 2025","id":13,"isImplemented":true,"name":"Professional Machine Learning Engineer","provider":"Google","isMCOnly":true},"currentPage":40},"__N_SSP":true}