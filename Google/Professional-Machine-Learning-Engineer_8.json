{"pageProps":{"questions":[{"id":"VilY4XWeSMVKqduj6b3n","answer_description":"","url":"https://www.examtopics.com/discussions/google/view/92333-exam-professional-machine-learning-engineer-topic-1-question/","question_images":[],"discussion":[{"content":"Selected Answer: C\nC. According Google:\n\"Instead of deploying the model to an endpoint, you can use the RunInference API to serve machine learning models in your Apache Beam pipeline. This approach has several advantages, including flexibility and portability. However, deploying the model in Vertex AI offers many additional benefits, such as the platform's built-in tools for model monitoring, TensorBoard, and model registry governance.\nVertex AI also provides the ability to use Optimized TensorFlow runtime in your endpoints. To do this, simply specify the TensorFlow runtime container when you deploy your model.\"\n\nhttps://cloud.google.com/blog/products/ai-machine-learning/streaming-prediction-with-dataflow-and-vertex","comments":[{"timestamp":"1731525960.0","poster":"f084277","upvote_count":"2","content":"The quote you cite describes B as the right answer, not C... the question asks *only* about minimizing latency.","comment_id":"1311590"}],"comment_id":"1137922","timestamp":"1706820780.0","upvote_count":"7","poster":"guilhermebutzke"},{"poster":"hghdh5454","upvote_count":"5","comment_id":"853981","content":"Selected Answer: B\nB. Load the model directly into the Dataflow job as a dependency, and use it for prediction.\n\nBy loading the model directly into the Dataflow job as a dependency, you minimize the serving latency since the model is available within the pipeline itself. This way, you avoid additional network latency that would be introduced by invoking external services, such as Cloud Run, Vertex AI endpoints, or TFServing containers.","comments":[{"comments":[{"upvote_count":"1","comment_id":"1364662","content":"Overhead and Latency are not the same thing. the question ask to minimize latency not cost.","timestamp":"1741052040.0","poster":"desertlotus1211"}],"timestamp":"1681846920.0","comment_id":"874056","content":"Actually in retrospect C is the correct answer, not B because loading the model directly into the Dataflow job as a dependency may cause unnecessary overhead, as Dataflow jobs are primarily designed for batch processing and may not be optimized for real-time prediction. Additionally, loading the model as a dependency may increase the size of the Dataflow job and introduce complexity in managing dependencies.","upvote_count":"1","poster":"Antmal"}],"timestamp":"1680062880.0"},{"poster":"desertlotus1211","upvote_count":"2","comment_id":"1364661","content":"Selected Answer: B\nBy loading the TensorFlow model directly into the Dataflow job, you ensure that inference happens inline within the pipeline on the worker nodes.\n\nUsing external endpoints (Options A, C, and D) introduces extra latency due to network round trips, which is not ideal for real-time prediction in a cybersecurity context","timestamp":"1741051980.0"},{"poster":"f084277","content":"Selected Answer: B\nthe question asks *only* about minimizing latency. Doing everything in Dataflow minimizes latency over all the other options.","upvote_count":"4","timestamp":"1731526080.0","comment_id":"1311593"},{"upvote_count":"4","content":"Selected Answer: B\nIt's a toss up between B and C.\n\nI chose B because using vertex AI as an endpoint introduces network latency which naturally does not meet the criteria of \"minimizing latency\".\n\nHowever, choosing option B also implies that I have more overhead by directly running the model in the dataflow pipeline. Since the question didn't mention any limitations on resources, I assumed that the resources can be scaled accordingly to minimize latency. I might be overthinking on this option though seeing how most of Google questions have a strong preference on their \"recommended platforms\" like vertex AI. Most of the questions and the community answers seem to tend towards anything that mentions \"vertex ai\".","poster":"SausageMuffins","comment_id":"1206793","timestamp":"1714892460.0"},{"content":"According Google:\n\"Instead of deploying the model to an endpoint, you can use the RunInference API to serve machine learning models in your Apache Beam pipeline. This approach has several advantages, including flexibility and portability. However, deploying the model in Vertex AI offers many additional benefits, such as the platform's built-in tools for model monitoring, TensorBoard, and model registry governance.\nVertex AI also provides the ability to use Optimized TensorFlow runtime in your endpoints. To do this, simply specify the TensorFlow runtime container when you deploy your model.\"\n\nhttps://cloud.google.com/blog/products/ai-machine-learning/streaming-prediction-with-dataflow-and-vertex","timestamp":"1706820720.0","poster":"guilhermebutzke","comment_id":"1137921","upvote_count":"2"},{"content":"Selected Answer: C\nIn this case, the best way to minimize the serving latency of the system log anomaly detection model is to deploy it to a Vertex AI endpoint. This will allow Dataflow to invoke the model directly, without having to load it into the job as a dependency. This will significantly reduce the serving latency, as Dataflow will not have to wait for the model to load before it can make a prediction.\n\nOption B would involve loading the model directly into the Dataflow job as a dependency. This would also add an additional layer of latency, as Dataflow would have to load the model into memory before it could make a prediction.","upvote_count":"3","poster":"tavva_prudhvi","timestamp":"1688834700.0","comment_id":"946630"},{"upvote_count":"1","timestamp":"1685971500.0","poster":"Voyager2","content":"C. Deploy the model to a Vertex AI endpoint, and invoke this endpoint in the Dataflow job\nhttps://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning","comment_id":"915468"},{"comment_id":"901621","upvote_count":"1","comments":[{"timestamp":"1712476080.0","comment_id":"1190842","content":"Dataflow has a streaming pipeline solution as well.","upvote_count":"1","poster":"7cb0ab3"}],"poster":"julliet","timestamp":"1684467480.0","content":"Selected Answer: C\nC\nI eliminate B because Dataflow is a batch-prediction solution, not real-time"},{"upvote_count":"1","comment_id":"893130","poster":"M25","timestamp":"1683639060.0","content":"Selected Answer: C\nWent with C"},{"content":"Selected Answer: C\nI believe it is C when deploying the model to a Vertex AI endpoint it provides a dedicated prediction service optimised for real-time inference. Vertex AI endpoints are designed for high performance and low latency, making them ideal for real-time prediction use cases. Dataflow can easily invoke the Vertex AI endpoint to perform predictions, minimising serving latency.","timestamp":"1681847040.0","upvote_count":"1","comment_id":"874059","poster":"Antmal"},{"content":"Selected Answer: B\nBy loading the model directly into the Dataflow job as a dependency, you can perform predictions within the same job. This approach helps minimize serving latency since there is no need to make external calls to another service or endpoint. Instead, the model is directly available within the Dataflow pipeline, allowing for efficient and fast processing of the streaming data.","poster":"wlts","upvote_count":"1","timestamp":"1679760540.0","comment_id":"850273"},{"content":"Selected Answer: C\nC. Deploy the model to a Vertex AI endpoint, and invoke this endpoint in the Dataflow job.\n\nThe reason for this choice is that deploying the model to a Vertex AI endpoint and invoking it in the Dataflow job is the most efficient and scalable option for real-time prediction. Vertex AI provides a fully managed, serverless platform for deploying and serving machine learning models. It allows for high availability and low-latency serving of models, and can handle a large volume of requests in parallel. Invoking the model via an endpoint in the Dataflow job minimizes the latency for model prediction, as it avoids any unnecessary data transfers or containerization","poster":"TNT87","upvote_count":"2","timestamp":"1678192500.0","comments":[{"timestamp":"1681708140.0","upvote_count":"1","content":"Using private endpoints to serve online predictions with Vertex AI provides a low-latency, secure connection to the Vertex AI online prediction service. This guide shows how to configure private endpoints on Vertex AI by using VPC Network Peering to peer your network with the Vertex AI online prediction service\n\nhttps://cloud.google.com/vertex-ai/docs/predictions/using-private-endpoints \n\nAnswer C","comment_id":"872334","poster":"TNT87"}],"comment_id":"831879"},{"timestamp":"1677234000.0","comment_id":"820342","content":"Selected Answer: C\nOption B, loading the model directly into the Dataflow job as a dependency and using it for prediction, may not provide the optimal performance because Dataflow may not be optimized for low-latency predictions.","poster":"shankalman717","upvote_count":"1"},{"comment_id":"807091","content":"These are anwser\nhttps://cloud.google.com/dataflow/docs/notebooks/run_inference_tensorflow\n\nhttps://beam.apache.org/documentation/sdks/python-machine-learning/\n\nhttps://beam.apache.org/documentation/transforms/python/elementwise/runinference/","poster":"John_Pongthorn","upvote_count":"2","timestamp":"1676265840.0"},{"upvote_count":"1","content":"C OR B.\nit is straightforward that it should be C as the follwing link https://cloud.google.com/architecture/detecting-anomalies-in-financial-transactions\nbut B it seems a newer way. it keeps questionable.","poster":"John_Pongthorn","comments":[{"content":"Reading through this link, look like dataflow itself is doing prediction directly .. so B","comment_id":"849798","poster":"Yajnas_arpohc","upvote_count":"1","timestamp":"1679719860.0"}],"timestamp":"1674465180.0","comment_id":"785163"},{"poster":"ares81","upvote_count":"1","content":"Selected Answer: C\nC, for me.","comment_id":"764400","timestamp":"1672740540.0"},{"comment_id":"752884","content":"Selected Answer: C\nC\n- https://cloud.google.com/architecture/detecting-anomalies-in-financial-transactions\n- https://cloud.google.com/blog/topics/financial-services/detect-anomalies-in-real-time-forex-data-with-ml","poster":"hiromi","upvote_count":"4","timestamp":"1671667260.0"},{"upvote_count":"2","comment_id":"752263","content":"Selected Answer: B\nhttps://cloud.google.com/blog/products/data-analytics/influsing-ml-models-into-production-pipelines-with-dataflow","poster":"pshemol","timestamp":"1671624900.0"}],"topic":"1","choices":{"D":"Deploy the model in a TFServing container on Google Kubernetes Engine, and invoke it in the Dataflow job.","A":"Containerize the model prediction logic in Cloud Run, which is invoked by Dataflow.","B":"Load the model directly into the Dataflow job as a dependency, and use it for prediction.","C":"Deploy the model to a Vertex AI endpoint, and invoke this endpoint in the Dataflow job."},"answer":"C","answer_images":[],"isMC":true,"answer_ET":"C","question_text":"You are working on a system log anomaly detection model for a cybersecurity organization. You have developed the model using TensorFlow, and you plan to use it for real-time prediction. You need to create a Dataflow pipeline to ingest data via Pub/Sub and write the results to BigQuery. You want to minimize the serving latency as much as possible. What should you do?","timestamp":"2022-12-21 13:15:00","exam_id":13,"question_id":36,"unix_timestamp":1671624900,"answers_community":["C (54%)","B (46%)"]},{"id":"GoatEM8zsAOSTgGnXjz1","isMC":true,"answer":"B","answer_description":"","answer_images":[],"choices":{"D":"Dimensionality reduction","B":"Dynamic range quantization","C":"Model distillation","A":"Weight pruning"},"question_id":37,"answers_community":["B (100%)"],"question_text":"You are an ML engineer at a mobile gaming company. A data scientist on your team recently trained a TensorFlow model, and you are responsible for deploying this model into a mobile application. You discover that the inference latency of the current model doesn’t meet production requirements. You need to reduce the inference time by 50%, and you are willing to accept a small decrease in model accuracy in order to reach the latency requirement. Without training a new model, which model optimization technique for reducing latency should you try first?","exam_id":13,"discussion":[{"comment_id":"831878","poster":"TNT87","timestamp":"1725705300.0","upvote_count":"6","content":"B. Dynamic range quantization\n\nThe reason for this choice is that dynamic range quantization is a model optimization technique that can significantly reduce model size and inference time while maintaining reasonable model accuracy. Dynamic range quantization uses fewer bits to represent the weights of the model, reducing the memory required to store the model and the time required for inference."},{"poster":"julliet","comment_id":"901625","content":"Selected Answer: B\nB.\nA, C, D --> have to retrain","timestamp":"1731995220.0","upvote_count":"3"},{"timestamp":"1731169200.0","content":"Selected Answer: B\nPlus: “Magnitude-based weight pruning gradually zeroes out model weights during the training process to achieve model sparsity. Sparse models are easier to compress, and we can skip the zeroes during inference for latency improvements.” https://www.tensorflow.org/model_optimization/guide/pruning, where “during the training process” disqualifies Option A.","comment_id":"893165","poster":"M25","comments":[{"upvote_count":"1","timestamp":"1731169200.0","comment_id":"893166","poster":"M25","content":"https://en.wikipedia.org/wiki/Knowledge_distillation is the process of transferring knowledge from a large model to a smaller one. As smaller models are less expensive to evaluate, they can be deployed on less powerful hardware (such as a mobile device). https://en.wikipedia.org/wiki/Dimensionality_reduction is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data. \n“Without training a new model” disqualifies both Option C and D."}],"upvote_count":"1"},{"upvote_count":"3","poster":"ares81","comment_id":"764379","content":"Selected Answer: B\n'Without training a new model' --> B","timestamp":"1719993360.0"},{"upvote_count":"4","comment_id":"752890","poster":"hiromi","timestamp":"1719007920.0","content":"Selected Answer: B\nB\n- https://www.tensorflow.org/lite/performance/post_training_quantization#dynamic_range_quantization"},{"timestamp":"1718297580.0","upvote_count":"3","poster":"mil_spyro","comment_id":"744378","content":"Selected Answer: B\nThe requirement is \"Without training a new model\" hence dynamic range quantization.\nhttps://www.tensorflow.org/lite/performance/post_training_quant"}],"url":"https://www.examtopics.com/discussions/google/view/91489-exam-professional-machine-learning-engineer-topic-1-question/","topic":"1","unix_timestamp":1670957580,"timestamp":"2022-12-13 19:53:00","question_images":[],"answer_ET":"B"},{"id":"XsLUVD6lecxYP0MXfDsG","question_images":[],"timestamp":"2022-12-13 19:43:00","answers_community":["D (96%)","4%"],"answer_ET":"D","topic":"1","answer":"D","exam_id":13,"unix_timestamp":1670956980,"question_text":"You work on a data science team at a bank and are creating an ML model to predict loan default risk. You have collected and cleaned hundreds of millions of records worth of training data in a BigQuery table, and you now want to develop and compare multiple models on this data using TensorFlow and Vertex AI. You want to minimize any bottlenecks during the data ingestion state while considering scalability. What should you do?","answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/91488-exam-professional-machine-learning-engineer-topic-1-question/","question_id":38,"choices":{"D":"Use TensorFlow I/O’s BigQuery Reader to directly read the data.","C":"Convert the data into TFRecords, and use tf.data.TFRecordDataset() to read them.","A":"Use the BigQuery client library to load data into a dataframe, and use tf.data.Dataset.from_tensor_slices() to read it.","B":"Export data to CSV files in Cloud Storage, and use tf.data.TextLineDataset() to read them."},"discussion":[{"upvote_count":"8","comment_id":"753102","content":"Selected Answer: D\nD\n- https://www.tensorflow.org/io/api_docs/python/tfio/bigquery","poster":"hiromi","timestamp":"1687416960.0"},{"upvote_count":"6","content":"Selected Answer: D\nVote on D. This will allow to directly access the data from BigQuery without having to first load it into a dataframe or export it to files in Cloud Storage.","poster":"mil_spyro","timestamp":"1686674580.0","comment_id":"744368"},{"comment_id":"1364666","content":"Selected Answer: C\nWhy not C? Answer D may introduce latency or bottlenecks due to network constraints and is not as optimized for large-scale training as the TFRecord approach. \n\nThoughts?","upvote_count":"1","poster":"desertlotus1211","timestamp":"1741052280.0"},{"timestamp":"1729824540.0","comment_id":"1201718","comments":[{"poster":"fitri001","content":". BigQuery Client Library and Dataframe: While the BigQuery client library can access BigQuery data, loading it into a DataFrame and using tf.data.Dataset.from_tensor_slices() is inefficient for massive datasets due to memory limitations and potential processing bottlenecks.\nB. CSV Files and TextLineDataset: Exporting data to CSV and using tf.data.TextLineDataset() introduces unnecessary data movement and processing overhead, hindering both efficiency and scalability.\nC. TFRecords: TFRecords can be efficient for certain use cases, but converting hundreds of millions of records into TFRecords can be time-consuming and resource-intensive.\n\npen_spark\nexclamation Additionally, reading them might require parsing logic within your TensorFlow script.","upvote_count":"1","timestamp":"1729824540.0","comment_id":"1201719"}],"poster":"fitri001","content":"Selected Answer: D\nDirect Data Access: TensorFlow I/O's BigQuery Reader allows you to directly access data from BigQuery tables within your TensorFlow script.expand_more This eliminates the need for intermediate data movement (e.g., to CSV files) and data manipulation steps (e.g., loading into DataFrames).exclamation\nScalability: BigQuery Reader is designed to handle large datasets efficiently. It leverages BigQuery's parallel processing capabilities to stream data into your TensorFlow training pipeline, minimizing processing bottlenecks and enabling scalability as your data volume grows.","upvote_count":"2"},{"timestamp":"1722539040.0","comment_id":"1137935","upvote_count":"1","poster":"guilhermebutzke","content":"Selected Answer: D\nD\nhttps://cloud.google.com/blog/products/ai-machine-learning/tensorflow-enterprise-makes-accessing-data-on-google-cloud-faster-and-easier"},{"timestamp":"1700373060.0","upvote_count":"2","content":"Selected Answer: D\nD\nBigQuery is more compact way to store the data than TFRecords","poster":"julliet","comment_id":"901628"},{"poster":"M25","comment_id":"893178","upvote_count":"1","content":"Selected Answer: D\nWent with D","timestamp":"1699547880.0"},{"content":"Selected Answer: D\nD. Use TensorFlow I/O’s BigQuery Reader to directly read the data.\n\nThe reason for this choice is that using TensorFlow I/O’s BigQuery Reader is the most efficient and scalable option for reading data directly from BigQuery into TensorFlow models. It allows for distributed processing and avoids unnecessary data duplication, which can cause bottlenecks and consume large amounts of storage. Additionally, the BigQuery Reader is optimized for reading data in parallel from BigQuery tables and streaming them directly into TensorFlow. This eliminates the need for any intermediate file formats or data copies, reducing latency and increasing performance.","comment_id":"831876","poster":"TNT87","timestamp":"1694082780.0","upvote_count":"2"}],"isMC":true,"answer_description":""},{"id":"Cm61wjpTAuxIoLQFBhhU","exam_id":13,"discussion":[{"comments":[{"content":"this is the perfect answer and explanation.","timestamp":"1709209860.0","poster":"LFavero","comment_id":"1162529","upvote_count":"2"}],"content":"Selected Answer: B\nB: Here's why:\n**Embedding Dimension:** UNIT_LINEAR_SCALE is appropriate for integer hyperparameters with a continuous range like the embedding dimension. It linearly scales the search space from minValue to maxValue.\n\n**Learning Rate:** UNIT_LOG_SCALE is generally recommended for hyperparameters with values spanning multiple orders of magnitude like the learning rate (10e-05 - 10e-02). This ensures equal sampling probability across different log-scaled ranges.\n\n**Parallel Trials:** as the documentation specifies, parallelization speeds up. However, this speedup comes at the cost of potentially sacrificing the quality of the results. Since training time is not a factor in this case, the benefit of speeding things up with many parallel trials is less valuable.\n\nhttps://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning#parallel-trials","poster":"guilhermebutzke","comment_id":"1137941","upvote_count":"14","timestamp":"1706822520.0"},{"comment_id":"744685","content":"Selected Answer: B\nVote B","timestamp":"1670988480.0","poster":"YangG","upvote_count":"11"},{"poster":"NamitSehgal","comment_id":"1359571","upvote_count":"1","timestamp":"1740100080.0","content":"Selected Answer: A\nmaxParallelTrials:\n Small number of parallel trials: A small number of trials would limit the exploration of the hyperparameter space and might prevent you from finding the best possible model.\nSince training time is not a concern for you, and you want to maximize model accuracy, using a large number of parallel trials is beneficial."},{"poster":"AB_C","upvote_count":"3","comment_id":"1318260","content":"Selected Answer: A\nas training time is not an issue hence the answer should be A","timestamp":"1732647540.0"},{"timestamp":"1713198540.0","comment_id":"1196127","poster":"pinimichele01","upvote_count":"1","content":"Training time is not a concern -> B (the benefit of speeding things up with many parallel trials is less valuable)"},{"content":"Selected Answer: B\nVote B","upvote_count":"1","timestamp":"1713155880.0","comment_id":"1195800","poster":"gscharly"},{"comment_id":"1068723","content":"Selected Answer: A\nbecause training time is not a concern and you want to maximize accuracy, using a large number of maxParallelTrials (option A) allows thoroughly searching the hyperparameter space.","poster":"Mickey321","timestamp":"1699810620.0","upvote_count":"3"},{"poster":"Voyager2","upvote_count":"1","comment_id":"916175","content":"Selected Answer: B\nB. Use UNIT_LINEAR_SCALE for the embedding dimension, UNIT_LOG_SCALE for the learning rate, and a small number of parallel trials.\nhttps://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning\nFirst we should choos an option with small trials: \n\"Before starting a job with a large number of trials, you may want to start with a small number of trials to gauge the effect your chosen hyperparameters have on your model's accuracy.\"\nNow, the embeddings should be linear https://cloud.google.com/blog/products/gcp/hyperparameter-tuning-on-google-cloud-platform-is-now-faster-and-smarter","timestamp":"1686050340.0"},{"timestamp":"1683644760.0","upvote_count":"1","content":"Selected Answer: B\nWent with B","poster":"M25","comment_id":"893224"},{"timestamp":"1680085740.0","comment_id":"854262","upvote_count":"2","poster":"JamesDoe","content":"Selected Answer: B\nhttps://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning#parallel-trials\n\"Running parallel trials has the benefit of reducing the time the training job takes (real time—the total processing time required is not typically changed). However, running in parallel can reduce the effectiveness of the tuning job overall.\"\nSince opt. for accuracy and ignore training time, use above. \nLinear for learning rate doesn't really make sense, think that one is obvious imo."},{"comment_id":"834283","timestamp":"1678388220.0","poster":"TNT87","content":"Selected Answer: B\nAnswer is B , even my explanation is on B not C\nOption B is the best choice: Use UNIT_LOG_SCALE for the embedding dimension, UNIT_LINEAR_SCALE for the learning rate, and a large number of parallel trials.\n\nThe reason for this choice is as follows:\n\nFor the embedding dimension, it is better to use a logarithmic scale because the effect of increasing the dimensionality is likely to diminish as the dimension grows larger. Therefore, the logarithmic scale will allow the tuning algorithm to explore a wider range of values with less bias towards higher values","upvote_count":"1"},{"comments":[{"upvote_count":"1","content":"Meant to choose B ahhhh","timestamp":"1678388160.0","poster":"TNT87","comment_id":"834282"}],"timestamp":"1678192320.0","upvote_count":"1","comment_id":"831873","content":"Selected Answer: C\nOption C is the best choice: Use UNIT_LOG_SCALE for the embedding dimension, UNIT_LINEAR_SCALE for the learning rate, and a large number of parallel trials.\n\nThe reason for this choice is as follows:\n\nFor the embedding dimension, it is better to use a logarithmic scale because the effect of increasing the dimensionality is likely to diminish as the dimension grows larger. Therefore, the logarithmic scale will allow the tuning algorithm to explore a wider range of values with less bias towards higher values","poster":"TNT87"},{"content":"Selected Answer: B\nLearning Rage is subtle and take time so, it use Log Scale","upvote_count":"2","poster":"John_Pongthorn","timestamp":"1674472260.0","comment_id":"785270"},{"upvote_count":"1","comment_id":"768445","content":"Selected Answer: B\nIt's B!","poster":"ares81","timestamp":"1673088000.0"},{"upvote_count":"1","comments":[{"comment_id":"758547","poster":"hiromi","upvote_count":"1","content":"Sorry, B is the answer","timestamp":"1672147500.0"}],"content":"Selected Answer: A\nA\n- https://cloud.google.com/ai-platform/training/docs/reference/rest/v1/projects.jobs#HyperparameterSpec\n- https://cloud.google.com/vertex-ai/docs/reference/rest/v1beta1/StudySpec","comment_id":"753111","poster":"hiromi","timestamp":"1671699960.0"},{"comment_id":"744301","content":"Selected Answer: D\nVote D, this can help the tuning algorithm explore a wider range of values for the learning rate, while also focusing on a smaller range of values for the embedding dimension.","timestamp":"1670951880.0","upvote_count":"2","poster":"mil_spyro"}],"choices":{"A":"Use UNIT_LINEAR_SCALE for the embedding dimension, UNIT_LOG_SCALE for the learning rate, and a large number of parallel trials.","D":"Use UNIT_LOG_SCALE for the embedding dimension, UNIT_LINEAR_SCALE for the learning rate, and a small number of parallel trials.","B":"Use UNIT_LINEAR_SCALE for the embedding dimension, UNIT_LOG_SCALE for the learning rate, and a small number of parallel trials.","C":"Use UNIT_LOG_SCALE for the embedding dimension, UNIT_LINEAR_SCALE for the learning rate, and a large number of parallel trials."},"question_id":39,"answers_community":["B (76%)","A (18%)","4%"],"unix_timestamp":1670951880,"topic":"1","timestamp":"2022-12-13 18:18:00","isMC":true,"url":"https://www.examtopics.com/discussions/google/view/91467-exam-professional-machine-learning-engineer-topic-1-question/","question_images":[],"answer_images":[],"question_text":"You have recently created a proof-of-concept (POC) deep learning model. You are satisfied with the overall architecture, but you need to determine the value for a couple of hyperparameters. You want to perform hyperparameter tuning on Vertex AI to determine both the appropriate embedding dimension for a categorical feature used by your model and the optimal learning rate. You configure the following settings:\n• For the embedding dimension, you set the type to INTEGER with a minValue of 16 and maxValue of 64.\n• For the learning rate, you set the type to DOUBLE with a minValue of 10e-05 and maxValue of 10e-02.\n\nYou are using the default Bayesian optimization tuning algorithm, and you want to maximize model accuracy. Training time is not a concern. How should you set the hyperparameter scaling for each hyperparameter and the maxParallelTrials?","answer_ET":"B","answer_description":"","answer":"B"},{"id":"RetLaIalEuM3iXL1SPF3","answer_description":"","exam_id":13,"question_text":"You are the Director of Data Science at a large company, and your Data Science team has recently begun using the Kubeflow Pipelines SDK to orchestrate their training pipelines. Your team is struggling to integrate their custom Python code into the Kubeflow Pipelines SDK. How should you instruct them to proceed in order to quickly integrate their code with the Kubeflow Pipelines SDK?","timestamp":"2022-12-13 18:21:00","unix_timestamp":1670952060,"isMC":true,"discussion":[{"poster":"hiromi","comment_id":"753116","timestamp":"1719040200.0","content":"Selected Answer: A\nA\n-https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.components.html?highlight=func_to_container_op%20#kfp.components.func_to_container_op","upvote_count":"5"},{"comment_id":"893239","upvote_count":"2","poster":"M25","content":"Selected Answer: A\nWent with A","timestamp":"1731172560.0"},{"poster":"Antmal","timestamp":"1729281420.0","content":"Selected Answer: A\nThe answer is A. because the Kubeflow Pipelines SDK provides a convenient way to create custom components from existing Python code using the func_to_container_op function. This allows data science team to encapsulate the custom code as containerised components that can be easily integrated into the kubeflow pipeline. This approach allows for seamless integration of custom Python code into the Kubeflow Pipelines SDK without requiring additional dependencies or infrastructure setup.","comment_id":"874065","upvote_count":"2"},{"upvote_count":"3","content":"Selected Answer: A\nA. Use the func_to_container_op function to create custom components from the Python code.\n\nThe func_to_container_op function in the Kubeflow Pipelines SDK is specifically designed to convert Python functions into containerized components that can be executed in a Kubernetes cluster. By using this function, the Data Science team can easily integrate their custom Python code into the Kubeflow Pipelines SDK without having to learn the details of containerization or Kubernetes.","comment_id":"831872","poster":"TNT87","timestamp":"1725705000.0"},{"timestamp":"1718292060.0","upvote_count":"2","content":"Selected Answer: A\nUse the func_to_container_op function to create custom components from their code. This function allows you to define a Python function that can be used as a pipeline component, and it automatically creates a Docker container with the necessary dependencies","comment_id":"744303","poster":"mil_spyro"}],"question_images":[],"topic":"1","answer":"A","answer_images":[],"answer_ET":"A","url":"https://www.examtopics.com/discussions/google/view/91468-exam-professional-machine-learning-engineer-topic-1-question/","question_id":40,"choices":{"B":"Use the predefined components available in the Kubeflow Pipelines SDK to access Dataproc, and run the custom code there.","D":"Deploy the custom Python code to Cloud Functions, and use Kubeflow Pipelines to trigger the Cloud Function.","C":"Package the custom Python code into Docker containers, and use the load_component_from_file function to import the containers into the pipeline.","A":"Use the func_to_container_op function to create custom components from the Python code."},"answers_community":["A (100%)"]}],"exam":{"numberOfQuestions":304,"isImplemented":true,"lastUpdated":"11 Apr 2025","id":13,"isMCOnly":true,"isBeta":false,"provider":"Google","name":"Professional Machine Learning Engineer"},"currentPage":8},"__N_SSP":true}