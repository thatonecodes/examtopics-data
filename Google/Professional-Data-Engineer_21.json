{"pageProps":{"questions":[{"id":"VQf961XtAQruPSD42OAe","isMC":true,"url":"https://www.examtopics.com/discussions/google/view/16870-exam-professional-data-engineer-topic-1-question-19/","timestamp":"2020-03-17 16:29:00","answers_community":["A (88%)","13%"],"topic":"1","discussion":[{"poster":"anji007","comment_id":"462672","content":"Ans: A\nB: Wrong eVM wont solve the problem of larger storage prices.\nC: May be, but nothing mentioned in terms of what to tune in the question, also this is like-for-like migration so tuning may not be part of the migration.\nD: Again, this is like-for-like so need to define what is hot data and which is cold data, also persistent disk costlier than cloud storage.","timestamp":"1727155800.0","upvote_count":"8"},{"comment_id":"1559569","content":"Selected Answer: A\nA like-for-like migration to Cloud Dataproc that replicates on-premises Hadoop would require each node to have 50 TB of persistent disk, which is costly. Instead, you can minimize storage costs by leveraging Google Cloud Storage (GCS). Cloud Dataproc seamlessly integrates with GCS through the Hadoop connector, allowing you to store your data cost-effectively in Cloud Storage and run ephemeral clusters that read data directly from GCS. This approach eliminates the need for each node to carry 50 TB of expensive persistent disk storage while still supporting your Hadoop workload.","upvote_count":"1","timestamp":"1744286700.0","poster":"fassil"},{"comment_id":"1398857","poster":"Parandhaman_Margan","upvote_count":"1","timestamp":"1742047140.0","content":"Selected Answer: D\n. Migrate some of the cold data into Google Cloud Storage, and keep only the hot data in Persistent Disk.\nGoogle Cloud Storage (GCS) is a cost-effective alternative to Persistent Disk for storing less frequently accessed (\"cold\") data.\nHot data that requires fast access can remain on Persistent Disk, reducing storage costs while maintaining performance.\nCloud Dataproc supports HDFS-to-GCS integration, allowing Hadoop jobs to access data in GCS seamlessly.D"},{"upvote_count":"1","poster":"Vullibabu","content":"You are most of the people looking at like for like migration would require 50TB persistent storage but missing to look at CIO concern about cost of block storage...considering CIO concern the option here is cloud storage... moreover that is recommended as well ..","comment_id":"1114870","timestamp":"1704504360.0"},{"upvote_count":"2","timestamp":"1696647960.0","content":"Option A: Put the data into Google Cloud Storage.\n\n This is the best option. Google Cloud Dataproc is designed to work well with Google Cloud Storage. Using GCS instead of Persistent Disk can save money, and GCS offers advantages such as higher durability and the ability to share data across multiple clusters.","poster":"imran79","comment_id":"1027040"},{"poster":"emmylou","timestamp":"1696351140.0","upvote_count":"1","comment_id":"1024079","content":"I have seen this question in other places and I believe that you store the older data in Cloud Storage and retain processing data in persistent disk. D"},{"comment_id":"998830","comments":[{"upvote_count":"1","poster":"suku2","content":"Google Cloud Storage is designed for 11 9's availability. So it is also kind of persistent storage. Also, it is a Google product, hence recommended.\nhttps://cloud.google.com/storage/docs/availability-durability#key-concepts","comment_id":"1008592","timestamp":"1694795700.0"}],"poster":"hxy8","timestamp":"1693859460.0","content":"Answer: D\nQuestion: A like-for- like migration of the cluster would require 50 TB of Google Persistent Disk per node.\nwhich means Persistent is still required.","upvote_count":"1"},{"upvote_count":"1","comment_id":"982660","poster":"GHOST1985","content":"the question is talking about block storage , GCS is object storage !","timestamp":"1692195240.0"},{"comment_id":"970828","upvote_count":"1","content":"Selected Answer: A\nGCS is cost-effective and also Google's recommendation!","poster":"hjava","timestamp":"1691045820.0"},{"comment_id":"835668","timestamp":"1678510380.0","upvote_count":"1","poster":"bha11111","content":"Selected Answer: A\nMinimize cost then GCS"},{"poster":"Nirca","comment_id":"771646","upvote_count":"1","content":"Selected Answer: A\nA - is the right answer.","timestamp":"1673372220.0"},{"upvote_count":"1","comment_id":"743395","content":"Selected Answer: A\nA - dataproc - storage - cost effective is cloud storage","timestamp":"1670885820.0","poster":"DGames"},{"content":"Selected Answer: A\nCloud Storage","poster":"devaid","timestamp":"1665163440.0","upvote_count":"1","comment_id":"688843"},{"comment_id":"609287","timestamp":"1653925500.0","content":"Selected Answer: A\nCloud Storage is google recommended one","upvote_count":"1","poster":"sankar_s"},{"comments":[{"upvote_count":"6","poster":"sumanshu","timestamp":"1727155860.0","comment_id":"401776","content":"A is correct because Google recommends using Cloud Storage instead of HDFS as it is much more cost effective especially when jobs aren’t running.\nB is not correct because this will decrease the compute cost but not the storage cost.\nC is not correct because while this will reduce cost somewhat, it will not be as cost effective as using Cloud Storage.\nD is not correct because while this will reduce cost somewhat, it will not be as cost effective as using Cloud Storage."}],"content":"Vote for 'A\"","upvote_count":"2","timestamp":"1624646580.0","poster":"sumanshu","comment_id":"390715"},{"comment_id":"349176","timestamp":"1620108720.0","upvote_count":"1","poster":"anudeepgupta42","content":"A, Moving the data to GCS will reduce the cost of running the dataproc clusters all the time \\"},{"comment_id":"284981","poster":"naga","content":"Correct A","timestamp":"1612630920.0","upvote_count":"2"},{"timestamp":"1604324880.0","poster":"Alasmindas","content":"The correct answer is A - Put the data into google cloud storage. This is what Google always recommend.","upvote_count":"2","comment_id":"211256"},{"comment_id":"173666","poster":"AaronLee","upvote_count":"4","content":"Because the cluster is going to end-of-life. So maybe don't need to put hot data in the disk. All can be put in the Cloud Storage. The answer is A.","timestamp":"1599262920.0"},{"comments":[{"timestamp":"1596281040.0","upvote_count":"16","poster":"MadHolm","content":"That would be correct if we wanted to maximize performance without sacrificing cost too much, but we want to minimize the cost. That's the only requirement, hence answer A.","comment_id":"148529"}],"upvote_count":"4","comment_id":"89222","poster":"aadaisme","content":"I would go for D. First rule of dataproc is to keep data in GCS. Use persistant disk for high I/O data (hot data).","timestamp":"1589502540.0"}],"answer_images":[],"answer_description":"","question_text":"Your company's on-premises Apache Hadoop servers are approaching end-of-life, and IT has decided to migrate the cluster to Google Cloud Dataproc. A like-for- like migration of the cluster would require 50 TB of Google Persistent Disk per node. The CIO is concerned about the cost of using that much block storage. You want to minimize the storage cost of the migration. What should you do?","answer_ET":"A","exam_id":11,"answer":"A","unix_timestamp":1584458940,"question_id":101,"question_images":[],"choices":{"A":"Put the data into Google Cloud Storage.","D":"Migrate some of the cold data into Google Cloud Storage, and keep only the hot data in Persistent Disk.","C":"Tune the Cloud Dataproc cluster so that there is just enough disk for all data.","B":"Use preemptible virtual machines (VMs) for the Cloud Dataproc cluster."}},{"id":"23UnYqu8gnDgnMMiEZ27","answer_description":"","question_images":[],"question_text":"You are loading CSV files from Cloud Storage to BigQuery. The files have known data quality issues, including mismatched data types, such as STRINGs and\nINT64s in the same column, and inconsistent formatting of values such as phone numbers or addresses. You need to create the data pipeline to maintain data quality and perform the required cleansing and transformation. What should you do?","isMC":true,"answer":"A","topic":"1","answers_community":["A (87%)","13%"],"question_id":102,"answer_images":[],"answer_ET":"A","discussion":[{"comment_id":"748741","upvote_count":"6","content":"Selected Answer: A\nI'm kinda inclined towards C as SQL seems a powerful option to treat this kind of use case.\n\nAlso, I didn't get how the transformations mentioned on this page will help to clean the data (https://cloud.google.com/data-fusion/docs/concepts/transformation-pushdown#supported_transformations)\n\nBut I guess using Wrangler plugin, this kind of stuff can be done on DataFusion, also the question talks about an pipeline, so A is the final choice.","poster":"saurabhsingh4k","timestamp":"1687077360.0"},{"poster":"MaxNRG","comments":[{"poster":"MaxNRG","content":"Why other options are less suitable:\n\nB. Converting to AVRO: While AVRO is a self-describing format, it doesn't inherently address data quality issues. Transformations would still be needed, and Data Fusion provides a more comprehensive environment for this.\nC. Staging table: Requires manual SQL transformations, which can be time-consuming and error-prone for large datasets with complex data quality issues.\nD. Transformations in place: Modifying data directly in the destination table can lead to data loss or corruption if errors occur. It's generally safer to keep raw data intact and perform transformations separately.\nBy using Data Fusion, you can create a robust and efficient data pipeline that addresses data quality issues upfront, ensuring that only clean and consistent data is loaded into BigQuery for accurate analysis and insights.","upvote_count":"1","timestamp":"1718961360.0","comment_id":"1102388"}],"comment_id":"1102387","timestamp":"1718961360.0","content":"Selected Answer: A\nData Fusion's advantages:\n\nVisual interface: Offers a user-friendly interface for designing data pipelines without extensive coding, making it accessible to a wider range of users.\nBuilt-in transformations: Includes a wide range of pre-built transformations to handle common data quality issues, such as:\nData type conversions\nData cleansing (e.g., removing invalid characters, correcting formatting)\nData validation (e.g., checking for missing values, enforcing constraints)\nData enrichment (e.g., adding derived fields, joining with other datasets)\nCustom transformations: Allows for custom transformations using SQL or Java code for more complex cleaning tasks.\nScalability: Can handle large datasets efficiently, making it suitable for processing CSV files with potential data quality issues.\nIntegration with BigQuery: Integrates seamlessly with BigQuery, allowing for direct loading of transformed data.","upvote_count":"3"},{"content":"The answer is C. That is what we do at work. We have landing/staging table, sort table and deliver table,","comment_id":"1051814","upvote_count":"4","comments":[{"timestamp":"1713875100.0","upvote_count":"4","poster":"squishy_fishy","content":"Okay, second thought, it is asking for a pipeline, so the answer should be A. At work, we use dataflow inside the composer to build a pipeline injecting data into landing/staging table, then transform/clean data in the sort table, then send the cleaned data to deliver table.","comment_id":"1051818"}],"timestamp":"1713874800.0","poster":"squishy_fishy"},{"content":"Selected Answer: A\nKeyword: Data Pipeline","upvote_count":"4","timestamp":"1702318980.0","poster":"phidelics","comment_id":"920836"},{"timestamp":"1699341480.0","content":"Selected Answer: A\nsame as @saurabhsingh4k","upvote_count":"2","comment_id":"891170","poster":"mialll"},{"content":"Selected Answer: C\nC is the right answer. Do ELT in BigQuery. Data Fusion is not the right too for this job.","comment_id":"872217","poster":"Adswerve","upvote_count":"4","timestamp":"1697498580.0"},{"timestamp":"1692443520.0","content":"Answer C, \nDatafusion is costly and current transformation is just a cast transformation in a column. \nI guess no one wanna pay for datafusion for this little transformation and Staging table processing handles such minor cleaning.","upvote_count":"4","poster":"musumusu","comment_id":"814099"},{"timestamp":"1690315920.0","content":"Selected Answer: A\nData Fusion enables changing the data type directly as shown in this lab: https://www.cloudskillsboost.google/focuses/25335?parent=catalog \nWrangler is the feature to enable that, as already mentioned: https://stackoverflow.com/questions/58699872/google-cloud-data-fusion-how-to-change-datatype-from-string-to-date","poster":"maci_f","upvote_count":"4","comment_id":"788172"},{"timestamp":"1688198340.0","comment_id":"763134","upvote_count":"2","poster":"Mike422","content":"Apparently chatGPT thinks C is the correct answer just sayin (for the same reason that @saurabhsingh4k wrote)"},{"timestamp":"1686898800.0","comment_id":"746987","poster":"Atnafu","upvote_count":"1","content":"A\nhttps://cloud.google.com/data-fusion/docs/concepts/overview#:~:text=The%20Cloud%20Data%20Fusion%20web%20UI%20lets%20you%20to%20build%20scalable%20data%20integration%20solutions%20to%20clean%2C%20prepare%2C%20blend%2C%20transfer%2C%20and%20transform%20data%2C%20without%20having%20to%20manage%20the%20infrastructure."},{"timestamp":"1685285580.0","comments":[{"comment_id":"763414","poster":"AzureDP900","timestamp":"1688251200.0","content":"thx for sharing link","upvote_count":"1"}],"content":"Selected Answer: A\nA is the answer.\n\nhttps://cloud.google.com/data-fusion/docs/concepts/overview\nloud Data Fusion is a fully managed, cloud-native, enterprise data integration service for quickly building and managing data pipelines.\n\nThe Cloud Data Fusion web UI lets you to build scalable data integration solutions to clean, prepare, blend, transfer, and transform data, without having to manage the infrastructure.","upvote_count":"4","poster":"zellck","comment_id":"729444"},{"comments":[{"poster":"jkhong","comment_id":"748196","content":"although this is my preferred answer. this doesn’t satisfy how this becomes a pipeline.","upvote_count":"1","timestamp":"1687011360.0"}],"content":"The Correct Ans is C","comment_id":"725425","timestamp":"1684874220.0","poster":"samirzubair","upvote_count":"2"},{"upvote_count":"1","comment_id":"723527","content":"Selected Answer: A\nData Fusion","poster":"hiromi","timestamp":"1684667580.0"},{"comment_id":"667696","upvote_count":"1","content":"Selected Answer: A\nAns A\nhttps://cloud.google.com/data-fusion/docs/concepts/transformation-pushdown#supported_transformations","poster":"TNT87","timestamp":"1678694400.0"},{"comment_id":"657959","upvote_count":"1","content":"Selected Answer: A\nA is correct for me","timestamp":"1677814980.0","poster":"ducc"},{"upvote_count":"1","poster":"AWSandeep","comment_id":"657846","timestamp":"1677798900.0","content":"Selected Answer: A\nA. Use Data Fusion to transform the data before loading it into BigQuery."}],"exam_id":11,"timestamp":"2022-09-02 23:15:00","url":"https://www.examtopics.com/discussions/google/view/79609-exam-professional-data-engineer-topic-1-question-190/","choices":{"D":"Create a table with the desired schema, load the CSV files into the table, and perform the transformations in place using SQL.","A":"Use Data Fusion to transform the data before loading it into BigQuery.","C":"Load the CSV files into a staging table with the desired schema, perform the transformations with SQL, and then write the results to the final destination table.","B":"Use Data Fusion to convert the CSV files to a self-describing data format, such as AVRO, before loading the data to BigQuery."},"unix_timestamp":1662153300},{"id":"ollaLBMsiYjD3pxkGWzB","url":"https://www.examtopics.com/discussions/google/view/79643-exam-professional-data-engineer-topic-1-question-191/","question_text":"You are developing a new deep learning model that predicts a customer's likelihood to buy on your ecommerce site. After running an evaluation of the model against both the original training data and new test data, you find that your model is overfitting the data. You want to improve the accuracy of the model when predicting new data. What should you do?","question_images":[],"answer":"B","question_id":103,"answer_ET":"B","topic":"1","choices":{"A":"Increase the size of the training dataset, and increase the number of input features.","B":"Increase the size of the training dataset, and decrease the number of input features.","C":"Reduce the size of the training dataset, and increase the number of input features.","D":"Reduce the size of the training dataset, and decrease the number of input features."},"answer_images":[],"answers_community":["B (96%)","4%"],"timestamp":"2022-09-03 03:46:00","discussion":[{"comment_id":"680408","timestamp":"1679894340.0","content":"Selected Answer: B\nThere 2 parts and they are relevant to each other\n1. Overfit is fixed by decreasing the number of input features (select only essential features)\n2. Accuracy is improved by increasing the amount of training data examples.","upvote_count":"11","poster":"John_Pongthorn","comments":[{"comment_id":"680409","timestamp":"1679894340.0","upvote_count":"2","content":"https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html","poster":"John_Pongthorn"}]},{"poster":"Matt_108","comment_id":"1122029","upvote_count":"2","content":"Selected Answer: B\nOption B, the model learned to listen to too much stuff/noise. We need to reduce it, by decreasing the number of input feature, and we need to give the model more data, by increasing the amount of training data","timestamp":"1720895280.0"},{"upvote_count":"1","poster":"NeoNitin","content":"Increase the size of the training dataset: By adding more diverse examples of customers and their buying behavior to the training data, the model will have a broader understanding of different scenarios and be better equipped to generalize to new customers.\n\nIncrease the number of input features: Providing the model with more relevant information about customers can help it identify meaningful patterns and make better predictions. These input features could include things like the customer's age, past purchase history, browsing behavior, or any other relevant data that might impact their buying likelihood.","timestamp":"1706597640.0","comment_id":"966864"},{"content":"Selected Answer: B\nA. can be a solution for a specific case, but it is not the academic answer as we do not know the quantity and proportion between them of n and k added. More records and more variables together can lead to even more overfitting due also to the curse of dimensionality. Adding a variable is much more impactful than records.\nB. just more records can lead to a more robust estimation and fewer variables certainly lead to at most the same estimation, but potentially reduce the fit on the training set.\nC. reduce n in favor of k is never a choice. it is against logic and it will lead to more overfitting.\nD. decrease both will reduce overfitting for sure but at the price of losing robustness on the model predictive power","comment_id":"901141","upvote_count":"1","timestamp":"1700314560.0","poster":"vaga1"},{"content":"B. Increase the size of the training dataset, and decrease the number of input features.","upvote_count":"1","timestamp":"1688251380.0","comment_id":"763415","poster":"AzureDP900"},{"poster":"pluiedust","timestamp":"1678794060.0","content":"Selected Answer: B\nB is correct","comment_id":"668848","upvote_count":"2"},{"poster":"TNT87","upvote_count":"3","timestamp":"1678258440.0","content":"Answer B\nhttps://machinelearningmastery.com/impact-of-dataset-size-on-deep-learning-model-skill-and-performance-estimates/","comment_id":"663116"},{"content":"Selected Answer: B\nOption B\nFeature selection is the one the ways to resolve overfitting. Which means reducing the features\nwhen the size of the training data is small, then the network tends to have greater control over the training data. so increasing the size of data would help.","poster":"HarshKothari21","timestamp":"1678213920.0","upvote_count":"3","comment_id":"662701"},{"timestamp":"1678108140.0","poster":"YorelNation","content":"Selected Answer: B\nBest option is not mentioned: generalize you neural net by decreasing the complexity of it's structure.\n\nA part from that I guess you could remove some features and increase the size of the training dataset ==> B","upvote_count":"1","comment_id":"661136"},{"comment_id":"659541","timestamp":"1677970560.0","upvote_count":"1","poster":"AWSandeep","content":"Selected Answer: B\nB. Increase the size of the training dataset, and decrease the number of input features.\n\nSorry, B is right. Read through extensive best-practices on ML."},{"poster":"ducc","timestamp":"1677899880.0","upvote_count":"1","content":"Selected Answer: D\nD is correct","comment_id":"658883"},{"upvote_count":"1","timestamp":"1677821940.0","comment_id":"658032","content":"D. Reduce the size of the training dataset, and decrease the number of input features.\nReveal Solution","poster":"AWSandeep"},{"content":"Selected Answer: B\nB. Increase the size of the training dataset, and decrease the number of input features.","timestamp":"1677815160.0","comment_id":"657963","poster":"ducc","upvote_count":"1"}],"exam_id":11,"isMC":true,"unix_timestamp":1662169560,"answer_description":""},{"id":"1WBRAtpaAqBhNNwkvvrU","answers_community":["D (88%)","12%"],"answer_ET":"D","unix_timestamp":1662141240,"question_images":[],"question_id":104,"answer_description":"","answer":"D","question_text":"You are implementing a chatbot to help an online retailer streamline their customer service. The chatbot must be able to respond to both text and voice inquiries.\nYou are looking for a low-code or no-cade option, and you want to be able to easily train the chatbot to provide answers to keywords. What should you do?","answer_images":[],"discussion":[{"content":"Selected Answer: D\nD is correct:\nhttps://cloud.google.com/dialogflow/es/docs/how/detect-intent-tts#:~:text=Dialogflow%20can%20use%20Cloud%20Text,to%2Dspeech%2C%20or%20TTS.","poster":"PhuocT","comment_id":"657682","timestamp":"1677786840.0","upvote_count":"12"},{"upvote_count":"2","content":"Selected Answer: D\nThe best option would be to use Dialogflow to implement the chatbot, defining the intents based on the most common queries collected.\n\nDialogflow is a conversational AI platform that allows for easy implementation of chatbots without needing to code. It has built-in integration for both text and voice input via APIs like Cloud Speech-to-Text. Defining intents and entity types allows you to map common queries and keywords to responses. This would provide a low/no-code way to quickly build and iteratively improve the chatbot capabilities.\n\nOption A and B would require more heavy coding to handle speech input/output. Option C still requires coding the complex query handling. Only option D leverages the full capabilities of Dialogflow to enable no-code chatbot development and ongoing improvements as more conversational data is collected. Hence, option D is the best approach given the requirements.","timestamp":"1718984100.0","comment_id":"1102804","poster":"MaxNRG"},{"content":"Selected Answer: D\nLow-code or no-cade requirement makes it easy to decide.","poster":"Lanro","upvote_count":"1","comment_id":"968102","timestamp":"1706711640.0"},{"timestamp":"1685285460.0","poster":"zellck","upvote_count":"4","comment_id":"729440","content":"Selected Answer: D\nD is the answer.\n\nhttps://cloud.google.com/dialogflow/docs\nDialogflow is a natural language understanding platform that makes it easy to design and integrate a conversational user interface into your mobile app, web application, device, bot, interactive voice response system, and so on. Using Dialogflow, you can provide new and engaging ways for users to interact with your product.\n\nDialogflow can analyze multiple types of input from your customers, including text or audio inputs (like from a phone or voice recording). It can also respond to your customers in a couple of ways, either through text or with synthetic speech.","comments":[{"upvote_count":"1","comment_id":"763418","timestamp":"1688251920.0","content":"Agree with D","poster":"AzureDP900"}]},{"content":"D\nhttps://cloud.google.com/dialogflow/es/docs/how/detect-intent-tts#:~:text=Dialogflow%20can%20use%20Cloud%20Text,to%2Dspeech%2C%20or%20TTS.","poster":"Atnafu","timestamp":"1684904880.0","comment_id":"725617","upvote_count":"1"},{"upvote_count":"4","comment_id":"697463","poster":"devaid","content":"Selected Answer: D\nD definitely, as the documentation says (specially that you can call the detect Intect method for audio inputs):\nhttps://cloud.google.com/dialogflow/es/docs/how/detect-intent-tts \nAlso Speech-To-Text API does nothing more than translate.","timestamp":"1681740900.0"},{"comment_id":"663123","timestamp":"1678258800.0","content":"Answer D\n\nhttps://cloud.google.com/dialogflow/es/docs/how/detect-intent-tts","poster":"TNT87","upvote_count":"4"},{"poster":"nwk","timestamp":"1678016460.0","comment_id":"660000","upvote_count":"2","content":"https://cloud.google.com/dialogflow/es/docs/how/detect-intent-stream\nVote D"},{"comment_id":"657965","content":"Selected Answer: C\nC. Use Dialogflow for simple queries and the Cloud Speech-to-Text API for complex queries.\n\nThis seem the best answer here but not the best answer in real world.\nBut with the Question, the answer must be the combination of both Diagflow and Speech API","poster":"ducc","timestamp":"1677815340.0","upvote_count":"3"}],"url":"https://www.examtopics.com/discussions/google/view/79526-exam-professional-data-engineer-topic-1-question-192/","choices":{"B":"Use the Cloud Speech-to-Text API to build a Python application in a Compute Engine instance.","C":"Use Dialogflow for simple queries and the Cloud Speech-to-Text API for complex queries.","A":"Use the Cloud Speech-to-Text API to build a Python application in App Engine.","D":"Use Dialogflow to implement the chatbot, defining the intents based on the most common queries collected."},"exam_id":11,"timestamp":"2022-09-02 19:54:00","topic":"1","isMC":true},{"id":"WmIjlKRHzhA2SFqkHTMk","unix_timestamp":1662169920,"exam_id":11,"question_images":[],"timestamp":"2022-09-03 03:52:00","answer_ET":"D","answer_images":[],"answer":"D","url":"https://www.examtopics.com/discussions/google/view/79644-exam-professional-data-engineer-topic-1-question-193/","question_text":"An aerospace company uses a proprietary data format to store its flight data. You need to connect this new data source to BigQuery and stream the data into\nBigQuery. You want to efficiently import the data into BigQuery while consuming as few resources as possible. What should you do?","discussion":[{"content":"This has to be D. How could it even be B? The source is a proprietary format. Dataflow wouldn't have a built-in template to ead the file. You will have to create something custom.","upvote_count":"17","timestamp":"1682909580.0","comment_id":"708849","poster":"beanz00"},{"upvote_count":"12","timestamp":"1681741380.0","poster":"devaid","comment_id":"697465","content":"Selected Answer: D\nFor me it's clearly D\nIt's between B and D, but read B, store raw data in Big Query? Use a Dataflow pipeline just to store raw data into Big Query, and transform later? You'd need to do another pipeline for that, and is not efficient."},{"comment_id":"1102811","timestamp":"1718984460.0","upvote_count":"3","poster":"MaxNRG","content":"Selected Answer: D\nOption D is the best approach given the constraints - use an Apache Beam custom connector to write a Dataflow pipeline that streams the data into BigQuery in Avro format.\nThe key reasons:\n• Dataflow provides managed resource scaling for efficient stream processing\n• Avro format has schema evolution capabilities and efficient serialization for flight telemetry data\n• Apache Beam connectors avoid having to write much code to integrate proprietary data sources\n• Streaming inserts data efficiently compared to periodic batch jobs\nIn contrast, option A uses Cloud Functions which lack native streaming capabilities. Option B stores data in less efficient JSON format. Option C uses Dataproc which requires manual cluster management.\nSo leveraging Dataflow + Avro + Beam provides the most efficient way to stream proprietary flight data into BigQuery while using minimal resources."},{"poster":"Aman47","content":"Its talking about streaming? none of the options talk about triggering a load to begin. We need a trigger or schedule to run first.","upvote_count":"1","comment_id":"1096641","timestamp":"1718374980.0"},{"upvote_count":"2","comment_id":"1044098","timestamp":"1713182040.0","poster":"AjoseO","content":"Selected Answer: D\nOption D allows you to use a custom connector to read the proprietary data format and write the data to BigQuery in Avro format."},{"comment_id":"1002883","upvote_count":"1","timestamp":"1709963460.0","content":"Selected Answer: D\nthe keyword is streaming","poster":"sergiomujica"},{"timestamp":"1706416200.0","content":"Between B and D. Firstly transformation is not mentioned in the question, So B is less probable. Then Efficient import is mentioned in the question, Converting to Avro will consume less space. I am going with D","comment_id":"965227","upvote_count":"3","poster":"knith66"},{"comment_id":"814109","poster":"musumusu","timestamp":"1692444120.0","content":"Answer is D , \nWhy not B, changing data format before uploading to bigquery is good approach.","upvote_count":"1"},{"comment_id":"785337","upvote_count":"1","content":"Selected Answer: B\nI believe keyword here is \"An aerospace company uses a proprietary data format\"\nSo if we list the connectors available in Apache Beam, we are listed with these options;\nhttps://beam.apache.org/documentation/io/connectors/\n\nSo I believe, we have to create our own custom connector to read from the proprietary data format hence the answer should be B","poster":"cetanx","timestamp":"1690108200.0","comments":[{"poster":"cetanx","content":"sorry the answer should be D","timestamp":"1690108260.0","upvote_count":"1","comment_id":"785339"}]},{"content":"D is right","upvote_count":"1","poster":"AzureDP900","timestamp":"1688252040.0","comment_id":"763419"},{"content":"Selected Answer: D\nD is the answer.","timestamp":"1685951820.0","comment_id":"735822","comments":[{"timestamp":"1689072240.0","comment_id":"772489","poster":"TNT87","upvote_count":"1","content":"There is dataflow connector and D isnt cost effective"}],"poster":"zellck","upvote_count":"3"},{"timestamp":"1685858040.0","content":"Selected Answer: B\nB is the most efficient","comment_id":"734927","upvote_count":"2","poster":"hauhau"},{"content":"https://cloud.google.com/spanner/docs/change-streams/use-dataflow#core-concepts","poster":"TNT87","timestamp":"1680774960.0","comment_id":"687656","upvote_count":"1"},{"content":"Ans B\nhttps://cloud.google.com/architecture/streaming-avro-records-into-bigquery-using-dataflow\nIs there a reason to use apache beam connector yet there is dataflow which is a standard solution for that scenario?","comments":[{"upvote_count":"1","poster":"TNT87","comments":[{"comment_id":"669078","comments":[{"comments":[{"comment_id":"686054","upvote_count":"2","timestamp":"1680599400.0","poster":"John_Pongthorn","content":"D: just have your team develop custom connector. \nhttps://cloud.google.com/architecture/bigquery-data-warehouse#storage_management\nInternally, BigQuery stores data in a proprietary columnar format called Capacitor, which has a number of benefits for data warehouse workloads. BigQuery uses a proprietary format\n\nI suppose this matter , it mean BQ use proprietary format by itself to work internally\nbut the question means data as proprietary format as input for ingesting into BQ."},{"timestamp":"1679496240.0","poster":"[Removed]","comments":[{"content":"BigQuery uses a proprietary format because it can evolve in tandem with the query engine, which takes advantage of deep knowledge of the data layout to optimize ...","timestamp":"1680064800.0","comment_id":"682340","upvote_count":"1","poster":"TNT87"}],"content":"Can Bigquery handle a proprietary file format?","comment_id":"676128","upvote_count":"1"}],"timestamp":"1678870860.0","upvote_count":"1","poster":"TNT87","comment_id":"669569","content":"Do you mind reading the links i provided and revisiting the question, then you will understand why D isnt the best. Why use Apache beam yet there is Dataflow"},{"timestamp":"1678870980.0","content":"Option D streams, thats not cost effective. We need something that is cost effectictive, hence B is the option","comments":[{"timestamp":"1679471460.0","comment_id":"675763","poster":"TNT87","content":"I mean that consumes fewer resources","upvote_count":"1"}],"comment_id":"669572","upvote_count":"1","poster":"TNT87"}],"content":"Can standard dataflow be used to ingest any proprietary format of file ?\nshouldn't we use custom apache beam connector ? \nSo I think it is D ,though it isn't simple ,But in this scenario they have asked to use less resources to import data","upvote_count":"1","timestamp":"1678809480.0","poster":"learner2610"}],"comment_id":"666712","timestamp":"1678614540.0","content":"https://cloud.google.com/blog/topics/developers-practitioners/bigquery-explained-data-ingestion"}],"timestamp":"1678259340.0","upvote_count":"2","poster":"TNT87","comment_id":"663133"},{"upvote_count":"3","timestamp":"1677822720.0","content":"Selected Answer: D\nD. Use an Apache Beam custom connector to write a Dataflow pipeline that streams the data into BigQuery in Avro format.\nReveal Solution","poster":"AWSandeep","comment_id":"658040"},{"poster":"ducc","comment_id":"657966","comments":[{"timestamp":"1677835080.0","comment_id":"658160","content":"Sorry, D is correct","poster":"ducc","upvote_count":"2"}],"upvote_count":"2","timestamp":"1677815520.0","content":"Selected Answer: B\nB is the most efficient for me."}],"question_id":105,"topic":"1","answers_community":["D (83%)","B (17%)"],"isMC":true,"choices":{"C":"Use Apache Hive to write a Dataproc job that streams the data into BigQuery in CSV format.","D":"Use an Apache Beam custom connector to write a Dataflow pipeline that streams the data into BigQuery in Avro format.","A":"Write a shell script that triggers a Cloud Function that performs periodic ETL batch jobs on the new data source.","B":"Use a standard Dataflow pipeline to store the raw data in BigQuery, and then transform the format later when the data is used."},"answer_description":""}],"exam":{"isMCOnly":true,"provider":"Google","isBeta":false,"id":11,"isImplemented":true,"name":"Professional Data Engineer","lastUpdated":"11 Apr 2025","numberOfQuestions":319},"currentPage":21},"__N_SSP":true}