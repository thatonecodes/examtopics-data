{"pageProps":{"questions":[{"id":"aXSi8OH7KQFHHI0vg9Np","choices":{"D":"Copy the ORC files on Cloud Storage, then create external BigQuery tables for the data scientist team.","A":"Import the ORC files to Bigtable tables for the data scientist team.","B":"Import the ORC files to BigQuery tables for the data scientist team.","C":"Copy the ORC files on Cloud Storage, then deploy a Dataproc cluster for the data scientist team."},"isMC":true,"answer_images":[],"answer_ET":"D","answer":"D","answer_description":"","answers_community":["D (80%)","C (15%)","5%"],"discussion":[{"poster":"raaad","content":"Selected Answer: D\n- It leverages the strengths of BigQuery for SQL-based exploration while avoiding additional costs and complexity associated with data transformation or migration. \n- The data remains in ORC format in Cloud Storage, and BigQuery's external tables feature allows direct querying of this data.","timestamp":"1704828000.0","comment_id":"1117778","upvote_count":"8","comments":[{"content":"There is a requirement to use a 'hive query engine'', and BQ is using only the hive metastore and his own engine, so 'D' seems a better fit here.","timestamp":"1724820060.0","comment_id":"1273786","poster":"nadavw","upvote_count":"1"}]},{"content":"I think C is the correct answer, DS want to explore the data in a \"similar way as they used the on-premises HDFS cluster with SQL on the Hive query engine\". Dataproc can help to create clusters quickly with the Hadoop cluster. CMIIW","comments":[{"timestamp":"1735400100.0","comment_id":"1333029","poster":"apoio.certificacoes.closer","upvote_count":"2","content":"I think \"Similar\" is doing a lot of heavy lift on the confusion. If it was equal, I'd say C. Since it similar, it can be GoogleSQL (Bigquery)."}],"comment_id":"1175921","poster":"kaisarfarel","timestamp":"1710689700.0","upvote_count":"6"},{"comment_id":"1337164","upvote_count":"1","content":"Selected Answer: D\nD. Copy the ORC files on Cloud Storage, then create external BigQuery tables for the data scientist team.\n\nThis approach allows you to leverage the scalability and cost-effectiveness of Cloud Storage while enabling your data scientists to query the data using BigQuery's powerful SQL engine without the need to move or transform the data. This setup also minimizes the need for additional infrastructure and maintenance, making it a practical choice for your analytics environment.","timestamp":"1736170440.0","poster":"Pime13"},{"poster":"SamuelTsch","content":"Selected Answer: B\nusing external tables have always limitations - affecting performance, no preview of the data and no cost estimation. So, why option D is correct?","upvote_count":"1","timestamp":"1730325480.0","comment_id":"1305205"},{"comment_id":"1174741","upvote_count":"1","content":"Selected Answer: D\noption d","timestamp":"1710565920.0","poster":"hanoverquay"},{"comment_id":"1171210","timestamp":"1710180000.0","poster":"0725f1f","content":"Selected Answer: C\nit is talking about partition as well","upvote_count":"3"},{"content":"Selected Answer: D\nOption D","timestamp":"1708497360.0","poster":"JyoGCP","comment_id":"1155315","upvote_count":"1"},{"content":"Selected Answer: D\nOption D - leverages BigQuery for SQL-based exploration on direct querying to cloud storage","comment_id":"1121824","timestamp":"1705161540.0","poster":"Matt_108","upvote_count":"2"},{"comment_id":"1116001","content":"Selected Answer: D\nThis approach leverages BigQuery's powerful analytics capabilities without the overhead of data transformation or maintaining a separate cluster, while also allowing your team to use SQL for data exploration, similar to their experience with the on-premises Hadoop/Hive environment.","poster":"Smakyel79","upvote_count":"3","timestamp":"1704644220.0"}],"timestamp":"2024-01-07 17:17:00","unix_timestamp":1704644220,"question_images":[],"exam_id":11,"topic":"1","question_id":196,"question_text":"You created an analytics environment on Google Cloud so that your data scientist team can explore data without impacting the on-premises Apache Hadoop solution. The data in the on-premises Hadoop Distributed File System (HDFS) cluster is in Optimized Row Columnar (ORC) formatted files with multiple columns of Hive partitioning. The data scientist team needs to be able to explore the data in a similar way as they used the on-premises HDFS cluster with SQL on the Hive query engine. You need to choose the most cost-effective storage and processing solution. What should you do?","url":"https://www.examtopics.com/discussions/google/view/130517-exam-professional-data-engineer-topic-1-question-275/"},{"id":"YuRcc8TaVGVSgpKy471Y","answer_ET":"C","exam_id":11,"isMC":true,"answers_community":["C (100%)"],"question_images":[],"choices":{"B":"Set the pipeline staging location as a regional Cloud Storage bucket.","C":"Specify a worker region by using the --region flag.","D":"Create an Eventarc trigger to resubmit the job in case of zonal failure when submitting the job.","A":"Submit duplicate pipelines in two different zones by using the --zone flag."},"topic":"1","unix_timestamp":1704306060,"timestamp":"2024-01-03 19:21:00","question_id":197,"discussion":[{"upvote_count":"10","timestamp":"1720879440.0","poster":"Matt_108","comment_id":"1121828","content":"Selected Answer: C\nOption C: https://cloud.google.com/dataflow/docs/guides/pipeline-workflows#zonal-failures"},{"poster":"raaad","comment_id":"1117788","upvote_count":"6","timestamp":"1720546380.0","content":"Selected Answer: C\n- Specifying a worker region (instead of a specific zone) allows Google Cloud's Dataflow service to manage the distribution of resources across multiple zones within that region"},{"comment_id":"1337165","poster":"Pime13","content":"Selected Answer: C\nC. Specify a worker region by using the --region flag.\n\nThis ensures that your Dataflow job is submitted to a region rather than a specific zone, providing higher availability and resilience against zonal failures\nhttps://cloud.google.com/dataflow/docs/guides/pipeline-workflows#zonal-failures","timestamp":"1736170680.0","upvote_count":"1"},{"content":"Selected Answer: C\nOption C","timestamp":"1724215320.0","poster":"JyoGCP","upvote_count":"1","comment_id":"1155319"},{"timestamp":"1720532100.0","poster":"Sofiia98","content":"Selected Answer: C\nhttps://cloud.google.com/dataflow/docs/guides/pipeline-workflows#zonal-failures","upvote_count":"1","comment_id":"1117605"},{"timestamp":"1720023660.0","comment_id":"1113016","content":"Selected Answer: C\nC. Specify a worker region by using the --region flag.","poster":"scaenruy","upvote_count":"2"}],"answer_images":[],"answer_description":"","question_text":"You are designing a Dataflow pipeline for a batch processing job. You want to mitigate multiple zonal failures at job submission time. What should you do?","url":"https://www.examtopics.com/discussions/google/view/130223-exam-professional-data-engineer-topic-1-question-276/","answer":"C"},{"id":"waw0a8MCmUceVZoOuB2C","answers_community":["B (100%)"],"discussion":[{"comments":[{"poster":"anushree09","timestamp":"1712848680.0","comment_id":"1193893","content":"Hopping windows are sliding windows. It makes sense to use that over tumbling (fixed) window because the ask is to collect last 30 seconds of data every 5 second","upvote_count":"2"}],"comment_id":"1117838","timestamp":"1704832980.0","upvote_count":"12","content":"Selected Answer: B\n- Hopping Window: Hopping windows are fixed-sized, overlapping intervals. \n- Aggregate data over the last 30 seconds, every 2 seconds, as hopping windows allow for overlapping data analysis.\n- Memorystore: Ideal for low-latency access required for real-time visualization and analysis.","poster":"raaad"},{"content":"OPTION A. (IGNORE MY Previous Comment)\n\n Tumbling windows are the best choice for this ride-hailing app because they provide accurate 2-second aggregations without the complexities of overlapping data. This is crucial for real-time decision-making and ensuring accurate visualization of supply and demand.\nHopping windows introduce potential inaccuracies and complexity, making them less suitable for this scenario. While they can be useful in other situations, they are not the optimal choice for real-time aggregation with strict accuracy requirements.","comment_id":"1254091","timestamp":"1721793600.0","upvote_count":"1","poster":"Jeyaraj"},{"comment_id":"1254089","upvote_count":"1","timestamp":"1721793540.0","poster":"Jeyaraj","content":"Option B.\n\n Tumbling windows are the best choice for this ride-hailing app because they provide accurate 2-second aggregations without the complexities of overlapping data. This is crucial for real-time decision-making and ensuring accurate visualization of supply and demand.\nHopping windows introduce potential inaccuracies and complexity, making them less suitable for this scenario. While they can be useful in other situations, they are not the optimal choice for real-time aggregation with strict accuracy requirements."},{"content":"Selected Answer: B\nOption B","upvote_count":"1","timestamp":"1708500360.0","poster":"JyoGCP","comment_id":"1155334"},{"content":"hopping window is clear but memorystore vs bigquery?? Why memorystore and not bigquery?","timestamp":"1708168380.0","comment_id":"1152502","poster":"ashdam","comments":[{"upvote_count":"1","comments":[{"upvote_count":"1","poster":"ea2023","timestamp":"1713204540.0","comment_id":"1196162","content":"Let me complete your answer MS vs BQ in this case is a matter of low latence where MS is the winner but if precision were stated about a large amount of data BQ then would\"ve been the best choice."}],"comment_id":"1153259","poster":"ML6","content":"Memory store is an in-memory key-value database for use cases such as real-time application.","timestamp":"1708259100.0"}],"upvote_count":"1"},{"content":"why not D?","comments":[{"timestamp":"1708941540.0","poster":"RenePetersen","upvote_count":"1","comment_id":"1159573","content":"Because BigQuery is not a low latency system..."}],"comment_id":"1136007","timestamp":"1706637420.0","poster":"Jordan18","upvote_count":"1"},{"timestamp":"1704341640.0","poster":"scaenruy","content":"Selected Answer: B\nB. Group the data by using a hopping window in a Dataflow pipeline, and write the aggregated data to Memorystore.","upvote_count":"2","comment_id":"1113331"}],"answer_ET":"B","choices":{"D":"Group the data by using a hopping window in a Dataflow pipeline, and write the aggregated data to BigQuery.","A":"Group the data by using a tumbling window in a Dataflow pipeline, and write the aggregated data to Memorystore.","C":"Group the data by using a session window in a Dataflow pipeline, and write the aggregated data to BigQuery.","B":"Group the data by using a hopping window in a Dataflow pipeline, and write the aggregated data to Memorystore."},"answer":"B","question_images":[],"isMC":true,"exam_id":11,"question_id":198,"topic":"1","answer_description":"","url":"https://www.examtopics.com/discussions/google/view/130262-exam-professional-data-engineer-topic-1-question-277/","timestamp":"2024-01-04 05:14:00","question_text":"You are designing a real-time system for a ride hailing app that identifies areas with high demand for rides to effectively reroute available drivers to meet the demand. The system ingests data from multiple sources to Pub/Sub, processes the data, and stores the results for visualization and analysis in real-time dashboards. The data sources include driver location updates every 5 seconds and app-based booking events from riders. The data processing involves real-time aggregation of supply and demand data for the last 30 seconds, every 2 seconds, and storing the results in a low-latency system for visualization. What should you do?","unix_timestamp":1704341640,"answer_images":[]},{"id":"0e7WmBrV9Dl6opGAXfE1","answers_community":["B (100%)"],"discussion":[{"content":"Selected Answer: B\n- Exception Handling in DoFn: Implementing an exception handling block within DoFn in Dataflow to catch failures during processing is a direct way to manage errors.\n- Side Output to New Topic: Using a side output to redirect failed messages to a new Pub/Sub topic is an effective way to isolate and manage these messages.\n- Monitoring: Monitoring the num_unacked_messages_by_region on the new topic can alert you to the presence of failed messages.","timestamp":"1704837420.0","poster":"raaad","upvote_count":"14","comment_id":"1117872"},{"poster":"chrissamharris","upvote_count":"2","content":"Option C - dead letter topic is built in and requires no changes https://cloud.google.com/pubsub/docs/handling-failures\n\nEnable dead lettering in your Pub/Sub pull subscription, and specify a new Pub/Sub topic as the dead letter topic. Use Cloud Monitoring to monitor the subscription/dead_letter_message_count metric on your pull subscription.","comment_id":"1289100","comments":[{"content":"Dead lettering is used to handle messages that have not been acknowledged, but that's unrelated to the processing that Dataflow does, which takes place later in the chain. A message could still be acknowledged and fail processing for whatever reason, so it would not be sent to the dead letter topic.\n\nAlso, Google advises against using dead lettering with Dataflow anyway: https://cloud.google.com/dataflow/docs/concepts/streaming-with-cloud-pubsub#dead-letter-topics\n\nCorrect answer is B. The error handling has to be written into the Dataflow pipeline itself.","comment_id":"1317401","upvote_count":"1","timestamp":"1732527180.0","poster":"Positron75"}],"timestamp":"1727283360.0"},{"timestamp":"1726304340.0","content":"Selected Answer: B\nSee here:\nhttps://cloud.google.com/dataflow/docs/concepts/streaming-with-cloud-pubsub#unsupported-features\nIt's not recommended to use Pub/Sub dead-letter topics with Dataflow (...) Instead, implement the dead-letter pattern explicitly in the pipeline","poster":"7787de3","comment_id":"1283558","upvote_count":"1"},{"upvote_count":"1","content":"Option B.\n\n Here's why:\n\n Side Output for Failed Messages: Dataflow allows you to use side outputs to handle messages that fail processing. In your DoFn , you can catch exceptions and write the failed messages to a separate PCollection . This PCollection can then be written to a new Pub/Sub topic.\n New Pub/Sub Topic for Monitoring: Creating a dedicated Pub/Sub topic for failed messages allows you to monitor it specifically for alerting purposes. This provides a clear view of any issues with your business logic.\n topic/num_unacked_messages_by_region Metric: This Cloud Monitoring metric tracks the number of unacknowledged messages in a Pub/Sub topic. By monitoring this metric on your new topic, you can identify when messages are failing to be processed correctly.","timestamp":"1721793780.0","comment_id":"1254092","poster":"Jeyaraj"},{"poster":"joao_01","timestamp":"1712644560.0","comments":[{"timestamp":"1712645100.0","upvote_count":"2","comment_id":"1192062","content":"I think that C is not right anyways: In order to use dead_letter feature, the message CANNOT be acknowledge (somehow) by the subscriber. In this question it says that the messages are first acknowledge and then applied the business logic. So, if there are error in the business logic we cannot use the feature dead_letter, beacuse the message was already acknowledge. Thus, option B is the right one.","poster":"joao_01"}],"content":"I would like to know why isn't anyone considering the option C.","upvote_count":"1","comment_id":"1192057"},{"poster":"hanoverquay","comment_id":"1174490","upvote_count":"1","timestamp":"1710535680.0","content":"Selected Answer: B\noption B"},{"upvote_count":"1","timestamp":"1708500600.0","comment_id":"1155336","content":"Selected Answer: B\nOption B","poster":"JyoGCP"},{"poster":"Matt_108","content":"Selected Answer: B\nOption B - Raaad explanation is complete","upvote_count":"1","timestamp":"1705163280.0","comment_id":"1121847"},{"timestamp":"1704342240.0","upvote_count":"1","comment_id":"1113333","content":"Selected Answer: B\nB. Use an exception handling block in your Dataflow’s DoFn code to push the messages that failed to be transformed through a side output and to a new Pub/Sub topic. Use Cloud Monitoring to monitor the topic/num_unacked_messages_by_region metric on this new topic.","poster":"scaenruy"}],"answer_ET":"B","choices":{"B":"Use an exception handling block in your Dataflow’s DoFn code to push the messages that failed to be transformed through a side output and to a new Pub/Sub topic. Use Cloud Monitoring to monitor the topic/num_unacked_messages_by_region metric on this new topic.","A":"Enable retaining of acknowledged messages in your Pub/Sub pull subscription. Use Cloud Monitoring to monitor the subscription/num_retained_acked_messages metric on this subscription.","D":"Create a snapshot of your Pub/Sub pull subscription. Use Cloud Monitoring to monitor the snapshot/num_messages metric on this snapshot.","C":"Enable dead lettering in your Pub/Sub pull subscription, and specify a new Pub/Sub topic as the dead letter topic. Use Cloud Monitoring to monitor the subscription/dead_letter_message_count metric on your pull subscription."},"answer":"B","question_images":[],"isMC":true,"exam_id":11,"question_id":199,"answer_description":"","topic":"1","url":"https://www.examtopics.com/discussions/google/view/130263-exam-professional-data-engineer-topic-1-question-278/","question_text":"Your car factory is pushing machine measurements as messages into a Pub/Sub topic in your Google Cloud project. A Dataflow streaming job, that you wrote with the Apache Beam SDK, reads these messages, sends acknowledgment to Pub/Sub, applies some custom business logic in a DoFn instance, and writes the result to BigQuery. You want to ensure that if your business logic fails on a message, the message will be sent to a Pub/Sub topic that you want to monitor for alerting purposes. What should you do?","timestamp":"2024-01-04 05:24:00","answer_images":[],"unix_timestamp":1704342240},{"id":"YR7QXd8D7HpKnHTfclYX","question_images":[],"question_id":200,"discussion":[{"timestamp":"1704837900.0","poster":"raaad","upvote_count":"10","content":"Selected Answer: C\n- Data Viewer on Shared Dataset: Grants read-only access to the shared dataset.\n- Data Editor on Individual Datasets: Giving each analyst Data Editor role on their respective dataset creates private workspaces where they can create and store personal tables without exposing them to other analysts.","comment_id":"1117877"},{"timestamp":"1723277700.0","comment_id":"1263380","upvote_count":"1","poster":"meh_33","content":"Selected Answer: C\nWill GO with C"},{"upvote_count":"1","comment_id":"1174488","poster":"hanoverquay","timestamp":"1710535500.0","content":"Selected Answer: C\nvoted C"},{"poster":"JyoGCP","timestamp":"1708500960.0","upvote_count":"1","content":"Selected Answer: C\nOption C","comment_id":"1155340"},{"timestamp":"1705163640.0","comment_id":"1121854","content":"Selected Answer: C\nOption C","upvote_count":"1","poster":"Matt_108"},{"timestamp":"1704815040.0","comment_id":"1117613","content":"Selected Answer: C\noption C, because analysts can not see the individual datasets of other analysts","poster":"Sofiia98","upvote_count":"2"},{"content":"Selected Answer: C\nC. Give analysts the BigQuery Data Viewer role on the shared dataset. Create a dataset for each analyst, and give each analyst the BigQuery Data Editor role at the dataset level for their assigned dataset.","poster":"scaenruy","upvote_count":"2","timestamp":"1704342360.0","comment_id":"1113334"}],"question_text":"You want to store your team’s shared tables in a single dataset to make data easily accessible to various analysts. You want to make this data readable but unmodifiable by analysts. At the same time, you want to provide the analysts with individual workspaces in the same project, where they can create and store tables for their own use, without the tables being accessible by other analysts. What should you do?","isMC":true,"url":"https://www.examtopics.com/discussions/google/view/130264-exam-professional-data-engineer-topic-1-question-279/","choices":{"A":"Give analysts the BigQuery Data Viewer role at the project level. Create one other dataset, and give the analysts the BigQuery Data Editor role on that dataset.","C":"Give analysts the BigQuery Data Viewer role on the shared dataset. Create a dataset for each analyst, and give each analyst the BigQuery Data Editor role at the dataset level for their assigned dataset.","D":"Give analysts the BigQuery Data Viewer role on the shared dataset. Create one other dataset and give the analysts the BigQuery Data Editor role on that dataset.","B":"Give analysts the BigQuery Data Viewer role at the project level. Create a dataset for each analyst, and give each analyst the BigQuery Data Editor role at the project level."},"exam_id":11,"answer_description":"","answer_ET":"C","unix_timestamp":1704342360,"answer_images":[],"answers_community":["C (100%)"],"timestamp":"2024-01-04 05:26:00","topic":"1","answer":"C"}],"exam":{"isBeta":false,"lastUpdated":"11 Apr 2025","provider":"Google","isMCOnly":true,"numberOfQuestions":319,"id":11,"name":"Professional Data Engineer","isImplemented":true},"currentPage":40},"__N_SSP":true}