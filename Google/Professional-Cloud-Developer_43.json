{"pageProps":{"questions":[{"id":"A7I98vBi96C0EcePUFwt","isMC":true,"answers_community":["D (100%)"],"timestamp":"2024-10-12 04:59:00","url":"https://www.examtopics.com/discussions/google/view/149114-exam-professional-cloud-developer-topic-1-question-289/","discussion":[{"poster":"saratk1984","content":"Selected Answer: D\n1. Sign Attestation with Cloud KMS (Google Best Practice)\n • Google Cloud recommends using Cloud KMS to securely manage signing keys.\n • Using a Kubernetes secret is less secure and not recommended.\n2. Apply Policy in Production GCP Project (Enforce at Deployment Target)\n • The Binary Authorization policy needs to be enforced in the production environment, where the deployment happens.\n • UAT attestation should be verified during production deployment.\n • Configure a GKE cluster-specific rule in the production project to require attestation.","upvote_count":"1","timestamp":"1742231580.0","comment_id":"1399722"},{"content":"Selected Answer: D\nAfter the UAT phase, sign the attestation with a key stored in Cloud Key Management Service (KMS). Add a GKE cluster-specific rule in Binary Authorization for the production Google Cloud project policy","timestamp":"1728701940.0","comment_id":"1296316","poster":"anshad666","upvote_count":"1"}],"question_id":211,"question_text":"You manage an application deployed on GKE clusters across multiple environments. You are using Cloud Build to run user acceptance testing (UAT) tests. You have integrated Cloud Build with Artifact Analysis, and enabled the Binary Authorization API in all Google Cloud projects hosting your environments. You want only container images that have passed certain automated UAT tests to be deployed to the production environment. You have already created an attestor. What should you do next?","exam_id":7,"answer":"D","answer_description":"","question_images":[],"answer_images":[],"answer_ET":"D","choices":{"C":"After the UAT phase, sign the attestation with a key stored in Cloud Key Management Service (KMS). Add a default rule in Binary Authorization for the UAT Google Cloud project.","D":"After the UAT phase, sign the attestation with a key stored in Cloud Key Management Service (KMS). Add a GKE cluster-specific rule in Binary Authorization for the production Google Cloud project policy.","A":"After the UAT phase, sign the attestation with a key stored as a Kubernetes secret. Add a GKE cluster-specific rule in Binary Authorization for the UAT Google Cloud project.","B":"After the UAT phase, sign the attestation with a key stored as a Kubernetes secret. Add a GKE cluster-specific rule in Binary Authorization for the production Google Cloud project policy."},"topic":"1","unix_timestamp":1728701940},{"id":"dPvYIK6FDwxRHbYlQpwP","answer":"B","choices":{"D":"A comma-delimited string","A":"A linked list","B":"A hash table","C":"A two-dimensional array"},"question_text":"You are parsing a log file that contains three columns: a timestamp, an account number (a string), and a transaction amount (a number). You want to calculate the sum of all transaction amounts for each unique account number efficiently.\nWhich data structure should you use?","question_id":212,"answer_images":[],"answers_community":["B (89%)","11%"],"url":"https://www.examtopics.com/discussions/google/view/55800-exam-professional-cloud-developer-topic-1-question-29/","answer_ET":"B","exam_id":7,"discussion":[{"poster":"__rajan__","content":"Selected Answer: B\nHash Table will store unique values.","comment_id":"1011068","upvote_count":"1","timestamp":"1726722060.0"},{"comment_id":"770103","timestamp":"1704783960.0","poster":"telp","upvote_count":"1","content":"Selected Answer: B\nB. A hash table for the efficient to find a spzcific number."},{"content":"B. A hash table. A hash table allows for fast insertion and lookup of data, which would be useful in this case for quickly looking up the transaction amount for a given account number and adding it to the total. A linked list, two-dimensional array, and comma-delimited string would not be as efficient for this purpose.","upvote_count":"2","comment_id":"768301","poster":"omermahgoub","timestamp":"1704612660.0"},{"upvote_count":"1","timestamp":"1697288280.0","content":"Selected Answer: C\nA and C are obviously wrong.\n\nWith hash tables, you cannot store multiple values (amounts) in single key (account number), but this is exactly what you need to do.\n\nTwo dimensional array can be used to store all the couples Account-amount (timestamp is useless).\nSo my selected answer is C.","poster":"Andrea_P","comment_id":"694749","comments":[{"timestamp":"1731597060.0","comments":[{"content":"The details don't really matter here - the account number has to be unique, so hash table is the only way from the provided options.","upvote_count":"1","poster":"wanrltw","comment_id":"1070543","timestamp":"1731597300.0"}],"content":"The value is not limited to Sting type, though.\nWhat about storing unique account as a key, while keeping timestamp and transaction amount in an array as its value? ;)","comment_id":"1070538","upvote_count":"1","poster":"wanrltw"}]},{"comment_id":"649190","timestamp":"1692509820.0","upvote_count":"3","poster":"tomato123","content":"Selected Answer: B\nB is correct"},{"upvote_count":"3","timestamp":"1681040280.0","poster":"morenocasado","content":"Selected Answer: B\nHash table with the account number as the key, the timestamp is useless for this question, so we can safely discard it.","comment_id":"583286"},{"content":"in this case, if you use hashtable, the key must be account name+ timestamp, so I think linkedlist is better","comments":[{"upvote_count":"1","timestamp":"1685083080.0","poster":"ruben82","comment_id":"607497","content":"You don't need to use timestamp for this request."}],"poster":"yuchun","upvote_count":"1","timestamp":"1656111240.0","comment_id":"389963"},{"timestamp":"1655860860.0","comment_id":"387527","poster":"ralf_cc","content":"Hash Table seems right - https://open4tech.com/array-vs-linked-list-vs-hash-table/","upvote_count":"2","comments":[{"comment_id":"390920","content":"I agree with you on this one","poster":"syu31svc","upvote_count":"2","timestamp":"1656217920.0"}]}],"answer_description":"","isMC":true,"question_images":[],"timestamp":"2021-06-22 03:21:00","unix_timestamp":1624324860,"topic":"1"},{"id":"34kF9Zms0VWTgVyAuzYl","exam_id":7,"question_text":"You work for a company that operates an ecommerce website. You are developing a new integration that will manage all order fulfillment steps after orders are placed. You have created multiple Cloud Functions to process each order. You need to orchestrate the execution of the functions, using the output of each function to determine the flow. You want to minimize the latency of this process. What should you do?","topic":"1","answer_ET":"B","timestamp":"2024-10-12 05:02:00","answers_community":["B (100%)"],"isMC":true,"answer":"B","unix_timestamp":1728702120,"question_images":[],"choices":{"C":"Use Cloud Composer to call the functions, and use an Apache Airflow HTTP operator to handle the execution logic.","A":"Use Workflows to call the functions, and use callbacks to handle the execution logic.","B":"Use Workflows to call the functions, and use conditional jumps to handle the execution logic.","D":"Use Cloud Composer to call the functions, and use an Apache Airflow operator to handle the execution logic."},"answer_images":[],"discussion":[{"poster":"saratk1984","comment_id":"1399723","content":"Selected Answer: B\nWhy Workflows + Conditional Jumps (Option B):\n • Workflows is Google Cloud’s native orchestration service, ideal for chaining Cloud Functions and APIs.\n • It supports low-latency orchestration with native conditional logic, which allows dynamic flow control based on function outputs.\n • Conditional jumps in Workflows allow execution flow to branch or jump based on runtime evaluations — perfect for order fulfillment logic.","timestamp":"1742231700.0","upvote_count":"1"},{"content":"Selected Answer: B\nWorkflows is a service in Google Cloud that allows you to orchestrate serverless functions and services in a sequence or in parallel. It is specifically designed for orchestrating tasks like this in a simple and efficient manner.\nUsing conditional jumps within Workflows allows you to dynamically control the flow based on the outputs of the functions. This means you can easily determine which function to call next based on the results of the previous steps.","timestamp":"1728702120.0","comment_id":"1296317","poster":"anshad666","upvote_count":"2"}],"url":"https://www.examtopics.com/discussions/google/view/149115-exam-professional-cloud-developer-topic-1-question-290/","answer_description":"","question_id":213},{"id":"Rv0DoRMKZFalrGtc571w","question_text":"You are currently pushing container images to Artifact Registry and deploying a containerized microservices application to GKE. After deploying the application, you notice that the services do not behave as expected. You use the kubectl get pods command to inspect the state of the application Pods, and discover that one of the Pods has a state of CrashLoopBackoff. How should you troubleshoot the Pod?","discussion":[{"content":"Selected Answer: C\nhttps://cloud.google.com/kubernetes-engine/docs/troubleshooting/deployed-workloads?hl=ja#CrashLoopBackOff","poster":"yokoyan","timestamp":"1744262220.0","upvote_count":"1","comment_id":"1559472"},{"timestamp":"1742231880.0","poster":"saratk1984","comment_id":"1399726","upvote_count":"1","content":"Selected Answer: C\nWhy Option C is Correct:\n • kubectl logs POD_NAME is the standard command to retrieve the stdout/stderr logs from a Pod.\n • You can add --previous to see logs from the previous container instance that crashed, which is especially helpful in CrashLoopBackOff situations.\n • These logs are often the fastest way to identify root cause errors such as missing dependencies, configuration issues, or environment variables."},{"content":"Selected Answer: C\ncontainer in crashloopbackoff , check logs first","poster":"anshad666","timestamp":"1728702300.0","upvote_count":"2","comment_id":"1296319"}],"url":"https://www.examtopics.com/discussions/google/view/149116-exam-professional-cloud-developer-topic-1-question-291/","answer_description":"","question_images":[],"answer_images":[],"isMC":true,"topic":"1","exam_id":7,"question_id":214,"answers_community":["C (100%)"],"unix_timestamp":1728702300,"answer_ET":"C","timestamp":"2024-10-12 05:05:00","choices":{"D":"In the Google Cloud console, navigate to Cloud Logging in the project of the cluster’s VPC. Enter a filter to show denied egress traffic to the Private Google Access CIDR range. Validate if egress traffic is denied from your GKE cluster to the Private Google Access CIDR range.","A":"Connect to the problematic Pod by running the kubectl exec -it POD_NAME - /bin/bash command where the POD_NAME parameter is the name of the problematic Pod. Inspect the logs in the /var/log/messages folder to determine the root cause.","B":"Execute the gcloud projects get-iam-policy PROJECT_ID command where the PROJECT_ID parameter is the name of the project where your Artifact Registry resides. Inspect the IAM bindings of the node pool s service account. Validate if the service account has the roles/artifactregistry.reader role.","C":"Run the kubectl logs POD_NAME command where the POD_NAME parameter is the name of the problematic Pod. Analyze the logs of the Pod from previous runs to determine the root cause of failed start attempts of the Pod."},"answer":"C"},{"id":"rlpQPmCYO2aVyzRqRqn8","exam_id":7,"question_text":"You use Cloud Build to build and test container images prior to deploying them to Cloud Run. Your images are stored in Artifact Registry. You need to ensure that only container images that have passed testing are deployed. You want to minimize operational overhead. What should you do?","topic":"1","answer_ET":"B","timestamp":"2024-10-12 05:06:00","answers_community":["B (100%)"],"answer":"B","isMC":true,"unix_timestamp":1728702360,"choices":{"A":"Deploy a new revision to a Cloud Run service. Assign a tag that allows access to the revision at a specific URL without serving traffic. Test that revision again. Migrate the traffic to the Cloud Run service after you confirm that the new revision is performing as expected.","D":"Configure build provenance on your Cloud Build pipeline. Verify that all the tests have passed, and then deploy the image to a Cloud Run service.","C":"Create a GKE cluster. Verify that all tests have passed, and then deploy the image to the GKE cluster.","B":"Enable Binary Authorization on your Cloud Run service. Create an attestation if the container image has passed all tests. Configure Binary Authorization to allow only images with appropriate attestation to be deployed to the Cloud Run service."},"question_images":[],"answer_images":[],"discussion":[{"timestamp":"1728702360.0","poster":"anshad666","content":"Selected Answer: B\nEnable Binary Authorization on your Cloud Run service. Create an attestation if the container image has passed all tests. Configure Binary Authorization to allow only images with appropriate attestation to be deployed to the Cloud Run service.","upvote_count":"2","comment_id":"1296320"}],"url":"https://www.examtopics.com/discussions/google/view/149117-exam-professional-cloud-developer-topic-1-question-292/","answer_description":"","question_id":215}],"exam":{"provider":"Google","isMCOnly":false,"isImplemented":true,"name":"Professional Cloud Developer","id":7,"lastUpdated":"11 Apr 2025","numberOfQuestions":338,"isBeta":false},"currentPage":43},"__N_SSP":true}