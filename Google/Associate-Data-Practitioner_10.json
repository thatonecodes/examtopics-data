{"pageProps":{"questions":[{"id":"8IdYBqgSvGqGzdlj8Aha","exam_id":2,"answer_ET":"A","discussion":[{"comment_id":"1366605","content":"Selected Answer: D\nD. Keeps original data easily queried, while also transforming into BQ.","poster":"n2183712847","timestamp":"1741444200.0","upvote_count":"1"},{"timestamp":"1740666360.0","content":"Selected Answer: D\nOption D is the clear winner. BigQuery subscriptions + materialized views are purpose-built for this. Dataflow (A) works, but is more overhead. Cloud Run (B) isn't suitable for continuous processing, and Cloud Storage (C) adds unnecessary steps. Option D is the most efficient and keeps the raw data accessible.","poster":"n2183712847","upvote_count":"2","comment_id":"1362553"}],"timestamp":"2025-02-27 15:26:00","answer_images":[],"unix_timestamp":1740666360,"answers_community":["D (100%)"],"url":"https://www.examtopics.com/discussions/google/view/157197-exam-associate-data-practitioner-topic-1-question-50/","answer_description":"","isMC":true,"answer":"D","topic":"1","question_text":"Your organization’s ecommerce website collects user activity logs using a Pub/Sub topic. Your organization’s leadership team wants a dashboard that contains aggregated user engagement metrics. You need to create a solution that transforms the user activity logs into aggregated metrics, while ensuring that the raw data can be easily queried. What should you do?","choices":{"D":"Create a BigQuery subscription to the Pub/Sub topic, and load the activity logs into the table. Create a materialized view in BigQuery using SQL to transform the data for reporting","B":"Create an event-driven Cloud Run function to trigger a data transformation pipeline to run. Load the transformed activity logs into a BigQuery table for reporting.","A":"Create a Dataflow subscription to the Pub/Sub topic, and transform the activity logs. Load the transformed data into a BigQuery table for reporting.","C":"Create a Cloud Storage subscription to the Pub/Sub topic. Load the activity logs into a bucket using the Avro file format. Use Dataflow to transform the data, and load it into a BigQuery table for reporting."},"question_images":[],"question_id":46},{"id":"7U9bnKoTpVp2K1j0W3e5","choices":{"A":"Set up a Cloud CDN in front of the bucket.","D":"Store the data in Nearline storage.","C":"Store the data in a multi-region bucket.","B":"Enable Object Versioning on the bucket."},"topic":"1","answer_ET":"C","discussion":[{"poster":"n2183712847","comment_id":"1365678","upvote_count":"1","content":"Selected Answer: C\nmulti region to compensate for single zone outage disaster","timestamp":"1741229640.0"},{"content":"Selected Answer: C\nChoosing C only because all the other answers are stupid. A) Setting up a CDN does not ensure High Availability (single zone outage). B) Versioning does not improve file access or HA. D) Storage class is irrelevant to availability. C) is the only answer that improved availability. However, the ask was for High Availability and not Disaster Recovery which would require multi-region for an answer. So, C goes way beyond the ask and increases costs. If there was a use default / standard storage as your answer, it would be the better answer.","poster":"rich_maverick","upvote_count":"1","comment_id":"1362777","timestamp":"1740700620.0"}],"url":"https://www.examtopics.com/discussions/google/view/157256-exam-associate-data-practitioner-topic-1-question-51/","answers_community":["C (100%)"],"answer_description":"","unix_timestamp":1740700620,"question_id":47,"answer_images":[],"question_images":[],"timestamp":"2025-02-28 00:57:00","answer":"C","question_text":"You are constructing a data pipeline to process sensitive customer data stored in a Cloud Storage bucket. You need to ensure that this data remains accessible, even in the event of a single-zone outage. What should you do?","exam_id":2,"isMC":true},{"id":"IGb4UQlI4G8gYz6R7uDC","question_text":"Your retail company collects customer data from various sources:\nOnline transactions: Stored in a MySQL database\nCustomer feedback: Stored as text files on a company server\nSocial media activity: Streamed in real-time from social media platforms\nYou are designing a data pipeline to extract this data. Which Google Cloud storage system(s) should you select for further analysis and ML model training?","exam_id":2,"answer_ET":"B","discussion":[{"comment_id":"1366606","content":"Selected Answer: B\nstoring online transactions would be good to put into BigQuery for analytics.","poster":"n2183712847","timestamp":"1741444320.0","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: B\nonline = cloud sql\ntext file = cloud storage\nstream = bigquery / bigtable (if data pipeline, use dataflow to stream)","poster":"n2183712847","comment_id":"1365679","timestamp":"1741229820.0"}],"answers_community":["B (100%)"],"topic":"1","unix_timestamp":1741229820,"question_id":48,"answer_description":"","answer":"B","timestamp":"2025-03-06 03:57:00","isMC":true,"question_images":[],"choices":{"C":"1. Online transactions: Bigtable\n2. Customer feedback: Cloud Storage\n3. Social media activity: CloudSQL for MySQL","B":"1. Online transactions: BigQuery\n2. Customer feedback: Cloud Storage\n3. Social media activity: BigQuery","D":"1. Online transactions: Cloud SQL for MySQL\n2. Customer feedback: BigQuery\n3. Social media activity: Cloud Storage","A":"1. Online transactions: Cloud Storage\n2. Customer feedback: Cloud Storage\n3. Social media activity: Cloud Storage"},"url":"https://www.examtopics.com/discussions/google/view/157625-exam-associate-data-practitioner-topic-1-question-52/","answer_images":[]},{"id":"NE6k7jXZOJNJ2Jm7aPhr","url":"https://www.examtopics.com/discussions/google/view/157643-exam-associate-data-practitioner-topic-1-question-53/","answer_images":[],"timestamp":"2025-03-06 05:49:00","answer":"B","exam_id":2,"discussion":[{"comment_id":"1365742","upvote_count":"1","timestamp":"1741236540.0","content":"Selected Answer: B\nThe best option is B. Define a new measure. Option B is best: Measures are for on-the-fly calculations in Looker, quick & efficient. Option A (Derived table) is incorrect: Slower, pre-calculation not needed. Option C (New dimension) is incorrect: Dimensions are for categories, not metrics. Option D (Apply filter) is incorrect: Filter restricts data, doesn't calculate margin. Therefore, Option B, new measure, is the fastest, most efficient LookML solution.","poster":"n2183712847"}],"unix_timestamp":1741236540,"answer_description":"","answer_ET":"B","question_images":[],"choices":{"C":"Create a new dimension that categorizes products based on their profit margin ranges (e.g., high, medium, low).","B":"Define a new measure that calculates the profit margin by using the existing revenue and cost fields.","A":"Create a derived table that pre-calculates the profit margin for each product, and include it in the Looker model.","D":"Apply a filter to only show products with a positive profit margin."},"topic":"1","answers_community":["B (100%)"],"question_text":"Your company uses Looker as its primary business intelligence platform. You want to use LookML to visualize the profit margin for each of your company’s products in your Looker Explores and dashboards. You need to implement a solution quickly and efficiently. What should you do?","question_id":49,"isMC":true},{"id":"pnLkZn0JSLShh2FOByrQ","answers_community":["A (100%)"],"answer_images":[],"timestamp":"2025-03-06 04:09:00","discussion":[{"timestamp":"1741230540.0","upvote_count":"1","comment_id":"1365682","poster":"n2183712847","content":"Selected Answer: A\nThe best option is A. Enable access control by using IAM roles. Option A is optimal because IAM roles are the standard Google Cloud method for managing access, directly enabling least privilege for BigQuery. Option B (CMEK) is incorrect because encryption secures data but doesn't control access. Option C (SQL GRANT) is less preferred than IAM for broader GCP access management. Option D (Export and signed URLs) is incorrect as it's complex, less secure, and not for controlling BigQuery query access. Therefore, Option A is the most direct and secure way to manage access to sensitive BigQuery data."}],"question_text":"You are a data analyst working with sensitive customer data in BigQuery. You need to ensure that only authorized personnel within your organization can query this data, while following the principle of least privilege. What should you do?","answer":"A","exam_id":2,"choices":{"C":"Update dataset privileges by using the SQL GRANT statement.","B":"Encrypt the data by using customer-managed encryption keys (CMEK).","A":"Enable access control by using IAM roles.","D":"Export the data to Cloud Storage, and use signed URLs to authorize access."},"unix_timestamp":1741230540,"question_id":50,"answer_ET":"A","url":"https://www.examtopics.com/discussions/google/view/157626-exam-associate-data-practitioner-topic-1-question-54/","answer_description":"","topic":"1","question_images":[],"isMC":true}],"exam":{"numberOfQuestions":72,"isImplemented":true,"lastUpdated":"11 Apr 2025","provider":"Google","isMCOnly":true,"name":"Associate Data Practitioner","id":2,"isBeta":false},"currentPage":10},"__N_SSP":true}