{"pageProps":{"questions":[{"id":"ZbNSxCABWIVl3OLcKE1x","answer":"B","choices":{"A":"Create two jobs: one that checks whether the container can connect to the database, and another that runs the shutdown script if the Pod is failing.","D":"Create the Deployment with an initContainer that checks the service availability. Configure a Prestop lifecycle handler that runs the shutdown script if the Pod is failing.","C":"Create the Deployment with a PostStart lifecycle handler that checks the service availability. Configure a PreStop lifecycle handler that runs the shutdown script if the container is failing.","B":"Create the Deployment with a livenessProbe for the container that will fail if the container can't connect to the database. Configure a Prestop lifecycle handler that runs the shutdown script if the container is failing."},"answer_ET":"B","answers_community":["B (86%)","14%"],"question_id":126,"question_text":"You need to configure a Deployment on Google Kubernetes Engine (GKE). You want to include a check that verifies that the containers can connect to the database. If the Pod is failing to connect, you want a script on the container to run to complete a graceful shutdown. How should you configure the Deployment?","discussion":[{"timestamp":"1685909520.0","poster":"TNT87","content":"Selected Answer: B\nhttps://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke#make_sure_your_applications_are_shutting_down_in_accordance_with_kubernetes_expectations","comment_id":"735488","upvote_count":"6"},{"poster":"alpha_canary","comment_id":"1194851","timestamp":"1728813720.0","content":"Selected Answer: B\n\"Most programs don't stop accepting requests right away. However, if you're using third-party code or are managing a system that you don't have control over, such as nginx, the preStop hook is a good option for triggering a graceful shutdown without modifying the application. One common strategy is to execute, in the preStop hook, a sleep of a few seconds to postpone the SIGTERM. This gives Kubernetes extra time to finish the Pod deletion process, and reduces connection errors on the client side.\"\n\nhttps://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke#:~:text=If%20your%20application%20doesn%27t%20follow%20the%20preceding%20practice%2C%20use%20the%20preStop%20hook\n\nhttps://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks","upvote_count":"1"},{"content":"Selected Answer: B\nI go with B, that is liveness probe and if failed for max retries then call prestop hook to gracefully shutdown the container. D is also very close, but it used init container to check for the database connectivity first. I am not sure whether we can prestop hook if initContainer fails to starts.","upvote_count":"1","timestamp":"1707335280.0","poster":"purushi","comment_id":"974882"},{"content":"Selected Answer: B\nhttps://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke#make_sure_your_applications_are_shutting_down_in_accordance_with_kubernetes_expectations -> the preStop hook is a good option for triggering a graceful shutdown without modifying the application.\nhttps://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#hook-details ->\nThis hook is called immediately before a container is terminated due to an API request or management event such as a liveness/startup probe failure, preemption, resource contention and others. A call to the PreStop hook fails if the container is already in a terminated or completed state and the hook must complete before the TERM signal to stop the container can be sent. The Pod's termination grace period countdown begins before the PreStop hook is executed, so regardless of the outcome of the handler, the container will eventually terminate within the Pod's termination grace period. No parameters are passed to the handler.","poster":"Pime13","timestamp":"1692532200.0","upvote_count":"1","comment_id":"815313"},{"comment_id":"772295","content":"Selected Answer: B\nAnswer B","upvote_count":"1","timestamp":"1689061020.0","poster":"telp"},{"upvote_count":"2","timestamp":"1687350360.0","poster":"zellck","comment_id":"752418","content":"Selected Answer: B\nB is the answer."},{"upvote_count":"2","content":"Selected Answer: D\nD could be the right answer","comments":[{"content":"The answer is definitely D. Anytime you need to do some work before a container can be considered ready, you use init containers. With a liveness probe we would need to add an endpoint that checks whether we can connect to the database, with init containers we can separate this logic.","timestamp":"1687436880.0","comments":[{"content":"Liveness probe also supports other methods to do work beside http, but I hope you understand my message. Here's more on init containers btw: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/","comments":[{"content":"initContainer requires separate container to run in addition to the application container, but the question asked for script to be run in the same container.","poster":"zellck","timestamp":"1687469520.0","comment_id":"753744","upvote_count":"1"}],"poster":"[Removed]","timestamp":"1687437060.0","comment_id":"753380","upvote_count":"1"}],"upvote_count":"1","comment_id":"753374","poster":"[Removed]"}],"timestamp":"1685903460.0","comment_id":"735435","poster":"kisswd"}],"exam_id":7,"url":"https://www.examtopics.com/discussions/google/view/90011-exam-professional-cloud-developer-topic-1-question-211/","question_images":[],"answer_images":[],"timestamp":"2022-12-04 21:31:00","unix_timestamp":1670185860,"isMC":true,"answer_description":"","topic":"1"},{"id":"VUhGm1jtS7l1NFZlFj9R","question_id":127,"unix_timestamp":1670817060,"exam_id":7,"isMC":true,"discussion":[{"poster":"info_appsatori","comment_id":"1302414","content":"Selected Answer: D\nOption D is correct. You need loadbalancer and dns to name your 3 services (yourcompany.com/path). without it, if you select answer B, you will have cloudFunctionName-1/path, cloudFunctionName-2/path, cloudFunctionName-3/path. Its ruined user experience.","timestamp":"1729768140.0","upvote_count":"1"},{"poster":"phantomsg","comment_id":"1174573","timestamp":"1710547320.0","upvote_count":"1","content":"Selected Answer: B\nThere's no purpose for a Load Balancer here as you are not balancing traffic across multiple backend server instances. You need 3 different Cloud Functions each with their own Endpoint that's all. See this example: https://cloud.google.com/functions/docs/create-deploy-gcloud-1st-gen"},{"comment_id":"1156816","content":"Selected Answer: D\noption D. \nWhen users query \"yourcompany.com,\" they receive an IP address and access the load balancer. Consequently, the load balancer then executes path-based routing.\nB is Wrong. It is not possible to deploy three cloud functions with the same domain name.","timestamp":"1708650000.0","upvote_count":"3","poster":"ka219ra"},{"content":"Selected Answer: B\ni go for B because i don't understand the necessity of LB in this case","comment_id":"1105406","poster":"Kadhem","timestamp":"1703522640.0","upvote_count":"1"},{"timestamp":"1691431500.0","content":"Selected Answer: B\nI go with B. Exposed using an HTTPS load balancer is not required. Those three are different end points of the service. We no need to setup load balancer in case of Cloud functions, it is serverless.","upvote_count":"1","poster":"purushi","comment_id":"974888"},{"poster":"zanhsieh","content":"Selected Answer: D\nD. The differences between B and D are:\n1. Cost: 3 Cloud Function exposed directly (B) will create 3 endpoints / load balancers, whereas D only exposed one load balancer.\n2. Scalability: exposing directly with endpoint or instance itself would cause scalability problem - can't upscale the endpoint instance fast enough.\n3. Handling service-to-service call: In B, all services rely on external DNS resolution, which is slower. In D, it has chance that cross-service call can be resolved internally.","timestamp":"1686450780.0","comment_id":"920370","upvote_count":"2"},{"poster":"Teraflow","comment_id":"859868","upvote_count":"1","content":"Selected Answer: B\nOption B (Create three Cloud Functions exposed directly) is the best choice in this scenario, as it allows you to create a separate Cloud Function for each API URL path and configure each one to invoke a different function in your code.\n\nOption A (Create one Cloud Function as a backend service exposed using an HTTPS load balancer) and Option D (Create three Cloud Functions as three backend services exposed using an HTTPS load balancer) both involve using an HTTPS load balancer, which adds additional complexity and configuration overhead. These options may be appropriate for more complex scenarios, but in this case, they are not necessary.\n\nOption C (Create one Cloud Function exposed directly) would require all three API URL paths to invoke the same function in your code, which does not meet the requirement of invoking different functions for each URL path.","timestamp":"1680522960.0","comments":[{"comment_id":"878053","poster":"closer89","content":"B is wrong, in API context you need to map each external url to cloud function url, to do that you need LB","timestamp":"1682238180.0","upvote_count":"1"}]},{"comment_id":"815319","comments":[{"comment_id":"824540","poster":"Pime13","timestamp":"1677572100.0","content":"https://cloud.google.com/load-balancing/docs/https/setting-up-https-serverless\nhttps://cloud.google.com/load-balancing/docs/negs/serverless-neg-concepts","upvote_count":"1"}],"poster":"Pime13","timestamp":"1676901180.0","upvote_count":"1","content":"Selected Answer: D\ni choose D"},{"comment_id":"797322","upvote_count":"3","comments":[{"timestamp":"1675871040.0","poster":"mrvergara","upvote_count":"1","content":"Using option D, where you create three Cloud Functions as backend services exposed through an HTTPS load balancer, is not necessary in this scenario. An HTTPS load balancer would be useful in scenarios where you need to balance incoming traffic across multiple instances of a backend service to distribute the workload, ensure high availability, and provide failover protection. However, in this case, you only need to map each API URL path to a different function, which can be achieved by creating three separate Cloud Functions, each exposed directly. This would be a simpler and more straightforward solution for this specific use case.","comment_id":"802195"}],"content":"Selected Answer: B\nEach function is defined as an HTTP trigger, which allows it to be triggered by incoming HTTP requests. The endpoint for each function is defined in the function name (e.g. \"students\", \"teachers\", \"classes\").\n\nThis means that the APIs would be accessible at the following endpoints:\n\n• https://yourcompany.com/students\n• https://yourcompany.com/teachers\n• https://yourcompany.com/classes\n\nNote that you would need to configure \"yourcompany.com\" DNS registry.\n\nIn this case, option B, \"Create three Cloud Functions exposed directly\", would be correct.","poster":"mrvergara","timestamp":"1675454280.0"},{"upvote_count":"1","timestamp":"1674841320.0","content":"Why there is the need of LB?","comment_id":"789845","poster":"mrvergara"},{"upvote_count":"1","timestamp":"1670997900.0","poster":"sharath25","content":"Selected Answer: D\noption D","comment_id":"744765"},{"upvote_count":"1","timestamp":"1670848320.0","comments":[{"timestamp":"1670856120.0","content":"https://cloud.google.com/load-balancing/docs/https/setup-global-ext-https-serverless","comment_id":"742925","poster":"zellck","upvote_count":"2"}],"content":"Selected Answer: D\nD is the answer.","poster":"zellck","comment_id":"742776"},{"content":"Selected Answer: D\nD is correct","comment_id":"742380","poster":"melisargh","timestamp":"1670817060.0","upvote_count":"1"}],"url":"https://www.examtopics.com/discussions/google/view/91107-exam-professional-cloud-developer-topic-1-question-212/","question_images":[],"answer_images":[],"answer":"D","choices":{"B":"Create three Cloud Functions exposed directly.","D":"Create three Cloud Functions as three backend services exposed using an HTTPS load balancer.","A":"Create one Cloud Function as a backend service exposed using an HTTPS load balancer.","C":"Create one Cloud Function exposed directly."},"topic":"1","answers_community":["D (59%)","B (41%)"],"question_text":"You are responsible for deploying a new API. That API will have three different URL paths:\n\n• https://yourcompany.com/students\n• https://yourcompany.com/teachers\n• https://yourcompany.com/classes\n\nYou need to configure each API URL path to invoke a different function in your code. What should you do?","timestamp":"2022-12-12 04:51:00","answer_description":"","answer_ET":"D"},{"id":"xnEGhgLNPKejIwKmdJ8z","choices":{"D":"Use the Container Analysis REST API to call Container Analysis to scan new container images. Review the vulnerability results before each deployment.","C":"Enable Container Analysis, and upload new container images to Artifact Registry. Review the critical vulnerability results before each deployment.","A":"Use the gcloud CLI to call Container Analysis to scan new container images. Review the vulnerability results before each deployment.","B":"Enable Container Analysis, and upload new container images to Artifact Registry. Review the vulnerability results before each deployment."},"timestamp":"2022-12-02 20:31:00","question_images":[],"answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/89806-exam-professional-cloud-developer-topic-1-question-213/","topic":"1","discussion":[{"comment_id":"931159","upvote_count":"2","poster":"zanhsieh","timestamp":"1719108180.0","content":"Selected Answer: B\nB. Actually the tricky part for this question is: Is the Container Analysis enabled by default? Can Container Analysis be called on-demand via REST without specifically enabled it? By default GCP does not enable Container Analysis; that's why D is out."},{"timestamp":"1708437300.0","upvote_count":"1","poster":"Pime13","comment_id":"815322","content":"Selected Answer: B\nhttps://cloud.google.com/artifact-registry/docs/analysis\n\nVulnerability scanning can occur automatically or on-demand:\n\nWhen automatic scanning is enabled, scanning triggers automatically every time you push a new image to Artifact Registry or Container Registry. Vulnerability information is continuously updated when new vulnerabilities are discovered.\n\nWhen On-Demand Scanning is enabled, you must run a command to scan a local image or an image in Artifact Registry or Container Registry. On-Demand Scanning gives you more flexibility around when you scan containers. For example, you can scan a locally-built image and remediate vulnerabilities before storing it in a registry.\n\nScanning results are available for up to 48 hours after the scan is completed, and vulnerability information is not updated after the scan."},{"upvote_count":"1","content":"https://cloud.google.com/blog/products/application-development/understanding-artifact-registry-vs-container-registry","comment_id":"764300","timestamp":"1704268320.0","poster":"TNT87"},{"content":"Selected Answer: B\nContainer Analysis is a service that provides vulnerability scanning and metadata storage for containers. The scanning service performs vulnerability scans on images in Container Registry and Artifact Registry, then stores the resulting metadata and makes it available for consumption through an API. Metadata storage allows storing information from different sources, including vulnerability scanning, other Google Cloud services, and third-party providers.\n\nhttps://cloud.google.com/container-analysis/docs/container-analysis","poster":"TNT87","timestamp":"1703678700.0","comment_id":"758445","upvote_count":"1"},{"poster":"sharath25","content":"Selected Answer: B\noption B","comment_id":"744773","upvote_count":"1","timestamp":"1702534260.0"},{"comment_id":"741381","poster":"zellck","content":"Selected Answer: B\nB is the answer.\n\nhttps://cloud.google.com/container-analysis/docs/automated-scanning-howto","upvote_count":"1","timestamp":"1702263060.0"},{"poster":"TNT87","comment_id":"735474","timestamp":"1701725400.0","upvote_count":"1","content":"Answer B\nIf you have done Devops you will understand"},{"content":"Selected Answer: B\nAnswer B","upvote_count":"1","comment_id":"735261","poster":"TNT87","timestamp":"1701708720.0"},{"timestamp":"1701685740.0","comment_id":"734994","comments":[{"upvote_count":"1","poster":"TNT87","comment_id":"758442","content":"Do not confuse yourself, there is Container analysis API, it exists. check what the question requires, ok\nhttps://cloud.google.com/container-analysis/docs/reference/rest","timestamp":"1703678520.0"},{"content":"After reviewing the document again, I changed my answer to D.","comment_id":"734999","comments":[{"timestamp":"1703678580.0","upvote_count":"1","content":"It cant be D,thats not how the Container analysis API works","comment_id":"758444","poster":"TNT87"}],"upvote_count":"2","timestamp":"1701685980.0","poster":"kisswd"}],"upvote_count":"1","poster":"kisswd","content":"Selected Answer: B\n\"Container Analysis REST API\" doesn't exist. \n\nhttps://cloud.google.com/container-analysis/docs/os-overview says: \nThe Container Scanning API allows you to automate OS vulnerability detection, scanning each time you push an image to Container Registry or Artifact Registry. Enabling this API also triggers language package scans for Go and Java vulnerabilities (Preview)."},{"comment_id":"734761","poster":"ladannylondo","timestamp":"1701649080.0","upvote_count":"1","content":"Selected Answer: B\nhttps://cloud.google.com/container-analysis/docs/enable-container-scanning"},{"poster":"melisargh","timestamp":"1701545460.0","upvote_count":"1","content":"Selected Answer: D\nhttps://cloud.google.com/container-analysis/docs/os-overview","comment_id":"734025","comments":[{"poster":"gardislan18","content":"Answer is B\nhttps://cloud.google.com/container-analysis/docs/automated-scanning-howto","upvote_count":"2","timestamp":"1701699480.0","comments":[{"comment_id":"742382","poster":"melisargh","timestamp":"1702353300.0","upvote_count":"1","content":"after re review i think B is correct too but im still not sure"}],"comment_id":"735134"}]}],"answers_community":["B (90%)","10%"],"answer_description":"","question_id":128,"answer_ET":"B","answer":"B","exam_id":7,"question_text":"You are deploying a microservices application to Google Kubernetes Engine (GKE). The application will receive daily updates. You expect to deploy a large number of distinct containers that will run on the Linux operating system (OS). You want to be alerted to any known OS vulnerabilities in the new containers. You want to follow Google-recommended best practices. What should you do?","isMC":true,"unix_timestamp":1670009460},{"id":"FfDcKZaSdzBsfyxatKIM","answers_community":["D (78%)","A (22%)"],"answer_images":[],"answer_description":"","answer":"D","question_id":129,"unix_timestamp":1670818800,"choices":{"B":"Create a Google service account with BigQuery access. Add the Google service account JSON key as a Kubernetes secret, and configure the application to use this secret.","A":"Create a Google service account with BigQuery access. Add the JSON key to Secret Manager, and use the Go client library to access the JSON key.","C":"Create a Google service account with BigQuery access. Add the Google service account JSON key to Secret Manager, and use an init container to access the secret for the application to use.","D":"Create a Google service account and a Kubernetes service account. Configure Workload Identity on the GKE cluster, and reference the Kubernetes service account on the application Deployment."},"discussion":[{"timestamp":"1731280500.0","content":"Option D doesn't say Service Account has BigQuery access. How is it correct?","comment_id":"1309726","upvote_count":"1","poster":"SpecialEdition"},{"comment_id":"1194861","content":"Selected Answer: D\n\"Applications running on GKE might need access to Google Cloud APIs such as Compute Engine API, BigQuery Storage API, or Machine Learning APIs.\"\n\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/workload-identity#what_is","poster":"alpha_canary","timestamp":"1713003120.0","upvote_count":"1"},{"content":"Selected Answer: D\nWorkload Identity allows a Kubernetes service account in your GKE cluster to act as an IAM service account. Pods that use the configured Kubernetes service account automatically authenticate as the IAM service account when accessing Google Cloud APIs. Using Workload Identity allows you to assign distinct, fine-grained identities and authorization for each application in your cluster.\n\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/workload-identity#what_is","poster":"Pime13","upvote_count":"1","comment_id":"815327","timestamp":"1676901480.0"},{"content":"Selected Answer: D\nThe answer is D because the best pratice is to use workload identity\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/workload-identity#what_is","comment_id":"772301","poster":"telp","timestamp":"1673430180.0","upvote_count":"1"},{"timestamp":"1672818240.0","upvote_count":"1","poster":"TNT87","comment_id":"765350","content":"https://cloud.google.com/kubernetes-engine/docs/quickstarts/deploy-app-container-image#deploying_to_gke"},{"content":"Selected Answer: D\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/workload-identity#what_is\nAnswer D","timestamp":"1672142280.0","upvote_count":"1","comment_id":"758437","poster":"TNT87"},{"poster":"sharath25","content":"Selected Answer: D\noption D","upvote_count":"1","comment_id":"744774","timestamp":"1670998380.0"},{"timestamp":"1670982120.0","comment_id":"744614","content":"Selected Answer: D\na go???? no!! D is correct","upvote_count":"1","poster":"jcataluna"},{"content":"Selected Answer: D\nD is the answer.\n\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/workload-identity#what_is\nApplications running on GKE might need access to Google Cloud APIs such as Compute Engine API, BigQuery Storage API, or Machine Learning APIs.\n\nWorkload Identity allows a Kubernetes service account in your GKE cluster to act as an IAM service account. Pods that use the configured Kubernetes service account automatically authenticate as the IAM service account when accessing Google Cloud APIs. Using Workload Identity allows you to assign distinct, fine-grained identities and authorization for each application in your cluster.","timestamp":"1670850900.0","comment_id":"742816","upvote_count":"1","poster":"zellck"},{"content":"Selected Answer: A\nvote A because the type of auth supported by bq and the recommended way of auth which is use go libraries \n\nhttps://cloud.google.com/bigquery/docs/authorization\nhttps://pkg.go.dev/golang.org/x/oauth2/google?utm_source=cloud.google.com&utm_medium=referral#JWTAccessTokenSourceFromJSON","comment_id":"742391","poster":"melisargh","timestamp":"1670818800.0","upvote_count":"2"}],"timestamp":"2022-12-12 05:20:00","answer_ET":"D","question_text":"You are a developer at a large organization. You have an application written in Go running in a production Google Kubernetes Engine (GKE) cluster. You need to add a new feature that requires access to BigQuery. You want to grant BigQuery access to your GKE cluster following Google-recommended best practices. What should you do?","topic":"1","exam_id":7,"isMC":true,"url":"https://www.examtopics.com/discussions/google/view/91108-exam-professional-cloud-developer-topic-1-question-214/","question_images":[]},{"id":"rn1JxgjaACKcya7xz1gG","url":"https://www.examtopics.com/discussions/google/view/89954-exam-professional-cloud-developer-topic-1-question-215/","answers_community":["A (100%)"],"answer":"A","question_text":"You have an application written in Python running in production on Cloud Run. Your application needs to read/write data stored in a Cloud Storage bucket in the same project. You want to grant access to your application following the principle of least privilege. What should you do?","choices":{"A":"Create a user-managed service account with a custom Identity and Access Management (IAM) role.","B":"Create a user-managed service account with the Storage Admin Identity and Access Management (IAM) role.","C":"Create a user-managed service account with the Project Editor Identity and Access Management (IAM) role.","D":"Use the default service account linked to the Cloud Run revision in production."},"answer_description":"","discussion":[{"upvote_count":"1","timestamp":"1728822360.0","comment_id":"1194951","content":"Selected Answer: A\nA. Create a user-managed service account with a custom Identity and Access Management (IAM) role.","poster":"alpha_canary"},{"content":"Selected Answer: A\nA is correct.","poster":"__rajan__","comment_id":"1015665","upvote_count":"1","timestamp":"1711282440.0"},{"poster":"purushi","content":"Selected Answer: A\nprinciple of least privilege -> custom Identity and Access Management (IAM) role","timestamp":"1707337680.0","comment_id":"974900","upvote_count":"1"},{"upvote_count":"2","poster":"telp","content":"Selected Answer: A\nAnswer is A\nThe others give too many acess","timestamp":"1689061500.0","comment_id":"772303"},{"timestamp":"1686444120.0","content":"Selected Answer: A\nA is the answer.","poster":"zellck","upvote_count":"2","comment_id":"741379"},{"upvote_count":"1","comment_id":"735472","timestamp":"1685906940.0","poster":"TNT87","content":"Answer A"},{"comment_id":"735033","content":"Selected Answer: A\nA - assign the needed permissions, following the least privilege rule\n\nNot B - https://cloud.google.com/iam/docs/understanding-roles#storage.admin\nC and D gives too many access","poster":"gardislan18","upvote_count":"1","timestamp":"1685870520.0"}],"timestamp":"2022-12-04 12:22:00","question_id":130,"answer_images":[],"topic":"1","isMC":true,"answer_ET":"A","question_images":[],"unix_timestamp":1670152920,"exam_id":7}],"exam":{"isImplemented":true,"isBeta":false,"name":"Professional Cloud Developer","id":7,"provider":"Google","numberOfQuestions":338,"isMCOnly":false,"lastUpdated":"11 Apr 2025"},"currentPage":26},"__N_SSP":true}