{"pageProps":{"questions":[{"id":"AnHZwCQAGEMnhcTJlyu3","answer":"A","answer_description":"","choices":{"A":"Use Cloud Scheduler and Cloud Functions to run the daily export.","D":"Run the cron job on a Compute Engine instance to continue the export.","C":"Set up Cloud Composer, and create a task to export the table daily.","B":"Create a streaming Datatlow job to export the table."},"discussion":[{"comment_id":"757318","timestamp":"1703583780.0","content":"Selected Answer: A\nA is Google recommended couple","poster":"chelbsik","upvote_count":"10"},{"comment_id":"996159","content":"Selected Answer: A\nhttps://cloud.google.com/blog/topics/developers-practitioners/scheduling-cloud-sql-exports-using-cloud-functions-and-cloud-scheduler","timestamp":"1725203760.0","poster":"DPonly","upvote_count":"1"},{"poster":"dynamic_dba","upvote_count":"3","comment_id":"840197","content":"A.\nMinimize cost and operational overhead eliminates D. Cloud Composer is a workflow orchestration service for managing workflow pipelines that span across clouds and on premises data centers. Eliminate C. Dataflow processes data. Eliminate B. A is the best answer. \nhttps://cloud.google.com/blog/topics/developers-practitioners/scheduling-cloud-sql-exports-using-cloud-functions-and-cloud-scheduler","timestamp":"1710528540.0"},{"comment_id":"805271","upvote_count":"1","poster":"zanhsieh","timestamp":"1707659820.0","content":"Selected Answer: C\nC\nA: Cloud Scheduler and Cloud Functions can't run on the on-premises MySQL database.\nB: Dataflow is mostly good for real time job. Again, need special tweaking to access the on-premises DB.\nC: Best answer. Composer deals with various kinds of inputs. Also handle retry / HA for free.\nD: Run cron job on a GCE instance then we have to maintain GCE instance stability."},{"poster":"orbo","content":"Selected Answer: B\nThe correct answer is \"Create a streaming Datatlow job to export the table\" because it is the most cost-effective and efficient solution. A streaming Datatlow job will allow you to export the table in real time, without having to manually run a cron job. This will save you time and money, and it will also ensure that the data is always up to date.","comment_id":"787610","upvote_count":"1","timestamp":"1706186040.0"},{"content":"Please ignore last two submits:\n A: Use Cloud Scheduler *** and Cloud Functions to run the daily export.","poster":"pk349","upvote_count":"3","timestamp":"1703454420.0","comment_id":"755181"},{"poster":"pk349","content":"A: Use Database Migration Service","comment_id":"755179","upvote_count":"2","timestamp":"1703454180.0"}],"url":"https://www.examtopics.com/discussions/google/view/92696-exam-professional-cloud-database-engineer-topic-1-question/","answers_community":["A (85%)","Other"],"question_id":6,"answer_ET":"A","topic":"1","exam_id":5,"answer_images":[],"timestamp":"2022-12-24 22:43:00","isMC":true,"unix_timestamp":1671918180,"question_text":"You finished migrating an on-premises MySQL database to Cloud SQL. You want to ensure that the daily export of a table, which was previously a cron job running on the database server, continues. You want the solution to minimize cost and operations overhead. What should you do?","question_images":[]},{"id":"noQHjRhvnvA4oXA0KyiP","answer":"A","answer_description":"","choices":{"B":"1. Build a Cloud Data Fusion pipeline for each table to migrate data from the on-premises MySQL database to Cloud SQL for MySQL.\n2. Schedule downtime to run each Cloud Data Fusion pipeline.\n3. Verify that the migration was successful.\n4. Re-point the applications to the Cloud SQL for MySQL instance.","A":"1. Use Database Migration Service to connect to your on-premises database, and choose continuous replication.\n2. After the on-premises database is migrated, promote the Cloud SQL for MySQL instance, and connect applications to your Cloud SQL instance.","C":"1. Pause the on-premises applications.\n2. Use the mysqldump utility to dump the database content in compressed format.\n3. Run gsutil –m to move the dump file to Cloud Storage.\n4. Use the Cloud SQL for MySQL import option.\n5. After the import operation is complete, re-point the applications to the Cloud SQL for MySQL instance.","D":"1 Pause the on-premises applications.\n2. Use the mysqldump utility to dump the database content in CSV format.\n3. Run gsutil –m to move the dump file to Cloud Storage.\n4. Use the Cloud SQL for MySQL import option.\n5. After the import operation is complete, re-point the applications to the Cloud SQL for MySQL instance."},"url":"https://www.examtopics.com/discussions/google/view/92856-exam-professional-cloud-database-engineer-topic-1-question/","discussion":[{"content":"Selected Answer: A\nDatabase Migration Service supports one-time and continuous migrations from source databases to Cloud SQL destination databases.","upvote_count":"1","comment_id":"996184","poster":"DPonly","timestamp":"1725204780.0"},{"content":"A.\nPreserve transactions and minimize downtime. That eliminates B, C and D. A is the best answer.","comment_id":"840201","upvote_count":"2","timestamp":"1710528780.0","poster":"dynamic_dba"},{"comment_id":"833278","upvote_count":"2","content":"Selected Answer: A\nA is the right one for me.","poster":"Nirca","timestamp":"1709923500.0"},{"timestamp":"1706186160.0","comment_id":"787612","content":"Selected Answer: A\nTo migrate the database while preserving transactions and minimizing downtime, you should use Database Migration Service. This service will allow you to migrate the database in a way that is transparent to your users and applications. It will also allow you to test the migration before you make it live, so that you can be sure that everything will work as expected.","poster":"orbo","upvote_count":"2"},{"timestamp":"1703728500.0","upvote_count":"1","poster":"SVGoogle89","comment_id":"759261","content":"A\nhttps://cloud.google.com/database-migration/docs/mysql/configure-source-database"},{"poster":"chelbsik","comment_id":"757319","content":"Selected Answer: A\nA looks like the best option to me","upvote_count":"3","timestamp":"1703583900.0"}],"answers_community":["A (100%)"],"answer_ET":"A","question_id":7,"topic":"1","exam_id":5,"answer_images":[],"timestamp":"2022-12-26 10:45:00","unix_timestamp":1672047900,"isMC":true,"question_text":"Your organization needs to migrate a critical, on-premises MySQL database to Cloud SQL for MySQL. The on-premises database is on a version of MySQL that is supported by Cloud SQL and uses the InnoDB storage engine. You need to migrate the database while preserving transactions and minimizing downtime. What should you do?","question_images":[]},{"id":"5exN8LkhJE8aGg7VUsF8","answer_description":"","unix_timestamp":1671918060,"choices":{"A":"Use Cloud SQL for data storage.","D":"Use Bigtable for data storage.","C":"Use Memorystore for data storage.","B":"Use Cloud Spanner for data storage."},"answer_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/google/view/92695-exam-professional-cloud-database-engineer-topic-1-question/","question_images":[],"answers_community":["B (100%)"],"discussion":[{"poster":"honeymania23","comment_id":"1190245","timestamp":"1728192720.0","upvote_count":"1","content":"if the data from shooping kart is transactional, then Yes B, else it should be D"},{"timestamp":"1715917620.0","content":"very low latency = Bigtable","comment_id":"1073051","poster":"Gunde","upvote_count":"1"},{"upvote_count":"3","content":"Selected Answer: B\nSpanner because \"Global\" eliminates others","timestamp":"1709314800.0","poster":"DPonly","comment_id":"996190"},{"comment_id":"840372","poster":"dynamic_dba","content":"B.\nGlobal eliminates Cloud SQL (A). Memorystore is a cache not a data storage solution. Eliminate C. Spanner has multi-region but doesn’t autoscale (unless you use Autoscaler). Bigtable instances can be in different regions and does autoscale. Both Bigtable and Spanner can handle the writes and uptime. The word “durable” and no mention of huge quantities of data suggests the best answer is B, Cloud Spanner.","timestamp":"1694810040.0","upvote_count":"2"},{"comment_id":"804220","poster":"zanhsieh","content":"Selected Answer: B\nB\nNote that what BT differ with Spanner is BT can't do very high throughput (insert single row one-by-one) and low latency at the same time. The official document suggested we should do that in batch; that's a rare case in the shopping cart and no customer will like to wait till other customers finishing their transaction so BT could handle together. Plus, it's hard for BT handling join, e.g. complex upsale, different type of products, etc.\nUptime 99.9% => BT, Spanner\nLow latency write => BT, Spanner\nHigh throughput low latency => Spanner\nECommerce & Shopping cart => SQL => Spanner\nhttps://cloud.google.com/bigtable/docs/writes","timestamp":"1691656560.0","upvote_count":"4"},{"timestamp":"1690281600.0","content":"Selected Answer: B\noogle Cloud Spanner is a highly scalable, reliable, and fully managed relational database service that runs on Google's infrastructure. It's designed to handle large amounts of data and provide high availability, even in the face of failures. Spanner can be used to store and manage data for a variety of applications, including e-commerce websites.\n\nSpanner is a good choice for this scenario because it can handle high write throughput and provides 99.99% uptime. It's also a good fit for applications that need to be highly available, even in the face of failures.","upvote_count":"1","poster":"orbo","comment_id":"787619"},{"comment_id":"771288","poster":"marpayer","upvote_count":"1","timestamp":"1688977200.0","content":"B- I will go with Spanner because it is a GLOBAL website and BT has a zonal location \nhttps://cloud.google.com/bigtable/docs/locations even it supports replication https://cloud.google.com/bigtable/docs/replication-overview\n\nIt doesn't mention ACID but it mentions durable"},{"poster":"ssaporylo","comment_id":"764349","timestamp":"1688368260.0","upvote_count":"1","content":"Low latency writes => BigTable\nNo mention ACID here\nthen my vote D"},{"timestamp":"1687783680.0","poster":"chelbsik","comment_id":"757557","content":"Selected Answer: B\nScalable, =>99,99, low write latency -> Spanner","upvote_count":"3"},{"content":"B: Use Cloud Spanner for data storage.\nCloud Spanner Service Level Agreement (SLA)\nDuring the term of the agreement under which Google has agreed to provide Google Cloud Platform to Customer (as applicable, the \"Agreement\"), the Covered Service will provide a Monthly Uptime Percentage to Customer as follows (the \"Service Level Objective\" or \"SLO\"):\nCovered Service Monthly Uptime Percentage\nCloud Spanner - Multi-Regional Instance >= 99.999%\nCloud Spanner - Regional Instance >= 99.99%","poster":"pk349","timestamp":"1687635660.0","comment_id":"755177","upvote_count":"1"}],"answer":"B","answer_ET":"B","timestamp":"2022-12-24 22:41:00","exam_id":5,"question_text":"Your company is developing a global ecommerce website on Google Cloud. Your development team is working on a shopping cart service that is durable and elastically scalable with live traffic. Business disruptions from unplanned downtime are expected to be less than 5 minutes per month. In addition, the application needs to have very low latency writes. You need a data storage solution that has high write throughput and provides 99.99% uptime. What should you do?","isMC":true,"question_id":8},{"id":"2gKHFTW2IkzL1l51ee2S","topic":"1","answer_images":[],"timestamp":"2022-12-24 22:39:00","question_images":[],"choices":{"D":"Build indexes on heavily accessed tables.","C":"Run the Recommender API to identify overprovisioned instances.","A":"Use Query Insights to identify idle instances.","B":"Remove inactive user accounts."},"discussion":[{"comment_id":"1205358","content":"Selected Answer: C\nhttps://cloud.google.com/sql/docs/mysql/recommender-sql-overprovisioned","upvote_count":"1","poster":"Pime13","timestamp":"1730698080.0"},{"comment_id":"840400","poster":"dynamic_dba","timestamp":"1694812440.0","upvote_count":"3","content":"C.\nThis is a cost question, so eliminate anything that doesn’t address cost. Eliminate A, B and C. The right answer is to use Recommender.\nhttps://cloud.google.com/recommender/docs/overview"},{"timestamp":"1688112360.0","content":"C\nThe Cloud SQL overprovisioned instance recommender helps you detect instances that are unnecessarily large for a given workload. It then provides recommendations on how to resize such instances and reduce cost. This page describes how this recommender works and how to use it.\nhttps://cloud.google.com/sql/docs/mysql/recommender-sql-overprovisioned#:~:text=The%20Cloud%20SQL%20overprovisioned%20instance%20recommender%20helps%20you%20detect%20instances%20that%20are%20unnecessarily%20large%20for%20a%20given%20workload.%20It%20then%20provides%20recommendations%20on%20how%20to%20resize%20such%20instances%20and%20reduce%20cost.%20This%20page%20describes%20how%20this%20recommender%20works%20and%20how%20to%20use%20it.","poster":"Atnafu","comment_id":"761831","upvote_count":"4"},{"poster":"chelbsik","content":"Selected Answer: C\nhttps://cloud.google.com/sql/docs/mysql/recommender-sql-overprovisioned","upvote_count":"4","timestamp":"1687783740.0","comment_id":"757558"},{"comment_id":"755176","poster":"pk349","timestamp":"1687635540.0","upvote_count":"1","content":"C: Run the Recommender *** API to identify overprovisioned instances.\nThe Cloud SQL idle instance recommender helps you detect instances that might be idle and provides you insights and recommendations to help you reduce costs .\nThe Cloud SQL overprovisioned instance recommender helps you detect instances that are unnecessarily large for a given workload."}],"answer":"C","exam_id":5,"unix_timestamp":1671917940,"answers_community":["C (100%)"],"question_text":"Your organization has hundreds of Cloud SQL for MySQL instances. You want to follow Google-recommended practices to optimize platform costs. What should you do?","question_id":9,"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/92694-exam-professional-cloud-database-engineer-topic-1-question/","isMC":true,"answer_ET":"C"},{"id":"RvEgLrQtojuQmDVUiuQm","question_text":"Your organization is running a critical production database on a virtual machine (VM) on Compute Engine. The VM has an ext4-formatted persistent disk for data files. The database will soon run out of storage space. You need to implement a solution that avoids downtime. What should you do?","unix_timestamp":1672066440,"url":"https://www.examtopics.com/discussions/google/view/92882-exam-professional-cloud-database-engineer-topic-1-question/","answer_ET":"A","isMC":true,"answer_description":"","discussion":[{"content":"Selected Answer: A\nIf you are using ext4, use the resize2fs command to extend the file system\nhttps://cloud.google.com/compute/docs/disks/resize-persistent-disk#resize_partitions","upvote_count":"7","poster":"chelbsik","timestamp":"1687784040.0","comment_id":"757563"},{"upvote_count":"1","content":"Selected Answer: A\nhttps://cloud.google.com/compute/docs/disks/resize-persistent-disk#resize_partitions","comment_id":"1205359","poster":"Pime13","timestamp":"1730541360.0"},{"upvote_count":"1","content":"Selected Answer: A\nA, this is almost the same question as the one on PCA too","timestamp":"1721992680.0","poster":"PKookNN","comment_id":"1132533"},{"timestamp":"1715687640.0","comment_id":"1070423","upvote_count":"1","poster":"[Removed]","content":"Selected Answer: C\nAi wrong for: -Requires the database to be offline. -Can be prone to errors."},{"timestamp":"1709316240.0","upvote_count":"1","poster":"DPonly","comment_id":"996226","content":"Selected Answer: A\nKey words , Critical application , Avoid downtime"},{"timestamp":"1694873040.0","comment_id":"841153","poster":"dynamic_dba","upvote_count":"2","content":"A.\nUsing fdisk by itself won’t achieve anything. A is wrong. Snapshotting a disk with database instances in flight is a potential problem. C is wrong. D, with some tweaking could move things to a larger disk, but there would be downtime. D is therefore wrong. There are several steps to achieving this task and they start with A. The link provided by chelbsik is spot on."}],"choices":{"C":"In the Google Cloud Console, create a snapshot of the persistent disk, restore the snapshot to a new larger disk, unmount the old disk, mount the new disk, and restart the database service.","D":"In the Google Cloud Console, create a new persistent disk attached to the VM, and configure the database service to move the files to the new disk.","A":"In the Google Cloud Console, increase the size of the persistent disk, and use the resize2fs command to extend the disk.","B":"In the Google Cloud Console, increase the size of the persistent disk, and use the fdisk command to verify that the new space is ready to use"},"question_id":10,"question_images":[],"topic":"1","exam_id":5,"answer":"A","answer_images":[],"answers_community":["A (91%)","9%"],"timestamp":"2022-12-26 15:54:00"}],"exam":{"numberOfQuestions":132,"isImplemented":true,"isMCOnly":true,"provider":"Google","name":"Professional Cloud Database Engineer","lastUpdated":"11 Apr 2025","id":5,"isBeta":false},"currentPage":2},"__N_SSP":true}