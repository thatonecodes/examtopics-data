{"pageProps":{"questions":[{"id":"YP32QBE0eVHPNElx2W8v","question_images":[],"timestamp":"2023-02-01 07:31:00","topic":"1","discussion":[{"comment_id":"1191987","content":"Selected Answer: B\nquite straightforward","upvote_count":"1","poster":"alpha_canary","timestamp":"1728450660.0"},{"poster":"__rajan__","timestamp":"1711364700.0","upvote_count":"1","content":"Selected Answer: B\nB is correct.","comment_id":"1016649"},{"upvote_count":"1","poster":"__rajan__","timestamp":"1711297260.0","comment_id":"1015880","content":"Selected Answer: B\nB is correct."},{"content":"Selected Answer: B\nB is correct. Simple and straight forward.\nCreate SA in Project A, Assign SA the role of object creator to push objects to Cloud bucket in Project B.","timestamp":"1707405300.0","poster":"purushi","upvote_count":"1","comment_id":"975712"},{"content":"took my exam yesterday (01-03-2023) and this question was there","poster":"Pime13","upvote_count":"4","comment_id":"826673","timestamp":"1693637040.0"},{"upvote_count":"2","content":"Selected Answer: B\nit's B.\n\nhttps://articles.wesionary.team/multi-project-account-service-account-in-gcp-ba8f8821347e","comment_id":"822491","timestamp":"1693051560.0","poster":"Pime13"},{"content":"Selected Answer: B\nA is not correct because you cannot run a Cloud Function with a service account that is not in the same Google Cloud project.\nB is correct because it follows the least privilege principle and for a Cloud Function, the service account must be created in the same project where the function is getting executed.","comment_id":"805257","upvote_count":"3","timestamp":"1691754060.0","poster":"mrvergara"},{"timestamp":"1691363760.0","comment_id":"800418","content":"option B is right. We have permissions to object creation in project for the SA created in proejct A. https://www.youtube.com/watch?v=ctACCk80H-w","poster":"anukulk","upvote_count":"2"},{"content":"Selected Answer: A\nIn option B, a service account is created in Project A, but this service account would have access to all the resources within Project A, which is more than is necessary for the task of saving output to a storage bucket in Project B.\n\nOptions C and D use the default App Engine service account, which would have more permissions than necessary, as it would have access to all App Engine resources within Project A or B, rather than just the permissions needed for the task of saving output to a storage bucket in Project B.","upvote_count":"2","comments":[{"poster":"TNT87","content":"No it cant be A, check the link provided below please. it cant be A, there is no way","upvote_count":"1","timestamp":"1691259540.0","comments":[{"comments":[{"timestamp":"1691754120.0","upvote_count":"1","poster":"mrvergara","comment_id":"805258","content":"It is the B option"},{"comment_id":"805070","poster":"TNT87","content":"Anyway i passed my exam last week","timestamp":"1691734260.0","upvote_count":"1","comments":[{"content":"Congrats, this time you are right. The answer is option B","upvote_count":"1","comment_id":"806341","timestamp":"1691838000.0","poster":"mrvergara"}]}],"upvote_count":"1","content":"https://cloud.google.com/docs/authentication/production#providing_credentials_to_your_application\n\nIn this guide, it explains the best practice for providing authentication credentials to your application. By creating a separate Google service account in the project that owns the resource you want to access (in this case, Project B), and then using that service account to perform actions on the resource (writing to the Cloud Storage bucket in Project B), you are following the principle of least privilege. This means that you are granting the minimum permissions necessary to perform the desired action.","comment_id":"802430","poster":"mrvergara","timestamp":"1691514180.0"}],"comment_id":"799141"}],"comment_id":"797891","poster":"mrvergara","timestamp":"1691142180.0"},{"poster":"TNT87","content":"Selected Answer: C\nhttps://cloud.google.com/functions/docs/concepts/iam#runtime_service_accounts","comment_id":"794879","timestamp":"1690864260.0","upvote_count":"1"}],"isMC":true,"question_id":161,"answers_community":["B (75%)","A (17%)","8%"],"answer_description":"","unix_timestamp":1675233060,"choices":{"A":"1. Create a Google service account in Project B.\n2. Deploy the Cloud Function with the service account in Project A.\n3. Assign this service account the roles/storage.objectCreator role on the storage bucket residing in Project B.","C":"1. Determine the default App Engine service account (PROJECT_ID@appspot.gserviceaccount.com) in Project A.\n2. Deploy the Cloud Function with the default App Engine service account in Project A.\n3. Assign the default App Engine service account the roles/storage.objectCreator role on the storage bucket residing in Project B.","B":"1. Create a Google service account in Project A\n2. Deploy the Cloud Function with the service account in Project A.\n3. Assign this service account the roles/storage.objectCreator role on the storage bucket residing in Project B.","D":"1. Determine the default App Engine service account (PROJECT_ID@appspot.gserviceaccount.com) in Project B.\n2. Deploy the Cloud Function with the default App Engine service account in Project A.\n3. Assign the default App Engine service account the roles/storage.objectCreator role on the storage bucket residing in Project B."},"answer":"B","answer_ET":"B","url":"https://www.examtopics.com/discussions/google/view/97512-exam-professional-cloud-developer-topic-1-question-243/","exam_id":7,"question_text":"You have two Google Cloud projects, named Project A and Project B. You need to create a Cloud Function in Project A that saves the output in a Cloud Storage bucket in Project B. You want to follow the principle of least privilege. What should you do?","answer_images":[]},{"id":"xgXbEm3iucV90uSCBj0y","answer_description":"","answers_community":["A (100%)"],"answer_ET":"A","timestamp":"2023-02-04 12:48:00","unix_timestamp":1675511280,"choices":{"D":"Create a job that copies the System Event logs from the _Required log bucket into the security team’s log bucket in their project.","A":"Create user-defined log buckets in the security team’s project. Configure a Cloud Logging sink to route your application’s logs to log buckets in the security team’s project.","B":"Create a job that copies the logs from the _Required log bucket into the security team’s log bucket in their project.","C":"Modify the _Default log bucket sink rules to reroute the logs into the security team’s log bucket."},"answer_images":[],"exam_id":7,"discussion":[{"upvote_count":"1","timestamp":"1727187720.0","content":"Selected Answer: A\nA is correct.","poster":"__rajan__","comment_id":"1015885"},{"upvote_count":"1","poster":"purushi","timestamp":"1723123260.0","content":"Selected Answer: A\nI go with A.\nThis question is to test Cloud Logging Sink feature.","comment_id":"975715"},{"timestamp":"1708365480.0","content":"Selected Answer: A\ni also choose A. https://cloud.google.com/architecture/security-log-analytics","poster":"Pime13","upvote_count":"2","comment_id":"814340"},{"content":"Selected Answer: A\nI choose option A because it provides a direct and automated solution for duplicating the specific application logs and sending them to the security team's project. This method uses Cloud Logging's sink feature, which is a powerful tool for routing logs to other destinations, such as log buckets or Pub/Sub topics. By using a sink, you can ensure that the duplication of logs is performed in real-time and automatically, which would minimize manual intervention and minimize the risk of errors.","upvote_count":"3","timestamp":"1707047280.0","poster":"mrvergara","comment_id":"797901"}],"topic":"1","isMC":true,"question_text":"A governmental regulation was recently passed that affects your application. For compliance purposes, you are now required to send a duplicate of specific application logs from your application’s project to a project that is restricted to the security team. What should you do?","question_images":[],"answer":"A","question_id":162,"url":"https://www.examtopics.com/discussions/google/view/97946-exam-professional-cloud-developer-topic-1-question-244/"},{"id":"VNi7bsIEj1YDZmHgOwge","question_text":"You plan to deploy a new Go application to Cloud Run. The source code is stored in Cloud Source Repositories. You need to configure a fully managed, automated, continuous deployment pipeline that runs when a source code commit is made. You want to use the simplest deployment solution. What should you do?","isMC":true,"url":"https://www.examtopics.com/discussions/google/view/97506-exam-professional-cloud-developer-topic-1-question-245/","discussion":[{"content":"Selected Answer: D\nD is correct.","comment_id":"1015889","poster":"__rajan__","timestamp":"1727187840.0","upvote_count":"1"},{"content":"Selected Answer: D\nD is more suitable since we need \"simplest deployment solution\".\nC is close but it is limited to only building the image and pushing that into artifact registry. It donot take part in deployment.\nB is also close but it takes more work than D.","timestamp":"1723123860.0","upvote_count":"1","comment_id":"975723","poster":"purushi"},{"upvote_count":"1","content":"Selected Answer: C\nThe correct answer is C, which is simpler than D.\nIn option C, there is no need to create a Dockerfile, making it a more straightforward solution compared to option D.","poster":"MaroonCDL","comment_id":"960052","timestamp":"1721706060.0"},{"comments":[{"timestamp":"1726478880.0","upvote_count":"1","content":"https://cloud.google.com/blog/products/serverless/build-and-deploy-an-app-to-cloud-run-with-a-single-command","comment_id":"1009051","poster":"kldn"}],"upvote_count":"1","poster":"azinge","comment_id":"933428","timestamp":"1719308460.0","content":"C. Configure continuous deployment of new revisions from a source repository for Cloud Run using buildpacks.\n\nThis is because Google Cloud Run offers the ability to automate the deployment of new revisions directly from a source repository using buildpacks. This is an extremely simple and managed way to set up a continuous deployment pipeline.\n\nOption D, while a valid method for automating deployments, is not as simple as using Cloud Run's integrated deployment feature, as it involves the additional service of Cloud Build."},{"comment_id":"822495","timestamp":"1708956480.0","content":"Selected Answer: D\nhttps://cloud.google.com/run/docs/continuous-deployment-with-cloud-build\nCloud Build is a fully managed, scalable, and efficient service provided by Google Cloud that allows you to automate your software delivery pipeline, including building, testing, and deploying applications. By using a trigger with Cloud Build, you can automatically build and deploy your Go application to Cloud Run whenever a source code commit is made in Cloud Source Repositories. This provides a simple, fully managed solution for continuous deployment, and eliminates the need for manual processes or external tools like Jenkins.","upvote_count":"1","poster":"Pime13"},{"comment_id":"797904","poster":"mrvergara","timestamp":"1707047460.0","upvote_count":"1","content":"Selected Answer: D\nCloud Build is a fully managed, scalable, and efficient service provided by Google Cloud that allows you to automate your software delivery pipeline, including building, testing, and deploying applications. By using a trigger with Cloud Build, you can automatically build and deploy your Go application to Cloud Run whenever a source code commit is made in Cloud Source Repositories. This provides a simple, fully managed solution for continuous deployment, and eliminates the need for manual processes or external tools like Jenkins."},{"upvote_count":"1","comment_id":"794765","poster":"TNT87","content":"Selected Answer: D\nhttps://cloud.google.com/run/docs/continuous-deployment-with-cloud-build","timestamp":"1706748120.0"}],"answer_description":"","answer":"D","timestamp":"2023-02-01 01:42:00","answer_ET":"D","answers_community":["D (83%)","C (17%)"],"answer_images":[],"unix_timestamp":1675212120,"question_images":[],"question_id":163,"topic":"1","exam_id":7,"choices":{"D":"Use Cloud Build with a trigger configured to run the container build and deploy process for each source code commit to Cloud Source Repositories.","B":"Configure a Jenkins trigger to run the container build and deploy process for each source code commit to Cloud Source Repositories.","A":"Configure a cron job on your workstations to periodically run gcloud run deploy --source in the working directory.","C":"Configure continuous deployment of new revisions from a source repository for Cloud Run using buildpacks."}},{"id":"XemNjIeyXmxDTN9iIMO7","answer_description":"","answer_ET":"AC","answers_community":["AC (100%)"],"timestamp":"2023-02-01 01:33:00","unix_timestamp":1675211580,"choices":{"B":"Use a proxyless Traffic Director configuration to connect the application to the service.","C":"Configure the legacy service's firewall to allow health checks originating from the proxy.","E":"Configure the legacy service's firewall to allow health checks originating from the Traffic Director control plane.","D":"Configure the legacy service's firewall to allow health checks originating from the application.","A":"Use Traffic Director with a sidecar proxy to connect the application to the service."},"answer_images":[],"exam_id":7,"discussion":[{"comment_id":"1015893","timestamp":"1727188020.0","poster":"__rajan__","upvote_count":"1","content":"Selected Answer: AC\nAC are correct."},{"upvote_count":"3","timestamp":"1709368680.0","comment_id":"826662","poster":"Pime13","content":"took my exam yesterday (01-03-2023) and this question was there"},{"content":"Selected Answer: AC\ni agree, AC","poster":"Pime13","timestamp":"1708955160.0","comment_id":"822468","upvote_count":"1"},{"poster":"mrvergara","comment_id":"797907","content":"Selected Answer: AC\nA. Using Traffic Director with a sidecar proxy can provide resilience for your application by allowing for failover to the secondary region in the event of an outage. The sidecar proxy can route traffic to the legacy service in either of the two GKE clusters, ensuring high availability.\n\nC. Configuring the legacy service's firewall to allow health checks originating from the proxy allows the proxy to periodically check the health of the legacy service and ensure that it is functioning properly. This helps to ensure that traffic is only routed to healthy instances of the legacy service, further improving the resilience of the setup.","upvote_count":"3","timestamp":"1707047760.0"},{"poster":"TNT87","comment_id":"794759","content":"Selected Answer: AC\nhttps://cloud.google.com/load-balancing/docs/health-checks#health_check_categories_protocols_and_ports","timestamp":"1706747760.0","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: AC\nhttps://cloud.google.com/traffic-director/docs/advanced-setup#routing-rule-maps\nhttps://cloud.google.com/traffic-director/docs/advanced-setup","comments":[{"poster":"TNT87","comment_id":"797080","content":"https://cloud.google.com/load-balancing/docs/health-check-concepts","timestamp":"1706968260.0","upvote_count":"1"}],"poster":"TNT87","comment_id":"794757","timestamp":"1706747580.0"}],"topic":"1","isMC":true,"question_text":"Your team has created an application that is hosted on a Google Kubernetes Engine (GKE) cluster. You need to connect the application to a legacy REST service that is deployed in two GKE clusters in two different regions. You want to connect your application to the target service in a way that is resilient. You also want to be able to run health checks on the legacy service on a separate port. How should you set up the connection? (Choose two.)","question_images":[],"answer":"AC","question_id":164,"url":"https://www.examtopics.com/discussions/google/view/97504-exam-professional-cloud-developer-topic-1-question-246/"},{"id":"pUS6jn8r0Idx0wQl1q5R","unix_timestamp":1675210680,"answer_description":"","answers_community":["C (100%)"],"exam_id":7,"topic":"1","isMC":true,"question_text":"You have an application running in a production Google Kubernetes Engine (GKE) cluster. You use Cloud Deploy to automatically deploy your application to your production GKE cluster. As part of your development process, you are planning to make frequent changes to the application’s source code and need to select the tools to test the changes before pushing them to your remote source code repository. Your toolset must meet the following requirements:\n• Test frequent local changes automatically.\n• Local deployment emulates production deployment.\n\nWhich tools should you use to test building and running a container on your laptop using minimal resources?","timestamp":"2023-02-01 01:18:00","choices":{"A":"Docker Compose and dockerd","C":"Minikube and Skaffold","B":"Terraform and kubeadm","D":"kaniko and Tekton"},"answer_images":[],"answer":"C","question_images":[],"discussion":[{"comment_id":"1082688","content":"How is this even related to GCP","poster":"tesix79748","upvote_count":"2","timestamp":"1732807260.0"},{"upvote_count":"1","poster":"__rajan__","content":"Selected Answer: C\nMinikube is a tool for running Kubernetes locally on your laptop. Skaffold is a tool for scaffolding, building, and deploying Kubernetes applications.","timestamp":"1727188140.0","comment_id":"1015898"},{"upvote_count":"1","content":"Selected Answer: C\nC is the correct choice. Since GKE local environment is required, Minikube and scaffold are right choices.","comments":[{"poster":"purushi","upvote_count":"1","timestamp":"1723321560.0","content":"Tilt and Octant are also very good local development tools to test K8S applications.","comment_id":"978046"}],"poster":"purushi","timestamp":"1723144020.0","comment_id":"975951"},{"content":"took my exam yesterday (01-03-2023) and this question was there","poster":"Pime13","comment_id":"826663","timestamp":"1709368740.0","upvote_count":"4"},{"poster":"mrvergara","timestamp":"1707048840.0","content":"Selected Answer: C\nMinikube is a tool that runs a single-node Kubernetes cluster locally on your laptop, allowing you to test and run your application on a simulated production environment. Skaffold is a command line tool that automates the process of building and deploying your application to a local or remote Kubernetes cluster.\n\nTogether, Minikube and Skaffold allow you to test your frequent changes locally, with a deployment that emulates a production environment, using minimal resources. Minikube provides the simulated production environment, while Skaffold takes care of building and deploying your application, making the development process smoother and more efficient.","comment_id":"797922","upvote_count":"3"},{"poster":"TNT87","content":"Selected Answer: C\nAnswer C\nMinikube is a lightweight Kubernetes implementation that creates a VM on your local machine and deploys a simple cluster containing only one node. Minikube is available for Linux, macOS, and Windows systems.\n\nSkaffold is a tool that handles the workflow for building, pushing and deploying your application. You can use Skaffold to easily configure a local development workspace, streamline your inner development loop, and integrate with other tools such as Kustomize and Helm to help manage your Kubernetes manifests","upvote_count":"1","comment_id":"794745","timestamp":"1706746680.0"}],"answer_ET":"C","url":"https://www.examtopics.com/discussions/google/view/97503-exam-professional-cloud-developer-topic-1-question-247/","question_id":165}],"exam":{"isBeta":false,"lastUpdated":"11 Apr 2025","name":"Professional Cloud Developer","isImplemented":true,"numberOfQuestions":338,"id":7,"isMCOnly":false,"provider":"Google"},"currentPage":33},"__N_SSP":true}