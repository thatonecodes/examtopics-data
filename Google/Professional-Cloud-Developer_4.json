{"pageProps":{"questions":[{"id":"e5NYnM2OxiYsv4OJBrnj","topic":"1","unix_timestamp":1641506880,"discussion":[{"upvote_count":"9","timestamp":"1673251320.0","comment_id":"520013","content":"Selected Answer: D\nShould be D definitely","poster":"ParagSanyashiv"},{"timestamp":"1726923660.0","poster":"__rajan__","upvote_count":"1","content":"Selected Answer: D\nD is correct.","comment_id":"1013087"},{"timestamp":"1705150620.0","upvote_count":"1","comment_id":"774493","poster":"telp","content":"Selected Answer: D\nA is not correct because local memory is lost on process termination, so you would lose the cart information.\nB is not correct because accessing a Cloud Storage bucket is slow and expensive for session information. This is not a Google Cloud best practice.\nC is not correct because BigQuery wouldn't be able to handle the frequent updates made to carts and sessions.\nD is correct because Memorystore is fast and a standard solution to store session information, and Firestore is ideal for small structured data such as a shopping cart. The user will be mapped to the shopping cart with a new session, if required."},{"comment_id":"772088","timestamp":"1704952500.0","content":"Selected Answer: D\nAnswer is D\nWhen storing session and shopping cart information for an ecommerce platform, it's important to consider scalability, reliability, and security. One solution that follows Google-recommended best practices would be to use Memorystore for Redis or Memorystore for Memcached to store session information and Firestore to store shopping cart information.\n\nMemorystore can store session information and easily handle a large number of concurrent connections, which is crucial for an ecommerce platform where users are logged in and adding items to their shopping cart frequently.\n\nFirestore can easily handle large amounts of semi-structured data, such as a shopping cart's item. Firestore is also a scalable and reliable solution, and it supports automatic scaling and replication.\n\nBy separating the session information and shopping cart information into different services, you can also increase security and avoid any potential data breaches. Using different services will also allows you to scale them independently.","upvote_count":"1","poster":"omermahgoub"},{"timestamp":"1703081040.0","poster":"zellck","content":"Selected Answer: D\nD is the answer.","upvote_count":"1","comment_id":"750967"},{"comments":[{"comment_id":"723377","content":"D is correct\nhttps://cloud.google.com/memorystore/docs/redis/redis-overview","timestamp":"1700561580.0","upvote_count":"1","poster":"TNT87"}],"poster":"N8dagr8","timestamp":"1699637160.0","upvote_count":"1","content":"anyone actually seen this on a test? is A actually correct?","comment_id":"715426"},{"timestamp":"1692511680.0","upvote_count":"3","comment_id":"649283","content":"Selected Answer: D\nD is correct","poster":"tomato123"},{"timestamp":"1691297940.0","poster":"akshaychavan7","content":"Selected Answer: D\nAgree with D","comment_id":"643193","upvote_count":"1"},{"poster":"nqthien041292","upvote_count":"1","timestamp":"1682659980.0","comment_id":"593547","content":"Selected Answer: D\nVote D"},{"comment_id":"518653","poster":"assuf","content":"Selected Answer: D\nvote D","timestamp":"1673042880.0","upvote_count":"4"}],"answers_community":["D (100%)"],"exam_id":7,"choices":{"D":"Store the session information in Memorystore for Redis or Memorystore for Memcached, and store the shopping cart information in Firestore.","C":"Store the session and shopping cart information in a MySQL database running on multiple Compute Engine instances.","B":"Store the shopping cart information in a file on Cloud Storage where the filename is the SESSION ID.","A":"Store the session information in Pub/Sub, and store the shopping cart information in Cloud SQL."},"timestamp":"2022-01-06 23:08:00","answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/69601-exam-professional-cloud-developer-topic-1-question-112/","answer_ET":"D","question_id":16,"question_images":[],"answer":"D","isMC":true,"question_text":"Your team is developing an ecommerce platform for your company. Users will log in to the website and add items to their shopping cart. Users will be automatically logged out after 30 minutes of inactivity. When users log back in, their shopping cart should be saved. How should you store users' session and shopping cart information while following Google-recommended best practices?","answer_description":""},{"id":"cz8ONA5k2KqwgJAONnRM","url":"https://www.examtopics.com/discussions/google/view/69764-exam-professional-cloud-developer-topic-1-question-113/","answers_community":["BC (74%)","AB (21%)","5%"],"answer_description":"","answer_ET":"BC","answer":"BC","choices":{"D":"Create a Kubernetes service account (KSA) for each application, and assign each KSA to the namespace.","B":"Create a namespace for each team, and attach resource quotas to each namespace.","C":"Create a LimitRange to specify the default compute resource requirements for each namespace.","A":"Specify the resource limits and requests in the object specifications.","E":"Use the Anthos Policy Controller to enforce label annotations on all namespaces. Use taints and tolerations to allow resource sharing for namespaces."},"answer_images":[],"question_text":"You are designing a resource-sharing policy for applications used by different teams in a Google Kubernetes Engine cluster. You need to ensure that all applications can access the resources needed to run. What should you do? (Choose two.)","unix_timestamp":1641745560,"topic":"1","question_images":[],"question_id":17,"isMC":true,"timestamp":"2022-01-09 17:26:00","exam_id":7,"discussion":[{"comment_id":"520326","comments":[{"comment_id":"524324","upvote_count":"6","content":"Yes agree B, C \nhttps://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-resource-requests-and-limits","poster":"Blueocean","timestamp":"1642268640.0"}],"poster":"scaenruy","timestamp":"1641745560.0","upvote_count":"13","content":"I vote B, C\nhttps://kubernetes.io/docs/concepts/policy/resource-quotas/\nhttps://kubernetes.io/docs/concepts/policy/limit-range/"},{"content":"Selected Answer: BC\nB. Create a namespace for each team and attach resource quotas to each namespace. This is a fundamental best practice for multi-tenancy in Kubernetes. Namespaces provide isolation and organization, and resource quotas allow you to control the maximum resources that applications within a namespace can consume. This ensures fair resource allocation and prevents one team from monopolizing resources.\nC. Create a LimitRange to specify the default compute resource requirements for each namespace. LimitRanges define minimum and maximum resource limits for objects (like Pods) within a namespace. This helps standardize resource usage and prevents applications from requesting excessive resources.","poster":"thewalker","comments":[{"poster":"thewalker","timestamp":"1721370540.0","comment_id":"1250904","content":"Let's look at why the other options are less ideal:\n\nA. Specify the resource limits and requests in the object specifications. While this is important for individual Pods, it doesn't provide a centralized way to manage resource sharing across multiple teams.\nD. Create a Kubernetes service account (KSA) for each application and assign each KSA to the namespace. Service accounts are for authentication and authorization, not for resource management.\nE. Use the Anthos Policy Controller to enforce label annotations on all namespaces. Use taints and tolerations to allow resource sharing for namespaces. Anthos Policy Controller is a powerful tool for policy enforcement, but it's not the primary mechanism for managing resource quotas and limits. Taints and tolerations are used for node affinity and scheduling, not for resource allocation.","upvote_count":"1"}],"comment_id":"1250903","timestamp":"1721370540.0","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: AC\nWhen it comes to Google's recommended best practices for Kubernetes, especially in the context of Google Kubernetes Engine (GKE), the emphasis is generally placed on setting specific resource requests and limits for each pod and container (Option A). This approach aligns with Kubernetes best practices, as it ensures efficient and reliable operation of applications by maximizing infrastructure utilization and guaranteeing smooth application performance.\n\nThis granular level of configuration, where resource requests and limits are explicitly set for each workload, is key to operating applications as efficiently and reliably as possible in Kubernetes clusters. It allows for the classification of pods into different Quality of Service (QoS) classes, such as 'Guaranteed' and 'Burstable', which further aids in resource management and scheduling decisions.","comment_id":"1080456","timestamp":"1700968920.0","poster":"plutonians123"},{"comment_id":"1013088","content":"Selected Answer: AB\nSpecify the resource limits and requests in the object specifications. This will ensure that each application is allocated the resources it needs to run, and that no application can consume more resources than it was allocated.\nCreate a namespace for each team, and attach resource quotas to each namespace. This will allow you to isolate each team's applications from each other, and to ensure that each team's applications are not consuming more resources than they were allocated.","upvote_count":"1","timestamp":"1695301560.0","poster":"__rajan__"},{"upvote_count":"2","content":"I go with B and C. Defining namespaces and Limiting resource quotas for each team in order to avoid resource collisions and hunger.","comment_id":"972901","timestamp":"1691231160.0","poster":"purushi"},{"timestamp":"1673417040.0","poster":"omermahgoub","content":"Selected Answer: BC\nIn the context of the problem statement, B and C are appropriate solution for ensuring that all applications can access the resources needed to run:\n\nB. Create a namespace for each team, and attach resource quotas to each namespace. This way, you can set limits on the resources that a team can consume, so that one team does not consume all the resources of the cluster, and that resources are shared among all teams in a fair way.\n\nC. Create a LimitRange to specify the default compute resource requirements for each namespace. LimitRanges allow you to set default limits and requests for all the pods in a specific namespace, it also ensure that pods in that namespace can never consume more resources than the LimitRange defined.\n\nYou can use a combination of resource limits, quotas, and limit ranges to prevent a single team or application from consuming too many resources, as well as to ensure that all teams and applications have access to the resources they need to run.","upvote_count":"1","comments":[{"comment_id":"772102","poster":"omermahgoub","upvote_count":"1","content":"Option A: Specify the resource limits and requests in the object specifications, is a valid method for controlling the resources that a pod or container needs, but it may not be sufficient by itself to fully manage the resources in a multi-tenant cluster where multiple teams and applications need to share resources.\n\nWhen you set resource limits and requests on the pod or container level, you have a fine-grained control over the resources that a specific pod or container needs, but it doesn't provide a way to set limits or quotas on the level of a whole team or namespace. It also doesn't provide a default configuration for all pods created in a namespace.\n\nBy itself, this method does not give you the visibility and control you need over the overall resource usage across multiple teams and applications. With creating a namespace per team and attaching quotas, you can limit the resources each team can use, and with LimitRange you can ensure that no pod created in the namespace can go beyond specific limits.","timestamp":"1673417040.0"}],"comment_id":"772101"},{"comment_id":"750964","poster":"zellck","timestamp":"1671544740.0","upvote_count":"2","content":"Selected Answer: BC\nBC is the answer.\n\nhttps://kubernetes.io/docs/concepts/policy/resource-quotas/\nA resource quota, defined by a ResourceQuota object, provides constraints that limit aggregate resource consumption per namespace. It can limit the quantity of objects that can be created in a namespace by type, as well as the total amount of compute resources that may be consumed by resources in that namespace.\n\nhttps://kubernetes.io/docs/concepts/policy/limit-range/\nA LimitRange is a policy to constrain the resource allocations (limits and requests) that you can specify for each applicable object kind (such as Pod or PersistentVolumeClaim) in a namespace."},{"comment_id":"723400","content":"Selected Answer: BC\nhttps://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-resource-requests-and-limits\nAns B,C","timestamp":"1669027020.0","upvote_count":"3","poster":"TNT87"},{"poster":"tomato123","content":"Selected Answer: BC\nBC are correct","upvote_count":"4","timestamp":"1660975740.0","comment_id":"649284"},{"poster":"bk7","comments":[{"upvote_count":"1","timestamp":"1660829640.0","poster":"alex8081","content":"why A&B?","comment_id":"648445"}],"timestamp":"1660653480.0","comment_id":"647651","content":"Selected Answer: AB\nA&B as obvious !","upvote_count":"3"},{"upvote_count":"1","poster":"akshaychavan7","content":"Selected Answer: BC\nI will go with B and C.","timestamp":"1659762420.0","comment_id":"643200"},{"timestamp":"1657630560.0","comment_id":"630534","upvote_count":"2","poster":"jdx000","content":"Selected Answer: BC\nB,C are the right options"},{"content":"A, B\nLimitRanges are great and all, but they don't actually guarantee that running containers have the resources available that they need, since they're not object-specific.","poster":"[Removed]","timestamp":"1651077180.0","upvote_count":"1","comment_id":"593295"}]},{"id":"kURCFbyVvwilIsDcAq64","exam_id":7,"answer_images":[],"answer":"A","discussion":[{"poster":"omermahgoub","comments":[{"content":"What about the versioning aspect?","poster":"AscendedCrow","upvote_count":"1","comment_id":"828765","timestamp":"1677920280.0"},{"comment_id":"772159","upvote_count":"2","content":"Option A: 1. Store the application and infrastructure source code in a Git repository. 2. Use Cloud Build to deploy the application infrastructure with Terraform. 3. Deploy the application to a Cloud Function as a pipeline step, can potentially satisfies the requirement of versioning and auditing the infrastructure changes, but it may not meet the other two requirements of using Google-managed services and deploying the application on a serverless compute platform:\n\n- By using Terraform, which is a third-party infrastructure as code tool, it is not a Google-managed service and it may not have the same level of integration as Google-managed services.\n- Cloud Functions are a serverless compute platform, but it's mainly used to run event-driven, short-lived functions, while it's not a suitable choice for running long running processes, web servers and so on.","timestamp":"1673421480.0","poster":"omermahgoub","comments":[{"timestamp":"1673421480.0","comment_id":"772160","upvote_count":"1","poster":"omermahgoub","comments":[{"timestamp":"1673421480.0","upvote_count":"1","poster":"omermahgoub","comment_id":"772162","content":"Although, all of the options may have their own merits and depending on the specific requirement of the application any of them can be suitable, but considering all the requirements stated in the question option D could be the best fit."}],"content":"In addition, deploying the infrastructure using Terraform, which is not fully integrated with the google cloud, may lead to additional cost and management effort.\nAlso, deploying the application on Cloud Functions may not be able to meet some of the requirements like long running processes, stateful workloads and other requirements that Cloud Run can fulfill."}]}],"timestamp":"1673421420.0","comment_id":"772157","upvote_count":"6","content":"Selected Answer: D\nOption D is the best fit for designing the architecture of the new application as it satisfies all the design requirements of versioning and auditing the infrastructure changes, using Google-managed services and deploying the application on a serverless compute platform. The approach includes:\n\n- Deploy the application infrastructure using gcloud commands.\n- Use Cloud Build to define a continuous integration pipeline for changes to the application source code.\n- Configure a pipeline step to pull the application source code from a Git repository, and create a containerized application.\n- Deploy the new container on Cloud Run as a pipeline step.\nIt's worth noting that all options could potentially satisfy the requirements, as long as they use Google-managed services and track infrastructure creation and changes, the choice of different services, platform and tools depend on the specific requirements of your application and development preferences."},{"poster":"morenocasado","comment_id":"586656","comments":[{"comment_id":"1078635","timestamp":"1700763360.0","content":"Cloud Functions are intended for single-purpose functions, not an entire app.\n\nD is a far better fit here and nobody is talking about \"versioning gcloud commands\" - Cloud Run has revisions (=versions), which meets the task's criteria.","poster":"wanrltw","upvote_count":"2"}],"timestamp":"1650094680.0","upvote_count":"6","content":"Selected Answer: A\nA is the correct choice.\n\nB - use Jenkins as the deployment tool instead of Cloud Build (The application and deployment infrastructure uses Google-managed services as much as possible).\nC - uses Compute Engine to run containers. CE is not serverless.\nD - we can't version gcloud commands"},{"upvote_count":"1","timestamp":"1742964300.0","poster":"Jason_Cloud_at","content":"Selected Answer: A\nomitting B&C as its not native solution and using compute engine is not preferable for serverless, based on \"Version & Audit\" im choosing A , in option D , they have mentioned about gcloud commands , how can you store the entire application source code in gcloud and use it for versioning, if it is not for versioning , option D is the best choice","comment_id":"1410242"},{"comment_id":"1251219","upvote_count":"2","poster":"thewalker","timestamp":"1721397120.0","content":"Selected Answer: D\nD. 1. Deploy the application infrastructure using gcloud commands. 2. Use Cloud Build to define a continuous integration pipeline for changes to the application source code. 3. Configure a pipeline step to pull the application source code from a Git repository and create a containerized application. 4. Deploy the new container on Cloud Run as a pipeline step.\nVersioned and Auditable Infrastructure: While Terraform (option A) is a great choice for infrastructure as code, using gcloud commands directly allows for version control and auditing through your Git repository. This ensures a clear history of infrastructure changes.\nGoogle-Managed Services: Cloud Build, Cloud Run, and Git repositories are all Google-managed services, fulfilling the requirement for using Google-managed services as much as possible.\nServerless Compute: Cloud Run is a serverless platform that perfectly fits the requirement for a serverless compute environment.","comments":[{"upvote_count":"2","poster":"thewalker","timestamp":"1721397120.0","comment_id":"1251220","content":"Why the other options are less suitable:\n\nA. While Terraform is great for infrastructure as code, it's not as directly integrated with Google Cloud's CI/CD tools as gcloud commands.\nB. Jenkins is a third-party tool, not a Google-managed service. It also doesn't inherently provide the same level of integration with Google Cloud's CI/CD tools as Cloud Build.\nC. Deployment Manager is a good option for infrastructure as code, but it's not as flexible as gcloud commands for direct control. Also, deploying to a Compute Engine instance doesn't meet the serverless requirement."}]},{"comment_id":"1247208","content":"Selected Answer: D\nCorrect answer D based on the requirements","upvote_count":"2","poster":"d_ella2001","timestamp":"1720862400.0"},{"comment_id":"1246560","timestamp":"1720768320.0","content":"Selected Answer: D\nI agree with omermahgoub.","upvote_count":"2","poster":"d_ella2001"},{"timestamp":"1713323940.0","comment_id":"1196956","content":"Selected Answer: A\nD would be true if & only it didn't mention using gcloud commands to deploy application infrastructure","poster":"alpha_canary","upvote_count":"3"},{"poster":"wanrltw","upvote_count":"4","content":"Selected Answer: D\nI vote D:\n- gcloud, Cloud Build, Cloud Run - are Google-managed services\n- Cloud Run has revisions (=versions)\n- Cloud Run is serverless\n\nA is wrong, as Cloud Functions are intended for single-purpose functions - not an entire app.","comment_id":"1078637","timestamp":"1700763540.0"},{"comment_id":"1013089","timestamp":"1695301740.0","poster":"__rajan__","content":"Selected Answer: A\nA is correct","upvote_count":"1"},{"content":"Selected Answer: A\nMy answer is A:\nVersion and auditable: GIT\nGCP managed deployment infrastructure: Cloud build, Cloud Deployment Manager, Terraform\nServerless: Cloud Functions","comment_id":"972909","upvote_count":"2","timestamp":"1691231700.0","poster":"purushi"},{"comment_id":"885827","poster":"NewComer200","upvote_count":"1","timestamp":"1682911860.0","content":"Selected Answer: A\nIt is definitely A vs. B, though.\nI still think the deciding factor is \"Creation and changes to the application infrastructure are versioned and auditable\".\nWhether to deploy to Cloud Run or Cloud Functions is irrelevant because we don't know the contents of the application.\nBoth are serverless."},{"poster":"AscendedCrow","upvote_count":"4","comment_id":"828762","timestamp":"1677919860.0","content":"Selected Answer: D\nWhat put me off A is that at the end there is deploy to ` Cloud Function` and it should be all serverless applications and not just a cloud function, that is what Cloud Run should do."},{"upvote_count":"1","poster":"zellck","content":"Selected Answer: A\nA is the answer.","comment_id":"750958","timestamp":"1671544440.0"},{"upvote_count":"1","poster":"TNT87","timestamp":"1669027260.0","comment_id":"723407","content":"Ans A\nhttps://cloud.google.com/docs/ci-cd/products#featured-products-for-cicd"},{"poster":"[Removed]","comment_id":"676975","upvote_count":"3","content":"D is correct, applications should not be deployed on cloud functions.","timestamp":"1663927140.0"},{"comment_id":"649285","poster":"tomato123","content":"Selected Answer: A\nA is correct","upvote_count":"4","timestamp":"1660975740.0"},{"upvote_count":"1","content":"B and C are not correct (we cant use jenkins in option B, and cant use compute engine as it is not serverless in option C)\n\nSo in A and D option - \noption A is not right becoz we can deploy on cloud function not suitable as serverless compute \nSo i think Answer is D","comment_id":"603288","poster":"dishum","comments":[{"upvote_count":"2","content":"Changing my answer to A, becoz of versioning. can't use gcloud commands in versioning in option D","poster":"dishum","timestamp":"1653021000.0","comment_id":"604237"}],"timestamp":"1652878020.0"},{"timestamp":"1651124220.0","comment_id":"593553","poster":"nqthien041292","upvote_count":"2","content":"Selected Answer: A\nVote A"},{"content":"Correct answer is C","poster":"jitu028","timestamp":"1649510340.0","upvote_count":"1","comment_id":"583322"},{"content":"I vote for A, simply because the infrastructure setup must be versioned, which is not reflected in D.","timestamp":"1649163660.0","comment_id":"581257","poster":"BackendBoi","upvote_count":"4","comments":[{"comment_id":"581258","poster":"BackendBoi","timestamp":"1649163720.0","upvote_count":"1","content":"https://cloud.google.com/architecture/managing-infrastructure-as-code"}]},{"content":"Yes, I vote D","poster":"scaenruy","timestamp":"1641745800.0","comment_id":"520328","upvote_count":"3"}],"unix_timestamp":1641745800,"answer_ET":"A","topic":"1","url":"https://www.examtopics.com/discussions/google/view/69765-exam-professional-cloud-developer-topic-1-question-114/","question_id":18,"answers_community":["A (51%)","D (49%)"],"question_images":[],"timestamp":"2022-01-09 17:30:00","question_text":"You are developing a new application that has the following design requirements:\n✑ Creation and changes to the application infrastructure are versioned and auditable.\n✑ The application and deployment infrastructure uses Google-managed services as much as possible.\n✑ The application runs on a serverless compute platform.\nHow should you design the application's architecture?","isMC":true,"answer_description":"","choices":{"C":"1. Create a continuous integration pipeline on Cloud Build, and configure the pipeline to deploy the application infrastructure using Deployment Manager templates. 2. Configure a pipeline step to create a container with the latest application source code. 3. Deploy the container to a Compute Engine instance as a pipeline step.","D":"1. Deploy the application infrastructure using gcloud commands. 2. Use Cloud Build to define a continuous integration pipeline for changes to the application source code. 3. Configure a pipeline step to pull the application source code from a Git repository, and create a containerized application. 4. Deploy the new container on Cloud Run as a pipeline step.","A":"1. Store the application and infrastructure source code in a Git repository. 2. Use Cloud Build to deploy the application infrastructure with Terraform. 3. Deploy the application to a Cloud Function as a pipeline step.","B":"1. Deploy Jenkins from the Google Cloud Marketplace, and define a continuous integration pipeline in Jenkins. 2. Configure a pipeline step to pull the application source code from a Git repository. 3. Deploy the application source code to App Engine as a pipeline step."}},{"id":"uwRVPmEltJsZQatLcQD3","answer_description":"","question_id":19,"answer":"B","answer_ET":"B","answer_images":[],"question_text":"You are creating and running containers across different projects in Google Cloud. The application you are developing needs to access Google Cloud services from within Google Kubernetes Engine (GKE). What should you do?","exam_id":7,"choices":{"A":"Assign a Google service account to the GKE nodes.","B":"Use a Google service account to run the Pod with Workload Identity.","C":"Store the Google service account credentials as a Kubernetes Secret.","D":"Use a Google service account with GKE role-based access control (RBAC)."},"unix_timestamp":1641509700,"topic":"1","discussion":[{"comment_id":"529583","content":"Option B \nhttps://cloud.google.com/kubernetes-engine/docs/concepts/workload-identity","upvote_count":"7","poster":"Blueocean","timestamp":"1658454960.0"},{"comment_id":"1196957","poster":"alpha_canary","upvote_count":"1","timestamp":"1729135200.0","content":"Selected Answer: B\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/workload-identity#:~:text=Workload%20Identity%20Federation%20for%20GKE%20is%20the%20recommended%20way%20for%20your%20workloads%20running%20on%20Google%20Kubernetes%20Engine%20(GKE)%20to%20access%20Google%20Cloud%20services%20in%20a%20secure%20and%20manageable%20way."},{"content":"Selected Answer: B\nThe best way to access Google Cloud services from within Google Kubernetes Engine (GKE) is to use a Google service account to run the Pod with Workload Identity.\n\nWorkload Identity allows your pods to authenticate to Google Cloud services using their Kubernetes service account credentials, without you having to expose any sensitive credentials in your code.","timestamp":"1711034040.0","upvote_count":"1","poster":"__rajan__","comment_id":"1013093"},{"comment_id":"972934","upvote_count":"2","content":"Selected Answer: B\nApplication images runs as a container within a POD as a process. So Pod should be identified as a principle here and it should have a service account to access other services within GKE cluster.","poster":"purushi","timestamp":"1707138900.0"},{"poster":"omermahgoub","comment_id":"772168","timestamp":"1689052860.0","content":"Selected Answer: B\nIn summary, using Workload Identity allows you to authenticate your application to Google Cloud services using the same identity that runs the application, this makes it simple to manage the access and permissions to resources, and also ensures that your application only has the necessary permissions to access the services.","upvote_count":"2"},{"comments":[{"timestamp":"1689052800.0","upvote_count":"1","poster":"omermahgoub","comment_id":"772165","content":"Use a Google service account with GKE role-based access control (RBAC) (Option D) is not the recommended approach, while RBAC is good to restrict and manage access to resources, it's not the best fit for authenticating the workloads to access the Google Cloud services."}],"content":"The correct answer is B: Use a Google service account to run the Pod with Workload Identity.\n\nWorkload Identity allows you to authenticate to Google Cloud services using the same identity that runs your application, instead of creating and managing a separate service account. This simplifies the process of granting permissions to your application, and ensures that it only has the necessary access to resources.\n\nWhen you assign a Google service account to GKE nodes (Option A), it can be difficult to manage the permissions needed by the application and also could be a security issue since it grants access to all the services that the service account has permissions to.","comment_id":"772164","timestamp":"1689052800.0","poster":"omermahgoub","upvote_count":"1"},{"poster":"zellck","comment_id":"750834","content":"Selected Answer: B\nB is the answer.\n\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/workload-identity#what_is\nApplications running on GKE might need access to Google Cloud APIs such as Compute Engine API, BigQuery Storage API, or Machine Learning APIs.\n\nWorkload Identity allows a Kubernetes service account in your GKE cluster to act as an IAM service account. Pods that use the configured Kubernetes service account automatically authenticate as the IAM service account when accessing Google Cloud APIs. Using Workload Identity allows you to assign distinct, fine-grained identities and authorization for each application in your cluster.","timestamp":"1687255920.0","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: B\nB is correct","timestamp":"1676880600.0","comment_id":"649286","poster":"tomato123"},{"upvote_count":"1","comment_id":"643205","content":"Selected Answer: B\nI will go with option B.","timestamp":"1675668780.0","poster":"akshaychavan7"},{"content":"Selected Answer: B\nVote B","timestamp":"1666935480.0","comment_id":"593555","upvote_count":"1","poster":"nqthien041292"},{"poster":"jitu028","content":"Correct answer is B","timestamp":"1665321600.0","upvote_count":"1","comment_id":"583323"},{"poster":"assuf","upvote_count":"4","content":"vote B","comment_id":"518662","timestamp":"1657140900.0"}],"isMC":true,"answers_community":["B (100%)"],"url":"https://www.examtopics.com/discussions/google/view/69603-exam-professional-cloud-developer-topic-1-question-115/","timestamp":"2022-01-06 23:55:00","question_images":[]},{"id":"7ieTvpqTRAQtwcqqFVSL","answer_images":[],"choices":{"C":"Use the COPY statement in the Dockerfile to load the configuration into the container image. Verify that the configuration is available, and start the service using an ENTRYPOINT script.","B":"Create a PersistentVolumeClaim on the GKE cluster. Access the configuration files from the volume, and start the service using an ENTRYPOINT script.","A":"Use the gsutil utility to copy files from within the Docker container at startup, and start the service using an ENTRYPOINT script.","D":"Add a startup script to the GKE instance group to mount the NFS share at node startup. Copy the configuration files into the container, and start the service using an ENTRYPOINT script."},"question_id":20,"exam_id":7,"timestamp":"2022-01-07 00:13:00","url":"https://www.examtopics.com/discussions/google/view/69604-exam-professional-cloud-developer-topic-1-question-116/","unix_timestamp":1641510780,"answer":"B","answer_description":"","isMC":true,"question_images":[],"topic":"1","answers_community":["B (100%)"],"discussion":[{"timestamp":"1645356900.0","poster":"Ksamilosb","comments":[{"poster":"GCPCloudArchitectUser","timestamp":"1645909680.0","upvote_count":"1","content":"I am not convinced but it does seem to be best option among all options","comment_id":"556994"}],"comment_id":"551736","upvote_count":"8","content":"It's not necessary to mount NFS to each node in GKE. Just create PVC point to shared NFS, mount to container, and use configuration in ENTRYPOINT. Vote B"},{"poster":"thewalker","timestamp":"1721397540.0","upvote_count":"1","content":"Selected Answer: B\nThe best answer here is B. Create a PersistentVolumeClaim on the GKE cluster. Access the configuration files from the volume and start the service using an ENTRYPOINT script. \n[1] \n\nHere's why:\n\nPersistentVolumeClaim (PVC): A PVC is the Kubernetes way to request storage. By creating a PVC that points to your NFS share, you ensure that your pods have access to the configuration files. This approach is more robust and scalable than copying files at startup.\nENTRYPOINT Script: An ENTRYPOINT script is the ideal way to handle the startup logic. You can use it to:\nMount the PVC volume.\nVerify that the configuration files are present.\nStart your application's main process.","comment_id":"1251226"},{"upvote_count":"1","comment_id":"1013095","timestamp":"1695302100.0","poster":"__rajan__","content":"Selected Answer: B\nB is correct."},{"upvote_count":"1","content":"Selected Answer: B\nB is more formal and standardized way to mount NFS onto the worker node compared to A where it asks us to create a startup script to mount the volume.","timestamp":"1691234760.0","poster":"purushi","comment_id":"972943"},{"poster":"purushi","comment_id":"972938","timestamp":"1691234520.0","content":"Selected Answer: B\nWith PersisentVolumeClaim object, we can claim the volume what we need dynamically. The storage class will be defined by network administrator. Container/Pod needs to wait until it reads configuration from the mounted volume before serving traffic to its clients.","upvote_count":"1"},{"poster":"jnas","content":"Selected Answer: B\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/persistent-volumes. PersistentVolume resources are used to manage durable storage in a cluster. In GKE, a PersistentVolume is typically backed by a persistent disk. You can also use other storage solutions like NFS. Filestore is a NFS solution on Google Cloud","upvote_count":"1","comment_id":"956657","timestamp":"1689771900.0"},{"timestamp":"1673421900.0","upvote_count":"1","content":"Selected Answer: B\nB and D are the main candidate answers\n\nOption B: allows the application to be stateless and have no dependencies on the filesystem of the host.\n\nD: is a good solution since it allows the application to access its configuration as soon as the application starts, without having to copy the configuration files into the container.\n\nBut the best option is B, because it allows the application to be stateless and have no dependencies on the filesystem of the host. This approach is more flexible, makes it easy to update the configuration files, and reduces the size of the container image.","comment_id":"772170","poster":"omermahgoub"},{"upvote_count":"1","comment_id":"768711","content":"Selected Answer: B\nB is correct using default tools","poster":"telp","timestamp":"1673107980.0"},{"poster":"TNT87","timestamp":"1672216620.0","comment_id":"759576","upvote_count":"1","content":"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes\n\n \nhttps://cloud.google.com/filestore/docs/accessing-fileshares\n\nhttps://cloud.google.com/storage/docs/gcs-fuse"},{"timestamp":"1671538200.0","content":"Selected Answer: B\nB is the answer.","poster":"zellck","comment_id":"750833","upvote_count":"1"},{"comment_id":"649287","poster":"tomato123","content":"Selected Answer: B\nB is correct","upvote_count":"3","timestamp":"1660975800.0"},{"comment_id":"604185","content":"Selected Answer: B\nI think B is more suitable in this situation","upvote_count":"2","poster":"americoleonardo","timestamp":"1653006420.0"},{"timestamp":"1646087580.0","poster":"juancambb","comments":[{"comment_id":"558397","content":"answer is B","poster":"juancambb","timestamp":"1646087640.0","upvote_count":"1"}],"upvote_count":"2","content":"B and C are valid, but B use native tools of Kubernetes, so is a best practice and easy to implement.","comment_id":"558396"},{"poster":"assuf","upvote_count":"4","content":"vote C","comment_id":"518666","timestamp":"1641510780.0"}],"answer_ET":"B","question_text":"You have containerized a legacy application that stores its configuration on an NFS share. You need to deploy this application to Google Kubernetes Engine\n(GKE) and do not want the application serving traffic until after the configuration has been retrieved. What should you do?"}],"exam":{"isImplemented":true,"isBeta":false,"isMCOnly":false,"id":7,"name":"Professional Cloud Developer","provider":"Google","numberOfQuestions":338,"lastUpdated":"11 Apr 2025"},"currentPage":4},"__N_SSP":true}