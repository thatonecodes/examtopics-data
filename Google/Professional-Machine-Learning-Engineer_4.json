{"pageProps":{"questions":[{"id":"pXVsiPDNvsCERtQuN2xa","discussion":[{"upvote_count":"11","timestamp":"1687043820.0","content":"Selected Answer: A\nSyntactic Analysis is not for sentiment analysis","comment_id":"748528","poster":"mymy9418"},{"upvote_count":"5","poster":"fitri001","content":"Selected Answer: A\nA. Convert speech to text and extract sentiments based on sentences: This method focuses on the content of the conversation, minimizing the influence of factors like voice tone (which can be culturally or gender-specific). Sentiment analysis techniques can analyze the meaning and context of sentences to identify positive, negative, or neutral sentiment.","comment_id":"1202325","timestamp":"1729905900.0","comments":[{"upvote_count":"2","comment_id":"1202326","poster":"fitri001","timestamp":"1729905960.0","content":"B. Convert speech to text and build a model based on the words: While words are important, relying solely on them can miss the context and lead to bias. For example, \"great\" might be positive in most cases, but in some cultures, it might be used sarcastically.\n\nC. Extract sentiment directly from voice recordings: This approach can be biased as voice characteristics like pitch or pace can vary based on gender, age, and cultural background.\n\nD. Convert speech to text and extract sentiment using syntactical analysis: While syntax can provide some clues, it's not the strongest indicator of sentiment. Additionally, cultural differences in sentence structure could impact accuracy."}]},{"upvote_count":"1","poster":"desertlotus1211","comment_id":"1362339","content":"Selected Answer: D\ngoes a step further by analyzing the structure and grammatical relationships within the sentences. This approach abstracts away from just the words used and focuses on how the words are put together, capturing deeper semantic and contextual information, less likely to be influenced by demographic variations in language use","timestamp":"1740609180.0"},{"upvote_count":"2","poster":"RioGrande","comment_id":"1082747","timestamp":"1716907440.0","content":"The correct answer should be A. Word embeddings have static embeddings for the same words, while contextual embeddings vary depending on the context.\n\n\"May’s sentence embedding adaptation of WEAT, known as the Sentence Embedding Association Test (SEAT), shows less clear racial and gender bias in language models and embeddings than the corresponding word embedding formulation\"\n\nFrom: https://medium.com/institute-for-applied-computational-science/bias-in-nlp-embeddings-b1dabb8bbe20"},{"content":"Selected Answer: B\nThis approach involves converting the speech to text, which allows you to analyze the content of the conversations without directly dealing with the speakers' gender, age, or cultural differences. By building a model based on the words, you can focus on the language used in the conversations to predict sentiment, making the model more inclusive and less sensitive to demographic factors.\n\nOption A could be influenced by the syntactical nuances and structures used in different cultures, and option C might be impacted by the variations in voice tones across genders and ages. Option B, on the other hand, relies on the text content, which provides a more neutral and content-focused basis for sentiment analysis.","comment_id":"1072469","poster":"pico","timestamp":"1715861280.0","upvote_count":"2"},{"content":"Selected Answer: B\nB: People of different cultures will often use difference sentence structures, so words would be safer than sentences","comment_id":"1050453","poster":"MCorsetti","upvote_count":"1","timestamp":"1713782580.0","comments":[{"comment_id":"1067047","timestamp":"1715321160.0","poster":"tavva_prudhvi","upvote_count":"1","content":"Yeah, but they(words) may miss the context of the sentiment, leading to inaccuracies!"}]},{"content":"Selected Answer: A\nbuilding a model based on words, may also be effective but could potentially be influenced by factors such as accents, dialects, or language variations that may differ between speakers.extracting sentiment directly from voice recordings, may be less accurate due to the subjective nature of interpreting emotions from audio alone.using syntactical analysis, may be useful in certain contexts but may not capture the full range of sentiment expressed in a conversation. Therefore, A provides the most comprehensive and unbiased approach to sentiment analysis in this scenario.","upvote_count":"1","comments":[{"poster":"pico","upvote_count":"1","timestamp":"1715861340.0","comments":[{"poster":"tavva_prudhvi","upvote_count":"1","timestamp":"1716394500.0","content":"See, both have their own advantages & dissadvantages, but we should choose the option which is more relevant","comment_id":"1077651"}],"content":"Option A could be influenced by the syntactical nuances and structures used in different cultures","comment_id":"1072472"}],"timestamp":"1706731680.0","poster":"tavva_prudhvi","comment_id":"968376"},{"upvote_count":"1","comments":[{"timestamp":"1706344800.0","content":"Answer B*","poster":"ciro_li","comment_id":"964443","upvote_count":"1"}],"content":"Selected Answer: A\nAnswer A","comment_id":"964442","poster":"ciro_li","timestamp":"1706344800.0"},{"comment_id":"953429","poster":"erenklclar","content":"Selected Answer: C\nBy working directly with the audio data, you can account for important aspects like tone, pitch, and rhythm of speech, which might provide valuable information regarding sentiment.","upvote_count":"2","comments":[{"timestamp":"1705857300.0","poster":"[Removed]","upvote_count":"1","content":"But the audio will be affected by gender, age, and cultural differences of the customers. When you convert the recording to text, this problem is less pronounced. So the answer cannot be C","comment_id":"958632"}],"timestamp":"1705426380.0"},{"content":"vote for A\nbetween words and sentences:\nAge and gender considerations: Sentences provide a broader view of sentiment that can help mitigate age and gender biases. Analyzing at the sentence level allows you to observe sentiment patterns across various demographic groups, which can help identify any biases that may arise. By considering the overall sentiment expressed in sentences, you can minimize the impact of individual words that might carry specific biases.","upvote_count":"1","poster":"NickHapton","timestamp":"1704733860.0","comment_id":"946581"},{"content":"Selected Answer: C\nThere is the possibility for a more sophisticated architecture for an audio processing pipeline, and the “not impact any stage of the model development pipeline and results” somewhat calls for a more holistic answer: https://cloud.google.com/architecture/categorizing-audio-files-using-ml#converting_speech_to_text. Plus, it adds “voice emotion information, related to an audio recording, indicating that a vocal utterance of a speaker is spoken with negative or positive emotion”: https://patents.google.com/patent/US20140220526A1/en.","comments":[{"comment_id":"892430","content":"A reason why one could exclude “Convert the speech to text” altogether [Options A, B & D] could be, for instance, because “speech transcription may have higher error rates for African Americans than White Americans [3]”: https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html.","comments":[{"content":"“Cloud NL API can perform syntactic analysis directly on a file located in Cloud Storage.” “Syntactic Analysis [Option D] breaks up the given text into a series of sentences [Option A] and tokens (generally, words [Option B]) and provides linguistic information about those tokens”: https://cloud.google.com/natural-language/docs/analyzing-syntax. \nIt “can be used to identify the parts of speech, determine the structure of a sentence, and determine the meaning of words in context”: https://ts2.space/en/a-comprehensive-guide-to-google-cloud-natural-language-apis-syntax-analysis/.","poster":"M25","comment_id":"892431","timestamp":"1699475280.0","upvote_count":"1"}],"poster":"M25","timestamp":"1699475220.0","upvote_count":"1"}],"poster":"M25","comment_id":"892429","timestamp":"1699475220.0","upvote_count":"2"},{"content":"Selected Answer: B\nCan anyone explain how to choose between words and sentences? I feel like the model could pick up bias from both","poster":"[Removed]","comment_id":"876677","timestamp":"1697902740.0","upvote_count":"1"},{"content":"Selected Answer: B\nI agree with qaz09. To avoid demographical variables influence model shoud be built on the words.","timestamp":"1697548320.0","upvote_count":"2","comment_id":"872714","poster":"formazioneQI"},{"upvote_count":"1","timestamp":"1694084100.0","content":"Selected Answer: A\nAnswer A","poster":"TNT87","comment_id":"831909"},{"comment_id":"804108","upvote_count":"3","content":"Selected Answer: B\nFor \"ensuring that the gender, age, and cultural differences of the customers who called the contact center do not impact any stage of the model development pipeline and results\" I think the model should be built on the words rather than sentences","poster":"qaz09","timestamp":"1691647440.0"},{"timestamp":"1688384400.0","comment_id":"764694","content":"Selected Answer: A\nA makes sense, to me.","upvote_count":"1","poster":"ares81"},{"content":"Selected Answer: A\nA\nConvert the speech to text and extract sentiments based on the sentences.","comment_id":"752061","poster":"hiromi","timestamp":"1687332000.0","upvote_count":"1"},{"upvote_count":"1","comments":[{"poster":"Yajnas_arpohc","timestamp":"1695020460.0","comment_id":"842608","content":"Based only on words might be misleading; at a minimum need to go w sentences","upvote_count":"2"}],"poster":"mil_spyro","timestamp":"1686678480.0","content":"Selected Answer: D\nvote D","comment_id":"744416"}],"answer":"A","answers_community":["A (58%)","B (25%)","Other"],"url":"https://www.examtopics.com/discussions/google/view/91491-exam-professional-machine-learning-engineer-topic-1-question/","timestamp":"2022-12-13 20:48:00","isMC":true,"answer_description":"","question_text":"You are an ML engineer in the contact center of a large enterprise. You need to build a sentiment analysis tool that predicts customer sentiment from recorded phone conversations. You need to identify the best approach to building a model while ensuring that the gender, age, and cultural differences of the customers who called the contact center do not impact any stage of the model development pipeline and results. What should you do?","choices":{"D":"Convert the speech to text and extract sentiment using syntactical analysis.","B":"Convert the speech to text and build a model based on the words.","C":"Extract sentiment directly from the voice recordings.","A":"Convert the speech to text and extract sentiments based on the sentences."},"topic":"1","answer_images":[],"question_images":[],"question_id":16,"unix_timestamp":1670960880,"exam_id":13,"answer_ET":"A"},{"id":"zxElvySV4SlTzgr3Y8OL","url":"https://www.examtopics.com/discussions/google/view/91957-exam-professional-machine-learning-engineer-topic-1-question/","answer_images":[],"question_id":17,"unix_timestamp":1671326820,"topic":"1","answer_description":"","question_images":[],"timestamp":"2022-12-18 02:27:00","isMC":true,"discussion":[{"comment_id":"751406","content":"Selected Answer: A\nPreviously Google pattern was Pub/Sub -> Dataflow -> BQ\nbut now it looks as there is new Pub/Sub -> BQ\nhttps://cloud.google.com/blog/products/data-analytics/pub-sub-launches-direct-path-to-bigquery-for-streaming-analytics","poster":"pshemol","upvote_count":"20","timestamp":"1671565200.0","comments":[{"content":"New pub sub??? heheheh","comment_id":"831904","poster":"TNT87","upvote_count":"1","timestamp":"1678193460.0"},{"timestamp":"1678193640.0","upvote_count":"2","poster":"TNT87","comment_id":"831907","content":"https://cloud.google.com/blog/products/data-analytics/pub-sub-launches-direct-path-to-bigquery-for-streaming-analytics\nYou should have said pub sub has been upgrade to directly stream to bigquery templates...not new pub sub"}]},{"upvote_count":"1","comment_id":"1373350","poster":"HaroonRaizada01","timestamp":"1741541460.0","content":"Selected Answer: D\nThis approach provides a scalable, flexible, and efficient solution for real-time data ingestion and transformation, ensuring that user activity data is seamlessly integrated into BigQuery for analysis and explanations.\n\nBigQuery can perform powerful data transformations using SQL. However, there are key differences in how BigQuery and Dataflow handle data, especially in the context of real-time data ingestion."},{"upvote_count":"1","timestamp":"1740609420.0","comment_id":"1362341","poster":"desertlotus1211","comments":[{"comment_id":"1362342","poster":"desertlotus1211","comments":[{"content":"Ahh it's D, the our team will use BigQuery for data analysis, transformation, and experimentation... this is key!","poster":"desertlotus1211","comment_id":"1362343","upvote_count":"1","timestamp":"1740609720.0"}],"upvote_count":"1","content":"Now I'm torn between A or D... since you can use BQ subscriptions...","timestamp":"1740609660.0"}],"content":"Selected Answer: D\nPub/Sub and Dataflow are needed for real-time ingestion. Pub/Sub cannot do it alone"},{"timestamp":"1733441100.0","comment_id":"1322563","content":"Selected Answer: D\nPub/Sub is used for message ingestion and it cannot directly load data into BigQuery. Pub/Sub only delivers messages, but we will need Dataflow or another processing tool to transform and load the data into BigQuery. Hence it is D in my opinion.","upvote_count":"1","poster":"rajshiv"},{"comment_id":"1321532","upvote_count":"1","content":"Selected Answer: D\nSince Data transformation is needed.","timestamp":"1733256480.0","poster":"Pau1234"},{"content":"Selected Answer: A\nThe question specifies that transformation occurs in Bigquery. This means the new direct pub/sub to bigquery streaming path is correct.","poster":"baimus","timestamp":"1725894360.0","upvote_count":"2","comment_id":"1281011"},{"content":"Selected Answer: D\nNeed PubSub and Dataflow both for this","comment_id":"1239307","poster":"Prakzz","timestamp":"1719669060.0","upvote_count":"1"},{"poster":"ludovikush","content":"Selected Answer: D\nWerner123 i agree","comment_id":"1163372","timestamp":"1709286360.0","upvote_count":"1"},{"upvote_count":"2","comment_id":"1162527","timestamp":"1709209800.0","content":"Selected Answer: D\nUser data would most likely include PII, for that case it is still recommended to use Dataflow since you need to remove/anonymise sensitive data.","poster":"Werner123"},{"poster":"pico","content":"I would have added \"with / without data transformation\" to the question to choose the right answer between A or D","timestamp":"1700144220.0","upvote_count":"1","comment_id":"1072475"},{"content":"Selected Answer: A\nI had my doubts between A and D.\nBut since the transformation will occur in bigquery I think Pubsub suffices.","timestamp":"1694360700.0","poster":"andresvelasco","upvote_count":"3","comment_id":"1004097"},{"content":"Selected Answer: D\nAgree with TNT87. From the same link: “For Pub/Sub messages where advanced preload transformations or data processing before landing data in BigQuery (such as masking PII) is necessary, we still recommend going through Dataflow.” It’s “analyze user activity data”, not merely streaming IoT into BigQuery so that concerns like privacy are per se n/a. One can deal with PII after landing in BigQuery as well, but apparently that’s not what they recommend.","poster":"M25","timestamp":"1683606960.0","upvote_count":"3","comment_id":"892663"},{"comment_id":"855078","upvote_count":"2","content":"Selected Answer: D\nPub/Sub -> DataFlow -> BigQuery","poster":"PHD_CHENG","timestamp":"1680132960.0"},{"comment_id":"831908","poster":"TNT87","timestamp":"1678193700.0","content":"Starting today, you no longer have to write or run your own pipelines for data ingestion from Pub/Sub into BigQuery. We are introducing a new type of Pub/Sub subscription called a “BigQuery subscription” that writes directly from Cloud Pub/Sub to BigQuery. This new extract, load, and transform (ELT) path will be able to simplify your event-driven architecture. For Pub/Sub messages where advanced preload transformations or data processing before landing data in BigQuery (such as masking PII) is necessary, we still recommend going through Dataflow","upvote_count":"1"},{"poster":"hiromi","content":"Selected Answer: A\nA\nagree with pshemol","timestamp":"1671615420.0","comment_id":"752080","upvote_count":"3"},{"poster":"mymy9418","upvote_count":"2","comments":[{"poster":"mil_spyro","content":"transformation will be handled in BQ hence I think A","upvote_count":"6","timestamp":"1671523800.0","comments":[{"poster":"mymy9418","upvote_count":"1","comment_id":"761012","timestamp":"1672318620.0","content":"agree."}],"comment_id":"750637"}],"content":"Selected Answer: D\nneed dataflow","comment_id":"748531","timestamp":"1671326820.0"}],"answer":"A","answers_community":["A (62%)","D (38%)"],"answer_ET":"A","exam_id":13,"choices":{"A":"Configure Pub/Sub to stream the data into BigQuery.","C":"Run a Dataflow streaming job to ingest the data into BigQuery.","B":"Run an Apache Spark streaming job on Dataproc to ingest the data into BigQuery.","D":"Configure Pub/Sub and a Dataflow streaming job to ingest the data into BigQuery,"},"question_text":"You need to analyze user activity data from your company’s mobile applications. Your team will use BigQuery for data analysis, transformation, and experimentation with ML algorithms. You need to ensure real-time ingestion of the user activity data into BigQuery. What should you do?"},{"id":"u3naRS0d1HFtRwWNgMHL","question_images":[],"exam_id":13,"url":"https://www.examtopics.com/discussions/google/view/92218-exam-professional-machine-learning-engineer-topic-1-question/","choices":{"D":"Rate of return as measured by additional revenue generated minus the cost of developing a new model","B":"Precision and recall of assigning players to teams based on their predicted versus actual ability","C":"User engagement as measured by the number of battles played daily per user","A":"Average time players wait before being assigned to a team"},"timestamp":"2022-12-20 20:50:00","question_id":18,"topic":"1","discussion":[{"content":"Selected Answer: C\nThe game is more enjoyable - the better and \"business metrics\" points me to user engagement as best metric","comment_id":"751414","upvote_count":"10","poster":"pshemol","timestamp":"1671565800.0"},{"content":"Selected Answer: B\nI do not agree with C as User engagement would not help directly to evaluate whether players are enjoying more balanced matches due to the model’s performance. As the goal of the model is to assign players to teams based on their skill level so that teams are balanced and enjoyable for all participants I will go with B as the more appropriate answer.","comment_id":"1322564","timestamp":"1733441460.0","poster":"rajshiv","upvote_count":"2"},{"content":"Selected Answer: C\nThis question doesn't specify how \"additional revenue\" is measured. Most businesses I've worked for would love \"D\" for all our models instead of anything else. That being said, C is the only measurable business metric there.","comment_id":"1281018","timestamp":"1725894720.0","poster":"baimus","upvote_count":"1"},{"comments":[{"content":"A. Average time players wait before being assigned to a team: While faster matchmaking is desirable, it shouldn't come at the expense of balanced teams. If wait times are very low but battles are imbalanced due to poor matchmaking, user engagement might suffer.\nB. Precision and recall of assigning players to skill level: These metrics are valuable for evaluating the model's ability to predict skill accurately. However, they don't directly measure the impact on user experience and enjoyment.\nD. Rate of return: This metric focuses on financial gain, which might not be the primary objective in this case. Prioritizing balanced teams for a more enjoyable experience can indirectly lead to higher user retention and potentially more revenue in the long run.","comment_id":"1202334","timestamp":"1714095540.0","poster":"fitri001","upvote_count":"2"}],"timestamp":"1714095540.0","poster":"fitri001","comment_id":"1202333","upvote_count":"1","content":"Selected Answer: C\nfocusing on user engagement through the number of battles played daily provides a clearer indication of whether the model successfully creates balanced and enjoyable matches, which is the core objective. If players find battles more engaging due to fairer competition, they're more likely to keep playing. This can then translate to long-term benefits like increased retention and potential monetization opportunities."},{"poster":"edoo","content":"Selected Answer: C\nTempted by B but \"user engagement\" is the keyword.","comments":[{"timestamp":"1709718660.0","comment_id":"1167054","poster":"edoo","content":"I meant \"business metric\".","upvote_count":"2"}],"comment_id":"1167053","timestamp":"1709718660.0","upvote_count":"2"},{"content":"Selected Answer: C\nLooking for \"business metrics to track,\" I think C could be the most important metric. Although, option B is also a good choice.","poster":"guilhermebutzke","timestamp":"1706040600.0","upvote_count":"2","comment_id":"1130023"},{"content":"Selected Answer: C\nC: Business metric i.e. outcome driven","poster":"MCorsetti","timestamp":"1697971440.0","upvote_count":"1","comment_id":"1050455"},{"upvote_count":"3","poster":"tavva_prudhvi","comment_id":"968370","timestamp":"1690825740.0","content":"\"Business metrics\" does suggest that the question is looking for metrics that are relevant to the business goals of the company, rather than purely technical metrics. In that case, C.could be a good choice. User engagement is an important metric for any online service, as it reflects how much users are enjoying and using the product. In the context of a multiplayer game, the number of battles played daily per user can indicate how well the model is doing in creating balanced teams that are enjoyable to play against. If the model is successful in creating balanced teams, then users are likely to play more games, which would increase user engagement.\n\nTherefore, C could be a suitable choice to track the performance of the model."},{"timestamp":"1688585760.0","poster":"Nxtgen","comment_id":"944084","upvote_count":"2","content":"Selected Answer: C\nThe focus is to obtain a model that assigns players to teams with players with similar level of skill (or average team 1 skill == average team 2 skill)\n\nA: A fast queue assignment may not focus on pearing players with the same levels of skills. A random assignment would work.\n\nB: This would be an option but is more difficult to measure than C, we don’t know If we have a measure of skill level. Also, for new players this metric would not be available at the beginning. I think “There are many new players every day.” is a key point important to discard answer B.\n\nC: Players play more games daily ← players enjoy the game more frequently and the other way round should also apply. Easy to measure also for new players.\n\nD:This focus on costs and revenue not on players matchmaking.\n\nI would go with C."},{"comment_id":"897663","upvote_count":"3","content":"Selected Answer: C\nC because \"user engagement\" is a business metric https://support.google.com/analytics/answer/11109416?hl=en","timestamp":"1684078080.0","poster":"Antmal"},{"upvote_count":"1","poster":"M25","content":"Selected Answer: C\nWent with C","comment_id":"892802","timestamp":"1683610140.0"},{"upvote_count":"4","comment_id":"876696","timestamp":"1682094660.0","content":"Selected Answer: B\nThis is B, as it directly relates to our model's ability to predict player ability. There are many factors beyond our model which will impact user engagement (e.g. whether the game is actually enjoyable) so it's not a good measurement of the model performance","poster":"[Removed]"},{"timestamp":"1681709520.0","comment_id":"872348","content":"Selected Answer: C\nAnswer C","poster":"TNT87","upvote_count":"1"},{"comment_id":"855238","content":"Selected Answer: C\nThe question is asking about \"available players\". Therefore, the business metric is the user engagement.","timestamp":"1680146460.0","poster":"PHD_CHENG","upvote_count":"4"},{"comment_id":"853266","content":"Selected Answer: C\nAsks for >business metric<, and problem states \"user research indicates that the game is more enjoyable when battles have players with similar skill levels.\", which means more battles per user if your model is performing well.","timestamp":"1680008520.0","poster":"JamesDoe","upvote_count":"1"},{"poster":"dfdrin","content":"Selected Answer: C\nIt's C. The question specifically asks for a business metric. Precision and recall are not business metrics, but user engagement is","upvote_count":"4","timestamp":"1678810440.0","comment_id":"839024"},{"timestamp":"1678803900.0","poster":"guilhermebutzke","upvote_count":"3","comment_id":"838931","content":"Selected Answer: B\nThe template uses the 'ability' to create teams. For this, we can conclude that the system measures the player's skill. So, nothing better than comparing the predict ability with the actual ability to understand the performance of the model."},{"timestamp":"1678193340.0","upvote_count":"4","poster":"TNT87","comments":[{"comment_id":"872347","upvote_count":"2","content":"Answer C , user engagement","timestamp":"1681709460.0","poster":"TNT87"}],"content":"Selected Answer: B\nA. Average time players wait before being assigned to a team\nB. Precision and recall of assigning players to teams based on their predicted versus actual ability\n\nThese two metrics are the most relevant for measuring the performance of the model in assigning players to teams based on skill level. The average wait time can indicate whether the model is making efficient and quick team assignments, while precision and recall can measure the accuracy of the model's predictions. It's important to balance precision and recall since assigning players to a team with a large difference in skill level could have a negative impact on the players' gaming experience.\n\nC and D are also important metrics to track, but they may not be as directly tied to the performance of the team assignment model. User engagement can indicate the success of the overall gaming experience, but it can be influenced by other factors beyond team assignments. The rate of return is also an important metric, but it may not be a direct measure of the success of the team assignment model.","comment_id":"831899"},{"upvote_count":"3","comment_id":"820213","content":"Selected Answer: B\nTo measure the performance of a model that assigns available players to teams in real time, the business metrics that should be tracked should reflect the ability of the model to effectively balance the skill levels of players in battles. Therefore, the best answer is B, precision and recall of assigning players to teams based on their predicted versus actual ability.","poster":"shankalman717","timestamp":"1677224160.0"},{"poster":"rayban3981","content":"Selected Answer: B\nOption B since it incorporates assessment of skill in the form of ability","comment_id":"798020","timestamp":"1675520220.0","upvote_count":"3"},{"comment_id":"764678","content":"Selected Answer: C\nIt should be C.","timestamp":"1672752420.0","poster":"ares81","upvote_count":"2"},{"poster":"Nayak8","timestamp":"1672207020.0","content":"Selected Answer: B\nVersus actual ability will help to match the skill level","comment_id":"759420","upvote_count":"3"},{"comment_id":"752092","content":"Selected Answer: C\nI think it's C","timestamp":"1671615840.0","poster":"hiromi","upvote_count":"2"}],"answer_ET":"C","question_text":"You work for a gaming company that manages a popular online multiplayer game where teams with 6 players play against each other in 5-minute battles. There are many new players every day. You need to build a model that automatically assigns available players to teams in real time. User research indicates that the game is more enjoyable when battles have players with similar skill levels. Which business metrics should you track to measure your model’s performance?","isMC":true,"answer_description":"","answer":"C","answer_images":[],"unix_timestamp":1671565800,"answers_community":["C (63%)","B (37%)"]},{"id":"k2xlryNZ5mHY9bPXyLS5","question_text":"You are building an ML model to predict trends in the stock market based on a wide range of factors. While exploring the data, you notice that some features have a large range. You want to ensure that the features with the largest magnitude don’t overfit the model. What should you do?","answers_community":["D (61%)","A (32%)","7%"],"question_id":19,"question_images":[],"answer_ET":"D","timestamp":"2022-12-18 02:35:00","answer":"D","url":"https://www.examtopics.com/discussions/google/view/91959-exam-professional-machine-learning-engineer-topic-1-question/","unix_timestamp":1671327300,"exam_id":13,"answer_images":[],"topic":"1","isMC":true,"choices":{"D":"Normalize the data by scaling it to have values between 0 and 1.","A":"Standardize the data by transforming it with a logarithmic function.","B":"Apply a principal component analysis (PCA) to minimize the effect of any particular feature.","C":"Use a binning strategy to replace the magnitude of each feature with the appropriate bin number."},"answer_description":"","discussion":[{"upvote_count":"5","timestamp":"1729906920.0","comments":[{"timestamp":"1729906980.0","comment_id":"1202337","content":"A. Standardize the data by transforming it with a logarithmic function: While logarithmic transformation can help compress the range of skewed features, it might not be suitable for all features, and it can introduce non-linear relationships that might not be ideal for all machine learning algorithms.\n\nB. Apply a principal component analysis (PCA) to minimize the effect of any particular feature: PCA is a dimensionality reduction technique that can be useful, but its primary function is to reduce the number of features, not specifically address differences in feature scales.\n\nC. Use a binning strategy to replace the magnitude of each feature with the appropriate bin number: Binning can introduce information loss and might not capture the nuances within each bin, potentially affecting the model's accuracy.","upvote_count":"1","poster":"fitri001"}],"comment_id":"1202336","poster":"fitri001","content":"Selected Answer: D\nD. Normalize the data by scaling it to have values between 0 and 1 (Min-Max scaling): This technique ensures all features contribute proportionally to the model's learning process.\n\npen_spark\nexpand_more It prevents features with a larger magnitude from dominating the model and reduces the risk of overfitting.expand_more"},{"content":"Selected Answer: D\nagree with pico","poster":"gscharly","comment_id":"1199556","timestamp":"1729500300.0","upvote_count":"2"},{"upvote_count":"4","content":"Selected Answer: D\nNot A because a logarithmic transformation may be appropriate for data with a skewed distribution, but it doesn't necessarily address the issue of features having different scales.","poster":"pico","timestamp":"1715862180.0","comment_id":"1072481"},{"comment_id":"1067933","poster":"Krish6488","content":"Selected Answer: D\nFeatures with a larger magnitude might still dominate after a log transformation if the range of values is significantly different from other features. Scaling is better, will go with Option D","upvote_count":"1","timestamp":"1715434860.0"},{"upvote_count":"1","content":"by abylead: Min-Max scaling is a popular technique for normalizing stock price data. Logs are commonly used in finance to normalize relative data, such as returns.https://itadviser.dev/stock-market-data-normalization-for-time-series/","comment_id":"975636","poster":"envest","timestamp":"1707401220.0"},{"upvote_count":"2","comment_id":"960557","timestamp":"1706029440.0","poster":"[Removed]","content":"Selected Answer: D\nThe correct answer is D. Min-max scaling will render all variables comparable by bringing them to a common ground.\n\nA is wrong for the following reasons:\n1. It is never mentioned that all variables are positive. If some columns have negative values, log transformation is not applicable.\n2. Log transformation of variables having small positive values (close to 0) will increase their magnitude. For example, ln(0.0001) = -9.2, which will increase this variable's effect considerably."},{"poster":"djo06","timestamp":"1705142340.0","upvote_count":"1","comment_id":"950480","content":"Selected Answer: D\nD is the right answer"},{"upvote_count":"1","content":"go for D, z-score. This question doesn't mention outlier, just large range.\nreason why not log transformation:\nlog transformation is more suitable for addressing skewed distributions and reducing the impact of outliers. It compresses the range of values, especially for features with a large dynamic range. While it can help normalize the distribution, it doesn't directly address the issue of feature magnitude overpowering the model.","comment_id":"946589","poster":"NickHapton","timestamp":"1704734520.0"},{"upvote_count":"1","comments":[{"content":"n cases where the data has significant skewness or a large number of outliers, option A (log transformation) might be more suitable. However, if the primary concern is to equalize the influence of features with different magnitudes and the data is not heavily skewed or has few outliers, option D (normalizing the data) would be more appropriate.","timestamp":"1705001100.0","comment_id":"949201","poster":"tavva_prudhvi","upvote_count":"1"}],"timestamp":"1704700320.0","poster":"SamuelTsch","content":"Selected Answer: A\nFrom my point of view, log transformation is more tolerant to outliers. Thus, went to A.","comment_id":"946200"},{"content":"Selected Answer: A\nSee https://developers.google.com/machine-learning/data-prep/transform/normalization","poster":"coolmenthol","upvote_count":"2","timestamp":"1703578380.0","comment_id":"934095"},{"poster":"Antmal","content":"Selected Answer: A\nA is a better option because Log transform data used when we want a heavily skewed feature to be transformed into a normal distribution as close as possible, because when you normalize data using Minimum Maximum scaler, It doesn't work well with many outliers and its prone to unexpected behaviours if values go out of the given range in the test set. It is a less popular alternative to scaling.","comment_id":"897639","timestamp":"1699982220.0","comments":[{"timestamp":"1705001040.0","content":"If your data is heavily skewed and has a significant number of outliers, log transformation (option A) might be a better choice. However, if your primary concern is to ensure that the features with the largest magnitudes don't overfit the model and the data does not have a significant skew or too many outliers, normalizing the data (option D) would be more appropriate.","comment_id":"949200","poster":"tavva_prudhvi","upvote_count":"1"}],"upvote_count":"2"},{"timestamp":"1699521120.0","comment_id":"892873","poster":"M25","content":"Selected Answer: D\nThe challenge is the “scale” (significant variations in magnitude and spread): https://stats.stackexchange.com/questions/462380/does-data-normalization-reduce-over-fitting-when-training-a-model,\napparently largely used anyhow: https://itadviser.dev/stock-market-data-normalization-for-time-series/.","upvote_count":"1","comments":[{"upvote_count":"1","timestamp":"1699521180.0","comment_id":"892874","content":"“(…) some features have a large range”, possible presence of outliers exclude standardization [excluding A]: https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/. \n“(…) a wide range of factors”, PCA transform the data so that it can be described with fewer dimensions / features: https://en.wikipedia.org/wiki/Principal_component_analysis, but [excluding B]: it asks to “ensure that the features with largest magnitude don’t overfit the model”.","poster":"M25"}]},{"timestamp":"1696057440.0","upvote_count":"1","poster":"niketd","content":"Selected Answer: D\nThe question doesn't talk about the skewness within each feature. It talks about normalizing the effect of features with large range. So scaling each feature within (0,1) range will solve the problem","comment_id":"855351"},{"content":"Really need more info to answer this: what does \"large range\" mean? Distribution follows a power law --> use log(). Or are they more evenly/linearly distributed --> use (0,1) scaling.","timestamp":"1695906300.0","upvote_count":"1","poster":"JamesDoe","comment_id":"853268"},{"upvote_count":"3","content":"Selected Answer: C\nI think C could be a better choice. Bucketizing the data we can fix the distribution problem by bins. \n\nin letter A, standardization by log could not be effective if the range of the data has negative and positive values. \n\nIn letter D, definitely normalization does not resolve the skew problem. Data normalization assumes that data has some normal distribution. \n\nhttps://medium.com/analytics-vidhya/data-transformation-for-numeric-features-fb16757382c0","timestamp":"1694694900.0","comment_id":"838939","poster":"guilhermebutzke"},{"comment_id":"831896","timestamp":"1694083620.0","poster":"TNT87","content":"Selected Answer: D\nD. Normalize the data by scaling it to have values between 0 and 1.\n\nStandardization and normalization are common techniques to preprocess the data to be more suitable for machine learning models. Normalization scales the data to be within a specific range (commonly between 0 and 1 or -1 and 1), which can help prevent features with large magnitudes from dominating the model. This approach is especially useful when using models that are sensitive to the magnitude of features, such as distance-based models or neural networks.","upvote_count":"1"},{"comment_id":"821000","upvote_count":"1","timestamp":"1692908580.0","poster":"FherRO","content":"Selected Answer: A\nhttps://developers.google.com/machine-learning/data-prep/transform/normalization#log-scaling"},{"content":"Selected Answer: D\nhe best approach to handle features with a large range in an ML model is to normalize the data by scaling it to have values between 0 and 1. Therefore, the correct answer is D.\n\nNormalization ensures that the features have similar scales, which is important for many machine learning algorithms. If some features have a larger magnitude than others, they can dominate the objective function and make the model unable to learn from other features correctly. By scaling all the features to a similar range, we can avoid this problem and make the objective function less sensitive to the scale of the input features.\n\nStandardizing the data by transforming it with a logarithmic function (option A) is not suitable for all types of data and may not always be effective in reducing the impact of features with a large range.","upvote_count":"1","timestamp":"1692856320.0","comment_id":"820228","poster":"shankalman717"},{"content":"Selected Answer: A\nLog scaling is helpful when a handful of your values have many points, while most other values have few points. This data distribution is known as the power law distribution. \nPlus we know the range, so I would say A.","timestamp":"1692809400.0","upvote_count":"1","poster":"ailiba","comment_id":"819578"},{"poster":"abneural","content":"Selected Answer: A\nAgree with mymy9418\nhttps://developers.google.com/machine-learning/data-prep/transform/normalization#scaling-to-a-range\n\n“Scaling to a range is a good choice when both of the following conditions are met:\n\nYou know the approximate upper and lower bounds on your data with few or no outliers.\nYour data is approximately uniformly distributed across that range.\nA good example is age. Most age values falls between 0 and 90, and every part of the range has a substantial number of people.”","comment_id":"810709","upvote_count":"1","timestamp":"1692186420.0"},{"timestamp":"1689672840.0","poster":"John_Pongthorn","comment_id":"779942","content":"Selected Answer: D\nThere is no way to standardize data with log. llogarithmic function is used for another purpose.\n\nhttps://developers.google.com/machine-learning/data-prep/transform/normalization#log-scaling\n\nhttps://developers.google.com/machine-learning/data-prep/transform/normalization#scaling-to-a-range\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\nD : is MinMax scaler.","upvote_count":"1"},{"comment_id":"764674","poster":"ares81","content":"Selected Answer: D\nNormalization is preferred over standardization when our data doesn’t follow a normal distribution. Hence D.","upvote_count":"1","timestamp":"1688383500.0"},{"content":"Selected Answer: D\nNormalization should work","upvote_count":"3","poster":"Nayak8","comment_id":"759422","timestamp":"1687924740.0"},{"poster":"Abhijat","content":"Selected Answer: D\nhttps://developers.google.com/machine-learning/data-prep/transform/normalization#scaling-to-a-range","timestamp":"1687350600.0","upvote_count":"1","comments":[{"comment_id":"755984","content":"from your link:\n\"Log scaling computes the log of your values to compress a wide range to a narrow range.\" -> hence A","timestamp":"1687716480.0","poster":"hiromi","upvote_count":"2"}],"comment_id":"752423"},{"comment_id":"752123","content":"Selected Answer: A\nA\nagree with mymy9418","timestamp":"1687335120.0","poster":"hiromi","upvote_count":"3"},{"comment_id":"752109","poster":"MithunDesai","timestamp":"1687334160.0","content":"I think D is right","upvote_count":"2"},{"upvote_count":"2","timestamp":"1687044900.0","content":"Selected Answer: A\nhttps://developers.google.com/machine-learning/data-prep/transform/normalization#scaling-to-a-range","comment_id":"748535","poster":"mymy9418"}]},{"id":"GnbvB14EVVUVt3UXJmWO","choices":{"B":"A cluster with 2 a2-megagpu-16g machines, each with 16 NVIDIA Tesla A100 GPUs (640 GB GPU memory in total), 96 vCPUs, and 1.4 TB RAM","A":"A cluster with 2 n1-highcpu-64 machines, each with 8 NVIDIA Tesla V100 GPUs (128 GB GPU memory in total), and a n1-highcpu-64 machine with 64 vCPUs and 58 GB RAM","D":"A cluster with 4 n1-highcpu-96 machines, each with 96 vCPUs and 86 GB RAM","C":"A cluster with an n1-highcpu-64 machine with a v2-8 TPU and 64 GB RAM"},"unix_timestamp":1671618300,"answers_community":["D (67%)","B (33%)"],"question_text":"You work for a biotech startup that is experimenting with deep learning ML models based on properties of biological organisms. Your team frequently works on early-stage experiments with new architectures of ML models, and writes custom TensorFlow ops in C++. You train your models on large datasets and large batch sizes. Your typical batch size has 1024 examples, and each example is about 1 MB in size. The average size of a network with all weights and embeddings is 20 GB. What hardware should you choose for your models?","timestamp":"2022-12-21 11:25:00","answer_description":"","discussion":[{"comment_id":"902835","content":"Selected Answer: D\nD: use CPU when models that contain many custom TensorFlow operations written in C++\nhttps://cloud.google.com/tpu/docs/intro-to-tpu#cpus","timestamp":"1684630620.0","poster":"aw_49","upvote_count":"7"},{"upvote_count":"2","poster":"rajshiv","timestamp":"1733441820.0","comments":[{"poster":"bc3f222","upvote_count":"1","comment_id":"1399160","content":"TensorFlow operations written in C++, so D","timestamp":"1742111280.0"}],"content":"Selected Answer: B\nI do not agree that D is correct. The option provides significant CPU resources, but it lacks GPU acceleration, which is necessary for efficiently training large deep learning models with large datasets. While CPUs can handle certain operations, they are generally much slower for training deep learning models compared to GPUs or TPUs. Choice B provides the best hardware for deep learning workload, offering 16 NVIDIA A100 GPUs with 640 GB of GPU memory, along with sufficient CPU and RAM resources to handle large datasets and complex model architectures.","comment_id":"1322566"},{"timestamp":"1709807700.0","upvote_count":"2","poster":"edoo","content":"Selected Answer: D\nB looks like unleashing a rocket launcher to swat a fly (\"early-stage experiments\"). D is enough (c++).","comment_id":"1167905"},{"comment_id":"967371","upvote_count":"1","timestamp":"1690739640.0","content":"While it is true that using CPUs can be more efficient when dealing with custom TensorFlow operations written in C++, it is important to consider the specific requirements of your models. In his case, we mentioned large batch sizes (1024 examples), large example sizes (1 MB each), and large network sizes (20 GB). 4 n1-highcpu-96 machines, each with 96 vCPUs and 86 GB RAM. While this configuration would provide a high number of vCPUs for custom TensorFlow operations, it lacks the GPU memory and overall RAM necessary to handle the large batch sizes and network sizes of your models.","poster":"tavva_prudhvi"},{"timestamp":"1690085880.0","poster":"ciro_li","comments":[{"timestamp":"1712986680.0","poster":"pinimichele01","content":"so D, not B...","comment_id":"1194664","upvote_count":"1"}],"content":"B: https://cloud.google.com/tpu/docs/intro-to-tpu#cpus","upvote_count":"2","comment_id":"960059"},{"comment_id":"918341","upvote_count":"3","timestamp":"1686233340.0","content":"Selected Answer: D\nD: use CPU when models that contain many custom TensorFlow operations written in C++\nhttps://cloud.google.com/tpu/docs/intro-to-tpu#cpus","poster":"Voyager2"},{"upvote_count":"3","content":"Wouldn't all PC's work here? I could do this model on my own home PC just fine.","timestamp":"1685079540.0","comment_id":"907084","poster":"LoveExams"},{"comment_id":"892895","poster":"M25","timestamp":"1683617760.0","upvote_count":"2","content":"Selected Answer: D\n“writes custom TensorFlow ops in C++” -> use CPUs when “Models that contain many custom TensorFlow operations written in C++”: https://cloud.google.com/tpu/docs/intro-to-tpu#when_to_use_tpus"},{"upvote_count":"4","timestamp":"1681558740.0","comment_id":"870885","content":"Selected Answer: B\nThe best hardware for your models would be a cluster with 2 a2-megagpu-16g machines, each with 16 NVIDIA Tesla A100 GPUs (640 GB GPU memory in total), 96 vCPUs, and 1.4 TB RAM.\n\nThis hardware will give you the following benefits:\n\nHigh GPU memory: Each A100 GPU has 40 GB of memory, which is more than enough to store the weights and embeddings of your models.\nLarge batch sizes: With 16 GPUs per machine, you can train your models with large batch sizes, which will improve training speed.\nFast CPUs: The 96 vCPUs on each machine will provide the processing power you need to run your custom TensorFlow ops in C++.\nAdequate RAM: The 1.4 TB of RAM on each machine will ensure that your models have enough memory to train and run.\nThe other options are not as suitable for your needs. Option A has less GPU memory, which will slow down training. Option B has more GPU memory, but it is also more expensive. Option C has a TPU, which is a good option for some deep learning tasks, but it is not as well-suited for your needs as a GPU cluster. Option D has more vCPUs and RAM, but it does not have enough GPU memory to train your models.\n\nTherefore, the best hardware for your models is a cluster with 2 a2-megagpu-16g machines.","poster":"Antmal"},{"comment_id":"831895","upvote_count":"3","timestamp":"1678193100.0","content":"Selected Answer: B\nTo determine the appropriate hardware for training the models, we need to calculate the required memory and processing power based on the size of the model and the size of the input data.\n\nGiven that the batch size is 1024 and each example is 1 MB, the total size of each batch is 1024 * 1 MB = 1024 MB = 1 GB. Therefore, we need to load 1 GB of data into memory for each batch.\n\nThe total size of the network is 20 GB, which means that it can fit in the memory of most modern GPUs.","poster":"TNT87"},{"upvote_count":"1","comments":[{"content":"https://cloud.google.com/tpu/docs/tpus","upvote_count":"2","timestamp":"1674027720.0","comment_id":"779714","poster":"JeanEl"}],"timestamp":"1674027360.0","comment_id":"779704","content":"Selected Answer: D\nIt's D","poster":"JeanEl"},{"timestamp":"1671618300.0","comments":[{"timestamp":"1674042240.0","content":"GPU can apply through C++ implement,but C rule out for sure.","poster":"John_Pongthorn","upvote_count":"3","comment_id":"779949"}],"poster":"hiromi","comment_id":"752132","upvote_count":"3","content":"Selected Answer: D\nD\nCPUs are recommended for TensorFlow ops written in C++\n- https://cloud.google.com/tpu/docs/tensorflow-ops (Cloud TPU only supports Python)"}],"topic":"1","question_images":[],"isMC":true,"question_id":20,"answer_ET":"D","answer":"D","answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/92322-exam-professional-machine-learning-engineer-topic-1-question/","exam_id":13}],"exam":{"provider":"Google","numberOfQuestions":304,"name":"Professional Machine Learning Engineer","isImplemented":true,"id":13,"isBeta":false,"lastUpdated":"11 Apr 2025","isMCOnly":true},"currentPage":4},"__N_SSP":true}