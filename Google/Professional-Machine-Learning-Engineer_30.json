{"pageProps":{"questions":[{"id":"7rov66P63h24bZOJOVCY","answer_description":"","question_text":"Your organization's call center has asked you to develop a model that analyzes customer sentiments in each call. The call center receives over one million calls daily, and data is stored in Cloud Storage. The data collected must not leave the region in which the call originated, and no Personally Identifiable Information (PII) can be stored or analyzed. The data science team has a third-party tool for visualization and access which requires a SQL ANSI-2011 compliant interface. You need to select components for data processing and for analytics. How should the data pipeline be designed?\n//IMG//","topic":"1","exam_id":13,"choices":{"A":"1= Dataflow, 2= BigQuery","C":"1 = Dataflow, 2 = Cloud SQL","D":"1 = Cloud Function, 2= Cloud SQL","B":"1 = Pub/Sub, 2= Datastore"},"isMC":true,"discussion":[{"content":"The correct answer is A","timestamp":"1638912660.0","poster":"inder0007","upvote_count":"19","comments":[{"content":"Evidence here https://github.com/GoogleCloudPlatform/dataflow-contact-center-speech-analysis","poster":"GogoG","timestamp":"1648829100.0","upvote_count":"7","comment_id":"455625"}],"comment_id":"377016"},{"upvote_count":"8","timestamp":"1638896160.0","comment_id":"376859","poster":"salsabilsf","content":"Should be A"},{"upvote_count":"1","comment_id":"1470879","poster":"coupet","timestamp":"1743777780.0","content":"Selected Answer: A\nCorrect Answer A\nDataflow allows you to create data pipelines that read from one or more sources, transform the data, and write it to a destination.\nBigQuery is designed for large-scale analytics on structured and semi-structured data"},{"content":"Selected Answer: A\nA) Dataflow & BigQuery (Analytics)","poster":"PhilipKoku","comment_id":"1225486","upvote_count":"2","timestamp":"1733495700.0"},{"timestamp":"1715777640.0","comment_id":"1071563","content":"Selected Answer: A\nA - because it has BigQuery.\nAlmost never would you see an answer that prefers CloudSQL over BQ","poster":"Sum_Sum","upvote_count":"3"},{"upvote_count":"2","comment_id":"892700","content":"Selected Answer: A\nWent with A","poster":"M25","timestamp":"1699513200.0"},{"poster":"MithunDesai","upvote_count":"1","content":"Selected Answer: A\ncorrect answer is A","comment_id":"750512","timestamp":"1687230180.0"},{"timestamp":"1686128820.0","poster":"Moulichintakunta","upvote_count":"1","comment_id":"737759","content":"Selected Answer: A\nwe need a dataflow to process data from cloud storage and data is unstructured and if we want to perform analysis on unstructured with SQL interface BIgQuery is the only option"},{"timestamp":"1684914120.0","upvote_count":"1","poster":"EFIGO","content":"Selected Answer: A\nYou need to do analytics, so the answer needs to contain BigQuery and only option A does.\nMoreover, BigQuery is fine with SQL and Dataflow is the right tool for the processing pipline.","comment_id":"725705"},{"upvote_count":"1","timestamp":"1676472660.0","content":"Selected Answer: A\nCorrect answer is \"A\"","comment_id":"647199","poster":"GCP72"},{"comments":[{"content":"Sorry incorrect - Dataflow can call external API so stand corrected . Answer : A","upvote_count":"2","comment_id":"610370","timestamp":"1669944720.0","poster":"SUNWS7"}],"content":"D - to call API you need Cloud Functions. Dataflow would be for ETL","upvote_count":"2","timestamp":"1669944300.0","comment_id":"610369","poster":"SUNWS7"},{"content":"Selected Answer: A\nDataflow & BigQuery","comment_id":"575240","poster":"SUNWS7","timestamp":"1664128320.0","upvote_count":"2"},{"comment_id":"482015","poster":"skipper_com","content":"A, https://cloud.google.com/architecture/architecture-for-mlops-using-tfx-kubeflow-pipelines-and-cloud-build Fig.6","timestamp":"1652979540.0","upvote_count":"1"},{"poster":"mousseUwU","timestamp":"1650372060.0","content":"A is correct\nDataflow - Unified stream and batch data processing that's serverless, fast, and cost-effective\nBigQuery - Good for analytics and dashboards","upvote_count":"3","comment_id":"464657"},{"content":"BQ is SQL ANSI-2011 compliant","timestamp":"1648372260.0","comment_id":"452290","poster":"pddddd","upvote_count":"1"},{"timestamp":"1646781840.0","content":"A or C. Not sure how many third-party tool supports BigQuery. If not, then the answer is C.","comment_id":"441654","upvote_count":"2","poster":"Danny2021","comments":[{"upvote_count":"1","poster":"David_ml","content":"wrong. cloud sql is not for analytics.","timestamp":"1667854560.0","comment_id":"598261"}]},{"poster":"Jijiji","comment_id":"431161","upvote_count":"3","timestamp":"1645774620.0","content":"it's def A"}],"answer_ET":"A","url":"https://www.examtopics.com/discussions/google/view/54825-exam-professional-machine-learning-engineer-topic-1-question/","timestamp":"2021-06-07 16:56:00","question_images":["https://www.examtopics.com/assets/media/exam-media/03841/0001400001.png"],"answer_images":[],"answers_community":["A (100%)"],"unix_timestamp":1623077760,"question_id":146,"answer":"A"},{"id":"VB9SZABs3mmmTTrhpfNl","question_id":147,"answer":"D","question_images":[],"topic":"1","question_text":"You are training models in Vertex AI by using data that spans across multiple Google Cloud projects. You need to find, track, and compare the performance of the different versions of your models. Which Google Cloud services should you include in your ML workflow?","answer_images":[],"timestamp":"2024-01-17 14:57:00","choices":{"A":"Dataplex, Vertex AI Feature Store, and Vertex AI TensorBoard","B":"Vertex AI Pipelines, Vertex AI Feature Store, and Vertex AI Experiments","C":"Dataplex, Vertex AI Experiments, and Vertex AI ML Metadata","D":"Vertex AI Pipelines, Vertex AI Experiments, and Vertex AI Metadata"},"answer_description":"","discussion":[{"timestamp":"1713399720.0","comments":[{"content":"Vertex AI Pipelines (Optional): While optional, pipelines can automate your training workflow, including data access from BigQuery tables in different projects. It helps orchestrate the training process across projects.\nVertex AI Experiments: This service is crucial for tracking and comparing the performance of different model versions. It allows you to:\nRun multiple training experiments with different configurations.\nTrack experiment metrics like accuracy, precision, recall, etc.\nCompare the performance of different model versions trained in various projects.\nVertex AI Metadata: This service provides a centralized view of your ML workflow, including model lineage and versioning. It's particularly helpful in your scenario because:\nIt tracks the origin and relationships between models, including the specific data used for training, regardless of the project.\nYou can see how different model versions (potentially trained across projects) relate to each other and the data they were trained on.","upvote_count":"2","poster":"fitri001","comment_id":"1197599","timestamp":"1713399780.0"}],"upvote_count":"5","comment_id":"1197597","poster":"fitri001","content":"Selected Answer: D\nWhy not the others?\nA. Dataplex & Vertex AI Feature Store: While Dataplex can manage data across projects, it's not directly tied to model versioning and comparison. Feature Store focuses on feature engineering, not model version management.\nB. Vertex AI Feature Store & Vertex AI TensorBoard: Similar to option A, Feature Store isn't directly involved in model version tracking, and TensorBoard is primarily for visualizing training data and metrics, not model version comparison across projects.\nC. Dataplex & Vertex AI ML Metadata: Dataplex, as mentioned earlier, doesn't directly address model version comparison. While ML Metadata tracks lineage, it might not have the experiment management features of Vertex AI Experiments."},{"upvote_count":"1","content":"Selected Answer: C\nAns : C\n\n1. Dataplex : https://cloud.google.com/vertex-ai/docs/model-registry/introduction#search_and_discover_models_usings_service\n2. Vertex AI Experiments: This service is crucial for tracking and comparing the performance of different model versions. It allows you to:\nRun multiple training experiments with different configurations.\nTrack experiment metrics like accuracy, precision, recall, etc.\nCompare the performance of different model versions trained in various projects.\n3. Vertex AI Metadata: This service provides a centralized view of your ML workflow, including model lineage and versioning. It's particularly helpful in your scenario because:","comment_id":"1320524","timestamp":"1733049360.0","poster":"Umanga"},{"poster":"Aastha_Vashist","upvote_count":"2","content":"Selected Answer: D\nwent with D","comment_id":"1182610","timestamp":"1711382100.0"},{"content":"Selected Answer: D\nI would go with option D.\n\nNo Vertex AI pipeline no orchestration. So rule out A and C.\nVertex AI Metadata is for 'spans across multiple Google Cloud projects' data used by the model.","poster":"Yan_X","comment_id":"1170106","upvote_count":"2","timestamp":"1710056220.0"},{"timestamp":"1708949400.0","content":"Why not Option D?","upvote_count":"1","poster":"Carlose2108","comment_id":"1159656"},{"comment_id":"1153629","timestamp":"1708300140.0","content":"Selected Answer: B\nMy Answer: B\nVertex AI Pipelines: to create, deploy, and manage ML pipelines, which are essential for orchestrating your ML workflow, especially when dealing with data spanning multiple projects.\nVertex AI Feature Store: It's crucial for managing feature data across different projects.\nVertex AI Experiments: track and compare the performance of different versions of your models, enabling you to experiment \n\nWhy not the other:\nDataplex: not specifically tailored for managing ML workflows or model training.\nVertex AI ML metadata: not sufficient on its own to cover all aspects of managing the ML workflow across multiple projects.\nVertex AI TensorBoard: not specifically designed for managing the end-to-end ML workflow or tracking model versions across multiple projects.","comments":[{"timestamp":"1715075580.0","content":"I feel, Vertex AI Feature Store is valuable for managing and serving features for ML models, but it doesn't address the need for tracking experiments and managing metadata, right?","comment_id":"1207810","upvote_count":"1","poster":"tavva_prudhvi"}],"upvote_count":"2","poster":"guilhermebutzke"},{"comment_id":"1135308","upvote_count":"1","timestamp":"1706561280.0","content":"Selected Answer: B\nDataplex works well with the data across projects and even on-prem, but doesn't work well with the ML related data like tracking and performance. So options A and C are considered wrong. \n\nMetadata is to store metadata. So it is not required while we consider to compare the model performance. So option D is wrong. \n\nOn the other hand Feature store brings meaningful data for comparing the Models performance based on feature data. So Option B is correct","poster":"SKDE"},{"content":"Selected Answer: C\nI go with C.\nDataplex to centralize different Google projects.\nVertex AI experiments + ML Metadata to track experiment lineage, parameter usage etc and compare models.","comment_id":"1125016","upvote_count":"3","poster":"b1a8fae","comments":[{"comment_id":"1132434","timestamp":"1706263860.0","upvote_count":"5","content":"How about Option D? the Pipeline can also do the cross project data processing.","poster":"daidai75"}],"timestamp":"1705499820.0"}],"url":"https://www.examtopics.com/discussions/google/view/131392-exam-professional-machine-learning-engineer-topic-1-question/","isMC":true,"exam_id":13,"answers_community":["D (56%)","C (25%)","B (19%)"],"unix_timestamp":1705499820,"answer_ET":"D"},{"id":"At3iiak79mRjPmKd4Pk6","discussion":[{"content":"Selected Answer: C\nEasiest to preprocess the data on BigQuery.","upvote_count":"6","comment_id":"1125032","timestamp":"1721218200.0","poster":"b1a8fae"},{"upvote_count":"2","poster":"pinimichele01","timestamp":"1728406200.0","comment_id":"1191690","content":"Selected Answer: C\nwent with C","comments":[{"upvote_count":"2","content":"Easiest to preprocess the data on BigQuery.","timestamp":"1729949340.0","poster":"pinimichele01","comment_id":"1202621"}]},{"upvote_count":"3","comment_id":"1121357","content":"Selected Answer: C\nA. Spark on Dataproc: While powerful, it incurs additional cluster setup and management costs, potentially less cost-effective for this specific use case.\nB. pandas DataFrame: Loading large datasets into memory might lead to resource constraints and performance issues, especially for large-scale preprocessing.\nD. Apache Beam on Dataflow: While scalable, it introduces extra complexity for managing a separate pipeline and storage for preprocessed data.","poster":"pikachu007","timestamp":"1720846620.0"}],"timestamp":"2024-01-13 07:57:00","question_text":"You are using Keras and TensorFlow to develop a fraud detection model. Records of customer transactions are stored in a large table in BigQuery. You need to preprocess these records in a cost-effective and efficient way before you use them to train the model. The trained model will be used to perform batch inference in BigQuery. How should you implement the preprocessing workflow?","answer_description":"","answer_ET":"C","topic":"1","answers_community":["C (100%)"],"question_images":[],"url":"https://www.examtopics.com/discussions/google/view/131052-exam-professional-machine-learning-engineer-topic-1-question/","choices":{"D":"Implement a preprocessing pipeline by using Apache Beam, and run the pipeline on Dataflow. Save the preprocessed data as CSV files in a Cloud Storage bucket.","B":"Load the data into a pandas DataFrame. Implement the preprocessing steps using pandas transformations, and train the model directly on the DataFrame.","A":"Implement a preprocessing pipeline by using Apache Spark, and run the pipeline on Dataproc. Save the preprocessed data as CSV files in a Cloud Storage bucket.","C":"Perform preprocessing in BigQuery by using SQL. Use the BigQueryClient in TensorFlow to read the data directly from BigQuery."},"question_id":148,"exam_id":13,"isMC":true,"answer":"C","answer_images":[],"unix_timestamp":1705129020},{"id":"exer7FladAUcP2CK6BKZ","discussion":[{"poster":"pinimichele01","content":"Selected Answer: A\nmillions of labeled images -> dataflow\ntfrecord faster than folder-based","timestamp":"1712595180.0","comment_id":"1191693","upvote_count":"7"},{"comment_id":"1242322","upvote_count":"1","poster":"AzureDP900","timestamp":"1720132260.0","content":"A is correct Here's why \nYou need to prepare the data before training an image classification model.\nUsing TFRecord files allows you to store your data in a format that can be efficiently read and processed by TensorFlow.\nSharding the data into multiple files allows for parallel processing and scalability.\nDataflow is a Google Cloud service that provides a scalable and reliable way to process large datasets.\nBy using Vertex AI Training with a V100 GPU, you can train your model in an efficient and cost-effective manner."},{"poster":"b1a8fae","content":"Selected Answer: A\nIdeally you want to export your data in TFRecords (most efficient image format) in Cloud Storage, and not in the instance (to improve scalability)","timestamp":"1705501080.0","comment_id":"1125036","upvote_count":"3"},{"comment_id":"1121358","content":"Selected Answer: A\nB. Folder-Based Structure: While viable, it's less efficient for large datasets compared to TFRecord files, potentially leading to slower I/O during training.\nC. Workbench Processing: Local preprocessing on a single instance can be less scalable and efficient for millions of images, potentially introducing bottlenecks.\nD. Workbench Training: While Workbench offers a Jupyter environment, Vertex AI Training is specifically designed for scalable model training, providing optimized hardware and infrastructure.","upvote_count":"2","timestamp":"1705129260.0","poster":"pikachu007"}],"timestamp":"2024-01-13 08:01:00","question_text":"You need to use TensorFlow to train an image classification model. Your dataset is located in a Cloud Storage directory and contains millions of labeled images. Before training the model, you need to prepare the data. You want the data preprocessing and model training workflow to be as efficient, scalable, and low maintenance as possible. What should you do?","answer_description":"","answers_community":["A (100%)"],"topic":"1","answer_ET":"A","question_images":[],"url":"https://www.examtopics.com/discussions/google/view/131053-exam-professional-machine-learning-engineer-topic-1-question/","choices":{"A":"1. Create a Dataflow job that creates sharded TFRecord files in a Cloud Storage directory.\n2. Reference tf.data.TFRecordDataset in the training script.\n3. Train the model by using Vertex AI Training with a V100 GPU.","C":"1. Create a Jupyter notebook that uses an nt-standard-64 V100 GPU Vertex AI Workbench instance.\n2. Write a Python script that creates sharded TFRecord files in a directory inside the instance.\n3. Reference tf.data.TFRecordDataset in the training script.\n4. Train the model by using the Workbench instance.","B":"1. Create a Dataflow job that moves the images into multiple Cloud Storage directories, where each directory is named according to the corresponding label\n2. Reference tfds.folder_dataset:ImageFolder in the training script.\n3. Train the model by using Vertex AI Training with a V100 GPU.","D":"1. Create a Jupyter notebook that uses an n1-standard-64, V100 GPU Vertex AI Workbench instance.\n2. Write a Python script that copies the images into multiple Cloud Storage directories, where each. directory is named according to the corresponding label.\n3. Reference tfds.foladr_dataset.ImageFolder in the training script.\n4. Train the model by using the Workbench instance."},"question_id":149,"exam_id":13,"isMC":true,"answer":"A","answer_images":[],"unix_timestamp":1705129260},{"id":"snJ2RUmZopVBT9R0rR46","isMC":true,"answer_description":"","unix_timestamp":1705129440,"question_text":"You are building a custom image classification model and plan to use Vertex AI Pipelines to implement the end-to-end training. Your dataset consists of images that need to be preprocessed before they can be used to train the model. The preprocessing steps include resizing the images, converting them to grayscale, and extracting features. You have already implemented some Python functions for the preprocessing tasks. Which components should you use in your pipeline?","question_images":[],"url":"https://www.examtopics.com/discussions/google/view/131054-exam-professional-machine-learning-engineer-topic-1-question/","exam_id":13,"answers_community":["B (100%)"],"question_id":150,"choices":{"C":"dsl.ParallelFor, dsl.component, and CustomTrainingJobOp","A":"DataprocSparkBatchOp and CustomTrainingJobOp","B":"DataflowPythonJobOp, WaitGcpResourcesOp, and CustomTrainingJobOp","D":"ImageDatasetImportDataOp, dsl.component, and AutoMLImageTrainingJobRunOp"},"topic":"1","discussion":[{"comment_id":"1151266","upvote_count":"5","poster":"guilhermebutzke","content":"Selected Answer: B\nMy Answer: B\n\nLooking for the options, DataflowPythonJobOp can be used for parallelizing the preprocessing tasks, which is suitable for image resizing, converting to grayscale, and extracting features. dsl.ParallelFor could be useful for parallelizing tasks but might not be the most straightforward option for image preprocessing. \n\nGenerally DataflowPythonJobOp is followed by WaitGcpResourcesOp. \n\nhttps://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/fe7d3e4b8edc137d90ec061789b879b7cc8d3854/notebooks/community/ml_ops/stage3/get_started_with_dataflow_flex_template_component.ipynb","timestamp":"1708028100.0"},{"comment_id":"1285175","poster":"Dirtie_Sinkie","timestamp":"1726573080.0","content":"Selected Answer: B\nB is definitely right, no doubt","upvote_count":"2"},{"comment_id":"1191696","upvote_count":"1","poster":"pinimichele01","timestamp":"1712595420.0","content":"Selected Answer: B\nhttps://cloud.google.com/vertex-ai/docs/pipelines/dataflow-component#dataflowpythonjobop"},{"upvote_count":"2","poster":"b1a8fae","comment_id":"1125041","timestamp":"1705501740.0","content":"Selected Answer: B\nI go with B. Custom training is surely required. Discarding A because Spark is not mentioned anywhere in the problem description. C involves Kubeflow which seems a bit overkill imo. DataflowPythonJobOp operator lets you create a Vertex AI Pipelines component that prepares data -> seems like the appropriate course of action to me. https://cloud.google.com/vertex-ai/docs/pipelines/dataflow-component#dataflowpythonjobop"},{"comment_id":"1121359","upvote_count":"1","timestamp":"1705129440.0","content":"Selected Answer: B\nA. DataprocSparkBatchOp: While capable of data processing, it's less well-suited for image-specific tasks like resizing and grayscale conversion compared to DataflowPythonJobOp.\nC. dsl.ParallelFor, dsl.component: While offering flexibility, they require more manual orchestration and potentially less efficient for image preprocessing compared to DataflowPythonJobOp.\nD. ImageDatasetImportDataOp, AutoMLImageTrainingJobRunOp: These components are designed for AutoML Image training, not directly compatible with custom preprocessing and training tasks.","poster":"pikachu007"}],"answer_ET":"B","answer":"B","answer_images":[],"timestamp":"2024-01-13 08:04:00"}],"exam":{"lastUpdated":"11 Apr 2025","provider":"Google","isImplemented":true,"id":13,"isMCOnly":true,"name":"Professional Machine Learning Engineer","isBeta":false,"numberOfQuestions":304},"currentPage":30},"__N_SSP":true}