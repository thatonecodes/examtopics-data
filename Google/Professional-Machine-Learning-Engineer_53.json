{"pageProps":{"questions":[{"id":"auxTerYSjP3odOCBKyIv","answers_community":["B (80%)","D (20%)"],"discussion":[{"poster":"esuaaaa","comments":[{"comment_id":"464093","timestamp":"1634557200.0","poster":"mousseUwU","content":"Agree with you, B is correct","upvote_count":"1"},{"poster":"q4exam","timestamp":"1631172060.0","comment_id":"441822","content":"Agree, B as it extends to new products.","upvote_count":"1"},{"content":"D could have sense considering that is mentioned the intention to use AI Platform's continuous evaluation service","comment_id":"894047","upvote_count":"2","poster":"VincenzoP84","timestamp":"1683726960.0"},{"content":"it's D for two reasons:\n- explicitly required in the question to leverage Continuous evaluation service\n- the threshod check allows to decide when perform the retrain avoiding making it for every single new data arrived.","poster":"maukaba","comment_id":"1071237","timestamp":"1700037960.0","upvote_count":"3"}],"comment_id":"375529","content":"I think B is the right answer.\n\nA: Doesn't make sense. If you don't use the new product, it becomes useless.\nC: Conventional products are also necessary as data.\nD: I don't understand the need to wait until the threshold is exceeded.","upvote_count":"34","timestamp":"1622940240.0"},{"comment_id":"375290","poster":"gcp2021go","content":"answer is B","upvote_count":"11","timestamp":"1622907540.0"},{"upvote_count":"1","timestamp":"1742079480.0","poster":"bc3f222","comment_id":"1399068","content":"Selected Answer: B\nbecause other options are wrong"},{"timestamp":"1731941220.0","comment_id":"1314038","content":"Selected Answer: D\nTask: \"You also want to use AI Platform's continuous evaluation service to ensure that the models have high accuracy on your test dataset.\"\nDocs: https://cloud.google.com/vertex-ai/docs/evaluation/introduction\n\"After your model is deployed to production, periodically evaluate your model with new incoming data. If the evaluation metrics show that your model performance is degrading, consider re-training your model. This process is called continuous evaluation.\"","upvote_count":"1","poster":"joqu"},{"comments":[{"comment_id":"1322734","upvote_count":"1","timestamp":"1733488260.0","content":"I dont see why adding data before would mean that continuos evaluation is not featured; you would use it anyways","poster":"Franui"}],"poster":"503b759","upvote_count":"1","comment_id":"1296663","content":"D:\nIts definitely not a clear choice. B is the most obvious answer - you know you've got new data coming in, so why not incorporate it immediately into training. EXCEPT the question clearly states that Vertex continual evaluation should feature.","timestamp":"1728775260.0"},{"comment_id":"507157","poster":"MisterHairy","upvote_count":"4","timestamp":"1727240460.0","content":"=New Question6=\nYou work for a global footwear retailer and need to predict when an item will be out of stock based on historical inventory dat a. Customer behavior is highly dynamic since footwear demand is influenced by many different factors. You want to serve models that are trained on all available data, but track your performance on specific subsets of data before pushing to production. What is the most streamlined and reliable way to perform this validation? \n\nA. Use the TFX Mode!Validator tools to specify performance metrics for production readiness\nB. Use k-fold cross-validation as a validation strategy to ensure that your model is ready for production. \nC. Use the last relevant week of data as a validation set to ensure that your model is performing accurately on current data.\nD. Use the entire dataset and treat the area under the receiver operating characteristics curve (AUC ROC) as the main metric.","comments":[{"timestamp":"1726187040.0","upvote_count":"1","content":"Option A\nYou can define specific performance metrics for different subsets of your data","comment_id":"1282903","poster":"oddsoul"},{"comments":[{"content":"TFX ModelValidator tools are designed to integrate performance tracking into the ML pipeline, providing robust validation on specific subsets of data before deploying models to production.","poster":"VJlaxmi","timestamp":"1717379160.0","upvote_count":"1","comment_id":"1223371"}],"poster":"VJlaxmi","timestamp":"1717379100.0","content":"option A is correct","comment_id":"1223369","upvote_count":"1"},{"poster":"wences","comment_id":"542786","upvote_count":"6","timestamp":"1644281640.0","content":"A is the correct"},{"upvote_count":"2","content":"B looks to be ok as using cross validation testing results are more even","comment_id":"523817","timestamp":"1642203300.0","poster":"sid515"},{"poster":"MisterHairy","comment_id":"507166","upvote_count":"1","content":"Answer?","timestamp":"1640183520.0"},{"content":"I think C\nB is wrong if we train on all data https://medium.com/@soumyachess1496/cross-validation-in-time-series-566ae4981ce4 \nwhen testing on time series we generally use newest data so C","upvote_count":"3","poster":"Magda123212321","timestamp":"1642961340.0","comment_id":"530731"},{"content":"C is correct, \nA i think that is incorrect bcause in the question does not mention about TensorFlow.","timestamp":"1664455920.0","comments":[{"content":"https://www.tensorflow.org/tfx/guide/non_tf","comment_id":"860546","timestamp":"1680574980.0","upvote_count":"1","poster":"ranula123"}],"upvote_count":"1","poster":"odiez3","comment_id":"682706"},{"comment_id":"537716","upvote_count":"6","poster":"Sylh","content":"I think A, the TFX ModelValidator tools allow you to determine which slices the model is evaluated on, which is clearly stated in the question: \"track your performance on specific subsets of data before pushing to production\" https://www.tensorflow.org/tfx/guide/evaluator","timestamp":"1643702520.0"}]},{"content":"Selected Answer: B\nA. Keep the original test dataset unchanged even if newer products are incorporated into retraining. : This would not test on new products.\nB. Extend your test dataset with images of the newer products when they are introduced to retraining. Most Voted : old+new products testing. Great\nC. Replace your test dataset with images of the newer products when they are introduced to retraining. : No need of old product to be tested? old product recognition might change when new products are added in training. Option Not good.\nD. Update your test dataset with images of the newer products when your evaluation metrics drop below a pre-decided threshold.: why wait? no need","poster":"harithacML","comment_id":"946824","timestamp":"1727240460.0","upvote_count":"1"},{"comment_id":"725177","content":"Selected Answer: B\nYou need to correctly classify newer products, so you need the new training data ==> A is wrong;\nYou need to keep doing a good job on older dataset, you can't just ignore it ==> C is wrong;\nYou know when you are introducing new products, there is no need to wait for a drop in preformaces ==> D is wrong;\nB is correct","upvote_count":"2","poster":"EFIGO","timestamp":"1727240460.0"},{"comment_id":"1282901","content":"Selected Answer: B\nB correct","timestamp":"1726186440.0","upvote_count":"1","poster":"oddsoul"},{"content":"Selected Answer: B\nThe best approach is option B: Extend your test dataset with images of the newer products. This ensures accurate evaluation as your product catalog evolves.","upvote_count":"1","comment_id":"1224874","timestamp":"1717607220.0","poster":"PhilipKoku"},{"comment_id":"1128034","content":"Selected Answer: B\nMy initial confusion with option B arose from the phrase \"with images of the newer products when they are introduced to retraining.\" Initially, I mistakenly interpreted it as recommending the use of the same images in both training and testing, which is incorrect. However, upon further reflection, I realized that using the same product does not necessarily mean using identical images. Therefore, I now believe that option B is the most suitable choice.","poster":"guilhermebutzke","timestamp":"1705860180.0","upvote_count":"1"},{"timestamp":"1701871440.0","upvote_count":"1","content":"Selected Answer: B\nA and C make no sense - you don't want to lose any of the performance on existing products.\nD - Why would you wait for your performance to drop in the first place? That's a reactive rather than proactive approach.\nThe answer is B","poster":"bugger123","comment_id":"1089406"},{"poster":"fragkris","comment_id":"1085174","upvote_count":"1","timestamp":"1701430440.0","content":"Selected Answer: B\nB for sure"},{"comment_id":"1070402","content":"B is the only thing we do in practice","poster":"Sum_Sum","timestamp":"1699968660.0","upvote_count":"1"},{"timestamp":"1683608040.0","comment_id":"892679","poster":"M25","upvote_count":"2","content":"Selected Answer: B\nWent with B"},{"comment_id":"831617","content":"Selected Answer: B\nyou can't just replace the old product data with just new product, until you don't sell old product anymore","timestamp":"1678172160.0","upvote_count":"2","poster":"will7722"},{"upvote_count":"1","poster":"SharathSH","timestamp":"1672481280.0","comment_id":"762596","content":"Ans: B\nA would not use the newer data hence not a ideal option\nC Replacing will not be a good option as it will replace older data with newer data which in turn hampers accuracy\nD waiting for threshold is not a better option"},{"content":"B is the most plausible answer. The key principle is that test set should represent ground truth distribution to infer credible model evaluation. So once new products become available, test set should be updated to reflect the new product distribution","poster":"koakande","comment_id":"731186","timestamp":"1669792260.0","upvote_count":"2"},{"content":"it should be B as its inclusive","comment_id":"709337","timestamp":"1667325000.0","upvote_count":"1","poster":"abhi0706"},{"comment_id":"708335","poster":"abhi0706","content":"B as it extends to new products","timestamp":"1667211120.0","upvote_count":"2"},{"timestamp":"1660565340.0","content":"Selected Answer: B\nCorrect answer is \"B\"","comment_id":"647167","poster":"GCP72","upvote_count":"2"},{"comment_id":"643199","timestamp":"1659762240.0","poster":"sachinxshrivastav","upvote_count":"1","content":"Selected Answer: B\nB is the right one"},{"comment_id":"615281","upvote_count":"4","poster":"Mohamed_Mossad","content":"Selected Answer: D\nanswer between B,D but in the question \"You also want to use AI Platform's continuous evaluation service\" will make me biased towards D , also retrain is done when model performance is below threshold , not whenever new data is intoroduce","timestamp":"1655033340.0"},{"comment_id":"568297","upvote_count":"2","content":"This one looks like B to me. It could be B or D, but there seems no advantage to allowing your model to drop BELOW the required threshold. That leaves B.","poster":"baimus","timestamp":"1647340740.0"},{"timestamp":"1646015160.0","upvote_count":"2","content":"Selected Answer: B\nB is correct","comment_id":"557778","poster":"caohieu04"},{"timestamp":"1645306080.0","upvote_count":"1","comment_id":"551330","content":"the answer should be C: the purpose is to evaluate whether the model can effectively classify the NEW products. It has already been proven (assumed) that it is working on the older products. limiting the test set to only new products will give a definitive conclusion of whether the model can effectively classify the new products","poster":"SlipperySlope"},{"content":"Selected Answer: B\ndoesn't make sense to replace or wait...then B is the correct answer to me","comment_id":"549328","timestamp":"1645096440.0","poster":"lordcenzin","upvote_count":"2"},{"upvote_count":"1","comment_id":"518685","poster":"NamitSehgal","content":"B Extending data is a better approach.","timestamp":"1641516540.0"},{"comment_id":"495046","content":"B is right. \n\nExtending sample will preserve the logic.","upvote_count":"1","poster":"alphard","timestamp":"1638786540.0"},{"comment_id":"464014","upvote_count":"1","content":"It's B","poster":"93alejandrosanchez","timestamp":"1634551320.0"},{"comment_id":"444248","poster":"Y2Data","content":"Answer is B","timestamp":"1631580660.0","upvote_count":"1"},{"timestamp":"1631338080.0","content":"B\n\nThey are not deprecating old products so why remove those from the test data set.","upvote_count":"3","comment_id":"442826","poster":"gcper"}],"question_id":261,"isMC":true,"url":"https://www.examtopics.com/discussions/google/view/54652-exam-professional-machine-learning-engineer-topic-1-question/","exam_id":13,"question_text":"You work for an online retail company that is creating a visual search engine. You have set up an end-to-end ML pipeline on Google Cloud to classify whether an image contains your company's product. Expecting the release of new products in the near future, you configured a retraining functionality in the pipeline so that new data can be fed into your ML models. You also want to use AI Platform's continuous evaluation service to ensure that the models have high accuracy on your test dataset. What should you do?","choices":{"A":"Keep the original test dataset unchanged even if newer products are incorporated into retraining.","D":"Update your test dataset with images of the newer products when your evaluation metrics drop below a pre-decided threshold.","B":"Extend your test dataset with images of the newer products when they are introduced to retraining.","C":"Replace your test dataset with images of the newer products when they are introduced to retraining."},"topic":"1","timestamp":"2021-06-05 17:39:00","unix_timestamp":1622907540,"answer_description":"","question_images":[],"answer_ET":"B","answer":"B","answer_images":[]},{"id":"kQ9tDAOrEAxrVQE64wFx","question_images":[],"topic":"1","answer_description":"","exam_id":13,"choices":{"A":"Write your data in TFRecords.","D":"Use one-hot encoding on all categorical features.","B":"Z-normalize all the numeric features.","C":"Oversample the fraudulent transaction 10 times."},"answer_images":[],"answer_ET":"C","answer":"C","unix_timestamp":1625891040,"question_text":"You work for a bank and are building a random forest model for fraud detection. You have a dataset that includes transactions, of which 1% are identified as fraudulent. Which data transformation strategy would likely improve the performance of your classifier?","isMC":true,"question_id":262,"timestamp":"2021-07-10 06:24:00","answers_community":["C (100%)"],"discussion":[{"upvote_count":"14","timestamp":"1625891040.0","content":"C - https://swarit.medium.com/detecting-fraudulent-consumer-transactions-through-machine-learning-25b1f2cabbb4","comment_id":"403108","poster":"ralf_cc"},{"content":"Selected Answer: C\nC is the answer","timestamp":"1641355980.0","comment_id":"517158","upvote_count":"5","poster":"NamitSehgal"},{"comment_id":"1232427","content":"Selected Answer: C\nAgree with C","timestamp":"1718715000.0","poster":"dija123","upvote_count":"1"},{"poster":"PhilipKoku","content":"Selected Answer: C\nC) Oversample","comment_id":"1225693","upvote_count":"1","timestamp":"1717696320.0"},{"comment_id":"1187277","timestamp":"1711957140.0","upvote_count":"3","poster":"MultiCloudIronMan","content":"Selected Answer: C\nOversampling increases the number of fraudulent transaction in the training data to enable the machine to learn how to predict them"},{"timestamp":"1701775740.0","content":"Selected Answer: C\nC - Even though most similar questions propose to downsample the majority (not fraudulent) and add weights to it.","comment_id":"1088423","upvote_count":"1","poster":"fragkris"},{"timestamp":"1683609120.0","comment_id":"892743","content":"Selected Answer: C\nWent with C","upvote_count":"2","poster":"M25"},{"poster":"wish0035","upvote_count":"1","content":"Selected Answer: C\nans: C\n\nA, B, D => wouldnt help with imbalance","comment_id":"746589","timestamp":"1671147240.0"},{"poster":"hiromi","timestamp":"1671139020.0","comment_id":"746511","content":"Selected Answer: C\nC\nhttps://medium.com/analytics-vidhya/credit-card-fraud-detection-how-to-handle-imbalanced-dataset-1f18b6f881","upvote_count":"1"},{"timestamp":"1657529220.0","content":"Selected Answer: C\nthe best option is C","upvote_count":"1","poster":"Mohamed_Mossad","comment_id":"629925"}],"url":"https://www.examtopics.com/discussions/google/view/57556-exam-professional-machine-learning-engineer-topic-1-question/"},{"id":"ZWcWqnpvcl6CgQoA0UAb","isMC":true,"answer_description":"","unix_timestamp":1670423100,"discussion":[{"comment_id":"746594","content":"Selected Answer: D\nans: D\n\nA, C => local storage, NFS... discarded. Google encourages you to use Cloud Storage.\nB => could do the job, but here I would focus on the \"daily training\" thing, because Vertex AI Training jobs are better for this. Also I think that Google usually encourages to use Vertex AI over VMs.","timestamp":"1671147480.0","poster":"wish0035","upvote_count":"14"},{"timestamp":"1735318260.0","comment_id":"1332529","poster":"thescientist","upvote_count":"1","content":"Selected Answer: D\nD because you only pay for infra when you use AI Platform(Vertex) and for VM's you pay them as long as they are open. No auto shut down"},{"content":"Selected Answer: D\nAnswer: D\nauto scaling","upvote_count":"1","poster":"oddsoul","comment_id":"1296452","timestamp":"1728727500.0"},{"comment_id":"1252984","timestamp":"1721641740.0","poster":"San1111111111","content":"D because automatic scaling","upvote_count":"1"},{"timestamp":"1717696500.0","poster":"PhilipKoku","content":"Selected Answer: D\nD) Is the best answer","comment_id":"1225696","upvote_count":"1"},{"upvote_count":"1","comment_id":"1082829","content":"Selected Answer: D\nI'll go with D. How is C correct?","timestamp":"1701195720.0","poster":"abhay669"},{"content":"Selected Answer: A\nD as need to minimize cost","poster":"Mickey321","upvote_count":"1","comment_id":"1071799","timestamp":"1700074500.0"},{"content":"Selected Answer: A\nI think it is A. Refer to Q20 of the GCP Sample Questions - they say managed services (such as Kubeflow Pipelines / Vertex AI) are not the options for 'minimizing costs'. In this case, you should configure your own infrastructure to train the model leaving A,B. Undecided between A,B because A would minimize costs, but also result in inefficient I/O operations during training.","poster":"Mdso","upvote_count":"2","comment_id":"970268","timestamp":"1690985580.0"},{"content":"Selected Answer: D\nThe pre-trained EfficientNet model can be easily loaded from Cloud Storage, which eliminates the need for local storage or an NFS server. Using AI Platform Training allows for the automatic scaling of resources based on the size of the dataset, which can save costs compared to using a fixed-size VM or node pool. Additionally, the ability to use custom scale tiers allows for fine-tuning of resource allocation to match the specific needs of the training job.","timestamp":"1688491440.0","comment_id":"943032","poster":"tavva_prudhvi","upvote_count":"2"},{"comment_id":"892744","content":"Selected Answer: D\nWent with D","poster":"M25","upvote_count":"1","timestamp":"1683609120.0"},{"timestamp":"1677059880.0","upvote_count":"3","poster":"shankalman717","comment_id":"817692","content":"Selected Answer: B\nB. A Deep Learning VM with 4 V100 GPUs and Cloud Storage.\n\nFor this scenario, a Deep Learning VM with 4 V100 GPUs and Cloud Storage is likely the most cost-effective solution while still providing sufficient computing resources for the model training. Using Cloud Storage can allow the model to be trained and the data to be stored in a scalable and cost-effective way.\n\nOption A, using a Deep Learning VM with local storage, may not provide enough storage capacity to store the training data and model checkpoints. Option C, using a Kubernetes Engine cluster, can be overkill for the size of the job and adds additional complexity. Option D, using an AI Platform Training job, is a good option as it is designed for running machine learning jobs at scale, but may be more expensive than a Deep Learning VM with Cloud Storage."},{"upvote_count":"1","content":"Selected Answer: D\nbecouse it's cheap","poster":"enghabeth","comment_id":"802682","timestamp":"1675900920.0"},{"upvote_count":"3","comment_id":"747009","content":"Selected Answer: D\nit seems D","timestamp":"1671183480.0","poster":"hiromi"},{"upvote_count":"2","timestamp":"1670633580.0","content":"Selected Answer: D\nI think it's D","poster":"OzoneReloaded","comment_id":"740583"},{"comment_id":"740147","timestamp":"1670593200.0","content":"Selected Answer: B\nIt's D","poster":"JeanEl","upvote_count":"2"},{"poster":"ares81","upvote_count":"4","content":"It seems D to me.","comment_id":"737951","timestamp":"1670423100.0"}],"question_images":[],"timestamp":"2022-12-07 15:25:00","answers_community":["D (77%)","14%","9%"],"exam_id":13,"url":"https://www.examtopics.com/discussions/google/view/90424-exam-professional-machine-learning-engineer-topic-1-question/","question_text":"You are using transfer learning to train an image classifier based on a pre-trained EfficientNet model. Your training dataset has 20,000 images. You plan to retrain the model once per day. You need to minimize the cost of infrastructure. What platform components and configuration environment should you use?","topic":"1","answer_images":[],"answer_ET":"D","answer":"D","choices":{"C":"A Google Kubernetes Engine cluster with a V100 GPU Node Pool and an NFS Server","B":"A Deep Learning VM with 4 V100 GPUs and Cloud Storage.","A":"A Deep Learning VM with 4 V100 GPUs and local storage.","D":"An AI Platform Training job using a custom scale tier with 4 V100 GPUs and Cloud Storage"},"question_id":263},{"id":"kEpWN57nPg5iqBpl6BJe","question_text":"While conducting an exploratory analysis of a dataset, you discover that categorical feature A has substantial predictive power, but it is sometimes missing. What should you do?","unix_timestamp":1670451540,"url":"https://www.examtopics.com/discussions/google/view/90506-exam-professional-machine-learning-engineer-topic-1-question/","answer_ET":"D","exam_id":13,"isMC":true,"topic":"1","question_id":264,"question_images":[],"timestamp":"2022-12-07 23:19:00","answer_description":"","choices":{"C":"Replace the missing values with the values of the feature with the highest Pearson correlation with feature A.","B":"Compute the mode of feature A and then use it to replace the missing values in feature A.","A":"Drop feature A if more than 15% of values are missing. Otherwise, use feature A as-is.","D":"Add an additional class to categorical feature A for missing values. Create a new binary feature that indicates whether feature A is missing."},"answers_community":["D (66%)","B (34%)"],"discussion":[{"upvote_count":"14","comments":[{"timestamp":"1697097600.0","upvote_count":"2","comment_id":"868042","poster":"frangm23","content":"I guess I would go with D, but it confuses me the fact that in option D, it doesn't say that NaN values are replaced (only that there's a new column added) and this could lead to problems like exploding gradients.\nPlus, Google encourages to replace missing values. https://developers.google.com/machine-learning/testing-debugging/common/data-errors\nAny thoughts on this?"}],"timestamp":"1686865800.0","poster":"wish0035","comment_id":"746604","content":"Selected Answer: D\nans: D\n\nA => no, you don't want to drop a feature with high prediction power.\nB => i think this could confuse the model... a better solution could be to fill missing values using an algorithm like Expectation Maximization, but using the mode i think is a bad idea in this case, because if you have a significant number of missing values (for example >10%) this would modify the \"predictive power\". you don't want to lose predictive power of a feature, just guide the model to learn when to use that feature and when to ignore it.\nC => this doesn't make any sense for me. not sure what i would do that.\nD => i think this could be a really good approach, and i'm pretty sure it would work pretty well a lot of models. the model would learn that when \"is_available_feat_A\" == True, then it would use the feature A, but whenever it is missing then it would try to use other features."},{"poster":"hiromi","comments":[{"poster":"tavva_prudhvi","upvote_count":"1","content":"While this approach may seem reasonable, it can introduce bias in the dataset by over-representing the mode, especially if the missing values are not missing at random.","timestamp":"1715095020.0","comment_id":"1065021"}],"content":"Selected Answer: B\nB\n\"For categorical variables, we can usually replace missing values with mean, median, or most frequent values\"\nDr. Logan Song - Journey to Become a Google Cloud Machine Learning Engineer - Page 48","timestamp":"1686901740.0","upvote_count":"5","comment_id":"747017"},{"content":"Selected Answer: D\nD) Good approach","comment_id":"1225698","poster":"PhilipKoku","timestamp":"1733515200.0","upvote_count":"1"},{"comment_id":"1187281","poster":"MultiCloudIronMan","timestamp":"1727768880.0","content":"Selected Answer: B\nGoogle encourages filling missing value and using mode is one of the examples given. D only tell the obvious - data is missing!","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: D\nB and D are correct, but I decided to go with D.","comment_id":"1088454","timestamp":"1717581240.0","poster":"fragkris"},{"comment_id":"1071812","poster":"Mickey321","content":"Selected Answer: D\nhighly predictive","upvote_count":"1","timestamp":"1715793540.0"},{"upvote_count":"2","timestamp":"1715447580.0","comment_id":"1068042","poster":"ichbinnoah","content":"Selected Answer: B\nDefinitely not D, it does not even solve the problem of NA values."},{"upvote_count":"1","poster":"andresvelasco","content":"Options B or D\nBut isnt there an inconsistency in option D? if you replace missing values with a new category (\"missing\") why would you haveto create an extra feature?","comment_id":"1012593","timestamp":"1710971160.0"},{"timestamp":"1704643200.0","comment_id":"945750","poster":"Liting","content":"Selected Answer: D\nAgree with wish0035, answer should be D","upvote_count":"1"},{"poster":"PST21","timestamp":"1703233440.0","comment_id":"930205","upvote_count":"2","content":"By creating a new class for the missing values, you explicitly capture the absence of data, which can provide valuable information for predictive modeling. Additionally, creating a binary feature allows the model to distinguish between cases where feature A is present and cases where it is missing, which can be useful for identifying potential patterns or relationships in the data."},{"timestamp":"1702244100.0","poster":"amtg","content":"Selected Answer: B\nBy imputing the missing values with the mode (the most frequent value), you retain the original feature's predictive power while handling the missing values","upvote_count":"1","comment_id":"920256"},{"comment_id":"906471","timestamp":"1700903940.0","upvote_count":"2","poster":"Scipione_","content":"Selected Answer: D\nBoth B and D are possible, but the correct answer is D because of the feature high predictive power."},{"content":"Selected Answer: D\nWent with D","upvote_count":"1","poster":"M25","timestamp":"1699513920.0","comment_id":"892746"},{"comment_id":"840087","poster":"tavva_prudhvi","content":"I think, its D. \nOption B of imputing the missing values of feature A with the mode of feature A could be a reasonable approach if the mode provides a good representation of the distribution of feature A. However, this method may lead to biased results if the mode is not representative of the missing values. This could be the case if the missing values have a different distribution than the observed values.\n\nSimilarly, When a categorical feature has substantial predictive power, it is important not to discard it. Instead, missing values can be handled by adding an additional class for missing values and creating a new binary feature that indicates whether feature A is missing or not. This approach ensures that the predictive power of feature A is retained while accounting for missing values. Computing the mode of feature A and replacing missing values may distort the distribution of the feature and create bias in the analysis. Similarly, replacing missing values with values from another feature may introduce noise and lead to incorrect results.","timestamp":"1694789820.0","upvote_count":"2"},{"poster":"BenMS","timestamp":"1693141440.0","comment_id":"823853","content":"Selected Answer: D\nIf our objective was to produce a complete dataset then we might use some average value to fill in the gaps (option B) but in this case we want to predict an outcome, so inventing our own data is not going to help in my view.\n\nOption D is the most sensible approach to let the model choose the best features.","upvote_count":"1"},{"upvote_count":"2","comment_id":"746990","poster":"Pancy","timestamp":"1686899040.0","content":"B. Because the important feature is already known. By using mode, contribution of other features will not be missed"},{"poster":"ares81","timestamp":"1686478380.0","content":"Mode is the way to go for categorical features. B, for me.","upvote_count":"3","comment_id":"741666"},{"poster":"LearnSodas","comment_id":"740990","timestamp":"1686395400.0","upvote_count":"3","content":"Selected Answer: B\nI agree with B"},{"poster":"OzoneReloaded","comment_id":"740591","timestamp":"1686351960.0","content":"Selected Answer: D\nI'll stick to D because for categorical features it's a nice approach","upvote_count":"1"},{"poster":"Vedjha","content":"I will go with 'B' dropping feature is not a solution to handle missing values. We can replace it by mean/mode or by any calculated values.\nhttps://www.analyticsvidhya.com/blog/2021/10/handling-missing-value/#:~:text=There%20are%202%20primary%20ways,Imputing%20the%20Missing%20Values","comment_id":"738431","upvote_count":"3","timestamp":"1686169140.0"}],"answer_images":[],"answer":"D"},{"id":"awWuX7O0sEDnqw1KX2J5","answer_description":"","answers_community":["A (96%)","4%"],"answer_images":[],"answer_ET":"A","question_images":[],"url":"https://www.examtopics.com/discussions/google/view/90507-exam-professional-machine-learning-engineer-topic-1-question/","unix_timestamp":1670452200,"timestamp":"2022-12-07 23:30:00","topic":"1","exam_id":13,"isMC":true,"discussion":[{"comment_id":"1225700","content":"Selected Answer: A\nA) K-means is ideal for unsupervised clustering","timestamp":"1733515320.0","upvote_count":"2","poster":"PhilipKoku"},{"content":"Selected Answer: A\nK-means algorithm is used for grouping/clustering data in unsupervised learning experiments.","upvote_count":"3","timestamp":"1727769060.0","comment_id":"1187285","poster":"MultiCloudIronMan"},{"content":"Selected Answer: A\nWent with A","poster":"M25","comment_id":"892747","timestamp":"1699513980.0","upvote_count":"4"},{"content":"Selected Answer: A\nwhen to use k-means : Your data may contain natural groupings or clusters of data. You may want to identify these groupings descriptively in order to make data-driven decisions. For example, a retailer may want to identify natural groupings of customers who have similar purchasing habits or locations. This process is known as customer segmentation.\nhttps://cloud.google.com/bigquery/docs/kmeans-tutorial","comment_id":"892647","timestamp":"1699509420.0","poster":"CloudKida","upvote_count":"4"},{"comment_id":"840094","poster":"tavva_prudhvi","timestamp":"1694790060.0","upvote_count":"2","content":"A\nThis is the most efficient solution for segmenting customers based on their purchasing habits, as it utilizes BigQuery's built-in machine learning capabilities to identify distinct clusters of customers based on their purchasing behavior. By allowing BigQuery to automatically optimize the number of clusters, you can ensure that the model identifies the most appropriate number of segments based on the data, without having to manually select the number of clusters."},{"content":"Selected Answer: A\nI correct myself. It's A:\nAccording to the documentation, if you omit the num_clusters option, BigQuery ML will choose a reasonable default based on the total number of rows in the training data.","poster":"ares81","upvote_count":"2","timestamp":"1688544420.0","comment_id":"766505"},{"upvote_count":"3","comment_id":"747019","timestamp":"1686901920.0","content":"Selected Answer: A\nA\nhttps://cloud.google.com/bigquery-ml/docs/kmeans-tutorial\nhttps://towardsdatascience.com/how-to-use-k-means-clustering-in-bigquery-ml-to-understand-and-describe-your-data-better-c972c6f5733b","poster":"hiromi"},{"comment_id":"746608","poster":"wish0035","timestamp":"1686866340.0","upvote_count":"4","content":"Selected Answer: A\nans: A, pretty sure.\n\nC, D => discarded, very time consuming.\nB => yes, you can identify similarities within each column, but when i read \"you don’t yet understand the commonalities in their behavior\" i understand that this job would be difficult, because there could be many columns to analyze, and i don't think that this would be efficient.\n\nA => BigQuery ML is compatible with kmeans clustering, it's easy and efficient to create, and i would automatically detect the number of clusters.\n\nAlso from the BigQuery ML docs: \"K-means clustering for data segmentation; for example, identifying customer segments.\"\n(Source: https://cloud.google.com/bigquery-ml/docs/introduction#supported_models_in)"},{"upvote_count":"3","comment_id":"746337","content":"Selected Answer: A\nK-means is a good unsupervised learning algorithm to segment a population based on similarity\n\nWe can usa K-means directly in BQ, so I think it's \"the most efficient way\"\n\nLabeling is not a good option since we don't really know what make a customer similar to another, and why dataprep if we can use directly BQ?","timestamp":"1686841080.0","poster":"LearnSodas"},{"comment_id":"744060","poster":"ares81","content":"It seems B, to me.","timestamp":"1686655260.0","upvote_count":"1"},{"poster":"neochaotic","comment_id":"741189","upvote_count":"1","content":"Selected Answer: B\nIts B! Dataprep provides Data profiling functionalities","timestamp":"1686416880.0"},{"upvote_count":"1","content":"The question is about commonalities of clients by characteristics, no about characteristics by client. I mean with B you are looking for segments of the characteristics which define a client. But you need segments of clients defined by characteristics.","timestamp":"1686400080.0","comment_id":"741047","poster":"japoji"},{"poster":"Vedjha","upvote_count":"4","timestamp":"1686169800.0","content":"Will go for 'A' as it is easy to build model in BQML where data is already present and optimization would be auto in case of K-mean algo","comment_id":"738436"}],"question_text":"You work for a large retailer and have been asked to segment your customers by their purchasing habits. The purchase history of all customers has been uploaded to BigQuery. You suspect that there may be several distinct customer segments, however you are unsure of how many, and you don’t yet understand the commonalities in their behavior. You want to find the most efficient solution. What should you do?","answer":"A","choices":{"C":"Use the Data Labeling Service to label each customer record in BigQuery. Train a model on your labeled data using AutoML Tables. Review the evaluation metrics to understand whether there is an underlying pattern in the data.","B":"Create a new dataset in Dataprep that references your BigQuery table. Use Dataprep to identify similarities within each column.","D":"Get a list of the customer segments from your company’s Marketing team. Use the Data Labeling Service to label each customer record in BigQuery according to the list. Analyze the distribution of labels in your dataset using Data Studio.","A":"Create a k-means clustering model using BigQuery ML. Allow BigQuery to automatically optimize the number of clusters."},"question_id":265}],"exam":{"isMCOnly":true,"numberOfQuestions":304,"lastUpdated":"11 Apr 2025","provider":"Google","id":13,"isImplemented":true,"name":"Professional Machine Learning Engineer","isBeta":false},"currentPage":53},"__N_SSP":true}