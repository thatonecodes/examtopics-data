{"pageProps":{"questions":[{"id":"2BRSlUq24IfNu9IJR2lO","question_images":[],"url":"https://www.examtopics.com/discussions/google/view/130634-exam-professional-machine-learning-engineer-topic-1-question/","answer_description":"","unix_timestamp":1704742200,"answer_ET":"C","answers_community":["C (50%)","D (50%)"],"answer":"C","question_text":"You have developed a BigQuery ML model that predicts customer chum, and deployed the model to Vertex AI Endpoints. You want to automate the retraining of your model by using minimal additional code when model feature values change. You also want to minimize the number of times that your model is retrained to reduce training costs. What should you do?","timestamp":"2024-01-08 20:30:00","choices":{"B":"1. Enable request-response logging on Vertex AI Endpoints\n2. Schedule a TensorFlow Data Validation job to monitor training/serving skew\n3. Execute model retraining if there is significant distance between the distributions","D":"1. Create a Vertex AI Model Monitoring job configured to monitor training/serving skew\n2. Configure alert monitoring to publish a message to a Pub/Sub queue when a monitoring alert is detected\n3. Use a Cloud Function to monitor the Pub/Sub queue, and trigger retraining in BigQuery","C":"1. Create a Vertex AI Model Monitoring job configured to monitor prediction drift\n2. Configure alert monitoring to publish a message to a Pub/Sub queue when a monitoring alert is detected\n3. Use a Cloud Function to monitor the Pub/Sub queue, and trigger retraining in BigQuery","A":"1 Enable request-response logging on Vertex AI Endpoints\n2. Schedule a TensorFlow Data Validation job to monitor prediction drift\n3. Execute model retraining if there is significant distance between the distributions"},"answer_images":[],"isMC":true,"discussion":[{"timestamp":"1708281600.0","upvote_count":"7","content":"Selected Answer: D\nMy answer: D\n\nGiven the emphasis on \"model feature values change\" in the question, the most suitable option would be D.\n\nAlthough option C involves monitoring prediction drift, which may indirectly capture changes in feature values, option D directly addresses the need to monitor training/serving skew. By detecting discrepancies between the training and serving data distributions, option D is more aligned with the requirement to automate retraining when model feature values change. Therefore, option D is the most suitable choice in this context.","poster":"guilhermebutzke","comment_id":"1153492"},{"timestamp":"1742065860.0","comment_id":"1399012","upvote_count":"1","content":"Selected Answer: C\nTraining/serving skew monitoring is best used to detect mismatches between training and serving data schemas—not feature drift over time. Prediction drift is more relevant for this use case.","poster":"bc3f222"},{"poster":"f084277","timestamp":"1731604860.0","content":"Selected Answer: C\nSkew is static, drift happens over time. Answer is C.","upvote_count":"2","comment_id":"1312189"},{"upvote_count":"4","comment_id":"1225740","comments":[{"timestamp":"1733188140.0","upvote_count":"1","poster":"rajshiv","comment_id":"1321159","content":"the issue is \"drift\" and not \"Skew\". Hence C is more correct as compared to D."},{"timestamp":"1719748560.0","upvote_count":"1","content":"Agreed","comment_id":"1239627","poster":"Prakzz"}],"poster":"bobjr","timestamp":"1717701600.0","content":"Selected Answer: C\nSkew should be detected at the beginning of the productionalisation of the model -> skew test the training data Vs the real data -> a skew indicates you trained in a dataset that is not alined with your data that you have in input\n\nDrift is used when the model works well at the beginning, but the world change and the data input changes -> drift is more long term\n\nhere it is a drift issue"},{"poster":"Shno","content":"if the model training is done through bigquery ML, we don't have access to the training data after export, so I don't understand how training/serving skew can be applied. Can someone who is voting in favour of D clarify?","upvote_count":"1","comment_id":"1204496","timestamp":"1714473720.0"},{"timestamp":"1713329280.0","poster":"gscharly","comment_id":"1196989","content":"Selected Answer: D\nI go with D","upvote_count":"1"},{"timestamp":"1712495220.0","comments":[{"poster":"pinimichele01","comment_id":"1200779","content":"see guilhermebutzke","upvote_count":"1","timestamp":"1713881460.0"}],"upvote_count":"1","content":"Selected Answer: D\nIt's D","poster":"pinimichele01","comment_id":"1190957"},{"upvote_count":"3","comment_id":"1151623","poster":"CHARLIE2108","timestamp":"1708040220.0","content":"Selected Answer: D\nchanged my mind it's D"},{"content":"Selected Answer: C\nI go with C but D is pretty similar.\n\nC -> Prediction drift (When the overall distribution of predictions changes significantly between training and serving data).\n\nD -> Training/serving skew (When the distribution of specific features between training and serving data differs significantly).","timestamp":"1707505920.0","upvote_count":"3","comment_id":"1145722","poster":"CHARLIE2108","comments":[{"poster":"CHARLIE2108","comment_id":"1151620","upvote_count":"1","timestamp":"1708039980.0","content":"It's D"}]},{"content":"Selected Answer: C\nOption C: \n\nThis option directly addresses your requirements:\nVertex AI Model Monitoring: It allows efficient monitoring of prediction drift through metrics like Mean Squared Error or AUC-ROC.\nPub/Sub alerts: Alert triggers notification upon significant drift, minimizing unnecessary retraining.\nCloud Function: It reacts to Pub/Sub messages and triggers retraining in BigQuery using minimal additional code.","comment_id":"1136833","poster":"ddogg","upvote_count":"3","timestamp":"1706710680.0"},{"comment_id":"1123292","poster":"b1a8fae","upvote_count":"3","timestamp":"1705318560.0","content":"Selected Answer: C\nAfter reconsidering, I think it is C:\n- No need to use TF to enable model monitoring as stated here: https://cloud.google.com/vertex-ai/docs/model-monitoring/using-model-monitoring\n(even if it uses it under the hood: https://cloud.google.com/vertex-ai/docs/model-monitoring/overview#calculating-skew-and-drift)\n\n- The problem speaks about alerting of model feature changes, which happens over time, and uses a baseline of the historical production data -> prediction skew. (if the problem specified that it changes compared to training data, then it would be training-skew) (https://cloud.google.com/vertex-ai/docs/model-monitoring/monitor-explainable-ai#feature_attribution_training-serving_skew_and_prediction_drift)"},{"poster":"b1a8fae","timestamp":"1705309260.0","upvote_count":"4","content":"Selected Answer: D\nI would avoid using TensorFlow validation to minimize code written. That leaves us with options C and D. Now, since it is the values of the features that we want to flag and not the value of the predictions, this sounds more like training-serving skew situation than prediction drift. Hence, I would go for D.","comment_id":"1123204"},{"poster":"BlehMaks","comment_id":"1122496","upvote_count":"1","content":"Selected Answer: D\ni've changed my mind) it's D\nhttps://www.evidentlyai.com/blog/machine-learning-monitoring-data-and-concept-drift","timestamp":"1705232460.0"},{"timestamp":"1705229400.0","upvote_count":"1","content":"Selected Answer: D\nwe might need to retrain if the feature data distribution in the production and training are significantly different(training/serving skew). Prediction drift occurs when feature data distribution in production changes significantly over time. Should we retrain our model every time when we meet prediction drift? I dont think so, better to analyze why this drift happens.\nhttps://cloud.google.com/vertex-ai/docs/model-monitoring/overview#considerations","poster":"BlehMaks","comment_id":"1122466"},{"upvote_count":"2","poster":"36bdc1e","timestamp":"1705166820.0","content":"C\nThe best option for automating the retraining of your model by using minimal additional code when model feature values change, and minimizing the number of times that your model is retrained to reduce training costs, is to create a Vertex AI Model Monitoring job configured to monitor prediction drift, configure alert monitoring to publish a message to a Pub/Sub queue when a monitoring alert is detected, and use a Cloud Function to monitor the Pub/Sub queue, and trigger retraining in\nBigQuery. This option allows you to leverage the power and simplicity of Vertex AI, Pub/Sub, and Cloud Functions to monitor your model performance and retrain your model when needed. Vertex AI is a unified platform for building and deploying machine learning solutions on Google Cloud.","comment_id":"1121907"},{"poster":"pikachu007","content":"Selected Answer: C\nA and B: TensorFlow Data Validation jobs require more setup and maintenance, and they might not integrate as seamlessly with Vertex AI Endpoints for automated retraining.\nD: Monitoring training/serving skew focuses on differences between training and deployment environments, which might not directly address feature value changes.","upvote_count":"2","comment_id":"1119636","timestamp":"1704971100.0"},{"comment_id":"1116940","content":"I go with : C. \n1. Create a Vertex AI Model Monitoring job configured to monitor prediction drift - > if the modle is already in production we have to considet Prediction drift\n2. Configure alert monitoring to publish a message to a Pub/Sub queue when a monitoring alert is detected -> set Pub/Sub notification channels.\n3. Use a Cloud Function to monitor the Pub/Sub queue, and trigger retraining in BigQuery -> to eimport new data in BQ","upvote_count":"2","poster":"vale_76_na_xxx","timestamp":"1704742200.0"}],"topic":"1","question_id":96,"exam_id":13},{"id":"o0qkDwJogXzbY4vtLHs1","answer":"C","choices":{"A":"Create a Vertex AI Workbench notebook. Use the notebook to submit the Dataproc Serverless feature engineering job. Use the same notebook to submit the custom model training job. Run the notebook cells sequentially to tie the steps together end-to-end.","D":"Use the Kubeflow pipelines SDK to write code that specifies two components\n- The first component initiates an Apache Spark context that runs the PySpark feature engineering code\n- The second component runs the TensorFlow custom model training code\nCreate a Vertex AI Pipelines job to link and run both components.","B":"Create a Vertex AI Workbench notebook. Initiate an Apache Spark context in the notebook and run the PySpark feature engineering code. Use the same notebook to run the custom model training job in TensorFlow. Run the notebook cells sequentially to tie the steps together end-to-end.","C":"Use the Kubeflow pipelines SDK to write code that specifies two components:\n- The first is a Dataproc Serverless component that launches the feature engineering job\n- The second is a custom component wrapped in the create_custom_training_job_from_component utility that launches the custom model training job\nCreate a Vertex AI Pipelines job to link and run both components"},"question_text":"You have been tasked with deploying prototype code to production. The feature engineering code is in PySpark and runs on Dataproc Serverless. The model training is executed by using a Vertex AI custom training job. The two steps are not connected, and the model training must currently be run manually after the feature engineering step finishes. You need to create a scalable and maintainable production process that runs end-to-end and tracks the connections between steps. What should you do?","unix_timestamp":1704742320,"question_id":97,"exam_id":13,"isMC":true,"url":"https://www.examtopics.com/discussions/google/view/130635-exam-professional-machine-learning-engineer-topic-1-question/","timestamp":"2024-01-08 20:32:00","answer_images":[],"question_images":[],"answer_ET":"C","discussion":[{"comment_id":"1218973","content":"Selected Answer: C\nThe first is a Dataproc Serverless component that launches the feature engineering job\nThe second is a custom component wrapped in the create_custom_training_job_from_component utility that launches the custom model training job\nCreate a Vertex AI Pipelines job to link and run both components","timestamp":"1732630860.0","poster":"Akel123","upvote_count":"2"},{"content":"Selected Answer: C\nThe first is a Dataproc Serverless component that launches the feature engineering job\nThe second is a custom component wrapped in the create_custom_training_job_from_component utility that launches the custom model training job\nCreate a Vertex AI Pipelines job to link and run both components","comments":[{"poster":"fitri001","timestamp":"1729494540.0","upvote_count":"2","comment_id":"1199496","content":"A. Vertex AI Workbench notebook: While notebooks are a good way to prototype workflows, they are not ideal for production due to limitations in scalability and version control. Running everything sequentially also doesn't allow for potential parallelization of tasks.\nB. Apache Spark context in notebook: Similar to A, notebooks are not ideal for production. Additionally, running the model training with TensorFlow within the notebook ties the process to a specific framework, making it less flexible.\nD. Kubeflow pipelines with Spark context: This option gets close, but it's unnecessary to initiate a Spark context within the first component. Dataproc Serverless already handles the Spark environment for running PySpark code."}],"timestamp":"1729494540.0","poster":"fitri001","comment_id":"1199495","upvote_count":"1"},{"content":"Selected Answer: C\nI went with C","comment_id":"1145550","upvote_count":"1","poster":"CHARLIE2108","timestamp":"1723206660.0"},{"comment_id":"1122115","timestamp":"1720904940.0","content":"Selected Answer: C\nVote C","poster":"kalle_balle","upvote_count":"1"},{"timestamp":"1720884660.0","content":"C\nThe best option for creating a scalable and maintainable production process that runs end-to-end and tracks the connections between steps, using prototype code to production, feature engineering code in PySpark that runs on Dataproc Serverless, and model training that is executed by using a Vertex AI custom training job, is to use the Kubeflow pipelines SDK to write code that specifies two components. The first is a Dataproc Serverless component that launches the feature engineering job. The second is a custom component wrapped in the create_custom_training_job_from_component utility that launches the custom model training job. This option allows you to leverage the power and simplicity of Kubeflow pipelines to orchestrate and automate your machine learning workflows on Vertex AI. Kubeflow pipelines is a platform that can build, deploy, and manage machine learning pipelines on Kubernetes.","upvote_count":"1","comment_id":"1121911","poster":"36bdc1e"},{"upvote_count":"3","timestamp":"1720829100.0","comment_id":"1121218","poster":"pikachu007","content":"Selected Answer: C\nBy using Kubeflow Pipelines, you establish a structured, scalable, and maintainable production process for end-to-end model development and deployment, ensuring proper orchestration, tracking, and integration with the chosen services."},{"timestamp":"1720459920.0","content":"I go for C","comment_id":"1116942","poster":"vale_76_na_xxx","upvote_count":"1"}],"answer_description":"","topic":"1","answers_community":["C (100%)"]},{"id":"2zB6bNUTa7Cvn3IPinao","question_text":"You recently deployed a scikit-learn model to a Vertex AI endpoint. You are now testing the model on live production traffic. While monitoring the endpoint, you discover twice as many requests per hour than expected throughout the day. You want the endpoint to efficiently scale when the demand increases in the future to prevent users from experiencing high latency. What should you do?","answer":"B","discussion":[{"comment_id":"1199497","upvote_count":"5","poster":"fitri001","timestamp":"1729494780.0","content":"Selected Answer: B\nAutoscaling based on baseline: Vertex AI endpoints have built-in autoscaling capabilities. Setting a minReplicaCount ensures there are always at least that many replicas running, handling the baseline traffic efficiently. When demand increases above the baseline, autoscaling will automatically provision additional replicas to maintain performance.\nEfficient scaling: This approach allows the endpoint to scale up smoothly as traffic increases, preventing sudden spikes in latency for users.\nTargeted resource allocation: Unlike option A (deploying multiple models), this method avoids redundant resources when traffic is low. Additionally, option D (switching to GPUs) might be unnecessary if the bottleneck isn't processing power.","comments":[{"timestamp":"1729494840.0","upvote_count":"1","content":"A. Deploying multiple models: This creates additional overhead and resource usage without directly addressing autoscaling. Traffic distribution may also not be perfectly even.\nC. Increasing target utilization: Raising the target utilization could lead to under-provisioning during peak hours, causing latency issues. It's better to set a baseline with minReplicaCount and let autoscaling handle peak loads.\nD. Switching to GPUs: While GPUs can be beneficial for computationally intensive models, it might be an unnecessary expense if the current model doesn't heavily utilize the CPU. Analyze the CPU usage before switching to a GPU-based machine type.","comment_id":"1199499","poster":"fitri001"}]},{"upvote_count":"2","content":"Selected Answer: B\nMy Answer B\n\nThe letter C would be the correct answer if the target were set lower to anticipate traffic spikes, not set higher as the answer says. However, considering that the minReplicaCount is now twice the known value, letter B is the most appropriate answer as it suggests considering setting a new minReplicaCount, which could be the best choice.","poster":"guilhermebutzke","timestamp":"1723400460.0","comment_id":"1147642"},{"timestamp":"1722166200.0","comment_id":"1134103","content":"Selected Answer: B\nNot C, if set to a higher value, it is less easier to autoscale to another instance, as it will wait the utilisation to a even higher value.","poster":"Yan_X","upvote_count":"4"},{"content":"Selected Answer: C\nI go with C. It calculates the number of replicas based on CPU utilization. https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1.types.AutoscalingMetricSpec","upvote_count":"1","timestamp":"1721027820.0","comment_id":"1123213","poster":"b1a8fae"},{"upvote_count":"2","comment_id":"1121914","timestamp":"1720884780.0","content":"B\nThis\noption allows you to leverage the power and simplicity of Vertex AI to automatically scale your endpoint resources according to the traffic patterns.","poster":"36bdc1e"},{"content":"Selected Answer: C\nc as it is dynamic","upvote_count":"1","timestamp":"1720829400.0","comments":[{"content":"yes it's dynamic, but the target should be set lower, not higher, if you want to anticipate traffic spikes.","timestamp":"1722336300.0","upvote_count":"2","poster":"sonicclasps","comment_id":"1135785"}],"poster":"pikachu007","comment_id":"1121221"}],"answer_images":[],"topic":"1","question_id":98,"unix_timestamp":1705111800,"timestamp":"2024-01-13 03:10:00","isMC":true,"question_images":[],"answers_community":["B (85%)","C (15%)"],"answer_description":"","choices":{"C":"Set the target utilization percentage in the autoscailngMetricSpecs configuration to a higher value","D":"Change the model’s machine type to one that utilizes GPUs","A":"Deploy two models to the same endpoint, and distribute requests among them evenly","B":"Configure an appropriate minReplicaCount value based on expected baseline traffic"},"url":"https://www.examtopics.com/discussions/google/view/131000-exam-professional-machine-learning-engineer-topic-1-question/","exam_id":13,"answer_ET":"B"},{"id":"gSdWYfGESwhHNZuQTp0R","answer_ET":"A","question_id":99,"url":"https://www.examtopics.com/discussions/google/view/131001-exam-professional-machine-learning-engineer-topic-1-question/","answer":"A","answer_description":"","discussion":[{"content":"Selected Answer: A\nA.\nMinimum effort -> ditch refactoring (hopefully not needed)\nTraining data not available -> can't be skew, so it must be drift","comment_id":"1123296","poster":"b1a8fae","upvote_count":"5","timestamp":"1721036460.0"},{"content":"Selected Answer: A\nTraining data not available -> can't be skew, so it must be drift","upvote_count":"3","poster":"pinimichele01","timestamp":"1728307560.0","comment_id":"1190969"},{"comments":[{"content":"Feature skew is typically used to compare the feature distribution between training data and serving data, which is not as relevant here because you do not have access to the training data. Therefore, Option B is less suitable.","poster":"tavva_prudhvi","timestamp":"1730279700.0","upvote_count":"4","comment_id":"1204384"}],"content":"I have a doubt, could someone please help with this?\nWhile \"drift\" (Option A) might imply gradual changes, \"skew\" (Option B) is more suitable for sudden shifts in feature distributions, potentially relevant for sensitive data.\nIs option B better than A?","timestamp":"1723122300.0","upvote_count":"1","poster":"CHARLIE2108","comment_id":"1144599"},{"upvote_count":"1","comment_id":"1121226","timestamp":"1720829700.0","poster":"pikachu007","content":"Selected Answer: A\nHandles string input format: Vertex AI Model Monitoring can parse comma-separated feature values, avoiding the need to refactor the serving container.\n\n It directly monitors feature distribution over time, aligning with the goal of detecting potential drifts."}],"choices":{"D":"1. Refactor the serving container to accept key-value pairs as input format\n2. Upload the model to Vertex AI Model Registry, and deploy the model to a Vertex AI endpoint\n3. Create a Vertex AI Model Monitoring job with feature skew detection as the monitoring objective","A":"1. Upload the model to Vertex AI Model Registry, and deploy the model to a Vertex AI endpoint\n2. Create a Vertex AI Model Monitoring job with feature drift detection as the monitoring objective, and provide an instance schema","B":"1. Upload the model to Vertex AI Model Registry, and deploy the model to a Vertex AI endpoint\n2. Create a Vertex AI Model Monitoring job with feature skew detection as the monitoring objective, and provide an instance schema","C":"1. Refactor the serving container to accept key-value pairs as input format\n2. Upload the model to Vertex AI Model Registry, and deploy the model to a Vertex AI endpoint\n3. Create a Vertex AI Model Monitoring job with feature drift detection as the monitoring objective."},"question_text":"You work at a bank. You have a custom tabular ML model that was provided by the bank’s vendor. The training data is not available due to its sensitivity. The model is packaged as a Vertex AI Model serving container, which accepts a string as input for each prediction instance. In each string, the feature values are separated by commas. You want to deploy this model to production for online predictions and monitor the feature distribution over time with minimal effort. What should you do?","timestamp":"2024-01-13 03:15:00","question_images":[],"exam_id":13,"topic":"1","unix_timestamp":1705112100,"answer_images":[],"isMC":true,"answers_community":["A (100%)"]},{"id":"qFBGCd5IhyLA6Ow64Kl4","choices":{"B":"Import the TensorFlow model by using the CREATE MODEL statement in BigQuery ML. Apply the historical data to the TensorFlow model","A":"Export the historical data to Cloud Storage in Avro format. Configure a Vertex AI batch prediction job to generate predictions for the exported data","C":"Export the historical data to Cloud Storage in CSV format. Configure a Vertex AI batch prediction job to generate predictions for the exported data","D":"Configure a Vertex AI batch prediction job to apply the model to the historical data in BigQuery"},"answers_community":["B (62%)","D (24%)","14%"],"question_id":100,"answer_images":[],"answer_ET":"B","topic":"1","isMC":true,"answer_description":"","discussion":[{"content":"Selected Answer: D\nManaged Service: Vertex AI batch prediction","upvote_count":"1","poster":"NamitSehgal","comment_id":"1357534","timestamp":"1739756280.0"},{"comment_id":"1323309","timestamp":"1733619960.0","upvote_count":"1","content":"Selected Answer: A\nA) - BigQuery ML is not designed for the scale of a 10TB dataset\n- Batch Prediction performs efficient batch inference on large GCS datasets\n- AVRO is a binary format, more compact and efficient to process than CSV\n *B uses BQML; C uses CSV format; exporting to GCS is more efficient than performing Vertex AI predictions directly on BQ for this volumetry.","poster":"lunalongo"},{"comments":[{"content":"Not true at all\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-tensorflow","comment_id":"1325071","timestamp":"1733929800.0","upvote_count":"2","poster":"Omi_04040"}],"upvote_count":"1","comment_id":"1321174","poster":"rajshiv","content":"Selected Answer: A\nIt should be A. The \"CREATE MODEL\" statement in BigQuery ML is meant for BigQuery-specific models, and do not support models like TensorFlow SavedModel out of the box. This option would not work for using a TensorFlow model stored in Cloud Storage.","timestamp":"1733191620.0"},{"content":"My answer is D.","poster":"Foxy2021","timestamp":"1728670500.0","upvote_count":"1","comment_id":"1296203"},{"content":"Selected Answer: B\nhttps://cloud.google.com/vertex-ai/docs/tabular-data/classification-regression/get-batch-predictions#input_data_requirements","timestamp":"1712496660.0","poster":"pinimichele01","upvote_count":"1","comment_id":"1190974"},{"poster":"edoo","upvote_count":"4","content":"Selected Answer: B\nThe choice is between B and D, both good BUT:\nImporting and making batch predictions is quite straightforward in BQ ML\nhttps://cloud.google.com/bigquery/docs/making-predictions-with-imported-tensorflow-models\nif not pre-processing needed on the data. If we need a more complete pipeline I'd chose D, but the tables need partitioning (100GB is the limit in Vertex AI):\nhttps://cloud.google.com/vertex-ai/docs/tabular-data/classification-regression/get-batch-predictions#input_data_requirements","comment_id":"1168756","timestamp":"1709897940.0"},{"timestamp":"1708277520.0","poster":"guilhermebutzke","content":"Selected Answer: D\nMy Answer: D \n\nThe historical dataset is stored in BigQuery, which can be directly accessed by Vertex AI. Vertex AI offers batch prediction capabilities, allowing you to apply the model to the data stored in BigQuery without the need to export it. So, This approach leverages the scalability of Google Cloud infrastructure and avoids unnecessary data movement, being not necessary to export data to Cloud Store (options A and C), nor Import the TensorFlow model to BQ (option B).","comment_id":"1153453","upvote_count":"1"},{"comment_id":"1137423","poster":"ddogg","timestamp":"1706778480.0","content":"Selected Answer: B\nhttps://cloud.google.com/bigquery/docs/making-predictions-with-imported-tensorflow-models#:~:text=Import%20TensorFlow%20models,-To%20import%20TensorFlow&text=In%20the%20Google%20Cloud%20console%2C%20go%20to%20the%20BigQuery%20page.&text=In%20the%20query%20editor%2C%20enter,MODEL%20statement%20like%20the%20following.&text=The%20preceding%20query%20imports%20a,BigQuery%20ML%20model%20named%20imported_tf_model%20.","upvote_count":"2"},{"upvote_count":"2","poster":"sonicclasps","content":"Selected Answer: B\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-tensorflow#limitations","comment_id":"1135794","timestamp":"1706619300.0"},{"upvote_count":"4","poster":"Zwi3b3l","content":"Selected Answer: B\nHas to be B, because D has limitations:\nBigQuery data source tables must be no larger than 100 GB.\nhttps://cloud.google.com/vertex-ai/docs/tabular-data/classification-regression/get-batch-predictions#input_data_requirements","timestamp":"1706450340.0","comment_id":"1134125"},{"comments":[{"content":"i mean B","timestamp":"1705938480.0","comment_id":"1128848","upvote_count":"1","poster":"BlehMaks"}],"poster":"BlehMaks","timestamp":"1705938420.0","upvote_count":"1","comment_id":"1128846","content":"Selected Answer: A\nSame platform as data == less computation required to load and pass it to model"},{"comment_id":"1123305","timestamp":"1705319520.0","poster":"b1a8fae","upvote_count":"2","content":"Selected Answer: D\nIt could either be B or D. It seems like most of the limitations of B are mentioned in the problem (https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-tensorflow#limitations) but some of them are not and we are left questioning if the model will match the remaining requirements.\n\nTherefore, I would go for D, which can import data from BigQuery. https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions#bigquery"},{"content":"Selected Answer: D\nLimitations of other options:\n\nA and C. Exporting data: Exporting 10 TB of data to Cloud Storage incurs additional storage costs, transfer time, and potential data management complexities.\nB. BigQuery ML: While BigQuery ML supports some TensorFlow models, it might have limitations with certain model architectures or features. Additionally, it might not be as optimized for large-scale batch inference as Vertex AI.","upvote_count":"1","comment_id":"1121236","poster":"pikachu007","timestamp":"1705113420.0"}],"exam_id":13,"url":"https://www.examtopics.com/discussions/google/view/131005-exam-professional-machine-learning-engineer-topic-1-question/","answer":"B","question_text":"You are implementing a batch inference ML pipeline in Google Cloud. The model was developed using TensorFlow and is stored in SavedModel format in Cloud Storage. You need to apply the model to a historical dataset containing 10 TB of data that is stored in a BigQuery table. How should you perform the inference?","unix_timestamp":1705113420,"timestamp":"2024-01-13 03:37:00","question_images":[]}],"exam":{"isBeta":false,"provider":"Google","numberOfQuestions":304,"isMCOnly":true,"id":13,"isImplemented":true,"name":"Professional Machine Learning Engineer","lastUpdated":"11 Apr 2025"},"currentPage":20},"__N_SSP":true}