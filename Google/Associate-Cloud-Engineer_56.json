{"pageProps":{"questions":[{"id":"X2gOzmV7Cpx1SuVny8D9","answer_ET":"B","timestamp":"2020-06-12 13:25:00","answer_description":"","unix_timestamp":1591961100,"answer_images":[],"choices":{"A":"Add a bucket lifecycle rule that archives data with newer versions after 30 days to Coldline Storage.","D":"Add a bucket lifecycle rule that archives data from regional storage after 30 days to Nearline Storage.","C":"Add a bucket lifecycle rule that archives data from regional storage after 30 days to Coldline Storage.","B":"Add a bucket lifecycle rule that archives data with newer versions after 30 days to Nearline Storage."},"url":"https://www.examtopics.com/discussions/google/view/22961-exam-associate-cloud-engineer-topic-1-question-90-discussion/","question_text":"You want to configure a solution for archiving data in a Cloud Storage bucket. The solution must be cost-effective. Data with multiple versions should be archived after 30 days. Previous versions are accessed once a month for reporting. This archive data is also occasionally updated at month-end. What should you do?","isMC":true,"answers_community":["B (100%)"],"exam_id":1,"discussion":[{"comment_id":"108619","timestamp":"1607779500.0","upvote_count":"33","poster":"neelesh88","content":"B is correct"},{"content":"Correct Answer (B):\n\nNumberOfNewerVersions\nThe NumberOfNewerVersions condition is typically only used in conjunction with Object Versioning. If the value of this condition is set to N, an object version satisfies the condition when there are at least N versions (including the live version) newer than it. For a live object version, the number of newer versions is considered to be 0. For the most recent noncurrent version, the number of newer versions is 1 (or 0 if there is no live object version), and so on.\n\nImportant: When specifying this condition in a .json configuration file, you must use numNewerVersions instead of NumberOfNewerVersions.\n\nhttps://cloud.google.com/storage/docs/lifecycle#numberofnewerversions","poster":"ESP_SAP","upvote_count":"24","comment_id":"164795","timestamp":"1614138360.0"},{"upvote_count":"2","timestamp":"1714987980.0","poster":"BAofBK","content":"The correct answer is B","comment_id":"1063787"},{"poster":"Captain1212","upvote_count":"4","timestamp":"1709379060.0","comment_id":"996770","content":"Selected Answer: B\nB is the right answer, because of data is accessing infrequently and nearline storage is good for it"},{"content":"Selected Answer: B\nB is correct","poster":"SanjeevKumar1983","upvote_count":"1","comment_id":"992653","timestamp":"1709167800.0"},{"content":"Correct ans is A.\n\nExplanation: In this scenario, you need to archive data after 30 days, which implies that the data with multiple versions is considered for archiving. Since you need to access previous versions once a month for reporting, using Coldline Storage is the most cost-effective option.","upvote_count":"1","poster":"jayjani66","timestamp":"1705817100.0","comment_id":"958035","comments":[{"poster":"omunoz","timestamp":"1723564260.0","upvote_count":"1","content":"Retrieval fees is more expensive in Coldline.\n\nStandard storage Nearline storage Coldline storage Archive storage\n$0 per GB $0.01 per GB $0.02 per GB $0.05 per GB","comment_id":"1149434"}]},{"comments":[{"comment_id":"1082012","upvote_count":"1","content":"the logic is simple :) i agree with you","poster":"kelliot","timestamp":"1716846660.0"}],"content":"Selected Answer: B\nsince accessed frequently it will be nearline","poster":"Partha117","timestamp":"1695466380.0","comment_id":"848215","upvote_count":"3"},{"poster":"Buruguduystunstugudunstuy","comment_id":"818389","upvote_count":"10","content":"Selected Answer: B\nAnswer B, adding a bucket lifecycle rule that archives data with newer versions after 30 days to Nearline Storage, is the correct answer for this scenario.\n\nNearline Storage is designed for data that is accessed less frequently, such as for backup and archival purposes. It has a minimum storage duration of 30 days, which makes it suitable for archiving data that needs to be kept for a long time but is accessed infrequently. Additionally, Nearline Storage has lower storage costs than Coldline Storage, making it more cost-effective for this use case.\n\nBy adding a bucket lifecycle rule that archives data with newer versions after 30 days to Nearline Storage, you can ensure that the data is automatically moved to a more cost-effective storage class while still being easily accessible for reporting purposes.","timestamp":"1692729840.0","comments":[{"comments":[{"upvote_count":"5","poster":"chikorita","comment_id":"872722","timestamp":"1697548620.0","content":"just FYI that my lord, @Buruguduystunstugudunstuy, is always right!!!!1"},{"comment_id":"850560","timestamp":"1695678480.0","poster":"eaakgul","upvote_count":"1","content":"The question tells us that the previous versions are accessed once a month for reporting. So, nearline makes more sense in this case. 'Buruguduystunstugudunstuy' has mentioned that nearline has lower storage costs for only this 'use case'"}],"comment_id":"848494","timestamp":"1695482760.0","upvote_count":"4","poster":"dnur","content":"You're incorrect. Coldline storage has a lower costs that Nearline Storage. https://cloud.google.com/storage/docs/storage-classes."}]},{"poster":"leogor","upvote_count":"1","timestamp":"1683876840.0","comment_id":"716608","content":"Selected Answer: B\narchives data with newer versions after 30 days to Nearline Storage."},{"timestamp":"1679219100.0","content":"Selected Answer: B\nB should be correct:\n\nNearline has min storage of 30 days, while Coldline has 90 days.\n\nSince \"archive data is also occasionally updated at month-end\", updating object before min storage period is allowed but causes early deletion fees as if the object was stored for the min duration, so using Coldline will always charge for 90 days and not likely to save cost.\n\nhttps://cloud.google.com/storage/pricing#early-delete","upvote_count":"1","comment_id":"673042","poster":"kadc"},{"comments":[{"timestamp":"1671825240.0","content":"regional storage after 30 days to Nearline Storage option is trick you :)","comment_id":"621184","poster":"AzureDP900","upvote_count":"1"}],"upvote_count":"1","content":"B is right and straight forward.","timestamp":"1671825240.0","comment_id":"621183","poster":"AzureDP900"},{"timestamp":"1670201100.0","upvote_count":"1","poster":"haroldbenites","comment_id":"611599","content":"Go for B"},{"poster":"Rukman","upvote_count":"1","timestamp":"1663397700.0","comment_id":"569579","content":"Selected Answer: B\nB is correct"},{"upvote_count":"2","content":"B is perfect","comment_id":"495166","poster":"Vidyaji","timestamp":"1654516080.0"},{"content":"B. Add a bucket lifecycle rule that archives data with newer versions after 30 days to Nearline Storage.","comment_id":"482531","upvote_count":"1","timestamp":"1653042900.0","poster":"vishnukumartr"},{"upvote_count":"1","content":"B. Add a bucket lifecycle rule that archives data with newer versions after 30 days to Nearline Storage.","comment_id":"481867","poster":"shawnkkk","timestamp":"1652966400.0"},{"timestamp":"1646054280.0","content":"Agree B is the correct","upvote_count":"2","poster":"Chotebhaisahab","comment_id":"433797"},{"comment_id":"430911","poster":"Rahul183","timestamp":"1645728120.0","upvote_count":"1","content":"B- https://cloud.google.com/storage/docs/storage-classes"},{"comment_id":"366031","timestamp":"1637814000.0","poster":"jcols","content":"B is correct. C and D don't mention newer object versions. Then, it's Coldline vs. Nearline, the best option is Nearline because:\n* \"Previous versions are accessed once a month for reporting\"\n* \"Nearline Storage is ideal for data you plan to read or modify on average once per month or less. For example, if you want to continuously add files to Cloud Storage and plan to access those files once a month for analysis, Nearline Storage is a great choice.\" -- https://cloud.google.com/storage/docs/storage-classes#nearline","comments":[{"upvote_count":"1","poster":"zerozero7","comment_id":"719257","timestamp":"1684193040.0","content":"jcols ++"}],"upvote_count":"6"},{"poster":"mcaromit","timestamp":"1636804920.0","content":"B is correct","comment_id":"356238","upvote_count":"1"},{"comment_id":"320016","upvote_count":"2","content":"B is correct. Add a bucket lifecycle rule that archives data with newer versions after 30 days to Nearline Storage.","timestamp":"1632555960.0","poster":"[Removed]"},{"timestamp":"1631259000.0","upvote_count":"1","content":"B is correct","poster":"EABDAJA","comment_id":"307054"},{"content":"B. Add a bucket lifecycle rule that archives data with newer versions after 30 days to Nearline Storage.","poster":"GCP_Student1","timestamp":"1629726120.0","upvote_count":"2","comment_id":"297500"},{"timestamp":"1629246420.0","comment_id":"293018","content":"\"Previous versions are accessed once a month for reporting\"\n\nonly option C and D says, that you are storing the data in \"regional\" storage and then trying to move from either \"coldline\" or \"nearline\"\n\nso, i will go with option D. to hold the data which is accessed once a month in \"regional\" storage and then after 30 days i archive it and move to \"nearline\"\n\nalso note, question says you occasionally update the archive data. so near-line would be right choice.","poster":"arsav","upvote_count":"3"},{"timestamp":"1629140580.0","comments":[{"poster":"Buruguduystunstugudunstuy","upvote_count":"1","comments":[{"comment_id":"818410","poster":"Buruguduystunstugudunstuy","content":"Answer D, which suggests archiving data from regional storage after 30 days to Nearline Storage, is also a viable solution. However, it adds an extra step of first transitioning data to regional storage before archiving to Nearline Storage. This may not be necessary if the data is already stored in regional storage.","upvote_count":"1","timestamp":"1692731160.0"}],"content":"Both Answer B and Answer D are cost-effective solutions for archiving data in a Cloud Storage bucket. Both options have their advantages and disadvantages, and the best option would depend on the specific needs and requirements of the organization.\n\nI would choose Answer B: Add a bucket lifecycle rule that archives data with newer versions after 30 days to Nearline Storage.\n\nThis option satisfies the requirements of the scenario by archiving data with newer versions after 30 days to a cost-effective storage option, Nearline Storage. Nearline Storage has a minimum storage duration of 30 days and is designed for data that is accessed less frequently, making it a good fit for this scenario where data is accessed once a month for reporting. Additionally, since the data is occasionally updated at month-end, Nearline Storage provides the flexibility to quickly restore the archived data for updates before the next reporting cycle.","timestamp":"1692731100.0","comment_id":"818409"},{"poster":"user843983409","upvote_count":"3","timestamp":"1632161340.0","content":"D doesnt refer to the \"data with versions\" hence it applies to all data which is not what is asked in the question. My vote for B","comment_id":"315849"}],"upvote_count":"2","poster":"nitinz","comment_id":"292125","content":"I disagree with all of you, correct answer is D. The key line in question is \" The solution must be cost-effective.\" When you are doing multi-region bucket then you pay for data at rest per location. It will be nearline storage but single region."},{"upvote_count":"2","content":"B. Add a bucket lifecycle rule that archives data with newer versions after 30 days to Nearline Storage.","comment_id":"281061","poster":"DucSiu","timestamp":"1627789260.0"},{"timestamp":"1624890780.0","comments":[{"upvote_count":"2","content":"D, question says cost effective solution.... you are paying for data at rest per location.","comment_id":"292126","timestamp":"1629140640.0","poster":"nitinz"}],"content":"Key Requirements - \n1. Data with ***multiple versions*** should be archived after 30 days.\n2. Previous versions are accessed once a month for reporting. This archive data is also occasionally updated at month-end.\n\n\nA. ...Coldline Storage... Not suitable because data is accessed once a month\n\nB. Add a bucket lifecycle rule that archives data with newer versions after 30 days to Nearline Storage.\nThis is the right option\nRef - https://cloud.google.com/storage/docs/storage-classes#nearline and \nhttps://cloud.google.com/storage/docs/lifecycle\n\nC. ...Coldline Storage....Not suitable because data is accessed once a month\n\nD. ...regional storage after 30 days to Nearline Storage... Nothing is mentioned about regional storage in the question. where did we get this from in the options? Not suitable","comment_id":"254188","upvote_count":"5","poster":"mohdafiuddin"},{"comment_id":"221600","upvote_count":"2","poster":"swatititame","timestamp":"1621309260.0","content":"• B. Add a bucket lifecycle rule that archives data with newer versions after 30 days to Nearline Storage."},{"timestamp":"1618229760.0","comment_id":"198396","upvote_count":"5","content":"B. Add a bucket lifecycle rule that archives data with newer versions after 30 days to Nearline Storage.","poster":"glam"},{"timestamp":"1608538680.0","upvote_count":"2","content":"B is correct answer","comment_id":"115287","poster":"Ciumela"},{"poster":"nithinpb180","content":"B is correct, mentioned that data with multiple versions need to be archived.","upvote_count":"2","timestamp":"1608099660.0","comment_id":"111302"},{"content":"I think C is correct answer because archival data","timestamp":"1607918160.0","comment_id":"109845","comments":[{"poster":"[Removed]","content":"C would be correct if Archival data would not be accessed, as cold line have retention of 90 days.","comment_id":"177721","upvote_count":"1","timestamp":"1615480800.0"},{"upvote_count":"5","timestamp":"1608559800.0","comment_id":"115554","poster":"raffiq","content":"sorry, its Answer B Nearline storage is correct answer."}],"upvote_count":"1","poster":"raffiq"}],"question_images":[],"topic":"1","question_id":276,"answer":"B"},{"id":"E24uiTGl57e0RhnEwqNX","timestamp":"2020-06-04 23:40:00","answers_community":["D (100%)"],"question_text":"Your company's infrastructure is on-premises, but all machines are running at maximum capacity. You want to burst to Google Cloud. The workloads on Google\nCloud must be able to directly communicate to the workloads on-premises using a private IP range. What should you do?","answer_ET":"D","question_images":[],"topic":"1","answer_images":[],"unix_timestamp":1591306800,"answer":"D","answer_description":"","question_id":277,"exam_id":1,"choices":{"C":"Create bastion hosts both in your on-premises environment and on Google Cloud. Configure both as proxy servers using their public IP addresses.","B":"In Google Cloud, configure the VPC for VPC Network Peering.","D":"Set up Cloud VPN between the infrastructure on-premises and Google Cloud.","A":"In Google Cloud, configure the VPC as a host for Shared VPC."},"discussion":[{"poster":"SIX","upvote_count":"57","content":"I believe D is the right answer","comments":[{"comment_id":"104048","upvote_count":"2","content":"B is correct - https://cloud.google.com/solutions/best-practices-vpc-design . this answer also on all machines are running at maximum capacity.","comments":[{"timestamp":"1690869360.0","comment_id":"640365","content":"\"Google Cloud VPC Network Peering allows internal IP address connectivity across two Virtual Private Cloud (VPC) networks regardless of whether they belong to the same project or the same organization.\" \nhttps://cloud.google.com/vpc/docs/vpc-peering\n\nwhile \n\"Cloud Interconnect provides low latency, high availability connections that enable you to reliably transfer data between your on-premises and Google Cloud Virtual Private Cloud (VPC) networks.\"\nhttps://cloud.google.com/network-connectivity/docs/interconnect/concepts/overview\n\nand\n\"HA VPN is a high-availability (HA) Cloud VPN solution that lets you securely connect your on-premises network to your VPC network through an IPsec VPN connection in a single region.\"\nhttps://cloud.google.com/network-connectivity/docs/vpn/concepts/overview\n\nso, cloud vpn is the best answer for the question requirement","poster":"xharf","upvote_count":"7"},{"comment_id":"104878","content":"vpc network peering does not connect to on-prem. Cloud VPN is the correct solution. https://cloud.google.com/vpn/docs/concepts/overview","poster":"JustLearning","timestamp":"1623103740.0","comments":[{"content":"You need VPN, so D is the correct. VPC network peering is between VPCs.","upvote_count":"15","poster":"mlantonis","comment_id":"106121","timestamp":"1623256560.0"}],"upvote_count":"27"}],"timestamp":"1623005520.0","poster":"dan80"}],"comment_id":"102660","timestamp":"1622842800.0"},{"upvote_count":"35","timestamp":"1629248700.0","poster":"ESP_SAP","comment_id":"160439","content":"Correct Answer is (D):\n\nAccess internal IPs directly\nYour VPC network's internal (RFC 1918) IP addresses are directly accessible from your on-premises network with peering, no NAT device or VPN tunnel required.\n\nHybrid made easy\nToday’s business climate demands flexibility. Connecting your on-premises resources to your cloud resources seamlessly, with minimum latency or interruption, is a business-critical requirement. The speed and reliability of Cloud Interconnect lets you extend your organization’s data center network into Google Cloud, simply and easily, while options such as Cloud VPN provide flexibility for all your workloads. This unlocks the potential of hybrid app development and all the benefits the cloud has to offer.\n\nIn the graphic below: What GCP Connection is right for you? shows clearly what is the method for extend your on premise network (IP Private communication).\nWhat GCP Connection is right for you?\nhttps://cloud.google.com/hybrid-connectivity"},{"poster":"BAofBK","upvote_count":"1","content":"The correct answer is D","comment_id":"1063791","timestamp":"1730893080.0"},{"poster":"Captain1212","timestamp":"1725340980.0","comment_id":"997329","content":"Selected Answer: D\nD is the right answer as they need the private range and the machine are also on high working load","upvote_count":"4"},{"comment_id":"908411","poster":"rosapersiani","timestamp":"1716882180.0","content":"Selected Answer: D\nd is right","upvote_count":"1"},{"content":"Selected Answer: D\nVPN to connect your on-premise network to the cloud","timestamp":"1713348000.0","comment_id":"872573","upvote_count":"2","poster":"sabrinakloud"},{"poster":"Partha117","content":"Selected Answer: D\nVPN for on premise connection to GCP","timestamp":"1711198320.0","comment_id":"848214","upvote_count":"2"},{"comment_id":"818420","upvote_count":"8","poster":"Buruguduystunstugudunstuy","timestamp":"1708636320.0","content":"Selected Answer: D\nAnswer D. Set up Cloud VPN between the infrastructure on-premises and Google Cloud.\n\nTo burst into Google Cloud from the on-premises infrastructure, a VPN connection can be established between the on-premises network and Google Cloud. VPN provides a secure, private tunnel to transfer data between on-premises infrastructure and Google Cloud. Cloud VPN would allow workloads on Google Cloud to communicate with workloads on-premises over private IP addresses, making it a suitable option for this scenario. \n\nAnswer A (Shared VPC) and Answer B (VPC Network Peering) do not address the requirement of communicating over a private IP range between on-premises and Google Cloud. \n\nAnswer C (bastion hosts) involves the use of public IP addresses, which may not be suitable for a private, secure connection."},{"content":"Selected Answer: D\nD is the right answer","poster":"cslince","timestamp":"1702021980.0","comment_id":"738746","upvote_count":"1"},{"upvote_count":"1","poster":"leogor","timestamp":"1699781880.0","comment_id":"716611","content":"Selected Answer: D\nCloud VPN"},{"upvote_count":"1","comment_id":"680267","poster":"VaneA","content":"Selected Answer: D\nIt is the answer","timestamp":"1695773640.0"},{"timestamp":"1688319780.0","comment_id":"626258","poster":"RanjithK","upvote_count":"1","content":"Selected Answer: D\nGo with D"},{"upvote_count":"1","timestamp":"1687543440.0","poster":"AzureDP900","content":"D is right answer","comment_id":"621192"},{"content":"Cloud VPN is way to establish connection between on prem to cloud . D is correct.","timestamp":"1687542960.0","poster":"AzureDP900","comment_id":"621186","upvote_count":"2"},{"content":"Selected Answer: D\nD is the right answer.","upvote_count":"2","comment_id":"522731","timestamp":"1673600400.0","poster":"Uqqasha"},{"upvote_count":"3","comment_id":"504255","content":"On-premise -> GCP \nThere are 2 ways\n1. Cloud VPN\n2. Interconnect\nSince we have VPN as an option, others is not recommended","timestamp":"1671372840.0","poster":"ARVII"},{"poster":"Vidyaji","timestamp":"1670334480.0","upvote_count":"1","content":"D is perfect","comment_id":"495167"},{"timestamp":"1668947760.0","content":"D. Set up Cloud VPN between the infrastructure on-premises and Google Cloud.","poster":"vishnukumartr","comment_id":"482532","upvote_count":"1"},{"poster":"alaahakim","timestamp":"1668887640.0","content":"Ans : D","comment_id":"482054","upvote_count":"1"},{"timestamp":"1661685660.0","upvote_count":"2","content":"yes VPN is needed. D is correct","comment_id":"433802","poster":"Chotebhaisahab"},{"upvote_count":"1","timestamp":"1661277300.0","comment_id":"430231","poster":"nenoAZ","content":"D is correct. 100%."},{"upvote_count":"1","poster":"mcaromit","timestamp":"1652446980.0","content":"D is correct","comment_id":"356409"},{"timestamp":"1649565840.0","comment_id":"332294","content":"Not A and B, because for network peering, shared VPC you need to either be in the same project or same organization. Here we are in on-premise and cloud.\nMaybe C, but why do you need an extra server on both GCP and On-premise for a proxy?. We already have solutions for proxy inside GCP.\nD is correct because, we generally connect GCP to on-premise using VPN","upvote_count":"4","poster":"[Removed]"},{"comment_id":"320019","poster":"[Removed]","content":"D is correct. Set up Cloud VPN between the infrastructure on-premises and Google Cloud.","timestamp":"1648201620.0","upvote_count":"2"},{"timestamp":"1646490420.0","upvote_count":"1","comment_id":"304208","content":"D is correct since VPC peering only works on GCP cloud network","poster":"Hi2ALL"},{"poster":"GCP_Student1","comment_id":"297694","content":"D. Set up Cloud VPN between the infrastructure on-premises and Google Cloud.","upvote_count":"2","timestamp":"1645645500.0"},{"comment_id":"281062","poster":"DucSiu","timestamp":"1643694060.0","upvote_count":"2","content":"D. Set up Cloud VPN between the infrastructure on-premises and Google Cloud."},{"timestamp":"1643559780.0","upvote_count":"1","content":"D is the right answer","comment_id":"280033","poster":"rvgcp"},{"poster":"LearningGCP","upvote_count":"1","content":"D correct Answer","comment_id":"278926","timestamp":"1643419800.0"},{"timestamp":"1643171760.0","comment_id":"276591","poster":"INASR","content":"D is correct","upvote_count":"2"},{"poster":"victory108","upvote_count":"3","comment_id":"275099","timestamp":"1643012160.0","content":"D - Set up Cloud VPN between the infrastructure on-premises and Google Cloud."},{"comment_id":"221604","upvote_count":"3","poster":"swatititame","timestamp":"1637214420.0","content":"• D. Set up Cloud VPN between the infrastructure on-premises and Google Cloud."},{"upvote_count":"2","comment_id":"218259","timestamp":"1636774980.0","content":"I will go for D. VPN is required to connect to on-premises network.","poster":"gcpace"},{"comment_id":"217645","upvote_count":"2","poster":"awscloudgeek","timestamp":"1636684500.0","content":"D is the right answer"},{"content":"Network peering is for connection between VPC\nVote for D","upvote_count":"3","comment_id":"207779","timestamp":"1635418740.0","poster":"nwk"},{"comment_id":"198407","timestamp":"1634041560.0","poster":"glam","content":"D. Set up Cloud VPN between the infrastructure on-premises and Google Cloud.","upvote_count":"7"},{"poster":"GopinathM","comment_id":"183076","timestamp":"1632150960.0","content":"B is Correct as per understanding","comments":[{"content":"Answer B (VPC Network Peering) does not address the requirement of communicating over a private IP range between on-premises and Google Cloud.","comment_id":"818424","upvote_count":"1","timestamp":"1708636440.0","poster":"Buruguduystunstugudunstuy"},{"content":"Your understanding is wrong....it is clearly D","comment_id":"346863","timestamp":"1651403040.0","poster":"Rightsaidfred","upvote_count":"1"},{"timestamp":"1636476360.0","content":"You understand wrong, Peering in for cloud networds connect (VPCs). You need VPN or interconnet to on-prem.","upvote_count":"6","comment_id":"216082","poster":"Eshkrkrkr"}],"upvote_count":"1"},{"poster":"szakaria","content":"In this case only VPN tunnel or interconnect are possible solutions. Hence, D is the correct answer.","comment_id":"136768","upvote_count":"4","timestamp":"1626473460.0"},{"upvote_count":"3","timestamp":"1624647540.0","comment_id":"119726","poster":"ahmed812","content":"https://cloud.google.com/hybrid-connectivity"},{"comment_id":"115291","content":"D is correct","upvote_count":"5","timestamp":"1624256400.0","poster":"Ciumela"}],"isMC":true,"url":"https://www.examtopics.com/discussions/google/view/22127-exam-associate-cloud-engineer-topic-1-question-91-discussion/"},{"id":"p2rh4M0uj3b5wULyvHqF","choices":{"A":"Select Multi-Regional Storage. Add a bucket lifecycle rule that archives data after 30 days to Coldline Storage.","B":"Select Multi-Regional Storage. Add a bucket lifecycle rule that archives data after 30 days to Nearline Storage.","D":"Select Regional Storage. Add a bucket lifecycle rule that archives data after 30 days to Coldline Storage.","C":"Select Regional Storage. Add a bucket lifecycle rule that archives data after 30 days to Nearline Storage."},"answer_images":[],"isMC":true,"answer_description":"","timestamp":"2020-06-04 23:45:00","answer_ET":"D","exam_id":1,"question_id":278,"unix_timestamp":1591307100,"topic":"1","url":"https://www.examtopics.com/discussions/google/view/22128-exam-associate-cloud-engineer-topic-1-question-92-discussion/","question_images":[],"discussion":[{"upvote_count":"46","content":"D\nGoogle Cloud Coldline is a new cold-tier storage for archival data with access frequency of less than once per year. Unlike other cold storage options, Nearline has no delays prior to data access, so now it is the leading solution among competitors.","comments":[{"comment_id":"104051","comments":[{"upvote_count":"18","timestamp":"1652094960.0","poster":"lxgywil","content":"The answer is D.\n\nThe main thing here is how often the data is retrieved. The question is saying that data needs to be accessed annually - i.e. once a year. Therefore, you should choose Coldline Storage, as it implies less frequent access than Nearline. (Archival Storage would fit even better but there's no such option)\n\nFrom the link you provided:\n\n\"Nearline Storage is ideal for data you plan to read or modify on average once per month or less.\"\n\n and\n\n\"Coldline Storage is ideal for data you plan to read or modify at most once a quarter. \"","comment_id":"352971"},{"upvote_count":"8","timestamp":"1623818640.0","comment_id":"111311","poster":"nithinpb180","content":"That is the minimum storage duration. I would go with D. Coldline storage is more suitable for infrequent data access."},{"poster":"UtsavDM","comment_id":"438954","content":"I get what you are trying to say but treat coldline storage as archival storage in this case. Option D is correct.","timestamp":"1662274440.0","upvote_count":"2"},{"upvote_count":"4","timestamp":"1636389900.0","content":"you have misunderstood minimum storage period here, nearline storage class minimum you have to plan for 30 days","poster":"gh999l","comment_id":"215372"},{"content":"D, It is saying AFTER 30 days. We should use coldline storage","timestamp":"1634324520.0","comments":[{"content":"not really, the key word is: annual access","timestamp":"1743049260.0","poster":"Phat","comment_id":"1410735","upvote_count":"1"}],"comment_id":"200714","upvote_count":"5","poster":"y2kniel"},{"upvote_count":"2","timestamp":"1631372400.0","content":"30 day is min retention policy any access before this is chargeable. Here they are saying the archival data is accessed after 1 yr so cold line. The 30 days statement is to archive data after 30 days.","comment_id":"177731","poster":"[Removed]"},{"upvote_count":"1","content":"dan80 is right","timestamp":"1623256380.0","comment_id":"106119","poster":"mlantonis"}],"poster":"dan80","timestamp":"1623005760.0","upvote_count":"12","content":"C is correct - This data is archived after 30 days - Nearline Storage 30 days , Coldline Storage 90 days https://cloud.google.com/storage/docs/storage-classes"},{"timestamp":"1623104640.0","content":"D is correct. Coldline is a better choice.","comment_id":"104883","poster":"JustLearning","upvote_count":"9"}],"timestamp":"1622843100.0","poster":"SIX","comment_id":"102665"},{"upvote_count":"16","poster":"ESP_SAP","comment_id":"160451","timestamp":"1629250380.0","comments":[{"content":"CORRECTION.\nCorrect Answer is (D):\n\nThe Real description is about Coldline storage Class:\n\nColdline Storage\nColdline Storage is a very-low-cost, highly durable storage service for storing infrequently accessed data. Coldline Storage is a better choice than Standard Storage or Nearline Storage in scenarios where slightly lower availability, a 90-day minimum storage duration, and higher costs for data access are acceptable trade-offs for lowered at-rest storage costs.\n\nColdline Storage is ideal for data you plan to read or modify at most once a quarter. Note, however, that for data being kept entirely for backup or archiving purposes, Archive Storage is more cost-effective, as it offers the lowest storage costs.\n\nhttps://cloud.google.com/storage/docs/storage-classes#coldline","timestamp":"1629770520.0","poster":"ESP_SAP","comment_id":"164805","upvote_count":"13"}],"content":"Correct Answer is (D):\n\nhttps://cloud.google.com/storage/docs/storage-classes\n\nNearline Storage\nNearline Storage is a low-cost, highly durable storage service for storing infrequently accessed data. Nearline Storage is a better choice than Standard Storage in scenarios where slightly lower availability, a 30-day minimum storage duration, and costs for data access are acceptable trade-offs for lowered at-rest storage costs.\n\nNearline Storage is ideal for data you plan to read or modify on average once per month or less. For example, if you want to continuously add files to Cloud Storage and plan to access those files once a month for analysis, Nearline Storage is a great choice.\n\nNearline Storage is also appropriate for data backup, long-tail multimedia content, and data archiving. Note, however, that for data accessed less frequently than once a quarter, Coldline Storage or Archive Storage are more cost-effective, as they offer lower storage costs.\nhttps://cloud.google.com/storage/docs/storage-classes#nearline"},{"timestamp":"1732751640.0","upvote_count":"2","comment_id":"1082013","content":"D is correct.\n\"from one geographic location\" clues the answer","poster":"kelliot"},{"comment_id":"1063796","poster":"BAofBK","upvote_count":"1","timestamp":"1730893380.0","content":"The correct answer is D"},{"timestamp":"1725341100.0","comment_id":"997330","upvote_count":"2","content":"Selected Answer: D\nD is the correrct answer, as the data in access only once a year","poster":"Captain1212"},{"poster":"sabrinakloud","content":"Selected Answer: D\n\"This data is archived after 30 days and needs to be accessed annually\"\nideally archive; coldine is the closest.","timestamp":"1713348120.0","comment_id":"872575","upvote_count":"1"},{"timestamp":"1712373960.0","comment_id":"862669","poster":"Elya","upvote_count":"1","content":"The best option would be to select Regional Storage and add a bucket lifecycle rule that archives data after 30 days to Nearline Storage. Nearline Storage is designed for data that is accessed less frequently, but still needs to be readily available when accessed. It has a lower storage cost than Regional Storage, and retrieval costs are lower than those of Coldline Storage."},{"poster":"inbalinbal","comment_id":"860131","upvote_count":"2","content":"Selected Answer: D\nD is correct","timestamp":"1712161560.0"},{"upvote_count":"4","comment_id":"818444","poster":"Buruguduystunstugudunstuy","content":"Selected Answer: D\nAnswer D is the CORRECT answer. The scenario mentioned in the question requires archiving data after 30 days and accessing it annually. As per the Cloud Storage documentation, Coldline storage is ideal for data that is accessed at most once a quarter. Hence, selecting regional storage and adding a bucket lifecycle rule that archives data after 30 days to Coldline Storage is the best solution to meet the compliance objectives and cost-effectiveness requirements.","comments":[{"poster":"Buruguduystunstugudunstuy","timestamp":"1708637160.0","content":"INCORRECT:\n\nAnswer A, selecting Multi-Regional Storage and adding a bucket lifecycle rule that archives data after 30 days to Coldline Storage, is not a good fit for this scenario because Multi-Regional Storage is more expensive than Regional Storage and it does not provide a clear advantage for this use case.\n\nAnswer B, selecting Multi-Regional Storage and adding a bucket lifecycle rule that archives data after 30 days to Nearline Storage, is also not the best option because Nearline Storage is more appropriate for data that is accessed less than once a month, while in this scenario, the data needs to be accessed at least once a year.\n\nAnswer C, selecting Regional Storage and adding a bucket lifecycle rule that archives data after 30 days to Nearline Storage, is not ideal because Nearline Storage is more suitable for data that is accessed less than once a month. If the data is accessed only once a year, it might be more cost-effective to choose Coldline Storage instead.","comment_id":"818446","upvote_count":"1"}],"timestamp":"1708637160.0"},{"upvote_count":"1","timestamp":"1702022160.0","poster":"cslince","content":"Selected Answer: D\nThe answer is D.","comment_id":"738747"},{"upvote_count":"1","comment_id":"716614","content":"Selected Answer: D\nRegional Storage, Coldline Storage.","poster":"leogor","timestamp":"1699782000.0"},{"poster":"biren111","content":"As in question it is asking for \"one geographic location\" .So multi region options A & B is eliminated. And Between C & D \"D is correct\" as data will be accessed once a year.","upvote_count":"1","comment_id":"697382","timestamp":"1697544780.0"},{"timestamp":"1695780180.0","poster":"gcpBeginner","content":"its C because archive data is 30 days and Nearline storage support that. https://cloud.google.com/storage/docs/storage-classes","upvote_count":"1","comment_id":"680317"},{"content":"had this question today","timestamp":"1695573240.0","poster":"Cornholio_LMC","comment_id":"678006","upvote_count":"1"},{"poster":"AzureDP900","timestamp":"1687543680.0","comment_id":"621197","upvote_count":"1","content":"D is for sure."},{"content":"Go for D\n“… and needs to be accessed annually”\nCold line is the better choice.","timestamp":"1685925300.0","comment_id":"611640","poster":"haroldbenites","upvote_count":"1"},{"comment_id":"569590","poster":"Rukman","upvote_count":"1","content":"Selected Answer: D\nD is the right answer.","timestamp":"1679043780.0"},{"upvote_count":"1","comment_id":"522735","poster":"Uqqasha","content":"Selected Answer: D\nD is the right answer.","timestamp":"1673600520.0"},{"comment_id":"495168","content":"D is perfect","timestamp":"1670334540.0","poster":"Vidyaji","upvote_count":"2"},{"poster":"vishnukumartr","timestamp":"1668947880.0","comment_id":"482533","content":"D. Select Regional Storage. Add a bucket lifecycle rule that archives data after 30 days to Coldline Storage.","upvote_count":"1"},{"poster":"alaahakim","content":"Ans: D","upvote_count":"1","timestamp":"1668887700.0","comment_id":"482055"},{"timestamp":"1668871560.0","poster":"shawnkkk","comment_id":"481873","content":"D. Select Regional Storage. Add a bucket lifecycle rule that archives data after 30 days to Coldline Storage.","upvote_count":"1"},{"content":"\"One geographic location\" and \"needs to be accessed annually\" \n-> \"Archive Storage\" class is most appropriate, but in this question, \"D\" is the best choice.","upvote_count":"2","poster":"ankatsu2010","comment_id":"458580","timestamp":"1665127680.0"},{"content":"Ans D , even though coldline storage is for data being accessed in a quarter. As archive storage is not mentioned the next feasible option is coldline hence , option D\nhttps://cloud.google.com/storage/docs/storage-classes","comment_id":"434587","timestamp":"1661776800.0","upvote_count":"1","poster":"vikram___"},{"timestamp":"1661685900.0","poster":"Chotebhaisahab","upvote_count":"1","content":"Needed regional storage for one geographic location cold line for annual access of data. D is correct.","comment_id":"433805"},{"timestamp":"1660021620.0","content":"D is the right answer","upvote_count":"1","comment_id":"421934","poster":"Israel"},{"poster":"bunnyabi","upvote_count":"1","timestamp":"1656296100.0","comment_id":"391683","content":"D is correct"},{"upvote_count":"1","timestamp":"1652447160.0","content":"D is correct","comment_id":"356413","poster":"mcaromit"},{"upvote_count":"1","timestamp":"1648201680.0","poster":"[Removed]","content":"D is correct. Select Regional Storage. Add a bucket lifecycle rule that archives data after 30 days to Coldline Storage.","comment_id":"320021"},{"comment_id":"311106","timestamp":"1647312300.0","content":"D for sure.","poster":"Devgela","upvote_count":"1"},{"comment_id":"306479","content":"Can someone verify that Coldline storage is now archive? I beleive the storage classes have chnages to \n1.standard \n2.nearline 30 days\n3.coldline 90 days\n4.archive 365 days. \n\nAns for this is D but think this might be an outdated one. Please correct me if im wrong.","poster":"drizzydroo","upvote_count":"1","timestamp":"1646837580.0"},{"content":"Question is asking Geo Location when data needed so I think A is correct in this scenario","timestamp":"1646490600.0","poster":"Hi2ALL","comment_id":"304209","upvote_count":"1"},{"timestamp":"1646236440.0","poster":"neerajgoyal","upvote_count":"1","comment_id":"302093","content":"D is the correct answer"},{"poster":"GCP_Student1","timestamp":"1645660140.0","upvote_count":"2","content":"D. Select Regional Storage. Add a bucket lifecycle rule that archives data after 30 days to Coldline Storage.","comment_id":"297808"},{"content":"People who are choosing option D is forgetting about minimum storage duration for coldline is 90 days. In the question the requirement is to archive the data after 30 days which is eligible for nearline. :)","comment_id":"294872","upvote_count":"2","timestamp":"1645347960.0","comments":[{"poster":"rmout","timestamp":"1645628580.0","content":"A minimum storage duration applies to data stored using one of these storage classes. You can delete the object before it has been stored for this duration, but at the time of deletion you are charged as if the object was stored for the minimum duration","upvote_count":"2","comment_id":"297463"}],"poster":"_batman_"},{"comments":[{"poster":"RKS_2021","upvote_count":"1","content":"Ans should be D from given choices other wise the right answer is Archive storage.","comment_id":"293124","timestamp":"1645162260.0"}],"timestamp":"1643694120.0","content":"C. Select Regional Storage. Add a bucket lifecycle rule that archives data after 30 days to Nearline Storage.","comment_id":"281064","upvote_count":"2","poster":"DucSiu"},{"upvote_count":"2","content":"D - One geography and annually ..so regional and cold line","comment_id":"280036","timestamp":"1643559960.0","poster":"rvgcp"},{"poster":"INASR","upvote_count":"1","timestamp":"1643171820.0","comment_id":"276593","content":"D , annually for coldline"},{"poster":"desekis","content":"go for D","comment_id":"255394","timestamp":"1640844240.0","upvote_count":"1"},{"upvote_count":"2","timestamp":"1640744880.0","poster":"DucSiu","comment_id":"254494","content":"D is best choice"},{"content":"Breaking the question into key points - \n\n1. Storing and archiving data on Google Cloud Platform.\n2. One geographic location.\n3. Data is archived after 30 days\n4. Needs to be accessed annually.\n\nA. Select Multi-Regional Storage. -Not needed because we only need one geographic location.\n\nB. Select Multi-Regional Storage. -Not needed because we only need one geographic location.\n\nC. Regional...Nearline Storage - Not needed because we don't plan on accessing the data frequently (like once a month)\n\nD. Regional....Coldline Storage - Right answer because we are only accessing data once a year.","timestamp":"1640709840.0","upvote_count":"5","comments":[{"timestamp":"1640709900.0","content":"Resources - \nhttps://cloud.google.com/storage/docs/storage-classes#available_storage_classes\n\nNote - Don't confuse the minimum storage duration requirement with what's mentioned in the question. The question only speaks about the age at which the data needs to be moved out of our regular storage into one of the other storage options. we are going to keep this data in the other storage option for really long and that's why the question mentions that the data will be accessed once a year AFTER we move it to a different storage.","comment_id":"254193","poster":"mohdafiuddin","upvote_count":"4"}],"comment_id":"254192","poster":"mohdafiuddin"},{"poster":"Bhagirathi","upvote_count":"4","comment_id":"248404","content":"200% D. pls do not confuse anymore.","timestamp":"1639978680.0"},{"content":"Ans is D","comment_id":"240996","upvote_count":"1","poster":"srija02","timestamp":"1639233060.0"},{"upvote_count":"1","content":"It is D - Regional as the data needs to be from one Geographic location, and Coldline as it would be accessed only once a year","poster":"devscorpio2001","timestamp":"1637696100.0","comment_id":"226063"},{"poster":"swatititame","upvote_count":"3","comments":[{"comment_id":"384967","upvote_count":"1","timestamp":"1655573640.0","poster":"gcp_aspirant1","content":"I see your answer on almost every question! And following it for confirmation."}],"timestamp":"1637214900.0","comment_id":"221606","content":"• D. Select Regional Storage. Add a bucket lifecycle rule that archives data after 30 days to Coldline Storage."},{"content":"As per the new Storage classes added by Google, it is now Archive which says for access in a year.","poster":"Anand2608","comment_id":"215955","upvote_count":"1","timestamp":"1636464780.0"},{"timestamp":"1635418860.0","comment_id":"207781","content":"D for coldline, access once annually","upvote_count":"3","poster":"nwk"},{"content":"D. Select Regional Storage. Add a bucket lifecycle rule that archives data after 30 days to Coldline Storage.","upvote_count":"3","comment_id":"198993","timestamp":"1634105340.0","poster":"glam"},{"comment_id":"183078","content":"C is Correct for 30 days its nearline","poster":"GopinathM","timestamp":"1632151020.0","upvote_count":"1","comments":[{"timestamp":"1632183480.0","poster":"raj231","comment_id":"183341","content":"They want to access annually so Coldline is best option. One geographic location so regional is best","upvote_count":"2"}]},{"poster":"Alvaro89","upvote_count":"1","timestamp":"1630739220.0","content":"Why is not A??. Option \"multi regional\"?","comment_id":"173199","comments":[{"content":"Hi Alvaro89, because it says \"You need to support compliance objectives for data from one geographic location\". For example, in the Europe region the government requires certain conditions for sensitive data, and in the Asia region the laws may be different. Replicating the data in different regions may incur compliance violations.","timestamp":"1631138340.0","comment_id":"176129","upvote_count":"8","poster":"amalave"}]},{"comment_id":"154370","upvote_count":"2","poster":"qafro","content":"D should be right","timestamp":"1628586960.0"},{"upvote_count":"3","content":"Answer is D, question is one geographic location and after 30 days archive to storage that is accessible annually once.","comment_id":"151300","timestamp":"1628178240.0","poster":"mbiy"},{"timestamp":"1624714980.0","comment_id":"120580","upvote_count":"4","poster":"professor","content":"Ans is D\n\nSingle region, be accessed anually"},{"comment_id":"115293","upvote_count":"4","timestamp":"1624256520.0","poster":"Ciumela","content":"D is correct"},{"upvote_count":"5","comment_id":"106726","poster":"poogcp","content":"its D. accesses only year so coldline best suitable storage class","timestamp":"1623322680.0"}],"answers_community":["D (100%)"],"answer":"D","question_text":"You want to select and configure a solution for storing and archiving data on Google Cloud Platform. You need to support compliance objectives for data from one geographic location. This data is archived after 30 days and needs to be accessed annually. What should you do?"},{"id":"FZRKtKCXQ59ijRvsETRY","unix_timestamp":1591786740,"answers_community":["A (100%)"],"choices":{"B":"Write a shell script that uses the bq command line tool to loop through all the projects in your organization.","A":"Go to Data Catalog and search for employee_ssn in the search box.","D":"Write a Cloud Dataflow job that loops through all the projects in your organization and runs a query on INFORMATION_SCHEMA.COLUMNS view to find employee_ssn column.","C":"Write a script that loops through all the projects in your organization and runs a query on INFORMATION_SCHEMA.COLUMNS view to find the employee_ssn column."},"question_images":[],"timestamp":"2020-06-10 12:59:00","exam_id":1,"answer_images":[],"question_id":279,"discussion":[{"poster":"poogcp","timestamp":"1607605140.0","comment_id":"106727","content":"Its A.","upvote_count":"40"},{"comment_id":"141844","content":"Correct is A. \nI tested on my account following this procedure: https://cloud.google.com/bigquery/docs/quickstarts/quickstart-web-ui?authuser=4\nI created a data set and through Data Catalog I easily and effortlessly searched for the column name \"gender\"","timestamp":"1611398520.0","upvote_count":"32","poster":"filco72"},{"content":"The best option to minimize effort in performing this task is A. Go to Data Catalog and search for employee_ssn in the search box.\n\nGoogle Cloud’s Data Catalog is a fully managed and scalable metadata management service that empowers organizations to quickly discover, manage, and understand all their data in Google Cloud. It offers a simple and easy-to-use search interface for data discovery. By searching for “employee_ssn” in the Data Catalog, you can quickly find all tables across all datasets and projects that contain this column. This approach is more efficient and requires less effort compared to writing and maintaining scripts or jobs.\n\nPlease note that access to Data Catalog and the visibility of datasets, tables, and columns are subject to permissions and roles in IAM policy. Make sure you have the necessary permissions to view the metadata.","poster":"JB28","timestamp":"1720633320.0","upvote_count":"3","comment_id":"1118990"},{"poster":"ogerber","comment_id":"1088576","upvote_count":"4","timestamp":"1717591800.0","content":"Selected Answer: A\nData Catalog lets you search and tag entries such as BigQuery tables with metadata. Some examples of metadata that you can use for tagging include public and private tags, data stewards, and rich text overview.\n\nhttps://cloud.google.com/data-catalog/docs/tag-bigquery-dataset"},{"comment_id":"1066342","poster":"Vik96","content":"I am preparing for the GCP-ACE exam, I was able to access 96 questions only, if anyone has the entire questions please share them with my vittoriaprovenza@tiscali.it address. I have exam on next week,pls share Thanks in advance. I would be forever grateful.","upvote_count":"2","timestamp":"1715243640.0"},{"timestamp":"1714991760.0","content":"I will go for A","poster":"BAofBK","upvote_count":"1","comment_id":"1063834"},{"poster":"BAofBK","content":"The correct answer is A","comment_id":"1063830","upvote_count":"1","timestamp":"1714991460.0"},{"content":"Selected Answer: A\nAnswer is A, as it requires the less effort and other options are more time consuming and error prone","poster":"Captain1212","comment_id":"997332","timestamp":"1709450880.0","upvote_count":"1"},{"poster":"deadsong","content":"The most efficient approach to identify tables that contain an employee_ssn column in BigQuery would be to query the INFORMATION_SCHEMA.COLUMNS view, which provides metadata about all columns in all tables in a given dataset. Therefore, options C and D are both possible solutions.\n\nOption A, searching for the column name in Data Catalog, may not be efficient if there are too many datasets to search through manually.\n\nOption B, writing a shell script to loop through all the projects in your organization, may work, but it would require more effort and time than options C and D. Also, it would be more error-prone since the script would need to handle authentication and authorization, handle exceptions and errors, and collect the results.\n\nTherefore, options C and D are better choices, but option D, using Cloud Dataflow, might be overkill for this specific task. Option C, looping through all projects and querying INFORMATION_SCHEMA.COLUMNS view, is the simplest and most effective solution to minimize effort.","upvote_count":"1","comment_id":"891715","timestamp":"1699405740.0"},{"content":"Hi All, I have my GCP ACE exam scheduled for tomorrow. However, I am only being able to access 96 questions. Can anyone kindly share the entire list of questions as I have hardly anytime left before my exam. oniyi6@yahoo.com. Thank you all so much","upvote_count":"1","comments":[{"poster":"arnika98","comment_id":"885129","timestamp":"1698670260.0","upvote_count":"1","content":"Did you pass the exam? If so any questions from here came? Please let us know so that it will be helpful"}],"timestamp":"1697985840.0","comment_id":"877349","poster":"Neeyo"},{"upvote_count":"1","timestamp":"1693226040.0","poster":"R4F","content":"C is IMO the correct answer (https://stackoverflow.com/questions/68746567/big-query-find-all-column-name-containing-surname-across-all-tables)","comment_id":"824905"},{"comment_id":"818483","content":"Selected Answer: A\nAnswer A is the correct answer. Go to Data Catalog and search for employee_ssn in the search box.\n\nData Catalog is a fully managed and scalable metadata management service that allows you to discover, understand, and manage your data. It provides search functionality that allows you to search for datasets, tables, columns, and other metadata across your organization. Therefore, you can simply go to Data Catalog and search for \"employee_ssn\" in the search box to find all datasets that contain this column. This is the most efficient and straightforward solution to the problem.\n\nAnswers B, C, and D are not ideal solutions. \n\nAnswer B requires writing a shell script and using the bq command line tool to loop through all the projects, which is time-consuming and error-prone. \n\nAnswer C requires writing a script that loops through all the projects and runs a query on INFORMATION_SCHEMA.COLUMNS view, which is also time-consuming and error-prone. \n\nAnswer D involves writing a Cloud Dataflow job, which is unnecessary and OVERKILL for this simple task.","poster":"Buruguduystunstugudunstuy","timestamp":"1692733980.0","upvote_count":"8"},{"upvote_count":"1","timestamp":"1690630860.0","comment_id":"791674","poster":"Kirangm","content":"The answer is A"},{"upvote_count":"2","content":"Hello, I have my GCP ACE exam scheduled early next week. However, I am only being able to access 96 questions. Can anyone kindly share the entire list of questions as I have hardly anytime left before my exam.","poster":"Ushnishm","timestamp":"1689790560.0","comment_id":"781573"},{"timestamp":"1683878580.0","content":"Selected Answer: A\nData Catalog","comment_id":"716631","upvote_count":"1","poster":"leogor"},{"timestamp":"1680777660.0","poster":"Charumathi","upvote_count":"4","content":"A is the correct answer, Data Catalog can be used to search the column with keyword:value pair,\n\nFilter your search by adding a keyword:value to your search terms in the search box:\n\nKeyword Description\nname: Match data asset name\n***column: Match column name or nested column name\ndescription: Match table description","comment_id":"687678"},{"upvote_count":"1","comment_id":"672206","timestamp":"1679141040.0","poster":"mdvp","content":"ITS A COLUMN WE ARE SEARCHING FOR AND I DONT SEE DATA CATALOGUE CAN SEARCH FOR COLUMN"}],"question_text":"Your company uses BigQuery for data warehousing. Over time, many different business units in your company have created 1000+ datasets across hundreds of projects. Your CIO wants you to examine all datasets to find tables that contain an employee_ssn column. You want to minimize effort in performing this task.\nWhat should you do?","answer":"A","answer_description":"","url":"https://www.examtopics.com/discussions/google/view/22731-exam-associate-cloud-engineer-topic-1-question-93-discussion/","isMC":true,"answer_ET":"A","topic":"1"},{"id":"0gMAi2MVfpM3xSPG1W3g","topic":"1","timestamp":"2020-06-11 14:30:00","answers_community":["B (53%)","D (47%)"],"url":"https://www.examtopics.com/discussions/google/view/22847-exam-associate-cloud-engineer-topic-1-question-94-discussion/","question_text":"You create a Deployment with 2 replicas in a Google Kubernetes Engine cluster that has a single preemptible node pool. After a few minutes, you use kubectl to examine the status of your Pod and observe that one of them is still in Pending status:\n//IMG//\n\nWhat is the most likely cause?","question_images":["https://www.examtopics.com/assets/media/exam-media/04338/0004900001.jpg"],"isMC":true,"choices":{"C":"The node pool is configured with a service account that does not have permission to pull the container image used by the pending Pod.","A":"The pending Pod's resource requests are too large to fit on a single node of the cluster.","B":"Too many Pods are already running in the cluster, and there are not enough resources left to schedule the pending Pod.","D":"The pending Pod was originally scheduled on a node that has been preempted between the creation of the Deployment and your verification of the Pods' status. It is currently being rescheduled on a new node."},"answer_ET":"B","discussion":[{"poster":"ESP_SAP","content":"Correct Answer is (B):\n\nReasons for a Pod Status Pending:\nTroubleshooting Reason #1: Not enough CPU\nTroubleshooting Reason #2: Not enough memory\nTroubleshooting Reason #3: Not enough CPU and memory\nhttps://managedkube.com/kubernetes/k8sbot/troubleshooting/pending/pod/2019/02/22/pending-pod.html","comments":[{"upvote_count":"5","comments":[{"comment_id":"183132","comments":[{"comments":[{"comment_id":"833902","timestamp":"1678361340.0","upvote_count":"1","content":"No, it will show as pending initially while realocating","poster":"ashtonez"}],"timestamp":"1617382020.0","upvote_count":"3","poster":"tavva_prudhvi","comment_id":"326822","content":"If it was preempted, then it has to be restarted right? then it will show its failing not pending, check the articles mentioned by ESP_SAP"},{"poster":"Finger41","comment_id":"372326","content":"Its in a deployment, the pod will be recreated. There is insufficient resources in the node, not because its preemptible but because there is no memory/cpu......","upvote_count":"2","timestamp":"1622603280.0"}],"poster":"[Removed]","content":"D gives you the reason why the resource could not be available a it was preempted","upvote_count":"3","timestamp":"1600617900.0"}],"comment_id":"162043","poster":"SSPC","content":"I agree with you. The correct answer is B","timestamp":"1597907760.0"},{"timestamp":"1619355720.0","content":"The real crux of this question is the mention about \"Pre-emptible Node pool\". That need to take into consider while determining the answer. If we choose B, then the importance of \"Pre-emptible node pool\" is not there. Whether the node pool is pre-emptible or not, resource scarcity can lead to pending pods.\n\nWhen we consider the mention of \"Pre-emptible Node Poll\" , then the answer is obviously D. if a pre-meptible Node get pre-empted there will be a delay in cluster to sync it.\n\nAnswer is D.","comment_id":"342556","upvote_count":"28","poster":"Linus11","comments":[{"comment_id":"1279108","poster":"iooj","timestamp":"1725560700.0","upvote_count":"1","content":"they just wanted to confuse us and added \"preemptible\", be strong"},{"comments":[{"comment_id":"544985","content":"A node can have multiple pods. So that is not a problem.","upvote_count":"2","poster":"brvinod","timestamp":"1644548040.0"},{"comment_id":"528958","timestamp":"1642744500.0","content":"It says a single node pool, not a single node. Meaning there can be multiple nodes, right?","poster":"MidhunJose","upvote_count":"10"}],"comment_id":"450835","timestamp":"1632480000.0","content":"Questions says \"Single Node\" at that case the second pod can't be in running state.","poster":"alexgrig","upvote_count":"4"},{"upvote_count":"9","timestamp":"1644548640.0","comment_id":"544996","poster":"brvinod","content":"Pre-emptible would have been an issue if the cluster had more than one node. The question clearly states that it is a single node cluster. That means if that single VM was pre-empted, neither of the pods should have been running. Since one pod is running, that means that (the only) VM is running. So, the reason the second pod is still pending because the VM is not having enough resources to run both the pods. Hence B.","comments":[{"poster":"mplibunao","comment_id":"613134","timestamp":"1654674060.0","content":"Actually the question stated \"single preemptible node pool\" and not \"single node\" so it's possible that there are multiple nodes and one of the node on which the pod was scheduled on was preempted","upvote_count":"4"}]},{"upvote_count":"3","comment_id":"372325","timestamp":"1622603100.0","poster":"Finger41","content":"This is to throw you off, when there is insufficient resources for a Pod to stand up, then the status will equal pending : https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#:~:text=If%20a%20Pod%20is%20stuck,be%20scheduled%20onto%20a%20node.&text=You%20don't%20have%20enough,new%20nodes%20to%20your%20cluster."},{"poster":"nssrr","timestamp":"1677382320.0","upvote_count":"4","content":"if you look at snapshot the pod age is 9m and it is still pending. Hence, the correct answer is B.","comment_id":"822056"}]}],"upvote_count":"59","timestamp":"1597715640.0","comment_id":"160466"},{"upvote_count":"21","poster":"cloudenthu01","comment_id":"119703","content":"D is correct as the node on which pod was scheduled to run was preempted & now this pod is scheduled to run on different preemtible node from the node-pool","timestamp":"1593109680.0","comments":[{"comment_id":"450231","poster":"myuniquename","content":"Incorrect. There is a single preemtible instance, if it was preempted then both pods would show as 'Pending'. B is correct.","comments":[{"upvote_count":"2","timestamp":"1678361400.0","comment_id":"833903","poster":"ashtonez","content":"No, because one of the pods may run on another node that its still up"},{"content":"> There is a single preemtible instance\n\nWhere does it say that? It doesn't. Don't make things up. There's a single pre-emptible node pool. A single pool is not the same as a single node.","poster":"obeythefist","upvote_count":"11","comment_id":"569639","timestamp":"1647512100.0"}],"upvote_count":"6","timestamp":"1632398880.0"}]},{"comment_id":"1410730","timestamp":"1743046800.0","content":"Selected Answer: B\nCorrect Answer is B. As one of the indicators of incorrect D option is 0 in Restart section. If D was correct answer we would see in Restart number > 0.","poster":"85c887f","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: B\nB is correct as 0 restarts of the pod","timestamp":"1740599340.0","comment_id":"1362260","poster":"navel"},{"timestamp":"1740552000.0","upvote_count":"2","poster":"swktopic","comment_id":"1361785","content":"Selected Answer: B\nrestart is 0, so B is correct"},{"poster":"peddyua","timestamp":"1737450900.0","content":"Selected Answer: D\nPreemptible nodes can be terminated at any time, and if there are not enough available preemptible nodes at the moment to accommodate the pod, it will stay in Pending.","comment_id":"1344078","upvote_count":"1"},{"comment_id":"1321037","poster":"1e62a4f","content":"Selected Answer: B\npreemptible node can be stopped anytime. It can be not available as long as long resources are not available.","upvote_count":"1","timestamp":"1733158020.0"},{"upvote_count":"1","poster":"denno22","content":"Selected Answer: D\nThe most likely cause is given in the question. We have a single preemptible node pool.","timestamp":"1727425620.0","comment_id":"1289885"},{"content":"Selected Answer: B\n100% - not enough CPU / memory.","timestamp":"1725560820.0","upvote_count":"1","poster":"iooj","comment_id":"1279109"},{"content":"Selected Answer: D\nThe correct answer is D.","timestamp":"1724946900.0","poster":"Timfdklfajlksdjlakf","upvote_count":"1","comment_id":"1274606"},{"comment_id":"1247746","content":"Selected Answer: D\nI think so because main word of the question is preemptible pool nodes it can initialize some latency during creating node to find compliance machine.","timestamp":"1720956960.0","upvote_count":"1","poster":"BuenaCloudDE"},{"comment_id":"1162136","timestamp":"1709173620.0","poster":"tmwf","content":"Selected Answer: B\nB is more correct. Maybe Troubleshooting Reason Not enough CPU or Memory or both of them.","upvote_count":"1"},{"content":"Selected Answer: B\nI would go for B","comment_id":"1161894","timestamp":"1709143740.0","poster":"blackBeard33","upvote_count":"1"},{"upvote_count":"1","poster":"moumou","comment_id":"1155392","content":"Selected Answer: D\nD good choice","timestamp":"1708507980.0"},{"upvote_count":"1","timestamp":"1707259740.0","poster":"jareiner","comment_id":"1142814","content":"The term 'preemptible node pool' is in the question. D is the answer."},{"content":"Selected Answer: B\nI would go with B.\nWhile D is possible, this scenario is less likely compared to the resource constraint issue, especially if the Pending status is observed consistently over a few minutes. Preemption would usually lead to a quicker rescheduling unless there are resource constraints.","poster":"Cynthia2023","timestamp":"1703989560.0","upvote_count":"1","comment_id":"1110212"},{"poster":"yash_1199","content":"Selected Answer: B\nB is correct","upvote_count":"1","timestamp":"1703775180.0","comment_id":"1107873"},{"comment_id":"1063835","poster":"BAofBK","upvote_count":"1","content":"I go with B","timestamp":"1699274220.0"},{"timestamp":"1698885420.0","comment_id":"1060150","content":"Selected Answer: B\nanswer is B","poster":"wongwong","upvote_count":"1"},{"timestamp":"1698841260.0","comment_id":"1059681","upvote_count":"2","content":"Selected Answer: D\nWhen we consider the mention of \"Pre-emptible Node Poll\" , then the answer is obviously D.","poster":"gsmasad"},{"upvote_count":"1","timestamp":"1696954860.0","content":"I will pick option D","comment_id":"1039644","poster":"abhiishere"},{"timestamp":"1693719000.0","comment_id":"997336","poster":"Captain1212","content":"Selected Answer: B\nB , seems more correct as it dont have enough resources","upvote_count":"2"},{"upvote_count":"2","timestamp":"1693361340.0","comment_id":"993647","poster":"respawn","content":"Selected Answer: B\nD is a decoy meant to confuse you, answer is B"},{"content":"option D: The pending Pod was originally scheduled on a node that has been preempted between the creation of the Deployment and your verification of the Pods' status. It is currently being rescheduled on a new node.","timestamp":"1689913440.0","poster":"jayjani66","upvote_count":"1","comment_id":"958047"},{"content":"Selected Answer: D\nI'm going D ....","upvote_count":"1","poster":"geeroylenkins","comment_id":"956801","timestamp":"1689782760.0"},{"upvote_count":"1","timestamp":"1685843340.0","poster":"trainingexam","content":"Selected Answer: D\nI bet my answer on \"preemptible node pool\" keyword on the problem statement.","comment_id":"914045"},{"upvote_count":"1","timestamp":"1682530260.0","poster":"vivekvj","content":"Selected Answer: B\nMost likely because of unavailability of resources. Look at the age 9m. If the pod was deleted and being rescheduled, it will not take 9m.","comment_id":"881887"},{"upvote_count":"1","comment_id":"874185","poster":"hanweiCN","content":"Selected Answer: B\nthe \"age\" the same with the running pod and \"restart\" both are 0 , means, the containers in both pod never been restarted, the \"pending\" status pod is the created at the same time with the \"running\" status pod. they r both NO preempted or they r both preempted, in second scenario, the \"running\" status pod should have 1 in \"restart\" . so i will go with B","timestamp":"1681861800.0"},{"poster":"sabrinakloud","timestamp":"1681726140.0","upvote_count":"1","comment_id":"872583","content":"Selected Answer: B\nhttps://kubernetes.io/docs/tasks/debug/debug-application/debug-pods/#debugging-pods\nanswer B"},{"comment_id":"860140","poster":"inbalinbal","content":"Selected Answer: D\nthey have mention preemptible node pool","timestamp":"1680539400.0","upvote_count":"1"},{"timestamp":"1678361160.0","comment_id":"833899","poster":"ashtonez","content":"Selected Answer: D\nD is Right , I think a lot of people discussing here has very low k8s experience . \nNode pool is a term for refering one or more (usually 4 or more nodes) nodes, you can view nodes as servers that run containers (remember pods are one or more containers) .\nWhen one of the nodes from the node pool is down because its a preemtible, all of their pods go down and the kube controller (which is an element running on control plance ) reallocates the pods to other nodes from their node pools and then those pods rill show as running (if the nodes accomplish the resource quotas) .\nSo basically what happend here is that we just did a kubectl get pods command inthe mid time where one of the nodes from the node pool went down and the cluster tried to reposition the pods into other nodes","upvote_count":"6"},{"content":"Both Answer B and Answer D could be valid answers depending on the specific circumstances of the cluster and deployment.\n\nIf the node pool is not experiencing any preemptions, then Answer B is likely the correct answer, indicating that there are not enough resources left in the cluster to schedule the pending Pod. In this case, you could try increasing the resources available in the cluster or scaling down other workloads to free up resources for the pending Pod.\n\nIf preemptions are occurring in the node pool, then Answer D may be the correct answer, indicating that the pending Pod was originally scheduled on a preempted node and is being rescheduled on a new node. In this case, you may want to consider using a different type of node pool or adjusting your workload to better handle node preemptions.","upvote_count":"4","comments":[{"upvote_count":"1","content":"So, which option is more likely to be the cause? It's hard to say without more information because GOOGLE loves to confuse us. If the cluster is running many Pods and the nodes are close to maximum capacity, Answer B is more likely. If the cluster is not heavily utilized and preemptible nodes are being used, Answer D is more likely.","comment_id":"819413","poster":"Buruguduystunstugudunstuy","timestamp":"1677170820.0"},{"upvote_count":"2","comment_id":"873188","content":"agreed, my lord!\nbut i believe they particularly want to test us on the role of \"preemptible nodes\" in a GKE cluster\nfrom that pov; D weighs more","poster":"chikorita","timestamp":"1681776540.0"}],"comment_id":"819412","timestamp":"1677170760.0","poster":"Buruguduystunstugudunstuy"},{"timestamp":"1675628940.0","poster":"akhun","upvote_count":"1","content":"Selected Answer: B\nIf a Pod is stuck in Pending it means that it can not be scheduled onto a node. Generally this is because there are insufficient resources of one type or another that prevent scheduling. \nYou don't have enough resources: You may have exhausted the supply of CPU or Memory in your cluster, in this case you need to delete Pods, adjust resource requests, or add new nodes to your cluster.","comment_id":"799147"},{"timestamp":"1675395480.0","comment_id":"796693","content":"Selected Answer: B\nThere is a single preemtible instance, if it was preempted then both pods would show as 'Pending'. B is correct.","upvote_count":"1","poster":"Nazz1977"},{"poster":"Di4sa","content":"Selected Answer: D\nD is the correct answer.\nA single preemptible node pool can have multiple nodes\nhttps://cloud.google.com/blog/products/containers-kubernetes/cutting-costs-with-google-kubernetes-engine-using-the-cluster-autoscaler-and-preemptible-vms","comment_id":"793106","upvote_count":"2","timestamp":"1675100160.0"},{"poster":"Di4sa","upvote_count":"1","content":"D is the correct answer.\nA single preemptible node pool can have multiple nodes\nhttps://cloud.google.com/blog/products/containers-kubernetes/cutting-costs-with-google-kubernetes-engine-using-the-cluster-autoscaler-and-preemptible-vms","timestamp":"1675100100.0","comment_id":"793103"},{"content":"Selected Answer: D\nThe questions itself wants us to select the D option (premptible)","upvote_count":"1","timestamp":"1674785820.0","comment_id":"789271","poster":"Akhs722"},{"upvote_count":"1","content":"Selected Answer: D\nPre-emptible Node pool keyword.","comment_id":"787153","poster":"bagusilham","timestamp":"1674610140.0"},{"content":"Selected Answer: D\nD is correct","timestamp":"1672515180.0","upvote_count":"1","comment_id":"762898","poster":"juss4friendz"},{"poster":"nosense","content":"Selected Answer: D\nPre-emptible Node pool keyword.","upvote_count":"2","timestamp":"1666678500.0","comment_id":"703588"},{"timestamp":"1663331460.0","content":"B is correct, but why not A?","comment_id":"670815","upvote_count":"2","poster":"darcal95"},{"content":"Selected Answer: D\nOption D , I have practically experienced this before on live envs whenever a node gets tainted and pods are supposed to be rescheduled it goes to pending state","poster":"sandipk91","timestamp":"1662208440.0","upvote_count":"1","comment_id":"658454"},{"content":"B is right","comments":[{"comments":[{"upvote_count":"2","content":"if you meant of using https://github.com/estafette/estafette-gke-preemptible-killer, then that is an out of the box solution. So can not be assumed as part of the possible components in the answers.","comment_id":"690575","timestamp":"1665355920.0","poster":"theBestStudent"}],"poster":"Passerofexams","content":"After doing a bit of research within the Google Docs, I agree. \n\nThere is specific logic that can and should be put in place to prevent the scenario mentioned in “D” from happening. \n\nhttps://cloud.google.com/blog/products/containers-kubernetes/cutting-costs-with-google-kubernetes-engine-using-the-cluster-autoscaler-and-preemptible-vms","comment_id":"624356","upvote_count":"1","timestamp":"1656465420.0"}],"upvote_count":"2","poster":"AzureDP900","timestamp":"1656008340.0","comment_id":"621203"},{"poster":"lamboz","comment_id":"617491","content":"I think:\n- Not enough CPU & RAM --> status: OMMKilled\n- Cannot pull image or syntax errors --> status: Crash...\n- Cannot mount volumes --> status: ContainerCreating or Evited\n- Cannot scheduled because Node not ready --> status: Pending\n\nIf a preemptible node not ready, pod cannot start, so pod's status is Pending.\nSo, the answer is D","upvote_count":"1","timestamp":"1655433120.0"},{"comment_id":"613360","content":"Selected Answer: B\nCorrect answer is B. If the node was pre-empted the other pod would have been terminated","poster":"Tirthankar17","timestamp":"1654703160.0","upvote_count":"1"},{"content":"Go for B","poster":"haroldbenites","comment_id":"611673","upvote_count":"1","timestamp":"1654403280.0"},{"comment_id":"596700","content":"Correct Answer is (D):\nB options is not correct because if the node resources are not enough kubernetes scheduler will reschedule the pod deployment to an another cluster node and pod would not enter pending status except if all cluster nodes resources are fully consumed.\n\nOption D makes more since","poster":"salehm","upvote_count":"1","timestamp":"1651658220.0"},{"upvote_count":"1","comment_id":"595089","timestamp":"1651330260.0","poster":"hiranfilho","content":"Selected Answer: B\nhttps://managedkube.com/kubernetes/k8sbot/troubleshooting/pending/pod/2019/02/22/pending-pod.html"},{"content":"Selected Answer: D\nI vote 'D'\nB says: Too many 'Pods' are already running in the cluster, and there are not enough resources left to schedule the pending Pod.\n\nThere are only 2 pods and setup as 2 pods, where is the 'too many' coming from??","timestamp":"1648679760.0","comment_id":"578573","poster":"zaxma","upvote_count":"3","comments":[{"comment_id":"609659","poster":"wjtb","upvote_count":"2","timestamp":"1653990720.0","content":"look at the command, the pods are filtered on label: app=myapp, this doesnt mean all the pods are listed (only the pods with that label)"}]},{"timestamp":"1648434780.0","content":"Selected Answer: D\nD is correct as the node on which pod was scheduled to run was preempted & now this pod is scheduled to run on different preemtible node from the node-pool","poster":"Mr_Tiwariji","upvote_count":"1","comment_id":"576565"},{"upvote_count":"4","timestamp":"1647329760.0","content":"Selected Answer: B\nage is important\nboth pods have been scheduled the same time (so if the pending one got pending due to restart of a preemptied one it should be rescheduled to another node and the age would be shorter right?)\nin this case it is pending becuase there is no such node which fits the pod capacity requirement so either the pod (assuming there are only 2 nodes) is being restarted/preemptied and the other running node is unable to host the pod due to low capacity\nto be honest this is again both options are correct but I'm rather with option B looking at the result not the cause of the issue (pendig because unable to schedule not because anything got restarted)","poster":"ryzior","comment_id":"568204"},{"timestamp":"1646989980.0","upvote_count":"1","content":"Selected Answer: B\nCorrect Answer is (B)","comment_id":"565339","poster":"special1"},{"comment_id":"553169","upvote_count":"1","content":"B is the correct answer, any just because the node is preemptible doesn't mean that it was removed while tehre's a pod already running.","timestamp":"1645471800.0","poster":"luciorifa"},{"comment_id":"552811","upvote_count":"1","content":"B is correct","timestamp":"1645444380.0","poster":"raimangsxr"},{"poster":"[Removed]","comment_id":"541461","upvote_count":"1","timestamp":"1644119940.0","content":"Selected Answer: B\nf a Pod is stuck in Pending it means that it can not be scheduled onto a node. Generally this is because there are insufficient resources of one type or another that prevent scheduling.\n\nhttps://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/"},{"content":"Selected Answer: B\nB is correct answer","poster":"Durgesh1997","comment_id":"538133","upvote_count":"2","timestamp":"1643739660.0"},{"content":"Selected Answer: B\nB is right","comment_id":"535882","timestamp":"1643509680.0","poster":"Lowballed","upvote_count":"1"},{"timestamp":"1642744800.0","upvote_count":"2","poster":"MidhunJose","content":"Selected Answer: D\nWe have a single \"node pool\", meaning there would have been multiple nodes.\n- The pods would have been scheduled in two different nodes. \n- One of them went down and then we checked the \"kubectl get pods\". \n- At that time kubernetes is scheduling the pod in a new node, and hence the pending status.\n\nB is also correct, but when we put in the \"preemptible node pool\" scenario, D would be a better assumption","comment_id":"528960"},{"timestamp":"1642469040.0","upvote_count":"1","comment_id":"526231","content":"Right answer is B\n\nOption D get eliminated becoz, in question the preemtable vms don't get turned off within few minutes of starting (very very less chance) only once in 24 hrs.","poster":"dishum"},{"poster":"Greg_Exam","content":"Selected Answer: B\nC is incorrect because another pod already pulled the image.\nD is incorrect because Age of the pending pod is 9 minutes (if it is being rescheduled age should be less then running pod).\nA is probably incorrect because resource request of the running pod fits on a single node.\nSo B is the correct answer.","upvote_count":"4","timestamp":"1642006920.0","comment_id":"522324"},{"comment_id":"513290","poster":"jx2019","timestamp":"1640863080.0","content":"I vote for B. If preempted, pods should be rescheduled with status either success or failed but not pending.","upvote_count":"1"},{"timestamp":"1638910920.0","poster":"wh1t4k3r","content":"This one still confusing me, so please note this is not an ultimate statement, im just sharing my thoughts:\nI was going for D, but i looked closer and i think that being preemptible, the node would not reschedule pods to other nodes, it would just terminate and create new ones. If it is still pending i would say it is because it has not even started yet, so lack of resources sound correct. But again, i am still confused on this one","comment_id":"496332","upvote_count":"2"},{"comment_id":"493294","content":"Selected Answer: D\nPlease re-read the question carefully. It states that workloads are running on a single preemptible node POOL, not a single node. The node it was scheduled on was probably being preempted/recreated and therefore has to wait until the node becomes ready. B is incomplete, as it does not state the root cause (if we imply that the node pool scales elastically)","upvote_count":"2","timestamp":"1638554460.0","poster":"jabrrJ68w02ond1"},{"timestamp":"1637903640.0","upvote_count":"1","poster":"Ridhanya","comment_id":"487086","content":"D is right"},{"comments":[{"poster":"vishnukumartr","comment_id":"496552","content":"B. Too many Pods are already running in the cluster, and there are not enough resources left to schedule the pending Pod.","upvote_count":"1","timestamp":"1638941400.0"}],"content":"D. The pending Pod was originally scheduled on a node that has been preempted between the creation of the Deployment and your verification of the Podsג€™ status. It is currently being rescheduled on a new node.","timestamp":"1637412480.0","comment_id":"482537","upvote_count":"1","poster":"vishnukumartr"},{"comment_id":"452100","upvote_count":"2","poster":"vamgcp","timestamp":"1632703500.0","content":"I will go with option B"},{"content":"~ one of them is \"still in Pending status\". Which means the pod is pending for 9m. So, the most obvious answer is B.","poster":"ryan_m","comment_id":"435667","upvote_count":"2","timestamp":"1630340880.0"},{"poster":"gcp_aspirant1","comment_id":"384982","timestamp":"1624038780.0","upvote_count":"1","content":"Will go with B!"},{"content":"Why not C? \nPod can be failed for the following reasons;\n1. If there is not enough memory\n2. Kubernetes couldn't pull the images\n3. Not appropriate permissions.","upvote_count":"1","poster":"Hasaaaan","timestamp":"1623216480.0","comments":[{"upvote_count":"2","timestamp":"1627930080.0","comment_id":"418892","poster":"benuk78","content":"I'm no expert, but presumably because if the image couldn't be pulled then the other pod would be pending too. Since one pod is running the deployment has been successful at least once, so managed to pull the image at least once."}],"comment_id":"378002"},{"content":"B is the correction option I see some people opting for D .... if the node goes down both pods would have been in pending state","upvote_count":"2","comment_id":"363451","timestamp":"1621664280.0","poster":"viswanand"},{"poster":"Finger41","timestamp":"1620966180.0","comment_id":"356881","content":"Its B, 100%. The pre-emptible thing was added to trick you. See : https://cloud.google.com/kubernetes-engine/docs/troubleshooting#node_allocatable","upvote_count":"1"},{"comment_id":"356417","timestamp":"1620911760.0","content":"B is correct...A pod is never rescheduled to a node, instead a new pod with a new UID is assigned to the node.","upvote_count":"1","poster":"mcaromit"},{"upvote_count":"2","timestamp":"1617775500.0","content":"D is incorrect because a rescheduled pod will be a new pod with younger age. I vote for B.","comment_id":"330079","poster":"Konnon"},{"content":"B is correct. Too many Pods are already running in the cluster, and there are not enough resources left to schedule the pending Pod.","comment_id":"320026","poster":"[Removed]","upvote_count":"2","timestamp":"1616666280.0"},{"content":"correct answer is B , because in the question it is given 'single preemptible node pool'","upvote_count":"1","timestamp":"1616589840.0","poster":"yuvi69","comment_id":"319101"},{"upvote_count":"2","poster":"GCP_Student1","comment_id":"307310","content":"B - Too many Pods are already running in the cluster, and there are not enough resources left to schedule the pending Pod.","timestamp":"1615394040.0"},{"content":"B is correct.","comments":[{"content":"If there are \"Too many Pods\" then why aren't those pods showing up when you run the command to get pods? It displays one running and one pending and the replica is set for 2. I said D originally but now I am just confused.","comment_id":"306512","poster":"drizzydroo","timestamp":"1615303020.0","upvote_count":"2","comments":[{"comment_id":"473009","poster":"_JP_","timestamp":"1636110180.0","upvote_count":"1","content":"The command is run with a label filter (\"-l\") and in the default namespace"}]}],"upvote_count":"1","comment_id":"305938","poster":"EABDAJA","timestamp":"1615234680.0"},{"poster":"TAvenger","timestamp":"1613692200.0","comments":[{"comments":[{"upvote_count":"4","comments":[{"upvote_count":"2","poster":"learntogether","timestamp":"1633363080.0","comment_id":"457238","content":"This should be higher up. I'm still very much confused if it's B or D but so many people here are assuming that there is one single node when it says 'single preemptible node POOL' which hints at D."}],"timestamp":"1615066680.0","comment_id":"304842","poster":"TAvenger","content":"It is not \"pool with as single preemtible node\", this is single preemptible NODE POOL. \"Node pool\" can be the the only pool (single pool) that is present on cluster.\nIn this case GKE can have multiple nodes. And in this current situation one node stayed alive (and another was preempted)"}],"comment_id":"297135","poster":"nitinz","content":"read the question, \"You create a Deployment with 2 replicas in a Google Kubernetes Engine cluster that has a single preemptible node pool.\" It is single NODE. if it was preempted then both pods should be gone not one..... Answer is B.","upvote_count":"1","timestamp":"1614052800.0"}],"comment_id":"293823","upvote_count":"3","content":"It could be \"D\"\nPlease note, it is said that \"node pool\". And the pool may have many nodes. In theory it may happen that some nodes were preempted.\n\nIt's really suspicious, that it was mentioned that nodes are preemptible, that we waited for a few minutes and decided to check the status. Also we have the link\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/preemptible-vms#best_practices\nwhere it is mentioned that \"It may take several minutes for GKE to detect that the node was preempted and that the Pods are no longer running, which will delay the rescheduling of the Pods to a new node.\"\nI have a feeling that they talk about answer \"D\""},{"poster":"arsav","content":"you are on \"single preemptible node pool\" - so if that preemptible node pool is evicted, both the pod should not running.\n\nas you see one pod is already running and only one is in pending status,\nwhich clearly tells the node was not preempted. \n\nso the reason for pending could be due to \"not enough resources\" is the obvious reason.","comment_id":"292997","timestamp":"1613611860.0","upvote_count":"2","comments":[{"poster":"TAvenger","content":"Please note, it is said that \"node pool\". And the pool may have many nodes. In theory it may happen that all nodes except the last one were preempted.\n\nIt's really suspicious, that it was mentioned that nodes are preemptible, that we waited for a few minutes and decided to check the status. Also we have the link\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/preemptible-vms#best_practices\nwhere it is mentioned that \"It may take several minutes for GKE to detect that the node was preempted and that the Pods are no longer running, which will delay the rescheduling of the Pods to a new node.\"\nI have a feeling that they talk about answer \"D\"","timestamp":"1613692020.0","comment_id":"293822","upvote_count":"1","comments":[{"timestamp":"1613957820.0","poster":"arsav","content":"we are attaching the preemptible VM to the \"node pool\" hence if that VM is evicted it becomes unavailable to the whole \"node pool\" all the nodes which tries to perform its work load / batch becomes unavailable. \n\nAlso the document says\n****If you want to ensure that your jobs or workloads are processed even if no preemptible VMs are available, you can create both non-preemptible and preemptible node pools in your cluster.******\n\nso you need to have both preemptible and non-premptible VM in the node pool for uninterrupted pods to be servered.\n\nin this case of this question, one pod is already running which clearly confirms preemption didnt happen yet.\n\nthe reason for the \"pending\" could be different due to other resource constraints.","comment_id":"296223","upvote_count":"2"}]}]},{"timestamp":"1612784760.0","upvote_count":"1","content":"B - Too many Pods are already running in the cluster, and there are not enough resources left to schedule the pending Pod.","comment_id":"286153","poster":"victory108"},{"poster":"DucSiu","comment_id":"281067","content":"B. Too many Pods are already running in the cluster, and there are not enough resources left to schedule the pending Pod.","timestamp":"1612158540.0","upvote_count":"1"},{"upvote_count":"4","timestamp":"1608831780.0","content":"B is right answer if you want to pass exam.","comments":[{"upvote_count":"1","poster":"gcp_aspirant1","timestamp":"1624038660.0","content":"haha yess","comment_id":"384980"}],"comment_id":"251681","poster":"PhilipAWS"},{"content":"I strongly believe the correct answer is B and NOT D.\nThe question said you have one preemptive node and if that is the case then it is not possible to have one of two pods up and running as specified in the replicas=2. \nIn addition, the kubectl get query indicates that you are trying to query only pods labeled 'app=.....' from the list of pods and if you are only able to get one of two pods running then it is an indication that your preemptive node is running out of resources.\nhttps://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#my-pod-stays-pending","comment_id":"240455","poster":"Range2019","timestamp":"1607635440.0","upvote_count":"1"},{"poster":"swatititame","upvote_count":"2","comment_id":"221626","timestamp":"1605683340.0","content":"• B. Too many Pods are already running in the cluster, and there are not enough resources left to schedule the pending Pod."},{"comment_id":"220609","timestamp":"1605557460.0","upvote_count":"2","poster":"ayj","content":"Probably B, bit of a mean question trying to throw with the preemptible part\n\nIf a Pod is stuck in Pending it means that it can not be scheduled onto a node. Generally this is because there are insufficient resources of one type or another that prevent scheduling. Look at the output of the kubectl describe. There should be messages from the scheduler about why it can not schedule your pod. Reasons include:\n\nYou don't have enough resources: You may have exhausted the supply of CPU or Memory in your cluster, in this case you need to delete Pods, adjust resource requests, or add new nodes to your cluster. See Compute Resources document for more information.\n\nYou are using hostPort: When you bind a Pod to a hostPort there are a limited number of places that pod can be scheduled. In most cases, hostPort is unnecessary, try using a Service object to expose your Pod. If you do require hostPort then you can only schedule as many Pods as there are nodes in your Kubernetes cluster."},{"content":"Vote D - https://cloud.google.com/kubernetes-engine/docs/how-to/preemptible-vms#best_practices\nIt may take several minutes for GKE to detect that the node was preempted and that the Pods are no longer running, which will delay the rescheduling of the Pods to a new node.","comment_id":"207858","timestamp":"1603897020.0","upvote_count":"4","poster":"nwk"},{"comments":[{"comment_id":"201466","upvote_count":"5","timestamp":"1602927240.0","content":"If the node is preemptible and had being restarted, we wouldn't be able to check the pods as they would all be failing. Reason for not opting 'D'.","poster":"glam"}],"content":"B. Too many Pods are already running in the cluster, and there are not enough resources left to schedule the pending Pod.","poster":"glam","upvote_count":"8","timestamp":"1602927120.0","comment_id":"201465"},{"poster":"raj231","content":"B looks correct..","timestamp":"1600647660.0","upvote_count":"2","comment_id":"183343"},{"content":"B is correct","upvote_count":"1","comment_id":"183083","timestamp":"1600615080.0","poster":"GopinathM"},{"comment_id":"182789","upvote_count":"3","content":"Correct Answer is B\nRef : https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#my-pod-stays-pending","poster":"sourabhjagtap95","timestamp":"1600586760.0"},{"upvote_count":"3","poster":"someoneinthecloud","content":"It's B....\nB is correct.\nIf the node is preemptible and had being restarted, we wouldn't be able to check the pods as they would all be failling.","timestamp":"1596306540.0","comment_id":"148691"},{"timestamp":"1595744940.0","content":"D. by the question mention: a single preemptible node pool which is not stable and could be restarted.\nThe pod is try to reschedule but not found the node resource.","comment_id":"143856","upvote_count":"1","poster":"vinhnguyen"},{"comment_id":"137928","content":"The question doesn't offer a lot of info and it's very ambiguous but it says we have a single preemptible node pool and not a single node pool. This is import because now we can assume that we have enough resources in the pool. This info leads me to D. The pod was scheduled in a preempted node and is now being rescheduled.","poster":"someoneinthecloud","upvote_count":"8","timestamp":"1595080920.0"},{"content":"B is correct","upvote_count":"2","poster":"Ciumela","comment_id":"119715","timestamp":"1593110760.0"},{"comment_id":"119389","content":"B or D?","timestamp":"1593084780.0","upvote_count":"1","poster":"DarioFama23"},{"comment_id":"107980","content":"B is correct","timestamp":"1591897800.0","poster":"[Removed]","comments":[{"content":"There are only two pods running","upvote_count":"1","timestamp":"1614320280.0","poster":"Ibitayo","comment_id":"299576"}],"upvote_count":"8"},{"timestamp":"1591878600.0","content":"I think it is D","upvote_count":"5","poster":"torres_xyz","comment_id":"107750"}],"question_id":280,"answer_description":"","exam_id":1,"answer_images":[],"unix_timestamp":1591878600,"answer":"B"}],"exam":{"isBeta":false,"name":"Associate Cloud Engineer","lastUpdated":"11 Apr 2025","isMCOnly":true,"provider":"Google","id":1,"numberOfQuestions":285,"isImplemented":true},"currentPage":56},"__N_SSP":true}