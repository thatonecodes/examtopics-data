{"pageProps":{"questions":[{"id":"gCBzQFBiY7XvSvLxcWH0","answer":"B","url":"https://www.examtopics.com/discussions/google/view/79492-exam-professional-data-engineer-topic-1-question-167/","question_text":"Your company currently runs a large on-premises cluster using Spark, Hive, and HDFS in a colocation facility. The cluster is designed to accommodate peak usage on the system; however, many jobs are batch in nature, and usage of the cluster fluctuates quite dramatically. Your company is eager to move to the cloud to reduce the overhead associated with on-premises infrastructure and maintenance and to benefit from the cost savings. They are also hoping to modernize their existing infrastructure to use more serverless offerings in order to take advantage of the cloud. Because of the timing of their contract renewal with the colocation facility, they have only 2 months for their initial migration. How would you recommend they approach their upcoming migration strategy so they can maximize their cost savings in the cloud while still executing the migration in time?","answer_images":[],"timestamp":"2022-09-02 18:57:00","question_images":[],"answer_ET":"B","unix_timestamp":1662137820,"isMC":true,"topic":"1","exam_id":11,"question_id":76,"choices":{"C":"Migrate the Spark workload to Dataproc plus HDFS, and modernize the Hive workload for BigQuery.","B":"Migrate the workloads to Dataproc plus Cloud Storage; modernize later.","D":"Modernize the Spark workload for Dataflow and the Hive workload for BigQuery.","A":"Migrate the workloads to Dataproc plus HDFS; modernize later."},"discussion":[{"timestamp":"1685443860.0","content":"Selected Answer: B\nB is the answer.\n\nhttps://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#overview\nWhen you want to move your Apache Spark workloads from an on-premises environment to Google Cloud, we recommend using Dataproc to run Apache Spark/Apache Hadoop clusters. Dataproc is a fully managed, fully supported service offered by Google Cloud. It allows you to separate storage and compute, which helps you to manage your costs and be more flexible in scaling your workloads.\n\nhttps://cloud.google.com/bigquery/docs/migration/hive#data_migration\nMigrating Hive data from your on-premises or other cloud-based source cluster to BigQuery has two steps:\n1. Copying data from a source cluster to Cloud Storage\n2. Loading data from Cloud Storage into BigQuery","comments":[{"upvote_count":"1","poster":"AzureDP900","comment_id":"762858","content":"B. Migrate the workloads to Dataproc plus Cloud Storage; modernize later.","timestamp":"1688143740.0"}],"poster":"zellck","comment_id":"731477","upvote_count":"8"},{"upvote_count":"2","comment_id":"1100929","content":"Selected Answer: B\nBased on the time constraint of 2 months and the goal to maximize cost savings, I would recommend option B - Migrate the workloads to Dataproc plus Cloud Storage; modernize later.\nThe key reasons are:\n• Dataproc provides a fast, native migration path from on-prem Spark and Hive to the cloud. This allows meeting the 2 month timeline.\n• Using Cloud Storage instead of HDFS avoids managing clusters for variable workloads and provides cost savings.\n• Further optimizations and modernization to serverless (Dataflow, BigQuery) can happen incrementally later without time pressure.","timestamp":"1718817360.0","poster":"MaxNRG","comments":[{"comment_id":"1100930","timestamp":"1718817360.0","content":"Option A still requires managing HDFS.\nOption C and D require full modernization of workloads in 2 months which is likely infeasible.\nTherefore, migrating to Dataproc with Cloud Storage fast tracks the migration within 2 months while realizing immediate cost savings, enabling the flexibility to iteratively modernize and optimize the workloads over time.","poster":"MaxNRG","upvote_count":"3"}]},{"content":"Selected Answer: B\nB is most likely\n1. migrate job and infrastructure to dataproc on clound\n2. any data, move from hdfs on-premise to google cloud storage ( one of them is Hive)\nIf you want to modernize Hive to Bigquery , you are need to move it into GCS(preceding step) first and load it into bigquery\nthat is all. \n\nhttps://cloud.google.com/blog/products/data-analytics/apache-hive-to-bigquery\nhttps://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc\nhttps://cloud.google.com/architecture/hadoop/hadoop-gcp-migration-data","poster":"John_Pongthorn","timestamp":"1680519060.0","comment_id":"685444","upvote_count":"3"},{"content":"Selected Answer: D\nAnswer D","comments":[{"timestamp":"1679505060.0","upvote_count":"1","comments":[{"timestamp":"1680069780.0","content":"apache beam for what???","poster":"TNT87","upvote_count":"1","comment_id":"682388","comments":[{"content":"dataflow uses apache beam","comment_id":"693433","upvote_count":"1","timestamp":"1681340460.0","comments":[{"content":"@adarifian Why use apache beam yet there is Dataflow an inhouse gcp solution to solve the problem? hence i said apache beam for what","upvote_count":"1","comment_id":"772487","timestamp":"1689072000.0","poster":"TNT87","comments":[{"content":"Dataflow IS apache beam, Dataflow is a Beam Runner.\nIf you go for that soulution you will need to modify your pipeline to use Beam","poster":"ExamCtechs","upvote_count":"1","comment_id":"1060883","timestamp":"1714675200.0"}]}],"poster":"adarifian"}]}],"comment_id":"676289","content":"you sould migrate spark to apache beam which is not the case here","poster":"dn_mohammed_data"}],"comment_id":"669683","upvote_count":"1","timestamp":"1678876440.0","poster":"TNT87"},{"comment_id":"665362","poster":"GyaneswarPanigrahi","timestamp":"1678455600.0","upvote_count":"3","content":"D isn't feasible, within 2 months. Anyone who has worked in a Hadoop/ Big Data data warehousing or data lake project, knows how less time 2 months is, given the amount of data and associated complexities abound. \n\nIt should be B to begin with. And then gradually move towards D."},{"comment_id":"664238","poster":"TNT87","upvote_count":"2","content":"Selected Answer: B\nAns B\n-cost saving \n-time factor\n-Spark -Data proc","timestamp":"1678345620.0","comments":[{"comment_id":"664240","timestamp":"1678345800.0","poster":"TNT87","upvote_count":"1","content":"Ans D is also relevant if you read this. Onthe other hand cloud storage isnt severless but bigquery is\nhttps://cloud.google.com/hadoop-spark-migration"}]},{"poster":"damaldon","upvote_count":"1","content":"Ans.B as per the following link\nhttps://blog.devgenius.io/migrating-spark-jobs-to-google-cloud-file-event-sensor-to-dynamically-create-spark-cluster-7eff2c75423d","comment_id":"661569","timestamp":"1678134720.0"},{"timestamp":"1678093680.0","comment_id":"660915","upvote_count":"2","content":"Selected Answer: B\nFor the time window of two month I would recommend B and then start to implement D.","poster":"YorelNation"},{"content":"It is B or D, still confusing","poster":"ducc","comment_id":"658186","timestamp":"1677836040.0","upvote_count":"2"},{"comments":[{"timestamp":"1714675260.0","upvote_count":"1","poster":"ExamCtechs","content":"Dataflow is a Runner of Beam it self","comment_id":"1060884"}],"comment_id":"657627","content":"Selected Answer: D\nD because the Apache Spark Runner can be used to execute Beam pipelines using Apache Spark. Also, Hive to BigQuery is not a difficult modernization/migration.","poster":"AWSandeep","timestamp":"1677783420.0","upvote_count":"1"}],"answers_community":["B (89%)","11%"],"answer_description":""},{"id":"nJltyz5ImN93XNXRsUXd","discussion":[{"comment_id":"657631","content":"Selected Answer: B\nB. While C and D are intriguing, they don't specify how to enable customer service representatives to receive access to the encryption token.","comments":[{"content":"B will show the values to the customer support service all the time as they have access to it, so no redaction as per the ask. Another thing is the requirement is to view when necessary, so D fits this requirement and format preserving encryption can be reverted when necessary.","comment_id":"1320383","poster":"cloud_rider","upvote_count":"1","timestamp":"1733003640.0"},{"poster":"MaxNRG","comment_id":"1100935","content":"B. BigQuery column-level security:\n\nPros: Granular control over column access, ensures only authorized users see the SSN column.\nCons: Doesn't truly redact the data. The SSN values are still stored in BigQuery, even if hidden from unauthorized users. A potential security breach could expose them.","timestamp":"1703013900.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"1053642","content":"there is no SSN in question, it can be any ID.","poster":"ffggrre","timestamp":"1698230880.0"}],"upvote_count":"11","poster":"AWSandeep","timestamp":"1662138060.0"},{"content":"Selected Answer: D\nI don't see why we should use DLP since we know exactly the column that should be locked or encrypted. On the other hand having a cryptographic representation of SSN helps to aggregate/analyse entries. So I will vote for D, but B is much more easy to implement. Garbage question indeed.","timestamp":"1690798080.0","poster":"Lanro","comment_id":"967960","upvote_count":"6"},{"timestamp":"1730104920.0","poster":"SamuelTsch","content":"Selected Answer: D\nIn the question, there is no mention of SSN column.","comment_id":"1303885","comments":[{"comment_id":"1303886","content":"also, in the question, \"you decide to REDACT ...\". Option B does not redact the values.","poster":"SamuelTsch","upvote_count":"1","timestamp":"1730105040.0"}],"upvote_count":"2"},{"upvote_count":"2","poster":"MohaSa1","timestamp":"1729163640.0","content":"Selected Answer: D\nAuthorized users can decrypt the FPE tokens back to the original GIINs, D is the best option.","comment_id":"1299177"},{"content":"Selected Answer: D\nD (FPE) does indeed allow encryption to be reversed if desired, allowing operatives to review the original key. This makes it preferable to B, as it's also more secure.","comment_id":"1295053","poster":"baimus","timestamp":"1728464100.0","upvote_count":"2"},{"comment_id":"1226851","timestamp":"1717867320.0","poster":"Topg4u","content":"D:\nSSN is only tied to USA not in any other countries, The question did not mention SSN.","upvote_count":"2"},{"comment_id":"1100933","content":"Selected Answer: D\nThe best option is D - Before loading the data into BigQuery, use Cloud Data Loss Prevention (DLP) to replace input values with a cryptographic format-preserving encryption token.\n\nThe key reasons are:\n\nDLP allows redacting sensitive PII like SSNs before loading into BigQuery. This provides security by default for the raw SSN values.\nUsing format-preserving encryption keeps the column format intact while still encrypting, allowing business logic relying on SSN format to continue functioning.\nThe encrypted tokens can be reversed to view original SSNs when required, meeting the access requirement for customer service reps.","timestamp":"1703013840.0","poster":"MaxNRG","upvote_count":"3","comments":[{"upvote_count":"1","comments":[{"comment_id":"1100936","timestamp":"1703013960.0","upvote_count":"2","content":"Why not B. BigQuery column-level security:\nDoesn't truly redact the data. The SSN values are still stored in BigQuery, even if hidden from unauthorized users. A potential security breach could expose them.","poster":"MaxNRG"}],"poster":"MaxNRG","comment_id":"1100934","content":"Option A does encrypt SSN but requires managing keys separately.\n\nOption B relies on complex IAM policy changes instead of encrypting by default.\n\nOption C hashes irreversibly, preventing customer service reps from viewing original SSNs when required.\n\nTherefore, using DLP format-preserving encryption before BigQuery ingestion balances both security and analytics requirements for SSN data.","timestamp":"1703013840.0"}]},{"timestamp":"1702548360.0","content":"Selected Answer: D\nEven if you provide Column level access control, The Data Owners or other hierarchies above it will also be able to view very sensitive data. Better to just use encryption and decryption. As this data can also never be used for any analytic workloads","comment_id":"1096286","upvote_count":"3","poster":"Aman47"},{"upvote_count":"3","timestamp":"1699587420.0","comment_id":"1066926","poster":"spicebits","content":"Selected Answer: D\nAnswer has to be D. Question says \"you decide to redact your customers' Government issued Identification Number while allowing customer service representatives to view the original values when necessary\"... Redact... view the original values... D is the only choice."},{"comment_id":"1059471","content":"Selected Answer: B\nIt might not be D! \nSince - only the Frame is kept. the data will be changed. \nFormat Preserving Encryption (FPE), endorsed by NIST, is an advanced encryption technique that transforms data into an encrypted format while preserving its original structure. For instance, a 16-digit credit card number encrypted with FPE will still be a 16-digit number","poster":"Nirca","timestamp":"1698828060.0","upvote_count":"1","comments":[{"comment_id":"1109727","upvote_count":"1","poster":"Helinia","content":"No, the value using FPE can be decrypted with key. \n\"Encrypted values can be re-identified using the original cryptographic key and the entire output value, including surrogate annotation.\"\n\nhttps://cloud.google.com/dlp/docs/pseudonymization#supported-methods","timestamp":"1703939520.0"}]},{"comment_id":"1046741","upvote_count":"1","content":"Selected Answer: B\nCustomer service needs to see the original value, not possible with other options.","timestamp":"1697616960.0","poster":"ffggrre"},{"poster":"kcl10","timestamp":"1696417260.0","comment_id":"1024669","content":"Selected Answer: B\nof course B","upvote_count":"1"},{"upvote_count":"3","poster":"ckanaar","comment_id":"1012322","timestamp":"1695213540.0","comments":[{"poster":"ckanaar","content":"Nevermind, this can actually also be done in the case of answer B. They are both correct, just different implementations. No idea","comment_id":"1013141","upvote_count":"2","timestamp":"1695304500.0"}],"content":"Selected Answer: D\nI believe the crux to the question is that the cryptographic format-preserving encryption token is re-identifiable, whereas the cryptographic hash is not: https://cloud.google.com/dlp/docs/transformations-reference\n\nTherefore, customer service can view the original values when necessary in case of D."},{"upvote_count":"2","content":"the question mentions that \"user data is sent to Pub/Sub before being ingested\" instead of just saying data goes to big query through pub/sub. So some alteration is expected before being injected into the big query. So option D should work.","comment_id":"964328","poster":"knith66","timestamp":"1690432380.0"},{"content":"Selected Answer: D\nD. The question says giving CSR's access to values \"when necessary\" - not default access like given in B. D is a better option using the token.","timestamp":"1690136520.0","upvote_count":"2","comment_id":"960722","poster":"sr25"},{"timestamp":"1688969280.0","poster":"ZZHZZH","content":"Selected Answer: B\nOne of the key requirement is to be able to let authorized personel see the ID. D doesn't specify that.","comment_id":"947796","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: D\nThe answer is between B and D as well described in many comments. \n\nI personally do not see any reason to keep the information available using a token or a mask. It is not a PAN card number, it's just a personal ID. It should not be useful for analytical purposes. \n\nI'm gonna go for D then","comments":[{"comment_id":"897681","timestamp":"1684079520.0","poster":"vaga1","upvote_count":"1","content":"sorry B"}],"timestamp":"1684079520.0","comment_id":"897680","poster":"vaga1"},{"upvote_count":"4","timestamp":"1683108420.0","content":"Selected Answer: D\nhttps://cloud.google.com/dlp/docs/classification-redaction","poster":"mialll","comment_id":"888380"},{"comments":[{"timestamp":"1682847960.0","poster":"Oleksandr0501","content":"gpt: Option D uses Cloud Data Loss Prevention (DLP) to replace input values with a cryptographic format-preserving encryption token before loading the data into BigQuery. This approach allows for more granular control over data access and can provide an added layer of security. However, it may require additional configuration and implementation effort, and it may also affect the performance of queries on the encrypted data.\n\nGoogle recommends using a combination of data protection techniques to safeguard sensitive data, such as encryption, data masking, and access controls. In this scenario, a possible best practice would be to use both options B and D together to provide multiple layers of protection for the sensitive data while still allowing authorized users to view the original values when necessary.","upvote_count":"1","comment_id":"885074","comments":[{"comment_id":"885076","comments":[{"comment_id":"888644","upvote_count":"1","content":"now i ve read and think about better choosing A or B ... \ngarbage question","timestamp":"1683125640.0","poster":"Oleksandr0501"}],"upvote_count":"1","poster":"Oleksandr0501","content":"i`ll take D","timestamp":"1682848020.0"}]}],"content":"gpt: Both options B and D can be used to redact sensitive data while still allowing authorized users to view the original values when necessary. However, the choice between them would depend on specific business requirements and security considerations.\n\nOption B uses BigQuery column-level security to set table permissions for users, allowing only members of the Customer Service user group to view the SSN column. This approach is straightforward and can be implemented easily. However, it requires creating a separate user group for customer service representatives and granting them access to only the required data columns.","comment_id":"885073","poster":"Oleksandr0501","timestamp":"1682847960.0","upvote_count":"1"},{"timestamp":"1681821420.0","upvote_count":"1","content":"Answer is B,\nIf we select C then This approach would also prevent unauthorized access to sensitive data, but it would not allow customer service representatives to view the original values when necessary.","comment_id":"873616","poster":"muhusman"},{"content":"Selected Answer: D\nPII and DLP go hand in hand","comment_id":"870979","upvote_count":"2","timestamp":"1681568160.0","comments":[{"content":"That is not an argument. Option D does not explain how Customer Services will have access.","timestamp":"1690010760.0","comment_id":"959327","poster":"El_Bosco","upvote_count":"1"}],"poster":"streeeber"},{"comment_id":"847094","upvote_count":"1","poster":"juliobs","content":"Selected Answer: D\nD instead of B, because I don't consider access restriction the same as redacting/masking/obscuring/obfuscating/anonymizing.","timestamp":"1679491260.0"},{"comment_id":"844620","upvote_count":"1","timestamp":"1679297820.0","content":"B - Column level access control should be used here.\nNote that there is a better approach is to use dynamic data masking. WIth that, when we build the Query, we don't have to exclude the restricted column","poster":"midgoo"},{"timestamp":"1677361980.0","comment_id":"821855","upvote_count":"3","content":"Selected Answer: B\nOption B is a better solution because it uses BigQuery column-level security to restrict access to the SSN column. This allows customer service representatives to view the original SSN when necessary, while still preventing unauthorized users from accessing the sensitive data. Additionally, this solution does not require any additional steps like encryption or data masking, which can simplify the data pipeline and reduce security risks.","poster":"jonathanthezombieboy"},{"timestamp":"1676746320.0","comment_id":"813404","poster":"musumusu","content":"Answer B: \nc&D will permanently change the value in the column, but they govt wants to see original value whenver they want","upvote_count":"1"},{"content":"Selected Answer: D\nD - the requirement is to redact but B only restricts the access","timestamp":"1676566320.0","poster":"kostol","comment_id":"810931","upvote_count":"1"},{"timestamp":"1675527360.0","upvote_count":"1","content":"Selected Answer: D\nthe requirement to allow customer service representatives to view the original data is the reason why option B (using BigQuery column-level security) is not the best solution. This method would only restrict access to the data, but not allow customer service representatives to view the original values when necessary. Option D (using Cloud Data Loss Prevention (DLP) to replace input values with a cryptographic format-preserving encryption token) would provide the necessary level of redaction while still allowing authorized personnel to access the original data when needed.","comment_id":"798155","poster":"donbigi"},{"timestamp":"1675355820.0","upvote_count":"3","content":"Selected Answer: D\nD. The question does not say SSN data as a column, which might be a string just contains SSN. In this case, option A and B should be out since both them assumed SSN must be a column. Option C is out since hash function is one-way and can't revert back. So option D is the answer. There are couple similar questions in Security Engineer exam for this.","poster":"zanhsieh","comment_id":"796235"},{"timestamp":"1671639960.0","upvote_count":"2","content":"Selected Answer: B\nProblem: Handle PII data Considerations: Redact PII data, but still allow re-identification \nC -> this hashing does not ensure reidentification\nA, D -> this requires that the token and keys to be managed separately. I would be paranoid of staff disclosing these tokens and keys. A more secure way would be to rely on IAM.\n\nWe can mask data using column level security\n- https://cloud.google.com/bigquery/docs/column-data-masking-intro\n- only authorised personnel would be able to access\n\nI'd be more comfortable with B since it relies on IAM roles, rather than having the potential leak of token and keys.","comment_id":"752562","poster":"jkhong"},{"content":"B is answer \nredact- hiding the column i will use bq ACL\nmask- i use DLP","upvote_count":"3","poster":"Atnafu","timestamp":"1671232800.0","comment_id":"747679"},{"comments":[{"poster":"NicolasN","upvote_count":"1","comment_id":"746094","timestamp":"1671109500.0","content":"In case of any doubts, what [A] proposes can be easily done with the following steps:\n1. Load customer data in a table, let's say CUSTOMER\n2. Create keys for every customer id with KEYS.NEW_KEYSET, and store the key and the Customer Id in a new table, named CUST_KEYS\n3. Join the two tables on Customer Id and update the PII field by calling AEAD.ENCRYPT(CUST_KEYS.KEY, ...)\n4. Limit access to CUST_KEYS table only to customer service representatives.\n5. Whenever needed, a representative will have access to raw info by calling AEAD.DECRYPT_STRING on a query joining the two tables"}],"comment_id":"746090","poster":"NicolasN","content":"Selected Answer: A\nIt's [A]! \nAfter thinking a lot about this question, I see that [A] is the most straightforward solution to satisfy both requirements, data redaction and ability to view original data when needed.\n\n[B] works but it doesn't cover the \"when necessary\" condition. Customer service representatives will always have access to PII data.\n[C] is rejected as it is a non reversible de-identification method.\n[D] doesn't describe the re-identification process for customer service representatives and what special rights should be granted to them.\n\nThe following official document describes the same solution with [A] but uses Cloud KMS instead of a separate table for keys:\nhttps://cloud.google.com/bigquery/docs/column-key-encrypt","upvote_count":"2","timestamp":"1671109440.0"},{"upvote_count":"1","poster":"hauhau","comments":[{"timestamp":"1670233980.0","poster":"zellck","content":"for D, encrypted values can be re-identified using the original cryptographic key.\n\nhttps://cloud.google.com/dlp/docs/pseudonymization#supported-methods\nFormat preserving encryption: An input value is replaced with a value that has been encrypted using the FPE-FFX encryption algorithm with a cryptographic key, and then prepended with a surrogate annotation, if specified. By design, both the character set and the length of the input value are preserved in the output value. Encrypted values can be re-identified using the original cryptographic key and the entire output value, including surrogate annotation.","upvote_count":"2","comment_id":"735819"}],"comment_id":"735143","content":"Selected Answer: B\nB just hide but not redact\nD redact but cheange the original value\nthere is no correct answer","timestamp":"1670164260.0"},{"upvote_count":"4","timestamp":"1669812000.0","content":"Selected Answer: D\nD is the answer.\n\nhttps://cloud.google.com/dlp/docs/pseudonymization#supported-methods\nCloud DLP supports three pseudonymization techniques, all of which use cryptographic keys. Following are the available methods:\n- Format preserving encryption: An input value is replaced with a value that has been encrypted using the FPE-FFX encryption algorithm with a cryptographic key, and then prepended with a surrogate annotation, if specified. By design, both the character set and the length of the input value are preserved in the output value. Encrypted values can be re-identified using the original cryptographic key and the entire output value, including surrogate annotation.","comments":[{"upvote_count":"1","content":"\"REDACT\" == DLP","timestamp":"1686105900.0","poster":"29angeltouch","comment_id":"916806"},{"poster":"Wonka87","content":"why would you not consider B which is simpler than maintaining keys with access restriction and then decrypting it for only authorized people. It can be achieved using B also and redact doesn't necessarily mean changing / removing values, restricting to view is also redacting in my opinion. Kindly suggest.","upvote_count":"2","timestamp":"1672216140.0","comment_id":"759570"}],"poster":"zellck","comment_id":"731463"},{"comment_id":"711203","upvote_count":"1","comments":[{"comment_id":"746098","content":"My voting answer explains why finally I prefer [A]","timestamp":"1671109620.0","upvote_count":"1","poster":"NicolasN"}],"timestamp":"1667571180.0","content":"B seems indeed to be a correct answer. Documentation (https://cloud.google.com/bigquery/docs/column-level-security-intro) is referring indirectly to a similar case: \"For example, a policy can enforce access checks such as: You must be in group:high-access to see the columns containing TYPE_SSN\"\nBut I can't argue why answer A couldn't fit except for being more complex.\nIf anyone knows please provide your arguments.","poster":"NicolasN"},{"comment_id":"696684","poster":"devaid","upvote_count":"1","content":"Selected Answer: D\nAnswer: D","timestamp":"1665966780.0"},{"upvote_count":"4","content":"Its D, B does not make sense, they are asking to redact, not hide it away completely","comment_id":"675693","comments":[{"upvote_count":"1","comment_id":"686532","content":"Agree, the key here is \"to see the original values\", that means it already shows a protected value","timestamp":"1664938620.0","poster":"devaid"}],"timestamp":"1663816260.0","poster":"clouditis"},{"poster":"him98934","timestamp":"1662978540.0","content":"I will go with D, https://cloud.google.com/bigquery/docs/column-level-security-intro","upvote_count":"2","comment_id":"666824"},{"content":"Selected Answer: B\nAns B \nhttps://cloud.google.com/bigquery/docs/column-level-security\nRemember the custpomer service group has to view so thry need access from time to time","upvote_count":"1","timestamp":"1662700620.0","comment_id":"664243","poster":"TNT87"},{"comment_id":"658465","poster":"PhuocT","comments":[{"content":"mistake, I did mean B","poster":"PhuocT","comment_id":"658466","timestamp":"1662208980.0","upvote_count":"3"}],"upvote_count":"3","timestamp":"1662208920.0","content":"Selected Answer: D\nI will go with D, https://cloud.google.com/bigquery/docs/column-level-security-intro"}],"answer_ET":"D","answer_images":[],"topic":"1","exam_id":11,"timestamp":"2022-09-02 19:01:00","unix_timestamp":1662138060,"question_id":77,"isMC":true,"choices":{"A":"Use BigQuery's built-in AEAD encryption to encrypt the SSN column. Save the keys to a new table that is only viewable by permissioned users.","C":"Before loading the data into BigQuery, use Cloud Data Loss Prevention (DLP) to replace input values with a cryptographic hash.","D":"Before loading the data into BigQuery, use Cloud Data Loss Prevention (DLP) to replace input values with a cryptographic format-preserving encryption token.","B":"Use BigQuery column-level security. Set the table permissions so that only members of the Customer Service user group can see the SSN column."},"question_images":[],"answer":"D","answers_community":["D (67%)","B (31%)","3%"],"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/79494-exam-professional-data-engineer-topic-1-question-168/","question_text":"You work for a financial institution that lets customers register online. As new customers register, their user data is sent to Pub/Sub before being ingested into\nBigQuery. For security reasons, you decide to redact your customers' Government issued Identification Number while allowing customer service representatives to view the original values when necessary. What should you do?"},{"id":"vUVrKqlY505wc0OmQ7VX","answer_ET":"A","answer":"A","unix_timestamp":1662181800,"timestamp":"2022-09-03 07:10:00","question_id":78,"answer_images":[],"answers_community":["A (72%)","B (21%)","7%"],"answer_description":"","question_images":[],"url":"https://www.examtopics.com/discussions/google/view/79685-exam-professional-data-engineer-topic-1-question-169/","isMC":true,"discussion":[{"comment_id":"659536","poster":"AWSandeep","upvote_count":"9","timestamp":"1662324660.0","content":"Selected Answer: A\nA. Partition by transaction time; cluster by state first, then city, then store ID."},{"comment_id":"747677","timestamp":"1671232320.0","upvote_count":"7","content":"A\nPartitioning is obvious\nClustering is already mentioned in the question \n past 30 days and to look at purchasing trends by \nstate, \ncity, and \nindividual store","poster":"Atnafu"},{"upvote_count":"1","poster":"SamuelTsch","timestamp":"1730105280.0","comment_id":"1303888","content":"Selected Answer: A\ngo to A."},{"comments":[{"timestamp":"1703015040.0","content":"For optimal query performance in BigQuery, especially for the described use cases of analyzing sales data by time and geographical hierarchies, the data should be organized to minimize the amount of data scanned during queries. Given the frequent queries over the past 30 days and analysis by location, the best approach is:\n\nOption A: Partition by transaction time; cluster by state first, then city, then store ID.","poster":"MaxNRG","comments":[{"content":"Partitioning the table by transaction time allows for efficient querying over specific time ranges, such as the past 30 days, which reduces costs and improves performance because it limits the amount of data scanned. \n\nClustering by state, then city, and then store ID aligns with the hierarchy of geographical data and the types of queries that are run against the dataset. It organizes the data within each partition so that queries filtering by state, city, or store ID—or any combination of these—are optimized, as BigQuery can limit the scan to just the relevant clusters within the partitions.","poster":"MaxNRG","upvote_count":"2","comment_id":"1100950","timestamp":"1703015040.0"}],"upvote_count":"1","comment_id":"1100949"}],"content":"Selected Answer: B\nover the past 30 days -> partitioning\nby state, city, and individual store -> cluster order","timestamp":"1703015040.0","poster":"MaxNRG","comment_id":"1100948","upvote_count":"3"},{"poster":"tibuenoc","timestamp":"1700493240.0","comment_id":"1075537","content":"Selected Answer: B\nPartition by ingest time\nPartition by specified data column (Id, State and City)","upvote_count":"1"},{"poster":"ffggrre","comments":[{"content":"Even though its a timestamp, the partitioning can be configured on a daily granularity, so A is correct (https://cloud.google.com/bigquery/docs/partitioned-tables#date_timestamp_partitioned_tables)","upvote_count":"1","comment_id":"1259348","timestamp":"1722512940.0","poster":"sylva1212"}],"content":"Selected Answer: C\nPartition by transaction time would lead to too many partitions - if it was a date, it would have made sense.","upvote_count":"1","comment_id":"1046743","timestamp":"1697617140.0"},{"timestamp":"1696441260.0","poster":"aureole","comment_id":"1025042","upvote_count":"1","content":"Selected Answer: C\nIt should be C. not A"},{"comment_id":"1025041","timestamp":"1696441140.0","upvote_count":"1","content":"I think it should be C.\nThe fact that we partition the table with the time of the transaction will result many transactions in each day, so it will affect negatively the query performance.\ni.e : by the end of the day I will have many partitions if I use the transaction time. A would be correct if the partition was by date and not by time.\nResponse: C.","poster":"aureole"},{"comment_id":"897683","poster":"vaga1","upvote_count":"4","timestamp":"1684080120.0","content":"Selected Answer: A\nPartitioning for time is obvious to improve performance and costs of querying only the last 30 days of the table. \n\nSo, the answer is A or B.\n\nhttps://cloud.google.com/bigquery/docs/querying-clustered-tables\n\n\"... To get the benefits of clustering, include all of the clustered columns or a subset of the columns in left-to-right sort order, starting with the first column.\"\n\nThis means that it is a better choice to sort the table rows by region-province-city (region-state-city in the US case). \n\nSo, the answer is A."},{"comment_id":"750878","content":"Selected Answer: B\nShould be B\nThe clustering should be according to the filtering needs","poster":"Prakzz","timestamp":"1671540420.0","upvote_count":"2"},{"poster":"zellck","upvote_count":"4","comment_id":"731458","content":"Selected Answer: A\nA is the answer.\n\nhttps://cloud.google.com/bigquery/docs/partitioned-tables\nThis page provides an overview of partitioned tables in BigQuery. A partitioned table is a special table that is divided into segments, called partitions, that make it easier to manage and query your data. By dividing a large table into smaller partitions, you can improve query performance, and you can control costs by reducing the number of bytes read by a query.\nYou can partition BigQuery tables by:\n- Time-unit column: Tables are partitioned based on a TIMESTAMP, DATE, or DATETIME column in the table.\n\nhttps://cloud.google.com/bigquery/docs/clustered-tables\nClustered tables in BigQuery are tables that have a user-defined column sort order using clustered columns. Clustered tables can improve query performance and reduce query costs.","timestamp":"1669811760.0"},{"upvote_count":"2","content":"https://cloud.google.com/bigquery/docs/querying-clustered-tables","timestamp":"1662701580.0","poster":"TNT87","comment_id":"664252"},{"content":"Selected Answer: A\nA\nThe question mention that the query is 30 days recently","upvote_count":"3","comment_id":"658088","poster":"ducc","timestamp":"1662181800.0"}],"exam_id":11,"topic":"1","question_text":"You are migrating a table to BigQuery and are deciding on the data model. Your table stores information related to purchases made across several store locations and includes information like the time of the transaction, items purchased, the store ID, and the city and state in which the store is located. You frequently query this table to see how many of each item were sold over the past 30 days and to look at purchasing trends by state, city, and individual store. How would you model this table for the best query performance?","choices":{"C":"Top-level cluster by state first, then city, then store ID.","A":"Partition by transaction time; cluster by state first, then city, then store ID.","B":"Partition by transaction time; cluster by store ID first, then city, then state.","D":"Top-level cluster by store ID first, then city, then state."}},{"id":"HToIeEndBV38qdctGlXP","answer":"D","answer_images":[],"exam_id":11,"answers_community":["D (100%)"],"isMC":true,"discussion":[{"comment_id":"475020","timestamp":"1636485060.0","content":"D is correct because it uses managed services, and also allows for the data to persist on GCS beyond the life of the cluster.\nA is not correct because the goal is to re-use their Hadoop jobs and MapReduce and/or Spark jobs cannot simply be moved to Dataflow.\nB is not correct because the goal is to persist the data beyond the life of the ephemeral clusters, and if HDFS is used as the primary attached storage mechanism, it will also disappear at the end of the cluster’s life.\nC is not correct because the goal is to use managed services as much as possible, and this is the opposite.\nE is not correct because the goal is to use managed services as much as possible, and this is the opposite.","comments":[{"timestamp":"1732714020.0","content":"B is incorrect bcoz, it did not say 'off cluster' persistent HDFS discs???","comment_id":"1318682","poster":"certs4pk","upvote_count":"1"}],"poster":"MaxNRG","upvote_count":"12"},{"comment_id":"214273","content":"The correct answer is D. Here is the explanation to why Data proc and why not Data flow. \nWhen a company wants to move their existing Hadoop jobs on premise to cloud, we can simply move the jobs in cloud data prod and replace hdfs with gs:// which is google storage. This way you are keeping compute and storage separately. Hence the correct answer is D. However, if the company wants to complete create a new jobs and don’t want to use the existing Hadoop jobs running on premise, the option is to create new data flow jobs.","poster":"Radhika7983","timestamp":"1604692200.0","upvote_count":"6"},{"comment_id":"1008581","upvote_count":"2","content":"Selected Answer: D\nD. Create a Cloud Dataproc cluster that uses the Google Cloud Storage connector.\nDataproc clusters can be created to lift and shift existing Hadoop jobs\nData stored in Google Cloud Storage extends beyond the life of a Dataproc cluster.","timestamp":"1727155680.0","poster":"suku2"},{"comments":[{"upvote_count":"1","comment_id":"1318683","content":"what if option B said, 'off cluster' persistent HDFS disks?","timestamp":"1732714080.0","poster":"certs4pk"}],"timestamp":"1727155680.0","comment_id":"1027037","content":"D. Create a Cloud Dataproc cluster that uses the Google Cloud Storage connector.\n\nHere's why:\n\n Cloud Dataproc allows you to run Apache Hadoop jobs with minimal management. It is a managed Hadoop service.\n\n Using the Google Cloud Storage (GCS) connector, Dataproc can access data stored in GCS, which allows data persistence beyond the life of the cluster. This means that even if the cluster is deleted, the data in GCS remains intact. Moreover, using GCS is often cheaper and more durable than using HDFS on persistent disks.","poster":"imran79","upvote_count":"1"},{"timestamp":"1727155680.0","content":"Selected Answer: D\nD. Create a Cloud Dataproc cluster that uses the Google Cloud Storage connector.\n\nGoogle Cloud Dataproc is a managed Hadoop and Spark service that allows you to easily create and manage Hadoop clusters in the cloud. By using the Google Cloud Storage connector, you can persist data in Google Cloud Storage, which provides durable storage beyond the cluster's lifecycle. This approach ensures data is retained even if the cluster is terminated, and it allows you to reuse your existing Hadoop jobs.\n\nOption B (Creating a Dataproc cluster that uses persistent disks for HDFS) is another valid choice. However, using Google Cloud Storage for data storage and processing is often more cost-effective and scalable, especially when migrating to the cloud.\n\nOptions A, C, and E do not take full advantage of Google Cloud's services and the benefits of cloud-native data storage and processing with Google Cloud Storage and Dataproc.","upvote_count":"3","comment_id":"1050496","poster":"rtcpost"},{"content":"Option D is incorrect, as it would not provide persistent HDFS storage within cluster itself. Rather B should be the correct answer.","timestamp":"1719577080.0","comment_id":"1238709","upvote_count":"1","poster":"fahadminhas"},{"upvote_count":"1","content":"Correct D","timestamp":"1694771520.0","poster":"kshehadyx","comment_id":"1008345"},{"content":"Selected Answer: D\nHadoop --> Dataproc Persistent storage after the processing --> GCS","upvote_count":"2","poster":"bha11111","timestamp":"1678510140.0","comment_id":"835665"},{"timestamp":"1673451540.0","poster":"samdhimal","comment_id":"772685","upvote_count":"1","content":"Selected Answer: D\nD Seems right. Cloud storage can be used to achieve data storage even after the life of cluster."},{"comment_id":"768261","upvote_count":"1","content":"Selected Answer: D\nThe answer is D! Dataproc have no need for use to manage the infra and cloudstorage also no need for us to manage too!","timestamp":"1673070180.0","poster":"korntewin"},{"timestamp":"1670776680.0","comment_id":"741892","poster":"Nirca","upvote_count":"1","content":"Selected Answer: D\nD. Create a Cloud Dataproc cluster that uses the Google Cloud Storage connector."},{"poster":"assU2","upvote_count":"1","comment_id":"712031","comments":[{"comments":[{"comment_id":"712041","content":"and it says that only VM Boot disks are deleted when the cluster is deleted.","upvote_count":"2","timestamp":"1667687700.0","poster":"assU2"}],"content":"although:\nBy default, when no local SSDs are provided, HDFS data and intermediate shuffle data is stored on VM boot disks, which are Persistent Disks.","timestamp":"1667687340.0","upvote_count":"1","comment_id":"712036","poster":"assU2"}],"timestamp":"1667687220.0","content":"Seems like it is D. https://cloud.google.com/dataproc/docs/concepts/dataproc-hdfs\nNever saw they mentioned persistent disks, although they are not deleted with the clusters..."},{"timestamp":"1665987900.0","content":"Selected Answer: D\nCorrect Answer : D","comment_id":"696989","poster":"achafill","upvote_count":"1"},{"comment_id":"681457","upvote_count":"1","timestamp":"1664346780.0","content":"Selected Answer: D\nDataproc cluster set up will be ephemeral to run HDFS Jobs and can be killed after Job execution killing persistent storage with cluster","poster":"nkunwar"},{"upvote_count":"1","content":"Anwer: D","comment_id":"648698","timestamp":"1660876500.0","poster":"crisimenjivar"},{"poster":"Asheesh1909","comment_id":"612259","upvote_count":"1","timestamp":"1654504380.0","content":"Isn't it A and D both dataflow for reusable jobs and gcs for data peraistance?"},{"comment_id":"584107","timestamp":"1649664240.0","content":"Selected Answer: D\nTwo key points:\nManaged hadoop cluster - dataproc\nPersistent storage: GCS (dataproc uses gcs connector to connect to gcs)","poster":"kmaiti","upvote_count":"2"},{"comment_id":"523640","timestamp":"1642180560.0","content":"Selected Answer: D\nThis question if from Practice Test by Google, they gave D as right answer","poster":"deep_ROOT","upvote_count":"2"},{"content":"Ans: D\ni) re-use hadoop jobs with minimum cluster management - Cloud Dataproc\nii) Persist data beyond the life of cluster - use Cloud Storage.\n\nSo Cloud Dataproc with Google Cloud Storage connector to output results to cloud storage.","timestamp":"1634310180.0","poster":"anji007","upvote_count":"3","comment_id":"462666"},{"comment_id":"373355","content":"why not B?","comments":[{"timestamp":"1624644480.0","content":"Because in Question it's mentioned - They also want to be able to persist data beyond the life of the cluster. \nIf we use 'Persistent disk of cluster' - So once cluster get stopped or deleted - data get lost","poster":"sumanshu","upvote_count":"5","comment_id":"390694"}],"poster":"Jeyakaran","timestamp":"1622706780.0","upvote_count":"1"},{"content":"correct D","upvote_count":"3","comment_id":"284972","timestamp":"1612630380.0","poster":"naga"},{"content":"I think it should be B. The question targets as is migration of hadoop jobs. So the HDFS storage should be retained as is in persistent disk.","comment_id":"261930","poster":"DeepakSai010101","timestamp":"1610031960.0","comments":[{"upvote_count":"6","poster":"balseron99","content":"minimize the management of the cluster as much as possible. They also want to be able to persist data beyond the life of the cluster.\n\nIn Dataproc, best practice is as soon as the job is done, the cluster is shutdown/deleted which will remove the data also in the cluster. They want storage beyond the life of the cluster which B option won't provide.","timestamp":"1612105680.0","comment_id":"280609"}],"upvote_count":"2"},{"poster":"arghya13","comment_id":"204490","content":"D is the correct answer..","timestamp":"1603431240.0","upvote_count":"2"},{"timestamp":"1601297340.0","comment_id":"189023","upvote_count":"4","content":"This quote is from the Official Google Cloud Certified Professional Data Engineer Study Guide by Dan Sullivan: \"Cloud Dataproc is a good choice for implementing ETL processes if you are migrating existing Hadoop or Spark programs. Cloud Dataflow is the recommended tool for developing new ETL processes.\" This would seem to support the use of Dataproc instead of Dataflow. Answer D does seem to be the best choice.","poster":"GregDT"},{"upvote_count":"3","comment_id":"150658","poster":"Archy","content":"D, as it will also give data beyond the cluster life.","timestamp":"1596566220.0"},{"upvote_count":"2","content":"Correct Answer : C\nThis option is correct as the requirement is to reuse Hadoop\njobs with minimizing the infrastructure management with the ability to store\ndata in a durable external storage, Dataproc with Cloud Storage would be\nan ideal solution.","timestamp":"1595160180.0","poster":"VishalB","comments":[{"content":"so it should be D, Dataproc + GCS via GCS connector","timestamp":"1597870800.0","poster":"atnafu2020","comment_id":"161796","upvote_count":"5"},{"comment_id":"390696","timestamp":"1624644900.0","content":"In Question, it's mention - minimize the management of the cluster as much as possible.\n\nIf we create our own Hadoop Cluster then we are NOT minimizing the overhead of infrastructure","upvote_count":"1","poster":"sumanshu"}],"comment_id":"138646"}],"choices":{"B":"Create a Google Cloud Dataproc cluster that uses persistent disks for HDFS.","C":"Create a Hadoop cluster on Google Compute Engine that uses persistent disks.","A":"Create a Google Cloud Dataflow job to process the data.","E":"Create a Hadoop cluster on Google Compute Engine that uses Local SSD disks.","D":"Create a Cloud Dataproc cluster that uses the Google Cloud Storage connector."},"answer_ET":"D","timestamp":"2020-03-16 11:26:00","answer_description":"","question_id":79,"unix_timestamp":1584354360,"url":"https://www.examtopics.com/discussions/google/view/16730-exam-professional-data-engineer-topic-1-question-17/","question_images":[],"topic":"1","question_text":"Your company is migrating their 30-node Apache Hadoop cluster to the cloud. They want to re-use Hadoop jobs they have already created and minimize the management of the cluster as much as possible. They also want to be able to persist data beyond the life of the cluster. What should you do?"},{"id":"yOzIwfJGy9H9m5bw86nK","timestamp":"2022-09-02 19:38:00","choices":{"B":"Create a Pub/Sub snapshot before deploying new subscriber code. Use a Seek operation to re-deliver messages that became available after the snapshot was created.","D":"Enable dead-lettering on the Pub/Sub topic to capture messages that aren't successfully acknowledged. If an error occurs after deployment, re-deliver any messages captured by the dead-letter queue.","C":"Use Cloud Build for your deployment. If an error occurs after deployment, use a Seek operation to locate a timestamp logged by Cloud Build at the start of the deployment.","A":"Set up the Pub/Sub emulator on your local machine. Validate the behavior of your new subscriber logic before deploying it to production."},"answers_community":["B (83%)","Other"],"discussion":[{"poster":"AWSandeep","content":"Selected Answer: B\nB. Create a Pub/Sub snapshot before deploying new subscriber code. Use a Seek operation to re-deliver messages that became available after the snapshot was created.\n\nAccording to the second reference in the list below, a concern with deploying new subscriber code is that the new executable may erroneously acknowledge messages, leading to message loss. Incorporating snapshots into your deployment process gives you a way to recover from bugs in new subscriber code.\n\nAnswer cannot be C because To seek to a timestamp, you must first configure the subscription to retain acknowledged messages using retain-acked-messages. If retain-acked-messages is set, Pub/Sub retains acknowledged messages for 7 days.\n\nReferences: \nhttps://cloud.google.com/pubsub/docs/replay-message\nhttps://cloud.google.com/pubsub/docs/replay-overview#seek_use_cases","comment_id":"657667","upvote_count":"13","timestamp":"1677785880.0","comments":[{"upvote_count":"1","timestamp":"1686145440.0","poster":"jkhong","comment_id":"738065","content":"Don't think we need to configure subscription to retain ack messages. It is defaulted to retain for 7 days"}]},{"comments":[{"comment_id":"1337857","upvote_count":"1","timestamp":"1736319600.0","poster":"Pime13","content":"not D because: Dead-lettering is useful for handling messages that can't be processed successfully, but it doesn't help with messages that have been erroneously acknowledged. Once a message is acknowledged, it is considered processed and won't be sent to the dead-letter queue."}],"comment_id":"1337856","timestamp":"1736319600.0","upvote_count":"1","poster":"Pime13","content":"Selected Answer: B\nCreating a snapshot allows you to capture the state of your subscription at a specific point in time. If an error occurs, you can use the Seek operation to reset the acknowledgment state of messages to the snapshot, ensuring that no messages are lost\n\nhttps://cloud.google.com/pubsub/docs/reference/rest/v1/Snapshot"},{"comment_id":"1335224","poster":"f74ca0c","content":"Selected Answer: C\nC. Use a BigQuery view to define your preprocessing logic. When creating your model, use the view as your model training data. At prediction time, use BigQuery's ML.EVALUATE clause without specifying any transformations on the raw input data.\n\nExplanation:\nPreventing Data Skew:\n\nTraining-serving skew occurs when the transformations applied to training data are not identically applied to prediction data. Using a BigQuery view ensures consistent preprocessing for both training and prediction.\nAdvantages of BigQuery Views:\n\nViews encapsulate preprocessing logic, ensuring that the same transformations are applied whenever the view is queried.\nBy referencing the view during both training and prediction, you eliminate the need for manual transformations and the risk of discrepancies.","upvote_count":"1","timestamp":"1735747680.0"},{"content":"Selected Answer: B\nTaking a snapshot allows redelivering messages that were published while any faulty subscriber logic was running. \nThe seek timestamp would come after deployment so even erroneously acknowledged messages could be recovered.\nhttps://cloud.google.com/pubsub/docs/replay-overview#seek_use_cases\nBy creating a snapshot of the subscription before deploying new code, you can preserve the state of unacknowledged messages. If after deployment you find that the new subscriber code is erroneously acknowledging messages, you can use the Seek operation with the snapshot to reset the subscription's acknowledgment state to the time the snapshot was created. This would effectively re-deliver messages available since the snapshot, ensuring you can recover from errors. This approach does not require setting up a local emulator and directly addresses the concern of message loss due to erroneous acknowledgments.","comment_id":"1100954","upvote_count":"2","poster":"MaxNRG","timestamp":"1718819580.0"},{"comment_id":"1005111","timestamp":"1710190380.0","content":"Selected Answer: B\nB.\n\nfrom the documentation: \nhttps://cloud.google.com/pubsub/docs/replay-message \nPub/Sub cannot retrieve the messages after you have acknowledged them. However, sometimes you might find it necessary to replay the acknowledged messages, for example, if you performed an erroneous acknowledgment. Then you can use the Seek feature to mark previously acknowledged messages as unacknowledged, and force Pub/Sub to redeliver those messages. You can also use seek to delete the unacknowledged messages by changing their state to acknowledged.","poster":"[Removed]","upvote_count":"3"},{"comment_id":"962501","content":"pls correct me if I am wrong , option B Option B only allows you to re-deliver messages that were available before the snapshot was created. If an error occurs after the snapshot was created, you will not be able to re-deliver those messages.","poster":"vamgcp","upvote_count":"2","timestamp":"1706177640.0"},{"poster":"cetanx","content":"Selected Answer: A\nQ: You are concerned that upon deployment the subscriber may erroneously acknowledge messages, leading to message loss. \n -> So the message is mistakenly acked and removed from topic/subscription. This means even if you have a snapshot of pre-deployment but you don't have a backup or copy of post-deployment messages.\n\nQ: Your subscriber is not set up to retain acknowledged messages.\n -> To seek to a time in the past and replay previously-acknowledged messages, \"you must first configure message retention on the topic\" or \"configure the subscription to retain acknowledged messages\" (ref: https://cloud.google.com/pubsub/docs/replay-overview#configuring_message_retention)\n\nSo B, C, D do not solve the problem of erroneously acked messages as long as you don't have message retention configured on topic/subscription.","comment_id":"943771","timestamp":"1704467340.0","upvote_count":"3"},{"comment_id":"850854","content":"Selected Answer: D\nYou are updating the code for a subscriber to a Pub/Sub feed. You are concerned that upon deployment the subscriber may erroneously acknowledge messages, leading to message loss. Your subscriber is not set up to retain acknowledged messages. What should you do to ensure that you can recover from errors after deployment?\nA. Set up the Pub/Sub emulator on your local machine. Validate the behavior of your new subscriber logic before deploying it to production.\nB. Create a Pub/Sub snapshot before deploying new subscriber code. Use a Seek operation to re-deliver messages that became available after the snapshot was created.\nC. Use Cloud Build for your deployment. If an error occurs after deployment, use a Seek operation to locate a timestamp logged by Cloud Build at the start of the deployment.\nD. Enable dead-lettering on the Pub/Sub topic to capture messages that aren't successfully acknowledged. If an error occurs after deployment, re-deliver any messages captured by the dead-letter queue.","poster":"lucaluca1982","upvote_count":"1","timestamp":"1695720120.0"},{"timestamp":"1692378360.0","comment_id":"813416","content":"Option D:\nDead letter option allow you to recover message from errors after deployment by re-delivering any messages captured by the dead-letter queue. \nhttps://cloud.google.com/pubsub/docs/handling-failures#dead_letter_topic\nwhy not B, \nbecause snapshot is time taking process and if messages were erroneously acknowledged, it will not bring them back. It is useful when you want to secure the current data and want to make changes","comments":[{"comment_id":"833053","poster":"wjtb","upvote_count":"7","timestamp":"1694175840.0","content":"Dead letter queue would help if the messages would not get acknowledged, however here they are talking about messages being erroneously acknowledged. Pub/Sub would interpret the message as being succesfully processed -> they would not end up in the dead-letter queue -> D is wrong"}],"upvote_count":"2","poster":"musumusu"},{"poster":"zellck","timestamp":"1685442660.0","content":"Selected Answer: B\nB is the answer.\n\nhttps://cloud.google.com/pubsub/docs/replay-overview\nThe Seek feature extends subscriber functionality by allowing you to alter the acknowledgement state of messages in bulk. For example, you can replay previously acknowledged messages or purge messages in bulk. In addition, you can copy the state of one subscription to another by using seek in combination with a Snapshot.","comment_id":"731453","upvote_count":"3"},{"content":"B\nThe Seek feature extends subscriber functionality by allowing you to alter the acknowledgement state of messages in bulk. For example, you can replay previously acknowledged messages or purge messages in bulk. In addition, you can copy the state of one subscription to another by using seek in combination with a Snapshot.\n\nhttps://cloud.google.com/pubsub/docs/replay-overview","poster":"Atnafu","timestamp":"1684852560.0","comment_id":"725268","upvote_count":"1"},{"upvote_count":"1","timestamp":"1678348020.0","content":"Selected Answer: B\nAnswer B","comment_id":"664257","poster":"TNT87"},{"comment_id":"658473","timestamp":"1677854760.0","upvote_count":"1","content":"Selected Answer: B\nshould be B.","poster":"PhuocT"}],"url":"https://www.examtopics.com/discussions/google/view/79515-exam-professional-data-engineer-topic-1-question-170/","answer_ET":"B","unix_timestamp":1662140280,"exam_id":11,"answer_description":"","question_text":"You are updating the code for a subscriber to a Pub/Sub feed. You are concerned that upon deployment the subscriber may erroneously acknowledge messages, leading to message loss. Your subscriber is not set up to retain acknowledged messages. What should you do to ensure that you can recover from errors after deployment?","question_images":[],"topic":"1","question_id":80,"answer":"B","answer_images":[],"isMC":true}],"exam":{"numberOfQuestions":319,"isBeta":false,"isMCOnly":true,"provider":"Google","lastUpdated":"11 Apr 2025","isImplemented":true,"name":"Professional Data Engineer","id":11},"currentPage":16},"__N_SSP":true}