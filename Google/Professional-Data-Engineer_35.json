{"pageProps":{"questions":[{"id":"fG7EfqvRu85Xhsix0Kis","answer_ET":"D","choices":{"D":"Create a denormalized, append-only model with nested and repeated fields. Use the ingestion timestamp to track historical data.","B":"Create a normalized model with tables for each entity. Keep all input files in a Cloud Storage bucket to track historical data.","C":"Create a denormalized model with nested and repeated fields. Update the table and use snapshots to track historical data.","A":"Create a normalized model with tables for each entity. Use snapshots before updates to track historical data."},"answer_images":[],"question_id":171,"isMC":true,"question_text":"You are designing a data warehouse in BigQuery to analyze sales data for a telecommunication service provider. You need to create a data model for customers, products, and subscriptions. All customers, products, and subscriptions can be updated monthly, but you must maintain a historical record of all data. You plan to use the visualization layer for current and historical reporting. You need to ensure that the data model is simple, easy-to-use, and cost-effective. What should you do?","url":"https://www.examtopics.com/discussions/google/view/130203-exam-professional-data-engineer-topic-1-question-252/","discussion":[{"content":"- A denormalized, append-only model simplifies query complexity by eliminating the need for joins. \n- Adding data with an ingestion timestamp allows for easy retrieval of both current and historical states. \n- Instead of updating records, new records are appended, which maintains historical information without the need to create separate snapshots.","timestamp":"1720131480.0","comment_id":"1114139","upvote_count":"11","poster":"raaad"},{"comment_id":"1154484","poster":"JyoGCP","upvote_count":"1","content":"Selected Answer: D\nOption D","timestamp":"1724120040.0"},{"timestamp":"1721197980.0","content":"Selected Answer: D\nStraight forward, good for costs","poster":"JimmyBK","upvote_count":"1","comment_id":"1124792"},{"timestamp":"1720510620.0","comment_id":"1117347","content":"Selected Answer: D\nD looks logical","poster":"Sofiia98","upvote_count":"1"},{"poster":"GCP001","upvote_count":"1","comment_id":"1117059","content":"Selected Answer: D\nEasy, cost effective and no cpmpexity","timestamp":"1720469640.0"},{"comment_id":"1112884","content":"Selected Answer: D\nD. Create a denormalized, append-only model with nested and repeated fields. Use the ingestion timestamp to track historical data.","poster":"scaenruy","upvote_count":"2","timestamp":"1720012800.0"}],"answer_description":"","question_images":[],"unix_timestamp":1704295200,"exam_id":11,"answers_community":["D (100%)"],"timestamp":"2024-01-03 16:20:00","topic":"1","answer":"D"},{"id":"ASz8QJv0U4lMfIhz0MuZ","question_text":"You are deploying a batch pipeline in Dataflow. This pipeline reads data from Cloud Storage, transforms the data, and then writes the data into BigQuery. The security team has enabled an organizational constraint in Google Cloud, requiring all Compute Engine instances to use only internal IP addresses and no external IP addresses. What should you do?","answer_description":"","topic":"1","discussion":[{"poster":"raaad","comments":[{"comments":[{"upvote_count":"3","poster":"BIGQUERY_ALT_ALT","content":"VPC Service Controls are typically used to define and enforce security perimeters around APIs and services, restricting their access to a specified set of Google Cloud projects. In this scenario, the security constraint is focused on Compute Engine instances used by Dataflow, and VPC Service Controls might be considered a bit heavy-handed for just addressing the internal IP address requirement.","comment_id":"1119298","timestamp":"1720660500.0"},{"comments":[{"comment_id":"1117072","content":"ref - https://cloud.google.com/dataflow/docs/guides/routes-firewall","upvote_count":"4","poster":"GCP001","timestamp":"1720470240.0"}],"timestamp":"1720470180.0","poster":"GCP001","upvote_count":"5","comment_id":"1117069","content":"Even if you create VPC service control, your dataflow worker will run on google compute engine instances with private ips only after policy enforcement. \nWithout external IP addresses, you can still perform administrative and monitoring tasks. \nYou can access your workers by using SSH through the options listed in the preceding list. However, the pipeline cannot access the internet, and internet hosts cannot access your Dataflow workers."}],"comment_id":"1115542","timestamp":"1720308720.0","upvote_count":"3","content":"why not C?","poster":"Jordan18"}],"content":"Selected Answer: D\n- Private Google Access for services allows VM instances with only internal IP addresses in a VPC network or on-premises networks (via Cloud VPN or Cloud Interconnect) to reach Google APIs and services. \n- When you launch a Dataflow job, you can specify that it should use worker instances without external IP addresses if Private Google Access is enabled on the subnetwork where these instances are launched. \n- This way, your Dataflow workers will be able to access Cloud Storage and BigQuery without violating the organizational constraint of no external IPs.","timestamp":"1720132380.0","upvote_count":"5","comment_id":"1114143"},{"timestamp":"1733655180.0","content":"Selected Answer: D\nNo way it is C. \nLike the use case for Google VPC Service Controls perimeter is not to establish secure connectivity on its own but rather to control connectivity, like allowing vms within x premise to access, and blocking vms outside premise even if in same VPC from access. \n\nD on the other hand is completely sensical.","upvote_count":"1","comment_id":"1226646","poster":"Lestrang"},{"upvote_count":"1","poster":"Moss2011","comment_id":"1163248","content":"Selected Answer: C\nAccording to this documentation: https://cloud.google.com/vpc-service-controls/docs/overview I think the correct answer is C. Take into account the phrase \"organizational constraint\" and the VPC Service Control allow you to do that.","timestamp":"1725164400.0"},{"upvote_count":"1","timestamp":"1724954460.0","comment_id":"1162975","poster":"Tryolabs","content":"Selected Answer: D\nhttps://cloud.google.com/vpc/docs/private-google-access\n\n\"VM instances that only have internal IP addresses (no external IP addresses) can use Private Google Access. They can reach the external IP addresses of Google APIs and services.\""},{"timestamp":"1722190200.0","upvote_count":"1","content":"Selected Answer: C\nIt should be C","comment_id":"1134398","poster":"pandeyspecial"},{"poster":"Matt_108","comments":[{"upvote_count":"2","content":"Missclicked the answer <.<","comment_id":"1121714","poster":"Matt_108","timestamp":"1720872840.0"}],"timestamp":"1720872780.0","content":"Selected Answer: C\nOption D, as GCP001 said","upvote_count":"1","comment_id":"1121713"},{"upvote_count":"4","timestamp":"1720469940.0","content":"Selected Answer: D\nhttps://cloud.google.com/dataflow/docs/guides/routes-firewall","poster":"GCP001","comment_id":"1117065"},{"content":"Selected Answer: C\nC. Create a VPC Service Controls perimeter that contains the VPC network and add Dataflow, Cloud Storage, and BigQuery as allowed services in the perimeter. Use Dataflow with only internal IP addresses.","comment_id":"1112891","timestamp":"1720013220.0","comments":[],"upvote_count":"1","poster":"scaenruy"}],"question_images":[],"exam_id":11,"answer_images":[],"answer_ET":"D","unix_timestamp":1704295620,"question_id":172,"answers_community":["D (73%)","C (27%)"],"url":"https://www.examtopics.com/discussions/google/view/130204-exam-professional-data-engineer-topic-1-question-253/","timestamp":"2024-01-03 16:27:00","choices":{"D":"Ensure that Private Google Access is enabled in the subnetwork. Use Dataflow with only internal IP addresses.","C":"Create a VPC Service Controls perimeter that contains the VPC network and add Dataflow, Cloud Storage, and BigQuery as allowed services in the perimeter. Use Dataflow with only internal IP addresses.","A":"Ensure that your workers have network tags to access Cloud Storage and BigQuery. Use Dataflow with only internal IP addresses.","B":"Ensure that the firewall rules allow access to Cloud Storage and BigQuery. Use Dataflow with only internal IPs."},"answer":"D","isMC":true},{"id":"U3CwR9Ou9ZZ6H4GPDT8p","answer":"B","isMC":true,"answer_images":[],"exam_id":11,"choices":{"A":"Enable Vertical Autoscaling to let the pipeline use larger workers.","B":"Change the pipeline code, and introduce a Reshuffle step to prevent fusion.","C":"Update the job to increase the maximum number of workers.","D":"Use Dataflow Prime, and enable Right Fitting to increase the worker resources."},"answers_community":["B (92%)","4%"],"unix_timestamp":1704296040,"question_images":[],"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/130205-exam-professional-data-engineer-topic-1-question-254/","answer_ET":"B","discussion":[{"timestamp":"1704415140.0","content":"Selected Answer: B\n- Fusion optimization in Dataflow can lead to steps being \"fused\" together, which can sometimes hinder parallelization. \n- Introducing a Reshuffle step can prevent fusion and force the distribution of work across more workers. \n- This can be an effective way to improve parallelism and potentially trigger the autoscaler to increase the number of workers.","upvote_count":"16","comment_id":"1114145","poster":"raaad"},{"timestamp":"1723287720.0","comment_id":"1263441","upvote_count":"1","content":"Selected Answer: B\nhttps://cloud.google.com/dataflow/docs/pipeline-lifecycle#prevent_fusion","poster":"meh_33"},{"comment_id":"1226654","content":"Selected Answer: C\nRight fitting is for declaration, declaring the correct resources will not help. Reshuffling step is what can prevent fusion which can lead to unused workers.","upvote_count":"1","poster":"Lestrang","timestamp":"1717838100.0"},{"comment_id":"1152540","upvote_count":"3","poster":"ML6","content":"Selected Answer: B\nFusion occurs when multiple transformations are fused into a single stage, which can limit parallelism and hinder performance, especially in streaming pipelines. By introducing a Reshuffle step, you break fusion and allow for better parallelism.","timestamp":"1708173900.0"},{"comment_id":"1146402","content":"https://cloud.google.com/dataflow/docs/guides/right-fitting","timestamp":"1707578400.0","poster":"srivastavas08","upvote_count":"2"},{"upvote_count":"3","content":"Selected Answer: B\nProblem is performnace and not using all workers properly, https://cloud.google.com/dataflow/docs/pipeline-lifecycle#fusion_optimization","timestamp":"1704753000.0","comment_id":"1117080","poster":"GCP001"},{"poster":"scaenruy","content":"Selected Answer: D\nD. Use Dataflow Prime, and enable Right Fitting to increase the worker resources.","upvote_count":"1","timestamp":"1704296040.0","comment_id":"1112897"}],"question_id":173,"timestamp":"2024-01-03 16:34:00","question_text":"You are running a Dataflow streaming pipeline, with Streaming Engine and Horizontal Autoscaling enabled. You have set the maximum number of workers to 1000. The input of your pipeline is Pub/Sub messages with notifications from Cloud Storage. One of the pipeline transforms reads CSV files and emits an element for every CSV line. The job performance is low, the pipeline is using only 10 workers, and you notice that the autoscaler is not spinning up additional workers. What should you do to improve performance?","topic":"1"},{"id":"BC2edYqSih09p0wJRIKu","isMC":true,"answer":"D","answer_images":[],"choices":{"A":"Deploy Apache Kafka in the same VPC network, use Kafka Connect Oracle Change Data Capture (CDC), and Dataflow to stream the Kafka topic to BigQuery.","C":"Deploy Apache Kafka in the same VPC network, use Kafka Connect Oracle change data capture (CDC), and the Kafka Connect Google BigQuery Sink Connector.","B":"Create a Pub/Sub subscription to write to BigQuery directly. Deploy the Debezium Oracle connector to capture changes in the Oracle database, and sink to the Pub/Sub topic.","D":"Create a Datastream service from Oracle to BigQuery, use a private connectivity configuration to the same VPC network, and a connection profile to BigQuery."},"exam_id":11,"answers_community":["D (100%)"],"unix_timestamp":1704296280,"question_images":[],"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/130206-exam-professional-data-engineer-topic-1-question-255/","answer_ET":"D","discussion":[{"comment_id":"1114150","upvote_count":"11","poster":"raaad","timestamp":"1720133340.0","content":"Selected Answer: D\n- Datastream is a serverless and easy-to-use change data capture (CDC) and replication service. \n- You would create a Datastream service that sources from your Oracle database and targets BigQuery, with private connectivity configuration to the same VPC. \n- This option is designed to minimize the need to manage infrastructure and is a fully managed service."},{"poster":"JyoGCP","upvote_count":"2","content":"Selected Answer: D\nD. Datastream","timestamp":"1724121360.0","comment_id":"1154489"},{"content":"Selected Answer: D\nD. Create a Datastream service from Oracle to BigQuery, use a private connectivity configuration to the same VPC network, and a connection profile to BigQuery.","comment_id":"1112898","upvote_count":"2","poster":"scaenruy","timestamp":"1720013880.0"}],"question_id":174,"timestamp":"2024-01-03 16:38:00","question_text":"You have an Oracle database deployed in a VM as part of a Virtual Private Cloud (VPC) network. You want to replicate and continuously synchronize 50 tables to BigQuery. You want to minimize the need to manage infrastructure. What should you do?","topic":"1"},{"id":"RZOfMZE6ENWaKEfjN25y","answer_images":[],"discussion":[{"poster":"raaad","content":"Selected Answer: C\n- Enable Airflow REST API: In Cloud Composer, enable the \"Airflow web server\" option.\n- Set Up Cloud Storage Notifications: Create a notification for new files, routing to a Cloud Function.\n- Create PSC Endpoint: Establish a PSC endpoint for Cloud Composer.\n- Write Cloud Function: Code the function to use the Airflow REST API (via PSC endpoint) to trigger the DAG.\n\n========\nWhy not Option D\n- Using the web server URL directly wouldn't work without internet access or a direct path to the web server.","comment_id":"1114552","upvote_count":"12","comments":[{"comment_id":"1127089","upvote_count":"4","timestamp":"1705733220.0","poster":"AllenChen123","content":"Why not B, use Cloud Composer API"}],"timestamp":"1704463920.0"},{"content":"Selected Answer: A\nThis is the guidance how to use method in A:\nhttps://cloud.google.com/composer/docs/composer-2/triggering-gcf-pubsub\n\"In this specific example, you create a Cloud Function and deploy two DAGs. The first DAG pulls Pub/Sub messages and triggers the second DAG according to the Pub/Sub message content.\"\n\nFor C & D, this guidance says it can't be done when you have Private or VPS Service Controls set up:\nhttps://cloud.google.com/composer/docs/composer-2/triggering-with-gcf#check_your_environments_networking_configuration\n\"This solution does not work in Private IP and VPC Service Controls configurations because it is not possible to configure connectivity from Cloud Functions to the Airflow web server in these configurations.\"","poster":"STEVE_PEGLEG","timestamp":"1723031880.0","comment_id":"1262084","upvote_count":"6"},{"comment_id":"1351166","upvote_count":"1","content":"Selected Answer: B\nOption B is the only viable solution because:\n\n It uses the Cloud Composer API, which is compatible with Private IP configurations.\n\n It leverages VPC Serverless Access to allow Cloud Functions to securely access the Airflow web server within the subnetwork.\n\n It avoids the limitations of the Airflow REST API in Private IP environments.","timestamp":"1738633920.0","poster":"Augustax"},{"upvote_count":"2","poster":"Pime13","comment_id":"1338288","timestamp":"1736418420.0","content":"Selected Answer: D\nWhy Option D is the Best Choice:\nAirflow REST API: Enabling the Airflow REST API allows you to programmatically trigger DAG runs, which is essential for a reactive setup.\nCloud Storage Notifications: Setting up notifications ensures that your DAG is triggered every time a new file is received in the Cloud Storage bucket.\nVPC Serverless Access: This allows your Cloud Function to securely access the Cloud Composer web server URL without needing external IP addresses, complying with your subnetwork's no Internet access constraint."},{"upvote_count":"2","content":"Selected Answer: A\nThis is A, as steve_pegleg says, there is no way to connect the cloud function to the Airflow instance, without first enabling private access. The pubsub pattern makes sense in this context.","poster":"baimus","timestamp":"1728370860.0","comment_id":"1294609"},{"upvote_count":"4","comment_id":"1213887","content":"Selected Answer: A\nC is not correct because \"this solution does not work in Private IP and VPC Service Controls configurations because it is not possible to configure connectivity from Cloud Functions to the Airflow web server in these configurations\".\nhttps://cloud.google.com/composer/docs/how-to/using/triggering-with-gcf\nThe correct answer is A using Pub/Sub https://cloud.google.com/composer/docs/composer-2/triggering-gcf-pubsub","timestamp":"1716137820.0","poster":"josech"},{"upvote_count":"3","comments":[{"poster":"chrissamharris","comment_id":"1184165","timestamp":"1711549080.0","content":"https://cloud.google.com/vpc/docs/serverless-vpc-access: Serverless VPC Access makes it possible for you to connect directly to your Virtual Private Cloud (VPC) network from serverless environments such as Cloud Run, App Engine, or Cloud Functions","upvote_count":"2"}],"poster":"chrissamharris","content":"Selected Answer: D\nWhy not Option C? C involves creating a Private Service Connect (PSC) endpoint, which, while viable for creating private connections to Google services, adds complexity and might not be required when simpler solutions like VPC Serverless Access (as in Option D) can suffice.","timestamp":"1711548960.0","comment_id":"1184163"},{"content":"Selected Answer: D\nThe answer should be D \nServerless VPC Access makes it possible for you to connect directly to your Virtual Private Cloud (VPC) network from serverless environments such as Cloud Run, App Engine, or Cloud Functions. Configuring Serverless VPC Access allows your serverless environment to send requests to your VPC network by using internal DNS and internal IP addresses (as defined by RFC 1918 and RFC 6598). The responses to these requests also use your internal network.\nYou can use Serverless VPC Access to access Compute Engine VM instances, Memorystore instances, and any other resources with internal DNS or internal IP address.\n(Reference: https://cloud.google.com/vpc/docs/serverless-vpc-access)\nWhen you use Airflow Rest API to tigger the job, the url is based on the private IP address of Cloud Composer Instance, so you need to use Serverless VPC Access for it.","upvote_count":"3","comments":[{"poster":"d11379b","timestamp":"1711296600.0","upvote_count":"2","comment_id":"1181837","content":"Why not C:\nThe reference here (https://cloud.google.com/vpc/docs/private-service-connect#published-services) limits the available use cases:\nPrivate Service Connect supports access to the following types of managed services:\nPublished VPC-hosted services, which include the following:\n Google published services, such as Apigee or the GKE control plane\n Third-party published services provided by Private Service Connect partners\n Intra-organization published services, where the consumer and producer might be two different VPC networks within the same company\nGoogle APIs, such as Cloud Storage or BigQuery\n\nUnfortunately your airflow Rest API is not published as a service in the list, so you can not use it \nThis is also one of the reasons why you should reject A","comments":[{"poster":"d11379b","content":"B is not appropriate while Cloud Composer API can really execute Airflow command，but It’s not via web server Url to run a DAG in this case, and I doubt if it is really possible","comment_id":"1181840","timestamp":"1711296660.0","upvote_count":"2"}]}],"comment_id":"1181835","timestamp":"1711296540.0","poster":"d11379b"},{"timestamp":"1705156200.0","poster":"Matt_108","upvote_count":"1","content":"Selected Answer: C\nOption C, raaad explained well why","comment_id":"1121737"},{"upvote_count":"1","poster":"scaenruy","content":"Selected Answer: C\nC. \n1. Enable the Airflow REST API, and set up Cloud Storage notifications to trigger a Cloud Function instance.\n2. Create a Private Service Connect (PSC) endpoint.\n3. Write a Cloud Function that connects to the Cloud Composer cluster through the PSC endpoint.","comment_id":"1112912","timestamp":"1704297360.0"}],"topic":"1","answer_description":"","timestamp":"2024-01-03 16:56:00","answers_community":["C (40%)","A (34%)","D (23%)","3%"],"answer_ET":"C","unix_timestamp":1704297360,"answer":"C","question_id":175,"choices":{"B":"1. Enable the Cloud Composer API, and set up Cloud Storage notifications to trigger a Cloud Function.\n2. Write a Cloud Function instance to call the DAG by using the Cloud Composer API and the web server URL.\n3. Use VPC Serverless Access to reach the web server URL.","C":"1. Enable the Airflow REST API, and set up Cloud Storage notifications to trigger a Cloud Function instance.\n2. Create a Private Service Connect (PSC) endpoint.\n3. Write a Cloud Function that connects to the Cloud Composer cluster through the PSC endpoint.","A":"1. Enable Private Google Access in the subnetwork, and set up Cloud Storage notifications to a Pub/Sub topic.\n2. Create a push subscription that points to the web server URL.","D":"1. Enable the Airflow REST API, and set up Cloud Storage notifications to trigger a Cloud Function instance.\n2. Write a Cloud Function instance to call the DAG by using the Airflow REST API and the web server URL.\n3. Use VPC Serverless Access to reach the web server URL."},"url":"https://www.examtopics.com/discussions/google/view/130209-exam-professional-data-engineer-topic-1-question-256/","exam_id":11,"question_text":"You are deploying an Apache Airflow directed acyclic graph (DAG) in a Cloud Composer 2 instance. You have incoming files in a Cloud Storage bucket that the DAG processes, one file at a time. The Cloud Composer instance is deployed in a subnetwork with no Internet access. Instead of running the DAG based on a schedule, you want to run the DAG in a reactive way every time a new file is received. What should you do?","isMC":true,"question_images":[]}],"exam":{"isMCOnly":true,"numberOfQuestions":319,"isBeta":false,"id":11,"provider":"Google","lastUpdated":"11 Apr 2025","name":"Professional Data Engineer","isImplemented":true},"currentPage":35},"__N_SSP":true}