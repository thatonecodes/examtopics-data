{"pageProps":{"questions":[{"id":"xZfsmAjL9HUHOot5lQ9i","answer":"C","question_text":"Your team is running microservices in Google Kubernetes Engine (GKE). You want to detect consumption of an error budget to protect customers and define release policies. What should you do?","timestamp":"2023-10-01 08:18:00","question_id":181,"choices":{"B":"Use the metrics from Anthos Service Mesh to measure the health of the microservices.","C":"Create a SLO. Create an Alert Policy on select_slo_burn_rate.","A":"Create SLIs from metrics. Enable Alert Policies if the services do not pass.","D":"Create a SLO and configure uptime checks for your services. Enable Alert Policies if the services do not pass."},"answer_images":[],"unix_timestamp":1696141080,"url":"https://www.examtopics.com/discussions/google/view/122003-exam-professional-cloud-devops-engineer-topic-1-question-85/","answers_community":["C (70%)","D (30%)"],"answer_description":"","topic":"1","answer_ET":"C","exam_id":6,"question_images":[],"isMC":true,"discussion":[{"comments":[{"content":"Why other options are less suitable:\n\nA. Create SLIs from metrics. Enable Alert Policies if the services do not pass: While creating SLIs is a good first step, it doesn't directly address the error budget consumption. Alerting on individual SLIs might not be sufficient to protect against exceeding the overall error budget.\nB. Use the metrics from Anthos Service Mesh to measure the health of the microservices: Anthos Service Mesh provides valuable metrics, but it doesn't inherently handle error budget management. You'll still need to define SLOs and create alerts based on the burn rate.\nD. Create a SLO and configure uptime checks for your services. Enable Alert Policies if the services do not pass: Uptime checks are important for availability, but they don't directly monitor error budget consumption. You need a mechanism to track the burn rate of your error budget, which is best achieved through SLOs and the select_slo_burn_rate metric.","upvote_count":"1","timestamp":"1721278380.0","comment_id":"1250140","poster":"thewalker"}],"upvote_count":"2","content":"Selected Answer: C\nThe best answer is C. Create a SLO. Create an Alert Policy on select_slo_burn_rate. Here's why:\n\nSLOs (Service Level Objectives): SLOs are crucial for defining the acceptable performance levels of your microservices. They help you set clear targets for things like latency, availability, and error rates.\nError Budget: An error budget is a defined amount of \"acceptable\" errors or performance degradation within a given time period. It allows for some flexibility while still ensuring overall service health.\nAlerting on Burn Rate: The select_slo_burn_rate metric in Cloud Monitoring allows you to track how quickly your error budget is being consumed. By creating an alert policy based on this metric, you can be notified when the burn rate exceeds a predefined threshold, indicating a potential risk of exceeding your error budget.","comment_id":"1250139","timestamp":"1721278380.0","poster":"thewalker"},{"content":"Selected Answer: C\nhttps://cloud.google.com/stackdriver/docs/solutions/slo-monitoring/alerting-on-budget-burn-rate#:~:text=The%20burn%2Drate%20metric%20is%20retrieved%20by%20the%20time%2Dseries%20selector%20select_slo_burn_rate.%20A%20burn%2Drate%20alerting%20policy%20notifies%20you%20when%20your%20error%20budget%20is%20consumed%20faster%20than%20a%20threshold%20you%20define%2C%20measured%20over%20the%20alert%27s%20compliance%20period.","upvote_count":"1","comment_id":"1144344","poster":"alpha_canary","timestamp":"1707386940.0"},{"timestamp":"1701708960.0","comment_id":"1087788","content":"Selected Answer: C\nThis approach involves defining specific SLOs for your services, which are quantitative measures of the desired reliability of a service. Once you have these SLOs, you can set up Alert Policies based on the rate at which your error budget is consumed (burn rate).","upvote_count":"1","poster":"filipemotta"},{"upvote_count":"3","content":"Selected Answer: C\nI am voting for C we need to detect consumption of an error budget. This is what SLO burn rate is.","comment_id":"1070452","poster":"Andrei_Z","timestamp":"1699971060.0"},{"poster":"mshafa","timestamp":"1699282920.0","comment_id":"1063963","content":"Selected Answer: D\nBoth option C & D are effective in detecting consumption of error budget, but they have different strengths and weaknesses.\nCreating an SLO and configuring uptime checks is a good way to get a high-level view of the health of your services. It can also help you to identify trends over time. However, it can be difficult to configure uptime checks for complex services, and it may not be possible to detect all types of errors.\nUsing select_slo_burn_rate is a more granular way to detect consumption of error budget. It can be used to monitor individual SLOs and to identify specific types of errors. However, it can be more difficult to set up and to interpret the results.","upvote_count":"1"},{"content":"Selected Answer: D\nhttps://cloud.google.com/service-mesh/docs/observability/alert-policy-slo","timestamp":"1698508020.0","poster":"koo_kai","comment_id":"1056304","upvote_count":"2"},{"content":"using metrics from Anthos Service Mesh, which can be helpful for monitoring, but it lacks the explicit focus on SLOs, uptime checks, and Alert Policies for managing error budgets and protecting customers.\n\nCorrect Answer is D. Create a SLO and configure uptime checks for your services. Enable Alert Policies if the services do not pass.","upvote_count":"3","poster":"ManishKS","comment_id":"1022041","timestamp":"1696141080.0"}]},{"id":"pzc09caVCNYNfqjwhBvR","answer_images":[],"exam_id":6,"url":"https://www.examtopics.com/discussions/google/view/122004-exam-professional-cloud-devops-engineer-topic-1-question-86/","choices":{"D":"Install the Ops Agent on the Compute Engine image by using a startup script","C":"Use the gcloud CLI to create an Agent Policy.","A":"Use the gcloud CLI to install the Ops Agent on each VM listed in the Cloud Asset Inventory,","B":"Select all VMs with an Agent status of Not detected on the Cloud Operations VMs dashboard. Then select Install agents."},"question_images":[],"answers_community":["C (100%)"],"question_text":"Your organization wants to collect system logs that will be used to generate dashboards in Cloud Operations for their Google Cloud project. You need to configure all current and future Compute Engine instances to collect the system logs, and you must ensure that the Ops Agent remains up to date. What should you do?","discussion":[{"poster":"PrayasMohanty","upvote_count":"5","content":"I vote for C as agent must install in current and feature VMs.\nhttps://cloud.google.com/stackdriver/docs/solutions/agents/ops-agent/managing-agent-policies","comment_id":"1039343","timestamp":"1696930320.0"},{"timestamp":"1723416540.0","upvote_count":"1","comments":[{"poster":"6a8c7ad","comment_id":"1265875","timestamp":"1723654620.0","content":"Correction. It is C","upvote_count":"1"}],"comment_id":"1264324","content":"Definitely not C","poster":"6a8c7ad"},{"upvote_count":"3","content":"Selected Answer: C\nOnly C will ensure the installation will be done in the future as well.","comment_id":"1070459","timestamp":"1699971240.0","poster":"Andrei_Z"},{"poster":"ReachTango73","upvote_count":"2","comment_id":"1041921","timestamp":"1697128680.0","content":"Option B"},{"upvote_count":"4","comment_id":"1039168","content":"Selected Answer: C\nAnswer C is the only one that can keep agents up to date, if automatic updates are set.","poster":"florian_cartron","timestamp":"1696914360.0"},{"timestamp":"1696141320.0","content":"Option A suggests using the gcloud CLI to install the Ops Agent on each VM manually, which can be time-consuming and error-prone, especially when dealing with a large number of instances.\n\noption B is the most efficient and appropriate choice for deploying and ensuring the continued use of the Ops Agent on both existing and future Compute Engine instances in your Google Cloud project.","upvote_count":"2","comments":[{"poster":"lelele2023","content":"no, B cannot cover future instances","timestamp":"1699269060.0","comment_id":"1063761","upvote_count":"1"}],"poster":"ManishKS","comment_id":"1022045"}],"timestamp":"2023-10-01 08:22:00","answer_ET":"C","topic":"1","answer":"C","question_id":182,"unix_timestamp":1696141320,"answer_description":"","isMC":true},{"id":"qLq3hW1qlqHY4A3b6GMq","topic":"1","question_images":[],"answers_community":["C (56%)","D (44%)"],"question_text":"Your company has a Google Cloud resource hierarchy with folders for production, test, and development. Your cyber security team needs to review your company's Google Cloud security posture to accelerate security issue identification and resolution. You need to centralize the logs generated by Google Cloud services from all projects only inside your production folder to allow for alerting and near-real time analysis. What should you do?","answer_description":"","timestamp":"2023-10-12 18:43:00","discussion":[{"upvote_count":"11","timestamp":"1697128980.0","poster":"ReachTango73","comment_id":"1041926","content":"D is correct as when you use buckets you can do log analysis","comments":[{"content":"Does it address near-real-time analysis?","upvote_count":"1","comments":[{"timestamp":"1699678800.0","content":"There is a delay with the Cloud Storage Bucket, but no delay appears to occur with the Log Bucket.\nhttps://cloud.google.com/logging/docs/export/configure_export_v2\n> New log sinks to Cloud Storage buckets might take several hours to start routing logs. Sinks to Cloud Storage are processed hourly while other destination types are processed in real time.","upvote_count":"1","poster":"YushiSato","comment_id":"1067667"}],"timestamp":"1699282440.0","comment_id":"1063953","poster":"mshafa"}]},{"poster":"CAnalytics123","upvote_count":"1","comment_id":"1402347","timestamp":"1742744760.0","content":"Selected Answer: D\nD is correct"},{"upvote_count":"1","timestamp":"1740057720.0","comment_id":"1359248","content":"Selected Answer: C\nBetween options C and D, option C (Create an aggregated log sink associated with the production folder that uses a Pub/Sub topic as the destination) is the best choice for the requirement of \"alerting and near-real-time analysis.\" This is because Pub/Sub can facilitate streaming of logs to other services like Cloud Functions, Cloud Run, or third-party services for immediate processing and alerting.","poster":"manishk39"},{"upvote_count":"1","timestamp":"1739747820.0","content":"Selected Answer: D\nI will go for D.\nAggregated log sinks allow you to collect logs from multiple projects (in this case, all projects within the production folder) and send them to a single destination. This is exactly what you need for centralized log analysis. Using a Cloud Logging bucket as the destination is efficient, cost-effective, and integrates seamlessly with other Google Cloud services.\n\nWhile using Pub/Sub as a destination is possible, it's generally more complex than using a Cloud Logging bucket for this specific use case. Pub/Sub is better suited for streaming logs to multiple destinations or for real-time processing. For simple centralized log storage and analysis, a Cloud Logging bucket is more straightforward and efficient. It also adds another component to manage","poster":"JonathanSJ","comment_id":"1357460"},{"comment_id":"1335950","timestamp":"1735894680.0","content":"Selected Answer: C\nD doesn't have the option to analyze and wont let you able to do the real time analysis! But pub/Sub does","poster":"Min3em","upvote_count":"1"},{"poster":"mohan999","timestamp":"1731210480.0","content":"Many seem to getting confused with logging bucket and cloud storage buckets. Both are different things. Cloud storage is never an option for logs analysis but with custom logging bucket, you can analyze the logs and infact all logs that are seen in logs explorer are stored in default & required logging bucket.\n\nComing to the Pub/Sub, it is messaging service & can be good option for near real time analysis when you are sending these logs to any other 3rd party tool like SIEM or Splunk analysis tool. You can't just analyze logs just from retrieving the logs from Pub/Sub topic. Unless there is another destination, Pub/Sub will be useless.\n\nIn the question, it is not mentioned whether any 3rd part tool is being used, so D is correct option.","comment_id":"1309300","upvote_count":"1"},{"upvote_count":"3","comment_id":"1283239","poster":"ccpmad","content":"Selected Answer: D\nit is D, why should we use pub/sub for logs? are we crazy?","timestamp":"1726240920.0"},{"comment_id":"1264227","timestamp":"1723394100.0","poster":"surfer111","content":"Customers would be very annoyed if they had to use an additional technology for something as simple as logs and analysis. Configuring pub/sub is foreign to a lot of orgs when they are used to tech like kafka. All you need is a sink to a Cloud Logging bucket. D","upvote_count":"1"},{"comment_id":"1158668","upvote_count":"1","content":"Selected Answer: D\nhttps://cloud.google.com/logging/docs/export/using_exported_logs#:~:text=Logs%20that%20you%20route%20to%20Cloud%20Logging%20buckets%20are%20available%20immediately.","poster":"alpha_canary","timestamp":"1708861020.0"},{"poster":"fixeres","content":"Selected Answer: D\nSo, after some research. The correct answer is D.\n\nThere seems to be a lot of discussion, wether near-real time analysis is given or not. In Fact both C and D support near real-time analysis.\n\nhttps://cloud.google.com/logging/docs/export/using_exported_logs\n\"Logs that you route to Cloud Logging buckets are available immediately.\"\n\nhttps://cloud.google.com/logging/docs/export/pubsub\n\" Routed logs are generally available within seconds of their arrival to Logging, with 99% of logs available in less than 60 seconds.\"\n\nThe main difference lays somewhere else. The question states that the logs are retrieved from different projects. And in fact for this use case Cloud Logging is the preffered option:\nhttps://cloud.google.com/logging/docs/export/configure_export_v2\nCloud Logging: \" A log bucket can store logs that are received by multiple Google Cloud projects.\"","upvote_count":"1","timestamp":"1708177320.0","comment_id":"1152569"},{"poster":"alpha_canary","comment_id":"1144349","upvote_count":"4","timestamp":"1707387720.0","content":"Selected Answer: C\nIt clearly mentions here that \n\"Sinks to Cloud Storage are processed hourly while other destination types are processed in real time.\"\nhttps://cloud.google.com/logging/docs/export/configure_export_v2#:~:text=New%20log%20sinks%20to%20Cloud%20Storage%20buckets%20might%20take%20several%20hours%20to%20start%20routing%20logs.%20Sinks%20to%20Cloud%20Storage%20are%20processed%20hourly%20while%20other%20destination%20types%20are%20processed%20in%20real%20time.\n\nD is eliminated","comments":[{"content":"This is correct, but you are mixing two things up. A Cloud Logging Bucket is not the same as a Cloud Storage Bucket.\nhttps://cloud.google.com/logging/docs/export/configure_export_v2\n\nA Cloud Logging Bucket does process in real-time and is the preferred Option here.","upvote_count":"1","comment_id":"1152574","poster":"fixeres","timestamp":"1708177560.0"}]},{"poster":"Feliphus","content":"Selected Answer: D\nI would choose C ans to export the logs to a SIEM product, but if we can use a Cloud Logging bucket as a central repository I prefer and the statement doesn't say anything about exportation.","comment_id":"1105474","timestamp":"1703529060.0","upvote_count":"2"},{"upvote_count":"1","content":"Answer is C. Cloud Logging includes the capability for log archival in Google Cloud Storage and the ability to send logs to Google BigQuery. In addition, Cloud Logging also allows you to forward these logs to any custom endpoint including third party log management services for advanced and tailored log analytics via the near real-time streaming Google Cloud Pub/Sub API.","comment_id":"1096136","poster":"Nkay17","timestamp":"1702538340.0"},{"poster":"filipemotta","timestamp":"1701712680.0","upvote_count":"1","comment_id":"1087820","content":"Selected Answer: D\nBy creating an aggregated log sink at the folder level for production, you can collect logs from all projects within that folder. Using a Cloud Logging bucket as the destination simplifies management and enables straightforward integration with Cloud Monitoring and alerting tools for security analysis."},{"content":"Selected Answer: C\nI would vote C because from a security perspective it would be better to stream the logs to a SIEM or SOAR for near-real time analysis and alerting. A SIEM is not really mentioned here but streaming them to a bucket and analysing them from stackdriver would be nuts","comment_id":"1070468","upvote_count":"4","poster":"Andrei_Z","timestamp":"1699971660.0"},{"timestamp":"1699678920.0","poster":"YushiSato","content":"Selected Answer: D\nI think D is correct","upvote_count":"1","comment_id":"1067669"},{"content":"Selected Answer: C\nC seems an to be correct.","upvote_count":"4","timestamp":"1699276320.0","poster":"mshafa","comment_id":"1063865"},{"timestamp":"1697904300.0","content":"Answer C seems to be correct. \nhttps://cloudplatform.googleblog.com/2015/06/Real-Time-Log-Streaming-and-Analysis-with-Google-Cloud-Platform-Logentries.html","upvote_count":"4","poster":"activist","comment_id":"1049637"},{"timestamp":"1697601240.0","upvote_count":"4","comment_id":"1046525","poster":"lelele2023","content":"C is the answer: sink is the native feature of GCP to route logs and this excludes A and B. Also being asked to achieve near-real time analysis, and the pub-sub works better then a bucket."}],"url":"https://www.examtopics.com/discussions/google/view/123441-exam-professional-cloud-devops-engineer-topic-1-question-87/","unix_timestamp":1697128980,"exam_id":6,"isMC":true,"choices":{"A":"Enable the Workflows API and route all the logs to Cloud Logging.","C":"Create an aggregated log sink associated with the production folder that uses a Pub/Sub topic as the destination.","D":"Create an aggregated log sink associated with the production folder that uses a Cloud Logging bucket as the destination.","B":"Create a central Cloud Monitoring workspace and attach all related projects."},"answer":"C","answer_images":[],"question_id":183,"answer_ET":"C"},{"id":"Kgxg67BJqilDzgw74Fez","answers_community":["D (100%)"],"discussion":[{"upvote_count":"7","content":"Selected Answer: D\nI think D is correct for saving cost and single region","timestamp":"1729647360.0","poster":"SunyaCat","comment_id":"1051336"},{"content":"Selected Answer: D\nD is the cheapest and would work in this scenario","poster":"Andrei_Z","timestamp":"1731594180.0","comment_id":"1070471","upvote_count":"2"},{"comment_id":"1063862","poster":"mshafa","upvote_count":"2","timestamp":"1730898600.0","content":"Selected Answer: D\nD is correct."},{"comment_id":"1022047","upvote_count":"4","content":"D is correct Answer","poster":"ManishKS","timestamp":"1727764140.0"}],"exam_id":6,"isMC":true,"choices":{"C":"Standard Tier with a global load balancer","B":"Premium Tier with a regional load balancer","A":"Premium Tier with a global load balancer","D":"Standard Tier with a regional load balancer"},"answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/122005-exam-professional-cloud-devops-engineer-topic-1-question-88/","answer_ET":"D","question_images":[],"answer_description":"","topic":"1","answer":"D","timestamp":"2023-10-01 08:29:00","unix_timestamp":1696141740,"question_text":"You are configuring the frontend tier of an application deployed in Google Cloud. The frontend tier is hosted in nginx and deployed using a managed instance group with an Envoy-based external HTTP(S) load balancer in front. The application is deployed entirely within the europe-west2 region, and only serves users based in the United Kingdom. You need to choose the most cost-effective network tier and load balancing configuration. What should you use?","question_id":184},{"id":"LuIwKZYXsBCEjfRGwcdD","url":"https://www.examtopics.com/discussions/google/view/122006-exam-professional-cloud-devops-engineer-topic-1-question-89/","topic":"1","answer_ET":"D","question_images":[],"answer_description":"","unix_timestamp":1696141800,"isMC":true,"choices":{"C":"Perform a canary deployment, and test your new application periodically after the new version is deployed.","B":"Perform A/B testing, and test your application periodically after the deployment is complete.","D":"Perform a blue/green deployment, and test your new application after the deployment is complete.","A":"Perform a rolling deployment, and test your new application after the deployment is complete."},"answer":"D","discussion":[{"timestamp":"1723106460.0","poster":"alpha_canary","upvote_count":"1","content":"Selected Answer: D\nhttps://cloud.google.com/architecture/application-deployment-and-testing-strategies#key_benefits_3","comment_id":"1144358"},{"comment_id":"1098281","poster":"PhuocT","content":"Selected Answer: D\ninstantly roll back, so I think it's D","timestamp":"1718545140.0","upvote_count":"2"},{"timestamp":"1717518780.0","comment_id":"1087853","content":"Selected Answer: D\nhis method involves deploying the new version of your application alongside the old version (two separate but identical environments: blue for the old version and green for the new one). You then switch traffic from blue to green. If any issues arise with the green environment (the new version), you can instantly route traffic back to the blue environment (the old version). This approach offers the fastest rollback mechanism as it merely involves a change in the traffic routing.","poster":"filipemotta","upvote_count":"1"},{"upvote_count":"1","comment_id":"1063861","timestamp":"1714993680.0","poster":"mshafa","content":"Selected Answer: D\nD is correct."},{"upvote_count":"2","content":"Selected Answer: D\nD is correct","comment_id":"1045419","poster":"ABZ10","timestamp":"1713316260.0"},{"poster":"activist","comment_id":"1041245","content":"Agree. D is the correct answer.","upvote_count":"1","timestamp":"1712880900.0"},{"poster":"tuanuv1","timestamp":"1712642040.0","content":"D is correct\nhttps://cloud.google.com/architecture/application-deployment-and-testing-strategies","upvote_count":"1","comment_id":"1028532"},{"timestamp":"1711953000.0","comment_id":"1022049","upvote_count":"2","poster":"ManishKS","content":"D is correct Answer"}],"answers_community":["D (100%)"],"question_id":185,"question_text":"You recently deployed your application in Google Kubernetes Engine (GKE) and now need to release a new version of the application. You need the ability to instantly roll back to the previous version of the application in case there are issues with the new version. Which deployment model should you use?","exam_id":6,"answer_images":[],"timestamp":"2023-10-01 08:30:00"}],"exam":{"provider":"Google","isImplemented":true,"name":"Professional Cloud DevOps Engineer","id":6,"isMCOnly":true,"isBeta":false,"lastUpdated":"11 Apr 2025","numberOfQuestions":196},"currentPage":37},"__N_SSP":true}