{"pageProps":{"questions":[{"id":"dyIEhfca7HoIYeKOC4Xm","timestamp":"2024-10-04 22:15:00","answers_community":["D (70%)","A (30%)"],"question_id":231,"exam_id":9,"question_images":[],"isMC":true,"answer_description":"","unix_timestamp":1728072900,"answer":"D","topic":"1","choices":{"B":"Assess the keys in the Cloud Key Management Service by implementing code in Cloud Run. If a key is not rotated after 90 days, raise a finding in Security Command Center.","D":"Identify keys that have not been rotated by using Security Health Analytics. If a key is not rotated after 90 days, a finding in Security Command Center is raised.","C":"Define a metric that checks for timely key updates by using Cloud Logging. If a key is not rotated after 90 days, send an alert message through your incident notification channel.","A":"Analyze the crypto key versions of the keys by using data from Cloud Asset Inventory. If an active key is older than 90 days, send an alert message through your incident notification channel."},"question_text":"You must ensure that the keys used for at-rest encryption of your data are compliant with your organization's security controls. One security control mandates that keys get rotated every 90 days. You must implement an effective detection strategy to validate if keys are rotated as required. What should you do?","url":"https://www.examtopics.com/discussions/google/view/148661-exam-professional-cloud-security-engineer-topic-1-question/","answer_ET":"D","answer_images":[],"discussion":[{"poster":"Pime13","upvote_count":"1","timestamp":"1733736780.0","content":"Selected Answer: D\nhttps://cloud.google.com/security-command-center/docs/how-to-remediate-security-health-analytics-findings#kms_key_not_rotated","comment_id":"1323948"},{"comment_id":"1319503","content":"Selected Answer: A\nWhy A is Correct:\nCloud Asset Inventory:\n\nCloud Asset Inventory offers a detailed view of cryptographic keys, including the age of each key version.\nBy periodically analyzing this data, you can determine if a key version has been in use for more than 90 days.\nProactive Monitoring:\n\nThis approach allows you to set up automated checks and send alerts to incident notification channels (e.g., email, Slack, PagerDuty) when keys exceed the allowed age.","poster":"BPzen","timestamp":"1732842660.0","upvote_count":"1"},{"timestamp":"1732286760.0","upvote_count":"2","poster":"MoAk","content":"Selected Answer: D\nD - https://cloud.google.com/security-command-center/docs/how-to-remediate-security-health-analytics-findings#kms_key_not_rotated","comment_id":"1316355"},{"poster":"jmaquino","content":"Selected Answer: A\nhttps://cloud.google.com/secret-manager/docs/analyze-resources?hl=es-419","upvote_count":"1","timestamp":"1730298780.0","comment_id":"1305031"},{"poster":"koo_kai","content":"Selected Answer: D\nIt's D\nhttps://cloud.google.com/security-command-center/docs/how-to-remediate-security-health-analytics-findings#kms_key_not_rotated","timestamp":"1728746340.0","upvote_count":"4","comment_id":"1296572"},{"comment_id":"1295803","timestamp":"1728608580.0","upvote_count":"1","content":"Selected Answer: A\nVOTE A","poster":"siheom"},{"content":"D - Security Health Analytics: Security Health Analytics is a specialized tool designed to assess the security posture of your Google Cloud environment. It can effectively identify keys that have not been rotated within the specified timeframe.\nFinding in Security Command Center: Raising a finding in Security Command Center ensures that the non-compliance issue is clearly documented and can be addressed promptly.\nEfficiency: Security Health Analytics provides a streamlined and efficient way to monitor key rotation compliance without requiring custom code or manual analysis.","poster":"abdelrahman89","timestamp":"1728072900.0","comment_id":"1293266","upvote_count":"4"}]},{"id":"Iiz23ug1ov5E5tPoUzOh","answer_description":"","choices":{"A":"De-identify sensitive data before model training by using Cloud Data Loss Prevention (DLP)APIs. and implement strict Identity and Access Management (IAM) policies to control access to BigQuery.","C":"Implement at-rest encryption by using customer-managed encryption keys (CMEK) for the pipeline. Implement strict Identity and Access Management (IAM) policies to control access to BigQuery.","D":"Deploy the model on Confidential VMs for enhanced protection of data and code while in use. Implement strict Identity and Access Management (IAM) policies to control access to BigQuery.","B":"Implement Identity-Aware Proxy to enforce context-aware access to BigQuery and models based on user identity and device."},"exam_id":9,"topic":"1","answers_community":["A (100%)"],"unix_timestamp":1728073080,"answer":"A","question_text":"Your organization is developing a sophisticated machine learning (ML) model to predict customer behavior for targeted marketing campaigns. The BigQuery dataset used for training includes sensitive personal information. You must design the security controls around the AI/ML pipeline. Data privacy must be maintained throughout the modelâ€™s lifecycle and you must ensure that personal data is not used in the training process. Additionally, you must restrict access to the dataset to an authorized subset of people only. What should you do?","answer_images":[],"timestamp":"2024-10-04 22:18:00","discussion":[{"timestamp":"1732724160.0","poster":"532b5da","comment_id":"1318783","upvote_count":"1","content":"Selected Answer: A\nAns is A\nWe want data privacy through out lifecycle.\nC is at rest\nD is in use\nB says nothing about data privacy"},{"content":"Selected Answer: A\nIt's A\nWell explained below.","poster":"json4u","timestamp":"1729047720.0","comment_id":"1298517","upvote_count":"1"},{"upvote_count":"1","content":"A - Data De-identification: De-identifying sensitive data using Cloud DLP APIs ensures that the data used for model training does not contain personally identifiable information (PII). This protects data privacy and reduces the risk of unauthorized access or misuse.\nIAM Policies: Implementing strict IAM policies controls access to BigQuery, ensuring that only authorized personnel can access and use the dataset. This further protects data privacy and reduces the risk of unauthorized access.\nComprehensive Approach: This approach combines data de-identification and IAM controls to provide a robust and effective security solution for the AI/ML pipeline.","comment_id":"1293268","poster":"abdelrahman89","timestamp":"1728073080.0"}],"isMC":true,"question_images":[],"answer_ET":"A","question_id":232,"url":"https://www.examtopics.com/discussions/google/view/148662-exam-professional-cloud-security-engineer-topic-1-question/"},{"id":"sM1gy2OyrnGLPoYEVZ2Q","question_images":[],"question_text":"Your organization wants to publish yearly reports of your website usage analytics. You must ensure that no data with personally identifiable information (PII) is published by using the Cloud Data Loss Prevention (Cloud DLP) API. Data integrity must be preserved. What should you do?","isMC":true,"url":"https://www.examtopics.com/discussions/google/view/148663-exam-professional-cloud-security-engineer-topic-1-question/","exam_id":9,"unix_timestamp":1728073140,"answer":"C","choices":{"C":"Discover and transform PII data in your reports by using the Cloud DLP API.","B":"Discover and quarantine your PII data in your storage by using the Cloud DLP API.","A":"Detect all PII in storage by using the Cloud DLP API. Create a cloud function to delete the PII.","D":"Encrypt the PII from the report by using the Cloud DLP API."},"topic":"1","answers_community":["C (100%)"],"answer_ET":"C","question_id":233,"timestamp":"2024-10-04 22:19:00","answer_description":"","discussion":[{"poster":"json4u","timestamp":"1729047960.0","upvote_count":"2","content":"Selected Answer: C\nIt's C.\nWell explained below.","comment_id":"1298518"},{"content":"C - Data Discovery: Cloud DLP API can effectively discover PII within your reports, identifying sensitive information that needs to be protected.\nData Transformation: Once PII is detected, Cloud DLP can transform it into a format that removes personally identifiable elements, such as anonymization or generalization. This ensures that the data remains usable for analytics purposes while protecting privacy.\nData Integrity: By transforming PII rather than deleting it, you preserve the overall structure and context of the data, maintaining its integrity for analysis.","poster":"abdelrahman89","comment_id":"1293269","timestamp":"1728073140.0","upvote_count":"4"}],"answer_images":[]},{"id":"4YslC2rtuEjq33bnUZ5m","question_text":"Your development team is launching a new application. The new application has a microservices architecture on Compute Engine instances and serverless components, including Cloud Functions. This application will process financial transactions that require temporary, highly sensitive data in memory. You need to secure data in use during computations with a focus on minimizing the risk of unauthorized access to memory for this financial application. What should you do?","answer_images":[],"answer":"A","timestamp":"2024-10-04 22:20:00","answer_ET":"A","url":"https://www.examtopics.com/discussions/google/view/148664-exam-professional-cloud-security-engineer-topic-1-question/","isMC":true,"exam_id":9,"unix_timestamp":1728073200,"question_images":[],"answer_description":"","discussion":[{"timestamp":"1729048380.0","content":"Selected Answer: A\nIt's A.\nWell explained in abdelrahman89's comment.","comment_id":"1298521","upvote_count":"1","poster":"json4u"},{"timestamp":"1728073200.0","comments":[{"comment_id":"1319801","poster":"nah99","upvote_count":"1","timestamp":"1732897440.0","content":"I would think A, but how do Cloud Functions leverage hardware-based memory isolation?\n\nis this you or chatgpt speaking"}],"comment_id":"1293270","poster":"abdelrahman89","upvote_count":"2","content":"A - Confidential VMs: Using Confidential VMs provides a strong security boundary around the memory of the VM instances, protecting sensitive data from unauthorized access, even if the VM is compromised.\nHardware-Based Memory Isolation: Leveraging hardware-based memory isolation ensures that the data within the VM's memory is protected by hardware-enforced mechanisms, making it significantly more difficult for attackers to access.\nComprehensive Protection: This approach provides a comprehensive solution for securing data in use, as it combines both software-based (Confidential VMs) and hardware-based (memory isolation) protections."}],"choices":{"B":"Use data masking and tokenization techniques on sensitive financial data fields throughout the application and the application's data processing workflows.","C":"Use the Cloud Data Loss Prevention (Cloud DLP) API to scan and mask sensitive data before feeding the data into any compute environment.","D":"Store all sensitive data during processing in Cloud Storage by using customer-managed encryption keys (CMEK), and set strict bucket-level permissions.","A":"Enable Confidential VM instances for Compute Engine, and ensure that relevant Cloud Functions can leverage hardware-based memory isolation."},"question_id":234,"topic":"1","answers_community":["A (100%)"]},{"id":"sR8ibkvCdXejSyuNwOGl","discussion":[{"timestamp":"1601779740.0","upvote_count":"24","poster":"xhova","comment_id":"70906","content":"Ans is B. A cost efficient disaster recovery solution is needed not a data warehouse."},{"timestamp":"1725779700.0","poster":"madcloud32","content":"Selected Answer: B\nB is correct. It is about data backup, DR, not the database backup to GCP. BQ is not cost efficient compare to GCS","comment_id":"1168685","upvote_count":"1"},{"upvote_count":"1","comment_id":"1111753","comments":[{"content":"For later analysis means not now, so Bigquery is not required at this moment. Cloud storage content can be ingested in BigQuery 'later'. So should be B instead of A.","upvote_count":"1","timestamp":"1725534780.0","poster":"Nachtwaker","comment_id":"1166430"}],"content":"the two keywords here are 'later' and 'cost-efficient'. The company doesnt even know when the analysis will occur but they want to store the data. Storing it in BigQuery will not be cost-efficient for later analysis. Cloud Storage Archive is the best deal here.","poster":"tunstila","timestamp":"1719906480.0"},{"upvote_count":"1","timestamp":"1718440320.0","content":"Selected Answer: A\nImho A: \n\"The first step the organization wants to take is to migrate its current data backup and disaster recovery solutions to GCP for later analysis\"\nboth solutions are scalable and cost efficient, but cloud storage is not designed for queirng, therefore data analysis would be easier in BigQuery.","poster":"W00kie","comment_id":"1097205"},{"timestamp":"1705877820.0","poster":"[Removed]","upvote_count":"2","comment_id":"958936","content":"Selected Answer: B\nThe keyword in the question here is \"cost-effective\".\nOut of the 3 Disaster Recovery patterns (Cold, Warm, Hot HA), Cold is the most cost-effective which utilizes cloud storage.\n\nReferences:\nhttps://cloud.google.com/architecture/dr-scenarios-for-applications#cold-pattern-recovery-to-gcp\n\nhttps://cloud.google.com/architecture/dr-scenarios-planning-guide#use-cloud-storage-as-part-of-your-daily-backup-routine"},{"comment_id":"957327","poster":"raj117","content":"Right Answer is B","timestamp":"1705748160.0","upvote_count":"2"},{"timestamp":"1705748040.0","content":"Correct Answer: B","poster":"SMB2022","upvote_count":"2","comment_id":"957323"},{"content":"Selected Answer: B\nB confirmed :-) https://cloud.google.com/solutions/dr-scenarios-planning-guide#use-cloud-storage-as-part-of-your-daily-backup-routine","comment_id":"687410","comments":[{"content":"It is B","timestamp":"1683320100.0","poster":"AzureDP900","upvote_count":"2","comment_id":"712053"}],"poster":"AwesomeGCP","upvote_count":"3","timestamp":"1680751500.0"},{"timestamp":"1677140640.0","comment_id":"650624","poster":"giovy_82","content":"I would go for B, but a doubt remains: it is talking about Disaster Recovery solution, which could not only be related to data but also to VM and applications running inside VMs. any way B is more cost-efficient than A, considering also that data backup need to be moved to GCP.","upvote_count":"1"},{"content":"B of course","timestamp":"1670734380.0","poster":"absipat","upvote_count":"2","comment_id":"614795"},{"comment_id":"311682","poster":"DebasishLowes","content":"Ans : B. Cloud storage is cost efficient one.","timestamp":"1631724900.0","upvote_count":"4"},{"comment_id":"208622","content":"Ans - B","poster":"[Removed]","upvote_count":"2","timestamp":"1619698080.0"},{"comment_id":"189557","timestamp":"1617009120.0","poster":"CHECK666","upvote_count":"2","content":"B is the answer."},{"poster":"paxjoshi","content":"B is the correct answer. They need the data for later analysis and they are looking for cost-effective service.","comment_id":"163364","timestamp":"1613978820.0","upvote_count":"2"},{"poster":"aiwaai","timestamp":"1613715360.0","comment_id":"161232","upvote_count":"1","content":"Correct Answer: A","comments":[{"poster":"aiwaai","timestamp":"1614148800.0","upvote_count":"1","comment_id":"164873","content":"I make corrections, B is Correct Answer."}]},{"poster":"ArizonaClassics","content":"Answer B works for me as the type of workload to be stored is not stated or defined","timestamp":"1612438980.0","upvote_count":"1","comment_id":"150326"},{"comment_id":"141176","content":"B confirmed: https://cloud.google.com/solutions/dr-scenarios-planning-guide#use-cloud-storage-as-part-of-your-daily-backup-routine","upvote_count":"3","poster":"SilentSec","timestamp":"1611330180.0"}],"choices":{"C":"Compute Engine Virtual Machines using Persistent Disk","B":"Cloud Storage using a scheduled task and gsutil","A":"BigQuery using a data pipeline job with continuous updates","D":"Cloud Datastore using regularly scheduled batch upload jobs"},"url":"https://www.examtopics.com/discussions/google/view/17841-exam-professional-cloud-security-engineer-topic-1-question/","topic":"1","exam_id":9,"unix_timestamp":1585968540,"answer_images":[],"question_images":[],"answer":"B","answers_community":["B (86%)","14%"],"question_text":"An organization is starting to move its infrastructure from its on-premises environment to Google Cloud Platform (GCP). The first step the organization wants to take is to migrate its current data backup and disaster recovery solutions to GCP for later analysis. The organization's production environment will remain on- premises for an indefinite time. The organization wants a scalable and cost-efficient solution.\nWhich GCP solution should the organization use?","answer_ET":"B","question_id":235,"timestamp":"2020-04-04 04:49:00","isMC":true,"answer_description":""}],"exam":{"isBeta":false,"provider":"Google","name":"Professional Cloud Security Engineer","numberOfQuestions":321,"isMCOnly":false,"id":9,"lastUpdated":"11 Apr 2025","isImplemented":true},"currentPage":47},"__N_SSP":true}