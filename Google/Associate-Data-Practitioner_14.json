{"pageProps":{"questions":[{"id":"93XC13Ac7l7dBFmxqn6F","question_images":[],"choices":{"A":"Use authorized views to share query results with the payroll specialist.","C":"Create a table with the aggregated performance data. Use table-level permissions to grant access to the payroll specialist.","B":"Create row-level and column-level permissions and policies on the table that contains performance data in the dataset. Provide the payroll specialist with the appropriate permission set.","D":"Create a SQL query with the aggregated performance data. Export the results to an Avro file in a Cloud Storage bucket. Share the bucket with the payroll specialist."},"question_id":66,"answer_images":[],"exam_id":2,"question_text":"Your organization has a BigQuery dataset that contains sensitive employee information such as salaries and performance reviews. The payroll specialist in the HR department needs to have continuous access to aggregated performance data, but they do not need continuous access to other sensitive data. You need to grant the payroll specialist access to the performance data without granting them access to the entire dataset using the simplest and most secure approach. What should you do?","discussion":[{"content":"Selected Answer: A\nThe best option is A. Use authorized views. Option A is best because authorized views are the simplest and most secure way to share query results (aggregated data) without granting access to underlying sensitive data. The payroll specialist only sees the view, not the raw data. Option B (Row/column permissions) is incorrect because it's more complex to set up and manage than authorized views, and might still grant access to the sensitive table, just with restrictions. Option C (New aggregated table) is incorrect because creating a new table adds unnecessary data duplication and management overhead compared to views. Option D (Export to Cloud Storage) is incorrect because it's the most complex and least secure - exporting data creates copies and sharing a bucket is less controlled than BigQuery access controls. Therefore, Option A, authorized views, is the simplest and most secure approach for this scenario.","comment_id":"1365719","timestamp":"1741234860.0","poster":"n2183712847","upvote_count":"1"},{"poster":"SaquibHerman","content":"Selected Answer: A\nCreating a separate table duplicates data and introduces potential data consistency issues. Authorized views provide a more dynamic and maintainable solution.","upvote_count":"2","timestamp":"1740076560.0","comment_id":"1359414"},{"poster":"a_vi","timestamp":"1738037820.0","comment_id":"1347722","upvote_count":"1","content":"Selected Answer: C\nThe best approach is C. Create a table with the aggregated performance data. Use table-level permissions to grant access to the payroll specialist."}],"topic":"1","url":"https://www.examtopics.com/discussions/google/view/155526-exam-associate-data-practitioner-topic-1-question-69/","timestamp":"2025-01-28 05:17:00","answer_description":"","unix_timestamp":1738037820,"isMC":true,"answers_community":["A (75%)","C (25%)"],"answer":"A","answer_ET":"A"},{"id":"Z2xWafXQnzpj9qy3cDAV","answers_community":["D (100%)"],"topic":"1","answer_ET":"D","url":"https://www.examtopics.com/discussions/google/view/157644-exam-associate-data-practitioner-topic-1-question-7/","answer":"D","isMC":true,"exam_id":2,"timestamp":"2025-03-06 05:58:00","answer_images":[],"choices":{"C":"Create a Cloud Run function to periodically check object metadata, and move objects to the appropriate storage class based on age and access patterns. Use object holds to enforce immutability for specific objects.","A":"Configure lifecycle management rules to transition objects to appropriate storage classes based on access patterns. Set up Object Versioning for all objects to meet immutability requirements.","B":"Move objects to different storage classes based on their age and access patterns. Use Cloud Key Management Service (Cloud KMS) to encrypt specific objects with customer-managed encryption keys (CMEK) to meet immutability requirements.","D":"Use object holds to enforce immutability for specific objects, and configure lifecycle management rules to transition objects to appropriate storage classes based on age and access patterns."},"question_images":[],"question_id":67,"answer_description":"","discussion":[{"upvote_count":"1","timestamp":"1741237080.0","content":"Selected Answer: D\nThe best option is D. Object holds and lifecycle rules. Option D is best because Object holds enforce immutability for compliance, and lifecycle rules ensure cost efficiency by managing storage classes. Option A (Object Versioning) is incorrect because Versioning is for recovery, not strict immutability. Option B (CMEK) is incorrect because CMEK is for encryption, not immutability. Option C (Cloud Run) is incorrect because Cloud Run for storage class transition is less efficient than lifecycle rules. Therefore, Option D is the most effective and efficient solution for both immutability and cost.","poster":"n2183712847","comment_id":"1365749"}],"unix_timestamp":1741237080,"question_text":"You manage a large amount of data in Cloud Storage, including raw data, processed data, and backups. Your organization is subject to strict compliance regulations that mandate data immutability for specific data types. You want to use an efficient process to reduce storage costs while ensuring that your storage strategy meets retention requirements. What should you do?"},{"id":"hrTPGX3Y9ftXmBLnU1SG","answers_community":["C (100%)"],"topic":"1","question_images":[],"url":"https://www.examtopics.com/discussions/google/view/157635-exam-associate-data-practitioner-topic-1-question-70/","isMC":true,"answer_ET":"C","exam_id":2,"unix_timestamp":1741235520,"answer_images":[],"choices":{"C":"Configure and create a high-availability Cloud SQL instance with the primary instance in zone A and a secondary instance in any zone other than zone A.","B":"Create a read replica in another region. Promote the replica to primary if a failure occurs.","A":"Continuously back up the Cloud SGL instance to Cloud Storage. Create a Compute Engine instance with PostgreSCL in a different region. Restore the backup in the Compute Engine instance if a failure occurs.","D":"Create a read replica in the same region but in a different zone."},"timestamp":"2025-03-06 05:32:00","question_id":68,"answer":"C","question_text":"You work for a global financial services company that trades stocks 24/7. You have a Cloud SGL for PostgreSQL user database. You need to identify a solution that ensures that the database is continuously operational, minimizes downtime, and will not lose any data in the event of a zonal outage. What should you do?","discussion":[{"poster":"n2183712847","comment_id":"1365724","upvote_count":"1","timestamp":"1741235520.0","content":"Selected Answer: C\nThe best option is C. HA Cloud SQL. Option C is best: HA is built for continuous ops and zonal failure protection. Option A is incorrect: Backup/restore = downtime. Option B is incorrect: Replica less direct for zonal failover. Option D is incorrect: Read replica no auto-failover for primary. Option C = HA for best uptime."}],"answer_description":""},{"id":"YdW2gvJBPAhC5cLkyWXr","answer_description":"","discussion":[{"upvote_count":"1","content":"Selected Answer: C\nThe best option is C. Use Database Migration Service (DMS). Option C is best because DMS is built for MySQL migration to Cloud SQL, ensuring integrity and minimizing downtime. Option A (Composer, Python, Compute Engine) is incorrect because Composer adds complexity, Python is manual, and Compute Engine is less managed. Option B (CSV, Storage Transfer, Cloud SQL) is incorrect because CSV export is slow, risks integrity, and has downtime. Option D (Data Fusion to Compute Engine) is incorrect because Data Fusion is overkill for simple migration, and Compute Engine is less managed than Cloud SQL. Therefore, Option C, DMS, is the most direct and efficient migration solution.","poster":"n2183712847","timestamp":"1741235700.0","comment_id":"1365726"}],"timestamp":"2025-03-06 05:35:00","question_images":[],"answer_ET":"C","url":"https://www.examtopics.com/discussions/google/view/157636-exam-associate-data-practitioner-topic-1-question-71/","topic":"1","question_id":69,"answer_images":[],"unix_timestamp":1741235700,"answers_community":["C (100%)"],"exam_id":2,"choices":{"D":"Use Cloud Data Fusion to migrate the MySQL database to MySQL on Compute Engine.","C":"Use Database Migration Service to replicate the MySQL database to a Cloud SQL for MySQL instance.","A":"Set up a Cloud Composer environment to orchestrate a custom data pipeline. Use a Python script to extract data from the MySQL database and load it to MySQL on Compute Engine.","B":"Export the MySQL database to CSV files, transfer the files to Cloud Storage by using Storage Transfer Service, and load the files into a Cloud SQL for MySQL instance."},"answer":"C","question_text":"You are migrating data from a legacy on-premises MySQL database to Google Cloud. The database contains various tables with different data types and sizes, including large tables with millions of rows and transactional data. You need to migrate this data while maintaining data integrity, and minimizing downtime and cost. What should you do?","isMC":true},{"id":"SbDxihaFep2frPTsSRTY","answers_community":["C (100%)"],"timestamp":"2025-03-06 05:36:00","answer_images":[],"isMC":true,"discussion":[{"content":"Selected Answer: C\nThe best option is C. 1. Cloud Storage 2. BigQuery 3. Firestore. Option C is best because: 1. Cloud Storage is ideal for audio files for training access. 2. BigQuery is perfect for SQL analysis of CSV PII data. 3. Firestore is designed for large volumes of small application documents. Option A is incorrect because CloudSQL is not for SQL analysis of large CSVs, and Bigtable is overkill for general document files. Option B is incorrect because Filestore is for shared file systems, not audio files, and CloudSQL/Datastore are wrong types for CSV analysis and document storage. Option D is incorrect because Filestore and BigQuery are mismatched for audio and document files. Therefore, Option C best aligns tools with use cases.","poster":"n2183712847","comment_id":"1365728","timestamp":"1741235760.0","upvote_count":"1"}],"question_id":70,"question_images":[],"question_text":"You created a customer support application that sends several forms of data to Google Cloud. Your application is sending:\n1. Audio files from phone interactions with support agents that will be accessed during trainings.\n2. CSV files of users’ personally identifiable information (Pll) that will be analyzed with SQL.\n3. A large volume of small document files that will power other applications.\nYou need to select the appropriate tool for each data type given the required use case, while following Google-recommended practices. Which should you choose?","exam_id":2,"topic":"1","choices":{"A":"1. Cloud Storage\n2. CloudSQL for PostgreSQL\n3. Bigtable","B":"1. Filestore\n2. Cloud SQL for PostgreSQL\n3. Datastore","D":"1. Filestore\n2. Bigtable\n3. BigQuery","C":"1. Cloud Storage\n2. BigQuery\n3. Firestore"},"unix_timestamp":1741235760,"answer_ET":"C","url":"https://www.examtopics.com/discussions/google/view/157637-exam-associate-data-practitioner-topic-1-question-72/","answer":"C","answer_description":""}],"exam":{"id":2,"lastUpdated":"11 Apr 2025","provider":"Google","name":"Associate Data Practitioner","isMCOnly":true,"isImplemented":true,"numberOfQuestions":72,"isBeta":false},"currentPage":14},"__N_SSP":true}