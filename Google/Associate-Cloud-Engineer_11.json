{"pageProps":{"questions":[{"id":"L2WGvnPJuX4Up276fEtU","answer_images":[],"question_text":"You have a Compute Engine instance hosting an application used between 9 AM and 6 PM on weekdays. You want to back up this instance daily for disaster recovery purposes. You want to keep the backups for 30 days. You want the Google-recommended solution with the least management overhead and the least number of services. What should you do?","discussion":[{"timestamp":"1613974080.0","upvote_count":"44","comment_id":"163330","poster":"ESP_SAP","content":"Correct Answer is (B):\n\nCreating scheduled snapshots for persistent disk\nThis document describes how to create a snapshot schedule to regularly and automatically back up your zonal and regional persistent disks. Use snapshot schedules as a best practice to back up your Compute Engine workloads. After creating a snapshot schedule, you can apply it to one or more persistent disks.\n\nhttps://cloud.google.com/compute/docs/disks/scheduled-snapshots","comments":[{"timestamp":"1615341720.0","upvote_count":"10","poster":"[Removed]","content":"Definitely B. \n\nWith something like this, you should not have to write any custom scripts, custom functions, or cron jobs. This is google's way of saying 'hey, we've already built that stuff in to our snapshot schedules feature.","comment_id":"176786"}]},{"timestamp":"1654666560.0","content":"it is b. we cannot define snapshot config in instance metadata.\nVM instance metadata is used only for:\nstartup and shutdown scripts\nhost maintanence\nguest attributes","comment_id":"496611","poster":"Ridhanya","upvote_count":"5"},{"timestamp":"1732551660.0","comment_id":"1218367","upvote_count":"2","content":"Now in 2024, use backup&DR for this. But in 2020 it was B","poster":"ccpmad"},{"comments":[{"poster":"ccpmad","upvote_count":"1","comment_id":"1218369","content":"No, it is not about schedule start-stop, is about snapshot schedule of the disks...\nbut yes, it is B.","timestamp":"1732551720.0"}],"comment_id":"1211333","content":"Selected Answer: B\nhttps://cloud.google.com/compute/docs/instances/schedule-instance-start-stop","timestamp":"1731587700.0","upvote_count":"1","poster":"Jonassamr"},{"content":"Selected Answer: B\nI think when we think about best practice, we should always think about being practical. The most practical method is usually the best practice with a few exceptions. In this scenario, Answers C and D require a lot of effort. Answer A seems not quite relevant. Answer B is the only correct option.","timestamp":"1721642220.0","comment_id":"1128564","poster":"idk_4","upvote_count":"1"},{"upvote_count":"2","poster":"Captain1212","comment_id":"998555","timestamp":"1709564160.0","content":"Selected Answer: B\nB is the correct answer"},{"poster":"Rajat2309sharma","comment_id":"797969","content":"Selected Answer: B\nB is ans","timestamp":"1691147460.0","upvote_count":"1"},{"poster":"slcvlctetri","upvote_count":"3","content":"Selected Answer: B\ngot this question 2 days ago. B is right.","comment_id":"760605","timestamp":"1688009040.0"},{"content":"B is more appropriate","upvote_count":"1","timestamp":"1671839640.0","comment_id":"621339","poster":"AzureDP900"},{"content":"Selected Answer: B\nCorrect Answer B","timestamp":"1669072860.0","upvote_count":"1","poster":"nhadi82","comment_id":"605034"},{"upvote_count":"3","content":"why not C?","comments":[{"timestamp":"1653804600.0","upvote_count":"3","comment_id":"489689","content":"The question calls for \"Google-recommended solution with the least management overhead and the least number of services\"","poster":"Gianfry"}],"comment_id":"416380","poster":"gcpengineer","timestamp":"1643401320.0"},{"comment_id":"362226","content":"B is correct","timestamp":"1637422080.0","upvote_count":"2","poster":"arsh1916"},{"poster":"Hi2ALL","comment_id":"313244","content":"B is more realistic approach","timestamp":"1631875620.0","upvote_count":"3"},{"poster":"GCP_user","comment_id":"312502","upvote_count":"2","content":"B is the best option so far. However just wonder this: Schedule frequency: Daily \" Start time: 1:00 AM \" 2:00 AM\" Autodelete snapshots: after 30 days; For Saturday and Sunday it will be a waste of resource to create snapshots since the instance is running during weekdays.","timestamp":"1631799960.0"},{"poster":"GCP_Student1","timestamp":"1630103100.0","content":"B. 1. In the Cloud Console, go to the Compute Engine Disks page and select your instance's disk. 2. In the Snapshot Schedule section, select Create Schedule and configure the following parameters: \"\" Schedule frequency: Daily \"\" Start time: 1:00 AM \"\" 2:00 AM \"\" Autodelete snapshots after 30 days","comment_id":"300523","upvote_count":"2"},{"poster":"DucSiu","upvote_count":"1","content":"It's B","comment_id":"278336","timestamp":"1627446780.0"},{"timestamp":"1621851120.0","comment_id":"226634","poster":"Bhagirathi","content":"B for sure, any doubt?","upvote_count":"1"},{"timestamp":"1621680300.0","upvote_count":"2","comment_id":"224948","content":"• B. 1. In the Cloud Console, go to the Compute Engine Disks page and select your instance's disk. 2. In the Snapshot Schedule section, select Create Schedule and configure the following parameters: \"\" Schedule frequency: Daily \"\" Start time: 1:00 AM \"\" 2:00 AM \"\" Autodelete snapshots after 30 days","poster":"swatititame"},{"upvote_count":"2","timestamp":"1615459980.0","content":"Has to B... \n- scripting which means overhead and maintenance So Option D neglected. \n- Cloud function, adding another service which not require.\n- MetaData, I am not sure if meta data have something to define as cron job schedule. So not be an option.","poster":"hiteshrup","comment_id":"177576"},{"timestamp":"1614370740.0","poster":"stepkurniawan","comment_id":"166973","content":"Either A or B, I am not sure. \nI have tested B and it works, but I dont know the name of the key to do A.","upvote_count":"1"},{"timestamp":"1613999160.0","comment_id":"163547","upvote_count":"3","poster":"juliandm","content":"B is right https://www.cloudbooklet.com/backup-google-cloud-vm-automatically-with-snapshot-schedules"},{"content":"The Ans is B.\nChecked on my GCP account.","poster":"baonguyen","timestamp":"1613980140.0","comment_id":"163371","upvote_count":"4"},{"upvote_count":"1","poster":"SSPC","timestamp":"1613670060.0","comments":[{"comment_id":"178309","comments":[{"timestamp":"1615574640.0","comment_id":"178310","content":"So will go with B","upvote_count":"3","poster":"[Removed]"}],"poster":"[Removed]","timestamp":"1615574580.0","upvote_count":"2","content":"You need to write the script and test it that is a big step."}],"comment_id":"160989","content":"I would go with the answer \"D\" but I don't totally sure"},{"poster":"SSPC","comments":[{"poster":"Eshkrkrkr","upvote_count":"3","content":"It's B - question asks for Google-recommended and with the least management overhead what makes D the wrong answer.","timestamp":"1620665820.0","comment_id":"216873"},{"poster":"francisco_guerra","comment_id":"157608","timestamp":"1613256960.0","upvote_count":"1","content":"Yes maybe A or D"}],"comment_id":"157297","timestamp":"1613226060.0","content":"B?? I think is \"D\". https://cloud.google.com/solutions/dr-scenarios-planning-guide#design_for_end-to-end_recovery","upvote_count":"1"}],"answer":"B","isMC":true,"url":"https://www.examtopics.com/discussions/google/view/28464-exam-associate-cloud-engineer-topic-1-question-144/","question_images":[],"topic":"1","answers_community":["B (100%)"],"unix_timestamp":1597321260,"timestamp":"2020-08-13 14:21:00","choices":{"B":"1. In the Cloud Console, go to the Compute Engine Disks page and select your instance's disk. 2. In the Snapshot Schedule section, select Create Schedule and configure the following parameters: - Schedule frequency: Daily - Start time: 1:00 AM ג€\" 2:00 AM - Autodelete snapshots after: 30 days","A":"1. Update your instances' metadata to add the following value: snapshotג€\"schedule: 0 1 * * * 2. Update your instances' metadata to add the following value: snapshotג€\"retention: 30","C":"1. Create a Cloud Function that creates a snapshot of your instance's disk. 2. Create a Cloud Function that deletes snapshots that are older than 30 days. 3. Use Cloud Scheduler to trigger both Cloud Functions daily at 1:00 AM.","D":"1. Create a bash script in the instance that copies the content of the disk to Cloud Storage. 2. Create a bash script in the instance that deletes data older than 30 days in the backup Cloud Storage bucket. 3. Configure the instance's crontab to execute these scripts daily at 1:00 AM."},"answer_ET":"B","answer_description":"","question_id":51,"exam_id":1},{"id":"UDCSsoUIPBc5F2bLKWLW","answer":"B","question_id":52,"url":"https://www.examtopics.com/discussions/google/view/46376-exam-associate-cloud-engineer-topic-1-question-145/","isMC":true,"answers_community":["B (78%)","D (22%)"],"answer_description":"","question_images":[],"discussion":[{"comment_id":"312463","poster":"GCP_Student1","content":"B is correct answer, read below form google docs;\n\nThis tutorial demonstrates how to migrate workloads running on a Google Kubernetes Engine (GKE) cluster to a new set of nodes within the same cluster without incurring downtime for your application. Such a migration can be useful if you want to migrate your workloads to nodes with a different machine type.\n\nBackground\nA node pool is a subset of machines that all have the same configuration, including machine type (CPU and memory) authorization scopes. Node pools represent a subset of nodes within a cluster; a container cluster can contain one or more node pools.\n\nWhen you need to change the machine profile of your Compute Engine cluster, you can create a new node pool and then migrate your workloads over to the new node pool.\n\nTo migrate your workloads without incurring downtime, you need to:\n\nMark the existing node pool as unschedulable.\nDrain the workloads running on the existing node pool.\nDelete the existing node pool.\n\nhttps://cloud.google.com/kubernetes-engine/docs/tutorials/migrating-node-pool#creating_a_node_pool_with_large_machine_type","upvote_count":"33","timestamp":"1647443340.0"},{"upvote_count":"1","comment_id":"998559","poster":"Captain1212","timestamp":"1725454680.0","content":"Selected Answer: B\nB is the right answer , if you need the new, and if you want to old one also then its D"},{"timestamp":"1710278880.0","comment_id":"837410","poster":"ashtonez","upvote_count":"1","content":"Selected Answer: B\nB is correct, creating another cluster just doesnt make any sense, node pools are intended for this situations"},{"comment_id":"832459","poster":"Chiunara","timestamp":"1709864100.0","upvote_count":"1","content":"Selected Answer: B\nAnswer is obviously B (read @GCP_Student1 and @Bobbybash replies)"},{"timestamp":"1707899280.0","upvote_count":"3","poster":"Bobbybash","comment_id":"808206","content":"Selected Answer: B\nB. Create a new Node Pool and specify machine type n2\"highmem\"16. Deploy the new pods.\n\nCreating a new Node Pool with the required machine type is the correct approach to deploy additional pods without any downtime. This approach allows you to scale the cluster horizontally by adding more nodes to the existing cluster. By creating a new Node Pool, you can add n2\"highmem\"16 nodes to the existing cluster, and deploy new pods on these nodes without affecting the existing services running on the n1\"standard\"2 nodes. This way, you can ensure high availability and zero downtime during the deployment. Option A (gcloud container clusters upgrade) upgrades the entire cluster, and Option C and D (creating a new cluster) involve deleting the existing cluster, which may cause downtime."},{"comment_id":"797566","poster":"BlueJay20","upvote_count":"2","comments":[{"comment_id":"802140","timestamp":"1707403980.0","poster":"swa99","content":"The keyword is \"additional\", in option D you are deleting the old cluster. SO the answer is B","upvote_count":"4"}],"content":"Selected Answer: D\nThe keyword is \"additional\". Answer B is good if you want to replace with the new VMs. In this case you want the existing ones as well as the new ones. Therefore D.","timestamp":"1707017340.0"},{"upvote_count":"1","content":"B makes perfect sense.","poster":"AzureDP900","comment_id":"621341","timestamp":"1687557360.0"},{"content":"Selected Answer: B\nB is correct","upvote_count":"1","timestamp":"1686305880.0","comment_id":"613821","poster":"Tirthankar17"},{"timestamp":"1653053340.0","upvote_count":"4","poster":"arsh1916","comment_id":"362227","content":"B is correct"},{"timestamp":"1649149920.0","content":"ANS : B\n\n1. The title did not say to delete four GKE n1.","comment_id":"328514","poster":"Jacky_YO","upvote_count":"4"},{"timestamp":"1648018320.0","upvote_count":"3","poster":"pondai","comment_id":"317830","content":"B,You need to create new node pool for cluster"},{"upvote_count":"2","poster":"dunhill","comment_id":"310368","timestamp":"1647244260.0","content":"I guess it's B. I couldn't find resize parameter under cluster upgrade. C and D are incorrect because it's no need to create new cluster."},{"poster":"GCP_Student1","timestamp":"1647208020.0","content":"A. Use gcloud container clusters upgrade. Deploy the new services.","upvote_count":"1","comment_id":"310011","comments":[{"timestamp":"1647443280.0","content":"I take it back, the correct answer is \"B\"\nB. Create a new Node Pool and specify machine type n2ג€\"highmemג€\"16. Deploy the new pods.","comment_id":"312462","upvote_count":"2","poster":"GCP_Student1"}]},{"content":"Answer is B - When you need to change the machine profile of your Compute Engine cluster, you can create a new node pool and then migrate your workloads over to the new node pool.","poster":"Jamaal_a","upvote_count":"3","timestamp":"1647007200.0","comment_id":"307906"}],"exam_id":1,"choices":{"A":"Use gcloud container clusters upgrade. Deploy the new services.","C":"Create a new cluster with n2ג€\"highmemג€\"16 nodes. Redeploy the pods and delete the old cluster.","B":"Create a new Node Pool and specify machine type n2ג€\"highmemג€\"16. Deploy the new pods.","D":"Create a new cluster with both n1ג€\"standardג€\"2 and n2ג€\"highmemג€\"16 nodes. Redeploy the pods and delete the old cluster."},"answer_ET":"B","question_text":"Your existing application running in Google Kubernetes Engine (GKE) consists of multiple pods running on four GKE n1`\"standard`\"2 nodes. You need to deploy additional pods requiring n2`\"highmem`\"16 nodes without any downtime. What should you do?","answer_images":[],"unix_timestamp":1615471200,"timestamp":"2021-03-11 15:00:00","topic":"1"},{"id":"TOXxIFTfDGOT1Jm7rzb2","timestamp":"2020-08-13 14:02:00","answer_description":"","answer":"D","exam_id":1,"answers_community":["D (67%)","B (33%)"],"question_text":"You have an application that uses Cloud Spanner as a database backend to keep current state information about users. Cloud Bigtable logs all events triggered by users. You export Cloud Spanner data to Cloud Storage during daily backups. One of your analysts asks you to join data from Cloud Spanner and Cloud\nBigtable for specific users. You want to complete this ad hoc request as efficiently as possible. What should you do?","question_images":[],"discussion":[{"comments":[{"content":"The question says: \" Join data from Cloud Spanner and Cloud Bigtable for specific users\" You can see the Google documentation in the link https://cloud.google.com/spanner/docs/export","timestamp":"1597926660.0","upvote_count":"3","comments":[{"content":"Oh my god, SSPC read you your links!\nThe process uses Dataflow and exports data to a folder in a Cloud Storage bucket. The resulting folder contains a set of Avro files and JSON manifest files. And what next? I will tell - next you read below: Compute Engine: Before running your export job, you must set up initial quotas for Recommended starting values are:\n\n CPUs: 200\n In-use IP addresses: 200\n Standard persistent disk: 50 TB\nStill think its A?","comment_id":"216920","upvote_count":"4","poster":"Eshkrkrkr","timestamp":"1605038820.0"}],"poster":"SSPC","comment_id":"162221"}],"content":"I think it should be D. https://cloud.google.com/bigquery/external-data-sources","timestamp":"1597886820.0","poster":"AmitKM","upvote_count":"38","comment_id":"161899"},{"poster":"ESP_SAP","timestamp":"1598070900.0","content":"Correct Answer is (D):\n\nIntroduction to external data sources\nThis page provides an overview of querying data stored outside of BigQuery.\n\nhttps://cloud.google.com/bigquery/external-data-sources","comment_id":"163341","upvote_count":"29","comments":[{"content":"As per your comment D is the answer. \nI also agree.\nBut can BigQurey read backed up data? , as we have backup data on Cloud storage, did not get any evidence in the link you shared.","upvote_count":"3","comment_id":"178315","timestamp":"1599929760.0","poster":"[Removed]"},{"comment_id":"163342","content":"BigQuery offers support for querying data directly from:\n\nBigtable\nCloud Storage\nGoogle Drive\nCloud SQL (beta)","timestamp":"1598070960.0","poster":"ESP_SAP","comments":[{"poster":"djgodzilla","timestamp":"1623610440.0","content":"but here we're not talking about joining Cloud Storage and Cloud Bigtable external tables.\nthe join happens between a distributed relational database (Spanner) and key-value NoSQL Database (BigTable) . how's converting Spanner to cloud storage an implicit and trivial step.","comments":[{"timestamp":"1623611580.0","upvote_count":"3","content":"\"The Cloud Spanner to Cloud Storage Text template is a batch pipeline that reads in data from a Cloud Spanner table, optionally transforms the data via a JavaScript User Defined Function (UDF) that you provide, and writes it to Cloud Storage as CSV text files.\"\nhttps://cloud.google.com/dataflow/docs/guides/templates/provided-batch#cloudspannertogcstext\n\n\"The Dataflow connector for Cloud Spanner lets you read data from and write data to Cloud Spanner in a Dataflow pipeline\"\nhttps://cloud.google.com/spanner/docs/dataflow-connector","comment_id":"381292","poster":"djgodzilla"}],"upvote_count":"1","comment_id":"381283"},{"timestamp":"1646661600.0","upvote_count":"6","content":"update:\nBigQuery supports the following external data sources:\n Bigtable\n Cloud Spanner\n Cloud SQL\n Cloud Storage\n Drive","comment_id":"562664","poster":"ryzior"}],"upvote_count":"6"}]},{"comment_id":"1316879","poster":"Ice_age","timestamp":"1732413780.0","content":"Interesting how most people are choosing D, yet that answer makes no reference to Cloud Spanner. I'm going to have to go with B since it specifically mentions Cloud Spanner and Cloud Bigtable.","upvote_count":"1"},{"upvote_count":"1","comment_id":"1151061","timestamp":"1708008000.0","poster":"kuracpalac","content":"Selected Answer: B\nThe Q says that an analyst wants to analyze data about a user from 2 different sources, which Dataflow will give you, plus as Google states, it allows you more time analyzing stuff and less time fiddling with setting things up, which option D is talking about, which is wrong per the asked Q."},{"comment_id":"1078556","timestamp":"1700755680.0","poster":"thewalker","content":"Selected Answer: D\nD is apt and possible.","upvote_count":"2"},{"timestamp":"1686113220.0","upvote_count":"3","content":"Selected Answer: D\nBigQuery is powerful. If you have data in one of the popular sources like Cloud Storage or Bigtable, it is much more efficient - both for cost and computation - to create an external table on those data sources, than to copy their data around.\n\nBesides that, also keep in mind that table clones and snapshots are much more efficient than full table copy etc.","poster":"KC_go_reply","comment_id":"916865"},{"timestamp":"1682884800.0","comment_id":"885581","upvote_count":"1","content":"Selected Answer: B\nI go for option B. As in option D, the data is backed up data and not the most recent data.","poster":"Praxii"},{"upvote_count":"2","comment_id":"808218","timestamp":"1676364000.0","poster":"Bobbybash","content":"Selected Answer: B\nB. Create a dataflow job that copies data from Cloud Bigtable and Cloud Spanner for specific users.\n\nTo join data from Cloud Spanner and Cloud Bigtable for specific users, creating a dataflow job that copies data from both sources is the most efficient option. This approach allows you to process the data in parallel, and you can take advantage of Cloud Dataflow's autoscaling feature to handle large volumes of data. You can use Cloud Dataflow to read data from Cloud Bigtable and Cloud Spanner, join the data based on the user fields, and write the output to a new location or send it to the analyst. Option A (copying data from Cloud Storage) does not provide data from Cloud Spanner, and option C (running a Spark job on a Dataproc cluster) involves higher overhead costs. Option D (using BigQuery external tables) is not efficient for ad hoc requests, as data is exported from Spanner to Cloud Storage during backups, so there may be a delay in data availability."},{"comment_id":"709934","timestamp":"1667407440.0","poster":"anolive","upvote_count":"1","content":"Selected Answer: D\nI thinks is D, but not 100% sure, because D does not have any infomation about the specific user like others options."},{"timestamp":"1665283380.0","poster":"Charumathi","content":"Selected Answer: D\nD is the correct answer,\nAn external data source is a data source that you can query directly from BigQuery, even though the data is not stored in BigQuery storage.\n\nBigQuery supports the following external data sources:\n\nAmazon S3\nAzure Storage\nCloud Bigtable\nCloud Spanner\nCloud SQL\nCloud Storage\nDrive","comment_id":"689884","upvote_count":"2"},{"content":"Selected Answer: D\nD makes sense as the BigQuery external tables are made for such use cases. and \"efficient\" keyword makes sense to use this way as resources used are less.","timestamp":"1665044880.0","upvote_count":"1","poster":"DualCore573","comment_id":"687613"},{"content":"First of all, using Dataflow can perhaps be effective, but NOT efficient, specially because of costs.\n\nSecond:\n\n“To query Cloud Bigtable data using a permanent external table, you: Create a table definition file (for the API or bq command-line tool); Create a table in BigQuery linked to the external data source; Query the data using the permanent table.”\n\nSource: https://cloud.google.com/bigquery/docs/external-data-bigtable#:~:text=To%20query%20Cloud%20Bigtable%20data,data%20using%20the%20permanent%20table\n\nThird:\n\n“To query a Cloud Storage external data source, provide the Cloud Storage URI path to your data and create a table that references the data source.”\n\nSource:\nhttps://cloud.google.com/bigquery/docs/external-data-cloud-storage\n\nCorrect answer: D.","poster":"soaresleo","comment_id":"643557","timestamp":"1659835260.0","upvote_count":"1"},{"comment_id":"638158","content":"Selected Answer: D\n\"efficiently as possible\" -> use the least amount of resources and achieve the same result... so I think it's D","upvote_count":"1","timestamp":"1658933940.0","poster":"jeffangel28"},{"timestamp":"1657452060.0","poster":"tomis2","upvote_count":"1","content":"Selected Answer: D\nMost \"cloud\" solution is D","comment_id":"629539"},{"timestamp":"1647695580.0","poster":"sabbella","content":"Selected Answer: D\noption d","upvote_count":"1","comment_id":"571060"},{"comment_id":"570408","upvote_count":"1","timestamp":"1647594000.0","content":"Selected Answer: D\nOption is D","poster":"sabbella"},{"comments":[{"content":"How does this create a \"join\" between the two tables?","timestamp":"1646215800.0","comments":[{"comment_id":"569001","content":"why do you think one cannot join 2 subsets of data in dataflow Its meant for processing sets of data.","upvote_count":"1","poster":"BigQuery","timestamp":"1647426840.0"}],"upvote_count":"1","comment_id":"559327","poster":"obeythefist"}],"upvote_count":"3","content":"Selected Answer: B\nI think it is B. \nThe data in Cloud storage is not up to date as backup window is daily. SO, there are chances is missing one day worth of data.\nAs it is mentioned as \"efficiently\" instead of quickly, I would choose \"B\".","comment_id":"540680","poster":"rljjhk","timestamp":"1644014160.0"},{"upvote_count":"1","content":"B is correct","comment_id":"540262","timestamp":"1643961480.0","poster":"sasithra"},{"timestamp":"1643898000.0","upvote_count":"1","comment_id":"539747","content":"Selected Answer: D\nAn external data source is a data source that you can query directly from BigQuery, even though the data is not stored in BigQuery storage.\n\nBigQuery supports the following external data sources:\n\nBigtable\nCloud Spanner\nCloud SQL\nCloud Storage\nDrive","poster":"raaj_p"},{"timestamp":"1638949260.0","upvote_count":"1","poster":"Ridhanya","comment_id":"496614","content":"I think it has to be option A because Dataflow needs to be used for bigtable export and cloud spanner data is already backed up in cloud storage on a daily basis","comments":[{"content":"Um, the questions says \"join\" so D seems right. I take back my previous","comment_id":"496616","timestamp":"1638949500.0","upvote_count":"2","poster":"Ridhanya"}]},{"timestamp":"1636415580.0","comment_id":"474542","content":"I vote D.","poster":"maggieli","upvote_count":"1"},{"poster":"vamgcp","content":"Option B - https://cloud.google.com/bigquery/external-data-sources","comment_id":"452075","upvote_count":"2","timestamp":"1632699840.0"},{"poster":"BalSan","timestamp":"1625989140.0","content":"One of the key word here is \"ad hoc\" request.... It doesn't make sense to create Dataflow Job for an adhoc request, ruling out A & B. C doesn't make sense for this use case and hence the answer is D","upvote_count":"3","comment_id":"403760"},{"timestamp":"1621517340.0","comment_id":"362229","poster":"arsh1916","upvote_count":"1","content":"D is best option"},{"poster":"arsh1916","timestamp":"1620229860.0","content":"Ans B, simple they are asking about Cloud bigtable & spanner","comment_id":"350423","upvote_count":"2"},{"comments":[{"timestamp":"1619898840.0","poster":"Amu89","content":"Can you please confirm why you choose Option A over D","comment_id":"347217","upvote_count":"1"}],"poster":"GCP_Student1","upvote_count":"2","comment_id":"310124","content":"A. Create a dataflow job that copies data from Cloud Bigtable and Cloud Storage for specific users.","timestamp":"1615682940.0"},{"poster":"EABDAJA","comment_id":"307084","timestamp":"1615372200.0","content":"D is correct","upvote_count":"1"},{"timestamp":"1614473700.0","poster":"GCP_Student1","upvote_count":"1","content":"D. Create two separate BigQuery external tables on Cloud Storage and Cloud Bigtable. Use the BigQuery console to join these tables through user fields, and apply appropriate filters.","comment_id":"300528"},{"content":"Answer will be D","upvote_count":"1","poster":"jamesET209","timestamp":"1608091380.0","comment_id":"245168"},{"poster":"Bhagirathi","upvote_count":"2","content":"D for for sure. it fits well to the ask.","timestamp":"1606220160.0","comment_id":"226639"},{"content":"You have an application that uses Cloud Spanner as a database backend to keep current state information about users. Cloud Bigtable logs all events triggered by users. You export Cloud Spanner data to Cloud Storage during daily backups. One of your analysts asks you to join data from Cloud Spanner and Cloud\nBigtable for specific users. You want to complete this ad hoc request as efficiently as possible. What should you do?","timestamp":"1606049340.0","comment_id":"224954","poster":"swatititame","upvote_count":"1"},{"upvote_count":"2","timestamp":"1602035700.0","poster":"cpd","comment_id":"194763","content":"BigQuery supports:\nCloud Bigtable\nCloud Storage\nGoogle Drive\nCloud SQL\n\nD"},{"upvote_count":"2","content":"D for me","timestamp":"1601928360.0","comment_id":"193766","poster":"RockAJ"},{"content":"B option does what the analyst is asking for. \nD is trying to merge data that is backed from cloud storage same is A and C,\nfrom A, C, D options D is the best if we know BigQuery External Source can be configured to read Cloud Spanner data backed (that can if read a back up file)","comments":[{"poster":"[Removed]","timestamp":"1600666980.0","upvote_count":"2","content":"Take back my comment, D is the solution. B only copes but does not talk about joining.","comment_id":"183459"}],"upvote_count":"1","comment_id":"178319","poster":"[Removed]","timestamp":"1599930180.0"},{"content":"I cant decide between A or B","upvote_count":"1","comment_id":"166985","timestamp":"1598466780.0","poster":"stepkurniawan"},{"comment_id":"163554","timestamp":"1598095620.0","upvote_count":"2","content":"D is the correct answer! Agreed to AmitKM","poster":"technicalguru"},{"comment_id":"160125","upvote_count":"1","poster":"SSPC","content":"What do you think the correct answer is A?","timestamp":"1597676400.0"},{"timestamp":"1597320120.0","upvote_count":"1","content":"https://cloud.google.com/spanner/docs/export\nI think that A is correct","comment_id":"157277","poster":"SSPC"}],"question_id":53,"isMC":true,"unix_timestamp":1597320120,"choices":{"D":"Create two separate BigQuery external tables on Cloud Storage and Cloud Bigtable. Use the BigQuery console to join these tables through user fields, and apply appropriate filters.","A":"Create a dataflow job that copies data from Cloud Bigtable and Cloud Storage for specific users.","B":"Create a dataflow job that copies data from Cloud Bigtable and Cloud Spanner for specific users.","C":"Create a Cloud Dataproc cluster that runs a Spark job to extract data from Cloud Bigtable and Cloud Storage for specific users."},"answer_images":[],"answer_ET":"D","topic":"1","url":"https://www.examtopics.com/discussions/google/view/28459-exam-associate-cloud-engineer-topic-1-question-146/"},{"id":"wILS1q96S0ab67DGz0QO","answer_description":"","url":"https://www.examtopics.com/discussions/google/view/46861-exam-associate-cloud-engineer-topic-1-question-147/","answers_community":["A (100%)"],"answer_images":[],"topic":"1","timestamp":"2021-03-13 11:11:00","question_text":"You are hosting an application from Compute Engine virtual machines (VMs) in us`\"central1`\"a. You want to adjust your design to support the failure of a single\nCompute Engine zone, eliminate downtime, and minimize cost. What should you do?","unix_timestamp":1615630260,"question_id":54,"answer_ET":"A","exam_id":1,"question_images":[],"choices":{"B":"ג€\" Create a Managed Instance Group and specify usג€\"central1ג€\"a as the zone. ג€\" Configure the Health Check with a short Health Interval.","A":"ג€\" Create Compute Engine resources in usג€\"central1ג€\"b. ג€\" Balance the load across both usג€\"central1ג€\"a and usג€\"central1ג€\"b.","D":"ג€\" Perform regular backups of your application. ג€\" Create a Cloud Monitoring Alert and be notified if your application becomes unavailable. ג€\" Restore from backups when notified.","C":"ג€\" Create an HTTP(S) Load Balancer. ג€\" Create one or more global forwarding rules to direct traffic to your VMs."},"isMC":true,"discussion":[{"comment_id":"310131","content":"A. Create Compute Engine resources in us \"central1 \"b. \" Balance the load across both us \"central1\"a and us \"central1\"b.","timestamp":"1631573520.0","poster":"GCP_Student1","upvote_count":"18"},{"content":"This seems straightforward. \"A\" is the only answer that involves putting instances in more than one zone!\n\nA. Yes, creating instances in another zone and balancing the loads will fix this problem\nB. Wrong. This keeps all the instances in one zone, but the question says we want to protect against zone failures.\nC. Wrong. This keeps all the instances in one zone, but the question says we want to protect against zone failures.\nD. Wrong. This keeps all the instances in one zone, but the question says we want to protect against zone failures.","upvote_count":"14","poster":"obeythefist","comment_id":"558632","timestamp":"1662008760.0"},{"comment_id":"1329711","content":"Selected Answer: A\nThe ideal solution would be to have a 'regional' instance group with a Load Balancer","upvote_count":"1","poster":"JoseCloudEng1994","timestamp":"1734729720.0"},{"content":"the Answer is B. the 2 Main ask are 1. Single Zone and Minimizes Cost \n\noption B is a cost-effective solution that can provide high availability within a single zone. By creating a Managed Instance Group in us-central1-a and configuring a Health Check with a short Health Interval, you can ensure that if one instance becomes unavailable, the Managed Instance Group will automatically create a new instance to replace it. This can help minimize downtime and ensure that your application remains available within the us-central1-a zone.","poster":"accd3fd","timestamp":"1729340880.0","comment_id":"1198600","upvote_count":"1","comments":[{"upvote_count":"1","content":"No, it is not B.","timestamp":"1732551120.0","comment_id":"1218359","poster":"ccpmad"}]},{"timestamp":"1709564880.0","poster":"Captain1212","upvote_count":"3","content":"Selected Answer: A\nA seems more right as it help with the Zone failure, all other create the same in same zone","comment_id":"998570"},{"content":"Selected Answer: A\nThe Answer is B","comment_id":"859615","timestamp":"1696316700.0","poster":"DrLegendgun","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: A\nA is best option","poster":"Angel_99","timestamp":"1676788380.0","comment_id":"648722"},{"poster":"abirroy","comment_id":"643095","content":"Selected Answer: A\nA is the best option","timestamp":"1675638180.0","upvote_count":"2"},{"content":"A is fine.","poster":"AzureDP900","upvote_count":"1","comment_id":"621343","timestamp":"1671839880.0"},{"upvote_count":"3","comment_id":"547049","poster":"[Removed]","timestamp":"1660465200.0","content":"Option A.\nCreate VMs across more than one region and zone so that you have alternative VMs to point to if a zone or region containing one of your VMs is disrupted. If you host all your VMs in the same zone or region, you won't be able to access any of those VMs if that zone or region becomes unreachable.\nhttps://cloud.google.com/compute/docs/tutorials/robustsystems#distribute"},{"timestamp":"1654668420.0","poster":"Ridhanya","comment_id":"496625","content":"A is correct because we have to eliminate single zone failure problem","upvote_count":"1"},{"comment_id":"489711","timestamp":"1653806760.0","upvote_count":"4","content":"Why not \"B\" selecting \"Regional (multi zone)\" ?\n\"Regional (multiple zone) coverage. Regional MIGs let you spread app load across multiple zones. This replication protects against zonal failures. If that happens, your app can continue serving traffic from instances running in the remaining available zones in the same region.\"\nhttps://cloud.google.com/compute/docs/instance-groups/","poster":"Gianfry"},{"comment_id":"488178","content":"it should be B , but because it specify the one Zone we can't pick this answer , the closest other option is A","timestamp":"1653652080.0","upvote_count":"1","poster":"kimharsh"},{"upvote_count":"6","comment_id":"362230","timestamp":"1637422140.0","content":"A is best option","poster":"arsh1916"},{"comment_id":"332494","poster":"mj98","timestamp":"1633866180.0","content":"Can someone explain how A?","upvote_count":"2","comments":[{"poster":"tifo16","content":"in order to remediate to the problem of single point of failure, we have to replicate VMs within multiple zones. Only A choice consider this concern","upvote_count":"10","timestamp":"1634586960.0","comment_id":"338421"},{"upvote_count":"1","content":"Other options do not prepare you for zonal outages","comment_id":"486620","timestamp":"1653472740.0","poster":"jabrrJ68w02ond1"}]},{"poster":"nzexamtopics","upvote_count":"1","comment_id":"331276","content":"A? Really? how?","timestamp":"1633703220.0"},{"poster":"NARWAL","timestamp":"1633283580.0","comment_id":"327527","content":"A is correct.","upvote_count":"4"},{"comment_id":"309613","content":"A - ג€\" Create Compute Engine resources in usג€\"central1ג€\"b. ג€\" Balance the load across both usג€\"central1ג€\"a and usג€\"central1ג€\"b.","upvote_count":"2","poster":"victory108","timestamp":"1631520660.0"}],"answer":"A"},{"id":"XzHDqEqkb4J8lRdXWeK0","isMC":true,"discussion":[{"poster":"ESP_SAP","timestamp":"1613976480.0","comment_id":"163346","content":"Correct Answer is (D):\n\nA simple approach would be to use the command flags available when listing all the IAM policy for a given project. For instance, the following command:\n\n`gcloud projects get-iam-policy $PROJECT_ID --flatten=\"bindings[].members\" --format=\"table(bindings.members)\" --filter=\"bindings.role:roles/owner\"`\n\noutputs all the users and service accounts associated with the role ‘roles/owner’ in the project in question. \n\nhttps://groups.google.com/g/google-cloud-dev/c/Z6sZs7TvygQ?pli=1","upvote_count":"46"},{"timestamp":"1613125440.0","content":"D: is the answer","comments":[{"upvote_count":"3","poster":"SSPC","comment_id":"157256","comments":[{"poster":"yurstev","comment_id":"250689","timestamp":"1624421940.0","content":"D IS THE ANSWER","upvote_count":"4"}],"timestamp":"1613223780.0","content":"D is the correct."}],"poster":"MohammedGhouse","comment_id":"156259","upvote_count":"13"},{"comment_id":"1144683","timestamp":"1723126320.0","content":"Selected Answer: D\nThe Answer is D. Per documentation: https://cloud.google.com/sdk/gcloud/reference/projects/get-iam-policy.\n\nAlso, just tried in my own account and it brought a list if all users and their roles.","poster":"blackBeard33","upvote_count":"1"},{"comment_id":"998573","poster":"Captain1212","content":"Selected Answer: D\nD seems, more correct","timestamp":"1709564940.0","upvote_count":"1"},{"timestamp":"1673357100.0","upvote_count":"1","poster":"tomis2","content":"Selected Answer: D\ngcloud iam get-iam-policy","comment_id":"629540"},{"poster":"AzureDP900","timestamp":"1671840000.0","content":"D is right","upvote_count":"1","comment_id":"621344"},{"content":"Selected Answer: D\nAnswer is D","comment_id":"606591","timestamp":"1669288620.0","poster":"Rutu_98","upvote_count":"2"},{"upvote_count":"1","poster":"somenick","timestamp":"1663942560.0","comment_id":"573756","content":"Selected Answer: D\ngcloud projects get-iam-policy $PROJECT_ID"},{"upvote_count":"12","poster":"obeythefist","timestamp":"1662009000.0","comments":[{"comment_id":"569014","timestamp":"1663318440.0","content":"NICE EXPLANATION; WAY TO G0 D","upvote_count":"1","poster":"BigQuery"}],"content":"I chose D by a process of elimination. Here's my take: \n\nA. There's more than one way to access an instance than just the SSH keys, and SSH keys have nothing to do with Project Owner role.\nB. Barking up the wrong tree here, Identity-Aware Proxy is more for remotely accessing resources, rather than Project Owner IAM roles. \nC. This will only work if everyone who is a Project Owner accesses the system so you can see them in the logs. What if a Project Owner doesn't access the Project for a while? How long will you wait? Nope. \nD. By elimination, this is the best result.","comment_id":"558634"},{"timestamp":"1659444120.0","upvote_count":"1","poster":"HansKloss611","content":"Selected Answer: D\nD is correct","comment_id":"538797"},{"comments":[{"poster":"tvinay","upvote_count":"1","content":"Confusion!! that's the main goal here so that we all go to the docs and Study hard xD","timestamp":"1654619880.0","comment_id":"496245"}],"timestamp":"1653251220.0","content":"how can the admin be so inconsistent throughout with the answers..not good ..its so confusing","poster":"PR0704","comment_id":"484602","upvote_count":"2"},{"comment_id":"454399","content":"D is the correct answer","timestamp":"1648579500.0","poster":"mohamedmahmoudf97","upvote_count":"1"},{"poster":"arsh1916","upvote_count":"2","timestamp":"1637422140.0","content":"D is correct","comment_id":"362231"},{"poster":"GCP_Student1","upvote_count":"3","content":"D. Use the command gcloud projects get\"\"iam\"\"policy to view the current role assignments.","timestamp":"1630166160.0","comment_id":"300920"},{"comment_id":"249917","content":"D 200%","poster":"Bhagirathi","timestamp":"1624338360.0","upvote_count":"3"},{"poster":"Bhagirathi","comment_id":"226647","timestamp":"1621851780.0","upvote_count":"1","content":"anyone will be confused - solution says one answer \n same time, all you guys have different choices here. what to take from this ?"},{"upvote_count":"1","content":"D. Use the command gcloud projects get\"\"iam\"\"policy to view the current role assignments.","poster":"swatititame","timestamp":"1621680660.0","comment_id":"224956"},{"content":"C is the likely answer. With D you see the current users with permission. With audit log you see those with prev. and current permission.","upvote_count":"1","timestamp":"1618250520.0","poster":"adeyemi5700","comment_id":"198656"},{"upvote_count":"2","timestamp":"1615576200.0","poster":"[Removed]","content":"D is the answer but the command is wrong\ngcloud projects get-iam-policy","comment_id":"178327"},{"timestamp":"1614372840.0","poster":"stepkurniawan","content":"Why C is not the answer?","comments":[{"timestamp":"1615576080.0","comment_id":"178325","content":"You need to wait until all of them access to determine who has access.","poster":"[Removed]","comments":[{"timestamp":"1620666720.0","comment_id":"216888","poster":"Eshkrkrkr","content":"No, you don't. With the command you'll see the current list of Owners (and all the other assignments as well).","upvote_count":"3"}],"upvote_count":"1"},{"poster":"francisco_guerra","content":"Because D is a easy way to do the same","comment_id":"168570","timestamp":"1614533760.0","upvote_count":"2"}],"upvote_count":"1","comment_id":"166998"}],"topic":"1","answer":"D","url":"https://www.examtopics.com/discussions/google/view/28240-exam-associate-cloud-engineer-topic-1-question-148/","exam_id":1,"unix_timestamp":1597220640,"question_text":"A colleague handed over a Google Cloud Platform project for you to maintain. As part of a security checkup, you want to review who has been granted the Project\nOwner role. What should you do?","answer_description":"","timestamp":"2020-08-12 10:24:00","choices":{"B":"Navigate to Identity-Aware Proxy and check the permissions for these resources.","A":"In the console, validate which SSH keys have been stored as project-wide keys.","C":"Enable Audit Logs on the IAM & admin page for all resources, and validate the results.","D":"Use the command gcloud projects getג€\"iamג€\"policy to view the current role assignments."},"question_id":55,"answers_community":["D (100%)"],"question_images":[],"answer_ET":"D","answer_images":[]}],"exam":{"numberOfQuestions":285,"name":"Associate Cloud Engineer","lastUpdated":"11 Apr 2025","id":1,"isBeta":false,"isImplemented":true,"provider":"Google","isMCOnly":true},"currentPage":11},"__N_SSP":true}