{"pageProps":{"questions":[{"id":"CARATpmpyDQcmsDoYsLQ","url":"https://www.examtopics.com/discussions/google/view/51259-exam-associate-cloud-engineer-topic-1-question-176/","unix_timestamp":1619788860,"answer":"A","topic":"1","answer_description":"","question_images":[],"exam_id":1,"question_text":"You will have several applications running on different Compute Engine instances in the same project. You want to specify at a more granular level the service account each instance uses when calling Google Cloud APIs. What should you do?","answers_community":["A (100%)"],"answer_ET":"A","answer_images":[],"timestamp":"2021-04-30 15:21:00","discussion":[{"comment_id":"349062","upvote_count":"24","timestamp":"1651628520.0","content":"A .","poster":"GoCloud"},{"poster":"JieHeng","upvote_count":"20","content":"A, when you create an instance using the gcloud command-line tool or the Google Cloud Console, you can specify which service account the instance uses when calling Google Cloud APIs - https://cloud.google.com/compute/docs/access/service-accounts#associating_a_service_account_to_an_instance","timestamp":"1656303420.0","comment_id":"391734"},{"comment_id":"1000074","timestamp":"1725587400.0","poster":"Captain1212","content":"Selected Answer: A\nOption A is correct , when you create the instance , that time itself you can specify the service account of each instance","upvote_count":"2"},{"timestamp":"1707948360.0","poster":"Bobbybash","comments":[{"timestamp":"1709976540.0","content":"used chatgpt","comment_id":"833775","poster":"VarunGo","upvote_count":"3"}],"content":"Selected Answer: A\nA. When creating the instances, specify a Service Account for each instance.\n\nTo specify a more granular level of service account for each Compute Engine instance, you should specify a Service Account for each instance when you create it. This can be done through the Compute Engine API or the Cloud Console. By doing so, the specified Service Account will be used when calling Google Cloud APIs from that instance.\n\nOption B, assigning the name of each Service Account as instance metadata, is not the best solution as metadata can be accessed by anyone with access to the instance, which could potentially lead to security issues.\n\nOptions C and D, using gcloud compute instances update to specify a Service Account or assign the name of a Service Account as instance metadata after starting the instances, can also be done, but it is a less efficient approach as it requires additional steps and can lead to human error if not properly documented.","upvote_count":"4","comment_id":"808895"},{"content":"Selected Answer: A\nVote for A, because there is no instance running yet. \"You will have several applications running...\"","timestamp":"1691481360.0","upvote_count":"3","comment_id":"644015","poster":"ryumada"},{"poster":"Roro_Brother","upvote_count":"1","comment_id":"626902","timestamp":"1688457660.0","content":"Selected Answer: A\nA, there is no instance running yet"},{"upvote_count":"1","poster":"AzureDP900","content":"A is good option for given scenario.","timestamp":"1686689160.0","comment_id":"615950"},{"upvote_count":"3","content":"Selected Answer: A\nYou can set/update the service account only when the instance is not running","poster":"somenick","timestamp":"1679828400.0","comment_id":"575549"},{"content":"Selected Answer: A\nA - the instances are not running yet","timestamp":"1677176940.0","poster":"Majkl93","comment_id":"554760","upvote_count":"2"},{"content":"A: you can define which GCP service account is associated with a Compute Engine instance when creating one. It is still possible to change the service account later.\nLink to the GCP docs: https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#using","poster":"Raz0r","timestamp":"1674594000.0","upvote_count":"1","comment_id":"531577"},{"poster":"liyux21","comment_id":"466138","upvote_count":"3","content":"A is correct. You can change the assigned service account, use gcloud compute instances set-service-account ...., not the update","timestamp":"1666442760.0"},{"timestamp":"1663348740.0","content":"Answer is C. The instances are already running. So you need to change the Service account","comments":[{"upvote_count":"8","timestamp":"1663348860.0","comments":[{"comment_id":"924996","timestamp":"1718528220.0","content":"Even if they would be running, I don't think it's possible to change the service account with the \"update\" command. You need to use \"set-service-account\" appropriately: https://cloud.google.com/sdk/gcloud/reference/compute/instances/set-service-account","upvote_count":"1","poster":"kaes"}],"content":"My bad - \"you will have\" . Correct asnwer - A","comment_id":"446110","poster":"techabhi2_0"}],"poster":"techabhi2_0","upvote_count":"2","comment_id":"446109"},{"timestamp":"1655278920.0","comment_id":"382426","upvote_count":"4","poster":"AD_0525","content":"A should be correct"},{"content":"A is the answer","timestamp":"1651343100.0","upvote_count":"4","poster":"norrec9","comment_id":"346377"},{"upvote_count":"3","poster":"Biju1","comment_id":"346220","timestamp":"1651324860.0","content":"It should be A"}],"isMC":true,"choices":{"B":"When creating the instances, assign the name of each Service Account as instance metadata.","A":"When creating the instances, specify a Service Account for each instance.","D":"After starting the instances, use gcloud compute instances update to assign the name of the relevant Service Account as instance metadata.","C":"After starting the instances, use gcloud compute instances update to specify a Service Account for each instance."},"question_id":86},{"id":"sPeyhXfhZFu9a3yg7vr1","answer_ET":"B","question_images":[],"exam_id":1,"answer_images":[],"isMC":true,"answer_description":"","discussion":[{"timestamp":"1653063840.0","content":"Simple it's B","poster":"arsh1916","comment_id":"362343","upvote_count":"17"},{"upvote_count":"14","comments":[{"poster":"lxgywil","upvote_count":"7","timestamp":"1652257380.0","comment_id":"354489","content":"I think that's it. The answer is B"}],"comment_id":"353356","timestamp":"1652142360.0","poster":"lxgywil","content":"MongoDB Atlas is actually managed and supported by third-party service providers.\n\nhttps://console.cloud.google.com/marketplace/details/gc-launcher-for-mongodb-atlas/mongodb-atlas"},{"upvote_count":"1","poster":"halifax","content":"Selected Answer: B\nI see why some of you are confused by the word \"Managed and SLA\". MongoDB is owned and run by a company called MongoDB Inc :-)\n\nWhen you deploy a service like MongoDB Atlas from the Google Cloud Marketplace, MongoDB, Inc. is responsible for the management of that service, including uptime guarantees and support. They provide an SLA that outlines their commitments regarding availability, performance, and support response times.","timestamp":"1734256800.0","comment_id":"1326773"},{"content":"Selected Answer: B\nThe keyword is managed MongoDB environment, both C and D are managed by users, A is irrelevant, So B","poster":"rahulrauki","comment_id":"1018374","timestamp":"1727406540.0","upvote_count":"4"},{"timestamp":"1725587460.0","upvote_count":"2","content":"Selected Answer: B\nB seems more correct , just deploy it from the Market place.","comment_id":"1000076","poster":"Captain1212"},{"poster":"WendyLC","timestamp":"1718827140.0","upvote_count":"1","content":"Selected Answer: B\nAnswer is B","comment_id":"927875"},{"upvote_count":"1","timestamp":"1698144180.0","poster":"PKookNN","content":"Selected Answer: B\nthe best answer is B","comment_id":"702924"},{"timestamp":"1698132420.0","poster":"nosense","upvote_count":"1","content":"Selected Answer: B\nb. fast and simple","comment_id":"702786"},{"timestamp":"1698092160.0","upvote_count":"1","poster":"11kc03","content":"Selected Answer: C\nAnswer is B","comment_id":"702455"},{"poster":"learn_GCP","content":"Selected Answer: B\nB. is the answer","timestamp":"1695991740.0","upvote_count":"1","comment_id":"682702"},{"timestamp":"1693744680.0","upvote_count":"1","comment_id":"658458","content":"B, keyword \" support SLA\"","poster":"snkhatri"},{"upvote_count":"2","poster":"AzureDP900","comment_id":"615949","content":"https://cloud.google.com/mongodb","timestamp":"1686688980.0","comments":[{"comment_id":"621674","upvote_count":"1","poster":"AzureDP900","content":"B is right","timestamp":"1687613280.0"}]},{"content":"B is correct","poster":"AzureDP900","timestamp":"1686688920.0","upvote_count":"1","comment_id":"615948"},{"comment_id":"594006","poster":"rsuresh27","content":"Anytime the question mentions a third party software, always use the cloud marketplace. Answer is B.","upvote_count":"6","timestamp":"1682709720.0"},{"comment_id":"525484","poster":"dishum","upvote_count":"1","content":"the question says \"want to deploy a managed MongoDB environment\" which means it should be managed by something, i.e compute engine or MIG.\n\nNear Ans is C or D\nI choose C - becoz no need of Mongo db running on MIG, GKE can easily handle mongoDb on compute engine.","timestamp":"1673934900.0"},{"comment_id":"507080","upvote_count":"1","content":"Deploy MongoDB Atlas its free Tier - does free tier provides a Support SLA?","timestamp":"1671712800.0","poster":"fazalmf"},{"content":"Selected Answer: B\nB is the right option to use managed services.","poster":"jaffarali","upvote_count":"2","comment_id":"501277","timestamp":"1671011340.0"},{"poster":"alaahakim","upvote_count":"1","comment_id":"482124","timestamp":"1668895800.0","content":"I Pick B"},{"content":"Answer should be B","timestamp":"1655278980.0","poster":"AD_0525","upvote_count":"2","comment_id":"382428"},{"poster":"GoCloud","upvote_count":"1","comment_id":"349065","timestamp":"1651628640.0","content":"B ."},{"comment_id":"346376","content":"Answer is b","timestamp":"1651343040.0","poster":"norrec9","upvote_count":"2"},{"poster":"Biju1","comment_id":"346201","content":"it should be B","upvote_count":"1","timestamp":"1651324020.0"}],"answer":"B","url":"https://www.examtopics.com/discussions/google/view/51204-exam-associate-cloud-engineer-topic-1-question-177/","unix_timestamp":1619772780,"question_text":"You are creating an application that will run on Google Kubernetes Engine. You have identified MongoDB as the most suitable database system for your application and want to deploy a managed MongoDB environment that provides a support SLA. What should you do?","choices":{"C":"Download a MongoDB installation package, and run it on Compute Engine instances.","B":"Deploy MongoDB Atlas from the Google Cloud Marketplace.","D":"Download a MongoDB installation package, and run it on a Managed Instance Group.","A":"Create a Cloud Bigtable cluster, and use the HBase API."},"topic":"1","timestamp":"2021-04-30 10:53:00","question_id":87,"answers_community":["B (93%)","7%"]},{"id":"w7blRmNjdErwBtJ6Q9X5","answer_description":"","exam_id":1,"answer":"D","answer_images":[],"answer_ET":"D","discussion":[{"timestamp":"1652023260.0","upvote_count":"31","content":"D is correct\nroles/bigquery.user\nWhen applied to a dataset, this role provides the ability to read the dataset's metadata and list tables in the dataset.\nWhen applied to a project, this role also provides the ability to run jobs, including queries, within the project. A member with this role can enumerate their own jobs, cancel their own jobs, and enumerate datasets within a project. Additionally, allows the creation of new datasets within the project; the creator is granted the BigQuery Data Owner role (roles/bigquery.dataOwner) on these new datasets.","comment_id":"352508","poster":"ApaMokus"},{"upvote_count":"9","timestamp":"1660353600.0","poster":"blan_ak","content":"Why on the earth would the answer be C? It has no relevance to the question. The answer is D, hands down","comment_id":"424000"},{"timestamp":"1734257940.0","comment_id":"1326779","content":"Selected Answer: D\nBad question.\nOption D is the best answer (of all the options), but it doesn't give the Custom role to run SQL queries against their data in BigQuery. If you are in doubt, check what the BigQuery User role allows.","upvote_count":"1","poster":"halifax"},{"upvote_count":"3","poster":"Captain1212","comment_id":"1000080","timestamp":"1725587640.0","content":"Selected Answer: D\nD is the right answer, just assign them the role by IAM and they will be able to use BQ"},{"comment_id":"737441","poster":"ankyt9","timestamp":"1701926640.0","upvote_count":"1","content":"Selected Answer: D\nD is correct"},{"content":"Selected Answer: D\nmakes mor sense","poster":"anolive","upvote_count":"1","comment_id":"709824","timestamp":"1698930960.0"},{"upvote_count":"2","timestamp":"1694851500.0","poster":"sylva91","comment_id":"670603","content":"Selected Answer: D\nD is correct because google recommendations are always to privilege groups to individual accounts and this is what can make the users query the database unlike the Data Studio"},{"upvote_count":"1","content":"Selected Answer: D\nD is right","comment_id":"658460","timestamp":"1693744800.0","poster":"snkhatri"},{"poster":"snkhatri","content":"D is correct","timestamp":"1693744740.0","comment_id":"658459","upvote_count":"1"},{"content":"D is the answer\n\nHint - to **run the custom SQL queries*** against the latest data in BigQuery","poster":"patashish","upvote_count":"2","timestamp":"1689200880.0","comment_id":"630708"},{"poster":"TaniaMalfoy","timestamp":"1688661780.0","comment_id":"627993","content":"C is correct, data pipeline is the key:\n\nhttps://cloud.google.com/dataflow/docs/guides/data-pipelines#create_a_batch_data_pipeline\n\nTo create this sample batch data pipeline, you must have access to the following resources in your project:\n\nA Cloud Storage bucket to store input and output files\nA BigQuery dataset where you will create a table.","upvote_count":"1"},{"content":"Selected Answer: D\nAnswer: D\nThe simplest.\nIt is not requested to automate the query. The BI team may also need to modify their query or have several different ones to meet the needs.","comment_id":"617724","upvote_count":"2","poster":"S00999","timestamp":"1687007100.0"},{"poster":"AzureDP900","content":"D is correct\nhttps://cloud.google.com/bigquery/docs/access-control","comment_id":"615946","upvote_count":"1","timestamp":"1686688800.0"},{"comment_id":"531575","timestamp":"1674593760.0","upvote_count":"1","poster":"Raz0r","content":"Selected Answer: D\nD sounds perfect with minimal steps.\nQuote from the GCP docs: \"BigQuery User\n(roles/bigquery.user)\n\nWhen applied to a dataset, this role provides the ability to read the dataset's metadata and list tables in the dataset.\n\nWhen applied to a project, this role also provides the ability to run jobs, including queries, within the project. A principal with this role can enumerate their own jobs, cancel their own jobs, and enumerate datasets within a project. Additionally, allows the creation of new datasets within the project; the creator is granted the BigQuery Data Owner role (roles/bigquery.dataOwner) on these new datasets.\""},{"timestamp":"1674029040.0","poster":"dishum","comment_id":"526400","content":"Answer is C\nOption C says, there is a probability of an internal BI datawarehouse. Before providing the iam permissions, it is better to copy data to internal BI. \nMy view","upvote_count":"1"},{"timestamp":"1673813280.0","poster":"arvsrv","upvote_count":"2","comment_id":"524405","content":"Selected Answer: D\nagree With D"},{"poster":"gioresin1","upvote_count":"1","timestamp":"1673560200.0","comment_id":"522483","content":"maybe D is not sufficient because, as per documentation:\n\"Note: For a user to be able to query the tables in a dataset, it is not sufficient for the user to have access to the dataset. A user must also have permission to run a query job in a project. If you want to give a user permission to run a query from your project, give the user the bigquery.jobs.create permission for the project. You can do this by assigning the user the roles/bigquery.jobUser role for your project. For more information, see Access control examples\"."},{"comment_id":"482125","poster":"alaahakim","timestamp":"1668895860.0","content":"I agree With D","upvote_count":"2"},{"content":"D should be the answer","timestamp":"1655279040.0","comment_id":"382430","poster":"AD_0525","upvote_count":"4"},{"poster":"victory108","timestamp":"1651622940.0","comment_id":"349032","upvote_count":"5","content":"D - Assign the IAM role of BigQuery User to a Google Group that contains the members of the BI team."},{"poster":"norrec9","comment_id":"346383","content":"D is the answer","upvote_count":"3","timestamp":"1651343340.0"},{"upvote_count":"3","content":"D is the answer","poster":"Biju1","comment_id":"346222","timestamp":"1651324980.0"}],"question_text":"You are managing a project for the Business Intelligence (BI) department in your company. A data pipeline ingests data into BigQuery via streaming. You want the users in the BI department to be able to run the custom SQL queries against the latest data in BigQuery. What should you do?","question_images":[],"isMC":true,"question_id":88,"url":"https://www.examtopics.com/discussions/google/view/51261-exam-associate-cloud-engineer-topic-1-question-178/","unix_timestamp":1619788980,"timestamp":"2021-04-30 15:23:00","topic":"1","choices":{"B":"Create a Service Account for the BI team and distribute a new private key to each member of the BI team.","C":"Use Cloud Scheduler to schedule a batch Dataflow job to copy the data from BigQuery to the BI team's internal data warehouse.","D":"Assign the IAM role of BigQuery User to a Google Group that contains the members of the BI team.","A":"Create a Data Studio dashboard that uses the related BigQuery tables as a source and give the BI team view access to the Data Studio dashboard."},"answers_community":["D (100%)"]},{"id":"JQ3VMDlFuq9QBng88rep","exam_id":1,"answers_community":["A (100%)"],"choices":{"D":"1. Create a VPC with a subnet for the DMZ and another VPC with a subnet for the LAN. 2. Set up firewall rules to open up relevant traffic between the DMZ and the LAN subnets, and another firewall rule to allow public egress traffic for the DMZ.","B":"1. Create a single VPC with a subnet for the DMZ and a subnet for the LAN. 2. Set up firewall rules to open up relevant traffic between the DMZ and the LAN subnets, and another firewall rule to allow public egress traffic for the DMZ.","C":"1. Create a VPC with a subnet for the DMZ and another VPC with a subnet for the LAN. 2. Set up firewall rules to open up relevant traffic between the DMZ and the LAN subnets, and another firewall rule to allow public ingress traffic for the DMZ.","A":"1. Create a single VPC with a subnet for the DMZ and a subnet for the LAN. 2. Set up firewall rules to open up relevant traffic between the DMZ and the LAN subnets, and another firewall rule to allow public ingress traffic for the DMZ."},"timestamp":"2021-04-30 15:24:00","question_text":"Your company is moving its entire workload to Compute Engine. Some servers should be accessible through the Internet, and other servers should only be accessible over the internal network. All servers need to be able to talk to each other over specific ports and protocols. The current on-premises network relies on a demilitarized zone (DMZ) for the public servers and a Local Area Network (LAN) for the private servers. You need to design the networking infrastructure on\nGoogle Cloud to match these requirements. What should you do?","question_images":[],"answer_description":"","isMC":true,"answer":"A","question_id":89,"unix_timestamp":1619789040,"answer_ET":"A","topic":"1","discussion":[{"comment_id":"402756","content":"Passed the test today. About 80% of the questions are here.","poster":"perdigiorno","upvote_count":"29","comments":[{"content":"Congratulations!","upvote_count":"5","timestamp":"1657728300.0","poster":"associatecloudexamuser","comment_id":"405550"},{"upvote_count":"3","comment_id":"460433","timestamp":"1665474240.0","poster":"sumanthrao1","content":"you got same questions from this examtopics"}],"timestamp":"1657373520.0"},{"comments":[{"upvote_count":"12","comments":[{"comments":[{"comment_id":"656896","timestamp":"1693623600.0","content":"and where do you have the VPC peering to communicate both VPCs?","upvote_count":"2","poster":"BenKenGo6"}],"poster":"Ashii","timestamp":"1653509580.0","content":"C is Create a VPC with a subnet for the DMZ and another VPC with a subnet for the LAN. 2. Set up firewall rules to open up relevant traffic between the DMZ and the LAN subnets, and another firewall rule to allow public ingress traffic for the DMZ. Without peering 2 VPC's how this this be done ?","upvote_count":"6","comment_id":"366658"},{"upvote_count":"2","poster":"demnok_lannik","comment_id":"535478","content":"of course you do","timestamp":"1674998820.0"},{"comment_id":"416599","poster":"gcpengineer","content":"You need fw rules","timestamp":"1659067440.0","upvote_count":"1"}],"content":"A is wrong. You don't need to set up firewall rules between subnets of the same VPC. C is the answer","comment_id":"365643","timestamp":"1653399960.0","poster":"Alela"}],"comment_id":"348415","timestamp":"1651565520.0","content":"A is the Right answer. You can discard B and C because they lack the need of creating Network Peering to communicate the DMZ VPC with the LAN VPC (LAN VPC is not exposed to public so they need to communicate via private addresses which cannot be achieved with 2 VPCs without Network Peering). Plus, you can discard B, as you don't need to enable the egress traffic, you always need to enable the ingress traffic as this is never enabled by default.","upvote_count":"28","poster":"yvinisiupacuando"},{"poster":"halifax","content":"Selected Answer: A\nOption C is NOT valid as it overlooks the requirement for VPC peering or another connection method to enable communication between two separate VPCs. \n1. There is no default connection between different VPC \n2. By default all incoming(ingress) traffic is denied. So, a firewall rule is needed even in the same VPC.","comment_id":"1326787","timestamp":"1734259440.0","upvote_count":"1"},{"poster":"taylz876","content":"Selected Answer: A\nThe answer is A:\nHere's the explanation:\n\n-->Single VPC: Creating a single Virtual Private Cloud (VPC) is a common practice to manage your resources in Google Cloud.\nSubnet for DMZ and LAN: Creating separate subnets within the same VPC for the DMZ (public-facing) and LAN (private) resources is a recommended approach to segregate your resources.\n-->Firewall Rules: Setting up firewall rules allows you to control traffic between the DMZ and LAN subnets and enables you to define specific access policies. You also need to allow public traffic (ingress) into the DMZ to make the public-facing resources accessible from the internet.","upvote_count":"8","comment_id":"1042289","timestamp":"1728794280.0"},{"poster":"Captain1212","upvote_count":"2","timestamp":"1725934860.0","comment_id":"1003624","content":"Selected Answer: A\nA is the correct answer, as it meet the question requirment"},{"comment_id":"700259","content":"Selected Answer: A\nA is the right choice","timestamp":"1697831520.0","poster":"diasporabro","upvote_count":"1"},{"timestamp":"1693744860.0","content":"Selected Answer: A\nA seems right","comment_id":"658463","upvote_count":"1","poster":"snkhatri"},{"upvote_count":"1","poster":"an0nym0us1","timestamp":"1692956100.0","content":"hi All what is the ans","comment_id":"651724"},{"comment_id":"621677","content":"1 VPC enough for LAN and DMZ , Need to open appropriate firewall rules. A is right.","upvote_count":"1","poster":"AzureDP900","timestamp":"1687613520.0"},{"comment_id":"617731","upvote_count":"2","poster":"S00999","timestamp":"1687008180.0","content":"Selected Answer: A\nVote for A\nBy default traffic between subnets on a VPC network is not allowed (except on the \"default\" network). \n(This blocks traffic between all instances, not just traffic between subnets => FW rules must be defined to allow communications between all instances, regardless the subnets)\n2 VPC will not work without peering."},{"poster":"somenick","content":"Selected Answer: A\nYou can't explicitly create a FW rule for the subnet, but connections are allowed or denied on a per-instance basis. You can think of the VPC firewall rules as existing not only between your instances and other networks, but also between individual instances within the same network.\nC will not work without peering...","comment_id":"575572","upvote_count":"2","timestamp":"1679831820.0"},{"poster":"akshaym87","comment_id":"535265","upvote_count":"6","comments":[{"content":"congratulation, i need to pass exam in end month .Can you give me some advise please?","upvote_count":"1","timestamp":"1691644800.0","poster":"Boumer","comment_id":"644792"}],"timestamp":"1674976260.0","content":"Guys i cleared my exam last week. This question bank is must. 80% questions were from here."},{"poster":"HansKloss611","comment_id":"531334","content":"Selected Answer: A\nA - my vote. Two different vpc need vpc peering.","timestamp":"1674568080.0","upvote_count":"3"},{"comment_id":"522470","poster":"gioresin1","timestamp":"1673558220.0","content":"I don't understand why you say that the answer is A. If you have 2 subnets in the same network you won't have firewall between the 2 subnets. So you can't have a DMZ that can communicate with a private network. So the answer should be C.","upvote_count":"1"},{"upvote_count":"1","comments":[{"comments":[{"comment_id":"462434","content":"Yes it is. I passed my exam on 3rd Oct received certificate on 7th Oct. Exam topics and a study course in Udemy which I bought during their discount sale, helped me.","timestamp":"1665813360.0","upvote_count":"1","poster":"jackwillis"}],"poster":"maan2935","comment_id":"450615","content":"Hi, I have an exam today. Are the questions still to some extent valid?","timestamp":"1663987440.0","upvote_count":"3"}],"content":"A is correct","poster":"[Removed]","comment_id":"450236","timestamp":"1663935240.0"},{"upvote_count":"4","comment_id":"426824","timestamp":"1660823400.0","content":"Textbook example of DMZ and private subnet topology, hence answer A. Anyone who thinks C or multiple VPCs or whatever I strongly suggest you do CCNA before coming here.","poster":"gerhardbl"},{"upvote_count":"2","timestamp":"1657727940.0","poster":"associatecloudexamuser","content":"Yes. Correct answer is A. No need to complicate the setup by creating two different VPC networks.","comment_id":"405546"},{"content":"All questions are still valid. I cleared my paper yesterday (shayan18@live.com)","upvote_count":"5","comment_id":"398395","timestamp":"1656943140.0","comments":[{"poster":"associatecloudexamuser","upvote_count":"1","timestamp":"1657728360.0","content":"Congratulations!","comment_id":"405551"}],"poster":"shayanahmed"},{"poster":"JieHeng","timestamp":"1656304560.0","comment_id":"391743","content":"Should be A.\nNot B, allow public egress traffic for DMZ won’t help anything (also by default there is already this allow public egress traffic rule)\nNot C & D, Network Peering is needed to allow internal IP address connectivity across two Virtual Private Cloud (VPC) https://cloud.google.com/vpc/docs/vpc-peering","upvote_count":"5"},{"poster":"AD_0525","timestamp":"1655392140.0","comment_id":"383499","content":"C should be the correct answer. Once you put both set of VMs in a single VPC, either all will be exposed to external traffic or none will be exposed.","upvote_count":"2"},{"content":"as we are looking to emulate the existing onsite environment - having 2 separate networks with firewalls in-between the networks better matches a DMZ and internal network setup.\n\nTherefore my choice is C.","timestamp":"1654587960.0","poster":"Tez1","upvote_count":"2","comment_id":"376607"},{"timestamp":"1654293540.0","content":"whats the ans?","comment_id":"373918","poster":"zsdfaq","upvote_count":"1"},{"upvote_count":"3","timestamp":"1653759720.0","content":"vote for C","comment_id":"368939","poster":"MQQ"},{"poster":"norrec9","timestamp":"1651343400.0","comment_id":"346385","upvote_count":"3","content":"A is the answer"},{"upvote_count":"3","timestamp":"1651325040.0","content":"A is correct","comment_id":"346224","poster":"Biju1"}],"url":"https://www.examtopics.com/discussions/google/view/51262-exam-associate-cloud-engineer-topic-1-question-179/","answer_images":[]},{"id":"blslrS3Kl79LJLYv3rRW","answers_community":["B (75%)","A (24%)","1%"],"unix_timestamp":1590954840,"choices":{"A":"Use Cloud Storage Object Lifecycle Management using Age conditions with SetStorageClass and Delete actions. Set the SetStorageClass action to 90 days and the Delete action to 275 days (365 ג€\" 90)","D":"Use gsutil rewrite and set the Delete action to 365 days.","B":"Use Cloud Storage Object Lifecycle Management using Age conditions with SetStorageClass and Delete actions. Set the SetStorageClass action to 90 days and the Delete action to 365 days.","C":"Use gsutil rewrite and set the Delete action to 275 days (365-90)."},"answer_ET":"B","timestamp":"2020-05-31 21:54:00","isMC":true,"url":"https://www.examtopics.com/discussions/google/view/21682-exam-associate-cloud-engineer-topic-1-question-18-discussion/","topic":"1","answer_images":[],"question_images":[],"discussion":[{"upvote_count":"59","poster":"Sammigbo","comments":[{"upvote_count":"6","content":"The correct ans is A.","comments":[{"comment_id":"348635","poster":"yvinisiupacuando","content":"Right answer is clearly B, \"A\" does not make any sense.","timestamp":"1620047100.0","upvote_count":"13"}],"comment_id":"232459","timestamp":"1606875420.0","poster":"JKRowlings"}],"comment_id":"99564","content":"Answer is B. There should be no reason to recalculate the time needed to delete after a year.","timestamp":"1590954840.0"},{"poster":"cloudenthu01","content":"Correct is B.\nYou only re-calculate expiry date when objects are re-written using re-write option to another storage class in which case creation date is rest.\nBut in this case objects is moveed to Coldline class after 90 days and then we want to delete the object after 365 days.","comments":[{"content":"You can change the storage class of an existing object either by rewriting the object or by using Object Lifecycle Management...Since Object Life cycle management was used there was no need to recalculate the expiration date and delete action still remains 365 days.\n\nhttps://cloud.google.com/storage/docs/storage-classes","timestamp":"1598689320.0","comment_id":"169035","upvote_count":"16","poster":"T_T_M"}],"timestamp":"1593070200.0","comment_id":"119193","upvote_count":"44"},{"timestamp":"1738832100.0","poster":"vaclavbenes1","content":"Selected Answer: B\nage\nThe age condition is satisfied when a resource reaches the specified age (in days). Age is measured from the resource's creation time.\n\nhttps://cloud.google.com/storage/docs/lifecycle#age","upvote_count":"2","comment_id":"1352285"},{"content":"Selected Answer: D\nWhy answer is not D","comment_id":"1341965","timestamp":"1737082200.0","poster":"devanshgoyal12","upvote_count":"1"},{"poster":"peddyua","content":"Selected Answer: B\n{\n \"rule\": [\n {\n \"action\": { \"type\": \"SetStorageClass\", \"storageClass\": \"COLDLINE\" },\n \"condition\": { \"age\": 90 }\n },\n {\n \"action\": { \"type\": \"Delete\" },\n \"condition\": { \"age\": 365 }\n }\n ]\n}","upvote_count":"1","timestamp":"1736955720.0","comment_id":"1341101"},{"content":"Selected Answer: A\nCloud Storage Object Lifecycle Management allows you to define conditions and actions for objects based on their age, creation time, or other attributes.\nTo meet the requirements:\nMove to Coldline after 90 days: This is achieved by setting a SetStorageClass action with an Age condition of 90 days.\nDelete after 1 year (365 days): Since the object would have been moved to Coldline after 90 days, the remaining time for deletion is 365 - 90 = 275 days. Set a Delete action with an Age condition of 275 days.","comment_id":"1333386","upvote_count":"1","poster":"narop","timestamp":"1735457460.0"},{"content":"Selected Answer: B\nA is not correct. age is defined based on creation time. SetStorageClass only affects the modification time, not the creation time. So the 90 days are not relevant for the deletion rule.","poster":"user263263","comment_id":"1318552","upvote_count":"2","timestamp":"1732700400.0"},{"comment_id":"1286096","upvote_count":"1","content":"Selected Answer: B\nLa gestión del ciclo de vida de objetos en Google Cloud Storage permite automatizar el cambio de la clase de almacenamiento y la eliminación de los objetos en función de su antigüedad. Para este caso:\n\nDespués de 90 días, se debe cambiar la clase de almacenamiento a Coldline, lo que se hace usando la acción SetStorageClass.\n\nLuego, después de 365 días (1 año), los objetos deben ser eliminados usando la acción Delete.\n\nA es incorrecta porque sugiere eliminar los objetos a los 275 días, lo cual no coincide con el requisito de eliminar los videos después de un año.\n\nC y D son incorrectas porque gsutil rewrite no es la herramienta correcta para gestionar políticas de ciclo de vida.","poster":"nubelukita45852","timestamp":"1726712880.0"},{"poster":"JackSkeletonCoder","upvote_count":"3","content":"Selected Answer: B\noption A can be misguiding but you don't have to specify 275 days since the rule would be implemented based on the creation of object not after the effect of the previous rule. Hence option B","comment_id":"1284936","timestamp":"1726518120.0"},{"upvote_count":"1","poster":"sh00001","content":"Option A\nCloud Storage Object Lifecycle Management allows you to define a set of rules that manage the lifecycle of your objects. The Age condition specifies the number of days since the object's creation.\nThe SetStorageClass action changes the storage class of objects within the bucket. Setting it to 90 days means that 90 days after the object's creation, it will be moved to Coldline Storage.\nThe Delete action specifies when the object should be deleted. Because the SetStorageClass action occurs at 90 days, you would set the Delete action at 275 days (365 total days from the creation - 90 days already passed until the class change), resulting in the object being deleted one year from its creation.","comment_id":"1239745","timestamp":"1719765240.0"},{"upvote_count":"1","content":"Selected Answer: B\nA is wrong cause there is no need of re-calculating. B is the right choice","timestamp":"1709205540.0","poster":"torresbytea","comment_id":"1162476"},{"comment_id":"1067526","upvote_count":"1","content":"Answer is B","poster":"Rahaf99","timestamp":"1699650300.0"},{"upvote_count":"2","comment_id":"1059491","poster":"gsmasad","content":"Selected Answer: B\nTime of creation is the reference NOT the time of movement","timestamp":"1698830160.0"},{"upvote_count":"1","timestamp":"1697015760.0","content":"Answer B","comment_id":"1040423","poster":"Evan7557"},{"content":"Selected Answer: B\nThe correct answer is B. \n\nCloud Storage Object Lifecycle Management is a feature that allows you to automatically transition objects to different storage classes or delete them based on user-defined rules.\n\nTo set up a lifecycle management policy to move videos to Coldline after 90 days and then delete them after one year, you would create a rule with the following conditions and actions:\n\n* Condition: Age is greater than 90 days\n* Action: Set storage class to Coldline\n* Condition: Age is greater than 365 days\n* Action: Delete\n\nThis policy will ensure that your videos are automatically moved to Coldline after 90 days, where they will be stored at a lower cost. After one year, the videos will be automatically deleted.\n\nHere is an example of how to create a lifecycle management policy using the gcloud command-line tool:\n\n\ngcloud storage lifecycle management policies set my-bucket my-policy --action-set-storage-class coldline --condition-age-days 90 --action-delete --condition-age-days 365","poster":"YourCloudGuru","comment_id":"1016959","upvote_count":"8","timestamp":"1695657540.0"},{"timestamp":"1694184660.0","poster":"CarlosMarin","comment_id":"1002589","upvote_count":"1","content":"Selected Answer: A\n\"... and then deleted after one year FROM THEIR CREATION\". I vote for A."},{"upvote_count":"1","timestamp":"1689850200.0","poster":"ExamsFR","comment_id":"957410","content":"Selected Answer: B\nAnswer is B"}],"answer_description":"","exam_id":1,"question_id":90,"question_text":"You need to set up a policy so that videos stored in a specific Cloud Storage Regional bucket are moved to Coldline after 90 days, and then deleted after one year from their creation. How should you set up the policy?","answer":"B"}],"exam":{"isMCOnly":true,"isImplemented":true,"name":"Associate Cloud Engineer","id":1,"numberOfQuestions":285,"lastUpdated":"11 Apr 2025","provider":"Google","isBeta":false},"currentPage":18},"__N_SSP":true}