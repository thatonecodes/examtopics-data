{"pageProps":{"questions":[{"id":"izZM79rl5Ig0J3Yu86iz","url":"https://www.examtopics.com/discussions/google/view/131103-exam-professional-machine-learning-engineer-topic-1-question/","exam_id":13,"choices":{"C":"Deploy the new model to the existing Vertex AI endpoint. Use traffic splitting to send 5% of production traffic to the new model. Monitor end-user metrics, such as listening time. If end-user metrics improve between models over time, gradually increase the percentage of production traffic sent to the new model.","A":"Create a new Vertex AI endpoint for the new model and deploy the new model to that new endpoint. Build a service to randomly send 5% of production traffic to the new endpoint. Monitor end-user metrics such as listening time. If end-user metrics improve between models over time, gradually increase the percentage of production traffic sent to the new endpoint.","B":"Capture incoming prediction requests in BigQuery. Create an experiment in Vertex AI Experiments. Run batch predictions for both models using the captured data. Use the user’s selected song to compare the models performance side by side. If the new model’s performance metrics are better than the previous model, deploy the new model to production.","D":"Configure a model monitoring job for the existing Vertex AI endpoint. Configure the monitoring job to detect prediction drift and set a threshold for alerts. Update the model on the endpoint from the previous model to the new model. If you receive an alert of prediction drift, revert to the previous model."},"answer_ET":"C","unix_timestamp":1705158720,"timestamp":"2024-01-13 16:12:00","question_id":186,"discussion":[{"timestamp":"1729045680.0","poster":"fitri001","content":"Selected Answer: C\nFor Simplicity: If speed and simplicity are your top priorities, deploying to the existing endpoint with caution (close monitoring during deployment) can work.--> choose C\nFor Safety and Control: If minimizing risk and having better control over the testing process are more important, creating a new endpoint is the better option. This is generally the recommended approach for most production deployments. --> choose A","comment_id":"1196298","upvote_count":"2"},{"upvote_count":"3","content":"Selected Answer: C\nHere's why the option C is preferable:\nMinimized complexity:\nLeverages existing endpoint: No need to create and manage a new endpoint, reducing setup and maintenance overhead.\nTraffic splitting readily available: Vertex AI provides built-in traffic splitting functionality, simplifying traffic distribution.\nEfficient testing and monitoring:\nDirect comparison: Sending a percentage of traffic to the new model allows for direct comparison with the current model's performance on real user data.\nGradual rollout: Starting with a small percentage mitigates potential risks and allows for gradual transition based on observed improvements.\nEnd-user metric monitoring: Focusing on metrics like listening time directly reflects user engagement and preference for the new recommendations.","comment_id":"1129195","poster":"daidai75","timestamp":"1721701800.0"},{"poster":"b1a8fae","comment_id":"1128735","content":"Selected Answer: C\nTraffic splitting is a feature of Vertex AI that allows you to distribute the prediction requests among multiple models or model versions within the same endpoint. You can specify the percentage of traffic that each model or model version receives, and change it at any time. Traffic splitting can help you test the new model in production without creating a new endpoint or a separate service. You can deploy the new model to the existing Vertex AI endpoint, and use traffic splitting to send 5% of production traffic to the new model. You can monitor the end-user metrics, such as listening time, to compare the performance of the new model and the previous model. If the end-user metrics improve between models over time, you can gradually increase the percentage of production traffic sent to the new model. This solution can help you test the new model in production while minimizing complexity and cost.","timestamp":"1721650800.0","upvote_count":"2"},{"upvote_count":"2","content":"Selected Answer: C\nOption A: Building a separate service adds unnecessary complexity and requires managing two endpoints.\nOption B: Batch predictions in Vertex AI Experiments might not reflect real-time user behavior and don't directly affect the production environment.\nOption D: Model monitoring alerts for prediction drift might be triggered by natural variations in user behavior instead of genuine performance issues and could lead to unnecessary model rollbacks.","poster":"pikachu007","comment_id":"1121781","timestamp":"1720876320.0"}],"answer":"C","isMC":true,"question_images":[],"answer_description":"","question_text":"You work for an organization that operates a streaming music service. You have a custom production model that is serving a “next song” recommendation based on a user's recent listening history. Your model is deployed on a Vertex AI endpoint. You recently retrained the same model by using fresh data. The model received positive test results offline. You now want to test the new model in production while minimizing complexity. What should you do?","answers_community":["C (100%)"],"answer_images":[],"topic":"1"},{"id":"flpmVvURGnlX70GGS77y","url":"https://www.examtopics.com/discussions/google/view/131104-exam-professional-machine-learning-engineer-topic-1-question/","exam_id":13,"choices":{"D":"Use the BigQuery API Connector and Cloud Scheduler to trigger Workflows every week that retrains the model.","B":"Create a pipeline in Vertex AI Pipelines that executes the retraining query, and use the Cloud Scheduler API to run the query weekly.","C":"Use Cloud Scheduler to trigger a Cloud Function every week that runs the query for retraining the model.","A":"Use BigQuery’s scheduling service to run the model retraining query periodically."},"answer_ET":"A","unix_timestamp":1705158900,"question_id":187,"timestamp":"2024-01-13 16:15:00","discussion":[{"upvote_count":"6","poster":"daidai75","timestamp":"1705984320.0","comment_id":"1129197","content":"Selected Answer: A\nNo additional setup: BigQuery's scheduling feature is built-in, eliminating the need to create pipelines, functions, or workflows.\nStraightforward configuration: Setting up a schedule for a query is a simple process within the BigQuery interface."},{"upvote_count":"2","poster":"AzureDP900","comment_id":"1242830","content":"A is right\nUsing BigQuery's scheduling service allows you to automate the retraining process without needing to write custom code or manage additional dependencies.","timestamp":"1720190280.0"},{"poster":"b1a8fae","timestamp":"1705933440.0","comment_id":"1128740","upvote_count":"3","content":"Selected Answer: A\nNo-brainer A."},{"content":"Selected Answer: A\nOption B: Vertex AI Pipelines offer flexibility for complex workflows, but it involves more development effort and potential costs for pipeline execution.\nOption C: Cloud Functions provide a serverless way to execute code, but they incur execution costs and require additional configuration for triggering and permissions.\nOption D: Workflows can manage complex orchestration, but configuring the BigQuery API Connector and Cloud Scheduler adds complexity and potential costs.","comment_id":"1121784","poster":"pikachu007","upvote_count":"3","timestamp":"1705158900.0"}],"isMC":true,"answer":"A","question_images":[],"answer_description":"","question_text":"You created a model that uses BigQuery ML to perform linear regression. You need to retrain the model on the cumulative data collected every week. You want to minimize the development effort and the scheduling cost. What should you do?","answers_community":["A (100%)"],"answer_images":[],"topic":"1"},{"id":"pnkxy7B7p8uI8jHj8G5u","url":"https://www.examtopics.com/discussions/google/view/131105-exam-professional-machine-learning-engineer-topic-1-question/","exam_id":13,"choices":{"D":"Use the aiplatform.log_metrics function to log the F1 score: and use the aiplatform.log_classification_metrics function to log the confusion matrix.","C":"Use the aiplatform.log_metrics function to log the F1 score and the confusion matrix.","A":"Use the aiplatform.log_classification_metrics function to log the F1 score, and use the aiplatform.log_metrics function to log the confusion matrix.","B":"Use the aiplatform.log_classification_metrics function to log the F1 score and the confusion matrix."},"answer_ET":"D","unix_timestamp":1705159020,"timestamp":"2024-01-13 16:17:00","question_id":188,"discussion":[{"timestamp":"1705934040.0","comment_id":"1128754","content":"Selected Answer: D\nI go with D.\nlog_classification_metrics currently support confusion matrix and ROC curve. https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#google_cloud_aiplatform_log_classification_metrics\nBecause it is not explicitly mentioned in the docs of log_classification_metrics, I assume F1 Score must be logged with log_metrics. https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#google_cloud_aiplatform_log_metrics (if accuracy and recall are logged in the example, probably F1 is done the same way)","poster":"b1a8fae","upvote_count":"6"},{"poster":"Omi_04040","timestamp":"1733811660.0","comment_id":"1324405","content":"Selected Answer: D\nD. Utilize the aiplatform.log_metrics function to log the F1 score, and employ the aiplatform.log_classification_metrics function to log the confusion matrix.\nUtilize the aiplatform.log_metrics function to log the F1 score, and employ the aiplatform.log_classification_metrics function to log the confusion matrix. This is the correct approach. aiplatform.log_metrics is appropriate for logging general metrics such as the F1 score, and aiplatform.log_classification_metrics is ideal for logging classification-specific metrics like the confusion matrix.\n\nReference:\nhttps://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#google_cloud_aiplatform_log_classification_metrics","upvote_count":"3"},{"timestamp":"1733151600.0","poster":"rajshiv","upvote_count":"1","content":"Selected Answer: B\nI think we can log both metrics together. D is a close second but B seems to be a better answer","comment_id":"1320991"},{"comment_id":"1296654","timestamp":"1728771720.0","upvote_count":"2","poster":"YangG","content":"Selected Answer: D\nd"},{"timestamp":"1717528020.0","content":"Selected Answer: D\nhttps://cloud.google.com/vertex-ai/docs/experiments/log-data#classification-metrics\n\nlog_classification_metrics -> only the confusion matrix, not the F1scores\nlog_metrics -> any number you want -> you can use it to store a F1 scores","poster":"bobjr","comment_id":"1224307","upvote_count":"4"},{"content":"Selected Answer: B\naiplatform.log_classification_metrics is specifically designed for logging classification metrics, which includes F1 score and confusion matrix.\naiplatform.log_metrics is a more generic function for logging any kind of metric, but it wouldn't capture the rich structure of a confusion matrix.\nTherefore, using aiplatform.log_classification_metrics allows you to log both F1 score and confusion matrix in a single call, simplifying your code and ensuring proper handling of these classification-specific metrics.","comment_id":"1203002","comments":[{"timestamp":"1714314120.0","content":"https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#google_cloud_aiplatform_log_classification_metrics","poster":"pinimichele01","upvote_count":"1","comment_id":"1203601"},{"content":"While aiplatform.log_metrics can handle numeric values like F1 score, it wouldn't capture the complexity of a confusion matrix. Confusion matrix is a two-dimensional table and requires specific handling for proper logging.expand_more\naiplatform.log_classification_metrics is designed for classification tasks and understands the structure of both F1 score and confusion matrix, allowing them to be logged efficiently in a single function call.","comment_id":"1203005","poster":"fitri001","upvote_count":"1","comments":[{"poster":"fitri001","comment_id":"1203006","timestamp":"1714205460.0","upvote_count":"1","comments":[{"timestamp":"1723384740.0","upvote_count":"2","comment_id":"1264186","content":"Hi fitri001. You are usually right but, I this particular case, I think D is the right answer.\n\nAs you can see here in the link I provide you below, it \"Currently support confusion matrix and ROC curve.\"\n\nLink: \n\nhttps://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#google_cloud_aiplatform_log_classification_metrics","poster":"tardigradum"}],"content":"Therefore, using separate functions like log_metrics for F1 score and log_classification_metrics for confusion matrix would be inefficient and might not capture the matrix structure accurately."}],"timestamp":"1714205460.0"}],"poster":"fitri001","upvote_count":"3","timestamp":"1714205400.0"},{"comments":[{"poster":"gscharly","timestamp":"1713516720.0","content":"forgot to add the link: https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#google_cloud_aiplatform_log_classification_metrics","comment_id":"1198460","upvote_count":"2"}],"comment_id":"1198459","timestamp":"1713516660.0","poster":"gscharly","content":"Selected Answer: D\nAccording to docs, log_classification_metrics supports confusion matrix and ROC curve. Not sure if it means that it only supports those... Assuming those are the only ones supported, I would got with D","upvote_count":"3"},{"upvote_count":"1","comments":[{"comment_id":"1203603","upvote_count":"1","timestamp":"1714314180.0","poster":"pinimichele01","content":"link??\n\ni find only: https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#google_cloud_aiplatform_log_classification_metrics\n\nso D NOT B"}],"comment_id":"1194698","timestamp":"1712990280.0","poster":"omermahgoub","content":"Selected Answer: B\naiplatform.log_classification_metrics to log metrics relevant to classification tasks, including F1 score and confusion matrix."},{"comment_id":"1169279","content":"Selected Answer: B\nThe aiplatform.log_classification_metrics function is designed to log classification metrics, including the F1 score and the confusion matrix. It takes the following arguments:\n\npredictions: The predicted labels.\nlabels: The true labels.\nweight: The weight of each sample.\nlogger: The logger to use.\n----------------------------\nThe aiplatform.log_metrics function is designed to log general metrics, such as accuracy, loss, and precision. It takes the following arguments:\n\nmetric: The metric to log.\nvalue: The value of the metric.\nstep: The step at which the metric was logged.\nlogger: The logger to use.","timestamp":"1709960340.0","poster":"Yan_X","upvote_count":"1"},{"timestamp":"1705984620.0","poster":"daidai75","upvote_count":"2","content":"Selected Answer: B\nActually, the F1 score is calculated by the Precision and recall metrics. The the log_classification_metrics is OK for both confusion matrix and F1 score","comment_id":"1129199"},{"content":"Selected Answer: B\nOption A: It's incorrect because aiplatform.log_metrics is a more general function that doesn't provide the same specialized structure for classification metrics.\nOption C: While technically possible to log both metrics using aiplatform.log_metrics, it's less optimal as it requires manual formatting and might not be as easily interpreted by Vertex AI's visualization tools.\nOption D: This is incorrect as it suggests using aiplatform.log_classification_metrics for the confusion matrix, but that function doesn't support logging confusion matrices directly.","upvote_count":"1","comment_id":"1121786","poster":"pikachu007","comments":[{"upvote_count":"4","timestamp":"1705933920.0","content":"Option B also suggests sing aiplatform.log_classification_metrics for the confusion matrix. Which is supported, btw. https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#google_cloud_aiplatform_log_classification_metrics","poster":"b1a8fae","comment_id":"1128751"}],"timestamp":"1705159020.0"}],"isMC":true,"answer":"D","question_images":[],"answer_description":"","question_text":"You want to migrate a scikit-learn classifier model to TensorFlow. You plan to train the TensorFlow classifier model using the same training set that was used to train the scikit-learn model, and then compare the performances using a common test set. You want to use the Vertex AI Python SDK to manually log the evaluation metrics of each model and compare them based on their F1 scores and confusion matrices. How should you log the metrics?","answers_community":["D (67%)","B (33%)"],"answer_images":[],"topic":"1"},{"id":"h9dA8amzs4rKIikpwnlF","answer_ET":"D","timestamp":"2024-01-13 16:20:00","topic":"1","url":"https://www.examtopics.com/discussions/google/view/131106-exam-professional-machine-learning-engineer-topic-1-question/","question_images":[],"discussion":[{"upvote_count":"3","content":"Selected Answer: E\nD&E - right","timestamp":"1732717380.0","comment_id":"1318712","poster":"AB_C"},{"comment_id":"1283803","content":"Selected Answer: D\nFrom my statistical point of view, D and E will mitigate the effect of bias.","poster":"wences","timestamp":"1726358940.0","upvote_count":"2"},{"upvote_count":"1","poster":"AzureDP900","comment_id":"1242924","timestamp":"1720200180.0","content":"D and E is right answer, question asks us to select 2 right answers\n• To avoid creating or reinforcing unfair bias in the model, you should collect a representative and diverse dataset (option D) that includes a stratified sample of production traffic. This ensures that your training data is inclusive and accurately represents the diversity of your target audience.\n• Once you have collected your training dataset, you should conduct fairness tests across sensitive categories and demographics on the trained model (option E). This involves evaluating whether the model treats different demographic groups fairly and without bias. If biases are detected, you can take steps to mitigate them and ensure that your model is fair and accurate."},{"upvote_count":"3","timestamp":"1720200120.0","content":"D and E is right answer, question asks us to select 2 right answers\n• To avoid creating or reinforcing unfair bias in the model, you should collect a representative and diverse dataset (option D) that includes a stratified sample of production traffic. This ensures that your training data is inclusive and accurately represents the diversity of your target audience.\n• Once you have collected your training dataset, you should conduct fairness tests across sensitive categories and demographics on the trained model (option E). This involves evaluating whether the model treats different demographic groups fairly and without bias. If biases are detected, you can take steps to mitigate them and ensure that your model is fair and accurate.","poster":"AzureDP900","comment_id":"1242921"},{"poster":"dija123","timestamp":"1719943800.0","content":"Selected Answer: D\nAgree with D and E","upvote_count":"1","comment_id":"1240955"},{"comment_id":"1194699","content":"Selected Answer: D\nD. Stratified sampling to ensure the different demographic groups or categories are proportionally represented in the training data. This helps mitigate bias that might arise if certain groups are under-represented.\nE. Fairness tests can reveal disparities in how the model treats different populations, allowing you to identify and address potential biases.","timestamp":"1712990460.0","upvote_count":"3","poster":"omermahgoub"},{"comment_id":"1190463","upvote_count":"2","content":"Selected Answer: D\nD and E is the two answers. Two selections are required","timestamp":"1712417100.0","comments":[{"poster":"pinimichele01","timestamp":"1712641860.0","upvote_count":"2","content":"why not D and A?","comment_id":"1192012"}],"poster":"MultiCloudIronMan"},{"content":"Selected Answer: D\nI went D, E","poster":"CHARLIE2108","comment_id":"1153610","timestamp":"1708295940.0","upvote_count":"2"},{"poster":"guilhermebutzke","timestamp":"1708127580.0","content":"Selected Answer: D\nDE\n\nD. Collect a stratified sample of production traffic to build the training dataset: This ensures that the training data represents the diverse demographics that will be targeted by the advertising campaigns. Random sampling might unintentionally underrepresent certain groups, leading to biased model outputs.\n\nE. Conduct fairness tests across sensitive categories and demographics on the trained model: This allows you to identify and address any potential biases that may have emerged during the training process. Evaluating the model's performance on different groups helps ensure fair and responsible deployment.","upvote_count":"1","comment_id":"1152291"},{"timestamp":"1705984740.0","comment_id":"1129200","content":"Selected Answer: D\nI go for D & E:\nA stratified sample ensures that the training data represents the distribution of the target population across relevant demographics or other sensitive categories. This helps mitigate bias arising from underrepresented groups in the data.\nRegularly testing the model for fairness across sensitive categories helps identify and address potential bias issues before deploying the model in production. This can involve metrics like precision, recall, and F1 score for different demographic groups.","poster":"daidai75","upvote_count":"1"},{"comment_id":"1128761","upvote_count":"3","poster":"b1a8fae","timestamp":"1705934220.0","content":"Selected Answer: D\nD E. ChatGPT explanation below (but I think makes quite a lot of sense)\n\nCollect a Stratified Sample (Option D): Stratified sampling involves dividing the population into subgroups (strata) and then randomly sampling from each subgroup. This ensures that the training dataset represents the diversity of the population, helping to avoid biases. By collecting a stratified sample of production traffic, you are more likely to have a balanced representation of different demographic groups, reducing the risk of biased model outcomes.\n\nConduct Fairness Tests (Option E): After training the model, it's crucial to conduct fairness tests to evaluate its performance across different sensitive categories and demographics. This involves measuring the model's predictions and outcomes for various groups to identify any disparities. Fairness tests help you assess and address biases that may have been inadvertently introduced during the training process."},{"poster":"shadz10","comments":[{"upvote_count":"1","comment_id":"1148874","poster":"tavva_prudhvi","timestamp":"1707793860.0","content":"Check b1a8fae comment on why D is better than C!"}],"comment_id":"1125768","upvote_count":"1","timestamp":"1705576620.0","content":"Selected Answer: C\nC, D - Conducting fairness tests across sensitive categories and demographics on the trained model is indeed important. However, this option focuses on post-training analysis rather than dataset creation. While it's a crucial step for ensuring fairness, it doesn't directly address how to create a training dataset to avoid bias.\nHence C,D"},{"upvote_count":"1","timestamp":"1705159200.0","content":"Selected Answer: D\nD. Stratified Sampling: Randomly sampling your data might not accurately represent the diversity of your target audience, potentially introducing bias by over- or under-representing certain demographics. Stratified sampling ensures your training dataset reflects the distribution of sensitive features (e.g., age, gender, income) observed in your production traffic, helping mitigate bias during model training.\n\nE. Fairness Testing: Simply collecting unbiased data isn't enough. Regularly testing your trained model for fairness across sensitive categories is crucial. This involves measuring and analyzing metrics like accuracy, precision, recall, and F1 score for different demographic groups. Identifying disparities in performance can trigger further investigation and potential re-training to address bias.","poster":"pikachu007","comment_id":"1121788"}],"choices":{"B":"Include only the demographic groups that most frequently interact with advertisements","D":"Collect a stratified sample of production traffic to build the training dataset","E":"Conduct fairness tests across sensitive categories and demographics on the trained model","A":"Include a comprehensive set of demographic features","C":"Collect a random sample of production traffic to build the training dataset"},"isMC":true,"answer":"D","unix_timestamp":1705159200,"answer_description":"","answer_images":[],"question_id":189,"question_text":"You are developing a model to help your company create more targeted online advertising campaigns. You need to create a dataset that you will use to train the model. You want to avoid creating or reinforcing unfair bias in the model. What should you do? (Choose two.)","exam_id":13,"answers_community":["D (80%)","E (15%)","5%"]},{"id":"Ad3AYaqq1AdWXAI1X9xi","answers_community":["B (74%)","D (26%)"],"question_text":"You are an ML engineer at a regulated insurance company. You are asked to develop an insurance approval model that accepts or rejects insurance applications from potential customers. What factors should you consider before building the model?","topic":"1","answer_images":[],"answer_description":"","exam_id":13,"choices":{"A":"Redaction, reproducibility, and explainability","C":"Federated learning, reproducibility, and explainability","D":"Differential privacy, federated learning, and explainability","B":"Traceability, reproducibility, and explainability"},"answer":"B","discussion":[{"content":"I think the answer should be B. as I review the OECD document on impact of AI on insurance, the document mention explainability, traceable. However, open for discussion. https://www.oecd.org/finance/Impact-Big-Data-AI-in-the-Insurance-Sector.pdf","comment_id":"407927","upvote_count":"32","timestamp":"1626444120.0","poster":"gcp2021go"},{"poster":"salsabilsf","comment_id":"376863","content":"Should be B","comments":[{"comment_id":"395531","timestamp":"1625116080.0","poster":"DucLee3110","content":"I think it should be A, as it is regulated, so need to have PII","upvote_count":"2"}],"timestamp":"1623078240.0","upvote_count":"13"},{"upvote_count":"1","comment_id":"1266804","poster":"Rajashekharc","content":"As per ChatGPT, anwser is B","timestamp":"1723780260.0"},{"content":"Selected Answer: B\nAgree with B","timestamp":"1720205040.0","upvote_count":"1","comment_id":"1242967","poster":"dija123"},{"poster":"PhilipKoku","timestamp":"1717679940.0","upvote_count":"1","comment_id":"1225531","content":"Selected Answer: B\nB) Traceability, Reproducibility and Explainability"},{"comment_id":"1224902","poster":"Goosemoose","content":"A kinda makes sense here, because redaction means remove sensitive or private information before sharing it..","timestamp":"1717610040.0","upvote_count":"2"},{"comment_id":"1198573","upvote_count":"2","poster":"gscharly","content":"Selected Answer: B\nwent with B","timestamp":"1713526860.0"},{"poster":"Sum_Sum","comments":[{"timestamp":"1716462840.0","upvote_count":"1","comment_id":"1216493","poster":"nmnm22","content":"you are a lifesaver, sum sum. thank you"}],"content":"Selected Answer: B\nB. Traceability, reproducibility, and explainability.\n\nTraceability: This involves maintaining records of the data, decisions, and processes used in the model. This is crucial in regulated industries for audit purposes and to ensure compliance with regulatory standards. It helps in understanding how the model was developed and how it makes decisions.\n\nReproducibility: Ensuring that the results of the model can be reproduced using the same data and methods is vital for validating the model's reliability and for future development or debugging.\n\nExplainability: Given the significant impact of the model’s decisions on individuals' lives, it's crucial that the model's decisions can be explained in understandable terms. This is not just a best practice in AI ethics; in many jurisdictions, it's a legal requirement under regulations that mandate transparency in automated decision-making.","upvote_count":"5","timestamp":"1700062620.0","comment_id":"1071583"},{"comment_id":"940673","timestamp":"1688287680.0","upvote_count":"4","poster":"tavva_prudhvi","content":"Selected Answer: B\nB. Traceability, reproducibility, and explainability are the most important factors to consider before building an insurance approval model. \nTraceability ensures that the data used in the model is reliable and can be traced back to its source. \nReproducibility ensures that the model can be replicated and tested to ensure its accuracy and fairness. \nExplainability ensures that the model's decisions can be explained to customers and regulators in a transparent manner. These factors are crucial for building a trustworthy and compliant model for an insurance company.\nRedaction is also important for protecting sensitive customer information, but it is not as critical as the other factors listed. Federated learning and differential privacy are techniques used to protect data privacy, but they are not necessarily required for building an insurance approval model."},{"upvote_count":"1","timestamp":"1683608460.0","poster":"M25","content":"Selected Answer: B\nWent with B","comment_id":"892704"},{"timestamp":"1676890680.0","upvote_count":"4","comment_id":"815151","content":"Selected Answer: B\nB. Traceability, reproducibility, and explainability\n\nWhen developing an insurance approval model, it's crucial to consider several factors to ensure that the model is fair, accurate, and compliant with regulations. The factors to consider include:\n\nTraceability: It's important to be able to trace the data used to build the model and the decisions made by the model. This is important for transparency and accountability.\n\nReproducibility: The model should be built in a way that allows for its reproducibility. This means that other researchers should be able to reproduce the same results using the same data and methods.\n\nExplainability: The model should be able to provide clear and understandable explanations for its decisions. This is important for building trust with customers and ensuring compliance with regulations.\n\nOther factors that may also be important to consider, depending on the specific context of the insurance company and its customers, include data privacy and security, fairness, and bias mitigation.","poster":"shankalman717"},{"poster":"shankalman717","comment_id":"815150","content":"B. Traceability, reproducibility, and explainability\n\nWhen developing an insurance approval model, it's crucial to consider several factors to ensure that the model is fair, accurate, and compliant with regulations. The factors to consider include:\n\nTraceability: It's important to be able to trace the data used to build the model and the decisions made by the model. This is important for transparency and accountability.\n\nReproducibility: The model should be built in a way that allows for its reproducibility. This means that other researchers should be able to reproduce the same results using the same data and methods.\n\nExplainability: The model should be able to provide clear and understandable explanations for its decisions. This is important for building trust with customers and ensuring compliance with regulations.\n\nOther factors that may also be important to consider, depending on the specific context of the insurance company and its customers, include data privacy and security, fairness, and bias mitigation.","upvote_count":"1","timestamp":"1676890620.0"},{"comments":[{"content":"Please mention the links","timestamp":"1678284720.0","comment_id":"833039","upvote_count":"2","poster":"tavva_prudhvi"}],"content":"Selected Answer: D\nChecking Google documents, it seems D.","poster":"ares81","timestamp":"1672933920.0","comment_id":"766794","upvote_count":"2"},{"poster":"wish0035","content":"Selected Answer: B\nans: B","timestamp":"1671141240.0","comment_id":"746547","upvote_count":"1"},{"content":"Selected Answer: B\nCorrect answer is \"B\"","timestamp":"1660568040.0","upvote_count":"1","comment_id":"647205","poster":"GCP72"},{"upvote_count":"3","poster":"capt2101akash","comment_id":"633669","content":"Selected Answer: D\nshould be D as all of the techniques abide to any problems related to insurance","timestamp":"1658246460.0"},{"comment_id":"632481","timestamp":"1658040780.0","upvote_count":"1","content":"B should be True","poster":"suresh_vn"},{"content":"Selected Answer: B\nTraceability is a key factor due to GDPR laws","upvote_count":"3","poster":"rgrand8","comment_id":"596048","timestamp":"1651496640.0"},{"comment_id":"569727","poster":"baimus","timestamp":"1647521700.0","upvote_count":"2","content":"Just to add weight the correct side: this is B."},{"upvote_count":"3","content":"Selected Answer: B\ndon't undertsand why you are thinking of the privacy issue. Here it is not mentioned nor relevant imo. Moreover the traceability is a key. For me B","poster":"lordcenzin","comment_id":"549750","timestamp":"1645133280.0"},{"poster":"sid515","timestamp":"1642831080.0","comment_id":"529633","upvote_count":"2","content":"It should be D. Differential privacy - so that PII data is masked, Federated learning - Federated Learning enables mobile phones to collaboratively learn a shared prediction model while keeping all the training data on device, decoupling the ability to do machine learning from the need to store the data in the cloud"},{"upvote_count":"2","poster":"MK_Ahsan","timestamp":"1641799680.0","comment_id":"520711","content":"ANSWER: B\nhttps://medium.com/artefact-engineering-and-data-science/including-ethics-best-practices-in-your-data-science-project-from-day-one-c15b26c2bf99"},{"upvote_count":"4","content":"Selected Answer: D\nAnswer D","poster":"Sneg42","timestamp":"1638139980.0","comment_id":"489459"},{"upvote_count":"1","timestamp":"1631601600.0","content":"Should be B.","poster":"Y2Data","comment_id":"444376"},{"comment_id":"441661","timestamp":"1631137740.0","upvote_count":"1","content":"Answer is B. Traceability means you need to store the raw data, redaction will edit / extract the raw data, without the raw data, reproducibility is impossible.","poster":"Danny2021"},{"comments":[{"poster":"sensev","content":"I don't think D is correct, since federated learning is used for de-centralized learning. In this scenario for insurance approval, there is no need to contentiously train the model in de-centralized fashion such as on a user device.","upvote_count":"3","timestamp":"1627543740.0","comment_id":"416695"}],"upvote_count":"3","content":"ANS: D\nhttps://developers.googleblog.com/2021/01/how-were-helping-developers-with-differential-privacy.html\nBy adding differential privacy to these new app metrics, we’ll provide meaningful insights to help developers improve their apps without compromising people’s privacy, or developer confidentiality. \nhttps://ai.googleblog.com/2017/04/federated-learning-collaborative.html\nFederated Learning allows for smarter models, lower latency, and less power consumption, all while ensuring privacy.","timestamp":"1626752220.0","comment_id":"410002","poster":"celia20200410"},{"content":"D - Federated learning and differential privacy are the key features for this regulated data","comments":[{"content":"Given the nature of insurance approval process which always applies to an individual, I dont think differential privacy mechanism is suitable here","timestamp":"1627544280.0","poster":"sensev","comment_id":"416700","upvote_count":"3"}],"upvote_count":"1","comment_id":"402441","timestamp":"1625809260.0","poster":"ralf_cc"}],"unix_timestamp":1623078240,"answer_ET":"B","isMC":true,"timestamp":"2021-06-07 17:04:00","question_id":190,"question_images":[],"url":"https://www.examtopics.com/discussions/google/view/54826-exam-professional-machine-learning-engineer-topic-1-question/"}],"exam":{"name":"Professional Machine Learning Engineer","id":13,"provider":"Google","lastUpdated":"11 Apr 2025","isBeta":false,"numberOfQuestions":304,"isMCOnly":true,"isImplemented":true},"currentPage":38},"__N_SSP":true}