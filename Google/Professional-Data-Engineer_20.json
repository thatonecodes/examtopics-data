{"pageProps":{"questions":[{"id":"eD25nfkCWfRTdSiMZQiD","answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/79599-exam-professional-data-engineer-topic-1-question-185/","choices":{"B":"Exceptions in worker code","A":"Job validation","C":"Graph or pipeline construction","D":"Insufficient permissions"},"answers_community":["B (100%)"],"timestamp":"2022-09-02 22:47:00","discussion":[{"content":"Selected Answer: B\nB. Exceptions in worker code\n\nWhile your job is running, you might encounter errors or exceptions in your worker code. These errors generally mean that the DoFns in your pipeline code have generated unhandled exceptions, which result in failed tasks in your Dataflow job.\n\nExceptions in user code (for example, your DoFn instances) are reported in the Dataflow monitoring interface.\n\nReference (Lists all answer choices and when to pick each one):\nhttps://cloud.google.com/dataflow/docs/guides/troubleshooting-your-pipeline#Causes","timestamp":"1677797220.0","upvote_count":"13","poster":"AWSandeep","comment_id":"657833"},{"content":"Selected Answer: B\nB is the answer.\n\nhttps://cloud.google.com/dataflow/docs/guides/troubleshooting-your-pipeline#detect_an_exception_in_worker_code\nWhile your job is running, you might encounter errors or exceptions in your worker code. These errors generally mean that the DoFns in your pipeline code have generated unhandled exceptions, which result in failed tasks in your Dataflow job.\n\nExceptions in user code (for example, your DoFn instances) are reported in the Dataflow monitoring interface.","timestamp":"1685358780.0","comment_id":"730418","poster":"zellck","upvote_count":"6"},{"timestamp":"1718957880.0","poster":"MaxNRG","content":"Selected Answer: B\nThe most likely cause of the errors you're experiencing in Dataflow, particularly if they are related to a particular DoFn (Dataflow's parallel processing operation), is B. Exceptions in worker code.\nWhen a Dataflow job processes a few elements successfully before failing, it suggests that the overall job setup, permissions, and pipeline graph are likely correct, as the job was able to start and initially process data. However, if it fails during execution and the errors are associated with a specific DoFn, this points towards issues in the code that executes within the workers. This could include:\n1. Runtime exceptions in the code logic of the DoFn.\n2. Issues handling specific data elements that might not be correctly managed by the DoFn code (e.g., unexpected data formats, null values, etc.).\n3. Resource constraints or timeouts if the DoFn performs operations that are resource-intensive or long-running.","upvote_count":"2","comments":[{"timestamp":"1718957880.0","poster":"MaxNRG","content":"To resolve these issues, you should:\n1. Inspect the stack traces and error messages in the Dataflow monitoring interface for details on the exception.\n2. Test the DoFn with a variety of data inputs, especially edge cases, to ensure robust error handling.\n3. Review the resource usage and performance characteristics of the DoFn if the issue is related to resource constraints.","comment_id":"1102354","upvote_count":"2"}],"comment_id":"1102353"},{"content":"Selected Answer: B\nA. Job validation - since it started successfully, it must have been validated.\nB. Exceptions in worker code - possible\nC. Graph or pipeline construction - same as A.\nD. Insufficient permissions - no elements to say that, and it should led to invalidation.","poster":"vaga1","upvote_count":"3","comment_id":"900183","timestamp":"1700234640.0"},{"timestamp":"1684895760.0","poster":"Atnafu","content":"C\nCode error","upvote_count":"1","comment_id":"725555"},{"poster":"pluiedust","upvote_count":"2","timestamp":"1678676940.0","content":"Selected Answer: B\nB is correct","comment_id":"667563"},{"poster":"ducc","upvote_count":"1","timestamp":"1677899880.0","comment_id":"658884","content":"Selected Answer: B\nB is correct"}],"answer_ET":"B","answer_description":"","question_images":[],"isMC":true,"unix_timestamp":1662151620,"question_text":"You issue a new batch job to Dataflow. The job starts successfully, processes a few elements, and then suddenly fails and shuts down. You navigate to the\nDataflow monitoring interface where you find errors related to a particular DoFn in your pipeline. What is the most likely cause of the errors?","topic":"1","question_id":96,"answer":"B","exam_id":11},{"id":"VaptAvAYlLVVtSGN641n","exam_id":11,"answer":"A","discussion":[{"comment_id":"657839","upvote_count":"8","poster":"AWSandeep","timestamp":"1677797880.0","content":"A. Do daily exports of Cloud Logging data to BigQuery. Create views filtering by project, log type, resource, and user.\n\nYou cannot import custom or filtered billing criteria into BigQuery. There are three types of Cloud Billing data tables with a fixed schema that must further drilled-down via BigQuery views.\n\nReference:\nhttps://cloud.google.com/billing/docs/how-to/export-data-bigquery#setup"},{"comment_id":"1102355","timestamp":"1718958360.0","content":"Selected Answer: A\nFor generating daily reports that show net consumption of Google Cloud compute resources and user details, the most efficient approach would be:\n\nA. Do daily exports of Cloud Logging data to BigQuery. Create views filtering by project, log type, resource, and user.","comments":[{"comment_id":"1102356","upvote_count":"1","comments":[{"poster":"MaxNRG","upvote_count":"1","comment_id":"1102357","content":"Option B (exporting data in CSV format) and Option D (using Cloud Storage and Dataprep) are less efficient due to the additional steps and manual handling involved. Option C is similar to A but lacks the specificity of creating views directly in BigQuery for filtering, which is a more streamlined approach.","timestamp":"1718958360.0"}],"content":"Here's why this option is the most effective:\n\nIntegration with BigQuery: BigQuery is a powerful tool for analyzing large datasets. By exporting Cloud Logging data directly to BigQuery, you can leverage its fast querying capabilities and advanced analysis features.\n\nAutomated Daily Exports: Setting up automated daily exports to BigQuery streamlines the reporting process, ensuring that data is consistently and efficiently transferred.\n\nCreating Views for Specific Filters: By creating views in BigQuery that filter data by project, log type, resource, and user, you can tailor the reports to the specific needs of your customer. Views also simplify repeated analysis by encapsulating complex SQL queries.\n\nEfficiency and Scalability: This method is highly efficient and scalable, handling large volumes of data without the manual intervention required for CSV exports and data cleansing.","poster":"MaxNRG","timestamp":"1718958360.0"}],"upvote_count":"1","poster":"MaxNRG"},{"comments":[{"comment_id":"1096606","poster":"Aman47","content":"Option C","upvote_count":"1","timestamp":"1718372580.0"}],"comment_id":"1096604","timestamp":"1718372520.0","content":"You can choose a sink in which you want Cloud logging to continously send Logging data. You can choose which columns you want to see (filter).","upvote_count":"1","poster":"Aman47"},{"comment_id":"900187","poster":"vaga1","upvote_count":"3","timestamp":"1700234940.0","content":"Selected Answer: A\nB, C, D do no generate a daily scalable solution."},{"upvote_count":"2","content":"Selected Answer: C\nI see A as quite inefficient as you are exporting ALL logs (hundreds of thousands) to bq and the filtering them with views. I would go for C, assuming that it does not involve doing it manually but rather creating a SINK with the correct filters and then using BQ Dataset as sink destination. But a lot of assumptions are taking place here as I believe the questions does not provide much context.","timestamp":"1698847200.0","comment_id":"886188","poster":"Siant_137"},{"comment_id":"845442","poster":"midgoo","timestamp":"1695254340.0","content":"Selected Answer: A\nI almost got it wrong by choosing C. By doing C, that means we will manually filter first one by one. We should just import them all and filter using BigQuery","upvote_count":"2"},{"comment_id":"788118","upvote_count":"4","poster":"maci_f","content":"Selected Answer: A\nB and D do not consider the log type field. \nC looks good and I would go for it.\nHowever, A looks equally good and I've found a CloudSkillsBoost lab that is exactly describing what answer A does, i.e. exporting logs to BQ and then creating a VIEW. https://www.cloudskillsboost.google/focuses/6100?parent=catalog I think the advantage of exporting complete logs (i.e. filtering them after they reach BQ) is that in case we would want to adjust the reporting in the future, we would have the complete logs with all fields available, whereas with C we would need to take extra steps.","timestamp":"1690309980.0"},{"upvote_count":"2","comment_id":"747748","timestamp":"1686967440.0","content":"Selected Answer: A\nA is the answer.","poster":"zellck"},{"comment_id":"725576","comments":[{"upvote_count":"1","content":"You need to quickly and efficiently generate these daily reports by using Materialized view /View\nA materialized view is the best solution and having filtered value with a view is good solution so A is an answer","poster":"Atnafu","comment_id":"747529","timestamp":"1686936600.0"}],"timestamp":"1684899540.0","upvote_count":"1","poster":"Atnafu","content":"A\nBad exporting data in csv or json due lack of some data\nso export is best practice\n1:10 \nhttps://www.youtube.com/watch?v=ZyMO9XabUUM"},{"comment_id":"722734","upvote_count":"1","poster":"hauhau","timestamp":"1684589760.0","content":"Selected Answer: A\nA because you filter data daily by view not just once by cloud logging"},{"timestamp":"1680718860.0","upvote_count":"3","comments":[{"timestamp":"1681738260.0","comment_id":"697429","content":"2nd tought: Definitely A. If you go to google documentation for export billing, you see a message that \"Exporting to JSON or CSV is obsolet. Use Big Query instead\". \nAlso why A? Look\nhttps://cloud.google.com/billing/docs/how-to/export-data-bigquery\nhttps://cloud.google.com/billing/docs/how-to/bq-examples#total-costs-on-invoice\nYou can make a fast report template al Data Studio that read a Big Query view.","upvote_count":"5","poster":"devaid","comments":[{"upvote_count":"1","content":"A comment regarding the links you provided (and not the correctness of the selected answer).\nUsing Cloud Billing is something different than detecting compute consumption data from Cloud Logging.\nIn fact, manual exporting to CSV (and JSON) is possible through the Logs Explorer interface (I think without user data break-down):\n🔗 https://cloud.google.com/logging/docs/view/logs-explorer-interface#download_logs","poster":"NicolasN","timestamp":"1684572000.0","comment_id":"722541"}]}],"comment_id":"687125","content":"Selected Answer: A\nA. The D isn't filtering by log type. B and C are discarded because you need to drill down the exported loggs in Big Query or other.","poster":"devaid"},{"timestamp":"1680102720.0","content":"Selected Answer: D\nThe Google Cloud Storage bucket where you would like your reports to be delivered.\n\nYou can select any Cloud Storage bucket for which you are an owner, including buckets that are from different projects. This bucket must exist before you can start exporting reports and you must have owner access to the bucket. Google Cloud Storage charges for usage, so you should review the Cloud Storage pricesheet for information on how you might incur charges for the service.\n\nhttps://cloud.google.com/compute/docs/logging/usage-export","poster":"AHUI","comment_id":"682797","upvote_count":"1"},{"content":"Ans is C\n\nhttps://cloud.google.com/logging/docs/export/aggregated_sinks\nD isn't correct because Cloud storage is used as a sink when logs are in json format not csv. https://cloud.google.com/logging/docs/export/aggregated_sinks#supported-destinations","timestamp":"1678696620.0","upvote_count":"4","poster":"TNT87","comment_id":"667719","comments":[{"content":"The question explicitly mentions daily generation of data so this would highlight, B and C seems that it is only suggesting a one-off filtering","comments":[{"timestamp":"1689072600.0","comment_id":"772500","content":"so wjhaytsa your argument about daily generartion of data??","upvote_count":"1","poster":"TNT87"}],"comment_id":"718001","upvote_count":"1","timestamp":"1684065720.0","poster":"jkhong"},{"comment_id":"667721","content":"On the other hand Ans A makes sense \nhttps://cloud.google.com/logging/docs/export/bigquery#overview","poster":"TNT87","upvote_count":"1","timestamp":"1678696740.0"}]},{"timestamp":"1678682460.0","upvote_count":"2","content":"Selected Answer: D\nQuickly and efficiently! It's a flag to guide to DataPrep. And importing data to Bigquery does not mean a report.","poster":"changsu","comment_id":"667596"},{"content":"Why not C?","upvote_count":"1","timestamp":"1678678020.0","poster":"pluiedust","comment_id":"667571"},{"upvote_count":"1","poster":"Wasss123","comment_id":"667438","content":"why not D ?","timestamp":"1678659600.0"},{"upvote_count":"1","content":"Challenging. B is right one but with B you do not automate wich makes it hard, with A you ensure automation but there is no SQL support being mentioned which also makes me think that A is not the best choice.","poster":"Remi2021","timestamp":"1678556520.0","comment_id":"666246"},{"content":"B is possible? https://cloud.google.com/logging/docs/view/logs-explorer-interface?cloudshell=true","poster":"John_Pongthorn","timestamp":"1678442220.0","upvote_count":"1","comment_id":"665226"},{"comment_id":"665218","content":"Is it Possible to B ? This is the Easy way","upvote_count":"1","timestamp":"1678441620.0","poster":"John_Pongthorn"},{"timestamp":"1677830460.0","comment_id":"658116","content":"Selected Answer: A\nA seem good for me","poster":"ducc","upvote_count":"2"}],"topic":"1","unix_timestamp":1662152280,"url":"https://www.examtopics.com/discussions/google/view/79603-exam-professional-data-engineer-topic-1-question-186/","answer_description":"","question_text":"Your new customer has requested daily reports that show their net consumption of Google Cloud compute resources and who used the resources. You need to quickly and efficiently generate these daily reports. What should you do?","answers_community":["A (78%)","13%","9%"],"question_id":97,"timestamp":"2022-09-02 22:58:00","question_images":[],"answer_ET":"A","answer_images":[],"isMC":true,"choices":{"D":"Export Cloud Logging data to Cloud Storage in CSV format. Cleanse the data using Dataprep, filtering by project, resource, and user.","C":"Filter data in Cloud Logging by project, log type, resource, and user, then import the data into BigQuery.","B":"Filter data in Cloud Logging by project, resource, and user; then export the data in CSV format.","A":"Do daily exports of Cloud Logging data to BigQuery. Create views filtering by project, log type, resource, and user."}},{"id":"hXBN4FmezaYGUcfZS2cd","question_text":"The Development and External teams have the project viewer Identity and Access Management (IAM) role in a folder named Visualization. You want the\nDevelopment Team to be able to read data from both Cloud Storage and BigQuery, but the External Team should only be able to read data from BigQuery. What should you do?\n//IMG//","answer_ET":"D","exam_id":11,"answers_community":["D (78%)","13%","9%"],"choices":{"A":"Remove Cloud Storage IAM permissions to the External Team on the acme-raw-data project.","C":"Create a VPC Service Controls perimeter containing both projects and BigQuery as a restricted API. Add the External Team users to the perimeter's Access Level.","D":"Create a VPC Service Controls perimeter containing both projects and Cloud Storage as a restricted API. Add the Development Team users to the perimeter's Access Level.","B":"Create Virtual Private Cloud (VPC) firewall rules on the acme-raw-data project that deny all ingress traffic from the External Team CIDR range."},"answer_images":[],"answer":"D","isMC":true,"url":"https://www.examtopics.com/discussions/google/view/79604-exam-professional-data-engineer-topic-1-question-187/","discussion":[{"poster":"AWSandeep","timestamp":"1677798120.0","content":"Selected Answer: D\nD. Create a VPC Service Controls perimeter containing both projects and Cloud Storage as a restricted API. Add the Development Team users to the perimeter's Access Level.\nReveal Solution","comments":[{"content":"no, https://cloud.google.com/blog/products/serverless/cloud-run-gets-enterprise-grade-network-security-with-vpc-sc?utm_source=youtube&utm_medium=unpaidsoc&utm_campaign=CDR_pri_gcp_m0v4tedeiao_ThisWeekInCloud_082621&utm_content=description","upvote_count":"1","comments":[{"comment_id":"894087","poster":"Oleksandr0501","content":"damn, i am confused anyway. Can be D.","timestamp":"1699634160.0","upvote_count":"1","comments":[{"timestamp":"1699634460.0","poster":"Oleksandr0501","comment_id":"894089","content":"should be D, as i think now, because we create a \"magic bulb\" around around Cloud storage and Dev team, and it will be protected from external influence like a human cell. Meantime Dev team will still be able to acess Bigquery. But external team will not manage to access Cloud storage.","upvote_count":"1"}]}],"timestamp":"1699634040.0","comment_id":"894084","poster":"Oleksandr0501"},{"poster":"TNT87","comment_id":"675777","timestamp":"1679472840.0","content":"WHy do you have to put the development team at the access perimeter???","upvote_count":"1"}],"upvote_count":"16","comment_id":"657840"},{"poster":"maci_f","content":"Selected Answer: D\n\"The grouping of GCP Project(s) and Service API(s) in the Service Perimeter result in restricting unauthorized access outside of the Service Perimeter to Service API endpoint(s) referencing resources inside of the Service Perimeter.\" \nhttps://scalesec.com/blog/vpc-service-controls-in-plain-english/\n\nDevelopment team: needs to access both Cloud Storage and BQ -> therefore we put the Development team inside a perimeter so it can access both the Cloud Storage and the BQ\nExternal team: allowed to access only BQ -> therefore we put Cloud Storage behind the restricted API and leave the external team outside of the perimeter, so it can access BQ, but is prohibited from accessing the Cloud Storage","timestamp":"1690313220.0","upvote_count":"10","comment_id":"788148"},{"content":"Selected Answer: A\nIt is not a network issue but a IAM permissions issue.\n https://cloud.google.com/iam/docs/deny-overview#inheritance","upvote_count":"4","comment_id":"1214656","timestamp":"1732162200.0","poster":"josech"},{"comment_id":"1096616","poster":"Aman47","content":"Comments are saying it correct its C","timestamp":"1718373480.0","upvote_count":"1"},{"content":"It's D for sure","comment_id":"1011167","timestamp":"1710849120.0","upvote_count":"1","poster":"Mamko"},{"upvote_count":"5","content":"A - Simple and straight forward","poster":"techabhi2_0","comment_id":"999904","timestamp":"1709679360.0"},{"content":"Why not B, I think CD will cause one of the team can not reach one or two of those DBs. A is not correct either","comment_id":"978685","poster":"wan2three","timestamp":"1707667500.0","upvote_count":"1"},{"poster":"[Removed]","content":"Selected Answer: D\nD. VPC Service Controls can create a service perimeter and define a restrictive API (service to protect). In this case, two projects are inside the perimeter and Cloud Storage is defined as the restrictive API. This means only services running on these two projects can access the Cloud Storage. And to allow users to access the Cloud Storage, they need have the access to the service perimeter. Hence, D is the correct answer.","upvote_count":"4","comment_id":"976418","timestamp":"1707477720.0"},{"comment_id":"886142","content":"Selected Answer: C\nC is correct","timestamp":"1698844500.0","upvote_count":"1","poster":"izekc"},{"content":"Selected Answer: D\nD sounds more correct, but if the project is already in the Service Control, would External people can access the BigQuery dataset in that project?","upvote_count":"1","comment_id":"845462","poster":"midgoo","timestamp":"1695256980.0"},{"comment_id":"779107","poster":"[Removed]","timestamp":"1689606540.0","upvote_count":"4","content":"seriously why u guys use VPC? the question never mentioned VPN or Interconnect, how can on-premise use VPC?\n\nA is the answer."},{"comment_id":"730408","content":"Selected Answer: D\nD is the answer.","poster":"zellck","comments":[{"poster":"TNT87","comments":[{"content":"you might be correct. C.\nhttps://cloud.google.com/blog/products/serverless/cloud-run-gets-enterprise-grade-network-security-with-vpc-sc?utm_source=youtube&utm_medium=unpaidsoc&utm_campaign=CDR_pri_gcp_m0v4tedeiao_ThisWeekInCloud_082621&utm_content=description\nhttps://www.youtube.com/watch?v=ABlY7FexJJI&ab_channel=GoogleCloudTech","comment_id":"894086","poster":"Oleksandr0501","timestamp":"1699634100.0","upvote_count":"1"}],"content":"Answer C, i dnt know if you have studied cloud security? if you have you will know","comment_id":"772494","timestamp":"1689072480.0","upvote_count":"1"}],"timestamp":"1685358180.0","upvote_count":"1"},{"upvote_count":"2","comments":[{"comment_id":"725602","poster":"Atnafu","timestamp":"1684903020.0","content":"I meant D Not C","upvote_count":"2"}],"content":"c\nExtend perimeters to authorized VPN or Cloud Interconnect\nYou can configure private communication to Google Cloud resources from VPC networks that span hybrid environments with Private Google Access on-premises extensions. A VPC network must be part of a service perimeter for VMs on that network to privately access managed Google Cloud resources within that service perimeter.\nhttps://cloud.google.com/vpc-service-controls/docs/overview#internet","comment_id":"725601","timestamp":"1684902840.0","poster":"Atnafu"},{"comment_id":"712689","content":"Selected Answer: D\nD makes the most sense to me","timestamp":"1683406740.0","poster":"cloudmon","comments":[{"content":"Because \"You want the\nDevelopment Team to be able to read data from both Cloud Storage and BigQuery, but the External Team should only be able to read data from BigQuery.\"","poster":"cloudmon","comment_id":"712690","upvote_count":"2","comments":[{"poster":"cloudmon","timestamp":"1683406860.0","content":"Therefore, Cloud Storage should be the restricted API, and you add the Development Team users to the perimeter's Access Level to allow them to access the restricted API.","upvote_count":"3","comment_id":"712692"}],"timestamp":"1683406800.0"}],"upvote_count":"4"},{"timestamp":"1683282600.0","upvote_count":"1","comment_id":"711743","poster":"yu_","content":"why C?\nI thought the development team would not be able to access BigQuery as I would include BigQuery in the service perimeter and add External Team to the access level","comments":[{"content":"Exactly, why would we need to consider BigQuery as a restricted service when it can already be accessed by both Dev and External team. The restricted service we are concerned with is Cloud Storage. If we go with C, we are only adding the external team into the access level... this means that the development team still wouldn't be able to access it","poster":"jkhong","upvote_count":"2","comment_id":"717989","timestamp":"1684065120.0"}]},{"timestamp":"1683017400.0","upvote_count":"1","content":"Selected Answer: C\nAnswer C\nhttps://cloud.google.com/vpc-service-controls/docs/overview#isolate","comment_id":"709746","poster":"josrojgra"},{"comment_id":"688301","poster":"TNT87","timestamp":"1680843300.0","content":"Selected Answer: C\nAnswer C\nhttps://cloud.google.com/vpc-service-controls/docs/overview#isolate","upvote_count":"4"},{"upvote_count":"6","comment_id":"663329","poster":"Wasss123","timestamp":"1678268520.0","content":"Shoud be C \nhttps://cloud.google.com/vpc-service-controls/docs/vpc-accessible-services\n\nWhen configuring VPC accessible services for a perimeter, you can specify a list of individual services, as well as include the RESTRICTED-SERVICES value, which automatically includes all of the services protected by the perimeter.\nTo ensure access to the expected services is fully limited, you must:\n Configure the perimeter to protect the same set of services that you want to make accessible.\n Configure VPCs in the perimeter to use the restricted VIP.\n Use layer 3 firewalls."},{"content":"Ans C\nhttps://cloud.google.com/vpc-service-controls/docs/overview#isolate","poster":"TNT87","timestamp":"1678261020.0","upvote_count":"2","comment_id":"663162"},{"timestamp":"1677991200.0","comment_id":"659682","upvote_count":"3","content":"Vote C\nhttps://cloud.google.com/vpc-service-controls/docs/vpc-accessible-services","poster":"nwk"},{"timestamp":"1677814620.0","comment_id":"657955","content":"A is more straight forward\nAnyone can confirm?","poster":"ducc","comments":[{"content":"A. can not be correct. Roles are always inherited, and there is no way to explicitly remove a permission for a lower-level resource that is granted at a higher level in the resource hierarchy. Given the above example, even if you were to remove the Project Editor role from Bob on the \"Test project\", he would still inherit that role from the \"Department Y\" folder, so he would still have the permissions for that role on \"Test project\".\nhttps://cloud.google.com/resource-manager/docs/cloud-platform-resource-hierarchy","comment_id":"659137","poster":"soichirokawa","timestamp":"1677928440.0","upvote_count":"8"}],"upvote_count":"1"}],"answer_description":"","unix_timestamp":1662152520,"question_images":["https://www.examtopics.com/assets/media/exam-media/04341/0012100001.jpg"],"question_id":98,"timestamp":"2022-09-02 23:02:00","topic":"1"},{"id":"nXI6CRLFhWXNSQMxIH1P","question_text":"Your startup has a web application that currently serves customers out of a single region in Asia. You are targeting funding that will allow your startup to serve customers globally. Your current goal is to optimize for cost, and your post-funding goal is to optimize for global presence and performance. You must use a native\nJDBC driver. What should you do?","exam_id":11,"answer_ET":"A","answers_community":["A (66%)","D (34%)"],"choices":{"D":"Use a Cloud SQL for PostgreSQL zonal instance first, and Cloud SQL for PostgreSQL with highly available configuration after securing funding.","B":"Use a Cloud SQL for PostgreSQL highly available instance first, and Bigtable with US, Europe, and Asia replication after securing funding.","C":"Use a Cloud SQL for PostgreSQL zonal instance first, and Bigtable with US, Europe, and Asia after securing funding.","A":"Use Cloud Spanner to configure a single region instance initially, and then configure multi-region Cloud Spanner instances after securing funding."},"answer_images":[],"answer":"A","isMC":true,"url":"https://www.examtopics.com/discussions/google/view/79606-exam-professional-data-engineer-topic-1-question-188/","discussion":[{"content":"Selected Answer: A\nA. Use Cloud Spanner to configure a single region instance initially, and then configure multi-region Cloud Spanner instances after securing funding.\n\nWhen you create a Cloud Spanner instance, you must configure it as either regional (that is, all the resources are contained within a single Google Cloud region) or multi-region (that is, the resources span more than one region). \n\nYou can change the instance configuration to multi-regional (or global) at anytime.","upvote_count":"11","comment_id":"657843","timestamp":"1662152760.0","poster":"AWSandeep"},{"timestamp":"1682939820.0","content":"Selected Answer: D\nAlthough A is good, but concerning about the cost. Then D will be much more suitable","poster":"izekc","upvote_count":"9","comment_id":"886148"},{"timestamp":"1730140320.0","comment_id":"1304103","poster":"SamuelTsch","content":"Selected Answer: D\nI go to D. Cloud SQL is usually used for web application (CRM) (https://cloud.google.com/blog/topics/developers-practitioners/your-google-cloud-database-options-explained?hl=en)","upvote_count":"1"},{"poster":"4a8ffd7","upvote_count":"1","content":"Selected Answer: D\nAlthough A is good, but concerning about the cost. Then D will be much more suitable","comment_id":"1285930","timestamp":"1726679400.0"},{"content":"Selected Answer: D\nAlthough A is good, but concerning about the cost. Then D will be much more suitable","timestamp":"1712355900.0","upvote_count":"1","comments":[{"comment_id":"1336818","timestamp":"1736096220.0","content":"While a zonal instance is cost-effective, transitioning to a highly available Cloud SQL instance does not support global replication. Cloud SQL lacks the scalability and global presence needed for your post-funding goals.\n\nSo I beleive Spanner is the right answer","poster":"Ronn27","upvote_count":"2"}],"poster":"CGS22","comment_id":"1190126"},{"comment_id":"1123221","upvote_count":"1","comments":[{"upvote_count":"1","poster":"Ronn27","content":"While a zonal instance is cost-effective, transitioning to a highly available Cloud SQL instance does not support global replication. Cloud SQL lacks the scalability and global presence needed for your post-funding goals.","timestamp":"1736096160.0","comment_id":"1336817"}],"content":"Selected Answer: D\nI think is D \n\nThe best for Web app is Cloud SQL, and Spanner is the best for data more than 30GB","poster":"tibuenoc","timestamp":"1705310820.0"},{"poster":"MaxNRG","timestamp":"1703156580.0","upvote_count":"6","content":"Selected Answer: A\nA - This option allows for optimization for cost initially with a single region Cloud Spanner instance, and then optimization for global presence and performance after funding with multi-region instances.\nCloud Spanner supports native JDBC drivers and is horizontally scalable, providing very high performance. A single region instance minimizes costs initially. After funding, multi-region instances can provide lower latency and high availability globally.\nCloud SQL does not scale as well and has higher costs for multiple high availability regions. Bigtable does not support JDBC drivers natively. Therefore, Spanner is the best choice here for optimizing both for cost initially and then performance and availability globally post-funding.","comment_id":"1102375"},{"poster":"lucaluca1982","comment_id":"847809","content":"Spanner has some limitations with JDBC. Maybe the quetion wants to help us tp choose Cloud SQL","timestamp":"1679546400.0","upvote_count":"3"},{"upvote_count":"1","poster":"musumusu","comment_id":"821030","content":"Answer D:\nCost effective transactional database Cloud SQL. Spanner is good case for data more than 30 GB","timestamp":"1677282660.0"},{"comment_id":"739961","content":"Selected Answer: A\nB and C has no sense because of the driver.\nD looks like a good option, but HA it's not to improve performance or global presence:\nThe purpose of an HA configuration is to reduce downtime when a zone or instance becomes unavailable. This might happen during a zonal outage, or when an instance runs out of memory. With HA, your data continues to be available to client applications.\nSo the best option is A.","timestamp":"1670578500.0","upvote_count":"7","poster":"odacir"},{"content":"Selected Answer: A\nhttps://cloud.google.com/spanner/docs/jdbc-drivers\nAns A\nhttps://cloud.google.com/spanner/docs/instance-configurations#tradeoffs_regional_versus_multi-region_configurations\nThe last part of the question makes it easy","upvote_count":"5","comment_id":"675780","poster":"TNT87","timestamp":"1663827480.0"},{"poster":"TNT87","upvote_count":"3","comments":[{"comment_id":"669579","timestamp":"1663225980.0","content":"https://cloud.google.com/spanner/docs/instance-configurations#tradeoffs_regional_versus_multi-region_configurations\nAns A","poster":"TNT87","upvote_count":"1"}],"timestamp":"1663049220.0","content":"Yes Spanner is expensive , but the question expresslty states that \"after securing funding you want to have a global presence\" the word globally is repeatedly stated there. \nAnswer is A.","comment_id":"667702"},{"upvote_count":"3","poster":"badrisrinivas9","content":"Selected Answer: D\nSpanner is expensive, they haven't mentioned the size of db... optimize for cost then option is Cloud SQL which cost effective and highly available in case of multi region.","comment_id":"666660","timestamp":"1662964500.0"},{"comment_id":"664270","upvote_count":"3","timestamp":"1662703200.0","content":"Selected Answer: A\nA is the best option. It is globally scalable and it also meets the cost goal as it says that initially it will be configurated as single region wich is cheaper than multi region.","poster":"Quevedo"},{"content":"Selected Answer: D\nSpanner is expensive can't be A\n\nWould choose D","comment_id":"661105","timestamp":"1662461220.0","poster":"YorelNation","comments":[{"content":"Actually maybe C as you don't really need relational database for a webapp and BigTable is super performant and highly available","upvote_count":"1","comment_id":"661114","comments":[{"content":"no its A","poster":"TNT87","upvote_count":"1","comment_id":"688297","timestamp":"1665118380.0"}],"timestamp":"1662461580.0","poster":"YorelNation"},{"timestamp":"1663226160.0","poster":"TNT87","upvote_count":"2","comment_id":"669586","content":"The fact that its global cloud spanner is the answer. Secondly Option D, the fact that it has to be highly avaible and multi regional its already more expensive than Cloud spanner Regional instance. https://cloud.google.com/spanner/docs/instance-configurations#tradeoffs_regional_versus_multi-region_configurations"}],"upvote_count":"2"},{"comment_id":"657953","timestamp":"1662168960.0","poster":"ducc","content":"Selected Answer: A\nSpanner still support JDBC\nhttps://cloud.google.com/spanner/docs/jdbc-drivers","upvote_count":"3"}],"answer_description":"","unix_timestamp":1662152760,"question_id":99,"question_images":[],"topic":"1","timestamp":"2022-09-02 23:06:00"},{"id":"fODLPGH0CPtygcldHqaN","unix_timestamp":1662153180,"answer":"A","answer_ET":"A","url":"https://www.examtopics.com/discussions/google/view/79608-exam-professional-data-engineer-topic-1-question-189/","question_images":[],"timestamp":"2022-09-02 23:13:00","topic":"1","isMC":true,"question_text":"You need to migrate 1 PB of data from an on-premises data center to Google Cloud. Data transfer time during the migration should take only a few hours. You want to follow Google-recommended practices to facilitate the large data transfer over a secure connection. What should you do?","answers_community":["A (63%)","B (37%)"],"question_id":100,"discussion":[{"content":"Selected Answer: A\nWell it doesn't mentions anything about not enough bandwidth to meet your project deadline. I guess you can assume they have 200GBps+ of bandwith, otherwise it shouldn't take only a few hours.","poster":"devaid","timestamp":"1665007560.0","upvote_count":"5","comment_id":"687264"},{"upvote_count":"3","comment_id":"1260427","timestamp":"1722714600.0","content":"One who wanted to use Transfer Appliance to migrate data in a few hours, you should live near Google office and run really fast :D","poster":"iooj"},{"comment_id":"1102380","poster":"MaxNRG","content":"Selected Answer: A\nCloud Interconnect provides a dedicated private connection between on-prem and Google Cloud for high bandwidth (up to 100 Gbps) and low latency. This facilitates large, fast data transfers.\nStorage Transfer Service supports parallel data transfers over Cloud Interconnect. It can transfer petabyte-scale datasets faster by transferring objects in parallel.\nStorage Transfer Service uses HTTPS encryption in transit and at rest by default for secure data transfers.\nIt follows Google-recommended practices for large data migrations vs ad hoc methods like gsutil or scp.\nThe other options would take too long for a 1 PB transfer (VPN capped at 3 Gbps, manual transfers) or introduce extra steps like batching and checksums. Cloud Interconnect + Storage Transfer is the recommended Google solution.","upvote_count":"2","timestamp":"1703156940.0"},{"poster":"LanaOjisan","timestamp":"1698555120.0","content":"It is believed that A.\nOne reason is that for \"secure\" and \"in a few hours,\" the communication can be done securely using a direct physical line without going through an ISP. Also, depending on the case, in the case of \"Dedicated Interconnect,\" the maximum transfer can be as high as 200 Gbps, and the fastest data transfer of 1 PB can be completed in 11 hours.\nTherefore, A.","comment_id":"1056584","upvote_count":"2"},{"poster":"arien_chen","upvote_count":"3","comment_id":"986614","timestamp":"1692628380.0","content":"Selected Answer: A\nA\nhttps://cloud.google.com/storage-transfer/docs/transfer-options#:~:text=Transferring%20more%20than%201%20TB%20from%20on%2Dpremises"},{"comment_id":"965216","timestamp":"1690510080.0","content":"Selected Answer: A\nDedicated Interconnect provides direct physical connections between your on-premises network and Google's network. Dedicated Interconnect enables you to transfer large amounts of data between networks, which can be more cost-effective than purchasing additional bandwidth over the public internet. https://cloud.google.com/network-connectivity/docs/interconnect/concepts/dedicated-overview","upvote_count":"1","comments":[{"comment_id":"965217","timestamp":"1690510200.0","upvote_count":"1","content":"This link has additional clarity\nhttps://cloud.google.com/network-connectivity/docs/interconnect/concepts/terminology","poster":"knith66"}],"poster":"knith66"},{"comment_id":"941626","content":"Selected Answer: B\n1 PB and \"few hours\". It is clearly referring to Transfer Appliance\n\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#time","poster":"vaga1","timestamp":"1688372640.0","upvote_count":"3","comments":[{"content":"Transfer Appliance is a slow process. wont be able to do in few hours","poster":"knith66","timestamp":"1690509960.0","upvote_count":"3","comment_id":"965215"}]},{"poster":"Oleksandr0501","content":"Selected Answer: A\ngpt: Based on security and speed, if the data is highly sensitive and security is the top priority, then option B (using a Transfer Appliance) may be a better choice. Transfer Appliance uses hardware encryption to transfer data and is designed to securely transfer large amounts of data. However, if speed is the primary concern, then option A (using Cloud Interconnect and Storage Transfer Service) may be a better choice as it allows for faster transfer speeds over a dedicated and secure connection. It ultimately depends on the specific needs and priorities of the organization.\n\n A vague treaky question. Bad author of it...\n B is also good. As were said in discuss. by smb, a question asks \"safe connection\", so - a Cloud Interconnect (A)","timestamp":"1682872200.0","upvote_count":"1","comment_id":"885438"},{"timestamp":"1679377020.0","comments":[{"poster":"spicebits","upvote_count":"1","timestamp":"1699323540.0","comment_id":"1064432","content":"B can't be the answer - You have to wait 25 days to receive the appliance and another 25 days to get the appliance back and data loaded to cloud storage: https://cloud.google.com/transfer-appliance/docs/4.0/overview#transfer-speeds"}],"poster":"midgoo","content":"Selected Answer: B\nEither this question is very tricky or very poor written. It says 'Data transfer time during the migration should take only a few hours'. We should not add the 20days for overhead time for Appliance into the total time of migration.\n\nIf 'a few hours' = 30hours or more, A will be good enough.\nIf 'a few hours' = 10 or less, B is the only way (with multiple devices to copy at the same time)","comment_id":"845552","upvote_count":"3"},{"upvote_count":"1","comments":[{"content":"it says data transfer during the migration. It mean from when the migration is \"activated\", which means from when the Transfer Appliace device is plugged and ready to be used","poster":"vaga1","comment_id":"941625","timestamp":"1688372520.0","upvote_count":"1"}],"timestamp":"1679214660.0","comment_id":"843554","content":"Selected Answer: A\nExpected time via transfer appliance is around 20 days , and achieving the same using Storage transfer service with highest bandwidth of 100GPS is 30 hrs, so hence its been asked for hrs .. its A\nAcquiring a Transfer Appliance is straightforward. In the Google Cloud console, you request a Transfer Appliance, indicate how much data you have, and then Google ships one or more appliances to your requested location. You're given a number of days to transfer your data to the appliance (\"data capture\") and ship it back to Google.\n\nThe expected turnaround time for a network appliance to be shipped, loaded with your data, shipped back, and rehydrated on Google Cloud is 20 days. If your online transfer timeframe is calculated to be substantially more than this timeframe, consider Transfer Appliance. The total cost for the 300 TB device process is less than $2,500.","poster":"Nandhu95"},{"poster":"wjtb","timestamp":"1678356600.0","upvote_count":"2","comment_id":"833826","content":"Selected Answer: B\nEven with 100gbps bandwith, you will not reach a data transfer time within the range of \"hours\" for 1PB. Transfer appliance is the way to go. https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#time"},{"timestamp":"1676812080.0","poster":"musumusu","comment_id":"814095","upvote_count":"1","content":"Answer A, \nOne time transfer is cheaper and less secure always using Transfer Appliance. \nyou need to do it in faster way, set up Interconnect speed limit is 50mbps - 10GBps\nand Transfer Appliance speed can goes up to 40GBps. \nI am choosing A for security concern only."},{"comment_id":"763412","timestamp":"1672619820.0","upvote_count":"1","content":"A is right","poster":"AzureDP900","comments":[{"content":"A. Establish a Cloud Interconnect connection between the on-premises data center and Google Cloud, and then use the Storage Transfer Service. Most Voted","poster":"AzureDP900","comment_id":"763413","upvote_count":"1","timestamp":"1672619880.0"}]},{"comment_id":"729449","content":"Selected Answer: A\nA is the answer.\n\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#storage-transfer-service-for-large-transfers-of-on-premises-data\nLike gsutil, Storage Transfer Service for on-premises data enables transfers from network file system (NFS) storage to Cloud Storage. Although gsutil can support small transfer sizes (up to 1 TB), Storage Transfer Service for on-premises data is designed for large-scale transfers (up to petabytes of data, billions of files).","upvote_count":"2","poster":"zellck","timestamp":"1669654920.0"},{"content":"B\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#:~:text=Few%20things%20in,not%20be%20obtained.","comments":[{"comment_id":"727125","timestamp":"1669417080.0","upvote_count":"4","poster":"Atnafu","content":"B\nIt takes 30hrs with 100Gbps bandwidth- more than a day to transfer\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#:~:text=addresses%20or%20NATs.-,Online%20versus%20offline%20transfer,A%20certain%20amount%20of%20management%20overhead%20is%20built%20into%20these%20calculations.,-As%20noted%20earlier"}],"comment_id":"727118","upvote_count":"2","poster":"Atnafu","timestamp":"1669415400.0"},{"poster":"pluiedust","comment_id":"667743","content":"Selected Answer: A\nA is correct.","upvote_count":"1","timestamp":"1663053060.0"},{"timestamp":"1662942300.0","comment_id":"666520","poster":"bigquery1102","content":"Selected Answer: A\nA is correct\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#transfer_appliance_for_larger_transfers","upvote_count":"1"},{"poster":"MounicaN","upvote_count":"1","timestamp":"1662916560.0","comment_id":"666307","content":"Selected Answer: A\nHuge data can be migrated over cloud interconnect"},{"upvote_count":"1","content":"1. Transfer over a secure connection. \n2. Should take only a few hours. \n\nHence, Storage Transfer Service instead of Transfer Appliance.","poster":"GyaneswarPanigrahi","timestamp":"1662820620.0","comment_id":"665479"},{"comment_id":"662114","content":"Answer A\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets","upvote_count":"1","timestamp":"1662533460.0","poster":"TNT87"},{"comment_id":"661125","content":"Selected Answer: A\nSame A","upvote_count":"1","timestamp":"1662462060.0","poster":"YorelNation"},{"comment_id":"659684","upvote_count":"4","poster":"nwk","content":"Vote A\nQuestions states \"transfer over a secure connection\" , not offline with Transfer Appliance","timestamp":"1662345960.0"},{"poster":"ducc","content":"Selected Answer: B\nI vote for B","timestamp":"1662168840.0","upvote_count":"2","comment_id":"657951"},{"comment_id":"657845","content":"Selected Answer: B\nB. Use a Transfer Appliance and have engineers manually encrypt, decrypt, and verify the data.","poster":"AWSandeep","upvote_count":"1","timestamp":"1662153180.0"}],"answer_description":"","choices":{"C":"Establish a Cloud VPN connection, start gcloud compute scp jobs in parallel, and run checksums to verify the data.","B":"Use a Transfer Appliance and have engineers manually encrypt, decrypt, and verify the data.","A":"Establish a Cloud Interconnect connection between the on-premises data center and Google Cloud, and then use the Storage Transfer Service.","D":"Reduce the data into 3 TB batches, transfer the data using gsutil, and run checksums to verify the data."},"answer_images":[],"exam_id":11}],"exam":{"lastUpdated":"11 Apr 2025","isBeta":false,"name":"Professional Data Engineer","isImplemented":true,"isMCOnly":true,"numberOfQuestions":319,"id":11,"provider":"Google"},"currentPage":20},"__N_SSP":true}