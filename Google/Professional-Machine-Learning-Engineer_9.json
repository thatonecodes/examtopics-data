{"pageProps":{"questions":[{"id":"pOfAH8UQTbShTnzDXq3K","answer_description":"","answer_images":[],"question_text":"You work for the AI team of an automobile company, and you are developing a visual defect detection model using TensorFlow and Keras. To improve your model performance, you want to incorporate some image augmentation functions such as translation, cropping, and contrast tweaking. You randomly apply these functions to each training batch. You want to optimize your data processing pipeline for run time and compute resources utilization. What should you do?","unix_timestamp":1670952660,"isMC":true,"answer":"A","url":"https://www.examtopics.com/discussions/google/view/91471-exam-professional-machine-learning-engineer-topic-1-question/","question_images":[],"answers_community":["A (83%)","B (17%)"],"timestamp":"2022-12-13 18:31:00","discussion":[{"content":"Selected Answer: A\nOption A: By embedding augmentation in the tf.data pipeline, data augmentation is applied on-the-fly during training, reducing the need to store pre-augmented data. Option B could be a choice, but since Keras generators are built on top of tf.data, they are less flexible and have a lower level of optimization compared to tf.data.","timestamp":"1722543480.0","comment_id":"1137961","upvote_count":"4","poster":"guilhermebutzke"},{"comment_id":"1068769","upvote_count":"2","timestamp":"1715531460.0","content":"Selected Answer: A\nA is best because, \n1. It allows you to apply the augmentations on-the-fly during training, which eliminates the need for pre-processing and storing a large number of augmented images. This saves both storage space and compute resources.\n2. The tf.Data pipeline is highly optimized for efficient data loading and processing, ensuring that your model training process is not bottlenecked by data preprocessing.\n3. By applying augmentations randomly to each training batch, you increase the diversity of your training data, which can help your model generalize better to unseen data.\n\nKeras generators can be used for data augmentation, but tf.Data pipelines are generally more efficient and flexible for creating complex data processing pipelines.","poster":"tavva_prudhvi"},{"content":"Selected Answer: B\nB (but also A)\n\nB is a common and efficient approach for applying data augmentation during training. This allows you to apply data augmentation on-the-fly without the need to pre-generate or store augmented images separately, which saves storage space and reduces the preprocessing time. Keras provides various tools and functions for data augmentation, and you can easily incorporate them into your training data pipeline.\n\nA can also be a good choice, especially if you are using TensorFlow's tf.data API for data loading and preprocessing. It can provide similar benefits by applying augmentations on-the-fly, but it may require more custom code to implement compared to Keras data generators.","upvote_count":"1","comment_id":"1068665","comments":[{"upvote_count":"1","timestamp":"1722543540.0","content":"Yes, but I think the questions says: \"You want to optimize your data processing pipeline for run time and compute resources utilization\". keras is not optmizated as tf.Data","comment_id":"1137963","poster":"guilhermebutzke"}],"timestamp":"1715524380.0","poster":"pico"},{"content":"by abylead: B) Keras generators embedded augmentation functions offers at least Translation, Crop, and Contrast preprocessing. You can either permanently integrate the functions or you randomly use a dataset with non CPU blocking async training batches & optimized GPU processing overlapping. By applying Keras embedded augmentation functions, the tf.data pipeline can still be performance optimized.\nWith tf.image pipelines you lack pipeline performance optimization & the deprecated translation function. In addition, the complex application hinders random operation flexibilty.","poster":"envest","comment_id":"976656","upvote_count":"1","timestamp":"1707489900.0"},{"content":"B - TensorFlow's Keras API provides built-in support for data augmentation using various image preprocessing layers, such as RandomTranslation, RandomCrop, and RandomContrast, among others. You can create custom image augmentation functions and include them as part of your Keras generators, tailoring them to your specific use case and needs.\n\nIn summary, Option B, embedding the augmentation functions dynamically as part of Keras generators, offers efficient on-the-fly data augmentation, reduced storage overhead, optimized resource utilization, and greater flexibility, making it the best choice for thisscenario.","comment_id":"956699","comments":[{"timestamp":"1706383560.0","poster":"tavva_prudhvi","content":"lthough Keras generators can be used for data augmentation, using the tf.data pipeline provides better performance and efficiency. The tf.data API is more flexible and better integrated with TensorFlow, allowing for more optimizations.especially if you have a large number of images to process.","comment_id":"964942","upvote_count":"2"}],"timestamp":"1705679520.0","poster":"PST21","upvote_count":"1"},{"timestamp":"1699638480.0","upvote_count":"1","poster":"M25","content":"Selected Answer: A\nWent with A","comment_id":"894129"},{"poster":"matamata415","upvote_count":"2","timestamp":"1696847580.0","comments":[{"content":"https://www.tensorflow.org/tutorials/load_data/images#using_tfdata_for_finer_control","poster":"matamata415","timestamp":"1696847700.0","upvote_count":"2","comment_id":"865426"}],"content":"Selected Answer: A\nhttps://www.tensorflow.org/tutorials/load_data/images?hl=ja#tfdata_%E3%82%92%E4%BD%BF%E7%94%A8%E3%81%97%E3%81%A6%E3%82%88%E3%82%8A%E7%B2%BE%E5%AF%86%E3%81%AB%E5%88%B6%E5%BE%A1%E3%81%99%E3%82%8B","comment_id":"865424"},{"poster":"Yajnas_arpohc","content":"Selected Answer: A\nhttps://towardsdatascience.com/time-to-choose-tensorflow-data-over-imagedatagenerator-215e594f2435","upvote_count":"1","timestamp":"1695613920.0","comment_id":"849817"},{"comment_id":"831870","timestamp":"1694082480.0","poster":"TNT87","upvote_count":"2","content":"Selected Answer: A\nA. Embed the augmentation functions dynamically in the tf.Data pipeline is the best approach to optimize the data processing pipeline for runtime and compute resource utilization.\n\nUsing the tf.data pipeline, you can apply data augmentation functions dynamically to each batch during training. This approach avoids the overhead of creating preprocessed TFRecords or Keras generators, which can consume additional disk space, memory, and CPU. Additionally, using the tf.data pipeline, you can parallelize data preprocessing, input pipeline operations, and model training"},{"comment_id":"820370","poster":"shankalman717","content":"Selected Answer: A\nEmbedding the augmentation functions dynamically in the tf.Data pipeline allows the data pipeline to apply the augmentations on the fly as the data is being loaded into the model during training. This means that the model can utilize the compute resources effectively by loading and processing the data as needed, rather than pre-generating all possible augmentations ahead of time (as in options C and D), which could be computationally expensive and time-consuming.\n\nOption B is also a viable choice, but it may not be as efficient as option A since the data augmentation functions would be applied during training using Keras generators, which could cause some overhead.","timestamp":"1692866940.0","upvote_count":"2"},{"comment_id":"788827","content":"Selected Answer: B\nwill go for B too\nhttps://www.analyticsvidhya.com/blog/2020/08/image-augmentation-on-the-fly-using-keras-imagedatagenerator/","upvote_count":"1","timestamp":"1690376220.0","poster":"pshemol"},{"content":"Either of A or B : I am not convinced of what the right answer is. but it is on https://www.tensorflow.org/tutorials/images/data_augmentation#apply_augmentation_to_a_dataset certainly","timestamp":"1690107060.0","upvote_count":"1","comment_id":"785322","poster":"John_Pongthorn"},{"comment_id":"753158","upvote_count":"1","poster":"hiromi","timestamp":"1687420140.0","content":"Selected Answer: A\nA (not sure)"},{"timestamp":"1686706440.0","comment_id":"744687","content":"Selected Answer: B\nwill go for B \nhttps://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly","upvote_count":"2","poster":"YangG"},{"upvote_count":"4","timestamp":"1686670260.0","content":"Selected Answer: A\nincorporating the augmentation functions into the pipeline, you can apply them dynamically to each training batch, without the need to generate all possible augmentations in advance or stage them as TFRecords.","poster":"mil_spyro","comment_id":"744313"}],"topic":"1","exam_id":13,"question_id":41,"answer_ET":"A","choices":{"B":"Embed the augmentation functions dynamically as part of Keras generators.","C":"Use Dataflow to create all possible augmentations, and store them as TFRecords.","A":"Embed the augmentation functions dynamically in the tf.Data pipeline.","D":"Use Dataflow to create the augmentations dynamically per training run, and stage them as TFRecords."}},{"id":"FxX5FiaXrtKtAQBmct6R","unix_timestamp":1670952540,"url":"https://www.examtopics.com/discussions/google/view/91469-exam-professional-machine-learning-engineer-topic-1-question/","question_id":42,"discussion":[{"comment_id":"831869","timestamp":"1694082420.0","upvote_count":"8","poster":"TNT87","content":"Selected Answer: C\nOption C is the best answer. Since all the information needed to compute the success metric is available in BigQuery and is updated hourly, scheduling a weekly query in BigQuery to compute the success metric is the simplest and most cost-effective way to monitor the model's performance. By comparing the computed success metric against the acceptable baseline, you can determine when the model's performance has degraded below the threshold, and retrain the model accordingly. This approach avoids the cost of additional monitoring infrastructure and leverages existing data processing capabilities."},{"comments":[{"timestamp":"1729576500.0","content":"A. Vertex AI Model Monitoring for feature skew: This monitors data drift, which can be helpful, but it doesn't directly address the success metric of article opens and dwell time.\nB. Cron job for weekly retraining: Retraining every week, regardless of performance, is excessive and costly, considering the 12-hour training time.\nD. Daily Dataflow job: While daily computation provides more data points, it might be overkill compared to a weekly check. Additionally, Cloud Composer adds complexity for a simple task.","comment_id":"1199989","upvote_count":"1","poster":"fitri001"}],"timestamp":"1729576440.0","comment_id":"1199988","poster":"fitri001","content":"Selected Answer: C\nWeekly checks are frequent enough to catch performance degradation before the next newsletter (5-week threshold).\nThe success metric can be directly calculated within the query, providing a clear indication for retraining.","upvote_count":"2"},{"content":"Selected Answer: C\nAs we have all the data in BigQuery","comment_id":"901618","timestamp":"1700371500.0","upvote_count":"2","poster":"julliet"},{"timestamp":"1699638600.0","comment_id":"894130","content":"Selected Answer: C\nWent with C","poster":"M25","upvote_count":"3"},{"upvote_count":"1","comment_id":"874077","content":"Selected Answer: A\nOption A because when using Vertex AI Model Monitoring, you can set up automated monitoring of the model's performance by detecting skew of the input features, which can help you identify any changes in the data distribution that may impact the model's performance. Setting the sample rate to 100% ensures that all incoming data is monitored, and a monitoring frequency of two days allows for timely detection of any deviations from the expected data distribution","timestamp":"1697660160.0","poster":"Antmal","comments":[{"timestamp":"1699827480.0","comment_id":"896228","poster":"Antmal","content":"I have changed my mind. I will choose C","upvote_count":"1"}]},{"upvote_count":"2","timestamp":"1692017700.0","poster":"John_Pongthorn","content":"Selected Answer: C\nThis question tweak from this article surely.\nhttps://cloud.google.com/blog/topics/developers-practitioners/continuous-model-evaluation-bigquery-ml-stored-procedures-and-cloud-scheduler","comment_id":"808483"},{"comment_id":"785336","timestamp":"1690108020.0","upvote_count":"2","poster":"John_Pongthorn","content":"The anwner is on here https://cloud.google.com/blog/topics/developers-practitioners/continuous-model-evaluation-bigquery-ml-stored-procedures-and-cloud-scheduler"},{"upvote_count":"1","comment_id":"753124","content":"Selected Answer: C\nC (not sure)","poster":"hiromi","timestamp":"1687418160.0"},{"upvote_count":"3","poster":"pshemol","timestamp":"1687345920.0","content":"Selected Answer: C\n\"All the information needed to compute the success metric is available in BigQuery\" and \"on average its performance degrades below the acceptable baseline after five weeks\"\nso once per week is enough to check models performance. And it's the cheapest solution too.","comment_id":"752329"},{"comment_id":"744311","poster":"mil_spyro","content":"Selected Answer: D\nThis can help to ensure that the model’s performance is above the baseline, while minimizing cost by avoiding unnecessary retraining.","timestamp":"1686670140.0","upvote_count":"1"}],"answer_images":[],"topic":"1","timestamp":"2022-12-13 18:29:00","answer_description":"","choices":{"D":"Schedule a daily Dataflow job in Cloud Composer to compute the success metric.","A":"Use Vertex AI Model Monitoring to detect skew of the input features with a sample rate of 100% and a monitoring frequency of two days.","C":"Schedule a weekly query in BigQuery to compute the success metric.","B":"Schedule a cron job in Cloud Tasks to retrain the model every week before the newsletter is created."},"question_text":"You work for an online publisher that delivers news articles to over 50 million readers. You have built an AI model that recommends content for the company’s weekly newsletter. A recommendation is considered successful if the article is opened within two days of the newsletter’s published date and the user remains on the page for at least one minute.\n\nAll the information needed to compute the success metric is available in BigQuery and is updated hourly. The model is trained on eight weeks of data, on average its performance degrades below the acceptable baseline after five weeks, and training time is 12 hours. You want to ensure that the model’s performance is above the acceptable baseline while minimizing cost. How should you monitor the model to determine when retraining is necessary?","answer_ET":"C","question_images":[],"answers_community":["C (91%)","4%"],"isMC":true,"answer":"C","exam_id":13},{"id":"64jKDdLjCL2gen5NmfOU","answer_ET":"D","question_id":43,"question_text":"You deployed an ML model into production a year ago. Every month, you collect all raw requests that were sent to your model prediction service during the previous month. You send a subset of these requests to a human labeling service to evaluate your model’s performance. After a year, you notice that your model's performance sometimes degrades significantly after a month, while other times it takes several months to notice any decrease in performance. The labeling service is costly, but you also need to avoid large performance degradations. You want to determine how often you should retrain your model to maintain a high level of performance while minimizing cost. What should you do?","unix_timestamp":1670953140,"answer_description":"","discussion":[{"upvote_count":"5","content":"Selected Answer: D\nOption D is the best approach to determine how often to retrain the model while minimizing cost. Running training-serving skew detection batch jobs every few days to compare the aggregate statistics of the features in the training dataset with recent serving data is an effective way to detect when the model's performance has degraded. If skew is detected, the most recent serving data should be sent to the labeling service to evaluate the model's performance. This approach is more cost-effective than sending a subset of requests to the labeling service every month because it only sends data when there is a high probability that the model's performance has degraded. By doing this, the model can be retrained at the right time, and the cost of the labeling service can be minimized.","poster":"TNT87","timestamp":"1678191840.0","comment_id":"831864"},{"content":"Selected Answer: B\nskew detection does not tell overall model performance and running too frequent is not good.","timestamp":"1740102840.0","comment_id":"1359590","poster":"NamitSehgal","upvote_count":"1"},{"comment_id":"1311673","upvote_count":"1","timestamp":"1731548640.0","poster":"f084277","content":"Selected Answer: D\nClearly D. B is just guesswork."},{"upvote_count":"2","content":"Selected Answer: D\nWent with D","poster":"M25","timestamp":"1683733800.0","comment_id":"894131"},{"timestamp":"1674316200.0","upvote_count":"2","comment_id":"783502","poster":"John_Pongthorn","content":"Selected Answer: D\nD https://cloud.google.com/blog/topics/developers-practitioners/monitor-models-training-serving-skew-vertex-aiew-vertex-ai&ved=2ahUKEwiRg_aoj9n8AhWb7TgGHcGCDREQFnoECAwQAQ&usg=AOvVaw197NneIJM0ra7fLq2zsOin"},{"poster":"ares81","timestamp":"1673085840.0","upvote_count":"2","content":"Selected Answer: B\nB looks the only option, to me.","comment_id":"768419"},{"comment_id":"753133","timestamp":"1671701340.0","upvote_count":"4","poster":"hiromi","content":"Selected Answer: D\nD\n- https://cloud.google.com/blog/topics/developers-practitioners/monitor-models-training-serving-skew-vertex-ai\n- https://developers.google.com/machine-learning/guides/rules-of-ml"},{"poster":"mymy9418","upvote_count":"1","content":"Selected Answer: D\nI think D","comment_id":"752071","timestamp":"1671615060.0"},{"timestamp":"1670953140.0","comment_id":"744320","content":"Selected Answer: B\n\"After a year, you notice that your model's performance sometimes degrades significantly after a month, while other times it takes several months to notice any decrease in performance.\" Hence I vote B","upvote_count":"2","poster":"mil_spyro"}],"timestamp":"2022-12-13 18:39:00","choices":{"A":"Train an anomaly detection model on the training dataset, and run all incoming requests through this model. If an anomaly is detected, send the most recent serving data to the labeling service.","D":"Run training-serving skew detection batch jobs every few days to compare the aggregate statistics of the features in the training dataset with recent serving data. If skew is detected, send the most recent serving data to the labeling service.","C":"Compare the cost of the labeling service with the lost revenue due to model performance degradation over the past year. If the lost revenue is greater than the cost of the labeling service, increase the frequency of model retraining; otherwise, decrease the model retraining frequency.","B":"Identify temporal patterns in your model’s performance over the previous year. Based on these patterns, create a schedule for sending serving data to the labeling service for the next year."},"url":"https://www.examtopics.com/discussions/google/view/91473-exam-professional-machine-learning-engineer-topic-1-question/","isMC":true,"answer_images":[],"topic":"1","exam_id":13,"answers_community":["D (75%)","B (25%)"],"answer":"D","question_images":[]},{"id":"nd6F0rUGxJqLoiIR3wHR","timestamp":"2022-12-13 18:43:00","unix_timestamp":1670953380,"question_images":[],"question_text":"You work for a company that manages a ticketing platform for a large chain of cinemas. Customers use a mobile app to search for movies they’re interested in and purchase tickets in the app. Ticket purchase requests are sent to Pub/Sub and are processed with a Dataflow streaming pipeline configured to conduct the following steps:\n1. Check for availability of the movie tickets at the selected cinema.\n2. Assign the ticket price and accept payment.\n3. Reserve the tickets at the selected cinema.\n4. Send successful purchases to your database.\n\nEach step in this process has low latency requirements (less than 50 milliseconds). You have developed a logistic regression model with BigQuery ML that predicts whether offering a promo code for free popcorn increases the chance of a ticket purchase, and this prediction should be added to the ticket purchase process. You want to identify the simplest way to deploy this model to production while adding minimal latency. What should you do?","isMC":true,"discussion":[{"content":"Selected Answer: D\nD as you want to do the prediction before the purchase","comment_id":"766347","timestamp":"1688536740.0","poster":"behzadsw","upvote_count":"10"},{"poster":"hiromi","content":"Selected Answer: D\nD\n- https://www.tensorflow.org/lite/guide","upvote_count":"5","comment_id":"753142","timestamp":"1687419420.0"},{"upvote_count":"1","content":"Selected Answer: B\nThe incorrect answers introduce latency issues or operational inefficiencies:\nA: Running batch inference with BigQuery ML every five minutes causes delays due to interval-based processing.\nC: Deploying the model on Vertex AI introduces network latency from HTTP requests.\nD: Using TensorFlow Lite on mobile decentralizes inference but adds inconsistencies due to device variability and complicates updates.\n\nCorrect Answer:\nB: Exporting the model in TensorFlow and integrating it into the Dataflow pipeline with tfx_bsl.public.beam.RunInference minimizes latency by keeping inference within the real-time streaming process. This ensures efficient and low-latency predictions.","comment_id":"1364513","timestamp":"1741024140.0","poster":"Amer95"},{"upvote_count":"1","timestamp":"1740103320.0","comment_id":"1359591","content":"Selected Answer: A\nNear Real-Time is Sufficient\nD. Convert to TFLite and deploy to the mobile app: This is impractical due to data availability, model updates, privacy concerns, and likely introduces more latency than a BigQuery ML batch prediction.","poster":"NamitSehgal"},{"poster":"bobjr","upvote_count":"2","content":"Selected Answer: C\nD makes no senses -> if the prediction is made on the phone, why send it to the server ? \n\nC is the best choice because it splits the responsability, and use best practices and scalable tools","timestamp":"1733484720.0","comment_id":"1225328"},{"timestamp":"1728813240.0","upvote_count":"1","poster":"omermahgoub","content":"Selected Answer: B\nB. Export your model in TensorFlow format, and add a tfx_bsl.public.beam.RunInference step to the Dataflow pipeline.\n\nHere's why this approach offers minimal latency:\n\nIn-Pipeline Prediction: The model is integrated directly into the Dataflow pipeline, enabling real-time predictions for each ticket purchase request without external calls.\nDataflow Integration: tfx_bsl.public.beam.RunInference is a Beam utility specifically designed for integrating TensorFlow models into Dataflow pipelines, ensuring efficient execution.","comment_id":"1194845"},{"timestamp":"1726554060.0","comment_id":"1175671","upvote_count":"1","content":"Selected Answer: B\nB\nFor D - How can we assume the model does be feasible to convert to Mobile app?","poster":"Yan_X"},{"upvote_count":"2","comment_id":"1067836","poster":"Krish6488","content":"Selected Answer: D\nQuestion looks ambiguous! However considering some keywords like low latency and more importantly ML usage for maximising the ticket purchase requests using the promo code means that model embedded to the device looks more appropriate, however there are a lot of downsides to it like model management and upgrades but that does not seem to be the consideration here anyway. Just looking at low latency and ML to maximise the ticket sales, I will go with D as thats much simpler to implement","timestamp":"1715425020.0"},{"content":"the whole question does not make too much sense to me.\nfirst of all, it seems that the Dataflow streaming job would \"accept payment\" meaning it communicates with payment gateways and back to the user, which does not sound right to do in Dataflow.\nthe model \"predicts whether offering a promo code for free popcorn increases the chance of a ticket purchase\" is necesssarily executed before processing payment, so D seems the best. \n\nAwkward ....","upvote_count":"2","timestamp":"1710145320.0","poster":"andresvelasco","comment_id":"1004490"},{"upvote_count":"1","timestamp":"1699638660.0","content":"Selected Answer: D\nWent with D","poster":"M25","comment_id":"894132"},{"upvote_count":"4","poster":"TNT87","comment_id":"872321","content":"Answer D","timestamp":"1697516760.0"},{"comments":[{"comment_id":"964934","poster":"tavva_prudhvi","timestamp":"1706382780.0","upvote_count":"1","content":"would also not be suitable because adding a tfx_bsl.public.beam.RunInference step to the Dataflow pipeline would still require the model to be executed within the same pipeline, potentially introducing additional latency and computational overhead."}],"poster":"TNT87","content":"Selected Answer: B\nis the simplest way to deploy the logistic regression model to production with minimal latency. Exporting the model in TensorFlow format and adding a tfx_bsl.public.beam.RunInference step to the existing Dataflow pipeline enables the model to be integrated directly into the ticket purchase process.","timestamp":"1694924580.0","upvote_count":"1","comment_id":"841673"},{"comments":[{"poster":"TNT87","timestamp":"1694278380.0","upvote_count":"1","content":"Aiiii between B and C","comment_id":"834279"},{"comment_id":"841671","content":"Answer B","upvote_count":"1","poster":"TNT87","timestamp":"1694924520.0"}],"timestamp":"1694082120.0","poster":"TNT87","upvote_count":"1","content":"Selected Answer: C\nOption C is the best solution. Since the entire process has low latency requirements, running batch inference every five minutes is not a suitable option. Option B requires a TensorFlow model format, which may not be available since the model is created using BigQuery ML. Option D is not recommended because it requires deploying the model to the mobile app, which may not be feasible or desired. Deploying the model on Vertex AI and querying the prediction endpoint from the streaming pipeline adds minimal latency and is the simplest solution.","comment_id":"831862"},{"poster":"Scipione_","upvote_count":"3","comment_id":"811885","content":"Selected Answer: D\nI perfectly agree with behzadsw. \nYou send a Pub/Sub request when you already want to buy, you must add the coupon before this process.","timestamp":"1692270540.0"},{"comments":[{"upvote_count":"1","timestamp":"1692019440.0","poster":"John_Pongthorn","content":"https://www.tensorflow.org/tfx/tfx_bsl/api_docs/python/tfx_bsl/public/beam/RunInference","comment_id":"808515"}],"upvote_count":"3","timestamp":"1692019440.0","comment_id":"808514","poster":"John_Pongthorn","content":"Selected Answer: B\nB (is it possible) along with What I get fromthis question.\n1. this prediction should be added to the ticket purchase process ( it mean that it have to be include in rocessed with a Dataflow streaming pipeline\n2.Each step in this process has low latency requirements (less than 50 milliseconds) , which signifies that whatever you will process in dataflow, there are no latency requirements issues"},{"content":"Answer D\nhttps://www.tensorflow.\norg/lite/guide","comments":[{"comment_id":"834280","content":"Nope answer is B","timestamp":"1694278440.0","upvote_count":"1","poster":"TNT87"}],"timestamp":"1687852500.0","comment_id":"758338","upvote_count":"1","poster":"TNT87"},{"upvote_count":"1","poster":"mil_spyro","content":"Selected Answer: C\nBy deploying your model on Vertex AI, you can quickly and easily add the prediction step to your streaming pipeline, without the need to add additional infrastructure or manage model deployment and management.","comment_id":"744325","timestamp":"1686670980.0","comments":[{"comment_id":"748586","comments":[{"comment_id":"748970","poster":"mil_spyro","timestamp":"1687089120.0","upvote_count":"4","content":"Hey I think you're right, predictions on the device itself, which avoids the need to send requests over the network. Should be D","comments":[]}],"content":"but maybe D is faster?","timestamp":"1687052040.0","poster":"mymy9418","upvote_count":"1"}]}],"answer":"D","exam_id":13,"choices":{"C":"Export your model in TensorFlow format, deploy it on Vertex AI, and query the prediction endpoint from your streaming pipeline.","B":"Export your model in TensorFlow format, and add a tfx_bsl.public.beam.RunInference step to the Dataflow pipeline.","D":"Convert your model with TensorFlow Lite (TFLite), and add it to the mobile app so that the promo code and the incoming request arrive together in Pub/Sub.","A":"Run batch inference with BigQuery ML every five minutes on each new set of tickets issued."},"answer_images":[],"answers_community":["D (64%)","B (21%)","Other"],"topic":"1","url":"https://www.examtopics.com/discussions/google/view/91474-exam-professional-machine-learning-engineer-topic-1-question/","question_id":44,"answer_description":"","answer_ET":"D"},{"id":"iOz8V2bPds0Ixc5d6VgX","url":"https://www.examtopics.com/discussions/google/view/91476-exam-professional-machine-learning-engineer-topic-1-question/","answer_description":"","choices":{"A":"Train a time-series model to predict the machines’ performance values. Configure an alert if a machine’s actual performance values significantly differ from the predicted performance values.","C":"Develop a simple heuristic (e.g., based on z-score) to label the machines’ historical performance data. Train a model to predict anomalies based on this labeled dataset.","B":"Develop a simple heuristic (e.g., based on z-score) to label the machines’ historical performance data. Use this heuristic to monitor server performance in real time.","D":"Hire a team of qualified analysts to review and label the machines’ historical performance data. Train a model based on this manually labeled dataset."},"question_text":"You work on a team in a data center that is responsible for server maintenance. Your management team wants you to build a predictive maintenance solution that uses monitoring data to detect potential server failures. Incident data has not been labeled yet. What should you do first?","question_id":45,"answers_community":["B (52%)","C (38%)","10%"],"exam_id":13,"discussion":[{"timestamp":"1699889100.0","comments":[{"content":"Compare to 94","upvote_count":"1","poster":"Mickey321","timestamp":"1699889340.0","comment_id":"1069453"}],"content":"Selected Answer: B\nShould be B","upvote_count":"5","poster":"Mickey321","comment_id":"1069447"},{"upvote_count":"1","timestamp":"1740103560.0","comment_id":"1359593","poster":"NamitSehgal","content":"Selected Answer: C\nThe model trained on heuristically labeled data can then be used to identify potential anomalies."},{"content":"Selected Answer: C\nThe heuristic may work for monitoring in real time, but training a model on labeled data provides more accuracy over time as it adapts and improves. Simply using a heuristic to monitor the data does not allow for scalable automation of anomaly detection.","comment_id":"1322182","poster":"rajshiv","timestamp":"1733363160.0","upvote_count":"2"},{"upvote_count":"1","comments":[{"content":"\"What should you do first?\"....","timestamp":"1714113120.0","poster":"pinimichele01","upvote_count":"2","comment_id":"1202413"}],"poster":"omermahgoub","timestamp":"1713004560.0","comment_id":"1194875","content":"Selected Answer: C\nC. Develop a simple heuristic (e.g., based on z-score) to label the machines’ historical performance data. Train a model to predict anomalies based on this labeled dataset.\n\nReal-Time Heuristic Monitoring (Option B): Using a z-score based heuristic for real-time monitoring can be helpful as an initial step, but it might not capture complex anomaly patterns that a trained model could identify."},{"upvote_count":"1","content":"Selected Answer: C\nI go for C because is more practical and efficient","timestamp":"1711025760.0","poster":"ludovikush","comment_id":"1179242"},{"content":"Selected Answer: D\nD: This approach involves creating a labeled dataset through human analysis, which serves as the ground truth for training a predictive maintenance model. Manual labeling allows you to identify instances of actual failures and non-failure states in the historical performance data. Once the dataset is labeled, you can train a machine learning model to detect patterns associated with potential server failures.","comment_id":"1073249","upvote_count":"1","comments":[{"upvote_count":"1","timestamp":"1700220180.0","comment_id":"1073250","content":"why not B (or C): While heuristics can be quick to implement, they may lack accuracy and may not capture complex patterns associated with server failures. Additionally, using a heuristic alone might not provide the necessary foundation for a robust predictive maintenance model.","poster":"pico","comments":[{"timestamp":"1709266560.0","comment_id":"1163218","poster":"Werner123","upvote_count":"3","content":"Google Rules of ML: Rule #1: Don’t be afraid to launch a product without machine learning.\nhttps://developers.google.com/machine-learning/guides/rules-of-ml#rule_1_don%E2%80%99t_be_afraid_to_launch_a_product_without_machine_learning"}]}],"poster":"pico","timestamp":"1700220120.0"},{"timestamp":"1699863900.0","upvote_count":"2","poster":"pico","comment_id":"1069150","content":"Selected Answer: C\nOption B also falls short as it focuses on real-time monitoring based on a heuristic but doesn't utilize historical data to create a predictive model. This approach might raise false alarms and lacks the ability to learn from the data over time."},{"poster":"Krish6488","comment_id":"1067828","timestamp":"1699706280.0","comments":[{"comment_id":"1163220","poster":"Werner123","timestamp":"1709266620.0","content":"It does not say use ML. It only says a predictive maintenance solution, that could be using a simple heuristic.","upvote_count":"2"}],"content":"Selected Answer: C\nClearly the ask is an approach to build an ML application to detect potential server failures. Using labelled data to monitor it in real time does not give a proactive solution rather it becomes a reactive solution. I will go with C","upvote_count":"2"},{"timestamp":"1683733860.0","content":"Selected Answer: C\nThe goal / “your” task is to predict or “build a predictive maintenance solution”, i.e., “Train a model to predict anomalies” [Option C]; not to perform monitoring or “to monitor server performance in real time” [Option B], there is a whole team “responsible for server maintenance”.\nThe “do first” part refers to the use of a simple heuristic for initial labeling, not to what to do with the results of it. \nThe more sophisticated solution: https://cloud.google.com/blog/products/ai-machine-learning/event-monitoring-with-explanations-on-the-google-cloud.","comments":[{"content":"Changed to B, based on the comparison with #94, assuming that by “Use this heuristic to monitor server performance in real time” is meant to “first” test this heuristic for labelling in a Prod. environment, as a quick reality-check, before training a whole model on a roughly inaccurate labelled dataset.","poster":"M25","comment_id":"898130","timestamp":"1684138500.0","comments":[{"poster":"pico","timestamp":"1700220600.0","comment_id":"1073256","upvote_count":"1","content":"why do you assume that this needs to be done \"quick\" instead of \"good\"?"}],"upvote_count":"3"}],"poster":"M25","upvote_count":"1","comment_id":"894133"},{"content":"Selected Answer: B\nANSWER B","timestamp":"1679639940.0","poster":"TNT87","upvote_count":"2","comment_id":"849056"},{"upvote_count":"3","comments":[{"poster":"YushiSato","timestamp":"1723332600.0","upvote_count":"1","comment_id":"1263659","content":"https://developers.google.com/machine-learning/guides/rules-of-ml#rule_1_don%E2%80%99t_be_afraid_to_launch_a_product_without_machine_learning\n> Rule #1: Don’t be afraid to launch a product without machine learning.\n> Machine learning is cool, but it requires data. Theoretically, you can take data from a different problem and then tweak the model for a new product, but this will likely underperform basic heuristics. If you think that machine learning will give you a 100% boost, then a heuristic will get you 50% of the way there.\n\n> For instance, if you are ranking apps in an app marketplace, you could use the install rate or number of installs as heuristics. If you are detecting spam, filter out publishers that have sent spam before. Don’t be afraid to use human editing either. If you need to rank contacts, rank the most recently used highest (or even rank alphabetically). If machine learning is not absolutely required for your product, don't use it until you have data."}],"content":"Selected Answer: B\nhttps://developers.google.com/machine-learning/guides/rules-of-ml","timestamp":"1678710900.0","comment_id":"837917","poster":"osaka_monkey"},{"comment_id":"831858","comments":[],"poster":"TNT87","content":"Selected Answer: D\nD. Hire a team of qualified analysts to review and label the machines' historical performance data. Training a model based on this manually labeled dataset would be the most accurate and effective approach. Developing a simple heuristic to label the machines' historical performance data may not be accurate enough to detect all potential failures, and training a model without labeled data could result in poor performance. Additionally, it's important to ensure that the team of analysts is qualified and experienced in labeling this type of data accurately to ensure the model is trained with high-quality labeled data.","upvote_count":"1","timestamp":"1678191600.0"},{"timestamp":"1676638980.0","content":"Selected Answer: C\nI like this question because it's helpful to remember that ML is used when needed.\nIn this case you have unlabeled target classes so you can use unsupervised learning techniques like clustering to identify patterns or just develop a heuristic method.\nAnswer 'B' in my opinion.","comments":[{"timestamp":"1676639040.0","comment_id":"811880","content":"sorry I meant 'B'","poster":"Scipione_","upvote_count":"1"}],"upvote_count":"1","poster":"Scipione_","comment_id":"811879"},{"content":"Selected Answer: B\nhttps://developers.\ngoogle.com/machine-\nlearning/\nguides/rules-of-ml \nAnswer B","poster":"TNT87","timestamp":"1672134780.0","upvote_count":"2","comment_id":"758337"},{"comment_id":"753166","timestamp":"1671702960.0","content":"Selected Answer: B\nB\n- https://developers.google.com/machine-learning/guides/rules-of-ml","poster":"hiromi","upvote_count":"2"},{"content":"Selected Answer: B\nB should be first to do","timestamp":"1671334560.0","comment_id":"748587","poster":"mymy9418","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: D\nVote D","comments":[{"timestamp":"1671371280.0","poster":"mil_spyro","comment_id":"748965","upvote_count":"2","content":"Should be B"}],"timestamp":"1670953740.0","poster":"mil_spyro","comment_id":"744328"}],"timestamp":"2022-12-13 18:49:00","topic":"1","answer_ET":"B","answer":"B","isMC":true,"unix_timestamp":1670953740,"answer_images":[],"question_images":[]}],"exam":{"isImplemented":true,"numberOfQuestions":304,"isBeta":false,"provider":"Google","name":"Professional Machine Learning Engineer","id":13,"isMCOnly":true,"lastUpdated":"11 Apr 2025"},"currentPage":9},"__N_SSP":true}