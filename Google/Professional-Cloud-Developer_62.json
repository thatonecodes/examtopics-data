{"pageProps":{"questions":[{"id":"UHKxxq9nMWMowQoV4PEo","isMC":true,"question_id":306,"answers_community":["C (57%)","B (43%)"],"question_text":"You are deploying your application to a Compute Engine virtual machine instance with the Stackdriver Monitoring Agent installed. Your application is a unix process on the instance. You want to be alerted if the unix process has not run for at least 5 minutes. You are not able to change the application to generate metrics or logs.\nWhich alert condition should you configure?","answer":"C","discussion":[{"timestamp":"1614429660.0","content":"B\n\nProcess-health policy\nA process-health policy can notify you if the number of processes that match a pattern crosses a threshold. This can be used to tell you, for example, that a process has stopped running.","comment_id":"300243","poster":"gcper","upvote_count":"5"},{"poster":"Sebasi","content":"Selected Answer: C\nC\n\nMetric absence\nYou want to be alerted if the UNIX process has not run for at least 5 minutes. Since you cannot modify the application to generate metrics or logs, you need to rely on detecting the absence of metrics collected by the Stackdriver Monitoring Agent.","comment_id":"1320588","upvote_count":"1","timestamp":"1733062500.0"},{"content":"Selected Answer: C\nIf the requirement is to simply determine whether a process has stopped reporting metrics, Metric Absence is the preferred choice.","poster":"wuhaosheng","upvote_count":"1","timestamp":"1732759620.0","comment_id":"1318988"},{"poster":"ravia__","comment_id":"1280661","upvote_count":"1","timestamp":"1725843900.0","content":"Selected Answer: C\nMetric absence allows you to trigger an alert if no metrics (e.g., CPU usage, memory usage, etc.) related to the process are received for a specific period, such as 5 minutes"},{"upvote_count":"1","comment_id":"1168059","content":"Selected Answer: C\nTo be alerted if your Unix process has not run for at least 5 minutes, you should configure a **metric absence** alert condition. Since you cannot modify the application to generate metrics or logs, this approach allows you to monitor the absence of data for a specific duration. When the Unix process stops running, the metric data will be absent, triggering the alert.\n\nTherefore, the correct answer is **C. Metric absence**. This type of alerting policy will help you stay informed about any unexpected interruptions in your process execution.","timestamp":"1709820060.0","poster":"santoshchauhan"},{"timestamp":"1695130800.0","poster":"__rajan__","comment_id":"1011374","content":"Selected Answer: B\nB is correct.","upvote_count":"1"},{"poster":"studyingveryhard","content":"Complete explanations for correct and incorrect answers can be seen on https://examlab.co/google/google-cloud-professional-cloud-developer","upvote_count":"1","timestamp":"1689573240.0","comment_id":"953871"},{"upvote_count":"1","timestamp":"1683127440.0","comment_id":"888680","content":"Selected Answer: B\nhttps://cloud.google.com/monitoring/alerts/policies-in-json#json-process-health","poster":"closer89"},{"timestamp":"1660973100.0","poster":"tomato123","upvote_count":"1","comment_id":"649164","content":"Selected Answer: B\nB is correct"},{"content":"B is correct","poster":"wilwong","timestamp":"1626831780.0","upvote_count":"1","comment_id":"410594"},{"comment_id":"385985","upvote_count":"2","timestamp":"1624168680.0","content":"https://cloud.google.com/monitoring/uptime-checks:\n\"An uptime check is a request sent to a resource to see if it responds\"\n\nA is wrong\n\nMetric absence and threshold don't make sense\n\nProcess health is correct for sure so answer is B","poster":"syu31svc"},{"content":"B is correct answer\nhttps://cloud.google.com/monitoring/alerts/types-of-conditions#metric-threshold","timestamp":"1604597700.0","comment_id":"213600","poster":"saurabh1805","upvote_count":"2"}],"topic":"1","url":"https://www.examtopics.com/discussions/google/view/36175-exam-professional-cloud-developer-topic-1-question-7/","answer_description":"","unix_timestamp":1604597700,"answer_ET":"C","answer_images":[],"exam_id":7,"timestamp":"2020-11-05 18:35:00","question_images":[],"choices":{"D":"Metric threshold","B":"Process health","A":"Uptime check","C":"Metric absence"}},{"id":"PQBVPXEqIReAPusXMJeS","question_text":"Your application is deployed in a Google Kubernetes Engine (GKE) cluster. You want to expose this application publicly behind a Cloud Load Balancing HTTP(S) load balancer.\nWhat should you do?","isMC":true,"answer_description":"","unix_timestamp":1608584700,"discussion":[{"content":"A(https://cloud.google.com/kubernetes-engine/docs/tutorials/http-balancer)","upvote_count":"7","comment_id":"249678","comments":[{"upvote_count":"2","poster":"[Removed]","content":"For me the right answer is D.\nThe Ingress Object create a global http(s) L.B. The advantage of having a layer 7 load balancer is that we can configure advanced things at network layer 7. For example to route traffic based on the path of the request and so on. In the question it seems that is not needed, so for me a Load Balancer Service is ok since it creates a network load balancer (TCP/UDP) that is ok for HTTP or HTTPS exposing (since we can configure any TCP/UDP port to be exposed to The Internet)","timestamp":"1653935880.0","comment_id":"609418"},{"poster":"mastodilu","timestamp":"1621578060.0","upvote_count":"1","comment_id":"362713","content":"an ingress works if you already have a service, like an https load balancer or a NodePort."}],"poster":"donchick","timestamp":"1608584700.0"},{"content":"A) \nThe important part of the question is this \"...expose this application publicly behind a Cloud Load Balancing HTTP(S) load balancer.\" This means it is an L7 exposure using HTTPS (a Service of type \"LoadBalancer\" would only create an L4 exposure - IP only... No HTTPS).\n\nSo Ingress is the choice you should make. And in GKE, luckily this is one thing - create an ingress and the LB will be attached automagically ;D","upvote_count":"5","poster":"Valant","timestamp":"1635799380.0","comment_id":"471406"},{"upvote_count":"1","timestamp":"1721227920.0","comment_id":"1249703","comments":[{"poster":"thewalker","content":"Let's break down why the other options are incorrect:\n\nB. Configure a GKE Service resource: Services in Kubernetes are used to expose applications within the cluster. While you can create a Service of type LoadBalancer , this will create an internal load balancer within the cluster, not an external one.\nC. Configure a GKE Ingress resource with type: LoadBalancer: Ingress resources don't have a type field. The LoadBalancer type is used for Services, not Ingress.\nD. Configure a GKE Service resource with type: LoadBalancer: This would create an internal load balancer within the cluster, not an external one.","timestamp":"1721227920.0","comment_id":"1249704","upvote_count":"1"}],"poster":"thewalker","content":"Selected Answer: A\nThe correct answer is A. Configure a GKE Ingress resource.\n\nHere's why:\n\nIngress for External Load Balancing: Ingress is the Kubernetes resource specifically designed for managing external HTTP(S) traffic to services within your cluster. When you create an Ingress resource, GKE automatically provisions and configures a Cloud Load Balancing HTTP(S) load balancer to route traffic to your application."},{"poster":"d_ella2001","content":"Selected Answer: A\nA. Configure a GKE Ingress resource. This option correctly configures the Ingress resource which is designed to expose HTTP and HTTPS routes from outside the cluster to services within the cluster.\nB. Configure a GKE Service resource.\nThis option only exposes the application internally within the cluster unless the service type is specified as LoadBalancer.\nC. Configure a GKE Ingress resource with type: LoadBalancer.\nThis option is incorrect because Ingress resources do not have a type: LoadBalancer property. The Ingress resource itself manages the creation of a load balancer.\nD. Configure a GKE Service resource with type: LoadBalancer.\nThis option creates a TCP/UDP load balancer, not an HTTP(S) load balancer, which does not meet the requirements for an HTTP(S) load balancer.","upvote_count":"1","comment_id":"1246124","timestamp":"1720701720.0"},{"content":"Selected Answer: D\nYou want to expose this application publicly behind a Cloud Load Balancing HTTP(S) load balancer. Hence using Service with type LoadBalancer","poster":"Ibrahim_Hasan","comment_id":"1203909","upvote_count":"1","timestamp":"1714378740.0"},{"upvote_count":"3","comment_id":"1011895","content":"Selected Answer: A\nTo expose your application publicly behind a Cloud Load Balancing HTTP(S) load balancer in a Google Kubernetes Engine (GKE) cluster, we should configure a GKE Ingress resource. This approach allows you to define rules for routing external HTTP(S) traffic to internal services based on hostnames and URL paths.","poster":"__rajan__","timestamp":"1695183360.0"},{"comment_id":"797766","timestamp":"1675501920.0","content":"Selected Answer: D\nThe correct answer is D\n\nConfiguring a GKE ingress resource is not enough, you also need to expose the service with the type NodePort and then configure the ingress resource to point to that service.\n\nD is sufficient, then D is the correct answer. A lacks some more work.","upvote_count":"4","poster":"[Removed]"},{"upvote_count":"2","content":"To expose your application publicly behind a Cloud Load Balancing HTTP(S) load balancer in a GKE cluster, you should configure a GKE Ingress resource or a GKE Service resource with type: LoadBalancer.\n\nTo configure a GKE Ingress resource, you need to define rules for routing HTTP(S) traffic to the application in the cluster. This is done by creating an Ingress object, which is associated with one or more Service objects, each of which is associated with a set of Pods. The GKE Ingress controller will then create a Google Cloud HTTP(S) Load Balancer and configure it according to the information in the Ingress and its associated Services.\n\nAlternatively, you can configure a GKE Service resource with type: LoadBalancer to expose your application publicly. This will create a Cloud Load Balancing HTTP(S) load balancer and associate it with the Service. The Service will then route traffic to the application Pods.","comment_id":"769205","timestamp":"1673167140.0","poster":"omermahgoub"},{"timestamp":"1660974720.0","upvote_count":"2","content":"Selected Answer: A\nA is correct","comment_id":"649234","poster":"tomato123"},{"timestamp":"1659079680.0","content":"Selected Answer: A\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/ingress\nGKE Ingress for HTTP(S) Load Balancing \nThis page provides a general overview of what Ingress for HTTP(S) Load Balancing is and how it works. Google Kubernetes Engine (GKE) provides a built-in and managed Ingress controller called GKE Ingress. This controller implements Ingress resources as Google Cloud load balancers for HTTP(S) workloads in GKE.","poster":"nehaxlpb","upvote_count":"1","comment_id":"639063"},{"timestamp":"1653474420.0","comments":[{"comment_id":"607608","upvote_count":"1","poster":"ruben82","timestamp":"1653563520.0","content":"It's A... 'Cos Kind: Ingress create automatically a LB HTTP(S)"},{"content":"kind Service in GKE creates TCP/IP LB. If you want to leverage HTTP(s) you need to create Ingress class.","upvote_count":"1","comment_id":"762597","poster":"lxs","timestamp":"1672481280.0"}],"content":"I think it's D.\nThey need a Load Balancer too. \nIngress just permits to expose a cluster, then A answer is not complete according to requirement.","upvote_count":"2","comment_id":"607161","poster":"ruben82"},{"upvote_count":"4","timestamp":"1626444240.0","poster":"[Removed]","comment_id":"407929","content":"(D) is not correct as service with type LoadBalancer create network load balancer not http load balancer. \n(A) is correct ingress will create http balancer without the need of specify type\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/ingress#overview"},{"timestamp":"1625878680.0","content":"https://cloud.google.com/kubernetes-engine/docs/tutorials/http-balancer:\n\"When you specify kind: Service with type: LoadBalancer in the resource manifest, GKE creates a Service of type LoadBalancer\"\n\nD is correct","poster":"syu31svc","comments":[{"poster":"syu31svc","content":"Changing my answer to A\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/ingress:\n\"In GKE, an Ingress object defines rules for routing HTTP(S) traffic to applications running in a cluster. An Ingress object is associated with one or more Service objects, each of which is associated with a set of Pods. To learn more about how Ingress exposes applications using Services, see Service networking overview.\n\nWhen you create an Ingress object, the GKE Ingress controller creates a Google Cloud HTTP(S) Load Balancer and configures it according to the information in the Ingress and its associated Services.\"","timestamp":"1627692240.0","upvote_count":"5","comment_id":"417647"}],"upvote_count":"2","comment_id":"403030"},{"upvote_count":"3","comment_id":"362711","content":"answer is D","poster":"mastodilu","timestamp":"1621578000.0"}],"timestamp":"2020-12-21 22:05:00","answers_community":["A (62%)","D (38%)"],"url":"https://www.examtopics.com/discussions/google/view/40504-exam-professional-cloud-developer-topic-1-question-70/","question_images":[],"answer":"A","question_id":307,"answer_images":[],"topic":"1","exam_id":7,"choices":{"C":"Configure a GKE Ingress resource with type: LoadBalancer.","D":"Configure a GKE Service resource with type: LoadBalancer.","B":"Configure a GKE Service resource.","A":"Configure a GKE Ingress resource."},"answer_ET":"A"},{"id":"M5jp2cmIeQw77LVWpXGu","choices":{"C":"Create a Cloud Dataproc cluster on Google Cloud Platform, and then migrate your Hadoop environment to the new Cloud Dataproc cluster. Move your HDFS data into larger HDD disks to save on storage costs.","B":"Create Compute Engine instances with HDD instead of SSD to save costs. Then perform a full migration of your existing environment into the new one in Compute Engine instances.","D":"Create a Cloud Dataproc cluster on Google Cloud Platform, and then migrate your Hadoop code objects to the new cluster. Move your data to Cloud Storage and leverage the Cloud Dataproc connector to run jobs on that data.","A":"Migrate your data stored in Hadoop to BigQuery. Change your jobs to source their information from BigQuery instead of the on-premises Hadoop environment."},"timestamp":"2020-12-21 22:07:00","unix_timestamp":1608584820,"url":"https://www.examtopics.com/discussions/google/view/40505-exam-professional-cloud-developer-topic-1-question-71/","isMC":true,"question_images":[],"exam_id":7,"question_text":"Your company is planning to migrate their on-premises Hadoop environment to the cloud. Increasing storage cost and maintenance of data stored in HDFS is a major concern for your company. You also want to make minimal changes to existing data analytics jobs and existing architecture.\nHow should you proceed with the migration?","answer_ET":"D","answer_images":[],"answers_community":["D (100%)"],"answer":"D","answer_description":"","topic":"1","discussion":[{"comment_id":"249679","poster":"donchick","timestamp":"1640120820.0","upvote_count":"7","content":"I'd choose D."},{"comment_id":"403559","poster":"syu31svc","timestamp":"1657494900.0","upvote_count":"6","content":"https://cloud.google.com/architecture/hadoop/hadoop-gcp-migration-overview:\n\"Keeping your data in a persistent HDFS cluster using Dataproc is more expensive than storing your data in Cloud Storage, which is what we recommend, as explained later. Keeping data in an HDFS cluster also limits your ability to use your data with other Google Cloud products.\"\n\"Google Cloud includes Dataproc, which is a managed Hadoop and Spark environment. You can use Dataproc to run most of your existing jobs with minimal alteration, so you don't need to move away from all of the Hadoop tools you already know\"\n\nD is the answer"},{"content":"Selected Answer: D\nOption D is correct.","timestamp":"1726806540.0","comment_id":"1011899","poster":"__rajan__","upvote_count":"1"},{"poster":"tranvanchau9494","upvote_count":"1","comment_id":"993087","timestamp":"1724935560.0","content":"D is correct"},{"timestamp":"1704703260.0","poster":"omermahgoub","comment_id":"769206","content":"Option D is the most appropriate choice because it allows you to migrate your Hadoop code objects to a Cloud Dataproc cluster, which is a fully-managed Apache Hadoop and Apache Spark service on Google Cloud. This will allow you to make minimal changes to your existing data analytics jobs and existing architecture. Additionally, moving your data to Cloud Storage and using the Cloud Dataproc connector to run jobs on that data will allow you to take advantage of the scalability, durability, and security of Cloud Storage while also minimizing storage costs.","upvote_count":"1","comments":[{"comment_id":"769207","content":"A would involve a significant change to your existing data analytics jobs and architecture, as it would involve migrating your data to BigQuery and changing your jobs to source their information from BigQuery instead of Hadoop.\n\nB is not a feasible option because Compute Engine instances do not have the capability to run HDFS.\n\nC would not allow you to save on storage costs as it involves moving your data to larger HDD disks rather than a more cost-effective storage solution like Cloud Storage.","timestamp":"1704703320.0","poster":"omermahgoub","upvote_count":"1"}]},{"comment_id":"649236","content":"Selected Answer: D\nD is correct","upvote_count":"2","timestamp":"1692510780.0","poster":"tomato123"},{"content":"D is correct","comment_id":"602927","timestamp":"1684328100.0","upvote_count":"1","poster":"szl0144"}],"question_id":308},{"id":"vo1c3Sdau6pNTq77iGBy","choices":{"D":"gcloud services test ג€\"o output.json gs://my-bucket","C":"gcloud compute scp example-instance:~/test-data ג€\"o output.json gs://my-bucket","B":"gsutil perfdiag ג€\"o output.json gs://my-bucket","A":"gsutil test ג€\"o output.json gs://my-bucket"},"answer":"B","question_text":"Your data is stored in Cloud Storage buckets. Fellow developers have reported that data downloaded from Cloud Storage is resulting in slow API performance.\nYou want to research the issue to provide details to the GCP support team.\nWhich command should you run?","url":"https://www.examtopics.com/discussions/google/view/40506-exam-professional-cloud-developer-topic-1-question-72/","exam_id":7,"answer_ET":"B","answer_images":[],"timestamp":"2020-12-21 22:11:00","answers_community":["B (100%)"],"answer_description":"","unix_timestamp":1608585060,"discussion":[{"content":"B(https://cloud.google.com/storage/docs/gsutil/commands/perfdiag#providing-diagnostic-output-to-cloud-storage-team)","timestamp":"1608585060.0","poster":"donchick","upvote_count":"14","comments":[{"poster":"syu31svc","timestamp":"1625959260.0","upvote_count":"1","content":"Spot-on link and answer","comment_id":"403562"}],"comment_id":"249684"},{"comments":[{"content":"Let's break down why the other options are incorrect:\n\nA. gsutil test -o output.json gs://my-bucket : gsutil test is not a valid command. There's no command in gsutil called test .\nC. gcloud compute scp example-instance:~/test-data -o output.json gs://my-bucket : This command is used to copy files from a Compute Engine instance to a Cloud Storage bucket. It doesn't perform any performance diagnostics.\nD. gcloud services test -o output.json gs://my-bucket : gcloud services test is used to test the availability and performance of Google Cloud services, but it's not specific to Cloud Storage.","poster":"thewalker","comment_id":"1249708","timestamp":"1721228280.0","upvote_count":"1"}],"timestamp":"1721228280.0","comment_id":"1249707","content":"Selected Answer: B\nThe correct answer is B. gsutil perfdiag -o output.json gs://my-bucket .\n\nHere's why:\n\ngsutil perfdiag : This command is specifically designed to run a suite of diagnostic tests on a Cloud Storage bucket. It measures various performance aspects, including latency, throughput, and listing operations. This is exactly what you need to gather detailed information about the slow download performance.\n-o output.json : This option specifies that the results of the diagnostic tests should be saved to a JSON file named output.json . This file will contain detailed performance metrics that you can provide to the GCP support team.","poster":"thewalker","upvote_count":"1"},{"upvote_count":"1","timestamp":"1695184260.0","comment_id":"1011901","poster":"__rajan__","content":"Selected Answer: B\nTo research the issue of slow API performance when downloading data from Cloud Storage, you can use the gsutil perfdiag command. This command runs a set of tests to report the actual performance of a Cloud Storage bucket and provides detailed information on the performance of individual operations."},{"content":"B. gsutil perfdiag -o output.json gs://my-bucket\n\nThe gsutil perfdiag command is used to diagnose performance issues with Cloud Storage. It can be used to perform various tests such as download, upload, and metadata operations. By using the -o flag, you can specify an output file where the results of the tests will be stored in JSON format. This output file can then be provided to the GCP support team to help them investigate the issue.","poster":"omermahgoub","timestamp":"1673167380.0","comment_id":"769211","upvote_count":"2"},{"content":"CORRECT","poster":"ynaitam","timestamp":"1670159040.0","upvote_count":"1","comment_id":"735096"},{"upvote_count":"2","comment_id":"649237","content":"Selected Answer: B\nB is correct","poster":"tomato123","timestamp":"1660974780.0"}],"isMC":true,"topic":"1","question_images":[],"question_id":309},{"id":"FZ25cMIml45HJGYP8P4A","unix_timestamp":1604924640,"answer_images":[],"discussion":[{"comment_id":"376465","timestamp":"1623036540.0","poster":"LCL8338","upvote_count":"20","content":"C, since digests are immutable, whilst docker tags are mutable (hence not D). \nhttps://cloud.google.com/architecture/using-container-images"},{"timestamp":"1611516060.0","poster":"dxxdd7","comment_id":"275413","upvote_count":"9","comments":[{"content":"This is correct","poster":"StelSen","upvote_count":"3","timestamp":"1613794020.0","comment_id":"294757"},{"timestamp":"1672481640.0","content":"You are not correct. The question is to ensure the images are the same not about docker image naming convention best practices. The only way to compare two images and say they are the same is digest hash. You can mistakenly tag two different images using the same semver tag.","upvote_count":"3","poster":"lxs","comment_id":"762599"}],"content":"For me it's D, it's not a best practice to use image with the latest tag. And using the semantic version will ensure that all the environment use the exact same image with the wanted code."},{"upvote_count":"1","comment_id":"1249715","poster":"thewalker","content":"Selected Answer: C\nThe best answer is C. Use the digest of the Docker image.\n\nHere's why:\n\nDocker Image Digest: A digest is a unique identifier for a specific Docker image. It's a cryptographic hash of the image's contents, ensuring that you're always deploying the exact same image across environments.","timestamp":"1721228580.0","comments":[{"upvote_count":"1","content":"Let's break down why the other options are less ideal:\n\nA. Use the latest Docker image tag: The latest tag is mutable. If you push a new image with the latest tag, it will overwrite the previous latest image. This can lead to inconsistencies across environments if different environments pull the latest tag at different times.\nB. Use a unique Docker image name: While using unique names can help with organization, it doesn't guarantee that you're deploying the same image across environments. You could accidentally push a different image with the same name, leading to inconsistencies.\nD. Use a semantic version Docker image tag: Semantic versioning is a good practice for managing software releases, but it doesn't guarantee that the image content is identical across environments. You could accidentally push a new image with the same semantic version but different content.","poster":"thewalker","timestamp":"1721228640.0","comment_id":"1249716"}]},{"comment_id":"1168531","upvote_count":"1","poster":"santoshchauhan","timestamp":"1709869200.0","content":"Selected Answer: C\nC. Use the digest of the Docker image.\n\nWhen promoting Docker images across different environments in a CI/CD pipeline, it's crucial to ensure that exactly the same image is deployed to each environment. The most reliable way to identify a Docker image is by using its digest.\n\nHere's why using the digest is the best approach:\n\nThe digest is a SHA256 hash of the image's content and configuration, which uniquely identifies an image. If anything about the image changes, the digest changes. This means that if you deploy an image by its digest, you are guaranteed to deploy the exact same image in each environment.\n\nUsing the digest is more reliable than using tags like 'latest' or semantic versioning. Tags can be moved to point to different images, but digests are immutable. Once an image is pushed to a registry, its digest can never change."},{"content":"Selected Answer: C\nI would go with C.","timestamp":"1695184440.0","comment_id":"1011904","upvote_count":"1","poster":"__rajan__"},{"content":"Selected Answer: C\nC. Use the digest of the Docker image.\n\nUsing the digest of the Docker image is the most reliable way to ensure that the exact same Docker image is deployed to each environment. The digest is a hash of the image content and metadata, which is unique to each image. This means that even if the image is tagged with different versions or names, the digest will remain the same as long as the content and metadata are identical.\n\nOn the other hand, using the latest Docker image tag or a semantic version tag may not guarantee that the exact same image is deployed to each environment. These tags are mutable and can be overwritten or updated, which could result in different images being deployed to different environments.\n\nUsing a unique Docker image name could work, but it may be more difficult to manage and track multiple images with different names, especially if there are many environments or frequent updates.","upvote_count":"2","comment_id":"854151","timestamp":"1680076440.0","poster":"Teraflow"},{"content":"Selected Answer: C\nAnser C because nees to be sure that the same image for the 3 envs. A tag version can be change between the deployment of the env.","upvote_count":"1","poster":"telp","comment_id":"773207","timestamp":"1673506860.0"},{"comment_id":"769212","poster":"omermahgoub","timestamp":"1673167440.0","upvote_count":"3","content":"Selected Answer: C\nC. Use the digest of the Docker image.\n\nThe digest of the Docker image is a unique identifier for the specific version of the image. By using the digest, you can ensure that the same exact version of the image is deployed to each environment. Using the latest tag or a unique image name may not necessarily guarantee that the same version is deployed, as these tags may change over time. Using a semantic version tag would only ensure that the same version is deployed if you follow a strict versioning policy and only update the image by incrementing the patch or minor version number."},{"timestamp":"1669877280.0","content":"Selected Answer: C\nC is the answer","comment_id":"732343","poster":"kisswd","upvote_count":"1"},{"poster":"tomato123","upvote_count":"2","comment_id":"649238","content":"Selected Answer: D\nD is correct","timestamp":"1660974840.0"},{"upvote_count":"3","timestamp":"1652792700.0","poster":"szl0144","content":"C is 100% correct","comment_id":"602930"},{"poster":"KillerGoogle","upvote_count":"4","content":"Read the question, it asks to ensure that the 'same' Docker image is deployed to every environment, so to identify the docker image, we have to use digests","comment_id":"556736","timestamp":"1645888920.0"},{"upvote_count":"3","comments":[{"comment_id":"648330","poster":"alex8081","content":"\"By design, the Git commit hash is immutable and references a specific version of your software\"..\nhttps://cloud.google.com/architecture/best-practices-for-building-containers","timestamp":"1660804440.0","upvote_count":"1"}],"timestamp":"1645331760.0","poster":"nazonazonazo","content":"C is correct.\nanother answers are not immutable.","comment_id":"551527"},{"timestamp":"1625960040.0","poster":"syu31svc","content":"https://cloud.google.com/architecture/best-practices-for-building-containers#tagging_using_semantic_versioning\n\nAnswer is D","upvote_count":"3","comment_id":"403564","comments":[{"timestamp":"1701249900.0","content":"I vote for this. We are likely looking for the best-practice way to 'promote' an image through dev, test, and prod environments. It is normal to use a tag with standard naming convention to identify/select images to promote e.g. tag v1.0.0. Using the digest would work, but this is not normal practice.","upvote_count":"1","poster":"Rupo7","comment_id":"1083315"}]},{"comment_id":"215912","poster":"guidogiordano","content":"for me the correct answer is A)","timestamp":"1604924640.0","upvote_count":"5"}],"timestamp":"2020-11-09 13:24:00","topic":"1","choices":{"B":"Use a unique Docker image name.","D":"Use a semantic version Docker image tag.","C":"Use the digest of the Docker image.","A":"Use the latest Docker image tag."},"question_text":"You are using Cloud Build build to promote a Docker image to Development, Test, and Production environments. You need to ensure that the same Docker image is deployed to each of these environments.\nHow should you identify the Docker image in your build?","answer":"C","question_id":310,"url":"https://www.examtopics.com/discussions/google/view/36568-exam-professional-cloud-developer-topic-1-question-73/","question_images":[],"answers_community":["C (83%)","D (17%)"],"answer_description":"","isMC":true,"answer_ET":"C","exam_id":7}],"exam":{"provider":"Google","isImplemented":true,"name":"Professional Cloud Developer","isMCOnly":false,"lastUpdated":"11 Apr 2025","isBeta":false,"numberOfQuestions":338,"id":7},"currentPage":62},"__N_SSP":true}