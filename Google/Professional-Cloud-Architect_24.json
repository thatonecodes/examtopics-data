{"pageProps":{"questions":[{"id":"afe90zbcFQUC37lvagJ9","timestamp":"2021-06-03 14:08:00","discussion":[{"content":"C & E:\nC: Smaller the base image with minimum dependency faster the container will start\nE: Docker image build uses caching. Docker Instructions sequence matter because \napplication’s dependencies change less frequently than the Python code which will help to reuse the cached layer of dependency and only add new layer for code change for Python Source code.","timestamp":"1624700940.0","poster":"aviratna","upvote_count":"61","comment_id":"391093"},{"poster":"vincy2202","content":"C & E are the correct answers. \nKindly refer - https://www.docker.com/blog/intro-guide-to-dockerfile-best-practices/","upvote_count":"12","comment_id":"483830","timestamp":"1637549280.0"},{"comment_id":"1359295","content":"Selected Answer: AC\nA. Remove Python after running pip: This translates to using a multi-stage build. \nC. Use a slimmed-down base image like Alpine Linux: While the multi-stage build (option A) is the most powerful optimization, you can also change the final stage by using Alpine. A smaller base image means less data to download and store, resulting in faster builds and deployments.","poster":"halifax","timestamp":"1740062400.0","upvote_count":"1"},{"comment_id":"1339759","upvote_count":"2","timestamp":"1736730600.0","content":"Selected Answer: BE\ndo go against some others here i think B makes more sense that C\n\nC wont actually decrease build time as the docker repo should already have the base image downloaded. where as if there are indeed dependancies in requirements.txt that can be removed then B would reduce the build time for the install.","poster":"zacrparsons"},{"upvote_count":"4","comment_id":"1309907","content":"Selected Answer: CE\nC. Use a slimmed-down base image like Alpine Linux: The ubuntu:16.04 image is a full-fledged operating system, which means it's larger and takes longer to download and build. Alpine Linux is a minimal distribution designed for containers, resulting in significantly smaller images and faster deployments.\n\nE. Copy the source after the package dependencies (Python and pip) are installed: Docker builds images in layers. Each RUN, COPY, and ADD instruction creates a new layer. By copying the source code after installing dependencies, you can take advantage of Docker's caching mechanism. If your source code changes, only the layers related to the source code need to be rebuilt, not the layers related to dependencies.","timestamp":"1731315180.0","poster":"Ekramy_Elnaggar"},{"upvote_count":"1","comment_id":"1200707","content":"Hi everyone, I have a doubt. The question talks about \"deployment\" not \"build\". CE are more correct, in my opinion, to accelerate the build phase (and application management) rather than the simple deployment (with docker swarm, simple docker, kubernetes etc etc)","timestamp":"1713873120.0","poster":"alessandroGPC"},{"comment_id":"1103298","poster":"nickcin77","timestamp":"1703240640.0","content":"C&E\nC - Use small images.\nE - \"Try to make expensive steps appear near the beginning of the Dockerfile. Steps that change often should appear near the end of the Dockerfile\" \nhttps://docs.docker.com/build/cache/","upvote_count":"3"},{"timestamp":"1695600180.0","content":"Selected Answer: CE\nC is an obvious choice, as an optimized image will be much better for a container than the one in the file. For E, I found an explanation here about COPY that helped me confirm that's the other solution - https://test-dockerrr.readthedocs.io/en/latest/userguide/eng-image/dockerfile_best-practices/#:~:text=required%20files%20change.-,For%20example%3A,-COPY%20requirements.txt.","upvote_count":"1","comment_id":"1016247","poster":"jrisl1991"},{"upvote_count":"4","poster":"poplot321","content":"I know C & E makes the most sense, but the Dockerfile is has a bug. If you don't copy requirements.txt from source, you have nothing to pip install. The line after FROM should be: \n\nCOPY requirements.txt requirements.txt","comment_id":"945076","timestamp":"1688687280.0"},{"content":"The two actions that should be taken to optimize the Dockerfile for faster deployment times without adversely affecting the app's functionality are:\n\nB. Remove dependencies from requirements.txt: The requirements.txt file should only contain necessary dependencies to reduce the number of packages to be installed.\n\nC. Use a slimmed-down base image like Alpine Linux: The Alpine Linux image is smaller than Ubuntu and has a smaller attack surface, which reduces the container's build time and image size.\n\nTherefore, options A, D, and E are not correct as they do not directly address the issue of slow deployment times caused by a bloated Dockerfile.","comment_id":"854987","comments":[{"comment_id":"1016225","timestamp":"1695597960.0","content":"But how do you know that requirements.txt has other dependencies? We don't have the file to confirm if there are unnecessary dependencies. Assuming that we only have the necessary ones, removing dependencies would break the deployment.","poster":"jrisl1991","upvote_count":"1"}],"poster":"Badri9898","timestamp":"1680126060.0","upvote_count":"1"},{"content":"Selected Answer: CE\nC: use smaller image decrease pull time\nE: optimize build time using previous cache layer image. generate new layer only for a different app code and requirements\n\nA: can't remove python\nB: the developer choose right deps\nD: changing istance type don't directly reduce deploy time","upvote_count":"2","timestamp":"1679523240.0","poster":"alekonko","comment_id":"847561"},{"timestamp":"1671604200.0","content":"C & E\nUsing a slimmed-down base image like Alpine Linux can help reduce the size of your Docker image, which can lead to faster deployment times. Alpine Linux is a lightweight Linux distribution that is often used as a base image for Docker images because of its small size.\n\nAdditionally, copying the source code after installing the package dependencies can help reduce the image build time because the dependencies will only need to be installed once, rather than every time the source code is changed. This can lead to faster deployment times because the image build process will be faster.","poster":"omermahgoub","upvote_count":"7","comments":[{"content":"It is not recommended to remove dependencies from the requirements.txt file or remove Python after running pip, as this could adversely affect the functionality of the application. Similarly, using larger machine types for your Google Container Engine node pools may not directly affect the deployment times of your application, as the deployment times are primarily dependent on the size and complexity of the Docker image being deployed.","timestamp":"1671604200.0","upvote_count":"1","poster":"omermahgoub","comment_id":"751933"}],"comment_id":"751932"},{"poster":"sfsdeniso","content":"B & E\nbase image is already cached - so no improvement in build time\nB is about removing unnecessary dependencies and not about all of them\ni remember saw this question on google's site - BE are correct","upvote_count":"2","comment_id":"700675","timestamp":"1666341840.0"},{"comment_id":"696526","timestamp":"1665951600.0","poster":"AzureDP900","content":"CE and is perfect","upvote_count":"1"},{"content":"Selected Answer: CE\nC. Use a slimmed-down base image like Alpine Linux\nE. Copy the source after he package dependencies (Python and pip) are installed","timestamp":"1665648000.0","upvote_count":"1","comment_id":"693724","poster":"minmin2020"},{"comment_id":"635894","poster":"backhand","upvote_count":"1","timestamp":"1658643420.0","content":"vote C, E\nhttps://cloud.google.com/architecture/best-practices-for-building-containers"},{"comment_id":"625276","poster":"MathMedrado","content":"As far as I know, it is necessary to copy the requirements.txt first in order to run pip, it is not possible to run pip without first copying the requirements.txt. if they copy requirements.txt first then E would work.","timestamp":"1656594960.0","upvote_count":"2"},{"content":"Selected Answer: CE\nBy means of elimination A B, dont make sence.D . is optimizing the mackie not script. Hence we are left with C& E","upvote_count":"3","poster":"amxexam","timestamp":"1651930260.0","comment_id":"598132"},{"upvote_count":"1","timestamp":"1651390860.0","comment_id":"595488","poster":"potorange","content":"Selected Answer: CE\nC.Alpine is a ligweight Linux distro, with smaller image E. Pushing often changing files down the Dockerfile helps reducing image layers variations. Both help faster image pull operations"},{"comment_id":"551517","upvote_count":"5","content":"A -invalid\nB-dependencies are required\nC-it will help as it is one of the best practices to make use of lighter image if possible\nD-Not helpful\nE-is the best practice to do the steps that changes more frequently at the end . so copy . should be performed at last as it will be changing more frequently and we can make use of docker caching \nhence Answer E is most then C","timestamp":"1645328880.0","poster":"belly265"},{"poster":"aeme","timestamp":"1643347560.0","comment_id":"534393","upvote_count":"2","content":"What I don't understand is why alpine linux decreases deployment time? Because the base-image layer will be cached after first installation and won't change until a new os version is used. Therefore after first deployment it won't be faster. But giving more power to the machine that starts up the container would. So I clearly would argument for D & E"},{"comment_id":"512014","content":"C and E appear to be in conflict.\nIf you install the dependencies in advance, will the size increase?","timestamp":"1640770680.0","poster":"OrangeTiger","upvote_count":"1"},{"content":"C and E","comment_id":"493008","upvote_count":"1","timestamp":"1638517260.0","poster":"abhinavbihade"},{"poster":"IKGx1iGetOWGSjAQDD2x3","content":"pip installs on alpine will take forever as you'll end up having to compile a lot of stuff; best get rid of that `copy .` operation though... E makes sense if you're building locally in an environment with caching, but in a CI/CD system there will be no impact. Very strange question...","comment_id":"447889","upvote_count":"1","timestamp":"1632092220.0"},{"comment_id":"442176","timestamp":"1631219340.0","poster":"pabloinigo","upvote_count":"1","content":"But E could slow down the build, not the deployment, moving COPY at the end is not going to change the size, and DEPLOYMENT is going to the the same. BUILD, yes will be improved, but no DEPLOYMENT","comments":[{"upvote_count":"2","timestamp":"1643347380.0","comment_id":"534383","poster":"aeme","content":"Moving the copy cmd to the end drastically changes the size. Because all the layers (commands) AFTER the copy command will create a new layer for every build. This is problematic because caching won't work and so all the data added from apt, python install and pip have to be downloaded for every deployment."}]},{"content":"Still no idea why E is a good option. https://cloud.google.com/architecture/best-practices-for-building-containers","timestamp":"1630751280.0","poster":"JustJack21","comment_id":"439057","upvote_count":"1"},{"comment_id":"404181","poster":"yzy","comments":[{"content":"I don't understand either.","comment_id":"638857","timestamp":"1659037680.0","poster":"jay9114","upvote_count":"1"},{"upvote_count":"7","content":"Read aviratna's answer: you are more likely to change your python source code than your python dependencies.\nIf in your Dockerfile you place COPY . /src before RUN pip install -r requirements.txt, and if your source code changes, Docker will generate a different layer of the Docker image. This way the subsequent steps in the Dockerfile (like RUN pip install -r requirements.txt) will have to be repeated and the previously cached Docker image layer won't be used.","comment_id":"416788","timestamp":"1627557240.0","poster":"jackdbd","comments":[{"timestamp":"1659038100.0","comment_id":"638858","poster":"jay9114","upvote_count":"1","content":"This was helpful! Thank you."}]}],"upvote_count":"3","content":"Don't understand why the copy after the install will make the deployment faster, someone can explain to me plz ?","timestamp":"1626029400.0"},{"content":"Agree with C&E","poster":"bala786","timestamp":"1625698560.0","comment_id":"401293","upvote_count":"2"},{"poster":"victory108","comment_id":"398365","timestamp":"1625405160.0","upvote_count":"2","content":"C. Use a slimmed-down base image like Alpine Linux\nE. Copy the source after the package dependencies (Python and pip) is installed"},{"upvote_count":"2","timestamp":"1624423980.0","comment_id":"388496","poster":"Papafel","comments":[{"comment_id":"399524","timestamp":"1625528580.0","poster":"kopper2019","upvote_count":"4","content":"they just looked common sense for me"}],"content":"Can anyone help to explain why the correct answer is C & E?\nC. Use a slimmed-down base image like Alpine Linux \nE. Copy the source after he package dependencies (Python and pip) are installed"},{"upvote_count":"4","poster":"rishab86","content":"c and e looks correct to me.","comment_id":"373542","timestamp":"1622722080.0","comments":[{"comment_id":"381496","content":"What does e really help with?","upvote_count":"2","poster":"Pokchok","timestamp":"1623641100.0"}]}],"question_id":116,"url":"https://www.examtopics.com/discussions/google/view/54406-exam-professional-cloud-architect-topic-1-question-22/","answer_images":[],"unix_timestamp":1622722080,"topic":"1","exam_id":4,"answer_description":"","question_text":"One of the developers on your team deployed their application in Google Container Engine with the Dockerfile below. They report that their application deployments are taking too long.\n//IMG//\n\nYou want to optimize this Dockerfile for faster deployment times without adversely affecting the app's functionality.\nWhich two actions should you take? (Choose two.)","isMC":true,"question_images":["https://www.examtopics.com/assets/media/exam-media/04339/0008300001.png"],"answers_community":["CE (80%)","13%","7%"],"choices":{"E":"Copy the source after he package dependencies (Python and pip) are installed","A":"Remove Python after running pip","B":"Remove dependencies from requirements.txt","D":"Use larger machine types for your Google Container Engine node pools","C":"Use a slimmed-down base image like Alpine Linux"},"answer":"CE","answer_ET":"CE"},{"id":"p4MKex1AzHPZaEtZmHve","discussion":[{"poster":"ghitesh","comment_id":"38807","content":"Question Statement: You want to adjust your test and deployment procedures to avoid this problem in the future\n\nSo based on this, I think the option \"C\" is correct, since it is the only one talking about doing changes in the test environment.","upvote_count":"84","timestamp":"1578990900.0","comments":[{"comments":[{"poster":"alihabib","comment_id":"1320767","content":"\"Performance\" bug is a result of Load","upvote_count":"2","timestamp":"1733108520.0"}],"upvote_count":"3","content":"There is no indication given anywhere that the load is the problem or that the bugs are a result of load and not some other issue encountered when using a specific feature.","poster":"Sephethus","comment_id":"1229259","timestamp":"1718204100.0"},{"content":"C. Increase the load on your test and staging environments.\n\nAs you have pointed out in \"Question Statement\", I do not see C covering \"deployment procedures\". Test and Staging environment is more on testing, but not about deployment procedure to production.\n\nSo, the only option that cover test and deployment is D. (Yes, kind of unacceptable to have the users to do \"testing\", but we make it \"ok\" by calling it \"canary deployment\")","comments":[{"comment_id":"1044105","timestamp":"1697371500.0","content":"With canary deployment we expose the new version to a small portion of users. With this approach maybe we don't see performance bugs in the canary release, since we don't have the 100% of traffic on the canary. But when we migrate the 100% of traffic to the new release (previous canary) we can see performance bugs.","poster":"francescogugliottagm","upvote_count":"9"}],"poster":"VedaSW","upvote_count":"20","timestamp":"1601097420.0","comment_id":"187449"},{"poster":"Urban_Life","comment_id":"495442","upvote_count":"9","timestamp":"1638829560.0","content":"The answer is D"},{"content":"\"Your solution is producing performance bugs in production...\" - I don't see how \"D\" would help to detect performance bugs. \n- \"C\" looks more adequate.","timestamp":"1640860800.0","comment_id":"513253","poster":"RegisFTM","upvote_count":"19"}]},{"upvote_count":"38","comments":[{"poster":"nitinz","upvote_count":"7","content":"D, canary rollout","comment_id":"303484","comments":[{"content":"It has nothing to do with the \"performance bugs\"","poster":"michael_m","timestamp":"1660367520.0","comment_id":"646129","upvote_count":"1"}],"timestamp":"1614877200.0"},{"content":"According to the question, [Your solution is producing \"performance\" bugs in production], so I think it is about the load. Plus canary test will not reproduce the bugs related to high load, I vote for C","timestamp":"1660367280.0","upvote_count":"2","poster":"michael_m","comment_id":"646127"},{"comment_id":"151759","timestamp":"1596697680.0","poster":"Sreekey","content":"The question is about the performance of the existing Code that they did not detect in Test environments . This is not about new API release . In order to test the performance they should increase the load in test environment and hence answer C.","upvote_count":"18"},{"content":"C is the best","comment_id":"696528","timestamp":"1665951780.0","poster":"AzureDP900","upvote_count":"2"}],"content":"A wouldn't prevent the bugs, it would just avoid them. B would help with root-cause analysis because it'd be a smaller change to review. C would test the performance of the system at its peak processing rates, so this assumes the bugs in production only occur because of usage. D would allow you to test the new code against smaller user sets to see if it occurs then, and if it still does you know it is not because of more user responses. So it's a tossup between C and D, D would be the cheaper/quicker answer so I'd choose D first then C if it's because of usage.","poster":"Eroc","comment_id":"17273","timestamp":"1571953440.0"},{"content":"Selected Answer: C\nperformance = load","comment_id":"1360457","timestamp":"1740304260.0","upvote_count":"1","poster":"Yass92"},{"poster":"halifax","timestamp":"1740064260.0","upvote_count":"1","content":"Selected Answer: D\nNo matter how efficiently you load tests in the development stage, you will never catch 100% of possible bugs. This is why CI/CD was invented (option D: continuous development).","comment_id":"1359317"},{"comment_id":"1347944","content":"Selected Answer: C\nI have that king of questions as C and D would be correct.\nBut as it is said to \"adjust\", I would prefer C than setting a new canary deployment strategy.\nFurthermore in case D would a small set of users rises up the performance issue ? not sure.","poster":"hpf97","upvote_count":"1","timestamp":"1738074000.0"},{"poster":"RVivek","upvote_count":"1","content":"Selected Answer: C\nPerformance Bug (not functional). Hence increasing the load on test environment will help to identify and ifx the bugs","comment_id":"1336676","timestamp":"1736061660.0"},{"timestamp":"1733953740.0","poster":"deep316","comment_id":"1325279","content":"Selected Answer: C\nBugs are related to performance. So you would need to perform performance testing thoroughly.","upvote_count":"1"},{"comment_id":"1324756","content":"Selected Answer: D\nC could be correct but this can help identify performance bottlenecks, but it might not fully replicate the complexity and unpredictability of real-world production traffic.\nSo D is the most correct one.","timestamp":"1733866080.0","poster":"Daniaw","upvote_count":"1"},{"comment_id":"1320765","timestamp":"1733108460.0","poster":"alihabib","upvote_count":"1","content":"Selected Answer: C\n\"Performance\" bug, not a \"Development\" Bug, so it more aligns with C, to increase the load to determine performance related bugs"},{"upvote_count":"1","poster":"drinkwater","timestamp":"1732370580.0","comment_id":"1316683","content":"I think the right answer here is D. It applies a best best practice of the release managment like canary deplyments. \nWhy is not C; adding more load in staging and testing environments can help identify some performance issues, it is often impossible to replicate the exact conditions of a production environment."},{"upvote_count":"1","poster":"Ekramy_Elnaggar","timestamp":"1731317880.0","content":"Selected Answer: C\nGuys, you need to focus on the KEYWORDS in any question, it will help you to determine the best answer. The keyword for this question is \"Performance\", it is very clear that the load test on stage was not planned correctly (i.e/ lower than it should), so the performance bugs didn't appear, but when it comes to production with much bigger load the issues appear.","comment_id":"1309935"},{"upvote_count":"1","timestamp":"1729758840.0","comment_id":"1302369","content":"C & D both works but D make sense","poster":"nareshthumma"},{"timestamp":"1728814260.0","comment_id":"1296850","content":"Selected Answer: D\nIt's D","poster":"dfizban","upvote_count":"1"},{"content":"Selected Answer: C\nC because The performance issues in production might not have been seen in staging or test environments because the load (number of users, transactions, data volume, etc.) in those environments is not representative of the load in production. By increasing the load on your test and staging environments to match or exceed production levels, you can better simulate real-world conditions and catch performance issues before deployment","poster":"maxdanny","upvote_count":"2","timestamp":"1725353220.0","comment_id":"1277374"},{"poster":"Hungdv","upvote_count":"1","comment_id":"1262343","timestamp":"1723098300.0","content":"I will choose D. \nC: The question does not say the error caused by the load."},{"content":"Selected Answer: D\nWithout overthinking the wording, canary (and similar) deployment methodologies are often recommended in Google documentation, whereas increasing load in dev environments aren't. (My $0.02...)","comment_id":"1236350","poster":"Haigk","upvote_count":"3","timestamp":"1719234000.0"},{"upvote_count":"1","timestamp":"1717559760.0","comment_id":"1224491","content":"Selected Answer: C\nMe too! I can't see how a \"performance bug\" might be mitigated via a canary deployment.\nHowever, I see that C doesn't cover the \"deployment\" part of the question, then I deduce that the question is ambiguously formulated.","poster":"a2le"}],"answer_description":"","topic":"1","answer_ET":"C","choices":{"D":"Deploy changes to a small subset of users before rolling out to production","C":"Increase the load on your test and staging environments","A":"Deploy fewer changes to production","B":"Deploy smaller changes to production"},"isMC":true,"answer":"C","question_images":[],"url":"https://www.examtopics.com/discussions/google/view/7167-exam-professional-cloud-architect-topic-1-question-23/","answers_community":["C (62%)","D (34%)","4%"],"unix_timestamp":1571953440,"question_text":"Your solution is producing performance bugs in production that you did not see in staging and test environments. You want to adjust your test and deployment procedures to avoid this problem in the future.\nWhat should you do?","exam_id":4,"question_id":117,"answer_images":[],"timestamp":"2019-10-24 23:44:00"},{"id":"mEbuSoEft9bUQ2EyeR84","exam_id":4,"question_id":118,"answers_community":["D (96%)","4%"],"question_text":"A small number of API requests to your microservices-based application take a very long time. You know that each request to the API can traverse many services.\nYou want to know which service takes the longest in those cases.\nWhat should you do?","choices":{"A":"Set timeouts on your application so that you can fail requests faster","B":"Send custom metrics for each of your requests to Stackdriver Monitoring","D":"Instrument your application with Stackdriver Trace in order to break down the request latencies at each microservice","C":"Use Stackdriver Monitoring to look for insights that show when your API latencies are high"},"unix_timestamp":1577254680,"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/10898-exam-professional-cloud-architect-topic-1-question-24/","answer_images":[],"timestamp":"2019-12-25 07:18:00","isMC":true,"question_images":[],"discussion":[{"comments":[{"upvote_count":"9","content":"D is ok","timestamp":"1596687900.0","poster":"tartar","comment_id":"151675"},{"poster":"nitinz","comment_id":"303485","content":"D, trace is just for latency testing.","upvote_count":"4","timestamp":"1614877260.0"}],"poster":"euclid","comment_id":"32526","upvote_count":"23","timestamp":"1577254680.0","content":"D is correct !"},{"comment_id":"751936","upvote_count":"10","content":"D. Instrument your application with Stackdriver Trace in order to break down the request latencies at each microservice\n\nStackdriver Trace is a distributed tracing system that allows you to understand the relationships between requests and the various microservices that they touch as they pass through your application. By instrumenting your application with Stackdriver Trace, you can get a detailed breakdown of the latencies at each microservice, which can help you identify which service is taking the longest in those cases where a small number of API requests take a very long time.\n\nSetting timeouts on your application or sending custom metrics to Stackdriver Monitoring may not provide the level of detail that you need to identify the specific service that is causing the latency issues. Looking for insights in Stackdriver Monitoring may also not provide the necessary level of detail, as it may not show the individual latencies at each microservice.","timestamp":"1671604500.0","poster":"omermahgoub"},{"content":"Selected Answer: D\nTo identify latency in a microservice chain, only Trace can be useful, so D","poster":"hpf97","upvote_count":"1","timestamp":"1738074120.0","comment_id":"1347946"},{"content":"Selected Answer: D\nThe service is named Cloud trace now","comment_id":"1332871","upvote_count":"1","timestamp":"1735375080.0","poster":"Ishu_awsguy"},{"poster":"Ekramy_Elnaggar","content":"Selected Answer: D\n1. Distributed Tracing: Stackdriver Trace (now called Cloud Trace) is specifically designed to analyze the performance of requests as they travel through multiple services in a microservices architecture. It helps you pinpoint exactly where latency is occurring.\n2. Detailed Breakdown: Cloud Trace will provide a detailed breakdown of how long each microservice takes to process the request, allowing you to identify the bottleneck quickly.\n3. Visualization: Cloud Trace provides visualizations like flame graphs and waterfall diagrams that make it easy to see the flow of the request and identify slow services.","upvote_count":"3","comment_id":"1309943","timestamp":"1731318840.0"},{"upvote_count":"1","poster":"Hungdv","timestamp":"1723098660.0","content":"Choose D","comment_id":"1262344"},{"comment_id":"1218515","poster":"Robert0","upvote_count":"4","content":"Selected Answer: D\nThis question is not updated. It will be refered as Cloud Trace as part of Google Cloud Operation suite","timestamp":"1716662100.0"},{"upvote_count":"2","comment_id":"1218510","content":"This question is not updated. Stackdriver is now called \"Google Cloud operations\"","timestamp":"1716661920.0","poster":"Robert0"},{"comment_id":"963773","poster":"eka_nostra","timestamp":"1690378320.0","content":"Selected Answer: D\nTrace is a signal that can be used to follow the program's data and flow, including the duration of the program's components.","upvote_count":"2"},{"comments":[{"comment_id":"912281","content":"even got confused with the C ... after reading the conversation and reference doc D make sense to me.","timestamp":"1685638140.0","upvote_count":"1","poster":"LaxmanTiwari"}],"timestamp":"1685638080.0","content":"D should be correct, the headline in GC trace documentation says it all: \"Cloud Trace is a distributed tracing system for Google Cloud that collects latency data from applications and displays it in near real-time in the Google Cloud Console.\"","upvote_count":"2","comment_id":"912278","poster":"LaxmanTiwari"},{"content":"Selected Answer: D\nD is the correct answer","upvote_count":"1","comment_id":"847565","poster":"alekonko","timestamp":"1679523420.0"},{"comment_id":"827877","timestamp":"1677839760.0","upvote_count":"1","poster":"MestreCholas","content":"Why not C?"},{"poster":"surajkrishnamurthy","content":"Selected Answer: D\nD is the correct answer","upvote_count":"1","comment_id":"749832","timestamp":"1671454980.0"},{"poster":"AniketD","upvote_count":"1","content":"Selected Answer: D\nStacdrive Trace would trace the APIs and helps to identify the bottleneck","comment_id":"721936","timestamp":"1668853980.0"},{"poster":"kchandank","content":"Selected Answer: D\nD is correct trace would report to the latency","timestamp":"1668756780.0","upvote_count":"1","comment_id":"721140"},{"upvote_count":"1","poster":"Mahmoud_E","timestamp":"1666461360.0","comment_id":"701701","content":"D is correct trace would report to the latency"},{"comment_id":"693737","comments":[{"content":"this is the best option to find more details","timestamp":"1665951900.0","comment_id":"696529","poster":"AzureDP900","upvote_count":"1"}],"poster":"minmin2020","upvote_count":"2","content":"Selected Answer: D\nD. Instrument your application with Stackdriver Trace in order to break down the request latencies at each microservice","timestamp":"1665649020.0"},{"upvote_count":"2","poster":"abirroy","content":"Selected Answer: D\nInstrument your application with Stackdriver Trace in order to break down the request latencies at each microservice","comment_id":"651069","timestamp":"1661307360.0"},{"content":"Selected Answer: B\nD is correct !","upvote_count":"1","timestamp":"1660719780.0","poster":"[Removed]","comment_id":"647959"},{"content":"Selected Answer: D\nLatency issue in your applicant. trace loy is way to go. Hence D","comment_id":"598137","upvote_count":"1","poster":"amxexam","timestamp":"1651930980.0"},{"timestamp":"1650461940.0","content":"Selected Answer: D\nD is best.","poster":"Nirca","upvote_count":"1","comment_id":"588759"},{"comment_id":"566268","content":"What is used to trace functions… stack trace","upvote_count":"1","poster":"SAMBIT","timestamp":"1647102780.0"},{"comment_id":"564864","poster":"kuszner","upvote_count":"1","timestamp":"1646926020.0","content":"Selected Answer: D\nno Doubt ..."},{"content":"Go for D","upvote_count":"3","comment_id":"493580","timestamp":"1638606000.0","poster":"haroldbenites"},{"comment_id":"483840","poster":"vincy2202","upvote_count":"2","content":"D is the right answer.","timestamp":"1637552340.0"},{"poster":"victory108","upvote_count":"2","content":"D. Instrument your application with Stackdriver Trace in order to break down the request latencies at each microservice","comment_id":"360265","timestamp":"1621326540.0"},{"content":"D is correct","comment_id":"353959","timestamp":"1620666960.0","poster":"un","upvote_count":"1"},{"comment_id":"329275","poster":"Ausias18","upvote_count":"1","content":"Answer is D","timestamp":"1617683400.0"},{"upvote_count":"1","poster":"lynx256","timestamp":"1617097620.0","content":"D is ok","comment_id":"324157"},{"timestamp":"1611520500.0","upvote_count":"3","content":"Stackdriver Trace is like DynaTrace PurePath. That's the tool we should use for tracing the API latencies on each hop.","poster":"bnlcnd","comment_id":"275470"},{"comment_id":"259578","content":"Correct is D.","poster":"HBM","upvote_count":"1","timestamp":"1609781880.0"},{"upvote_count":"1","comment_id":"179482","content":"D - Stackdriver Trace","poster":"AshokC","timestamp":"1600107240.0"},{"comment_id":"117108","content":"Stackdriver Trace is what we need to understand the latency of our app. So D is the correct.","poster":"mlantonis","upvote_count":"3","timestamp":"1592892240.0"},{"poster":"Tushant","timestamp":"1592631060.0","comment_id":"114448","upvote_count":"2","content":"D is the correct answer."},{"comment_id":"100866","poster":"Nirms","upvote_count":"2","content":"D is the correct answer","timestamp":"1591105440.0"},{"comment_id":"98336","timestamp":"1590766680.0","poster":"Ziegler","content":"Di is the correct answer","upvote_count":"1"},{"content":"Final Decision to go with Option D","poster":"AD2AD4","upvote_count":"2","timestamp":"1590737280.0","comment_id":"98090"},{"timestamp":"1590645780.0","comment_id":"97294","content":"they are talking about latency BETWEEN services, so i think answer is C:\nhttps://cloud.google.com/blog/products/gcp/drilling-down-into-stackdriver-service-monitoring","poster":"cwbat","upvote_count":"3"},{"upvote_count":"3","timestamp":"1590400080.0","comment_id":"95316","poster":"wazza88","content":"D should be correct, the headline in GC trace documentation says it all: \"Cloud Trace is a distributed tracing system for Google Cloud that collects latency data from applications and displays it in near real-time in the Google Cloud Console.\""},{"comments":[{"timestamp":"1590195900.0","content":"Hey, GunjGupta, Did you appear for the exam. If so how was it and how difficult it is. Any tips would help me a lot...","poster":"Griff0","comment_id":"94118","upvote_count":"1"}],"upvote_count":"2","timestamp":"1589386200.0","poster":"GunjGupta","content":"will go for D as question is about tracing the latency of the services","comment_id":"88366"},{"poster":"gcp_aws","content":"D is the correct answer","timestamp":"1589317140.0","upvote_count":"2","comment_id":"87957"},{"timestamp":"1584147960.0","poster":"vindahake","upvote_count":"3","content":"D \nTracing is correct","comment_id":"63677"},{"content":"answer: D","poster":"2g","comment_id":"44717","upvote_count":"3","timestamp":"1580390520.0"}],"topic":"1","answer":"D","answer_ET":"D"},{"id":"tT7dh2GCI4sb8kPH9FT3","exam_id":4,"question_id":119,"question_text":"During a high traffic portion of the day, one of your relational databases crashes, but the replica is never promoted to a master. You want to avoid this in the future.\nWhat should you do?","url":"https://www.examtopics.com/discussions/google/view/7118-exam-professional-cloud-architect-topic-1-question-25/","answer":"D","topic":"1","question_images":[],"timestamp":"2019-10-24 10:19:00","answer_description":"","choices":{"B":"Choose larger instances for your database","A":"Use a different database","C":"Create snapshots of your database more regularly","D":"Implement routinely scheduled failovers of your databases"},"answers_community":["D (68%)","B (32%)"],"isMC":true,"answer_ET":"D","discussion":[{"comment_id":"24701","timestamp":"1574810760.0","upvote_count":"100","poster":"Narigdo","comments":[{"upvote_count":"20","poster":"Jos","timestamp":"1576249080.0","content":"Yep, +1 for D","comment_id":"29365"}],"content":"Answer is D"},{"upvote_count":"45","content":"@chiar, I agree the question i s not clear. In GCP larger instances have larger number of CPUs, Memory and come with their own private network. So increases the instance size would help prevent the need for failover during high traffic times. However, routinely scheduled failovers would allow the team to test the failover when it is not requried. This would make sure it is working when it is required.","timestamp":"1572006900.0","comments":[{"poster":"Shariq","content":"exactly how do you know the optimal size. it will be a guess. answer should be D","comment_id":"26113","timestamp":"1575335940.0","upvote_count":"9"}],"poster":"Eroc","comment_id":"17371"},{"comment_id":"1347945","content":"Selected Answer: D\nD : Chaos testing","upvote_count":"1","timestamp":"1738074060.0","poster":"hpf97"},{"upvote_count":"1","content":"Selected Answer: D\nD - I think question is related to resiliency & disaster recovery, no matter how big or small the instance is.. Is the application able to achieve its RTO & RPO ?.. We don't know what cause the crash, but we could have avoided it if we had a Routine Scheduled Failovers Check done in past rather than anticipating & waiting for a real crash.","timestamp":"1737806700.0","poster":"alihabib","comment_id":"1346431"},{"upvote_count":"1","timestamp":"1733120940.0","poster":"alihabib","content":"Selected Answer: D\nOnly a routine check, would indicate a best practice. Larger Instance doesn't guarantee a failover would be prevented","comment_id":"1320807"},{"content":"Selected Answer: D\n1. Proactive Testing: Regularly scheduled failovers proactively test your database's ability to recover and ensure the replica can successfully take over as the master. This helps identify and address any issues in the failover process before a real crisis occurs.\n\n2. Confidence in Failover: By routinely testing failover, you gain confidence that your system can recover automatically in case of a primary database failure, minimizing downtime and data loss.\n\n3. Improved Recovery Time: Regular failovers help optimize the recovery process, reducing the time it takes to switch to the replica.","poster":"Ekramy_Elnaggar","comment_id":"1309944","upvote_count":"3","timestamp":"1731319380.0"},{"timestamp":"1730565780.0","comment_id":"1306244","content":"Selected Answer: D\nAnswer is D","upvote_count":"1","poster":"beagle_Masato"},{"upvote_count":"3","comment_id":"1220179","content":"Selected Answer: D\n- **A. Use a different database**: Simply switching to a different database does not inherently solve the problem of failover and promotion mechanisms. The issue is more about the setup and management of failover strategies rather than the specific database technology used.\n\n- **B. Choose larger instances for your database**: While using larger instances might improve performance and potentially reduce the risk of a crash due to resource constraints, it does not address the failover mechanism. The key problem is the replica not being promoted, which larger instances alone won't fix.\n\n- **C. Create snapshots of your database more regularly**: Regular snapshots are useful for backups and recovery but do not help with automatic failover and high availability. Snapshots do not ensure that a replica will be promoted to a master if the primary fails.","timestamp":"1716895320.0","poster":"james2033"},{"comment_id":"1215728","upvote_count":"1","content":"I think the answer is B, as the most important thing is customer experience. We can NOT expect database fails as a normal event which have direct customer and business impacts (if current data base fail because of load then replica database may fail as well).\n\nOf course we need to setup the failover process to work, but the more important task will be to increase database load first, thus I choose B.","poster":"huuthanhdlv","timestamp":"1716382320.0"},{"poster":"Jen3","comment_id":"1167578","upvote_count":"1","timestamp":"1709773080.0","content":"I think the answer is D because we can not assume the crash was due to load.","comments":[{"comment_id":"1167579","upvote_count":"2","poster":"Jen3","timestamp":"1709773200.0","content":"Further the issue isn't that a crash occurred, the issue is the contingency that was in place didn't kick in. Hence D as my choice."}]},{"upvote_count":"2","comment_id":"1128772","poster":"ashishdwi007","timestamp":"1705934820.0","content":"Selected Answer: D\nD is correct with given choice, because what if larger instance size is also failed. How ever question ignores logging and networking completely, the best way is to use logging why the DB is crashed, failover is just a remedy to solve the issues at current, not solving the problem itself."},{"comment_id":"1114221","poster":"discuss24","content":"It is important to identify key words, the issue is replica not being promoted when primary instance fails. Regular testing would identify issue","timestamp":"1704423840.0","upvote_count":"2"},{"comment_id":"1114134","upvote_count":"1","timestamp":"1704412320.0","content":"The correct answer is D\nThe question is asking how to avoid the situation of not failing over. The answer is to test the failover procedure. The question does *NOT*ask about what happens in the future and whether the secondary node is large enough.","poster":"AWS_Sam"},{"poster":"pakilodi","content":"Selected Answer: D\nI would say D. Beacuse also B is correct, but you have a replica here, that is never promoted. So we need failover strategy.","timestamp":"1701411060.0","comment_id":"1084978","upvote_count":"1"},{"content":"Selected Answer: B\nMy Answer is B\nWhy not D ?\nI think\n- Each day we cannot know. What times of the day are peak usage times? Implementing routinely scheduled failovers won't solve the problem.\n- if Implement routinely scheduled failovers of your databases but replica database server is a same spec with main database server , replica database server will crashes by high traffic portion same a main database server","comment_id":"1073026","poster":"owenshinobi","timestamp":"1700195640.0","upvote_count":"2"},{"timestamp":"1699392480.0","content":"Selected Answer: D\nAnswer is D. Implement routinely scheduled failovers of your databases\n\nThis option is most aligned with addressing the issue. Routine failovers can help ensure that the failover process is working correctly and that the system is resilient to crashes. It can be part of a disaster recovery plan, where you routinely test the failover to the replica to ensure that it can handle being promoted to a master if needed. For the above reasons, i believe D is correct.","upvote_count":"1","comment_id":"1065181","poster":"Nora9"},{"content":"Why not B? \n\nUsing a large instance during low traffic hours means incurring more cost than benefit except the instance is elastic. Therefore using a larger instance is not cost effective and the answer is D.","poster":"piiizu","timestamp":"1695579480.0","comment_id":"1016085","upvote_count":"1"}],"answer_images":[],"unix_timestamp":1571905140},{"id":"5w66iQnvlSdI66POxHfS","question_id":120,"question_text":"Your organization requires that metrics from all applications be retained for 5 years for future analysis in possible legal proceedings.\nWhich approach should you use?","answer_images":[],"discussion":[{"content":"D is correct and best practice for long term log storage","comment_id":"21548","upvote_count":"151","poster":"JoeShmoe","timestamp":"1573743360.0","comments":[{"upvote_count":"1","timestamp":"1719231960.0","poster":"AndreaMa","content":"same for me. The best approach for the long time log is to export it from monitoring to Cloud Storage Archival type","comment_id":"1236325"},{"timestamp":"1615989060.0","comments":[{"comment_id":"518784","content":"+1 Due to long term storage, cloud storage is better answer than BigQuery","timestamp":"1641534060.0","upvote_count":"14","poster":"anjuagrawal"}],"poster":"AndreUanKenobi","upvote_count":"22","content":"+1. For archival purposes, Customer should use Cloud Storage. BigQuery is a datawarehouse, and could eventually import data from Cloud Storage if necessary.","comment_id":"313318"}]},{"comment_id":"34840","content":"A and C can be quickly ruled out because none of them is solution for the requirements \"retained for 5 years\"\n\nBetween B and D, the different is where to store, BigQuery or Cloud Storage. Since the main concern is extended storing period, D (Correct Answer) is better choice, and the \"retained for 5 years for future analysis\" further qualifies it, for example, using Coldline storage class.\n\nWith regards of BigQuery, while it is also a low-cost storage, but the main purpose is for analysis. Also, logs stored in Cloud Storage is easy to transport to BigQuery or do query directly against the files saved in Cloud Storage if and whenever needed.","upvote_count":"74","poster":"MeasService","comments":[{"comments":[{"comment_id":"35202","poster":"Shyeom","timestamp":"1578136500.0","upvote_count":"4","content":"I mean answer : D"}],"upvote_count":"2","comment_id":"35200","content":"point : organization requires that metrics from all applications be retained for 5 years","poster":"Shyeom","timestamp":"1578136440.0"},{"upvote_count":"5","timestamp":"1601839500.0","content":"If you have 2 viable solutions (B&D), then always chose the one that is cost optimised - I chose D","comments":[{"upvote_count":"11","comment_id":"571197","content":"Bigquery long term storage cost: $0.020 per GB\nCloud Storage archive cost: $0,0012 per GB\nOnly if metrics need less than 10 GB (free service part on Bigquery) then the correct solution will be B... But all metrics for all applications during more than 5 years... I think never will be the case :D","timestamp":"1647712320.0","poster":"jvale"}],"comment_id":"193158","poster":"Cloudy_Apple_Juice"},{"poster":"Vika","comment_id":"297999","content":"second that! like the way u explained..","timestamp":"1614148200.0","upvote_count":"1"},{"upvote_count":"18","comments":[{"poster":"bnlcnd","upvote_count":"5","timestamp":"1611522120.0","comment_id":"275493","content":"This is a good example. thanks.\nBut, we can easily change that implementation to dump the metrics to buckets to save lots of money. And, when talking about legal purpose, 1 hour interval may not be enough. You may have to keep more frequent metrics. So, only cold line or archive work for that purpose."}],"timestamp":"1607258400.0","poster":"trainor","content":"The question is about metrics, not logs. I'd go for B.\nSee https://cloud.google.com/solutions/stackdriver-monitoring-metric-export","comment_id":"236420"}],"timestamp":"1578039480.0"},{"timestamp":"1743527640.0","poster":"gaufchamp","content":"Selected Answer: B\nStackdriver Monitoring (now known as Google Cloud Monitoring) allows you to collect metrics from all your applications and services.\n\nBy exporting the metrics to BigQuery, you ensure long-term storage and analysis capability. BigQuery allows you to store vast amounts of data (such as metrics) and retain it for extended periods, like 5 years, while providing powerful querying capabilities for future analysis, including in legal proceedings.\n\nBigQuery is ideal because it allows you to query the data at any time, and it is designed for long-term storage and analysis of large datasets.","upvote_count":"1","comment_id":"1418585"},{"content":"Selected Answer: B\nthe question specifically mentions \"future analysis in possible legal proceedings.\" This makes BigQuery the most appropriate choice, even if it's more expensive. option D is cheaper (cloud storage), but data stored at cloud storage (object storage) can not be analysed.","comment_id":"1359354","upvote_count":"1","poster":"halifax","timestamp":"1740069060.0"},{"content":"Selected Answer: D\nD : Stackdriver for logging, then Storage with data policy for moving data to less expensive areas. And data is restored in BigQuery only if legal investigation required.","poster":"hpf97","comment_id":"1347963","timestamp":"1738075140.0","upvote_count":"1"},{"comment_id":"1320810","content":"Selected Answer: D\nSince the data is not actively queried, and cited as \"possible\" use qualifies it to be archived. Which means Cloud Storage","timestamp":"1733121240.0","upvote_count":"1","poster":"alihabib"},{"timestamp":"1732371600.0","poster":"drinkwater","comment_id":"1316686","content":"B is the right answer \nwhy is not D, because while Google Cloud Storage can handle long-term storage, it is less efficient than BigQuery for analysis. Retrieving and querying metrics from Cloud Storage would require additional tools or steps, making it less suitable for the described use case.","upvote_count":"1"},{"comment_id":"1312837","timestamp":"1731711360.0","upvote_count":"1","content":"B in my opinion.\nTake a look: https://cloud.google.com/architecture/monitoring-metric-export#store_metrics \nBQ is also long-term storage with option to reduce cost of older data.\nhttps://cloud.google.com/bigquery/docs/best-practices-storage","poster":"alpay"},{"comment_id":"1309945","content":"Selected Answer: B\n1. Long-term Retention: BigQuery is a data warehouse designed for long-term storage and analysis of large datasets. It's the ideal place to store metrics for 5 years to meet your organization's legal requirements.\n2. Cost-Effective: BigQuery's storage pricing is very competitive, especially for long-term data retention.\n3. Analysis and Reporting: BigQuery provides powerful tools for analyzing and querying data, making it easy to extract insights and generate reports from the stored metrics.\n4. Integration: Stackdriver Monitoring (now Cloud Monitoring) can be easily configured to export metrics to BigQuery.\n\nD is not correct as while Cloud Storage can store data for long periods, it's not optimized for querying and analyzing data like BigQuery.","timestamp":"1731320100.0","upvote_count":"1","poster":"Ekramy_Elnaggar"},{"comment_id":"1277099","timestamp":"1725328740.0","poster":"VedaSW","content":"I go for B, as the question is about 5 years worth of data \"for future analysis in possible legal proceedings\", and the \"future\" can be next day, based on when the legal proceeding happen.\nIt is not about long term log storage.\nEven the argument of \"future\" means 100 years later, the Cold Storage Archival still does not fulfill the \"analysis\" portion of the requirements.\nYou will need to move the data from Cold Storage to BigQuery for the analysis.\nSo the ideal answer should be combination of D and B, but we do not have such option, hence the answer can meet all requirements is B.","upvote_count":"1"},{"content":"D is answer. Monitoring has only 24 months retention.","upvote_count":"1","timestamp":"1723099800.0","poster":"Hungdv","comment_id":"1262353"},{"content":"Selected Answer: D\nfor storing the logs you need cloud storage.","comments":[{"upvote_count":"1","comment_id":"1336537","timestamp":"1736024160.0","content":"The question didn't mention logs.","poster":"ryaryarya"}],"poster":"joecloud12","comment_id":"1261643","upvote_count":"1","timestamp":"1722944160.0"},{"timestamp":"1721973540.0","poster":"monus","comment_id":"1255489","content":"Selected Answer: B\nB should be correct. How can Cloud Storage analyze the data?","comments":[{"timestamp":"1722379080.0","poster":"desertlotus1211","comment_id":"1258443","content":"The statement is not about the actual analysis of the data, but 'where' to store the data for future analysis. Who know when that will be??? So GCS is the best answer. When need be, it can be import into BQ","upvote_count":"1","comments":[{"timestamp":"1722379140.0","comment_id":"1258444","upvote_count":"1","content":"Also it said retained for 5 years for future analysis... you don't store in BQ","poster":"desertlotus1211"}]}],"upvote_count":"1"},{"timestamp":"1717610520.0","comment_id":"1224906","upvote_count":"1","content":"Selected Answer: D\nI mean, \"for possible future legal proceedings\", I think that immutable storage that grants data integrity is the best option here, what's more, it's also the cheapest one...","poster":"a2le"},{"content":"Selected Answer: D\nGoogle Cloud Storage for 5 years storing legally.","comment_id":"1220183","poster":"james2033","timestamp":"1716895620.0","upvote_count":"1"},{"timestamp":"1712837700.0","comment_id":"1193755","upvote_count":"1","content":"Selected Answer: D\nbest practice for long term log storage","poster":"arrase"},{"content":"Selected Answer: B\nI think B is the right answer because transferring the data could be a basis for discrediting the data for legal use. Since BIg Query can store the data and retain it with all the metadata intact, I will go for it.","poster":"tosinogunfile","comment_id":"1140574","timestamp":"1707085980.0","upvote_count":"1"}],"answer_ET":"D","choices":{"C":"Configure Stackdriver Monitoring for all Projects with the default retention policies","D":"Configure Stackdriver Monitoring for all Projects, and export to Google Cloud Storage","A":"Grant the security team access to the logs in each Project","B":"Configure Stackdriver Monitoring for all Projects, and export to BigQuery"},"answers_community":["D (73%)","B (27%)"],"answer":"D","unix_timestamp":1571957160,"exam_id":4,"url":"https://www.examtopics.com/discussions/google/view/7172-exam-professional-cloud-architect-topic-1-question-26/","timestamp":"2019-10-25 00:46:00","topic":"1","isMC":true,"question_images":[],"answer_description":""}],"exam":{"provider":"Google","numberOfQuestions":279,"id":4,"lastUpdated":"11 Apr 2025","isMCOnly":false,"isBeta":false,"name":"Professional Cloud Architect","isImplemented":true},"currentPage":24},"__N_SSP":true}