{"pageProps":{"questions":[{"id":"ptWzgcLg5y5bSz8vncey","timestamp":"2021-06-01 20:11:00","question_images":[],"question_text":"You support a Node.js application running on Google Kubernetes Engine (GKE) in production. The application makes several HTTP requests to dependent applications. You want to anticipate which dependent applications might cause performance issues. What should you do?","discussion":[{"poster":"Manh","timestamp":"1636357080.0","upvote_count":"11","content":"Answer is B.\n\nThe keyword is \"make several requests to dependent app\". So you need trace for it.\n\nCloud Trace\nFind performance bottlenecks in production.\n\nCloud Profiler\nContinuous CPU and heap profiling to improve performance and reduce costs.","comment_id":"474156"},{"upvote_count":"6","comment_id":"393244","poster":"Charun","content":"I have submitted B answer","timestamp":"1624915560.0"},{"timestamp":"1742984040.0","comment_id":"1410312","content":"Selected Answer: B\nanswer is B","poster":"kirthi0771","upvote_count":"1"},{"poster":"Paxtons_Aunders","upvote_count":"1","comment_id":"1401476","timestamp":"1742544660.0","content":"Selected Answer: B\nB. Instrument all applications with Stackdriver Trace and review inter-service HTTP requests.\n\nhttps://docs.google.com/document/d/1VV6vkkjShXDgPLSG6V_7-0dweLmZTUnYiTSxo6C5ERY/edit?tab=t.0\n\nStackdriver Trace allows you to collect and analyze performance data for all the applications that make up your system, including the applications running on GKE. By instrumenting your application with Stackdriver Trace, you can see detailed performance information for each request, including the time spent in each component of your system, as well as any inter-service HTTP requests. This will allow you to identify which dependent applications might be causing performance issues, so that you can focus on optimizing those services specifically."},{"upvote_count":"4","timestamp":"1727341860.0","comment_id":"771993","content":"Selected Answer: B\nB. Instrument all applications with Stackdriver Trace and review inter-service HTTP requests.\n\nStackdriver Trace allows you to collect and analyze performance data for all the applications that make up your system, including the applications running on GKE. By instrumenting your application with Stackdriver Trace, you can see detailed performance information for each request, including the time spent in each component of your system, as well as any inter-service HTTP requests. This will allow you to identify which dependent applications might be causing performance issues, so that you can focus on optimizing those services specifically.","poster":"JonathanSJ"},{"content":"correct","upvote_count":"1","timestamp":"1722851220.0","poster":"quismorioej","comment_id":"1261010"},{"upvote_count":"1","timestamp":"1714798740.0","content":"Selected Answer: B\nAgree with B","poster":"dija123","comment_id":"1206345"},{"comment_id":"1060542","upvote_count":"3","poster":"Jason_Cloud_at","timestamp":"1698928080.0","content":"Selected Answer: B\nRight Answer, Cleared the exam yesterday , All the questions were from this study material , Always go with community answers and prepare well."},{"upvote_count":"1","timestamp":"1694450040.0","poster":"LowJi","content":"B is right answer.","comment_id":"1004989"},{"timestamp":"1692173160.0","comment_id":"982316","poster":"tuanuv1","content":"Answer is B.","upvote_count":"1"},{"timestamp":"1690861260.0","comment_id":"968691","upvote_count":"1","content":"B is correct answer.","poster":"Hiren_Meghnathi"},{"content":"B is the answer","comment_id":"963793","upvote_count":"1","poster":"Pulk","timestamp":"1690379520.0"},{"timestamp":"1671865560.0","poster":"floppino","comment_id":"754737","upvote_count":"1","content":"Selected Answer: B\nAns: B\nKeyword: HTTP requests -> Trace"},{"timestamp":"1670314980.0","comment_id":"736617","upvote_count":"1","content":"Selected Answer: B\nSubmitted B on the exam","poster":"chelbsik"},{"content":"Selected Answer: B\nB is the answer.\n\nhttps://cloud.google.com/trace/docs/overview\nCloud Trace, a distributed tracing system for Google Cloud, helps you understand how long it takes your application to handle incoming requests from users or other applications, and how long it takes to complete operations like RPC calls performed when handling the requests.","timestamp":"1666612020.0","poster":"zellck","upvote_count":"1","comment_id":"702977"},{"content":"B is right answer.","timestamp":"1666551180.0","comment_id":"702405","poster":"AzureDP900","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: B\nThe answer is B.","timestamp":"1660395180.0","comment_id":"646331","poster":"GCP72"},{"poster":"Sav94","content":"The answer is B.","upvote_count":"1","comment_id":"589890","timestamp":"1650622680.0"},{"timestamp":"1634640900.0","poster":"WakandaF","comments":[{"upvote_count":"1","poster":"giammydell","content":"For selecting SLO target i will select C ( because D it is a too relexed for a SLA), but I'm not sure B could also be a good choice if you want a more challenging SLO","comment_id":"469030","timestamp":"1635401640.0"}],"content":"You need to define Service Level Objectives (SLOs) for a high-traffic multi-region web\napplication. Customers expect the application to always be available and have fast response times.\nCustomers are currently happy with the application performance and availability. Based on current\nmeasurement, you observe that the 90th percentile of latency is 120ms and the 95th percentile of\nlatency is 275ms over a 28-day window. What latency SLO would you recommend to the team to\npublish?\nA. 90th percentile - 100ms\n95th percentile - 250ms\nB. 90th percentile - 120ms\n95th percentile - 275ms\nC. 90th percentile - 150ms\n95th percentile - 300ms\nD. 90th percentile - 250ms\n95th percentile - 400ms","comment_id":"464610","upvote_count":"4"},{"comments":[{"poster":"JonathanSJ","comment_id":"771984","content":"B. Assign all instances a label specific to the system they run. Configure BigQuery billing export and query costs per label.\n\nOne way to determine the cost of running each system on GCP is to use labels to tag each Compute Engine instance with the specific system it runs. Then, you can configure BigQuery billing export to export the billing data to BigQuery. This will allow you to use SQL queries to break down the costs based on the labels and see how much each system costs. You can use the GCP Console or the gcloud command-line tool to assign labels to instances. Once you've done that, you can configure BigQuery billing export in the console, and create a Bigquery table to store the exported billing data.","timestamp":"1673408160.0","upvote_count":"1"}],"poster":"WakandaF","comment_id":"464194","content":"You manage several production systems that run on Compute Engine in the same Google\nCloud Platform (GCP) project. Each system has its own set of dedicated Compute Engine instances.\nYou want to know how must it costs to run each of the systems. What should you do?\nA. In the Google Cloud Platform Console, use the Cost Breakdown section to visualize the costs per\nsystem.\nB. Assign all instances a label specific to the system they run. Configure BigQuery billing export and\nquery costs per label.\nC. Enrich all instances with metadata specific to the system they run. Configure Stackdriver Logging\nto export to BigQuery, and query costs based on the metadata.\nD. Name each virtual machine (VM) after the system it runs. Set up a usage report export to a Cloud\nStorage bucket. Configure the bucket as a source in BigQuery to query costs based on VM name","upvote_count":"3","timestamp":"1634572020.0"},{"comment_id":"440930","content":"You are deploying an application that needs to access sensitive information. You need to ensure that this informatioins\nencrypted and the risk of exposure is minimal if a breach occurs. What should you do?\nA.) Store the encryption keys in Cloud Key Management Service (KMS) and rotate the keys frequently.\nB. _> Inject the secret at the time of instance creation via an encrypted configuration management system.\nC._) Integrate the application with a Single sign-on (SSO) system and do not expose secrets to the application.\nD. > Leverage a continuous build pipeline that produces multiple versions of the secret for each instance of the\napplication.","timestamp":"1631020680.0","comments":[{"comment_id":"441831","poster":"syslog","content":"I think A, you rotate the keys so the risk is minimal if a breach occurs.","comments":[{"poster":"JonathanSJ","upvote_count":"1","content":"Agree with A option","timestamp":"1673408580.0","comment_id":"771986"}],"timestamp":"1631174040.0","upvote_count":"5"}],"poster":"vipulg291280","upvote_count":"4"},{"comment_id":"431221","poster":"vipulg291280","comments":[{"poster":"Prosperous_Kenny","comments":[{"poster":"JonathanSJ","comment_id":"771989","upvote_count":"1","content":"Agree with A option.\nThe option D could be the second thing to check:\n\"Confirm that the application is using the required client library and the service account key has proper permissions.\"","timestamp":"1673408940.0"}],"timestamp":"1630048260.0","content":"The answer is A, you should check if the stackdriver agent is installed","comment_id":"432848","upvote_count":"2"}],"timestamp":"1629875760.0","upvote_count":"2","content":"You are using Stackdriver to monitor applications hosted on Google Cloud Platform (GCP). You recently deployed a new\napplication, but its logs are not appearing on the Stackdriver dashboard. You need to troubleshoot the issue. What should\nyou do?\nConfirm that the Stackdriver agent has been installed in the hosting virtual machine.\nConfirm that your account has the proper permissions to use the Stackdriver dashboard.\nConfirm that port 25 has been opened in the firewall to allow messages through to Stackdriver.\nConfirm that the application is using the required client library and the service account key has proper\npermissions."},{"poster":"devops202","upvote_count":"1","timestamp":"1629089040.0","content":"Anyone recently passed the exam ? How many question came from here?","comment_id":"425586"},{"poster":"devopsbatch","upvote_count":"1","content":"@Aldo86 did you pass?","timestamp":"1626978120.0","comment_id":"411833"},{"upvote_count":"1","content":"@charun.. have you given exam? wat was ur result?","timestamp":"1624972080.0","comment_id":"393799","poster":"devopsbatch"},{"content":"I feel B is the answer. If we re-word the question, in simple English, as some HTTPS reqs to couple apps, and use which tool to decide which app is slow. Option A profile only measure functions in an app slow; it does not compare ACROSS apps. C and D wrong obviously.","poster":"zanhsieh","timestamp":"1624930620.0","upvote_count":"3","comment_id":"393401"},{"timestamp":"1624085460.0","upvote_count":"5","poster":"ralf_cc","comment_id":"385300","content":"B - you want to find out latency info for dependent apps, Trace is the tool"},{"poster":"akg001","comments":[{"timestamp":"1624676040.0","content":"performance issue could be identified by latency between different services in the requests. Hence, tracer is the answer, B.","poster":"guruguru","comment_id":"390878","upvote_count":"3"},{"comments":[{"timestamp":"1631520660.0","upvote_count":"1","comment_id":"443827","poster":"cetanx","content":"Also as stated here: https://cloud.google.com/trace\n\nDistributed tracing for everyone\n\nCloud Trace is a distributed tracing system that collects latency data from your applications and displays it in the Google Cloud Console. You can track how requests propagate through your application and receive detailed near real-time performance insights. Cloud Trace automatically analyzes all of your application's traces to generate in-depth latency reports to surface performance degradations, and can capture traces from all of your VMs, containers, or App Engine projects."}],"comment_id":"385590","upvote_count":"8","timestamp":"1624116300.0","content":"The answer is B because the question says: \"anticipate\" \nLooks like A could be the answer but remember: You want to anticipate which dependent applications might cause performance issues. NO to optimize performance and reduce cost of computation of the application.\n\nStackdriver Trace provides detailed insight into your application’s run time performance and latency in near real-time. The service continuously evaluates data from each traced request and checks for patterns that indicate performance bottlenecks. To remove the operational overhead for performance analysis, Stackdriver Trace automatically analyzes your application’s performance over time.","poster":"francisco_guerra"}],"comment_id":"379678","timestamp":"1623406620.0","content":"After carefully reading the question and study, answer A is very close.\n\n\nPerformance and cost management Cloud Profiler provides continuous profiling of resource consumption in your production applications, helping you identify and eliminate potential performance issues.\n\nLatency management Cloud Trace provides latency sampling and reporting for App Engine, including per-URL statistics and latency distributions\n\nplease refer below URL :\nhttps://cloud.google.com/products/operations#all-features","upvote_count":"5"},{"timestamp":"1622578140.0","content":"B this could be right one","poster":"devopsbatch","upvote_count":"1","comment_id":"372185"}],"answers_community":["B (100%)"],"isMC":true,"question_id":1,"answer_images":[],"answer":"B","url":"https://www.examtopics.com/discussions/google/view/54119-exam-professional-cloud-devops-engineer-topic-1-question-1/","topic":"1","choices":{"A":"Instrument all applications with Stackdriver Profiler.","D":"Modify the Node.js application to log HTTP request and response times to dependent applications. Use Stackdriver Logging to find dependent applications that are performing poorly.","B":"Instrument all applications with Stackdriver Trace and review inter-service HTTP requests.","C":"Use Stackdriver Debugger to review the execution of logic within each application to instrument all applications."},"unix_timestamp":1622571060,"answer_ET":"B","answer_description":"","exam_id":6},{"id":"1oZUH64bhY6nKJG6EEDE","choices":{"A":"ג€¢ Deploy the Stackdriver logging agent to the application servers. ג€¢ Give the developers the IAM Logs Viewer role to access Stackdriver and view logs.","B":"ג€¢ Deploy the Stackdriver logging agent to the application servers. ג€¢ Give the developers the IAM Logs Private Logs Viewer role to access Stackdriver and view logs.","C":"ג€¢ Deploy the Stackdriver monitoring agent to the application servers. ג€¢ Give the developers the IAM Monitoring Viewer role to access Stackdriver and view metrics.","D":"ג€¢ Install the gsutil command line tool on your application servers. ג€¢ Write a script using gsutil to upload your application log to a Cloud Storage bucket, and then schedule it to run via cron every 5 minutes. ג€¢ Give the developers the IAM Object Viewer access to view the logs in the specified bucket."},"question_images":[],"topic":"1","timestamp":"2021-06-02 16:56:00","exam_id":6,"unix_timestamp":1622645760,"question_id":2,"answer_ET":"A","answers_community":["A (93%)","7%"],"url":"https://www.examtopics.com/discussions/google/view/54235-exam-professional-cloud-devops-engineer-topic-1-question-10/","answer":"A","answer_images":[],"isMC":true,"answer_description":"","question_text":"You have a pool of application servers running on Compute Engine. You need to provide a secure solution that requires the least amount of configuration and allows developers to easily access application logs for troubleshooting. How would you implement the solution on GCP?","discussion":[{"comment_id":"372802","poster":"devopsbatch","content":"A \nroles/logging.viewer (Logs Viewer) gives you read-only access to all features of Logging, except Access Transparency logs and Data Access audit logs.","comments":[{"comment_id":"702665","timestamp":"1682306760.0","upvote_count":"2","content":"A is right","poster":"AzureDP900"},{"upvote_count":"9","timestamp":"1639305300.0","comment_id":"380318","poster":"akg001","content":"correct - A . least privilege principle"}],"upvote_count":"27","timestamp":"1638464160.0"},{"upvote_count":"11","content":"correct A","timestamp":"1640734320.0","comment_id":"393256","poster":"Charun"},{"comment_id":"1167970","comments":[{"upvote_count":"1","comment_id":"1210118","content":"For access to all logs in the _Required and _Default buckets, including data access logs, grant the Private Logs Viewer (roles/logging.privateLogViewer) role.\n\nhttps://cloud.google.com/logging/docs/access-control#logging.privateLogViewer","poster":"habla2019pasta","timestamp":"1731394440.0"}],"content":"Selected Answer: A\nA is the correct answer. As B is talking about private logs viewer, there is nothing like that role in GCP.","poster":"jinaldesailive","upvote_count":"1","timestamp":"1725704880.0"},{"upvote_count":"1","poster":"fdsfsdgsdfxcvxcv","comment_id":"1107268","timestamp":"1719524580.0","content":"B, https://cloud.google.com/logging/docs/routing/overview"},{"content":"A - correct option","timestamp":"1717300860.0","poster":"jomonkp","comment_id":"1085732","upvote_count":"1"},{"poster":"maxdanny","upvote_count":"1","comment_id":"1066438","timestamp":"1715254380.0","content":"Selected Answer: A\nthe correct answer is A, the privateLogViewer gives extra access to Data Access Logs that's is not required\n\nhttps://cloud.google.com/logging/docs/view/logs-explorer-interface"},{"poster":"JonathanSJ","content":"Selected Answer: A\nAnswer A","upvote_count":"1","comment_id":"772036","timestamp":"1689043680.0"},{"content":"A is correct, Private Logs Viewer gives you extra access to Data access logs and the question was about viewing application logs.","comment_id":"715277","poster":"mohan999","upvote_count":"2","timestamp":"1683719820.0"},{"timestamp":"1676358600.0","content":"Selected Answer: A\nThe correct answer is \"A\"","comment_id":"646570","poster":"GCP72","upvote_count":"2"},{"comment_id":"628086","timestamp":"1673038500.0","content":"Selected Answer: B\nDefault tier is premium. There is NO mention of the load balancer being used and there is no default for this.","upvote_count":"1","poster":"mgm7"},{"upvote_count":"1","comment_id":"606193","content":"Selected Answer: A\nAns: Option A. :Logs Viewer role. Least config setup (as per question). Option B is incorrect due to additional audit log viewing access which is inappropriate to this question. ref: https://cloud.google.com/logging/docs/access-control","timestamp":"1669226880.0","poster":"prasathdv"},{"upvote_count":"2","content":"Option A (Least config settings).\nOption B - Private viewer log is for viewing data audit logs. \"The Logs Viewer role doesn't let principals read the Data Access audit logs that are in the _Default bucket. To read these Data Access audit logs, principals need the Private Logs Viewer role (roles/logging.privateLogViewer) for the appropriate log view.\" ref: https://cloud.google.com/logging/docs/access-control","poster":"prasathdv","timestamp":"1669226760.0","comment_id":"606192"},{"timestamp":"1660627260.0","comment_id":"548366","poster":"buldas","upvote_count":"3","content":"Selected Answer: A\nA. OK\nB. Logs Private Logs is for Data Logs\nC. Nope\nD. what?"},{"comment_id":"539902","timestamp":"1659545220.0","poster":"PhilipKoku","content":"Selected Answer: A\nA) You only need logging.viewer https://cloud.google.com/logging/docs/access-control","upvote_count":"2"},{"content":"Looks like answer A is correct. A logging agent is required to enable the custom logs pushed to Stackdriver https://cloud.google.com/logging/docs/agent/logging . Developers need only Log Viewer permission, which is enough in this case and Private Log viewer is a superset of log viewer permission with elevated permission to view the private data in logs. Which is not needed in this case.","poster":"roastc","timestamp":"1657413240.0","upvote_count":"3","comment_id":"520584"},{"poster":"simbu1299","upvote_count":"1","comment_id":"516382","timestamp":"1656918000.0","content":"Correct Answer is A"},{"poster":"scjs","comments":[{"upvote_count":"1","content":"and why do you need the Data Access Logs?","timestamp":"1658824440.0","comment_id":"532793","poster":"pddddd"}],"comment_id":"515432","timestamp":"1656825180.0","content":"B is correct as it talks about application logs \nhttps://cloud.google.com/logging/docs/access-control\n\nThe Logs Viewer role doesn't let you read the Data Access audit logs that are in the _Default bucket\n\nroles/logging.privateLogViewer (Private Logs Viewer) includes all the permissions contained by roles/logging.viewer, plus the ability to read Data Access audit logs in the _Default","upvote_count":"1"},{"comment_id":"494297","upvote_count":"1","content":"Ans : A","timestamp":"1654423380.0","poster":"alaahakim"},{"upvote_count":"2","content":"Selected Answer: A\nA is good","comment_id":"491526","poster":"Wwhite44","timestamp":"1654071240.0"},{"upvote_count":"1","timestamp":"1652098740.0","comment_id":"474844","poster":"Manh","content":"A for sure"},{"timestamp":"1646071380.0","poster":"danchoif2","comment_id":"434718","upvote_count":"4","content":"https://cloud.google.com/logging/docs/audit#access-control\n\n\n\nA is correct.\nB is incorrect because developers only need to access application logs, not private logs."}]},{"id":"z7mFt13QqhYuirqgU0fp","question_images":[],"choices":{"C":"Deploy the application by using kubectl and use Config Connector to slowly ramp up traffic between versions. Use Cloud Monitoring to look for performance issues.","B":"Deploy the application through a continuous delivery pipeline by using blue/green deployments. Migrate traffic to the new version of the application and use Cloud Monitoring to look for performance issues.","D":"Deploy the application by using kubectl and set the spec.updateStrategy.type field to RollingUpdate. Use Cloud Monitoring to look for performance issues, and run the kubectl rollback command if there are any issues.","A":"Deploy the application through a continuous delivery pipeline by using canary deployments. Use Cloud Monitoring to look for performance issues, and ramp up traffic as supported by the metrics."},"topic":"1","timestamp":"2023-10-11 05:05:00","exam_id":6,"unix_timestamp":1696993500,"answer_ET":"A","question_id":3,"answers_community":["A (94%)","6%"],"url":"https://www.examtopics.com/discussions/google/view/123218-exam-professional-cloud-devops-engineer-topic-1-question-100/","answer":"A","answer_images":[],"isMC":true,"answer_description":"","question_text":"The new version of your containerized application has been tested and is ready to be deployed to production on Google Kubernetes Engine (GKE). You could not fully load-test the new version in your pre-production environment, and you need to ensure that the application does not have performance problems after deployment. Your deployment must be automated. What should you do?","discussion":[{"comment_id":"1040131","upvote_count":"15","poster":"PrayasMohanty","timestamp":"1712804700.0","comments":[{"timestamp":"1714064880.0","content":"You meant option B?","comments":[{"content":"no PrayasM meant that canary fits what got asked more","comment_id":"1060468","upvote_count":"1","poster":"lelele2023","timestamp":"1714638300.0"}],"upvote_count":"1","poster":"Jason_Cloud_at","comment_id":"1053903"}],"content":"Selected Answer: A\nI vote for A as in Blue/Green deployment you can rollback quickly after facing the performance issue, but in Canary you can detect performance issue on partial deployment and rollback before the issue get affected."},{"comment_id":"1176585","timestamp":"1726664880.0","upvote_count":"1","poster":"BleHi","content":"Selected Answer: A\nAfter consideration, Canary approach is better in this specific scenario as it allows for monitoring the performance of the new version in production while minimizing the risk of widespread issues."},{"upvote_count":"1","content":"Selected Answer: B\nB. Blue/green deployments involve deploying the new version alongside the existing one, routing only a portion of the traffic to the new version initially. Once you verify that the new version is performing well and there are no issues, you can fully migrate traffic to the new version. This allows for a safe rollback if any issues arise.","comments":[{"content":"After consideration, Canary approach is better in this specific scenario as it allows for monitoring the performance of the new version in production while minimizing the risk of widespread issues.","timestamp":"1726664820.0","poster":"BleHi","comment_id":"1176584","upvote_count":"1"}],"comment_id":"1148531","poster":"BleHi","timestamp":"1723483200.0"},{"poster":"xhilmi","upvote_count":"2","content":"The recommended approach to automate the deployment of the new version of a containerized application to production on Google Kubernetes Engine (GKE) while addressing potential performance issues is (Option A).\n\nUtilizing canary deployments within a continuous delivery pipeline allows for a controlled and gradual rollout of the new version. By monitoring performance metrics with Cloud Monitoring, the deployment process can be informed by real-time insights. The traffic can be incrementally increased as supported by the monitored metrics, minimizing the risk of performance problems.\n\nThis approach provides a safety net, allowing for quick identification and mitigation of issues before a full deployment, ensuring a smooth transition to the new version with minimal impact on production.","timestamp":"1717630980.0","comment_id":"1088939"}]},{"id":"RugOjxeC3YVo9bVMrArA","discussion":[{"timestamp":"1696993860.0","content":"Selected Answer: D\nOption D uses fewest number of steps.","comment_id":"1040137","upvote_count":"6","poster":"PrayasMohanty"},{"content":"Selected Answer: C\nIP's can be missed using flow logs. Right answer is C:\nVPC Flow Logs samples packets using a primary sampling rate. The primary sampling rate is dynamic and varies depending on the load of the physical host running the VM or gateway at the time of sampling. The probability of sampling any single IP connection increases with the volume of packets. You can't control the primary flow log sampling process or adjust the primary samplSource: https://cloud.google.com/vpc/docs/flow-logs#log-sampling","timestamp":"1743354000.0","upvote_count":"1","poster":"soady","comment_id":"1413577"},{"timestamp":"1741865940.0","comments":[{"comment_id":"1388274","poster":"cachopo","timestamp":"1741866000.0","upvote_count":"1","content":"A (Enable Packet Mirroring on the VPC):\n- Packet Mirroring captures entire network packets, which is useful for deep packet inspection but is complex to configure and requires additional analysis tools.\n- Overhead is high, and it is not needed just to log source IPs.\n\nB (Install the Ops Agent on Compute Engine instances):\n- Ops Agent collects system and application logs, but it won’t automatically log incoming requests unless the custom HTTP server itself logs them.\n- This would require additional configuration and modifying the application’s logging behavior.\n\nD (Enable VPC Flow Logs on the subnet):\n- VPC Flow Logs capture network-level traffic metadata, including source and destination IPs.\n- However, this requires additional processing in Cloud Logging to extract IPs accessing the API.\n- Firewall logging is more straightforward for tracking incoming connections."}],"content":"Selected Answer: C\nSince there is already a firewall rule allowing access from 0.0.0.0/0, enabling firewall rule logging will capture details of every incoming connection, including the source IP address.\nThis is the simplest approach and requires only one step (enabling logging on the existing firewall rule).","comment_id":"1388273","poster":"cachopo","upvote_count":"1"},{"comment_id":"1324724","upvote_count":"1","content":"Selected Answer: C\nVPC logs is sample log collected at intervals","timestamp":"1733858760.0","poster":"abildikar"},{"upvote_count":"1","poster":"JohnJamesB1212","comment_id":"1299545","content":"Selected Answer: C\nThe correct option is C: Enable logging on the firewall rule.\n\nExplanation:\nFirewall rule logging allows you to capture the traffic that matches a specific firewall rule, including details such as the source IP address. Since your firewall rule allows access to the API port from 0.0.0.0/0, enabling logging on this rule will log the IP addresses of incoming connections to the API.\nThis is the most straightforward way to log the IP addresses accessing the API using the fewest steps, as it leverages existing firewall configurations and integrates with Cloud Logging.\nD: VPC Flow Logs provide network-level logging for traffic flowing within the VPC but would log all traffic in the subnet. While it could work, it's a more complex solution compared to enabling firewall rule logging directly.\nTherefore, C provides the quickest and simplest method to log IP addresses accessing the API.","timestamp":"1729229520.0"},{"comment_id":"1248213","upvote_count":"1","content":"Selected Answer: C\nC) Enabling Logging of firewall rules","poster":"PhilipKoku","timestamp":"1721035020.0"},{"upvote_count":"2","content":"Selected Answer: C\nC is correct. VPC Flows logs can show source IP addresses, but they sample packets, do not provide the level of detail about individual API calls compared to firewall rule logging.","poster":"winston9","comment_id":"1225463","timestamp":"1717675500.0"},{"upvote_count":"1","content":"Selected Answer: C\nBe careful. The question states \"each IP address that accesses the API\". VPC Flow Logs is sampling records:\n\n\"VPC Flow Logs records a sample of network flows sent from and received by VM instances, including instances used as GKE nodes. These logs can be used for network monitoring, forensics, real-time security analysis, and expense optimization.\"\nSource: https://cloud.google.com/vpc/docs/using-flow-logs\n\nC. Is the correct answer.","comment_id":"1184636","timestamp":"1711614240.0","poster":"N_A"},{"comment_id":"1088945","poster":"xhilmi","timestamp":"1701827460.0","upvote_count":"3","content":"Selected Answer: D\nChoose option D.\n\nTo configure Cloud Logging to log each IP address accessing the API with the fewest steps in a Compute Engine environment using an internal TCP/UDP load balancer, the first step would be to enable VPC Flow Logs on the subnet. That will allows you to capture network flow information, including source and destination IP addresses, as traffic passes through the load balancer.\n\nVPC Flow Logs provide detailed visibility into network activity without requiring modifications to individual instances or the installation of additional agents. Enabling VPC Flow Logs is a straightforward and efficient way to capture the necessary information for logging IP addresses accessing the API in a Compute Engine environment."},{"content":"D. Enable VPC Flow Logs on the subnet.\n\nThis will capture the network traffic details you need for logging in Cloud Logging without requiring additional configurations on the instances or firewall rules.","comment_id":"1022216","upvote_count":"2","timestamp":"1696160880.0","poster":"ManishKS"}],"choices":{"B":"Install the Ops Agent on the Compute Engine instances.","D":"Enable VPC Flow Logs on the subnet.","C":"Enable logging on the firewall rule.","A":"Enable Packet Mirroring on the VPC."},"timestamp":"2023-10-01 13:48:00","answers_community":["D (53%)","C (47%)"],"unix_timestamp":1696160880,"question_text":"You are managing an application that runs in Compute Engine. The application uses a custom HTTP server to expose an API that is accessed by other applications through an internal TCP/UDP load balancer. A firewall rule allows access to the API port from 0.0.0.0/0. You need to configure Cloud Logging to log each IP address that accesses the API by using the fewest number of steps. What should you do first?","topic":"1","question_id":4,"exam_id":6,"answer_description":"","isMC":true,"url":"https://www.examtopics.com/discussions/google/view/122025-exam-professional-cloud-devops-engineer-topic-1-question-101/","question_images":[],"answer_ET":"D","answer":"D","answer_images":[]},{"id":"pogbT5aKkRVwVkNxYUrQ","exam_id":6,"question_text":"Your company runs an ecommerce website built with JVM-based applications and microservice architecture in Google Kubernetes Engine (GKE). The application load increases during the day and decreases during the night. Your operations team has configured the application to run enough Pods to handle the evening peak load. You want to automate scaling by only running enough Pods and nodes for the load. What should you do?","url":"https://www.examtopics.com/discussions/google/view/122026-exam-professional-cloud-devops-engineer-topic-1-question-102/","unix_timestamp":1696161240,"discussion":[{"poster":"dija123","comment_id":"1210120","content":"Selected Answer: D\nVoting for D","timestamp":"1731395220.0","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: D\nplease read this how to use vertical pod autoscaler https://cloud.google.com/kubernetes-engine/docs/concepts/verticalpodautoscaler , so it suggest use Horizontal Pod Autoscaler (HPA) with the VPA to get the best of autoscaling.","comment_id":"1101761","poster":"LaxmanTiwari","timestamp":"1718895480.0"},{"upvote_count":"3","content":"Selected Answer: D\nTo automate scaling for the JVM-based applications and microservices architecture in Google Kubernetes Engine (GKE), especially to handle varying loads throughout the day, the recommended approach is (Option D).\n\nThis involves configuring the Horizontal Pod Autoscaler (HPA) to dynamically adjust the number of Pods based on application metrics. Additionally, enabling the cluster autoscaler allows for automatic scaling of the node pool, ensuring an adequate number of nodes to accommodate the varying workload. The HPA monitors resource utilization and adjusts the number of Pods, while the cluster autoscaler ensures that the underlying infrastructure scales up or down to meet the demand.\n\nThis combined approach optimizes resource allocation, providing an efficient and automated solution for managing workload fluctuations in a cost-effective manner.","comment_id":"1088955","timestamp":"1717632120.0","poster":"xhilmi"},{"comment_id":"1070661","content":"D is correct\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler?hl=es-419\nhttps://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-metrics-apis","timestamp":"1715698080.0","poster":"TereRolon","upvote_count":"2"},{"timestamp":"1712884860.0","upvote_count":"3","comment_id":"1041277","content":"You want to automate scaling by only running enough Pods and nodes for the load. Answer D answers this requirement.","poster":"activist"},{"comment_id":"1040141","poster":"PrayasMohanty","content":"Selected Answer: D\nThere is no Vertical Pod Autoscaler, and cluster autoscaler must be enabled. Therefor D appears the best answer.","upvote_count":"4","timestamp":"1712805480.0"},{"upvote_count":"3","timestamp":"1711972440.0","poster":"ManishKS","content":"Answer Should be D","comment_id":"1022220"}],"question_id":5,"question_images":[],"topic":"1","answer":"D","answer_description":"","answer_ET":"D","timestamp":"2023-10-01 13:54:00","choices":{"A":"Configure the Vertical Pod Autoscaler, but keep the node pool size static.","D":"Configure the Horizontal Pod Autoscaler, and enable the cluster autoscaler.","C":"Configure the Horizontal Pod Autoscaler, but keep the node pool size static.","B":"Configure the Vertical Pod Autoscaler, and enable the cluster autoscaler."},"answers_community":["D (100%)"],"isMC":true,"answer_images":[]}],"exam":{"isMCOnly":true,"name":"Professional Cloud DevOps Engineer","numberOfQuestions":196,"provider":"Google","isImplemented":true,"lastUpdated":"11 Apr 2025","isBeta":false,"id":6},"currentPage":1},"__N_SSP":true}