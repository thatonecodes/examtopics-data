{"pageProps":{"questions":[{"id":"eBSar9uHqb9oCiL4ZMTe","question_images":[],"answer_images":[],"answer_ET":"D","choices":{"B":"1. Create standard instances on Compute Engine.\n2. Select as the OS the same Microsoft Windows version that is currently in use in the on-premises environment.","D":"1. Create an image of the on-premises virtual machines.\n2. Import the image as a virtual disk on Compute Engine using --os=windows-2022-dc-v.\n3. Create a sole-tenancy instance on Compute Engine that uses the imported disk as a boot disk.","C":"1. Create an image of the on-premises virtual machine.\n2. Import the image as a virtual disk on Compute Engine.\n3. Create a standard instance on Compute Engine, selecting as the OS the same Microsoft Windows version that is currently in use in the on-premises environment.\n4. Attach a data disk that includes data that matches the created image.","A":"1. Create an image of the on-premises virtual machines and upload into Cloud Storage.\n2. Import the image as a virtual disk on Compute Engine."},"answers_community":["D (84%)","Other"],"isMC":true,"topic":"1","unix_timestamp":1695554700,"discussion":[{"timestamp":"1703823360.0","comment_id":"1108338","content":"Selected Answer: D\nWell, yes, you actually need a sole-tenant instance to install a windows server with your licence.\nA lot of links were pasted here, but after reading a lot of them, I reached this one who explicitly states:\n'To create a VM instance that uses the custom BYOL image, you must provision the VM instance on a sole-tenant node.'\nhttps://cloud.google.com/compute/docs/images/creating-custom-windows-byol-images#use_the_custom_image","poster":"e5019c6","upvote_count":"23"},{"upvote_count":"6","timestamp":"1702336440.0","comment_id":"1093921","content":"Selected Answer: D\nSole tenant purpose is to facilitate importing licenses BYOL\n\nSole-tenant nodes can help you meet dedicated hardware requirements for bring your own license (BYOL) scenarios that require per-core or per-processor licenses. When you use sole-tenant nodes, you have some visibility into the underlying hardware, which lets you track core and processor usage. To track this usage, Compute Engine reports the ID of the physical server on which a VM is scheduled. Then, by using Cloud Logging, you can view the historical server usage of a VM. To optimize the use of the host hardware, you can overcommit sole-tenant VM CPUs, share sole-tenant node groups and manually live migrate VMs.","poster":"MahAli"},{"poster":"frank_tsai_tech","upvote_count":"1","content":"Selected Answer: D\nExplanation:\n• To bring your on‐premises Windows Server 2022 licenses (BYOL) to Google Cloud, you must adhere to Microsoft’s licensing rules.\n• Microsoft requires that on-premises licenses used under License Mobility be deployed on sole-tenancy instances in Google Cloud.\n• Option D specifies importing the image with the proper OS flag (–os=windows-2022-dc-v) and launching a sole-tenancy instance, which is the correct procedure to utilize your existing licenses.\n\nThis approach meets the licensing requirements while optimizing cost.","comment_id":"1400936","timestamp":"1742457240.0"},{"timestamp":"1732382280.0","poster":"Ekramy_Elnaggar","upvote_count":"4","comment_id":"1316749","content":"Selected Answer: D\nTo create a VM instance that uses the custom BYOL image, you must provision the VM instance on a sole-tenant node."},{"content":"Selected Answer: A\nNo need to have a sole-tenancy instance","timestamp":"1728920940.0","upvote_count":"1","comment_id":"1297718","poster":"dfizban"},{"timestamp":"1709446500.0","content":"Selected Answer: D\nD is answer due to BYOL conditions of MS Mobility Licenses","upvote_count":"1","comment_id":"1164528","poster":"madcloud32"},{"poster":"AGHPE","upvote_count":"1","timestamp":"1709274600.0","content":"Selected Answer: A\nA. is a valid and used strategy to migrate Windows Server virtual machines from an on-premises environment to Google Cloud Platform (GCP), especially when you want to carry over existing licenses using the Bring Your Own License (BYOL) approach.","comment_id":"1163262"},{"upvote_count":"1","poster":"Amrita2012","comment_id":"1146321","content":"Selected Answer: D\nTo create a VM instance that uses the custom BYOL image, you must provision the VM instance on a sole-tenant node.","timestamp":"1707570180.0"},{"upvote_count":"1","poster":"didek1986","comment_id":"1127439","content":"Selected Answer: C\nIt shouod be c","timestamp":"1705775040.0"},{"timestamp":"1701741720.0","upvote_count":"1","comment_id":"1088143","content":"Answer is A: \nSteps:\n1 - Export an existing VM in .OVA format\n2 - Install and Authorize the gCloud SDK\n3 - Copy the .OVA file to a Google Storage Bucket\n4 - Import the .OVA file to Google Cloud from the bucket. \nhttps://www.youtube.com/watch?v=NG38am3Y8hM \ngcloud compute instances import gcpinstanename --os=windows-10-x64-byol","poster":"squishy_fishy"},{"content":"Eligible Products: License Mobility typically applies to Microsoft software such as SQL Server and other Microsoft applications, but it's important to note that Windows Server licenses are generally not eligible for License Mobility. So, D.","poster":"Jconnor","comment_id":"1086866","upvote_count":"1","timestamp":"1701614160.0"},{"comment_id":"1084940","timestamp":"1701405360.0","content":"Selected Answer: A\nLicensing scenarios such as licenses related to Linux BYOS with RHEL or SLES, as well as Microsoft applications don't require sole-tenant nodes. If you are considering bringing licenses from Microsoft applications such as SharePoint Server and SQL Server, use Microsoft License Mobility.\n\nhttps://cloud.google.com/compute/docs/nodes/bringing-your-own-licenses#importing_and_creating_an_image_from_an_offline_virtual_disk","poster":"techtitan","upvote_count":"2"},{"timestamp":"1700587380.0","comment_id":"1076491","poster":"issaprank","content":"Selected Answer: C\neither c or d but c seems right too","upvote_count":"1"},{"poster":"Ahmed_Safwat","timestamp":"1699981200.0","upvote_count":"2","comment_id":"1070675","content":"Selected Answer: A\nNo need to have a sole-tenancy instance"},{"comment_id":"1028112","upvote_count":"1","timestamp":"1696779480.0","poster":"Prakzz","content":"Selected Answer: D\nhttps://cloud.google.com/compute/docs/import/importing-virtual-disks"},{"timestamp":"1695912780.0","comment_id":"1020023","content":"Selected Answer: D\nD for sure with BOYL","poster":"ductrinh","upvote_count":"2"},{"comment_id":"1016002","timestamp":"1695572520.0","poster":"Murtuza","content":"If the customer truly wants to BYOL, then sole-tenant nodes are required which is a requirement for this question \nYes it is required for BYOL because that's a requirement from Microsoft themselves\nhttps://www.microsoft.com/en-us/licensing/news/updated-licensing-rights-for-dedicated-cloud","upvote_count":"1"},{"content":"Correct answer is D\nSole-tenant nodes can help you meet dedicated hardware requirements for bring your own license (BYOL) scenarios that require per-core or per-processor licenses. When you use sole-tenant nodes, you have some visibility into the underlying hardware, which lets you track core and processor usage.","timestamp":"1695572280.0","comment_id":"1015995","poster":"Murtuza","upvote_count":"1"},{"timestamp":"1695554700.0","poster":"alankrita1987","upvote_count":"2","comment_id":"1015707","content":"Selected Answer: D\nhttps://cloud.google.com/migrate/compute-engine/docs/4.11/how-to/organizing-migrations/sole-tenant-nodes"}],"answer":"D","timestamp":"2023-09-24 13:25:00","question_id":106,"exam_id":4,"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/121314-exam-professional-cloud-architect-topic-1-question-194/","question_text":"Your company is planning to migrate their Windows Server 2022 from their on-premises data center to Google Cloud. You need to bring the licenses that are currently in use in on-premises virtual machines into the target cloud environment. What should you do?"},{"id":"jrMQwQiVYz8LMmLdWeSj","url":"https://www.examtopics.com/discussions/google/view/121313-exam-professional-cloud-architect-topic-1-question-195/","timestamp":"2023-09-24 13:12:00","question_images":[],"isMC":true,"answer_ET":"A","exam_id":4,"answer":"A","answer_images":[],"answers_community":["A (91%)","9%"],"answer_description":"","topic":"1","question_id":107,"question_text":"You are deploying an application to Google Cloud. The application is part of a system. The application in Google Cloud must communicate over a private network with applications in a non-Google Cloud environment. The expected average throughput is 200 kbps. The business requires:\n\n• 99.99% system availability\n• cost optimization\n\nYou need to design the connectivity between the locations to meet the business requirements. What should you provision?","discussion":[{"upvote_count":"7","timestamp":"1719627720.0","poster":"e5019c6","comment_id":"1108341","content":"Yep, this question is a duplicate of #191."},{"content":"Selected Answer: A\nHA VPN supports two tunnels to achieve 99.99%. Classic VPN does not. Any more than 2 tunnels is excessive cost.","poster":"TopTalk","comment_id":"1016021","upvote_count":"5","timestamp":"1711305240.0"},{"timestamp":"1742455860.0","upvote_count":"1","comment_id":"1400929","content":"Selected Answer: A\nExplanation:\n • High Availability (HA): HA Cloud VPN provides an SLA of up to 99.99% when properly configured with redundant tunnels. This meets the requirement for nearly 100% system availability.\n • Cost Optimization: For low throughput (200 kbps is minimal), one HA VPN gateway with two tunnels is both cost-effective and sufficient. Using two separate gateways (as in option C) would add unnecessary cost.\n • Simplicity: An HA VPN gateway with two tunnels is simpler to manage than deploying multiple VPN gateways.\n\nThus, option A meets the business requirements while optimizing cost and availability.","poster":"frank_tsai_tech"},{"poster":"ryaryarya","timestamp":"1736304000.0","upvote_count":"1","comment_id":"1337797","content":"Selected Answer: C\nWith A, there is a single point of failure. What if that single VPN gateway in the on-prem environment goes down? There goes your 99.99% system availability, which is the first business requirement."},{"content":"Selected Answer: A\nDuplicated question in somewhere. Answer is A.","timestamp":"1711376520.0","poster":"sheucm89","comment_id":"1016785","upvote_count":"4"},{"comment_id":"1015984","upvote_count":"1","content":"Correct answer is A","poster":"Murtuza","timestamp":"1711303740.0"},{"poster":"thisisbob","timestamp":"1711285920.0","upvote_count":"3","content":"A, I think the question is duplicated","comment_id":"1015700"}],"choices":{"A":"An HA Cloud VPN gateway connected with two tunnels to an on-premises VPN gateway.","B":"A Classic Cloud VPN gateway connected with two tunnels to an on-premises VPN gateway.","D":"A Classic Cloud VPN gateway connected with one tunnel to an on-premises VPN gateway.","C":"Two HA Cloud VPN gateways connected to two on-premises VPN gateways. Configure each HA Cloud VPN gateway to have two tunnels, each connected to different on-premises VPN gateways."},"unix_timestamp":1695553920},{"id":"Xf4lO0r2opFdqsLx9Ok4","answer":"D","topic":"1","timestamp":"2023-09-24 18:12:00","discussion":[{"upvote_count":"13","timestamp":"1698599220.0","comment_id":"1057005","content":"Selected Answer: D\nThis Transfer Appliance docs says it is suitable when \"It would take more than one week to upload your data over the network\"\nSince 10TB would take way less than a week for that bandwidth, I would go for D\nhttps://cloud.google.com/transfer-appliance/docs/4.0/overview#suitability","poster":"cchiaramelli","comments":[{"comment_id":"1307813","timestamp":"1730895120.0","content":"your link says: Transfer Appliance is a good fit for your data transfer needs if It would take more than one week to upload your data over the network, but this workload takes 1 day to complete by CLi","upvote_count":"2","poster":"exam4c3"}]},{"comments":[{"content":"So how would it work with B ? The data needs to still end up in GCS. Also, who says the export is 1 large file ?","upvote_count":"2","comment_id":"1242127","poster":"mstaicu","timestamp":"1720101900.0"},{"timestamp":"1705417560.0","content":"so no answer is possible according to you ?...","poster":"klefevre08","upvote_count":"2","comment_id":"1124307"},{"timestamp":"1707560820.0","comment_id":"1146063","upvote_count":"1","poster":"spuyol","content":"you could solve that using split command"}],"comment_id":"1089774","timestamp":"1701896580.0","upvote_count":"6","content":"Selected Answer: B\nmaximum object size in GCS is 5TB","poster":"mgm7"},{"timestamp":"1742455560.0","comment_id":"1400928","poster":"frank_tsai_tech","upvote_count":"1","content":"Selected Answer: D\nExplanation:\n• With a 1 Gbps connection, transferring 10 TB online is practical—theoretical estimates show it can take around 22–30 hours if the link is fully utilized.\n• Compressing the data reduces the amount to transfer and the –m flag enables parallel (multi-threaded) uploads, further reducing transfer time.\n• This approach minimizes both the transfer time and overall cost, and it avoids the added delays and expense associated with ordering and shipping a Transfer Appliance.\n• Google’s best practices indicate that if you have sufficient network bandwidth (like 1 Gbps) and the dataset is in the 10–100 TB range, an online transfer using gsutil (with optimizations like compression) is the recommended approach."},{"comment_id":"1400737","upvote_count":"1","poster":"Blackstile","content":"https://cloud.google.com/storage-transfer/docs/transfer-options\nTransferring more than 1 TB from on-premises Use Storage Transfer Service or Transfer Appliance.\nThis question has a trick, because the question talks about cost and Transfer Appliance is very expensive, if it were Storage Transfer Service then yes I would choose this option over gsuti.","timestamp":"1742417220.0"},{"timestamp":"1741817460.0","content":"Selected Answer: D\nD is correct for Saving time and cost optimized too","comment_id":"1388100","poster":"cloud_rider","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: D\nTransfer Appliance will take more time than gcloud storage cp :)","comment_id":"1362430","timestamp":"1740635760.0","poster":"PetarMarinkovic"},{"content":"Selected Answer: B\nB is correct answer. I just did a through questioning in Gemini and this is a good explanation (after referring to official GCP resources too):\nTime: Transferring 10TB over a 1 Gbps connection will take a very long time (likely more than a day, even under ideal conditions). The gcloud storage cp command, while capable of resumable uploads for individual files, doesn't handle interruptions well for large directory transfers.\n\nEstimated 22 hours to complete transfer, provided in the most ideal situation for the network. But still a big risk if a large file is interrupted halfway and needs to start over. Although likely this 10TB transfer will be via many files, there still could be some large files in between. Hence to optimize cost and time B using transfer appliance is the answer.","upvote_count":"1","comment_id":"1361746","poster":"david_tay","timestamp":"1740546240.0"},{"timestamp":"1736304600.0","comment_id":"1337799","content":"Selected Answer: D\nNo where in the question does it say the database export is one file (for anyone who is hung up on the 5TB object size limit). And even if it was, it still needs to end up in GCS, using the appliance doesn't solve that. Also it would be way more expensive and take a lot longer before your data would be available in GCS using the appliance.","poster":"ryaryarya","upvote_count":"2"},{"upvote_count":"1","poster":"rrope","content":"Selected Answer: B\nThe best option is B. Use the Data Transfer appliance to perform an offline migration.","comment_id":"1332104","timestamp":"1735243080.0"},{"comment_id":"1325525","timestamp":"1733997480.0","poster":"VishalMoon","content":"Selected Answer: B\nI think the key factor here is \"Google Recommended Practices\". Based on this alone we have to select B. If this was not there and given the 1 GBPS speed, D would have been feasible.","upvote_count":"1"},{"content":"Selected Answer: B\nB. Use the Data Transfer Appliance to perform an offline migration.\n\nusing the Data Transfer Appliance aligns with Google's recommended practice for large-scale migrations where bandwidth limitations are a concern, ensuring efficient, secure, and cost-effective transfer of your on-premises database export into Google Cloud Storage.","comments":[{"upvote_count":"1","timestamp":"1732379580.0","content":"What about cost? And time to get to GCP Region/Zone? And chain of custody of the data as well as upload and available? What is the of cost of that? How ling will it take?","comment_id":"1316730","poster":"desertlotus1211"}],"poster":"Zonci","comment_id":"1230963","upvote_count":"3","timestamp":"1718457240.0"},{"timestamp":"1715341560.0","poster":"Wasamela","comment_id":"1209356","content":"The current maximum object size supported by GCS is 5 TB, so it should be B","upvote_count":"1"},{"poster":"smithloo","comment_id":"1194068","upvote_count":"1","content":"thanks for sharing it <a href=\"https://www.qualitybacklink.net\">Link building SEO</a>","timestamp":"1712884560.0"},{"comment_id":"1193166","upvote_count":"2","timestamp":"1712767620.0","content":"Option B. Cloud Storage object limit is 5 TB. https://cloud.google.com/storage/quotas?hl=en#objects\n\n\nhttps://cloud.google.com/storage/quotas?hl=en#objects","poster":"a53fd2c"},{"poster":"madcloud32","upvote_count":"4","comment_id":"1163655","timestamp":"1709315580.0","content":"Selected Answer: B\nAnswer is B. Max cp limit of file is 5 TB"},{"content":"Selected Answer: D\nSince we would want to do it in the shortest time possible, using gsutil cp would take only 30 hours to move 10TB. So answer is D.\n\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets?hl=en#online_versus_offline_transfer","poster":"JohnDohertyDoe","timestamp":"1705144620.0","upvote_count":"6","comment_id":"1121548"},{"timestamp":"1703609340.0","poster":"Prudvi3266","upvote_count":"4","comment_id":"1106227","content":"Selected Answer: D\nAs we have 1Ggigabit network we can transfer this with CLI command quicker than Transfer appliance. which takes time like more than week. Incase of lesser band width may be we use transfer appliance"},{"upvote_count":"1","content":"Selected Answer: B\nill go for b","comment_id":"1098037","poster":"anjanc","timestamp":"1702720020.0"},{"comment_id":"1095351","timestamp":"1702461300.0","poster":"Roro_Brother","upvote_count":"3","content":"Selected Answer: B\nBecause there is 10 TB of data, it's B"},{"timestamp":"1702337160.0","comment_id":"1093927","content":"Selected Answer: D\nAppliance is overkilling for 30 hours cli command, the appliance could take more than a week with shipping, how many times you run jobs which take hours to completed? Many times I would go with D","upvote_count":"2","poster":"MahAli"},{"comments":[{"timestamp":"1702328700.0","poster":"MikeH20","upvote_count":"3","content":"Respectfully, I feel option D is the better choice. According to https://cloud.google.com/transfer-appliance/docs/4.0/overview#suitability, Google recommends using a transfer appliance if the data transfer would take longer than 1 week. At 1 Gbps and 10TB of data, the transfer would take 30 hours (https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#online_versus_offline_transfer). We also need to take into account shipping time for the appliance itself, to and from. That would take a couple weeks.\n\nThe question mentions to minimize time and cost, and follow Google best practices. In this case, D checks those boxes. The question does not mention a customer concern about network interruptions. If they did, then B could be argued as the more appropriate answer.","comment_id":"1093861"}],"comment_id":"1072396","poster":"Nora9","timestamp":"1700137260.0","content":"Selected Answer: B\noption B (Use the Data Transfer appliance to perform an offline migration) seems to be the most appropriate. It addresses the need for a speedy transfer of a large amount of data and is a cost-effective solution recommended by Google for large-scale data migrations. This option circumvents potential network bandwidth limitations and provides a reliable way to transfer large datasets.\n\nWhy option D is not a good choice : \nD. Upload the data with gcloud storage cp: This method uses the gcloud command-line tool to copy files to Cloud Storage. While simple, it might not be the most efficient for a 10-TB migration given the 1 Gbps bandwidth. The process could be slow and may require additional handling for potential interruptions and resuming uploads.","upvote_count":"5"},{"comments":[{"upvote_count":"1","timestamp":"1714543020.0","comment_id":"1204877","content":"No exactly duplicate. In #146 choice betwee Data Transfer Appliance and gsutil -m. here between DTA and gcloud storage cp....but stilll a tricky question","poster":"Gino17m"},{"upvote_count":"1","comment_id":"1074507","poster":"LifeWins","timestamp":"1700380500.0","content":"Answer has to be D."}],"content":"Duplicate so review question #146 and the choice over there is offline transfer appliance.This is a tricky question","timestamp":"1696418460.0","comment_id":"1024687","poster":"Murtuza","upvote_count":"3"},{"timestamp":"1695879900.0","comment_id":"1019458","content":"Selected Answer: D\nD is the answer","upvote_count":"2","poster":"RJAY123"},{"comment_id":"1018742","comments":[{"content":"1 Gbps and not 1GB BW, this means it will take 23 hours approx, still faster than the appliance.","comment_id":"1093142","timestamp":"1702275960.0","upvote_count":"4","poster":"shop2"}],"content":"The google math is 1TB data, 1GB bandwidth will take 3 hrs. We have 10TB of data so we are looking at 30 HRS hence the storage appliance","upvote_count":"3","timestamp":"1695813420.0","poster":"Murtuza"},{"upvote_count":"1","timestamp":"1695813360.0","comment_id":"1018738","content":"age\n Dataset size\n1\nTB\n12\nnetwork_check\n Bandwidth\n1\nGbps\n9\ntimer\n Transfer time\n3\nhr\n4","poster":"Murtuza"},{"content":"The Data Transfer Appliance is a physical device that can be sent to the customer to transfer large amounts of data quickly and securely. The appliance is designed to transfer large amounts of data, and it comes with built-in security features to protect the data during the transfer process. It is also easy to set up and use. The appliance can be connected to the on-premises environment, and the data can be transferred directly to Google Cloud Storage, which minimizes the database load and reduces the time it takes to complete the migration.\n\nTherefore, the best option to migrate a large on-premises database export into Cloud Storage, while minimizing cost, time, and database load, is to use the Data Transfer Appliance.","comment_id":"1018735","upvote_count":"3","timestamp":"1695813180.0","poster":"Murtuza"},{"content":"Option D (Compress the data and upload it with gsutil -m) is a valid option but might not be the most efficient. Compressing the data can help reduce the size of the data, but it might not significantly reduce the amount of time it takes to transfer the data to Cloud Storage. Additionally, multi-threaded copy could help speed up the process, but it could still take significant time to complete the tr","poster":"Murtuza","upvote_count":"1","timestamp":"1695813120.0","comment_id":"1018734"},{"poster":"AlainP1980","comment_id":"1017540","timestamp":"1695714900.0","content":"Selected Answer: D\nGoogle practices: gsutil or gcloud storage commands are ok up to few TB\nbandwidth is 1 Gbps\nminimize the time and the overall cost => answer D","upvote_count":"2"},{"content":"Selected Answer: D\nGoogle practices: gsutil or storage are ok up to few GB\nbandwidth is 1 Gbps\nminimize the time and the overall cost => answer D","comment_id":"1017537","poster":"AlainP1980","timestamp":"1695714840.0","upvote_count":"2"},{"content":"Vote D.\nThe bandwidth is 1 Gbps. It will only take less than 3 hours to complete the upload of 10TB data by using copy command.\nThe Data Transfer appliance can take several days to weeks. The question ask to minimize the time and the overall cost, so D is suitable.","poster":"sheucm89","timestamp":"1695645540.0","upvote_count":"4","comments":[{"timestamp":"1696417560.0","comment_id":"1024675","content":"Your math is not correct it should be 30 HRS for 10TB of data and not 3 HRS.","upvote_count":"2","poster":"Murtuza"},{"content":"1TB = 1024 GB bro","poster":"ArtistS","timestamp":"1700346420.0","upvote_count":"2","comment_id":"1074323"}],"comment_id":"1016796"},{"content":"Answer is B google practices is to use a storage transfer appliance for any data that is over 1TB","comment_id":"1015990","upvote_count":"2","poster":"Murtuza","timestamp":"1695571920.0"}],"isMC":true,"question_text":"Your company wants to migrate their 10-TB on-premises database export into Cloud Storage. You want to minimize the time it takes to complete this activity and the overall cost. The bandwidth between the on-premises environment and Google Cloud is 1 Gbps. You want to follow Google-recommended practices. What should you do?","question_images":[],"answer_ET":"D","url":"https://www.examtopics.com/discussions/google/view/121324-exam-professional-cloud-architect-topic-1-question-196/","answer_description":"","answer_images":[],"exam_id":4,"choices":{"C":"Use a commercial partner ETL solution to extract the data from the on-premises database and upload it into Cloud Storage.","D":"Upload the data with gcloud storage cp.","A":"Develop a Dataflow job to read data directly from the database and write it into Cloud Storage.","B":"Use the Data Transfer appliance to perform an offline migration."},"question_id":108,"answers_community":["D (61%)","B (39%)"],"unix_timestamp":1695571920},{"id":"uyFKZc16wSjaKmoEBxEh","unix_timestamp":1713087060,"timestamp":"2024-04-14 11:31:00","choices":{"A":"Create a retention policy on the bucket for the duration of 5 years. Create a lock on the retention policy.","D":"Create a retention policy organizational constraint constraints/storage.retentionPolicySeconds at the project level. Set the duration to 5 years.","C":"Use a customer-managed key for the encryption of the bucket. Rotate the key after 5 years.","B":"Create a retention policy organizational constraint constraints/storage.retentionPolicySeconds at the organization level. Set the duration to 5 years."},"answer_description":"","answers_community":["A (100%)"],"topic":"1","url":"https://www.examtopics.com/discussions/google/view/138677-exam-professional-cloud-architect-topic-1-question-197/","answer_images":[],"answer_ET":"A","discussion":[{"poster":"david_tay","comment_id":"1361749","upvote_count":"1","timestamp":"1740546480.0","content":"Selected Answer: A\nmust have retention lock, else someone could change the retention policy."},{"timestamp":"1731153960.0","comment_id":"1208794","upvote_count":"3","content":"Selected Answer: A\nYes, its A - same question as 125 :)","poster":"japij10711"},{"timestamp":"1730449200.0","poster":"Gino17m","upvote_count":"2","comment_id":"1204885","content":"Selected Answer: A\nA is correct.\n - retention policy must be locked\n- the is now need for retention policy for all buckets in organization or all buckets in the project"},{"upvote_count":"2","timestamp":"1729940160.0","content":"Selected Answer: A\nCreate a retention policy on the bucket for the duration of 5 years. Create a lock on the retention policy","poster":"666Amitava666","comment_id":"1202549"},{"comment_id":"1195528","poster":"MoeHaydar","timestamp":"1728911880.0","content":"Selected Answer: A\nA. Create a retention policy on the bucket for the duration of 5 years. Create a lock on the retention policy","upvote_count":"3"},{"poster":"201b6fa","upvote_count":"2","comment_id":"1195419","timestamp":"1728898260.0","content":"Selected Answer: A\nI think a ist correct"}],"question_text":"You are working at a financial institution that stores mortgage loan approval documents on Cloud Storage. Any change to these approval documents must be uploaded as a separate approval file. You need to ensure that these documents cannot be deleted or overwritten for the next 5 years. What should you do?","exam_id":4,"question_id":109,"question_images":[],"isMC":true,"answer":"A"},{"id":"9T051LAgG0LLW4oqJRUm","answer":"D","exam_id":4,"question_images":[],"topic":"1","timestamp":"2024-08-27 19:07:00","answer_description":"","question_text":"Your company has decided to make a major revision of their API in order to create better experiences for their developers. They need to keep the old version of the API available and deployable, while allowing new customers and testers to try out the new API. They want to keep the same SSL and DNS records in place to serve both APIs.\n\nWhat should they do?","discussion":[{"content":"Selected Answer: D\nD is the correct answer as two versions (Old and New) have to be maintained with a single end point exposed to the developers.","poster":"f9bc58e","comment_id":"1281260","upvote_count":"3","timestamp":"1725917640.0"},{"timestamp":"1724778420.0","comment_id":"1273593","upvote_count":"4","content":"Selected Answer: D\nThis solution allows you to meet all the requirements in a clean and maintainable way","poster":"JamesKarianis"}],"choices":{"C":"Have the old API forward traffic to the new API based on the path","A":"Configure a new load balancer for the new version of the API","B":"Reconfigure old clients to use a new endpoint for the new API","D":"Use separate backend pools for each API path behind the load balancer"},"url":"https://www.examtopics.com/discussions/google/view/146571-exam-professional-cloud-architect-topic-1-question-198/","answer_images":[],"answer_ET":"D","unix_timestamp":1724778420,"isMC":true,"question_id":110,"answers_community":["D (100%)"]}],"exam":{"isMCOnly":false,"lastUpdated":"11 Apr 2025","name":"Professional Cloud Architect","id":4,"numberOfQuestions":279,"isImplemented":true,"provider":"Google","isBeta":false},"currentPage":22},"__N_SSP":true}