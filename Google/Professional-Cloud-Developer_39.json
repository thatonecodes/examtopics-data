{"pageProps":{"questions":[{"id":"d06gMeOwVmXDWxoomCpB","answer":"A","timestamp":"2023-11-13 20:17:00","answer_images":[],"question_text":"Your company's security team uses Identity and Access Management (IAM) to track which users have access to which resources. You need to create a version control system that can integrate with your security team's processes. You want your solution to support fast release cycles and frequent merges to your main branch to minimize merge conflicts. What should you do?","choices":{"D":"Create a GitHub repository, mirror it to a Cloud Source Repositories repository, and use feature-based development.","C":"Create a GitHub repository, mirror it to a Cloud Source Repositories repository, and use trunk-based development.","B":"Create a Cloud Source Repositories repository, and use feature-based development.","A":"Create a Cloud Source Repositories repository, and use trunk-based development."},"answer_description":"","topic":"1","isMC":true,"answer_ET":"A","exam_id":7,"question_images":[],"answers_community":["A (56%)","C (44%)"],"unix_timestamp":1699903020,"question_id":191,"discussion":[{"poster":"pico","comment_id":"1225530","upvote_count":"1","content":"Selected Answer: A\nC and D (GitHub repository, mirrored to CSR): While GitHub offers feature-based development workflows, mirroring to CSR introduces an additional step and potential delay in the development process. It also doesn't provide the same level of native IAM integration as using CSR directly.","timestamp":"1733498160.0"},{"content":"Selected Answer: A\nI will go for A instead of C because it's more simple and straightforward.\n\nThere isn't a requirement to have PR's or something like that to choose to have GitHub.","upvote_count":"1","timestamp":"1722880020.0","comment_id":"1141403","poster":"JonathanSJ"},{"content":"Selected Answer: A\nI choose A.\n\nWhere does the GitHub repository requirement come from? \n\nThe security team uses IAM for user accesses and we only need to create a version control system that can integrate with their processes. IAM can't control who is pushing stuff in GitHub and with options C or D it will end up in CSR regardless.\n\nFast release cycles and frequent merges to the main branch to minimize merge conflicts -> trunk-based development.","poster":"wanrltw","comment_id":"1100884","upvote_count":"3","timestamp":"1718813280.0"},{"poster":"plutonians123","timestamp":"1717365420.0","content":"Selected Answer: C\n\"You want your solution to support fast release cycles and frequent merges to your main branch to minimize merge conflicts.\"\n\nThe requirement for fast release cycles and frequent merges with minimal merge conflicts aligns well with trunk-based development. In trunk-based development, developers work in short-lived branches and merge their changes frequently into the main branch, which helps in reducing merge conflicts and supports a more rapid and continuous release cycle.","comment_id":"1086483","upvote_count":"1"},{"comment_id":"1071774","upvote_count":"1","content":"Selected Answer: C\nC. using an external git repository as Github or Bitbucket synced with Cloud Repositories is a best practice, as it provides with a lot more features as PRs and branch permissions. Trunk or master development is okay in this case as is a small project with fast development. For more large projects, larger teams is better to use a feature development as GitFlow","poster":"diegodoal","timestamp":"1715790000.0"},{"poster":"vspringe","content":"Selected Answer: C\nGitHub Repository: GitHub is a popular and powerful platform for version control. It supports a wide range of development workflows and integrates well with various CI/CD tools, making it suitable for fast release cycles.\n\nMirroring to Cloud Source Repositories: By mirroring the GitHub repository to Google Cloud Source Repositories, you can leverage Google Cloud's IAM features for access control and security. This integration allows your security team to track and manage user access effectively within the Google Cloud environment.\n\nTrunk-Based Development: This development methodology involves developers merging their changes into the main branch frequently. It's well-suited for fast-paced development environments, as it minimizes the duration of branches and reduces the likelihood of significant merge conflicts.","comment_id":"1069672","upvote_count":"2","timestamp":"1715620620.0"}],"url":"https://www.examtopics.com/discussions/google/view/125973-exam-professional-cloud-developer-topic-1-question-270/"},{"id":"o149yGc7FAWpvwENWXYJ","timestamp":"2023-11-13 20:21:00","answer":"C","answer_images":[],"question_text":"You recently developed an application that monitors a large number of stock prices. You need to configure Pub/Sub to receive messages and update the current stock price in an in-memory database. A downstream service needs the most up-to-date prices in the in-memory database to perform stock trading transactions. Each message contains three pieces or information:\n\n• Stock symbol\n• Stock price\n• Timestamp for the update\n\nHow should you set up your Pub/Sub subscription?","choices":{"B":"Create a pull subscription with both ordering and exactly-once delivery turned off.","A":"Create a push subscription with exactly-once delivery enabled.","C":"Create a pull subscription with ordering enabled, using the stock symbol as the ordering key.","D":"Create a push subscription with both ordering and exactly-once delivery turned off."},"answer_description":"","isMC":true,"topic":"1","answer_ET":"C","question_images":[],"exam_id":7,"answers_community":["C (63%)","A (25%)","13%"],"unix_timestamp":1699903260,"question_id":192,"discussion":[{"upvote_count":"1","poster":"ravia__","content":"Selected Answer: C\nOrdering enabled: In this scenario, it's crucial that updates for the same stock symbol arrive in the correct order because the latest price update must reflect the actual current stock price. Enabling ordering ensures that messages with the same ordering key (e.g., stock symbol) are delivered in the order they were published.\n\nPull subscription: This gives your application control over when and how messages are processed, which is often beneficial for managing concurrency and ensuring that updates are handled properly.\n\nStock symbol as the ordering key: This ensures that updates for the same stock are processed in sequence, preventing issues where an older price update could overwrite a newer one.","comment_id":"1280299","timestamp":"1725791340.0"},{"upvote_count":"1","poster":"mrgarfield","comment_id":"1280278","content":"Selected Answer: D\nI'll go with D, push subscription because of large amount of data, exactly once is not possible for push according to the documentation, and it's not important here to have exactly once. If rate is not changing, the event shouldn't be published anyway","timestamp":"1725785280.0"},{"timestamp":"1725785220.0","comment_id":"1280277","content":"I'll go with D, push subscription because of large amount of data, exactly once is not possible for push according to the documentation, and it's not important here to have exactly once. If rate is not changing, the event shouldn't be published anyway","upvote_count":"1","poster":"mrgarfield"},{"upvote_count":"1","content":"Selected Answer: A\niven the requirement that the downstream service needs the most up-to-date prices, you don't need to order the messages. In this case, a push subscription would be suitable as it delivers messages as they arrive, ensuring the downstream service receives the latest stock prices promptly. Exactly-once delivery can prevent the same message from being processed multiple times, which could be beneficial in this scenario.","poster":"alpha_canary","comment_id":"1193650","timestamp":"1712828220.0"},{"timestamp":"1707163740.0","comment_id":"1141424","poster":"JonathanSJ","content":"Selected Answer: A\nI will go for A.\n\nB and C aren't good because you need to receive the prices in real time as they come.\n\nBetween A and D:\n\nD with exactly-once delivery turned off you can process the same message many times and it isn't good for financial systems.","upvote_count":"1","comments":[{"comment_id":"1141597","poster":"JonathanSJ","timestamp":"1707175140.0","content":"I change my mind.\n\nI will go for D, because the question says “A downstream service needs the most up-to-date prices”.\nThe pull subscriptions introduce the possibility of latency between the time a message is published and when it's pulled by the app.\nThen B and C are discarded.\n\nBetween A and D, A is not correct because the exactly-once delivery feature is only for pull subscriptions.\n\nAlso, this questions is a duplicate from Question #271.","upvote_count":"2"}]},{"comment_id":"1087025","content":"A is correct (I am not a contributor, so unable to vote). Rationale:\n\n1. The downstream application needs only the most up-to-date value for a stock price. There's no need of historical values from a time series, so \"ordering\" does not make any sense in this scenario. This eliminates alternatives B, C and D. In addition, in alternative C, \"using the stock symbol as the ordering key\" has no practical effect, once \"ordering\" is not necessary.\n\n2. About \"push\" and \"pull\": in a \"push\" subscription, whenever the topic is fed with a new value, it will keep pushing it to the application until an acknowledgement is received. Latency is lower in this case. In a \"pull\" subscription, there's an additional burden on the application to keep pulling from the topic. This increases latency. A \"push\" subscription is recommended in such scenarios.","timestamp":"1701627840.0","upvote_count":"4","poster":"pbrvgl"},{"content":"Selected Answer: C\nPull Subscription for Controlled Processing: A pull subscription gives you control over when and how messages are processed. This can be particularly important for maintaining the integrity of the in-memory database, as it allows for more deliberate handling of message backlogs and peak loads.\n\nMessage Ordering Is Crucial: The ordering of stock price updates is critical. Using the stock symbol as the ordering key ensures that updates for a specific stock are processed in the order they were sent. This is vital to ensure the accuracy of stock price data, as prices must be updated in the sequence they were received to reflect the true market conditions.\n\nNo Need for Exactly-Once Delivery: In most financial data scenarios, the latest data supersedes the old. If a message is delivered more than once, the last update for a given timestamp will leave the database in the correct state. Therefore, exactly-once delivery, which can add complexity and overhead, might not be necessary.","upvote_count":"4","poster":"vspringe","comment_id":"1069673","timestamp":"1699903260.0"}],"url":"https://www.examtopics.com/discussions/google/view/125974-exam-professional-cloud-developer-topic-1-question-271/"},{"id":"yU8ZdACzGULtpwh4FxLm","answers_community":["B (100%)"],"answer_ET":"B","url":"https://www.examtopics.com/discussions/google/view/128264-exam-professional-cloud-developer-topic-1-question-272/","question_text":"You are a developer at a social media company. The company runs their social media website on-premises and uses MySQL as a backend to store user profiles and user posts. Your company plans to migrate to Google Cloud, and your learn will migrate user profile information to Firestore. You are tasked with designing the Firestore collections. What should you do?","answer_images":[],"question_id":193,"answer":"B","choices":{"C":"Create one root collection for user profiles, and store each user's post as a nested list in the user profile document.","D":"Create one root collection for user posts, and create one subcollection for each user's profile.","B":"Create one root collection for user profiles, and create one subcollection for each user's posts.","A":"Create one root collection for user profiles, and create one root collection for user posts."},"unix_timestamp":1702275840,"topic":"1","exam_id":7,"isMC":true,"discussion":[{"upvote_count":"1","comment_id":"1193653","timestamp":"1728639720.0","content":"Selected Answer: B\nB: The best practice for this scenario would be to create one root collection for user profiles and one subcollection for each user's posts. This allows for easy retrieval of all posts for a given user and aligns well with the hierarchical data model of Firestore. It also provides good isolation of data as each user's posts are stored separately.","poster":"alpha_canary"},{"content":"Selected Answer: B\nI will go for B.","timestamp":"1722881460.0","upvote_count":"1","poster":"JonathanSJ","comment_id":"1141427"},{"timestamp":"1719388380.0","comment_id":"1105877","poster":"Kadhem","content":"Selected Answer: B\nB is more appropriate","upvote_count":"1"},{"comment_id":"1093141","poster":"kapara","timestamp":"1718079840.0","upvote_count":"1","content":"Selected Answer: B\nFor migrating user profile information to Firestore in your social media company, the best approach is:\n\nB. Create one root collection for user profiles, and create one subcollection for each user's posts.\n\nThis structure offers better scalability, efficient data retrieval, and clearer organization, while also simplifying access control and data modeling. Options A, C, and D are less optimal due to potential performance issues, complex querying, and counterintuitive data relationships."}],"question_images":[],"answer_description":"","timestamp":"2023-12-11 07:24:00"},{"id":"4OiPiMgoUwtm7Mrm704x","url":"https://www.examtopics.com/discussions/google/view/125968-exam-professional-cloud-developer-topic-1-question-273/","unix_timestamp":1699902300,"isMC":true,"answers_community":["B (100%)"],"answer_description":"","topic":"1","choices":{"D":"Write a script that pulls the memory consumption of the instance at the OS level and sends an email alert if the average memory consumption is outside the defined range.","B":"In Cloud Monitoring, create an alerting policy to notify you if the average memory consumption is outside the defined range.","A":"Create a Cloud Function that consumes the Monitoring API. Create a schedule to trigger the Cloud Function hourly and alert you if the average memory consumption is outside the defined range.","C":"Create a Cloud Function that runs on a schedule, executes kubectl top on all the workloads on the cluster, and sends an email alert if the average memory consumption is outside the defined range."},"question_text":"Your team recently deployed an application on Google Kubernetes Engine (GKE). You are monitoring your application and want to be alerted when the average memory consumption of your containers is under 20% or above 80%. How should you configure the alerts?","exam_id":7,"answer":"B","question_id":194,"question_images":[],"discussion":[{"content":"Selected Answer: B\nI will go for B.","timestamp":"1722881520.0","comment_id":"1141430","poster":"JonathanSJ","upvote_count":"1"},{"poster":"plutonians123","content":"Selected Answer: B\nTo monitor average memory consumption of containers in Google Kubernetes Engine (GKE), the best approach is to use Cloud Monitoring. You can create an alerting policy in Cloud Monitoring to notify you if the average memory consumption is outside the defined range. This method leverages Google Cloud's built-in capabilities for efficient and accurate monitoring.\n\nFor detailed instructions, please refer to:\nGoogle Cloud Documentation: https://cloud.google.com/monitoring","comment_id":"1096026","upvote_count":"1","timestamp":"1718332320.0"},{"content":"Selected Answer: B\nNo need to use CloudFunction in this case and the custom script is overengineering. Answer B fits the needs.","poster":"dar_gor","comment_id":"1092369","upvote_count":"1","timestamp":"1718004300.0"},{"poster":"plutonians123","upvote_count":"1","timestamp":"1717311240.0","comment_id":"1085996","content":"Selected Answer: B\nCloud Monitoring provides a user-friendly interface to create complex alerting policies. You can set up thresholds for specific metrics, like memory consumption, and receive notifications if these thresholds are exceeded or undercut. This feature negates the need for custom scripts or functions to monitor these metrics."},{"timestamp":"1716492480.0","poster":"MoanaMacknzy","comment_id":"1078833","content":"Selected Answer: B\nB is the right answer, alert in the cloud monitoring","upvote_count":"1"},{"upvote_count":"2","content":"B. using Cloud Monitoring to create an alerting policy is the most efficient, reliable, and straightforward method to monitor and be alerted about the memory consumption of your containers in GKE.","comment_id":"1069661","timestamp":"1715619900.0","poster":"vspringe"}],"answer_ET":"B","timestamp":"2023-11-13 20:05:00","answer_images":[]},{"id":"pyesx2f1puBrD2tZk6Sd","question_text":"You manage a microservice-based ecommerce platform on Google Cloud that sends confirmation emails to a third-party email service provider using a Cloud Function. Your company just launched a marketing campaign, and some customers are reporting that they have not received order confirmation emails. You discover that the services triggering the Cloud Function are receiving HTTP 500 errors. You need to change the way emails are handled to minimize email loss. What should you do?","timestamp":"2023-11-13 20:08:00","answers_community":["B (100%)"],"url":"https://www.examtopics.com/discussions/google/view/125969-exam-professional-cloud-developer-topic-1-question-274/","answer_description":"","answer_images":[],"answer_ET":"B","choices":{"B":"Configure the sender application to publish the outgoing emails in a message to a Pub/Sub topic. Update the Cloud Function configuration to consume the Pub/Sub queue.","D":"Configure the sender application to retry the execution of the Cloud Function every one second if a request fails.","A":"Increase the Cloud Function's timeout to nine minutes.","C":"Configure the sender application to write emails to Memorystore and then trigger the Cloud Function. When the function is triggered, it reads the email details from Memorystore and sends them to the email service."},"isMC":true,"question_id":195,"discussion":[{"poster":"plutonians123","content":"Selected Answer: B\nThis is a robust and scalable approach. By decoupling the email sending process using Pub/Sub, you introduce a queueing mechanism. This ensures that even if the Cloud Function encounters an issue, the email messages are not lost but remain in the queue. Additionally, Pub/Sub can handle high throughput and provides retry mechanisms.","timestamp":"1733135580.0","upvote_count":"3","comment_id":"1086057"},{"poster":"MoanaMacknzy","upvote_count":"1","comment_id":"1078835","content":"Selected Answer: B\nB is the right answer","timestamp":"1732397400.0"},{"poster":"diegodoal","comment_id":"1071785","timestamp":"1731695640.0","upvote_count":"1","content":"Selected Answer: B\nB. With pub sub you can scale the load of sending emails to the Cloud Function. Also can configure exponential backoff if errors arise in the third party service and ensure the email is delivered"},{"comment_id":"1069664","upvote_count":"1","poster":"vspringe","content":"B. the most effective solution would be B. - using Pub/Sub to queue email messages and having the Cloud Function process these messages is a robust, scalable, and reliable way to handle email sending in your ecommerce platform, especially during high load conditions.","timestamp":"1731524880.0"}],"exam_id":7,"topic":"1","unix_timestamp":1699902480,"answer":"B","question_images":[]}],"exam":{"numberOfQuestions":338,"isImplemented":true,"name":"Professional Cloud Developer","lastUpdated":"11 Apr 2025","isMCOnly":false,"isBeta":false,"provider":"Google","id":7},"currentPage":39},"__N_SSP":true}