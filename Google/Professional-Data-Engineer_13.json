{"pageProps":{"questions":[{"id":"vKRKezjx74HEbjuau822","isMC":true,"choices":{"D":"Use Kafka Connect to link your Kafka message queue to Pub/Sub. Use a Dataflow template to write your messages from Pub/Sub to BigQuery. Use Cloud Scheduler to run a script every five minutes that counts the number of rows created in BigQuery in the last hour. If that number falls below 4000, send an alert.","B":"Consume the stream of data in Dataflow using Kafka IO. Set a fixed time window of 1 hour. Compute the average when the window closes, and send an alert if the average is less than 4000 messages.","C":"Use Kafka Connect to link your Kafka message queue to Pub/Sub. Use a Dataflow template to write your messages from Pub/Sub to Bigtable. Use Cloud Scheduler to run a script every hour that counts the number of rows created in Bigtable in the last hour. If that number falls below 4000, send an alert.","A":"Consume the stream of data in Dataflow using Kafka IO. Set a sliding time window of 1 hour every 5 minutes. Compute the average when the window closes, and send an alert if the average is less than 4000 messages."},"answer_description":"","answer_images":[],"timestamp":"2020-03-22 08:12:00","answer_ET":"A","question_images":[],"answers_community":["A (100%)"],"unix_timestamp":1584861120,"question_text":"You operate an IoT pipeline built around Apache Kafka that normally receives around 5000 messages per second. You want to use Google Cloud Platform to create an alert as soon as the moving average over 1 hour drops below 4000 messages per second. What should you do?","topic":"1","question_id":61,"url":"https://www.examtopics.com/discussions/google/view/17218-exam-professional-data-engineer-topic-1-question-153/","answer":"A","exam_id":11,"discussion":[{"content":"Should be A","timestamp":"1600751520.0","comment_id":"66839","upvote_count":"27","poster":"[Removed]"},{"upvote_count":"17","poster":"[Removed]","timestamp":"1601043720.0","content":"Correct: A\n\nDataflow can connect with Kafka and sliding window is used for taking averages","comment_id":"68164"},{"poster":"mothkuri","timestamp":"1725460080.0","content":"Selected Answer: A\nOption A is correct answer.\nOption B is not correct. There could be a chance middle of 1st window to middle of 2nd window less messages(i.e > 4000).\nOption C & D out of scope.","comment_id":"1165793","upvote_count":"2"},{"content":"Selected Answer: A\nDataflow with Sliding Time Windows: Dataflow allows you to work with event-time windows, making it suitable for time-series data like incoming IoT messages. Using sliding windows every 5 minutes allows you to compute moving averages efficiently.\n\nSliding Time Window: The sliding time window of 1 hour every 5 minutes enables you to calculate the moving average over the specified time frame.\n\nComputing Averages: You can efficiently compute the average when each sliding window closes. This approach ensures that you have real-time visibility into the message rate and can detect deviations from the expected rate.\n\nAlerting: When the calculated average drops below 4000 messages per second, you can trigger an alert from within the Dataflow pipeline, sending it to your desired alerting mechanism, such as Cloud Monitoring, Pub/Sub, or another notification service.\n\nScalability: Dataflow can scale automatically based on the incoming data volume, ensuring that you can handle the expected rate of 5000 messages per second.","upvote_count":"2","poster":"barnac1es","timestamp":"1711294860.0","comment_id":"1015838"},{"upvote_count":"2","comment_id":"963242","content":"Selected Answer: A\nOption A\n\nPros:\n\nThis option is relatively simple to implement.\nIt can be used to compute the moving average over any time window.\nCons:\n\nThis option can be computationally expensive, especially if the data stream is large.\nIt can be difficult to troubleshoot if the alert does not fire when it is supposed to.","poster":"vamgcp","timestamp":"1706242320.0"},{"timestamp":"1699620840.0","content":"Selected Answer: A\nthe correct answer is between A and B since it doesn't make sense to use Pub/Sub combined with Kafka. To have a Moving Average then we should go for A, updating the average estimation every 5 minutes using the new data that came in and eliminating the \"most far\" 5 minutes.","upvote_count":"2","poster":"vaga1","comment_id":"893837"},{"poster":"zellck","upvote_count":"4","content":"Selected Answer: A\nA is the answer.\n\nhttps://cloud.google.com/dataflow/docs/concepts/streaming-pipelines#windows\nWindowing functions divide unbounded collections into logical components, or windows. Windowing functions group unbounded collections by the timestamps of the individual elements. Each window contains a finite number of elements.\n\nYou set the following windows with the Apache Beam SDK or Dataflow SQL streaming extensions:\n- Hopping windows (called sliding windows in Apache Beam)\n\nA hopping window represents a consistent time interval in the data stream. Hopping windows can overlap, whereas tumbling windows are disjoint.\n\nFor example, a hopping window can start every thirty seconds and capture one minute of data. The frequency with which hopping windows begin is called the period. This example has a one-minute window and thirty-second period.","timestamp":"1685614500.0","comment_id":"732576"},{"content":"Selected Answer: A\nas explained by Alasmindas","poster":"medeis_jar","upvote_count":"2","timestamp":"1657356240.0","comment_id":"520128"},{"timestamp":"1654943100.0","upvote_count":"2","comment_id":"499389","content":"Selected Answer: A\nCorrect Answer: A","poster":"AACHB"},{"comment_id":"486425","upvote_count":"1","content":"Correct: A","poster":"JG123","timestamp":"1653444540.0"},{"content":"A is enough","poster":"Chelseajcole","upvote_count":"1","comment_id":"455572","timestamp":"1648819200.0"},{"poster":"daghayeghi","content":"A:\nthe correct answer is between A and B, But because used \"Moving Average\" then we should go for A.","timestamp":"1629397980.0","comment_id":"294585","upvote_count":"2"},{"upvote_count":"2","content":"yes , using KafkaIO , we can connect to Kafka cluster.","comment_id":"256667","poster":"apnu","timestamp":"1625111280.0"},{"timestamp":"1624502220.0","upvote_count":"3","content":"yes A is correct , because sliding window can only help here.","comment_id":"251440","poster":"ashuchip"},{"comment_id":"218912","content":"Option A is the correct answer. Reasons:-\na) Kafka IO and Dataflow is a valid option for interconnect (needless where Kafka is located - On Prem/Google Cloud/Other cloud) \nb) Sliding Window will help to calculate average.\n\nOption C and D are overkill and complex, considering the scenario in the question,\nhttps://cloud.google.com/solutions/processing-messages-from-kafka-hosted-outside-gcp","timestamp":"1620957480.0","poster":"Alasmindas","upvote_count":"7"},{"poster":"Alasmindas","comment_id":"218906","content":"Option A is the correct answer. Reasons:-\na) Kafka IO and Dataflow is a valid option for interconnect (needless where Kafka is located - On Prem/Google Cloud/Other cloud) \nb) Sliding Window will help to calculate average.\n\nOption C and D are overkill and complex, considering the scenario in the question,","upvote_count":"6","timestamp":"1620956460.0"},{"timestamp":"1614533820.0","content":"A\nTo take running averages of data, use hopping windows. You can use one-minute hopping windows with a thirty-second period to compute a one-minute running average every thirty seconds.","comment_id":"168573","upvote_count":"2","poster":"atnafu2020"},{"comments":[{"upvote_count":"3","poster":"FARR","timestamp":"1613276160.0","comment_id":"157745","content":"Yes, via KafkaIO. See the link in above comment"}],"poster":"Prakzz","content":"I don't think its A or B. Dataflow can't connect directly to kafka.","upvote_count":"1","timestamp":"1612286400.0","comment_id":"149209"},{"comments":[{"poster":"SPutri","timestamp":"1621469160.0","content":"the link that you share above is saying, \"..illustrates a popular scenario: you use Dataflow to process the messages, where Kafka is hosted either on-premises or in another public cloud such as Amazon Web Services (AWS).\"\nbut in this case, we are processing data coming from IoT pipeline, not from on-premise or other cloud. so, i don't think A is the proper solution. I consider option C instead.","upvote_count":"2","comment_id":"223233"}],"poster":"kino2020","upvote_count":"4","content":"\"You operate an IoT pipeline built around Apache Kafka\"\nThe statement in question states. Therefore, building with kafka is the requirement definition for this problem.\n\nJust in case you are wondering, a case along with this problem is listed on google by the architects.\n\"Using Cloud Dataflow to Process Outside-Hosted Messages from Kafka\"\nhttps://cloud.google.com/solutions/processing-messages-from-kafka-hosted-outside-gcp\n\nTherefore, A is the correct answer.","timestamp":"1611498900.0","comment_id":"142724"},{"content":"C and D is incorrect . It moves data from one queue(Kafka) to another queue (Pub/Sub).\nA is Correct.","timestamp":"1610030880.0","poster":"Rajuuu","upvote_count":"3","comment_id":"128966","comments":[{"comment_id":"441120","upvote_count":"1","poster":"Ral17","timestamp":"1646697960.0","content":"Why not B?"}]},{"comment_id":"126296","content":"A, As we need to capture the moving average that's sliding window. \nOption D also looks good but complex and costly, using dataflow with kfka IO a better option","upvote_count":"3","poster":"saurabhsingh92","timestamp":"1609783920.0"},{"poster":"AJKumar","timestamp":"1605382740.0","comment_id":"89075","content":"Correct Answer C - Question specifically mentions use of GCP (Pub/Sub-Dataflow).","upvote_count":"3"},{"comment_id":"71677","content":"Why not C? It uses the Google cloud platform and that is what the question states i believe?","poster":"mrkuul1","timestamp":"1601961360.0","upvote_count":"5"},{"timestamp":"1601685180.0","content":"Should be A.","poster":"Rajokkiyam","comment_id":"70574","upvote_count":"6"}]},{"id":"VoZoo7ckzaBdwg2U9c6Z","discussion":[{"content":"A should be correct answer","comment_id":"64410","timestamp":"1600186620.0","upvote_count":"32","comments":[{"upvote_count":"5","poster":"tycho","comment_id":"504309","content":"yes A is correct, whe creating ne cloud sql instance there is an option\n \"Multiple zones (Highly available)\nAutomatic failover to another zone within your selected region. Recommended for production instances. Increases cost.\"","timestamp":"1655560320.0"}],"poster":"madhu1171"},{"content":"Correct: A\n\nhttps://cloud.google.com/sql/docs/mysql/high-availability","poster":"[Removed]","timestamp":"1601043960.0","upvote_count":"14","comment_id":"68165"},{"upvote_count":"2","content":"Answer : A\nQuestion is about high availability in the event of zone failure. So create Fail over replica in another zone in same region.","comment_id":"1163908","poster":"mothkuri","timestamp":"1725242700.0"},{"content":"Selected Answer: A\nA (failover replicas) as this is an old question:\n\nIn a legacy HA configuration, a Cloud SQL for MySQL instance uses a failover replica to add high availability to the instance. This functionality isn't available in Google Cloud console. \n\nThe new configuration doesn't use failover replicas. Instead, it uses Google's regional persistent disks, which synchronously replicate data at the block-level between two zones in a region.\nhttps://cloud.google.com/sql/docs/mysql/configure-legacy-ha","poster":"MaxNRG","timestamp":"1718775180.0","comment_id":"1100414","upvote_count":"6"},{"poster":"pss111423","content":"Option A is good fro leagacy soultion \nNote: Cloud SQL plans to discontinue support for legacy HA instances in the future and will soon be announcing a date to do so. Currently, legacy HA instances are still covered by the Cloud SQL SLA. We recommend you upgrade your existing legacy HA instances to regional persistent disk HA instances and create new instances using regional persistent disk HA as soon as possible\nOption C makes more sense in this regrard","comment_id":"1076632","upvote_count":"1","timestamp":"1716315360.0"},{"poster":"emmylou","content":"A - Although it is legacy and will be deprecated. The correct answer is not an option--\n\"The legacy configuration for high availability used a failover replica instance. The new configuration does not use a failover replica. Instead, it uses Google's regional persistent disks, which synchronously replicate data at the block level between two zones in a region.\"","timestamp":"1715704140.0","comment_id":"1070731","upvote_count":"1"},{"content":"Selected Answer: A\nFailover Replica: By creating a failover replica in another zone within the same region, you establish a high-availability configuration. The failover replica is kept in sync with the primary instance, and it can quickly take over in case of a failure of the primary instance.\n\nSame Region: Placing the failover replica in the same region ensures minimal latency and data consistency. In the event of a zone failure, the failover can happen within the same region, reducing potential downtime.\n\nZone Resilience: Google Cloud's regional design ensures that zones within a region are independent of each other, which adds resilience to zone failures.\n\nAutomatic Failover: In case of a primary instance failure, Cloud SQL will automatically promote the failover replica to become the new primary instance, minimizing downtime.","timestamp":"1711295160.0","comment_id":"1015843","poster":"barnac1es","upvote_count":"2"},{"timestamp":"1708797900.0","upvote_count":"1","poster":"samstar4180","content":"Per latest Google cloud document, B is the correct answer.","comment_id":"989325"},{"upvote_count":"3","poster":"wan2three","timestamp":"1705411380.0","content":"Selected Answer: B\nCross-region read replicas\nCross-region replication lets you create a read replica in a different region from the primary instance. You create a cross-region read replica the same way as you create an in-region replica.\n\nCross-region replicas:\n\nImprove read performance by making replicas available closer to your application's region.\nProvide additional disaster recovery capability to guard against a regional failure.\nLet you migrate data from one region to another.\nhttps://cloud.google.com/sql/docs/mysql/replication#cross-region-read-replicas:~:text=memory%20(OOM)%20events.-,Cross%2Dregion%20read%20replicas,Let%20you%20migrate%20data%20from%20one%20region%20to%20another.,-See%20Promoting%20replicas","comment_id":"953254"},{"comment_id":"946572","content":"Selected Answer: B\nThe legacy process for adding high availability to MySQL instances uses a failover replica. The legacy functionality isn't available in the Google Cloud console. See Legacy configuration: Creating a new instance configured for high availability or Legacy configuration: Configuring an existing instance for high availability.","poster":"MoeHaydar","upvote_count":"2","timestamp":"1704733020.0"},{"comment_id":"941387","timestamp":"1704257280.0","poster":"KK0202","content":"Selected Answer: B\nThe correct answer is most probably B as this his scenario has an update(As of July 2023). Failover replicas are not available anymore. Same region different zone read replicas are used in case of a failover or if primary zone is not available","upvote_count":"4"},{"comment_id":"902712","timestamp":"1700510340.0","poster":"MBRSDG","content":"Selected Answer: B\nThe answer is B, the failover replica is a legacy feature. \nSee here: https://cloud.google.com/sql/docs/mysql/high-availability#legacy_mysql_high_availability_option","comments":[{"upvote_count":"1","poster":"forepick","comment_id":"912040","timestamp":"1701438840.0","content":"Read replica isn't an alternative to the standby instance"}],"upvote_count":"2"},{"upvote_count":"3","poster":"vaga1","timestamp":"1699621380.0","content":"Selected Answer: A\nread replica (B) and external read replica (C) doesn't make sense here, since we potentially need all the functionalities. Using Cloud SQL in a region combined with Cloud Storage backup may not be the best choice (D) thinking about compliance reasons starting from what has been asked, it seems also \"too much\" compared with A that fullfills the request with simpler actions. Also, compliance is required at the regional level, so then A fits.","comment_id":"893843"},{"content":"Failover replica's are a legacy feature. This question is outdated: https://cloud.google.com/sql/docs/mysql/configure-ha","upvote_count":"6","timestamp":"1694172060.0","comment_id":"832990","poster":"wjtb"},{"timestamp":"1692353820.0","poster":"musumusu","upvote_count":"2","content":"Answer A, key words to remember, High Scale use extra read replica. High availablity use extra failure replica. Both should be in different zone but in same region.","comment_id":"812933"},{"poster":"desertlotus1211","comment_id":"789198","comments":[{"timestamp":"1690411200.0","comment_id":"789199","upvote_count":"1","content":"The questions asks to ensure high availability in the event of a zone failure","poster":"desertlotus1211"}],"upvote_count":"2","timestamp":"1690411140.0","content":"Answer is B: https://cloud.google.com/sql/docs/mysql/replication#read-replicas\n\n'As a best practice, put read replicas in a different zone than the primary instance when you use HA on your primary instance'"},{"upvote_count":"2","comment_id":"732572","poster":"zellck","content":"Selected Answer: A\nA is the answer.\n\nhttps://cloud.google.com/sql/docs/mysql/high-availability#HA-configuration\nThe HA configuration provides data redundancy. A Cloud SQL instance configured for HA is also called a regional instance and has a primary and secondary zone within the configured region. Within a regional instance, the configuration is made up of a primary instance and a standby instance. Through synchronous replication to each zone's persistent disk, all writes made to the primary instance are replicated to disks in both zones before a transaction is reported as committed. In the event of an instance or zone failure, the standby instance becomes the new primary instance. Users are then rerouted to the new primary instance. This process is called a failover.","timestamp":"1685614320.0"},{"content":"Selected Answer: A\nA should be the answer as the question is asking HA in event of a zone failure. \n”Read Replicas CAN be promoted to master nodes in the case of DR. However, there is downtime entailed.\nFailover Replicas are designed to automatically become master nodes.”\nhttps://googlecloudarchitect.us/read-replica-versus-failover-replica-in-cloud-sql/","comment_id":"701560","timestamp":"1682171340.0","upvote_count":"2","poster":"louisgcpde"},{"poster":"dn_mohammed_data","timestamp":"1679496480.0","upvote_count":"1","content":"vote for A","comment_id":"676134"},{"upvote_count":"1","content":"Selected Answer: B\nB https://cloud.google.com/sql/docs/mysql/high-availability#read_replicas\nThis is the trick To find out the Answer without Much knowledge\nIf you try count the words Read replica asb compared to with faiuare repkica , The former is more than another one. It must be anwser.","comment_id":"666689","poster":"John_Pongthorn","timestamp":"1678612440.0"},{"content":"Selected Answer: A\nshould be A","comment_id":"657556","poster":"PhuocT","upvote_count":"1","timestamp":"1677778200.0"},{"content":"Selected Answer: A\nReplicas can be created within the same region as the primary instance. \nAnd Failover replica stored in other zone within the same region is used in case of any failure of the primary instance.","upvote_count":"1","poster":"Zorro27","comment_id":"630648","timestamp":"1673556000.0"},{"timestamp":"1668514500.0","upvote_count":"2","content":"Can't be B \"Read replicas cannot be made highly available like primary instances. During a zonal outage, traffic to read replicas in that zone stops\"\nhttps://cloud.google.com/sql/docs/mysql/high-availability\nA seems to be correct","poster":"watiobil","comment_id":"602028"},{"timestamp":"1657356360.0","content":"Selected Answer: A\nhttps://cloud.google.com/sql/docs/mysql/high-availability\n\"Multiple zones (Highly available)\nAutomatic failover to another zone within your selected region. Recommended for production instances. Increases cost.\"","poster":"medeis_jar","upvote_count":"2","comment_id":"520130"},{"timestamp":"1653444720.0","upvote_count":"1","content":"Correct: A","comment_id":"486426","poster":"JG123"},{"timestamp":"1653306960.0","comment_id":"485078","poster":"chpoola","upvote_count":"1","content":"A should be the right answer even though it is legacy \nI got confused with option : D , but D just stores the data as back up but not reroute the request"},{"comment_id":"416071","content":"Seems to depend on the date the question was published.\n\nA) is mentioned in legacy config \"The legacy configuration for high availability used a failover replica instance.\" https://cloud.google.com/sql/docs/mysql/configure-legacy-ha\nB) Read replica is mentioned here https://cloud.google.com/sql/docs/mysql/high-availability scroll down to the diagrams\nC) external read replica seems to be wrong, never heard of it\nD) Aligns which Google steps, but automatic backup is not mentioned for HA https://cloud.google.com/sql/docs/mysql/configure-ha#ha-create just regional instance mentioned\n\nI would pick D now, since regional instance is described as HA, although the automatic backup seems like a bonus.","upvote_count":"7","poster":"hdmi_switch","timestamp":"1643364660.0"},{"content":"D - This is not RDS or Aurora, read replica cannot be used as a failover replica.","timestamp":"1641276240.0","upvote_count":"3","poster":"ralf_cc","comment_id":"397982"},{"timestamp":"1631165400.0","comment_id":"306202","poster":"baudrual","upvote_count":"4","content":"Option A - regarding this : \"The HA (High Availability) configuration, sometimes called a cluster, provides data redundancy. A Cloud SQL instance configured for HA is also called a regional instance and is located in a primary and secondary zone within the configured region. Within a regional instance, the configuration is made up of a primary instance and a standby instance.\" On that link : https://cloud.google.com/sql/docs/mysql/high-availability."},{"poster":"ArunSingh1028","content":"D is correct option for creating the failover while creating the instance in a region","timestamp":"1629208740.0","comment_id":"292670","upvote_count":"1"},{"content":"ANSWER-D. I Logged in GCP Console and Attempted to create Cloud SQL and all I could see is Single Zone and High Availability (Regional). There is no way I can specify failover replica.","comments":[{"upvote_count":"1","poster":"shilpa","content":"Option D, The high availability configuration is described at this link https://cloud.google.com/sql/docs/mysql/configure-ha","timestamp":"1627892520.0","comment_id":"281817"}],"comment_id":"266676","poster":"StelSen","upvote_count":"6","timestamp":"1626222960.0"},{"timestamp":"1622025780.0","content":"I think its D : as we need to ensure high availability !!\n\"Automated backups and point-in-time recovery must be enabled for high availability (point-in-time recovery uses binary logging)\"\nhttps://cloud.google.com/sql/docs/mysql/high-availability#backups-and-restores","upvote_count":"1","comments":[{"poster":"Nams_139","comment_id":"248378","upvote_count":"3","content":"Sorry, its failover replica and the answer is A","timestamp":"1624156440.0"}],"comment_id":"228309","poster":"Nams_139"},{"content":"OPTION B is correct ..... google words\n\"be sure that your application can still access data when a single instance is down\nfor any reason, which is recommended for production systems, you should choose the High\nAvailability option. That will create a second instance in a second zone and synchronously\nreplicate data from the primary instance to the standby instance\"","comment_id":"223880","poster":"federicohi","timestamp":"1621527540.0","upvote_count":"2"},{"poster":"SPutri","upvote_count":"1","timestamp":"1621482180.0","content":"i choose B.\nreason: there is no such thing called failover replica in Cloud SQL.\nhttps://cloud.google.com/sql/docs/mysql/replication","comment_id":"223307"},{"content":"From reading the link I think the option should be B.\nBest practice seems to be having a read replica in another zone which it would use for a failover. \nFailover replicas are used for legacy mysql and will be discontinued after Q1 2021 the way i read it?\nhttps://cloud.google.com/sql/docs/mysql/high-availability","poster":"Diqtator","timestamp":"1616307000.0","comment_id":"183425","upvote_count":"4"},{"comments":[{"poster":"atnafu2020","upvote_count":"3","content":"My bad its failover replica and the answer is A","comment_id":"168959","timestamp":"1614497700.0"}],"upvote_count":"2","content":"B \nFailover Replica is not in CloudSql but in Legacy Mysql\nIf an HA-configured instance becomes unresponsive, Cloud SQL automatically switches to serving data from the standby instance. This is called a failover. \nNote: If failover occurs, read replicas do not change zones; they continue to serve data even if they are in a different zone than the primary instance.\nLegacy Mysql \nUntil Q1 2021, you have the option of using the legacy process for adding high availability to MySQL instances, which uses a failover replica. The legacy functionality is not available in the Cloud Console.\nhttps://cloud.google.com/sql/docs/mysql/high-availability#normal","timestamp":"1614534300.0","poster":"atnafu2020","comment_id":"168581"},{"timestamp":"1601685240.0","poster":"Rajokkiyam","content":"Answer A","comment_id":"70575","upvote_count":"5"},{"content":"Answer: A\nDescription: Another zone will ensure failover from one zone to another","poster":"[Removed]","timestamp":"1601303460.0","upvote_count":"4","comment_id":"68916"},{"poster":"[Removed]","comment_id":"66840","upvote_count":"7","content":"Should be A","timestamp":"1600751580.0"}],"unix_timestamp":1584296220,"question_id":62,"url":"https://www.examtopics.com/discussions/google/view/16688-exam-professional-data-engineer-topic-1-question-154/","isMC":true,"question_text":"You plan to deploy Cloud SQL using MySQL. You need to ensure high availability in the event of a zone failure. What should you do?","topic":"1","answer_ET":"A","timestamp":"2020-03-15 19:17:00","answers_community":["A (61%)","B (39%)"],"answer_description":"","exam_id":11,"question_images":[],"choices":{"D":"Create a Cloud SQL instance in a region, and configure automatic backup to a Cloud Storage bucket in the same region.","B":"Create a Cloud SQL instance in one zone, and create a read replica in another zone within the same region.","A":"Create a Cloud SQL instance in one zone, and create a failover replica in another zone within the same region.","C":"Create a Cloud SQL instance in one zone, and configure an external read replica in a zone in a different region."},"answer":"A","answer_images":[]},{"id":"Owv7BqklmWTXKAK6H9Ix","question_id":63,"topic":"1","answer":"A","choices":{"A":"Apache Kafka","D":"Firebase Cloud Messaging","C":"Dataflow","B":"Cloud Storage"},"timestamp":"2022-09-06 07:57:00","url":"https://www.examtopics.com/discussions/google/view/80517-exam-professional-data-engineer-topic-1-question-155/","question_images":["https://www.examtopics.com/assets/media/exam-media/04341/0010500003.png"],"unix_timestamp":1662443820,"answer_ET":"A","discussion":[{"comment_id":"660855","poster":"YorelNation","timestamp":"1678089420.0","content":"Selected Answer: A\nA I think it's the only technology that met the requirements","upvote_count":"10"},{"upvote_count":"7","comment_id":"676138","poster":"dn_mohammed_data","timestamp":"1679496540.0","content":"vote for A: topics, offsets --> apache kafka"},{"comment_id":"1165796","content":"Selected Answer: A\nOnly Kafka can support publish/subscribe semantics on hundreds of topics","upvote_count":"1","timestamp":"1725460200.0","poster":"mothkuri"},{"poster":"barnac1es","timestamp":"1711295760.0","upvote_count":"4","content":"Ability to Seek to a Particular Offset: Kafka allows consumers to seek to a specific offset in a topic, enabling you to read data from a specific point, including back to the start of all data ever captured. This is a fundamental capability of Kafka.\n\nSupport for Publish/Subscribe Semantics: Kafka supports publish/subscribe semantics through topics. You can have hundreds of topics in Kafka, and consumers can subscribe to these topics to receive messages in a publish/subscribe fashion.\n\nRetain Per-Key Ordering: Kafka retains the order of messages within a partition. If you have a key associated with your messages, you can ensure per-key ordering by sending messages with the same key to the same partition.\n\nScalability: Kafka is designed to handle high-throughput data streaming and is capable of scaling to meet your needs.\n\nApache Kafka aligns well with the requirements you've outlined for centralized data ingestion and delivery. It's a robust choice for scenarios that involve data streaming, publish/subscribe, and retaining message ordering.","comment_id":"1015856"},{"poster":"musumusu","comment_id":"812938","upvote_count":"3","content":"Answer A: Apache Kafka\nKey words: Ingestion and Delivery together ( it is combination of pub/sub for ingestion, and delivery = dataflow+any database in gcp)\nOffset of a topic = Partition of a topic and reprocess specific part of topic, its not possible in pub/sub as it is designed for as come and go for 1 topic. \nPer key ordering. means message with same key can be process or assigned to a user in kafka.","timestamp":"1692354600.0"},{"upvote_count":"1","timestamp":"1679878680.0","content":"deberia ser la C, debido a que siempre es mejor escoger los servicios de google","comment_id":"680285","poster":"aquevedos91"}],"question_text":"Your company is selecting a system to centralize data ingestion and delivery. You are considering messaging and data integration systems to address the requirements. The key requirements are:\n✑ The ability to seek to a particular offset in a topic, possibly back to the start of all data ever captured\n✑ Support for publish/subscribe semantics on hundreds of topics\n\nRetain per-key ordering -\n//IMG//\n\nWhich system should you choose?","exam_id":11,"answer_description":"","answer_images":[],"isMC":true,"answers_community":["A (100%)"]},{"id":"JPGP39XRHNyQGcVbbjDy","topic":"1","question_id":64,"answer":"A","choices":{"B":"Deploy a Dataproc cluster. Use an SSD persistent disk and 50% preemptible workers. Store data in Cloud Storage, and change references in scripts from hdfs:// to gs://","D":"Install Hadoop and Spark on a 10-node Compute Engine instance group with preemptible instances. Store data in HDFS. Change references in scripts from hdfs:// to gs://","A":"Deploy a Dataproc cluster. Use a standard persistent disk and 50% preemptible workers. Store data in Cloud Storage, and change references in scripts from hdfs:// to gs://","C":"Install Hadoop and Spark on a 10-node Compute Engine instance group with standard instances. Install the Cloud Storage connector, and store the data in Cloud Storage. Change references in scripts from hdfs:// to gs://"},"url":"https://www.examtopics.com/discussions/google/view/17211-exam-professional-data-engineer-topic-1-question-156/","timestamp":"2020-03-22 07:31:00","unix_timestamp":1584858660,"question_images":[],"answer_ET":"A","discussion":[{"poster":"[Removed]","content":"Correct: A\n\nAsk for cost effective so persistent disk are HDD which are cheaper in comparison to SSD.","comment_id":"68169","upvote_count":"33","timestamp":"1601044440.0"},{"timestamp":"1600749060.0","comments":[{"comment_id":"506063","poster":"baubaumiaomiao","timestamp":"1655803800.0","content":"\"You need to ensure that the deployment is as cost-effective as possible\"\nhence, no SSD unless stated otherwise","upvote_count":"3"}],"content":"Confused between A and B. For r/w intensive jobs need to use SSDs. But questions doesnt state anything about the nature of the jobs. So better to start with a default option.\nChoose A","upvote_count":"16","comment_id":"66827","poster":"[Removed]"},{"comment_id":"1165800","poster":"mothkuri","upvote_count":"2","timestamp":"1725460380.0","content":"Selected Answer: A\nOptions A is the right answer. \nOption B using SSD persistent disk which will add more cost than default HDD\nOption C & D are out of scope."},{"poster":"barnac1es","upvote_count":"2","comment_id":"1015862","content":"Selected Answer: A\nDataproc Managed Service: Dataproc is a fully managed service for running Apache Hadoop and Spark. It provides ease of management and automation.\n\nStandard Persistent Disk: Using standard persistent disks for Dataproc workers ensures durability and is cost-effective compared to SSDs.\n\nPreemptible Workers: By using 50% preemptible workers, you can significantly reduce costs while maintaining fault tolerance. Preemptible VMs are cheaper but can be preempted by Google, so having a mix of preemptible and non-preemptible workers provides cost savings with redundancy.\n\nStoring Data in Cloud Storage: Storing data in Cloud Storage is highly durable, scalable, and cost-effective. It also makes data accessible to Dataproc clusters, and you can leverage native connectors for reading data from Cloud Storage.\n\nChanging References to gs://: Updating your scripts to reference data in Cloud Storage using gs:// ensures that your jobs work seamlessly with the cloud storage infrastructure.","timestamp":"1711296060.0"},{"timestamp":"1699622940.0","content":"Selected Answer: A\nApache Hadoop -> Dataproc or Compute Engine with proper SW installation\ncost-effective -> use standard persistent disk + store data in Cloud Storage\nbatch -> Dataproc or Compute Engine with proper SW installation\nmanaged service -> Dataproc","comment_id":"893862","upvote_count":"1","poster":"vaga1"},{"timestamp":"1685448060.0","content":"Selected Answer: A\nA is the answer.\n\nhttps://cloud.google.com/dataproc/docs/concepts/overview\nDataproc is a managed Spark and Hadoop service that lets you take advantage of open source data tools for batch processing, querying, streaming, and machine learning. Dataproc automation helps you create clusters quickly, manage them easily, and save money by turning clusters off when you don't need them. With less time and money spent on administration, you can focus on your jobs and your data.","comment_id":"731572","poster":"zellck","upvote_count":"2"},{"content":"Selected Answer: A\nit says cost effective , hence no SSD","poster":"MounicaN","upvote_count":"1","timestamp":"1678519980.0","comment_id":"665834"},{"timestamp":"1653445020.0","comment_id":"486430","poster":"JG123","upvote_count":"2","content":"Correct: A"},{"content":"Correct : A\nOption B is usefull if you use HDFS, and in this case as you use preemtible machines it isn't worth use SSD disks.","poster":"LORETOGOMEZ","timestamp":"1642425240.0","comment_id":"408401","upvote_count":"2"},{"content":"Answer - B","upvote_count":"1","comment_id":"292804","poster":"ArunSingh1028","timestamp":"1629220080.0"},{"comments":[{"timestamp":"1675742460.0","poster":"NM1212","upvote_count":"1","comment_id":"643562","content":"Caution about the link you provided as reference. It's intendedfor BigTable which is GC's low-latency solution which is totally different requirement. Mentioning only because on first read I thought SSD is the obvious choice.\nPer below link, SSD may not be required unless there is a low-latency requirement or a high I/O requirement. Since the question does not specify anything like that, A looks correct.\nhttps://cloud.google.com/solutions/migration/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc"}],"upvote_count":"5","comment_id":"266684","content":"Look at this link. https://cloud.google.com/bigtable/docs/choosing-ssd-hdd\nAt the First look I chose Option-B as they mentioned SSD is cost-effective on most cases. But after reading the whole page, they also mentioned that for batch workloads, HDD is suggested as long as not heavy read. So I changed my mind to Option-A (I assumed this is not ready heavy process?).","poster":"StelSen","timestamp":"1626223920.0"},{"content":"Option B - SSD disks, reasons:-\nThe question asks \"fault-tolerant and cost-effective as possible for long-running batch job\". \n3 Key words are - fault tolerant / cost effective / long running batch jobs..\n\nThe cost efficiency part mentioned in the question could be addressed by 50% preemptible disks and storing the data in cloud storage than HDFS. \nFor long running batch jobs and as standard approach for Dataproc - we should always go with SSD disk types as per google recommendations.","comments":[{"upvote_count":"2","comment_id":"238814","timestamp":"1623193800.0","comments":[{"upvote_count":"6","comment_id":"303270","timestamp":"1630742580.0","content":"https://cloud.google.com/solutions/migration/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc\nAs per this, SSD is only recommended if it is high IO intensive. In this question no where mentioned its high IO intensive, and asks for cost effective (as much as possible), so no need to use SSD. \nI will go with A.","poster":"Raangs"}],"poster":"beedle","content":"where is the proof...show me the link?"}],"poster":"Alasmindas","comment_id":"218915","timestamp":"1620958020.0","upvote_count":"4"},{"content":"Ans is B, for long running SDD suitable. HDD maintenance will be additional charge for long running jobs","upvote_count":"2","poster":"Ravivarma4786","timestamp":"1613713140.0","comment_id":"161216"},{"content":"Answer is A…Cloud Dataproc for Managed Cloud native application and HDD for cost-effective solution.","upvote_count":"7","comment_id":"128646","poster":"Rajuuu","timestamp":"1610005500.0"},{"content":"Answer A","poster":"Rajokkiyam","upvote_count":"5","timestamp":"1601697780.0","comment_id":"70613"}],"question_text":"You are planning to migrate your current on-premises Apache Hadoop deployment to the cloud. You need to ensure that the deployment is as fault-tolerant and cost-effective as possible for long-running batch jobs. You want to use a managed service. What should you do?","exam_id":11,"answer_images":[],"answer_description":"","isMC":true,"answers_community":["A (100%)"]},{"id":"lCjwc7pMcaoXIj9tyGVS","question_text":"Your team is working on a binary classification problem. You have trained a support vector machine (SVM) classifier with default parameters, and received an area under the Curve (AUC) of 0.87 on the validation set. You want to increase the AUC of the model. What should you do?","unix_timestamp":1584858780,"question_id":65,"choices":{"C":"Deploy the model and measure the real-world AUC; it's always higher because of generalization","A":"Perform hyperparameter tuning","B":"Train a classifier with deep neural networks, because neural networks would always beat SVMs","D":"Scale predictions you get out of the model (tune a scaling factor as a hyperparameter) in order to get the highest AUC"},"answer":"A","topic":"1","discussion":[{"timestamp":"1607566620.0","poster":"aadaisme","content":"Seems to be A. Preprocessing/scaling should be done with input features, instead of predictions (output)","comment_id":"106342","upvote_count":"42"},{"content":"A\nDeep LEarning is not always the best solution \nD talks about fudgin the output which is wrong","timestamp":"1613285640.0","poster":"FARR","comment_id":"157813","upvote_count":"11"},{"content":"Selected Answer: A\nhttps://www.quora.com/How-can-I-improve-Precision-Recall-AUC-under-Imbalanced-Classification","upvote_count":"3","poster":"MaxNRG","comment_id":"1100423","timestamp":"1718776380.0"},{"upvote_count":"2","comment_id":"893864","poster":"vaga1","content":"Selected Answer: A\nB,C are simply not true. D is modifing the scoring, making it not realiable anymore. A makes sense, is potentially increasing the model accuracy.","timestamp":"1699623360.0"},{"timestamp":"1699331400.0","upvote_count":"1","poster":"rishu2","comment_id":"891120","content":"Selected Answer: A\na is the correct answer"},{"timestamp":"1692355200.0","comment_id":"812944","poster":"musumusu","upvote_count":"1","content":"Answer A, \nwhy not B, Deep Neu Net. are better for sure but AUC is 0.87 is already good. Don't go for complex and time taking model. If AUC more than 0.95, it can be a reason of overfit. \nNow just check SVM params for hypertuning if you can bring it close to 0,9-0,95"},{"poster":"Kvk117","content":"Selected Answer: A\na is the correct answer","timestamp":"1690453560.0","upvote_count":"1","comment_id":"789584"},{"poster":"Dan137","content":"Also a good read is: https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview","upvote_count":"1","comment_id":"712769","timestamp":"1683420300.0"},{"content":"Selected Answer: A\nas mentioned by Spider7 \"performing tuning rather than using the model default parameters there's a way to increase the overall model performance --> A.\"","upvote_count":"2","poster":"medeis_jar","comment_id":"520139","timestamp":"1657357260.0"},{"upvote_count":"1","poster":"JG123","timestamp":"1653445140.0","content":"Correct: A","comment_id":"486433"},{"upvote_count":"3","timestamp":"1652378580.0","comments":[{"comment_id":"477145","upvote_count":"1","poster":"Spider7","timestamp":"1652378760.0","content":"0.87 precisely"}],"poster":"Spider7","content":"0.89 it's already not bad but by performing tuning rather then using the model default parameters there's a way to increase the overall model performance --> A.","comment_id":"477142"},{"timestamp":"1643365080.0","comment_id":"416078","upvote_count":"3","content":"Not C because real-world AUC value falls between 0.5 and 1.0 usually, this wouldn't help.\n\nA seems the most straigh forward.","poster":"hdmi_switch"},{"upvote_count":"1","comment_id":"316628","timestamp":"1632246060.0","content":"For a large enough training set DNN will most likely beat a SVM. However the opposite may or may not be true. It also depends on the complexity of the problem. Which we don’t know from the question. For image, nlp, I say B can be a good answer\nHowever, if we decide to stick with SVM, D reduces overfitting and may increase AUC.\nI am torn between the two!","poster":"Mitra123"},{"comment_id":"292812","poster":"ArunSingh1028","upvote_count":"1","timestamp":"1629220860.0","content":"Ans - D when the model is overfitted means want to increase the AUC, we always perform hyperparameter tuning, Increase regularisations, decrease input feature parameters etc."},{"comment_id":"202380","timestamp":"1618809480.0","upvote_count":"2","content":"AUC is scale-invariant. It measures how well predictions are ranked, rather than their absolute values. So answer shall be A\nhttps://developers.google.com/machine-learning/crash-course/classification/roc-and-auc?hl=en","poster":"nitinbhatia"},{"content":"Definitely not D\nhttps://developers.google.com/machine-learning/crash-course/classification/check-your-understanding-roc-and-auc","comment_id":"200885","timestamp":"1618549620.0","poster":"arghya13","upvote_count":"3"},{"comment_id":"161008","content":"A for me, read below link for more details.\n\nhttps://towardsdatascience.com/understanding-hyperparameters-and-its-optimisation-techniques-f0debba07568","timestamp":"1613671980.0","upvote_count":"4","poster":"saurabh1805"},{"timestamp":"1608564600.0","upvote_count":"2","content":"B: SVM is a single layer solution that tries to find a boundary between the two sets of data points. Deep neural networks can use many layers to define a more complex model to better fit the solution","poster":"Callumr","comment_id":"115601","comments":[{"timestamp":"1612099140.0","comment_id":"147985","content":"Neural Network can be better, but not always. That isn't true that more complex architecture/method are always better, especially we can't say that when we know nothing about dataset. Generally SVM is a quite good ml method for many problems. \n\nA is the correct answer. \nWhy B is a wrong one I've just described above. \nC is wrong because we can't assume that our model will be work better with new data and generally models don't work better. \nD doesn't make sense, it's not about preprocessing features (which is usefull), but only about a kind of postprocessing.","poster":"zonc","upvote_count":"3"}]},{"poster":"[Removed]","upvote_count":"1","comment_id":"68920","content":"Answer: D\nDescription: Preprocessing of data needs to be done to improve AUC","timestamp":"1601304180.0"},{"poster":"[Removed]","upvote_count":"1","comment_id":"66828","content":"Looks like D","timestamp":"1600749180.0"}],"timestamp":"2020-03-22 07:33:00","answer_description":"","question_images":[],"exam_id":11,"answers_community":["A (100%)"],"answer_ET":"A","answer_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/google/view/17212-exam-professional-data-engineer-topic-1-question-157/"}],"exam":{"isMCOnly":true,"isImplemented":true,"provider":"Google","isBeta":false,"name":"Professional Data Engineer","lastUpdated":"11 Apr 2025","id":11,"numberOfQuestions":319},"currentPage":13},"__N_SSP":true}