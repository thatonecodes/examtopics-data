{"pageProps":{"questions":[{"id":"HCHyToxx8AsLMwdjdcAT","answer_images":[],"answer_ET":"D","question_text":"You recently developed a deep learning model using Keras, and now you are experimenting with different training strategies. First, you trained the model using a single GPU, but the training process was too slow. Next, you distributed the training across 4 GPUs using tf.distribute.MirroredStrategy (with no other changes), but you did not observe a decrease in training time. What should you do?","question_images":[],"choices":{"B":"Create a custom training loop.","A":"Distribute the dataset with tf.distribute.Strategy.experimental_distribute_dataset","D":"Increase the batch size.","C":"Use a TPU with tf.distribute.TPUStrategy."},"isMC":true,"question_id":6,"answers_community":["D (63%)","A (32%)","3%"],"url":"https://www.examtopics.com/discussions/google/view/92019-exam-professional-machine-learning-engineer-topic-1-question/","timestamp":"2022-12-18 20:46:00","discussion":[{"poster":"egdiaa","upvote_count":"11","timestamp":"1672125060.0","content":"Selected Answer: D\nAns D: Check this link https://www.tensorflow.org/guide/gpu_performance_analysis for details on how to Optimize the performance on the multi-GPU single host","comment_id":"758229"},{"timestamp":"1740538080.0","comment_id":"1361681","content":"Selected Answer: D\nWhen using distributed training with tf.distribute.MirroredStrategy, each GPU processes a slice of the batch. If you keep the batch size constant, each GPU receives a smaller effective batch, which might not fully utilize the computational power of each device. Increasing the batch size allows each GPU to process more data in parallel, which can lead to improved training speed and better resource utilization without modifying your training loop or switching strategies","poster":"desertlotus1211","upvote_count":"1"},{"poster":"rajshiv","timestamp":"1733426580.0","upvote_count":"1","content":"Selected Answer: A\nI will go with A. By using tf.distribute.Strategy.experimental_distribute_dataset we can ensure that the dataset is effectively split across the GPUs, which will help fully utilize the GPUs and achieve faster training times. Increasing the batch size can improve training performance on GPUs by allowing them to process more data in parallel. However, if the dataset is not properly distributed across GPUs, simply increasing the batch size won't lead to improved training times. In fact, using a larger batch size can lead to memory bottlenecks if not handled correctly. The key here is to first ensure proper data distribution before tweaking batch size.","comment_id":"1322501"},{"timestamp":"1732639260.0","poster":"AB_C","content":"Selected Answer: A\nA is the right answer","comment_id":"1318190","upvote_count":"1"},{"upvote_count":"3","comment_id":"1195793","content":"Selected Answer: D\nwhen using tf.distribute.MirroredStrategy, TensorFlow automatically takes care of distributing the dataset across the available devices (GPUs in this case).\n\nTo make sure that the data is efficiently distributed across the GPUs, you should increase the global batch size. This ensures that each GPU receives a larger batch of data to process, effectively utilizing the additional computational power. The global batch size is the sum of the batch sizes for all devices. For example, if you had a batch size of 64 for a single GPU, you would set the global batch size to 256 (64 * 4) when using 4 GPUs.","timestamp":"1713154680.0","poster":"pinimichele01"},{"comments":[{"timestamp":"1699982280.0","poster":"pico","upvote_count":"1","content":"option D can be a reasonable step to try, but it's important to carefully monitor the training process, consider memory constraints, and assess the impact on model performance. It might be a good idea to try both option A (distributing the dataset) and option D (increasing the batch size) to see if there is any improvement in training time.","comment_id":"1070687"}],"comment_id":"1070686","poster":"pico","timestamp":"1699982220.0","upvote_count":"3","content":"Selected Answer: A\nWhen you distribute the training across multiple GPUs using tf.distribute.MirroredStrategy, the training time may not decrease if the dataset loading and preprocessing become a bottleneck. In this case, option A, distributing the dataset with tf.distribute.Strategy.experimental_distribute_dataset, can help improve the performance."},{"timestamp":"1690810380.0","poster":"PST21","comment_id":"968151","upvote_count":"1","content":"A. Distribute the dataset with tf.distribute.Strategy.experimental_distribute_dataset\nWhen you distribute the training across multiple GPUs using tf.distribute.MirroredStrategy, you need to make sure that the data is also distributed across the GPUs to fully utilize the computational power. By default, the tf.distribute.MirroredStrategy replicates the model and uses synchronous training, but it does not automatically distribute the dataset across the GPUs.","comments":[{"content":"You are right, However, when using tf.distribute.MirroredStrategy, TensorFlow automatically takes care of distributing the dataset across the available devices (GPUs in this case).\n\nTo make sure that the data is efficiently distributed across the GPUs, you should increase the global batch size. This ensures that each GPU receives a larger batch of data to process, effectively utilizing the additional computational power. The global batch size is the sum of the batch sizes for all devices. For example, if you had a batch size of 64 for a single GPU, you would set the global batch size to 256 (64 * 4) when using 4 GPUs.","comment_id":"1067025","timestamp":"1699601880.0","poster":"tavva_prudhvi","upvote_count":"1"}]},{"content":"Selected Answer: D\nWhen going from training with a single GPU to multiple GPUs on the same host, ideally you should experience the performance scaling with only the additional overhead of gradient communication and increased host thread utilization. Because of this overhead, you will not have an exact 2x speedup if you move from 1 to 2 GPUs.\n\nTry to maximize the batch size, which will lead to higher device utilization and amortize the costs of communication across multiple GPUs. Using the memory profiler helps get a sense of how close your program is to peak memory utilization. Note that while a higher batch size can affect convergence, this is usually outweighed by the performance benefits.","upvote_count":"2","poster":"CloudKida","timestamp":"1683616980.0","comment_id":"892884"},{"content":"Selected Answer: D\nWent with D","comment_id":"892794","poster":"M25","timestamp":"1683609900.0","upvote_count":"1"},{"comment_id":"852281","content":"Selected Answer: D\nIf distributing the training across multiple GPUs did not result in a decrease in training time, the issue may be related to the batch size being too small. When using multiple GPUs, each GPU gets a smaller portion of the batch size, which can lead to slower training times due to increased communication overhead. Therefore, increasing the batch size can help utilize the GPUs more efficiently and speed up training.","timestamp":"1679935140.0","poster":"tavva_prudhvi","upvote_count":"3"},{"upvote_count":"1","poster":"TNT87","content":"Selected Answer: D\nAnswer D","comment_id":"834319","timestamp":"1678390080.0"},{"timestamp":"1675839780.0","poster":"John_Pongthorn","comment_id":"801715","upvote_count":"4","content":"D: it is best https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_keras_modelfit\nEach epoch will then train faster as you add more GPUs. Typically, you would want to increase your batch size as you add more accelerators,\nC is rule out because of GPU\nA and B , as reading on https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_custom_training_loops To use custom loop , we have call If you are writing a custom training loop, you will need to call a few more methods, see the guide:\n\nStart by creating a tf.data.Dataset normally.\nUse tf.distribute.Strategy.experimental_distribute_dataset to convert a tf.data.Dataset to something that produces \"per-replica\" values. If you want to\nhttps://www.tensorflow.org/api_docs/python/tf/distribute/Strategy"},{"upvote_count":"1","timestamp":"1672925400.0","comment_id":"766662","content":"Selected Answer: D\nTo speed up the training of the deep learning model, increasing the batch size. When using multiple GPUs with tf.distribute.MirroredStrategy, increasing the batch size can help to better utilize the additional GPUs and potentially reduce the training time. This is because larger batch sizes allow each GPU to process more data in parallel, which can help to improve the efficiency of the training process.","poster":"zeic"},{"comment_id":"764793","poster":"ares81","upvote_count":"1","content":"Selected Answer: C\nTPUs are Google's specialized ASICs designed to dramatically accelerate machine learning workloads. Hence it should be C.","timestamp":"1672758960.0"},{"content":"Selected Answer: D\nI think it's D","upvote_count":"1","comment_id":"761609","poster":"Nayak8","timestamp":"1672366320.0"},{"poster":"MithunDesai","comment_id":"751787","upvote_count":"4","content":"Selected Answer: A\nI think its A","timestamp":"1671589320.0"},{"poster":"hiromi","comments":[{"timestamp":"1672225680.0","comment_id":"759738","upvote_count":"1","content":"Sorry, ans D (by ediaa link)","poster":"hiromi"},{"upvote_count":"1","poster":"hiromi","comment_id":"758516","content":"It's should A","timestamp":"1672146180.0"}],"content":"Selected Answer: B\nB (not sure)\n- https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch\n-https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_custom_training_loops","comment_id":"750777","upvote_count":"1","timestamp":"1671534180.0"},{"upvote_count":"3","timestamp":"1671392760.0","poster":"mil_spyro","content":"Selected Answer: A\nI think it's A, \n\nhttps://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#in_short","comment_id":"749127"}],"topic":"1","answer_description":"","answer":"D","unix_timestamp":1671392760,"exam_id":13},{"id":"VwOnPmrZyVKIDTjrMhIm","isMC":true,"question_images":[],"question_text":"You work for a gaming company that has millions of customers around the world. All games offer a chat feature that allows players to communicate with each other in real time. Messages can be typed in more than 20 languages and are translated in real time using the Cloud Translation API. You have been asked to build an ML system to moderate the chat in real time while assuring that the performance is uniform across the various languages and without changing the serving infrastructure.\n\nYou trained your first model using an in-house word2vec model for embedding the chat messages translated by the Cloud Translation API. However, the model has significant differences in performance across the different languages. How should you improve it?","choices":{"B":"Train a classifier using the chat messages in their original language.","D":"Remove moderation for languages for which the false positive rate is too high.","C":"Replace the in-house word2vec with GPT-3 or T5.","A":"Add a regularization term such as the Min-Diff algorithm to the loss function."},"answer_description":"","answer":"B","answers_community":["B (57%)","A (29%)","14%"],"unix_timestamp":1671898980,"exam_id":13,"question_id":7,"discussion":[{"comment_id":"834318","timestamp":"1694280480.0","upvote_count":"9","content":"Selected Answer: B\nAnswer B\nSince the performance of the model varies significantly across different languages, it suggests that the translation process might have introduced some noise in the chat messages, making it difficult for the model to generalize across languages. One way to address this issue is to train a classifier using the chat messages in their original language.","poster":"TNT87"},{"poster":"desertlotus1211","content":"Selected Answer: C\nThe issue is with the language translation - GPT-3 or T5 are trained on large multilingual datasets and are designed to capture the nuances of multiple languages. By replacing your in-house word2vec model with one of these state-of-the-art models, you can leverage their robust, context-aware embeddings to achieve more uniform performance across various languages.","upvote_count":"1","timestamp":"1740538320.0","comment_id":"1361684"},{"timestamp":"1721825640.0","content":"Selected Answer: A\nuniform performance","comments":[{"upvote_count":"1","poster":"pinimichele01","timestamp":"1728966000.0","content":"Adding a regularization term to the loss function can help prevent overfitting of the model, but it may not necessarily address the language-specific differences in performance. The Min-Diff algorithm is a type of regularization technique that aims to minimize the difference between the model predictions and the ground truth while ensuring that the model remains simple. While this can improve the generalization performance of the model, it may not be sufficient to address the language-specific differences in performance. Therefore, training a classifier using the chat messages in their original language can be a better solution to improve the performance of the moderation system across different languages.","comment_id":"1195794"}],"upvote_count":"1","poster":"Zwi3b3l","comment_id":"1130736"},{"content":"Selected Answer: B\nMin-diff may reduce model unfairness, but here the concern is about improving performance. Training models avoiding Cloud Natural API should be more suitable.","upvote_count":"2","timestamp":"1706336700.0","poster":"ciro_li","comment_id":"964325","comments":[{"content":"Adding a regularization term to the loss function can help prevent overfitting of the model, but it may not necessarily address the language-specific differences in performance. The Min-Diff algorithm is a type of regularization technique that aims to minimize the difference between the model predictions and the ground truth while ensuring that the model remains simple. While this can improve the generalization performance of the model, it may not be sufficient to address the language-specific differences in performance. Therefore, training a classifier using the chat messages in their original language can be a better solution to improve the performance of the moderation system across different languages.","poster":"tavva_prudhvi","comment_id":"968391","upvote_count":"1","timestamp":"1706732340.0"}]},{"poster":"[Removed]","upvote_count":"3","timestamp":"1705839540.0","content":"Selected Answer: A\nA is correct since it encourages the model to have similar performance across languages. \n\nB would entail training 20 word2vec embeddings + maintaining 20 models at the same time. On top of that, there would be no guarantee that those models will have comparable performance across languages. This is certainly not something you would do after training your first model.","comment_id":"958316"},{"poster":"friedi","upvote_count":"2","comment_id":"930797","content":"Selected Answer: A\nA is correct, the key part of the question is „[…] assuring the performance is uniform […]“ which is baked into the Min-Diff regularisation: https://ai.googleblog.com/2020/11/mitigating-unfair-bias-in-ml-models.html","timestamp":"1703270640.0"},{"comment_id":"892795","timestamp":"1699514760.0","poster":"M25","content":"Selected Answer: B\nWent with B","upvote_count":"1"},{"timestamp":"1695833280.0","upvote_count":"4","comment_id":"852287","content":"Selected Answer: B\nSince the current model has significant differences in performance across the different languages, it is likely that the translations produced by the Cloud Translation API are not of uniform quality across all languages. Therefore, it would be best to train a classifier using the chat messages in their original language instead of relying on translations.\n\nThis approach has several advantages. First, the model can directly learn the nuances of each language, leading to better performance across all languages. Second, it eliminates the need for translation, reducing the possibility of errors and improving the overall speed of the system. Finally, it is a relatively simple approach that can be implemented without changing the serving infrastructure.","poster":"tavva_prudhvi"},{"content":"Selected Answer: A\nshould be A\nhttps://ai.googleblog.com/2020/11/mitigating-unfair-bias-in-ml-models.html","timestamp":"1694114880.0","poster":"hakook","comment_id":"832309","upvote_count":"2"},{"timestamp":"1693778160.0","upvote_count":"3","comment_id":"828472","poster":"Ml06","content":"B i think is the correct answer \nC is an overkill , you have just developed your first model you don’t jump into solution like C , in addition the problem is that there is a significant difference between language note the model is enormously underperforming . \nFinally you are serving millions of users , running chat GPT or T5 for a task like chat moderation (and in real time) is extremely wasteful ."},{"content":"Given that GPT-3 is rival of google , C is not possible certainly .","upvote_count":"3","comments":[{"comment_id":"801747","poster":"John_Pongthorn","content":"we are taking into account 20 muti classification, it is relevant about FP or FN.","timestamp":"1691473200.0","upvote_count":"1"}],"comment_id":"801731","timestamp":"1691472420.0","poster":"John_Pongthorn"},{"poster":"egdiaa","content":"Selected Answer: C\nGPT-3 is best for generating human-like Text","comments":[{"comment_id":"820395","comments":[{"comment_id":"1361686","poster":"desertlotus1211","timestamp":"1740538440.0","upvote_count":"1","content":"yes, what else would you generate when you need to communicate over a messaging system?"}],"poster":"lightnessofbein","timestamp":"1692868140.0","upvote_count":"2","content":"Does \"moderate\" means we need to generate text?"}],"timestamp":"1687843200.0","upvote_count":"3","comment_id":"758235"},{"poster":"kunal_18","comment_id":"755006","timestamp":"1687616580.0","content":"Ans : C\nhttps://towardsdatascience.com/poor-mans-gpt-3-few-shot-text-generation-with-t5-transformer-51f1b01f843e","upvote_count":"1"}],"timestamp":"2022-12-24 17:23:00","topic":"1","url":"https://www.examtopics.com/discussions/google/view/92657-exam-professional-machine-learning-engineer-topic-1-question/","answer_ET":"B","answer_images":[]},{"id":"NuF2IjSBvewZcGaPUEBF","answers_community":["A (61%)","D (25%)","11%"],"url":"https://www.examtopics.com/discussions/google/view/92224-exam-professional-machine-learning-engineer-topic-1-question/","question_id":8,"answer":"A","discussion":[{"timestamp":"1687287420.0","comment_id":"751464","poster":"hiromi","content":"Selected Answer: A\nit seens A (not sure)\n- https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-tensorflow","upvote_count":"12"},{"comment_id":"1362748","upvote_count":"1","poster":"bc3f222","content":"Selected Answer: A\nThe hint is \"You built a TensorFlow model that predicts whether players will make in-app purchases of more than $10 in the next two weeks.\" this means that for this particular use case prediction is not realtime and batch is in fact suitable. Furthermore, BQML allows you to load tensorflow model for serving. This makes BQML the best choice for cost consideration.","timestamp":"1740695040.0"},{"poster":"desertlotus1211","content":"Selected Answer: C\nit's an online gaming service- you in to stream data in realtime, not batch processing","comments":[{"poster":"desertlotus1211","timestamp":"1740538740.0","upvote_count":"1","content":"My mistake - I meant to click D.","comment_id":"1361689"}],"upvote_count":"1","comment_id":"1361688","timestamp":"1740538740.0"},{"upvote_count":"1","comment_id":"1359076","timestamp":"1740022200.0","poster":"NamitSehgal","content":"Selected Answer: B\nBigQuery ML is a useful tool for certain machine learning tasks, it's not the right tool for serving a complex TensorFlow model and integrating it into a game's user experience adaptation system. Vertex AI Prediction is a better choice for this scenario due to its superior support for serving complex models, its optimized infrastructure for serving, and its ease of management."},{"comment_id":"1329415","poster":"phani49","content":"Selected Answer: D\nWhy D is the Best Choice:\n\nIt provides real-time predictions, which is crucial for a good user experience in an MMO setting.\nIt leverages Google Cloud’s managed services (Dataflow, Pub/Sub, Cloud SQL) to reduce operational overhead and simplify management.\nIt allows you to centrally manage your model and easily update it without requiring changes to client applications.\nIt optimizes cost by using a pay-as-you-go, autoscaling service rather than running large-scale batch jobs or deploying models on individual user devices.\n\nOption A: Import model into BigQuery ML and do batch predictions.\nUser Experience: Batch predictions are not real-time. This approach introduces a significant delay between data ingestion and predictions. Not ideal if you need to adapt the user experience quickly based on recent behavior.","upvote_count":"1","timestamp":"1734693060.0"},{"poster":"pinimichele01","timestamp":"1728966240.0","content":"Selected Answer: A\nMake predictions after every in-app purchase it it not necessary -> A","upvote_count":"2","comment_id":"1195796"},{"comment_id":"1071113","poster":"Mickey321","upvote_count":"1","content":"Selected Answer: D\nEmbedding the model in a streaming Dataflow pipeline allows low latency predictions on real-time events published to Pub/Sub. This provides a responsive user experience.\nDataflow provides a managed service to scale predictions and integrate with Pub/Sub, without having to manage servers.\nStreaming predictions only when events occur optimizes cost compared to bulk or client-side prediction.\nPushing results to Cloud SQL provides a managed database for persistence.\nIn contrast, options A and B use inefficient batch predictions. Option C increases mobile app size and cost.","timestamp":"1715743560.0"},{"content":"Selected Answer: D\nD could be correct","upvote_count":"1","timestamp":"1704697440.0","comment_id":"946180","poster":"SamuelTsch"},{"timestamp":"1704395880.0","upvote_count":"1","comments":[{"content":"Why do you want to make a prediction after every app purchase bro?","poster":"tavva_prudhvi","upvote_count":"3","comment_id":"1067029","timestamp":"1715319960.0"}],"content":"Selected Answer: D\nThese were my reasonings to choose D as best option:\nB -> Vertex AI would not minimize cost\nC -> Would not optimize user experience (this may lead to slow running of the game (lag)?)\nA- > Would not optimize ease of management / automatization\nD -> Best choice?","comment_id":"943028","poster":"Nxtgen"},{"comment_id":"892279","upvote_count":"3","content":"Selected Answer: D\nFor \"used to adapt each user's game experience\" points out to non-batch, hence excludes A & B, and embedding the model in the mobile app would not necessarily \"optimize cost\". Plus, the classical streaming solution builds on Dataflow along with Pub/Sub and BigQuery, embedding ML in Dataflow is low-code https://cloud.google.com/blog/products/data-analytics/latest-dataflow-innovations-for-real-time-streaming-and-aiml and apparently a modified version of the question points to the same direction https://mikaelahonen.com/en/data/gcp-mle-exam-questions/","timestamp":"1699463220.0","comments":[{"upvote_count":"3","comment_id":"964329","timestamp":"1706337300.0","poster":"ciro_li","content":"there's no need to make a prediction after every in-app purchase event. Am i wrong?"}],"poster":"M25"},{"content":"Selected Answer: A\nYeah its A","upvote_count":"2","comment_id":"872355","poster":"TNT87","timestamp":"1697521920.0"},{"poster":"TNT87","comment_id":"834316","upvote_count":"2","content":"Selected Answer: C\nAnswer C","comments":[{"comment_id":"852294","poster":"tavva_prudhvi","timestamp":"1695833700.0","upvote_count":"2","content":"Option C, embedding the model in the mobile application, can increase the size of the application and may not be suitable for real-time prediction."}],"timestamp":"1694280300.0"}],"unix_timestamp":1671569820,"answer_images":[],"topic":"1","exam_id":13,"question_images":[],"choices":{"A":"Import the model into BigQuery ML. Make predictions using batch reading data from BigQuery, and push the data to Cloud SQL","C":"Embed the model in the mobile application. Make predictions after every in-app purchase event is published in Pub/Sub, and push the data to Cloud SQL.","D":"Embed the model in the streaming Dataflow pipeline. Make predictions after every in-app purchase event is published in Pub/Sub, and push the data to Cloud SQL.","B":"Deploy the model to Vertex AI Prediction. Make predictions using batch reading data from Cloud Bigtable, and push the data to Cloud SQL."},"answer_description":"","answer_ET":"A","isMC":true,"timestamp":"2022-12-20 21:57:00","question_text":"You work for a gaming company that develops massively multiplayer online (MMO) games. You built a TensorFlow model that predicts whether players will make in-app purchases of more than $10 in the next two weeks. The model’s predictions will be used to adapt each user’s game experience. User data is stored in BigQuery. How should you serve your model while optimizing cost, user experience, and ease of management?"},{"id":"GQFQkxGggwIJI2BrnVP5","timestamp":"2022-12-20 22:03:00","question_id":9,"topic":"1","url":"https://www.examtopics.com/discussions/google/view/92226-exam-professional-machine-learning-engineer-topic-1-question/","answer_images":[],"answer_description":"","exam_id":13,"choices":{"D":"Use Dataprep to transform the state column using a one-hot encoding method, and make each city a column with binary values.","B":"Create a new view with BigQuery that does not include a column with city information","A":"Use TensorFlow to create a categorical variable with a vocabulary list. Create the vocabulary file, and upload it as part of your model to BigQuery ML.","C":"Use Cloud Data Fusion to assign each city to a region labeled as 1, 2, 3, 4, or 5, and then use that number to represent the city in the model."},"answers_community":["D (83%)","C (17%)"],"answer_ET":"D","question_images":[],"unix_timestamp":1671570180,"isMC":true,"discussion":[{"comment_id":"1201804","content":"Selected Answer: D\nA. Using TensorFlow: This is an overkill for this scenario. BigQuery ML can handle one-hot encoding natively within Dataprep.\nB. Excluding City Information: This removes a potentially important predictive variable, reducing model accuracy.\nC. Assigning Region Labels: This approach loses granularity and might not capture the specific variations between cities.","poster":"fitri001","upvote_count":"3","timestamp":"1729840260.0"},{"poster":"andresvelasco","comment_id":"1003892","content":"Selected Answer: D\nD by elimination but ...\nDoes not bigquery automatically do one-hot encoding of categorical features for you?\nAlso the wording of the question does not seem right: a linear regression model to predict the likelihodd that the customer ... isnt that a classification model?","timestamp":"1710071760.0","upvote_count":"1"},{"poster":"M25","timestamp":"1699514820.0","content":"Selected Answer: D\nWent with D","comment_id":"892797","upvote_count":"1"},{"comment_id":"842584","upvote_count":"1","content":"Is it correct to say that A is technically a better way to do things if the ask wast for separate columns?","comments":[{"poster":"tavva_prudhvi","content":"\"least amount of coding\"","comment_id":"852298","timestamp":"1695833820.0","upvote_count":"4"}],"timestamp":"1695018900.0","poster":"Yajnas_arpohc"},{"comment_id":"838903","content":"Selected Answer: D\nOne-hot is a good way to use categorical variables in regressions problems\nhttps://academic.oup.com/rheumatology/article/54/7/1141/1849688\nhttps://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-auto-preprocessing","timestamp":"1694692200.0","poster":"guilhermebutzke","upvote_count":"3"},{"timestamp":"1694280180.0","poster":"TNT87","content":"Selected Answer: D\nAnswer D","upvote_count":"1","comment_id":"834313"},{"timestamp":"1692189240.0","comment_id":"810757","content":"Selected Answer: C\nfor a fuller answer, D--> transforms “state” column not city column\nC--> at least works with city column","comments":[{"content":"Read smarques comment","comment_id":"852299","poster":"tavva_prudhvi","upvote_count":"1","timestamp":"1695833880.0"}],"poster":"abneural","upvote_count":"1"},{"poster":"John_Pongthorn","upvote_count":"2","comment_id":"802102","content":"Selected Answer: D\nhttps://docs.trifacta.com/display/SS/Prepare+Data+for+Machine+Processing","timestamp":"1691496660.0"},{"timestamp":"1689678060.0","upvote_count":"1","poster":"smarques","comment_id":"780015","content":"Selected Answer: D\nThis will allow you to maintain the city name variable as a predictor while ensuring that the data is in a format that can be used to train a linear regression model on BigQuery ML."},{"timestamp":"1688082660.0","poster":"Abhijat","content":"Selected Answer: D\nAnswer D","comment_id":"761598","upvote_count":"1"},{"comment_id":"761326","upvote_count":"1","content":"Answer is D","poster":"Abhijat","timestamp":"1688053860.0"},{"timestamp":"1687331220.0","upvote_count":"2","poster":"mymy9418","content":"Selected Answer: D\none-hot encoding makes sense to me","comment_id":"752041"},{"content":"Selected Answer: C\nI vote for C","comments":[{"comment_id":"758558","upvote_count":"3","timestamp":"1687865760.0","content":"Changing my vote to D","poster":"hiromi"}],"poster":"hiromi","upvote_count":"2","timestamp":"1687287780.0","comment_id":"751471"}],"question_text":"You are building a linear regression model on BigQuery ML to predict a customer’s likelihood of purchasing your company’s products. Your model uses a city name variable as a key predictive component. In order to train and serve the model, your data must be organized in columns. You want to prepare your data using the least amount of coding while maintaining the predictable variables. What should you do?","answer":"D"},{"id":"yKe6hvMGbsB38qkulEAz","answers_community":["B (90%)","10%"],"timestamp":"2022-12-13 21:52:00","url":"https://www.examtopics.com/discussions/google/view/91498-exam-professional-machine-learning-engineer-topic-1-question/","exam_id":13,"question_id":10,"question_text":"You are an ML engineer at a bank that has a mobile application. Management has asked you to build an ML-based biometric authentication for the app that verifies a customer’s identity based on their fingerprint. Fingerprints are considered highly sensitive personal information and cannot be downloaded and stored into the bank databases. Which learning strategy should you recommend to train and deploy this ML mode?","topic":"1","answer_description":"","isMC":true,"discussion":[{"comment_id":"751479","poster":"hiromi","content":"Selected Answer: B\nB\nWith federated learning, all the data is collected, and the model is trained with algorithms across multiple decentralized edge devices such as cell phones or websites, without exchanging them.\n(Journey to Become a Google Cloud Machine Learning Engineer: Build the mind and hand of a Google Certified ML professional)","timestamp":"1687288080.0","upvote_count":"10"},{"poster":"fitri001","comment_id":"1201805","comments":[{"poster":"fitri001","timestamp":"1729840380.0","upvote_count":"1","comment_id":"1201806","content":"Data Loss Prevention API (DLAPI): This focuses on protecting data at rest and in transit, not relevant to training a model without storing data.\nMD5 Encryption: This is a one-way hashing function, not suitable for encryption and decryption needed for training.expand_more\nDifferential privacy: While it adds noise to protect privacy, it's not ideal for training image recognition models like fingerprint identification."}],"timestamp":"1729840380.0","upvote_count":"1","content":"Selected Answer: B\nFederated learning allows training the model on the user's devices themselves.\n\npen_spark\nexpand_more The model updates its parameters based on local training data on the device without ever needing the raw fingerprint information to leave the device. This ensures the highest level of privacy for sensitive biometric data."},{"poster":"Voyager2","comment_id":"915431","upvote_count":"2","content":"B. Federated learning. \n\"information and cannot be downloaded and stored into the bank databases\" That excludes DLP. ederated Learning enables mobile phones to collaboratively learn a shared prediction model while keeping all the training data on device, decoupling the ability to do machine learning from the need to store the data in the cloud.","timestamp":"1701785760.0"},{"content":"Selected Answer: B\nWent with B","timestamp":"1699514820.0","comment_id":"892798","poster":"M25","upvote_count":"1"},{"content":"Selected Answer: B\nI think the giveaway is in the question \"Which learning strategy..\"... Federated Learning seems to be the only one !","upvote_count":"3","timestamp":"1695268800.0","comment_id":"845564","poster":"Yajnas_arpohc"},{"timestamp":"1694084280.0","comment_id":"831911","upvote_count":"2","content":"Selected Answer: B\nB. Federated learning would be the best learning strategy to train and deploy the ML model for biometric authentication in this scenario. Federated learning allows for training an ML model on distributed data without transferring the raw data to a centralized location.","poster":"TNT87"},{"content":"Selected Answer: A\nAns is A for me","poster":"zzzzzooooo","upvote_count":"1","comment_id":"817897","timestamp":"1692705900.0"},{"comment_id":"768609","content":"Selected Answer: A\nIt seems A, to me.","timestamp":"1688730960.0","poster":"ares81","upvote_count":"1"},{"comment_id":"744457","upvote_count":"1","poster":"mil_spyro","content":"Selected Answer: B\nFederated Learning enables mobile phones to collaboratively learn a shared prediction model while keeping all the training data on device.\nhttps://ai.googleblog.com/2017/04/federated-learning-collaborative.html","timestamp":"1686682320.0"}],"answer_ET":"B","answer":"B","choices":{"C":"MD5 to encrypt data","A":"Data Loss Prevention API","D":"Differential privacy","B":"Federated learning"},"answer_images":[],"question_images":[],"unix_timestamp":1670964720}],"exam":{"provider":"Google","name":"Professional Machine Learning Engineer","isMCOnly":true,"isImplemented":true,"numberOfQuestions":304,"isBeta":false,"lastUpdated":"11 Apr 2025","id":13},"currentPage":2},"__N_SSP":true}