{"pageProps":{"questions":[{"id":"qRAIN1hm0w6RywwN31W5","discussion":[{"comment_id":"1239657","poster":"VinaoSilva","timestamp":"1719753720.0","content":"Selected Answer: B\nminimal development work + regression model = BigQuery ML","upvote_count":"3"},{"content":"B. Develop a regression model using BigQuery ML.\n\n\nYou're looking for a solution that scales smoothly and requires minimal development work. BigQuery ML is an excellent choice because it allows you to create machine learning models directly in BigQuery, without the need to write code or set up complex infrastructure.","comment_id":"1234746","upvote_count":"1","timestamp":"1718992620.0","poster":"AzureDP900"},{"timestamp":"1713761100.0","poster":"fitri001","content":"Selected Answer: B\nScalability: BigQuery is a serverless data warehouse designed to handle massive datasets. It can efficiently process tens of millions of records daily for model training.\nMinimal Development Work: BigQuery ML offers built-in regression models like linear regression that you can train directly on your data stored in BigQuery. This eliminates the need for extensive custom code development with TensorFlow, PyTorch, or scikit-learn (options A, C, and D).\nDaily Training Runs:\n BigQuery ML allows scheduling queries for automated model training. You can set up a daily scheduled query to train your model on the latest data.","comment_id":"1199952","upvote_count":"3"},{"upvote_count":"3","timestamp":"1712498940.0","poster":"7cb0ab3","comment_id":"1191003","content":"Selected Answer: B\nMinimal development effort can be achieved with BigQuery ML. Also the amount of data is already in BQ."},{"poster":"pinimichele01","content":"Selected Answer: B\nMinimal dev effort => BigQueryML","comment_id":"1190904","upvote_count":"1","timestamp":"1712486580.0"},{"content":"Selected Answer: C\nI went C.","timestamp":"1709120820.0","poster":"Carlose2108","upvote_count":"1","comment_id":"1161578"},{"comment_id":"968284","content":"Selected Answer: B\nMinimal development effort => BigQueryML","upvote_count":"3","poster":"Mdso","timestamp":"1690820580.0"},{"timestamp":"1689862080.0","upvote_count":"1","content":"Selected Answer: B\nfor scheduling daily training runs with minimal development work and seamless scaling, the best option is to develop a regression model using BigQuery ML (Option B). It allows you to perform model training and inference directly within BigQuery, taking advantage of its distributed processing capabilities to handle large datasets effortlessly.","comment_id":"957675","poster":"PST21"}],"answer_description":"","timestamp":"2023-07-20 16:08:00","unix_timestamp":1689862080,"url":"https://www.examtopics.com/discussions/google/view/115868-exam-professional-machine-learning-engineer-topic-1-question/","answer":"B","answer_images":[],"topic":"1","exam_id":13,"question_text":"You are a data scientist at an industrial equipment manufacturing company. You are developing a regression model to estimate the power consumption in the company’s manufacturing plants based on sensor data collected from all of the plants. The sensors collect tens of millions of records every day. You need to schedule daily training runs for your model that use all the data collected up to the current date. You want your model to scale smoothly and require minimal development work. What should you do?","question_id":66,"answer_ET":"B","isMC":true,"answers_community":["B (93%)","7%"],"question_images":[],"choices":{"B":"Develop a regression model using BigQuery ML.","C":"Develop a custom scikit-learn regression model, and optimize it using Vertex AI Training.","A":"Develop a custom TensorFlow regression model, and optimize it using Vertex AI Training.","D":"Develop a custom PyTorch regression model, and optimize it using Vertex AI Training."}},{"id":"MrKGPl91DEKWjXKgXwpw","answer_description":"","question_images":[],"discussion":[{"poster":"NamitSehgal","content":"Selected Answer: A\nBy adding synthetic training data that includes benign references to the underrepresented religious groups, you can help the model better understand the context in which these phrases are used and reduce the false positive rate. T","timestamp":"1739842140.0","comment_id":"1358074","upvote_count":"2"},{"timestamp":"1738828140.0","comment_id":"1352245","poster":"vini123","upvote_count":"2","content":"Selected Answer: A\nOption A actively teaches the model to differentiate toxic from non-toxic uses of religious references, reducing bias while maintaining strong moderation."},{"content":"Selected Answer: A\nMain issue is to address the bias in the model","upvote_count":"1","comment_id":"1325588","poster":"Omi_04040","timestamp":"1734004200.0"},{"comment_id":"1325213","poster":"Laur_C","content":"Selected Answer: A\nI chose A - least expensive/time consuming way to actually solve the problem. D sacrifices model quality and does not change the inherent bias of the model, meaning that the biases would still remain if this solution was chosen. Does not seem like the ethical/best practices solution","upvote_count":"2","timestamp":"1733947680.0"},{"timestamp":"1733248080.0","upvote_count":"2","comment_id":"1321476","poster":"rajshiv","content":"Selected Answer: A\nD is not a good answer. Raising the threshold would reduce the number of toxic comments flagged (perhaps lowering false positives), but it would also increase the number of actual toxic comments being missed (higher false negatives). This exacerbates the problem and do not address the bias in the model. I think A is the best answer."},{"content":"Selected Answer: D\nThe answer is D. Of course A would be ideal, but it totally ignores the constraints presented in the question that your team is already overextended.","poster":"f084277","upvote_count":"2","timestamp":"1731551820.0","comment_id":"1311690"},{"poster":"Dirtie_Sinkie","timestamp":"1726566480.0","upvote_count":"1","content":"Selected Answer: A\nGonna go with A on this one. Some toxic comments will still make it through if you choose D, whereas A addresses the problem fully and directly. Therefore I think A is a more complete answer than D.","comments":[{"content":"Even though in the question it says \"Your team has a limited budget and is already overextended\" I still think A is the better answer because it doesn't take much effort to create synthetic data and add it to train. The outcome will be more accurate than D.","upvote_count":"2","timestamp":"1726566840.0","poster":"Dirtie_Sinkie","comments":[{"content":"Of course A is \"better\", but it ignores the constraints of the question and is therefore wrong.","upvote_count":"1","poster":"f084277","comment_id":"1311689","timestamp":"1731551760.0"}],"comment_id":"1285128"}],"comment_id":"1285125"},{"comment_id":"1281602","timestamp":"1725978120.0","content":"Selected Answer: A\nA is better than D, because D means that more geniunely toxic comments will make it through. A will teach the model to acknowledge the small subset of mislabelled comments, without exposing the customers to additional toxicity.","upvote_count":"1","poster":"baimus"},{"content":"option A (Add synthetic training data where those phrases are used in non-toxic ways) directly addresses the specific issue of bias and improves the model's accuracy by providing more contextually relevant training examples. This approach is more targeted and has a lower risk of introducing new biases or negatively impacting other aspects of comment moderation.\n\n\nI hope this additional explanation helps clarify why option D might not be the best choice in this scenario!","comment_id":"1234750","timestamp":"1718992980.0","poster":"AzureDP900","comments":[{"upvote_count":"1","comment_id":"1234752","poster":"AzureDP900","timestamp":"1718993040.0","content":"Raising the threshold would mean increasing the minimum score required for a comment to be classified as toxic or harmful. This could potentially reduce the number of false positives (benign comments being misclassified as toxic) by making it harder for the model to classify a comment as toxic."}],"upvote_count":"2"},{"timestamp":"1716826200.0","content":"A option directly addresses the bias issue without incurring significant ongoing costs or burdening the moderation team. By augmenting the training dataset with synthetic examples where phrases related to underrepresented religious groups are used in non-toxic ways, the classifier can learn to distinguish between toxic and benign comments more accurately.","upvote_count":"2","comment_id":"1219672","poster":"Simple_shreedhar"},{"comment_id":"1199575","content":"Selected Answer: D\nagree with daidai75","upvote_count":"1","timestamp":"1713691980.0","poster":"gscharly"},{"timestamp":"1713003660.0","comment_id":"1194862","content":"Selected Answer: D\nYour team has a limited budget and is already overextended","poster":"pinimichele01","upvote_count":"2"},{"poster":"7cb0ab3","comment_id":"1191009","timestamp":"1712499480.0","upvote_count":"2","content":"Selected Answer: A\nI went fo A because it directly tackels the issue of misclassification and improving the models unterstanding of religious references. B and C don't make sense.\nD would generally reduce the number of comments flagged as toxic, which could decrease the false positive rate. However, this approach risks allowing genuinely harmful comments to go unflagged. It addresses the symptom (high false positive rate) rather than the underlying cause"},{"timestamp":"1709752740.0","comment_id":"1167431","content":"Selected Answer: A\nB and C are non sense, I don't want to risk potentially increasing the FNR by reducing the FPR (Raise the threshold). Thus A.","poster":"edoo","upvote_count":"1"},{"upvote_count":"2","poster":"daidai75","content":"Selected Answer: D\nYour team has a limited budget and is already overextended, that means the re-training is hardly possible.","timestamp":"1706775300.0","comment_id":"1137388"},{"upvote_count":"1","content":"In the long run, usually we go with A, but Option D could be a temporary solution to reduce false positives, while being aware that it may allow some genuinely toxic comments to go unnoticed. However, this may be a necessary trade-off until your team has the resources to improve the classifier or find a better solution.","poster":"tavva_prudhvi","comment_id":"963992","timestamp":"1690389960.0"},{"upvote_count":"2","timestamp":"1690286760.0","poster":"powerby35","comment_id":"962678","content":"Selected Answer: D\n\"Your team has a limited budget and is already overextended\""},{"upvote_count":"2","timestamp":"1690091100.0","poster":"[Removed]","comment_id":"960116","content":"Selected Answer: D\nBy raising the threshold for comments to be considered toxic or harmful, you will decrease the number of false positives.\n\nB is wrong because we are taking a Google MLE exam :) A and C are wrong because both of them involve a good amount of additional work, either for extending the dataset or training/experimenting with a new model. Considering your team is already over the budget and has too many tasks on their plate (overextended), these two options are not available for you.","comments":[{"poster":"tavva_prudhvi","timestamp":"1700069340.0","comment_id":"1071712","content":"But, by raising the threshold, we might be allowing some genuinely toxic comments to pass through without being flagged. This could potentially lead to an increase in the false negative rate, right?","upvote_count":"1"}]},{"upvote_count":"1","poster":"PST21","content":"Selected Answer: A\nA. Add synthetic training data where those phrases are used in non-toxic ways.\n\nIn this situation, where your automated text classifier is misclassifying benign comments referencing certain underrepresented religious groups as toxic or harmful, adding synthetic training data where those phrases are used in non-toxic ways can be a cost-effective solution to improve the model's performance.","comment_id":"957677","timestamp":"1689862200.0"}],"answer":"A","choices":{"D":"Raise the threshold for comments to be considered toxic or harmful.","B":"Remove the model and replace it with human moderation.","C":"Replace your model with a different text classifier.","A":"Add synthetic training data where those phrases are used in non-toxic ways."},"answer_images":[],"unix_timestamp":1689862200,"url":"https://www.examtopics.com/discussions/google/view/115869-exam-professional-machine-learning-engineer-topic-1-question/","topic":"1","question_text":"Your organization manages an online message board. A few months ago, you discovered an increase in toxic language and bullying on the message board. You deployed an automated text classifier that flags certain comments as toxic or harmful. Now some users are reporting that benign comments referencing their religion are being misclassified as abusive. Upon further inspection, you find that your classifier's false positive rate is higher for comments that reference certain underrepresented religious groups. Your team has a limited budget and is already overextended. What should you do?","question_id":67,"answer_ET":"A","timestamp":"2023-07-20 16:10:00","answers_community":["A (58%)","D (42%)"],"exam_id":13,"isMC":true},{"id":"hb5S9wBP7zZOQIyemsRz","discussion":[{"content":"The answer is C. Use RNN because it is a time series analysis.","timestamp":"1622886840.0","poster":"esuaaaa","comment_id":"374980","upvote_count":"29"},{"content":"As Y2Data pointed out, your reasoning for choosing B does not make much sense. \n\nFurthermore, Reinforcement Learning for this question does not make much sense to me. Reinforcement Learning is basically agent - task problems. You give the agent a task i.e. get out of a maze and then through trial and error and many many iterations the agent learns the correct way to perform the task. It is called Reinforcement because you ... well ... reinforce the agent, you reward the agent for correct choices and penalize for incorrect choices. In RL you dont use many / any previous data because the data is generated with each iteration I think.","upvote_count":"7","comment_id":"459634","poster":"george_ognyanov","timestamp":"1633784460.0"},{"content":"Selected Answer: B\nI chose B because the model need to learn","timestamp":"1727460000.0","comment_id":"1290205","poster":"kamparia","upvote_count":"1"},{"poster":"bludw","timestamp":"1719466200.0","comment_id":"1237888","upvote_count":"1","content":"Selected Answer: A\nI would choose A. And it is only because the features already have time-series information (like demand). And it would be way easier to train XGBoost than RNN model."},{"content":"Selected Answer: C\nC) The best choice for this scenario would be C. Recurrent Neural Networks (RNN).\n\nRationale:\n\nThe task at hand is a time-series prediction problem, where the goal is to predict future inventory levels based on historical data. RNNs are particularly well-suited for such tasks because they have “memory” and can learn patterns in sequential data1.\nFeatures like region, location, historical demand, and seasonal popularity can be used as input to the RNN. The network can then learn the temporal dependencies between these features and the inventory levels.\nRNNs can be trained incrementally, which means the model can be updated daily with new inventory data, allowing the model to adapt to changing trends and patterns","poster":"PhilipKoku","comment_id":"1225171","timestamp":"1717653780.0","upvote_count":"2"},{"upvote_count":"1","comment_id":"1103253","poster":"vale_76_na_xxx","timestamp":"1703233740.0","content":"go for C\nhttps://www.akkio.com/post/deep-learning-vs-reinforcement-learning-key-differences-and-use-cases#:~:text=Reinforcement%20learning%20is%20particularly%20well,of%20reinforcement%20learning%20in%20action."},{"poster":"Sum_Sum","content":"Selected Answer: C\nThe question asks for \"prediction model\"\nclassification and RL do not fit the bill\nCNN are used for vision\nso only answer left is C","upvote_count":"2","comment_id":"1070469","timestamp":"1699971720.0"},{"comment_id":"946857","content":"Selected Answer: C\nI'm not sure that daily basis means it is time series. It could mean updating the model daily.\nBut I'll follow collective intelligence.","poster":"12112","upvote_count":"2","timestamp":"1688871420.0"},{"poster":"M25","content":"Selected Answer: C\nWent with C","timestamp":"1683608280.0","upvote_count":"1","comment_id":"892692"},{"poster":"enghabeth","comment_id":"799784","content":"Selected Answer: B\nReinforcement Learning(RL) is a type of machine learning technique that enables an agent to learn in an interactive environment by trial and error using feedback from its own actions and experiences.","upvote_count":"1","timestamp":"1675691220.0"},{"upvote_count":"1","timestamp":"1671139020.0","poster":"wish0035","comment_id":"746512","content":"Selected Answer: C\nans: C"},{"comment_id":"725253","poster":"EFIGO","upvote_count":"1","content":"Selected Answer: C\nRNN are a fit tool to work with time-series as this one, so C","timestamp":"1669219740.0"},{"timestamp":"1660567560.0","poster":"GCP72","content":"Selected Answer: C\nCorrect answer is \"C\"","upvote_count":"2","comment_id":"647191"},{"upvote_count":"1","comment_id":"615357","timestamp":"1655042700.0","content":"Selected Answer: C\n\"algorithm to learn from new inventory data on a daily basis\" = time series model , best option to deal with time series is forsure RNN , vote for C","poster":"Mohamed_Mossad"},{"timestamp":"1649238960.0","upvote_count":"3","comment_id":"581740","content":"Selected Answer: C\nIt's C.","poster":"morgan62"},{"timestamp":"1643867400.0","upvote_count":"2","content":"C - for time series","poster":"A4M","comment_id":"539420"},{"content":"My option is B.\n\n\"You want the algorithm to learn from new inventory data on a daily basis\". The implication is a feedback with reward or punishment, which can optimise the mode. But, all other options can only practice prediction against new data rather than learning knowledge from new data automatically.","timestamp":"1638876720.0","upvote_count":"4","comment_id":"495921","poster":"alphard"},{"content":"I think it's D (CNN).\n\nI'd use C (RNN) in case we are predicting only based on historical demand (time series). However, as we are also taking region, location and seasonality popularity into consideration, it is not a time series problem anymore.","comment_id":"467097","poster":"majejim435","upvote_count":"1","timestamp":"1635102180.0"},{"upvote_count":"5","comment_id":"464280","poster":"mousseUwU","timestamp":"1634584620.0","content":"*RNN* is the \"Preferred algorithm for sequential data like time series, speech, text, financial data, audio, video, weather and much more\" since \"It learns over time what information is important and what is not\" because they \"can remember important things about the input they received, which allows them to be very precise in predicting what’s coming next\".\n\n- source: https://builtin.com/data-science/recurrent-neural-networks-and-lstm\n\nAnd *Reinforcement Learning* doesn't mean that the model will learn from new data (better explained by george_ognyanov)."},{"poster":"votvalenok","timestamp":"1627992060.0","comments":[{"comment_id":"444281","content":"how is \"learn from new data\" constituting option B?","poster":"Y2Data","timestamp":"1631584980.0","upvote_count":"4"}],"comment_id":"419203","content":"B. \"you want the algorithm to LEARN FROM NEW inventory data on a daily basis\"","upvote_count":"2"},{"upvote_count":"3","timestamp":"1622884200.0","poster":"salsabilsf","content":"Should be C\nscince we are training a time series model","comment_id":"374942"}],"choices":{"D":"Convolutional Neural Networks (CNN)","A":"Classification","C":"Recurrent Neural Networks (RNN)","B":"Reinforcement Learning"},"isMC":true,"question_text":"You are an ML engineer at a large grocery retailer with stores in multiple regions. You have been asked to create an inventory prediction model. Your model's features include region, location, historical demand, and seasonal popularity. You want the algorithm to learn from new inventory data on a daily basis. Which algorithms should you use to build the model?","answer_description":"","question_id":68,"answers_community":["C (83%)","Other"],"url":"https://www.examtopics.com/discussions/google/view/54591-exam-professional-machine-learning-engineer-topic-1-question/","unix_timestamp":1622884200,"answer":"C","topic":"1","answer_ET":"C","exam_id":13,"timestamp":"2021-06-05 11:10:00","answer_images":[],"question_images":[]},{"id":"TlsyJcLFscxErCwkEhWa","choices":{"B":"Use Vertex Explainable AI. Submit each prediction request with the explain' keyword to retrieve feature attributions using the sampled Shapley method.","D":"Use the What-If tool in Google Cloud to determine how your model will perform when individual features are excluded. Rank the feature importance in order of those that caused the most significant performance drop when removed from the model.","A":"Stream prediction results to BigQuery. Use BigQuery’s CORR(X1, X2) function to calculate the Pearson correlation coefficient between each feature and the target variable.","C":"Use Vertex AI Workbench user-managed notebooks to perform a Lasso regression analysis on your model, which will eliminate features that do not provide a strong signal."},"isMC":true,"discussion":[{"upvote_count":"4","poster":"fitri001","content":"Selected Answer: B\nFeature Importance per Prediction: Vertex Explainable AI with the Shapley method provides feature attributions for each individual prediction. This allows you to understand which attributes were most influential in the model's decision for that specific customer.\nNo Code Required: This approach leverages a built-in Vertex AI service and doesn't require writing additional code for Lasso regression (option C) or using the What-If tool (option D).","comment_id":"1199956","timestamp":"1729572780.0"},{"content":"Selected Answer: B\nI went for B, but not sure why it is not D. Is it even possible to model time series with the What If tool?","poster":"7cb0ab3","timestamp":"1728310800.0","comment_id":"1191011","upvote_count":"1"},{"timestamp":"1715493240.0","poster":"Mickey321","upvote_count":"3","content":"Selected Answer: B\nOption B","comment_id":"1068357"},{"timestamp":"1705767120.0","comment_id":"957681","poster":"PST21","upvote_count":"2","content":"Selected Answer: B\nto determine which customer attribute has the most predictive power for each prediction served by the model, you should use Vertex Explainable AI (Option B) with the 'explain' keyword to retrieve feature attributions using the sampled Shapley method. This will give you insights into feature importance at the individual prediction level, allowing you to understand the model's behavior for specific customers."}],"exam_id":13,"url":"https://www.examtopics.com/discussions/google/view/115870-exam-professional-machine-learning-engineer-topic-1-question/","answers_community":["B (100%)"],"timestamp":"2023-07-20 16:12:00","question_text":"You work for a magazine distributor and need to build a model that predicts which customers will renew their subscriptions for the upcoming year. Using your company’s historical data as your training set, you created a TensorFlow model and deployed it to Vertex AI. You need to determine which customer attribute has the most predictive power for each prediction served by the model. What should you do?","answer_description":"","question_id":69,"question_images":[],"answer_images":[],"topic":"1","answer":"B","answer_ET":"B","unix_timestamp":1689862320},{"id":"icRFjnnHqtXEX3buhgGh","answer_ET":"C","question_text":"You are an ML engineer at a manufacturing company. You are creating a classification model for a predictive maintenance use case. You need to predict whether a crucial machine will fail in the next three days so that the repair crew has enough time to fix the machine before it breaks. Regular maintenance of the machine is relatively inexpensive, but a failure would be very costly. You have trained several binary classifiers to predict whether the machine will fail, where a prediction of 1 means that the ML model predicts a failure.\n\nYou are now evaluating each model on an evaluation dataset. You want to choose a model that prioritizes detection while ensuring that more than 50% of the maintenance jobs triggered by your model address an imminent machine failure. Which model should you choose?","discussion":[{"content":"Selected Answer: C\nA (High AUC ROC, Precision > 0.5): AUC ROC is a good overall metric, but it doesn't directly address the specific priorities of this problem. A model with a high AUC might have a good balance of precision and recall on average, but it might not have the highest recall while maintaining the precision threshold. The focus here is on maximizing recall subject to the precision constraint.","poster":"NamitSehgal","timestamp":"1739842560.0","comment_id":"1358076","upvote_count":"1"},{"content":"C. The model with the highest recall where precision is greater than 0.5.\n\n\nIn this predictive maintenance use case, you want to prioritize detection (i.e., detecting imminent failures) while ensuring that most of the maintenance jobs triggered by your model address actual machine failures (i.e., true positives). Recall measures the proportion of actual failures detected by the model, which aligns with your goal of prioritizing detection.","poster":"AzureDP900","comment_id":"1234758","timestamp":"1718993460.0","upvote_count":"1"},{"timestamp":"1713762360.0","upvote_count":"4","poster":"fitri001","content":"Selected Answer: C\nPrioritizing Detection: Recall measures how well the model identifies true positives (correctly predicts failures). A high recall ensures most imminent failures are caught.\nBalancing with Precision: Precision measures how many of the predicted failures are true positives (avoiding unnecessary maintenance). The requirement of a precision greater than 0.5 ensures a reasonable number of triggered maintenances actually address failures.","comment_id":"1199963"},{"timestamp":"1712487900.0","poster":"pinimichele01","content":"Selected Answer: C\nwent with C","comment_id":"1190907","upvote_count":"1"},{"poster":"guilhermebutzke","comment_id":"1142358","upvote_count":"3","content":"Selected Answer: C\nEarly detection of potential failures is crucial, even if it leads to some unnecessary maintenance (\"false positives\"). Therefore, we prioritize recall, which measures the ability to correctly identify true failures.\nWhile detection is important, we don't want to trigger too many unnecessary repairs (\"false positives\"). So, we set a minimum threshold of precision greater than 0.5, meaning at least 50% of triggered maintenance should address real failures.","timestamp":"1707236460.0"},{"upvote_count":"2","timestamp":"1703756640.0","comment_id":"1107644","content":"Selected Answer: C\nPriority is to detect(Pointing to Recall) and correctly detect (more that 50% - pointing to Precision)","poster":"vfg"}],"isMC":true,"choices":{"A":"The model with the highest area under the receiver operating characteristic curve (AUC ROC) and precision greater than 0.5","C":"The model with the highest recall where precision is greater than 0.5.","B":"The model with the lowest root mean squared error (RMSE) and recall greater than 0.5.","D":"The model with the highest precision where recall is greater than 0.5."},"topic":"1","answer_description":"","answer":"C","answer_images":[],"question_id":70,"question_images":[],"exam_id":13,"timestamp":"2023-12-10 21:30:00","answers_community":["C (100%)"],"url":"https://www.examtopics.com/discussions/google/view/128242-exam-professional-machine-learning-engineer-topic-1-question/","unix_timestamp":1702240200}],"exam":{"provider":"Google","lastUpdated":"11 Apr 2025","isMCOnly":true,"name":"Professional Machine Learning Engineer","isImplemented":true,"numberOfQuestions":304,"id":13,"isBeta":false},"currentPage":14},"__N_SSP":true}