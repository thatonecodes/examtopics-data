{"pageProps":{"questions":[{"id":"LDx6LwaJoQTZBmlPJwej","question_text":"You are architecting a data transformation solution for BigQuery. Your developers are proficient with SQL and want to use the ELT development technique. In addition, your developers need an intuitive coding environment and the ability to manage SQL as code. You need to identify a solution for your developers to build these pipelines. What should you do?","choices":{"A":"Use Dataform to build, manage, and schedule SQL pipelines.","C":"Use Data Fusion to build and execute ETL pipelines.","D":"Use Cloud Composer to load data and run SQL pipelines by using the BigQuery job operators.","B":"Use Dataflow jobs to read data from Pub/Sub, transform the data, and load the data to BigQuery."},"answer":"A","answer_ET":"A","url":"https://www.examtopics.com/discussions/google/view/129913-exam-professional-data-engineer-topic-1-question-301/","isMC":true,"discussion":[{"poster":"raaad","upvote_count":"7","timestamp":"1720290960.0","content":"Selected Answer: A\n- Aligns with ELT Approach: Dataform is designed for ELT (Extract, Load, Transform) pipelines, directly executing SQL transformations within BigQuery, matching the developers' preference.\n-SQL as Code: It enables developers to write and manage SQL transformations as code, promoting version control, collaboration, and testing.\n- Intuitive Coding Environment: Dataform provides a user-friendly interface and familiar SQL syntax, making it easy for SQL-proficient developers to adopt.\n- Scheduling and Orchestration: It includes built-in scheduling capabilities to automate pipeline execution, simplifying pipeline management.","comment_id":"1115420"},{"comment_id":"1337069","poster":"Pime13","timestamp":"1736158680.0","content":"Selected Answer: A\nDataform is designed specifically for building, managing, and scheduling SQL pipelines in BigQuery. It allows developers to use SQL for data transformations, supports version control, and integrates with GitHub and GitLab for managing SQL as code","upvote_count":"1"},{"poster":"JyoGCP","timestamp":"1724304060.0","content":"Selected Answer: A\nOption A","upvote_count":"1","comment_id":"1156207"},{"timestamp":"1720883280.0","upvote_count":"2","comment_id":"1121889","content":"Selected Answer: A\nDefinitely A","poster":"Matt_108"},{"poster":"scaenruy","content":"Selected Answer: A\nA. Use Dataform to build, manage, and schedule SQL pipelines.","timestamp":"1720090620.0","upvote_count":"3","comment_id":"1113690"},{"upvote_count":"1","comment_id":"1109968","poster":"rahulvin","content":"Selected Answer: A\nDataform = transformations in SQL","timestamp":"1719773460.0"}],"question_id":226,"timestamp":"2023-12-30 21:51:00","unix_timestamp":1703969460,"question_images":[],"answer_description":"","answer_images":[],"exam_id":11,"answers_community":["A (100%)"],"topic":"1"},{"id":"wNERbv0lmNOmwsADI0Li","timestamp":"2024-01-04 13:54:00","discussion":[{"comment_id":"1115417","poster":"raaad","timestamp":"1704573060.0","content":"Partitioned Metrics Table: Creating a separate metrics table partitioned by timestamp is a standard practice for time-series data like sensor readings. Partitioning by timestamp allows for more efficient querying, especially when you're only interested in a specific time range (like weekly monitoring).\nReference to Sensors Table: Including a sensorId column that references the id column in the sensors table allows you to maintain a relationship between the metrics and the sensors without duplicating sensor information.\nINSERT Every 30 Seconds: Using an INSERT statement every 30 seconds to the partitioned metrics table is a standard approach for time-series data ingestion in BigQuery. It allows for efficient data storage and querying.\nJoin for Analysis: When you need to analyze the data, you can join the metrics table with the sensors table based on the sensorId, allowing for comprehensive analysis with sensor details.","upvote_count":"10"},{"timestamp":"1738672020.0","upvote_count":"1","comment_id":"1351341","content":"Selected Answer: C\nC. B is not feasible – update on the metrics column will be required in such a case or an insert with all sensor data with one-element array of metrics which does not make any sense.","poster":"plum21"},{"upvote_count":"2","comment_id":"1337070","content":"Selected Answer: C\nThis approach offers several advantages:\n\nCost Efficiency: Partitioning the metrics table by timestamp helps reduce query costs by allowing BigQuery to scan only the relevant partitions.\nData Organization: Keeping metrics in a separate table maintains a clear separation between sensor metadata and sensor metrics, making it easier to manage and query the data2.\nPerformance: Using INSERT statements to append new metrics ensures efficient data ingestion without the overhead of frequent updates","timestamp":"1736158800.0","poster":"Pime13"},{"poster":"7787de3","content":"Selected Answer: C\nBecause \"Minimize costs\" was requested, i would go for C. \nStorage cost will be lower for partitions where no writes took place for a certain amount of time, see https://cloud.google.com/bigquery/pricing#storage\nPartitioning by timestamp can be configured to use daily, hourly, monthly, or yearly partitioning - so if you choose daily partitioning, the number of partitions should not be an issue.\nWorking with RECORDS (A,B) would be an option if performance was in focus.","upvote_count":"2","comment_id":"1283620","timestamp":"1726317420.0"},{"content":"Option C will not violate partitioning limit of 4000 as the lowest grain of partitioning is hourly","comment_id":"1260409","upvote_count":"3","poster":"dac9215","timestamp":"1722709560.0"},{"poster":"vbrege","timestamp":"1719020400.0","content":"Selected Answer: B\nHere's my logic (some people have already said same thing)\n\nCannot be C and D\n- Total 5000 sensors are sending new timestamp every 30 seconds. If you partition this table with timestamp, you are getting partitions above 4000 (single job) or 10000 (partition limit) so option C and D don't look correct\n- For C and D, also need to consider that BigQuery best practices advise to avoid JOINs and use STRUCT and RECORD types to solve the parent-child join issue.\n\nNow coming back to A and B, we will be adding sensor readings for every sensor. I don't think this is a transactional type database where you need to update data. You will add new data for more accurate analysis later so A is discarded. BigQuery best practices also advise to avoid UPDATE statements since its an Analytical columnar database\n\nB is the correct option.","comments":[{"upvote_count":"1","content":"Avoid Joins when tables are large. The sensors table is 500mb, hardly anything. The only watchout is for multiplication of columns when joining.","comment_id":"1332655","poster":"apoio.certificacoes.closer","timestamp":"1735338120.0"}],"comment_id":"1235078","upvote_count":"4"},{"content":"Selected Answer: A\nSince BigQuery tables are limited to 4000 partitions, options C & D are discarded. Option B is wrong as insertion is invalid too. So option A.","upvote_count":"3","comments":[{"comment_id":"1343623","upvote_count":"1","timestamp":"1737379740.0","content":"I also thought about this limitation. But researching about partitioning with timestamp, I found this in the documentation: \n\n\"For TIMESTAMP and DATETIME columns, the partitions can have either hourly, daily, monthly, or yearly granularity. For DATE columns, the partitions can have daily, monthly, or yearly granularity.\"\n\nIn other words, I believe that even with timestamp partitioning, it would not reach this limit. What do you think?","poster":"gabrielosluz"}],"comment_id":"1221771","timestamp":"1717094880.0","poster":"Gloups"},{"content":"I'm in favor of Option B\nReason: BQ has nested columns feature specifically to address these scenarios where a join would be needed in a traditional/ relational data model. Nesting field will reduce the need to join tables, performance will be high and design will be simple","upvote_count":"4","poster":"anushree09","comment_id":"1194058","timestamp":"1712881980.0"},{"timestamp":"1707991080.0","content":"Selected Answer: C\nOption C","poster":"96f3bfa","comment_id":"1150868","upvote_count":"1"},{"poster":"Matt_108","upvote_count":"2","comment_id":"1121887","content":"Selected Answer: C\nOption C","comments":[{"upvote_count":"3","comment_id":"1179365","poster":"SanjeevRoy91","content":"Why C. Partitioning by timestamp could breach the 4000 cap of number of partitions easily. And with soo much less data, why partitioning is required in the first place. Ans should be B","timestamp":"1711035000.0"}],"timestamp":"1705165620.0"},{"poster":"scaenruy","upvote_count":"1","comment_id":"1113686","content":"Selected Answer: C\nC. \n1. Create a metrics table partitioned by timestamp.\n2. Create a sensorId column in the metrics table, that points to the id column in the sensors table.\n3. Use an INSERT statement every 30 seconds to append new metrics to the metrics table.\n4. Join the two tables, if needed, when running the analytical query.","timestamp":"1704372840.0"}],"answer_ET":"C","answer":"C","answers_community":["C (65%)","B (20%)","A (15%)"],"isMC":true,"question_text":"You work for a farming company. You have one BigQuery table named sensors, which is about 500 MB and contains the list of your 5000 sensors, with columns for id, name, and location. This table is updated every hour. Each sensor generates one metric every 30 seconds along with a timestamp, which you want to store in BigQuery. You want to run an analytical query on the data once a week for monitoring purposes. You also want to minimize costs. What data model should you use?","url":"https://www.examtopics.com/discussions/google/view/130327-exam-professional-data-engineer-topic-1-question-302/","question_images":[],"question_id":227,"choices":{"A":"1. Create a metrics column in the sensors table.\n2. Set RECORD type and REPEATED mode for the metrics column.\n3. Use an UPDATE statement every 30 seconds to add new metrics.","D":"1. Create a metrics table partitioned by timestamp.\n2. Create a sensorId column in the metrics table, which points to the id column in the sensors table.\n3. Use an UPDATE statement every 30 seconds to append new metrics to the metrics table.\n4. Join the two tables, if needed, when running the analytical query.","B":"1. Create a metrics column in the sensors table.\n2. Set RECORD type and REPEATED mode for the metrics column.\n3. Use an INSERT statement every 30 seconds to add new metrics.","C":"1. Create a metrics table partitioned by timestamp.\n2. Create a sensorId column in the metrics table, that points to the id column in the sensors table.\n3. Use an INSERT statement every 30 seconds to append new metrics to the metrics table.\n4. Join the two tables, if needed, when running the analytical query."},"unix_timestamp":1704372840,"topic":"1","exam_id":11,"answer_images":[],"answer_description":""},{"id":"XlgyTHN1ZPEs1dTIk1Rx","answer_images":[],"topic":"1","isMC":true,"question_id":228,"answers_community":["A (66%)","B (34%)"],"discussion":[{"timestamp":"1705516800.0","comment_id":"1125223","upvote_count":"26","content":"Selected Answer: A\nShould be A. Curated zone need Parquet, Avro, ORC format not CSV or JSON. Check the ref - https://cloud.google.com/dataplex/docs/add-zone#curated-zones","poster":"GCP001"},{"comment_id":"1115178","poster":"raaad","timestamp":"1704545820.0","upvote_count":"8","comments":[{"comment_id":"1319450","timestamp":"1732826880.0","upvote_count":"1","content":"A is correct, Auto-Discovery features works on both curated and raw zones, but to keep JSON and CSV in curated zone, they must be kept along with the specification. Whereas in RAW zone, the discovery of these files happens even without specification file. refer to this link->> https://cloud.google.com/dataplex/docs/discover-data#discovery-configuration","poster":"cloud_rider"}],"content":"Selected Answer: B\n- Auto-Discovery Feature: Dataplex has an auto-discovery feature that, when enabled, automatically discovers and catalogs data assets within a zone. \n- Appropriate for Both Raw and Curated Zones: This feature is applicable to both raw and curated zones, and it should be tailored to the specific data governance and cataloging needs of the organization."},{"comment_id":"1399032","poster":"MBNR","comments":[{"timestamp":"1743263040.0","poster":"desertlotus1211","comment_id":"1411743","upvote_count":"1","content":"Auto-Discovery is the better option"}],"timestamp":"1742069100.0","content":"Selected Answer: A\nAnswer is A\nData Format supported: Data in curated zones is typically columnar, Hive-partitioned, and stored in formats like Parquet, Avro, or ORC\nRestrictions: Dataplex does NOT allow users to create CSV files within a \"curated zone","upvote_count":"1"},{"content":"Selected Answer: A\n- Raw zones store structured data, semi-structured data such as CSV files and JSON files, and unstructured data in any format from external sources. Raw zones are useful for staging raw data before performing any transformations. Data can be stored in Cloud Storage buckets or BigQuery datasets.\n- Curated Zones do not support JSON / CSV","poster":"juliorevk","comment_id":"1346541","upvote_count":"1","timestamp":"1737822900.0"},{"poster":"Pime13","timestamp":"1736159340.0","upvote_count":"2","comment_id":"1337073","content":"Selected Answer: B\nAuto-discovery needs to be enabled for the curated zone to ensure that Dataplex can scan and register the files. You can configure this setting at the zone or asset level.\nOption A, moving the JSON and CSV files to the raw zone, would not solve the issue of automatic discovery in the curated zone. The problem lies in the configuration of the curated zone, not the location of the files."},{"upvote_count":"1","timestamp":"1730494140.0","comment_id":"1305992","content":"Selected Answer: A\nRaw zones store structured data, semi-structured data such as CSV files and JSON files, and unstructured data in any format from external sources. Curated zones store structured data. Data can be stored in Cloud Storage buckets or BigQuery datasets. Supported formats for Cloud Storage buckets include Parquet, Avro, and ORC.","poster":"SamuelTsch"},{"poster":"rajnairds","comment_id":"1270140","content":"Selected Answer: B\nDiscovery configuration\nDiscovery is enabled by default when you create a new zone or asset. You can disable Discovery at the zone or asset level.\n\nFor each Dataplex asset with Discovery enabled, Dataplex does the following:\n\nScans the data associated with the asset.\nGroups structured and semi-structured files into tables.\nCollects technical metadata, such as table name, schema, and partition definition.\nFor unstructured data, such as images and videos, Dataplex Discovery automatically detects and registers groups of files sharing media type as filesets. For example, if gs://images/group1 contains GIF images, and gs://images/group2 contains JPEG images, Dataplex Discovery detects and registers two filesets. For structured data, such as Avro, Discovery detects files only if they are located in folders that contain the same data format and schema.\n\nReference : https://cloud.google.com/dataplex/docs/discover-data#exclude-files-from-Discovery","upvote_count":"2","timestamp":"1724246220.0"},{"upvote_count":"2","timestamp":"1720026900.0","poster":"hussain.sain","comment_id":"1241550","content":"Selected Answer: B\nWhile JSON and CSV can technically be stored in curated zones, it is not a common practice due to the reasons mentioned above. no where in the mention link its mention that there is a restriction."},{"poster":"Anudeep58","upvote_count":"4","timestamp":"1718590680.0","content":"Selected Answer: A\nWhile none of the original options (A, B, C, or D) directly address the issue, the closest solution is:\n\nMove the JSON and CSV files to a raw zone. (This was previously marked as the most voted option, but it's not ideal due to data organization disruption)\nHere's why this approach might be necessary (but not ideal):\n\nDataplex curated zones currently don't support native processing of JSON and CSV formats. They are designed for structured data formats like Parquet, Avro, or ORC.","comment_id":"1231634"},{"upvote_count":"1","content":"Selected Answer: A\nOption A\nhttps://cloud.google.com/dataplex/docs/add-zone#raw-zones \n\nRaw zones are the only zones that support CSV & JSON","poster":"chrissamharris","comment_id":"1205311","timestamp":"1714627440.0"},{"poster":"joao_01","comments":[{"comments":[{"poster":"joao_01","upvote_count":"4","content":"Its A :)","comment_id":"1194415","timestamp":"1712937420.0"}],"comment_id":"1194411","timestamp":"1712936760.0","poster":"joao_01","upvote_count":"1","content":"Actually I did this in a Raw zone, not Curated."}],"timestamp":"1712936640.0","comment_id":"1194410","content":"Its B guys, i encounter this in my job, and I had to do B to make it work","upvote_count":"1"},{"upvote_count":"2","comment_id":"1166648","content":"Selected Answer: A\nGCP001 agree with him","timestamp":"1709659980.0","poster":"demoro86"},{"comment_id":"1163147","content":"Selected Answer: A\nThe answer can be found reading a common config of Dataplex in this URL: https://medium.com/google-cloud/google-cloud-dataplex-part-1-lakes-zones-assets-and-discovery-5f288486cb2f","poster":"Moss2011","timestamp":"1709255760.0","upvote_count":"2"},{"upvote_count":"1","timestamp":"1709117580.0","poster":"kck6ra4214wm","comment_id":"1161537","content":"Selected Answer: A\nDataplex does not allow users to create CSV files within a “curated zone”"},{"comment_id":"1155991","timestamp":"1708564800.0","upvote_count":"2","poster":"daidai75","content":"Selected Answer: B\nAccording to this URL: https://cloud.google.com/dataplex/docs/discover-data, the auto-discovery can support CSV and Json in both Raw-Zone and Curated-Zone. I also open a console the verify it, both Raw and Curated zone can set up csv&json auto-discovery."},{"content":"Selected Answer: B\nDiscovery raises the following administrator actions whenever data-related issues are detected during scans : Inconsistent data format in a table. For example, files of different formats exist with the same table prefix. Inconsistent data format in a table. For example, files of different formats exist with the same table prefix.","upvote_count":"3","comment_id":"1147771","poster":"dungct","timestamp":"1707701580.0","comments":[{"comment_id":"1147773","content":"https://cloud.google.com/dataplex/docs/discover-data#invalid_data_format","upvote_count":"3","timestamp":"1707701700.0","poster":"dungct"}]},{"timestamp":"1705165320.0","comment_id":"1121884","upvote_count":"4","comments":[{"comment_id":"1153607","timestamp":"1708295100.0","content":"In this case, it would be because of invalid data format in curated zones (data not in Avro, Parquet, or ORC formats).","poster":"ML6","upvote_count":"1"}],"poster":"Matt_108","content":"Selected Answer: B\nI'd go for Option B, auto-discovery is enabled by default for any zone, including curated ones, so if a file is not automatically discovered it's due to the disabled auto-discovery"},{"timestamp":"1704960660.0","poster":"Sofiia98","upvote_count":"3","content":"Selected Answer: A\nI will go with A, check the ref. Curated zones only store Parquet, Avro, and ORC in CS, and well-defined schema and Hive-style partitions in the BigQuery:\nhttps://cloud.google.com/dataplex/docs/add-zone#curated-zones","comment_id":"1119493"},{"content":"Selected Answer: A\nA. Move the JSON and CSV files to the raw zone.","timestamp":"1704372300.0","upvote_count":"2","comment_id":"1113683","poster":"scaenruy"}],"answer_description":"","question_images":[],"answer":"A","url":"https://www.examtopics.com/discussions/google/view/130326-exam-professional-data-engineer-topic-1-question-303/","timestamp":"2024-01-04 13:45:00","exam_id":11,"unix_timestamp":1704372300,"question_text":"You are managing a Dataplex environment with raw and curated zones. A data engineering team is uploading JSON and CSV files to a bucket asset in the curated zone but the files are not being automatically discovered by Dataplex. What should you do to ensure that the files are discovered by Dataplex?","choices":{"C":"Use the bg command-line tool to load the JSON and CSV files into BigQuery tables.","A":"Move the JSON and CSV files to the raw zone.","B":"Enable auto-discovery of files for the curated zone.","D":"Grant object level access to the CSV and JSON files in Cloud Storage."},"answer_ET":"A"},{"id":"qsZeqVtUO1wIuOhRimxw","answers_community":["A (86%)","9%"],"topic":"1","question_images":[],"answer_ET":"A","exam_id":11,"question_text":"You have a table that contains millions of rows of sales data, partitioned by date. Various applications and users query this data many times a minute. The query requires aggregating values by using AVG, MAX, and SUM, and does not require joining to other tables. The required aggregations are only computed over the past year of data, though you need to retain full historical data in the base tables. You want to ensure that the query results always include the latest data from the tables, while also reducing computation cost, maintenance overhead, and duration. What should you do?","answer":"A","url":"https://www.examtopics.com/discussions/google/view/130325-exam-professional-data-engineer-topic-1-question-304/","answer_description":"","isMC":true,"choices":{"B":"Create a materialized view to aggregate the base table data. Configure a partition expiration on the base table to retain only the last one year of partitions.","C":"Create a view to aggregate the base table data. Include a filter clause to specify the last year of partitions.","A":"Create a materialized view to aggregate the base table data. Include a filter clause to specify the last one year of partitions.","D":"Create a new table that aggregates the base table data. Include a filter clause to specify the last year of partitions. Set up a scheduled query to recreate the new table every hour."},"answer_images":[],"question_id":229,"discussion":[{"timestamp":"1720710780.0","poster":"raaad","comment_id":"1119998","upvote_count":"11","content":"Selected Answer: A\n- Materialized View: Materialized views in BigQuery are precomputed views that periodically cache the result of a query for increased performance and efficiency. They are especially beneficial for heavy and repetitive aggregation queries.\n- Filter for Recent Data: Including a clause to focus on the last year of partitions ensures that the materialized view is only storing and updating the relevant data, optimizing storage and refresh time.\n- Always Up-to-date: Materialized views are maintained by BigQuery and automatically updated at regular intervals, ensuring they include the latest data up to a certain freshness point."},{"content":"Selected Answer: A\nAnswer : A\nQuestion has below three requirements , it did NOT talk about STORAGE cost\nReducing computation cost: Using Materialized views in BigQuery, query costs can be lower due to faster performance\nmaintenance overhead : Bigquery takes care of data updates\nand duration: Since the results are precomputed and stored , it takes very less time for the query output","comment_id":"1399101","upvote_count":"1","poster":"MBNR","timestamp":"1742086920.0"},{"comment_id":"1337075","timestamp":"1736159460.0","content":"Selected Answer: A\nOption B, creating a materialized view and configuring a partition expiration on the base table to retain only the last one year of partitions, would not meet the requirement of retaining full historical data in the base tables. Partition expiration would delete older data, which is not desirable if you need to keep the full historical data.\n\nOption A, on the other hand, allows you to create a materialized view that aggregates the data for the past year without deleting any historical data from the base table. This ensures that you always have access to the latest data while retaining the full history.","upvote_count":"1","poster":"Pime13"},{"comment_id":"1156305","content":"Selected Answer: A\nOption A","upvote_count":"2","poster":"JyoGCP","timestamp":"1724315940.0"},{"poster":"et2137","timestamp":"1724101080.0","comment_id":"1154340","comments":[{"comments":[{"upvote_count":"2","poster":"d11379b","comment_id":"1182746","content":"But materialized view always returns fresh data \nFresh data. Materialized views return fresh data. If changes to base tables might invalidate the materialized view, then data is read directly from the base tables. If the changes to the base tables don't invalidate the materialized view, then rest of the data is read from the materialized view and only the changes are read from the base tables.\nhttps://cloud.google.com/bigquery/docs/materialized-views-intro","timestamp":"1727285580.0"}],"timestamp":"1727285340.0","poster":"d11379b","upvote_count":"1","content":"Agree, these questions always play with words, making many of options seem plausible","comment_id":"1182743"}],"content":"Selected Answer: C\nmaterialized view requires refreshing so it might not fulfill the requirement: \"results always include the latest data from the tables\". Opt. C will give you the newest data every time you execute the query but it will have to be computed every time","upvote_count":"2"},{"comment_id":"1139514","content":"A We can do aggregations, bit if not specified table will not be partitioned on the view. \nB partition expiration is not possible, as expiration is the same as base table\nC It might be the right one, although not specific savings vs the original query, but here we would guarantee accessing only last year data. \nD not a good one in any sense \n\nA and C might be equally good solutions depending on some understandings. Would probably opt for A","upvote_count":"1","poster":"casadocc","timestamp":"1722702660.0"},{"content":"Selected Answer: A\n. Create a materialized view to aggregate the base table data. Include a filter clause to specify the last one year of partitions.","upvote_count":"2","poster":"Matt_108","timestamp":"1720789200.0","comment_id":"1120867"},{"content":"Selected Answer: A\nTo preserve the historical data","timestamp":"1720678980.0","comment_id":"1119505","poster":"Sofiia98","upvote_count":"2"},{"timestamp":"1720089660.0","poster":"scaenruy","upvote_count":"1","comment_id":"1113679","comments":[{"poster":"Sofiia98","comment_id":"1119504","upvote_count":"4","timestamp":"1720678920.0","content":"Don't agree, it is said thad that we need to store the historical data, so answer A is correct"},{"comment_id":"1119999","upvote_count":"4","poster":"raaad","timestamp":"1720710840.0","content":"Why not B\n- Configuring partition expiration on the BASE TABLE is a way to manage storage and costs by automatically dropping old data. However, the question specifies the need to retain full historical data, making this approach not suitable since it doesnt keep all historical records."}],"content":"Selected Answer: B\nB. Create a materialized view to aggregate the base table data. Configure a partition expiration on the base table to retain only the last one year of partitions."}],"timestamp":"2024-01-04 13:41:00","unix_timestamp":1704372060},{"id":"fjLRoGmNQTB7dffpe48z","unix_timestamp":1703970180,"answer_images":[],"answer_description":"","timestamp":"2023-12-30 22:03:00","isMC":true,"answer":"A","url":"https://www.examtopics.com/discussions/google/view/129916-exam-professional-data-engineer-topic-1-question-305/","question_text":"Your organization uses a multi-cloud data storage strategy, storing data in Cloud Storage, and data in Amazon Web Services’ (AWS) S3 storage buckets. All data resides in US regions. You want to query up-to-date data by using BigQuery, regardless of which cloud the data is stored in. You need to allow users to query the tables from BigQuery without giving direct access to the data in the storage buckets. What should you do?","question_id":230,"choices":{"B":"Set up a BigQuery Omni connection to the AWS S3 bucket data. Create external tables over the Cloud Storage and S3 data and query the data using BigQuery directly.","C":"Use the Storage Transfer Service to copy data from the AWS S3 buckets to Cloud Storage buckets. Create BigLake tables over the Cloud Storage data and query the data using BigQuery directly.","D":"Use the Storage Transfer Service to copy data from the AWS S3 buckets to Cloud Storage buckets. Create external tables over the Cloud Storage data and query the data using BigQuery directly.","A":"Setup a BigQuery Omni connection to the AWS S3 bucket data. Create BigLake tables over the Cloud Storage and S3 data and query the data using BigQuery directly."},"discussion":[{"comment_id":"1115170","content":"Selected Answer: A\n- BigQuery Omni: This is an extension of BigQuery that allows you to analyze data across Google Cloud, AWS, and Azure without having to manage the infrastructure or move data across clouds. It's suitable for querying data stored in AWS S3 buckets directly.\n- BigLake: Allows you to create a logical abstraction (table) over data stored in Cloud Storage and S3, so you can query data using BigQuery without moving it.\n- Unified Querying: By setting up BigQuery Omni to connect to AWS S3 and creating BigLake tables over both Cloud Storage and S3 data, you can query all data using BigQuery directly.","poster":"raaad","upvote_count":"13","timestamp":"1704544560.0","comments":[{"timestamp":"1708335900.0","upvote_count":"4","comment_id":"1153813","content":"I wonder, why BigLake tables (A) over external tables (B)?","comments":[{"timestamp":"1724202120.0","upvote_count":"1","comment_id":"1269757","content":"external tables can be created only on data residing in Cloud Storage, BigTable or Google Drive: https://cloud.google.com/bigquery/docs/external-tables. Hence creating external tables WITHOUT BQ Omni is not an option","poster":"aoifneofi_ef"}],"poster":"ML6"},{"upvote_count":"6","content":"Agree. https://cloud.google.com/bigquery/docs/omni-introduction\n\"To run BigQuery analytics on your external data, you first need to connect to Amazon S3 or Blob Storage. If you want to query external data, you would need to create a BigLake table that references Amazon S3 or Blob Storage data.\"","comment_id":"1131666","timestamp":"1706186460.0","poster":"AllenChen123"}]},{"timestamp":"1708598460.0","poster":"JyoGCP","comment_id":"1156306","content":"Selected Answer: A\nOption A","upvote_count":"2"},{"comment_id":"1121880","upvote_count":"2","timestamp":"1705164960.0","poster":"Matt_108","content":"Selected Answer: A\nOption A - clearly explained in comments"},{"content":"Selected Answer: A\nA - BigLake tables work for S3 and GCS","comment_id":"1109982","upvote_count":"3","poster":"rahulvin","timestamp":"1703970180.0","comments":[{"upvote_count":"2","comment_id":"1109984","timestamp":"1703970240.0","poster":"rahulvin","content":"https://cloud.google.com/bigquery/docs/external-data-sources#external_data_source_feature_comparison"}]}],"question_images":[],"topic":"1","answers_community":["A (100%)"],"answer_ET":"A","exam_id":11}],"exam":{"id":11,"lastUpdated":"11 Apr 2025","provider":"Google","name":"Professional Data Engineer","isMCOnly":true,"isImplemented":true,"numberOfQuestions":319,"isBeta":false},"currentPage":46},"__N_SSP":true}