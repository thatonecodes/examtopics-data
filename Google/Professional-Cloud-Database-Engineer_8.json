{"pageProps":{"questions":[{"id":"n2gToGbvZVCSvpPGbcgK","topic":"1","answer_images":[],"answer":"A","timestamp":"2023-03-28 01:34:00","answer_description":"","question_text":"Your DevOps team is using Terraform to deploy applications and Cloud SQL databases. After every new application change is rolled out, the environment is torn down and recreated, and the persistent database layer is lost. You need to prevent the database from being dropped. What should you do?","question_images":[],"url":"https://www.examtopics.com/discussions/google/view/104143-exam-professional-cloud-database-engineer-topic-1-question/","isMC":true,"answer_ET":"A","question_id":36,"choices":{"C":"Create a read replica.","B":"Rerun terraform apply.","A":"Set Terraform deletion_protection to true.","D":"Use point-in-time-recovery (PITR) to recover the database."},"answers_community":["A (100%)"],"unix_timestamp":1679960040,"exam_id":5,"discussion":[{"upvote_count":"9","comment_id":"852568","poster":"dynamic_dba","content":"A.\nIt makes sense that a Terraform problem would use a Terra form solution. From Google's documentation, \"For stateful resources, such as databases, ensure that deletion protection is enabled. The syntax is:\nlifecycle {\n prevent_destroy = true\n}\nhttps://cloud.google.com/docs/terraform/best-practices-for-terraform#stateful-resources","timestamp":"1695857640.0"},{"upvote_count":"1","poster":"Pime13","comment_id":"1205343","content":"Selected Answer: A\nhttps://cloud.google.com/docs/terraform/best-practices-for-terraform#stateful-resources","timestamp":"1730539200.0"},{"content":"Selected Answer: A\nFor stateful resources, such as databases, ensure that deletion protection is enabled.","poster":"DPonly","comment_id":"997135","upvote_count":"1","timestamp":"1709420520.0"},{"upvote_count":"2","content":"Selected Answer: A\nagreed with dynamic_dba","timestamp":"1705774020.0","comment_id":"957802","poster":"dedotes"}]},{"id":"qI97HegifOpzwt5aaADA","answer_ET":"A","answer_images":[],"question_images":[],"question_id":37,"exam_id":5,"answer":"A","timestamp":"2023-03-23 21:37:00","question_text":"Your company's mission-critical, globally available application is supported by a Cloud Spanner database. Experienced users of the application have read and write access to the database, but new users are assigned read-only access to the database. You need to assign the appropriate Cloud Spanner Identity and Access Management (IAM) role to new users being onboarded soon. What roles should you set up?","unix_timestamp":1679603820,"discussion":[{"timestamp":"1695857760.0","content":"A.\nNothing to do with backups. Eliminate D. databaseUser allows read and write. Eliminate B. viewer is at the instance level and does not allow database reads. Eliminate C. Leaves A.","comment_id":"852571","poster":"dynamic_dba","upvote_count":"7"},{"content":"The correct answer is A! With Spanner.viewer user cannot read database, but he can visualize only database List. https://cloud.google.com/spanner/docs/iam?hl=it","timestamp":"1695494220.0","upvote_count":"5","comment_id":"848643","poster":"EmiNard84"},{"comment_id":"1205345","timestamp":"1730539380.0","poster":"Pime13","upvote_count":"2","content":"Selected Answer: A\nA:\nhttps://cloud.google.com/spanner/docs/iam#roles\n\nviewer can only see the databases not read data"},{"content":"Selected Answer: A\nSee this link https://cloud.google.com/spanner/docs/iam","upvote_count":"5","poster":"EueChan","timestamp":"1695494580.0","comment_id":"848654"}],"answers_community":["A (100%)"],"choices":{"C":"roles/spanner.viewer","A":"roles/spanner.databaseReader","D":"roles/spanner.backupWriter","B":"roles/spanner.databaseUser"},"topic":"1","isMC":true,"url":"https://www.examtopics.com/discussions/google/view/103700-exam-professional-cloud-database-engineer-topic-1-question/","answer_description":""},{"id":"gxZK6Nd4cBtIXUxobVie","answer_ET":"CE","answer":"CE","answer_description":"","choices":{"C":"Use Database Migration Service to migrate the databases to Cloud SQL.","E":"Use replication from an external server to migrate the databases to Cloud SQL.","D":"Use a cross-region read replica to migrate the databases to Cloud SQL.","A":"Use an external read replica to migrate the databases to Cloud SQL.","B":"Use a read replica to migrate the databases to Cloud SQL."},"question_images":[],"topic":"1","answers_community":["CE (86%)","14%"],"answer_images":[],"timestamp":"2023-03-23 21:39:00","question_text":"Your company is shutting down their data center and migrating several MySQL and PostgreSQL databases to Google Cloud. Your database operations team is severely constrained by ongoing production releases and the lack of capacity for additional on-premises backups. You want to ensure that the scheduled migrations happen with minimal downtime and that the Google Cloud databases stay in sync with the on-premises data changes until the applications can cut over.\n\nWhat should you do? (Choose two.)","isMC":true,"question_id":38,"exam_id":5,"url":"https://www.examtopics.com/discussions/google/view/103701-exam-professional-cloud-database-engineer-topic-1-question/","discussion":[{"timestamp":"1695857880.0","poster":"dynamic_dba","content":"C, E.\nClassic use case for the Database Migration Service (C). E is effectively doing what the DMS does as well.","comment_id":"852572","upvote_count":"9"},{"content":"Selected Answer: CE\nCE correct answers!","timestamp":"1700666280.0","comment_id":"904071","poster":"KennyHuang","upvote_count":"5"},{"timestamp":"1727763240.0","poster":"nmnm22","comment_id":"1187235","upvote_count":"1","content":"goodluck to all"},{"poster":"PKookNN","content":"Selected Answer: CE\nThis should be CE","upvote_count":"1","timestamp":"1723199400.0","comment_id":"1145448"},{"timestamp":"1717925040.0","comment_id":"1091765","poster":"gienek_jarzynka","upvote_count":"3","content":"where are questions 133-136 ???"},{"comment_id":"848650","timestamp":"1695494340.0","poster":"EmiNard84","upvote_count":"5","content":"Sorry CE"}],"unix_timestamp":1679603940},{"id":"xcuNmgQ8yr0BDfUqlQCa","isMC":true,"answer":"A","answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/92020-exam-professional-cloud-database-engineer-topic-1-question/","question_text":"Your ecommerce website captures user clickstream data to analyze customer traffic patterns in real time and support personalization features on your website. You plan to analyze this data using big data tools. You need a low-latency solution that can store 8 TB of data and can scale to millions of read and write requests per second. What should you do?","timestamp":"2022-12-18 21:21:00","question_id":39,"choices":{"D":"Stream your data into BigQuery and use Dataproc and the BigQuery Storage API to analyze large volumes of data.","C":"Use Memorystore to handle your low-latency requirements and for real-time analytics.","B":"Deploy a Cloud SQL environment with read replicas for improved performance. Use Datastream to export data to Cloud Storage and analyze with Dataproc and the Cloud Storage connector.","A":"Write your data into Bigtable and use Dataproc and the Apache Hbase libraries for analysis."},"exam_id":5,"answer_ET":"A","topic":"1","answer_description":"","unix_timestamp":1671394860,"question_images":[],"discussion":[{"timestamp":"1694444040.0","content":"A.\nCloud SQL could not handle the load, so B is wrong. Memorystore can scale up to 300 GB. The question mentions needing 8 TB, so C must be wrong. BigQuery could not handle the latency requirements of the question, which leaves A. Bigtable could handle the volume of writes at the speeds required.","upvote_count":"7","comment_id":"836295","poster":"dynamic_dba"},{"comment_id":"755770","poster":"pk349","timestamp":"1687699260.0","content":"A: Write your data into Bigtable ***** and use Dataproc and the Apache Hbase libraries for analysis.","upvote_count":"6"},{"content":"Selected Answer: A\nBigtable is ideal for clickstream and IOT use cases, also it can process high performance read and writes globally.","upvote_count":"4","poster":"Jason_Cloud_at","comment_id":"1155261","timestamp":"1724210340.0"},{"timestamp":"1723711680.0","upvote_count":"2","comment_id":"1150898","content":"Selected Answer: D\nThis option uses BigQuery, that has a low latency and is a big data","poster":"ToniTovar"},{"upvote_count":"4","content":"Selected Answer: D\nA is not correct because Bigtable is not designed for real-time analytics. It is a good choice for storing and retrieving small amounts of data quickly, but it is not as efficient for analyzing large volumes of data.\nB is not correct because it cannot support Million of Read and Write\nC is not correct because of storage limitation\nD is correct","comment_id":"1063705","timestamp":"1714981680.0","poster":"VG1900"},{"timestamp":"1710747660.0","comments":[{"comment_id":"1093298","content":"Why opt A? It is not real-time, and the question mentions that they want to analysis, why not use bigquery, only 8 TB","poster":"ArtistS","timestamp":"1718090400.0","upvote_count":"1"}],"poster":"goodsport","upvote_count":"2","comment_id":"1010299","content":"Selected Answer: A\nI would opt for A."},{"content":"The correct answer is D. Stream your data into BigQuery and use Dataproc and the BigQuery Storage API to analyze large volumes of data.. A is used for NOSQL","poster":"learnazureportal","timestamp":"1710655980.0","comment_id":"1009545","upvote_count":"2"},{"content":"Selected Answer: A\nAt a high level, Bigtable is a NoSQL wide-column database. It's optimized for low latency, large numbers of reads and writes, and maintaining performance at scale. Bigtable use cases are of a certain scale or throughput with strict latency requirements, such as IoT, AdTech, FinTech, and so on. If high throughput and low latency at scale are not priorities for you, then another NoSQL database like Firestore might be a better fit.","upvote_count":"2","comment_id":"934185","poster":"CloudKida","timestamp":"1703584740.0"},{"comments":[{"poster":"Hilab","comment_id":"832511","upvote_count":"1","comments":[{"content":"The above answer is for Question #15, my mistake I put the comments here","comment_id":"832515","poster":"Hilab","upvote_count":"1","timestamp":"1694138700.0"}],"timestamp":"1694138400.0","content":"Normalization is a recommended best practice in database schema design, including in Cloud Spanner. It involves breaking down large tables into smaller, more manageable tables that are linked together by relationships. This can help reduce duplication of data and improve performance by reducing the amount of data that needs to be read or written to the database.\n\nPromoting high-cardinality attributes in multi-attribute primary keys is also recommended in Cloud Spanner schema design. High-cardinality attributes are those that have a large number of distinct values, such as product IDs or customer IDs. Including these attributes in the primary key can help distribute data more evenly across nodes, reducing the likelihood of hotspots.\n\nUsing an auto-incrementing value as the primary key or a bit-reverse sequential value as the primary key can result in hotspots, particularly if new data is being added at a high rate. These approaches can cause all new data to be inserted into a single node, leading to performance issues."}],"upvote_count":"1","comment_id":"832510","poster":"Hilab","timestamp":"1694138340.0","content":"B. Normalize the data model.\nD. Promote high-cardinality attributes in multi-attribute primary keys.\n\nWhen designing a schema for Cloud Spanner, it is important to follow best practices to avoid hotspots and ensure optimal performance. Hotspots occur when too many requests are targeted at a single node or group of nodes, causing them to become overloaded and potentially impacting performance."},{"comment_id":"832507","timestamp":"1694138040.0","content":"D. Stream your data into BigQuery and use Dataproc and the BigQuery Storage API to analyze large volumes of data.\n\nBigQuery is a fully managed, serverless data warehouse that allows you to store and analyze large datasets using SQL-like queries. It is designed to handle petabyte-scale data and is optimized for fast query performance. By streaming your clickstream data into BigQuery, you can store and process large amounts of data in real-time.\n\nDataproc, on the other hand, is a fully-managed cloud service for running Apache Hadoop and Spark clusters. It provides a managed, easy-to-use environment for data processing, which can be used to analyze the data stored in BigQuery.\n\nThe BigQuery Storage API allows you to directly access data stored in BigQuery from external applications, including Dataproc, which enables you to run advanced analytics on large volumes of data with low latency.\n\nThis approach provides a scalable, low-latency solution for storing and analyzing large volumes of data, making it a good fit for your requirements.","upvote_count":"1","poster":"Hilab"},{"upvote_count":"2","comment_id":"830203","content":"Selected Answer: A\nA. Write your data into Bigtable and use Dataproc and the Apache Hbase libraries for analysis. Most Voted","poster":"H_S","timestamp":"1693932960.0"},{"content":"Selected Answer: A\nA looks like best option.","comment_id":"828351","poster":"Nirca","timestamp":"1693764240.0","upvote_count":"2"},{"comment_id":"754707","timestamp":"1687577100.0","poster":"GCP72","content":"Selected Answer: A\nA is correct answer, C wouldn't be handled 8TB data\nScalable: Start with the lowest tier and smallest size and then grow your instance as needed. Memorystore provides automated scaling using APIs, and optimized node placement across zones for redundancy. Memorystore for Memcached can support clusters as large as 5 TB, enabling millions of QPS at very low latency","upvote_count":"3"},{"timestamp":"1687182240.0","upvote_count":"5","comment_id":"750000","content":"Answer is A. Click stream and time series data and the size is 8TB. Read low latency with reads and writes. Correct answer is A to use BigTable for storage and use either CBT or Hbase API to interact with data.","poster":"Kloudgeek"},{"timestamp":"1687179600.0","poster":"fredcaram","upvote_count":"3","content":"Selected Answer: A\nB couldn't handle this volume of writes and read, D wouldn't be able to handle the writing and C wouldn't be suited for this.","comment_id":"749945"},{"comment_id":"749153","timestamp":"1687112460.0","poster":"juancambb","upvote_count":"2","content":"Selected Answer: A\nmust be A"}],"answers_community":["A (77%)","D (23%)"]},{"id":"SwRF0gl0mRybxNR2bFas","topic":"1","isMC":true,"answer_description":"","answers_community":["DE (73%)","CE (18%)","6%"],"timestamp":"2022-12-19 16:11:00","unix_timestamp":1671462660,"question_text":"Your company uses Cloud Spanner for a mission-critical inventory management system that is globally available. You recently loaded stock keeping unit (SKU) and product catalog data from a company acquisition and observed hotspots in the Cloud Spanner database. You want to follow Google-recommended schema design practices to avoid performance degradation. What should you do? (Choose two.)","answer_images":[],"question_id":40,"answer_ET":"DE","discussion":[{"content":"Selected Answer: DE\nSpanner needs high cardinality primary key to avoid hotspotting.","timestamp":"1693232580.0","comment_id":"825024","poster":"PrtkKA","upvote_count":"7"},{"content":"Selected Answer: DE\nI would go with D and E","poster":"PKookNN","upvote_count":"3","comment_id":"1126636","timestamp":"1721382720.0"},{"timestamp":"1712020980.0","upvote_count":"3","poster":"juliorevk","comment_id":"1022720","content":"Selected Answer: DE\nhttps://cloud.google.com/spanner/docs/schema-design\n\nD because high cardinality means you have more unique values in the collumn. That's a good thing for a hot-spotting issue. \nE because Spanner specifically has this feature to reduce hot spotting. Basically, it generates unique values https://cloud.google.com/spanner/docs/schema-design#bit_reverse_primary_key"},{"timestamp":"1706509920.0","poster":"nescafe7","content":"Selected Answer: BD\nI agree with Hilab's comment below.","upvote_count":"2","comment_id":"966079"},{"upvote_count":"4","content":"Selected Answer: DE\nD and E, the docs are below.\nD: https://cloud.google.com/bigtable/docs/schema-design#row-keys-avoid\nE: https://cloud.google.com/spanner/docs/schema-design#bit_reverse_primary_key","timestamp":"1698322080.0","poster":"felipeschossler","comment_id":"881579"},{"timestamp":"1698269340.0","comment_id":"880879","content":"high-cardinality","poster":"Carpediem78","upvote_count":"1"},{"timestamp":"1696957740.0","comment_id":"866465","poster":"PATILDXB","upvote_count":"3","content":"Correct answers are D,E.\nRefer to the link which is self explanatory.\nhttps://cloud.google.com/spanner/docs/schema-design"},{"timestamp":"1695733380.0","upvote_count":"3","poster":"BenMS","content":"Selected Answer: DE\nA - incrementing values are an explicitly documented antipattern\nB - normalising the schema does not specifically address hotspotting\nC - low cardinality values in the primary key will also cause hotspotting\nD - promoting high cardinality values in the primary key (i.e. moving them nearer the front of the value) is a recommended approach to reduce hotspotting\nE - bit-reversed keys are an explicitly recommended best practice","comment_id":"851029"},{"comment_id":"836307","poster":"dynamic_dba","content":"D, E.\nA is wrong because that will promote hotspots. C is wrong because low cardinality attributes being part of the key (particularly at the front multi-attribute keys) will also promote hotspots. That makes D correct by definition. This leave B or D at the other correct answer. The fact the new data has already been added to the database suggests the data model is already properly normalized. In addition, one of the techniques to reduce or eliminate hotspots is to bit reverse sequential values. It’s in Google’s docs here:\nhttps://cloud.google.com/spanner/docs/schema-design","upvote_count":"3","timestamp":"1694445000.0"},{"comments":[{"upvote_count":"2","comment_id":"832517","timestamp":"1694138760.0","poster":"Hilab","content":"Normalization is a recommended best practice in database schema design, including in Cloud Spanner. It involves breaking down large tables into smaller, more manageable tables that are linked together by relationships. This can help reduce duplication of data and improve performance by reducing the amount of data that needs to be read or written to the database.\n\nPromoting high-cardinality attributes in multi-attribute primary keys is also recommended in Cloud Spanner schema design. High-cardinality attributes are those that have a large number of distinct values, such as product IDs or customer IDs. Including these attributes in the primary key can help distribute data more evenly across nodes, reducing the likelihood of hotspots.\n\nUsing an auto-incrementing value as the primary key or a bit-reverse sequential value as the primary key can result in hotspots, particularly if new data is being added at a high rate. These approaches can cause all new data to be inserted into a single node, leading to performance issues."}],"content":"B. Normalize the data model.\nD. Promote high-cardinality attributes in multi-attribute primary keys.\n\nWhen designing a schema for Cloud Spanner, it is important to follow best practices to avoid hotspots and ensure optimal performance. Hotspots occur when too many requests are targeted at a single node or group of nodes, causing them to become overloaded and potentially impacting performance.","timestamp":"1694138700.0","comment_id":"832516","upvote_count":"2","poster":"Hilab"},{"timestamp":"1693849260.0","comment_id":"829331","poster":"Nirca","content":"Selected Answer: DE\n\"hotspots\" in a database means that many IOPS (usually writes/updates) are happening on the same data-block; usually due to calling the same DATA. \nlow cardinality => same value in the column ==> hotspots. \nHigh cardinality => different values in the column ==> avoiding hotspotting.","upvote_count":"4"},{"poster":"zanhsieh","content":"Selected Answer: CE\nCE\nA and D: WRONG. Anti-pattern\nSince the question specifically stated the hotspots cause by new SKUs and product catalog data added, so the goal would be:\n1. The old data keeps distributed without any extra work needed.\n2. Resolving the new data hot spots problem.\nIt seems to me that SKU and product catalog are already normalized, so further normalize might touch the old data. This means B is out. If the new data already normalized, then it must have some high-cardinality attributes, e.g. SKU_id, and some low-cardinality attributes, e.g. category_id. So I picked low-cardinality attibutes in multi-attribute primary keys as C. I agreed with E as already Google recommended practice.\nReference:\nhttps://cloud.google.com/spanner/docs/schema-design","timestamp":"1691833560.0","comment_id":"806276","upvote_count":"3"},{"comment_id":"764108","content":"CE. Normalizing the data is not generally recommended if interleaving can suffice.","poster":"TFMV","timestamp":"1688332260.0","upvote_count":"1"},{"comment_id":"755769","upvote_count":"2","timestamp":"1687699200.0","poster":"pk349","content":"B: Normalize the data model.\n E: Use bit-reverse sequential value as the primary key.","comments":[{"timestamp":"1692409680.0","poster":"gabrielosluz","comment_id":"813714","content":"Wrong B","upvote_count":"1"}]},{"content":"Selected Answer: CE\nLooks CE is correct for me","timestamp":"1687579680.0","comment_id":"754716","upvote_count":"2","poster":"GCP72"},{"timestamp":"1687446540.0","content":"Selected Answer: CE\nCorrect answer - CE","poster":"jitu028","comment_id":"753511","upvote_count":"1"},{"poster":"Kloudgeek","timestamp":"1687182660.0","upvote_count":"1","comment_id":"750004","content":"Answer is B & E for schema design. https://cloud.google.com/spanner/docs/schema-design . B&E are correct answers"},{"content":"Selected Answer: BE\nA and D are anti-patterns","upvote_count":"1","comment_id":"749963","poster":"fredcaram","timestamp":"1687180260.0"}],"url":"https://www.examtopics.com/discussions/google/view/92092-exam-professional-cloud-database-engineer-topic-1-question/","choices":{"D":"Promote high-cardinality attributes in multi-attribute primary keys.","C":"Promote low-cardinality attributes in multi-attribute primary keys.","B":"Normalize the data model.","E":"Use bit-reverse sequential value as the primary key.","A":"Use an auto-incrementing value as the primary key."},"exam_id":5,"question_images":[],"answer":"DE"}],"exam":{"lastUpdated":"11 Apr 2025","isBeta":false,"provider":"Google","numberOfQuestions":132,"isImplemented":true,"id":5,"isMCOnly":true,"name":"Professional Cloud Database Engineer"},"currentPage":8},"__N_SSP":true}