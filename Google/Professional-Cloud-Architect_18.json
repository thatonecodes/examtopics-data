{"pageProps":{"questions":[{"id":"lOxFgy0h6BhsCnTRHJrl","exam_id":4,"question_text":"Your company has an application that is running on multiple instances of Compute Engine. It generates 1 TB per day of logs. For compliance reasons, the logs need to be kept for at least two years. The logs need to be available for active query for 30 days. After that, they just need to be retained for audit purposes. You want to implement a storage solution that is compliant, minimizes costs, and follows Google-recommended practices. What should you do?","answer_ET":"A","answer_images":[],"topic":"1","unix_timestamp":1640658900,"timestamp":"2021-12-28 03:35:00","discussion":[{"timestamp":"1656926760.0","comment_id":"516516","upvote_count":"24","poster":"gggsrs","comments":[{"comment_id":"516517","timestamp":"1656926820.0","poster":"gggsrs","upvote_count":"4","content":"https://cloud.google.com/logging/docs/agent/logging/installation\nhttps://cloud.google.com/logging/docs/export/configure_export_v2\nhttps://cloud.google.com/bigquery/external-data-cloud-storage"}],"content":"The answer is A.\n\nThe practice for managing logs generated on Compute Engine on Google Cloud is to install the Cloud Logging agent and send them to Cloud Logging.\n\nThe sent logs will be aggregated into a Cloud Logging sink and exported to Cloud Storage.\nThe reason for using Cloud Storage as the destination for the logs is that the requirement in question requires setting up a lifecycle based on the storage period.\nIn this case, the log will be used for active queries for 30 days after it is saved, but after that, it needs to be stored for a longer period of time for auditing purposes.\n\nIf the data is to be used for active queries, we can use BigQuery's Cloud Storage data query feature and move the data past 30 days to Coldline to build a cost-optimal solution.\n\nTherefore, the correct answer is as follows\n1. Install the Cloud Logging agent on all instances.\nCreate a sync that exports the logs to the region's Cloud Storage bucket.\n3. Create an Object Lifecycle rule to move the files to the Coldline Cloud Storage bucket after one month. 4.\n4. set up a bucket-level retention policy using bucket locking.\""},{"comments":[{"comment_id":"1203957","timestamp":"1730204820.0","poster":"Gino17m","upvote_count":"1","content":"The question comes from the times when there was no Archive storage class in GCS yet"}],"timestamp":"1713504960.0","poster":"SANTHEDAN","upvote_count":"5","comment_id":"1047569","content":"None of the options are correct:\n\nA - It should be archive (>= 365 days) and not coldline ( >= 90 days) - Proposed solution is more expensive that what is possible. Also no way to query unless you use BigQuery external tables.\nB & C - Wrong because CRON is not the way to do this.\nD - Wrong because data is deleted after 30 days and not retained for 2 years."},{"timestamp":"1736003280.0","upvote_count":"1","poster":"plumbig11","comment_id":"1336419","content":"Selected Answer: A\nThe best option is cloud logging, with lifecycle rule and as this have to be kept for at least two year you should use cloud storage and the most important part is create the correct retention policy for it;"},{"upvote_count":"2","comment_id":"1154765","content":"Selected Answer: A\nNo need to look at any other options.","timestamp":"1724155500.0","poster":"Tirthankar17"},{"content":"Answer A is misleading/confusing: \"3. Create an Object Lifecycle rule to move files into a Coldline Cloud Storage bucket after one month.\" \n\nA lifecycle rule will NOT move files in another bucket (coldline bucket etc). It will just change the storage class of the file.","comment_id":"995441","poster":"devnul","timestamp":"1709241960.0","upvote_count":"1"},{"timestamp":"1692779640.0","upvote_count":"1","poster":"someCloudUser","content":"Selected Answer: A\nA is correct","comment_id":"819067"},{"poster":"someCloudUser","timestamp":"1692533880.0","upvote_count":"1","content":"Selected Answer: A\nA is correct.","comment_id":"815355"},{"content":"A. 1. Install a Cloud Logging agent on all instances. 2. Create a sink to export logs into a regional Cloud Storage bucket. 3. Create an Object Lifecycle rule to move files into a Coldline Cloud Storage bucket after one month. 4. Configure a retention policy at the bucket level using bucket lock.\n\nThis approach would allow you to use Cloud Logging to collect and export the logs from the Compute Engine instances into a Cloud Storage bucket. You can then use an Object Lifecycle rule to automatically move the logs from the regional bucket to a Coldline bucket after one month, which will reduce storage costs for logs that are not actively being queried. By configuring a retention policy using bucket lock, you can ensure that the logs are retained for at least two years for audit purposes. This approach follows Google-recommended practices for storing logs and minimizing costs.","comment_id":"757298","poster":"omermahgoub","timestamp":"1687764000.0","upvote_count":"2"},{"content":"Selected Answer: A\nA Is the Correct Answer","poster":"surajkrishnamurthy","timestamp":"1686903780.0","upvote_count":"1","comment_id":"747046"},{"comment_id":"733832","poster":"habros","content":"Selected Answer: A\nA is perfect answer… the rest doesn’t sound rational","timestamp":"1685709900.0","upvote_count":"1"},{"poster":"megumin","timestamp":"1684389840.0","comment_id":"721157","upvote_count":"1","content":"Selected Answer: A\nA is ok"},{"poster":"Mahmoud_E","content":"Selected Answer: A\nA is the correct answer","comment_id":"699474","upvote_count":"1","timestamp":"1681954860.0"},{"upvote_count":"2","content":"I agree with A, There is no need of BigQuery.","comment_id":"695632","timestamp":"1681585380.0","poster":"AzureDP900"},{"comment_id":"693844","timestamp":"1681386120.0","content":"Selected Answer: A\nA is fine","upvote_count":"1","poster":"manis68"},{"content":"For compliance reasons, the logs need to be kept for at least two years... In Bigquery time partitioned after 30 days ..how the logs be present for 2 years ..Hence going with A","poster":"Imran109","comment_id":"646087","upvote_count":"1","timestamp":"1676264460.0"},{"timestamp":"1675593600.0","upvote_count":"1","content":"Selected Answer: A\nA is a no-brainer","comment_id":"642850","poster":"harutheorochimaru"},{"poster":"exam9391","content":"Selected Answer: A\nA is ok","comment_id":"634456","timestamp":"1674294360.0","upvote_count":"1"},{"upvote_count":"2","timestamp":"1670850840.0","poster":"H_S","content":"Selected Answer: A\nwhen a partition expires, the data in the partition is no longer available","comment_id":"615276"},{"content":"Selected Answer: A\nfor big query, when a partition expires, the data in the partition is no longer available for querying. BigQuery will eventually deletes the expired partition which doesn't meet the requirement described.","poster":"shasha_zhang","upvote_count":"4","timestamp":"1667360100.0","comment_id":"595805"},{"poster":"cloudmon","comment_id":"582572","timestamp":"1665166980.0","upvote_count":"3","content":"Selected Answer: A\nThe answer is A."},{"content":"Selected Answer: A\nVoting for A","timestamp":"1664597820.0","comment_id":"579317","upvote_count":"3","poster":"sergaebi"},{"comment_id":"548073","upvote_count":"2","poster":"azureaspirant","comments":[{"content":"Is it 2/15/22?","upvote_count":"2","comment_id":"571605","poster":"SCVinod","timestamp":"1663672200.0"}],"content":"2/15/21 exam","timestamp":"1660591740.0"},{"poster":"blk_rook","upvote_count":"4","comment_id":"518540","timestamp":"1657130160.0","content":"Selected Answer: A\nA meets all requirements"},{"upvote_count":"2","comment_id":"518256","timestamp":"1657107720.0","content":"Go for A","poster":"AJapieGuru"},{"poster":"Fotofilico","comment_id":"517106","upvote_count":"3","content":"Selected Answer: A\nA meets all the requirements and specifies the detail for the 2 years retentiont","timestamp":"1656978060.0"},{"upvote_count":"2","timestamp":"1656835380.0","content":"Selected Answer: A\nCorrect Answer is A","comment_id":"515591","poster":"GauravLahoti"},{"content":"A. 1. Install a Cloud Logging agent on all instances. 2. Create a sink to export logs into a regional Cloud Storage bucket. 3. Create an Object Lifecycle rule to move files into a Coldline Cloud Storage bucket after one month. 4. Configure a retention policy at the bucket level using bucket lock.","upvote_count":"1","poster":"victory108","timestamp":"1656644040.0","comment_id":"514365"},{"content":"In question it is mentioned that \" logs need to be available for active query for 30 days\". Because of active query Big query based solution is require. Therefore Option C.","comment_id":"513238","upvote_count":"1","poster":"Pravin269","timestamp":"1656577320.0"},{"comments":[{"upvote_count":"2","comment_id":"526906","poster":"Wonka","timestamp":"1658161020.0","content":"and where does this option mention about retention policy on coldline?"}],"timestamp":"1656504300.0","content":"Selected Answer: B\nCorrect answer is B. It need to be moved to coldline as after 30 days it need to be used only rarely","comment_id":"512236","poster":"menon_sarath","upvote_count":"1"},{"comment_id":"511058","poster":"edilramos","content":"Selected Answer: A\nI think the correct answer is A.\nBecause in the case of C, the 2-year retention solution is not mentioned.","timestamp":"1656410160.0","upvote_count":"3"},{"upvote_count":"2","poster":"simbu1299","comment_id":"510985","content":"Correct answer is A","timestamp":"1656403980.0"},{"timestamp":"1656400620.0","comment_id":"510937","poster":"Mikelala31","upvote_count":"1","content":"I think is A"},{"comment_id":"510749","poster":"StelSen","content":"Option-A is correct and met all the requirements.","upvote_count":"1","timestamp":"1656376500.0"}],"choices":{"D":"1. Create a daily cron job, running on all instances, that uploads logs into a partitioned BigQuery table. 2. Set a time_partitioning_expiration of 30 days.","B":"1. Write a daily cron job, running on all instances, that uploads logs into a Cloud Storage bucket. 2. Create a sink to export logs into a regional Cloud Storage bucket. 3. Create an Object Lifecycle rule to move files into a Coldline Cloud Storage bucket after one month.","C":"1. Install a Cloud Logging agent on all instances. 2. Create a sink to export logs into a partitioned BigQuery table. 3. Set a time_partitioning_expiration of 30 days.","A":"1. Install a Cloud Logging agent on all instances. 2. Create a sink to export logs into a regional Cloud Storage bucket. 3. Create an Object Lifecycle rule to move files into a Coldline Cloud Storage bucket after one month. 4. Configure a retention policy at the bucket level using bucket lock."},"url":"https://www.examtopics.com/discussions/google/view/68688-exam-professional-cloud-architect-topic-1-question-176/","answers_community":["A (97%)","3%"],"isMC":true,"answer_description":"","question_id":86,"question_images":[],"answer":"A"},{"id":"y7kdk48FceppuSHfp5Hu","url":"https://www.examtopics.com/discussions/google/view/68690-exam-professional-cloud-architect-topic-1-question-177/","answer_images":[],"timestamp":"2021-12-28 03:38:00","discussion":[{"comment_id":"596741","content":"LOL , if we give this question to someone who know nothing about GCP they will select A","upvote_count":"18","timestamp":"1667568480.0","poster":"kimharsh"},{"poster":"Fotofilico","timestamp":"1656978060.0","upvote_count":"16","content":"Selected Answer: A\nhttps://cloud.google.com/resource-manager/docs/organization-policy/restricting-domains","comment_id":"517107"},{"poster":"Tirthankar17","content":"Whoever wrote option D was high af.","comment_id":"1153141","upvote_count":"4","timestamp":"1723965360.0"},{"comment_id":"1086822","timestamp":"1717412580.0","content":"D is Ridiculous.","poster":"Jconnor","upvote_count":"1"},{"upvote_count":"1","poster":"Atanu","comment_id":"910820","content":"Selected Answer: A\nOption D is just to create confusion only.","timestamp":"1701328800.0"},{"comment_id":"834369","content":"Selected Answer: A\nThe correct answer is: A. Configure an organization policy to restrict identities by domain.\n\nThis solution will allow the security team to secure projects that will be part of the Organization by prohibiting IAM users outside the domain from gaining permissions.\n\nThe other options are not as efficient or effective. Option B would not be efficient, as it would block the creation of all service accounts, which are necessary for some applications. Option C would not be effective, as it would not prevent IAM users from gaining permissions, as it would only remove users that do not belong to the Cloud Identity domain from all projects. Option D would not be efficient, as it would require a Compute Engine instance to be created and a cron job to be configured, which would add complexity and cost to the solution.","timestamp":"1694283000.0","poster":"CGS22","upvote_count":"2"},{"upvote_count":"1","comment_id":"819069","poster":"someCloudUser","content":"Selected Answer: A\nA is correct","timestamp":"1692779760.0"},{"timestamp":"1687764120.0","comment_id":"757300","poster":"omermahgoub","content":"The security team should configure an organization policy to restrict identities by domain. This will allow them to specify a list of allowed domains, and prevent users from outside those domains from gaining permissions in the Organization.\n\nAlternatively, the security team could configure an organization policy to block creation of service accounts. This would prevent the creation of new service accounts, which could be used to grant permissions to users outside the domain.\n\nThe other options are not recommended. Option C involves manually removing users every hour, which could be time-consuming and error-prone. Option D involves creating a technical user and writing a bash script to delete users, which is not a recommended approach. It would be more secure and efficient to use an organization policy to restrict identities by domain.","upvote_count":"2"},{"comment_id":"747053","poster":"surajkrishnamurthy","content":"Selected Answer: A\nA Is the Correct Answer","timestamp":"1686904140.0","upvote_count":"1"},{"poster":"megumin","comment_id":"721160","timestamp":"1684390080.0","content":"Selected Answer: A\nA is ok","upvote_count":"1"},{"poster":"Mahmoud_E","comment_id":"699477","content":"Selected Answer: A\nA is the correct answer\nhttps://cloud.google.com/resource-manager/docs/organization-policy/restricting-domains","timestamp":"1681954920.0","upvote_count":"1"},{"timestamp":"1681585620.0","upvote_count":"1","content":"A is right","comment_id":"695637","poster":"AzureDP900"},{"upvote_count":"1","comment_id":"693846","poster":"manis68","content":"Selected Answer: A\nA is OK","timestamp":"1681386300.0"},{"content":"Selected Answer: A\nA is ok","upvote_count":"3","comment_id":"634460","timestamp":"1674294480.0","poster":"exam9391"},{"content":"2/15/21 exam","upvote_count":"5","poster":"azureaspirant","timestamp":"1660591740.0","comment_id":"548074"},{"upvote_count":"3","timestamp":"1657130460.0","content":"Selected Answer: A\nmust restrict the access, not clean up every hour. see reference from Fotofilico","comment_id":"518543","poster":"blk_rook"},{"comment_id":"518262","timestamp":"1657108140.0","upvote_count":"1","content":"Go for A","poster":"AJapieGuru"},{"comment_id":"515594","timestamp":"1656835500.0","content":"Correct Answer is A","poster":"GauravLahoti","upvote_count":"2"},{"poster":"brushek","upvote_count":"1","comment_id":"515224","timestamp":"1656788700.0","content":"Selected Answer: A\nvote A"},{"poster":"victory108","upvote_count":"1","comment_id":"514364","timestamp":"1656643860.0","content":"A. Configure an organization policy to restrict identities by domain."},{"timestamp":"1656560040.0","content":"\"A\" is way to go.","upvote_count":"1","comment_id":"513898","poster":"GMats"},{"content":"Correct answer definitely seems ot be A","timestamp":"1656504480.0","comment_id":"512238","poster":"menon_sarath","upvote_count":"1"},{"comment_id":"511905","poster":"spoxman","content":"Selected Answer: A\nC and D are nonsense because of \"every hour\". Why not 2 hours or every minute?\nB will not help at all.","timestamp":"1656479880.0","upvote_count":"4"},{"upvote_count":"2","content":"Correct Ans is A","timestamp":"1656435840.0","poster":"ACasper","comment_id":"511500"},{"comment_id":"510988","timestamp":"1656404040.0","upvote_count":"2","poster":"simbu1299","content":"Correct answer is A\nhttps://cloud.google.com/resource-manager/docs/organization-policy/restricting-domains#:~:text=The%20Resource%20Manager%20provides%20a,Identity%20and%20Access%20Management%20policies."},{"timestamp":"1656400500.0","content":"I think is A,","upvote_count":"2","poster":"Mikelala31","comment_id":"510934"},{"poster":"StelSen","upvote_count":"1","content":"Option-A is correct. https://cloud.google.com/resource-manager/docs/organization-policy/restricting-domains","comment_id":"510752","timestamp":"1656376680.0"}],"question_text":"Your company has just recently activated Cloud Identity to manage users. The Google Cloud Organization has been configured as well. The security team needs to secure projects that will be part of the Organization. They want to prohibit IAM users outside the domain from gaining permissions from now on. What should they do?","unix_timestamp":1640659080,"answer":"A","topic":"1","answers_community":["A (100%)"],"question_images":[],"exam_id":4,"answer_ET":"A","answer_description":"","choices":{"B":"Configure an organization policy to block creation of service accounts.","A":"Configure an organization policy to restrict identities by domain.","D":"Create a technical user (e.g., crawler@yourdomain.com), and give it the project owner role at root organization level. Write a bash script that: ג€¢ Lists all the IAM rules of all projects within the organization. ג€¢ Deletes all users that do not belong to the company domain. Create a Compute Engine instance in a project within the Organization and configure gcloud to be executed with technical user credentials. Configure a cron job that executes the bash script every hour.","C":"Configure Cloud Scheduler to trigger a Cloud Function every hour that removes all users that don't belong to the Cloud Identity domain from all projects."},"isMC":true,"question_id":87},{"id":"5rrWTiOAbi5ERg2lOc25","answer_description":"","question_id":88,"discussion":[{"comment_id":"510753","content":"Option-C is correct: https://cloud.google.com/bigtable/docs/schema-design#row-keys","upvote_count":"13","poster":"StelSen","timestamp":"1687913040.0"},{"timestamp":"1719386700.0","poster":"omermahgoub","upvote_count":"9","content":"C. Review your RowKey strategy and ensure that keys are evenly spread across the alphabet.\n\nThe RowKey is used to sort data within a Cloud Bigtable cluster. If the keys are not evenly spread across the alphabet, it can result in a hotspot and slow down queries. To prevent this from happening in the future, you should review your RowKey strategy and ensure that keys are evenly spread across the alphabet. This will help to distribute the data evenly across the cluster and improve query performance. Other potential solutions to consider include adding more nodes to the cluster or optimizing your query patterns. However, deleting records older than 30 days or advising clients to use HBase APIs instead of NodeJS APIs would not address the issue of a hotspot in the cluster.","comment_id":"757301"},{"timestamp":"1725188880.0","comment_id":"825854","poster":"AugustoKras011111","content":"Selected Answer: C\nC in the only answer that make sense.","upvote_count":"1"},{"poster":"surajkrishnamurthy","upvote_count":"1","timestamp":"1718526660.0","content":"Selected Answer: C\nC Is the Correct Answer","comment_id":"747054"},{"content":"Selected Answer: C\nC\nhttps://cloud.google.com/bigtable/docs/overview#load-balancing","comment_id":"743141","upvote_count":"1","poster":"gonlafer","timestamp":"1718208180.0"},{"content":"Selected Answer: C\nC is ok","timestamp":"1716012600.0","comment_id":"721164","poster":"megumin","upvote_count":"1"},{"upvote_count":"1","poster":"Mahmoud_E","content":"Selected Answer: C\nC is the right answer","timestamp":"1713897120.0","comment_id":"702388"},{"poster":"AzureDP900","content":"C is better option","upvote_count":"1","comment_id":"695639","timestamp":"1713208200.0"},{"poster":"Jeyakumar","upvote_count":"1","content":"Option-C is the better one.","timestamp":"1706852580.0","comment_id":"640997"},{"upvote_count":"2","content":"The issue described is with \"querying\" meaning reading. Not writing. C: distributing across the Alphabet is good for Writing.","poster":"Nirca","timestamp":"1704819780.0","comment_id":"629222"},{"comment_id":"603126","upvote_count":"8","poster":"JoeyCASD","timestamp":"1700282700.0","content":"Suggest to study the following reference, it's important to design the row key pattern in Bigtable.\nhttps://cloud.google.com/bigtable/docs/overview#architecture\nhttps://cloud.google.com/bigtable/docs/overview#load-balancing"},{"content":"C looks good, I dont think we have to control number of nodes in Big table","poster":"AjayPrajapati","timestamp":"1689020160.0","comment_id":"521169","upvote_count":"3"},{"upvote_count":"2","comment_id":"518265","poster":"AJapieGuru","content":"Vote C","timestamp":"1688644200.0"},{"timestamp":"1688179800.0","upvote_count":"1","comment_id":"514363","poster":"victory108","content":"C. Review your RowKey strategy and ensure that keys are evenly spread across the alphabet."},{"timestamp":"1688102280.0","poster":"spoxman","upvote_count":"1","comment_id":"513094","content":"Selected Answer: C\ncorrect row-key strategy improves performance"},{"upvote_count":"2","poster":"GMats","content":"C is answer.Hot key/partitions are created due to improper row key design.","timestamp":"1688095800.0","comment_id":"513896"},{"poster":"ACasper","content":"I think the ans is D","timestamp":"1687971960.0","upvote_count":"1","comment_id":"511501"}],"answer_images":[],"unix_timestamp":1640659440,"answer_ET":"C","url":"https://www.examtopics.com/discussions/google/view/68691-exam-professional-cloud-architect-topic-1-question-178/","answer":"C","question_text":"Your company has an application running on Google Cloud that is collecting data from thousands of physical devices that are globally distributed. Data is published to Pub/Sub and streamed in real time into an SSD Cloud Bigtable cluster via a Dataflow pipeline. The operations team informs you that your Cloud\nBigtable cluster has a hotspot, and queries are taking longer than expected. You need to resolve the problem and prevent it from happening in the future. What should you do?","exam_id":4,"answers_community":["C (100%)"],"question_images":[],"choices":{"B":"Delete records older than 30 days.","C":"Review your RowKey strategy and ensure that keys are evenly spread across the alphabet.","D":"Double the number of nodes you currently have.","A":"Advise your clients to use HBase APIs instead of NodeJS APIs."},"topic":"1","timestamp":"2021-12-28 03:44:00","isMC":true},{"id":"e7VeNwKUNLhdaLAWVP3i","isMC":true,"question_text":"Your company has a Google Cloud project that uses BigQuery for data warehousing. There are some tables that contain personally identifiable information (PII).\nOnly the compliance team may access the PII. The other information in the tables must be available to the data science team. You want to minimize cost and the time it takes to assign appropriate access to the tables. What should you do?","question_images":[],"topic":"1","timestamp":"2021-12-28 03:49:00","exam_id":4,"answer_description":"","question_id":89,"answer":"C","answer_ET":"C","answers_community":["C (63%)","A (32%)","5%"],"unix_timestamp":1640659740,"url":"https://www.examtopics.com/discussions/google/view/68692-exam-professional-cloud-architect-topic-1-question-179/","discussion":[{"content":"Selected Answer: A\nMaterialized view is too costly for the requirement. So B & D is out.\n\nTo protect PII, there is no need to create another dataset. Creating a view on the original dataset should be sufficient. In addition, according to https://cloud.google.com/bigquery/docs/view-access-controls, view access can be granted at the 'dataset' level.","poster":"[Removed]","timestamp":"1652621940.0","comments":[{"timestamp":"1668972960.0","content":"Sorry I mean C","upvote_count":"4","comment_id":"722937","poster":"ashrafh"},{"content":"Authorized views should be created in a different dataset from the source data. That way, data owners can give users access to the authorized view without simultaneously granting access to the underlying data.","upvote_count":"8","comment_id":"672314","timestamp":"1663502520.0","poster":"zellck"},{"comment_id":"722936","timestamp":"1668972840.0","content":"A is correct \n\nGiving a view access to a dataset is also known as creating an authorized view in BigQuery. An authorized view lets you share query results with particular users and groups without giving them access to the underlying tables. You can also use the view's SQL query to restrict the columns (fields) the users are able to query. In this tutorial, you create an authorized view.\n\nhttps://cloud.google.com/bigquery/docs/share-access-views","upvote_count":"4","poster":"ashrafh"}],"comment_id":"602105","upvote_count":"17"},{"timestamp":"1641228960.0","content":"Selected Answer: C\nC is correct here. You need view to avoid PII data. So materialized view is not needed.","upvote_count":"14","comment_id":"515921","poster":"[Removed]","comments":[{"content":"also can't query data from a view, so A not.","comment_id":"700251","upvote_count":"1","timestamp":"1666294560.0","poster":"melono"},{"poster":"melono","upvote_count":"1","timestamp":"1666295100.0","content":"https://cloud.google.com/bigquery/docs/share-access-views","comment_id":"700255"}]},{"timestamp":"1736004840.0","poster":"plumbig11","comment_id":"1336424","upvote_count":"2","content":"Selected Answer: C\nYou should create a dataset for the data science team and use view on the original dataset. \nNO materialized view because isn't cost effective."},{"comment_id":"1284212","poster":"192dcc7","content":"Devil is in the details. Question mentioned table\"s\". Which mean we have more than 1 table which will result in more than 1 view. You can authorize each view on \"same\" dataset or group all view\"s\" in a dataset then authorize the dataset. This does not answer the question (may be wording) but this is the concept behind the question.","upvote_count":"1","timestamp":"1726416900.0"},{"comment_id":"998224","upvote_count":"9","timestamp":"1693803780.0","comments":[{"upvote_count":"1","poster":"cchiaramelli","content":"Yes, good source","comment_id":"1056249","timestamp":"1698502620.0"}],"poster":"someone2011","content":"C: view in a different dataset (https://cloud.google.com/bigquery/docs/share-access-views: \"Authorized views should be created in a different dataset from the source data. That way, data owners can give users access to the authorized view without simultaneously granting access to the underlying data.\")"},{"comment_id":"973579","content":"Selected Answer: C\nI went with C. A will prevent data scientists from viewing PII in the view, it doesn't stop them from viewing it in the table however.","timestamp":"1691304000.0","poster":"Mournes","upvote_count":"7"},{"poster":"gary_cooper","comment_id":"954421","content":"Selected Answer: C\nhttps://cloud.google.com/bigquery/docs/share-access-views#create_a_dataset_where_you_can_store_your_view","upvote_count":"2","timestamp":"1689613860.0"},{"poster":"red_panda","upvote_count":"1","content":"Selected Answer: A\nIt's no needed to create a new dataset and for sure is not cost-effective. A is best option","comment_id":"949057","timestamp":"1689087180.0"},{"comment_id":"943278","content":"It should be option A the question states minimize cost and time, option C though has better security requires additional step.","upvote_count":"1","timestamp":"1688529240.0","poster":"Umesh09"},{"comment_id":"862694","content":"Selected Answer: C\nOption A is not the best choice because it doesn't involve creating a separate dataset for the data science team. Creating a separate dataset provides better organization and access control management for different teams.\n\nIn option C, you create a separate dataset specifically for the data science team and then create views that exclude PII. This allows for more granular access controls and a better separation of concerns. By authorizing the view to access the source dataset, you ensure that the data science team can only access the non-PII data through the views, maintaining privacy and compliance.","timestamp":"1680757740.0","poster":"JC0926","upvote_count":"3"},{"poster":"JC0926","timestamp":"1679645820.0","comment_id":"849109","comments":[{"content":"Wrong , because you then have to give access at view level to prevent spillover , C is better.\nAlso, creating another dataset costs nothing, BQ charges for data stored and processed only, you can have a thousand datasets, it won't matter","timestamp":"1722028980.0","comment_id":"1255904","upvote_count":"1","poster":"Toothpick"}],"upvote_count":"1","content":"Selected Answer: A\nOption C are not appropriate because creating a new dataset is not necessary in this scenario. Creating views of the tables that exclude PII is a simpler and more cost-effective solution. Additionally, authorizing the view to access the source dataset is not necessary because the view already contains the relevant data."},{"poster":"BeCalm","upvote_count":"1","comment_id":"836607","timestamp":"1678578360.0","content":"C = A + One additional step to create the dataset which is not necessary so the answer is A"},{"upvote_count":"1","timestamp":"1678578240.0","poster":"BeCalm","comment_id":"836605","content":"Selected Answer: A\nNo need for materialized view which is an operational overhead."},{"timestamp":"1678393320.0","upvote_count":"1","comment_id":"834378","content":"Selected Answer: A\nA. From the dataset where you have the source data, create views of tables that you want to share, excluding PII. 2. Assign an appropriate project-level IAM role to the members of the data science team. 3. Assign access controls to the dataset that contains the view. This solution will minimize cost and the time it takes to assign appropriate access to the tables. The other options are not as efficient or effective.","poster":"CGS22"},{"timestamp":"1677676320.0","content":"Selected Answer: C\nI vote for C. Option C provides better security option.","upvote_count":"3","comment_id":"825863","poster":"AugustoKras011111"},{"poster":"szagarella","comment_id":"814439","upvote_count":"1","timestamp":"1676835240.0","content":"Selected Answer: A\nA is my answer."},{"content":"Selected Answer: C\nAgree with C from the link with google best practice\nhttps://cloud.google.com/bigquery/docs/share-access-views#create_a_dataset_where_you_can_store_your_view\nCreate a dataset where you can store your view\nAfter creating your source dataset, you create a new, separate dataset to store the authorized view that you share with your data analysts. In a later step, you grant the authorized view access to the data in the source dataset. Your data analysts then have access to the authorized view, but not direct access to the source data.","poster":"telp","comment_id":"809803","timestamp":"1676482440.0","upvote_count":"6"},{"content":"Selected Answer: A\nMaterialized views costs more than normal ones. Creating a new dataset is not cost-effective. You can use authorized views to restrict data access.\nFrom Google doc:\n\nGiving a view access to a dataset is also known as creating an authorized view in BigQuery. An authorized view lets you share query results with particular users and groups without giving them access to the underlying tables. You can also use the view's SQL query to restrict the columns (fields) the users are able to query.\n\n\nUsers need the bigquery.tables.getData permission on all tables and views that their query references. In addition, when querying a view users need this permission on all underlying tables and views. However, if you are using authorized views or authorized datasets, you don't need to give users access to the underlying source data.\n\nReference:\n\nhttps://cloud.google.com/bigquery/docs/share-access-views\n\nhttps://cloud.google.com/bigquery/docs/table-access-controls#required_permission_to_query_tables_and_views","timestamp":"1675986660.0","comment_id":"803805","poster":"Jeena345","upvote_count":"2"},{"timestamp":"1666550100.0","upvote_count":"5","poster":"Mahmoud_E","content":"Selected Answer: C\nC is the correct answer https://cloud.google.com/bigquery/docs/share-access-views#create_a_dataset_where_you_can_store_your_view","comment_id":"702393"},{"content":"C is right","comment_id":"695644","poster":"AzureDP900","timestamp":"1665861360.0","upvote_count":"2"},{"upvote_count":"1","comment_id":"675701","content":"Selected Answer: C\nhttps://cloud.google.com/bigquery/docs/share-access-views#create_a_dataset_where_you_can_store_your_view","timestamp":"1663816920.0","poster":"ppandey96"},{"comments":[{"timestamp":"1663166460.0","comment_id":"669115","content":"Authorized views should be created in a different dataset from the source data. That way, data owners can give users access to the authorized view without simultaneously granting access to the underlying data.","poster":"zellck","upvote_count":"2"}],"comment_id":"669113","timestamp":"1663166460.0","poster":"zellck","upvote_count":"2","content":"Selected Answer: C\nAnswer is C. You need to create a new dataset based on a query from the source dataset first before you can create the authorised view.\n\nhttps://cloud.google.com/bigquery/docs/share-access-views#create_a_dataset_where_you_can_store_your_view\n\nAfter creating your source dataset, you create a new, separate dataset to store the authorized view that you share with your data analysts. In a later step, you grant the authorized view access to the data in the source dataset. Your data analysts then have access to the authorized view, but not direct access to the source data."},{"comments":[{"content":"Thank you Kiappy to share the video. That video clearly confirms the right answer is C","timestamp":"1666347180.0","poster":"GordonLeo","upvote_count":"1","comment_id":"700732"}],"content":"Selected Answer: C\nif you watch this video there is a useful explanation of authorized views that, IMHO, matches with the question. https://cloud.google.com/bigquery/docs/authorized-views","comment_id":"668912","timestamp":"1663153080.0","upvote_count":"3","poster":"kiappy81"},{"poster":"alexandercamachop","content":"Selected Answer: A\nRequirements: \"Cost and Time\"\nMaterialized views are very expensive. Therefor, B / D discarted.\nBetween A & C: \nC works but creates a new dataset, hence we need time / cost for the new dataset, which is not needed.\nA seems like the best answer.","upvote_count":"1","timestamp":"1662938280.0","comment_id":"666501"},{"timestamp":"1662748500.0","content":"Selected Answer: C\nhttps://cloud.google.com/bigquery/docs/share-access-views","comment_id":"664874","upvote_count":"3","poster":"bossdellacert"},{"poster":"certifiedfra","upvote_count":"2","comment_id":"646323","comments":[{"content":"More details from https://cloud.google.com/bigquery/docs/table-access-controls#required_permission_to_query_tables_and_views :\n\n\"Users need the bigquery.tables.getData permission on all tables and views that their query references. In addition, when querying a view users need this permission on all underlying tables and views. However, if you are using authorized views or authorized datasets, you don't need to give users access to the underlying source data.\"","poster":"certifiedfra","timestamp":"1660393980.0","upvote_count":"2","comment_id":"646328"},{"timestamp":"1662647100.0","comment_id":"663696","upvote_count":"2","content":"very useful info. thanks for this","poster":"kuboraam"}],"timestamp":"1660393740.0","content":"Selected Answer: A\nI think it's A. Materialized views costs more than normal ones. Creating a new dataset is not cost-effective as you can use AUTHORIZED VIEWS to restrict data acces.\nGoogle doc:\nGiving a view access to a dataset is also known as creating an authorized view in BigQuery. An authorized view lets you share query results with particular users and groups without giving them access to the underlying tables. You can also use the view's SQL query to restrict the columns (fields) the users are able to query.\nReference: https://cloud.google.com/bigquery/docs/share-access-views"},{"upvote_count":"1","comment_id":"631004","poster":"szefco","timestamp":"1657734120.0","content":"Selected Answer: C\nC is correct. We want to avoid granting access directly on tables (even though it's possible, it's just not necessary work :) )\nCreate new dataset for data science, create views in that dataset (excluding pii columns) grant access to Data Science Team to the whole dataset and grant Authorized Views."},{"comment_id":"618599","upvote_count":"3","timestamp":"1655629560.0","content":"Selected Answer: A\nYou want to minimize cost and the time it takes to assign appropriate access to the tables.","poster":"kapara"},{"content":"Selected Answer: C\nIt's definitely C. \nhttps://cloud.google.com/bigquery/docs/share-access-views\n\nMaterialized views are only relevant for speeding up access to data; not for controlling access.\n\nOption A would not work because the following link says \"Users need the bigquery.tables.getData permission on all tables and views that their query references. In addition, when querying a view users need this permission on all underlying tables and views. However, if you are using authorized views or authorized datasets, you don't need to give users access to the underlying source data.\"\nhttps://cloud.google.com/bigquery/docs/table-access-controls#required_permission_to_query_tables_and_views","poster":"cloudmon","timestamp":"1649365380.0","upvote_count":"3","comment_id":"582602"},{"content":"Selected Answer: C\nThose who are confused between A and C, Option A does not create a new dataset. Its talking about granting the access to the same dataset where data and view both resides. So definitely option C provides better security option.","timestamp":"1645981920.0","poster":"Paragkk","upvote_count":"5","comment_id":"557459"},{"content":"Can't decide between C & A, I think this Reference indicates the elimination of the materialistic view. Any thoughts guys?\nhttps://cloud.google.com/blog/topics/developers-practitioners/bigquery-admin-reference-guide-data-governance","timestamp":"1643645340.0","comment_id":"537226","upvote_count":"2","poster":"TharaLN"},{"comment_id":"534780","content":"Selected Answer: C\ni vote c: https://cloud.google.com/bigquery/docs/scan-with-dlp#scanning-bigquery-data-using-the-cloud-console -> For Step 3: Add actions, enable Save to BigQuery to publish your Cloud DLP findings to a BigQuery table. \nhttps://cloud.google.com/bigquery/docs/share-access-views -> Giving a view access to a dataset is also known as creating an authorized view in BigQuery. \ni think the key is \"project level\" permissions to access the data. i think it's better to isolate the de-identified data in a separate dataset","upvote_count":"2","timestamp":"1643381460.0","poster":"Pime13"},{"poster":"sjmsummer","content":"Assuming D/S team needs to access the data for research more often, it makes sense to have materialized view (actually stored as dataset) than view. I will select B.","upvote_count":"1","comment_id":"527174","timestamp":"1642559760.0"},{"poster":"Sekierer","comment_id":"520804","timestamp":"1641812940.0","upvote_count":"1","content":"Selected Answer: A\nI vote for A\n\nBecause questions states:\nYou want to minimize cost and the time it takes to assign appropriate access to the tables. What should you do?\n\nAnd C would be better but increase cost due to storage.\n\nMaterialzed View would only speed up the results of the queries, not how fast you can grant access to the tables."},{"comments":[{"content":"No, materialized view will bring additional cost as it will use additional storage. Simple view is better as you don't store copies of same (non-pii) data. You pay for querying anyway, so it's better to store in 1 place and use view for non-pii access.","poster":"szefco","comment_id":"631001","upvote_count":"2","timestamp":"1657733940.0"}],"content":"Selected Answer: B\nI think B is better.\nIn terms of View, it is better to use Materialized View to reduce cost.\n(https://cloud.google.com/blog/products/data-analytics/bigquery-materialized-views-now-ga)\n\nIn terms of access control, assigning access controls to the dataset that contains the view is good because of the time it takes to assign appropriate access.","poster":"gggsrs","comment_id":"516395","upvote_count":"5","timestamp":"1641287940.0"},{"timestamp":"1641012540.0","poster":"victory108","upvote_count":"1","comment_id":"514361","content":"C. 1. Create a dataset for the data science team. 2. Create views of tables that you want to share, excluding PII. 3. Assign an appropriate project-level IAM role to the members of the data science team. 4. Assign access controls to the dataset that contains the view. 5. Authorize the view to access the source dataset."},{"content":"I would go for A.If the question was asking about \"Reduction in the execution time and cost for queries\" then Materialized view could have been one of option.View is logical.So it will be fastest to create and no additional cost. Additionally if we create Dataset then we are incurring storage cost.","upvote_count":"2","poster":"GMats","comment_id":"513893","timestamp":"1640928480.0"},{"upvote_count":"2","poster":"kvr247","timestamp":"1640788440.0","comment_id":"512259","content":"B is OK - https://cloud.google.com/bigquery/docs/materialized-views-intro"},{"poster":"menon_sarath","timestamp":"1640787120.0","content":"Option A is the correct answer as it do nto expose the PII and is less costly","upvote_count":"1","comment_id":"512241"},{"timestamp":"1640762520.0","comment_id":"511909","poster":"spoxman","content":"I would say C.\nA is not limiting access to the dataset, which still contains PII.\nC is creating a new dataset without PII.","upvote_count":"3"},{"content":"Option-A: I know both view and materialized view can solve this. But MV is mainly for aggregated view, which we don't need in this usecase and MV comes with extra cost. https://cloud.google.com/bigquery/docs/materialized-views-intro#comparison","upvote_count":"2","poster":"StelSen","comment_id":"510756","timestamp":"1640659740.0"}],"choices":{"C":"1. Create a dataset for the data science team. 2. Create views of tables that you want to share, excluding PII. 3. Assign an appropriate project-level IAM role to the members of the data science team. 4. Assign access controls to the dataset that contains the view. 5. Authorize the view to access the source dataset.","D":"1. Create a dataset for the data science team. 2. Create materialized views of tables that you want to share, excluding PII. 3. Assign an appropriate project-level IAM role to the members of the data science team. 4. Assign access controls to the dataset that contains the view. 5. Authorize the view to access the source dataset.","B":"1. From the dataset where you have the source data, create materialized views of tables that you want to share, excluding PII. 2. Assign an appropriate project-level IAM role to the members of the data science team. 3. Assign access controls to the dataset that contains the view.","A":"1. From the dataset where you have the source data, create views of tables that you want to share, excluding PII. 2. Assign an appropriate project-level IAM role to the members of the data science team. 3. Assign access controls to the dataset that contains the view."},"answer_images":[]},{"id":"FidZuF0IFLwIl1M0wd2e","answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/7154-exam-professional-cloud-architect-topic-1-question-18/","answers_community":["B (100%)"],"answer_ET":"B","isMC":true,"discussion":[{"timestamp":"1578770640.0","poster":"AWS56","content":"\"B. Google Cloud Dataproc\" is the answer","comment_id":"37819","upvote_count":"19"},{"upvote_count":"15","poster":"VinayakBudapanahalli","comment_id":"266655","comments":[{"timestamp":"1665951240.0","comment_id":"696517","poster":"AzureDP900","upvote_count":"1","content":"Agreed"}],"content":"Dataproc is a managed Spark and Hadoop service that lets you take advantage of open source data tools for batch processing, querying, streaming, and machine learning. Dataproc automation helps you create clusters quickly, manage them easily, and save money by turning clusters off when you don't need them. With less time and money spent on administration, you can focus on your jobs and your data.\nhttps://cloud.google.com/dataproc/docs/concepts/overview#:~:text=Dataproc%20is%20a%20managed%20Spark,%2C%20streaming%2C%20and%20machine%20learning.&text=With%20less%20time%20and%20money,your%20jobs%20and%20your%20data.","timestamp":"1610587080.0"},{"upvote_count":"2","poster":"Ekramy_Elnaggar","comment_id":"1309629","timestamp":"1731271920.0","content":"Selected Answer: B\n1. Managed Hadoop and Spark: Dataproc is specifically designed for running and managing Apache Spark and Hadoop clusters, which directly addresses your company's needs.\n2. Scalability: Dataproc allows you to easily scale your clusters to handle the increasing number and size of jobs. You can add or remove nodes as needed to accommodate the workload.\n3. Minimal Operations Work: Dataproc automates cluster creation, configuration, and management, minimizing the operational overhead. This is crucial since you want to reduce operations work.\n4. Code Compatibility: Dataproc is compatible with existing Spark and Hadoop code, so you can migrate your jobs with minimal or no code changes."},{"upvote_count":"1","timestamp":"1725687540.0","comment_id":"1279902","poster":"JohnJamesB1212","content":"Selected Answer: B\nB is correct because Dataproc is uded for Apache Hadoop and Spark"},{"content":"Selected Answer: B\nDataflow for data stream and batch.\nDataproc for data process with Apache Spark and Hadoop.\nCompute Engine for VM.\nKubernetes Engine for Kubernetes Cluster with Compute Engine under the hood.","comment_id":"963639","poster":"eka_nostra","upvote_count":"3","timestamp":"1690369980.0"},{"upvote_count":"2","poster":"alekonko","content":"Selected Answer: B\nB, Dataproc is Hadoop/Spark managed service in GCP","timestamp":"1679521500.0","comment_id":"847548"},{"comment_id":"754838","poster":"examch","upvote_count":"1","timestamp":"1671882960.0","content":"Selected Answer: B\nDataproc is a fully managed and highly scalable service for running Apache Hadoop, Apache Spark, Apache Flink, Presto, and 30+ open source tools and frameworks. Use Dataproc for data lake modernization, ETL, and secure data science, at scale, integrated with Google Cloud, at a fraction of the cost.\n\nhttps://cloud.google.com/dataproc"},{"upvote_count":"2","timestamp":"1671525600.0","content":"To scale the number and size of Apache Spark and Hadoop jobs being run on a local datacenter with the least amount of operations work and code change, you should consider using Google Cloud Dataproc, option B. Google Cloud Dataproc is a fully-managed service that makes it easy to run Apache Spark and Hadoop workloads in the cloud. It is designed to simplify the process of setting up and managing clusters for data processing, and allows you to scale quickly and easily as demand increases.\n\nWith Cloud Dataproc, you can create and delete clusters in just a few minutes, and you can use the familiar Apache Spark and Hadoop APIs and tools to process data. This means that you can utilize the cloud to scale your workloads with minimal changes to your code and operations work.\n\nOption A: Google Cloud Dataflow, option C: Google Compute Engine, and option D: Google Kubernetes Engine, would not be suitable for this use case, as they do not provide the same level of support for running Apache Spark and Hadoop workloads as Cloud Dataproc.","poster":"omermahgoub","comment_id":"750675"},{"timestamp":"1668849600.0","poster":"AniketD","content":"Selected Answer: B\nB. Dataproc is managed Apache Spark and Hadoop in GCP","comment_id":"721887","upvote_count":"1"},{"comment_id":"696757","upvote_count":"1","timestamp":"1665975240.0","content":"Dataproc for Hadoop and spark ecosystem","poster":"zr79"},{"content":"Selected Answer: B\nB. Google Cloud Dataproc","comment_id":"693684","timestamp":"1665645420.0","upvote_count":"1","poster":"minmin2020"},{"content":"B data proc for hadoop and spark","timestamp":"1663678500.0","upvote_count":"1","comment_id":"674179","poster":"holerina"},{"comment_id":"617660","timestamp":"1655460360.0","poster":"Dhiraj03","content":"Keyword - Apache Spark and Hadoop jobs - Go with Dataproc","upvote_count":"1"},{"poster":"Superr","timestamp":"1653993840.0","comment_id":"609670","upvote_count":"1","content":"Selected Answer: B\ndataproc"},{"comment_id":"588484","upvote_count":"2","content":"Selected Answer: B\nGoogle Cloud Dataproc == managed Spark and Hadoop service","poster":"Nirca","timestamp":"1650439320.0"},{"timestamp":"1650258540.0","poster":"pakochiu","upvote_count":"1","comment_id":"587478","content":"Selected Answer: B\nB - Dataproc Lift&Shift of Apache Spark and Hadoop jobs"},{"timestamp":"1643115360.0","content":"B is ok.","upvote_count":"1","poster":"llanerox","comment_id":"532130"},{"content":"B Cloud Data Proc　is correct.\nCloud Data Proc can easly migration form hadoop , spark.","timestamp":"1640769000.0","poster":"OrangeTiger","comment_id":"511995","upvote_count":"1"},{"content":"Go for B.","poster":"haroldbenites","upvote_count":"1","comment_id":"493264","timestamp":"1638550980.0"},{"timestamp":"1638015360.0","poster":"vincy2202","comment_id":"488111","content":"Answer is B","upvote_count":"1"},{"timestamp":"1621325940.0","comment_id":"360249","content":"B. Google Cloud Dataproc","upvote_count":"1","poster":"victory108"},{"upvote_count":"1","timestamp":"1620663840.0","comment_id":"353910","content":"B is correct","poster":"un"},{"content":"B is ok","upvote_count":"1","comment_id":"324123","timestamp":"1617093960.0","poster":"lynx256"},{"content":"Answer is B","poster":"Ausias18","comment_id":"323985","timestamp":"1617080820.0","upvote_count":"1"},{"content":"B is the right answer","comment_id":"260909","upvote_count":"1","timestamp":"1609927440.0","poster":"dlzhang"},{"timestamp":"1603737960.0","upvote_count":"1","content":"B is good","comment_id":"206509","poster":"nimso"},{"content":"B - Dataproc\nLift&Shift of Apache Spark and Hadoop jobs","comment_id":"179440","timestamp":"1600103520.0","poster":"AshokC","upvote_count":"1"},{"poster":"gkdinesh","timestamp":"1599746880.0","upvote_count":"1","content":"B is correct","comment_id":"177142"},{"timestamp":"1592890320.0","content":"It says Apache Spark and Hadoop, so Dataproc. B is fine","comment_id":"117066","poster":"mlantonis","upvote_count":"1"},{"poster":"Tushant","content":"B is the right answer","comment_id":"114437","upvote_count":"1","timestamp":"1592630220.0"},{"comment_id":"106505","poster":"gfhbox0083","upvote_count":"1","timestamp":"1591767960.0","content":"B, for sure.\nDataproc for Apache Spark and Hadoop"},{"poster":"Nirms","content":"B is the correct answer","comment_id":"100854","upvote_count":"1","timestamp":"1591104660.0"},{"poster":"Ziegler","upvote_count":"2","content":"B is correct","timestamp":"1590765720.0","comment_id":"98318"},{"content":"Final Decision to go with Option B","timestamp":"1590737040.0","poster":"AD2AD4","comment_id":"98087","upvote_count":"1"},{"upvote_count":"2","comment_id":"86290","poster":"clouddude","content":"I'll go with B as Dataproc is meant for Hadoop replacement.","timestamp":"1589071800.0"},{"comment_id":"84710","upvote_count":"2","timestamp":"1588795020.0","content":"B is the correct answer","poster":"gcp_aws"},{"poster":"2g","comment_id":"44710","timestamp":"1580389980.0","upvote_count":"3","content":"answer: B"},{"upvote_count":"3","timestamp":"1571936460.0","content":"All four answers could work. Cloud Dataproc requires a Compute Engine instance to run, and Kubernetes lets many Compute Engine instances run. If you don't want to use your sotware on the GCP Dataproc is the best choice. Otherwise, C or D would work.","comments":[{"content":"B, Hadoop + Spark = DataProc","timestamp":"1614869400.0","upvote_count":"2","poster":"nitinz","comment_id":"303403","comments":[{"poster":"nitinz","timestamp":"1614869460.0","content":"Full equation is : -\nHadoop + Spark + Hive + Pig = DataProc","upvote_count":"3","comment_id":"303404"}]},{"timestamp":"1596685980.0","poster":"tartar","content":"B is ok","upvote_count":"3","comment_id":"151649"}],"poster":"Eroc","comment_id":"17228"}],"choices":{"A":"Google Cloud Dataflow","B":"Google Cloud Dataproc","D":"Google Kubernetes Engine","C":"Google Compute Engine"},"question_id":90,"question_images":[],"timestamp":"2019-10-24 19:01:00","answer_description":"","exam_id":4,"answer":"B","question_text":"Your company is forecasting a sharp increase in the number and size of Apache Spark and Hadoop jobs being run on your local datacenter. You want to utilize the cloud to help you scale this upcoming demand with the least amount of operations work and code change.\nWhich product should you use?","unix_timestamp":1571936460,"topic":"1"}],"exam":{"lastUpdated":"11 Apr 2025","isBeta":false,"isMCOnly":false,"provider":"Google","name":"Professional Cloud Architect","isImplemented":true,"numberOfQuestions":279,"id":4},"currentPage":18},"__N_SSP":true}