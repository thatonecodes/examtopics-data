{"pageProps":{"questions":[{"id":"okkwOIkC7p3DUYwR8L3z","timestamp":"2021-07-06 11:24:00","topic":"1","answer_images":[],"question_images":[],"question_id":256,"unix_timestamp":1625563440,"answers_community":["C (94%)","6%"],"answer":"C","discussion":[{"poster":"inder0007","upvote_count":"21","comments":[{"timestamp":"1654531380.0","content":"I also think is C:\nreference : https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/aad9f93b86b7addfea4c419b9100c6cdd26cacea.pdf","upvote_count":"1","poster":"simoncerda","comment_id":"495329"},{"upvote_count":"4","content":"performance monitoring is a continuous effort that happens all time. but reproducibility makes more sense to be added to model QA","timestamp":"1642311180.0","comment_id":"407558","comments":[{"poster":"sensev","content":"The question was not about model QA but production readiness, thus I think the answer is C because monitor model performance in production is important. As regard to A, I would I argue it could fall under \"model development\", since reproducible training is already important during model development.","comments":[{"timestamp":"1652400600.0","poster":"vivid_cucumber","content":"To my understanding, I think A might be correct since model performance monitoring is happens \"in production\". but the question said the project \"will soon release\" which means right now is before launching, so to me testing the reproducible would make more sense. (I was confused about A and C for a long time)\nreference: \n- Testing reproducibility: https://developers.google.com/machine-learning/testing-debugging/pipeline/deploying\n- Testing in Production: https://developers.google.com/machine-learning/testing-debugging/pipeline/production","comment_id":"477244","upvote_count":"7"}],"comment_id":"416655","upvote_count":"4","timestamp":"1643441940.0"}],"poster":"omar_bh"}],"comment_id":"399803","content":"I think it should be C","timestamp":"1641468240.0"},{"poster":"ralf_cc","upvote_count":"9","timestamp":"1641792540.0","comment_id":"403081","content":"A - important one before moving to the production","comments":[{"timestamp":"1643279220.0","poster":"salsabilsf","upvote_count":"5","comment_id":"415296","content":"Testing for Deploying Machine Learning Models:\n- Test Model Updates with Reproducible Training\nhttps://developers.google.com/machine-learning/testing-debugging/pipeline/deploying"}]},{"content":"Selected Answer: C\nC) Model monitoring","timestamp":"1733513460.0","upvote_count":"1","comment_id":"1225680","poster":"PhilipKoku"},{"content":"C is not a readiness check. Monitoring is a continuous effort. IMO A is the correct answer. If the training is not reproducible it's not ready for production. If any error happens, data drifts / skews, then there is no way to recreate the model.\n\nThis is a check BEFORE going to production. Once it's in production, then yes C is important.","timestamp":"1731005640.0","poster":"SahandJ","comment_id":"1207954","upvote_count":"1"},{"poster":"fragkris","content":"Selected Answer: C\nMonitoring is crucial. So - C","upvote_count":"2","comment_id":"1088393","timestamp":"1717576560.0"},{"content":"Selected Answer: C\nWent with C","poster":"M25","comment_id":"892737","upvote_count":"1","timestamp":"1699513800.0"},{"poster":"e707","comment_id":"881251","timestamp":"1698305640.0","content":"Selected Answer: C\nI'll go with C. \nMonitoring model performance is an important aspect of production readiness. It allows the team to detect and respond to changes in performance that may affect the quality of the model. The other options are also important, but they are more focused on the development phase of the project rather than the production phase.","upvote_count":"1"},{"comment_id":"810348","content":"Selected Answer: C\nHey! all guys\nA+B+D=The team has already tested features and data, model development, and infrastructure. we are about to go live with production. \nMonitoring readiness is the last thing to account for.\n\nIt will be very rediculous if you launch model as production regardless of how we will have about monitoring. you will lauch model as production for while and will make plan to model performance monitoring later ??? you are too reckless.\n\nPls . Read it carefully https://developers.google.com/machine-learning/testing-debugging/pipeline/production\nhttps://developers.google.com/machine-learning/testing-debugging/pipeline/overview#what-is-an-ml-pipeline.\nYou \n\nMost guys prefer A : https://developers.google.com/machine-learning/testing-debugging/pipeline/deploying I think that it is all about model development prior to deploying .","timestamp":"1692163140.0","poster":"John_Pongthorn","upvote_count":"4"},{"content":"Selected Answer: C\nI think that your team ensure that all hypermarameters were turned yet when tested features... i think that it's more important that they ensure that model performance is monitored than thaining is reproducible for best practices.\nhttps://cloud.google.com/architecture/ml-on-gcp-best-practices","timestamp":"1691522340.0","comment_id":"802570","poster":"enghabeth","upvote_count":"1"},{"timestamp":"1690176480.0","comment_id":"786254","upvote_count":"1","poster":"John_Pongthorn","content":"Selected Answer: C\nReproducible Training is more likely to be in the Deployment step in that it referred to the question \"The team has already tested features and data, model development\" but the question focuses on Production readiness \nhttps://developers.google.com/machine-learning/testing-debugging/pipeline/production\nMonitor section is part of this above link"},{"poster":"ares81","comment_id":"766597","content":"Selected Answer: C\nC, for me.","upvote_count":"1","timestamp":"1688552220.0"},{"poster":"vakati","comment_id":"714026","content":"Selected Answer: C\nIt's mentioned that the team has already tested features and data, implying that data generation is reproducible. If you have to test features data has to be reproducible to compare model outputs. ( https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/randomization). Hence C makes more sense","upvote_count":"2","timestamp":"1683562260.0"},{"poster":"bL357A","timestamp":"1678006260.0","content":"Selected Answer: C\nhttps://cloud.google.com/ai-platform/docs/ml-solutions-overview","comment_id":"659821","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: C\nWith the specific focus on \"production readiness\" as stated, I'd pick C above the others.","poster":"u_phoria","timestamp":"1672079460.0","comment_id":"622679"},{"upvote_count":"1","poster":"KD1988","timestamp":"1671433560.0","content":"I think it's C. \nA is related to infrastructure, B is related to model development and D is related to Data and features. It clearly mentioned that team has already tested for model development, data and features and infrastructure.","comment_id":"618526"},{"content":"Selected Answer: A\n\"production readiness\" means that we are still in dev-test phase , and \"performance \n monitoring\" happens in production , and what if monitoring is applied but the model re-train is difficult , so \"A\" is the best answer","upvote_count":"1","comment_id":"613195","poster":"Mohamed_Mossad","timestamp":"1670504520.0"},{"comment_id":"549427","content":"A makes more sense than C.","timestamp":"1660737960.0","poster":"abc0000","upvote_count":"2"}],"answer_ET":"C","url":"https://www.examtopics.com/discussions/google/view/57271-exam-professional-machine-learning-engineer-topic-1-question/","question_text":"You recently joined a machine learning team that will soon release a new project. As a lead on the project, you are asked to determine the production readiness of the ML components. The team has already tested features and data, model development, and infrastructure. Which additional readiness check should you recommend to the team?","choices":{"D":"Ensure that feature expectations are captured in the schema.","B":"Ensure that all hyperparameters are tuned.","A":"Ensure that training is reproducible.","C":"Ensure that model performance is monitored."},"answer_description":"","exam_id":13,"isMC":true},{"id":"DX7isv95VWnDxEO1xDDS","isMC":true,"answer_images":[],"answer_ET":"C","answer":"C","question_text":"You work for a credit card company and have been asked to create a custom fraud detection model based on historical data using AutoML Tables. You need to prioritize detection of fraudulent transactions while minimizing false positives. Which optimization objective should you use when training the model?","question_id":257,"url":"https://www.examtopics.com/discussions/google/view/57276-exam-professional-machine-learning-engineer-topic-1-question/","answers_community":["C (89%)","11%"],"unix_timestamp":1625564160,"discussion":[{"upvote_count":"21","content":"This is a case of imbalanced data.\nAns: C \nhttps://stats.stackexchange.com/questions/262616/roc-vs-precision-recall-curves-on-imbalanced-dataset\n\nhttps://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc","comments":[{"poster":"GogoG","upvote_count":"2","content":"C is wrong - correct answer is D. ROC basically compares True Positives against False Negative, exactly what we are trying to optimise for.","timestamp":"1634501940.0","comment_id":"463675"}],"timestamp":"1627787400.0","poster":"Paul_Dirac","comment_id":"418093"},{"content":"D - https://en.wikipedia.org/wiki/Receiver_operating_characteristic","comments":[{"timestamp":"1626408660.0","comment_id":"407576","poster":"omar_bh","comments":[{"upvote_count":"2","poster":"tavva_prudhvi","content":"A larger area under the ROC curve does indicate a better model performance in terms of correctly identifying true positives. However, it does not take into account the imbalance in the class distribution or the costs associated with false positives and false negatives.\n\nIn contrast, the AUC PR curve focuses on the trade-off between precision (Y-axis) and recall (X-axis), making it more suitable for imbalanced datasets and applications with different costs for false positives and false negatives, like credit card fraud detection.","comment_id":"959301","timestamp":"1690008540.0"}],"upvote_count":"2","content":"True. The true positive is presented by Y axis. The bigger the area the graph take, the higher TP ratio"},{"comment_id":"959300","upvote_count":"2","timestamp":"1690008480.0","poster":"tavva_prudhvi","content":"AUC ROC is more suitable when the class distribution is balanced and false positives and false negatives have similar costs.\n\nIn the case of credit card fraud detection, the class distribution is typically imbalanced (fewer fraudulent transactions compared to non-fraudulent ones), and the cost of false positives (incorrectly identifying a transaction as fraudulent) and false negatives (failing to detect a fraudulent transaction) are not the same.\n\nBy maximizing the AUC PR (area under the precision-recall curve), the model focuses on the trade-off between precision (proportion of true positives among predicted positives) and recall (proportion of true positives among actual positives), which is more relevant in imbalanced datasets and for applications where the costs of false positives and false negatives are not equal. This makes option C a better choice for credit card fraud detection."}],"comment_id":"403092","timestamp":"1625889060.0","upvote_count":"8","poster":"ralf_cc"},{"upvote_count":"1","poster":"jkkim_jt","content":"Selected Answer: C\no AUC-PR focuses on how well the classifier performs for the positive class (precision and recall are both concerned with positives) --> more suitable when the focus is on indentifying the positive class in imbalanced data \no AUC-ROC looks at the trade-off between the true positive rate (sensitivity) and the false positive rate, considering both classes. --> general purpose meric that works well when both classes are of similiar size \n( ChatGPT )","timestamp":"1729401840.0","comment_id":"1300293"},{"upvote_count":"1","content":"Selected Answer: C\nC) PR (Precision Recall)","poster":"PhilipKoku","timestamp":"1717695480.0","comment_id":"1225686"},{"poster":"PhilipKoku","timestamp":"1717695420.0","upvote_count":"1","content":"Selected Answer: C\nC) PR ROC","comment_id":"1225685"},{"upvote_count":"4","comment_id":"942000","content":"Selected Answer: C\nIn fraud detection, it's crucial to minimize false positives (transactions flagged as fraudulent but are actually legitimate) while still detecting as many fraudulent transactions as possible. AUC PR is a suitable optimization objective for this scenario because it provides a balanced trade-off between precision and recall, which are both important metrics in fraud detection. A high AUC PR value indicates that the model has high precision and recall, which means it can detect a large number of fraudulent transactions while minimizing false positives.\n\nLog loss (A) and AUC ROC (D) are also commonly used optimization objectives in machine learning, but they may not be as effective in this particular scenario. Precision at a Recall value of 0.50 (B) is a specific metric and not an optimization objective.","poster":"tavva_prudhvi","timestamp":"1688398680.0"},{"poster":"M25","comment_id":"892738","upvote_count":"1","content":"Selected Answer: C\nWent with C","timestamp":"1683609000.0"},{"poster":"John_Pongthorn","upvote_count":"1","timestamp":"1676533500.0","content":"Selected Answer: C\nHi Everyone\nI discover, there are some clues that this question is likely to refer to the last section of https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc \nThis is what it tries to tell us especially with the last sentence\nClassification-threshold invariance is not always desirable. In cases where there are wide disparities in the cost of false negatives vs. false positives, it may be critical to minimize one type of classification error. For example, when doing email spam detection, you likely want to prioritize minimizing false positives (even if that results in a significant increase of false negatives). AUC isn't a useful metric for this type of optimization.\n\nAdditionally, it tells me which of the following choices is the answer to this question as below.\nhttps://cloud.google.com/automl-tables/docs/train#opt-obj.","comment_id":"810387"},{"poster":"enghabeth","timestamp":"1675892100.0","content":"Selected Answer: D\nWhat is different however is that ROC AUC looks at a true positive rate TPR and false positive rate FPR while PR AUC looks at positive predictive value PPV and true positive rate TPR.\n\nDetect Fraudulent transactions = Max TP\nMinimizing false positives -> min FP\n\nhttps://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc#:~:text=ROC%20AUC%20vs%20PR%20AUC&text=What%20is%20different%20however%20is,and%20true%20positive%20rate%20TPR","comment_id":"802576","upvote_count":"1"},{"comment_id":"786366","content":"Selected Answer: C\nDetection of fraudulent transactions seems to be imbalanced data.\nhttps://cloud.google.com/automl-tables/docs/train#opt-obj\nAUC ROC : Distinguish between classes. Default value for binary classification.\n\nAUC PR Optimize results for predictions for the less common class.\nit is straightforward to answer, you just have to capture key word to get the right way. (Almost banlanced Or Imbalanced)\nhttps://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n\nWhen to Use ROC vs. Precision-Recall Curves?\nGenerally, the use of ROC curves and precision-recall curves are as follows:\n\nROC curves should be used when there are roughly equal numbers of observations for each class.\nPrecision-Recall curves should be used when there is a moderate to large class imbalance.","upvote_count":"3","poster":"John_Pongthorn","timestamp":"1674553080.0"},{"timestamp":"1672920900.0","content":"Selected Answer: C\nFraud Detection --> Imbalanced Dataset ---> AUC PR --> C, for me","comment_id":"766593","upvote_count":"1","poster":"ares81"},{"upvote_count":"1","comment_id":"746582","content":"Selected Answer: C\nans: C\nPaul_Dirac and giaZ are correct.","timestamp":"1671146280.0","poster":"wish0035"},{"timestamp":"1671137640.0","upvote_count":"2","comment_id":"746495","poster":"hiromi","content":"Selected Answer: C\nC\nhttps://towardsdatascience.com/on-roc-and-precision-recall-curves-c23e9b63820c"},{"content":"\"You need to prioritize detection of fraudulent transactions while minimizing false positives.\"\nSeems that answer B fits this well. If we want to focus exactly on minimizing false positives we can do that by maximising Precision at a specific Recall value. C is about balance between these two, and D doesn't care about false positive/negatives.","timestamp":"1662538020.0","comment_id":"662221","upvote_count":"2","poster":"itallix"},{"poster":"suresh_vn","content":"Selected Answer: D\nD \nhttps://en.wikipedia.org/wiki/Receiver_operating_characteristic\nC optimize precision only","comments":[{"content":"Sorry, C is my final decision\nhttps://cloud.google.com/automl-tables/docs/train#opt-obj","comment_id":"650564","timestamp":"1661227620.0","upvote_count":"1","poster":"suresh_vn"}],"comment_id":"650562","upvote_count":"1","timestamp":"1661227380.0"},{"content":"Selected Answer: C\nAnswer is c.","timestamp":"1659776580.0","upvote_count":"1","comment_id":"643264","poster":"rtnk22"},{"poster":"giaZ","timestamp":"1646922900.0","upvote_count":"6","comment_id":"564850","content":"https://icaiit.org/proceedings/6th_ICAIIT/1_3Fayzrakhmanov.pdf\nThe problem of fraudulent transactions detection, which is an imbalanced classification problem (most transactions are not fraudulent), you want to maximize both precision and recall; so the area under the PR curve. As a matter of fact, the question asks you to focus on detecting fraudulent transactions (maximize true positive rate, a.k.a. Recall) while minimizing false positives (a.k.a. maximizing Precision). Another way to see it is this: for imbalanced problems like this one you'll get a lot of true negatives even from a bad model (it's easy to guess a transaction as \"non-fraudulent\" because most of them are!), and with high TN the ROC curve goes high fast, which would be misleading. So you wanna avoid dealing with true negatives in your evaluation, which is precisely what the PR curve allows you to do."},{"poster":"ramen_lover","upvote_count":"3","comment_id":"495972","timestamp":"1638880140.0","content":"The following is the official document for the list of optimization objectives for AutoML Tables\n\"About model optimization objectives\"\nhttps://cloud.google.com/automl-tables/docs/train#opt-obj\n\nAUC PR: Optimize results for predictions for the less common class."},{"comment_id":"471477","poster":"attaraya","content":"I also vote for Ans:C since this is an outlier detection which is imbalanced. So best metric is AUC-PR to evaluate the model","timestamp":"1635820080.0","upvote_count":"2"}],"exam_id":13,"topic":"1","answer_description":"","choices":{"A":"An optimization objective that minimizes Log loss","C":"An optimization objective that maximizes the area under the precision-recall curve (AUC PR) value","D":"An optimization objective that maximizes the area under the receiver operating characteristic curve (AUC ROC) value","B":"An optimization objective that maximizes the Precision at a Recall value of 0.50"},"timestamp":"2021-07-06 11:36:00","question_images":[]},{"id":"iRykWdXxcOFVMz22BWJl","topic":"1","answers_community":["C (83%)","A (17%)"],"answer_description":"","answer_ET":"C","answer":"C","discussion":[{"content":"Ans: C (See https://developers.google.com/machine-learning/problem-framing/framing#quantify-it; though it's just an example.)\n(A) The absolute number of likes shouldn't be used because no information about subscribers or visits to the website is provided. The number may vary.\n(B) Clickbait videos are a subset of uploaded videos. Using them is an improper criterion.\n(D) The coefficient should reach 1. (Ref:https://arxiv.org/pdf/1510.06223.pdf)","poster":"Paul_Dirac","upvote_count":"18","comments":[{"comment_id":"416658","timestamp":"1643442360.0","content":"Thanks for the detailed unswer and reference!","upvote_count":"5","poster":"sensev"}],"timestamp":"1640619120.0","comment_id":"392091"},{"timestamp":"1737477240.0","comment_id":"1344227","upvote_count":"1","poster":"moammary","content":"Selected Answer: A\nThe answer is A.\nBecause the number of previous user likes is the only feature available on inference time (when the video has just been uploaded). Watch time and clicks are unavailable at inference time and should not be used for training!"},{"comment_id":"1225689","poster":"PhilipKoku","content":"Selected Answer: C\nC) Watch time","timestamp":"1733514120.0","upvote_count":"1"},{"poster":"M25","content":"Selected Answer: C\nWent with C","upvote_count":"1","comment_id":"892739","timestamp":"1699513860.0"},{"content":"ans: C\nIn this type of questions, I think a good idea is trying to copy already existing solutions. For this case, YouTube cares a lot about watchtime. In a previous question, Amazon implemented \"Usually buy together\" for maximizing profit.","upvote_count":"4","poster":"wish0035","comment_id":"746586","timestamp":"1686864300.0"},{"content":"Selected Answer: C\nMust be C","poster":"hiromi","comment_id":"746496","upvote_count":"1","timestamp":"1686855540.0"},{"upvote_count":"2","poster":"Mohamed_Mossad","comment_id":"629921","content":"Selected Answer: C\nwatch time among all other options is the most KPI to rely on","timestamp":"1673433420.0"},{"comment_id":"569935","poster":"baimus","timestamp":"1663423500.0","content":"I think this is B. The question specifies \"popular\" and also that \"newly uploaded\" videos need prioritising. C is therefore wrong because you don't have that metric until 30 days has passed from upload time. \"Click through rate\" is one measure of popularity, so it fits, and is instant.","upvote_count":"1"},{"poster":"NamitSehgal","content":"C looks correct.","comment_id":"517151","timestamp":"1656986580.0","upvote_count":"1"},{"timestamp":"1642665420.0","comment_id":"410043","upvote_count":"3","content":"ANS: C\n\nD is wrong. \nPearson's Correlation Coefficient is a linear correlation coefficient that returns a value of between -1 and +1. \nA -1 means there is a strong negative correlation\n+1 means that there is a strong positive correlation\n0 means that there is no correlation","poster":"celia20200410"}],"timestamp":"2021-06-27 15:32:00","choices":{"A":"The model predicts videos as popular if the user who uploads them has over 10,000 likes.","B":"The model predicts 97.5% of the most popular clickbait videos measured by number of clicks.","C":"The model predicts 95% of the most popular videos measured by watch time within 30 days of being uploaded.","D":"The Pearson correlation coefficient between the log-transformed number of views after 7 days and 30 days after publication is equal to 0."},"exam_id":13,"question_id":258,"url":"https://www.examtopics.com/discussions/google/view/56170-exam-professional-machine-learning-engineer-topic-1-question/","isMC":true,"unix_timestamp":1624800720,"question_images":[],"answer_images":[],"question_text":"Your company manages a video sharing website where users can watch and upload videos. You need to create an ML model to predict which newly uploaded videos will be the most popular so that those videos can be prioritized on your company's website. Which result should you use to determine whether the model is successful?"},{"id":"LkslCbu8NjgWEM9Tvnw6","isMC":true,"answer_images":[],"answer_ET":"B","answer":"B","question_text":"You are working on a Neural Network-based project. The dataset provided to you has columns with different ranges. While preparing the data for model training, you discover that gradient optimization is having difficulty moving weights to a good solution. What should you do?","question_id":259,"url":"https://www.examtopics.com/discussions/google/view/57554-exam-professional-machine-learning-engineer-topic-1-question/","answers_community":["B (94%)","6%"],"unix_timestamp":1625890020,"discussion":[{"poster":"kurasaki","timestamp":"1625908860.0","content":"Vote for B. We could impute instead of remove the column to avoid loss of information","comment_id":"403241","upvote_count":"26"},{"timestamp":"1632812880.0","content":"I also think it is B:\n\"The presence of feature value X in the formula will affect the step size of the gradient descent. The difference in ranges of features will cause different step sizes for each feature. To ensure that the gradient descent moves smoothly towards the minima and that the steps for gradient descent are updated at the same rate for all the features, we scale the data before feeding it to the model.\"","poster":"pddddd","comment_id":"453128","upvote_count":"11"},{"timestamp":"1722699000.0","content":"Selected Answer: B\nclearly B","comment_id":"1260354","upvote_count":"1","poster":"jsalvasoler"},{"upvote_count":"2","content":"Selected Answer: B\nB) Option B (Use the representation transformation technique) is the most relevant choice. Normalizing the features will help gradient descent converge efficiently, leading to better weight updates and improved model performance.\nRemember that feature scaling is crucial for gradient optimization, especially when dealing with features that have different ranges. By ensuring consistent scales, you’ll enhance the effectiveness of your Neural Network training process.","poster":"PhilipKoku","comment_id":"1225690","timestamp":"1717695960.0"},{"timestamp":"1711956660.0","upvote_count":"2","comment_id":"1187271","poster":"MultiCloudIronMan","content":"Selected Answer: B\nBecause the range needs to normalize"},{"upvote_count":"2","poster":"fragkris","content":"Selected Answer: B\nB - The key phrase is \"different ranges\", therefore we need to normalize the values.","comment_id":"1088420","timestamp":"1701775500.0"},{"upvote_count":"1","poster":"M25","timestamp":"1683609060.0","content":"Selected Answer: B\nWent with B","comment_id":"892740"},{"comment_id":"887308","content":"Selected Answer: B\nNormalization","poster":"SergioRubiano","upvote_count":"1","timestamp":"1683029340.0"},{"poster":"ares81","content":"Selected Answer: B\nNormalization is the word.","comment_id":"766587","upvote_count":"2","timestamp":"1672920060.0"},{"poster":"ares81","timestamp":"1672920000.0","comment_id":"766586","content":"Selected Answer: C\nNormalization is the word.","upvote_count":"1"},{"poster":"hiromi","timestamp":"1671138180.0","content":"Selected Answer: B\nB\n \"Normalization\" is the keyword","upvote_count":"1","comment_id":"746497"},{"timestamp":"1642625460.0","upvote_count":"4","comment_id":"527914","content":"Selected Answer: B\nnormalization\nhttps://developers.google.com/machine-learning/data-prep/transform/transform-numeric","poster":"ggorzki"},{"timestamp":"1641631440.0","upvote_count":"4","comment_id":"519386","content":"B. The problem does not mention anything about missing values. It needs to normalize the features with different ranges.","poster":"MK_Ahsan"},{"upvote_count":"1","poster":"NamitSehgal","content":"Looking at explanation I would choose C as well","comment_id":"517152","timestamp":"1641355560.0"},{"content":"(B)\n- NN models needs features with close ranges\n- SGD converges well using features in [0, 1] scale\n- The question specifically mention \"different ranges\"\nDocumentation - https://developers.google.com/machine-learning/data-prep/transform/transform-numeric","comment_id":"480259","poster":"kaike_reis","timestamp":"1637181360.0","upvote_count":"3"},{"content":"When gradient descent fails, it's out of the lacking of a powerful feature. Using normalization would make it worse.\nInstead, using either A or C would increase the strength of certain feature.\nBut, C should come first since A is only feasible after at least 1 meaningful training.\nSo C.","timestamp":"1631833920.0","comment_id":"446212","poster":"Y2Data","upvote_count":"2"},{"timestamp":"1625890020.0","comments":[{"upvote_count":"4","poster":"omar_bh","timestamp":"1626411720.0","content":"Normalization is more complicated than that. \n\nNormalization changes the values of dataset's numeric fields to be in a common scale, without impacting differences in the ranges of values. Normalization is required only when features have different ranges.","comment_id":"407588"}],"upvote_count":"3","comment_id":"403098","content":"B - remove the outliers?","poster":"ralf_cc"}],"exam_id":13,"topic":"1","choices":{"B":"Use the representation transformation (normalization) technique.","C":"Improve the data cleaning step by removing features with missing values.","D":"Change the partitioning step to reduce the dimension of the test set and have a larger training set.","A":"Use feature construction to combine the strongest features."},"answer_description":"","timestamp":"2021-07-10 06:07:00","question_images":[]},{"id":"xnadUf6QtQ5GKJyvN4cY","unix_timestamp":1625890680,"exam_id":13,"choices":{"A":"Use Kubeflow Pipelines to execute the experiments. Export the metrics file, and query the results using the Kubeflow Pipelines API.","D":"Use AI Platform Notebooks to execute the experiments. Collect the results in a shared Google Sheets file, and query the results using the Google Sheets API.","B":"Use AI Platform Training to execute the experiments. Write the accuracy metrics to BigQuery, and query the results using the BigQuery API.","C":"Use AI Platform Training to execute the experiments. Write the accuracy metrics to Cloud Monitoring, and query the results using the Monitoring API."},"topic":"1","answer":"A","url":"https://www.examtopics.com/discussions/google/view/57555-exam-professional-machine-learning-engineer-topic-1-question/","question_images":[],"question_id":260,"answer_ET":"A","isMC":true,"answer_images":[],"answers_community":["A (69%)","C (21%)","10%"],"discussion":[{"upvote_count":"17","timestamp":"1676430480.0","content":"Selected Answer: A\nOld answer is A. New answer (not available) would be Virtex AI experiments which comes with monitoring API inbuilt. https://cloud.google.com/blog/topics/developers-practitioners/track-compare-manage-experiments-vertex-ai-experiments","comment_id":"809043","poster":"Dunnoth"},{"upvote_count":"12","poster":"Celia20210714","comment_id":"409207","timestamp":"1626650880.0","content":"ANS: A\nhttps://codelabs.developers.google.com/codelabs/cloud-kubeflow-pipelines-gis\nKubeflow Pipelines (KFP) helps solve these issues by providing a way to deploy robust, repeatable machine learning pipelines along with monitoring, auditing, version tracking, and reproducibility. Cloud AI Pipelines makes it easy to set up a KFP installation."},{"poster":"mouthwash","comment_id":"1334175","content":"Selected Answer: B\nA is an old answer. The platform is evolving.\n\nSo B is the right answer.","upvote_count":"1","timestamp":"1735566540.0"},{"timestamp":"1733536020.0","comment_id":"1322964","upvote_count":"2","poster":"rajshiv","content":"Selected Answer: B\nA is not correct. Agreed that Kubeflow Pipelines is a powerful tool for running and managing ML workflows, but exporting metrics to an external file (e.g., CSV or JSON) requires extra manual work for managing the data and querying it. Kubeflow Pipelines do not have the same native integration with BigQuery for storing metrics, and querying via the Kubeflow API can be more complex than using BigQuery, especially when managing large-scale experiments. So I will go with B."},{"upvote_count":"1","poster":"eico","timestamp":"1724532300.0","comment_id":"1271851","content":"Selected Answer: A\nThis is an old question, when Vertex AI didn't have Vertex AI Experiments. The old answer is A"},{"comment_id":"1252983","content":"Shoudlnt it be B? VAI has inbuilt VAI experiments and metadata to track metrics..","upvote_count":"1","poster":"San1111111111","timestamp":"1721641620.0"},{"poster":"dija123","comment_id":"1232424","timestamp":"1718714880.0","upvote_count":"1","content":"Selected Answer: A\nShould agree with A"},{"poster":"PhilipKoku","content":"Selected Answer: A\nA) Kubeflow pipelines","comment_id":"1225692","timestamp":"1717696260.0","upvote_count":"1"},{"content":"Selected Answer: C\neither A or C but going with C due to minimal effort","upvote_count":"5","timestamp":"1700069460.0","comment_id":"1071717","poster":"Mickey321"},{"comment_id":"945747","poster":"Liting","timestamp":"1688738220.0","upvote_count":"1","content":"Selected Answer: A\nI agree with tavva_prudhvi that cloud monitoring is not the best option to do machine learning tracking, Metadata is a better option for that purpose"},{"poster":"tavva_prudhvi","content":"Selected Answer: A\nOption C suggests using AI Platform Training to execute the experiments and write the accuracy metrics to Cloud Monitoring. While Cloud Monitoring can be used to monitor and collect metrics from various services in Google Cloud, it is not specifically designed for machine learning experiments tracking.\n\nUsing Cloud Monitoring for tracking machine learning experiments may not provide the same level of functionality and flexibility as Kubeflow Pipelines or AI Platform Training. Additionally, querying the results from Cloud Monitoring may not be as straightforward as using the APIs provided by Kubeflow Pipelines or AI Platform Training.\n\nTherefore, while Cloud Monitoring can be used as a general-purpose monitoring solution, it may not be the best option for tracking and reporting machine learning experiments.","timestamp":"1688399220.0","comment_id":"942012","upvote_count":"2"},{"comment_id":"927544","timestamp":"1687181280.0","poster":"PST21","upvote_count":"1","content":"Cloud monitoring may not be the most suitable option for tracking and reporting experiments, only because of this option C is out & I stick to A"},{"content":"Selected Answer: A\nWent with A","upvote_count":"2","timestamp":"1683609060.0","poster":"M25","comment_id":"892741"},{"content":"Selected Answer: B\nIt is B","poster":"lucaluca1982","timestamp":"1682664720.0","upvote_count":"1","comment_id":"883282"},{"comments":[{"upvote_count":"1","comment_id":"810904","content":"As The lab walk me through how to create pipe line to experiment , it use Kubeflow and apply experiment SDK","poster":"John_Pongthorn","timestamp":"1676564640.0"}],"poster":"John_Pongthorn","timestamp":"1676557380.0","content":"This is the question, Try out and choose what is the closet to this lab.Last updated Jan 21, 2023\nhttps://codelabs.developers.google.com/vertex_experiments_pipelines_intro#0","upvote_count":"1","comment_id":"810747"},{"upvote_count":"3","content":"Selected Answer: C\nVertex AI Experiments + Cloud Monitoring for the metrics. It's C!","comment_id":"766531","timestamp":"1672914900.0","poster":"ares81"},{"comment_id":"760927","upvote_count":"1","poster":"mymy9418","content":"Selected Answer: C\nI like C\nhttps://cloud.google.com/monitoring/mql","timestamp":"1672312860.0"},{"comment_id":"746999","upvote_count":"1","poster":"Pancy","timestamp":"1671182580.0","content":"C: Google has already provided inhouse monitoring mechanism so no need to query or use any other tool. https://cloud.google.com/bigquery/docs/monitoring"},{"poster":"Mohamed_Mossad","content":"https://www.kubeflow.org/docs/components/pipelines/introduction/#what-is-kubeflow-pipelines","timestamp":"1654374780.0","upvote_count":"1","comment_id":"611567"},{"content":"Selected Answer: A\nkubeflow pipelines has already experiment tracking API , so A is the correct , B is valid also but the question states \"minimizing manual effort\"","upvote_count":"2","comment_id":"611560","comments":[],"timestamp":"1654372920.0","poster":"Mohamed_Mossad"},{"content":"For me A is wrong because it is not \"rapidly\", I like B more than C","poster":"fdmenendez","upvote_count":"1","timestamp":"1642011540.0","comment_id":"522367"},{"content":"Selected Answer: A\nI think A is correct unless we are using Bigquery ML to create our models, we can select C","timestamp":"1641355920.0","poster":"NamitSehgal","upvote_count":"2","comment_id":"517157"},{"comment_id":"473917","upvote_count":"4","timestamp":"1636292880.0","poster":"ramen_lover","content":"Answer A.\n\n> \"Kubeflow Pipelines supports the export of scalar metrics. You can write a list of metrics to a local file to describe the performance of the model. The pipeline agent uploads the local file as your run-time metrics. You can view the uploaded metrics as a visualization in the Runs page for a particular experiment in the Kubeflow Pipelines UI.\"\nhttps://www.kubeflow.org/docs/components/pipelines/sdk/pipelines-metrics/"},{"timestamp":"1633788420.0","comment_id":"459664","content":"Let me put a strong word for answer D here: \n\nA: While yes, experiments would work perfectly, setting up KFP is time-consuming, and exporting the metrics file is manual work. \nB: AI Platform Training is again time-consuming provided we are not talking about the prebuilt-algorithms since they want to experiment with model architecture. And yes BigQuery is overkill.\nC: Same case from answer B for AI Platform Training applies here and Cloud Monitoring is for infrastructure monitoring. While I image it can be used for ML model performance it is again, timely. \nD: Setting up a notebook is fast and easy. Model architecture – no problem, different features – bring it. Once happy, can be easily productionized and expand with different services. Google Sheets is the easiest to use and write to. \n\nAnd beside, reading the google-recommended best practices, they say use notebooks to evaluate and understand your models. \n\nhttps://cloud.google.com/architecture/ml-on-gcp-best-practices#use-notebooks-to-evaluate-and-understand-your-models","comments":[{"timestamp":"1647588600.0","comment_id":"570307","upvote_count":"3","poster":"baimus","content":"D might be what you personally would do, and find it useful, but A is absolutely the expected google answer."},{"comment_id":"480263","upvote_count":"3","timestamp":"1637181900.0","content":"D is screaming \"manual effort\". I don't think that is a good alternative.","poster":"kaike_reis"},{"comment_id":"480275","poster":"kaike_reis","upvote_count":"2","timestamp":"1637183160.0","content":"Kubeflow Pipelines it's the complete solution. Looking for the documentations provided in this question, you going to see: \"easy experimentation\" and \"API for metric monitoring\". So (A) is the correct one: all the requirements in one solution means lower effort and manual intervention"}],"upvote_count":"2","poster":"george_ognyanov"},{"content":"C - BQ is an overkill?","timestamp":"1625890680.0","poster":"ralf_cc","upvote_count":"2","comments":[{"content":"Involves manual effort: imagine that every new experiment you need to access your BQ. It's unpractical.","upvote_count":"1","timestamp":"1637183040.0","poster":"kaike_reis","comment_id":"480272"},{"comment_id":"416662","poster":"sensev","content":"In my opinion should be A. Using cloud monitoring to log the metrics is not necessary since this is already possible via Kubeflow Pipeline","timestamp":"1627538040.0","upvote_count":"4"}],"comment_id":"403102"}],"answer_description":"","timestamp":"2021-07-10 06:18:00","question_text":"Your data science team needs to rapidly experiment with various features, model architectures, and hyperparameters. They need to track the accuracy metrics for various experiments and use an API to query the metrics over time. What should they use to track and report their experiments while minimizing manual effort?"}],"exam":{"isBeta":false,"isMCOnly":true,"lastUpdated":"11 Apr 2025","isImplemented":true,"name":"Professional Machine Learning Engineer","id":13,"provider":"Google","numberOfQuestions":304},"currentPage":52},"__N_SSP":true}