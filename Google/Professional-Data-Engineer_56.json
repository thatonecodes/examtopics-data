{"pageProps":{"questions":[{"id":"j3MyLBucYaBzWQ7ydMEP","choices":{"C":"Retry the query every second until it comes back online to minimize staleness of data.","A":"Issue a command to restart the database servers.","B":"Retry the query with exponential backoff, up to a cap of 15 minutes.","D":"Reduce the query frequency to once every hour until the database comes back online."},"answer":"B","unix_timestamp":1584258180,"topic":"1","isMC":true,"url":"https://www.examtopics.com/discussions/google/view/16639-exam-professional-data-engineer-topic-1-question-6/","answer_ET":"B","answer_images":[],"question_text":"Your weather app queries a database every 15 minutes to get the current temperature. The frontend is powered by Google App Engine and server millions of users. How should you design the frontend to respond to a database failure?","answers_community":["B (100%)"],"timestamp":"2020-03-15 08:43:00","answer_description":"","question_images":[],"discussion":[{"content":"Correct answer is B. App engine create applications that use Cloud SQL database connections effectively. Below is what is written in google cloud documnetation.\n\nIf your application attempts to connect to the database and does not succeed, the database could be temporarily unavailable. In this case, sending too many simultaneous connection requests might waste additional database resources and increase the time needed to recover. Using exponential backoff prevents your application from sending an unresponsive number of connection requests when it can't connect to the database.\n\nThis retry only makes sense when first connecting, or when first grabbing a connection from the pool. If errors happen in the middle of a transaction, the application must do the retrying, and it must retry from the beginning of a transaction. So even if your pool is configured properly, the application might still see errors if connections are lost.\n\nreference link is https://cloud.google.com/sql/docs/mysql/manage-connections","comment_id":"213170","upvote_count":"54","timestamp":"1604546640.0","poster":"Radhika7983"},{"upvote_count":"12","timestamp":"1594997580.0","comment_id":"137289","poster":"llamaste","content":"https://cloud.google.com/sql/docs/mysql/manage-connections#backoff"},{"poster":"willyunger","comment_id":"1399883","timestamp":"1742253000.0","upvote_count":"1","content":"Selected Answer: B\nExponential backoff avoids swamping the server. Higher rates may only make problem worse. Front-end should not have option to restart DB."},{"comment_id":"1339956","upvote_count":"1","poster":"cqrm3n","timestamp":"1736782440.0","content":"Selected Answer: B\nWe should use exponential backoff because it reduces load on the failing database, optimizes retry timing and is the industry best practice. Exponential backoff is a retry strategy where the wait time between retry increase exponentially. By gradually increasing the retry interval, the system avoids wasting resources on immediate retries when the database is likely still down."},{"timestamp":"1727154420.0","comment_id":"1050468","poster":"rtcpost","upvote_count":"3","content":"Selected Answer: B\nExponential backoff is a commonly used technique to handle temporary failures, such as a database server becoming temporarily unavailable. This approach retries the query, initially with a short delay and then with increasingly longer intervals between retries. Setting a cap of 15 minutes ensures that you don't excessively burden your system with constant retries.\n\nOption C (retrying the query every second) can be too aggressive and may lead to excessive load on the server when it comes back online.\n\nOption D (reducing the query frequency to once every hour) would result in significantly stale data and a poor user experience, which is generally not desirable for a weather app.\n\nOption A (issuing a command to restart the database servers) is not a suitable action for a frontend component and might not address the issue effectively. Database server restarts should be managed as a part of the infrastructure and not initiated by the frontend."},{"upvote_count":"2","poster":"samdhimal","timestamp":"1727154360.0","comment_id":"529920","comments":[{"comment_id":"784802","timestamp":"1674431760.0","content":"Exponential backoff with a cap is a common technique used to handle temporary failures, such as database outages. In this approach, the frontend will retry the query with increasing intervals (e.g., 1s, 2s, 4s, 8s, etc.) up to a maximum interval (in this case, 15 minutes), this will help to avoid overwhelming the database servers with too many requests at once, and minimize the impact of the failure on the users.\n\nOption A, is not recommended because it's not guaranteed that restarting the database servers will fix the problem, it could be a network or a configuration problem and it could cause more downtime.\n\nOption C is not recommended because it could cause too many requests to be sent to the server, overwhelming the database and causing more downtime.\n\nOption D is not recommended because reducing the query frequency too much would result in stale data, and users will not receive the most up-to-date information.","poster":"samdhimal","upvote_count":"2"}],"content":"correct answer -> Retry the query with exponential backoff, up to a cap of 15 minutes.\n\nIf your application attempts to connect to the database and does not succeed, the database could be temporarily unavailable. In this case, sending too many simultaneous connection requests might waste additional database resources and increase the time needed to recover. Using exponential backoff prevents your application from sending an unresponsive number of connection requests when it can't connect to the database."},{"content":"Selected Answer: B\nRetries with exponential backoff seems like the most efficient option in this scenario","timestamp":"1699380660.0","poster":"RT_G","comment_id":"1065065","upvote_count":"1"},{"content":"Selected Answer: B\nCorrect answer is B","upvote_count":"1","timestamp":"1698981360.0","poster":"rocky48","comment_id":"1061045"},{"upvote_count":"1","poster":"gudguy1a","content":"Selected Answer: B\ngood answer, good answer @radhika7983.","timestamp":"1693919160.0","comment_id":"999527"},{"poster":"Datardp","upvote_count":"1","content":"B is anser","comment_id":"916192","timestamp":"1686051720.0"},{"content":"Selected Answer: B\nI agree with the exponential backoff technique, even thoght I do not see why 15 minutes should be a desired choice.","timestamp":"1684502820.0","comments":[{"poster":"vaga1","timestamp":"1684503000.0","content":"I guess that when u have failed after 15 minutes, your app must go through a serious review before being used again, since it is not able to provide the updated results as quickly as desired.","upvote_count":"1","comment_id":"901968"}],"poster":"vaga1","upvote_count":"1","comment_id":"901965"},{"poster":"yafsong","content":"Truncated exponential backoff is a standard error-handling strategy for network applications. In this approach, a client periodically retries a failed request with increasing delays between requests","comment_id":"746054","upvote_count":"4","timestamp":"1671107040.0"},{"poster":"hiromi","timestamp":"1668775500.0","content":"Selected Answer: B\nB is right","comment_id":"721282","upvote_count":"1"},{"upvote_count":"1","poster":"shiv14","content":"Selected Answer: B\nAccording to the documentation","comment_id":"546452","timestamp":"1644756660.0"},{"upvote_count":"1","timestamp":"1642164300.0","content":"B is Correct; this question appeared in Cloud Architect exam also","poster":"deep_ROOT","comment_id":"523529"},{"poster":"MaxNRG","content":"B, \nbackoff is a standard error handling strategy for network applications in which a client periodically retries a failed request with increasing delays between requests. Clients should use truncated exponential backoff for all requests to Cloud Storage that return HTTP 5xx and 429 response codes, including uploads and downloads of data or metadata.","upvote_count":"2","timestamp":"1636309620.0","comment_id":"474004"},{"content":"Ans: B","comment_id":"462018","poster":"anji007","upvote_count":"1","timestamp":"1634215200.0"},{"content":"Vote for B.\n\nhttps://cloud.google.com/iot/docs/how-tos/exponential-backoff","timestamp":"1617576360.0","upvote_count":"3","poster":"sumanshu","comment_id":"328272"},{"poster":"lbhhoya82","comment_id":"319782","content":"Correct : B","upvote_count":"1","timestamp":"1616647680.0"},{"content":"B:\nhttps://cloud.google.com/sql/docs/mysql/manage-connections","upvote_count":"1","poster":"daghayeghi","timestamp":"1615169160.0","comment_id":"305454"},{"timestamp":"1614520260.0","comment_id":"300810","content":"B is correct answer.\nhttps://cloud.google.com/sql/docs/mysql/manage-connections","upvote_count":"3","poster":"sid091"},{"comment_id":"284920","upvote_count":"2","content":"Correct B","timestamp":"1612626060.0","poster":"naga"},{"comment_id":"159789","upvote_count":"2","timestamp":"1597654740.0","content":"Option B: Its the best way to connect the database again in case of failure","poster":"PRABHUKKARTHI"},{"comment_id":"72550","upvote_count":"4","poster":"Nidie","content":"https://cloud.google.com/sql/docs/mysql/manage-connections","timestamp":"1586410860.0"}],"exam_id":11,"question_id":276},{"id":"xBUAxb5lbh5IxA6QUHbU","topic":"1","question_text":"You launched a new gaming app almost three years ago. You have been uploading log files from the previous day to a separate Google BigQuery table with the table name format LOGS_yyyymmdd. You have been using table wildcard functions to generate daily and monthly reports for all time ranges. Recently, you discovered that some queries that cover long date ranges are exceeding the limit of 1,000 tables and failing. How can you resolve this issue?","unix_timestamp":1584793680,"discussion":[{"content":"should be B\nhttps://cloud.google.com/bigquery/docs/creating-partitioned-tables#converting_date-sharded_tables_into_ingestion-time_partitioned_tables","upvote_count":"38","timestamp":"1616329680.0","comments":[{"upvote_count":"1","poster":"jin0","timestamp":"1709106120.0","content":"is it already partitioned? there is a table [table]_yyyymmdd it seems to partitioned by date from log files. but I confuse why D. is not a answer? if there is only reason to fail from query that exceeding 1,000 tables then I think creating views could be solution because querying views containing under 1,000 tables by a view could be queried.","comment_id":"824525"},{"comment_id":"126770","comments":[{"timestamp":"1676539380.0","content":"https://cloud.google.com/bigquery/docs/partitioned-tables provides that info you are looking for. Shortly, partitioning performs better than sharding (PREFIX_yymmdd). and it is easy and supported that you can convert sharded tables into ingestion-time partitioned table.\nSo, B is only option and better one.","upvote_count":"3","poster":"Tanzu","comment_id":"548443"},{"comment_id":"459283","timestamp":"1665241920.0","poster":"vholti","content":"The question mentions tables are sharded. So B is more appropriate answer I think.\nhttps://cloud.google.com/bigquery/docs/creating-partitioned-tables#convert-date-sharded-tables","upvote_count":"5"},{"comments":[{"comment_id":"454337","timestamp":"1664471040.0","upvote_count":"12","content":"you are right.\n\nPartitioning versus sharding\nTable sharding is the practice of storing data in multiple tables, using a naming prefix such as [PREFIX]_YYYYMMDD.\n\nPartitioning is recommended over table sharding, because partitioned tables perform better. With sharded tables, BigQuery must maintain a copy of the schema and metadata for each table. BigQuery might also need to verify permissions for each queried table. This practice also adds to query overhead and affects query performance.\n\nIf you previously created date-sharded tables, you can convert them into an ingestion-time partitioned table.","poster":"Chelseajcole"}],"upvote_count":"6","comment_id":"265595","poster":"g2000","content":"keyword is single","timestamp":"1642000680.0"}],"content":"The above link does mention about shard ing benefits but only about partition tables.\nA is correct.","timestamp":"1625483100.0","poster":"Rajuuu","upvote_count":"5"}],"comment_id":"66509","poster":"[Removed]"},{"timestamp":"1616893380.0","comments":[{"poster":"lgdantas","comments":[{"poster":"Tumri","content":"https://cloud.google.com/bigquery/docs/partitioned-tables#dt_partition_shard","comment_id":"436922","upvote_count":"7","timestamp":"1662016140.0"}],"upvote_count":"2","content":"Can you please share the reference?","timestamp":"1630567620.0","comment_id":"171834"}],"poster":"[Removed]","content":"Answer: B\nDescription: Google says that when you have multiple wildcard tables, best option is to shard it into single partitioned table. Time and cost efficient","upvote_count":"26","comment_id":"68719"},{"comment_id":"1324994","poster":"jatinbhatia2055","content":"Selected Answer: B\nSharded tables, like LOGS_yyyymmdd, are useful for managing data, but querying across a long date range with table wildcards can lead to inefficiencies and exceed the 1,000 table limit in BigQuery. Instead of using multiple sharded tables, you should consider converting these into a partitioned table.\n\nA partitioned table allows you to store all the log data in a single table, but logically divides the data into partitions (e.g., by date). This way, you can efficiently query data across long date ranges without hitting the 1,000 table limit.","upvote_count":"1","timestamp":"1733915700.0"},{"poster":"Oleksandr0501","timestamp":"1714019160.0","comment_id":"879921","content":"Selected Answer: B\ngpt: Thank you for your feedback and additional information. You are correct that partitioned tables have a limit of 4,000 partitions, so partitioning tables by date could potentially run into this limit in the future. In this case, option B, converting sharded tables into a single partitioned table, could be a reasonable solution to avoid exceeding the maximum number of tables in BigQuery.\n\nAs you mentioned, sharded tables require additional metadata and permissions verification, which can impact query performance. Converting sharded tables into a single partitioned table can improve performance and reduce query overhead.\n\nTherefore, based on the information provided, option B seems to be the most appropriate solution for avoiding the limit of 1,000 tables in BigQuery and optimizing query performance.","upvote_count":"1"},{"timestamp":"1711239540.0","poster":"luks_skywalker","content":"The question seems pretty badly written. One important thing to remember is that partitioned tables also have a limit of 4000 partitions (https://cloud.google.com/bigquery/docs/partitioned-tables#ingestion_time), so moving everything to one table would just delay the problem. However, option A is not clear on how it will be done. One table per year with daily partitions? Best solution as no limit will be reached. One table per day? Then we have the same 1000 tables problem.\nAll things considered I'll stick to B, simply because the problem will definitely be solved for the next few years, so I'd say it's a reasonable solution.","comment_id":"848800","upvote_count":"2"},{"timestamp":"1706283060.0","upvote_count":"2","comment_id":"788861","poster":"PolyMoe","content":"Selected Answer: B\nAnswer is B.\nTable sharding is the practice of storing data in multiple tables, using a naming prefix such as [PREFIX]_YYYYMMDD.\nPartitioning is recommended over table sharding, because partitioned tables perform better. With sharded tables, BigQuery must maintain a copy of the schema and metadata for each table. BigQuery might also need to verify permissions for each queried table. This practice also adds to query overhead and affects query performance.\nIn answer A. we still are creating tableS (even though partioned). So we still facing the issue of max 1000 tables. In B. we have only ONE table (partioned)"},{"upvote_count":"3","timestamp":"1705978560.0","comment_id":"784894","poster":"samdhimal","content":"Why not A?\nBy converting all daily log tables into date-partitioned tables, you can take advantage of partition pruning to limit the number of tables that need to be scanned during a query. Partition pruning allows BigQuery to skip scanning partitions that are not within the date range specified in the query, thus reducing the number of tables that need to be scanned and can help to avoid reaching the 1,000 table limit.\nA Seems like the correct answer but I can be wrong..."},{"upvote_count":"1","content":"Selected Answer: B\nB. Convert the sharded tables into a single partitioned table\nIt was a sharded Table (format is the HINT here); converting to partition table is the option.\nAlso as per GCP its recommended to use Partition over Sharding","timestamp":"1704997920.0","poster":"RoshanAshraf","comment_id":"772802"},{"comment_id":"769274","poster":"korntewin","content":"Selected Answer: A\nI chose option A. From all the comments I have seen, there are various things that are misunderstood.\n1. Option A is a single table with multiple shards! Google does recommend to use partition rather than shard as it has a better performance (https://cloud.google.com/bigquery/docs/partitioned-tables#dt_partition_shard)\n2. Option B is a single table with single partition! Single partition is a no for large table","timestamp":"1704708780.0","upvote_count":"1"},{"poster":"DipT","timestamp":"1702729260.0","upvote_count":"1","content":"Selected Answer: B\nhttps://cloud.google.com/bigquery/docs/partitioned-tables","comment_id":"747164"},{"upvote_count":"1","timestamp":"1702590840.0","content":"Selected Answer: B\nOption A - already doing same loading data in separate table daily and reached 1000 table limit. \nOption B - Use wild card to query the data\nOption C & D - make no sense","poster":"DGames","comment_id":"745481"},{"poster":"odacir","timestamp":"1701939840.0","upvote_count":"1","comment_id":"737628","content":"its B.\nA - Even if you have 100+ partitioned tables, you still have the limit of less than 1000 tables. So this doesn't work for this problem.\nC It's a no sense. Cache its 24h for every table that has been query in the last 24 and has no changes. Also, cache is not support with wildcard multiple tables.\nD Will not work because it's a recursive issue. You still will have 100+ tables, beam query\nB will work, you materialize in only one table, so will be working perfectly."},{"content":"Selected Answer: B\nConvert MANY sharded tables into a single ONE (partitioned) table","poster":"Nirca","upvote_count":"2","comment_id":"692229","timestamp":"1697039640.0"},{"comment_id":"653541","poster":"rrr000","timestamp":"1693321860.0","content":"selecting for daily/monthly data from one single partition will be very expensive. I think A is the best answer","upvote_count":"1"},{"comment_id":"616867","timestamp":"1686848340.0","poster":"Preemptible_cerebrus","content":"Selected Answer: B\nC'mon, how much time are you going to take to partition every single table you have? second point and the most important, you have a table for every SINGLE DAY \"LOGS_YYYYMMDD\" partitioning every table will end on scanning all the records of each table when you query them by date ranges using the wildcards, there will be no difference on time-partitioning each table versus consuming them as described.","upvote_count":"2"},{"content":"If you follow option A, you will end up with the same amount of tables, e.g 1500 tables, though they will all be partitioned, which is not helpful.\nOption B takes all the sharded tables and makes one large partitioned table.","timestamp":"1686314820.0","comments":[{"poster":"rrr000","comment_id":"653542","timestamp":"1693322100.0","content":"Partitions are not tables. The issue is not performance. It is the limit imposed by bq regarding how many tables you can query.","upvote_count":"1"}],"upvote_count":"1","poster":"AmirN","comment_id":"613997"},{"timestamp":"1683202140.0","comment_id":"596777","content":"Selected Answer: B\nIt's B\nhttps://cloud.google.com/bigquery/docs/creating-partitioned-tables#converting_date-sharded_tables_into_ingestion-time_partitioned_tables","poster":"mihaioff","upvote_count":"1"},{"timestamp":"1672845240.0","content":"Selected Answer: B\nPartitioning > table sharding:\nhttps://cloud.google.com/bigquery/docs/partitioned-tables#dt_partition_shard","poster":"medeis_jar","comment_id":"516759","upvote_count":"2"},{"poster":"kishanu","content":"Its A, as google, recommends the best practice to use partition over sharding.","timestamp":"1671470580.0","comment_id":"505003","upvote_count":"2"},{"poster":"hendrixlives","timestamp":"1671336120.0","upvote_count":"3","comment_id":"504008","content":"Selected Answer: B\nB\nPartitioning is recommended over table sharding:\nhttps://cloud.google.com/bigquery/docs/partitioned-tables#dt_partition_shard"},{"timestamp":"1665866340.0","comment_id":"462800","upvote_count":"1","content":"Ans: B\nAt one point both A & B looks same for me.","poster":"anji007"},{"upvote_count":"5","content":"A- all tables to partioned tables (so each table will have one partition which is not correct)\n\nB is the right one","poster":"[Removed]","timestamp":"1665838020.0","comment_id":"462593"},{"timestamp":"1659889200.0","poster":"sandipk91","comment_id":"421287","content":"B is the correct answer","upvote_count":"1"},{"comment_id":"397759","timestamp":"1656868320.0","upvote_count":"1","content":"My take is B\n The current architecture is \"Sharded Tables\" (as the log files are stored in a separate BQ Table with table name format as LOGS_yyymmdd). To resolve the queries failing issue, we need to convert this Sharded Tables to partitioned table\nhttps://cloud.google.com/bigquery/docs/creating-partitioned-tables#convert-date-sharded-tables","poster":"raf2121"},{"content":"Vote for 'B'\n\nPartitioned tables perform better than tables sharded by date.\n\nhttps://stackoverflow.com/questions/39514899/bigquery-shard-vs-bigquery-partition","upvote_count":"2","poster":"sumanshu","timestamp":"1656444420.0","comment_id":"393213"},{"timestamp":"1647141420.0","content":"Correct A","upvote_count":"3","poster":"BhupiSG","comment_id":"309367"},{"comment_id":"260511","timestamp":"1641410700.0","upvote_count":"2","content":"Should be b. BigQuery has a limit of 4,000 partitions for a partitioned table","poster":"fabenavideso"},{"upvote_count":"2","timestamp":"1634224560.0","poster":"Faizero","comment_id":"199881","content":"should be A, confuse on the point of 1,000 table , we create partitioned_table , then it will be 1,000 partitioned , not table.","comments":[{"upvote_count":"1","timestamp":"1697039580.0","content":"hence = B","comment_id":"692228","poster":"Nirca"}]},{"upvote_count":"1","comment_id":"190009","comments":[{"timestamp":"1656443940.0","poster":"sumanshu","comment_id":"393205","content":"This is the Previous question comment.","upvote_count":"3"}],"content":"Should be D.\n1. here we are using app engine\n2. Transact data\n3. Bi tool.\nso all of these features exists in Datastore","poster":"Ankush_j","timestamp":"1632968880.0"},{"timestamp":"1632302460.0","upvote_count":"4","poster":"SteelWarrior","content":"Should be B. Date-partitioned tables will create same number of tables as the current situation and hence of no help. With single partitioned tables we will get one table as a replacement for 1000 tables and hence solve the problem.","comment_id":"184297"},{"timestamp":"1632138540.0","comment_id":"182939","upvote_count":"5","content":"Should be B\nThe reason for failing is that the number of tables exceeds 1,000. (over 3 years, 365*3)\nTo solve this issue, have to reduce the number of tables.\nHowever, A does not change the number of tables. \n(the number of daily batch tables = the number of date-partitioned tables)\nAs shown in B, it must be changed to single table(not tables) partitioned by date.\nTherefore, the correct answer is B","poster":"bartshim","comments":[{"comments":[{"comment_id":"289931","timestamp":"1644805320.0","upvote_count":"1","content":"no, it will create partitions by date in a single table","poster":"karthik89"}],"content":"A changes number of table.","timestamp":"1641836160.0","upvote_count":"1","poster":"tikna","comment_id":"264158"}]},{"comment_id":"175045","timestamp":"1631001600.0","poster":"Tanmoyk","content":"A should be the proper answer","upvote_count":"1"},{"content":"B\nf you have previously created date-sharded tables, you can convert the entire set of related tables into a single ingestion-time partitioned table by using the partition command in the bq command-line tool","poster":"atnafu2020","comment_id":"162518","comments":[{"content":"not single partition but time-partitioned/date so A must be right answer ignore my B","timestamp":"1629497460.0","comment_id":"162519","upvote_count":"3","poster":"atnafu2020"}],"upvote_count":"4","timestamp":"1629497280.0"},{"poster":"Archy","comment_id":"150709","timestamp":"1628109480.0","upvote_count":"2","content":"A, as currently bigquery has 4000 partition limit.","comments":[{"poster":"lgdantas","content":"Yes, it would support more than 10 years of logs.","timestamp":"1630567920.0","comment_id":"171841","upvote_count":"1","comments":[{"comment_id":"171850","content":"So, B.","upvote_count":"2","poster":"lgdantas","comments":[{"timestamp":"1630568400.0","content":"oops! A!!! Date-partitioned!","poster":"lgdantas","upvote_count":"1","comment_id":"171852"}],"timestamp":"1630568220.0"}]}]},{"timestamp":"1625734680.0","poster":"pshemol","comment_id":"129597","upvote_count":"4","content":"A doesn't decrease number of tables (number of them stays the same) in long range query \"exceeding the limit of 1,000 tables\"\nShould be B"},{"content":"\"A\" should be the correct answer. \"B\" speaks about ingestion time partition table - implying a real time ingest of data. You can do batch data load and use a data partitioned table.","comment_id":"127797","poster":"dg63","timestamp":"1625574180.0","upvote_count":"1"}],"question_id":277,"exam_id":11,"isMC":true,"answers_community":["B (94%)","6%"],"answer_ET":"B","answer_description":"","timestamp":"2020-03-21 13:28:00","choices":{"C":"Enable query caching so you can cache data from previous months","D":"Create separate views to cover each month, and query from these views","B":"Convert the sharded tables into a single partitioned table","A":"Convert all daily log tables into date-partitioned tables"},"answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/17096-exam-professional-data-engineer-topic-1-question-60/","question_images":[],"answer":"B"},{"id":"sSJuZ1nDHAtPwE94CuEp","topic":"1","answer_images":[],"answers_community":["B (100%)"],"timestamp":"2020-03-16 15:32:00","exam_id":11,"discussion":[{"timestamp":"1584369120.0","poster":"jvg637","content":"B. (Hadoop/Spark jobs are run on Dataproc, and the pre-emptible machines cost 80% less)","upvote_count":"47","comment_id":"64739"},{"content":"I think the answer should be B:\n\nhttps://cloud.google.com/dataproc/docs/concepts/compute/preemptible-vms","upvote_count":"18","poster":"rickywck","comment_id":"65042","timestamp":"1584426240.0"},{"timestamp":"1734167160.0","comment_id":"1326397","poster":"AmitK121981","content":"Selected Answer: B\nall are saying its pre-emptibles. but spot VMs can only be used in secondary worker, not on master and primary worker so not sure why this will cause savings, and secondary workers are not mandatory too","upvote_count":"1"},{"poster":"theseawillclaim","upvote_count":"2","comment_id":"954534","content":"I believe it might be \"B\", but what if the job is mission critical? \nPre-emptible VMs would be of no use.","timestamp":"1689619020.0","comments":[{"content":"Mission critical workloads can't be needed \"weekly\"","comment_id":"1256298","timestamp":"1722084060.0","upvote_count":"1","poster":"enivid007"}]},{"content":"I believe Exam Topics ought to provide brief explanation or supporting link to picked correct answers such as this one. Option A may be correct from the view point that Dataflow is a Serverless service that is fast, cost-effective and the fact that Preemptible VMs though can give large price discount may not always be available. It will be great to know the reason(s) behind Exam Topic selected option.","comment_id":"878927","timestamp":"1682298960.0","upvote_count":"8","poster":"abi01a"},{"content":"B. Use pre-emptible virtual machines (VMs) for the cluster\n\nUsing pre-emptible VMs allows you to take advantage of lower-cost virtual machine instances that may be terminated by Google Cloud after a short period of time, typically after 24 hours. These instances can be a cost-effective way to handle workloads that can be interrupted, such as batch processing jobs like the one described in the question.\n\nOption A is not ideal, as it would require you to migrate the workload to Google Cloud Dataflow, which may cause additional complexity and would not address the issue of cost optimization.\nOption C is not ideal, as it would require you to use a higher-memory node which would increase the cost.\nOption D is not ideal, as it would require you to use SSDs on the worker nodes which would increase the cost.\n\nUsing pre-emptible VMs is a better option as it allows you to take advantage of lower-cost virtual machine instances and handle workloads that can be interrupted, which can help to optimize the cost of the cluster.","timestamp":"1674442920.0","poster":"samdhimal","comment_id":"784897","upvote_count":"4"},{"poster":"Rodolfo_Marcos","upvote_count":"2","content":"What is happening with this test \"correct answer\" a lot of times it doesn't make any sense. As this one... Clear it's B","timestamp":"1672945320.0","comment_id":"766964"},{"content":"Selected Answer: B\nUsing preemtible machines are cost effective , and because is suitable for a job mentioned here as it is fault tolerant .","timestamp":"1671193560.0","comment_id":"747168","poster":"DipT","upvote_count":"2"},{"poster":"DGames","timestamp":"1671056160.0","upvote_count":"1","comment_id":"745495","content":"Selected Answer: B\nUser Pre-emptible VM machine and save process cost, and question want simple solution."},{"comment_id":"737693","poster":"odacir","upvote_count":"1","timestamp":"1670406720.0","content":"Selected Answer: B\nA- Data flow it's not cost-effective in comparison with dataproc\nB- Preemptible VM instances are available at much lower price—a 60-91% discount—compared to the price of standar, so this is the answer \nC and D are more expensive."},{"timestamp":"1662565200.0","poster":"Remi2021","upvote_count":"1","comment_id":"662654","content":"Selected Answer: B\nB is right way to go"},{"comment_id":"609410","content":"Selected Answer: B\nPreemptible workers are the default secondary worker type. They are reclaimed and removed from the cluster if they are required by Google Cloud for other tasks. Although the potential removal of preemptible workers can affect job stability, you may decide to use preemptible instances to lower per-hour compute costs for non-critical data processing or to create very large clusters at a lower total cost \n\nhttps://cloud.google.com/dataproc/docs/concepts/compute/secondary-vms","timestamp":"1653935160.0","poster":"FrankT2L","upvote_count":"1"},{"upvote_count":"4","poster":"Remi2021","content":"B is teh right answer. examtopics update your answers or make your site free again.","comment_id":"573754","timestamp":"1648052040.0"},{"upvote_count":"4","comment_id":"568849","poster":"OmJanmeda","content":"Selected Answer: B\nB is right answer.\nmy experience is not good with Examtopics, so many wrong answers.","timestamp":"1647416760.0"},{"comment_id":"540098","upvote_count":"2","content":"Selected Answer: B\nB should be the right answer. \nI am amazed that almost 60% of the marked answers on the site are wrong.","timestamp":"1643931060.0","poster":"Yaa"},{"poster":"byash1","comment_id":"531332","upvote_count":"1","timestamp":"1643031840.0","content":"Ans : B, \nhere we are checking on reducing cost, so pre-emptiable machines are best choice"},{"timestamp":"1641309480.0","poster":"medeis_jar","comment_id":"516761","upvote_count":"4","content":"Selected Answer: B\n\"this workload can run in approximately 30 minutes on a 15-node cluster,\"\nso you need performance for only 30 mins -> preemptible VMs\n\nhttps://cloud.google.com/dataproc/docs/concepts/compute/preemptible-vms"},{"upvote_count":"1","poster":"MaxNRG","comment_id":"505630","timestamp":"1640025060.0","content":"Selected Answer: B\nA is not valid, for apache spark jobs dataproc y the best choice.\nC and D are not correct, that might speed up the job or not.\nFor sure if we use pre-emptible machines this will be cheaper and since we don’t have severe time restriction…thats the one. B"},{"upvote_count":"2","comment_id":"504011","content":"Selected Answer: B\nB\nUse pre-emptible virtual machines (VMs) for the cluster: this will reduce costs of the dataproc cluster. The other options will increase the costs. The work is required to run weekly and takes 15 min, so the performance is not an issue.\n\nThe option \"A\" of migrating to dataflow will impose a migration of the technology from spark to beam, which will be costly and even difficult if the developers are not experienced in Beam. This can be considered for future evolution, but it is out of the scope of the requirements stated in the question.","poster":"hendrixlives","timestamp":"1639800540.0"},{"content":"Why there are so many wrong answers? Examtopics.com are you enjoying paid subscription by giving random answers from people?\nAns: B","poster":"JG123","upvote_count":"2","comment_id":"487537","timestamp":"1637949660.0"},{"content":"Selected Answer: B\nB. Pre-emptible machines are cheaper","poster":"ArturVemado","upvote_count":"2","comment_id":"484393","timestamp":"1637600820.0"},{"poster":"anji007","content":"Ans: B\nA: they want to run as Spark job. So dataflow doesn't applicable.\nC & D: job may run quicker but these options are costlier...also as per question jobs are running for 15 mins so need to bring down the run time.\nB: PVMs are 91% cheaper than standard VMs.","comment_id":"463243","upvote_count":"2","timestamp":"1634411520.0"},{"upvote_count":"3","timestamp":"1628426040.0","poster":"sandipk91","content":"option B is the correct answer, option A doesn't make any sense...","comment_id":"421629"},{"timestamp":"1624912440.0","comment_id":"393223","content":"Vote for 'B'","upvote_count":"3","poster":"sumanshu"},{"comments":[{"timestamp":"1634599080.0","poster":"squishy_fishy","content":"The question already said \"\"recommended using Google Cloud Dataproc to execute this job\", so the answer is B.","upvote_count":"1","comment_id":"464333"},{"content":"Question asks about Cluster cost optimization. So data proc usage is required and costs can be reduced by using Preemptible VMs","poster":"navemula","timestamp":"1625234640.0","upvote_count":"4","comment_id":"396946"}],"comment_id":"377541","upvote_count":"1","poster":"koundi_aws","content":"I would go with A. Dataproc is not serverless and we need to consider the overhead of creating the cluster every time we need to run the job(or choose running persistent cluster which doesn't make any sense for a weekly job). Instead, it is far more easier and effective to go serverless and run the job in Dataflow at more or less same or lesser cost and probably better performance. The certification also looks for easier ways to implement solutions and not just the correct ones.","timestamp":"1623157320.0"},{"poster":"[Removed]","comment_id":"370363","timestamp":"1622395620.0","content":"The objective is saving cost. Dataflow is cheaper in this regard. \n\"Dataflow also provides an option with discounted CPU and memory pricing for batch processing. Flexible Resource Scheduling (FlexRS) combines regular and preemptible VMs in a single Dataflow worker pool, giving users access to cheaper processing resources. \"\nReference: https://cloud.google.com/dataflow/pricing","upvote_count":"1"},{"poster":"BhupiSG","content":"Answer: B","upvote_count":"1","timestamp":"1617283140.0","comment_id":"325869"},{"poster":"apnu","content":"B , yes for sure","timestamp":"1609316580.0","upvote_count":"3","comment_id":"255454"},{"comment_id":"246124","poster":"NamitSehgal","timestamp":"1608171780.0","upvote_count":"2","content":"B should be cost saving. Ofcourse SSD is for better performance but not int his case as job finish in 30 minutes or so."},{"upvote_count":"2","content":"B is the correct answer.","timestamp":"1606726620.0","comment_id":"230958","poster":"ceak"},{"comment_id":"184299","timestamp":"1600766760.0","poster":"SteelWarrior","upvote_count":"2","content":"Answer should be B. The workload in itself is already completing in very less time i.e. in 30 minutes so no optimization is required to reduce the same. However, we have to make sure that our cluster runs only for 30 mins in a week and hence we choose pre-emtible VMs."},{"content":"B Is correct","timestamp":"1597841820.0","poster":"haroldbenites","upvote_count":"2","comment_id":"161502"},{"timestamp":"1595366400.0","comment_id":"140632","upvote_count":"4","poster":"atnafu2020","content":"B\npre-emptible machines saves cost and allow compute and storage separation to save cost."},{"poster":"Rajuuu","comment_id":"126775","content":"Answer is B.\nCOST is the key factor .","timestamp":"1593947280.0","upvote_count":"3"},{"poster":"[Removed]","comment_id":"68721","upvote_count":"6","timestamp":"1585364760.0","content":"Answer: B\nDescription: Ask is cost, so preemptible vms will reduce the cost"},{"comment_id":"66529","poster":"[Removed]","timestamp":"1584798780.0","upvote_count":"6","content":"Answer: B"}],"answer_ET":"B","question_text":"Your analytics team wants to build a simple statistical model to determine which customers are most likely to work with your company again, based on a few different metrics. They want to run the model on Apache Spark, using data housed in Google Cloud Storage, and you have recommended using Google Cloud\nDataproc to execute this job. Testing has shown that this workload can run in approximately 30 minutes on a 15-node cluster, outputting the results into Google\nBigQuery. The plan is to run this workload weekly. How should you optimize the cluster for cost?","answer":"B","answer_description":"","url":"https://www.examtopics.com/discussions/google/view/16747-exam-professional-data-engineer-topic-1-question-61/","question_images":[],"isMC":true,"unix_timestamp":1584369120,"question_id":278,"choices":{"A":"Migrate the workload to Google Cloud Dataflow","D":"Use SSDs on the worker nodes so that the job can run faster","B":"Use pre-emptible virtual machines (VMs) for the cluster","C":"Use a higher-memory node so that the job runs faster"}},{"id":"PIinJUGTwxIeO7HswC2p","question_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/google/view/17104-exam-professional-data-engineer-topic-1-question-62/","answer_images":[],"answers_community":["C (88%)","6%"],"unix_timestamp":1584800160,"answer":"C","isMC":true,"question_id":279,"choices":{"B":"Set sliding windows to capture all the lagged data.","D":"Ensure every datasource type (stream or batch) has a timestamp, and use the timestamps to define the logic for lagged data.","C":"Use watermarks and timestamps to capture the lagged data.","A":"Set a single global window to capture all the data."},"question_text":"Your company receives both batch- and stream-based event data. You want to process the data using Google Cloud Dataflow over a predictable time period.\nHowever, you realize that in some instances data can arrive late or out of order. How should you design your Cloud Dataflow pipeline to handle data that is late or out of order?","exam_id":11,"timestamp":"2020-03-21 15:16:00","discussion":[{"comment_id":"68723","content":"Answer: C\nDescription: A watermark is a threshold that indicates when Dataflow expects all of the data in a window to have arrived. If new data arrives with a timestamp that's in the window but older than the watermark, the data is considered late data.","timestamp":"1616893860.0","upvote_count":"44","poster":"[Removed]"},{"timestamp":"1616336160.0","content":"Answer: C","comment_id":"66535","upvote_count":"18","poster":"[Removed]"},{"poster":"MikkelRev","comment_id":"1017066","timestamp":"1727285520.0","content":"Selected Answer: C\noption C: Use watermarks and timestamps to capture the lagged data.","upvote_count":"1"},{"comment_id":"784904","comments":[{"comment_id":"784905","upvote_count":"4","poster":"samdhimal","content":"Option A: Set a single global window to capture all the data is not a good idea because it may not allow for late or out-of-order data to be processed.\n\nOption B: Set sliding windows to capture all the lagged data is not suitable for the case where you want to process the data over a predictable time period. Sliding windows are used when you want to process data over a period of time that is continuously moving forward, not a fixed period.\n\nOption D: Ensure every datasource type (stream or batch) has a timestamp, and use the timestamps to define the logic for lagged data is a good practice but not a complete solution, because it only ensures that data is ordered correctly, but it does not account for data that may be late.","timestamp":"1705979160.0"}],"poster":"samdhimal","timestamp":"1705979160.0","upvote_count":"9","content":"C: Use watermarks and timestamps to capture the lagged data.\n\nWatermarks are a way to indicate that some data may still be in transit and not yet processed. By setting a watermark, you can define a time period during which Dataflow will continue to accept late or out-of-order data and incorporate it into your processing. This allows you to maintain a predictable time period for processing while still allowing for some flexibility in the arrival of data.\n\nTimestamps, on the other hand, are used to order events correctly, even if they arrive out of order. By assigning timestamps to each event, you can ensure that they are processed in the correct order, even if they don't arrive in that order."},{"content":"Answer is C:\n\nThere is no such thing as a sliding windows using by dataflow.","timestamp":"1705448100.0","upvote_count":"1","comments":[{"poster":"Mathew106","upvote_count":"1","comment_id":"959548","content":"The naming in Apache Beam is: Fixed, Sliding, Session\nIn Dataflow it's: Tumbling, Hopping, Session.\nI was very confused at first too when I saw \"hopping\" in a question.","timestamp":"1721654400.0"},{"content":"I highly doubt, DataFlow windowing is divided into three(3) types:\n\n1. Fixed\n2. Sliding \n3. Session","poster":"DeeData","timestamp":"1705825800.0","comment_id":"783122","upvote_count":"1"}],"poster":"desertlotus1211","comment_id":"778377"},{"timestamp":"1704419880.0","comment_id":"766193","content":"Answer is Use watermarks and timestamps to capture the lagged data.\n\nA watermark is a threshold that indicates when Dataflow expects all of the data in a window to have arrived. If new data arrives with a timestamp that's in the window but older than the watermark, the data is considered late data.","upvote_count":"1","poster":"AzureDP900"},{"poster":"DGames","timestamp":"1702593360.0","upvote_count":"2","comment_id":"745507","content":"Selected Answer: C\nWatermark is use for late date,"},{"comment_id":"715338","upvote_count":"3","poster":"[Removed]","timestamp":"1699629120.0","content":"Watermark doesn't solve the out-of-order data problem. It only solves the problem of late data. However, with D, you can use the timestamps to solve both problems (for instance, if you're storing incoming data in a table, you can easily insert any late data to its correct place a time-partionned table using the timestamp of the element)","comments":[{"timestamp":"1700454240.0","comments":[{"poster":"ovokpus","content":"C even says watermarks AND timestamps.","comment_id":"722383","upvote_count":"1","timestamp":"1700454300.0"}],"poster":"ovokpus","comment_id":"722382","content":"with watermarks, when the late data arrives, it goes into its rightful window and gets in order","upvote_count":"1"}]},{"comment_id":"609405","upvote_count":"1","timestamp":"1685470980.0","poster":"FrankT2L","content":"Selected Answer: B\nPreemptible workers are the default secondary worker type. They are reclaimed and removed from the cluster if they are required by Google Cloud for other tasks. Although the potential removal of preemptible workers can affect job stability, you may decide to use preemptible instances to lower per-hour compute costs for non-critical data processing or to create very large clusters at a lower total cost \n\nhttps://cloud.google.com/dataproc/docs/concepts/compute/secondary-vms","comments":[{"timestamp":"1685471160.0","content":"delete this answer. The answer belongs to another question","upvote_count":"8","poster":"FrankT2L","comment_id":"609409"}]},{"upvote_count":"1","poster":"Tanzu","content":"Selected Answer: A\nThat's why we have watermarks in apache beam.","timestamp":"1676563920.0","comment_id":"548747"},{"comment_id":"545841","upvote_count":"1","poster":"VishalBule","content":"Answer is C Use watermarks and timestamps to capture the lagged data.\n\nA watermark is a threshold that indicates when Dataflow expects all of the data in a window to have arrived. If new data arrives with a timestamp that's in the window but older than the watermark, the data is considered late data.","timestamp":"1676206020.0"},{"content":"Selected Answer: C\n\"Watermark in implementation is a monotonically increasing timestamp. When Beam/Dataflow see a record with an event timestamp that is earlier than the watermark, the record is treated as late data.\"","timestamp":"1672845720.0","comment_id":"516763","upvote_count":"3","poster":"medeis_jar"},{"comment_id":"505617","comments":[{"content":"C is, I think, the correct answer: Watermark is different from late data. Watermark in implementation is a monotonically increasing timestamp. When Beam/Dataflow see a record with an event timestamp that is earlier than the watermark, the record is treated as late data.\nI’ll try to explain: Late data is inherent to Beam’s model for out-of-order processing. What does it mean for data to be late? The definition and its properties are intertwined with watermarks that track the progress of each computation across the event time domain. The simple intuition behind handling lateness is this: only late input should result in late data anywhere in the pipeline.\nSo, is not easy to decide between C and D. If you ask me I’d say C since for D we ought to make some suppositions.","upvote_count":"3","comment_id":"505621","poster":"MaxNRG","timestamp":"1671560340.0"}],"poster":"MaxNRG","timestamp":"1671560280.0","upvote_count":"4","content":"Selected Answer: C\nA is a direct No, if data don’t have timestamp, we’ll only have the procesing time and not the “event time”.\nB is not either, sliding windows are not for this. Hopping|sliding windowing is useful for taking running averages of data, but not to process late data.\nD looks correct but has one concept missing, the watermark to know if the process time is ok with the event time or not. I’m not 100% sure is incorrect. If, since we have a “predictable time period”, might be this will do. I mean, if our dashboard is shown after the last input data has arrived (single global window), this should be ok. We’d have a “perfect watermark”. Anyway we’d need triggering ."},{"content":"Selected Answer: C\n\"Expert Verified\" but >50% questions have random answer. \"Sliding window\" really? Please, this can be fixed easyly with our most voted answer. Of course, the correct answer is C.","upvote_count":"4","poster":"Jlozano","timestamp":"1671308640.0","comment_id":"503894"},{"comment_id":"487538","poster":"JG123","content":"Why there are so many wrong answers? Examtopics.com are you enjoying paid subscription by giving random answers from people?\nAns: C","upvote_count":"4","timestamp":"1669485840.0"},{"content":"Ans: C","poster":"anji007","comment_id":"463633","timestamp":"1666025460.0","upvote_count":"2"},{"poster":"safiyu","timestamp":"1660423260.0","comment_id":"424532","content":"Answer should be C. sliding windows are meant for calculating running average and not lagging data. Watermark is best for this purpose","upvote_count":"7"},{"poster":"sumanshu","comment_id":"393224","upvote_count":"4","content":"vote for 'C\"","timestamp":"1656448680.0"},{"content":"C is correct even though D is telling what C is doing. Keyword here are Window and timestamp.","comment_id":"246126","upvote_count":"4","timestamp":"1639707900.0","poster":"NamitSehgal"},{"comment_id":"161506","poster":"haroldbenites","comments":[{"comment_id":"161511","timestamp":"1629378360.0","poster":"haroldbenites","content":"Although, C appear to be better. I think that Igo for the C.","upvote_count":"6"}],"upvote_count":"1","content":"B Is correct. Using windowing, we should to choice Sliding windows.","timestamp":"1629378000.0"},{"content":"C seems to be correct one:\n\nhttps://cloud.google.com/dataflow/docs/concepts/streaming-pipelines#watermarks","upvote_count":"6","poster":"saurabh1805","timestamp":"1629235860.0","comment_id":"160368"},{"upvote_count":"8","content":"We need a combination of window + watermark (timestamps) + trigger to treat the late data. So D.","timestamp":"1616436960.0","poster":"jvg637","comment_id":"67055"},{"content":"Answer C","upvote_count":"5","timestamp":"1616374440.0","comment_id":"66697","poster":"Rajokkiyam"}],"answer_ET":"C","answer_description":""},{"id":"tuwW5BNdThgiPnRtXxWG","topic":"1","answer_images":[],"answers_community":["A (80%)","B (20%)"],"timestamp":"2020-03-16 14:39:00","exam_id":11,"discussion":[{"poster":"jvg637","content":"For fitting a linear classifier when the data is in a circle use A.","comment_id":"64726","timestamp":"1584365940.0","upvote_count":"41"},{"poster":"[Removed]","timestamp":"1585365240.0","content":"Answer: A","comment_id":"68724","upvote_count":"15"},{"upvote_count":"2","poster":"SamuelTsch","content":"Selected Answer: A\nI think A should be x^2+y^2. We need a circle to classify the data.","comment_id":"1301704","timestamp":"1729627680.0"},{"upvote_count":"1","content":"Selected Answer: A\nJust a note, they are using X2 and Y2 to mean Xsquared, and Ysquared. This is a circle in the form X2+Y2 = k, so for a given k will split that dataset nicely.","timestamp":"1727069280.0","poster":"baimus","comment_id":"1287996"},{"content":"It's not obvious to me it is A.\n\nAs others said, cos(X) does ignore the Y value. But answer A does not seem good either. The differences seem minimal.\n\nIf you do A then you have the following issues. If you take elements in the bottom right or the top left of the circle, they will all have the same value, ZERO. Not only that, they will actually have the same value with the elements in the middle of the circle which are completely black. Moreover, elements on the extreme right and extreme right will have different values (-x_max and +x_max). \n\nHowever, if you use a cos(x) then the elements in the beginning","poster":"Mathew106","timestamp":"1690032780.0","comment_id":"959561","upvote_count":"1","comments":[{"upvote_count":"3","poster":"Mathew106","comment_id":"959627","timestamp":"1690036560.0","content":"Nevermind I did not understand that X2 and Y2 meant X^2 and Y2. Answer is A because that gives the distance from the circle. Circle radius = sqrt(X^2 + Y^2). So even though it's not a perfect answer, it makes sense."}]},{"content":"A. X2+Y2\n\nThe synthetic feature that should be added in this case is the squared value of the distance from the origin (0,0). This is equivalent to X2+Y2. By adding this feature, the classifier will be able to make more accurate predictions by taking into account the distance of each data point from the origin.\n\nX2 and Y2 alone will not give enough information to classify the data because they do not take into account the relationship between X and Y.\n\nD. cos(X) is not a suitable option because it does not take into account the Y coordinate.","timestamp":"1674443280.0","upvote_count":"3","poster":"samdhimal","comment_id":"784910"},{"comment_id":"781913","poster":"GCPpro","upvote_count":"2","content":"A is the correct answer as graph of circle is x^2 + y^2","timestamp":"1674194280.0"},{"comment_id":"778378","timestamp":"1673912340.0","upvote_count":"1","content":"Answer is A:\nThe answer reflects 'x' to the 2nd power + 'y' the 2nd power. \nI guess they can't use carots in the exam answers!","poster":"desertlotus1211"},{"upvote_count":"1","comment_id":"766092","poster":"AzureDP900","timestamp":"1672866300.0","content":"A is right\nReference:\nhttps://medium.com/@sachinkun21/using-a-linear-model-to-deal-with-nonlinear-dataset-c6ed0f7f3f51"},{"poster":"DipT","comment_id":"747174","content":"Selected Answer: A\nhttps://developers.google.com/machine-learning/crash-course/feature-crosses/video-lecture","upvote_count":"1","timestamp":"1671194100.0"},{"poster":"DGames","upvote_count":"2","comment_id":"745522","timestamp":"1671059880.0","content":"Selected Answer: A\nlinear circle X2+Y2 https://www.stat.cmu.edu/~cshalizi/dm/20/lectures/08/lecture-08.html"},{"upvote_count":"2","comment_id":"619889","content":"If the shape was a circle, it would be (x^2 + y ^2). But I think that a quadric curve will do a better job of separating the two classes, so it would be (x^2)","poster":"mvww11","timestamp":"1655822880.0"},{"upvote_count":"1","content":"Answer: A.\nX^2+Y^2 is the equation of a circle.","timestamp":"1651935000.0","comment_id":"598155","poster":"gabrysave"},{"content":"Selected Answer: A\nC'est A","poster":"diagniste","comment_id":"595387","timestamp":"1651368900.0","upvote_count":"2"},{"poster":"Tanzu","upvote_count":"1","timestamp":"1645027920.0","comment_id":"548745","content":"Selected Answer: A\nonly A is draw a circle"},{"comment_id":"524345","upvote_count":"1","content":"Selected Answer: A\nEquation of circle as represented in the question","poster":"sraakesh95","timestamp":"1642270740.0"},{"timestamp":"1641543000.0","comments":[{"poster":"NR22","upvote_count":"1","comment_id":"593386","content":"A B C will only have positive values\nimaginary numbers (i + j) : am I a joke to you?","timestamp":"1651090500.0"}],"upvote_count":"1","content":"F(x) as A B C will have always a positive values as result, for A will need a third dimenssion Z to represent data, only D:cos(x) can be presented as the shown classification. this is a math question","poster":"moumou","comment_id":"518834"},{"comment_id":"516766","content":"Selected Answer: A\nThe 2 variables that make a circle in http://playground.tensorflow.org are x1^2 and x2^2.\nSin(x) or cos(x) would just make horizontal stripes.\n\nTo do this you’d use those 2 variables, learning rate 0,3 for example, classification type, no regularization needed and any activation function will work fine.","poster":"medeis_jar","timestamp":"1641310020.0","upvote_count":"5"},{"content":"Selected Answer: A\nA is the equation of a circle which looks like a good separation for the black vs grey dots.\nhttp://playground.tensorflow.org","comment_id":"505623","upvote_count":"1","poster":"MaxNRG","timestamp":"1640024400.0"},{"upvote_count":"4","timestamp":"1638181500.0","poster":"StefanoG","comment_id":"489782","content":"Selected Answer: B\nI think the right answer is B, we have 3 shades that are specular on Y axis so the classificatin for -x is the same for +x so i think that we must use the tuple(X^2 , y) to classify the value"},{"comment_id":"487060","upvote_count":"1","content":"X^2 + Y^2 would give you the circle, but question is asking about classification of shade. which can be only possible using cos(x). cos(-1) & Cos(1) gives you 0.54 value while cos(-0.5) & Cos(0.5) gives you 0.87.","poster":"MAhlawat","timestamp":"1637896140.0"},{"comment_id":"475923","timestamp":"1636596420.0","content":"here, the requirement is to convert those data points in such a way that a linear(straight line) algorithm can divide those converted points into two sections, so we can do this by creating a new feature by option D.","poster":"Abhi16820","comments":[{"upvote_count":"1","content":"but here are 3 shades not 2","poster":"BigDataBB","timestamp":"1643796180.0","comment_id":"538542"}],"upvote_count":"1"},{"timestamp":"1635119400.0","comment_id":"467181","content":"Answer: A. If you do not understand, you should go to high-school!","upvote_count":"1","poster":"littlewat"},{"poster":"anji007","timestamp":"1634490180.0","content":"Ans: D","comment_id":"463635","upvote_count":"1"},{"content":"If you draw a circle, its circle equation, x^2 + y ^2 \nVote for A","timestamp":"1625773380.0","poster":"sumanshu","comment_id":"402192","upvote_count":"6"},{"comment_id":"398706","content":"Guys, the questions wants a linear and cos x is non linear.. So it is not the answer. D is def not correct.","timestamp":"1625441580.0","upvote_count":"3","poster":"awssp12345"},{"timestamp":"1616239080.0","content":"I think it's A as explai ned here https://medium.com/@sachinkun21/using-a-linear-model-to-deal-with-nonlinear-dataset-c6ed0f7f3f51","poster":"xs91","upvote_count":"7","comment_id":"315571"},{"poster":"federicohi","comment_id":"222095","upvote_count":"4","timestamp":"1605720660.0","content":"its D no doubt"},{"comment_id":"199621","upvote_count":"4","poster":"DarkMatterOne","timestamp":"1602662340.0","content":"Think it's D. It gives nice linear regression around (x,y)"},{"poster":"VIncent9261111","upvote_count":"2","timestamp":"1599602460.0","content":"why not B? Feature Crosses - [A x A]: a feature cross formed by squaring a single feature.","comment_id":"176131"},{"content":"D should the answer as Circle only could not draw the decision boundary in this case","poster":"Tanmoyk","timestamp":"1599466560.0","comment_id":"175052","upvote_count":"1"},{"content":"Answer: A","comment_id":"66536","timestamp":"1584800340.0","poster":"[Removed]","upvote_count":"9"}],"answer_ET":"A","question_text":"You have some data, which is shown in the graphic below. The two dimensions are X and Y, and the shade of each dot represents what class it is. You want to classify this data accurately using a linear algorithm. To do this you need to add a synthetic feature. What should the value of that feature be?\n//IMG//","answer":"A","answer_description":"","url":"https://www.examtopics.com/discussions/google/view/16745-exam-professional-data-engineer-topic-1-question-63/","question_images":["https://www.examtopics.com/assets/media/exam-media/04341/0005000001.jpg"],"isMC":true,"unix_timestamp":1584365940,"question_id":280,"choices":{"B":"X2","A":"X2+Y2","D":"cos(X)","C":"Y2"}}],"exam":{"isImplemented":true,"isBeta":false,"isMCOnly":true,"lastUpdated":"11 Apr 2025","numberOfQuestions":319,"provider":"Google","id":11,"name":"Professional Data Engineer"},"currentPage":56},"__N_SSP":true}