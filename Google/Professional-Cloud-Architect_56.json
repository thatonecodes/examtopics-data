{"pageProps":{"questions":[{"id":"eGWSdIjFNRDONpDdYxoq","isMC":true,"question_id":276,"question_images":[],"answer_images":[],"answers_community":["A (88%)","13%"],"answer":"A","unix_timestamp":1572089100,"url":"https://www.examtopics.com/discussions/google/view/7260-exam-professional-cloud-architect-topic-9-question-3/","topic":"9","answer_ET":"A","choices":{"B":"Replace the existing data warehouse with a Compute Engine instance with 96 CPUs.","C":"Replace the existing data warehouse with BigQuery. Use federated data sources.","A":"Replace the existing data warehouse with BigQuery. Use table partitioning.","D":"Replace the existing data warehouse with a Compute Engine instance with 96 CPUs. Add an additional Compute Engine preemptible instance with 32 CPUs."},"exam_id":4,"answer_description":"","discussion":[{"comments":[{"content":"A is ok","comment_id":"155275","timestamp":"1644579060.0","upvote_count":"9","poster":"tartar"},{"upvote_count":"2","timestamp":"1662343080.0","content":"A is correct","comment_id":"303913","poster":"nitinz"}],"comment_id":"17572","poster":"jcmoranp","content":"Bigquery partitioning, A. Federated makes no sense...","timestamp":"1619436300.0","upvote_count":"41"},{"comment_id":"95074","poster":"Ziegler","content":"A is the correct answer because the question was asking for a reliable way of improving the data warehouse. The reliable way is to have a table partitioned and that can be well managed. \nhttps://cloud.google.com/solutions/bigquery-data-warehouse\nBigQuery supports partitioning tables by date. You enable partitioning during the table-creation process. BigQuery creates new date-based partitions automatically, with no need for additional maintenance. In addition, you can specify an expiration time for data in the partitions.\nhttps://cloud.google.com/solutions/bigquery-data-warehouse#partitioning_tables\nFederated is an option but not a reliable option.\nYou can run queries on data that exists outside of BigQuery by using federated data sources, but this approach has performance implications. Use federated data sources only if the data must be maintained externally. You can also use query federation to perform ETL from an external source to BigQuery. This approach allows you to define ETL using familiar SQL syntax.\nhttps://cloud.google.com/solutions/bigquery-data-warehouse#external_sources","timestamp":"1637789460.0","upvote_count":"23"},{"content":"Selected Answer: A\nA is the only correct answer","poster":"szagarella","timestamp":"1724682240.0","upvote_count":"1","comment_id":"822679"},{"upvote_count":"1","content":"Selected Answer: A\nok for A","timestamp":"1714917000.0","poster":"megumin","comment_id":"711867"},{"timestamp":"1713264180.0","poster":"AzureDP900","comment_id":"696180","content":"A is fine","upvote_count":"2"},{"comment_id":"669988","content":"Selected Answer: A\nBigquery partitioning, A. Federated makes no sense...","timestamp":"1710518220.0","upvote_count":"1","poster":"Nirca"},{"timestamp":"1706961900.0","upvote_count":"1","content":"Selected Answer: A\nBigquery partitioning, A. Federated makes no sense...","poster":"DrishaS4","comment_id":"641751"},{"upvote_count":"1","poster":"Nirca","comment_id":"639972","timestamp":"1706694420.0","content":"Selected Answer: C\nC! Expand beyond a single datacenter to decrease latency to the"},{"upvote_count":"1","poster":"AzureDP900","content":"A is right.","timestamp":"1704424500.0","comment_id":"627175"},{"timestamp":"1702298520.0","comment_id":"614943","upvote_count":"1","poster":"H_S","content":"Selected Answer: A\nBigquery partitioning, A. Federated makes no sense..."},{"poster":"vincy2202","upvote_count":"1","comment_id":"498614","content":"Selected Answer: A\nA is the correct answer","timestamp":"1686393180.0"},{"upvote_count":"1","content":"Selected Answer: A\nvote A","poster":"joe2211","comment_id":"487936","timestamp":"1685165280.0"},{"poster":"MaxNRG","upvote_count":"3","timestamp":"1682336100.0","content":"A – BigQuery in time-partitioned mode.\nC – federated data source won’t be effective. It assumes that time-series data is stored in BigTable and BigQuery federates this table for analytics. But, that’s expensive.\n- BigTable charges for egress 0.08 $ GB/read (that adds charges in analytics mode)\n- BigTable (HDD) – 0.026 $ GB/mo vs BigQuery 0.010 $ GB/mo (and first 10 GB are free monthly).\nSo, no point for BigTable at all. Stream everything to BiqQuery for storage and analytics. Also, BiqQuery can setup partitions expiration period.","comment_id":"466933"},{"poster":"VishalB","timestamp":"1673895180.0","content":"Answer A\no Existing Datawarehouse was hosted on single PostgreSQL server on with below configuration, replacing it with serverless Bigquery using table partition is best recommended soltuion\n RedHat Linux\n 64 CPUs \n 128 GB of RAM\n 4x 6TB HDD in RAID 0","comment_id":"408014","upvote_count":"2"},{"comment_id":"406815","upvote_count":"2","content":"A. Replace the existing data warehouse with BigQuery. Use table partitioning.","poster":"victory108","timestamp":"1673769240.0"},{"poster":"MamthaSJ","upvote_count":"4","comment_id":"401149","content":"Answer is A","timestamp":"1673120400.0"},{"upvote_count":"1","comment_id":"325858","poster":"Ausias18","content":"Answer is A","timestamp":"1664629860.0"},{"poster":"lynx256","content":"IMO - A is ok","timestamp":"1664448600.0","comment_id":"323352","upvote_count":"1"},{"comments":[{"upvote_count":"1","content":"Using table partitioning in BigQuery is a best practice for performant queries.","timestamp":"1665264480.0","poster":"mrhege","comment_id":"331560"}],"comment_id":"322105","timestamp":"1664303400.0","poster":"ashish9_a","content":"For those saying A, where is the technical requirement for table partitioning. I would go with C.","upvote_count":"1"},{"upvote_count":"1","comment_id":"321863","content":"A is ok","poster":"praj777","timestamp":"1664278380.0"},{"timestamp":"1664236800.0","poster":"AD3","upvote_count":"1","content":"'C' as matches the technical requirement \"Increase security of data transfer from equipment to the datacenter.\" 'A' doesn't match much any of the 5 Technical requirements.","comment_id":"321545"},{"content":"Perhaps C makes sense if we look at federation as a feature to support Data Lake as indicated on this doc - https://cloud.google.com/solutions/bigquery-data-warehouse","poster":"HKim","timestamp":"1657297440.0","comment_id":"262771","upvote_count":"1"},{"content":"Another external source limitation tat rules out C:\nYou are limited to 4 concurrent queries against a Cloud Bigtable external data source\nhttps://cloud.google.com/bigquery/external-data-sources#external_data_source_limitations","comment_id":"238616","upvote_count":"1","poster":"trainor","timestamp":"1654708740.0"},{"content":"A BigQuery partitioning.\n\nBigQuery is serverless, highly scalable, and cost-effective multi-cloud data warehouse designed for this type of workload and is the right choice for our requirement. Enabling table partitioning ensures that data is stored in relevant partitions and gives you the ability to delete unwanted partitions rather than deleting individual records. A perfect example is time-based partitioning that lets you delete say data older than 1 year which is no longer required.","upvote_count":"1","timestamp":"1652436660.0","comment_id":"218446","poster":"hems4all"},{"content":"It's A: BigQuery is the best choice for data warehousing. Table partitioning will allow them to more easily handle the massive amount of data they will be dealing with by breaking it up into days.","upvote_count":"1","timestamp":"1650289800.0","comment_id":"202099","poster":"timolo"},{"poster":"LalBisota","content":"A is correct\nReason: As per techinical requirement data needs to be improved in warehouse for understanding customer needs. So you need data to be queried multiple times by the analyst. instead of using federated data source try to load in Big query partition tables which will provide you less query charges and low latency.","upvote_count":"1","timestamp":"1649411460.0","comment_id":"195933"},{"comment_id":"182818","timestamp":"1647772320.0","content":"I don't see reason to go for Partitionining, instead I am thinking that using Federation would help to meet the business requirement of having connectivity and partnership with many suppliers","upvote_count":"2","poster":"hcross"},{"poster":"richardxyz","timestamp":"1647293700.0","upvote_count":"1","content":"A is correct, C has many limitations and can only serve as an addtion to A","comment_id":"179518"},{"poster":"Kabiliravi","content":"A is correct","upvote_count":"1","timestamp":"1646064180.0","comment_id":"169296"},{"upvote_count":"2","poster":"droidmasta","content":"Terramearth wants to improve data IN THE DATAWAREHOUSE they don't see to want to move that data in GCS, thus, partitioning the data would improve it in the way that it'd be organized. Also partitioning would reduce the costs of querying and speed up the time it takes to query parts of the data","comment_id":"163973","timestamp":"1645577220.0"},{"upvote_count":"2","content":"According to Google doc:\n\n\"Use cases for external data sources include:\n\nLoading and cleaning your data in one pass by querying the data from an external data source (a location external to BigQuery) and writing the cleaned result into BigQuery storage.\nHaving a small amount of frequently changing data that you join with other tables. As an external data source, the frequently changing data does not need to be reloaded every time it is updated.\"\nSo, A) is the right answer. C) is not the best choice to create a generic datawarehouse","poster":"jespinosar","timestamp":"1645267200.0","comment_id":"161344"},{"upvote_count":"1","timestamp":"1641899520.0","content":"Agree with Ans: A","poster":"RM07","comment_id":"131980"},{"poster":"mlantonis","upvote_count":"2","comment_id":"118269","content":"Partitioned tables are more reliable than Federated data source. Federated data source, has many limitations.\n\nThe correct answer should be A.","timestamp":"1640344860.0"},{"comment_id":"117778","upvote_count":"2","content":"A. Why do we need federated data here ?","timestamp":"1640294340.0","poster":"motty"},{"upvote_count":"1","content":"should be A imo. We need to reduce downtime, improve data in data warehouse, anticipate customer needs (using bigQuery ML), so keeping data in BQ makes more sense.","poster":"Rafaa","timestamp":"1638303600.0","comment_id":"99504"},{"content":"A is correct - BigQuery works well with time partitioning and suffers a performance degradation with federated queries","comment_id":"76242","poster":"PRC","timestamp":"1634615640.0","upvote_count":"2"},{"poster":"KNG","comment_id":"53308","content":"\"C\" could be the answer\nBigQuery has a federated data access model that allows for querying data directly from Bigtable, Cloud Storage, and Google Drive with permanent and temporary tables. If you have data in multiple Google Cloud Platform services and are building a data lake or data warehouse strategy, you might want to learn more about this capability.\nWhile the storage costs in BigQuery are comparable to regional per GB charges in Cloud Storage, there may be cases where you want to consider querying an external data source instead of importing and storing data in BigQuery. \nhttps://cloud.google.com/blog/products/gcp/accessing-external-federated-data-sources-with-bigquerys-data-access-layer","upvote_count":"1","comments":[{"timestamp":"1635327780.0","comment_id":"80256","upvote_count":"2","content":"There are lots of limitations for BigQuery Federated Data Access. I'd go with A.","poster":"Ayzen"}],"timestamp":"1629521700.0"},{"poster":"suryalsp","content":"Answer should be A. https://cloud.google.com/bigquery/external-data-sources","comment_id":"31849","timestamp":"1624388160.0","upvote_count":"3"},{"poster":"passnow","comments":[{"upvote_count":"1","content":"In cloud, cost is always a requirement unless otherwise is stated.\nAnd to reduce the storage costs, BQ partitioned tables with a retention is a good measure to spend less money on BQ storage costs since customer already pays for GCS storage costs.","timestamp":"1641651120.0","comment_id":"129727","poster":"cetanx"},{"poster":"MyPractice","comment_id":"34514","timestamp":"1625214600.0","content":"\"BigQuery supports querying Cloud Storage data from these storage classes: Multi-Regional, Regional, Nearline, Coldline\" meaning no need to have federated data source. \n\n\"An external data source (also known as a federated data source) is a data source that you can query directly even though the data is not stored in BigQuery\"\nhttps://cloud.google.com/bigquery/external-data-sources\nI choose A","upvote_count":"4"}],"comment_id":"30797","timestamp":"1624047360.0","content":"Guys, cost was not a requirement for the fact that the vehicles are in different regions, federated yes. As par time partitioned tables, you were not asked to consider expiration of data, I would go with the chosen option","upvote_count":"2"},{"poster":"chiar","comments":[{"poster":"dayody","comment_id":"170882","upvote_count":"1","timestamp":"1646068800.0","content":"correct"}],"content":"I think it's A because in business requirement the priority is speed , and the it important the time, federated source is slow in queries","comment_id":"25055","upvote_count":"5","timestamp":"1622228880.0"},{"poster":"cjsammaejs","comment_id":"23695","upvote_count":"1","timestamp":"1621703940.0","content":"I think C is possible. From the GCP training proposed training solution, IOT Device and message management would go from IOT Core > to Pub/Sub > Dataflow > to Cloud storage. I think you need federated to read from cloud storage. Also from the proposed GCP training solution there was \"Do not need low latency I/O\" as as a technical watch point. If those assumptions and solutions are accurate, then federated is fine."},{"upvote_count":"3","comment_id":"21730","comments":[],"timestamp":"1621063980.0","poster":"JoeShmoe","content":"Federating data sources here would be costly and have latency issues so its A"}],"question_text":"For this question, refer to the TerramEarth case study. You need to implement a reliable, scalable GCP solution for the data warehouse for your company,\nTerramEarth.\nConsidering the TerramEarth business and technical requirements, what should you do?","timestamp":"2019-10-26 13:25:00"},{"id":"wucwety0QfSN9vfvtKSs","answer_images":[],"answers_community":["D (77%)","A (23%)"],"question_images":[],"question_id":277,"answer_ET":"D","topic":"9","question_text":"For this question, refer to the TerramEarth case study. A new architecture that writes all incoming data to BigQuery has been introduced. You notice that the data is dirty, and want to ensure data quality on an automated daily basis while managing cost.\nWhat should you do?","isMC":true,"answer":"D","discussion":[{"comments":[{"comment_id":"155284","poster":"tartar","timestamp":"1597138860.0","content":"D is ok","upvote_count":"13"},{"content":"looks like D\nhttps://cloud.google.com/dataprep","poster":"melono","upvote_count":"2","timestamp":"1666362720.0","comment_id":"700994"},{"upvote_count":"5","poster":"motty","comment_id":"117779","timestamp":"1592940060.0","content":"dataprep is GUI driven process to analyse adhoc data dumped on GCS, it has not place in this use case"}],"content":"Option D, as data needs to be cleaned ..\nDataprep has the capabilities to clean dirty data","timestamp":"1581844680.0","upvote_count":"34","comment_id":"51147","poster":"Sj10"},{"comment_id":"64345","content":"automated daily ... answer is D","timestamp":"1584285900.0","upvote_count":"12","poster":"vindahake"},{"upvote_count":"1","timestamp":"1734294240.0","poster":"nbneeraj","content":"Selected Answer: A\nQuestion says data is dirty. So we need to clean the dirty data before loading it into big query. Option D says using Dataprep once dirty data is in big query. \nOption A says use ETL: Dataflow to clean it first. \nA datawarehouse developer will always go with Option A. Use ETL tool to clean the data first and then load in big query warehouse. \nSo the option is A","comment_id":"1327045"},{"poster":"odacir","timestamp":"1700406000.0","comment_id":"1074687","upvote_count":"1","content":"Selected Answer: D\nCloud Dataprep is not cheap. Today i will recommend to used a schedule DataForm or dbt for cleaning..."},{"content":"Selected Answer: D\nD without any doubt.\nDataflow is for data elaboration. Dataprep is for data preparation (and cleaning).","comment_id":"938055","timestamp":"1688040480.0","poster":"red_panda","upvote_count":"2"},{"comment_id":"803919","poster":"RVivek","upvote_count":"4","content":"Selected Answer: D\nB & C does not make sense.\nA is costly and in realtime\nThe question says on daily basis and cost effective hence D","timestamp":"1675997820.0"},{"timestamp":"1671335520.0","poster":"surajkrishnamurthy","comment_id":"748590","upvote_count":"2","content":"Selected Answer: D\nD is the correct answer"},{"poster":"megumin","timestamp":"1668077700.0","upvote_count":"1","content":"Selected Answer: D\nD is ok","comment_id":"715161"},{"poster":"cbarg","timestamp":"1662650220.0","upvote_count":"4","content":"Selected Answer: D\nAns is D. Please refer to this example: https://medium.com/google-cloud/how-to-schedule-a-bigquery-etl-job-with-dataprep-b1c314883ab9","comment_id":"663733"},{"comments":[{"comment_id":"672648","timestamp":"1663524060.0","poster":"dayody","upvote_count":"2","comments":[{"content":"Why not ?? we have done it using both....","timestamp":"1727321640.0","comment_id":"1289271","poster":"Begum","upvote_count":"1"}],"content":"you cannot clean data with Dataflow only with Dataprep"}],"poster":"ShadowLord","comment_id":"661028","content":"Selected Answer: A\nOptions should be A.\n1. Cost in D would be higher. e.g. First load dirty data into DB and then run Data Prep Jobs to clean the data and load into some different target Data . Overall cost of scanning the data and the loading is like double the cost. Then identifying already clean data and dirty data is again a challenge on daily basis after the data growth is significant\n2. Data Stream can be utilized to cleanse the data while loading","upvote_count":"5","timestamp":"1662454860.0"},{"poster":"DrishaS4","upvote_count":"3","timestamp":"1659521220.0","content":"Selected Answer: D\nautomated daily ... answer is D","comment_id":"641754"},{"poster":"AzureDP900","upvote_count":"2","timestamp":"1656983880.0","comment_id":"627177","content":"D is perfect to cleanup the data daily!"},{"timestamp":"1639138860.0","upvote_count":"1","comment_id":"498606","poster":"vincy2202","content":"Selected Answer: D\nD is the correct answer"},{"comment_id":"493843","upvote_count":"1","content":"Selected Answer: D\nVote D","poster":"pakilodi","timestamp":"1638638400.0"},{"upvote_count":"1","timestamp":"1637998140.0","content":"Selected Answer: D\nvote D","comment_id":"487937","poster":"joe2211"},{"timestamp":"1636563000.0","upvote_count":"4","poster":"gonzalopf94","content":"Option is A, Dataprep uses a UI to perform the cleaning process and under the hood it is using Dataflow to perform the process, so I will go with A.","comment_id":"475638"},{"content":"A and D are both will solve the purpose. A is more expensive and ask is daily basis clean-up of data. D is right choice.","timestamp":"1635187380.0","comment_id":"467641","upvote_count":"2","poster":"[Removed]"},{"poster":"ZappsterB","upvote_count":"2","content":"Should be A. Dirty data may not be formatted to suit the table structure and then won't go in to be 'cleansed' later.","timestamp":"1635136440.0","comment_id":"467247"},{"timestamp":"1635075540.0","content":"D – use Cloud Dataprep and configure the BigQuery tables as the source. Schedule daily jobs to clean the data.\nCloud Dataprep – is for fast exploration and anomaly detection.\nIt supports scheduling (as Q asks): “Schedule the execution of recipes in your flows on a recurring or as-needed basis. When the scheduled job successfully executes, you can collect the wrangled output in the specified output location, where it is available in the published form you specify”.\nAlso, Dataprep integrates naturally with BigQuery (and Cloud Storage, upload file directly). And it uses Dataflow under the hood.\nA, B – are about real-time processing, which is not needed (per req on daily basis). Also, to find if data is dirty you may analyze several adjacent rows, so real-time processing may not physically solve a problem;","comment_id":"466936","comments":[{"content":"C – is about just SQL processing, which is likely not enough to fix algorithmically data problems. Also, it is not automated, and requires new table for cleaned data.\nPricing-wise, option A and D should be comparable, since Dataprep uses underneath Dataflow workers.\nDataprep pricing (0.056$ vCPU per hour – batch worker)\nDataflow pricing (0.069$ x 4 vCPU per hour, for streaming worker)\nIn general, batch processing is more price-effective, since it eliminates potential data-waiting cycles. So, batched vCPUs should be 100% busy by your work, though streaming worker can be idle, but you still need to pay for its time.","upvote_count":"1","timestamp":"1635075600.0","comments":[{"content":"How about Bigquery cost which would be double in afterwards scanning with Options D","comment_id":"661031","upvote_count":"1","poster":"ShadowLord","timestamp":"1662454980.0"}],"comment_id":"466937","poster":"MaxNRG"}],"poster":"MaxNRG","upvote_count":"5"},{"timestamp":"1633508220.0","upvote_count":"1","comment_id":"458120","content":"not sure if D is the only option, since Dataprep sounds like a third party app from Trifacta, maybe B is more secure?","poster":"Bill831231"},{"comments":[{"content":"But cleaning old data can be a seperate project ... Question is on cost","upvote_count":"1","comment_id":"661033","poster":"ShadowLord","timestamp":"1662455040.0"}],"timestamp":"1630625640.0","comment_id":"438172","poster":"Sarin","content":"Cleaning up data during ingestion looks right but that won’t fix the already existing data in big query So option D","upvote_count":"2"},{"poster":"victory108","content":"D. Use Cloud Dataprep and configure the BigQuery tables as the source. Schedule a daily job to clean the data.","timestamp":"1626328140.0","comment_id":"406813","upvote_count":"1"},{"upvote_count":"6","timestamp":"1625679660.0","poster":"MamthaSJ","comment_id":"401151","content":"Answer is A"},{"timestamp":"1617282780.0","comment_id":"325860","upvote_count":"1","poster":"Ausias18","content":"Answer is D"},{"upvote_count":"6","content":"I wouldn't write dirty data to my warehouse. D seems fancy. \nA is the best option.","timestamp":"1616877240.0","comment_id":"322109","poster":"ashish9_a"},{"comment_id":"318116","content":"I'll go with D. \nApart form other considerations, A (streaming via DataFlow) is much more expensive :)","upvote_count":"2","timestamp":"1616505840.0","poster":"lynx256"},{"upvote_count":"3","timestamp":"1614140700.0","comment_id":"297933","content":"This is the sales story from Trifactor (The Datapred vendor) - how do you clean up data stored in BigQuery? Of the many strengths of Google BigQuery—incredible performance and scalability, to name a few—BigQuery dataprep is not one. To complete BigQuery dataprep, analysts leverage another Google Cloud service, Cloud Dataprep by Trifacta. \n\nThe question says data is stored in BQ and the cleaning must be run daily. D mentions BQ, and it can be scheduled. Option A -- is a generic streaming data cleaning solution. So i'd go with D.","poster":"hkmsn"},{"poster":"guid1984","upvote_count":"2","timestamp":"1613398080.0","content":"Its D: You can schedule daily automated jobs in dataprep to clean data. And its a standard tool for the use-case mentioned in the requirement, rather than building something out of the scratch","comment_id":"291036"},{"content":"D DataPrep is for data cleansing.","poster":"bnlcnd","upvote_count":"1","timestamp":"1612317240.0","comment_id":"282331"},{"content":"A should be the answer. D is not since Data is initially stored in Cloud Storage and not in the BQ. Hence source should be either Cloud Storage for batch data or use Cloud Dataflow to clean and reorder the data. https://cloud.google.com/community/tutorials/data-science-preprocessing","poster":"Arimaverick","timestamp":"1610039520.0","comment_id":"262000","upvote_count":"1"},{"timestamp":"1608426960.0","comment_id":"248279","content":"I vote A.\nDataprep is an UI based tool. The question requires the automatic process. \nDataprep is a service for visually exploring, cleaning, and preparing data for analysis. You can use Dataprep using a browser based-UI, without writing code.\nrefer --> https://cloud.google.com/solutions/data-lifecycle-cloud-platform#visual_data_exploration_cleaning_and_processing","poster":"ffgcloud","upvote_count":"5"},{"content":"Its A.\nD is taking the data from BigQuery and then it will be cleaned and loaded again? Will take more time. Business requirement is to cut down the time. (Decrease unplanned vehicle downtime to less than 1 week)\nA is the best solution to cut down the time and make processing faster.","timestamp":"1607810040.0","comment_id":"242047","upvote_count":"3","poster":"OSNG"},{"timestamp":"1603919400.0","content":"Data Prep works well for Analysis and ML.\nDataFlow works well for Batch and streaming data.","upvote_count":"3","comment_id":"208111","poster":"AdityaGupta"},{"content":"D is the answer as the Dataprep to cleaning the data.","poster":"LoganIsh","upvote_count":"1","timestamp":"1602754380.0","comment_id":"200405"},{"upvote_count":"1","comment_id":"196008","timestamp":"1602156540.0","content":"Some have said A, because DataPrep is a GUI, but you can schedule daily jobs so I'm going with D. Also the question states BigQuery and D states BigQuery.\n\nhttps://cloud.google.com/dataprep/docs/html/Schedule-a-Job_118228655","poster":"f0x"},{"timestamp":"1602011400.0","comment_id":"194580","comments":[{"poster":"sealvarezmx","upvote_count":"2","timestamp":"1609882740.0","content":"true, but if you have time partitions on BigQuery? then you wouldn't be running it against 900TB.","comment_id":"260577"}],"content":"In real world use Dataprep to clean the data already in BigQuery as a one time operation, then use dataflow to clean the streamed data before feeding to bigQuery as a ongoing daily service. Difficult to call and I would not want to run Dataprep on a daily basis on data that grows by 900 TB per day, far cheaper to let dataflow do this - So overall D followed by A. However I am forced to choose D as that is a working solution although expensive","upvote_count":"1","poster":"Cloudy_Apple_Juice"},{"comment_id":"190563","content":"Agree with D, the requirements state that the data cleaning process has to be automated on a daily basis (option D satisfies this requirement), not constantly via streaming (option A suggests this process). Maybe option A would be more efficient in real world scenarios, but not for this specific question.","timestamp":"1601496180.0","upvote_count":"1","poster":"bidibidiiii"},{"poster":"kmanexamtop","comment_id":"187500","upvote_count":"3","content":"B & C are not good options. Its between A & D. In the question it mentions to have an automation run \"daily\". Looking at the choices, the data is now already arriving at BigQuery. So we need to use BigQuery as source and have daily clean up using Dataprep. Hence D is correct choice.","timestamp":"1601104140.0"},{"timestamp":"1600591020.0","upvote_count":"1","content":"I think D is the answer. B and C seem not very relevant while A mentions \"Streaming\" which appears not in the scope of the Q. Additionally I think it is a bit generic in the cleansing process it proposes","comment_id":"182822","poster":"hcross"},{"timestamp":"1598137020.0","comment_id":"163981","poster":"droidmasta","content":"I'd say A. It's more efficient to use the currently existing Dataflow pipeline to clean the data than introduce a new service and therefore cost. (im assuming the solution for terramearth includes dataflow, actually IoT core -pubsub - dataflow - bigquery","upvote_count":"2"},{"upvote_count":"1","timestamp":"1597826580.0","poster":"jespinosar","comment_id":"161347","content":"Due to the question \"...automated daily basis...\", Dataprep is discarded. As @motty stated, Dataprep is a GUI tool, not a batch tool.\na) is the right answer"},{"poster":"AmazonAu","upvote_count":"2","comment_id":"140220","timestamp":"1595328480.0","content":"https://www.stitchdata.com/vs/google-cloud-dataprep/google-cloud-dataflow/\n\nA"},{"upvote_count":"1","timestamp":"1592989620.0","poster":"mlantonis","comment_id":"118264","content":"DataPrep can clean data, but the functionalities of DataPrep are limited. Dataflow provides customizable data cleaning process.\n\nI prefer Dataflow, but answer A does not mention Bigquery.\nThe correct answer is either A or D."},{"poster":"motty","upvote_count":"3","timestamp":"1592940120.0","comment_id":"117783","content":"Dataflow is proper way to handle established data ingestion before offloading to BQ.\nI think, it is A"},{"poster":"rockstar101","timestamp":"1592872680.0","content":"A - if clean the data in the ingestion pipeline you dont have to store it twice. Also Dataprep uses Dataproc under the hood for scaling so A","comment_id":"116929","upvote_count":"1"},{"poster":"BeppeIta","timestamp":"1591682820.0","content":"\"new architecture that writes all incoming data to BigQuery has been introduced. You notice that the data is dirty,\" the dirty data is just inside BQ. Acting before will not clean old data, using dataprep, AFTER ingestion, will work for old and new data.So..normaly D. BUT \"managisng cost\" is there. A is cheaper than D cause of\n D add to dataflow the dataprep cost ( per hour)","upvote_count":"1","comment_id":"105718"},{"comment_id":"102497","upvote_count":"1","content":"No argument with A if carefully considered the pricing.\n\"When you submit a job to Dataprep, it is executed by Dataflow workers. Dataprep is billed according to the number of Dataflow worker virtual CPUs (vCPUs) that are needed to process a job and the time that the vCPUs are used multiplied by the Dataprep service rate of $0.60 per hour.\"\nhttps://cloud.google.com/dataprep/pricing/\nDataproc is most cost efficient for handling a whole new dataset. It will be wasting money to have Dataproc repeatedly processing those processed data that are accumulated for a whole year everyday.\nSo the perfect solution is to have Dataflow for new data ingestion and a once-off Dataproc for the existing data. However it doesn't make sense to have daily Dataproc in this case.","comments":[{"timestamp":"1591288380.0","poster":"jakpen","comment_id":"102498","content":"By Dataproc I mean Dataprep...","upvote_count":"1"}],"poster":"jakpen","timestamp":"1591288320.0"},{"content":"I would say that cleaning the data before it enters is better. and also it lowers the cost of daily cleaning bigquery table wirth lots of data so A","timestamp":"1591251840.0","poster":"pf38120","comment_id":"102165","upvote_count":"2"},{"upvote_count":"6","comment_id":"95075","comments":[{"upvote_count":"3","content":"https://cloud.google.com/blog/products/gcp/guide-to-common-cloud-dataflow-use-case-patterns-part-1\nPattern: Dealing with bad data\nDescription:\nYou should always defensively plan for bad or unexpectedly shaped data. A production system not only needs to guard against invalid input in a try-catch block but also to preserve that data for future re-processing.","poster":"Ziegler","timestamp":"1591473240.0","comment_id":"104073"}],"content":"A is the correct answer e.g. When gathering information from the real world, the data will often contain errors, omissions, or inconsistencies that should be corrected before you can analyze it effectively. Instead of doing it by hand, or performing a separate cleansing step, Google Cloud Dataflow allows you to define simple functions that can cleanse your data in a pipeline, which you can plug into your data ingestion pipeline for automatic cleansing. \nhttps://cloud.google.com/community/tutorials/data-science-preprocessing","poster":"Ziegler","timestamp":"1590349020.0"},{"poster":"PRC","content":"D is correct - DataPrep is a managed service to clean and prepare the data. Streaming data is for real-time streaming data and moreover is a costly option.","upvote_count":"5","timestamp":"1587269040.0","comment_id":"76248"},{"comment_id":"71295","poster":"anton_royce","timestamp":"1586055240.0","upvote_count":"10","content":"Answer D. Look at the question. It says \"A new architecture that writes all incoming data to BigQuery has been introduced\". so, all the data is written in BigQuery and use that as the source to clean the data."},{"comment_id":"70269","upvote_count":"2","content":"CHECK A – It is also a potential best answer. It is better to clean in the ingestion phase before writing to bigquery. Need to see cost wise which one is better. Dataprep job once a day by querying a vast set of bigquery data or dataflow as part of ingestion for every data that is getting ingested","timestamp":"1585778100.0","poster":"Jeysolomon"},{"poster":"SMS","comment_id":"67084","content":"https://cloudx-bricks-prod-bucket.storage.googleapis.com/29692cfceb42f30e1b6fcd317be2856dca4957fd917742bc355ae67fbbbd9e98.svg\n\nA & D both can be used for since Q asks for daily instead of streaming, it would be D","timestamp":"1584910620.0","upvote_count":"1"},{"upvote_count":"10","poster":"rickywck","timestamp":"1583635020.0","comment_id":"60562","content":"I think D is correct:\nhttps://cloud.google.com/dataprep"},{"timestamp":"1580931540.0","comments":[{"comments":[{"timestamp":"1606935240.0","poster":"bjuneja","upvote_count":"1","comment_id":"233308","content":"Both B and D are valid approaches but the question says cost effective for me Dataflow is cost effective as Dataprep uses dataflow workers internally and we can also clean the data in an Apache Beam PipeLine","comments":[{"comment_id":"233311","upvote_count":"1","content":"Sorry A and D i will go with A","timestamp":"1606935360.0","poster":"bjuneja"}]}],"poster":"tartar","timestamp":"1597139280.0","upvote_count":"7","content":"D is ok","comment_id":"155291"},{"poster":"nitinz","upvote_count":"1","comment_id":"303914","timestamp":"1614916740.0","content":"A is correct."}],"upvote_count":"2","content":"Its A .","poster":"Barniyah","comment_id":"47040"}],"url":"https://www.examtopics.com/discussions/google/view/13467-exam-professional-cloud-architect-topic-9-question-4/","choices":{"A":"Set up a streaming Cloud Dataflow job, receiving data by the ingestion process. Clean the data in a Cloud Dataflow pipeline.","B":"Create a Cloud Function that reads data from BigQuery and cleans it. Trigger the Cloud Function from a Compute Engine instance.","C":"Create a SQL statement on the data in BigQuery, and save it as a view. Run the view daily, and save the result to a new table.","D":"Use Cloud Dataprep and configure the BigQuery tables as the source. Schedule a daily job to clean the data."},"answer_description":"","unix_timestamp":1580931540,"timestamp":"2020-02-05 20:39:00","exam_id":4},{"id":"kkab1eUPB9rzP95kXkrN","discussion":[{"content":"Definitely A","upvote_count":"40","comments":[{"poster":"AdityaGupta","timestamp":"1603919760.0","upvote_count":"7","content":"Once all the vehicle are connected to network, there is no need to use FTP; data can be ingested directly to BQ using Pub/Sub and DataFlow.","comment_id":"208116"}],"timestamp":"1586742420.0","comment_id":"73904","poster":"balajee14"},{"content":"A is good...simple streaming of data with managed services approach","comment_id":"76250","timestamp":"1587269160.0","poster":"PRC","upvote_count":"10"},{"content":"Selected Answer: A\nA. Use BigQuery as the data warehouse. Connect all vehicles to the network and stream data into BigQuery using Cloud Pub/Sub and Cloud Dataflow. Use Google Data Studio for analysis and reporting.\n\nThis approach leverages the real-time data streaming capabilities of Cloud Pub/Sub and Cloud Dataflow, the scalability and efficiency of BigQuery for data analysis, and the powerful visualization and reporting features of Google Data Studio. This combination ensures timely insights and quick response to issues, thereby reducing unplanned vehicle downtime.","comment_id":"1233864","upvote_count":"1","timestamp":"1718906760.0","poster":"CID2024"},{"content":"Selected Answer: A\nA looks like the correct one","comment_id":"741745","upvote_count":"1","timestamp":"1670765880.0","poster":"Aninina"},{"upvote_count":"1","timestamp":"1668077820.0","poster":"megumin","comment_id":"715162","content":"Selected Answer: A\nA is ok"},{"upvote_count":"1","comment_id":"696178","timestamp":"1665916680.0","content":"A is good","poster":"AzureDP900"},{"upvote_count":"1","poster":"cbarg","comment_id":"663737","content":"Selected Answer: A\nAns is A.","timestamp":"1662650640.0"},{"upvote_count":"1","content":"A is right, all other options doesn't make sense.","timestamp":"1656984120.0","poster":"AzureDP900","comment_id":"627178"},{"timestamp":"1654944240.0","poster":"H_S","content":"Selected Answer: A\nDefinitely A","upvote_count":"1","comment_id":"614944"},{"upvote_count":"1","timestamp":"1648785660.0","content":"A should be better.\n\nhttps://cloud.google.com/architecture/designing-connected-vehicle-platform#data_ingestion","poster":"[Removed]","comment_id":"579312"},{"comment_id":"487940","timestamp":"1637998200.0","poster":"joe2211","content":"Selected Answer: A\nvote A","upvote_count":"1"},{"content":"A. Use BigQuery as the data warehouse. Connect all vehicles to the network and stream data into BigQuery using Cloud Pub/Sub and Cloud Dataflow. Use Google Data Studio for analysis and reporting.","poster":"victory108","comment_id":"406811","timestamp":"1626328020.0","upvote_count":"3"},{"comment_id":"401152","upvote_count":"4","timestamp":"1625679720.0","content":"Answer is A","poster":"MamthaSJ"},{"comment_id":"325861","poster":"Ausias18","timestamp":"1617282840.0","upvote_count":"1","content":"answer is A"},{"poster":"lynx256","timestamp":"1617014760.0","comment_id":"323346","content":"A is ok","upvote_count":"1"},{"upvote_count":"1","poster":"sekhrivijay","content":"Technical requirement : Create a backup strategy\n\nIs bigquery a suitable system for data backup . Wouldn't a better system for backup be cloud storage.\n\nOnly B has that option","timestamp":"1613683560.0","comment_id":"293736"},{"content":"A is correct, using dataflow to clean and/or convert the data for analysis makes more sense.\n\nB does not show any sign of how data will be loaded to bigquery (as gzip) or after conversion, it seems broken process to me.","timestamp":"1607809740.0","upvote_count":"2","comment_id":"242041","poster":"OSNG"},{"poster":"AdityaGupta","content":"Once all the vehicle are connected to network, there is no need to use FTP; data can be ingested directly to BQ using Pub/Sub and DataFlow.","upvote_count":"1","comment_id":"208115","timestamp":"1603919700.0"},{"poster":"hcross","upvote_count":"4","content":"For me it's A. This could be relevant here https://cloud.google.com/solutions/designing-connected-vehicle-platform","comment_id":"182826","timestamp":"1600591380.0"},{"timestamp":"1597826700.0","poster":"jespinosar","comment_id":"161349","upvote_count":"1","content":"The key point here is being aware without all vehicles upgrading to connected vehicles there will not be time reduction because vehicles will have to wait to be served to get their data."},{"timestamp":"1592989020.0","content":"A is fine","poster":"mlantonis","upvote_count":"4","comment_id":"118257"},{"comments":[{"comment_id":"95078","content":"Please ignore my response above, I go with A as the right answer.","poster":"Ziegler","upvote_count":"6","timestamp":"1590350400.0"}],"poster":"Ziegler","timestamp":"1590350280.0","upvote_count":"2","comment_id":"95077","content":"B is the correct answer because the question says \"...how should you reduce the unplanned vehicle downtime in GCP?\" As for me, the best is to compress the files locally where it would be upload to GCS where it can be streamed to pubsub->dataflow->bigquery etc. This will ensure those vehicle with no cellular are also taken care of easily"},{"comments":[{"timestamp":"1588963020.0","content":"Hence B should be better than A","comment_id":"85810","upvote_count":"1","poster":"Shabje"}],"timestamp":"1588962960.0","content":"Connecting to the network refers to the service centers. Won’t that be cost prohibitive to get them connected to a streaming pipeline on GCP?","upvote_count":"1","poster":"Shabje","comment_id":"85809"},{"upvote_count":"2","timestamp":"1582757340.0","content":"Why is nota A?","comment_id":"55706","poster":"amendez"},{"comments":[{"timestamp":"1597139400.0","content":"A is ok","poster":"tartar","upvote_count":"10","comment_id":"155294"},{"content":"A is correct","poster":"nitinz","upvote_count":"2","timestamp":"1614917040.0","comment_id":"303917"}],"content":"is B correct?","poster":"ROPATE","upvote_count":"1","comment_id":"54135","timestamp":"1582466940.0"}],"answers_community":["A (100%)"],"answer_description":"","question_id":278,"isMC":true,"question_text":"For this question, refer to the TerramEarth case study. Considering the technical requirements, how should you reduce the unplanned vehicle downtime in GCP?","topic":"9","answer_ET":"A","choices":{"B":"Use BigQuery as the data warehouse. Connect all vehicles to the network and upload gzip files to a Multi-Regional Cloud Storage bucket using gcloud. Use Google Data Studio for analysis and reporting.","A":"Use BigQuery as the data warehouse. Connect all vehicles to the network and stream data into BigQuery using Cloud Pub/Sub and Cloud Dataflow. Use Google Data Studio for analysis and reporting.","D":"Use Cloud Dataproc Hive as the data warehouse. Directly stream data into partitioned Hive tables. Use Pig scripts to analyze data.","C":"Use Cloud Dataproc Hive as the data warehouse. Upload gzip files to a Multi-Regional Cloud Storage bucket. Upload this data into BigQuery using gcloud. Use Google Data Studio for analysis and reporting."},"answer":"A","url":"https://www.examtopics.com/discussions/google/view/14729-exam-professional-cloud-architect-topic-9-question-5/","exam_id":4,"timestamp":"2020-02-23 15:09:00","answer_images":[],"question_images":[],"unix_timestamp":1582466940},{"id":"YzAf0qTVnHjbwYBqKGSu","answer_images":[],"question_images":[],"discussion":[{"content":"Why not B?","upvote_count":"31","comments":[{"timestamp":"1628676180.0","poster":"tartar","comment_id":"155302","upvote_count":"10","content":"B is ok"},{"poster":"nitinz","upvote_count":"4","content":"It is B","comment_id":"303919","timestamp":"1646453220.0"}],"poster":"KouShikyou","timestamp":"1603438140.0","comment_id":"16886"},{"upvote_count":"21","content":"Google Cloud IoT Core is being retired on August 16, 2023","timestamp":"1706876040.0","comment_id":"796013","poster":"dataqueen_3110"},{"poster":"desertlotus1211","upvote_count":"1","content":"Selected Answer: B\nFYI: \nClearBlade IoT Core is a replacement product that provides the same functionality as GCP IoT Core. ClearBlade IoT Core offers 1:1 product parity including a Device Table, Security, MQTT+ Messaging, Service Integrations, Edges, and Monitoring. ClearBlade also provides a free \"1-click\" migration tool that fully automates the move to ClearBlade IoT Core.","timestamp":"1733759460.0","comment_id":"1324123"},{"content":"Selected Answer: B\nB is ok","upvote_count":"1","poster":"megumin","timestamp":"1699614000.0","comment_id":"715165"},{"content":"IoT core is fine.. B is right!","timestamp":"1688520300.0","comment_id":"627179","poster":"AzureDP900","upvote_count":"3"},{"content":"Selected Answer: B\nIt's B","upvote_count":"2","poster":"H_S","timestamp":"1686480300.0","comment_id":"614945"},{"upvote_count":"1","content":"Selected Answer: B\nB is correct","poster":"Bobch","timestamp":"1671530400.0","comment_id":"505320"},{"content":"Selected Answer: B\nB is the correct answer","upvote_count":"1","comment_id":"498554","poster":"vincy2202","timestamp":"1670671440.0"},{"content":"Selected Answer: B\nvote B","poster":"joe2211","timestamp":"1669534260.0","upvote_count":"1","comment_id":"487942"},{"content":"B – Cloud IoT Core with public / private key pairs.\nhttps://cloud.google.com/iot-core/\nIoT Core was developed for connecting existing devices spread around the world to GCP. Also, it supports end-to-end security using asymmetric key authentication over TLS 1.2. So, this is exact match for Q.","upvote_count":"4","poster":"MaxNRG","timestamp":"1666611960.0","comment_id":"466939"},{"timestamp":"1657863780.0","comment_id":"406805","content":"B. Cloud IoT Core with public/private key pairs","upvote_count":"4","poster":"victory108"},{"poster":"MamthaSJ","timestamp":"1657215840.0","content":"Answer is B","comment_id":"401153","upvote_count":"3"},{"upvote_count":"1","comment_id":"325863","timestamp":"1648818960.0","content":"Answer is B","poster":"Ausias18"},{"timestamp":"1648545720.0","content":"IMO B is ok","comment_id":"323274","poster":"lynx256","upvote_count":"1"},{"content":"All options look okay but B tops them all.","upvote_count":"1","poster":"ashish9_a","timestamp":"1648406820.0","comment_id":"322120"},{"content":"'B' is more correct even though the technical requirement doesn't clearly say about the new technology, the executive summary does say it \"transformation of technology\".","comment_id":"321553","upvote_count":"1","timestamp":"1648343580.0","poster":"AD3"},{"comment_id":"291040","content":"Keywords \"You are asked to design a new Architecture\" Cloud IOT core is the way for this requirement","poster":"guid1984","upvote_count":"2","timestamp":"1644934320.0"},{"content":"Seems everyone vote for B.\nBut why not A? SSL ingress Loadbalancer works well for FTP and no need to change from FTP to HTTP or MQTT. Isn't this better?","upvote_count":"1","poster":"bnlcnd","comment_id":"282344","timestamp":"1643854800.0"},{"content":"B is the man","comment_id":"246687","timestamp":"1639760220.0","upvote_count":"2","poster":"okixavi"},{"upvote_count":"1","poster":"hems4all","comment_id":"218485","timestamp":"1636810500.0","content":"B is the Answer:\n\nAs described in the connected vehicle use case: https://cloud.google.com/solutions/designing-connected-vehicle-platform, you use Cloud IoT Core with per device public/private key pairs to meet this requirement."},{"upvote_count":"1","comment_id":"209744","content":"B is the correct answer.","timestamp":"1635660300.0","poster":"gcparchitect007"},{"content":"B is correct here as per google best practice.","poster":"wiqi","upvote_count":"1","timestamp":"1629974580.0","comment_id":"166643"},{"upvote_count":"1","comment_id":"129742","content":"Answer: B\nThis article perfectly explains how;\nhttps://cloud.google.com/iot/docs/concepts/device-security","poster":"cetanx","timestamp":"1625747520.0","comments":[{"upvote_count":"1","timestamp":"1626898800.0","poster":"Musk","comment_id":"140586","content":"URL not working"}]},{"comment_id":"118297","poster":"mlantonis","upvote_count":"1","content":"It's B the correct answer","timestamp":"1624528740.0"},{"upvote_count":"2","timestamp":"1623229860.0","content":"B, for sure.\nCloud IoT Core Devices limits, per project, per region, is 'Unlimited', but the default is 100,000\nhttps://cloud.google.com/iot/quotas","comment_id":"105819","poster":"gfhbox0083"},{"timestamp":"1623009720.0","upvote_count":"2","comment_id":"104075","content":"B is the correct answer for me","poster":"Ziegler"},{"timestamp":"1622247360.0","upvote_count":"1","poster":"AD2AD4","comment_id":"97900","content":"Final Decision to go with Option B"},{"comments":[{"upvote_count":"1","comment_id":"106126","content":"A talks about ssl .. That is layer 4 .. Ftp is layer 7 so its wrong option","poster":"CoolCat","timestamp":"1623257220.0","comments":[{"comment_id":"282342","poster":"bnlcnd","timestamp":"1643854620.0","content":"even if B is right, this explanation is wrong. Cloud IoT is mostly for HTTPs or MQTT. Not for FTP.","upvote_count":"1"}]}],"timestamp":"1622161440.0","content":"The answer should be A as I have checked the IOT one seems doesnt have that much rate to support 200k devices","poster":"q4exam","upvote_count":"1","comment_id":"97150"},{"content":"B is correct..IoT Core can be used to generate private public key","upvote_count":"2","comment_id":"76252","timestamp":"1618805400.0","poster":"PRC"},{"timestamp":"1608440400.0","content":"20 Dec, 2019. This question came in my exam.","poster":"ChiggaBoy","comment_id":"31225","upvote_count":"8"},{"timestamp":"1607366460.0","comment_id":"27671","upvote_count":"6","content":"it's B.\nthis link show IoT case.\nhttps://cloud.google.com/community/tutorials/cloud-iot-prometheus-monitoring","poster":"JJu"},{"upvote_count":"9","timestamp":"1606073400.0","comment_id":"23699","content":"I agree its B as well. https://cloud.google.com/iot/docs/how-tos/credentials/keys","poster":"cjsammaejs"},{"upvote_count":"18","comment_id":"17575","timestamp":"1603719000.0","content":"It's B:\n\nhttps://cloud.google.com/iot-core/?utm_source=google&utm_medium=cpc&utm_campaign=emea-es-all-es-dr-bkws-all-all-trial-e-gcp-1007176&utm_content=text-ad-none-any-DEV_c-CRE_253501321408-ADGP_Hybrid+%7C+AW+SEM+%7C+BKWS+~+EXA_M:1_ES_ES_Cloud_IoT_IoT+Core_ES+Localisation-KWID_43700025425525852-kwd-316837064614-userloc_20297&utm_term=KW_google%20iot%20cloud-ST_google+iot+cloud&ds_rl=1242853&ds_rl=1245734&ds_rl=1245734&gclid=CjwKCAjw3c_tBRA4EiwAICs8CqQNooCVeuVB9Ki6V9e3PRdnJLa7LcEce9arjbqUkeqo0jg5F0_Z5BoC3csQAvD_BwE","poster":"jcmoranp"}],"question_id":279,"answers_community":["B (100%)"],"question_text":"For this question, refer to the TerramEarth case study. You are asked to design a new architecture for the ingestion of the data of the 200,000 vehicles that are connected to a cellular network. You want to follow Google-recommended practices.\nConsidering the technical requirements, which components should you use for the ingestion of the data?","unix_timestamp":1571479620,"answer_ET":"B","answer_description":"","choices":{"C":"Compute Engine with project-wide SSH keys","A":"Google Kubernetes Engine with an SSL Ingress","B":"Cloud IoT Core with public/private key pairs","D":"Compute Engine with specific SSH keys"},"url":"https://www.examtopics.com/discussions/google/view/6785-exam-professional-cloud-architect-topic-9-question-6/","exam_id":4,"timestamp":"2019-10-19 12:07:00","topic":"9","isMC":true,"answer":"B"}],"exam":{"provider":"Google","numberOfQuestions":279,"lastUpdated":"11 Apr 2025","isMCOnly":false,"name":"Professional Cloud Architect","isBeta":false,"id":4,"isImplemented":true},"currentPage":56},"__N_SSP":true}