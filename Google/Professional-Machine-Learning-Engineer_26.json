{"pageProps":{"questions":[{"id":"c2L47wNZ03yaCfOj0zz4","answer_images":[],"answer_ET":"D","discussion":[{"poster":"guilhermebutzke","timestamp":"1723649040.0","upvote_count":"7","content":"Selected Answer: D\nMy answer: D\n\nAccording to this documentation, it is very clear that using BigQuery is not a good approach for online prediction at the instance level. That's because we won't use the same code for both training and prediction serving. In the same documentation, the final table on the page recommends using Dataflow with TensorFlow Transform for instance-level data transformation.\n\nhttps://www.tensorflow.org/tfx/guide/tft_bestpractices","comment_id":"1150368"},{"content":"Selected Answer: D\nhttps://www.tensorflow.org/tfx/guide/tft_bestpractices#preprocessing_options_summary","timestamp":"1729075680.0","upvote_count":"1","comment_id":"1196532","poster":"pinimichele01"},{"upvote_count":"2","comment_id":"1145951","content":"Selected Answer: D\nD - Apache Beam + tf.transform or Dataflow.\nhttps://notebook.community/GoogleCloudPlatform/training-data-analyst/courses/machine_learning/deepdive/04_advanced_preprocessing/a_dataflow","timestamp":"1723264920.0","poster":"Yan_X"},{"comment_id":"1124287","upvote_count":"1","content":"Selected Answer: A\nthe simplest way","timestamp":"1721133720.0","poster":"BlehMaks"},{"content":"Selected Answer: D\nD- Vertex AI isn't designed for instance-level data transformations","poster":"shadz10","comments":[{"upvote_count":"1","timestamp":"1721243880.0","comment_id":"1125333","content":"This document also provides an overview of TensorFlow Transform (tf.Transform), a library for TensorFlow that lets you define both instance-level and full-pass data transformation through data preprocessing pipelines. These pipelines are executed with Apache Beam, and they create artifacts that let you apply the same transformations during prediction as when the model is served.\nhttps://www.tensorflow.org/tfx/guide/tft_bestpractices","poster":"shadz10"}],"timestamp":"1721009880.0","comment_id":"1123048","upvote_count":"1"},{"comment_id":"1121300","poster":"pikachu007","timestamp":"1720839120.0","content":"Selected Answer: C\nAddressing limitations of other options:\n\nA. Data validation: While essential, it doesn't guarantee consistency if the preprocessing logic itself differs between pipeline and endpoint.\nC. Sharing code with end users: This shifts the preprocessing burden to end users, potentially leading to inconsistencies and errors, and isn't feasible for real-time inference.\nD. Batching real-time requests: This introduces latency and might not align with real-time requirements, as users expect immediate responses.","upvote_count":"2"}],"topic":"1","question_text":"You need to develop a custom TensorFlow model that will be used for online predictions. The training data is stored in BigQuery You need to apply instance-level data transformations to the data for model training and serving. You want to use the same preprocessing routine during model training and serving. How should you configure the preprocessing routine?","isMC":true,"answer_description":"","question_id":126,"choices":{"C":"Create a preprocessing function that reads and transforms the data from BigQuery. Create a Vertex AI custom prediction routine that calls the preprocessing function at serving time.","B":"Create a pipeline in Vertex AI Pipelines to read the data from BigQuery and preprocess it using a custom preprocessing component.","D":"Create an Apache Beam pipeline to read the data from BigQuery and preprocess it by using TensorFlow Transform and Dataflow.","A":"Create a BigQuery script to preprocess the data, and write the result to another BigQuery table."},"exam_id":13,"answer":"D","unix_timestamp":1705121520,"url":"https://www.examtopics.com/discussions/google/view/131031-exam-professional-machine-learning-engineer-topic-1-question/","answers_community":["D (79%)","14%","7%"],"timestamp":"2024-01-13 05:52:00","question_images":[]},{"id":"uxY96X0xlCTMc2GaOV7C","answer_description":"","answer_images":[],"answer_ET":"A","discussion":[{"poster":"pikachu007","comment_id":"1121301","timestamp":"1705121640.0","content":"Selected Answer: B\nTPU Advantages:\n\nHighly Specialized: TPUs (Tensor Processing Units) are custom-designed hardware accelerators specifically optimized for machine learning workloads, particularly those involving large batch sizes and matrix-heavy computations, common in large language models.\nExceptional Performance: TPUs can significantly outperform CPUs and GPUs in terms of speed and efficiency for these types of tasks.\nCost-Effective: While TPUs might have a higher hourly cost, their exceptional performance often leads to lower overall costs due to faster training times and reduced resource usage.\nTPU Pod Slice:\n\nScalability: TPU Pod slices allow you to distribute training across multiple TPUv4 chips for even greater performance and scalability.\nCustom Operations: The tf.distribute.TPUStrategy ensures compatibility with custom TensorFlow operations,","upvote_count":"9"},{"poster":"AK2020","upvote_count":"5","content":"Selected Answer: A\nB is not correct as TPUs not suitable for TensorFlow custom operations and C doesn't make any sense. A or D?. I would go with A","timestamp":"1722779340.0","comment_id":"1260678"},{"comment_id":"1357193","content":"Answer is B designed and highly optimized for the type of large matrix multiplications and computations involved in training large language models","upvote_count":"1","poster":"NamitSehgal","timestamp":"1739697480.0"},{"upvote_count":"4","content":"Selected Answer: A\nThe question says \"model includes custom TensorFlow operations in the training loop\", this is not supported by TPU. Hence A","poster":"Omi_04040","comment_id":"1324958","timestamp":"1733911560.0"},{"comment_id":"1324792","poster":"Pau1234","content":"Selected Answer: D\nTPUs are not suitable since we are talking about customer operations. Then between A and D. I'd go with D, because it is more cost effective than A. 16g will be more expensive.","upvote_count":"1","timestamp":"1733879280.0"},{"content":"Selected Answer: A\nTPUs not recommended for custom operations","upvote_count":"3","comment_id":"1318821","timestamp":"1732728540.0","poster":"9fbd29a"},{"upvote_count":"2","content":"B is wrong:","poster":"DaleR","comment_id":"1317214","timestamp":"1732481400.0"},{"poster":"f084277","upvote_count":"3","timestamp":"1731631680.0","comment_id":"1312356","content":"All the people voting B are wrong. TPUs cannot be used with TF custom operations"},{"timestamp":"1726135620.0","poster":"baimus","upvote_count":"2","content":"Selected Answer: A\nThis could be A or D, because they both will perform will with custom Tensorflow operations. A is likely to be better with large batch sizes, which require bigger GPUs, so I went A.","comment_id":"1282570"},{"upvote_count":"2","comment_id":"1231904","poster":"info_appsatori","content":"Should be A or D. TPU is ok, but TPUs not suitable for TensorFlow custom operations.","timestamp":"1718623440.0"},{"upvote_count":"4","timestamp":"1718342700.0","poster":"ccb23cc","comment_id":"1230255","content":"Selected Answer: A\nB. TPU Acceleration: the question says that uses Tensorflow custom operations in the main loop and Google documentation literatelly says about TPU use: \"Models with no custom TensorFlow/PyTorch/JAX operations inside the main training loop\" \n\nC. High-CPU Machines: Make no sense because tell you to use a cpu (which does not help us in this case)\n\nSo the correct answer is between A and D. However the question says that they are planning to use a large batch size so we need RAM. Therefore we should take the one with more.\n\nCorrect answer: Option A"},{"comments":[{"timestamp":"1713415080.0","comment_id":"1197689","upvote_count":"2","poster":"fitri001","content":"why not the others?\nA. MultiWorkerMirroredStrategy with GPUs: While GPUs offer some acceleration, TPUs are generally better suited for large language model pre-training due to their architectural optimizations. Additionally, managing 8 workers across separate machines can introduce communication overhead compared to a tightly coupled TPU Pod.\nC. MirroredStrategy with High-CPU Machines: CPU-based training would be significantly slower than TPUs or even GPUs for a large language model. While the high CPU count might seem beneficial for custom operations, the overall training speed would still be limited.\nD. MultiWorkerMirroredStrategy with Multiple High-GPU Machines: Similar to option A, using multiple high-GPU machines with this strategy would incur communication overhead and potentially be less cost-effective compared to a single TPU Pod slice."}],"upvote_count":"2","comment_id":"1197688","content":"Selected Answer: B\nTPU Acceleration: TPUs are specifically designed for machine learning workloads and offer significant speedups compared to GPUs or CPUs, especially for large models like yours. Utilizing a TPU Pod slice provides access to a collection of interconnected TPUs for efficient parallel training.\ntf.distribute.TPUStrategy: This strategy is specifically designed to work with TPUs in TensorFlow. It handles data distribution, model replication, and gradient aggregation across the TPU cores, enabling efficient training with custom TensorFlow operations.","poster":"fitri001","timestamp":"1713415080.0"},{"poster":"BlehMaks","upvote_count":"2","content":"Selected Answer: B\nIt should be TPU but i'm a bit concerned about this point from Google documentation:\nModels with no custom TensorFlow/PyTorch/JAX operations inside the main training loop\nhttps://cloud.google.com/tpu/docs/intro-to-tpu#TPU","timestamp":"1705418160.0","comment_id":"1124313"},{"poster":"b1a8fae","content":"Selected Answer: B\nB.\nNGL quite lost on this one but if the training set is big enough to span over several weeks I would go with the most powerful resource (TPUs) but I might be completely wrong.","comment_id":"1123486","timestamp":"1705332600.0","upvote_count":"3"}],"exam_id":13,"question_id":127,"url":"https://www.examtopics.com/discussions/google/view/131032-exam-professional-machine-learning-engineer-topic-1-question/","choices":{"C":"Implement 16 workers of c2d-highcpu-32 machines by using tf.distribute.MirroredStrategy.","B":"Implement a TPU Pod slice with -accelerator-type=v4-l28 by using tf.distribute.TPUStrategy.","D":"Implement 16 workers of a2-highgpu-8g machines by using tf.distribute.MultiWorkerMirroredStrategy.","A":"Implement 8 workers of a2-megagpu-16g machines by using tf.distribute.MultiWorkerMirroredStrategy."},"answers_community":["A (51%)","B (46%)","3%"],"unix_timestamp":1705121640,"question_text":"You are pre-training a large language model on Google Cloud. This model includes custom TensorFlow operations in the training loop. Model training will use a large batch size, and you expect training to take several weeks. You need to configure a training architecture that minimizes both training time and compute costs. What should you do?","answer":"A","topic":"1","question_images":[],"isMC":true,"timestamp":"2024-01-13 05:54:00"},{"id":"Pp8NpeOSC7JlaIxory64","question_images":[],"answer_images":[],"answers_community":["D (52%)","C (48%)"],"exam_id":13,"isMC":true,"answer_ET":"D","choices":{"B":"Use the MLFlow SDK and deploy it on a Google Kubernetes Engine cluster. Create multiple components that use Dataflow and Vertex AI services.","C":"Use the Kubeflow Pipelines (KFP) SDK to create multiple components that use Dataflow and Vertex AI services. Deploy the workflow on Vertex AI Pipelines.","A":"Use the Apache Airflow SDK to create multiple operators that use Dataflow and Vertex AI services. Deploy the workflow on Cloud Composer.","D":"Use the TensorFlow Extended (TFX) SDK to create multiple components that use Dataflow and Vertex AI services. Deploy the workflow on Vertex AI Pipelines."},"topic":"1","discussion":[{"poster":"pinimichele01","content":"Selected Answer: C\nIf you use TensorFlow in an ML workflow that processes terabytes of structured data or text data, we recommend that you build your pipeline using TFX.\nFor other use cases, we recommend that you build your pipeline using the Kubeflow Pipelines SDK\n\nhttps://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline#sdk","timestamp":"1712565120.0","comment_id":"1191466","upvote_count":"5"},{"content":"Selected Answer: C\nTFX -> processes terabytes of structured data or text data","poster":"kornick","upvote_count":"1","timestamp":"1736927040.0","comment_id":"1340724"},{"comment_id":"1287505","timestamp":"1726966800.0","poster":"wences","content":"Selected Answer: D\nin this one will go with D, TFX is more specialized than kfp","upvote_count":"2"},{"content":"Selected Answer: D\nTFX is going to be easier than kubeflow with custom code, as it basically does exactly what is listed there, by default.","upvote_count":"2","timestamp":"1726135800.0","comment_id":"1282572","poster":"baimus"},{"timestamp":"1719590400.0","content":"Selected Answer: D\nAgree with TFX","poster":"dija123","comment_id":"1238786","upvote_count":"2"},{"timestamp":"1718039460.0","content":"Selected Answer: D\nD) TFX is the way forward as it has services to support every step of the use case presented.","comment_id":"1228002","poster":"PhilipKoku","upvote_count":"2"},{"timestamp":"1713415440.0","comments":[{"timestamp":"1713415440.0","upvote_count":"2","content":"why not others?\nA. Airflow with Dataflow and Vertex AI: While Airflow is a powerful workflow management tool, deploying it on Cloud Composer adds additional complexity compared to the managed environment of Vertex AI Pipelines.\nB. MLflow with Dataflow and Vertex AI: MLflow focuses primarily on model lifecycle management. While it can be used for building pipelines, KFP offers a more specialized and user-friendly approach for this specific use case.\nD. TFX with Dataflow and Vertex AI: TFX is a comprehensive end-to-end ML platform. While it offers several functionalities, it might be an overkill for this scenario focusing on data processing, training, and validation. KFP provides a simpler solution for this specific workflow.","poster":"fitri001","comment_id":"1197692"}],"upvote_count":"2","poster":"fitri001","comment_id":"1197691","content":"Selected Answer: C\nKFP Pipelines: Kubeflow Pipelines (KFP) is a popular open-source framework for building and deploying machine learning workflows. It provides a user-friendly SDK for defining pipelines as components and simplifies workflow orchestration.\nVertex AI Pipelines Integration: Vertex AI Pipelines is a managed service from Google Cloud that integrates seamlessly with KFP. You can deploy your KFP-defined workflow on Vertex AI Pipelines, leveraging its features like scheduling, monitoring, and versioning.\nDataflow and Vertex AI Services: Both Dataflow and Vertex AI are Google Cloud services well-suited for this workflow"},{"content":"Selected Answer: D\nC and D are valid options. if the model is created in TF, use TFX, in any other case, use KFP; therefore, here is D","comments":[],"upvote_count":"2","timestamp":"1705657620.0","comment_id":"1126571","poster":"winston9"},{"comment_id":"1124349","timestamp":"1705421940.0","content":"Selected Answer: C\nhttps://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline#sdk","poster":"BlehMaks","upvote_count":"3"},{"content":"Selected Answer: D\nAirflow (A): While versatile, Airflow often requires more manual configuration and integration with ML services, potentially increasing maintenance effort.\nMLFlow (B): MLFlow focuses on experiment tracking and model management, lacking built-in pipeline components for data processing and model training.\nKubeflow Pipelines (C): KFP is flexible but requires more setup and infrastructure management compared to TFX's managed services.","timestamp":"1705121940.0","upvote_count":"2","comment_id":"1121305","comments":[],"poster":"pikachu007"}],"answer":"D","timestamp":"2024-01-13 05:59:00","unix_timestamp":1705121940,"question_id":128,"question_text":"You are building a TensorFlow text-to-image generative model by using a dataset that contains billions of images with their respective captions. You want to create a low maintenance, automated workflow that reads the data from a Cloud Storage bucket collects statistics, splits the dataset into training/validation/test datasets performs data transformations trains the model using the training/validation datasets, and validates the model by using the test dataset. What should you do?","url":"https://www.examtopics.com/discussions/google/view/131033-exam-professional-machine-learning-engineer-topic-1-question/","answer_description":""},{"id":"4yomMgbgzjXvRgrRIn6J","isMC":true,"question_id":129,"question_images":[],"choices":{"B":"Use the Vertex AI ModelEvaluationOp component to evaluate the model","D":"Chain the Vertex AI ModelUploadOp and ModelDeployOp components together","C":"Use the Vertex AI SDK for Python within a custom component based on a python:3.10 image","A":"Use the Vertex AI REST API within a custom component based on a vertex-ai/prediction/xgboost-cpu image"},"answer_images":[],"unix_timestamp":1705122060,"timestamp":"2024-01-13 06:01:00","answer_description":"","discussion":[{"timestamp":"1729227000.0","upvote_count":"2","comments":[{"timestamp":"1729227000.0","comment_id":"1197699","poster":"fitri001","upvote_count":"1","content":"why not the others?\nA. Custom Component with Vertex AI REST API: While this approach provides flexibility, it requires writing custom code to interact with the Vertex AI REST API within a container image. This adds complexity compared to using pre-built components.\nB. ModelEvaluationOp: This component is designed for model evaluation within the pipeline, not for uploading or deploying models.\nC. Custom Component with Python SDK: Similar to option A, using the Python SDK within a custom component offers flexibility but requires writing more code compared to using the pre-built ModelUploadOp and ModelDeployOp components."}],"content":"Selected Answer: D\nBuilt-in Functionality: Both ModelUploadOp and ModelDeployOp are pre-built components within Vertex AI Pipelines specifically designed for uploading models and deploying them to endpoints.\nEase of Use: These components offer a user-friendly interface within the pipeline definition. You only need to specify essential details like the model path, container image URI (pre-built for XGBoost is available), endpoint configuration, etc.\nReduced Code Complexity: Using these components eliminates the need for writing custom code within your pipeline for model upload and deployment, simplifying your pipeline logic.","poster":"fitri001","comment_id":"1197698"},{"timestamp":"1728376500.0","poster":"pinimichele01","comment_id":"1191471","upvote_count":"2","content":"Selected Answer: D\nhttps://cloud.google.com/vertex-ai/docs/pipelines/model-endpoint-component"},{"comment_id":"1125360","poster":"shadz10","content":"Selected Answer: D\nhttps://cloud.google.com/vertex-ai/docs/pipelines/model-endpoint-component","timestamp":"1721245140.0","upvote_count":"2"},{"comment_id":"1121306","timestamp":"1720839660.0","content":"Selected Answer: D\nA. Custom Component with REST API: This involves more manual coding and understanding of REST API endpoints, potentially increasing complexity and maintenance.\nB. ModelEvaluationOp: This component is primarily for model evaluation, not model upload and deployment.\nC. Custom Component with SDK: While feasible, it involves more setup and dependency management compared to using built-in components.","poster":"pikachu007","upvote_count":"1"}],"url":"https://www.examtopics.com/discussions/google/view/131034-exam-professional-machine-learning-engineer-topic-1-question/","answer_ET":"D","topic":"1","exam_id":13,"answers_community":["D (100%)"],"answer":"D","question_text":"You are developing an ML pipeline using Vertex AI Pipelines. You want your pipeline to upload a new version of the XGBoost model to Vertex AI Model Registry and deploy it to Vertex AI Endpoints for online inference. You want to use the simplest approach. What should you do?"},{"id":"pX6hhOUFVLFstSaDiCim","timestamp":"2024-01-13 06:02:00","unix_timestamp":1705122120,"answer_images":[],"discussion":[{"content":"Selected Answer: C\nQuick Implementation: BigQuery ML simplifies the process. You can train and deploy the model directly within BigQuery, eliminating the need for complex model deployment or data movement.\nMinimal Effort: ARIMA_PLUS is a pre-built statistical model available in BigQuery ML. You don't need to write custom code for a complex neural network (NN) model like in option B or D.\nTime Series Data: ARIMA models are well-suited for time series forecasting, which is ideal for your monthly sales prediction task.","comment_id":"1197701","comments":[{"upvote_count":"1","content":"why not others?\nA. Prophet on Vertex AI Training: While Prophet is a good choice for time series forecasting with holidays and seasonality, using Vertex AI Training requires additional setup and potentially custom code compared to the readily available ARIMA_PLUS model within BigQuery ML.\nB. Vertex AI Forecast with NN-based Model: Building a custom NN-based model using Vertex AI Forecast offers flexibility but requires more effort and expertise in model development and potentially hyperparameter tuning. This might not be ideal for a quick implementation.\nD. TensorFlow on Vertex AI Training: Similar to option B, using TensorFlow for a custom model offers flexibility but requires significant coding and expertise, making it less suitable for a quick and low-effort approach.","poster":"fitri001","comment_id":"1197703","timestamp":"1729227240.0"}],"timestamp":"1729227180.0","upvote_count":"2","poster":"fitri001"},{"content":"Selected Answer: C\ndata on bigquery + minimal effort -> C","poster":"pinimichele01","upvote_count":"1","timestamp":"1728376560.0","comment_id":"1191472"},{"upvote_count":"1","comment_id":"1124019","content":"Selected Answer: C\nGiven amount of data (few thousand short-cycled products) and frequency of predictions (monthly) C is the way to go.","poster":"b1a8fae","timestamp":"1721111880.0"},{"upvote_count":"4","timestamp":"1720839720.0","comment_id":"1121307","poster":"pikachu007","content":"Selected Answer: C\nEase of Use: BigQuery ML integrates seamlessly with BigQuery, allowing you to create and train models directly within SQL queries, eliminating the need for separate environments or coding.\nStatistical ARIMA_PLUS Strengths: This model is well-suited for time series forecasting, automatically handling seasonality, trends, and holidays, making it appropriate for monthly sales predictions.\nMinimal Effort: BigQuery ML handles model training and tuning, reducing the need for manual configuration or hyperparameter tuning.\nFast Implementation: Model creation and training can be done in a few lines of SQL, enabling rapid deployment."}],"answer":"C","question_images":[],"choices":{"D":"Use TensorFlow on Vertex AI Training to build a custom model.","B":"Use Vertex AI Forecast to build a NN-based model.","A":"Use Prophet on Vertex AI Training to build a custom model.","C":"Use BigQuery ML to build a statistical ARIMA_PLUS model."},"isMC":true,"topic":"1","answer_description":"","answer_ET":"C","answers_community":["C (100%)"],"exam_id":13,"question_text":"You work for an online retailer. Your company has a few thousand short lifecycle products. Your company has five years of sales data stored in BigQuery. You have been asked to build a model that will make monthly sales predictions for each product. You want to use a solution that can be implemented quickly with minimal effort. What should you do?","url":"https://www.examtopics.com/discussions/google/view/131035-exam-professional-machine-learning-engineer-topic-1-question/","question_id":130}],"exam":{"isMCOnly":true,"numberOfQuestions":304,"name":"Professional Machine Learning Engineer","provider":"Google","id":13,"isImplemented":true,"isBeta":false,"lastUpdated":"11 Apr 2025"},"currentPage":26},"__N_SSP":true}